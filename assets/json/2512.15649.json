{
    "paper_title": "VTCBench: Can Vision-Language Models Understand Long Context with Vision-Text Compression?",
    "authors": [
        "Hongbo Zhao",
        "Meng Wang",
        "Fei Zhu",
        "Wenzhuo Liu",
        "Bolin Ni",
        "Fanhu Zeng",
        "Gaofeng Meng",
        "Zhaoxiang Zhang"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "The computational and memory overheads associated with expanding the context window of LLMs severely limit their scalability. A noteworthy solution is vision-text compression (VTC), exemplified by frameworks like DeepSeek-OCR and Glyph, which convert long texts into dense 2D visual representations, thereby achieving token compression ratios of 3x-20x. However, the impact of this high information density on the core long-context capabilities of vision-language models (VLMs) remains under-investigated. To address this gap, we introduce the first benchmark for VTC and systematically assess the performance of VLMs across three long-context understanding settings: VTC-Retrieval, which evaluates the model's ability to retrieve and aggregate information; VTC-Reasoning, which requires models to infer latent associations to locate facts with minimal lexical overlap; and VTC-Memory, which measures comprehensive question answering within long-term dialogue memory. Furthermore, we establish the VTCBench-Wild to simulate diverse input scenarios.We comprehensively evaluate leading open-source and proprietary models on our benchmarks. The results indicate that, despite being able to decode textual information (e.g., OCR) well, most VLMs exhibit a surprisingly poor long-context understanding ability with VTC-compressed information, failing to capture long associations or dependencies in the context.This study provides a deep understanding of VTC and serves as a foundation for designing more efficient and scalable VLMs."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 7 1 ] . [ 1 9 4 6 5 1 . 2 1 5 2 : r VTCBench: Can Vision-Language Models Understand Long Context with Vision-Text Compression? Hongbo Zhao,1,2, Meng Wang,3 , Fei Zhu3 , Wenzhuo Liu1,2, Bolin Ni4 , Fanhu Zeng1,2, Gaofeng Meng1,2,3 , Zhaoxiang Zhang1,2 1Institute of Automation, Chinese Academy of Sciences 2School of Artificial Intelligence, University of Chinese Academy of Sciences 3Centre for Artificial Intelligence and Robotics, Hong Kong Institute of Science & Innovation, CAS 4Tencent Hunyuan Team Equal Contributor, Corresponding Author"
        },
        {
            "title": "Abstract",
            "content": "The computational and memory overheads associated with expanding the context window of LLMs severely limit their scalability. noteworthy solution is vision-text compression (VTC), exemplified by frameworks like DeepSeek-OCR [51] and Glyph [9], which convert long texts into dense 2D visual representations, thereby achieving token compression ratios of 3x20x. However, the impact of this high information density on the core long-context capabilities of vision-language models (VLMs) remains under-investigated. To address this gap, we introduce the first benchmark for VTC and systematically assess the performance of VLMs across three long-context understanding settings: VTC-Retrieval, which evaluates the models ability to retrieve and aggregate information; VTC-Reasoning, which requires models to infer latent associations to locate facts with minimal lexical overlap; and VTC-Memory, which measures comprehensive question answering within long-term dialogue memory. Furthermore, we establish the VTCBench-Wild to simulate diverse input scenarios. We comprehensively evaluate leading open-source and proprietary models on our benchmarks. The results indicate that, despite being able to decode textual information (e.g., OCR) well, most VLMs exhibit surprisingly poor long-context understanding ability with VTC-compressed information, failing to capture long associations or dependencies in the context. This study provides deep understanding of VTC and serves as foundation for designing more eﬀicient and scalable VLMs. (cid:135) Github: https://github.com/Moenupa/VTCBench database Huggingface: https://huggingface.co/datasets/MLLM-CL/VTCBench"
        },
        {
            "title": "1 Introduction",
            "content": "The dramatic success of large language models (LLMs) has been accompanied by persistent challenge: the scalability of their context window. As the context length expands, the computational and memory costs increase rapidly, severely limiting practical deployment and training eﬀiciency. Existing techniques, including eﬀicient attention [5, 12, 58], position encoding extrapolation [14, 37, 38, 56], prompt compression [16, 59, 61] and external memory [47, 50, 62], offer partial solutions. However, these methods often suffer from notable performance degradation when extrapolated to much longer contexts. Recently, new paradigm, vision-text compression (VTC), also known as context optical compression, has emerged to address this bottleneck, as illustrated in Figure 1a. Recent works like DeepSeek-OCR [51] and Glyph [9] propose rendering long text documents into compressed two-dimensional image sequences, thereby leveraging the high information density of the visual modality. This cross-modal approach achieves significantly higher compression ratios, 1 clipboard-question question: What are all the special magic numbers for absorbed-cloakroom and threatening-blessing mentioned in the provided text? needle1: One of the special magic numbers for absorbed-cloakroom is: 7456447., needle2: One of the special magic numbers for threatening-blessing is: 6921626. Φtext tokenize render barcode Tokens barcode Tokens image Images 2-10 compress LLM VLM process Φvision e C L Retrieval Reasoning Memory Wild VLM dizzy dizzy t v c r e (a) (b) Figure 1 (a) Vision-text compression offers more eﬀicient alternative for long-context tasks: instead of feeding plain text directly to LLMs, it renders text as compact images for VLM, achieving substantial input-token compression. (b) We propose VTCBench and VTCBench-Wild to comprehensively evaluate the long context understanding ability of VLMs within vision-text compression framework. ranging from 3 to 20 fewer input tokens compared to the original text sequence, offering novel avenue for eﬀicient long-context modeling. By transforming the input space from token sequence to pixel grid, VTC shifts the burden of information management from sequential attention to spatial and visual reasoning. As novel long context modeling paradigm, VTC [9, 51] has been demonstrated to not only offer substantial input-token compression but also yield strong performance for OCR tasks (e.g., document parsing [36], multilingual recognition) or long context processing [3, 20]. However, key argument is that such benchmarks, either explicitly or implicitly, rely on identifying exact textual correspondences between the query and the provided context for success [33]. Therefore, natural and important question arises: Can vision-language models really understand long context with vision-text compression? Intuitively, the shift from sparse, sequential text input relying on linear positional encodings to dense, twodimensional visual input fundamentally changes how information is encoded and processed by the models attention mechanism. Therefore, the reliance of VLMs on spatial locality in the 2D input might lead to unique sensitivities in long-context understanding. In this work, we present the first systematic benchmark, VTCBench (Figure 1b), specifically designed for the VTC framework, rigorously quantifying the long-content comprehension capabilities of VLM models. Precisely, we focus on three critical tasks: (1) VTC-Retrieval requires models to retrieve, trace, and aggregate information (needles) placed at varying distances within random text (haystack). (2) VTC-Reasoning tests the VLMs capacity of associative reasoning [33] over long context within the compressed visual space. The queried input has minimal literal matches with the context, thus assessing the models ability to reason associatively beyond mere lexical retrieval. (3) VTC-Memory assesses the VLMs performance in very long-term dialogue memory [10, 32, 53, 62], evaluating the models resilience to temporal and structural degradation under vision-text compression. In addition, to fully capture the robustness against real-world visual variations, we introduce challenging variant, VTCBench-Wild. These VTC-oriented long-context comprehension tasks establish foundation to understand the effectiveness of VTC, offering crucial guidance for future design of highly eﬀicient and capable next-generation long-context VLMs. We conduct comprehensive evaluation of leading open-source and proprietary VLMs on VTCBench and VTCBench-Wild, yielding the following findings: Overall, existing VLM architectures exhibit weaker long-context comprehension abilities under the VTC framework compared to dedicated text-only LLMs. This indicates that existing VLM architectures have significant room for improvement when processing information compressed via VTC. In simple retrieval and matching-related tasks (e.g., simple Needle-in-a-Haystack), VLMs demonstrate relatively strong performance. This suggests that the VTC approach, which compresses long text 2 Figure 2 Long context understanding performance on VTCBench (first row) and VTCBench-Wild (second row). Under the VTC paradigm, existing VLMs show good textual perception ability, leading to relatively strong performance in simple retrieval tasks. However, they still exhibit weaker long-content comprehension (associative reasoning and long-term dialogue memory) compared to LLMs, highlighting substantial opportunity to enhance VLMsespecially when processing vision-text compression-based information. into an image representation, minimally impacts the models fundamental ability to perceive textual content. In complex long-context understanding tasks, such as associative reasoning and long-term dialogue memory, the performance of existing VLMs under vision-text compression is notably poor, falling significantly below that of LLMs (e.g., Qwen3-8B [57]). This demonstrates that while VTC maintains satisfactory OCR performance, it seriously impedes core capabilities such as advanced associative reasoning and knowledge integration within long contexts. To gain deeper insights into VLM failures, we conducted qualitative error analysis that revealed recurring issues ranging from fundamental retrieval and visual-to-textual grounding to complex deficiencies in logical and associative reasoning. Further experiments show that the VLM performance under VTC is critically dependent on both the font size and the spatial position of the information within the compressed image. We also establish VTCBench-Wild to simulate visual diversity encountered in real-world scenarios."
        },
        {
            "title": "2.1 Long-Context Modeling",
            "content": "The challenge of eﬀiciently processing extensive textual contexts remains persistent pursuit for large models, driven by the inherent complexity of the standard Transformer self-attention mechanism [31]. Recent 3 advancements aim at developing long context language models that can be broadly categorized into architectural modifications and external component integration for workflow-based augmentation. Architectural modifications improve model eﬀiciency internally through sparse and hierarchical attention [4, 15, 52, 54], and recurrent transformer [23, 34]. Additionally, positional encoding extrapolation methods [14, 37, 38, 56] adjust position encodings to handle larger sequence lengths. complementary line of research utilizes external components to intelligently manage and compress context through workflow-based augmentation. Prompt compression [16, 59, 61] reduces the input size through selective token retention or text summarization, thereby optimizing the utility of fixed context window. Retrieval-augmented generation [6, 17] shifts the paradigm by replacing long-context ingestion with dynamic information extraction or recall. Memorybased methods [47, 50, 62] utilize external memory modules to store and retrieve long-term conversational or document history, ensuring contextual coherence over long interactions without burdening the active context window. Rather than generating human-readable memory, C3 [30] introduces pure text-to-latent compression pipeline using two cascaded LLMs to achieve superior text compression ratios. Besides, there are also some training strategies [2, 7, 21] to enable the model to handle long contexts."
        },
        {
            "title": "2.2 Vision-Text Compression",
            "content": "VTC presents cross-modal paradigm that addresses the long-context challenge by leveraging visual encoding to increase information density. Several recent studies have pioneered this framework. For instance, VIST [55] compresses distant, low-salience context by rendering it into images; these are subsequently processed by lightweight vision encoder to produce semantically dense visual tokens, achieving 2.3 token reduction while maintaining state-of-the-art accuracy on in-context learning benchmarks. Similarly, VisInContext [44] and Li et al., [28] transform long textual sequences into visual representations, significantly lowering computational overhead while enhancing performance in document-understanding tasks. Further advancing this paradigm, DeepSeek-OCR [51] demonstrates the feasibility of high-ratio compression via optical 2D mapping, achieving over 96% OCR decoding precision at 910 compression ratio. Concurrently, Glyph [9] renders long text contexts into compact images and processes them with VLMs. The system successfully demonstrated stable 34 token compression for long text sequences, while preserving performance on OCR and other longcontext processing tasks [3, 20]. These works undertake preliminary exploration of utilizing the visual modality as an eﬀicient compression medium to facilitate long textual information processing."
        },
        {
            "title": "2.3 Long-Context Comprehension Benchmarks",
            "content": "The Needle-In-A-Haystack (NIAH) test is classic method for evaluating long-context understanding capability of LLMs. The vanilla NIAH benchmark [25] evaluates simple retrieval robustness by embedding specific fact (needle) within long, irrelevant context (haystack). While useful for measuring basic context window functionality, it falls short of testing context understanding ability. Later, the NIAH task was enhanced by introducing multiple targets, adding distractor noise, and linking facts to enforce complex, or associative reasoning [20, 26, 43]. For example, NoLiMa [43] requires models to perform indirect inference, where the query and the required context do not share direct lexical overlap, forcing the model to rely on deeper semantic understanding and association maintenance across long distances. Beyond text-only benchmarks, NIAH in multimodal tasks has also been explored recently [45, 48]. Several benchmarks [10, 32, 53, 62] have been designed to evaluate long-term dialogue memory and understanding over very extended conversations."
        },
        {
            "title": "3 VTCBench",
            "content": "VTC introduces new paradigm that uses visual modality as an eﬀicient compression medium for textual information processing in LLMs. Formally, given long text = (t1, t2, , tNT ), where ti is the i-th text token, we use rendering operator to transfer text into images = R(T ) = (I1, I2, , Im). VLM ΦVLM encodes these images into NI visual tokens, i.e., ΦVLM(I) = (v1, v2, , vNI ), where vi denotes the i-th visual token fed to the LLM component of the VLMs. We define the VTC ratio as rVTC = NT NI . 4 (1) Given that different VLMs adopt different image-processing strategies [1, 9, 49, 51], the visual tokens of the same image differ. Therefore, VTCBench comprises two settings (predefined VTC ratio and predefined VTC rendering), where each setting contains three tasks across different long context understanding abilities: VTC-Retrieval, VTC-Reasoning, and VTC-Memory. Figures S3a to S3c show examples of VTCBench."
        },
        {
            "title": "3.1 VTC Tasks",
            "content": "VTC-Retrieval NIAH [20, 25, 43] test is standard evaluation in natural language processing that measures models ability to understand long contexts. These benchmarks are often characterized by retrieval-based tasks, where the querys solution has direct, verbatim correspondence within the source context. VTCRetrieval is visual NIAH task that provides dense-text images as context. Following RULER [20], we design 4 sub-tasks, with different settings of needle (kay-value pairs) inserted into haystack (long distractor texts, i.e., Paul Graham essays [24]) to formulate context. Single NIAH (S-NIAH): This setting represents the vanilla NIAH test, wherein the model is tasked with retrieving single key-value pair (the needle) from the haystack. Multi-keys NIAH (MK-NIAH): Multiple distinct key-value pairs are inserted, but the model must retrieve the value corresponding to exactly one specific key. This sub-task assesses the models ability to disregard hard distractors and precisely locate the queried information. Multi-values NIAH (MV-NIAH): This sub-task involves inserting multiple needles that share an identical key, requiring the model to retrieve all associated values. It is designed to evaluate the models capacity for high-recall retrieval without omitting relevant information for given key. Multi-queries NIAH (MQ-NIAH): In this setting, the model must retrieve multiple, distinct key-value pairs in response to single request that queries all keys. This task evaluates the models proficiency in multi-query associative recall, testing its ability to return complete set of requested items. VTC-Reasoning Literal matches make it much easier for VLMs to locate the relevant information and answer correctly [33]. The model only needs to implicitly translate the image into text, possibly without understanding the meaning of the text. Therefore, our VTC-Reasoning adds associative reasoning to minimize literal overlap between questions and their corresponding needles. Questions and needles encompass keywords whose relationships are defined by associative links, such as those informed by real-world information or commonsense facts. VTC-Memory In VTC-Retrieval and VTC-Reasoning tasks, the needles have no relationship with the haystack [18], which can not comprehensively evaluate the performance of long context understanding. Hence, we integrate long-term memory task into our benchmark. Long-term memory [10, 29, 32, 53, 62] is the capability to distill, store, and retrieve key information, such as persona details and causally linked events. We adapt LoCoMos [32] corpus to formulate VTC-Memory. Each sample consists of long, multi-turn dialogue as contextual passages, and question-answer pair for evaluation. VTC-Memory spans over 4 sub-tasks: Singlehop: The answer can be derived from single utterance or fact. Multihop: The answer requires aggregating information from multiple turns. Temporal: The answer depends on the ordering or timing of events within the dialogue. Opendomain: The answer draws on world knowledge beyond the immediate conversation."
        },
        {
            "title": "3.2 VTC settings",
            "content": "To systematically evaluate how VLMs perform under the VTC paradigm, it is crucial to control the experimental conditions. As illustrated in Figure 3, the final compression ratio, rVTC, is function of both the rendering operator R, which converts text to images, and the VLMs visual encoder Φvision, which encodes images into visual tokens. To disentangle these factors and provide comprehensive analysis, we design two distinct evaluation settings. 5 align-left text Rendering images images Tokenizer Φtext barcode text tokens Vision-Text Compression ratio rVTC Φvision VLM Processor barcode visual tokens Figure 3 Illustration of the vision-text compression ratio (rVTC). The ratio is determined by the interplay between the rendering operator and the VLMs visual encoder Φvision compared to standard text tokenization. Predefined VTC Ratio The primary objective of this setting is to establish controlled benchmark for direct comparison of different VLMs comprehension abilities. To achieve this, we fix the vision-text compression ratio rVTC to predefined target value (e.g., rVTC = 2). Since different VLMs employ different visual encoding strategies (as detailed in Section B), they generate varying number of visual tokens NI for the same input image. Consequently, to maintain constant rVTC across all models, we must dynamically adjust the rendering operator for each specific model. The most practical parameter to adjust within to control the density of the rendered text is the font size. The relationship between font size and rVTC can be approximated as follows, assuming negligible spacing effects. For given number of words nword rendered into nimg images of size (Himg, Wimg): fontsize2 Hch Wch nimgHimgWimg nchar_per_word nword NI nchar_per_word NT 1 nchar_per_word rVTC , = (2) (3) (4) (5) where Hch and Wch denote the pixel height and width for character, respectively; spacing such as kerning, tracking, line spacing, indentation, and margins are ignored; the number of characters per word nchar_per_word is statistically constant. Therefore, we approximate the font size of each model by fontsize 1 rVTC . (6) By using this relationship, we can set model-specific font size to ensure consistent compression ratio. To minimize performance variations caused by model-specific image preprocessing, we standardize the image size to 896 896 pixels, dimension divisible by common patch sizes (e.g., 14, 16) and tile sizes (e.g., 448). This approach allows us to isolate and assess the intrinsic long-context understanding capability of each VLM at standardized level of information density. Predefined VTC Rendering Conversely, this setting simulates more realistic scenario where the visual In this representation of the text is standardized, akin to processing document with fixed formatting. setup, we fix the rendering operator by defining consistent preset for all models. As result, while the input images are identical across all evaluations, the resulting compression ratio rVTC will vary, reflecting the unique eﬀiciency and architecture of each models vision processor Φvision. In this setting, we adopt plain-text rendering preset Rplain = { dpi = 96, font-family = Helvetica, font-size = 12, line-height = 1.2, margins = 0, img-size = (896, 896)}. This choice balances four goals: following standard typesetting conventions (print & digital), optimizing visionencoder performance, speeding up evaluation, and improving visiontext compression12-pt Helvetica offers readability and moderate glyph 6 Table 1 VTCBench performance (%) on retrieval, reasoning, and memory tasks. rVTC denotes vision-text compression ratio; for VTC-Retrieval and VTC-Reasoning, 1k, 2k, , 32k denotes equivalent text-form context length in text tokens; for VTC-Memory, SH, MH, TP, and OD denotes Single-Hop, Multi-Hop, Temporal, and Open-Domain subtasks respectively. All Qwen-VL models are instruct models. GPT-5 uses image dimensions (768, 768) in the predefined compression ratio setting to avoid resizing. Retrieval (containsAll ) Reasoning (containsAll ) Memory (ROUGE-L ) Model rVTC 1k 2k 4k 8k 16k 32k 1k 2k 4k 8k 16k 32k SH MH TP OD Qwen3-8B (LLM) [57] NA 98.86 98.86 96.82 97.16 96.59 95.57 94.18 81.45 66.18 49.82 36.18 17.45 52.55 30.23 27.83 17.96 Predefined Compression Ratio rVTC = 2.00 0.02 2.00 87.05 57.61 48.98 44.21 44.32 42.96 26.73 11.45 9.27 6.73 3.45 2.18 54.21 34.87 19.11 22.03 Kimi-VL-A3B [40] 1.98 100.0 53.64 54.54 54.55 38.64 40.57 96.18 92.51 32.36 15.82 10.18 OOM 43.28 26.18 14.69 17.53 Gemini-2.5-Pro [11] Gemma3-27B [39] OOM 26.31 19.08 10.76 7.20 2.00 88.86 84.89 82.27 74.43 OOM GLM-4.1V-9B-Thinking [19] 2.00 17.16 11.48 7.73 8.64 3.87 2.50 14.55 9.09 4.18 2.18 1.64 1.27 1.37 1.45 0.52 2.22 2.00 91.48 81.37 84.89 78.53 80.46 75.68 40.09 22.07 17.65 11.76 9.12 5.05 4.10 2.43 1.69 2.16 Glyph [9] 2.00 81.93 74.55 71.14 62.73 60.68 57.16 58.73 41.82 25.64 26.82 25.00 22.73 50.41 30.81 27.36 24.38 GPT-5 [35] 2.00 15.00 16.59 9.89 11.71 8.52 7.96 3.09 3.27 3.09 2.36 0.36 0.00 33.19 23.55 24.49 12.35 InternVL3.5-8B [49] 2.00 20.11 14.66 11.82 12.96 9.78 9.89 4.18 5.82 3.27 3.09 1.09 0.18 40.96 27.92 18.54 17.19 InternVL3.5-38B [49] 2.00 85.23 75.57 74.77 76.82 76.59 69.21 32.60 15.52 11.13 6.71 3.86 1.60 52.22 32.60 23.76 18.30 Qwen2.5-VL-7B [1] 2.00 88.98 84.55 87.73 86.59 82.27 76.48 52.79 36.55 33.29 24.04 15.61 9.52 55.55 31.61 20.96 18.75 Qwen2.5-VL-72B [1] 2.00 95.23 88.41 87.39 88.98 86.93 79.59 23.27 21.45 18.18 9.27 6.73 4.18 46.09 29.31 23.63 15.43 Qwen3-VL-8B [41] 2.00 97.16 88.52 94.77 92.16 91.21 81.34 23.27 20.55 19.82 17.09 11.82 8.91 45.10 32.62 26.77 19.71 Qwen3-VL-235B-A22B [41] OOM 19.09 16.91 12.18 7.64 5.27 Predefined Rendering Rplain Rconversation OOM 1.69 0.38 0.06 0.00 0.00 3.12 18.18 21.82 14.66 0.34 0.00 Deepseek-OCR [51] 2.00 87.05 57.61 48.98 44.21 44.32 42.96 26.73 11.45 9.27 6.73 3.45 2.18 54.21 34.87 19.11 22.03 Kimi-VL-A3B [40] 1.98 100.0 53.64 54.54 54.55 38.64 40.57 96.18 92.51 32.36 15.82 10.18 OOM 43.28 26.18 14.69 17.53 Gemini-2.5-Pro [11] Gemma3-27B [39] 8.00 22.05 17.96 22.50 18.52 14.43 13.86 2.73 2.73 1.64 1.82 1.64 1.64 18.12 15.34 10.56 8.45 GLM-4.1V-9B-Thinking [19] 2.00 17.16 11.48 7.73 8.64 3.87 2.50 14.55 9.09 4.18 2.18 1.64 1.27 1.37 1.45 0.52 2.22 2.00 91.48 81.37 84.89 78.53 80.46 75.68 40.09 22.07 17.65 11.76 9.12 5.05 4.10 2.43 1.69 2.16 Glyph [9] 3.17 45.57 38.64 32.05 32.96 29.09 25.34 32.51 10.47 11.22 10.85 11.07 8.12 38.94 27.86 23.69 26.66 GPT-5 [35] 1.56 23.64 21.82 23.64 20.80 16.02 12.50 7.45 6.00 3.64 2.91 3.09 0.73 22.29 18.30 21.38 15.65 InternVL3.5-8B [49] 1.56 21.59 21.25 21.82 21.25 17.73 18.98 6.36 6.00 4.91 5.45 3.64 2.55 24.53 19.84 18.47 17.20 InternVL3.5-38B [49] 2.00 85.23 75.57 74.77 76.82 76.59 69.21 32.60 15.52 11.13 6.71 3.86 1.60 52.22 32.60 23.76 18.30 Qwen2.5-VL-7B [1] 2.00 88.98 84.55 87.73 86.59 82.27 76.48 52.79 36.55 33.29 24.04 15.61 9.52 55.55 31.61 20.96 18.75 Qwen2.5-VL-72B [1] 2.55 94.21 72.39 66.48 61.48 49.43 53.30 17.45 3.45 2.73 0.91 0.91 0.91 37.03 27.83 22.37 15.40 Qwen3-VL-8B [41] Qwen3-VL-235B-A22B [41] 2.55 97.73 86.93 85.57 79.89 63.64 58.54 22.55 8.00 5.27 3.64 2.00 2.36 45.28 30.91 27.92 19.27 OOM OOM OOM OOM OOM density; 96dpi resolution matches typical screen rendering and avoids unnecessary upsampling; minimal text spacing maximizes content per page, enhancing compression while preserving legibility. We also preset rendering Rconversation for VTC-Memory that colors one speakers utterances in green background and the other in white, inspired by messaging apps that use green backgrounds for clear speaker identification."
        },
        {
            "title": "4.1 Setup",
            "content": "Models and Inference Setup We select 13 VLMs, comprising 11 open-source models (Qwen2.5-VL-7B [1], Qwen2.5-VL-72B [1], Qwen3-VL-8B [41], Qwen3-VL-235B-A22B [41], Deepseek-OCR [51], Kimi-VL-A3BInstruct [40], Gemma3-27B [39], GLM-4.1V-9B-Thinking [19], InternVL3.5-8B [49], InternVL3.5-38B [49], and Glyph [9]) and two closed-source models (Gemini-2.5 Pro [11] and GPT-5 [35]). Collectively, these models cover the three main visual encoding categories and varying image-processing strategies discussed in Section B, covering diverse sizes (3B to 235B) and architectures (dense or MoE). We also test Qwen3-8B [57] as strong LLM baseline to show the performance gap between VLMs and LLMs. We evaluate all opensource models with vllm [27], using each models default generation configuration, running in BFloat16 on NVIDIA A100 GPUs. More experimental details can be found in Section C. 7 Task configurations We test all models on two settings (predefined VTC ratio and predefined rendering) with the VTC-Retrieval, VTC-Reasoning, and VTC-Memory tasks. For VTC-Retrieval and VTC-Reasoning, we evaluate the models on the corpus from the series (1k, 2k, 4k, 8k, 16k, 32k). The evaluation is conducted by inserting single needle at specific position within the context. This process is repeated 11 times, with the needle placed at depths of 0%, 10%, 20%, ..., up to 100% of the context length. We found that most VLMs perform poorly when the corpus size is 32k, which is why we do not consider longer texts."
        },
        {
            "title": "4.2 Main Results",
            "content": "We present the main results of our evaluation on VTCBench, analyzing the performance of various VLMs across the three core long-context understanding tasks. The results, detailed in Table 1, are discussed below. VTC-Retrieval The results for VTC-Retrieval demonstrate that many leading VLMs possess strong fundamental OCR and literal matching capabilities. In the predefined compression ratio setting (rVTC = 2), models like Qwen3-VL-235B (97.16% at 1k), Qwen3-VL-8B (95.23% at 1k), and Glyph (91.48% at 1k) achieve high accuracy on shorter contexts. However, noticeable performance gap remains when compared to the textonly Qwen3-8B baseline, which consistently scores above 95% even at 32k context length. Crucially, nearly all VLMs exhibit clear degradation in performance as the context length increases. For instance, Qwen3VL-235Bs accuracy drops from 97.16% at 1k to 81.34% at 32k, indicating that VTC introduces degree of perceptual fragility that impairs reliable information extraction from longer, denser visual contexts. It should be pointed out that DeepSeek-OCRs lower performance is attributable to its training procedures, which lack the supervised fine-tuning (SFT) stage common in instruction models (see Section 4.5). In contrast, some models like InternVL3.5 show surprisingly low performance, hinting at architectural mismatches with dense-text images (as analyzed in Section 4.5). These results confirm that while VLMs can perceive text within the VTC framework, their retrieval reliability is sensitive to context density and length. VTC-Reasoning The VTC-Reasoning task, which requires associative inference rather than simple lexical matching, reveals stark performance collapse across all evaluated VLMs. As shown in Table 1, even models that excelled at VTC-Retrieval struggle significantly here. While the Qwen3-8B LLM baseline also shows degradation, its performance (e.g., 94.18% at 1k) far surpasses that of most VLMs. For example, in the rVTC = 2 setting, Qwen2.5-VL-72B achieves respectable 52.79% at 1k, but this is an outlier; most other open-source models score below 40%. This finding highlights critical limitation: current VLMs can successfully perform surface-level text recognition under the VTC paradigm, but their ability to transform this visual perception into deeper semantic comprehension and establish latent connections is severely limited. Intriguingly, the newer Qwen3-VL series consistently underperforms the older Qwen2.5-VL models on this task. We hypothesize this is due to an over-reliance on literal matches, causing them to refuse to answer associative queries (see Section 4.5). Furthermore, comparing the two settings in Table 1 confirms that the performance is inversely correlated with the VTC ratio, and higher compression disproportionately harms complex reasoning. VTC-Memory In contrast to the synthetic nature of the previous tasks, VTC-Memory assesses performance on cohesive dialogues, offering more practical evaluation of long-context understanding. The results in Table 1 again reveal significant performance gap between most VLMs and the text-only baseline, particularly on multi-hop (MH), temporal (TP), and open-domain (OD) questions that require information aggregation. However, the Qwen2.5-VL series emerges as notable exception, achieving strong results that are competitive with, and in some cases even exceed, the LLM baseline (e.g., on the Single-Hop subtask, Qwen2.5-VL-72B scores 55.55% ROUGE-L versus the LLMs 52.55%). Proprietary models like GPT-5 also demonstrate competent performance. This suggests that while high information density generally impairs long-term memory capabilities, it is not an insurmountable challenge, and that certain architectures are already developing resilience. Nevertheless, for the majority of models, the sharp drop in performance on complex memory subtasks underscores the diﬀiculty of integrating and reasoning over interconnected facts within visually compressed context. 8 Table 2 Performance of Qwen2.5-VL-7B-Instruct with different rendering operators. Each follow-up experiment alters exactly one rendering parameter from baseline. (a) Performance on VTC-Retrieval and VTC-Reasoning. (b) Performance (ROUGE-L) on VTC-Memory. Retrieval Reasoning Rendering Config SingleHop Rendering Parameters S-NIAH 1k 1k Rplain-16 R(font-size = 10) R(font-size = 12) R(font-size = 14) R(font-size = 16) R(font-size = 18) R(font-size = 20) R(font = Arial) R(font = Courier New) R(font = Times New Roman) R(bg-color = red) R(bg-color = green) R(bg-color = blue) R(bg-color = gray) R(color = red) R(color = green) R(color = blue) R(color = gray) R(line-height = 1.25) R(line-height = 1.5) R(line-height = 2) 98.79 73.64 86.06 99.39 98.79 100.00 100. 99.09 98.79 99.09 98.79 99.70 96.67 99.39 99.09 99.70 99.09 99.09 98.79 100.00 99.70 36.73 20.00 27.64 31.27 36.73 39.09 46. 35.82 38.18 36.91 41.82 34.73 39.27 34.18 35.64 34.91 37.09 30.18 38.36 38.91 50.18 Rconversation R(font-size = 7) R(font-size = 8) R(font-size = 9) R(font-size = 10) R(font-size = 11) R(font-size = 12) R(font-size = 13) R(font-size = 14) R(font = Arial) R(font = Courier New) R(font = Times New Roman) R(bg-color = white) Rplain-12 R(bg-color = red) R(bg-color = green) R(bg-color = blue) R(bg-color = gray) R(color = red) R(color = green) R(color = blue) 50.54 21.81 29.29 33.81 40.93 45.54 50.54 49.94 53.06 49.79 53.15 47.05 49.81 49. 43.49 43.68 39.93 48.72 49.14 48.11 46."
        },
        {
            "title": "4.3 Analysis of Rendering Configuration",
            "content": "To comprehensively assess the impact of the rendering operator on VLMs capabilities beyond simple OCR, we conducted an ablation study on VTC-Retrieval, VTC-Reasoning, and VTC-Memory tasks. All experiments employed the Qwen2.5-VL-7B-Instruct model. We established baseline using Rplain-16 for VTC-Retrieval and VTC-Reasoning, and Rconversation for VTC-Memory, and then introduced perturbations to exactly one rendering attribute, selected from font size, font family, text/background color, and line height. The results in Table 2 demonstrate that font size is the primary determinant of model performance across all three tasks. This is likely attributable to increased character legibility, which facilitates more accurate OCR by the models vision encoder. However, this improvement comes at the cost of lower compression ratio, as discussed in Section A.1. In contrast, the model demonstrates impressive robustness to stylistic changes: variations in font family and text or background colors exert only minor effects on performance across all tasks, provided suﬀicient contrast is maintained. This indicates that the models internal representations are well-generalized and do not overfit to specific visual styles. These findings indicate that the model and potentially other VLMs are generally robust to stylistic rendering changes and that readability, rather than stylistic variation, is the critical determinant in VTC scenarios."
        },
        {
            "title": "4.4 Analysis of Needle Position",
            "content": "To further dissect the performance of VLMs under VTC paradigm, we analyze the impact of the needles position within the visually compressed context. This analysis is crucial for understanding whether models process the entire visual space uniformly or exhibit positional biases. Figure 4 visualizes the models accuracy on the VTC-Retrieval and VTC-Reasoning as function of both context length and the relative depth of the needle within the document (more results can be found in Figures S4 to S7). 9 (a) Performance on VTC-Retrieval MQ-NIAH. (b) Performance on VTC-Reasoning. Figure 4 Performance on VTC-Retrieval and VTC-Reasoning with respect to needle placement depth using Qwen2.5VL-72B-Instruct. The U-shaped performance curve reveals lost in the middle phenomenon: accuracy is highest at the contexts edges but plummets in the center, an effect that worsens as context length increases. The results reveal pronounced lost in the middle phenomenon, challenge well-documented in text-only long-context models [33] but now observed in the VTC spatial domain. Taking the results on VTC-Reasoning task as an example, the performance is consistently highest when the needle is located at the very beginning (0% depth) or end (100% depth) of the context, forming distinct U-shaped curve in accuracy. As shown in the heatmap, accuracy drops precipitously for needles placed in the central portion of the context. For instance, with 4k context length, accuracy is relatively high at the edges but plummets to as low as 12.1% in the middle. This positional bias is severely exacerbated as the context length increases. For 16k context, the model can still retrieve information from the edges with over 40% accuracy, but its performance on needles in the middle section collapses to near-zero (e.g., 3.4%), rendering the central part of the context almost entirely inaccessible for complex reasoning. This finding has critical implications for the VTC paradigm. It suggests that even when text is successfully rendered and theoretically perceptible, the spatial arrangement within the image creates strong attentional biases. Current VLM architectures appear to struggle with distributing attention evenly across dense, uniform grid of text. Instead, they prioritize information at the start and end of the visual sequence, analogous to how text-only models handle the beginning and end of 1D token sequence. Therefore, the ability to reason over long contexts via VTC is not only function of overall context length and information density but is also critically dependent on the spatial location of the relevant facts. Overcoming this positional fragility is key challenge for developing truly effective VTC-based models."
        },
        {
            "title": "4.5 Error Analysis",
            "content": "To gain deeper insights into the failure modes of VLMs on our benchmark, we performed qualitative error analysis on the model responses. Our analysis identified several recurring categories of errors, which span from fundamental retrieval issues to more complex reasoning and visual-to-textual grounding failures. These categories are not mutually exclusive; single incorrect response may exhibit multiple failure modes. Logical and Associative Reasoning Deficiencies In the VTC-Reasoning task, many errors were not due to failed retrieval but breakdown in the subsequent reasoning step. Models would often successfully extract the relevant facts (e.g., Katie is vegan) but fail to perform the required logical inference (e.g., concluding that Katie cannot eat fish). That is why most models perform worse than VTC-Retrieval on the VTC-Reasoning. Refusal to Conduct Associative Reasoning We observed distinct failure mode in the Qwen3-VL series models, where the models frequently refuse to answer the prompt, often outputting responses such as none of [category] or stating that the information is not present. This behavior appears to stem from failure to perform the necessary multi-step inference when the visual needle (evidence) and the textual question 10 Table 3 Refusal ratio (%) of Qwen3-VL-235B-A22B-Instruct (preset compression ratio setting in VTC-Reasoning). Model VTC-Reasoning (Refusal Ratio) 1k 2k 4k 8k 16k 32k Qwen3-VL-235B-A22B 66.18 62.00 62.91 60.55 64. 63.45 do not share literal match. In the VTC-Reasoning tasks, the correct answer requires linking the visual context to the query via associative logic rather than direct keyword retrieval. However, Qwen3-VL seems to over-rely on lexical correspondence; consequently, when the specific phrasing of the question does not explicitly appear in the compressed visual text, the model conservatively defaults to refusal, incorrectly assuming the requisite information is absent from the context. We also found this in the InternVL3.5 series model. Table 3 shows the refusal ratio of Qwen3-VL-235B-A22B-Instruct. We observed that the model refused to answer more than 60% of the questions, which severely degraded its performance on reasoning tasks. Despite possessing strong literal matching capabilities (VTC-Retrieval), the model underperformed relative to its predecessor, Qwen2.5-VL, under VTC-Reasoning. We guess this may stem from over-tuned safety alignments that incorrectly flag the associative query as unanswerable. Missing in the Haystack Another prevalent failure mode is what we term Missing in the Haystack. This error occurs when the model fails to pinpoint the exact needle, instead returning plausible but incorrect piece of information from the surrounding haystack. For instance, in the VTC-Retrieval task, when prompted for the special magic number for long-context (the needle key, with the correct answer being 2026), models would sometimes respond with 2025, distractor value deliberately placed in the context. This type of error highlights failure in fine-grained grounding and retrieval precision. It suggests that even when the models vision component successfully performs OCR, its attention mechanism can be easily swayed by semantically similar distractors, leading it to retrieve the wrong fact. This issue becomes more pronounced as the context lengthens and the density of distracting information increases, underscoring the challenge of precise information extraction from visually compressed text. Table 4 Performance of Qwen2.5-VL-7B-Instruct on VTC-Retrieval S-NIAH with respect to needle key-value types. Model Retrieval S-NIAH 1k (containsAll ) word-word word-number uuid-number Qwen2.5-VL-7B 91.82 80.91 85. Sensitivity to Needle Types Following [20], we examine how the models retrieval performance varies with different needle key and value types. In the VTCRetrieval SNIAH subtask, we isolate three subconditions that each pair specific keytype with specific valuetype. By keeping all other factors constant, we can attribute performance differences directly to the nature of the needle. The results  (Table 4)  reveal clear hierarchy: The results show systematic decline in retrieval accuracy as the semantic richness of the needle diminishes. This finding aligns with the observations reported by Hsieh et al. [20] on the brittleness of retrievaloriented LLMs to identifierlike queries. Inaccurate Information Aggregation To assess the capacity of VLMs to perform information aggregation, we inspect the aggregation subtasks. representative example is VTC-Retrievals MultiValue-NIAH, in which two needles share an identical key but are annotated with two distinct values, and are away from each other at least 20% of total context length. The model is required to synthesize these cues and output composite answer that reflects both of the two values. As shown in Figure 5, the performance gap between containsAny (finding at least one value) and containsAll (finding both) widens slightly with increasing context length. This result suggests that the primary bottleneck is not the aggregation logic itself, but rather the failure to retrieve all relevant pieces of information from the extensive visual context. The models are 11 (a) MV-NIAH: 1 key, 2 values. (b) MQ-NIAH: 2 keys, 1 value each. Figure 5 Metric containsAny and containsAll comparison on aggregation subtasks in VTC-Retrieval (context length from 1k to 32k) evaluated with Qwen2.5-VL-7B-Instruct. generally capable of synthesizing facts they can find, but their ability to find all facts diminishes over longer sequences. Sensitivity to Rendering Parameters OCR-specific VLMs are known to be sensitive to the parameters used when rendering text images, e.g., image resolution, typographic style, and background colour. In particular, Glyph reports its lack of generalization to various rendering styles. To assess this, we applied plain-text rendering style Rplain = Rconversation R(bg-color = white) for VTC-Memory and observed notable performance gap shown in Table 5, thereby corroborating the Glyphs claim that it is highly sensitive to rendering parameters. Table 5 Ablation with Glyph against different text background colorgreen in Rconversation, or white in Rplain. Glyph VTC-Memory (ROUGE-L ) Rendering SingleHop MultiHop Temporal OpenDomain Rconversation Rplain 4.10 30.92 2.43 6.49 1.69 7.48 2.16 13. Sensitivity to Prompt OCR-specific VLMs may also be sensitive to the evaluation prompt. DeepseekOCRs training pipeline does not include supervised-finetuning stage [51], resulting in lack of instructionfollowing abilities. We used completion prompt instead of question-answering, which showed substantial improvements across all subtasks in VTC-Retrieval  (Table 6)  . Table 6 Ablation with Deepseek-OCR against different promptcompletion (e.g., one of the magic numbers of long-context is ), or QA (e.g., What is one of the magic numbers of long-context?). VTC-Retrieval (containsAll ) S-NIAH MK-NIAH MQ-NIAH MV-NIAH Prompt 1k 2k 4k 8k 16k 1k 2k 4k 8k 16k 1k 2k 4k 8k 16k 1k 2k 4k 8k 16k Completion 51.82 50.91 50.00 16.36 1.82 40.00 50.91 51.82 12.73 0.91 21.82 18.18 19.09 6.82 0.00 20.91 24.09 17.73 14.55 1.36 1.36 0.00 30.91 32.73 25.45 0.00 0.00 23.64 26.36 17.27 0.00 0.00 7.73 11.36 6.82 0.00 0.00 10.45 16.82 9.09 QA Thumbnail Type Models specific failure mode arises from the architectural design of certain VLMs, such as InternVL3.5 [49] and GPT-5 [35]. This approach typically involves encoding both downscaled, low-resolution thumbnail of the entire image to capture global context, alongside high-resolution tiles for fine-grained detail. InternVL3.5 explicitly processes 448 448 thumbnail, which costs 256 tokens. For proprietary models like GPT-5 [35], the exact internal architecture is opaque. However, we hypothesize similar structure based on its token calculation formula1, which adds fixed 70 base tokens on top of the tokens for high-resolution tiles. This suggests the base tokens are used for thumbnail-like global overview. While this strategy is highly effective for natural images, where global composition and local details are distinct, it proves ineﬀicient and counter-productive in our VTC setting. The images in VTCBench consist of uniformly dense text. When such an image is downscaled to create thumbnail, the individual characters and words blur into complete illegibility. The resulting thumbnail offers the model no useful textual information, appearing as little more than noisy, gray texture that the model can hardly see clearly. Consequently, the tokens generated from this thumbnail are effectively wasted. For InternVL3.5, this means 256 tokens, fifth of the total 1280 tokens for an 896 896 image, are spent processing visually indecipherable global overview. Under our hypothesis for GPT-5, its 70 base tokens are similarly allocated to this low-information signal. This token waste represents fundamental mismatch between the models vision architecture and the specific nature of VTC data. The model expends significant portion of its visual processing budget on component that cannot contribute to the core task of text recognition, which likely contributes to the pronounced performance degradation observed in models like the InternVL3.5 series  (Table 1)  ."
        },
        {
            "title": "5 VTCBench-Wild",
            "content": "Our ablation studies in Table 2 and the error analysis in Section 4.5 have demonstrated that the performance of VLMs is sensitive to various rendering parameters, such as font size, color, and line height. These findings indicate that evaluating models using single, fixed rendering configuration may not fully capture their robustness to the visual diversity encountered in real-world documents. To bridge this gap and establish more challenging evaluation standard, we introduce VTCBench-Wild, new benchmark variant designed to simulate visually diverse in-the-wild scenarios. We construct VTCBench-Wild by sampling scenes from rendering pool and questions from question pool. Rendering Pool The core idea of VTCBench-Wild is to assess VLM performance under conditions of inconsistent and varied visual styling. To achieve this, we first constructed diverse pool of 99 distinct rendering hyperparameter configurations. This pool was created by systematically varying key CSS properties from the Rplain baseline, including: Font Size: Ranging from 10 pixels to 20 pixels. Font Family: Including Times New Roman, Helvetica, and Courier New fonts. Line Height: Varying among values 1.0, 1.2, and 1.5. For each test instance in VTCBench-Wild, each sample is rendered using configuration randomly sampled from this pool of 99 styles (1133). Question Pool To create diverse set of evaluation questions that test different aspects of long-context understanding, the question pool is constructed by systematically generating questions for VTC-Retrieval and VTC-Reasoning, and by sampling from the existing dataset for VTC-Memory. For VTC-Retrieval and VTC-Reasoning, which are based on synthetic needle-in-a-haystack tests, we systematically vary two key parameters: haystack length and the depth of the needle (i.e., the target information). Haystack Length: We generate test cases for six distinct context lengths: 1k, 2k, 4k, 8k, 16k, and 32k tokens. Needle Depth: To evaluate how models performance is affected by the location of crucial information, we generate separate test instances by inserting single needle at various depths. For each context length, this evaluation is repeated for every specified depth, and the results are averaged. This process is conducted at 10% Granularity, i.e., creating 11 separate test cases, placing the needle at depths of 0%, 10%, 20%, ..., up to 100%. 1https://platform.openai.com/docs/guides/images-vision Table 7 Experimental results (%) on VTCBench-Wild. Model Retrieval Reasoning Memory Overall Qwen3-8B (LLM baseline) [57] GPT-5 [35] Gemini-2.5-Pro [11] Qwen3-VL-8B [41] Qwen3-VL-235B-A22B [41] InternVL3.5-8B [49] Qwen2.5-VL-7B [1] Qwen2.5-VL-72B [1] Gemma3-27B [39] Kimi-VL-A3B-Instruct [40] Glyph [9] GLM-4.1V-9B-Thinking [19] 99. 72.63 87.38 89.00 97.50 51.38 91.63 91.50 49.38 82.50 93.63 76.75 62.25 62.88 58.75 11.50 14.63 7.63 15.63 37.50 3.75 8.75 36.75 43.13 54.17 43.83 38.67 33.67 48.50 17.33 33.83 38.33 17.33 33.33 36.17 38.17 73. 61.23 63.68 45.73 54.00 26.18 48.23 57.36 24.05 42.27 57.27 54.00 For VTC-Memory, which utilizes predefined dataset of natural conversations, our goal is to preserve the original data distribution. Instead of synthetically controlling variables, we use 1,540 question-answer pairs from the full VTC-Memory dataset. Finally, our question pool contains 28,600 unique questions, comprising 7,920 retrieval questions, 19,140 reasoning questions, and 1,540 memory questions. By combining the question pool with the rendering pool, we obtain 2,831,400 unique combinations. From these combinations, we sample 600 items for memory, 800 items for reasoning, and 800 items for retrieval to form the final VTCBench-Wild. Evaluation Metrics For the retrieval and reasoning questions, we use containsAll to judge if the ground truth answer is in the prediction. For the memory questions, we use gpt-4o-mini [22] to judge the correctness of the answer. Experimental Results We evaluated diverse set of VLMs and an LLM baseline on VTCBench-Wild, with the comprehensive results presented in Table 7. key finding is that the Qwen3-8B LLM baseline achieves the highest overall score (73.55%), driven by near-perfect retrieval performance (99.38%). This suggests that for legible, albeit visually varied text, strong language understanding can be more effective than current VLM pipelines, which may introduce noise or errors during visual encoding. Proprietary models lead the field, with Gemini-2.5-Pro (63.68%) and GPT-5 (61.23%) achieving the highest overall scores among all tested VLMs and outperforming their open-source counterparts. Among the two, Gemini-2.5-Pro exhibits stronger retrieval capabilities (87.38%) than GPT-5 (72.63%), though both fall short of the LLM baseline. Conversely, GPT-5 shows slight edge in reasoning (62.88% vs. 58.75%). Both models, however, struggle significantly on the memory task, indicating that comprehending long, visually inconsistent conversational histories remains major challenge. The performance of open-source models reveals distinct architectural strengths and weaknesses. The Qwen3VL series demonstrates exceptional retrieval scores (e.g., 97.50% for the 235B model), yet their reasoning performance is exceptionally poor (11.50% for 8B and 14.63% for 235B). This stark gap corroborates our error analysis in Section 4.5, highlighting tendency to refuse associative reasoning. In contrast, its predecessor, Qwen2.5-VL-72B-Instruct, and Glyph show more balanced capabilities, achieving both high retrieval scores and substantially better reasoning performance. Notably, GLM-4.1V-9B-Thinking stands out by achieving the highest reasoning score among all VLMs, demonstrating superior robustness in multi-hop inference under diverse visual conditions. Conversely, models like InternVL3.5-8B and Gemma3-27B struggle across all tasks, with particularly low scores in reasoning and memory, underscoring their sensitivity to the visual diversity presented in VTCBench-Wild."
        },
        {
            "title": "6 Conclusion",
            "content": "In this work, we conducted the first systematic investigation into the long-context understanding capabilities of VLMs under the vision-text compression (VTC) paradigm. We introduced comprehensive benchmarks VTCBench and VTCBench-Wild, which evaluate models on three complex tasks: information retrieval, associative reasoning, and long-term memory. Comprehensive experiments on range of leading VLMs yielded valuable findings. While current models exhibit respectable ability to perform simple text retrieval from compressed images, their performance is fragile, degrading with increased context length and compression ratios. More critically, we observed near-total collapse in their ability to perform associative reasoning and maintain long-term memory, revealing significant gap between visual perception and deep semantic comprehension. Furthermore, our analysis underscores that rendering parameters, particularly font size, are not trivial implementation detail but critical factor directly influencing model performance. High compression ratios achieved through smaller fonts severely impair even basic retrieval, confirming that VTC introduces unique perceptual challenges absent in text-only models. Our evaluation demonstrates that VTC is not simple, drop-in solution for the long-context problem. The eﬀiciency gains of the VTC paradigm come at the cost of sacrificing advanced cognitive capabilities and visual robustness. Moving forward, the development of truly effective VTC-based models will require more than just scaling existing VLM architectures. It necessitates targeted research into novel pre-training objectives and architectural designs that explicitly bridge the gap between spatial perception and abstract, long-range semantic reasoning. By establishing rigorous evaluation framework and highlighting these core limitations, we hope VTCBench will guide and accelerate the creation of next-generation VLMs that are both eﬀicient and genuinely capable of understanding long contexts."
        },
        {
            "title": "A VTCBench Description",
            "content": "VTCBench is the first systematic benchmark to evaluate the long-context understanding of vision-language models (VLMs) using vision-text compression (VTC), paradigm that renders long text into compact images to achieve substantial token compression. The benchmark assesses models on three tasks: information retrieval (VTC-Retrieval), associative reasoning (VTC-Reasoning), and long-term dialogue comprehension (VTC-Memory). We will introduce how we curate VTCBench and show some examples. A.1 Rendering Pipeline context align-left text (cid:498) markdownit code HTML (cid:247) playwright file-pdf pdf file-clipboard pymupdf image page-to-image images image(s) camera-alt screenshot (exactly 1 image) Figure S1 Rendering pipeline of VTCBench. (cid:498) markdownit interprets context as HTML. (cid:247) playwright is browser-based rendering engine prints HTML to PDF, where styles are injected as CSS. file-clipboard pymupdf converts each page in PDF as an image. Figure S2 Distribution of context length (in number of text tokens) versus number of images for VTC-Memory with rendering operator Rconversation. strong correlation of approximately 2,000 tokens per image can be inferred. Figure S1 depicts the rendering pipeline, real-time browserbased text-to-image engine, which stacks markdown-it2, playwright3, and pymupdf4 for end-to-end conversion and applies text styling by injecting CSS. For example, the rendering preset Rplain contains only the basic rule: \"p{font-size:12px;}\". Crucially, significantly influences the compression ratio rVTC: VTC-Reasoning with Rplain yields an average of 2,000 text tokens per image, whereas employing Rplain-16(\"p{font-size:16px;}\") results in only 1,100 tokens per image, nearly halving rVTC. As for VTC-Memory, which has predefined contexts, we observed text-token versus number-of-image distribution in Figure S2, using rendering preset Rconversation. A.2 Examples of VTCBench We permute configurations including task type, context length, evaluation template, and needle type (Table S1) and dynamically generate examples; for each configuration, we sample from its pool (including needle keys, needle values, and distractors) to fill placeholders in the haystack to form the context. The rendering pipeline then converts the context to images (Figure S3). We concatenate the visual context and prompt (usually an instruction followed by question) as input for VLMs. Prompt templates are provided in Section D. 2https://pypi.org/project/markdown-it-py 3https://pypi.org/project/playwright 4https://pypi.org/project/pymupdf 16 Table S1 Illustrations of VTC-Retrieval, VTC-Reasoning, and VTC-Memory examples. We highlight queries, keys, values, and distractors accordingly. For VTC-Retrieval and VTC-Reasoning, queries, keys, values, and distractors are randomly generated; for VTC-Memory, queries, keys, values, and distractors are provided as-is. Task Task Categories Context Example Evaluation Example VTC-Retrieval (NIAH) Lexical Matching, Multi-Hop Tracing, Aggregation, Question-Anwering VTC-Reasoning (NIAH) Associative Reasoning, Question-Answering VTC-Memory (QA) Memory, Question-Answering (Dynamic query/key-value with types: word-word, word-number, uuid-number.) (essays) . . . One of the special magic numbers for long-context is: 2026. . . . One of the special magic numbers for distracting-information is: 2025. . . . QA Variant. Q: Whats the special magic number for long-context? A: 2026. Completion Variant. Prompt: one of the special magic numbers for long-context is: Completion: 2026. (Dynamic query/key-value with types: person-event/action.) (books) . . . There was vegan guest, named Katie. . . . (No dynamic query/key-value, fully static.) (conversations) . . . Caroline: Researching adoption agencies its been dream to have family and give loving home to kids who need it. . . . Caroline: And heres one of the adoption agencies Im looking into. . . . One-Hop Reasoning. Q: Which character cannot eat fish-based meals? A: Katie. Two-Hop Reasoning. Q: Which character cannot eat Brandade meals? A: Katie. Q: What did Caroline research? A: Adoption agencies. (a) VTC-Retrieval example with context length of 2k. (b) VTC-Reasoning example, with context length of 2k. (c) VTC-Memory example (1st of 15), about 30,000 text tokens in total. Figure S3 Examples of Rendered Context for VTC-Retrieval, VTC-Reasoning, VTC-Memory using Rplain, Rplain, and Rconversation, respectively."
        },
        {
            "title": "B Calculation of Compression Ratio",
            "content": "Recall that the VTC ratio rVTC = NT is the ratio between context of NT text tokens and its image NI counterpart requiring NI image tokens. This ratio is affected by both the rendering operator : text 7 image and VLMs vision processing pipeline, so we provide some calculation examples of rVTC for VLMs given = Rplain. For simplicity, the following formulas ignore model-specific resizings and paddings, assuming an input image of size (H, ) pixels. According to different image-processing strategies, we can roughly categorize open-source multimodal LLMs into three groups: (1) Models with dynamic resolution like the Qwen-VL series [1, 41, 46], Kimi-VL [40], and GLM-4V [9, 19]; (2) Models using global thumbnail, represented by the InternVL family [49]; (3) Other architectures with different visual pipelines like DeepseekOCR [51] and Gemma3 [39] family. B.1 Dynamic Resolution Models Qwen2.5-VL Series [1] Qwen2.5-VL is multimodal vision-language model series developed by the Qwen Team at Alibaba Group that understands and processes both images, videos, and text. It uses NaViT [13] structure, and the visual token can be obtained by HW 14144 . Specifically, an 896 896 image is 1024 tokens. Qwen3-VL Series [41] Qwen3-VL is the latest and most powerful series of open-source VLMs developed by the Qwen team. It is available in various versions, including dense and MoE (Mixture of Experts) architectures and Instruct and Thinking editions. Qwen3-VL uses Siglip2 [42] vision encoder with dynamic resolution, and the number of visual tokens injected into the LLM is HW 16164 . Specifically, an 896 896 image is 784 tokens. GLM-4.1V [19] & Glyph [9] GLM-4.1V and Glyph use similar structure as Qwen2.5-VL, and the calculation formula is HW 14144 . Kimi-VL-A3B-Instruct [40] Kimi-VL-A3B-Instruct, designed by MoonshotAI, is an eﬀicient open-source VLM that uses MoE architecture to handle multimodal tasks like image, video, and text understanding. Kimi-VL employs MoonViT as its native-resolution vision encoder; MoonViT is initialized from and continually pre-trained on SigLIP-SO-400M [60]. Its image patch size is 14 14; it performs 2 2 spatial downsampling while correspondingly expanding the channel dimension. Consequently, the number of visual tokens is HW 14144 . For an 896 896 image input, we have 1024 visual tokens. B.2 Thumbnail-based Models InternVL3.5 uses InternViT [8] to split an image into 448448 tiles and 448448 InternVL3.5 Series [49] thumbnail to capture the global context. Each tile refers to 256 tokens. For an 896 896 image input, we have 5 256 = 1280 tokens. GPT-5 [35] GPT-5 is VLM developed by OpenAI, representing the fifth iteration in its series of generative pre-trained transformer (GPT) foundation models. We use GPT-5 with detail=high for all our experiments, and the pipeline5 is: 1. Scale to fit in 2048 2048px square. No effect here because all our images satisfy this threshold. 2. Scale so that the images shortest side is 768px long. Images are resized to 768 768px. 3. Count the number of 512px squares (tiles) in the image, each costing 140 tokens. 4 tiles cost 560 tokens. 4. Add the base tokens to the total. GPT-5 has 70, yielding final 560 + 70 = 630 tokens per image. 5https://platform.openai.com/docs/guides/images-vision B.3 Others Gemini-2.5-Pro [11] Gemini-2.5-Pro is Googles advanced reasoning model, excelling at complex problemsolving, coding, and analyzing large datasets, documents, and multimedia content. Gemini-2.5-Pro6 tiles images into 768 768 pixel tiles, each costing 258 tokens. This means an 896 896 image would cost 258 4 = 1032 tokens. Deepseek-OCR [51] DeepSeek-OCR is an open-source AI system that uses an innovative Contexts Optical Compression approach to read and compress documents, making it more eﬀicient for LLMs. We use the Gundam model for Deepseek-OCR, and the visual token is 4 100 + 256 = 656. Gemma3 Series [39] Gemma3 vision processor7 uses SigLip [60], which resizes images to 896 896 pixels and applies 4 4 average pooling to output 256 tokens per image. This high-factor pooling strategy ensures substantial compression ratio, condensing the high-resolution visual data into compact embeddings. Table S2 Inference parameters of VLMs and LLM baseline. Model Parameters Deepseek-OCR [51] temperature=0, ngram_size=30, window_size=90, whitelist_token_ids={128821, 128822}, skip_special_tokens=False Gemma3 Series [39] top_k=64,top_p=0.95 Gemini-2.5-Pro [11] reasoning_effort=minimal GLM-4.1V-9B-Thinking [19] Glyph [9] GPT-5 [35] top_k=2, top_p=0.6, temperature=0.8 temperature=1. detail=high, reasoning_effort=minimal, verbosity=low InternVL3.5 Series [49] None Kimi-VL-A3B-Instruct [40] temperature=0.2 Qwen2.5-VL Series [1] Qwen3-VL Series [41] Qwen3-8B [57] (LLM baseline) repetition_penalty=1.05, temperature=0.000001 top_k=20, top_p=0.8, repetition_penalty=1.0, temperature=0.7 top_k=20, top_p=0.95, temperature=0."
        },
        {
            "title": "C Experimental Details",
            "content": "Table S2 presents the inference parameters employed in our experiments. We follow the default generation configuration provided in HuggingFace and use the minimal thinking configuration possible. 6https://ai.google.dev/gemini-api/docs/image-understanding 7https://developers.googleblog.com/gemma-explained-whats-new-in-gemma-"
        },
        {
            "title": "D Prompts",
            "content": "VTC-Retrieval {haystack} {question} VTC-Reasoning {haystack} Answer question based on the above book snippet. Your answer should be short and based on either explicitly stated facts or strong, logical inferences. Return only the final answer with no additional explanation or reasoning. {question} VTC-Memory {haystack} Based on the above context, write an answer in the form of short phrase for the following question. Answer with exact words from the context whenever possible. {question} VTC-Wild-Retrieval {haystack} Answer question based on the above book snippet. Your answer should be short and based on either explicitly stated facts or strong, logical inferences. Return only the final answer with no additional explanation or reasoning. Question: {question} VTC-Wild-Reasoning {haystack} Answer question based on the above book snippet. Some special magic numbers are hidden within the following text. Make sure to memorize it. will quiz you about the numbers afterwards. Question: {question} VTC-Wild-Memory {haystack} Based on the above context, write an answer in the form of short phrase for the following question. Answer with exact words from the context whenever possible. Question: {question} LLM Judge Please as grading expert, judge whether the final answers given by the candidates below are consistent with the standard answers, that is, whether the candidates answered correctly. Here are some evaluation criteria: 1. Please refer to the given standard answer. You dont need to re-generate the answer to the question because the standard answer has been given. You only need to judge whether the candidates answer is consistent with the standard answer according to the form of the question. THE STANDARD ANSWER IS ALWAYS CORRECT AND THE QUESTION IS PERFECTLY VALID. NEVER QUESTION THEM. 2. ONLY compare the FINAL ANSWER - COMPLETELY IGNORE any potential errors in the REASONING PROCESSES. 3. Some answers may be expressed in different ways, such as some answers may be mathematical expression, some answers may be textual description, as long as the meaning expressed is the same. Before making judgment, please understand the question and the standard answer first, and then judge whether the candidates answer is correct. If the standard answer does not specify unit, but the candidates answer includes unit that is correct for the value given, consider it correct. 4. Some answers may consist of multiple items, such as multiple-choice questions, multiple-select questions, fill-in-the-blank questions, etc. Regardless of the question type, the final answer will be considered correct as long as it matches the standard answer, regardless of whether the reasoning process is correct. For multiple-select questions and multi-blank fill-in-the-blank questions, all corresponding options or blanks must be answered correctly and match the standard answer exactly to be deemed correct. 5. If the prediction is given with boxed{{}}, please ignore the boxed{{}} and only judge whether the candidates answer is consistent with the standard answer. 6. If the candidates answer is invalid (e.g., incomplete (cut off mid-response), lots of unnormal repetitive content, or irrelevant to the question, saying it cant answer the question because some irresistible factors, like ethical issues, no enough information, etc.), select option (INVALID). Please judge whether the following answers are consistent with the standard answer based on the above criteria. Grade the predicted answer of this new question as one of: A: CORRECT B: INCORRECT C: INVALID Just return the letters A, B, or C, with no text around it. Here is your task. Simply reply with either CORRECT, INCORRECT, or INVALID. Dont apologize or correct yourself if there was mistake; we are just trying to grade the answer.n <Original Question Begin>: {question}n <Original Question End>n <Standard Answer Begin>: {gold_answer}n <Standard Answer End>n <Candidates Answer Begin>: {prediction}n <Candidates Answer End>n Judging the correctness of the candidates answer:"
        },
        {
            "title": "E Limitation",
            "content": "While this work provides the first systematic investigation into the long-context capabilities of VLMs under the VTC paradigm, we acknowledge several limitations, which offer clear directions for future research. Language Our benchmark is currently limited to the English language and primarily uses general-domain prose and conversations as the context haystack. The effectiveness of VTC, including achievable compression ratios and perceptual robustness, may vary for other languages, especially those with complex, non-alphabetic scripts (e.g., Chinese, Arabic) or different typographic conventions. Similarly, the performance on highly specialized or structured content, such as source code, legal documents, or scientific papers, remains unassessed and is vital area for future expansion. Scope of evaluation While comprehensive in its inclusion of leading open-source and proprietary models, VTCBench is necessarily snapshot in rapidly evolving field. The performance characteristics identified may not be fully generalizable to all VLM architectures, particularly those that may emerge with novel visual processing or attention mechanisms. Future work should involve the continuous integration and evaluation of new models to maintain the benchmarks relevance. Constraints of API Our evaluation of proprietary models is inherently constrained by the nature of their API-based access. Unlike open-source models, where we have full control over the inference pipeline, closedsource models operate as black boxes. Their APIs may perform opaque pre-processing steps, such as image resizing, compression, or normalization, before the data reaches the model. This lack of transparency means that the carefully rendered images we provide may be altered in ways that are beyond our control, potentially confounding our analysis of rendering parameters and their impact on performance. Besides, some APIs (e.g., Gemini-2.5-Pro) are constrained by the maximum number of input images, which will affect the performance of the model. Despite these limitations, we believe VTCBench establishes crucial foundation and rigorous evaluation framework for advancing the development of more eﬀicient and capable long-context VLMs. Table S3 VTC-Retrieval S-NIAH Results."
        },
        {
            "title": "Model",
            "content": "Qwen3-8B InternVL3.5-8B InternVL3.5-38B Gemin-2.5-Pro Gemma3-27B GLM-4.1V-9B-Thinking Glyph GPT-5 InternVL3.5-8B InternVL3.5-38B Kimi-VL-A3B-Instruct Qwen2.5-VL-7B-Instruct Qwen2.5-VL-72B-Instruct Qwen3-VL-8B-Instruct Qwen3-VL-235B-A22B-Instruct Qwen3-VL-8B-Instruct Qwen3-VL-235B-A22B-Instruct Deepseek-OCR (Completion) Deepseek-OCR (QA) GPT-5 Gemma3-27B rVTC NA 1.56 1.56 1.98 2.00 2.00 2.00 2.00 2.00 2.00 2.00 2.00 2.00 2.00 2.00 2.55 2.55 3.12 3.12 3.17 8.00 1k 2k 4k 8k 16k 32k 98. 100.00 96.36 97.73 95.45 94.55 32.73 22.73 100.00 89.09 26.36 92.73 76.36 19.09 17.27 98.18 84.55 92.73 99.09 97.27 97.27 98.18 51.82 30.91 50.00 20. 23.64 20.00 50.00 84.55 16.36 83.64 76.36 22.73 16.36 67.27 77.27 94.55 89.09 82.73 72.73 89.09 50.91 32.73 48.18 19.09 36.36 16.36 55.45 85.45 11.82 90.00 71.82 10.91 12.73 59.09 82.73 95.45 93.64 92.73 75.45 89.09 50.00 25.45 32.73 20.91 21.82 22.73 54.55 87.27 14.55 83.64 61.82 13.64 13.64 55.45 84.55 97.27 90.91 88.18 74.55 84.55 16.36 0.00 36.36 19.09 20.91 15.45 40.91 OOM 3.64 94.55 62.73 12.73 9.09 61.82 78.18 91.82 90.00 89.09 56.36 59.09 1.82 0.00 33.64 20.00 12.73 24.55 41.82 OOM 2.73 85.45 60.00 10.00 11.82 66.36 84.55 98.18 89.09 74.55 59.09 74.55 OOM OOM 26.36 19.09 Table S4 VTC-Retrieval MK-NIAH Results. Model Qwen3-8B InternVL3.5-8B InternVL3.5-38B Gemini-2.5-Pro Gemma3-27B GLM-4.1V-9B-Thinking Glyph GPT-5 InternVL3.5-8B InternVL3.5-38B Kimi-VL-A3B-Instruct Qwen2.5-VL-7B-Instruct Qwen2.5-VL-72B-Instruct Qwen3-VL-8B-Instruct Qwen3-VL-235B-A22B-Instruct Qwen3-VL-8B-Instruct Qwen3-VL-235B-A22B-Instruct Deepseek-OCR (Completion) Deepseek-OCR (QA) GPT-5 Gemma3-27B rVTC NA 1.56 1.56 1.98 2.00 2.00 2.00 2.00 2.00 2.00 2.00 2.00 2.00 2.00 2.00 2.55 2.55 3.12 3.12 3.17 8. 1k 2k 4k 8k 16k 32k 98.18 97.27 90.91 92.73 91.82 95. 29.09 32.73 53.64 91.82 13.64 81.82 71.82 19.09 16.36 68.18 80.00 80.00 84.55 92.73 78.18 84.55 50.91 26.36 42.73 26.36 22.73 29.09 49.09 88.18 6.36 83.64 68.18 7.27 12.73 48.18 81.82 87.27 85.45 99.09 69.09 89.09 51.82 17.27 34.55 32.73 19.09 24.55 49.09 81.82 10.91 81.82 61.36 11.82 15.45 44.55 79.09 90.91 90.00 96.36 69.09 83.64 12.73 0.00 32.73 22.73 10.91 20.00 30.91 OOM 8.18 82.73 59.09 6.36 14.55 47.27 88.18 86.36 90.91 92.12 55.45 62.73 0.91 0.00 28.18 10.91 10.00 20.00 35.45 OOM 3.64 78.18 49.09 8.18 12.73 44.55 76.36 80.91 85.86 85.18 66.36 52.73 OOM OOM 21.82 15.45 28.18 26.36 100.00 88.18 10.00 92.73 85.45 12.73 27.27 84.55 93.64 89.09 93.64 100.00 90.91 100.00 40.00 23.64 44.55 28. 22 Table S5 VTC-Retrieval MV-NIAH Results."
        },
        {
            "title": "Model",
            "content": "Qwen3-8B InternVL3.5-8B InternVL3.5-38B Gemini-2.5-Pro Gemma3-27B GLM-4.1V-9B-Thinking Glyph GPT-5 InternVL3.5-8B InternVL3.5-38B Kimi-VL-A3B-Instruct Qwen2.5-VL-7B-Instruct Qwen2.5-VL-72B-Instruct Qwen3-VL-8B-Instruct Qwen3-VL-235B-A22B-Instruct Qwen3-VL-8B-Instruct Qwen3-VL-235B-A22B-Instruct Deepseek-OCR (Completion) Deepseek-OCR (QA) GPT-5 Gemma3-27B rVTC NA 1.56 1.56 1.98 2.00 2.00 2.00 2.00 2.00 2.00 2.00 2.00 2.00 2.00 2.00 2.55 2.55 3.12 3.12 3.17 8.00 1k 2k 4k 8k 16k 32k 99. 100.00 100.00 100.00 99.09 96.82 11.82 17.27 100.00 90.91 17.73 96.36 84.09 12.27 20.45 77.73 80.45 83.64 93.18 96.36 93.64 96.82 20.91 10.45 41.36 23. 15.45 15.91 50.00 83.64 9.09 85.45 75.00 11.36 14.09 41.82 69.55 80.45 90.00 86.36 62.73 81.82 24.09 16.82 32.27 14.55 16.82 19.55 55.45 83.64 5.00 86.36 72.27 10.45 11.82 40.00 70.45 83.64 88.18 92.27 55.91 74.55 17.73 9.09 32.73 20.45 21.82 20.45 54.55 67.27 4.09 76.82 64.55 8.18 14.55 33.64 76.36 73.64 85.45 93.18 46.36 70.91 14.55 1.36 34.55 17.27 15.45 18.64 40.00 OOM 0.91 76.82 62.73 8.18 7.73 32.27 69.55 75.45 82.27 86.82 38.64 56.82 1.36 0.00 30.45 13.18 15.45 18.64 41.82 OOM 2.27 75.45 61.82 8.64 8.18 28.64 62.73 59.55 69.55 76.68 38.64 44.55 OOM OOM 29.09 9.55 Table S6 VTC-Retrieval MQ-NIAH Results. Model Qwen3-8B rVTC 1k 2k 4k 8k 16k 32k NA 100.00 98.18 100. 98.18 100.00 95.45 InternVL3.5-8B InternVL3.5-38B Gemini-2.5-Pro Gemma3-27B GLM-4.1V-9B-Thinking Glyph GPT-5 InternVL3.5-8B InternVL3.5-38B Kimi-VL-A3B-Instruct Qwen2.5-VL-7B-Instruct Qwen2.5-VL-72B-Instruct Qwen3-VL-8B-Instruct Qwen3-VL-235B-A22B-Instruct Qwen3-VL-8B-Instruct Qwen3-VL-235B-A22B-Instruct Deepseek-OCR (Completion) Deepseek-OCR (QA) GPT-5 Gemma3-27B 1.56 1.56 1.98 2.00 2.00 2.00 2.00 2.00 2.00 2.00 2.00 2.00 2.00 2.00 2.55 2.55 3.12 3.12 3.17 8.00 21.82 20.00 100.00 87.27 14.55 84.09 81.82 15.91 15.45 87.73 82.27 90.45 95.00 95.00 95.00 95.91 21.82 7.73 46.36 16. 23 19.09 16.36 60.91 79.55 6.82 74.55 75.00 13.18 11.82 53.18 75.45 83.18 90.00 92.27 75.91 92.27 18.18 11.36 31.36 11.82 18.64 22.27 58.18 71.82 7.73 79.55 72.27 10.91 10.00 48.64 64.09 84.55 82.27 95.00 65.45 89.55 19.09 6.82 28.18 15.91 20.45 17.27 60.00 61.36 5.00 71.82 63.18 13.18 8.18 43.18 67.27 84.55 89.55 90.91 55.91 80.45 6.82 0.00 28.18 15.00 16.82 16.82 42.73 OOM 2.73 67.73 58.18 6.82 7.73 35.91 70.45 75.45 84.55 96.82 47.27 75.91 0.00 0.00 24.09 13.64 11.82 12.73 43.18 OOM 1.36 63.64 57.73 5.00 6.82 32.27 53.18 67.27 73.86 88.96 49.09 62.34 OOM OOM 24.09 11. (a) Gemini-2.5-Pro (b) Gemma3-27B (c) GLM-4.1V-9B-Thinking (d) Glyph (e) GPT-5 (f) InternVL3.5-8B (g) InternVL3.5-38B (h) Kimi-VL-A3B-Instruct (i) Qwen2.5-VL-7B-Instruct (j) Qwen2.5-VL-72B-Instruct (k) Qwen3-VL-8B-Instruct (l) Qwen3-VL-235B-A22B-Instruct Figure S4 VTC-Retrieval S-NIAH (preset = Rplain) accuracy w.r.t. needle placement depth. Blank cells indicate OOM. 24 (a) Gemini-2.5-Pro (b) Gemma3-27B (c) GLM-4.1V-9B-Thinking (d) Glyph (e) GPT-5 (f) InternVL3.5-8B (g) InternVL3.5-38B (h) Kimi-VL-A3B-Instruct (i) Qwen2.5-VL-7B-Instruct (j) Qwen2.5-VL-72B-Instruct (k) Qwen3-VL-8B-Instruct (l) Qwen3-VL-235B-A22B-Instruct Figure S5 VTC-Retrieval MK-NIAH (preset = Rplain) accuracy w.r.t. needle placement depth. Blank cells indicate OOM. 25 (a) Gemini-2.5-Pro (b) Gemma3-27B (c) GLM-4.1V-9B-Thinking (d) Glyph (e) GPT-5 (f) InternVL3.5-8B (g) InternVL3.5-38B (h) Kimi-VL-A3B-Instruct (i) Qwen2.5-VL-7B-Instruct (j) Qwen2.5-VL-72B-Instruct (k) Qwen3-VL-8B-Instruct (l) Qwen3-VL-235B-A22B-Instruct Figure S6 VTC-Retrieval MV-NIAH (preset = Rplain) accuracy w.r.t. needle placement depth. Blank cells indicate OOM. (a) Gemini-2.5-Pro (b) Gemma3-27B (c) GLM-4.1V-9B-Thinking (d) Glyph (e) GPT-5 (f) InternVL3.5-8B (g) InternVL3.5-38B (h) Kimi-VL-A3B-Instruct (i) Qwen2.5-VL-7B-Instruct (j) Qwen2.5-VL-72B-Instruct (k) Qwen3-VL-8B-Instruct (l) Qwen3-VL-235B-A22B-Instruct Figure S7 VTC-Retrieval MQ-NIAH (preset = Rplain) accuracy w.r.t. needle placement depth. Blank cells indicate OOM. 27 (a) Qwen3-8B (LLM baseline) (b) Gemini-2.5-Pro (c) Gemma3-27B (d) GLM-4.1V-9B-Thinking (e) Glyph (f) GPT-5 (g) InternVL3.5-8B (h) InternVL3.5-38B (i) Kimi-VL-A3B-Instruct (j) Qwen2.5-VL-7B-Instruct (k) Qwen2.5-VL-72B-Instruct Figure S8 VTC-Reasoning (preset = Rplain) accuracy w.r.t. needle placement depth. Blank cells indicate OOM."
        },
        {
            "title": "References",
            "content": "[1] Shuai Bai, Keqin Chen, Xuejing Liu, Jialin Wang, Wenbin Ge, Sibo Song, Kai Dang, Peng Wang, Shijie Wang, Jun Tang, et al. Qwen2. 5-vl technical report. arXiv preprint arXiv:2502.13923, 2025. [2] Yushi Bai, Xin Lv, Jiajie Zhang, Yuze He, Ji Qi, Lei Hou, Jie Tang, Yuxiao Dong, and Juanzi Li. Longalign: recipe for long context alignment of large language models. arXiv preprint arXiv:2401.18058, 2024. [3] Yushi Bai, Xin Lv, Jiajie Zhang, Hongchang Lyu, Jiankai Tang, Zhidian Huang, Zhengxiao Du, Xiao Liu, Aohan Zeng, Lei Hou, et al. Longbench: bilingual, multitask benchmark for long context understanding. In Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 31193137, 2024. [4] Iz Beltagy, Matthew Peters, and Arman Cohan. Longformer: The long-document transformer. arXiv preprint arXiv:2004.05150, 2020. [5] Aili Chen, Aonian Li, Bangwei Gong, Binyang Jiang, Bo Fei, Bo Yang, Boji Shan, Changqing Yu, Chao Wang, Cheng Zhu, et al. Minimax-m1: Scaling test-time compute eﬀiciently with lightning attention. arXiv preprint arXiv:2506.13585, 2025. [6] Jianlv Chen, Shitao Xiao, Peitian Zhang, Kun Luo, Defu Lian, and Zheng Liu. Bge m3-embedding: Multilingual, multi-functionality, multi-granularity text embeddings through self-knowledge distillation. arXiv preprint arXiv:2402.03216, 2024. [7] Yukang Chen, Shengju Qian, Haotian Tang, Xin Lai, Zhijian Liu, Song Han, and Jiaya Jia. Longlora: Eﬀicient fine-tuning of long-context large language models. arXiv preprint arXiv:2309.12307, 2023. [8] Zhe Chen, Jiannan Wu, Wenhai Wang, Weijie Su, Guo Chen, Sen Xing, Muyan Zhong, Qinglong Zhang, Xizhou Zhu, Lewei Lu, et al. Internvl: Scaling up vision foundation models and aligning for generic visual-linguistic tasks. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 24185 24198, 2024. [9] Jiale Cheng, Yusen Liu, Xinyu Zhang, Yulin Fei, Wenyi Hong, Ruiliang Lyu, Weihan Wang, Zhe Su, Xiaotao Gu, Xiao Liu, et al. Glyph: Scaling context windows via visual-text compression. arXiv preprint arXiv:2510.17800, 2025. [10] Prateek Chhikara, Dev Khant, Saket Aryan, Taranjeet Singh, and Deshraj Yadav. Mem0: Building productionready ai agents with scalable long-term memory. arXiv preprint arXiv:2504.19413, 2025. [11] Gheorghe Comanici, Eric Bieber, Mike Schaekermann, Ice Pasupat, Noveen Sachdeva, Inderjit Dhillon, Marcel Blistein, Ori Ram, Dan Zhang, Evan Rosen, et al. Gemini 2.5: Pushing the frontier with advanced reasoning, multimodality, long context, and next generation agentic capabilities. arXiv preprint arXiv:2507.06261, 2025. [12] Zihang Dai, Zhilin Yang, Yiming Yang, Jaime Carbonell, Quoc Le, and Ruslan Salakhutdinov. Transformerxl: Attentive language models beyond fixed-length context. In Proceedings of the 57th annual meeting of the association for computational linguistics, pages 29782988, 2019. [13] Mostafa Dehghani, Basil Mustafa, Josip Djolonga, Jonathan Heek, Matthias Minderer, Mathilde Caron, Andreas Steiner, Joan Puigcerver, Robert Geirhos, Ibrahim Alabdulmohsin, et al. Patch npack: Navit, vision transformer for any aspect ratio and resolution. Advances in Neural Information Processing Systems, 36:2252 2274, 2023. [14] Yiran Ding, Li Lyna Zhang, Chengruidong Zhang, Yuanyuan Xu, Ning Shang, Jiahang Xu, Fan Yang, and Mao Yang. Longrope: Extending llm context window beyond 2 million tokens. arXiv preprint arXiv:2402.13753, 2024. [15] Tianyu Fu, Haofeng Huang, Xuefei Ning, Genghan Zhang, Boju Chen, Tianqi Wu, Hongyi Wang, Zixiao Huang, Shiyao Li, Shengen Yan, et al. Moa: Mixture of sparse attention for automatic large language model compression. arXiv preprint arXiv:2406.14909, 2024. [16] Tao Ge, Hu Jing, Lei Wang, Xun Wang, Si-Qing Chen, and Furu Wei. In-context autoencoder for context compression in large language model. In The Twelfth International Conference on Learning Representations, 2023. [17] Michael Günther, Isabelle Mohr, Daniel James Williams, Bo Wang, and Han Xiao. Late chunking: contextual chunk embeddings using long-context embedding models. arXiv preprint arXiv:2409.04701, 2024. 29 [18] Kelly Hong, Anton Troynikov, and Jeff Huber. Context rot: How increasing input tokens impacts llm performance. Technical report, Chroma, 2025. [19] Wenyi Hong, Wenmeng Yu, Xiaotao Gu, Guo Wang, Guobing Gan, Haomiao Tang, Jiale Cheng, Ji Qi, Junhui Ji, Lihang Pan, et al. Glm-4.1 v-thinking: Towards versatile multimodal reasoning with scalable reinforcement learning. arXiv e-prints, pages arXiv2507, 2025. [20] Cheng-Ping Hsieh, Simeng Sun, Samuel Kriman, Shantanu Acharya, Dima Rekesh, Fei Jia, Yang Zhang, and arXiv preprint Boris Ginsburg. Ruler: Whats the real context size of your long-context language models? arXiv:2404.06654, 2024. [21] Zhiyuan Hu, Yuliang Liu, Jinman Zhao, Suyuchen Wang, WangYan WangYan, Wei Shen, Qing Gu, Luu Anh Tuan, See Kiong Ng, Zhiwei Jiang, et al. Longrecipe: Recipe for eﬀicient long context generalization in large language models. In Proceedings of the 63rd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 1185711870, 2025. [22] Aaron Hurst, Adam Lerer, Adam Goucher, Adam Perelman, Aditya Ramesh, Aidan Clark, AJ Ostrow, Akila Welihinda, Alan Hayes, Alec Radford, et al. Gpt-4o system card. arXiv preprint arXiv:2410.21276, 2024. [23] DeLesley Hutchins, Imanol Schlag, Yuhuai Wu, Ethan Dyer, and Behnam Neyshabur. Block-recurrent transformers. Advances in neural information processing systems, 35:3324833261, 2022. [24] Greg Kamradt. Needle in haystack-pressure testing llms. GitHub repository, page 28, 2023. [25] Gregory Kamradt. Needle in haystack - pressure testing llms. https://github.com/gkamradt/LLMTest_ NeedleInAHaystack, 2023. [26] Yuri Kuratov, Aydar Bulatov, Petr Anokhin, Dmitry Sorokin, Artyom Sorokin, and Mikhail Burtsev. In search of needles in 11m haystack: Recurrent memory finds what llms miss. arXiv preprint arXiv:2402.10790, 2024. [27] Woosuk Kwon, Zhuohan Li, Siyuan Zhuang, Ying Sheng, Lianmin Zheng, Cody Hao Yu, Joseph E. Gonzalez, Hao Zhang, and Ion Stoica. Eﬀicient memory management for large language model serving with pagedattention. In Proceedings of the ACM SIGOPS 29th Symposium on Operating Systems Principles, 2023. [28] Yanhong Li, Zixuan Lan, and Jiawei Zhou. Text or pixels? it takes half: On the token eﬀiciency of visual text inputs in multimodal llms. arXiv preprint arXiv:2510.18279, 2025. [29] Zhiyu Li, Shichao Song, Hanyu Wang, Simin Niu, Ding Chen, Jiawei Yang, Chenyang Xi, Huayi Lai, Jihao Zhao, Yezhaohui Wang, et al. Memos: An operating system for memory-augmented generation (mag) in large language models. arXiv preprint arXiv:2505.22101, 2025. [30] Fanfan Liu and Haibo Qiu. Context cascade compression: Exploring the upper limits of text compression. arXiv preprint arXiv:2511.15244, 2025. [31] Jiaheng Liu, Dawei Zhu, Zhiqi Bai, Yancheng He, Huanxuan Liao, Haoran Que, Zekun Wang, Chenchen Zhang, Ge Zhang, Jiebin Zhang, et al. comprehensive survey on long context language modeling. arXiv preprint arXiv:2503.17407, 2025. [32] Adyasha Maharana, Dong-Ho Lee, Sergey Tulyakov, Mohit Bansal, Francesco Barbieri, and Yuwei Fang. Evaluating very long-term conversational memory of llm agents. arXiv preprint arXiv:2402.17753, 2024. [33] Ali Modarressi, Hanieh Deilamsalehy, Franck Dernoncourt, Trung Bui, Ryan Rossi, Seunghyun Yoon, and Hinrich Schütze. Nolima: Long-context evaluation beyond literal matching. arXiv preprint arXiv:2502.05167, 2025. [34] Tsendsuren Munkhdalai, Manaal Faruqui, and Siddharth Gopal. Leave no context behind: Eﬀicient infinite context transformers with infini-attention. arXiv preprint arXiv:2404.07143, 101, 2024. [35] OpenAI. Introducing gpt-5 openai, 2025. [36] Linke Ouyang, Yuan Qu, Hongbin Zhou, Jiawei Zhu, Rui Zhang, Qunshu Lin, Bin Wang, Zhiyuan Zhao, Man Jiang, Xiaomeng Zhao, et al. Omnidocbench: Benchmarking diverse pdf document parsing with comprehensive annotations. In Proceedings of the Computer Vision and Pattern Recognition Conference, pages 2483824848, 2025. [37] Ofir Press, Noah Smith, and Mike Lewis. Train short, test long: Attention with linear biases enables input length extrapolation. arXiv preprint arXiv:2108.12409, 2021. 30 [38] Jianlin Su, Murtadha Ahmed, Yu Lu, Shengfeng Pan, Wen Bo, and Yunfeng Liu. Roformer: Enhanced transformer with rotary position embedding. Neurocomputing, 568:127063, 2024. [39] Gemma Team, Aishwarya Kamath, Johan Ferret, Shreya Pathak, Nino Vieillard, Ramona Merhej, Sarah Perrin, Tatiana Matejovicova, Alexandre Ramé, Morgane Rivière, et al. Gemma 3 technical report. arXiv preprint arXiv:2503.19786, 2025. [40] Kimi Team, Angang Du, Bohong Yin, Bowei Xing, Bowen Qu, Bowen Wang, Cheng Chen, Chenlin Zhang, Chenzhuang Du, Chu Wei, et al. Kimi-vl technical report. arXiv preprint arXiv:2504.07491, 2025. [41] Qwen3-VL Team. Qwen3-vl: Sharper vision, deeper thought, broader action, 2025. [42] Michael Tschannen, Alexey Gritsenko, Xiao Wang, Muhammad Ferjad Naeem, Ibrahim Alabdulmohsin, Nikhil Parthasarathy, Talfan Evans, Lucas Beyer, Ye Xia, Basil Mustafa, et al. Siglip 2: Multilingual vision-language encoders with improved semantic understanding, localization, and dense features. arXiv preprint arXiv:2502.14786, 2025. [43] Kiran Vodrahalli, Santiago Ontanon, Nilesh Tripuraneni, Kelvin Xu, Sanil Jain, Rakesh Shivanna, Jeffrey Hui, Nishanth Dikkala, Mehran Kazemi, Bahare Fatemi, et al. Michelangelo: Long context evaluations beyond haystacks via latent structure queries. arXiv preprint arXiv:2409.12640, 2024. [44] Alex Jinpeng Wang, Linjie Li, Yiqi Lin, Min Li, Lijuan Wang, and Mike Zheng Shou. Leveraging visual tokens for extended text contexts in multi-modal learning. Advances in Neural Information Processing Systems, 37: 1432514348, 2024. [45] Hengyi Wang, Haizhou Shi, Shiwei Tan, Weiyi Qin, Wenyuan Wang, Tunyu Zhang, Akshay Nambi, Tanuja Ganu, and Hao Wang. Multimodal needle in haystack: Benchmarking long-context capability of multimodal large language models. In Proceedings of the 2025 Conference of the Nations of the Americas Chapter of the Association for Computational Linguistics: Human Language Technologies (Volume 1: Long Papers), pages 32213241, 2025. [46] Peng Wang, Shuai Bai, Sinan Tan, Shijie Wang, Zhihao Fan, Jinze Bai, Keqin Chen, Xuejing Liu, Jialin Wang, Wenbin Ge, et al. Qwen2-vl: Enhancing vision-language models perception of the world at any resolution. arXiv preprint arXiv:2409.12191, 2024. [47] Weizhi Wang, Li Dong, Hao Cheng, Xiaodong Liu, Xifeng Yan, Jianfeng Gao, and Furu Wei. Augmenting language models with long-term memory. Advances in Neural Information Processing Systems, 36:7453074543, 2023. [48] Weiyun Wang, Shuibo Zhang, Yiming Ren, Yuchen Duan, Tiantong Li, Shuo Liu, Mengkang Hu, Zhe Chen, Kaipeng Zhang, Lewei Lu, et al. Needle in multimodal haystack. Advances in Neural Information Processing Systems, 37:2054020565, 2024. [49] Weiyun Wang, Zhangwei Gao, Lixin Gu, Hengjun Pu, Long Cui, Xingguang Wei, Zhaoyang Liu, Linglin Jing, Shenglong Ye, Jie Shao, et al. Internvl3. 5: Advancing open-source multimodal models in versatility, reasoning, and eﬀiciency. arXiv preprint arXiv:2508.18265, 2025. [50] Yu Wang, Yifan Gao, Xiusi Chen, Haoming Jiang, Shiyang Li, Jingfeng Yang, Qingyu Yin, Zheng Li, Xian Li, Bing Yin, et al. Memoryllm: Towards self-updatable large language models. arXiv preprint arXiv:2402.04624, 2024. [51] Haoran Wei, Yaofeng Sun, and Yukun Li. Deepseek-ocr: Contexts optical compression. arXiv preprint arXiv:2510.18234, 2025. [52] Chuhan Wu, Fangzhao Wu, Tao Qi, and Yongfeng Huang. Hi-transformer: Hierarchical interactive transformer for eﬀicient and effective long document modeling. arXiv preprint arXiv:2106.01040, 2021. [53] Di Wu, Hongwei Wang, Wenhao Yu, Yuwei Zhang, Kai-Wei Chang, and Dong Yu. Longmemeval: Benchmarking chat assistants on long-term interactive memory. arXiv preprint arXiv:2410.10813, 2024. [54] Guangxuan Xiao, Jiaming Tang, Jingwei Zuo, Junxian Guo, Shang Yang, Haotian Tang, Yao Fu, and Song Han. Duoattention: Eﬀicient long-context llm inference with retrieval and streaming heads. arXiv preprint arXiv:2410.10819, 2024. [55] Ling Xing, Alex Jinpeng Wang, Rui Yan, Xiangbo Shu, and Jinhui Tang. Vision-centric token compression in large language model. Advances in Neural Information Processing Systems, 2025. 31 [56] Wenhan Xiong, Jingyu Liu, Igor Molybog, Hejia Zhang, Prajjwal Bhargava, Rui Hou, Louis Martin, Rashi Rungta, Karthik Abinav Sankararaman, Barlas Oguz, et al. Effective long-context scaling of foundation models. In Proceedings of the 2024 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (Volume 1: Long Papers), pages 46434663, 2024. [57] An Yang, Anfeng Li, Baosong Yang, Beichen Zhang, Binyuan Hui, Bo Zheng, Bowen Yu, Chang Gao, Chengen Huang, Chenxu Lv, et al. Qwen3 technical report. arXiv preprint arXiv:2505.09388, 2025. [58] Songlin Yang, Bailin Wang, Yikang Shen, Rameswar Panda, and Yoon Kim. Gated linear attention transformers with hardware-eﬀicient training. In Forty-first International Conference on Machine Learning, 2024. [59] Chanwoong Yoon, Taewhoo Lee, Hyeon Hwang, Minbyul Jeong, and Jaewoo Kang. Compact: Compressing retrieved documents actively for question answering. arXiv preprint arXiv:2407.09014, 2024. [60] Xiaohua Zhai, Basil Mustafa, Alexander Kolesnikov, and Lucas Beyer. Sigmoid loss for language image preIn Proceedings of the IEEE/CVF international conference on computer vision, pages 1197511986, training. 2023. [61] Qianchi Zhang, Hainan Zhang, Liang Pang, Hongwei Zheng, and Zhiming Zheng. Adacomp: Extractive context compression with adaptive predictor for retrieval-augmented large language models. arXiv preprint arXiv:2409.01579, 2024. [62] Wanjun Zhong, Lianghong Guo, Qiqi Gao, He Ye, and Yanlin Wang. Memorybank: Enhancing large language models with long-term memory. In Proceedings of the AAAI Conference on Artificial Intelligence, pages 19724 19731, 2024."
        }
    ],
    "affiliations": [
        "Centre for Artificial Intelligence and Robotics, Hong Kong Institute of Science & Innovation, CAS",
        "Institute of Automation, Chinese Academy of Sciences",
        "School of Artificial Intelligence, University of Chinese Academy of Sciences",
        "Tencent Hunyuan Team"
    ]
}