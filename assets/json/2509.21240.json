{
    "paper_title": "Tree Search for LLM Agent Reinforcement Learning",
    "authors": [
        "Yuxiang Ji",
        "Ziyu Ma",
        "Yong Wang",
        "Guanhua Chen",
        "Xiangxiang Chu",
        "Liaoni Wu"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Recent advances in reinforcement learning (RL) have significantly enhanced the agentic capabilities of large language models (LLMs). In long-term and multi-turn agent tasks, existing approaches driven solely by outcome rewards often suffer from the problem of sparse supervision. To address the challenge, we propose Tree-based Group Relative Policy Optimization (Tree-GRPO), a grouped agent RL method based on tree search, where each tree node represents the complete agent interaction step. By sharing common prefixes, the tree search sampling increases the number of rollouts achievable within a fixed budget of tokens or tool calls. Moreover, we find that the tree-structured trajectory naturally allows the construction of step-wise process supervised signals even using only the outcome reward. Based on this, Tree-GRPO estimates the grouped relative advantages both on intra-tree and inter-tree levels. Through theoretical analysis, we demonstrate that the objective of intra-tree level group relative policy optimization is equivalent to that of step-level direct preference learning. Experiments across 11 datasets and 3 types of QA tasks demonstrate the superiority of the proposed tree-based RL over the chain-based RL method."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 5 2 ] . [ 1 0 4 2 1 2 . 9 0 5 2 : r Preprint."
        },
        {
            "title": "TREE SEARCH FOR LLM AGENT REINFORCEMENT\nLEARNING",
            "content": "Yuxiang Ji1,2, Ziyu Ma2, Yong Wang2, Guanhua Chen3, Xiangxiang Chu2, Liaoni Wu1 1Xiamen University 2AMAP, Alibaba Group 3Southern University of Science and Technology (cid:135) GitHub: https://github.com/AMAP-ML/Tree-GRPO"
        },
        {
            "title": "ABSTRACT",
            "content": "Recent advances in reinforcement learning (RL) have significantly enhanced the agentic capabilities of large language models (LLMs). In long-term and multi-turn agent tasks, existing approaches driven solely by outcome rewards often suffer from the problem of sparse supervision. To address the challenge, we propose Tree-based Group Relative Policy Optimization (Tree-GRPO), grouped agent RL method based on tree search, where each tree node represents the complete agent interaction step. By sharing common prefixes, the tree search sampling increases the number of rollouts achievable within fixed budget of tokens or tool calls. Moreover, we find that the tree-structured trajectory naturally allows the construction of step-wise process supervised signals even using only the outcome reward. Based on this, Tree-GRPO estimates the grouped relative advantages both on intra-tree and inter-tree levels. Through theoretical analysis, we demonstrate that the objective of intra-tree level group relative policy optimization is equivalent to that of step-level direct preference learning. Experiments across 11 datasets and 3 types of QA tasks demonstrate the superiority of the proposed tree-based RL over the chain-based RL method. Figure 1: Comparison of chain-based and tree-based sampling strategies in LLM multi-turn agent RL. The tree structure brings two major advantages: (i) less rollout budget (both on tokens and toolcalls); (ii) higher performance."
        },
        {
            "title": "INTRODUCTION",
            "content": "Reinforcement Learning (RL) has emerged as pivotal post-training paradigm for Large Language Models (LLMs), catalyzing the development of several frontier models (DeepSeek-AI Team, 2025; Yang et al., 2025a; OpenAI, 2024). RL-tuned LLMs trained only with outcome rewards acquire complex reasoning abilities and achieve remarkable gains in single-turn tasks, such as mathematical proof and code generation (Kimi Team, 2025b; Yu et al., 2025; Chu et al., 2025a; Shao et al., 2024; Xin et al., 2024). This suggests that LLMs can learn not only through static imitation, but also by actively interacting with dynamic environments. Guided by this prospect, recent works have extended this RL paradigm to more complex agent settings involving dynamic, multi-turn interactions (Feng et al., 2025b; Singh et al., 2025; Wang et al., 2025a; Qian et al., 2025; Feng et al., Work done during internship at AMAP, Alibaba Group. Project Lead. 1 Preprint. Figure 2: Comparison between chain-based and tree-based rollout at different levels. Left: Chainbased rollout. Mid: Tree search with nodes corresponding to tokens/sentence. Right (Ours): Tree search with nodes corresponding to complete agent step. 2025a). It is believed that such agentic intelligence through long-horizon interaction in open-ended environments is essential for next-generation foundation models (Kimi Team, 2025a). The agentic RL manifests in two key challenges: i) Heavy budget taken in LLM rollouts. Agent settings require LLMs to interact with the environment over multi-turns and complete tasks through sequential decision-making, which consequently leads to agent trajectories with thousands or more tokens alongside multiple tool-calls (Dong et al., 2025; Feng et al., 2025b). Existing group-based RL methods sample multiple independent trajectories for each task in chain-based rollout scheme, with considerable redundancy in the sampling process. This is reflected in practical agentic RL by the fact that the rollout phase dominates overall training time and tool-calls can incur substantial costs (e.g., high-priced search APIs). ii) Sparse supervision in long-horizon, multi-turn trajectories. Although agent trajectories grow with the number of turns, current agent RL approaches are still primarily driven by outcome rewards. Such trajectory-level sparse signals make it difficult to identify which specific steps or actions in multi-turn, interdependent sequence contributed to success or failure. This means that even with substantial increase in rollout budget, the sampled agent trajectories are still supervised by the same limited amount of training signal, resulting in highly imbalanced learning process and even the training collapse (Wang et al., 2025b;a; Jin et al., 2025b). These two challenges raise question: Can we construct more fine-grained supervision signals for agent RL under limited rollout budget while still solely based on outcome rewards? In this paper, we propose Tree-based Group Relative Policy Optimization (Tree-GRPO) with an online rollout strategy based on tree search. Unlike current RL approaches that independently sample complete trajectory rollouts (Figure 2 left), we replace the chain-based sampling logic with tree-search process, which yields interleaved trajectories with shared prefix segments. Existing treebased RL methods (Hou et al., 2025; Li et al., 2025b) often use token/sentence-level units as tree nodes (Figure 2 mid). For the agent tasks that have clear step structure, it is natural to treat complete Thought-Action-Observation step as the tree node unit (Figure 2 right). This design with clear contextual segmentation proves more suitable for agent RL (Appendix B.3) and explicitly constrains the rollout budget in both tokens and tool-calls. Under the same budget, our tree-search method can obtain around 1.5 samples (depending on the tree structure) compared to the chain-based method, which is highly significant for multi-turn agentic RL training where rollout costs are substantial. Furthermore, to address the challenge of sparse supervision, we construct more fine-grained process supervision signals by estimating relative advantages based on the tree structure. Specifically, at every branching point of the tree, we back-propagate outcome rewards from the respective subtree leaves. The differences across sibling branches serve as preference-learning objective, providing process-level supervision signals between subtrees, where the subtree depth determines the granularity of the process signal. Since our tree search strategy uses random expansion, it inherently yields process signals of varying granularity, enabling the model to learn intermediate decision making. This meticulous design leverages the tree structure to transform trajectory-level signals into process-level supervision. Its reliance solely on outcome rewards without additional supervision highlights its scalability and plug-and-play nature. In our experiments, we evaluate Tree-GRPO on 11 datasets across single-hop and multi-hop knowledge-intensive tasks, along with hard web-agent tasks. Compared to chain-based methods, our proposed tree-based method demonstrates consistent improvements across models of varying series and scales. It is noteworthy that Tree-GRPO can successfully enable base model to adopt pre-defined multi-turn agent interaction paradigm without any supervised fine-tuning (SFT), despite operating under an extremely limited rollout budget (tokens/tool calls). Based on Qwen2.5-3b, our Tree-GRPO achieves superior performance over the chain-based method while using only quarter of the rollout budget. Our contributions are summarized as follows: 2 Preprint. We introduce tree-based rollout strategy with nodes anchored at the agent step level in place of independent chain-based approaches for multi-turn agentic RL. We propose group-relative advantage estimation in both intra-tree and inter-tree level, incorporating an implicit step-level preference learning objective with relatively stable baseline estimate. We provide theoretical and empirical evidence that Tree-GRPO outperforms chain-based methods in agentic RL, attaining higher performance under less rollout budgets."
        },
        {
            "title": "2.1 MULTI-TURN AGENT FRAMEWORK",
            "content": "We adopt the widely used ReAct (Yao et al., 2023b) as the agent framework. Unlike static singleturn interaction, the agent engages in multi-turn Thought-Action-Observation cycles with the environment to solve given task. Specifically, at each step = 0, 1, ..., 1, the LLM generates thought τt and parsable textual action αt based on the existing context st. The action typically corresponds to tool use, through which the agent dynamically interacts with the environment to obtain new observations ot. complete -step agent episode consists of three interleaved trajectories: = {(τ0, α0, o0), (τ1, α1, o1), ..., (τT 1, αT 1, oT 1)}. (1) Such trajectories grow linearly with the number of steps, and for complex tasks requiring multiple interactions, the full trajectory can reach tens of thousands of tokens. Following analysis in related work (Wang et al., 2025a), such dynamic process can be described as Markov Decision Process = {S, A, }, where represents states (the complete interaction context up to given time step H<t), denotes the compound action space (each action comprising thought-action pair (τt, αt)), and denotes the transition dynamics (includes both the external environment Penv and the concatenation of the full context over time steps). The complete process can be formulated based on LLM policy model πθ as: pθ(s0:T , τ0:T , α0:T , o0:T ) = p(s0) 1 (cid:89) (cid:21) (cid:20) . πθ(τtst)πθ(αtst, τt)Penv(ot+1αt) (2) 2.2 AGENTIC REINFORCEMENT LEARNING t=0 After formalizing the ReAct-based process as Markov Decision Process, RL can be directly applied to optimize over the policy space by maximizing the expected return of the full state-action trajectory (Wang et al., 2025a; Dong et al., 2025; Zhang et al., 2025b): J(θ) = EHpθ [R (H)] . (3) In practice, optimization is performed with variance-reduced advantage estimator ˆA(H), which stabilizes gradient updates (Schulman et al., 2018; DeepSeek-AI Team, 2025; Zhang et al., 2025a). Most existing agentic RL systems adopt an outcome-based reward, where single scalar reward R() determined by predefined rules or model-based scoring functions is delivered to the entire trajectory. Our method is built upon the group-based RL algorithm (DeepSeek-AI Team, 2025). Unlike estimating advantages based on extra value functions like PPO (Schulman et al., 2018), the group-based RL methods estimate advantages ˆA by sampling group of candidate rollouts to estimate an ingroup baseline to guide the optimization direction."
        },
        {
            "title": "3 TREE-BASED GROUP RELATIVE POLICY OPTIMIZATION (TREE-GRPO)",
            "content": "To achieve more effective allocation of the rollout budget and address the sparse supversion challenges in multi-turn agentic RL, we propose to employ tree-search based sampling. By sharing partial prefixes between rollouts, tree search method could obtain more rollouts under the same token/tool-call budget. Based on the tree structure, we can further derive step-level process signals purely from outcome rewards, introducing an implicit step-level preference learning target into online RL. Figure 3 presents an overview of our proposed Tree-GRPO. In the following section, we will detail the implementation of agent tree search ( 3.1), the construction of tree-structured group relative advantages ( 3.2), and the analysis of step-level process signals ( 3.3). 3 Preprint. Figure 3: The overview of the Tree-GRPO training pipeline. The rollout is conducted in tree-search manner, where each node corresponds to complete thought-action-observation step. The group relative advantages are estimated at both intra-tree and inter-tree levels. Tree-GRPO constructs steplevel process supervision signals through tree structure with less rollout budget. 3.1 TREE SEARCH FOR AGENT ROLLOUT Tree-search methods such as Monte Carlo Tree Search (MCTS) have proven effective for constructing offline DPO datasets (Xie et al., 2024; Lai et al., 2024) and for test-time scaling (Yao et al., 2023a; Xin et al., 2025), yet they are seldom used in online RL training. The main obstacle is its reliance on multi-turn sequential rollouts, which are poorly suited for parallelized LLM inference engines. This mismatch bottlenecks rollout throughput and severely limits online RL efficiency. To address this, we adopt an initialize-then-expand approach, in which multiple chains are first initialized in parallel and nodes are then iteratively sampled for expansion. Furthermore, the clear structured agent step-level process allows us to replace the token-level nodes with agent step-level nodes, meaning that each node in the tree represents corresponding (τ, α, o) tuple. More formally, the overall tree-search sampling process is as follows: 1. Initialization. For each prompt xi, we first generate independent chain-based trajectories = {Hi πθ(xi)}M by policy model πθ as the initialization for trees . 2. Sampling. Then we randomly sample nodes Pi = {pi,j Ti}N except the leaf node (agent answer) from each tree Ti for expansion. <t = {proot 3. Expansion. For each selected node pi,j, we take the entire context from the root to that node , pi,j} and the original prompt xi as the input, continue generating the <t)}N , then insert it into the Hi remaining part of the response by Ynew = {Hi source tree as new branch by Ti Ti Ynew. πθ(xi, Hi i,j , ..., pfather i,j By iteratively repeating steps 2 and 3 times, this tree search process results in total of (L + 1) rollouts as final group size for single prompt. These rollouts are evenly distributed across the trees. Let the expectation rollout budget (both in tokens and tool-calls) of single agent trajectory be B. For each single random tree expansion, the expected depth of the selected node is half of the maximum depth, and the corresponding expected cost is 2 . This means we can obtain larger number of agent rollouts for training using tree search under the same token/tool-call budget. Specifically, the total expected budget for tree-search sampling is determined by: E[Btree] = + B/2. Under fixed sampling budget, decreasing tree number while increasing expansion number N, can raise the number of rollouts, but it also narrows the exploration scope, as more trajectories share the same prefix. In our experiments, different tree configurations exhibit varying effects. (4) 3.2 TREE-BASED GROUP RELATIVE ADVANTAGES Beyond enabling more rollouts under the fixed budget, more significant potential advantage of tree search lies in the process supervision signals naturally embedded within the tree structure. Given group of complete trajectory rollouts {Hi}G based on each prompt, naive way to apply group-based policy optimization for agent RL is to organize the rollouts into trajectory-level groups. 4 Preprint. For each rollout, the reward R() is only computed at outcome, and thus the advantage estimation is also at the trajectory level. This means that the whole multi-turn agent trajectory including multisteps is assigned an identical credit as: A(H) = A({(τ0, α0, o0), ..., (τT , αT , oT )}) = A({τ0, α0, o0}) = ... = A({τT , αT , oT }). (5) Due to the coarse credit assignment, such sparse rewards severely affect the stability of RL training for long-horizon multi-turn agents. Tree-based credit. Unlike independent chain-based rollouts, tree-structured rollouts with shared prefixes naturally embed process credit signals. As shown in Figure 4, at every branching point of the tree, the difference between the back-propagated outcome rewards from respective leaves naturally constitutes preference-learning objective for the different subtrees. Such form of preference learning results in process signals of varying granularity modulated by subtree depth. To achieve this form of tree-based credit assignment, we perform grouped advantage estimation within each tree Gintra-tree(Ti), serving as: Figure 4: Comparison between chainbased and tree-based rollouts. ˆAIntra/Inter-tree(Hi) = (cid:2)R(Hi) mean({R(Hj)}GIntra/Inter-tree(Ti) )(cid:3)/std({R(Hj)}GIntra/Inter-tree(Ti) ). (6) Although the intra-tree group relative advantage incorporates explicit preference objectives, the limited number of rollouts within each tree may lead to unreliable baseline estimation. To better stabilize the RL training, we also group rollouts across inter-trees (rollouts from all trees) and combine the intra-tree and inter-tree group relative advantages to obtain the final advantage estimate as: ˆAtree(Hi) = ˆAIntra-tree(Hi) + ˆAInter-tree(Hi). (7) The final tree-based group relative policy optimization object is: JTree-GRPO(θ) = xD,Htree-search πold(x) (cid:34) 1 (cid:88) i=1 1 Hi Hi (cid:88) t=1 (cid:16) min ri,t(θ) ˆAtree(Hi), (cid:17) clip(ri,t(θ), 1 ϵ, 1 + ϵ) ˆAtree(Hi) βDKL (cid:0)πθ(Hx) πref(Hx)(cid:1) (8) (cid:35) where πref and πold represent the reference LLM and previous LLM, and DKL denotes the KL divergence. The importance sampling ratio ri,t(θ) is defined at the token level t. The complete Tree-GRPO workflow is Algorithm 1. 3.3 IMPLICIT STEP-LEVEL PREFERENCE LEARNING Leaving aside cost and scalability, potential way to address the granularity of sparse supervision is to explicitly construct step-level DPO data with positive and negative pairs, thereby enabling preference optimization at each step. In this section, to better understand Tree-GRPO in agentic RL, we establish that intra-tree GRPO admits the same gradient structure as step-level DPO, with the only difference at the weight term. Assumption 3.1 (Binary Preference Setting) For each intermediate tree node (x, H<t), the subsequent trajectory in terms of reward falls into two categories, denoted as Hwin , with associated rewards {1,0}. The trajectory probabilities are defined as and Hloss pθ(Hwin ) = 1 pθ(Hloss ) = (cid:89) τ =t πθ(Hwin τ x, H<τ ). (9) Preprint. Under this assumption, the step-level DPO objective can be expressed as optimizing the BradleyTerry likelihood between the winning and losing outcomes by: θJstep-DPO(θ) = (x,H<t,Hwin t,Hloss t)D (cid:20) σ (cid:0)β log pθ (cid:0)Hloss (cid:1) β log pθ (cid:0)Hwin (cid:1)(cid:1) (cid:0)θ log pθ (cid:0)Hwin (cid:1) θ log pθ (cid:0)Hloss (cid:1)(cid:1) (10) (cid:21) . Correspondingly, the gradient of intra-tree GRPO can be derived into combined form consisting of Hwin and Hloss as: θJIntra-tree(θ) = pθ(Hwin ) pθ(Hloss ) [θ log pθ (cid:0)Hwin (cid:1) θ log pθ (cid:0)Hloss (cid:1)] (11) Proposition 3.1 (Structural Equivalence of step-level DPO and Intra-tree GRPO) Under Assumption C.1, both step-level DPO and intra-tree GRPO admit gradient estimators of the form θJunified(θ) = (cid:124)(cid:123)(cid:122)(cid:125) Weight (cid:0)θ log pθ (cid:124) (cid:0)Hwin (cid:1) θ log pθ (cid:123)(cid:122) Preference Advantage Gradient (cid:0)Hloss (cid:1)(cid:1) , (cid:125) (12) where the only difference lies in the choice of the weight term w. Proposition C.1 indicates that intra-tree GRPO can be interpreted as implicitly performing step-level preference optimization, thereby inheriting the key property of step-level DPO while operating in an online rollout setting. Detailed derivations are put in Appendix C."
        },
        {
            "title": "4 EXPERIMENT",
            "content": "4.1 EXPERIMENTAL SETUP Datasets. To evaluate the effectiveness of our proposed Tree-GRPO in LLM agentic RL, we conduct experiments on 11 benchmarks across three categories: (i) Multi-Hop QA including: HotpotQA (Yang et al., 2018), 2WikiMultiHopQA (Ho et al., 2020), Musique (Trivedi et al., 2022), and Bamboogle (Press et al., 2023); (ii) Single-Hop QA including: NQ (Kwiatkowski et al., 2019), TriviaQA (Joshi et al., 2017), and PopQA (Mallen et al., 2023); (iii) Web-Agent QA including: SimpleQA (Press et al., 2023), GAIA (Mialon et al., 2023), WebWalkerQA (Wu et al., 2025b), and BrowseComp (Wei et al., 2025). Baselines. We compare the proposed Tree-GRPO against the following baseline: (i) Direct Prompting Methods including direct inference, ReAct agent framework (Yao et al., 2023b); (ii) Advanced RAG Method Search-o1 (Li et al., 2025a); (iii) RL-based Methods including GRPO (DeepSeek-AI Team, 2025) and GSPO (Zheng et al., 2025). Our implementation is built upon the Search-R1 (Jin et al., 2025b) repository, including prompt template and agent-environment interaction. Experimental Setups. For all settings, we exclusively use search engines as the designated tool. For Multi-Hop QA and Single-Hop QA settings, we use an E5-based local retrieval server (Wang et al., 2024) built on Wikipedia dump (Karpukhin et al., 2020), while the Web-Agent QA setting employs real web search API for retrieval. Metrics. Exact Match (EM) is used for both the training and test score in Single-Hop QA and Multi-Hop QA. For Web-Agent QA, we utilize F1 scores for both training and test. We conduct experiments using two series of models Qwen-2.5 (Base/Instruct) (Qwen et al., 2025) and Llama-3.2 (Base/Instruct) (Llama Team, 2024) across different parameter scales including 1.5b, 3b, 7b, and 14b. If not specified, the default rollout budget is 4 for each prompt during training. More implementation and experiment details are in Appendix and B. 4.2 MAIN RESULTS Multi-Hop QA. As shown in Table 1, in multi-hop QA setting that requires multi-turn interactions, although models within the ReAct framework can acquire information via tool calls, small models (<7b parameters) exhibit little improvement over direct inference. This indicates that prompting 6 Preprint. Table 1: Overall performance on single-hop QA and multi-hop QA, with EM scores for each dataset. The best results are indicated in bold. Method Single-Hop QA Multi-Hop QA NQ Trivia PopQA Avg./% rel Hotpot 2wiki Musiq Bamb Direct Inference 5 Search-o1 . 1 - ReAct 5 . 2 Q 7.1 10.2 9.5 + GRPO 39.4 + GSPO 36.8 + Tree-GRPO 43.6 3 - 5 . 2 Q Direct Inference Search-o1 ReAct 10.6 15.1 21.1 + GRPO 44.4 + GSPO 43.0 + Tree-GRPO 46.8 Direct Inference Search-o1 ReAct 16.2 24.2 23.9 + GRPO 45.5 + GSPO 41.2 + Tree-GRPO 47.7 3 - 2 . 3 l 7 - 5 . 2 Q Direct Inference Search-o1 ReAct 13.4 23.8 30.6 + GRPO 45.8 + GSPO 47.0 + Tree-GRPO 48.1 Direct Inference Search-o1 ReAct 19.8 34.7 36.1 + GRPO 51.3 + GSPO 50.7 + Tree-GRPO 51.7 4 1 - 5 . 2 Q 22.4 30.9 22.1 51.0 48.9 57. 28.8 44.3 43.5 58.0 58.8 59.7 29.6 48.4 42.4 58.2 57.8 59.9 40.8 47.2 56.3 61.5 64.5 63.3 53.1 63.5 64.2 67.2 67.4 68.1 9.9 15.0 13.8 39.7 37.3 41.6 10.8 13.1 28.3 42.0 42.5 43. 7.4 8.8 21.7 42.4 40.8 42.3 14.0 26.2 34.6 44.3 46.1 45.2 18.4 24.1 39.3 46.7 47.1 47.3 13.1 15.4 15.1 43.4 base 41.05.5% 47.5+9.5% 16.7 24.2 31.0 48.1 base 48.1+0.0% 50.0+4.0% 17.7 27.1 29.3 48.7 base 46.64.3% 50.0+2.7% 22.7 32.4 40.5 50.5 base 52.5+4.0% 52.2+3.4% 30.4 40.8 46.5 55.1 base 55.1+0.0% 55.7+1.1% 5.9 11.6 7.3 14.6 15.8 29.5 14.9 18.7 19.2 39.0 40.2 42.4 12.6 19.4 16.2 36.0 28.1 44.6 18.3 22.1 27.9 42.5 40.0 44. 21.7 26.8 39.1 47.7 50.1 50.2 4.3 12.2 8.0 24.4 23.7 26.8 24.4 17.6 19.1 36.3 39.8 43.7 9.2 17.4 10.4 26.9 24.5 38.4 25.0 21.8 25.3 40.7 38.2 42.3 25.3 16.1 33.8 42.6 50.2 50. 2.6 3.1 1.9 2.2 2.5 6.6 2.0 5.8 4.8 15.2 17.0 17.8 2.0 6.0 3.5 11.8 8.6 17.6 3.1 5.4 11.3 19.1 19.2 20.2 4.5 9.9 15.0 23.2 23.8 25.9 8.0 13.0 11.2 4.0 4.8 13. 2.4 29.6 20.0 36.8 36.8 43.2 8.0 32.0 23.2 32.0 32.0 46.4 12.0 32.0 28.8 43.2 44.0 44.0 16.0 41.6 43.2 53.6 52.8 54.4 Avg./% rel 5.2 10.0 7.1 11.3 base 11.7+3.5% 19.1+69% 10.9 17.9 15.8 31.8 base 33.5+5.2% 36.8+16% 8.0 14.1 13.3 26.7 base 23.313% 36.8+38% 14.6 20.3 23.3 36.4 base 35.42.8% 37.8+3.9% 16.9 23.6 32.8 41.8 base 44.2+5.7% 45.3+8.4% alone is insufficient for models to complete long-horizon agent tasks. Among RL approaches, our Tree-GRPO method achieves substantial margin over chain-based GRPO baseline on models below 3b, yielding relative improvements ranging from 16% to 69% across both the Llama and Qwen series models. Remarkably, Tree-GRPO remains effective on Qwen2.5-1.5b, whereas chain-based methods struggle to stimulate multi-turn tool-use behavior. Although RL offers limited benefits on Qwen2.5-14b, our tree-based method still achieves an average relative improvement of 8.4%. These results demonstrate the superiority of the process signal provided by the tree-based method. Single-Hop QA. In the single-hop QA setting which requires fewer interaction turns, the 14b model already exhibits agentic capabilities to complete tasks under the ReAct framework as Table 1. Compared to chain-based RL methods, Tree-GRPO still shows stable improvements, especially for small models like Qwen2.5-1.5b and Qwen2.5-3b. However, for most single-hop questions, the agent does not require multi-turn ReAct-style interactions and can usually solve the problem with just one round of retrieval followed by one round of answering. Due to the tree depth in this setting being limited (typically 2), the gains from process-level signals over trajectory-level are also limited. Web-Agent QA. Existing open-source web-agent QA benchmarks are predominantly test sets, with notable lack of training sets. Moreover, most of these test benchmarks are highly challenging, with some tasks requiring dozens of web interactions. The limited training data we were able to collect fails to match this level of difficulty and quality. Consequently, the performance improvement from RL is relatively limited, as shown in Table 2. In this case, Tree-GRPO consistently outperforms the chain-based GRPO across four test datasets, most notably on GAIA with 28% average improvement. However, on more challenging benchmarks such as BrowseComp, RL yields only marginal gains, which is primarily constrained by the training data. 7 Preprint. Table 2: Overall performance on web-agent QA, with F1 scores for each dataset. The best results are indicated in bold. Method SimpleQA General AI Assistant WebWalkerQA BrowseComp Avg. Lv.1 Lv.2 Lv.3 Avg. Easy Med. Hard Avg. Avg. e 5 . 2 Q 5 . 2 Q Qwen2.5-32b-Instruct 2 3 DeepSeek-R1-Distill-32b 7 4 1 ReAct + GRPO + Tree-GRPO ReAct + GRPO + Tree-GRPO 7.7 12.6 25.1 61.5 62.4 43.3 65.4 67.8 8.8 19.2 6.2 17.7 19.3 11.4 21.6 20. 7.7 7.8 3.5 14.9 17.5 7.1 15.0 24.3 3.0 4.1 1.1 4.5 5.7 0.9 5.5 7. 7.6 11.7 4.2 14.7 16.8 8.0 16.4 21.0 6.2 9.4 8.0 8.9 9.3 9.5 11.4 11. 9.4 13.3 9.2 11.4 11.8 11.3 14.8 15.5 5.8 9.4 5.6 11.6 11.9 7.4 10.3 10. 7.4 11.0 7.6 10.9 11.2 9.5 12.4 12.8 2.2 2.4 1.3 2.3 2.7 1.2 2.4 2. Table 3: Performance with different training budget (defined as the cost of several complete agent trajectories per prompt). The base model is Qwen2.5-3b. The best results are indicated in bold. Method Single-Hop QA Multi-Hop QA NQ Trivia PopQA Avg./% rel Hotpot 2wiki Musiq Bamb Avg./% rel Rollout Token/Tool Budget 2/per prompt Chain-based Tree-based (M = 1, = 2, = 1) 42.0 46.1 56.7 59.4 40.8 43.6 46.5 base 49.7+6.9% 17.9 39.5 25.6 40. 3.3 13.7 12.8 32.8 14.9 base 31.6+112% Rollout Token/Tool Budget 4/per prompt Chain-based Tree-based (M = 2, = 2, = 1) 44.4 46. 58.0 59.7 42.0 43.6 48.1 base 50.0+4.0% 39.0 42.4 36.3 43.7 15.2 17. 36.8 43.2 31.8 base 36.8+16% Rollout Token/Tool Budget 8/per prompt Chain-based Tree-based (M = 4, = 2, = 1) 46.5 47.6 59.2 60. 44.3 44.2 50.0 base 50.8+1.6% 39.4 42.0 36.4 42.9 16.1 19.5 33.6 36. 31.4 base 35.1+12% Rollout Token/Tool Budget 16/per prompt Chain-based Tree-based (M = 8, = 2, = 1) Tree-based (M = 6, = 3, = 1) Tree-based (M = 4, = 5, = 1) 47.8 48.6 48.5 48.4 61.1 61.7 61.6 61.3 44.7 44.9 45.0 43. 51.2 base 51.7+1.0% 51.7+1.0% 51.2+0.0% 40.1 44.6 45.3 45.0 38.8 43.2 44.1 43.9 17.5 18.2 18.8 18.5 39.2 38.4 37.6 41.6 33.9 base 36.1+6.5% 36.5+7.7% 37.3+10% 4.3 QUANTITATIVE ANALYSIS In this section, we extend our study to more training configurations and analyze what the tree-based method affords beyond performance. Different Training Budget. In LLM agent RL training, the token/tool-call costs introduced by multi-turn interactions are an important concern. Here we assess our method under different cost constraints. As shown in Table 3, the tree-based method consistently demonstrates improvements under different budget settings. Especially under highly constrained rollout budgets (e.g., when only two complete rollouts budget per prompt), chain-based RL struggles to learn multi-turn interactions, whereas the tree-based method achieves substantially better results (112% relative improvement). As the rollout budget increases, the superiority of the tree-based method having more training trajectories gradually diminishes in the single-hop setting, whereas the benefit of finer process supervision signals remains in the multi-hop setting. Remarkably, our Tree-GRPO achieves superior performance over the chain-based method while using only quarter of the rollout budget. In addition, when the rollout budget is larger, tree-based sampling offers more flexibility in parameter choices. More analysis is in Appendix B.4. Chain-based vs. Tree-based Beyond Performance. Due to the sparse nature of outcome rewards in multi-turn agentic RL, the model often struggles to learn more complex processes, tending instead to favor shorter interaction paths rather than extended exploration, and in some cases even learning toward unreasonable shortcuts. While such behavior is generally acceptable, it becomes limitation for agent tasks that inherently require longer multi-turn interactions. In our experiments on the multi-hop QA setting in Figure 5, we find that beyond performance improvements in training reward, the tree-based method also encourages the LLM agent to engage in longer interactions (i.e., making more tool calls) from 2.4 to 3.0 on average to solve each multi-hop QA. This is particularly 8 Preprint. (a) Comparison on Qwen2.5-3b (b) Comparison on Llama3.2-3b-it Figure 5: Comparison between tree-based and chain-based RL on reward and action number. Table 4: Ablation study on tree-based advantages. Advantage Hotpot 2Wiki Musiq Bamb Avg. Qwen2.5-3b w. Chain-based GRPO 39.0 36.3 15. 36.8 31.8 Qwen2.5-3b w. Tree-based ˆAintra-tree ˆAinter-tree ˆAintra-tree + ˆAinter-tree 1.1 40.6 42.4 1.7 41.3 43. 0.2 16.5 17.8 1.6 36.8 43.2 1.2 33.8 36.8 Figure 6: Ablation study on LR warmup ratio. meaningful for training agents capable of solving more complex long-horizon tasks. Moreover, we observe that the learning rate (LR) warmup is particularly sensitive hyperparameter when training small models below 3b. The results in Figure 6 show that our tree-based method outperforms the chain-based method under all LR warmup ratio settings. Tree-based Advantage. In Table 4, we compare the performance of different advantage estimating methods. As result, the training with only ˆAintra-tree is unstable and eventually leads to collapse. When ˆAintra-tree and ˆAinter-tree are combined, the agent RL can introduce step-level preference learning property while maintaining greater stability. It is worth noting that due to the more efficient sampling of tree search under the same budget, when using vanilla global group relative advantage estimation (e.g., ˆAinter-tree), the tree-based results are still better than the chain-based GRPO."
        },
        {
            "title": "5 RELATED WORK",
            "content": "Reinforcement Learning for LLM and Agent. Recent advances in RL (Kaufmann et al., 2024; Lambert et al., 2025) have demonstrated the effectiveness in LLM alignment and reasoning. Although process reward models have been shown to provide more gains on complex reasoning tasks (Lightman et al.; Shao et al., 2024; Zhang et al., 2025c; Wang et al., 2025b), most existing works still rely solely on outcome rewards for training due to the additional costs and limited scalability of process reward models. Techniques ranging from PPO (Schulman et al., 2018), GRPO (DeepSeek-AI Team, 2025), GSPO (Zheng et al., 2025), along with more policy variants such as DAPO (Yu et al., 2025) and GPG (Chu et al., 2025b) are employed for LLM RL training. Recent works (Wang et al., 2025a; Feng et al., 2025b; Dong et al., 2025) apply this paradigm to end-to-end agent training. In addition to these online RL approaches, another line of work (Wang et al., 2025b; Xie et al., 2024; Xiong et al., 2024) directly constructs step-level DPO training data in an offline manner to achieve more fine-grained optimization objectives, while increasing the complexity of the training pipeline. Tree Search for LLM Reasoning. One line of LLM tree-search research focuses on test-time scaling. Yao et al. (2023a); Long (2023); Snell et al. (2024) propose tree-of-thought to allow LLMs to consider multiple reasoning paths during solving complex tasks. Xin et al. (2024; 2025) employs the Monte-Carlo tree search strategy to generate diverse proof paths for theorem proving problems. Another line of research (He et al., 2024; Feng et al., 2024; Wu et al., 2024; Xie et al., 2024; Zhang et al., 2025c; Lai et al., 2024) is to utilize tree-search structures for constructing step-level preference learning data, which are then used in DPO or SFT. There are also some works (Hou et al., 2025; Zhang et al., 2024; Yang et al., 2025b) similar to ours that employ tree search for sampling in LLM online RL. However, since these tree search methods are still conducted at the token/sentence level and cannot be directly employed on agent tasks. 9 Preprint."
        },
        {
            "title": "6 CONCLUSION",
            "content": "In this work, we propose Tree-based Group Relative Policy Optimization (Tree-GRPO), adopting tree-search rollout strategy in place of independent chain-based rollouts for LLM agent RL. Based on agent step-level nodes, Tree-GRPO carries out rollout sampling over semantically well-defined search tree. By sharing common prefixes, the tree search sampling significantly reduces the rollout budget in terms of both tokens and tool calls during training. Tree-GRPO leverages the tree structure to conduct tree-based grouping for advantage estimation, introducing an implicit step-level preference-learning objective. Empirical evaluations on 11 datasets demonstrate the superiority of our tree-based approach for agentic RL."
        },
        {
            "title": "REFERENCES",
            "content": "Tianzhe Chu, Yuexiang Zhai, Jihan Yang, Shengbang Tong, Saining Xie, Dale Schuurmans, Quoc V. Le, Sergey Levine, and Yi Ma. SFT Memorizes, RL Generalizes: Comparative Study of Foundation Model Post-training, May 2025a. URL http://arxiv.org/abs/2501.17161. arXiv:2501.17161 [cs]. Xiangxiang Chu, Hailang Huang, Xiao Zhang, Fei Wei, and Yong Wang. GPG: Simple and Strong Reinforcement Learning Baseline for Model Reasoning, May 2025b. URL http://arxiv. org/abs/2504.02546. arXiv:2504.02546 [cs]. DeepSeek-AI Team. DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning, January 2025. URL http://arxiv.org/abs/2501.12948. arXiv:2501.12948 [cs]. Guanting Dong, Hangyu Mao, Kai Ma, Licheng Bao, Yifei Chen, Zhongyuan Wang, Zhongxia Chen, Jiazhen Du, Huiyang Wang, Fuzheng Zhang, Guorui Zhou, Yutao Zhu, Ji-Rong Wen, and Zhicheng Dou. Agentic Reinforced Policy Optimization, July 2025. URL http://arxiv. org/abs/2507.19849. arXiv:2507.19849 [cs]. Jiazhan Feng, Shijue Huang, Xingwei Qu, Ge Zhang, Yujia Qin, Baoquan Zhong, Chengquan Jiang, Jinxin Chi, and Wanjun Zhong. ReTool: Reinforcement Learning for Strategic Tool Use in LLMs, April 2025a. URL http://arxiv.org/abs/2504.11536. arXiv:2504.11536 [cs]. Lang Feng, Zhenghai Xue, Tingcong Liu, and Bo An. Group-in-Group Policy Optimization for LLM Agent Training, May 2025b. URL http://arxiv.org/abs/2505.10978. arXiv:2505.10978 [cs]. Xidong Feng, Ziyu Wan, Muning Wen, Stephen Marcus McAleer, Ying Wen, Weinan Zhang, and Jun Wang. Alphazero-like Tree-Search can Guide Large Language Model Decoding and Training, February 2024. URL http://arxiv.org/abs/2309.17179. arXiv:2309.17179 [cs]. Jiaxuan Gao, Wei Fu, Minyang Xie, Shusheng Xu, Chuyi He, Zhiyu Mei, Banghua Zhu, and Yi Wu. Beyond Ten Turns: Unlocking Long-Horizon Agentic Search with Large-Scale Asynchronous RL, August 2025. URL http://arxiv.org/abs/2508.07976. arXiv:2508.07976 [cs]. Mingqian He, Yongliang Shen, Wenqi Zhang, Zeqi Tan, and Weiming Lu. Advancing Process Verification for Large Language Models via Tree-Based Preference Learning, June 2024. URL http://arxiv.org/abs/2407.00390. arXiv:2407.00390 [cs]. Xanh Ho, Anh-Khoa Duong Nguyen, Saku Sugawara, and Akiko Aizawa. Constructing Multi-hop QA Dataset for Comprehensive Evaluation of Reasoning Steps, November 2020. URL http: //arxiv.org/abs/2011.01060. arXiv:2011.01060 [cs]. Zhenyu Hou, Ziniu Hu, Yujiang Li, Rui Lu, Jie Tang, and Yuxiao Dong. TreeRL: LLM Reinforcement Learning with On-Policy Tree Search, June 2025. URL http://arxiv.org/abs/ 2506.11902. arXiv:2506.11902 [cs]. Bowen Jin, Jinsung Yoon, Priyanka Kargupta, Sercan O. Arik, and Jiawei Han. An Empirical Study on Reinforcement Learning for Reasoning-Search Interleaved LLM Agents, May 2025a. URL http://arxiv.org/abs/2505.15117. arXiv:2505.15117 [cs]. 10 Preprint. Bowen Jin, Hansi Zeng, Zhenrui Yue, Jinsung Yoon, Sercan Arik, Dong Wang, Hamed Zamani, and Jiawei Han. Search-R1: Training LLMs to Reason and Leverage Search Engines with Reinforcement Learning, April 2025b. URL http://arxiv.org/abs/2503.09516. arXiv:2503.09516 [cs]. Mandar Joshi, Eunsol Choi, Daniel Weld, and Luke Zettlemoyer. TriviaQA: Large Scale Distantly Supervised Challenge Dataset for Reading Comprehension. In Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), Vancouver, Canada, 2017. Association for Computational Linguistics. doi: 10.18653/v1/p17-1147. URL http://aclweb.org/anthology/P17-1147. Vladimir Karpukhin, Barlas Oguz, Sewon Min, Patrick Lewis, Ledell Wu, Sergey Edunov, Danqi Chen, and Wen-tau Yih. Dense Passage Retrieval for Open-Domain Question Answering, September 2020. URL http://arxiv.org/abs/2004.04906. arXiv:2004.04906 [cs]. Timo Kaufmann, Paul Weng, Viktor Bengs, and Eyke Hüllermeier. Survey of Reinforcement Learning from Human Feedback, April 2024. URL http://arxiv.org/abs/2312. 14925. arXiv:2312.14925 [cs]. Kimi Team. Kimi K2: Open Agentic Intelligence, July 2025a. URL http://arxiv.org/abs/ 2507.20534. arXiv:2507.20534 [cs]. Kimi Team. Kimi k1.5: Scaling Reinforcement Learning with LLMs, June 2025b. URL http: //arxiv.org/abs/2501.12599. arXiv:2501.12599 [cs]. Tom Kwiatkowski, Jennimaria Palomaki, Olivia Redfield, Michael Collins, Ankur Parikh, Chris Alberti, Danielle Epstein, Illia Polosukhin, Jacob Devlin, Kenton Lee, Kristina Toutanova, Llion Jones, Matthew Kelcey, Ming-Wei Chang, Andrew M. Dai, Jakob Uszkoreit, Quoc Le, and Slav Petrov. Natural Questions: Benchmark for Question Answering Research. Transactions of the Association for Computational Linguistics, 7:453466, November 2019. ISSN 2307-387X. doi: 10.1162/tacl_a_00276. URL https://direct.mit.edu/tacl/article/43518. Publisher: MIT Press - Journals. Xin Lai, Zhuotao Tian, Yukang Chen, Senqiao Yang, Xiangru Peng, and Jiaya Jia. Step-DPO: Step-wise Preference Optimization for Long-chain Reasoning of LLMs, June 2024. URL http: //arxiv.org/abs/2406.18629. arXiv:2406.18629 [cs]. Nathan Lambert, Jacob Morrison, Valentina Pyatkin, Shengyi Huang, Hamish Ivison, Faeze Brahman, Lester James V. Miranda, Alisa Liu, Nouha Dziri, Shane Lyu, Yuling Gu, Saumya Malik, Victoria Graf, Jena D. Hwang, Jiangjiang Yang, Ronan Le Bras, Oyvind Tafjord, Chris Wilhelm, Luca Soldaini, Noah A. Smith, Yizhong Wang, Pradeep Dasigi, and Hannaneh Hajishirzi. Tulu 3: Pushing Frontiers in Open Language Model Post-Training, April 2025. URL http://arxiv.org/abs/2411.15124. arXiv:2411.15124 [cs]. Xiaoxi Li, Guanting Dong, Jiajie Jin, Yuyao Zhang, Yujia Zhou, Yutao Zhu, Peitian Zhang, and Zhicheng Dou. Search-o1: Agentic Search-Enhanced Large Reasoning Models, January 2025a. URL http://arxiv.org/abs/2501.05366. arXiv:2501.05366 [cs]. Yizhi Li, Qingshui Gu, Zhoufutu Wen, Ziniu Li, Tianshun Xing, Shuyue Guo, Tianyu Zheng, Xin Zhou, Xingwei Qu, Wangchunshu Zhou, Zheng Zhang, Wei Shen, Qian Liu, Chenghua Lin, Jian Yang, Ge Zhang, and Wenhao Huang. TreePO: Bridging the Gap of Policy Optimization and Efficacy and Inference Efficiency with Heuristic Tree-based Modeling, August 2025b. URL http://arxiv.org/abs/2508.17445. arXiv:2508.17445 [cs]. Hunter Lightman, Vineet Kosaraju, Yura Burda, Harri Edwards, Bowen Baker, Teddy Lee, Jan Leike, John Schulman, Ilya Sutskever, and Karl Cobbe. Lets Verify Step by Step. Llama Team. The Llama 3 Herd of Models, November 2024. URL http://arxiv.org/abs/ 2407.21783. arXiv:2407.21783 [cs]. Jieyi Long. Large Language Model Guided Tree-of-Thought, May 2023. URL http://arxiv. org/abs/2305.08291. arXiv:2305.08291 [cs]. 11 Preprint. Alex Mallen, Akari Asai, Victor Zhong, Rajarshi Das, Daniel Khashabi, and Hannaneh Hajishirzi. When Not to Trust Language Models: Investigating Effectiveness of Parametric and NonParametric Memories. In Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), Toronto, Canada, 2023. Association for Computational Linguistics. doi: 10.18653/v1/2023.acl-long.546. URL https://aclanthology. org/2023.acl-long.546. Grégoire Mialon, Clémentine Fourrier, Craig Swift, Thomas Wolf, Yann LeCun, and Thomas Scialom. GAIA: benchmark for General AI Assistants, November 2023. URL http: //arxiv.org/abs/2311.12983. arXiv:2311.12983 [cs]. OpenAI. OpenAI o1 System Card, December 2024. URL http://arxiv.org/abs/2412. 16720. arXiv:2412.16720 [cs]. Ofir Press, Muru Zhang, Sewon Min, Ludwig Schmidt, Noah A. Smith, and Mike Lewis. Measuring and Narrowing the Compositionality Gap in Language Models, October 2023. URL http: //arxiv.org/abs/2210.03350. arXiv:2210.03350 [cs]. Cheng Qian, Emre Can Acikgoz, Qi He, Hongru Wang, Xiusi Chen, Dilek Hakkani-Tür, Gokhan Tur, and Heng Ji. ToolRL: Reward is All Tool Learning Needs, April 2025. URL http:// arxiv.org/abs/2504.13958. arXiv:2504.13958 [cs]. Qwen, An Yang, Baosong Yang, Beichen Zhang, Binyuan Hui, Bo Zheng, Bowen Yu, Chengyuan Li, Dayiheng Liu, Fei Huang, Haoran Wei, Huan Lin, Jian Yang, Jianhong Tu, Jianwei Zhang, Jianxin Yang, Jiaxi Yang, Jingren Zhou, Junyang Lin, Kai Dang, Keming Lu, Keqin Bao, Kexin Yang, Le Yu, Mei Li, Mingfeng Xue, Pei Zhang, Qin Zhu, Rui Men, Runji Lin, Tianhao Li, Tianyi Tang, Tingyu Xia, Xingzhang Ren, Xuancheng Ren, Yang Fan, Yang Su, Yichang Zhang, Yu Wan, Yuqiong Liu, Zeyu Cui, Zhenru Zhang, and Zihan Qiu. Qwen2.5 Technical Report, January 2025. URL http://arxiv.org/abs/2412.15115. arXiv:2412.15115 [cs]. John Schulman, Philipp Moritz, Sergey Levine, Michael Jordan, and Pieter Abbeel. HighDimensional Continuous Control Using Generalized Advantage Estimation, October 2018. URL http://arxiv.org/abs/1506.02438. arXiv:1506.02438 [cs]. Zhihong Shao, Peiyi Wang, Qihao Zhu, Runxin Xu, Junxiao Song, Xiao Bi, Haowei Zhang, Mingchuan Zhang, Y. K. Li, Y. Wu, and Daya Guo. DeepSeekMath: Pushing the Limits of Mathematical Reasoning in Open Language Models, April 2024. URL http://arxiv.org/ abs/2402.03300. arXiv:2402.03300 [cs]. Joykirat Singh, Raghav Magazine, Yash Pandya, and Akshay Nambi. Agentic Reasoning and Tool Integration for LLMs via Reinforcement Learning, April 2025. URL http://arxiv.org/ abs/2505.01441. arXiv:2505.01441 [cs]. Charlie Snell, Jaehoon Lee, Kelvin Xu, and Aviral Kumar. Scaling LLM Test-Time Compute Optimally can be More Effective than Scaling Model Parameters, August 2024. URL http: //arxiv.org/abs/2408.03314. arXiv:2408.03314 [cs]. Harsh Trivedi, Niranjan Balasubramanian, Tushar Khot, and Ashish Sabharwal. MuSiQue: Multihop Questions via Single-hop Question Composition, May 2022. URL http://arxiv.org/ abs/2108.00573. arXiv:2108.00573 [cs]. Liang Wang, Nan Yang, Xiaolong Huang, Binxing Jiao, Linjun Yang, Daxin Jiang, Rangan Majumder, and Furu Wei. Text Embeddings by Weakly-Supervised Contrastive Pre-training, February 2024. URL http://arxiv.org/abs/2212.03533. arXiv:2212.03533 [cs]. Zihan Wang, Kangrui Wang, Qineng Wang, Pingyue Zhang, Linjie Li, Zhengyuan Yang, Xing Jin, Kefan Yu, Minh Nhat Nguyen, Licheng Liu, Eli Gottlieb, Yiping Lu, Kyunghyun Cho, Jiajun Wu, Li Fei-Fei, Lijuan Wang, Yejin Choi, and Manling Li. RAGEN: Understanding Self-Evolution in LLM Agents via Multi-Turn Reinforcement Learning, May 2025a. URL http://arxiv. org/abs/2504.20073. arXiv:2504.20073 [cs]. Ziliang Wang, Xuhui Zheng, Kang An, Cijun Ouyang, Jialu Cai, Yuhang Wang, and Yichao Wu. StepSearch: Igniting LLMs Search Ability via Step-Wise Proximal Policy Optimization, May 2025b. URL http://arxiv.org/abs/2505.15107. arXiv:2505.15107 [cs]. 12 Preprint. Jason Wei, Zhiqing Sun, Spencer Papay, Scott McKinney, Jeffrey Han, Isa Fulford, Hyung Won Chung, Alex Tachard Passos, William Fedus, and Amelia Glaese. BrowseComp: Simple Yet Challenging Benchmark for Browsing Agents, April 2025. URL http://arxiv.org/abs/ 2504.12516. arXiv:2504.12516 [cs]. Jialong Wu, Baixuan Li, Runnan Fang, Wenbiao Yin, Liwen Zhang, Zhengwei Tao, Dingchu Zhang, Zekun Xi, Yong Jiang, Pengjun Xie, Fei Huang, and Jingren Zhou. WebDancer: Towards Autonomous Information Seeking Agency, May 2025a. URL http://arxiv.org/abs/ 2505.22648. arXiv:2505.22648 [cs]. Jialong Wu, Wenbiao Yin, Yong Jiang, Zhenglin Wang, Zekun Xi, Runnan Fang, Linhai Zhang, Yulan He, Deyu Zhou, Pengjun Xie, and Fei Huang. WebWalker: Benchmarking LLMs in Web Traversal, August 2025b. URL http://arxiv.org/abs/2501.07572. arXiv:2501.07572 [cs]. Zijian Wu, Suozhi Huang, Zhejian Zhou, Huaiyuan Ying, Jiayu Wang, Dahua Lin, and Kai Chen. InternLM2.5-StepProver: Advancing Automated Theorem Proving via Expert Iteration on LargeScale LEAN Problems, October 2024. URL http://arxiv.org/abs/2410.15700. arXiv:2410.15700 [cs]. Yuxi Xie, Anirudh Goyal, Wenyue Zheng, Min-Yen Kan, Timothy P. Lillicrap, Kenji Kawaguchi, and Michael Shieh. Monte Carlo Tree Search Boosts Reasoning via Iterative Preference Learning, June 2024. URL http://arxiv.org/abs/2405.00451. arXiv:2405.00451 [cs]. Huajian Xin, Z. Z. Ren, Junxiao Song, Zhihong Shao, Wanjia Zhao, Haocheng Wang, Bo Liu, Liyue Zhang, Xuan Lu, Qiushi Du, Wenjun Gao, Qihao Zhu, Dejian Yang, Zhibin Gou, Z. F. Wu, Fuli Luo, and Chong Ruan. DeepSeek-Prover-V1.5: Harnessing Proof Assistant Feedback for Reinforcement Learning and Monte-Carlo Tree Search, August 2024. URL http://arxiv. org/abs/2408.08152. arXiv:2408.08152 [cs]. Ran Xin, Chenguang Xi, Jie Yang, Feng Chen, Hang Wu, Xia Xiao, Yifan Sun, Shen Zheng, and Kai Shen. BFS-Prover: Scalable Best-First Tree Search for LLM-based Automatic Theorem Proving, February 2025. URL http://arxiv.org/abs/2502.03438. arXiv:2502.03438 [cs]. Wei Xiong, Chengshuai Shi, Jiaming Shen, Aviv Rosenberg, Zhen Qin, Daniele Calandriello, Misha Khalman, Rishabh Joshi, Bilal Piot, Mohammad Saleh, Chi Jin, Tong Zhang, and Tianqi Liu. Building Math Agents with Multi-Turn Iterative Preference Learning, September 2024. URL http://arxiv.org/abs/2409.02392. arXiv:2409.02392 [cs] version: 1. An Yang, Anfeng Li, Baosong Yang, Beichen Zhang, Binyuan Hui, Bo Zheng, Bowen Yu, Chang Gao, Chengen Huang, Chenxu Lv, Chujie Zheng, Dayiheng Liu, Fan Zhou, Fei Huang, Feng Hu, Hao Ge, Haoran Wei, Huan Lin, Jialong Tang, Jian Yang, Jianhong Tu, Jianwei Zhang, Jianxin Yang, Jiaxi Yang, Jing Zhou, Jingren Zhou, Junyang Lin, Kai Dang, Keqin Bao, Kexin Yang, Le Yu, Lianghao Deng, Mei Li, Mingfeng Xue, Mingze Li, Pei Zhang, Peng Wang, Qin Zhu, Rui Men, Ruize Gao, Shixuan Liu, Shuang Luo, Tianhao Li, Tianyi Tang, Wenbiao Yin, Xingzhang Ren, Xinyu Wang, Xinyu Zhang, Xuancheng Ren, Yang Fan, Yang Su, Yichang Zhang, Yinger Zhang, Yu Wan, Yuqiong Liu, Zekun Wang, Zeyu Cui, Zhenru Zhang, Zhipeng Zhou, and Zihan Qiu. Qwen3 Technical Report, May 2025a. URL http://arxiv.org/abs/2505.09388. arXiv:2505.09388 [cs]. Zhicheng Yang, Zhijiang Guo, Yinya Huang, Xiaodan Liang, Yiwei Wang, and Jing Tang. TreeRPO: Tree Relative Policy Optimization, June 2025b. URL http://arxiv.org/abs/2506. 05183. arXiv:2506.05183 [cs]. Zhilin Yang, Peng Qi, Saizheng Zhang, Yoshua Bengio, William W. Cohen, Ruslan Salakhutdinov, and Christopher D. Manning. HotpotQA: Dataset for Diverse, Explainable Multihop Question Answering, September 2018. URL http://arxiv.org/abs/1809.09600. arXiv:1809.09600 [cs]. Shunyu Yao, Dian Yu, Jeffrey Zhao, Izhak Shafran, Thomas L. Griffiths, Yuan Cao, and Karthik Narasimhan. Tree of Thoughts: Deliberate Problem Solving with Large Language Models, December 2023a. URL http://arxiv.org/abs/2305.10601. arXiv:2305.10601 [cs]. 13 Preprint. Shunyu Yao, Jeffrey Zhao, Dian Yu, Nan Du, Izhak Shafran, Karthik Narasimhan, and Yuan Cao. ReAct: Synergizing Reasoning and Acting in Language Models, March 2023b. URL http: //arxiv.org/abs/2210.03629. arXiv:2210.03629 [cs]. Qiying Yu, Zheng Zhang, Ruofei Zhu, Yufeng Yuan, Xiaochen Zuo, Yu Yue, Weinan Dai, Tiantian Fan, Gaohong Liu, Lingjun Liu, Xin Liu, Haibin Lin, Zhiqi Lin, Bole Ma, Guangming Sheng, Yuxuan Tong, Chi Zhang, Mofan Zhang, Wang Zhang, Hang Zhu, Jinhua Zhu, Jiaze Chen, Jiangjie Chen, Chengyi Wang, Hongli Yu, Yuxuan Song, Xiangpeng Wei, Hao Zhou, Jingjing Liu, Wei-Ying Ma, Ya-Qin Zhang, Lin Yan, Mu Qiao, Yonghui Wu, and Mingxuan Wang. DAPO: An Open-Source LLM Reinforcement Learning System at Scale, May 2025. URL http://arxiv.org/abs/2503.14476. arXiv:2503.14476 [cs]. Chong Zhang, Yue Deng, Xiang Lin, Bin Wang, Dianwen Ng, Hai Ye, Xingxuan Li, Yao Xiao, Zhanfeng Mo, Qi Zhang, and Lidong Bing. 100 Days After DeepSeek-R1: Survey on Replication Studies and More Directions for Reasoning Language Models, May 2025a. URL http://arxiv.org/abs/2505.00551. arXiv:2505.00551 [cs]. Dan Zhang, Sining Zhoubian, Ziniu Hu, Yisong Yue, Yuxiao Dong, and Jie Tang. ReST-MCTS*: LLM Self-Training via Process Reward Guided Tree Search, November 2024. URL http: //arxiv.org/abs/2406.03816. arXiv:2406.03816 [cs]. Guibin Zhang, Hejia Geng, Xiaohang Yu, Zhenfei Yin, Zaibin Zhang, Zelin Tan, Heng Zhou, Zhongzhi Li, Xiangyuan Xue, Yijiang Li, Yifan Zhou, Yang Chen, Chen Zhang, Yutao Fan, Zihu Wang, Songtao Huang, Yue Liao, Hongru Wang, Mengyue Yang, Heng Ji, Michael Littman, Jun Wang, Shuicheng Yan, Philip Torr, and Lei Bai. The Landscape of Agentic Reinforcement Learning for LLMs: Survey, September 2025b. URL http://arxiv.org/abs/2509.02547. arXiv:2509.02547 [cs]. Wenlin Zhang, Xiangyang Li, Kuicai Dong, Yichao Wang, Pengyue Jia, Xiaopeng Li, Yingyi Zhang, Derong Xu, Zhaocheng Du, Huifeng Guo, Ruiming Tang, and Xiangyu Zhao. Process vs. Outcome Reward: Which is Better for Agentic RAG Reinforcement Learning, May 2025c. URL http://arxiv.org/abs/2505.14069. arXiv:2505.14069 [cs]. Chujie Zheng, Shixuan Liu, Mingze Li, Xiong-Hui Chen, Bowen Yu, Chang Gao, Kai Dang, Yuqiong Liu, Rui Men, An Yang, Jingren Zhou, and Junyang Lin. Group Sequence Policy Optimization, July 2025. URL http://arxiv.org/abs/2507.18071. arXiv:2507.18071 [cs]. 14 Preprint."
        },
        {
            "title": "A DATASETS",
            "content": "Since the agent exhibits varying characteristics across different benchmark settings, we consider three experimental settings, where distinct training sets are employed. A.1 SINGLE-HOP QUESTION ANSWERING Single-hop QA is question answering solvable with single supporting passage. For single-hop QA training, we directly use the training split from NQ (Kwiatkowski et al., 2019). The whole training data includes 79,168 QA pairs. For single-hop QA test, we utilize the following datasets: Natural Questions (NQ) (Kwiatkowski et al., 2019) test set includes 3,610 QA pairs. TriviaQA (Joshi et al., 2017) test set includes 11,313 QA pairs. PopQA (Mallen et al., 2023) test set includes 14,267 QA pairs. A.2 MULTI-HOP QUESTION ANSWERING Multi-hop QA is question answering that requires compositional reasoning and aggregation of evidence across multiple documents or entities. For multi-hop QA training, we directly use the training split from HotpotQA (Yang et al., 2018). The whole training data includes 90,447 multi-hop QA pairs. For multi-hop QA test, we utilize the following datasets: HotpotQA (Yang et al., 2018) test set includes 7,405 QA pairs. 2WikiMultiHopQA (Ho et al., 2020) test set includes 12,576 QA pairs. Musique (Trivedi et al., 2022) is challenging multi-hop benchmark, which requires not only the models information retrieval capabilities, but also deeper semantic understanding and logical reasoning. The test set includes 2,417 QA pairs. Bamboogle (Press et al., 2023) is 2-hop hand maded dataset. The test set includes only 125 QA pairs. A.3 WEB-AGENT QUESTION ANSWERING Web-agent QA is question answering in interactive web environments where the agent must search, navigate, and perform singleor multi-turn exploration and information integration. The dataset for web-agent QA training consists of two parts. For the first part, we sample 2k samples from ASearcher-35K (Gao et al., 2025), where each question and answer is filtered and verified. The second part is from released samples from WebDancer (Wu et al., 2025a), which includes 200 hard web QA pairs. Due to the high cost of real search APIs and the difficulty of obtaining high-quality training samples, the scale of our Web-Agent QA experimental setup is relatively small. For web-agent QA test, we utilize the following datasets: SimpleQA (Press et al., 2023) includes 500 short, fact-seeking QA pairs, which is adversarially collected against GPT-4 responses. GAIA (Mialon et al., 2023) is hard general AI assistant benchmark consists of real-world questions. It requires the model to have the abilities such as reasoning, multi-modality handling, web browsing, and generally tool-use. We only use 103 text-only questions following previous works. WebWalkerQA (Wu et al., 2025b) contains 680 web traversal QA tasks, which is splited into three levels: easy, medium and hard. BrowseComp (Wei et al., 2025) is challenging benchmark for measuring the abilityf ro agents to browse the web. It comprises 1,266 extremely complex, hard-to-find information questions. 15 Preprint."
        },
        {
            "title": "B EXPERIMENT DETAILS",
            "content": "B."
        },
        {
            "title": "IMPLEMENTATION DETAILS",
            "content": "Our implementation is built upon Search-R1 (Jin et al., 2025b) based on VeRL. As Table 5, for all experimental settings, we use the learning rate 1e-6, and K3 KL in loss with 0.001 coefficient. If not specified, we use group size 4 for all chain-based RL, and (M = 2, = 2, = 1) for Tree-GRPO. In particular, we follow Jin et al. (2025a) to add format score λf to the training reward r(y) as: r(y) = (cid:26)score(y) λf score(y) if fformat(y) = False else (13) where λf is set to 0.2 in all experiments. The following are the different setups for different experimental settings: For Single-Hop QA and Multi-Hop QA settings, our standard setup includes total training step of 180, training batch size 512, PPO mini batch size 64. The max response length is set to 4096 tokens, and the top 3 passages from local retrieval server will be passed to the agent. For Web-Agent QA setting, we set the total training setp 34 (which corresponds to 2 epoch), training batch size 128, PPO mini batch size 64. The max response length is set to 8000 tokens, and the top 10 passages from web serp API will be passed to agent. Table 5: Hyperparameters for Tree-GRPO and baseline methods for all experiments. Config Single-Hop QA Multi-Hop QA Web-Agent QA optimizer learning rate learning rate warmup ratio KL type KL coefficient training data total training steps training batch size PPO mini batch size max response length max observation length max tool-calls reward metrics format scores retriever top-K retrieval passages AdamW 1e-6 0.285/0.5 K3 0.001 79,168 180 512 64 4096 500 3 EM 0.2 local wiki 3 AdamW 1e-6 0.285/0.5 K3 0.001 90,447 180 512 64 4096 500 3 EM 0.2 local wiki 3 AdamW 1e-6 0 K3 0.001 2,200 34 128 64 8000 1000 5 F1 score 0.2 Bing API 10 Following Jin et al. (2025b), content enclosed within <search> </search> tags is parsed as the search query, which corresponds to the Action α. The returned search results are then wrapped in <information> </information> tags to form the Observation o, thereby completing the ReAct tuple (τ, α, o). The prompt template for each question as: Prompt Template Answer the given question. You must conduct reasoning inside <think> and </think> first every time you get new information. After reasoning, if you find you lack some knowledge, you can call search engine by <search> query </search> and it will return the top searched results between <information> and </information>. You can search as many times as your want. If you find no further external knowledge needed, you can directly provide the answer inside <answer> and </answer>, without detailed illustrations. For example, <answer> Beijing </answer>. Question: 16 Preprint. B.2 BASELINES Direct Inference. For direct inference, we directly employ instruct model to answer the question. The prompt template no longer includes any tool-use instructions, and keeps only the directive Answer the given question. You must put the answer inside <answer> and </answer>. Search-o1 (Li et al., 2025a) is the search-enhanced reasoning framework, which integrates the agentic RAG mechanism and reason-in-document module. ReAct (Yao et al., 2023b) interleaves reasoning traces Though with Actions (tool calls) to enable deliberate, step-by-step problem solving. The model decides when to think and when to act, using observations to refine subsequent reasoning. We use the instruct model based on ReAct as the baseline. All the RL-based methods are also based on ReAct. GRPO (DeepSeek-AI Team, 2025) is group-relative policy optimization method that updates the policy using relative advantages computed across multiple trajectories for the same prompt. Compared to PPO, it discards the value/critic model and associated losses, yielding policy-only objective that simplifies the training pipeline with fewer components and hyperparameters. Our approach is built on GRPO, and we adopt GRPO as the primary baseline for chain-based RL. GSPO (Zheng et al., 2025) is variant of GRPO that replaces token-level importance ratio calculation with trajectory-level reweighting. By unifying importance ratio computation and advantage estimation at the trajectory level, GSPO improves the stability of LLM RL training. B.3 TREE SEARCH AT DIFFERENT LEVELS To verify the effectiveness of tree search at different levels, we also conduct tree search at the token/sentence level. Since existing tree-based RL methods (Hou et al., 2025; Yang et al., 2025b; Li et al., 2025b) are not designed for agent tasks and cannot be directly applied, here we separately implement token/sentence level tree search for agent RL. Specifically, we build the tree where each node corresponds to token, and modify step 2 of Tree-GRPO to randomly sample tokens. During sampling, we mask out tokens from the observation that are not generated by the LLM for each trajectory Hi in order to prevent information confusion as: [MASK]i,j = (cid:26)1, Hi 0, Hi {τ, α} Pi = Sample({pi,j Ti [MASK]i,j = 1}). (14) (15) The other settings remain consistent with Tree-GRPO. Table 6: Test score comparison between tree search at token/sentence-level and agent step-level. The base model is Qwen2.5-3b. The rollout budget is 4/per prompt. Tree search parameters are = 2, = 2, = 1. Method GRPO Token/sentence level Agent step level Single-Hop QA Multi-Hop QA NQ Trivia PopQA Avg. Hotpot 2wiki Musiq Bamb Avg. 44.4 42.1 46.8 58.0 56.0 59. 42.0 40.6 43.6 48.1 46.2 50.0 39.0 32.0 42. 36.3 30.8 43.7 15.2 8.4 17.8 36.8 17.6 43. 31.8 22.2 36.8 Table 6 and Figure 7 show that tree search at the token/sentence level performs worse than the agent-step level in both single-hop QA and multi-hop QA, and even falls below the performance of chain-based GRPO. We attribute this effect to two factors. On the one hand, exploring branches in the middle of an agent step lacks clear purpose and may lead to rollout budget waste. On the other hand, the credit derived from token/sentence-level tree structure undermines the integrity of the agent step, and the resulting process signal instead hinders the learning performance. Therefore, we argue that tree search conducted at the token or sentence level is not suitable for agent RL tasks. 17 Preprint. Figure 7: Training reward comparison on multi-hop QA between tree search at token/sentence-level and agent step-level. B.4 PERFORMANCE WITH DIFFERENT TREE STRUCTURES We try different tree-search structures in Table 7. When (N = 0, = 0), the Tree-GRPO degenerates into vanilla GRPO. The results show that the impact of and is not much different. However, since the expansion iterations directly affect the rollout efficiency (as the iterations are executed serially), we consider using larger and set = 1 in most of the experiments. In addition, while reducing the number of trees saves rollout budget, the more constrained exploration in tree search adversely impacts the performance of RL. Results indicate that, for different rollout budgets, balancing and can strike trade-off between exploration and exploitation in tree search, leading to better performance. Table 7: Performance on multi-hop QA with different tree structures. The base model is Qwen2.53b. The best results are indicated in bold. M, N, Hotpot 2wiki Musiq Bamb Avg. Rollout Token/Tool Budget 2/per prompt (M = 2, = 0, = 0) (M = 2, = 2, = 1) (M = 2, = 1, = 2) (M = 1, = 5, = 1) 39.0 42.4 42.3 41.5 36.3 43.7 43.2 39.3 15.2 17.8 17.6 15.8 Rollout Token/Tool Budget 16/per prompt (M = 16, = 0, = 0) (M = 8, = 2, = 1) (M = 6, = 3, = 1) (M = 4, = 5, = 1) (M = 2, = 11, = 1) (M = 2, = 6, = 2) (M = 5, = 2, = 2) (M = 2, = 4, = 3) 40.1 44.6 45.3 45.0 43.0 43.2 44.6 43.6 38.8 43.2 44.1 43.9 42.2 43.1 43.8 43.1 17.5 18.2 18.8 18.5 16.1 17.0 17.9 16.8 36.8 43.2 41.9 37.6 39.2 38.4 37.6 41.6 40.0 40.0 36.8 40. 31.8 36.8 36.3 33.6 33.9 36.1 36.5 37.3 35.3 35.8 35.8 36."
        },
        {
            "title": "C THEORETICAL ANALYSIS",
            "content": "Assumption C.1 (Binary Preference Setting) For each intermediate tree node (x, H<t), the subsequent trajectory in terms of reward falls into two categories, denoted as Hwin , with associated rewards {1,0}. The trajectory probabilities are defined as and Hloss pθ(Hwin ) = 1 pθ(Hloss ) = (cid:89) τ =t πθ(Hwin τ x, H<τ ). (16) 18 Preprint. By the assumption, the probability of Hwin {Hwin , Hloss } as: could be normalized within the binary set = pθ(Hwin ) = (cid:89) πθ(Hwin τ x, H<τ ) τ =t = pθ(Hwin = C, x, H<t) elog pθ(Hwin tx,H<t) elog pθ(Hwin = σ(log pθ(Hwin tx,H<t) + elog pθ(Hloss tx,H<t) x, H<t) log pθ(Hloss x, H<t)), and the loss probability is: pθ(Hloss ) = 1 pθ(Hwin ) = σ(log pθ(Hloss x, H<t) log pθ(Hwin x, H<t)). For step-level DPO, the objective is to optimize Bradley-Terry preference likelihood by: JDPO(θ) = (x,H<t,Hwin t,Hloss t)D (cid:2)log σ (cid:0)βθ (cid:0)x, H<t, Hwin , Hloss (cid:1)(cid:1)(cid:3) , where preference is defined by: θ(x, H<t, Hwin , Hloss ) = log pθ(Hwin x, H<t) log pθ(Hloss x, H<t). Let = βθ(x, H<t, Hwin , Hloss ), (17) (18) (19) (20) (21) where β is temperature parameter, and we assume β = 1 for simplicity. Since DPO usually treats the expectation as the empirical average of the sampled pairs (Hwin ), the gradient of Eq. 19 JDPO(θ) can be directly put inside as: θJDPO(θ) = E[θ log σ (cid:0)θ = E[θ log σ(z)] (cid:0)x, H<t, Hwin , Hloss t , Hloss (cid:1)(cid:1)] = dz log σ(z) θz = σ(z) θz = σ(z) θθ(x, H<t, Hwin = σ(z) (cid:2)θ log pθ = σ(θ(x, H<t, Hwin = σ(log pθ(Hloss (cid:0)Hwin , Hloss x, H<t) log pθ(Hwin , Hloss ) (cid:1) θ log pθ x, H<t )) (cid:2)θ log pθ elog pθ(Hloss x, H<t (cid:0)Hwin tx,H<t) (cid:0)Hloss x, H<t (cid:1)(cid:3) x, H<t)) (cid:1) θ log pθ (cid:0)Hloss x, H<t (22) (cid:1)(cid:3) = tx,H<t) (cid:0)Hwin elog pθ(Hloss tx,H<t) + elog pθ(Hwin (cid:2)θ log pθ x, H<t x, H<t) (cid:2)θ log pθ = pθ(Hloss (cid:2)θ log pθ(Hwin = p(Hloss ) (cid:124) (cid:124) (cid:123)(cid:122) (cid:125) Weight (cid:1) θ log pθ x, H<t (cid:0)Hwin ) θ log pθ(Hloss (cid:123)(cid:122) Preference Advantage Gradient (cid:0)Hloss (cid:1) θ log pθ )(cid:3) (cid:125) . x, H<t (cid:1)(cid:3) (cid:0)Hloss x, H<t (cid:1)(cid:3) For our intra-tree group policy optimization, the objective can be expressed as the combination of two categories Hwin and Hloss: JIntra-tree(θ) = [x,H<t,Htπθ(x,H<t)] 1 Gtree Gtree(cid:88) i=1 (cid:104) ˆAwin + ˆAloss (cid:105) , (23) Preprint. where Gtree is the number of leaves within the tree. Then the gradient of Eq. 23 JIntra-tree(θ) is: θJIntra-tree(θ) (cid:104) ˆAwinθ log pθ (cid:0)Hwin x, H<t (cid:0)Hloss + pθ (cid:0)Hwin x, H<t (cid:1) ˆAwinθ log pθ (cid:1) + ˆAlossθ log pθ (cid:0)Hwin (cid:1) ˆAlossθ log pθ x, H<t (cid:0)Hloss x, H<t (cid:1) x, H<t = pθ The baseline Rbase could be estimated by probability-weighting as ) + 0 pθ(Hloss Rbase = 1 pθ(Hwin ). (cid:0)Hloss x, H<t (cid:1)(cid:105) (cid:1) . (24) (25) Here we simply use the difference between Rwin/loss and Rbaseline to express the advantage estimate as: ˆAwin = Rwin Rbase = 1 pθ(Hwin ) = pθ(Hloss ), ˆAloss = Rloss Rbase = pθ(Hwin ). (26) Combining Eq. 17, Eq. 18, Eq. 24, and Eq. 26, the gradient of intra-tree group policy optimization can be reformulated as θJIntra-tree(θ) = pθ (cid:0)Hwin x, H<t + pθ (cid:1) ˆAwinθ log pθ (cid:0)Hloss x, H<t x, H<t) pθ(Hloss (cid:0)Hwin (cid:1) ˆAlossθ log pθ x, H<t (cid:1) x, H<t) θ log pθ(Hwin (cid:0)Hloss x, H<t (cid:1) = pθ(Hwin x, H<t) pθ(Hwin pθ(Hloss (cid:2)θ log pθ(Hwin )pθ(Hloss ) (cid:125) (cid:124) (cid:123)(cid:122) Weight = pθ(Hwin (cid:124) x, H<t) x, H<t) θ log pθ(Hloss ) θ log pθ(Hloss )(cid:3) (cid:125) (cid:123)(cid:122) Preference Advantage Gradient (27) x, H<t) From Eq. 22 and Eq. 27, we can have the following Proposition C.1. Proposition C.1 (Structural Equivalence of step-level DPO and Intra-tree GRPO) Under Assumption C.1, both step-level DPO and intra-tree GRPO admit gradient estimators of the form (cid:0)θ log pθ (cid:124) θJunified(θ) = (cid:1) θ log pθ (cid:123)(cid:122) Preference Advantage Gradient (cid:0)Hloss (cid:0)Hwin (cid:124)(cid:123)(cid:122)(cid:125) Weight (28) (cid:1)(cid:1) t (cid:125) , where the only difference lies in the choice of the weight term w. ALGORITHM WORKFLOW OF TREE-GRPO The complete Tree-GRPO procedure is detailed in Algorithm 1."
        },
        {
            "title": "E CASE STUDY",
            "content": "In this section, we provide some cases of the model trained by Tree-GRPO. As illustrated by successful Case 8 and Case 9, the model can execute multi-turn agent tasks through iterative tool calls and autonomous information aggregation. For failed cases 10 and 11, the model selected single candidate solution path at the beginning from among several that only partially met the requirements and did not pursue further exploration. In subsequent reasoning, it neither reconsidered nor verified its choice in light of newly acquired information, resulting in an incorrect final answer. These findings motivate future work to integrate reflective reasoning and richer exploration into the training loop for complex, open-domain agents. THE USE OF LARGE LANGUAGE MODELS (LLMS) In this paper, LLMs are used to polish the content, adjust the format, write tables, and provide writing suggestions in some chapters. 20 Preprint. Algorithm 1: Tree-based Group Relative Policy Optimization Input: Policy model πθ, search environment e, dataset D, hyperparameters M, N, L, maximum action budget Procedure GenerateRollout(Hprefix) // This procedure completes trajectory from prefix Hprefix; budget of existing prefixHprefix; while < do Generate Ht πθ(x, H<t); + Ht; if yt = </search> then Parse last action Parse(y); Observation e(q); + <information>o</information>; else if Ht {</answer>, <eos>} then break; return H; /* Main algorithm logic starts here for step = 1 to Stotal do */ Reference model πref πθ; Training data x, Dstep; // Step 1: Generate independent chains for initial trees for = 1 to do Hinitial GenerateRollout(\"\"); Add Hinitial to Ti; for Expansion iter = 1 to do // Step 2: Sample nodes from each tree Ti for expansion Randomly sample expansion node from existing trees Psample = {p1, p2, ..., pM }; // Step 3: Do expansion foreach pi Psample do Hexpansion GenerateRollout(pi); Update trees with Hexpansion; for Updating iter = 1, ... do Calculate intra-tree group relative advantage ˆAintra-tree by Eq. 6; Calculate inter-tree group relative advantage ˆAinter-tree by Eq. 6; Update the policy model πθ by maximizing the Tree-GRPO objective (Eq. 8); 21 Preprint. Table 8: Tree-GRPO case study 1 (successful). Example from Multi-Hop QA Dataset Musique Question: Who is the owner of the record label of the performer of Groovy Little Summer Song? Ground Truth: Warner Music Group, Warner Music Model Output by Tree-GRPO (14B) <think> To answer this question, need to identify the performer of the song \"Groovy Little Summer Song\" and then find out who owns the record label that this performer is associated with. </think> <search> Performer of the song \"Groovy Little Summer Song\" </search> <information> Doc 1(Title: \"Groovy Little Summer Song\") Groovy Little Summer Song \"\"Groovy Little Summer Song\"\" is song co-written recorded by American country music artist James Otto. It was released in March 2010 as the first single from his album \"\"Shake What God Gave Ya\"\". The song was written by Otto ... Doc 2(Title: \"Emm Gryner\") is written about her. Gryner was married to visual artist Sean Odell from 2004-2016. They have two children together. Emm Gryner Emm Gryner (born 8 June 1975 in Sarnia, Ontario) is Canadian singer-songwriter best known for her 1998 indie hit \"\"Summerlong\"\". Gryners childhood ... Doc 3(Title: \"Emm Gryner\") Emm Gryner Emm Gryner (born 8 June 1975 in Sarnia, Ontario) is Canadian singersongwriter best known for her 1998 indie hit \"\"Summerlong\"\". Gryners childhood was spent in Forest, Lambton County, Ontario. Her father was of half Irish heritage and her mother was Filipina. Gryner ... </information> <think> From the search results, it seems that the performer of the song \"Groovy Little Summer Song\" is James Otto. Now, need to find out who owns the record label that James Otto is associated with. </think> <search> Record label of the performer James Otto </search> <information> Doc 1(Title: \"James Otto\") In June 2015, Otto signed with Blaster Records, and \"\"Somewhere Tonight\"\" was released to country radio on July 13, 2015 as his first single with the record label. James Otto James Allen Otto (born July 29, 1973) is an American country music artist who is ... Doc 2(Title: \"James Otto\") James Otto James Allen Otto (born July 29, 1973) is an American country music artist who is member of the MuzikMafia, group of country musicians known for their \"\"country music without prejudice\"\". Otto began his career on Mercury Nashville Records in 2002 ... Doc 3(Title: \"James Otto\") signed as an opening act for Shania Twain on her Up! Tour. His fourth single for Mercury, \"\"Sunday Morning and Saturday Night\"\", peaked at No. 58. After the song fell from the charts, Otto exited Mercurys roster. From there, he began working with the MuzikMafia ... </information> <think> From the search results, it seems that James Otto is associated with record labels such as Mercury Nashville Records and Warner Bros. Records. Now, need to find out who owns these record labels. </think> <search> Who owns Mercury Nashville Records and Warner Bros. Records </search> <information> Doc 1(Title: \"Warner Bros. Records\") Records from EMI as part of its sale to Universal Music Group. Most Parlophone artists (excluding Coldplay and Tinie Tempah, who were placed under Atlantic) were placed under Warner Bros ... Doc 2(Title: \"Warner Bros. Records\") Warner Bros. Records Warner Bros. Records Inc. is an American major record label owned by Warner Music Group and headquartered in Burbank, California. It was founded in 1958 as the ... Doc 3(Title: \"Mercury Records\") Mercury Records Mercury Records is an American record label owned by Universal Music Group. In the United States, it operates through Island Records; in the UK, it is distributed by Virgin EMI Records ... </information> <think> From the search results, it seems that Mercury Nashville Records is owned by Sony Music Entertainment and Warner Bros. Records is owned by Warner Music Group. </think> <answer> Warner Music Group </answer> 22 Preprint. Table 9: Tree-GRPO case study 2 (successful). Example from Web-Agent QA Dataset GAIA Question: What is the absolute difference in tens of thousands between the population of chinstrap penguins on the Wikipedia page for penguin species populations as of the end of 2018 and the population recorded in the Nature.com \"global population assessment of the Chinstrap penguin\" article from 2020, assuming two penguins per breeding pair? Ground Truth: Model Output by Tree-GRPO (14B) <think> The user wants to know the absolute difference in tens of thousands between the population of chinstrap penguins mentioned on the Wikipedia page for penguin species populations as of the end of 2018 and the population recorded in the Nature.com article from 2020, considering two penguins per breeding pair. To find the answer, we need to search for the specific population figures for chinstrap penguins in both sources and then calculate the difference. </think> <search> chinstrap penguin population in 2018 on Wikipedia </search> <information> Page 1: The population of chinstrap penguins was estimated by IUCN in August 2018 and it is reported that there is total of 8 million individual chinstrap penguins are living in the world. Page 2: According to the IUCN Red List, the total population size of the Chinstrap penguin is 8 million individuals or 4 million breeding pairs. Currently, this species is classified as Least Concern . . . Page 3: Jun 13, 2018 Most of the data analyses suggests single large population of chinstrap penguins throughout the Southern Ocean, with minimal population structure in the WAP region, and . . . Page 4: This is list of Sphenisciformes species by global population. While numbers are estimates, they have been made by the experts in their fields. Page 5: How many chinstrap penguins are there today? In 2018, the IUCN estimated about 8 million mature chinstrap penguins worldwide, though the population is decreasing. Page 6: Most of the data analyses suggests single large popula-tion of chinstrap penguins throughout the Southern Ocean, with minimal population structure in the WAP re-gion, and absence of . . . Page 7: BirdLife International has assessed 18 species. 16 (89% of total species) have had their population estimated: those missing are the king and little penguins, both of which have been assessed as being of least concern. [1][2] Page 8: Jun 13, 2018 High genetic diversity, evidence of female bias-dispersal and sign of population expansion after the last glacial maximum around 10,000 mya were detected. Page 9: Nov 10, 2020 Twenty-three previously known Chinstrap penguin colonies are found to be absent or extirpated ... </information> <think> From the Wikipedia page, the population of chinstrap penguins in 2018 was estimated to be 8 million. </think> <search> chinstrap penguin population in 2020 Nature.com </search> <information> Page 1: Nov 10, 2020 Using satellite imagery, drone imagery, and ground counts, we have assembled the first comprehensive global population assessment of Chinstrap penguins (Pygoscelis . . . Page 2: We estimate the global population of Chinstrap penguins at 3.42 (95thpercentile CI: [2.98, 4.00]) million breeding pairs  (Table 1)  in 375 extant breeding sites, not including recent... Page 3: Population Convey et al. (1999) estimate world population of 4 million breeding pairs. Page 4: Using satellite imagery, drone imagery, and ground counts, we have assembled the first comprehensive global population ... Page 5: The Antarctic Penguin Biogeography Project is an effort to collate all known information ... Page 6: Nov 10, 2020 Using satellite imagery, drone imagery, and ground counts, we have assembled the first comprehensive global population assessment of Chinstrap penguins (Pygoscelis antarctica) at 3.42... Page 7: Using satellite imagery, drone imagery, and ground counts, we have assembled the first comprehensive global population assessment of Chinstrap penguins (Pygoscelis antarctica) at 3.42... Page 8: Feb 11, 2020 Scientists discovered that colonies of chinstrap penguins also known as ringed or bearded penguins have dropped dramatically since they were last surveyed almost 50 years . . . </information> <think> The Nature.com article estimates the global population of chinstrap penguins in 2020 to be 3.42 million breeding pairs. Since the user mentioned \"two penguins per breeding pair\", the population would be approximately 3.42 million * 2 = 6.84 million chinstrap penguins. Now, we need to find the absolute difference between the 2018 population (8 million) and the 2020 population (6.84 million) in tens of thousands. The difference is 8,000,000 - 6,840,000 = 1,160,000. In tens of thousands, this is 116. </think> <answer> 116 </answer> 23 Preprint. Table 10: Tree-GRPO case study 3 (failed). Example from Multi-Hop QA Dataset Musique Question: Who is cast member of the show that contains the character Michael Knight? Ground Truth: Deanna Russo, Bruce Davison Model Output by Tree-GRPO (14B) <think> To answer this question, need to identify the show in which the character Michael Knight appears and then find out who is cast member of that show. </think> <search> Character Michael Knight in TV show </search> <information> Doc 1(Title: \"Michael Knight (Knight Rider)\") Michael Knight (Knight Rider) Michael Knight is fictional character and the protagonist of the 1980s television series \"\"Knight Rider\"\", played by David Hasselhoff. The character first appeared in the opening scenes as Michael Long, played by Larry Anderson in the beginning of the pilot. Born Michael Arthur Long on January 9, 1949 outside of Los Angeles, California, raised by blue-collar working family. In the late 1960s, while in his 20s, Long joined the Army, was part of the Green Berets, and spent three years in counter-intelligence work in Vietnam. He got captured at one point while on Doc 2(Title: \"Knight Rider (1982 TV series)\") Knight Rider (1982 TV series) Knight Rider is an American television series created and produced by Glen A. Larson. The series was originally broadcast on NBC from 1982 to 1986. The show stars David Hasselhoff as Michael Knight, high-tech modern crime fighter assisted by KITT, an advanced artificially intelligent, self-aware and nearly indestructible car. This was the last series Larson devised at Universal Television before he moved to 20th Century Fox. Self-made billionaire Wilton Knight rescues police Detective Lieutenant Michael Arthur Long after near fatal shot to the face, giving him new identity (by plastic surgery) and Doc 3(Title: \"Michael Knight (Knight Rider)\") Industries, an organization founded by Wilton Knight, billionaire philanthropist. Michael was not Wilton Knights first choice for the pilot program, but Wilton changed his mind and reopened the foundation due to Michaels arrival. In the context of the pilot, Wilton Knight (Richard Basehart) is dying of an undisclosed illness. Wilton picked up Michael passed out in front of his car, on desert highway. Michael barely survived, thanks to the metal plate. Given new face via facial reconstructive surgery, Long is resurrected as Michael Knight (now played by David Hasselhoff). Together with high-tech automobile called the Knight </information> <think> From the search results, it is clear that Michael Knight is character in the show \"Knight Rider\". Now, need to find out who is cast member of this show. </think> <search> Cast members of Knight Rider </search> <information> Doc 1(Title: \"Knight Rider\") Orci. On February 13, 2014, Schmoes Know reports that actors Chris Pratt and Danny McBride are in talks for roles and may use sort of actioncomedy hybrid in the same vein as \"\"21 Jump Street\"\". In December 2015, media report indicated that new movie named \"\"Knight Rider Heroes\"\" is in the works and will include David Hasselhoff. Knight Rider Knight Rider is an American entertainment franchise created by Glen A. Larson. The core of \"\"Knight Rider\"\" is its three television series: \"\"Knight Rider\"\" (1982-1986), \"\"Team Knight Rider\"\" (19971998), and \"\"Knight Rider\"\" (20082009). The franchise also includes three Doc 2(Title: \"Knight Rider\") to write the first script draft, with the series lead actor David Hasselhoff attached as an advisor and also have an onscreen role. In April 2003, Revolution Studios hired screenwriters David Elliott and Paul Lovett to pen the films script. In April 2004, the premise of the film was described as having Hasselhoff reprise his role as Michael Knight, now the mentor to the protagonist as Devon Miles mentored Knight in the television series. The protagonist would be Knights son, inheriting his fathers role and driving the vehicle KITT. The producers choice for the role was actor Ben Affleck. In Doc 3(Title: \"Knight Rider (2008 TV series)\") Knight Rider (2008 TV series) Knight Rider is 2008 series that follows the 1982 television series of the same title created by Glen A. Larson and the 2008 television movie. The series stars Justin Bruening as Mike Traceur, the estranged son of Michael Knight. The series also stars Deanna Russo as Sarah Graiman, Traceurs former girlfriend and love interest. Sarah is the daughter of Charles Graiman. Graiman, played by Bruce Davison, is the creator of new generation of KITT (Knight Industries Three Thousand), which is voiced by Val Kilmer. The series was in production for just one season. </information> <think> From the search results, it is clear that David Hasselhoff is cast member of the show \"Knight Rider\". </think> <answer> David Hasselhoff </answer> 24 Preprint. Table 11: Tree-GRPO case study 4 (failed). Example from Web-Agent QA Dataset WebWalkerQA Question: As of December 2023, this individual was announced as the football teams coach as per an article published between 2019 and 2021 inclusive. The football team initially started as social football club in 1995 before changing its name in 2011 as per an article published that began as social club as per an article published between 2019 and 2021 inclusive. The football club won title in its home country league for the fourth successive time in row as reported in an article published between 2020 and 2023 inclusive. This league win was this individuals third with the football club and the fourth overall as reported in an article published between 2020 and 2023 inclusive. The individual underwent surgery for back injury as per an article published in 2018. Tell me the full name of this individual. Ground Truth: Norman Takanyariwa Mapeza Model Output by Tree-GRPO (14B) <think> To find the full name of the individual, need to identify the football team and its league in their home country. Then, will look for articles about this team between 2020 and 2023 that mention coach who won title for the fourth consecutive time and is the third time for this coach. Additionally, should look for an article about the coachs surgery in 2018. The coachs name will likely be mentioned in these articles. </think> <search> football team social club 1995 </search> <information> Page 1: 1995 Stadium Club Football checklist, gallery, release date, inserts, parallels, forum, and collection tracking Page 2: 1995 Stadium Club card list & price guide. Ungraded & graded values for all 1995-96 Topps Stadium Club Football Cards ... Page 3: Get the best deals on 1995 Topps Stadium Club Football Trading Cards when you shop the largest online selection at eBay.com ... Page 4: wide selection of single cards from the 1995 Stadium Club football card set. Page 5: Shop COMCs extensive selection of 1995 topps stadium club members only football cards ... Page 6: Get the latest and most accurate trading card values and sets info for 1995 Stadium Club Football sports cards ... Page 7: 1995 Stadium Club Teams Arizona Cardinals Atlanta Falcons Buffalo Bills Carolina Panthers Chicago Bears ... </information> <think> The search results do not provide enough information to identify the football team or the individual. need to narrow down my search query ... </think> <search> football team started as social club 1995, changed name 2011, coach won league title four consecutive times between 2020 and 2023 </search> <information> Page 1: The Crossword Solver found 30 answers to \"football club changed name 2011 league title ... Page 2: May 27, 2016 Leicester became the sixth club to win the Premier League, completing ... Page 3: 18 hours ago Live updating Football Club Started As Social Club 1995 Renamed 2011 ... Page 4: List of football league clubs in the top four tiers of English football (i.e. the ... Page 5: May 22, 2024 Manchester City saw off West Ham United on the final 2023/24 ... Page 6: Aug 29, 2023 Manchester City wrapped up the 2022/23 season with an incredible ... </information> <think> From the search results, learned that Manchester City won four consecutive domestic league titles between 2020 and 2023. need to find articles between 2020 and 2023 that mention the name of the coach of Manchester City who won the league titles. Additionally, should look for an article about the coachs surgery in 2018. </think> <search> manchester city coach won league title four consecutive times between 2020 and 2023, coach surgery 2018 </search> <information> Page 1: May 19, 2024 Manchester City won an unprecedented fourth consecutive Premier League title on Sunday as Phil ... Page 2: May 19, 2024 Pep Guardiola and Manchester City made history on Sunday, when 3-1 win over West Ham ... Page 3: May 20, 2024 But Guardiola now stands alone as the only boss to manage quality quartet of title ... Page 4: May 21, 2024 Pep Guardiola was announced as the winner of the Premier League Manager of the Season ... Page 5: May 19, 2024 Manchester City have won their fourth consecutive Premier League title, sealing yet ... Page 6: May 19, 2024 Some wondered whether Manchester City would retain their hunger after winning the ... Page 7: May 19, 2024 Taking in spells at Barcelona and Bayern Munich, Guardiola has now won the league ... </information> <think> Based on the search results, the full name of the individual who was announced as the football teams coach is Pep Guardiola. </think> <answer> Pep Guardiola </answer>"
        }
    ],
    "affiliations": [
        "AMAP, Alibaba Group",
        "Southern University of Science and Technology",
        "Xiamen University"
    ]
}