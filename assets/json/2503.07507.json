{
    "paper_title": "PE3R: Perception-Efficient 3D Reconstruction",
    "authors": [
        "Jie Hu",
        "Shizun Wang",
        "Xinchao Wang"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Recent advancements in 2D-to-3D perception have significantly improved the understanding of 3D scenes from 2D images. However, existing methods face critical challenges, including limited generalization across scenes, suboptimal perception accuracy, and slow reconstruction speeds. To address these limitations, we propose Perception-Efficient 3D Reconstruction (PE3R), a novel framework designed to enhance both accuracy and efficiency. PE3R employs a feed-forward architecture to enable rapid 3D semantic field reconstruction. The framework demonstrates robust zero-shot generalization across diverse scenes and objects while significantly improving reconstruction speed. Extensive experiments on 2D-to-3D open-vocabulary segmentation and 3D reconstruction validate the effectiveness and versatility of PE3R. The framework achieves a minimum 9-fold speedup in 3D semantic field reconstruction, along with substantial gains in perception accuracy and reconstruction precision, setting new benchmarks in the field. The code is publicly available at: https://github.com/hujiecpp/PE3R."
        },
        {
            "title": "Start",
            "content": "PE3R: Perception-Efficient 3D Reconstruction Jie Hu 1 Shizun Wang 1 Xinchao Wang 1 5 2 0 2 0 1 ] . [ 1 7 0 5 7 0 . 3 0 5 2 : r Figure 1: Visualizations for Perception-Efficient 3D Reconstruction. PE3R reconstructs 3D scenes using only 2D images and enables semantic understanding through language. The framework achieves efficiency in two key aspects. First, input efficiency allows it to operate solely with 2D images, eliminating the need for additional 3D data such as camera parameters or depth information. Second, time efficiency ensures significantly faster 3D semantic reconstruction compared to previous methods. These capabilities make PE3R highly suitable for scenarios where obtaining 3D data is challenging and for applications requiring large-scale or real-time processing."
        },
        {
            "title": "Abstract",
            "content": "Recent advancements in 2D-to-3D perception have significantly improved the understanding of 3D scenes from 2D images. However, existing methods face critical challenges, including limited generalization across scenes, suboptimal perception accuracy, and slow reconstruction speeds. To address these limitations, we propose Perception-Efficient 3D Reconstruction (PE3R), novel framework designed to enhance both accuracy and efficiency. PE3R employs feedforward architecture to enable rapid 3D semantic field reconstruction. The framework demonstrates robust zero-shot generalization across diverse scenes and objects while significantly improving reconstruction speed. Extensive experiments on 2D-to-3D open-vocabulary segmentation and 3D reconstruction validate the effectiveness and versatility of PE3R. The framework 1xML Lab, National University of Singapore. achieves minimum 9-fold speedup in 3D semantic field reconstruction, along with substantial gains in perception accuracy and reconstruction precision, setting new benchmarks in the field. The code is publicly available at: https: //github.com/hujiecpp/PE3R. 1. Introduction Machine vision systems have made significant strides in 2D perception tasks, particularly with single-view images. However, humans perceive the world by integrating information from multiple viewpoints (Ayzenberg & Behrmann, 2024; Sinha & Poggio, 1996; Welchman et al., 2005). This raises fundamental question in machine learning: How can advanced 2D perception models be enhanced to achieve comprehensive understanding of 3D scenes without relying on explicit 3D information? Addressing this question has become pivotal research focus. Recent advancements in 2D-to-3D perception provide foundation for reconstructing and interpreting 3D scenes using 2D inputs (Goel et al., 2023; Kobayashi et al., 2022; Tang et al., 2023; Tschernezki et al., 2022; Peng et al., 2023; Takmaz et al., 2023; Kerr et al., 2023; Cen et al., 2023; Liu et al., 2024; Hu et al., 2024; Ye et al., 2023; Zhou et al., 2024; Qin et al., 2024; Cen et al., 2024). These methods enable models to reconstruct 3D environments from multiple 2D images captured from different viewpoints. Despite significant progress, existing approaches face key limitations, including poor generalization across diverse scenes, suboptimal perception accuracy, and slow reconstruction speeds. State-of-the-art methods, primarily based on Neural Radiance Fields (NeRF) (Mildenhall et al., 2021) and 3D Gaussian Splatting (3DGS) (Kerbl et al., 2023), achieve impressive reconstruction quality. However, their reliance on scene-specific training and semantic extraction introduces significant computational overhead, limiting scalability for real-world applications. To address these challenges, we propose PerceptionEfficient 3D Reconstruction (PE3R), novel framework designed for efficient and accurate 3D semantic reconstruction. Inspired by recent advancements in efficient 3D scene reconstruction (Wang et al., 2024; Leroy et al., 2024), PE3R employs feed-forward mechanism to enable rapid 3D semantic reconstruction. The framework incorporates three key modules to enhance perception and reconstruction capabilities: pixel embedding disambiguation, semantic field reconstruction, and global view perception. Pixel embedding disambiguation integrates cross-view, multi-level semantic information to resolve ambiguities across hierarchical objects and ensure viewpoint consistency. Semantic field reconstruction embeds semantic information directly into the reconstruction process, improving accuracy. Global view perception aligns global semantics, mitigating noise introduced by single-view perspectives. As illustrated in Figure 1, these innovations enable robust and efficient zero-shot generalization across diverse scenes and objects. We evaluate PE3R on tasks including 2D-to-3D openvocabulary segmentation and 3D reconstruction, using datasets such as Mipnerf360 (Barron et al., 2022), Replica (Straub et al., 2019), KITTI (Geiger et al., 2013), ScanNet (Dai et al., 2017), ScanNet++(Yeshwanth et al., 2023), ETH3D(Schops et al., 2017), DTU (Aan√¶s et al., 2016), and Tanks and Temples (Knapitsch et al., 2017). Our results demonstrate 9-fold improvement in reconstruction speed, and also the enhancement in segmentation accuracy as well as reconstruction precision, establishing new performance benchmarks. The contributions of this work are as follows: We propose PE3R, an efficient feed-forward framework for 2D-to-3D semantic reconstruction. We introduce three novel modules, pixel embedding disambiguation, semantic field reconstruction, and global view perception, that enable better reconstruction speed and accuracy. We validate PE3R through extensive experiments, demonstrating robust zero-shot generalization, improved performance metrics, and practical scalability. The code will be publicly available to promote reproducibility and future research. 2. Related Work 2D-to-3D Reconstruction. Recent advancements in 2D image-based 3D reconstruction have significantly improved surface reconstruction. Methods such as (Fu et al., 2022; Guo et al., 2023; Long et al., 2022; Wang et al., 2021; 2022) employ signed distance functions (SDFs) to represent surfaces, combining them with advanced volume rendering techniques to achieve higher accuracy. Neural Radiance Fields (NeRF) (Mildenhall et al., 2021) have demonstrated exceptional performance in generating realistic novel viewpoints for view synthesis. Extending this, 3D Gaussian Splatting (3DGS) (Kerbl et al., 2023) introduces explicit 3D scene representations to reduce the computational complexity of NeRFs implicit reconstruction. DUSt3R (Wang et al., 2024) proposes novel approach for dense and unconstrained 3D reconstruction from arbitrary image collections, eliminating dependencies on scene-specific training, camera calibration, or viewpoint poses. MASt3R (Leroy et al., 2024) enhances DUSt3R by integrating image keypoint matching into the reconstruction pipeline, further improving performance. Our proposed PE3R advances this field by incorporating semantic information into the 3D reconstruction process, enabling more precise and context-aware reconstructions. 2D-to-3D Perception. Inspired by advancements in NeRF (Mildenhall et al., 2021) and 3DGS (Kerbl et al., 2023), researchers have integrated 2D-to-3D segmentation into these frameworks. For instance, GNeRF (Chen et al., 2023) and InPlace (Zhi et al., 2021) incorporate 2D semantic masks into NeRF, using 2D semantic supervision to enhance 3D segmentation during training. Both supervised (Liu et al., 2023b; Bhalgat et al., 2023; Siddiqui et al., 2023) and unsupervised methods (Niemeyer & Geiger, 2021; Yu et al., 2021) have been developed for 3D instance segmentation using NeRF. SA3D (Cen et al., 2023), built on SAM (Kirillov et al., 2023), introduces an automated cross-view prompt collection strategy to guide 3D feature learning. Similarly, Feature3DGS (Zhou et al., 2024) maps features from SAMs encoder into 3D space and employs its decoder to generate segmentation masks. Mask-liftingbased methods project 2D segmentation masks from SAM directly into 3D space. Examples include SAGA (Cen et al., 2024), Gaussian Grouping (Ye et al., 2023), SAGS (Hu et al., 2024), Click-Gaussian (Choi et al., 2024), and FlashSplat (Shen et al., 2024). Recently, large spatial model (Fan et al., 2024) directly converts unposed images into semantic 3D representations. Although these methods reduce time 2 Figure 2: PE3R Framework. In pixel embedding disambiguation, foundational segmentation model (e.g., SAM) segments the input image into multi-level masks. tracking model (e.g., SAM2) then assigns consistent labels to these masks across different views. The image regions filtered by these masks are encoded using an image encoder (e.g., CLIP), aggregated through area-moving, and mapped back to generate pixel embeddings. For semantic field reconstruction, feed-forward model (e.g., DUSt3R) predicts pointmaps. These pointmaps are combined with pixel embeddings through semantic-guided refinement to produce refined 3D semantic field. In global view perception, text embeddings generated by text encoder (e.g., CLIP) are matched with 3D point embeddings to locate semantic targets via global similarity normalization. and computational costs for feature-to-mask conversions, they remain limited by slow reconstruction processes. Our proposed PE3R achieves feed-forward segmentation without requiring scene-specific pretraining, offering more efficient and generalizable solution. 2D Foundational Perception Models. Foundational models are trained on extensive datasets, contain numerous parameters, and exhibit versatility across diverse downstream tasks. Contrastive Language-Image Pre-training (CLIP) (Radford et al., 2021) aligns images and text by training on large dataset of image-text pairs. Sigmoid Loss for Language-Image Pre-Training (SigLIP) (Zhai et al., 2023) introduces simple pairwise sigmoid loss function for language-image pretraining. Unlike traditional contrastive learning methods that use softmax normalization, SigLIP operates exclusively on image-text pairs, eliminating the need for global normalization. Segment Anything Model (SAM) (Kirillov et al., 2023) is pioneering model that segments any object in an image using visual and textual cues, enhancing its adaptability for various tasks. Its successor, SAM2 (Ravi et al., 2024), incorporates advanced pretraining techniques and more robust architecture, improving performance in complex scenes and reducing annotation dependency. DINOv2 (Oquab et al., 2023) leverages selfsupervised learning by combining contrastive learning and distillation to produce robust visual representations. Grounding DINO (Liu et al., 2023a) extends the Transformer-based DINO detector by introducing grounded pretraining for open-set object detection. Our proposed PE3R bridges the gap between 2D foundational models and 3D applications, enabling 3D perception without retraining or additional 3D data, thus providing seamless and efficient solution. 3. Method 3.1. Problem Formulation In real-world scenarios, images are frequently captured from multiple viewpoints without 3D metadata, such as camera parameters or depth maps. This lack of explicit 3D information poses significant challenges in constructing accurate and semantically rich 3D representations. To address this, we aim to reconstruct 3D semantic field from 2D images, capturing both the scenes geometry and its semantic understanding. Our proposed method operates efficiently without relying on 3D annotations or pre-calibrated camera parameters. Furthermore, the reconstructed semantic field must support text-based queries to locate and identify semantic objects within the scene, enabling seamless integration of 3D understanding with natural language interaction. For example, query such as black chair should allow the system to recognize and highlight the corresponding object in the reconstructed scene. PE3R Framework. The PE3R framework, illustrated in Figure 2, consists of three stages aimed at achieving accurate and efficient 3D semantic reconstruction. The process begins with pixel embedding disambiguation, which resolves semantic ambiguities caused by varying levels and 3 viewpoints. This step ensures precise semantic assignment for each pixel, establishing robust foundation for 3D semantic reconstruction. Subsequently, semantic field reconstruction integrates semantic information into the reconstruction pipeline, generating spatial semantic field that improves the precision and fidelity of the reconstructed scene. Finally, global view perception incorporates holistic perspective, enabling comprehensive understanding of the scene. This stage facilitates intuitive, text-based interactions and enhances the semantic interpretation of the 3D environment. The following sections detail each stage, elucidating their collaborative role in fulfilling the objectives of the PE3R framework. 3.2. Pixel Embedding Disambiguation To construct 3D semantic fields, we generate semantic embeddings for each pixel in the input multi-view images. This process encounters challenges arising from semantic ambiguities at both object and perspective levels. At the object level, single pixel may simultaneously belong to multiple objects. For example, pixel from donut placed on box could represent either the donut or the box. Additionally, occlusions and perspective changes introduce inconsistencies, leading to varying semantic interpretations of the same pixel across different viewpoints. To resolve these challenges, we develop pixel embeddings that are both object-distinguishable and view-consistent. Image Embedding Extraction. We consider set of images captured from distinct perspectives, denoted as X1, X2, . . . , Xn R3HW , where and represent the height and width of the images, respectively. First, we apply foundational segmentation models, e.g., SAM1 (Kirillov et al., 2023) and SAM2 (Ravi et al., 2024), to segment and track objects across all perspectives. This process generates multi-level masks and ensures consistent object indexing across viewpoints for coherence. Subsequently, objects are extracted from the masks, yielding M1, M2, . . . , Mn Rm3HW , where denotes the total number of masks across all perspectives. Next, an image encoder Fimg(), e.g., CLIP (Radford et al., 2021), encodes the masked images and extracts image embeddings as follows: where F1, F2, . . . , Fn Rmd, represents the embedding dimension, and () denotes L2 normalization. Area-Moving Aggregation. To address semantic ambiguity and ensure viewpoint consistency, we aggregate multilevel cross-view image embeddings from the previous step. Smaller objects, which occupy less area in an image, are more prone to losing semantic information during encoding (Radford et al., 2021). To mitigate this issue, we incorporate object area as critical factor in the aggregation process. The aggregated embeddings must satisfy two conditions: (1) vector normalization (embeddings remain in the same semantic space as pre-aggregation), and (2) semantic vectorization (aggregation effectively integrates semantic information). To meet these requirements, we construct spherical unit vectors for aggregation. For two masks and with embeddings FA and FB, respectively, the aggregation is defined as: ÀÜFB = aFA + bFB, = sin((1 t)Œ∏) sin(Œ∏) , = sin(tŒ∏) sin(Œ∏) , (2) where Œ∏ is the angle between FA and FB, and is an interpolation parameter determined by the area ratio of the two masks, i.e., = areaB/(areaA + areaB). Proposition 3.1. Vector Normalization: For any unit vectors FA and FB, ÀÜFB remains unit vector, ensuring it lies within the same semantic space as FA and FB. Proof. The norm of ÀÜFB is given by: ÀÜFB2 = aFA + bFB2 . Expanding this expression: ÀÜFB2 = (aFA + bFB) (aFA + bFB) = a2FA2 + 2abFA FB + b2FB2. Since FA and FB are unit vectors: (3) (4) FA = 1, FB = 1, FA FB = cos(Œ∏). (5) Substituting these values, we get: ÀÜFB2 = 1 sin2(Œ∏) (sin2((1 t)Œ∏) + sin2(tŒ∏) + 2 sin((1 t)Œ∏) sin(tŒ∏) cos(Œ∏)). Using trigonometric identities: sin2(Œ∏) = sin2((1 t)Œ∏) + sin2(tŒ∏) + 2 sin((1 t)Œ∏) sin(tŒ∏) cos(Œ∏), (6) (7) (8) ÀÜFB2 = sin2(Œ∏)/ sin2(Œ∏) = 1. Thus, ÀÜFB is confirmed to be unit vector. Proposition 3.2. Semantic Vectorization: For any vector FC satisfying FA FC > FB FC, it follows that ÀÜFB FC > FB FC. This ensures that ÀÜFB integrates the semantic information of both FA and FB. 4 F1, F2, ..., Fn = (Fimg(M1, M2, ..., Mn)), (1) we find that: Table 1: 2D-to-3D Open-Vocabulary Segmentation on small datasets, i.e., Mipnerf360 (Mip.) and Replica (Rep.). Dataset Method mIoU mPA mP Mip. Rep. 0.2698 0.8183 0.6553 LERF (Kerr et al., 2023) 0.3889 0.8279 0.7085 F-3DGS (Zhou et al., 2024) GS Grouping (Ye et al., 2023) 0.4410 0.7586 0.7611 0.5545 0.8071 0.8600 LangSplat (Qin et al., 2024) 0.8646 0.9569 0.9362 GOI (Qu et al., 2024) PE3R, ours 0.8951 0.9617 0. 0.2815 0.7071 0.6602 LERF (Kerr et al., 2023) F-3DGS (Zhou et al., 2024) 0.4480 0.7901 0.7310 GS Grouping (Ye et al., 2023) 0.4170 0.7370 0.7276 0.4703 0.7694 0.7604 LangSplat (Qin et al., 2024) 0.6169 0.8367 0.8088 GOI (Qu et al., 2024) PE3R, ours 0.6531 0.8377 0.8444 Proof. The cosine similarity between ÀÜFB and FC is: ÀÜFB FC = a(FA FC) + b(FB FC). (9) Since FA FC > FB FC, we have: a(FA FC) + b(FB FC) > (a + b)(FB FC). (10) Using trigonometric properties: + = sin((1 t)Œ∏) sin(Œ∏) + sin(tŒ∏) sin(Œ∏) = 1, (11) we conclude: ÀÜFB FC = a(FA FC) + b(FB FC) > FB FC. (12) This confirms that ÀÜFB combines the semantic information of FA and FB. Pixel Embedding Ensemble. To resolve ambiguity, we first sort the masks by area in descending order. We then apply multi-level area-moving aggregation to the embeddings of smaller masks. To ensure cross-view consistency, we align the embeddings of the same semantic object across different views using mask indices. This iterative process generates the final embeddings. Finally, we sequentially assign the image embeddings to the corresponding pixels of the original image, starting with larger masks and progressing to smaller ones. This results in pixel embeddings E1, E2, . . . , En RHW d, where each pixel is assigned semantic embedding. 3.3. Semantic Field Reconstruction Inspired by recent advances in feed-forward 3D pointmap prediction (Wang et al., 2024; Leroy et al., 2024), we employ this efficient approach to construct 3D semantic fields. Given multi-view images, feed-forward predictors, e.g., Table 2: Running Speed comparison on Mipnerf360. Method Preprocess Training Total LERF (Kerr et al., 2023) F-3DGS (Zhou et al., 2024) GS Grouping (Ye et al., 2023) LangSplat (Qin et al., 2024) GOI (Qu et al., 2024) PE3R, ours 3mins 25mins 27mins 50mins 8mins 5mins 40mins 43mins 623mins 648mins 138mins 165mins 99mins 149mins 45mins 37mins 5mins - Table 3: 2D-to-3D Open-Vocabulary Segmentation on the large-scale dataset, i.e., ScanNet++. Method mIoU mPA mP LERF (Kerr et al., 2023) Features 0.1824 0.6024 0.5873 0.2101 0.6216 0.6013 0.2248 0.6542 0.6315 GOI (Qu et al., 2024) Features PE3R, ours DUSt3R (Wang et al., 2024), estimate spatial coordinates (pointmaps) for each pixel in every view: P1, P2, . . . , Pn = Fpts(X1, X2, ..., Xn), (13) where P1, P2, . . . , Pn RHW 3 denote the spatial coordinates (x, y, z). However, in practice, these predictions often contain noise due to reflections, transparency, and occlusions. To mitigate these challenges, we integrate semantic information to refine the noisy pointmaps, thereby enhancing the accuracy of 3D reconstruction. Anomaly Point Detection. Adjacent pixels within the same semantic category generally show small spatial distance variations. This property enables the identification of noise points in point maps. For given point Pi,j, the average distance to its k-neighborhood pixels in 3D space can be computed as: Li,j = (cid:80) dx,dy I(Mi,j, Mi+dx,j+dy)D(Pi,j, Pi+dx,j+dy) dx,dy I(Mi,j, Mi+dx,j+dy) (cid:80) , (14) where dx, dy [k/2, +k/2], I(, ) checks if two pixels share the same semantic label, and D(, ) represents the L2 distance in 3D space. By applying sliding window across the image, semantic pixel distance averages are calculated for all pixels, producing L1, L2, . . . , Ln RHW . After normalization, anomaly points are filtered using predefined distance threshold. Semantic-Guided Refinement. To address spatial anomalies, we refine point maps by incorporating semantic information. Traditional methods often rely on Least Squares fitting to adjust out-of-distribution points, but this approach is computationally expensive due to the complexity of semantic shapes in the scene. Our analysis reveals that noise points primarily originate from non-smooth input images 5 Table 4: Multi-View Depth Evaluation. The settings are: (a) classical approaches, (b) with known poses and depth range, but without alignment, (c) absolute scale evaluation using poses, but without depth range or alignment, (d) without poses and depth range, but with alignment, and (e) feed-forward architectures that does not use any 3D information. Methods KITTI rel œÑ ScanNet rel œÑ ETH3D rel œÑ (a) COLMAP (Schonberger & Frahm, 2016) COLMAP Dense (Schonberger et al., 2016) 12.0 58.2 26.9 52.7 (b) MVSNet (Yao et al., 2018) MVSNet Inv. Depth (Yao et al., 2018) Vis-MVSSNet (Zhang et al., 2023b) MVS2D ScanNet (Yang et al., 2022) MVS2D DTU (Yang et al., 2022) 22.7 36.1 18.6 30.7 55.4 9.5 21.2 8.7 226.6 0.7 14.6 38.0 24.6 22.7 8.9 27.2 32.3 34.2 22.5 20.4 20.9 33.5 5.3 11. (c) DeMon (Ummenhofer et al., 2017) DeepV2D KITTI (Teed & Deng, 2018) DeepV2D ScanNet (Teed & Deng, 2018) MVSNet (Yao et al., 2018) MVSNet Inv. Depth (Yao et al., 2018) Vis-MVSNet (Zhang et al., 2023b) MVS2D ScanNet (Yang et al., 2022) MVS2D DTU (Yang et al., 2022) Robust MVD (Schroppel et al., 2022) (d) DeMoN (Ummenhofer et al., 2017) DeepV2D KITTI (Teed & Deng, 2018) DeepV2D ScanNet (Teed & Deng, 2018) (e) DUSt3R (Wang et al., 2024) DUSt3R (Wang et al., 2024), our imp. MASt3R (Leroy et al., 2024), our imp. PE3R, ours 75.0 25.8 3.8 0.0 16.7 13.4 8.1 20.4 16.3 60.2 61.9 5.2 14.0 35.8 1568.0 5.7 28.5 29.6 8.1 10.3 54.4 15.6 54.1 0.0 73.4 1.6 0.0 93.3 7.1 38.4 41.9 65.2 84.9 4.5 51.5 7.4 15.5 15.2 3.1 74.9 10.0 36.2 9.1 39.5 11.0 33.2 5.4 36.9 48.6 9.4 12.0 23.7 4. 4.9 4.8 22.0 5.5 21.0 11.1 54.8 60.2 60.3 9.6 55.1 16.4 55.1 89.8 23.2 35.4 31.4 21.6 35.6 10.8 43.3 27.4 4.8 99.0 11.6 19.0 16.2 30.1 9.4 18.7 28.7 507.7 8.3 60.3 5.8 51.5 17.4 30.7 14.4 0.0 78.0 42.6 9. 17.4 15.4 27.1 10.1 11.8 29.3 2.9 3.1 27.9 2.3 76.9 74.5 9.9 82.0 DTU T&T Ave. rel œÑ rel œÑ 0.7 20.8 1.8 1.8 1.8 17.2 3. 96.5 69.3 86.0 86.7 87.4 9.8 64.2 2.7 95.0 25.7 76.4 73.0 8.3 74.6 6.5 87.2 4.1 29.2 4.4 25.8 28.0 rel 9.3 40. 18.6 14.2 7.0 24.4 77.5 œÑ 67.8 48.8 49.4 49.7 61.4 6.6 23.1 23.7 24.6 9.2 11.5 8.2 27.4 4429.1 0.1 48.9 1.7 57.9 92.3 82. 28.7 374.2 5.0 1.6 2.7 30.4 27.9 25.4 11.9 17.6 18.3 10.3 38.5 9.6 33.5 38.0 31.9 118.2 50.7 1327.4 20.1 47.0 51.4 14.6 21.2 108.4 31.0 21.1 65.6 27.5 34.0 56.4 11.1 18.8 62.4 0.0 87.5 56.0 6.3 75.1 5.0 21.8 24.8 7.7 3.5 2.7 13.6 3.2 16.6 8.1 33. 69.3 75.7 13.7 69.1 13.0 23.2 9.1 34.1 46.4 8.9 76.7 3.2 78.5 2.9 22.1 14.6 85.3 2.1 16.0 22.6 8.6 4.7 4.9 24.5 4.5 18.3 22.7 39. 64.5 64.4 10.6 68.0 processed by the point map predictor. To resolve this, we smooth the input images using their semantic masks. For each anomaly point, we replace its RGB value with the mean RGB value of its corresponding semantic mask. The smoothed image is then fed into the point map predictor to generate refined, semantic-guided point maps. Finally, we perform global alignment to synchronize spatial pixels. 3.4. Global View Perception For global view perception, we process point embeddings from all viewpoints alongside given text query. First, the input text is encoded using text encoder Ftxt(), e.g., CLIP (Radford et al., 2021), to generate its corresponding embedding: = Ftxt(text), (15) where Rd represents the text embeddings. Next, we compute the cosine similarity between the text embedding and the point embeddings E1, E2, . . . , En: [S1, S2, . . . , Sn] = D(T, [E1, E2, . . . , En]), (16) where D() denotes the similarity computation. We then globally normalize the similarity scores using min-max normalization. Finally, similarity threshold is applied to identify points within P1, P2, . . . , Pn that share semantic similarity with the input text. 4. Experiments 4.1. Experimental Details Datasets. We conduct 2D-to-3D open-vocabulary segmentation experiments using the Mipnerf360 dataset (Barron et al., 2022) extended with open-vocabulary capabilities (Qu et al., 2024) and the Replica dataset (Straub et al., 2019). To evaluate large-scale generalization, we further test our model on the ScanNet++ dataset (Yeshwanth et al., 2023). For 3D reconstruction experiments, we employ multi-view depth estimation datasets, including KITTI (Geiger et al., 2013), ScanNet (Dai et al., 2017), DTU (Aan√¶s et al., 2016), ETH3D (Schops et al., 2017), and Tanks and Temples (T&T) (Knapitsch et al., 2017). These datasets provide diverse indoor and outdoor scenes for comprehensive evaluation. Pre-trained Models. PE3R integrates multiple pre-trained models. For segmentation, we use MobileSAMv2 (Zhang et al., 2023a), lightweight version of SAM. For object 6 Table 5: Ablation Studies for 2D-to-3D open-vocabulary segmentation on ScanNet++ dataset. Method mIoU mPA mP PE3R, w/o Multi-Level Disam. PE3R, w/o Cross-View Disam. 0.1624 0.5892 0.5623 0.1895 0.6012 0.5923 PE3R, w/o Global MinMax Norm. 0.2035 0.6253 0.6186 0.2248 0.6542 0.6315 PE3R Figure 4: Ablation Studies on Cross-View Disambiguation. Without cross-view disambiguation, semantic inconsistencies arise due to the challenges posed by varying viewing angles and occlusions. Figure 3: Ablation Studies on Multi-Level Disambiguation. Without the use of multi-level disambiguation, the model is able to identify parts of objects but faces challenges in accurately localizing the semantics of entire objects. tracking, we employ SAM2 (Ravi et al., 2024), and for feedforward prediction, we utilize the DUSt3R (Wang et al., 2024) models. Evaluation Metrics. We evaluate 2D-to-3D openvocabulary segmentation using mean Intersection over Union (mIoU), mean Pixel Accuracy (mPA), and mean Precision (mP). Additionally, we record training time to evaluate model efficiency. Training time is also recorded to assess model efficiency. For multi-view depth estimation, we report Absolute Relative Error (rel) and Inlier Ratio (œÑ ), with threshold of 1.03. These metrics are provided for individual test sets and as averages across all datasets. 4.2. Main Results 2D-to-3D Open-Vocabulary Segmentation. In our experiments on 2D-to-3D open-vocabulary segmentation, we first evaluate our method on the smaller datasets, Mipnerf360 and Replica. These datasets are relatively small, making Figure 5: Ablation Studies for PE3R with or without Global Min-Max Normalization. The absence of global min-max normalization leads to noise in the results. them manageable for all current methods, including those based on NeRF and 3DGS. As shown in Table 1, our method, PE3R, outperforms the state-of-the-art GOI across all metrics, i.e., mIoU, mPA, and mP, on both datasets. We also measure the running time for constructing semantic fields, as detailed in Table 2. The fastest existing methods, LERF and GOI, take 43 and 45 minutes, respectively, to construct the 3D semantic field. In contrast, our method completes the process in just about 5 minutes, making it roughly nine times faster. To further evaluate the scene generalization capability of our approach, we conduct experiments on the large-scale ScanNet++ dataset. Due to limitations in running speed and 3D information, we use the semantic features from LERF and GOI on 2D images as baselines. As shown in Table 3, our proposed method, PE3R, demonstrates competitive performance, confirming its effectiveness. Finally, in Figure 1, the visualization experiments illustrate the strengths of our method. The results clearly showcase its robust generalization across diverse scenes, including indoor, outdoor, and natural environments, while effectively handling wide range of object semantics. Table 6: Ablation Studies on semantic field reconstruction. Method rel œÑ Run Time PE3R, w/o Semantic Field Rec. PE3R 5.3 4.5 60.2 68. 10.4021s 11.1934s Figure 6: Ablation Studies on semantic field reconstruction. Without semantic field reconstruction, the object reconstructions are more susceptible to the inclusion of outlier points. Multi-view Depth Estimation. We evaluate 3D reconstruction performance in the context of multi-view depth estimation. Table 4 compares five different types of algorithms: (a) classical methods, (b) approaches that use poses and depth range but lack alignment, (c) methods that evaluate absolute scale with poses but without depth range or alignment, (d) techniques that do not use poses or depth range but incorporate alignment, and (e) feed-forward architectures that do not rely on 3D information. Our experiments show that PE3R outperforms the baseline methods, DUSt3R and MASt3R, on most scene datasets, achieving the highest average performance. 4.3. Ablation Studies To demonstrate the effectiveness of the proposed modules, we conduct ablation studies on both segmentation and depth estimation tasks. For 2D-to-3D open-vocabulary segmentation, we evaluate the contributions of multi-level disambiguation, cross-view disambiguation, and global min-max normalization. In the case of multi-view depth estimation, we analyze the impact of semantic field reconstruction. 2D-to-3D Open-Vocabulary Segmentation. Table 5 presents the segmentation performance without multi-level disambiguation, cross-view disambiguation, and global minmax normalization. The results highlight the importance of multi-level disambiguation for overall performance; without it, performance drops significantly. Both cross-view disam8 biguation and global min-max normalization also contribute to performance improvements. We further visualize the effects of these modules in Figure 3, Figure 4, and Figure 5. Figure 3 shows the impact of multi-level disambiguation. In panel (a), without this module, smaller objects such as the Drip tray and Chocolate donut can be identified, larger objects like the Flowerpot, Espresso machine, and box of donuts lose their semantics. In panel (b), with multi-level disambiguation, the semantics of objects at different granularities are successfully preserved. For instance, the Brew head and Drip tray of the Espresso machine are all correctly identified. This is because multilevel disambiguation aggregates the semantics of smaller objects, preventing the loss of larger or composite objects. Figure 4 illustrates the effect of cross-view disambiguation. In panel (a), without this module, semantic loss occurs due to changes in perspective and occlusion, making it difficult to match the same object (e.g., bicycle or chair) from different view-points. In panel (b), cross-view disambiguation captures information from multiple perspectives, providing valuable supplementary data for 3D reconstruction and enhancing both reconstruction and segmentation performance. Figure 5 demonstrates the effect of global min-max normalization. From the results, we can see that the segmentation results are noisy without the global min-max normalization. Multi-view Depth Estimation. Table 6 shows the impact of semantic field reconstruction. This approach notably enhances overall performance while incurring only minimal increase in time cost (from 10s to 11s). Figure 6 presents visual results, highlighting how semantic field reconstruction enhances performance, especially under challenging conditions like transparency, reflection, and occlusion. 5. Conclusion In this paper, we introduced Perception-Efficient 3D Reconstruction (PE3R), novel framework designed to address the challenges of 2D-to-3D perception. PE3R enhances both the speed and accuracy of 3D semantic reconstruction while eliminating the need for scene-specific training or pre-calibrated 3D data. By integrating pixel embedding disambiguation, semantic field reconstruction, and global view perception, PE3R enables efficient and robust zero-shot generalization across variety of scenes and objects. Our comprehensive experiments in 3D open-vocabulary segmentation and reconstruction show significant improvements, including 9-fold speedup in reconstruction, along with enhancements in segmentation accuracy and precision. These results establish new benchmarks in the field and underscore the practical scalability of PE3R for real-world applications. We believe that PE3R paves the way for future research in 2D-to-3D perception and hope our work inspires further exploration and innovation in this area."
        },
        {
            "title": "Impact Statement",
            "content": "This paper introduces novel framework, PerceptionEfficient 3D Reconstruction (PE3R), which aims to advance the field of 2D-to-3D semantic reconstruction. Our approach has the potential to significantly enhance both the efficiency and accuracy of 3D scene understanding, offering broad applications across industries such as robotics, autonomous vehicles, augmented reality, and computer vision. PE3R enables 3D reconstruction from 2D images without requiring scene-specific training or pre-calibrated 3D data. This makes it possible to process unstructured real-world data more effectively, thereby reducing the barriers to deploying machine vision systems in practical environments. In turn, this could make advanced 3D scene understanding more accessible and scalable. Ethically, our framework could encourage the broader application of 3D perception technologies in fields such as disaster response, healthcare, and environmental monitoring, where collecting 3D data is challenging or costly. However, like any advanced technology, it is crucial to consider the potential risks of misuse or biases in data, especially as 3D vision systems become increasingly integrated into critical decision-making processes. We believe PE3R could have significant impact on the field, particularly by making real-time 3D reconstruction more practical and accessible. We are committed to ensuring its deployment aligns with ethical guidelines and promotes positive societal outcomes, including fairness, transparency, and inclusivity."
        },
        {
            "title": "References",
            "content": "Aan√¶s, H., Jensen, R. R., Vogiatzis, G., Tola, E., and Dahl, A. B. Large-scale data for multiple-view stereopsis. International Journal of Computer Vision, 120:153168, 2016. Ayzenberg, V. and Behrmann, M. Development of visual object recognition. Nature Reviews Psychology, 3(2): 7390, 2024. Barron, J. T., Mildenhall, B., Verbin, D., Srinivasan, P. P., and Hedman, P. Mip-nerf 360: Unbounded anti-aliased neural radiance fields. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pp. 54705479, 2022. Bhalgat, Y., Laina, I., Henriques, J. F., Zisserman, A., and Vedaldi, A. Contrastive lift: 3d object instance segmentation by slow-fast contrastive fusion. arXiv preprint arXiv:2306.04633, 2023. Cen, J., Zhou, Z., Fang, J., Shen, W., Xie, L., Jiang, D., Zhang, X., Tian, Q., et al. Segment anything in 3d with nerfs. Advances in Neural Information Processing Systems, 36:2597125990, 2023. Cen, J., Fang, J., Yang, C., Xie, L., Zhang, X., Shen, W., and Tian, Q. Segment any 3d gaussians, 2024. URL https://arxiv.org/abs/2312.00860. Chen, H., Li, C., Guo, M., Yan, Z., and Lee, G. H. Gnesf: Generalizable neural semantic fields. Advances in Neural Information Processing Systems, 36:3655336565, 2023. Choi, S., Song, H., Kim, J., Kim, T., and Do, H. Clickgaussian: Interactive segmentation to any 3d gaussians. arXiv preprint arXiv:2407.11793, 2024. Dai, A., Chang, A. X., Savva, M., Halber, M., Funkhouser, T., and Nie√üner, M. Scannet: Richly-annotated 3d reconstructions of indoor scenes. In Proceedings of the IEEE conference on computer vision and pattern recognition, pp. 58285839, 2017. Fan, Z., Zhang, J., Cong, W., Wang, P., Li, R., Wen, K., Zhou, S., Kadambi, A., Wang, Z., Xu, D., et al. Large spatial model: End-to-end unposed images to semantic 3d. Advances in Neural Information Processing Systems, 37:4021240229, 2024. Fu, Q., Xu, Q., Ong, Y. S., and Tao, W. Geo-neus: Geometry-consistent neural implicit surfaces learning for multi-view reconstruction. Advances in Neural Information Processing Systems, 35:34033416, 2022. Geiger, A., Lenz, P., Stiller, C., and Urtasun, R. Vision meets robotics: The kitti dataset. The International Journal of Robotics Research, 32(11):12311237, 2013. Goel, R., Sirikonda, D., Saini, S., and Narayanan, P. Interactive segmentation of radiance fields. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 42014211, 2023. Guo, J., Deng, N., Li, X., Bai, Y., Shi, B., Wang, C., Ding, C., Wang, D., and Li, Y. Streetsurf: Extending multiview implicit surface reconstruction to street views. arXiv preprint arXiv:2306.04988, 2023. Hu, X., Wang, Y., Fan, L., Fan, J., Peng, J., Lei, Z., Li, Q., and Zhang, Z. Semantic anything in 3d gaussians. arXiv preprint arXiv:2401.17857, 2024. Kerbl, B., Kopanas, G., Leimkuhler, T., and Drettakis, G. 3d gaussian splatting for real-time radiance field rendering. ACM Trans. Graph., 42(4):1391, 2023. Kerr, J., Kim, C. M., Goldberg, K., Kanazawa, A., and Tancik, M. Lerf: Language embedded radiance fields. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pp. 1972919739, 2023. 9 Kirillov, A., Mintun, E., Ravi, N., Mao, H., Rolland, C., Gustafson, L., Xiao, T., Whitehead, S., Berg, A. C., Lo, W.-Y., et al. Segment anything. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pp. 40154026, 2023. Peng, S., Genova, K., Jiang, C., Tagliasacchi, A., Pollefeys, M., Funkhouser, T., et al. Openscene: 3d scene understanding with open vocabularies. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pp. 815824, 2023. Knapitsch, A., Park, J., Zhou, Q.-Y., and Koltun, V. Tanks and temples: Benchmarking large-scale scene reconstruction. ACM Transactions on Graphics (ToG), 36(4):113, 2017. Qin, M., Li, W., Zhou, J., Wang, H., and Pfister, H. Langsplat: 3d language gaussian splatting. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 2005120060, 2024. Kobayashi, S., Matsumoto, E., and Sitzmann, V. Decomposing nerf for editing via feature field distillation. Advances in Neural Information Processing Systems, 35: 2331123330, 2022. Leroy, V., Cabon, Y., and Revaud, J. Grounding imarXiv preprint age matching in 3d with mast3r. arXiv:2406.09756, 2024. Liu, S., Zeng, Z., Ren, T., Li, F., Zhang, H., Yang, J., Li, C., Yang, J., Su, H., Zhu, J., and Zhang, L. Grounding dino: Marrying dino with grounded pre-training for openset object detection. arXiv preprint arXiv:2303.05499, 2023a. Liu, Y., Hu, B., Huang, J., Tai, Y.-W., and Tang, C.-K. In Proceedings of the Instance neural radiance field. IEEE/CVF International Conference on Computer Vision, pp. 787796, 2023b. Liu, Y., Hu, B., Tang, C.-K., and Tai, Y.-W. Sanerf-hq: Segment anything for nerf in high quality. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 32163226, 2024. Long, X., Lin, C., Wang, P., Komura, T., and Wang, W. Sparseneus: Fast generalizable neural surface reconstruction from sparse views. In European Conference on Computer Vision, pp. 210227. Springer, 2022. Mildenhall, B., Srinivasan, P. P., Tancik, M., Barron, J. T., Ramamoorthi, R., and Ng, R. Nerf: Representing scenes as neural radiance fields for view synthesis. Communications of the ACM, 65(1):99106, 2021. Niemeyer, M. and Geiger, A. Giraffe: Representing scenes as compositional generative neural feature fields. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 1145311464, 2021. Oquab, M., Darcet, T., Moutakanni, T., Vo, H., Szafraniec, M., Khalidov, V., Fernandez, P., Haziza, D., Massa, F., ElNouby, A., et al. Dinov2: Learning robust visual features without supervision. arXiv preprint arXiv:2304.07193, 2023. Qu, Y., Dai, S., Li, X., Lin, J., Cao, L., Zhang, S., and Ji, R. Goi: Find 3d gaussians of interest with an optimizable open-vocabulary semantic-space hyperplane. In Proceedings of the 32nd ACM International Conference on Multimedia, pp. 53285337, 2024. Radford, A., Kim, J. W., Hallacy, C., Ramesh, A., Goh, G., Agarwal, S., Sastry, G., Askell, A., Mishkin, P., Clark, J., et al. Learning transferable visual models from natural In International conference on language supervision. machine learning, pp. 87488763. PMLR, 2021. Ravi, N., Gabeur, V., Hu, Y.-T., Hu, R., Ryali, C., Ma, T., Khedr, H., Radle, R., Rolland, C., Gustafson, L., et al. Sam 2: Segment anything in images and videos. arXiv preprint arXiv:2408.00714, 2024. Schonberger, J. L. and Frahm, J.-M. Structure-from-motion In Proceedings of the IEEE conference on revisited. computer vision and pattern recognition, pp. 41044113, 2016. Schonberger, J. L., Zheng, E., Frahm, J.-M., and Pollefeys, M. Pixelwise view selection for unstructured multi-view stereo. In Computer VisionECCV 2016: 14th European Conference, Amsterdam, The Netherlands, October 11-14, 2016, Proceedings, Part III 14, pp. 501518. Springer, 2016. Schops, T., Schonberger, J. L., Galliani, S., Sattler, T., Schindler, K., Pollefeys, M., and Geiger, A. multiview stereo benchmark with high-resolution images and multi-camera videos. In Proceedings of the IEEE conference on computer vision and pattern recognition, pp. 32603269, 2017. Schroppel, P., Bechtold, J., Amiranashvili, A., and Brox, T. benchmark and baseline for robust multi-view depth estimation. In 2022 International Conference on 3D Vision (3DV), pp. 637645. IEEE, 2022. Shen, Q., Yang, X., and Wang, X. Flashsplat: 2d to 3d gaussian splatting segmentation solved optimally. arXiv preprint arXiv:2409.08270, 2024. Yang, Z., Ren, Z., Shan, Q., and Huang, Q. Mvs2d: Efficient multi-view stereo via attention-driven 2d convolutions. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pp. 85748584, 2022. Yao, Y., Luo, Z., Li, S., Fang, T., and Quan, L. Mvsnet: Depth inference for unstructured multi-view stereo. In Proceedings of the European conference on computer vision (ECCV), pp. 767783, 2018. Ye, M., Danelljan, M., Yu, F., and Ke, L. Gaussian grouping: Segment and edit anything in 3d scenes. arXiv preprint arXiv:2312.00732, 2023. Yeshwanth, C., Liu, Y.-C., Nie√üner, M., and Dai, A. Scannet++: high-fidelity dataset of 3d indoor scenes. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pp. 1222, 2023. Yu, H.-X., Guibas, L. J., and Wu, J. Unsupervised arXiv preprint discovery of object radiance fields. arXiv:2107.07905, 2021. Zhai, X., Mustafa, B., Kolesnikov, A., and Beyer, L. Sigmoid loss for language image pre-training. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pp. 1197511986, 2023. Zhang, C., Han, D., Zheng, S., Choi, J., Kim, T.-H., and Hong, C. S. Mobilesamv2: Faster segment anything to everything. arXiv preprint arXiv:2312.09579, 2023a. Zhang, J., Li, S., Luo, Z., Fang, T., and Yao, Y. Vis-mvsnet: Visibility-aware multi-view stereo network. International Journal of Computer Vision, 131(1):199214, 2023b. Zhi, S., Laidlow, T., Leutenegger, S., and Davison, A. J. In-place scene labelling and understanding with implicit scene representation. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pp. 15838 15847, 2021. Zhou, S., Chang, H., Jiang, S., Fan, Z., Zhu, Z., Xu, D., Chari, P., You, S., Wang, Z., and Kadambi, A. Feature 3dgs: Supercharging 3d gaussian splatting to enable distilled feature fields. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 2167621685, 2024. Siddiqui, Y., Porzi, L., Bulo, S. R., Muller, N., Nie√üner, M., Dai, A., and Kontschieder, P. Panoptic lifting for 3d scene understanding with neural fields. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 90439052, 2023. Sinha, P. and Poggio, T. Role of learning in threedimensional form perception. Nature, 384(6608):460 463, 1996. Straub, J., Whelan, T., Ma, L., Chen, Y., Wijmans, E., Green, S., Engel, J. J., Mur-Artal, R., Ren, C., Verma, S., et al. The replica dataset: digital replica of indoor spaces. arXiv preprint arXiv:1906.05797, 2019. Takmaz, A., Fedele, E., Sumner, R. W., Pollefeys, M., Tombari, F., and Engelmann, F. Openmask3d: Openvocabulary 3d instance segmentation. arXiv preprint arXiv:2306.13631, 2023. Tang, S., Pei, W., Tao, X., Jia, T., Lu, G., and Tai, Y.-W. Scene-generalizable interactive segmentation of radiance In Proceedings of the 31st ACM International fields. Conference on Multimedia, pp. 67446755, 2023. Teed, Z. and Deng, J. Deepv2d: Video to depth with differentiable structure from motion. arXiv preprint arXiv:1812.04605, 2018. Tschernezki, V., Laina, I., Larlus, D., and Vedaldi, A. Neural feature fusion fields: 3d distillation of self-supervised 2d image representations. In 2022 International Conference on 3D Vision (3DV), pp. 443453. IEEE, 2022. Ummenhofer, B., Zhou, H., Uhrig, J., Mayer, N., Ilg, E., Dosovitskiy, A., and Brox, T. Demon: Depth and motion network for learning monocular stereo. In Proceedings of the IEEE conference on computer vision and pattern recognition, pp. 50385047, 2017. Wang, P., Liu, L., Liu, Y., Theobalt, C., Komura, T., and Wang, W. Neus: Learning neural implicit surfaces by volume rendering for multi-view reconstruction. arXiv preprint arXiv:2106.10689, 2021. Wang, S., Leroy, V., Cabon, Y., Chidlovskii, B., and Revaud, J. Dust3r: Geometric 3d vision made easy. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 2069720709, 2024. Wang, Y., Skorokhodov, I., and Wonka, P. Hf-neus: Improved surface reconstruction using high-frequency details. Advances in Neural Information Processing Systems, 35:19661978, 2022. Welchman, A. E., Deubelius, A., Conrad, V., Bulthoff, H. H., and Kourtzi, Z. 3d shape perception from combined depth cues in human visual cortex. Nature neuroscience, 8(6): 820827, 2005."
        }
    ],
    "affiliations": [
        "xML Lab, National University of Singapore"
    ]
}