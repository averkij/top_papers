{
    "paper_title": "Efficient Continual Learning in Language Models via Thalamically Routed Cortical Columns",
    "authors": [
        "Afshin Khadangi"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Continual learning is a core requirement for deployed language models, yet standard training and fine-tuning pipelines remain brittle under non-stationary data. Online updates often induce catastrophic forgetting, while methods that improve stability frequently increase latency, memory footprint, or dense computation in ways that do not scale well to long contexts. We introduce TRC$^{2}$ (Thalamically Routed Cortical Columns), a decoder-only backbone that addresses continual learning at the architectural level. TRC$^{2}$ combines sparse thalamic routing over cortical columns with mechanisms for modulation, prediction, memory, and feedback, together with a fast corrective pathway that supports rapid adaptation without destabilizing slower parameters. The resulting block is sparse and chunk-parallel, enabling efficient training and inference while preserving clean ablations of each subsystem. We instantiate a reproducible training and evaluation stack and a continual-learning harness that measures proxy forgetting under streaming domain shifts. Across language modeling and continual learning benchmarks, TRC$^{2}$ improves the stability-plasticity tradeoff at comparable compute, enabling rapid on-stream adaptation while preserving previously acquired behavior."
        },
        {
            "title": "Start",
            "content": "6 2 0 2 5 2 ] . [ 1 9 7 4 2 2 . 2 0 6 2 : r a"
        },
        {
            "title": "Efficient Continual Learning in Language Models via\nThalamically Routed Cortical Columns",
            "content": "Afshin Khadangi SnT, University of Luxembourg afshin.khadanki@uni.lu"
        },
        {
            "title": "Working Paper",
            "content": "W&B Logs"
        },
        {
            "title": "Abstract",
            "content": "Continual learning is core requirement for deployed language models, yet standard training and fine-tuning pipelines remain brittle under non-stationary data. Online updates often induce catastrophic forgetting, while methods that improve stability frequently increase latency, memory footprint, or dense computation in ways that do not scale well to long contexts. We introduce TRC2 (Thalamically Routed Cortical Columns), decoder-only backbone that addresses continual learning at the architectural level. TRC2 combines sparse thalamic routing over cortical columns with mechanisms for modulation, prediction, memory, and feedback, together with fast corrective pathway that supports rapid adaptation without destabilizing slower parameters. The resulting block is sparse and chunk-parallel, enabling efficient training and inference while preserving clean ablations of each subsystem. We instantiate reproducible training and evaluation stack and continual-learning harness that measures proxy forgetting under streaming domain shifts. Across language modeling and continual learning benchmarks, TRC2 improves the stabilityplasticity tradeoff at comparable compute, enabling rapid on-stream adaptation while preserving previously acquired behavior."
        },
        {
            "title": "Introduction",
            "content": "Large language models are increasingly deployed as long-lived systems that must remain useful under shifting data, shifting user intents, and shifting domains. In practice, this creates persistent tension: the model must adapt quickly to new distributions while preserving previously learned behavior. The default remedy, periodic retraining or heavy fine-tuning, is expensive and slow. Lightweight updates such as adapters and low-rank tuning reduce cost, but sequential updates still induce interference and forgetting, especially when task boundaries are unclear and storage of prior data is restricted. Recent work has exposed both the opportunity and the limits of current architectures. On the efficiency side, modern state-space models have narrowed the quality gap with Transformers while offering favorable inference scaling; Mamba-3 pushes this line further with improved discretization, richer dynamics, and hardware-aware decoding efficiency [12]. Hybrid designs such as Jamba combine attention and Mamba-like blocks to trade off long-context capability and throughput [15]. On the stability side, gating has emerged as surprisingly powerful primitive: Gated Attention shows that small, structured modification to attention can improve training stability, reduce attention pathologies, and support long-context extrapolation [21]. At the same time, sparse routing and mixtures introduce Preprint. Working Paper. their own fragility when the data distribution evolves, motivating careful study of router robustness in continual pre-training [30] and new routing schemes for scaling SSMs [35]. In parallel, the community has begun to treat adaptation at inference time as first-class capability. Test-Time Learning for LLMs frames adaptation as input perplexity minimization on unlabeled test streams and shows large gains under distribution shift when updates are constrained to low-rank subspaces [10]. Model-merging approaches provide complementary lens: local mixtures constructed via model merging can approximate test-time training while amortizing cost to training time [3], and null-space constrained gating can reduce interference during continual merging [22]. These results underscore key point: useful adaptation signals exist at deployment time, but today they are typically exploited through bolt-on procedures that are not native to the backbone and therefore remain difficult to scale, difficult to stabilize, and hard to compare cleanly across settings. This paper argues that continual learning should be treated as an architectural property. We introduce TRC2 (Thalamically Routed Cortical Columns), decoder-only backbone designed around two principles. First, communication should be sparse and controllable, so that new information can be routed to small subset of computation without globally perturbing the model. Second, plasticity should be localized in fast mechanisms that can update online at low cost, while slower representational structures remain stable and support abstraction across time. TRC2 implements these principles with looped layer structure. Each layer contains thalamic router that selects top-k set of cortical columns per token and encourages temporal continuity via topology-aware prior. Each selected column is compact microcircuit whose core is selective state-space update, augmented with explicit excitatory and inhibitory modulation. cerebellar fastweight corrector provides dedicated, low-rank pathway for online updates driven by deployment data, enabling rapid adjustment without rewriting the slow cortical parameters. The resulting layer is linear-time in sequence length within each active column, with constant-time routing overhead, and supports chunked scan implementations that reduce kernel-launch overhead in practice. The architecture is motivated by an empirical gap in current continual learning for LLMs. Replay-free adapter methods such as ELLA show that careful control of update subspaces can substantially reduce forgetting, but they still treat the backbone as static substrate and rely on external regularizers [4]. TRC2 instead makes interference control and rapid adaptation part of the computation graph through routing, inhibition, and fast weights. This also aligns with recent evidence that local, iterative learning mechanisms can be scaled in deep networks; predictive-coding style training has reached 100+ layer regimes, suggesting that looped correction dynamics need not be confined to toy scales [11]. Our contributions are as follows. We present TRC2, decoder-only backbone for continual learning that combines sparse thalamic top-k routing over cortical columns with biologically grounded mechanisms for modulation, prediction, memory, feedback, and fast correction. We develop sparse, chunk-parallel implementation of TRC2 that supports efficient training and inference on modern accelerators, including topology-aware routing, chunk-level computation, and memory-aware execution with optional activation checkpointing. We provide reproducible continual-learning evaluation stack with distributed multi-GPU training, standardized logging, and task-wise evaluations that track forgetting and forward transfer under streaming domain shifts. The framework includes targeted ablations and strong baselines, enabling direct analysis of which TRC2 components drive gains in adaptation and retention. The remainder of the paper details the TRC2 layer, then evaluates efficiency and adaptation across language modeling and continual learning benchmarks, with direct comparisons to strong Transformer, hybrid, and state space model baselines."
        },
        {
            "title": "2 Related Work",
            "content": "Continual learning for large language models has expanded from classic task-incremental settings to broader regimes such as continual pre-training, domain-adaptive pre-training, instruction updates, and lifelong knowledge maintenance. Recent surveys organize this space into internal model updates versus external augmentation, and they highlight open evaluation issues that become more severe at 2 scale [37, 28]. This framing motivates backbones that are themselves robust to streaming distribution shift, rather than relying only on training-time interventions. dominant line of work for post-training adaptation constrains updates to small parameter subspaces. DoRA improves low-rank adaptation by decomposing weight updates into magnitude and direction, narrowing the gap to full fine-tuning without changing inference cost [16]. Other work studies composition across many updates, including gated combinations of LoRA modules [32] and lifelong mixtures with routing constraints and order sensitivity [31]. These results suggest that the structure of the update pathway and the routing mechanism both matter for long adaptation sequences. Mixture-of-Experts remains practical route to higher capacity under bounded per-token compute. DeepSeekMoE studies expert specialization and shared experts to reduce redundancy and improve routing behavior [6]. LLaMA-MoE shows that dense decoders can be converted into sparse expert systems and recovered through continued pre-training [38]. At the tuning stage, sparse expertization can also be made highly parameter-efficient for instruction adaptation [34]. Work on router design, including mixtures of routers, further emphasizes that routing quality is often the limiting factor in sparse systems [36]. Efficient sequence backbones have also shifted attention away from dense attention-only designs. Mamba established selective state-space computation as competitive foundation-model backbone with linear-time sequence processing [8]. BlackMamba combines state-space dynamics with sparse experts, showing that routing and recurrent sequence cores can be integrated in one architecture [2]. RWKV-family models provide another recurrent path with stronger state parameterization [20]. At the systems level, FlashAttention-3 highlights how strongly performance depends on kernel-level implementation choices, which is directly relevant when evaluating alternative backbones [27]. Our design is informed by computational and systems neuroscience as architectural guidance. The predictive branch follows predictive-coding formulations that separate top-down prediction from bottom-up mismatch signals [25], while the modulation controller is motivated by classical accounts of reward prediction and uncertainty-dependent gain control [26, 1]. The gated readout is inspired by compartment-specific integration and coincidence effects in cortical pyramidal neurons [13], and is further supported by recent evidence that cortical feedback engages active dendritic processing [7]. The routing-weight refinement stage is motivated by reciprocal cortico-thalamic feedback loops that shape thalamic processing [5]. The associative memory pathway uses modern Hopfield retrieval as differentiable content-addressable memory mechanism [24], and is broadly consistent with recent work on systems consolidation and predictive reward representations in hippocampal-cortical circuits [14, 33]. We also view recent studies on large-scale neurotransmitter-system organization and biologically grounded learning principles as complementary motivation for structured control signals and local computation in scalable sequence models [9, 17, 29]."
        },
        {
            "title": "3 Method",
            "content": "3.1 Overview and notation Let x1:T be token sequence from vocabulary of size . TRC2 is decoder-only language model with hidden width and stacked blocks. For batch size and sequence length , the hidden representation at layer ℓ is Token and position embeddings are learned: (ℓ) RBT d. Each block uses pre-normalization, (0) b,t = E[xb,t] + [t]. (ℓ) = RMSNorm(X (ℓ)). (1) (2) TRC2 1 combines chunk-level sparse routing, routed cortical computation, an optional modulation and predictive pathway, an optional associative memory with top-down gating, an optional routingweight refinement step, and an optional low-rank corrective path. Each subsystem is independently toggleable. Implementation details that are useful for exact reproduction, including padding and tensor layouts, are summarized in Appendix A. Figure 1: TRC2 architecture block. 4 (3) (4) (5) (6) (7) (8) (9) (10) (11) (12) (13) (14) (15) (16) 3.2 Block computation For one block, let RBT be the input and let + denote the output. The core computation is = RMSNorm(X), (sroute, spred, sgain) = ModCtrl(U ), ˆU , Lpred = PredictivePath(U, spred), (I, R, S, Lroute) = Router( ˆU ), mem = AssocMem( ), = Cortex( ˆU , I, R, mem), = RefineWeights(Y, I, S), Cortex( ˆU , I, R, mem) = Corrector( ˆU , ), ggain = + Drop(Y + ), (if modulation is enabled), if refinement is enabled, (cid:16) + = + Drop (cid:16) SwiGLU RMSNorm( X) (cid:17)(cid:17) . Here are top-k routed column indices, are routing mixture weights, and are the selected router logits used by the refinement step. The block returns + together with auxiliary terms Lroute and, when enabled, Lpred. 3.3 Modulation controller and predictive pathway Modulation controller. When enabled, the controller outputs three sequence-level scalars in [0, 1]: routing-control signal sroute, predictive-blend signal spred, and global-gain signal sgain. The controller is small MLP that operates on per-sequence statistics of together with deviations from running exponential moving averages: (cid:88) Ub,t,:, σb = Stdt(Ub,t,:), dσ = σb vema . µb = 1 t=1 dµ = µb µema, ; dσ The concatenated vector [µb; σb; dµ ] R4d is passed through two-layer MLP with SiLU and sigmoid to produce the three control signals. The implementation broadcasts these signals across token positions and channels. Predictive pathway. When enabled, the block predicts each normalized token representation from its left context using causal depthwise 1D convolution followed by pointwise 1D convolution: ˆP RBT d. (17) The convolution is implemented with left padding and one-step shift so that position depends only on positions < (Appendix A.1). The predictive auxiliary loss is Lpred = λpc MSE (cid:17) (cid:16) ˆP:,2:T,:, stopgrad(U:,2:T,:) . The predictor enters the block through controller-weighted prediction-error blend: ˆU = (1 spred) , (18) (19) where is either ˆP or stopgrad( ˆP ) depending on whether gradient flow through the predictive branch is enabled. If the modulation controller is disabled, the implementation uses the fixed value spred = 0.5. 5 3.4 Chunked sparse routing Routing is computed at chunk resolution. Let be the routing chunk size and nc = /C. The sequence is padded by repeating the last representation if needed, reshaped to and pooled within each chunk: ˆUchunk RBncCd, Ub,c = (cid:40) ˆUchunk[b, c, 1, :] (first-position pooling), 1 (cid:80)C τ = ˆUchunk[b, c, τ, :] (mean pooling). The exact padding and chunking behavior is given in Appendix A.2. Router logits. Given router width dr and columns, the router computes RBncdr , = (r) (r) RM dr , b,c,m = Qb,c,:, (r) Lbase m,:. Topology-aware prior. When enabled, the router predicts 2D coordinate for each chunk, and applies distance penalty to fixed column coordinates Pm R2: πb,c = tanh( Ub,cWpos + bpos) R2, Ltopo b,c,m = γπb,c Pm2 2. The column coordinates form grid when is square and circle otherwise. (20) (21) (22) (23) (24) (25) Routing-logit modulation and top-k selection. When the modulation controller is enabled, the router scales logits by sequence-level factor: and uses aroute = 1 + ρroute(2sroute 1), Lb,c,m = aroute (cid:16) b,c,m + 1topoLtopo Lbase b,c,m (cid:17) . The router then selects top-k columns per chunk, and forms routing weights by softmax on the selected logits: Ib,c,1:k = TopK(Lb,c,:, k), Rb,c,j = exp(Lb,c,Ib,c,j ) j=1 exp(Lb,c,Ib,c,j ) (cid:80)k . The selected pre-softmax values are retained for the routing-weight refinement step. Sb,c,j = Lb,c,Ib,c,j (26) (27) (28) (29) Routing auxiliary loss. When routing regularization is enabled, the implementation accumulates the top-k routing mass back into dense (B, nc, ) tensor and applies the quadratic penalty Lroute = λlb (cid:88) m=1 p2 m, (30) where pm is the normalized routing mass assigned to column across the batch and chunks (Appendix A.2). 6 3.5 Associative memory and routed cortical computation Associative memory (optional). The associative-memory module operates on chunk summaries RBncd using Modern Hopfield retrieval. It stores ns learnable slots normalizes both projected chunk queries and slots, and retrieves Ξ Rnsdh , This retrieved context is used both in the readout stage and in chunk-level lateral propagation. The exact retrieval equations are listed in Appendix A.4. mem RBncd. (31) Routed cortical computation. The cortex reuses the chunk-level routing decisions (I, R) for all tokens in the chunk. dense projection maps each padded token to column-specific parameters: Proj( ˆUb,t) RM (3n+3), (32) where is the cortical state width. After reshaping to (B, nc, C, M, 3n + 3) and gathering the selected columns, the block obtains and three gate tensors δ, Bin, Cin RBncCkn, gstate, gout, gdis (0, 1)BncCk. When excitatory-inhibitory gating is enabled, the third gate acts as disinhibitory controller: gstate (1 gdis) gstate + gdis, gout (1 gdis) gout + gdis. (33) (34) The state-related tensors are then scaled by gstate. Next, the block forms token-dependent coefficient from the projected state parameters and learned per-column tensor Alog RM n: Abase = σ(Alog), α = σ(δ) Asel, Dstate = (1 α) Bin. (35) (36) (37) causal depthwise 1D convolution followed by pointwise 1D convolution is applied along the within-chunk token axis, producing filtered state tensor RBncCkn. This is chunk-causal operation. Cross-chunk propagation is handled later by separate chunk-level convolution. Readout, output gating, and routed mixture. The bottom-up readout input is If the top-down gated readout is enabled and associative memory is active, the code uses two-branch readout: Bread = Cin H. (38) Sread = WbotBread, Gtop = σ(Wgate ϕ(WtopC mem Ysel = RMSNorm(Sread (1 + Gtop)) , bcast)) , (39) (40) (41) where mem axes, and ϕ is SiLU. Otherwise, the block uses linear readout: bcast denotes the chunk-level memory retrieval broadcast across token and selected-column Ysel = WoutBread. (42) 7 (44) (45) (46) The selected-column outputs are then scaled by the output-control gate: Ysel Ysel gout. (43) If the skip connection is enabled, learned scalar coefficient per selected column adds gated skip from the chunk input: Ysel Ysel + tanh(sI ) ˆUchunk."
        },
        {
            "title": "The routed chunk output is the weighted sum",
            "content": "Ychunk[b, c, τ, :] = (cid:88) j=1 Rb,c,j Ysel[b, c, τ, j, :]. Chunk-level lateral propagation. The cortex forms chunk summaries Cctx[b, c, :] = (cid:88) Ychunk[b, c, τ, :],"
        },
        {
            "title": "1\nC",
            "content": "τ =1 optionally adds the associative-memory retrieval mem, and applies causal depthwise-pluspointwise convolution along the chunk axis. The resulting chunk signal is broadcast back to all token positions in the chunk and added to Ychunk. The final cortex output RBT is obtained after reshaping and removing padding. Appendix A.3 gives the exact tensorized implementation and the activation-checkpointing option used to reduce memory. 3.6 Routing-weight refinement and low-rank corrective path Routing-weight refinement (optional). After first cortex pass, the block can refine routing weights without recomputing top-k indices. The cortex output is pooled to chunk summaries, projected back to router space, and scored against the router keys to obtain feedback logits. The code gathers only the logits for the already-selected columns and mixes them with the original selected router logits: b,c,: = softmax(cid:0)Sb,c,: + αfb Sfb (cid:1) , The cortex is then executed second time with the same indices and refined weights R. This is full second cortex pass over the fixed routing support, not post-hoc reweighting of cached outputs (Appendix A.4). αfb = tanh(cmix) (1, 1). (47) b,c,: Low-rank corrective path (optional). The corrective path computes low-rank residual from the normalized block input and cortex output. Let dz be the intermediate width and the low-rank width. The implementation uses split linear map (equivalent to single linear layer on [ ˆU ; ]): Zpre = ˆU (W (1) ) + (W (2) ) + bz, = ϕ(Zpre), with ϕ = SiLU. The low-rank correction is dz(cid:88) b,t,q = (cid:88) p=1 r=1 If this path is disabled, the block sets = 0. Zb,t,p Vp,r Uq,r. 3.7 Global gain modulation, output head, and training objective (48) (49) When the modulation controller is enabled, the block applies sequence-level gain to the cortex output before the residual update: ggain = 1 + ρgain(2sgain 1), ggain Y, with broadcasting over token and feature dimensions. After blocks, the model applies final RMS normalization and tied output projection: logits = LMHead (cid:16) RMSNorm(X (L)) (cid:17) RBT . (50) (51) 8 Each block may produce routing auxiliary regularizer and predictive reconstruction loss. The wrapper accumulates these terms across layers: LΣ route = (cid:88) ℓ=1 L(ℓ) route, LΣ pred = (cid:88) ℓ=1 L(ℓ) pred. (52) The wrapper computes token cross-entropy LCE from logits and labels. In the provided trainer, the total objective is Ltrain = LCE + 0.1 LΣ pred + LΣ route, (53) with fixed coefficients (Appendix A.5). detailed complexity analysis of TRC2 is provided in Appendix B."
        },
        {
            "title": "4 Experiments and Results",
            "content": "4.1 Experimental setup We evaluate TRC2 as drop-in decoder-only language modeling backbone under two requirements: (i) competitive next-token modeling and efficiency, and (ii) stable adaptation under streaming shifts without task boundaries. All experiments run on single node with 4 NVIDIA V100 (32GB) using mixed precision. Training data. For dense pre-training style runs we use C4 [23], large web corpus that approximates evolving deployment text; we train either in streaming mode (to model non-stationary inputs) or from cached snapshot for controlled comparisons. For held-out perplexity we evaluate on wikitext103-v1 [18] and LAMBADA [19] as fixed anchors: WikiText is curated Wikipedia benchmark that is sensitive to over-specialization, while LAMBADA probes discourse-level, long-context prediction. Together these evaluations help quantify the stability side of the stabilityplasticity tradeoff while the model adapts. Models and baselines. We compare TRC2 against parameter-matched Transformer, and Mamba decoder baselines trained under the same pipeline. Training and evaluation protocol. All experiments run on single node with 4 NVIDIA V100 GPUs (32GB) using distributed data parallelism and mixed precision. We use batch size 8 per GPU, gradient accumulation over 4 micro-steps, and sequence length 1024, giving an effective global batch of 128 sequences (131,072 tokens) per optimizer step. Unless stated otherwise, runs use AdamW with learning rate 2 104, weight decay 0.1, (β1, β2) = (0.9, 0.95), 1,000 warmup steps, cosine decay, and gradient clipping at 1.0. The main training budget is 22,000 optimizer steps, which corresponds to 2,883,584,000 tokens (approximately 2.88B). Training uses streaming C4 with the GPT-NeoX tokenizer and context length 1024. Evaluation is performed every 500 optimizer steps on fixed validation probes (C4, WikiText, and LAMBADA). Full configuration details, including tokenizer, data caps, logging, and checkpoint selection, are listed in Appendix A.6. Metrics. We report (i) held-out loss and perplexity, (ii) efficiency metrics including end-to-end throughput (tokens/s and sequences/s) and peak memory, and (iii) continual-learning metrics computed over the validation probes treated as task stream. For continual evaluation, we maintain historical best value for each probe and report forgetting proxy: for lower-is-better metrics (such as perplexity), forgetting is the increase from the best-so-far value; for higher-is-better metrics (such as token accuracy or BLEU), forgetting is the drop from the best-so-far value; in both cases, values are clipped at zero. We report mean forgetting across probes together with aggregate best-task and worst-task summaries. The evaluation pipeline can also compute teacher-forced text metrics from arg max predictions on labeled positions, including token accuracy, exact match, BLEU, chrF, and ROUGE (when available). Additional implementation details for metric computation and trainer-side aggregation are summarized in Appendix A. 9 4.2 Results Table 1 reports held-out perplexity, Bleu score, and throughput. Table 2 summarizes continual learning on streaming task suite. Table 1: Evaluation performance and efficiency compared to baselines; tokens/s measured during steady-state training. dm and nb represent model depth and the number of trc2 blocks, respectively. Wiki and LAM represent WikiText, and LAMBADA datasets respectively. Model Params dm nb PPL Bleu c4 Wiki LAM c4 Wiki LAM Tokens/s MemHour GPU Transformer Mamba TRC2 (ours) 162M 176M 169M 768 768 512 13 11 8 60.70 70.45 2. 215.18 357.67 2.56 105.72 116.73 2.02 8.12 6.90 71.66 8.23 2.87 66.57 5.09 3.97 70.07 127000 108000 118 GB 178 GB 268 GB Table 2: Continual-learning evaluation on streaming tasks. Avg Forgetting is the mean increase in PPL (or decrease in token accuracy, bleu score) relative to the best-so-far per task after each update. Model Params dm nb Average forgetting - last step Average forgetting - normalized AUC PPL tokenacc Bleu PPL tokenacc Transformer Mamba TRC2 (ours) 162M 176M 169M 768 768 512 13 11 8 0.0000 0.0000 0. 0.0014 0.0006 0.0010 0.3757 0.0900 0.0435 0.0669 0.3371 0.0018 0.0008 0.0011 0.0008 Bleu 0.1684 0.1957 0."
        },
        {
            "title": "5 Discussion",
            "content": "The results support the main design claim of TRC2: continual learning can be improved by allocating plasticity to small, explicit pathway while keeping most representational structure stable. In Table 2, TRC2 shows markedly lower normalized forgetting area under the curve on perplexity and Bleu than the baselines, which indicates that the model retains earlier behavior more consistently over the full stream rather than only at the end of training. At the same time, the last-step forgetting values suggest that stability is not achieved by freezing learning entirely, since the model continues to move and occasionally pays small short-term cost in some probes. From an efficiency perspective, TRC2 trades throughput for structured sparsity and online-correctable computation. Table 1 shows lower tokens/s than the dense Transformer baseline in this implementation. This is consistent with additional routing, gathering, and per-column computation. The chunked routing scheme helps amortize router overhead, but end to end performance is still sensitive to kernel fusion, memory layout, and the fraction of active columns. In practice, the favorable scaling regime appears when routing decisions are stable across neighboring tokens and when the implementation can keep column-local scans contiguous in memory. Several mechanisms likely contribute to the observed stability trend. Topology-aware routing encourages temporal continuity in column selection, which can reduce parameter interference by keeping related updates localized. The excitatory-inhibitory gating provides simple control handle that can suppress unstable activations before they propagate through residual pathways. The corrective path offers fast route for stream-driven adjustment without rewriting slower parameters, which is aligned with the continual-learning objective used in training. limitation of the current study is robustness under sharper distribution shifts, longer contexts, and more frequent regime changes, where routers can become brittle and chunk summaries may lose fine-grained signals."
        },
        {
            "title": "6 Conclusion",
            "content": "This work introduced TRC2, decoder-only backbone that targets continual learning through architectural separation of stable representation, sparse routed computation, and low-rank corrective pathway for rapid updates. The model combines chunk-level top-k routing over cortical columns with modulation, prediction, associative memory, feedback refinement, and fast correction, while retaining systems-friendly, chunk-parallel execution strategy. Empirically, TRC2 demonstrates improved 10 retention over streaming evaluation suite as reflected by lower accumulated proxy forgetting, while maintaining competitive held-out behavior under the same training pipeline and domain shifts. The results suggest that continual learning can benefit from making interference control part of the forward computation rather than relying only on external fine-tuning procedures. Future work should extend the evaluation to larger scales and longer contexts, and study router stability under harder nonstationary streams. promising direction is to couple the corrective pathway with deployment-time constraints, so adaptation can be bounded, interpretable, and reversible when the stream contains noisy or adversarial segments."
        },
        {
            "title": "References",
            "content": "[1] Yu Angela and Peter Dayan. Uncertainty, neuromodulation, and attention. Neuron, 46(4):681692, 2005. [2] Quentin Gregory Anthony, Yury Tokpanov, Paolo Glorioso, and Beren Millidge. Blackmamba: Mixture of experts for state-space models. In ICLR 2024 Workshop on Mathematical and Empirical Understanding of Foundation Models, 2024. [3] Ryo Bertolissi, Jonas Hübotter, Ido Hakimi, and Andreas Krause. Local mixtures of experts: Essentially free test-time training via model merging. In Second Conference on Language Modeling, 2025. [4] Shristi Das Biswas, Yue Zhang, Anwesan Pal, Radhika Bhargava, and Kaushik Roy. ELLA: Efficient lifelong learning for adapters in large language models. In AI That Keeps Up: NeurIPS 2025 Workshop on Continual and Compatible Foundation Model Updates, 2025. [5] Gregory Born, Felix Schneider-Soupiadis, Sinem Erisken, Agne Vaiceliunaite, Chu Lan Lao, Milad Mobarhan, Martin Spacek, Gaute Einevoll, and Laura Busse. Corticothalamic feedback sculpts visual spatial integration in mouse thalamus. Nature neuroscience, 24(12):17111720, 2021. [6] Damai Dai, Chengqi Deng, Chenggang Zhao, RX Xu, Huazuo Gao, Deli Chen, Jiashi Li, Wangding Zeng, Xingkai Yu, Yu Wu, et al. Deepseekmoe: Towards ultimate expert specialization in mixture-of-experts language models. In Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 12801297, 2024. [7] Mehmet Fisek, Dustin Herrmann, Alexander Egea-Weiss, Matilda Cloves, Lisa Bauer, Tai-Ying Lee, Lloyd Russell, and Michael Häusser. Cortico-cortical feedback engages active dendrites in visual cortex. Nature, 617(7962):769776, 2023. [8] Albert Gu and Tri Dao. Mamba: Linear-time sequence modeling with selective state spaces. In First Conference on Language Modeling, 2024. [9] Justine Hansen, Golia Shafiei, Ross Markello, Kelly Smart, Sylvia ML Cox, Martin Nørgaard, Vincent Beliveau, Yanjun Wu, Jean-Dominique Gallezot, Étienne Aumont, et al. Mapping neurotransmitter systems to the structural and functional organization of the human neocortex. Nature neuroscience, 25(11):15691581, 2022. [10] Jinwu Hu, Zitian Zhang, Guohao Chen, Xutao Wen, Chao Shuai, Wei Luo, Bin Xiao, Yuanqing Li, and Mingkui Tan. Test-time learning for large language models. In Aarti Singh, Maryam Fazel, Daniel Hsu, Simon Lacoste-Julien, Felix Berkenkamp, Tegan Maharaj, Kiri Wagstaff, and Jerry Zhu, editors, Proceedings of the 42nd International Conference on Machine Learning, volume 267 of Proceedings of Machine Learning Research, pages 2482324849. PMLR, 1319 Jul 2025. [11] Francesco Innocenti, El Mehdi Achour, and Christopher Buckley. µpc: Scaling predictive coding to 100+ layer networks. In Advances in Neural Information Processing Systems (NeurIPS), 2025. NeurIPS 2025 poster; also available as arXiv:2505.13124. [12] Aakash Lahoti, Kevin Li, Berlin Chen, Caitlin Wang, Aviv Bick, J. Zico Kolter, Tri Dao, and Albert Gu. Mamba-3: Improved sequence modeling using state space principles. In International Conference on Learning Representations (ICLR), 2026. [13] Matthew Larkum, Julius Zhu, and Bert Sakmann. new cellular mechanism for coupling inputs arriving at different cortical layers. Nature, 398(6725):338341, 1999. [14] Ji-Hye Lee, Woong Bin Kim, Eui Ho Park, and Jun-Hyeong Cho. Neocortical synaptic engrams for remote contextual memories. Nature Neuroscience, 26(2):259273, 2023. [15] Barak Lenz, Opher Lieber, Alan Arazi, Amir Bergman, Avshalom Manevich, Barak Peleg, Ben Aviram, Chen Almagor, Clara Fridman, Dan Padnos, et al. Jamba: Hybrid transformer-mamba language models. In The thirteenth international conference on learning representations, 2025. [16] Shih-Yang Liu, Chien-Yi Wang, Hongxu Yin, Pavlo Molchanov, Yu-Chiang Frank Wang, Kwang-Ting Cheng, and Min-Hung Chen. Dora: Weight-decomposed low-rank adaptation. In Forty-first International Conference on Machine Learning, 2024. [17] Yuxiang Andy Liu, Yuhan Nong, Jiesi Feng, Guochuan Li, Paul Sajda, Yulong Li, and Qi Wang. Phase synchrony between prefrontal noradrenergic and cholinergic signals indexes inhibitory control. Nature Communications, 16(1):7260, 2025. 12 [18] Stephen Merity, Caiming Xiong, James Bradbury, and Richard Socher. Pointer sentinel mixture models. In International Conference on Learning Representations, 2017. [19] Denis Paperno, Germán Kruszewski, Angeliki Lazaridou, Ngoc-Quan Pham, Raffaella Bernardi, Sandro Pezzelle, Marco Baroni, Gemma Boleda, and Raquel Fernández. The lambada dataset: Word prediction requiring broad discourse context. In Proceedings of the 54th annual meeting of the association for computational linguistics (volume 1: Long papers), pages 15251534, 2016. [20] Bo Peng, Daniel Goldstein, Quentin Gregory Anthony, Alon Albalak, Eric Alcaide, Stella Biderman, Eugene Cheah, Teddy Ferdinan, Kranthi Kiran GV, Haowen Hou, Satyapriya Krishna, Ronald McClelland Jr., Niklas Muennighoff, Fares Obeid, Atsushi Saito, Guangyu Song, Haoqin Tu, Ruichong Zhang, Bingchen Zhao, Qihang Zhao, Jian Zhu, and Rui-Jie Zhu. Eagle and finch: RWKV with matrix-valued states and dynamic recurrence. In First Conference on Language Modeling, 2024. [21] Zihan Qiu, Zekun Wang, Bo Zheng, Zeyu Huang, Kaiyue Wen, Songlin Yang, Rui Men, Le Yu, Fei Huang, Suozhi Huang, Dayiheng Liu, Jingren Zhou, and Junyang Lin. Gated attention for large language models: Non-linearity, sparsity, and attention-sink-free. In Advances in Neural Information Processing Systems (NeurIPS), 2025. NeurIPS 2025 oral; also available as arXiv:2505.06708. [22] Zihuan Qiu, Yi Xu, Chiyuan He, Fanman Meng, Linfeng Xu, Qingbo Wu, and Hongliang Li. Mingle: Mixture of null-space gated low-rank experts for test-time continual model merging. In Advances in Neural Information Processing Systems (NeurIPS), 2025. NeurIPS 2025 poster; also available as arXiv:2505.11883. [23] Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, and Peter Liu. Exploring the limits of transfer learning with unified text-to-text transformer. Journal of machine learning research, 21(140):167, 2020. [24] Hubert Ramsauer, Bernhard Schäfl, Johannes Lehner, Philipp Seidl, Michael Widrich, Lukas Gruber, Markus Holzleitner, Thomas Adler, David Kreil, Michael Kopp, Günter Klambauer, Johannes Brandstetter, and Sepp Hochreiter. Hopfield networks is all you need. In International Conference on Learning Representations, 2021. [25] Rajesh PN Rao and Dana Ballard. Predictive coding in the visual cortex: functional interpretation of some extra-classical receptive-field effects. Nature neuroscience, 2(1):7987, 1999. [26] Wolfram Schultz, Peter Dayan, and Read Montague. neural substrate of prediction and reward. Science, 275(5306):15931599, 1997. [27] Jay Shah, Ganesh Bikshandi, Ying Zhang, Vijay Thakkar, Pradeep Ramani, and Tri Dao. Flashattention-3: Fast and accurate attention with asynchrony and low-precision. In The Thirty-eighth Annual Conference on Neural Information Processing Systems, 2024. [28] Haizhou Shi, Zihao Xu, Hengyi Wang, Weiyi Qin, Wenyuan Wang, Yibin Wang, Zifeng Wang, Sayna Ebrahimi, and Hao Wang. Continual learning of large language models: comprehensive survey. ACM Computing Surveys, 58(5):142, 2025. [29] Yuhang Song, Beren Millidge, Tommaso Salvatori, Thomas Lukasiewicz, Zhenghua Xu, and Rafal Bogacz. Inferring neural activity before plasticity as foundation for learning beyond backpropagation. Nature neuroscience, 27(2):348358, 2024. [30] Benjamin Thérien, Charles-Étienne Joseph, Zain Sarwar, Ashwinee Panda, Anirban Das, Shi-Xiong Zhang, Stephen Rawls, Sambit Sahu, Eugene Belilovsky, and Irina Rish. Continual pre-training of moes: How robust is your router? Transactions on Machine Learning Research, 2025. [31] Renzhi Wang and Piji Li. Lemoe: Advanced mixture of experts adaptor for lifelong model editing of large language models. In Proceedings of the 2024 Conference on Empirical Methods in Natural Language Processing, pages 25512575, 2024. [32] Xun Wu, Shaohan Huang, and Furu Wei. Mixture of loRA experts. In The Twelfth International Conference on Learning Representations, 2024. [33] Mohammad Yaghoubi, Ganesh Kumar, Andres Nieto-Posadas, Coralie-Anne Mosser, Thomas Gisiger, Émmanuel Wilson, Cengiz Pehlevan, Sylvain Williams, and Mark Brandon. Predictive coding of reward in the hippocampus. Nature, pages 17, 2026. [34] Ted Zadouri, Ahmet Üstün, Arash Ahmadian, Beyza Ermis, Acyr Locatelli, and Sara Hooker. Pushing mixture of experts to the limit: Extremely parameter efficient moe for instruction tuning. In The Twelfth International Conference on Learning Representations, 2024. 13 [35] Zheng Zhan, Liliang Ren, Shuohang Wang, Liyuan Liu, Yang Liu, Yeyun Gong, Yanzhi Wang, and Yelong Shen. Routing mamba: Scaling state space models with mixture-of-experts projection. In Advances in Neural Information Processing Systems (NeurIPS), 2025. NeurIPS 2025 poster. [36] Jia-Chen Zhang, Yu-Jie Xiong, Xi-He Qiu, Chun-Ming Xia, Fei Dai, and Zheng Zhou. Mixture of routers. arXiv preprint arXiv:2503.23362, 2025. [37] Junhao Zheng, Shengjie Qiu, Chengming Shi, and Qianli Ma. Towards lifelong learning of large language models: survey. ACM Computing Surveys, 57(8):135, 2025. [38] Tong Zhu, Xiaoye Qu, Daize Dong, Jiacheng Ruan, Jingqi Tong, Conghui He, and Yu Cheng. Llama-moe: Building mixture-of-experts from llama with continual pre-training. In Proceedings of the 2024 conference on empirical methods in natural language processing, pages 1591315923, 2024."
        },
        {
            "title": "A Technical Derivations and Implementation Details",
            "content": "This appendix records implementation details needed for exact reproduction and clarifies points that are easy to misread from the compact method description. It also summarizes the optimization protocol and the ablation controls exposed by the code. We log full run configurations and metrics to Weights & Biases. A.1 Predictive pathway: causal one-step-ahead convolution"
        },
        {
            "title": "The predictive pathway operates on the normalized token representation",
            "content": "U RBT d, and uses depthwise 1D convolution followed by pointwise 1D convolution. Let RBdT denote the channel-first view used by the implementation. Let the depthwise kernel width be kpc. The code implements causal one-step-ahead predictor by left-padding by kpc, applying the depthwise convolution, and then dropping the final output position: (A.1) (A.2) (A.3) (A.4) (cid:101)U = DWConvpc (cid:0)PadLeft(U , kpc)(cid:1) :,:,1:T . pointwise convolution then produces the predictor output ˆP = PWConvpc( (cid:101)U ), ˆP RBT d. The predictive reconstruction loss used by the block is Lpred = λpc MSE (cid:16) ˆP:,2:T,:, stopgrad(U:,2:T,:) (cid:17) , The predictive blend used as cortex input is ˆU = (1 spred) , where = (cid:40) ˆP , stopgrad( ˆP ), if predictive blending allows gradient flow, otherwise. If the modulation controller is disabled, the implementation uses the fixed blending value spred = 0.5. A.2 Chunked routing and padded execution Routing is performed at chunk resolution. Let be the routing chunk size and nc = (cid:25) . (cid:24) If is not divisible by C, the code pads by repeating the last valid token representation: ˆUpad = (cid:40) ˆU , (cid:2) ˆU ; ˆU:,T,: repeated (ncC ) times(cid:3), if = ncC, otherwise, (A.5) then reshapes to ˆUchunk RBncCd. This padding choice matters because the padded values are not zeros. Chunk summaries are computed by either first-position pooling or mean pooling: Ub,c = (cid:40) ˆUchunk[b, c, 1, :] (first-position pooling), 1 (cid:80)C τ =1 ˆUchunk[b, c, τ, :] (mean pooling). (A.6) Router logits and topology term. Let dr denote the router width and let be the number of columns. The router computes RBncdr , = (r) (r) RM dr , Lbase = Q(K (r)) RBncM . If topology-aware routing is enabled, the code predicts 2D chunk coordinates πb,c = tanh( Ub,cWpos + bpos) R2, and uses fixed column coordinates Pm R2 stored as buffers. The topology penalty is Ltopo b,c,m = γπb,c Pm2 2. We compute the squared distance with 2 = πb,c2 πb,c Pm2 using precomputed buffer for Pm2 2. 2 + Pm2 2 2πb,c, Pm, (A.7) (A.8) (A.9) (A.10) (A.11) (A.12) Routing-logit modulation and top-k selection. scalar sroute [0, 1] scales the router logits: If routing modulation is enabled, sequence-level aroute = 1 + ρroute(2sroute 1), and the final logits are Lb,c,m = aroute (cid:16) b,c,m + 1topoLtopo Lbase b,c,m (cid:17) . The code then computes top-k indices and selected logits and routing weights by softmax over the selected values: Ib,c,1:k = TopK(Lb,c,:, k), Sb,c,j = Lb,c,Ib,c,j , Rb,c,j = exp(Sb,c,j) j=1 exp(Sb,c,j) (cid:80)k . (A.13) (A.14) (A.15) (A.16) These routing decisions are shared by all token positions in the chunk. Routing auxiliary term. When routing regularization is enabled, the implementation scatters the selected weights back into dense tensor Mimp RBncM , with Mimp[b, c, m] = (cid:88) j=1 1[Ib,c,j = m] Rb,c,j. (A.17) Summing across batch and chunks gives column-importance vector nc(cid:88) (cid:88) um = Mimp[b, c, m], pm = um m=1 um + ε (cid:80)M The routing auxiliary loss is b=1 c=1 Lroute = λlb (cid:88) m= p2 m. . (A.18) (A.19) A.3 Parallel cortical computation The proposed method does not maintain persistent token-by-token recurrent state across the full sequence. Instead, it performs fully tensorized routed computation within chunks, using causal convolutions along the within-chunk token axis and separate causal convolution along the chunk axis. 16 Dense projection and routed gather. For each padded token representation ˆUpad[b, t, :], shared projection emits parameters for all columns: Proj( ˆUpad[b, t, :]) RM (3n+3), (A.20) where is the cortical state width. After reshaping, the raw tensor has shape Praw RBncCM (3n+3). Using the chunk-level routed indices I, the code gathers only the selected columns:"
        },
        {
            "title": "This tensor is split into",
            "content": "Psel RBncCk(3n+3). coef , Bin, Cin RBncCkn, g1, g2, g3 RBncCk, where the three gates are obtained by applying sigmoid to the final three channels: gstate = σ(g1), gout = σ(g2), gdis = σ(g3). (A.21) (A.22) (A.23) Excitatory-inhibitory gate remapping. When excitatory-inhibitory gating is enabled, the third gate acts as disinhibitory controller that modifies the first two gates: gstate (1 gdis) gstate + gdis, gout (1 gdis) gout + gdis. (A.24) (A.25) The state-related tensors are then scaled by gstate: coef gstate coef , Bin gstate Bin, Cin gstate Cin. (A.26) If this option is disabled, the remapping is skipped and the raw sigmoid gates are used. Adaptive state coefficients and within-chunk causal filtering. Each column has learned parameter tensor Alog RM n. Then Abase = σ(Alog) (0, 1)M n, (A.27) gathers the selected rows using I, and obtains Asel RBnckn. Broadcasting over the token axis yields the token-dependent coefficient α = σ(coef ) A(bcast) sel RBncCkn. The driven state signal is Dstate = (1 α) Bin. (A.28) (A.29) To apply causal filtering within each chunk, the tensor is permuted and reshaped to merge batch, chunk, and routed-column axes: Dflat R(Bnck)nC. We apply depthwise 1D convolution followed by pointwise 1D convolution along the length-C axis: Hflat = PWConvmem(DWConvmem(PadLeft(Dflat, kmem 1))) . (A.30) Reshaping back gives the filtered state tensor RBncCkn. This operation is causal only within chunks. Long-range propagation is handled separately at chunk resolution. 17 Readout with optional top-down gating. The bottom-up readout input is Bread = Cin H. (A.31) If the top-down gated readout is enabled and associative memory is active, let mem RBncd be the retrieved chunk context and define its broadcast form"
        },
        {
            "title": "C mem",
            "content": "bcast RBnc11d."
        },
        {
            "title": "The readout is",
            "content": "Sread = Wbot Bread RBncCkd, Gtop = σ(Wgate ϕ(Wtop mem Ysel = RMSNorm(Sread (1 + Gtop)) , bcast)) RBnc11d, with ϕ = SiLU. If top-down gated readout is disabled, the method uses linear readout: Ysel = Wout Bread. (A.32) (A.33) (A.34) (A.35) Output gate, skip connection, and routed mixture. The selected-column outputs are scaled by the output-control gate: Ysel Ysel g(bcast) out . (A.36) If the skip connection is enabled, the block uses learned scalar per column, RM , gathers the selected entries using I, and adds gated skip from the chunk input: Ysel Ysel + tanh(sI )(bcast) ˆU (bcast) chunk . (A.37) The routed chunk output is then formed by the weighted sum over the selected columns: Ychunk[b, c, τ, :] = (cid:88) j=1 Rb,c,j Ysel[b, c, τ, j, :]. (A.38) Chunk-level lateral propagation. We compute chunk summaries by averaging over the token axis: Cctx[b, c, :] = 1 (cid:88) τ =1 Ychunk[b, c, τ, :]. If associative memory is enabled, the retrieved context is added: Cctx Cctx + mem. (A.39) (A.40) causal depthwise 1D convolution and pointwise 1D convolution are then applied along the chunk axis: (A.41) After transposing back, the result is broadcast across the token axis and added to the chunk outputs: ctx = PWConvlat (cid:0)PadLeft(C ctx, klat 1)(cid:1)(cid:1) . (cid:0)DWConvlat (cid:101)C Finally, the chunk tensor is reshaped back to (B, ncC, d) and trimmed to the original length . Ychunk Ychunk + (cid:101)C (bcast) ctx . (A.42) checkpointing. Activation torch.utils.checkpoint.checkpoint (non-reentrant mode) with exact gradients. reduces activation memory without changing the forward computation. is wrapped with This enabled, function cortex the If 18 A.4 Associative memory and routing-weight refinement Associative-memory retrieval. The associative-memory module operates on chunk summaries"
        },
        {
            "title": "It stores ns learned memory slots",
            "content": "U RBncd. Ξ Rnsdh , and uses learned projections into the memory space of width dh: Qmem = (mem) Rmem = (retrieval in memory space). RBncdh, We normalize both projected queries and memory slots: (cid:101)Qmem = normalize(Qmem), (cid:101)Ξ = normalize(Ξ). With inverse temperature β, retrieval weights are Amem = softmax (cid:16) β (cid:101)Qmem (cid:101)Ξ(cid:17) RBncns, and the retrieved memory vector is Rmem = Amem (cid:101)Ξ RBncdh. (A.43) (A.44) (A.45) (A.46) (A.47) (A.48) final projection and RMS normalization produce the chunk-level context used by the cortex: mem = RMSNorm(RmemW (mem) ) RBncd. (A.49) Routing-weight refinement (cortico-thalamic feedback). When enabled, the block first computes cortex output RBT d, then pools it to chunk resolution and maps it back to router space. We pad to length ncC using zeros (not repeated-token padding), reshape to chunks, and average over the positions: The feedback router scores are Yb,c,: = 1 (cid:88) τ =1 Ypad[b, c, τ, :]. Qfb = Wfb RBncdr , Lfb = Qfb(K (r)) RBncM . Only the already-selected columns are gathered: b,c,j = Lfb[b, c, Ib,c,j] RBnck. Sfb (A.50) (A.51) (A.52) (A.53) learned scalar parameter is passed through tanh to obtain bounded mixing coefficient and the refined routing weights are αfb = tanh(cmix) (1, 1), b,c,: = softmax(cid:0)Sb,c,: + αfb Sfb b,c,: (cid:1) . (A.54) The top-k indices are not recomputed. key implementation detail is that we then execute the cortex second time with the same routing support and refined weights R. This is full second cortex call, not lightweight reweighting of cached selected-column outputs. 19 A.5 Model wrapper, auxiliary losses, and trainer objective The model wrapper stacks decoder blocks, applies final RMS normalization, and uses tied output projection. At each layer, the wrapper accumulates the routing auxiliary term and the predictive reconstruction term when present: LΣ route = (cid:88) ℓ=1 L(ℓ) route, LΣ pred = (cid:88) ℓ=1 L(ℓ) pred. (A.55) The wrapper returns the token cross-entropy loss, logits, and the accumulated auxiliary quantities. In the provided training script, the total optimization objective is formed explicitly as Ltrain = LCE + wpred LΣ pred + wrouter LΣ route. (A.56) We use wpred = 0.1 and wrouter = 0.1 in the trainer. A.6 Main training configuration This subsection records the optimization and training configuration details. Hardware and precision. Runs use single node with 4 NVIDIA V100 GPUs (each 32GB) and fp16 mixed precision with gradient scaling. Optimization and schedule. The trainer uses AdamW with learning rate 2 104, weight decay 0.1, and (β1, β2) = (0.9, 0.95). The learning-rate schedule is linear warmup for 1,000 optimizer steps followed by cosine decay, indexed by optimizer steps (not micro-steps). Gradient clipping is applied with threshold 1.0. We use: batch size per GPU = 8, gradient accumulation = 4, world size = 4, = 1024. This gives an effective global batch of sequences per optimizer step, or tokens per optimizer step. 8 4 4 = 128 128 1024 = 131, With 22,000 optimizer steps, the total training budget is 22,000 131,072 = 2,883,584,000 tokens (approximately 2.88B tokens). Data and tokenization. We use the EleutherAI/gpt-neox-20b tokenizer, sequence length 1024, streaming training and evaluation. The training dataset is allenai/c4:en (train split). The evaluation probes are: allenai/c4:en (validation split), wikitext:wikitext-103-v1, EleutherAI/lambada_openai. The configuration caps the number of training and evaluation samples at 3,000,000 and 300,000, respectively. The data loader uses 8 workers. Logging and checkpointing. Training metrics are logged every 20 optimizer steps. Evaluation runs every 500 optimizer steps. The trainer saves only improved best checkpoints according to the selected validation criterion, and it defaults to average perplexity. 20 Model hyperparameters. Model hyperparameters are read from the model section of the experiment configuration and are logged with the run metadata. The code supports multiple backbones (trc2, neurocognitive, transformer, mamba, and moe) through the same training pipeline. For the neurocognitive TRC2 block used in the main method, the exact architectural options are controlled by booleans for routing topology, excitatory-inhibitory gating, skip connections, modulation controller, predictive pathway, associative memory, top-down gated readout, routing-weight refinement, and the cerebellar corrective path."
        },
        {
            "title": "B Complexity",
            "content": "Let be batch size, sequence length, model width, routing chunk size, and nc = /C the number of routing chunks. Let be the number of cortical columns, the routed columns per chunk, the cortical state width (n_state), dr the router width, dh the hippocampal memory width (d_memory), ns the number of hippocampal slots, dz the cerebellar hidden width, and the cerebellar low-rank width (fast_rank). Modulation controller. The neuromodulator path computes batch and per-sequence statistics over RBT and applies small MLP on 4d input per sequence. Its cost is O(BT d) + O(Bd dnm), where dnm is the hidden size of the neuromodulator MLP. This term is small relative to the main routed cortex path for the default settings. Predictive coding. The predictive path applies causal depthwise 1D convolution and pointwise 1D convolution over the full token sequence, both at width d, followed by an MSE loss: O(BT kpc) + O(BT d2) + O(BT d), where kpc is the predictive kernel width. The pointwise convolution term O(BT d2) is usually the leading term inside this subsystem. Chunked sparse router. Routing is performed on chunk summaries, so the router runs over nc positions instead of . The base router cost is O(Bncddr) + O(BncdrM ), for the query projection and dense logits over columns. If topology is enabled, the additional cost is O(Bncd) + O(BncM ), from the 2D position projection and distance-to-column computations. Top-k selection and the top-k softmax are then applied per chunk over the logits. Associative memory. The associative memory module also runs at chunk resolution. Its cost is O(Bncddh) + O(Bncnsdh) + O(Bncnsdh) + O(Bncdhd), which corresponds to the query projection, Hopfield score computation, Hopfield retrieval, and projection back to model width. The two middle terms come from QhΞ and (softmax())Ξ. Cortical field, one pass. The cortex is the main computation in the block. For one cortex pass, the cost has two parts. (1) Dense token-to-column projection. Each padded token is projected to all columns: This is dense in and is often major cost term. O(BT (3n + 3)) . (2) Routed selected-column path. After gathering the routed columns, the selected-column path scales with rather than . It includes: gather and parameter splitting for selected columns, with tensor sizes proportional to BncCk(3n + 3), 21 membrane filtering within chunks using depthwise 1D convolution and pointwise 1D convolution on width n: O(Bnck kmem) + O(Bnck n2), readout from state width to model width d: O(BncCk nd), both for the plain readout and for the basal branch of the dendritic readout, routed weighted mixing across the selected columns: O(BncCkd), chunk-level lateral propagation using depthwise and pointwise convolutions on chunk summaries: O(Bncd klat) + O(Bncd2). When the readout is enabled, the gating path adds chunk-level cost (not multiplied by Ck in the linear layers because the signal is broadcast): plus broadcasted elementwise operations over the selected-token tensor. O(Bnc dap) + O(Bnc dap d), Corrector feedback. The feedback path pools the first cortex output to chunk resolution, projects it to router space, computes logits over all columns, gathers feedback scores for the already-selected columns, and forms refined top-k weights: O(BT d) + O(Bncddr) + O(BncdrM ) + O(Bnck). It does not run second top-k search. It does, however, call the cortex second time with the same indices and new weights. Therefore, enabling feedback adds approximately one extra cortex pass (including the dense token-to-column projection) plus the chunk-level feedback projection above. Low-rank corrective pathway. The corrective path computes two linear terms into width dz, applies SiLU, and then low-rank projection: O(BT 2ddz) + O(BT dz) + O(BT dzr) + O(BT dr). The O(BT 2ddz) term comes from the split implementation of the linear map on (U, ), which is equivalent to single linear layer on [U ; ] but avoids explicitly materializing the concatenation. Summary. The block is sparse in the routed column dimension after routing, but it still contains dense token-to-column projection to produce per-column parameters. In one cortex pass, this dense projection scales as O(BT (3n + 3)) , while the routed computations scale with k. With corrective feedback enabled, the cortex is executed twice with the same selected indices, which roughly doubles the cortex-side cost without repeating the top-k search."
        }
    ],
    "affiliations": [
        "SnT, University of Luxembourg",
        "W&B"
    ]
}