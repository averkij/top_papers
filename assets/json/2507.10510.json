{
    "paper_title": "Chat with AI: The Surprising Turn of Real-time Video Communication from Human to AI",
    "authors": [
        "Jiangkai Wu",
        "Zhiyuan Ren",
        "Liming Liu",
        "Xinggong Zhang"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "AI Video Chat emerges as a new paradigm for Real-time Communication (RTC), where one peer is not a human, but a Multimodal Large Language Model (MLLM). This makes interaction between humans and AI more intuitive, as if chatting face-to-face with a real person. However, this poses significant challenges to latency, because the MLLM inference takes up most of the response time, leaving very little time for video streaming. Due to network uncertainty and instability, transmission latency becomes a critical bottleneck preventing AI from being like a real person. To address this, we propose Artic, an AI-oriented Real-time Communication framework, exploring the network requirement shift from \"humans watching video\" to \"AI understanding video\". To reduce bitrate dramatically while maintaining MLLM accuracy, we propose Context-Aware Video Streaming that recognizes the importance of each video region for chat and allocates bitrate almost exclusively to chat-important regions. To avoid packet retransmission, we propose Loss-Resilient Adaptive Frame Rate that leverages previous frames to substitute for lost/delayed frames while avoiding bitrate waste. To evaluate the impact of video streaming quality on MLLM accuracy, we build the first benchmark, named Degraded Video Understanding Benchmark (DeViBench). Finally, we discuss some open questions and ongoing solutions for AI Video Chat."
        },
        {
            "title": "Start",
            "content": "Chat with AI: The Surprising Turn of Real-time Video Communication from Human to AI Jiangkai Wu, Zhiyuan Ren, Liming Liu, Xinggong Zhang* Peking University 5 2 0 2 4 1 ] . [ 1 0 1 5 0 1 . 7 0 5 2 : r ABSTRACT AI Video Chat emerges as new paradigm for Real-time Communication (RTC), where one peer is not human, but Multimodal Large Language Model (MLLM). This makes interaction between humans and AI more intuitive, as if chatting face-to-face with real person. However, this poses significant challenges to latency, because the MLLM inference takes up most of the response time, leaving very little time for video streaming. Due to network uncertainty and instability, transmission latency becomes critical bottleneck preventing AI from being like real person. To address this, we propose Artic, an AI-oriented Real-time Communication framework, exploring the network requirement shift from \"humans watching video\" to \"AI understanding video\". To reduce bitrate dramatically while maintaining MLLM accuracy, we propose Context-Aware Video Streaming that recognizes the importance of each video region for chat and allocates bitrate almost exclusively to chat-important regions. To avoid packet retransmission, we propose Loss-Resilient Adaptive Frame Rate that leverages previous frames to substitute for lost/delayed frames while avoiding bitrate waste. To evaluate the impact of video streaming quality on MLLM accuracy, we build the first benchmark, named Degraded Video Understanding Benchmark (DeViBench). Finally, we discuss some open questions and ongoing solutions for AI Video Chat."
        },
        {
            "title": "1 INTRODUCTION\nAI Video Chat is a new paradigm for Real-time Commu-\nnication (RTC). Since OpenAI released GPT-4o [13], Multi-\nmodal Large Language Models (MLLMs) have been continu-\nously emerging, such as Qwen2.5-Omni [31], Gemini-1.5 [26],\nVITA-1.5 [11], and OmniLive [35]. Compared to traditional\nLLMs, MLLMs enable users to directly input video and audio\nfor interaction, rather than just text. This makes the inter-\naction between humans and AI more intuitive as if chatting\nface-to-face with a real person [6]. However, MLLMs require\nhigh-performance computing devices to support real-time\ninference (such as 8*A100 GPUs [22]). Mobile devices (like\nphones or smart glasses) cannot meet the computing require-\nments, which makes MLLMs inevitably deployed in the cloud.\nSo in existing systems, the client sends user video and audio\nto the cloud for MLLM inference, and the cloud then feeds\nback responses to users, as shown in Figure 1.",
            "content": "Figure 1: AI Video Chat is new paradigm for real-time communication. The user sends video and audio to the AI for thinking. The AI feeds back to the user. Low latency is crucial for making AI act like real person. AI Video Chat raises significant challenges to RTC transmission latency. To ensure fluent interactive experience, the end-to-end response latency of video chat needs to remain below 300 ms [15]. In traditional video chat, the human peer on the other side can respond instantly. Therefore, transmission latency accounts for the vast majority of end-to-end response latency. To reduce transmission latency, current state-of-theart RTC frameworks (such as WebRTC [5]) adopt technologies like Adaptive Bitrate (ABR) [2, 12, 14, 18, 32], Congestion Control [4, 9, 10, 20], and Forward Error Correction (FEC) [3, 8, 16, 19, 24] to satisfy user experience. However, in AI Video Chat, responses are generated through MLLM in an autoregressive manner, which is time-consuming. Even when inputting only audio tokens, the computational latency is at least 232 ms [13]. To constrain the response latency below 300 ms, the time left for transmission is at most 68 ms, which is difficult to guarantee. So the large latency makes users clearly feel that the other side is not real person. Is it possible to reduce the latency of AI Video Chat to an extremely low level? This vision allows us to grasp the \"holy grail\" [6] of AI research from the perspective of network systems: making AI like real humans. To realize this vision, we propose Artic, an AI-oriented Real-time Communication framework, exploring the network requirement shift from \"humans watching video\" to \"AI understanding video\". We begin by recognizing the main differences between AI Video Chat and traditional RTC: First, QoE changes from human perceptual quality to MLLM response accuracy. Second, the receiver throughput is far lower than the sender throughput. Third, uplink is more pressing than Downlink. Then, through prototype measurements, we identify 1 two key factors for low latency: ultra-low bitrate and loss resilience. Based on three insights, we make contributions: Video should be Context-Aware (3.2). To reduce bitrate dramatically while maintaining MLLM accuracy, we propose Context-Aware Video Streaming, allocating more bitrate to chat-important video regions while allocating as little bitrate as possible to chat-irrelevant regions. Frame Rate is FEC (3.3). Since MLLMs process videos at very low frame rate, most received frames are redundant. Although redundant frames cause bitrate waste, they can also substitute for lost/delayed frames. So we propose LossResilient Adaptive Frame Rate to simultaneously minimize bitrate waste and packet retransmission. The First Benchmark (3.1). Considering that there is no benchmark that can evaluate the impact of video quality on MLLM accuracy, we propose the first one, named Degraded Video Understanding Benchmark (DeViBench)."
        },
        {
            "title": "Chat and traditional RTC",
            "content": "QoE changes from human perceptual quality to MLLM response accuracy. In traditional RTC, QoE focuses on measuring the perceptual quality of human eyes. For example, using penalty terms like stalling time [8, 32] or quality variance [7] to measure temporal stability. Using SSIM [32] or VMAF [1] to measure visual quality. However, in AI Video Chat, the viewer of the video changes from humans to MLLMs. At this point, most perception-related metrics are no longer needed, and the optimization objective of QoE becomes the accuracy and latency of MLLMs responses. So RTC strategy can undergo significant turn. For example, to reduce latency, temporal stability (such as frequent bitrate adjustments) and visual quality (like lowering bitrate in Figure 3) can be sacrificed, as long as accuracy is enough. The receiver throughput is far lower than the sender throughput. In traditional RTC, the data throughput at the receiver is comparable to that at the sender, for example, both are 1920*1080 resolution at 30 FPS frame rate. However, in AI Video Chat, MLLM is limited by context length (finite number of tokens) and cannot fully process the received video. Therefore, the received video needs to be actively downsampled before being fed to the MLLM. In terms of frame rate, existing AI Video Chat systems support maximum processing rate of only 2 FPS [11, 29, 31]. In terms of resolution, regardless of how high the original resolution is, it will be downsampled to no more than 602,112 pixels [31]. Uplink is more pressing than Downlink. In traditional RTC, each peer is both video sender and receiver. In contrast, AI Video Chat is unidirectional video transmission, where Jiangkai Wu et al. Figure 2: How bitrate and packet loss affect latency (with 10 Mbps bandwidth). To optimize video quality, traditional RTC systems select bitrate from the gray region. But in AI video chat, to maintain accuracy, we only need to select bitrate from the yellow region (2.2). the user only acts as the video sender and MLLM only acts as the video receiver. MLLM sends responses to the user in the form of audio or text, and these representations have much lower bitrates than video. Thus, uplink needs better network conditions than downlink, for example, larger bandwidth."
        },
        {
            "title": "2.2 What factors affect the transmission",
            "content": "latency of AI Video Chat? To analyze the factors affecting transmission latency in AI Video Chat, we build prototype and conduct preliminary measurements. Specifically, we implement WebRTC-based unidirectional video transmission system and network emulator. Under given bandwidth (10 Mbps) and one-way network delay (30 ms), we run video transmission for total duration of 40,489 seconds, and collect statistics on frame latency (the time difference from the frame being sent to being completely received) with different packet loss rates and bitrates, as shown in Figure 2. The results indicate: First, when the bitrate exceeds the bandwidth, frame latency becomes enormous. This is because excessive bitrate causes congestion, where packet accumulation causes latency to increase rapidly. Therefore, existing RTC systems employ ABR algorithms to set the bitrate as close as possible to (but below) the bandwidth, maximizing video quality while avoiding stalling, as shown in the grey region of Figure 2. Second, when the bitrate does not exceed the bandwidth, frame latency also increases as the bitrate increases. This is due to the fact that each packet has limited size (around 1400 bytes). higher bitrate means each frame will be divided into more packets. Due to packet loss, more packets mean the probability of each frame being completely received in one attempt decreases. For packets that are not received, retransmission may be required, leading to increased latency. Therefore, even when the bitrate is below 2 Chat with AI: The Surprising Turn of Real-time Video Communication from Human to AI Figure 3: Why video should be context-aware in AI Video Chat. In the first dialogue, even if the video bitrate decreases from 4000 Kbps to 200 Kbps, the MLLM can still response accurately. But in the second dialogue, the blurry video leads to incorrect responses. Therefore, rather than reducing bitrate in contextagnostic manner, bitrate allocation should be determined by the current chat context (2.3). the bandwidth, AI Video Chat can further reduce the bitrate to achieve lower latency. This differs from traditional ABR and offers another space for bitrate selection, as shown in the yellow region of Figure 2. Third, when the packet loss rate increases, frame latency increases. Similarly, this is also caused by more retransmissions."
        },
        {
            "title": "2.3 Key Insights and Potential Gains\nVideo should be Context-Aware. According to Â§2.2 re-\nducing video bitrate can decrease transmission latency. To\nreduce bitrate, existing methods typically increase quantiza-\ntion parameters or lower resolution [25], which inevitably\ndegrades video quality. Interestingly, the degradation in video\nquality does not necessarily lead to a decrease in MLLM accu-\nracy, which depends on the current chat context. As illustrated\nin Figure 3, when the user asks \"Could you tell me the present\nscore of the game?\", even if the video bitrate is reduced from\n4000 Kbps to 200 Kbps, the MLLM can still answer accurately.\nHowever, when the user asks \"What logo is seen on the jer-\nsey of the player covering his mouth?\", the blurry video leads\nto incorrect responses. This is because, in different chat con-\ntexts, the MLLM needs to focus on different video regions.\nMeanwhile, different video regions are affected differently by\nlow bitrate. Thus, rather than reducing bitrate in a context-\nagnostic manner, the video should be context-aware. More\nbitrate should be allocated to chat-important regions, while\nless bitrate should be allocated to chat-irrelevant regions.",
            "content": "How to be aware of the chat context? Our idea is: the user words can indicate which video regions are important for 3 Figure 4: Frame Rate is FEC. MLLM processes video at very low frame rate (green region), so higher frame rates will introduce redundancy (red region). Higher frame rates on one hand cause bitrate waste, while on the other hand enhance loss resilience (2.3). the current chat. Therefore, we can take the user words as reference to compute the semantic correlation of different video regions. For this, we adopt the Contrastive LanguageImage Pre-Training (CLIP) model [23], which maps images and language to the same feature space. Hence, to derive semantic correlation, we only need to compute the similarity of features between video regions and user words. We show some examples in Figure 5, which demonstrates that user words and CLIP can well point out the importance of different video regions for chatting. For example, when the user asks \"Is the dog in the video erect-eared or floppy-eared?\", the dogs head region exhibits the highest correlation. On the other hand, even when the user words do not explicitly indicate the object, CLIP can still estimate correlation based on high-level understanding. For example, when the user asks \"Infer what season it might be in the video\", grass has the highest correlation. This is because the growth of grass can imply the current season (CLIP even ignores the blurry grass in the distance). Thus, this context-aware mechanism allows us to optimally allocate the bitrate (3.2). Frame Rate is FEC. According to 2.2, besides bitrate, packet loss rate also has an important impact on latency. We find that frame rate can make trade-offs between bitrate and packet loss rate in existing AI Video Chat systems. As described in 2.1, MLLM processes videos at very low frame rates (such as 2 FPS), while the client still transmits videos at conventional frame rates (like 30 FPS). After receiving the video, MLLM will sample frames at uniform time interval (such as 500 ms) for processing, while the remaining frames are not used. This causes most received video frames to be redundant. So to reduce video bitrate, the most reasonable solution is to use the same low frame rate (2 FPS) at the sender. Figure 4 shows the relationship between frame rate Jiangkai Wu et al. Figure 5: How to achieve context awareness? The user words can indicate which regions in the video are important for the current chat context. Based on CLIP, we can even recognize important regions through high-level understanding. For example, in the third dialogue, the growth of grass implies the current season (2.3). benchmarks, each video includes several Question-Answer (QA) samples for evaluating the response accuracy. However, these benchmarks aim to test the MLLMs intelligence, so all the input videos are ideally high-bitrate (e.g., 4000 Kbps). To evaluate how video streaming quality affects accuracy, we transcode videos from StreamingBench to 200 Kbps. Then we conduct testing on these low-bitrate videos with the original QA samples. The results show that only 8% of QA samples are answered incorrectly at low bitrate and correctly at high bitrate. This is because the QA samples in StreamingBench are too simple and high-level, requiring only coarse-grained video content to answer correctly. For example, in Figure 3, when the question is \"What is the player doing?\", even if the video quality is particularly poor, the MLLM can still provide the correct answer \"He is covering his mouth.\" However, in real-world scenarios, there are often many detail-rich questions that are very sensitive to video quality. For example, in Figure 3, when the question is \"How many spectators can be seen?\", even slight blurriness will prevent the MLLM from providing the correct answer. So it is necessary to establish more challenging benchmark to reflect the real-world impact of video degradation on MLLM accuracy (3.1)."
        },
        {
            "title": "3.1 DeViBench\nIn this section, we propose DeViBench. As described in Â§2.3,\nwe need to construct QA samples that are sensitive to video\nquality. For this, the most straightforward way is to hire\nvolunteers to ask tricky questions about degraded videos.\nHowever, this is too expensive and inefficient, hindering the\nscale-up of the dataset. So we ask: Can QA samples be\nconstructed automatically and cheaply? Rethinking the\nbackground of AI Video Chat, MLLMs are already capable",
            "content": "Figure 6: How to achieve loss resilience? When the expected frame is not completely received on time, the MLLM directly discards it and uses the previous redundant frame, without waiting for retransmission. and video bitrate. It can be seen that lowering the frame rate can reduce bitrate while not affecting MLLM accuracy. In contrast to reducing bitrate, increasing the frame rate can enhance resilience to packet loss. As illustrated in Figure 6, rather than sending video at 2 FPS, the client can send at higher frame rate. When the expected frame (e.g., sampled at 500 ms intervals) is not received on time, the MLLM can directly discard it and use previous (redundant) frame, without waiting for retransmission. In this way, redundant frames can handle packet loss in an FEC manner. This presents trade-off: higher frame rates increase redundancy for better loss tolerance, but also lead to higher bitrates. So the frame rate should be selected based on loss rate (3.3). The first benchmark evaluates how video streaming quality affects MLLM accuracy. According to 2.1, QoE metrics in AI Video Chat change from perception to accuracy. This causes existing benchmarks in the video streaming field to be inapplicable, as they focus on perceptual quality and do not involve response accuracy. In the MLLM field, there are some benchmarks targeting Streaming Video Understanding tasks [29, 33], such as StreamingBench [17]. In these 4 Chat with AI: The Surprising Turn of Real-time Video Communication from Human to AI Figure 7: DeViBenchs pipeline for automatic QA sample construction. Details can be found in 3.1. of understanding videos and giving responses. So we leverage MLLMs to replace human volunteers and develop an automatic QA sample construction pipeline. As illustrated in Figure 7, this pipeline consists of 5 steps: Video Collection. We first collect videos to ask questions. To align with the domain and scale of existing MLLM benchmarks [17], we directly use their videos (discarding QA). Video Preprocessing. To allow MLLMs to understand the quality degradation caused by low bitrates, we transcode the original videos to low-bitrate versions (200 Kbps). The low bitrate video and the original video are horizontally concatenated into one video. Then this concatenated video will be input to the MLLM for understanding and QA generation. QA Generation. To enable MLLMs to generate QA pairs based on the concatenated video, we carefully designed prompt with guidance from persona, context, core task, execution steps, constraints, and output format, as shown in Figure 8. This prompt ensures that MLLMs can recognize quality differences and generate quality-sensitive QA pairs. QA Filtering. The generated QA pairs will be filtered. We separately input the original video and the low bitrate video into the MLLM and use the generated QA pairs for questioning. If the answer from the original video is correct and the answer from the low bitrate video is wrong, we accept this QA pair. In practice, 25.2% of the QA pairs can be accepted. Cross Verification. Since the answer generated by MLLM may also be incorrect, this cannot be filtered out through Figure 8: Our prompt for QA Sample Generation. the above testing. Hence, we utilize another MLLM for crossverification. We feed the above accepted question into another MLLM, and if the new answer is consistent with the above accepted answer, we finally approve this QA pair. In our experiments, 57.7% of the accepted QA pairs can pass cross-verification. Considering all the above validations together, finally 14.6% of the generated QA pairs are valid."
        },
        {
            "title": "3.2 Context-Aware Video Streaming\nIn this section, we describe how to achieve context-aware\nstreaming, significantly reducing bitrate while maintaining\nMLLM accuracy. According to Â§2.3, we first leverage the\nCLIP model to compute the semantic correlation between\nuser words and video regions. To ensure real-time computing\non mobile devices, we adopt Mobile-Clip [27]. Specifically,\ngiven the current user words T and the latest video frame ğ¹ âˆˆ\nRğ» Ã—ğ‘Š Ã—3, we first partition ğ¹ into non-overlapping patches\n{ğ‘ƒğ‘šğ‘› | 1 â‰¤ ğ‘š â‰¤ âŒŠğ» /ğ‘ âŒ‹ , 1 â‰¤ ğ‘› â‰¤ âŒŠğ‘Š /ğ‘ âŒ‹}, where each patch\nğ‘ƒğ‘šğ‘› âˆˆ Rğ‘ Ã—ğ‘ Ã—3 represents a video region. Then, the CLIP\nvisual encoder ğœ™ğ‘£ (Â·) : Rğ‘ Ã—ğ‘ Ã—3 â†’ Rğ‘‘ is employed to extract\npatch-wise features ğ‘“ ğ‘£\nğ‘šğ‘› = ğœ™ğ‘£ (ğ‘ƒğ‘šğ‘›), while the CLIP language\nencoder ğœ™ğ‘™ (Â·) : T â†’ Rğ‘‘ encodes the user words T into a\nsemantic features ğ‘“ ğ‘™ = ğœ™ğ‘™ (T ). Here ğ‘‘ denotes the unified\nfeature dimension. Semantic correlation ğœŒğ‘šğ‘› between user\nwords and patches are then computed as cosine similarities:",
            "content": "ğœŒğ‘šğ‘› = ğ‘šğ‘› ğ‘“ ğ‘™ ğ‘“ ğ‘£ ğ‘šğ‘› (cid:13) (cid:13)ğ‘“ ğ‘™ (cid:13) ğ‘“ ğ‘£ (cid:13) [1, 1] (1) Semantic correlation ğœŒğ‘šğ‘› can measure the importance of region ğ‘ƒğ‘šğ‘› for the current chat context. The larger ğœŒğ‘šğ‘› is, the more important ğ‘ƒğ‘šğ‘› is. So we can allocate more bitrate to important regions while allocating as little bitrate as possible to irrelevant regions. To achieve this, we adjust the Quantization Parameters (QP) of different regions during video Jiangkai Wu et al. Figure 9: Context-aware streaming can dramatically lower the bitrate while maintaining MLLM accuracy. encoding. When QP is larger (0 QP 51), the region occupies less bitrate, but the quality becomes worse. Specifically, for region ğ‘ƒğ‘šğ‘›, its QPğ‘šğ‘› is derived as: QPğ‘šğ‘› = 51 (cid:18) 1 (cid:18) ğœŒğ‘šğ‘› + 1 2 (cid:19)ğ›¾ (cid:19) (2) Where ğ›¾ is the temperature parameter, which we set 3 to aggressively penalizes irrelevant regions (ğœŒğ‘šğ‘› 1). We evaluate the performance gains in Figure 9. The results demonstrate that context-aware streaming can dramatically lower the bitrate while maintaining MLLM accuracy. For example, when the bitrate is reduced from 800 Kbps to 400 Kbps (50% reduction), the MLLM (Qwen2.5-Omni [31]) accuracy drops from 0.73 to 0.33. After integrating context-aware streaming, the accuracy only decreases from 0.93 to 0.87."
        },
        {
            "title": "3.3 Loss-Resilient Adaptive Frame Rate\nIn this section, we describe how to select the frame rate based\non the current loss rate, thereby jointly reducing redundancy\nand packet retransmission. According to Figure 6, when the\nexpected frame is not received, Artic uses a previous redun-\ndant frame as the substitute. So given the loss rate, we aim\nto minimize the frame rate while maximizing the probability\nthat at least one redundant frame is received on time.",
            "content": "Specifically, let ğ‘ğ‘¡ [0, 1] denote the packet loss rate during decision epoch ğ‘¡ (simplified to i.i.d. when the decision epoch is sufficiently small). Let ğœ…ğ‘¡ Z+represent the average number of packets per frame within epoch ğ‘¡. The probability of successfully receiving frame is then given by: ğ‘ƒğ‘“ = (1 ğ‘ğ‘¡ )ğœ…ğ‘¡ . (3) Let ğ‘…ğ‘€ denote the inference frame rate of the MLLM and let ğ‘…ğ‘¡ represent the video frame rate. So the MLLM samples one frame from every ğ¾ = ğ‘…ğ‘¡ /ğ‘…ğ‘€ frames. The probability of successfully receiving at least one frame in ğ¾ frames is: 6 Figure 10: Eliminating the latency caused by stalling, while avoiding the waste brought by high frame rates. ğ‘ƒğ‘  = 1 (1 ğ‘ƒğ‘“ )ğ¾ (4) To avoid retransmissions, it should satisfy ğ‘ƒğ‘  (1 ğœ–), where ğœ– = 0.001 is the residual term. Therefore, we finally select the minimum frame rate ğ‘…ğ‘¡ : ğ‘…ğ‘¡ ğ‘…ğ‘€ (cid:24) ln ğœ– ln (1 (1 ğ‘ğ‘¡ )ğœ…ğ‘¡ ) (cid:25) (5) We evaluate the performance gains in Figure 10. The preliminary results show that our method can reduce the latency caused by stalling from 25.3 ms to 1.2 ms, while avoiding the bitrate waste brought by high frame rates."
        },
        {
            "title": "4 DISCUSSIONS AND OPEN QUESTIONS\nProactive context-aware. In this paper, Artic leverages\nuser words to achieve context awareness. It may not nec-\nessarily perform well in practice. Because it requires user\nwords to be known before video encoding. But users may\nspeak at any moment in the video, causing user words not\nto cover some segments. For example, in some benchmarks\nlike [17, 29, 33], they assume that users ask questions at the\nend of the video. As our next step, we are building a proac-\ntive context-aware mechanism that can actively recognize\nimportant video regions even if users do not speak.\nMLLM long-term memory. To minimize bitrate, Artic\ndiscards most video content irrelevant to the current chat\ncontext. This is based on the assumption that the current\nchat only references real-time video content. However, some\nMLLMs have developed long-term memory mechanisms [21,\n28, 30], allowing chats to reference historical video content.\nSome video content, even if not relevant in the current chat\ncontext, may be needed in future chats. As our next step, we\nare developing a semantic layered video streaming framework.\nDifferent from SVC [25] that layers based on video quality,\nwe layer by semantic correlation. The base layer contains the\nmost important video content for the current chat context,",
            "content": "Chat with AI: The Surprising Turn of Real-time Video Communication from Human to AI so it must ensure low latency. The enhancement layers contain complete video details, used to offline build long-term memory, so they are not sensitive to latency. Token pruning. To further lower the end-to-end latency, it is necessary to reduce the inference latency of MLLMs. Since MLLMs run in an autoregressive manner, straightforward solution is to decrease the number of input tokens. Some related work exploits attention mechanisms [36] or video redundancy [34] to prune most visual tokens, without affecting MLLM accuracy. In this paper, context-aware streaming has already recognized important video regions, so it makes much sense to prune tokens from chat-irrelevant regions. As our next step, we are developing context-aware token pruning mechanisms to accelerate MLLM inference. REFERENCES [1] 2025. VMAF. https://github.com/Netflix/vmaf. [2] Zahaib Akhtar, Yun Seong Nam, Ramesh Govindan, Sanjay Rao, Jessica Chen, Ethan Katz-Bassett, Bruno Ribeiro, Jibin Zhan, and Hui Zhang. 2018. Oboe: Auto-tuning video ABR algorithms to network conditions. In Proceedings of the 2018 Conference of the ACM Special Interest Group on Data Communication. 4458. [3] Congkai An, Huanhuan Zhang, Shibo Wang, Jingyang Kang, Anfu Zhou, Liang Liu, Huadong Ma, Zili Meng, Delei Ma, Yusheng Dong, et al. 2025. Tooth: Toward Optimal Balance of Video {QoE} and Redundancy Cost by {Fine-Grained} {FEC} in Cloud Gaming Streaming. In 22nd USENIX Symposium on Networked Systems Design and Implementation (NSDI 25). 635651. [4] Neal Cardwell, Yuchung Cheng, Stephen Gunn, Soheil Hassas Yeganeh, and Van Jacobson. 2017. BBR: Congestion-based congestion control. Commun. ACM 60, 2 (2017), 5866. [5] Gaetano Carlucci, Luca De Cicco, Stefan Holmer, and Saverio Mascolo. 2016. Analysis and design of the google congestion control for web real-time communication (WebRTC). In Proceedings of the 7th International Conference on Multimedia Systems. 112. [6] Joya Chen, Zhaoyang Lv, Shiwei Wu, Kevin Qinghong Lin, Chenan Song, Difei Gao, Jia-Wei Liu, Ziteng Gao, Dongxing Mao, and Mike Zheng Shou. 2024. Videollm-online: Online video large language model for streaming video. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. 1840718418. [7] Tianyu Chen, Yiheng Lin, Nicolas Christianson, Zahaib Akhtar, Sharath Dharmaji, Mohammad Hajiesmaili, Adam Wierman, and Ramesh Sitaraman. 2024. SODA: An adaptive bitrate controller for consistent high-quality video streaming. In Proceedings of the ACM SIGCOMM 2024 Conference. 613644. [8] Yihua Cheng, Ziyi Zhang, Hanchen Li, Anton Arapin, Yue Zhang, Qizheng Zhang, Yuhan Liu, Kuntai Du, Xu Zhang, Francis Yan, et al. 2024. {GRACE}:{Loss-Resilient} {Real-Time} video through neural codecs. In 21st USENIX Symposium on Networked Systems Design and Implementation (NSDI 24). 509531. [9] Mo Dong, Qingxi Li, Doron Zarchy, Brighten Godfrey, and Michael Schapira. 2015. {PCC}: Re-architecting congestion control for consistent high performance. In 12th USENIX Symposium on Networked Systems Design and Implementation (NSDI 15). 395408. [10] Mo Dong, Tong Meng, Doron Zarchy, Engin Arslan, Yossi Gilad, Brighten Godfrey, and Michael Schapira. 2018. {PCC} vivace:{OnlineLearning} congestion control. In 15th USENIX symposium on networked systems design and implementation (NSDI 18). 343356. [11] Chaoyou Fu, Haojia Lin, Xiong Wang, Yi-Fan Zhang, Yunhang Shen, Xiaoyu Liu, Haoyu Cao, Zuwei Long, Heting Gao, Ke Li, et al. 2025. Vita-1.5: Towards gpt-4o level real-time vision and speech interaction. arXiv preprint arXiv:2501.01957 (2025). [12] Te-Yuan Huang, Ramesh Johari, Nick McKeown, Matthew Trunnell, and Mark Watson. 2014. buffer-based approach to rate adaptation: Evidence from large video streaming service. In Proceedings of the 2014 ACM conference on SIGCOMM. 187198. [13] Aaron Hurst, Adam Lerer, Adam Goucher, Adam Perelman, Aditya Ramesh, Aidan Clark, AJ Ostrow, Akila Welihinda, Alan Hayes, Alec Radford, et al. 2024. Gpt-4o system card. arXiv preprint arXiv:2410.21276 (2024). [14] Junchen Jiang, Vyas Sekar, and Hui Zhang. 2012. Improving fairness, efficiency, and stability in http-based adaptive video streaming with festive. In Proceedings of the 8th international conference on Emerging networking experiments and technologies. 97108. [15] Zeqi Lai, Weisen Liu, Qian Wu, Hewu Li, Jingxi Xu, and Jianping Wu. 2022. SpaceRTC: Unleashing the low-latency potential of megaconstellations for real-time communications. In IEEE INFOCOM 2022IEEE Conference on Computer Communications. IEEE, 13391348. [16] Tianhong Li, Vibhaalakshmi Sivaraman, Pantea Karimi, Lijie Fan, Mohammad Alizadeh, and Dina Katabi. 2023. Reparo: Loss-resilient generative codec for video conferencing. arXiv preprint arXiv:2305.14135 (2023). [17] Junming Lin, Zheng Fang, Chi Chen, Zihao Wan, Fuwen Luo, Peng Li, Yang Liu, and Maosong Sun. 2024. Streamingbench: Assessing the gap for mllms to achieve streaming video understanding. arXiv preprint arXiv:2411.03628 (2024). [18] Hongzi Mao, Ravi Netravali, and Mohammad Alizadeh. 2017. Neural adaptive video streaming with pensieve. In Proceedings of the conference of the ACM special interest group on data communication. 197210. [19] Zili Meng, Xiao Kong, Jing Chen, Bo Wang, Mingwei Xu, Rui Han, Honghao Liu, Venkat Arun, Hongxin Hu, and Xue Wei. 2024. Hairpin: Rethinking packet loss recovery in edge-based interactive video streaming. In 21st USENIX Symposium on Networked Systems Design and Implementation (NSDI 24). 907926. [20] Zili Meng and Mingwei Xu. 2024. Feedback on Control Path: Early Congestion Feedback. In Latency Optimization in Interactive Multimedia Streaming. Springer, 2342. [21] Rui Qian, Shuangrui Ding, Xiaoyi Dong, Pan Zhang, Yuhang Zang, Yuhang Cao, Dahua Lin, and Jiaqi Wang. 2025. Dispider: Enabling Video LLMs with Active Real-Time Interaction via Disentangled Perception, Decision, and Reaction. arXiv preprint arXiv:2501.03218 (2025). [22] Haoran Qiu, Anish Biswas, Zihan Zhao, Jayashree Mohan, Alind Khare, Esha Choukse, ÃÃ±igo Goiri, Zeyu Zhang, Haiying Shen, Chetan Bansal, et al. [n. d.]. ModServe: Scalable and Resource-Efficient Large Multimodal Model Serving. ([n. d.]). [23] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, et al. 2021. Learning transferable visual models from natural language supervision. In International conference on machine learning. PmLR, 87488763. [24] Michael Rudow, Francis Yan, Abhishek Kumar, Ganesh Ananthanarayanan, Martin Ellis, and KV Rashmi. 2023. Tambur: Efficient loss recovery for videoconferencing via streaming codes. In 20th USENIX Symposium on Networked Systems Design and Implementation (NSDI 23). 953971. [25] Heiko Schwarz, Detlev Marpe, and Thomas Wiegand. 2007. Overview of the scalable video coding extension of the H. 264/AVC standard. IEEE Transactions on circuits and systems for video technology 17, 9 (2007), 11031120. [26] Gemini Team, Petko Georgiev, Ving Ian Lei, Ryan Burnell, Libin Bai, Anmol Gulati, Garrett Tanzer, Damien Vincent, Zhufeng Pan, Shibo Wang, et al. 2024. Gemini 1.5: Unlocking multimodal understanding across millions of tokens of context. arXiv preprint arXiv:2403.05530 (2024). [27] Pavan Kumar Anasosalu Vasu, Hadi Pouransari, Fartash Faghri, Raviteja Vemulapalli, and Oncel Tuzel. 2024. Mobileclip: Fast imagetext models through multi-modal reinforced training. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. 1596315974. [28] Haibo Wang, Bo Feng, Zhengfeng Lai, Mingze Xu, Shiyu Li, Weifeng Ge, Afshin Dehghan, Meng Cao, and Ping Huang. 2025. StreamBridge: Turning Your Offline Video Large Language Model into Proactive Streaming Assistant. arXiv preprint arXiv:2505.05467 (2025). [29] Yuxuan Wang, Yueqian Wang, Bo Chen, Tong Wu, Dongyan Zhao, and Zilong Zheng. 2025. OmniMMI: Comprehensive Multi-modal Interaction Benchmark in Streaming Video Contexts. In Proceedings of the Computer Vision and Pattern Recognition Conference. 1892518935. [30] Haomiao Xiong, Zongxin Yang, Jiazuo Yu, Yunzhi Zhuge, Lu Zhang, Jiawen Zhu, and Huchuan Lu. 2025. Streaming Video Understanding and Multi-round Interaction with Memory-enhanced Knowledge. arXiv preprint arXiv:2501.13468 (2025). [31] Jin Xu, Zhifang Guo, Jinzheng He, Hangrui Hu, Ting He, Shuai Bai, Keqin Chen, Jialin Wang, Yang Fan, Kai Dang, et al. 2025. Qwen2. Jiangkai Wu et al. 5-omni technical report. arXiv preprint arXiv:2503.20215 (2025). [32] Francis Yan, Hudson Ayers, Chenzhi Zhu, Sadjad Fouladi, James Hong, Keyi Zhang, Philip Levis, and Keith Winstein. 2020. Learning in situ: randomized experiment in video streaming. In 17th USENIX Symposium on Networked Systems Design and Implementation (NSDI 20). 495511. [33] Zhenyu Yang, Yuhang Hu, Zemin Du, Dizhan Xue, Shengsheng Qian, Jiahong Wu, Fan Yang, Weiming Dong, and Changsheng Xu. [n. d.]. SVBench: Benchmark with Temporal Multi-Turn Dialogues for Streaming Video Understanding. In The Thirteenth International Conference on Learning Representations. [34] Linli Yao, Yicheng Li, Yuancheng Wei, Lei Li, Shuhuai Ren, Yuanxin Liu, Kun Ouyang, Lean Wang, Shicheng Li, Sida Li, et al. 2025. TimeChatOnline: 80% Visual Tokens are Naturally Redundant in Streaming Videos. arXiv preprint arXiv:2504.17343 (2025). [35] Pan Zhang, Xiaoyi Dong, Yuhang Cao, Yuhang Zang, Rui Qian, Xilin Wei, Lin Chen, Yifei Li, Junbo Niu, Shuangrui Ding, et al. 2024. Internlm-xcomposer2. 5-omnilive: comprehensive multimodal system for long-term streaming video and audio interactions. arXiv preprint arXiv:2412.09596 (2024). [36] Yiwu Zhong, Zhuoming Liu, Yin Li, and Liwei Wang. 2024. Aim: Adaptive inference of multi-modal llms via token merging and pruning. arXiv preprint arXiv:2412.03248 (2024)."
        }
    ],
    "affiliations": [
        "Peking University"
    ]
}