{
    "paper_title": "UniVid: Unifying Vision Tasks with Pre-trained Video Generation Models",
    "authors": [
        "Lan Chen",
        "Yuchao Gu",
        "Qi Mao"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Large language models, trained on extensive corpora, successfully unify diverse linguistic tasks within a single generative framework. Inspired by this, recent works like Large Vision Model (LVM) extend this paradigm to vision by organizing tasks into sequential visual sentences, where visual prompts serve as the context to guide outputs. However, such modeling requires task-specific pre-training across modalities and sources, which is costly and limits scalability to unseen tasks. Given that pre-trained video generation models inherently capture temporal sequence dependencies, we explore a more unified and scalable alternative: can a pre-trained video generation model adapt to diverse image and video tasks? To answer this, we propose UniVid, a framework that fine-tunes a video diffusion transformer to handle various vision tasks without task-specific modifications. Tasks are represented as visual sentences, where the context sequence defines both the task and the expected output modality. We evaluate the generalization of UniVid from two perspectives: (1) cross-modal inference with contexts composed of both images and videos, extending beyond LVM's uni-modal setting; (2) cross-source tasks from natural to annotated data, without multi-source pre-training. Despite being trained solely on natural video data, UniVid generalizes well in both settings. Notably, understanding and generation tasks can easily switch by simply reversing the visual sentence order in this paradigm. These findings highlight the potential of pre-trained video generation models to serve as a scalable and unified foundation for vision modeling. Our code will be released at https://github.com/CUC-MIPG/UniVid."
        },
        {
            "title": "Start",
            "content": "UniVid: Unifying Vision Tasks with Pre-trained Video Generation Models 1MIPG, Communication University of China 2Show Lab, National University of Singapore Lan Chen1 Yuchao Gu2 Qi Mao1, (cid:66) 5 2 0 2 6 2 ] . [ 1 0 6 7 1 2 . 9 0 5 2 : r a"
        },
        {
            "title": "Abstract",
            "content": "Large language models, trained on extensive corpora, successfully unify diverse linguistic tasks within single generative framework. Inspired by this, recent works like Large Vision Model (LVM) extend this paradigm to vision by organizing tasks into sequential visual sentences, where visual prompts serve as the context to guide outputs. However, such modeling requires task-specific pre-training across modalities and sources, which is costly and limits scalability to unseen tasks. Given that pre-trained video generation models inherently capture temporal sequence dependencies, we explore more unified and scalable alternative: can pre-trained video generation model adapt to diverse image and video tasks? To answer this, we propose UniVid, framework that fine-tunes video diffusion transformer to handle various vision tasks without task-specific modifications. Tasks are represented as visual sentences, where the context sequence defines both the task and the expected output modality. We evaluate the generalization of UniVid from two perspectives: (1) cross-modal inference with contexts composed of both images and videos, extending beyond LVMs uni-modal setting; (2) cross-source tasks from natural to annotated data, without multi-source pretraining. Despite being trained solely on natural video data, UniVid generalizes well in both settings. Notably, understanding and generation tasks can easily switch by simply reversing the visual sentence order in this paradigm. These findings highlight the potential of pre-trained video generation models to serve as scalable and unified foundation for vision modeling. Our code will be released at https://github.com/CUC-MIPG/UniVid. 1. Introduction Large language models (LLMs) such as GPT [26] and DeepSeek-R1 [6] have garnered significant attention for their ability to tackle broad spectrum of language tasks within unified framework. This success motivates the pursuit of developing similar unified models for various vision tasks. representative effort in this direction is the Large"
        },
        {
            "title": "Within",
            "content": "sequential framework, Vision Model (LVM) [1], which seeks to bridge the gap by representing diverse vision tasksincluding both natural data (images, videos) and annotated data (e.g., segmentation maps)as visual sentences in pixel space. LVM [1] processes these heterogeneous inputs using unified sequential architecture, analogous to how language models handle natural language sequences. this task-specific dataincluding images, videos, or annotationscompose the visual prompt (or visual context) that guides output generation, as illustrated in Fig. 1(a). However, realizing such unified visual modeling in practice remains challenging: LVM [1] still requires pre-training on separate, taskand modality-specific datasets (e.g., for generation vs. understanding, and for images vs. videos). This fragmented data curation process is highly labor-intensive and fundamentally constrains scalability to new tasks. These limitations motivate us to explore more unified and scalable approach for vision modeling. Unlike the complex data curation required for sequential modeling in LVM [1], pre-trained video generation models benefit from the inherent sequential structure of video data, allowing them to naturally capture temporal dependencies without the need for extensive taskand modality-specific annotations. Inspired by LLaVA [13], which adapts single generative backbone pre-trained on large-scale corpora to variety of downstream tasks via supervised fine-tuning (SFT), we formulate the central hypothesis of this work: Could single large video-generation model, pre-trained once for synthesis, serve as universal visual backbone that can be efficiently adapted to broad range of vision tasks through SFT? To investigate this hypothesis, we propose UniVid, unified paradigm that fine-tunes pre-trained video diffusion transformer (DiT) to address generation and pixel-level understanding vision tasks without any task-specific architectural modifications. Within this framework, both image and video tasks are naturally organized as visual sentences that align with the temporal dimension of video data. As illustrated in the right block of Fig. 1(a), each training sample is structured as B, where the context (A, A, B) defines the vision task and specifies the desired 1 Figure 1. LVM [1] vs. UniVid. (a) LVM [1] requires large-scale, modalityand source-specific paired data for pre-training to support diverse vision tasks. In contrast, UniVid explores whether pre-trained video generation model can be efficiently adapted to broad range of vision tasks via lightweight SFT with minimal paired data. (b) At inference, LVM [1] is limited to uni-modal visual contexts, whereas UniVid enables unified framework that accommodates both cross-modal and cross-source vision tasks. Stacked blocks represent videos; single block represents an image. output modality. We evaluate the generalization capacity of UniVid from two perspectives. First, while LVM [1] is limited to uni-modal contexts (i.e., either images or videos alone) at inference, we examine whether UniVid can accommodate cross-modal contextswhere the output modality is inferred from mixed image-video inputs. Second, we investigate UniVids capability for cross-source tasks, such as depth estimation from natural videos to annotated data, even without the multi-source pre-training required by LVM [1]. As shown in Fig. 1(b), despite being pre-trained solely on continuous natural video data, our model adapts effectively to both cross-modal and cross-source tasks through SFT. Importantly, under this unified paradigm, the distinction between generation and understanding tasks is reduced to the ordering of elements within the visual sentence. These findings highlight the potential of pre-trained video generation models as unified pre-training backbone for generalpurpose visual modeling. Our contributions are summarized as follows: To the best of our knowledge, we are the first to explore unified vision modeling using pre-trained video generation model, eliminating the need for taskand modality-specific pre-training data. We propose UniVid, unified framework that leverages lightweight SFT to efficiently adapt pre-trained video DiT to broad range of image and video tasks, without any task-specific architectural modifications. Extensive experiments show that UniVid effectively generalizes to both cross-modal and cross-source scenarios, highlighting its potential as unified and scalable foundation for general-purpose vision modeling."
        },
        {
            "title": "Query",
            "content": "A"
        },
        {
            "title": "Video\nImage\nImage\nImage",
            "content": "A"
        },
        {
            "title": "Video\nImage\nImage\nVideo",
            "content": "B"
        },
        {
            "title": "Video\nImage\nVideo\nImage",
            "content": "B"
        },
        {
            "title": "Video\nImage\nVideo\nVideo",
            "content": "Table 1. Four Types of Visual Contexts. 2. Related Works Large Vision Models. Recent advancements in universal vision frameworks predominantly follow two paradigms: image-resembling generation [2, 30, 31] and sequential Image-resembling generation methmodeling [1, 24]. ods [2, 30, 31] reformulate diverse vision tasks as image inpainting problems, enabling models to make predictions through generating masked regions. Sequential modeling [1, 24] approaches draw inspiration from LLMs, treating visual data as sequences of discrete tokens and optimizing models via next-token prediction. However, these approaches heavily rely on large-scale annotated data to construct task-specific training samples, which is resourceintensive and hinders scalability. In contrast, we demonstrate that model trained solely on continuous video data can be effectively adapted to broad range of vision tasks. Vision In-Context Learning. In-context learning, where models perform tasks prompted by examples, has been extensively studied in LLMs [26]. Inspired by this success, many efforts [1, 2, 4, 8, 17, 24, 25, 30, 31, 35] have extended the paradigm to vision, demonstrating its potential across wide range of vision tasks. Early methods [1, 2, 24, 30] primarily rely on sequential models trained on large-scale annotated input-output pairs to perform vision tasks. More recently, in-context generation capability has been demonstrated in DiT-based text-to-image (T2I) models [4, 8, 17, 25, 35] for controllable image generation and manipulation. Similarly, the DiT-based video model captures temporal dependencies through full attention across frames, which we leverage to adapt the pre-trained model to wide range of vision tasks. Video Generation Models. Recent progress in video generation has been driven by both autoregressive [7, 11, 14, 27, 32, 33] and diffusion-based [3, 10, 12, 15, 16, 20, 28, 29, 34] architectures. Autoregressive models [7, 11, 14, 27, 32, 33] compress video frames into discrete tokens and employ transformers to generate video sequences token by token. Early diffusion-based video models [3, 10, 29] extend UNet [22] architectures originally designed for T2I generation, lacking temporal consistency. Recent hybrid architectures [12, 15, 16, 20, 28, 34] adopt DiT framework with advanced attention mechanism, yielding improved generation quality and temporal coherence. In this work, we take the Wan model [28] as strong foundation to assess the effectiveness of video generative pre-training for downstream vision tasks. Figure 2. The framework of UniVid. 3. Methodology In this section, we first review existing vision sequential models in Section 3.1 and define the problems we explore in Section 3.2. Based on the discussion, we introduce our method, experimental setup, and results analysis in Section 3.3. 3.1. Preliminaries: Limitations of Sequential Vision"
        },
        {
            "title": "Models",
            "content": "Recent advances in sequential vision modeling, exemplified by the LVM [1], aim to unify diverse vision tasks under single framework. However, LVM [1] faces two significant limitations: (a) Reliance on Annotated Pairs for Pre-training. As shown in Fig. 1(a), LVM [1] requires large-scale, taskand modality-specific annotated pairs for pre-training in order to support wide range of downstream tasks. This data curation process is labor-intensive and fundamentally restricts scalability. In this work, we question whether annotated pairs are truly necessary at the pre-training stage. (b) Limited Generalization Beyond Single-Modality Contexts. As shown in Fig. 1(b), despite being pre-trained on heterogeneous data, LVM [1] is limited to tasks within imageonly or video-only contexts at inference, leaving cross-modal scenarios largely underexplored. Furthermore, LVM [1] is restricted to video extrapolation and fails to generalize to cross-source video tasks, such as semantic segmentation, despite relevant annotations in pre-training. 3 Figure 3. Main observations. The top colored row serves as legend indicating the modality and role of each clip shown below. The following figure follows the same format. (a) The model infers the correct output modality from cross-modal contexts. (b) Despite being pre-trained solely on natural video data, it generalizes to cross-source understanding tasks. (c) Under the UniVid framework, understanding and generation tasks are unified and can be converted by reordering the visual sentence. These limitations motivate us to seek an alternative unified vision modeling paradigm that reduces data annotation burdens and extends generalization across modalities and sources. 3.2. Problem Formulation: Unified Visual Sentence"
        },
        {
            "title": "Paradigm",
            "content": "To address these challenges, we revisit the video generation model, treating it as naturally pre-trained visual sequential learner. We reformulate broad set of vision tasks using unified visual sentence paradigm: given an example inputoutput pair A, the model is tasked to predict for query B, with (A, A, B) collectively defining the task context and output modality. We focus on evaluating the adaptation capacity of pre-trained video generation model from two key perspectives:(a) Cross-modal generalization: the ability to handle tasks prompted by mixed-modal contexts (e.g., combining images and videos), as illustrated in Table 1 III and IV. (b) Cross-source generalization: the ability to perform vision tasks across heterogeneous data sources, spanning both pixel-level understanding and generation tasks. Our central question is: Can video generation model pre-trained solely on natural videos adapt to diverse vision tasks within unified visual sentence paradigm, even when the contexts span modalities and data sources? 3.3. Video Generative Pre-training with SFT for"
        },
        {
            "title": "Unified Vision Modeling",
            "content": "We start from video DiT model i.e., Wan [28], pre-trained exclusively on continuous natural video data, as our unified backbone. To enable adaptation to diverse vision tasks, we employ Low-Rank Adaptation (LoRA) modules for efficient SFT. Data Structure. We leverage the temporal dimension of video by representing each input-output pair as sequence of video clips concatenated along the time axis. As shown in Fig. 2(a), each training sample is structured as visual sentence = [A, A, B, B]. and comprise an example pair that demonstrates reference vision task (e.g., source video and its scribble map). is the query in4 Figure 4. Performance across diverse vision tasks and context formats. We show results for scribble map transfer, motion transfer, and salient object tracking under various visual contexts. Each task is fine-tuned independently within each context configuration, demonstrating that the pre-trained video generation model adapts well across all applicable settings listed in Table 1. With fixed example pair, outputs change with the query, reflecting context-based inference. put, and is the expected output, which should undergo the same transformation as A. Each element can be either an image or video segment, allowing unified processing of diverse contexts. Fine-Tuning and Inference. During training, is first embedded into latent tokens. We treat the tokens corresponding to (A, A, B) as the context C, which remains clean during training. Noise is added only to the target clip B, producing noisy latent tokens zt, as shown in Fig. 2(a). The complete token sequence is then processed by the DiT blocks using 3D full attention [28], enabling cross-clip interactions throughout the temporal dimension. We insert LoRA modules into both Cross-Attention (CA) and Self-Attention (SA) layers for efficient adaptation. At inference, the model generates conditioned on (A, A, B), with clean context tokens guiding the generation process as illustrated in Fig. 2(b). Experimental Protocol. To systematically assess adaptation and generalization, we collect paired data across six representative tasks, including generation tasks (1) scribble map transfer, (2) Van Gogh style transfer and (3) camera movement transfer and understanding tasks (4) depth map prediction, (5) semantic segmentation prediction, and (6) salient object tracking. Each task consists of only 20 training samples. To validate the question posed in Section 3.2, we conduct straightforward fine-tuning experiments: First, for each vision task, we fine-tune the video generation model on its applicable contexts listed in Table 1 separately. Additionally, we construct generation variants of tasks (4) and (6) by reversing them into conditional generation settings to further establish its generalization. Results and Analysis. Based on our experimental results (see Fig. 3), we summarize the following three key observations regarding the models adaptability to diverse vision tasks: Observation 1: Robust cross-modal adaptation. Although pre-trained solely on continuous video data, the finetuned model effectively interprets manually composed visual sentences and flexibly handles various task contexts, including those spanning multiple modalities (see Fig. 3(a)). Observation 2: Effective cross-source generalization. Despite only being exposed to natural video data during pre-training, the model adapts successfully to cross-source tasks such as predicting depth maps from natural video data (see Fig. 3(b)). Observation 3: Unified formulation of understanding and generation tasks. As illustrated in Fig. 3(c), the model can perform both understanding and generation tasks by simply reordering the elements within visual sentence. This demonstrates that, under the unified paradigm, these tasks are seamlessly interchangeable through sentence organization. 4. Experiments In this section, we first present the implementation details in Section 4.1. In Section 4.2, we assess how well UniVid generalizes to range of vision tasks under varied visual contexts. Next, we explore the results of mixed fine-tuning 5 Figure 5. Unified understanding and generation tasks. Our proposed UniVid allows flexible switching between understanding and generation tasks by simply reordering visual sentences. strategies in Section 4.3 and the effects of shot number in Section 4.4. Quantitative comparisons with LVM [1] are provided in Section 4.5 to validate the effectiveness of our approach. Finally, we discuss the remaining limitations and future work in Section 4.6. 4.1. Implementation Details. We employ Wan2.1-T2V-1.3B as the backbone video generation model for our experiments. The number of frames in each clip of (A, A, B, B) is set to 1 for image modality and 17 for video modality. We use the Wan Encoder [28] to embed the four clips separately. During fine-tuning, we set the LoRA rank to 16, the learning rate to 1 104, and the batch size to 1. Please refer to appendix for additional details. 4.2. Generalization across Tasks and Contexts Task-level Generalization. We evaluate UniVid on the six tasks under diverse context configurations  (Table 1)  . Specifically, the camera motion transfer task is evaluated under contexts and IV, while the other tasks are trained under contexts I, II, and III. As shown in Fig. 4, UniVid demonstrates strong adaptability across modalities and data sources. Please refer to appendix for remaining results. Unified Understanding and Generation. We reverse the visual sentence structure from (A B) to (A B), converting understanding tasks into generation tasks. As illustrated in Fig. 5, the fine-tuned model generates coherent videos under both sequence orderings, highlighting the unified formulation of both understanding and generation. Context-Conditioned Inference. As shown in Fig. 4, given fixed example pair (A, A), the output changes based on query B, confirming the models context-conditioned reasoning capability. 4.3. Mixed Fine-tuning Strategy Building on the results showing that the video generation model performs well when each vision task is fine-tuned with single visual context, as demonstrated in Fig. 3 and Fig. 4, we further explore its adaptability under joint training regimes. We consider two configurations: (1) train6 Figure 6. Results of mixed fine-tuning strategy. (a) When fine-tuned on visual sequences spanning all context types, the model can dynamically infer the output modality at inference time based on the context (A, A, B). (b) We co-train the model on all vision tasks, each covering all applicable contexts. Under this mixed setup, the model consistently performs well across all tasks and contexts, demonstrating strong generalization. Additional results are shown in the appendix. ing each task with mixture of visual contexts, and (2) jointly training all tasks, where each task is exposed to all applicable contexts. Per-task Mixed Context Fine-tuning. For the camera motion transfer task, the training data is composed of mixture of contexts and IV, while the other tasks are trained under contexts I, II, and III. As shown in Fig. 6(a), when trained on mixed contexts, the model can automatically adapt to the specific context (A, A, B) and produce consistent results in correct modality, demonstrating strong generalization within the unified training paradigm. Please refer to appendix for the results of other tasks. Joint Multi-task Fine-tuning. We co-train the model on all six vision tasks, with each task provided 20 training examples covering all applicable contexts. Fig. 6 presents results for the six vision tasks under Context I, with the remaining three contexts provided in appendix. These results demonstrate that even with limited fine-tuning data mixed from multiple vision tasks and contexts, the model generalizes robustly across diverse vision tasks and contexts, validating its potential as unified foundation for visual modeling. We further report quantitative results of joint multi-task finetuning, employing CLIP-T for style transfer task. For the camera movement task, we extract the first, middle, and last frames and ask GPT-4o to rate the quality on scale from 0 to 10. Remaining tasks are measured by pixel-space RMSE. As shown in Table 2, the co-training strategy consistently achieves better performance than separate fine-tuning across all tasks. 4.4. Impact of Shot Number We investigate the effect of shot number by fine-tuning three separate models using 4-shot, 6-shot, and 8-shot configurations, respectively. Each model is then evaluated under all three shot settings. Specifically, the 4-shot, 6-shot, and 8-shot settings are defined as follows: 4 shots: B 6 shots: B 8 shots: B In all cases, the final clip (e.g., B, , or D) is the target to be generated. We evaluate on classic understanding task (depth estimation) and generation task (style transfer) on uni-image Context II, reporting results in Table 3. For depth estimation, performance tends to degrade when the number of test shots exceeds that used in fine-tuning, as the model never saw depth maps during pre-training and relies solely on fine-tuning supervision. In contrast, style transfer remains stable across shot counts since similar visual data were seen during pre-training. While longer contexts yield better results, they also increase inference time. For consistent analysis, we adopt four-shot setting in this paper. 7 Task Training Strategy Separate Co-training Style Transfer (CLIP-T ) Camera Movement (GPT-4o ) Scribble Map (RMSE ) Depth Estimation (RMSE ) Semantic Segmentation (RMSE ) Salient Object Track (RMSE ) 18. 6.33 61.03 74.16 126.12 33.35 24. 6.73 51.94 2.55 123.03 30.59 Table 2. Quantitative comparisons between different training strategies."
        },
        {
            "title": "Test\nshots",
            "content": "Time (s) Depth Estimation (RMSE ) Style Transfer (CLIP-T ) 4 6 4 6 8 4 6 8 4 6 8 13.37 19.36 25.47 13.37 19.36 25.47 13.37 19.36 25. 61.03 74.25 86.58 44.85 54.51 47.84 49.5 52.07 55.16 21.28 21.00 20.93 21.16 21.00 21.01 21.65 21.57 21. Table 3. Effects of the shot number. 4.5. Quantitative Comparisons To assess the effectiveness of UniVid, we compare it with LVM [1] across five tasks: Van Gogh style transfer is evaluated with CLIP-T (alignment with Van Gogh style) and CLIP-I (consistency with reference image A). Edge map prediction on the BIPED [19] dataset, is evaluated by fixed contour threshold (ODS), per-image best threshold (OIS), and average precision (AP). Semantic segmentation is conducted on the ADE20K dataset [36], measured by mean Intersection over Union (mIoU) and pixel accuracy (pAcc). Depth estimation is evaluated on the NYU-v2 dataset [23], reporting the percentage of pixels within various δ thresholds, absolute relative error (AbsRel), squared relative error (SqRel), root mean square logarithmic error (RMSELog), and scale-invariant logarithmic error (SILog). Surface normal estimation is also evaluated on the NYU-v2 dataset [23], assessed by Mean Angular errors (Mean) and Median Angular Errors (Med), along with accuracy under thresholds of 5, 11.25, and 30. All tasks are trained on small subsets of the standard training sets (such as 65 of 175K in NYU-v2 [23]) and evaluated on the full test splits, whereas LVM [1] uses the full training split. As shown in Table 4, Table 5 and Table 6, despite being Method Style Transfer Edge Map Prediction Semantic Segmentation CLIP-T CLIP-I ODS OIS AP mIoU pACC LVM Ours 16.24 19.76 0.712 0. 0.656 0.678 0.630 1.423 0.873 0.877 0.871 8.712 23.30 53.13 Table 4. Comparison across style transfer, edge map prediction, and semantic segmentation. Method δ1 δ3 δ3 AbsRel SqRel RMSElog SILog LVM 0.15 0.31 0.48 0.43 0.76 0.91 Ours 0.53 0. 0.18 0.28 1.15 0.42 72.91 30.74 Table 5. Depth estimation performance. Method Mean Med 5 11.25 30 LVM Ours 30.76 29.84 13.73 13.52 24.70% 25.22% 45.10% 45.57% 65.98% 67.53% Table 6. Surface normal estimation performance. trained on limited data, our method consistently outperforms LVM [1], demonstrating strong generalization capability to both visual generation and understanding tasks. 4.6. Limitations and Future Work While our study validates promising approach to unifying vision tasks using video generation model, certain limitations remain. The context length of Wan model [28] is limited to 81 frames per sequence, restricting the duration of each visual clip. Additionally, due to the inherent randomness of generative processes, label consistency across instance types in the segmentation task cannot be guaranteed. In the future, we plan to explore long-context video generation [5] architectures and mitigate ambiguity in understanding tasks such as segmentation. 5. Conclusions In this work, we explore new direction for building unified visual backbone by reusing pre-trained video generation model for various vision tasks through lightweight supervised fine-tuning. Unlike prior approaches that rely on large-scale, task-specific data to train visual sequential model, we simply fine-tune pre-trained video generation model using minimal supervised data. The visual data are organized into visual sentences, where the context defines both the vision task and the expected output modality. Despite being pre-trained solely on natural, continuous videos without annotations, the fine-tuned model generalizes well to cross-modal and cross-source contexts. Under this unified paradigm, understanding and generation tasks are differentiated only by the order of visual sentences and can be seamlessly interchanged. Moreover, results show that with minimal supervision, the model can jointly adapt to diverse vision tasks and context types through single fine-tuning."
        },
        {
            "title": "References",
            "content": "[1] Yutong Bai, Xinyang Geng, Karttikeya Mangalam, Amir Bar, Alan Yuille, Trevor Darrell, Jitendra Malik, and Alexei Efros. Sequential modeling enables scalable learning for large vision models. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 22861 22872, 2024. 1, 2, 3, 6, 8, 11 [2] Amir Bar, Yossi Gandelsman, Trevor Darrell, Amir Globerson, and Alexei Efros. Visual prompting via image inpainting. Advances in Neural Information Processing Systems, 35: 2500525017, 2022. 3 [3] Haoxin Chen, Yong Zhang, Xiaodong Cun, Menghan Xia, Xintao Wang, Chao Weng, and Ying Shan. Videocrafter2: Overcoming data limitations for high-quality video diffusion In Proceedings of the IEEE/CVF Conference on models. Computer Vision and Pattern Recognition, pages 73107320, 2024. 3 [4] Lan Chen, Qi Mao, Yuchao Gu, and Mike Zheng Shou. Edit transfer: Learning image editing via vision in-context relations. arXiv preprint arXiv:2503.13327, 2025. 3 [5] Yuchao Gu, weijia Mao, and Mike Zheng Shou. Long-context autoregressive video modeling with next-frame prediction. arXiv preprint arXiv:2503.19325, 2025. 8 [6] Daya Guo, Dejian Yang, Haowei Zhang, Junxiao Song, Ruoyu Zhang, Runxin Xu, Qihao Zhu, Shirong Ma, Peiyi Wang, Xiao Bi, et al. Deepseek-r1: Incentivizing reasoning capability in llms via reinforcement learning. arXiv preprint arXiv:2501.12948, 2025. [7] Wenyi Hong, Ming Ding, Wendi Zheng, Xinghan Liu, and Jie Tang. Cogvideo: Large-scale pretraining for text-to-video generation via transformers. arXiv preprint arXiv:2205.15868, 2022. 3 [8] Lianghua Huang, Wei Wang, Zhi-Fan Wu, Yupeng Shi, Huanzhang Dou, Chen Liang, Yutong Feng, Yu Liu, and Jingren Zhou. In-context lora for diffusion transformers. arXiv preprint arXiv:2410.23775, 2024. 3 [9] Zeyinzi Jiang, Zhen Han, Chaojie Mao, Jingfeng Zhang, Yulin Pan, and Yu Liu. Vace: All-in-one video creation and editing. arXiv preprint arXiv:2503.07598, 2025. 11 [10] Levon Khachatryan, Andranik Movsisyan, Vahram Tadevosyan, Roberto Henschel, Zhangyang Wang, Shant Navasardyan, and Humphrey Shi. Text2video-zero: Textto-image diffusion models are zero-shot video generators. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 1595415964, 2023. 3 [11] Dan Kondratyuk, Lijun Yu, Xiuye Gu, José Lezama, Jonathan Huang, Grant Schindler, Rachel Hornung, Vighnesh Birodkar, Jimmy Yan, Ming-Chang Chiu, et al. Videopoet: large language model for zero-shot video generation. arXiv preprint arXiv:2312.14125, 2023. 3 [12] Weijie Kong, Qi Tian, Zijian Zhang, Rox Min, Zuozhuo Dai, Jin Zhou, Jiangfeng Xiong, Xin Li, Bo Wu, Jianwei Zhang, et al. Hunyuanvideo: systematic framework for large video generative models. arXiv preprint arXiv:2412.03603, 2024. 3 [13] Haotian Liu, Chunyuan Li, Qingyang Wu, and Yong Jae Lee. Visual instruction tuning. Advances in neural information processing systems, 36:3489234916, 2023. [14] Haozhe Liu, Shikun Liu, Zijian Zhou, Mengmeng Xu, Yanping Xie, Xiao Han, Juan Pérez, Ding Liu, Kumara Kahatapitiya, Menglin Jia, et al. Mardini: Masked autoregressive diffusion for video generation at scale. arXiv preprint arXiv:2410.20280, 2024. 3 [15] Haoyu Lu, Guoxing Yang, Nanyi Fei, Yuqi Huo, Zhiwu Lu, Ping Luo, and Mingyu Ding. Vdt: General-purpose video diffusion transformers via mask modeling. arXiv preprint arXiv:2305.13311, 2023. 3 [16] Xin Ma, Yaohui Wang, Gengyun Jia, Xinyuan Chen, Ziwei Liu, Yuan-Fang Li, Cunjian Chen, and Yu Qiao. Latte: Latent diffusion transformer for video generation. arXiv preprint arXiv:2401.03048, 2024. 3 [17] Chaojie Mao, Jingfeng Zhang, Yulin Pan, Zeyinzi Jiang, Zhen Han, Yu Liu, and Jingren Zhou. Ace++: Instruction-based image creation and editing via context-aware content filling. arXiv preprint arXiv:2501.02487, 2025. 3 [18] Jiaxu Miao, Yunchao Wei, Yu Wu, Chen Liang, Guangrui Li, and Yi Yang. Vspw: large-scale dataset for video scene parsing in the wild. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 41334143, 2021. 11 [19] Xavier Soria Poma, Edgar Riba, and Angel Sappa. Dense extreme inception network: Towards robust cnn model for edge detection. In Proceedings of the IEEE/CVF winter conference on applications of computer vision, pages 1923 1932, 2020. [20] Di Qiu, Zhengcong Fei, Rui Wang, Jialin Bai, Changqian Yu, Mingyuan Fan, Guibin Chen, and Xiang Wen. Skyreels-a1: Expressive portrait animation in video diffusion transformers. arXiv preprint arXiv:2502.10841, 2025. 3 [21] Liao Qu, Huichao Zhang, Yiheng Liu, Xu Wang, Yi Jiang, Yiming Gao, Hu Ye, Daniel Du, Zehuan Yuan, and Xinglong Wu. Tokenflow: Unified image tokenizer for multimodal understanding and generation. arXiv preprint arXiv:2412.03069, 2024. 11 [22] Olaf Ronneberger, Philipp Fischer, and Thomas Brox. Unet: Convolutional networks for biomedical image segmentation. In Medical image computing and computer-assisted interventionMICCAI 2015: 18th international conference, Munich, Germany, October 5-9, 2015, proceedings, part III 18, pages 234241. Springer, 2015. 3 [23] Nathan Silberman, Derek Hoiem, Pushmeet Kohli, and Rob Fergus. Indoor segmentation and support inference from rgbd images. In European conference on computer vision, pages 746760. Springer, 2012. 8 [24] Zeyi Sun, Ziyang Chu, Pan Zhang, Tong Wu, Xiaoyi Dong, Yuhang Zang, Yuanjun Xiong, Dahua Lin, and Jiaqi Wang. X-prompt: Towards universal in-context image generation in auto-regressive vision language foundation models. arXiv preprint arXiv:2412.01824, 2024. 3 [25] Zhenxiong Tan, Songhua Liu, Xingyi Yang, Qiaochu Xue, and Xinchao Wang. Ominicontrol: Minimal and uniarXiv preprint versal control for diffusion transformer. arXiv:2411.15098, 2024. [26] OpenAI Team. Language models are few-shot learners. Advances in neural information processing systems, 33:1877 1901, 2020. 1, 3 9 [27] Ruben Villegas, Mohammad Babaeizadeh, Pieter-Jan Kindermans, Hernan Moraldo, Han Zhang, Mohammad Taghi Saffar, Santiago Castro, Julius Kunze, and Dumitru Erhan. Phenaki: Variable length video generation from open domain textual description. arXiv preprint arXiv:2210.02399, 2022. 3 [28] Ang Wang, Baole Ai, Bin Wen, Chaojie Mao, Chen-Wei Xie, Di Chen, Feiwu Yu, Haiming Zhao, Jianxiao Yang, Jianyuan Zeng, et al. Wan: Open and advanced large-scale video generative models. arXiv preprint arXiv:2503.20314, 2025. 3, 4, 5, 6, 8, 11 [29] Jiuniu Wang, Hangjie Yuan, Dayou Chen, Yingya Zhang, Xiang Wang, and Shiwei Zhang. Modelscope text-to-video technical report. arXiv preprint arXiv:2308.06571, 2023. 3 [30] Xinlong Wang, Wen Wang, Yue Cao, Chunhua Shen, and Images speak in images: generalist Tiejun Huang. In Proceedings of painter for in-context visual learning. the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 68306839, 2023. 3 [31] Zhaoqing Wang, Xiaobo Xia, Runnan Chen, Dongdong Yu, Changhu Wang, Mingming Gong, and Tongliang Liu. Lavin-dit: Large vision diffusion transformer. arXiv preprint arXiv:2411.11505, 2024. 3 [32] Jinheng Xie, Weijia Mao, Zechen Bai, David Junhao Zhang, Weihao Wang, Kevin Qinghong Lin, Yuchao Gu, Zhijie Chen, Zhenheng Yang, and Mike Zheng Shou. Show-o: One single transformer to unify multimodal understanding and generation. arXiv preprint arXiv:2408.12528, 2024. [33] Wilson Yan, Yunzhi Zhang, Pieter Abbeel, and Aravind Srinivas. Videogpt: Video generation using vq-vae and transformers. arXiv preprint arXiv:2104.10157, 2021. 3 [34] Zhuoyi Yang, Jiayan Teng, Wendi Zheng, Ming Ding, Shiyu Huang, Jiazheng Xu, Yuanming Yang, Wenyi Hong, Xiaohan Zhang, Guanyu Feng, et al. Cogvideox: Text-to-video diffusion models with an expert transformer. arXiv preprint arXiv:2408.06072, 2024. 3 [35] Yuxuan Zhang, Yirui Yuan, Yiren Song, Haofan Wang, and Jiaming Liu. Easycontrol: Adding efficient and flexible control for diffusion transformer. arXiv preprint arXiv:2503.07027, 2025. 3 [36] Bolei Zhou, Hang Zhao, Xavier Puig, Tete Xiao, Sanja Fidler, Adela Barriuso, and Antonio Torralba. Semantic understanding of scenes through the ade20k dataset. International Journal of Computer Vision, 127(3):302321, 2019. 8 [37] Bojia Zi, Penghui Ruan, Marco Chen, Xianbiao Qi, Shaozhe Hao, Shihao Zhao, Youze Huang, Bin Liang, Rong Xiao, and Kam-Fai Wong. Señorita-2m: high-quality instructionbased dataset for general video editing by video specialists. arXiv preprint arXiv:2502.06734, 2025. 11 10 A. Implementation Details Data Collection. We collect paired data across six representative tasks, including generation tasks (1) scribble map transfer, (2) Van Gogh style transfer and (3) camera movement transfer and perception tasks (4) depth map prediction, (5) semantic segmentation prediction, and (6) salient object tracking. For task (1)(4)(6), we collect the source clips (A, B) from the Señorita-2M dataset [37], and the annotated clips (A, B) are obtained using preprocessing tools from VACE [9] code repository1. For task (2)(3), we use the source videos from Señorita-2M dataset [37] and edit them using TokenFlow [21] and computer software CapCut2, respectively. For task (5), we source data from VSPW dataset [18]. Fine-tuning Details. We employ Wan2.1-T2V-1.3B3 as the backbone video generation model for our experiments. The model is trained for 2040 epochs for each vision tasks, with each epoch consisting of 200 iterations. For co-training across all vision tasks and contexts, we train the model for 20 epochs, with 1,200 iterations per epoch. Training on two A800 GPUs, each epoch takes about 12 minutes for 200 iterations and roughly one hour for 1200 iterations. Experimental Details. In fine-tuning with mixed vision contexts, we randomly choose the vision contexts of each training sample. For tasks (1)(2)(4)(5)(6), we sample vision contexts and II each with probability = 0.3, context III with = 0.4. For task (3), since the transformation pertains to the temporal dimension, sampling is limited to contexts and IV, each with probability = 0.5. In the ablation study investigating the impact of text, we utilize prompts at multiple levels of granularity, including detailed, rough, and null texts. The prompt template is illustrated in Fig. 7. For all other experiments, we consistently use the detailed text prompt. B. Comparison between LVM and video gener-"
        },
        {
            "title": "Text Prompts",
            "content": "Detailed Text: [clip1] is the original source video, and [clip2] is its corresponding segmentation map. [clip3] is another, different source video. In [clip4], the segmentation map transformation applied from [clip1] to [clip2] is similarly applied to [clip3]. Rough Text: [clip1] and [clip2] form an editing pair. Apply the same transformation observed from [clip1] to [clip2] to [clip3], and generate [clip4]. Null Text: Figure 7. Text Prompts at multiple levels of granularity."
        },
        {
            "title": "Dataset\nComposition",
            "content": "1. Single images 2. Image sequences 3. Images with annotations 4. Image sequences with annotations"
        },
        {
            "title": "Downsample\nRatio\nParameters",
            "content": "420B tokens 16X 7B (released version)"
        },
        {
            "title": "Raw\nimages and\nvideos",
            "content": "O(1)T tokens [28] 8X 1.3B Table 7. Comparison between LVM [1] and the video generation model Wan [28]. of each vision task across all contexts in Section C.2, the performance of each task under mixed-context fine-tuning in Section C.3, the impact of text prompts in Section C.5, and the results of co-training with all tasks and contexts in Section C.4. ation model C.1. Conditional Generation Tasks As summarized in Table 7, the training data required by LVM [1] is complex to construct, while the video generation model Wan is pretrained only on raw images and videos. Although Wan uses more total training tokens than LVM, it achieves higher visual quality by employing an 8 downsampling encoder, compared to LVMs 16 downsampling. Additionally, Wan has fewer parameters than the released version of LVM, resulting in lower computational costs. C. Additional Experimental Results We provide additional results related to four experiments presented in the main paper. Specifically, we show the results 1https://github.com/ali-vilab/VACE 2https://www.capcut.com/ 3https://github.com/Wan-Video/Wan2.1 As shown in Fig. 8, the video generation model is capable of performing conditional generation tasks based on depth maps or masked videos. C.2. Performance Across Different Contexts We present the performance of each vision task across different contexts in Fig. 10. The results demonstrate that the fine-tuned video generation model effectively handles not only image and video tasks, but also cross-modal and crossdata-source tasks. C.3. Performance under Mixed-Context Fine-"
        },
        {
            "title": "Tuning",
            "content": "As shown in Fig. 11, Fig. 12 and Fig. 13 , when fine-tuned on mixed vision contexts, the model can automatically adjust 11 Figure 8. Results of conditional generation tasks. Figure 9. Impact of texts under contexts III. to each vision context, demonstrating strong generalization ability. C.5. Impact of Texts Across Different Vision Contexts C.4. Co-training with All Vision Tasks and Contexts As shown in Fig. 17, when co-trained on all vision tasks under mixed contexts, the model achieves consistent performance across tasks and contexts, demonstrating robust generalization ability with limited supervision. 12 As shown in Fig. 14, Fig. 15, Fig. 9, and Fig. 16, the model effectively learns the relationships among the four clips across different vision contexts without explicit textual guidance, demonstrating strong in-context learning capabilities in the temporal dimension. Figure 10. Additional results of various vision tasks across context types. Figure 11. Performance under separate and mixed training for context I. Figure 12. Performance under separate and mixed training for context II. 14 Figure 13. Performance under separate and mixed training for context III. Figure 14. Impact of texts under contexts I. Figure 15. Impact of texts under contexts II. Figure 16. Impact of texts under contexts IV. 16 Figure 17. Co-training with all vision tasks and contexts."
        }
    ],
    "affiliations": [
        "Communication University of China",
        "National University of Singapore"
    ]
}