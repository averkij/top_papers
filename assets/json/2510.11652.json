{
    "paper_title": "ACADREASON: Exploring the Limits of Reasoning Models with Academic Research Problems",
    "authors": [
        "Xin Gui",
        "King Zhu",
        "JinCheng Ren",
        "Qianben Chen",
        "Zekun Moore Wang",
        "Yizhi LI",
        "Xinpeng Liu",
        "Xiaowan Li",
        "Wenli Ren",
        "Linyu Miao",
        "Tianrui Qin",
        "Ziqi Shu",
        "He Zhu",
        "Xiangru Tang",
        "Dingfeng Shi",
        "Jiaheng Liu",
        "Yuchen Eleanor Jiang",
        "Minghao Liu",
        "Ge Zhang",
        "Wangchunshu Zhou"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "In recent years, the research focus of large language models (LLMs) and agents has shifted increasingly from demonstrating novel capabilities to complex reasoning and tackling challenging tasks. However, existing evaluations focus mainly on math/code contests or general tasks, while existing multi-domain academic benchmarks lack sufficient reasoning depth, leaving the field without a rigorous benchmark for high-level reasoning. To fill this gap, we introduce the Acadreason benchmark, designed to evaluate the ability of LLMs and agents to acquire and reason over academic knowledge. It consists of 50 expert-annotated academic problems across five high-reasoning domains, including computer science, economics, law, mathematics, and philosophy. All questions are sourced from top-tier publications in recent years and undergo rigorous annotation and quality control to ensure they are both challenging and answerable. We conduct systematic evaluations of over 10 mainstream LLMs and agents. The results show that most LLMs scored below 20 points, with even the cutting-edge GPT-5 achieving only 16 points. While agents achieved higher scores, none exceeded 40 points. This demonstrates the current capability gap between LLMs and agents in super-intelligent academic research tasks and highlights the challenges of Acadreason."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 3 1 ] . [ 1 2 5 6 1 1 . 0 1 5 2 : r ACADREASON: Exploring the Limits of Reasoning Models with Academic Research Problems OPPO AI Agent Team"
        },
        {
            "title": "Abstract",
            "content": "In recent years, the research focus of large language models (LLMs) and agents has shifted increasingly from demonstrating novel capabilities to complex reasoning and tackling challenging tasks. However, existing evaluations focus mainly on math/code contests or general tasks, while existing multi-domain academic benchmarks lack sufficient reasoning depth, leaving the field without rigorous benchmark for high-level reasoning. To fill this gap, we introduce the ACADREASON benchmark, designed to evaluate the ability of LLMs and agents to acquire and reason over academic knowledge. It consists of 50 expertannotated academic problems across five high-reasoning domains, including computer science, economics, law, mathematics, and philosophy. All questions are sourced from top-tier publications in recent years and undergo rigorous annotation and quality control to ensure they are both challenging and answerable. We conduct systematic evaluations of over 10 mainstream LLMs and agents. The results show that most LLMs scored below 20 points, with even the cutting-edge GPT-5 achieving only 16 points. While agents achieved higher scores, none exceeded 40 points. This demonstrates the current capability gap between LLMs and agents in super-intelligent academic research tasks and highlights the challenges of ACADREASON. Date: October 14, 2025 Open Source: Code Correspondence: Wangchunshu Zhou at zhouwangchunshu@oppo.com, Ge Zhang at gezhang@umich.edu, Minghao Liu at dreamforever.liu@gmail.com Dataset"
        },
        {
            "title": "1 Introduction",
            "content": "The ability to reason effectively is cornerstone of advanced artificial intelligence, enabling systems to tackle complex problems across diverse domains. Recent advancements in large language models (LLMs), exemplified by models such as OpenAIs o3 [16], have demonstrated significant strides in reasoning capabilities. These models leverage techniques like inference-time scaling and learning-to-reason, showcasing robust performance across reasoning tasks [9]. However, as reasoning LLMs continue to evolve, limitations in existing reasoning benchmarkssuch as MMLUPro [28], GPQA [22] and SuperGPQA [25]have become apparent. These benchmarks, designed for simpler tasks like arithmetic, algebra, grade-school knowledge, or commonsense reasoning, are becoming outdated and saturated, failing to capture the trends of advanced reasoning. For example, GAIA [14] assesses LLMs general agentic abilities through real-world questions, while PaperBench [23] challenges LLMs to replicate 20 ICML machine learning papers, testing their abilities in coding, debugging, paper comprehension, and scientific reasoning. Despite these advancements, these benchmarks often lack either domain breadth - failing to comprehensively cover fields like science and humanities - or depth of difficulty - missing the professional rigor, timeliness, and complexity required for cutting-edge reasoning tasks. To address these shortcomings, we propose ACADREASON, benchmark designed evaluate the academic-level reasoning abilities of LLM and agent. 1 Figure 1 Overview of the ACADREASON benchmark construction and evaluation pipeline. It consists of three stages: (1) High-Quality Academic Papers Collection experts filter 430 papers across 5 domains into 50 top-tier theoretical works; (2) High-Reasoning Research Question Extraction research questions are refined into formal queries with golden answers containing sufficient reasoning; (3) Checklists and Hints Extraction background, definition, and methodology hints are provided together with verifiable, independent checklists. For evaluation, candidate responses are compared against golden answers and checklists, and GPT-5 mini assigns final scores. As shown in Figure 1, our approach involves extracting knowledge and synthesizing high-quality reasoning data from diverse, authoritative, and timely academic literature spanning domains such as philosophy, statistics, mathematics, economics, computer science, among others. Specifically, based on publication date and top-tier journal status, we select 430 papers from leading journals. From each paper, we extract only one research question, with the corresponding golden answer designed to cover the full scope of the papers contributions, thereby making each task demanding in workload and reasoning depth. Building on the extracted information, we further develop scoring checklist and hints, providing more detailed evaluation rules and experiments. Ultimately, we compile total of 50 high-quality research question, forming the ACADREASON benchmark. For evaluation, we adopt LLM-as-Judge as our evaluation method and utilize GPT-5-mini as the judge model, which conducts assessments based on detailed scoring criteria and checklist. Our experimental results demonstrate that ACADREASON provides challenging tasks for LLMs and agents. Even the latest and most powerful model, GPT-5, achieves only 16 points in pass rate and 40.6 points in checklist score. Furthermore, we find that reasoning models outperformed general models, with DeepSeek-R1 attaining checklist score of 23.8, higher than DeepSeek-V3s 15.9 points. We also test cutting-edge agents, OAgents achieve the highest score of 34 points among all models and frameworks, demonstrating agents strong capability in solving research problems, though significant room for improvement remains. Additionally, we introduce hints to investigate the impact of different types of knowledge during problem-solving. The experimental results indicate that the incorporation of hints, as supplementary information, positively contributes to model performance, with methodology hints yielding the most significant gains. This suggests that, compared to simple and easily accessible background information, the ACADREASON benchmark places greater emphasis on evaluating LLMs mastery of methods. Our contributions are as follows: We introduce ACADREASON benchmark, which provides multi-domain evaluation of LLMs high-level reasoning abilities, and introduces challenges to existing models in terms of both knowledge and reasoning capability. We evaluate SOTA LLMs and Agents, Our testing experiments demonstrate that general models underperform on ACADREASON while reasoning models and agents exhibit stronger but still improvable performance, validating the datasets challenge level and reasoning-centric design. We provide comprehensive and detailed evaluation metrics, along with different types of knowledge hints. This offers insights for uncovering the potential of LLMs and Agents, as well as guiding future improvement directions."
        },
        {
            "title": "2 Related Work",
            "content": "Large Reasoning Models and Agent With the release of Deepseek-R1 [8] and OpenAIs o1 [20] model, LRMs(Large Reasoning Models) have demonstrated remarkable performance in areas such as inference and academic competitions. Deepseek-R1 [8] extends the models reasoning chain through reinforcement learning approach, achieving impressive results. Qwen3 [31] offers hybrid reasoning mode alongside default mode, providing more flexible thinking strategies. Although LRMs possess exceptional capabilities in reasoning, they are constrained by their limited internal knowledge. The Agent Framework [5, 21, 26, 35, 37] builds upon the foundational abilities of LRMs and extends them with corresponding tools, enhancing the models capacity to acquire external knowledge. OAgents [35] conduct systematic empirical study on the GAIA benchmark and BrowseComp, achieving outstanding performance. MiroFlow [26] constructs its agent framework based on MCP and has achieved state-of-the-art results on multiple leaderboards. Reasoning Benchmark. Evaluating advanced reasoning capabilities remains central challenge in the development of language models. Benchmarks such as arXivBench [10] and PaperBench [23] have been designed to assess the research-related reasoning abilities of LLMs. arXivBench requires LLMs to generate accurate paper names and corresponding links, while PaperBench evaluates models ability to reproduce ICML papers. DeepResearch Bench [4] assembles multi-domain tasks to evaluate LLMs research-oriented reasoning. GAIA[14] presents real-world challenges that require models to demonstrate proficient tool usage, web search capabilities, and reasoning. BrowseComp [29] places greater emphasis on web search and the ability to synthesize information from multiple web pages. However, existing benchmarks are limited in two key aspects: some lack breadth of coverage, being overly focused on math and coding at the expense of fields like science and humanities, while others lack depth of reasoning, testing only superficial information integration rather than advanced, professional knowledge. In contrast, our work bridges this gap by integrating both dimensions, presenting novel and comprehensive challenge to evaluate the ability of LLMs and Agents to tackle cutting-edge academic research questions."
        },
        {
            "title": "3 ACADREASON Benchmark",
            "content": "In this section, we introduce the ACADREASON benchmark, which focuses on measuring the cutting-edge reasoning capabilities of LLMs. Our human annotation process encompasses multiple stages, including data collection, question extraction, and quality assurance, to ensure the quality and challenge level of the questions. To establish comprehensive evaluation framework, we incorporate hints and checklists based on questions and answers, thereby constructing robust evaluation methodology with corresponding metrics (specific data can be found in Appendix )."
        },
        {
            "title": "3.1 Task Specification",
            "content": "In ACADREASON, LLMs and agents serve as candidates and are tasked in the role of researcher. They are required to solve complex research questions extracted from high-level theoretical articles without access to the original text, relying either on internal knowledge or utilizing search tools to obtain additional information. Unlike simple information retrieval and integration, ACADREASON simulates real-world research scenarios, demanding that the models not only possess cutting-edge academic knowledge but also demonstrate deep reasoning capabilities. Formally, each task in ACADREASON benchmark contains such atomic fields: Question: Each question is research question constructed from the selected paper, which is self-contained, comprising (a) specific problem from the paper and (b) the minimal background necessary for comprehension. Hints: Supporting information provided to the candidate model. To analyze the impact of different information types, hints are divided into three categories: Background Hints: background knowledge and related work. Definition Hints: key concepts and terminology introduced in the paper. Methodology Hints: theoretical tools required for reasoning and proof. Checklist: Expert-designed checkpoints that capture key milestones in the reasoning process (e.g., logical steps or critical facts). Unlike static checklists in prior work, ours are dynamic, tailored to each question, and adapt in length to problem complexity. 3 Golden Answer: complete solution trajectory that fully satisfies all checklist requirements, covering background, definitions, derivations, and conclusions."
        },
        {
            "title": "3.2 Data annotation",
            "content": "Task construction in the ACADREASON benchmark follows strict principles to ensure quality, clarity, and alignment with high-information, high-reasoning challenges. Our data annotation pipeline consists of three components: 1. Collection of high-quality academic papers as raw material. 2. Extraction of high-reasoning question-answer pairs. 3. Development of checklists and hints based on golden answers. The annotation guideline can be found in Appendix High-Quality Academic Papers Collection To ensure the challenging nature of the questions in ACADREASON, we design meticulous data selection protocol. First, based on criteria including publication date and top-tier journal status, we collect 430 eligible papers from various leading journal websites. These papers cover wide range of domains and exhibit diverse domain-specific logics, though not all are necessarily suitable for conversion into question-answer format. Annotation experts are instructed to carefully review and filter these articles according to the following principles: 1. whether they contain challenging reasoning questions, 2. whether they consist of purely theoretical content. High-Reasoning Research Questions Extraction Based on the collected high-quality papers, annotators are required to extract high-reasoning questions and golden answers from them. First, annotators read the entire paper and identify its main contributions and core research questions. Then, they refine the research questions into formal questions that must meet the requirements of being Comprehensive and Challenging. Finally, based on the question and the full content of the paper, the annotators formulate golden answer that includes sufficient reasoning detailssuch as definitions, formulas, key concepts, and derivationswhile also satisfying the criteria of being Independent and Comprehensive. Checklist and Hints Extraction Based on the extracted questions, golden answers, and the full paper content, annotators further derive and organize hints and checklists. For hints, there are three types: background hints compiled from the introduction section of the paper, definition hints derived from core formulas and definitions in the paper, and methodology hints summarized from the main methodology section. These hints represent critical prompt information from the paper. For the checklist, annotators distill key scoring points from the golden answer, ensuring these points are verifiable and independent."
        },
        {
            "title": "3.3 Validation Process",
            "content": "To ensure that each question in the benchmark strictly adheres to the design principles and expectations, and to address the issues encountered in the annotation process, we implement multi-stage data validation pipeline. Only after successfully passing through all filtering stages and the final iterative validation loop will task be included in the final benchmark. The validation process guideline is shown in Figure 10. Data Screening Principles The ACADREASON benchmark is built upon 50 high-level theoretical papers as targeted papers, which are selected by panel of 10 experts specializing in five distinct fields: computer science, economics, law, mathematics, and philosophy. Annotation is performed by experts with masters degree or higher, or those pursuing Ph.D. or masters at leading universities, Papers are chosen according to three criteria: 1) publication in top-tier journals or conferences within their respective domains; 2) publication between 2023 and 2025; 3) purely theoretical content, excluding empirical research, reviews, and supplementary materials. These Screening principles ensure the difficulty and quality of ACADREASON. In Table 3, we present the sources of the 20 representative papers. Question Answerability Verification Since the ACADREASON benchmark requires models to conduct detailed research and demonstration, to prevent questions from being answered too broadly or evaluated ineffectively, we implement Question Answerability Verification. For each annotated question, it is assigned to three domain experts for quality inspection, the experts evaluate the questions based on three principles: clear boundaries of the question, completeness of information elements, and compliance with domain-specific logic. Only questions that meet all these criteria are retained."
        },
        {
            "title": "3.4 Evaluation method and metrics",
            "content": "Evaluation Prompt Previous work [13, 22, 32, 33, 36] often use exact match as the evaluation metrics, to provide comprehensive evaluation framework, we select GPT-5-mini as the judge model and design an LLM-as-Judge assessment scheme. Given question, the golden answer, and the corresponding checklist, the judge model evaluates the candidates response in two aspects: (i) exact correspondence to the golden answer (1 if all required information is present and non-contradictory; 0 otherwise); (ii) independent satisfaction of each checklist item (1 if fully satisfied; 0 for partial, missing, or conflicting content). The prompt can be found in Appendix B. Evaluation Metrics We use the following two metrics as our evaluation criteria: the pass rate, measuring exact agreement with the golden answer, and the checklist score, capturing the proportion of satisfied checklist items. Pass Rate (Rp): Probability of full match with standard answers. Scoring: sq {0, 1} per question. (cid:80)50 Total: Rp = q=1 sq 50 100 (max =100). Checklist Score (Rj): Probability of meeting checklist criteria. Scoring: cq,i {0, 1} per checklist item. (cid:80)5 (cid:80)50 Total: Rj = q=1 i=1 cq,i 250 100 (max = 100)."
        },
        {
            "title": "4.1 Experiments Settings",
            "content": "To comprehensively evaluate the performance of the ACADREASON benchmark, we conduct experiments from the following four perspectives: the performance of mainstream advanced reasoning models and general models on the benchmark, the performance of leading agent frameworks on the benchmark, the model performance with critical hint prompts, and detailed analysis of failure cases. For mainstream general models and reasoning models, we directly require the models to answer the corresponding questions. For agentic frameworks, we maintain their basic tool configurations. To further analyze the models mastery of knowledge across different dimensions, we design detailed ablation experiments to evaluate three distinct types of hints. Finally, we also provide an analysis of the failure reasons for current advanced models and agents, along with potential directions for future development. General Model & Reasoning Model For general models and reasoning models, the acareason benchmark focuses on evaluating their knowledge reserves and reasoning capabilities. We select general models such as GPT-oss [19], GPT4.1 [15], GPT-5 [17], DeepSeek-V3 [12], DeepSeek-V3.1 [3]and Claude-4-Sonnet [1], as well as powerful reasoning models including Qwen3 [31], DeepSeek-R1 [7], Kimi-k2 [24], Gemini-2.5-Pro [2], and o3 [16] as our baseline models. Agent Framework& Agent Model Compared to LLMs, the agent can actively gather necessary information using tools like web search and database queries, giving it enhanced retrieval capabilities. We select current state-ofthe-art OAgents (GPT-5 as basic model) [35], Gemini-2.5-Pro-DeepResearch [6], and o3-DeepResearch [18] as our agent framework baselines, and Tongyi DeepResearch [27], AFM [34], MiroThinker [26], WebDancer[30] and WebThinker [11] as our Agent baseline. Ablation Experiment with Hints To provide more comprehensive experimental analysis and insights, we conduct an ablation study to systematically investigate the effectiveness of the multi-hint mechanism. The hints, meticulously curated by hand, encapsulate high-quality background information, methodologies, and key definitions extracted from relevant research. In this experiment, we compare baseline models without hints against ablated models integrated with hints, evaluating their performance across GPT-5, GPT-4.1, o3, and etc. Detailed Failure Case Analysis The ACADREASON benchmark assesses the graduate-level reasoning abilities of LLMs, which typically require models to engage in deep thinking and generate multi-step reasoning chains. To thoroughly investigate the multi-step reasoning process and analyze failure patterns, we conduct detailed Failure Case Analysis. We select representative models GPT-5 and OAgents, presenting their reasoning chains and logic pathways."
        },
        {
            "title": "4.2 Main Experiment Result",
            "content": "Table 1 Performance of various Models and Agents on ACADREASON benchmark. Each entry shows Pass Rate Rp on the left and Checklist Score Rj on the right. Note that the best results are in bold. Model"
        },
        {
            "title": "General Model",
            "content": "GPT-5 GPT-oss DeepSeek-V3.1 DeepSeek-V3 Claude-4-sonnet GPT-4.1 Reasoning Model Qwen3 Kimi-k2 o3 DeepSeek-R1 Gemini-2.5-Pro Agent OAgents Gemini-2.5-Pro-Deepresearch Tongyi DeepResearch o3-Deepresearch AFM WebThinker MiroThinker WebDancer Overall CS Econ Law Math Phi 16/40.5 4/32.2 2/24.8 2/15.9 0/24.7 0/21. 6/20.3 6/20.3 4/33.4 2/23.8 2/22.3 34/65.1 28/53.4 20/30.9 14/47.1 14/40.5 8/36.4 0/26.5 0/16.4 0/13.5 0/12.6 0/9.0 0/5.4 0/4.5 0/0.0 0/6.3 0/6.3 0/8.1 0/0.0 0/2.7 30/55.0 40/45.0 0/5.4 20/36.0 10/46.5 22/50.0 0/26.3 0/14.0 20/46.1 0/34.2 0/27.6 10/15.8 0/23.7 0/18. 0/21.1 0/21.1 0/38.2 0/22.4 0/15.8 30/63.2 20/56.6 10/34.2 0/38.2 0/15.8 0/18.4 0/10.5 0/6.6 40/52.1 10/41.7 10/45.8 0/10.4 0/33.3 0/31.2 20/45.8 20/45.8 10/50.0 0/41.7 0/41.7 50/68.8 40/66.7 60/62.5 30/52.1 40/58.3 10/54.2 0/25.6 0/18.8 0/51.4 10/38.3 0/22.4 0/20.6 0/29.5 0/31. 0/12.1 0/12.1 0/40.2 0/30.8 0/25.2 50/75.7 10/44.9 0/32.7 0/54.2 10/32.7 0/19.4 0/29.0 0/15.0 20/56.6 0/49.1 0/39.6 0/34.0 0/47.2 0/37.7 10/41.5 10/41.5 10/50.9 10/45.3 10/49.1 10/64.2 30/71.7 30/47.2 20/64.2 10/62.3 11/51.1 0/45.3 0/35.8 As shown in Table 1, we present the results of over 10 LLMs and Agents. In terms of pass rate, even the most powerful models on the market exhibit subpar performance. For example, the latest and most powerful model, GPT-5, achieved only 16 pass rate and 40.6 checklist score. Most general models scored below 10 points in total. It is worth mentioning that powerful models such as GPT-4.1 and Claude-4-sonnet receive score of 0, indicating that ACADREASON is highly challenging. Compared to the stringent pass rate score, the checklist score effectively assesses how well models meet the established criteria and provides more detailed evaluation framework. We investigate the variation in checklist scores across different academic disciplines. As shown in Figure 2, the results indicate that Computer Science and Economics exhibit relatively lower score distributions, while Law and Philosophy demonstrate higher scores. This suggests that CS and Econ present greater challenges in the ACADREASON benchmark. 6 Figure 2 General performance on different domains in Checklist Score When comparing general models and reasoning models, the latter generally exhibit superior and more balanced performance. We compare general models and reasoning models from the same series. For example, compared to DeepSeek-V3 (2.0/15.9), DeepSeek-R1 achieves higher score (2.0/23.8). Similarly, o3 also outperforms GPT-4.1, demonstrating that reasoning models exhibit stronger performance within their respective series. ACADREASON-eval focuses more on assessing the reasoning capabilities of models. Within the same model families, newer versions consistently outperform their older counterparts. For example, GPT-5 outperforms GPT-4.1 in both pass rate and checklist score across multiple subjects. GPT-5 achieves an overall score of 16 and 40.5 for the pass rate and checklist score, respectively, while GPT-4.1 only manages 0 and 21.0. Similarly, DeepSeek-V3.1 shows notable improvements over DeepSeek-V3, with overall scores of 2.0/24.8 and 2.0/15.9. These comparisons clearly demonstrate the positive impact of model updates and iterations on performance enhancement, newer models generally have enhanced knowledge and reasoning ability. Agent frameworks outperform LLMs. OAgents achieves the best overall results among all evaluated models, with an overall pass rate of 34.0 and checklist score of 65.1, which consistently outperform both general and reasoning models across most domains, achieving top scores in Econ, Law, Math. This is because ACADREASON contains the most challenging knowledge sections currently available as the evaluation set, which places extremely high demands on both reasoning ability and knowledge mastery. For LLMs, even though they possess strong reasoning capabilities, they lack cutting-edge academic knowledge reserve. In contrast, the agent framework can compensate for knowledge gaps through autonomous information retrieval. However, the significant gap from the full score of 100 indicates that there is still room for improvement in the academic research tasks."
        },
        {
            "title": "4.3 Ablation Study",
            "content": "The multi-hint mechanism effectively bolsters the reasoning capabilities of large language models by supplying critical contextual. As shown in Table 2, the models performance on ACADREASON benchmark significantly improves when hints are provided, reaching the highest score when all hints are given. Taking GPT-5 as an example, without hints, the model only achieves score of (16.0/40.5). However, with all hints, it attains score of (40.0/67.8), surpassing the current state-of-the-art agent framework, OAgents. Table 2 Ablation experiment results across different hint settings. Each entry shows Pass Rate Rp on the left and Checklist Score Rj on the right. Note that best results are in bold. Models No Hint background definition methodology ALL Hints General Model GPT-5 GPT-oss DeepSeek-V3.1 DeepSeek-V3 GPT-4.1 Claude-4-sonnet Reasoning Model Qwen3 Kimi-k2.0 o3 DeepSeek-R1 16/40.5 4/32.2 2/24.8 2/15.9 0/21.0 0/24.7 6/30.4 6/20.3 4/33.4 2/23. 16/42.5 14/40.5 2/30.9 4/25.1 2/26.3 2/24.6 14/35.7 2/32.9 12/38.0 4/30.6 24/50.9 10/42.3 8/37.2 4/26.1 0/29.9 2/30.6 10/40.5 10/36.5 10/48.9 6/35.7 34/64.3 16/52.2 12/45.3 4/38.5 8/42.8 14/40.8 20/49.1 16/46.8 28/56.2 8/45. 40/67.8 22/58.5 20/54.7 6/44.1 20/51.6 11.3/49.3 22/52.7 16/51.6 26/60.8 20/50.4 Different hint types provide varying benefits, with methodology hints yielding the most significant gains. We further compare the impact of different hint types on models. As shown in the Figure 3a, we calculate the absolute gain in model accuracy for each hint type. We find that for the vast majority of models, methodology hints provide the highest gain, while background hints provide the smallest relative gain. This suggests that in ACADREASON benchmark, the focus is more on testing models mastery of deep methods, rather than its ability to process simple, easily accessible background information. 7 (a) The performance gain of various models across different hint types. (b) The average performance gain across different hint types and disciplinary categories. Figure 3 Ablation study results. (a) shows the performance gain per model, while (b) presents the average gain across disciplines. The benefits of different types of hints vary across different disciplines. As shown in Figure 3b, we present the impact of different types of hints across various academic disciplines. We calculate the average improvement for all models, with additional results available in Appendix E. The experimental results indicate that compared to humanities subjects (Eco, Law, Phi), STEM subjects (CS, Math) achieve less improvement. This suggests that humanities disciplines place greater emphasis on the acquisition of external knowledge, while STEM fields require deeper reasoning. Furthermore, each discipline exhibits distinct focuses. For Law and Phi, hints related to methodology and background information are more important, whereas for Eco, definitions are more emphasized. This reflects the unique characteristics of different academic domains and demonstrates the comprehensiveness of the ACADREASON evaluation."
        },
        {
            "title": "5 Case study",
            "content": "To provide comparison of the leading technical paradigms, we conduct case study featuring the top-scoring agent, OAgents [35], and the top-scoring single model, GPT-5 [17]. We select representative case from the ACADREASON benchmark where models are required to analyze the misuse of the term \"counterfeit\" in design patent law, as shown in figure 4. We evaluate the models responses against checklist of four required actions: to point out the legal fallacy, refute the false safety proposition, analyze the root cause, and identify the judicial impact. The comparison reveals difference: OAgents successfully address all four points for perfect score, while GPT-5 only addresses two. OAgents provide complete analysis, covering all required dimensions, whereas GPT-5 only succeeds in identifying the direct legal fallacy and the judicial impacts (Points 1 and 4). The evaluation data indicates this performance gap is due to difference in reasoning depth, not simple lack of knowledge. GPT-5s failure on the \"false safety proposition\" (Point 2) stemmed from an inability to move beyond surface-level association of \"counterfeit\" with \"consumer harm\". It did not perform the deeper reasoning required to explicitly refute the narrative by stating that design patents are not quality certifications. Similarly, for the \"root cause\" (Point 3), GPT-5 identified general \"procedural leverage\" but failed to synthesize this with political and economic context to identify the specific \"coordinated lobbying strategy\" required by the checklist. This case demonstrates that while top-tier single model can handle direct legal analysis, the agentic framework of OAgents enables higher-order, critical synthesis necessary to deconstruct the underlying rhetorical and political motives of complex issue."
        },
        {
            "title": "6 Conclusion",
            "content": "In this work, we introduce the ACADREASON benchmark, which comprehensively evaluates the ability of LLMs and agents to acquire and reason over advanced knowledge. The ACADREASON benchmark includes 50 evaluation items across five domains, providing comprehensive assessment of models research capabilities. Our experimental results show that even the most advanced model, GPT-5, achieves only 16.0 points, while an advanced agent framework scores 34.0 points. These results demonstrate the difficulty and challenging nature of ACADREASON, indicating that 8 Figure 4 Side-by-side comparison of OAgents and GPT-5 on the legal reasoning task. current models still have considerable room for improvement. By releasing the entire annotated data and preliminary benchmarking results, we aim to empower the research community to better evaluate and enhance LLMs reasoning capabilities. Our approach represents significant step towards diversifying LLM reasoning benchmarks and utilizing the vast potential of academic research artifacts in advancing LLM research."
        },
        {
            "title": "7 Contributions",
            "content": "Core Contributors Xin Gui King Zhu Contributors Qianben Chen Xiaowan Li Zekun Moore Wang Yizhi Li Xinpeng Liu Wenli Ren Linyu Miao Corresponding Authors Ge Zhang Wangchunshu Zhou"
        },
        {
            "title": "References",
            "content": "Jincheng Ren Tianrui Qin Ziqi Shu He Zhu Dingfeng Shi Xiangru Tang Jiaheng Liu Yuchen Eleanor Jiang Minghao Liu [1] anthropic. Claude 4 sonnet, 2025. URL https://www.anthropic.com/news/claude-4. [2] Gheorghe Comanici, Eric Bieber, Mike Schaekermann, Ice Pasupat, Noveen Sachdeva, Inderjit Dhillon, Marcel Blistein, Ori Ram, Dan Zhang, Evan Rosen, et al. Gemini 2.5: Pushing the frontier with advanced reasoning, multimodality, long context, and next generation agentic capabilities. arXiv preprint arXiv:2507.06261, 2025. [3] DeepSeek-AI. Deepseek-v3 technical report, 2024. URL https://arxiv.org/abs/2412.19437. [4] Mingxuan Du, Benfeng Xu, Chiwei Zhu, Xiaorui Wang, and Zhendong Mao. Deepresearch bench: comprehensive benchmark for deep research agents. arXiv preprint, 2025. [5] Tianqing Fang, Zhisong Zhang, Xiaoyang Wang, Rui Wang, Can Qin, Yuxuan Wan, Jun-Yu Ma, Ce Zhang, Jiaqi Chen, Xiyun Li, et al. Cognitive kernel-pro: framework for deep research agents and agent foundation models training. arXiv preprint arXiv:2508.00414, 2025. [6] google. Gemini deep-research, 2025. URL https://gemini.google/overview/deep-research/. [7] Daya Guo, Dejian Yang, Haowei Zhang, Junxiao Song, Ruoyu Zhang, Runxin Xu, Qihao Zhu, Shirong Ma, Peiyi Wang, Xiao Bi, et al. Deepseek-r1: Incentivizing reasoning capability in llms via reinforcement learning. arXiv preprint arXiv:2501.12948, 2025. [8] Daya Guo, Dejian Yang, Haowei Zhang, Junxiao Song, Ruoyu Zhang, Runxin Xu, Qihao Zhu, Shirong Ma, Peiyi Wang, Xiao Bi, et al. Deepseek-r1: Incentivizing reasoning capability in llms via reinforcement learning. arXiv preprint arXiv:2501.12948, 2025. [9] Zixuan Ke, Fangkai Jiao, Yifei Ming, Xuan-Phi Nguyen, Austin Xu, Do Xuan Long, Minzhi Li, Chengwei Qin, Peifeng Wang, Silvio Savarese, Caiming Xiong, and Shafiq Joty. survey of frontiers in llm reasoning: Inference scaling, learning to reason, and agentic systems. arXiv preprint arXiv: 2504.09037, 2025. [10] Ning Li, Jingran Zhang, and Justin Cui. Arxivbench: Can llms assist researchers in conducting research? arXiv preprint arXiv: 2504.10496, 2025. URL https://arxiv.org/abs/2504.10496. 10 [11] Xiaoxi Li, Jiajie Jin, Guanting Dong, Hongjin Qian, Yutao Zhu, Yongkang Wu, Ji-Rong Wen, and Zhicheng Dou. Webthinker: Empowering large reasoning models with deep research capability. CoRR, abs/2504.21776, 2025. doi: 10.48550/ARXIV.2504. 21776. URL https://doi.org/10.48550/arXiv.2504.21776. [12] Aixin Liu, Bei Feng, Bin Wang, Bingxuan Wang, Bo Liu, Chenggang Zhao, Chengqi Deng, Chong Ruan, Damai Dai, Daya Guo, et al. Deepseek-v2: strong, economical, and efficient mixture-of-experts language model. CoRR, 2024. [13] David Ma, Yuanxing Zhang, Jincheng Ren, Jarvis Guo, Yifan Yao, Zhenlin Wei, Zhenzhu Yang, Zhongyuan Peng, Boyu Feng, Jun Ma, et al. Iv-bench: benchmark for image-grounded video perception and reasoning in multimodal llms. arXiv preprint arXiv:2504.15415, 2025. [14] Grégoire Mialon, Clémentine Fourrier, Thomas Wolf, Yann LeCun, and Thomas Scialom. Gaia: benchmark for general ai assistants. International Conference on Learning Representations, 2024. [15] OpenAI. Introducing gpt-4.1, 2024. URL https://openai.com/index/gpt-4-1/. [16] OpenAI. Introducing o3 and o4 mini. https://openai.com/index/introducing-o3-and-o4-mini/, 2024. Blog post. Accessed: October 6, 2024. [17] openai. gpt-5-system-card, 2025. URL https://openai.com/index/gpt-5-system-card/. [18] openai. openai deep-research, 2025. URL https://openai.com/index/introducing-deep-research/. [19] OpenAI. gpt-oss-120b & gpt-oss-20b model card, 2025. URL https://arxiv.org/abs/2508.10925. [20] OpenAI, :, Aaron Jaech, Adam Kalai, Adam Lerer, Adam Richardson, Ahmed El-Kishky, Aiden Low, Alec Helyar, Aleksander Madry, Alex Beutel, Alex Carney, Alex Iftimie, Alex Karpenko, Alex Tachard Passos, Alexander Neitz, Alexander Prokofiev, Alexander Wei, Allison Tam, Ally Bennett, Ananya Kumar, Andre Saraiva, Andrea Vallone, Andrew Duberstein, Andrew Kondrich, Andrey Mishchenko, Andy Applebaum, Angela Jiang, Ashvin Nair, Barret Zoph, Behrooz Ghorbani, Ben Rossen, Benjamin Sokolowsky, Boaz Barak, Bob McGrew, Borys Minaiev, Botao Hao, Bowen Baker, Brandon Houghton, Brandon McKinzie, Brydon Eastman, Camillo Lugaresi, Cary Bassin, Cary Hudson, Chak Ming Li, Charles de Bourcy, Chelsea Voss, Chen Shen, Chong Zhang, Chris Koch, Chris Orsinger, Christopher Hesse, Claudia Fischer, Clive Chan, Dan Roberts, Daniel Kappler, Daniel Levy, Daniel Selsam, David Dohan, David Farhi, David Mely, David Robinson, Dimitris Tsipras, Doug Li, Dragos Oprica, Eben Freeman, Eddie Zhang, Edmund Wong, Elizabeth Proehl, Enoch Cheung, Eric Mitchell, Eric Wallace, Erik Ritter, Evan Mays, Fan Wang, Felipe Petroski Such, Filippo Raso, Florencia Leoni, Foivos Tsimpourlas, Francis Song, Fred von Lohmann, Freddie Sulit, Geoff Salmon, Giambattista Parascandolo, Gildas Chabot, Grace Zhao, Greg Brockman, Guillaume Leclerc, Hadi Salman, Haiming Bao, Hao Sheng, Hart Andrin, Hessam Bagherinezhad, Hongyu Ren, Hunter Lightman, Hyung Won Chung, Ian Kivlichan, Ian OConnell, Ian Osband, Ignasi Clavera Gilaberte, Ilge Akkaya, Ilya Kostrikov, Ilya Sutskever, Irina Kofman, Jakub Pachocki, James Lennon, Jason Wei, Jean Harb, Jerry Twore, Jiacheng Feng, Jiahui Yu, Jiayi Weng, Jie Tang, Jieqi Yu, Joaquin Quiñonero Candela, Joe Palermo, Joel Parish, Johannes Heidecke, John Hallman, John Rizzo, Jonathan Gordon, Jonathan Uesato, Jonathan Ward, Joost Huizinga, Julie Wang, Kai Chen, Kai Xiao, Karan Singhal, Karina Nguyen, Karl Cobbe, Katy Shi, Kayla Wood, Kendra Rimbach, Keren Gu-Lemberg, Kevin Liu, Kevin Lu, Kevin Stone, Kevin Yu, Lama Ahmad, Lauren Yang, Leo Liu, Leon Maksin, Leyton Ho, Liam Fedus, Lilian Weng, Linden Li, Lindsay McCallum, Lindsey Held, Lorenz Kuhn, Lukas Kondraciuk, Lukasz Kaiser, Luke Metz, Madelaine Boyd, Maja Trebacz, Manas Joglekar, Mark Chen, Marko Tintor, Mason Meyer, Matt Jones, Matt Kaufer, Max Schwarzer, Meghan Shah, Mehmet Yatbaz, Melody Y. Guan, Mengyuan Xu, Mengyuan Yan, Mia Glaese, Mianna Chen, Michael Lampe, Michael Malek, Michele Wang, Michelle Fradin, Mike McClay, Mikhail Pavlov, Miles Wang, Mingxuan Wang, Mira Murati, Mo Bavarian, Mostafa Rohaninejad, Nat McAleese, Neil Chowdhury, Neil Chowdhury, Nick Ryder, Nikolas Tezak, Noam Brown, Ofir Nachum, Oleg Boiko, Oleg Murk, Olivia Watkins, Patrick Chao, Paul Ashbourne, Pavel Izmailov, Peter Zhokhov, Rachel Dias, Rahul Arora, Randall Lin, Rapha Gontijo Lopes, Raz Gaon, Reah Miyara, Reimar Leike, Renny Hwang, Rhythm Garg, Robin Brown, Roshan James, Rui Shu, Ryan Cheu, Ryan Greene, Saachi Jain, Sam Altman, Sam Toizer, Sam Toyer, Samuel Miserendino, Sandhini Agarwal, Santiago Hernandez, Sasha Baker, Scott McKinney, Scottie Yan, Shengjia Zhao, Shengli Hu, Shibani Santurkar, Shraman Ray Chaudhuri, Shuyuan Zhang, Siyuan Fu, Spencer Papay, Steph Lin, Suchir Balaji, Suvansh Sanjeev, Szymon Sidor, Tal Broda, Aidan Clark, Tao Wang, Taylor Gordon, Ted Sanders, Tejal Patwardhan, Thibault Sottiaux, Thomas Degry, Thomas Dimson, Tianhao Zheng, Timur Garipov, Tom Stasi, Trapit Bansal, Trevor Creech, Troy Peterson, Tyna Eloundou, Valerie Qi, Vineet Kosaraju, Vinnie Monaco, Vitchyr Pong, Vlad Fomenko, Weiyi Zheng, Wenda Zhou, Wes McCabe, Wojciech Zaremba, Yann Dubois, Yinghai Lu, Yining Chen, Young Cha, Yu Bai, Yuchen He, Yuchen Zhang, Yunyun Wang, Zheng Shao, and Zhuohan Li. Openai o1 system card. arXiv preprint arXiv: 2412.16720, 2024. [21] Tianrui Qin, Qianben Chen, Sinuo Wang, He Xing, King Zhu, He Zhu, Dingfeng Shi, Xinxin Liu, Ge Zhang, Jiaheng Liu, et al. Flash-searcher: Fast and effective web agents via dag-based parallel execution. arXiv preprint arXiv:2509.25301, 2025. 11 [22] David Rein, Betty Li Hou, Asa Cooper Stickland, Jackson Petty, Richard Yuanzhe Pang, Julien Dirani, Julian Michael, and Samuel R. Bowman. Gpqa: graduate-level google-proof q&a benchmark. arXiv preprint arXiv: 2311.12022, 2023. [23] Giulio Starace, Oliver Jaffe, Dane Sherburn, James Aung, Jun Shern Chan, Leon Maksin, Rachel Dias, Evan Mays, Benjamin Kinsella, Wyatt Thompson, Johannes Heidecke, Amelia Glaese, and Tejal Patwardhan. Paperbench: Evaluating ais ability to replicate ai research. arXiv preprint arXiv: 2504.01848, 2025. URL https://arxiv.org/abs/2504.01848. [24] Kimi Team, Yifan Bai, Yiping Bao, Guanduo Chen, Jiahao Chen, Ningxin Chen, Ruijue Chen, Yanru Chen, Yuankun Chen, Yutian Chen, Zhuofu Chen, Jialei Cui, Hao Ding, Mengnan Dong, Angang Du, Chenzhuang Du, Dikang Du, Yulun Du, Yu Fan, Yichen Feng, Kelin Fu, Bofei Gao, Hongcheng Gao, Peizhong Gao, Tong Gao, Xinran Gu, Longyu Guan, Haiqing Guo, Jianhang Guo, Hao Hu, Xiaoru Hao, Tianhong He, Weiran He, Wenyang He, Chao Hong, Yangyang Hu, Zhenxing Hu, Weixiao Huang, Zhiqi Huang, Zihao Huang, Tao Jiang, Zhejun Jiang, Xinyi Jin, Yongsheng Kang, Guokun Lai, Cheng Li, Fang Li, Haoyang Li, Ming Li, Wentao Li, Yanhao Li, Yiwei Li, Zhaowei Li, Zheming Li, Hongzhan Lin, Xiaohan Lin, Zongyu Lin, Chengyin Liu, Chenyu Liu, Hongzhang Liu, Jingyuan Liu, Junqi Liu, Liang Liu, Shaowei Liu, T. Y. Liu, Tianwei Liu, Weizhou Liu, Yangyang Liu, Yibo Liu, Yiping Liu, Yue Liu, Zhengying Liu, Enzhe Lu, Lijun Lu, Shengling Ma, Xinyu Ma, Yingwei Ma, Shaoguang Mao, Jie Mei, Xin Men, Yibo Miao, Siyuan Pan, Yebo Peng, Ruoyu Qin, Bowen Qu, Zeyu Shang, Lidong Shi, Shengyuan Shi, Feifan Song, Jianlin Su, Zhengyuan Su, Xinjie Sun, Flood Sung, Heyi Tang, Jiawen Tao, Qifeng Teng, Chensi Wang, Dinglu Wang, Feng Wang, Haiming Wang, Jianzhou Wang, Jiaxing Wang, Jinhong Wang, Shengjie Wang, Shuyi Wang, Yao Wang, Yejie Wang, Yiqin Wang, Yuxin Wang, Yuzhi Wang, Zhaoji Wang, Zhengtao Wang, Zhexu Wang, Chu Wei, Qianqian Wei, Wenhao Wu, Xingzhe Wu, Yuxin Wu, Chenjun Xiao, Xiaotong Xie, Weimin Xiong, Boyu Xu, Jing Xu, Jinjing Xu, L. H. Xu, Lin Xu, Suting Xu, Weixin Xu, Xinran Xu, Yangchuan Xu, Ziyao Xu, Junjie Yan, Yuzi Yan, Xiaofei Yang, Ying Yang, Zhen Yang, Zhilin Yang, Zonghan Yang, Haotian Yao, Xingcheng Yao, Wenjie Ye, Zhuorui Ye, Bohong Yin, Longhui Yu, Enming Yuan, Hongbang Yuan, Mengjie Yuan, Haobing Zhan, Dehao Zhang, Hao Zhang, Wanlu Zhang, Xiaobin Zhang, Yangkun Zhang, Yizhi Zhang, Yongting Zhang, Yu Zhang, Yutao Zhang, Yutong Zhang, Zheng Zhang, Haotian Zhao, Yikai Zhao, Huabin Zheng, Shaojie Zheng, Jianren Zhou, Xinyu Zhou, Zaida Zhou, Zhen Zhu, Weiyu Zhuang, and Xinxing Zu. Kimi k2: Open agentic intelligence, 2025. URL https://arxiv.org/abs/2507.20534. [25] M-A-P Team, Xinrun Du, Yifan Yao, Kaijing Ma, Bingli Wang, Tianyu Zheng, Kang Zhu, Minghao Liu, Yiming Liang, Xiaolong Jin, Zhen-Nan Wei, Chujie Zheng, Kaixin Deng, Shuyue Guo, Shian Jia, Sichao Jiang, Yiyan Liao, Rui Li, Qinrui Li, Sirun Li, Yizhi Li, Yunwen Li, Dehua Ma, Yuansheng Ni, Haoran Que, Qiyao Wang, Zhoufutu Wen, Si-Xuan Wu, Tianshun Xing, Ming Xu, Zhenzhu Yang, Ze Wang, Junting Zhou, Yu Bai, Xingyuan Bu, Chenglin Cai, Liang Chen, Yifan Chen, Chengtuo Cheng, Tianhao Cheng, Keyi Ding, Siming Huang, Yun-Jing Huang, Yaoru Li, Yizhe Li, Zhaoqun Li, Tianhao Liang, Chengdong Lin, Hongquan Lin, Yi-Hui Ma, Zhongyuan Peng, Zifan Peng, Qige Qi, Shi Qiu, Xingwei Qu, Yizhou Tan, Zili Wang, Chenqing Wang, Hao Wang, Yiya Wang, Yubo Wang, Jiajun Xu, Kexin Yang, Ru-Qing Yuan, Yuan hao Yue, Tianyang Zhan, Chun Zhang, Jing-Yun Zhang, Xiyue Zhang, Xing Zhang, Yue Zhang, Yongchi Zhao, Xiangyu Zheng, Chenghua Zhong, Yang Gao, Zhoujun Li, Dayiheng Liu, Qian Liu, Tianyu Liu, Shiwen Ni, Junran Peng, Yujia Qin, Wenbo Su, Guoyin Wang, Shi Wang, Jian Yang, Min Yang, Meng Cao, Xiang Yue, Zhaoxiang Zhang, Wangchunshu Zhou, Jiaheng Liu, Qunshu Lin, Wenhao Huang, and Ge Zhang. Supergpqa: Scaling llm evaluation across 285 graduate disciplines. 2025. [26] MiroMind AI Team. Miroflow: high-performance open-source research agent framework. https://github.com/ MiroMindAI/MiroFlow, 2025. [27] Tongyi DeepResearch Team. Tongyi-deepresearch. https://github.com/Alibaba-NLP/DeepResearch, 2025. [28] Yubo Wang, Xueguang Ma, Ge Zhang, Yuansheng Ni, Abhranil Chandra, Shiguang Guo, Weiming Ren, Aaran Arulraj, Xuan He, Ziyan Jiang, Tianle Li, Max W.F. Ku, Kai Wang, Alex Zhuang, Rongqi \"Richard\" Fan, Xiang Yue, and Wenhu Chen. Mmlu-pro: more robust and challenging multi-task language understanding benchmark. Neural Information Processing Systems, 2024. doi: 10.48550/arXiv.2406.01574. [29] Jason Wei, Zhiqing Sun, Spencer Papay, Scott McKinney, Jeffrey Han, Isa Fulford, Hyung Won Chung, Alex Tachard Passos, William Fedus, and Amelia Glaese. Browsecomp: simple yet challenging benchmark for browsing agents. arXiv preprint arXiv:2504.12516, 2025. [30] Jialong Wu, Baixuan Li, Runnan Fang, Wenbiao Yin, Liwen Zhang, Zhengwei Tao, Dingchu Zhang, Zekun Xi, Gang Fu, Yong Jiang, Pengjun Xie, Fei Huang, and Jingren Zhou. WebDancer: Towards autonomous information seeking agency. arXiv preprint arXiv:2505.22648, 2025. [31] An Yang, Anfeng Li, Baosong Yang, Beichen Zhang, Binyuan Hui, Bo Zheng, Bowen Yu, Chang Gao, Chengen Huang, Chenxu Lv, et al. Qwen3 technical report. arXiv preprint arXiv:2505.09388, 2025. [32] Xiang Yue, Yuansheng Ni, Kai Zhang, Tianyu Zheng, Ruoqi Liu, Ge Zhang, Samuel Stevens, Dongfu Jiang, Weiming Ren, Yuxuan Sun, et al. Mmmu: massive multi-discipline multimodal understanding and reasoning benchmark for expert agi. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 95569567, 2024. 12 [33] Ge Zhang, Xinrun Du, Bei Chen, Yiming Liang, Tongxu Luo, Tianyu Zheng, Kang Zhu, Yuyang Cheng, Chunpu Xu, Shuyue Guo, et al. Cmmmu: chinese massive multi-discipline multimodal understanding benchmark. arXiv preprint arXiv:2401.11944, 2024. [34] Yusen Zhang, Ruoxi Sun, Yanfei Chen, Tomas Pfister, Rui Zhang, and Sercan Ö. Arik. Chain of agents: Large language models collaborating on long-context tasks. arXiv preprint arXiv: 2406.02818, 2024. [35] He Zhu, Tianrui Qin, King Zhu, Heyuan Huang, Yeyi Guan, Jinxiang Xia, Yi Yao, Hanhao Li, Ningning Wang, Pai Liu, et al. Oagents: An empirical study of building effective agents. arXiv preprint arXiv:2506.15741, 2025. [36] King Zhu, Qianbo Zang, Shian Jia, Siwei Wu, Feiteng Fang, Yizhi Li, Shawn Gavin, Tuney Zheng, Jiawei Guo, Bo Li, et al. Lime: Less is more for mllm evaluation. arXiv preprint arXiv:2409.06851, 2024. [37] King Zhu, Hanhao Li, Siwei Wu, Tianshun Xing, Dehua Ma, Xiangru Tang, Minghao Liu, Jian Yang, Jiaheng Liu, Yuchen Eleanor Jiang, et al. Scaling test-time compute for llm agents. arXiv preprint arXiv:2506.12928, 2025."
        },
        {
            "title": "A Data Statistics",
            "content": "The rigorous curation pipeline culminates in the final Acadreason benchmark, which comprises 50 high-reasoning academic questions. In Figure 5, we present the category distribution of the dataset. Each category in the ACADREASON benchmark includes 5 samples. Table 3 presents 20 representative papers included in the ACADREASON benchmark. All papers were selected from publicly available top-tier journals or conferences, curation strategy that ensures the academic rigor and quality of the benchmark dataset originate from its source. Figure 5 Category Distribution Table 3 Representative List of 20 Papers in AcadReason benchmark. Paper Category Source Reliability and Latency Analysis for Wireless Communication Systems with Secret-Key Budget On the PopovBelevitchHautus tests for functional observability and output controllability Algebraic Geometry codes in the sum-rank metric Variance Decay Property for Filter Stability Once more, without feeling Patchwork ethnography Pig-feast democracy. . . in West Papua Moral Understanding Between You and Me Women who pay their own brideprice. . . Math Math Math Math Philosophy Philosophy Philosophy Philosophy Philosophy Computer Science Computer Science Lower Bound for Light Spanners in General Graphs Tight Streaming Lower Bounds for Deterministic Approximate Counting Refutation of the PachTardos Conjecture for 0-1 Matrices Computer Science Computer Science Universal Perfect Samplers for Incremental Streams Computer Science Quasi-Monte Carlo Beyond Hardy-Krause Law Waste, Property, and Useless Things Law The Law and Lawlessness of U.S. Immigration Detention Law Human Rights Obligations in Maritime Search and Rescue State Immunity from Non-Judicial Measures of Constraint Law Informational Black Holes in Financial Markets Theory of Dynamic Inflation Targets Economy Economy IEEE Transactions on Communications, 2024, 72(2): 10331044 Automatica, 2025, 174(1): 112122 IEEE Transactions on Information Theory, 2024, 70(5): 33453356 IEEE Transactions on Automatic Control, 2024, online first Philosophy & Phenomenological Research, 2025, 111(1): 343365 American Ethnologist, 2024, 51(1): 131139 American Ethnologist, 2024, 51(2): 193206 Philosophy & Public Affairs, 2024, 52(3): 327357 Journal of the Royal Anthropological Institute, 2025, 31(2): 493512 Proceedings of SODA 2025: 43274337 Proceedings of SODA 2025 (Best Student Paper) Proceedings of SODA 2025 Proceedings of SODA 2025: 34093429 Proceedings of SODA 2025: 20512075 Harvard Law Review, 2025, Vol. 138 (accepted) Harvard Law Review, 2025, 138(5): 1186 International & Comparative Law Quarterly, 2025, 74(1): 3360 International & Comparative Law Quarterly, 2025, 74(1): 179204 Journal of Finance, 2023, 78(6): 30993140 American Economic Review, 2025, 115(2):"
        },
        {
            "title": "B Prompt for Infer and Evaluation",
            "content": "The prompts we used for ACADREASON benchmark are shown below. PROMPT FOR INFER Please answer the following question: Question: {query} Provide precise and detailed response. PROMPT FOR EVAL Figure 6 Prompt for infer Task: Judge the following attempt answer to an academic question based on the provided question, checklist criteria, and golden answer reference. Judgement Criteria Aspect 1: Answer Correspondence Judge if the answer corresponds to the golden answer: 1 point: The answer contains all the information of the golden answer 0 point: The answer completely fails to meet or only partially meets the key information required by the golden answer, or if there are contradictions Aspect 2: Checklist Requirements For every item on the checklist, judge independently whether the answer meets the requirement. The hints are provided to help you judge: 1 point: The reasoning and answer meet the requirement 0 point: The reasoning and answer do not meet the requirement, or only partially meet the requirement Data Information Inputs Question: {query} Checklist: {checklist Answer to judge Attempted Answer: {response} Golden Output Golden Answer: {golden_answer} Output Format Please respond strictly in the JSON format provided below. Note that the number of items in the checklist should be equal to the number of items in the justifications and scores for aspect 2. The number of checklist items can vary. Example Output {{ \"aspect_1_analysis\": \"Give the reason for how to score the aspect 1\", \"aspect_1_score\": 0, \"aspect_2_analysis_1\": \"Give the reason for how to score the first item in the checklist in aspect 2\", \"aspect_2_score_1\": 0, \"aspect_2_analysis_2\": \"Give the reason for how to score the second item in the checklist in aspect 2\", \"aspect_2_score_2\": 0, ... }} Figure 7 Prompt for eval"
        },
        {
            "title": "C LLM Usage",
            "content": "Large language models (LLMs) are used in this work exclusively for text polishing and language refinement during the writing process. Specifically, LLMs assist in improving the fluency, clarity, and conciseness of the writing. LLMs are not used for any aspects of experimental design, methodological development or scientific interpretation. All scientific contributions and innovations presented in this work are entirely human-originated."
        },
        {
            "title": "D Annotation and validation guideline",
            "content": "RESEARCH QUESTIONS ANNOTATION GUIDELINE Your task is to extract one high-quality research question from provided academic paper and then construct comprehensive golden answer for it. The final question-answer pair should be self-contained, accurately reflecting the papers core theoretical contribution, and must be solvable without requiring access to the original text. The primary goal is to create challenging benchmark item that tests advanced reasoning. Research Question Clarity and Self-consistency: Questions should have well-defined boundaries and include minimal necessary background, focusing on specific theoretical problems. Alignment and Independence: Questions must align with the papers core contribution and be answerable without requiring access to the full text. Structural Constraints: Avoid open-ended formulations, composite questions requiring decomposition, or references to the original papers structure. Golden Answer Comprehensive Coverage: Answers should cover background, definitions, derivations/proofs, and conclusions, satisfying all checklist requirements. Verifiability: Provide key intermediate steps and essential formulas to ensure reproducibility and self-contained reasoning. Content Integrity: Maintain logical continuity without skipping critical steps, introducing external information, or violating domain-specific conventions. Figure 8 Guideline For Research Question Annotation 16 HINTS AND CHECKLIST ANNOTATION GUIDELINE Your task is to create Hints and Checklist based on the provided Research Question, Golden Answer, and the original paper. The Hints should provide necessary but incomplete support for reasoning, while the Checklist must enable clear and objective verification of complete answer. Hints Annotaion Background: Context from the introduction and related work necessary to understand the problem. Definitions: Standardized statements of core concepts and terminology. Methods: Essential theoretical tools, methodological frameworks, and key technical tips. Selection Principle: Include only information necessary to facilitate reasoning, avoiding final conclusions or direct answers. Checklists Annotaion Atomicity: Each item contains single step or fact. Decidability: Criteria for fulfilling each item are clear and binary. Independence: Minimizing dependencies between different items. Source: Key steps and evidential facts are extracted from the Golden Answer. Explicit Referencing: Phrasing items as checks for specific statements (e.g., Did it prove that [statement]?) instead of referencing internal labels. Figure 9 Guideline For hints and checklist Annotation VALIDATION GUIDELINE Your task is to ensure the creation of high-quality dataset for complex reasoning. You will be responsible for reviewing and refining data items, each consisting of Research Question, Hints, Checklist, and Golden Answer. Data Screening Source Verification: Confirm the academic authority and timeliness of data sources. Content Qualification: Ensure the content is purely theoretical, excluding applied and empirical materials. Difficulty Assessment: Filter for problems with high reasoning complexity and intellectual challenge. Question Answerability Verification The core principle is to ensure the question itself is well-defined and answerable. Clear Boundaries: The input conditions, solution scope, and final objectives of the question must be unambiguous. Complete Information: Provide the minimal necessary background knowledge and key information points required for understanding and solving the problem. Logical Compliance: The problem statement and reasoning process must strictly adhere to the norms and theoretical framework of the respective discipline. Consistency Check Conduct systematic verification of the four core components that constitute complete data item: Overall Consistency: The Question, Hints, Checklist, and Golden Answer must be logically self-consistent, mutually supportive, and free of contradictions. Verifiability of Checklist Items: Each item in the checklist must correspond to explicit evidence in the Golden Answer, with verification criteria that are clear and actionable. Figure 10 Guideline For Quality Validation"
        },
        {
            "title": "E More Experiment Result",
            "content": "Table 4 Performance of various Models on ACADREASON benchmark, providing with background hint. Each entry shows Pass Rate Rp on the left and Checklist Score Rj on the right. Note that best results are in bold. Model Overall CS Econ Law Math Phi"
        },
        {
            "title": "General Model",
            "content": "GPT-5 GPT-oss DeepSeek-V3 GPT-4.1 Claude-4-sonnet DeepSeek-V3.1 Reasoning Model 16.0/42.5 14.0/40.5 4.0/25.1 2.0/26.3 2.0/24.6 2.0/30.9 0.0/8.1 0.0/19.8 0.0/2.7 0.0/2.7 0.0/0.0 0.0/13.5 0.0/47.4 0.0/32.9 0.0/17.1 0.0/21.1 0.0/17.1 0.0/19.7 50.0/60.4 40.0/56.2 10.0/47.9 0.0/41.7 10.0/45.8 0.0/39. 0.0/49.5 10.0/42.1 0.0/28.0 0.0/30.8 0.0/29.0 0.0/33.6 30.0/77.4 20.0/77.4 10.0/56.6 10.0/60.4 0.0/58.5 10.0/69.8 Qwen3 o3 DeepSeek-R1 Gemini-2.5-Pro Kimi-k2 14.0/35.7 12.0/38.0 4.0/30.6 4.0/26.6 2.0/32.9 0.0/6.3 0.0/7.2 0.0/5.4 0.0/6.3 0.0/8.1 10.0/34.2 10.0/31.6 0.0/38.2 0.0/23.7 0.0/26. 40.0/68.8 20.0/56.2 10.0/41.7 10.0/50.0 0.0/50.0 0.0/31.8 0.0/46.7 0.0/29.9 0.0/20.6 0.0/35.5 20.0/77.4 30.0/77.4 10.0/64.2 10.0/64.2 10.0/73.6 Table 5 Performance of various Models and on ACADREASON benchmark, providing with Definition Hint. Each entry shows Pass Rate Rp on the left and Checklist Score Rj on the right. Note that best results are in bold. Model Overall CS Econ Law Math Phi General Model GPT-5 GPT-oss DeepSeek-V3.1 DeepSeek-V3 Claude-4-sonnet GPT-4."
        },
        {
            "title": "Reasoning Model",
            "content": "24.0/50.9 10.0/42.3 8.0/37.2 4.0/26.1 2.0/30.6 0.0/29.9 0.0/18.9 0.0/17.1 0.0/10.8 0.0/3.6 0.0/2.7 0.0/8.1 40.0/72.4 10.0/59.2 10.0/51.3 10.0/34.2 0.0/38.2 0.0/42.1 60.0/64.6 20.0/62.5 10.0/52.1 10.0/43.8 10.0/47.9 0.0/41.7 10.0/55.1 0.0/37.4 20.0/41.1 0.0/24.3 0.0/35.5 0.0/29.9 10.0/66.0 20.0/62.3 0.0/50.9 0.0/49.1 0.0/52.8 0.0/47. o3 Qwen3 Kimi-k2 DeepSeek-R1 Gemini-2.5-Pro 10.0/48.9 10.0/40.5 10.0/36.5 6.0/35.7 4.0/38.5 10.0/35.1 0.0/16.2 0.0/20.7 0.0/4.5 0.0/8.1 0.0/61.8 20.0/57.9 10.0/44.7 20.0/63.2 10.0/63.2 20.0/56.2 10.0/60.4 20.0/54.2 0.0/43.8 0.0/41.7 0.0/42.1 0.0/33.6 0.0/27.1 0.0/33.6 0.0/43. 20.0/66.0 20.0/62.3 20.0/60.4 10.0/58.5 10.0/54.7 18 Table 6 Performance of various Models on ACADREASON benchmark, providing with Methodology Hints. Each entry shows Pass Rate Rp on the left and Checklist Score Rj on the right. Note that the best results are in bold. Model Overall CS Econ Law Math Phi"
        },
        {
            "title": "Commercial API",
            "content": "GPT-5 34.0/64.3 0.0/37.8 20.0/69.7 70.0/75.0 40.0/70. 40.0/90."
        },
        {
            "title": "General Model",
            "content": "GPT-oss Claude-4-sonnet DeepSeek-V3.1 GPT-4.1 DeepSeek-V3 Reasoning Model 16.0/52.2 14.0/40.8 12.0/45.3 8.0/42.8 4.0/38.5 0.0/27.0 0.0/13.5 0.0/19.8 0.0/18.9 0.0/14.4 0.0/60.5 0.0/34.2 20.0/47.4 0.0/43.4 0.0/32.9 30.0/54.2 50.0/62.5 20.0/62.5 30.0/56.2 10.0/58. 20.0/57.0 0.0/43.0 10.0/46.7 10.0/46.7 0.0/43.0 30.0/81.1 20.0/83.0 10.0/77.4 0.0/71.7 10.0/69.8 o3 Qwen3 Kimi-k2 Gemini-2.5-Pro DeepSeek-R1 28.0/56.2 20.0/49.1 16.0/46.8 10.0/48.6 8.0/45.3 0.0/31.5 0.0/18.9 0.0/22.5 0.0/25.2 0.0/16.2 40.0/73.7 10.0/42.1 0.0/53.9 0.0/50.0 0.0/48. 50.0/58.3 60.0/70.8 40.0/64.6 20.0/56.2 20.0/50.0 10.0/57.0 10.0/56.1 10.0/39.3 10.0/48.6 0.0/48.6 40.0/79.2 20.0/88.7 30.0/86.8 20.0/88.7 20.0/90.6 Figure 11 General performance on different domains with Pass Rate."
        },
        {
            "title": "F Specific Case of Acadreason benchmark",
            "content": "PHILOSOPHY Title: Moral Understanding Between You and Me Category: Philosophy Research Question: Why shared moral understanding is important? Golden Answer: Moral understanding is an epistemic achievement about moral matters. To have it, your conception of moral issue must be accurate (the objective dimension) and also \"make sense to you\" (the subjective dimension); your moral beliefs must be based directly in the reasons that make them true. You can know more than you understand; for example... Shared Understanding as the Aim of Interpersonal Justification,Within the central moral practices of interpersonal justification... Shared Understanding and the Norms of Apology,An apology should aim to reflect shared moral understanding of the wrong... Reasoning with the Unreasonable,A worry about the shared understanding requirement is... Checklist: Hints: 1. Does the answer include content related to sharing moral understanding? 2. Does the answer include the view that the constitutive aim of interpersonal justification is shared moral understanding? 3. Does the answer include content on shared understanding in the context of interpersonal justification? 4. Does the answer include the view that an apology should aim to reflect shared moral understanding of the wrong done to its recipient? 5. Does the answer include content on the Shared Understanding Condition of apology? 1. Background: Much attention has been paid to moral understanding as an individual achievement, when single agent gains insight into distinctly moral matters. But the importance of moral understanding cannot be fully explained by merely focusing on individuals moral understanding... 2. Definition: understanding:The capacity to grasp the moral significance of actions, principles, or situations. It involves not only knowing moral facts or rules but also appreciating the reasons behind them, recognizing the perspectives and experiences of others, and being able to make sense of moral demands in context... 3. Methodology: giving an account of what it takes for you and me to share moral understanding.2.Through comparison of the Delivery Model, the moral address view, and shared moral understanding, the constitutive aim of interpersonal justification is clarified as shared moral... Figure 12 The sample of Philosophy domain 20 MATH Title: Sorting permutations using pop stack with bypass Category: Math Research Question: How can permutations be characterized and enumerated under sorting by pop stack equipped with bypass operation? In particular, which forbidden patterns give necessary and sufficient criteria for sortability, how can bijection with suitably restricted Motzkin paths be constructed so that the counting sequence is the odd-indexed Fibonacci numbers, and how can one design and analyze an algorithm to compute preimagesespecially for permutations with few preimages and for principal classeswith structural description of these sets? Furthermore, how do these results extend to several pop stacks in parallel with bypass, yielding explicit bases for the sortable permutations, rational generating functions, and connections to classical sorting algorithms, with rigorous proofs throughout? Golden Answer: Pattern Characterization and Algorithm Optimality: Permutations sortable by the pop stack with bypass (PSB) are precisely those that avoid the patterns 231 and 4213... Enumeration via Motzkin Paths and Fibonacci Numbers: Sortable permutations can be encoded as ternary words built from the PSB operations (PUSH = 0, BYPASS = 1, POP+PUSH = 2)... Preimages under PSB: Every permutation has well-defined set of preimages under PSB. The algorithm for constructing preimages relies on decomposing permutation by its left-to-right maxima and... Preimages of Permutation Classes: For certain principal classes, preimages under PSB remain classes. If the basis permutation begins with its maximum (nα) or begins with the second maximum... Checklist: 1. Defines fundamental concepts: permutation π, pop stack operations (PUSH, POP, BYPASS), and the pattern avoidance framework. 2. Characterizes PSB-sortable permutations by avoidance of patterns 231 and 4213, showing necessity and sufficiency. 3. Establishes bijection between sortable permutations and restricted Motzkin paths, proving the enumeration equals odd-indexed Fibonacci numbers. 4. Provides an algorithm for computing preimages under PSB and analyzes its correctness. 5. ... 1. Background: 1. Sorting permutations in combinatorics Central research topic, studied through containers like stacks, queues, and pop stacks. Pattern... 2. Definition: 1. Permutation basics permutation π of size is bijection from [1, n] to [1, n], written as π = π1 πn. Identity permutation: idn = 12 n. Sets: Sn = all permutations of size n, = (cid:83) Sn. 3. Methodology: 1. Sortability characterization Goal: determine necessary and sufficient conditions for PSB sortability.... Hints: Figure 13 The sample of Math domain LAW Title: The Dilemmas of Schrödingers Citizenship Category: Law Research Question: Exploring the contradiction of whether an individual can simultaneously hold the citizenship of one or more countries and be stateless (the \"Schrödingers citizen\" dilemma), analyzes its legal roots and the impact on the international human rights system. Golden Answer: The Essence of the Theoretical Dilemma. There is fundamental split in citizenship: The declarative nature asserts that nationality is declaration of natural rights (such as the principle of bloodline), and requires that the effect of nationality be retroactive to birth... Reconstruction plans for the third-level systems. Level One: Amendment to international law. Amend Article 1A(2) of the Refugee Convention: A. An explanatory clause has been added to clarify that the determination of \"multiple nationalities\" requires meeting three conditions: ... Level Two: Reform of domestic laws. Establish special tribunal for judicial review of nationality: A. In the initial trial stage, two types of cases are distinguished: for those involving the determination of foreign nationality... ... Checklist: Hints: 1. Point out the core issue: three major problems in determining citizenship (retroactivity, foreign courts power, and mismatch of rights with operability). 2. Conflict of laws path: prohibit foreign courts from interpreting nationality laws, require nationality review courts within the sovereign state, and ensure independent review of domestic cases. 3. Human rights path: establish graded responses to statelessness, redefine refugee standards, and change potential nationality to actual administrative feasibility. 4. Institutional design: eliminate obstacles to naturalization by setting maximum processing time and creating cross-border nationality verification centers. 1. Background: 1.Theoretical Background (1The declaratory-constitutive dichotomy (Ross, Austin): Legal acts are divided into declaring natural facts (such as birth) and creating new rights (such as naturalization). (2The theory of exclusive state sovereignty over nationality (Article 1 of the Hague Convention on Nationality)... 2. Definition: 1.Schrödinger citizenship: legal status where an individual is entitled to the nationality of certain country under law, but in practice, it is not recognized by that country and is forcibly attributed by third country. 2.Declaratory citizenship... 3. Methodology: 1.Normative Analysis research method: Deconstructing the Semantic Ambiguity of the \"Multiple Nationality\" Clause in the Refugee Convention. 2.Empirical research method: Citing the naturalization rate of the United Arab Emirates in 2010... Figure 14 The sample of Law domain 22 COMPUTER SCIENCE Title: Tight Bounds and Phase Transitions for Incremental and Dynamic Retrieval Category: Computer Science Research Question: Determining the optimal redundancy for retrieval data structures in the incremental and dynamic settings when the universe size is polynomial, i.e., = poly(n). Golden Answer: For polynomial universe = poly(n), the optimal redundancy := nv for retrieval data structures is: Incremental setting (insert-only). The optimal redundancy is: (cid:16) Rinc = Θ + max{0, log( log (cid:17) )} . Equivalently: If log (for constant > 0), then Rinc = Θ(n). If = log n/ log log n, then Rinc = Θ(n log log log n). If = log0.99 n, then Rinc = Θ(n log log n). These bounds are tight: there is an incremental structure with nv + O(n) + (cid:16) log (cid:17)(cid:17) (cid:16) log and matching lower bound nv + Ω(n) + Ω (cid:16) log (cid:17)(cid:17) (cid:16) log (for n3), giving the phase transition around log n. Timewise... Checklist: 1. Setup States = poly(n). Defines redundancy := nv. 2. Incremental formula Gives Rinc = Θ(cid:0)n + max{0, log( log )}(cid:1). Mentions phase transition at log n. 3. Incremental cases log Rinc = Θ(n). = log log log Rinc = Θ(n log log log n). = log0.99 Rinc = Θ(n log log n). 4. Incremental bounds Upper bound: nv + O(n) + O(cid:0)n log( log )(cid:1), for n3. nv + Ω(n) + Ω(cid:0)n log( log )(cid:1). Lower bound: 5. ... Hints: 1. Background: Retrieval data structures are designed to answer key-value queries without explicitly storing the keys... 2. Definition: - Retrieval Data Structure: data structure that answers key-value queries without storing keys explicitly. Given set of keys and v-bit value (k) for each key K, it supports queries of the form: Query(k) = (cid:40) (k), anything, if K, otherwise. Figure 15 The sample of Computer Science domain 23 ECONOMICS Title: Informational Black Holes in Financial Markets Category: Economics Research Question: Suppose project can be either benign type or an inferior type B, with investors receiving independent private signals that satisfy the strict monotone likelihood ratio property (MLRP). The project is undertaken if and only if at least one investor participates. Please address the following: In competitive market with investors, demonstrate rigorously that robust symmetric equilibrium is characterized by unique participation threshold, sN (0, 1), where an investor participates if and only if their signal si sN . Explain the economic intuition behind the existence and uniqueness of this threshold. In the limit as , the number of participants, κ, converges to Poisson distribution. Derive the limiting distributions for κ conditional on project types and B. Furthermore, provide complete derivation for the closed-form expression of the parameter τ = lim Pr(Si sN θ = B), expressing it in terms of the signals top likelihood ratio (λ), the prior probability (π0), and the projects break-even posterior probability (π). Golden Answer: Existence and Uniqueness of the Participation Threshold (sN ): robust symmetric equilibrium is one that holds even with small, non-zero participation cost. The equilibrium is characterized by unique cutoff sN satisfying the marginal investors indifference conditioni.e., the investor with signal sN is exactly indifferent between participating or not. Asymptotic Analysis: As , the threshold sN 1. The probability of any single investor participating, Pr(Si sN ), approaches zero. The number of participants, κN , which follows binomial distribution, therefore converges to Poisson distribution under these conditions. The rate parameter τ is defined under the bad state (θ = B) as: τ = lim Pr(Si sN θ = B), and can be expressed as function of (λ, π0, π) based on the likelihood ratio and posterior thresholds. Checklist: 1. Define robust equilibrium and explain how the winners curse creates threshold participation strategy. 2. Prove existence and uniqueness of the participation threshold sN via the marginal investors breakeven condition. 3. Define parameter τ as the limit of pB, where pB is participation probability in the bad state. 4. Specify the limiting distributions of participant count κ under and as Poisson(λτ ) and Poisson(τ ). 5. ... Hints: 1. Background: This problem is rooted in the economic theory of asymmetric information, where different parties in transaction hold unequal knowledge. This leads to two key phenomena: adverse selection and information cascades. 2. Definition: Strict Monotone Likelihood Ratio Property (MLRP): The ratio of conditional densities fG(s)/fB(s) is strictly increasing in the signal s. This ensures higher signal is unambiguously good news. Robust Equilibrium: An equilibrium that persists under small perturbations or participation costs. 3. Methodology: The existence of the threshold sN is proven by analyzing the zero-profit condition for the marginal investor, who must break even when they are the sole participant (i.e., when their signal is the highest in the market). Figure 16 The sample of Economics domain"
        }
    ],
    "affiliations": [
        "OPPO AI Agent Team"
    ]
}