{
    "paper_title": "NarraScore: Bridging Visual Narrative and Musical Dynamics via Hierarchical Affective Control",
    "authors": [
        "Yufan Wen",
        "Zhaocheng Liu",
        "YeGuo Hua",
        "Ziyi Guo",
        "Lihua Zhang",
        "Chun Yuan",
        "Jian Wu"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Synthesizing coherent soundtracks for long-form videos remains a formidable challenge, currently stalled by three critical impediments: computational scalability, temporal coherence, and, most critically, a pervasive semantic blindness to evolving narrative logic. To bridge these gaps, we propose NarraScore, a hierarchical framework predicated on the core insight that emotion serves as a high-density compression of narrative logic. Uniquely, we repurpose frozen Vision-Language Models (VLMs) as continuous affective sensors, distilling high-dimensional visual streams into dense, narrative-aware Valence-Arousal trajectories. Mechanistically, NarraScore employs a Dual-Branch Injection strategy to reconcile global structure with local dynamism: a \\textit{Global Semantic Anchor} ensures stylistic stability, while a surgical \\textit{Token-Level Affective Adapter} modulates local tension via direct element-wise residual injection. This minimalist design bypasses the bottlenecks of dense attention and architectural cloning, effectively mitigating the overfitting risks associated with data scarcity. Experiments demonstrate that NarraScore achieves state-of-the-art consistency and narrative alignment with negligible computational overhead, establishing a fully autonomous paradigm for long-video soundtrack generation."
        },
        {
            "title": "Start",
            "content": "NarraScore: Bridging Visual Narrative and Musical Dynamics via Hierarchical Affective Control Zhaocheng Liu ByteDance Beijing, China lio.h.zen@gmail.com Yufan Wen Tsinghua University Shenzhen, China wenyf24@mails.tsinghua.edu.cn YeGuo Hua ByteDance Beijing, China huayeguo@bytedance.com 6 2 0 2 2 1 ] . [ 2 0 7 0 9 0 . 2 0 6 2 : r Ziyi Guo ByteDance Shenzhen, China ziyi.94@bytedance.com Lihua Zhang ByteDance Beijing, China lizhiyu.0@bytedance.com Jian Wu ByteDance Beijing, China wujian@bytedance.com Chun Yuan Tsinghua University Shenzhen, China yuanc@sz.tsinghua.edu.cn Abstract Synthesizing coherent soundtracks for long-form videos remains formidable challenge, currently stalled by three critical impediments: computational scalability, temporal coherence, and, most critically, pervasive semantic blindness to evolving narrative logic. To bridge these gaps, we propose NarraScore, hierarchical framework predicated on the core insight that emotion serves as high-density compression of narrative logic. Uniquely, we repurpose frozen Vision-Language Models (VLMs) as continuous affective sensors, distilling high-dimensional visual streams into dense, narrative-aware Valence-Arousal trajectories. Mechanistically, NarraScore employs Dual-Branch Injection strategy to reconcile global structure with local dynamism: Global Semantic Anchor ensures stylistic stability, while surgical Token-Level Affective Adapter modulates local tension via direct element-wise residual injection. This minimalist design bypasses the bottlenecks of dense attention and architectural cloning, effectively mitigating the overfitting risks associated with data scarcity. Experiments demonstrate that NarraScore achieves state-of-the-art consistency and narrative alignment with negligible computational overhead, establishing fully autonomous paradigm for long-video soundtrack generation. CCS Concepts Applied computing Sound and music computing. Keywords Video-to-Music Generation, Long-form Video Soundtrack Generation, Affective Narrative Alignment, Controllable Music Generation"
        },
        {
            "title": "1 Introduction\nBackground music serves as the emotional pulse of multimedia con-\ntent, functioning as a narrative engine that actively shapes viewer\nimmersion rather than merely accompanying visuals [7, 29, 30]. A\nprofessional-grade soundtrack is defined by its organic synchroniza-\ntion with visual progression, mirroring the underlying temporal",
            "content": "Corresponding author. dynamics and evolving emotional arcs. With the rapid advancement Figure 1: Narrative-aware video background music generation. Unlike baselines that rely on surface visuals and fail to capture narrative tension or hidden subtext, our approach leverages global plot ğ‘†ğ‘”ğ‘™ğ‘œğ‘ğ‘ğ‘™ and local emotion ğ¸ğ‘™ğ‘œğ‘ğ‘ğ‘™ to generate soundtracks that are temporally coherent and narratively resonant. of generative models[16, 27, 31, 41] enabling the transition from short clips to long-form video creation, the core challenge has fundamentally shifted, transcending the synthesis of high-fidelity static loops to necessitate the orchestration of complex, evolving soundtracks that maintain stylistic unity while dynamically responding to the shifting pacing and intensity of the visual narrative. Recent paradigms, while achieving impressive fidelity on short clips via strict frame-wise guidance [10, 14, 20, 26, 35, 49], encounter systemic bottlenecks when extrapolated to long-form narratives. Physically, maintaining dense frame-level attention across minutelong sequences incurs prohibitive quadratic memory costs and suffers from attention dilution, where critical narrative cues are drowned out by visual redundancy. Structurally, the absence of global semantic anchors in standard autoregressive models [1, 6, 18] leads to severe style drift, causing the musical identity to fragment Conference acronym XX, June 0305, 2018, Woodstock, NY Yufan Wen, Zhaocheng Liu, YeGuo Hua, Ziyi Guo, Lihua Zhang, Chun Yuan, and Jian Wu over time. Most critically, at cognitive level, these methods rely on surface-level visual representations [12, 33, 39, 40], resulting in semantic blindnessan inability to capture the deep narrative logic, such as rising tension or resolution, which is essential for cinematic storytelling [19]. To bridge this semantic gap, we propose NarraScore, hierarchical framework predicated on the core insight that emotion serves as high-density compression of deep narrative logic. Moving beyond paradigms that depend on unstable external classifiers or expensive manual annotation, we introduce Lightweight Latent Affective Decoder designed to probe the rich semantic priors encapsulated within frozen Vision-Language Models (VLMs)[4, 45, 46]. By explicitly disentangling deep narrative cues from surface-level visual redundancy, this module projects high-dimensional video streams into compact, autonomous affective manifold. Consequently, NarraScore empowers the system to endogenously deduce evolving emotional arcs directly from raw pixels, establishing new paradigm of autonomous narrative alignment. Translating this insight into generative architecture, NarraScore employs Dual-Branch Injection strategy to reconcile global coherence with local dynamism. At the macro level, Global Semantic Anchor conditions the model on the overarching genre and atmosphere, ensuring stylistic stability. At the micro level, we introduce Token-Level Affective Adapter to modulate finer narrative tension [24, 47]. Diverging from prevalent paradigms that rely on heavy architectural cloning, we adopt minimalist projection strategy: distilled continuous affective cues (Valence & Arousal) are injected as lightweight additive bias directly into the decoders hidden states. This mechanism modulates the semantic manifold via direct additive bias, enabling precise token-level alignment while introducing negligible parameter overhead and preserving the generative priors of the frozen backbone. In summary, this work makes three key contributions to the field of intelligent content generation. First, we establish Pioneering Affective-Semantic Bridge, explicitly validating the emergent capability of frozen VLMs to distill complex narrative intents into continuous emotion curves, thus bypassing the data bottleneck of traditional recognition. Second, we propose data-efficient Dynamic Control mechanism via token-level adapter. This module enables fine-grained tension modulation with minimal training overhead, achieving precise narrative alignment without disrupting the overarching musical structure. Finally, NarraScore achieves synergistic balance between global style and local emotion, paving the way for fully automated, high-quality cinematic soundtrack generation."
        },
        {
            "title": "2 Related Work\n2.1 Video to Music Generation\nThe trajectory of video soundtrack generation has evolved from\nrule-based symbolic mappings to data-driven deep generative mod-\neling. Early paradigms, represented by methods like CMT [10] and\nVideo2Music [20], pioneered the Video-to-Music task by analyzing\nvisual motion features to predict symbolic MIDI events. While foun-\ndational, these approaches often necessitated explicit user guidance\nto bridge the modality gap, resulting in limited expressive diversity\nand a heavy reliance on manual intervention.",
            "content": "The advent of audio language models marked shift toward token-based generation. Recent frameworks such as MuVi [25], VMAS [26], GVMGen [49], and VeM [38] leverage adapters to project dense video frames into latent conditions compatible with pre-trained backbones like MusicGen [6]. Although effective for short clips, these methods predominantly rely on dense frame-level attention mechanisms. This design introduces severe scalability bottlenecks: for minute-level videos, the computational cost becomes prohibitive, and the dilution of attention leads to style drift and loss of long-term coherence [37]. To address these constraints, recent works have explored specific mechanisms for long-form generation. VidMuse [37] proposes specialized adapters to effectively model both long-term and short-term temporal features, employing sliding-window inference strategy to ensure computational viability. JenBridge [44] adopts divideand-conquer approach, segmenting videos for independent scoring and stitching them via transition techniques. However, while these methods achieve acoustic continuity, they overlook the fundamental semantic shift in long-form content. Unlike short clips where static mood suffices, long narratives possess evolving logictension rises, resolves, and shifts. By treating long videos merely as extended sequences, current solutions fail to capture these dynamic arcs, yielding monotonous background ambience rather than responsive, narrative-aligned soundtracks.Consequently, enabling the soundtrack to dynamically evolve in resonance with the narrative trajectorytranscending mere acoustic continuitystands as the pivotal challenge for advancing video scoring toward professional-grade viability."
        },
        {
            "title": "2.2 Emotion-Driven Music Generation\nGiven the intrinsic link between visual storytelling and musical\nexpression, leveraging emotion as a conditioning signal has become\na focal point. Early approaches primarily relied on discrete clas-\nsification or global mapping. Video2Music [20] utilized CLIP for\nframe-level emotion classification, while EMSYNC [36] advanced\nthis by employing a psychology-driven mapping mechanism to\ntranslate discrete categorical predictions into continuous Valence\nand Arousal (VA) values. Despite these efforts, these methods rely\nheavily on surface-level visual semantic analysis, where the inher-\nent accuracy limitations of standard CLIP-based classifiers often\nlead to coarse or noisy affective guidance.",
            "content": "To improve semantic fidelity of emotional control, subsequent works have integrated more robust priors.Methods like M2UGen [28], FilmComposer [43], and JenBridge [44] leverage Large Language Models (LLMs) to analyze visual content and generate descriptive emotion captions. While these methods demonstrate proficiency in synthesizing accurate global emotion labels, they largely circumvent the use of continuous emotion curves. This avoidance stems primarily from the scarcity of continuous affective data and the prohibitive cost of fine-grained annotation. Yet, for long-form video soundtrack generation, such dense temporal control is indispensable for maintaining plot consistency. Although approaches like VeM [38] and MTCV2M [42] attempt to incorporate fine-grained control, they are fundamentally limited by their reliance on extrinsic guidance. This requirement for NarraScore: Bridging Visual Narrative and Musical Dynamics via Hierarchical Affective Control Conference acronym XX, June 0305, 2018, Woodstock, NY manual intervention renders such paradigms unscalable for autonomous, large-scale applications. Consequently, the automated and parameter-efficient extraction of deep, continuous affective cues solely from visual narratives remains critical gap in achieving robust, high-fidelity control."
        },
        {
            "title": "3 Methodology\n3.1 Problem Definition\nThe primary objective of this work is to generate a musical se-\nquence that mirrors the narrative progression of a long-form video.\nFormally, let V = {ğ‘£1, . . . , ğ‘£ğ‘‡ğ‘£ } denote the input video sequence\nconsisting of ğ‘‡ğ‘£ frames. The target output is a discrete acoustic se-\nquence A âˆˆ {1, . . . , ğ‘ }ğ‘‡ğ‘ Ã—ğ¾ , derived from a neural audio codec [8]\nwith ğ¾ residual codebooks and a vocabulary size of ğ‘ . Here, ğ‘‡ğ‘\ndenotes the sequence length. The acoustic sequence is inherently\ndense, whereas the visual sequence ğ‘‡ğ‘£ is kept sparse align with\nthe memory capacity limits when processing minute-level inputs.\nUnlike short-clip generation, this task imposes dual constraints:\nglobal coherence for unified musical style, and local alignment for\nframe-level narrative synchrony.",
            "content": "ğ‘‡ğ‘ (cid:214)"
        },
        {
            "title": "3.2 Overview\nTo effectively reconcile these requirements, we introduce NarraS-\ncore, a hierarchical framework that disentangles the video-to-music\ngeneration task into two orthogonal dimensions: macro-scale at-\nmospheric modeling and micro-scale tension tracking. Specifically,\nwe bridge the semantic gap between the visual stream V and the\nacoustic domain A by introducing two decoupled priors: a global\nsemantic anchor Sğ‘”ğ‘™ğ‘œğ‘ğ‘ğ‘™ and a frame-level affective trajectory Eğ‘™ğ‘œğ‘ğ‘ğ‘™ .\nConsequently, the generative process is formulated as an auto re-\ngressive sequence prediction conditioned on these hierarchical\ncues:",
            "content": "ğ‘ (A V) = ğ‘ (ğ‘ğ‘¡ ğ‘<ğ‘¡ , Sğ‘”ğ‘™ğ‘œğ‘ğ‘ğ‘™, Eğ‘™ğ‘œğ‘ğ‘ğ‘™ ) (1) ğ‘¡ =1 As illustrated in Figure , the proposed architecture operationalizes this formulation through cascaded Perception-Synthesis Pipeline. The workflow is anchored by Unified Visual-Narrative Backbone, instantiated as hybrid encoder integrating Vision Transformer (ViT) [11] front-end with deep contextual reasoning stack. Leveraging bifurcated projection mechanism, this unified system decomposes the raw visual stream into hierarchical control priors: it simultaneously regresses the frame-level affective trajectory Eğ‘™ğ‘œğ‘ğ‘ğ‘™ to delineate the micro-evolution of narrative tension, while abstracting the global semantic anchor Sğ‘”ğ‘™ğ‘œğ‘ğ‘ğ‘™ via language modeling head to encapsulate the macro-atmospheric context. In the subsequent generative phase, these disentangled representations are integrated into conditional acoustic transformer via distinct conditioning pathways, where the global anchor establishes the timbral and stylistic foundation, while the affective trajectory modulates the evolving narrative tension and musical dynamics."
        },
        {
            "title": "3.3 Narrative-Aware Affective Reasoning\nDeriving accurate emotion from video necessitates transcending\nstatic visual perception to capture evolving narrative dynamics.\nRather than training a task-specific temporal encoder from scratch,\nwe introduce a paradigm of Latent Semantic Probing. This approach\nfunctions as a unified spatiotemporal engine and orchestrates the\nrich reasoning priors of a frozen Vision-Language Model [45] to\nlift raw pixel streams into a continuous affective manifold.",
            "content": "Semantically-Anchored Temporal Alignment. To facilitate robust causal reasoning over long-form sequences, our framework adopts strategy of Semantically-Anchored Temporal Alignment. This formulation leverages the architectural priors of the pre-trained backbone by employing linguistically grounded progression instead of rigid temporal embeddings. We discretize the video stream into uniform 1Hz sequence to balance narrative granularity against the computational constraints of the backbone. These snapshots are interleaved with discrete semantic clocks ğœğ‘¡ , formatted textually as Time: ğ‘¡s, to strictly conform to the native interleaved schema of the model. The visual tokens ğ‘‰ğ‘¡ retain their spatial fidelity via the intrinsic positional encoding mechanism. This effectively reconstructs the video as linear causal sequence and establishes continuous spatiotemporal context for subsequent affective reasoning. Instruction-Driven Semantic Steering. We bridge the domain gap between generalist object recognition and nuanced affective analysis by repurposing the instruction-following interface of the Conference acronym XX, June 0305, 2018, Woodstock, NY Yufan Wen, Zhaocheng Liu, YeGuo Hua, Ziyi Guo, Lihua Zhang, Chun Yuan, and Jian Wu Figure 2: Overview of our framework backbone. Rather than introducing external control modules, we optimize system instruction Tğ‘–ğ‘›ğ‘ ğ‘¡ to serve as Semantic Primer. By prepending this primer to the aligned video sequence, we formulate the input representation ğ‘‹ as: ğ‘‹ = [Tğ‘–ğ‘›ğ‘ ğ‘¡ , ğœ1, ğ‘‰1, ğœ2, ğ‘‰2, . . . , ğœğ‘‡ , ğ‘‰ğ‘‡ ] (2) Functionally, ğ‘–ğ‘›ğ‘ ğ‘¡ modulates the self-attention mechanism to explicitly suppress the activation of low-level object enumeration patterns while activating high-level narrative reasoning pathways. This strategy steers the pre-trained capabilities towards analyzing narrative tension and emotional evolution to ensure that the extracted representations align with the affective task. Latent Affective Probing. To quantify the narrative tension distilled by the backbone, we introduce lightweight probing head designed to extract the continuous affective trajectory Eğ‘™ğ‘œğ‘ğ‘ğ‘™ . Let ğ‘ğ‘¡ ğ» (ğ¿) denote the set of contextualized hidden states corresponding to the visual tokens of the ğ‘¡-th frame. We first aggregate these tokens via Spatial Average Pooling to obtain holistic frame-level representation. This vector is then projected onto the Valence-Arousal plane [34] via Multi-Layer Perceptron: (cid:32) ğ‘’ğ‘¡ = Clip[1,1] MLP (cid:33)(cid:33) (cid:32) 1 ğ‘€ ğ‘§ ğ‘§ ğ‘ğ‘¡ (3) We freeze the massive backbone and train only this lightweight probe. This effectively distills the inherent ability of the VLM to correlate visual changes with explicit temporal progression into the specific task of affective regression and achieves high-fidelity prediction with minimal computational overhead. Optimization Objective. We employ hybrid objective function combining L2 and L1 norms to calibrate the probing head against the target affective curves. This formulation balances the convergence stability provided by the Mean Squared Error with the robustness against outliers offered by the Mean Absolute Error. Let Ë†ğ‘’ğ‘¡ R2 denote the ground-truth Valence-Arousal vector for the ğ‘¡-th frame. The optimization objective Lğ‘’ğ‘šğ‘œ is defined as: Lğ‘’ğ‘šğ‘œ = 1 ğ‘‡ ğ‘¡ = 1 ğ‘‡ (cid:0)ğ‘’ğ‘¡ Ë†ğ‘’ğ‘¡ 2 + ğœ†ğ‘’ğ‘¡ Ë†ğ‘’ğ‘¡ 1 (cid:1) (4) where ğœ† serves as balancing coefficient."
        },
        {
            "title": "3.4 Holistic Musical Conceptualization\nComplementing the micro-level tension tracking, this component\nfocuses on distilling the visual narrative into a global semantic\nanchor. The primary objective is to orchestrate the macro-level\nauditory identity that governs the overarching musical style and\nstructural coherence.",
            "content": "To bridge the semantic inference gap between visual features and musical concepts, we re-frame the interpretation task as cross-modal sensory translation. Instead of relying on raw visual features, we leverage the reasoning priors of the VLM to identify the visual scene while exclusively describing the implied auditory imagery. This strategy achieves modality decoupling by explicitly suppressing references to specific visual objects and cinematography. It forces the model to abstract away from scene-specific visual nouns and project the content directly into acoustic descriptors semantically aligned with the downstream audio synthesis model [6].To guarantee robustness and consistency, we employ structured instruction paradigm rather than free-form generation. We impose strict schema that compels the VLM to synthesize unified natural language description encompassing four essential musical dimensions: genre and stylistic context, instrumentation and timbral texture, emotional atmosphere, and rhythmic pacing. By projecting the visual content onto this predefined semantic subspace, we significantly reduce output variance and mitigate the ambiguity often associated with open-ended captioning. This structured constraint ensures that the generated control signals remain musically coherent and aligned with the intended aesthetic direction."
        },
        {
            "title": "3.5 Hierarchical Acoustic Synthesis\nThe synthesis framework transforms the extracted narrative priors\ninto a coherent acoustic waveform. To achieve this translation\nwithout compromising the high-fidelity generative distribution\nof the pre-trained music backbone [6], we employ a Dual-Stream\nInjection strategy. This mechanism aligns distinct control signals,",
            "content": "NarraScore: Bridging Visual Narrative and Musical Dynamics via Hierarchical Affective Control Conference acronym XX, June 0305, 2018, Woodstock, NY Figure 3: Our Method of Token-Wise Control Injection. comprising global style and local tension, with the hierarchical levels of the acoustic decoder. Explicit Semantic Bridging. The global semantic anchor Sğ‘”ğ‘™ğ‘œğ‘ğ‘ğ‘™ is integrated directly into the acoustic decoder via the pre-trained cross-attention mechanism. By utilizing the derived semantic description as the conditioning context, we steer the generative trajectory toward the target genre and atmosphere identified in the visual analysis phase. This standard conditioning approach ensures that the fundamental musical structure and instrumentation align with the narrative intent while preserving the original synthesis capabilities of the backbone without requiring invasive architectural modifications. Dense Affective Projection. structural challenge in narrative alignment lies in the significant resolution discrepancy between the sparse visual emotion cues and the dense acoustic tokens. Directly injecting these sparse signals can lead to stepped and unnatural transitions in the musical output. To resolve this, we introduce Temporal Super-resolution Adapter Fğœ™ . This module first performs linear interpolation to align the discrete affective trajectory Eğ‘™ğ‘œğ‘ğ‘ğ‘™ with the target acoustic sequence length. Subsequently, stack of dilated temporal convolutions[2] is applied to smooth local jitter and expand the receptive field. This ensures that instantaneous emotional shifts are translated into fluid musical evolution. Formally, the dense control signal ğ¶ğ‘™ğ‘œğ‘ğ‘ğ‘™ Rğ‘‡ğ‘ ğ· is derived as: ğ¶ğ‘™ğ‘œğ‘ğ‘ğ‘™ = Fğœ™ (Interp(Eğ‘™ğ‘œğ‘ğ‘ğ‘™ )) (5) where Fğœ™ acts as learnable mapping function that projects the 2-dimensional Valence-Arousal manifold into the ğ·-dimensional latent space of the acoustic decoder. Token-Level Control Injection. To incorporate the dense affective features into the pre-trained backbone, we employ residual modulation strategy [47]. Informed by observations in prior studies regarding the hierarchical distribution of information in music generation models [24], we restrict the injection of the control signal ğ¶ğ‘™ğ‘œğ‘ğ‘ğ‘™ exclusively to the shallow Transformer blocks. This design choice implies that early layers are utilized to align the generation trajectory with the continuous affective constraints, thereby allowing the deeper layers to focus on the optimization of acoustic fidelity and harmonic coherence without interference. Formally, we apply learnable additive bias to the hidden states â„ (ğ‘™ ) at time step ğ‘¡ ğ‘¡ and layer ğ‘™: â„ (ğ‘™ ) ğ‘¡ ğ‘¡ + ğ›¾ ğ¶ğ‘™ğ‘œğ‘ğ‘ğ‘™,ğ‘¡, ğ‘™ {1, . . . , ğ¿ğ‘ â„ğ‘ğ‘™ğ‘™ğ‘œğ‘¤ } = â„ (ğ‘™ ) (6) Here, ğ›¾ is learnable scalar initialized to zero to ensure the model retains its original distribution at the start of training. This selective injection scheme balances the trade-off between control precision and audio quality. Optimization Objective. To train the adapter module, we adhere to the native learning objective of the acoustic backbone. Keeping all parameters of the pre-trained autoregressive decoder frozen, we strictly optimize the adapter parameters ğœ™ and the gating scalars ğ›¾ using the standard autoregressive modeling objective. The process minimizes the Cross-Entropy loss between the predicted probability distribution and the ground-truth acoustic tokens, formulated as: Lğ‘”ğ‘’ğ‘› = 1 ğ‘‡ğ‘ ğ‘‡ğ‘ ğ‘¡ =1 log ğ‘ (ğ‘ğ‘¡ ğ‘<ğ‘¡ , Sğ‘”ğ‘™ğ‘œğ‘ğ‘ğ‘™, ğ¶ğ‘™ğ‘œğ‘ğ‘ğ‘™ ) (7) During the inference stage, the model utilizes the learned adapter to guide the backbone in generating the sequence of discrete acoustic tokens, which are subsequently reconstructed into the continuous high-fidelity waveform via the EnCodec [8] decoder."
        },
        {
            "title": "3.6 Scalable Long-Form Inference\nTo address the memory constraints inherent in processing minute-\nlevel sequences, we employ an overlapping sliding-window strat-\negy. This approach enables the synthesis of temporally consistent\nsoundtracks for long-form videos by combining global narrative\nabstraction with local context-dependent continuation.",
            "content": "Conference acronym XX, June 0305, 2018, Woodstock, NY Yufan Wen, Zhaocheng Liu, YeGuo Hua, Ziyi Guo, Lihua Zhang, Chun Yuan, and Jian Wu Global Semantic Reasoning. Prior to sequential processing, we derive the global semantic anchor Sğ‘”ğ‘™ğ‘œğ‘ğ‘ğ‘™ by leveraging the keyframe extraction capability of the VLM. This mechanism compresses the long-form visual content into sparse sequence of representative frames and allows the model to perform holistic reasoning over the entire narrative arc within limited context window. By distilling the global context into unified stylistic description, we ensure that the subsequent music generation maintains consistent thematic identity throughout the video duration. Continuous Affective Reasoning. In contrast to the global analysis, the extraction of local affective cues necessitates high temporal density. Consequently, we explicitly eschew temporal compression strategies to prevent the loss of fine-grained narrative dynamics and employ instead an overlapping sliding-window approach to extract the frame-level affective trajectory. The input video is processed in sequential windows that share defined intersection. By utilizing this temporal overlap, we ensure that the prediction for the current window is contextually aligned with the preceding frames. This continuation approach guarantees that the extracted Valence and Arousal values evolve smoothly across window boundaries and effectively yields continuous emotional curve despite the segmented processing. Autoregressive Acoustic Continuation. Similarly, the acoustic synthesis stage operates within this sliding-window framework. We maintain the global semantic anchor Sğ‘”ğ‘™ğ‘œğ‘ğ‘ğ‘™ as stationary condition to preserve the overarching stylistic foundation while the dynamic narrative progression is governed by the continuous flow of the adapter signals. To ensure musical coherence at the overlap regions, we utilize the final sequence of acoustic tokens generated in the preceding window as the prompt prefix for the current window. This technique prompts the model to logically extend the musical phrase from the previous context. It guarantees seamless rhythmic continuity and acoustic causality throughout the long-form video."
        },
        {
            "title": "4 Experiments\n4.1 Experimental Setup",
            "content": "Datasets. We utilize publicly available benchmark datasets designed for continuous affective content analysis to train the videoto-emotion and emotion-to-music modules respectively. For the video-to-emotion prediction task, we employ continuous movie dataset designed for induced emotion prediction, which provides frame-level Valence-Arousal annotations capturing the emotional responses elicited in viewers during film watching. Unlike facial expression datasets that capture actors portrayed emotions, this dataset annotates the holistic atmosphere and narrative tension perceived by audiencesprecisely the type of induced affect required for background music generation. This alignment with the viewers internal state is essential for generating background music that matches the narrative atmosphere. For the emotion-to-music generation task, we utilize music emotion dataset annotated for dynamic affective content, comprising excerpts and full songs with dense per-second valence-arousal labels. This dataset enables learning the mapping from emotional trajectories to musical characteristics, where continuous shifts in valence and arousal correspond to changes in harmony, tempo, and instrumentation. To ensure the model learns to control emotion independently of specific instrumentation, we aggregate metadata including genre and tags into textual captions. These captions are used as conditioning prompts during fine-tuning to decouple the representation of emotion from genre-specific stylistic conventions. We apply unified preprocessing pipeline to both datasets. The source separation model Demucs [9] is utilized to suppress vocal tracks and isolate the instrumental background. We segment the continuous media streams into 30-second clips with 15-second overlap to preserve temporal context. For the music generation task, we further refine the training set by discarding samples where the silence ratio exceeds 40%. Following these procedures, the effective dataset sizes are approximately 884 minutes for the video dataset and 1351 minutes for the music dataset. Ethical Considerations. This research is conducted solely for academic and scientific purposes. The proposed method and experimental results are intended to advance the understanding of video-music alignment and affective computing. This work is not integrated into any commercial products or production systems. Implementation Details. Our framework integrates the pretrained VideoLlama-3 [45] visual backbone and the MusicGenSmall [6] acoustic decoder. The architecture incorporates two trainable modules to facilitate feature alignment, namely the Projector and the Temporal Adapter.The Projector serves as semantic interface. It accepts the extracted visual features ğ¹ğ‘£ Rğ‘‡ ğ·ğ‘£ where ğ·ğ‘£ denotes the visual embedding dimension. two-layer MLP with GELU activation maps ğ¹ğ‘£ to the acoustic feature space Rğ‘‡ ğ·ğ‘ . We apply dropout rate of 0.1 during this projection to regularize the mapping process. The Temporal Adapter models long-term affective dependencies. It utilizes dilated convolution layer on the sequence dimension to expand the temporal receptive field. This operation maintains the channel dimension ğ·ğ‘ and is followed by LeakyReLU activation and linear layer to produce the final condition embeddings ğ¶ Rğ‘‡ ğ·ğ‘ . We employ two-stage training strategy to ensure stability. The Projector is first trained for 150 epochs to align the static semantic features from ğ·ğ‘£ to ğ·ğ‘. Subsequently, the Adapter is fine-tuned for 50 epochs to capture temporal dynamics. Baselines. To ensure comprehensive evaluation, we benchmark NarraScore against diverse set of five representative approaches spanning different generative paradigms. We first include M2UGEN [28] and video2music [20] as foundational multimodal frameworks that serve as established benchmarks in the field. To assess performance against the current state-of-the-art, we compare our method with VidMuse and GVMGEN, both of which represent the latest advancements in synchronized video-music synthesis. Furthermore, we construct strong two-stage pipeline baseline named Caption2Music to evaluate the efficacy of disjoint modality processing. This baseline explicitly utilizes VideoLlama3-2B to generate detailed visual captions, which subsequently prompt MusicGen for audio synthesis. Including this cascaded model allows us to rigorously validate whether our proposed hierarchical injection strategy outperforms naive combination of state-of-the-art vision and audio models. NarraScore: Bridging Visual Narrative and Musical Dynamics via Hierarchical Affective Control Conference acronym XX, June 0305, 2018, Woodstock, NY Method FAD () FD () KLD () IB() Method EDC GSM LTC MQ OP GT VidMuse [37] Video2Music [20] GVMGen [49] M2UGEN [28] 0 2.459 14.954 2.362 9.647 0 29.946 115.008 41.466 74.625 0 0.734 1.269 0.350 0.953 0.241 0.202 0.100 0.213 0.182 NarraScore 1.923 36.411 0.320 0.219 Table 1: Quantitative comparison with state-of-the-art methods on objective metrics. indicates higher is better, indicates lower is better.Bold indicates the best performance, and underlined indicates the second best."
        },
        {
            "title": "4.3 Subjective Evaluation\nRecognizing human perception as the definitive benchmark for\nartistic generation, we conducted a user study involving 10 par-\nticipants to evaluate the generated soundtracks. The assessment\nfocused on five key dimensions: Emotional Dynamic Consistency\n(EDC),Global Style Matching (GSM),Long-term Coherence (LTC),Music\nQuality (MQ),Overall Preference (OP).",
            "content": "As presented in Table 2, our method demonstrates superior performance across all metrics, establishing significant lead over baseline approaches, particularly in Emotional Consistency. This advantage substantiates the models capability to capture and articulate evolving affective content within the video. Conversely, the pipeline-based baseline (Caption2Music) exhibits marked deficiencies in visualaudio correspondence. This result suggests that exclusive reliance on textual captions creates an information bottleneck, filtering out the critical temporal and dynamic cues essential for precise audiovisual alignment. The comparative analysis detailed in Tables 2 and 3 reveals that performance disparities are intrinsically linked to the temporal modeling capabilities and conditioning mechanisms of each method. While baselines remain competitive in shortand mid-length scenarios due to limited temporal horizons, user feedback indicates significant degradation in output quality when scaling to long-form videos. In these extended contexts, baseline methods frequently fail to sustain stable musical trajectory, manifesting as narrative drift, inconsistent motifs, or disjointed segmentation. In contrast, our VidMuse [37] Video2Music [20] GVMGen [49] Caption2Music Ours 2.29 1.48 1.65 2.66 2.86 2.49 1.56 1.69 2.77 3.02 2.49 2.41 1.41 2.88 3. 2.13 3.39 1.64 3.18 3.41 2.18 1.88 1.34 2.82 3.06 Table 2: Subjective Evaluation of Video-to-Music Generation on Long-form Videos.Bold indicates the best performance, and underlined indicates the second best. Method EDC GSM LTC MQ OVR VidMuse [37] Video2Music [20] GVMGen [49] Caption2Music Ours 3.09 1.66 2.68 2.83 3.11 3.27 1.68 2.75 3.01 3.36 3.03 2.38 2.80 3.04 3.24 3.00 3.50 2.87 3.21 3.52 3.02 1.84 2.64 2.96 3.32 Table 3: Subjective Evaluation of Video-to-Music Generation on Shortand Mid-length Videos.Bold indicates the best performance, and underlined indicates the second best. approach achieves the highest overall preference in both settings, with widening margin in the long-form evaluation. This validates the effectiveness of our design in handling extended temporal dependencies, preserving consistent global musical identity while adaptively responding to local visual cues. Further analysis elucidates the specific limitations inherent in competing paradigms. The pipeline baseline, Caption2Music, highlights the limitations of text-only control, where coarse captions may capture the general mood but lack the temporal granularity required for fine-grained emotional transitions. Similarly, MIDI-based baselines such as Video2Music expose dichotomy between acoustic fidelity and narrative alignment; despite receiving high ratings for audio texture, these tracks are frequently characterized by users as generic and weakly coupled to the visual storyline. Consequently, while short-term settings naturally narrow the performance gap by reducing the burden of long-range consistency, the sustained top ranking of our method across all metrics underscores that structural and affective coherence remain the defining factors in successful narrative music generation."
        },
        {
            "title": "4.4 Ablation Study\nTo investigate the contribution of individual components and assess\nthe frameworkâ€™s adaptability across different reasoning backbones,\nwe conduct a comprehensive ablation study, as summarized in Ta-\nble 4. The experimental results reveal distinct behaviors of the\nHolistic Musical Conceptualization (HMC) module across different\nscales of vision-language backbones. When employing lightweight\nVLMs as the captioner, the absence of HMC leads to noticeable per-\nformance degradation, as the model tends to produce literal descrip-\ntions of visual scenes without establishing meaningful connections\nto musical elements. However, an interesting phenomenon emerges\nwhen switching to powerful large-scale VLMs: the HMC module\nappears to constrain rather than enhance the modelâ€™s capabilities.",
            "content": "Conference acronym XX, June 0305, 2018, Woodstock, NY Yufan Wen, Zhaocheng Liu, YeGuo Hua, Ziyi Guo, Lihua Zhang, Chun Yuan, and Jian Wu Figure 4: Visualization of the generated spectrograms and the corresponding narrative emotion curves. This suggests that advanced VLMs possess inherent cross-modal understanding abilities that allow them to directly infer appropriate musical characteristics from visual content without explicit highlevel guidance. Regarding the adaptive in-attention mechanism, the results demonstrate that the 75% injection ratio employed in NarraScore represents an optimal balance. Both reducing and increasing this proportion adversely affect performance, indicating that excessive narrative guidance may overwhelm the acoustic modeling capacity of the diffusion backbone while insufficient injection fails to provide adequate semantic alignment. Furthermore, the consistent improvements brought by the Narrative-Aware Affective Reasoning (NAR) mechanism across all backbone configurations validate its effectiveness as model-agnostic component. Even when equipped with state-of-the-art VLMs, the incorporation of NAR continues to enhance music quality by bridging the affective gap between visual narratives and auditory expressions, underscoring the importance of explicit emotional reasoning in video-to-music generation tasks."
        },
        {
            "title": "4.5 Qualitative Analysis\nTo qualitatively evaluate the acoustic characteristics and narrative\nalignment of the generated music, we perform a detailed compara-\ntive analysis of Mel-spectrograms produced by various models un-\nder identical video guidance. As illustrated in Figure 4, the spectral\narchitectures reveal fundamental differences in how each frame-\nwork internalizes temporal dynamics and semantic logic. Specif-\nically, the Mel-spectrum of caption2music is dominated by exces-\nsively smooth horizontal energy bands with a notable absence of\nvertical transients. This spectral stagnation indicates a lack of rhyth-\nmic pulses and dynamic fluctuations, which results in audio that\nfails to reflect the temporal energy shifts inherent in the video. In\ncontrast, gvmgen exhibits significant discontinuities in its spectral\nmanifold characterized by fragmented energy distributions and\nabrupt temporal breaks. Such erratic spectral patterns suggest a\nfailure in maintaining long-term acoustic coherence and a mis-\nalignment with the narrative progression of the visual input. The\nresults from v2m further highlight these challenges because its\nspectrum appears overly monotonic and mechanical. The absence\nof structured harmonic variations and distinct beat markers implies\na deficiency in narrative expression where the model produces a\nflat auditory output that remains unresponsive to visual climaxes.\nFurthermore, while vidmuse generates content with high spectral",
            "content": "density, it is plagued by high concentration of stochastic noise in the high-frequency regions above 8000Hz. This is visualized as disordered pixel-like artifacts, which suggests that vidmuse is confined to literal object-level associations instead of synthesizing cohesive and narratively-driven soundtrack. In contrast, our model demonstrates superior spectral hierarchy that balances harmonic stability with rhythmic precision. The presence of clear fundamental frequencies alongside vertical onset markers indicates that our method successfully synchronizes discrete rhythmic events with the narrative arc. By capturing both the instantaneous motion cues and the global semantic flow, our model generates music that is not only acoustically clear but also contextually and emotionally resonant with the video content. Setting FAD () FD () KLD () IB () Component Analysis NarraScore w/o HMC w/o NAR In-attention Analysis 50% Blocks 100% Blocks Backbone Analysis Gemini2.5pro [5] Gemini (w/o HMC) Gemini (w/o NAR) Gemini (cap-only) 1.923 2.235 3. 36.411 36.069 41.146 0.320 0.388 0.545 0.219 0.203 0.202 2.021 1.964 37.282 36.503 0.353 0. 0.196 0.200 2.322 1.906 2.430 2.002 33.752 31.299 32.568 32.239 0.376 0.320 0.403 0.324 0.226 0.223 0.214 0.203 Table 4: Ablation study on key components and different LLM backbones. NAR denotes Narrative-Aware Affective Reasoning.Bold indicates the best performance, and underlined indicates the second best."
        },
        {
            "title": "5 Conclusion\nWe present NarraScore, pioneering the first direct emotional control\npathway from video narratives to musical dynamics. Our findings\ndemonstrate that fine-tuning small mLLMs with limited labeled",
            "content": "NarraScore: Bridging Visual Narrative and Musical Dynamics via Hierarchical Affective Control Conference acronym XX, June 0305, 2018, Woodstock, NY data enables robust continuous temporal regression. Simultaneously, we confirm that lightweight adapter is sufficient to steer the complex acoustic backbone. By reconciling global style with local tension, NarraScore achieves state-of-the-art coherence, establishing strong baseline for autonomous soundtrack generation."
        },
        {
            "title": "6 Limitations\nCurrent limitations stem from the limited temporal granularity of\nthe affective control, which precludes frame-perfect synchroniza-\ntion with rapid visual events. Additionally, the cascaded design\nrisks error propagation from upstream affective reasoning. Future\nwork will focus on end-to-end joint optimization to mitigate these\ndependencies and explore knowledge distillation to reduce the com-\nputational latency of the visual backbone.",
            "content": "References [1] Andrea Agostinelli, Timo Denk, ZalÃ¡n Borsos, Jesse Engel, Mauro Verzetti, Antoine Caillon, Qingqing Huang, Aren Jansen, Adam Roberts, Marco Tagliasacchi, et al. 2023. Musiclm: Generating music from text. arXiv preprint arXiv:2301.11325 (2023). [2] Shaojie Bai, J. Zico Kolter, and Vladlen Koltun. 2018. An Empirical Evaluation of Generic Convolutional and Recurrent Networks for Sequence Modeling. arXiv:1803.01271 [cs.LG] https://arxiv.org/abs/1803.01271 [3] Yoann Baveye, Emmanuel Dellandrea, Christel Chamaret, and Liming Chen. 2015. LIRIS-ACCEDE: video database for affective content analysis. IEEE Transactions on Affective Computing 6, 1 (2015), 4355. [4] Zesen Cheng, Sicong Leng, Hang Zhang, Yifei Xin, Xin Li, Guanzheng Chen, Yongxin Zhu, Wenqi Zhang, Ziyang Luo, Deli Zhao, et al. 2024. Videollama 2: Advancing spatial-temporal modeling and audio understanding in video-llms. arXiv preprint arXiv:2406.07476 (2024). [5] Gheorghe Comanici, Eric Bieber, Mike Schaekermann, Ice Pasupat, Noveen Sachdeva, Inderjit Dhillon, Marcel Blistein, Ori Ram, Dan Zhang, Evan Rosen, et al. 2025. Gemini 2.5: Pushing the frontier with advanced reasoning, multimodality, long context, and next generation agentic capabilities. arXiv preprint arXiv:2507.06261 (2025). [6] Jade Copet, Felix Kreuk, Itai Gat, Tal Remez, David Kant, Gabriel Synnaeve, Yossi Adi, and Alexandre DÃ©fossez. 2023. Simple and controllable music generation. Advances in Neural Information Processing Systems 36 (2023), 4770447720. [7] Johanna Dasovich-Wilson, Marc Thompson, and Suvi Saarikallio. 2022. Exploring music video experiences and their influence on music perception. Music & Science 5 (2022), 20592043221117651. [8] Alexandre DÃ©fossez, Jade Copet, Gabriel Synnaeve, and Yossi Adi. 2022. High fidelity neural audio compression. arXiv preprint arXiv:2210.13438 (2022). [9] Alexandre DÃ©fossez, Nicolas Usunier, LÃ©on Bottou, and Francis Bach. 2019. Music source separation in the waveform domain. arXiv preprint arXiv:1911.13254 (2019). [10] Shangzhe Di, Zeren Jiang, Si Liu, Zhaokai Wang, Leyan Zhu, Zexin He, Hongming Liu, and Shuicheng Yan. 2021. Video background music generation with controllable music transformer. In Proceedings of the 29th ACM International Conference on Multimedia. 20372045. [11] Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, Jakob Uszkoreit, and Neil Houlsby. 2021. An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale. arXiv:2010.11929 [cs.CV] https://arxiv.org/abs/2010.11929 [12] Christoph Feichtenhofer, Haoqi Fan, Jitendra Malik, and Kaiming He. 2019. Slowfast networks for video recognition. In Proceedings of the IEEE/CVF international conference on computer vision. 62026211. [13] Niki Maria Foteinopoulou and Ioannis Patras. 2024. Emoclip: vision-language method for zero-shot video facial expression recognition. In 2024 IEEE 18th International Conference on Automatic Face and Gesture Recognition (FG). IEEE, 110. [14] Chuang Gan, Deng Huang, Peihao Chen, Joshua Tenenbaum, and Antonio Torralba. 2020. Foley music: Learning to generate music from videos. In European Conference on Computer Vision. Springer, 758775. [15] Rohit Girdhar, Alaaeldin El-Nouby, Zhuang Liu, Mannat Singh, Kalyan Vasudev Alwala, Armand Joulin, and Ishan Misra. 2023. ImageBind: One Embedding Space To Bind Them All. arXiv:2305.05665 [cs.CV] https://arxiv.org/abs/2305.05665 [16] Google DeepMind. 2025. Veo 3 Tech Report. https://storage.googleapis.com/ deepmind-media/veo/Veo-3-Tech-Report.pdf. [17] Martin Heusel, Hubert Ramsauer, Thomas Unterthiner, Bernhard Nessler, and Sepp Hochreiter. 2017. Gans trained by two time-scale update rule converge to local nash equilibrium. Advances in neural information processing systems 30 (2017). [18] Cheng-Zhi Anna Huang, Ashish Vaswani, Jakob Uszkoreit, Noam Shazeer, Ian Simon, Curtis Hawthorne, Andrew Dai, Matthew Hoffman, Monica Dinculescu, and Douglas Eck. 2018. Music transformer. arXiv preprint arXiv:1809.04281 (2018). [19] Shulei Ji, Songruoyao Wu, Zihao Wang, Shuyu Li, and Kejun Zhang. 2025. Comprehensive Survey on Generative AI for Video-to-Music Generation. arXiv preprint arXiv:2502.12489 (2025). [20] Jaeyong Kang, Soujanya Poria, and Dorien Herremans. 2024. Video2music: Suitable music generation from videos using an affective multimodal transformer model. Expert Systems with Applications 249 (2024), 123640. [21] Kevin Kilgour, Mauricio Zuluaga, Dominik Roblek, and Matthew Sharifi. 2019. FrÃ©chet Audio Distance: Metric for Evaluating Music Enhancement Algorithms. arXiv:1812.08466 [eess.AS] https://arxiv.org/abs/1812.08466 [22] Dimitrios Kollias and Stefanos Zafeiriou. 2018. Aff-wild2: Extending the aff-wild database for affect recognition. arXiv preprint arXiv:1811.07770 (2018). [23] Jean Kossaifi, Georgios Tzimiropoulos, Sinisa Todorovic, and Maja Pantic. 2017. AFEW-VA database for valence and arousal estimation in-the-wild. Image and Vision Computing 65 (2017), 2336. Conference acronym XX, June 0305, 2018, Woodstock, NY Yufan Wen, Zhaocheng Liu, YeGuo Hua, Ziyi Guo, Lihua Zhang, Chun Yuan, and Jian Wu [48] Sitao Zhang, Yimu Pan, and James Wang. 2023. Learning emotion representations from verbal and nonverbal communication. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. 1899319004. [49] Heda Zuo, Weitao You, Junxian Wu, Shihong Ren, Pei Chen, Mingxu Zhou, Yujia Lu, and Lingyun Sun. 2025. Gvmgen: general video-to-music generation model with hierarchical attentions. In Proceedings of the AAAI Conference on Artificial Intelligence, Vol. 39. 2309923107. [24] Yun-Han Lan, Wen-Yi Hsiao, Hao-Chung Cheng, and Yi-Hsuan Yang. 2024. Musicongen: Rhythm and chord control for transformer-based text-to-music generation. arXiv preprint arXiv:2407.15060 (2024). [25] Ruiqi Li, Siqi Zheng, Xize Cheng, Ziang Zhang, Shengpeng Ji, and Zhou Zhao. 2024. Muvi: Video-to-music generation with semantic alignment and rhythmic synchronization. arXiv preprint arXiv:2410.12957 (2024). [26] Yan-Bo Lin, Yu Tian, Linjie Yang, Gedas Bertasius, and Heng Wang. 2025. Vmas: Video-to-music generation via semantic alignment in web music videos. In 2025 IEEE/CVF Winter Conference on Applications of Computer Vision (WACV). IEEE, 11551165. [27] Jie Liu, Gongye Liu, Jiajun Liang, Ziyang Yuan, Xiaokun Liu, Mingwu Zheng, Xiele Wu, Qiulin Wang, Menghan Xia, Xintao Wang, et al. 2025. Improving video generation with human feedback. arXiv preprint arXiv:2501.13918 (2025). [28] Shansong Liu, Atin Sakkeer Hussain, Qilong Wu, Chenshuo Sun, and Ying Shan. 2023. M2UGen: Multi-modal Music Understanding and Generation with the Power of Large Language Models. arXiv preprint arXiv:2311.11255 (2023). [29] Lin Ma. 2022. Research on the effect of different types of short music videos on viewers psychological emotions. Frontiers in public health 10 (2022), 992200. [30] Barbara Millet, Juan Chattah, and Soyeon Ahn. 2021. Soundtrack design: The impact of music on visual attention and affective responses. Applied ergonomics 93 (2021), 103301. [31] OpenAI. 2024. Video generation models as world simulators. https://openai.com/ index/video-generation-models-as-world-simulators/. [32] Zhihang Ren, Jefferson Ortega, Yifan Wang, Zhimin Chen, Yunhui Guo, Stella Yu, and David Whitney. 2024. Veatic: Video-based emotion and affect tracking in context dataset. In Proceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision. 44674477. [33] Andrew Rouditchenko, Angie Boggust, David Harwath, Brian Chen, Dhiraj Joshi, Samuel Thomas, Kartik Audhkhasi, Hilde Kuehne, Rameswar Panda, Rogerio Feris, et al. 2020. Avlnet: Learning audio-visual language representations from instructional videos. arXiv preprint arXiv:2006.09199 (2020). [34] James Russell. 1980. circumplex model of affect. Journal of personality and social psychology 39, 6 (1980), 1161. [35] Kun Su, Judith Yue Li, Qingqing Huang, Dima Kuzmin, Joonseok Lee, Chris Donahue, Fei Sha, Aren Jansen, Yu Wang, Mauro Verzetti, et al. 2024. V2meow: Meowing to the visual beat via video-to-music generation. In Proceedings of the AAAI Conference on Artificial Intelligence, Vol. 38. 49524960. [36] Serkan Sulun, Paula Viana, and Matthew EP Davies. 2025. Video Soundtrack Generation by Aligning Emotions and Temporal Boundaries. arXiv preprint arXiv:2502.10154 (2025). [37] Zeyue Tian, Zhaoyang Liu, Ruibin Yuan, Jiahao Pan, Qifeng Liu, Xu Tan, Qifeng Chen, Wei Xue, and Yike Guo. 2025. Vidmuse: simple video-to-music generation framework with long-short-term modeling. In Proceedings of the Computer Vision and Pattern Recognition Conference. 1878218793. [38] Xinyi Tong, Yiran Zhu, Jishang Chen, Chunru Zhan, Tianle Wang, Sirui Zhang, Nian Liu, Tiezheng Ge, Duo Xu, Xin Jin, et al. 2025. Video Echoed in Music: Semantic, Temporal, and Rhythmic Alignment for Video-to-Music Generation. arXiv preprint arXiv:2511.09585 (2025). [39] Zhan Tong, Yibing Song, Jue Wang, and Limin Wang. 2022. Videomae: Masked autoencoders are data-efficient learners for self-supervised video pre-training. Advances in neural information processing systems 35 (2022), 1007810093. [40] Limin Wang, Bingkun Huang, Zhiyu Zhao, Zhan Tong, Yinan He, Yi Wang, Yali Wang, and Yu Qiao. 2023. Videomae v2: Scaling video masked autoencoders with dual masking. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition. 1454914560. [41] ThaddÃ¤us Wiedemer, Yuxuan Li, Paul Vicol, Shixiang Shane Gu, Nick Matarese, Kevin Swersky, Been Kim, Priyank Jaini, and Robert Geirhos. 2025. Video models are zero-shot learners and reasoners. arXiv preprint arXiv:2509.20328 (2025). [42] Junxian Wu, Weitao You, Heda Zuo, Dengming Zhang, Pei Chen, and Lingyun Sun. 2025. Controllable video-to-music generation with multiple time-varying conditions. In Proceedings of the 33rd ACM International Conference on Multimedia. 1042710436. [43] Zhifeng Xie, Qile He, Youjia Zhu, Qiwei He, and Mengtian Li. 2025. FilmComposer: LLM-Driven Music Production for Silent Film Clips. In Proceedings of the Computer Vision and Pattern Recognition Conference. 1351913528. [44] Jiashuo Yu, Yao Yao, Boyu Chen, and Alex Wang. [n. d.]. JenBridge: Adaptive Long-Form Video Soundtracking across Scene Transition. ([n. d.]). [45] Boqiang Zhang, Kehan Li, Zesen Cheng, Zhiqiang Hu, Yuqian Yuan, Guanzheng Chen, Sicong Leng, Yuming Jiang, Hang Zhang, Xin Li, et al. 2025. Videollama 3: Frontier multimodal foundation models for image and video understanding. arXiv preprint arXiv:2501.13106 (2025). [46] Hang Zhang, Xin Li, and Lidong Bing. 2023. Video-llama: An instructiontuned audio-visual language model for video understanding. arXiv preprint arXiv:2306.02858 (2023). [47] Lvmin Zhang, Anyi Rao, and Maneesh Agrawala. 2023. Adding conditional control to text-to-image diffusion models. In Proceedings of the IEEE/CVF international conference on computer vision. 38363847."
        }
    ],
    "affiliations": [
        "ByteDance Beijing, China",
        "ByteDance Shenzhen, China",
        "Tsinghua University Shenzhen, China"
    ]
}