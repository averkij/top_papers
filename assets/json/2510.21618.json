{
    "paper_title": "DeepAgent: A General Reasoning Agent with Scalable Toolsets",
    "authors": [
        "Xiaoxi Li",
        "Wenxiang Jiao",
        "Jiarui Jin",
        "Guanting Dong",
        "Jiajie Jin",
        "Yinuo Wang",
        "Hao Wang",
        "Yutao Zhu",
        "Ji-Rong Wen",
        "Yuan Lu",
        "Zhicheng Dou"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Large reasoning models have demonstrated strong problem-solving abilities, yet real-world tasks often require external tools and long-horizon interactions. Existing agent frameworks typically follow predefined workflows, which limit autonomous and global task completion. In this paper, we introduce DeepAgent, an end-to-end deep reasoning agent that performs autonomous thinking, tool discovery, and action execution within a single, coherent reasoning process. To address the challenges of long-horizon interactions, particularly the context length explosion from multiple tool calls and the accumulation of interaction history, we introduce an autonomous memory folding mechanism that compresses past interactions into structured episodic, working, and tool memories, reducing error accumulation while preserving critical information. To teach general-purpose tool use efficiently and stably, we develop an end-to-end reinforcement learning strategy, namely ToolPO, that leverages LLM-simulated APIs and applies tool-call advantage attribution to assign fine-grained credit to the tool invocation tokens. Extensive experiments on eight benchmarks, including general tool-use tasks (ToolBench, API-Bank, TMDB, Spotify, ToolHop) and downstream applications (ALFWorld, WebShop, GAIA, HLE), demonstrate that DeepAgent consistently outperforms baselines across both labeled-tool and open-set tool retrieval scenarios. This work takes a step toward more general and capable agents for real-world applications. The code and demo are available at https://github.com/RUC-NLPIR/DeepAgent."
        },
        {
            "title": "Start",
            "content": "DeepAgent: General Reasoning Agent with Scalable Toolsets Xiaoxi Li1,2, Wenxiang Jiao2, Jiarui Jin2, Guanting Dong1, Jiajie Jin1, Yinuo Wang2, Hao Wang2, Yutao Zhu1, Ji-Rong Wen1, Yuan Lu2, Zhicheng Dou1 1Renmin University of China 2Xiaohongshu Inc. {xiaoxi_li, dou}@ruc.edu.cn, luyuan3@xiaohongshu.com 5 2 0 2 4 2 ] . [ 1 8 1 6 1 2 . 0 1 5 2 : r Abstract Large reasoning models have demonstrated strong problem-solving abilities, yet real-world tasks often require external tools and longhorizon interactions. Existing agent frameworks typically follow predefined workflows, which limit autonomous and global task completion. In this paper, we introduce DeepAgent, an end-to-end deep reasoning agent that performs autonomous thinking, tool discovery, and action execution within single, coherent reasoning process. To address the challenges of long-horizon interactions, particularly the context length explosion from multiple tool calls and the accumulation of interaction history, we introduce an autonomous memory folding mechanism that compresses past interactions into structured episodic, working, and tool memories, reducing error accumulation while preserving critical information. To teach general-purpose tool use efficiently and stably, we develop an end-to-end reinforcement learning strategy, namely ToolPO, that leverages LLM-simulated APIs and applies tool-call advantage attribution to assign fine-grained credit to the tool invocation tokens. Extensive experiments on eight benchmarks, including general tool-use tasks (ToolBench, API-Bank, TMDB, Spotify, ToolHop) and downstream applications (ALFWorld, WebShop, GAIA, HLE), demonstrate that DeepAgent consistently outperforms baselines across both labeled-tool and open-set tool retrieval scenarios. This work takes step toward more general and capable agents for real-world applications. The code and demo are available at https://github.com/RUC-NLPIR/DeepAgent. Keywords Large Reasoning Models, Autonomous Agents, Tool Retrieval, Memory Mechanism, Reinforcement Learning"
        },
        {
            "title": "1 Introduction\nThe rapid advancement of large language models (LLMs) has in-\nspired the development of LLM-powered agents, which have found\nbroad applications in scenarios such as web information seeking,\nsoftware engineering, and personal assistance [19, 39, 53]. Existing\nagent frameworks predominantly rely on predefined workflows,\nexemplified by methods like ReAct [67] and Plan-and-Solve [54],",
            "content": "Work done during internship at Xiaohongshu. Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. Copyrights for components of this work owned by others than the author(s) must be honored. Abstracting with credit is permitted. To copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior specific permission and/or fee. Request permissions from permissions@acm.org. Conference17, Washington, DC, USA 2026 Copyright held by the owner/author(s). Publication rights licensed to ACM. ACM ISBN 978-x-xxxx-xxxx-x/YYYY/MM https://doi.org/xxxxxxx.xxxxxxx Figure 1: Overall performance on (a) general tool usage tasks and (b) downstream applications (best score as 100%). which employ structured planning processes and iterative ReasonAct-Observe cycles as illustrated in Figure 2(a). Although effective in simpler tasks, these approaches suffer from several critical limitations: (1) lack of autonomy in execution steps and overall procedure; (2) inability to dynamically discover tools during task execution; (3) deficiency in fully autonomous management of interactive memory; and (4) insufficient depth and coherence in reasoning about the entire task. These fundamental shortcomings severely constrain the agents ability to tackle real-world problems, particularly for complex tasks that demand general tool-use and long-horizon interaction with the environment. Recently, the advent of large reasoning models (LRMs) has demonstrated the capability to solve complex problems in domains like mathematics, programming, and scientific reasoning through stepby-step slow thinking process [2, 28]. However, many real-world tasks necessitate the use of external tools for their completion. While some studies have explored new paradigms for integrating tool use within the reasoning process, such as Search-o1 [25], DeepResearcher [74], and ToRL [27], these approaches are often restricted to limited set of predefined tools, such as web search, page browsing, and coding (Figure 2(b)). This constrained set of tools significantly hinders their applicability to wide range of complex, real-world scenarios. To address these challenges, we introduce DeepAgent, an endto-end deep reasoning agent that can complete an entire task by dynamically retrieving and calling tools within single, coherent agentic reasoning process. As depicted in Figure 2(c), DeepAgent operates by autonomously thinking, searching for tools, and executing actions. This paradigm shifts away from traditional, predefined workflows that rely on predefined tools, task planning, and iterative tool use, where each generation step focuses only on the immediate objective. Instead, DeepAgent maintains global perspective on Conference17, July 2017, Washington, DC, USA Xiaoxi Li et al. Figure 2: Comparison of agent paradigms: (a) Traditional agents with predefined workflows, (b) Deep Research agents that can autonomously call limited tools, and (c) Our DeepAgent, fully autonomous reasoning agent that dynamically discovers and invokes helpful tools, all within continuous agentic reasoning process. the entire task, unconstrained by the need to deliberate on specific, isolated operations. Tools are not pre-retrieved in advance but are dynamically discovered on an as-needed basis, thereby fully unlocking the autonomous potential of the large reasoning model. To empower DeepAgent to thoroughly and robustly explore new tools and navigate complex environments during long-horizon interactions, we equip it with memory management capabilities. We introduce an Autonomous Memory Folding strategy that allows DeepAgent to consolidate its previous thoughts and interaction history into structured memory schema at any point during its thinking before resuming the agentic reasoning process. This mechanism not only saves tokens and enhances reasoning efficiency over extended interactions but also provides the agent an opportunity to take breath, preventing it from becoming trapped in wrong exploration paths and enabling it to reconsider its strategy, thus improving the overall success rate. To mitigate information loss during this folding process, we introduce brain-inspired memory architecture comprising episodic memory, working memory, and tool memory, all structured with an agent-usable data schema to ensure the stability and utility of the compressed memory. To enhance DeepAgents proficiency in mastering these mechanisms, we propose ToolPO, an end-to-end reinforcement learning (RL) training method tailored for general tool use. Existing agentic RL training in general domains presents two significant challenges: (1) The reliance on multitude of real-world APIs during training can lead to instability, slow execution, and high costs. To prevent this, we leverage LLM-simulated APIs, which enhance the stability and efficiency of the training process. (2) sparse reward based solely on the final outcome is often insufficient to guarantee the accuracy of intermediate tool calls. We address this by implementing tool-call advantage attribution, which precisely assigns credit to the specific tokens responsible for correct tool invocations, thereby providing more granular and effective learning signal. We conduct extensive experiments on wide range of benchmarks. For (1) General Tool-Use Tasks, we evaluate DeepAgent on ToolBench, API-Bank, TMDB, Spotify, and ToolHop, which feature toolsets scaling from tens to over ten thousand distinct tools. For (2) Downstream Applications, we test its performance on ALFWorld, WebShop, GAIA, and Humanitys Last Exam (HLE), which require the use of domain-specific toolsets. The overall results in Figure 1 show that DeepAgent achieves superior performance across all scenarios. Our main contributions are summarized as follows: (1) We propose DeepAgent, the first agentic framework that enables reasoning models to autonomously think, discover tools, and execute actions within unified reasoning process, empowering LRMs to harness toolsets of arbitrary scale and generalize to complex real-world tasks. (2) We introduce an autonomous memory folding mechanism, complemented by brain-inspired memory design. This endows the agent with the ability to take breath and reconsider its exploration strategies following unsuccessful attempts. (3) We propose an end-to-end reinforcement learning training methodology for general-purpose tool use, ensuring stability and efficiency in large-scale tool execution during training, as well as accuracy in tool invocation during reasoning. (4) We conduct extensive experiments across eight benchmarks, demonstrating DeepAgents superior tool-use capabilities and high adaptability to real-world tasks."
        },
        {
            "title": "2 Related Work\n2.1 Large Reasoning Models\nLarge Reasoning Models (LRMs) [5, 16] have demonstrated signifi-\ncant performance improvements in mathematical, scientific, and\ncoding tasks by employing step-by-step slow thinking processes\nbefore generating final responses. Existing research has explored\nvarious approaches to elicit extended Chain-of-Thought (CoT) rea-\nsoning [58] from models, including data synthesis for Supervised\nFine-Tuning (SFT) [33, 36, 69], and end-to-end RL [5, 14]. Addition-\nally, substantial work has investigated optimization strategies for\nreasoning models, such as advanced RL training algorithms [70, 73]\nand improving reasoning efficiency [3, 65]. However, models re-\nlying solely on parametric knowledge face inherent limitations\nand cannot interact with the real world. Recent studies have be-\ngun exploring tool-augmented reasoning approaches, including\nSearch-o1 [25], Search-R1 [18], ToRL [27], DeepResearcher [74],\nand SimpleTIR [63]. However, these methods typically support only",
            "content": "DeepAgent: General Reasoning Agent with Scalable Toolsets Conference17, July 2017, Washington, DC, USA Figure 3: Overview of the DeepAgent framework. The main reasoning model autonomously discovers tools, executes actions, and folds previous memory to restart with structured memories, all within unified thinking process. The DeepAgent is trained end-to-end with ToolPO, an RL method that uses tool simulator to simulate large-scale real-world tool APIs, and rewards both final task success and correct intermediate tool calls through fine-grained advantage attribution. limited set of research-oriented tools, such as web search, page browsing, and code execution, which constrains their applicability to real-world scenarios that demand access to more diverse tools. of DeepAgent, including the mechanism for autonomous tool use and memory folding, the brain-inspired memory schema, and our end-to-end reinforcement learning training method, ToolPO."
        },
        {
            "title": "3.1 Problem Formulation\nWe frame the agent’s task as a sequential decision-making process.\nThe agent receives a user-provided question 𝑄 and an instruction 𝐼 ,\nand interacts with an environment over a series of steps 𝑡 = 1, . . . ,𝑇\nto accomplish the specified goal. The environment provides access\nto a collection of tools T at an arbitrary scale.",
            "content": "At each step 𝑡, the agents state 𝑠𝑡 consists of the history of all previous actions and their resulting observations, i.e., 𝑠𝑡 = (𝑎1, 𝑜1, . . . , 𝑎𝑡 1, 𝑜𝑡 1). The agent, driven by policy 𝜋 parameterized by 𝜃 , selects an action 𝑎𝑡 based on the current state, the user question, and the instruction: 𝑎𝑡 𝜋𝜃 (𝑠𝑡 , 𝑄, 𝐼 ). (1) An action 𝑎𝑡 can be one of four types: Internal Thought (𝑎think ): textual reasoning step generated by the LRM to analyze the problem or plan its next steps. The corresponding observation 𝑜𝑡 is typically empty. 𝑡 𝑡 𝑡 Tool Search (𝑎search ): natural language query 𝑞𝑠 to find relevant tools from the toolset . The observation 𝑜𝑡 is list of retrieved tools. Tool Call (𝑎call ): The invocation of specific tool 𝜏 with set of arguments. The observation 𝑜𝑡 is the execution result returned by the tool. Memory Fold (𝑎fold ): special action to compress the interaction history 𝑠𝑡 into structured memory summary. The subsequent state 𝑠𝑡 +1 is then initialized with this compressed memory. The sequence of states, actions, and observations forms trajectory 𝜏 = (𝑠1, 𝑎1, 𝑜1, . . . , 𝑠𝑇 , 𝑎𝑇 , 𝑜𝑇 ). The process terminates when the 𝑡 Conference17, July 2017, Washington, DC, USA Xiaoxi Li et al. agent completes the task or reaches maximum step limit. The objective is to learn an optimal policy 𝜋 𝜃 that maximizes the expected cumulative reward for given task: 𝜋 𝜃 = arg max 𝜋𝜃 E𝜏𝜋𝜃 [𝑅(𝜏)], (2) where 𝑅(𝜏) is reward function that evaluates the overall success of the trajectory 𝜏."
        },
        {
            "title": "3.2 Overview of the DeepAgent Framework\nAs illustrated in Figure 3, the DeepAgent framework is architected\naround a main reasoning process, which is supported by several\nauxiliary mechanisms to ensure robustness and efficiency.\n• Main Reasoning Process: The core of DeepAgent is a powerful\nlarge reasoning model that drives the entire task-completion\nprocess. In a single stream of thought, the LRM autonomously\nreasons about the task, dynamically discovers necessary tools,\nexecutes actions, and manages its own memory. This unified ap-\nproach departs from traditional, rigid agent workflows, allowing\nthe LRM to maintain a global perspective on the task.",
            "content": "Auxiliary Mechanisms: DeepAgent employs an auxiliary LLM to handle complex interactions with large toolsets and manage long histories. This background model enhances system stability by: (1) filtering and summarizing retrieved tool documentation if its too lengthy, (2) denoising and condensing verbose information returned from tool calls, and (3) compressing long interaction histories into structured memory. This division of labor allows the main LRM to concentrate on high-level strategic reasoning."
        },
        {
            "title": "3.3 Autonomous Tool Search and Calling\nDeepAgent’s main LRM performs all actions by generating specific\ntextual prompts within its continuous reasoning process. These\nactions are then intercepted and executed by the system.",
            "content": "Tool Search. When the agent determines it needs tool, it generates tool search query 𝑞𝑠 encapsulated within special tokens: <tool_search> 𝑞𝑠 </tool_search>. The systems tool retriever operates via dense retrieval. First, we build an index by pre-computing an embedding 𝐸 (𝑑𝑖 ) for the documentation 𝑑𝑖 of each tool 𝜏𝑖 using an embedding model 𝐸. During inference, given the query 𝑞𝑠 , the system retrieves the top-𝑘 tools by ranking them based on the cosine similarity sim(, ): Tretrieved = top-k 𝜏𝑖 (sim(𝐸 (𝑞𝑠 ), 𝐸 (𝑑𝑖 ))) . (3) The retrieved tool documentation is then processed by the auxiliary LLM summarized if too lengthy, otherwise provided directly and returned to the main LRMs context: <tool_search_result> relevant tools </tool_search_result>. Tool Call. To execute tool, the agent generates structured call including the tools name and arguments: <tool_call> {\"name\": \"tool_name\", \"arguments\": ...} </tool_call>. The framework parses this call, executes the tool, and captures the output. This output is, if necessary, summarized by the auxiliary LLM to ensure it is concise and helpful, before being fed back into the reasoning context: <tool_call_result> helpful information </tool_call_result>."
        },
        {
            "title": "3.4 Autonomous Memory Folding and",
            "content": "Brain-Inspired Memory Schema The agent can trigger memory folding at any logical point in its reasoning processsuch as after completing sub-task or realizing an exploration path was incorrectby generating special token: <fold_thought>. Upon detecting this token, the system initiates the memory folding process. The auxiliary LLM (parameterized by 𝜃aux) processes the entire preceding interaction history 𝑠𝑡 and generates three structured memory components in parallel: (𝑀𝐸, 𝑀𝑊 , 𝑀𝑇 ) = 𝑓compress (𝑠𝑡 ; 𝜃aux). These compressed episodic (𝑀𝐸), working (𝑀𝑊 ), and tool (𝑀𝑇 ) memories then replace the raw interaction history, enabling the agent to proceed with refreshed and condensed view of its progress while avoiding entrapment in incorrect exploration paths. (4) Inspired by human cognitive systems, the structured memory 𝑀𝑡 is composed of three distinct components that are generated in parallel: 𝑀𝑡 = (𝑀𝐸, 𝑀𝑊 , 𝑀𝑇 ), where 𝑀𝐸, 𝑀𝑊 , 𝑀𝑇 denote episodic, working, and tool memories, respectively. Episodic Memory (𝑀𝐸): This component serves as high-level log of the task, recording key events, major decision points, and sub-task completions. It provides the agent with long-term context regarding the overall task structure and its overarching goals. Working Memory (𝑀𝑊 ): This contains the most recent information, such as the current sub-goal, obstacles encountered, and near-term plans. It is the core component that ensures the continuity of the agents reasoning across the memory fold. Tool Memory (𝑀𝑇 ): This consolidates all tool-related interactions, including which tools have been used, how they were invoked, and their effectiveness. It allows the agent to learn from its experiences, refining its tool selection and usage strategies. To ensure that the compressed memory is stable and easily parsed by the agent, we employ an agent-usable data schema in JSON format instead of unstructured natural language. This structured format offers two main benefits: it maintains controllable and predictable structure, and it mitigates the loss of critical details that can occur when summarizing long-form text. Details of the data schema are provided in Appendix D."
        },
        {
            "title": "3.5 End-to-end RL Training with ToolPO\nWe train DeepAgent end-to-end with Tool Policy Optimization\n(ToolPO), an RL approach designed for general tool-using agents.",
            "content": "Training Data Collection. We first collect diverse training dataset spanning four categories. To instill general tool-use capabilities, we use ToolBench [37]. For real-world interaction, we leverage ALFWorld [46] and WebShop [66]. To enhance deep research skills, we incorporate data from WebDancer [59] and WebShaperQA [50]. Lastly, to improve mathematical reasoning with code, we use DeepMath [12]. Further details are available in Appendix A.1. Tool Simulator. Training an agent that interacts with thousands of real-world APIs is often impractical due to instability, latency, and cost. To address this, we develop an LLM-based Tool Simulator. This simulator, powered by an auxiliary LLM, mimics the responses of real-world APIs (e.g., RapidAPI). This approach provides stable, efficient, and low-cost environment for robust RL training. DeepAgent: General Reasoning Agent with Scalable Toolsets Conference17, July 2017, Washington, DC, USA Table 1: Main results on general tool usage tasks, encompassing scenarios with both labeled tools and open-set tool retrieval over large-scale toolsets. We report Pass@1 metric for all tasks. For 32B models, the best results are in bold and the second are underlined. Results from larger or closed-sourced models are in gray color for reference. Method Backbone ToolBench API-Bank TMDB Spotify ToolHop Success Path Success Path Success Path Success Path Correct Path Scenario 1: Completing Tasks w/ Ground-truth Tools Workflow-based Methods ReAct CodeAct Plan-and-Solve ReAct CodeAct Plan-and-Solve ReAct ReAct ReAct Autonomous Tool Usage within Reasoning DeepAgent-32B-Base DeepAgent-32B-RL Qwen2.5-32B Qwen2.5-32B Qwen2.5-32B QwQ-32B QwQ-32B QwQ-32B Qwen2.5-72B GPT-4o DeepSeek-R1 QwQ-32B QwQ-32B 41.0 53.0 52.0 52.0 54.0 55.0 56.0 52.0 57.0 63.0 69.0 64.7 68.3 65.4 61.6 63.4 64.7 69.3 53.9 68.3 74.3 78. 60.4 62.4 58.4 73.3 74.3 70.3 73.3 79.2 71.3 76.2 75.3 68.3 70.6 67.5 78.6 79.4 75.4 78.6 83.3 76.2 81.0 80.2 46.0 48.0 51.0 43.0 55.0 48.0 47.0 77.0 76.0 85.0 89. 65.3 67.4 71.6 65.3 74.5 61.3 67.7 89.3 89.0 92.0 94.8 Scenario 2: Completing Tasks w/ Open-Set Tool Retrieval Workflow-based Methods Qwen2.5-32B ReAct Qwen2.5-32B CodeAct Qwen2.5-32B Plan-and-Solve QwQ-32B ReAct QwQ-32B CodeAct QwQ-32B Plan-and-Solve Qwen2.5-72B ReAct GPT-4o ReAct ReAct DeepSeek-R1 Autonomous Tool Retrieval and Usage within Reasoning QwQ-32B DeepAgent-32B-Base QwQ-32B DeepAgent-32B-RL 20.8 19.0 20.4 19.0 21.6 19.6 21.6 28.9 22.3 55.0 51.0 54.0 44.0 48.0 45.0 52.0 41.0 47. 60.0 64.0 35.7 37.2 16.0 22.0 18.0 20.0 16.0 18.0 14.0 18.0 12.0 22.0 24.0 42.0 49.6 42.8 52.7 45.0 44.3 38.9 42.8 57.3 61.8 64. 11.0 19.0 15.0 18.0 31.0 24.0 28.0 35.0 34.0 52.0 55.0 34.5 46.8 40.5 40.3 52.8 46.8 50.7 56.8 53.1 71.8 74.3 29.8 33.3 28.1 47.4 52.6 49.1 57.9 47.4 64.9 70.2 75. 7.0 10.5 8.8 22.8 24.6 19.3 21.1 17.5 29.8 49.1 50.9 56.3 58.7 54.8 69.4 75.4 70.6 76.6 70.6 81.3 89.3 92.0 25.4 31.6 26.3 45.5 49.6 42.7 48.5 26.3 51.7 68.6 74. 37.6 34.7 39.2 47.4 43.2 45.4 44.8 40.0 50.2 49.1 51.3 13.2 12.7 12.0 27.1 29.0 25.7 21.1 24.1 36.2 38.4 40.6 49.1 48.8 49.7 51.6 53.4 50.6 55.4 53.7 61.8 59.8 62. 17.9 17.4 16.3 22.3 26.1 20.8 19.9 28.6 32.9 40.3 40.5 Global and Tool-Call Advantage Attribution. For each input prompt, we sample group of 𝐾 trajectories {𝜏1, . . . , 𝜏𝐾 }. ToolPO defines two distinct reward components. The first is reward for overall task success, 𝑅succ (𝜏), which is task success score reflecting the quality of the final outcome (e.g., the accuracy of the final answer). The second is tool-call reward, 𝑅action (𝜏), which reflects the quality of intermediate actions. This action-level reward is composed of rewards for correct tool invocations and efficient memory folding. Specifically, 𝑅action (𝜏) = 𝜆1 ) + 𝜆2𝑆pref (𝜏), where 𝑡 =1 𝐶 (𝑎call ) is 1 if tool call is correct and 0 otherwise. 𝑆pref (𝜏) is 𝑡 preference score encouraging efficient use of memory folding, defined by comparing trajectory with folding (𝜏fold) to one without (𝜏direct): 𝑆pref = (𝐿(𝜏direct) 𝐿(𝜏fold))/(𝐿(𝜏direct) + 𝐿(𝜏fold)). 𝐶 (𝑎call 𝑡 (cid:205)𝑇 Based on these rewards, we compute two separate group-relative advantages. The task success advantage for trajectory 𝜏𝑘 is: 𝐴succ (𝜏𝑘 ) = 𝑅succ (𝜏𝑘 ) 1 𝐾 𝐾 𝑗= 𝑅succ (𝜏 𝑗 ). (5) This advantage is attributed to all generated tokens in the trajectory, providing global learning signal. Similarly, the action-level advantage is: 𝐴action (𝜏𝑘 ) = 𝑅action (𝜏𝑘 ) 1 𝐾 𝐾 𝑗=1 𝑅action (𝜏 𝑗 ). (6) Crucially, this advantage is attributed only to the specific tokens that constitute the tool call and memory folding actions. This finegrained credit assignment provides more targeted signal for learning correct and efficient tool use. Optimization Objective. The total advantage for given token 𝑦𝑖 in trajectory 𝜏𝑘 is the sum of the global and local advantages: 𝐴(𝑦𝑖 ) = 𝐴succ (𝜏𝑘 ) + 𝑀 (𝑦𝑖 ) 𝐴action (𝜏𝑘 ), (7) where 𝑀 (𝑦𝑖 ) is mask that is 1 if 𝑦𝑖 is part of tool-call or memoryfold token sequence, and 0 otherwise. ToolPO then optimizes the policy using clipped surrogate objective function: LToolPO (𝜃 ) = (cid:104)𝜏𝑘 𝑖=1 E𝜏𝑘 min (cid:16) 𝜌𝑖 (𝜃 )𝐴(𝑦𝑖 ), clip(𝜌𝑖 (𝜃 ), 1 𝜖, 1 + 𝜖)𝐴(𝑦𝑖 ) (8) (cid:17)(cid:105) , old 𝜋𝜃 (𝑦𝑖 𝑦<𝑖 ,𝑠 ) 𝜋𝜃 (𝑦𝑖 𝑦<𝑖 ,𝑠 ) is the probability ratio for token 𝑦𝑖 . Here, 𝜌𝑖 (𝜃 ) = This objective encourages the model to increase the probability of both intermediate actions and end-to-end task accomplishment that exhibit positive relative advantage, thereby ensuring stable and effective policy updates. Conference17, July 2017, Washington, DC, USA Xiaoxi Li et al. Table 2: Main results on downstream task applications, spanning Embodied AI (ALFWorld), Online Shopping (WebShop), General AI Assistants (GAIA), and Humanitys Last Exam (HLE). We report Pass@1 for all tasks. For 32B models, the best results are in bold and the second are underlined. Results from larger or closed-sourced models are in gray color for reference. Method Backbone ALFWorld WebShop GAIA HLE Success Path Success Score Text MM File All Text MM All Completing Tasks w/ Task-specific Toolsets 60.4 65.7 66.4 63.4 82.1 78.4 85.1 79.1 86.0 86.5 79.1 65.7 93.3 Qwen2.5-32B Qwen2.5-32B Qwen2.5-32B Qwen2.5-32B QwQ-32B QwQ-32B QwQ-32B QwQ-32B Llama2-70B Qwen2.5-72B DeepSeek-R1 GPT-4o Claude-4 Workflow-based Methods ReAct CodeAct Reflextion Plan-and-Solve ReAct CodeAct Reflextion Plan-and-Solve AgentLM* ReAct ReAct ReAct ReAct Autonomous Tool Usage within Reasoning - Deep Research - WebThinker 84.3 HiRA 88.1 DeepAgent-32B-Base 91.8 DeepAgent-32B-RL OpenAI (o3) QwQ-32B QwQ-32B QwQ-32B QwQ-32B 79.1 83.3 86.0 80.4 87.8 86.2 88.4 84.7 - 86.5 85.8 87.8 91.5 - - 87.6 91.4 92.0 6.0 12.4 9.2 7.6 17.2 18.0 21.6 16.0 - 22.0 19.6 15.6 20.4 - - 23.2 32.0 34.4 28.8 34.5 31.6 29.3 45.3 46.4 50.4 43.8 64.9 44.5 49.7 52.5 56. - - 51.9 55.4 56.3 25.2 28.2 29.1 27.2 35.0 38.8 37.9 36.9 - 32.0 43.7 35.0 56.3 - 48.5 44.7 49.5 58.3 16.7 20.8 20.8 16.7 8.3 20.8 20.8 16.7 - 20.8 29.2 16.7 37.5 - 25.0 33.3 37.5 33.3 13.2 18.4 18.4 15.8 36.8 31.6 36.8 34.2 - 31.6 39.5 36.8 52. - 13.2 42.1 44.7 52.6 21.2 24.8 25.5 23.0 31.5 34.5 35.2 33.3 - 30.3 40.6 32.7 52.7 67.4 37.0 42.5 46.7 53.3 6.5 7.5 5.9 7.2 13.2 14.2 11.9 12.9 - 9.0 14.2 13.2 15.5 - 14.2 14.5 19.1 21.7 7.1 8.0 5.3 6.2 8.8 8.0 7.1 9.7 - 8.0 8.8 10.6 16. - 8.8 10.6 13.3 15.0 6.6 7.6 5.8 7.0 12.2 12.8 10.8 12.2 - 8.8 13.0 12.6 15.8 26.6 13.0 13.6 17.8 20."
        },
        {
            "title": "4 Experimental Settings\n4.1 Tasks and Datasets\nWe conduct extensive experiments on a wide range of benchmarks,\nincluding general tool-use and downstream applications.",
            "content": "General Tool-Use. These benchmarks encompass broad range of distinct tools, scaling from tens to over ten thousand, making them ideal for evaluating the scalability of different approaches. We utilize four representative scenarios: ToolBench [37], based on over 16,000 real-world APIs, for which we use the G3 subset requiring multi-step, multi-tool calls; API-Bank [24], which includes 314 human-annotated dialogues with 73 APIs and 753 API calls, to assess planning, retrieval, and calling capabilities; RestBench [47], comprising scenarios from the TMDB movie database (54 tools, avg. 2.3 calls/question) and the Spotify music player (40 tools, avg. 2.6 calls/question) to simulate typical REST applications; and ToolHop [68], multi-hop reasoning dataset with 3,912 locally executable tools that necessitate 3 to 7 sequential tool calls per task. For these tasks, we adopt two settings: given ground-truth tools and given entire toolsets with tool retrieval capabilities. Downstream Applications. We evaluate our approach on several downstream applications that require domain-specific toolsets. These include ALFWorld [46], text-based embodied AI task where agents complete goals using nine basic actions (e.g., move, take); WebShop [66], an online shopping environment with search and click actions to fulfill users specific product purchasing requirements; GAIA [32], complex information-seeking benchmark where we equip the agent with tools for web search, page browsing, Visual Question Answering (VQA), code compilation, and file reading; and Humanitys Last Exam (HLE) [35], set of highly difficult reasoning problems, for which we provide code, search, page browsing, and VQA tools. These benchmarks test the agents ability to perform long-horizon planning and robust interaction in complex, real-world scenarios. For this category of tasks, we provide agents with task-specific toolsets."
        },
        {
            "title": "4.3 Implementation Details\nWe use QwQ-32B [51] as DeepAgent’s backbone model, with Qwen2.5-\n32B-Instruct [40] as the auxiliary model in our main results. Text\ngeneration employs a maximum of 81,920 tokens with temperature\n0.7, top_p 0.8, top_k 20, and repetition penalty 1.05. Web search",
            "content": "DeepAgent: General Reasoning Agent with Scalable Toolsets Conference17, July 2017, Washington, DC, USA Table 4: Effectiveness analysis of autonomous tool retrieval strategy in open-set scenarios compared to pre-retrieved tool methods. Numbers in parentheses indicate toolset sizes. Figure 4: Visualization of training dynamics, including (a) reward scores and (b) validation scores across training steps. Table 3: Ablation studies on the components of DeepAgent, where the best results are in bold. Method ToolB. ToolH. TMDB Spotify (16k) (3.9k) (54) (40) ReAct Workflow Input Retrieved Tool Auto. Tool Retrieval 35.0 34. Plan-and-Solve Workflow 37.0 Input Retrieved Tool 45.0 Auto. Tool Retrieval 25.4 37.1 24.8 25.7 14.0 18.0 19.0 24.0 15.0 27. 16.0 19.3 Avg. 22.4 28.0 24.2 28.5 End-to-end Agentic Reasoning (DeepAgent) 34.0 Input Retrieved Tool 55.0 Auto. Tool Retrieval 53.0 64. 37.0 40.6 43.9 50.9 42.0 52.6 Method Tool-Usage Application ToolB. ToolH. WebS. GAIA Avg. on ToolBench by up to 6.0% and on Spotify (labeled) by 5.2%. This validates the effectiveness of the ToolPO strategy, which uses an LLM-based tool simulator and fine-grained advantage attribution. DeepAgent-32B-RL 64.0 40.6 34. 53.3 48.1 60.0 w/o Training (Base) 63.0 w/o Memory Folding w/o Tool Simulation 62.0 w/o Tool Adv. Attribution 62.0 38.4 36.6 35.2 39.6 32.0 32.4 33.6 33.2 46.7 44.7 48.5 49. 44.3 44.2 44.8 46.1 and page browsing are implemented using Google Serper API and Jina Reader API, respectively. The VQA tool is based on Qwen2.5VL-32B-Instruct [1]. Tool retrieval is performed using bge-largeen-v1.5 [61]. Training consists of 100 steps of ToolPO with batch size 64, 𝜆1 = 𝜆2 = 1, rollout size 𝐾 = 8, and maximum sequence length 32,768. Additional details are provided in Appendix C. All experiments are conducted on 64 NVIDIA H20-141GB GPUs."
        },
        {
            "title": "5.3 Analysis of Training Dynamics\nFigure 4 shows the training dynamics of DeepAgent, including the\nreward scores and validation scores across training steps. As shown\nin the figure, (1) DeepAgent trained with ToolPO achieves\nhigher upper bounds on both reward and validation scores\ncompared to the commonly used GRPO. (2) Moreover, the\ntraining reward exhibits less fluctuation than GRPO, demon-\nstrating better training stability. This indicates that using tool",
            "content": "Conference17, July 2017, Washington, DC, USA Xiaoxi Li et al. Table 5: Performance comparison with different reasoning model backbones, spanning MOE-based models with 30B and 235B parameters. Method Tool-Usage Application ToolB. ToolH. ALF. WebS. GAIA Qwen3-30B-A3B-Thinking ReAct Plan-and-Solve DeepAgent (Base) 52.0 50.0 59.0 22.0 23.6 47.5 Qwen3-235B-A22B-Thinking ReAct Plan-and-Solve DeepAgent (Base) 61.0 63.0 67. 40.9 43.0 48.2 67.9 68.7 69.4 79.9 78.4 85.8 18.4 20.4 31.4 21.6 24.4 37.2 34.5 35.2 39. 36.4 38.4 51.5 Avg. 35.7 37.0 46.9 45.1 46.0 55.7 varied. The results yield several key insights. (1) DeepAgent consistently and significantly outperforms the ReAct baseline across all tested action limits on both datasets, demonstrating its superior effectiveness. (2) For both agents, performance generally improves as the maximum number of actions increases. This suggests that complex tasks benefit from longer interaction horizon, allowing for more thorough exploration and reasoning. (3) DeepAgent exhibits stronger scalability. As the action limit increases, the performance gap between DeepAgent and ReAct widens, particularly on WebShop. This sustained gain suggests DeepAgent strategically selects effective, task-relevant actions, avoiding the wasteful steps that limit ReActs scalability."
        },
        {
            "title": "6 Conclusion\nIn this work, we introduce DeepAgent, an end-to-end reasoning\nagent that unifies thinking, tool discovery, and execution into a\nsingle, coherent agentic reasoning process. To enable robust long-\nhorizon interaction, we propose an autonomous memory folding\nmechanism that compresses interaction history into a structured\nmemory, allowing the agent to \"take a breath\" and reconsider its\nstrategy. We also introduce ToolPO, an end-to-end RL method that\nleverages LLM simulated APIs for stable training and fine-grained\nadvantage attribution for precise credit assignment to tool invoca-\ntions. Extensive experiments on general tool-use and downstream\napplications demonstrate that DeepAgent significantly outperforms\nvarious baseline agents, particularly in open-set scenarios requiring\ndynamic tool discovery over scalable toolsets.",
            "content": "Figure 5: Scaling analysis of performance with respect to maximum action limits on WebShop and GAIA datasets. simulators instead of directly training with unstable real-world APIs, along with employing tool-call process supervision, enables more stable and effective training of tool-usage capabilities."
        },
        {
            "title": "5.6 Scaling Analysis of Action Limits\nFigure 5 illustrates the performance of DeepAgent and ReAct on\nthe WebShop and GAIA datasets as the maximum action limit is",
            "content": "DeepAgent: General Reasoning Agent with Scalable Toolsets Conference17, July 2017, Washington, DC, USA References [1] Shuai Bai, Keqin Chen, Xuejing Liu, Jialin Wang, Wenbin Ge, Sibo Song, Kai Dang, Peng Wang, Shijie Wang, Jun Tang, Humen Zhong, Yuanzhi Zhu, Ming-Hsuan Yang, Zhaohai Li, Jianqiang Wan, Pengfei Wang, Wei Ding, Zheren Fu, Yiheng Xu, Jiabo Ye, Xi Zhang, Tianbao Xie, Zesen Cheng, Hang Zhang, Zhibo Yang, Haiyang Xu, and Junyang Lin. 2025. Qwen2.5-VL Technical Report. CoRR abs/2502.13923 (2025). arXiv:2502.13923 doi:10.48550/ARXIV.2502.13923 [2] Qiguang Chen, Libo Qin, Jinhao Liu, Dengyun Peng, Jiannan Guan, Peng Wang, Mengkang Hu, Yuhang Zhou, Te Gao, and Wanxiang Che. 2025. Towards Reasoning Era: Survey of Long Chain-of-Thought for Reasoning Large Language Models. CoRR abs/2503.09567 (2025). arXiv:2503.09567 doi:10.48550/ARXIV.2503.09567 [3] Xingyu Chen, Jiahao Xu, Tian Liang, Zhiwei He, Jianhui Pang, Dian Yu, Linfeng Song, Qiuzhi Liu, Mengfei Zhou, Zhuosheng Zhang, Rui Wang, Zhaopeng Tu, Haitao Mi, and Dong Yu. 2024. Do NOT Think That Much for 2+3=? On the Overthinking of o1-Like LLMs. arXiv:2412.21187 [cs.CL] https://arxiv.org/abs/ 2412.21187 [4] Yifei Chen, Guanting Dong, and Zhicheng Dou. 2025. Toward Effective Tool-Integrated Reasoning via Self-Evolved Preference Learning. arXiv:2509.23285 [cs.AI] https://arxiv.org/abs/2509. [5] DeepSeek-AI, Daya Guo, Dejian Yang, Haowei Zhang, Junxiao Song, Ruoyu Zhang, Runxin Xu, Qihao Zhu, Shirong Ma, Peiyi Wang, Xiao Bi, Xiaokang Zhang, Xingkai Yu, Yu Wu, Z. F. Wu, Zhibin Gou, Zhihong Shao, Zhuoshu Li, Ziyi Gao, Aixin Liu, Bing Xue, Bingxuan Wang, Bochao Wu, Bei Feng, Chengda Lu, Chenggang Zhao, Chengqi Deng, Chenyu Zhang, Chong Ruan, Damai Dai, Deli Chen, Dongjie Ji, Erhang Li, Fangyun Lin, Fucong Dai, Fuli Luo, Guangbo Hao, Guanting Chen, Guowei Li, H. Zhang, Han Bao, Hanwei Xu, Haocheng Wang, Honghui Ding, Huajian Xin, Huazuo Gao, Hui Qu, Hui Li, Jianzhong Guo, Jiashi Li, Jiawei Wang, Jingchang Chen, Jingyang Yuan, Junjie Qiu, Junlong Li, J. L. Cai, Jiaqi Ni, Jian Liang, Jin Chen, Kai Dong, Kai Hu, Kaige Gao, Kang Guan, Kexin Huang, Kuai Yu, Lean Wang, Lecong Zhang, Liang Zhao, Litong Wang, Liyue Zhang, Lei Xu, Leyi Xia, Mingchuan Zhang, Minghua Zhang, Minghui Tang, Meng Li, Miaojun Wang, Mingming Li, Ning Tian, Panpan Huang, Peng Zhang, Qiancheng Wang, Qinyu Chen, Qiushi Du, Ruiqi Ge, Ruisong Zhang, Ruizhe Pan, Runji Wang, R. J. Chen, R. L. Jin, Ruyi Chen, Shanghao Lu, Shangyan Zhou, Shanhuang Chen, Shengfeng Ye, Shiyu Wang, Shuiping Yu, Shunfeng Zhou, Shuting Pan, and S. S. Li. 2025. DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning. CoRR abs/2501.12948 (2025). arXiv:2501.12948 doi:10.48550/ARXIV.2501.12948 [6] Guanting Dong, Licheng Bao, Zhongyuan Wang, Kangzhi Zhao, Xiaoxi Li, Jiajie Jin, Jinghan Yang, Hangyu Mao, Fuzheng Zhang, Kun Gai, Guorui Zhou, Yutao Zhu, Ji-Rong Wen, and Zhicheng Dou. 2025. Agentic Entropy-Balanced Policy Optimization. arXiv:2510.14545 [cs.LG] https://arxiv.org/abs/2510.14545 [7] Guanting Dong, Yifei Chen, Xiaoxi Li, Jiajie Jin, Hongjin Qian, Yutao Zhu, Hangyu Mao, Guorui Zhou, Zhicheng Dou, and Ji-Rong Wen. 2025. Tool-Star: Empowering LLM-Brained Multi-Tool Reasoner via Reinforcement Learning. CoRR abs/2505.16410 (2025). arXiv:2505.16410 doi:10.48550/ARXIV.2505.16410 [8] Guanting Dong, Hangyu Mao, Kai Ma, Licheng Bao, Yifei Chen, Zhongyuan Wang, Zhongxia Chen, Jiazhen Du, Huiyang Wang, Fuzheng Zhang, Guorui Zhou, Yutao Zhu, Ji-Rong Wen, and Zhicheng Dou. 2025. Agentic Reinforced Policy Optimization. CoRR abs/2507.19849 (2025). arXiv:2507.19849 doi:10.48550/ ARXIV.2507.19849 [9] Runnan Fang, Shihao Cai, Baixuan Li, Jialong Wu, Guangyu Li, Wenbiao Yin, Xinyu Wang, Xiaobin Wang, Liangcai Su, Zhen Zhang, Shibin Wu, Zhengwei Tao, Yong Jiang, Pengjun Xie, Fei Huang, and Jingren Zhou. 2025. Towards General Agentic Intelligence via Environment Scaling. arXiv:2509.13311 [cs.CL] https://arxiv.org/abs/2509.13311 [10] Jiazhan Feng, Shijue Huang, Xingwei Qu, Ge Zhang, Yujia Qin, Baoquan Zhong, Chengquan Jiang, Jinxin Chi, and Wanjun Zhong. 2025. ReTool: Reinforcement Learning for Strategic Tool Use in LLMs. arXiv:2504.11536 [cs.CL] https://arxiv. org/abs/2504.11536 [11] Jiaxuan Gao, Wei Fu, Minyang Xie, Shusheng Xu, Chuyi He, Zhiyu Mei, Banghua Zhu, and Yi Wu. 2025. Beyond Ten Turns: Unlocking Long-Horizon Agentic Search with Large-Scale Asynchronous RL. CoRR abs/2508.07976 (2025). arXiv:2508.07976 doi:10.48550/ARXIV.2508. [12] Zhiwei He, Tian Liang, Jiahao Xu, Qiuzhi Liu, Xingyu Chen, Yue Wang, Linfeng Song, Dian Yu, Zhenwen Liang, Wenxuan Wang, Zhuosheng Zhang, Rui Wang, Zhaopeng Tu, Haitao Mi, and Dong Yu. 2025. DeepMath-103K: Large-Scale, Challenging, Decontaminated, and Verifiable Mathematical Dataset for Advancing Reasoning. (2025). arXiv:2504.11456 [cs.CL] https://arxiv.org/abs/2504.11456 [13] Xinyi Hou, Yanjie Zhao, Shenao Wang, and Haoyu Wang. 2025. Model Context Protocol (MCP): Landscape, Security Threats, and Future Research Directions. CoRR abs/2503.23278 (2025). arXiv:2503.23278 doi:10.48550/ARXIV.2503.23278 [14] Jingcheng Hu, Yinmin Zhang, Qi Han, Daxin Jiang, Xiangyu Zhang, and HeungYeung Shum. 2025. Open-Reasoner-Zero: An Open Source Approach to Scaling Up Reinforcement Learning on the Base Model. CoRR abs/2503.24290 (2025). arXiv:2503.24290 doi:10.48550/ARXIV.2503.24290 [15] Yuxuan Huang, Yihang Chen, Haozheng Zhang, Kang Li, Meng Fang, Linyi Yang, Xiaoguang Li, Lifeng Shang, Songcen Xu, Jianye Hao, Kun Shao, and Jun Wang. 2025. Deep Research Agents: Systematic Examination And Roadmap. CoRR abs/2506.18096 (2025). arXiv:2506.18096 doi:10.48550/ARXIV.2506.18096 [16] Aaron Jaech, Adam Kalai, Adam Lerer, Adam Richardson, Ahmed El-Kishky, Aiden Low, Alec Helyar, Aleksander Madry, Alex Beutel, Alex Carney, et al. 2024. OpenAI o1 System Card. arXiv preprint arXiv:2412.16720 (2024). [17] Dongfu Jiang, Yi Lu, Zhuofeng Li, Zhiheng Lyu, Ping Nie, Haozhe Wang, Alex Su, Hui Chen, Kai Zou, Chao Du, Tianyu Pang, and Wenhu Chen. 2025. VerlTool: Towards Holistic Agentic Reinforcement Learning with Tool Use. arXiv:2509.01055 [cs.AI] https://arxiv.org/abs/2509.01055 [18] Bowen Jin, Hansi Zeng, Zhenrui Yue, Dong Wang, Hamed Zamani, and Jiawei Han. 2025. Search-R1: Training LLMs to Reason and Leverage Search Engines with Reinforcement Learning. CoRR abs/2503.09516 (2025). arXiv:2503.09516 doi:10.48550/ARXIV.2503. [19] Haolin Jin, Linghan Huang, Haipeng Cai, Jun Yan, Bo Li, and Huaming Chen. 2024. From LLMs to LLM-based Agents for Software Engineering: Survey of Current, Challenges and Future. CoRR abs/2408.02479 (2024). arXiv:2408.02479 doi:10.48550/ARXIV.2408.02479 [20] Jiajie Jin, Xiaoxi Li, Guanting Dong, Yuyao Zhang, Yutao Zhu, Zhao Yang, Hongjin Qian, and Zhicheng Dou. 2025. Decoupled Planning and Execution: Hierarchical Reasoning Framework for Deep Search. CoRR abs/2507.02652 (2025). arXiv:2507.02652 doi:10.48550/ARXIV.2507.02652 [21] Jiajie Jin, Yuyao Zhang, Yimeng Xu, Hongjin Qian, Yutao Zhu, and Zhicheng Dou. 2025. FinSight: Towards Real-World Financial Deep Research. arXiv:2510.16844 [cs.CL] https://arxiv.org/abs/2510.16844 [22] Minki Kang, Wei-Ning Chen, Dongge Han, Huseyin A. Inan, Lukas Wutschitz, Yanzhi Chen, Robert Sim, and Saravan Rajmohan. 2025. ACON: Optimizing Context Compression for Long-horizon LLM Agents. arXiv:2510.00615 [cs.AI] https://arxiv.org/abs/2510.00615 [23] Kuan Li, Zhongwang Zhang, Huifeng Yin, Rui Ye, Yida Zhao, Liwen Zhang, Litu Ou, Dingchu Zhang, Xixi Wu, Jialong Wu, Xinyu Wang, Zile Qiao, Zhen Zhang, Yong Jiang, Pengjun Xie, Fei Huang, and Jingren Zhou. 2025. WebSailorV2: Bridging the Chasm to Proprietary Agents via Synthetic Data and Scalable Reinforcement Learning. CoRR abs/2509.13305 (2025). arXiv:2509.13305 doi:10. 48550/ARXIV.2509.13305 [24] Minghao Li, Yingxiu Zhao, Bowen Yu, Feifan Song, Hangyu Li, Haiyang Yu, Zhoujun Li, Fei Huang, and Yongbin Li. 2023. API-Bank: Comprehensive Benchmark for Tool-Augmented LLMs. In Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing, EMNLP 2023, Singapore, December 6-10, 2023, Houda Bouamor, Juan Pino, and Kalika Bali (Eds.). Association for Computational Linguistics, 31023116. doi:10.18653/V1/2023.EMNLP-MAIN.187 [25] Xiaoxi Li, Guanting Dong, Jiajie Jin, Yuyao Zhang, Yujia Zhou, Yutao Zhu, Peitian Zhang, and Zhicheng Dou. 2025. Search-o1: Agentic Search-Enhanced Large Reasoning Models. CoRR abs/2501.05366 (2025). arXiv:2501.05366 doi:10.48550/ ARXIV.2501. [26] Xiaoxi Li, Jiajie Jin, Guanting Dong, Hongjin Qian, Yutao Zhu, Yongkang Wu, Ji-Rong Wen, and Zhicheng Dou. 2025. WebThinker: Empowering Large Reasoning Models with Deep Research Capability. CoRR abs/2504.21776 (2025). arXiv:2504.21776 doi:10.48550/ARXIV.2504.21776 [27] Xuefeng Li, Haoyang Zou, and Pengfei Liu. 2025. ToRL: Scaling Tool-Integrated RL. arXiv:2503.23383 [cs.CL] https://arxiv.org/abs/2503.23383 [28] Zhong-Zhi Li, Duzhen Zhang, Ming-Liang Zhang, Jiaxin Zhang, Zengyan Liu, Yuxuan Yao, Haotian Xu, Junhao Zheng, Pei-Jie Wang, Xiuyi Chen, Yingying Zhang, Fei Yin, Jiahua Dong, Zhijiang Guo, Le Song, and Cheng-Lin Liu. 2025. From System 1 to System 2: Survey of Reasoning Large Language Models. CoRR abs/2502.17419 (2025). arXiv:2502.17419 doi:10.48550/ARXIV.2502.17419 [29] Zhuofeng Li, Haoxiang Zhang, Seungju Han, Sheng Liu, Jianwen Xie, Yu In-the-Flow Agentic SysZhang, Yejin Choi, James Zou, and Pan Lu. 2025. tem Optimization for Effective Planning and Tool Use. arXiv:2510.05592 [cs.AI] https://arxiv.org/abs/2510.05592 [30] Junteng Liu, Yunji Li, Chi Zhang, Jingyang Li, Aili Chen, Ke Ji, Weiyu Cheng, Zijia Wu, Chengyu Du, Qidi Xu, Jiayuan Song, Zhengmao Zhu, Wenhu Chen, Pengyu Zhao, and Junxian He. 2025. WebExplorer: Explore and Evolve for Training Long-Horizon Web Agents. CoRR abs/2509.06501 (2025). arXiv:2509.06501 doi:10. 48550/ARXIV.2509. [31] Zichen Liu, Anya Sims, Keyu Duan, Changyu Chen, Simon Yu, Xiangxin Zhou, Haotian Xu, Shaopan Xiong, Bo Liu, Chenmien Tan, Chuen Yang Beh, Weixun Wang, Hao Zhu, Weiyan Shi, Diyi Yang, Michael Shieh, Yee Whye Teh, Wee Sun Lee, and Min Lin. 2025. GEM: Gym for Agentic LLMs. arXiv:2510.01051 [cs.LG] https://arxiv.org/abs/2510.01051 [32] Grégoire Mialon, Clémentine Fourrier, Thomas Wolf, Yann LeCun, and Thomas Scialom. 2024. GAIA: benchmark for General AI Assistants. In The Twelfth International Conference on Learning Representations, ICLR 2024, Vienna, Austria, May 7-11, 2024. OpenReview.net. https://openreview.net/forum?id=fibxvahvs3 [33] Yingqian Min, Zhipeng Chen, Jinhao Jiang, Jie Chen, Jia Deng, Yiwen Hu, Yiru Tang, Jiapeng Wang, Xiaoxue Cheng, Huatong Song, et al. 2024. Imitate, explore, and self-improve: reproduction report on slow-thinking reasoning systems. arXiv preprint arXiv:2412.09413 (2024). Conference17, July 2017, Washington, DC, USA Xiaoxi Li et al. [34] OpenAI. 2025. Introducing deep research. https://openai.com/index/introducingAdvances in Neural Information Processing Systems 36 (2024). deep-research. [35] Long Phan, Alice Gatti, Ziwen Han, Nathaniel Li, Josephina Hu, Hugh Zhang, Sean Shi, Michael Choi, Anish Agrawal, Arnav Chopra, Adam Khoja, Ryan Kim, Jason Hausenloy, Oliver Zhang, Mantas Mazeika, Daron Anderson, Tung Nguyen, Mobeen Mahmood, Fiona Feng, Steven Y. Feng, Haoran Zhao, Michael Yu, Varun Gangal, Chelsea Zou, Zihan Wang, Jessica P. Wang, Pawan Kumar, Oleksandr Pokutnyi, Robert Gerbicz, Serguei Popov, John-Clark Levin, Mstyslav Kazakov, Johannes Schmitt, Geoff Galgon, Alvaro Sanchez, Yongki Lee, Will Yeadon, Scott Sauers, Marc Roth, Chidozie Agu, Søren Riis, Fabian Giska, Saiteja Utpala, Zachary Giboney, Gashaw M. Goshu, Joan of Arc Xavier, Sarah-Jane Crowson, Mohinder Maheshbhai Naiya, Noah Burns, Lennart Finke, Zerui Cheng, Hyunwoo Park, Francesco Fournier-Facio, John Wydallis, Mark Nandor, Ankit Singh, Tim Gehrunger, Jiaqi Cai, Ben McCarty, Darling Duclosel, Jungbae Nam, Jennifer Zampese, Ryan G. Hoerr, Aras Bacho, Gautier Abou Loume, Abdallah Galal, Hangrui Cao, Alexis C. Garretson, Damien Sileo, Qiuyu Ren, Doru Cojoc, Pavel Arkhipov, Usman Qazi, Lianghui Li, Sumeet Motwani, Christian Schröder de Witt, Edwin Taylor, Johannes Veith, Eric Singer, Taylor D. Hartman, Paolo Rissone, Jaehyeok Jin, Jack Wei Lun Shi, Chris G. Willcocks, Joshua Robinson, Aleksandar Mikov, Ameya Prabhu, Longke Tang, Xavier Alapont, Justine Leon Uro, Kevin Zhou, Emily de Oliveira Santos, Andrey Pupasov Maksimov, Edward Vendrow, Kengo Zenitani, Julien Guillod, Yuqi Li, Joshua Vendrow, Vladyslav Kuchkin, and Ng Ze-An. 2025. Humanitys Last Exam. CoRR abs/2501.14249 (2025). arXiv:2501.14249 doi:10.48550/ARXIV.2501.14249 [36] Yiwei Qin, Xuefeng Li, Haoyang Zou, Yixiu Liu, Shijie Xia, Zhen Huang, Yixin Ye, Weizhe Yuan, Hector Liu, Yuanzhi Li, et al. 2024. O1 Replication Journey: Strategic Progress ReportPart 1. arXiv preprint arXiv:2410.18982 (2024). [37] Yujia Qin, Shihao Liang, Yining Ye, Kunlun Zhu, Lan Yan, Yaxi Lu, Yankai Lin, Xin Cong, Xiangru Tang, Bill Qian, Sihan Zhao, Lauren Hong, Runchu Tian, Ruobing Xie, Jie Zhou, Mark Gerstein, Dahai Li, Zhiyuan Liu, and Maosong Sun. 2024. ToolLLM: Facilitating Large Language Models to Master 16000+ Real-world APIs. In The Twelfth International Conference on Learning Representations, ICLR 2024, Vienna, Austria, May 7-11, 2024. OpenReview.net. https://openreview.net/forum? id=dHng2O0Jjr [38] Changle Qu, Sunhao Dai, Xiaochi Wei, Hengyi Cai, Shuaiqiang Wang, Dawei Yin, Jun Xu, and Ji-Rong Wen. 2025. From Exploration to Mastery: Enabling LLMs to Master Tools via Self-Driven Interactions. In The Thirteenth International Conference on Learning Representations, ICLR 2025, Singapore, April 24-28, 2025. OpenReview.net. https://openreview.net/forum?id=QKBu1BOAwd [39] Changle Qu, Sunhao Dai, Xiaochi Wei, Hengyi Cai, Shuaiqiang Wang, Dawei Yin, Jun Xu, and Ji-Rong Wen. 2025. Tool learning with large language models: survey. Frontiers Comput. Sci. 19, 8 (2025), 198343. doi:10.1007/S11704-024-406782 [40] Qwen, :, An Yang, Baosong Yang, Beichen Zhang, Binyuan Hui, Bo Zheng, Bowen Yu, Chengyuan Li, Dayiheng Liu, Fei Huang, Haoran Wei, Huan Lin, Jian Yang, Jianhong Tu, Jianwei Zhang, Jianxin Yang, Jiaxi Yang, Jingren Zhou, Junyang Lin, Kai Dang, Keming Lu, Keqin Bao, Kexin Yang, Le Yu, Mei Li, Mingfeng Xue, Pei Zhang, Qin Zhu, Rui Men, Runji Lin, Tianhao Li, Tingyu Xia, Xingzhang Ren, Xuancheng Ren, Yang Fan, Yang Su, Yichang Zhang, Yu Wan, Yuqiong Liu, Zeyu Cui, Zhenru Zhang, and Zihan Qiu. 2024. Qwen2.5 Technical Report. arXiv:2412.15115 [cs.CL] https://arxiv.org/abs/2412. [41] Timo Schick, Jane Dwivedi-Yu, Roberto Dessì, Roberta Raileanu, Maria Lomeli, Eric Hambro, Luke Zettlemoyer, Nicola Cancedda, and Thomas Scialom. 2023. Toolformer: Language Models Can Teach Themselves to Use Tools. In Advances in Neural Information Processing Systems 36: Annual Conference on Neural Information Processing Systems 2023, NeurIPS 2023, New Orleans, LA, USA, December 10 - 16, 2023, Alice Oh, Tristan Naumann, Amir Globerson, Kate Saenko, Moritz Hardt, and Sergey Levine (Eds.). http://papers.nips.cc/paper_files/paper/2023/ hash/d842425e4bf79ba039352da0f658a906-Abstract-Conference.html [42] Guangming Sheng, Chi Zhang, Zilingfeng Ye, Xibin Wu, Wang Zhang, Ru Zhang, Yanghua Peng, Haibin Lin, and Chuan Wu. 2024. HybridFlow: Flexible and Efficient RLHF Framework. arXiv preprint arXiv: 2409.19256 (2024). [43] Zhengliang Shi, Yuhan Wang, Lingyong Yan, Pengjie Ren, Shuaiqiang Wang, Dawei Yin, and Zhaochun Ren. 2025. Retrieval Models Arent Tool-Savvy: Benchmarking Tool Retrieval for Large Language Models. In Findings of the Association for Computational Linguistics, ACL 2025, Vienna, Austria, July 27 - August 1, 2025, Wanxiang Che, Joyce Nabende, Ekaterina Shutova, and Mohammad Taher Pilehvar (Eds.). Association for Computational Linguistics, 24497 24524. https://aclanthology.org/2025.findings-acl.1258/ [44] Noah Shinn, Federico Cassano, Ashwin Gopinath, Karthik Narasimhan, and Shunyu Yao. 2023. Reflexion: language agents with verbal reinforcement learning. In Advances in Neural Information Processing Systems 36: Annual Conference on Neural Information Processing Systems 2023, NeurIPS 2023, New Orleans, LA, USA, December 10 - 16, 2023, Alice Oh, Tristan Naumann, Amir Globerson, Kate Saenko, Moritz Hardt, and Sergey Levine (Eds.). http://papers.nips.cc/paper_files/paper/ 2023/hash/1b44b878bb782e6954cd888628510e90-Abstract-Conference.html [45] Noah Shinn, Federico Cassano, Ashwin Gopinath, Karthik Narasimhan, and Shunyu Yao. 2024. Reflexion: Language agents with verbal reinforcement learning. [46] Mohit Shridhar, Xingdi Yuan, Marc-Alexandre Côté, Yonatan Bisk, Adam Trischler, and Matthew J. Hausknecht. 2021. ALFWorld: Aligning Text and Embodied Environments for Interactive Learning. In 9th International Conference on Learning Representations, ICLR 2021, Virtual Event, Austria, May 3-7, 2021. OpenReview.net. https://openreview.net/forum?id=0IOX0YcCdTn [47] Yifan Song, Weimin Xiong, Dawei Zhu, Cheng Li, Ke Wang, Ye Tian, and Sujian Li. 2023. RestGPT: Connecting Large Language Models with Real-World Applications via RESTful APIs. CoRR abs/2306.06624 (2023). arXiv:2306.06624 doi:10.48550/ ARXIV.2306. [48] Liangcai Su, Zhen Zhang, Guangyu Li, Zhuo Chen, Chenxi Wang, Maojia Song, Xinyu Wang, Kuan Li, Jialong Wu, Xuanzhong Chen, Zile Qiao, Zhongwang Zhang, Huifeng Yin, Shihao Cai, Runnan Fang, Zhengwei Tao, Wenbiao Yin, Chenxiong Qian, Yong Jiang, Pengjun Xie, Fei Huang, and Jingren Zhou. 2025. Scaling Agents via Continual Pre-training. arXiv:2509.13310 [cs.CL] https: //arxiv.org/abs/2509.13310 [49] Weiwei Sun, Miao Lu, Zhan Ling, Kang Liu, Xuesong Yao, Yiming Yang, and Jiecao Chen. 2025. Scaling Long-Horizon LLM Agent via Context-Folding. arXiv:2510.11967 [cs.CL] https://arxiv.org/abs/2510.11967 [50] Zhengwei Tao, Jialong Wu, Wenbiao Yin, Junkai Zhang, Baixuan Li, Haiyang Shen, Kuan Li, Liwen Zhang, Xinyu Wang, Yong Jiang, Pengjun Xie, Fei Huang, and Jingren Zhou. 2025. WebShaper: Agentically Data Synthesizing via InformationSeeking Formalization. CoRR abs/2507.15061 (2025). arXiv:2507.15061 doi:10. 48550/ARXIV.2507.15061 [51] Qwen Team. 2024. Qwq: Reflect deeply on the boundaries of the unknown. Hugging Face (2024). [52] Haoming Wang, Haoyang Zou, Huatong Song, Jiazhan Feng, Junjie Fang, Junting Lu, Longxiang Liu, Qinyu Luo, Shihao Liang, Shijue Huang, Wanjun Zhong, Yining Ye, Yujia Qin, Yuwen Xiong, Yuxin Song, Zhiyong Wu, Aoyan Li, Bo Li, Chen Dun, Chong Liu, Daoguang Zan, Fuxing Leng, Hanbin Wang, Hao Yu, Haobin Chen, Hongyi Guo, Jing Su, Jingjia Huang, Kai Shen, Kaiyu Shi, Lin Yan, Peiyao Zhao, Pengfei Liu, Qinghao Ye, Renjie Zheng, Shulin Xin, Wayne Xin Zhao, Wen Heng, Wenhao Huang, Wenqian Wang, Xiaobo Qin, Yi Lin, Youbin Wu, Zehui Chen, Zihao Wang, Baoquan Zhong, Xinchun Zhang, Xujing Li, Yuanfan Li, Zhongkai Zhao, Chengquan Jiang, Faming Wu, Haotian Zhou, Jinlin Pang, Li Han, Qi Liu, Qianli Ma, Siyao Liu, Songhua Cai, Wenqi Fu, Xin Liu, Yaohui Wang, Zhi Zhang, Bo Zhou, Guoliang Li, Jiajun Shi, Jiale Yang, Jie Tang, Li Li, Qihua Han, Taoran Lu, Woyu Lin, Xiaokang Tong, Xinyao Li, Yichi Zhang, Yu Miao, Zhengxuan Jiang, Zili Li, Ziyuan Zhao, Chenxin Li, Dehua Ma, Feng Lin, Ge Zhang, Haihua Yang, Hangyu Guo, Hongda Zhu, Jiaheng Liu, Junda Du, Kai Cai, Kuanye Li, Lichen Yuan, Meilan Han, Minchao Wang, Shuyue Guo, Tianhao Cheng, Xiaobo Ma, Xiaojun Xiao, Xiaolong Huang, Xinjie Chen, and Yidi Du. 2025. UI-TARS-2 Technical Report: Advancing GUI Agent with MultiTurn Reinforcement Learning. CoRR abs/2509.02544 (2025). arXiv:2509.02544 doi:10.48550/ARXIV.2509. [53] Lei Wang, Chen Ma, Xueyang Feng, Zeyu Zhang, Hao Yang, Jingsen Zhang, Zhiyuan Chen, Jiakai Tang, Xu Chen, Yankai Lin, Wayne Xin Zhao, Zhewei Wei, and Jirong Wen. 2024. survey on large language model based autonomous agents. Frontiers Comput. Sci. 18, 6 (2024), 186345. doi:10.1007/S11704-024-40231-1 [54] Lei Wang, Wanyu Xu, Yihuai Lan, Zhiqiang Hu, Yunshi Lan, Roy Ka-Wei Lee, and Ee-Peng Lim. 2023. Plan-and-Solve Prompting: Improving Zero-Shot Chain-ofThought Reasoning by Large Language Models. In Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), ACL 2023, Toronto, Canada, July 9-14, 2023, Anna Rogers, Jordan L. Boyd-Graber, and Naoaki Okazaki (Eds.). Association for Computational Linguistics, 26092634. doi:10.18653/V1/2023.ACL-LONG.147 [55] Renxi Wang, Xudong Han, Lei Ji, Shu Wang, Timothy Baldwin, and Haonan Li. 2025. ToolGen: Unified Tool Retrieval and Calling via Generation. In The Thirteenth International Conference on Learning Representations, ICLR 2025, Singapore, April 24-28, 2025. OpenReview.net. https://openreview.net/forum?id= XLMAMmowdY [56] Xingyao Wang, Yangyi Chen, Lifan Yuan, Yizhe Zhang, Yunzhu Li, Hao Peng, and Heng Ji. 2024. Executable Code Actions Elicit Better LLM Agents. In Forty-first International Conference on Machine Learning, ICML 2024, Vienna, Austria, July 21-27, 2024. OpenReview.net. https://openreview.net/forum?id=jJ9BoXAfFa [57] Zihan Wang, Kangrui Wang, Qineng Wang, Pingyue Zhang, Linjie Li, Zhengyuan Yang, Xing Jin, Kefan Yu, Minh Nhat Nguyen, Licheng Liu, Eli Gottlieb, Yiping Lu, Kyunghyun Cho, Jiajun Wu, Li Fei-Fei, Lijuan Wang, Yejin Choi, and Manling Li. 2025. RAGEN: Understanding Self-Evolution in LLM Agents via Multi-Turn Reinforcement Learning. CoRR abs/2504.20073 (2025). arXiv:2504.20073 doi:10. 48550/ARXIV.2504.20073 [58] Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Brian Ichter, Fei Xia, Ed H. Chi, Quoc V. Le, and Denny Zhou. 2022. Chain-of-Thought Prompting Elicits Reasoning in Large Language Models. In Advances in Neural Information Processing Systems 35: Annual Conference on Neural Information Processing Systems 2022, NeurIPS 2022, New Orleans, LA, USA, November 28 - December 9, 2022, Sanmi Koyejo, S. Mohamed, A. Agarwal, Danielle Belgrave, K. Cho, and A. Oh (Eds.). http://papers.nips.cc/paper_files/paper/2022/hash/ DeepAgent: General Reasoning Agent with Scalable Toolsets Conference17, July 2017, Washington, DC, USA 9d5609613524ecf4f15af0f7b31abca4-Abstract-Conference.html [59] Jialong Wu, Baixuan Li, Runnan Fang, Wenbiao Yin, Liwen Zhang, Zhengwei Tao, Dingchu Zhang, Zekun Xi, Yong Jiang, Pengjun Xie, Fei Huang, and Jingren Zhou. 2025. WebDancer: Towards Autonomous Information Seeking Agency. CoRR abs/2505.22648 (2025). arXiv:2505.22648 doi:10.48550/ARXIV.2505.22648 [60] Zhiheng Xi, Jixuan Huang, Chenyang Liao, Baodai Huang, Honglin Guo, Jiaqi Liu, Rui Zheng, Junjie Ye, Jiazheng Zhang, Wenxiang Chen, Wei He, Yiwen Ding, Guanyu Li, Zehui Chen, Zhengyin Du, Xuesong Yao, Yufei Xu, Jiecao Chen, Tao Gui, Zuxuan Wu, Qi Zhang, Xuanjing Huang, and Yu-Gang Jiang. 2025. AgentGym-RL: Training LLM Agents for Long-Horizon Decision Making through Multi-Turn Reinforcement Learning. arXiv:2509.08755 [cs.LG] https: //arxiv.org/abs/2509.08755 [61] Shitao Xiao, Zheng Liu, Peitian Zhang, Niklas Muennighoff, Defu Lian, and Jian-Yun Nie. 2024. C-Pack: Packed Resources For General Chinese Embeddings. In Proceedings of the 47th International ACM SIGIR Conference on Research and Development in Information Retrieval, SIGIR 2024, Washington DC, USA, July 14-18, 2024, Grace Hui Yang, Hongning Wang, Sam Han, Claudia Hauff, Guido Zuccon, and Yi Zhang (Eds.). ACM, 641649. doi:10.1145/3626772.3657878 [62] Yang Xiao, Mohan Jiang, Jie Sun, Keyu Li, Jifan Lin, Yumin Zhuang, Ji Zeng, Shijie Xia, Qishuo Hua, Xuefeng Li, Xiaojie Cai, Tongyu Wang, Yue Zhang, Liming Liu, Xia Wu, Jinlong Hou, Yuan Cheng, Wenjie Li, Xiang Wang, Dequan Wang, and Pengfei Liu. 2025. LIMI: Less is More for Agency. arXiv:2509.17567 [cs.AI] https://arxiv.org/abs/2509.17567 [63] Zhenghai Xue, Longtao Zheng, Qian Liu, Yingru Li, Xiaosen Zheng, Zejun Ma, and Bo An. 2025. Simpletir: End-to-end reinforcement learning for multi-turn tool-integrated reasoning. arXiv preprint arXiv:2509.02479 (2025). [64] An Yang, Anfeng Li, Baosong Yang, Beichen Zhang, Binyuan Hui, Bo Zheng, Bowen Yu, Chang Gao, Chengen Huang, Chenxu Lv, Chujie Zheng, Dayiheng Liu, Fan Zhou, Fei Huang, Feng Hu, Hao Ge, Haoran Wei, Huan Lin, Jialong Tang, Jian Yang, Jianhong Tu, Jianwei Zhang, Jian Yang, Jiaxi Yang, Jingren Zhou, Junyang Lin, Kai Dang, Keqin Bao, Kexin Yang, Le Yu, Lianghao Deng, Mei Li, Mingfeng Xue, Mingze Li, Pei Zhang, Peng Wang, Qin Zhu, Rui Men, Ruize Gao, Shixuan Liu, Shuang Luo, Tianhao Li, Tianyi Tang, Wenbiao Yin, Xingzhang Ren, Xinyu Wang, Xinyu Zhang, Xuancheng Ren, Yang Fan, Yang Su, Yichang Zhang, Yinger Zhang, Yu Wan, Yuqiong Liu, Zekun Wang, Zeyu Cui, Zhenru Zhang, Zhipeng Zhou, and Zihan Qiu. 2025. Qwen3 Technical Report. CoRR abs/2505.09388 (2025). arXiv:2505.09388 doi:10.48550/ARXIV.2505.09388 [65] Wenkai Yang, Shuming Ma, Yankai Lin, and Furu Wei. 2025. Towards ThinkingOptimal Scaling of Test-Time Compute for LLM Reasoning. CoRR abs/2502.18080 (2025). arXiv:2502.18080 doi:10.48550/ARXIV.2502.18080 [66] Shunyu Yao, Howard Chen, John Yang, and Karthik Narasimhan. 2022. WebShop: Towards Scalable Real-World Web Interaction with Grounded Language Agents. In Advances in Neural Information Processing Systems 35: Annual Conference on Neural Information Processing Systems 2022, NeurIPS 2022, New Orleans, LA, USA, November 28 - December 9, 2022, Sanmi Koyejo, S. Mohamed, A. Agarwal, Danielle Belgrave, K. Cho, and A. Oh (Eds.). http://papers.nips.cc/paper_files/paper/2022/ hash/82ad13ec01f9fe44c01cb91814fd7b8c-Abstract-Conference.html [67] Shunyu Yao, Jeffrey Zhao, Dian Yu, Nan Du, Izhak Shafran, Karthik Narasimhan, and Yuan Cao. 2022. React: Synergizing reasoning and acting in language models. arXiv preprint arXiv:2210.03629 (2022). [68] Junjie Ye, Zhengyin Du, Xuesong Yao, Weijian Lin, Yufei Xu, Zehui Chen, Zaiyuan Wang, Sining Zhu, Zhiheng Xi, Siyu Yuan, Tao Gui, Qi Zhang, Xuanjing Huang, and Jiecao Chen. 2025. ToolHop: Query-Driven Benchmark for Evaluating Large Language Models in Multi-Hop Tool Use. In Proceedings of the 63rd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), ACL 2025, Vienna, Austria, July 27 - August 1, 2025, Wanxiang Che, Joyce Nabende, Ekaterina Shutova, and Mohammad Taher Pilehvar (Eds.). Association for Computational Linguistics, 29953021. https://aclanthology.org/2025.acl-long.150/ [69] Yixin Ye, Zhen Huang, Yang Xiao, Ethan Chern, Shijie Xia, and Pengfei Liu. 2025. LIMO: Less is More for Reasoning. CoRR abs/2502.03387 (2025). arXiv:2502.03387 doi:10.48550/ARXIV.2502.03387 [70] Qiying Yu, Zheng Zhang, Ruofei Zhu, Yufeng Yuan, Xiaochen Zuo, Yu Yue, Weinan Dai, Tiantian Fan, Gaohong Liu, Lingjun Liu, et al. 2025. Dapo: An opensource llm reinforcement learning system at scale. arXiv preprint arXiv:2503.14476 (2025). [71] Aohan Zeng, Mingdao Liu, Rui Lu, Bowen Wang, Xiao Liu, Yuxiao Dong, and Jie Tang. 2024. AgentTuning: Enabling Generalized Agent Abilities for LLMs. In Findings of the Association for Computational Linguistics, ACL 2024, Bangkok, Thailand and virtual meeting, August 11-16, 2024, Lun-Wei Ku, Andre Martins, and Vivek Srikumar (Eds.). Association for Computational Linguistics, 30533077. doi:10.18653/V1/2024.FINDINGS-ACL.181 [72] Yuyao Zhang, Zhicheng Dou, Xiaoxi Li, Jiajie Jin, Yongkang Wu, Zhonghua Li, Ye Qi, and Ji-Rong Wen. 2025. Neuro-Symbolic Query Compiler. In Findings of the Association for Computational Linguistics, ACL 2025, Vienna, Austria, July 27 - August 1, 2025, Wanxiang Che, Joyce Nabende, Ekaterina Shutova, and Mohammad Taher Pilehvar (Eds.). Association for Computational Linguistics, 1213812155. https://aclanthology.org/2025.findings-acl.628/ [73] Chujie Zheng, Shixuan Liu, Mingze Li, Xiong-Hui Chen, Bowen Yu, Chang Gao, Kai Dang, Yuqiong Liu, Rui Men, An Yang, Jingren Zhou, and Junyang Lin. 2025. Group Sequence Policy Optimization. CoRR abs/2507.18071 (2025). arXiv:2507.18071 doi:10.48550/ARXIV.2507.18071 [74] Yuxiang Zheng, Dayuan Fu, Xiangkun Hu, Xiaojie Cai, Lyumanshan Ye, Pengrui Lu, and Pengfei Liu. 2025. DeepResearcher: Scaling Deep Research via Reinforcement Learning in Real-world Environments. arXiv preprint arXiv:2504.03160 (2025). Conference17, July 2017, Washington, DC, USA Xiaoxi Li et al. Appendix Datasets A.1 Training Data We collected diverse training dataset spanning four task categories to instill comprehensive agent capabilities. General Tool-Use: We sample 1k instances for labeled-tool scenarios and 1k for tool-retrieval from the ToolBench [37] training set. This data is intended to instill generalized ability to use diverse tools and leverage large toolsets through retrieval. Real-World Interaction: We utilize 500 instances from ALFWorld [46] and 500 from WebShop [66], sampled from their training sets, to teach the model to interact effectively with environments, manage state transitions, and achieve user goals. Deep Research: We include 200 instances from WebDancer [59] and 500 from WebShaperQA [50] to enhance the models proficiency in using web search and page browsing for in-depth information gathering. Mathematical Reasoning: We collect 0.9k problems from the DeepMath dataset [12] to strengthen the models ability to use code as tool for complex mathematical computations. A.2 Benchmarks We conduct extensive experiments on wide range of benchmarks, including general tool-use and downstream applications. General Tool-Use. These benchmarks encompass broad range of distinct tools (from tens to over ten thousand), thus offering testbed for evaluating different approaches to toolset scaling. ToolBench [37]: large-scale benchmark containing over 16,000 real-world REST APIs spanning 49 categories. Test subsets include 100 test cases, designed to evaluate LLMs in both single-tool and complex multi-tool scenarios. API-Bank [24]: comprehensive benchmark for tool-augmented LLMs. It features runnable evaluation system with 73 API tools and large training set (over 2,200 dialogues across 2,211 APIs from 1,008 domains), assessing LLMs capabilities in planning, retrieving, and calling APIs. TMDB [47]: sub-scenario of RestBench focused on the TMDB movie database, consisting of 100 questions that utilize 54 local tools and require an average of 2.3 sequential API calls. Spotify [47]: sub-scenario of RestBench simulating Spotify music player, featuring 57 questions and 40 local tools, demanding an average of 2.6 sequential API calls to complete the tasks. ToolHop [68]: multi-hop reasoning dataset comprising 995 complex questions. It leverages 3,912 locally executable tools and requires between 3 to 7 sequential tool calls per task. Downstream Applications. These benchmarks test the capability of different approaches in handling complex real-world tasks, which often require the use of domain-specific toolsets. ALFWorld [46]: benchmark for simple Embodied AI tasks set in text environment. Agents must complete objectives using finite set of low-level embodied actions (eg., move, take) to test navigation and object manipulation. WebShop [66]: challenging online shopping environment that provides 12,087 crowd-sourced tasks over catalog of 1.18 million products. Agents interact with the simulated e-commerce website using core APIs: search[Query] and choose[Text Button]. GAIA [32]: complex benchmark for General AI Assistants, consisting of 466 real-world questions (with 300-question heldout test set). It requires the flexible application of broad generalpurpose toolset including web browsing, code execution, multimodal processing, and file handling. Humanitys Last Exam (HLE) [35]: benchmark featuring 2,500 highly difficult, multi-disciplinary questions (graduatelevel). It primarily evaluates the models intrinsic deep reasoning and multi-modal understanding capabilities, as the questions are designed to be insoluble by simple external search tools. Baselines We compare our proposed method with several baseline agents. The details of these baselines are introduced as follows: ReAct (Reasoning and Acting) [67]: ReAct is general paradigm that combines reasoning and acting with language models. It prompts the model to generate sequence of interleaved thought, action, and observation steps to solve given task. CodeAct [56]: This is framework where the agents actions are expressed as Python code, which are then executed in an interpreter. By using code as the action space, the agent can interact with wide variety of tools, APIs, and system functionalities. Plan-and-Solve [54]: This method follows two-stage process to tackle complex problems. First, the model devises detailed, step-by-step plan to solve the problem without using any tools. Then, it executes the plan, carrying out the necessary calculations or actions as outlined. Reflexion [44]: Reflexion is an approach that enhances agent learning through verbal self-reflection. After failed attempt, the agent reflects on what went wrong and records this reflection in its memory. AgentLM [71]: An instruction tuning method designed to enhance the general agent capabilities of LLMs. It uses lightweight, specially curated dataset called AgentInstruct to fine-tune LLMs. WebThinker [26]: WebThinker is deep research agent designed for complex information-seeking tasks. It autonomously explores the web by interleaving \"think-search-write\" process to gather and synthesize information. HiRA (Hierarchical Reasoning Agent) [20]: HiRA employs hierarchical agent architecture to solve complex, multi-modal tasks. It decouples high-level planning from low-level execution by using planner to decompose the task and executor agents to perform specific actions like searching or coding. OpenAI Deep Research [34]: This is feature within ChatGPT that performs in-depth research on complex topics by exploring wide range of web sources. It takes more time than standard query to synthesize information and generate detailed, structured response. Implementation Details For DeepAgent, we use QwQ-32B [51] as the main reasoning model for the results in Table 1 and Table 2, and Qwen3-30B-A3B-Thinking2507 [64] with Qwen3-235B-A22B-Thinking-2507 [64] in Table 5. We use Qwen2.5-32B-Instruct [40] as the auxiliary model for (1) filtering lengthy tool search results and execution outputs (this DeepAgent: General Reasoning Agent with Scalable Toolsets Conference17, July 2017, Washington, DC, USA Table 6: case study of DeepAgent for multi-step, multi-tool tasks on ToolBench, showing only key actions. Example of DeepAgent on Multi-Tool Task User Query: Im organizing film festival and need assistance in finding the best films. Can you search for videos related to documentary on Vimeo? Additionally, fetch the related people in the cinema category to invite them as guest speakers. Finally, provide me with streaming link for YouTube video with the ID UxxajLWwzqY. DeepAgent Processing Output Okay, need to solve three tasks: find documentaries on Vimeo, find cinema-related people on Vimeo, and get YouTube streaming link. Ill start by searching for the right tools. <tool_search> Vimeo video search for documentaries </tool_search> The search returned several tools. search_videos and get_videos_by_tag seem most relevant. Ill use the search tool first to find documentaries. <tool_call> {\"name\": \"search_videos\", \"arguments\": {\"query\": \"documentary\", \"sort\": \"most_liked\"}} </tool_call> The tool returned list of popular documentary videos. Now for the second task: finding people in the cinema category. Since there is no direct tool to search for people, will try searching for videos with the cinema tag and identify their creators. <tool_call> {\"name\": \"get_videos_by_tag\", \"arguments\": {\"tag\": \"cinema\", \"sort\": \"most_liked\"}} </tool_call> This call returned videos tagged with cinema and included user information for each video, such as \"John Doe\" and \"Jane Smith\", who are potential guest speakers. Finally, for the YouTube streaming link, Ill first verify the video ID exists using the YouTube search tool. <tool_call> {\"name\": \"search_for_youtube_v3\", \"arguments\": {\"part\": \"snippet\", \"type\": \"video\", \"q\": \"UxxajLWwzqY\"}} </tool_call> The tool confirmed the video ID is valid. can now construct the streaming link. will now consolidate all the information to answer the users query. Final Answer: 1. Documentaries on Vimeo: Found nature and space documentaries. 2. Cinema Guest Speakers: John Doe, Jane Smith with Vimeo profiles. 3. YouTube Link: https://www.youtube.com/watch?v=UxxajLWwzqY is also applied to all baselines), (2) simulating RapidAPIs during ToolPO training, and (3) generating folded memory from interaction history. For the baselines, we use either QwQ-32B or Qwen2.5-32BInstruct as the backbone model. Text generation for all models uses maximum of 81,920 tokens, with temperature of 0.7, top_p of 0.8, top_k of 20, and repetition penalty of 1.05. The maximum number of actions is set to 50. Web search and page browsing are implemented using the Google Serper API and Jina Reader API, respectively. The VQA tool is based on Qwen2.5-VL-32B-Instruct [1], which takes question and an image as input and outputs model-generated response. Tool retrieval is performed using bge-large-en-v1.5 [61]. All tool documentation follows the standard OpenAI function definition format: {\"name\": \"...\", \"description\": \"...\", \"parameters\": {\"type\": \"object\", \"properties\": {\"param1\": {\"type\": \"...\", \"description\": \"...\"}, ..., \"required\": [\"param1\"]}}. This format is used for building the toolset index and for all prompts given to the agents. Training consists of 100 steps of ToolPO with batch size of 64, 𝜆1 = 𝜆2 = 1, rollout size 𝐾 = 8, and maximum sequence length of 32,768. The maximum number of actions is 50. The training framework is based on VeRL [42] for multi-node distributed training. All experiments are conducted on 64 NVIDIA H20-141GB GPUs. Memory Schema Our brain-inspired memory architecture consists of three components: episodic, working, and tool memory. To ensure stable memory folding and prevent information loss, each component is defined by specific JSON schema. This structured format enables the agent to reliably parse and utilize the compressed memory, facilitating robust long-term reasoning. Episodic Memory Schema. Episodic memory provides highlevel summary of the agents task progression, major milestones, decisions, and outcomes. This allows the agent to maintain longterm context and reflect on its overall strategy. The format is: {\"task_description\": \"A general summary of what the reasoning history has been doing and the overall goals it has been striving for.\", \"key_events\": [{\"step\": \"step number\", \"description\": \"A detailed description of the specific action taken, decision made, or milestone achieved at this step, including relevant context and reasoning behind the choice.\", \"outcome\": \"A detailed account of the direct result, observation, or feedback received from this action or decision, including any new information gained or changes in the task state.\"}], \"current_progress\": \"A general summary of the current progress of the task, including what has been completed and what is left to be done.\"} Working Memory Schema. Working memory functions as the agents short-term buffer, holding information relevant to its immediate context. It focuses on the current sub-goal, active challenges, and planned next steps, ensuring continuity of reasoning across memory folds. The format is: {\"immediate_goal\": \"A clear summary of the current subgoalwhat you are actively working toward at this moment.\", \"current_challenges\": \"A concise summary of the main obstacles or difficulties you are presently encountering.\", \"next_actions\": [{type\": \"tool_call or planning ordecision\", description\": \"Anticipate and describe the next concrete action you intend to take to advance the task.\"}]} Tool Memory Schema. Tool memory consolidates the agents experiences with various tools. It tracks usage patterns, success rates, effective parameter combinations, and common errors. This structured knowledge enables the agent to learn from its interactions and refine its tool-use strategies over time. The format is: {\"tools_used\": [{\"tool_name\": \"string\", \"success_rate\": \"float\", \"effective_parameters\": [\"param1\", \"param2\"], \"common_errors\": [\"error_type1\", \"error_type2\"], \"response_pattern\": \"description of typical output\", \"experience\": \"Reflect and summarize your experience, including both successes and failures.\"}], \"derived_rules\": [\"When condition occurs, prefer tool Y\", \"Tool works best with parameter set to B\"]} Case Study To illustrate the effectiveness of our DeepAgent framework in handling complex, multi-step tasks that require coordinated use of multiple tools, we present detailed case in Table 6. This example demonstrates how DeepAgent autonomously navigates tool selection, executes sequential actions, and synthesizes results to provide comprehensive solutions to user queries."
        }
    ],
    "affiliations": [
        "Renmin University of China",
        "Xiaohongshu Inc."
    ]
}