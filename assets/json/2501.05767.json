{
    "paper_title": "Migician: Revealing the Magic of Free-Form Multi-Image Grounding in Multimodal Large Language Models",
    "authors": [
        "You Li",
        "Heyu Huang",
        "Chi Chen",
        "Kaiyu Huang",
        "Chao Huang",
        "Zonghao Guo",
        "Zhiyuan Liu",
        "Jinan Xu",
        "Yuhua Li",
        "Ruixuan Li",
        "Maosong Sun"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "The recent advancement of Multimodal Large Language Models (MLLMs) has significantly improved their fine-grained perception of single images and general comprehension across multiple images. However, existing MLLMs still face challenges in achieving precise grounding in complex multi-image scenarios. To address this, we first explore a Chain-of-Thought (CoT) framework that integrates single-image grounding with multi-image comprehension. While partially effective, it remains unstable and struggles to capture abstract visual information due to its non-end-to-end nature. Therefore, we introduce Migician, the first multi-image grounding model capable of performing free-form and accurate grounding across multiple images. To support this, we present the MGrounding-630k dataset, which comprises data for several multi-image grounding tasks derived from existing datasets, along with newly generated free-form grounding instruction-following data. Furthermore, we propose MIG-Bench, a comprehensive benchmark specifically designed for evaluating multi-image grounding capabilities. Experimental results demonstrate that our model achieves significantly superior multi-image grounding capabilities, outperforming the best existing MLLMs by 21.61% and even surpassing much larger 70B models. Our code, model, dataset, and benchmark are fully open-sourced."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 0 1 ] . [ 1 7 6 7 5 0 . 1 0 5 2 : r Migician: Revealing the Magic of Free-Form Multi-Image Grounding in"
        },
        {
            "title": "Multimodal Large Language Models",
            "content": "You Li1*, Heyu Huang2*, Chi Chen3, Kaiyu Huang1, Chao Huang1, Zonghao Guo3, Zhiyuan Liu3, Jinan Xu1, Yuhua Li2, Ruixuan Li2, Maosong Sun3 1 State Key Laboratory of Advanced Rail Autonomous Operation, Beijing Jiaotong University, Beijing, China 2 Huazhong University of Science and Technology, Wuhan, China 3 Tsinghua University, Beijing, China Figure 1. Left: Examples of free-form multi-image grounding. The task is to identify and localize relevant visual regions across multiple images based on free-form query. Right: Our proposed model, Migician, significantly outperforms other MLLMs on various multi-image grounding tasks."
        },
        {
            "title": "Abstract",
            "content": "The recent advancement of Multimodal Large Language Models (MLLMs) has significantly improved their finegrained perception of single images and general comprehension across multiple images. However, existing MLLMs still face challenges in achieving precise grounding in complex multi-image scenarios. To address this, we first explore Chain-of-Thought (CoT) framework that integrates singleimage grounding with multi-image comprehension. While partially effective, it remains unstable and struggles to capture abstract visual information due to its non-end-to-end nature. Therefore, we introduce Migician, the first multiimage grounding model capable of performing free-form and accurate grounding across multiple images. To support this, we present the MGrounding-630k dataset, which comprises data for several multi-image grounding tasks derived from existing datasets, along with newly generated free- *Equal contribution. Corresponding authors: Chi Chen (chenchithu@gmail.com) and Kaiyu Huang (kyhuang@bjtu.edu.cn). form grounding instruction-following data. Furthermore, we propose MIG-Bench, comprehensive benchmark specifically designed for evaluating multi-image grounding capabilities. Experimental results demonstrate that our model achieves significantly superior multi-image grounding capabilities, outperforming the best existing MLLMs by 21.61% and even surpassing much larger 70B models. Our code, model, dataset, and benchmark are fully open-sourced at https://migician.github.io/. 1. Introduction Multimodal Large Language Models (MLLMs) have exhibited significant advancements recently, demonstrating exceptional cross-modal understanding capabilities and achieving outstanding performance in various vision-language tasks [1, 7, 12, 15, 20, 28, 46]. As these models continue to evolve, their capabilities have expanded beyond imagelevel understanding to include fine-grained visual grounding [5, 37, 48]. This enables MLLMs to process region1 specific inputs and outputs, unlocking broader spectrum of real-world multimodal application scenarios [33]. and reveal the potential and challenges of current MLLMs by through proposed CoT framework. Despite the promising visual grounding capabilities demonstrated by existing MLLMs, these abilities are largely confined to single-image scenarios [19, 48]. The potential of MLLMs in free-form multi-image grounding (MIG) remains underexplored. Free-form MIG challenges the model to perform grounding across multiple images effectively, where the input queries and image contexts can be organized in arbitrary forms, enabling flexible and dynamic interactions. For instance, as shown in Figure 1, the model must understand the white car in the query image and relate it to the textual prompt \"black in color\" to identify the corresponding target in the target image. This capability unlocks wide range of applications, such as fine-grained environmental perception in autonomous driving [39], anomaly detection in surveillance systems [2], and target localization for embodied robotics [11]. To address the free-form MIG, the model needs to possess the capability for visual grounding while achieving cross-image understanding. As result, question naturally arises: Can we integrated the single-image grounding and multi-image understanding capabilities of existing MLLMs to tackle the MIG task? In this work, we propose Chain-of-Thought (CoT) framework that first leverages multi-image understanding to generate textual referring query, and then utilizes it for localization through single-image grounding. This approach is proven highly effective for MIG tasks, particularly in simple scenarios where textual descriptions are sufficiently distinctive, revealing the potential of MLLMs in handling such tasks. However, the proposed CoT framework struggles with describing abstract visual semantics in multi-image scenarios, and the two-step process results in doubling of the inference time. To address this, we further propose Migician, competitive MLLM capable of free-form and accurate grounding across multiple images, which is an end-to-end solution for MIG. To progressively establish flexible grounding capabilities, we employ two-stage training procedure based on our proposed large-scale MIG dataset (MGrounding630k). First, the grounding ability of Migician is enhanced through combination of data of MIG tasks and general tasks. Then, Migician is further refined using high-quality free-form MIG instruction data. In addition, to evaluate the challenges of the free-form MIG scenario, we construct comprehensive multi-image grounding benchmark, MIGbench, comprising total of 10 different tasks, 5.9k diverse images and more than 4.2k test instances. We observe significant gap between the performance of existing mainstream MLLMs and human performance on the MIG-bench. In contrast, Migician can effectively alleviate this gap and improve the performance of free-form MIG. To sum up, our contributions can be concluded as follows: We explore the task of multi-image grounding for MLLMs We introduce Migician, the first MLLM capable of effectively performing free-form MIG. We also present MGrounding-630k, the first large-scale MIG instruction tuning dataset for training this model. We introduce MIG-Bench, comprehensive benchmark for evaluating multi-image grounding capabilities. Experimental results demonstrate that Migician significantly outperforms the current best methods. 2. Related Work Multimodal Large Language Models Recent developments in multimodal large language models (MLLMs) have shifted from single image-text understanding towards more versatile capabilities [3, 23, 38, 45]. Among these efforts, some focus on enabling models to achieve fine-grained visual grounding, either through simple instruction tuning [5, 33] or by integrating additional auxiliary visual components [4, 48, 51]. However, these models primarily focus on visual grounding within single image. Some other studies explore multi-image understanding tasks, such as multi-image comparison, reasoning, and temporal comprehension [3, 17, 23, 24, 45, 47]. Nevertheless, fine-grained visual grounding at the multi-image level remains an underexplored area. To the best of our knowledge, our proposed Migician is the first MLLM designed to address the challenge of multi-image grounding. MLLM Benchmarks Most existing benchmarks for evaluating MLLMs focus on single-image tasks [9, 22]. few recent benchmarks have started assessing the performance of MLLMs on multi-image understanding [10, 17, 27, 29, 36], but they primarily emphasize image-level comprehension. The most relevant benchmark to our work is MC-Bench [42], contemporaneous study. MC-Bench evaluates the multicontext grounding capabilities of MLLMs by asking them to accurately locate the corresponding object based on text prompt in the correct image from given pair. However, it exhibits limitations in the fixed number of input images and the restricted forms of queries. In contrast, the proposed MIG-Bench in this work offers more flexible task formats, focusing on evaluating models capabilities in free-form multi-image understanding. 3. Task Definition The task of free-form multi-image grounding is to identify and localize relevant visual regions across set of images based on free-form query. Unlike traditional grounding tasks with fixed input formats, the query in free-form multiimage grounding can be an arbitrary combination of text and images, making it highly flexible and versatile. For2 Figure 2. An illustration of the multi-image grounding tasks included in MIG-Bench. These tasks are divided into two categories: spontaneous grounding and referential grounding, depending on the whether there are explicit referential requirements. mally, let the query consist of natural language description, reference images {R1, R2, . . . , Rk} or hybrid combination of both (e.g., [a white car image] find car like this image except it is black). Given set of target images {I1, I2, . . . , In}, the task is to identify set of visual regions {G1, G2, . . . , Gm} where Gi is the target region within an image Ij that satisfies the semantic and contextual constraints defined by Q. As shown in Figure 2, based on whether the task involves explicit reference requirements, multi-image grounding tasks can be further categorized into two types: Spontaneous Grounding and Referential Grounding. Spontaneous Grounding refers to recognizing and grounding the target object in corresponding images without explicitly pointing it out. Unlike the conventional Reference Expression Comprehension task [19] that explicitly refer to the target object, Spontaneous Grounding typically utilizes the relationships between multiple images as contextual cues to autonomously identify and localize the objects to be grounded (e.g., finding and locating differences between images). Referential Grounding, on the other hand, requires an explicit reference to the target object. As mentioned earlier, such references can take the form of arbitrary combinations of images and textual descriptions. 4. Methods In this section, we delve into the methods for enabling freeform multi-image grounding capabilities in MLLMs. Since free-form MIG requires the ability to perform visual grounding while simultaneously understanding multiple images, we begin by investigating Chain-of-Thought (CoT) framework 3 Figure 3. Illustration of the CoT framework and its failure case. Different from (a) direct inference, the (b) CoT method decomposes the task into two subtasks, solving each task deploying the models existing capabilities. failure case of CoT is shown in (c) where the model struggles at handling abstract visual information. Green and red background colors indicate correct and incorrect answers, respectively. Figure 4. Statistics of the MGrounding-630k dataset and MIG-Bench. to combine these two capabilities within existing MLLMs to tackle this task. Furthermore, we develop an end-to-end MIG model, Migician, through instruction tuning to overcome the limitations of the CoT framework and achieve enhanced MIG performance. 4.1. Chain-of-Thought Framework Although some existing MLLMs such as Qwen2-VL-7B [38] demonstrate strong multi-image understanding and singleimage grounding capabilities, we find that directly prompting them to perform MIG tasks often leads to significant performance degradation as illustrated in Figure 3(a). To better explore the potential of existing models for MIG tasks, we design Chain-of-Thought (CoT) framework that enables the model to effectively leverage and combine its exitsing abilities during the MIG execution. Specifically, we decompose the MIG task into two subtasks as illustrated in Figure 3(b). The model is first prompted to engage in \"reasoning process\" by performing multi-image understanding based on the input images and the given prompt, generating textual referring expression that describes the target object. Next, the model performs the visual grounding task, using the referring expression from the previous step to locate the objects in corresponding images. This framework leads to notable performance improvement on MIG tasks, indicating that existing MLLMs possess the underlying capabilities required for such tasks but need an effective method to elicit them. However, the CoT framework suffers from several inherent limitations. On one hand, the multi-step process introduces error propagation issues [44] and impacts reasoning efficiency. On the other hand, many scenarios require grounding through abstract visual semantics across multiimage contexts (as shown in Figure 3(c)), making the use 4 Figure 5. Above are the four representative failure patterns of the single-image CoT. From left to right, top to bottom, they are (a) special multi-image format, (b) abstract visual information, (c) CoT error propagation, (d) step-2 inference error. of an intermediate textual referring expression impractical. This highlights the need for an end-to-end model capable of directly performing the MIG task. More failure patterns of the CoT framework are illustrated in Figure 5, categorized into perceptual and reasoning flaws. For the former, the framework falls short when multiple images are organized in manner where only integrating all their visual information could tackle MIG (i.e. finding the location of missing people in the second image), or when the textual content could not sufficiently represent the visual information. Regarding reasoning errors, inaccuracies can arise at various stages of the reasoning process, undermining the frameworks overall accuracy and effectiveness. 4.2. Data Construction The CoT framework has demonstrated that an MLLM with both multi-image understanding and single-image grounding capabilities inherently holds strong potential for free-form MIG. In the following section, we employ instruction tuning to explicitly bridge these capabilities in existing MLLMs to achieve MIG. For this purpose, we first construct an instruction tuning dataset for MIG, named MGrounding-630k, with its statistics presented in Figure 4. This dataset is primarily constructed through the following two ways. Transforming Existing Data. By analyzing the tasks and annotation types of existing datasets, we identify multiple multi-image grounding (MIG) tasks whose data could be derived through transformation of the existing. Specifically, we collect and organize data from existing sources, combining or automatically synthesizing single-image annotations to create datasets for 6 types of MIG tasks. Each task contains over 70k examples, resulting in total of 530k training samples. The details of these task data are in Appendix C.1. Synthesizing Free-form MIG Data. The data obtained through the aforementioned methods still do not fully meet the requirements for free-form MIG. To acquire MIG data with richer and more diverse formats, which would enhancing the models instruction-following and flexible grounding capabilities, we design MIG data synthesis pipeline. This pipeline uses the Objects365 [35] images with object annotations, select multiple images as group, and generate high-quality instructions for multi-image grounding. Specifically, we first employ Qwen2-VL-72B [38] to generate captions of each individual image and then perform error filtering and refinement on the annotated bounding boxes. Next, we prompt Qwen2.5-72B [43] to automatically generate high-quality, free-form MIG question-answering pairs by integrating information from multiple images. To optimize the selection of appropriate image groups, we adopt different image grouping methods, including random selection, selection of images with common objects, and grouping images based on CLIP similarity to select semantically similar images for each. Using these methods, we generate total of 100k Free-Form MIG data. For more detailed information please refer to Appendix C.2. 5 Spontaneous Grounding Referential Grounding Models Difference Similarity Visual Reference Textual Visual+Textual AVE Static Robust Common OT MV Region Refer GG Reason Co-Re Human Performance Human 99.50* 97.87 98.00* 100.00 96.88 100.00* 98.99 91.06* 92. 97.44 97.18 LLaVA-OV-72B InternVL2-76B Qwen2-VL-72B Mantis LLaVA-OV-7B Minicpm2.6 mPLUG-Owl3 InternVL2-8B Qwen2-VL-7B mPLUG-Owl3+CoT InternVL2-8B+CoT Qwen2-VL-7B+CoT Migician 13.26 15.91 46. 1.52 6.06 14.58 18.56 6.92 27.84 16.29 14.58 23.48 65.15 5.34 10.64 46.81 0.00 3.19 2.13 6.38 7.45 38.30 8.51 7.45 40.43 46.81 70B-Scale MLLMs 12.91 30.73 26.73 7.64 20.83 22.57 7B-Scale MLLMs 12.18 0.18 9.82 8.55 20.73 20.73 44.36 40.91 62.73 2.08 1.04 6.25 7.64 9.72 11.81 25.35 27.78 42.71 70. 60.07 2.14 5.74 18.62 1.00 1.08 1.75 2.41 3.49 25.95 19.04 28.60 24.85 74.31 26.84 36.40 64.46 3.31 3.43 14.34 34.93 25.49 19.36 55.39 72.54 63. 84.19 17.83 46.46 33.33 1.01 9.09 11.11 7.07 28.28 23.23 36.36 67.68 54.55 76.77 21.60 41.28 62.53 10.02 15.43 10.02 22.85 30.26 58.52 30.86 44.49 43. 66.53 11.88 32.67 50.50 0.00 6.93 2.97 9.09 17.82 48.51 18.81 41.58 51.49 59.41 8.55 26.50 17.09 13.65 26.72 38. 0.85 0.85 2.56 5.98 9.40 11.97 10.26 11.97 30.77 3.20 4.73 7.55 12.35 15.96 28.62 26.52 35.76 43.82 34.19 63.82 Table 1. Performance comparison of different models on MIG-Bench. OT, MV, GG and Co-Re respectively means object tracking, multi-view grounding, group grounding and correspondence. For values marked with *, we randomly sample 20% testing examples for human evaluation on the corresponding task. 4.3. Instruction Tuning for MIG Using the constructed dataset, we perform instruction tuning based on Qwen2-VL-7B [38] to develop Migician, enabling it to achieve end-to-end free-form MIG capabilities. Two-Stage Training. To effectively equip the model with free-form MIG capabilities, we propose two-stage training approach. In the first stage, the model learns to perform multi-image grounding by training on the six representative MIG tasks of MGrounding-630k, acquiring the ability to simultaneously comprehend multiple images and execute visual grounding. In the second stage, the model is further fine-tuned on free-form MIG instruction data in MGrounding-630k, enabling it to adapt to more flexible and diverse instruction types and transfer the MIG skills learned in the first stage to broader range of scenarios. To prevent the model from forgetting its existing capabilities during training, we also incorporate single-image understanding, multi-image understanding, and single-image grounding data into each training stage. More details are in Appendix D. Model Merging. After the second stage of fine-tuning, we observe trade-off between model performance and flexibility: while the model adapts to the free-form MIG instructions, there is performance drop in common multiimage grounding tasks. To better balance these two aspects, we adopt the model merging technique [14], averaging the model weights obtained from stage-2 with different training settings as the final weights. We find this approach mitigates the performance loss in common MIG tasks while preserving the ability to follow free-form MIG instructions effectively. 5. MIG-Bench To thoroughly assess the multi-image grounding abilities of current MLLMs, we have meticulously curated MIG-Bench. This benchmark consists of 5.9k images and 4.3k testing instances, covering 10 tasks. The distribution of these tasks is illustrated in Figure 4. The benchmark tasks are divided into two categories: spontaneous and referential multi-image grounding. Spontaneous grounding tasks require the model to recognize and ground differences or common objects across images, with total of 1.4k testing instances. Referential grounding tasks, on the other hand, require the model to utilize different forms of reference queries (i.e., textual, visual, or multimodal) to locate the target objects, comprising 6 distinctive tasks and 2.9k testing instances. The details of these tasks are provided in Figure 2 and Appendix A. To ensure diversity, the images are sourced from variety of sources, existing datasets, web images and manually captured photos. For existing datasets, we use examples that exhibits significant movement from GOT-10k_val [13] for 6 Model MuirBench BLINK val MIBench Mantis_eval MMIU AVE V* Bench Attribute Spatial Overall GPT-4o Gemini-Pro LLaVA-1.5 CogVLM Idefics2-8B mPLUG-Owl3 InternVL2-8B Mantis LLaVA-OV-7B Minicpm2.6 Qwen2-VL-7B Migician 62.31 49.35 23.46 20.85 26.08 39.67 48.70 44.50 41.80 42.65 42. 53.69 Closed-Source Model 60.04 45.16 71.88 Open-Source Model 37.13 41.54 50.30 50.57 49.05 48.20 51.45 52. 51.53 26.83 46.39 56.66 52.91 45.09 71.29 71.09 68.06 71.42 62.67 31.34 45.16 48.85 63.10 60.37 57.14 64.20 69.12 70.97 69. 55.7 53.4 62.52 49.30 19.20 23.57 27.80 21.72 42.00 45.60 44.46 50.19 54.36 27.59 32.78 37.28 46.29 50.05 48.28 53.99 56.90 57.56 60.32 61. Human Level Random Guess 98.26 26.73 100.00 50.00 98.95 35.99 Tool-using Pipeline MM-React Visprog SEAL 34.78 31.30 74.78 51.31 56.57 76.31 End-to-end MLLMs InternVL2-8B Gemini Pro LLaVA-1.5 Minicpm2.6 GPT-4V Migicianzero_shot Migicianslice 29.56 40.86 43.47 40.86 51. 59.16 77.49 56.57 59.21 56.57 64.47 60.53 60.53 67.11 41.36 41.36 75.39 43.07 48.16 48.68 52.67 54.97 59.85 72. Table 2. Performance comparison on various multi-image understanding benchmarks. The highest score is highlighted in bold and the second highest score is underlined for all open-source models. Table 3. On V* Bench, Migician generalizes well to the hyper-resolution single image in zero-shot manner. the Object Tracking task, and manually modify the images from Objects365 [35] for Common Object task. For Multiview Grounding, we collect 288 examples spanning both indoor and outdoor scenes from Ego4D [11]. The Static Difference task is sourced from the MagicBrush_dev set [50]. We also manually capture 97 image pairs with view differences in real-world settings and collect, on average, over 100 image pairs from Google Search for tasks related to Visual Referring, Reasoning, and Referring Grounding tasks. For quantitative and objective evaluation of different MLLMs, we ensure that each testing instance contains only one clear target region, eliminating any potential ambiguity. Our MIG-Bench offers comprehensive evaluation across various real-world scenarios and domains. We believe this benchmark will provide valuable insights into the challenges of MIG and inspire further research in related areas. 6. Experiments 6.1. Implementation Details Migician undergoes development based on the Qwen2-VL7B [38] foundation model with global batch size of 48, total of 25,000 steps for the two-stage training procedure, and learning rate of 5e-6, using 8A100-80G GPUs. For the evaluation in our proposed MIG-Bench, we use the conventional metric Acc0.5 in referring expression comprehension [19]. This metric measures the accuracy of object localization, defining prediction as correct if the Intersection over Union (IoU) with the ground truth bounding box is greater than 0.5. 6.2. Results on MIG-Bench As shown in Table 1, Migican achieves the state-of-the-art performance across all tasks on MIG-bench, with an average improvement of 21.61% compared to the second-best model, Qwen2-VL-72B (38.88%), despite having significantly fewer parameters. Note that there is substantial gap between human performance and that of all MLLMs across all tasks, indicating that MLLMs have significant potential for improvement in freeform MIG. In particular, for 7B-scale models, even advanced multi-image models like InternVL2-8B and Qwen2-VL-7B struggle to perform, particularly in tasks such as multi-view grounding, region locating, and correspondence. For models equipped with preliminary grounding capabilities, such as mPLUG-Owl3, InternVL2 series, and Qwen2VL series, their inherent localization ability provides an implicit advantage over other baselines. Furthermore, the proposed single-image CoT method (+CoT) effectively integrates the grounding and multi-image understanding capabilities of the MLLMs where different abilities assist each other in different reasoning steps, achieving comprehensive improvements on multi-image grounding tasks. Moreover, this approach is effective for all the aforementioned models. 6.3. Results on Multi-Image Understanding Benchmarks As shown in Table 2, Migician not only establishes its multiimage grounding ability, but also remarkably stimulates its general multi-image understanding ability. In particular, Migician achieves the best average results on the multi-image It surpasses the second-best understanding benchmarks. model (Mantis) on MuirBench by 9.77%, achieving SOTA performance on MMIU and shows 1.40% improvement on the large-scale MIBench. We attribute this to the training on mixture of multi-image understanding and grounding data, which indicates that our proposed MGrounding-630k can enhance general multi-image comprehension. 7 Model RefCOCO RefCOCO+ RefCOCOg val testA testB val testA testB val test VisionLLM v2 [40] Shikra [5] InternVL2-8B [3] GroundingGPT [25] Griffon v2 [49] InternVL2-8B [3] Qwen2-VL-7B [38] 79.20 87.00 87.10 88.02 89.6 87.10 91.70 82.30 90.60 91.10 91.55 91.80 91.10 93.60 77.00 80.20 80.70 82.47 86.50 80.70 87. 68.90 81.60 79.80 81.61 81.90 79.80 85.80 75.80 87.40 87.90 87.18 85.50 87.90 90.50 61.80 72.10 71.40 73.18 76.20 71.40 79.50 73.30 82.30 82.70 81.67 85.00 82.70 87.30 74.80 82.20 82.70 81.99 86.00 82.70 87.80 AVE 74.14 82.97 82.94 83.57 85.30 82.94 87.96 Migician 91.62 93.49 87.22 86. 91.06 79.93 88.06 87.80 88.16 Models Spontaneous Referential AVE mPLUG-Owl3 mPLUG-Owl3+mCoT mPLUG-Owl3+CoT InternVL2-8B InternVL2-8B+mCoT InternVL2-8B+CoT Qwen2-VL-7B Qwen2-VL-7B+mCoT Qwen2-VL-7B+CoT 19.96 23.78 26.73 13.29 23.78 31. 19.96 41.83 42.59 9.08 14.10 26.43 17.10 21.99 37.57 28.67 26.23 44.34 13.04 17.62 26.54 15.71 22.64 35. 28.61 31.90 43.70 Table 4. The performance of different competitive single image grounding models on Refcoco, Refcoco+ and Refcocog benchmarks. Continual grounding training in the multi-image scenario further enhances the overall grounding ability of the model. Migician achieves top performance among all grounding models. Table 5. The comparison among different CoT variants. We compare three representative MLLMs among direct reference, single-image CoT (+CoT), multiimage CoT (+mCoT) as described in Section 7.1. Multi-image General Benchmarks Multi-image Grounding Setting Base MuirBench BLINK MIBench Mantis 42. 52.35 68.06 70.97 MMIU 54.36 Full data (Stage-1) -w/o grounding -w/o general 53.77 44.54(9.23) 53.62(0.15) 51.27 51.32(+0.42) 49.25(2.02) 71.76 71.68(0.08) 65.22(6.54) 66.36 67.74(+1.38) 64.52(1.84) 53.31 52.12(1.19) 48.61(4.70) Table 6. The ablation study about the contribution of different data subsets. MIG 28.62 62.79 22.43(40.36) 62.21(0.58) 6.4. Results on Single-Image Grounding Benchmarks As presented in Table 4, Migician not only acquires freeform multi-image grounding capabilities but also demonstrates continual and consistent performance improvements on the RefCOCO series single-image grounding benchmark, surpassing specialized grounding models such as Griffon v2 and GroundingGPT by large margin. Additionally, Migician outperforms Qwen2-VL-7B in terms of average scores. 7. Analysis 7.1. Effects of Different CoT Strategies The CoT framework in Section 4.1, after obtaining referring expression, has the MLLM perform grounding in each image in polling manner (denoted as single-image CoT), which incurs significant inference overhead. Here, we explore multi-image CoT, where the MLLM directly performs grounding across all images based on the obtained referring expression. As shown in Table 5, multi-image CoT achieves some effectiveness but it still falls significantly behind single-image CoT. In contrast, our proposed Migician is able to perform end-to-end reasoning, offering significant advantages in both efficiency and effectiveness. Typically, these tasks involve images with very high resolution, where the relevant visual information is often quite small, posing significant challenges to the models grounding ability. In this section, we analyze and demonstrate that the multi-image grounding capability of Migician can be leveraged to efficiently address this task. Specifically, we slice single high-resolution image into multiple sub-images and directly transform the problem into multi-image grounding task. By utilizing the MIG ability of Migician, we can locate the regions relevant to the input question. Afterward, the model combines the identified region with the original image to generate the answer for the input question. We test this approach on the V*Bench [41] and list the In the table, we refer to the method results in Table 3. that directly asks Migician to answer the question based on the original image as Migicianzero_shot, while Migicianslice denotes the method that transforms this task into MIG task as mentioned before. Its detailed implementation could be found in Appendix E. The results remarkably demonstrate the effectiveness of using the MIG approach for highresolution image visual search. Notably, on the Attribute recognition task, Migician even surpasses the specialized visual searching system SEAL [41]. 7.3. Effects of Different Data on Multi-Image Un7.2. Visual Search in High-Resolution Images derstanding Finding visual details in high-resolution images is challenging task, and many recent works have explored this area [41]. As observed in Table 6, Migician shows an improvement in multi-image understanding. We further conduct an ab8 lation study to analyze the effects of different data subsets. Specifically, we train two models with either the multi-image grounding and multi-image understanding data removed from the training set. The results in Table 6 reveal that grounding data generally aids multi-image understanding. In 4 out of 5 benchmarks, the full dataset achieves the highest performance compared to models trained with any subset of data removed. In contrast, directly fine-tuning with only general data does not consistently lead to performance boost. However, when combined with fine-grained grounding data, the model experiences notable improvement. 8. Conclusion In this work, we explore the task of multi-image grounding and propose Migician, the first MLLM to overcome the barriers between fine-grained visual grounding and multi-image inputs. With our proposed large-scale MGrounding-630k dataset, Migician seamlessly integrates grounding across multiple images, enabling free-form multi-image grounding. To further advance research in this area, we introduce MIG-Bench, comprehensive benchmark for evaluating the multi-image grounding capabilities of MLLMs. Experimental results demonstrate that our model significantly outperforms existing methods. We hope this work will inspire further developments in multi-image grounding and contribute to the creation of more versatile multimodal models in the future."
        },
        {
            "title": "Limitation",
            "content": "Despite our comprehensive discussion of the MIG challenge, there still remain several limitations. First, due to the computational budget, we havent verified the effectiveness of our training methods on larger 70B scale models. Secondly, in spite of intensive grounding training, our model is still confronted with inaccurate grounding issue, especially in complicated or messy scenarios. Lastly, our training methods and benchmark construction mainly focus on the REC task. Although Migician possesses decent REG capacity, this topic is still insufficiently discussed."
        },
        {
            "title": "Acknowledgement",
            "content": "This work is supported by the Fundamental Research Funds for the Central Universities of China under Grant 2024JBGP008 and the National Natural Science Foundation of China (No. 62406018). We extend our heartfelt gratitude to the dedicated human volunteers, Mai Sun, Pujian Zhan, Xingyu Zhang, Binhao Liu, for performance human-level their evaluation. Their contributions are deeply appreciated. and Huiting Pei, tireless efforts in"
        },
        {
            "title": "References",
            "content": "[1] Aida Amini, Saadia Gabriel, Peter Lin, Rik Konceland Hannaneh Hajishirzi. Kedziorski, Yejin Choi, Mathqa: interpretable math word problem solving with operation-based formalisms. arXiv preprint arXiv:1905.13319, 2019. 1 Towards [2] James Black, Tim Ellis, and Paul Rosin. Multi view image surveillance and tracking. In Workshop on Motion and Video Computing, 2002. Proceedings., pages 169174. IEEE, 2002. 2 [3] Zheng Cai, Maosong Cao, Haojiong Chen, Kai Chen, Keyu Chen, Xin Chen, Xun Chen, Zehui Chen, Zhi Chen, Pei arXiv preprint Chu, et al. arXiv:2403.17297, 2024. 2, 8 Internlm2 technical report. [4] Chi Chen, Ruoyu Qin, Fuwen Luo, Xiaoyue Mi, Peng Li, Maosong Sun, and Yang Liu. Position-enhanced visual instruction tuning for multimodal large language models. arXiv preprint arXiv:2308.13437, 2023. [5] Keqin Chen, Zhao Zhang, Weili Zeng, Richong Zhang, Feng Zhu, and Rui Zhao. Shikra: Unleashing multimodal llms referential dialogue magic. arXiv preprint arXiv:2306.15195, 2023. 1, 2, 8 [6] Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li Fei-Fei. Imagenet: large-scale hierarchical image database. In 2009 IEEE conference on computer vision and pattern recognition, pages 248255. Ieee, 2009. 13 [7] Desmond Elliott and Akos Kádár. Imagination improves multimodal translation. arXiv preprint arXiv:1705.04350, 2017. 1 [8] Heng Fan, Liting Lin, Fan Yang, Peng Chu, Ge Deng, Sijia Yu, Hexin Bai, Yong Xu, Chunyuan Liao, and Haibin Ling. Lasot: high-quality benchmark for large-scale single object tracking. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 53745383, 2019. 13 [9] Chaoyou Fu, Peixian Chen, Yunhang Shen, Yulei Qin, Mengdan Zhang, Xu Lin, Jinrui Yang, Xiawu Zheng, Ke Li, Xing Sun, et al. Mme: comprehensive evaluation benchmark for multimodal large language models. arXiv preprint arXiv:2306.13394, 2023. [10] Xingyu Fu, Yushi Hu, Bangzheng Li, Yu Feng, Haoyu Wang, Xudong Lin, Dan Roth, Noah Smith, Wei-Chiu Ma, and Ranjay Krishna. Blink: Multimodal large language modIn European Conference on els can see but not perceive. Computer Vision, pages 148166. Springer, 2025. 2 [11] Kristen Grauman, Andrew Westbury, Eugene Byrne, Zachary Chavis, Antonino Furnari, Rohit Girdhar, Jackson Hamburger, Hao Jiang, Miao Liu, Xingyu Liu, et al. Ego4d: Around the world in 3,000 hours of egocentric video. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 1899519012, 2022. 2, 7 [12] Anwen Hu, Haiyang Xu, Jiabo Ye, Ming Yan, Liang Zhang, Bo Zhang, Chen Li, Ji Zhang, Qin Jin, Fei Huang, et al. mplug-docowl 1.5: Unified structure learning for ocr-free document understanding. arXiv preprint arXiv:2403.12895, 2024. 1 9 [13] Lianghua Huang, Xin Zhao, and Kaiqi Huang. Got-10k: large high-diversity benchmark for generic object tracking in the wild. IEEE transactions on pattern analysis and machine intelligence, 43(5):15621577, 2019. 6, 13 [14] Gabriel Ilharco, Marco Tulio Ribeiro, Mitchell Wortsman, Suchin Gururangan, Ludwig Schmidt, Hannaneh Hajishirzi, and Ali Farhadi. Editing models with task arithmetic. arXiv preprint arXiv:2212.04089, 2022. 6, [15] Julia Ive, Pranava Madhyastha, and Lucia Specia. Distilling translations with visual awareness. arXiv preprint arXiv:1906.07701, 2019. 1 [16] Harsh Jhamtani and Taylor Berg-Kirkpatrick. Learning to describe differences between pairs of similar images. arXiv preprint arXiv:1808.10584, 2018. 12 [17] Dongfu Jiang, Xuan He, Huaye Zeng, Cong Wei, Max Ku, Qian Liu, and Wenhu Chen. Mantis: Interleaved multi-image instruction tuning. arXiv preprint arXiv:2405.01483, 2024. 2 [18] Qirui Jiao, Daoyuan Chen, Yilun Huang, Yaliang Li, and Ying Shen. Img-diff: Contrastive data synthesis for multimodal large language models. arXiv preprint arXiv:2408.04594, 2024. 12 [19] Sahar Kazemzadeh, Vicente Ordonez, Mark Matten, and Tamara Berg. Referitgame: Referring to objects in photographs of natural scenes. In Proceedings of the 2014 conference on empirical methods in natural language processing (EMNLP), pages 787798, 2014. 2, 3, 7 [20] Ranjay Krishna, Yuke Zhu, Oliver Groth, Justin Johnson, Kenji Hata, Joshua Kravitz, Stephanie Chen, Yannis Kalantidis, Li-Jia Li, David Shamma, et al. Visual genome: Connecting language and vision using crowdsourced dense image annotations. International journal of computer vision, 123:3273, 2017. 1 [21] Woosuk Kwon, Zhuohan Li, Siyuan Zhuang, Ying Sheng, Lianmin Zheng, Cody Hao Yu, Joseph E. Gonzalez, Hao Zhang, and Ion Stoica. Efficient memory management for large language model serving with pagedattention. In Proceedings of the ACM SIGOPS 29th Symposium on Operating Systems Principles, 2023. [22] Bohao Li, Yuying Ge, Yixiao Ge, Guangzhi Wang, Rui Wang, Ruimao Zhang, and Ying Shan. Seed-bench: Benchmarking multimodal large language models. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 1329913308, 2024. 2 [23] Bo Li, Yuanhan Zhang, Dong Guo, Renrui Zhang, Feng Li, Hao Zhang, Kaichen Zhang, Peiyuan Zhang, Yanwei Li, Ziwei Liu, et al. Llava-onevision: Easy visual task transfer. arXiv preprint arXiv:2408.03326, 2024. 2 [24] Feng Li, Renrui Zhang, Hao Zhang, Yuanhan Zhang, Bo Li, Wei Li, Zejun Ma, and Chunyuan Li. Llava-next-interleave: Tackling multi-image, video, and 3d in large multimodal models. arXiv preprint arXiv:2407.07895, 2024. 2, 15 [25] Zhaowei Li, Qi Xu, Dong Zhang, Hang Song, Yiqing Cai, Qi Qi, Ran Zhou, Junting Pan, Zefeng Li, Vu Tu, et al. Groundinggpt: Language enhanced multi-modal grounding model. In Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 66576678, 2024. 8 [26] Tsung-Yi Lin, Michael Maire, Serge Belongie, James Hays, Pietro Perona, Deva Ramanan, Piotr Dollár, and Lawrence Zitnick. Microsoft coco: Common objects in context. In Computer VisionECCV 2014: 13th European Conference, Zurich, Switzerland, September 6-12, 2014, Proceedings, Part 13, pages 740755. Springer, 2014. 13 [27] Haowei Liu, Xi Zhang, Haiyang Xu, Yaya Shi, Chaoya Jiang, Ming Yan, Ji Zhang, Fei Huang, Chunfeng Yuan, Bing Li, et al. Mibench: Evaluating multimodal large language models over multiple images. arXiv preprint arXiv:2407.15272, 2024. 2 [28] Pan Lu, Liang Qiu, Jiaqi Chen, Tony Xia, Yizhou Zhao, Wei Zhang, Zhou Yu, Xiaodan Liang, and Song-Chun Zhu. Iconqa: new benchmark for abstract diagram understanding and visual language reasoning. arXiv preprint arXiv:2110.13214, 2021. [29] Fanqing Meng, Jin Wang, Chuanhao Li, Quanfeng Lu, Hao Tian, Jiaqi Liao, Xizhou Zhu, Jifeng Dai, Yu Qiao, Ping Luo, et al. Mmiu: Multimodal multi-image understanding for evaluating large vision-language models. arXiv preprint arXiv:2408.02718, 2024. 2 [30] Anton Milan. Mot16: benchmark for multi-object tracking. arXiv preprint arXiv:1603.00831, 2016. 13 [31] Matthias Muller, Adel Bibi, Silvio Giancola, Salman Alsubaihi, and Bernard Ghanem. Trackingnet: large-scale dataset and benchmark for object tracking in the wild. In Proceedings of the European conference on computer vision (ECCV), pages 300317, 2018. 13 [32] Dong Huk Park, Trevor Darrell, and Anna Rohrbach. Robust change captioning. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 46244633, 2019. 12 [33] Zhiliang Peng, Wenhui Wang, Li Dong, Yaru Hao, Shaohan Huang, Shuming Ma, and Furu Wei. Kosmos-2: Grounding multimodal large language models to the world. arXiv preprint arXiv:2306.14824, 2023. [34] Hanoona Rasheed, Muhammad Maaz, Sahal Shaji, Abdelrahman Shaker, Salman Khan, Hisham Cholakkal, Rao Anwer, Eric Xing, Ming-Hsuan Yang, and Fahad Khan. Glamm: Pixel grounding large multimodal model. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 1300913018, 2024. 13 [35] Shuai Shao, Zeming Li, Tianyuan Zhang, Chao Peng, Gang Yu, Xiangyu Zhang, Jing Li, and Jian Sun. Objects365: large-scale, high-quality dataset for object detection. In Proceedings of the IEEE/CVF international conference on computer vision, pages 84308439, 2019. 5, 7 [36] Fei Wang, Xingyu Fu, James Huang, Zekun Li, Qin Liu, Xiaogeng Liu, Mingyu Derek Ma, Nan Xu, Wenxuan Zhou, Kai Zhang, et al. Muirbench: comprehensive benchmark for robust multi-image understanding. arXiv preprint arXiv:2406.09411, 2024. 2 [37] Peng Wang, Shijie Wang, Junyang Lin, Shuai Bai, Xiaohuan Zhou, Jingren Zhou, Xinggang Wang, and Chang Zhou. Onepeace: Exploring one general representation model toward unlimited modalities. arXiv preprint arXiv:2305.11172, 2023. 1 10 [51] Shilong Zhang, Peize Sun, Shoufa Chen, Min Xiao, Wenqi Shao, Wenwei Zhang, Yu Liu, Kai Chen, and Ping Luo. Gpt4roi: Instruction tuning large language model on regionof-interest. arXiv preprint arXiv:2307.03601, 2023. [38] Peng Wang, Shuai Bai, Sinan Tan, Shijie Wang, Zhihao Fan, Jinze Bai, Keqin Chen, Xuejing Liu, Jialin Wang, Wenbin Ge, et al. Qwen2-vl: Enhancing vision-language models perception of the world at any resolution. arXiv preprint arXiv:2409.12191, 2024. 2, 4, 5, 6, 7, 8 [39] Yuqi Wang, Jiawei He, Lue Fan, Hongxin Li, Yuntao Chen, and Zhaoxiang Zhang. Driving into the future: Multiview visual forecasting and planning with world model for autonomous driving. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 1474914759, 2024. 2 [40] Jiannan Wu, Muyan Zhong, Sen Xing, Zeqiang Lai, Zhaoyang Liu, Wenhai Wang, Zhe Chen, Xizhou Zhu, Lewei Lu, Tong Lu, et al. Visionllm v2: An end-to-end generalist multimodal large language model for hundreds of vision-language tasks. arXiv preprint arXiv:2406.08394, 2024. 8 [41] Penghao Wu and Saining Xie. V?: Guided visual search as core mechanism in multimodal llms. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 1308413094, 2024. 8 [42] Yunqiu Xu, Linchao Zhu, and Yi Yang. Mc-bench: benchmark for multi-context visual grounding in the era of mllms. arXiv preprint arXiv:2410.12332, 2024. 2 [43] An Yang, Baosong Yang, Binyuan Hui, Bo Zheng, Bowen Yu, Chang Zhou, Chengpeng Li, Chengyuan Li, Dayiheng Liu, Fei Huang, et al. Qwen2 technical report. arXiv preprint arXiv:2407.10671, 2024. [44] Shunyu Yao, Jeffrey Zhao, Dian Yu, Nan Du, Izhak Shafran, Karthik Narasimhan, and Yuan Cao. React: Synergizing reasoning and acting in language models. arXiv preprint arXiv:2210.03629, 2022. 4, 12 [45] Yuan Yao, Tianyu Yu, Ao Zhang, Chongyi Wang, Junbo Cui, Hongji Zhu, Tianchi Cai, Haoyu Li, Weilin Zhao, Zhihui He, et al. Minicpm-v: gpt-4v level mllm on your phone. arXiv preprint arXiv:2408.01800, 2024. 2 [46] Jiabo Ye, Anwen Hu, Haiyang Xu, Qinghao Ye, Ming Yan, Guohai Xu, Chenliang Li, Junfeng Tian, Qi Qian, Ji Zhang, et al. Ureader: Universal ocr-free visually-situated language understanding with multimodal large language model. arXiv preprint arXiv:2310.05126, 2023. 1 [47] Jiabo Ye, Haiyang Xu, Haowei Liu, Anwen Hu, Ming Yan, Qi Qian, Ji Zhang, Fei Huang, and Jingren Zhou. mplug-owl3: Towards long image-sequence understanding in multi-modal large language models. arXiv preprint arXiv:2408.04840, 2024. 2 [48] Haoxuan You, Haotian Zhang, Zhe Gan, Xianzhi Du, Bowen Zhang, Zirui Wang, Liangliang Cao, Shih-Fu Chang, and Yinfei Yang. Ferret: Refer and ground anything anywhere at any granularity. arXiv preprint arXiv:2310.07704, 2023. 1, 2 [49] Yufei Zhan, Yousong Zhu, Hongyin Zhao, Fan Yang, Ming Tang, and Jinqiao Wang. Griffon v2: Advancing multimodal perception with high-resolution scaling and visual-language co-referring. arXiv preprint arXiv:2403.09333, 2024. 8 [50] Kai Zhang, Lingbo Mo, Wenhu Chen, Huan Sun, and Yu Su. Magicbrush: manually annotated dataset for instructionguided image editing. Advances in Neural Information Processing Systems, 36, 2024. 7, 12 A. Benchmark Tasks Definition A.1. Spontaneous Grounding Our benchmark evaluates the spontaneous grounding through three distinct tasks below, which aim at assessing models ability to autonomously discover insidious connections across various images and accurately recognize then locate the target. Spot the Difference Given two similar images with single subtle difference, the model is instructed to recognize and ground this difference in the second image, requiring keen perceptual skills. Common Object Grounding It refers to automatically recognizing and grounding the common object appearing in all images within an image group, which in our bench, each shares one definite common object. Robust Image Difference Grounding Models must focus on the primary difference between two images captured from slightly different perspectives, ignoring other minor variations caused by shifts in the viewpoint. A.2. Reference Grounding Textual Reference Query This challenge, which mainly includes Group Grounding, tests models ability to link textual reference to target object within its corresponding specific image. Given set of images and one textual query, the model must identify the correct image then accurately ground the target object within it. Visual Reference Query These tasks examine models ability to effectively utilize visual reference information and incorporate it into the searching process. (1) Visual Referring Grounding. In this task, pair of images is provideda source image with clear object and target image containing multiple elements. The model must locate the referenced object in the target image. (2) Region Locating. Models are tasked with identifying multiple region images within source image, which often requires perceptive and discerning observation as the model may encounter person recognition, similar object distinguishing, tiny item searching and etc. (3) Object Tracking. This task involves tracking target object across sequence of video frames. The object is highlighted with red bounding box in the first frame, and the model must follow it throughout the sequence. (4) Multi-view Grounding. Here, the model must locate the same target object across multiple images taken from distinct viewpoints. Visual+Textual Reference Query These tasks combine information from both modalities to assess cross-modal reasoning abilities. (1) Correspondence. The model must ground semantically or functionally similar regions within target image. This finer-grained task focuses on object regions rather than whole objects, demanding an in-depth understanding of visual semantics. (2) Reasoning. This task requires the model to perform reasoning-based grounding by integrating cross-modality information. Several examples are shown in Figure2. Our comprehensive benchmark offers rich, multi-faceted evaluation across various real-world scenarios and domains, extending beyond simple image pairs to include longer and more complex image contexts. By ensuring that each task is well-defined and unambiguous, we facilitate objective and definitive assessments.. B. Single-Image CoT Failure Patterns As shown in Figure 5, the four representative failure patterns are (a) special multi-image format, (b) abstract visual information, (c) CoT error propagation, (d) step-2 inference error. When the multiple images are formatted in special pattern, where our target object is missing in the target image, like, the information in this image is insufficient to perform grounding. The abstract visual information refers to situations where the intricate visual cannot be adequately converted in textual description to perform accurate grounding. In Figure 5, the simple description \"a close-up of womans face\" cannot distinguish which face the target is in Image-1. Each reasoning step of CoT could be incorrect, which could potentially leads to the error propagation issue [44]. In Figure 5, the conclusion draw from the first step is incorrect, which directly leads to the mistake in the second step. The last failure pattern refers to cases where the erroneous reasoning step appears at the second step, failing to accurately ground or follow instruction. C. MGrounding-630k Data Curation Details C.1. Transforming Existing Data Static Diff Describing the differences of the two nearly same pictures is well discussed topic, yet they focus on the coarse-grained semantic feature, failing to precisely recognize the part of differences. After comprehensive survey on this area, we have collected high-quality and fully labeled image difference data from Spot-the-diff [16], Img-diff [18], MagicBrush [50] and CLEVR-change [32]. During the construction process, we ensure the diversity of the content by (1) incorporating numerous prompt formats generated by GPT-4, (2) constructing CoT process to assist 12 the model gradually and progressively reaching the final answer, while also fully utilizing the annotation available in the dataset. Common Object Grounding Recognizing and grounding the main common object in multiple images is an interesting yet non-trivial task for models, which firstly requires them to simultaneous look at multiple images, disentangle the common object, then finally grounding the target object in every single image. We take diverse data sources from ImageNet [6], COCO [26] and Objects365, where the annotations are abundant and rich. Through their annotations, we group the images that share the same object together with threshold of the proportion it takes for the whole image to filter out too tiny objects. We empirically find such threshold effective at eliminating ambiguity where there could be multiple common object candidates, which results in clear and definite training examples. We further reduce ambiguity by skipping inappropriate classes where there are always multiple possible candidate such as couch, dinning table, keyboard and etc. Object Tracking The original object tracking emphasize more on tracking the target object in video sequence, which resembles with multi-image sequences. As well discussed topic, we select the large scale TrackingNet [31], LaSOT [8], GOT-10K [13] and MOT-2017 [30] datasets as our data sources. During the dataset construction process, we have simplified the original long sequence to 4-6 image per training example, maintaining its original core feature while keeping efficiency. For each image, we extract the key frame with an appropriate interval to ensure the obvious movement of the key target. We also involve small proportion of the ordering judge for continuous images to enhance the models temporal understanding ability. Referring Grounding This part of training data is designed to imitate finding the object of the source image in the target image. We mainly utilize the ImageNet-2012 subset to construct image pairs, with the first one taking large proportion of the whole image, and the second one containing smaller target object that may take efforts to spot. In total, the referring grounding dataset covers wide range of objects that is beneficial for the model to generalize. Group Grounding Conventional visual grounding is mostly limited to single image context, while in real world scenario, we often need to recognize the target object from messy piles of pictures. Group Grounding, which locates the target among group of different images, is the exact task to fill in such gap and enrich the versatility of traditional grounding. When constructing this part of data, we take advantage of the single image GranD rec and reg conversation data [34] with the quantity of 3M. With further filtering and combining 3-5 images per group, we finally obtained collection of 12w high-quality training data for stage-1 training(grounding injection training), which is effectively at enhancing the co-reference and image-level locating ability of models. Region Locating Region locating refers splitting several pieces of semantic-rich region from the source image and then recognize the exact location of this region image in the source picture. To guarantee extracting meaningful regions, we utilize the Objects365 dataset and extract the bounding box area as the regions. To further improve quality, we set series of filtering mechanism:(1) content richness: we select the images that have more than 10 bounding box annotations (2) aspect ratio: we to avoid too plain or simple cases. keep the aspect ratio between 0.5-2 to avoid the excessively thin bounding box areas that models may fail to effectively tackle. (3) size: we keep the region ratio of the whole image between 0.2-0.49 and an absolute pixels above 2000 to avoid excessively tiny and obscure region images. Noticeably, due to our meticulously crafted mechanism and the feature of this task, the resulting training data encompasses the cases of person recognition in the source image, analogous objects distinguishing and tiny details recognition that are non-trivial even for human. C.2. Synthesizing Free-form MIG Data The algorithm for CLIP adaptive similarity image input is shown in Algorithm 1. We further display our prompt template for image caption generation, bounding box label refinement and instruction tuning data generation in the following pages. Specifically, we deploy Qwen2-VL-7B for detailed image caption generation and Qwen2-VL-72B for bbox label refinement. The inference process is accelerated through vLLM framework [21]. D. Details of Two-Stage Training This section outlines the data proportions and their respective sources for the two training stages, as summarized in Table 10. In stage 1, we leverage both single-image and multiimage datasets encompassing general understanding and grounding tasks to comprehensively enhance the models capabilities. At the stage-1 subset from MGrounding-630k constitutes the largest portion of the training data, with total of 530k examples. The total training examples for stage-1 is 1 million. this stage, In stage 2, the focus shifts to stimulating the models freeform MIG abilities by integrating all free-form grounding 13 Training Methods Referring Object Tracking Group Grounding Region Static Diff Common Object Base Multi-Task Learning Separate Learning Model Merging 23.23 60.00 69.70 60.61 20.73 61.65 74.55 50.00 58.52 62.28 63.13 64. 25.95 57.95 65.42 18.95 27.84 55.68 68.94 29.92 19.36 81.37 79.53 65. Table 7. Comparison between different training methods. We compare the learning efficiency between multi-task learning, separate learning and merging all these task-specialized modes. We mainly focus on the in-domain tasks that M-Grounding dataset covers. Models Settings Common Object Multi-view Grounding Object Tracking Region Locating Random Guess Qwen2-VL-7B Qwen2-VL-7B Migician Migician Polling All Polling All 26.47 19.96 19.36 81.99 72.43 1.04 11.83 6.60 44.44 43. 2.13 20.73 13.09 61.09 58.55 0.00 25.95 11.80 59.65 34.91 Table 8. Comparison of different answering forms. For random guess, we set the default answer as (0,0),(999,999). Algorithm 1 CLIP Adaptive Similarity Selection Require: Images I, adaptive selection range k, thres (0, 1) Ensure: Final Image Set 1: Initialize 2: Extract FI Features of 3: while FI is not empty do 4: 5: Randomly select thres Uniform(0.1, 1) for each fi FI do sij = similarity(fi, fj), fj FI , = 6: 7: 8: 9: 10: 11: end for Sort_Si = Sort(sij)[1 :] thres (len(Sort_Si)) Candidates Sort_Si[: k] Randomly select Uniform(3, 5) Selected Sample(Candidates, r) Append fi and Selected to Remove fi and Selected from FI 12: 13: 14: 15: end while 16: return data from MGrounding-630k. significant proportion of stage-1 data is also reused to maintain the previously learned abilities. The total number of training examples in this stage is 200k. E. Evaluation Implementation When directly requiring the model to generate bounding box coordinates for each image, due to their limited multi-image grounding ability and insufficient instruction following ability, the answer obtained in this way is largely unfaithful 14 and mostly unsatisfactory in instruction following, failing to objectively reflecting the real grounding ability of the model. Considering current models feeble performance, we transform from directly generating all answers to polling every single image, which facilitates definite and objective evaluation. Empirically, directly generating all the bounding box coordinates for all images results in lower performance. Yet as illustrated in Table 8, Migician still demonstrates great robustness to the variation of evaluation format. The performance of Migician is presented in Table 4. Although mainly targeted at multi-image grounding, Migician still maintains well on conventional single-image grounding task. 70B Scale Models The performances of three competitive 70B scale models are illustrated in Table 9 when equipped with single-image CoT. The general effectiveness of CoT framework is tremendous, with the average performance boost at 20 points. Yet even competitive and much larger model like Qwen2-VL-72B (58.70%) still cant surpass our Migician (60.49%) in multi-image grounding, demonstrating great competence. V*-Bench Implementation When adapting single highresolution image to multi-image setting, we require the model to first ground, then judge the attributes. We start by dividing the original image into four equally sized subimages. This transforms the single-image localization problem into group grounding task, where the model is required to perform both image-level and object-level grounding. The model generates outputs such as: \"The tiny dog is located in Image-4, (346,763),(431,835).\" Spontaneous Grounding Referential Grounding Models Difference Similarity Visual Reference Textual Visual+Textual AVE Static Robust Common OT MV Region Refer GG Reason Co-Re 70B Scale Models LLaVA-OV-72B InternVL2-76B Qwen2-VL-72B LLaVA-OV-72B+CoT InternVL2-76B+CoT Qwen2-VL-72B+CoT 13.26 15.91 46.12 20.27 16.86 33.33 5.34 10.64 46.81 21.28 6.38 47. 26.84 36.40 64.46 52.57 70.34 69.24 12.91 30.73 26.73 44.36 70.55 70.18 7.64 20.83 22.57 20.83 33.33 60.42 2.14 5.74 18.62 25.60 27.27 51.04 17.83 46.46 33.33 37.37 68.69 78.79 21.60 41.28 62.53 35.07 57.31 70. 11.88 32.67 50.50 31.68 52.48 70.30 8.55 26.50 17.09 28.21 23.08 35.04 13.65 26.72 38.88 31.72 42.63 58.70 Table 9. Performance Comparison of 70B scale models equipped with CoT. Type Source Stage-1 Ratio G. Case Study We provide detailed cases comprehensively reflecting the free-form MIG ability of Migician in Figure 6, 7, as well as our instruction tuning data details examples in Figure 8. S-Understanding S-Grounding M-Understanding M-Grounding LLaVA-OV-data RefCOCO series, Groma-Instruct M4-Instruct[24] MGrounding-630k (Stage-1) Stage-2 S-Understanding S-Grounding M-Understanding M-Grounding LLaVA-OV-data RefCOCO series, Groma-Instruct M4-Instruct[24] M-Grounding (Stage-1) M-Grounding (Stage-2) 17% 13% 16% 54% 9% 7% 8% 27% 49% Table 10. Training data proportion for two stages. Once the object is localized, we send only the identified sub-image and its corresponding region of interest to the model for attribute recognition and relative relationship analysis. To ensure contextual completeness, we expand the extracted region by slightly enlarging its width and height, adding an additional 50 pixels to each dimension. F. Multi-Task Learning Our whole training process involves the learning process of multiple distinct tasks. How does the actual learning efficiency alter compared with learning these tasks separately, can they contribute to each other or comprise to some extent? We conduct experiments that only expose the model to omni-task dataset and the results are shown in Table 7. It clearly reveals the conflicts of learning various tasks, with mixes multi-task training consistently surpassing omni-task learning by huge margin. When we directly merge the checkpoints of all these trained specialized models [14], the merged model fail at excelling at most tasks, with the average performance falling behind simple multi-task learning. Figure 6. Example cases of the free-form multi-image grounding ability of Migician. 16 Prompt Template for Single-Image CoT Task: Static diff Step-1: Compare these two images carefully and tell me where does they differ. Please answer briefly in single phrase or words. Step-2: According to the object difference/change: [RESPONCE], please ground this difference with bounding box coordinates. Task: Robust diff Step-1: Compare these two images carefully and describe the prominent different object with really simple words or phrase. Step-2: Now ground the object difference/change : \"[RESPONCE]\" with bounding box coordinates. Task: Referring Grounding Step-1: Watch carefully and briefly describe the object in the Image-1. Step-2: Please find and ground the object <object_ref_start>[RESPONCE]<object_ref_end> with bounding box coordinates. Task: Common Object Step-1: These images share one object in common. Recognize it and tell me its name in single phrase or words. Step-2: Please locate and ground the target object according to the reference: <object_ref_start> [RESPONCE] <object_ref_end> Task: Region Locating Step-1: Describe the content of the XXXth picture with simple phrase or words. Step-2: Please ground the object <object_ref_start>[RESPONCE]<object_ref_end> with bounding box coordinates. Task: Multi-View Step-1: Describe the object in the first image marked with red bounding box(<box_start> (A,B),(C,D) <box_end>) with simple phrase or word. You can refer to other images for more precise recognition and description. Step-2: Locate and ground the object <object_ref_start> [RESPONCE] <object_ref_end> with bounding box coordinates. Task: Object Tracking Step-1: Describe the object in the first image marked with red bounding box with simple phrase. Step-2: Now ground the target moving object [RESPONCE] with bounding box coordinates. Task: Group Grounding Step-1: Just recognize and tell me which image is it in. Answer from: Image1 Image2 Image3... Step-2: [Selected Image] + [Original Question] Note: For group grounding, the single image at step-2 is selected by matching the answer from step-1. If the framework fails to extract the target image, we send the first image by default. Task: Reasoning Step-1: [Original Question] + Name this object in the Image-2 with simple phrase. Step-2: Please locate and ground the object <object_ref_start>[RESPONCE]<object_ref_end> with bounding box coordinates. Task: Correspondence Step-1: For the first (<box_start>(A,B),(C,D)<box_end>). Step-2: Ground the area that shares the same semantic or functional meaning of: [RESPONCE]. image, describe the semantic/functional feature of the area marked by the red bounding box Format Prompt Format: <box_start>(x1,y1),(x2,y2)<box_end>. Dont generate additional words. Note: we deploy this prompt for better instruction following. 17 Figure 7. Example cases of the free-form multi-image grounding ability of Migician. Figure 8. Training Examples of the free-form instruction tuning data."
        },
        {
            "title": "Prompt Template for Caption and Instruction Data Generation",
            "content": "Bbox Refinement Template Now Id like you to inspect the original image carefully. Then filter, refine and enhance these annotated objects. Finally, just give me your final modified annotations. *Filtering* Based on you insightful observation of the image, please eliminate the obviously inaccurate (object,bbox) pairs, which in supposed to be small in quantity. *Refine* Refine and enhance the original class/name of each object like color, position, <box_start>(x1,y1),(x2,y2)<box_end>). into short yet richer caption containing its attributes feature(e.g plane <box_start>(x1,y1),(x2,y2)<box_end> -> dark gray plane flying in the sky *Amplify* If any important objects are missing from the annotations, and you believe they are significant and essential, and you are confident of their location, feel free to add them to the final annotations. *Output Format* Modified object caption followed by its bounding box coordinates. Now the original bounding box annotations give to you are: Caption Generation Describe this image thoroughly in fluent paragraph. Include all the objects and their attributes(color, shape, size and feature), relative position and relationship. Multi-image Grounding Instruction Generation Template 1 Based on the following detailed information of multiple images, please compose meaningful and flexible CROSS-IMAGE grounding questions that link different objects across the images by their attributes similarity/contrastsuch as color, position, features, gender, size, shape, etc.or by other potential logical connection between them. Specifically: 1.The questions should include CROSS-IMAGE grounding requests that requires the answer to identify and locate various potentially connected object across different images. You can use the connection or similarity between these objects to refer the target item. 2.When referring an object in the question, keep the reference description concise and avoid giving away unnecessary information(like bbox or over-detailed caption) that could lead to answering too easily. You are encouraged to refer the target object to be grounded by the connection of these objects, instead of explicitly point out the object. For instance: ground the car in image-2 that contrasts most in quality with the shabby vehicle in image-4, rather than ground the fancy red sports car(explicitly pointing out) in image-2 that contrasts most in quality with the shabby vehicle in image-4, by doing so we can also introduce bit reasoning process. 3.Include the bounding box coordinates of referred object in the answer as well as the explanation. (Actually you can get lot of information from the coordinates, which are formatted as (x1,y1),(x2,y2)) 4.Strictly format the output as simple Q: A:. In answer, follow the format <ref>object</ref> for objects mentioned. Below are the detailed image captions and the objects in the corresponding images:"
        }
    ],
    "affiliations": [
        "Huazhong University of Science and Technology, Wuhan, China",
        "State Key Laboratory of Advanced Rail Autonomous Operation, Beijing Jiaotong University, Beijing, China",
        "Tsinghua University, Beijing, China"
    ]
}