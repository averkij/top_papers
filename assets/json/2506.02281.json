{
    "paper_title": "Angles Don't Lie: Unlocking Training-Efficient RL Through the Model's Own Signals",
    "authors": [
        "Qinsi Wang",
        "Jinghan Ke",
        "Hancheng Ye",
        "Yueqian Lin",
        "Yuzhe Fu",
        "Jianyi Zhang",
        "Kurt Keutzer",
        "Chenfeng Xu",
        "Yiran Chen"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Current Reinforcement Fine-tuning (RFT) paradigms for Large Language Models (LLMs) suffer from sample inefficiency due to the redundant exposure of identical queries under uniform data sampling. While previous work has explored curriculum learning via heuristic difficulty metrics, these strategies exhibit limitations by neglecting the intrinsic learning signals generated by the model itself, thus leading to suboptimal training regimes. In this paper, we identify a model-inherent signal termed angle concentration that effectively reflects an LLM's capacity to learn from specific data. We theoretically and empirically demonstrate a correlation between the angular distribution of token hidden state vectors and the resulting gradient, revealing a learning preference for data exhibiting higher angle concentration. Inspired by this finding, we propose GAIN-RL, a Gradient-driven Angle-Informed Navigated RL framework. By leveraging the model's intrinsic angle concentration signal, GAIN-RL dynamically selects training data in each epoch, ensuring consistently impactful gradient updates and thus significantly enhancing overall training efficiency. Empirical evaluations show that GAIN-RL (GRPO) achieves over a 2.5x acceleration in training efficiency across diverse mathematical and coding tasks and varying model scales. Furthermore, GAIN-RL (GRPO)'s efficient sampling yields data-efficient training, achieving better performance with half the original data compared to vanilla GRPO with full training data. Code is realsed at https://github.com/wangqinsi1/GAINRL/tree/main."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 2 ] . [ 1 1 8 2 2 0 . 6 0 5 2 : r Angles Dont Lie: Unlocking Training-Efficient RL Through the Models Own Signals Qinsi Wang1 Jinghan Ke2 Hancheng Ye1 Yueqian Lin1 Yuzhe Fu1 Jianyi Zhang1 Kurt Keutzer2 Chenfeng Xu2 Yiran Chen1 2University of California, Berkeley 1Duke University"
        },
        {
            "title": "Abstract",
            "content": "Current Reinforcement Fine-tuning (RFT) paradigms for Large Language Models (LLMs) suffer from sample inefficiency due to the redundant exposure of identical queries under uniform data sampling. While previous work has explored curriculum learning via heuristic difficulty metrics, these strategies exhibit limitations by neglecting the intrinsic learning signals generated by the model itself, thus leading to suboptimal training regimes. In this paper, we identify model-inherent signal termed angle concentration that effectively reflects an LLMs capacity to learn from specific data. We theoretically and empirically demonstrate correlation between the angular distribution of token hidden state vectors and the resulting gradient, revealing learning preference for data exhibiting higher angle concentration. Inspired by this finding, we propose GAIN-RL, Gradient-driven Angle-Informed Navigated RL framework. By leveraging the models intrinsic angle concentration signal, GAIN-RL dynamically selects training data in each epoch, ensuring consistently impactful gradient updates and thus significantly enhancing overall training efficiency. Empirical evaluations show that GAIN-RL (GRPO) achieves over 2.5 acceleration in training efficiency across diverse mathematical and coding tasks and varying model scales. Furthermore, GAIN-RL (GRPO)s efficient sampling yields data-efficient training, achieving better performance with half the original data compared to vanilla GRPO with full training data. Code is realsed at https://github.com/wangqinsi1/GAINRL/tree/main."
        },
        {
            "title": "Introdction",
            "content": "Since the emergence of groundbreaking Reinforcement Learning Fine-tuning (RFT) techniques exemplified by Deepseek-R1 [1] and OpenAIs O1 [2], significant attention has converged on leveraging these approaches to enhance performance, notably in mathematical reasoning [3, 4, 5] and code generation tasks [6, 7]. This interest has catalyzed the development of numerous algorithmic optimization methods, including GRPO [8], ReMax [9], and Reinforce++ [10]. Despite such remarkable progress, critical challenges persist: RFT remains hindered by persistent issues of low sample efficiency and prohibitively high computational costs. For instance, the GRPO fine-tuning phase on Qwen 2.5-7B (Ray + vLLM) still consumed roughly 240 GPU hours (16 H100-80 GB for 15h) to complete only 100 steps over 8k samples [11]. This low sample efficiency prompts the question: Is it truly necessary to repeatedly expose every data point to the model hundreds of times? Our answer is No. Humans adaptively learn by focusing on what they dont yet understand, rather than rote repetition of simple concepts. We extend this insight into reinforcement learning fine-tuning for LLM reasoning models from the perspective of data manipulation. Manipulating the data is crucial for accelerating data-driven LLM training. Existing data manipulation techniques fall into two main categories: Sample selection methods, such as LIMO [12] and S1 [13], Corresponding authors. Preprint. Under review. Figure 1: Overview of GAIN-RL. GAIN-RL consists of three steps: (1)Angle-based Data Reordering: Before training, the model pre-fills all data and ranks them by the combined angle concentration signals: Cinter + Cintra. (2)Gaussian-based Data Sampling: During training, each epoch begins by sampling reordered data using Gaussian distribution. (3)Dynamic Probability Update: Epoch-wise accuracy and angle concentration are collected to dynamically update Âµt+1. GAIN-RL guides the model to focus on high-angle, high-loss data, promoting effective gradients and faster convergence. have shown that training on carefully curated subset of high-quality data can improve performance. Separately, data ordering strategies, like ADARFT [14], have demonstrated that dynamically adjusting data difficulty during training can accelerate convergence. However, existing approaches suffer from two fundamental limitations that hinder their effectiveness in RFT. First and foremost, current methods neglect the intrinsic characteristics of the models themselves. They rely on fixed, model-agnostic criteriasuch as difficulty or diversityto evaluate data without accounting for how the target model itself perceives the data. We point out that different models interpret the same problem in markedly different ways. As shown in Fig. 2, different models produce diverging accuracy distributions on the same set of data, indicating that one-size-fits-all difficulty measures can lead to suboptimal training outcomes. Second, existing methods suffer from high data preprocessing costs. For instance, S1 and LIMO necessitate running large-scale models (e.g., Qwen2.5-Math-7B) across entire datasets to compute quality and difficulty scores. Similarly, curriculum learning often relies on manual annotation or expert-defined difficulty labels. These resource-intensive preprocessing steps significantly limit the scalability and practical responsiveness of these approaches. We aim to attack the aforementioned challenges and propose model-informed signal that (1) reflects the learning capacity of specific model on particular data, (2) incurs minimal computational costs, and (3) maintains generalizability across diverse models and datasets. Achieving such requirements is inherently challenging, as accurately capturing model-data interactions typically needs resourceintensive decoding stage. To overcome this challenge, we explore three key questions in this work: 1. Which signal should we focus on? By reformulating the gradient expressions, we find that the cosine similarity between token hidden states during inference (hereafter referred to as angle concentration) directly influences gradient norm, underscoring it as critical signal. 2. What are the characteristics of the signal? By tracking the layer-wise evolution, we observe transition from inter-segment to intra-segment angle concentration, which jointly facilitates information flow and constitutes the Layer-wise Angle Concentration Pattern. 3. How can these signals accelerate training? By monitoring angle concentration throughout training, we observe continuous convergence of intraand inter-segment angles over epochs, revealing an Epoch-wise Angle Concentration Pattern. Moreover, the model preferentially learns samples with higher angle concentration before those with lower concentration, indicating Data-wise Angle Concentration Pattern. These patterns highlight the models data-learning preferences and can be leveraged to accelerate training. Building upon these insights, we propose GAIN-RL, Gradient-driven Angle-Informed Navigated Reinforcement Learning Framework, illustrated in Fig. 1. GAIN-RL comprises three primary components: Data Reordering, Data Sampling and Probability Update. Before training, we reorder 2 Figure 2: Visualization of models responses. We evaluate the first 100 GSM8K questions by having Qwen2.5-0.5B-Instruct and LLaMA3.2-1B-Instruct each generate 10 answers per question, recording the number of correct responses. ChatGPT-4o rates question ease on 010 scale (10 = easiest). the training data based on model-informed angular concentration to enhance learning efficiency. During training, dynamic Gaussian probability sampling strategy progressively guides the model towards data with lower angular concentration, with the pace adjusted according to real-time accuracy and angular signals. Notably, our data preprocessing involves only the inexpensive pre-filling stage, requiring under 10 minutes for over 7,000 samples. Overall, GAIN-RL provides plug-and-play, broadly applicable, and data-efficient reinforcement learning solution. Experiments validate that GAIN-RL accelerates training by over 2.5 and, remarkably, surpasses full-sample performance using only half the datademonstrating its powerful capability in driving data-efficient RFT. In summary, our key contributions include: We propose that the angle concentration serves as critical signal influencing gradient norm during training, thus predicting the models ability to learn specific data samples. We demonstrate that angle concentration intrinsically reflects information propagation during inference and learning dynamics during training, and explicitly reveals Layerwise, Epoch-wise, and Data-wise Angle Concentration Patterns. We introduce GAIN-RL, framework that dynamically allocates training data per epoch based on angle concentration signals. GAIN-RL is the first framework to utilize intrinsic model signals for data scheduling, providing novel paradigm for efficient RFT."
        },
        {
            "title": "2 Model-Informed Data Evaluation Signals",
            "content": "With the goal of identifying signals that can effectively reflect specific models learning capability for data, in this section, we explore three key questions: (1) Which signal should we focus on? (2) What are the characteristics of this signal? (3) How can this signal be leveraged to accelerate training? 2.1 Which Signals Should We Focus On? As mentioned in Sect. 1, obtaining model feedback via decoding is computationally expensive. In contrast, the pre-filling stage incurs significantly lower costs, as it requires only single forward pass. Intuitively, we hope to identify signals from the pre-filling stage that can inform the training process. Model training is driven by incremental gradient updates. Hence, to investigate how forward signals influence the backward process, we begin by examining and reformulating the gradient representation. Given weight matrix Rdh, the hidden states of input can be denoted as Rmd, where consists of tokens, and the hidden state of the i-th token is represented as xi. The output activation is denoted as = xW , where Rmh. Assuming the loss function has gradient aL Rmh with respect to the activation a, the gradient of with respect to is given by = aL = xi (cid:0)aL(cid:1) i, (1) (cid:88) i=1 where (aL)i denotes the gradient of the loss with respect to the i-th activation vector ai. To more precisely quantify the magnitude of the gradients, we consider the Frobenius norm of L. Leveraging the linearity of the Frobenius inner product, L2 (cid:88) can be expanded as: (cid:10)xi L2 (cid:0)aL(cid:1) (cid:0)aL(cid:1) (cid:0)aL(cid:1) (cid:0)aL(cid:1) (cid:28) (cid:88) (cid:88) (cid:88) xj xi = (cid:29) (cid:11) = . (2) i, xj , i=1 j=1 i= j=1 3 Next, to simplify Eq. 2, we utilize the compatibility between the Frobenius inner product and the matrix outer product. In particular, for any u, Rd and v, Rh, the following identity holds: (cid:10)u v, z(cid:11) = tr(cid:0)(u v)(w z)(cid:1) = (uw) (vz) , (3) where tr denotes the matrix trace operator. Applying this identity term-wise in Eq. 2 gives L2 = (cid:88) (cid:88) (cid:0)xix i=1 j=1 (cid:1)(cid:0)(aL)i(aL) (cid:1) = (cid:88) (cid:88) i=1 j=1 xixj cos Î¸i,j (cid:0)(aL)i(aL) (cid:1), (4) where cos Î¸i,j denotes the cosine similarity of the angle between xi and xj. Eq. 4 explicitly reveals that, during inference, both the magnitudes of token hidden states and the angles between them directly influence the gradient values computed during backpropagation. Since the magnitudes of token hidden states are normalized in each layer during inference, they cannot effectively convey useful information. Consequently, in this paper, we specifically focus on exploring the characteristics of relative angles of token hidden states. Furthermore, in the Appendix B.1, we provide proofs demonstrating that the nonlinear transformations in LLM inferenceincluding both attention mechanisms and activation functionsare inherently angle-dependent and continuously modify the angles among token hidden states. We can now answer the first question: A1. We should focus on the relative angles between token hidden states during inference as it fundamentally impacts the gradient Frobenius norm computed during backpropagation. Specifically, the more concentrated the angles between tokens, the larger the gradient norm. 2.2 What Are the Characteristics of This Signal? In the previous subsection, we show that angles between token hidden states directly influence the gradients. To identify the characteristics we should focus on, we explore its layer-wise evolution. Layer-wise Observation. We conducted experiments on the Qwen2.5-0.5b-Instruct model to observe the evolution process across different layers. The experimental results, shown as Fig. 3, clearly illustrate that in the initial layers of the model, no distinct pattern emerges, with the angles primarily determined by the input embeddings. As the layer depth increases, the angles gradually exhibit segmented structure, whereby the hidden states of tokens within the same segment tend to cluster more closely. Upon further examination, we found these segments correspond precisely to distinct parts of the input sequence: the system prompt, few-shot examples, and the question. Eventually, in the final layers, angles between tokens from different segments begin to converge, reaching the highest degree of concentration. We give more vivid demonstration in Fig. 1. Based on the above observations, we introduce Layer-wise Angle Concentration Pattern: during inference, the model first induces intra-segment angle concentration and subsequently promotes inter-segment angle concentration. These two forms of clustering collaboratively facilitate information propagation through the model. detailed demonstration is provided in Appendix C.1. Therefore, assume the length of the input tokens is m. The first tokens constitute the system prompt and the few-shot examples, which are the same across all data samples. The remaining tokens represent the specific question to be answered. The characteristics we should focus on are: Cintra = 1 (m n) (cid:88) (cid:88) i=n+1 j=n+1 cos Î¸i,j, Cinter = 1 (m n)n (cid:88) (cid:88) i=n+1 j=1 cos Î¸i,j, (5) where Cintra measures angle concentration within the question and Cinter measures angle concentration between question tokens and the system prompt and few-shot tokens. We measure both at the final layer, where inter-segment clustering is maximal. Concentration within the system prompt and few-shot tokens is omitted, as it is constant across different questions. Attention-based Explanation. To better understand the observed pattern, we also provide an analytical explanation from attention scores. In general, we find that tokens with higher angle concentration correspond to higher attention scores. Specifically, Cintra represents the strength of attention within the question itself, while Cinter indicates the models ability to follow instructions. Furthermore, the presence of sink attention encourages intra-segment and inter-segment angle concentration. Details can be found in Appendix B.2. Our answer to the second question is: 4 Figure 3: Visualization of Layer-wise Concentration Pattern. Experiment is performed on the Qwen2.5-0.5b-Instruct. In each subplot, the pixel at row i, column represents the cosine similarity of the angle between the i-th and j-th token hidden states of the layer output. Blue, yellow, and gray arrows above the figure represent the tokens of the system prompt, few-shot examples and question, respectively. To better highlight the pattern, values are clipped between the 3rd and 97th percentiles. Figure 4: Visualization of Epoch-wise Concentration Pattern. (Left) Cintra; (Mid) Cinter; (Right) Cintra + Cinter. We train Qwen2.5-0.5B-Instruct and LLaMA3.2-1b-Instruct on GSM8K using GRPO for 250 epochs. To accelerate observation, we use training batch size of 16, generate 4 responses per question, and set the learning rate to 1e-5. After each epoch, we perform pre-filling on the model on the entire dataset and record the angle concentration from the hidden states of the final layer output. A2. The angles between token hidden states show both intra-segment and inter-segment concentrations during inference. In particular, we should pay attention to the final layer as the inter-segment clustering is most pronounced, resulting in the highest overall concentration. 2.3 How Can This Signal Be Leveraged to Accelerate Training? Having established that intra-segment and inter-segment angular concentrations at the final layer are crucial characteristics, we now further examine their evolution during training to deepen our understanding of how they reflect and influence the training progress. Epoch-wise Observation. To track how angular concentrations evolve during training, we perform inference on the same dataset on models of different epochs and monitor three signals: (1) Intraquestion concentration Cintra, (2) Inter-segment concentration Cinter, (3) Combined signal Cintra + Cinter. As illustrated in Fig. 4, we can observe the Epoch-wise Angle Concentration Pattern: during training, both inter-segment and intra-segment angle concentration increase progressively. Additionally, we note that the intra-question concentration initially decreases before subsequently increasing, which suggests the model prioritizes mastering instruction-following capabilities before refining its focus internally on individual questions. These observations validate our hypotheses from the previous subsections, reinforcing that angular concentration effectively mirrors training dynamics. Data-wise Observation. Furthermore, to examine data-wise angle behavior during training, we track the models responses to samples with varying angle concentrations over epochs. As depicted in Fig. 5, surprisingly, the results revealed that during training, the model tends to prioritize learning from higher-angle concentration data before addressing lower-angle concentration data, which we introduce as Data-wise Angle Concentration Pattern. For instance, by epoch 100, questions with maximal angular measurements were almost entirely answered correctly, whereas those with smaller angles remained uncorrected. Note that the angle concentration distribution is measured on the untrained model, indicating that despite its evolution during training, the initial angle concentration of the data provides meaningful guidance for the training process. To understand these patterns, we provide explanations from two perspectives: gradients and neurons. 5 Figure 5: Visualization of Data-wise Concentration Pattern. Experiments are conducted on Qwen2.5-0.5b-Instruct. We first performed pre-filling on 1,000 samples of GSM8K using the untrained model to collect Cintra + Cinter of samples, and plotted their statistical distributions (the histogram in the figure). Then we monitored the model responses to these samples at various epochs and recorded the average number of correct responses from samples in each angle concentration interval (brighter colors indicate higher number of correctly answered samples within the interval). Figure 6: Relationship between neuron activation patterns and accuracy over training. At selected epochs, we collect both activation and answers for the first 1000 GSM8K samples. For each question, we identify the most frequently activated 20% of neurons across all tokens in the final layer, and use their indices to construct binary core-neuron vector. We apply PCA to reduce these vectors to 2D. Each dot represents sample; brighter colors indicate higher answer accuracy for the sample. Gradient-based Explanation. As shown in Eq. 4, gradients are influenced by both angle concentration and loss. Early in training, when losses are relatively uniform across samples, those with higher angle concentration receive stronger gradients and are learned faster. As training continues, angle concentration rises overall: high-angle samples, already mastered, see lower losses; while low-angle samples gain concentration, inherit larger gradients, and are learned next. This creates natural, angle-driven learning progression. Unlike traditional curriculum learning based on task difficulty, angle concentration offers more intuitive, model-centric training signal. Neuron-based Explanation. Consider FFN block with weights Wu and Wd, and activation function SiLU. Given the input denoted as x, the transformation process can be represented as: = xWu, = SiLU(z), = AWd, where and denote the pre-activation and activation outputs, respectively, and represents the output. The gradients of j-th neuron in Wu and Wd are (WuL):,j = (cid:88) i=1 (cid:16) (cid:17) (AL)i,j SiLU(zi,j) xi , (WdL):,j = (cid:88) i=1 Ai,j (yL)i, (6) where SiLU(zi,j) is the derivative of the SiLU activation with respect to zi,j. When zi,j < 0, we have SiLU(zi,j) 0 and SiLU(zi,j) 0. This indicates that the number of gradient components received by neuron is proportional to the frequency of its activation. Further analysis shows that tokens with higher angle concentration activate similar neurons due to shared value patterns (see Appendix B.3). These neurons receive stronger cumulative gradients, making them more effectively trained. Fig. 6 further illustrates how neuron activations converge during training, forming distinct cluster correlated with higher accuracy. Samples activating neurons far from this cluster are harder to learn. Following prior work on neuron specialization, we hypothesize this cluster encodes domain-specific knowledge [15, 16, 17]. Synthesizing the above analyses, we provide an answer to the third question as follows: 6 A3. We should follow the models inherent learning dynamics prioritizing higher-angle concentration data in the early stages and gradually transitioning to lower-angle concentration data. This progression ensures more effective gradient updates and improves training efficiency."
        },
        {
            "title": "3 GAIN-RL Framework",
            "content": "Based on the conclusions from the Sect. 2, we introduce GAIN-RL, Gradient-driven AngleInformed Navigated-data RL framework, plug-and-play training acceleration framework compatible with any model and dataset, incurring negligible costs. GAIN-RL consists of three components: Data Reordering Based on Angular Concentration. Guided by our findings in Sect. 2.3that models preferentially learn from data with higher angular concentrationwe order the training data by angular concentration before training to improve efficiency. Given model and dataset = {d1, d2, . . . , dN }, we first perform pre-filling on all data samples using to collect angular information. Subsequently, the data is sorted based on the combined signal at the final layer output, CM (di) = CM intra(di) + CM inter(di), Ds = SortM (D; CM (di), descending) (7) the sorted dataset Ds is directly employed in subsequent training. Notably, this sorting process is computationally efficient, as it only requires the pre-filling step, which can be efficiently batched. For example, sorting approximately 7000 samples of GSM8K with the Qwen-2.5-0.5-instruct model takes less than 10 minutes on single NVIDIA A100 GPU. In contrast, previous approaches often required manual annotation or generation via large models, typically consuming several days. Data Sampling Guided by Gaussian Probability. During training, we consistently prioritize data with higher angular concentration. At the tth training step, we assign sampling probabilities to each data sample in the sorted dataset Ds = {ds } based on Gaussian distribution parameterized by Âµt and Ït. subset d(t) of size is then sampled according to, 2, . . . , ds 1, ds Pt(ds ) = 1 Zt (cid:18) exp (i Âµt)2 2Ï2 (cid:19) , d(t) Sample(Ds; Pt, n), (8) where Zt is normalization constant ensuring that probabilities sum to unity. Employing probabilistic sampling instead of strictly sequential sampling enhances the stability and robustness of training. Probability Update Based on Accuracy and Angular Signals. The Gaussian mean Âµt sets the peak-sampling region. We initialize Âµ0 = 0 to prioritize high-angle concentration data, then gradually increase Âµt as the model masters these samples, shifting focus to lower-angle concentration ones. At each step, Âµt is updated from the batch d(t) using its average accuracy and angle concentration, Acc(t) = 1 n (cid:88) i=1 AccMt(d(t) ), C(t) = 1 n (cid:88) i=1 CMt(d(t) ), (9) where Mt represents the model at the tth training step, and AccMt(d(t) ) denote accuracy and angular concentration signals. Notably, computing these signals incurs no additional cost as model inference is inherently performed during training. The update rule for Âµt+1 is given by, (cid:17) Î±(Acc(t) Î²) ) and CMt(d(t) Î³ C(t)(cid:17) Âµt+1 = Âµt + tanh tanh (10) + (cid:16) (cid:16) , 2 where Î± adjusts accuracy sensitivity, Î² sets the target accuracy, and Î³ controls angle sensitivity (see Sect. 4 for guidelines). This update strategy maintains high-gradient training by targeting samples near the desired accuracy while gradually incorporating harder, lower-angle data efficiently. Appendix F.1 experiments reveal that weighting the signal as CM inter can further boost performance. For simplicity and broad applicability, we adopt the unweighted signal here and leave signal optimization to future work. Our main goal is to highlight angle concentration as key signal. intra + CM"
        },
        {
            "title": "4 Experiments",
            "content": "To evaluate the effectiveness of GAIN-RL, we conducted comprehensive experimental study on five levels: (1) Training Efficiency(Sect. 4.1), (2) Data Efficiency(Sect. 4.2), (3) RL Algorithms Generalization(Sect. 4.3), (4) Performance on Individual Tasks(Sect. 4.4), and (5) Ablation Studies(Sect. 4.5). For detailed descriptions of the used models and datasets, please refer to the Appendix E. 7 Table 1: Comparison of Pass@1 accuracy on Math benchmarks. We report the accuracy at epoch 200 and the number of epochs needed to match vanilla GRPOs 200-epoch accuracy (Epo@Same Acc). ADARFT(GRPO) and GAIN-RL(GRPO) denote GRPO combined with the respective optimization. Hardware Efficiency Speed Up Task Performance (200 Epoch) Olympiad Bench[20] Epo@ Same Acc Experiments Setting Minerva Math[21] AIME 24 [19] AMC 23 [18] GSM8K [3] Math [4] Method Model Avg Qwen 2.5 Math 1.5B Instruct LLaMA 3.2 3B Instruct Qwen 2.5 Math 7B Instruct GRPO ADARFT(GRPO) GAIN-RL(GRPO) GRPO ADARFT(GRPO) GAIN-RL(GRPO) GRPO ADARFT(GRPO) GAIN-RL(GRPO) 84.15 85.52 88.09 74.60 78.01 76.04 91.96 92.65 93.71 64.40 66.00 67. 40.20 39.00 42.00 68.40 70.20 72.80 38.55 40.96 43.37 19.28 18.07 21.69 40.96 42.17 45.78 10.00 13.33 13. 6.67 6.67 6.67 10.00 10.00 13.33 25.63 26.07 27.26 11.70 12.89 14.22 25.78 25.33 26.81 13.97 14.71 16. 8.46 8.56 10.29 20.22 21.32 23.53 39.95 41.09 42.63 26.8 27.2 28.5 42.89 43.61 46.33 200 150 200 140 80 200 150 70 1 1.33 2.50 1 1.43 2.50 1 1.33 2.86 Table 2: Comparison of model performance on Code benchmarks. ADARFT is not compared because DeepCoder lacks the difficulty coefficients required by ADARFT. Experiments Setting Task Performance (200 Epoch) Hardware Efficiency Model Method LCB [6] Pass@ LCB Pass@8 Codeforces [22] Codeforces Humaneval+ [23] Avg Pass@1 Pass@8 Pass@1 Epo@ Same Acc Qwen 2.5 Coder 3B Instruct GRPO GAIN-RL(GRPO) 10.8 12.8 21.5 25.1 5.15 6.14 17.8 18. 78.3 81.5 26.7 28.8 200 110 Speed Up 1 1.81 Training and Hyperparameter Settings. We set the target accuracy Î² = 0.5 to maintain strong gradients during training. Sensitivity parameters Î± = 2 (for accuracy) and Î³ = 0.5 (for angle concentration) are tuned on validation set to ensure stable learning, keeping the tanh function approximately linear over Acc(t) [0, 1] and C(t) [1, 1]. Training is conducted using GRPO with batch size and sampling number of 1024, implemented on the VerL framework with 8 NVIDIA A100 GPUs. Additional details are provided in the Appendix E. Baseline Settings. We compare GAIN-RL with the vanilla GRPO and ADARFT[14], state-of-theart dynamic curriculum learning method in RL. The training settings are the same for all methods. 4.1 Training Efficiency of GAIN-RL To evaluate the training efficiency of GAIN-RL, we use the DeepScaleR [24] and DeepCoder [25] datasets to train models for math and code tasks, respectively. They both cover diverse problem types and difficulty levels. Performance is tested on six math and three code benchmarks. Model Performance. We report the performance of each method at 200 epochs during training. As demonstrated in Tab. 1 and Tab. 2, models trained with GAIN-RL(GRPO) exhibited superior performance across nearly all math and code datasets compared to models trained by vanilla GRPO and ADARFT(GRPO). Notably, with the Qwen2.5-Math-1.5B-Instruct model, GAIN-RL(GRPO) increased average accuracy across six math datasets from 39.95% and 41.09% to 42.63%, representing gains of 2.68% and 1.54%, respectively. Furthermore, the performance improvement observed on LLaMA3.2-3B-Instruct demonstrates that GAIN-RL(GRPO) generalizes across different model families. These results highlight both model-level and task-level generality of our method. Hardware Efficiency. To evaluate hardware efficiency, we report the number of epochs required for each method to reach the performance of vanilla GRPO at 200 epochs. As shown in Tab. 1 and Tab. 2, GAIN-RL(GRPO) requires approximately only half the epochs across various models and tasks. Specifically, for the Qwen2.5-Math-1.5B-Instruct and LLaMA3.2-3B-Instruct models, GAINRL(GRPO) achieves approximately 2.5 speedup, significantly outperforming ADARFT(GRPO) speedups of 1.33 and 1.43, respectively. Fig. 7 further visualizes performance trajectories during training, where GAIN-RL(GRPO) consistently demonstrates faster convergence and superior performance at every epoch. As analyzed in Sect. 2, the acceleration mainly stems from the ability of our method to maintain strong gradient signals throughout training, leading to faster learning. Figure 7: Learning Dynamics of Different Methods on (Left) Qwen2.5-Math-1.5b-Instruct and (Right) LLaMA-3.2-3b-Instruct. The y-axis shows average accuracy over GSM8K, Math, AMC 23, AIME 24, OlympiadBench, and Minerva Math. Performance is evaluated every 5 epochs. Figure 8: Data Efficiency Analysis of GAIN-RL. We sampled half of the data from the math dataset using three distinct sampling methods to train the Qwen2.5-0.5b-instruct model using GAINRL(GRPO): (1) High Angle Concentration-biased Sampling, (2) Uniform Sampling, and (3) Low Angle Concentration-biased Sampling. The first three figures illustrate the distribution of the sampled data (highlighted in bright colors) within the overall dataset (grey) under each sampling scenario. The rightmost figure presents the performance of models trained on differently distributed data, where GAIN-RL (Full) and GRPO (Full) denote results obtained by training with the complete dataset. 4.2 Data Efficiency of GAIN-RL To further investigate whether GAIN-RL can enhance data effectiveness in RFT, we fine-tuned the Qwen2.5-0.5b-Instruct model on the Math dataset. We sampled half of the training dataset using three distinct distribution strategies for GAIN-RL(GRPO) training: (1) Uniform Sampling, where half of the training data was randomly selected; (2) High Angular Concentration-biased Sampling, where data points were sampled based on their angular concentration scores, assigning linear sampling probability with slope of 1 (prioritizing data points with higher angular concentration); and (3) Low Angular Concentration-biased Sampling, applying linear sampling probability with slope of -1. 4.3 RL Algorithms Generalization To verify the algorithm generalization of GAIN-RL, we also combine GAIN-RL with PPO. We evaluate the final performance and hardware efficiency. Results in Tab. 3 highlight substantial gains in both performance and hardware efficiency with GAIN-RL(PPO), achieving an average of 2.2 training speedup across three mathematical benchmarks. This consistent benefit arises because gradient updates are fundamental across RL algorithms, validating GAIN-RLs universal acceleration capability. Table 3: Performance of GAIN-RL combined with PPO. Qwen2.5-0.5b-Instruct is trained and evaluated on three datasets. Implementation details are in Appendix E. Task Performance Hardware Efficiency Dataset PPO GAIN-RL (PPO) Epo@ 200Acc Speed Up GSM8K 42.46 32.15 7. Math AMC 23 45.26 34.80 8.43 80 100 100 2.5 2.0 2.0 We compared the performance of models trained under these sampling strategies. The experimental results, illustrated in Fig. 8, reveal that when using only half of the data, the model trained with High Angular Concentration-biased Sampling outperformed even the model trained with the full dataset. Based on the analysis in Sect. 2.3, we hypothesize this phenomenon occurs as data points with low angular concentration tend to produce smaller gradient updates and activate dispersed neural regions. Consequently, excluding these data points may enhance training efficiency. This insight provides new guidance for data selection: prioritizing high angular concentration data and discarding low angular concentration data can significantly improve data effectiveness of RFT. These findings highlight GAIN-RLs potential and effectiveness in guiding data selection. With uniform sampling, Figure 9: Fine-tuning Performance on Single Tasks. (Left) GSM8k. (Mid) Math. (Right) AMC 23. Models are trained on the training sets and evaluated on their validation sets. ADARFT is excluded from GSM8K due to missing difficulty coefficients. Figure 10: (Left) Ablation Study of Module Components. (Right) Small Batch Scalability Test. Experiments use Qwen2.5-0.5b-Instruct on Math training set, evaluate on the test set every 20 epochs. GAIN-RL using half the data performs slightly worse than with full data but remains on par with vanilla GRPO. In contrast, biased sampling toward low-angle concentration leads to unstable and poor results, consistent with our analysis of weaker gradients and dispersed neuron activations. 4.4 Performance on Individual Tasks To further demonstrate the generality of GAIN-RL(GRPO), we also evaluate its performance on single-task RFT, which demands higher precision in data ordering and selection due to narrower difficulty ranges. Using Qwen2.5-0.5b-Instruct, we fine-tuned separately on GSM8K, MATH, and AMC training sets and evaluated on their test sets. Results in Fig. 9 indicate that GAIN-RL(GRPO) consistently yielded higher performance and efficiency. Specifically, on GSM8K, GAIN-RL(GRPO) achieved 3.33 training speedup and 4.72% final accuracy improvement. On the more challenging MATH and AMC 23 datasets, GAIN-RL(GRPO) also achieves 2.5 and 2 speedups, respectively. In contrast, ADARFT(GRPO) provided less improvement due to its fixed difficulty scoring, which may not align precisely with actual model-perceived difficulty. In contrast, by leveraging modelinformed angle signals, GAIN-RL(GRPO) can predict learning priorities more accurately. 4.5 Ablation Studies Module Ablation. To better understand the contribution of different components in GAIN-RL, we conduct ablation studies, including (1) Data ordering + Accuracy-based probability updates, (2) Data ordering + Angle-based probability updates, and (3) Accuracy-only (no ordering, discarding fully correct data to reduce training costs). As shown in Fig. 10 (left), all ablation variants exhibit degraded performance. The Accuracy-only group exhibits marked performance drop, initially improving but later declining due to forgetting from prematurely discarding data. This underscores that data ordering and sampling constitute GAIN-RL(GRPO)s primary performance advantages. Moreover, while both Data ordering + Accuracy-based probability updates and Data ordering + Angle-based probability updates show steady improvements over vanilla GRPO, they do not match the performance of GAIN-RL(GRPO), as each only captures single aspect of gradient. Small-Batch Scalability Test. To validate the scalability of GAIN-RL under reduced training batch sizes, we evaluated the models performance with varying batch sizes. Specifically, we conducted experiments using the Qwen2.5-0.5b-instruct model on the Math dataset with training batch sizes of 512, 768, and 1024, respectively. The experimental outcomes, depicted in Fig. 10 (right), illustrate 10 consistent and stable performance improvement across epochs for all batch sizes. Remarkably, even with the batch size reduced to half of the original size (n=512), which significantly reduces computational time and memory usage per epoch, GAIN-RL maintained performance comparable to vanilla GRPO. These results demonstrate that GAIN-RL effectively scales with smaller batch sizes, offering the flexibility to accelerate training by lowering batch size, with only minor performance degradation, especially beneficial under limited computational or memory resources."
        },
        {
            "title": "5 Conclusion",
            "content": "We propose GAIN-RL, novel Reinforcement Learning Fine-tuning framework that leverages the angle concentration signals to dynamically allocate training data at each epoch. GAIN-RL leverage the models intrinsic angle concentration signal to dynamically selects training data in each epoch, ensuring consistently impactful gradient updates and thus significantly enhancing overall training efficiency. Furthermore, our empirical results further show that GAIN-RL (GRPO) achieves over 2.5 acceleration in training efficiency across diverse mathematical and coding tasks and varying model scales. Overall, GAIN-RL introduces novel paradigm for training-efficiency RFT, highlighting how model-centric data-processing approaches can remedy the sub-optimality of current RFT methods and further elevate their effectiveness."
        },
        {
            "title": "References",
            "content": "[1] Daya Guo, Dejian Yang, Haowei Zhang, Junxiao Song, Ruoyu Zhang, Runxin Xu, Qihao Zhu, Shirong Ma, Peiyi Wang, Xiao Bi, et al. Deepseek-r1: Incentivizing reasoning capability in llms via reinforcement learning. arXiv preprint arXiv:2501.12948, 2025. [2] Aaron Jaech, Adam Kalai, Adam Lerer, Adam Richardson, Ahmed El-Kishky, Aiden Low, Alec Helyar, Aleksander Madry, Alex Beutel, Alex Carney, et al. Openai o1 system card. arXiv preprint arXiv:2412.16720, 2024. [3] Karl Cobbe, Vineet Kosaraju, Mohammad Bavarian, Mark Chen, Heewoo Jun, Lukasz Kaiser, Matthias Plappert, Jerry Tworek, Jacob Hilton, Reiichiro Nakano, et al. Training verifiers to solve math word problems. arXiv preprint arXiv:2110.14168, 2021. [4] Dan Hendrycks, Collin Burns, Saurav Kadavath, Akul Arora, Steven Basart, Eric Tang, Dawn Song, and Jacob Steinhardt. Measuring mathematical problem solving with the math dataset. arXiv preprint arXiv:2103.03874, 2021. [5] Martin Kuo, Jianyi Zhang, Aolin Ding, Qinsi Wang, Louis DiValentin, Yujia Bao, Wei Wei, Hai Li, and Yiran Chen. H-cot: Hijacking the chain-of-thought safety reasoning mechanism to jailbreak large reasoning models, including openai o1/o3, deepseek-r1, and gemini 2.0 flash thinking. arXiv preprint arXiv:2502.12893, 2025. [6] Naman Jain, King Han, Alex Gu, Wen-Ding Li, Fanjia Yan, Tianjun Zhang, Sida Wang, Armando Solar-Lezama, Koushik Sen, and Ion Stoica. Livecodebench: Holistic and contamination free evaluation of large language models for code. arXiv preprint arXiv:2403.07974, 2024. [7] Qinsi Wang and Sihai Zhang. Dgl: Device generic latency model for neural architecture search on mobile devices. IEEE Transactions on Mobile Computing, 23(2):19541967, 2023. [8] Zhihong Shao, Peiyi Wang, Qihao Zhu, Runxin Xu, Junxiao Song, Xiao Bi, Haowei Zhang, Mingchuan Zhang, YK Li, Wu, et al. Deepseekmath: Pushing the limits of mathematical reasoning in open language models. arXiv preprint arXiv:2402.03300, 2024. [9] Ziniu Li, Tian Xu, Yushun Zhang, Zhihang Lin, Yang Yu, Ruoyu Sun, and Zhi-Quan Luo. Remax: simple, effective, and efficient reinforcement learning method for aligning large language models. arXiv preprint arXiv:2310.10505, 2023. [10] Jian Hu. Reinforce++: simple and efficient approach for aligning large language models. arXiv preprint arXiv:2501.03262, 2025. 11 [11] Weihao Zeng, Yuzhen Huang, Qian Liu, Wei Liu, Keqing He, Zejun Ma, and Junxian He. Simplerl-zoo: Investigating and taming zero reinforcement learning for open base models in the wild. arXiv preprint arXiv:2503.18892, 2025. [12] Yixin Ye, Zhen Huang, Yang Xiao, Ethan Chern, Shijie Xia, and Pengfei Liu. Limo: Less is more for reasoning. arXiv preprint arXiv:2502.03387, 2025. [13] Niklas Muennighoff, Zitong Yang, Weijia Shi, Xiang Lisa Li, Li Fei-Fei, Hannaneh Hajishirzi, Luke Zettlemoyer, Percy Liang, Emmanuel CandÃ¨s, and Tatsunori Hashimoto. s1: Simple test-time scaling. arXiv preprint arXiv:2501.19393, 2025. [14] Taiwei Shi, Yiyang Wu, Linxin Song, Tianyi Zhou, and Jieyu Zhao. Efficient reinforcement finetuning via adaptive curriculum learning. arXiv preprint arXiv:2504.05520, 2025. [15] Qinsi Wang, Saeed Vahidian, Hancheng Ye, Jianyang Gu, Jianyi Zhang, and Yiran Chen. Coreinfer: Accelerating large language model inference with semantics-inspired adaptive sparse activation. arXiv preprint arXiv:2410.18311, 2024. [16] Zeping Yu and Sophia Ananiadou. Neuron-level knowledge attribution in large language models. arXiv preprint arXiv:2312.12141, 2023. [17] Chaojun Xiao, Zhengyan Zhang, Chenyang Song, Dazhi Jiang, Feng Yao, Xu Han, Xiaozhi Wang, Shuo Wang, Yufei Huang, Guanyu Lin, et al. Configurable foundation models: Building llms from modular perspective. arXiv preprint arXiv:2409.02877, 2024. [18] Proceedings of the ACM Web Conference 2023, Austin, TX, USA, 2023. Association for Computing Machinery. URL https://dl.acm.org/doi/proceedings/10.1145/3543507. [19] 2024 american invitational mathematics examination (aime). https://www.maa.org/ math-competitions, 2024. Accessed: 2025-05-14. [20] Chaoqun He, Renjie Luo, Yuzhuo Bai, Shengding Hu, Zhen Leng Thai, Junhao Shen, Jinyi Hu, Xu Han, Yujie Huang, Yuxiang Zhang, et al. Olympiadbench: challenging benchmark for promoting agi with olympiad-level bilingual multimodal scientific problems. arXiv preprint arXiv:2402.14008, 2024. [21] Aitor Lewkowycz, Anders Andreassen, David Dohan, Ethan Dyer, Henryk Michalewski, Vinay Ramasesh, Ambrose Slone, Cem Anil, Imanol Schlag, Theo Gutman-Solo, et al. Solving quantitative reasoning problems with language models. Advances in Neural Information Processing Systems, 35:38433857, 2022. [22] Mikhail Mirzayanov. Open codeforces rating system. https://codeforces.com/blog/ entry/20762, 2015. Accessed: 2025-05-14. [23] Mark Chen, Jerry Tworek, Heewoo Jun, Qiming Yuan, Henrique Ponde De Oliveira Pinto, Jared Kaplan, Harri Edwards, Yuri Burda, Nicholas Joseph, Greg Brockman, et al. Evaluating large language models trained on code. arXiv preprint arXiv:2107.03374, 2021. [24] Michael Luo, Sijun Tan, Justin Wong, Xiaoxiang Shi, William Tang, Manan Roongta, Colin Cai, Jeffrey Luo, Tianjun Zhang, Li Erran Li, et al. Deepscaler: Surpassing o1-preview with 1.5 model by scaling rl. Notion Blog, 2025. [25] Michael Luo, Sijun Tan, Roy Huang, Xiaoxiang Shi, Rachel Xin, Colin Cai, Ameen Patel, Alpay Ariyak, Qingyang Wu, Ce Zhang, et al. Deepcoder: fully open-source 14b coder at o3-mini level, 2025. [26] Qinsi Wang, Hancheng Ye, Ming-Yu Chung, Yudong Liu, Yueqian Lin, Martin Kuo, Mingyuan Ma, Jianyi Zhang, and Yiran Chen. Corematching: co-adaptive sparse inference framework with token and neuron pruning for comprehensive acceleration of vision-language models. arXiv preprint arXiv:2505.19235, 2025. [27] Wang Qinsi, Jinghan Ke, Masayoshi Tomizuka, Kurt Keutzer, and Chenfeng Xu. Dobi-svd: Differentiable svd for llm compression and some new perspectives. In The Thirteenth International Conference on Learning Representations. 12 [28] Judy Hanwen Shen, Archit Sharma, and Jun Qin. Towards data-centric rlhf: Simple metrics for preference dataset comparison. In NeurIPS, 2024. URL https://arxiv.org/abs/2409. 09603. [29] Xiangning Chen, Chen Liang, Da Huang, Esteban Real, Kaiyuan Wang, Hieu Pham, Xuanyi Dong, Thang Luong, Cho-Jui Hsieh, Yifeng Lu, et al. Symbolic discovery of optimization algorithms. Advances in neural information processing systems, 36:4920549233, 2023. [30] Jean Kaddour, Oscar Key, Piotr Nawrot, Pasquale Minervini, and Matt Kusner. No train no gain: Revisiting efficient training algorithms for transformer-based language models. Advances in Neural Information Processing Systems, 36:2579325818, 2023. [31] Dante Everaert and Christopher Potts. Gio: Gradient information optimization for training dataset selection. arXiv preprint arXiv:2306.11670, 2023. [32] Logan Engstrom, Axel Feldmann, and Aleksander Madry. Dsdm: Model-aware dataset selection with datamodels. arXiv preprint arXiv:2401.12926, 2024. [33] Cody Coleman, Christopher Yeh, Stephen Mussmann, Baharan Mirzasoleiman, Peter Bailis, Percy Liang, Jure Leskovec, and Matei Zaharia. Selection via proxy: Efficient data selection for deep learning. arXiv preprint arXiv:1906.11829, 2019. [34] Rafael Rafailov, Archit Sharma, Eric Mitchell, Christopher Manning, Stefano Ermon, and Chelsea Finn. Direct preference optimization: Your language model is secretly reward model. Advances in Neural Information Processing Systems, 36:5372853741, 2023. [35] Nirjhar Das, Souradip Chakraborty, Aldo Pacchiano, and Sayak Ray Chowdhury. Active preference optimization for sample efficient rlhf. arXiv preprint arXiv:2402.10500, 2024. [36] Yoshua Bengio, JÃ©rÃ´me Louradour, Ronan Collobert, and Jason Weston. Curriculum learning. In Proceedings of the 26th annual international conference on machine learning, pages 4148, 2009. [37] Shuai Li, Kui Jia, Yuxin Wen, Tongliang Liu, and Dacheng Tao. Orthogonal deep neural networks. IEEE transactions on pattern analysis and machine intelligence, 43(4):13521368, 2019. [38] Nitin Bansal, Xiaohan Chen, and Zhangyang Wang. Can we gain more from orthogonality regularizations in training deep networks? Advances in Neural Information Processing Systems, 31, 2018. [39] Aitor Lewkowycz, Anders Andreassen, David Dohan, Ethan Dyer, Henryk Michalewski, Vinay Ramasesh, Ambrose Slone, Cem Anil, Imanol Schlag, Theo Gutman-Solo, et al. Solving quantitative reasoning problems with language models. Advances in Neural Information Processing Systems, 35:38433857, 2022. 13 Organization In this Appendix, we provide in-depth descriptions of the materials that are not covered in the main paper, and report additional experimental results. The document is organized as follows: ARelated Work BTheoretical Supplement B.1 Theoretical Justification of Angle-Dependent Effects in LLMs B.2 Attention-Based Explanation of Layer-wise Angle Concentration Patterns B.3 Neuron-Based Explanation of Data-wise Angle Concentration Patterns CVisualization Results C.1 Layer-wise Angle Concentration Patterns C.2 Data-wise Angle Concentration Patterns DGAIN-RL Algorithm EExperimental Setup FAdditional Experimental Results F.1 Performance of Weighted Signals F.2 Model Performance on Single Task GDiscussion and Future Work"
        },
        {
            "title": "A Related Work",
            "content": "Reinforcement Fine-Tuning (RFT)[8, 10] has demonstrated significant effectiveness in enhancing the reasoning capabilities of large language models [15, 26, 27]. However, despite its promising potential, its low sample efficiency and high computational costs remain critical barriers to the broader adoption of RFT. Recent efforts aimed at addressing these efficiency have primarily focused on algorithmic optimizations and data-centric strategies. Algorithmic optimizations, exemplified by methods such as GRPO [8], REINFORCE++ [10], and ReMax [9], seek to reduce computational complexity by streamlining underlying RL algorithms. Although these methods typically improve efficiency and stability, they often involve inherent trade-offs. For instance, GRPO estimates advantages through relative comparisons within output groups, thereby eliminating the need for value functions. While this approach reduces complexity and the reliance on critics, it can introduce instability due to increased noise in advantage estimation, higher variance in updates, and greater sample requirements. In parallel, data-centric strategies [28, 29, 30, 31, 32, 33] have emerged as promising alternatives for efficient fine-tuning. For example, [28] provides the first quantitative audit of preference datasets, introducing metrics for scale, noise, and information density that expose quality bottlenecks before any policy update. Direct Preference Optimization (DPO) [34] simplifies the entire loop by replacing on-policy RL with closed-form classification loss, making the choice of high-value preference data the primary driver of alignment quality. To further reduce annotation cost, Active Preference Optimization [35] casts RLHF as an active-learning bandit, adaptively querying only the prompts expected to maximize reward-model improvement. The latest data-centric strategies can be categorized based on their approach to data manipulation: data selection and data sequencing. Data selection techniques involve filtering extensive datasets to retain only small subset of high-quality training data based on predefined metrics. Methods such as LIMO[12] and s1[13] have demonstrated that carefully curated small supervised fine-tuning datasets can achieve robust performance using orders of magnitude less data. On the other hand, data sequencing strategies[36] enhance model learning speed by rearranging the order of training data within existing datasets. Approaches like ADARFT[14] have shown that dynamically selecting data for each epoch can effectively accelerate the training process. Nevertheless, existing data-centric strategies have generally failed to account for the unique characteristics of different models, applying uniform data handling procedures across diverse model architectures. Such uniformity can lead to suboptimal outcomes because models differ significantly in their sensitivity and response to the same datasets. To overcome this challenge, this paper aims to identify intrinsic signals within models that can reflect their perceptual capabilities toward data, thereby enabling tailored data strategies without incurring substantial additional costs."
        },
        {
            "title": "B Theoretical Supplement",
            "content": "In this section, we provide the theoretical explanation supporting the main text in Section 2. Specifically, we elaborate on the Angle-Dependent Effects of Attention and Activation (corresponding to Section 2.1 of the main text), the Attention-Based Explanation of Layer-wise Angle Concentration Patterns (corresponding to Section 2.2 of the main text), and the Neuron-Based Explanation of Data-wise Angle Concentration Patterns (corresponding to Section 2.3 of the main text). B.1 Theoretical Justification of Angle-Dependent Effects in LLMs In this section, we demonstrate through derivation of the LLM computational process that the nonlinear operations in LLMsnamely attention and activation computationsare inherently dependent on the angles between the input hidden states. To support this claim, we first introduce two empirical assumptions based on observation: Observation 1. Wq and Wk are nearly approximately orthogonal to each other, i.e., Wq Î¸ is constant. (cid:0)Wo, Wu, Wd constant. Observation 2. For activation function output vectors Ai and AM , cos((Ai, AM )) is proportional to the number of intersections of activated neurons, i.e., (cid:12) Î¸I. (cid:1) are approximately orthogonal matrices, i.e., Î»I. Î» is (cid:12)Î(xi) Î(xM )(cid:12) (cid:12). Figure 11: Visualization of WQ@WK.T at different layers in LLaVA-1.5-7b. Figure 12: Visualization of WD@WD.T at different layers in LLaVA-1.5-7b. 15 Figure 13: Visualization of WV @WV .T at different layers in LLaVA-1.5-7b. We provide empirical validation to support both of these assumptions. For Observation 1, we show WQ@WK.T , WV @WV .T , WD@WD.T of all different layers of LLaVA-1.5-7B in Fig. 11, 12, and 13. It can be seen that different layers have this orthogonal relationship. In fact, the orthogonal relationship of matrices in neural networks has been studied since long time ago. In particular, [37] proposed new regularization method that encourages the weight matrix of the neural network to maintain orthogonality during training by introducing self-orthogonality module. This method helps to improve the training stability and generalization ability of the model. [38] explores adding orthogonal regularization to weights during training to improve training stability. The author proposed an orthogonal regularization method for weights, aiming to solve the gradient vanishing and explosion problems encountered by deep convolutional neural networks during training. It can be seen that modules with orthogonality are found in various different models to improve the training stability and performance of the model. To the best of our knowledge, we are the first work to intuitively show this orthogonal performance in LLM, which can be more fully explored in subsequent research. For Observation 2, this observation is illustrated in Fig. 14. An intuitive understanding is that if xi and xM activate more of the same neurons, Ai and AM will have more positive values in common positions, making cos(Ai, AM ) larger. Figure 14: Visualization of cos((Ai, AM )) and co-act neurons number at different layers in LLaVA1.5-7b. 16 Theoretical Insight 1. Within an attention block, the degree of interaction between two tokens is governed by the relative angle of their input hidden states. Justification. For single token, suppose its input to the Attention block is y. Consider sequence of tokens [y1, y2, . . . , yM ], for token ym , its computation in the Attention layer can be expressed as: Ëym = LayerNorm(ym), Î±im = Sof tmax(cid:0)(Ëyi Wq) (cid:0)Ëym Wk Om = Î±1m V1 + Î±2m V2 + + Î±mm Vm, Vm = Ëym Wv, (cid:1)T (cid:1), < (11) where Î±im is the attention score between the i-th and the m-th token. Om is the output vector of the m-th token. To examine the influence of the i-th token yi on the final output Om of the m-th token, we can consider the following projection value: (cid:13) = (cid:13) (cid:1)(cid:13) (cid:13) (cid:13)ProjOm (12) where (cid:13) (cid:1)(cid:13) (cid:13) is the projection value of Î±im Vi on Oi. And cos (cid:0)(Vi, Om)(cid:1) is the cosine value of the angle between the Vi and Om vectors. From Eq. 11, Om is sum of vectors in different directions. Since the self-attention score Î±mm is typically much higher than Î±im for other tokens, we can simplify the projection by assuming that Om is primarily determined by Î±mmVm, i.e., (cid:13) (cid:13) cos(cid:0)(Vi, Om)(cid:1). (cid:0)Î±im Vi (cid:0)Î±im Vi (cid:13)ProjOm (cid:13)Î±im Vi ProjOm (Î±imVi) Î±imVi cos((Vi, Vm)), (13) where Î±im = Sof tmax(cid:0)(Ëyi Wq) (cid:0)Ëym Wk Sof tmax(x) x. We can have Î±im (Ëyi Wq) (cid:0)Ëym Wk (cid:1)T / (cid:1)T . Therefore, d(cid:1). Since the Softmax function is monotonic, that is, Î±imVi cos((Vi, Vm)) (Ëyi Wq) (cid:0)Ëym Wk = ËyiWq, ËymWkVi, Vm/Vm = (cid:0)Ëyi(WqW )ËyT )ËyT (cid:1)(cid:0)Ëyi(WvW (cid:1)T Vi cos((Vi, Vm)) (cid:1)/Vm (14) Based on the Observation 1, WqW Î¸I, WvW (Î±imVi) Î¸Î»Ëyi, ËymËyi, Ëym/Vm Î»I. Therefore, combining Eq. 13 and Eq. 14, (15) ProjOm Given that Ëy is the result of after LayerNorm, Ëy and share the same direction, and Ëy = 1, it holds that Ëyi, Ëym = cos((yi, ym)). We can have ProjOm (Î±imVi) cos((yi, ym)), (16) Eq. 19 shows that the angle between different tokens directly affects their mutual interaction in attention layer. The closer the angles of two tokens, the greater their mutual influence. We also demonstrate that the computation of activations is influenced by the angular relationships between hidden states. This justification is presented as Theoretical Insight 4 in Section B.3. B.2 Attention-Based Explanation of Layer-wise Angle Concentration Patterns In this section, we present an attention-based explanation of Layer-wise angular concentration patterns. Specifically, we theoretically show that: (1) the degree of angular concentration between two hidden states influences the magnitude of their attention scores, and (2) the presence of sink attention structure encourages angular concentration among tokens. Theoretical Insight 2. The smaller the angle between the input hidden states of two tokens to the attention block, the higher their corresponding attention score. Justification. For two tokens and j, let their inputs to an attention block be denoted as xi and xj, respectively. Then, their attention score Î±ij can be expressed as: Ëyi = LayerNorm(yi), Î±ij = Sof tmax(cid:0)(Ëyi Wq) (cid:0)Ëyj Wk Ëyj = LayerNorm(yj), (cid:1)T (cid:14) d), (17) 17 Based on the Observation 1, WqW Î¸I, Therefore, Î±im = Sof tmax(cid:0)(Ëyi Wq) (cid:0)Ëyj Wk = Sof tmax(cid:0)Ëyi(WqW )ËyT / (cid:1)T d(cid:1) = Sof tmax(cid:0)ËyiWq, ËyjWk/ / d(cid:1) = Sof tmax(cid:0)Î¸ Ëyi, Ëyj/ d(cid:1) d(cid:1) (18) Furthermore, since LayerNorm preserves the direction of vectors by normalizing only their magnitude, the inner product between normalized outputs satisfies Ëyi, Ëyj = cos((yi, yj)), d(cid:1) cos((yi, yj)(cid:1) Î±im = Sof tmax(cid:0)Î¸ cos((yi, yj)/ (19) Equation 19 indicates that the more aligned the hidden states of two input tokens are (i.e., the smaller the angle between them), the higher their attention score. Therefore, in conjunction with the layer-wise angle concentration pattern discussed in the main text, we observe that inter-segment angular concentration reflects the models degree of attention to internal components of the problem, while intra-segment angular concentration captures the level of attention between the question and the system promptserving as an indicator of the models instruction-following capability. Theoretical Insight3. Presence of sink attention promotes angular concentration among hidden states. Figure 15: Visualization of attention score at different layers in Qwen2.5-0.5B-Instruct. Justification. To understand why token angles exhibitLayer-wise angular concentration patterns, we start our analysis from the phenomenon of sink attention, which similarly shows segment-wise characteristics yet remains insufficiently understood. In LLMs, attention scores exhibit segment-wise tendencies, and, apart from self-attention, attention scores typically peak at the first token of each segment. This phenomenon is termed sink attention, as illustrated in Figure 15. Given an input representation for an attention block in an LLM as Rmd, where the i-th token is denoted as yi, the attention mechanism output is defined as = Î±V , where Î± represents the attention scores, and = LayerNorm(y)Wv denotes the value vectors. Suppose the attention scores within segment sink to the first token vector yi, then the outputs for the sink token yi and another token yk within the same segment can be approximated as: Oi Î±iiVi, Ok Î±ikVi + Î±kkVk, where Î±ik denotes the attention score between tokens and k. This approximation arises because the sink tokens self-attention score significantly surpasses its attention scores to other tokens, while other tokens primarily attend to themselves and the sink token. Substituting these approximations, the angle between outputs Oi and Ok can be expressed as: cos((Oi, Ok)) = Î±ikVi + Î±kkVk cos((Vi, Vk)) (cid:112)Î± ikVi2 + Î±2 kkVk2 + 2Î±ikÎ±kkViVk cos((yi, yk)) 18 Furthermore, since Wv is approximately an orthonormal matrix (detailed proof in the appendix) and thus preserves angles, we have WvW Î²I, where Î² is constant. Combined with the fact that LayerNorm scales only magnitudes without altering angles, we get: cos((Vi, Vk)) = cos((yi, yk)). Substituting into the earlier expression, we derive: cos((Oi, Ok)) = Î±ik + Î±kk cos((yi, yk)) ik + Î±2 kk + 2Î±ikÎ±kk cos((yi, yk)) Î²(cid:112)Î±2 Squaring both sides and simplifying, we arrive at the condition: cos((Oi, Ok)) > cos((yi, yk)) if Î²(cos((yi, yk)))2 < 1 and Î² < 1. Since weight parameters in LLMs are typically constrained to values less than 1, both Î² < 1 and Î²(cos((yi, yk)))2 < 1 generally hold. Thus, Equation (7) demonstrates that sink attention inherently promotes angle concentration within segments. detailed derivation is provided in the appendix. In Figure 15, we present the distribution of attention scores across different layers. In intermediate layers, sink tokens operate primarily within segments to enhance intra-segment angle concentration. In later layers, sink tokens across segments begin to interact, promoting inter-segment concentration. This indicates that, due to the influence of sink tokens, token angles are increasingly concentrated through the forward pass. The final layers angle concentration is particularly important as it reflects the culmination of this process and directly determines the models output. B.3 Neuron-Based Explanation of Data-wise Angle Concentration Patterns In Section 2.3 of the main text, we demonstrate that the greater the number of tokens activating the same neuron, the more gradient components that neuron receive. In this section, we further show that the number of commonly activated neurons between tokens has direct effect on the angle between their output hidden states at the current layer. Theoretical Insight 4. Within the FFN block, the extent of overlap in activated neurons between two tokens directly affects the angular relationship between their output hidden states. Justification. To analyze how the activation layer affects cos((yi, yM )), we first decompose its computation formula. Suppose the activation output of the i-th token is Ai, and yi = AiWd. Based on Observation 1, Wd is scalar multiple of unitary self-orthogonal matrix, applying the same rotation to any input while preserving the inner product and angle between any two input vectors. Thus, we can have: cos((yi, yM )) = AiWd, AM Wd/(yiyM ) )AT = Ai(WdW /(yiyM ) = Î·Ai, AM /(yiyM ), where Î· is constant based on Observation 1. Furthermore, since Wd is an orthogonal matrix, yi2 = AiWd2 = (AiWd)(AiWd)T = Î·Ai2. )AT Î·Ai. Substituting this into Eq. 20 we can have cos((yi, yM )) = Î·Ai, AM /(Î·AiAM ) which means yi = = Ai(WdW = Î·AiAT = cos((Ai, AM )) (20) (21) (22) This shows that the orthogonal matrix Wd does not change the angles between the input vectors. Furthermore, based on Observation 2, cos((Ai, AM )) (cid:12) (cid:12), we can get cos((yi, yM )) (cid:12) (cid:12)Î(xi) Î(xM )(cid:12) (cid:12)Î(xi) Î(xM )(cid:12) (cid:12). (23) which is consistent with Insight 2. Eq. 23 shows that activation layers adjust token angles by controlling the intersections of their activated neurons. More shared activated neurons lead to smaller angles and greater mutual influence. 19 Visualization Results In the main text, we presented layer-wise, epoch-wise, and data-wise patterns of angular concentration. In this section, we provide more comprehensive visualizations to further support our conclusions. C.1 Layer-wise Angle Concentration Patterns In Fig. 16, 17 and 18, we present the layer-wise angular concentration patterns across all layers of the Qwen2.5-0.5B-Instruct model for tasks of easy, medium, and high difficulty, respectively. Notably, the model consistently demonstrates first intra-segment angle concentration and subsequently intersegment angle concentrationregardless of problem difficulty. This consistency suggests that the observed pattern is generalizable property of the models internal representation dynamics. Figure 16: Visualization of Layer-wise Angle Concentration at different layers in Qwen2.5-0.5BInstruct at easy sample (correct num = 10). Figure 17: Visualization of Layer-wise Angle Concentration at different layers in Qwen2.5-0.5BInstruct at medium difficulty sample (correct num = 5). Figure 18: Visualization of Layer-wise Angle Concentration at different layers in Qwen2.5-0.5BInstruct at difficult sample (correct num = 0). C.2 Data-wise Angle Concentration Patterns Fig. 19 provides more fine-grained view of the data-wise angle-concentration patterns. Consistent with the conclusions in the main text, it reveals curriculum-like trend: the model learns samples exhibiting high angular concentration earlier, followed by samples with lower angular concentration. Figure 19: Visualization of Data-wise Angle Concentration at different epoch in Qwen2.5-0.5BInstruct at difficult sample. The training setting is consistent with Figure 3 in the main text. 21 GAIN-RL Algorithm Algorithm 1: GAINRL (GRPO): Gradient-driven Angle-Informed Navigated RL Framework Input: Training data = {d1, d2, . . . , dN }; model ; training steps ; batch size n; accuracy sensitivity Î±; target accuracy Î²; angle sensitivity Î³; sampling variance Ï Output: Trained model MT 1 Step 1: Reorder training data by angle signal 2 for 1 to do 3 Prefill di with model Extract angle signal CM (di) = CM intra(di) + CM 4 5 Sort by CM () in descending order to obtain Ds 6 Step 2: Train with dynamic probabilistic sampling 7 Initialize Gaussian distribution P0 (0, Ï2) 8 for 1 to do 9 Sample batch d(t) Sample(Ds; Pt, n) for 1 to do inter(di) 10 11 12 13 15 16 Let model Mt answer d(t) Record CMt(d(t) ) and accuracy AccMt(d(t) ) Update Mt with accuracy-based loss Compute mean accuracy Acc(t) = 1 (cid:80)n Compute mean angle C(t) = 1 Update sampling-mean (cid:80)n i=1 AccMt(d(t) ) i=1 CMt(d(t) ) Âµt+1 = Âµt + 2 tanh(cid:0)Î±(Acc(t) Î²)(cid:1) + tanh(cid:0)Î³ C(t)(cid:1)"
        },
        {
            "title": "E Experimental Setup",
            "content": "In this section, we describe our experimental setup in detail, covering the models, datasets, and hyperparameters used. E.1 Model and Dataset To comprehensively evaluate the effectiveness of GAIN-RL, we conduct experiments across multiple models and datasets. Specifically, we select models varying in size, including Qwen2.5-0.5bInstruct, Qwen2.5-Math-1.5B-Instruct, Qwen2.5-Math-7B-Instruct, Qwen2.5-Coder-3B-Instruct, and LLaMA3.2-3B-Instruct. We primarily focus on two tasks: Math and Code. To evaluate the training efficiency of GAIN-RL (Section 4.1 in the main text), we use the DeepScaleR [24] dataset for mathematical task training and DeepCoder [25] for coding tasks training, each integrating problems from diverse sources and covering wide range of difficulty levels. For mathematical evaluations, we employed six benchmark datasets of varying difficulty: GSM8K [3], MATH [4], AMC 23 [18], AIME 24 [19], OlympiadBench [20], and Minerva Math [39]. For coding evaluations, we utilized three standard benchmark datasets: LivecodeBench (8/1/242/1/25) [6], Codeforces [22], and Humaneval+ [23]. For other experiments (Section 4.2-Section 4.5), we train model on the training dataset of single tasks including GSM8K, MATH and AMC 23 to facilitate more convenient comparisons. E.2 Training Configuration We trained the models using the GRPO algorithm. The training was performed on single node equipped with 8 A100 GPUs. Each model was trained for about 200 steps using the veRL library. To evaluate the training efficiency on GRPO-RL, the main training configuration for Qwen2.5Math-7B-Instruct is shown below. For Qwen2.5-Math-1.5B-Instruct and LLaMA3.2-3B-Instruct, we set max_response_length=3000 to accommodate its shorter context window of 4096 tokens, while keeping all other parameters unchanged. For Qwen2.5-0.5B-Instruct and single task training, we set max_prompt_length=max_response_length=512, while keeping other parameters unchanged. For Qwen/Qwen2.5-Coder-3B-Instruct, we set max_prompt_length=2048, max_response_length=16384,train_batch_size=512 and ppo_mini_batch_size=64 due to its higher single sample memory usage. python3 -m verl.trainer.main_ppo algorithm.adv_estimator=grpo data.train_files=\"$train_files\" data.val_files=\"$test_files\" data.train_batch_size=1024 data.max_prompt_length=1024 data.max_response_length=8192 actor_rollout_ref.model.path=Qwen/Qwen2.5-Math-7B-Instruct actor_rollout_ref.actor.optim.lr=1e-6 actor_rollout_ref.model.use_remove_padding=True actor_rollout_ref.actor.ppo_mini_batch_size=256 actor_rollout_ref.actor.ppo_micro_batch_size_per_gpu=16 actor_rollout_ref.actor.use_dynamic_bsz=True actor_rollout_ref.actor.ppo_max_token_len_per_gpu=8000 actor_rollout_ref.actor.use_kl_loss=True actor_rollout_ref.actor.kl_loss_coef=0.001 actor_rollout_ref.actor.kl_loss_type=low_var_kl actor_rollout_ref.actor.entropy_coeff=0 actor_rollout_ref.model.enable_gradient_checkpointing=True actor_rollout_ref.actor.fsdp_config.param_offload=False actor_rollout_ref.actor.fsdp_config.optimizer_offload=False actor_rollout_ref.rollout.tensor_model_parallel_size=1 actor_rollout_ref.rollout.log_prob_micro_batch_size_per_gpu=16 actor_rollout_ref.rollout.name=vllm actor_rollout_ref.rollout.gpu_memory_utilization=0.6 actor_rollout_ref.rollout.n=8 actor_rollout_ref.ref.log_prob_micro_batch_size_per_gpu=16 actor_rollout_ref.ref.fsdp_config.param_offload=True algorithm.use_kl_in_reward=False trainer.critic_warmup=0 trainer.logger=[console,wandb] trainer.project_name=verl_grpo_example_gsm8k_math trainer.experiment_name=qwen2_MATH_7b_Instruct_function_rm trainer.n_gpus_per_node=8 trainer.nnodes=1 trainer.save_freq=20 trainer.test_freq=5 trainer.total_epochs=200 $@"
        },
        {
            "title": "F Additional Experimental Results",
            "content": "In this section, we present additional experiments to further validate the effectiveness of GAIN-RL. F.1 Performance of Weighted Signals In the main text, we employed an unweighted angular signal of the form: CM (di) = CM CM inter(di). Here, we explore weighted variant of the signal: CM (di) = CM intra(di) + CM where is constant weight. intra(di) + inter(di), 23 As shown in Tab. 4, we investigate the final performance of the Qwen2.5-0.5B-Instruct model after 200 training epochs on the Math dataset, using different values of in the weighted signal. The results show that setting = 4.0 yields the best performance for this model, suggesting that carefully designed angle-based signals can further improve training effectiveness. However, selecting the optimal weighting coefficient requires extensive empirical tuning. To ensure scalability and practical applicability, we adopt the unweighted version of the signal in this work and leave signal optimization for future research. Our goal is to demonstrate that even without finely tuned weighting, GAIN-RL is still capable of accelerating both training and data efficiencyhighlighting the strong potential of model-signal-based RLHF strategies. Table 4: Model performance at epoch 200 under different weighting coefficients c. Accuracy 0.25 36.20 0.5 37.20 1.0 37. 2.0 38.00 4.0 38.40 8.0 38.20 F.2 Model Performance on Single Task Fig. 9 in the main text illustrates the training dynamics of Qwen-2.5-0.5B-Instruct on three single-task datasets. For more detailed comparison, Tab. 5 reports the final performance at epoch 200 and the corresponding training speedup across different training sets and methods. Notably, on the GSM8K dataset, GAIN-RL outperforms the original GRPO by 4.72% in final accuracy and achieves 3.3 improvement in training speed. These results demonstrate that GAIN-RL can effectively distinguish between samples of varying learnability even in the single-task setting, highlighting its efficiency and general applicability. Table 5: Fine-tuning performance on single tasks. Models are trained on the training sets and evaluated on their validation sets. ADARFT is excluded on GSM8K due to missing difficulty coefficients. Prepare Metric Time GRPO - ADARFT(GRPO) Difficulty GAIN-RL(GRPO) Angle - > 1 day < 10 min ACC@ 200Epo 48.43 - 53.15 GSM8K Epo@ 200Acc 200 - 60 Speed Up 1 - 3.33 ACC@ 200Epo 34.80 35.80 37.40 Math Epo@ 200Acc 200 150 80 Speed Up 1 1.33 2.50 ACC@ 200Epo 9.64 9.64 12.04 AMC Epo@ 200Acc 200 160 100 Speed Up 1 1.25 2."
        },
        {
            "title": "G Discussion and Future Work",
            "content": "In Section 3, we demonstrate that angles between token hidden states fundamentally mirror and influence both the information propagation during inference and the learning dynamics throughout model training. The proposed angle-based signals can, in fact, be generalized beyond RFT to enhance model-centric effectiveness in various other domains. For instance, during pre-training, monitoring angle signals could enable real-time evaluation of models learning capacity across different domains, thus allowing adjustments to training data to improve stability and final performance. Furthermore, during inference, tracking changes in angle concentration between layers could provide insights into the models comprehension of inputs and indicate whether additional test-time adjustments are necessary to boost output accuracy. In future work, we plan to further investigate how this signal can be leveraged across multiple domains to achieve comprehensive, model-centric optimizations."
        }
    ],
    "affiliations": [
        "Duke University",
        "University of California, Berkeley"
    ]
}