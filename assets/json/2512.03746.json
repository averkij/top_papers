{
    "paper_title": "Thinking with Programming Vision: Towards a Unified View for Thinking with Images",
    "authors": [
        "Zirun Guo",
        "Minjie Hong",
        "Feng Zhang",
        "Kai Jia",
        "Tao Jin"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Multimodal large language models (MLLMs) that think with images can interactively use tools to reason about visual inputs, but current approaches often rely on a narrow set of tools with limited real-world necessity and scalability. In this work, we first reveal a critical and previously overlooked weakness: even state-of-the-art MLLMs are surprisingly brittle, showing significant performance degradation on images with simple orientation changes or natural corruptions, underscoring the need for more robust tool-based reasoning. To address this, we propose CodeVision, a flexible and scalable code-as-tool framework where the model generates code as a universal interface to invoke any image operation, moving beyond fixed tool registries. We train our model using a two-stage methodology, beginning with Supervised Fine-Tuning (SFT) on a high-quality dataset curated for complex, multi-turn tool composition and error recovery, followed by Reinforcement Learning (RL) with a novel and dense process reward function to encourage strategic and efficient tool use. To facilitate this research, we construct new SFT and RL datasets and introduce a challenging new benchmark suite designed to rigorously evaluate robustness to orientation changes and multi-tool reasoning. Experiments on Qwen2.5-VL and Qwen3-VL series show that our approach significantly improves model performance and fosters emergent capabilities such as flexible tool composition, efficient chained execution, and robust error recovery from runtime feedback. Code is available at https://github.com/ByteDance-BandAI/CodeVision."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 3 ] . [ 1 6 4 7 3 0 . 2 1 5 2 : r Thinking with Programming Vision: Towards Unified View for Thinking with Images Zirun Guo1,2 Minjie Hong1 Feng Zhang2 Kai Jia2 Tao Jin1 1 Zhejiang University 2 ByteDance, BandAI Abstract Multimodal large language models (MLLMs) that think with images can interactively use tools to reason about visual inputs, but current approaches often rely on narrow set of tools with limited real-world necessity and scalability. In this work, we first reveal critical and previously overlooked weakness: even state-of-the-art MLLMs are surprisingly brittle, showing significant performance degradation on images with simple orientation changes or natural corruptions, underscoring the need for more robust tool-based reasoning. To address this, we propose CodeVision, flexible and scalable code-as-tool framework where the model generates code as universal interface to invoke any image operation, moving beyond fixed tool registries. We train our model using two-stage methodology, beginning with Supervised Fine-Tuning (SFT) on high-quality dataset curated for complex, multi-turn tool composition and error recovery, followed by Reinforcement Learning (RL) with novel and dense process reward function to encourage strategic and efficient tool use. To facilitate this research, we construct new SFT and RL datasets and introduce challenging new benchmark suite designed to rigorously evaluate robustness to orientation changes and multi-tool reasoning. Experiments on Qwen2.5-VL and Qwen3-VL series show that our approach significantly improves model performance and fosters emergent capabilities such as flexible tool composition, efficient chained execution, and robust error recovery from runtime feedback. # Email: zrguo.cs@gmail.com Code: https://github.com/ByteDance-BandAI/CodeVision 1. Introduction Thinking with images (OpenAI, 2025c) equips multimodal large language models (MLLMs) with interactive, tool-augmented reasoning over visual inputs. Rather than passively describing an image, the model actively manipulates it using tools (e.g., zoom in, OCR) to acquire the evidence needed for reliable reasoning. Despite rapid progress, current research still faces three limitations: 1 The necessity of tools: Current methods mainly emphasize the crop tool, which zooms in on regions of interest for clearer observation, and evaluate tool ability on benchmarks such as V* (Wu and Xie, 2024) and HRBench (Wang et al., 2025b). Yet the benefit is marginal: using tools often yields only 25% accuracy gains, and reinforcement learning (RL) without tools can match those results. This suggests that the potentialand necessityof tools is not being fully exercised by existing tasks. 2 Flexibility and scalability: Methods frequently require manually specifying tool names and arguments (Su et al., 2025b), which is brittle and not scalable. Even renaming tool (e.g., crop zoomin) can necessitate retraining, hindering generalization to new tools and argument schemas. 3 Multi-turn, multi-tool use: Many systems support only single tool or few tools within single turn. While several works (Lai et al., 2025; Zhang et al., 2025) explore multi-turn settings, they largely focus on repeated cropping rather than composing different tools across multiple turns, which is what real-world tasks often require. Based on these observations, we propose new framework to address all three challenges. To create scenarios where tools are genuinely required, we examine ubiquitous real-world perturbations: incorrect image orientation due to landscape/portrait capture and mirrored selfies that confound text recognition. Compared with cropping, restoring the canonical orientation is often strictly more necessary for downstream recognition and reasoning. We conduct simple diagnostic: starting from 200 images from various domains, we uniformly apply one of five transformationsrotation by 90/180/270 degrees, horizontal flip, or vertical flipand ask models to identify the transformation (a fiveway multiple-choice question). As illustrated in Figure 1, even state-of-the-art models like GPT-5 (OpenAI, 2025b) and Gemini2.5 Pro (Comanici et al., 2025) perform poorly, whereas humans achieve 100% accuracy easily. Remarkably, as summarized in Table 1, simple rotation/flip operations can reduce model performance by up to 80%, revealing substantial brittleness of current MLLMs and underscoring the necessity of using tools for these real-world corruptions. Figure 1: Diagnostic results on image orientation identification. To address the second challenge, inspired by OpenAI o3 (OpenAI, 2025c), we treat code itself as unified tool: the model writes code that invokes whatever image operations are needed. This eliminates hand-crafted tool name/argument specifications and dramatically improves generalization: the model is no longer restricted to fixed registry but can call an effectively unbounded set of tools through code. While the paradigm of using code as tool has been explored previously (Chen et al., 2023; Zhang et al., 2025), we unlock three notable advantages through our principled RL training and rigorous dataset construction (see Figure 2): (1) Emergence of new tools: the model calls tools that never appeared in the RL training data to solve novel problems; (2) Efficiency: the model chains multiple tools within single execution; (3) Robustness: the model leverages runtime error messages and outputs to revise code, improving failFigure 2: Three advantages of CodeVision we observe in the training and inference stage. 2 ure recovery and out-of-distribution generalization. To target the third challenge, we design datasets and benchmark that require composing multiple tools across multiple turns, and we introduce dense process rewards that encourage effective tool selection and steady progress throughout the dialogue. This setting better reflects realistic problem solving and promotes the development of models that plan, adapt, and utilize different tools across steps. In summary, our contributions are threefold: We identify critical brittleness in state-of-the-art MLLMs and propose flexible code-as-tool framework that treats code as universal interface for tool invocation, enhancing scalability and generalization. We construct high-quality SFT and RL datasets focused on multi-turn, multi-tool composition and error handling, and introduce three new benchmarks covering both singleand multi-tool scenarios to rigorously evaluate model robustness and complex tool use. Our experiments demonstrate that our approach significantly improves model performance on these challenging benchmarks and validates the benefits of the code-as-tool paradigm, which enables emergent tool use, efficient tool chaining, and robust error recovery. 2. Related Work Thinking with Images. OpenAIs o3 (OpenAI, 2025c) popularized the idea of thinking with images, where MLLMs actively operate on images via tools. Most follow-ups focus on the crop/zoom tool (Zheng et al., 2025b; Lai et al., 2025; Su et al., 2025a; Fan et al., 2025), in part because groundingrich pretraining eases integration (Bai et al., 2025b). However, cropping is often unnecessary or the benefits are hard to observe for many tasks because similar performance can be achieved without it. Meanwhile, broader tool suites (e.g., line drawing, OCR, segmentation) (Su et al., 2025b; Zhang et al., 2025; Liu et al., 2025) have been investigated, but their effectiveness has not been sufficiently validated because they are not evaluated on tasks that truly require tools. Tool Integration. Equipping large models with tools (Comanici et al., 2025; OpenAI, 2025c,b) mitigates intrinsic limitations by enabling external capabilities such as search, code execution, and generative models. For instance, LLM-I (Guo et al., 2025b) leverages tools such as search, code, and diffusion models, enabling pure language model to produce multimodal content, and recent work integrates web search to support fact-grounded reasoning (Jin et al., 2025; Li et al., 2025). DeepResearch (Team et al., 2025; OpenAI, 2025a) is more advanced tool-integrated reasoning paradigm, which requires multi-turn search, data collection, and reasoning and achieves state-ofthe-art performance on various difficult benchmarks. These directions highlight the promise of tool-augmented agents, yet most image-centric systems still rely on narrow, pre-registered tool sets with hand-specified interfaces. MLLM Reasoning. Reinforcement learning has become central to strengthening reasoning in LLMs, from PPO (Schulman et al., 2017) to more recent GRPO (Shao et al., 2024), DAPO (Yu et al., 2025), and GSPO (Zheng et al., 2025a). For MLLMs, reasoning spans both text and vision, such as chart reasoning (Wang et al., 2025a; Masry et al., 2025); recent work (Guo et al., 2025a) emphasizes careful visual inspection prior to reasoning via an explicit observe tag. Similarly, series of works (Hong et al., 2025; Gao et al., 2025) have explored enhancing reasoning in MLLMs through advanced policy optimization. Recent efforts are transitioning from thinking about images to thinking with 3 Figure 3: Pipeline for cold-start SFT data construction. images (OpenAI, 2025c; Su et al., 2025b; Zhang et al., 2025), but robust, multi-turn, multi-tool composition and strong generalization to unseen tools remain open challenges. 3. Methodology 3.1. Overview We train an agent to use tools robustly and efficiently through two-stage process. We begin with cold-start phase, where Supervised Fine-Tuning (SFT) teaches the model the fundamental syntax and patterns of tool use on diverse, high-quality dataset that explicitly covers challenging scenarios such as multi-tool sequences, error handling, and coarse-to-fine localization. Following SFT, we transition to Reinforcement Learning (RL) phase to move beyond pattern imitation and teach the model to strategize. In this stage, the agent learns to make deliberate, efficient, and robust decisions via dense, multi-component reward function designed to encourage the use of necessary tools, facilitate the discovery of beneficial strategies, and penalize reward-hacking behaviors, ensuring the final model is effective and deliberate in its tool application. We next detail the cold-start data and training, followed by the RL setting, including data construction and reward design. 3.2. Cold Start 3.2.1. Dataset Construction The overall pipeline for cold-start SFT data construction is shown in Figure 3. We build an SFT corpus that couples tool-use trajectories with final answers by combining diverse sources, explicit metadata, and programmatic verification. As noted in Figure 1, even state-of-theart models struggle to reliably determine which tool is needed, so we guide data generation with structured metadata and automatic checks. Data sources across domains. To increase diversity, we aggregate examples from multiple domains: handwriting datasets (Marti and Bunke, 2002; Liu et al., 2011), in-the-wild OCR/VQA datasets for everyday scenes (Long et al., 2022), table and chart understanding datasets (Masry et al., 2023), and math reasoning datasets (Qiao et al., 2025). Task types and metadata. For each sample, we create metadata containing the ground-truth answer and target type from five categories: single-tool, multi-tool, multi-crop, error-handling, and no-tool. We sample types according to preset proportions and, conditioned on the type, uniformly sample candidate tools. For crop-type data, we select text regions annotated in the source dataset whose areas are at most 0.01% of the image, ensuring that cropping is necessary for successful 4 Figure 4: Rollout and inference process, and token masking used during SFT/RL. recognition and reasoning. For the multi-crop type, we simulate coarse-to-fine zoom-in by enforcing monotonically shrinking, spatially contiguous crop windows across steps to strengthen spatial awareness and localization. For the error-handling type, we intentionally surface failures, such as using an incorrect tool or triggering code and runtime errors (e.g., invalid arguments, missing imports), and require the model to read error logs, revise its code, and retrypotentially switching to the correct toolwith limited retries before fallback. Metadata-conditioned image transformations. After sampling the metadata, we transform the canonical (unaltered) image according to the selected tool to construct the models initial observation. For example, if the required tool is rotate-180, we rotate the original image by 180 degrees and use this transformed image as GPT-5s initial input. This setup makes the subsequent tool invocation necessary to recover the canonical view. SFT generation with GPT-5. Given the question, the (possibly transformed) image, and the metadata, GPT-5 produces step-by-step reasoning trace and an action at each turn. Actions may include calling tool or emitting an intermediate or final answer. When an action calls tool, we execute it in controlled runtime to obtain an updated image. We then compare the output against the canonical, untransformed reference. If they agree, the tool call is marked correct. Otherwise, we either discard the trajectory or attempt targeted correction. For the error-handling type, we feed error logs back to the model to trigger code revision before any discard. Iterative multi-turn construction. After each step, we update the metadata (step count, remaining/required tools) and resubmit the updated context to GPT-5, repeating until the per-type turn budget is reached or stop action is produced. Following this pipeline, we construct approximately 5,000 high-quality SFT examples consisting of aligned reasoning, action sequences, and answers. 3.2.2. Training We train the model with an SFT objective over our multi-turn trajectories. As shown in Figure 4, each training example is formatted as an interleaved dialogue with alternating user prompts, assistant reasoning/tool-call outputs, and recorded tool returns. During optimization, we mask out user tokens and tool-return tokens; only assistant tokens corresponding to chain-of-thought reasoning and tool-call specifications contribute to the loss. Concretely, we apply teacher-forced next-token prediction with token-wise mask mt {0, 1} that is 1 for assistant reasoning/tool-call tokens and 0 otherwise. LSFT(θ) = (cid:88) t=1 mt log pθ(yt x, y<t) , (1) where denotes the dialogue context (including masked user inputs and tool returns), y<t are preceding assistant tokens, and mt selects only assistant reasoning/tool-call tokens. We do not execute tools online during SFT. All tool outputs used as subsequent context are the cached 5 results from dataset construction. Training therefore reduces to standard causal LM training with masked loss over the assistant side across multiple turns (including the final answer), preserving the multi-turn structure while avoiding runtime variance from external tool execution. 3.3. Reinforcement Learning To further enhance reasoning and generalization, we conduct RL training after the cold-start stage. 3.3.1. Dataset Beyond the SFT sources, we augment RL training data with additional reasoning-heavy and perception samples (Mathew et al., 2021; Wang et al., 2025c; Singh et al., 2019) to increase coverage and diversity of reasoning patterns. We then perform difficulty filtering to focus optimization on informative instances. For each candidate item, we sample multiple rollouts and remove degenerate cases where trajectories are uniformly trivial (all-correct) or uninformative (all-incorrect). This concentrates training on items with signal for policy improvement. Furthermore, each item is annotated with must-use tool field drawn from {rotate90, rotate180, rotate270, flip-horizontal, flip-vertical, crop}. For crop-required items, we additionally attach the bounding box of the target region. Items that require no tools set this field to None. These constraints encourage policies to learn when and how to invoke the right tool. Following this pipeline, we construct approximately 40,000 RL training items. 3.3.2. Reward Function Training LLMs to use tools reliably is challenging in practice: training collapse and abnormal tool-call rates frequently occur. We therefore adopt dense, rich, and multi-component reward to stabilize and guide RL. Let trajectory be τ = (s1, a1, . . . , sT , aT ), where st is the environment state (dialogue context, current image, and any tool returns) and at is the agent action (reasoning/tool-call program or textual answer) at turn t. The total reward decomposes as Rtotal(τ ) = Routcome(τ ) + β1 (cid:88) t=1 Rstrategy(at) β2Pcost(τ ). (2) where Routcome(τ ) is the outcome reward consisting of the terminal accuracy signal and the formatting reward, Rstrategy(at) is the process reward for strategy supervision at turn t, Pcost(τ ) is the constraint penalty for trajectory efficiency and avoiding reward hacking, and β1, β2 control the trade-off between these terms. Outcome Reward. We use terminal accuracy signal racc {0, +1} assessed on the final answer, and format reward rfmt {0, +1} for correct reasoning formatting (<think> and <answer> tags). Strategy Shaping. Pure outcome rewards are sparse and often lead to brittle behavior (e.g., abnormal tool-call rates). We therefore introduce process-level signals with two parts: (i) must-use tool set that encodes task prerequisites, and (ii) suggested tools discovered on-the-fly that empirically improve success. (a) Must-use tools Sreq. Our RL metadata specifies set of tools, Sreq, that are prerequisites for solving the task robustly. For example, an image with incorrect orientation must be rotated first, regardless of the question. Let = Sreq be the number of required tools. We assign total reward budget of 1/N to each required tool. For categorical tools like rotate or flip, this reward is given as one-time bonus of 1/N when the tool is correctly used for the first time. For crop, which has 6 continuous quality measure, the reward is proportional to the Intersection-over-Union (IoU)a measure of overlap between the predicted and target bounding boxes. To encourage refinement over multiple steps, we only reward the improvement in IoU over the best attempt so far. Additionally, we introduce bonus for perfectly matching the required tool-use trajectory. This reward is granted only if the agent executes the complete sequence of tools specified in Sreq correctly and in the prescribed order, without any redundant or incorrect steps. This mechanism serves two purposes. First, for single-tool scenarios, it encourages precision by distinguishing between an immediate, correct tool call and solution reached through trial-and-error. Second, for multi-tool tasks, it incentivizes the agent to learn and follow the optimal solution path, ensuring it explores and masters the entire sequence of required actions. (b) Suggested tools bonus. Not all useful tools can be predefined. In addition to the must-use tools in Sreq, we consider optional tools that are not required but can empirically improve success. This is where the power of our code-as-a-tool framework becomes critical, as it liberates the agent from fixed tool registry and allows it to invoke any function or library dynamically. For example, applying contrast enhancement on blurry image might be beneficial but is not universal prerequisite. We reward the discovery of such emergent, helpful tools through rollout comparison mechanism. For given problem, we collect trajectories (with = 8 in our experiments) from the current policy and partition them into those that use an optional tool, Gtool, and those that do not, Gnotool. If the tool-using group shows higher accuracy rate and the no-tool group achieves at most one success out of rollouts, it suggests that the additional tools will improve the success rate of solving this problem. We then compute this empirical performance gain as an inferred tool necessity reward for this problem: rnec = max (cid:16) 0, (cid:80) iGtool ri Gtool acc (cid:80) iGnotool ri Gnotool acc (cid:17) , (3) where is the number of trajectories in group G. This reward rnec is then added as bonus to all successful trajectories that use the beneficial optional tool, encouraging the agent to explore and adopt various emergent tools that demonstrably improve performance. In addition, to further reward the use of extra tools, we introduce per-trajectory bonus that is granted whenever the agent both invokes at least one optional tool and produces correct final answer. Constraint Penalties. While dense rewards are effective at guiding the agent, they can also be exploited. We observe emergent reward hacking behaviors during training, where the agent would learn to maximize rewards in ways that misalign with our goal of efficient and intelligent problem-solving. For instance, an agent might continue to crop an image to fractionally improve its IoU score long after the correct answer has been determined, or attempt to rotate an already correctly-oriented image to seek reward. These actions lead to inefficient, low-quality trajectories. To counteract this, we introduce three targeted penalties that act as guardrails: Turn Limit Penalty: The primary motivation for this penalty is to prevent reward hacking and improve efficiency. We observe that the agent could learn to exhaustively call all possible required tools (e.g., trying rotate90, rotate180, and rotate270 sequentially) simply to maximize its Rstrategy score, even if the first action already solves the orientation problem. To curb this behavior and promote deliberate, accurate solutions, we penalize trajectories that use more tool-call turns than necessary. problem with Sreq required tools is given buffer of one extra turn for error handling and exploration; any tool-call turn exceeding Sreq + 1 incurs penalty. 7 Table 1: Main results on OCRBench (Perception) and ChartQAPro (Reasoning) under five types of transformations. OCRBench ChartQAPro Model GPT-4o Gemini2.5-Pro Qwen2.5-VL-32B Qwen2.5-VL-72B Qwen3-VL-30B-Thinking Qwen3-VL-235B-Thinking InternVL3.5-30B InternVL3.5-241B Thyme Qwen2.5-VL-7B CodeVision-7B Qwen3-VL-8B-Thinking CodeVision-8B Qwen3-VL-32B-Thinking CodeVision-32B Source Rot90 Rot180 Rot270 Hori Verti Avg Source Rot90 Rot180 Rot270 Hori Verti Avg 83.1 87.1 85.0 88.5 87.6 88.8 88.7 92.3 86.3 86.4 87.2 82.4 83.5 86.6 87.8 61.9 68.2 64.5 72.6 72.1 76.4 45.8 58.2 67.0 70.2 72.3 64.8 78.6 67.8 82.8 46.6 67.9 45.6 58.3 60.5 71.0 7.2 32.4 51.9 58.0 73.1 57.6 77.4 64.3 79. 63.7 71.5 64.8 73.5 66.6 74.3 45.8 57.6 67.8 71.7 75.2 58.4 76.7 66.4 81.1 48.1 39.4 25.5 35.1 49.3 45.9 5.9 25.6 27.2 32.4 65.1 35.7 68.7 35.1 77.7 12.9 41.3 8.7 18.2 31.8 23.8 6.2 9.4 13.3 17.0 67.4 14.5 67.3 13.8 68. 52.7 62.6 49.0 57.7 61.3 63.4 33.3 45.9 52.3 56.0 73.4 52.2 75.4 55.7 79.5 50.3 66.8 39.5 38.2 50.2 56.9 37.2 45.9 30.3 37.3 39.1 47.2 50.3 52.3 57.4 40.9 63.8 29.7 29.5 38.7 45.0 27.2 31.5 23.8 23.4 30.8 32.8 39.0 39.9 54. 33.8 59.7 26.0 27.7 36.5 43.4 18.0 24.4 20.4 22.2 29.8 30.0 38.1 38.3 53.6 41.3 61.6 30.7 30.3 38.2 46.9 27.9 34.7 22.9 23.7 31.4 31.9 39.2 42.0 54.7 33.1 45.6 22.6 23.8 31.3 35.1 17.5 22.5 17.8 19.5 30.1 21.3 39.7 26.1 52. 25.3 58.2 17.6 20.3 29.0 26.0 18.0 21.3 14.4 20.1 29.0 13.9 38.0 18.7 53.1 37.4 59.3 27.7 28.3 37.3 42.2 24.3 30.0 21.6 24.4 31.7 29.5 40.7 36.2 54.3 Poor Reasoning Penalty: An agent must arrive at the right answer for the right reasons. If trajectory yields the correct final answer but relies on crop action with negligible IoU (< 0.1) with the true region of interest, it indicates that the agents reasoning was not grounded in the correct visual evidence. We penalize such low-quality trajectories to enforce stronger link between visual evidence and the final conclusion. Inappropriate Tool Use Penalty: core aspect of tool use is knowing when not to act. For samples where no tools are required (Sreq = None), any attempt to use an orientation-adjusting tool is penalized. Applying such transformations to normal image actively increases the tasks difficulty and demonstrates flawed, low-quality strategy. Finally, the constraint penalty Pcost(τ ) consists of the sum of the above three penalties where each penalty {0, +1}. 4. Experiments 4.1. MVToolBench To rigorously evaluate models ability to compose multiple tools in sequence, we construct the MVToolBench. The construction process is designed to create challenging scenarios that necessitate the combined use of multiple tools. The pipeline consists of three main stages: data filtering, question generation, and multi-tool augmentation. Data Source and Filtering. We build our benchmark upon the HierText dataset (Long et al., 2022), which provides rich annotations including bounding box coordinates and corresponding text for words, lines, and paragraphs. To ensure that our benchmark effectively tests the models ability to perceive fine-grained details, we first perform filtering step based on the relative size of the text annotations. For each image, we calculate the area of every annotated word, line, and paragraph as percentage of the total image area. We then retain only the annotations that are exceptionally small. For instance, for word-level questions, only words occupying less than 0.01% of the total image area are selected as candidates. This filtering serves two critical purposes: 1) it guarantees high level of difficulty, as the target text is not easily visible in the original view, and 2) it enforces strong dependency on the crop tool, as zooming in becomes prerequisite for successful recognition and reasoning. Table 2: Results on single-tool (V*, HRBench) and multi-tool (MVToolBench) benchmarks. Model V* HRBench4k HRBench8k MVToolBench GPT-4o Gemini2.5-Pro Qwen2.5-VL-32B Qwen3-VL-30B-Thinking Qwen3-VL-235B-Thinking Thyme Qwen2.5-VL-7B CodeVision-7B Qwen3-VL-8B-Thinking CodeVision-8B Qwen3-VL-32B-Thinking CodeVision-32B 67.9 83.8 81.9 81.2 85.9 82. 74.6 83.7 77.5 82.4 84.8 86.2 65.0 86.2 73.8 77.8 84.3 77.0 69.4 75.6 72.4 77.1 82.1 84.3 60.1 85.1 70.5 71.3 76.6 72.0 67.5 72.2 68.1 73.4 74.8 76.1 8.5 32.6 16.4 23.7 30.1 24. 18.1 60.1 19.7 62.7 28.6 65.4 Question and Answer Generation. Using the filtered, small-sized text annotations, we programmatically generate variety of reasoning questions. These questions cover tasks such as text recognition (e.g., What does the line beginning with Busy say?), counting (e.g., How many times does the letter appear in the paragraph ending with Television?), and question answering that requires information retrieval from specific paragraphs. crucial design principle during this stage is the deliberate avoidance of any explicit positional cues. We do not use phrases like the word on the left or provide bounding box coordinates in the prompts. This forces the model to rely entirely on its own grounding capabilities to first locate the relevant region and then perform an accurate crop, significantly increasing the challenge of the benchmark. Multi-Tool Augmentation. After the question-answer pairs are finalized, we perform final augmentation step on the images to create multi-tool requirement. Each image is randomly subjected to one of several transformations: rotation (90, 180, or 270 degrees), horizontal flip, or vertical flip. This step ensures that solving problem requires not only crop operation but also an initial orientation-correction tool. By setting the proportion of each transformation to be equal, we maintain balanced and unbiased dataset. This three-stage process results in comprehensive benchmark that effectively evaluates the models capacity for complex, multi-step tool composition. 4.2. Implementation Details Training Settings: To demonstrate the generalizability of our method, we use three distinct backbone models: Qwen2.5-VL-7B (Bai et al., 2025b), Qwen3-VL-8B-Thinking, and Qwen3-VL-32B-Thinking (Bai et al., 2025a). We employ GRPO (Shao et al., 2024) as the base RL algorithm. In the initial SFT stage, we train for 2 epochs with batch size of 128, learning rate of 5e-6, cosine learning rate scheduler, and warmup ratio of 0.05. Building on the SFT-tuned checkpoint, we then conduct the RL training phase for 2 epochs. For RL, the learning rate is set to 1e-6, with batch size of 64, and we perform 8 rollouts for each sample. The coefficients for the format reward, strategy reward (must-use and suggested tools), and constraint penalties are set to 0.1, 1.0 (1.0 and 0.2), and 0.5, respectively. The KL divergence coefficient is set to 0.001. 9 Figure 5: RL training curves for outcome, strategy, and total rewards. The consistent upward trend demonstrates that the agent effectively learns to use tools strategically to solve tasks. Figure 6: multi-turn example of error recovery. The model initially calls the wrong tool ( flip-horizontal), but after receiving the execution result, it identifies the mistake and corrects it by applying the right tool ( rotate-90). Evaluation and Benchmarks: Our evaluation is designed to test tool use across range of scenarios. For the crop tool, we follow prior work and evaluate on V* (Wu and Xie, 2024), HRBench4k, and HRBench8k (Wang et al., 2025b). To assess the models ability to handle orientation changes, we create new benchmark suite by applying five transformations (rotate 90/180/270 degrees, and horizontal/vertical flip) to OCRBench (Liu et al., 2024) and ChartQAPro (Masry et al., 2025). We choose these two datasets to test distinct capabilities: OCRBench is text recognition benchmark where high performance on the base version indicates that model must accurately perceive the text to succeed. In contrast, ChartQAPro is reasoning-based benchmark that requires understanding the charts content, task that does not depend on perfectly recognizing every character. This allows us to evaluate performance in both perception-critical and reasoning-heavy scenarios. Furthermore, to evaluate multi-tool proficiency, we utilize our newly created MVToolBench, the construction of which is detailed in Section 4.1. 4.3. Main Results Table 1 shows the performance of our CodeVision models against suite of strong MLLMs on benchmarks with challenging image orientations. On both OCRBench and ChartQAPro, baseline models exhibit significant performance degradation when faced with rotated or flipped images, confirming the brittleness we identified. In contrast, our CodeVision-7B and CodeVision-8B models consistently and substantially outperform their base models and other strong competitors. For instance, on the transformed OCRBench subset, CodeVision-7B achieves an average score of 73.4, +17.4 improvement over its base model, demonstrating its robust ability to recognize the correct transformation and apply the corresponding tool to restore the canonical image view for successful 10 reasoning. Table 2 evaluates performance on established tool-use benchmarks. While our models perform competitively on singletool tasks requiring cropping (V*, HRBench4k, HRBench8k), they establish new state-of-the-art on MVToolBench, benchmark designed for multi-tool reasoning. CodeVision7B achieves score of 60.1, nearly doubling the performance of the next-best model, Gemini2.5-Pro (32.6). This highlights our methods superior capability in composing multiple tools to solve complex problems. The effectiveness of our RL strategy is further illustrated in Figure 5, which plots the reward curves during training. The steady increase in all three reward componentsoutcome, strategy, and the total rewardindicates that the agent is not only learning to achieve the correct final answer but is also mastering the strategic process of tool selection and application as guided by our dense reward function. Additionally, Figure 7 presents the reward curve specifically for successful emergent tool use. This metric tracks instances where the model employs extra toolsbeyond the required setand correctly solves the task. The consistent upward trend signifies that the model is actively discovering and utilizing beneficial tools to enhance its reasoning and success rate, validating the capability of our framework to foster emergent behaviors. Figure 7: Reward curve for successful emergent tool use during RL training. We provide qualitative examples of our models advanced capabilities in Figures 6 and 9. Figure 6 showcases multiturn dialogue where the model initially calls an incorrect tool, receives feedback, and robustly recovers by executing the correct tool sequence. This demonstrates sophisticated error-handling and adaptability. Figure 9 highlights two key advantages of our code-as-tool framework: emergent tool use and efficiency. The model chains multiple operations (contrast and grayscale) within single turn to solve the users request, even though these specific tools were not included in the RL training data. This ability to generalize and compose novel tool sequences underscores the flexibility of our approach. As shown in Figure 8, our model learns to use wide variety of other tools not required in our training data, such as adjusting brightness, applying blurs, and detecting edges, demonstrating the near-infinite toolset enabled by the code-as-tool paradigm. Furthermore, Figure 10 presents striking example where the model spontaneously combines five tools, three of which never appeared in the RL data to solve request. Figure 8: word cloud of emergent tools discovered during RL training. 4.4. Case Study To provide clearer understanding of our models behavior and the impact of our reward design, we present four case studies. Figure 11 illustrates common failure mode known as reward hacking, which we observe when training model without our proposed constraint penalties. In this example, the agent is tasked with problem that requires 90-degree rotation. The agent correctly identifies and applies the rotate90 tool in its first turn, successfully correcting the images orientation. However, driven solely 11 Figure 9: An example of emergent and efficient tool use. The model chains two tools, contrast enhancement and grayscale conversion, within single turn to fulfill the users request. These tools did not appear in the RL training set, demonstrating the generalization capability of the code-as-tool framework. Figure 10: An example of emergent and efficient tool use. The model chains brightness up, contrast up, crop, rotate90, sharpness tools to solve the users request. by the goal of maximizing the strategy reward, it does not stop. Instead, it proceeds to call additional, unnecessary orientation tools in subsequent turns, which corrupts the already-correct image and 12 Figure 11: An example of reward hacking from model trained without constraint penalties. After correctly applying rotate90, the agent continues to call superfluous tools, leading to task failure. ultimately leads to failure. This case underscores the critical importance of the penalty functions, which act as guardrails to prevent such inefficient and counterproductive behavior. In contrast, Figure 12 showcases the robust, multi-step reasoning capabilities of our fully-trained CodeVision model. The task requires the model to read text from specific region that is too small to be seen in the initial view. The model first performs crop operation to zoom in. Recognizing that its initial crop was incomplete and missed part of the target area, it dynamically adjusts its strategy. In the next turn, it executes second, more precise crop that successfully isolates the entire region of interest. With the full context now visible, the model is able to accurately provide the correct answer. This example highlights the models ability to self-correct and perform fine-grained, iterative adjustments, key capability for solving complex real-world visual tasks. Furthermore, Figure 13 presents nuanced success case that reveals areas for further refinement. The task requires sequence of orientation correction followed by localizing and reading very small line of text. The model adeptly handles the first step by correcting the images orientation. It then correctly reasons about the texts location and performs crop. Although the cropped region successfully includes the target text, allowing the model to answer correctly, the crop itself is suboptimala long, narrow strip containing significant irrelevant context. This example demonstrates that while the models high-level reasoning is effective, its ability to generate precise, tight bounding boxes could be improved, suggesting it may sometimes adopt an overly cautious safe cropping strategy. To further probe the models limitations, we present clear failure case in Figure 14. The task requires correcting the image orientation and then cropping small, specific region to examine 13 Figure 12: successful multi-step reasoning case. The model first performs an initial crop, then recognizes its incompleteness and refines the cropped region in second step to include all necessary information before providing the final answer. details. The model successfully completes the first two stages: it correctly adjusts the orientation and accurately reasons about the approximate location of the region of interest. However, it fails at the final, crucial step of precise localization. The coordinates it generates for the crop are slightly off, causing the cropped area to be adjacent to the target region but missing it entirely. This case clearly demonstrates that while the model excels at high-level reasoning and coarse localization, its fine-grained coordinate prediction remains challenge and key area for future improvement. 4.5. Ablation Study To validate the effectiveness of our proposed training methodology and reward components, we conduct series of ablation studies using the CodeVision-7B model as the baseline. Effectiveness of dense reward. We investigate the impact of our dense reward function by training two variants of our model: one without the strategy-shaping reward (Rstrategy) and another without the constraint penalties (Pcost). The results are presented in Table 3 and Figure 15. As shown in Table 3, removing the strategy reward leads to substantial performance drop across all benchmarks, with the most significant degradation observed on MVToolBench (from 60.1 to 50.7). This confirms that simple outcome-based reward is insufficient for learning complex tool-use strategies. The dense process signals from the strategy reward are critical for guiding the model to discover and reinforce effective reasoning paths. Similarly, removing the constraint penalties also degrades performance, particularly on V* and MVToolBench. This highlights the importance of penalizing inefficient or illogical actions. Without these penalties, the model is susceptible to \"reward hacking,\" where it learns to maximize rewards through superfluous actions (e.g., rotating an already-correct image) that do not contribute to solving the task. The penalties serve as essential guardrails that promote more deliberate and efficient problem-solving. Figure 15 visualizes this by showing that our full reward function leads to more 14 Figure 13: successful but inefficient localization case. The model corrects the orientation and successfully crops region containing the target text. However, the crop is long, narrow strip that includes much irrelevant information, pointing to potential improvements in generating more precise bounding boxes. Figure 14: failure case highlighting the models limitations in precise localization. The model correctly adjusts the orientation and identifies the general area of interest, but provides slightly inaccurate coordinates for the crop, resulting in cropped region that just misses the target. 15 Table 3: Ablation study of our key reward components on the CodeVision-7B model. Removing either the strategy reward or the constraint penalties leads to notable drop in performance. Model OCRBench ChartQAPro Rot180 Verti Rot180 Hori V* MVToolBench Qwen2.5-VL-7B Qwen2.5-VL-7B-SFT CodeVision-7B w/o Strategy Reward w/o Penalty 70.2 57.0 72.3 60.9 68.3 17.0 35.8 67.4 61.5 66.3 23.4 23.2 30. 24.6 24.0 19.5 20.9 30.1 28.9 24.3 74.6 71.7 83.7 78.5 71.2 18.1 26.6 60. 50.7 55.9 (a) Penalty Term (b) Entropy (c) Tool Turns Figure 15: Training dynamics for our full reward function versus ablated versions. The complete reward function leads to more stable and efficient learning. stable and higher final success rate during RL training compared to the ablated versions. (a) Entropy (b) Tool Turns (c) Accuracy Reward (d) Strategy Reward Figure 16: Comparison of training with and without the SFT cold-start stage. Starting RL from SFT-tuned checkpoint is critical for effective learning, while training from scratch fails to converge. Necessity of the cold start. We also study the importance of our two-stage training process. We compare our full model, which is first fine-tuned with SFT and then trained with RL, against baseline trained with RL directly on the base model without the SFT cold start. As illustrated in Figure 16, the model trained without SFT fails to achieve meaningful improvement. This is because the vast and unstructured action space of code generation makes it extremely difficult for the agent to discover useful tool-use policies through pure RL exploration. The SFT stage is crucial for bootstrapping the model, teaching it the fundamental syntax of tool calls and providing strong initial policy. This warm start makes the subsequent RL phase significantly more stable and effective, allowing the model to refine its strategic reasoning on solid foundation. 16 (a) OCRBench-Rot90 (b) ChartQAPro-Hori (c) MVToolBench Figure 17: Accuracy on OCRBench, ChartQAPro, and MVToolBench during RL training. The steady improvement across all benchmarks indicates that the models learning has not yet saturated and could benefit from further scaling of data and tasks. 5. Conclusion In this paper, we identify critical vulnerability in state-of-the-art MLLMs: their pronounced brittleness to natural corruptions such as orientation changes. We introduce CodeVision, the codeas-tool framework, flexible paradigm that empowers models to generate code for virtually unlimited range of visual operations, moving beyond fixed toolsets. We train our models using two-stage process combining SFT and RL with dense, process-oriented reward function to foster strategic tool use. Experiments on our newly created benchmarks demonstrate that this approach remedies the identified weakness and significantly enhances model reasoning. Our framework fosters emergent behaviors like efficient tool-chaining and error recovery, paving the way for more capable MLLMs that treat visual interaction as programming task. 6. Limitations and Future Work In this work, we have focused on core set of toolsprimarily orientation correction and croppingto establish the CodeVision framework and demonstrate its effectiveness in addressing model brittleness. While this targeted approach has proven successful, creating truly general-purpose visual agent requires scaling and expansion in several key dimensions. Our work reveals several limitations that open up exciting avenues for future research: Expanding Tool Diversity and Compositionality. Our current research deliberately concentrates on limited toolset. To enhance the models generalization capabilities, future work should incorporate much wider variety of tool types and data. This includes training on tasks that require more complex tool compositions and even multi-image tool use (e.g., comparing, merging, or analyzing multiple images simultaneously). Furthermore, the code-as-tool paradigm can be extended beyond standard Python libraries to include custom tools, such as proprietary search engines or generative models. By exposing only simple API endpoint to the model, we can empower it to leverage powerful, black-box functionalities, further broadening its problem-solving horizons. Refining Process Supervision with Beneficial Tools. Our current training methodology relies on must-use tool list to provide strong process supervision. While effective, this can be expanded to more flexible framework. Future iterations could incorporate broader set of beneficial tools. These are tools that are not strictly necessary to solve task but are highly likely to improve performance, such as contrast enhancement for low-light images or preliminary crop for focusing attention. By rewarding the use of such advantageous tools, we can encourage the model to learn more nuanced strategies and generalize better to tasks where the optimal toolset is not rigidly defined. 17 Scaling Data, Tasks, and Model Exploration. Our experiments indicate that the models learning has not yet reached saturation. The entropy of its policy suggests that there is still significant room for exploration and improvement. Moreover, as shown in Figure 17, we observe that performance on our benchmarks continues to increase steadily throughout training with no signs of plateauing. This strongly suggests that our approach would benefit from scaling up. By sourcing more diverse data, incorporating wider array of tool types, and designing more varied tasks, we believe there is clear path toward developing even more capable and robust visual agents. References Bai, S., Cai, Y., Chen, R., Chen, K., Chen, X., Cheng, Z., Deng, L., Ding, W., Gao, C., Ge, C., et al. (2025a). Qwen3-vl technical report. arXiv preprint arXiv:2511.21631. Bai, S., Chen, K., Liu, X., Wang, J., Ge, W., Song, S., Dang, K., Wang, P., Wang, S., Tang, J., et al. (2025b). Qwen2.5-vl technical report. arXiv preprint arXiv:2502.13923. Chen, W., Ma, X., Wang, X., and Cohen, W. W. (2023). Program of thoughts prompting: Disentangling computation from reasoning for numerical reasoning tasks. Transactions on Machine Learning Research. Comanici, G., Bieber, E., Schaekermann, M., Pasupat, I., Sachdeva, N., Dhillon, I., Blistein, M., Ram, O., Zhang, D., Rosen, E., et al. (2025). Gemini 2.5: Pushing the frontier with advanced reasoning, multimodality, long context, and next generation agentic capabilities. arXiv preprint arXiv:2507.06261. Fan, Y., He, X., Yang, D., Zheng, K., Kuo, C.-C., Zheng, Y., Narayanaraju, S. J., Guan, X., and Wang, X. E. (2025). Grit: Teaching mllms to think with images. arXiv preprint arXiv:2505.15879. Gao, C., Zheng, C., Chen, X.-H., Dang, K., Liu, S., Yu, B., Yang, A., Bai, S., Zhou, J., and Lin, J. (2025). Soft adaptive policy optimization. arXiv preprint arXiv:2511.20347. Guo, Z., Hong, M., and Jin, T. (2025a). Observe-r1: Unlocking reasoning abilities of mllms with dynamic progressive reinforcement learning. arXiv preprint arXiv:2505.12432. Guo, Z., Zhang, F., Jia, K., and Jin, T. (2025b). Llm-i: Llms are naturally interleaved multimodal creators. arXiv preprint arXiv:2509.13642. Hong, M., Guo, Z., Xia, Y., Wang, Z., Zhang, Z., Jin, T., and Zhao, Z. (2025). Apo: Enhancing reasoning ability of mllms via asymmetric policy optimization. arXiv preprint arXiv:2506.21655. Jin, B., Zeng, H., Yue, Z., Yoon, J., Arik, S., Wang, D., Zamani, H., and Han, J. (2025). Search-r1: Training llms to reason and leverage search engines with reinforcement learning. arXiv preprint arXiv:2503.09516. Lai, X., Li, J., Li, W., Liu, T., Li, T., and Zhao, H. (2025). Mini-o3: Scaling up reasoning patterns and interaction turns for visual search. arXiv preprint arXiv:2509.07969. Li, X., Dong, G., Jin, J., Zhang, Y., Zhou, Y., Zhu, Y., Zhang, P., and Dou, Z. (2025). Search-o1: Agentic search-enhanced large reasoning models. arXiv preprint arXiv:2501.05366. Liu, C.-L., Yin, F., Wang, D.-H., and Wang, Q.-F. (2011). Casia online and offline chinese handwriting databases. In 2011 international conference on document analysis and recognition, pages 3741. IEEE. Liu, Y., Li, Z., Huang, M., Yang, B., Yu, W., Li, C., Yin, X.-C., Liu, C.-L., Jin, L., and Bai, X. (2024). Ocrbench: on the hidden mystery of ocr in large multimodal models. Science China Information Sciences, 67(12):220102. Liu, Z., Zang, Y., Zou, Y., Liang, Z., Dong, X., Cao, Y., Duan, H., Lin, D., and Wang, J. (2025). Visual agentic reinforcement fine-tuning. arXiv preprint arXiv:2505.14246. Long, S., Qin, S., Panteleev, D., Bissacco, A., Fujii, Y., and Raptis, M. (2022). Towards end-to-end unified scene text detection and layout analysis. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. Marti, U.-V. and Bunke, H. (2002). The iam-database: an english sentence database for offline handwriting recognition. International journal on document analysis and recognition, 5(1):3946. 18 Masry, A., Islam, M. S., Ahmed, M., Bajaj, A., Kabir, F., Kartha, A., Laskar, M. T. R., Rahman, M., Rahman, S., Shahmohammadi, M., et al. (2025). Chartqapro: more diverse and challenging benchmark for chart question answering. arXiv preprint arXiv:2504.05506. Masry, A., Kavehzadeh, P., Do, X. L., Hoque, E., and Joty, S. (2023). Unichart: universal vision-language pretrained model for chart comprehension and reasoning. In Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing, pages 1466214684. Mathew, M., Karatzas, D., and Jawahar, C. (2021). Docvqa: dataset for vqa on document images. In Proceedings of the IEEE/CVF winter conference on applications of computer vision, pages 22002209. OpenAI (2025a). Introducing deep research. OpenAI (2025b). Introducing gpt-5. OpenAI (2025c). Introducing openai o3 and o4-mini. Qiao, R., Tan, Q., Yang, P., Wang, Y., Wang, X., Wan, E., Zhou, S., Dong, G., Zeng, Y., Xu, Y., et al. (2025). We-math 2.0: versatile mathbook system for incentivizing visual mathematical reasoning. arXiv preprint arXiv:2508.10433. Schulman, J., Wolski, F., Dhariwal, P., Radford, A., and Klimov, O. (2017). Proximal policy optimization algorithms. arXiv preprint arXiv:1707.06347. Shao, Z., Wang, P., Zhu, Q., Xu, R., Song, J., Bi, X., Zhang, H., Zhang, M., Li, Y., Wu, Y., et al. (2024). Deepseekmath: Pushing the limits of mathematical reasoning in open language models. arXiv preprint arXiv:2402.03300. Singh, A., Natarajan, V., Shah, M., Jiang, Y., Chen, X., Batra, D., Parikh, D., and Rohrbach, M. (2019). Towards vqa models that can read. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 83178326. Su, A., Wang, H., Ren, W., Lin, F., and Chen, W. (2025a). Pixel reasoner: Incentivizing pixel-space reasoning with curiosity-driven reinforcement learning. arXiv preprint arXiv:2505.15966. Su, Z., Li, L., Song, M., Hao, Y., Yang, Z., Zhang, J., Chen, G., Gu, J., Li, J., Qu, X., et al. (2025b). Openthinkimg: Learning to think with images via visual tool reinforcement learning. arXiv preprint arXiv:2505.08617. Team, T. D., Li, B., Zhang, B., Zhang, D., Huang, F., Li, G., Chen, G., Yin, H., Wu, J., Zhou, J., et al. (2025). Tongyi deepresearch technical report. arXiv preprint arXiv:2510.24701. Wang, S., Yang, S., Lin, W., Guo, Z., Cai, S., Huang, H., Wang, Y., Chen, J., and Jin, T. (2025a). Omni-chart600k: comprehensive dataset of chart types for chart understanding. Findings of the Association for Computational Linguistics: NAACL 2025, pages 40514069. Wang, W., Ding, L., Zeng, M., Zhou, X., Shen, L., Luo, Y., Yu, W., and Tao, D. (2025b). Divide, conquer and combine: training-free framework for high-resolution image perception in multimodal large language models. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 39, pages 79077915. Wang, X., Yang, Z., Feng, C., Lu, H., Li, L., Lin, C.-C., Lin, K., Huang, F., and Wang, L. (2025c). Sota with less: Mcts-guided sample selection for data-efficient visual reasoning self-improvement. arXiv preprint arXiv:2504.07934. Wu, P. and Xie, S. (2024). V?: Guided visual search as core mechanism in multimodal llms. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 1308413094. Yu, Q., Zhang, Z., Zhu, R., Yuan, Y., Zuo, X., Yue, Y., Dai, W., Fan, T., Liu, G., Liu, L., et al. (2025). Dapo: An open-source llm reinforcement learning system at scale. arXiv preprint arXiv:2503.14476. Zhang, Y.-F., Lu, X., Yin, S., Fu, C., Chen, W., Hu, X., Wen, B., Jiang, K., Liu, C., Zhang, T., et al. (2025). Thyme: Think beyond images. arXiv preprint arXiv:2508.11630. Zheng, C., Liu, S., Li, M., Chen, X.-H., Yu, B., Gao, C., Dang, K., Liu, Y., Men, R., Yang, A., et al. (2025a). Group sequence policy optimization. arXiv preprint arXiv:2507.18071. 19 Zheng, Z., Yang, M., Hong, J., Zhao, C., Xu, G., Yang, L., Shen, C., and Yu, X. (2025b). Deepeyes: Incentivizing\" thinking with images\" via reinforcement learning. arXiv preprint arXiv:2505.14362."
        }
    ],
    "affiliations": [
        "ByteDance, BandAI",
        "Zhejiang University"
    ]
}