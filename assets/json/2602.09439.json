{
    "paper_title": "Fine-T2I: An Open, Large-Scale, and Diverse Dataset for High-Quality T2I Fine-Tuning",
    "authors": [
        "Xu Ma",
        "Yitian Zhang",
        "Qihua Dong",
        "Yun Fu"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "High-quality and open datasets remain a major bottleneck for text-to-image (T2I) fine-tuning. Despite rapid progress in model architectures and training pipelines, most publicly available fine-tuning datasets suffer from low resolution, poor text-image alignment, or limited diversity, resulting in a clear performance gap between open research models and enterprise-grade models. In this work, we present Fine-T2I, a large-scale, high-quality, and fully open dataset for T2I fine-tuning. Fine-T2I spans 10 task combinations, 32 prompt categories, 11 visual styles, and 5 prompt templates, and combines synthetic images generated by strong modern models with carefully curated real images from professional photographers. All samples are rigorously filtered for text-image alignment, visual fidelity, and prompt quality, with over 95% of initial candidates removed. The final dataset contains over 6 million text-image pairs, around 2 TB on disk, approaching the scale of pretraining datasets while maintaining fine-tuning-level quality. Across a diverse set of pretrained diffusion and autoregressive models, fine-tuning on Fine-T2I consistently improves both generation quality and instruction adherence, as validated by human evaluation, visual comparison, and automatic metrics. We release Fine-T2I under an open license to help close the data gap in T2I fine-tuning in the open community."
        },
        {
            "title": "Start",
            "content": "Fine-T2I: An Open, Large-Scale, and Diverse Dataset for High-Quality T2I Fine-Tuning Dataset: https://huggingface.co/datasets/ma-xu/fine-t2i Space: https://huggingface.co/spaces/ma-xu/fine-t2i-explore Xu Ma 1 Yitian Zhang 1 Qihua Dong 1 Yun Fu 1 6 2 0 2 0 ] . [ 1 9 3 4 9 0 . 2 0 6 2 : r a"
        },
        {
            "title": "Abstract",
            "content": "High-quality and open datasets remain major bottleneck for text-to-image (T2I) fine-tuning. Despite rapid progress in model architectures and training pipelines, most publicly available finetuning datasets suffer from low resolution, poor textimage alignment, or limited diversity, resulting in clear performance gap between open research models and enterprise-grade models. In this work, we present Fine-T2I, large-scale, high-quality, and fully open dataset for T2I finetuning. Fine-T2I spans 10 task combinations, 32 prompt categories, 11 visual styles, and 5 prompt templates, and combines synthetic images generated by strong modern models with carefully curated real images from professional photographers. All samples are rigorously filtered for textimage alignment, visual fidelity, and prompt quality, with over 95% of initial candidates removed. The final dataset contains over 6 million textimage pairs, around 2 TB on disk, approaching the scale of pretraining datasets while maintaining fine-tuning-level quality. Across diverse set of pretrained diffusion and autoregressive models, fine-tuning on Fine-T2I consistently improves both generation quality and instruction adherence, as validated by human evaluation, visual comparison, and automatic metrics. We release Fine-T2I under an open license to help close the data gap in T2I fine-tuning in the open community. 1. Introduction Text-to-image (T2I) generation has advanced rapidly in the most recent two years, enabling the synthesis of highly aesthetic images while faithfully adhering to diverse and com1Department of Electrical & Computer Engineering, NorthCorrespondence to: Xu Ma eastern University, Boston. <ma.xu1@northeastern.edu>. Preprint. February 11, 2026. 1 Dataset JourneyDB Pick-a-Pic T2I-2M LAION-Art LAION-Aesthetic Blip3o-60k Fine-T2I (ours) High Resolution? High Quality? Designed Prompts? Diverse Resolutions? Distribution Analysis? Large Scale? Table 1. We compare our Fine-T2I with open-sourced fine-tuning datasets. High-resolution indicates the resolution of most samples is greater than 1K. More details can be found in Sec. 3. See Fig. 13 for qualitative visual comparison illustrating the data quality. Figure 1. Visual comparison across datasets. For each dataset, we randomly sample three textimage pairs (no cherry-picking) to illustrate overall dataset quality. Zoom in for details. Dataset names for each row are provided on the following page. Fine-T2I: An Open, Large-Scale, and Diverse Dataset for High-Quality T2I Fine-Tuning plex instructions. This progress has been driven by new wave of enterprise-grade generative models, including GPT Image 1.5, Nano Banana Pro (Team et al., 2025), Seedream 4.0 (Seedream et al., 2025), and Qwen-Image (Wu et al., 2025a), among others. Such achievements always stem from two complementary factors: model-centric advances in architectures (Wu et al., 2025a; Gong et al., 2025), training objectives (Liu et al., 2025a;b), and inference tricks (Ma et al., 2025a); and data-centric progress in dataset scale, curation, and quality. Together, they push the frontier of high-fidelity image generation. While both model-centric advances and data-centric progress are critical in image generation, they do not propagate equally across our open community. Model-centric advances are always accessible. Model architectures, training objectives, and inference techniques are routinely shared through publications or blogs and can be adopted by many teams. In contrast, the data-centric parts, especially carefully curated and instruction-aligned fine-tuning data, are often private and not accessible. As result, SOTA models have become increasingly concentrated in top industry groups. Even when the open community matches the model training recipes and has enough computations, the lack of comparable high-quality data can lead to persistent and potentially growing gap. As we can see, the text-to-image leaderboard 1 is dominated by enterprise-grade products. Similar to the open dissemination of model-centric techniques, the community has strong interest in open-sourcing large-scale, high-quality data for T2I training and alignment. Yet this is difficult for two reasons. First, truly high-quality fine-tuning images are extremely expensive (often $10+ per image2). Second, such images are typically restricted by licenses and cannot be legally redistributed. Consequently, while many companies open-sourced the production-grade T2I models, high-quality fine-tuning datasets are rarely made public, and academic groups have no such resource to build large-scale and high-quality fine-tuning dataset. Therefore, the open community can only rely on datasets that are comparatively small, noisy, or distribution-specific, as summarized in Table 1, which clearly bottleneck the training of practical strong models. Although these datasets established the groundwork for the field and made significant contributions, clear gap remains in both scale and quality regarding the requirements for modern T2I fine-tuning. We provide examples from each dataset in Fig. 1, with dataset names presented in the footnote 3. Please see Fig. 13 for more examples. 1https://huggingface.co/spaces/ArtificialAnalysis/Text-toImage-Leaderboard 2https://stock.adobe.com/photos 3Dataset A: LAION-art; Dataset B: Our Fine-T2I (one synthetic subset); Dataset C: JourneyDB; Dataset D: BLIP3o60k (geneval train set). To alleviate this bottleneck, we introduce Fine-T2I, largescale, high-quality, and fully open dataset for text-to-image fine-tuning. Fine-T2I is constructed to close the gap between (i) production-grade model releases and (ii) the lack of publicly available data that is both high-quality and legally licensed. Fine-T2I dataset mixes (a) synthetic data generated by strong diffusion models and (b) curated real images licensed and shared by photographers. For the synthetic portion, we build the dataset from scratch, jointly designing prompts and generating images, covering diverse styles, categories, tasks, and prompt lengths, prompt formats, etc. We further refine each prompt using fine-tuned prompt enhancer model (Wang et al., 2025b), and generate images at either fixed square resolution or randomly sampled predefined aspect ratios, yielding four synthetic subsets. For the curated real-image set, we use strong VLM to generate both short and long prompts. We then design detailed filtering pipeline for our data, including de-duplication, textimage alignment checks, safety guard, and aesthetic quality assessment, removing over 95% of the initially collected data. Finally, the resulting Fine-T2I contains roughly 6 million examples and occupies approximately 2 TB on-disk storage, offering exceptional scale and quality for open T2I fine-tuning. Fig. 2 shows examples of our Fine-T2I dataset. Unlike other open datasets that often assume fixed resolutions (e.g., 512 512), fixed prompt templates (e.g., photo of . . . ), or simple settings (e.g., fixed object and position combinations), Fine-T2I targets more practical goal: closing the gap between open models and production-grade generations. Fine-T2I is designed to improve human-preferred generation quality, aligning with large-scale human evaluation settings. Beyond releasing the Fine-T2I dataset, we also provide detailed pipeline for constructing our large-scale and highquality dataset. Furthermore, we systematically validate its improvements across diverse T2I architectures, from autoregressive to diffusion models. With human evaluations, qualitative comparisons, and quantitative evaluations, we observe consistent and clear improvements when fine-tuning models on our Fine-T2I, indicating the effectiveness of our dataset. We hope that our Fine-T2I dataset, together with the full construction pipeline, can serve as practical and beneficial foundation for image generation. 2. Related Work 2.1. Text-to-Image Generation Currently, most text-to-image generation models are relying primarily on two architectural designs: autoregressive models, which predict discrete visual tokens sequentially (Sun et al., 2024; Wang et al., 2024; Ma et al., 2025b; Tian et al., 2024; Li et al., 2024), and diffusion models (Ho et al., 2020; Rombach et al., 2022; Lipman et al., 2022; Dai et al., 2 Fine-T2I: An Open, Large-Scale, and Diverse Dataset for High-Quality T2I Fine-Tuning Figure 2. Examples of our introduced Fine-T2I dataset samples, which include diverse resolutions, aspect ratios, styles, categories, tasks, etc. Please check the supplementary for examples with detailed attributes and prompts. Images above the dashed line are our synthetic samples, and those below the dashed line are our curated real images. Please also refer our Huggingface Space Page to explore more. 2023; Wu et al., 2025a), which progressively remove noise from randomly initialized Gaussian noise to construct an image. Regardless of the differences in architectural designs, these models typically follow progressive training strategy. First, we begin with large-scale pretraining on massive text-image pair datasets, like LAION-5B (Schuhmann et al., 2022), CC12M (Changpinyo et al., 2021), and OpenImages (Kuznetsova et al., 2020), to learn broad world knowledge and semantic understanding. Next, models are fine-tuned on curated, high-aesthetic datasets, shifting the distribution to enhanced visual fidelity and detailed generation. To close the gap between original training objectives and human preference, recent methods incorporate Reinforcement Learning with Human Feedback (RLHF) such Fine-T2I: An Open, Large-Scale, and Diverse Dataset for High-Quality T2I Fine-Tuning as Direct Preference Optimization (DPO) (Rafailov et al., 2023; Esser et al., 2024; Wang et al., 2024) or Reinforcement Learning (RL) (Liu et al., 2025a; Seedream et al., 2025; Wu et al., 2025b; Wang et al., 2025a). Crucially, this alignment is not one-time process for SOTA image generation models. We see studies on multi-turn training strategy. By iteratively training with human feedback alignment optimization and fine-tuning on high-quality datasets, the model progressively aligns with human preferences while mitigating reward hacking and maintaining training stability (Deng et al., 2025). That indicates the importance of SFT data in the training of text-to-image generation system. 2.2. Fine-tuning Datasets While fine-tuning is crucial for image generation quality, open and licensed high-quality datasets remain scarce, as discussed previously. Table 1 provides detailed summarization for popular fine-tuning datasets, including JourneyDB (Sun et al., 2023), Pick-a-Pic (Kirstain et al., 2023), T2I-2M (jackyhate, 2024), LAION-Art (Schuhmann et al., 2022), LAION-Aesthetic (Schuhmann et al., 2022), and BLIP3o-60k (Chen et al., 2025). While these datasets have laid the groundwork for open image generation research, they still exhibit clear gaps relative to modern productiongrade models. First, many samples from these datasets are low-resolution (often 1024 1024), whereas recent models (e.g., Qwen-Image (Wu et al., 2025a), SeedDream (Gao et al., 2025; Seedream et al., 2025), and Nano Banana Pro (Comanici et al., 2025)) natively generate images at substantially higher resolutions. Second, even after applying aesthetics filters like LAION-Art and LAION-Aesthetic, the resulting data often falls short of the visual fidelity and aesthetics (see Fig. 13) expected by todays models. Most importantly, while always overlooked, existing open datasets rarely provide careful distribution analysis and organization, despite growing evidence that such curation is critical for strong generative performance (Wu et al., 2025a; Seedream et al., 2025; Cai et al., 2025). These limitations motivate the need for new, high-quality fine-tuning dataset tailored to modern T2I generation. 3. Fine-T2I Dataset We introduce Fine-T2I, new fine-tuning dataset designed for modern high-quality image generation in the open community. In this section, we describe the full pipeline, covering both the synthetic sets and the curated real-image set. 3.1. Generating Synthetic Set The synthetic set constitutes the majority of Fine-T2I. We first construct controlled prompts with LLMs and apply rigorous filtering and de-duplication. Each retained prompt is then enhanced into detailed, more descriptive counterpart Figure 3. Semantic cosine-similarities distribution in random prompt subset. We set deduplication threshold to 0.8. using fine-tuned prompt enhancer (Wang et al., 2025b). Using both the original and enhanced prompts, we generate images with SOTA diffusion models (Cai et al., 2025; Labs, 2025) under two generation settings: (i) square aspect ratio and (ii) randomized resolutions. This yields four synthetic sub-datasets in total. Finally, we perform additional filtering to remove low-quality and poorly aligned samples, resulting in 6,145,693 high-quality synthetic samples. 3.1.1. PROMPT GENERATION We generate synthetic prompts using the LLaMA3 instruction model (Grattafiori et al., 2024). To obtain distribution that is both diverse and representative of real usage, we follow the dataset analyses reported in recent public reports (e.g., Qwen-Image (Wu et al., 2025a) and Seedream (Seedream et al., 2025; Gong et al., 2025)) and complement them with trends reflected by the Image Arena leaderboard. Concretely, we design weighted taxonomy of high-level categories, including Nature, Design, People, Text rendering, and set of rare cases. We further decompose each category into fine-grained sub-categories to improve coverage and controllability. We then synthesize controlled prompts by composing the sampled category with several predefined attributes, including weighted style, prompt length, prompt structure (see supplementary Sec A), and task type. Categories and styles are sampled according to calibrated weights to approximate real-world distribution. We emphasize frequently requested content like natureand peoplecentric prompts and broadly useful styles like photorealistic, while still considering non-trivial probability mass to longtail rare scenarios. The remaining attributes are sampled uniformly at random to increase variability. To further reduce template outputs, we set higher sampling temperature 1.4 and lower nucleus threshold top-p = 0.8. We additionally filter out trivial generations by discarding prompts shorter than five words. In total, this yields 44,800,567 controlled prompts. We discuss practical issues encountered during prompt generation in Sec. B. 4 Fine-T2I: An Open, Large-Scale, and Diverse Dataset for High-Quality T2I Fine-Tuning 3.1.2. DEDUPLICATION 3.1.4. PROMPT ENHANCING As standard data-cleaning step (Wu et al., 2025a; Cai et al., 2025; Cao et al., 2025), de-duplication is required to remove identical or near-identical samples. For our synthetic prompt pool, this step is particularly important to increase diversity and to avoid over-representing repeated instructions. In practice, prompt de-duplication is commonly implemented either via (i) MinHash-LSH over token shingles (n-grams) (Wu et al., 2025a) or (ii) embedding-based semantic filtering. Although MinHash-LSH is fast and scalable, shingle-based hashing is largely lexical-overlap driven. Prompts that share templates or phrasing may be removed even when their semantics differ. Since our prompts are LLM-generated and often vary in meaning with critical details under similar syntactic patterns, we adopt semantic de-duplication. We encode each prompt with the all-MiniLM-L6-v2 sentence encoder (Reimers & Gurevych, 2019; Wang et al., 2020) into 384-dimensional vector embedding, and then we cache the embeddings for further processing. Computing full pairwise similarities is infeasible at our scale, so we employ two-stage hierarchical strategy. We first partition the prompt set into 128 groups and perform de-duplication within each group. Next, we run second pass to remove remaining near-duplicates across groups. In both stages, we treat two prompts as duplicates if their cosine similarity exceeds 0.8, see Fig. 3 for illustration. After semantic de-duplication, we retain 5,555,147 diverse prompts. 3.1.3. PROMPT FILTERING We next clean the generated prompt based on the content guard, alignment with corresponding attributes. Guard To ensure the resulting prompts are safe and compliant, we apply content filtering with LLaMA-Guard-38B (Inan et al., 2023). We remove prompts flagged as violent crimes, child sexual exploitation, privacy, and other unacceptable content. In addition, we filter by length and drop the prompts longer than 150 words. Overall, this step provides 5,497,062 safety-checked prompts. Filtering based on attributes In addition to the safety guard, we further filter prompts by checking whether each generated prompt aligns with its intended control attributes. Here, we focus on style and category consistency. We use Qwen3-VL-8B-Instruct (Bai et al., 2025a) as an attribute verifier. Given prompt and its target attributes, the model judges whether the prompt semantically matches the specified style and category. Prompts that fail the alignment check are discarded. After attribute-alignment filtering, we obtain final set of 5,158,969 prompts for subsequent image generation. See Sec. for more discussions. We see image generation models increasingly consider enhanced user inputs during inference. By rewriting the users input into more detailed instruction, models often provide better image quality and stronger alignment (Ma et al., 2025b; Wu et al., 2025a; Comanici et al., 2025). We argue that this idea should not be viewed solely as an inference trick, it can also be valuable during training. Motivated by this, we explicitly provide enhanced prompts in Fine-T2I. For each filtered prompt, we apply fine-tuned prompt enhancer (Wang et al., 2025b) to rewrite it into longer, more descriptive counterpart while maintaining the original intent. The resulting enhanced prompts typically add additional visual attributes and reduce ambiguity. In our pipeline, we keep both counterparts, enabling models to benefit from diverse user input formats while also taking advantages from detailed instructions. 3.1.5. IMAGE GENERATION We generate synthetic images from both prompt sets in FineT2I: the original generated prompts and their enhanced (rewritten) counterparts. For each variant, we generate images under two complementary settings. The first uses randomized resolutions and aspect ratios to better reflect real-world generation requests and to encourage robustness beyond single canonical format. Please check Tab. 8 for detailed resolutions. The second restricts square images with multiple square sizes, since many existing pipelines and benchmarks still primarily consider square inputs. For each prompt, we prioritize quality via generate-andselect strategy. We sample 1-3 candidate images using two strong open-source generators, Z-Image (Cai et al., 2025) and FLUX2 (Labs, 2025), and keep the candidate with the highest Aesthetic Predictor V2.5 score4. Compared with the commonly-used models like FLUX (Labs et al., 2025; Labs, 2024), GPT-4o, MidJourney used in T2I-2M, BLIP3o, JourneyDB, the aforementioned selected models can provide significantly better generation quality and stronger aesthetic appeal, which is critical for constructing high-quality fine-tuning set. Combining the two prompt variants with the two resolution settings yields four synthetic subsets: Enhanced+Square, Enhanced+Random, Original+Square, and Original+Random. Since FLUX2 is substantially more expensive to run, most images in Fine-T2I are generated with Z-Image. Please check Fig. 2 for generated examples. 3.1.6. TEXT-IMAGE PAIR FILTERING Although we are using strong generators, the generated images are not always reliable. In practice, failures mainly arise from two reasons: (i) insufficient visual quality and 4https://github.com/discus0434/aesthetic-predictor-v2-5 Fine-T2I: An Open, Large-Scale, and Diverse Dataset for High-Quality T2I Fine-Tuning Table 2. Number of samples and storage for the synthetic set after filtering. PE indicates enhanced prompts, PO denotes original prompts, AR stands for random aspect ratios, and AS means square aspect ratio. Table 3. Number of samples for curated real-image set. We curated about 168k extremely high-quality samples from open-sourced platforms for this set in Fine-T2I. Subset PE-AR PE-AS PO-AR PO-AS Final Samples Storage 1,615,592 476G 1,538,253 517G 1,686,498 479G 1,305,350 436G aesthetics, and (ii) imperfect text-image alignment. We therefore apply two-stage filtering pipeline for these issues. We first filter images using Aesthetic Predictor V2.5 that focuses on image aesthetics. We keep an image only if its score > 5.5, which can be considered very high visual quality. Filtering text-image alignment is substantially more challenging. Automated metrics like HPSv2 (Wu et al., 2023) and HPSv3 (Ma et al., 2025c) provide useful insights, but we notice that they are insufficiently strict for fine-tuning data: they often miss fine-grained mismatches and fail to detect common generation artifacts. Indeed, we did not observe an off-the-shelf automatic metric that meets the high requirements needed for building production-grade fine-tuning dataset. To address this, we perform alignment verification (as well as artifacts check) with VLM with reasoning ability or thinking mode. Although this brings much higher computational cost, we find that, under carefully designed system prompt, the VLM becomes highly accurate at checking prompt compliance and typical failure modes in generated images. Our system prompt explicitly enumerates these alignment and artifact criteria. If sample violates any requirement, we filter it out. This reasoning-powered and VLM-based verification is crucial for ensuring that the remaining synthetic subset meets the quality bar required for fine-tuning. The detailed system prompt is provided in Fig. 12. On average, we further filtered out about 70% of text-image pairs in these steps, and the final samples are presented in Table 2. 3.2. Curating Real-Image Set In addition to synthetic samples, Fine-T2I also includes curated real-image set. Curating high-quality real data is inherently ambiguous: aesthetic quality is difficult to formalize, and purely automatic scorers can behave unpredictably on professional photography (e.g., subtle composition and lighting choices are not always captured by single scalar score). Please see Sec. for more discussions on image aesthetics and the current trend for human preference of generated images. We therefore start from sources where aesthetic judgment is already exercised by humans. Specifically, we collect publicly available images from creatordriven platforms: Pexels5, Pixabay6, and Unsplash-Lite7, 5https://www.pexels.com/ 6https://pixabay.com/ 7https://unsplash.com/ Data Source Downloaded After Filter Kept ratio Storage Pexels Pixabay Unsplash lite 233,342 163,524 24, 117,389 32,654 18,381 50.3% 20.0% 73.6% 192.6 GB 9.7 GB 56.2 GB where photos are uploaded and curated by photographers and creators under open terms. This gives us strong initial prior on aesthetics while retaining broad coverage of everyday photographic content. We then impose additional quality constraints to meet the fine-tuning requirements. We filter images using Aesthetic Predictor V2.5 and keep only those with scores 6.5, showing our stricter standard for this subset. We also discard extremely large images for potential training stability issue. To make these images qualified for text-conditioned training, we generate captions with fine-tuned8 Qwen2.5-VL-7B model (Bai et al., 2025b). Same as the synthetic set, we provide two prompts for each image: an initial prompt (short length) and more detailed enhanced version generated by the same prompt enhancement procedure. This mirrors the way prompts are used in practice, from short user inputs to more explicit, descriptive instructions, and provides richer instructions for fine-tuning. After filtering and captioning steps, the real-image subset contains 168,424 images. The detailed statistics are presented in Table 3. 4. Fine-T2I Dataset Specifications The above section detailed the pipeline for building our dataset. Next, we provide detailed insights into our dataset. Analysis on the attributes We analyze the distribution of Fine-T2I to understand its coverage and balance. Since the synthetic set dominates the dataset and comes with explicit attribute labels, we focus our analysis on the synthetic subset. Fig. 4 summarizes the distributions over categories, styles, and tasks. As shown in Fig. 4a, the category mixture is focusing on realistic, high-demand content: People (37.9%) and Nature (27.8%) form the majority, while Text Rendering (17.4%) and Design (10.6%) provide substantial coverage of instruction-heavy and layout-sensitive cases, and Rare cases (6.3%) ensures non-trivial long-tail support. Fig. 4b indicates that styles are similarly balanced: General & Photorealistic is the largest portion (20%), complemented by diverse artistic and design-oriented styles, which helps the dataset span both photorealistic generation and stylized creation. Finally, Fig. 4c shows that while most prompts correspond to single-task instructions (63.1%), sizable fraction involves two-task compositions (36.9%), covering 8https://huggingface.co/Ertugrul/Qwen2.5-VL-7B-CaptionerRelaxed Fine-T2I: An Open, Large-Scale, and Diverse Dataset for High-Quality T2I Fine-Tuning (a) Categories Analysis (b) Styles Analysis Figure 4. Analysis for our synthetic set, we provide the distribution for sample categories, sample styles, and the related tasks. Notice that for the tasks, we only consider the prompts that have specific requirements for the task when generating the prompts, while about 61.5% of prompts did not ask for specific tasks. Please check Sec. in the supplementary for details. (c) Tasks Analysis (a) Human-evaluation results on LlamaGen. Figure 5. The aesthetic score distribution of our Fine-T2I. Both the synthetic sets and the curated set demonstrate high aesthetic scores, implying strong visual quality for fine-tuning. attributes such as reasoning, counting, colors, and positions. These tasks provide meaningful instructions on hard sample generation. Please also check Fig. 9 in the supplementary for the study of prompt length distribution. Although these empirical distributions deviate slightly from the initial sampling weights (due to non-uniform rejection during safety, alignment, and quality filtering), the final dataset still closely matches the intended real-world demand. It emphasizes common user scenarios without collapsing diversity, and preserves long-tail and multi-attribute coverage, which is extremely critical for training strong modern T2I models like Qwen-Image or Z-Image. Analysis on the aesthetic quality We further examine the aesthetic quality of Fine-T2I using the Aesthetic Predictor V2.5 scores, with distributions for the synthetic and curated real-image sets shown in Fig. 5. For this metric, scores above 5.5 are generally regarded as indicative of high aesthetic quality. We have two observations from the distribution. First, the curated real-image subset is more concentrated at higher scores, with the majority falling in [6.5, 7.0) (63.21%) and substantial portion in [7.0, 7.5), plus nontrivial tail beyond 7.5. This is consistent with our sourcing (b) Human-evaluation results on SD-XL. Figure 6. Human-evaluation results for the autoregressive model LlamaGen and the diffusion model SD-XL. We compare generations produced with and without fine-tuning on our dataset. strategy from creator-driven photography platforms and our stricter filtering criteria. Second, the synthetic subset exhibits broader score distribution. Most samples lie in the range of [5.5, 6.5) (36.64% in [5.5, 6.0) and 35.70% in [6.0, 6.5)), while meaningful fraction reaches higher aesthetic ranges, with small tail above 7.5. This reflects the inherent variability of generative models and our use of permissive threshold to preserve scale and diversity. Overall, the two subsets are complementary. The curated real-image set provides consistently high-aesthetic standard, while the synthetic set contributes large-scale, diverse guidance with controlled attributes. 5. Experiments with Fine-T2I We next evaluate Fine-T2I for image generation, investigating two problems: whether Fine-T2I consistently improves generation quality and instruction following across model families, and (ii) which aspects Fine-T2I improves the most. 7 Fine-T2I: An Open, Large-Scale, and Diverse Dataset for High-Quality T2I Fine-Tuning Table 4. GenEval benchmark results for LlamaGen and SD-XL. While this auto-evaluation does not fully capture the benefits of our dataset, consistent improvements are still observed. Model LlamaGen 0.71 - w/ Fine-T2I 0. SD-XL 0.98 - w/ Fine-T2I 0.98 Single Two Count Color Position Attribute Overall 0.34 0.21 0.40 0.37 0.74 0.39 0.79 0.40 0.58 0.74 0.85 0. 0.07 0.11 0.15 0.21 0.04 0.14 0.23 0.27 0.32 0.41 (+0.09) 0.55 0.61 (+0.06) can be misaligned with human preference, due to restricted prompt diversity. This limitation is especially pronounced when the goal is to assess the quality of fine-tuning dataset, where improvements may manifest in subtle aspects such as aesthetics, style fidelity, and nuanced instruction following. To better reflect real-world usage and human judgment, we therefore construct an evaluation suite by randomly sampling 500 public prompts from the Artificial Analysis Image Arena leaderboard. These prompts cover diverse user requests and have been widely used to compare the leading T2I models. We conduct large-scale human preference evaluation on the resulting generations, focusing on text-image alignment and overall human-preferred visual quality. For completeness, we also report GenEval results in Table 4, which provide reference point for improvements on established automatic protocols. But we emphasize that we should not heavily rely on these automated metrics. Results analysis We primarily evaluate the improvements from fine-tuning on Fine-T2I based on human evaluation, since our goal is to assess Fine-T2I as fine-tuning dataset and the improvements we target (overall aesthetics, realism, and instruction following) are not reliably captured by automatic benchmarks. As shown in Fig. 6, fine-tuning on Fine-T2I leads to clear preference gains on both models. By fine-tuning LlamaGen on our dataset, we achieve an 80.7% win rate for visual quality compared to counterpart without further fine-tuning, and 65.3% win rate for textimage alignment. We also see substantive improvements for SD-XL. These observations suggest that our Fine-T2I is able to boost different architectures and generation designs, providing consistent benefits across different model families. Meanwhile, we also notice that our dataset boosts the visual quality the most. This might be due to the extremely high aesthetic quality of our dataset. The qualitative examples are presented in Fig. 7. After fine-tuning on Fine-T2I, generations look more natural and coherent, with cleaner local details and fewer distracting artifacts, while better reflecting the intended prompt content. Although we do not view it as the main evidence, GenEval also improves for both models, as shown in Table 4, providing an additional signal that Fine-T2I enhances controllability on standard automatic protocols. Figure 7. Visual comparison between original generations and generations fine-tuned on our Fine-T2I. One can observe clearly better generation quality on our fine-tuned model. Experimental Setting Rather than focusing on single modeling family, we consider two representative T2I backbones spanning different generation technologies: diffusion-based model SD-XL (Podell et al., 2023) and an autoregressive model LlamaGen (Sun et al., 2024). We select these two models because they are not well-trained on closed high-quality datasets like Qwen-Image or NanoBanana, better demonstrating the performance gap. For each model, we start from the official publicly released pretrained checkpoint and perform continued fine-tuning on Fine-T2I. Fine-T2I includes four synthetic subsets (original/enhanced square/random resolution) as well as curated real-image set; unless otherwise specified, we fine-tune the two models using mixture of all subsets. For SD-XL, we fine-tune with LoRA adapters to ensure lightweight training setup, batch size of 8 on each GPU and learning rate of 1 104. For LlamaGen, we fine-tune the full model using batch size 24 and learning rate 3 105. We filter out samples with relatively long prompts, considering the limited token length supported by the two models. We fine-tune each model with around 1 epoch to avoid overfitting our dataset. While better settings may further improve the fine-tuning results, that is not our main target here. Evaluation As noted in HunyuanImage (Cao et al., 2025) and related work, commonly used automatic benchmarks such as GenEval (Ghosh et al., 2023) and T2ICompBench (Huang et al., 2023) have limited coverage and 8 Fine-T2I: An Open, Large-Scale, and Diverse Dataset for High-Quality T2I Fine-Tuning Y., Tang, J., Tu, J., Wan, J., Wang, P., Wang, P., Wang, Q., Wang, Y., Xie, T., Xu, Y., Xu, H., Xu, J., Yang, Z., Yang, M., Yang, J., Yang, A., Yu, B., Zhang, F., Zhang, H., Zhang, X., Zheng, B., Zhong, H., Zhou, J., Zhou, F., Zhou, J., Zhu, Y., and Zhu, K. Qwen3-vl technical report. arXiv preprint arXiv:2511.21631, 2025a. Bai, S., Chen, K., Liu, X., Wang, J., Ge, W., Song, S., Dang, K., Wang, P., Wang, S., Tang, J., et al. Qwen2. 5-vl technical report. arXiv preprint arXiv:2502.13923, 2025b. Cai, H., Cao, S., Du, R., Gao, P., Hoi, S., Hou, Z., Huang, S., Jiang, D., Jin, X., Li, L., et al. Z-image: An efficient image generation foundation model with single-stream diffusion transformer. arXiv preprint arXiv:2511.22699, 2025. Cao, S., Chen, H., Chen, P., Cheng, Y., Cui, Y., Deng, X., Dong, Y., Gong, K., Gu, T., Gu, X., et al. Hunyuanimage 3.0 technical report. arXiv preprint arXiv:2509.23951, 2025. Changpinyo, S., Sharma, P., Ding, N., and Soricut, R. Conceptual 12m: Pushing web-scale image-text pre-training to recognize long-tail visual concepts. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pp. 35583568, 2021. Chen, J., Xu, Z., Pan, X., Hu, Y., Qin, C., Goldstein, T., Huang, L., Zhou, T., Xie, S., Savarese, S., et al. Blip3-o: family of fully open unified multimodal models-architecture, training and dataset. arXiv preprint arXiv:2505.09568, 2025. Comanici, G., Bieber, E., Schaekermann, M., Pasupat, I., Sachdeva, N., Dhillon, I., Blistein, M., Ram, O., Zhang, D., Rosen, E., et al. Gemini 2.5: Pushing the frontier with advanced reasoning, multimodality, long context, and next generation agentic capabilities. arXiv preprint arXiv:2507.06261, 2025. Dai, X., Hou, J., Ma, C.-Y., Tsai, S., Wang, J., Wang, R., Zhang, P., Vandenhende, S., Wang, X., Dubey, A., et al. Emu: Enhancing image generation models using photogenic needles in haystack. arXiv preprint arXiv:2309.15807, 2023. Deng, Y., Bansal, H., Yin, F., Peng, N., Wang, W., and Chang, K.-W. Openvlthinker: Complex vision-language reasoning via iterative sft-rl cycles. In The Thirty-ninth Annual Conference on Neural Information Processing Systems, 2025. Esser, P., Kulal, S., Blattmann, A., Entezari, R., Muller, J., Saini, H., Levi, Y., Lorenz, D., Sauer, A., Boesel, F., et al. Scaling rectified flow transformers for high-resolution Figure 8. Human evaluation results (win rate) comparing models fine-tuned on different datasets. For each comparison, the best generation result among the three is selected as the winning example. Comparing with other SFT datasets Previous studies demonstrate the high-quality of our Fine-T2I and the benefits of our dataset. Here, we compare our dataset with other datasets. We use LlamaGen as the candidate model to finetune considering the simplicity. We finetune LlamaGen with the same settings on T2I-2M, BLIP3o-60k, and our Fine-T2I, respectively. We do human evaluation (select the best from three generations) on the 500 prompts as mentioned before, and report the win rate results in Fig. 8. The generated images can be found in Fig. 11 for visual comparison. Clearly, models fine-tuned on Fine-T2I dataset generate much better results than the other two datasets on both text alignment and visual quality. We also suggest referring to Fig. 13 for visual quality comparison of the samples in different fine-tuning datasets. 6. Conclusion Instead of model-centric innovations, public and open highquality fine-tuning data remains key bottleneck for highquality text-to-image generation in the open community. To address this gap, we introduce Fine-T2I, large-scale, fully open dataset (over 6M samples) that combines diverse synthetic textimage pairs with rigorously curated realimage set. Fine-T2I is built with systematic data design and filtering pipeline that prioritizes diversity, alignment, and visual quality at scale. We show that fine-tuning on Fine-T2I consistently improves different model families, yielding stronger instruction adherence and much higher visual quality. With Fine-T2I released under an open license, we aim to provide solid foundation for future work on textto-image generation, and pave the way to promising modern text-to-image models in the open community."
        },
        {
            "title": "References",
            "content": "Bai, S., Cai, Y., Chen, R., Chen, K., Chen, X., Cheng, Z., Deng, L., Ding, W., Gao, C., Ge, C., Ge, W., Guo, Z., Huang, Q., Huang, J., Huang, F., Hui, B., Jiang, S., Li, Z., Li, M., Li, M., Li, K., Lin, Z., Lin, J., Liu, X., Liu, J., Liu, C., Liu, Y., Liu, D., Liu, S., Lu, D., Luo, R., Lv, C., Men, R., Meng, L., Ren, X., Ren, X., Song, S., Sun, 9 Fine-T2I: An Open, Large-Scale, and Diverse Dataset for High-Quality T2I Fine-Tuning image synthesis. In Forty-first international conference on machine learning, 2024. Labs, B. F. FLUX.2: Frontier Visual Intelligence. https: //bfl.ai/blog/flux-2, 2025. Gao, Y., Gong, L., Guo, Q., Hou, X., Lai, Z., Li, F., Li, L., Lian, X., Liao, C., Liu, L., et al. Seedream 3.0 technical report. arXiv preprint arXiv:2504.11346, 2025. Ghosh, D., Hajishirzi, H., and Schmidt, L. Geneval: An object-focused framework for evaluating text-to-image alignment. Advances in Neural Information Processing Systems, 36:5213252152, 2023. Gong, L., Hou, X., Li, F., Li, L., Lian, X., Liu, F., Liu, L., Liu, W., Lu, W., Shi, Y., et al. Seedream 2.0: native chinese-english bilingual image generation foundation model. arXiv preprint arXiv:2503.07703, 2025. Grattafiori, A., Dubey, A., Jauhri, A., Pandey, A., Kadian, A., Al-Dahle, A., Letman, A., Mathur, A., Schelten, A., Vaughan, A., et al. The llama 3 herd of models. arXiv preprint arXiv:2407.21783, 2024. Ho, J., Jain, A., and Abbeel, P. Denoising diffusion probabilistic models. Advances in neural information processing systems, 33:68406851, 2020. Huang, K., Sun, K., Xie, E., Li, Z., and Liu, X. T2icompbench: comprehensive benchmark for open-world compositional text-to-image generation. Advances in Neural Information Processing Systems, 36:7872378747, 2023. Inan, H., Upasani, K., Chi, J., Rungta, R., Iyer, K., Mao, Y., Tontchev, M., Hu, Q., Fuller, B., Testuggine, D., et al. Llama guard: Llm-based input-output safeguard for human-ai conversations. arXiv preprint arXiv:2312.06674, 2023. jackyhate. text-to-image-2m. huggingface.co/datasets/jackyhate/ text-to-image-2M, 2024. https:// Kirstain, Y., Polyak, A., Singer, U., Matiana, S., Penna, J., and Levy, O. Pick-a-pic: An open dataset of user preferences for text-to-image generation. Advances in neural information processing systems, 36:3665236663, 2023. Kuznetsova, A., Rom, H., Alldrin, N., Uijlings, J., Krasin, I., Pont-Tuset, J., Kamali, S., Popov, S., Malloci, M., Kolesnikov, A., et al. The open images dataset v4: Unified image classification, object detection, and visual reInternational journal of lationship detection at scale. computer vision, 128(7):19561981, 2020. Labs, B. F. Flux. black-forest-labs/flux, 2024. https://github.com/ Labs, B. F., Batifol, S., Blattmann, A., Boesel, F., Consul, S., Diagne, C., Dockhorn, T., English, J., English, Z., Esser, P., Kulal, S., Lacey, K., Levi, Y., Li, C., Lorenz, D., Muller, J., Podell, D., Rombach, R., Saini, H., Sauer, A., and Smith, L. Flux.1 kontext: Flow matching for incontext image generation and editing in latent space, 2025. URL https://arxiv.org/abs/2506.15742. Li, T., Tian, Y., Li, H., Deng, M., and He, K. Autoregressive image generation without vector quantization. Advances in Neural Information Processing Systems, 37:56424 56445, 2024. Lipman, Y., Chen, R. T., Ben-Hamu, H., Nickel, M., and Le, M. Flow matching for generative modeling. arXiv preprint arXiv:2210.02747, 2022. Liu, J., Liu, G., Liang, J., Li, Y., Liu, J., Wang, X., Wan, P., Zhang, D., and Ouyang, W. Flow-grpo: Training flow matching models via online rl. arXiv preprint arXiv:2505.05470, 2025a. Liu, Z., Chen, C., Li, W., Qi, P., Pang, T., Du, C., Lee, W. S., and Lin, M. Understanding r1-zero-like training: critical perspective. arXiv preprint arXiv:2503.20783, 2025b. Ma, N., Tong, S., Jia, H., Hu, H., Su, Y.-C., Zhang, M., Yang, X., Li, Y., Jaakkola, T., Jia, X., et al. Inference-time scaling for diffusion models beyond scaling denoising steps. arXiv preprint arXiv:2501.09732, 2025a. Ma, X., Sun, P., Ma, H., Tang, H., Ma, C.-Y., Wang, J., Li, K., Dai, X., Shi, Y., Ju, X., et al. Token-shuffle: Towards high-resolution image generation with autoregressive models. arXiv preprint arXiv:2504.17789, 2025b. Ma, Y., Wu, X., Sun, K., and Li, H. Hpsv3: Towards widespectrum human preference score. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pp. 1508615095, 2025c. Podell, D., English, Z., Lacey, K., Blattmann, A., Dockhorn, T., Muller, J., Penna, J., and Rombach, R. Sdxl: Improving latent diffusion models for high-resolution image synthesis. arXiv preprint arXiv:2307.01952, 2023. Rafailov, R., Sharma, A., Mitchell, E., Manning, C. D., Ermon, S., and Finn, C. Direct preference optimization: Your language model is secretly reward model. Advances in neural information processing systems, 36: 5372853741, 2023. Reimers, N. and Gurevych, I. Sentence-bert: Sentence embeddings using siamese bert-networks. arXiv preprint arXiv:1908.10084, 2019. 10 Fine-T2I: An Open, Large-Scale, and Diverse Dataset for High-Quality T2I Fine-Tuning Wu, C., Li, J., Zhou, J., Lin, J., Gao, K., Yan, K., Yin, S.-m., Bai, S., Xu, X., Chen, Y., et al. Qwen-image technical report. arXiv preprint arXiv:2508.02324, 2025a. Wu, J., Gao, Y., Ye, Z., Li, M., Li, L., Guo, H., Liu, J., Xue, Z., Hou, X., Liu, W., et al. Rewarddance: Reward scaling in visual generation. arXiv preprint arXiv:2509.08826, 2025b. Wu, X., Hao, Y., Sun, K., Chen, Y., Zhu, F., Zhao, R., and Li, H. Human preference score v2: solid benchmark for evaluating human preferences of text-to-image synthesis. arXiv preprint arXiv:2306.09341, 2023. Rombach, R., Blattmann, A., Lorenz, D., Esser, P., and Ommer, B. High-resolution image synthesis with latent diffusion models. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pp. 1068410695, 2022. Schuhmann, C., Beaumont, R., Vencu, R., Gordon, C., Wightman, R., Cherti, M., Coombes, T., Katta, A., Mullis, C., Wortsman, M., et al. Laion-5b: An open large-scale dataset for training next generation image-text models. Advances in neural information processing systems, 35: 2527825294, 2022. Seedream, T., Chen, Y., Gao, Y., Gong, L., Guo, M., Guo, Q., Guo, Z., Hou, X., Huang, W., Huang, Y., et al. Seedream 4.0: Toward next-generation multimodal image generation. arXiv preprint arXiv:2509.20427, 2025. Sun, K., Pan, J., Ge, Y., Li, H., Duan, H., Wu, X., Zhang, R., Zhou, A., Qin, Z., Wang, Y., et al. Journeydb: benchmark for generative image understanding. Advances in neural information processing systems, 36:4965949678, 2023. Sun, P., Jiang, Y., Chen, S., Zhang, S., Peng, B., Luo, P., and Yuan, Z. Autoregressive model beats diffusion: Llama for scalable image generation. arXiv preprint arXiv:2406.06525, 2024. Team, G., Kamath, A., Ferret, J., Pathak, S., Vieillard, N., Merhej, R., Perrin, S., Matejovicova, T., Rame, A., Rivi`ere, M., et al. Gemma 3 technical report. arXiv preprint arXiv:2503.19786, 2025. Tian, K., Jiang, Y., Yuan, Z., Peng, B., and Wang, L. Visual autoregressive modeling: Scalable image generation via next-scale prediction. Advances in neural information processing systems, 37:8483984865, 2024. Wang, G.-H., Cao, L., Cui, T., Fu, M., Chen, X., Zhan, P., Zhao, J., Li, L., Fu, B., Liu, J., et al. Ovis-image technical report. arXiv preprint arXiv:2511.22982, 2025a. Wang, L., Xing, X., Cheng, Y., Zhao, Z., Li, D., Hang, T., Tao, J., Wang, Q., Li, R., Chen, C., et al. Promptenhancer: simple approach to enhance text-to-image models via chain-of-thought prompt rewriting. arXiv preprint arXiv:2509.04545, 2025b. Wang, W., Wei, F., Dong, L., Bao, H., Yang, N., and Zhou, M. Minilm: Deep self-attention distillation for task-agnostic compression of pre-trained transformers. Advances in neural information processing systems, 33: 57765788, 2020. Wang, X., Zhang, X., Luo, Z., Sun, Q., Cui, Y., Wang, J., Zhang, F., Wang, Y., Li, Z., Yu, Q., et al. Emu3: Next-token prediction is all you need. arXiv preprint arXiv:2409.18869, 2024. 11 Fine-T2I: An Open, Large-Scale, and Diverse Dataset for High-Quality T2I Fine-Tuning A. Detailed Distribution analysis Table 6. Detailed statistics for categories in our Fine-T2I four synthetic subsets. In this section, we provide more detailed analysis of our Fine-T2I dataset. Table 5 summarizes style statistics for our synthetic sets. To further characterize the data, Fig. 9 shows the prompt-length distribution. Original prompts are generally short (typically under 50 words), whereas enhanced prompts are substantially longer and span wider range of lengths. We observe the same trend in both the synthetic sets and the curated real-image set. Finally, we report detailed statistics for categories and tasks in Table 6 and Table 7, respectively. For transparency, we also report our pre-defined resolutions and aspect ratios for creating synthetic images in Table 8. To ensure diversity in prompt structure and template, we design five distinct instructions (which is added to the system prompts) that guide LLaMA3 during prompt generation. These templates are listed as follows: simple structure starts like: photo of xxx, an image of xxx, picture of xxxx, etc. Design Nature People Rare like user is asking for request, for examples, please help generate xxx, Could you please help provide an image xxxx, generate an image of, etc. Text rendering Category PE-AR PE-AS PO-AR PO-AS Total Arts Cartoon Creative Logos Posters Rare cases Slides UI/UX Animals Cityscape Creative Food Indoor Landscape Objects / Entities Plants Rare cases Activities Age Creative Emotions Portrait Rare cases Sports Rare cases Creative Handwriting Long text Rare cases Short text Stylized text Text in scene 59722 21975 3734 3434 28819 4963 19475 28266 59710 64063 4540 61647 47106 35308 122078 34259 149561 61683 11868 98799 219778 10186 42077 56909 22994 3593 3616 27444 4736 18751 27310 57187 58235 4336 56695 43443 33341 115692 32686 5869 136908 57572 11057 93809 209840 9289 38626 67021 25973 4245 3240 28142 5654 17708 28417 66406 71190 4703 77209 57773 35869 131568 36440 174234 67566 13494 101281 242322 8822 47790 51893 20517 3267 2489 21159 4050 13326 20424 51122 52812 3694 61570 45450 28995 102185 29212 4674 136109 54927 10478 82550 197778 6290 33910 235545 91459 14839 12779 105564 19403 69260 104417 234425 246300 17273 257121 193772 133513 471523 132597 596812 241748 46897 376439 869718 34587 162403 104062 100557 105787 76979 5466 13841 74115 3938 111131 23797 79861 5425 13614 70762 3770 110970 24374 78843 4755 11586 43075 3172 101908 20920 72041 3598 8692 29381 2209 77247 15560 52803 19244 47733 217333 13089 401256 84651 283548 one or multiple sentences. combinations of sentence(s) and words (e.g., sentence of multiple sentence, + 8K resolution, photorealistic, natural, etc.), etc. simple combination of of words (e.g., dog, flying, over sea, etc.). Table 5. Detailed statistics for styles in our Fine-T2I four synthetic subsets. PE indicates enhanced prompt, PO indicates original prompt, AR means random pre-defined aspect ratios, and AS means square aspect ratio. This table shows the detailed sample numbers belonging to each pre-defined style. Styles PE-AR PE-AS PO-AR PO-AS Total 3D & CGI Abstract & Artistic Anime Cartoon & Illustration Cultural & Folk Art Futuristic & Sci-Fi Graphic& Digital Others Traditional/modern Art Vintage & Retro General & Photorealistic 60309 143509 189551 134538 141827 92631 210362 36275 155099 124268 327223 58141 138761 183866 132375 128671 92110 200629 34006 149882 114807 305005 62090 139430 189392 140222 161804 86677 221038 37378 159576 151822 337069 47831 109619 143096 112841 123120 59929 175241 29387 125033 115733 228371 531319 705905 519976 555422 331347 807270 137046 589590 506630 1232817 B. Problems, Ambiguity, Expectation Diversity of generated Prompt During prompt synthesis, we observed severe redundancy. Prompts generated across Table 7. Detailed statistics for tasks in our Fine-T2I four synthetic subsets. For general generation, over half of the prompts are not assigned particular task. Tasks Colors Counting Position Reasoning Colors + Counting Colors + Position Colors + Reasoning Counting + Position Counting + Reasoning Position + Reasoning PE-AR PE-AS PO-AR PO-AS Total 84885 83826 101398 127420 29754 40304 46128 37795 38366 81386 77285 96048 118657 89151 76807 103106 136928 27932 38229 43543 35262 35129 46546 25489 40570 48277 33496 34176 50872 70316 56341 80280 106034 18589 31972 37638 24721 24983 325738 294259 380832 489039 101764 151075 175586 131274 132654 186433 Not Specified 976018 938236 815159 3777039 different batches and even across different GPUs were often highly similar, and in many cases even identical. Assigning different random seeds for each GPU did not help the problem lot. As mitigation, we increased the LLM sampling temperature to encourage stochasticity. However, this also raised the variance of outputs and occasionally produced malformed generations that were not valid prompts (which we filtered out later). Instead, conditioning on predefined attributes such as style and category further alleviated the issue, but substantial duplication persisted and we did not find principled way to eliminate it. Consequently, the deduplication stage alone removed nearly 90% of the generated candidates, leading to significant wasted computation. 12 Fine-T2I: An Open, Large-Scale, and Diverse Dataset for High-Quality T2I Fine-Tuning (a) synthetic sets prompt length distribution. (b) curated real-image set prompt length distribution. Figure 9. Prompt length distribution analysis. Table 8. Pre-defined resolutions and aspect ratios for generating synthetic images. Aspect ratio Resolution Square Landscape Portrait 1:1 1:1 1:1 1:1 1:1 1:1 4:3 4:3 4:3 3:2 3:2 16:9 16:9 16:9 5:4 5:4 3:4 3:4 3:4 2:3 2:3 9:16 9:16 9:16 4:5 4:5 [512, 512] [768, 768] [1024, 1024] [1536, 1536] [2048, 2048] [2560, 2560] [2048, 1536] [1024, 768] [1600, 1200] [1152, 768] [1536, 1024] [1280, 720] [2048, 1152] [2560, 1440] [1280, 1024] [1920, 1536] [768, 1024] [1200, 1600] [1536, 2048] [768, 1152] [1024, 1536] [720, 1280] [1152, 2048] [1440, 2560] [1024, 1280] [1536, 1920] Attributes alignment with prompt Although we apply attribute-based filtering in Sec. 3.1.3, we do not guarantee the attributes (category, style, and tasks) can be perfectly aligned with the resulting prompt. In practice, some attribute combinations are internally inconsistent or overly constrained. For example, we may request an extremely concise prompt (e.g., less than 10 words) while simultaneously requiring long textual rendering description, or impose strong multi-factor constraints such as UI/UX design + cultural style + counting and colors tasks. Such contradictory or complex conditions can prevent the attributes from being faithfully expressed in the generated prompt. We therefore recommend treating the attributes as soft metadata when using the dataset. They are useful for reference and analysis but may not be accurate, while we ensure the textimage pairs remain reliable for training and evaluation. If stricter attribute adherence is required, one can apply an additional LLM-based reasoning filter (as in our later pipeline stages in Sec. 3.1.6), but this typically results in substantially more aggressive filtering. Ambiguity of aesthetics and human preference What people call high-quality image in text-to-image generation has been changing quickly. few years ago, highly stylized, visually appearing generations were often considered as higher quality. Instead, people now prefer realistic, coherent, and photograph-like generations. This makes aesthetics an inherently ambiguous target for dataset collection, and single automatic aesthetic score (or even combination of multiple scores) can disagree with what humans actually prefer. Fig. 10 shows representative example. The top row enjoys higher aesthetic score, yet people may believe the bottom row is better because it looks more natural. Given this mismatch, we do not treat any aesthetic metric as ground truth. Instead, we follow pragmatic criterion aligned with current preferences, using the quality signals embodied by strong image generators when generating our synthetic sets, and we trust the taste of professional photographers when collecting our curated real-image set. Figure 10. Aesthetics scores may be ambiguous and may not be aligned with human preference. Indeed, aesthetics always change with the development and can be biased. Please zoom in to see details. Fine-T2I: An Open, Large-Scale, and Diverse Dataset for High-Quality T2I Fine-Tuning pairs. This also suggests that both the open community and industry need to put more effort into dataset quality, which is often overlooked. Next, we show fine-tuning examples on different datasets and present the results in Fig. 11. During fine-tuning, we observe that BLIP3o-60k achieves the lowest training loss, but our Fine-T2I clearly delivers the best fine-tuning results. This can be explained by the data characteristics. Images in BLIP3o-60k are generally structurally simple and clear, with limited background and details. This makes it easier for the model to find shortcuts during optimization, but does not indicate better generation performance. Human Evaluation We conduct human study by asking anonymous volunteers to perform the evaluation. Specifically, we suggest that fewer samples should be labeled as Tie in the A/B comparison, as shown in Fig. 6. For comparing fine-tuning results across different datasets, as shown in Fig. 8, we explicitly require that one best sample be selected. We report the results averaged over all participating annotators. Figure 11. Visual comparison of LlamaGen fine-tuned on different datasets. We randomly pick examples from generated images for visualization. Expectation for better image evaluation Building an extremely high-quality fine-tuning dataset required many, sometimes even cumbersome filtering steps. We rely on heavy filtering because there is still no strong, robust, widely accepted indicator for assessing the quality of an image or an imagetext pair. We expect that strong and reliable evaluation metric would substantially simplify this process, and we believe such metric may benefit from the reasoning capabilities of VLMs. C. Detailed studies We first provide our system prompt for the Qwen-VL thinking model in Fig. 12, which ensures high-quality textimage alignment and flawless image generation (explicitly specified in the system prompt). We also provide samples from different fine-tuning datasets, as shown in Fig. 13. Instead of cherry-picking specific images, we randomly select tar file from each dataset and visualize the first several samples. Surprisingly, we observe that some samples from other datasets should not even be included in the fine-tuning set. They are low-resolution, lowquality, and exhibit poor textimage alignment, etc., which is far below our expectations for fine-tuning textimage 14 Fine-T2I: An Open, Large-Scale, and Diverse Dataset for High-Quality T2I Fine-Tuning System Prompt You are Strict Visual Quality Auditor. Your sole task is to perform binary pass/fail audit on text-image pairs based on absolute prompt alignment and technical flawlessness. Evaluation Criteria: I. Semantic & Text-Image Alignment - Object Completeness: Every entity, person, or item required in the prompt must be clearly visible and identifiable. Missing any requested element results in an immediate False. - Quantity Precision: The number of objects in the image must exactly match the count specified in the text. - Text Rendering (OCR): Any text, words, or letters requested in the prompt must be rendered with 100 spelling accuracy, correct font style (if specified), and zero character distortion. - Attribute & Color Consistency: All specified colors, materials, sizes, and specific properties of objects must be strictly followed. - Spatial & Relational Logic: Objects must be in the exact positions described . Actions/interactions between objects must be logical. - Negative Constraints: If the prompt specifies what not to include, the image must not contain those elements. II. Technical & Aesthetic Quality - Anatomical Integrity: Zero tolerance for AI hallucinations in biological structures. This includes extra/missing fingers, limbs, distorted faces, unnatural joints, or merged body parts, etc. - Physical & Geometric Logic: Objects must follow the laws of physics (unless the prompt says otherwise). No floating objects, impossible perspectives, unrealistic interactions, or melting textures where surfaces bleed into each other. - Image Artifacts: Zero tolerance for unintended blurring, watermarks, signature-like scribbles, garbled characters, distorted characters or text, etc. - Style & Medium Fidelity: The image must perfectly embody the requested style (e.g., CGI, Oil Painting, Macro Photography). If the style is inconsistent across the frame, it is failure. Decision Logic: - Output True ONLY if the image is perfect realization of the prompt with zero technical defects. - Output False if there is even one minor discrepancy. - Extra details are permitted only if they enhance the scene without violating the prompt or the laws of physics. Constraint: - Please be concise but thorough in your thinking. - You must think step-by-step to verify every detail. - Output ONLY the word True or False as your final answer. Figure 12. Detailed system prompt used for final text-image pair filtering, for both text-image alignment and aesthetic quality. 15 Fine-T2I: An Open, Large-Scale, and Diverse Dataset for High-Quality T2I Fine-Tuning Figure 13. Comparison of text-image pairs from different fine-tuning datasets. We randomly select one tar file from the dataset, and list the first examples as comparison, without any cherry-picking. The resolution is marked bottom-right. Clearly, our Fine-T2I presents the best alignment, visual quality, and high-resolution, etc."
        }
    ],
    "affiliations": [
        "Department of Electrical & Computer Engineering, Northeastern University, Boston"
    ]
}