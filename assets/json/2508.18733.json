{
    "paper_title": "Drawing2CAD: Sequence-to-Sequence Learning for CAD Generation from Vector Drawings",
    "authors": [
        "Feiwei Qin",
        "Shichao Lu",
        "Junhao Hou",
        "Changmiao Wang",
        "Meie Fang",
        "Ligang Liu"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Computer-Aided Design (CAD) generative modeling is driving significant innovations across industrial applications. Recent works have shown remarkable progress in creating solid models from various inputs such as point clouds, meshes, and text descriptions. However, these methods fundamentally diverge from traditional industrial workflows that begin with 2D engineering drawings. The automatic generation of parametric CAD models from these 2D vector drawings remains underexplored despite being a critical step in engineering design. To address this gap, our key insight is to reframe CAD generation as a sequence-to-sequence learning problem where vector drawing primitives directly inform the generation of parametric CAD operations, preserving geometric precision and design intent throughout the transformation process. We propose Drawing2CAD, a framework with three key technical components: a network-friendly vector primitive representation that preserves precise geometric information, a dual-decoder transformer architecture that decouples command type and parameter generation while maintaining precise correspondence, and a soft target distribution loss function accommodating inherent flexibility in CAD parameters. To train and evaluate Drawing2CAD, we create CAD-VGDrawing, a dataset of paired engineering drawings and parametric CAD models, and conduct thorough experiments to demonstrate the effectiveness of our method. Code and dataset are available at https://github.com/lllssc/Drawing2CAD."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 4 ] . [ 3 3 3 7 8 1 . 8 0 5 2 : r Drawing2CAD: Sequence-to-Sequence Learning for CAD Generation from Vector Drawings Feiwei Qin Hangzhou Dianzi University Hangzhou, China qinfeiwei@hdu.edu.cn Shichao Lu Hangzhou Dianzi University Hangzhou, China lushichao@hdu.edu.cn Junhao Hou Zhejiang University Hangzhou, China junhaohou@zju.edu.cn Changmiao Wang Shenzhen Research Institute of Big Data Shenzhen, China cmwangalbert@gmail.com Meie Fang Guangzhou University Guangzhou, China fme@gzhu.edu.cn Ligang Liu University of Science and Technology of China Hefei, China lgliu@ustc.edu.cn Figure 1: An intuitive comparison of SVG drawing and CAD construction processes. Both SVG drawing and CAD construction rely on specific set of commands, and their respective processes can be formally represented as parametric command sequences with unified structural format. Abstract Computer-Aided Design (CAD) generative modeling is driving significant innovations across industrial applications. Recent works have shown remarkable progress in creating solid models from various inputs such as point clouds, meshes, and text descriptions. However, these methods fundamentally diverge from traditional industrial workflows that begin with 2D engineering drawings. The automatic generation of parametric CAD models from these 2D vector drawings remains underexplored despite being critical step in engineering design. To address this gap, our key insight is to reframe CAD generation as sequence-to-sequence learning problem where vector drawing primitives directly inform the generation of parametric CAD operations, preserving geometric precision and design intent throughout the transformation process. Equal contributions. Corresponding authors. Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. Copyrights for components of this work owned by others than the author(s) must be honored. Abstracting with credit is permitted. To copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior specific permission and/or fee. Request permissions from permissions@acm.org. MM 25, Dublin, Ireland 2025 Copyright held by the owner/author(s). Publication rights licensed to ACM. ACM ISBN 979-8-4007-2035-2/2025/10 https://doi.org/10.1145/3746027.3755782 We propose Drawing2CAD, framework with three key technical components: network-friendly vector primitive representation that preserves precise geometric information, dual-decoder transformer architecture that decouples command type and parameter generation while maintaining precise correspondence, and soft target distribution loss function accommodating inherent flexibility in CAD parameters. To train and evaluate Drawing2CAD, we create CAD-VGDrawing, dataset of paired engineering drawings and parametric CAD models, and conduct thorough experiments to demonstrate the effectiveness of our method. Code and dataset are available at https://github.com/lllssc/Drawing2CAD. CCS Concepts Computing methodologies Computer vision tasks. Keywords CAD generative modeling; Engineering drawings; Vector graphics; Multi-modal learning ACM Reference Format: Feiwei Qin, Shichao Lu, Junhao Hou, Changmiao Wang, Meie Fang, and Ligang Liu. 2025. Drawing2CAD: Sequence-to-Sequence Learning for CAD Generation from Vector Drawings. In Proceedings of the 33rd ACM International Conference on Multimedia (MM 25), October 2731, 2025, Dublin, Ireland. ACM, New York, NY, USA, 11 pages. https://doi.org/10.1145/3746027.3755782 MM 25, October 2731, 2025, Dublin, Ireland Feiwei Qin et al."
        },
        {
            "title": "1 Introduction\nIn modern design and engineering, Computer-Aided Design (CAD)\nmodels are essential for product definition and iteration, playing a\nkey role in prototyping, simulations, and manufacturing processes\n[50, 53, 55]. Parametric CAD modeling, which defines models as\nsequences of operations with geometric constraints, has become\nthe industry standard due to its flexibility in enabling rapid design\niterations through parameter adjustments [21, 43, 44, 54]. The indus-\ntrial design workflow typically initiates with engineering drawings,\nwhich function as the primary medium for conveying design in-\ntent and establishing the foundation for subsequent development\nstages [3, 9, 27, 29, 46]. Designers then create 3D parametric CAD\nmodels based on these 2D drawings to continue the product devel-\nopment process. This modeling process involves complex manual\noperations in CAD tools, requiring considerable time and expertise\n[14, 41, 49], thus limiting design efficiency. This paper focuses on\nthe specific task of generating parametric CAD models from vector\nengineering sketches represented in Scalable Vector Graphics (SVG)\nformat.",
            "content": "Current research has explored generating parametric CAD models from point clouds [23, 38], meshes [36], voxels [17], images [6, 52], and text descriptions [21]. However, these approaches deviate from industrial design workflows that begin with 2D engineering drawings. While text-to-CAD generation has gained recent research attention, text descriptions struggle with precise dimensional specifications and spatial relationships. In contrast, sketches inherently excel at conveying geometric constraints, offering an intuitive medium for expressing design intent that directly addresses the ambiguity of text-based approaches. Despite these advantages of sketches, existing sketch-based CAD generation approaches predominantly utilize raster image inputs [6, 13, 19], facing fundamental limitations in extracting precise geometric information from pixels. These methods struggle with scale invariance, line thickness variations, and differentiating between design elements and rasterization artifacts, thus compromising geometric accuracy and design intent preservation. Vector formats such as SVG present an unexplored opportunity, as they inherently encode precise geometric primitives that align with engineering design intent. Nevertheless, generating parametric CAD models from SVG sketches introduces three key challenges: (1) preprocessing and parsing SVG files to extract meaningful geometric information [12, 47]; (2) bridging the dimensional gap through cross-modal synthesis that transforms 2D vector sketches into 3D parametric CAD models; and (3) the absence of standardized SVG-to-CAD datasets. To address these challenges, we introduce Drawing2CAD, novel framework that enables cross-modal generation from 2D vector drawings to parametric CAD models. The basic idea is illustrated in Figure 1. We redefine CAD model generation as sequence-tosequence learning problem, where our framework encodes vector drawing primitives from SVG sketches and synthesizes corresponding parametric CAD operations while preserving geometric precision and design intent. Specifically, Drawing2CAD operates in three key stages: (1) we first develop network-friendly representation for vector engineering drawings that embeds SVG primitives while preserving precise geometric information and spatial relationships; (2) these embedded representations are then fed into our proposed dual-decoder architecture, where the first decoder generates CAD command types while the second decoder produces corresponding parameters, with command-guided generation, ensuring contextually appropriate parameters through this task-specialized decomposition; and (3) our end-to-end framework is optimized using novel soft target distribution loss function that acknowledges inherent flexibility in CAD parameters, allowing subtle variations while preserving design intent. Our approach offers several key advantages over existing methods. First, by using vector drawings as input, our method aligns naturally with standard CAD design workflows, where engineers typically begin with 2D sketches before proceeding to 3D modeling. Second, our direct processing of SVG primitives preserves the precise geometric information and relationships critical for accurate CAD modeling, enabling high-fidelity representation of design intent compared to raster image-based approaches. Third, the framework accepts flexible input configurations (a single isometric view, three orthographic views, or all four views combined), making it widely applicable across different application scenarios and adaptable to diverse designer preferences. These advantages collectively enable our approach to generate high-quality parametric CAD models that maintain engineering fidelity while supporting efficient design workflows. To support this research, we create CAD-VGDrawing, largescale dataset containing over 150,000 CAD models and their corresponding engineering drawings in both vector and raster formats. Experimental results demonstrate that vector engineering drawings provide more suitable and information-rich input for CAD operation sequence generation compared to raster-based representations, underscoring the advantages of vector graphics in advancing 3D CAD modeling. Furthermore, our method surpasses baseline approaches, highlighting its effectiveness in generating CAD operation sequences that better align with the original design intent, thereby improving overall model performance. In summary, this work has the following contributions: We develop network-friendly representation for vector engineering drawings that preserves precise geometric information and spatial relationships, enabling deep learning models to effectively process structured SVG primitives. We introduce Drawing2CAD, novel framework for crossmodal generation from 2D vector engineering drawings to parametric CAD models, redefining sketch-based CAD generation as sequence-to-sequence learning problem that directly processes SVG primitives to preserve precise geometric information and spatial relationships. We propose novel dual-decoder architecture that decomposes the complex CAD generation task into command type prediction and parameter estimation, incorporating soft target distribution loss function to enable more precise geometric control and enhance the effectiveness of the generation model. We create CAD-VGDrawing, comprehensive dataset containing paired vector engineering drawings and parametric CAD models, and demonstrate significant performance improvements of our approach compared to baseline methods Drawing2CAD: Sequence-to-Sequence Learning for CAD Generation from Vector Drawings MM 25, October 2731, 2025, Dublin, Ireland in command accuracy, parameter precision, and CAD model validity."
        },
        {
            "title": "2.3 Vector Graphics in Deep Learning\nIn the field of computer graphics, two predominant image formats\nprevail: raster images, characterized by pixel matrices, and vec-\ntor images, such as Scalable Vector Graphics (SVG), characterized\nby a series of code language commands [37]. A pioneering work\nin vector graphics generation, DeepSVG [5], introduced a hierar-\nchical Transformer-based generative model specifically for vector\ngraphics generation. It also curated a large-scale SVG dataset and\nintegrated deep learning techniques for SVG manipulation and edit-\ning, laying the groundwork for our study. Subsequent research has\nmade great achievements in SVG representation learning [4, 24, 35]\nand generative models [34, 42]. However, existing studies have pri-\nmarily focused on traditional vector graphic applications, such as",
            "content": "fonts, icons, and digital illustrations, without exploring the intersection of vector graphics and CAD engineering drawings. To explore the potential synergies arising from the intersection of these two fields, we leverage vector graphics techniques in deep learning and, for the first time, propose method to generate CAD operation sequences from vector engineering drawings."
        },
        {
            "title": "4.2 CAD-VGDrawing Dataset\nWhile existing datasets provide CAD models with construction\nprocess records [43, 54] and vector representations of operation\nsequences [44], they lack corresponding engineering drawings. To\naddress this gap, we introduce CAD-VGDrawing, an SVG-to-CAD\ndataset that pairs engineering drawings with their corresponding\nCAD models and operation sequences.",
            "content": "MM 25, October 2731, 2025, Dublin, Ireland Feiwei Qin et al. Figure 2: The pipeline of our proposed method. Drawing2CAD takes vector engineering drawings in one of three view configurations as input, encodes them into latent vector, and employs dual-decoder to generate CAD command types and their parameters. The resulting complete operation sequences are processed by CAD kernel to build the final 3D model. Collection of Vector Engineering Drawings. The CAD models used in our study were sourced from the DeepCAD dataset [44]. To streamline the engineering drawing generation process, we developed custom Python script in FreeCAD [10]. This script utilizes FreeCADs TechDraw module to automatically convert imported STEP files into engineering drawings and export them as SVG files. For each model, we generated four standard views: three orthographic projections (front, top, and right) and one isometric (front-top-right) view. We then used CairoSVG [2] to convert these vector drawings to PNG format. This rasterization step enables integration with image-based learning pipelines and supporting comparative analysis between vector and raster representations as input sources. Vector Engineering Drawings Preprocessing. We performed systematic preprocessing on all engineering drawings. First, we attempted to simplify BÃ©zier curve segments following Carlier et al. [5], but found FreeCADs native curve segmentation already optimal through the comparative analysis, requiring no further refinement. Next, we addressed path ordering issues by implementing path reordering, as FreeCAD typically generates SVG paths in an irregular and inconsistent sequence. To standardize path ordering, we used the canvas top-left corner as origin and applied graph theory algorithms [28] to identify all contours. These contours were then arranged by increasing distance from origin, with each contour drawn clockwise. Finally, we normalized all SVG drawings to standardized 200 200 viewbox. For the bitmap-format engineering drawings, we uniformly resized all PNG images to 224 224, ensuring consistency for image-based learning tasks and facilitating standardized input processing. These preprocessing steps enabled accurate geometric information extraction for our representation method, detailed in Section 4.3. Statistics of CAD-VGDrawing Dataset. Our automated preprocessing workflow handled approximately 176,000 CAD models from the DeepCAD dataset. However, FreeCAD encountered limitations when processing subset of complex models, producing either invalid engineering drawings or representations that significantly deviated from the original CAD models. To ensure dataset quality and reliability, we systematically filtered these problematic instances, resulting in curated dataset of 161,407 CAD models with corresponding engineering drawings across four distinct views. detailed statistical analysis was performed on our dataset, covering three aspects: (1) the distribution of engineering drawings categorized by SVG command types (e.g., LineTo, CubicBÃ©zier, or their combination), (2) the sequence length distribution of SVG drawing commands, and (3) the CAD sequence length distributions in the retained versus filtered subsets of the dataset. These results are presented in the supplementary material. Based on this analysis, we selected SVG engineering drawings with sequence lengths not exceeding 100 to construct our final dataset. This strategic decision ensured data completeness and diversity while reducing the negative impact of variable-length sequences on model performance, thereby decreasing model complexity and improving stability. The final dataset comprised 157,591 SVG-to-CAD pairs, randomly divided into training (90%), validation (5%), and test (5%) sets."
        },
        {
            "title": "Representation",
            "content": "Unlike previous approaches [5], we adopt simplified representation for vector engineering drawings by focusing exclusively on core geometric information. We exclude non-essential path attributes (visibility, color, fill properties) and restrict our command set to LineTo (L) and CubicBÃ©zier (C). This approach standardizes each command to an 8-value parameter list: ğ‘‹ = (ğ‘¥1, ğ‘¦1, ğ‘ğ‘¥1, ğ‘ğ‘¦1, ğ‘ğ‘¥2, ğ‘ğ‘¦2, ğ‘¥2, ğ‘¦2) R8, (1) where ğ‘¥1, ğ‘¦1 and ğ‘¥2, ğ‘¦2 represent start and end points, while ğ‘ğ‘¥1, ğ‘ğ‘¦1 and ğ‘ğ‘¥2, ğ‘ğ‘¦2 are control points for BÃ©zier curves. For LineTo commands, only the start and end points are utilized, with control point Drawing2CAD: Sequence-to-Sequence Learning for CAD Generation from Vector Drawings MM 25, October 2731, 2025, Dublin, Ireland parameters set to -1, while CubicBÃ©zier commands employ all eight parameters. Since each CAD model corresponds to four engineering drawings (Front, Top, Right, and Isometric views), we assign specific view labels to maintain proper contextual identification. Overall, we define each vector engineering drawing as an ordered sequence ğ·ğ‘– = {ğ‘†1, ğ‘†2, ..., ğ‘†ğ‘ }, where ğ·ğ‘– represents the ğ‘–-th engineering drawing containing ğ‘ = 100 command sequences. Each sub-sequence ğ‘†ğ‘– = (ğ‘£ğ‘–, ğ¶ğ‘– ) includes view label ğ‘£ğ‘– {ğ¹ğ‘Ÿğ‘œğ‘›ğ‘¡,ğ‘‡ğ‘œğ‘, ğ‘…ğ‘–ğ‘”â„ğ‘¡, ğ¼ğ‘ ğ‘œğ‘šğ‘’ğ‘¡ğ‘Ÿğ‘–ğ‘} and command sub-sequence ğ¶ğ‘– = (ğ‘ ğ‘— is one of the elements in the command set {ğ‘†ğ‘‚ğ‘†, ğ¿, ğ¶, ğ¸ğ‘‚ğ‘†}, which denotes the command type (ğ¿ for LineTo, ğ¶ for CubicBÃ©zier, with start and end sequence markers), and ğ‘‹ ğ‘— ğ‘– contains the 8-dimensional command parameters: ğ‘¥1,ğ‘–, ğ‘ ğ‘— ğ‘– = (ğ‘ ğ‘— ğ‘‹ ğ‘— ğ‘ğ‘¦1,ğ‘–, ğ‘ ğ‘— This representation enables us to encode the complete geometric information of engineering drawings while maintaining consistent, standardized format across different command types. ğ‘– ). Here, ğ‘ ğ‘— ğ‘ğ‘¦2,ğ‘–, ğ‘ ğ‘— ğ‘ğ‘¥ 1,ğ‘–, ğ‘ ğ‘— ğ‘ğ‘¥ 2,ğ‘–, ğ‘ ğ‘— ğ‘¦1,ğ‘–, ğ‘ ğ‘— ğ‘¥2,ğ‘–, ğ‘ ğ‘— ğ‘– , ğ‘‹ ğ‘— ğ‘¦2,ğ‘– ). (2) ğ‘–"
        },
        {
            "title": "4.4 Architecture\nEmbedding. To effectively structure input for the Transformer\nnetwork, we project the view labels, SVG commands, and parame-\nters into a continuous embedding space of dimension ğ‘‘ğ¸ . Unlike\nprior methods [5, 21, 44, 54] that fuse embeddings via direct linear\naddition, treating command and parameter information indepen-\ndently, our approach explicitly models the interactions among the\nview, command, and parameter components through concatenation-\nbased embedding learning. This design choice is supported by our\nAblation Study 5.2. Specifically, we concatenate the respective em-\nbeddings and apply a linear transformation (MLP) to produce a\nunified fused representation, which enables richer cross-field inter-\nactions:\nğ¸ğ· (ğ‘–) = ğ‘´ğ‘³ğ‘· (ğ‘ªğ‘¶ ğ‘µ ğ‘ªğ‘¨ğ‘» (ğ‘’ğ‘£ğ‘–ğ‘’ğ‘¤\nThe view embedding ğ‘’ğ‘£ğ‘–ğ‘’ğ‘¤\n=\nğ‘–\nğ‘– , where ğ‘Šğ‘£ğ‘–ğ‘’ğ‘¤ âˆˆ Rğ‘‘ğ‘’ Ã—4 is a learnable matrix, and ğ›¿ ğ‘£\nğ‘Šğ‘£ğ‘–ğ‘’ğ‘¤ğ›¿ ğ‘£\nğ‘– âˆˆ R4\nis a one-hot vector indicating one of four standard views. The\ncommand embedding ğ‘’ğ‘ğ‘šğ‘‘\nrepresents the command type ğ‘ğ‘– as:\nğ‘– , where ğ‘Šğ‘ğ‘šğ‘‘ âˆˆ Rğ‘‘ğ‘’ Ã—4 is a learnable matrix, and\nğ‘’ğ‘ğ‘šğ‘‘\nğ‘–\nğ›¿ğ‘\nğ‘– âˆˆ R4 is a one-hot encoding of the predefined command set. The\nparameter embedding ğ‘’ğ‘ğ‘ğ‘Ÿğ‘ğ‘š\nencodes the command parameters. As\ndescribed in Section 4.3, each command contains eight parameters\nquantized into 8-bit integers. Each integer is converted into a one-\nhot vector ğ›¿ğ‘\nğ‘–,ğ‘— (ğ‘— = 1, . . . , 8) of dimension 28 + 1 = 257, with the\nextra dimension accommodating unused parameters. These vectors\nform a matrix ğ›¿ğ‘\nğ‘– âˆˆ R257Ã—8. To embed each parameter, we apply a\nğ‘ğ‘ğ‘Ÿğ‘ğ‘š âˆˆ Rğ‘‘ğ‘’ Ã—257 column-wise, and then\nshared learnable matrix ğ‘Š ğ‘\nflatten the resulting embeddings before passing them through a\nlinear projection ğ‘Š ğ‘",
            "content": "ğ‘– encodes the view type ğ‘£ğ‘– as: ğ‘’ğ‘£ğ‘–ğ‘’ğ‘¤ ğ‘ğ‘ğ‘Ÿğ‘ğ‘š Rğ‘‘ğ‘’ 8ğ‘‘ğ‘’ : Rğ‘‘ğ¸ . (3) = ğ‘Šğ‘ğ‘šğ‘‘ğ›¿ğ‘ ))+ğ‘’ğ‘ğ‘œğ‘  ğ‘– , ğ‘’ğ‘ğ‘ğ‘Ÿğ‘ğ‘š , ğ‘’ğ‘ğ‘šğ‘‘ ğ‘– ğ‘– ğ‘– ğ‘– ğ‘– ğ‘’ğ‘ğ‘ğ‘Ÿğ‘ğ‘š ğ‘– = ğ‘Š ğ‘ ğ‘ğ‘ğ‘Ÿğ‘ğ‘š flat(ğ‘Š ğ‘ ğ‘ğ‘ğ‘Ÿğ‘ğ‘šğ›¿ğ‘ ğ‘– ), (4) where flat() flattens the input matrix into vector. The ğ‘´ğ‘³ğ‘· () learns cross-field interactions through linear projection. When only the isometric view serves as input, the view embedding becomes redundant due to the lack of variation, and the overall embedding will be simplified: ğ‘– ğ‘– ğ‘– (5) , ğ‘’ğ‘ğ‘ğ‘Ÿğ‘ğ‘š )) + ğ‘’ğ‘ğ‘œğ‘  ğ‘– Rğ‘‘ğ¸ . ğ¸ğ· (ğ‘–) = ğ‘´ğ‘³ğ‘· (ğ‘ªğ‘¶ ğ‘µ ğ‘ªğ‘¨ğ‘» (ğ‘’ğ‘ğ‘šğ‘‘ The function of positional encoding ğ‘’ğ‘ğ‘œğ‘  is the same as in the original Transformer [39], which is used to record the index of the command in the complete vector engineering drawing SVG sequence. In our implementation, the dimension of ğ‘‘ğ¸ is set to 256. For the multi-view setting, we adopt straightforward strategy by stacking the three orthographic (front, top, and right) views followed by the isometric (front-top-right) view in fixed order. Encoder. Our encoder ğ¸ consists of four Transformer blocks, each containing eight attention heads and feed-forward dimension of 512. The encoder ğ¸ takes the embedded sequence [ğ‘’1, ..., ğ‘’ğ‘ğ‘ ] as input and outputs sequence of vectors [ Ë†ğ‘’1, ..., Ë†ğ‘’ğ‘ğ‘ ], where each vector has the same dimension ğ‘‘ğ¸ = 256. Finally, the output vectors are averaged to produce single ğ‘‘ğ¸ -dimensional latent vector ğ‘§. Dual Decoder. Our decoder adopts dual-decoder architecture consisting of two independent Transformer decoders with identical hyper-parameter settings as the encoder. Both decoders take learned constant embedding as input while attending to the latent vector ğ‘§ to capture global features. In CAD operations, each command type often requires specific set of parameters, and even the same command may require different parameters depending on the context. To address this complexity, we enforce one-to-one correspondence between command types and their parameters, and decompose the generation task accordingly: the Command Decoder predicts the CAD operation type Ë†ğ‘¡ğ‘– while the Argument Decoder generates the associated parameter vector Ë†ğ‘ğ‘– = [ğ‘¥, ğ‘¦, ğ›¼, ğ‘“ , ğ‘Ÿ, ğœƒ, ğ›¾, ğ‘ğ‘¥ , ğ‘ğ‘¦, ğ‘ğ‘ , ğ‘ , ğ‘’1, ğ‘’2, ğ‘, ğœ‡]. key innovation in our architecture is the command-guided parameter generation. To ensure that the Argument Decoder generates parameters consistent with the command semantics, we add the output of the Command Decoder to the output of the Argument Decoder. This fusion injects command-level information into the parameter generation process, effectively enhancing the decoders capacity to produce contextually appropriate and semantically aligned parameters. The outputs from each decoders Transformer block are then projected through separate linear layers to obtain the predicted command and parameters respectively. Finally, the operation command types and parameters are combined to form the complete CAD operation sequences. The generated CAD operation sequences are subsequently processed by an OpenCASCADE-based CAD kernel to build the final CAD model."
        },
        {
            "title": "4.5 Loss Function\nOur approach employs a composite loss function consisting of Com-\nmand Loss Lğ‘ğ‘šğ‘‘ and Parameter Loss Lğ‘ğ‘Ÿğ‘”ğ‘  , similar to existing CAD\noperation sequence generation methods. However, we introduce\na significant enhancement to the Parameter Loss component. Tra-\nditional approaches typically rely on hard classification, requiring\nexact matches between predictions and ground truth values. In\ncontrast, our method recognizes that CAD operation parameters\nnaturally tolerate minor variations that can maintain design intent\nwhile introducing beneficial diversity to the resulting models. Based",
            "content": "MM 25, October 2731, 2025, Dublin, Ireland Feiwei Qin et al. on this insight, we formulated our Parameter Loss using soft target distributions rather than rigid one-hot encodings: ğ‘ğ‘ ğ‘ğ‘ ğ¶ Lğ‘ğ‘Ÿğ‘”ğ‘  = ğ‘¦ğ‘˜ log ( Ë†ğ‘¦ğ‘˜ ) , (6) ğ‘–=1 ğ‘˜=1 ğ‘—=1 where ğ‘ğ‘ denotes the command sequence length, ğ‘ğ‘ represents the number of command parameters, and ğ¶ is the number of discrete categories for each parameter. The term ğ‘¦ğ‘˜ is smoothed probability distribution, which assigns penalization weights for predictions deviating from the true parameter category: ğ‘¦ğ‘˜ = ğ‘’ ğ›¼ ğ‘˜ ğ‘¦ ğ‘ , (7) where ğ›¼ controls the strength of tolerance decay, ğ‘˜ ğ‘¦ is the distance between the predicted parameter category and the true parameter category, and ğ‘ is normalization factor ensuring that the probabilities sum to 1. Smoothing weights are applied exclusively to categories within the range [ğ‘¦ ğ‘¡ğ‘œğ‘™ğ‘’ğ‘Ÿğ‘ğ‘›ğ‘ğ‘’, ğ‘¦ +ğ‘¡ğ‘œğ‘™ğ‘’ğ‘Ÿğ‘ğ‘›ğ‘ğ‘’], with zero weights assigned to all other categories. In practice, we set ğ›¼ to 2.0 and the ğ‘¡ğ‘œğ‘™ğ‘’ğ‘Ÿğ‘ğ‘›ğ‘ğ‘’ to 3. This innovative loss formulation alleviates excessive penalties for predictions that slightly deviate from the ground truth but still fall within an acceptable range. By relaxing overly strict constraints, it enhances the models generalization ability. As demonstrated in our Ablation Study 5.2, this refinement leads to consistent improvements in parameter-related metrics."
        },
        {
            "title": "4.6 Metrics\nCommand Accuracy. To evaluate the prediction accuracy of\ncommands, we employ two metrics: Command Type Accuracy\n(ğ´ğ¶ğ¶ğ‘ğ‘šğ‘‘ ) and Parameter Accuracy (ğ´ğ¶ğ¶ğ‘ğ‘ğ‘Ÿğ‘ğ‘š). The Command\nType Accuracy (ğ´ğ¶ğ¶ğ‘ğ‘šğ‘‘ ) measures the correctness of predicted\nCAD command types:",
            "content": "ğ´ğ¶ğ¶ğ‘ğ‘šğ‘‘ = 1 ğ‘ğ‘ ğ‘ğ‘ ğ‘–=1 I[ğ‘¡ğ‘– = Ë†ğ‘¡ğ‘– ], (8) where ğ‘ğ‘ is the total length of the CAD command sequence, ğ‘¡ğ‘– and Ë†ğ‘¡ğ‘– represent the ground truth and predicted command type for the ğ‘–-th command, respectively. The indicator function returns 1 when the condition is satisfied and 0 otherwise. For commands with correctly predicted types, we further evaluate the correctness of command parameters, defined as: where ğ¾ represents the total parameters count in correctly predicted commands, ğ‘ğ‘–,ğ‘— and Ë†ğ‘ğ‘–,ğ‘— denote the ground truth and predicted values for the ğ‘—-th parameter of the ğ‘–-th command, respectively. ğœ‚ is tolerance threshold, defining the acceptable error margin between predicted and ground truth parameters. The indicator function I[ğ‘¡ğ‘– = Ë†ğ‘¡ğ‘– ] ensures that parameter accuracy is only evaluated for correctly predicted command types. We set ğœ‚ = 3 to account for parameter quantization, consistent with the tolerance threshold in our loss function. Shape Construction and Evaluation. When CAD model is constructed from generated CAD operation sequences, we can convert it into point clouds by randomly sampling points on its surface. In practice, we set ğ¾ = 2000. To measure the differences between real shape and the predicted shape, we calculate the Mean Chamfer Distance (MCD) of them. Additionally, we report the Invalidity Ratio (IR), which quantifies the percentage of generated CAD operation sequences that fail to produce valid 3D shapes."
        },
        {
            "title": "5 Experiments\n5.1 Experimental Setup\nTo the best of our knowledge, we propose the first framework that\ngenerates parametric CAD models directly from vector engineer-\ning drawings. Given the absence of existing models or benchmark\ndatasets for this task, there are currently no available methods for\ndirect comparison. To comprehensively evaluate our approach, we\ndesign a series of experiments covering different aspects of the task.\nWe begin by comparing the effect of input formats, analyzing the\nperformance differences between using vector (.svg) engineering\ndrawings and raster (.png) engineering drawings as input. Follow-\ning this, we assess the effectiveness of our method by comparing it\nagainst a modified baseline model under various evaluation metrics,\nand further perform a visual comparative analysis with traditional\nmethods to better understand the differences in output quality and\nstructural correctness. To further understand the internal mecha-\nnisms and the contribution of each component in our framework,\nwe conduct a set of ablation studies. Moreover, we analyze several\nimperfect or failure cases to uncover current limitations of our\nmethod and provide directions for future research.",
            "content": "The experiments were conducted on one NVIDIA RTX 4090 GPU with batch size of 256 under 200 epochs. The training process utilized the Adam optimizer with learning rate of 0.001, incorporating linear warm-up for the first 2000 steps. We applied dropout rate of 0.1 to all Transformer blocks and used gradient clipping with threshold of 1.0 during backpropagation. Table 1: The comparison of results with raster (DeepCADraster) and vector (DeepCAD-vector) engineering drawing inputs. ACCcmd, ACCparam and IR are multiplied by 100%. MCD is multiplied by 102. means higher metric value indicates better results. means lower metric value indicates better results. Input Method ACCcmd ACCparam IR MCD DeepCAD-raster DeepCAD-vector DeepCAD-raster DeepCAD-vector DeepCAD-raster DeepCAD-vector 75.60 80.78 76.81 81.39 77.69 81. 69.44 73.73 70.74 74.76 70.49 75.14 30.54 23.29 30.38 22.97 29.79 23. 17.67 11.52 18.73 12.15 18.16 11.37 orthographic (3x) isometric + orthographic (4x)"
        },
        {
            "title": "5.2 Experimental Results\nVector and Raster Inputs. To conduct our designed comparative\nexperiments, we built a modified baseline model based on Deep-\nCAD [44]. We replaced the original CAD command input with\nSVG command input while keeping all other settings unchanged\n(referred to as \"DeepCAD-vector\"). For raster (.png) engineering\ndrawings, we replaced the original encoder with a SOTA Vision",
            "content": "ğ´ğ¶ğ¶ğ‘ğ‘ğ‘Ÿğ‘ğ‘š = 1 ğ¾ ğ‘ğ‘ Ë†ğ‘ğ‘– ğ‘–=1 ğ‘—= I[ğ‘ğ‘–,ğ‘— Ë†ğ‘ğ‘–,ğ‘— < ğœ‚]I[ğ‘¡ğ‘– = Ë†ğ‘¡ğ‘– ], (9) isometric (1x) Drawing2CAD: Sequence-to-Sequence Learning for CAD Generation from Vector Drawings MM 25, October 2731, 2025, Dublin, Ireland successfully reconstructs these challenging models while preserving the geometric features and design intent specified in the input drawings. Table 2: The comparison of DeepCAD-vector and Drawing2CAD on the vector engineering drawings to parametric CAD sequences generation. Input Method ACCcmd ACCparam IR MCD isometric (1x) orthographic (3x) isometric + orthographic (4x) DeepCAD-vector Drawing2CAD DeepCAD-vector Drawing2CAD DeepCAD-vector Drawing2CAD 80.78 81. 81.39 82.12 81.51 82.43 73.73 74.35 74.76 75.43 75.14 76.09 23.29 21. 22.97 20.99 23.40 20.31 11.52 12.10 12.15 11.98 11.37 10.88 Figure 4: Comparison results of engineering drawings to parametric CAD models when using three orthographic views or single isometric view as input. Figure 5: Comparison results of engineering drawings to parametric CAD models when using all four views as input. Ablation Study. We conducted ablation studies across three input configurations (single isometric view, three orthographic views, and all four views combined) to evaluate component contributions Figure 3: Comparison results with raster (DeepCAD-raster) and vector (DeepCAD-vector) engineering drawing inputs. \"\" means the generated parametric sequences that fail to reconstruct 3D shape. Transformer (ViT) pre-trained feature extractor [30] (referred to as \"DeepCAD-raster\"). The detailed results of comparison between DeepCAD-vector and DeepCAD-raster are presented in Table 1, which show that DeepCAD-vector outperforms DeepCAD-raster consistently, regardless of whether we use single isometric view, three orthographic views, or all four views combined as input. Meanwhile, as illustrated in Figure 3, when vector inputs are used, the model is able to generate CAD models that better align with the design intent. In contrast, with raster inputs, the generated CAD models often fail to meet design expectations and are more likely to result in invalid shapes. These findings highlight the advantage of using vector inputs, which provide more precise and semantically rich structural information compared to raster pixel data. This allows the model to more effectively incorporate sketch design into its feature representation, thereby conveying more specific and meaningful design intent. Performance Comparison. In our quantitative evaluation, we compared our approach with the baseline method DeepCAD-vector. As shown in Table 2, our proposed method, Drawing2CAD, consistently outperforms DeepCAD-vector in all metrics, regardless of whether single isometric view, three orthographic views, or all four views combined are used as input. When using only the isometric view, our method exhibits slightly higher Mean Chamfer Distance (CD) compared to DeepCAD-vector. However, this difference should be considered alongside our significantly lower Invalidity Ratio (IR), as CD is only calculated for successfully generated models. In our qualitative analysis, as illustrated in Figures 4 and 5, Drawing2CAD generates CAD models that better align with the design intent conveyed in the engineering drawings, demonstrating clear improvements over the baseline methods. We also compared our approach with Photo2CAD [13], traditional rule-based method that generates CAD models from orthographic drawings by extracting geometric features from three orthographic views, establishing hierarchical structures, and applying Boolean operations to create 3D models. As shown in Figure 4, Photo2CAD struggles to effectively generate accurate CAD models for our test cases, particularly for complex geometries where rule-based approaches fail to capture the intricate design details. In contrast, our learning-based method MM 25, October 2731, 2025, Dublin, Ireland Feiwei Qin et al. Table 3: Ablation study on incremental versions of the model in three optional inputs. \"dual dec.\" means dual-decoder architecture of our method. \"guidance\" means command-guidance operation used in parameters generation process. The concatenationbased learning embedding is added as the final component. (a) isometric (1x) (b) orthographic (3x) (c) isometric + orthographic (4x) Method ACCcmd ACCparam IR CD Method ACCcmd ACCparam IR CD Method ACCcmd ACCparam IR CD DeepCAD-vector (baseline) dual dec. + baseline loss dual dec. + our loss dual dec. + our loss + guidance Drawing2CAD 80.78 81.94 81.86 81.56 81.81 73.73 73.79 74.47 74.30 74.35 23.29 23.41 23.88 22.00 21.40 11.52 12.47 11.87 11.72 12.10 DeepCAD-vector (baseline) dual dec. + baseline loss dual dec. + our loss dual dec. + our loss + guidance Drawing2CAD 81.39 82.08 82.10 82.05 82. 74.76 74.44 75.17 75.25 75.43 22.97 23.64 22.59 21.52 20.99 12.15 12.80 12.51 12.30 11.98 DeepCAD-vector (baseline) dual dec. + baseline loss dual dec. + our loss dual dec. + our loss + guidance Drawing2CAD 81.51 82.34 82.28 81.84 82.43 75.14 74.93 75.56 75.82 76. 23.40 22.45 22.43 21.37 20.31 11.37 12.08 11.23 11.40 10.88 in Drawing2CAD, as shown in Table 3. Starting with the DeepCADvector baseline, we incrementally added our proposed components: dual-decoder architecture, soft target distribution loss function, command-guided parameter generation, and concatenation-based embedding fusion. With single isometric view as input, our model achieves the lowest Invalid Ratio while maintaining competitive performance in command accuracy, parameter accuracy, and Chamfer Distance. Validity is particularly important metric in generative tasks, as it directly determines whether models can be used in practice. We argue that our approach strikes careful balance among prediction accuracy, geometric fidelity, and structural validity, demonstrating robustness even under limited-view conditions. For orthographic views (3x) and combined views (4x), Drawing2CAD significantly outperforms all reduced versions, indicating that each component in our framework is both essential and effective. Notably, in the initial four reduced versions, we employed linear addition strategy for embedding fusion, consistent with previous research. However, replacing this linear addition strategy with concatenation-based embedding fusion yielded significant performance improvement. These results confirm our hypothesis that concatenation-based embedding fusion better captures interdependencies among view, command, and parameter embeddings, creating more expressive and informative representations."
        },
        {
            "title": "6 Limitations and Discussions\nDespite its promising results in generating parametric CAD models\nfrom vector engineering drawings, our method still has room for\nimprovement. Figure 6 presents representative cases that highlight\nlimitations and offer insights for future research. More details can\nbe referred in the supplementary material.\nParameter Precision Issues. As shown in Figure 6 (a), our method\nmay capture the overall shape yet show noticeable size deviations\ndue to the tolerance design in the loss function. While this design\nstabilizes optimization, it can compromise parameter-level accuracy.\nThis highlights the need for uncertainty-aware modeling to better\nbalance precision and robustness.\nView-Specific Information Trade-offs. Different views intro-\nduce representational biases: orthographic views preserve sym-\nmetry or layout but lack depth, whereas isometric views enhance\ndepth perception at the cost of planar alignment (Figure 6 (b)). Fu-\nture work could integrate geometric priors or neural rendering\nto compensate for such limitations and improve reconstruction\nconsistency.\nMulti-View Integration Challenges. Multiple-view input en-\nhances spatial completeness, but inconsistent cues such as the pro-\ntrusion mismatch can lead to ambiguity (Figure 6 (c), (d)). This",
            "content": "Figure 6: Five representative types of imperfect cases in our experiments which present limitations and insights of our method: Parameter Precision Issues (a), View-Specific Information Trade-offs (b), Multi-View Integration Challenges (c)(d), and View Information Dependency (e). \"1x\", \"3x\", \"4x\" means using single isometric view, three orthographic views, or combination of all four views as input respectively. suggests the need for robust multi-view fusion strategies to resolve visual conflicts and maintain structural coherence. View Information Dependency. Our approach fails to recover features invisible in all views, as demonstrated by the missing side hole in Figure 6 (e). This limitation points to the potential of semisupervised learning or implicit priors to infer occluded geometry and enrich the models ability to reconstruct hidden features."
        },
        {
            "title": "7 Conclusion\nIn this work, we propose Drawing2CAD, a novel approach for gen-\nerating CAD operation sequences directly from vector engineering\ndrawings, bridging the gap between 2D drawings and 3D CAD\nmodeling. By redefining CAD model generation as a sequence-\nto-sequence learning task, our method leverages rich geometric\ninformation embedded in vector drawing sequences to produce\nCAD operation sequences that create functional CAD models. Ex-\ntensive experiments demonstrate that vector engineering drawings\noutperform raster inputs in command accuracy, parameter preci-\nsion, and 3D reconstruction quality. Moreover, our method, with its\ntailored architecture and novel loss function, ensures effectiveness\nin CAD operation sequence generation. Additionally, we introduce\nCAD-VGDrawing, a large-scale dataset containing over 150,000 en-\ngineering drawings in both vector and raster formats, providing a\nvaluable resource for future research in automated CAD design. Our\nfindings highlight the potential of integrating vector graphics and\nCAD operations in generative modeling, paving the way for more\nadvanced, efficient, and intelligent CAD modeling frameworks.",
            "content": "Drawing2CAD: Sequence-to-Sequence Learning for CAD Generation from Vector Drawings MM 25, October 2731, 2025, Dublin, Ireland Acknowledgments National Natural Science Foundation of China (Nos. 62025207, 62072126), the Fundamental Research Funds for the Provincial Universities of Zhejiang (No. GK259909299001-006), and the Anhui Provincial Joint Construction Key Laboratory of Intelligent Education Equipment and Technology (No. IEET202401). References [1] Md Ferdous Alam and Faez Ahmed. 2024. Gencad: Image-conditioned computeraided design generation with transformer-based contrastive representation and diffusion priors. arXiv preprint arXiv:2409.16294 (2024). [2] CairoSVG Contributors. 20122025. CairoSVG: Simple SVG Converter for Python. https://cairosvg.org/ [3] Jorge D. Camba, Pedro Company, and Ferran Naya. 2022. Sketch-Based Modeling in Mechanical Engineering Design: Current Status and Opportunities. ComputerAided Design 150 (2022), 103283. [4] Neill D. F. Campbell and Jan Kautz. 2014. Learning manifold of fonts. ACM Transactions on Graphics (TOG) 33, 4 (July 2014), 11 pages. [5] Alexandre Carlier, Martin Danelljan, Alexandre Alahi, and Radu Timofte. 2020. DeepSVG: hierarchical generative network for vector graphics animation. In Proceedings of the International Conference on Neural Information Processing Systems (NIPS) (Vancouver, BC, Canada) (NIPS 20). Curran Associates Inc., Red Hook, NY, USA, 11 pages. [6] Tianrun Chen, Chunan Yu, Yuanqi Hu, Jing Li, Tao Xu, Runlong Cao, Lanyun Zhu, Ying Zang, Yong Zhang, Zejian Li, et al. 2024. Img2cad: Conditioned 3d cad model generation from single image with structured visual geometry. arXiv preprint arXiv:2410.03417 (2024). [7] Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, et al. 2020. An image is worth 16x16 words: Transformers for image recognition at scale. arXiv preprint arXiv:2010.11929 (2020). [8] Elona Dupont, Kseniya Cherenkova, Dimitrios Mallis, Gleb Gusev, Anis Kacem, and Djamila Aouada. 2024. TransCAD: Hierarchical Transformer for CAD Sequence Inference from Point Clouds. In Proceedings of the European Conference on Computer Vision (ECCV) (Milan, Italy). Springer-Verlag, Berlin, Heidelberg, 1936. [9] Rubin Fan, Fazhi He, Yuxin Liu, and Jing Lin. 2025. history-based parametric CAD sketch dataset with advanced engineering commands. Computer-Aided Design 182 (2025), 103848. [10] FreeCAD Community. 20022025. FreeCAD: Open Source Parametric 3D CAD Modeler. https://www.freecad.org/ [11] Jie-Hui Gong, Gui-Fang Zhang, Hui Zhang, and Jia-Guang Sun. 2006. Reconstruction of 3D curvilinear wire-frame from three orthographic views. Computers & Graphics 30, 2 (2006), 213224. [12] Wenyu Han, Siyuan Xiang, Chenhui Liu, Ruoyu Wang, and Chen Feng. 2020. Spare3d: dataset for spatial reasoning on three-view line drawings. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). 1469014699. [13] Ajay Harish and Abhishek Rajendra Prasad. 2021. Photo2CAD: Automated arXiv preprint 3D solid reconstruction from 2D drawings using OpenCV. arXiv:2101.04248 (2021). [14] Wentao Hu, Jia Zheng, Zixin Zhang, Xiaojun Yuan, Jian Yin, and Zihan Zhou. 2023. PlankAssembly: Robust 3D Reconstruction from Three Orthographic Views with Learnt Shape Programs. In Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV). 1844918459. [15] Mohammad Sadil Khan, Elona Dupont, Sk Aziz Ali, Kseniya Cherenkova, Anis Kacem, and Djamila Aouada. 2024. Cad-signet: Cad language inference from point clouds using layer-wise sketch instance guided attention. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). 47134722. [16] Mu-Hsing Kuo. 1998. Reconstruction of quadric surface solids from three-view engineering drawings. Computer-Aided Design 30, 7 (1998), 517527. [17] Joseph George Lambourne, Karl Willis, Pradeep Kumar Jayaraman, Longfei Zhang, Aditya Sanghi, and Kamal Rahimi Malekshan. 2022. Reconstructing editable prismatic CAD from rounded voxel models. In SIGGRAPH Asia 2022 Conference Papers (Daegu, Republic of Korea) (SA 22). Association for Computing Machinery, New York, NY, USA, Article 53, 9 pages. [18] Joseph Lambourne, Karl DD Willis, Pradeep Kumar Jayaraman, Aditya Sanghi, Peter Meltzer, and Hooman Shayani. 2021. Brepnet: topological message passing system for solid models. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). 1277312782. [19] Changjian Li, Hao Pan, Adrien Bousseau, and Niloy J. Mitra. 2022. Free2CAD: parsing freehand drawings into CAD commands. ACM Transactions on Graphics (TOG) 41, 4, Article 93 (2022), 16 pages. [20] Xingang Li and Zhenghui Sha. 2025. Image2CADSeq: Computer-Aided Design Sequence and Knowledge Inference from Product Images. arXiv preprint arXiv:2501.04928 (2025). [21] Xueyang Li, Yu Song, Yunzhong Lou, and Xiangdong Zhou. 2024. CAD Translator: An Effective Drive for Text to 3D Parametric Computer-Aided Design Generative Modeling. In Proceedings of the 32nd ACM International Conference on Multimedia (Melbourne VIC, Australia) (MM 24). Association for Computing Machinery, New York, NY, USA, 84618470. [22] Shi-Xia Liu, Shi-Min Hu, Yu-Jian Chen, and Jia-Guang Sun. 2001. Reconstruction of curved solids from engineering drawings. Computer-Aided Design 33, 14 (2001), 10591072. [23] Yujia Liu, Anton Obukhov, Jan Dirk Wegner, and Konrad Schindler. 2024. Point2CAD: Reverse engineering CAD models from 3D point clouds. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). 37633772. [24] Raphael Gontijo Lopes, David Ha, Douglas Eck, and Jonathon Shlens. 2019. learned representation for scalable vector graphics. In Proceedings of the IEEE/CVF International Conference on Computer Vision (CVPR). 79307939. [25] Weijian Ma, Shuaiqi Chen, Yunzhong Lou, Xueyang Li, and Xiangdong Zhou. 2024. Draw Step by Step: Reconstructing CAD Construction Sequences from Point Clouds via Multimodal Diffusion. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). 2715427163. [26] Weijian Ma, Minyang Xu, Xueyang Li, and Xiangdong Zhou. 2023. MultiCAD: Contrastive Representation Learning for Multi-modal 3D Computer-Aided Design Models. In Proceedings of the 32nd ACM International Conference on Information and Knowledge Management (Birmingham, United Kingdom) (CIKM 23). Association for Computing Machinery, New York, NY, USA, 17661776. [27] Tovey Michael. 1989. Drawing and CAD in industrial design. Design Studies 10, 1 (1989), 2439. [28] Networkx Contributors. 20142024. Networkx: Network Analysis in Python. https://networkx.org/ [29] Ke Niu, Yuwen Chen, Haiyang Yu, Zhuofan Chen, Xianghui Que, Bin Li, and Xiangyang Xue. 2025. PHT-CAD: Efficient CAD Parametric Primitive Analysis with Progressive Hierarchical Tuning. arXiv preprint arXiv:2503.18147 (2025). [30] Maxime Oquab, TimothÃ©e Darcet, Theo Moutakanni, Huy V. Vo, Marc Szafraniec, Vasil Khalidov, Pierre Fernandez, Daniel Haziza, Francisco Massa, Alaaeldin El-Nouby, Russell Howes, Po-Yao Huang, Hu Xu, Vasu Sharma, Shang-Wen Li, Wojciech Galuba, Mike Rabbat, Mido Assran, Nicolas Ballas, Gabriel Synnaeve, Ishan Misra, Herve Jegou, Julien Mairal, Patrick Labatut, Armand Joulin, and Piotr Bojanowski. 2023. DINOv2: Learning Robust Visual Features without Supervision. arXiv:2304.07193 (2023). [31] Ivan Puhachov, Cedric Martens, Paul G. Kry, and Mikhail Bessmeltsev. 2023. Reconstruction of Machine-Made Shapes from Bitmap Sketches. ACM Transactions on Graphics (TOG) 42, 6 (Dec. 2023), 16 pages. [32] PythonOCC Contributors. 2008-2025. PythonOCC: 3D CAD/CAE Modeling for Python. https://dev.opencascade.org/project/pythonocc [33] Charles R. Qi, Li Yi, Hao Su, and Leonidas J. Guibas. 2017. PointNet++: deep hierarchical feature learning on point sets in metric space. In Proceedings of the International Conference on Neural Information Processing Systems (NIPS) (Long Beach, California, USA) (NIPS17). Curran Associates Inc., Red Hook, NY, USA, 51055114. [34] Pradyumna Reddy, Michael Gharbi, Michal Lukac, and Niloy Mitra. 2021. Im2vec: Synthesizing vector graphics without vector supervision. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). 7342 7351. [35] Pradyumna Reddy, Zhifei Zhang, Zhaowen Wang, Matthew Fisher, Hailin Jin, and Niloy J. Mitra. 2021. multi-implicit neural representation for fonts. In Proceedings of the International Conference on Neural Information Processing Systems (NIPS) (NIPS 21). Curran Associates Inc., Red Hook, NY, USA, 11 pages. [36] Zeyu Shen, Mingyang Zhao, Dong-Ming Yan, and Wencheng Wang. 2025. Mesh2Brep: B-Rep Reconstruction Via Robust Primitive Fitting and IntersectionAware Constraints. IEEE Transactions on Visualization and Computer Graphics (TVCG) (2025), 117. [37] Zecheng Tang, Chenfei Wu, Zekai Zhang, Minheng Ni, Shengming Yin, Yu Liu, Zhengyuan Yang, Lijuan Wang, Zicheng Liu, Juntao Li, and Nan Duan. 2024. StrokeNUWA: tokenizing strokes for vector graphic synthesis. In Proceedings of the International Conference on Machine Learning (ICML) (Vienna, Austria) (ICML24). JMLR.org, 16 pages. [38] Mikaela Angelina Uy, Yen-Yu Chang, Minhyuk Sung, Purvi Goel, Joseph Lambourne, Tolga Birdal, and Leonidas Guibas. 2022. Point2cyl: Reverse engineering 3d objects from point clouds to extrusion cylinders. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). 1185011860. [39] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Åukasz Kaiser, and Illia Polosukhin. 2017. Attention is all you need. In Proceedings of the International Conference on Neural Information Processing Systems (NIPS) (Long Beach, California, USA) (NIPS17). 60006010. [40] Weidong Wang and Georges Grinstein. 1993. survey of 3D solid reconstruction from 2D projection line drawings. In Computer Graphics Forum, Vol. 12. MM 25, October 2731, 2025, Dublin, Ireland Feiwei Qin et al. Wiley Online Library, 137158. [41] Xilin Wang, Jia Zheng, Yuanchao Hu, Hao Zhu, Qian Yu, and Zihan Zhou. 2024. From 2D CAD Drawings to 3D Parametric Models: Vision-Language Approach. arXiv preprint arXiv:2412.11892 (2024). [42] Yizhi Wang and Zhouhui Lian. 2021. DeepVecFont: synthesizing high-quality vector fonts via dual-modality learning. ACM Transactions on Graphics (TOG) 40 (Dec. 2021), 15 pages. [43] Karl D. D. Willis, Yewen Pu, Jieliang Luo, Hang Chu, Tao Du, Joseph G. Lambourne, Armando Solar-Lezama, and Wojciech Matusik. 2021. Fusion 360 gallery: dataset and environment for programmatic CAD construction from human design sequences. ACM Transactions on Graphics (TOG) 40, 4 (July 2021), 24 pages. [44] Rundi Wu, Chang Xiao, and Changxi Zheng. 2021. Deepcad: deep generative network for computer-aided design models. In Proceedings of the IEEE/CVF International Conference on Computer Vision (CVPR). 67726782. [45] Xiang Xu, Karl DD Willis, Joseph Lambourne, Chin-Yi Cheng, Pradeep Kumar Jayaraman, and Yasutaka Furukawa. 2022. SkexGen: Autoregressive Generation of CAD Construction Sequences with Disentangled Codebooks. In Proceedings of the International Conference on Machine Learning (ICML). PMLR, 2469824724. [46] Volpe Yary, Palai Matteo, Governi Lapo, and Furferi Rocco. 2010. From 2D Orthographic views to 3D Pseudo-Wireframe: An Automatic Procedure. International Journal of Computer Applications 5 (08 2010), 1824. [47] Mohsen Yavartanoo, Sangmin Hong, Reyhaneh Neshatavar, and Kyoung Mu Lee. 2024. Text2CAD: Text to 3D CAD Generation via Technical Drawings. arXiv preprint arXiv:2411.06206 (2024). [48] Aijia Zhang, Weiqiang Jia, Zou Qiang, Yixiong Feng, Xiaoxiang Wei, and Ye Zhang. 2025. Diffusion-CAD: Controllable Diffusion Model for Generating ComputerAided Design Models. IEEE Transactions on Visualization and Computer Graphics (2025), 112. [49] Chao Zhang, Romain PinquiÃ©, Arnaud Polette, Gregorio Carasi, Henri De Charnace, and Jean-Philippe Pernot. 2023. Automatic 3D CAD models reconstruction from 2D orthographic drawings. Computers & Graphics 114 (2023), 179189. [50] Chao Zhang, Arnaud Polette, Romain PinquiÃ©, Gregorio Carasi, Henri De Charnace, and Jean-Philippe Pernot. 2025. eCAD-Net: Editable Parametric CAD Models Reconstruction from Dumb B-Rep Models Using Deep Neural Networks. Computer-Aided Design 178 (2025), 103806. [51] Shuming Zhang, Zhidong Guan, Hao Jiang, Tao Ning, Xiaodong Wang, and Pingan Tan. 2024. Brep2Seq: dataset and hierarchical deep learning network for reconstruction and generation of computer-aided design models. Journal of Computational Design and Engineering 11, 1 (01 2024), 110134. [52] Yi Zhang, Fazhi He, Rubin Fan, and Bo Fan. 2024. View2CAD: Parsing Multi-view into CAD Command Sequences. In Proceedings of the International Conference on Computer Supported Cooperative Work in Design (CSCWD). IEEE, 29492954. [53] Jiwei Zhou and Jorge D. Camba. 2025. The status, evolution, and future challenges of multimodal large language models (LLMs) in parametric CAD. Expert Systems with Applications (2025), 127520. [54] Shengdi Zhou, Tianyi Tang, and Bin Zhou. 2023. CADParser: learning approach of sequence modeling for B-Rep CAD. In Proceedings of the International Joint Conference on Artificial Intelligence (IJCAI) (Macao, P.R.China) (IJCAI 23). 9 pages. [55] Qiang Zou, Yincai Wu, Zhenyu Liu, Weiwei Xu, and Shuming Gao. 2024. Intelligent CAD 2.0. Visual Informatics 8, 4 (2024), 112. Drawing2CAD: Sequence-to-Sequence Learning for CAD Generation from Vector Drawings MM 25, October 2731, 2025, Dublin, Ireland"
        },
        {
            "title": "Supplementary Material",
            "content": "A Imperfect Case Analysis As mentioned in Section 6, our method presents several opportunities for improvement. To better understand these opportunities, we present several representative types of imperfect cases and discuss them in the following sections. A.1 Parameter Precision Issues As shown in Figure 7, the generated model generally matches the overall shape of the ground-truth, but size discrepancies remain due to the tolerance allowed in our loss function. In case (a), the model is thicker and the hole radius is smaller. Case (b) shows an overestimated disc thickness, while in case (c), the entire model is thinner than expected. Figure 7: Imperfect cases about parameter precision issues. \"4x\" means using combination of all four views as input. A.2 View-Specific Information Trade-offs As illustrated in Figure 8, three orthographic views help the model capture planar features like symmetry and layout, but often lead to inaccurate depth interpretation, as seen in case (a) where extrusion depth is misestimated. Conversely, the isometric view enhances depth perception but lacks precise planar alignment, resulting in disordered hole placement in case (b). These discrepancies highlight how different view types convey complementary geometric cues, influencing the models focus and generation behavior. A.3 Multi-View Integration Challenges Combining all four views as input can provide complementary geometric cues, thereby enhancing the models overall understanding, as shown in Figure 9 (a). However, this strategy is not always reliable. When the information conveyed by different views is inconsistent, the model may receive conflicting signals, resulting in ambiguity and inaccurate reconstructions that deviate from the intended design. For instance, in Figure 9 (b), the orthographic views indicate protrusion at the bottom of the object, while the isometric view omits this feature, leading to inconsistent guidance and an erroneous output. In more extreme cases, such as the one shown Figure 8: Imperfect cases about view-specific information trade-offs. \"1x\", \"3x\" means using single isometric view or three orthographic views as input respectively. in Figure 9 (c), none of the four views reveal critical structural elementa hole on the left side of the modeldue to occlusion or viewpoint limitations. Consequently, the generated model only captures the visible surfaces, failing to infer occluded or hidden geometry, which compromises both the completeness and the fidelity of the reconstruction. Figure 9: Imperfect cases about multi-view integration challenges. \"1x\", \"3x\", \"4x\" means using single isometric view, three orthographic views, or combination of all four views as input respectively."
        }
    ],
    "affiliations": [
        "Guangzhou University, Guangzhou, China",
        "Hangzhou Dianzi University, Hangzhou, China",
        "Shenzhen Research Institute of Big Data, Shenzhen, China",
        "University of Science and Technology of China, Hefei, China",
        "Zhejiang University, Hangzhou, China"
    ]
}