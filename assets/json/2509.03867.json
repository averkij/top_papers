{
    "paper_title": "Drivel-ology: Challenging LLMs with Interpreting Nonsense with Depth",
    "authors": [
        "Yang Wang",
        "Chenghao Xiao",
        "Chia-Yi Hsiao",
        "Zi Yan Chang",
        "Chi-Li Chen",
        "Tyler Loakman",
        "Chenghua Lin"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "We introduce Drivelology, a unique linguistic phenomenon characterised as \"nonsense with depth\", utterances that are syntactically coherent yet pragmatically paradoxical, emotionally loaded, or rhetorically subversive. While such expressions may resemble surface-level nonsense, they encode implicit meaning requiring contextual inference, moral reasoning, or emotional interpretation. We find that current large language models (LLMs), despite excelling at many natural language processing (NLP) tasks, consistently fail to grasp the layered semantics of Drivelological text. To investigate this, we construct a small but diverse benchmark dataset of over 1,200 meticulously curated examples, with select instances in English, Mandarin, Spanish, French, Japanese, and Korean. Annotation was especially challenging: each of the examples required careful expert review to verify that it truly reflected Drivelological characteristics. The process involved multiple rounds of discussion and adjudication to address disagreements, highlighting the subtle and subjective nature of the Drivelology. We evaluate a range of LLMs on classification, generation, and reasoning tasks. Our results reveal clear limitations of LLMs: models often confuse Drivelology with shallow nonsense, produce incoherent justifications, or miss the implied rhetorical function altogether. These findings highlight a deeper representational gap in LLMs' pragmatic understanding and challenge the assumption that statistical fluency implies cognitive comprehension. We release our dataset and code to facilitate further research in modelling linguistic depth beyond surface-level coherence."
        },
        {
            "title": "Start",
            "content": "Drivel-ology: Challenging LLMs with Interpreting Nonsense with Depth Yang Wang1, Chenghao Xiao2, Chia-Yi Hsiao2, Zi Yan Chang3, Chi-Li Chen3, Tyler Loakman3, Chenghua Lin1, 1The University of Manchester, 2Durham University, 3The University of Sheffield 5 2 0 2 4 ] . [ 1 7 6 8 3 0 . 9 0 5 2 : r a"
        },
        {
            "title": "Abstract",
            "content": "We introduce Drivelology, unique linguistic phenomenon characterised as nonsense with depth utterances that are syntactically coherent yet pragmatically paradoxical, emotionally loaded, or rhetorically subversive. While such expressions may resemble surface-level nonsense, they encode implicit meaning requiring contextual inference, moral reasoning, or emotional interpretation. We find that current large language models (LLMs), despite excelling at many natural language processing (NLP) tasks, consistently fail to grasp the layered semantics of Drivelological text. To investigate this, we construct small but diverse benchmark dataset of over 1,200 meticulously curated examples, with select instances in English, Mandarin, Spanish, French, Japanese, and Korean. Annotation was especially challenging: each of the examples required careful expert review to verify that it truly reflected Drivelological characteristics. The process involved multiple rounds of discussion and adjudication to address disagreements, highlighting the subtle and subjective nature of the Drivelology. We evaluate range of LLMs on classification, generation, and reasoning tasks. Our results reveal clear limitations of LLMs: models often confuse Drivelology with shallow nonsense, produce incoherent justifications, or miss the implied rhetorical function altogether. These findings highlight deeper representational gap in LLMs pragmatic understanding and challenge the assumption that statistical fluency implies cognitive comprehension. We release our dataset and code to facilitate further research in modelling linguistic depth beyond surface-level coherence."
        },
        {
            "title": "Introduction",
            "content": "Large language models (LLMs) have achieved impressive success across wide range of natural language processing tasks, from machine translation and summarisation to commonsense reasoning and dialogue generation (Achiam et al., 2023; Qwen Team, 2024; Liu et al., 2024a; Guo et al., 2025). These models exhibit high degrees of fluency, contextual awareness, and even emergent reasoning capabilities. However, whether such performance reflects genuine understanding or merely statistical pattern-matching, remains an open and pressing question (Rayhan et al., 2023). The continuous evolution of Internet language as distinct linguistic style offers novel and insightful avenue for exploring the depth of understanding in LLMs (Ignacio et al., 2024; Mei et al., 2024). Internet language, characterised by its dynamic evolution and cultural embedding, serves as an effective indicator to assess whether models truly grasp deeper semantics or simply rely on superficial pattern recognition. In particular, we introduce the term Drivelology, combining drivel (i.e., nonsense) with -ology (i.e., the study of), which exemplifies this complexity. Drivelology often involves narratives with dual or multiple layers of meaning, employing non-linear structures and ambiguous expressions that challenge LLMs. Unlike purely nonsensical yet grammatically correct sentences such as Colourless green ideas sleep furiously (Lees, 1957), or simplistic tautologies like either it is or it isnt, Drivelology intentionally embeds subtle cultural references, irony, or satire within superficially trivial or absurd narratives. Hence, it differs significantly from typical internet content, such as inspirational quotes or prose, by demanding deeper interpretative engagement from both human readers and LLMs. Previous studies have explored the difficulty for LLMs to understand humour, sarcasm, and irony (Loakman et al., 2023; Romanowski et al., 2025; Zheng et al., 2025). However, Drivelology differs fundamentally from these phenomena by employing more complex, non-linear narratives and deeper ambiguities, making it uniquely challenging benchmark for assessing LLMs semantic comprehension. Examining the capability of LLMs to comprehend Drivelology is critical, as it provides unique window into their social and semantic comprehension abilities. As significant facet of human creative expression, Drivelology encapsulates subtle emotions and culturally embedded meanings, offering valuable insights into the complexities of human communication. Understanding such linguistic forms is essential for developing socially intelligent systems (Gandhi et al., 2023; Kosinski, 2024; Mittelst√§dt et al., 2024). Moreover, enhancing AIs grasp of Drivelology can potentially boost creativity in AI applications, improving user experience in content creation tools (Hu et al., 2024), and even aiding model safety (Matamoros Fernandez et al., 2023) through better detection of contextually ambiguous content. To this end, we collected and annotated novel benchmark dataset called DRIVELHUB for understanding Drivelology from the internet, focusing on narratives that are nonsense with depth. Each sample is annotated with the underlying meaning it reveals. The annotation process was exceptionally rigorous: every candidate example underwent multiple rounds of independent review by expert annotators, followed by in-depth group discussions to resolve ambiguities and disagreements. Given the subtlety and subjectivity of Drivelological meaning, we established detailed annotation guidelines (Figure 6) and required consensus among annotators before finalising each label. This intensive process ensured high-quality, reliable annotations and reflected the complexity inherent in capturing implicit meaning and layered rhetorical intent. We introduce four tasks: (1) Drivelology Detection: binary classification task to determine whether given text is non-Drivelology or Drivelology, (2) Drivelology Tagging: multi-label classification task to assign one or more of the following categories (3.1) to Drivelology samples: Misdirection, Paradox, Switchbait, Inversion, and Wordplay, (3) Implicit Narrative Writing: This task involves generating an implicit narrative for given Drivelology sample, and (4) Narrative Selection: multiple-choice task where the model selects the correct narrative from five options. These tasks collectively encompass various levels of Drivelology understanding, ranging from literal content comprehension to more sophisticated narrative reasoning, thereby providing comprehensive assessment of Drivelology understanding capabilities. We conducted extensive experiments using the DRIVELHUB dataset, evaluating both proprietary and opensource LLMs. We release our annotations and code to facilitate future AI research on understanding human creative expression at https://github.com/ ExtraOrdinaryLab/drivelology."
        },
        {
            "title": "2 Related Work",
            "content": "LLMs Evaluations. Recent LLMs have demonstrated remarkable performance in following human instructions and performing various downstream tasks through zero-shot prompting (Naveed et al., 2023; Liang et al., 2024; Chang et al., 2024; Liu et al., 2024b). Various benchmarks have been proposed to evaluate their performance, primarily focusing on assessing the fundamental capabilities of LLMs (Welbl et al., 2017; Wang et al., 2018; Zellers et al., 2019; Sakaguchi et al., 2021; Hendrycks et al., 2021; Suzgun et al., 2023; Zheng et al., 2023; Zhou et al., 2023; Chollet et al., 2024; Jimenez et al., 2024; Yang et al., 2025). However, the ability of large models to perform in-depth social reasoning and accurately understand human contexts remains underexplored (Hu and Shu, 2023; Feng et al., 2024). Humour, Irony, and Sarcasm. Humour, irony, and sarcasm are fundamental elements of human interaction, each requiring deep understanding of language, context, and social cues (Palmer, 2003; Filik et al., 2016; K√∂der and Falkum, 2021; Mir and Laskurain-Ibarluzea, 2021; Loakman et al., 2023). While computational approaches have addressed these phenomena (Yang et al., 2015; Jentzsch and Kersting, 2023; Boutsikaris and Polykalas, 2024), they often focus on resolving central contradiction between literal statement and its context (Kreuz and Link, 2002; Misra and Arora, 2019). Classic sarcasm, for instance, is typically understood through single cognitive step: inverting words meaning based on negative premise, as in, Forgetting assignments and stressing over grades, what fun semester, an example from the BIGBench Snarks subset (Srivastava et al., 2023). We argue that Drivelology presents more profound challenge, distinguished by two key characteristics: (1) its compositional, multi-layered structure, and (2) its use of pragmatic paradox and ambiguity. For example, the statement deeply admire Che Guevaras anti-capitalist spirit, so bought all his merchandise is not simple semantic inversion. Its critique of performative activism requires synthesising cultural knowledge, making irony just one component of its layered meaning. Furthermore, Drivelology uses pragmatic paradoxes like Im good at everything except what cant do. This statement is not explicitly sarcastic nor ironic; its challenge lies in navigating the ambiguity of the speakers intent (e.g., humour, self-deprecation). This reliance on compositional meaning and deliberate ambiguity is what sets Drivelology apart. Distinguishing Drivelology from Deceptive and Nonsensical Language. To properly situate our work, it is crucial to distinguish Drivelology from related pragmatic concepts. Cappelen and Dever (2019) identify category termed deep bullshit: utterances defined by an indifference to whether the words make any sense at all, resulting in genuine nonsense. For example, statement like Colourless green ideas sleep furiously qualifies as deep bullshit as it is semantically null. This is distinct from the more widely discussed Frankfurt-style bullshit, which is characterised by an indifference to truth rather than meaning, often deployed to persuade without regard for fact (Frankfurt, 2005). For instance, politician might declare they bring fresh perspective, unburdened by the stagnant thinking of Washington insiders, statement chosen for its persuasive effect, not its accuracy. Drivelology shares superficial resemblance with deep bullshit, as both can appear nonsensical. However, the two are fundamentally antithetical in their purpose and construction. Whereas deep bullshit arises from disregard for meaning, Drivelology is meticulously crafted for the sake of conveying hidden meaning. It is, as we define it, nonsense with depth. The surface-level absurdity of Drivelological text is deliberate rhetorical framework, designed to guide an audience toward an implicit critique, observation, or emotional payload. Thus, unlike vacuous deep bullshit, Drivelology is rhetorically complex and purposeful. While other forms of bad language, such as lying or misleading, are defined by their deceptive relationship to truth (Cappelen and Dever, 2019), Drivelologys defining feature is its purposeful and creative use of apparent nonsense to generate layered semantics. Clarifying these boundaries is essential for developing AI systems that can appreciate subtle human expression. truly capable model must differentiate between genuinely meaningless utterances (deep bullshit) and the sophisticated, implicit communication of Drivelology, task that requires moving beyond surface-level coherence to grasp complex rhetorical intent."
        },
        {
            "title": "3 The DRIVELHUB Dataset",
            "content": "Our benchmark dataset, DRIVELHUB, is designed to evaluate how well LLMs understand Drivelology. Each entry in the dataset includes: (1) Drivelology sample, (2) the underlying message that the sample aims to convey, and (3) one or more categories describing the main type of the Drivelology sample. These components form the basis for variety of tasks that assess different aspects of Drivelology comprehension and reasoning. An overview diagram of the multi-stage process for constructing the DRIVELHUB dataset is presented in Appendix A.1. 3.1 What is Drivelology? Drivelology refers to unique style of language that blends humour, ambiguity, and rhetorical complexity to create statements that are intentionally puzzling or nonsensical. Unlike ordinary nonsense or straightforward jokes, Drivelology often relies on layered meanings, unexpected twists, and linguistic playfulness to engage readers in deeper interpretation or amusement. The defining characteristics of Drivelology can be broadly categorised as follows. Misdirection. This technique leads the listener down an expected path before final twist reveals different, often more literal or absurd, destination. The humour comes from the subversion of the narrative expectation. Example: Dont give up on your dream so easily! Keep sleeping! (The expected path is motivational encouragement; the twist is literal interpretation of \"dream\"). Paradox. This relies on statement that appears logically self-contradictory but contains latent, often humorous or profound, truth. The core of the technique is the clash of seemingly incompatible ideas. Example: will not forget this favour until forget it. (A logically circular statement that humorously asserts the certainty of remembering). Switchbait.1 This technique hinges on specific word or phrase (the bait) that has double meaning. The initial, obvious meaning is suddenly replaced (the switch) by second, surprising one. Example: British: Youve got gun problem. American: Yeah, at least its modern problem. (The bait is the direct criticism of US gun violence. The switch is the Americans reply, which reframes the conversation entirely. It 1The term Switchbait is coined in this work to describe rhetorical device where an initial phrase (the bait) sets up an expectation that is then subverted by sudden switch in context, cultural reference, or language. Figure 1: Overview of the Drivelology evaluation framework for LLMs. The figure illustrates four core tasks designed to systematically assess LLMs ability to understand and reason about Drivelology: Drivelology Detection (binary classification), Drivelology Tagging (multi-label classification), Implicit Narrative Writing (generative reasoning), and Narrative Selection (multiple-choice question answering with both Easy and Hard settings). pivots from the initial topic to dark, sarcastic comparison, implicitly mocking the UKs issues with knife crime as being less modern. The humour is generated by this cynical and culturally-specific counter-attack). Inversion. This technique takes well-known phrase, clich√©, or social script and flips it on its head. The humour arises from reversing familiar structure to create new, often satirical, meaning. Example: Other than being good-looking, having great figure, and having money, have nothing else. (This inverts the structure of humble complaint into an arrogant boast). Wordplay. This is the general use of linguistic creativity for humorous effect, often by exploiting the sounds or multiple meanings of words. It includes puns, double entendres, and phonetic similarities where the structure isnt specific switch or inversion. Example: Do you have any raisins? No? How about date? (A classic pun playing on two meanings of the word date). We note that the defining characteristic of Drivelology is not the use of single technique, but the creative and often simultaneous combination of several within single utterance to produce its layered, nonsensical effect. This inherent complexity is central to our study, which is why the Drivelology Tagging task is formulated as multi-label classification problem (see 3.4), allowing single sample to be annotated with one or more of the following categories."
        },
        {
            "title": "3.2 Drivelology Collection",
            "content": "To ensure comprehensive evaluation, we prioritised high diversity in our benchmark dataset by selecting wide range of topics from different languages. Our data was collected from variety of popular platforms (e.g., Instagram, Threads, TikTok, Facebook, Line, RedNote, Pinterest, Naver, and YouTube). These platforms were chosen strategically as their largest user demographic falls between 25 to 34 years old2, which aligns well with our research focus since Drivelology content predominantly originates from younger generations (Sha, 2024). We collected all the Drivelology examples by manually and randomly browsing through 2According to Statistas social media demographics data as of 2025. these platforms, rather than using search terms. It was difficult to apply search term methods to target Drivelology samples because most of the search results were normal posts rather than Drivelology text. For non-Drivelology samples, we curated content from sources such as famous quotes, proverbs, and Ruozhiba (a popular online forum). These non-Drivelology samples are also multilingual, covering English, Mandarin, Spanish, French, Japanese, and Korean. We further categorised nonDrivelology samples into two types: normal sentences (such as meaningful quotes or proverbs) and pure nonsense (text that lacks logical structure or meaning). significant proportion of the pure nonsense samples were collected from Ruozhiba. 3.3 Data Annotation Labelling Drivelology requires both content comprehension and cultural context understanding. We implemented rigorous four-step annotation protocol: (1) Annotator Selection. We assembled team of seven multilingual annotators3 who all held at least Masters degree and demonstrated proficiency in multiple languages. (2) Drivelology Detection and Tagging. Annotators identified texts as either Drivelology or non-Drivelology, and classified Drivelology samples into categories including Misdirection, Paradox, Switchbait, In- (3) Implicit Narrative version, and Wordplay. Writing. We employed human-in-the-loop process to create the narrative descriptions. For each Drivelology sample, human experts drafted and refined the correct narrative explanation. We then utilised GPT-4.54 as an assistive tool to generate four plausible but incorrect narrative counterparts, all of which underwent final stage of manual verification and editing to ensure their quality as effective distractors. (4) Quality Check. metareviewer with linguistics and psychology expertise reviewed all annotations, excluding ambiguous or controversial samples. The meta-reviewer also revised the narratives as needed to ensure consistent length, uniform writing style, and improve overall readability. Further details about the annotation process can be found in Appendix A.1. 3The annotation team consisted of four authors of this paper and three paid annotators recruited for their linguistic expertise. 4gpt-4.5-preview-2025-02-27 3.4 Task Design To evaluate an LLMs ability to understand Drivelology, we designed four tasks to assess different facets of social and non-linear reasoning. An overview of these tasks is provided in Figure 1. Drivelology Detection. binary classification task where the model must determine if given text is Drivelology or non-Drivelology. Drivelology Tagging. multi-label classification task. The model assigns one or more descriptive categories (see 3.1) to Drivelology sample to capture its layered rhetorical structure. Narrative Writing. generative task that assesses the models ability to articulate the coherent implicit narrative and underlying meaning of Drivelology sample, requiring it to move beyond surface-level reading. Narrative Selection. This multiple-choice question answering (MCQA) task asks the model to pick the correct narrative for Drivelology sample from several options. The Easy version offers one correct answer and four distractors, which we generated by prompting GPT-4.5 to produce plausible but incorrect interpretations of the sample. These AI-generated options were then manually reviewed and refined by human experts to ensure they were effective and challenging. The Hard version adds none of the above option, requiring deeper reasoning, as this option should only be chosen if none of the provided narratives adequately capture the underlying meaning of the Drivelology sample. This additional step significantly increases the tasks complexity, as it prevents reliance on simple elimination strategies."
        },
        {
            "title": "4.1 Models and Settings",
            "content": "We evaluate the models performance in zero-shot setting using recent LLMs. We include both proprietary models such as GPT-4 (Achiam et al., 2023) and Claude-3 (Anthropic, 2024), as well as opensourced models including Qwen3 (Qwen Team, 2025), Qwen2.5 (Qwen Team, 2024), Llama3.1 (Grattafiori et al., 2024), Llama3 (Grattafiori et al., 2024), and DeepSeek V3 (Liu et al., 2024a). To minimise variance across task prompts, we design three distinct prompts for each task and report the average performance over three runs (one for each prompt). Detailed descriptions of the prompts and additional experimental settings are provided in Appendix B.1. 4.2 Evaluation Metrics 5.1 Prompt Language Influence For the understanding tasks, we use accuracy for Drivelology Detection task, F1 score for Drivelology Tagging task, and accuracy for the MCQA task. For the generation task that involves narrative writing, we apply reference-based evaluation metrics commonly used in text generation studies (Celikyilmaz et al., 2020). Specifically, we use BERTScore (Zhang et al., 2020) and LLM-as-ajudge (Zheng et al., 2023). Recent work shows that GPT-based evaluation aligns well with human judgments (Chan et al., 2023; Liu et al., 2023; Hu et al., 2024; Gu et al., 2024), thus we apply GPT4 for LLM-as-a-judge evaluation. Note that we use different GPT variants for different purposes: gpt-4.5 for data annotation, gpt-4o-mini for zeroshot experiments, and gpt-4.1 for LLM-as-a-judge evaluation in text generation tasks. This helps reduce potential evaluation bias toward GPT-4s own generation."
        },
        {
            "title": "5 Main Results",
            "content": "The main results  (Table 1)  show clear hierarchy in model performance. Deepseek-v3 is the dominant model, achieving the top score in five of the six evaluated metrics. The contrast between the two evaluation metrics in the Narrative Writing task is particularly noteworthy. While BERTScorerecall values are high across all models (ranging from 84.67% to 87.11%), suggesting universal proficiency in generating fluent text, the GPT-4-asa-judge scores (rated on 1-5 Likert scale) provide much clearer picture of true narrative quality. On this scale, deepseek-v3 (3.59) and claude-3.5-haiku (3.39) are the only models to score comfortably above three, indicating their outputs were judged as possessing high semantic quality. In stark contrast, other models like llama-3-8b-instruct (2.63) and qwen3-8b-instruct (2.64) fall below this threshold, suggesting their narratives failed to capture the required depth and were deemed qualitatively weaker by the LLM-as-a-judge. The most striking performance gap presents in the MCQA task. The Hard setting causes steep decline in accuracy for all models, exposing critical weakness in subtle reasoning. Notably, qwen3-8b-instruct is notable outlier here, scoring 26.78%, which far surpasses the next-best model. In the Classification tasks, deepseek-v3 again confirms its superior understanding by leading in both Detection (81.67%) and Tagging (55.32%) tasks. An analysis of the impact of prompt language on model performance reveals metric-dependent pattern. As shown in Figure 2, the choice between English and Mandarin prompts is not neutral design but significant factor that influences evaluation outcomes. We identify two distinct and opposing patterns. First, English prompts consistently yield superior performance on tasks that reward lexical precision and complex logical reasoning. In the Narrative Writing task, this is most evident in the BERTScore results, where every model scores higher when prompted in English. This suggests that English instructions may prime the models to generate outputs with greater lexical overlap with the reference translations, feature to which BERTScore is highly sensitive (Hanna and Bojar, 2021). similar advantage for English prompts is observed in MCQA task. The uniform improvement in both Easy and Hard settings implies that English may serve as more robust internal language of reasoning. Conversely, Mandarin prompts produce consistent, albeit smaller, advantage on tasks that prioritise direct content comprehension. This pattern holds across three metrics: the GPTas-a-judge scores for Narrative Writing, and both Detection and Tagging tasks. The improved performance under GPT-as-a-judge, which evaluates qualitative coherence, indicates that Mandarin prompts better align the models with the semantic and narrative intent of the source material. The consistent gains in the classification tasks further suggest that for direct comprehension and categorisation, instructions in Mandarin are more effective."
        },
        {
            "title": "5.2 Role of Language in MCQA Task",
            "content": "A closer look at the Narrative Selection (MCQA) results from Table 1 reveals that aggregate scores mask significant performance variations across the different languages in our DRIVELHUB dataset. As shown in the breakdown in Figure 3, we can analyse the difficulty of each languages content for the models. Among the models, deepseek-v3 consistently demonstrates the most robust cross-lingual performance, achieving the highest accuracy across nearly all languages in both the Easy and Hard settings. The analysis also pinpoints which languages pose greater challenge. Korean and Mandarin consistently result in the lowest accuracy, especially in the Hard task, marking their content as the most difficult for the models to process. Models Narrative MCQA Classification BERT GPT Easy Hard Detect Tag gpt-4o-mini claude-3.5-haiku deepseek-v3 llama-3-8b-instruct llama-3.1-8b-instruct qwen2.5-7b-instruct qwen3-8b-instruct 85.81 86.51 87.11 84.67 85.60 85.51 85.91 2.90 3. 3.59 2.63 2.75 2.78 2.64 81.89 83.17 86.83 77.39 77.56 77.50 83.17 4.67 11.56 15.50 1.67 1.89 3.78 26.78 75.00 71. 81.67 57.81 58.57 62.66 65.00 49.52 52.03 55.32 39.90 36.21 42.49 38.04 Table 1: Main results. For narrative writing task, we report BERTScore-recall (BERT) and GPT-4-as-a-judge (GPT) evaluation scores. For narrative selection task, we report accuracy. For Drivelology classification tasks, we report accuracy for detection task and report weighted F1 score for tagging task. Best scores are bold and the second best ones are marked with underline. Figure 2: Model performance on the multilingual DRIVELHUB dataset, contrasted by prompt language (English vs. Mandarin). Each reported score is the average performance over three distinct prompts to minimise variance. Figure 3: language-based breakdown of Narrative Selection (MCQA) accuracy from Table 1. The charts disaggregate the overall Easy and Hard accuracy scores based on the original language of the Drivelology sample. Prompt Qwen3 Size English Mandarin 4B 8B 14B 4B 8B 14B MCQA Easy Hard 81.00 83.17 83.94 77.61 81.11 83.50 6.00 26.78 45.83 2.44 19.11 47. Table 2: MCQA performance of Qwen3 models. This table shows the accuracy on the Easy and Hard tasks when prompted in English and Mandarin. Full version of all tasks can be checked in Table 5. 5.3 Model Size Scaling on Narrative Selection The results in Table 2 clearly demonstrate positive scaling trend for the Qwen3 series, particularly on the more challenging Hard version of the MCQA task. For the Easy task, performance gains are modest but consistent: as model size increases from 4B to 14B, accuracy improves by approximately 3% for English prompts and 6% for Mandarin prompts. However, the Hard task reveals much more spiking trend. Here, performance scales exceptionally well with model size. With an English prompt, the score leaps from mere 6.00% for the 4B model to 45.83% for the 14B model. The effect is even more pronounced with Mandarin prompt, where the score skyrockets from 2.44% to 47.89%. This indicates that the more complex reasoning required by the Hard task is key differentiator that is unlocked by larger model sizes. Therefore, the ability to handle such complex reasoning appears to be an emergent property in the Qwen3 architecture, strongly correlated with its parameter count."
        },
        {
            "title": "6.1 Analysis in Model Reasoning",
            "content": "In Narrative Writing task, claude-3.5-haiku and deepseek-v3 achieve the highest GPT-4-as-a-judge scores (3.39 and 3.59) and also perform strongly in Drivelology Tagging task (52.03% and 55.32%). This correlation between their performance in both tasks raises an important question: Do these models arrive at correct Drivelology classifications through appropriate reasoning that reflects true understanding of the underlying meaning? To investigate this question, we analyse their reasoning processes across several representative examples. For example: Meng Po: Those who have forgotten their names, please follow me. Deepseekv3 categorise this as switchbait, emphasising the cultural significance of Meng Po, mythological figure who administers the Soup of Forgetfulness in Chinese folklore. Its reasoning explicitly highlights the importance of cultural knowledge, suggesting they treat Meng Pos mythological role as important context that readers must understand to appreciate the Drivelology. In contrast, claude-3.5-haiku categorises it as an paradox, focusing on the logically self-contradictory statement: how can someone who has forgotten their name respond to such call? This divergence in reasoning approaches suggests varying degrees of cultural knowledge internalisation among models. Claude-3.5-haiku appears to have so thoroughly internalised the cultural context of Meng Po that it treats it as implicit knowledge, allowing it to focus on the logical structure of the text rather than its cultural elements. This observation raises important questions about how different models process and prioritise cultural knowledge versus logical reasoning in their analysis of culturally-embedded texts, and how such internalisation affects their ability to identify different categories of Drivelology."
        },
        {
            "title": "6.2 Analysis in Human Reasoning",
            "content": "Drivelology challenges not only LLMs but also human annotators, who often bring diverse perspectives and interpretations to the same text. Because Drivelology is intentionally ambiguous, contradictory, or ironic, it invites multiple plausible readings. Annotators rely on their own linguistic, cultural, and contextual knowledge, which means that the same Drivelology sample can evoke different analytical frameworks depending on who is interpreting it. Consider the statement: hate two kinds of people: the first kind is those who dont finish their.... From paradox perspective, the speaker claims to dislike people who speak incompletely, yet the sentence itself is left unfinished, ironically exemplifying the very behaviour it criticises. This self-contradiction highlights the speakers insincerity and creates paradoxical effect. Alternatively, from misdirection viewpoint, the statement sets up the expectation of complete list, but then abruptly stops, leaving the audience anticipating an answer that never comes. The humour and irony arise from this unresolved expectation. Another example is: deeply admire Che Guevaras anticapitalist spirit, so bought all his merchandise. Here, the paradox lies in admiring Guevaras anticapitalist stance while simultaneously engaging in capitalist consumerism by buying his merchandise. This contradiction turns ideological admiration into commercial participation. The switchbait interpretation depends on cultural knowledge: recognising Che Guevara as symbol of anti-capitalism is key. Without this context, the contradiction, and the humour, may not be apparent. The texts irony and layered meaning rely on shared cultural and historical understanding, making switchbait also an appropriate label."
        },
        {
            "title": "7 Conclusion",
            "content": "In this work, we introduce Drivelology, unique linguistic phenomenon that challenges the semantic and pragmatic understanding of LLMs. By constructing and evaluating the DRIVELHUB dataset across multiple languages and task settings, we demonstrate that current LLMs, despite their strong ability, still struggle to interpret and reason about texts that blend surface-level nonsense with deeper, implicit meaning. Our findings highlight the need for more advanced models capable of non-linear comprehension beyond statistical pattern-matching. We hope that our dataset and benchmarks will inspire further research into the modelling of creative and contextually rich language, ultimately advancing the development of AI systems with deeper social and cultural awareness."
        },
        {
            "title": "Limitations",
            "content": "Language Imbalanced. Almost half of the samples in the DRIVELHUB dataset are in Mandarin, while the remaining samples are distributed among English, Spanish, French, Japanese, and Korean. This results in slight language imbalance, which may affect the generalisability of our findings across different linguistic and cultural contexts. We do our best in 5 to control for potential content distribution bias arising from this imbalance. Additionally, as the DRIVELHUB dataset is still being expanded, we will continue to focus on addressing distribution differences by increasing the representation of Drivelology samples in underrepresented languages. Limited Computation Resources. Due to budget constraints, we were unable to evaluate stronger proprietary LLMs such as GPT-4.5, Claude-3.7, or DeepSeek R1, as their usage costs are prohibitively high. For open-source models, we restricted our experiments to 14B parameter models because of limited computational resources, and were unable to run larger models within our available infrastructure. We encourage researchers and the broader community to expand on this work by evaluating larger or more advanced LLMs as resources permit. Focus on Understanding Rather Than Generation. In this paper, we focus on evaluating the understanding and reasoning abilities of LLMs with respect to Drivelology, rather than their capacity to generate fluent and human-like Drivelology text. While generation is an important aspect, it falls outside the main scope of our study. Nevertheless, we include discussion in the Appendix with sample generations, illustrating that current LLMs often require over 20 attempts to produce Drivelology text that achieves comprehensive alignment between topic, rhetorical category, and sentence structure."
        },
        {
            "title": "Ethics Statement",
            "content": "Copyright and License. All data samples used in this study are collected exclusively from publicly available content on social media platforms. We respect the intellectual property rights of original authors by ensuring that no proprietary or paywalled material is included. The dataset is released solely for research purposes under license that prohibits commercial use and redistribution of original content. Content Review and Harm Mitigation. To uphold ethical standards, we carefully review all collected samples and filter out any content that may be offensive, harmful, or violate privacy. Our annotation process is designed to ensure that sensitive information is excluded and that the dataset does not propagate hate speech, harassment, or other forms of harmful language. Intended Use. The dataset and accompanying resources are intended strictly for academic research and the advancement of natural language processing technologies. Users are advised to adhere to ethical guidelines and local regulations when using the dataset."
        },
        {
            "title": "Acknowledgments",
            "content": "Tyler Loakman is supported by the Centre for Doctoral Training in Speech and Language Technologies (SLT) and their Applications funded by UK Research and Innovation (EP/S023062/1)."
        },
        {
            "title": "References",
            "content": "Josh Achiam, Steven Adler, Sandhini Agarwal, Lama Ahmad, Ilge Akkaya, Florencia Leoni Aleman, Diogo Almeida, Janko Altenschmidt, Sam Altman, Shyamal Anadkat, and 1 others. 2023. Gpt-4 technical report. arXiv preprint arXiv:2303.08774. AI Anthropic. 2024. The claude 3 model family: Opus, sonnet, haiku. Claude-3 Model Card, 1:1. Leonidas Boutsikaris and Spyros Polykalas. 2024. comparative review of deep learning techniques on the classification of irony and sarcasm in text. IEEE Transactions on Artificial Intelligence. Herman Cappelen and Josh Dever. 2019. Bad language. Oxford University Press. Asli Celikyilmaz, Elizabeth Clark, and Jianfeng Gao. 2020. Evaluation of text generation: survey. arXiv preprint arXiv:2006.14799. David Chan, Suzanne Petryk, Joseph Gonzalez, Trevor Darrell, and John Canny. 2023. Clair: Evaluating image captions with large language models. arXiv preprint arXiv:2310.12971. Yupeng Chang, Xu Wang, Jindong Wang, Yuan Wu, Linyi Yang, Kaijie Zhu, Hao Chen, Xiaoyuan Yi, Cunxiang Wang, Yidong Wang, and 1 others. 2024. survey on evaluation of large language models. ACM transactions on intelligent systems and technology, 15(3):145. Francois Chollet, Mike Knoop, Gregory Kamradt, and Bryan Landers. 2024. Arc prize 2024: Technical report. arXiv preprint arXiv:2412.04604. Tao Feng, Chuanyang Jin, Jingyu Liu, Kunlun Zhu, Haoqin Tu, Zirui Cheng, Guanyu Lin, and Jiaxuan You. 2024. How far are we from AGI: Are LLMs all we need? Transactions on Machine Learning Research. Survey Certification. Ruth Filik, Alexandra T, urcan, Dominic Thompson, Nicole Harvey, Harriet Davies, and Amelia Turner. 2016. Sarcasm and emoticons: Comprehension and emotional impact. Quarterly Journal of Experimental Psychology, 69(11):21302146. Harry G. Frankfurt. 2005. On bullshit. Princeton University Press, Princeton, NJ. Kanishk Gandhi, Jan-Philipp Fr√§nken, Tobias Gerstenberg, and Noah Goodman. 2023. Understanding social reasoning in language models with language In Thirty-seventh Conference on Neural models. Information Processing Systems Datasets and Benchmarks Track. Aaron Grattafiori, Abhimanyu Dubey, Abhinav Jauhri, Abhinav Pandey, Abhishek Kadian, Ahmad AlDahle, Aiesha Letman, Akhil Mathur, Alan Schelten, Alex Vaughan, and 1 others. 2024. The llama 3 herd of models. arXiv preprint arXiv:2407.21783. Jiawei Gu, Xuhui Jiang, Zhichao Shi, Hexiang Tan, Xuehao Zhai, Chengjin Xu, Wei Li, Yinghan Shen, Shengjie Ma, Honghao Liu, and 1 others. 2024. survey on llm-as-a-judge. arXiv preprint arXiv:2411.15594. Daya Guo, Dejian Yang, Haowei Zhang, Junxiao Song, Ruoyu Zhang, Runxin Xu, Qihao Zhu, Shirong Ma, Peiyi Wang, Xiao Bi, and 1 others. 2025. Deepseek-r1: Incentivizing reasoning capability in arXiv preprint llms via reinforcement learning. arXiv:2501.12948. Michael Hanna and OndÀárej Bojar. 2021. fine-grained analysis of bertscore. In Proceedings of the Sixth Conference on Machine Translation, pages 507517. Dan Hendrycks, Collin Burns, Steven Basart, Andy Zou, Mantas Mazeika, Dawn Song, and Jacob Steinhardt. 2021. Measuring massive multitask language understanding. In International Conference on Learning Representations. Zhe Hu, Tuo Liang, Jing Li, Yiren Lu, Yunlai Zhou, Yiran Qiao, Jing Ma, and Yu Yin. 2024. Cracking the code of juxtaposition: Can ai models understand the humorous contradictions. arXiv preprint arXiv:2405.19088. Zhiting Hu and Tianmin Shu. 2023. Language models, agent models, and world models: The law for machine reasoning and planning. arXiv preprint arXiv:2312.05230. Marvin John Ignacio, Thanh Tin Nguyen, Hulin Jin, and Yong-guk Kim. 2024. Meme analysis using llmbased contextual information and u-net encapsulated transformer. IEEE Access, pages 11. Sophie Jentzsch and Kristian Kersting. 2023. Chatgpt is fun, but it is not funny! humor is still challenging large language models. arXiv preprint arXiv:2306.04563. Carlos Jimenez, John Yang, Alexander Wettig, Shunyu Yao, Kexin Pei, Ofir Press, and Karthik Narasimhan. 2024. SWE-bench: Can language models resolve real-world github issues? In The Twelfth International Conference on Learning Representations. Franziska K√∂der and Ingrid Lossius Falkum. 2021. Irony and perspective-taking in children: The roles of norm violations and tone of voice. Frontiers in Psychology, 12:624604. Michal Kosinski. 2024. Evaluating large language models in theory of mind tasks. Proceedings of the National Academy of Sciences, 121(45):e2405460121. Roger Kreuz and Kristen Link. 2002. Asymmetries in the use of verbal irony. Journal of language and social psychology, 21(2):127143. Robert Lees. 1957. Syntactic structures. Alexander Lex, Nils Gehlenborg, Hendrik Strobelt, Romain Vuillemot, and Hanspeter Pfister. 2014. Upset: visualization of intersecting sets. IEEE transactions on visualization and computer graphics, 20(12):19831992. Zijing Liang, Yanjie Xu, Yifan Hong, Penghui Shang, Qi Wang, Qiang Fu, and Ke Liu. 2024. survey of multimodel large language models. In Proceedings of the 3rd International Conference on Computer, Artificial Intelligence and Control Engineering, pages 405409. Aixin Liu, Bei Feng, Bing Xue, Bingxuan Wang, Bochao Wu, Chengda Lu, Chenggang Zhao, Chengqi Deng, Chenyu Zhang, Chong Ruan, and 1 others. 2024a. Deepseek-v3 technical report. arXiv preprint arXiv:2412.19437. Yang Liu, Dan Iter, Yichong Xu, Shuohang Wang, Ruochen Xu, and Chenguang Zhu. 2023. G-eval: Nlg evaluation using gpt-4 with better human alignment. arXiv preprint arXiv:2303.16634. Yuan Liu, Haodong Duan, Yuanhan Zhang, Bo Li, Songyang Zhang, Wangbo Zhao, Yike Yuan, Jiaqi Wang, Conghui He, Ziwei Liu, and 1 others. 2024b. Mmbench: Is your multi-modal model an all-around player? In European conference on computer vision, pages 216233. Springer. Tyler Loakman, Aaron Maladry, and Chenghua Lin. 2023. The iron(ic) melting pot: Reviewing human evaluation in humour, irony and sarcasm generation. In Findings of the Association for Computational Linguistics: EMNLP 2023, pages 66766689, Singapore. Association for Computational Linguistics. Ariadna Matamoros Fernandez, Louisa Bartolo, and Luke Troynar. 2023. Humour as an online safety issue: Exploring solutions to help platforms better address this form of expression. Internet Policy Review, 12(1). Lingrui Mei, Shenghua Liu, Yiwei Wang, Baolong Bi, and Xueqi Cheng. 2024. Slang: New concept comprehension of large language models. arXiv preprint arXiv:2401.12585. Montserrat Mir and Patxi Laskurain-Ibarluzea. 2021. Spanish and english verbal humour: comparative study of late-night talk show monologues. Contrastive Pragmatics, 3(2):278312. Rishabh Misra and Prahal Arora. 2019. Sarcasm detection using hybrid neural network. arXiv preprint arXiv:1908.07414. Justin Mittelst√§dt, Julia Maier, Panja Goerke, Frank Zinn, and Michael Hermes. 2024. Large language models can outperform humans in social situational judgments. Scientific Reports, 14(1):27449. Humza Naveed, Asad Ullah Khan, Shi Qiu, Muhammad Saqib, Saeed Anwar, Muhammad Usman, Naveed Akhtar, Nick Barnes, and Ajmal Mian. 2023. comprehensive overview of large language models. arXiv preprint arXiv:2307.06435. Jerry Palmer. 2003. Taking humour seriously. Routledge. Qwen Team. 2024. Qwen2.5: party of foundation models. Qwen Team. 2025. Qwen3 technical report. arXiv preprint arXiv:2505.09388. Rafael Rafailov, Archit Sharma, Eric Mitchell, Christopher Manning, Stefano Ermon, and Chelsea Finn. 2023. Direct preference optimization: Your language model is secretly reward model. In Thirty-seventh Conference on Neural Information Processing Systems. Abu Rayhan, Rajan Rayhan, and Swajan Rayhan. 2023. Artificial general intelligence: Roadmap to achieving human-level capabilities. DOI, 10:13140. Adrianna Romanowski, Pedro HV Valois, and Kazuhiro Fukui. 2025. From punchlines to predictions: metric to assess llm performance in identifying humor in stand-up comedy. arXiv preprint arXiv:2504.09049. Keisuke Sakaguchi, Ronan Le Bras, Chandra Bhagavatula, and Yejin Choi. 2021. Winogrande: An adversarial winograd schema challenge at scale. Communications of the ACM, 64(9):99106. John Schulman, Filip Wolski, Prafulla Dhariwal, ProxiAlec Radford, and Oleg Klimov. 2017. mal policy optimization algorithms. arXiv preprint arXiv:1707.06347. Rula Sha. 2024. Literary effect of \"nonsense literature\": perspective of rhetorical style. Journal of Eastern Liaoning University (Social Sciences), 26(1). Zhihong Shao, Peiyi Wang, Qihao Zhu, Runxin Xu, Junxiao Song, Xiao Bi, Haowei Zhang, Mingchuan Zhang, YK Li, Yang Wu, and 1 others. 2024. Deepseekmath: Pushing the limits of mathematical reasoning in open language models. Aarohi Srivastava, Abhinav Rastogi, Abhishek Rao, Abu Awal Md Shoeb, Abubakar Abid, Adam Fisch, Adam R. Brown, Adam Santoro, Aditya Gupta, Adri√† Garriga-Alonso, Agnieszka Kluska, Aitor Lewkowycz, Akshat Agarwal, Alethea Power, Alex Ray, Alex Warstadt, Alexander W. Kocurek, Ali Safaya, Ali Tazarv, and 431 others. 2023. Beyond the imitation game: Quantifying and extrapolating the capabilities of language models. Transactions on Machine Learning Research. Featured Certification. Mirac Suzgun, Nathan Scales, Nathanael Sch√§rli, Sebastian Gehrmann, Yi Tay, Hyung Won Chung, Aakanksha Chowdhery, Quoc Le, Ed Chi, Denny Zhou, and Jason Wei. 2023. Challenging BIG-bench tasks and whether chain-of-thought can solve them. In Findings of the Association for Computational Linguistics: ACL 2023, pages 1300313051, Toronto, Canada. Association for Computational Linguistics. Alex Wang, Amanpreet Singh, Julian Michael, Felix Hill, Omer Levy, and Samuel Bowman. 2018. GLUE: multi-task benchmark and analysis platform for natural language understanding. In Proceedings of the 2018 EMNLP Workshop BlackboxNLP: Analyzing and Interpreting Neural Networks for NLP, pages 353355, Brussels, Belgium. Association for Computational Linguistics. Johannes Welbl, Nelson F. Liu, and Matt Gardner. 2017. Crowdsourcing multiple choice science questions. In Proceedings of the 3rd Workshop on Noisy Usergenerated Text, pages 94106, Copenhagen, Denmark. Association for Computational Linguistics. Diyi Yang, Alon Lavie, Chris Dyer, and Eduard Hovy. 2015. Humor recognition and humor anchor extraction. In Proceedings of the 2015 conference on empirical methods in natural language processing, pages 23672376. John Yang, Carlos E. Jimenez, Alex L. Zhang, Kilian Lieret, Joyce Yang, Xindi Wu, Ori Press, Niklas Muennighoff, Gabriel Synnaeve, Karthik R. Narasimhan, Diyi Yang, Sida I. Wang, and Ofir Press. 2025. SWE-bench multimodal: Do ai systems generalize to visual software domains? In The Thirteenth International Conference on Learning Representations. Rowan Zellers, Ari Holtzman, Yonatan Bisk, Ali Farhadi, and Yejin Choi. 2019. HellaSwag: Can machine really finish your sentence? In Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages 47914800, Florence, Italy. Association for Computational Linguistics. Tianyi Zhang, Varsha Kishore, Felix Wu, Kilian Q. Weinberger, and Yoav Artzi. 2020. Bertscore: EvalIn International uating text generation with bert. Conference on Learning Representations. Lianmin Zheng, Wei-Lin Chiang, Ying Sheng, Siyuan Zhuang, Zhanghao Wu, Yonghao Zhuang, Zi Lin, Zhuohan Li, Dacheng Li, Eric Xing, Hao Zhang, Joseph E. Gonzalez, and Ion Stoica. 2023. Judging LLM-as-a-judge with MT-bench and chatbot arena. In Thirty-seventh Conference on Neural Information Processing Systems Datasets and Benchmarks Track. Yilun Zheng, Sha Li, Fangkun Wu, Yang Ziyi, Lin Hongchao, Zhichao Hu, Cai Xinjun, Ziming Wang, Jinxuan Chen, Sitao Luan, and 1 others. 2025. Fanchuan: multilingual and graph-structured benchmark for parody detection and analysis. arXiv preprint arXiv:2502.16503. Jeffrey Zhou, Tianjian Lu, Swaroop Mishra, Siddhartha Brahma, Sujoy Basu, Yi Luan, Denny Zhou, and Le Hou. 2023. Instruction-following evaluation for large language models. Preprint, arXiv:2311.07911."
        },
        {
            "title": "A Dataset Details",
            "content": "A.1 Overview of the Annotation Process Labelling Drivelology presents significant challenges, not only because it demands deep familiarity with both content and cultural context, but also due to the potential for divergent interpretations among annotators from varied backgrounds. For each Drivelology sample, we annotate the underlying narrative and the category of the Drivelology. To ensure high-quality and precise annotations, we designed multi-step annotation protocol as follows: (1) Annotator Selection. We recruited multilingual annotators, and ensured that they could comprehend the Drivelology. Eight human judges5 participated in the annotation process, all of whom are proficient Mandarin and English speakers (some speak more than three languages) and have at least Masters degree. (2) Drivelology Detection and Tagging. Each annotator was tasked with determining whether given text is nonDrivelology or Drivelology. Non-Drivelology includes both normal, meaningful sentences and pure nonsense that lacks rhetorical or semantic structure. If the text is identified as Drivelology, the annotators then perform multi-label classification task, assigning one or more of the following categories to the sample: Misdirection, Paradox, Switchbait, Inversion, and Wordplay. (3) Implicit Narrative Writing. Given Drivelology sample, we first prompt GPT-4 to generate narrative descriptions, illustrating the Drivelologys narrative and explaining the underlying meaning. Human annotators then double-check and modify the contents through dialogue interactions with the GPT-4 model to obtain correct narrative. Additionally, we prompt GPT-4 to generate four hard negative counterparts to form multiple-choice question answering task for our experiment. As narrative writing is inherently open-ended and involve subjectivity, we additionally frame this as selection tasks, and ensure that the correct option is clearly and objectively superior than the negative options to mitigate subjectivity. Following Achiam et al. (2023), We primarily rely on human annotators to obtain gold-standard annotations, while allowing the annotators to collaborate with GPT-4. (4) Quality Check with Verification. To further minimise annotation errors, an experienced meta-reviewer 5The original annotation was performed by seven annotators, and psychology/linguistics expert made the final decision. with background in linguistics and psychology systematically reviewed all annotated samples. The meta-reviewer excludes the samples with ambiguous or controversial narratives as some of them may introduce bias. This process ensures the quality of the annotated components for benchmark dataset construction. A.2 Dataset Distribution Table 4 presents the language distribution of samples in the DRIVELHUB dataset. As shown, the dataset is skewed toward Mandarin, which accounts for 277 out of the total Drivelology samples. In contrast, other languages such as Japanese and Korean are present only in limited quantities. To characterise the distribution and overlap of annotation categories in our dataset, we present an UpSet plot (Lex et al., 2014) is presented in Figure 5, summarising intersections among the five Drivelology categories."
        },
        {
            "title": "B Experimental Details",
            "content": "B.1 Experiment Prompts To ensure reproducibility and transparency, we provide the exact prompts used in each of our experimental tasks. These prompts were carefully designed to probe different aspects of Drivelology comprehension and generation across various LLMs. Below, we detail the prompts for each task: Drivelology Detection (Figure 7), Drivelology Tagging (Figure 8), Narrative Writing (Figure 9), Narrative Selection (Figure 10 for Easy and Figure 11 for Hard)."
        },
        {
            "title": "C Drivelology Generation",
            "content": "To explore the generative capabilities of LLMs in Drivelology, we conducted case study using GPT4. Our goal was to assess whether the model can produce contextually natural and pragmatically rich Drivelology examples, focusing on both surface form and deeper pragmatic alignment. We designed two experiment settings: (1) Minimal Guidance and (2) With Category Definitions. In the Minimal Guidance setting, the model received only brief introduction to Drivelology, without any example texts or category definitions. In the Category Definitions setting, the model was provided with detailed definitions of the five Drivelology categories (see 3.1). For each stage, we tested three prompting strategies: zero-shot, oneshot, and five-shot. Figure 4: Overview of the multi-stage process for constructing the DRIVELHUB dataset. Text Translated Text Â§úÂ∫óÈÄôÁ®ÆÂú∞ÊñπÈÇÑÊòØÂ∞ëÂéªËÄ≥ÊúµÊúÉËÅæÊéâÊàëÈô™Êúã ÂèãÂéªÈÅé‰∏ÄÊ¨°Âæå‰æÜÁî∑ÂèãÂè´Êàë‰∏çË¶ÅÂéªÊàëÈÉΩËÅΩ‰∏ç Ë¶ã Nightclubs are the kind of place you should go to less, your ears will go deaf. went once to accompany friend, and later my boyfriend told me not to go, but couldnt hear him. Taggings wordplay ÊÑõ‰∏ÄÂÄã‰∫∫ÊòØËóè‰∏ç‰ΩèÁöÑ‰ΩÜÊÑõÂÖ©ÂÄã‰∏ÄÂÆöË¶ÅËóè‰Ωè Loving someone cannot be hidden, but loving two switchbait people must be hidden. ÊØçË¶™ÁØÄÂ∑≤Á∂ìÊÉ≥Â•ΩË¶ÅÈÄÅ‰ªÄÈ∫º‰∫ÜÁµ¶Ëá™Â∑±Ë≤∑‰ª∂Êñ∞Ë°£ ÊúçÈÄÅÂ™ΩÂ™Ω‰∏ÄÂÄãÊºÇ‰∫ÆÁöÑÂ•≥ÂÖí Mothers Day gift is already decided. Buy myself new dress and give my mom beautiful daughter. misdirection ÂêåÂ≠∏‰Ω†ÈÉΩÊÄéÈ∫º‰ΩúÂºäÊòéÂ§©ÊÆµËÄÉÊàëÂÅ∑ÂÅ∑ ÁöÑÊääË™≤Êú¨ÁöÑÂÖßÂÆπÈÉΩË®òÂú®ËÖ¶Ë¢ãË£°ËÄÅÂ∏´Ê†πÊú¨Êäì‰∏ç Âà∞ Classmate: How do you usually cheat? The midterm is tomorrow. Me: secretly memorize all the contents of the textbook in my head, the teacher cant catch me at all. inversion Âè™Ë¶ÅÂ§´Â¶ª‰∏§‰∏™‰∫∫‰∫íÁõ∏‰ø°‰ªªÂõõ‰∏™‰∫∫Â∞±ËÉΩÁõ∏ÂÆâÊó† ‰∫ã As long as the husband and wife trust each other, four people can get along in peace. inversion, wordplay ‰ª•ÂâçÊàëËÄÅÂ©ÜÂ∞çÊàëÁúüÁöÑË∂ÖÂÖáÁöÑÂæå‰æÜÊàëÂ∞±ËÆì‰ªñÂéª Â≠∏Á©∫ÊâãÈÅìË∑üÂäçÈÅìËá≥Â∞ëÁèæÂú®‰ªñÊâìÊàë‰πãÂâçÊúÉÂÖà Ë∑üÊàëÈû†Ë∫¨ In the past, my wife was really super mean to me. Later, let her go learn karate and kendo. At least now, before she hits me, she will bow to me first. inversion, switchbait È´òÈÄüÂÖ¨Ë∑ØÊóÅÁöÑË≠¶ËØ≠ÂÜôÁùÄÂºÄËΩ¶ËØ∑ÁúãÂâçÊñπ The warning sign by the highway reads: Please keep your eyes on the road while driving. inversion, paradox Â•≥Â≠©‰ªé‰∏ç‰ºöÂú®ÊÑè‰Ω†ÂºÄ‰ªÄ‰πàÈ¢úËâ≤ÁöÑÊ≥ïÊãâÂà© girl will never care what color Ferrari you drive. misdirection, inversion, wordplay ÁßÅ„ÅÆÈï∑ÊâÄ„ÅØÁ¥†Áõ¥„Å´ÈñìÈÅï„ÅÑ„ÇíË™ç„ÇÅ„Çã„Åì„Å®„Åß„Åô Áü≠ÊâÄ„ÅØÊ±∫„Åó„Å¶ÈñìÈÅï„ÅÑ„ÇíÊîπ„ÇÅ„Å™„ÅÑ„Åì„Å®„Åß„Åô My strength is that can honestly admit my mistakes. My weakness is that never correct my mistakes. paradox „ÅäÂÆ¢Êßò„ÅÆ„Åä„Åã„Åí„ÅßÂøçËÄêÂäõ„Åå„Ç¢„ÉÉ„Éó„Åó„Å¶„Åç„Åæ„Åó „Åü Thanks to the customer, my patience has improved. inversion, wordplay ·Ñå·Ö¶·ÑÄ·Ö°·ÑÜ·Ö©·Ü∫·Ñí·Ö°·ÑÇ·Ö≥·Ü´·ÑÄ·Ö•·Ü∫·Ñà·Ö¢·ÑÄ·Ö©·ÑÇ·Ö≥·Ü´·ÑÉ·Ö°·Ñå·Ö°·ÜØ·Ñí·Ö¢·Ñã·Ö≠ Im good at everything except what cant do. A: ·ÑÉ·Ö©·ÜØ·Ñå·Ö°·Ü´·Ñé·Öµ·ÑÄ·Öß·ÜØ·Ñí·Ö©·Ü´·Ñå·Ö°·Üº·ÑÖ·Ö®·Ñâ·Öµ·Ü®·ÑÉ·Ö≥·Üº·ÑÉ·Ö≥·Üº·Ñí·Ö°·Ü´·ÑÉ·Ö°·ÜØ·Ñå·Ö•·Ü´·Ñã·Ö¶·Ñã·Ö§·ÑÄ·Öµ·Ñí·Ö°·Ñâ·Ö¶·Ü∑·Ñã·Öß·Ü´ ·Ñé·Ö°·Ñã·Ö©·ÜØ·ÑÖ·Öß·Ñã·Ö£·Ñí·Ö°·ÑÇ·ÖµB: ·Ñí·Ö°·Ü´·ÑÉ·Ö°·ÜØ·Ñå·Ö•·Ü´·Ñã·Ö≥·Ü´·Ñà·Ö°·Ü®·Ñâ·Ö¶·ÑÇ·Ö¶·Ñå·Ö°·Üº·ÑÖ·Ö®·Ñâ·Öµ·Ü®·Ñí·Ö°·Ü´·ÑÉ·Ö°·ÜØ·Ñå·Ö•·Ü´ ·Ñã·Ö®·ÑÄ·Ö©·ÑÜ·Öß·Ü´·Ñâ·Ö°·ÜØ·Ñã·Öµ·Ü´·Ñã·Ö°·ÑÇ·Öµ·Ü∑? ·Ñã·Öß·Ñå·Ö°·Ñé·Öµ·Ü´·ÑÄ·ÖÆ·ÑÇ·Ö°·Ñâ·Ö°·ÜØ·Ñç·Öµ·Ü´·ÑÄ·Ö•·ÑÄ·Ö°·áÄ·Ñã·Ö°·ÑÇ·Ö°·Ü∑·Ñå·Ö°·Ñé·Öµ·Ü´·ÑÄ·ÖÆ·ÑÇ·Ö•·Ü´·Ñâ·Ö°·ÜØ·Ñã·Öµ·ÑÜ·ÖÆ·Ü´·Ñå·Ö¶ ·ÑÄ·Ö°·Ñã·Ö°·ÑÇ·Öµ·Ñã·Ö£ ·Ñå·Öµ·Ü∏·Ñã·Ö¶·Ñá·ÖÆ·ÜØ·Ñã·Öµ·ÑÇ·Ö°·Üª·ÑÉ·Ö°. ·Ñã·Ö©·Ü´·ÑÄ·Ö°·Ñå·Ö©·Ü®·Ñã·Öµ·ÑÉ·Ö°·Üº·Ñí·Ö™·Üº·Ñí·Ö¢·Ñâ·Ö•·Ñâ·Ö©·ÑÖ·Öµ·Ñé·Öµ·Ü´·ÑÉ·Ö°. ·Ñã·Ö° ·Ñá·Ö•·Ñå·Öµ: \"·Ñã·Ö£, 119·ÑÄ·Ö°·ÑÜ·Öß·Üæ·Ñá·Ö•·Ü´·Ñã·Öµ·Ñã·Ö£? 119·ÑÄ·Ö°·ÑÜ·Öß·Üæ·Ñá·Ö•·Ü´·Ñã·Öµ·ÑÇ·Ö£·ÑÄ·Ö©!!! \" ·Ñã·Ö°·ÑÉ·Ö≥·ÜØ: \"·Ñã·Ö°·Ñá·Ö•·Ñå·Öµ, ·Ñã·Öµ·ÑÖ·Ö•·ÜØ·ÑÑ·Ö¢·Ñã·Öµ·ÜØ·Ñâ·ÖÆ·ÑÖ·Ö©·Ü®·Ñé·Öµ·Ü∑·Ñé·Ö°·Ü®·Ñí·Ö°·Ñâ·Öß·Ñã·Ö£·ÑÉ·Ö´·Ñã·Ö≠. ·Ñå·Ö¶·ÑÄ·Ö°114·Ñã·Ö¶·Ñå·Ö•·Ü´·Ñí·Ö™·Ñí·Ö¢·Ñâ·Ö•·ÑÜ·ÖÆ·ÜØ·Ñã·Ö•·Ñá·Ö©·ÜØ·ÑÄ·Ö¶·Ñã·Ö≠ \" A: Let me know month in advance for events like funerals. B: months notice for funeral? Thats premeditated murder! paradox paradox \"Girlfriend: Do you think gained weight? Boyfriend: Your problem isnt your weight.\" inversion The house caught fire. The whole family was panicking and shouting. Father: \"Hey, whats the number for 119? Whats the number for 119!!!\" Son: \"Dad, you need to stay calm in situations like this. Ill call 114 and ask.\" misdirection, switchbait Table 3: Representative examples of Drivelology. The DRIVELHUB dataset is available on the HuggingFace Hub at https://huggingface.co/datasets/extraordinarylab/drivel-hub. Figure 5: UpSet plot (Lex et al., 2014) illustrating the overlap and intersection sizes among Drivelology categories. Each vertical bar represents the number of samples belonging to specific combination of categories, as indicated by the connected black dots below. Categories include Misdirection, Paradox, Switchbait, Inversion, and Wordplay. Language Drivelology Non-Drivelology Total Mandarin English Spanish French Korean Japanese Total 277 93 69 62 52 47 194 75 68 80 92 91 600 471 168 137 142 144 138 1200 Table 4: Language distribution of Drivelology and nonDrivelology samples in the DRIVELHUB. Mandarin includes Simplified Chinese and Traditional Chinese. C.1 Findings Our investigation into GPT-4s generative capabilities reveals significant gap between mimicking linguistic forms and achieving genuine pragmatic depth. The models performance varied with the level of guidance, but it consistently struggled to capture the essential qualities of Drivelology. Minimal Guidance. When prompted with only brief description of Drivelology, GPT-4 relied heavily on surface-level cues such as paradoxical, unexpected twist, or nonsensical. The resulting outputs were typically simple, declarative statements containing superficial contradictions (e.g., Hes an honest liar or bought one-way ticket with unlimited uses). These examples mimicked the form of Drivelology but lacked the semantic depth, layered meaning, and interpretive tension that define the genre. With Category Definitions. Providing explicit category definitions led to more complex outputs, including richer character interactions, emotional cues, and linguistic characteristics like personification. For example: He says hes vegetarian, but only eats plants that scream like carrots that wail when pulled from the ground. While this sentence demonstrates greater creativity and engagement with paradox and wordplay, it still falls short of Drivelologys essential qualities. The supposed contradiction is not inherently ironic, and the interpretive tension remains weak. Additionally, increasing the number of examples (five-shot) did not improve output quality. Instead, it often exposed deeper structural and semantic issues. Some outputs suffered from syntactic misalignment (e.g., Its not that you cant love others, its that love cant you, which is ungrammatical and uninterpretable), while others exhibited shallow or logically incompatible contradictions (e.g., Its not that dont want to work hard, its that Ive worked so hard it looks like Im not trying). Across all settings, GPT-4 struggled to internalise the subtle requirements of Drivelology. Out of 20 generations prompted with examples, only one output achieved comprehensive alignment between topic, rhetorical category, and sentence structure. For example: ËøôÊú¨‰π¶Â§™Ê∑±Â••‰∫ÜÊàëËä±‰∫Ü ‰∏ÄÊï¥ÊôöÊ≤°ÁúãÊáÇÂ∞ÅÈù¢(This book is too profound, spent the whole night and still couldnt understand the cover). Although providing more examples led to slightly more complex narratives, the outputs consistently lacked Drivelologys hallmark features: contextual misdirection, interpretive layering, and rhetorical paradox. These shortcomings were especially pronounced in scenarios requiring cultural knowledge, emotional nuance, and inferential reasoning. Overall, our findings highlight the persistent challenges LLMs face in generating text that aligns with the deeper pragmatic and rhetorical demands of Drivelology. Prompt Model Size English Mandarin 4B 8B 14B 4B 8B 14B Narrative MCQA Classification BERT GPT Easy Hard Detect Tag 77.45 85.91 86.00 67.79 65.07 64.23 2.64 2.64 2.67 2.96 3.08 3.19 81.00 83.17 83. 77.61 81.11 82.56 6.00 26.78 46.66 2.44 19.11 51.69 66.80 65.00 73.57 62.86 78.81 77.62 43.21 38.04 45. 46.10 41.71 49.35 Table 5: Performance of Qwen3 models of varying sizes (4B, 8B, 14B) across different tasks."
        },
        {
            "title": "D Future Work",
            "content": "D.2 Developing Metrics for Drivelology Generation Our current study focuses primarily on the understanding and reasoning abilities of LLMs rather than their capacity for generation. significant area for future work is to establish comprehensive framework for evaluating generated Drivelology. key limitation to address is the absence of metrics capable of quantifying the qualities that make text Drivelological. Simply measuring fluency or grammatical correctness is insufficient. robust evaluation would require developing novel metrics to assess specific aspects of generated Drivelology text, such as: (1) Entertainability: How humorous, witty, or engaging is the output? (2) Relevance: Given specific topic or context, how relevant is the generated Drivelology? (3) Cohesion and Paradoxical Depth: How well does the output maintain surfacelevel coherence while simultaneously embedding meaningful, non-obvious contradiction or twist? Developing such framework is challenging but essential next step. It would not only enable more rigorous assessment of generative models but also provide clearer objectives for training models to produce high-quality, creative, and contextually aware Drivelology. While this work successfully introduces the DRIVELHUB dataset and benchmarks the limitations of current LLMs, the rich structure of our data opens up significant avenues for future research. We outline two key directions: advancing model training methodologies and developing robust framework for evaluating Drivelology generation. D.1 Advancing Model Training with the"
        },
        {
            "title": "MCQA Task",
            "content": "We have identified that the Narrative Selection (MCQA) task within our dataset is perfect fit for GRPO (Shao et al., 2024). GRPO is an advanced preference optimisation technique that allows model to learn from the relative preferences within group of candidate responses, rather than relying on simple pairwise (Rafailov et al., 2023) or scalar rewards (Schulman et al., 2017). By learning from full ranking of multiple candidates, the model receives much richer training signal. The design of our MCQA task naturally lends itself to this paradigm. For each Drivelology sample, we provide one correct narrative and several carefully crafted distractors. This setup creates an inherent group-wise ranking (i.e., the correct option is preferred over all incorrect options), which can be directly leveraged by GRPO. Future work should explore fine-tuning LLMs using GRPO on the DRIVELHUB MCQA data. We hypothesise that this approach could substantially improve models ability to discern subtle semantic and pragmatic distinctions, thereby enhancing its capacity for the deep, non-linear reasoning required to truly comprehend Drivelology. This would represent significant step toward closing the gap between statistical fluency and genuine cognitive understanding that our current work highlights. Human Guidelines: # Annotation Guidelines for Drivelology Dataset ## Introduction These guidelines are designed to assist annotators in accurately labelling samples for the Drivelology dataset. Annotators should familiarise themselves with the definitions and characteristics of Drivelology and non-Drivelology texts before proceeding. ## Definitions Drivelology: Description: Statements that appear logically coherent but contain deeper, often paradoxical meanings. These challenge conventional interpretation by blending surface-level nonsense with underlying depth, often incorporating elements of humour, irony, or sarcasm. Understanding Drivelology requires contextual insight and emotional interpretation. Examples: * bought book on how to solve 50% of my problems, so bought two books. * Loving someone cannot be hidden, but loving two people must be hidden. non-Drivelology: Description: This includes pure nonsense (grammatically correct but semantically meaningless statements) and normal sentences, including quotes or proverbs, that convey clear or straightforward information without the layered complexity characteristic of Drivelology. Examples: * The cat sat on the mat. (normal sentence) * Colourless green ideas sleep furiously. (pure nonsense) ## Annotation Tasks Drivelology Tagging Task: Classify Drivelology samples into one or more categories only if the sample is Drivelology: * Misdirection: rhetorical technique where the focus shifts but connects back to the original topic through indirect hints. * Paradox: statement that combines ideas that do not logically fit together but conveys deeper meaning. * Switchbait: language trick that changes meaning based on cultural knowledge or idioms. * Inversion: Rearranging the usual order of words or ideas to create surprising effect. * Wordplay: Creative use of language through puns or double meanings. Instructions: * Identify the primary characteristics (i.e., the first strong impression) of the text. * Assign one or more categories based on the definitions above. Implicit Narrative Writing Task: Generate detailed description illustrating the implicit narrative of the Drivelology text. Instructions: * Analyse the text to uncover underlying themes, messages, or emotional undertones. * Write narrative that reflects the deeper significance of the text, going beyond surfacelevel summary. * Generate four contextualised, plausible, but ultimately incorrect narrative, wrong understanding of the given Drivelology text, each within three sentences as distractors. Keep the length and style the same as the correct narrative, and keep these negative narratives difficult to tell from the positive narrative. ## Quality Assurance Each annotation will undergo review process where meta-reviewer will assess the annotations for consistency and accuracy. Annotators should mark down any samples that exhibit ambiguities or uncertainties during the annotation process. The meta-reviewer will review these marked samples and finalise the answer based on thorough evaluation. Figure 6: Guidelines for human annotators. Prompt1: Instruction: Classify whether the given text is Drivelology sample or not. Definition: - Drivelology: Statements that appear logically coherent but contain deeper, often paradoxical meanings. These challenge conventional interpretation by blending surface-level nonsense with underlying depth, often incorporating elements of humor, irony, or sarcasm, and requiring contextual understanding and emotional insight to unravel their true significance. - non-Drivelology: This includes pure nonsense (grammatically correct but semantically meaningless statements, such as \"boys will be boys\") and normal sentences, including quotes or proverbs, that convey clear or straightforward information without the layered complexity characteristic of Drivelology. Input Text: {text} Output Format: Please provide the output in JSON format with the following keys: - answer: Specify whether the text is \"Drivelology\" or \"non-Drivelology.\" - reason: Provide clear explanation of why the text is classified as Drivelology or not. Prompt2: Instruction: Classify whether the given text is Drivelology sample or not. Definitions: - Drivelology: Statements that appear logically coherent but contain deeper, often paradoxical meanings. These challenge conventional interpretation by blending surface-level nonsense with underlying depth, often incorporating elements of humor, irony, or sarcasm, and requiring contextual understanding and emotional insight to unravel their true significance. - non-Drivelology: This includes pure nonsense (grammatically correct but semantically meaningless statements) and normal sentences, including quotes or proverbs, that convey clear or straightforward information without the layered complexity characteristic of Drivelology. Input Text: {text} Instructions for Reasoning: Analyse the input text by comparing it to the definitions above. Identify whether it contains logical coherence, paradox, layered meaning, or requires emotional/contextual insight. If uncertain, select the category that best fits and explain your reasoning. Output Format: Please provide the output in JSON format with the following keys: - answer: Specify \"Drivelology\" or \"non-Drivelology.\" - reason: Clearly explain why the text was classified as such, referencing specific features from the definitions. Prompt3: Classify the text as \"Drivelology\" or \"non-Drivelology.\" Definitions: - Drivelology: Logically coherent statements with paradox, layered or hidden meaning, often using humor, irony, or sarcasm. These require emotional or contextual insight to interpret. - non-Drivelology: Pure nonsense or straightforward statements without hidden complexity. Text: {text} Reasoning: Decide based on the definitions above. If uncertain, choose the closest fit and briefly explain. Output (JSON only): { \"answer\": \"Drivelology\", \"reason\": \"The text contains underlying meaning, fitting the Drivelology definition.\" } Figure 7: Prompts for Drivelology Detection task. Prompt1: Instruction: Classify the given text into one or more of the following categories: inversion, wordplay, switchbait, paradox, and misdirection. Definitions: - inversion: INVERSION DEFINITION. - wordplay: WORDPLAY DEFINITION. - switchbait: WITCHBAIT DEFINITION. - paradox: PARADOX DEFINITION. - misdirection: MISDIRECTION DEFINITION. Input Text: {text} Output Format: Please provide the output in JSON format with the following keys: - answer: List the applicable comma-separated categories for the text (e.g., \"category1, category2\"). - reason: Provide clear explanation for why the text is classified into each category. Prompt2: Instruction: Analyse the input text and classify it into one or more of the following categories: inversion, wordplay, switchbait, paradox, and misdirection. Use the definitions below to guide your classification. Definitions: - inversion: INVERSION DEFINITION. - wordplay: WORDPLAY DEFINITION. - switchbait: WITCHBAIT DEFINITION. - paradox: PARADOX DEFINITION. - misdirection: MISDIRECTION DEFINITION. Input Text: {text} Output Format (JSON): { \"answer\": \"category1, category2, ...\", \"reason\": \"Explain why the text fits each chosen category based on the definitions.\" } Prompt3: Instruction: Examine the input text and determine which of the following categories it belongs to: inversion, wordplay, switchbait, paradox, and misdirection. Base your classification strictly on the definitions provided below. Definitions: - inversion: INVERSION DEFINITION. - wordplay: WORDPLAY DEFINITION. - switchbait: WITCHBAIT DEFINITION. - paradox: PARADOX DEFINITION. - misdirection: MISDIRECTION DEFINITION. Input Text: {text} Please provide the output in JSON format: { \"answer\": \"category1, category2, ...\", \"reason\": \"Briefly explain how the text fits each selected category, using the definitions as basis.\" } Figure 8: Prompts for Drivelology Tagging task. Prompt1: You need to first read and understand the text given. Generate detailed description to illustrate the implicit narrative of the text. Text: {text} Output format should be JSON with the following keys: - narrative: The narrative of the text in English. Prompt2: Read and understand the provided text carefully. Task: Generate detailed description that illustrates the implicit narrative of the text. Input Text: {text} Output Format: { \"narrative\": \"The narrative of the text.\" } Please ensure the output is in JSON format and contains the key \"narrative\" with the developed description of the implicit narrative derived from the input text. Prompt3: Read and understand the provided text carefully. Task: Generate detailed description that illustrates the implicit narrative of the text. By \"implicit narrative,\" we mean the underlying message, theme, perspective, or emotional undertone that is not directly stated but can be inferred from the text. Your description should go beyond surface-level summary and provide insights into the texts underlying themes, perspectives, or intentions. Input Text: text Output Format: Output only the JSON object, with no extra commentary or explanation. { \"narrative\": \"A detailed description of the implicit narrative, including the underlying theme, emotional tone, and implied perspective.\" } Figure 9: Prompts for Narrative Writing task. Prompt1: Tell me the best option in the following options which represents the underlying narrative of the text? Text: {text} A. {narrative_1} B. {narrative_2} C. {narrative_3} D. {narrative_4} E. {narrative_5} Output format should be JSON with the following keys: - answer: The option the text belongs to, and it should be uppercase among A, B, C, D, E. Prompt2: Tell me the best option from the list below that represents the underlying narrative of the text. By \"underlying narrative,\" we mean the main theme, implicit message, or perspective the text conveys. Text: {text} A. {narrative_1} B. {narrative_2} C. {narrative_3} D. {narrative_4} E. {narrative_5} If more than one option seems plausible, pick the one that best represents the main narrative. Output format should be JSON with the following keys: - answer: The option should be single uppercase letter among A, B, C, D, or E. Output only the JSON object, with no extra commentary. Prompt3: Tell me which option best represents the underlying narrative (main theme, message, or perspective) of the text. Text: {text} A. {narrative_1} B. {narrative_2} C. {narrative_3} D. {narrative_4} E. {narrative_5} If more than one fits, pick the best. Output only JSON: - answer: One uppercase letter: A, B, C, D, or E. Example: { \"answer\": \"B\" } Figure 10: Prompts for Easy Narrative Selection task. Prompt1: Tell me the best option in the following options which represents the underlying narrative of the text? Text: {text} A. {narrative_1} B. {narrative_2} C. {narrative_3} D. {narrative_4} E. None of the above. Output format should be JSON with the following keys: - answer: The option the text belongs to, and it should be uppercase among A, B, C, D, E. Prompt2: Tell me the best option from the list below that represents the underlying narrative of the text. By \"underlying narrative,\" we mean the main theme, implicit message, or perspective the text conveys. Text: {text} A. {narrative_1} B. {narrative_2} C. {narrative_3} D. {narrative_4} E. None of the above. If none of the options fully fit, select \"E. None of the above.\" If more than one option seems plausible, pick the one that best represents the main narrative. Output format should be JSON with the following keys: - answer: The option the text belongs to, and it should be single uppercase letter among A, B, C, D, or E. Output only the JSON object, with no extra commentary. Example output: { \"answer\": \"B\" } Prompt3: Tell me which option best represents the underlying narrative (main theme, message, or perspective) of the text. Text: {text} A. {narrative_1} B. {narrative_2} C. {narrative_3} D. {narrative_4} E. None of the above. If none fit, choose E. If more than one fits, pick the best. Output only JSON: - answer: One uppercase letter: A, B, C, D, or E. Example: { \"answer\": \"B\" } Figure 11: Prompts for Hard Narrative Selection task."
        }
    ],
    "affiliations": [
        "Durham University",
        "The University of Manchester",
        "The University of Sheffield"
    ]
}