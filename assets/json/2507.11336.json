{
    "paper_title": "UGC-VideoCaptioner: An Omni UGC Video Detail Caption Model and New Benchmarks",
    "authors": [
        "Peiran Wu",
        "Yunze Liu",
        "Zhengdong Zhu",
        "Enmin Zhou",
        "Shawn Shen"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Real-world user-generated videos, especially on platforms like TikTok, often feature rich and intertwined audio visual content. However, existing video captioning benchmarks and models remain predominantly visual centric, overlooking the crucial role of audio in conveying scene dynamics, speaker intent, and narrative context. This lack of omni datasets and lightweight, capable models hampers progress in fine grained, multimodal video understanding. To address these challenges, we introduce UGC-VideoCap, a new benchmark and model framework specifically designed for detailed omnimodal captioning of short form user-generated videos. Unlike prior datasets, UGC-VideoCap emphasizes balanced integration of audio and visual modalities, featuring 1000 TikTok videos annotated through a structured three stage human-in-the-loop pipeline covering audio only, visual only, and joint audio visual semantics. The benchmark also includes 4000 carefully crafted QA pairs probing both unimodal and cross modal understanding. Alongside the dataset, we propose UGC-VideoCaptioner(3B), a 3B parameter captioning model distilled from Gemini 2.5 Flash. Using a novel two-stage training strategy supervised fine tuning followed by Group Relative Policy Optimization (GRPO), our approach enables efficient adaptation from limited data while maintaining competitive performance. Together, our benchmark and model offer a high-quality foundation and a data-efficient solution for advancing omnimodal video captioning in unconstrained real-world UGC settings."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 5 1 ] . [ 1 6 3 3 1 1 . 7 0 5 2 : r 2025-7-16 UGC-VideoCaptioner: An Omni UGC Video Detail Caption Model and New Benchmarks Peiran Wu1,2, Yunze Liu2, Zhengdong Zhu2, Enmin Zhou2, Shawn Shen1,2 1University of Bristol Work done during an internship at Memories.ai Research Corresponding Author 2Memories.ai Research Project Web Evaluation Code UGC-VideoCap Real-world user-generated videos, especially on platforms like TikTok, often feature rich and intertwined audio-visual content. However, existing video captioning benchmarks and models remain predominantly visual-centric, overlooking the crucial role of audio in conveying scene dynamics, speaker intent, and narrative context. This lack of full-modality datasets and lightweight, capable models hampers progress in fine-grained, multimodal video understanding. To address these challenges, we introduce UGCVideoCap, new benchmark and model framework specifically designed for detailed, omnimodal captioning of short-form, user-generated videos. Unlike prior datasets, UGC-VideoCap emphasizes balanced integration of audio and visual modalities, featuring 1,000 TikTok videos annotated through structured three-stage human-in-the-loop pipeline covering audio-only, visual-only, and joint audio-visual semantics. The benchmark also includes 4,000 carefully crafted QA pairs probing both unimodal and cross-modal understanding. Alongside the dataset, we propose UGC-VideoCaptioner-3B, 3B-parameter captioning model distilled from Gemini-2.5 Flash. Using novel two-stage training strategysupervised fine-tuning followed by Group Relative Policy Optimization (GRPO)our approach enables efficient adaptation from limited data while maintaining competitive performance. Together, our benchmark and model offer high-quality foundation and data-efficient solution for advancing omnimodal video captioning in unconstrained, real-world UGC settings. 1. Introduction Generating detailed video captions has long been key goal in video-language research. Traditional efforts have predominantly focused on describing visual contentsuch as actions, objects, scenes, and temporal dynamicswithin videos (Wang et al., 2022; Xu et al., 2023; Yan et al., 2022), often overlooking the complementary role of audio. This gap is especially pronounced in real-world video domains such as User-Generated Content (UGC) on TikTok and YouTube, and cinematic content where rich audio-visual interplay is central to semantic comprehension. In such settings, audio signalsincluding speech, music, ambient noise, and sound effects carry critical information that is not visually inferable. Prior to the advent of Large Language Models (LLMs), earlier approaches were constrained by limited model capacity and weak generalization. Even when trained on large-scale datasets like HowTo100M (Miech et al., 2019) or VideoCC3M (Nagrani et al., 2022), models produced only shallow or incomplete descriptions. Audio-based models, such as WaveNet (Oord et al., 2016) and Whisper (Radford et al., 2023), remained unimodal, lacking the ability to reason jointly over audio and visual signals. As result, most existing benchmarks and models are inherently visual-centric and fail to reflect the full multimodal nature of real-world video content. Recent advances in multimodal LLMs (MLLMs), such as LLaVA-OneVision (Li et al., 2024), Qwen2.5VL (Bai et al., 2025), and InternVL3 (Zhu et al., 2025), along with post-training strategies like supervised fine-tuning (SFT) and reinforcement learning with human feedback (RLHF), have signifiCorrespondence to: yunze.liu@memories.ai 2025 Memories.ai Research. All rights reserved UGC-VideoCaptioner: An Omni UGC Video Detail Caption Model and New Benchmarks cantly improved visual understanding capabilities. These models have shown strong performance in captioning (AuroraCap), inference (VideoCap-R1 (Meng et al., 2025), ST-R1 (Wu et al., 2025)), and domain-specific applications (FMBench (Wu et al., 2024), MedVLM-R1 (Pan et al., 2025)). However, their reliance on visual-only input leaves major gap: few models genuinely support full omnimodal input. Among open-source systems, only Qwen2.5-Omni (Xu et al., 2025), MiniCPM-o-2_6 (Yao et al., 2024), and Gemma3n (google deepmind, 2025) enable joint audio-visual understanding, while closed-source models like the Gemini series from Google also demonstrate such capability. This highlights fundamental deficiency in current benchmarks and training regimesthere is lack of large-scale, high-quality omnimodal annotation for real-world videos, especially for short-form UGC content. To address this deficiency, we introduce UGC-VideoCap, the first large-scale benchmarks explicitly designed for detailed captioning and QA over short-form, audio-visual UGC videos. Our key contributions include: We present an omnimodal benchmark built on 1,000 short TikTok videos (under 1 minute), each enriched with diverse and prominent audio signals. rigorous three-stage human annotation pipeline is applied: (i) audio-only annotation (e.g., number of speakers, voice type, background music, sound effects), with corresponding audio detail captions; (ii) visual-only annotation (e.g., OCR text, background transitions, motion dynamics, object types), with corresponding visual captions; and (iii) audio-visual joint annotation, yielding coherent and semantically rich omnimodal caption. We construct fine-grained QA benchmark with over 4,000 manually curated open-ended and multiple-choice questions that comprehensively probe both visual and auditory aspects of understanding, providing challenging and diagnostic evaluation protocol. We propose new captioning model, UGC-VideoCaptioner (3B), trained using novel twostage distillation framework. In Stage 1, we utilize Gemini-2.5-Flash to auto-label 20,000 TikTok videos with detailed captions. In Stage 2, we fine-tune using only 2,000 human-aligned captions via SFT, followed by lightweight RLHF. This process achieves performance comparable to full 20k SFT models, demonstrating that high-quality model adaptation can be achieved with significantly reduced human supervision. We hope our benchmark and training framework will serve as catalyst for future research in omni video understanding, particularly in real-world domains like UGC and cinema, where rich multimodal cues and fine-grained semantics are essential. The rarity and cost of acquiring fully-aligned omnimodal annotations make this resource especially valuable for benchmarking and training next-generation multimodal LLMs. 2. Related Work Multimodal Large Language Model for Video. Video understanding is critical area within the broader field of vision-language research, aiming to empower models with the ability to interpret and reason about the dynamic visual content presented in videos (Cheng et al., 2024; Shen et al., 2024; Wu et al., 2025). Traditionally, visual content has been parsed through feature extraction using visual encoders (Liu and Yi, 2025; Liu et al., 2025b). However, with the rapid advancement of large language models (LLMs), there has been growing trend toward the development of Multimodal Large Language Models (MLLMs) to support more comprehensive video understanding. In recent years, major technology companies such as Google, OpenAI, Meta, and Alibaba have made significant progress in this domain, continuously updating their MLLMsexamples include Gemini 2 UGC-VideoCaptioner: An Omni UGC Video Detail Caption Model and New Benchmarks Benchmark Theme # Videos # QA pairs Audio in Caption Visual in Caption VATEX (Wang et al., 2019) Human DREAM-1K (Wang et al., 2024) MSR-VTT (Xu et al., 2016) VDC (Chai et al., 2024) VidCapBench (Chen et al., 2025) UGC-VideoCap (Ours) Open Open Open UGC UGC 4,478 1,000 2,990 1, 643 1,000 4,478 6,298 2,990 96, 10,644 3,975 Table 1 Benchmark comparison for video caption evaluation. UGC-VideoCap has comprehensive and detailed caption evaluation content. Compared with the past video caption benchmarks that mainly focus on the visual part in the annotation process, we propose an omnimodal video caption benchmark. 2.5 (Gemini Team, 2025), GPT-4o (Hurst et al., 2024), and Qwen2.5-VL (Bai et al., 2025). These models demonstrate impressive capabilities in processing and reasoning over visual data. Nevertheless, it is widely acknowledged that visual content alone does not capture the full semantic richness of videos, particularly in platforms such as TikTok or Movie, where audio plays an equally crucial role. Therefore, we argue that models relying solely on visual inputs are inherently limited in their ability to perform holistic video understanding. Video Caption Model and Benchmark. The goal of video captioning is to describe the video in terms of several key aspects, thus aiding understanding (Doveh et al., 2023), retrieval (Ma et al., 2025) and reasoning (Wu et al., 2025). As shown in Table 1, most of the previous video caption benchmarks focus mainly on the visual information in the video, e.g., MSRVTT (Xu et al., 2016), VDC (Chai et al., 2024) and VidCapBench (Chen et al., 2025). However, it is well known that there is not only visual information in the video, but also audio information which is crucial for video understanding, especially for UGC, Movie and other types of videos. Often good audio caption can play key role in the video understanding task. We can see that there are many excellent omni models such as Gemini (Gemini Team, 2025), Qwen2.5-omni (Xu et al., 2025), etc. (Liu et al., 2025a; Zhang et al., 2023), but comprehensive omni video detail caption benchmark and small-parameter models are still lacking. 3. UGC Video Detail Caption Benchmark 3.1. Overview We present UGC-VideoCap, comprehensive benchmark designed to quantitatively evaluate the capability of multimodal large language models (MLLMs) in generating detailed video captions, particularly for short-form videos containing both visual and audio information. UGC-VideoCap includes around 4,000 high-quality question-answer (QA) pairs derived from 1,000 TikTok videos, each with rich vocal tracks and diverse content. As illustrated in Figure 1, the QA pairs are categorized into three distinct types: audio-focused, visual-focused, and comprehensive caption. In addition to detailed caption annotations, we provide fine-grained attribute labels, including phonetic features (e.g., number of speakers, gender), acoustic conditions (e.g., background music presence and type), and visual properties (e.g., OCR content, object types, background changes). Our evaluation framework is twofold: (1) single-aspect QA evaluation and (2) holistic caption generation. robust omnimodal model should perform well in both settings, whereas detail-oriented captioning model may only need to generate accurate final captions. We provided 15-20 audio/visual annotations as raw information, but we only selected some of them that could reflect the characteristics of UGC videos to construct the UGC-VideoCap benchmark. 3 UGC-VideoCaptioner: An Omni UGC Video Detail Caption Model and New Benchmarks Figure 1 Benchmark Collation Pipeline. This pipeline unifies disparate raw video annotation data into standardised format for consistent processing. QA pairs are then generated from question templates with fixed rules. To ensure quality, manual validation is performed at all key stages, with multiple people cycling through low quality and ambiguous annotated content to assess whether to re-labelling. Figure 2 Demonstration of tasks for UGC-VideoCap benchmark. It has many meta information for audio and visual. And we select some of them to be our benchmark QA pairs which are visual detail description, audio track detail description and final detail caption. 3.2. Benchmark Construction Data Collection and Standardization. We initiate benchmark construction by curating diverse set of 1,000 short TikTok videos (each under 60 seconds) that contain meaningful audio segments of at least 5 seconds in length. All videos are manually annotated and subsequently standardized into unified metadata schema to support structured QA generation. This ensures consistency in annotation quality and enables flexible benchmark expansion in future iterations. Question-Answer Generation. The entire data processing and annotation pipeline required over 350 hours of human effort, from initial filtering to final QA generation. We designed structured QA generation protocol tailored to assess diverse dimensions of video understanding. As shown in Figure 2, we generate three categories of QA pairs: 4 UGC-VideoCaptioner: An Omni UGC Video Detail Caption Model and New Benchmarks u oic Audio Detail o u k s bje Visual Detail . O Rank Avg. 2 1 6 7 3 5 4 64.3 67.4 38.1 32.6 50.9 48.6 48. 71.5 78.2 65.6 37.2 86.6 93.0 80.9 62.6 64.1 46.2 28.7 44.9 46.9 29.0 0.693 0.694 0.124 0.095 0.644 0.561 0. 51.3 53.6 4.8 9.3 21.6 18.4 26.1 38.7 43.9 15.9 22.3 15.7 18.3 9.9 io 70.8 74. 54.2 52.0 57.6 48.2 46.0 t al Vi Final Caption 75.8 78.8 54.6 52.4 59.4 55.6 70.4 74.8 77. 51.2 49.6 57.0 52.6 62.4 r 73.78 76.73 53.3 51.3 58.0 52.2 59.6 Methods Proprietary Models (API) Gemini-2.5-pro Gemini-2.5 Flash Open-source Models Gemma-3n-E4B-it Gemma-3n-E2B-it Qwen2.5-Omni-7B Qwen2.5-Omni-3B MiniCPM-o-2.6-8B Table 2 Evaluation on UGC-VideoCap Benchmark. We provided detailed evaluation of the audio and visual, and finally designed detailed caption with audio and video. Regarding the OCR score we are considering the average of the 4 NLP metrics (BLEU-1, ROUGE-1, ROUGE-2, and ROUGE-L), scaled to percentage for fair comparison in the average calculation. Visual QA: Focused on scene dynamics, object presence, OCR content, and background changes. Audio QA: Encompassing speaker attributes (e.g., count, gender), acoustic features (e.g., music genre), and environmental sounds. Comprehensive QA: Final captioning questions based on both visual and audio modalities, representing complete semantic understanding of the video. Each video is annotated with visual-only caption, an audio-only caption, and final omnimodal caption. These annotations form the basis for both QA and generation tasks. We believe UGC-VideoCap sets new standard for evaluating omnimodal video understanding. It is among the first benchmarks to emphasize the integral role of audio in video captioning, especially in real-world UGC settings. We posit that full-modality video understanding will become critical frontier in the next phase of multimodal AI research. Human-in-the-Loop Annotation Verification. To ensure high annotation fidelity, we adopt rigorous human-in-the-loop quality assurance protocol. Every batch of 50 annotated video samples is independently reviewed by two expert annotators. Reviewers assess the accuracy, completeness, and clarity of all annotation componentsincluding audio attributes, visual descriptors, and full audiovisual captions. Errors are defined as factual inaccuracies, omissions of salient content, or inconsistencies with the video. If the error rate in batch exceeds 3%, the batch is rejected and returned for re-annotation. Discrepancies between annotators are resolved through arbitration or consensus-based review. This dual-judge protocol ensures consistency and minimizes subjectivity, while establishing feedback loop between annotators and reviewers. Such systematic process is critical for maintaining benchmark integrity and provides high-quality supervision for downstream evaluation and training tasks. 3.3. Evaluation on UGC-VideoCap Benchmark Evaluation Setting & Benchmark Models. When evaluating the performance of each model in Benchmark, we consider the most influential omnimodal models in open source and commercial APIs, including Gemini, Gemma3n, Qwen2.5-Omni and Minicpm_o. All models are reasoned with the same set of prompts, and video samples are processed at 1fps (max 32 frames). In the open source models, all our models are run under single H200. In order to ensure the fairness of all model tests, all models are evaluated using uniform prompt. And the audio & video detailed caption prompt as 5 UGC-VideoCaptioner: An Omni UGC Video Detail Caption Model and New Benchmarks follow (for the rest of the questions, please refer to appendix): <prompt>You are given short video with both audio and visual content. Write detailed and coherent paragraph that naturally integrates all modalities. Your description should include: (1) the primary scene and background setting; (2) key characters or objects and their actions or interactions; (3) significant audio cues such as voices, background music, sound effects, and their emotional tone; (4) any on-screen text (OCR) and its role in the video context; and (5) the overall theme or purpose of the video. Ensure the output is fluent and objective paragraph, not bullet-point list, and captures the videos content in human-like, narrative style.</prompt> Metric Design. Our UGC-VideoCap benchmark includes two types of questions: Open-ended and Multiple-Choice (see Figure 2). For Multiple-Choice Questions (MCQs), we follow the standard evaluation protocol and use Accuracy (ACC)based on exact answer matchingas the primary metric. For Open-ended Questions, we employ GPT-4o-2024-08-06 (ensuring that neither the video nor the associated text content appears in its training set) as an automatic judge. The exact prompts used for each evaluation are provided in the Appendix. Finally, for OCR tasks within the visual-detail category, we report the average of BLEU and ROUGE scores (Lin, 2004; Papineni et al., 2002). Benchmark Results. As shown in Table 2, we conduct comprehensive evaluation of full-modal models on the UGC-VideoCap benchmark, where both audio and visual inputs are provided for every question. The Gemini family demonstrates superior performance across all metrics, with Gemini2.5 Flash achieving the highest overall score (76.73). In contrast, most open-source models show imbalanced performance between audio and visual tasks, especially on fine-grained visual details such as OCR, background transitions, and object recognition. UGC videos pose unique multimodal challenges due to their uncontrolled environments, diverse content styles, and frequent scene changes. These factors require models to handle non-studio speech, variable sound sources, noisy visual frames, and overlapping modalities. However, current models often treat audio and visual streams independently, leading to fragmented understanding. For example, Qwen2.5-Omni-3B achieves an excellent voice score (93.0) but struggles with visual object recognition (18.3). Interestingly, in final caption generation, most models (except the Gemma series) rely more heavily on visual cues than audio, suggesting limited audio integration. We believe future research should focus on enabling audio to assist visual understanding particularly in UGC settings where visual signals alone are often insufficient. Achieving balanced or audio-dominant performance could unlock better alignment with real-world multimodal understanding needs. 4. UGC Video Captioner 4.1. Distillation from Large Omni Model In recent years, visual multimodal large language models (MLLMs) have witnessed rapid development, with the research focus increasingly shifting from traditional large language models (LLMs) to their multimodal (Liu et al., 2023) counterparts, particularly in the domain of video understanding, where significant advancements have been achieved. However, real-world videos, especially user-generated content (UGC), typically encompass both visual and audio modalities. While the GPT series currently lacks native support for joint audio-visual processing, Gemini has emerged as the state-of-the-art model in the omni(audio+visual) domain. To address the scarcity of high-quality audio-visual captioning datasets, we adopt distillation-based approach using Gemini-2.5-Flash as the teacher model. We construct dataset comprising 20,000 TikTok videos with detailed annotations and use it to train our proposed model, UGC-VideoCaptioner, which is built upon Qwen2.5-Omni-3B, for comprehensive audio-visual video captioning. 6 UGC-VideoCaptioner: An Omni UGC Video Detail Caption Model and New Benchmarks Figure 3 Left: Distillation Stage. We use Gemini-2.5-flash to generate 20k omni video detail caption. Right: Reinforcement Learning Stage. We use GRPO algorithm and new caption reward model to enhance the detail caption ability. 4.2. Supervised Fine-Tuning for Cold Start In the absence of manually annotated data, we adopt supervised fine-tuning strategy via cold-start knowledge distillation, where smaller student model is trained to mimic the outputs of stronger teacher model. Let = {𝑥1, 𝑥2, . . . , 𝑥𝑁 } denote the set of input examples. For each input 𝑥𝑖, we obtain the teacher models response ˆ𝑦𝑖 = 𝑇 (𝑥𝑖), where 𝑇 denotes the teacher model. These responses act as pseudo-labels to supervise the training of the student model 𝑆. The student is optimized to maximize the likelihood of reproducing the teachers output under teacher-forcing training paradigm. The distillation loss for single training example is defined as: Ldistill(𝑥𝑖) = 𝑇 𝑡=1 log 𝑃𝑆 (cid:0)ˆ𝑦𝑖,𝑡 ˆ𝑦𝑖,<𝑡, 𝑥𝑖(cid:1) (1) where ˆ𝑦𝑖,𝑡 is the 𝑡-th token of the teachers generated output for input 𝑥𝑖, ˆ𝑦𝑖,<𝑡 denotes the sequence of preceding tokens before time step 𝑡, and 𝑃𝑆 is the conditional token probability distribution predicted by the student model. The overall objective is to minimize the average distillation loss over all input examples: Ltotal = 1 𝑁 𝑁 𝑖=1 Ldistill(𝑥𝑖) (2) This framework allows the student model to acquire domain-specific knowledge from powerful teacher without requiring any manually labeled data, making it particularly effective in cold-start scenarios where labeled supervision is unavailable. 4.3. Do we need GRPO for detail caption training? It is well known that the most direct and simple way we want to enhance small parametric model is to distil good models. With the recent proposal of RL post-training strategies for multimodal large language models (in particular, the emergence of Deepseeks GRPO (Guo et al., 2025)), it is beginning to be argued that the emergence of RL can allow for substantial improvement in model performance 7 UGC-VideoCaptioner: An Omni UGC Video Detail Caption Model and New Benchmarks through preference and reward through multiple sampling and reward function design. But without the involvement of data in the form of Chain of Thought, can GRPO further enhance the distilled model as third stage of post-training for non-thinking models? Our chapter will explore the GRPO post-training performance of detailed captions of omni videos with increasing amounts of data. Revisit Group Relative Policy Optimization. Group Relative Policy Optimization(GRPO) is recent policy optimization technique tailored for training large models (like language or multimodal models) with complex reasoning tasks (Shao et al., 2024). At its core, GRPO is similar in spirit to Proximal Policy Optimization (PPO) but introduces key differences in how the policy is updated and how the reward signal is used. Rather than relying on learned value function (critic) to estimate expected reward, GRPO entirely foregoes the critic model. Instead, it estimates the baseline or expected reward by sampling group of outputs from the current policy for each input and using their collective reward statistics. In other words, the model generates multiple candidate answers for given query or state and uses their relative rewards to decide how to adjust the policy. This group-based advantage estimation removes the need to train separate value network, simplifying the algorithm and reducing computational overhead. So, in the second stage of training, we apply the GRPO algorithm to fine-tune UGC-VideoCaptioner with reinforcement learning. At this stage UGC-VideoCaptioner has distilled certain amount of data through supervised finetuning, so the aim is to further improve its accuracy and detail captioning ability on omni caption task by optimizing directly for task-specific rewards. The objective of GRPO training is to adjust the models parameters to maximize the expected reward 𝔼[𝑅(𝑥, 𝑦)] over the distribution of video questions, using the group-based policy update scheme outlined earlier. UGCVideoCaptioners initial policy for this stage 𝜋𝜃init is set to the fine-tuned model from stage one. We then iteratively improve the policy using GRPO updates: in each iteration, for each training query 𝑥, multiple answers 𝑦1, . . . , 𝑦𝐾 are sampled and scored, and the policy is updated to prefer answers with higher scores, while maintaining closeness to 𝜋init. 𝐽 (𝜃) = 𝔼𝑥 (cid:20) 𝐾 𝑘=1 𝑤𝑘 log 𝜋𝜃 (cid:0) 𝑦𝑘 𝑥(cid:1) (cid:21) (cid:16) 𝛽 𝐷KL 𝜋𝜃( 𝑥) 𝜋ref ( 𝑥) (cid:17) (cid:13) (cid:13) (cid:13) (3) Here 𝑤𝑘 is the weight assigned to output 𝑦𝑘 after reward normalization (for example, 𝑤𝑘 could be exp(𝑟𝑘/𝜏) for some scaling temperature 𝜏), and the second term is KL divergence penalty. The KL (cid:205) 𝑗 exp(𝑟 𝑗/𝜏) term, with coefficient 𝛽, measures the divergence between the updated policy 𝜋𝜃 and reference policy 𝜋ref. Through these principles, GRPO provides stable and efficient way to fine-tune UGC-VideoCaptioner: it pushes the model toward higher-reward outputs while maintaining coherence with its initial learned behavior. Reward Function for Omni Video Detail Caption. To further enhance the quality of omni detail video caption, we introduce LLM-based omni reward and length-based reward to enhance the distillation quality and regulate the length of the models output. Specifically, this mechanism aims to enhanced the caption ability in visual, audio, detail and reduced hallucinations. So the LLM-based omni reward and length-based reward as follow. And in order to enhance the quality of distillation, we use Gemini-2.5-flash as our LLM judge model. The LLM-based omni reward: To assess the overall quality of predicted video description, we design reward function based on large language model (LLM) judgment. The model is prompted to compare the predicted and ground truth descriptions across the following five key dimensions: scene_background: the primary scene and background setting, 8 UGC-VideoCaptioner: An Omni UGC Video Detail Caption Model and New Benchmarks characters_objects: key characters or objects and their actions or interactions, audio_cues: voices, background music, sound effects, and their emotional tone, ocr_text: any on-screen text (OCR) and its contextual role, theme_purpose: the overall theme or purpose of the video. The LLM provides single integer score 𝑅LLM {1, 2, 3, 4, 5} reflecting how well the predicted caption captures these five aspects. The score is determined according to predefined criteria, with lower scores indicating hallucinations, omissions, or factual inconsistencies. Scoring rules: (1) If the predicted description includes hallucinated content that clearly contradicts the ground truth (e.g., non-existent objects, scenes, or sounds), the score must not exceed 2. (2) If the score appears between two levels, the lower score is always chosen to ensure conservative evaluation. The length-based reward: In the length control, in order to enhance the effect of distillation we here let the model learn the output length distribution of Gemini-2.5-flash under different samples by controlling the length, thus letting our small-parameter model learn as much as possible the full-modal detailcaption ability of Gemini-2.5-flash. 𝑅𝑙 = 0, 0.5, 1.0 if 𝑙𝑒𝑛(𝑐𝑜𝑚𝑝𝑙𝑒𝑡𝑖𝑜𝑛𝑠) < 0.5 𝑙𝑒𝑛(𝐺𝑇) if 0.7 𝑙𝑒𝑛(𝐺𝑇) > 𝑙𝑒𝑛(𝑐𝑜𝑚𝑝𝑙𝑒𝑡𝑖𝑜𝑛𝑠) > 0.5 𝑙𝑒𝑛(𝐺𝑇) 𝑙𝑒𝑛(𝑐𝑜𝑚𝑝𝑙𝑒𝑡𝑖𝑜𝑛𝑠) > 0.7 𝑙𝑒𝑛(𝐺𝑇) (4) Training Setting. We use Qwen2.5-Omni-3B as the baseline model. In the training phase we employed distillation technique using Gemini-2.5-Flash to annotate 20,000 of TikTok as training data for SFT and RL. In the training we used 1fps (max frames set to 32 frames, and the max pixels of each frame are 100176) and max prompt length is 8192. All experiments are conducted on 8xH200-144GB GPUs. Model Gemini-2.5-pro Gemini-2.5-flash Qwen2.5-Omni-3B Qwen2.5-Omni-3B Qwen2.5-Omni-3B Qwen2.5-Omni-3B UGC-VideoCaptioner-3B-zero UGC-VideoCaptioner-3B Training Data - - - 1k SFT 10k SFT 20k SFT 1k RL 1k SFT + 1k RL 𝐶𝑎𝑝𝑡𝑖𝑜𝑛 𝐴𝑢𝑑𝑖𝑜 70.8 𝐶𝑎𝑝𝑡𝑖𝑜𝑛 𝑉 𝑖𝑠𝑢𝑎𝑙 75. 𝐶𝑎𝑝𝑡𝑖𝑜𝑛 𝐷𝑒𝑡𝑎𝑖𝑙𝑠 74.8 Average 73.78 74.2 48.2 61.4 63.2 64.0 53.0 62.4 78.8 55.6 58.4 58.4 59.2 57.8 59.4 77.2 52.6 57.0 58.0 58.4 55.4 58. 76.73 52.18 58.96 (+6.78) 59.87 (+7.69) 60.50 (+8.32) 55.40 (+3.22) 60.01 (+7.83) Table 3 Performance comparison on audio-visual captioning. Gemini models serve as upper bounds. Reinforcement learning (RL) and supervised fine-tuning (SFT) on small datasets substantially improve caption quality. Training Results. Based on the above theoretical algorithm design and training setup, we conducted detailed experimental demonstration of the traditional sft distillation method and our two-stage distillation method under the 3B model. As shown in Table 3, we conducted supervised fine-tuning (SFT) with datasets of three different sizes: 1k, 10k, and 20k. The results indicate that the 3B model quickly approaches performance bottleneck under SFT. Specifically, doubling the data from 10k to 20k yields only marginal improvement of 0.63 points, suggesting diminishing returns and raising concerns about the efficiency of further data scaling for small models. To address this, we introduced new distillation strategy combining SFT with reinforcement learning (SFT+RL), alongside redesigned reward function. Remarkably, this approach, using only 1k samples for SFT and an additional 1k for RL, not only outperformed the 10k SFT baseline but also closely approached 9 UGC-VideoCaptioner: An Omni UGC Video Detail Caption Model and New Benchmarks the performance of the 20k SFT setting. While RL clearly enhances model performance, we further investigated whether it could replace SFT entirely. Experimental comparisons between 1k RL and 1k SFT show that SFT still provides stronger performance gain. This suggests that RL serves best as complementary stagerefining model that has already undergone supervised distillationrather than as standalone substitute. These findings highlight that, in the context of detailed caption modeling, large-scale distillation datasets are not strictly necessary to reach the performance ceiling of small models. Instead, carefully designed two-stage training pipeline (SFT followed by RL) can achieve competitive performance with significantly less data. 5. Conclusion and Future Work In this work, we presented UGC-VideoCap, large-scale benchmark and training framework tailored for evaluating and developing multimodal language models on detailed audio-visual understanding in UGC video scenarios. By introducing structured QA tasks and annotations for audio, visual, and final captions, UGC-VideoCap addresses the limitations of prior benchmarks that neglect the audio modality. Our proposed model, UGC-VideoCaptioner-3B, leverages two-stage training strategyfirst distilling knowledge from stronger teacher (Gemini-2.5 Flash), then applying reinforcement learning with GRPO and LLM-based rewardsto significantly improve caption quality while remaining lightweight and efficient. Looking ahead, we identify several promising directions. First, integrating automatic audio event detection and voice diarization into the annotation pipeline could further enrich the benchmark. Second, incorporating multilingual audio and text capabilities would enable broader applicability to diverse social media contexts. Lastly, we encourage the community to explore adaptive inference strategies and modality-aware attention mechanisms to better handle the noisy and heterogeneous nature of UGC content. We hope UGC-VideoCap can serve as catalyst for advancing real-world omnimodal video understanding. 10 UGC-VideoCaptioner: An Omni UGC Video Detail Caption Model and New Benchmarks"
        },
        {
            "title": "References",
            "content": "S. Bai, K. Chen, X. Liu, J. Wang, W. Ge, S. Song, K. Dang, P. Wang, S. Wang, J. Tang, et al. Qwen2. 5-vl technical report. arXiv preprint arXiv:2502.13923, 2025. W. Chai, E. Song, Y. Du, C. Meng, V. Madhavan, O. Bar-Tal, J.-N. Hwang, S. Xie, and C. D. Manning. Auroracap: Efficient, performant video detailed captioning and new benchmark. arXiv preprint arXiv:2410.03051, 2024. X. Chen, Y. Zhang, C. Rao, Y. Guan, J. Liu, F. Zhang, C. Song, Q. Liu, D. Zhang, and T. Tan. Vidcapbench: comprehensive benchmark of video captioning for controllable text-to-video generation. arXiv preprint arXiv:2502.12782, 2025. Z. Cheng, S. Leng, H. Zhang, Y. Xin, X. Li, G. Chen, Y. Zhu, W. Zhang, Z. Luo, D. Zhao, et al. Videollama 2: Advancing spatial-temporal modeling and audio understanding in video-llms. arXiv preprint arXiv:2406.07476, 2024. S. Doveh, A. Arbelle, S. Harary, R. Herzig, D. Kim, P. Cascante-Bonilla, A. Alfassy, R. Panda, R. Giryes, R. Feris, et al. Dense and aligned captions (dac) promote compositional reasoning in vl models. Advances in Neural Information Processing Systems, 36:7613776150, 2023. G. Gemini Team. Gemini 2.5: Pushing the frontier with advanced reasoning, multimodality, long context, and next generation agentic capabilities. https://storage.googleapis.com/modelcards/documents/gemini-2.5-pro-preview.pdf, 2025. google deepmind. Gemma 3n. https://deepmind.google/models/gemma/gemma-3n/, 2025. D. Guo, D. Yang, H. Zhang, J. Song, R. Zhang, R. Xu, Q. Zhu, S. Ma, P. Wang, X. Bi, et al. Deepseek-r1: Incentivizing reasoning capability in llms via reinforcement learning. arXiv preprint arXiv:2501.12948, 2025. A. Hurst, A. Lerer, A. P. Goucher, A. Perelman, A. Ramesh, A. Clark, A. Ostrow, A. Welihinda, A. Hayes, A. Radford, et al. Gpt-4o system card. arXiv preprint arXiv:2410.21276, 2024. B. Li, Y. Zhang, D. Guo, R. Zhang, F. Li, H. Zhang, K. Zhang, P. Zhang, Y. Li, Z. Liu, et al. Llava-onevision: Easy visual task transfer. arXiv preprint arXiv:2408.03326, 2024. C.-Y. Lin. Rouge: package for automatic evaluation of summaries. In Text summarization branches out, pages 7481, 2004. C. Liu, Y. Zhang, D. Zhang, W. Zhang, C. Gong, H. Li, Y. Lu, S. Zhou, Y. Lu, Z. Gan, et al. Nexus: An omniperceptive and-interactive model for language, audio, and vision. arXiv preprint arXiv:2503.01879, 2025a. H. Liu, C. Li, Q. Wu, and Y. J. Lee. Visual instruction tuning. Advances in neural information processing systems, 36:3489234916, 2023. Y. Liu and L. Yi. Map: Unleashing hybrid mamba-transformer vision backbones potential with masked autoregressive pretraining. In Proceedings of the Computer Vision and Pattern Recognition Conference, pages 96769685, 2025. Y. Liu, P. Wu, C. Liang, J. Shen, L. Wang, and L. Yi. Videomap: Toward scalable mamba-based video autoregressive pretraining. arXiv preprint arXiv:2503.12332, 2025b. 11 UGC-VideoCaptioner: An Omni UGC Video Detail Caption Model and New Benchmarks Z. Ma, C. Gou, H. Shi, B. Sun, S. Li, H. Rezatofighi, and J. Cai. Drvideo: Document retrieval based long video understanding. In Proceedings of the Computer Vision and Pattern Recognition Conference, pages 1893618946, 2025. D. Meng, R. Huang, Z. Dai, X. Li, Y. Xu, J. Zhang, Z. Huang, M. Zhang, L. Zhang, Y. Liu, et al. Videocapr1: Enhancing mllms for video captioning via structured thinking. arXiv preprint arXiv:2506.01725, 2025. A. Miech, D. Zhukov, J.-B. Alayrac, M. Tapaswi, I. Laptev, and J. Sivic. Howto100m: Learning text-video embedding by watching hundred million narrated video clips. In Proceedings of the IEEE/CVF international conference on computer vision, pages 26302640, 2019. A. Nagrani, P. H. Seo, B. Seybold, A. Hauth, S. Manen, C. Sun, and C. Schmid. Learning audio-video In European Conference on Computer Vision, pages 407426. modalities from image captions. Springer, 2022. A. v. d. Oord, S. Dieleman, H. Zen, K. Simonyan, O. Vinyals, A. Graves, N. Kalchbrenner, A. Senior, and K. Kavukcuoglu. Wavenet: generative model for raw audio. arXiv preprint arXiv:1609.03499, 2016. J. Pan, C. Liu, J. Wu, F. Liu, J. Zhu, H. B. Li, C. Chen, C. Ouyang, and D. Rueckert. Medvlm-r1: Incentivizing medical reasoning capability of vision-language models (vlms) via reinforcement learning. arXiv preprint arXiv:2502.19634, 2025. K. Papineni, S. Roukos, T. Ward, and W.-J. Zhu. Bleu: method for automatic evaluation of machine translation. In Proceedings of the 40th annual meeting of the Association for Computational Linguistics, pages 311318, 2002. A. Radford, J. W. Kim, T. Xu, G. Brockman, C. McLeavey, and I. Sutskever. Robust speech recognition via large-scale weak supervision. In International conference on machine learning, pages 2849228518. PMLR, 2023. Z. Shao, P. Wang, Q. Zhu, R. Xu, J. Song, X. Bi, H. Zhang, M. Zhang, Y. Li, Y. Wu, et al. Deepseekmath: Pushing the limits of mathematical reasoning in open language models. arXiv preprint arXiv:2402.03300, 2024. X. Shen, Y. Xiong, C. Zhao, L. Wu, J. Chen, C. Zhu, Z. Liu, F. Xiao, B. Varadarajan, F. Bordes, et al. Longvu: Spatiotemporal adaptive compression for long video-language understanding. arXiv preprint arXiv:2410.17434, 2024. J. Wang, Z. Yang, X. Hu, L. Li, K. Lin, Z. Gan, Z. Liu, C. Liu, and L. Wang. Git: generative image-to-text transformer for vision and language. arXiv preprint arXiv:2205.14100, 2022. J. Wang, L. Yuan, Y. Zhang, and H. Sun. Tarsier: Recipes for training and evaluating large video description models. arXiv preprint arXiv:2407.00634, 2024. X. Wang, J. Wu, J. Chen, L. Li, Y.-F. Wang, and W. Y. Wang. Vatex: large-scale, high-quality multilingual dataset for video-and-language research. In Proceedings of the IEEE/CVF international conference on computer vision, pages 45814591, 2019. P. Wu, C. Liu, C. Chen, J. Li, C. I. Bercea, and R. Arcucci. Fmbench: Benchmarking fairness in multimodal large language models on medical tasks. arXiv preprint arXiv:2410.01089, 2024. P. Wu, Y. Liu, M. Liu, and J. Shen. St-think: How multimodal large language models reason about 4d worlds from ego-centric videos. arXiv preprint arXiv:2503.12542, 2025. 12 UGC-VideoCaptioner: An Omni UGC Video Detail Caption Model and New Benchmarks H. Xu, Q. Ye, M. Yan, Y. Shi, J. Ye, Y. Xu, C. Li, B. Bi, Q. Qian, W. Wang, et al. mplug-2: modularized multi-modal foundation model across text, image and video. In International Conference on Machine Learning, pages 3872838748. PMLR, 2023. J. Xu, T. Mei, T. Yao, and Y. Rui. Msr-vtt: large video description dataset for bridging video and language. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 52885296, 2016. J. Xu, Z. Guo, J. He, H. Hu, T. He, S. Bai, K. Chen, J. Wang, Y. Fan, K. Dang, et al. Qwen2. 5-omni technical report. arXiv preprint arXiv:2503.20215, 2025. S. Yan, T. Zhu, Z. Wang, Y. Cao, M. Zhang, S. Ghosh, Y. Wu, and J. Yu. Videococa: Video-text modeling with zero-shot transfer from contrastive captioners. arXiv preprint arXiv:2212.04979, 2022. Y. Yao, T. Yu, A. Zhang, C. Wang, J. Cui, H. Zhu, T. Cai, H. Li, W. Zhao, Z. He, et al. Minicpm-v: gpt-4v level mllm on your phone. arXiv preprint arXiv:2408.01800, 2024. H. Zhang, X. Li, and L. Bing. Video-llama: An instruction-tuned audio-visual language model for video understanding. arXiv preprint arXiv:2306.02858, 2023. J. Zhu, W. Wang, Z. Chen, Z. Liu, S. Ye, L. Gu, H. Tian, Y. Duan, W. Su, J. Shao, et al. Internvl3: Exploring advanced training and test-time recipes for open-source multimodal models. arXiv preprint arXiv:2504.10479, 2025."
        }
    ],
    "affiliations": [
        "Memories.ai",
        "University of Bristol"
    ]
}