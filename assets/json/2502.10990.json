{
    "paper_title": "FinMTEB: Finance Massive Text Embedding Benchmark",
    "authors": [
        "Yixuan Tang",
        "Yi Yang"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Embedding models play a crucial role in representing and retrieving information across various NLP applications. Recent advances in large language models (LLMs) have further enhanced the performance of embedding models. While these models are often benchmarked on general-purpose datasets, real-world applications demand domain-specific evaluation. In this work, we introduce the Finance Massive Text Embedding Benchmark (FinMTEB), a specialized counterpart to MTEB designed for the financial domain. FinMTEB comprises 64 financial domain-specific embedding datasets across 7 tasks that cover diverse textual types in both Chinese and English, such as financial news articles, corporate annual reports, ESG reports, regulatory filings, and earnings call transcripts. We also develop a finance-adapted model, FinPersona-E5, using a persona-based data synthetic method to cover diverse financial embedding tasks for training. Through extensive evaluation of 15 embedding models, including FinPersona-E5, we show three key findings: (1) performance on general-purpose benchmarks shows limited correlation with financial domain tasks; (2) domain-adapted models consistently outperform their general-purpose counterparts; and (3) surprisingly, a simple Bag-of-Words (BoW) approach outperforms sophisticated dense embeddings in financial Semantic Textual Similarity (STS) tasks, underscoring current limitations in dense embedding techniques. Our work establishes a robust evaluation framework for financial NLP applications and provides crucial insights for developing domain-specific embedding models."
        },
        {
            "title": "Start",
            "content": "FinMTEB: Finance Massive Text Embedding Benchmark Yixuan Tang , Yi Yang The Hong Kong University of Science and Technology ytangch@connect.ust.hk, imyiyang@ust.hk 5 2 0 2 6 1 ] . [ 1 0 9 9 0 1 . 2 0 5 2 : r a"
        },
        {
            "title": "Abstract",
            "content": "Embedding models play crucial role in representing and retrieving information across various NLP applications. Recent advances in large language models (LLMs) have further enhanced the performance of embedding models. While these models are often benchmarked on general-purpose datasets, real-world applications demand domain-specific evaluation. In this work, we introduce the Finance Massive Text Embedding Benchmark (FinMTEB), specialized counterpart to MTEB designed for the financial domain. FinMTEB comprises 64 financial domain-specific embedding datasets across 7 tasks that cover diverse textual types in both Chinese and English, such as financial news articles, corporate annual reports, ESG reports, regulatory filings, and earnings call transcripts. We also develop finance-adapted model, Fin-E5, using persona-based data synthetic method to cover diverse financial embedding tasks for training. Through extensive evaluation of 15 embedding models, including Fin-E5, we show three key findings: (1) performance on general-purpose benchmarks shows limited correlation with financial domain tasks; (2) domain-adapted models consistently outperform their general-purpose counterparts; and (3) surprisingly, simple Bag-of-Words (BoW) approach outperforms sophisticated dense embeddings in financial Semantic Textual Similarity (STS) tasks, underscoring current limitations in dense embedding techniques. Our work establishes robust evaluation framework for financial NLP applications and provides crucial insights for developing domain-specific embedding models."
        },
        {
            "title": "Introduction",
            "content": "Embedding models, which transform text sequences into dense vector representations, serve as fundamental building blocks in natural language 1Github: https://github.com/yixuantt/FinMTEB 1 Figure 1: Word cloud visualization of Fin-E5s training data, contain common financial terms. processing (NLP) tasks (Mikolov et al., 2013; Pennington et al., 2014; Peters et al., 2018). The quality of text embeddings directly impacts the effectiveness of information retrieval, semantic understanding, and other downstream applications. Although recent large language model (LLM)-based embedding models have shown remarkable performance on general benchmarks (Wang et al., 2023; Li et al., 2023; Meng et al., 2024), their effectiveness in specialized domains, particularly finance, remains understudied. Financial text analysis requires precise handling of domain-specific terminology, temporal sensitivity, and complex numerical relationships (Li et al., 2024; Anderson et al., 2024). This raises two critical question: How effectively do modern embedding models capture domain-specific financial information? Can domain adaptation improve LLM-based embeddings for financial applications? These questions are motivated by three key insights. First, financial semantics often diverge from general language usage. For example, the term \"liability\" inherently conveys negative sentiment in financial contexts due to its association with obligations and risks, whereas in general usage, it neutrally denotes legal responsibility. Such semantic Figure 2: An overview of tasks and datasets used in FinMTEB. All the dataset descriptions and examples are provided in the Appendix A. divergence becomes particularly crucial for realworld applications such as Retrieval Augmented Generation (RAG) systems, where accurate document retrieval underpins effective knowledge enhancement. While recent work adapts RAG frameworks for finance (Li et al., 2024; Malandri et al., 2025), the fundamental role of embedding quality in retrieval performance remains overlooked. Second, empirical evidence increasingly suggests that domain adaptation is crucial for achieving optimal performance in specialized fields (Ling et al., 2023; Gururangan et al., 2020), even with recent advanced LLMs. This necessity for domain specialization has led to the development of fieldspecific models across various domains: BiMedLM (Bolton et al., 2024) for biomedical texts, SaulLM7B (Colombo et al., 2024) for legal documents, and BloombergGPT (Wu et al., 2023) for financial applications. This specialization trend extends to embedding models, where domain-specific variants have demonstrated superior performance in capturing specialized vocabulary and semantic relationships. For instance, BioWordVec (Zhang et al., 2019) and BioSentVec (Chen et al., 2019) are used in biomedical text analysis, while FinBERT (Yang et al., 2020) shows promising results in financial applications. However, while financial domain embedding models have shown promising improvement (e.g., BAM (Anderson et al., 2024), RoBERTa-based (Liu, 2019a) model outperforming the general model in retrieval tasks), they are still based on traditional architectures. Compared to the general domain, there is gap in the current landscape for finance NLP: while commercial solutions like voyage-finance-2 (VoyageAI, 2025) exist, there remains lack of open-source LLMbased financial embedding models available to researchers. Third, financial NLP lacks comprehensive evaluation frameworks for embedding models. Current benchmarks (Islam et al., 2023; Chen et al., 2021) primarily assess text generation rather than embedding quality. Even embedding-specific evaluations (FiQA, 2018; Liu et al., 2024a) focus narrowly on single task types (e.g., classification) or limited text genres (e.g., earnings call transcripts). This gap is deepened by financial texts unique characteristics, such as the prevalence of boilerplate language (e.g., \"The companys performance is subject to various risks...\") that creates noise in semantic representation. These standardized legal disclaimers appear frequently across documents but offer little information, complicating the models ability to differentiate meaningful business insights from routine compliance text. Thus, there is critical need for comprehensive financial embedding benchmarks. To bridge this gap, we introduce the Finance Massive Text Embedding Benchmark (FinMTEB), comprehensive evaluation framework specialized for the financial domain. FinMTEB comprises 64 domain-specific datasets that span both Chinese and English, covering seven distinct tasks: classification, clustering, retrieval, pair classification, reranking, summarization, and semantic textual similarity. We also develop Fin-E5, financeadapted version of e5-Mistral-7B-Instruct (Wang et al., 2023), utilizing persona-based data synthesis method. As shown in Figure 1, our training data 2 encompasses diverse range of financial topics concepts. Experimental results show that LLM-based embedding models consistently outperform traditional approaches, while domain adaptation further improves performance. Interestingly, in the STS task, we find that the simple Bag-of-Words (BoW) model outperforms all dense models. This indicates that current embedding models still encounter difficulties in interpreting complex financial texts. Our main contributions are twofold: First, we propose FinMTEB, the first comprehensive financial domain evaluation benchmark encompassing 64 datasets across seven distinct tasks in both Chinese and English. Second, we develop and release Fin-E5, finance-adapted embedding model that achieves state-of-the-art performance on FinMTEB. To support future research, we will make both the FinMTEB benchmark and our Fin-E5 model available as open source."
        },
        {
            "title": "2 Related Work",
            "content": "Recent advances in embedding models have shown remarkable success in general domain tasks, yet their effectiveness in specialized domains remains critical challenge."
        },
        {
            "title": "2.1 General-purpose Embedding Models",
            "content": "The evolution of embedding models marks significant progress in natural language processing. Starting with static word representations like Word2Vec (Mikolov et al., 2013) and GloVe (Pennington et al., 2014), the field advanced to contextualized embeddings through transformer-based architectures such as BERT (Devlin et al., 2019) and RoBERTa (Liu, 2019b). notable advancement came with Sentence-BERT (Reimers and Gurevych, 2019), which introduced Siamese and triplet network architectures to generate meaningful sentence-level representations. Recent developments in large language models have further pushed the boundaries, with models such as e5-mistral-7b-instruct (Wang et al., 2023) and gte-Qwen2-1.5B-instruct (Yang et al., 2024) achieving better performance in various embedding tasks. However, these generalpurpose models may not adequately capture the nuanced semantics of specialized domains."
        },
        {
            "title": "Landscape",
            "content": "To assess embedding quality, several evaluation frameworks have been developed. General-purpose embedding benchmarks, such as the Massive Text Embedding Benchmark (MTEB) (Muennighoff et al., 2022), provide broad coverage across multiple tasks and languages. Specialized benchmarks like BEIR (Thakur et al., 2021) focus on specific aspects, such as information retrieval. Although they incorporate some domain-specific datasets, such as FiQA (FiQA, 2018), the size of the data and the coverage of the task are limited."
        },
        {
            "title": "2.3 Domain Adaptation Approaches",
            "content": "Recognizing the limitations of general-purpose models in specialized domains, researchers have pursued two main adaptation strategies. The first approach develops domain-specific models from scratch, exemplified by BioMedLM (Bolton et al., 2024) for biomedicine, SaulLM-7B (Colombo et al., 2024) for legal texts, and BloombergGPT (Wu et al., 2023) for finance. The second strategy fine-tunes existing models for domain-specific tasks, as demonstrated by InvestLM (Yang et al., 2023b) and FinGPT (Yang et al., 2023a). This trend extends to embedding models, with specialized versions such as BioWordVec (Zhang et al., 2019), BioSentVec (Chen et al., 2019), and FinBERT (Yang et al., 2020) showing superior domainspecific performance. However, evaluating these specialized embedding models remains challenging due to the lack of comprehensive domain-specific benchmarks."
        },
        {
            "title": "2.4 The Gap in Domain-specific Evaluation",
            "content": "While domain-specific language models have stimulated the development of specialized evaluation frameworks across various fields, these benchmarks primarily emphasize generative and reasoning capabilities instead of embedding quality. The financial sector has seen the emergence of frameworks like CFLUE (Zhu et al., 2024), FinEval (Zhang et al., 2023), and FinanceBench (Islam et al., 2023), whereas the legal and medical domains have introduced LawBench (Fei et al., 2023), MedBench (Liu et al., 2024b), and DrBenchmark (Labrak et al., 2024). These benchmarks consistently illustrate that general-purpose models often fall short in specialized areas (Zhu et al., 2024; Fei et al., 2023), highlighting the necessity of domain adaptation (Ling et al., 2023). Despite this acknowledgment, there is still critical lack of comprehensive evaluation frameworks for domain-specific embeddings that assess performance across essential tasks such as semantic similarity, classification, 3 and retrieval. Even recent financial embedding developments, such as BAM embedding (Anderson et al., 2024), rely on narrow evaluation frameworks, typically focusing on single-task performance metrics (e.g., FinanceBench (Islam et al., 2023) for retrieval tasks). This limited evaluation may not fully reflect how the models perform in real-world financial applications."
        },
        {
            "title": "3 The FinMTEB Benchmark",
            "content": "In this section, we introduce the Finance MTEB (FinMTEB) benchmark. As illustrated in Figure 2, FinMTEB encompasses seven embedding tasks, following structure similar to MTEB (Muennighoff et al., 2022) but with datasets specifically curated for the finance domain."
        },
        {
            "title": "3.1 FinMTEB Tasks",
            "content": "Semantic Textual Similarity (STS) evaluates the semantic similarity between pairs of financial text. This task is crucial for automated financial analysis and risk management; for example, detecting subtle semantic differences between quarterly earnings statements could reveal important shifts in companys financial strategy that impact investment decisions. To ensure comprehensive evaluation, we incorporate diverse financial datasets, including FinSTS (Liu et al., 2024a) and FINAL (Ju et al., 2023) from company annual reports, and BQ-Corpus (Chen et al., 2018) from banking documents. Model performance is quantified using Spearmans rank correlation, which measures the alignment between predicted cosine similarity scores and human-annotated similarity ratings. Retrieval evaluates models capability to identify and extract relevant financial information in response to specific queries. Unlike general domain retrieval, financial information retrieval presents unique challenges, requiring precise handling of complex numerical data, temporal dependencies, and regulatory context. For comprehensive evaluation, we leverage established finance QA datasets including FinanceBench (Islam et al., 2023), FiQA2018 (FiQA, 2018), and HPC3 (Guo et al., 2023). To further assess models understanding of professional financial terminology, we introduce TheGoldman dataset, constructed from the Goldman Sachs Financial Dictionary. Performance is measured using NDCG@10, metric that evaluates both the relevance of retrieved information and its ranking position, reflecting the real-world requirement for highly precise top results in financial applications. Clustering evaluates models ability to automatically group similar financial texts based on their semantic content. To ensure comprehensive evaluation, we developed multiple specialized datasets that capture different aspects of financial text clustering: (1) FinanceArxiv-s2s and FinanceArxiv-p2p, constructed from titles and abstracts of finance-related papers on arXiv, providing rich academic financial content; (2) CompanyWiki2Industry dataset, derived from Wikipedia company descriptions, offering diverse industry categorization scenarios; and (3) complementary resources including consumer complaints from CFPB2, financial intent detection data (Gerz et al., 2021a; Watson et al., 2024), and other established datasets. Model performance is quantified using the V-measure (Rosenberg and Hirschberg, 2007), comprehensive metric that evaluates cluster quality through both completeness (all members of class are assigned to the same cluster) and homogeneity (each cluster contains only members of single class). Classification evaluates models ability to categorize financial texts into predefined classes based on their semantic content. This capability is essential for automated financial decision-making; for example, in algorithmic trading, accurately classifying sentiment in earnings calls or news articles can directly influence trading strategies and portfolio adjustments. The classification task encompasses diverse financial scenarios through multiple specialized datasets, including: financial sentiment analysis (Malo et al., 2014; FiQA, 2018; Cortis et al., 2017; Lu et al., 2023), Federal Reserve monetary policy classification (Shah et al., 2023), organizations strategy classification, and forward-looking statement identification (Yang et al., 2023b). Performance is measured using Mean Average Precision (MAP), which provides comprehensive assessment of classification accuracy while accounting for ranking quality and confidence scores. Reranking evaluates the models ability to order retrieved documents based on their relevance to financial queries. We utilize financial question-answering datasets such as Fin-Fact and FinQA(Rangapur et al., 2023; Chen et al., 2021) to 2https://huggingface.co/datasets/CFPB/consumerfinance-complaints 4 construct the reranking tasks. Specifically, for each query in these datasets, we retrieve top-k relevant documents along with the ground truth answers to construct the reranking training and evaluation pairs. The main evaluation metric for reranking in Finance MTEB is Mean Average Precision (MAP). Pair-Classification evaluates models ability to determine semantic relationships between financial text pairs. This task includes two datasets: (1) the AFQMC dataset3 for customer intention, and (2) three financial news headline datasets (Sinha and Khandait, 2021). We use Average Precision (AP) as the evaluation metric to assess model performance across different decision thresholds. Summarization is evaluated based on the correlation between dense embeddings derived from the summarized texts and those of the original texts, utilizing Spearmans correlation coefficient as the main metric. The evaluation corpus encompasses comprehensive range of financial texts, including earnings call transcripts (Mukherjee et al., 2022), financial news articles (Lu et al., 2023), and SEC Form 10-K filings (El-Haj et al., 2022), ensuring robust assessment across diverse financial contexts and writing styles."
        },
        {
            "title": "3.2 Characteristics of FinMTEB",
            "content": "FinMTEB contains 35 English datasets and 29 Chinese datasets. Detailed information about these datasets is provided in Appendix A. Linguistic Pattern. Table 9 presents comparative analysis of linguistic features between MTEB (Muennighoff et al., 2022) and FinMTEB benchmarks, examining aspects such as average sentence length, token length, syllables per token, and dependency distance (Oya, 2011). The results indicate that texts in FinMTEB consistently exhibit longer and more complex sentences than those in MTEB, with an average sentence length of 26.37 tokens compared to MTEBs 18.2 tokens. This highlights the linguistic differences between financial and general domain texts. Semantic Diversity. We examine the interdataset semantic similarity within FinMTEB. Using the all-MiniLM-L6-v2 model4, we embed 1,000 randomly sampled texts from each dataset, compute their mean embeddings to represent each dataset, and measure inter-dataset similarities using cosine similarity. As shown in Figure 4, most 3https://tianchi.aliyun.com/dataset/106411 4https://huggingface.co/sentence-transformers/allMiniLM-L6-v2 datasets in FinMTEB display inter-dataset similarity scores below 0.6, with mean cosine similarity of 0.4, indicating semantic distinctions among various types of financial texts."
        },
        {
            "title": "Embedding Model",
            "content": "Data is important for domain adaptation (Ling et al., 2023). However, existing public financial retrieval datasets exhibit significant limitations in their scope and applicability. For example, FiQA (FiQA, 2018), widely used financial retrieval dataset, primarily focuses on opinion-based content from online platforms, neglecting crucial aspects such as fundamental financial knowledge, technical terminology, and important investment data. This narrow task focus creates substantial gap in training comprehensive financial embedding models. Therefore, we use persona-based data generation to address this problem and synthesize diverse range of tasks, as illustrated in Figure 3."
        },
        {
            "title": "4.1 Data Formation",
            "content": "We aim to construct each training instance as triplet structure (q, d+, D), where represents financial query, d+ denotes relevant document that provides substantive information addressing the query, and comprises carefully selected negative examples that share the financial domain but differ in semantic intent."
        },
        {
            "title": "4.2 Training Data Construction",
            "content": "To create comprehensive dataset tailored for financial embedding training, we employ systematic approach that combines expert-curated seed data with persona-based synthetic data generation. Seed Data. Our seed data comes from the finance-specific QA dataset provided by InvestLM (Yang et al., 2023b), which offers expert-validated financial content across various domains, such as market analysis, investment strategies, and corporate finance. To ensure evaluation integrity, we conduct rigorous overlap checks between our training data and the FinMTEB benchmark, guaranteeing no overlap. Persona-based Data Augmentation. To enhance the diversity of financial task representations, we develop persona-based data augmentation framework derived from QA data generation (Ge et al., 2024). Our framework employs three-stage process that specifically targets the ex5 Figure 3: Distribution analysis of 5000 randomly sampled training data showing the breakdown of Tasks and Person Types. Left: Persona distribution. Right: Task distribution. pansion of task coverage while preserving domain consistency. Persona and Task Identification: We first employ Qwen2.5-72B-Instruct (Team, 2024) to analyze each question-answer pair in the seed data, aiming to identify the persona of the intended user (e.g., equity analyst, risk manager, financial advisor, retail investor) by using the prompt \"Who is likely to use this text?\" Different personas correspond to various tasks. Contextual Query Expansion: For each identified persona-task pair, we generate context-specific queries that reflect the personas unique information needs and risk preferences, using the prompt \"Guess prompt (i.e., instructions) that the following persona may ask you to do:\". For example, pension fund managers query might emphasize longterm asset allocation, while venture capitalists query would prioritize startup valuation metrics. Synthetic Document Generation: We used LLMs to synthesize financial documents d+ tailored to each personas task, ensuring that the dataset represents diverse perspectives in financial decision-making. This step improves the representativeness of the dataset, ensuring that the embeddings are trained in real-world financial contexts. The prompt is \"Please synthesize some real context information, which is related to this question:\". We randomly sample 5,000 data points from the training data, then use GPT-4o (OpenAI, 2024a) to annotate the job-related persona and task for the query. Visualized in Figure 3, it is clear that our data generation process produces diverse range of tasks and finance persona."
        },
        {
            "title": "4.3 Training Pipeline",
            "content": "Following the training recipe of e5-mistral-7binstruct (Wang et al., 2023), utilizing the last token pooling method, we construct training pairs by selecting queries as anchor points and their corresponding answers as positive samples. To enhance the effectiveness of contrastive learning, we identify challenging negative samples using the allMiniLM-L12-v2 model (Reimers and Gurevych, 2019). The training process applies the InfoNCE loss (Oord et al., 2018), calculated over in-batch negative samples. The detailed training parameter is illustrated in Appendix C."
        },
        {
            "title": "5 Experiment",
            "content": "In this section, we benchmark several existing models on FinMTEB, and then provide an in-depth analysis. Since most models are trained on English corpora, we only evaluate their performance on English datasets."
        },
        {
            "title": "5.1 Models",
            "content": "In addition to Fin-E5, we also evaluate four categories of embedding models on the FinMTEB benchmark in Table 1. The benchmark time is reported in Appendix D. 6 Model Size Tasks Avg. STS Retrieval Class. Cluster. Rerank. PairClass. Summ. 2 10 8 3 3 3 - 0.4845 110M 0.3789 110M 0.4198 110M 0.3732 335M 0.3396 335M 0. BOW Encoder based Models BERT FinBERT instructor-base bge-large-en-v1.5 AnglE-BERT LLM-based Models gte-Qwen1.5-7B-instruct Echo bge-en-icl NV-Embed v2 e5-mistral-7b-instruct Commercial Models text-embedding-3-small text-embedding-3-large voyage-3-large Finance Adapted LLM-based Models Fin-E5 7B 7B 7B 7B 7B 0.3758 0.4380 0.3233 0.3739 0.3800 0.3254 0.3615 0.4145 0.4342 7B - - - 0.2084 0.4696 0.2547 0.7628 0. 0.0542 0.4212 0.0207 0.1102 0.5772 0.6463 0.5730 0.6697 0.6443 0.6789 0.7061 0.6749 0.6641 0.7112 0.7463 0.5496 0.5923 0.6208 0.6436 0. 0.6438 0.6525 0.6569 0.6393 0.6449 0.6387 0.6596 0.6861 0.1744 0.2833 0.5300 0.5725 0.5774 0.5854 0.5776 0.5742 0.6096 0.5783 0.5802 0.6081 0.5944 0.3930 0.6404 0.9734 0.9825 0. 0.9890 0.9765 0.9898 0.9822 0.9875 0.9825 0.9910 0.9938 0.7111 0.6967 0.6138 0.7400 0.6891 0.6998 0.6261 0.6738 0.6043 0.7394 0.5957 0.7309 0.6519 0.0452 0.0417 0.1465 0.2019 0. 0.2350 0.4722 0.5197 0.5103 0.5275 0.5085 0.5671 0.6484 0.3247 0.3978 0.5479 0.5895 0.6088 0.5998 0.6267 0.6309 0.6322 0.6475 0.6136 0.6613 0.6765 0. 0.7565 0.5650 0.9896 0.8014 0.4797 0. Table 1: Performance comparison across different embedding models on FinMTEB benchmark. The evaluation metrics include semantic textual similarity (STS), retrieval, classification (Class.), clustering (Cluster.), reranking (Rerank.), pair classification (PairClass.), and summarization (Summ.). Best results are in bold. The underline represents the second-best performance. Bag-of-Words (BOW). As simple baseline, we implement the traditional BOW approach that represents text as sparse vectors based on word frequencies, providing reference point for comparing more sophisticated methods. Encoder-based Models. We evaluate various transformer encoder architectures, including: (1) classical models like BERT (CLS pooling) (Devlin et al., 2019) and domain-specific FinBERT (Yang et al., 2020); (2) optimized models such as msmarco-bert-base-dot-v5 and allMiniLM-L12-v2 (Reimers and Gurevych, 2019); and (3) advanced architectures including bge-largeen-v1.5 (Xiao et al., 2023), AnglE-BERT (Li and Li, 2023) and instructor-base (Su et al., 2022). LLM-based Models. We investigate several state-of-the-art decoder-based embedding models: (1) Mistral-7B-based models including bge-en-icl (Xiao et al., 2023) , e5-mistral-7binstruct (Wang et al., 2023) and Echo (Springer et al., 2024); (2) NV-Embed v2 (Lee et al., 2024); and (3) gte-Qwen1.5-7B-instruct (Li et al., 2023) built on the Qwen2 (Yang et al., 2024) architecture. These models utilize the powerful representation capabilities of LLMs to generate high-quality embeddings. Commercial Models. To provide comprehensive comparison with commercial solutions, we include industry-leading closed-source models, specifically OpenAIs text-embedding-3-large, textembedding-3-small (OpenAI, 2024b) and voyage3-large (VoyageAI, 2025)5."
        },
        {
            "title": "5.2 Analysis",
            "content": "Based on the results presented in Table 1, our analysis focuses on three key findings. 5.2."
        },
        {
            "title": "Impact of Domain Adaptation",
            "content": "As illustrated in Table 1, domain specialization considerably boosts performance: FinBERT outperforms BERT by 15.6% (0.6721 vs. 0.5812), while Fin-E5 exceeds its general-domain counterpart e5-mistral-7b-instruct by 4.5% (0.6767 vs. 0.6475), particularly excelling in classification (0.842 vs. 0.807) and semantic textual similarity (0.721 vs. 0.685). The finance-adapted Fin-E5 also achieves state-of-the-art performance (0.6767 average score) on the FinMTEB benchmark, exceeding both general-purpose and commercial models. Notably, this peak performance is achieved with 5We thank VoyageAI for supporting us in conducting the evaluation. 7 just 100 training steps, showcasing cost-effective adaptation without the risk of data leakage."
        },
        {
            "title": "5.2.2 The Role of Model Architecture and Size",
            "content": "Our experiments reveal three distinct performance tiers across architectural paradigms  (Table 1)  . Traditional bag-of-words (BOW) models achieve baseline performance (STS: 0.4845) and show notable limitations in retrieval tasks. Encoder-based architectures, such as bge-large-en-v1.5, demonstrate significant improvements, increasing retrieval performance by 107% (0.6463) and STS by 38% (0.6692) over BOW. paradigm shift occurs with LLM-based models; e5-mistral-7b-instruct sets new standards with an average score of 0.6475. This progression from BOW (lexical) to LLMbased (contextual) architectures reveals 52% overall performance improvement, suggesting that model capacity plays critical role in capturing financial semantics."
        },
        {
            "title": "Financial STS Tasks",
            "content": "The STS results reveal counterintuitive finding: BOW models (0.4845) outperform all dense architectures (maximum 0.4342) in terms of financial document similarity. This reversal of typical NLP performance hierarchies arises from two characteristics of the corpus: (1) Extensive boilerplate content in annual reports introduces noise for contextual embeddings, and (2) Specialized terminology (27% unique financial terms per document) decreases lexical overlap.BOW benefits from exact term matches in standardized disclosures; the best dense model only captures 64% of humanannotated similarity relationships, revealing fundamental limitations in current strategies for financial documents."
        },
        {
            "title": "Benchmark is needed",
            "content": "This section addresses another research question. To what extent do general-purpose embedding evaluations appropriately capture domain-specific performance? To solve this question, we run quantitative comparison between MTEB (Muennighoff et al., 2022) and FinMTEB. Models. We evaluate seven state-of-the-art general-purpose embedding model. Specifically, we consider the following models: bge-en-icl (Xiao et al., 2023) and e5-mistral-7b-instruct (Wang et al., 2023), which are developed from Mistral-7B-v0.1 (Jiang et al., 2023); gte-Qwen2-1.5B-instruct (Li et al., 2023), developed from Qwen2 (Yang et al., 2024); bge-large-en-v1.5 (Xiao et al., 2023) and allMiniLM-L12-v2 (Reimers and Gurevych, 2019), both developed from BERT (Devlin et al., 2019); instructor-base (Su et al., 2022) from T5Encoder (Raffel et al., 2020); and OpenAIs text-embedding3-small (OpenAI, 2024b). Method. To ensure robust statistical analysis, we use bootstrapping methods to generate large sample dataset. For each task in both MTEB and FinMTEB, we aggregate the datasets associated with the task into task pool. From each task pool, we randomly select 50 examples to create bootstrap sample and evaluate the embedding models performance on this bootstrap. We repeat this process 500 times, resulting in 500 bootstraps for each combination. Thus, we have 14 unique combinations (model and domain), each with 500 bootstraps and their corresponding performance scores. Analysis of Variance. We conduct an Analysis of Variance (ANOVA) that examines the effects of both the model and the domain. The results reveal that the Domain Factor demonstrates statistical significance across all tasks (p < 0.001), with notably large statistics in classification (F = 2086.30), clustering (F = 32161.37), and STS (F = 25761.71). Furthermore, the Domain Factor generally accounts for greater share of the variance than the Model Factor, as indicated by the Sum of Squares (e.g., in Classification: Domain = 56.82 vs. Model = 4.17). These findings suggest that domainspecific characteristics significantly impact model performance, reinforcing the importance of specialized evaluation frameworks such as FinMTEB for financial applications."
        },
        {
            "title": "7 Conclusion",
            "content": "This paper introduces FinMTEB, the first comprehensive benchmark for evaluating embedding models in the financial domain. Our main contributions include establishing large-scale evaluation framework with 64 datasets across seven tasks in Chinese and English, and developing Fin-E5, finance-adapted embedding model demonstrating competitive performance through persona-based data augmentation. Our empirical results highlight the importance of domain-specific adaptation and reveal current limitations in financial text embeddings. We believe FinMTEB will serve as valu8 able resource for both researchers and practitioners in advancing financial language models."
        },
        {
            "title": "8 Limitation",
            "content": "This work has two primary limitations. First, it relies on several existing financial datasets that could potentially overlap with the training data of contemporary embedding models. This overlap may introduce contamination, making it difficult to ensure completely fair comparisons between different models. Second, our adapted model and evaluation methods are currently limited to the English language, which restricts their applicability to nonEnglish financial texts."
        },
        {
            "title": "References",
            "content": "Peter Anderson, Mano Vikash Janardhanan, Jason He, Wei Cheng, and Charlie Flanagan. 2024. Greenback bears and fiscal hawks: Finance is jungle and text embeddings must adapt. In Proceedings of the 2024 Conference on Empirical Methods in Natural Language Processing: Industry Track, pages 362370, Miami, Florida, US. Association for Computational Linguistics. Elliot Bolton, Abhinav Venigalla, Michihiro Yasunaga, David Hall, Betty Xiong, Tony Lee, Roxana Daneshjou, Jonathan Frankle, Percy Liang, Michael Carbin, et al. 2024. Biomedlm: 2.7 parameter language model trained on biomedical text. arXiv preprint arXiv:2403.18421. CCKS. 2022. Ccks2022: Few-shot event extraction for the financial sector. https://www.biendata.xyz/ competition/ccks2022_eventext/. CFPB. 2024. Consumer finance complaints. https://huggingface.co/datasets/CFPB/ consumer-finance-complaints. Jing Chen, Qingcai Chen, Xin Liu, Haijun Yang, Daohe Lu, and Buzhou Tang. 2018. The BQ corpus: largescale domain-specific Chinese corpus for sentence semantic equivalence identification. In Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, pages 49464951, Brussels, Belgium. Association for Computational Linguistics. Qingyu Chen, Yifan Peng, and Zhiyong Lu. 2019. Biosentvec: creating sentence embeddings for biomedical texts. In 2019 IEEE International Conference on Healthcare Informatics (ICHI), pages 15. IEEE. Wei Chen, Qiushi Wang, Zefei Long, Xianyin Zhang, Zhongtian Lu, Bingxuan Li, Siyuan Wang, Jiarong Xu, Xiang Bai, Xuanjing Huang, and Zhongyu Wei. 2023. Disc-finllm: chinese financial large language model based on multiple experts fine-tuning. arXiv preprint arXiv:2310.15205. Zhiyu Chen, Wenhu Chen, Charese Smiley, Sameena Shah, Iana Borova, Dylan Langdon, Reema Moussa, Matt Beane, Ting-Hao Huang, Bryan Routledge, and William Yang Wang. 2021. FinQA: dataset of numerical reasoning over financial data. In Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, pages 36973711, Online and Punta Cana, Dominican Republic. Association for Computational Linguistics. Pierre Colombo, Telmo Pessoa Pires, Malik Boudiaf, Dominic Culver, Rui Melo, Caio Corro, Andre FT Martins, Fabrizio Esposito, Vera Lúcia Raposo, Sofia Morgado, et al. 2024. Saullm-7b: pioneering large language model for law. arXiv preprint arXiv:2403.03883. Keith Cortis, André Freitas, Tobias Daudert, Manuela Huerlimann, Manel Zarrouk, Siegfried Handschuh, and Brian Davis. 2017. SemEval-2017 task 5: Finegrained sentiment analysis on financial microblogs and news. In Proceedings of the 11th International Workshop on Semantic Evaluation (SemEval-2017), pages 519535, Vancouver, Canada. Association for Computational Linguistics. Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. 2019. BERT: Pre-training of deep bidirectional transformers for language understanding. In Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers), pages 41714186, Minneapolis, Minnesota. Association for Computational Linguistics. Mahmoud El-Haj, Nadhem Zmandar, Paul Rayson, Ahmed AbuRaed, Marina Litvak, Nikiforos Pittaras, George Giannakopoulos, Aris Kosmopoulos, Blanca Carbajo-Coronado, and Antonio Moreno-Sandoval. 2022. The financial narrative summarisation shared task (FNS 2022). In Proceedings of the 4th Financial Narrative Processing Workshop @LREC2022, pages 4352, Marseille, France. European Language Resources Association. Zhiwei Fei, Xiaoyu Shen, Dawei Zhu, Fengzhe Zhou, Zhuo Han, Songyang Zhang, Kai Chen, Zongwen Shen, and Jidong Ge. 2023. Lawbench: Benchmarking legal knowledge of large language models. arXiv preprint arXiv:2309.16289. FiQA. 2018. Financial question answering. https: //sites.google.com/view/fiqa. Tao Ge, Xin Chan, Xiaoyang Wang, Dian Yu, Haitao Mi, and Dong Yu. 2024. Scaling synthetic data creation with 1,000,000,000 personas. arXiv preprint arXiv:2406.20094. Daniela Gerz, Pei-Hao Su, Razvan Kusztos, Avishek Mondal, Michał Lis, Eshan Singhal, Nikola Mrkšic, Tsung-Hsien Wen, and Ivan Vulic. 2021a. Multilingual and cross-lingual intent detection from spoken In Proceedings of the 2021 Conference on data. 9 Empirical Methods in Natural Language Processing, pages 74687475, Online and Punta Cana, Dominican Republic. Association for Computational Linguistics. Daniela Gerz, Pei-Hao Su, Razvan Kusztos, Avishek Mondal, Michał Lis, Eshan Singhal, Nikola Mrkšic, Tsung-Hsien Wen, and Ivan Vulic. 2021b. Multilingual and cross-lingual intent detection from spoken data. arXiv preprint arXiv:2104.08524. Biyang Guo, Xin Zhang, Ziyuan Wang, Minqi Jiang, Jinran Nie, Yuxuan Ding, Jianwei Yue, and Yupeng Wu. 2023. How close is chatgpt to human experts? comparison corpus, evaluation, and detection. arXiv preprint arXiv:2301.07597. Suchin Gururangan, Ana Marasovic, Swabha Swayamdipta, Kyle Lo, Iz Beltagy, Doug Downey, and Noah A. Smith. 2020. Dont stop pretraining: Adapt language models to domains and tasks. In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 83428360, Online. Association for Computational Linguistics. Pranab Islam, Anand Kannappan, Douwe Kiela, Rebecca Qian, Nino Scherrer, and Bertie Vidgen. 2023. Financebench: new benchmark for financial question answering. arXiv preprint arXiv:2311.11944. Albert Jiang, Alexandre Sablayrolles, Arthur Mensch, Chris Bamford, Devendra Singh Chaplot, Diego de las Casas, Florian Bressand, Gianna Lengyel, Guillaume Lample, Lucile Saulnier, et al. 2023. Mistral 7b. arXiv preprint arXiv:2310.06825. Jia-Huei Ju, Yu-Shiang Huang, Cheng-Wei Lin, Che Lin, and Chuan-Ju Wang. 2023. compare-and-contrast multistage pipeline for uncovering financial signals in financial reports. In Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 14307 14321, Toronto, Canada. Association for Computational Linguistics. Yanis Labrak, Adrien Bazoge, Oumaima El Khettari, Mickaël Rouvier, Natalia Grabar, Beatrice Daille, Solen Quiniou, Emmanuel Morin, Pierre-Antoine Gourraud, Richard Dufour, et al. 2024. Drbenchmark: large language understanding evaluation benchmark for french biomedical domain. arXiv preprint arXiv:2402.13432. Yinyu Lan, Yanru Wu, Wang Xu, Weiqiang Feng, and Youhao Zhang. 2023. Chinese fine-grained financial sentiment analysis with large language models. arXiv preprint arXiv:2306.14096. Chankyu Lee, Rajarshi Roy, Mengyao Xu, Jonathan Raiman, Mohammad Shoeybi, Bryan Catanzaro, and Wei Ping. 2024. Nv-embed: Improved techniques for training llms as generalist embedding models. arXiv preprint arXiv:2405.17428. Xiang Li, Zhenyu Li, Chen Shi, Yong Xu, Qing Du, Mingkui Tan, Jun Huang, and Wei Lin. 2024. Alphafin: Benchmarking financial analysis with retrieval-augmented stock-chain framework. arXiv preprint arXiv:2403.12582. Xianming Li and Jing Li. 2023. Angle-optimized text embeddings. arXiv preprint arXiv:2309.12871. Zehan Li, Xin Zhang, Yanzhao Zhang, Dingkun Long, Pengjun Xie, and Meishan Zhang. 2023. Towards general text embeddings with multi-stage contrastive learning. arXiv preprint arXiv:2308.03281. Chen Ling, Xujiang Zhao, Jiaying Lu, Chengyuan Deng, Can Zheng, Junxiang Wang, Tanmoy Chowdhury, Yun Li, Hejie Cui, Xuchao Zhang, et al. 2023. Domain specialization as the key to make large language models disruptive: comprehensive survey. arXiv preprint arXiv:2305.18703. Jiaxin Liu, Yi Yang, and Kar Yan Tam. 2024a. Beyond surface similarity: Detecting subtle semantic shifts in financial narratives. In Findings of the Association for Computational Linguistics: NAACL 2024, pages 26412652, Mexico City, Mexico. Association for Computational Linguistics. Mianxin Liu, Jinru Ding, Jie Xu, Weiguo Hu, Xiaoyang Li, Lifeng Zhu, Zhian Bai, Xiaoming Shi, Benyou Wang, Haitao Song, et al. 2024b. Medbench: comprehensive, standardized, and reliable benchmarking system for evaluating chinese medical large language models. arXiv preprint arXiv:2407.10990. Shuaiqi Liu, Jiannong Cao, Ruosong Yang, and Zhiyuan Wen. 2022. Long text and multi-table summarization: Dataset and method. In Findings of the Association for Computational Linguistics: EMNLP 2022, pages 19952010, Abu Dhabi, United Arab Emirates. Association for Computational Linguistics. Yinhan Liu. 2019a. Roberta: robustly optimized bert pretraining approach. arXiv preprint arXiv:1907.11692, 364. Yinhan Liu. 2019b. Roberta: robustly optimized bert pretraining approach. arXiv preprint arXiv:1907.11692. Dakuan Lu, Hengkui Wu, Jiaqing Liang, Yipei Xu, Qianyu He, Yipeng Geng, Mengkun Han, Yingsi Xin, and Yanghua Xiao. 2023. Bbt-fin: Comprehensive construction of chinese financial domain pre-trained language model, corpus and benchmark. arXiv preprint arXiv:2302.09432. Lorenzo Malandri, Fabio Mercorio, Mario Mezzanzanica, and Filippo Pallucchini. 2025. RE-FIN: Retrieval-based enrichment for financial data. In Proceedings of the 31st International Conference on Computational Linguistics: Industry Track, pages 751759, Abu Dhabi, UAE. Association for Computational Linguistics. 10 Pekka Malo, Ankur Sinha, Pekka Korhonen, Jyrki Wallenius, and Pyry Takala. 2014. Good debt or bad debt: Detecting semantic orientations in economic texts. Journal of the Association for Information Science and Technology, 65(4):782796. Rui Meng, Ye Liu, Shafiq Rayhan Joty, Caiming Xiong, Yingbo Zhou, and Semih Yavuz. 2024. Sfrembedding-2: Advanced text embedding with multistage training. Tomas Mikolov, Kai Chen, Greg Corrado, and Jeffrey Dean. 2013. Efficient estimation of word arXiv preprint representations in vector space. arXiv:1301.3781. Niklas Muennighoff, Nouamane Tazi, Loïc Magne, and Nils Reimers. 2022. Mteb: Massive text embedding benchmark. arXiv preprint arXiv:2210.07316. Rajdeep Mukherjee, Abhinav Bohra, Akash Banerjee, Soumya Sharma, Manjunath Hegde, Afreen Shaikh, Shivani Shrivastava, Koustuv Dasgupta, Niloy Ganguly, Saptarshi Ghosh, et al. 2022. Ectsum: new benchmark dataset for bullet point summarization of long earnings call transcripts. arXiv preprint arXiv:2210.12467. Qiong Nan, Juan Cao, Yongchun Zhu, Yanyan Wang, and Jintao Li. 2021. Mdfend: Multi-domain fake news detection. In Proceedings of the 30th ACM International Conference on Information & Knowledge Management, pages 33433347. Aaron van den Oord, Yazhe Li, and Oriol Vinyals. 2018. Representation learning with contrastive predictive coding. arXiv preprint arXiv:1807.03748. OpenAI. 2024a. Openai (august 24 version). https: //api.openai.com/v1/chat. OpenAI. 2024b. Openai (august 24 version). https: //api.openai.com/v1/embeddings. Masanori Oya. 2011. Syntactic dependency distance In Proceedings as sentence complexity measure. of the 16th International Conference of Pan-Pacific Association of Applied Linguistics, volume 1. Jeffrey Pennington, Richard Socher, and Christopher Manning. 2014. GloVe: Global vectors for word representation. In Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 15321543, Doha, Qatar. Association for Computational Linguistics. Matthew E. Peters, Mark Neumann, Mohit Iyyer, Matt Gardner, Christopher Clark, Kenton Lee, and Luke Zettlemoyer. 2018. Deep contextualized word representations. In Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long Papers), pages 22272237, New Orleans, Louisiana. Association for Computational Linguistics. Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, and Peter Liu. 2020. Exploring the limits of transfer learning with unified text-to-text transformer. Journal of machine learning research, 21(140):167. Aman Rangapur, Haoran Wang, Ling Jian, and Kai Shu. 2023. Fin-fact: benchmark dataset for multimodal financial fact checking and explanation generation. arXiv preprint arXiv:2309.08793. Nils Reimers and Iryna Gurevych. 2019. Sentence-bert: Sentence embeddings using siamese bert-networks. In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing. Association for Computational Linguistics. Andrew Rosenberg and Julia Hirschberg. 2007. Vmeasure: conditional entropy-based external cluster evaluation measure. In Proceedings of the 2007 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning (EMNLP-CoNLL), pages 410 420, Prague, Czech Republic. Association for Computational Linguistics. Agam Shah, Suvan Paturi, and Sudheer Chava. 2023. Trillion dollar words: new financial dataset, task & market analysis. In Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 66646679, Toronto, Canada. Association for Computational Linguistics. Ankur Sinha and Tanmay Khandait. 2021. Impact of news on the commodity market: Dataset and results. In Advances in Information and Communication: Proceedings of the 2021 Future of Information and Communication Conference (FICC), Volume 2, pages 589601. Springer. Jacob Mitchell Springer, Suhas Kotha, Daniel Fried, Graham Neubig, and Aditi Raghunathan. 2024. Repetition improves language model embeddings. arXiv preprint arXiv:2402.15449. Hongjin Su, Weijia Shi, Jungo Kasai, Yizhong Wang, Yushi Hu, Mari Ostendorf, Wen-tau Yih, Noah Smith, Luke Zettlemoyer, and Tao Yu. 2022. One embedder, any task: Instruction-finetuned text embeddings. arXiv preprint arXiv:2212.09741. Maosong Sun, Jingyang Li, Zhipeng Guo, Yu Zhao, Yabin Zheng, Xiance Si, and Zhiyuan Liu. 2016. Thuctc: An efficient chinese text classifier. http: //thuctc.thunlp.org/. Qwen Team. 2024. Qwen2.5: party of foundation models. Nandan Thakur, Nils Reimers, Andreas Rücklé, Abhishek Srivastava, and Iryna Gurevych. 2021. BEIR: heterogeneous benchmark for zero-shot evaluation of information retrieval models. In Thirty-fifth Conference on Neural Information Processing Systems Datasets and Benchmarks Track (Round 2). VoyageAI. 2025. Voyageai (jan 25 version). https: //api.voyageai.com/v1/embeddings. Liang Wang, Nan Yang, Xiaolong Huang, Linjun Yang, Rangan Majumder, and Furu Wei. 2023. Improving text embeddings with large language models. arXiv preprint arXiv:2401.00368. Alex Watson, Yev Meyer, Maarten Van Segbroeck, Matthew Grossman, Sami Torbey, Piotr Mlocek, and Johnny Greco. 2024. Synthetic-PII-FinancialDocuments-North-America: synthetic dataset for training language models to label and detect pii in domain specific formats. Shijie Wu, Ozan Irsoy, Steven Lu, Vadim Dabravolski, Mark Dredze, Sebastian Gehrmann, Prabhanjan Kambadur, David Rosenberg, and Gideon Mann. 2023. Bloomberggpt: large language model for finance. arXiv preprint arXiv:2303.17564. Shitao Xiao, Zheng Liu, Peitian Zhang, and Niklas Muennighoff. 2023. C-pack: Packaged resources to advance general chinese embedding. arXiv preprint arXiv:2309.07597. Ziyue Xu, Peilin Zhou, Xinyu Shi, Jiageng Wu, Yikang Jiang, Bin Ke, and Jie Yang. 2024. Fintruthqa: benchmark dataset for evaluating the quality of arXiv preprint financial information disclosure. arXiv:2406.12009. An Yang, Baosong Yang, Binyuan Hui, Bo Zheng, Bowen Yu, Chang Zhou, Chengpeng Li, Chengyuan Li, Dayiheng Liu, Fei Huang, et al. 2024. Qwen2 technical report. arXiv preprint arXiv:2407.10671. Hongyang Yang, Xiao-Yang Liu, and Christina Dan Wang. 2023a. Fingpt: Open-source financial large language models. arXiv preprint arXiv:2306.06031. Yi Yang, Yixuan Tang, and Kar Yan Tam. 2023b. Investlm: large language model for investment using financial domain instruction tuning. arXiv preprint arXiv:2309.13064. Yi Yang, Mark Christopher Siy Uy, and Allen Huang. Finbert: pretrained language model arXiv preprint 2020. for financial communications. arXiv:2006.08097. Liwen Zhang, Weige Cai, Zhaowei Liu, Zhi Yang, Wei Dai, Yujie Liao, Qianru Qin, Yifei Li, Xingyu Liu, Zhiqiang Liu, et al. 2023. Fineval: chinese financial domain knowledge evaluation benchmark for large language models. arXiv preprint arXiv:2308.09975. Yijia Zhang, Qingyu Chen, Zhihao Yang, Hongfei Lin, and Zhiyong Lu. 2019. Biowordvec, improving biomedical word embeddings with subword information and mesh. Scientific data, 6(1):52. Zhihan Zhou, Liqian Ma, and Han Liu. 2021. Trade the event: Corporate events detection for news-based event-driven trading. In Findings of the Association for Computational Linguistics: ACL-IJCNLP 2021, pages 21142124, Online. Association for Computational Linguistics. Fengbin Zhu, Wenqiang Lei, Youcheng Huang, Chao Wang, Shuo Zhang, Jiancheng Lv, Fuli Feng, and Tat-Seng Chua. 2021. Tat-qa: question answering benchmark on hybrid of tabular and textual content in finance. arXiv preprint arXiv:2105.07624. Jie Zhu, Junhui Li, Yalong Wen, and Lifan Guo. 2024. Benchmarking large language models on cfluea chinese financial language understanding evaluation dataset. arXiv preprint arXiv:2405.10542."
        },
        {
            "title": "A Datasets",
            "content": "The detailed description of each dataset used in this work is listed in the Table tables 2 to 8."
        },
        {
            "title": "B Dataset Characteristic",
            "content": "Figure 4 presents the semantic similarity across all datasets in the FinMTEB benchmark. The semantic similarity is calculated by cosine similarity. Table 9 presents comparative analysis of linguistic features between MTEB (Muennighoff et al., 2022) and FinMTEB benchmarks, examining aspects such as average sentence length, token length, syllables per token, and dependency distance (Oya, 2011). Training Details For Fin-E5 The training dataset size is 19,467. The model is trained for 100 steps using the augmented dataset with batch size of 128. For optimization, we use the AdamW optimizer with learning rate of 1e-5 and implement linear warmup schedule. For given data (q, d+, D), we adopt an instructionbased methodology for embedding training. The instruction template is as follows: qinst = Instruct: {task_def inition}n{q} (1) where {task_def inition} represents concise single-sentence description of the embedding task. Benchmarking Time Reporting. The benchmarking was conducted on the NVIDIA H800 GPU using batch size of 512. Echo Embedding (Springer et al., 2024) required the longest processing time at 12 hours, followed by BeLLM (Li"
        },
        {
            "title": "Language Description",
            "content": "FINAL (Ju et al., 2023)"
        },
        {
            "title": "English",
            "content": "FinSTS (Liu et al., 2024a)"
        },
        {
            "title": "English",
            "content": "AFQMC"
        },
        {
            "title": "Chinese",
            "content": "BQ-Corpus (Chen et al., 2018) Chinese dataset designed for discovering financial signals in narrative financial reports. dataset focused on detecting subtle semantic shifts in financial narratives. Chinese dataset for customer service question matching in the financial domain. large-scale Chinese corpus for sentence semantic equivalence identification (SSEI) in the banking domain. Table 2: Summary of STS Datasets and Li, 2023) at 11.98 hours. AnglE-BERT (Li and Li, 2023) completed the evaluation in 8 hours, while NV-Embed v2 (Lee et al., 2024) demonstrated the highest efficiency, completing all tasks in just 5.6 hours. Spearmans Correlation of Embedding Models Performance We evaluate the performance ranking of embedding models on both the general MTEB and FinMTEB datasets, calculating Spearmans rank correlation between the two. The results, shown in Table 10, indicate that the ranking correlation is not statistically significant (p-values all greater than 0.05). In other words, general-purpose embedding model performing well on MTEB does not necessarily perform well on domain-specific tasks. Analysis of Variance (ANOVA) Table 11 illustrates the full results of ANOVA analysis. 8https://tianchi.aliyun.com/dataset/106411 9https://lighthouz.ai/blog/ rag-benchmark-finance-apple-10K-2022/ 10https://www.kaggle.com/datasets/jeet2016/ us-financial-news-articles 11https://github.com/alipay/financial_ evaluation_dataset/tree/main 12https://github.com/smoothnlp/SmoothNLP 13https://github.com/alipay/financial_ evaluation_dataset/tree/main 14https://github.com/amitkedia007/ Financial-Fraud-Detection-Using-LLMs/tree/main 15https://github.com/open-compass/OpenFinData? tab=readme-ov-file"
        },
        {
            "title": "Language Description",
            "content": "FiQA2018 (FiQA, 2018)"
        },
        {
            "title": "English",
            "content": "FinanceBench (Islam et al., 2023) HC3(Finance) (Guo et al., 2023) English"
        },
        {
            "title": "English",
            "content": "Apple-10K-"
        },
        {
            "title": "English",
            "content": "FinQA (Chen et al., 2021)"
        },
        {
            "title": "English",
            "content": "TAT-QA (Zhu et al., 2021)"
        },
        {
            "title": "English",
            "content": "US Financial News 8 TradeTheEvent (Trading Benchmark) (Zhou et al., 2021) TradeTheEvent (Domain Adaption) (Zhou et al., 2021) TheGoldman-en"
        },
        {
            "title": "English",
            "content": "FinTruthQA (Xu et al., 2024)"
        },
        {
            "title": "Chinese",
            "content": "Fin-Eva (Retrieval task)"
        },
        {
            "title": "Chinese",
            "content": "AlphaFin (Li et al., 2024)"
        },
        {
            "title": "Chinese",
            "content": "(Lu DISC-FinLLM (Retrieval Part Data) (Chen et al., 2023) FinQA (from DuEE-fin) et al., 2023) DISC-FinLLM (Computing) (Chen et al., 2023) SmoothNLP 10 THUCNews (Sun et al., 2016) Fin-Eva (Terminology) 11 TheGoldman-cn Financial opinion mining and question answering dataset. Open book financial question answering dataset. human-ChatGPT comparison corpus in the finance domain. retrieval-augmented generation (RAG) benchmark for finance applications. Financial numerical reasoning dataset with structured and unstructured evidence. Question answering benchmark combining tabular and textual content in finance. Finance news articles paired with headlines and stock ticker symbols. Finance news articles paired with headlines and stock ticker symbols. Financial terms and explanations dataset. English version of the Goldman Sachs Financial Dictionary. Dataset for evaluating the quality of financial information disclosure. Financial scenario QA dataset focusing on retrieval tasks. Comprehensive financial dataset including NLI, QA, and stock trend predictions. Financial scenario QA dataset."
        },
        {
            "title": "Chinese",
            "content": "Financial news bulletin event quiz dataset."
        },
        {
            "title": "Chinese\nChinese\nChinese\nChinese",
            "content": "Financial scenario QA dataset focusing on numerical tasks. Chinese finance news dataset. Chinese finance news dataset. Financial terminology dataset used in the industry. Chinese version of the Goldman Sachs Financial Dictionary. Table 3: Summary of Retrieval Datasets"
        },
        {
            "title": "Language Description",
            "content": "FinancialPhrasebank (Malo et al., 2014)"
        },
        {
            "title": "English",
            "content": "FinSent (Yang et al., 2023b) FiQA_ABSA (FiQA, 2018)"
        },
        {
            "title": "English",
            "content": "SemEva2017_Headline (Cortis et al., 2017)"
        },
        {
            "title": "English",
            "content": "FLS (Yang et al., 2023b) ESG (Yang et al., 2023b) FOMC (Shah et al., 2023) Financial-Fraud 12 FinNSP (Lu et al., 2023) FinChina (Lan et al., 2023)"
        },
        {
            "title": "Chinese",
            "content": "Chinese FinFE (Lu et al., 2023) OpenFinData 13 Chinese MDFEND-Weibo2 (finance) (Nan et al., 2021) Chinese Polar sentiment dataset of sentences from financial news, categorized by sentiment into positive, negative, or neutral. Polar sentiment dataset of sentences from the financial domain, categorized by sentiment into positive, negative, or neutral. Polar sentiment dataset of sentences from the financial domain, categorized by sentiment into positive, negative, or neutral. Polar sentiment dataset of sentences from the financial domain, categorized by sentiment into positive, negative, or neutral. finance dataset detects whether the sentence is forwardlooking statement. finance dataset performs sentence classification under the environmental, social, and corporate governance (ESG) framework. task of hawkish-dovish classification in finance domain. This dataset was used for research in detecting financial fraud. Financial negative news and its subject determination dataset. Polar sentiment dataset of sentences from the financial domain, categorized by sentiment into positive, negative, or neutral. Financial social media text sentiment categorization dataset. Financial scenario QA dataset including sentiment task. Fake news detection in the finance domain. Table 4: Summary of Classification Datasets"
        },
        {
            "title": "Language Description",
            "content": "MInDS-14-en (Gerz et al., 2021b)"
        },
        {
            "title": "English",
            "content": "Consumer Complaints (CFPB, 2024)"
        },
        {
            "title": "English",
            "content": "Synthetic PII finance (Watson et al., 2024) English FinanceArxiv-s2s 14 FinanceArxiv-p2p WikiCompany2Industry-en"
        },
        {
            "title": "English\nEnglish\nEnglish",
            "content": "MInDS-14-zh (Gerz et al., 2021b)"
        },
        {
            "title": "Chinese",
            "content": "FinNL (Lu et al., 2023) CCKS2022 (CCKS, 2022) CCKS2020 CCKS"
        },
        {
            "title": "Chinese\nChinese\nChinese\nChinese",
            "content": "MINDS-14 is dataset for intent detection in e-banking, covering 14 intents across 14 languages. The Consumer Complaint Database is collection of complaints about consumer financial products and services that sent to companies for response. Synthetic financial documents containing Personally Identifiable Information (PII). Clustering of titles from arxiv (q-fin). Clustering of abstract from arxiv (q-fin). Clustering the related industry domain according to the company description. MINDS-14 is dataset for intent detection in e-banking, covering 14 intents across 14 languages. Financial news categorization dataset. Clustering of financial events. Clustering of financial events. Clustering of financial events. Table 5: Summary of Clustering Datasets"
        },
        {
            "title": "Language Description",
            "content": "Ectsum (Mukherjee et al., 2022)"
        },
        {
            "title": "English",
            "content": "FINDSum (Liu et al., 2022)"
        },
        {
            "title": "English",
            "content": "English FNS-2022 (El-Haj et al., 2022) FiNNA (Lu et al., 2023) Chinese Fin-Eva (Headline) (Zhang et al., 2023) Chinese Fin-Eva (Abstract) (Zhang et al., 2023) Chinese Dataset For Bullet Point Summarization of Long Earnings Call Transcripts. Large-Scale Dataset for Long Text and Multi-Table Summarization. Financial Narrative Summarisation for 10K. financial news summarization dataset. financial summarization dataset. financial summarization dataset. Table 6: Summary of Summarization Datasets Dataset Name Language Description Fin-Fact (Rangapur et al., 2023) English FiQA2018 (FiQA, 2018) English HC3(Finance) (Guo et al., 2023) English Chinese Fin-Eva (Retrieval task) (Zhang et al., 2023) DISC-FinLLM (Retrieval Part Data) (Chen et al., 2023) Chinese Benchmark Dataset for Financial Fact Checking and Explanation Generation. Financial opinion mining and question answering. human-ChatGPT comparison finance corpus. Financial scenario QA dataset including retrieval task. Financial scenario QA dataset. Table 7: Summary of Reranking Datasets Dataset Name Language Description HeadlineAC-PairClassification (Sinha and Khandait, 2021) English HeadlinePDD-PairClassification (Sinha and Khandait, 2021) English HeadlinePDU-PairClassification (Sinha and Khandait, 2021) English Chinese AFQMC Financial text sentiment categorization dataset. Financial text sentiment categorization dataset. Financial text sentiment categorization dataset. Ant Financial Question Matching Corpus. Table 8: Summary of PairClassification Datasets Benchmark Sentence Length Token Length Syllables Per Token Dependency Distance MTEB FinMTEB 18.20 26. 4.89 5.12 1.49 1.52 2.49 2.85 Table 9: Comparison of Text Characteristics Between FinMTEB and MTEB. The numbers represent the average scores across all samples from all datasets. STS Class. Ret. Rerank. Clust. PairClass. Summ. Correlation 0.30 p-value 0. -0.80 0.10 0.30 0.62 -0.10 0.87 -0.70 0.18 -0.30 0.62 0.60 0. Table 10: Spearmans correlation of embedding models performance on MTEB and FinMTEB across different tasks. The p-value indicates that all correlations are statistically insignificant, suggesting lack of evidence for relationship between embedding model performance on the two benchmarks. 16 Figure 4: Semantic similarity across all the datasets in FinMTEB benchmark. 17 Task Factor Sum of Squares Degrees of Freedom F-Statistic p-value 6.00 1.00 6992.00 6.00 1.00 6992.00 6.00 1.00 6992.00 6.00 1.00 6992. 6.00 1.00 6992.00 6.00 1.00 7002.00 6.00 1.00 6992.00 6.00 1.00 6.00 25.55 2086.30 NA 9052.57 3207.72 NA 149.00 25761.71 NA 47.60 32161.37 NA 145.31 973.32 NA 489.05 346.78 NA 1.97 11989.92 NA 1.34 253.87 NA 3.41 1030 0 NA 0 0 NA 1.64 10178 0 NA 1.59 1057 0 NA 2.90 10174 3.60 10200 NA 0 1.39 1075 NA 0.07 0 NA 0.37 0 NA Classification Retrieval STS Clustering Summarization Reranking Pair Classification Average Model Factor Domain Factor Residual Model Factor Domain Factor Residual Model Factor Domain Factor Residual Model Factor Domain Factor Residual Model Factor Domain Factor Residual Model Factor Domain Factor Residual Model Factor Domain Factor Residual Model Factor Domain Factor Residual 4.17 56.82 190.42 104.25 6.16 13.42 10.55 304.09 82.53 0.29 32.25 7.01 12.98 14.49 104.07 5.38 0.64 12. 0.25 249.19 145.31 0.00 0.08 0.00 Table 11: Analysis of Variance (ANOVA) Results Across Tasks and Factors. Factor represents the independent variables analyzed: Model Factor pertains to variations attributed to different models, and Domain Factor pertains to variations due to different domains (MTEB or FinMTEB). Residual refers to the unexplained variance. The Sum of Squares, Degrees of Freedom, F-Statistic, and p-value are presented for each factor within each task. Asterisks denote significance levels, with lower p-values indicating higher statistical significance. The Domain Factor consistently shows high significance across all tasks."
        }
    ],
    "affiliations": [
        "The Hong Kong University of Science and Technology"
    ]
}