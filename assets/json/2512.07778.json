{
    "paper_title": "Distribution Matching Variational AutoEncoder",
    "authors": [
        "Sen Ye",
        "Jianning Pei",
        "Mengde Xu",
        "Shuyang Gu",
        "Chunyu Wang",
        "Liwei Wang",
        "Han Hu"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Most visual generative models compress images into a latent space before applying diffusion or autoregressive modelling. Yet, existing approaches such as VAEs and foundation model aligned encoders implicitly constrain the latent space without explicitly shaping its distribution, making it unclear which types of distributions are optimal for modeling. We introduce \\textbf{Distribution-Matching VAE} (\\textbf{DMVAE}), which explicitly aligns the encoder's latent distribution with an arbitrary reference distribution via a distribution matching constraint. This generalizes beyond the Gaussian prior of conventional VAEs, enabling alignment with distributions derived from self-supervised features, diffusion noise, or other prior distributions. With DMVAE, we can systematically investigate which latent distributions are more conducive to modeling, and we find that SSL-derived distributions provide an excellent balance between reconstruction fidelity and modeling efficiency, reaching gFID equals 3.2 on ImageNet with only 64 training epochs. Our results suggest that choosing a suitable latent distribution structure (achieved via distribution-level alignment), rather than relying on fixed priors, is key to bridging the gap between easy-to-model latents and high-fidelity image synthesis. Code is avaliable at https://github.com/sen-ye/dmvae."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 8 ] . [ 1 8 7 7 7 0 . 2 1 5 2 : r a"
        },
        {
            "title": "Distribution Matching Variational AutoEncoder",
            "content": "Sen Ye1,3 Jianning Pei2,3 Mengde Xu3 Shuyang Gu3 Chunyu Wang3 Liwei Wang1 Han Hu3 1 Peking University 2 UCAS 3 Tencent"
        },
        {
            "title": "Abstract",
            "content": "is trained to model the prior distribution p(z) of these latents. Most visual generative models compress images into latent space before applying diffusion or autoregressive modelling. Yet, existing approaches such as VAEs and foundation model aligned encoders implicitly constrain the latent space without explicitly shaping its distribution, making it unclear which types of distributions are optimal for modeling. We introduce Distribution-Matching VAE (DMVAE), which explicitly aligns the encoders latent distribution with an arbitrary reference distribution via distribution matching constraint. This generalizes beyond the Gaussian prior of conventional VAEs, enabling alignment with distributions derived from self-supervised features, diffusion noise, or other prior distributions. With DMVAE, we can systematically investigate which latent distributions are more conducive to modeling, and we find that SSL-derived distributions provide an excellent balance between reconstruction fidelity and modeling efficiency, reaching gFID equals 3.2 on ImageNet with only 64 training epochs. Our results suggest that choosing suitable latent distribution structure (achieved via distribution-level alignment), rather than relying on fixed priors, is key to bridging the gap between easy-to-model latents and high-fidelity image synthesis. Code is avaliable at https://github.com/senye/dmvae. 1. Introduction The field of visual synthesis has made staggering progress, with generative models now capable of producing highresolution, photorealistic images from complex text descriptions [25, 29]. dominant paradigm underpinning these successes is two-stage generation process [6, 28]. This approach first employs powerful autoencoder, or tokenizer, to compress high-dimensional image into compact, low-dimensional latent representation = E(x). Subsequently, powerful generative model, such as diffusion model [11, 31] or an autoregressive transformer [4, 24], Corresponding Author. The primary motivation for this compression is twofold: it simplifies the data distribution, and it drastically reduces the computational complexity of modeling in the highdimensional pixel space, thereby mitigating the curse of dimensionality. The efficacy of this entire pipeline, however, is critically dependent on the properties of the latent space . An ideal latent distribution must be evaluated by two key criteria: 1. Modeling Simplicity: The aggregate latent distribution q(z) = (cid:82) q(z x)p(x)dx must be simple and wellstructured, allowing the prior q(z) to be learned easily and efficiently. 2. Reconstruction Fidelity: The latent code must retain sufficient information to reconstruct the original image perfectly via the decoder G(z), maximizing p(x Finding distribution form that balances these two criteria is key challenge in designing effective generative models. z). Existing methods navigate this trade-off through various regularization strategies. The latent diffusion model [28], built upon VAE framework, enforce KL-constraint with simple Gaussian, which simplifies modeling at the cost of information loss. VQ-VAE [33] and VQ-GAN [4] employ vector quantization, constraining the latent space to finite, discrete codebook but introduces quantization artifacts. Recently, promising alternative uses fixed self-supervised learning (SSL) features (e.g. DINO [2, 21]) as the latent space. While semantically rich and easy to model, these encoders [41] are fixed and were not trained for reconstruction, leading to significant loss of fine-grained detail and poor reconstruction quality. To remedy this, SVG [30] adds learnable residual features, while VAVAE [36] and AlignTok [3] train the autoencoder end-to-end but add an auxiliary loss to align the learned latents with the fixed DINO features. While this improves the balance, the final latent distribution is still the implicit result of two competing objectives (reconstruction and regularization), which limits our ability to explore the impact of different distribution forms on modeling effectiveness. 1 In this paper, we propose Distribution Matching VAE (DMVAE), novel generative framework that generalizes the VAE objective. DMVAE explicitly constrains the encoders aggregate posterior q(z) to match an arbitrary, pre-defined reference distribution pr(z). This reference can be Gaussian, text-embedding distribution, or any empirically-defined distribution, such as the distribution of SSL features. To achieve this, we leverage Distribution Matching Distillation (DMD) [38], recent technique that aims to align two implicit distributions. DMD works by leveraging diffusion models as universal distribution estimators. It trains diffusion model vreal on the reference distribution pr(z) to learn its score function log pr(z, t). Our framework then trains the VAE by requiring that the score of its aggregate posterior, q(z), matches this pre-computed reference score. This process effectively distills the structure of pr(z) onto q(z), allowing us to mold the latent space into any desired shape, breaking free from the constraints of the Guassian distribution. DMVAE transforms the problem of autoencoder design into problem of reference distribution selection. This allows us, for the first time, to systematically explore what kind of distribution is more conducive to modeling. We conduct large-scale study evaluating various ref- (1) Gaussian distribution, (2) texterence distributions: embedding (SigLIP [40]) distribution, (3) supervised learning (ImageNet classifier [7]) feature distribution, (4) selfsupervised learning (DINO [2, 21]) feature distribution, and (5) noise distribution from intermediate diffusion [28] steps. Our findings are decisive. We discover that different distribution forms have significantly different impacts on the difficulty of modeling and the quality of reconstruction. Our key finding is that SSL-derived features provide an excellent prior distributionthey are semantically structured enough to be trivial to model, yet sufficiently rich in information to permit high-fidelity reconstruction, demonstrating their superiority as target distribution. In summary, our contributions are: 1. We propose Distribution Matching VAE (DMVAE), generalization of the VAE framework that can match the latent aggregate posterior to any pre-defined reference distribution. 2. We conduct the first large-scale, systematic study of various latent priors, systematically evaluating the pros and cons of different latent priors (e.g., SSL feature distributions) as modeling targets. 3. Using DMVAE as tokenizer with an SSL prior, our model achieves state-of-the-art gFID of 3.22 on ImageNet 256x256 with only 64 training epochs and gFID of 1.82 with 400 training epochs, demonstrating the profound impact of optimizing the latent prior. 2. Preliminary We first define the notation used throughout the paper. Let be an input image from the data distribution p(x). tokenizer consists of an encoder : that maps the image to latent code = E(x), and decoder : that reconstructs the image ˆx = G(z). The core problem is to design and such that is both informative (low reconstruction error) and easy to model (simple aggregate posterior q(z) = (cid:82) q(z x)p(x)dx). 2.1. Variational Autoencoders (VAEs) The standard β-VAE [13] framework trains the encoder x)) and decoder (which param- (which parameterizes q(z eterizes p(x z)) by optimizing the Evidence Lower Bound (ELBO): z)] βDKL(q(z VAE = Exp(x) (cid:2) Eq(zx)[log p(x p(z))(cid:3) (1) where p(z) is fixed, simple prior, typically standard Gaussian (0, I). The KL-divergence term acts as regux) to be close larizer, forcing the per-sample posterior q(z to p(z), which in turn simplifies the aggregate posterior q(z). x) While large β was initially intended for strong regularization to ensure generative capacity, it frequently causes model collapse. Consequently, in latent diffusion models which use VAE as tokenizer, β is often set to very small value (e.g., e6 in [28]). 2.2. Inject distribution prior into VAEs Recent works have explored moving beyond simple Gaussian prior. VAVAE [3, 36] proposes to explicitly align the latent codes = E(x) with the features from pre-trained DINO model, denoted as ϕ(x). This is implemented as an auxiliary pairwise similarity loss, balancing reconstruction with alignment: VAVAE = where sg[ Recon + λDpair(E(x) sg[ϕ(x)])2 (2) ] denotes stop-gradient operator. This encourages E(x) to inherit the semantic properties of ϕ(x). Conceptually, setting λ to an extremely large value can be seen as freezing the DINO model and using it directly as the encoder [41]. Diffusion Priors [17] attempts to directly use pretrained diffusion model as the prior p(z). The score of this powerful prior, log p(z), is used to optimize the encoder by maximizing the likelihood of the encoders output under p(z): flowprior = Et,xp(x),ϵN (0,I)[ vθ(E(x), t) (ϵ 2] E(x)) (3) While flexible, directly backpropagating the diffusion loss through the encoder is notoriously unstable and can lead to posterior collapse[22]. Adversarial Autoencoders (AAE) [20] uses GANbased objective to shape the latent distribution. An auxiliary discriminator is trained to distinguish between samples from the aggregate posterior q(z) and reference prior pr(z). The encoder is simultaneously trained as generator to fool D: Recon+ AAE = max min Ezpr(z)[log D(z)] + Exp(x)[log( D(E(x)))] (4) This allows q(z) to match pr(z). However, AAEs are limited by the instability of GAN training and the expressive capacity of D, which has practically restricted pr(z) to simple distributions like Gaussian mixtures. Our work aims to overcome this limitation by utilizing diffusion, which allows pr(z) to be more complex distributions. 3. Distribution Matching VAE The Pitfall of Per-Sample Regularization. Our goal is to solve the Tokenizers Dilemma by gaining explicit control over the aggregate posterior distribution q(z) = (cid:82) q(z x)p(x)dx. Previous methods [3, 36] attempt this indirectly via per-sample regularization, which is fundamentally insufficient. The classic VAE loss (Eq. (1)) regularizes the per-sample posterior q(z x). While this forces each x) towards the prior p(z), it places no explicit conq(z straint on the mixture of these posteriors, q(z). If the decoder is powerful, the encoder can learn to map different inputs to distinct, well-separated modes, (i.e., q(z x) collapses to delta function). The resulting aggregate posterior q(z) becomes complex, multi-modal, and holey manifold [26, 27], which is notoriously difficult for generative prior p(z) to model. Similarly, methods like VAVAE [3] (Eq. (2)) use per-sample distance loss, which only guarantees that each E(x) is near its corresponding ϕ(x), but does not constrain the global structure of the q(z) manifold. Instead, we turn to diffusion models. key insight is that any distribution p(z) is uniquely characterized by its timedependent score function log pt(zt) [31]. Therefore, we can utilize diffusion model as universal distribution estimator and compute the gradient of DKL(q(z) pr(z)) through their score functions, following the approach in [38] 3.1. The DMVAE Objective. Our method does not pre-compute and distill fixed score, as this fails when the student distribution (our q(z)) is constantly changing. Instead, we follow the joint-training mechanism of Distribution Matching Distillation [38]. Our framework involves real (teacher) score model ), ) and fake (student) score model sfake( sreal( trained alongside the VAE (Eθ, Gω). , , Step 1: Pre-train Reference Score Model (Teacher). First, we train time-conditioned velocity network vreal( ) on the fixed reference distribution pr(z) using the flow matching objective [18]: , (cid:2) (ϵ vreal(αtz0 + σtϵ, t) = Et,z0pr(z),ϵN (0,I) 2 z0) 2 (5) Once trained, vreal is frozen and serves as the canonical representation of our reference distribution. The score log pr,t(zt) can be obtained by linsreal(zt, t) = ear transformation of velocity vreal using SDE formulations [19]. (cid:3) Step 2: Joint Training of VAE and Fake Score Model (Student). We then jointly train the VAE (Eθ, Gω) and the fake score model sfake. The fake models job is to learn the score function of the encoders aggregate posterior q(z), even as q(z) changes during training. The VAE is, in turn, trained to change its output = E(x) so that the aggregated posterior q(z) matches with reference distribution pr(z). This results in total objective with three components: L Total = fm(Eθ, sϕq ) + λ recon(Eθ, Gω) + γ DM(Eθ, sϕq ) (6) Reconstruction Loss (Trains Eθ, Gω): we follow [28, 36] to leverage standard perceptual loss and GAN loss to ensure fidelity: L Score-Based Distribution Matching. To overcome this, we must impose constraint directly on the aggregate distribution q(z). We propose the Distribution Matching VAE (DMVAE), which forces q(z) to match an arbitrary, predefined reference distribution pr(z). The challenge is that both q(z) (from the encoder) and pr(z) (e.g., SSL features) are complex, implicit distribupr(z)). tions. We cannot tractably compute DKL(q(z) While an adversarial loss (Eq. (4)) is one option, it is unstable and expressively limited. recon = Exp(x) [d(x, Gω(Eθ(x)))] (7) Fake Score Model Loss (Trains sfake): This loss trains sfake to capture the score function for the current q(z). We stop the gradient of Eθ so that this loss only updates vfake. In practice implementation, we adopt flow matching objective to learn vfake and convert it to sfake: fm = Et,xp(x),ϵ (cid:2) vfake(αtz0 + σtϵ, t) (ϵ z0) 2 2 (cid:3) (8) 3 Figure 1. Illustrution of VAE [13], RAE [41], pointwise matching encoder [3, 36], and Distribution Matching VAE. where z0 is StopGradient[Eθ(x)]. Distribution Matching Loss (Trains Eθ): It penalizes the encoder Eθ by comparing the scores of the student and teacher models. Following [38], we compute the gradient directly without back-propagating through the fake score network: DM θL Et,x[wt(sfake(zt, t) sreal(zt, t)) dEθ(x) dθ ] (9) where zt = αtEθ(x) + σtϵ. wt are weighting functions to balance different noise levels following [38]. By default, is sampled uniformly from [0, 1]. In summary, this joint training creates dynamic where fm forces sfake to track the score of the aggregate posterior DM pushs Eθ to ensure q(z) structurally aligns q(z), and with pr(z). Step 3: Decoder refinement. After the joint training converges, we can optionally perform decoder refinement. Following [3], we freeze the encoder and finetune the decoder for few iterations using only the reconstruction loss. This step allows the decoder to fully adapt to the latent manifold, which can yield slight improvement in final reconstruction fidelity. Per-Sample vs. Distributional Constraints. The distribution matching formulation (Eq. (6)) is fundamentally different from VAEs (Eq. (1)) or VAVAE (Eq. (2)). The key difference lies in how they behave under the inevitable tenRecon and the prior sion between the reconstruction loss constraint. Per-sample losses, such as the VAEs KL-divergence or VAVAEs pairwise distance, optimize an expectation of (Eθ(x), ...)]. This objective can be delocal loss: Ex[ ceptively minimized. Consider special case: For half the align dominates the other data Recon dominates and , xA} { { half . Under the joint optimization, suppose the laxB} tents Eθ(xA) remain on the original reconstruction manirecon, and Eθ(xB) are pulled perfectly onto the prior fold pr(z). In this case, the total average alignment loss E[ align] is halved. However, the aggregate posterior q(z) has degenerated into disjoint mixture q(z) recon + 0.5 pr(z). This topologically damaged manifold is arguably worse for generative modeling than the original recon, despite the seemingly low average aligned loss. 0.5 In contrast, our DMVAE objective optimizes global geometric divergence DKL(q(z) pr(z)). This loss cannot be deceptively minimized in this way. The score function log q(z) of the disjoint mixture distribution described above is globally different from the (presumably smoother) score log pr(z) of the target prior. Therefore, sfake (which must track this complex score) will fail to match sreal, causing the distribution matching loss to remain high. Even if DM will always push q(z) towards holistic structural alignment with pr(z), resulting in denser, non-holey manifold that is far more suitable for generative prior. Recon prevents perfect match, In pracRde Matching the Dimensionality of Distributions. tice, the latent dimension of our encoder Eθ : may not match the reference distribution pr(z) (where Rdr ). To resolve this, we introduce light-weight, learnable projection head Hϕ : Rde Rdr . Specially, for projection to spatially aligned reference distribution, the head is two-layer MLPs and otherwise, the head is two-layer transformer decoder with learnable queries [1]. As depicted in Fig. 2, the data flow is as follows: The encoder produces latent code ze = Eθ(x). This ze is used by the decoder for reconstruction, Gω(ze). Concurrently, ze is passed through the projector to get z0 = Hϕ(ze), and zt is obtained by adding noise to it will be used in the DM loss (Eq. (9)). The gradient flow is crucial: The decoder Gω is optimized only by Recon. 4 Figure 2. The training pipeline of Distribution Matching VAE. The projection head Hϕ is optimized only by The encoder Eθ is optimized by both The diffusion loss is only used to update the fake diffusion Recon and DM. DM. model(vfake). This architecture forces Eθ to learn representation ze that is simultaneously optimal for reconstruction and matches the structure of the reference distribution pr(z). 3.2. Stabilizing the Distant Distribution Matching. We empirically found that the joint-training objective (Eq. (6)) can be unstable when the aggregate posterior q(z) is far from the reference pr(z). This stems from fundamental dilemma in score matching [31, 38]. On one hand, using large noise scale σt ensures support overlap, but this level of smoothing washes out the fine-grained, high-frequency differences between the disDM and tributions, leading to flat loss landscape for causing vanishing gradients. On the other hand, small noise scale σt preserves distribution details but, when q(z) and pr(z) is distant, it suffers from disjoint supports. This causes highvariance, explosive gradients that destabilize the entire joint-training process and can lead to mode collapse. Crucially, unlike prior works [12, 38] leveraging distribution matching for distillationwhere an implicit pairing or regression loss keeps the distributions proximateour framework is, to our knowledge, the first to use this mechanism to match the entire aggregate posterior q(z) of an autoencoder to an unpaired reference pr(z). The potentially pr(z)) makes the system vast initial discrepancy D(q(z) highly susceptible to the aforementioned instabilities. To effectively mitigate these inherent training difficulties and ensure convergence, we adopts several stablizing training strategies: we utilize the pretrained weights of the real score model to initialize the fake score model and train the VAE from pretrained encoder, providing stable starting point; implement alternating training between the VAE and the fake score model to balance the learning dynamics [38]. Furthermore, we reduce all latent spaces and priors to low dimension (e.g., = 32) with reconstruction objective. This step is critical as it helps mitigate the curse of dimensionality, thus reducing the sparsity and computational challenges associated with distribution matching in high-dimensional spaces. 3.3. Variations of Distribution Matching Objective To investigate the optimal objective for aligning the aggregate posterior q(z) with the reference prior pr(z), we analyze several distinct gradient fields derived from different score-based objectives. We visualize the learned latent distribution in 2D toy experiment by these objectives, as shown in Figure 3, and mathematically analyze their gradients to understand their convergence behaviors. For brevity, we omit time-dependent weights wt and the chain rule term dEθ(x) that acts on the dθ , focusing on the gradient field zL latent samples = Eθ(x). Real Score Maximization. This objective also referred to as Flow Prior [17], maximizes the likelihood of the encoders output under fixed, pre-trained reference teacher model sreal. This corresponds to minimize real model flow matching loss. Depending on the stop-gradient placement, we identify three variations, illustrated in Figure 3 (d, e, f). As the gradient update pushes latent codes solely towards high-density regions of pr(z), this objective lacks mechanism to encourage entropy or coverage. Consequently, it suffers from mode dropping, where the latent distribution collapses into few modes rather than covering the entire manifold. 5 for minimizing this objective is for the latent space to contract to single or few points, which is trivially easy to model but useless for generation. sfake(z) Score Differences minimization. An intuitive idea is to minimizes the L2 distance between score functions 2 thereby optimizing the input distribution. While theoretically sound, our experiments show that optimization is unstable, failing to align the distributions effectively as shown in Figure 3 (j, k, l). sreal(z) z) vfake 2 Loss Differences minimization. This objective minimizes the difference in flow matching losses: 2. While it successfully sg(ϵ aligns the distribution, the gradient computation involves the Jacobian of the score network, which is computationally expensive and memory-intensive for high-dimensional image latent spaces. sg(ϵ vreal z) = Ours: Distribution Matching objective. The distribution matching objective overcomes the aforementioned limitationsmode collapse, mode dropping, and computational inefficiency. The gradient is approximated as the difference between score functions: sreal(z). Crucially, this objective constructs difference vector field that pulls them towards the high-density modes of pr(z) (via +sreal) and avoids mode collapse to single mode through sfake. As illustrated in Figure 3 (b), this balanced dynamic achieves precise distribution alignment that preserves the global structure of the reference without the Jacobian overhead. sfake(z) zL DM Figure 3. Analysis of different distribution matching objectives on 2D setting. (a) illustrates the reference distribution; (b) denotes the distribution matching objective, (d,e,f) represent real score maximization with different stopping gradients; (g,h,i) represent methods for fake score maximization; (j,k,l) represent directly optimizing the difference between real and fake scores; Finally, (c) represents optimizing the difference between the real and fake diffusion losses. For each objective, we have listed the loss function and their gradient zL. Fake Score Maximization (End-to-End). This approach jointly trains the encoder and the diffusion model (student sfake) without an explicit reference anchor. It aims to minimize the denoising error of the student model on its own generated samples. Similar to the failure mode discussed in REPA-E [15], this creates self-reinforcing loop leads to mode collapse, which is more severe than real score maximization. As shown in Figure 3 (g, h, i), the trivial solution 4. Selection of Reference Distributions key advantage of DMVAE is its ability to flexibly incorporate any distribution as the latent prior pr(z). This allows us, for the first time, to systematically investigate the Tokenizers Dilemma: what choice of pr(z) yields the best balance of reconstruction fidelity and modeling simplicity? We conduct our primary investigation on the ImageNet 256x256 dataset. We categorize our candidate reference distributions into two families: Category 1: Data-Derived Distributions. These priors are derived directly from the ImageNet training data, and are therefore semantically aligned with the data manifold. SSL Features: The aggregate posterior of pre-trained DINOv2-ViT/L [21] model. Supervised Features: The aggregate posterior of supervised ResNet-34 [7] classifier. Text Features: We generate captions for ImageNet and extract features using pre-trained SigLIP [40] text encoder. Table 1. Variations for different reference distributions. Ref distribution rFID PSNR gFID-5k VAE-baseline DINO Resnet SigLIP-text Difftraj"
        },
        {
            "title": "SubDino\nGMM\nGaussian",
            "content": "0.54 0.81 1.46 1.63 0.60 0.29 0.42 0.47 25.7 21.8 20.9 24.0 26.9 25.6 27.3 27. 27.3 13.1 18.6 26.8 31.8 37.9 29.6 26.6 Diffusion Noise States: The aggregate distribution of noisy latents zt (at = 0.5) from pre-trained LDM [28] on ImageNet. Category 2: Data-Independent Distributions. These priors are either synthetic or are not representative of the full data distribution. Sub-sampled SSL features: DINO features extracted from only 10 classes of ImageNet, creating multi-modal distribution simpler than the full prior. Standard Gaussian: simple VAEs. (0, I), as used in classic Gaussian Mixture Model (GMM): 10-component GMM, representing simple, multi-modal synthetic baseline. Experimental Setting. fair comparison is challenging, as the difficulty of matching q(z) to pr(z) depends on the distance and structural similarity between pr(z) and the original data manifold. As preliminary setting for this exploration, we adopt heuristic-based weighting. For dataderived priors, which are presumably closer to the original data, we set higher matching weight of λDM = 10. For synthetic priors, we set λDM = 1. All other loss weights are held constant. All models are trained for 300k iterations with batch size 256. The results, summarized in Tab. 1, show that the selfsupervised DINO features provide the best overall balance of reconstruction quality and generative modeling performance. t-SNE Visualization and Analysis. To understand why the SSL prior yields superior results, we performed t-SNE visualization. We randomly sampled 100 images from 20 distinct ImageNet classes. We then visualized the reference distribution pr(z) and the latents q(z) from our DMVAE trained to match reference distribution. The results are shown in Fig. 4. Illustration of t-SNE on different distributions. (a-d) Figure 4. represent four different reference distributions, (e-h) represent the distribution of the DMVAE encoder learned from these four reference distributions, (i-k) represent the encoder distribution learned from the data-independent distribution, and (l) represents the distribution of the β-VAE encoder output. The visualization clearly shows that the original DINO features ( Fig. 4 a) possess superior semantic clustering. This inherent semantic organization likely reduces the complexity for the subsequent diffusion modelling, as it can focus on modeling intra-class variations rather than struggling with semantic confusion. Furthermore, the DMVAE latents ( Fig. 4 e) successfully replicate this strong clustering structure, demonstrating that the distribution matching constraint effectively enforces the global geometry of the reference prior. In contrast, the β-VAE latents ( Fig. 4 l) or DMVAE constrained by data-independent distributions demonstrated in the last row, struggles on distinguish different semantic modes. This confirms more structured and semantically meaningful latent space is crucial for efficient generation. 5. Experiments To investigate the effectiveness of DMVAE, we conducted all experiments using the common ImageNet-256 setting. We first trained the DMVAE model on the ImageNet training set and evaluated its reconstruction quality on the validation set using PSNR and rFID. To measure the modeling difficulty of the acquired latent representation, we then trained Lightning-DiT [36] diffusion model and measured its generative quality using gFID. By default, the reported generation results do not employ guidance strategies such as Classifier-free Guidance [10]. 7 Table 2. Ablation studies on diffusion matching weight, network size, CFG weight and timestep schedule. Factor"
        },
        {
            "title": "CFG Weight",
            "content": "Setting (λ = 10, L-Net, CFG=1.0, Uniform) λDM = 1 λDM = 20 λDM ="
        },
        {
            "title": "Small\nXL",
            "content": "3.0 5."
        },
        {
            "title": "Timestep Schedule",
            "content": "[0,1] annealing to [0,0.5] PSNR 22.1 25.2 21.4 19.6 22.9 21. 20.6 19.4 21.9 rFID 0.44 0.37 0.78 0. 0.67 0.91 1.02 1.26 0.55 gFID-5k 13. 16.7 12.5 12.6 13.9 12.5 11.6 11.5 12.7 Table 3. Class-conditional generation results on ImageNet 256256. Method Reconstruction Training Epochs #Params PSNR rFID AutoRegressive (AR) Generation gFID IS LlamaGen [32] MAR [16] 24.45 24.08 0.59 0.87 300 800 3.1B 945M 9.38 2. 112.9 227.8 Latent Diffusion Models SiT [19] FasterDit [35] RAE(DiTDH-XL) [41] RAE(DiT-XL) [41] VA-VAE [36] VA-VAE [36] AlignTok [3] AlignTok [3] DMVAE DMVAE DMVAE 26.0 26.0 18.9 18.9 27.6 27.6 25.8 25.8 21.5 21.5 21.5 0.61 0.61 0.57 0.57 0.28 0.28 0.26 0.26 0.64 0.64 0.64 1400 400 800 800 64 800 64 800 64 400 675M 675M 839M 675M 675M 675M 675M 675M 675M 675M 675M 8.61 7.91 1.51 1.87 5.14 2.17 3.71 2.04 3.22 1.82 1.64 131.7 131.3 242.9 209.7 130.2 205.6 148.9 206.2 171.7 206.9 216.3 sampled from [0, 1]. Besides, we do not adopt CFG [10] for both real and fake diffusion models and the evaluation. Each tokenizer is trained for 250K iterations and we use DiT-L model trained with 300K iterations with batch size of 256. We vary each factor independently, with results reported in Tab. 2. DM Weight (λDM): weight of 10 provides the best balance. lower weight (e.g., 1) results in poor generative quality due to weak regularization. Conversely, higher weight (e.g., 100) strengthens the generative prior but degrades reconstruction performance by enforcing an overly strict constraint. Score Network Size: We observe that larger score network, which can more accurately model the reference and latent distribution, leads to improved distribution matchFigure 5. Different CFG scale represents different distributions. 5.1. Ablation Studies Many factors influence the training of our tokenizer. We ablate the most critical hyperparameters, starting from default configuration: we adopt DINO to provide reference distributions, λDM = 10, and use Lightning-DiT-L for both real and fake score network, and timestep is uniformly 8 [33] pioneered this by introducing Vectorparadigm). Quantized Variational Autoencoder, which enabled autoregressive modeling in discrete latent space, successfully moving beyond direct pixel-space operations. Building on this, VQGAN [4] further improved performance by integrating an adversarial loss into the VAE training objective, which significantly enhanced the perceptual quality and synthesis of fine details in reconstructed images. Concurrently, Latent Diffusion Models [28] adapted this paradigm by training VAE [13] with modest KL regularization, and subsequently modeling the resulting continuous latent distribution using diffusion process. Recently, researches [36, 39] has highlighted the reconstruction-generation dilemma, noting that improved reconstruction quality does not always correlate with superior generative performance, suggesting potential tradeoff [36]. This observation has spurred research into alternative regularization methods. To mitigate this, approaches like VA-VAE [36] and AlignTok [3] propose aligning the VAEs latent representations with the semantic features extracted from powerful, pre-trained Vision Foundation Models (VFMs) [8, 21, 23, 40]. Furthermore, RAE [41] explored using frozen VFM as the encoder, training only corresponding decoder for reconstruction. However, this strategy often results in poor reconstruction quality, suffering from significant loss of fine-grained visual details. While these VFM-based methods initially explored semantic regularization, they all rely on point-wise alignment (e.g., minimizing distance between individual latent vectors and VFM features). Unlike these methods, we expand the concept of regularization to distribution matching and directly shape the entire latent distribution to desired prior, leading to more globally consistent and generativefriendly latent space. Distribution Matching Distribution matching is longstanding problem in generative modeling, focusing on finding transformation or mapping (transport) between different probability distributions. Variational Autoencoders (VAEs) [9, 13] apply this by aiming to match the encoded hidden states of images to simple Gaussian distribution, which then enables image generation from randomly sampled Gaussian vectors. Adversarial Autoencoders (AAE) [20] addressed the common mode collapse issue [26, 27] of VAEs by introducing adversarial learning [5] to enforce the alignment of the image latent distribution with an arbitrary prior. However, AAE struggles to match complex target distributions due to the limited capacity of the standard discriminator. The most related technique to our work is the Variational Score Matching [34] used in diffusion model distillation [34, 37, 38, 42]. This technique successfully distills student generative model from teacher by directly comparing and aligning their reFigure 6. Comparison of the convergence speed. ing and better final generative quality. Classifier-Free Guidance (CFG): While our default setting disables CFG (guidance weight of 1.0), we find that applying small CFG weight (e.g., 3.0) during the score matching process (Eq. (9)) can slightly improve generative quality, albeit at minor cost to reconstruction. We visualize the how the real diffusion models the reference distribution under different CFG weights in Fig. 5. We find that suitable CFG brings stronger semantic clustering properties, which can better accelerate convergence. When CFG scale is extremely large, the distribution may be disjoint that makes training unstable. Timestep schedule: We compare our default uniform sampling (t [0, 1]) with noise schedule annealing strategy. Following [34], the annealing setting starts at [0, 1] and linearly contracts the sampling range to [0, 0.5]. We find that annealing provides slight improvement by focusing on more precise, low-noise matching in the later stages of training. 5.2. Comparison with other methods Finally, we compare our DMVAE tokenizer against other state-of-the-art tokenizer methods. We generate 50000 samples through 250 sampling steps without CFG. We show the results in Tab. 3. DMVAE outperforms all the competing methods. With merely 400 training epochs, DMVAE surpasses the performance of the state-of-the-art methods, including RAE(DiT-XL) and AlignTok, achieving 2 improvement in training efficiency. Besides, we also plot the generation quality of diffusion model in the training process in Fig. 8. DMVAE enables significantly faster convergence than baseline methods. 6. Related Work Visual Tokenizers Visual Tokenizers aim to compress high-dimensional image data into compact latent representation, strategy that has become fundamental to modern visual generative models (the compress then model 9 spective score functions. We adopt this distribution matching technique, but apply it to match the hidden representations of visual tokenizers with predefined prior distribution. This distribution-level regularization allows us to shape better-structured and more generative latent space for subsequent image modeling. 7. Conclusion and Limitation In this work, we introduced the Distribution Matching VAE (DMVAE), framework that, for the first time, allows the aggregate posterior of an autoencoder to be constrained at the distributional level. This enabled us to systematically explore the core question of what kind of latent distribution is more beneficial for modeling. Our key finding is that using semantically-rich, self-supervised features like DINO as reference distribution performs excellently in terms of both reconstruction fidelity and generative tractability, proving the superiority of this type of distribution. This approach is highly efficient, achieving an excellent gFID of 3.22 on ImageNet after only 64 epochs of training. We believe DMVAE offers significant advancement for all twostage generative models and can be broadly applied to tasks in audio, video, and 3D generation. While our method is powerful, we acknowledge that matching distributions that are initially far apart remains challenge, often requiring careful tuning. This makes the reference distribution more like regularizer than complete matching, which limits our precise control over the tokenizers output distribution. In the future, we would focus on developing more robust optimization techniques for this distant-matching problem. Solving this will fully unlock the potential of designing generative latent spaces."
        },
        {
            "title": "References",
            "content": "[1] Nicolas Carion, Francisco Massa, Gabriel Synnaeve, Nicolas Usunier, Alexander Kirillov, and Sergey Zagoruyko. End-toend object detection with transformers. In European conference on computer vision, pages 213229. Springer, 2020. 4 [2] Mathilde Caron, Hugo Touvron, Ishan Misra, Herve Jegou, Julien Mairal, Piotr Bojanowski, and Armand Joulin. Emerging properties in self-supervised vision transformers. In Proceedings of the IEEE/CVF international conference on computer vision, pages 96509660, 2021. 1, 2 [3] Bowei Chen, Sai Bi, Hao Tan, He Zhang, Tianyuan Zhang, Zhengqi Li, Yuanjun Xiong, Jianming Zhang, and Kai Zhang. Aligning visual foundation encoders to tokenizers for diffusion models. arXiv preprint arXiv:2509.25162, 2025. 1, 2, 3, 4, 8, 9, 12 [4] Patrick Esser, Robin Rombach, and Bjorn Ommer. Taming In Protransformers for high-resolution image synthesis. ceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 1287312883, 2021. 1, 9, 12 [5] Ian Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley, Sherjil Ozair, Aaron Courville, and Yoshua Bengio. Generative adversarial networks. Communications of the ACM, 63(11):139144, 2020. 9 [6] Shuyang Gu, Dong Chen, Jianmin Bao, Fang Wen, Bo Zhang, Dongdong Chen, Lu Yuan, and Baining Guo. Vector quantized diffusion model for text-to-image synthesis. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 1069610706, 2022. 1 [7] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recognition. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 770778, 2016. 2, [8] Kaiming He, Xinlei Chen, Saining Xie, Yanghao Li, Piotr Dollar, and Ross Girshick. Masked autoencoders are scalable vision learners. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 16000 16009, 2022. 9 [9] Irina Higgins, Loic Matthey, Arka Pal, Christopher Burgess, Xavier Glorot, Matthew Botvinick, Shakir Mohamed, and Alexander Lerchner. beta-vae: Learning basic visual concepts with constrained variational framework. In International conference on learning representations, 2017. 9 [10] Jonathan Ho and Tim Salimans. Classifier-free diffusion guidance. arXiv preprint arXiv:2207.12598, 2022. 7, 8 [11] Jonathan Ho, Ajay Jain, and Pieter Abbeel. Denoising diffusion probabilistic models. Advances in neural information processing systems, 33:68406851, 2020. 1 [12] Xun Huang, Zhengqi Li, Guande He, Mingyuan Zhou, Self forcing: Bridging the trainand Eli Shechtman. test gap in autoregressive video diffusion. arXiv preprint arXiv:2506.08009, 2025. 5 [13] Diederik Kingma and Max Welling. Auto-encoding variational bayes. arXiv preprint arXiv:1312.6114, 2013. 2, 4, 9 [14] Black Forest Labs. Flux. https://github.com/ black-forest-labs/flux, 2024. [15] Xingjian Leng, Jaskirat Singh, Yunzhong Hou, Zhenchang Xing, Saining Xie, and Liang Zheng. Repa-e: Unlocking vae for end-to-end tuning with latent diffusion transformers. arXiv preprint arXiv:2504.10483, 2025. 6 [16] Tianhong Li, Yonglong Tian, He Li, Mingyang Deng, and Kaiming He. Autoregressive image generation without vector quantization. Advances in Neural Information Processing Systems, 37:5642456445, 2024. 8 [17] Yizhuo Li, Yuying Ge, Yixiao Ge, Ying Shan, and Ping Luo. Aligning latent spaces with flow priors. arXiv preprint arXiv:2506.05240, 2025. 2, 5 [18] Yaron Lipman, Ricky TQ Chen, Heli Ben-Hamu, Maximilian Nickel, and Matt Le. Flow matching for generative modeling. arXiv preprint arXiv:2210.02747, 2022. 3 [19] Nanye Ma, Mark Goldstein, Michael Albergo, Nicholas Boffi, Eric Vanden-Eijnden, and Saining Xie. Sit: Exploring flow and diffusion-based generative models with scalable interpolant transformers. In European Conference on Computer Vision, pages 2340. Springer, 2024. 3, 8 [20] Alireza Makhzani, Jonathon Shlens, Navdeep Jaitly, Ian Goodfellow, and Brendan Frey. Adversarial autoencoders. arXiv preprint arXiv:1511.05644, 2015. 3, 10 [35] Jingfeng Yao, Cheng Wang, Wenyu Liu, and Xinggang Wang. Fasterdit: Towards faster diffusion transformers training without architecture modification. Advances in Neural Information Processing Systems, 37:5616656189, 2024. 8 [36] Jingfeng Yao, Bin Yang, and Xinggang Wang. Reconstruction vs. generation: Taming optimization dilemma in latent diffusion models. In Proceedings of the Computer Vision and Pattern Recognition Conference, pages 1570315712, 2025. 1, 2, 3, 4, 7, 8, 9, 12 [37] Tianwei Yin, Michael Gharbi, Taesung Park, Richard Zhang, Eli Shechtman, Fredo Durand, and Bill Freeman. Improved distribution matching distillation for fast image synthesis. Advances in neural information processing systems, 37:4745547487, 2024. 9, 12 [38] Tianwei Yin, Michael Gharbi, Richard Zhang, Eli Shechtman, Fredo Durand, William Freeman, and Taesung Park. One-step diffusion with distribution matching distillation. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 66136623, 2024. 2, 3, 4, 5, 9 [39] Lijun Yu, Jose Lezama, Nitesh Gundavarapu, Luca Versari, Kihyuk Sohn, David Minnen, Yong Cheng, Vighnesh Birodkar, Agrim Gupta, Xiuye Gu, et al. Language model beats diffusiontokenizer is key to visual generation. arXiv preprint arXiv:2310.05737, 2023. 9 [40] Xiaohua Zhai, Basil Mustafa, Alexander Kolesnikov, and Lucas Beyer. Sigmoid loss for language image pre-training. In Proceedings of the IEEE/CVF international conference on computer vision, pages 1197511986, 2023. 2, 6, [41] Boyang Zheng, Nanye Ma, Shengbang Tong, and Saining Xie. Diffusion transformers with representation autoencoders. arXiv preprint arXiv:2510.11690, 2025. 1, 2, 4, 8, 9 [42] Mingyuan Zhou, Huangjie Zheng, Zhendong Wang, Mingzhang Yin, and Hai Huang. Score identity distillation: Exponentially fast distillation of pretrained diffusion models for one-step generation. In Forty-first International Conference on Machine Learning, 2024. 9 [21] Maxime Oquab, Timothee Darcet, Theo Moutakanni, Huy Vo, Marc Szafraniec, Vasil Khalidov, Pierre Fernandez, Daniel Haziza, Francisco Massa, Alaaeldin El-Nouby, et al. Dinov2: Learning robust visual features without supervision. arXiv preprint arXiv:2304.07193, 2023. 1, 2, 6, 9 [22] Ben Poole, Ajay Jain, Jonathan Barron, and Ben Mildenhall. Dreamfusion: Text-to-3d using 2d diffusion. arXiv preprint arXiv:2209.14988, 2022. 3 [23] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, et al. Learning transferable visual models from natural language supervision. In International conference on machine learning, pages 87488763. PmLR, 2021. 9 [24] Aditya Ramesh, Mikhail Pavlov, Gabriel Goh, Scott Gray, Chelsea Voss, Alec Radford, Mark Chen, and Ilya Sutskever. Zero-shot text-to-image generation. In International conference on machine learning, pages 88218831. Pmlr, 2021. 1 [25] Aditya Ramesh, Prafulla Dhariwal, Alex Nichol, Casey Chu, and Mark Chen. Hierarchical text-conditional image generation with clip latents. arXiv preprint arXiv:2204.06125, 1 (2):3, 2022. [26] Ali Razavi, Aaron van den Oord, Ben Poole, and Oriol Vinyals. Preventing posterior collapse with delta-vaes. arXiv preprint arXiv:1901.03416, 2019. 3, 9 [27] Danilo Jimenez Rezende and Fabio Viola. Taming vaes. arXiv preprint arXiv:1810.00597, 2018. 3, 9 [28] Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Bjorn Ommer. High-resolution image In Proceedings of synthesis with latent diffusion models. the IEEE/CVF conference on computer vision and pattern recognition, pages 1068410695, 2022. 1, 2, 3, 7, 9 [29] Chitwan Saharia, William Chan, Saurabh Saxena, Lala Li, Jay Whang, Emily Denton, Kamyar Ghasemipour, Raphael Gontijo Lopes, Burcu Karagol Ayan, Tim Salimans, et al. Photorealistic text-to-image diffusion models with deep language understanding. Advances in neural information processing systems, 35:3647936494, 2022. 1 [30] Minglei Shi, Haolin Wang, Wenzhao Zheng, Ziyang Yuan, Xiaoshi Wu, Xintao Wang, Pengfei Wan, Jie Zhou, and Jiwen Lu. Latent diffusion model without variational autoencoder. arXiv preprint arXiv:2510.15301, 2025. [31] Yang Song, Jascha Sohl-Dickstein, Diederik Kingma, Abhishek Kumar, Stefano Ermon, and Ben Poole. Score-based generative modeling through stochastic differential equations. arXiv preprint arXiv:2011.13456, 2020. 1, 3, 5 [32] Peize Sun, Yi Jiang, Shoufa Chen, Shilong Zhang, Bingyue Peng, Ping Luo, and Zehuan Yuan. Autoregressive model beats diffusion: Llama for scalable image generation. arXiv preprint arXiv:2406.06525, 2024. 8 [33] Aaron Van Den Oord, Oriol Vinyals, et al. Neural discrete representation learning. Advances in neural information processing systems, 30, 2017. 1, 9 [34] Zhengyi Wang, Cheng Lu, Yikai Wang, Fan Bao, Chongxuan Li, Hang Su, and Jun Zhu. Prolificdreamer: High-fidelity and diverse text-to-3d generation with variational score distillation. Advances in neural information processing systems, 36: 84068441, 2023."
        },
        {
            "title": "Appendix",
            "content": "A. Implementation Details Our methodology involves multi-stage training pipeline. We detail the hyperparameters and architecture for each stage below. VAE: The VAE is updated by minimizing composite objective: the reconstruction loss (Eq. 10) and the distribution matching (DM) loss (Eq. 9). The DM loss serves to align the VAEs latent posterior with the reference distribution defined by the teacher. A.1. Tokenizer Pretraining We first train an initial AutoEncoder compressing the latents to low dimension. We employ the pre-trained DINOv2-large model (300M parameters) as our visual encoder. Given an input resolution of 256 256, the encoder yields 16 latent feature map. Following [3], we introduce 16 an MLP projection head with hidden dimension of 2048 to project the encoder features into compact latent dimension of 32. For the decoder, we adopt convolution-based architecture similar to Flux [14]. During this phase, we freeze the parameters of the DINO-v2 encoder and exclusively optimize the MLP projector and the decoder. The model is trained on the ImageNet dataset with batch size of 256 and learning rate of 104 for 8 epochs (40k steps). We optimize the tokenizer using composite objective function comprising reconstruction and adversarial terms. Following [4], the total loss Ltotal is: Ltotal = L1 + Lperceptual + λganLgan (10) where we set the adversarial weight λgan = 0.5. A.2. Reference Distribution (Teacher) Pretraining In this stage, we train teacher model to capture the reference distribution. We first extract the latent representations for the entire ImageNet dataset using the frozen encoder and projector from the previous stage trained tokenizer. We then employ the LightningDiT-XL architecture with QKNorm (675M parameters) as our generative backbone to model these latents. This teacher model is trained to minimize the flow matching loss (Eq. 5). We follow the standard training recipe from VAVAE[36], using batch size of 1024, 104, and train for 400 epochs (500K learning rate of 2 steps). A.3. Distribution Matching VAE Training This stage is the core of our method, where we jointly train the VAE and student model to align the VAEs latent space with the reference distribution. The framework the AE (from Tokenizer preinvolves three components: training stage), the teacher model (from reference pretraining stage) and the student model (initialized from teacher model). We initialize the AE from the tokenizer pretraining stage checkpoint and the student model with the weights of the frozen teacher. The optimization is twofold: 12 Student Model: The student (fake) model is simultaneously optimized to model the VAEs evolving latent distribution by minimizing score-based loss (Eq. 8). We adopt learning rate of 2 105 for both trainable models (the VAE and the student). The distribution matching loss term is scaled by weight of λdm = 10, and we apply classifier-free guidance (CFG) scale of = 5 during the computation of the DM loss. Following the strategy in [37], we employ an alternating update schedule: the VAE is updated once every 5 iterations, while the student model is optimized at every training step. The joint training process is conducted for 350K steps. A.4. Decoder Fine-tuning Similar to [3], we fine-tune the decoder of the DM-VAE. We freeze the encoder and MLP projector, optimizing only the decoder with the reconstruction objective (Eq. 10). This fine-tuning is run for 250K steps, using batch size of 256 104. and learning rate of A.5. Diffusion Model Training After the DM-VAE is fully trained, we now train final, high-fidelity generative model within its latent space. We train new LightningDiT-XL model from scratch on DMVAE latents, again optimizing for the flow matching loss (Eq. 5). This training is conducted with batch size of 1024, 104, and runs for 800 epochs (1M learning rate of 2 steps). B. Visualization of DMD Gradient Direction We visualize the DMD update vectors (i.e., the negative gradient field) using 2D toy example in Figure 7. As observed, relying on large noise scale leads to coarse and potentially inaccurate update directions. However, as discussed in Section 3.2, high noise levels are practically necessary to bridge the gap between distributions with initially disjoint supports. Conversely, small noise scale yields highly precise update directions, but may fail to provide meaningful guidance when the distributions are far apart. The training process can be regarded as an averaging of different time steps, which demonstrates the value of dynamically adjusting the noise schedule during training. This needs further exploration. Figure 7. DMD update direction visualized at different timesteps in 2D toy setting. C. Convergence Speed Visualization We present visual comparison of the convergence speed between DMVAE and VAVAE in Figure 8. The results demonstrate that DMVAE achieves consistently superior visual generation quality compared to VAVAE when evaluated at equivalent training steps. 13 Figure 8. Quanlitative comparison of convergence speed on ImageNet 256256. We compare DMVAE with VAVAE and report conditional results without CFG."
        }
    ],
    "affiliations": [
        "Peking University",
        "Tencent",
        "UCAS"
    ]
}