{
    "paper_title": "Solaris: Building a Multiplayer Video World Model in Minecraft",
    "authors": [
        "Georgy Savva",
        "Oscar Michel",
        "Daohan Lu",
        "Suppakit Waiwitlikhit",
        "Timothy Meehan",
        "Dhairya Mishra",
        "Srivats Poddar",
        "Jack Lu",
        "Saining Xie"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Existing action-conditioned video generation models (video world models) are limited to single-agent perspectives, failing to capture the multi-agent interactions of real-world environments. We introduce Solaris, a multiplayer video world model that simulates consistent multi-view observations. To enable this, we develop a multiplayer data system designed for robust, continuous, and automated data collection on video games such as Minecraft. Unlike prior platforms built for single-player settings, our system supports coordinated multi-agent interaction and synchronized videos + actions capture. Using this system, we collect 12.64 million multiplayer frames and propose an evaluation framework for multiplayer movement, memory, grounding, building, and view consistency. We train Solaris using a staged pipeline that progressively transitions from single-player to multiplayer modeling, combining bidirectional, causal, and Self Forcing training. In the final stage, we introduce Checkpointed Self Forcing, a memory-efficient Self Forcing variant that enables a longer-horizon teacher. Results show our architecture and training design outperform existing baselines. Through open-sourcing our system and models, we hope to lay the groundwork for a new generation of multi-agent world models."
        },
        {
            "title": "Start",
            "content": "Solaris: Building Multiplayer Video World Model in Minecraft Georgy Savva Oscar Michel Daohan Lu Suppakit Waiwitlikhit"
        },
        {
            "title": "New York University",
            "content": "6 2 0 2 5 2 ] . [ 1 8 0 2 2 2 . 2 0 6 2 : r a"
        },
        {
            "title": "Abstract",
            "content": "Existing action-conditioned video generation models (video world models) are limited to singleagent perspectives, failing to capture the multi-agent interactions of real-world environments. We introduce Solaris, multiplayer video world model that simulates consistent multi-view observations. To enable this, we develop multiplayer data system designed for robust, continuous, and automated data collection on video games such as Minecraft. Unlike prior platforms built for single-player settings, our system supports coordinated multi-agent interaction and synchronized videos + actions capture. Using this system, we collect 12.64 million multiplayer frames and propose an evaluation framework for multiplayer movement, memory, grounding, building, and view consistency. We train Solaris using staged pipeline that progressively transitions from single-player to multiplayer modeling, combining bidirectional, causal, and Self Forcing training. In the final stage, we introduce Checkpointed Self Forcing, memory-efficient Self Forcing variant that enables longer-horizon teacher. Results show our architecture and training design outperform existing baselines. Through open-sourcing our system and models, we hope to lay the groundwork for new generation of multi-agent world models. Website https://solaris-wm.github.io/ Engine Code https://github.com/solaris-wm/solaris-engine Model Code https://github.com/solaris-wm/solaris"
        },
        {
            "title": "Datasets",
            "content": "Models https://huggingface.co/collections/nyu-visionx/solaris-data https://huggingface.co/collections/nyu-visionx/solaris-models Project lead; order determined by coin toss. Equal technical contribution."
        },
        {
            "title": "Contents",
            "content": "1 Introduction 2 Related Work 2.1 World Models and Video World Models . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 2.2 Autoregressive Video Generation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 2.3 AI Agents in Minecraft . . . . . 3 SolarisEngine: Framework for Multiplayer Gameplay at Scale 3.1 Enabling cooperative multiplayer gameplay . . . . . . . . . . . . . . . . . . . . . . . . . . 3.2 Extracting and aligning visuals with actions . . . . . . . . . . . . . . . . . . . . . . . . . . . 3.3 Robust and Scalable Docker System . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 3.4 Multiplayer Training Dataset . 4 Solaris Model Design . . 4.1 Preliminaries . 4.2 Network Architecture . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 5 Multiplayer Training Pipeline 5.1 5.2 5.3 5.4 Stage 1: Bidirectional Single-Player . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . Stage 2: Bidirectional Multiplayer . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . Stage 3: Causal Multiplayer . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . Stage 4: Self Forcing . . . . . . . 6 Evaluation Benchmark 7 Experiments 7.1 Qualitative Results . . 7.2 Architecture Experiments Self-Forcing Ablations . 7.3 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 8 Conclusion 9 Acknowledgments References Solaris Multiplayer Framework . A.1 Mineflayer Modifications . . . A.2 GPU Data Collection . . Multiplayer Training Dataset . . . . . B.1 Episode Types . . B.2 Episode filtering . B.3 Action Space . . . B.4 Mouse Action Distribution . . . . . . . . . . . . . . Model Training Evaluation Details Self Forcing . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . E.1 Overview . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . E.2 Teacher Forcing Mask Implementation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 2 3 4 4 5 5 5 6 7 7 7 8 8 9 9 9 10 10 11 13 13 14 15 15 16 17 21 21 21 21 22 22 22 23 23 24 24 24 1. Introduction Figure 1 Selected samples from our model. Our model takes in starting frames from each player as input and generates action-conditioned videos. The action descriptions shown here are summaries of the fine-grained action sequences given to the model that span many frames. The third-person ground truth visualizations are not given to the model. Video world models, where future agent observations are generated based on past observations and actions, hold tremendous promise as tool for embodied AI agents, and are useful for synthetic training [2, 58], inference-time planning [66, 4], and policy learning and evaluation [15, 31, 49]. However, todays world models are limited to simulating the observations of only single agent at time. In our fundamentally multi-agent world, video world models that are capable of simulating the perspectives of all agents in an environment are needed to capture an accurate world state. Toward this end, we study the creation of multiplayer video world models trained on multiplayer data collected from Minecraft. Modeling the perspectives of multiple players at once is significantly more challenging than single-player modeling. Generated observations need to be consistent not only across time but also across agent perspectives. Actions taken by one agent, such as movement or block placement, must be simultaneously and accurately reflected in the viewpoints of all other agents. Minecraft serves as an ideal testbed for multi-agent modeling. Its unbounded, fully 3D worlds rigorously test challenging aspects of viewpoint modeling, such as perspective consistency, occlusion handling, and spatial memory. Its dynamic and malleable environment tests models ability to keep track of alterations that occur over time. Furthermore, environmental stochasticity (e.g., mobs, weather) forces models to disentangle alterations caused by environmental randomness from those caused by controllable agents. And through its complex system of building, digging, and crafting, Minecraft allows for endless complexity. Lastly, its procedurally generated terrain naturally diversifies the environment. We believe Minecraft will continue to be valuable research platform in the long-term. 3 Prior to this work, there were no existing publicly available systems for simulating multiplayer Minecraft gameplay, motivating us to build SolarisEngine: system for collecting gameplay from preprogrammed bots. In SolarisEngine, our bots engage in non-trivial and realistic gameplay, including mining, attacking, building, and navigating terrains. Our system allows us to collect millions of frames in matter of hours, and its modular design makes it easy to extend to new agent behaviors. We used this data engine to generate multiplayer training dataset of 12.64 frames (6.32 per player) and an evaluation set testing multiplayer movement, memory, grounding, building, and view consistency. Our model, called Solaris, is video diffusion model with general-purpose architecture that adapts single-agent pre-trained video DiT to one that simulates multiple perspectives with minimal modifications. Our model takes in sequences of past player observations and actions and generates future observations using autoregressive diffusion. We show examples of our models capabilities in Fig. 1. To improve long-horizon autoregressive generations, we adopt the Self Forcing [30] paradigm. We extend the setting of Self Forcing to allow the student to benefit from long-context teacher. Naively applying Self Forcing with sliding-window generation leads to excessive memory usage, problem we mitigate with novel technique called Checkpointed Self Forcing analogous to gradient checkpointing. We also make simplifications to the initialization of the student model, finding that the initialization method of CausVid [63] can be replaced by simply finetuning with causal masking. In summary, we present Solaris, multi-agent video world model capable of simulating consistent multiplayer perspectives in Minecraft. We contribute scalable data system, SolarisEngine, largescale training dataset of multiplayer gameplay, an evaluation system, and new architecture that adapts pre-trained video DiT for multiplayer videos. Finally, we introduce Checkpointed Self Forcing, memory-efficient extension of the Self Forcing paradigm, enabling effective autoregressive training. 2. Related Work 2.1. World Models and Video World Models world model takes in an observation of system along with chosen action or intervention, and produces prediction of the resulting state or outcome. The notion of world model can be traced back to Kenneth Craik, who argued in his 1943 book The Nature of Explanation [12] that if an organism carries small internal model of reality and its possible actions, it can test options mentally, anticipate the future, use past experience, and respond more effectively and safely to challenges. These ideas later became central to dynamic systems and control theory [8, 34, 9], establishing mathematical tools for modeling, prediction, and planning in uncertain environments. The Dyna architecture proposed by Sutton [52] emphasized that learning an internal model of the world enables agents to plan their actions rather than rely solely on reactive trial-and-error interactions with the environment, such as in model-free reinforcement learning. More recently, the concept of world models has been combined with deep generative modeling. Several works [21, 22, 23, 24, 25] demonstrated learning compact latent dynamics model directly from pixel observations, enabling policy optimization within learned latent environment. Instead of modeling raw pixels or compression-based latent spaces such as those from variational autoencoders [35] that remain close to pixel space [65], an important direction is for world models to learn abstract representations where predictions can ignore irrelevant details, enabling better understanding, prediction, and planning [66, 7, 4]. With the emergence of video diffusion transformers [47] and major advances in text-to-video and image-to-video generation, growing trend in world modeling is to use video diffusion models as world simulators [45, 59, 41, 6]. Since this line of work typically does not focus on learning abstract representations or on planning and decision making within them, we refer to these approaches as video world models to emphasize their reliance on video diffusion for simulation. Video world models have been applied to numerous domains including robotics [44, 57, 39, 38, 60, 18], video games [14, 46, 53, 3, 26, 56], self-driving [1, 29, 32], and physical simulation [37, 64]. Our work contributes to this direction. However, our focus is on building multiplayer video world model. Instead of just generating pixels from actions, we want to create foundation for agent learning in setting where multiple agents share the same world, and where simply simulating pixels is not enough. To our knowledge, Multiverse [16] is the only video world model capable of simulating multiple agents. While their U-Net [50] model is trained on gameplay 4 from one race track (Tsukuba) in the 2004 video game Gran Turismo 4, we tackle the significantly more complex 3D open-world environment of Minecraft. We study applying their channel-concatenation design to our model in Sec. 7. 2.2. Autoregressive Video Generation Large scale Diffusion Transformer (DiT) [47] video generation models have made tremendous progress in recent years [45, 36, 54]. Diffusion Forcing [10] is technique where autoregressive generation emerges as byproduct of training with an independent noise level per frame. CausVid [63] was introduced to convert bidirectional video model to an efficient causal model of comparable quality. Self-Forcing [30, 13, 28] improves generation quality further by supervising on models own generations, mitigating autoregression train-test mismatch. concurrent work to ours, RELIC [28], also studies Self-Forcing with long-context teacher. Like our method, they propose memory-efficient implementation involving recomputation step for the backward pass. However, we distinguish our approach by performing this step in one parallel forward pass via masking, avoiding the multiple rolling passes required by RELIC. 2.3. AI Agents in Minecraft number of platforms for agents in Minecraft have been developed, which have primarily been used for research in reinforcement learning [33, 17, 20]. Mineflayer [48] is commonly used framework for developing bots in Minecraft, and was used to collect the LoopNav [40] dataset testing spatial memory, as well as in Voyager [55], Minecraft LLM agent. VPT [5] is large-scale dataset of single-player Minecraft data collected from humans. However, as we explain in Sec. 3, none of these frameworks are capable of simulating multiplayer gameplay with visuals, leading us to develop SolarisEngine. Figure 2 SolarisEngine Overview. (Left) Docker-based orchestration of containerized game server, camera, and controller bots. Cameras mirror Controllers state and actions via custom server-side plugin; Controllers are Mineflayer bots that run episode code and log low-level actions. (Right) Episodes compose reusable skill primitives from shared library. Simplified collector episode code is shown. 3. SolarisEngine: Framework for Multiplayer Gameplay at Scale We describe our core system, SolarisEngine, built from the ground up for capturing pre-programmed multiplayer Minecraft gameplay and large-scale data collection. Several frameworks exist for controlling agents in Minecraft, including Malmo [33], MineRL [20], Minedojo [17], and Mineflayer [48]. While these tools offer various capabilities (see Tab. 1), we found they could not be adapted to generate coherent, cooperative multiplayer gameplay. Our system enables us to collect millions of frames through primitive skills library, multiplayer communication layer, and modular, extensible episode system. Looking ahead, we envision this foundation serving other purposes, such as collecting data for Vision-LanguageAction models or testing large-scale collaboration among AI agents. 5 Method Controllability Multiplayer Graphics Malmo [33] MineRL [20] MineDojo [17] Voyager [55] Mineflayer [48] SolarisEngine Table 1 Comparison of Minecraft AI frameworks. Unlike prior systems, SolarisEngine enables controlled multiplayer gameplay collection with real Minecraft graphics. RL-based frameworks such as Malmo, MineRL, and MineDojo produce visual observations but offer limited controllability, as their low-level action spaces require training RL agents to collect meaningful data, which is prohibitively expensive and counterproductive for world-modeling data collection. Voyager and Mineflayer provide high-level behavioral control but operate in text-only mode without visual output. SolarisEngine supports multiplayer gameplay, fine-grained programmatic control and rich visual observations. Figure 3 Dataset Statistics of our training dataset. (Left) The dataset consists of four different episode categories focusing on building, combat, movement, and mining scenarios, respectively. (Middle) It has total of 9,240 episodes and 6.32 frames per player, for combined 12.64 frames. Episode types are chosen randomly with weights that decrease with respect to the typical episode length. (Right) Most episode lengths range from 128 to 512 frames or 6.4 to 25.6 seconds (we record at 20 fps). 3.1. Enabling cooperative multiplayer gameplay Existing frameworks like Malmo [33], MineRL [20], and Minedojo [17] use low-level action spaces, where, without RL training, agents can only take random actions. Data from these agents would be too simple and chaotic for world modeling, problem that gets worse with multiple agents. Though agents could be trained using RL to learn meaningful action policies, the visual data collected from such agents would be geared toward collecting rewards and would not necessarily be diverse, realistic, and human-like. Data for training video world models, on the other hand, demands realistic gameplay. We use Mineflayer, JavaScript Minecraft client library that the community has used for building game bots. Notably, in Voyager [55], it is also used as high-level API for programming agents, albeit in singleplayer setting. Mineflayer provides primitives like pathfinding, block placement, and combat that are composable, letting us generate quality gameplay through programming alone. However, Mineflayer was not designed for multiplayer coordination. To address Mineflayers lack of multiplayer support, we built communication layer enabling bot coordination to mimic collaborative human gameplay. We introduced high-level primitives such as building, scaffolding, tool use, and navigation. These primitives, shown in Fig. 2, when combined with the communication layer, form an episode where two bots achieve predefined goal. We created library of these episode types covering core Minecraft interaction aspects. For full list, please refer to Sec. B.1. Although our episode library is written in high-level code using predefined primitives, the system translates these into low-level actions as if they were collected from human player, making our dataset compatible with VPT [5]. See Sec. B.3 for the description of all supported actions and the relation to VPT. 6 3.2. Extracting and aligning visuals with actions Mineflayer has another significant limitation: it lacks rendering capabilities. To record visuals, we pair each controller bot with \"camera bot\" running the official Minecraft Java client in headless mode with GPU-accelerated rendering. custom server-side plugin synchronizes the camera bot to mirror the controllers state and actions, even including animations, in real time. Although implemented as separate processes, they form single logical player (see Fig. 2), with actions and graphical observations aligned via timestamps in post-processing step. While we currently run with two, the system architecture can theoretically accommodate any number of concurrent players. 3.3. Robust and Scalable Docker System We implement the controller bots, camera bots, and Minecraft server as Docker containers, orchestrated through Docker Compose to run in isolated units. suite of Python scripts manages these units, launching multiple Compose workers in parallel to enable scalable data collection. The bots operate in loop, sampling and executing episodes from our library. The bots teleport to random location at the start of an episode to diversify terrain. Minecrafts complexity and stochasticity mean that episodes inevitably encounter errors or get stuck. To handle this, we built safety mechanism that detects episode failures during execution, notifies all components, and aborts the current episode across all bots. The system then proceeds to new episode with fresh state, ensuring continuous data collection without manual intervention. Figure 4 Episode Demonstrations from our training dataset. We show the recorded frames from 3 different training episodes at various points in time. Note that the third-person start state and end state screenshots are for visualization only and are not part of the dataset. 3.4. Multiplayer Training Dataset Using our framework, we collect large-scale multiplayer training dataset. The dataset episodes, shown in Fig. 4, cover core aspects of the game: building, combat, movement, and mining. Our bots build houses, walls, towers, and bridges, fight mobs or each other, chase each other and navigate together, dig to find valuable ores, mine their way underground towards each other, or mine blocks on the ground. We 7 collect our episodes in an even split between Superflat and Normal Minecraft world types. To the best of our knowledge, our dataset is the first action-annotated multiplayer Minecraft dataset suitable for machine learning applications. The gameplay features diverse times of day, biomes, in-game tool use, and weather conditions. Our action annotations span wide range of Minecraft environment actions: WASD movement, jumping, sprinting, sneaking, camera changes, and interactive actions such as digging, placing, attacking, and item switching (see Sec. B.3 for the full action space). We collect total of 6.32 frames per player. We show important dataset statistics in Fig. 3. Figure 5 Our modified DiT block achieves multiplayer modeling through visual interleaving along the sequence dimension. We denote the number of players with ùëÅ and the number of tokens per video with ùëÄ. Multiplayer information is exchanged through shared self-attention block. The other modules are unchanged from Matrix Game 2.0 and applied independently per player. 4. Solaris Model Design 4.1. Preliminaries We consider world model that can predict the future observations of multiple agents given their past observations and actions. While traditional autoregressive diffusion model operates on latent frame xSP Rùêªùëä ùê∂, we generalize this to the multi-agent setting by augmenting the state space to include an extra player dimension P, performing diffusion on joint tensor of shape (P, H, W, C). Let xùë° = {ùë•ùë° 1, . . . , ùëéùë° ùëÉ } the joint actions of all ùëÉ agents at time ùë°. Let := x1:ùëá and := a1:ùëá denote the combined tensor of consecutive latent observation and action tensors across sequence of length ùëá. Thus has shape (B, P, T, H, W, C) and has shape (B, P, T, D). We model the probability distribution ùëùùúÉ(x) = (cid:206)ùëá ùëùùúÉ(xùë° x<ùë°, a<ùë°) using diffusion model. We restrict ùëÉ = 2 in this work, though our framework is flexible enough to be generalizable to any number of players. ùëÉ } denote the joint state and aùë° = {ùëéùë° 1, . . . , ùë•ùë° ùë°=1 We train our model with conditional Flow Matching [43, 42]. Concretely, we optimize the loss LùúÉ = Ex,a,ùúé,ùúñ (cid:2)ùë£ùúÉ(xùùà, ùùà, a) (ùúñ x)2 2 (cid:3) , 8 using the standard forward process xùùà = (1 ùùà)x + ùùàùúñ where ùúñ (0, I). The noise schedule differs depending on the model variant. For our bidirectional model, we use shared noise level across all players and frames: ùúé (0, 1), with ùùà = ùúé 1ùëÉùëá , so that all elements are noised and denoised jointly. For our causal model, we adopt Diffusion Forcing [10] to enable autoregressive generation, sampling independent noise levels per player and per frame: ùùà [0, 1] ùëÉùëá , where each entry ùúéùëù,ùë° (0, 1) is independent. 4.2. Network Architecture Our architecture is built on Matrix Game 2.0 [26], single-player controllable video Diffusion Transformer [47] (DiT) model that was trained on multiple video games, including Minecraft. visualization of our multiplayer DiT block can be found in Fig. 5. To adapt Matrix Game 2.0 to be capable of multiplayer modeling, we introduce several changes which we discuss below. Expanded action space. We extend the action space of Matrix Game 2.0 to use the full range of Minecraft actions encoded as the MineRL [20] action space by increasing the input dimension of the Matrix Game 2.0 keyboard action module and reinitializing its weights. We run the action module independently per-player, folding the player dimension into the batch. In psuedocode, this is achieved through rearrange(\"B -> (B P) D\"). Multiplayer attention. Information across different players is exchanged through Multiplayer SelfAttention layers in our DiT blocks. We apply 3D RoPE [51] to each players tokens independently and inject player information by adding learned player ID embeddings to each players tokens at the start of each Multiplayer Self-Attention layer. The cross-attention block, which provides first-frame conditioning, remains unchanged from Matrix Game 2.0 and is applied independently per-player. Figure 6 An overview of the full training pipeline. Starting with pretrained bidirectional video diffusion model, we first finetune it with single-player and then multiplayer data. We then finetune it with causal mask before using Self Forcing to achieve stable long-horizon autoregressive generations. 5. Multiplayer Training Pipeline We follow the established practice of first training bidirectional base model and then adapting it to be causal, enabling auto-regressive generation. We illustrate the stages of our training process in Fig. 6. 5.1. Stage 1: Bidirectional Single-Player We begin by initializing our model from the weights1 of Matrix-Game 2.0 [26]. Matrix Game 2.0 is trained to support multiple video games in addition to Minecraft and as result has limited action space 1Specifically, we use the base_distilled_model checkpoint. 9 consisting solely of viewpoint (camera) and directional (WASD) movement. We adapt our model to support the full range of Minecraft actions by finetuning it on the VPT [5] dataset, which contains over 2,000 hours of human gameplay. We train the model with bidirectional attention for 120K steps with context length of 33 frames. We find this single-player pretraining stage to be highly important as it gives our model an effective initialization for multiplayer modeling. We study this experimentally in Sec. 7. Throughout all stages of training, we use the 3D VAE from Matrix Game 2.0 and keep it frozen. 5.2. Stage 2: Bidirectional Multiplayer We next adapt our single-player model to support multiplayer data. We employ the architectural modifications described in Sec. 4.2 and train the model with full-sequence diffusion on our dataset for 120k steps, where we find that FID on held-out test set converges at this point. This checkpoint is used as the teacher during Self Forcing (see Sec. 5.4 for details). 5.3. Stage 3: Causal Multiplayer To save training time, we branch training once the bidirectional model reaches 60k steps. We use this intermediate checkpoint to initialize the causal model, while the bidirectional model continues to train independently to 120k steps to serve as high-quality teacher. Following Matrix Game 2.0, the causal model uses sliding window attention mask with window size of 6 latent frames (24 real frames), which serves as the maximum size of the rolling KV cache during inference. We train this model for 60k steps using diffusion forcing, and it serves as the initialization for the generator in Self Forcing. Our strategy of using Diffusion Forcing with causal mask to initialize the generator is simpler than the standard CausVid [63] initialization of Self Forcing, which involves ODE regression and few-step distillation with DMD [62, 61] before Self Forcing. We find our approach compares favorably to CausVid. We study this experimentally; please see Sec. 5.4 for details. 5.4. Stage 4: Self Forcing We apply Self Forcing [30] to improve long generations. The original Self Forcing method (see Sec. E.1 for an overview) requires the context length of the student model to be equal to that of the teacher. We extend the teachers context to be longer than the students, allowing the student to benefit from more powerful teacher. This requires using sliding-window context when generating the students video. However, naively applying Self Forcing when using this sliding window leads to excessive memory usage, as shown in Fig. 7. We fix this problem with an efficient implementation termed Checkpointed Self Forcing. Our method first produces an initial video and caches intermediate noisy frames with gradient computation disabled. We then recompute the final video from these intermediate states in an additional step with backpropagation enabled. This process saves redundant memory that would otherwise accumulate when doing backpropagation through sliding-window video generation. The core issue with naively applying Self Forcing in the sliding-window setting is that each generation step produces new window of context frames, and backpropagation requires retaining all of these windows in memory simultaneously. For student context length ùêøùë† and total generation length of ùêøùë° steps, the sliding window produces overlapping windows (e.g., frames 1:ùêøùë†, then 2:ùêøùë†+1, then 3:ùêøùë†+2), each sharing all but one frame with its predecessor. This results in memory cost of ùëÇ(ùêøùë° ùêøùë†). Our method eliminates this redundancy by decoupling the autoregressive rollout from backpropagation. 0 and the corresponding noisy transition states x1:ùëÅ Specifically, in Checkpointed Self Forcing, we first perform the autoregressive rollout to obtain the sequence of clean estimates ÀÜx1:ùëÅ , stopping all gradient propagation during this phase. We then recompute the generators outputs in single parallelized forward pass. This pattern of using recomputation to save memory is analogous to Gradient Checkpointing [11, 19], where we manually create checkpoint for ÀÜx1:ùëÅ ùúé during the autoregressive rollout. To strictly reproduce the inference-time conditioning where noisy frames attend to clean history, we double the input sequence length by concatenating the clean frames ÀÜx1:ùëÅ . We then apply custom Teacher Forcing attention mask that enforces causal, sliding-window dependencies: it forces each noisy frame xùëñ ùúé to attend exclusively to the preceding clean frames within its context window and the noisy frames x1:ùëÅ ùúé and x1:ùëÅ 0 0 ùúé 10 Figure 7 An overview of Checkpointed Self Forcing. (A) video is generated with sliding-window KV cache. The computation graph in Jax contains redundant tensors, making naive backpropagation memory prohibitive. (B) Our recomputation step simulates the last step of denoising for every frame in parallel. Clean context frames and final noisy frames are cached from the previous step. (C) An illustration of the attention mask used in part (black squares denote attention). This attention mask allows us to simulate the final denoising step of the rollout in parallel. (D) Leveraging these memory savings, we enable backpropagation through KV layers (avoiding the standard stop-gradient of Self Forcing), which we show improves visual generation in Sec. 7. Specifically, we allow gradients to flow in line 29 of Algorithm 1. (E) Peak memory comparison between naive and Checkpointed Self Forcing across varying depths. Scaled-down networks are used to prevent OOM errors in the naive baseline. ùëñùêøùë†:ùëñ1 of size ùêøùë†, i.e., ÀÜx 0 between Checkpointed Self Forcing and naive Self Forcing, is shown in Algorithm 1. . pseudocode visualization of this procedure, which highlights the differences This formulation effectively converts the sequential rolling cache operation into single parallel operation, reducing the memory footprint to ùëÇ(ùêøùë°). Due to our memory savings, we find that backpropagating through the recomputed KV representations (line 29 of Algorithm 1), though not part of the original Self Forcing algorithm, is now feasible and can improve generations (see Sec. 7 for results). 6. Evaluation Benchmark We develop set of held-out (i.e., completely unseen) episode types that evaluate models on Movement, Grounding, Memory, Building, and Consistency capabilities. We compute FID [27] to measure overall visual quality and introduce VLM as judge metric for measuring semantic adherence to the task: we ask the VLM judge verifiable question about generated video, which varies by task. If its answer matches the known expected answer (e.g. the VLM indeed judges there is player moving to the left), 11 Algorithm 1: Checkpointed Self Forcing Input: Denoise timesteps {ùë°1, . . . , ùë°ùëá }, teacher context length ùêøùë°, student context length ùêøùë† Input: AR diffusion model ùê∫ùúÉ (returns KV embeddings via ùê∫KV ùúÉ ) 1 Initialize model output X0 [] 2 + Initialize noisy inputs Xùë† [] 3 Initialize KV cache KV [] 4 Sample ùë† Uniform(1, 2, . . . , ùëá) 5 for ùëñ 1 to ùêøùë° do 6 ùë°ùëá (0, ùêº) Initialize ùë• ùëñ for ùëó ùëá to ùë† do if ùëó = ùë† then 7 8 9 10 12 13 14 15 16 18 19 20 21 22 + Xùë†.append(ùë• ùëñ ùë° ùëó ) ùê∫ùúÉ (ùë• ùëñ Set ÀÜùë• ùëñ ùë° ùëó ; ùë° ùëó, KV) 0 X0.append( ÀÜùë• ùëñ ) 0 ùëñ ùê∫KV - Cache kv ùëñ ùê∫KV ùëñ) + Cache kv KV.append(kv if KV > ùêøùë† then KV.pop(0) ùúÉ (stop_grad( ÀÜùë• ùëñ 0 ùúÉ ( ÀÜùë• ùëñ 0; 0, KV) ); 0, KV) end else ùë° ùëó ; ùë° ùëó, KV)) stop_grad(ùê∫ùúÉ (ùë• ùëñ ùê∫ùúÉ (ùë• ùëñ ùë° ùëó ; ùë° ùëó, KV) - Set ÀÜùë• ùëñ 0 + Set ÀÜùë• ùëñ 0 Sample ùúñ (0, ùêº) Set ùë• ùëñ euler_step( ÀÜùë• ùëñ ùë° ùëó1 0, ùúñ, ùë° ùëó1) end end 23 24 end 25 + Xùë† stop_grad(Xùë†) 26 + X0 stop_grad(X0) 27 + ùëÄ TeacherForcingMask(ùêøùë†, ùêøùë°) 28 + Xin [X0, Xùë†] 29 + ÀÜX0 ùê∫ùúÉ (Xin; [0, ùë°ùë†], mask = ùëÄ) 30 Update ùúÉ via distribution matching loss Figure 8 Diff-style pseudocode comparing single generator train step of naively applying slidingwindow self forcing to our efficient implementation, Checkpointed Self Forcing. Green +\" lines denote lines from our algorithm that have been added while red -\" denote lines that have been removed. Please also see Sec. E.2 for mask pseudocode and Fig. 7 for visualization. we count the generated video as accurate. Please see Sec. for further details, including text prompts and which frames are sent to the VLM judge. Below, we describe the five evaluation tasks on which we benchmark our models and how the VLM judge is used in each of them. Movement. We test the models ability to render visually consistent agent translation (WASD movement) and camera rotation (mouse commands) from both agents views. In an episode, one bot is moving, and the other one is observing. We use the VLM to judge the position in the observer view. Grounding. We assess the models ability to remember the position of the player in the world through the observation of the other player. To this end, we design an episode in the Superflat world in which two agents are near each other and facing each other. One player turns away and no longer sees the other agent, pauses, and then turns back to its original orientation. Since the turning agent is constantly observed by the stationary player, it should know its position in the world in relation to the other player. The VLM evaluates whether the turning agent sees the other player in its view after it has turned back, and whether the turning agent does not see the other player while it has turned away. 12 Figure 9 Qualitative comparison. Our model Solaris is able to produce stable and coherent frame generations for long-horizon, as shown here with 224 frames. Unlike the baselines, it maintains realistic fighting gameplay and displays complex terrain that maintains realistic texture. In contrast, the frame concatenation baseline shows severe degradation for the second player and flattened texture for the first player. Solaris w/o pretrain\" exhibits unnatural behavior such as duplicating player bodies, showing incorrect pop-up notifications, and degenerating to an unrealistic underwater setting. Memory. We assess the models ability to remember the environment and objects across time. To this end, we design an episode in the Superflat world in which two agents are near and facing each other. They both turn away from each other, pause, and then return to their original orientations. We ask the VLM whether both agents see the other player in their view after they have turned back, and whether both agents do not see the other player while they have both turned away. Building. We test the models ability to reflect environmental changes caused by agents actions. Specifically, we design simple block-building episode in the Superflat world. We begin with two agents near each other. One bot, the builder, builds simple pre-defined shape (square or strip). The other bot, the observer, watches as the building continues. The players look at the structure and the VLM evaluates whether the observer sees built structure. Consistency. We wish to test how well the model understands the correlation between co-visible regions in the two agents field-of-view. In real 3D environment, if two agents located near each other turn to look at previously unseen region, they should see the same region. Therefore, we design an episode in the Normal world in which two agents are near and facing each other. They then simultaneously turn 90 degrees to either the same side or opposite sides. The VLM judges whether their views are similar (for same-side turning) or different (for opposite-side turning). 7. Experiments 7.1. Qualitative Results Here we discuss qualitative results from our flagship, Solaris, model. As shown in Fig. 1, our model is capable of simulating complex aspects of Minecraft gameplay, including building, mining, fighting, and multiplayer viewpoint modeling. The action sequences for each of these scenarios come from bot gameplay not present in the training dataset. We also recorded third-person perspective of the bots, 13 Figure 10 Qualitative examples of learned capabilities. We present generated videos demonstrating the models ability to simulate complex game dynamics. The rows illustrate: (1) fine-grained state tracking, specifically inventory counter synchronization following block placement; (2) global environmental consistency, shown by the simultaneous onset of rain; (3) inventory active item synchronization, torch placement, and generating accurate mining animations; and (4) coherent PVP on complex terrain. shown in the left and right columns. As can be seen in the end frames from our model, its generations align with the ground truth game state. Examples of advanced model capabilitiesincluding inventory tracking, simulating weather, placing torches, generating animations, and simulating PvPare presented in Fig. 10. Fig. 9 presents qualitative results from our architecture comparison discussed in Sec. 7.2. While baseline methods degrade over time, our model maintains visual fidelity throughout long-horizon. 7.2. Architecture Experiments We compare our architecture implementation to the frame concatenation method of Multiverse [16], the only existing multiplayer world model prior to this work. We also test the necessity of single-player pretraining by comparing the performance of our method to the variant trained without the single-player model initialization. Our method produces superior visual results both qualitatively, as shown in Fig. 9, and quantitatively across all evaluation categories, as shown in Tab. 2. All architecture variants are strong at action following in motion-based trajectories, and get high VLM score on the evaluation in the corresponding category, as shown in Tab. 2. Our method shows superior performance on difficult scenarios involving building, scene consistency, and player grounding, reflected by the higher VLM 14 Movement Grounding Memory Building Consistency Method VLM FID VLM FID VLM FID VLM FID VLM FID Frame concat Solaris w/o pretrain Solaris 77.1 2.9 69.3 0.7 68.2 2.7 68.9 42.5 38.5 53.1 0.0 29.2 1.5 62.5 0.0 66.6 49.9 38.0 37.5 2.6 18.8 0.0 37.5 2. 74.4 67.8 55.1 0.0 0.0 0.0 0.0 20.8 1.5 103.2 86.6 83.6 49.5 5.3 49.5 3.2 71.4 7.1 129.4 121.4 99.4 Table 2 Quantitative comparison across tasks. We compare our method against concatenating player observations along the channel dimension following Enigma Multiverse [16] and training from scratch without single-player pretraining. VLM shows the VLM accuracy 1 (estimated) standard deviation. The standard deviation is estimated by repeating the VLM eval 3 times. scores in those categories. Although the frame concatenation method outperforms in our Movement evaluation, we find qualitatively that it exhibits action hallucinations in the presence of no-op actions. 7.3. Self-Forcing Ablations We ablate each component of our Self Forcing pipeline. We study the two main stages of CausVid, ODE regression initialization and few-step distillation, and find that straightforward causal finetuning is sufficient instead. Although the original Self Forcing paper assumes the generator to be few-step model at the start of training, we find that the few-step ability can be learned simultaneously with stable autoregressive generation in Self Forcing. This allows use simple finetuning instead of CausVid. Next, we study the ability to backpropagate to the KV representations of the self attention layer, which is feasible with the memory savings of our method. Allowing KV backpropagation achieves better visual results than all other variants based on FID, as shown in Tab. 3. We do observe that this causes decreased performance in action following for some categories. However, our method remains competitive across all categories and excels in the challenging Building and Consistency VLM tasks. Components Movement Grounding Memory Building Consistency Init. Pre-DMD KV-BP VLM FID VLM FID VLM FID VLM FID VLM FID ODE Reg Causal FT Causal FT Causal FT 23.4 1.5 21.4 2.7 78.6 0.7 68.2 2.7 65.3 49.1 60.3 38.5 3.1 0.0 3.1 0.0 72.9 1.5 62.5 0.0 56.6 40.4 55.2 38.0 0.0 0.0 0.0 0.0 49.0 2.9 37.5 2. 99.5 55.7 63.8 55.1 3.1 0.0 8.3 1.5 15.6 0.0 20.8 1.5 95.7 90.5 87.4 83.6 49.0 4.2 55.2 3.7 70.8 6.6 71.4 7.1 142.3 160.1 105.1 99.4 Table 3 Ablations on Self Forcing training variants. We study the initialization of the causal model, finding simple causal finetuning with Diffusion Forcing [10] to suffice. We also find that doing few-step distillation before Self-Forcing is ineffective. Finally, we find that enabling KV cache backpropagation improves visual quality. VLM shows the VLM accuracy 1 (estimated) standard deviation. The standard deviation is estimated by repeating the VLM eval 3 times. 8. Conclusion We present Solaris, multiplayer video world model capable of generating consistent multi-view observations from coordinated multi-agent interactions. By developing scalable multiplayer data collection system, staged training pipeline transitioning from single-player to multiplayer modeling, and Checkpointed Self Forcing for memory-efficient long-horizon training, we demonstrated that coherent multi-agent world simulation is achievable. While this paper leveraged SolarisEngine to collect two-player Minecraft data, the platforms potential extends well beyond this setting. It naturally supports more than two concurrent players and can serve variety of downstream research directions: generating multimodal training data for vision-language models or vision-language-action models, training unified models that jointly perceive and act, developing single-agent and multi-agent policies, studying neurosymbolic approaches where agents reason through code, and constructing benchmarks for 3D understanding, planning, and memory. 15 Several limitations of our current work also point to promising avenues for future research. First, our training data is entirely synthetic, which introduces gaps in both the action and visual distributions the model encounters, limiting its ability to generalize. While we have demonstrated the value of single-player pretraining, future work could further investigate how to leverage the more abundant single-player data to close this gap. Second, our model lacks persistent memory: when players leave each others field of view, the model loses track of their shared context, and their trajectories begin to diverge. Unlike true game engine, the world has no underlying persistent state, as it is specified only through two initial frames, with no mechanism to control or maintain the broader environment. Our work is valuable starting point for addressing these future research challenges. 9. Acknowledgments Srivats Poddar completed this work while studying at NYU. Suppakit Waiwitlikhit and Timothy Meehan contributed to the project during their time as visiting students at NYU. We thank Nanye Ma for his help with our Jax codebase. We are grateful for Jihan Yang, Sihyun Yu, Shusheng Yang, and Yucen Lily Li for their advice on the draft and Charles Herrmann for helpful discussions. Egor Gikalo made very useful improvements to the SolarisEngine codebase. This work was primarily supported by the Google TPU Research Cloud (TRC) program and the Google Cloud Research Credits program (GCP19980904). S.X. acknowledges support from the MSIT IITP grant (RS-2024-00457882) and the NSF award IIS-2443404. O.M. is supported by the NSF Graduate Research Fellowship Program."
        },
        {
            "title": "References",
            "content": "[1] Niket Agarwal, Arslan Ali, Maciej Bala, Yogesh Balaji, Erik Barker, Tiffany Cai, Prithvijit Chattopadhyay, Yongxin Chen, Yin Cui, Yifan Ding, et al. Cosmos world foundation model platform for physical ai. arXiv preprint arXiv:2501.03575, 2025. [2] Arslan Ali, Junjie Bai, Maciej Bala, Yogesh Balaji, Aaron Blakeman, Tiffany Cai, Jiaxin Cao, Tianshi Cao, Elizabeth Cha, Yu-Wei Chao, et al. World simulation with video foundation models for physical ai. arXiv preprint arXiv:2511.00062, 2025. [3] Eloi Alonso, Adam Jelley, Vincent Micheli, Anssi Kanervisto, Amos Storkey, Tim Pearce, and Fran√ßois Fleuret. Diffusion for world modeling: Visual details matter in atari. In NeurIPS, 2024. [4] Mido Assran, Adrien Bardes, David Fan, Quentin Garrido, Russell Howes, Matthew Muckley, Ammar Rizvi, Claire Roberts, Koustuv Sinha, Artem Zholus, et al. V-jepa 2: Self-supervised video models enable understanding, prediction and planning. arXiv preprint arXiv:2506.09985, 2025. [5] Bowen Baker, Ilge Akkaya, Peter Zhokhov, Joost Huizinga, Jie Tang, Adrien Ecoffet, Brandon Houghton, Raul Sampedro, and Jeff Clune. Video pretraining (vpt): Learning to act by watching unlabeled online videos. In NeurIPS, 2022. [6] Amir Bar, Gaoyue Zhou, Danny Tran, Trevor Darrell, and Yann LeCun. Navigation world models. In Proceedings of the Computer Vision and Pattern Recognition Conference, pages 1579115801, 2025. [7] Adrien Bardes, Quentin Garrido, Jean Ponce, Xinlei Chen, Michael Rabbat, Yann LeCun, Mahmoud Assran, and Nicolas Ballas. Revisiting feature prediction for learning visual representations from video. arXiv preprint arXiv:2404.08471, 2024. [8] Richard Bellman. Dynamic Programming. Princeton University Press, Princeton, NJ, 1957. [9] Arthur E. Bryson and Yu-Chi Ho. Applied Optimal Control: Optimization, Estimation and Control. Taylor & Francis, London, 1975. [10] Boyuan Chen, Diego Mart√≠ Mons√≥, Yilun Du, Max Simchowitz, Russ Tedrake, and Vincent Sitzmann. Diffusion forcing: Next-token prediction meets full-sequence diffusion. In NeurIPS, 2025. [11] Tianqi Chen, Bing Xu, Chiyuan Zhang, and Carlos Guestrin. Training deep nets with sublinear memory cost. arXiv preprint arXiv:1604.06174, 2016. [12] Kenneth J. W. Craik. The Nature of Explanation. Cambridge University Press, Cambridge, UK, 1943. [13] Justin Cui, Jie Wu, Ming Li, Tao Yang, Xiaojie Li, Rui Wang, Andrew Bai, Yuanhao Ban, and Cho-Jui Hsieh. Self-forcing++: Towards minute-scale high-quality video generation. In ICLR, 2026. [14] Decart, Julian Quevedo, Quinn McIntyre, Spruce Campbell, Xinlei Chen, and Robert Wachen. Oasis: universe in transformer. Project Website, 2024. [15] Yilun Du, Sherry Yang, Bo Dai, Hanjun Dai, Ofir Nachum, Josh Tenenbaum, Dale Schuurmans, and Pieter Abbeel. Learning universal policies via text-guided video generation. In NeurIPS, 2023. [16] Enigma-team. Introducing multiverse: The first ai multiplayer world model. Engima Blog, 2025. [17] Linxi Fan, Guanzhi Wang, Yunfan Jiang, Ajay Mandlekar, Yuncong Yang, Haoyi Zhu, Andrew Tang, De-An Huang, Yuke Zhu, and Anima Anandkumar. Minedojo: Building open-ended embodied agents with internet-scale knowledge. In NeurIPS, 2022. [18] Shenyuan Gao, William Liang, Kaiyuan Zheng, Ayaan Malik, Seonghyeon Ye, Sihyun Yu, Wei-Cheng Tseng, Yuzhu Dong, Kaichun Mo, Chen-Hsuan Lin, et al. Dreamdojo: generalist robot world model from large-scale human videos. arXiv preprint arXiv:2602.06949, 2026. [19] Andreas Griewank and Andrea Walther. Algorithm 799: revolve: an implementation of checkpointing for the reverse or adjoint mode of computational differentiation. ACM Trans. Math. Softw., 26(1):1945, March 2000. 17 [20] William H. Guss, Brandon Houghton, Nicholay Topin, Phillip Wang, Cayden Codel, Manuela Veloso, and Ruslan Salakhutdinov. Minerl: large-scale dataset of minecraft demonstrations. In IJCAI, 2019. [21] David Ha and J√ºrgen Schmidhuber. Recurrent world models facilitate policy evolution. In NeurIPS, 2018. [22] Danijar Hafner, Timothy Lillicrap, Jimmy Ba, and Mohammad Norouzi. Dream to control: Learning behaviors by latent imagination. In ICLR, 2020. [23] Danijar Hafner, Timothy Lillicrap, Mohammad Norouzi, and Jimmy Ba. Mastering atari with discrete world models. In ICLR, 2021. [24] Danijar Hafner, Jurgis Pasukonis, Jimmy Ba, and Timothy Lillicrap. Mastering diverse domains through world models. arXiv preprint arXiv:2301.04104, 2023. [25] Danijar Hafner, Wilson Yan, and Timothy Lillicrap. Training agents inside of scalable world models. arXiv preprint arXiv:2509.24527, 2025. [26] Xianglong He, Chunli Peng, Zexiang Liu, Boyang Wang, Yifan Zhang, Qi Cui, Fei Kang, Biao Jiang, Mengyin An, Yangyang Ren, et al. Matrix-game 2.0: An open-source real-time and streaming interactive world model. arXiv preprint arXiv:2508.13009, 2025. [27] Martin Heusel, Hubert Ramsauer, Thomas Unterthiner, Bernhard Nessler, and Sepp Hochreiter. Gans trained by two time-scale update rule converge to local nash equilibrium. NeurIPS, 2017. [28] Yicong Hong, Yiqun Mei, Chongjian Ge, Yiran Xu, Yang Zhou, Sai Bi, Yannick Hold-Geoffroy, Mike Roberts, Matthew Fisher, Eli Shechtman, et al. Relic: Interactive video world model with long-horizon memory. arXiv preprint arXiv:2512.04040, 2025. [29] Anthony Hu, Lloyd Russell, Hudson Yeo, Zak Murez, George Fedoseev, Alex Kendall, Jamie Shotton, and Gianluca Corrado. Gaia-1: generative world model for autonomous driving. arXiv preprint arXiv:2309.17080, 2023. [30] Xun Huang, Zhengqi Li, Guande He, Mingyuan Zhou, and Eli Shechtman. Self forcing: Bridging the train-test gap in autoregressive video diffusion. arXiv preprint arXiv:2506.08009, 2025. [31] Joel Jang, Seonghyeon Ye, Zongyu Lin, Jiannan Xiang, Johan Bjorck, Yu Fang, Fengyuan Hu, Spencer Huang, Kaushil Kundalia, Yen-Chen Lin, Lo√Øc Magne, Ajay Mandlekar, Avnish Narayan, You Liang Tan, Guanzhi Wang, Jing Wang, Qi Wang, Yinzhen Xu, Xiaohui Zeng, et al. Dreamgen: Unlocking generalization in robot learning through video world models. In CoRL, 2025. [32] Chiyu Max Jiang, Xander Masotto, and Bo Sun. The waymo world model: new frontier for autonomous driving simulation. Waymo Blog, 2026. [33] Matthew Johnson, Katja Hofmann, Tim Hutton, and David Bignell. The malmo platform for artificial intelligence experimentation. In IJCAI, 2016. [34] Rudolph Emil Kalman. new approach to linear filtering and prediction problems. Journal of Basic Engineering, 1960. [35] Diederik Kingma and Max Welling. Auto-encoding variational bayes. arXiv preprint arXiv:1312.6114, 2013. [36] Weijie Kong, Qi Tian, Zijian Zhang, Rox Min, Zuozhuo Dai, Jin Zhou, Jiangfeng Xiong, Xin Li, Bo Wu, Jianwei Zhang, et al. Hunyuanvideo: systematic framework for large video generative models. arXiv preprint arXiv:2412.03603, 2024. [37] Chenyu Li, Oscar Michel, Xichen Pan, Sainan Liu, Mike Roberts, and Saining Xie. PISA experiments: Exploring physics post-training for video diffusion models by watching stuff drop. In ICML, 2025. [38] Lin Li, Qihang Zhang, Yiming Luo, Shuai Yang, Ruilin Wang, Fei Han, Mingrui Yu, Zelin Gao, Nan Xue, Xing Zhu, et al. Causal world modeling for robot control. arXiv preprint arXiv:2601.21998, 2026. 18 [39] Shuang Li, Yihuai Gao, Dorsa Sadigh, and Shuran Song. Unified video action model. arXiv preprint arXiv:2503.00200, 2025. [40] Kewei Lian, Shaofei Cai, Yilun Du, and Yitao Liang. Toward memory-aided world models: Benchmarking via spatial consistency. arXiv preprint arXiv:2505.22976, 2025. [41] Junbang Liang, Ruoshi Liu, Ege Ozguroglu, Sruthi Sudhakar, Achal Dave, Pavel Tokmakov, Shuran Song, and Carl Vondrick. Dreamitate: Real-world visuomotor policy learning via video generation. In CoRL, 2024. [42] Yaron Lipman, Ricky T. Q. Chen, Heli Ben-Hamu, Maximilian Nickel, and Matthew Le. Flow matching for generative modeling. In ICLR, 2023. [43] Xingchao Liu, Chengyue Gong, and qiang liu. Flow straight and fast: Learning to generate and transfer data with rectified flow. In ICLR, 2023. [44] Riccardo Mereu, Aidan Scannell, Yuxin Hou, Yi Zhao, Aditya Jitta, Antonio Dominguez, Luigi Acerbi, Amos Storkey, and Paul Chang. Generative world modelling for humanoids: 1x world model challenge technical report. arXiv preprint arXiv:2510.07092, 2025. [45] OpenAI. Sora: text-to-video model by openai. OpenAI Blog, 2024. [46] Jack Parker-Holder and Shlomi Fruchter. Genie 3: new frontier for world models. Google DeepMind Blog, 2025. [47] William Peebles and Saining Xie. Scalable diffusion models with transformers. In ICCV, 2023. [48] PrismarineJS. Mineflayer: Create minecraft bots with javascript. https://github.com/Prismar ineJS/mineflayer, 2025. Version 4.35.0. [49] Julian Hector Quevedo, Ansh Kumar Sharma, Yixiang Sun, Varad Suryavanshi, Percy Liang, and Sherry Yang. Worldgym: World model as an environment for policy evaluation. In ICLR, 2026. [50] Olaf Ronneberger, Philipp Fischer, and Thomas Brox. U-net: Convolutional networks for biomedical image segmentation. In MICCAI, pages 234241. Springer, 2015. [51] Jianlin Su, Murtadha Ahmed, Yu Lu, Shengfeng Pan, Wen Bo, and Yunfeng Liu. Roformer: Enhanced transformer with rotary position embedding. Neurocomputing, 568:127063, 2024. [52] Richard S. Sutton. Dyna, an Integrated Architecture for Learning, Planning, and Reacting. SIGART Bulletin, 2(4):160163, July 1991. [53] Dani Valevski, Yaniv Leviathan, Moab Arar, and Shlomi Fruchter. Diffusion models are real-time game engines. In ICLR, 2025. [54] Team Wan, Ang Wang, Baole Ai, Bin Wen, Chaojie Mao, Chen-Wei Xie, Di Chen, Feiwu Yu, Haiming Zhao, Jianxiao Yang, et al. Wan: Open and advanced large-scale video generative models. arXiv preprint arXiv:2503.20314, 2025. [55] Guanzhi Wang, Yuqi Xie, Yunfan Jiang, Ajay Mandlekar, Chaowei Xiao, Yuke Zhu, Linxi Fan, and Anima Anandkumar. Voyager: An open-ended embodied agent with large language models. TMLR, 2024. [56] Zeqi Xiao, Yushi LAN, Yifan Zhou, Wenqi Ouyang, Shuai Yang, Yanhong Zeng, and Xingang Pan. Worldmem: Long-term consistent world simulation with memory. In NeurIPS, 2025. [57] Sherry Yang, Yilun Du, Seyed Kamyar Seyed Ghasemipour, Jonathan Tompson, Leslie Pack Kaelbling, Dale Schuurmans, and Pieter Abbeel. Learning interactive real-world simulators. ICLR, 2024. [58] Sherry Yang, Jacob Walker, Jack Parker-Holder, Yilun Du, Jake Bruce, Andre Barreto, Pieter Abbeel, and Dale Schuurmans. Position: Video as the new language for real-world decision making. In ICML, 2024. 19 [59] Ze Yang, Yun Chen, Jingkang Wang, Sivabalan Manivasagam, Wei-Chiu Ma, Anqi Joyce Yang, and Raquel Urtasun. Unisim: neural closed-loop sensor simulator. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 13891399, 2023. [60] Seonghyeon Ye, Yunhao Ge, Kaiyuan Zheng, Shenyuan Gao, Sihyun Yu, George Kurian, Suneel Indupuru, You Liang Tan, Chuning Zhu, Jiannan Xiang, et al. World action models are zero-shot policies. arXiv preprint arXiv:2602.15922, 2026. [61] Tianwei Yin, Micha√´l Gharbi, Taesung Park, Richard Zhang, Eli Shechtman, Fredo Durand, and Bill Freeman. Improved distribution matching distillation for fast image synthesis. NeurIPS, 2024. [62] Tianwei Yin, Micha√´l Gharbi, Richard Zhang, Eli Shechtman, Fredo Durand, William Freeman, and Taesung Park. One-step diffusion with distribution matching distillation. In CVPR, 2024. [63] Tianwei Yin, Qiang Zhang, Richard Zhang, William Freeman, Fredo Durand, Eli Shechtman, and Xun Huang. From slow bidirectional to fast autoregressive video diffusion models. In CVPR, 2025. [64] Jianhao Yuan, Xiaofeng Zhang, Felix Friedrich, Nicolas Beltran-Velez, Melissa Hall, Reyhane Askari-Hemmat, Xiaochuang Han, Nicolas Ballas, Michal Drozdzal, and Adriana Romero-Soriano. Inference-time physics alignment of video generative models with latent world models. arXiv preprint arXiv:2601.10553, 2026. [65] Boyang Zheng, Nanye Ma, Shengbang Tong, and Saining Xie. Diffusion transformers with representation autoencoders. In ICLR, 2026. [66] Gaoyue Zhou, Hengkai Pan, Yann LeCun, and Lerrel Pinto. DINO-WM: World models on pre-trained visual features enable zero-shot planning. In ICML, 2025. 20 A. Solaris Multiplayer Framework A.1. Mineflayer Modifications To facilitate wide range of Minecraft action recording, we modify the Mineflayer API to expose access to the most recently applied camera action in its physics module and extend its event system to send events on one-off semantic actions such as attacking, using, placing, and hotbar changes. In addition, we introduce the following Mineflayer code to improve the quality of the collected data: Make the bot correctly look at the face of the block when placing new block. Add camera smoothing to all non-Pathfinder look commands. A.2. GPU Data Collection We initially experimented with libOSMesa for CPU-based headless rendering for the Minecraft clients (camera bots), however, videos in the open, Normal world often appeared laggy, having up to four repeating frames at 20 fps recording in graphics-intensive scenes, such as in forest. Switching to GPU-based rendering produced consistently smooth videos without frame repetition. Additionally, switching to the GPU enabled the use of NVENC, Nvidias hardware video encoder, which further reduced CPU workload as it had to encode screen captures via ffmpeg. B. Multiplayer Training Dataset B.1. Episode Types Our dataset contains 14 distinct episode types that cover the 4 broader Minecraft game mechanics. We describe the episode types in Tab. 4. Table 4 Episode types in our training dataset. Episode Name Category Description buildStructure Building buildTower Building buildHouse towerBridge pve pvp collector Building Building Combat Combat Mining mine placeAndMine Mining Mining Each bot builds wall or tower in front of the other, or bots build platform together at midpoint Both bots build tall 1-block tower by jumping and placing blocks underneath themselves. Bots build house together. Bots build 1-block tower by jumping, then build bridge connecting the two towers, and meet. Bots defend two random positions on the ground, facing each other against spawning mobs. Bots equip swords and fight each other. One bot digs underground to search and mine ores within line-of-sight, while the other bot follows and places torches. Bots dig 1 block underground and mine their way towards each other. Bots stand facing each other. One bot places blocks, and the other one destroys them. chase orbit Movement One bot runs away in zig-zag pattern and the other bot pursues it. Movement Bots move in circular trajectory around shared center, periodically looking at each other. straightLineWalk Movement One bot runs towards and past the other bot, then spins to look at it. walkLook Movement One or both bots move in random direction with just WASD actions in front of each other. walkLookAway Movement One bot moves in random direction, looks away, looks back at the other bot. The other observes. 21 B.2. Episode filtering Due to the teleportation logic we use before every episode to diversify the terrain, the bots might end up underwater in some of the episodes, as shown in Fig. 11. The underwater mechanics break the logic of most of our episodes, so we choose not to include those. Out of 6000 episodes collected in the Normal world, 340 are underwater. Leveraging the fact that Minecraft shows oxygen bubbles in the GUI, see Fig. 12, when the character is underwater, we filter out the water episodes using linear classifier that we run on the episode frames, detecting the bubble icons. The classifier achieves 100% accuracy. Figure 11 An episode where bot has been teleported underwater. Figure 12 The bubbles HUD template PNG used to filter underwater episodes. B.3. Action Space The gold standard of actions for Minecraft dataset for world modeling is the VPT [5] dataset, which contains the full range of Minecraft game actions. SolarisEngine and hence our multiplayer dataset come close to it and support everything except for inventory opening/closing and raw mouse movement recording (only relative pitch/yaw character rotations are recorded). These two limitations dont allow SolarisEngine to capture GUI activities such as inventory manipulation and crafting. Another difference between VPT and our multiplayer dataset is that we record and store actions in the form of semantic actions (either continuous or discrete) instead of raw keyboard and mouse events, as the VPT dataset does. The full list of actions recorded by SolarisEngine and available in our multiplayer training dataset is presented in Tab. 5. B.4. Mouse Action Distribution Most of our episode types use the Pathfinder Mineflayer plugin for intelligent navigation of our bots. Pathfinder operates at the maximum camera speed of 178 degrees per second. Although we use manual 22 Table 5 Action space of our multiplayer dataset. Sustained denotes action is recorded for as long as it lasts. Once denotes action is recorded for one tick only, usually applicable to instantaneous events. Action key Type Description forward back left right jump sprint sneak camera attack use mount dismount place_block place_entity mine hotbar.1 hotbar.2 hotbar.3 hotbar.4 hotbar.5 hotbar.6 hotbar.7 hotbar.8 hotbar. Player moving forward (W). Player moving backward (S). Player strafing left (A). Player strafing right (D). Player jumping. Player sprinting. Player sneaking. bool/sustained bool/sustained bool/sustained bool/sustained bool/sustained bool/sustained bool/sustained vec2f/sustained Change in player camera orientation (yaw, pitch). bool/once bool/once bool/once bool/once bool/once bool/once bool/sustained bool/once bool/once bool/once bool/once bool/once bool/once bool/once bool/once bool/once Player attacks. Player uses / interacts with the environment. Player mounts an entity/vehicle. Player dismounts. Player places block using the currently selected item. Player places an entity item. Player mining block. Player selects hotbar slot 1. Player selects hotbar slot 2. Player selects hotbar slot 3. Player selects hotbar slot 4. Player selects hotbar slot 5. Player selects hotbar slot 6. Player selects hotbar slot 7. Player selects hotbar slot 8. Player selects hotbar slot 9. camera movements (not controlled by Pathfinder) in our dataset, which provides camera action at random, lower value ranges, the distribution of camera action of our dataset is heavily skewed towards the fast camera moves originating in Pathfinder, as shown in Fig. 13. C. Model Training We provide our training hyperparameters in Tab. 6. We train our Bidirectional Single-player model on v5p-128 and all other models on v5p-64 TPUs. Table 6 Training hyperparameters across all training stages. Setting Learning Rate Batch Size Num Steps Adam ùõΩ1 Adam ùõΩ2 Weight Decay Bidirectional Single-player Bidirectional Multiplayer Causal Multiplayer Self Forcing (generator) Self Forcing (critic) 1e4 1e4 1e4 3e6 3e7 64 32 32 32 120K 120K 60K 240 1200 0.9 0.9 0.9 0.9 0.9 0.95 0.95 0.95 0.95 0.95 0 0 0 0 0 D. Evaluation Details In this section, we present the implementation details of our VLM-as-a-judge evaluation method, providing the exact VLM prompts and its accuracy on ground truth videos. The VLM accuracy is calculated based on whether the VLM gives the expected answer for all queries across all perspectives for the same episode. E.g., for Memory, where both players turn away, an episode is judged as accurate if and only if the VLMs answer matches the expected answer at both query points and for both perspectives (2 2 total queries). By contrast, for Consistency, there is one query per episode, but it includes both players 23 Figure 13 histogram showing the mouse action magnitude distribution in our training dataset. perspectives. Please see Tab. 7 for the exact prompt and accuracy on ground-truth episodes (Episode GT Acc.), and Fig. 14 for visualization. E. Self Forcing E.1. Overview Here, we give brief overview of the original implementation of the Self Forcing algorithm. Let ùëÅ be the number of frames in the video sequence. We employ few-step denoising strategy with discretized noise schedule {ùúé0 = 0, . . . , ùúéùêæ = 1}. core component of Self Forcing is the ability to obtain clean prediction from any intermediate noisy state. In Flow Matching, given noisy latent xùúé and the vector field prediction ùë£ùúÉ(xùúé, ùúé), the estimated clean data ÀÜx0 can be derived analytically: ÀÜx0 = xùúé ùúéùë£ùúÉ(xùúé, ùúé). During training, fully unrolling the denoising process for every frame is memory-prohibitive. To address this, Self Forcing employs stochastic gradient truncation strategy. For each training iteration, random target noise level ùúéùë†ùë°ùëúùëù {ùúé0, . . . , ùúéùêæ } is sampled. The model generates the video autoregressively, but for each frame, the denoising process is truncated once it reaches ùúéùë†ùë°ùëúùëù. The input to the next frame ùëñ is conditioned on the clean estimates of the previous frames, derived from this truncated state: ÀÜx1:ùëÅ 0 ùëùùúÉ = ùëÅ (cid:214) ùëñ=1 ùëñ ùëùùúÉ( ÀÜx <ùëñ ÀÜx 0 ). The DMD loss is applied to the sequence of estimates ÀÜx1:ùëÅ . Gradients are backpropagated only from this final truncation step, and in the original implementation, stop-gradient operation is added to the KV cache. This random sampling of ùúéùë†ùë°ùëúùëù ensures that the model receives supervision across all noise levels while maintaining constant memory complexity with respect to the number of diffusion steps. 0 E.2. Teacher Forcing Mask Implementation In Checkpointed Self Forcing, special teacher forcing mask is used to re-simulate the final step of denoising across all frames in parallel. This mask enforces sliding window causality and that noisy 24 Task Ep. Type Query Point, Perspective Prompt Exp. Answer(s) GT Ep. Acc. 1 SD (%) Translation Movement Ends, Each Player Movement Rotation Turning Ends, Acting Player Grounding One Player Turns Away Memory Both Players Turn Away Turned away, Acting Player Turned back, Acting Player Turned away, Each Player Turned back, Each Player Building Building Building Ends, Observing Player Consistency Turn 90 (Same Side) Turn 90 (Opposite Sides) Turning Ends, Both Players Here are Minecraft screenshots showing another player on the screen. Between the first frame and the second frame, did the player being shown move closer, farther, to the left, or to the right on-screen? Answer with single word from closer, farther, left, right, or no motion. Here is Minecraft screenshot potentially showing another player on the screen. Where is the player located on the screen? Answer with single word from left, right, center. If there is no player on the screen, answer no player. Here is Minecraft screenshot. Is there another player visible on-screen? Answer with single word: yes, no. Here is Minecraft screenshot. Is there another player visible on-screen? Answer with single word: yes, no. Here is Minecraft screenshot. Can you tell me whether there is visible structure built about 6 blocks away from the player? Answer with single word from yes, no. You will be shown two Minecraft screenshots. Do these two screenshots show the same scenery? Be careful and answer based on the content of the screenshots, not just the camera angles. Answer with single word: yes, no. closer, farther, left, right left, right, center no yes no yes yes yes no 100.00 0 96.88 0 96.88 0 92.71 1.47 98.96 1.47 98.96 1. 93.75 4.42 Table 7 Detailed description for our evaluation episodes. The GT Ep. Acc. column shows the VLM accuracy for ground-truth episodes (which is expected to be 100%). In all tasks except for Memory and Consistency (where both players turn), one acting player performs actions while the other observing player stands completely still. In the Query Perspective column, Each Player means we ask the VLM the same text prompt twice, each time about different players perspective (2 queries). Both Players means we ask the VLM judge one text prompt, but about both players perspectives (1 query). Standard deviation calculated across 3 trials. frames can only attend to past clean frames. We present pseudocode in Algorithm 2. 25 Figure 14 An illustration of the frames and prompts provided to the VLM for evaluation. Each scenario is ground truth video for illustration. Above the third-person visualization, we show the frames that are sent to the VLM along with the corresponding question and correct answer. Prompts are abbreviated; please see Tab. 7 for the full text. Algorithm 2: Pseudocode of TeacherForcingMask in NumPy-like style. # L_s : student context length ( sliding window size ) # L_t : teacher context length ( number of frames ) # okens_pe r_f ra me : number of tokens per frame ctx_len = L_t * tok en s_ pe r_ fr am # create index grids for queries and keys q_idx = np . arange (2 * ctx_len )[: , None ] kv_idx = np . arange (2 * ctx_len )[ None , :] # compute frame indices q_frame = ( q_idx // ok en s_ pe r_ fr ame ) % kv_frame = ( kv_idx // ok en s_ pe r_ frame ) % # determine if query / key is from noisy or clean frames q_is_noisy = q_idx >= ctx_len kv_is_noisy = kv_idx >= ctx_len # teacher forcing mask logic tea cher_forcin = ( # noisy queries attend to same - frame noisy keys ( q_is_noisy & kv_is_noisy & ( q_frame == kv_frame )) # noisy queries attend to earlier clean keys ( q_is_noisy & kv_is_noisy & ( q_frame > kv_frame )) # clean queries attend causally to clean keys ( q_is_noisy & kv_is_noisy & ( q_frame >= kv_frame )) ) # sliding window constraint sliding_windo = kv_frame > ( q_frame - L_s ) mask = teacher _for cin & sliding_ window"
        }
    ],
    "affiliations": [
        "NYU VisionX"
    ]
}