{
    "paper_title": "PerceptionLM: Open-Access Data and Models for Detailed Visual Understanding",
    "authors": [
        "Jang Hyun Cho",
        "Andrea Madotto",
        "Effrosyni Mavroudi",
        "Triantafyllos Afouras",
        "Tushar Nagarajan",
        "Muhammad Maaz",
        "Yale Song",
        "Tengyu Ma",
        "Shuming Hu",
        "Suyog Jain",
        "Miguel Martin",
        "Huiyu Wang",
        "Hanoona Rasheed",
        "Peize Sun",
        "Po-Yao Huang",
        "Daniel Bolya",
        "Nikhila Ravi",
        "Shashank Jain",
        "Tammy Stark",
        "Shane Moon",
        "Babak Damavandi",
        "Vivian Lee",
        "Andrew Westbury",
        "Salman Khan",
        "Philipp Krähenbühl",
        "Piotr Dollár",
        "Lorenzo Torresani",
        "Kristen Grauman",
        "Christoph Feichtenhofer"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Vision-language models are integral to computer vision research, yet many high-performing models remain closed-source, obscuring their data, design and training recipe. The research community has responded by using distillation from black-box models to label training data, achieving strong benchmark results, at the cost of measurable scientific progress. However, without knowing the details of the teacher model and its data sources, scientific progress remains difficult to measure. In this paper, we study building a Perception Language Model (PLM) in a fully open and reproducible framework for transparent research in image and video understanding. We analyze standard training pipelines without distillation from proprietary models and explore large-scale synthetic data to identify critical data gaps, particularly in detailed video understanding. To bridge these gaps, we release 2.8M human-labeled instances of fine-grained video question-answer pairs and spatio-temporally grounded video captions. Additionally, we introduce PLM-VideoBench, a suite for evaluating challenging video understanding tasks focusing on the ability to reason about \"what\", \"where\", \"when\", and \"how\" of a video. We make our work fully reproducible by providing data, training recipes, code & models."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 7 1 ] . [ 1 0 8 1 3 1 . 4 0 5 2 : r PerceptionLM: Open-Access Data and Models for Detailed Visual Understanding Jang Hyun Cho1,2,,, Andrea Madotto1,, Effrosyni Mavroudi1,, Triantafyllos Afouras1,, Tushar Nagarajan1,, Muhammad Maaz3,,, Yale Song1,, Tengyu Ma1,, Shuming Hu1,, Suyog Jain1, Miguel Martin1, Huiyu Wang1, Hanoona Rasheed3,, Peize Sun1, Po-Yao Huang1, Daniel Bolya1, Nikhila Ravi1, Shashank Jain4, Tammy Stark4, Shane Moon4, Babak Damavandi4, Vivian Lee1, Andrew Westbury1, Salman Khan3, Philipp Krähenbühl2, Piotr Dollár1, Lorenzo Torresani1,, Kristen Grauman1,2,, Christoph Feichtenhofer1, 1Meta FAIR, 2UT Austin, 3MBZUAI, 4Meta Reality Labs Joint first author, Work done during internships at Meta, Project lead Vision-language models are integral to computer vision research, yet many high-performing models remain closed-source, obscuring their data, design and training recipe. The research community has responded by using distillation from black-box models to label training data, achieving strong benchmark results, at the cost of measurable scientific progress. However, without knowing the details of the teacher model and its data sources, scientific progress remains difficult to measure. In this paper, we study building Perception Language Model (PLM) in fully open and reproducible framework for transparent research in image and video understanding. We analyze standard training pipelines without distillation from proprietary models and explore large-scale synthetic data to identify critical data gaps, particularly in detailed video understanding. To bridge these gaps, we release 2.8M human-labeled instances of fine-grained video question-answer pairs and spatio-temporally grounded video captions. Additionally, we introduce PLMVideoBench, suite for evaluating challenging video understanding tasks focusing on the ability to reason about what, where, when, and how of video. We make our work fully reproducible by providing data, training recipes, code & models. Code: https://github.com/facebookresearch/perception_models Dataset: https://ai.meta.com/datasets/plm-data"
        },
        {
            "title": "1 Introduction",
            "content": "Vision-language models (VLMs) are now key part of computer vision research and are widely used in both academia and industry. Many of the strongest performing VLMs are closed-source, meaning their design, training methods, and the data they use are not publicly shared. To stay competitive, the research community has started to catch up to the proprietary models by using straightforward approach distillation from black-box models [26, 39, 104, 130, 224], where proprietary models are directly used to label training data [26, 28, 98], directly leading to strong benchmark results. Although distillation will unlock strong performance, there are two main issues for basic research. First, it makes it hard to track scientific progress. Specifically, we cannot tell if better results on benchmarks are due to advances in model design or training, or simply because the proprietary teacher models were trained on the evaluation sets of widely used benchmarks or internal data collected to resemble them this information is not available. Second, the heavy reliance on distillation leads to fundamental misunderstanding of the effectiveness of current methods for training VLMs from scratch. Several key questions remain unanswered, including the significance of each training stage, the influence of synthetic data on the training process, the data gaps that the research community should prioritize, and which of these gaps are currently being artificially addressed by data generated from proprietary models. To better understand these challenges, we develop the Perception Language Model (PLM), fully open and reproducible model for transparent research in image and video understanding (Fig. 1 right). PLM consists of vision encoder with small scale (<8B parameters) LLM decoder. We start by an analysis of standard 1 Figure 1 We introduce the largest collection of manually annotated fine-grained activity QA and spatiotemporal captioniong data, collected without any proprietary model artifacts, to demystify training VLMs from scratch (left panel). With this data, we train and release PLM family of open and fully reproducible models to facilitate research in vision-language model training (right panel). training pipelines with available data, without any proprietary model distillation. We investigate large-scale synthetic data and establish key scaling laws to identify critical data gaps that limit video understanding performance, especially for spatio-temporal reasoning and fine-grained understanding tasks. To fill these gaps, we create 2.8M high-quality human-labeled instances of fine-grained video QA and spatiotemporally grounded video captions, see Fig. 1. This release is nearly an order of magnitude larger than the largest existing video datasets of each type [96, 225]. Our annotations go beyond coarse action and object descriptions [145, 200, 211] to capture detailed movement directions, repetitions, states and arrangements of objects details about how actions are performed (Fig. 1 left). They specify both when and where key events occur throughout video, combination that existing datasets lack (Fig. 1 middle). This level of detail enables new model capabilities, such as fine-grained question answering and region-based dense video captioning, and in turn, supports novel applications such as AI-based coaching, assistance, and grounded video transcription. Finally, we introduce new benchmark suite, PLMVideoBench, specifically designed to evaluate VLM performance on these challenging video understanding capabilities. Our model, dataset and benchmark push the boundaries of video understanding, and provide foundation for reproducible and transparent training and evaluation of VLM research. Across 40 image and video benchmarks, we achieve comparable performance with existing state-of-the-art open-weight models (e.g., InternVL2.5 [29]), without distilling from proprietary models, and greatly outperform fully open models (i.e., Molmo [41]). Our final PLM-8B model outperforms e.g., the recent Qwen2.5VL [11] in 10 image and 15 video benchmarks, with particularly large improvements in perception-focused image tasks (+9.1 points on avg. across 4 benchmarks), video captioning (+39.8 CIDEr on avg. across 2 benchmarks), and fine-grained video QA (+3.8 points on avg. across 5 benchmarks), setting new state-of-the-art in detailed visual understanding."
        },
        {
            "title": "2 PLM: Overview",
            "content": "In this section, we overview the model, training stages and training data involved in the development of PLM. Please refer to Fig. A1 for detailed overview and Appendix for additional details. Model. PLM consists of vision encoder and language decoder, where the pre-trained vision encoder Perception Encoder (PE) [16] is connected to the Llama 3 [46] language decoder (1B, 3B, or 8B parameters) with 2-layer MLP projector. We use PE L/14 for Llama3.2 1B and 3B, and PE G/14 for Llama3.1 8B. For image input, PLM incorporates dynamic tiling to support high resolution images for up to 36 tiles of 4482 resolution, where each tile undergoes 2 2 average pooling to compress the visual tokens. For video input, PLM uses 32 frames at 4482 resolution, where the same pooling is applied across the spatial dimensions of each video frame. Image + Video 14M SFT Full 2 2 36/32 Image + Video 64M Synthetic Full 2 2 16/16 Modality Data Training Downsampling Tiles/Frames Table 1 Summary of three training stages to train PLM. Image 1M Synthetic Projector - 1/-"
        },
        {
            "title": "Midtraining",
            "content": "Stage 2 Stage 3 Stage"
        },
        {
            "title": "SFT",
            "content": "2 Data. The data used to train the PLM consists of synthetic and human-annotated samples. Synthetic data enhances the general capabilities of PLM, while human-annotated data broadens these capabilities to encompass more complex tasks. Synthetic data is sourced from diverse array of image and video datasets, covering fundamental VLM capabilities such as OCR, chart/document/diagram understanding, image/video captioning, and visual question answering. We design data engines for each data modality (e.g., natural images, charts, documents, figures, egocentric and exocentric videos) to efficiently scale up, creating 64.7M samples (3). The synthetic data can be noisy, but is available at large scale; on the other hand, human-annotated data provides rich, high-quality supervision for image and video tasks. Here, we combine existing human annotations of diverse image and video sources, with our own collected human-annotated data, specifically geared towards fine-grained video understanding and spatio-temporally grounded reasoning (4). We describe our approach to model training next. Training stages. PLM is trained in three stages: 1. Projector warm-up. First, we freeze the vision encoder and LLM and only train the vision projector on small amount of synthetic image data. This warms-up the newly initialized parameters in the projector and improves stability for later stages. We use 1M SA-1B [81] with image caption generated by our data engine. 2. Large-scale midtraining with synthetic data. Next, we train PLM on diverse domains of images and videos at scale, using maximum of 16 tiles for images and 16 frames for videos. PLM sees around 64.7M images and videos with synthetically generated captions and question-answer pairs. We employ our data engine to scale up synthetic data generation, which we describe in 3 in more detail. Our Human-annotated (3.18M) PLMFGQA PLMSTC 2.4M Fine-grained RCap + RTL"
        },
        {
            "title": "Samples",
            "content": "476.2K"
        },
        {
            "title": "Type",
            "content": "3 3 15.1M 31.7M"
        },
        {
            "title": "Caption\nCaption",
            "content": "Our Synthetic (64.7M) Natural Images Charts & Documents Exocentric Videos Egocentric Videos 17M Cap. + MCQA 900K Cap. + QA 1,2 2 2 2 3. Supervised finetuning with human-annotated data. Finally, we train PLM with higher image resolutions and more video frames, using up to 36 tiles for images and 32 frames for videos. We tackle more challenging video tasks using our data collection and final data mix (20M total), see 4. Existing Open Source (5.59M) Image (92 datasets) Video (27 datasets) 5M 900K"
        },
        {
            "title": "Diverse\nDiverse",
            "content": "3 3 Table 2 Summary of the data mix for training PLM. Please refer to Table A3 for the full data blend. Table 1 shows an overview of our training setup for each stage. Appendix A.1 has the full training recipe of each stage, including hyperparameters and data sources."
        },
        {
            "title": "3 Synthetic Data Generation and Scaling",
            "content": "The predominant paradigm for VLM training is to generate and train from synthetic annotations as cheap alternatives to human-labeled data [11, 29, 30, 41, 92, 104, 185]. Although seemingly promising to get the best results on benchmarks, there is major issue with this approach. The majority of such data shared in the community is derived from proprietary models. This trend makes it hard to decouple scientific progress from mere distillation impact. In fact, we have no knowledge of the architecture of these proprietary systems or what data they are using. In this section, we explore the efficacy of the current paradigm for VLM training in transparent manner. We design our data engine entirely from open-source models and scale up synthetic data generation to around 64.7M samples of images and videos. We establish the scaling laws of training from synthetic data on standard VLM tasks, including natural image, OCR/chart/document, and video tasks. We describe our image and video data engines in 3.1 and analyze synthetic data scaling in 3.2."
        },
        {
            "title": "3.1 Data Engine\nOur data engine is designed to target base capabilities of VLMs for image and video understanding.",
            "content": "Image Data Engine. We generate short and long captions, as well as question-answer pairs, for natural images and those containing documents, diagrams, and text recognizable by optical character recognition (OCR). 3 We prompt the openly accessible Llama 3 [46] model to produce factual, detailed image captions while minimizing hallucinations. To create informative question-answer pairs, we utilize OCR data, captions, and other metadata, which are fed into the prompt of text-only LLM. See Appendix for detailed pipelines. Video Data Engine. For videos, we first use an off-the-shelf scene detector [19] to extract video clips of approximately 30 seconds duration. Then, we extract the keyframes and generate frame-level captions using Llama 3, and video captions using our initial PLM trained with Stage 1 and Stage 3 data shown in Table 2. We then employ an LLM to refine the frame-level and video captions by incorporating existing video metadata (e.g., action labels, time tags) into cohesive, detailed video-level caption. Similarly, we generate question-answer pairs from the video-level captions. Details are in Appendix K. The resulting synthetic data is large-scale and diverse 64.7M samples carefully curated from variety of image and video sources including natural images, in-the-wild text, chart, figures, documents, egocentric and exocentric videos. Table 2 provides more details."
        },
        {
            "title": "3.2 Scaling Laws with Synthetic Data",
            "content": "Our goal here is to explore the impact of synthetically generated data on wide range of downstream VLM tasks. We examine scaling properties of our data under controlled experimentation and establish scaling laws. Figure 2 Synthetic Scaling Plots. Relationship between Average Error across benchmarks and training compute (in floatingpoint operations) for various PLM models. We report average errors across Video QA tasks [52, 94, 96, 120, 145, 196], OCR QA tasks [110, 123125], and Natural Images tasks [13, 59, 97, 121, 165, 199]. Models performance using only human-labeled data subset are reported (No Syth.) as well as the actual power-law fit of each subcategory. Setup. To establish power-law relationship between compute and validation-set errors of downstream benchmarks, we vary the scale of synthetic data, language model decoders (1B, 3B, and 8B), vision encoders (300M and 2B), and resolution/number of frames. For each configuration, we train model with the 64.7M synthetic data from our data engine and 4M publicly available human-labeled data, following stage 2 training described in 2. At every 2M samples, we evaluate the 1B, 3B, and 8B PLM on three categories of downstream benchmarks (VideoQA, OCR QA, Natural QA), constructed from 20 vision-language understanding benchmarks that provide comprehensive and general evaluation of multi-modal large language models. We compute the pareto frontier of these data points and fit power law relationship: Err. = (β FLOP)α and compare the exponents α of the power function as scalability of each setup, where smaller α implies better scaling. Scaling with decoder size. In Fig. 2, we show the scaling behavior of PLM across various decoder (language model) sizes. We show validation-set errors and training compute on logarithmic scale, with the black linear line representing the power-law relationship between them. Different colors (green, turquoise, and blue) represent different language model scales (1B, 3B, 8B) while keeping the vision encoder size constant at 300M. As described in the setup section above, we show the power law fit of the pareto frontier in each benchmark category. We also show the results of PLM only trained on 4M human-labeled datasets as baselines, denoted with horizontal lines of each color. The gap from the horizontal line to the data point marks the impact of the synthetic data. Interestingly, all three categories of benchmarks demonstrate clear power-law relationship between compute and average benchmark errors, with the power law exponent (α) of 0.15, 0.20, and 0.11 for Video QA, OCR QA, and Natural Image QA, respectively. In Appendix B, we provide more details and extend the analysis to (1) scaling the encoder size, and (2) scaling the image resolution and video frames. 4 Limitation of synthetic data. In Fig. 3, we evaluate stage 2 on an extended set of video benchmarks. More specifically, we show the result of 7 challenging video tasks on fine-grained activity understanding [18, 24, 64, 159, 220], temporal grounding [55] and long-video reasoning [186]. Unlike generic, high-level understanding (e.g., what is happening in this video), the challenging tasks require thorough understanding of video in space and time, and fine-grained semantic details. As shown in the figure, the challenging video task (HardQA in lavender, plum, magenta) show poor scaling trend (0.03) compared to general video QA (0.15). The stark difference between the two power law fits illustrate that scaling synthetic data is only effective for established, base tasks. Extending our VLMs to these more challenging, complex tasks still remain unsolved. Next, we address this with high-quality human-annotated video data, PLMFGQA and PLMSTC. Figure 3 Limitation of synthetic data. Challenging video tasks (HardQA) do not scale well with synthetic data."
        },
        {
            "title": "4 Human-annotated High Quality Data",
            "content": "As seen in previous section  (Fig. 3)  , the current paradigm with synthetic data has run out of steam. Training from tens of millions of synthetically annotated data hardly improves our model on new, challenging video benchmarks. Going beyond standard VLM tasks, these benchmarks focus on advanced VLM capabilities such as fine-grained activity understanding, temporal grounding, and long video understanding. Perhaps, the knowledge that these benchmarks examine is simply not present in the initial training set of our data engine nor in existing human-annotated data. Our community lacks high quality datasets for detailed visual understanding to start from, that covers what, where, when, and how of activities in video. To address this gap, we introduce two large-scale, human-annotated video datasets: PLMFGQA (4.1), containing 2.4M video question-answer pairs focused on how actions are performed, capturing nuanced fine-grained details through specially designed questions and carefully annotated answers. PLMSTC (4.2), containing 476.2K spatio-temporal captions, with each caption describing the activities performed by subject (human, animal, or object) associated with spatio-temporal segmentation mask. We present statistics, including comparisons to existing datasets in Table 2 and details in Table I11 and I12."
        },
        {
            "title": "4.1 PLM–FGQA: What and How",
            "content": "Figure 4 Overview PLMFGQA. Examples of question-answer pairs from PLMFGQA, focusing on fine-grained human activity understanding. PLMFGQA is approximately 8 times larger than the largest existing human-annotated video QA dataset and addresses wide range of fine-grained question types that are scarce in existing video QA datasets, such as ones that cover direction of movement, object states, locations and spatial relations. 5 PLMFGQA is fine-grained video question answering dataset collected by asking human annotators to watch short video segment and answer model-generated questions which focus on what activities humans perform and how they perform these activities. Question types include fine-grained recognition (action and object), fine-grained temporal perception (direction of movements, repetition counts, hand pose etc.), and fine-grained spatial understanding (object locations and spatial relationships). For example in Fig. 4, we show question-answer pairs that focus on hand pose and direction of movement for woodwork task (left), and the action count of flipping snacks, and the location of the skewer used for cooking activity (middle). We use multi-stage data engine to first extract video segments with salient actions from untrimmed videos through temporal clustering and shot-detection. Next, we generate questions and answers using either text-only LLM or an early version of PLM. Finally, we refine the answers by asking humans to verify them or replace them if they are incorrect, resulting in high-quality set of QA. Please see Table H9 for examples and Appendix for further details about the data engine and quality control mechanisms. Overall, we collect 2.4M question answer pairs from various open-access video datasets [60, 61, 128, 171, 232, 236] spanning over 780k unique video clips from diverse domains (e.g., cooking, DIY, carpentry, automotive and bike repair) and viewpoints (egocentric and third-person); refer to Fig. H14 for domain statistics. This is nearly 8 times larger than the size of the largest existing human-annotated video question-answering dataset in the community [153]. Moreover, as illustrated by the breakdown of question types1 in Fig. 4 (top-right), PLMFGQA contains large number of annotations about fine-grained details that have been largely missing in existing training video QA datasets [58, 75, 118, 134, 145, 179, 200, 208, 211]."
        },
        {
            "title": "4.2 PLM–STC: What, When and Where",
            "content": "Figure 5 Overview of PLMSTC. Examples of spatio-temporally grounded captions from PLMSTC, the first dataset to associate each caption both with temporal interval as well as high-fps sequence of segmentation masks of the subject - i.e., masklets (compared to just temporal interval or sparse sequence of bounding boxes). PLMSTC is spatio-temporal video captioning dataset that offers detailed activity descriptions for each video. It includes timestamps (when) of each activity and focuses on specific subjects identified by masklet (where), which is segmentation mask applied to each frame of the video. In Fig. 5 (right), we show an example of our spatio-temporal captions, describing little girl (highlighted in blue): (frame 0-81): little girl moves back as beluga whale approaches her face. (frame 82-85): Out of frame. (frame 86-98): She tries to feed the whale. This new form of annotations allows VLMs to focus on particular point in space and time. We employ two-stage annotation process to improve efficiency in collecting PLMSTC. In the first stage, annotators select interesting objects that exhibit significant motion changes in the video and use SAM 2 [152] to generate initial mask tublets, which they then refine to ensure high-quality spatial-temporal segmentation. For segments where the subject is out of frame, we automatically supplement out of frame caption. In the second stage, separate set of annotators write temporally localized descriptions of the highlighted subject focusing on the changes in action across time in relation to the whole video. Please see Appendix for full details about the collection and annotation protocol. 1obtained with LLM-based tagging. 6 Overall, we collect 476.2K spatio-temporal captions as the first existing large-scale dense video-region captioning dataset (see Table I11). Table I12 summarizes dataset statistics and Fig. I15 shows our annotation process."
        },
        {
            "title": "4.3 Supervised Finetuning with Human data\nTogether, PLM–FGQA and PLM–STC represent data covering the what, how, when, and where aspects\nthat are crucial for holistic video understanding. We convert this data into a supervised finetuning (SFT)\nformat as follows: For PLM–FGQA, we simply provide the model with the video and question, and train to\ngenerate the paired answer. For PLM–STC, we transform the spatio-temporal caption annotations into three\ndistinct SFT tasks: (1) Given a video and a masked region across the whole video, the model generates both\nthe timestamps and corresponding captions that describe the object’s actions within those timestamps. (2)\nGiven a video, a masked region across the whole video, and a caption, the model generates the start and\nend timestamps corresponding to the caption. (3) Given a video, a masked region across the whole video,\nand specific timestamps, the model generates the caption that describes the object’s actions during those\ntimestamps. These three tasks leverage the same underlying spatio-temporal data, but offer different ways to\nutilize it for SFT, allowing the model to learn a more comprehensive understanding of the video content.",
            "content": "We add our human-annotated SFT data to diverse mixture of other open-access SFT data to create our final training mix. Details about the data sources and sample counts are provided in Table A3."
        },
        {
            "title": "5 PLM–VideoBench",
            "content": "Our high-quality human-annotated data, PLMFGQA and PLMSTC, offers VLMs to train for broader range of capabilities for holistic video understanding. However, existing video benchmarks are not adequately equipped to evaluate these. Our goal is to develop benchmark that targets activities of specific subjects (what) and the fine-grained details of how they execute those activities (how), in the context of space and time (where and when). Towards this, we introduce PLMVideoBench, new and challenging video benchmark for detailed video understanding that comprehensively covers all these aspects. Our benchmark includes: Figure 6 Overview of PLMVideoBench tasks. To tackle PLMVideoBenchs tasks, models need to be able to (a) answer multi-choice questions by discriminating between options with subtle differences corresponding to fine-grained details (FGQA), (b) answer free-form questions about objects and activities in egocentric videos recorded with smart-glasses device, (c) densely caption video by focusing on the activities performed by the highlighted subject (RDCap), (d) correctly determine the temporal interval for the described activity performed by the highlighted subject (RTLoc), and (e) accurately describe the activities performed by the highlighted subject within given temporal interval (RCap). Fine-Grained Question Answering (FGQA). In this task, model must answer multiple-choice question (MCQ) that probes nuanced, fine-grained activity understanding (e.g., painting vertically vs. horizontally in Fig. 6, first). We report multi-binary accuracy (MBAcc) [18] following prior work. Specifically, each question is split into multiple binary-choice questions, where the correct answer is compared with one distractor at time; prediction is considered correct only when the correct answer is consistently selected across all binary comparisons. Our test set consists of 4,371 question-answer pairs. For more information, including statistics on video clips, segment duration, question types, and benchmark construction, see Table H10 and H.2. 7 Smart Glasses Question Answering (SGQA). In this task, model must answer open-ended questions about activities and objects visible in an egocentric video stream recorded by smart-glasses device (see Fig. 6, second). The questions are designed to simulate real-world scenarios where user would ask for assistance from their smart glasses, such as which of these two jackets would look better with this pair of shoes? or does this pasta look strained enough to you?. We manually collect the videos using commercial smart glasses, providing completely new, unique dataset that reflects modern use-cases such as online AI video assistance and activity coaching. For evaluation, we use LLM-judge accuracy with an open-access model (Llama3.3 70B). The test set consists of 665 human-annotated question-answer pairs. See Appendix for more data collection and benchmark construction details. Video Region Captioning (RCap). In this task, model must generate detailed description of an event involving subject of interest in the video. Given region masklet and specified time interval, the model is required to output caption that accurately describes the event occurring within that interval. Compared to traditional video captioning [48, 188, 232] where the aim is to generate video-level caption, the goal is to generate region-level caption tied to specific subject (e.g., person, object or animal). For example, Fig. 6 (third) describes the dogs actions, but our benchmark would also include an instance describing the person cycling in the background. The test set contains 10,060 human-annotated instances. We report LLM-judge accuracy with Llama3.3 70B to assesses the quality of the generated captions. See Appendix C.3 for details. Region Temporal Localization (RTLoc). In this task, model must identify the precise time interval within the video when the specified event takes place for the given subject. Given video, region masklet and text description of the event, the model is required to output the start and end timestamps that correspond to the occurrence of the event (see Fig. 6 fourth). Notably, this task is the inverse of RCap instead of generating the caption, the model receives it as input and generates the corresponding time interval. We filter the test set to include only the captions that are unambiguously localized, i.e., they map to single time window in the video. As result, the test set size is reduced to 7,910 instances compared to RCap. We report average recall@1 over IoU thresholds (0.3, 0.5, 0.7, 0.9). See Appendix C.3 for details. Region Dense Video Captioning (RDCap). In this task, model must generate detailed description of all events involving specific subject of interest (e.g., person, animal, or object) in video. Given video and region masklet, the model must produce sequence of (start, end, caption) tuples that cover the entire duration of the video, including periods when the subject is not visible (see Fig. 6, last). This task is composition of RTLoc and RCap, requiring the model to produce both temporal windows for events as well as captions directly from the video. The test set contains 2,620 video instances. We report SODA score [54], which leverages an LLM judge to assess the quality of the generated captions. See Appendix C.3 for details."
        },
        {
            "title": "6 Experiments",
            "content": "In this section, we first overview the baselines and evaluation setting (6.1). We then compare benchmark results of PLMs with the baselines on broad collection of image (6.2) and video (6.3) tasks as well as on our PLM-VideoBench (6.4). Finally, we provide analyses on data and model ablations (6.5)."
        },
        {
            "title": "6.1 Setup",
            "content": "Baselines. We compare PLMs against the following two classes of baselines. Proprietary models such as GPT-4o [139] (gpt-4o-2024-11-20), Gemini-Pro 1.5 [56] and Gemini-Flash 2.0 [57]. These models are powerful and widely used, however they are black-boxes details about how large they are, what data they are trained on and what additional models are invoked during single inference call are entirely hidden. We evaluate these models in zero-shot setting via API call. Open-access models such as Molmo-O [41], LLaVA-OneVision [90], Gemma 3 [175], Qwen2.5-VL [11] and InternVL2.5 [29] state-of-the-art open-access models, for which model scale, architecture, and inference code are available. We use the official input preprocessor and inference code for all models. 8 Inference protocol. For image benchmarks, we load images at 448 448 resolution and use 36 tiles per image for PLM. For video benchmarks, we uniformly sample 32 frames from the video. Gemini-Pro 1.5 and Gemini-Flash 2.0 process videos directly at 1 fps. For mask inputs in PLMVideoBench, we overlay colored box on the video frames to specify the regions, though other implementations can be considered (e.g., encoding the mask itself, overlaying only the boundary, prompt with text tokens, etc.). For each benchmark and model in result tables, we report the highest score between those reported in literature where available (marked with ) or reproduced by us using official code. We report validation set performance unless specified (in brackets) under the benchmark name. Metrics marked with use LLM as judge. Complete implementation details including inference hyper-parameters, task prompts, judge prompts and proprietary model evaluation protocol can be found in Appendix C.4."
        },
        {
            "title": "6.2 Image Benchmark Results",
            "content": "Charts, Diagrams and Documents Image Captioning Perception and Reasoning Hard Perception Halluc. ] 4 2 1 [ a ) t ( c Q h ] 9 2 2 [ a 92.8 85.7 84.2 94.0 84.8 93.0 t ] 4 6 1 [ 75.3 74.8 80.2 ] 8 7 [ a ) m / ( 2 ] 5 2 1 [ ) t ( o 80.7 94.2 81.0 95.7 94.0 81.0 75.3 90.1 84.6 80.3 81.7 72.9 70.5 50.9 70.5 84.8 75.9 72.0 56.0 77.8 84.9 82.1 90.7 63.0 65. 78.6 75.8 68.8 57.8 50.0 74.8 79.3 90.2 83.1 77.1 93.9 67.0 87.8 89.2 75.8 81.6 72.1 90.5 91.6 84.0 79.3 74.6 93.8 90.9 84.3 84.3 e O ] 0 1 1 [ 810 830 ] 1 0 1 [ C ) r a ( C 74.4 70.6 84. ] 5 [ C c 76.6 71.1 85.0 ] 9 0 2 [ D k F 71.7 68.2 66.6 107.1 809 757 49.1 785 122.6 807 101.2 86.0 25.9 36.5 86.1 110.5 138.6 124.2 100. - - 797 101.7 797 54.3 828 125.4 830 - 105.5 39.5 117.1 144.9 126.5 - 77.5 58.2 87.4 98.0 87.1 75.7 67.7 64.9 84.2 90.8 80.4 80.4 70.0 90.7 90.1 77.3 86.7 94.5 83.6 91.7 84.9 95.7 93.0 91.6 82.3 76.5 74.8 86.0 77.6 92.8 93.0 84.8 79.3 92.7 80.9 85.5 94.6 86. 68.8 76.5 82.6 80.0 83.6 87.3 - - 656 848 864 798 823 870 - - 112.1 98.7 36.8 88.7 125.8 146.7 - - - - 55.7 70.7 79.9 102.5 34.9 32.7 71.0 49.6 116.7 96.5 129.9 105. ] 4 1 2 [ ) ( M 70.7 63.2 69.9 41.1 36.7 40.9 34. ] 9 5 [ ) ( 2 V - - - 80.0 64.8 72.2 81. 48.8 62.4 53.1 80.8 47.9 78.4 80.9 52.3 41.2 84.3 59.6 71.6 39.3 85.3 83.5 48.9 83.8 51.0 58.6 70.1 79.3 46.1 80.6 56.0 46.1 85.6 K - ] 6 5 1 [ a 63.9 63.9 57.8 59.7 50.5 51.5 61.0 - 63.2 63.7 64.0 66.8 - - 69.6 67.9 61.0 66.1 69.2 69."
        },
        {
            "title": "Model",
            "content": "Proprietary GPT-4o [139] Gemini 1.5 Pro [57] Gemini 2.0 Flash [57] 1B scale Qwen2VL-2B [185] InternVL2-1B [29] InternVL2.5-1B [29] PLM-1B 3B scale Gemma 3-4B [175] Qwen2.5 VL-3B [11] InternVL2-4B [29] InternVL2.5-4B [29] PLM-3B 8B scale Gemma 3-12B [175] Molmo-7B-O [41] LLaVA-OV-7B [90] Qwen2VL-7B [185] Qwen2.5VL-7B [11] InternVL2-8B [29] InternVL2.5-8B [29] PLM-8B z ] 3 1 [ a ] 1 5 [ c M ] 1 9 [ a ) m ( S ) m - u ( I ] 3 5 [ h B - ] 6 7 1 [ a - - - 2291 77.1 68.0 2110 77.8 59.8 2386 77.0 64.4 72.5 81.0 82.3 17.3 10.9 42.1 73.8 - 54.4 32.8 55.9 81. 1940 72.9 1794 65.9 71.3 1950 1603 76.3 - - 2221 73.1 2059 73.8 75.6 2337 1879 78.5 44.4 39.5 42.4 46.8 - 47.6 46.1 50.8 55.4 - - - - - - 1930 76.4 49.4 2269 76.4 48.1 2347 73.2 56.4 2200 76.3 48.3 54.8 77.6 2353 56.0 1989 79.3 - - 75.0 25.7 11.9 14.1 53.9 81. 67.4 50.2 47.4 59.7 - 71.9 63.5 61.8 64.0 - - 63.4 70.5 73.5 64.0 64.3 67.0 l l ] 9 9 1 [ a 73.9 66.3 71.9 62.6 51.4 58.3 67.1 - 65.4 60.7 64.6 72.4 - 67.5 66.7 67.2 69.8 65.1 70.1 75.0 ] 3 0 1 [ a 78.0 76.1 74.8 73.0 62.1 65.4 68.8 - 78.5 73.9 80.0 80.4 - - 78.1 82.8 80.3 74.3 80.0 82. O ] 7 9 [ 87.2 88.2 - 87.2 87.7 90.2 88.4 - 88.2 87.4 91.0 88. - - 89.2 88.3 87.2 88.4 90.6 89.9 Table 3 Image benchmark results. We compare PLM with open-access baselines and proprietary models of comparable scale. We report results over 6 benchmarks of charts, diagrams, and document understanding tasks, 3 benchmarks of natural image captioning, 6 benchmarks of perception and reasoning, 4 benchmarks of hard perception tasks, and 1 benchmark of hallucination. Cells with * are reported numbers from literature. The remaining are reproduced using official code. Overall, PLM stands competitive across the board, against the baselines. We evaluate PLM on total of 20 image benchmarks. We divide these into the following categories. Image Captioning: generate short/detailed caption for an image; Perception and Reasoning: answer questions of varying difficulty about objects, actions, functional correspondence, multi-view reasoning, spatial layout etc. Charts, Diagrams and Cocuments: answer questions that require parsing images of documents and diagrams; and Hallucination: evaluate the robustness of image models to hallucinated details. Evaluation protocol and metrics details are in Appendix C.1. Table 3 shows our results. Our PLM models achieve competitive performance across all benchmarks. On all model scales, PLM largely outperforms the existing state-of-the-art models on image captioning tasks and hard perception benchmarks, showcasing the strong pretrained vision encoder (PE [16]) as well as diverse synthetic data in stage 2. PLM performs on-par with the state-of-the-art (Qwen2.5 VL [11]) on charts, diagrams, and documents understanding tasks as well as proprietary models such as GPT-4o and Gemini 2.0 Flash. Overall, PLM shows strong performance on wide spectrum of image benchmarks solely from open-access data with white-box data engine. Notably, PLM performs strongly on hard perception tasks (BLINK, CV-Bench, RealWorldQA, VSR, SEED, etc.), proving the importance of strong pre-trained vision encoder [16]. Additionally, we report Image Grounding task results on RefCOCO/+/g [25] datasets in Table E6, and show that PLM outperforms both specialist models as well as the VLM baselines in all model scales. Notably, our PLM-3B largely outperforms across all model scales with the overal average score of 90.9 % on RefCOCO/+/g benchmark despite not having grounding data during stage 2. We attribute this to the strong pre-trained vision encoder similar to the hard perception tasks."
        },
        {
            "title": "6.3 Video Benchmark Results",
            "content": "Video Cap. Video QA Fine-grained Video QA T.Loc. Halluc. ] 8 8 1 [ D E 62.1 63.0 62.8 57.5 43.2 27.6 92.5 - 55.3 46.0 24.4 96. - 21.7 62.3 27.7 53.1 23.4 99.7 1 - R ] 2 8 1 [ 1 - - - 26.8 21.6 27.7 34.3 - 20.3 19.0 29.2 37. - 28.0 27.9 23.3 21.3 28.5 35.9 2 ] 6 9 [ 87.9 87.2 87.2 79.3 73.2 78.6 86. - 83.3 82.5 85.4 89.4 - 86.0 83.2 84.8 83.9 85.9 90.7 e ] 4 9 [ 64.6 60.5 60. 63.2 57.5 64.8 70.1 - 67.0 64.7 71.7 74.7 - N ] 0 0 2 [ 79.1 81.6 81. 76.4 67.2 74.3 80.3 - 76.8 77.7 82.5 83.4 T t r ] 5 4 1 [ ) t ( - 65.9 - 53.9 47.4 59.4 72.7 50.6 66.9 53.8 67.9 79.3 54.9 - - 57.1 81.0 58.1 67.0 81.2 66.9 70.5 69.6 80.0 65.8 57.8 81.7 68.9 72.6 85.5 84.1 82.7 77.1 ] 6 9 1 [ R S 70.4 - - 67.3 67.5 73.0 83.7 - 63.0 69.9 77.2 84.8 - 66.0 68.1 68.1 70.3 77.6 84.9 - i ] 2 5 [ a 71.9 75.0 70.3 e ] 8 3 [ 55.2 52.8 58. 55.6 42.9 50.3 49.2 42.5 37.8 49.3 50.4 - 61.5 53.9 62.3 54.9 - 44.7 43.7 59.8 58.9 - 57.7 63.3 65.5 56.3 64.2 58.3 - 47.2 46.0 50.5 50.0 61.6 63. V ] 8 8 [ 67.4 71.2 70.8 53.1 38.0 44.5 50.3 - 54.2 48.4 55.4 55. - 57.2 55.7 54.3 52.2 56.9 59."
        },
        {
            "title": "Model",
            "content": "Proprietary GPT-4o [139] Gemini 1.5 Pro [57] Gemini 2.0 Flash [57] 1B scale Qwen2VL-2B [185] InternVL2-1B [29] InternVL2.5-1B [29] PLM-1B 3B scale Gemma 3-4B [175] Qwen2.5 VL-3B [11] InternVL2-4B [29] InternVL2.5-4B [29] PLM-3B 8B scale Gemma 3-12B [175] LLaVA-OV-7B [90] Qwen2VL-7B [185] Qwen2.5VL-7B [11] InternVL2-8B [29] InternVL2.5-8B [29] PLM-8B - y i ] 1 1 2 [ h B o T ] 8 1 [ B e o ] 0 2 1 [ ) t ( M ] 9 5 1 [ - 72.2 38.5 37.7 32.0 34.7 32.8 27.6 56.7 71.2 71.5 - 38.4 47.4 60.7 62.5 46.3 59.2 58.4 64.1 66.2 50.4 60.5 58.5 63.7 60.3 66.1 67.3 27.0 37.8 55.7 60.4 - 64.8 55.6 66.6 66.9 - 45.4 66.7 65.0 55.6 66.2 68. 13.1 13.7 27.7 18.2 - 17.2 17.3 23.7 23.4 - 19.5 20.4 24.5 22.5 24.3 28.3 25.7 21.3 25.0 25.5 - 23.5 25.7 27.4 30.9 - 27.6 31.5 24.6 25.3 29.4 33. n o M ] 4 6 [ ) ( 55.9 56.1 56.1 46.9 40.8 45.0 52. - 49.2 46.4 52.7 60.4 - 53.7 52.2 51.1 48.1 53.5 61.4 p p ] 9 0 1 [ ) M ( 74.5 75.6 76.9 62.3 58.3 56.4 64.6 - 63.0 62.6 65.2 69.3 - 67.8 67.9 71.7 66.5 68.3 72. ] 4 2 [ ) c ( e - 58.3 50.1 47.0 42.8 30.9 40.9 43.6 - 45.7 39.5 52.0 47.2 - 41.2 47.6 49.8 43.6 53.1 46.4 - a C ] 5 5 [ m 38.6 34.2 29.8 0.3 0.1 0.8 55.2 - 38.8 0.0 8.4 57.7 - 12.1 3.4 43.6 20.7 14.3 58.6 u H i ] 1 9 1 [ l e 56.4 56.0 60.1 34.9 25.2 31.0 49.2 - 45.2 40.7 49.6 55. - 34.7 46.4 50.1 52.0 57.1 57.7 s a e ] 0 2 2 [ ) n ( 91.9 80.9 81. 59.9 55.8 38.9 79.5 - 53.5 57.2 66.3 76.5 - 61.1 63.1 61.1 61.9 60.2 77.3 Table 4 Video benchmark results on general video understanding benchmarks. We compare PLM with open-access baselines and proprietary models of comparable scale, and report results over 3 video captioning and 10 short video QA benchmarks. Cells with * are reported numbers from literature. The remaining are reproduced using official code. We evaluate PLM on total of 25 video benchmarks. We divide these into the following categories. Video Captioning: generate short caption for video, or dense description of all events; Short video QA: answer question about short video (few seconds to minute), either by selecting from list of options, or providing free-form answer; Long video QA: answer question as before, about much longer video (minutes to hours); Finegrained QA: answer detailed questions about spatial location, motion, temporal information etc.; and Hallucination: evaluate the robustness of video models to hallucinated details about objects and events. In Table 4 we show video captioning, video QA, fine-grained video QA, and video hallucination. We achieve strong results on widely adopted benchmarks, despite only using open-access data mix free from proprietary model artifacts. In particular, we achieve state-of-the-art results on PerceptionTest (82.7 %), MVBench (77.1 %), TVBench (63.5 %), and ActivityNet-QA (67.3 %) and more, outperforming both the state-of-the-art open-access and proprietary models. Furthermore, we achieve competitive performance on the majority of challenging benchmarks, such as EgoSchema (68.8 %), MotionBench (61.4 %), TOMATO (33.2 %), TempCompass (72.7 %), TemporalBench (28.3 &), Charades-STA (58.6 %), and more. All our model scales show strong performance against both proprietary models as well as open-access baselines of same scale. Lastly, we also show that PLMs at all scale greatly outperform existing approaches on captioning tasks and hallucination detection tasks, owing to our focus on detailed, fine-grained spatio-temporal annotations in our human-annotated data collection. PLM exhibits strong performance across all model scales."
        },
        {
            "title": "6.4 PLM-VideoBench Results",
            "content": "c c A D a R"
        },
        {
            "title": "Model",
            "content": "A A c R D Human perf. We report the result on our proposed benchmark PLMVideoBench from 5 in Table 5. We evaluate our PLM as well as (proprietary and open-access) baselines. In addition, we provide human performance of each subtask in the first row. The results show significant gap between the baselines and PLM. Proprietary baselines and open-source baselines alike perform reasonably on FGQA tasks, though still 6.5 points lower than PLM (61.2 vs 67.7). On SGQA, where the video sources and the question-answer pairs are unseen to all models, PLM performs reasonably well, yet 2.1 points short from open-access best (InternVL2.5) and far from the best proprietary model (GPT-4o). On spatio-temporal tasks (RDCap, DCap, RTLoc), open-source baselines are unable to perform grounded reasoning, and default to either repeating the same caption for every time interval. Proprietary models perform reasonably well, yet far from the human performance. In all sub-tasks of PLM-VideoBench, PLM shows competitive performance compared to proprietary and open-access baselines. Results for all model scales are in Appendix D. Table 5 PLM-VideoBench results. We evaluate PLM against baselines on PLM-VideoBench and report breakdowns. We report human performance in the first row. Open-access LLaVA-OV-7B [90] Qwen2VL-7B [185] Qwen2.5VL-7B [11] InternVL2-8B [29] InternVL2.5-8B [29] PLM-8B Proprietary GPT-4o [139] Gemini 1.5 Pro [57] Gemini 2.0 Flash [57] 24.4 17.6 21.5 21.5 26.1 46.6 41.5 44.5 43.0 45.9 48.3 46.2 32.0 35.3 34.8 35.0 38.5 55. 4.7 4.1 2.5 1.2 5.7 52.8 40.2 49.2 49.8 47.7 53.7 67.7 13.9 15.1 10.7 11.6 8.8 59.1 63.7 49.9 44.8 61.2 57.1 58.7 35.7 33.1 30. 20.9 14.4 13.2 33.1 27.6 27.6 51.6 44.0 42.5 e . 90. 66.6 67.9 53.9 67.8 73.9 Note that the human performance varies based on the nature of the task and evaluation metrics. For example, FGQA human scores are naturally higher than RCap because the task is structured (select the correct option vs. open-ended captioning) and the metric is objective (accuracy vs. caption quality from an LLM-judge)."
        },
        {
            "title": "6.5 Ablation Studies",
            "content": "Setup. We perform an ablation study to assess the importance of each of our proposed data, both synthetic and human-annotated. We start with PLM 3B after stage 2 training, and finetune on 4M short image and video SFT data mix 2 for the data ablation. We evaluate and report average video benchmark performance across five categories video captioning, short video QA, fine-grained QA, and video hallucination, as well as spatial and temporal tasks, PLM-VideoBench and three image categories image OCR, image captioning, and image perception. Full details on which benchmarks are selected are in Appendix A.3. Ablations. First, we observe that stage 2 synthetic data training boosts model performance across the board. Moreover, adding our PLM-STC data further improves variety of benchmarks, including PLM-STC (+27.4 points), video captioning (+2.4 points), and most importantly, spatial and temporal tasks (+6.8 points). Adding our PLM-FGQA data improves distinct set of categories for fine-grained activity understanding; PLMFGQA (+13.1 points), PLM-SGQA (+7.3 points), Fine-grained video tasks (+1.3 points), video hallucination tasks (+3.0 points), and spatial and temporal tasks (+2.2 points). Using our data altogether results in the best performance overall, showcasing the impact of our collected data. . y - C - A - e e l T 48.5 54.3 57.9 56.7 PLM-3B 61.2 PLM-VideoBench Video Tasks F - P A 39.7 49.8 49.9 62.9 S - c 34.4 35.9 36.2 43. . c e 3 - 6.6 14.7 42.1 15.2 e r - F . a a n 5 42.2 48.8 48.6 50.1 . t o V 1 r 24.0 29.9 32.3 30. o V h . k h b 5 67.5 73.2 73.9 74. . k h b 2 . a d 64.9 73.3 74.2 76.3 o T & t S . k h b 4 50.6 56.1 62.9 58.3 63.6 44.0 42.2 50. 34.3 74.6 76.3 64.3 Image Tasks . a a n 6 e I 76.0 84.0 83.8 83.7 83. . t e I . k h b 3 64.3 65.9 67.5 64. 74.2 t r g . k h b 63.3 65.5 65.0 65.6 65.4 Table 6 Ablation study for PLM. We show the impact of individual data components in stage 2 and stage 3 PLM training. For this ablation, we use redued SFT datamix consists of 4M open-access image and video data. Results are aggregate validation-set performance over selected benchmarks in each category of tasks. 23.8M datamix: TextQA 500K, Image QA 2.8M, and Video QA 500K. Each detail can be found in Tab. A3."
        },
        {
            "title": "7 Limitations and Future Work",
            "content": "Our PLM models achieve strong performance against open-data baselines and proprietary models alike, however there is still room for improvement in both modeling and data. On the model front, we do not experiment extensively with long video modeling components (e.g., token compression, dynamic temporal resolution). As result, our performance on long video benchmarks [186, 197, 231] is less competitive (see Table E7). PLM is compatible with such newer advancements and can be incorporated in future work. Additionally, our results are sensitive to the characteristics of the base LLM. We see especially low performance of PLM on benchmarks such as MMMU [214], MME [51] and Video-MME [52] (see Tables 3 and 4), where the strongest baselines often rely on LLMs that are more verbose, but also have likely much larger language component (see the gap to proprietary models on some benchmarks). We also note that our model performs relatively poorly on our SGQA task  (Table 5)  , targeting mix of perception and knowledge based questions to smart glasses. Strong chatbot-focused systems like GPT-4o excel at tasks that go beyond core perception. On the data front, our mix focuses squarely on visual perception it does not include for example, multi-step reasoning, robotics or world-knowledge data. Despite these limitations, PLM contributes new capabilities and strong benchmark results, and set new standard for fully reproducible VLMs."
        },
        {
            "title": "8 Related Work",
            "content": "Vision-Language Models. Building on the strengths of large language models (LLMs), several vision-language models (VLMs) have recently been proposed for image understanding [6, 46, 92, 100, 104, 105, 176, 198, 207], video understanding [93, 99, 106, 118, 119, 161, 187, 195] and joint understanding of both images and videos [29, 90, 133, 185]. These works employ several modeling advancements such as dynamic high resolution inputs [105], adaptive token compression [34, 161], and multimodal positional embeddings [185] to unlock detailed image and video understanding. In the same spirit, we propose family of VLMs that natively supports both image and video input, and includes the latest modeling advancements to achieve state-of-the-art performance across wide range of VLM tasks. Open source, open data VLMs. Training data is key component in developing powerful VLMs. Many existing approaches train on proprietary data that is not released to the community [9, 56, 57, 138, 139] or on data generated using proprietary models (e.g., GPT4o) [26], effectively distilling the closed models. Doing so make measuring scientific progress difficult and limits research on how to train VLMs ground-up. Molmo [41] proposes class of open-data models, however, they are image VLMs trained on relatively small-scale data, limiting their performance as our experiments will show. To address this, we propose model for both image and video understanding that is fully reproducible and trained at scale with data that is sourced without using proprietary models. Moreover, we collect 1M video training samples covers key aspects for video understanding such as fine-grained activity and spatio-temporal grounding, making it the largest collection to date. VLM Benchmarks. Several benchmarks have been proposed to assess the capabilities of VLMs. Popular image benchmarks cover broad perception and reasoning [13, 27, 51, 53, 70, 108, 117, 156, 176, 181, 199, 210, 214] as well as capabilities like image captioning [5, 101, 209], document/diagram understanding [78, 91, 110, 114, 124, 125, 164, 192, 216, 229], mathematical reasoning [115, 183, 223], visual grounding [25, 83] and hallucination [62, 97]. Popular video benchmarks cover video question answering [38, 49, 52, 68, 88, 94, 96, 118, 119, 135, 145, 196, 200, 211, 221], video captioning [20, 23, 82, 182, 188, 202, 232], and hallucination in videos [191, 220]. Many of these video benchmarks remain image-centric they have questions that can be answered with few frames. Video-centric reasoning in benchmarks has been relatively neglected with benchmarks proposed only recently for long video understanding [24, 120, 153, 166, 174, 186, 197, 231, 237] and fine-grained, temporal reasoning [18, 64, 109, 155, 159]. We introduce PLMVideoBench benchmark suite aimed at the core, video-centric capabilities that current benchmarks neglect, namely fine-grained activity understanding and spatio-temporally grounded reasoning. Our benchmark offers advantages over existing QA benchmarks that cover few key categories of questions, are built often using LLMs and over existing video sources, and provide video-level questions. Specifically, it 12 has broader focus on fine-grained characteristics (e.g., motion direction, speed/force, object states etc.), provides region-level descriptions, is manually verified and includes completely unseen, real-world smart-glasses videos for establishing fair comparisons against propriatary models."
        },
        {
            "title": "9 Conclusion",
            "content": "This work presents Perception Language Model (PLM), fully-reproducible vision-language model to transparently tackle visual perception tasks without distillation of private black-box models. We trained PLM using data from existing open-access datasets and synthetic samples generated by our data engine. We identified gaps in detailed video understanding capabilities that cannot be filled with synthetic data. In response, we collected 2.8M human-labels for fine-grained video question answering and spatio-temporally grounded captioning, and created new benchmark, PLM-VideoBench, to evaluate these capabilities. We hope our open dataset, benchmark, and models will foster transparent research in visual perception. Additional Contributors and Acknowledgments. We thank Abhay Sottu Venkata Sesha, AJ Brush, Aohan Lin, Andrew Huang, Azita Shokrpour, Chay Ryali, Chloe Rolland, Daniel Licht, Devansh Kukreja, Don Pinkus, Fu-Jen Chu, George Orlin, Haitham Khedr, Hao Tang, Ida Cheng, Jiale Zhi, Jing Huang, Joseph Greer, Kalyan Vasudev Alwala, Kevin Liang, Kris Kitani, Manchen Wang, Matt Feiszli, Meng Wang, Nicolas Carion, Pengchuan Zhang, Pierre Gleize, Ronghang Hu, Sasha Mitts, Ty Toledano, Valentin Gabeur Yuan-Ting Hu, Vanessa Stark, Weiyao Wang, Xitong Yang, Xingyu Chen for their contributions and support for the project."
        },
        {
            "title": "A PLM Training Details",
            "content": "Figure A1 The figure provides an overview of the datasets used in the paper. PLM is trained with 46.8M synthetic image and 17.9M synthetic video samples, and 3.18M human-labeled video samples. Our data enables PLM to perform variety of tasks, including standard tasks like Image, Multi-image, and Video QA, as well as new video tasks such as Fine-grained QA (PLM-FGQA), Region Temporal Localization (PLM-RTLococ), Region Captioning (PLM-RCap), and Region Detailed Captioning (PLM-RDCap). In this section, we describe the training details of PLM. In A.1 we describe exact details of training setting such as hyper-parameters and implementation details. In A.2 we describe our datamix for both synthetically generated and human-annotated parts. A.1 PLM Training Setting For all three stages, we use AdamW optimizer [111] with weight decay of 0.05 and use FSDP [226] with FlashAttention2 [40] for overall implementation based on PyTorch [144]. Stage 1 training. In stage 1, we use subset of SA-1B [81] paired with detailed captions generated by our data engine (3.1). We use total 1M samples to train PLM with next token prediction loss, with vision encoder and LLM parameters frozen. This stage is commonly known as warm-up stage. We use learning rate 1 104 for all model scale with global batch size of 64 with input resolution 448 448. Stage 2 training. In stage 2, we use the collection of 64M samples total. 60M of them are the images and videos with synthetically generated annotations from our data engine. The remaining 4M are subset of our human-annotated images, videos, and text of our final datamix, which we describe later in A.2. We train with global batch size of 2048, learning rate of 4 105, weight decay of 0.05 for the full set of parameters (vision encoder, projector, and LLM). For both image and video input, we use 448 448 resolution for each tile/frame, which effectively generate 1024 vision tokens. We apply 2 2 spatial average pooling to reduce this to 256. We use adapt dynamic tiling with thumbnail to support any resolution and aspect ratio, similar to prior work [105], and uniform sampling of video frames after preprocessing the videos to 1 fps. We set the maximum number of tiles/frames to be 16, which results in 4096 vision tokens. We spare additional 2048 tokens for text modality, which ends up to 6144 tokens in total for maximum sequence length during training. Stage 3 training. In stage 3, we use total of 21M high-quality datamix spanning over multiple image, video, and text modalities. We describe this datamix in A.2. In this stage, we use global batch size of 1024, learning rate of 1 105 for 8B and 4 105 for 1B and 3B PLMs. We train the full set of parameters for all scales. Similar to stage 2, we adapt dynamic tiling and uniform frame sampling for up to 36 tiles for image and 32 frames for video, with 2 2 spatial average pooling, which generates 36 256 = 8448 vision tokens for image and 32 256 = 8192 vision tokens for video. For all modalities, we use 11264 max training sequence length. A.2 PLM Training Datamix Table A3 presents the full data mix used across all training stages in 4.3 of the main paper, apart from our manually collected data in 4. This contains annotations from existing, public datasets as well as synthetically generated data (see 3) built from them. We filter and include wide variety of existing datasets spanning across images (captioning, QA, grounding), videos (captioning, QA, temporal localization, region captioning and dense captioning) and text-only datasets to preserve the text-instruction following capabilities of our model. Most importantly, we filter out every dataset that contains annotations generated by proprietary models. Table A1 and Table A2 shows the exact number of samples for each datasets in Stage 2 and Stage 3 respectively. Marjory of the data in stage 2 are synthetic, with focus on captioning samples, since they curry the dense information about the image or video. In stage 3, we have one third of the data, mostly focusing on human annotated samples, covering large variety of tasks."
        },
        {
            "title": "Type",
            "content": "Image Synthetic PDFAcc (QA) [141] UCSF [140] ArxivCap [95] SA1B [81] Object365 [160] OpenImages [85] DocVQA [124] InfographicVQA [125] PixmoCap [41] Video Synthetic YT-1B (Cap.) [217] YT-1B (QA) [217] Ego4D (Cap.) [60] Ego4D (QA) [60] Spoken Moments [132] Charades [163] Kinetics710 [75] DiDeMo [8] Text Synthetic NaturalReasoning [212] Human Annotated Image QA [A3] Video QA [A3] Video TL [A3] Video Dense Cap. [A3] Text QA [A3] Total 12M 6M 1.8M 10M 3.5M 1.8M 50K 20K 600K 14M 3M 200K 700K 100K 80K 400K 80K 2M 2.8M 580K 40K 50K 2M 61.7M Table A1 Stage 2 training mix. QA QA Cap. + QA Cap. Cap. Cap. QA QA Cap Cap. MCQA Cap. QA Cap. Cap. Cap. Cap. QA QA QA Temporal Loc. Dense Cap. Mix Image Synthetic PDFAcc (QA) [141] ArxivCap [95] SA1B [81] Object365 [160] OpenImages [85] DocVQA [124] InfographicVQA [125] PixmoCap [41] Video Synthetic YT-1B (QA) [217] Ego4D (Cap.) [60] Ego4D (QA) [60] Spoken Moments [132] Charades [163] Kinetics710 [75] DiDeMo [8] Text Synthetic NaturalReasoning [212] Human Annotated Image QA [A3] Image Cap [A3] Image Grnd. [A3] Image Misc. [A3] Video QA [A3] Video Cap. [A3] Video TL [A3] Video Dense Cap. [A3] Video Region Captioning [A3] Text QA [A3] Human Annotated (Our) PLM FGQA PLM STC Total Table A2 Stage 3 training mix. 2M 1.5M 800K 300K 300K 100K 50K 500K 300K 200K 700K 50K 20K 10K 50K 2M 2.8M 200K 1.8M 706K 622K 310K 40K 60K 120K 1.5M QA Cap. + QA Cap. Cap. Cap. QA QA Cap MCQA Cap. QA Cap. Cap. Cap. Cap. QA QA QA QA QA QA/MCQA QA Temporal Loc. Dense Cap. Cap. Mix 2.4M 630K 20.06M QA RCap, Rtl, RDcap Dataset STAR [196] NExT-QA [200] VISION [10] FlintstonesSV [63] ImageCoDe [84] VizWiz [13] MIT-States (State Coherence) [66] MIT-States (Prop. Coherence) [66] WebQA [22] Birds-to-Words [50] AESOP [151] RecipeQA (Img. Coherence) [204] CLEVR-Change [142] IEdit [15] ChartQA [123] DocVQA [124] InfographicVQA [125] TextVQA [165] TextCaps [162] VisualMRC [169] WTQ [143] HME100k [213] chrome_writting [194] OK-VQA [121] Geometry3k [112] VQA-RAD [86] Size 3032 3870 9900 22341 16594 4900 1900 1900 9338 14281 6915 8699 3885 73888 109023 56599 69170 21324 45526 228994 74492 8825 27536 4802 1793 Total 3141377 Image Cap. Dataset DOCCI [137] DCI [177] Altogether [201] Total Image Misc. Dataset AI2d [78] COCO cap. [101] GQA-Balanced [65] Total ) e Size 13362 ( 7599 15166 I 36127 4 Size ) 12413 414113 943000 fi ( 36127 r Size 222222 157070 42761 167646 199998 119999 9713 73181 70000 116514 21953 11162 80757 10000 15961 100050 42495 47974 46287 27395 82772 35946 35995 5276 12089 18684 2199 10435 31418 7549 17720 45439 9009 21953 16713 2157 9742 12769 3027 9947 313 2118 4995 4863 2555 595 70939 15233 136799 56081 19340 Image QA Dataset DVQA [72] PlotQA [127] MapQA [21] OCRVQA [131] Localized Narratives [148] FigureQA [73] Hateful Memes [80] CLEVR [71] CLEVR v1.0 [71] IconQA [113] TextVQA [165] GeomVerse [76] RobuT (wikisql) [228] WebSight [87] Visual7W [235] TallyQA [3] Robut (WTQ) [228] DaTikz [12] CocoQA [154] ChartQA [123] VQAv2 [59] Chart2Text [136] VisText [170] FinQA [31] DocVQA [124] STVQA [14] TAT-QA [234] RenderedText [194] RAVEN [219] IAM [122] A-OKVQA [156] TabMWP [116] CocoQA [154] TextCaps [162] Screen2Words [180] VSR [102] TQA [79] Robut (SQA) [228] VisualMRC [169] ScienceQA [114] VQA-RAD [86] InfographicVQA [125] Hitab [32] AI2D [78] Inter-GPS [112] diagram_image_to_text [2] MIMIC-IT (CGD) [89] MultiHiertt [227] NLVR2 [168] RAVEN (Multi-image) [219] SpotTheDiff [69] d e l Grounding Video Temporal Loc. Dataset VisualGenome [83] FLickr Entities [147] DCI (Region Caption) [177] RefCOCO/g/+ [77] VCR [216] Size 154792 296332 304912 212923 Dataset HiREST [215] Charades [163] DiDeMo [8] Total Size 7919 7566 435 1398690 Video Region Captioning Total Dataset Image Synth. DocVQA [124] InfographicVQA [125] PDFAcc (Cap.) [141] PDFAcc (QA) [141] UCSF [140] ArxivCap [95] SA1B [81] Object365 [160] OpenImages [85] PixmoCap [41] Total Dataset Video QA EgoQA [134] NExT-QA (instruct) [200] NExT-QA (MCQ) [200] PerceptionTest [145] ActivityNetQA [211] VideoInstruct (human) [118] CLEVRER (MC) [208] CLEVRER (QA) [208] Kinetics710 [75] SSv2 (classification) [58] VidLN [179] VidLN (QA) [179] How2QA [96] STAR [196] Memento [189] Memento-MultiImage [189] Size 50170 21660 12024670 12024670 5953490 1859680 9834573 3484584 1740864 47579011 Size 7813 34114 34114 2403 23530 25803 42620 40000 39949 40000 43126 75090 45731 35297 40060 40060 Total 569710 Video Cap. Dataset VATEX (en caption) [188] Charades (caption) [163] ActivityNet (captions) [48] YouCook2 [232] Total Size 259910 11593 33375 10337 Dataset HC-STVG [172] VidLN (UVO subset) [179] Total Video Dense Cap. Dataset ActivityNet [48] YouCook [232] Total Dataset Video Synth. Spoken Moments [132] Charades [163] Kinetics710 [75] DiDeMo [8] Ego4D (Cap.) [60] Ego4D (QA) [60] YT-1B (Cap.) [217] YT-1B (QA) [217] Size 10131 15427 Size 8859 1039 9898 Size 449044 7919 39949 7566 183029 703935 14792983 Total 19568095 Text-QA Dataset no_robots [150] MathQA [7] LIMA [230] GSM8k (socratic) [36] GSM8k [36] FLAN [193] Dolly15k [37] Magpie Pro (MT) [203] Magpie Pro [203] Size 9485 29837 1030 7473 7473 1386050 15011 300000 300000 Total 2056359 Table A3 PLM training datamix. Our mix includes synthetic and manually annotated data across combination of image data (QA, captioning, OCR, Visual grounding), video data (captioning, grounded captioning, dense captioning, temporal localization) and text-only data . Importantly, all data is publicly accessible, and not generated by proprietary models. A.3 Ablation experiment details We provide additional details about the ablation experiment in 6.5. We report benchmark average scores across 5 categories, along with the average across all of them. We select representative set of benchmarks from the full set of image and video benchmarks in 6.2 and 6.3 that report comparable scores so the average results are meaningful. For Video captioning we select Dream 1K and report the LLM-judge score with Llama3.3 70B as judge. for Short Video QA, and Finegrained QA, we select benchmarks that report MCQ accuracy (and exclude open-ended QA). For Hallucination, we include both benchmarks. For Spatial and Temporal tasks, we select BLINK, CVBench, VSR, and Charades-STA. For Image Perception, we choose SEED, MMMU, VQAv2, OK-VQA, and VizWiz. We train the ablation setup of SFT with the exactly matching hyperparameters as our final run; only difference is the size of the SFT datamix."
        },
        {
            "title": "B Synthetic Scaling Experiments",
            "content": "In this section we provide additional results to the synthetic scaling experiments in 3.2. We report aggregate benchmark accuracies across three categories Video QA, OCR QA and Image QA by selecting representative benchmarks from each category. For VideoQA, these are STAR [196], EgoSchema [120], MVBench [94], VideoMME [52] and PerceptionTest [145]; For OCR QA, these are ChartQA [123], DocVQA [124], InfographicsQA [125], TextVQA [165] and OCRBench [110]; and for Natural Image QA, these are RealworldQA [199], OKVQA [121], VQAv2 [59], and VizWiz [13]. Figure B2 Scaling with encoder size. Scaling trends of PE-G vs. PE-L vision encoders. Larger encoders scale better in Video QA tasks while similar scaling in OCR and Natural QA is seen. Scaling with encoder size. After investigating the impact of the LLM decoder in Fig. 2, we examine the impact of increasing the vision encoder size from 300M (PE Large) to 2B (PE Giant) for each language model scale next. In Fig. B2, we overlay the new power-law with the 2B vision encoder (black dashed) line onto the 300M (red dashed) line. Notably, we find that the larger vision encoder (300M 2B) leads to greater scaling trend on video QA benchmarks. Quantitatively, the power law fit has improved from 0.15 to 0.19. The two lines intersect around 8B scale with PE-G, proving that 8B and larger PLM will benefit more with larger vision encoder. We use PE-L for 1B and 3B LLM scale and PE-G for 8B scale by default. Figure B3 Scaling with input size. Scaling trends of training with 16 tiles/frames vs. 8 tiles/frames. Higher input size scales better in Video QA and OCR QA tasks while similar trend is seen for Natural QA. Scaling with input size. In Fig. B3, we show the impact of increasing the input size to VLM through higher image resolution and more video frames. In this setting, each scale of PLM trains with dynamic tiling for image input and unif orm sampling for video input with maximum 8 or 16 tiles/frames per sample. In each plot, the average error of PLM trained with 16 tiles/frames are plotted. All models use 2 2 spatial average pooling before input to LLM, and each tile/frame has 448 448 resolution. Similar to Fig. 2, we show power law fit with black dashed line, and compare to 8 tiles/frames training denoted with red dashed line. Notably, we find out that on Video QA and OCR QA benchmarks, PLM shows better scalability with training with higher input size. This means with the same FLOP counts at 1013, training with 16 frames makes 2.0 points of metric error lower than 8 frames counterpart (32.2 vs 30.2). Similar trends are observed with OCR QA going from 8 tiles max. to 16 tiles max. Notably, higher resolution did not make difference for Natural QA tasks. We chose the 16 max-tiles and frames to be our final training setting for stage 2 PLM. Figure B4 Synthetic Scaling Plots. Relationship between Average Error and training compute (in floating-point operations) for various 1B, 3B, 8B PLM with L14 vision encoder. Each plot reports the individual error in VideoMME [52], STAR [196], EgoSchema [120], How2QA [96], MVBench [94], PerceptionTest [145], ChartQA [123], DocVQA [124], InfoVQA [125], OCRBench [110], RealworldQA [199], OKVQA [121], VQAv2 [59], VizWiz [13], and TextVQA [165]. Finally, we report Avg. All, which average over all the metrics. In Fig. B4, we show the breakdown of the scaling trend shown in 3.2. stands for human only (i.e., no synthetic) baseline. From the breakdown, the most notable point is the the scalability in OCR, Chart, Document QA tasks. In each benchmark, synthetic data makes more than 10 points of improvement on every model scale, compared to no synthetic baselines. Moreover, there is no sign of saturation; the performance will most likely improve with more synthetic data. We hypothesize that OCR, Chart, Document QA tasks reduce to translation task set of pixels has one-to-one mapping to text space. Remaining tasks exhibit clean power-law relationship between metric error and FLOPs. The last plot shows scaling trend on average over all benchmarks, which shows close power-law relationship."
        },
        {
            "title": "C VLM Benchmark Details",
            "content": "In this section, we provide details about all the image and video benchmarks considered in 6 including composition and evaluation metrics for image benchmarks (C.1), video benchmarks (C.2) and our PLM VideoBench (C.3. We also describe evaluation protocol for all these benchmarks including inference parameters and prompts (C.4). Pointers to evaluation code are linked where available. C.1 Image Benchmarks Image captioning We evaluate on single image captioning and grounded image captioning benchmarks like COCO [101], nocaps [5] and Flickr [209]. We report CIDEr as the evaluation metric. Perception and reasoning We evaluate on broad, general purpose VQA benchmarks like MMMU [214], VQAv2 [59], MMBench [108], OK-VQA [156], VizWiz [13] as well as hard perception benchmarks like BLINK [53], CV-Bench [176], RealWorldQA [199], and VSR [103]. For all MCQ benchmarks, we report accuracy of selecting the correct option. For MME, we report the sum of subtask accuracies, following the original work. Charts, diagrams and documents We evaluate on benchmarks for reasoning over various types of charts, graphs, diagrams, infographics etc. Specifically, DocVQA [124], ChartQA [229], TextVQA [164], InfographicsVQA [125], AI2D [78], OCRBench [110], and SEED [91]. We report accuracy of selecting the correct option. Image Hallucination Finally, we evaluate on benchmarks that evaluate robustness of models to hallucinated details in questions such as HallusionBench [62] and POPE [97]. For HallusionBench we report the aAcc metric (code) which accounts for correctness and consistency using an LLM judge. C.2 Video Benchmarks Video captioning We evaluate on short-video captioning benchmarks, namely YouCook2 [232] and VATEX [188] as well as recent detailed video captioning benchmarks DREAM-1k [182] and AuroraCap-VDC [20]. For YouCook2 and VATEX, we report CIDEr score [178]. For DREAM-1k we report AutoDQ F1-score (code) and for AuroraCap-VDC we report the VDC accuracy (code) following the authors proposed metric. Short video QA We evaluate on multiple-choice (MCQ) benchmarks such as How2QA [96], NExt-QA [200], PerceptionTest [145], STAR [196], TGIF-QA [68], TVQA [88], Video-MME [52] and TVBench [38]. We report accuracy of selecting the correct option. We also evaluate on open-ended question answering benchmarks (w/o options) such as ActivityNet-QA [211] (code), MMBench-Video [49] (code) and VCGBench-Diverse [119]. We report LLM-judge scores/accuracies for these benchmarks. For VCGBench-Diverse, we report the average of 5 LLM-judge scores (code). Long video QA We evaluate on popular long-video benchmarks such as EgoSchema [120], LVBench [186], LongVideoBench [197] and MLVU [231]. We report accuracy of selecting the correct option. Fine-grained video QA We evaluate on benchmarks for fine-grained spatial, temporal and detail reasoning in videos such as TemporalBench [18], TOMATO [159], MotionBench [64], TempCompass [109] and CGBench [24]. We report accuracy of selecting the correct option. For TemporalBench, we report the multi-binary accuracy (MBAcc) (code) proposed by the authors to reduce bias in evaluation. Hallucination We evaluate on benchmarks that evaluate robustness of models to hallucinated details in questions such as VideoHallucer [191] and EventHallusion [220]. We report accuracy of selecting the correct option. C.3 PLM-VideoBench We evaluate on our suite of benchmarks for fine-grained and spatio-temporal reasoning in videos. These include: 19 Fine-grained QA (FG-QA) We report multi-binary accuracy (MBAcc) following prior work [18]. In short, this entails presenting the model multiple independent, binary-choice questions about the same video (in our case, three questions) and requiring the model to gets all of them correct, to count towards accuracy. This sets higher bar for models, and combats bias in multiple-choice question benchmarks that prior work identifies. SmartGlasses-QA (SG-QA) We report LLM-judge accuracy of the predicted answer compared to the ground truth answer. We follow existing LLM judge prompts from ActivityNetQA (code). The prompt is repeated below for completeness. SG-QA judge prompt You are an intelligent chatbot designed for evaluating the correctness of generative outputs for question-answer pairs. Your task is to compare the predicted answer with the correct answer and determine if they match meaningfully. Heres how you can accomplish the task: ##INSTRUCTIONS: - Focus on the meaningful match between the predicted answer and the correct answer. - Consider synonyms or paraphrases as valid matches. - Evaluate the correctness of the prediction compared to the answer. Please evaluate the following video-based question-answer pair: Question: [question] Correct Answer: [target] Predicted Answer: [candidate] Provide your evaluation only as yes/no and score where the score is an integer value between 0 and 5, with 5 indicating the highest meaningful match. Please generate the response in the form of Python dictionary string with keys pred and score, where value of pred is string of yes or no and value of score is in INTEGER, not STRING. DO NOT PROVIDE ANY OTHER OUTPUT TEXT OR EXPLANATION. Only provide the Python dictionary string. For example, your response should look like this: {\"pred\": \"yes\", \"score\": 4.8}. Video Region Captioning (PLM-RCap) We use an LLM-judge to generate the similarity scores between predicted and ground truth captions. The prompt is below. PLM-RCap judge prompt Your task is to compare given pair of captions and provide single score indicating how correct the pred is compared to GT, on scale from 0 to 10. Focus on meaning and context, not exact word matches. Penalize missing and incorrect information, with lower scores for more significant errors. High scores require accurate conveyance of all key GT information. Respond with only the score, starting your response with the number and including no additional text. Output format: [score]. Dense Video Region Captioning (PLM-RDCap) We adapt the SODA metric [54] from dense video captioning literature for this task. To compute this metric, we use the same LLM-judge from above to generate the pairwise similiarity scores between predicted and ground truth captions, which is then fed to the standard metric computation routine. Region Temporal Localization (PLM-RTLoc) We report standard temporal localization metrics, namely Mean Recall@1, averaged over range of IoU thresholds [0.3, 0.5, 0.7, 0.9]. C.4 Evaluation Protocols Common evaluation protocol. For video benchmark evaluations, we sample 32 frames uniformly from the full video unless otherwise specified. For uniformity and consistency across benchmarks, we implement all LLM-judge evaluations using LLama3.3-70B-Instruct [46], following LLM judge prompts from popular evaluation frameworks [45, 222] where available. Outputs from all models are generated via greedy sampling (temperature 0). PLM-VideoBench inference prompts. Table C4 contains example inference prompt examples for each PLMVideoBench task. Note that some variation exists between instances in the benchmark. For example, for RCap prompt may be What is happening to the subject in the region highlighted by the red rectangle ... instead of Give detailed description of the events occurring in the region marked by the red rectangle ..., however they convey the same underlying instruction and information. 20 Proprietary models like GPT-4o and Gemini require more careful prompting to ensure that the output formatting is respected. For example, we append instructions to prevent model hallucinations (e.g., You must use these frames to answer the question; do not rely on any external knowledge or commonsense.), to prevent refusals to answer (e.g., Even if the information in these separate frames is not enough to answer the question, please try your best to guess an answer which you think would be the most possible one based on the question. Do not generate answer such as not possible to determine) and in-context examples to help guide the model towards the correct output format. Modeland benchmark-specific inference prompts will be released along with our code for full reproducibility."
        },
        {
            "title": "Task\nFGQA\nSGQA",
            "content": "Prompt Question: [question] Options: (A) [option1] (B) [option2] Only give the best option. The following question is asked by the camera wearer at the end of the video. Provide detailed answer even if unsure. Try to answer in around 20-30 words. Now answer the following question based on the video content: [question] RDCap Create dense caption of the subjects actions within the red rectangles, including action frames ids and brief descriptions. For each item use the format [start, end]: [description] separated by newline, where start and end are frame numbers between 0 and 31 in this 32 frame video. Give detailed description of the events occurring in the region marked by the red rectangle within frames ([start frame], [end frame]) in this 32 frame video"
        },
        {
            "title": "RCap",
            "content": "RTLoc Given the region marked by the red rectangle in the video, please provide the start and end frame of when [event] happens. Use the format (start, end), where start and end are frame numbers between 0 and 31 in this 32 frame video. Table C4 PLM-VideoBench task prompts. Items in square brackets are placeholders filled in for each benchmark instance. Additional PLM-VideoBench Results We present benchmarking results across all model scales (1B, 3B, 8B) in Table D5, to supplement the 8B model results in the main paper  (Table 5)  . Our approach consistently outperforms baselines across all scales, including proprietary models whose model scale is unknown. Model Human perf. Proprietary GPT-4o [139] Gemini 1.5 Pro [57] Gemini 2.0 Flash [57] 1B scale Qwen2VL-2B [185] InternVL2-1B [29] InternVL2.5-1B [29] PLM-1B 3B scale Qwen2.5 VL-3B [11] InternVL2-4B [29] InternVL2.5-4B [29] PLM-3B 8B scale LLaVA-OV-7B [90] Qwen2VL-7B [185] Qwen2.5VL-7B [11] InternVL2-8B [29] InternVL2.5-8B [29] PLM-8B F B 90.9 61.2 57.1 58. 39.0 35.8 42.3 57.6 43.7 43.2 50.0 67.1 40.2 49.2 49.8 47.7 53.7 67.7 S 67. 63.7 49.9 44.8 38.5 28.9 39.6 40.9 45.1 41.7 49.2 38.8 41.5 44.5 43.0 45.9 48.3 46.2 D S 66.6 20.9 14.4 13.2 0.9 0.3 6.7 50.3 0.3 0.5 4.9 53.1 4.7 4.1 2.5 1.2 5.7 52.8 R c 53.9 35.7 33.1 30.9 18.1 17.2 23.6 40.9 17.2 19.9 25.9 45.0 24.4 17.6 21.5 21.5 26.1 46. L n 67.8 33.1 27.6 27.6 10.8 2.7 1.6 57.7 13.9 9.6 15.4 58. 13.9 15.1 10.7 11.6 8.8 59.1 . 70.9 51.6 44.0 42.5 29.1 23.8 30.8 49. 33.1 30.3 35.3 53.0 32.0 35.3 34.8 35.0 38.5 55.6 Table D5 PLM-VideoBench results across all model scales to supplement results in Table 5."
        },
        {
            "title": "E Additional Results",
            "content": "O f a 90.6 92.6 90.6 C A t 93.2 94.3 93. O R e 88.2 91.5 88.2 + C l 88.2 85.2 88. + C A t 89.0 89.6 89.0 + C B t 75.9 79.8 75. C e v 86.1 88.7 86.1 O R t 87.0 89.4 87. . 86.6 88.9 86.6 88.5 91.5 84.8 83. 88.6 76.5 86.0 86.4 85.7 89.1 93. 91.7 94.9 84.0 89.5 82.4 89.8 88.0 93.6 74.1 84.2 85.2 90. 85.7 90.9 85.0 90.9 90.9 91.7 89.1 87.1 90.3 90.6 92.6 93.6 91.7 91.1 94.5 91.8 87.9 87.3 84.0 80.7 85.9 85.9 83.9 85.8 82.4 79.8 85.2 87. 89.2 90.5 88.0 87.9 91.5 91.3 77.4 79.5 74.1 71.4 78.8 81.1 86.6 87.3 85.2 82.7 86.7 88.8 87.2 87.8 85.7 82.7 87.6 89.2 87.0 87.9 85.0 82.9 87.6 88."
        },
        {
            "title": "Model",
            "content": "Specialists GroundingDINO [107] UNINEXT-H [205] ONE-PEACE [184] 1B scale PLM-1B 3B scale Qwen2.5 VL-3B [11] PLM-3B 8B scale Cube-LLM [33] Qwen2VL-7B [185] Qwen2.5VL-7B [11] InternVL2-8B [29] InternVL2.5-8B [29] PLM-8B Table E6 Image Grounding results on RefCOCO/+/g. PLM performs competitively compared to the baselines across all model scales, and outperforms specialist models for the image grounding task."
        },
        {
            "title": "Long Video QA",
            "content": "h B i o ] 7 9 1 [ ) ( e ] 6 8 1 [ a 66.7 37.2 33.1 64.0 61.6 - ] 1 3 2 [ M ) ( L 67.4 69.9 69.5 42.0 31.4 35.3 40.0 47.9 43.3 47.9 52.3 62.7 52.0 57.3 58.9 43.3 34.0 40.1 40. 54.2 68.2 53.0 59.9 56.3 68.3 65.0 57.9 38.8 55.7 55.8 46.0 45.3 56.0 55.4 37.0 43.2 60.0 56.9 44.5 64.6 69.8 70.2 64.0 68.9 66.4 Model Proprietary GPT-4o [139] Gemini 1.5 Pro [57] Gemini 2.0 Flash [57] 1B scale Qwen2VL-2B [185] InternVL2-1B [29] InternVL2.5-1B [29] PLM-1B 3B scale Qwen2.5 VL-3B [11] InternVL2-4B [29] InternVL2.5-4B [29] PLM-3B 8B scale LLaVA-OV-7B [90] Qwen2VL-7B [185] Qwen2.5VL-7B [11] InternVL2-8B [29] InternVL2.5-8B [29] PLM-8B Table E7 Video benchmark results on long video understanding tasks. We compare PLM with open-access baselines and proprietary models of comparable scale, and report results over 3 long video QA benchmarks. Cells with * are reported numbers from literature. The remaining are reproduced using official code."
        },
        {
            "title": "F Qualitative results",
            "content": "Question: Provide detailed description of the image. Answer: The image is pink postage stamp featuring map of the Grenadines of St. Vincent, with red border and black text. The map shows various locations on the island, including hotels, bays, and other points of interest. *Key Features:* * *Map:* pink map of the Grenadines of St. Vincent, with red border and black text. * *Locations:* Various locations on the island are marked, including hotels, bays, and other points of interest. * *Scale:* scale in miles is provided at the bottom of the map. * *Denomination:* The denomination \"30c\" is printed in the bottom-right corner of the stamp. * *Border:* The stamp has red border with serrated edge. *Overall Impression:* The image appears to be vintage postage stamp from the Grenadines of St. Vincent, featuring map of the island and various locations of interest. The stamp has classic design and is likely from the mid-20th century. Figure F5 PLM-8B OCR examples. Figure F6 PLM-8B OCR in the wild examples. Figure F7 PLM-8B OCR in the wild examples. Question: What does the word say together from the bikers? Answer: The word says Indianapolis. Question: How tall is the kitty character in the image? Answer: The kitty character is approximately 100 cm tall. 23 Question: What is inside the oven? Answer: There is baking sheet inside the oven. Question: What temperature is the oven set to? Answer: The oven is set to 200 degrees. Question: What unit is the ovens temperature set to? Answer: The ovens temperature is set to Celsius. Question: Describe the image in details Answer: city skyline with large body of water in the foreground. The city skyline is made up of tall buildings, some of which are skyscrapers. The buildings are densely packed together and vary in In the height and color. foreground, there is large body of water that appears to be harbor or bay. There are many boats and ships in the water, some of which are docked at the shore while others are sailing or anchored in the middle of the water. Figure F8 PLM-8B Natural image QA Figure F9 PLM-8B Captioning."
        },
        {
            "title": "RDCap",
            "content": "Question: Describe the subjects actions within the red rectangles, providing start and end frames ids and brief description for each action in dense caption format. For each item use the format [start, end]: [description] separated by newline, where start and end are frame numbers between 0 and 31 in this 32 frame video. Answer: Frame [0, 6]: Out of frame Frame [6, 15]: woman is walking towards the camera with child. She is holding the childs hand with her right hand and carrying bag in her left hand Frame [16, 31]: Out of frame."
        },
        {
            "title": "RCap",
            "content": "Question: What is happening to the subject in the region highlighted by the red rectangle during frames [6, 15] in this 32 frame video? Answer: woman is walking on muddy road with child. she is holding the childs hand with her right hand and adjusting her saree with her left hand while walking"
        },
        {
            "title": "RTLoc",
            "content": "Question: Please provide the start and end frame of when an individual standing near water body is drinking from water from bottle happens corresponding to the region marked by the blue rectangle. Use the format [start, end] where start and end are frame numbers between 0 and 31 in this 32 frame video. Answer: [23, 26]"
        },
        {
            "title": "Captioning",
            "content": "Question: Please describe the video in detail, focusing on actions. Answer: This video features group of people walking around muddy area with small bridge in the background. The video is taken outdoors and the weather is sunny. The video starts with man standing on the left side of the frame, facing the camera. He is wearing white shirt, black vest, black trousers and white shoes. He has scarf around his neck. The camera pans to the right and shows group of people walking around. The video is taken from low angle and the camera is handheld. The video ends with man walking towards the left side of the frame. He is wearing blue shirt, black trousers and brown shoes. He is holding bottle in his right hand and is drinking from it. Question: Please explain what the man in blue t-shirt is pointing at in the video. Answer: The man in blue t-shirt is pointing at the bridge. Figure F10 PLM8B Video Region Captioning and Grounding."
        },
        {
            "title": "Finegrained QA",
            "content": "25 Question: Please give me details of the video focusing on actions and interactions. Answer: This video features close up shot of hand holding plush toy mouse with grey kitten standing on white chair, reaching out and touching the toy with its paws. In the beginning of the video, the kitten is seen standing on the chair and reaching out with both paws to touch the toy mouse. The kitten then withdraws its paws and sits back on the chair. The hand holding the toy mouse is seen moving the toy closer to the kitten and then away from the kitten. The kitten then reaches out again and touches the toy with its paws. The hand then moves the toy away from the kitten again and the kitten looks to the left of the frame. This video is set against white background. Figure F11 PLM8B Video Captioning Question: What are the ingredients being thrown into the mixer? Answer: Carrots, apples, and bananas. Figure F12 PLM8B Video QA"
        },
        {
            "title": "G Baseline Implementation Details",
            "content": "We provide baseline-specific implementation details for all models in 6.1 of the main paper. Proprietary baselines We evaluate the GPT and Gemini family of models. For GPT-4o, we use the GPT4o-2024-11-20 checkpoint . We feed 32 uniformly sampled frames regardless of video length, loaded at high image quality setting. For Gemini, we evaluate Gemini-1.5-Pro and Gemini-2.0-Flash. For VQA tasks, we input the video (without audio) which is processed internally at 1 fps. For spatio-temporal tasks (RCap, RDCap, and RTLoc) we use the same inputs as for open-source models and GPT-4o. Open-source models We evaluate InternVL, Qwen, Molmo and Llava-OV models. We follow official implementation and preprocessing pipelines for each. Specifically, we evaluate InternVL2 and InternVL2.5 (code); QwenVL2 and QwenVL2.5 (code); Molmo-O-0924 (code) and Llava-OV (code). For QwenVL, we sample frames at 1 fps from videos. For InternVL2, we use 12 tiles per image as this more closely matches the reported results. Human performance baseline. In Table 5, we report human performance on PLM-VideoBench. For each task, we present annotators with the test sets directly and collect answers for each instance given the standard task prompt. Given the difficulty of RDCap, we reuse our data annotation pipeline in to collect new dense caption independently from an annotator, rather than providing the standard task instruction. 27 PLMFGQA: Fine-grained QA We present PLMFGQA Fine-grained QA (FGQA), video dataset focused on how actions are performed, capturing nuanced fine-grained details through specially designed questions and carefully annotated answers. Due to the scarcity of fine-grained video Q&A data, see Table H8, we built data engine to enable the collection of our 2.4M Q&A dataset, PLMFGQA. Year Dataset 2016 MovieQA 2017 MSRVTT-QA 2017 TGIF-QA 2017 MSVD-QA TVQA 2018 ActivityNetQA 2019 2020 How2QA 2021 NexT-QA #Q&As 6462 243690 165165 51000 152545 58000 44007 52044 PLM-FGQA 2025 Dataset STAR CLEVRER EgoQA PerceptionTest VideoInstruct MoVQA CinePile Sports-QA Year 2021 2023 2024 2024 2024 2024 2024 2025 #Q&As 60000 82620 19000 44146 25803 21953 303828 94000 Table H8 Comparison of our PLMFGQA dataset with existing video-QA datasets. H.1 Annotation process: Data Engine Our data engine is built upon the following modules: (1) Temporal Segment Generation, (2) Question Generation, (3) Answer Generation, (4) Human Annotation (answer verification/manual answer annotation), (5) Quality Control, as illustrated in Figure H13. Next, we describe each module in detail, and finally also provide additional details about the extra steps we took for forming the FG-QA component of PLM VideoBench out of these annotations. Figure H13 Data engine used to collect the PLMFGQA dataset. H.1.1 Temporal Segment Generation We source the video data that serves as basis for our annotations from publicly available datasets. Based on the video sources and the type of existing annotations, we split the videos into three distinct categories. Videos with existing ground-truth segment annotations: We directly adopt segments with their human-annotated action annotations from the following datasets: Ego4d Goal-Step[167], Ego4D Moments[60], EgoExo4D [61], HT-Step[4, 126], COIN [171], CrossTask [236], and YouCook2 [232]. All those sources provide video segment boundaries accompanied by some form of textual action descriptions, and are therefore readily usable with the rest of the pipeline. Unedited videos of physical activities: For physical activities videos (e.g. basketball, dancing, soccer), actions are usually atomic and short (e.g. dribble, dance move, kick) and therfore reuqire precise temporal localization. To source videos for these scenarios we used data from EgoExo4D [61] that contains temporally well-aligned and precise narrations; we obtained segments of 2-3 seconds centered around narration timings, and used the anchor narrations directly as the action description. Raw, untrimmed videos in-the-wild without temporal segment annotations. We source very large part of our data from untrimmed instructional videos in the large-scale HT100M dataset [128] which we first need to segment before use. The goal is to obtain video clips that contain meaningful, salient actions, and also caption 28 Figure H14 Distribution of question types (left) and video sources (right) in the FGQA component of PLMVideoBench. the resulting segments with concise but accurate action descriptions. We describe the automatic segmentation and captioning module in the following. The automatic segmentation and captioning pipeline involves the following three stages: Temporal segment proposal. Given untrimmed long videos, the first step is to identify semantically coherent segments within them. Inspired by prior work on unsupervised action proposal and segmentation, we leverage visual feature clustering to generate temporal segment proposals, and use shot-boundary detection results to further refine the segment boundaries. We extract clip-level visual features[190] using sliding window with temporal stride of 1 second. We then compute the pairwise similarity between neighborhood features and detect the class-agnostic action boundaries using boundary detection kernel (similar to those used in literature[44, 74]). Finally, since the detected segments are usually over-segmented, we perform bottom-up agglomerate clustering approach to group adjacent segments into clusters, using segment duration prior of 10 seconds. We also leverage shot boundary detection[1] to obtain precise moments of scene changes: we refine the boundaries of the segment proposals by aligning them to the detected shot boundaries when theyre sufficiently close ( 1 second). Segment filtering and ranking. How-to videos often include lot of content that is irrelevant to the demonstration of the activity at hand, such as the instructor explaining what they are about to do or showcasing tools and ingredients. It is therefore important to detect and filter segments with such uninformative content. To that end we rank candidate segments according to relevance using series of heuristics and learned models, described bellow. a. Talking head detection. common mode in instructional videos is instructors talking into the camera, describing objects or explaining actions theyre about to take. To detect and remove such segments, we employ an Active Speaker Detection (ASD) pipeline[35], which we run densely on every video and combine resulting talking head tracks, to produce an ASD score for every segment. b. Hand-object interaction (HOI) detection. The presence of hand-object interaction (HOI) can be good indicator of visually groundable actions. We leverage the temporal selection strategy[43] to filter out the segment proposals that contain hand-object interaction. We first employ an off-the-shelf robust HOI detector[158] to densely extract HOI regions within proposed segment. The HOI score is then calculated by measuring the likelihood of hand-object interaction in the segment and the averaged probability of all the detected hands. c. ASR groundability. HT100M videos contain timestamped ASR captions, which are speech transcriptions of the audio instructions. It is desirable to rank candidate segments based on how likely their ASR content is to their video content. The hypothesis here is that segments containing ASR transcriptions that align well to the video content, are more likely to be visual-information rich. Moreover since the action labeling pipeline (described next) relies on ASR metadata for producing descriptions, higher ASR groundability scores make it likelier to produce good quality segment descriptions. For every candidate segment, we compute an ASR-groundability score by computing video-text alignment scores[190] for each ASR caption within the segment and then averaging the ones that are above threshold (we use 0.5). 29 d. Relevance classification. The above heuristics work well for the clear-cut cases they are tailored for, but in practice we found that they struggle with more nuanced segments (e.g. instructor fiddling with an object and describing it rather than using it). To improve the detection of those cases, we manually labelled small amount of segments that passed through the other filters and trained binary classifier to classify them as relevant or irrelevant; to that end we trained simple 2-layer MLP classifier on top of temporally pooled video representations with logistic loss for binary classification. We deployed the trained model to provide relevance score for all the candidate segments. We combined the scores resulting from all the modules described above and determined cutoff thresholds, based on small manually annotated validation set. In production, we keep all the segments that have relevance scores above those thresholds. Segment captioning We follow two-step process to obtain action labels for each unlabeled segment: In the first step, collection of off-the-shelf perception models are used to extract individual image-level captions, video-level captions, and object detections from the segment. The output of all perception models is then fed as text into an LLM to generate long, fine-grained captions. At the second step, the detailed captions are fused with the ASR content of the segment, to obtain consice action description. Specifically, we query an LLM (Llama 3.3 70B [46]) with the following prompt:"
        },
        {
            "title": "Segment to action labels prompt",
            "content": "Detailed description: [fine grained caption] ASR transcription: [asr caption]. Given the detailed description above, identify the specific action performed as part of the activity [task name]. Your response must not be the same as the activity [task name] and needs to be specific substep within the activity [task name]. Please also supply rationale for your answer. The extracted labeled video segments obtained through the above process serve as the foundation for the subsequent Q&A generation. H.1.2 Automatic Question Generation We automatically generate questions about the fine-grained details of the way activities are executed in the video. Our questions is generated with variety of prompts and models which lead to increased question diversity and specificity. In Table H9 we present the question types and sample questions per question type. Here, we summarize how these questions are generated automatically with an ensemble with models and prompts: LLM-based action-conditioned question generation Given segment, its action name (e.g., cut potatoes), task name (e.g., How to make sweet potato gratin) and optionally other metadata about the segment (for example, recognized objects [? ]), we generate questions that can elicit descriptions of fine-grained details by raters with an LLM. We use tailored prompts for generating questions that cover how the activity is executed (tools, object locations, object states, direction of movements, hand pose), and the spatial arrangement of objects."
        },
        {
            "title": "Activity FG question generation prompt",
            "content": "I am learning how to [action name] while [task name]. Ask me [N ] most relevant questions that reveal the details of the way the step is executed in my environment, e.g., (a) part location, (b) types of tools/ingredients used, (c) direction of movements, (d) how are objects held, (e) object states at the beginning of the step, (f) object state at the end of the step. The questions must be answerable by visually observing the activity, without reading instructions or trying out. Please indicate the type of question from (a) to (f) for each question asked at the beginning of the question."
        },
        {
            "title": "Spatial FG question generation prompt",
            "content": "Imagine have no common sense or understanding of the 3D real world. am trying to [task name] and am at the step where am [action name]. Theres [object list] when Im [action name]. Ask me [N ] questions about the 3D position of objects, relative location between objects, distance between objects, spatial relationship using prepositions like above, below, next to, etc. that might want to know. The questions must be answerable by only visually observing me performing activity, without reading instructions or trying out. We explicitly encourage the LLM to provide questions that can be answered solely based on the video frames, in contrast to questions that are focused on external knowledge or non-groundable concepts or judging the execution of the step (e.g., avoid questions like is the pan hot enough to add the oil?, what tool is typically used to loosen the axle nut). The rationale for this is to collect as many Q&A pairs that model cannot answer just based on external knowledge/language prior, but they rather require vision perception to be 30 answered. Note that these questions are generated without visual input, hence they are not instance-specific and might not be answerable given the video segment. VLM-based instance-specific question generation After collecting first set of Q&As using the LLM-generated questions, we bootstrap VLM Question Generator model, which takes as input the video segment, question types and optionally the task name, and generates set of instance-specific visual questions. The VLM Question Generator model is obtained by supervised fine-tuning of PLM with question generation instructiontuning dataset which consists of triplets (video, prompt, response), where the prompt includes the instruction to generate questions based on question types and the response includes example questions to be generated for the given video. Due to the lack of such dataset with fine-grained question, we synthetically generated it by utilizing the Q&A pairs obtained based on the LLM-generated questions. Specifically, for each video segment, we use an LLM to (1) decompose existing Q&A pairs into multiple Q&A pairs, with each new question focusing on one detail of the original answer; (2) tag question types for the generated questions based on an expanded list of question types; and (3) generate (prompt, response) pair for the segment. This resulted in 600k training instances."
        },
        {
            "title": "VLM Question Generator training sample",
            "content": "Generate 3 different questions that reveal the fine-grained details of the way the activity is executed. In particular, focus on these question types: fine-grained object locations, hand pose, object/repetition counts, generating at least one question per type. Write each question in separate line, e.g., Q1. first question. Q2. second question. . . . QN. N-th question. Response: Q1. Where are the tomatoes positioned prior to being cut? Q2. How is the person grasping the tomato with their left hand? Q3. How many tomatoes did the person use in the segment? LLM-based follow-up question generation This final set of questions aims to increase coverage of video details and generate highly fine-grained questions by leveraging the already collected Q&A pairs for each segment and feed them to an LLM that generates follow-up questions that are more detailed and challenging than the initial questions. Follow-up question generation prompt have the following information gathered about the video: [list of previous Q&A samples] Utilizing information and details from all the provided Q&A pairs (make sure to specialize questions based on the already corrected answers, e.g., using referring expressions), ask [N ] most relevant and interesting, visual questions that we can ask annotators in order to reveal NEW, rich, additional fine-grained details about the video that we dont know yet, in particular about the following question types: tools/ingredients, object counts, repetition counts, direction of movement, hand pose, fine-grained object locations, spatial relations, initial state/end state, action happened before/after, clothes wearing, body pose, main action in the video, temporal extent of action, sizes. The questions should be specific and have specific answer. Avoid generic questions that can be very tedious to answer, e.g., how many objects are there in the scene. Also, do not generate questions that start with Is ... and then list options. Prefer open-ended questions, e.g., starting with How. [... More examples & formatting ...] H.1.3 Automatic Answer Generation The next step of the data engine aims to produce correct and comprehensive answers to the generated questions. We obtain automatic answers to the generated questions using version of PLM that has been fine-tuned with extra privileged information of various forms as input. The privileged information includes textual annotations from the metadata available with the candidate training videos and feature embeddings extracted from off-the-shelf models. Useful textual metadata include the video title, ASR captions or written descriptions, video-level task name (infered by an LLM using the title and captions), and any existing QAs for that video. Off-the-shelf embeddings include frame-level features extracted denseley at 1 fps; we use an open-vocabulary object detection model, OWLv2 [129], for embedding object detection information and CLIP ViT-L14 embeddings [149] for scene classification information. We incorporate the textual annotations directly into language prompts using the following template:"
        },
        {
            "title": "Automatic answer generation prompt",
            "content": "A video is showing task [video level task name], specifically the part where [ASR caption]. Here is what we already know about the video: [existing question-answer pairs]. Answer this question in detail: [question]"
        },
        {
            "title": "Spatial Relations",
            "content": "What is the process being performed on the sandpaper? What is the action shown? What does the person do after brewing the tea? What does the person do before marking the vinyl with pencil? What is the quantity of universal down cleaner being poured into the task area? How many branches does the person cut in total? How many times does the person spray Greased Lightning onto the ketchup spill? In what direction is the black welding tool pointing while the person is working on the metal joint? How does the person chop the garlic with the knife? What is the color of the seatpost shown in the video segment? What is the shape of the tube at the end of the step? What is the size of the knife being used to chop the spring onions? Where does the person put the honey bottle away? Where does the person position the clothes before ironing? What type of roller and paint are being used? What does the person place on top of the smooth half of the egg carton? What was the person initially holding in their left hand? How would you describe the sink at the beginning of the cleaning process? What is the state of the nematode after mixing it with water and sponge? At what point in the video is the person seen holding the wires? How are the womans legs positioned while she is sitting? How bent is the left elbow during the activity? How far is the bias tape maker from the right edge of the ironing board? What is the spatial relationship between the bowls and the Brussels sprouts on the kitchen countertop? Speed/Force How would you describe the consistency of pressure applied during sanding? How fast does the person initially push the stone? Table H9 PLMFGQA question types and sample questions The off-the-shelf embeddings are incorporated into the PLM input via an additional Perceiver-IO[67] tokenizer, which summarizes the embeddings at the segment level. We fine-tune the answer generator on 1M manually annotated QA pairs. After fine-tuning, we deploy the trained answer generator with privillged information access on the unlabelled questions produced in the previous step, to produce automatic answers. H.1.4 Human Annotation After obtaining segments and generating questions and automatic answers, we employ human annotators to obtain high-quality answers. Our answer annotations include the following: Human-verified answers: Raters are provided with the model-generated answer and are asked to accept or reject the answer. They can reject questions for being irrelevant or unanswerable, and answers for being factually incorrect or lacking details. Accepted question-answer pairs proceed without changes, while rejected ones are handled differently: question-related rejections (irrelevant or unanswerable) are discarded, whereas answer-related rejections (factually incorrect or lacking details) are marked for correction in the next phase. 17.8% of the total training samples are human-verified automatic answers. Human annotated answers: Raters answer the questions from scratch by ensuring to cover all the relevant details within the temporal segment. They receive reference information, such as video-level task names and ASR captions, and may use online resources like WikiHow for additional context. Questions that cannot be answered based on the video segment (for example, due to some false premise) are rejected (with an explanation). These manually annotated answers make up 82.2% of the PLMFGQA training split, and 100% of the evaluation set. 32 Quality Control. Data quality is crucial for model success. We followed several strategies to monitor and enhance annotation quality: annotation Certification - we reviewed small sample of annotations from each rater before they could work in production queues, ensuring that annotators met high-quality standards before advancing to production; golden Examples - annotators were provided with high-quality annotation examples, highlighting common error patterns and offering acceptable answers. targeted and Dual QA - we conducted daily audits, including vendor auditing and our own sampled quality control. In total, 13% of the training set was audited, and 100% of the samples in PLMVideoBench underwent quality control. H.2 FG-QA PLMVideoBench Construction Sources stats Total Videos Unique Source Videos Average Duration (sec.)"
        },
        {
            "title": "Test",
            "content": "767k 251k 9.8 3.6k 1.9 12.3 Annotations stats Number of QA Pairs Number Question Types Question Length (avg/max) Answer Length (avg/max) Annotation Type Open-Domain 2.4M 12 12/114 13.3/911 Human Yes 4.2k 12 12.3/56 14.1/62 Human Yes Table H10 Statistics of the PLM-FGQA training and test data. The test split refers to the FGQA module of PLM VideoBench. The FG-QA component of PLMVideoBench is formed from held-out portion of PLMFGQA. We refine this set and transform it into challenging MCQ-based benchmark by (1) generating MCQs, (2) filtering out samples that can be answered by text-only (blind) LLMs, (3) performing human verification of negatives, and (4) balancing the distribution of question types and domains. The statistics of the dataset are summarized in Table H10. In more detail the steps we followed are: MCQ Generation: To transform QAs into challenging MCQs for evaluation, instead of generating random incorrect answers, we prompt LLMs to produce hard negatives that are semantically close to the correct answer. We use the following prompt which was designed to generate distractors that differ from the correct answer by only single detail. In effect this enables evaluation to assess fine-grained reasoning about object attributes and tool distinctions."
        },
        {
            "title": "MCQ generation prompt",
            "content": "Here is question and answer pair about video: Q: [question] A: [answer ] You need to transform this into high-quality multiple-choice question. To do this, first rephrase the given correct answer and then provide distractor answers. The incorrect answers should be reasonable and valid responses to the question, but should have different meaning than the correct answer. You generate an incorrect answer from the correct one by changing single detail, e.g. an object or verb/action that is relevant to whats being asked. Make the incorrect answers realistic, plausible and similar enough to the correct answer so that it is very difficult for someone to distinguish between them with prior knowledge alone. Finding the correct answer should also require visual information about the scene. The distractor answers should answer the question, but should be incorrect but in non-obvious way. When changing single detail to create the distractors, make sure that this detail is the main point of the question. For example, if the question is about the color of an object, then the distractor should change the color of the object and not the kind of object. Here are some examples of good distractors (desired) and bad distractors (to be avoided): Q: What is the person wearing on their hands while applying varnish? A: The person is wearing white gloves on their hands while applying varnish with brush. Good distractors: - The person is wearing black gloves on their hands while applying varnish with brush. Bad distractors: - The person is wearing black gloves on their hands while applying paint with roller. ... More examples & formatting ... 33 Filtering Text-Only Answers: To ensure that video-based reasoning is required, we test whether text-only LLM can answer the question correctly without seeing the video. If question can be answered correctly from text alone, we remove or modify it to emphasize visual and temporal grounding. Human Verification of Negatives: Automatically generated negatives may sometimes be factually true despite being labeled as incorrect. To address this, we perform human verification, where annotators review distractors to confirm that they are both plausible yet definitively incorrect given the video context.MCQs with ambiguous distractors are removed. Balancing Question Types: Finally, after the above postprocessing and filtering is done, we rebalance the test set, to make sure that the question type and domain distributions are approximately uniform, by undersampling over-represented qyestion types and domains. Note on the evaluation metric. We report the multi-binary accuracy (MBAcc) [18] to evaluate on the FG-QA task. This accuracy is calculated by comparing the correct answer to each distractor individually. Specifically, for each question, we generate series of binary questions, where the correct answer is compared with one distractor at time. prediction is considered correct only if the correct answer is consistently selected across all binary comparisons. We preferred this metric to vanilla MCQ accuracy as it greatly reduces the predictability of automatically-generated MCQs. PLMSTC Details We present PLM Spatio-Temporal Captions (PLMSTC), novel dataset aimed at training and evaluating VLMs for spatial-temporal reasoning. We collected pairs of mask tublets for objects in videos, along with their corresponding detailed temporal descriptions. The annotations are collected on top of the SA-V [152] videos, which are diverse and high-quality. We excluded the test set videos from SA-V, to avoid any data cross contamination. Table I12 provides statistics about the dataset, such as number of total samples, training/val/test splits, object types, and time-segment duration. PLMSTC, is not only novel, but also larger and higher quality compared to existing datasets, see Table I11. Dataset DAVIS16-RVOS [146] DAVIS17-RVOS [17] YouCook2-BB [232] A2D Sentence [206] J-HMDB Sentence [47] ActivityNet Entities [233] VidSTG [225] Refer-Youtube-VOS [157] HC-STVG [173] VLN [179] MeVis [42] PLM-STC"
        },
        {
            "title": "Spatial Type\nSegmentation\nSegmentation\nBBox\nSegmentation\nSegmentation\nBBox\nBBox\nSegmentation\nBBox\nMouse Trace\nSegmentation\nSegmentation",
            "content": "Year 2018 2018 2018 2018 2018 2019 2020 2020 2021 2023 2023 2025 #Videos Regions Temp. Seg. Captions? 50 90 647 3.7K 928 14.3K 6.9K 3.9K 16K 50K 2K 45.7K 50 205 - 4.8K 928 1.5M 44.8K 7.5K 16K 43.1K 8.8K 127.8K - - 4.3K - - 52K - - - 43.1K - 197.3K"
        },
        {
            "title": "No\nNo\nNo\nNo\nNo\nNo\nNo\nNo\nNo\nYes\nNo\nYes",
            "content": "Table I11 Spatio-Temporal-Captioning datasets comparison. We describe the overall annotation process in Appendix I.1, and how we build the three sub-tasks in Appendix I.2. I.1 Annotation process Figure I15 PLM-STC Annotation pipeline. 34 The annotation process is summarized in Figure I15. The annotation process involves three stages: Object Selection and Tracking, Temporal Segmentation and Captioning and Verification and Quality Control. I.1.1 Object Selection and Tracking Annotators select interesting objects with significant motion changes in the video and use SAM 2 [152] to generate initial mask tublets, which they then refine to ensure high-quality spatial-temporal segmentation. We instructed the annotators by defining interesting regions in video footage as those with the presence of significant, dynamic actions performed by subjects, which can be human, animal, or object. These regions involve multiple major actions that evolve over time, rather than static or insignificant actions. We provided annotators with examples of interesting regions, such as one featuring person making sandwich, dog chasing cat, or kite getting stuck in tree. The goal for the annotator is to identify regions with high delta, where the subject performs sequence of significant activities that change over time, such as person entering room, sitting down, and then drinking from glass. By focusing on these dynamic and evolving actions, annotators can effectively select regions worthy of captioning. Finally, annotators are provided with several examples of good and bad annotations. I.1.2 Temporal Segmentation and Captioning Based on the selected mask tublets, another set of annotators provides time segments for each action and fills in the caption within each time segment. The annotators are instructed to focus on capturing major actions, avoiding minor details or unnecessary movements. When writing captions for each segment, they must ensure clarity in describing the subjects movements and directionality. Additionally, the annotators are advised to avoid making assumptions about the subjects actions or adding details not clearly visible, sticking only to what is directly observable in the frame. As in the previous task, the annotators are provided with several examples of good and bad annotations to guide their work. I.1.3 Verification and Quality Control final set of annotators manually verifies the tublets and time-segment captions to ensure accuracy and consistency. For mask refinement, we re-run the same pipeline as I.1.1, while not letting the annotators choose the interesting object, but only refine the quality of the mask. For captioning refinement, the annotators are tasked with three objectives: 1) Redundancy: eliminate any repeating or redundant information to ensure the caption is concise; 2) Accuracy: verify that every word in the caption accurately describes fact present in the video, correcting or removing any incorrect information; and 3) Actions: add missing major action information to the caption while preserving existing atomic actions, ensuring the caption effectively conveys the key events in the video. I.2 PLMSTC Benchmark We utilize the collected data to train and evaluate the PLM on three challenging tasks that are essential for video perception. Firstly, we created balanced validation and test split by the combination of tube categories and number of caption per tube while making sure no video overlaps with the training set. This is done to make sure we evaluate all the categories presents in the dataset equally. Then, we process the data for each task: Dense Video Region Captioning (RDCap). This comprehensive task combines both what and when aspects. The model takes the video and the tubelets as input and outputs the full time-segment captions. We also assign an out of frame caption to temporal segments for which the subject does not appear in the video to ensure dense temporal coverage of events across the video duration. Video Region Captioning (RCap). This task involves describing what activities are performed within specific time frame by the objects in the tubelets. The model receives the video, the tubelets, and the temporal region as input and outputs the corresponding captions. We filter out events that refer to the subject when it is out-of-frame to avoid evaluating trivial captions."
        },
        {
            "title": "Dataset stats\nNumber of Videos\nSpatio Temporal Caption\nTemporal Caption",
            "content": "Tubes categories Person Animal Object/things Temporal captions per Tube 1 caption per tube 2 caption per tube 3 or more Caption per tube"
        },
        {
            "title": "Test",
            "content": "45.2K 127.8K 198.7K 42.0K - - 104.5K 99.6K 13.2K 16.8K 4.4K 6.4K 804 - - 861 550 436 842 78.9K 30.9K 566 16.38K 14.15K 73.9K 27.8K 2.3K - - 2.4K 1.7K 1.2K 2.4K 1.7K 1.2K Tasks stats Region Detailed Captioning (RDCap) Region Captioning (RCap) Region Temporal Localization (RTLoc) 122.3K 117.2K 2.5K 2.6K 194.2K 179.5K 4.6K 10.1K 192.0K 179.5K 4.6K 7.9K Table I12 PLMSTC dataset statistics. Region Temporal Localization (RTLoc). This task requires the model to localize when specific events occur in relation to given tubelet. The input includes the video, the tubelet, and the caption, while the output is the start and end frames indicating when the captioned event occurs. Like RCap, we filter out out-of-frame events, as well as ambiguous events that may be be localized to multiple time segments. For example, if the subject opens the door twice, the event text are guaranteed to be unique (e.g., referring to the first and second time they opened the door) or dropped entirely if ambiguous (e.g., if the text only mentions the action). These tasks are designed to both improve and evaluate the models capabilities, with the same input-output format applied during both training and evaluation. Figure 6 illustrate an examples of the task, including the prompt used to train and evaluate the PLM."
        },
        {
            "title": "J Smart Glasses Data",
            "content": "J.1 Data collection and annotation We collected the source videos for PLM-SGQA using commercial smart glasses, which enable participants to capture egocentric videos in hands-free manner. Participants are presented with 14 categories of popular scenarios, such as shopping, cooking, and walking in neighborhood, and are instructed to ask questions about their surroundings as if interacting with multi-modal assistant that shares their visual perspective. Specifically, participants are asked to ask questions spontaneously, without delay, about the things they see and experience, and to focus on visual queries rather than dynamic information that may change regularly. After recording the videos, participants annotate the segments by marking the start and end points of the video relevant to each question, as well as providing the ground-truth answer. J.2 SGQA Benchmark To create the SGQA component of PLMVideoBench we first filtered the Q&As using an LLM to obtain shortlist of questions that focus on human activity and also are perception-based rather than based on general knowledge. This means that SGQA focus on questions that require good visual understanding of the scene to be accurately answered. This process yields an evaluation set consisting of 655 Q&As. For the resulting Q&As, we then trimmed the original videos to obtain clips within the temporal boundary that the human wearer/annotator specified. As the annotated segments end at the point where the smart-glass wearer asks the question, it is important for all evaluations to specify that the question refers to the end of the video clip e.g. see the prompt we used for PLM and baselines evaluation in C4. We summarize the statistics of the SGQA test set in Figures J16 and J17. 36 Sources stats Total Videos Average Duration (sec.) 663 29.4 Annotations stats Number of QA Pairs Number Domains Question Length (avg/max) Answer Length (avg/max) Annotation Type Open-Domain 665 14 9.0 / 52 21.6 / 40 Human Yes Figure J16 Statistics of the PLMSGQA test data. Figure J17 Domain distribution of video-clips in PLM-SGQA."
        },
        {
            "title": "K Synthetic Data Engine",
            "content": "Our data engine targets base capabilities of VLMs: image captioning, visual question answering, OCR, chart/diagram understanding, and video understanding. We developed different pipelines for images and videos, and includes different levels of metadata to generate captions and QAs. Image Captions: We caption high-quality images using Llama 3.1V 90B. An example is shown in Figure K18. We use this pipeline to caption SA1B [81], Object365 [160], and OpenImages [85]. OCR QAs: We leverage pre-extracted OCR and use it as input for LLM (i.e., Llama 3.3 70B) to generate set of five question-answer pairs. An example is shown in Figure K19. We use this pipeline to generate QAs for PDFAcc [141], and UCSF [140]. Image Captioning plus QAs: In cases for which OCR does not provide enough information to create questions (e.g., scientific figures), we futher caption the image using Llama 3.1V 90B. Then we pass the caption with auxiliary metadata (e.g., OCR) to LLM (i.e., Llama 3.3 70B) to generate question-answers pairs. An example is shown in Figure K20). We use this pipeline to generate captions and QAs for ArxivQA [192], DocVQA [124], InfoVQA [125] and Ai2d [78]. Video Captioning plus QAs: An image captioner is run on key-frames of the video, as well as video captioner on the overall video at 1 fps. The result captions are passed to LLM (i.e., Llama 3.3 70B, or Llama 3 405B) with additional metadata (e.g., video title etc.), so to generate detailed caption and multiple-choices question answers pair. An example is shown in Figure K21). We use this pipeline to generate captions and QAs for YT-1B [218], Ego4d [60] 3, DiDeMo [8], Charades [163], and Kinetics710 [75] 4. 3For this dataset we used Llama3 405B, rather than Llama 3.3 70B 4DiDeMo, Charades, Kinetics710 used simpler pipeline where only frame captions were used, and smaller scale LLM (i.e., Llama 3.1 8B)"
        },
        {
            "title": "Detailed Caption",
            "content": "The image depicts yellow building with red door and sign that reads \"YOLCU SALONU\" in black letters. The building has gable roof and small window on the left side. There are two trees in front of the building, one on each side of the entrance. The ground is wet, and there is snow on the trees and the ground. In the background, there is train station with train on the tracks. The sky is gray and overcast. The image appears to be photograph taken on cold winter day. The presence of snow and the gray sky suggest that it is winter scene. The building and the train station in the background suggest that it is public place, possibly train station or bus stop. The sign on the building reads \"YOLCU SALONU,\" which is Turkish for \"Passenger Lounge.\" This suggests that the building is waiting area for passengers. Overall, the image depicts quiet and peaceful winter scene, with focus on the building and the surrounding environment. Figure K18 Detailed caption example, and the corresponding pipeline. The captioner (i.e., Llama 3V 90B) is prompted to generate the caption for the provided image. Image: Link"
        },
        {
            "title": "OCR",
            "content": "Ladies Division Three THIS ROUND TOTAL Diamond CLUB RINKS WIN FOR AGAINST MARGIN POINTS RINKS WINS FOR AGAINST MARGIN POINTS Earned The Albert 2 0.5 57 57 0 6.5 10.5 4.5 315 276 39 36 525 Coolum Beach 1 0.5 57 57 0 3.5 7.5 3.5 312 274 38 26 375 Mooloolaba 2 1 72 50 22 7 7.5 2 239 238 1 24.5 375 Club Maroochy 0 0 0 0 0 0 7 2 250 225 25 23 350 Nambour 1 0 50 72 -22 3 5 1 240 256 -16 16 250 Tewantin-Noosa 1 0 42 66 -24 3 4.5 0 197 273 -76 13.5 225 Buderim 2 1 66 42 24 7 3 2 218 229 -11 11 150 TOP 3 TEAMS FROM EACH SECTION + THE NEXT TWO HIGHEST TEAMS ON POINTS, QUALIFY FOR FINALS"
        },
        {
            "title": "Question Answers",
            "content": "Q: How many teams qualify for the finals? A: The top 3 teams from each section, plus the next two highest teams on points, qualify for the finals, making total of 5 teams. Q: How many points has the Buderim team earned? A: The Buderim team has earned 150 points. Q: Which team is currently in the lead? A: The Albert team is currently in the lead with 36 points and margin of 39. Q: What is the current margin of the Mooloolaba team? A: The Mooloolaba team has current margin of 1. Q: What is the overall summary of the document? A: The document appears to be standings table for the Ladies Division Three of bowling or lawn bowls competition, showing the current standings of various teams, including their win-loss records, points, and margins."
        },
        {
            "title": "Pipeline",
            "content": "Figure K19 Visual Question Answering pairs and the corresponding pipeline. The OCR text is extracted from the image, and passed to the LLM (i.e., Llama 3.3 70B) to generate QA pairs. 38 Image: Link SB 1011 10 Fraction of objects 0.2 0.1 0.0 0.01 0.1 1.0 10.0 erg s"
        },
        {
            "title": "OCR",
            "content": "Detailed Caption The image depicts histogram of the distribution of objects, with the x-axis labeled \"LSB[1046ergs1]\" and the y-axis labeled \"Fraction of objects.\" The x-axis ranges from 0.01 to 10.0, while the y-axis ranges from 0.0 to 0.2. The histogram is divided into bins of varying widths, with the first bin spanning from 0.01 to 0.1, the second bin spanning from 0.1 to 1.0, and so on. Each bin contains bar representing the fraction of objects within that range. The bars are shaded in gray, with some bins having multiple bars. key feature of the histogram is the presence of peak in the middle bin, which corresponds to an LSB value of around 1.0. This suggests that the majority of objects have an LSB value close to this value. The histogram also shows tail extending towards higher LSB values, indicating that some objects have significantly higher LSB values than the majority. Overall, the histogram provides visual representation of the distribution of LSB values among the objects being studied. It allows for easy identification of patterns and trends in the data, such as the peak in the middle bin and the tail towards higher LSB values. Multi-Choice Question Answer (MCQA) What is the approximate LSB value at which the majority of objects have their peak? Options: (A) 0.1 (B) 1.0 (C) 5.0 (D) 10.0 Answer: (B) 1.0."
        },
        {
            "title": "Pipeline",
            "content": "Figure K20 Detailed Captions and Multi-Choice Question Answers (MCQAs) and the corresponding pipeline. The OCR text is extracted from the image, and the caption is generated by the captioner (i.e., Llama 3V 90B), which are all passed to the LLM (i.e., Llama 3.3 70B) to generate MCQAs. 39 Video: Link Start Time: 107s End Time: 119s Title: Lions VS Colts Highlights 2017 Preseason Game Description: Comment suggestions for future videos and Enjoy!"
        },
        {
            "title": "Frame Caption",
            "content": "Frame 435: The image shows man with dreadlocks standing in front of crowd of people in stadium. He is wearing white t-shirt and is surrounded by group of people standing on the ground. On the left side of the image, there is table fan, bottles, and other objects placed on table. In the background, there are people sitting on chairs, stairs, railings, boards with text, lights, and the sky. The text on the boards reads \"Indianapolis Colts vs San Francisco 49ers Frame 585: The image shows football game being played on TV screen, with group of people on the ground and few people standing in the background. At the bottom of the image, there is text and numbers indicating that the game is between the Indianapolis Colts and the Detroit Lions. Frame 765: The image shows group of people playing game of football on green field, with white lines marking the boundaries of the field. At the bottom of the image, there is text and numbers indicating that the game is between the Indianapolis Colts and the Detroit Lions. Frame 945: The image shows football game being played on TV screen, with people wearing helmets and playing on the ground. At the bottom of the image, there is text and numbers indicating that the game is between the Detroit Lions and the Indianapolis Colts."
        },
        {
            "title": "Video Caption",
            "content": "Football players wearing helmets, Detroit Lions vs Indianapolis Colts, player running with ball, falls down, touchdown scored."
        },
        {
            "title": "Detailed Caption",
            "content": "A football player is running with the ball and then falls down, the game is between the Detroit Lions and the Indianapolis Colts, with players wearing blue and white uniforms and helmets, and people sitting in the stadium, watermark on the image shows the teams playing, one player is laying on the ground next to another player wearing white and blue jersey and white helmet, horseshoe-shaped object is on the grass, text on the image indicates touchdown, players are wearing helmets and playing on the ground, with watermarks in the bottom left corner of the images, the game involves the Detroit Lions and the Indianapolis Colts, and also shows the Colts playing against the New England Patriots and the Lions playing against the Dallas Cowboys. Multi-Choice Question Answer (MCQA) What is the outcome of the football players action? Options: (A) the player runs out of bounds (B) the player scores touchdown (C) the player drops the ball (D) the player falls down (E) the player gets injured Answer: (B) the player scores touchdown."
        },
        {
            "title": "Pipeline",
            "content": "Figure K21 Detailed Captions and Multi-Choice Question Answers (MCQAs) and the corresponding pipeline. An LLM (i.e., Llama 3.3 70B) with video caption, frame captions at 1 fps, and additional metadata (e.g., video title etc.), generates detailed caption and multiple-choices question answers pair."
        },
        {
            "title": "References",
            "content": "[1] PySceneDetect: Video Cut Detection and Analysis Tool, https://github.com/breakthrough/pyscenedetect. [2] Diagram image to text dataset, 2023. https://huggingface.co/datasets/Kamizuru00/diagram_image_to_text. [3] Manoj Acharya, Kushal Kafle, and Christopher Kanan. Tallyqa: Answering complex counting questions. In AAAI, 2019. [4] Triantafyllos Afouras, Effrosyni Mavroudi, Tushar Nagarajan, Huiyu Wang, and Lorenzo Torresani. HT-step: Aligning instructional articles with how-to videos. In Thirty-seventh Conference on Neural Information Processing Systems Datasets and Benchmarks Track, 2023. https://openreview.net/forum?id=vv3cocNsEK. [5] Harsh Agrawal, Karan Desai, Yufei Wang, Xinlei Chen, Rishabh Jain, Mark Johnson, Dhruv Batra, Devi Parikh, Stefan Lee, and Peter Anderson. Nocaps: Novel object captioning at scale. In Proceedings of the IEEE/CVF international conference on computer vision, pages 89488957, 2019. [6] Jean-Baptiste Alayrac, Jeff Donahue, Pauline Luc, Antoine Miech, Iain Barr, Yana Hasson, Karel Lenc, Arthur Mensch, Katie Millican, Malcolm Reynolds, Roman Ring, Eliza Rutherford, Serkan Cabi, Tengda Han, Zhitao Gong, Sina Samangooei, Marianne Monteiro, Jacob Menick, Sebastian Borgeaud, Andy Brock, Aida Nematzadeh, Sahand Sharifzadeh, Mikolaj Binkowski, Ricardo Barreira, Oriol Vinyals, Andrew Zisserman, and Karen Simonyan. Flamingo: visual language model for few-shot learning. arXiv preprint arXiv:2204.14198, 2022. [7] Aida Amini, Saadia Gabriel, Peter Lin, Rik Koncel-Kedziorski, Yejin Choi, and Hannaneh Hajishirzi. Mathqa: Towards interpretable math word problem solving with operation-based formalisms. arXiv preprint arXiv:1905.13319, 2019. [8] Lisa Anne Hendricks, Oliver Wang, Eli Shechtman, Josef Sivic, Trevor Darrell, and Bryan Russell. Localizing moments in video with natural language. In Proceedings of the IEEE international conference on computer vision, pages 58035812, 2017. [9] Anthropic. The claude 3 model family: Opus, sonnet, haiku. 2024. [10] Haoping Bai, Shancong Mou, Tatiana Likhomanenko, Ramazan Gokberk Cinbis, Oncel Tuzel, Ping Huang, Jiulong Shan, Jianjun Shi, and Meng Cao. Vision datasets: benchmark for vision-based industrial inspection. arXiv preprint arXiv:2306.07890, 2023. [11] Shuai Bai, Keqin Chen, Xuejing Liu, Jialin Wang, Wenbin Ge, Sibo Song, Kai Dang, Peng Wang, Shijie Wang, Jun Tang, et al. Qwen2. 5-vl technical report. arXiv preprint arXiv:2502.13923, 2025. [12] Jonas Belouadi, Anne Lauscher, and Steffen Eger. Automatikz: Text-guided synthesis of scientific vector graphics with tikz, 2024. [13] Jeffrey Bigham, Chandrika Jayant, Hanjie Ji, Greg Little, Andrew Miller, Robert Miller, Robin Miller, Aubrey Tatarowicz, Brandyn White, Samual White, et al. Vizwiz: nearly real-time answers to visual questions. In Proceedings of the 23nd annual ACM symposium on User interface software and technology, pages 333342, 2010. [14] Ali Furkan Biten, Rubèn Tito, Andrés Mafla, Lluis Gomez, Marçal Rusiñol, C.V. Jawahar, Ernest Valveny, and Dimosthenis Karatzas. Scene text visual question answering. In 2019 IEEE/CVF International Conference on Computer Vision (ICCV), pages 42904300, 2019. doi: 10.1109/ICCV.2019.00439. [15] Rumeysa Bodur, Erhan Gundogdu, Binod Bhattarai, Tae-Kyun Kim, Michael Donoser, and Loris Bazzani. iedit: Localised text-guided image editing with weak supervision. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 74267435, 2024. [16] Daniel Bolya, Po-Yao Huang, Peize Sun, Jang Hyun Cho, Andrea Madotto, Chen Wei, Tengyu Ma, Jiale Zhi, Jathushan Rajasegaran, Hanoona Rasheed, Junke Wang, Marco Monteiro, Hu Xu, Shiyu Dong, Nikhila Ravi, Daniel Li, Piotr Dollár, and Christoph Feichtenhofer. Perception encoder: The best visual embeddings are not at the output of the network. arXiv, 2025. [17] Sergi Caelles, Jordi Pont-Tuset, Federico Perazzi, Alberto Montes, Kevis-Kokitsi Maninis, and Luc Van Gool. The 2019 davis challenge on vos: Unsupervised multi-object segmentation. arXiv:1905.00737, 2019. [18] Mu Cai, Reuben Tan, Jianrui Zhang, Bocheng Zou, Kai Zhang, Feng Yao, Fangrui Zhu, Jing Gu, Yiwu Zhong, Yuzhang Shang, et al. Temporalbench: Benchmarking fine-grained temporal understanding for multimodal video models. arXiv preprint arXiv:2410.10818, 2024. 41 [19] Brandon Castellano. PySceneDetect. https://github.com/Breakthrough/PySceneDetect. [20] Wenhao Chai, Enxin Song, Yilun Du, Chenlin Meng, Vashisht Madhavan, Omer Bar-Tal, Jeng-Neng Hwang, Saining Xie, and Christopher Manning. Auroracap: Efficient, performant video detailed captioning and new benchmark. arXiv preprint arXiv:2410.03051, 2024. [21] Shuaichen Chang, David Palzer, Jialin Li, Eric Fosler-Lussier, and Ningchuan Xiao. Mapqa: dataset for question answering on choropleth maps, 2022. [22] Yingshan Chang, Mridu Narang, Hisami Suzuki, Guihong Cao, Jianfeng Gao, and Yonatan Bisk. Webqa: Multihop and multimodal qa. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 1649516504, 2022. [23] David Chen and William Dolan. Collecting highly parallel data for paraphrase evaluation. In Proceedings of the 49th annual meeting of the association for computational linguistics: human language technologies, pages 190200, 2011. [24] Guo Chen, Yicheng Liu, Yifei Huang, Yuping He, Baoqi Pei, Jilan Xu, Yali Wang, Tong Lu, and Limin Wang. Cg-bench: Clue-grounded question answering benchmark for long video understanding. arXiv preprint arXiv:2412.12075, 2024. [25] Jierun Chen, Fangyun Wei, Jinjing Zhao, Sizhe Song, Bohuai Wu, Zhuoxuan Peng, S-H Gary Chan, and Hongyang Zhang. Revisiting referring expression comprehension evaluation in the era of large multimodal models. arXiv preprint arXiv:2406.16866, 2024. [26] Lin Chen, Jinsong Li, Xiaoyi Dong, Pan Zhang, Conghui He, Jiaqi Wang, Feng Zhao, and Dahua Lin. Sharegpt4v: Improving large multi-modal models with better captions. In European Conference on Computer Vision, pages 370387. Springer, 2024. [27] Lin Chen, Jinsong Li, Xiaoyi Dong, Pan Zhang, Yuhang Zang, Zehui Chen, Haodong Duan, Jiaqi Wang, Yu Qiao, Dahua Lin, et al. Are we on the right way for evaluating large vision-language models? arXiv preprint arXiv:2403.20330, 2024. [28] Lin Chen, Xilin Wei, Jinsong Li, Xiaoyi Dong, Pan Zhang, Yuhang Zang, Zehui Chen, Haodong Duan, Bin Lin, Zhenyu Tang, Li Yuan, Yu Qiao, Dahua Lin, Feng Zhao, and Jiaqi Wang. Sharegpt4video: Improving video understanding and generation with better captions. arXiv preprint arXiv:2406.04325, 2024. [29] Zhe Chen, Weiyun Wang, Yue Cao, Yangzhou Liu, Zhangwei Gao, Erfei Cui, Jinguo Zhu, Shenglong Ye, Hao Tian, Zhaoyang Liu, et al. Expanding performance boundaries of open-source multimodal models with model, data, and test-time scaling. arXiv preprint arXiv:2412.05271, 2024. [30] Zhe Chen, Jiannan Wu, Wenhai Wang, Weijie Su, Guo Chen, Sen Xing, Muyan Zhong, Qinglong Zhang, Xizhou Zhu, Lewei Lu, et al. Internvl: Scaling up vision foundation models and aligning for generic visual-linguistic tasks. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 2418524198, 2024. [31] Zhiyu Chen, Wenhu Chen, Charese Smiley, Sameena Shah, Iana Borova, Dylan Langdon, Reema Moussa, Matt Beane, Ting-Hao Huang, Bryan Routledge, and William Yang Wang. Finqa: dataset of numerical reasoning over financial data. In Marie-Francine Moens, Xuanjing Huang, Lucia Specia, and Scott Wen-tau Yih, editors, Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, pages 36973711, Online and Punta Cana, Dominican Republic, November 2021. Association for Computational Linguistics. doi: 10.18653/v1/2021.emnlp-main.300. https://aclanthology.org/2021.emnlp-main.300. [32] Zhoujun Cheng, Haoyu Dong, Zhiruo Wang, Ran Jia, Jiaqi Guo, Yan Gao, Shi Han, Jian-Guang Lou, and Dongmei Zhang. Hitab: hierarchical table dataset for question answering and natural language generation. In Smaranda Muresan, Preslav Nakov, and Aline Villavicencio, editors, Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 10941110, Dublin, Ireland, May 2022. Association for Computational Linguistics. doi: 10.18653/v1/2022.acl-long.78. https://aclanthology.org/ 2022.acl-long.78. [33] Jang Hyun Cho, Boris Ivanovic, Yulong Cao, Edward Schmerling, Yue Wang, Xinshuo Weng, Boyi Li, Yurong You, Philipp Kraehenbuehl, Yan Wang, and Marco Pavone. Language-image models with 3d understanding. In The Thirteenth International Conference on Learning Representations, 2025. https://openreview.net/forum? id=yaQbTAD2JJ. 42 [34] Rohan Choudhury, Guanglei Zhu, Sihan Liu, Koichiro Niinuma, Kris Kitani, and László Jeni. Dont look twice: Faster video transformers with run-length tokenization. arXiv preprint arXiv:2411.05222, 2024. [35] J. S. Chung and A. Zisserman. Out of time: automated lip sync in the wild. In Workshop on Multi-view Lip-reading, ACCV, 2016. [36] Karl Cobbe, Vineet Kosaraju, Mohammad Bavarian, Mark Chen, Heewoo Jun, Lukasz Kaiser, Matthias Plappert, Jerry Tworek, Jacob Hilton, Reiichiro Nakano, Christopher Hesse, and John Schulman. Training verifiers to solve math word problems. arXiv preprint arXiv:2110.14168, 2021. [37] Mike Conover, Matt Hayes, Ankit Mathur, Jianwei Xie, Jun Wan, Sam Shah, Ali Ghodsi, Patrick Wendell, Matei Zaharia, and Reynold Xin. Free dolly: Introducing the worlds first truly open instruction-tuned llm, 2023. https://www.databricks.com/blog/2023/04/12/dolly-first-open-commercially-viable-instruction-tuned-llm. [38] Daniel Cores, Michael Dorkenwald, Manuel Mucientes, Cees GM Snoek, and Yuki Asano. Tvbench: Redesigning video-language evaluation. arXiv preprint arXiv:2410.07752, 2024. [39] Erfei Cui, Yinan He, Zheng Ma, Zhe Chen, Hao Tian, Weiyun Wang, Kunchang Li, Yi Wang, Wenhai Wang, Xizhou Zhu, Lewei Lu, Tong Lu, Yali Wang, Limin Wang, Yu Qiao, and Jifeng Dai. Sharegpt-4o: Comprehensive multimodal annotations with gpt-4o, 2024. https://sharegpt4o.github.io/. [40] Tri Dao. Flashattention-2: Faster attention with better parallelism and work partitioning. arXiv preprint arXiv:2307.08691, 2023. [41] Matt Deitke, Christopher Clark, Sangho Lee, Rohun Tripathi, Yue Yang, Jae Sung Park, Mohammadreza Salehi, Niklas Muennighoff, Kyle Lo, Luca Soldaini, Jiasen Lu, Taira Anderson, Erin Bransom, Kiana Ehsani, Huong Ngo, YenSung Chen, Ajay Patel, Mark Yatskar, Chris Callison-Burch, Andrew Head, Rose Hendrix, Favyen Bastani, Eli VanderBilt, Nathan Lambert, Yvonne Chou, Arnavi Chheda, Jenna Sparks, Sam Skjonsberg, Michael Schmitz, Aaron Sarnat, Byron Bischoff, Pete Walsh, Chris Newell, Piper Wolters, Tanmay Gupta, Kuo-Hao Zeng, Jon Borchardt, Dirk Groeneveld, Jen Dumas, Crystal Nam, Sophie Lebrecht, Caitlin Wittlif, Carissa Schoenick, Oscar Michel, Ranjay Krishna, Luca Weihs, Noah A. Smith, Hannaneh Hajishirzi, Ross Girshick, Ali Farhadi, and Aniruddha Kembhavi. Molmo and pixmo: Open weights and open data for state-of-the-art multimodal models. arXiv preprint arXiv:2409.17146, 2024. [42] Henghui Ding, Chang Liu, Shuting He, Xudong Jiang, and Chen Change Loy. Mevis: large-scale benchmark for video segmentation with motion expressions. In Proceedings of the IEEE/CVF international conference on computer vision, pages 26942703, 2023. [43] Zi-Yi Dou, Xitong Yang, Tushar Nagarajan, Huiyu Wang, Jing Huang, Nanyun Peng, Kris Kitani, and Fu-Jen Chu. Unlocking exocentric video-language data for egocentric video representation learning. ArXiv, abs/2408.03567, 2024. https://api.semanticscholar.org/CorpusID:271745052. [44] Zexing Du, Xue Wang, Guoqing Zhou, and Qing Wang. Fast and unsupervised action boundary detection for action segmentation. In 2022 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pages 33133322, 2022. doi: 10.1109/CVPR52688.2022.00332. [45] Haodong Duan, Junming Yang, Yuxuan Qiao, Xinyu Fang, Lin Chen, Yuan Liu, Xiaoyi Dong, Yuhang Zang, Pan Zhang, Jiaqi Wang, et al. Vlmevalkit: An open-source toolkit for evaluating large multi-modality models. In Proceedings of the 32nd ACM International Conference on Multimedia, pages 1119811201, 2024. [46] Abhimanyu Dubey, Abhinav Jauhri, Abhinav Pandey, Abhishek Kadian, Ahmad Al-Dahle, Aiesha Letman, Akhil Mathur, Alan Schelten, Amy Yang, Angela Fan, et al. The llama 3 herd of models. arXiv preprint arXiv:2407.21783, 2024. [47] Ujjal Kr Dutta, Mehrtash Harandi, and Chellu Chandra Sekhar. Unsupervised deep metric learning via orthogonality based probabilistic loss. IEEE Transactions on Artificial Intelligence, 1(1):7484, 2020. [48] Bernard Ghanem Fabian Caba Heilbron, Victor Escorcia and Juan Carlos Niebles. Activitynet: large-scale video benchmark for human activity understanding. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 961970, 2015. [49] Xinyu Fang, Kangrui Mao, Haodong Duan, Xiangyu Zhao, Yining Li, Dahua Lin, and Kai Chen. Mmbench-video: long-form multi-shot benchmark for holistic video understanding. arXiv preprint arXiv:2406.14515, 2024. [50] Maxwell Forbes, Christine Kaeser-Chen, Piyush Sharma, and Serge Belongie. Neural naturalist: Generating fine-grained image comparisons. arXiv preprint arXiv:1909.04101, 2019. 43 [51] Chaoyou Fu, Peixian Chen, Yunhang Shen, Yulei Qin, Mengdan Zhang, Xu Lin, Jinrui Yang, Xiawu Zheng, Ke Li, Xing Sun, et al. Mme: comprehensive evaluation benchmark for multimodal large language models. arXiv preprint arXiv:2306.13394, 2023. [52] Chaoyou Fu, Yuhan Dai, Yongdong Luo, Lei Li, Shuhuai Ren, Renrui Zhang, Zihan Wang, Chenyu Zhou, Yunhang Shen, Mengdan Zhang, et al. Video-mme: The first-ever comprehensive evaluation benchmark of multi-modal llms in video analysis. arXiv preprint arXiv:2405.21075, 2024. [53] Xingyu Fu, Yushi Hu, Bangzheng Li, Yu Feng, Haoyu Wang, Xudong Lin, Dan Roth, Noah Smith, Wei-Chiu Ma, and Ranjay Krishna. Blink: Multimodal large language models can see but not perceive. In European Conference on Computer Vision, pages 148166, 2025. [54] Soichiro Fujita, Tsutomu Hirao, Hidetaka Kamigaito, Manabu Okumura, and Masaaki Nagata. Soda: Story In Computer VisionECCV 2020: 16th European oriented dense video captioning evaluation framework. Conference, Glasgow, UK, August 2328, 2020, Proceedings, Part VI 16, pages 517531. Springer, 2020. [55] Jiyang Gao, Chen Sun, Zhenheng Yang, and Ram Nevatia. Tall: Temporal activity localization via language query. In Proceedings of the IEEE international conference on computer vision, pages 52675275, 2017. [56] Gemini Team Google. Gemini: family of highly capable multimodal models. arXiv preprint arXiv:2312.11805, 2023. [57] Gemini Team Google. Gemini 1.5: Unlocking multimodal understanding across millions of tokens of context. arXiv preprint arXiv:2403.05530, 2024. [58] Raghav Goyal, Samira Ebrahimi Kahou, Vincent Michalski, Joanna Materzynska, Susanne Westphal, Heuna Kim, Valentin Haenel, Ingo Fruend, Peter Yianilos, Moritz Mueller-Freitag, et al. The\" something something\" video database for learning and evaluating visual common sense. In Proceedings of the IEEE international conference on computer vision, pages 58425850, 2017. [59] Yash Goyal, Tejas Khot, Douglas Summers-Stay, Dhruv Batra, and Devi Parikh. Making the in vqa matter: Elevating the role of image understanding in visual question answering, 2017. [60] Kristen Grauman, Andrew Westbury, Eugene Byrne, Zachary Chavis, Antonino Furnari, Rohit Girdhar, Jackson Hamburger, Hao Jiang, Miao Liu, Xingyu Liu, Miguel Martin, Tushar Nagarajan, Ilija Radosavovic, Santhosh Kumar Ramakrishnan, Fiona Ryan, Jayant Sharma, Michael Wray, Mengmeng Xu, Eric Zhongcong Xu, Chen Zhao, Siddhant Bansal, Dhruv Batra, Vincent Cartillier, Sean Crane, Tien Do, Morrie Doulaty, Akshay Erapalli, Christoph Feichtenhofer, Adriano Fragomeni, Qichen Fu, Abrham Gebreselasie, Cristina Gonzalez, James Hillis, Xuhua Huang, Yifei Huang, Wenqi Jia, Weslie Khoo, Jachym Kolar, Satwik Kottur, Anurag Kumar, Federico Landini, Chao Li, Yanghao Li, Zhenqiang Li, Karttikeya Mangalam, Raghava Modhugu, Jonathan Munro, Tullie Murrell, Takumi Nishiyasu, Will Price, Paola Ruiz Puentes, Merey Ramazanova, Leda Sari, Kiran Somasundaram, Audrey Southerland, Yusuke Sugano, Ruijie Tao, Minh Vo, Yuchen Wang, Xindi Wu, Takuma Yagi, Ziwei Zhao, Yunyi Zhu, Pablo Arbelaez, David Crandall, Dima Damen, Giovanni Maria Farinella, Christian Fuegen, Bernard Ghanem, Vamsi Krishna Ithapu, C. V. Jawahar, Hanbyul Joo, Kris Kitani, Haizhou Li, Richard Newcombe, Aude Oliva, Hyun Soo Park, James M. Rehg, Yoichi Sato, Jianbo Shi, Mike Zheng Shou, Antonio Torralba, Lorenzo Torresani, Mingfei Yan, and Jitendra Malik. Ego4d: Around the world in 3,000 hours of egocentric video. In IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), 2022. [61] Kristen Grauman, Andrew Westbury, Lorenzo Torresani, Kris Kitani, Jitendra Malik, Triantafyllos Afouras, Kumar Ashutosh, Vijay Baiyya, Siddhant Bansal, Bikram Boote, Eugene Byrne, Zachary Chavis, Joya Chen, Feng Cheng, Fu-Jen Chu, Sean Crane, Avijit Dasgupta, Jing Dong, María Escobar, Cristhian Forigua, Abrham Kahsay Gebreselasie, Sanjay Haresh, Jing Huang, Md Mohaiminul Islam, Suyog Dutt Jain, Rawal Khirodkar, Devansh Kukreja, Kevin Liang, Jia-Wei Liu, Sagnik Majumder, Yongsen Mao, Miguel Martin, Effrosyni Mavroudi, Tushar Nagarajan, Francesco Ragusa, Santhosh K. Ramakrishnan, Luigi Seminara, Arjun Somayazulu, Yale Song, Shan Su, Zihui Xue, Edward Zhang, Jinxu Zhang, Angela Castillo, Changan Chen, Xinzhu Fu, Ryosuke Furuta, Cristina Gonzalez, Prince Gupta, Jiabo Hu, Yifei Huang, Yiming Huang, Weslie Khoo, Anush Kumar, Robert Kuo, Sach Lakhavani, Miao Liu, Mingjing Luo, Zhengyi Luo, Brighid Meredith, Austin Miller, Oluwatumininu Oguntola, Xiaqing Pan, Penny Peng, Shraman Pramanick, Merey Ramazanova, Fiona Ryan, Wei Shan, Kiran Somasundaram, Chenan Song, Audrey Southerland, Masatoshi Tateno, Huiyu Wang, Yuchen Wang, Takuma Yagi, Mingfei Yan, Xitong Yang, Zecheng Yu, Shengxin Cindy Zha, Chen Zhao, Ziwei Zhao, Zhifan Zhu, Jeff Zhuo, Pablo Arbeláez, Gedas Bertasius, David J. Crandall, Dima Damen, Jakob Julian Engel, Giovanni Maria Farinella, Antonino Furnari, Bernard Ghanem, Judy Hoffman, C. V. Jawahar, Richard A. Newcombe, Hyun Soo Park, James M. Rehg, Yoichi Sato, Manolis Savva, Jianbo Shi, Mike Zheng Shou, and Michael Wray. Ego-exo4d: 44 Understanding skilled human activity from firstand third-person perspectives. IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pages 1938319400, 2023. [62] Tianrui Guan, Fuxiao Liu, Xiyang Wu, Ruiqi Xian, Zongxia Li, Xiaoyu Liu, Xijun Wang, Lichang Chen, Furong Huang, Yaser Yacoob, et al. Hallusionbench: an advanced diagnostic suite for entangled language hallucination and visual illusion in large vision-language models. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 1437514385, 2024. [63] Tanmay Gupta, Dustin Schwenk, Ali Farhadi, Derek Hoiem, and Aniruddha Kembhavi. Imagine this! scripts to compositions to videos. In Proceedings of the European conference on computer vision (ECCV), pages 598613, 2018. [64] Wenyi Hong, Yean Cheng, Zhuoyi Yang, Weihan Wang, Lefan Wang, Xiaotao Gu, Shiyu Huang, Yuxiao Dong, and Jie Tang. Motionbench: Benchmarking and improving fine-grained video motion understanding for vision language models. arXiv preprint arXiv:2501.02955, 2025. [65] Drew Hudson and Christopher Manning. Gqa: new dataset for real-world visual reasoning and compositional question answering. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 67006709, 2019. [66] Phillip Isola, Joseph Lim, and Edward Adelson. Discovering states and transformations in image collections. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 13831391, 2015. [67] Andrew Jaegle, Sebastian Borgeaud, Jean-Baptiste Alayrac, Carl Doersch, Catalin Ionescu, David Ding, Skanda Koppula, Daniel Zoran, Andrew Brock, Evan Shelhamer, et al. Perceiver io: general architecture for structured inputs & outputs. ICLR, 2022. [68] Yunseok Jang, Yale Song, Youngjae Yu, Youngjin Kim, and Gunhee Kim. Tgif-qa: Toward spatio-temporal reasoning in visual question answering. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 27582766, 2017. [69] Harsh Jhamtani et al. Learning to describe differences between pairs of similar images. In Ellen Riloff, David Chiang, Julia Hockenmaier, and Junichi Tsujii, editors, Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, pages 40244034, Brussels, Belgium, October-November 2018. Association for Computational Linguistics. doi: 10.18653/v1/D18-1436. https://aclanthology.org/D18-1436. [70] Dongfu Jiang, Xuan He, Huaye Zeng, Cong Wei, Max Ku, Qian Liu, and Wenhu Chen. Mantis: Interleaved multi-image instruction tuning. arXiv preprint arXiv:2405.01483, 2024. [71] Justin Johnson, Bharath Hariharan, Laurens van der Maaten, Li Fei-Fei, C. Lawrence Zitnick, and Ross Girshick. Clevr: diagnostic dataset for compositional language and elementary visual reasoning, 2016. [72] Kushal Kafle, Scott Cohen, Brian Price, and Christopher Kanan. Dvqa: Understanding data visualizations via question answering. In CVPR, 2018. [73] Samira Ebrahimi Kahou, Vincent Michalski, Adam Atkinson, Akos Kadar, Adam Trischler, and Yoshua Bengio. Figureqa: An annotated figure dataset for visual reasoning, 2018. [74] Hyolim Kang, Jinwoo Kim, Taehyun Kim, and Seon Joo Kim. Uboco: Unsupervised boundary contrastive learning for generic event boundary detection. 2022 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pages 2004120050, 2021. https://api.semanticscholar.org/CorpusID:244714876. [75] Will Kay, Joao Carreira, Karen Simonyan, Brian Zhang, Chloe Hillier, Sudheendra Vijayanarasimhan, Fabio Viola, Tim Green, Trevor Back, Paul Natsev, et al. The kinetics human action video dataset. arXiv preprint arXiv:1705.06950, 2017. [76] Mehran Kazemi, Hamidreza Alvari, Ankit Anand, Jialin Wu, Xi Chen, and Radu Soricut. Geomverse: systematic evaluation of large models for geometric reasoning, 2023. [77] Sahar Kazemzadeh, Vicente Ordonez, Mark Matten, and Tamara Berg. Referitgame: Referring to objects in photographs of natural scenes. In Proceedings of the 2014 conference on empirical methods in natural language processing (EMNLP), pages 787798, 2014. [78] Aniruddha Kembhavi, Mike Salvato, Eric Kolve, Minjoon Seo, Hannaneh Hajishirzi, and Ali Farhadi. diagram is worth dozen images, 2016. [79] Aniruddha Kembhavi, Minjoon Seo, Dustin Schwenk, Jonghyun Choi, Ali Farhadi, and Hannaneh Hajishirzi. Are you smarter than sixth grader? textbook question answering for multimodal machine comprehension. 45 In 2017 IEEE Conference on Computer Vision and Pattern Recognition (CVPR), pages 53765384, 2017. doi: 10.1109/CVPR.2017.571. [80] Douwe Kiela, Hamed Firooz, Aravind Mohan, Vedanuj Goswami, Amanpreet Singh, Pratik Ringshia, and Davide Testuggine. The hateful memes challenge: Detecting hate speech in multimodal memes. In H. Larochelle, M. Ranzato, R. Hadsell, M.F. Balcan, and H. Lin, editors, Advances in Neural Information Processing Systems, volume 33, pages 26112624. Curran Associates, Inc., 2020. https://proceedings.neurips.cc/paper_files/paper/ 2020/file/1b84c4cee2b8b3d823b30e2d604b1878-Paper.pdf. [81] Alexander Kirillov, Eric Mintun, Nikhila Ravi, Hanzi Mao, Chloe Rolland, Laura Gustafson, Tete Xiao, Spencer In Proceedings of the IEEE/CVF Whitehead, Alexander Berg, Wan-Yen Lo, et al. Segment anything. international conference on computer vision, pages 40154026, 2023. [82] Ranjay Krishna, Kenji Hata, Frederic Ren, Li Fei-Fei, and Juan Carlos Niebles. Dense-captioning events in videos. In Proceedings of the IEEE international conference on computer vision, pages 706715, 2017. [83] Ranjay Krishna, Yuke Zhu, Oliver Groth, Justin Johnson, Kenji Hata, Joshua Kravitz, Stephanie Chen, Yannis Kalantidis, Li-Jia Li, David Shamma, et al. Visual genome: Connecting language and vision using crowdsourced dense image annotations. International journal of computer vision, 123:3273, 2017. [84] Benno Krojer, Vaibhav Adlakha, Vibhav Vineet, Yash Goyal, Edoardo Ponti, and Siva Reddy. Image retrieval from contextual descriptions. arXiv preprint arXiv:2203.15867, 2022. [85] Alina Kuznetsova, Hassan Rom, Neil Alldrin, Jasper Uijlings, Ivan Krasin, Jordi Pont-Tuset, Shahab Kamali, Stefan Popov, Matteo Malloci, Alexander Kolesnikov, Tom Duerig, and Vittorio Ferrari. The open images dataset v4: Unified image classification, object detection, and visual relationship detection at scale. IJCV, 2020. [86] Jason Lau, Soumya Gayen, Asma Ben Abacha, and Dina Demner-Fushman. dataset of clinically generated visual questions and answers about radiology images. Scientific Data, 5:180251, 11 2018. doi: 10.1038/sdata.2018.251. [87] Hugo Laurençon, Léo Tronchon, and Victor Sanh. Unlocking the conversion of web screenshots into html code with the websight dataset, 2024. [88] Jie Lei, Licheng Yu, Mohit Bansal, and Tamara Berg. Tvqa: Localized, compositional video question answering. arXiv preprint arXiv:1809.01696, 2018. [89] Bo Li, Yuanhan Zhang, Liangyu Chen, Jinghao Wang, Fanyi Pu, Jingkang Yang, Chunyuan Li, and Ziwei Liu. Mimic-it: Multi-modal in-context instruction tuning, 2023. [90] Bo Li, Yuanhan Zhang, Dong Guo, Renrui Zhang, Feng Li, Hao Zhang, Kaichen Zhang, Yanwei Li, Ziwei Liu, and Chunyuan Li. Llava-onevision: Easy visual task transfer. arXiv preprint arXiv:2408.03326, 2024. [91] Bohao Li, Rui Wang, Guangzhi Wang, Yuying Ge, Yixiao Ge, and Ying Shan. Seed-bench: Benchmarking multimodal llms with generative comprehension. arXiv preprint arXiv:2307.16125, 2023. [92] Junnan Li, Dongxu Li, Silvio Savarese, and Steven Hoi. Blip-2: Bootstrapping language-image pre-training with frozen image encoders and large language models. arXiv preprint arXiv:2301.12597, 2023. [93] KunChang Li, Yinan He, Yi Wang, Yizhuo Li, Wenhai Wang, Ping Luo, Yali Wang, Limin Wang, and Yu Qiao. Videochat: Chat-centric video understanding. arXiv preprint arXiv:2305.06355, 2023. [94] Kunchang Li, Yali Wang, Yinan He, Yizhuo Li, Yi Wang, Yi Liu, Zun Wang, Jilan Xu, Guo Chen, Ping Luo, et al. Mvbench: comprehensive multi-modal video understanding benchmark. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 2219522206, 2024. [95] Lei Li, Yuqi Wang, Runxin Xu, Peiyi Wang, Xiachong Feng, Lingpeng Kong, and Qi Liu. Multimodal arxiv: dataset for improving scientific comprehension of large vision-language models. arXiv preprint arXiv:2403.00231, 2024. [96] Linjie Li, Yen-Chun Chen, Yu Cheng, Zhe Gan, Licheng Yu, and Jingjing Liu. Hero: Hierarchical encoder for video+ language omni-representation pre-training. arXiv preprint arXiv:2005.00200, 2020. [97] Yifan Li, Yifan Du, Kun Zhou, Jinpeng Wang, Wayne Xin Zhao, and Ji-Rong Wen. Evaluating object hallucination in large vision-language models. arXiv preprint arXiv:2305.10355, 2023. [98] Yuhui Li, Fangyun Wei, Chao Zhang, and Hongyang Zhang. Eagle-2: Faster inference of language models with dynamic draft trees, 2024b. URL https://arxiv. org/abs/2406.16858, 2024. 46 [99] Bin Lin, Bin Zhu, Yang Ye, Munan Ning, Peng Jin, and Li Yuan. Video-llava: Learning united visual representation by alignment before projection. arXiv preprint arXiv:2311.10122, 2023. [100] Ji Lin, Hongxu Yin, Wei Ping, Pavlo Molchanov, Mohammad Shoeybi, and Song Han. Vila: On pre-training for visual language models. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 2668926699, 2024. [101] Tsung-Yi Lin, Michael Maire, Serge Belongie, James Hays, Pietro Perona, Deva Ramanan, Piotr Dollár, and Lawrence Zitnick. Microsoft coco: Common objects in context. In Computer VisionECCV 2014: 13th European Conference, Zurich, Switzerland, September 6-12, 2014, Proceedings, Part 13, pages 740755. Springer, 2014. [102] Fangyu Liu, Guy Emerson, and Nigel Collier. Visual spatial reasoning, 2023. [103] Fangyu Liu, Guy Emerson, and Nigel Collier. Visual spatial reasoning. Transactions of the Association for Computational Linguistics, 11:635651, 2023. [104] Haotian Liu, Chunyuan Li, Qingyang Wu, and Yong Jae Lee. Visual instruction tuning. Advances in neural information processing systems, 36:3489234916, 2023. [105] Haotian Liu, Chunyuan Li, Yuheng Li, Bo Li, Yuanhan Zhang, Sheng Shen, and Yong Jae Lee. Llavahttps://llava-vl.github.io/blog/ Improved reasoning, ocr, and world knowledge, January 2024. next: 2024-01-30-llava-next/. [106] Jiajun Liu, Yibing Wang, Hanghang Ma, Xiaoping Wu, Xiaoqi Ma, Xiaoming Wei, Jianbin Jiao, Enhua Wu, and Jie Hu. Kangaroo: powerful video-language model supporting long-context video input. arXiv preprint arXiv:2408.15542, 2024. [107] Shilong Liu, Zhaoyang Zeng, Tianhe Ren, Feng Li, Hao Zhang, Jie Yang, Chunyuan Li, Jianwei Yang, Hang Su, Jun Zhu, et al. Grounding dino: Marrying dino with grounded pre-training for open-set object detection. arXiv preprint arXiv:2303.05499, 2023. [108] Yuan Liu, Haodong Duan, Yuanhan Zhang, Bo Li, Songyang Zhang, Wangbo Zhao, Yike Yuan, Jiaqi Wang, Conghui He, Ziwei Liu, et al. Mmbench: Is your multi-modal model an all-around player? In European conference on computer vision, pages 216233. Springer, 2024. [109] Yuanxin Liu, Shicheng Li, Yi Liu, Yuxiang Wang, Shuhuai Ren, Lei Li, Sishuo Chen, Xu Sun, and Lu Hou. Tempcompass: Do video llms really understand videos? arXiv preprint arXiv:2403.00476, 2024. [110] Yuliang Liu, Zhang Li, Mingxin Huang, Biao Yang, Wenwen Yu, Chunyuan Li, Xu-Cheng Yin, Cheng-Lin Liu, Lianwen Jin, and Xiang Bai. Ocrbench: on the hidden mystery of ocr in large multimodal models. Science China Information Sciences, 67(12):220102, 2024. [111] Ilya Loshchilov and Frank Hutter. Decoupled weight decay regularization. arXiv preprint arXiv:1711.05101, 2017. [112] Pan Lu, Ran Gong, Shibiao Jiang, Liang Qiu, Siyuan Huang, Xiaodan Liang, and Song-Chun Zhu. Inter-gps: Interpretable geometry problem solving with formal language and symbolic reasoning. In The Joint Conference of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (ACL-IJCNLP 2021), 2021. [113] Pan Lu, Liang Qiu, Jiaqi Chen, Tony Xia, Yizhou Zhao, Wei Zhang, Zhou Yu, Xiaodan Liang, and Song-Chun Zhu. Iconqa: new benchmark for abstract diagram understanding and visual language reasoning. In The 35th Conference on Neural Information Processing Systems (NeurIPS) Track on Datasets and Benchmarks, 2021. [114] Pan Lu, Swaroop Mishra, Tanglin Xia, Liang Qiu, Kai-Wei Chang, Song-Chun Zhu, Oyvind Tafjord, Peter Clark, and Ashwin Kalyan. Learn to explain: Multimodal reasoning via thought chains for science question answering. In S. Koyejo, S. Mohamed, A. Agarwal, D. Belgrave, K. Cho, and A. Oh, editors, Advances in Neural Information Processing Systems, volume 35, pages 25072521. Curran Associates, Inc., 2022. https://proceedings.neurips. cc/paper_files/paper/2022/file/11332b6b6cf4485b84afadb1352d3a9a-Paper-Conference.pdf. [115] Pan Lu, Hritik Bansal, Tony Xia, Jiacheng Liu, Chunyuan Li, Hannaneh Hajishirzi, Hao Cheng, Kai-Wei Chang, Michel Galley, and Jianfeng Gao. Mathvista: Evaluating mathematical reasoning of foundation models in visual contexts. arXiv preprint arXiv:2310.02255, 2023. 47 [116] Pan Lu, Liang Qiu, Kai-Wei Chang, Ying Nian Wu, Song-Chun Zhu, Tanmay Rajpurohit, Peter Clark, and Ashwin Kalyan. Dynamic prompt learning via policy gradient for semi-structured mathematical reasoning. In International Conference on Learning Representations (ICLR), 2023. [117] Yujie Lu, Dongfu Jiang, Wenhu Chen, William Yang Wang, Yejin Choi, and Bill Yuchen Lin. Wildvision: Evaluating vision-language models in the wild with human preferences. arXiv preprint arXiv:2406.11069, 2024. [118] Muhammad Maaz, Hanoona Rasheed, Salman Khan, and Fahad Shahbaz Khan. Video-chatgpt: Towards detailed video understanding via large vision and language models. arXiv preprint arXiv:2306.05424, 2023. [119] Muhammad Maaz, Hanoona Rasheed, Salman Khan, and Fahad Khan. Videogpt+: Integrating image and video encoders for enhanced video understanding. arXiv preprint arXiv:2406.09418, 2024. [120] Karttikeya Mangalam, Raiymbek Akshulakov, and Jitendra Malik. Egoschema: diagnostic benchmark for very long-form video language understanding. Advances in Neural Information Processing Systems, 36, 2024. [121] Kenneth Marino, Mohammad Rastegari, Ali Farhadi, and Roozbeh Mottaghi. Ok-vqa: visual question answering benchmark requiring external knowledge. In Conference on Computer Vision and Pattern Recognition (CVPR), 2019. [122] Urs-Viktor Marti and H. Bunke. The iam-database: An english sentence database for offline handwriting recognition. International Journal on Document Analysis and Recognition, 5:3946, 11 2002. doi: 10.1007/ s100320200071. [123] Ahmed Masry, Do Long, Jia Qing Tan, Shafiq Joty, and Enamul Hoque. Chartqa: benchmark for question answering about charts with visual and logical reasoning. In Findings of the Association for Computational Linguistics: ACL 2022, pages 22632279, Dublin, Ireland, May 2022. Association for Computational Linguistics. doi: 10.18653/v1/2022.findings-acl.177. https://aclanthology.org/2022.findings-acl.177. [124] Minesh Mathew, Dimosthenis Karatzas, and C. V. Jawahar. Docvqa: dataset for vqa on document images. In 2021 IEEE Winter Conference on Applications of Computer Vision (WACV), pages 21992208, 2021. doi: 10.1109/WACV48630.2021.00225. [125] Minesh Mathew, Viraj Bagal, Rubèn Tito, Dimosthenis Karatzas, Ernest Valveny, and C. V. Jawahar. Infographicvqa. In 2022 IEEE/CVF Winter Conference on Applications of Computer Vision (WACV), pages 25822591, 2022. doi: 10.1109/WACV51458.2022.00264. [126] Effrosyni Mavroudi, Triantafyllos Afouras, and Lorenzo Torresani. Learning to ground instructional articles in videos through narrations. In Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV), pages 1520115213, October 2023. [127] Nitesh Methani, Pritha Ganguly, Mitesh M. Khapra, and Pratyush Kumar. Plotqa: Reasoning over scientific plots. In The IEEE Winter Conference on Applications of Computer Vision (WACV), March 2020. [128] Antoine Miech, Dimitri Zhukov, Jean-Baptiste Alayrac, Makarand Tapaswi, Ivan Laptev, and Josef Sivic. Howto100m: Learning text-video embedding by watching hundred million narrated video clips. In Proceedings of the IEEE International Conference on Computer Vision (ICCV), 2019. [129] Matthias Minderer, Alexey Gritsenko, and Neil Houlsby. Scaling open-vocabulary object detection. In Proceedings of the 37th International Conference on Neural Information Processing Systems, 2023. [130] Farré Miquel, Marafioti Andres, Tunstall Lewis, von Werra Leandro, Conghui He, Cuenca Pedro, and Wolf Thomas. Finevideo: behind the scenes, 2024. https://huggingface.co/blog/fine-video. [131] Anand Mishra, Shashank Shekhar, Ajeet Kumar Singh, and Anirban Chakraborty. Ocr-vqa: Visual question answering by reading text in images. In 2019 International Conference on Document Analysis and Recognition (ICDAR), pages 947952, 2019. doi: 10.1109/ICDAR.2019.00156. [132] Mathew Monfort, SouYoung Jin, Alexander Liu, David Harwath, Rogerio Feris, James Glass, and Aude Oliva. Spoken moments: Learning joint audio-visual representations from video descriptions. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 1487114881, 2021. [133] Seungwhan Moon, Andrea Madotto, Zhaojiang Lin, Tushar Nagarajan, Matt Smith, Shashank Jain, Chun-Fu Yeh, Prakash Murugesan, Peyman Heidari, Yue Liu, et al. Anymal: An efficient and scalable any-modality augmented language model. In Proceedings of the 2024 Conference on Empirical Methods in Natural Language Processing: Industry Track, pages 13141332, 2024. 48 [134] Thong Thanh Nguyen, Zhiyuan Hu, Xiaobao Wu, Cong-Duy Nguyen, See-Kiong Ng, and Anh Tuan Luu. Encoding and controlling global semantics for long-form video question answering. arXiv preprint arXiv:2405.19723, 2024. [135] Munan Ning, Bin Zhu, Yujia Xie, Bin Lin, Jiaxi Cui, Lu Yuan, Dongdong Chen, and Li Yuan. Video-bench: comprehensive benchmark and toolkit for evaluating video-based large language models. arXiv preprint arXiv:2311.16103, 2023. [136] Jason Obeid and Enamul Hoque. Chart-to-text: Generating natural language descriptions for charts by adapting the transformer model. In Brian Davis, Yvette Graham, John Kelleher, and Yaji Sripada, editors, Proceedings of the 13th International Conference on Natural Language Generation, pages 138147, Dublin, Ireland, December 2020. Association for Computational Linguistics. doi: 10.18653/v1/2020.inlg-1.20. https: //aclanthology.org/2020.inlg-1.20. [137] Yasumasa Onoe, Sunayana Rane, Zachary Berger, Yonatan Bitton, Jaemin Cho, Roopal Garg, Alexander Ku, Zarana Parekh, Jordi Pont-Tuset, Garrett Tanzer, et al. Docci: Descriptions of connected and contrasting images. In European Conference on Computer Vision, pages 291309. Springer, 2024. [138] OpenAI. Gpt-4v(ision) system card, 2023. https://openai.com/research/gpt-4v-system-card. [139] OpenAI. Gpt-4o system card, 2024. https://openai.com/index/hello-gpt-4o/. [140] Montalvo Pablo and Wightman Ross. Industry documents library (idl), 2024. https://huggingface.co/datasets/ pixparse/idl-wds. [141] Montalvo Pablo and Wightman Ross. Pdf association dataset (pdfa), 2024. https://huggingface.co/datasets/ pixparse/pdfa-eng-wds. [142] Dong Huk Park, Trevor Darrell, and Anna Rohrbach. Robust change captioning. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 46244633, 2019. [143] Panupong Pasupat and Percy Liang. Compositional semantic parsing on semi-structured tables. In Chengqing Zong and Michael Strube, editors, Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing (Volume 1: Long Papers), pages 14701480, Beijing, China, July 2015. Association for Computational Linguistics. doi: 10.3115/v1/P15-1142. https://aclanthology.org/P15-1142. [144] Adam Paszke, Sam Gross, Soumith Chintala, Gregory Chanan, Edward Yang, Zachary DeVito, Zeming Lin, Alban Desmaison, Luca Antiga, and Alan Lerer. Automatic differentiation in pytorch, 2017. [145] Viorica Patraucean, Lucas Smaira, Ankush Gupta, Adria Recasens, Larisa Markeeva, Dylan Banarse, Skanda Koppula, Mateusz Malinowski, Yi Yang, Carl Doersch, et al. Perception test: diagnostic benchmark for multimodal video models. Advances in Neural Information Processing Systems, 36, 2024. [146] F. Perazzi, J. Pont-Tuset, B. McWilliams, L. Van Gool, M. Gross, and A. Sorkine-Hornung. benchmark dataset and evaluation methodology for video object segmentation. In Computer Vision and Pattern Recognition, 2016. [147] Bryan Plummer, Liwei Wang, Chris Cervantes, Juan Caicedo, Julia Hockenmaier, and Svetlana Lazebnik. Flickr30k entities: Collecting region-to-phrase correspondences for richer image-to-sentence models. In Proceedings of the IEEE international conference on computer vision, pages 26412649, 2015. [148] Jordi Pont-Tuset, Jasper Uijlings, Soravit Changpinyo, Radu Soricut, and Vittorio Ferrari. Connecting vision and language with localized narratives, 2020. [149] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, Gretchen Krueger, and Ilya Sutskever. Learning transferable visual models from natural language supervision. In International Conference on Machine Learning, 2021. https://api.semanticscholar.org/CorpusID:231591445. [150] Nazneen Rajani, Lewis Tunstall, Edward Beeching, Nathan Lambert, Alexander M. Rush, and Thomas Wolf. No robots. https://huggingface.co/datasets/HuggingFaceH4/no_robots, 2023. [151] Hareesh Ravi, Kushal Kafle, Scott Cohen, Jonathan Brandt, and Mubbasir Kapadia. Aesop: Abstract encoding of stories, objects, and pictures. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 20522063, 2021. 49 [152] Nikhila Ravi, Valentin Gabeur, Yuan-Ting Hu, Ronghang Hu, Chaitanya Ryali, Tengyu Ma, Haitham Khedr, Roman Rädle, Chloe Rolland, Laura Gustafson, et al. Sam 2: Segment anything in images and videos. arXiv preprint arXiv:2408.00714, 2024. [153] Ruchit Rawal, Khalid Saifullah, Miquel Farré, Ronen Basri, David Jacobs, Gowthami Somepalli, and Tom Goldstein. Cinepile: long video question answering dataset and benchmark. arXiv preprint arXiv:2405.08813, 2024. [154] Mengye Ren, Ryan Kiros, and Richard Zemel. Exploring models and data for image question answering. In C. Cortes, N. Lawrence, D. Lee, M. Sugiyama, and R. Garnett, editors, Advances in Neural Information Processing Systems, volume 28. Curran Associates, Inc., 2015. https://proceedings.neurips.cc/paper_files/paper/2015/ file/831c2f88a604a07ca94314b56a4921b8-Paper.pdf. [155] Mohammadreza Salehi, Jae Sung Park, Tanush Yadav, Aditya Kusupati, Ranjay Krishna, Yejin Choi, Hannaneh Hajishirzi, and Ali Farhadi. Actionatlas: videoqa benchmark for domain-specialized action recognition. arXiv preprint arXiv:2410.05774, 2024. [156] Dustin Schwenk, Apoorv Khandelwal, Christopher Clark, Kenneth Marino, and Roozbeh Mottaghi. A-okvqa: benchmark for visual question answering using world knowledge, 2022. [157] Seonguk Seo, Joon-Young Lee, and Bohyung Han. Urvos: Unified referring video object segmentation network with large-scale benchmark. In Computer VisionECCV 2020: 16th European Conference, Glasgow, UK, August 2328, 2020, Proceedings, Part XV 16, pages 208223. Springer, 2020. [158] Dandan Shan, Jiaqi Geng, Michelle Shu, and David Fouhey. Understanding human hands in contact at internet scale. In CVPR, 2020. [159] Ziyao Shangguan, Chuhan Li, Yuxuan Ding, Yanan Zheng, Yilun Zhao, Tesca Fitzgerald, and Arman Cohan. Tomato: Assessing visual temporal reasoning capabilities in multimodal foundation models. arXiv preprint arXiv:2410.23266, 2024. [160] Shuai Shao, Zeming Li, Tianyuan Zhang, Chao Peng, Gang Yu, Xiangyu Zhang, Jing Li, and Jian Sun. Objects365: large-scale, high-quality dataset for object detection. In Proceedings of the IEEE/CVF international conference on computer vision, pages 84308439, 2019. [161] Xiaoqian Shen, Yunyang Xiong, Changsheng Zhao, Lemeng Wu, Jun Chen, Chenchen Zhu, Zechun Liu, Fanyi Xiao, Balakrishnan Varadarajan, Florian Bordes, et al. Longvu: Spatiotemporal adaptive compression for long video-language understanding. arXiv preprint arXiv:2410.17434, 2024. [162] Oleksii Sidorov, Ronghang Hu, Marcus Rohrbach, and Amanpreet Singh. Textcaps: dataset for image captioning with reading comprehension, 2020. [163] Gunnar Sigurdsson, Gül Varol, Xiaolong Wang, Ali Farhadi, Ivan Laptev, and Abhinav Gupta. Hollywood in homes: Crowdsourcing data collection for activity understanding. In Computer VisionECCV 2016: 14th European Conference, Amsterdam, The Netherlands, October 1114, 2016, Proceedings, Part 14, pages 510526. Springer, 2016. [164] Amanpreet Singh, Vivek Natarjan, Meet Shah, Yu Jiang, Xinlei Chen, Devi Parikh, and Marcus Rohrbach. Towards vqa models that can read. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 83178326, 2019. [165] Amanpreet Singh, Vivek Natarjan, Meet Shah, Yu Jiang, Xinlei Chen, Devi Parikh, and Marcus Rohrbach. Towards vqa models that can read. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 83178326, 2019. [166] Enxin Song, Wenhao Chai, Guanhong Wang, Yucheng Zhang, Haoyang Zhou, Feiyang Wu, Haozhe Chi, Xun Guo, Tian Ye, Yanting Zhang, et al. Moviechat: From dense token to sparse memory for long video understanding. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 1822118232, 2024. [167] Yale Song, Eugene Byrne, Tushar Nagarajan, Huiyu Wang, Miguel Martin, and Lorenzo Torresani. Ego4d goal-step: Toward hierarchical understanding of procedural activities. In A. Oh, T. Naumann, A. Globerson, K. Saenko, M. Hardt, and S. Levine, editors, Advances in Neural Information Processing Systems, volume 36, pages 3886338886. Curran Associates, Inc., 2023. https://proceedings.neurips.cc/paper_files/paper/2023/ file/7a65606fa1a6849450550325832036e5-Paper-Datasets_and_Benchmarks.pdf. 50 [168] Alane Suhr, Stephanie Zhou, Ally Zhang, Iris Zhang, Huajun Bai, and Yoav Artzi. corpus for reasoning In Anna Korhonen, David Traum, and Lluís Màrquez, about natural language grounded in photographs. editors, Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages 6418 6428, Florence, Italy, July 2019. Association for Computational Linguistics. doi: 10.18653/v1/P19-1644. https://aclanthology.org/P19-1644. [169] Ryota Tanaka, Kyosuke Nishida, and Sen Yoshida. Visualmrc: Machine reading comprehension on document images. In AAAI, 2021. [170] Benny J. Tang, Angie Boggust, and Arvind Satyanarayan. Vistext: benchmark for semantically rich In The Annual Meeting of the Association for Computational Linguistics (ACL), 2023. chart captioning. http://vis.csail.mit.edu/pubs/vistext. [171] Yansong Tang, Dajun Wang, Zhenyu Xu, Jingjing Liu, Xiaoyong Wang, Xing Gao, Jinhui Tang, and Dong Wu. Coin: large-scale dataset for comprehensive instructional video analysis. In IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2019. [172] Zongheng Tang, Yue Liao, Si Liu, Guanbin Li, Xiaojie Jin, Hongxu Jiang, Qian Yu, and Dong Xu. Human-centric spatio-temporal video grounding with visual transformers. IEEE Transactions on Circuits and Systems for Video Technology, 32(12):82388249, 2021. [173] Zongheng Tang, Yue Liao, Si Liu, Guanbin Li, Xiaojie Jin, Hongxu Jiang, Qian Yu, and Dong Xu. Human-centric spatio-temporal video grounding with visual transformers. IEEE Transactions on Circuits and Systems for Video Technology, 32(12):82388249, 2021. [174] Makarand Tapaswi, Yukun Zhu, Rainer Stiefelhagen, Antonio Torralba, Raquel Urtasun, and Sanja Fidler. Movieqa: Understanding stories in movies through question-answering. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 46314640, 2016. [175] Gemma Team, Aishwarya Kamath, Johan Ferret, Shreya Pathak, Nino Vieillard, Ramona Merhej, Sarah Perrin, Tatiana Matejovicova, Alexandre Ramé, Morgane Rivière, et al. Gemma 3 technical report. arXiv preprint arXiv:2503.19786, 2025. [176] Shengbang Tong, Ellis Brown, Penghao Wu, Sanghyun Woo, Manoj Middepogu, Sai Charitha Akula, Jihan Yang, Shusheng Yang, Adithya Iyer, Xichen Pan, Austin Wang, Rob Fergus, Yann LeCun, and Saining Xie. Cambrian-1: fully open, vision-centric exploration of multimodal llms. arXiv preprint arXiv:2406.16860, 2024. [177] Jack Urbanek, Florian Bordes, Pietro Astolfi, Mary Williamson, Vasu Sharma, and Adriana Romero-Soriano. picture is worth more than 77 text tokens: Evaluating clip-style models on dense captions. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 2670026709, 2024. [178] Ramakrishna Vedantam, Lawrence Zitnick, and Devi Parikh. Cider: Consensus-based image description evaluation. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 45664575, 2015. [179] Paul Voigtlaender, Soravit Changpinyo, Jordi Pont-Tuset, Radu Soricut, and Vittorio Ferrari. Connecting vision and language with video localized narratives. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 24612471, 2023. [180] Bryan Wang, Gang Li, Xin Zhou, Zhourong Chen, Tovi Grossman, and Yang Li. Screen2words: Automatic mobile ui summarization with multimodal learning. In The 34th Annual ACM Symposium on User Interface Software and Technology, UIST 21, page 498510, New York, NY, USA, 2021. Association for Computing Machinery. ISBN 9781450386357. doi: 10.1145/3472749.3474765. https://doi.org/10.1145/3472749.3474765. [181] Fei Wang, Xingyu Fu, James Huang, Zekun Li, Qin Liu, Xiaogeng Liu, Mingyu Derek Ma, Nan Xu, Wenxuan Zhou, Kai Zhang, et al. Muirbench: comprehensive benchmark for robust multi-image understanding. arXiv preprint arXiv:2406.09411, 2024. [182] Jiawei Wang, Liping Yuan, Yuchen Zhang, and Haomiao Sun. Tarsier: Recipes for training and evaluating large video description models. arXiv preprint arXiv:2407.00634, 2024. [183] Ke Wang, Junting Pan, Weikang Shi, Zimu Lu, Mingjie Zhan, and Hongsheng Li. Measuring multimodal mathematical reasoning with math-vision dataset. arXiv preprint arXiv:2402.14804, 2024. [184] Peng Wang, Shijie Wang, Junyang Lin, Shuai Bai, Xiaohuan Zhou, Jingren Zhou, Xinggang Wang, and Chang Zhou. One-peace: Exploring one general representation model toward unlimited modalities. arXiv preprint arXiv:2305.11172, 2023. 51 [185] Peng Wang, Shuai Bai, Sinan Tan, Shijie Wang, Zhihao Fan, Jinze Bai, Keqin Chen, Xuejing Liu, Jialin Wang, Wenbin Ge, et al. Qwen2-vl: Enhancing vision-language models perception of the world at any resolution. arXiv preprint arXiv:2409.12191, 2024. [186] Weihan Wang, Zehai He, Wenyi Hong, Yean Cheng, Xiaohan Zhang, Ji Qi, Xiaotao Gu, Shiyu Huang, Bin Xu, Yuxiao Dong, et al. Lvbench: An extreme long video understanding benchmark. arXiv preprint arXiv:2406.08035, 2024. [187] Xiaohan Wang, Yuhui Zhang, Orr Zohar, and Serena Yeung-Levy. Videoagent: Long-form video understanding with large language model as agent. arXiv preprint arXiv:2403.10517, 2024. [188] Xin Wang, Jiawei Wu, Junkun Chen, Lei Li, Yuan-Fang Wang, and William Yang Wang. Vatex: large-scale, high-quality multilingual dataset for video-and-language research. In Proceedings of the IEEE/CVF international conference on computer vision, pages 45814591, 2019. [189] Xiyao Wang, Yuhang Zhou, Xiaoyu Liu, Hongjin Lu, Yuancheng Xu, Feihong He, Jaehong Yoon, Taixi Lu, Gedas Bertasius, Mohit Bansal, Huaxiu Yao, and Furong Huang. Mementos: comprehensive benchmark for multimodal large language model reasoning over image sequences, 2024. [190] Yi Wang, Kunchang Li, Xinhao Li, Jiashuo Yu, Yinan He, Chenting Wang, Guo Chen, Baoqi Pei, Rongkun Zheng, Jilan Xu, Zun Wang, et al. Internvideo2: Scaling video foundation models for multimodal video understanding. arXiv preprint arXiv:2403.15377, 2024. [191] Yuxuan Wang, Yueqian Wang, Dongyan Zhao, Cihang Xie, and Zilong Zheng. Videohallucer: Evaluating intrinsic and extrinsic hallucinations in large video-language models. arXiv preprint arXiv:2406.16338, 2024. [192] Zirui Wang, Mengzhou Xia, Luxi He, Howard Chen, Yitao Liu, Richard Zhu, Kaiqu Liang, Xindi Wu, Haotian Liu, Sadhika Malladi, et al. Charxiv: Charting gaps in realistic chart understanding in multimodal llms. arXiv preprint arXiv:2406.18521, 2024. [193] Jason Wei, Maarten Bosma, Vincent Zhao, Kelvin Guu, Adams Wei Yu, Brian Lester, Nan Du, Andrew Dai, and Quoc Le. Finetuned language models are zero-shot learners. In International Conference on Learning Representations. [194] Chris Wendler. Renderedtext, 2024. https://huggingface.co/datasets/wendlerc/RenderedText. [195] Yuetian Weng, Mingfei Han, Haoyu He, Xiaojun Chang, and Bohan Zhuang. Longvlm: Efficient long video understanding via large language models. In European Conference on Computer Vision, pages 453470. Springer, 2025. [196] Bo Wu, Shoubin Yu, Zhenfang Chen, Joshua Tenenbaum, and Chuang Gan. Star: benchmark for situated reasoning in real-world videos. In Thirty-fifth Conference on Neural Information Processing Systems (NeurIPS), 2021. [197] Haoning Wu, Dongxu Li, Bei Chen, and Junnan Li. Longvideobench: benchmark for long-context interleaved video-language understanding. Advances in Neural Information Processing Systems, 37:2882828857, 2025. [198] Zhiyu Wu, Xiaokang Chen, Zizheng Pan, Xingchao Liu, Wen Liu, Damai Dai, Huazuo Gao, Yiyang Ma, Chengyue Wu, Bingxuan Wang, et al. Deepseek-vl2: Mixture-of-experts vision-language models for advanced multimodal understanding. arXiv preprint arXiv:2412.10302, 2024. [199] xai. Realworldqa benchmark. https://huggingface.co/datasets/xai-org/RealworldQA, 2024. [200] Junbin Xiao, Xindi Shang, Angela Yao, and Tat-Seng Chua. Next-qa: Next phase of question-answering to explaining temporal actions. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 97779786, 2021. [201] Hu Xu, Po-Yao Huang, Xiaoqing Ellen Tan, Ching-Feng Yeh, Jacob Kahn, Christine Jou, Gargi Ghosh, Omer Levy, Luke Zettlemoyer, Wen-tau Yih, et al. Altogether: Image captioning via re-aligning alt-text. arXiv preprint arXiv:2410.17251, 2024. [202] Jun Xu, Tao Mei, Ting Yao, and Yong Rui. Msr-vtt: large video description dataset for bridging video and language. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 52885296, 2016. [203] Zhangchen Xu, Fengqing Jiang, Luyao Niu, Yuntian Deng, Radha Poovendran, Yejin Choi, and Bill Yuchen Lin. Magpie: Alignment data synthesis from scratch by prompting aligned llms with nothing. arXiv preprint arXiv:2406.08464, 2024. 52 [204] Semih Yagcioglu, Aykut Erdem, Erkut Erdem, and Nazli Ikizler-Cinbis. Recipeqa: challenge dataset for multimodal comprehension of cooking recipes. arXiv preprint arXiv:1809.00812, 2018. [205] Bin Yan, Yi Jiang, Jiannan Wu, Dong Wang, Zehuan Yuan, Ping Luo, and Huchuan Lu. Universal instance perception as object discovery and retrieval. In CVPR, 2023. [206] Yan Yan, Chenliang Xu, Dawen Cai, and Jason Corso. Weakly supervised actor-action segmentation via robust multi-task ranking. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 12981307, 2017. [207] Qinghao Ye, Haiyang Xu, Jiabo Ye, Ming Yan, Anwen Hu, Haowei Liu, Qi Qian, Ji Zhang, and Fei Huang. mplug-owl2: Revolutionizing multi-modal large language model with modality collaboration. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 1304013051, 2024. [208] Kexin Yi, Chuang Gan, Yunzhu Li, Pushmeet Kohli, Jiajun Wu, Antonio Torralba, and Joshua Tenenbaum. Clevrer: Collision events for video representation and reasoning. arXiv preprint arXiv:1910.01442, 2019. [209] Peter Young, Alice Lai, Micah Hodosh, and Julia Hockenmaier. From image descriptions to visual denotations: New similarity metrics for semantic inference over event descriptions. Transactions of the Association for Computational Linguistics, 2014. [210] Weihao Yu, Zhengyuan Yang, Linjie Li, Jianfeng Wang, Kevin Lin, Zicheng Liu, Xinchao Wang, and Lijuan Wang. Mm-vet: Evaluating large multimodal models for integrated capabilities. arXiv preprint arXiv:2308.02490, 2023. [211] Zhou Yu, Dejing Xu, Jun Yu, Ting Yu, Zhou Zhao, Yueting Zhuang, and Dacheng Tao. Activitynet-qa: dataset for understanding complex web videos via question answering. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 33, pages 91279134, 2019. [212] Weizhe Yuan, Jane Yu, Song Jiang, Karthik Padthe, Yang Li, Dong Wang, Ilia Kulikov, Kyunghyun Cho, Yuandong Tian, Jason Weston, et al. Naturalreasoning: Reasoning in the wild with 2.8 challenging questions. arXiv preprint arXiv:2502.13124, 2025. [213] Ye Yuan, Xiao Liu, Wondimu Dikubab, Hui Liu, Zhilong Ji, Zhongqin Wu, and Xiang Bai. Syntax-aware network for handwritten mathematical expression recognition. arXiv preprint arXiv:2203.01601, 2022. [214] Xiang Yue, Yuansheng Ni, Kai Zhang, Tianyu Zheng, Ruoqi Liu, Ge Zhang, Samuel Stevens, Dongfu Jiang, Weiming Ren, Yuxuan Sun, et al. Mmmu: massive multi-discipline multimodal understanding and reasoning benchmark for expert agi. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 95569567, 2024. [215] Abhay Zala, Jaemin Cho, Satwik Kottur, Xilun Chen, Barlas Oguz, Yashar Mehdad, and Mohit Bansal. Hierarchical video-moment retrieval and step-captioning. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 2305623065, 2023. [216] Rowan Zellers, Yonatan Bisk, Ali Farhadi, and Yejin Choi. From recognition to cognition: Visual commonsense reasoning. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 67206731, 2019. [217] Rowan Zellers, Ximing Lu, Jack Hessel, Youngjae Yu, Jae Sung Park, Jize Cao, Ali Farhadi, and Yejin Choi. Merlot: Multimodal neural script knowledge models. Advances in neural information processing systems, 34: 2363423651, 2021. [218] Rowan Zellers, Jiasen Lu, Ximing Lu, Youngjae Yu, Yanpeng Zhao, Mohammadreza Salehi, Aditya Kusupati, Jack Hessel, Ali Farhadi, and Yejin Choi. Merlot reserve: Neural script knowledge through vision and language and sound. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 1637516387, 2022. [219] Chi Zhang, Feng Gao, Baoxiong Jia, Yixin Zhu, and Song-Chun Zhu. Raven: dataset for relational and analogical visual reasoning. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2019. [220] Jiacheng Zhang, Yang Jiao, Shaoxiang Chen, Jingjing Chen, and Yu-Gang Jiang. Eventhallusion: Diagnosing event hallucinations in video llms. arXiv preprint arXiv:2409.16597, 2024. [221] Jianrui Zhang, Mu Cai, and Yong Jae Lee. Vinoground: Scrutinizing lmms over dense temporal reasoning with short videos. arXiv preprint arXiv:2410.02763, 2024. 53 [222] Kaichen Zhang, Bo Li, Peiyuan Zhang, Fanyi Pu, Joshua Adrian Cahyono, Kairui Hu, Shuai Liu, Yuanhan Zhang, Jingkang Yang, Chunyuan Li, and Ziwei Liu. Lmms-eval: Reality check on the evaluation of large multimodal models, 2024. https://arxiv.org/abs/2407.12772. [223] Renrui Zhang, Dongzhi Jiang, Yichi Zhang, Haokun Lin, Ziyu Guo, Pengshuo Qiu, Aojun Zhou, Pan Lu, Kai-Wei Chang, Yu Qiao, et al. Mathverse: Does your multi-modal llm truly see the diagrams in visual math problems? In European Conference on Computer Vision, pages 169186, 2025. [224] Yuanhan Zhang, Jinming Wu, Wei Li, Bo Li, Zejun Ma, Ziwei Liu, and Chunyuan Li. Video instruction tuning with synthetic data, 2024. [225] Zhu Zhang, Zhou Zhao, Yang Zhao, Qi Wang, Huasheng Liu, and Lianli Gao. Where does it exist: Spatiotemporal video grounding for multi-form sentences. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 1066810677, 2020. [226] Yanli Zhao, Andrew Gu, Rohan Varma, Liang Luo, Chien-Chin Huang, Min Xu, Less Wright, Hamid Shojanazeri, Myle Ott, Sam Shleifer, et al. Pytorch fsdp: experiences on scaling fully sharded data parallel. arXiv preprint arXiv:2304.11277, 2023. [227] Yilun Zhao, Yunxiang Li, Chenying Li, and Rui Zhang. Multihiertt: Numerical reasoning over multi hierarchical tabular and textual data. In Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 65886600, Dublin, Ireland, May 2022. Association for Computational Linguistics. https://aclanthology.org/2022.acl-long.454. [228] Yilun Zhao, Chen Zhao, Linyong Nan, Zhenting Qi, Wenlin Zhang, Xiangru Tang, Boyu Mi, and Dragomir Radev. Robut: systematic study of table qa robustness against human-annotated adversarial perturbations. In Anna Rogers, Jordan Boyd-Graber, and Naoaki Okazaki, editors, Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 60646081, Toronto, Canada, July 2023. Association for Computational Linguistics. doi: 10.18653/v1/2023.acl-long.334. https://aclanthology. org/2023.acl-long.334. [229] Hanwen Zheng, Sijia Wang, Chris Thomas, and Lifu Huang. Advancing chart question answering with robust chart component recognition. arXiv preprint arXiv:2407.21038, 2024. [230] Chunting Zhou, Pengfei Liu, Puxin Xu, Srinivasan Iyer, Jiao Sun, Yuning Mao, Xuezhe Ma, Avia Efrat, Ping Yu, Lili Yu, et al. Lima: Less is more for alignment. Advances in Neural Information Processing Systems, 36: 5500655021, 2023. [231] Junjie Zhou, Yan Shu, Bo Zhao, Boya Wu, Shitao Xiao, Xi Yang, Yongping Xiong, Bo Zhang, Tiejun Huang, and Zheng Liu. Mlvu: comprehensive benchmark for multi-task long video understanding. arXiv preprint arXiv:2406.04264, 2024. [232] Luowei Zhou, Chenliang Xu, and Jason Corso. Towards automatic learning of procedures from web instructional videos. In Proceedings of the AAAI conference on artificial intelligence, volume 32, 2018. [233] Luowei Zhou, Yannis Kalantidis, Xinlei Chen, Jason Corso, and Marcus Rohrbach. Grounded video description. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 65786587, 2019. [234] Fengbin Zhu, Wenqiang Lei, Youcheng Huang, Chao Wang, Shuo Zhang, Jiancheng Lv, Fuli Feng, and TatSeng Chua. Tat-qa: question answering benchmark on hybrid of tabular and textual content in finance. In Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers), pages 32773287, Online, August 2021. Association for Computational Linguistics. doi: 10.18653/v1/2021.acl-long.254. https: //aclanthology.org/2021.acl-long.254. [235] Yuke Zhu, Oliver Groth, Michael Bernstein, and Li Fei-Fei. Visual7w: Grounded question answering in images. In IEEE Conference on Computer Vision and Pattern Recognition, 2016. [236] Dimitri Zhukov, Jean-Baptiste Alayrac, Chen Sun, Ivan Laptev, Cordelia Schmid, and Josef Sivic. Cross-task weakly supervised learning from instructional videos. In IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2019. [237] Orr Zohar, Xiaohan Wang, Yann Dubois, Nikhil Mehta, Tong Xiao, Philippe Hansen-Estruch, Licheng Yu, Xiaofang Wang, Felix Juefei-Xu, Ning Zhang, et al. Apollo: An exploration of video understanding in large multimodal models. arXiv preprint arXiv:2412.10360, 2024."
        }
    ],
    "affiliations": [
        "MBZUAI",
        "Meta FAIR",
        "Meta Reality Labs",
        "UT Austin"
    ]
}