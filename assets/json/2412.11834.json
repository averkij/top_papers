{
    "paper_title": "Wonderful Matrices: Combining for a More Efficient and Effective Foundation Model Architecture",
    "authors": [
        "Jingze Shi",
        "Bingheng Wu"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "In order to make the foundation model more efficient and effective, our idea is combining sequence transformation and state transformation. First, we prove the availability of rotary position embedding in the state space duality algorithm, which reduces the perplexity of the hybrid quadratic causal self-attention and state space duality by more than 4%, to ensure that the combining sequence transformation unifies position encoding. Second, we propose dynamic mask attention, which maintains 100% accuracy in the more challenging multi-query associative recall task, improving by more than 150% compared to quadratic causal self-attention and state space duality, to ensure that the combining sequence transformation selectively filters relevant information. Third, we design cross domain mixture of experts, which makes the computational speed of expert retrieval with more than 1024 experts 8 to 10 times faster than the mixture of experts, to ensure that the combining state transformation quickly retrieval mixture. Finally, we summarize these matrix algorithms that can form the foundation model: Wonderful Matrices, which can be a competitor to popular model architectures."
        },
        {
            "title": "Start",
            "content": "4 2 0 2 6 1 ] . [ 1 4 3 8 1 1 . 2 1 4 2 : r Wonderful Matrices: Combining for More Efficient and Effective Foundation Model Architecture Jingze Shi1 and Bingheng Wu2 Independent Researcher losercheems@gmail.com, wubingheng52136@gmail.com Abstract In order to make the foundation model more efficient and effective, our idea is combining sequence transformation and state transformation. First, we prove the availability of rotary position embedding in the state space duality algorithm, which reduces the perplexity of the hybrid quadratic causal self-attention and state space duality by more than 4%, to ensure that the combining sequence transformation unifies position encoding. Second, we propose dynamic mask attention, which maintains 100% accuracy in the more challenging multi-query associative recall task, improving by more than 150% compared to quadratic causal self-attention and state space duality, to ensure that the combining sequence transformation selectively filters relevant information. Third, we design cross domain mixture of experts, which makes the computational speed of expert retrieval with more than 1024 experts 8 to 10 times faster than the mixture of experts, to ensure that the combining state transformation quickly retrieval mixture. Finally, we summarize these matrix algorithms that can form the foundation model: Wonderful Matrices, which can be competitor to popular model architectures."
        },
        {
            "title": "1 Introduction",
            "content": "The backbone of modern foundation models usually consists of two main parts: one is sequence transformation, which assigns dependencies to elements; the other is state transformation, which assigns knowledge information to elements. In the sequence transformation part, efficient algorithms aim to compress element dependency information in limited state, while effective sequence transformation algorithms aim to store all element dependencies. Transformer (Vaswani et al. 2017) Architecture is popular in modern language modeling, it directly captures the relationship between any two elements in the sequence by calculating the causal mask matrix, which can effectively handle long-distance dependency problems. However, the architecture has major drawback: the quadratic complexity of the Quadratic Causal Self-Attention in the sequence transformation part limits the ability to handle long contexts. State Space Model (Dao and Gu 2024) Architecture came into being, it balances the quadratic and linear calculation methods of relevant elements by calculating the semiseparable matrix, which can achieve linear scaling of sequence length during training and maintain constant state size during generation. However, the architecture also has major drawback: the dependency state of the State Space Duality in the sequence transformation part does not expand with the sequence length to cause dependency bias. In the state transformation part, efficient algorithms aim to sparsely activate knowledge parameters related to elements, while effective algorithms aim to densely activate knowledge parameters related to elements. Gated Multi-Layer Perceptron (Shazeer 2020) consists of Linear layer with dense activation and an activation function, it controls the flow of information through gate units, which can suppress the output of certain neurons. However, the structure has major drawback: each output unit of the Linear layer receives information from all input units, causing the computational complexity to increase with the number of units, leading to difficult to expand. Then series of sparse mixture of experts structures appeared, among which the most efficient is the Mixture of Million Experts (He 2024) mainly composed of embedding layers and activation functions, which maintains computational efficiency through parameter-efficient expert retrieval. However, the structure also has major drawback: one input unit of the Embedding layer only activates one output unit, causing the sharing ratio to not increase with the number of units, leading to redundancy to stored. Algorithm and Architecture. Experiment and Analysis. 1 Figure 1: Wonderful Matrices Architecture. Shows the matrices used in the Wonderful Matrices Architecture, including the Rotary Position Embedding Matrix, State Space Duality Matrix, Dynamic Mask Attention Matrix, Cross Domain Mixture of Experts Matrix, and the process of using these matrices. To build model that is both efficient and effective, the key is to balance the combination relationship between different sequence transformations and state transformations. Our main goal is to integrate the State Space Duality algorithm with the Quadratic Causal Self-Attention algorithm, combining the Linear layer with dense activation and the Embedding layer with sparse activation to overcome their respective limitations. Although this hybrid algorithm foundation model architecture will lose some of the extreme excellence of specific tasks, it will have more comprehensive capabilities. Position Encoding. The key to combining the State Space Duality algorithm and the Quadratic Causal Self-Attention algorithm is to integrate the position information. In Mamba (Gu and Dao 2023), the position information is implicitly provided by causal convolution, and then the matrix skip connect the input and output of the state space algorithm to re-extend the discrete position information. In Mamba2 (Dao and Gu 2024), it is mentioned that the cumulative product of the gate can be directly used to allow two positions to interact with each other as the position information of the state space algorithm. However, convolution operations for position encoding are time-consuming, and recursive position encoding can only be applied to State Space Duality and cannot be applied to Quadratic Causal Self-Attention. Therefore, we prove the availability of Rotary Position Embedding (Su et al. 2021) in State Space Duality to unify the position encoding. Selective Transformation. Another key to combining the State Space Duality algorithm with the Quadratic Causal In the State Space Duality algorithm, selective filtering of Self-Attention algorithm is the same transformation state. sequence state information is achieved through gate matrix, which is equivalent to trainable dynamic mask. In the Quadratic Causal Self-Attention algorithm, future information leakage is prevented by predefined causal mask, which is static mask that relies entirely on human design for additional information filtering. Therefore, we propose dynamic mask attention to allow Quadratic Causal Self-Attention to dynamically adjust attention score masks based on the current value state, to match the selectivity of the State Space Duality algorithm. Full Utilization. The key to combining the Linear layer with the Embedding layer is to full utilize the parameters. Knowledge is widely distributed in different domains, which are interconnected through common general knowledge and cross domain knowledge. feedforward network composed solely of Linear layers or Embedding layers cannot fully utilize the parameters to store this knowledge. Therefore, we design Cross Domain Mixture of Experts, which has shared parameters for storing general knowledge and professional parameters for storing domain specific knowledge, and can significantly improve the granularity of experts without causing rapid decrease in computational speed. Architecture Design. We use the Rotary Position Embedding Matrix as the position encoding method, the State Space Duality Matrix and the Dynamic Mask Attention Matrix as the sequence transformation, and the Cross Domain Mixture of Experts Matrix as the state transformation. These matrices form the Wonderful Matrices architecture 1. We evaluate the Wonderful Matrices architecture on the language modeling task, including the verification of each individual module and the overall architecture. The code is open-sourced at https://github.com/LoserCheems/Doge."
        },
        {
            "title": "2 Related Work",
            "content": "Self-Attention is mechanism that computes the relevance scores between each Quadratic Causal Self-Attention. element in the sequence and all other elements, allowing each element to \"attend\" to other elements. The most important variant of attention is the Quadratic Self-Attention. notable feature of Quadratic Self-Attention is that it can capture dependencies between any positions in the input sequence, without being limited by distance, and the state expands with the sequence length, which gives it an advantage in capturing long-range dependencies in long sequences. In causal language modeling, causal mask matrix is usually added to the attention score matrix to prevent future information leakage. We refer to it as Quadratic Causal Self-Attention. 𝑌 = softmax(𝑄𝐾 ) 𝑉 State Space Duality. Many variants of Quadratic Causal Self-Attention are proposed based on the calculation improvement of the attention score matrix. The most important variant is linear attention (Katharopoulos et al. 2020), which rewrites (𝑄𝐾 ) 𝑉 = 𝑄 (𝐾 𝑉 ) by folding softmax into the kernel feature map and using the kernel properties of matrix multiplication. In the case of causal attention, they show that when the causal mask is merged to the left (𝐿 𝑄𝐾 ) 𝑉 , where 𝐿 is lower triangular matrix, the right side can be expanded into recursive form, allowing attention to perform linear autoregressive reasoning. In Transformers are SSMs (Dao and Gu 2024), the State Space Duality is used to prove that by implementing the semiseparable matrix 𝑀 = 𝐿 𝐶𝐵 = 𝐿 𝑄𝐾 and performing quadratic matrix-vector multiplication, the result is equivalent to quadratic causal kernel attention. (𝐿 𝑄𝐾 ) 𝑉 = (𝐿 𝐶𝐵) 𝑋 Rotary Position Embedding. Position information is very important in language modeling, and there are three mainstream relative positional encodings: convolution, recursive, and inner product. Rotary Position Embedding (Su et al. 2021) adds absolute position information 𝑚 and 𝑛 to the 𝑄 and 𝐾 matrices in the Quadratic Causal Self-Attention, and calculates the inner product of 𝑄𝐾 to obtain the relative position encoding matrix. < 𝑓 (𝑄, 𝑚), 𝑓 (𝐾, 𝑛) > = 𝑔(𝑄, 𝐾, 𝑚 𝑛) Shared Expert Isolation. The sparse activation mixture of experts architecture aims to train larger model in fewer training steps with limited computational resources, which often performs better than training smaller model in more steps. In the routing expert strategy, to ensure that the experts learn non-redundant general knowledge, Shared Expert Isolation (Dai et al. 2024) shares knowledge by isolating experts 𝑒 (𝑥), adding the entire sequence state of the isolated experts to the state of each token of the routing expert. 𝑒 (𝑥) = 𝑘 𝑖=1 𝑒𝑖 (𝑥𝑖 ) + 𝑛𝑘 𝑖=1 𝑒𝑖 (𝑥) In knowledge-intensive modeling tasks, the finer the granularity of the sparse Parameter Efficient Expert Retrieval. activation mixture of experts, the lower the model perplexity, but the retrieval time of the routing expert strategy will also increase significantly. Mixture of Million Experts (He 2024) proposes parameter-efficient expert retrieval, which maintains computational efficiency with large number of experts. 𝑒𝑖 (𝑥) = 𝜎 (𝑑 𝑖 𝑥)𝑢𝑖"
        },
        {
            "title": "3 Methods",
            "content": "Wonderful Matrices is foundation architecture designed to build efficient and effective models. First, we prove the availability of Rotary Position Embedding Rotary Position Embedding for Hybrid Algorithms. in the hybrid State Space Duality and Quadratic Causal Self-Attention algorithms. Ensuring that the position encoding is consistent for the hybrid algorithm, whether it is training or inference. The method is described in Section 3.1. Second, we propose Dynamic Mask Attention with the same selectivity as State Space Dynamic Mask Attention. Duality. Ensuring that Quadratic Causal Self-Attention can selectively filter past states related to the current state, directly masking irrelevant states in the attention score matrix. The method is described in Section 3.2. Cross Domain Mixture of Experts. Third, we design Cross Domain Mixture of Experts composed of Embedding layers and Linear layers. Ensuring that in dense knowledge tasks such as language modeling, parameters can be fully utilized to store general knowledge and domain-specific knowledge. The method is described in Section 3.3. Finally, we combine Rotary Position Embedding, State Space Duality, Dynamic Mask Attention, Architecture Design. Cross Domain Mixture of Experts to design the Wonderful Matrices architecture in the language modeling task. The architecture is described in Section 3.4."
        },
        {
            "title": "3.1 Rotary Position Embedding for Hybrid Algorithms\nFor example, in the Self-Attention 𝑄𝐾 ⊤, the dot product of two vectors 𝑄𝑚 · 𝐾𝑛 is calculated, and the result is a scalar,\nwhich represents the correlation between position 𝑚 and position 𝑛. The basic idea of rotary position embedding is to\nencode the position information as a complex rotary matrix, whose angle is determined by the position index. When 𝑄𝐾\nor 𝐶𝐵 is applied with the Rotary Position Embedding, if an element position is close to the front, its rotation will affect\nthe direction of the 𝐾 or 𝐵 vector multiplied with it, thereby affecting the result of the inner product.",
            "content": "The first step is to define the absolute position information embedding function 1. We define four functions to add absolute position information, where the 𝑚-th position of the 𝑄 and 𝐶 matrices adds absolute position information 𝑚, and the 𝑛-th position of the 𝐾 and 𝐵 matrices adds absolute position information 𝑛. Where 𝜃 is the rotation angle. 𝑓 (𝑞, 𝑚) = 𝑞𝑒𝑖𝑚𝜃 𝑓 (𝑘, 𝑛) = 𝑘𝑒𝑖𝑛𝜃 𝑓 (𝑐, 𝑚) = 𝑐𝑒𝑖𝑚𝜃 𝑓 (𝑏, 𝑛) = 𝑏𝑒𝑖𝑛𝜃 (1) The second step is to define the score algorithm of the rotary position encoding for hybrid algorithms 2. In Appendix A, we prove how to achieve rotary position encoding in the semiseparable matrix used in State Space Duality. Then we can use the same method to calculate the score with relative position information for each position of 𝑄𝐾 or 𝐶𝐵. Where 𝑅𝑑 Θ is the rotation matrix, Θ is the rotation angle set. 𝑎𝑡𝑡𝑛𝑠𝑐𝑜𝑟𝑒 = 𝑓 (𝑞, 𝑚)𝑅𝑑 Θ,𝑚𝑛 𝑓 (𝑘, 𝑛) 𝑠𝑠𝑑𝑠𝑐𝑜𝑟𝑒 = 𝑓 (𝑐, 𝑚)𝑅𝑑 Θ,𝑚𝑛 𝑓 (𝑏, 𝑛) (2) The final step is to apply the causal mask 𝐿 to the score matrix after position encoding, and output the score matrix with position information to 𝑉 and 𝑋 3 to extract features. 𝑦 = 𝐴𝑡𝑡𝑛𝑠𝑐𝑜𝑟𝑒 𝐿 𝑉 𝑦 = 𝑆𝑆𝐷𝑠𝑐𝑜𝑟𝑒 𝐿 𝑋 (3) In addition, another important reason for unifying the position encoding of Quadratic Causal Self-Attention and State Space Duality into Rotary Position Embedding is to achieve effective position information for linear fast inference 4. Rotary Position Embedding is way to achieve relative position encoding with absolute position encoding, without operating the score matrix, it is relative position encoding method that can be directly used for linear attention, and linear attention 4 Figure 2: RoPE for Hybrid Algorithms. Shows the algorithm of Rotary Position Embedding. In the case of input containing sequence dimension and hidden dimension, first add absolute position information 𝑚 to the 𝑄 and 𝐶 matrices, add absolute position information 𝑛 to the 𝐾 and 𝐵 matrices, then multiply the rotation matrix R𝑑 Θ,𝑚 and R𝑑 Θ,𝑛 with the 𝑄𝐾 or 𝐶𝐵 matrix to obtain the rotary position encoding matrix, and finally apply the mask matrix and output. can use cache to achieve fast generation. State Space Duality can achieve two generation update methods, one is to use cache, and the other is to directly recursively all previous sequence states to the current sequence state, both of which can use Rotary Position Embedding to achieve relative position encoding. Where 𝜙 and 𝜑 are non-negative functions. 𝐴𝑡𝑡𝑛(𝑄, 𝐾, 𝑉 )𝑖 = (cid:205)𝑛 𝑗=1 [𝑅𝑖𝜙 (𝑞𝑖 )] [𝑅 𝑗𝜑 (𝑘 𝑗 )]𝑣 𝑗 𝑗=1 𝜙 (𝑞𝑖 )𝜑 (𝑘 𝑗 ) (cid:205)𝑛 𝑆𝑆𝐷 (𝐶, 𝐵, 𝑋 )𝑖 = (cid:205)𝑛 𝑗=1 [𝑅𝑖𝜙 (𝑐𝑖 )] [𝑅 𝑗𝜑 (𝑏 𝑗 )]𝑥 𝑗 𝑗=1 𝜙 (𝑐𝑖 )𝜑 (𝑏 𝑗 ) (cid:205)𝑛 (4) This unified Rotary Position Embedding method allows different sequence transformation algorithms to share the same position information, ensuring the consistency of sequence transformation, and does not need to operate the score matrix directly, and can be used in linear generation. The algorithm matrix of the rotary position embedding is shown in Figure 2. In Appendix B.1, an implementation code example of RoPE and its application in Attn and SSD are provided."
        },
        {
            "title": "3.2 Dynamic Mask Attention\nIn the Quadratic Causal Self-Attention algorithm, the 𝑄 and 𝐾 related to the entire input sequence are calculated to\nform the attention score matrix, to extract information from the 𝑉 related to the input sequence. Linear attention caches\nthe hidden state of 𝐾𝑉 , only calculates the current 𝑄 state with the cached 𝐾𝑉 state, to achieve linear complexity of\nautoregressive inference. If there is a gate that can selectively filter the information related to the current state from the\ncached state, then the attention score mask can be dynamically adjusted.",
            "content": "The first step is to define the input-related projection function 5. We define four functions to project the input state, the projection function of 𝑄 𝐾 𝑉 is the matrix multiplication of the input 𝑥 with the related matrix weight, while the projection function of the dynamic mask 𝐴 is to use the zero-order hold technique. Here we explain some of the reasons for using the zero-order hold technique for dynamic masks. First, considering that the states usually contain continuous correlation in language modeling, to reduce the computational cost, we will retain its value every time we receive continuous correlated state until we receive new non-continuous correlated state. Second, to maintain this value over time, we introduce learnable parameter 𝑊Δ, which linearly projects the 𝑉 state to directly obtain the correlation with the past cached state and the current input state. Third, we pass the time step through non-negative function 𝜏Δ, because we cannot guarantee that the value domain of the learnable parameter 𝐴 is non-negative. Finally, we calculate the time step and parameter 𝐴 through the exponential function to achieve the zero-order hold technique. 5 Figure 3: Dynamic Mask Attention. Shows the algorithm of Dynamic Mask Attention. The input is first projected through the projection function to obtain 𝑄𝐾𝑉 , then the attention score is calculated by the 𝑄 state and the concatenated past state 𝐾 state, the causal mask is applied to the attention score, and finally the score matrix is applied with the dynamic mask related to the concatenated past state 𝑉 state, and output to the 𝑉 state. 𝑓 (𝑥,𝑊𝑄 ) = 𝑥𝑊𝑄 𝑓 (𝑥,𝑊𝐾 ) = 𝑥𝑊𝐾 𝑓 (𝑥,𝑊𝑉 ) = 𝑥𝑊𝑉 𝑓 (𝑉𝑐𝑎𝑐ℎ𝑒, 𝑉 ,𝑊Δ, 𝐴) = exp(𝜏Δ (concat(𝑉𝑐𝑎𝑐ℎ𝑒, 𝑉 )𝑊Δ)𝐴) (5) The second step is to calculate the attention score and apply the causal mask 6. We first concatenate the current 𝐾 state with the past 𝐾 state, then perform dot product calculation with the current 𝑄 state to obtain the attention score. Finally, apply the causal mask 𝐿 to the attention score to obtain the causal mask attention score matrix 𝑀. 𝑀 = 𝑓 (𝑥,𝑊𝑄 ) concat(𝐾𝑐𝑎𝑐ℎ𝑒, 𝑓 (𝑥,𝑊𝐾 )) 𝐿 (6) The final step is to apply the dynamic mask to the score matrix and output it to the value state 7. We first concatenate the past 𝑉 state with the current 𝑉 state, then perform zero-order hold operation with the time step weight 𝑊Δ and parameter 𝐴 to obtain the dynamic mask, and finally apply the dynamic mask to the score matrix and perform matrix multiplication with the 𝑉 state to obtain the final state. Here we explain some additional operations on how the dynamic mask is applied to the score matrix. First, the time-sampled state will be operated by non-negative function, only the parameter 𝐴 will have negative values during continuous learning. Second, after the Δ𝐴 state is operated by the exponential function, the negative values will be converted to values less than 1. Finally, if the causal mask is applied to the attention score using addition, then its value domain will be (, 0], we can convert the part of the dynamic mask less than 1 to and apply it to the attention score. If the causal mask is applied to the attention score using multiplication, then its value domain will be [0, 1], we can directly perform element-wise multiplication between the dynamic mask and the causal mask, the part originally filled with 0 will not change, while the other parts can be dynamically attenuated or enhanced. 𝑦 = 𝑀 𝑓 (𝑉𝑐𝑎𝑐ℎ𝑒, 𝑉 ,𝑊Δ, 𝐴) concat(𝑉𝑐𝑎𝑐ℎ𝑒, 𝑓 (𝑥,𝑊𝑉 )) (7) This Quadratic Causal Self-Attention algorithm with dynamic mask can selectively filter the information related to the current state from the final superimposed state of self-attention, which can directly mask invalid states, and can attenuate or enhance some states, without causing decay or bias in past states. The algorithm matrix of inner function attention is shown in Figure 3. An implementation code example of dynamic mask attention is provided in Appendix B.3. 6 Figure 4: CDMoE. Shows the algorithm of Cross Domain Mixture of Experts. The inputs first passes through the query projection, then calculates the dot product with the keys to obtain the affinity with the private experts, then activates the top private expert parameters with high affinity, and finally mixes with the cross domain parameters and outputs."
        },
        {
            "title": "3.3 Cross Domain Mixture of Experts\nIn the conventional mixture of experts strategy, the tokens assigned to different experts need to have common knowledge,\nso each expert may have redundant parameters for storing the same information. The proportion of expert parameter\nredundancy increases with the increase in expert granularity and the decrease in the number of expert activations. In\nParameter Efficient Expert Retrieval, due to the high granularity of the experts, even if the expert weight rows are all\nshared, the proportion of expert parameter redundancy reaches an astonishing height due to the low number of expert\ncolumn activations. If the tokens assigned to different experts have passed through the parameters for storing common\nknowledge, the parameter redundancy can be reduced.",
            "content": "The first step is to initialize all weights 8. We define query projection matrix weight 𝑊𝑄 , learnable key parameter 𝜃𝐾 , two Embedding matrices 𝑊𝑑𝑜𝑤𝑛 and 𝑊𝑢𝑝 , and two Linear matrices 𝑊𝑢𝑝 and 𝑊𝑑𝑜𝑤𝑛. Where 𝜎 represents the activation function, 𝑑𝑚𝑜𝑑𝑒𝑙 represents the model hidden dimension, 𝑑𝑐𝑑 represents the cross domain dimension, 𝑑𝑟𝑒𝑡 represents the expert retrieval dimension, 𝑛𝑒 represents the number of experts, and 𝑛ℎ represents the number of expert heads. 𝑊𝑒𝑄 R𝑑𝑚𝑜𝑑𝑒𝑙 𝑛ℎ 𝑑𝑟𝑒𝑡 𝜃𝑒𝐾 R𝑛ℎ 𝑛𝑒 𝑑𝑟𝑒𝑡 𝑊𝑒𝑑𝑜𝑤𝑛,𝑊𝑒𝑢𝑝 R𝑛𝑒 𝑑𝑚𝑜𝑑𝑒𝑙 𝑊𝑐𝑑𝑢𝑝,𝑊𝑐𝑑𝑑𝑜𝑤𝑛 R𝑑𝑐𝑑 𝑑𝑚𝑜𝑑𝑒𝑙 (8) The second step is to retrieve the expert state 9. First, perform matrix multiplication of the input 𝑥 with 𝑊𝑒𝑄 to obtain the query projection, and perform matrix multiplication with 𝜃𝑇 𝑒𝐾 to obtain the dot product similarity 𝑔. Then, take the topk expert scores and indices corresponding to each ℎ in the 𝑛𝑒 dimension of 𝑔, and combine them to obtain the similarity score 𝑠 and expert index 𝑖. Finally, perform matrix multiplication of the index position 𝑖 with 𝑊 𝑇 𝑒𝑢𝑝 , which is the embedding extension hidden dimension, to take out the weight rows of the expert dimension, obtaining the embedding states 𝑑 and 𝑢 corresponding to the two index positions. and 𝑊 𝑇 𝑒𝑑𝑜𝑤𝑛 𝑒𝐾 R2𝑏𝑎𝑡𝑐ℎ𝑠𝑒𝑞𝑛ℎ 𝑔 = 𝑥 𝑊𝑒𝑄 𝜃𝑇 𝑠, 𝑖 = topk(𝑔, 𝑘, 𝑑𝑖𝑚 = 1) R𝑏𝑎𝑡𝑐ℎ𝑠𝑒𝑞𝑛ℎ 𝑘 𝑑, 𝑢 = 𝑖 𝑊 𝑇 𝑒𝑢𝑝 R𝑏𝑎𝑡𝑐ℎ𝑠𝑒𝑞𝑛ℎ 𝑘 𝑑𝑚𝑜𝑑𝑒𝑙 𝑒𝑑𝑜𝑤𝑛, 𝑖 𝑊 𝑇 𝑛𝑒 (9a) (9b) (9c) The final step is to mix the expert state with the cross domain state 10. First, perform matrix multiplication of the input state 𝑥 with 𝑑𝑇 , then perform non-linear activation and multiply with the score 𝑠 to obtain the expert weight 𝑤. The second step is to perform matrix multiplication of the expert weight 𝑤 with 𝑢, obtaining 𝑛ℎ 𝑘 different expert states, and then summing in the 𝑛ℎ, 𝑘 dimension to combine these different expert states to obtain the expert state 𝜑. The third step 7 is to calculate the cross domain state information, perform matrix multiplication of the input 𝑥 with 𝑊𝑐𝑑𝑢𝑝 , then perform non-linear activation and matrix multiplication with 𝑊𝑐𝑑𝑑𝑜𝑤𝑛 to obtain the cross domain state information 𝜙. Finally, add the two states to obtain the final cross domain mixture of experts state 𝑦. 𝜑 = 𝑛ℎ 𝑘 𝑖= 𝑗=1 𝜎 (𝑥 𝑑𝑇 𝑖,𝑗 𝑠𝑖,𝑗 ) 𝑢𝑖,𝑗 R𝑏𝑎𝑡𝑐ℎ𝑠𝑒𝑞𝑑𝑚𝑜𝑑𝑒𝑙 𝜙 = 𝜎 (𝑥𝑊𝑐𝑑𝑢𝑝 )𝑊𝑐𝑑𝑑𝑜𝑤𝑛 R𝑏𝑎𝑡𝑐ℎ𝑠𝑒𝑞𝑑𝑚𝑜𝑑𝑒𝑙 𝑦 = 𝜑 + 𝜙 R𝑏𝑎𝑡𝑐ℎ𝑠𝑒𝑞𝑑𝑚𝑜𝑑𝑒𝑙 (10a) (10b) (10c) This cross domain with efficient retrieval mixture of experts method not only has large MLP to store the main state transformation information but also can dynamically combine different small MLPs in the 𝑛ℎ dimension. The small MLPs share neurons by aggregating the ℎ singletons retrieved from the shared weight rows, which can efficiently retrieve the expert with the highest affinity for each token and maintain speed as the number of experts increases without rapid decline as in the routing strategy. The internal structure and calculation process of the cross domain mixture of experts matrix are shown in Figure 4. An implementation code example of CDMoE is provided in Appendix B.4."
        },
        {
            "title": "3.4 Architecture Design\nWe designed an architecture using these matrices in the language modeling task: Cheems. It first uses Word Embeddings\nto convert discrete vocabulary into continuous vectors, and outputs the vocabulary probability distribution after passing\nthrough Final Norm and LM Head. In the model backbone part, we use RoPE as the position encoding before each sequence\ntransformation module, and use a CDMoE module as the state transformation after the sequence transformation, with\nIn the\ninput normalization and residual connection between each sequence transformation and state transformation.\nsequence transformation combination method, we stack 7 SSD modules for each stack, and stack 1 DMAttn module for\neach stack, to ensure the performance in-context learning. The architecture of Cheems is shown in Figure 5.",
            "content": "Figure 5: Wonderful Matrices in Language Modeling: Cheems. Shows the architecture of Wonderful Matrices applied in language modeling, including Word Embedding, RMSNorm, Residual, RoPE, SSD, DMAttn, CDMoE, LM Head modules. The black arrows indicate the calculation order of the modules, the black dashed part indicates stacking this part 7 times, and the black solid line indicates stacking the entire backbone module part 𝑁 times. The dog in the upper right corner is the internet-famous Shiba Inu Cheems, which is our sense of humor, allowing us to relax and smile in strict formula derivation work. For the beauty of the table, in subsequent experiments, we will use Cheems as our model name."
        },
        {
            "title": "4.1 Effect of Modules",
            "content": "Table 1: 𝐶𝑜𝑛𝑣1𝑑 + 𝐷 vs. 𝑎𝑡 vs. 𝑅𝑜𝑃𝐸. We use QCAttn to represent Quadratic Causal Self-Attention, SSD to represent State Space Dual, and DMAttn to represent Dynamic Mask Attention. In single module, QCAttn cannot use 𝐶𝑜𝑛𝑣1𝑑 + 𝐷 and 𝑎𝑡 . With the sequence length set to 8192, the perplexity performance of all combinations, 𝑅𝑜𝑃𝐸 is better than 𝐶𝑜𝑛𝑣1𝑑 + 𝐷 and 𝑎𝑡 . Modules QCAttn SSD SSD + QCAttn SSD + DMA(add) SSD + DMA(mul) 𝐶𝑜𝑛𝑣1𝑑 + 𝐷 ppl 8.56 8.48 8.38 8.24 𝑎𝑡 ppl 8.62 8.56 8.44 8.38 𝑅𝑜𝑃𝐸 ppl 8.38 8.33 8.18 7.92 7. Table 2: MLP vs. CDMoE. We use to represent the SSD module, to represent the QCAttn module, to represent the MLP module, and to represent the CDMoE module. We strictly construct different models with the same number of parameters, and the perplexity on the pre-training subset gradually decreases as the MoE ratio increases. Modules MoE Ratio SMSMSMSMSMSMSMAM SE SMSMSMSMSMSMAM SMSMSMSMSMSMSM AE SE SMSMSMSMSMSM AE SM SE SE SE SE SE SE AM SE SE SE SE SE SE SE AE 0% 6.25% 6.25% 12.5% 37.5% 50% ppl 8.18 8.06 8.12 7.96 7.52 7. Figure 6: Multi-Query Associative Recall. We introduced more difficult version of the original multi-query associative recall task (Arora et al. 2024), including longer sequence lengths, smaller model dimensions, etc. For detailed parameters, see Appendix C.1. We compared the baseline methods, Quadratic Causal Self-Attention, State Space Dual, and our method, Dynamic Mask Attention, which maintained good effectiveness in most cases. Figure 7: Multi-Expert Retrieval Mixture. We tested Shared Expert Isolation Mixture of Experts, Product Key Memory (Lample et al. 2019), Parameter Efficient Expert Retrieval and our method in the retrieval mixture speed at different 𝑛𝑒 . When the number of experts reaches 1024 or more, CDMoE can maintain good efficiency. activation ratios 𝑎𝑐𝑡𝑟𝑎𝑡𝑖𝑜 9 Table 3: MoE vs. SEIMoE vs. CDMoE in CEvalBenchmark. We keep the sequence transformation part of the Transformer architecture unchanged, use classic Mixture of Experts, Shared Expert Isolation Mixture of Experts, and our Cross Domain Mixture of Experts as state transformation to construct three models with almost the same total parameters and activation parameters for pre-training. We list the zero-shot and five-shot accuracy of these three models on each subdomain task from CEvalBenchmark (Huang et al. 2023). CDMoE achieved the best results on all tasks. Task STEM Social Science Humanities Other Average MoE zero-shot 46.88 43.52 46.89 38.76 44.29 SEIMoE zero-shot 51.60 47.93 51.36 44.68 49.29 CDMoE zero-shot 53.94 48.66 53.20 48.00 51.31 MoE five-shot 47.42 41.78 48.54 41.80 44. SEIMoE five-shot 52.22 46.47 54.10 48.43 50.37 CDMoE five-shot 55.01 50.64 56.76 48.94 52.88 In Table 1, we can see that whether using the State Space Dual algorithm alone or the Quadratic Causal Self-Attention algorithm alone, or using both, the Rotary Position Embedding has the best perplexity performance on long sequences. When the State Space Dual algorithm is combined with Dynamic Mask Attention, not only the position encoding is unified but also the selective state filtering is unified, making this combination perform better on long sequences. In Table 2, we can see that as the proportion of Cross Domain Mixture of Experts using combination of dense activation Linear layers and sparse activation Embedding layers increases in the entire model, the perplexity on the pre-training subset gradually decreases. However, we also found that in the first and last state transformation modules, the perplexity performance of using completely dense activation MLP modules and using all Cross Domain Mixture of Experts modules is similar. Perhaps this can increase the stability of the input and output of the model backbone. However, for the sake of simple modeling, we chose to use all Cross Domain Mixture of Experts. In Figure 6, we can see that in the more difficult multi-query associative recall task, just keeping the dimension of single attention head at 128 or above, Dynamic Mask Attention can maintain good effectiveness. Especially when the sequence length is 2048, the recall ability of the baseline, Quadratic Causal Self-Attention, and State Space Dual are all limited by large amount of sequence noise, but Dynamic Mask Attention still maintains good associative recall performance. In Figure 7, we can see that when the number of experts is 16 or less and the activation ratio 𝑎𝑐𝑡𝑟𝑎𝑡𝑖𝑜 𝑛𝑒 is low, using the commonly used Shared Expert Isolation Mixture of Experts is still good choice. However, once the number of experts reaches 1024 or more, the retrieval mixture speed of Shared Expert Isolation Mixture of Experts, Product Key Memory, and Parameter Efficient Expert Retrieval will drop sharply, especially Shared Expert Isolation Mixture of Experts. Even if the number of experts is increased while keeping the number of activations unchanged, the retrieval mixture speed will decrease due to the linear cyclic retrieval method. However, Cross Domain Mixture of Experts maintains good retrieval mixture efficiency even with significant increase in expert granularity. In Table 3, we can see that by keeping the sequence transformation part of the Transformer architecture unchanged and only using different state transformation parts, that is, the feedforward network. With almost the same total parameters and activation parameters, compared with the classic Mixture of Experts and Shared Expert Isolation Mixture of Experts, our Cross Domain Mixture of Experts achieved the best zero-shot and five-shot accuracy on all subdomain tasks in CEvalBenchmark. The model parameters are similar to the 1.3𝐵 scale parameter setting in Table 7. At the same time, we also have to admire the high quality of the Smollm-Corpus (Ben Allal et al. 2024) and Chinese Cosmopedia datasets. Compared with using other pre-training datasets, mixing training with them has improved the accuracy of these three models by about 10%, especially in the five-shot."
        },
        {
            "title": "4.2 Language Modeling\nWe selected LlaMa3 (Grattafiori et al. 2024), Mamba2 (Dao and Gu 2024), and Jamba (Lieber et al. 2024) under the same\nconditions as the comparison objects of Cheems. In Figure 8, we can see that the forward and backward propagation\nefficiency of Cheems has surpassed LlaMa3 and Jamba, and maintains a lower gap with Mamba2. In Table 4, we can\nsee that Cheems is better than LlaMa3, Mamba2, and Jamba on most verification metrics. And with the increase of the\nparameter scale, the performance improvement of Cheems is more obvious.",
            "content": "10 Figure 8: Efficient Benchmark. The LlaMa3 architecture that uses QCAttn as the sequence transformation, the Mamba2 architecture that uses SSD as the sequence transformation, the Jamba architecture that uses SSD and QCAttn as the sequence transformation, and the Cheems architecture proposed in this paper. These architectures are train (both forward and backward) and valid (forward only) at different sequence lengths under the 1.3B parameter scale. Cheems is more efficient than LlaMa3 and Jamba, but slightly lower than Mamba2. Table 4: Effective Benchmark. The LlaMa3 architecture that uses QCAttn as the sequence transformation and SEIMoE as the state transformation, the Mamba2 architecture that uses SSD as the sequence transformation and SEIMoE as the state transformation, the Jamba architecture that uses SSD and QCAttn as the sequence transformation and SEIMoE as the state transformation, and our Cheems. The verification results of the models trained under the same conditions. The best results for each parameter scale are shown in bold, followed by underline. For each model parameter scale, Cheems performs better than other models in most cases. We do not provide the perplexity performance of pre-training because the model training completion time is before the gradient accumulation error fix in the Transformers Library (Wolf et al. 2020), and there is no reference value for using different gradient accumulation steps before the fix and new experiments after the fix. If our training results are reproduced in the future, the scores on these verification metrics may rise. For the introduction of the verification set and the specific model parameters, see Appendix C.2. Model MMLU TriviaQA ARC acc qem acc 8.86 LlaMa3-320M 33.65 9.36 Mamba2-320M 33.10 9.32 Jamba-320M 33.12 Cheems-320M 34.45 10.38 20.66 37.86 LlaMa3-1.3B 21.28 36.28 Mamba2-1.3B 21.60 37.43 Jamba-1.3B Cheems-1.3B 23.02 39.08 51.68 50.72 50.80 51. 59.82 58.02 59.33 59.69 acc 37.02 35.16 36.73 37.42 PIQA HellaSwag OBQA Winogrande acc 71.42 70.24 71.88 73.32 76.05 72.26 76.58 78.15 acc 53.15 54.17 55.24 55.61 55.40 58.72 59.20 62.09 acc 52.30 48.62 52.92 53.79 61.65 59.48 62.33 63.63 41.15 37.98 40.82 41. Avg 43.99 43.07 44.31 45.22 50.36 49.07 51.07 52."
        },
        {
            "title": "5 Discussion",
            "content": "In fact, we encountered many problems when completing this work, including various reasons that caused the mamba-ssm library to not work properly. Before solving this problem, we tried to directly remove the SSD sequence transformation module and modify the architecture to stack multiple MLP or CDMoE state transformation modules after single DMAttn sequence transformation module as shown in Figure 9. We found that using this model architecture, ensuring that the parameter amount is equal to or less than other architectures for language modeling, there is no significant decrease in most verification metrics. We speculate that on the one hand, DMAttn allows Transformer and SSM to transform each other, and on the other hand, in the current Transformer architecture, there may be some redundancy in the Attn layer. Studying the impact of attention scores on the depth of the layer may be research direction. 11 Figure 9: Doge Architecture. Remove the SSD sequence transformation module in the Cheems architecture and modify the architecture to stack multiple CDMoE state transformation modules after single DMAttn sequence transformation module. At the same time, Doge can also be understood as foundation model architecture where Transformer is used during training and SSM is used during inference."
        },
        {
            "title": "6 Conclusion",
            "content": "This paper explores the idea of modeling by integrating the state space dual algorithm with the quadratic causal selfattention algorithm. We studied the unified position encoding under the hybrid algorithm, proposed dynamic mask attention that can selectively filter information related to the current state, and designed cross domain mixture of experts to reduce parameter redundancy. Finally, this paper verifies that these algorithms achieve advanced performance in language modeling, promoting the development of language modeling in more efficient and effective direction. Acknowledgments We thank our families for their understanding and support in completing this work as independent researchers. At the same time, we also thank Professor Albert Gu of Carnegie Mellon University for providing us with an endorsement of ArXiv, allowing us to engage in scientific research at the undergraduate level. References [1] Simran Arora, Sabri Eyuboglu, Aman Timalsina, Isys Johnson, Michael Poli, James Zou, Atri Rudra, and Christopher Ré. Zoology: Measuring and Improving Recall in Efficient Language Models. In: The International Conference on Learning Representations (ICLR). 2024. [2] Loubna Ben Allal, Anton Lozhkov, Guilherme Penedo, Thomas Wolf, and Leandro von Werra. SmolLM-Corpus. 2024. url: https://huggingface.co/datasets/HuggingFaceTB/smollm-corpus. [3] Yonatan Bisk, Rowan Zellers, Jianfeng Gao, Yejin Choi, et al. PIQA: Reasoning about Physical Commonsense in Natural Language. In: Proceedings of the AAAI conference on Artificial Intelligence. Vol. 34. 2020. [4] Peter Clark, Isaac Cowhey, Oren Etzioni, Tushar Khot, Ashish Sabharwal, Carissa Schoenick, and Oyvind Tafjord. Think you have Solved Question Answering? Try ARC, the AI2 Reasoning Challenge. In: arXiv preprint arXiv:1803.05457 (2018). [5] Damai Dai, Chengqi Deng, Chenggang Zhao, R. X. Xu, Huazuo Gao, Deli Chen, Jiashi Li, Wangding Zeng, Xingkai Yu, Y. Wu, Zhenda Xie, Y. K. Li, Panpan Huang, Fuli Luo, Chong Ruan, Zhifang Sui, and Wenfeng Liang. DeepSeekMoE: Towards Ultimate Expert Specialization in Mixture-of-Experts Language Models. In: CoRR abs/2401.06066 (2024). url: https://arxiv.org/abs/2401.06066. 12 [6] Tri Dao and Albert Gu. Transformers are SSMs: Generalized Models and Efficient Algorithms Through Structured State Space Duality. In: International Conference on Machine Learning (ICML). 2024. [7] Leo Gao, Jonathan Tow, Stella Biderman, Sid Black, Anthony DiPofi, Charles Foster, Laurence Golding, Jeffrey Hsu, Kyle McDonell, Niklas Muennighoff, Jason Phang, Laria Reynolds, Eric Tang, Anish Thite, Ben Wang, Kevin Wang, and Andy Zou. Framework for Few-shot Language Model Evaluation. Version v0.0.1. Sept. 2021. doi: 10.5281/ zenodo.5371628. url: https://doi.org/10.5281/zenodo.5371628. [8] Aaron Grattafiori et al. The Llama 3 Herd of Models. 2024. arXiv: 2407.21783 [cs.AI]. url: https://arxiv.org/ abs/2407.21783. [9] Albert Gu and Tri Dao. Mamba: Linear-Time Sequence Modeling with Selective State Spaces. In: arXiv preprint arXiv:2312.00752 (2023). [10] Xu Owen He. Mixture of Million Experts. In: arXiv preprint arXiv:2407.04153 (2024). [11] Dan Hendrycks, Collin Burns, Steven Basart, Andy Zou, Mantas Mazeika, Dawn Song, and Jacob Steinhardt. Measuring Massive Multitask Language Understanding. 2021. arXiv: 2009.03300 [cs.CY]. url: https://arxiv.org/ abs/2009.03300. [12] Yuzhen Huang, Yuzhuo Bai, Zhihao Zhu, Junlei Zhang, Jinghan Zhang, Tangjun Su, Junteng Liu, Chuancheng Lv, Yikai Zhang, Jiayi Lei, Yao Fu, Maosong Sun, and Junxian He. C-Eval: Multi-Level Multi-Discipline Chinese Evaluation Suite for Foundation Models. In: Advances in Neural Information Processing Systems. 2023. [13] Mandar Joshi, Eunsol Choi, Daniel S. Weld, and Luke Zettlemoyer. TriviaQA: Large Scale Distantly Supervised Challenge Dataset for Reading Comprehension. 2017. arXiv: 1705.03551 [cs.CL]. [14] Angelos Katharopoulos, Apoorv Vyas, Nikolaos Pappas, and François Fleuret. Transformers are RNNs: Fast Autoregressive Transformers with Linear Attention. In: International Conference on Machine Learning. PMLR. 2020, pp. 51565165. [15] Guillaume Lample, Alexandre Sablayrolles, MarcAurelio Ranzato, Ludovic Denoyer, and Hervé Jégou. Large Memory Layers with Product Keys. In: NeurIPS. 2019, pp. 85468557. url: http://papers.nips.cc/paper/9061large-memory-layers-with-product-keys. [16] Opher Lieber, Barak Lenz, Hofit Bata, Gal Cohen, Jhonathan Osin, Itay Dalmedigos, Erez Safahi, Shaked Meirom, Yonatan Belinkov, Shai Shalev-Shwartz, et al. Jamba: Hybrid Transformer-Mamba Language Model. In: arXiv preprint arXiv:2403.19887 (2024). [17] Todor Mihaylov, Peter Clark, Tushar Khot, and Ashish Sabharwal. Can Suit of Armor Conduct Electricity? New Dataset for Open Book Question Answering. In: arXiv preprint arXiv:1809.02789 (2018). [18] Meta NVIDIA. PyTorch Container Image. https : / / catalog . ngc . nvidia . com / orgs / nvidia / containers / pytorch. 2022. [19] Keisuke Sakaguchi, Ronan Le Bras, Chandra Bhagavatula, and Yejin Choi. Winogrande: An Adversarial Winograd Schema Challenge at Scale. In: Communications of the ACM 64.9 (2021), pp. 99106. [20] Noam Shazeer. GLU Variants Improve Transformer. In: arXiv preprint arXiv:2002.05202 (2020). [21] Jianlin Su, Yu Lu, Shengfeng Pan, Ahmed Murtadha, Bo Wen, and Yunfeng Liu. Roformer: Enhanced Transformer with Rotary Position Embedding. In: arXiv preprint arXiv:2104.09864 (2021). [22] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz Kaiser, and Illia Polosukhin. Attention Is All You Need. In: Advances in Neural Information Processing Systems (NeurIPS). 2017. [23] Thomas Wolf, Lysandre Debut, Victor Sanh, Julien Chaumond, Clement Delangue, Anthony Moi, Pierric Cistac, Tim Rault, Rémi Louf, Morgan Funtowicz, Joe Davison, Sam Shleifer, Patrick von Platen, Clara Ma, Yacine Jernite, Julien Plu, Canwen Xu, Teven Le Scao, Sylvain Gugger, Mariama Drame, Quentin Lhoest, and Alexander M. Rush. Transformers: State-of-the-Art Natural Language Processing. In: Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing: System Demonstrations. Online: Association for Computational Linguistics, Oct. 2020, pp. 3845. url: https://www.aclweb.org/anthology/2020.emnlp-demos.6. [24] Rowan Zellers, Ari Holtzman, Yonatan Bisk, Ali Farhadi, and Yejin Choi. HellaSwag: Can Machine Really Finish Your Sentence? In: Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics. 2019."
        },
        {
            "title": "A RoPE for SSD",
            "content": "Proof of equation 2. by definition, ℎ0 = 𝐵0𝑥0. By induction, ℎ𝑡 = 𝐴𝑡 . . . 𝐴1𝐵0𝑥0 + 𝐴𝑡 . . . 𝐴2𝐵1𝑥1 + + 𝐴𝑡𝐴𝑡 1𝐵𝑡 2𝑥𝑡 2 + 𝐴𝑡 𝐵𝑡 1𝑥𝑡 1 + 𝐵𝑡𝑥𝑡 = 𝑡 𝑠=0 𝐴 𝑡 :𝑠𝐵𝑠𝑥𝑠 Multiplying by 𝐶𝑡 to produce 𝑦𝑡 , and vectorizing the equation to 𝑡 [T] (T is the sequence length), we derive the matrix transformation form of SSD. 𝑦𝑡 = 𝑡 𝑠=0 𝑡 𝐴 𝐶 𝑡 :𝑠𝐵𝑠𝑥𝑠 𝑦 = SSD(𝐴, 𝐵, 𝐶)(𝑥) = 𝑀𝑥 𝑗 𝐴 𝑗 𝐴𝑖+1𝐵𝑖 𝑀𝑗𝑖 (cid:66) 𝐶 Then the matrix form of SSD is represented using SSS (Sequentially Semiseparable) as 𝑀 = SSS(𝐴, 𝐵, 𝐶), where 𝑀𝑗𝑖 = 𝑗 𝐴 𝑗:𝑖𝐵𝑖 , and then considering 𝐴 is just scalar, rearranged as 𝐶 Vectorized as 𝑀𝑗𝑖 = 𝐴 𝑗:𝑖 (𝐶 𝑗 𝐵𝑖 ) 𝐿 (cid:66) 1SS(𝑎) 𝑀 = 𝐿 (𝐶𝐵) Finally, it is proved that the matrix transformation form of SSD is equivalent to Attention (𝐿 𝑄𝐾 ) 𝑉 = (𝐿 𝐶𝐵) 𝑋 . Now we have enough theoretical support to give rotational positional encoding to the 𝐶 and 𝐵 matrices in SSD. 𝐶𝑚 = 𝑓𝐶 (𝑥𝑚, 𝑚) 𝐵𝑛 = 𝑓𝐵 (𝑥𝑛, 𝑛) 𝐶𝑚 represents the output weight matrix of the 𝑚-th token corresponding to the word vector 𝑥𝑚 integrated with the position information 𝑚, 𝐵𝑛 represents the input weight matrix of the 𝑛-th token corresponding to the word vector 𝑥𝑛 integrated with the position information 𝑛. To utilize the relative positional information between tokens, we assume that the inner product operation between the 𝐶𝑚 vector and the 𝐵𝑛 vector can be represented by function 𝑔, where the input of the function 𝑔 is the word embedding vectors 𝑥𝑚 and 𝑥𝑛, and their relative positional information 𝑚 𝑛, the inner product of 𝐶𝑚 and 𝐵𝑛 and their relative positional information 𝑚 𝑛 is defined as < 𝑓𝐶 (𝑥𝑚, 𝑚), 𝑓𝐵 (𝑥𝑛, 𝑛) >= 𝑔(𝑥𝑚, 𝑥𝑛, 𝑚 𝑛) 14 Now, assuming the word embedding vector dimension is 𝑑 = 2, we have 𝑓𝐶 (𝑥𝑚, 𝑛) = (𝑊𝐶𝑥𝑚)𝑒𝑖𝑚𝜃 , for the first half of the formula 𝑊𝐶𝑥𝑚, we know that 𝑊𝐶 is two-dimensional matrix, 𝑥𝑚 is two-dimensional vector, the result of the multiplication is naturally two-dimensional vector, represented by 𝐶𝑚 𝐶𝑚 = (cid:35) (cid:34)𝐶 (1) 𝑚 𝐶 (2) 𝑚 = 𝑊𝐶𝑥𝑚 = (cid:34)𝑊 (11) 𝐶 𝑊 (21) 𝐶 𝑊 (12) 𝐶 𝑊 (22) 𝐶 (cid:35) (cid:35) (cid:34)𝑥 (1) 𝑚 𝑥 (2) 𝑚 For the second half 𝑒𝑖𝑚𝜃 , according to Eulers formula 𝑒𝑖𝑥 = cos(𝑥) + 𝑖 sin(𝑥), we have We know 𝐶𝑚 is represented in complex form, Thus, 𝑒𝑖𝑚𝜃 = cos(𝑚𝜃 ) + 𝑖 sin(𝑚𝜃 ) 𝑓𝐶 (𝑥𝑚, 𝑚) = (𝑊𝐶𝑥𝑚)𝑒𝑖𝑚𝜃 = 𝐶𝑚𝑒𝑖𝑚𝜃 𝐶𝑚 = (cid:104) 𝑚 , 𝐶 (2) 𝐶 (1) 𝑚 (cid:105) = (cid:104) 𝑚 + 𝑖𝐶 (2) 𝐶 (1) 𝑚 (cid:105) 𝑓𝐶 (𝑥𝑚, 𝑚) = 𝐶𝑚𝑒𝑖𝑚𝜃 = (cid:104) 𝑚 + 𝑖𝐶 (2) 𝐶 (1) 𝑚 (cid:105) 𝑒𝑖𝑚𝜃 According to the above derivation, we know that 𝑓𝐶 (𝑥𝑚, 𝑚) is the product of two complex numbers, 𝑓𝐶 (𝑥𝑚, 𝑚) = 𝐶𝑚𝑒𝑖𝑚𝜃 = (cid:104) 𝑚 + 𝑖𝐶 (2) 𝐶 (1) 𝑚 (cid:105) (cos(𝑚𝜃 ) + 𝑖 sin(𝑚𝜃 )) Considering the following two formulas about complex numbers (𝑎 + 𝑖𝑏) (𝑐 + 𝑖𝑑) = 𝑎𝑐 + 𝑖𝑏𝑐 + 𝑖𝑎𝑑 + 𝑖2𝑏𝑑 = (𝑎𝑐 𝑏𝑑) + 𝑖 (𝑏𝑐 + 𝑎𝑑) 𝑖2 = We have 𝐶𝑚𝑒𝑖𝑚𝜃 = (cid:104) 𝑚 + 𝑖𝐶 (2) 𝐶 (1) 𝑚 (cid:105) (cos(𝑚𝜃 ) + 𝑖 sin(𝑚𝜃 )) = (cid:104) 𝐶 (1) 𝑚 cos(𝑚𝜃 ) 𝐶 (2) 𝑚 sin(𝑚𝜃 ) (cid:105) + 𝑖 (cid:104) 𝐶 (2) 𝑚 cos(𝑚𝜃 ) + 𝐶 (1) 𝑚 sin(𝑚𝜃 ) (cid:105) Expressing this result as real vector, 𝐶𝑚𝑒𝑖𝑚𝜃 = (cid:104) 𝑚 cos(𝑚𝜃 ) 𝐶 (2) 𝐶 (1) 𝑚 sin(𝑚𝜃 ), 𝐶 (2) 𝑚 cos(𝑚𝜃 ) + 𝐶 (1) 𝑚 sin(𝑚𝜃 ) (cid:105) 15 Therefore, 𝐶𝑚 multiplied by rotation matrix is obtained. 𝑓𝐶 (𝑥𝑚, 𝑚) = (𝑊𝐶𝑥𝑚)𝑒𝑖𝑚𝜃 = 𝐶𝑚𝑒𝑖𝑚𝜃 (cid:104) 𝐶 (1) 𝑚 cos(𝑚𝜃 ) 𝐶 (2) = (cid:20)cos(𝑚𝜃 ) sin(𝑚𝜃 ) cos(𝑚𝜃 ) sin(𝑚𝜃 ) 𝑚 sin(𝑚𝜃 ), 𝐶 (2) (cid:21) (cid:34)𝐶 (1) 𝑚 𝐶 (2) 𝑚 (cid:35) 𝑚 cos(𝑚𝜃 ) + 𝐶 (1) 𝑚 sin(𝑚𝜃 ) (cid:105) = Similarly, 𝐵𝑛 vector can be obtained 𝑓𝐵 (𝑥𝑛, 𝑛) = (𝑊𝐵𝑥𝑛)𝑒𝑖𝑛𝜃 = 𝐵𝑛𝑒𝑖𝑛𝜃 𝐵 (1) 𝑛 cos(𝑛𝜃 ) 𝐵 (2) 𝑛 = (cid:104) (cid:20)cos(𝑛𝜃 ) sin(𝑛𝜃 ) cos(𝑛𝜃 ) sin(𝑛𝜃 ) sin(𝑛𝜃 ), 𝐵 (2) (cid:35) (cid:21) (cid:34)𝐵 (1) 𝑛 𝐵 (2) 𝑛 𝑛 cos(𝑛𝜃 ) + 𝐵 (1) 𝑛 (cid:105) sin(𝑛𝜃 ) = The function 𝑔 can be represented as 𝑔(𝑥𝑚, 𝑥𝑛, 𝑚 𝑛) = 𝑅 (cid:2)(𝑊𝐶𝑥𝑚)(𝑊𝐵𝑥𝑛)𝑒𝑖 (𝑚𝑛)𝜃 (cid:3) where 𝑅 represents the real part of the complex number 𝑥, (𝑊𝐶𝑥𝑚)(𝑊𝐵𝑥𝑛) represents the conjugate of the product of two complex numbers. Considering we have 𝑧 = 𝑎 + 𝑖𝑏 𝑧 = 𝑎 𝑖𝑏 𝑊𝐶𝑥𝑚 = 𝐶𝑚 = 𝐶 (1) 𝑊𝐵𝑥𝑛 = 𝐵𝑛 = 𝐵 (1) 𝑛 = 𝐵 (1) 𝑚 + 𝑖𝐶 (2) 𝑚 𝑛 + 𝑖𝐵 (2) 𝑛 𝑛 𝑖𝐵 (2) (𝑊𝐵𝑥𝑛) = 𝐵 𝑛 𝑒𝑖 (𝑚𝑛)𝜃 = cos((𝑚 𝑛)𝜃 ) + 𝑖 sin((𝑚 𝑛)𝜃 ) We now want to prove that 𝑔(𝑥𝑚, 𝑥𝑛, 𝑚 𝑛) = 𝑅 (cid:2)(𝑊𝐶𝑥𝑚)(𝑊𝐵𝑥𝑛)𝑒𝑖 (𝑚𝑛)𝜃 (cid:3) 𝑛 𝑖𝐵 (2) (𝐶 (1) 𝑚 + 𝑖𝐶 (2) 𝑚 )(𝐵 (1) = 𝑅 (cid:104) 𝑛 )(cos((𝑚 𝑛)𝜃 ) + 𝑖 sin((𝑚 𝑛)𝜃 )) (cid:105) (cid:104) = 𝑅 = (𝐶 (1) ((𝐶 (1) 𝑚 𝐵 (1) 𝑚 𝐵 (1) 𝑛 + 𝐶 (2) 𝑛 + 𝐶 (2) 𝑚 𝐵 (2) 𝑚 𝐵 (1) 𝑛 ) + 𝑖 (𝐶 (2) 𝑚 𝐵 (2) 𝑛 𝐶 (1) 𝑛 ) cos((𝑚 𝑛)𝜃 ) (𝐶 (2) 𝑚 𝐵 (2) 𝑚 𝐵 (1) 𝑛 ))(cos((𝑚 𝑛)𝜃 ) + 𝑖 sin((𝑚 𝑛)𝜃 )) 𝑛 𝐶 (1) 𝑛 ) sin((𝑚 𝑛)𝜃 ) 𝑚 𝐵 (2) (cid:105) 16 Recalling the vectorized form of SSD, the 𝐶 vector at position 𝑚 and the 𝐵 vector at position 𝑛 will perform an inner product operation, that is, 𝑓𝐶 (𝑥𝑚, 𝑚) = (cid:104) 𝐶 (1) 𝑚 cos(𝑚𝜃 ) 𝐶 (2) 𝑚 sin(𝑚𝜃 ), 𝐶 (2) 𝑚 cos(𝑚𝜃 ) + 𝐶 (1) 𝑚 sin(𝑚𝜃 ) (cid:105) 𝑓𝐵 (𝑥𝑛, 𝑛) = (cid:104) 𝑛 cos(𝑛𝜃 ) 𝐵 (2) 𝐵 (1) 𝑛 sin(𝑛𝜃 ), 𝐵 (2) 𝑛 cos(𝑛𝜃 ) + 𝐵 (1) 𝑛 sin(𝑛𝜃 ) (cid:105) < 𝑓𝐶 (𝑥𝑚, 𝑚), 𝑓𝐵 (𝑥𝑛, 𝑛) > = (cid:104) 𝑚 cos(𝑚𝜃 ) 𝐶 (2) 𝐶 (1) 𝑚 sin(𝑚𝜃 ) (cid:105) (cid:104) 𝑛 cos(𝑛𝜃 ) 𝐵 (2) 𝐵 (1) 𝑛 (cid:105) sin(𝑛𝜃 ) (cid:105) sin(𝑛𝜃 ) 𝑛 cos(𝑛𝜃 ) + 𝐵 (1) 𝐵 (2) 𝑛 (cid:105) (cid:104) (cid:104) 𝑚 cos(𝑚𝜃 ) + 𝐶 (1) 𝐶 (2) 𝑚 cos(𝑚𝜃 )𝐵 (1) 𝑚 sin(𝑚𝜃 )𝐵 (1) 𝑚 cos(𝑚𝜃 )𝐵 (2) 𝑚 sin(𝑚𝜃 )𝐵 (2) 𝑚 sin(𝑚𝜃 ) 𝑛 cos(𝑛𝜃 ) 𝐶 (1) 𝑛 cos(𝑛𝜃 ) + 𝐶 (2) 𝑛 cos(𝑛𝜃 ) + 𝐶 (2) 𝑛 cos(𝑛𝜃 ) + 𝐶 (1) + = 𝐶 (1) 𝐶 (2) + 𝐶 (2) + 𝐶 (1) 𝑚 cos(𝑚𝜃 )𝐵 (2) 𝑛 𝑚 sin(𝑚𝜃 )𝐵 (2) 𝑛 𝑚 cos(𝑚𝜃 )𝐵 (1) 𝑛 𝑚 sin(𝑚𝜃 )𝐵 (1) 𝑛 sin(𝑛𝜃 ) sin(𝑛𝜃 ) sin(𝑛𝜃 ) sin(𝑛𝜃 ) sin(𝑎 + 𝑏) = sin(𝑎) cos(𝑏) + cos(𝑎) sin(𝑏) sin(𝑎 𝑏) = sin(𝑎) cos(𝑏) cos(𝑎) sin(𝑏) cos(𝑎 + 𝑏) = cos(𝑎) cos(𝑏) sin(𝑎) sin(𝑏) cos(𝑎 𝑏) = cos(𝑎) cos(𝑏) + sin(𝑎) sin(𝑏) We have Considering We have 𝑚 𝐵 (1) < 𝑓𝐶 (𝑥𝑚, 𝑚), 𝑓𝐵 (𝑥𝑛, 𝑛) > = 𝐶 (1) 𝑚 𝐵 (2) + 𝐶 (1) 𝑚 𝐵 (1) + 𝐶 (2) + 𝐶 (2) 𝑚 𝐵 (2) = 𝐶 (1) 𝑚 𝐵 (1) 𝐶 (2) 𝑚 𝐵 (1) 𝑛 = (𝐶 (1) 𝑚 𝐵 (1) 𝑛 + 𝐶 (2) = (𝐶 (1) 𝑚 𝐵 (1) 𝑛 + 𝐶 (2) = 𝑔(𝑥𝑚, 𝑥𝑛, 𝑚 𝑛) 𝑛 (cos(𝑚𝜃 ) cos(𝑛𝜃 ) + sin(𝑚𝜃 ) sin(𝑛𝜃 )) 𝑛 ( cos(𝑚𝜃 ) sin(𝑛𝜃 ) + sin(𝑚𝜃 ) cos(𝑛𝜃 )) 𝑛 ( sin(𝑚𝜃 ) cos(𝑛𝜃 ) + cos(𝑚𝜃 ) sin(𝑛𝜃 )) 𝑛 (sin(𝑚𝜃 ) sin(𝑛𝜃 ) + cos(𝑚𝜃 ) cos(𝑛𝜃 )) 𝑛 cos((𝑚 𝑛)𝜃 ) + 𝐶 (1) sin((𝑚 𝑛)𝜃 ) + 𝐶 (2) 𝑚 𝐵 (2) 𝑛 𝑚 𝐵 (2) 𝑛 cos((𝑚 𝑛)𝜃 ) 𝑛 ) cos((𝑚 𝑛)𝜃 ) + (𝐶 (1) 𝑚 𝐵 (2) 𝑛 ) cos((𝑚 𝑛)𝜃 ) (𝐶 (2) 𝑚 𝐵 (1) 𝑚 𝐵 (2) 𝑚 𝐵 (2) sin((𝑚 𝑛)𝜃 ) 𝑛 𝐶 (2) 𝑛 𝐶 (1) 𝑚 𝐵 (1) 𝑚 𝐵 (2) 𝑛 ) sin((𝑚 𝑛)𝜃 ) 𝑛 ) sin((𝑚 𝑛)𝜃 ) It is proved that the inner product of the 𝐶 vector at position 𝑚 and the 𝐵 vector at position 𝑛 is the function 𝑔. Finally, using the matrix-vector multiplication form 17 < 𝑓𝐶 (𝑥𝑚, 𝑚), 𝑓𝐵 (𝑥𝑛, 𝑛) > = (cid:34) (cid:20)cos(𝑚𝜃 ) sin(𝑚𝜃 ) cos(𝑚𝜃 ) sin(𝑚𝜃 ) (cid:21) (cid:34)𝐶 (1) 𝑚 𝐶 (2) 𝑚 (cid:35) (cid:35)𝑇 (cid:34) (cid:20)cos(𝑛𝜃 ) sin(𝑛𝜃 ) cos(𝑛𝜃 ) sin(𝑛𝜃 ) (cid:104) 𝐶 (1) 𝑚 = 𝐶 (2) 𝑚 (cid:105) (cid:20) cos(𝑚𝜃 ) sin(𝑚𝜃 ) sin(𝑚𝜃 ) cos(𝑚𝜃 ) (cid:21) (cid:20)cos(𝑛𝜃 ) sin(𝑛𝜃 ) cos(𝑛𝜃 ) sin(𝑛𝜃 ) (cid:35) (cid:35) (cid:21) (cid:34)𝐵 (1) 𝑛 𝐵 (2) 𝑛 (cid:21) (cid:34)𝐵 (1) 𝑛 𝐵 (2) 𝑛 (cid:35) Expanding the product of the two rotary matrices, we have (cid:20) cos(𝑚𝜃 ) cos(𝑛𝜃 ) + sin(𝑚𝜃 ) sin(𝑛𝜃 ) sin(𝑚𝜃 ) cos(𝑛𝜃 ) + cos(𝑚𝜃 ) sin(𝑛𝜃 ) cos(𝑚𝜃 ) sin(𝑛𝜃 ) + sin(𝑚𝜃 ) cos(𝑛𝜃 ) sin(𝑚𝜃 ) sin(𝑛𝜃 ) + cos(𝑚𝜃 ) cos(𝑛𝜃 ) (cid:21) Finally, we get < 𝑓𝐶 (𝑥𝑚, 𝑚), 𝑓𝐵 (𝑥𝑛, 𝑛) > = (cid:104) 𝐶 (1) 𝑚 𝐶 (2) 𝑚 (cid:105) (cid:20)cos((𝑚 𝑛)𝜃 ) sin((𝑚 𝑛)𝜃 ) cos((𝑚 𝑛)𝜃 ) sin((𝑚 𝑛)𝜃 ) (cid:35) (cid:21) (cid:34)𝐵 (1) 𝑛 𝐵 (2) 𝑛 The above derivation is only for the case of word embedding dimension 𝑑 = 2, when 𝑑 > 2, the two-dimensional case can be extended to any dimension as follows 𝑓{𝐶,𝐵 } (𝑥𝑚, 𝑚) = 𝑅𝑑 Θ,𝑚𝑊{𝐶,𝐵 }𝑥𝑚 The inner product satisfies linearity, so for any even-dimensional RoPE, we can represent it as concatenation of the two-dimensional case, that is, grouping the elements of the word embedding vector in pairs 𝑅𝑑 Θ,𝑚 = cos 𝑚𝜃0 𝑠𝑖𝑛𝑚𝜃0 cos 𝑚𝜃0 sin 𝑚𝜃0 0 0 0 0 ... ... 0 0 0 0 0 0 0 0 cos 𝑚𝜃1 𝑠𝑖𝑛𝑚𝜃1 cos 𝑚𝜃1 sin 𝑚𝜃1 ... ... 0 0 0 0 . . . . . . . . . . . . . . . . . . . . . 0 0 0 0 ... 0 0 0 0 ... cos 𝑚𝜃𝑑/2 𝑠𝑖𝑛𝑚𝜃𝑑/21 cos 𝑚𝜃𝑑/21 sin 𝑚𝜃𝑑/2 Each group applies the same rotation operation and the rotation angle of each group is calculated as follows: Θ = {𝜃𝑖 = 100002(𝑖 1)/𝑑, 𝑖 [1, 2, . . . , 𝑑/2]}"
        },
        {
            "title": "B Implementation Code",
            "content": "B.1 RoPE Listing 1 PyTorch example of RoPE. class RotaryEmbedding: def __init__(self, dim, max_position_embeddings, base = 10000,scaling_factor = 1.0): self.dim, self.base, self.max_position_embeddings, self.scaling_factor = dim, base, max_position_embeddings, scaling_factor inv_freq = 1.0 / (self.base ** (torch.arange(0, self.dim, 2) / self.dim)) self.register_buffer(\"inv_freq\", inv_freq) def forward(self, x, position_ids): seq_len = torch.max(position_ids) + 1 if seq_len > self.max_position_embeddings: base = self.base * ((self.scaling_factor * seq_len / self.max_position_embeddings) - (self.scaling_factor - 1)) ** (self.dim / (self.dim - 2)) inv_freq = 1.0 / (base ** (torch.arange(0, self.dim, 2) / self.dim)) else: inv_freq = self.inv_freq inv_freq_expanded = inv_freq[None, :, None].expand(position_ids.shape[0], -1, 1) position_ids_expanded = position_ids[:, None, :] freqs = (inv_freq_expanded @ position_ids_expanded).transpose(1, 2) emb = torch.cat((freqs, freqs), dim = -1) cos, sin = emb.cos().to(x.dtype), emb.sin().to(x.dtype) return cos, sin def rotate_half(x): x1, x2 = x[..., : x.shape[-1] // 2], x[..., x.shape[-1] // 2 :] return torch.cat((-x2, x1), dim=-1) def apply_QK_rotary_pos_emb(q, k, cos, sin, unsqueeze_dim = 2): cos, sin = cos.unsqueeze(unsqueeze_dim), sin.unsqueeze(unsqueeze_dim) q_embed = (q * cos) + (rotate_half(q) * sin) k_embed = (k * cos) + (rotate_half(k) * sin) return q_embed, k_embed def apply_CB_rotary_pos_emb(c, b, cos, sin, unsqueeze_dim = 2): cos, sin = cos.unsqueeze(unsqueeze_dim), sin.unsqueeze(unsqueeze_dim) c_embed = (c * cos) + (rotate_half(c) * sin) b_embed = (b * cos) + (rotate_half(b) * sin) return c_embed, b_embed 19 B.2 SSD Listing 2 Example SSD helper function in PyTorch def pad_tensor_by_size(input_tensor, pad_size): # pad seq_len to be multiple of chunk_len return F.pad(input_tensor, (0, 0, 0, 0, 0, pad_size, 0, 0) if len(input_tensor.shape) == 4 else (0, 0, 0, pad_size, 0, 0)) def reshape_into_chunks(input_tensor, pad_size,chunk_len): # padding input_tensor with `pad_size` on the seq_len dim (dim=1) and simultaneously splitting it into chunk sequences. # ... -> (l c) ... if len(pad_tensor_by_size(input_tensor, pad_size).shape) == 3: return rearrange(input_tensor, 'b(lc)h->blch', = chunk_len) else: return rearrange(input_tensor, 'b(lc)hd->blchd', = chunk_len) def segment_sum(input_tensor): # uses cumulative sums and masking instead of direct subtractions. chunk_len = input_tensor.size(-1) # expand input tensor to have an additional dimension and repeat along that dimension # [..., chunk_len] -> [..., chunk_len, chunk_len] input_tensor = input_tensor[..., None].expand(*input_tensor.size(), chunk_len) # create lower triangular mask with the diagonal set to 0 to 0 out elements above diag mask = torch.tril(torch.ones(chunk_len, chunk_len, dtype = torch.bool), diagonal = -1) input_tensor = input_tensor.masked_fill(mask, 0) # compute actual cumsum tensor_segsum = torch.cumsum(input_tensor, dim=-2) # apply mask to keep only the lower triangular part of the cumulative sum result mask = torch.tril(torch.ones(chunk_len, chunk_len, dtype = torch.bool), diagonal = 0) tensor_segsum = tensor_segsum.masked_fill(mask, -torch.inf) return tensor_segsum 20 Listing 3 Example SSD algorithm in PyTorch. We have changed some slow methods to faster ones. The original SSD algorithm implementation can be found in the Mamba2 paper. def ssd(X, dt, A, B, C, chunk_len, D): seq_len = X.size(1) pad_size = (chunk_len - seq_len % chunk_len) % chunk_len D_residual = rearrange(D, '...->...1') * pad_tensor_by_size(X, pad_size) # discretize and X, = * rearrange(dt, '...->...1'), A.to(x.dtype) * dt # rearrange into blocks/chunks X, A, B, = [reshape_into_chunks(t, pad_size, chunk_len) for in (X, A, B, C)] # compute cumulative sum of = rearrange(A, 'bclh->bhcl', = chunk_len) A_cumsum = torch.cumsum(A, dim = -1) # compute the output for each intra-chunk (diagonal blocks) # this is the analog of causal mask = torch.exp(segment_sum(A)) # contraction of and to get (attention-weights like) = (rearrange(C, 'blchn->blc1hn') * rearrange(B, 'blchn->bl1chn')).sum(dim = -1) # shape: (b, c, l, s, h) # compute M, equivalent to applying attention mask to weights M_intermediate = rearrange(G, '...->...1') * rearrange(L, 'bhcst->bcsth1') = M_intermediate.sum(dim = -1) # compute Y_diag (apply to values) Y_diag = (rearrange(M, '...->...1') * rearrange(X, 'blchp->bl1chp')).sum(3) # (right term of low-rank factorization of off-diagonal blocks; terms) decay_states = torch.exp((A_cumsum[:, :, :, -1:] - A_cumsum)) B_decay_contraction = * rearrange(decay_states, 'bhcl->bclh1') # permute back * decay states states=(rearrange(B_decay_contraction, 'bclhs->bchls1') * rearrange(X, 'blchp->blhc1p')).sum(dim = 3).permute(0, 1, 2, 4, 3) previous_states = torch.zeros_like(states[:, :1]) states = torch.cat([previous_states, states], dim = 1) decay_chunk = torch.exp(segment_sum(F.pad(A_cumsum[:, :, :, -1], (1, 0)))) states_permuted = states.permute(0, 2, 1, 3, 4) result = (decay_chunk[..., None, None] * states_permuted[:, :, None, ...]).sum(dim = 2) new_states = result.permute(0, 2, 1, 3, 4) states=new_states[:, :-1] # compute state -> output conversion per chunk # (left term of low-rank factorization of off-diagonal blocks; terms) # compute Yoff C_times_states = rearrange(C, 'bclhn->bclh1n') * rearrange(states, 'bchpn->bc1hpn') Y_off = (C_times_states.sum(-1) * rearrange(torch.exp(A_cumsum), 'bhcl->bclh1')) # add output of intra-chunk and inter-chunk terms (diagonal and off-diagonal blocks) = rearrange(Y_diag + Y_off, 'bclhp->b(cl)hp') + D_residual # cutting off padded chunks if pad_size > 0: y=y[:, : seq_len, :, :] return 21 Listing 4 Example SSD implementation in PyTorch class SSD: def __init__(self, d_model, n_heads, n_groups, d_state, chunk_len, max_position_embedding): self.n_heads, self.n_groups, self.d_head, self.d_state, self.chunk_len = n_heads, n_groups, d_model // n_heads, d_state, chunk_len # Initialize parameters self.C_proj = nn.Linear(d_model, self.n_groups * self.d_state) self.B_proj = nn.Linear(d_model, self.n_groups * self.d_state) self.A = nn.Parameter(torch.ones(self.n_heads)) self.dt_proj = nn.Linear(d_model, self.n_heads) self.X_proj = nn.Linear(d_model, self.n_heads * self.d_head) self.D = nn.Parameter(torch.ones(self.n_heads)) self.out_proj = nn.Linear(d_model, d_model) # Rotary Position Embedding self.BC_rotary_emb = RotaryEmbedding(self.d_state, max_position_embedding) def forward(self, x): \"\"\" Notations: - batch size - d_model - n_heads - d_head - d_state - n_groups - target sequence length - source sequence length - n_chunks - chunk_len \"\"\" # linear projection B = rearrange(self.B_proj(x), 'bt(gn)->btgn', = self.n_groups, = self.d_state).repeat(1, 1, self.n_heads // self.n_groups, 1) = rearrange(self.C_proj(x), 'bt(gn)->btgn', = self.n_groups, = self.d_state).repeat(1, 1, self.n_heads // self.n_groups, 1) = rearrange(self.X_proj(x),'bt(hp)->bthp',h=self.n_heads) # apply rotary position embedding to and cos, sin = self.BC_rotary_emb(x, position_ids) C, = apply_CB_rotary_pos_emb(C, B, cos, sin) dt = F.softplus(self.dt_proj(x)) if mamba_libray: = mamba_chunk_scan_combined(X, dt, self.A, B, C, self.chunk_len, self.D) else: = ssd(X, dt, self.A, B, C, self.chunk_len, self.D) = self.out_proj(rearrange(y, 'bthp->bt(hp)')) return 22 B.3 DynamicMaskAttn Listing 5 Example Dynamic Mask Attention implementation in PyTorch class DMAttn: def __init__(self, d_model, n_heads, max_position): self.n_heads, self.d_head = n_heads, d_model // n_heads # Initialize Parameters self.Q_proj = Linear(d_model, d_model) self.K_proj = Linear(d_model, d_model) self.A = nn.Parameter(torch.ones(n_heads)) self.dt_proj = Linear(d_model, n_heads) self.V_proj = Linear(d_model, d_model) self.out_proj = Linear(d_model, d_model) # Rotary Position Embedding self.QK_rotary_emb = RotaryEmbedding(self.d_head, max_position) def forward(self, x, causal_mask, position_ids, past_kv): \"\"\" Notation: - batch - length - d_model - n_heads - d_head \"\"\" # linear projection Q, K, = self.Q_proj(x), self.K_proj(x), self.V_proj(x) # split into multiple heads = rearrange(Q, \"bt(hp)->bhtp\", = self.n_heads) = rearrange(K, \"bt(hp)->bhtp\", = self.n_heads) = rearrange(V, \"bt(hp)->bhtp\", = self.n_heads) # apply rotary position embedding to and cos, sin = self.QK_rotary_emb(x, position_ids) Q, = apply_QK_rotary_pos_emb(Q, K, cos, sin) # concatenate past key value K, = past_kv.update(K, V) # compute attention score matrix attn_score = torch.matmul(Q, K.transpose(-2, -1)) / math.sqrt(self.d_head) # add mask to attention score dt = self.dt_proj(rearrange(V, \"bhtp->btd\")) dynamic_mask = torch.exp(self.A * F.softplus(dt)) dynamic_mask = rearrange(dynamic_mask, \"bth->bht\") < 1.0 mask = causal_mask.masked_fill(dynamic_mask[:, :, None, :], '-inf') attn_score = F.softmax(attn_score + mask, dim = -1) # apply attention score to states = torch.matmul(attn_score, V) = self.out_proj(rearrange(y, \"bhtp->bt(hp)\")) return 23 B.4 Cross Domain Mixture of Experts Listing 6 Example CDMoE implementation in PyTorch class CDMoE: def __init__(self, act, d_model, d_cd, d_ret, n_experts, n_heads, k_per_head): self.act_fn, self.n_heads, self.k_per_head = ACT2FN[act], n_heads, k_per_head # Queries and Keys self.queries = Linear(d_model, d_ret * n_heads) self.num_keys = math.sqrt(n_experts) self.keys = Parameter(torch.zeros(n_heads, self.num_keys, 2, d_ret // 2)) # Experts self.down_embed = Embedding(n_experts, d_model) self.up_embed = Embedding(n_experts, d_model) # Cross Domain self.up_proj = Linear(d_model, d_cd) self.down_proj = Linear(d_cd, d_model) def forward(self, x): \"\"\" Notation: - batch - length - d_model - d_retrieval - n_heads - 2 for product key - number of keys \"\"\" # get similarity with queries and keys queries = self.queries(x) queries = rearrange(queries, 'bt(phn)->pbthn', = 2, = self.n_heads) # get experts with the highest similarity sim = einsum('pbthn,hkpn->pbthk', queries, self.keys) (s_x, s_y), (i_x, i_y) = sim.topk(self.k_per_head, dim = -1) all_s = einx.add('... i, ... -> ... (i j)', s_x, s_y) all_i = einx.add('... i, ... -> ... (i j)', i_x * self.num_keys, i_y) s, pk_i = all_s.topk(self.k_per_head, dim = -1) = all_i.gather(-1, pk_i) down_embed, up_embed = self.down_embed(i), self.up_embed(i) # mix experts states with cross domain states experts_w = self.act_fn(einsum('btd,bthkd->bthk', x, down_embed) * s) experts_states = einsum('bthk,bthkd->btd', experts_w, up_embed) cross_domain_states = self.down_proj(self.act_fn(self.up_proj(x))) = cross_domain_states + experts_states return"
        },
        {
            "title": "C Evaluation Parameters",
            "content": "C.1 Multi-Query Associative Recall Table 5: Data Parameters. We introduce more challenging task version based on the original multi-query associative recall (Arora et al. 2024), where tokens that are not query/key/value are replaced with random tokens. We also use more key-value pairs and longer sequence lengths. For each sequence length 𝑇 {256, 512, 1024, 2048}, we use 𝑇 /4 key-value pairs. The total vocabulary size is 8192, with approximately 250𝑘 training samples and 1𝑘 test samples. vocab seq len kv pairs train examples test examples powar batch max epochs 8192 8192 8192 8192 256 512 1024 2048 64 128 256 512 218 218 218 210 210 210 210 0.01 0.01 0.01 0.01 256 128 64 32 64 64 64 64 Table 6: Model Parameters. These algorithms can all split into multiple heads, so we set them to single heads and use common single head dimensions 𝑑𝑚𝑜𝑑𝑒𝑙 {32, 64, 128, 256}. For fairness, the SSD algorithm is different from the validation structure in Mamba2 (Dao and Gu 2024) and we remove the one-dimensional causal convolution and gated MLP. All algorithms use the structure of sequence transformation to state transformation and stack 2 layers. In preparation for subsequent algorithm mixing, the learning rate for each dimension of these algorithms is the same. Algorithm QCAttn SSD DMAttn 𝑑𝑚𝑜𝑑𝑒𝑙 32/64/128/256 32/64/128/256 32/64/128/256 𝑛𝑙𝑎𝑦𝑒𝑟𝑠 2 2 2 𝑛ℎ𝑒𝑎𝑑𝑠 1 1 1 𝑑𝑠𝑡𝑎𝑡𝑒 128 chunk_len leaning rate 256 4e-4/3e-4/2e-4/1e-4 4e-4/3e-4/2e-4/1e-4 4e-4/3e-4/2e-4/1e-4 Figure 10: Different Algorithms Parameters. At different dimensional scales, the number of parameters of DMAttn is not much different from QCAttn. SSD increases the number of parameters less when increasing the dimensional scale. 25 C.2 Downstream Evaluation To avoid score bias in downstream tasks due to different training data, we retrain four model architectures, including Llama3 using the QCAttn algorithm, Mamba2 using the SSD algorithm, Jamba using the hybrid of QCAttn and SSD, and our architecture. We train models of two scales, 360M and 1.3B, with parameters referenced in the table7. All models are trained on the Smollm-Corpus (Ben Allal et al. 2024) dataset using the NeoX tokenizer. The training environment is the Nvidia open-source PyTorch image (NVIDIA 2022) version 24.2, which is compatible with the cuda kernel SSD algorithm in the mamba-ssm library. Training is completed using the Trainer class in the Transformers (Wolf et al. 2020) library. AdamW optimizer hyperparameters 𝛽1 = 0.9, 𝛽2 = 0.999 and 𝑤𝑒𝑖𝑔ℎ𝑡_𝑑𝑒𝑐𝑎𝑦 = 0.01. The linear warm-up steps are 10% of the total steps, reaching the maximum learning rate of 2𝑒 4, and then cosine decay to the minimum learning rate of 2𝑒 5. No bias terms. RMSNorm instead of LayerNorm. Learnable residual connections. For downstream evaluation, we use LM evaluation harness from EleutherAI (L. Gao et al. 2021), the validation dataset includes the following tasks: MMLU (Hendrycks et al. 2021) TriviaQA (Joshi et al. 2017) ARC (Clark et al. 2018) PIQA (Bisk et al. 2020) HellaSwag (Zellers et al. 2019) OBQA (Mihaylov et al. 2018) Winogrande (Sakaguchi et al. 2021) Table 7: Model Parameters. For fairness, we adjust the important parameters of these four models to be as close in size as possible, and ensure that the total parameters and activation parameters of the four models are as close as possible by adding routing mixture of experts in LlaMa3 and Mamba2, and carefully adjusting the feedforward network expansion size of LlaMa3, Mamba2, and Jamba. Finally, we obtain models of two scales, 320M and 1.3B. Model LlaMa3-320M Mamba2-320M Jamba-320M Cheems-320M LlaMa3-1.3B Mamba2-1.3B Jamba-1.3B Cheems-1.3B 𝑑𝑚𝑜𝑑𝑒𝑙 768 768 768 768 2048 2048 2048 𝑛𝑙𝑎𝑦𝑒𝑟𝑠 24 24 24 24 𝑛ℎ𝑒𝑎𝑑𝑠 12 12 12 12 24 24 24 24 32 32 32 32 𝑑𝑠𝑡𝑎𝑡𝑒 128 128 128 128 128 chunk_len 256 256 256 256 256 256 𝑛𝑣 6 16 𝑛𝑒𝑥𝑝𝑒𝑟𝑡𝑠 4 4 4 4 4 4 8192 leaning rate batch size 3e-4 3e-4 3e-4 3e-4 2e-4 2e-4 2e-4 2e-4 1M tokens 1M tokens 1M tokens 1M tokens 2M tokens 2M tokens 2M tokens 2M tokens"
        }
    ],
    "affiliations": []
}