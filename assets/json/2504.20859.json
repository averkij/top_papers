{
    "paper_title": "X-Cross: Dynamic Integration of Language Models for Cross-Domain Sequential Recommendation",
    "authors": [
        "Guy Hadad",
        "Haggai Roitman",
        "Yotam Eshel",
        "Bracha Shapira",
        "Lior Rokach"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "As new products are emerging daily, recommendation systems are required to quickly adapt to possible new domains without needing extensive retraining. This work presents ``X-Cross'' -- a novel cross-domain sequential-recommendation model that recommends products in new domains by integrating several domain-specific language models; each model is fine-tuned with low-rank adapters (LoRA). Given a recommendation prompt, operating layer by layer, X-Cross dynamically refines the representation of each source language model by integrating knowledge from all other models. These refined representations are propagated from one layer to the next, leveraging the activations from each domain adapter to ensure domain-specific nuances are preserved while enabling adaptability across domains. Using Amazon datasets for sequential recommendation, X-Cross achieves performance comparable to a model that is fine-tuned with LoRA, while using only 25% of the additional parameters. In cross-domain tasks, such as adapting from Toys domain to Tools, Electronics or Sports, X-Cross demonstrates robust performance, while requiring about 50%-75% less fine-tuning data than LoRA to make fine-tuning effective. Furthermore, X-Cross achieves significant improvement in accuracy over alternative cross-domain baselines. Overall, X-Cross enables scalable and adaptive cross-domain recommendations, reducing computational overhead and providing an efficient solution for data-constrained environments."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 9 2 ] . [ 1 9 5 8 0 2 . 4 0 5 2 : r X-Cross: Dynamic Integration of Language Models for Cross-Domain Sequential Recommendation Guy Hadad guy_hadad@hotmail.com Ben-Gurion University of the Negev Beer Sheva, Israel Haggai Roitman haggair@gmail.com Ben-Gurion University of the Negev Beer Sheva, Israel Yotam Eshel yeshel@ebay.com eBay Netanya, Israel Bracha Shapira bracha.shapira@gmail.com Ben-Gurion University of the Negev Beer Sheva, Israel Lior Rokach liorrk@bgu.ac.il Ben-Gurion University of the Negev Beer Sheva, Israel Abstract As new products are emerging daily, recommendation systems are required to quickly adapt to possible new domains without needing extensive retraining. This work presents X-Cross novel cross-domain sequential-recommendation model that recommends products in new domains by integrating several domain-specific language models; each model is fine-tuned with low-rank adapters (LoRA). Given recommendation prompt, operating layer by layer, X-Cross dynamically refines the representation of each source language model by integrating knowledge from all other models. These refined representations are propagated from one layer to the next, leveraging the activations from each domain adapter to ensure domain-specific nuances are preserved while enabling adaptability across domains. Using Amazon datasets for sequential recommendation, X-Cross achieves performance comparable to model that is fine-tuned with LoRA, while using only 25% of the additional parameters. In cross-domain tasks, such as adapting from Toys domain to Tools, Electronics or Sports, X-Cross demonstrates robust performance, while requiring about 50%-75% less fine-tuning data than LoRA to make fine-tuning effective. Furthermore, X-Cross achieves significant improvement in accuracy over alternative cross-domain baselines. Overall, X-Cross enables scalable and adaptive crossdomain recommendations, reducing computational overhead and providing an efficient solution for data-constrained environments. CCS Concepts Information systems Recommender systems; Cross-domain recommendation; Computing methodologies Natural language processing; Language models; Transfer learning. Corresponding author Haggai Roitman was still affiliated with eBay during this work. Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. Copyrights for components of this work owned by others than the author(s) must be honored. Abstracting with credit is permitted. To copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior specific permission and/or fee. Request permissions from permissions@acm.org. SIGIR 25, July 1318, 2025, Padua, Italy 2025 Copyright held by the owner/author(s). Publication rights licensed to ACM. ACM ISBN 979-8-4007-1592-1/2025/07 https://doi.org/10.1145/3726302.3730117 Keywords Cross-domain recommendation, language models, natural language processing, dynamic integration, LoRA, parameter and data efficiency ACM Reference Format: Guy Hadad, Haggai Roitman, Yotam Eshel, Bracha Shapira, and Lior Rokach. 2025. X-Cross: Dynamic Integration of Language Models for Cross-Domain Sequential Recommendation. In Proceedings of the 48th International ACM SIGIR Conference on Research and Development in Information Retrieval (SIGIR 25), July 1318, 2025, Padua, Italy. ACM, New York, NY, USA, 11 pages. https://doi.org/10.1145/3726302."
        },
        {
            "title": "1 Introduction\nThe increasing variety of products and services available across\nonline platforms, along with the rapid emergence of new domains,\nunderscores the growing need for sequential recommendation mod-\nels with fast and efficient cross-domain adaptation. Traditional\nsequential recommendation models, often tailored to specific con-\ntent areas, struggle to generalize effectively as new domains emerge\nat an unprecedented pace. As a motivating example, online market-\nplaces frequently introduce new and particular product categories,\nsuch as “vintage collectibles”, “sustainable fashion”, “personalized\nDIY kits” or “specialized home automation devices”. These emerg-\ning sub-domains challenge conventional models trained on broader\ncategories like “consumer electronics” or “home goods”, since it\nrequires nuanced understanding to capture users’ preferences in\nsuch niche areas. As large language models (LLMs) continue to\ndevelop in scope and expressive power, they offer exciting poten-\ntial for cross-domain applications by leveraging their vast world\nknowledge to quickly adapt to new domains and tasks [57]. This\ncapacity to operate across numerous and increasingly specific do-\nmains positions LLMs as powerful tools for recommendation tasks.\nHowever, effective cross-domain recommendation with LLMs\npresents significant challenges. The large size and complexity of\nstate-of-the-art language models make them resource-intensive to\ntrain even with parameter-efficient fine-tuning (PEFT) techniques.\nWhile PEFT, such as Low-Rank Adaptation (LoRA) [22], has re-\nduced the need for extensive retraining, the parameter require-\nments for adapting large models across multiple domains remains\nhigh, often surpassing the computational budgets of many real-\nworld applications. A key challenge is ensuring that cross-domain\nrecommendation tasks capture domain-specific granularity while",
            "content": "SIGIR 25, July 1318, 2025, Padua, Italy Hadad et al. remaining adaptable across domains. Current methods [3, 47] rely heavily on the activations of pre-trained weights, which may lack the capacity to generate sufficiently distinctive representations for relatively similar domains. Consequently, these approaches might not adequately capture the unique nuances of each domain. The need for adaptability in cross-domain recommendation is intensified by the data demands of LLMs. These models typically require large, domain-specific datasets to achieve optimal performance, which is not always available especially in newly emerging domains where data is scarce or when domain-nuanced recommendations are required. To the best of our knowledge, such an approach, which introduces finer-grained adaptability across the model, has not yet been explored in existing research. The goal of our work is, therefore, to leverage language models that were trained with user-item interaction histories observed in other (source) domains for sequential recommendation in new (target) domain. Hence, we wish to learn to transfer knowledge that was acquired in source domains to the new target domain; this, while using as minimum as possible observed data from the target domain for training. Moreover, we do not assume knowledge sharing of users or items between domains. Trying to address the aforementioned challenges, we introduce the X-Cross model novel dynamic integration model for crossdomain adaptability that learns to transfer knowledge from several source domain language-models to new target domain. Given set of two or more LoRA fine-tuned source domain language models, for each (recommendation prompt) input, X-Cross operates layer-by-layer and computes weights, integrating the strengths of multiple domain-specific models into refined representations. These refined representations are propagated from one layer to another, ensuring domain-specific nuances are preserved while enabling cross-domain adaptability. By leveraging activations from each source domain (LoRA) adapter, the dynamic integration mechanism creates representations that are both granular and adaptable, offering practical solution. Such layer-wise dynamic integration eliminates the need to retrain or modify the original source domain LoRA adapters, allowing our model to achieve performance comparable to newly trained LoRA adapters using just 25% of the parameters required by LoRA. Moreover, X-Cross achieves competitive and sometime even better recommendation quality to model that is fine-tuned with LoRA while using 50-75% less training data. We further demonstrate that X-Cross performs better than other alternative language-model based cross-domain recommenders, including alternatives that utilize state-of-the-art mixture-of-LoRA methods. Overall, our work contributes to developing parameter-efficient, data-friendly, and adaptable solution for cross-domain sequential recommendation, offering scalable pathway for recommendation systems in data-limited and rapidly evolving domains."
        },
        {
            "title": "2.1 Language Models for",
            "content": "Sequential-Recommendation The sequential recommendation task has been extensively studied [2], with many previous works commonly model user-item interaction sequences using Transformer-models; having sequence items represented either by their identities (IDs) (e.g., SASRec [24], BERT4Rec [41], SSE-PT [45]) or by their attributes (e.g., FDSA [56], Trans2D [40]). The rise of large language models (LLMs) in recent years has revolutionized natural language processing (NLP), and specifically their applications to recommendation systems [30, 33, 46] (LLM4Rec). For sequential recommendation tasks, language models have been utilized so far in two primary ways. Firstly, language models have been utilized for feature encoding and augmentation, providing rich context for downstream sequential recommendation pipelines [30]. Secondly, previous works have utilized the in-context learning and instruction following capabilities of LLMs to provide recommendations directly to users. To this end, given user history described in textual form (prompt), language models were instructed either to select the next item from list of candidates (e.g., P5 [14], InstructRec [54], RecRanker [36]) or generate template that represents such an item (e.g., LlamaRec [52], RecPrompt [31]) which is then used to score and rank actual items. In this work, we also utilize language model for sequential-recommendation, where we cast the task as multi-choice problem, converting its outputs into scores for ranking candidates for users next item."
        },
        {
            "title": "2.2 Cross-Domain Recommendation\nThe cross-domain recommendation task aims to transfer knowl-\nedge between domains to improve performance, particularly in\nscenarios where the target domain suffers from limited or sparse\ntraining data [53]. Language models hold significant potential for\nsuch tasks due to their vast world knowledge and advanced reason-\ning capabilities, which are essential for the generalization required\nin cross-domain recommendation settings [4, 42].",
            "content": "Among the most relevant works to ours, ZESRec [7] has applied BERT [6] to create semantic representations for zero-shot crossdomain recommendations. TransRec [12] has explored adaptertuning for transferable recommendations. UniSRec [21] has addressed cross-domain sequential recommendation by utilizing Mixture-of-Experts module to integrate the BERT representations into the recommendation task. VQ-Rec [19] has adapted textual embeddings generated by pre-trained language models by leveraging vector-quantization. RecFormer [27] has utilized language representations to model user preferences and item features, enabling effective next-item prediction, particularly in low-resource and cold-start scenarios. Compared to our work, existing models rely on fixed representations derived from the language model [19, 21], which limits their ability to fully leverage the potential of the model during finetuning for the source domain. This is due to the inherent constraints of pre-trained language models when applied to recommendation tasks [25]. In addition, utilizing only the final hidden state disregards the rich knowledge embedded within intermediate layers [43], which may contain valuable information relevant to cross-domain recommendation tasks. X-Cross: Dynamic Integration of Language Models for Cross-Domain Sequential Recommendation SIGIR 25, July 1318, 2025, Padua, Italy"
        },
        {
            "title": "3.1.1 Cross-Domain Sequential Recommendation Task. Let 𝑈 rep-\nresent the set of users, 𝐼 the set of items, and 𝑆𝑢 = (𝑖1, 𝑖2, . . . , 𝑖𝑁 )\nrepresent the (sequence) history of 𝑁 last items from 𝐼 that user\n𝑢 ∈ 𝑈 has interacted with. Given user history 𝑆𝑢 , the sequential rec-\nommendation task is to predict the next user interaction 𝑖𝑁 +1 ∈ 𝐼 .\nIn the cross-domain recommendation setting that we study in\nthis work, items in 𝐼 are assumed to belong to a new or scarcely",
            "content": "Sequential Recommendation Prompt History: 1: SE 8787MS-SP 9-in-1 Emergency Shovel Tool Kit. 2: SE CCH7-1G 7-IN-1 Green Survival Whistle. 3: Attmu 2 Pack Outdoor Survival Paracord Bracelet. 4: Garmin DeLorme Atlas & Gazetteer Paper Maps - Arizona. 5: DANTENG Phone Cable Black Candidate: Mylar Mens Emergency Thermal Blankets (10 Pack) Figure 1: Example prompt for sequential recommendation task having user history with 5 items and candidate item. represented target domain 𝐷𝑡𝑎𝑟𝑔𝑒𝑡 ; where we aim to leverage recommendation models that were trained using user-item interaction histories that were observed in other source domains D𝑠𝑜𝑢𝑟𝑐𝑒 = {𝐷1, 𝐷2, . . . , 𝐷𝑛 }. Hence, we wish to learn to transfer knowledge that was acquired in the source domains to the new target domain. Moreover, we wish to achieve such transferability assuming minimum observed data from the target domain."
        },
        {
            "title": "3.1.2\nSequential Recommendation as a Multiple-Choice Problem.\nIn this work, we use a language model to recommend the next item\nto a user 𝑢 ∈ 𝑈 , given user’s interaction history 𝑆𝑢 . To this end, we\nfirst cast the recommendation task as a multiple-choice problem.\nFor any given candidate next item 𝑖 ∈ 𝐼 , the input to the language\nmodel is simply expressed in a textual form (prompt) as follows:\nPrompt = [History: 𝑆𝑢, Candidate: 𝑖]",
            "content": "Figure 1 illustrates an example prompt with history that contains five items and candidate item. We now define 𝑠𝑐𝑜𝑟𝑒 (𝑆𝑢, 𝑖) as the model confidence that item 𝑖s text is likely to follow the textual representation of the user history 𝑆𝑢 in the prompt. Using the language-model as an encoder, in this work, we obtain such score by simply pooling the representation of the last hidden layer of the language model and apply simple scoring head over it (see details in Section 3.2.5). Given set of candidate items in 𝐼 , using such prompting (and scoring) approach, allows us to score any candidate item and select the next item as the one with the highest score."
        },
        {
            "title": "3.1.3 Model Training. Following a common methodology in train-\ning recommender-systems [39, 50, 51], we train the (language)\nmodel by sampling several negative items 𝐼𝑛𝑒𝑔 for each true item 𝑖\nwith which a given user 𝑢 has interacted. For each true candidate\nitem 𝑖 ∈ 𝐼 , the model is, therefore, trained to maximize 𝑠𝑐𝑜𝑟𝑒 (𝑆𝑢, 𝑖)\nusing the negative log-likelihood loss:",
            "content": "(cid:32) L𝑖 = log exp(𝑠𝑐𝑜𝑟𝑒 (𝑆𝑢, 𝑖)) (cid:205)𝑖 {𝑖 }𝐼𝑛𝑒𝑔 exp(𝑠𝑐𝑜𝑟𝑒 (𝑆𝑢, 𝑖)) (cid:33) . (1)"
        },
        {
            "title": "3.1.4 LoRA for Domain Specific Fine-Tuning. Low-Rank Adapta-\ntion (LoRA) [22] is a method designed to efficiently fine-tune large\npre-trained models. To this end, LoRA introduces additional train-\nable low-rank matrices into specific layers, based on the hypothesis\nthat the updates required for fine-tuning reside in a low-dimensional",
            "content": "SIGIR 25, July 1318, 2025, Padua, Italy Hadad et al. in vertical arrangement within the figure. X-Cross pools information from all source domain models by introducing dedicated integrator at each (network) layer, enabling the computation of scaling factors (weights) dynamically for each source domain. This per-layer integration design captures the varying importance of layers in recommendation tasks, where different layers contribute uniquely to relevance signals. By dynamically adjusting the scaling factors at each layer, X-Cross ensures that the integrated representation effectively balances shared and domain-specific knowledge. Our experimental results (see Section 4.5) further validate the necessity of per-layer integration, demonstrating its critical role in achieving effective cross-domain recommendations. Overall, X-Cross implementation includes four main stages. The first three stages are applied at each layer-level for each source domain, while the final stage is applied on the outputs of the last layers of all source domains. For simplicity of presentation, the detailed stages describe representation calculations that are performed over every sequence input token (i.e., we exclude the sequence-dimension). Stage 1: Concatenation of source domain representations. For 3.2.1 (𝑙 ) each source domain 𝐷𝑚 (1 𝑚 𝑛), let 𝑚 denote the output of the LoRA-enhanced encoder for that domain at layer 𝑙 (1 𝑙 𝐿) of the network, which is obtained as follows: (𝑙 ) 𝑚 = (cid:104) (𝑙 ) 𝑚 + (𝑙 ) 𝑚 (𝑙 ) 𝑚 (cid:105) (𝑙 ) 𝑚 , (3) (𝑙 ) 𝑚 R𝑑 𝑘 denotes the frozen pre-trained weights matrix where (𝑙 ) 𝑚 R𝑟 𝑘 are the of the 𝑚-th encoder at layer 𝑙; (𝑙 ) 𝑚 R𝑘 represents the LoRA weights specific to the encoder; and input to layer 𝑙. Here we note that, we assume that the fine-tuned (𝑙 ) LoRA weights (i.e., 𝑚 ) are frozen as well. (𝑙 ) 𝑚 R𝑑 𝑟 and (𝑙 ) 𝑚 and In contrast to previous models [3, 47] that have applied integration directly on activations from pre-trained weights (i.e., Wx), our approach routes the adapted activations (i.e., (W + AB)x) produced after domain-specific LoRA adapters. This adjustment addresses known limitation in recommendation tasks, where pretrained model activations often lack sufficient knowledge about recommendation-specific data [25]. By leveraging the enriched activations from LoRA adapters, X-Cross dynamically adjusts weights across representations from different domains without requiring explicit supervision for the integrator. Instead, the weights are computed adaptively for each input, guided solely by the supervision provided by the label of the sequential recommendation task in the target domain. This approach enables label-free integration of domain representations, enhancing flexibility and performance in cross-domain recommendations."
        },
        {
            "title": "At this stage we obtain the concatenated representation across",
            "content": "all 𝑛 encoders: (𝑙 ) 2 ; . . . ; (𝑙 ) concat = (𝑙 ) 1 ; where ; represents the concatenation operation along the fea- (𝑙 ) 𝑚 R𝑑 contributes to the final representature dimension. Each (𝑙 ) concat R𝑛𝑑 . tion, resulting in (𝑙 ) 𝑛 , Stage 2: Dynamic scaling. At this stage, the concatenated 3.2.2 (𝑙 ) representation concat undergoes trainable linear transformation to compute the weights for domain-specific layer representations. Figure 2: X-Cross model architecture. Each source domain language model is implemented with several Transformer (vertical) layers. On the left side: at each layer, the hottrainable integrator receives activations from the frozen layers and then passes the integrated representations to the next layer. On the right side: zoom-in into an X-Cross integrator located at one of the network layers. subspace. Therefore, instead of updating all the parameters of the pre-trained model, LoRA freezes the original model weights and introduces learnable adjustment: WLoRA = + 𝛼 AB, (2) where: R𝑑 𝑘 represents the frozen pre-trained weights, with 𝑑 denoting the input dimension and 𝑘 the output dimension. R𝑑 𝑟 and R𝑟 𝑘 are trainable low-rank matrices, where 𝑟 min(𝑑, 𝑘), representing the adaptation rank. 𝛼 controls the adaptation magnitude. In this work, for each source domain 𝐷 D𝑠𝑜𝑢𝑟𝑐𝑒 , we train unique LoRA adapter to capture domain-specific patterns."
        },
        {
            "title": "3.2 The X-Cross Model\nThe X-Cross model (hereinafter termed “X-Cross” for short) learns\nto recommend items in a new target domain by integrating multiple\npre-trained language models, with each model that was previously\nadapted with LoRA for a specific source domain. X-Cross dynam-\nically combines activations from the LoRA-enhanced “encoders”\nacross all source domains, allowing the model to leverage domain-\nspecific knowledge while adapting flexibly to the input context.",
            "content": "Figure 2 now illustrates the architecture of X-Cross. As preliminary step, we assume the availability of 𝑛 2 source domain (language) models, assuming each model is fine-tuned with its own dedicated LoRA adapter. The source models are visually represented X-Cross: Dynamic Integration of Language Models for Cross-Domain Sequential Recommendation SIGIR 25, July 1318, 2025, Padua, Italy Formally, the weights are computed as: z(𝑙 ) = (𝑙 ) concat (𝑙 ) concat , (4) (𝑙 ) concat R2𝑛 (𝑛1) (𝑛𝑑 ) is trainable weight matrix. where Therefore, the output z(𝑙 ) R2𝑛 (𝑛1) contains for each domain: 1) n-1 domain-specific scaling factors which modulate the contribution of other domains to the integrated representation for that domain; 2) n-1 interaction terms per each pair of that domain with another domain. Hence, in total, z(𝑙 ) contains 2𝑛(𝑛 1) learnable weights. These weights in z(𝑙 ) enable the model in the next stage to dynamically balance the contributions from each source domain while capturing complex inter-domain dependencies, resulting in richer and more nuanced integrated representations. Unlike conventional methods that rely on softmax to produce only positive scaling factors [47, 59], our approach supports both positive and negative scaling. This flexibility allows X-Cross to suppress irrelevant domain contributions. This enhances X-Crosss ability to focus on meaningful features for the recommendation task. By allowing negative scaling, X-Cross can effectively downscale or even move away from less relevant domains when constructing the integrated representation. This capability is particularly advantageous in scenarios where certain domain adapters introduce noise or irrelevant information, ensuring the final representation prioritizes the most relevant features. Stage 3: Representation Refinement and Integration: This stage 3.2.3 ensures that the integrated representation of each domain 𝐷𝑚 (i.e., (𝑙 ) 𝑚 ) would blend knowledge from all domains while dynamically adapting to the context of each sample. The weights z(𝑙 ) modulate both the direct contributions of domain-specific outputs and their interactions with other domains. We next detail how these weights are applied to construct the final representation of each domain. (𝑙 ) For each domain-specific representation 𝑚 , we compute an (𝑙 ) integrated refined representation, denoted as 𝑚 . This representation incorporates contributions from other domains while excluding the current domain 𝐷𝑚, and is calculated as follows: (𝑙 ) 𝑚 = (𝑙 ) 𝑚 + (cid:16) 𝑚𝑚 𝛽 (𝑙,𝑚) [𝑚 ] (𝑙 ) 𝑚 + 𝛾 (𝑙,𝑚) [𝑚,𝑚 ] (cid:16) (𝑙 ) 𝑚 (𝑙 ) 𝑚 (cid:17)(cid:17) (cid:17) (cid:16) (𝑙,𝑚) [𝑚 ] (𝑙,𝑚) [𝑚 ] (5) (𝑙 ) Here, the term 𝛽 𝑚 integrates the direct influence of other domains 𝐷𝑚 into the refined representation of domain 𝐷𝑚, where is the corresponding scaling factor for domain (𝑙,𝑚) [𝑚,𝑚 ] (𝑙 ) 𝐷𝑚 . The term 𝛾 𝑚 further captures the interdomain interactions by measuring the differences between domain representations, enriching the unified representation for the (𝑙,𝑚) cross-domain task. The term [𝑚,𝑚 ] further denotes the relative weight of the inter-domain interaction between the two domains. We note that, the summation in Eq 5 excludes domain 𝐷𝑚 itself, ensuring that the refined representation focuses on its interactions with the other 𝑛 1 domains. The scalars 𝛽 and 𝛾 are hyperparameters that control the contribution strength of the two terms. (𝑙 ) 𝑚 The residual connection allows each domain-specific representation to retain its original characteristics by avoiding incorporation of superfluous knowledge, reducing noise and preserving key domain knowledge. The adaptive integration mechanism operates iteratively at each layer 1 𝑙 𝐿, enabling the model to dynamically balance shared and domain-specific knowledge. Through this progressive refinement, the model captures complex inter-domain relationships, producing robust integrated representations. Stage 4: Final weighted summation: After processing through 3.2.4 all layers, the refined domain-specific representations are aggregated into single integrated representation. This is achieved through weighted summation of the final layer outputs from all source domain encoders: hfinal = 𝑛 𝑚=1 𝑤𝑚 (𝐿) 𝑚 , (6) (𝐿) 𝑚 is the output of the 𝑚-th encoder at the last layer 𝐿, and where 𝑤𝑚 is the learnable weight of domain 𝑚."
        },
        {
            "title": "3.2.5 Candidate scoring. Once X-Cross has encoded the input\nprompt, we utilize the integrated representation hfinal (∈ R𝑑 ) to es-\ntimate the “likelihood” that the prompt’s candidate item is the next\nitem given user’s interactions history (see again Section 3.1.2). To\nthis end, hfinal undergoes a pooling operation to extract a compact\nvector that “summarizes” the sequence information. Specifically,\nwe employ a contextual token pooling mechanism that focuses on\nthe representation of the first ([CLS]) token in the sequence. This\npooling method is consistent with the one proposed in the De-\nBERTa model [16], which is the language model we utilize in our\nexperiments (see more details in Section 4.1).",
            "content": "Formally, the pooled representation hpooled is defined as follows: hpooled = GELU(cid:0)W𝑝 hfinal [CLS] + b𝑝 (cid:1), (7) where: hfinal [CLS] R𝑑 represents the hidden state corresponding to the first token ([CLS] context token). W𝑝 R𝑑 𝑑 and b𝑝 R𝑑 are learnable parameters. GELU() is the Gaussian Error Linear Unit activation function [18], as used in the DeBERTa model [16]. Finally, we score the prompts candidate item 𝑖 as the next item in the users sequence by applying simple scoring (regression) head over the pooled representation hpooled. For that, we implement the scorer using simple linear layer, calculated as follows: 𝑇 𝑐 hpooled + 𝑏𝑐, 𝑠𝑐𝑜𝑟𝑒 (𝑆𝑢, 𝑖) = (8) where V𝑐 R𝑑 and 𝑏𝑐 are the learnable weights vector and bias term of the linear layer, respectively. To remind, 𝑠𝑐𝑜𝑟𝑒 (𝑆𝑢, 𝑖) represents the models predicted score for the given prompts candidate item 𝑖 and user history 𝑆𝑢 (we kindly refer the reader again to Section 3.1.3 for model training details). Further to remind, at training time, we train the model to score the true candidate 𝑖 along with sample of negative items 𝐼𝑛𝑒𝑔. At inference time, given recall-set of candidate items, we simply choose the next user item as the one with the highest score. SIGIR 25, July 1318, 2025, Padua, Italy Hadad et al."
        },
        {
            "title": "4.1 Experimental Setup\n4.1.1 Datasets. We curate four datasets from the Amazon reviews\ncorpus [37], focusing on the domains of Electronics, Sports,\nTools and Toys, which are commonly used for sequential recom-\nmendation tasks [24, 28, 41]. Each dataset complies with the widely\nadopted “core 5” criteria [17, 39, 44], ensuring that every user and\nitem has at least five interactions. Further following [1], to ac-\ncommodate the input constraints of the language model in our\nevaluation, we limit each user’s interaction history to a minimum\nof 5 and a maximum of 15 unique items. We further represent each\nitem within the input prompt by its title (see again Figure 1).",
            "content": "We construct user histories by randomly selecting one interaction per day from each users activity. This approach addresses significant limitation of the Amazon datasets (2018), which only provide timestamps at the day level, leading to potential inaccuracies in time-sensitive tasks like sequential recommendation [20]. By ensuring consistent chronological sequence, this approach helps to preserve temporal order across days, reducing ambiguity and improving the reliability of sequence modeling.1 Table 1: Dataset statistics. For each domain, per number of interactions we also report the data density. Except for Tools dataset, all other domain dataset numbers are reported after user sampling is applied. Target Domain # Users # Items # Interactions Source Domains Tools+Toys 12,031 165,541 (5.2e-4) Electronics 10,036 120,122 (6.2e-4) Toys+Electronics Sports 5,820 Tools 66,164 (1.0e-3) Sports+Electronics 10,457 131,485 (5.9e-4) Sports+Electronics Toys 26,319 19,244 10,962 21,342 As shown in Table 1, our datasets exhibit low density, which poses significant challenge for traditional recommendation models. However, such data sparsity allows us to test our core hypothesis, that language models that are trained in different domains can still be used to provide recommendations in other, even different, domains. To balance computational efficiency with comprehensive evaluation, except for Tools domain, which is relatively small dataset, we focus on subset of users, selecting 40% of users from 1We recognize that such an approach does not account for potential behavioral dependencies between multiple interactions within the same day, which could provide additional insights into user preferences. the Toys and Sports domains and 30% from the Electronics domain, ensuring diverse and manageable datasets for analysis. Implementation. To recall, we predict the next item using 4.1.2 multiple-choice task (see again Section 3.1.2). Accordingly, during both training and inference, for each next true item 𝑖, we sample 29 negative items (resulting with 30 choices overall for the model to choose from). Following [29, 41], we obtain negative samples using popularity-based sampling strategy. During inference, we predict the next item (out of 30) as the one with the highest score. As the backbone language model in our evaluation, we adopt the DeBERTa V3 base model [15, 16]. We make this language model choice primarily to ensure fair comparison with other crossdomain baseline models [19, 21, 27], which have used similar model capacities (e.g., BERT) in their evaluation. Additionally, we choose DeBERTa model as our base (language-model) encoder since it was also evaluated in the original LoRA paper [22]. Therefore, such choice provides consistent and robust benchmark for assessing the effectiveness of X-Cross in cross-domain recommendation tasks. To comply with DeBERTas maximum sequence length (512 tokens), we truncate each items title to maximum of 8 words. We implement X-Cross using PyTorch and train the model for 40 epochs on two NVIDIA V100 GPUs, each with 32 GB of memory. The architecture utilizes 𝑛 = 2 source domains (experts), where we use holdout-set to select the top-two source domains with best zero-shot performance on the target domain. To enhance the models efficiency and specialization, we only modify the top-9 layers of the source domain models. This decision is based on the understanding that the top layers of language models typically capture more abstract and high-level knowledge [43], making them better suited for cross-domain adaptation. Table 1 further details the source domains that we consider per each target domain. We fine-tune the model using AdamW optimizer [35], configured with learning rate of 5 105, weight decay of 0.01, and batch size of 1 to address GPU memory constraints2. We configure training hyperparameters using holdout set, setting 𝛽 = 0.5 and 𝛾 = 0.4 for optimal performance. Additionally, for LoRA finetuning, we employ rank 𝑟 = 16 and scaling factor 𝛼 = 32, ensuring efficient parameter utilization while maintaining strong task performance [22]."
        },
        {
            "title": "4.1.3 Baselines. We compare X-Cross performance to several types\nof baselines. We first evaluate several state-of-the-art “traditional”\nsingle-domain sequential recommendation models, namely: SAS-\nRec [24] – a widely adopted self-attentive (causal) sequential recom-\nmendation model; BERT4Rec [41] – a bidirectional transformer-\nbased model which uses a masked learning approach; FDSA [56],\nwhich leverages self-attentive blocks to model item and feature\ntransition patterns; S3-Rec [58], which pre-trains the model to\nmaximize mutual information for enhanced feature fusion; and\nlastly, LLM-Rec [42], a language model that represents both user\nhistory and the next item within a shared embedding space. While\nthis model is generally designed for multi-domain settings, here we\napply it to a single-domain setting due to the lack of overlapping\nusers across domains. For this evaluation, we specifically use an",
            "content": "2We note that, each batch basically contains 30 prompts used to implement the models multiple-choice task. X-Cross: Dynamic Integration of Language Models for Cross-Domain Sequential Recommendation SIGIR 25, July 1318, 2025, Padua, Italy encoder-only language model because it achieves the best results. Additionally, to ensure fair comparison, we use the same encoder architecture (DeBERTa) as in our approach. As first-line of baselines for cross-domain recommendation, we fine-tune domain-specific LoRA model [22] for each source domain. Here we fine-tune each model independently on its respective (source) domain and evaluate it in zero-shot setting on other domains. These set of baselines allows to assess the potential of transferability among different domains. We next evaluate three state-of-the-art cross-domain sequentialrecommendation models, namely: UniSRec, VQ-Rec and RecFormer. All these models require pretraining. For fair comparison, we pretrain these models on the exact source domains that we use for X-Cross and then fine-tune them on the target domain. UniSRec [21] equips textual item representations with mixture-of-experts (MoE)-enhanced adapter for domain fusion and adaptation; leveraging item-sequence and sequence-sequence contrastive learning tasks to pre-train transferable sequence representations. VQ-Rec [19] learns vector-quantized item representations for transferable sequential recommendation [19], enabling effective representations across domains. Finally, RecFormer [27] leverages language representations to model user preferences and item features, enabling effective next-item prediction, particularly in low-resource and cold-start scenarios. As two competitive alternative baselines that also focus on integrating domain-specific knowledge, we consider XLoRA [3] and MeteoRA [47]. The XLoRA baseline incorporates core techniques from the XLoRA framework [3] into our setting. Specifically, we use embeddings from the last layer of the pre-trained model to scale all layer experts, leveraging cross-layer attention to improve generalization across domains. Further adapting the MeteoRA framework [47], we implement gating network for each layer. These networks dynamically integrate adapters by using embeddings from the pre-trained model as inputs, ensuring more effective integration of domain-specific knowledge. Finally, we implement Pooler + Scorer baseline, which simplifies the architecture by fine-tuning only the pooler and scorer layers of the best-performing pre-trained model. We use this baseline to examine the necessity of integrating multiple domain-specific components."
        },
        {
            "title": "4.2 Overall Performance\nWe analyze the overall performance of X-Cross compared to the\ndifferent types of baselines. First, examining the result of single-\ndomain baselines, the LLM-Rec baseline demonstrates a significant\nperformance improvement over the others. This showcase the bene-\nfit of leveraging embeddings derived from all item titles in the user’s",
            "content": "history. This holistic representation of user history enables LLMRec to outperform models that treat items text independently. Next, as we can observe, those baselines that we fine-tune with LoRA using multiple-choice task on the target domain, perform even better than LLM-Rec. Furthermore, the cross-domain potential becomes evident, as the baselines that we evaluate using zero-shot setting also outperform most single-domain baselines. Next, except for LLM-Rec, the cross-domain baselines achieve significantly better results than their single-domain counterparts (with RecFormer as the top performer). While these cross-domain baselines may not match LoRA fine-tunings results for singledomain tasks, this does not necessarily reflect poorly on their crossdomain capabilities. Instead, it highlights the strength of holistic user history representation, as seen in both LLM-Rec and LoRA fine-tuning and also the potential of cross-domain recommendation among single-domain baselines. We next examine the performance of the domain integration baselines, where we observe mixed performance both among themselves and compared to other baselines. Specifically, in some cases, these baselines perform worse than the Pooler + Scorer baseline, which fine-tunes the best source domains pooler and scorer. However, these integration approaches still provide strong baseline, particularly when considering their efficiency. Finally, we first compare X-Cross side-by-side with those baselines that require new LoRA adapter. Even though X-Cross requires significantly fewer parameters compared to training new LoRA adapter, on Sports and Toys domains it almost reach the same accuracy of these baselines, while for Tools and Electronics domains it even outperforms these baselines. Furthermore, compared to alternative integrator-baselines, X-Cross exceed their performance by large margin. All in all, these results serve as strong evidence that X-Cross serves as both efficient and effective solution for cross-domain sequential recommendation tasks. Figure 3: Accuracy (Hit@1) comparison across datasets for X-Cross and LoRA. The dashed red-line denotes the performance of the reference model. SIGIR 25, July 1318, 2025, Padua, Italy Hadad et al. Table 2: Overall performance of X-Cross and baselines. Bold values denote the best performer. Superscripts 𝑠 and 𝑖 and subscript 𝑐 denote (statistical) significant difference (𝑝 0.05) with the best single-domain, integrators and the best cross-domain baselines, respectively. Model Tools Sports Toys Electronics Hit@1 Hit@3 Hit@10 MRR Hit@1 Hit@3 Hit@10 MRR Hit@1 Hit@3 Hit@10 MRR Hit@1 Hit@3 Hit@10 MRR Single Domain Recommendation SASRec BERT4Rec FDSA 𝑆 3-Rec LLM-Rec 6.38 4.79 5.43 5.15 14. 18.74 Tools-LoRA 12.40 Sports-LoRA Toys-LoRA 11.26 Electronics-LoRA 12.40 14.23 11.13 12.59 11.67 35.70 39.26 25.58 22.16 26.32 37.07 33.33 36.84 35.02 74.14 13.26 11.02 12.19 11.56 30.14 9.79 7.85 8.21 8.68 22. 22.34 18.08 20.21 20.21 47.96 51.96 45.54 49.52 50.58 82.49 19.99 16.56 18.02 18.46 39.38 8.36 6.11 7.14 6.75 23.45 17.92 14.71 16.37 16.44 49.87 41.46 38.86 44.25 41.51 83. LoRA Fine-Tuning on Source Domain / Zero-shot on Target Domain 74.87 55.95 48.34 50.39 33.69 22.74 20.06 21.59 13.30 27.98 14.52 15.82 28.11 51.49 27.54 30.32 58.35 82.62 55.55 59. 24.64 43.49 24.71 26.74 13.91 14.10 27.99 14.34 26.40 25.49 52.02 25.84 53.99 49.50 84.77 52.85 16.13 13.52 15.55 14.72 40.87 23.79 22.92 44.05 23. 44.76 16.19 6.34 39.17 12.96 5.07 43.01 16.00 6.04 5.62 43.69 14.67 25.57 51.99 85.03 8.85 8.49 8.98 25.56 20.11 17.63 18.96 51.77 45.55 44.03 45.21 84.08 15.05 12.59 14.53 13.98 42.82 17.75 16.48 17.47 42. UniSRec VQ-Rec RecFormer 7.80 6.84 12.80 16.96 15.55 26.83 42.64 41.04 54.65 15.95 14.42 23.35 13.07 10.81 18. 27.49 24.06 36.82 58.92 55.18 69.15 24.39 21.58 31.83 10.03 8.03 16.74 19.98 17.36 30.31 46.10 43.15 57. 18.46 16.08 26.96 7.66 7.09 14.19 17.21 16.36 29.20 46.33 43.84 60.08 16.28 15.34 25.59 Cross-domain Recommendation Domain Integration XLoRA MeteoRA Pooler + Scorer X-Cross (ours) 18.01 17.74 14.09 20.11i,s 36.21 36.53 31.19 40.36i,s 69.95 70.27 61.70 76.06i,s 31.44 31.26 26.48 34.87i,s 24.45 21.38 22.97 28.40 i,s 44.89 41.62 42.56 50.09i,s 77.01 72.04 74.69 82.05i 38.68 35.10 36.74 43.36 i,s 23.28 22.30 20.73 28.23i,s 45.07 42.96 39.73 52.66 i,s 79.90 76.76 73.23 85.66i,s 38.66 37.00 34.45 44.65i,s 15.14 17.65 15.69 26.25i 32.43 37.10 33.91 50.72i 66.66 72.38 68.18 84.71i 28.23 31.91 29.07 42.84i Table 3: Models performance under training data limitations. Target Domain Tools Sports Toys Electronics Significant for < Samples 1000 100 300 300 X-Cross to Exceed Baseline 200 75 100 50 LoRA to Exceed Baseline 500 200 300 300 Gap (%) 60.0% 62.5% 66.7% 83.3%"
        },
        {
            "title": "4.3 Efficiency on Limited Training Data\nWe next wish to evaluate X-Cross performance under limited train-\ning data settings. To this end, for each target domain, we com-\npare X-Cross side-by-side with a model that is fine-tuned with\nLoRA. Here, we remind that, using 𝑛 = 2 source domains, X-Cross\nrequires only 25% of the parameters of LoRA3. Yet, as we shall\nshortly demonstrate, despite this modest decrease in parameter\nsize, X-Cross achieves competitive results against LoRA in terms of\nperformance, underscoring its effectiveness.",
            "content": "We evaluate both models (X-Cross and LoRA) using training datasets of varying sizes: {50, 75, 100, 200, 300, 400, 500, 750, 1000}. For each training dataset size, we randomly sample five distinct subsets of the specified size from the full dataset. We train both models separately on each of these subsets. As reference model for minimum performance requirement, for each target domain, we consider the source model with best zero-shot performance which is trained on one of the other (source) domains (e.g., for Tools, the reference model is LoRA-fine-tuned either on the Sports, Toys or Electronics domain). 3Per each layer 𝑙, X-Cross learns the weights matrix W(𝑙 ) concat which for 𝑛 = 2 translates to 4*2*768 total parameters; whereas LoRA learns two weight matrices A(𝑙 ) and B(𝑙 ) , together having 2*16*768 total parameters. We describe our results in Figure 3, where we evaluate all three models on the full test-set to identify the minimum amount of training data required for either LoRA or X-Cross to surpass the performance of the reference model (further represented by the red dashed line). We further summarize the training data requirements across domains of both models in Table 3. The first column indicates the maximum number of samples (𝑀) in the training set for which X-Cross performance remains statistically significantly better than LoRA. The second and third columns represent the minimum number of training samples required for X-Cross and the standard LoRA adapter to exceed the reference model, respectively. The final column, Gap (%), shows the percentage reduction in the amount of training data required by X-Cross compared to LoRA. Further note that, the values in Table 3 represent the smallest sampled training dataset sizes at which the performance threshold is exceeded. If the threshold is not surpassed at given size (e.g., 100) but is at the next increment (e.g., 200), the latter value (200) is recorded in the table to reflect this milestone in the sampling process. Overall, these empirical results indicate that X-Cross attains steeper initial learning curve, reaching to above-reference performance with substantially fewer samples."
        },
        {
            "title": "4.4 Impact of Number of Model Layers\nWe further investigate the contribution of individual layers within\nX-Cross. To this end, we vary the number of layers integrated into\nthe model. Specifically, we evaluate its performance when scaling\nand integrating either 1, 2, 4, or 8 layers during the first three stages,\nstarting from the topmost layer and down. The results in Figure 4\nshow that, while adding layers generally improves recommenda-\ntion accuracy, the performance gains are not uniform. Some layers",
            "content": "X-Cross: Dynamic Integration of Language Models for Cross-Domain Sequential Recommendation SIGIR 25, July 1318, 2025, Padua, Italy Table 4: Ablation study results: Impact of removing key components on performance. Variant Tools Sports Toys Electronics Hit@1 Hit@3 Hit@10 MRR Hit@1 Hit@3 Hit@10 MRR Hit@1 Hit@3 Hit@10 MRR Hit@1 Hit@3 Hit@10 MRR 20.11 X-Cross - Layers 16.60 - Interactions 17.46 19.33 - Experts 40.36 35.11 38.49 39.67 76.06 34.87 28.40 68.99 30.20 24.24 75.01 32.62 26.73 73.78 33.68 27. 50.09 44.58 49.55 49.75 82.05 43.36 28.23 76.57 38.53 22.96 81.84 42.10 26.10 81.55 42.62 25.81 52.66 45.26 50.97 49.03 85.66 44.65 26.25 81.73 38.73 15.67 83.35 42.51 23.02 83.98 41.80 26.06 50.72 33.33 46.12 50.06 84.71 42.84 69.47 29.15 80.72 38.95 82.88 42. Figure 4: Accuracy (Hit@1) vs number of layers. capture more domain-relevant knowledge and significantly impact performance, while others contribute less."
        },
        {
            "title": "4.5 Ablation Study\nWe next perform an ablation study to evaluate the relative contri-\nbutions of the key components of X-Cross. For that, each time, we\nsystematically alter the full implementation of X-Cross by remov-\ning a specific component and measuring its impact on performance.\nWe summarize the results of the ablation study in Table 4. First, by\nsetting both 𝛽 = 0 and 𝛾 = 0 (in Eq. 5) we remove the dynamic inte-\ngration mechanism between source domains at each layer, allowing\neach one to operate independently and combining their outputs\nonly at the final stage (Eq. 6). As we can observe, this simplifica-\ntion (denoted “-Layers” in Table 4) which eliminates layer-wise\ncollaboration, leads to a significant (and largest) performance drop,\nemphasizing the critical role of dynamic inter-layer integration.",
            "content": "Next, we exclude the interactions component during the integration process by setting 𝛾 = 0 while still keeping 𝛽 = 0.5 (denoted -Interactions in Table 4). This modification, which disables the (𝑙 ) 𝑚 ), reduces the models ability to capinteraction terms (h ture nuanced inter-domain relationships, resulting in notable performance decline. (𝑙 ) 𝑚 Lastly, we eliminate the contributions of other source domains entirely by setting 𝛽 = 0 while still keeping 𝛾 = 0.4 (denoted - Experts in Table 4). This modification leaves the domain-specific model and the interactions of the models active for each input. As we can observe, removing this functionality from X-Cross results in significant performance drop as well, which attests again to the importance of considering the contributions of other domains."
        },
        {
            "title": "4.6 Domain Convergence Analysis\nWe conclude this section by investigating why some domains con-\nverge faster than others during training. Referring back to Figure\n3, using five random samples of 1000 training examples from each\ndomain’s dataset, we observe significant differences in convergence\nperformance, with an average accuracy of 21.82, 19.42, 16.40 and",
            "content": "14.87 for Sports, Toys, Electronics and Tools domains, respectively. Despite all domains being trained on identical sample sizes, the disparities in performance are pronounced. Interestingly, the bottom two domains, Tools and Electronics, represent extremes in dataset size with Tools being the smallest and Electronics the largest. This contradiction prompted us to further investigate which possible factors are influencing domain convergence, such as inherent dataset characteristics or domain-specific complexities. Our initial attempts to explain these differences using classic features of recommendation datasets, such as the number of unique users and items, dataset density, average interactions per user, or per-item distributions, appear to be inconclusive. Similarly, analyzing the characteristics of pre-trained embeddings of individual items or of the entire history of the user offer no clear explanation. However, we do observe that the embedding cosine similarities within domains are exceptionally high, averaging 0.95, suggesting significant homogeneity in these domains representations. This drives us even further to investigate prompt-specific features. More specifically, we focus on two main prompt properties: 1. prompt length (measured by mean length) and 2. prompt diversity (measured as standard deviation of the length). To this end, we train simple linear regression model with the dependent parameter as the models accuracy and the regressors are both prompt length and prompt diversity. Overall, the two prompt properties explain 86% of the variance of the models accuracy, demonstrating strong relationship between prompt quality and performance. Moreover, based on the regression coefficients, prompt length exhibit negative relationship (-0.65) with model accuracy, whereas prompt diversity has positive one (1.60). This suggests that fine-tuning the model with shorter and more diverse prompts results in better accuracy."
        },
        {
            "title": "5 Conclusion\nWe have introduced X-Cross, a novel cross-domain sequential rec-\nommendation model that dynamically integrates multiple language\nmodels at both the layer and input (sample) levels. X-Cross achieves\nsuperior performance compared to state-of-the-art single-domain\nand cross-domain baselines while remaining highly efficient, out-\nperforming parameter-efficient fine-tuning (PEFT) methods like\nLoRA, particularly in limited training scenarios. Moreover, X-Cross\nintroduces a new approach to integrating LoRA adapters across\ndomains, yielding improvements in cross-domain sequential rec-\nommendation tasks that highlight its adaptability and robustness.\nAs future work, we wish to explore the relationship between\nsource and target domains to better understand how domain selec-\ntion impacts performance, paving the way for further improvements\nin cross-domain recommendation systems.",
            "content": "SIGIR 25, July 1318, 2025, Padua, Italy Hadad et al. References [1] Keqin Bao, Jizhi Zhang, Yang Zhang, Wenjie Wang, Fuli Feng, and Xiangnan He. 2023. Tallrec: An effective and efficient tuning framework to align large language model with recommendation. In Proceedings of the 17th ACM Conference on Recommender Systems. 10071014. [2] Tesfaye Fenta Boka, Zhendong Niu, and Rama Bastola Neupane. 2024. survey of sequential recommendation systems: Techniques, evaluation, and future directions. Information Systems 125 (2024), 102427. https://doi.org/10.1016/j.is.2024. 102427 [3] Eric Buehler and Markus Buehler. 2024. X-LoRA: Mixture of low-rank adapter experts, flexible framework for large language models with applications in protein mechanics and molecular design. APL Machine Learning 2, 2 (2024). [4] Jin Chen, Zheng Liu, Xu Huang, Chenwang Wu, Qi Liu, Gangwei Jiang, Yuanhao Pu, Yuxuan Lei, Xiaolong Chen, Xingmei Wang, et al. 2024. When large language models meet personalization: Perspectives of challenges and opportunities. World Wide Web 27, 4 (2024), 42. [5] Tim Dettmers, Artidoro Pagnoni, Ari Holtzman, and Luke Zettlemoyer. 2024. Qlora: Efficient finetuning of quantized llms. Advances in Neural Information Processing Systems 36 (2024). [6] Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. 2019. BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding. In Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers). 41714186. [7] Hao Ding, Yifei Ma, Anoop Deoras, Yuyang Wang, and Hao Wang. 2021. Zeroshot recommender systems. arXiv preprint arXiv:2105.08318 (2021). [8] Ning Ding, Yujia Qin, Guang Yang, Fuchao Wei, Zonghan Yang, Yusheng Su, Shengding Hu, Yulin Chen, Chi-Min Chan, Weize Chen, et al. 2023. Parameterefficient fine-tuning of large-scale pre-trained language models. Nature Machine Intelligence 5, 3 (2023), 220235. [9] William Fedus, Jeff Dean, and Barret Zoph. 2022. review of sparse expert models in deep learning. arXiv preprint arXiv:2209.01667 (2022). [10] William Fedus, Barret Zoph, and Noam Shazeer. 2022. Switch transformers: Scaling to trillion parameter models with simple and efficient sparsity. Journal of Machine Learning Research 23, 120 (2022), 139. [11] Wenfeng Feng, Chuzhan Hao, Yuewei Zhang, Yu Han, and Hao Wang. 2024. Mixture-of-loras: An efficient multitask tuning for large language models. arXiv preprint arXiv:2403.03432 (2024). [12] Junchen Fu, Fajie Yuan, Yu Song, Zheng Yuan, Mingyue Cheng, Shenghui Cheng, Jiaqi Zhang, Jie Wang, and Yunzhu Pan. 2024. Exploring adapter-based transfer learning for recommender systems: Empirical studies and practical insights. In Proceedings of the 17th ACM International Conference on Web Search and Data Mining. 208217. [13] Zihao Fu, Haoran Yang, Anthony Man-Cho So, Wai Lam, Lidong Bing, and Nigel Collier. 2023. On the effectiveness of parameter-efficient fine-tuning. In Proceedings of the AAAI conference on artificial intelligence, Vol. 37. 1279912807. [14] Shijie Geng, Shuchang Liu, Zuohui Fu, Yingqiang Ge, and Yongfeng Zhang. 2023. Recommendation as Language Processing (RLP): Unified Pretrain, Personalized Prompt & Predict Paradigm (P5). arXiv:2203.13366 [cs.IR] https://arxiv.org/abs/ 2203.13366 [15] Pengcheng He, Jianfeng Gao, and Weizhu Chen. 2021. DeBERTaV3: Improving DeBERTa using ELECTRA-Style Pre-Training with Gradient-Disentangled Embedding Sharing. arXiv:2111.09543 [cs.CL] [16] Pengcheng He, Xiaodong Liu, Jianfeng Gao, and Weizhu Chen. 2021. DEBERTA: DECODING-ENHANCED BERT WITH DISENTANGLED ATTENTION. In International Conference on Learning Representations. https://openreview.net/forum? id=XPZIaotutsD [17] Ruining He, Wang-Cheng Kang, and Julian McAuley. 2017. Translation-based recommendation. In Proceedings of the eleventh ACM conference on recommender systems. 161169. [18] Dan Hendrycks and Kevin Gimpel. 2016. Gaussian error linear units (gelus). arXiv preprint arXiv:1606.08415 (2016). [19] Yupeng Hou, Zhankui He, Julian McAuley, and Wayne Xin Zhao. 2023. Learning vector-quantized item representation for transferable sequential recommenders. In Proceedings of the ACM Web Conference 2023. 11621171. [20] Yupeng Hou, Jiacheng Li, Zhankui He, An Yan, Xiusi Chen, and Julian McAuley. 2024. Bridging language and items for retrieval and recommendation. arXiv preprint arXiv:2403.03952 (2024). [21] Yupeng Hou, Shanlei Mu, Wayne Xin Zhao, Yaliang Li, Bolin Ding, and Ji-Rong Wen. 2022. Towards universal sequence representation learning for recommender systems. In Proceedings of the 28th ACM SIGKDD Conference on Knowledge Discovery and Data Mining. 585593. [22] Edward Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean Wang, Lu Wang, and Weizhu Chen. 2022. LoRA: Low-Rank Adaptation of Large Language Models. In International Conference on Learning Representations. https: //openreview.net/forum?id=nZeVKeeFYf9 [23] Damjan Kalajdzievski. 2023. Rank Stabilization Scaling Factor for Fine-Tuning with LoRA. arXiv e-prints (2023), arXiv2312. [24] Wang-Cheng Kang and Julian McAuley. 2018. Self-attentive sequential recommendation. In 2018 IEEE international conference on data mining (ICDM). IEEE, 197206. [25] Wang-Cheng Kang, Jianmo Ni, Nikhil Mehta, Maheswaran Sathiamoorthy, Lichan Hong, Ed Chi, and Derek Zhiyuan Cheng. 2023. Do llms understand user preferences? evaluating llms on user rating prediction. arXiv preprint arXiv:2305.06474 (2023). [26] Sein Kim, Hongseok Kang, Seungyoon Choi, Donghyun Kim, Minchul Yang, and Chanyoung Park. 2024. Large language models meet collaborative filtering: An efficient all-round llm-based recommender system. In Proceedings of the 30th ACM SIGKDD Conference on Knowledge Discovery and Data Mining. 13951406. [27] Jiacheng Li, Ming Wang, Jin Li, Jinmiao Fu, Xin Shen, Jingbo Shang, and Julian McAuley. 2023. Text is all you need: Learning language representations for sequential recommendation. In Proceedings of the 29th ACM SIGKDD Conference on Knowledge Discovery and Data Mining. 12581267. [28] Jiacheng Li, Yujie Wang, and Julian McAuley. 2020. Time interval aware selfattention for sequential recommendation. In Proceedings of the 13th international conference on web search and data mining. 322330. [29] Defu Lian, Qi Liu, and Enhong Chen. 2020. Personalized ranking with importance sampling. In Proceedings of The Web Conference 2020. 10931103. [30] Jianghao Lin, Xinyi Dai, Yunjia Xi, Weiwen Liu, Bo Chen, Hao Zhang, Yong Liu, Chuhan Wu, Xiangyang Li, Chenxu Zhu, Huifeng Guo, Yong Yu, Ruiming Tang, and Weinan Zhang. 2024. How Can Recommender Systems Benefit from Large Language Models: Survey. ACM Trans. Inf. Syst. (July 2024). https: //doi.org/10.1145/3678004 [31] Dairui Liu, Boming Yang, Honghui Du, Derek Greene, Neil Hurley, Aonghus Lawlor, Ruihai Dong, and Irene Li. 2024. RecPrompt: Self-tuning Prompting Framework for News Recommendation Using Large Language Models. In Proceedings of the 33rd ACM International Conference on Information and Knowledge Management (CIKM 24). ACM, 39023906. https://doi.org/10.1145/3627673.3679987 [32] Haotian Liu, Chunyuan Li, Qingyang Wu, and Yong Jae Lee. 2024. Visual instruction tuning. Advances in neural information processing systems 36 (2024). [33] Peng Liu, Lemei Zhang, and Jon Atle Gulla. 2023. Pre-train, Prompt, and Recommendation: Comprehensive Survey of Language Modeling Paradigm Adaptations in Recommender Systems. Transactions of the Association for Computational Linguistics 11 (2023), 15531571. [34] Shih-yang Liu, Chien-Yi Wang, Hongxu Yin, Pavlo Molchanov, Yu-Chiang Frank Wang, Kwang-Ting Cheng, and Min-Hung Chen. 2024. DoRA: WeightDecomposed Low-Rank Adaptation. In Forty-first International Conference on Machine Learning. [35] Loshchilov. 2017. Decoupled weight decay regularization. arXiv preprint arXiv:1711.05101 (2017). [36] Sichun Luo, Bowei He, Haohan Zhao, Wei Shao, Yanlin Qi, Yinya Huang, Aojun Zhou, Yuxuan Yao, Zongpeng Li, Yuanzhang Xiao, Mingjie Zhan, and Linqi Song. 2024. RecRanker: Instruction Tuning Large Language Model as Ranker for Top-k Recommendation. arXiv:2312.16018 [cs.IR] https://arxiv.org/abs/2312.16018 [37] Julian McAuley, Christopher Targett, Qinfeng Shi, and Anton Van Den Hengel. 2015. Image-based recommendations on styles and substitutes. In Proceedings of the 38th international ACM SIGIR conference on research and development in information retrieval. 4352. [38] Akshara Prabhakar, Yuanzhi Li, Karthik Narasimhan, Sham Kakade, Eran Malach, and Samy Jelassi. 2024. LoRA Soups: Merging LoRAs for Practical Skill Composition Tasks. arXiv e-prints (2024), arXiv2410. [39] Xubin Ren, Wei Wei, Lianghao Xia, Lixin Su, Suqi Cheng, Junfeng Wang, Dawei Yin, and Chao Huang. 2024. Representation learning with large language models for recommendation. In Proceedings of the ACM on Web Conference 2024. 3464 3475. [40] Uriel Singer, Haggai Roitman, Yotam Eshel, Alexander Nus, Ido Guy, Or Levi, Idan Hasson, and Eliyahu Kiperwasser. 2022. Sequential Modeling with Multiple Attributes for Watchlist Recommendation in E-Commerce. In Proceedings of the Fifteenth ACM International Conference on Web Search and Data Mining (Virtual Event, AZ, USA) (WSDM 22). Association for Computing Machinery, New York, NY, USA, 937946. https://doi.org/10.1145/3488560. [41] Fei Sun, Jun Liu, Jian Wu, Changhua Pei, Xiao Lin, Wenwu Ou, and Peng Jiang. 2019. BERT4Rec: Sequential recommendation with bidirectional encoder representations from transformer. In Proceedings of the 28th ACM international conference on information and knowledge management. 14411450. [42] Zuoli Tang, Zhaoxin Huan, Zihao Li, Xiaolu Zhang, Jun Hu, Chilin Fu, Jun Zhou, and Chenliang Li. 2023. One model for all: Large language models are domainagnostic recommendation systems. arXiv preprint arXiv:2310.14304 (2023). [43] Ian Tenney, Dipanjan Das, and Ellie Pavlick. 2019. BERT Rediscovers the Classical NLP Pipeline. In Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics. 45934601. [44] Xiang Wang, Xiangnan He, Meng Wang, Fuli Feng, and Tat-Seng Chua. 2019. Neural graph collaborative filtering. In Proceedings of the 42nd international ACM SIGIR conference on Research and development in Information Retrieval. 165174. [45] Liwei Wu, Shuqing Li, Cho-Jui Hsieh, and James Sharpnack. 2020. SSE-PT: Sequential recommendation via personalized transformer. In Proceedings of the X-Cross: Dynamic Integration of Language Models for Cross-Domain Sequential Recommendation SIGIR 25, July 1318, 2025, Padua, Italy 14th ACM conference on recommender systems. 328337. [46] Likang Wu, Zhi Zheng, Zhaopeng Qiu, Hao Wang, Hongchao Gu, Tingjia Shen, Chuan Qin, Chen Zhu, Hengshu Zhu, Qi Liu, et al. 2024. survey on large language models for recommendation. World Wide Web 27, 5 (2024), 60. [47] Jingwei Xu, Junyu Lai, and Yunpeng Huang. 2024. MeteoRA: Multiple-tasks Embedded LoRA for Large Language Models. arXiv e-prints (2024), arXiv2405. [48] Prateek Yadav, Derek Tam, Leshem Choshen, Colin Raffel, and Mohit Bansal. 2023. Resolving interference when merging models. arXiv preprint arXiv:2306.01708 1 (2023). [49] Prateek Yadav, Derek Tam, Leshem Choshen, Colin Raffel, and Mohit Bansal. 2024. Ties-merging: Resolving interference when merging models. Advances in Neural Information Processing Systems 36 (2024). [50] Fajie Yuan, Xiangnan He, Alexandros Karatzoglou, and Liguang Zhang. 2020. Parameter-efficient transfer from sequential behaviors for user modeling and recommendation. In Proceedings of the 43rd International ACM SIGIR conference on research and development in Information Retrieval. 14691478. [51] Fajie Yuan, Guoxiao Zhang, Alexandros Karatzoglou, Joemon Jose, Beibei Kong, and Yudong Li. 2021. One person, one model, one world: Learning continual user representation without forgetting. In Proceedings of the 44th International ACM SIGIR Conference on Research and Development in Information Retrieval. 696705. [52] Zhenrui Yue, Sara Rabhi, Gabriel de Souza Pereira Moreira, Dong Wang, and Even Oldridge. 2023. LlamaRec: Two-Stage Recommendation using Large Language Models for Ranking. arXiv:2311.02089 [cs.IR] https://arxiv.org/abs/2311.02089 [53] Tianzi Zang, Yanmin Zhu, Haobing Liu, Ruohan Zhang, and Jiadi Yu. 2022. survey on cross-domain recommendation: taxonomies, methods, and future directions. ACM Transactions on Information Systems 41, 2 (2022), 139. [54] Junjie Zhang, Ruobing Xie, Yupeng Hou, Wayne Xin Zhao, Leyu Lin, and JiRong Wen. 2023. Recommendation as Instruction Following: Large Language Model Empowered Recommendation Approach. arXiv:2305.07001 [cs.IR] https: //arxiv.org/abs/2305. [55] Lvmin Zhang, Anyi Rao, and Maneesh Agrawala. 2023. Adding conditional control to text-to-image diffusion models. In Proceedings of the IEEE/CVF International Conference on Computer Vision. 38363847. [56] Tingting Zhang, Pengpeng Zhao, Yanchi Liu, Victor Sheng, Jiajie Xu, Deqing Wang, Guanfeng Liu, Xiaofang Zhou, et al. 2019. Feature-level deeper selfattention network for sequential recommendation.. In IJCAI. 43204326. [57] Wayne Xin Zhao, Kun Zhou, Junyi Li, Tianyi Tang, Xiaolei Wang, Yupeng Hou, Yingqian Min, Beichen Zhang, Junjie Zhang, Zican Dong, et al. 2023. survey of large language models. arXiv preprint arXiv:2303.18223 (2023). [58] Kun Zhou, Hui Wang, Wayne Xin Zhao, Yutao Zhu, Sirui Wang, Fuzheng Zhang, Zhongyuan Wang, and Ji-Rong Wen. 2020. S3-rec: Self-supervised learning for sequential recommendation with mutual information maximization. In Proceedings of the 29th ACM international conference on information & knowledge management. 18931902. [59] Yanqi Zhou, Tao Lei, Hanxiao Liu, Nan Du, Yanping Huang, Vincent Zhao, Andrew Dai, Quoc Le, James Laudon, et al. 2022. Mixture-of-experts with expert choice routing. Advances in Neural Information Processing Systems 35 (2022), 71037114."
        }
    ],
    "affiliations": [
        "Ben-Gurion University of the Negev Beer Sheva, Israel",
        "eBay Netanya, Israel"
    ]
}