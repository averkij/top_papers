{
    "paper_title": "From FLOPs to Footprints: The Resource Cost of Artificial Intelligence",
    "authors": [
        "Sophia Falk",
        "Nicholas Kluge Corrêa",
        "Sasha Luccioni",
        "Lisa Biber-Freudenberger",
        "Aimee van Wynsberghe"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "As computational demands continue to rise, assessing the environmental footprint of AI requires moving beyond energy and water consumption to include the material demands of specialized hardware. This study quantifies the material footprint of AI training by linking computational workloads to physical hardware needs. The elemental composition of the Nvidia A100 SXM 40 GB graphics processing unit (GPU) was analyzed using inductively coupled plasma optical emission spectroscopy, which identified 32 elements. The results show that AI hardware consists of about 90% heavy metals and only trace amounts of precious metals. The elements copper, iron, tin, silicon, and nickel dominate the GPU composition by mass. In a multi-step methodology, we integrate these measurements with computational throughput per GPU across varying lifespans, accounting for the computational requirements of training specific AI models at different training efficiency regimes. Scenario-based analyses reveal that, depending on Model FLOPs Utilization (MFU) and hardware lifespan, training GPT-4 requires between 1,174 and 8,800 A100 GPUs, corresponding to the extraction and eventual disposal of up to 7 tons of toxic elements. Combined software and hardware optimization strategies can reduce material demands: increasing MFU from 20% to 60% lowers GPU requirements by 67%, while extending lifespan from 1 to 3 years yields comparable savings; implementing both measures together reduces GPU needs by up to 93%. Our findings highlight that incremental performance gains, such as those observed between GPT-3.5 and GPT-4, come at disproportionately high material costs. The study underscores the necessity of incorporating material resource considerations into discussions of AI scalability, emphasizing that future progress in AI must align with principles of resource efficiency and environmental responsibility."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 3 ] . [ 1 2 4 1 4 0 . 2 1 5 2 : r From FLOPs to Footprints: The Resource Cost of Artificial Intelligence Sophia Falk1*, Nicholas Kluge Corr ˆea2, Sasha Luccioni3, Lisa Biber-Freudenberger4, and Aimee van Wynsberghe1 1Sustainable AI Lab, Institute for Science and Ethics, Bonn University, Germany *corresponding author: falk@iwe.uni-bonn.de 2Center for Science and Thought, Bonn University, Germany 3Hugging Face 4Center for Development Research, Bonn University, Germany"
        },
        {
            "title": "ABSTRACT",
            "content": "As computational demands continue to rise, assessing the environmental footprint of artificial intelligence (AI) requires moving beyond energy and water consumption to include the material demands of specialized hardware. This study quantifies the material footprint of AI training by linking computational workloads to physical hardware needs. The elemental composition of the Nvidia A100 SXM 40 GB graphics processing unit (GPU) was analyzed using inductively coupled plasma optical emission spectroscopy (ICP-OES), which identified 32 elements. The results show that AI hardware consists of approximately 90% heavy metals and only trace amounts of precious metals. The elements copper, iron, tin, silicon, and nickel dominate the GPU composition by mass. In multi-step methodology, we integrate these measurements with computational throughput per GPU across varying lifespans, accounting for the computational requirements of training specific AI models at different training efficiency regimes. Scenario-based analyses reveal that, depending on Model FLOPs Utilization (MFU) and hardware lifespan, training GPT-4 requires between 1,174 and 8,800 A100 GPUs, corresponding to the extraction and eventual disposal of up to 7 tons of toxic elements. Combined software and hardware optimization strategies can substantially reduce material demands: increasing MFU from 20% to 60% lowers GPU requirements by 67%, while extending lifespan from one to three years yields comparable savings; implementing both measures together reduces GPU needs by up to 93%. Our findings highlight that incremental performance gains, such as those observed between GPT-3.5 and GPT-4, come at disproportionately high material costs. The study underscores the necessity of incorporating material resource considerations into discussions of AI scalability, emphasizing that future progress in AI must align with principles of resource efficiency and environmental responsibility. Keywords: Sustainable AI, Resource Cost, AI computation, AI model training, FLOPs"
        },
        {
            "title": "1 Introduction",
            "content": "Newspaper headlines such as The world is running out of resources for IT [1] and Global shortage in computer chips reaches crisis point [2] reflect growing concern regarding the material constraints of the Fourth Industrial Revolution. At the center of this crisis is the semiconductor shortage, rooted in disruptions within the silicon supply chain. The previously significant semiconductor shortage occurred in 2021 during the COVID pandemic [2, 3], and now surge in demand for hardware accelerators for deep learning workloads could lead to the next global chip shortage [3]. It is important to note that the infrastructure powering emerging technologies, including artificial intelligence (AI)1, is built on far more than just silicon. Data centers, which form the physical backbone of emerging technologies, rely on wide array of increasingly scarce and valuable materials, including rare earth elements, tantalum, and cobalt [4]. While there is an increasing awareness of the environmental implications and supply chain constraints tied to the extraction of critical resources for green technologies and the energy sector, e.g. lithium for electric vehicles [5] or cobalt for renewable energy storage systems [6, 7], there is lack of equivalent scrutiny when it comes to the material demands of AI. As AI becomes defining force of the ongoing Fourth Industrial Revolution, it is essential to identify and understand the AI materials that constitute the infrastructure and materiality of AI. This becomes more urgent as global investments in digital infrastructure continue to accelerate. The rapid expansion of AI capabilities has driven unprecedented demand for high-performance computing, prompting substantial capital flows into 1By AI, here we refer specifically to large-scale deep learning systems, such as those used for generative AI applications like chatbots. 1 data center construction and modernization. In 2024 alone, Alphabet, Microsoft, Meta, and Amazon collectively invested approximately $246 USD billion in AI and data center development [8]. Projections indicate that this trajectory will continue: according to McKinsey, AI-ready data center capacity is expected to grow at an average annual rate of 33% between 2023 and 2030, with AI workloads accounting for nearly 70% of total data center demand by the end of the decade [9]. Despite this rapid expansion and increasing electricity demand, data centers accounted for only about 1.5% of global electricity consumption in 2024 [10], representing comparatively modest share within the energy sector. Although the accelerating energy use associated with AI development and use is not negligible, other major drivers, such as electric vehicles, air conditioning, and electricity-intensive manufacturing, at present have greater impact on overall global electricity consumption [11, 12]. Data centers current relatively contained global share is largely attributable to continuous improvements in energy efficiency, including advancements in cooling, workload management, and server performance. Nevertheless, the future trajectory of AIs energy demand, driven by its rapid development, ongoing trend of expanding model sizes, increasing adoption, and potential rebound effects remains uncertain and necessitate careful attention. Efficiency improvements are closely intertwined with increased accessibility. Advances in model optimization have not only made AI systems more computationally efficient but also more widely deployable across diverse hardware environments, lowering the barrier to entry for AI use. Specific techniques, such as knowledge distillation2 and quantization3, reduce model size and resource demands, improving both efficiency and deployability [13]. However, these gains in efficiency and accessibility have simultaneously fueled broader AI adoption, ultimately reinforcing the demand for more infrastructure and, by extension, more hardware and material inputs [15]. This is not new phenomenon. As Jevons Paradox, first articulated in 1866, demonstrated, increased efficiency often paradoxically leads to greater consumption instead of resource savings [16]. The assessment of AIs growing resource demands primarily focuses on its energy and water consumption. Energy analyses highlight the intensive computational requirements of training and deploying large-scale models [17, 18], along with the substantial electricity demand of data center cooling systems[19]. These findings push current debates on net-zero pathways for AI, energy provision challenges, and decarbonization of electricity grids [20]. Consequently, factors such as the geographic distribution of data centers, global trajectories of AI-related energy consumption, and energy infrastructure constraints play central role in shaping the long-term carbon footprint of AI operations [21]. More recently, water consumption has also gained attention as critical dimension of AIs environmental footprint [22]. However, the two dimensions of carbon and water alone do not capture the full scope of AIs environmental footprint. As the demand for computational power continues to rise, more comprehensive understanding of the physical footprint must also encompass AIs specialized hardware, hence including the materials that enable its operation. The Nvidia A100 Graphics Processing Unit (GPU), for instance, is known to have played transformative role in AI development [23]. Following its release in 2020, the A100 quickly became cornerstone of data center infrastructures at companies like OpenAI, Google, Microsoft, and Meta [23, 24]. This was due to the A100s substantially improved performance, which has revolutionized AI model training and deployment. The performance of GPU can be measured by the number of floating-point operations it can perform per second (FLOPs). FLOP is single mathematical calculation involving decimal numbers, such as addition or multiplication. FLOPs per second measure how many of these calculations processor can perform, making it standard metric for describing the speed and capacity of GPUs, as well as for estimating the computational cost of training AI models. Capable of delivering up to 312 teraFLOPs (in BF16 precision) for deep learning workloads, the A100 provided significant performance increase over its predecessor, the Volta series, with an approximately threefold increase in FP16 and sixfold in FP32 [25]. Beyond measuring GPU performance, FLOPs have emerged in regulatory contexts. The EU AI Act, for example, classifies high-risk AI models based on training compute budget, specifically defining general-purpose AI (GPAI) models as presenting systemic risks if their cumulative training exceeds 1025 FLOPs [26]. However, FLOPs go beyond computational intensity; they can also serve as proxy for the material footprint of AI training. More FLOPs typically require more parameters, data, and training iterations, and consequently, greater hardware utilization. To bridge the gap between digital computation metrics and physical resource consumption, we use the results from Falk et al. [27] life cycle assessments (LCA) background data, which presents the elemental composition of AI hardware using inductively coupled plasma optical emission spectroscopy (ICP-OES) on the Nvidia A100 SXM 40GB GPU to qualify and quantify the elements embedded in this critical infrastructure component. Second, we develop framework linking computational demands (measured in FLOPs) to material consumption. This is achieved by mapping model training requirements to the Nvidia A100s operational capacity over its lifespan. Thus, this study aims to establish direct connection between the resource requirements of AI models and model performance metrics. Our findings provide empirical evidence to inform technology policy and sustainable AI development toward 2The compression of large AI models into smaller versions that maintain high performance while significantly reducing computational requirements [13]. 3The reduction of parameter and computation precision (e.g., from 32-bit floating point to 8-bit integers), decreasing memory usage and increasing inference speed while maintaining accuracy [14]. 2/18 resource-conscious direction. By focusing on GPUs, critical component at the core of any AI infrastructure, we reveal new insights into the underexplored link between the computational demands of AI models and the material extraction and waste generation they entail at the hardware level."
        },
        {
            "title": "2 Method",
            "content": "The methodological framework of this study combines imported empirical data from prior physical component analysis with newly developed approach for translating computational demand (in FLOPs) into estimates of material resource consumption. Specifically, we draw on results from our previous work, which involved the systematic disassembly and documentation of an Nvidia A100 SXM 40GB GPU, followed by ICP-OES analysis to quantify its elemental composition. Building on that established physical characterization, the present study introduces method for estimating the maximum computational output single Nvidia A100 GPU can deliver over different operational lifetimes. These computational capacity estimates are compared with the FLOP requirements of training selected AI models under various training-efficiency assumptions. From this comparison, we derive multiple scenarios for the number of GPUs needed to train given model. Finally, these GPU-scaling scenarios are combined with the imported material composition dataset, enabling scenario-based assessment of the material demands associated with training large-scale AI systems."
        },
        {
            "title": "2.1 Elemental Analysis Data\nThe Nvidia A100 SXM 40GB GPU is disassembled into five component groups - casing, heatsink, printed circuit board (PCB),\nPower-on-Package (PoP), and GPU chip (VRAM and GPU die) (see Figure 1) - and subsequently subjected to elemental\ncharacterization via ICP-OES. While the elemental composition dataset originates from the same empirical analysis reported in\nFalk et al. (2025) [27], that earlier work focused primarily on integrating these data into a LCA. In this section, we shift the\nemphasis from the LCA lens to the analytical procedures and results of the elemental analysis, providing methodological detail\nand interpretation not included in the prior publication.",
            "content": "First, the sample preparation involves manual grinding to achieve particle sizes below 10 cm, followed by pyrolysis at 500C for 2 hours to remove plastic components. The resulting residue is sequentially ground to obtain final particle sizes of 20-200 µm. Random quartering between grinding steps reduces sample quantities to 1.2 per mineralization, with samples dried at 100C for 24 hours before analysis. Three distinct mineralization procedures are employed using an ANTON PAAR multiwave pro mineralizer to ensure comprehensive metal detection. The first procedure utilizes an oxidizing medium (H2O2 + HNO3) optimized for silver quantification. The second employs two-step process combining aqua regia with hydrogen peroxide, followed by nitric acid treatment. The third procedure used tetrafluoroboric acid and nitric acid to detect difficult-to-analyze elements, including silicon, tantalum, and gallium. All mineralizations are conducted at 240C and 60 bar pressure over 1 hour (10 minutes temperature rise, 50 minutes plateau). Post-mineralization solutions are centrifuged at 4000 rpm for 7 minutes and analyzed using Agilent ICP-OES 5100. Analysis employed both qualitative IntelliQuant screening (detection error <10%) and quantitative methods using calibration curves with minimum of three reference wavelengths per element to ensure accurate elemental composition determination. Figure 1. Annotation of the Nvidia A100 SXM 40 GB (left picture). For the chemical analysis conducted via ICP-OES, the GPU was disassembled and categorized into four main component categories: the printed circuit board (PCB) (top center), the heatsink (bottom center), the main GPU chip (top right), and the current regulators Power-on-Package (PoP) assemblies (bottom right) (author pictures). 2.2 GPU Throughput and AI model Demands To link GPU throughput to the demands of training given AI model, we assume sequential training model in which the AI model is trained exclusively on single GPU until the device reaches the end of its operational life, at which point the training is transferred to the following GPU. While this sequential training approach does not reflect real-world parallel training practices due to time and efficiency limitations, it provides straightforward framework for estimating the total number of GPU hardware requirements based on the cumulative compute necessary to train an AI model. The core calculation compares the GPUs throughput (i.e., the deliverable computational capacity) over its lifespan with the training compute demand of given AI model. Two key variables highly influence the outcome of this comparison: (1) the 3/18 assumed operational lifespan of the GPU and (2) the model training efficiency, defined as the proportion of delivered compute that effectively contributes to model training. Both factors are examined in detail in the following sections. 2.2.1 Nvidia A100: Computational Throughput Technical documentation and industry analyses provide insights into the lifespan of data center GPUs. The physical durability of GPUs, such as the A100, is affected by component wear, workload intensity, maintenance, and cooling design [28]. Anecdotal reports suggest that GPUs may last up to 5 years under heavy use and up to 7 years under moderate use [29, 30]. However, empirical studies and technical reports indicate that the typical operational lifespan of GPUs in industry data centers is relatively short, around 3 years [28, 31]. In 2024, Meta report on training the LLaMa 3 (405B) model found that GPU-related hardware issues accounted for significant portion of unexpected training interruptions. Specifically, 58.7% of all such incidents were linked to GPU failure, with faulty units alone accounting for 30.1% of disruptions. Other common issues included memory or thermal failure [28]. The report characterizes GPU failure as any event, hardware - or memory -related, that disrupts normal operations and requires human or automated intervention, such as GPU repair or replacement, to resume training [28]. These frequent hardware failures imply that individual components have relatively short mean time between failures in intensive training environments [28]. Continuous, high-utilization workloads, especially in large-scale parallel training, accelerate GPU degradation. Supporting this, Ostrouchov et al. [31] conducted long-term study on cluster of 18,688 Nvidia GPUs over nearly seven years. They found that the average time to hardware failure requiring intervention, such as replacement, is clustered around 2.8 years per GPU, reinforcing the relatively short lifespan of GPUs in demanding environments [31]. An often-cited statement by an anonymous AI architect at Google echoes this view, stating that the maximum lifespan of data center GPUs at current utilization levels is 3 years. More specifically, given continuous high utilization rates, the expected lifespan of GPU typically falls between 1 and 2 years, rarely exceeding 3 years [32, 33]. While improved maintenance and lower workload intensity can extend GPU lifespans, doing so may introduce additional costs and operational constraints that cloud providers are often unwilling to bear, since it effectively limits the amount of compute they can offer over time. Consequently, high GPU utilization remains the norm to maximize performance [31] and return on investment [33]. Therefore, for our estimations, we provide results for lifespans of 1, 2, and 3 years, which we believe are realistic for this type of hardware. Building on these lifespan assumptions, we next quantify the GPUs computational capacity over time. By assessing the A100 GPUs FLOPs performance using bfloat16 (BF16) precision and projecting this across different lifespan scenarios, we can estimate the total computational throughput the GPU is capable of delivering over its lifetime (see Table 1). The peak BF16 throughput of the A100 (both 40 GB and 80GB models) is 312 teraFLOPs per second4 [29, 34]. Based on continuous annual operation, the total yearly computational throughput can be calculated as: Annual Computational Throughput = (312 1012) (365 24 60 60) = 9.8 1021 FLOPs (1) Following Equation 1; at 100% utilization over one year, this corresponds to 9.8 1021 FLOPs annually. Table 1. Total amount of maximum theoretical throughput expressed in FLOPs processed per A100 (BF16) over its operational lifespan, assuming three lifespan scenarios. Hardware lifespan (years) Theoretical Computational Throughput 1 2 9.8 1021 1.9 1022 2.9x 1022 2.2.2 AI Model Training: Computational Requirements For this study, we examine the computational requirements of models trained using Nvidia A100 GPUs specifically (see Table 2). Since not all models report computational requirements in terms of FLOPs, we estimate them from available model specifications. For dense transformer architectures, widely used heuristic, proposed by Kaplan et al. (2020), assumes approximately 6 FLOPs per parameter per token during training [35]. The corresponding computational budget is expressed as: Compute Budget = 6 (2) Where is the number of model parameters and the number of training tokens. All AI models considered in this paper, except GPT-4, are or strongly presumed to be dense transformer models. 4Higher FLOPs are achieved only for sparsity, which is usually not applicable for most use cases involving deep learning workloads. 4/18 For sparse architectures, such as Mixture-of-Experts (MoE) models, only subset of parameters is activated per forward pass[36]. To estimate the compute budget for MoEs, Equation 2 remains valid; however, only the active parameters are considered for N. The architecture of GPT-4 remains officially undisclosed. OpenAIs GPT-4 technical report states that the model is Transformerbased but provides no further details on its size, number of parameters, or architectural configuration [37]. Nevertheless, multiple technical analyses strongly suggest that GPT-4 employs MoE architecture rather than dense Transformer design. This hypothesis is supported by two observations: First, inference latency is significantly lower than would be expected for dense model of comparable capacity[38, 39]. And second, the computational and financial costs of training and inference would be prohibitively high if GPT-4 were fully dense [40]. Several reports propose different MoE configurations for GPT-4. One such report suggests that GPT-4 consists of eight experts, each with approximately 220 billion parameters, with two experts active per forward pass, totaling about 1.76 trillion parameters [41]. Another analysis proposes 16 experts, each containing 111 billion parameters, with two experts active [42]. To account for uncertainty, we evaluate GPT-4 under multiple activation scenarios, similar to the MoE architectures of Ling1T, DeepSeek-V3, and Llama 4 Maverick. This scenario-based approach enables the estimation of GPT-4s computational and material footprints across plausible range of active-parameter configurations. These architectural assumptions serve as the foundation for estimating the hardware requirements associated with model training. Building on the computed training demands for both dense and sparse architectures, we next relate each models total computational budget to the operational throughput and lifespan of individual GPUs to determine the number of units effectively required for training: Required GPUs = Compute Budget Annual Computational Throughput Lifespan (3) Where the lifespan represents the expected productive duration in high-utilization data center environments. These estimates assume continuous 24/7 operation at peak theoretical performance, i.e., 100% model FLOPs utilization (MFU), which represents an idealized upper bound. In practice, more complex training setups (e.g., distributed multi-dimensional parallelism across multiple hosts), introduce latency, communication overheads, and input-output (I/O) bottlenecks that reduce effective utilization [4346]. 2.2.3 Efficiency Adjustments for Real-World Training Scenarios Empirical studies indicate that real-world MFU values typically range from 20% to 60% of peak throughput [47, 48]. For instance, Meta reported 38 - 41% during the training of LLaMa 3 (405B) [49], and 53% for LLaMa 2 [50]. At the same time, OpenAI achieved 19.6% MFU for pre-training GPT-3 and approximately 34% MFU for GPT-4, and Google reported 46.2% for PaLM [47]. MFU values above 50% are generally regarded as indicative of highly optimized training [43]. However, MFU is not consistently disclosed across models and can vary considerably depending on data center infrastructure, interconnect topology, networking bandwidth, storage I/O systems, software stack configurations, and operational practices [43, 51]. To account for this variability and the lack of standardized MFU reporting, we present GPU requirements across representative MFU range of 20% to 50% for the three hardware lifespan scenarios (1, 2, and 3 years). Note that lower MFU does not imply proportionally lower power consumption. Technical reports show that GPUs often operate at or near maximum power draw even at 20%-50% MFU [47, 52]. Since power draw and associated thermal dissipation are key drivers of component degradation and catastrophic failure [53, 54], reduced MFU does not extend GPU lifespan. To account for the difference between theoretical and real-world GPU usage, we apply scaling factor to compensate for training inefficiencies: Scaling factor = 1 MFU where MFU [0.20, 0.60] The adjusted GPU count is then computed as: GPUadjusted = GPUrequired Scaling factor (4) (5) These scenarios provide comprehensive assessment of potential hardware requirements under varying operational conditions. The estimates remain conservative, as they consider only training time, MFU, and lifespan, excluding additional delays or training interruptions that would further increase resource use. 5/18 Table 2. Overview of large-scale dense Transformer models published between 2022 and 2024 that were trained on Nvidia A100 SXM hardware. The estimated number of FLOPs per model is derived from reported or inferred parameter and token counts. Where official data on parameters or token counts is unavailable, italicized values indicate estimates based on industry conventions and credible leaked data sources. Model Parameter Token (N) (D) Amazon Titan TG-1 Mistral Large 2 LLama 2 DeepSeekLLM Bloom GPT-3.5 Falcon Phytia"
        },
        {
            "title": "200 B\n123 B\n70 B\n67 B\n176 B\n175 B\n40 B\n12 B",
            "content": "4 trillion 2 trillion 2 trillion 2 trillion 366 billion 300 billion 1 trillion 300 billion FLOPs Compute Budget 4.8 1024 1.48 1024 8.4 1023 8.04 1023 3.86 1023 3.15 1023 2.4 1023 2.16 1023 2.3 Material Footprint of AI Model Training In the final step of the analysis, the material composition of single GPU, determined through ICP-OES, is scaled by the number of GPUs required to train each AI model. The total material demand for each element is expressed as: Melement = melement GPUadjusted(n) (6) Where melement is the mass of specific element detected in the analyzed GPU, and GPUadjusted(n) corresponds to the adjusted GPU requirement accounting for real-world MFU and hardware lifespan. Multiplying these terms yields the total elemental mass required for training given AI model. This computation is repeated for all 32 detected elements across various training scenarios to capture the uncertainty range in hardware utilization and lifespan assumptions. For models with reported MFU data, material requirements are reported for the nearest MFU increment combined with the 1-3 year lifespan range. Models lacking publicly available MFU values are assessed across the entire parameter range to ensure comprehensive evaluation of their potential resource intensity."
        },
        {
            "title": "3 Results",
            "content": "This section presents the computational requirements for training state-of-the-art AI models on the Nvidia A100 SXM 40GB GPU. While AI training typically occurs in large-scale data center environments supporting diverse digital workloads, this analysis isolates the material requirements directly attributable to GPU-based AI training. It is important to note that the results here reflect only the GPU unit. Broader infrastructure components, such as networking, storage, and cooling systems, are excluded from this analysis. Including these hardware components would substantially increase the total material footprint, and the values reported here should therefore be interpreted as conservative lower-bound estimates of AIs overall material demand. 3.1 GPU requirement per AI model Using Equation 2, we derive the computational budgets of eight large-scale dense Transformer models developed between 2022 and 2024, trained on Nvidia A100 GPUs (see Table 2). For GPT-4, we separately estimate range of plausible computational budget scenarios based on MoE architectural assumptions (see Table 3). As previously explained, for our estimations, training is assumed to run sequentially, with each GPU used continuously until it reaches the end of its productive life, at which point the following GPU takes over. This lets us directly estimate how many GPUs are effectively used during training and, thus, the material demand involved. While it is also possible to estimate the proportional wear and tear across all GPUs used in parallel, the sequential modeling approach more effectively demonstrates the scale and intensity of hardware turnover implicit in contemporary AI training workloads. 6/18 Table 3. The estimated GPT-4 MoE configurations are based on publicly discussed architectural hypotheses and disclosed MoE architectures assuming training dataset of 13 trillion tokens. The computational budget is estimated using parameter activation ratios informed by the MoE architectures of Ling-1T, DeepSeek-V3, and Llama 4 Maverick. For the remainder of the study, we use the scenario laid out by SemiAnalysis as conservative approach. Scenario"
        },
        {
            "title": "George Hotz\nSemiAnalysis",
            "content": "Ling-1T-like DeepSeek-V3-like Llama 4 Maverick-like Total Active Parameter (N) Parameter (N) Token (D) 1.76T 1.76T 1.76T 1.76T 1.76T 440B 222B 50B 37B 17B 13 trillion 13 trillion 13 trillion 13 trillion 13 trillion FLOPs Compute Budget 3.43 1025 1.73 1025 3.90 1024 2.89 1024 1.33 Table 4. Total GPU requirements for training selected AI models under varying hardware lifespan (13 years) and model FLOPs utilization (MFU) scenarios (20% lower bound, 50% upper bound). Values are rounded up to the nearest integer. Model FLOPs Lifespan = 1 Lifespan = 2 Lifespan = MFU 20% MFU 50% MFU 20% MFU 50% MFU 20% MFU 50% GPT-4 Amazon Titan Mistral Large 2 LLama 2 1.73 1025 4.8 1024 1.48 1024 8.4 1023 DeepSeekLLM 8.04 1023 3.86 1023 3.15 1023 2.4 1023 2.16 1023 BLOOM GPT-3.5 Falcon Pythia 8,800 2,439 751 427 409 196 160 122 11 3,520 976 301 171 164 78 64 49 4,400 1,220 377 214 205 99 80 61 6 1,760 488 151 85.38 82 40 32 25 3 2,934 814 251 143 137 66 54 41 4 1,174 326 151 57 55 27 22 17 2 Using Equation 5, we estimate the number of Nvidia A100 GPUs required to train each AI model based on three life span scenarios and applying lower bound MFU of 20% and an upper bound estimate of 50% MFU (see Table 4). GPU requirements scale with model complexity, i.e., compute budget. Training GPT-4 (based on the MoE SemiAnalysis scenario) with an MFU of 20% and hardware lifespan of 1 year requires 8,800 A100 GPUs, decreasing to approximately 2,934 GPUs with hardware lifespan of 3 years. Improving the MFU to 50% reduces the range from 3,520 GPUs (1-year lifespan) to 1,174 GPUs (3-year lifespan). GPT-4 requires the highest number of GPUs, followed by Amazon Titan (2,439 to 326 GPUs), Mistral Large 2 (752 to 151 GPUs), LLaMa 2 (427 to 57 GPUs), DeepSeekLLM (409 to 55 GPUs), BLOOM (196 to 27 GPUs), GPT-3.5 (160 to 22 GPUs), and Falcon (122 to 17 GPUs). Pythia demonstrates the lowest requirements (11 to 2 GPUs), illustrating the substantial variation in computational demands across different model sizes. 3.2 Elemental Composition of the Nvidia A100 GPU The ICP-OES elemental analysis of the GPU identifies the presence of 32 elements from the periodic table (see Figure 2). About 84% of the present elements are metals, 12.5% are metalloids, and one is nonmetal. Four of the eight precious metals - gold, silver, platinum, and palladium - were detected, albeit primarily in trace amounts. Silver is the most abundant among them, with 0.55 grams per GPU. Consequently, the economic value of precious metals per individual GPU is relatively limited. The quantities of elements vary widely, with copper being the most abundant at 1,374 grams, while beryllium registers as the least abundant at just 0.0000237829 grams (see Table 5). The most dominant elements by mass are copper, iron, tin, silicon, and nickel. Beyond the quantitative elemental composition, the relative distribution of 32 elements across the four individual GPU components is examined. The elemental composition differs considerably across the GPU components, specifically the heatsink, PCB, GPU chip, and Power-on-Packages (PoP) (see Figure 3). 7/18 Figure 2. Proportion of elements in the Nvidia A100 SXM 40GB GPU (author illustration). Figure 3 shows the ranking of elements according to their total mass contribution across all components. The top 15 elements are visualized by their proportional contributions within each component. Copper and iron constitute substantial portions of the GPUs structural components, primarily in the heatsink and PCB. In contrast, silicon and nickel are predominantly concentrated in the functional components, namely the GPU chip and PoP. The heatsink, the most substantial component by mass, is composed of 98.1% copper, reflecting its primary role in thermal management. In contrast, the PCB exhibits more heterogeneous elemental profile, comprising 46.5% copper and 28% iron, along with smaller proportions of silicon, tin, and calcium. The internal computing structure, the GPU chip, and PoP are characterized by higher elemental complexity. The PoP is composed of 52.6% copper, 19% iron, 6.6% magnesium, with notable amounts of barium and zinc. The GPU chip primarily comprises 41% chromium, 29% silicon, 17% tin, and smaller portions of bismuth and aluminum. The substantial amount of silicon within the GPU chip is primarily attributable to the large die size of the main GPU processor. Our analysis of the dismantled unit reveals that approximately 1,353 mm2 of silicon area is integrated within single 55 mm 55 mm, 12-layer packed GPU inside the Nvidia A100 SXM. In comparison, its predecessor, the Nvidia Tesla V100, features significantly smaller die area of 815 mm2 [55]. To meet the computational demands of large-scale AI training, semiconductor devices are increasingly scaling their physical dimensions, either through larger die areas or advanced packaging that integrates multiple smaller dies. These trends lead to increased silicon consumption as AI models continue to grow. However, constraints such as the maximum reticle area, manufacturing costs, cooling challenges, and diminishing functional die yield impose practical limits on die scaling, phenomenon often referred to as area-wall [56]. Among these limitations, thermal management emerges as particularly critical challenge as chip size increases [56], underscoring the need for enhanced cooling solutions and the consideration of waste heat utilization [57]. Hence, we suggest that meeting the computational demands of increasingly large AI models will necessitate scaling via additional GPU units rather than fewer, enhanced individual chip capabilities. This Figure 3. Elemental composition of the Nvidia A100 SXM GPU by component group: heatsink, PCB, main GPU (GPU chip + VRAM), and Power-on-Packages. Displayed are the top 15 elements by weight proportion (% of total mass) (author illustration). 8/18 shift carries significant implications for material consumption. As AI models continue to grow, the full spectrum of resources required for manufacturing complete GPU units will be extracted, rather than primarily increasing silicon consumption by enlarging individual, more powerful chips. Table 5. Elemental composition of the Nvidia A100 SXM 40 GB GPU by component (heatsink, PCB, GPU chip, and PoP) and in total (mass in grams). Elements marked with (*) are either inherently toxic or present significant health hazards through exposure to their vapors and dusts during extraction and processing. Abbr. Heatsink Element 4.32E-01 Ag Silver 1.89E+00 Al Aluminum 2.69E-02 As Arsenic (*) 2.96E-02 Au Gold 2.77E-01 Boron 7.27E-02 Ba Barium 1.35E-03 Be Beryllium (*) 1.38E+00 Bi Bismuth 9.28E-01 Ca Calcium 2.69E-03 Cd Cadmium (*) 4.04E-03 Cobalt (*) Co 6.19E-02 Chromium (*) Cr 1.30E+03 Cu Copper (*) 1.58E+00 Fe Iron 2.69E-03 Potassium 2.69E-03 Li Lithium 9.83E-02 Mg Magnesium 8.62E-02 Mn Manganese 2.49E-01 Na Sodium 8.55E+00 Ni Nickel (*) 1.27E-01 Pb Lead (*) 4.85E-02 Pd Palladium 4.98E-02 Pt Platinum 6.60E-02 Sb Antimony (*) 6.06E-02 Se Selenium 0.00E+00 Si Silicon 9.20E+00 Sn Tin 8.08E-02 Ti Titanium 1.75E-02 Tl Thallium (*) 8.08E-03 Vanadium 3.25E-01 Zn Zinc (*) 5.39E-03 Sr Strontium PCB GPU chip 1.13E-02 4.18E-01 2.87E-04 1.15E-03 6.04E-02 7.24E-02 2.22E-02 6.47E-01 2.21E-05 5.75E-04 9.72E-04 5.25E+00 8.14E-02 1.75E-03 8.84E-05 6.99E-02 9.22E-03 4.40E-03 1.62E-02 8.17E-02 4.71E-03 9.50E-04 8.40E-04 9.50E-04 5.30E-04 3.70E+00 2.17E+00 6.42E-02 5.08E-04 2.65E-04 2.42E-02 1.02E-01 4.33E+00 5.92E-03 1.01E-02 4.30E-01 3.47E+00 2.19E-04 7.43E-01 4.65E+00 1.10E-03 5.70E-03 3.43E-01 6.87E+01 4.17E+01 2.87E-02 4.39E-04 1.86E-01 7.00E-02 3.88E-01 2.05E+00 5.00E-01 1.49E-01 5.49E-03 1.73E-02 3.16E-02 9.37E+00 8.50E+00 1.21E+00 4.17E-03 3.29E-03 6.69E-01 PoP 7.85E-03 2.65E-01 3.75E-04 1.71E-03 1.47E-02 4.31E-01 1.39E-05 3.18E-02 2.02E-01 6.95E-05 5.82E-03 1.33E-03 5.99E+00 2.20E+00 3.06E-04 5.56E-05 1.78E-02 7.57E-01 3.73E-02 3.76E-01 2.43E-02 1.86E-03 4.17E-04 7.78E-04 9.31E-04 3.27E-01 3.98E-01 1.39E-01 6.12E-04 5.84E-04 1.60E-01 Total 0.553 6.90E+00 3.35E-02 4.26E-02 7.82E-01 4.05E+00 2.38E-02 2.80E+00 5.78E+00 4.43E-03 1.65E-02 5.66E+00 1.37E+03 4.55E+01 3.18E-02 7.31E-02 3.11E-01 9.18E-01 6.91E-01 1.11E+01 6.56E-01 2.00E-01 5.65E-02 8.50E-02 9.37E-02 1.34E+01 2.03E+01 1.49E+00 2.28E-02 1.22E-02 1.18E+00 5.39E3.3 The Resource Cost of AI training Training GPT-4 with reported MFU of around 35% [58] requires the computational capacity of approximately 5,029 A100 GPUs over one-year hardware lifespan (see Figure 4). These computational demands translate directly into material impacts: one training round of this single model requires the extraction and eventual disposal of an estimated 7,003 kg of toxic materials. Electronic components contain numerous heavy metals, including arsenic (As), mercury (Hg), lead (Pb), cadmium (Cd), chromium (Cr), zinc (Zn), copper (Cu), nickel (Ni), antimony (Sb), cobalt (Co) and beryllium (Be), that pose acute ecological and health risks if released during mining, manufacturing or disposal at the end of their life [5961]. Many of these metals are classified as carcinogenic or highly toxic. Exposure through inhalation, dermal contact, or ingestion of contaminated water can lead to lung cancer, neurological impairment, gastrointestinal disorders, and other long-term health impacts [59]. Toxic metals pose health hazard in mining at varying concentrations depending on the metal; lead, for example, is dangerous at extremely low parts-per-billion (ppb) levels [61]. In developing countries, untreated or inadequately treated industrial wastewater and mining represent primary sources of metal pollution in freshwater systems[62]. Our elemental analysis indicates that all of these toxic metals are present in the A100 GPU, accounting for around 93% of 9/18 Figure 4. Estimated hardware and elemental requirement for training GPT-4 at 35% MFU across varying hardware lifespan scenarios (1 to 3 years). Results are expressed in terms of the total number of GPUs required and the total elemental mass (kg) on logarithmic scale. Extending the hardwares operational lifespan to 2 years halves the GPU demand to 2,515 GPUs, while 3-year lifespan reduces requirements by approximately 67% to 1,676 GPUs (authors illustration). the devices total mass. The dominant metal is copper (91%), followed by nickel (0.74%) and chromium (0.38%). While the concentrations may appear modest at the level of single unit, large-scale AI training requires thousands of GPUs, thereby magnifying the pressures of upstream extraction and the risks of downstream contamination across soil, air, and groundwater systems. In many African mining regions, concentrations of toxic metals in the soil and water substantially exceed WHO drinking water thresholds posing risks to communities [63] (e.g. levels are above 10 µg/L for As and Pb, 3 µg/L for Cd, 20 µg/L for Cr and 2000 µg/L for Cu [61].) Assuming an MFU of 20% and 1-year GPU lifespan, training the nine models listed in Table 4 requires 13,315 GPUs. This corresponds to material footprint of 19,940 kg extracted resources, with toxic materials comprising about 18,545 kg. Under more optimal conditions of 3-year lifespan and 50% MFU, material extraction decreases to about 2,740 kg, of which 2,550 kg remain toxic. However, these nine models represent only fraction of the aggregate AI industry resource consumption. To quantify sector-wide resource consumption and compare with other industries, annual A100 GPU shipment data is required. However, this information is not publicly disclosed by Nvidia. Consequently, current assessments remain constrained to training-run-level analyses rather than comprehensive industry-scale evaluations. These findings demonstrate that the environmental impact of large-scale AI model training extends beyond operational energy and carbon emissions. The material intensity of hardware production, including the extraction, processing, and disposal of toxic elements, constitutes significant yet frequently overlooked aspect of AI sustainability [27]. Furthermore, the environmental impact of mining and e-waste disposal is concentrated in regions with limited environmental governance and capacity to mitigate the health and environmental risks associated with exposure to toxic elements [64]. In sub-Saharan Africa, for example, the rapid expansion of mining and processing has increased the concentration of toxic metals in terrestrial, aquatic, and atmospheric systems, thereby exacerbating ecological degradation and health risks for workers and surrounding communities [63]. 3.4 Performance Vs Resource Consumption The AI research community has developed range of standardized benchmarks to systematically monitor technical progress in AI model capabilities over time [65]. Despite inherent limitations, these benchmarks serve as practical tools for evaluating discrete intelligent competencies such as image classification accuracy or multiple-choice question answering. This analysis examines the relationship between benchmark performance and GPU resource requirements, focusing on the following five widely used evaluation frameworks: MATH (mathematical reasoning): MATH serves as widely adopted benchmark for evaluating mathematical problemsolving skills based on 12,500 challenging competition mathematics problems. Each problem in MATH has full step-by-step solution which can be used to teach models to generate answer derivations and explanations [66]. MMLU (multidisciplinary knowledge): the Massive Multitask Language Understanding (MMLU) benchmark encompasses 57 tasks, including elementary mathematics, US history, computer science, law, and additional domains [67]. 10/18 HumanEval (programming proficiency): HumanEval measures functional correctness in program synthesis from docstrings, comprising 164 original programming problems that assess language comprehension, algorithmic thinking, and mathematical reasoning comparable to entry-level software engineering interviews [68]. ARC-c (multidisciplinary knowledge): The AI2s Reasoning Challenge (ARC-c) dataset presents multiple-choice questions derived from science examinations spanning grades 3-9, with the challenge partition containing complex problems requiring advanced reasoning capabilities [69]. HellaSwag (commonsense understanding): HellaSwag tests models commonsense reasoning via natural language inference, focusing on plausible sentence completions in everyday scenarios [70]. Figure 5. Illustration of the relationship between Falcon, Llama 2, GPT-3.5, and GPT-4 model performance - measured across five standard benchmarks - and the corresponding GPU requirements (on log scale) for model training (author illustration). To better understand the relationship between computational investments and AI performance, we compare models along three dimensions. First, we examine the transition from GPT-3.5 to GPT-4, both developed by OpenAI, to assess the impact of scaling large-scale models within consistent organizational context. Second, we compare two models trained with nearly identical computational budgets to isolate differences in training efficiency and model performance. Lastly, we contrast the second-smallest with the largest model in our dataset to explore how performance scales at the extremes of model size and resource consumption. To examine in-house performance improvements over time, we first compare OpenAIs GPT-3.5 with its successor, GPT-4 (see Figure 5 and 6 a)). The transition from GPT-3.5 to GPT-4 illustrates substantial computational scaling accompanied by mixed performance returns. GPT-4 required approximately 31.5 times more GPU resources for training than GPT-3.5 (2,515 vs 80 GPUs), representing more than 3,000% increase in computational resources. While GPT-4 delivered substantial performance improvements in certain domains, achieving +61.1% over GPT-3.5 on the MATH benchmark and +39.3% on HumanEval, other benchmarks showed only modest gains. Overall, these results suggest diminishing returns in terms of performance relative to computational investment. This raises critical questions about the efficiency and sustainability of current scaling trends and whether performance evaluation benchmarks are already saturated. 11/18 The computational demands of training large-scale AI models have increased significantly in recent years. The most straightforward way to measure this is the number of FLOPs required to train an AI model. In 2020, only 11 models required more than 1023 FLOPs to train [71]. However, between January 2020 and June 2025, 432 notable AI models entered the market [72]. Among these, 271 models provided estimates of their training FLOPs, while 160 did not. Notably, 111 models among those with available data, roughly 41%, exceed the 1023 FLOPs threshold during training. This trend reflects significant escalation in the computational scale of modern AI model development. While increasing computational resources has enabled notable advancement in some domains, it also underscores the limitations of brute-forcing intelligence. To further explore the relationship between compute efficiency and model performance, we compare models trained with similar computational budgets above the 1023 FLOPs threshold. This comparison helps isolate factors that contribute to performance increase beyond sheer scaling. Metas LLaMa 2 (8.4 1023 Flops) and OpenAIs GPT-3.5 (3.15 1023 Flops) utilize nearly identical training resources yet demonstrate contrasting efficiency profiles across hardware utilization and model performance evaluations (see Figure 6 b)). LLaMa 2 achieved higher MFU of 53% [50], compared to an estimated 20 to 35% MFU for GPT-3.5, based on OpenAIs reported 19.6% MFU for GPT-3 and about 35% MFU for GPT-4 [73]. This suggests that Metas training infrastructure and optimization strategies were more effective in extracting computational throughput from the hardware. However, despite this efficiency advantage and comparable computational budgets, GPT-3.5 consistently outperforms LLaMa 2 across all evaluated benchmarks (see Figure 6), especially in mathematical reasoning and programming tasks. This contrast illustrates an important distinction: models trained with similar computational resources can achieve substantially different performance outcomes depending on factors such as training data quality, model architectural design choices, and other optimization strategies (e.g., post-training reinforcement learning). GPT-3.5s stronger performance in mathematical and coding domains, despite lower MFU, may indicate that OpenAIs training data curation and model architecture were better suited for these specific capabilities, even if their training process was less computationally efficient. The comparison between the second smallest model in this study, Falcon (2.4 1023 Flops), and the largest model, GPT-4 (1.73 1025 FLOPs), illustrates the relationship between massive resource investment and capability increase (see Figure 6 c)). GPT-4 required approximately 81 times the computational resources of Falcon during training. Again, mathematical reasoning capabilities showed the most dramatic improvement, with GPT-4 achieving more than 7 times the performance of Falcon on the math benchmark. This substantial gain suggests that mathematical reasoning represents particularly resource-intensive capability that may justify extreme computational investments for specific applications. Conversely, commonsense understanding capabilities, as measured by HellaSwag, demonstrate only modest improvement of 14%, suggesting that distinct cognitive abilities exhibit differential scaling efficiency in response to increased computational resources. Figure 6. Comparison of model performance between a) GPT-3.5 and GPT-4, b) GPT-3.5 and LLaMa 2, and c) Falcon and GPT-4, across five benchmarks, alongside their respective GPU resource requirement for training (author illustration). 3.5 Resource Savings via Training Efficiency and Lifespan Improvements ML engineers, data center operators, and semiconductor manufacturers all play crucial role in reducing the material footprint of AI training. Two primary strategies can reduce the number of GPUs required for model training: software-based improvements, such as those that optimize GPU utilization; and hardware-based measures, maximizing the lifespan of GPUs in data centers. Data center design, particularly cooling efficiency, has substantial influence on GPU wear and longevity. These considerations begin at the semiconductor level, where chip designers work to improve thermal management. Prolonged high-energy workloads, common during AI training, can all lead to significant heat generation on silicon chips, accelerating hardware degradation, and 12/18 Figure 7. GPU requirements for training GPT-4 for varying MFU scenarios and varying life spans (author illustration). ultimately catastrophic failure [53, 54]. Increasing the MFU from 20% to 60%, while keeping the GPU lifespan constant, reduces the number of GPUs required for model training by approximately 67%. Similarly, lifespan expansion from 1 to 3 years, while keeping MFU constant, results in about 67% reduction. further lifespan extension to 5 years would yield an estimated 80% reduction. To illustrate the impact of these combined optimizations: training GPT-4 with relatively low MFU of 20% over one-year lifespan requires 8,800 GPUs. In contrast, under an optimized scenario, five-year lifespan and 60% MFU would require only 587 GPUs. This represents potential reduction of about 93% in GPU usage (see Figure 7). Notably, the decline in GPU requirements with increasing MFU is steeper for shorter lifespans and becomes more gradual for longer lifespans. This trend indicates diminishing returns in GPU savings from MFU improvements once hardware longevity is maximized (see Figure 7)."
        },
        {
            "title": "4 Discussion",
            "content": "By examining the performance and resource consumption of AI models, our findings reveal three critical insights for AI development strategies. First, the field appears to be approaching efficiency barriers where pure computational scaling yields diminishing returns for most capabilities. Second, architectural innovations and training methodologies may offer more effective performance improvements than simply scaling raw resources; for example, data quality can play bigger role than simply scaling the number of tokens. Third, specific capabilities, such as mathematical reasoning, may require disproportional computational investments. This finding aligns with the statement from the benchmark developers in mathematical reasoning, who note that although scaling Transformer models enhances performance on most text-based tasks, it does not currently systematically yield significant improvements in AI models mathematical capabilities [66]. To put it differently, we are trying to make square perform the function of circle by applying sheer force and scale. While this study establishes novel quantitative link between AIs computational demands and its material footprint, some methodological limitations should be considered when interpreting the results. We model GPU demand using sequential training scenario to standardize comparisons across models with heterogeneous training strategies. This approach is not intended to mirror real-world training workflows, which rely on large-scale parallelization. For example, GPT-4 was reportedly trained on approximately 25,000 A100 GPUs for 90100 days [58]. Under two-year lifespan assumption, such training run corresponds to roughly 13% wear of each GPUs productive capacity. The sequential framing serves as standardized metric for comparing material intensity across models with different training strategies, translating parallel computational workloads into equivalent hardware utilization. While this framework simplifies system-level complexities, including heterogeneous wear patterns and infrastructure inefficiencies, it captures the fractional depletion of hardware operational lifetime attributable to individual training processes. In addition, MFU alone does not fully characterize training efficiency or hardware wear. Distributed training introduces communication overheads, heterogeneous parallelization strategies, and deliberate efficiency trade-offs. Lower MFU values may result from strategic training design choices rather than inefficiency. For instance, moderately sized batches distributed across larger GPU cluster can shorten overall training time even as per-device utilization decreases. The standard FLOP-estimation formula defined in Equation 2 provides first-order approximation but omits critical implementation details that affect real-world performance. Performance can vary due to optimized kernel implementations [74, 75], re-materialization techniques [76], and architectural variations such as sparse models [36], which can either improve the throughput or reduce the required compute. These factors can introduce deviations from the theoretical FLOP count. While 13/18 Equation 2 has its limitations, these estimates remain helpful for comparing the relative computational complexity of different models [77] and can serve as conservative baseline for resource allocation and computational needs. Future work could be complemented with empirical measurements that account for hardware utilization patterns, parallelization strategies, and implementation-specific optimizations from real training runs. As GPT-4s architecture is not officially disclosed, our analysis adopted an informed active parameter estimate based on the most widely cited independent assessments. Alternative configurations, such as different expert counts or activation ratios, would yield different computational and material requirements. To accommodate this uncertainty, we provide an interactive tool5 that enables users to input alternative parameters and compute the corresponding results themselves. In addition, analytical constraints affect the characterization of the material. The ICP-OES analysis detected 32 elements, but excluded carbon lost during the polymers pyrolysis and oxygen bound in metal oxides. Unmeasured elements account for 1.78% of the GPU die, 1.73% of the heatsink, 7.55% of the PoP, and 4.05% of the PCB. Furthermore, the analysis captures only the materials in the final GPU assembly, not losses incurred during material refining, processing, or component manufacturing. This leads to conservative estimates of total resource requirements. Moreover, our assessment focuses exclusively on the GPU unit. Broader data center infrastructure, such as networking equipment, server racks, storage arrays, cooling systems, and power supply, is excluded. Including these infrastructure components would substantially increase AIs estimated material footprint. Recognizing these limitations is essential for refining future assessments of AIs material consumption and for developing system-level methodologies that more comprehensively capture the resource intensity of computational infrastructures. Ultimately, the material sustainability of AI training depends on multi-stakeholder approach spanning the entire technology stack. While our analysis focuses on hardware utilization metrics and component lifespan as levers for resource reduction, advancing toward more sustainable AI will require coordinated action across multiple domains, including greater transparency from data center operators and detailed utilization disclosures from AI developers to integrate real-world measurements into future assessments. By quantifying the material footprint of AI, this study expands the ongoing sustainability debate beyond energy and water consumption to the physical resource dependencies of computation. These findings underscore the need to implement both softwareand hardware-level optimization strategies to mitigate the environmental and material impacts of large-scale AI training."
        },
        {
            "title": "5 Conclusion",
            "content": "This study provides an integrated assessment linking the computational requirements of modern large-scale AI training to its underlying material footprint. By quantifying the resources required to train contemporary frontier AI models, we demonstrate that the environmental impacts of AI extend beyond operational energy use and further encompass substantial mineral extraction and disposal of toxic materials embedded in advanced computing hardware. Our results further indicate that the performance gains from continued computational scaling are diminishing across wide range of capability domains. Hence, architectural and algorithmic innovations may offer more sustainable pathways for advancing model performance than brute-forcing intelligence through sheer model scaling. While methodological uncertainties remain, this work establishes conservative baseline for understanding the physical resource dependencies of AI. Reducing these impacts will require coordinated action across developers, hardware manufacturers, and data-center operators, including greater transparency around training practices, standardized reporting of hardware utilization, and better integration of real-world performance data into environmental assessments. Overall, this study highlights the need to incorporate material considerations into the discussions of AI scalability and sustainability. As AI systems continue to expand in scale and adoption, integrating material impact assessments is crucial for guiding more environmental sustainable development of AI."
        },
        {
            "title": "Acknowledgment",
            "content": "The authors would like to thank ADEME for their material and financial support as well as the cloud providers who supplied broken GPUs for this study. We also extend thanks to Julien Comel, Head of Research at Terra Nova Développement, who conducted the elemental composition analysis. Funded by the TRA Sustainable Futures (University of Bonn) as part of the Excellence Strategy of the federal and state governments. Funding for this research was provided by the Alexander von Humboldt Foundation in the framework of the Alexander von Humboldt Professorship for Artificial Intelligence and endowed by the Federal Ministry of Research to Prof. Dr. Aimee van Wynsberghe. 5The tool can be accessed via the following URL: huggingface.co/spaces/sophia-falk/flops-2-footprints. 14/18 References [1] Z. Nasir. The world is running out of resources for IT. https://circularcomputing.com/news/theworld-is-running-out-of-resources-for-it/. Accessed on 10 April 2025. [2] M. Sweney. Global shortage in computer chips reaches crisis point. https : / / www . theguardian . com / business/2021/mar/21/globalshortageincomputer- chipsreachescrisispoint. 2021. Accessed on 10 April 2025. [3] A. Kharpal. Surging AI demand could cause the worlds next chip shortage, research says. https://www.cnbc. com/2024/09/25/surgingaidemandcould- causetheworldsnext- chipshortagereport.html. 2024. Accessed on 10 April 2025. [4] United Nations Conference on Trade and Development. Digital economy growth and mineral resources: implications for developing countries. Technical Note TN/UNCTAD/ICT4D/16. United Nations Conference on Trade and Development, 2020, p. 44. [5] UNEP. Were gobbling up the Earths resources at an unsustainable rate. https://www.unep.org/news-andstories/story/were-gobbling-earths-resources-unsustainable-rate. 2019. Accessed on 09 April 2025. [6] United Nations Environment Programme. Bend the trend. Pathways to liveable planet as resource use spikes. https: [7] //wedocs.unep.org/20.500.11822/44901. 2024. Accessed on 10 April 2025. IEA. Executive summary The Role of Critical Minerals in Clean Energy Transitions Analysis. https://www. iea . org / reports / the - role - of - critical - minerals - in - clean - energy - transitions / executive-summary. Accessed on 13 May 2025. [8] R. Uddin and S. Morris. Big Tech lines up over $300bn in AI spending for 2025. https://www.ft.com/content/ 634b7ec5-10c3-44d3-ae49-2a5b9ad566fa. 2025. Accessed on 11 April 2025. [9] McKinsey & Company. AI power: Expanding data center capacity to meet growing demand. https : / / www . mckinsey.com/industries/technologymediaandtelecommunications/ourinsights/ ai-power-expanding-data-center-capacity-to-meet-growing-demand. 2024. Accessed on 24 March 2025. [10] Energy and AI. World Energy Outlook Special Report. https://iea.blob.core.windows.net/assets/ 86ed1178-4d77-45ac-ab38-28e849f3b93f/EnergyandAI.pdf. 2025. Accessed on 11 April 2025. [11] T. Spencer and S. Singh. What the data centre and AI boom could mean for the energy sector. https://www.iea. org/commentaries/whatthedatacentre- andaiboomcould- meanfortheenergysector. 2024. Accessed on 24 March 2025. IEA. World Energy Outlook 2024. https://www.iea.org/reports/worldenergyoutlook2024. Paris, 2024. Accessed on 10 April 2025. [12] [13] Medium. Small Language Models (SLMs): The Future of Efficient AI. https://medium.com/@ianishgoswami/ small-language-models-slms-the-future-of-efficient-ai-653b033ab7e5. 2024. Accessed on 24 March 2025. [14] Jiedong Lang, Zhehao Guo and Shuyu Huang. comprehensive study on quantization techniques for large language models. In: 2024 4th International Conference on Artificial Intelligence, Robotics, and Communication (ICAIRC). IEEE. 2024, pp. 224231. [15] Alexandra Sasha Luccioni, Emma Strubell and Kate Crawford. From efficiency gains to rebound effects: The problem of jevons paradox in AIs polarized environmental debate. In: Proceedings of the 2025 ACM Conference on Fairness, Accountability, and Transparency. 2025, pp. 7688. [16] Stanley Jevons. The coal question: An Inquiry concerning the Progress of the Nation, and the Probable Exhaustion of our Coal-mines. In: The Economics of Population. Routledge, 1866, pp. 193204. [17] Alex De Vries. The growing energy footprint of artificial intelligence. In: Joule 7.10 (2023), pp. 21912194. [18] Sasha Luccioni, Yacine Jernite and Emma Strubell. Power hungry processing: Watts driving the cost of AI deployment? In: Proceedings of the 2024 ACM conference on fairness, accountability, and transparency. 2024, pp. 8599. [19] Qingxia Zhang et al. survey on data center cooling systems: Technology, power consumption modeling and control strategy optimization. In: Journal of Systems Architecture 119 (2021), p. 102253. 15/18 [20] Tianqi Xiao et al. Environmental impact and net-zero pathways for sustainable artificial intelligence servers in the USA. In: Nature Sustainability (2025), pp. 113. DOI: https://doi.org/10.1038/s41893-025-01681-y. [21] Sophia Chen. How much energy will AI really consume? The good, the bad and the unknown. In: Nature 639.8053 (2025), pp. 2224. [22] Pengfei Li et al. Making ai less thirsty. In: Communications of the ACM 68.7 (2025), pp. 5461. [23] K. Leswing. Meet the $10,000 Nvidia chip powering the race for A.I. https://www.cnbc.com/2023/02/23/ nvidias-a100-is-the-10000-chip-powering-the-race-for-ai-.html. 2023. Accessed on 24 March 2025. [24] A. Heath. Mark Zuckerbergs new goal is creating artificial general intelligence. https://www.theverge.com/ 2024/1/18/24042354/mark-zuckerberg-meta-agi-reorg-interview. 2024. Accessed on 24 March 2025. nvidia-ampere-architecture-whitepaper.pdf. https://images.nvidia.com/aem-dam/en-zz/Solutions/ data-center/nvidia-ampere-architecture-whitepaper.pdf. 2020. Accessed on 24 March 2025. [25] [26] Regulation (EU) 2024/1689 of the European Parliament and of the Council of 13 June 2024 laying down harmonised rules on artificial intelligence and amending Regulations (EC) No 300/2008, (EU) No 167/2013, (EU) No 168/2013, (EU) 2018/858, (EU) 2018/1139 and (EU) 2019/2144 and Directives 2014/90/EU, (EU) 2016/797 and (EU) 2020/1828 (Artificial Intelligence Act) (Text with EEA relevance). http://data.europa.eu/eli/reg/2024/1689/oj/ eng. 2024. Accessed on 24 March 2025. [27] Sophia Falk et al. More than Carbon: Cradle-to-Grave environmental impacts of GenAI training on the Nvidia A100 GPU. In: arXiv preprint arXiv:2509.00093 (2025). [28] A. Grattafiori et al. The Llama 3 Herd of Models. In: arXiv preprint arXiv:2407.21783 (2024). DOI: 10.48550/ arXiv.2407.21783. [29] E. Ohiri. NVIDIA A100 vs. V100: In-Depth GPU Comparison. https : / / www . cudocompute . com / blog / nvidia-a100-vs-v100-how-do-they-compare. 2024. Accessed on 21 March 2025. [30] Cybersided. How Long Do GPUs Last? (Average Lifespan & Effectiveness). https://cybersided.com/howlong-do-gpus-last/. Accessed on 21 March 2025. [31] G. Ostrouchov et al. GPU Lifetimes on Titan Supercomputer: Survival Analysis and Reliability. In: SC20: International Conference for High Performance Computing, Networking, Storage and Analysis. 2020, pp. 114. DOI: 10.1109/ SC41405.2020.00045. [32] Tech Fund [@techfund1]. AI architect at Google mentions the lifetime of datacenter GPUs at current utilization levels to be 3 years at most. This is bullish for $NVDA end-demand as most analysts were assuming lifetime of around 5 years. https://x.com/techfund1/status/1849031571421983140. 2024. Accessed on 21 March 2025. [33] A. Shilov. Datacenter GPU service life can be surprisingly short only one to three years is expected according to unnamed Google architect. https://www.tomshardware.com/pccomponents/gpus/datacentergpu - service - life - can - be - surprisingly - short - only - one - to - three - years - is - expected-according-to-unnamed-google-architect. 2024. Accessed on 21 March 2025. [34] D. Bahdanau. The FLOPs Calculus of Language Model Training. https://medium.com/@dzmitrybahdanau/ the-flops-calculus-of-language-model-training-3b19c1f025e4. 2022. Accessed on 24 March 2025. [35] J. Kaplan et al. Scaling Laws for Neural Language Models. In: arXiv preprint arXiv:2001.08361 (2020). DOI: 10.48550/arXiv.2001.08361. [36] William Fedus, Barret Zoph and Noam Shazeer. Switch Transformers: Scaling to trillion parameter models with simple and efficient sparsity.(2021). In: arXiv preprint cs.LG/2101.03961 (2021). [37] OpenAI et al. GPT-4 Technical Report. 2024. arXiv: 2303.08774 [cs.CL]. URL: https://arxiv.org/abs/ 2303.08774. [38] Dylan Patel and Gerald Wong. GPT-4 Architecture, Infrastructure, Training Dataset, Costs, Vision, MoE. In: SemiAnalysis. July (2023). Accessed on 11 Nov 2025. [39] Ege Erdil. How do mixture-of-experts models compare to dense models in inference? In: Epoch.ai (2024). Accessed on 11 Nov 2025. 16/18 [40] Dylan Patel. The ai brick walla practical limit for scaling dense transformer models, and how gpt 4 will break past it. In: SemiAnalysis. January 24 (2023), p. 2023. [41] Claudio Fischer Lemos. LLM Mixture of Experts Explained. In: tensorops.ai (2025). Accessed on 11 Nov 2025. [42] Dylan Patel and Gerald Wong. GPT-4 Architecture, Infrastructure, Training Dataset, Costs, Vision, MoE. https: //newsletter.semianalysis.com/p/gpt-4-architecture-infrastructure. 2023. Accessed on 19 Nov 2025. [43] Ziheng Jiang et al. {MegaScale}: Scaling large language model training to more than 10,000 {GPUs}. In: 21st USENIX Symposium on Networked Systems Design and Implementation (NSDI 24). 2024, pp. 745760. [44] Pinxue Zhao et al. Efficiently training 7b llm with 1 million sequence length on 8 gpus. In: arXiv e-prints (2024), arXiv2407. [45] Hasibul Jamil, MD Nine and Tevfik Kosar. EMLIO: Minimizing I/O Latency and Energy Consumption for Large-Scale AI Training. In: arXiv preprint arXiv:2508.11035 (2025). [46] Seokjin Go et al. Characterizing the Efficiency of Distributed Training: Power, Performance, and Thermal Perspective. In: arXiv preprint arXiv:2509.10371 (2025). [47] A. Chowdhery et al. PaLM: Scaling Language Modeling with Pathways. In: arXiv preprint arXiv:2204.02311 (2022). DOI: 10.48550/arXiv.2204.02311. [48] Large Scale Training of Hugging Face Transformers on TPUs With PyTorch/XLA FSDP. https://pytorch.org/ blog/large-scale-training-hugging-face/. 2023. Accessed on 11 April 2025. [49] C. Frye. paid for the whole GPU, am going to use the whole GPU: high-level guide to GPU utilization. https://modal.com/blog/gpu-utilization-guide?utm. 2025. Accessed on 11 April 2025. [50] High-Performance Llama 2 Training and Inference with PyTorch/XLA on Cloud TPUs PyTorch. https://pytorch. org/blog/high-performance-llama-2/. 2023. Accessed on 23 June 2025. [51] Maximizing training throughput using PyTorch FSDP PyTorch. https://pytorch.org/blog/maximizingtraining/. 2024. Accessed on 04 June 2025. [52] BehindTheKeyboard. llama/MODEL_CARD.md at main BehindTheKeyboard/llama. https://github.com/ BehindTheKeyboard/llama/blob/main/MODEL_CARD.md. 2024. Accessed on 24 June 2025. [53] S. Tang et al. Brief overview of the impact of thermal stress on the reliability of through silicon via: Analysis, characterization, and enhancement. In: Mater. Sci. Semicond. Process. 183 (2024), p. 108745. DOI: 10.1016/j. mssp.2024.108745. [54] Y. Cui et al. Flexible thermal interface based on self-assembled boron arsenide for high-performance thermal manage- [55] ment. In: Nature Communication 12.1 (2021), p. 1284. DOI: 10.1038/s41467-021-21531-7. volta-architecture-whitepaper.pdf. https://images.nvidia.com/content/voltaarchitecture/ pdf/volta-architecture-whitepaper.pdf. 2017. Accessed on 24 June 2025. [56] Y. Han et al. The Big Chip: Challenge, model and architecture. In: Fundam. Res. 4.6 (2023), pp. 14311441. DOI: 10.1016/j.fmre.2023.10.020. [57] Sophia Falk et al. The Potential of Data Center Waste Heat Recovery for Greenhouse Food Production in the US: Ramifications for Sustainable AI. In: Available at SSRN 5170348 (). [58] katerinaptrv. GPT4All Details Leaked. https : / / medium . com / @daniellefranca96 / gpt4 - all - details-leaked-48fa20f9a4a. 2023. Accessed on 05 June 2025. [59] N. Wang et al. Analysis of soil fertility and toxic metal characteristics in open-pit mining areas in northern Shaanxi. In: Sci. Rep. 14.1 (2024), p. 2273. DOI: 10.1038/s41598-024-52886-8. [60] Yihang Wu et al. Antimony, beryllium, cobalt, and vanadium in urban park soils in Beijing: Machine learning-based source identification and health risk-based soil environmental criteria. In: Environmental Pollution 293 (2022), p. 118554. [61] World Health Organization. World health statistics 2010. World Health Organization, 2010. [62] Deblina Dutta, Shashi Arya and Sunil Kumar. Industrial wastewater treatment: Current trends, bottlenecks, and best practices. In: Chemosphere (2021). [63] Asha Ripanda et al. Combatting toxic chemical elements pollution for Sub-Saharan Africas ecological health. In: Environmental Pollution and Management (2025). 17/18 [64] S. Falk, A. Van Wynsberghe and L. Biber-Freudenberger. The attribution problem of seemingly intangible industry. In: Environmental Challenges 16 (2024), p. 101003. DOI: 10.1016/j.envc.2024.101003. [65] The 2025 AI Index Report Stanford HAI. https://hai.stanford.edu/aiindex/2025aiindexreport. 2025. Accessed on 23 June 2025. [66] Dan Hendrycks et al. Measuring Mathematical Problem Solving With the MATH Dataset. In: arXiv preprint arXiv:2103.03874 (2021). DOI: 10.48550/arXiv.2103.03874. [67] Dan Hendrycks et al. Measuring Massive Multitask Language Understanding. In: arXiv preprint arXiv:2009. (2021). DOI: 10.48550/arXiv.2009.03300. [68] Mark Chen et al. Evaluating Large Language Models Trained on Code. In: arXiv preprint arXiv:2107.03374 (2021). DOI: 10.48550/arXiv.2107.03374. [69] Peter Clark et al. Think you have Solved Question Answering? Try ARC, the AI2 Reasoning Challenge. In: arXiv preprint arXiv:1803.05457 (2018). DOI: 10.48550/arXiv.1803.05457. [70] R. Zellers et al. HellaSwag: Can Machine Really Finish Your Sentence? In: arXiv preprint arXiv:1905.07830 (2019). [71] DOI: 10.48550/arXiv.1905.07830. Josh You R. Rahman David Owen. Tracking Large-Scale AI Models. https://epoch.ai/blog/trackinglarge-scale-ai-models. 2024. Accessed on 24 March 2025. [72] Epoch AI. Data on Notable AI Models. https://epoch.ai/data/notable-ai-models. 2025. Accessed on 05 June 2025. [73] Daan. Estimating efficiency improvements in LLM pre-training. https : / / www . lesswrong . com / posts / tJAD2LG9uweeEfjwq/estimatingefficiencyimprovementsin- llmpretraining. 2024. Accessed on 11 April 2025. [74] Tri Dao. Flashattention-2: Faster attention with better parallelism and work partitioning. In: arXiv preprint arXiv:2307. (2023). [75] Pin-Lun Hsu et al. Liger-Kernel: Efficient Triton Kernels for LLM Training. In: Championing Open-source DEvelopment in ML Workshop @ ICML25. 2025. URL: https://openreview.net/forum?id=36SjAIT42G. [76] Tianqi Chen et al. Training deep nets with sublinear memory cost. In: arXiv preprint arXiv:1604.06174 (2016). [77] J. Hoffmann et al. Training Compute-Optimal Large Language Models. In: arXiv preprint arXiv:2203.15556 (2022). DOI: 10.48550/arXiv.2203.15556. 18/"
        }
    ],
    "affiliations": [
        "Center for Development Research, Bonn University, Germany",
        "Center for Science and Thought, Bonn University, Germany",
        "Hugging Face",
        "Sustainable AI Lab, Institute for Science and Ethics, Bonn University, Germany"
    ]
}