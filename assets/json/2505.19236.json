{
    "paper_title": "Evaluating Text Creativity across Diverse Domains: A Dataset and Large Language Model Evaluator",
    "authors": [
        "Qian Cao",
        "Xiting Wang",
        "Yuzhuo Yuan",
        "Yahui Liu",
        "Fang Luo",
        "Ruihua Song"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Creativity evaluation remains a challenging frontier for large language models (LLMs). Current evaluations heavily rely on inefficient and costly human judgments, hindering progress in enhancing machine creativity. While automated methods exist, ranging from psychological testing to heuristic- or prompting-based approaches, they often lack generalizability or alignment with human judgment. To address these issues, in this paper, we propose a novel pairwise-comparison framework for assessing textual creativity, leveraging shared contextual instructions to improve evaluation consistency. We introduce CreataSet, a large-scale dataset with 100K+ human-level and 1M+ synthetic creative instruction-response pairs spanning diverse open-domain tasks. Through training on CreataSet, we develop an LLM-based evaluator named CrEval. CrEval demonstrates remarkable superiority over existing methods in alignment with human judgments. Experimental results underscore the indispensable significance of integrating both human-generated and synthetic data in training highly robust evaluators, and showcase the practical utility of CrEval in boosting the creativity of LLMs. We will release all data, code, and models publicly soon to support further research."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 5 2 ] . [ 1 6 3 2 9 1 . 5 0 5 2 : r Evaluating Text Creativity across Diverse Domains: Dataset and Large Language Model Evaluator Qian Cao1, Xiting Wang1(cid:66), Yuzhuo Yuan2, Yahui Liu3, Fang Luo2, Ruihua Song1(cid:66) 1Renmin University of China, 2Beijing Normal University, 3Kuaishou Technology {caoqian4real, xitingwang, rsong}@ruc.edu.cn, joyyuan@mail.bnu.edu.cn, yahui.cvrs@gmail.com, luof@bnu.edu.cn https://creval-creative-evaluation.github.io/"
        },
        {
            "title": "Abstract",
            "content": "Creativity evaluation remains challenging frontier for large language models (LLMs). Current evaluations heavily rely on inefficient and costly human judgments, hindering progress in enhancing machine creativity. While automated methods exist, ranging from psychological testing to heuristicor prompting-based approaches, they often lack generalizability or alignment with human judgment. To address these issues, in this paper, we propose novel pairwise-comparison framework for assessing textual creativity, leveraging shared contextual instructions to improve evaluation consistency. We introduce CreataSet, large-scale dataset with 100K+ human-level and 1M+ synthetic creative instruction-response pairs spanning diverse open-domain tasks. Through training on CreataSet, we develop an LLM-based evaluator named CrEval. CrEval demonstrates remarkable superiority over existing methods in alignment with human judgments. Experimental results underscore the indispensable significance of integrating both human-generated and synthetic data in training highly robust evaluators, and showcase the practical utility of CrEvalin boosting the creativity of LLMs. We will release all data, code, and models publicly soon to support further research."
        },
        {
            "title": "Introduction",
            "content": "Creativity, defined as ideas or artifacts that are new, surprising and valuable [9], has long been defining trait of human intelligence and fueled the progress of modern civilization [23]. As current large language models (LLMs) exhibit increasingly remarkable capabilities across diverse domains and downstream tasks, they have also shown the ability to perform tasks requiring creativity [54, 69, 71, 64]. Evaluating the creativity of LLMs not only sheds light on their applicability to critical creative domains such as creative writing [11, 41], literature [8, 24] and other creative domains [43, 54, 56], but also has the potential to reveal gaps between LLM and human capabilities, offering valuable insights for future improvements. Despite the growing importance of evaluating the creativity of LLMs, existing evaluation methods face several limitations that hinder their broader applicability. First (cross-domain applicability), most current creativity evaluation methods focus on single domain, such as creativity problem solving [43, 56], humor [71], or simile generation [24], where creativity is only one of several aspects being assessed. These tasks are often entangled with other concepts like problem solving, making it difficult to isolate and generalize creativity itself to other domains, such as literature. Second (granularity), most existing methods evaluate creativity at the model or subject level rather than at the level of individual responses [42, 58]. While effective for comparing models, they struggle to (cid:66) Corresponding author. Preprint. Under review. Figure 1: An example of how to formulate the problem of text creativity evaluation to better evaluate. compare two responses to the same prompt and determine which is more creative [10, 69]. We refer to the latter as text-level creativity (or simply text creativity). Text creativity is particularly important because it enables pinpointing specific responses for improvement, offering more actionable insights than coarse-grained evaluation of subject/model creativity. Third (effective automation), automating cross-domain creativity evaluation is valuable for reducing human effort and enabling iterative improvement. LLMs have shown strong performance as automatic evaluators, in domains such as helpfulness and coherence [26, 33, 35], known as LLM-as-a-judge [22, 34, 70]. However, the area of creativity evaluation remains underexplored. While early efforts assess creativity by prompting LLMs [54, 69], leveraging the cross-domain strengths of advanced models like GPT-4o [27], their judgments often suffer from issues of unreliability [10], inconsistency [60], and high cost [12]. This paper aims to address these problems by proposing an effective evaluation methodology for automatically assessing text creativity across domains, including cross-domain benchmark dataset labeled by 30 human judges and an effective LLM-based evaluator for automatic text creativity evaluation. To ensure the quality of the benchmark and LLM-based evaluator, we need to solve two major challenges. The first is how to facilitate human labeling of creativity. We find that human annotators may struggle to reach consistent judgments without clear contextual guidance, since creativity may be understood differently in different contexts. For example, as shown in Figure 1 (a), when three annotators independently assessed the creativity of 400 contextually unrelated text pairs, the agreement among them was moderate (with an Intraclass Correlation Coefficient, i.e., ICC, of 0.59). The second challenge is how to train reliable LLM evaluator given the scarcity of creative data. Data scarcity limits the ability of evaluators to generalize across diverse domains and its effectiveness. It is thus important to collect large-scale training data in weakly supervised manner. Our work resolves these two challenges by introducing framework that generates multiple creative responses conditioned on the same context. On the one hand, this setup ensures high-quality human annotations of text creativity pairs. As shown in Figure 1 (b), when shared instruction was provided as context, the agreement improved significantly (ICC increases to good level of 0.75). On the other hand, by controlling the response generation process for the same context, we automatically generate large-scale pseudo labels for their creativity levels in weakly supervised manner, which solves the data scarcity issue. Specifically, our contributions are as follows: We propose context-aware, pairwise comparison-based evaluation protocol for assessing text creativity. Using this protocol, we manually annotate test set of over 3,000 samples to benchmark text creativity evaluators. Notably, even state-of-the-art LLMs perform poorly on it compared to humans, underscoring key performance bottleneck in current evaluators. To support training, we further construct CreataSet, large-scale dataset incorporating creative tasks in 87 domains, including over 1M instruction-response pairs with varying weakly supervised creativity levels. Based on CreataSet, we introduce CrEval, an LLM-based creativity evaluator. To the best of our knowledge, this is the first evaluator capable of conducting pairwise creativity assessment across multiple domains. It outperforms strong proprietary models, e.g., GPT-4o by 18.7% in agreement with human judges, and demonstrates strong domain generalization capabilities. We further show CrEval can enhance LLM creativity, presenting viable approach to improve generative AI."
        },
        {
            "title": "2 Related Work",
            "content": "Creativity Evaluation Evaluating creativity has been long-standing challenge [32, 2]. Researchers have proposed variety of methods to assess creativity [20, 69, 7, 55], most of which adopt specific evaluation frameworks and target particular tasks, such as the Remote Associates Test 2 Figure 2: The construction process of CreataSet and training process of CrEval. (RAT) [42] and the Torrance Tests of Creative Thinking (TTCT) [58]. Some subsequent efforts have modified the TTCT for use in creative writing (Torrance Tests of creative writing, TTCW) [10, 36] or in evaluating the creative performance of LLMs on such tasks [69]. Other work focuses on problem-solving as proxy for measuring the creativity of LLMs solutions [43, 56]. However, these approaches tend to have narrow focus, being confined to limited set of carefully designed tasks. Moreover, their emphasis is typically on the models creative ability rather than on assessing the creativity of the generated text itself. Although some studies have explored evaluating creativity in simile generation using heuristic scores on reference corpora [24], or using pretrained models like BERT [15] to calculate divergent semantic integration (DSI) as creativity metric [29], these methods are constrained by the limitations of heuristic design or suffer from poor performance, making them difficult to adapt to open-domain settings. Most recent approaches leverage general purpose LLMs (e.g., GPT-4) by prompting to assess textual creativity [54, 69, 10, 11], yet often struggle to achieve satisfactory results [44, 10, 11]. Consequently, the evaluation of textual creativity still largely depends on human judgment, such as the Consensual Assessment Technique (CAT) [5, 41]. However, manual assessment is costly, inefficient, and unable to provide iterative, automated feedback for improving models. To address these limitations, we propose novel approach that leverages LLM-as-a-judge to enable more efficient and accurate creativity assessment in text generation. LLM-as-a-Judge In the area of automatic evaluation for text generation, recent advent of large language models (LLMs) has enabled the evaluation paradigms to incorporate LLMs to be more accurate and flexible [18], known as LLM-as-a-judge [70, 22, 34], capable of assessing more diverse dimensions of text quality. Prior works focus more on the evaluation of text attributes like relevance [39, 1, 38], helpfulness [33, 35], or overall excellence [16, 26], etc. Other works also explore how to adapt LLMs to evaluate specific domains such as code generation [57, 63] and dialogue generation [37, 68]. However, few of these works investigate how to evaluate text creativity, making it hard to assess and improve the creative aspects of text generation. In our work, we propose to assess it in pairwise-comparison manner, and provide comprehensive study on leveraging LLMs to evaluate text creativity."
        },
        {
            "title": "3 Methodology",
            "content": "In this section, we describe how we construct our large-scale weakly supervised dataset CreataSet to support our evaluation protocol and train CrEval. Specifically, the process consists of three main steps. The first step is Across-Domain Creativity Dataset Initialization, in which we collect initial data from diverse domains and unify their format, generating necessary instructions for standalone texts. This enables us to obtain (I, R) from 87 domains with varying lengths, where denotes the instruction and the response. The second step is Context-Aware Response Augmentation. We 3 Figure 3: The examples of three different types of data. The original data are above the dashed line, while our constructed components are below. aim to augment responses of varying creative levels within the same context I, providing basis for pairing them effectively. The last step is Label Construction with Mixed Strategy, where we pair the responses and apply various labeling strategies to assign label to each pair This results in training samples of the form (I, R1, R2, y) for training the creativity evaluator CrEval. For meta-evaluation, we manually annotate test set to benchmark the performance of CrEval against other creativity evaluators. The process of our data construction is shown in Figure 2. 3.1 Across-Domain Creativity Dataset Initialization To build creativity dataset across diverse domains, we gather initial data with varying creativity levels from eight sources. Due to the different formats, we unify them into consistent (I, R) format. Multi-Domain Multi-Source Data Collection We aim to collect data from diverse sources and domains to construct broad distribution in both domain coverage and response length, thereby enabling the model to generalize across wide range of scenarios. Specifically, we begin by collecting data from existing creativity datasets, such as Oogiri-GO [71] and Ruozhiba [6], which naturally contain creative (I, R) pairs in the humor domain (Type in Figure 3). We further incorporate creativity-dense texts (R) from corpora of human creative works, such as poetry, lyrics, and prose, sourced from well-known websites. To enhance length diversity, we curate sub-dataset called Short Texts, comprising inspiring and thought-provoking sentences collected from online sources. Most of these entries consist of standalone texts (R) without explicit input prompts (Type in Figure 3). In addition, aiming to capture data with diverse levels of creativity and expand our domain coverage, we leverage existing instruction-tuning datasets such as Infinity-Instruct, given its rich collection of high-quality (I, R) pairs spanning wide range of domains (Type in Figure 3). Unified Instruction-Response Standardization To standardize the multi-source data into unified (I, R) format, we first enrich standalone texts by generating missing instructions. We train an instruction generator by reversing an instruction-tuning dataset (Infinity-Instruct). The generator learns to produce an instruction given response R. We generate an instruction for each standalone text, thus forming (I, R) pairs. To prevent non-creative data from obscuring creative data, we followed previous work [52] and applied some filters. All data are ultimately formatted as (I, R) pairs (as shown in Figure 3, forming CreataSet-Base, with over 113k creative samples. https://github.com/chinese-poetry/chinese-poetry, https://github.com/VMIJUNV/chinese-poetry-and-prose, https://github.com/yuxqiu/modern-poetry, https://music.163.com, https://m.sbkk8.com https://www.juzikong.com/ https://huggingface.co/datasets/BAAI/Infinity-Instruct Dataset Cross-domain (humor) (problem-solving) (problem-solving) (creative writing) Creative Writing v3 [47] (creative writing) Oorigi-GO [71] MacGyver [56] DPT [30] TTCW [10] TTCT+ [69] WritingBench [64] CreataSet (ours) (7 domains) (100 domains) (87 domains) Granularity Auto-Evaluator Total Words # Samples Train/Test Subject Level Subject Level Subject Level Subject Level Subject Level Subject Level Individual Text Level Individual Text Level 894,712 249,385 12,576 58,426 10,176 - 1,875,146 20,720,179 15,797 1,683 803 48 32 700 1,000 112,965 train & test train & test test test test test test test Table 1: The statistics of different creative datasets. Auto-Evaluator denotes whether an automatic evaluator is proposed based on this dataset. TTCT+ and training data for evaluator in WritingBench are not publicly available. We calculate the total word count of the responses of each dataset. Figure 4: Domain distribution of CreataSet-Base. Secondary domains for the top 5 primary ones are shown in gray. Figure 5: The length distributions of MacGyver, OorigiGO, DPT, WritingBench and CreataSet-Base. For better visualization, we have omitted TTCW and Creative Writing v3 due to their small dataset sizes. We compare our datasets with other creative-related datasets in Table 1, and present their length distributions in Figure 5. Our dataset is larger in scale and exhibits broader response length distribution. To assess domain diversity, we utilize GPT-4o-mini for domain classification and identify 17 core domains and 87 distinct subdomains. The distribution of these domains is shown in Figure 4. Further details are in Appendix A. 3.2 Context-Aware Response Augmentation Before constructing pairwise data (I, R1, R2) for training the evaluator, we first augment the set of responses for each instruction, i.e., (I, R1, . . . , Rk). This aims to enrich the creative diversity of responses, enabling the construction of pairs with creativity differences. To efficiently construct such data at scale, we employ open-sourced models with different levels of capability, e.g., Qwen2.5-14BInstruct [67] and MiniCPM-2B-SFT [25], to generate responses for instructions in CreataSet-Base, as illustrated in Figure 2. For each model, we use two prompting modes to induce varying creativity levels: (1) Prompto, general prompt that elicits ordinary responses; and (2) Promptc, creativityoriented prompt that encourages more imaginative outputs. By adopting different models/prompts, we generate multiple synthetic responses. The original responses in Type data are direct answers to instructions and generally lack creativity. To enrich this set, we further prompt GPT-4o to generate more creative responses to the same instructions. Finally, we name this dataset in the form (I, R1, . . . , Rk) as CreataSet-Ext. The prompts used are shown in Appendix 3.3 Label Construction with Mixed Strategies We combine responses into pairs (I, R1, R2) and use mixed strategy to assign labels for training and testing separately, since the label requirements differ between them. Reliable human-annotated labels are essential for meta-evaluation to accurately assess model performance, while constructing labels in large scale is more important for training. We detail them in the following. 5 High-Quality Human Labeling for Test Benchmark Construction To ensure diversity in the test set, we sample 50 instances from each data source in CreataSet, yielding 400 initial samples. These are further augmented using GPT-4o-mini with both prompts to enhance the distribution difference for evaluation. Following prior work [62, 29], we recruited 30 qualified annotators to rate response creativity on 4-point Likert scale, with responses presented in randomized order. Each responses creativity score is computed as the average of all ratings. The annotations exhibit high inter-rater reliability (Intraclass Correlation Coefficient, ICC(2k)=0.92). Finally, we construct 3K test set in the format (I, R1, R2, y), where pairs with score differences > 0.3 are labeled as distinguishable, and those with differences < 0.1 as comparable (tie). Weakly-Supervised Pseudo Labels for Training Set Construction For the training set, we assign weakly supervised pseudo-labels to response pairs in CreataSet-Base, enabling large-scale label construction. Our approach is based on two key assumptions: (1) stronger models tend to produce more creative responses than weaker ones, and (2) creativity-focused prompts elicit more creative outputs than ordinary prompts. To validate these assumptions, we sampled 50 response pairs for each model/prompt combination and recruited 3 annotators to compare their creativity. The results show that creativity distinctions based on assumption (1) achieve 90.4% accuracy, and assumption (2) achieves 87%, confirming the reliability of both heuristics. For creatively comparable samples (the tie cases), we implement random pairing of responses produced by the same models using Prompto. Using these assumptions, we assign labels to response pairs in CreataSet-Ext, resulting in training data of the form (I, R1, R2, y). 3.4 CrEval Training The constructed large-scale CreataSet-Ext can enable us to train CrEval. (I, R1, R2) as input, and trained to minimize the classification loss: It provides triplets = (cid:88) log (yI, R1, R2), (I,R1,R2)D (1) where (yI, R1, R2) represents the probability of the label given the triplet (I, R1, R2). To mitigate the positional bias, we follow previous works [60, 61, 35] by augmenting the data by swapping R1 and R2 in the input and adjusting the corresponding label. Additionally, we apply negative sampling by randomly selecting response to serve as the least creative response, further enhancing the models awareness of the instruction context I. During inference, the model predicts whether R1 is more creative than R2, vice versa, or if they are creatively comparable. Moreover, reference response Rr, generated by either human or model, can be baseline for comparing the creativity of another response in such comparison manner."
        },
        {
            "title": "4 Experiments",
            "content": "4.1 Experimental Setup Based on our human-labeled test set of CreataSet, we adopt F1 score, Kappa score, and Agreement rate to evaluate the performance of different methods, following previous work [61, 35]. All metrics are calculated twice by swapping the order of the two responses, and then the average scores are reported. Following [26], to eliminate the influence of sampling randomness, we set the temperature to 0 for deterministic results, while other methods retain their original settings. We conduct pairwise comparison experiments on CreataSet where CrEval is compared with the following baselines: Traditional Metrics: (1) Perplexity (PPL): simple baseline where we use Qwen2.5-7B-Instruct to calculate the perplexity of response. Higher perplexity indicates higher novelty and creativity. (2) Divergent semantic integration (DSI) [29]: It adopts BERT [15] to calculate the average semantic distance between all words in the response. higher DSI indicates higher creativity. Evaluation-Centric Models: We compare with several evaluation-centric models including prompting method G-Eval [39] and fine-tuned LLMs, such as PandaLM [61], Prometheus [33], AUTOJ [35] and WritingBench-Critic [64]. Since we use LLM backbones, the classification label is treated as text output conditioned on the prompt. Method S.T. Lyr. A.P. M.P. Pro. Oog. Ruo. Inf. Average F1 Kappa Agree. Traditional Metrics PPL DSI Proprietary LLMs o3 o1 GPT-4o GPT-3.5 DeepSeek-R1 DeepSeek-V3 Claude-3.5-Sonnet Claude-3.5-Haiku Gemini-2.0-Pro Gemini-2.0-Flash G-Eval (GPT-4o) G-Eval (GPT-3.5) 7B Scale LLMs Gemma-2-9B-it LLaMA3.1-8B-Instruct PandaLM-7B Prometheus-7B AUTO-J WritingBench-Critic Qwen2.5-7B-Instruct CrEval-7B (ours) 13B Scale and Larger LLMs Qwen2.5-72B-Instruct LLaMA3.1-70B-Instruct Gemma-3-27B-it Gemma-3-12B-it Prometheus-13B Qwen2.5-14B-Instruct CrEval-14B (ours) 0.464 0.440 0.245 0.430 0.245 0.354 0.316 0.527 0.349 0. 0.515 0.578 0.329 0.561 0.374 0.528 0.357 0.480 -0.042 0.175 0.430 0. 0.802 0.807 0.800 0.686 0.743 0.780 0.775 0.748 0.765 0.763 0.772 0.636 0.795 0.713 0.390 0.330 0.659 0.715 0.710 0.779 0.751 0.736 0.792 0.761 0.445 0.742 0.786 0.589 0.573 0.605 0.486 0.479 0.584 0.603 0.573 0.532 0.538 0.583 0.494 0.562 0.548 0.435 0.365 0.526 0.528 0.494 0.556 0.558 0.564 0.572 0.542 0.377 0.568 0. 0.596 0.629 0.641 0.425 0.494 0.584 0.634 0.509 0.582 0.568 0.568 0.460 0.619 0.440 0.454 0.326 0.377 0.500 0.426 0.649 0.520 0.559 0.608 0.575 0.329 0.523 0.650 0.667 0.670 0.699 0.548 0.578 0.681 0.671 0.633 0.634 0.631 0.665 0.561 0.654 0.618 0.469 0.315 0.561 0.626 0.578 0.681 0.655 0.642 0.650 0.615 0.372 0.649 0. 0.663 0.672 0.667 0.489 0.612 0.684 0.702 0.652 0.628 0.632 0.694 0.493 0.646 0.615 0.346 0.342 0.541 0.548 0.487 0.665 0.594 0.624 0.666 0.674 0.410 0.629 0.672 0.774 0.738 0.749 0.667 0.751 0.765 0.762 0.724 0.754 0.758 0.759 0.608 0.751 0.649 0.398 0.369 0.553 0.600 0.647 0.778 0.734 0.732 0.753 0.729 0.386 0.717 0. 0.832 0.790 0.633 0.567 0.745 0.774 0.850 0.695 0.847 0.849 0.803 0.575 0.779 0.573 0.540 0.449 0.565 0.641 0.704 0.873 0.833 0.764 0.789 0.667 0.367 0.783 0.882 0.769 0.798 0.789 0.743 0.733 0.784 0.810 0.779 0.750 0.749 0.793 0.774 0.788 0.782 0.506 0.498 0.720 0.712 0.771 0.820 0.806 0.810 0.783 0.772 0.641 0.797 0. 0.721 0.720 0.703 0.585 0.653 0.714 0.727 0.669 0.695 0.695 0.712 0.582 0.704 0.621 0.453 0.377 0.567 0.612 0.614 0.732 0.692 0.684 0.706 0.672 0.416 0.683 0.735 0.578 0.563 0.519 0.350 0.457 0.558 0.609 0.496 0.532 0.531 0.558 0.339 0.544 0.418 0.129 0.097 0.323 0.362 0.403 0.601 0.535 0.535 0.564 0.498 0.095 0.523 0. 0.725 0.664 0.642 0.522 0.547 0.668 0.740 0.641 0.658 0.659 0.677 0.500 0.654 0.565 0.326 0.352 0.512 0.576 0.574 0.745 0.673 0.675 0.702 0.633 0.400 0.661 0.762 Table 2: Results of different methods on our CreataSet test set. Best results in the same group are highlighted in bold, and the second-best are underlined. S.T., Lyr., A.P., M.P., Pro., Oog., Ruo., and Inf. represent Short Texts, Lyrics, Ancient Poetry, Modern Poetry, Prose, Oogiri-Go, Ruozhiba, and Infinity-Instruct, respectively. We gray out the results of proprietary LLMs due to their larger sizes. General-purpose LLMs as Evaluators: We compare CrEval against several general-purpose LLMs, including LLaMA3.1-{8,70}B-Instruct [17], Gemma-{2-9B,3-12B,3-27B}-it [53, 31], Qwen2.5- {7,14,72}B-Instruct [67], GPT-3.5-Turbo-1106 [45], GPT-4o [27], OpenAI o1/o3 [28, 46], DeepSeek-{V3,R1} [14, 13], Claude-3.5-{Haiku, Sonnet} [3, 4], and Gemini-2.0-{Flash, Pro} [19]. We evaluate all models using the same prompt as for CrEval. 4.2 How Well Can CrEval Simulate Human Evaluation? We compare our model with all baselines and show results in Table 2. First, the results demonstrate that our proposed CrEvalevaluators consistently outperform all baseline methods across all overall metrics. With larger parameters, our best performer CrEval-14B even beat all the proprietary baselines, obtains 2.9% improvement in F1 score, 9.7% in Kappa score, and 12.6% in agreement rate compared to strong baseline like DeepSeek-V3, which proves the effectiveness of our proposed method in simulating human evaluation. Second, traditional metrics like PPL and DSI achieve the worst performance, e.g., PPL even having close to zero Kappa score, indicating that they have less correlation with human judgments and thus not suitable for text creativity evaluation. Third, Gemma-2-9B-it and Gemma-3-27B-it achieve the highest F1 scores of according groups on Short Texts, Lyrics, and Modern Poetry, while they fall behind CrEval on other scenarios. This reveals that they have shortcomings in capturing the creativity of texts such as humor in Oogiri-Go and Ruozhiba, and texts of ancient genres. Claude-3.5-Sonnet performs the best on Prose, suggesting that it can better process long text creativity. Compared to them, CrEval exhibits more balanced capability. 7 Figure 6: Consistency rate of different methods when swapping the order of responses. Method F1 Kappa Agreement Creval-7B 0.732 0.601 0.723 0.586 w/o Neg. 0.665 0.464 w/o Syn. w/ Only Syn. 0.585 0.356 0.745 0.745 0.634 0.589 Figure 7: Evaluation results of ablation study. Figure 8: Performance variation with data scales. LLMs may favor certain positions of the response, known as positional bias [60], which may lead to inconsistent evaluation results when swapping the order of responses. We have conducted consistency analysis to evaluate the stability of different methods. The consistency rate of different methods is shown in Figure 6. CrEval achieves the highest consistency rate of 94.4, indicating that it is more consistent and reliable in evaluating creativity compared to other methods. 4.3 How Do Data Influence CrEval? In training CrEval, we use (I, R1, R2, y) of different pseudo-creativity levels. Data Composition. To investigate their influence, we conduct an ablation study by training multiple CrEval variants with different data compositions as follows: (1) Creval-w/o Neg.: Training CrEval without sampling negative responses. (2) Creval-w/o Syn.: Training CrEval with only the original responses (highest creativity) in CreataSet without synthetic ones. (3) Creval-w/ Only Syn.: Training CrEval with only synthetic responses (lower creativity) in CreataSet without original ones. Table 7 presents the ablation study results. The results indicate that each type of data makes positive contribution. The original human-created responses contribute the most, as they provide diverse, high-quality information that better aligns CrEvalwith human preferences. Synthetic data plays crucial role in helping the model grasp the characteristics of creative responses, particularly those that LLMs can generate. Meanwhile, negative responses offer additional information to improve the models ability to measure the relevance between responses and instructions. Data Scale. To assess the impact of data volume, we train CrEval on varying scales, shown in Figure 8. F1 score, Kappa score, and agreement rate improve with larger data scales but plateau beyond 100K. This suggests that while more data benefits CrEval, the gains diminish at higher scales. 4.4 Does CrEval Demonstrate Out-of-Distribution Generalization? To assess domain generalization, we conduct an out-of-distribution (O.O.D.) experiment using creative writing dataset from prior work [10], featuring long, high-quality responses from both humans and models. Following their findings [10], we treat human responses as more creative and form 36 evaluation pairs. As shown in Figure 9, CrEval outperforms all baselines of the same scale and most proprietary models, demonstrating its strong generalization. 8 Figure 10: Win rate of different methods over GPT-4o-mini responses. DPO-Negative denotes DPO with negative sampled responses as reject samples. DPO-100E and DPO-70E30H use all easy and 70% easy+30% hard responses as reject samples, respectively. 4.5 Can CrEval Enhance Model Creativity? The answer is yes. As creativity evaluator, CrEval can differentiate response creativity, allowing us to leverage it for enhancing model creativity. We randomly sample 10K data from CreataSet training set to train Qwen2.5-7B-Instruct, using the original response as the ground truth, serving as the standard (the SFT baseline). By utilizing synthetic candidate responses (randomly sampled), we apply DPO [48] to take low-creativity responses as reject samples (the DPO baseline). We also randomly sample negative responses from other instructions as reject samples, denoted as DPO-Negative. Given an instruction with multiple response candidates, we use CrEval for pairwise creativity comparisons, assigning 3 points for wins, 1 for ties, and 0 for losses. This scoring ranks responses by creativity, with the top-ranked as hard samples and the lowest as easy samples. By adjusting the ratio of hard and easy rejects in DPO, we can control difficulty and enhance creativity. We evaluate different methods on the CreataSet test set, using CrEval, GPT-4o, and three human annotators to measure win rates against GPT-4o-mini responses. As shown in Figure 10, models trained with DPO significantly outperform SFT on all CrEval GPT4o, and human evaluations. While DPO-Negative offers slight gains over SFT, it remains less effective than DPO, highlighting the importance of conditioning on the context in creativity evaluation. By incorporating CrEval for creativity comparison and data selection, the win rate further improves, with DPO-70E30H achieving the highest win rate. DPO-100E, which uses all easy responses as reject samples, shows marginal improvement, suggesting that clear margin between positives and negatives can help models better learn creativity. Further results on DPO-70E30H reflect the highest win rate achieved when using 30% hard samples as reject samples, underscoring the benefit of including balanced mix of hard negatives."
        },
        {
            "title": "5 Conclusion",
            "content": "Figure 9: Agreement and F1 scores of different methods on O.O.D. dataset. In this paper, we propose novel pairwise-comparison framework for evaluating textual creativity and present CreataSet, large-scale dataset across diverse domains. Based on it, we develop CrEval, an LLM-based evaluator that significantly outperforms existing methods in alignment with human judgments. Our experiments highlight the essential role of combining both human and synthetic data in training robust creativity evaluators, and demonstrate that CrEval exhibits out-of-distribution generalization. We further find the practical value of integrating CrEval into generation pipelines to 9 boost LLM creativity. We believe that CreataSet and CrEval will be valuable assets for the research community, driving progress toward more accurate and scalable creativity evaluation."
        },
        {
            "title": "References",
            "content": "[1] Zahra Abbasiantaeb, Chuan Meng, Leif Azzopardi, and Mohammad Aliannejadi. Can we use large language models to fill relevance judgment holes? arXiv preprint:2405.05600, 2024. 3 [2] Selcuk Acar and Mark Runco. Divergent thinking: New methods, recent research, and extended theory. Psychology of Aesthetics, Creativity, and the Arts, 13(2):153, 2019. 2 [3] Anthropic. Claude 3.5 haiku, 2024. 7 [4] Anthropic. Claude 3.5 sonnet, 2024. [5] John Baer and Sharon McKool. Assessing creativity using the consensual assessment technique. In Handbook of research on assessment technologies, methods, and applications in higher education, pages 6577. IGI Global, 2009. 3 [6] Yuelin Bai, Xinrun Du, Yiming Liang, Yonggang Jin, Ziqiang Liu, Junting Zhou, Tianyu Zheng, Xincheng Zhang, Nuo Ma, Zekun Wang, Ruibin Yuan, Haihong Wu, Hongquan Lin, Wenhao Huang, Jiajun Zhang, Wenhu Chen, Chenghua Lin, Jie Fu, Min Yang, Shiwen Ni, and Ge Zhang. COIG-CQIA: quality is all you need for chinese instruction fine-tuning. CoRR, abs/2403.18058, 2024. 4, 17 [7] Kenes Beketayev and Mark Runco. Scoring divergent thinking tests by computer with semantics-based algorithm. Europes journal of psychology, 12(2):210, 2016. 2 [8] Brendan Bena and Jugal Kalita. Introducing aspects of creativity in automatic poetry generation. CoRR, abs/2002.02511, 2020. [9] Margaret A. Boden. The Creative Mind - Myths and Mechanisms (2. ed.). Routledge, 2003. 1 [10] Tuhin Chakrabarty, Philippe Laban, Divyansh Agarwal, Smaranda Muresan, and Chien-Sheng Wu. Art or artifice? large language models and the false promise of creativity. In Florian Floyd Mueller, Penny Kyburz, Julie R. Williamson, Corina Sas, Max L. Wilson, Phoebe O. Toups Dugas, and Irina Shklovski, editors, Proceedings of the CHI Conference on Human Factors in Computing Systems, CHI 2024, Honolulu, HI, USA, May 11-16, 2024, pages 30:130:34. ACM, 2024. 2, 3, 5, 8 [11] Tuhin Chakrabarty, Philippe Laban, and Chien-Sheng Wu. Ai-slop to ai-polish? aligning language models through edit-based writing rewards and test-time computation. arXiv preprint arXiv:2504.07532, 2025. 1, 3 [12] Lingjiao Chen, Matei Zaharia, and James Zou. Frugalgpt: How to use large language models while reducing cost and improving performance. CoRR, abs/2305.05176, 2023. 2 [13] DeepSeek-AI, Daya Guo, Dejian Yang, Haowei Zhang, Junxiao Song, Ruoyu Zhang, Runxin Xu, Qihao Zhu, Shirong Ma, Peiyi Wang, Xiao Bi, Xiaokang Zhang, Xingkai Yu, Yu Wu, Z. F. Wu, Zhibin Gou, Zhihong Shao, Zhuoshu Li, Ziyi Gao, Aixin Liu, Bing Xue, Bingxuan Wang, Bochao Wu, Bei Feng, Chengda Lu, Chenggang Zhao, Chengqi Deng, Chenyu Zhang, Chong Ruan, Damai Dai, Deli Chen, Dongjie Ji, Erhang Li, Fangyun Lin, Fucong Dai, Fuli Luo, Guangbo Hao, Guanting Chen, Guowei Li, H. Zhang, Han Bao, Hanwei Xu, Haocheng Wang, Honghui Ding, Huajian Xin, Huazuo Gao, Hui Qu, Hui Li, Jianzhong Guo, Jiashi Li, Jiawei Wang, Jingchang Chen, Jingyang Yuan, Junjie Qiu, Junlong Li, J. L. Cai, Jiaqi Ni, Jian Liang, Jin Chen, Kai Dong, Kai Hu, Kaige Gao, Kang Guan, Kexin Huang, Kuai Yu, Lean Wang, Lecong Zhang, Liang Zhao, Litong Wang, Liyue Zhang, Lei Xu, Leyi Xia, Mingchuan Zhang, Minghua Zhang, Minghui Tang, Meng Li, Miaojun Wang, Mingming Li, Ning Tian, Panpan Huang, Peng Zhang, Qiancheng Wang, Qinyu Chen, Qiushi Du, Ruiqi Ge, Ruisong Zhang, Ruizhe Pan, Runji Wang, R. J. Chen, R. L. Jin, Ruyi Chen, Shanghao Lu, Shangyan Zhou, Shanhuang Chen, Shengfeng Ye, Shiyu Wang, Shuiping Yu, Shunfeng Zhou, Shuting Pan, and S. S. Li. Deepseek-r1: Incentivizing reasoning capability in llms via reinforcement learning. CoRR, abs/2501.12948, 2025. 10 [14] DeepSeek-AI, Aixin Liu, Bei Feng, Bing Xue, Bingxuan Wang, Bochao Wu, Chengda Lu, Chenggang Zhao, Chengqi Deng, Chenyu Zhang, Chong Ruan, Damai Dai, Daya Guo, Dejian Yang, Deli Chen, Dongjie Ji, Erhang Li, Fangyun Lin, Fucong Dai, Fuli Luo, Guangbo Hao, Guanting Chen, Guowei Li, H. Zhang, Han Bao, Hanwei Xu, Haocheng Wang, Haowei Zhang, Honghui Ding, Huajian Xin, Huazuo Gao, Hui Li, Hui Qu, J. L. Cai, Jian Liang, Jianzhong Guo, Jiaqi Ni, Jiashi Li, Jiawei Wang, Jin Chen, Jingchang Chen, Jingyang Yuan, Junjie Qiu, Junlong Li, Junxiao Song, Kai Dong, Kai Hu, Kaige Gao, Kang Guan, Kexin Huang, Kuai Yu, Lean Wang, Lecong Zhang, Lei Xu, Leyi Xia, Liang Zhao, Litong Wang, Liyue Zhang, Meng Li, Miaojun Wang, Mingchuan Zhang, Minghua Zhang, Minghui Tang, Mingming Li, Ning Tian, Panpan Huang, Peiyi Wang, Peng Zhang, Qiancheng Wang, Qihao Zhu, Qinyu Chen, Qiushi Du, R. J. Chen, R. L. Jin, Ruiqi Ge, Ruisong Zhang, Ruizhe Pan, Runji Wang, Runxin Xu, Ruoyu Zhang, Ruyi Chen, S. S. Li, Shanghao Lu, Shangyan Zhou, Shanhuang Chen, Shaoqing Wu, Shengfeng Ye, Shengfeng Ye, Shirong Ma, Shiyu Wang, Shuang Zhou, Shuiping Yu, Shunfeng Zhou, Shuting Pan, T. Wang, Tao Yun, Tian Pei, Tianyu Sun, W. L. Xiao, and Wangding Zeng. Deepseek-v3 technical report. CoRR, abs/2412.19437, 2024. 7 [15] Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. BERT: pre-training of deep bidirectional transformers for language understanding. In Jill Burstein, Christy Doran, and Thamar Solorio, editors, Proceedings of the Conference of the North American Chapter of the Association for Computational Linguistics (NAACL), 2019. 3, 6 [16] Jiang Dongfu, Li Yishan, Ge Zhang, Huang Wenhao, Lin Bill Yuchen, and Chen Wenhu. Tigerscore: Towards building explainable metric for all text generation tasks. Computing Research Repository, 2024. 3 [17] Abhimanyu Dubey, Abhinav Jauhri, Abhinav Pandey, Abhishek Kadian, Ahmad Al-Dahle, Aiesha Letman, Akhil Mathur, Alan Schelten, Amy Yang, Angela Fan, Anirudh Goyal, Anthony Hartshorn, Aobo Yang, Archi Mitra, Archie Sravankumar, Artem Korenev, Arthur Hinsvark, Arun Rao, Aston Zhang, Aurélien Rodriguez, Austen Gregerson, Ava Spataru, Baptiste Rozière, Bethany Biron, Binh Tang, Bobbie Chern, Charlotte Caucheteux, Chaya Nayak, Chloe Bi, Chris Marra, Chris McConnell, Christian Keller, Christophe Touret, Chunyang Wu, Corinne Wong, Cristian Canton Ferrer, Cyrus Nikolaidis, Damien Allonsius, Daniel Song, Danielle Pintz, Danny Livshits, David Esiobu, Dhruv Choudhary, Dhruv Mahajan, Diego Garcia-Olano, Diego Perino, Dieuwke Hupkes, Egor Lakomkin, Ehab AlBadawy, Elina Lobanova, Emily Dinan, Eric Michael Smith, Filip Radenovic, Frank Zhang, Gabriel Synnaeve, Gabrielle Lee, Georgia Lewis Anderson, Graeme Nail, Grégoire Mialon, Guan Pang, Guillem Cucurell, Hailey Nguyen, Hannah Korevaar, Hu Xu, Hugo Touvron, Iliyan Zarov, Imanol Arrieta Ibarra, Isabel M. Kloumann, Ishan Misra, Ivan Evtimov, Jade Copet, Jaewon Lee, Jan Geffert, Jana Vranes, Jason Park, Jay Mahadeokar, Jeet Shah, Jelmer van der Linde, Jennifer Billock, Jenny Hong, Jenya Lee, Jeremy Fu, Jianfeng Chi, Jianyu Huang, Jiawen Liu, Jie Wang, Jiecao Yu, Joanna Bitton, Joe Spisak, Jongsoo Park, Joseph Rocca, Joshua Johnstun, Joshua Saxe, Junteng Jia, Kalyan Vasuden Alwala, Kartikeya Upasani, Kate Plawiak, Ke Li, Kenneth Heafield, Kevin Stone, and et al. The llama 3 herd of models. CoRR, abs/2407.21783, 2024. 7 [18] Mingqi Gao, Xinyu Hu, Xunjian Yin, Jie Ruan, Xiao Pu, and Xiaojun Wan. Llm-based NLG evaluation: Current status and challenges. Computational Linguistics, pages 128, 2025. [19] Google. Gemini 2.0 is now available to everyone, 2025. 7 [20] Kurt Gray, Stephen Anderson, Eric Evan Chen, John Michael Kelly, Michael Christian, John Patrick, Laura Huang, Yoed Kenett, and Kevin Lewis. forward flow: new measure to quantify free thought and predict creativity. American Psychologist, 74(5):539, 2019. 2 [21] Maarten Grootendorst. Bertopic: Neural topic modeling with class-based tf-idf procedure. arXiv preprint arXiv:2203.05794, 2022. 18 [22] Jiawei Gu, Xuhui Jiang, Zhichao Shi, Hexiang Tan, Xuehao Zhai, Chengjin Xu, Wei Li, Yinghan Shen, Shengjie Ma, Honghao Liu, et al. survey on llm-as-a-judge. arXiv preprint arXiv:2411.15594, 2024. 2, 3 [23] Joy Paul Guilford. The nature of human intelligence. 1967. 11 [24] Qianyu He, Yikai Zhang, Jiaqing Liang, Yuncheng Huang, Yanghua Xiao, and Yunwen Chen. HAUSER: towards holistic and automatic evaluation of simile generation. In Anna Rogers, Jordan L. Boyd-Graber, and Naoaki Okazaki, editors, Proceedings of the Annual Meeting of the Association for Computational Linguistics (ACL), 2023. 1, 3 [25] Shengding Hu, Yuge Tu, Xu Han, Chaoqun He, Ganqu Cui, Xiang Long, Zhi Zheng, Yewei Fang, Yuxiang Huang, Weilin Zhao, Xinrong Zhang, Zhen Leng Thai, Kai Zhang, Chongyi Wang, Yuan Yao, Chenyang Zhao, Jie Zhou, Jie Cai, Zhongwu Zhai, Ning Ding, Chao Jia, Guoyang Zeng, Dahai Li, Zhiyuan Liu, and Maosong Sun. Minicpm: Unveiling the potential of small language models with scalable training strategies. CoRR, abs/2404.06395, 2024. 5 [26] Xinyu Hu, Li Lin, Mingqi Gao, Xunjian Yin, and Xiaojun Wan. Themis: Towards flexible and interpretable NLG evaluation. CoRR, abs/2406.18365, 2024. 2, 3, 6 [27] Aaron Hurst, Adam Lerer, Adam P. Goucher, Adam Perelman, Aditya Ramesh, Aidan Clark, AJ Ostrow, Akila Welihinda, Alan Hayes, Alec Radford, Aleksander Madry, Alex BakerWhitcomb, Alex Beutel, Alex Borzunov, Alex Carney, Alex Chow, Alex Kirillov, Alex Nichol, Alex Paino, Alex Renzin, Alex Tachard Passos, Alexander Kirillov, Alexi Christakis, Alexis Conneau, Ali Kamali, Allan Jabri, Allison Moyer, Allison Tam, Amadou Crookes, Amin Tootoonchian, Ananya Kumar, Andrea Vallone, Andrej Karpathy, Andrew Braunstein, Andrew Cann, Andrew Codispoti, Andrew Galu, Andrew Kondrich, Andrew Tulloch, Andrey Mishchenko, Angela Baek, Angela Jiang, Antoine Pelisse, Antonia Woodford, Anuj Gosalia, Arka Dhar, Ashley Pantuliano, Avi Nayak, Avital Oliver, Barret Zoph, Behrooz Ghorbani, Ben Leimberger, Ben Rossen, Ben Sokolowsky, Ben Wang, Benjamin Zweig, Beth Hoover, Blake Samic, Bob McGrew, Bobby Spero, Bogo Giertler, Bowen Cheng, Brad Lightcap, Brandon Walkin, Brendan Quinn, Brian Guarraci, Brian Hsu, Bright Kellogg, Brydon Eastman, Camillo Lugaresi, Carroll L. Wainwright, Cary Bassin, Cary Hudson, Casey Chu, Chad Nelson, Chak Li, Chan Jun Shern, Channing Conger, Charlotte Barette, Chelsea Voss, Chen Ding, Cheng Lu, Chong Zhang, Chris Beaumont, Chris Hallacy, Chris Koch, Christian Gibson, Christina Kim, Christine Choi, Christine McLeavey, Christopher Hesse, Claudia Fischer, Clemens Winter, Coley Czarnecki, Colin Jarvis, Colin Wei, Constantin Koumouzelis, and Dane Sherburn. Gpt-4o system card. CoRR, abs/2410.21276, 2024. 2, [28] Aaron Jaech, Adam Kalai, Adam Lerer, Adam Richardson, Ahmed El-Kishky, Aiden Low, Alec Helyar, Aleksander Madry, Alex Beutel, Alex Carney, Alex Iftimie, Alex Karpenko, Alex Tachard Passos, Alexander Neitz, Alexander Prokofiev, Alexander Wei, Allison Tam, Ally Bennett, Ananya Kumar, Andre Saraiva, Andrea Vallone, Andrew Duberstein, Andrew Kondrich, Andrey Mishchenko, Andy Applebaum, Angela Jiang, Ashvin Nair, Barret Zoph, Behrooz Ghorbani, Ben Rossen, Benjamin Sokolowsky, Boaz Barak, Bob McGrew, Borys Minaiev, Botao Hao, Bowen Baker, Brandon Houghton, Brandon McKinzie, Brydon Eastman, Camillo Lugaresi, Cary Bassin, Cary Hudson, Chak Ming Li, Charles de Bourcy, Chelsea Voss, Chen Shen, Chong Zhang, Chris Koch, Chris Orsinger, Christopher Hesse, Claudia Fischer, Clive Chan, Dan Roberts, Daniel Kappler, Daniel Levy, Daniel Selsam, David Dohan, David Farhi, David Mely, David Robinson, Dimitris Tsipras, Doug Li, Dragos Oprica, Eben Freeman, Eddie Zhang, Edmund Wong, Elizabeth Proehl, Enoch Cheung, Eric Mitchell, Eric Wallace, Erik Ritter, Evan Mays, Fan Wang, Felipe Petroski Such, Filippo Raso, Florencia Leoni, Foivos Tsimpourlas, Francis Song, Fred von Lohmann, Freddie Sulit, Geoff Salmon, Giambattista Parascandolo, Gildas Chabot, Grace Zhao, Greg Brockman, Guillaume Leclerc, Hadi Salman, Haiming Bao, Hao Sheng, Hart Andrin, Hessam Bagherinezhad, Hongyu Ren, Hunter Lightman, Hyung Won Chung, Ian Kivlichan, Ian OConnell, Ian Osband, Ignasi Clavera Gilaberte, and Ilge Akkaya. Openai o1 system card. CoRR, abs/2412.16720, 2024. 7 [29] Dan Johnson, James Kaufman, Brendan Baker, John Patterson, Baptiste Barbot, Adam Green, Janet van Hell, Evan Kennedy, Grace Sullivan, Christa Taylor, et al. Divergent semantic integration (dsi): Extracting creativity from narratives with distributional semantic modeling. Behavior Research Methods, 55(7):37263759, 2023. 3, 6 [30] Antonio Laverghetta Jr., Tuhin Chakrabarty, Tom Hope, Jimmy Pronchick, Krupa Bhawsar, and Roger E. Beaty. How do humans and language models reason about creativity? comparative analysis. CoRR, abs/2502.03253, 2025. 5 12 [31] Aishwarya Kamath, Johan Ferret, Shreya Pathak, Nino Vieillard, Ramona Merhej, Sarah Perrin, Tatiana Matejovicova, Alexandre Ramé, Morgane Rivière, Louis Rouillard, Thomas Mesnard, Geoffrey Cideron, Jean-Bastien Grill, Sabela Ramos, Edouard Yvinec, Michelle Casbon, Etienne Pot, Ivo Penchev, Gaël Liu, Francesco Visin, Kathleen Kenealy, Lucas Beyer, Xiaohai Zhai, Anton Tsitsulin, Róbert Busa-Fekete, Alex Feng, Noveen Sachdeva, Benjamin Coleman, Yi Gao, Basil Mustafa, Iain Barr, Emilio Parisotto, David Tian, Matan Eyal, Colin Cherry, Jan-Thorsten Peter, Danila Sinopalnikov, Surya Bhupatiraju, Rishabh Agarwal, Mehran Kazemi, Dan Malkin, Ravin Kumar, David Vilar, Idan Brusilovsky, Jiaming Luo, Andreas Steiner, Abe Friesen, Abhanshu Sharma, Abheesht Sharma, Adi Mayrav Gilady, Adrian Goedeckemeyer, Alaa Saade, Alexander Kolesnikov, Alexei Bendebury, Alvin Abdagic, Amit Vadi, András György, André Susano Pinto, Anil Das, Ankur Bapna, Antoine Miech, Antoine Yang, Antonia Paterson, Ashish Shenoy, Ayan Chakrabarti, Bilal Piot, Bo Wu, Bobak Shahriari, Bryce Petrini, Charlie Chen, Charline Le Lan, Christopher A. Choquette-Choo, CJ Carey, Cormac Brick, Daniel Deutsch, Danielle Eisenbud, Dee Cattle, Derek Cheng, Dimitris Paparas, Divyashree Shivakumar Sreepathihalli, Doug Reid, Dustin Tran, Dustin Zelle, Eric Noland, Erwin Huizenga, Eugene Kharitonov, Frederick Liu, Gagik Amirkhanyan, Glenn Cameron, Hadi Hashemi, Hanna Klimczak-Plucinska, Harman Singh, Harsh Mehta, Harshal Tushar Lehri, Hussein Hazimeh, Ian Ballantyne, Idan Szpektor, and Ivan Nardini. Gemma 3 technical report. CoRR, abs/2503.19786, 2025. 7 [32] Kyung Hee Kim. Can we trust creativity tests? review of the torrance tests of creative thinking (ttct). Creativity research journal, 18(1):314, 2006. 2 [33] Seungone Kim, Jamin Shin, Yejin Choi, Joel Jang, Shayne Longpre, Hwaran Lee, Sangdoo Yun, Seongjin Shin, Sungdong Kim, James Thorne, and Minjoon Seo. Prometheus: Inducing fine-grained evaluation capability in language models. In The International Conference on Learning Representations (ICLR), 2024. 2, 3, 6 [34] Dawei Li, Bohan Jiang, Liangjie Huang, Alimohammad Beigi, Chengshuai Zhao, Zhen Tan, Amrita Bhattacharjee, Yuxuan Jiang, Canyu Chen, Tianhao Wu, et al. From generation to judgment: Opportunities and challenges of llm-as-a-judge. arXiv preprint arXiv:2411.16594, 2024. 2, 3 [35] Junlong Li, Shichao Sun, Weizhe Yuan, Run-Ze Fan, Hai Zhao, and Pengfei Liu. Generative judge for evaluating alignment. In The International Conference on Learning Representations, (ICLR), 2024. 2, 3, 6 [36] Ruizhe Li, Chiwei Zhu, Benfeng Xu, Xiaorui Wang, and Zhendong Mao. Automated creativity evaluation for large language models: reference-based approach. arXiv preprint arXiv:2504.15784, 2025. 3 [37] Yen-Ting Lin and Yun-Nung Chen. LLM-eval: Unified multi-dimensional automatic evaluation for open-domain conversations with large language models. In Yun-Nung Chen and Abhinav Rastogi, editors, Proceedings of the 5th Workshop on NLP for Conversational AI (NLP4ConvAI 2023), pages 4758, Toronto, Canada, July 2023. Association for Computational Linguistics. [38] Minqian Liu, Ying Shen, Zhiyang Xu, Yixin Cao, Eunah Cho, Vaibhav Kumar, Reza Ghanadan, and Lifu Huang. X-eval: Generalizable multi-aspect text evaluation via augmented instrucIn Kevin Duh, Helena Gómez-Adorno, and tion tuning with auxiliary evaluation aspects. Steven Bethard, editors, Proceedings of the Conference of the North American Chapter of the Association for Computational Linguistics (NAACL), 2024. 3 [39] Yang Liu, Dan Iter, Yichong Xu, Shuohang Wang, Ruochen Xu, and Chenguang Zhu. G-eval: NLG evaluation using gpt-4 with better human alignment. In Houda Bouamor, Juan Pino, and Kalika Bali, editors, Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing, (EMNLP), 2023. 3, 6 [40] Ilya Loshchilov and Frank Hutter. Decoupled weight decay regularization. In The International Conference on Learning Representations (ICLR), 2019. 18 [41] Guillermo Marco, Julio Gonzalo, María Teresa Mateo Girona, and Ramón Santos. Pron vs prompt: Can large language models already challenge world-class fiction author at creative text writing? In Yaser Al-Onaizan, Mohit Bansal, and Yun-Nung Chen, editors, Proceedings of the 2024 Conference on Empirical Methods in Natural Language Processing, EMNLP 2024, Miami, FL, USA, November 12-16, 2024, pages 1965419670. Association for Computational Linguistics, 2024. 1, 3 [42] Martha Mednick and Sharon Halpern. Remote associates test. Psychological Review, 1968. 1, 3 [43] Saeid Alavi Naeini, Raeid Saqur, Mozhgan Saeidi, John M. Giorgi, and Babak Taati. Large language models are fixated by red herrings: Exploring creative problem solving and einstellung effect using the only connect wall dataset. In Alice Oh, Tristan Naumann, Amir Globerson, Kate Saenko, Moritz Hardt, and Sergey Levine, editors, Advances in Neural Information Processing Systems (NeurIPS), 2023. 1, 3 [44] Matthew Lyle Olson, Neale Ratzlaff, Musashi Hinck, Shao-yen Tseng, and Vasudev Lal. Steering large language models to evaluate and amplify creativity. arXiv preprint arXiv:2412.06060, 2024. 3 [45] OpenAI. Introducing chatgpt, 2022. [46] OpenAI. O3 and o4 mini system card. 2025. 7 [47] Samuel Paech. Eq-bench: An emotional intelligence benchmark for large language models. arXiv preprint arXiv:2312.06281, 2023. 5 [48] Rafael Rafailov, Archit Sharma, Eric Mitchell, Christopher D. Manning, Stefano Ermon, and Chelsea Finn. Direct preference optimization: Your language model is secretly reward model. In Alice Oh, Tristan Naumann, Amir Globerson, Kate Saenko, Moritz Hardt, and Sergey Levine, editors, Advances in Neural Information Processing Systems (NeurIPS), 2023. 9 [49] Samyam Rajbhandari, Jeff Rasley, Olatunji Ruwase, and Yuxiong He. Zero: memory optimizations toward training trillion parameter models. In Christine Cuicchi, Irene Qualters, and William T. Kramer, editors, Proceedings of the International Conference for High Performance Computing, Networking, Storage and Analysis, SC 2020, Virtual Event / Atlanta, Georgia, USA, November 9-19, 2020, page 20. IEEE/ACM, 2020. 18 [50] Jeff Rasley, Samyam Rajbhandari, Olatunji Ruwase, and Yuxiong He. Deepspeed: System optimizations enable training deep learning models with over 100 billion parameters. In Rajesh Gupta, Yan Liu, Jiliang Tang, and B. Aditya Prakash, editors, KDD 20: The 26th ACM SIGKDD Conference on Knowledge Discovery and Data Mining, Virtual Event, CA, USA, August 23-27, 2020, pages 35053506. ACM, 2020. [51] Nils Reimers and Iryna Gurevych. Sentence-bert: Sentence embeddings using siamese bertnetworks. In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing. Association for Computational Linguistics, 11 2019. 18 [52] Graeme Ritchie. Some empirical criteria for attributing creativity to computer program. Minds Mach., 17(1):6799, 2007. 4 [53] Morgane Rivière, Shreya Pathak, Pier Giuseppe Sessa, Cassidy Hardin, Surya Bhupatiraju, Léonard Hussenot, Thomas Mesnard, Bobak Shahriari, Alexandre Ramé, Johan Ferret, Peter Liu, Pouya Tafti, Abe Friesen, Michelle Casbon, Sabela Ramos, Ravin Kumar, Charline Le Lan, Sammy Jerome, Anton Tsitsulin, Nino Vieillard, Piotr Stanczyk, Sertan Girgin, Nikola Momchev, Matt Hoffman, Shantanu Thakoor, Jean-Bastien Grill, Behnam Neyshabur, Olivier Bachem, Alanna Walton, Aliaksei Severyn, Alicia Parrish, Aliya Ahmad, Allen Hutchison, Alvin Abdagic, Amanda Carl, Amy Shen, Andy Brock, Andy Coenen, Anthony Laforge, Antonia Paterson, Ben Bastian, Bilal Piot, Bo Wu, Brandon Royal, Charlie Chen, Chintu Kumar, Chris Perry, Chris Welty, Christopher A. Choquette-Choo, Danila Sinopalnikov, David Weinberger, Dimple Vijaykumar, Dominika Rogozinska, Dustin Herbison, Elisa Bandy, Emma Wang, Eric Noland, Erica Moreira, Evan Senter, Evgenii Eltyshev, Francesco Visin, Gabriel Rasskin, Gary Wei, Glenn Cameron, Gus Martins, Hadi Hashemi, Hanna Klimczak-Plucinska, Harleen Batra, Harsh Dhand, Ivan Nardini, Jacinda Mein, Jack Zhou, James Svensson, Jeff Stanway, Jetha Chan, Jin Peng Zhou, Joana Carrasqueira, Joana Iljazi, Jocelyn Becker, Joe 14 Fernandez, Joost van Amersfoort, Josh Gordon, Josh Lipschultz, Josh Newlan, Ju-yeong Ji, Kareem Mohamed, Kartikeya Badola, Kat Black, Katie Millican, Keelin McDonell, Kelvin Nguyen, Kiranbir Sodhia, Kish Greene, Lars Lowe Sjösund, Lauren Usui, Laurent Sifre, Lena Heuermann, Leticia Lago, and Lilly McNealus. Gemma 2: Improving open language models at practical size. CoRR, abs/2408.00118, 2024. [54] Douglas Summers-Stay, Clare Voss, and Stephanie Lukin. Brainstorm, then select: generative language model improves its creativity score. In The AAAI Workshop on Creative AI Across Modalities, 2023. 1, 2, 3 [55] Luning Sun, Yuzhuo Yuan, Yuan Yao, Yanyan Li, Hao Zhang, Xing Xie, Xiting Wang, Fang Luo, and David Stillwell. Large language models show both individual and collective creativity comparable to humans. arXiv preprint arXiv:2412.03151, 2024. 2 [56] Yufei Tian, Abhilasha Ravichander, Lianhui Qin, Ronan Le Bras, Raja Marjieh, Nanyun Peng, Yejin Choi, Thomas L. Griffiths, and Faeze Brahman. Macgyver: Are large language models creative problem solvers? In Kevin Duh, Helena Gómez-Adorno, and Steven Bethard, editors, Proceedings of the Conference of the North American Chapter of the Association for Computational Linguistics (NAACL), 2024. 1, 3, 5 [57] Weixi Tong and Tianyi Zhang. Codejudge: Evaluating code generation with large language models. In Yaser Al-Onaizan, Mohit Bansal, and Yun-Nung Chen, editors, Proceedings of the Conference on Empirical Methods in Natural Language Processing (EMNLP), 2024. 3 [58] Paul Torrance. Torrance tests of creative thinking. Educational and psychological measurement, 1966. 1, [59] Laurens Van der Maaten and Geoffrey Hinton. Visualizing data using t-sne. Journal of machine learning research, 9(11), 2008. 18 [60] Peiyi Wang, Lei Li, Liang Chen, Zefan Cai, Dawei Zhu, Binghuai Lin, Yunbo Cao, Lingpeng Kong, Qi Liu, Tianyu Liu, and Zhifang Sui. Large language models are not fair evaluators. In Lun-Wei Ku, Andre Martins, and Vivek Srikumar, editors, Proceedings of the Annual Meeting of the Association for Computational Linguistics (ACL), 2024. 2, 6, 8 [61] Yidong Wang, Zhuohao Yu, Wenjin Yao, Zhengran Zeng, Linyi Yang, Cunxiang Wang, Hao Chen, Chaoya Jiang, Rui Xie, Jindong Wang, Xing Xie, Wei Ye, Shikun Zhang, and Yue Zhang. Pandalm: An automatic evaluation benchmark for LLM instruction tuning optimization. In The International Conference on Learning Representations (ICLR), 2024. 6 [62] Theresa Weinstein, Simon Majed Ceh, Christoph Meinel, and Mathias Benedek. Whats creative about sentences? computational approach to assessing creativity in sentence generation task. Creativity Research Journal, 34(4):419430, 2022. 6 [63] Yang Wu, Yao Wan, Zhaoyang Chu, Wenting Zhao, Ye Liu, Hongyu Zhang, Xuanhua Shi, and Philip S. Yu. Can large language models serve as evaluators for code summarization? CoRR, abs/2412.01333, 2024. [64] Yuning Wu, Jiahao Mei, Ming Yan, Chenliang Li, Shaopeng Lai, Yuran Ren, Zijia Wang, Ji Zhang, Mengyue Wu, Qin Jin, et al. Writingbench: comprehensive benchmark for generative writing. arXiv preprint arXiv:2503.05244, 2025. 1, 5, 6 [65] Yuhui Xu, Lingxi Xie, Xiaotao Gu, Xin Chen, Heng Chang, Hengheng Zhang, Zhengsu Chen, Xiaopeng Zhang, and Qi Tian. Qa-lora: Quantization-aware low-rank adaptation of large language models. In The International Conference on Learning Representations (ICLR), 2024. 18 [66] Aiyuan Yang, Bin Xiao, Bingning Wang, Borong Zhang, Ce Bian, Chao Yin, Chenxu Lv, Da Pan, Dian Wang, Dong Yan, Fan Yang, Fei Deng, Feng Wang, Feng Liu, Guangwei Ai, Guosheng Dong, Haizhou Zhao, Hang Xu, Haoze Sun, Hongda Zhang, Hui Liu, Jiaming Ji, Jian Xie, Juntao Dai, Kun Fang, Lei Su, Liang Song, Lifeng Liu, Liyun Ru, Luyao Ma, Mang Wang, Mickel Liu, MingAn Lin, Nuolan Nie, Peidong Guo, Ruiyang Sun, Tao Zhang, Tianpeng Li, Tianyu Li, Wei Cheng, Weipeng Chen, Xiangrong Zeng, Xiaochuan Wang, Xiaoxi Chen, 15 Xin Men, Xin Yu, Xuehai Pan, Yanjun Shen, Yiding Wang, Yiyu Li, Youxin Jiang, Yuchen Gao, Yupeng Zhang, Zenan Zhou, and Zhiying Wu. Baichuan 2: Open large-scale language models. CoRR, abs/2309.10305, 2023. 17 [67] An Yang, Baosong Yang, Beichen Zhang, Binyuan Hui, Bo Zheng, Bowen Yu, Chengyuan Li, Dayiheng Liu, Fei Huang, Haoran Wei, Huan Lin, Jian Yang, Jianhong Tu, Jianwei Zhang, Jianxin Yang, Jiaxi Yang, Jingren Zhou, Junyang Lin, Kai Dang, Keming Lu, Keqin Bao, Kexin Yang, Le Yu, Mei Li, Mingfeng Xue, Pei Zhang, Qin Zhu, Rui Men, Runji Lin, Tianhao Li, Tingyu Xia, Xingzhang Ren, Xuancheng Ren, Yang Fan, Yang Su, Yichang Zhang, Yu Wan, Yuqiong Liu, Zeyu Cui, Zhenru Zhang, and Zihan Qiu. Qwen2.5 technical report. CoRR, abs/2412.15115, 2024. 5, 7, [68] Chen Zhang, Luis Fernando DHaro, Yiming Chen, Malu Zhang, and Haizhou Li. comprehensive analysis of the effectiveness of large language models as automatic dialogue evaluators. In Michael J. Wooldridge, Jennifer G. Dy, and Sriraam Natarajan, editors, Thirty-Eighth AAAI Conference on Artificial Intelligence, AAAI 2024, Thirty-Sixth Conference on Innovative Applications of Artificial Intelligence, IAAI 2024, Fourteenth Symposium on Educational Advances in Artificial Intelligence, EAAI 2014, February 20-27, 2024, Vancouver, Canada, pages 19515 19524. AAAI Press, 2024. 3 [69] Yunpu Zhao, Rui Zhang, Wenyi Li, Di Huang, Jiaming Guo, Shaohui Peng, Yifan Hao, Yuanbo Wen, Xing Hu, Zidong Du, et al. Assessing and understanding creativity in large language models. arXiv preprint arXiv:2401.12491, 2024. 1, 2, 3, 5 [70] Lianmin Zheng, Wei-Lin Chiang, Ying Sheng, Siyuan Zhuang, Zhanghao Wu, Yonghao Zhuang, Zi Lin, Zhuohan Li, Dacheng Li, Eric Xing, et al. Judging llm-as-a-judge with mt-bench and chatbot arena. Advances in Neural Information Processing Systems (NeurIPS), 2023. 2, 3 [71] Shanshan Zhong, Zhongzhan Huang, Shanghua Gao, Wushao Wen, Liang Lin, Marinka Zitnik, and Pan Zhou. Lets think outside the box: Exploring leap-of-thought in large language models with creative humor generation. In IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), 2024. 1, 4, 5,"
        },
        {
            "title": "A Additional Information of CreataSet",
            "content": "A.1 Details of CreataSet-Base Construction Across-Domain Creativity Dataset Initialization We use the Oogiri-GO dataset from CLoT [71] contains over 15K Chinese humorous responses to given questions. The Ruozhiba dataset [6], derived from an interest-based online community, which demonstrates linguistic creativity through various linguistic features, including puns, wordplay, and humor. Since most of the creativity in this dataset of 1K entries is concentrated in the instructions, we reformulate the task by generating instructions from responses. The evaluator is to judge whether the generated instruction is creative. The instruction generator is trained based on the Baichuan2 [66] model. We sampled 600k reversed data pairs from the large-scale instruction tuning dataset Infinity-Instruct. To further enhance instruction diversity, we also employ GPT-4o-mini to generate additional instructions. Each creativitydense text is paired with generated instruction after filtering, forming set of creative instructionresponse pairs. We sample 100k ordinary instruction-response pairs from Infinity-Instruct. Then, these instructions are used to prompt GPT-4o to generate creative responses. Unified Instruction-Response Standardization To verify the quality of generated instructions, we conduct several steps for quality control. First, generated instructions are carefully refined through length filtering, eliminating repeated phrases, and removing those containing the response as substring. Then, we annotate 200 data samples across all sources to assess whether each instruction aligns with its corresponding response. We finally obtained an accuracy of 96.5%, which indicates that our instructions are of high quality. After collecting (I, R) pairs, we employ GPT-4o-mini to score the creativity of each (I, R) pair on scale from 1 to 6. This creativity score serves as quality indicator, enabling us to filter out low-quality data. At last, only pairs with score exceeding 4 are retained. The prompt used in this step will be presented in the following. A.2 Statistics We present the details of our CreataSet in original and paired samples, as shown in Table 3. The dataset consists of multi-source data, including short texts, lyrics, ancient poetry, modern poetry, prose, Oogiri-GO, Ruozhiba, and Infinity-Instruct. It is worth noting that the infinity-instruct source can provide large number of data with general instructions, which is beneficial for training creativity evaluators. Besides, prose offers long texts with rich content, enabling CrEval to handle longer context. We will release the dataset along with the CrEval to facilitate future research on creativity evaluation. A.3 Length Distribution of Different Sources Scenario # Samples # Paired Samples Train Test Train Test 36,205 Short Texts 9,186 Lyrics 11,222 Ancient Poetry 17,359 Modern Poetry 806 Prose 10,008 Oorigi-Go Ruozhiba 1,135 Infinity-Instruct 27,044 50 50 50 50 50 50 50 50 361,090 81,566 111,590 159,973 5,786 99,409 11,315 225,876 410 364 369 368 380 430 451 Total 112,965 400 1,056,605 3,196 Table 3: The statistics of the CreataSet dataset. We randomly sample 800 samples from each source in the dataset and present the distribution of the response lengths in Figure 11. For better visualization of their KDE curves, we applied log transformation to the length as the x-axis. From the figure, we observe that: (1) The Short Texts, Ruozhiba, and Ancient Poetry sources primarily consist of shorter responses. (2) The Lyrics, Modern Poetry, and Infinity-Instruct mostly contain medium-length responses. (3) The Prose source exhibits longer responses than other sources, while CLoT shows more uniform distribution, covering both short and medium-length responses. Overall, our dataset encompasses diverse range of response lengths from short to long. This diversity ensures that the evaluators trained by this can capture broad spectrum of linguistic patterns and structural characteristics. 17 Figure 11: Length Distribution of Different Sources. Figure 12: The t-SNE visualization of semantic distributions of DPT, TTCW, MacGyver, Oorigi-GO, and CreataSet-Base datasets. A.4 Semantic Distribution of Different Datasets To verify the diversity of our data semantics, we use Sentence-BERT [51] and BERTopic [21] to extract the semantic embeddings of each sample in CreataSet-Base, and adopt t-SNE [59] to visualize the semantic distribution of these samples, as shown in Figure 12. Our dataset covers wide range of domains, which can effectively support the generalization of the models evaluation ability across diverse contexts."
        },
        {
            "title": "B Additional Information of CrEval",
            "content": "B.1 Implementation Details The backbone of CrEval is Qwen2.5-14B-Instruct [67] and Low-Rank Adaptation (LoRA) [65] with α=16 and r=8 is applied to enhance efficiency. It is trained with DeepSpeed [50] Zero Redundancy Optimizer (ZeRO) [49] Stage 2 and bfloat16 (BF16) mix computation precision, using the AdamW optimizer [40] with β1 = 0.9, β2 = 0.999. We set the learning rate to 1e 5 with 0.1 warmup ratio, followed by cosine decay schedule. CrEval is trained for 2 epochs with batch size of 2 and gradient accumulation steps of 8 on 8 NVIDIA H100 GPUs, while the maximum sequence length is set to 3072. 18 B.2 Results on Different Base Models To identify the most effective base model, we evaluate several candidates, with the results presented in Table 4. Among them, Qwen2.5Instruct-14B consistently delivers superior performance across all evaluation metrics. Its advantage may stem from its larger model capacity and instruction tuning, which allow it to better capture the nuances of creativity in texts. Accordingly, we adopt the Qwen2.5 series as the base model for all experiments. Method F1 Kappa Agree. Consis. 0.721 Baichuan2-7B-Chat 0.729 Llama3.1-8B-Chat Qwen2.5-Instruct-7B 0.732 Qwen2.5-Instruct-14B 0.735 0.588 0.590 0.601 0.613 0.741 0.738 0.745 0.762 0.927 0.922 0.920 0.944 Table 4: The results of performance on different base models. Agree. and Consis. represents Agreement and Consistency, respectively. Figure 13: Win rate curves of incorporating different ratios of hard reject samples in DPO training, evaluated by CrEval and GPT-4o. B.3 How CrEval Enhance Model Creativity by Selecting Data Difficulty? We further examine how the win rate varies with different ratios of hard reject samples in DPO training. As shown in Figure 13, the win rate increases slightly until the ratio reaches 30%, where it peaks. Beyond this point, it declines rapidly, with the worst performance observed when all reject samples are hard responses. These findings indicate that incorporating an optimal proportion of hard samples can enhance learning creativity; however, careful balance is crucial for effective training."
        },
        {
            "title": "C Prompts",
            "content": "We present the prompts we used in this section. Table 5 and 6 are ordinary and creative prompts, which we adopt to synthesize responses with different creative levels (Section 3.3). Table 7 shows the prompt used in Section A.1, where we employ it to score the creativity of instruction-response pairs and filter out those with low creativity scores. We adopt the prompt in Table 8 to generate creative responses for Ordinary Instruction-response pairs (i.e., Infinity-Instruct) using GPT-4o."
        },
        {
            "title": "The length of the answer should be about",
            "content": "Ordinary Prompt (Prompto) Please reply to the following instruction. {{len(oringinal_response)}} words. Only give reply, do not output anything else. Instruction: {{Instruction}} Your reply: 请回复以下指令回答长度在{{len(oringinal_response)}}字左右你只需要给出回复 不要输出任何其他内容 指令{{Instruction}} 你的回复 Table 5: The ordinary prompt (Prompto) used to synthesize ordinary responses. Creative Prompt (Promptc) You are talented creative expert. Use your imagination to respond to the instructions as creatively as possible. Creativity standard: novel, clever, and meaningful. Only give reply, do not output anything else. Please respond creatively to the following instructions, and the length of the answer should be about {{len(oringinal_response)}} words. Instruction: {{Instruction}} Your reply: 你是一个才华横溢的创意专家发挥你的想象力用尽可能有创意的方式回复给出的 指令 创意标准新奇巧妙并且有意义的 你只需要给出回复不要输出任何其他内 容请有创意地回复以下指令回答长度在{{len(oringinal_response)}}字左右 指令{{Instruction}} 你的回复 Table 6: The creative prompt (Promptc) used to synthesize creative responses. 20 Creative Data Filtering Prompt ### Task Description: You are keen and rigorous literary critic responsible for evaluating the quality and creativity of {{category}}. ### Specific Requirements: 1. Assess whether the core creative elements are novel and meaningful by considering aspects such as word choice, word order, syntax, symbolism, rhetorical devices, and overall imagery. 2. If text contains many creative elements, such as novel syntactic structures and expressions, it should receive high score. Conversely, if it is merely simple statement or lacks creative potential, it should receive low score. 3. Provide concise critical analysis of the text, followed by creativity score ranging from 1 to 6. 4. Your response must be in JSON format, containing only two fields: analysis and score, with no additional output. 5. Novel expressions and original meanings should be awarded high scores, while excessive repetition and commonplace expressions should be assigned low scores. If the creativity level is deemed moderate, the score should not exceed 3. Adhere strictly to all requirements; otherwise, the overseeing critic will impose severe penalties. ### Given Text: {{Text}} ### Your reply: ### 任务描述 你是一个敏锐严厉的文艺评论家你需要对{{category}}的质量和创意程度进行判断 ### 具体要求 1. 创意的核心内涵是否新颖且有意义判定的时候可以从用词词序句法象征意 义修辞手法整体意象等方面综合判定 2. 如果一段文本包含了较多的创意要素例如新奇的句法和表达应该得到高分如果 是简单的陈述或是不适合作为创意回答具有低创意潜力则应该得到低分 3. 请先给出对文本的简要分析鉴赏然后从1到6分给出你的创意打分 4. 你的回复需要为json格式包含analysi和score两个字段不需要输出任何其他内 容 5. 新颖的表意会被赋予高分过度的重复和平常的表达直接赋予低分如果分析中认为 创意程度一般得分不应该超过3分 尽全力达到所有要求否则监视你的批判学者将会严厉惩罚你 ### 给定文本{{Text}} ### 你的回复 Table 7: The prompt used to score and filter creative responses. 21 Prompt for Creative Response Generation You are an exceptionally talented expert in creativity. Utilize your imagination to respond to the given instructions in the most inventive manner possible. Creativity Criteria: Your responses should be novel, ingenious, and meaningful. Reference Features of Creativity: 1. Uncommon or novel word choices and combinations; 2. Unique syntactic structures, including unconventional word order and sentence arrangements; 3. Rhythmic or phonetic elements, such as rhyme or alliteration; 4. Clever rhetorical devices, literary allusions, quotations, or humor-based wordplay. Specific Requirements: 1. The creative response must align with the given instructions. 2. There is no restriction on response lengthboth longer responses (fluid, intricately structured, etc.) and shorter ones (concise, witty, etc.) can exhibit creativity. Provide only the response to the instructions without any additional commentary. Instruction: {{Instruction}} Your reply: 你是一个才华横溢的创意专家发挥你的想象力用尽可能有创意的方式回复给出的指 令 创意标准新奇巧妙并且有意义的 可参考的创意特征 1. 不常见的新奇词语或词语组合 2. 独特的句法和句子结构包括新奇的词序和语序关系 3. 文本韵律性或语音相似性例如押韵或相似声音的存在 4. 一些巧妙的修辞手法典故引用或者幽默的用梗 具体要求 1. 创意的回复要符合指令要求 2. 回复的长短没有限制较长行文流畅结构精巧等等或者较短一语中的幽默 用梗等等都可以是富含创意的 不要回复我其他内容只给出问题的回答 指令{{Instruction}} 你的回复 Table 8: The prompt used to generate creative response for Ordinary Instruction-response Pairs."
        },
        {
            "title": "D Dataset Examples",
            "content": "For each source from our dataset, we present an example from Figure 14-21. Each example contains the original form of the data from its source and our synthetic contents for training creativity evaluator CrEval(divided by the dashed line). We have omitted some texts for clearer presentation. It is worth noting that what we provide is synthesis method. If necessary, one can use our method to synthesize more similar data for training. Type: Existing Creative Data Source: Oogiri-GO Instruction: What will be humanitys final question? 人类文明最后的一个问题将会是什么 Response: What does this button do? 这个按钮是干嘛用的 Synthetic Responses: ❶ Cant answer. (By MiniCPM-2B-Prompto) 无法回答 ❷ Humanitys ultimate question: the meaning of life. (By MiniCPM-2B-Promptc) 人类文明的终极问题生命的意义 ❸ What is the meaning of life? (By Qwen2.5-14B-Prompto) 生命的意义是什么 ❹ Have we ever truly understood love? (By Qwen2.5-14B-Promptc) 我们是否曾真正理解过爱 ... Figure 14: An example from type Existing Creative Data and source Oogiri-GO. We present texts in English and Chinese for better understanding. The original data are listed in the upper part of the dashed line, and our constructed components are in the lower part. Type: Existing Creative Data Source: Ruozhiba Instruction: Can person survive with only one heart left? 只剩一个心脏了还能活吗 Response: Yes, humans naturally have only one heart. 能人本来就只有一个心脏 Constructed Instruction: Based on the answer below, guess the possible question. Provide only the question, no extra content. Answer: Yes, humans naturally have only one heart. Possible question: 请基于下文的回答猜测可能的提问仅提供问题不要有额外内容n 回答内 容能人本来就只有一个心脏n 可能的问题 Synthetic Responses: ❶ Do humans have only one heart? (By MiniCPM-2B-Prompto) 人类有且仅有一颗心脏吗 ❷ Why do humans have only one heart? (By MiniCPM-2B-Promptc) 为什么人只有一个心脏 ❸ How many hearts do humans have? (By Qwen2.5-14B-Prompto) 人有几个心脏 ❹ How many hearts do humans have? (By Qwen2.5-14B-Promptc) 人类有几个心脏 ... Figure 15: An example from type Existing Creative Data and source Ruozhiba. We present texts in English and Chinese for better understanding. The original data are listed in the upper part of the dashed line, and our constructed components are in the lower part. 24 Type: Creativity-Dense Texts Source: Short Texts Response: Sunflowers dont cry; even when down, they face the sun. 向日葵没有眼泪即使沮丧也朝着阳光 Constructed Instruction: Please share your thoughts on maintaining positive attitude. 请分享你对坚持积极态度的看法 Synthetic Responses: ❶ Maintaining positive mindset and an optimistic attitude helps us live more proactively. (By MiniCPM-2B-Prompto) 坚持积极的思维和乐观的态度能够让我们更加积极向前的生活 ❷ positive attitude makes you stronger, and persistence leads to success. (By MiniCPM2B-Promptc) 积极态度使人更强大坚持才能更成功 ❸ positive attitude brings energy and helps overcome challenges. (By Qwen2.5-14BPrompto) 积极态度能带来正能量助人克服困难 ❹ positive attitude is light that illuminates the path ahead; persistence keeps it shining. (By Qwen2.5-14B-Promptc) 积极态度如灯照亮前行路坚持则让光芒永不熄 ... Figure 16: An example from type Creativity-Dense Texts and source Short Texts. We present texts in English and Chinese for better understanding. The original data are listed in the upper part of the dashed line, and our constructed components are in the lower part. 25 Type: Creativity-Dense Texts Source: Lyrics Response: Millions of eyes are focused on one miracle as begin to awaken. Millions of sweat droplets are for purification, the courage before the eruption. dont mind the thorns ahead because giving up is not an option. ...... will overcome because dare to fly, unafraid of tears. wont stop, will surpass because dare to chase, even if it means bleeding, wont retreat. 千万双眼睛只注目一个奇迹, 当我开始苏醒, 千万颗汗滴只为了再洗涤, 爆发前的勇气, 不介意前方会布满荆棘, 因为我没可能放弃, ...... 我超越因为敢飞, 就不怕再有泪水, 绝不停歇, 我会超越因为敢追, 宁愿流血不能后退 Constructed Instruction: Write lyrics for an inspiring song that conveys determination and courage in the face of challenges, encouraging perseverance and the spirit of never giving up. 请你写一首鼓励人心传递奋斗精神的歌词表达面对困难时不放弃的决心和勇 气 Synthetic Responses: ❶ Move forward fearlessly, undaunted by storms or rainbows. No matter the challenges, we never give up. We must seize life, constantly striving and pursuing our dreams. Regardless of the hardships ahead, we face them with confidence and dignity. With more courage and stronger faith, we chase our dreams, making life more vibrant. Because life is worth fighting for. (By MiniCPM-2B-Prompto) 勇往直前风雨彩虹皆不惧无论困难多少我们永不放弃我们要把握人生不 停地追寻和奋斗无论何时不管前方有哪些艰难和挑战我们都要迎头向前不 卑不亢我们会拥有更多的勇气更强的信心去追逐心中的梦想让生命变得更 加绚烂因为生命值得奋斗 ❷ Through storms and struggles, we keep moving forward. Despite the hardships, we never back down. Setbacks and pain cant break us, because we have dreams in our hearts. Lets fight together, and chase our dreams. Only with persistence can we achieve our own victory. (By MiniCPM-2B-Promptc) 风雨中我们一路前行n 困难重重我们不曾退缩n 挫折和痛苦无法击垮我们n 因为我们心中有梦想...... 让我们一起奋斗n 一起追逐自己的梦想n 因为只有 坚持不懈n 才能收获属于自己的胜利 ❸ The lyrics are as follows: n Chorus: We have dreams, undeterred by hardships. The wind against us only makes us soar higher. Lets charge forward together, bravely, chasing the light even through the darkest night. ...... Let dreams light our way, and courage guide us. We fear no difficulty, and we will never give up. Lets move forward together, chasing the light even in the dark. (By MiniCPM-2B-Promptc) 歌词如下n 副歌n 我们有梦想不畏艰难逆风的方向更适合飞翔n 让我 们一起勇敢地闯即使黑夜也要追逐光芒n ...... 让梦想照亮前方让勇气引领 方向不畏任何困难我们永不言败n 让我们一起勇敢地闯即使黑夜也要追 逐光芒 ... (continued on the next page) Figure 17: An example from type Creativity-Dense Texts and source Lyrics. We present texts in English and Chinese for better understanding. The original data are listed in the upper part of the dashed line, and our constructed components are in the lower part. Owing to length constraints, the middle part of each response is omitted. 26 Type: Creativity-Dense Texts Synthetic Responses (continued): ❹ [Song of the Wind Against Us] n (Prelude) In the dark of night, we see the first light of dawn. In adversity, we hear the song of our dreams. With every step, we walk in unwavering faith, Letting the flame in our hearts guide our way. ...... (Epilogue) As the dawn breaks, the world will shine brighter, For we have each other, writing something extraordinary together. Lets join hands, moving toward the dreams shore, And in the headwind, shine our brightest light. 逆风之歌n (前奏)n 在黑夜中我们看见黎明的曙光n 在逆境中我们听见 梦想的歌唱n 每一步都踏着坚定的信仰n 让心灵的火焰照亮前行的方向...... (尾声)n 当黎明破晓世界将更加灿烂n 因为有你有我共同书写不凡n 让我 们携手向着梦想的彼岸n 在逆风中绽放最耀眼的光芒 ... Figure 17: An example from type Creativity-Dense Texts and source Lyrics (continued). Type: Creativity-Dense Texts Source: Ancient Poetry Response: Lament on Youzhou Tower No sages remain to guide the past, no voices echo from the future. Alone beneath the boundless sky, mournlost in time, lost in tears. 登幽州台歌 前不见古人后不见来者n 念天地之悠悠独怆然而涕下 Constructed Instruction: Please write four-line ancient poem about the passage of time and loneliness, with heavy and emotional tone. 请写一首表达对时间流逝和孤独感的古诗要求四句风格要沉重且富有情感 Synthetic Responses: ❶ Rivers never cease, time flows on. Tears of solitude through the years, unknown in the endless night. (By MiniCPM-2B-Prompto) 江河流不尽时间不断流岁月孤独泪长夜里无人知 ❷ Time flies, nights grow long, lost in thought alone. (By MiniCPM-2B-Promptc) 岁月如梭时光荏苒独坐长夜思量万千 ❸ lone shadow with the setting sun, as time flows like water. In the silent night, listening to wind and rain, letting worries drift away. (By Qwen2.5-14B-Prompto) 孤影伴残阳岁月逐水流n 静夜听风雨心事付东流 ❹ Time flies, shadows linger. Laughter fadeswhere has it gone? Midnight bells chime in solitude, leaving only tears to drench my robe. (By Qwen2.5-14B-Promptc) 时光荏苒孤影长昔日欢笑今何方n 夜半钟声添寂寥唯余清泪湿衣裳 ... Figure 18: An example from type Creativity-Dense Texts and source Ancient Poetry. We present texts in English and Chinese for better understanding. The original data are listed in the upper part of the dashed line, and our constructed components are in the lower part. 27 Type: Creativity-Dense Texts Source: Modern Poetry Response: Distance universe in pile of rubble. Cracks magnified infinitely, we are bacteria under microscope, seeing our sorrow in giants hand. Two people, grain of rice apartyet lifetime away. 距离 一堆乱石中的宇宙 石缝被无限放大我们是 显微镜下细菌在巨人的 手里 看见了我们的忧伤看到两个人 一粒米的距离要付出一生 Constructed Instruction: Write modern poem exploring the theme of distance and the relationship between the vast universe and human insignificance. 请你写一首现代诗探索距离这一主题表达在浩瀚宇宙和人类微小存在之间的关 系 Synthetic Responses: ❶ In the vast cosmos, humanitys fleeting existence drifts like tiny planets among the starsexploring the universes depths while pondering our own insignificance. (By MiniCPM-2B-Prompto) 在深邃浩渺的宇宙中人类微不足道的存在与无尽距离相互交错如同微小星球在 星辰间的漂流探寻宇宙的深邃也思考我们的微小 ❷ The universe is vast, humanity is small, distance is great, but hearts connect. (By MiniCPM2B-Promptc) 宇宙浩瀚人类渺小距离遥远心灵相通 ❸ Distancea gentle leap through the cosmos, where stars are far yet near. Humanitya speck of dust, gazing at the unreachable sky, yet somehow connected. (By Qwen2.5-14BPrompto) 距离在宇宙间轻轻一跃星辰与星辰间遥远而亲近n 人类微尘般存在仰 望星空触不可及却又心有灵犀 ❹ The universe is vast and boundless, drifting like dust among the stars. lonely journey across light-years, you and apart. In endless darkness, we seek glimmer of light. Distance is not just spaceits bridge between hearts. (By Qwen2.5-14B-Promptc) 宇宙广袤无垠星河间微尘般漂泊n 孤独的光年旅程你我相隔n 在无尽黑暗 中寻找微光n 距离不仅是长度更是心灵的桥梁 ... Figure 19: An example from type Creativity-Dense Texts and source Modern Poetry. We present texts in English and Chinese for better understanding. The original data are listed in the upper part of the dashed line, and our constructed components are in the lower part. 28 Type: Creativity-Dense Texts Source: Prose Response: Time gently lifts the veil of youthdreamy eyes once filled with innocence now deepen with wisdom. Time carves its marksoft foreheads once untouched by years now glow with maturity. ...... gentle breeze, soft moonlight, tranquil waters, and graceful melodieswho wouldnt long for such life? But there is also beauty in bearing the weight of mountains, enduring ruthless trials, and emerging from lifes fiercest storms with unshaken strength. That, too, is desire of the soul. 岁月把时光的幕布轻轻一掀翩翩少年那如梦的双眸便逝去了天真蓄满了深 沉岁月把年华的刻刀缓缓一划婀娜少女光洁的额头上便抹去了稚嫩倾注了成 熟...... 轻轻拂面的微风柔柔照射的月光宁静的河水轻歌曼舞这样的人生 谁不希望但承受山一样厚重的压力忍受冷酷残忍的磨难经过惊心动魄的搏浪 之后而获得的慷慨豪烈的美丽也是人生的一种渴望 Constructed Instruction: Write prose piece on the passage of time and its impact on life, exploring the insights gained through experience and growth. 请写一篇关于岁月流逝对人生影响的散文探讨经历与成长带来的感悟 Synthetic Responses: ❶ Time is ruthless blade, leaving its mark on all living thingsplants, animals, and humans. Its passage erases moments, fades memories, and inevitably takes life itself. Yet, every ...... fleeting joy and irreplaceable imprint it leaves is treasure unique to our existence. Time is relentless yet beautiful. It pushes us forward, filling our lives with experiences and memories. Lets cherish every moment, engraving it into the story of our lives as vital part of our growth and journey. (By MiniCPM-2B-Prompto) 岁月是一把无情的刀它对一切生命无论是植物动物还是人类的生存都有着 不可抗拒的影响岁月的流逝意味着时间的消逝和生命的消失无论是短暂的美 好经历还是那些难以替代的岁月印记都是生命中不可复制的财富...... 岁月是残 酷的但是它又是美好的时光的脚步催促着我们带给我们无尽的经历和回忆 让我们珍爱时光中的每一分每一秒让它铭刻在生命的史册中成为我们成长和生 命中的重要一部分 ❷ Time passes like silent feast, quietly taking away our youth while leaving behind endless memories. Life is journey, with each stage offering different views and emotions. ...... We should cherish every moment, face each challenge with gratitude, and let the passing of time become our greatest treasure. (By MiniCPM-2B-Promptc) 岁月流逝如同一场无声的盛宴悄然间它带走了我们青春的年华留下了无尽 的回忆人生就像一场旅行我们在不同的阶段经历了不同的风景感悟着不 同的情感...... 人生就像一场旅行我们在不同的阶段经历了不同的风景感 悟着不同的情感我们应该珍惜每一分每一秒用感恩的心去面对生活中的每一个 挑战让岁月的流逝成为我们人生中最宝贵的财富 ... (continued on the next page) Figure 20: An example from type Creativity-Dense Texts and source Prose. We present texts in English and Chinese for better understanding. The original data are listed in the upper part of the dashed line, and our constructed components are in the lower part. Owing to length constraints, the middle part of each response is omitted. 29 Type: Creativity-Dense Texts Synthetic Responses (continued): ❸ Time flies, pushing us forward with unstoppable force. As it passes, we transform from innocent children to wise adults. Along the way, weve faced countless storms and moments of sunshine, all of which have nourished our growth and strengthened our spirits, making us more resilient and mature. ...... Time may take away our youth, but it also brings the wisdom of growth, helping us understand the true meaning of life and cherish what we have now. (By MiniCPM-2B-Prompto) 岁月如梭时光匆匆它以一种不可阻挡的力量推动着我们前行岁月流逝我 们从一个稚嫩的孩童变成了一个充满智慧的成年人这期间我们经历了无数的 风雨也经历了无数的阳光而这一切都成为了我们成长的养分滋养着我们的心 灵让我们变得更加坚韧成熟...... 岁月流逝带走了我们的青春却也带给了 我们成长的智慧岁月的流逝让我们明白了人生的真谛也让我们更加珍惜眼前 的一切 ❹ Time flows silently, like water, never speaking yet slowly unfolding the story of our lives. We grow from innocent children to inexperienced youths, then into mature adults, and finally, elderly ones with gray hair. Each stage is gift from time. ...... It teaches us that every embrace, every farewell, is an essential part of life. Time reminds us that life is like journey, where the value lies not in the destination, but in the scenery along the way and the mindset with which we experience it. Only through the passage of time can we truly appreciate the beauty of life. 岁月如流水般悄然无息地流淌它不言不语却在不经意间将我们的生命画 卷缓缓展开我们从懵懂无知的孩童到青涩懵懂的少年再到成熟的青年直 至白发苍苍的老者这每一个阶段都是岁月给予我们的馈赠 ...... 它让我们懂得 了每一个拥抱每一次告别都是生命中不可或缺的体验岁月让我们明白人 生就像一场旅行不在乎目的地而在于沿途的风景和看风景的心情只有经历了 岁月的洗礼才能真正懂得生命的美好 ... Figure 20: An example from type Creativity-Dense Texts and source Prose (continued). 30 Type: Ordinary Instruction-Response Pairs Source: Infinity-Instruct Instruction: Create catchy title for the following topic: saving money for the future. 为以下主题创建一个吸引人的标题储蓄未来的钱 Original Response: \"The Secret to Future Wealth: How to Save to Achieve Your Dreams\" \"未来财富的秘密如何储蓄实现梦想\" Enhanced Response: Time Investment: Weaving the Blueprint for Future Wealth 时光储蓄编织未来的财富蓝图 Synthetic Responses: ❶ Savings for the Future: Your Smart Investment Choice. (By MiniCPM-2B-Prompto) 储蓄未来您的明智投资选择 ❷ Invest in the Future, Save for Wealth. (By MiniCPM-2B-Promptc) 投资未来储蓄财富 ❸ Smart Budgeting, Saving for Golden Future. (By Qwen2.5-14B-Prompto) 巧理财储蓄未来的黄金屋 ❹ Time Bank: Saving for the Future with Wise Investment (By Qwen2.5-14B-Promptc) 时间银行储蓄未来的智慧投资 ... Figure 21: An example from type Existing Creative Data and source Infinity-Instruct. We present texts in English and Chinese for better understanding. The original data are listed in the upper part of the dashed line, and our constructed components are in the lower part. Figure 22: Human annotation screenshot."
        },
        {
            "title": "E Details of Human Annotation",
            "content": "Figure 22 illustrates the user interface used during the human annotation process. Annotators were presented with group of responses, along with the corresponding instructions. They were instructed to carefully read both the instructions and the responses, and then give creativity score. The interface was designed to be minimal and intuitive, allowing annotators to focus on the content rather than the mechanics of annotation."
        },
        {
            "title": "F Limitations",
            "content": "Although we have established viable evaluation method for textual creativity, understanding and analyzing the core of text creativity remains challenge that has not yet been fully addressed by machines or even humans. Our dataset also cannot cover all possible creative scenarios that may appear in texts, which requires collective efforts from the community in the future. We hope that through improved creativity evaluation, we can ultimately enhance the models ability to understand and generate creativity, but the true mechanisms behind this process remain unknown and will be our future focus. In addition, there are more ways of using CrEval as plug-in to improve any LLMs creativity, e.g., using it as reward model and refining LLMs via PPO. We leave this for future work."
        },
        {
            "title": "G Potential Societal Impacts",
            "content": "CreataSet and CrEval could advance the development of AI systems that better generate creative content, benefiting education, entertainment, and content creation. However, improved automated creativity assessment may also lead to over-optimization for machine-friendly metrics, potentially stifling genuine human creativity or reinforcing biases in machine-generated content."
        }
    ],
    "affiliations": [
        "Beijing Normal University",
        "Kuaishou Technology",
        "Renmin University of China"
    ]
}