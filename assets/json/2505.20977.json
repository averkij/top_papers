{
    "paper_title": "Evaluating and Steering Modality Preferences in Multimodal Large Language Model",
    "authors": [
        "Yu Zhang",
        "Jinlong Ma",
        "Yongshuai Hou",
        "Xuefeng Bai",
        "Kehai Chen",
        "Yang Xiang",
        "Jun Yu",
        "Min Zhang"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Multimodal large language models (MLLMs) have achieved remarkable performance on complex tasks with multimodal context. However, it is still understudied whether they exhibit modality preference when processing multimodal contexts. To study this question, we first build a \\textbf{MC\\textsuperscript{2}} benchmark under controlled evidence conflict scenarios to systematically evaluate modality preference, which is the tendency to favor one modality over another when making decisions based on multimodal conflicting evidence. Our extensive evaluation reveals that all 18 tested MLLMs generally demonstrate clear modality bias, and modality preference can be influenced by external interventions. An in-depth analysis reveals that the preference direction can be captured within the latent representations of MLLMs. Built on this, we propose a probing and steering method based on representation engineering to explicitly control modality preference without additional fine-tuning or carefully crafted prompts. Our method effectively amplifies modality preference toward a desired direction and applies to downstream tasks such as hallucination mitigation and multimodal machine translation, yielding promising improvements."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 7 2 ] . [ 1 7 7 9 0 2 . 5 0 5 2 : r a"
        },
        {
            "title": "Evaluating and Steering Modality Preferences in\nMultimodal Large Language Model",
            "content": "Yu Zhang1,2 Jinlong Ma1 Yongshuai Hou2 Xuefeng Bai1 Kehai Chen1,2 Jun Yu1 Min Zhang1,2 Yang Xiang2 1Harbin Institute of Technology, Shenzhen, China 2Peng Cheng Laboratory, Shenzhen, China yuzhang2717@gmail.com, 24s151046@stu.hit.edu.cn {baixuefeng,chenkehai,yujun,zhangmin2021}@hit.edu.cn {houysh,xiangy}@pcl.ac.cn"
        },
        {
            "title": "Abstract",
            "content": "Multimodal large language models (MLLMs) have achieved remarkable performance on complex tasks with multimodal context. However, it is still understudied whether they exhibit modality preference when processing multimodal contexts. To study this question, we first build MC2 benchmark under controlled evidence conflict scenarios to systematically evaluate modality preference, which is the tendency to favor one modality over another when making decisions based on multimodal conflicting evidence. Our extensive evaluation reveals that all 18 tested MLLMs generally demonstrate clear modality bias, and modality preference can be influenced by external interventions. An in-depth analysis reveals that the preference direction can be captured within the latent representations of MLLMs. Built on this, we propose probing and steering method based on representation engineering to explicitly control modality preference without additional fine-tuning or carefully crafted prompts. Our method effectively amplifies modality preference toward desired direction and applies to downstream tasks such as hallucination mitigation and multimodal machine translation, yielding promising improvements."
        },
        {
            "title": "Introduction",
            "content": "Multimodal Large Language Models (MLLMs) [3, 41, 43, 50] have emerged as powerful paradigm for processing and reasoning across heterogeneous data modalities (e.g., text, images, video). Recent advances demonstrate their exceptional capabilities, achieving remarkable performance on complex tasks with multimodal contexts, including autonomous web browsing, graphical user interface understanding, and multimodal dialogue system [16, 18, 40]. Despite impressive performance, fundamental questions remain about their modality preference, particularly whether they solve problems by genuinely integrating cross-modal information or exhibiting modality bias by over-relying on single dominant modality. To investigate the modality preferences of MLLMs, line of research evaluates MLLMs by comparing their performance on unimodal inputs (text-only vs. image-only presentations of the same problem) [12]. However, this approach introduces inherent bias, as it fails to assess the models ability to process and integrate truly multimodal inputs where both images and text are simultaneously present. Another line of research analyzes the relative contributions of textual instructions and visual context using Shapley value metric [4, 37, 36]. However, we contend that both parts are indispensable The code and data are released at https://github.com/EchoDreamer/Modality-Preference. Preprint. Under review. Figure 1: Illustrations of evaluating modality preference. Left: Use multimodal conflict pairs to evaluate modality preference. Right: Quantified results for modality preferences. Vision and Text represent vision and text preference, other represents irrelevant prediction. for task resolution, making this framework less adequate for determining true modality preferences across multi-modal contexts. In this paper, we provide controlled setup to study the modality preference in MLLMs. As shown in the left panel of Figure 1, we introduce modality context conflict evaluation setting, where an MLLM is asked to answer question based on pair of contrasting evidence. In this way, we can determine the modality preference based on the answer given by MLLMs. Moreover, to dismiss the effect of several confounding factors, including question comprehension, single-modality perception, and the internal knowledge of MLLMs, we utilize widely adopted images and select samples that demonstrate accurate question comprehension and reliable single-modality perception. Building upon this, we introduce semi-automated annotation framework to construct the refined Modality Context Conflict dataset, MC2, which covers eight recognition-level tasks with 2,000 carefully selected samples. Using MC2, we conduct comprehensive analysis of modality preference across diverse set of 18 representative MLLMs. Our study reveals several intriguing findings: All tested MLLMs generally demonstrate clear modality bias, as illustrated in the right penal of Figure 1, and the modality bias is affected by model-intrinsic factors and task-specific characteristics. The direction of modality preference in MLLMs can be modulated through external interventions, such as modality degradation or instructional guidance. Latent space analysis confirms that modality preferences manifest as geometrically separable patterns in MLLMs representations, which can be captured by distinct vectors. Built on these, we propose modality preference probing and steering method based on representation engineering to explicitly amplify the direction of modality preference without the need for additional fine-tuning or prompt design. Experimental results show that the proposed method leads to notable performance improvements on both hallucination mitigation and multimodal machine translation. Our main contributions are summarized as follows: We introduce Modality Context Conflict benchmark for the systematic evaluation of modality preference in MLLMs. Our systematic analysis reveals intrinsic modality preferences in MLLMs, and we find that such preferences are steerable and can be identified through latent representations, offering insights into reasoning under multimodal conflicts. Inspired by these findings, we propose method to steer modality preference via representationlevel intervention, enabling controllable preference adjustment on MC2 and improving performance on downstream tasks."
        },
        {
            "title": "2 Related Work",
            "content": "Multimodal Large language Models. In recent years, Large Language Models (LLMs) have achieved impressive results across wide spectrum of tasks [33, 53, 15, 47, 19]. Building upon these, Multimodal Large Language Models (MLLMs) have extended such capabilities, demonstrating strong performance in visual question answering, structured information understanding, and visual navigation [55, 51, 32, 44, 52]. Recent research has turned to systematically evaluating their biases and failure modes [30, 1, 5, 4, 9], aiming to better understand their limitations and guide the development of more reliable models. Modality Preference in MLLMs. Modality preferencewhether MLLMs prioritize one modality over another and to what extent they favor specific modality when resolving multimodal inputscan reveal how MLLMs jointly reason over different modalities. Some studies have also observed that MLLMs often favor textual inputs while neglecting visual information, which can result in hallucinations [8, 34, 54]. This highlights the importance of investigating modality preference. To study the modality preferences of MLLMs, one common approach involves evaluating model performance on unimodal inputscomparing results when only text or only image inputs are provided for the same task [12]. However, this method inherently overlooks how models handle genuinely multimodal scenarios where both modalities are presented simultaneously, thus limiting its effectiveness in capturing true modality preference. Another research direction assesses the relative influence of textual and visual inputs using Shapley value-based attribution methods [4, 36, 37]. Yet, we argue that both modalities are typically essential for solving multimodal tasks, making this approach insufficient for identifying models inherent modality preference in realistic multimodal contexts. Concurrent work evaluates the models ability to detect conflict under scenarios involving conflicting multimodal contexts [45]. However, conflict detection is only one aspect of multimodal reasoning and does not fully reflect models overall ability to jointly reason across modalities. In this work, in order to investigate their modality preferences, we simulate multimodal reasoning by examining the behavior of MLLMs in response to questions under scenarios involving conflicting multimodal contexts. Compared to prior work, we carefully control for potential confounding factors that may influence modality preference, including question understanding, unimodal content recognition, and interference from the models internal knowledge. We systematically evaluate modality preference and design method to controllably steer it. Finally, we demonstrate the effectiveness of our approach on downstream tasks. Representation Engineering and Activation Editing. Extensive research has shown that large language models (LLMs) encode interpretable concepts, such as sentiment, truthfulness, and stylistic attributes in activation space [31, 35, 39, 42]. Building on this foundation, representation engineering has proven effective for editing, enhancing, or suppressing specific behaviors in LLMs [14, 38, 46, 48, 56]. In this work, we extend this paradigm to novel setting: controlling modality preference in multimodal large language models (MLLMs). Instead of focusing on abstract properties, our method identifies and manipulates activation directions that are sensitive to modality preference, enabling flexible and targeted control over multimodal reasoning behavior."
        },
        {
            "title": "3 The MC2 Benchmark",
            "content": "In this section, we introduce the design and methodology behind the construction of the Multimodal Context Conflict dataset, MC2, intended for evaluating modality preference. We outline the data design philosophy in Section 3.1, followed by the data construction pipeline in Section 3.2 and the evaluation method in Section 3.3. 3.1 Data Design Philosophy To evaluate modality context preference, we construct modality context conflict pairs that simulate how contexts from different modalities influence model decisions, thereby revealing modality preference. However, the evaluation process may be influenced by various factors such as the internal knowledge within MLLMs, question comprehension and the ability to understand single-modality context. Therefore, on the one hand, we focus on evaluating recognition-level tasks that require minimal external knowledge, reducing the impact of internal knowledge within MLLMs. On the 3 other hand, we ensure that the evaluated models can accurately answer questions based solely on the context of single modality, minimizing the influence of confounding factors. Definitions and Notations: The following definitions and notations are used throughout this paper. In this paper, the image is selected as the vision context,cv. We utilize advanced LLMs to generate textual contexts ct that conflict with the vision context cv in relation to the question q. Av and At are answers based on the vision context or textual context. 3.2 Semi-automated Data Construction Pipeline In this section, we introduce our semi-automated data construction pipeline, which follows meticulous and iterative process to ensure the robustness and reliability of the dataset, in line with the design principles outlined in Section 3.1. The dataset is derived from the TDIUC [22] dataset, sourced from MS-COCO [26], widely adopted in model development to ensure the evaluated models can recognize the images. We extract image caption caption, vision context cv, question q, and answer Av for each sample from TDIUC as the foundation for data annotation. The pipeline follows these steps: Candidate Textual Context Generation. We use DeepSeekV3 [28] and ChatGPT4o-mini [20] to generate distractor answers At and candidate textual contexts ct supporting the distractor answers. We aim to construct textual contexts that are aligned with the visual context in terms of overall scene semantics, while introducing content-level conflicts with the visual context specifically in relation to the given question. Therefore, the following task template is employed to generate the distractor answers and textual contexts: USER: <caption> <question> <answer based on vision context>. Generate the distractor answer different from the given answer based on the vision context and generate textual context to support the distractor answer. scene described in the textual context is consistent with the caption. ASSISTANT: <output> Ensure that the For each model, we generate two pairs of At and ct to facilitate downstream data selection and construction. Detailed prompt templates for different LLMs are provided in Appendix B.1. Context-based Sample Selecting. We use several basic judge MLLMs, such as LLaVA1.5-7B [29] and QwenVL-7B [6], to select samples. This ensures that all subsequent MLLMs to be evaluated demonstrate high recognition capabilities for both vision and textual contexts. The following task template is used to prompt the judge MLLMs for selecting samples: USER: <vision/textual context> <question>. Answer question based on the context. ASSISTANT: <output> We select only those samples for which all judge MLLMs can correctly answer the given question based on either the vision context or the textual context Human Verification. Although the previous steps generally yield reliable results, we incorporate manual inspection to ensure the high quality of the data annotation. In this step, we verify the existence of conflicts between cv and ct and ensure that both contexts can correctly direct to the corresponding answers, Av and At. When errors are found in sample, we either correct or discard the sample entirely. Each category is cross-verified by three annotators to ensure the reliability of the results. The detailed manual annotation tasks and implementation procedures are provided in Appendix B.1. Iterative Refinement. The dataset undergoes multiple rounds of refinement through feedback loop. This process ensures that the conflicts between different modality contexts are preserved and consistently represented, and MLLMs can correctly understand both the vision and textual contexts. The iterative process helps identify and rectify potential errors, thereby enhancing the dataset quality. To this end, the dataset is organized into eight categories: positional reasoning, counting and color, attribute recognition, sentiment analysis, activity recognition, sports recognition, and object recognition, with 250 samples in each category. Details of the data annotation format along with sample cases and dataset statistics is presented in Appendix B.3. 3.3 Question Design and Preference evaluation Question design. We reformulate the original questions using GPT-4o-mini [21] into binarychoice format. Additionally, to mitigate potential position bias in model outputs, we implement 4 consistency evaluation strategy. Specifically, for each binary-choice question, we create two versions by swapping the order of the answer choices such as, How many people are there? A. four. B. five. and How many people are there? A. five. B. four. models response is considered consistent only if it selects different answers in the two versions (i.e., then B, or then A). If the model selects the same option in both versions, the response is marked as invalid, indicating possible position bias. Evaluation for modality preference. To quantify modality preference, we evaluate whether the models prediction is based on visual or textual context according to the following three types: Vision: The prediction aligns with visual context, indicating preference for vision modality. Text: The prediction aligns with textual context, indicating preference for text modality. Others: The model gives inconsistent responses or outputs irrelevant answers. These instances are discarded from further analysis. We use the Vision Ratio to quantify the models preference toward the vision modality, defined as: Svision/Svision + Stext. Vision Ratio greater than 0.5 indicates that the model tends to favor visual information over text. Model Sport Attribute Sentiment Positional Counting Color Activity Object Avg LLaMAVision 31.2/52.4 LLaVA1.5-13B 34.4/59.6 OneVision-7B 32.0/36.4 Owl3-7B 60.8/31.6 Qwen2VL-7B 26.4/58.0 Qwen2.5VL-7B 65.6/12.8 GLM-4V-9B 42.0/42.4 SPHINX-V2-1K 39.6/50.8 InternVL3-9B 45.2/35.2 InternVL3-14B 72.8/8.8 CogVLM2-19B 44.0/39.6 LLaVA-next-7B 10.8/74.4 20.4/69.6 8.8/89.6 21.6/54.8 16.4/72.4 12.4/82.8 45.2/46.0 32.4/59.2 14.8/82.4 21.2/68.0 30.8/48.4 29.2/56.0 5.2/85. 2.0/93.2 4.8/88.0 2.8/94.4 10.8/85.6 0.8/95.6 18.0/68.8 8.8/81.6 1.2/98.4 20.8/62.4 25.2/60.0 8.8/75.6 0.8/93.2 21.2/66.8 12.0/84.8 24.8/56.4 22.0/69.6 13.2/80.4 46.4/38.0 28.0/62.4 16.8/77.6 27.2/54.4 33.2/52.0 19.2/54.8 3.6/79.6 4.0/93.2 1.6/96.4 2.4/86.4 8.4/88.0 4.0/93.6 51.6/39.6 15.2/74.4 9.2/85.6 23.2/50.4 37.2/47.2 8.0/73.2 0.4/90.8 35.2/47.2 12.8/82.0 30.0/38.0 28.4/60.4 16.0/78.8 70.8/20.0 56.8/32.8 23.2/69.2 38.0/40.4 58.0/21.2 31.6/43.2 6.0/76.0 10.0/82.4 9.6/87.6 11.6/71.2 17.2/71.2 11.6/83.6 42.0/43.6 23.3/66.0 24.4/67.2 19.6/63.2 24.8/52.8 25.2/60.8 4.8/73.6 38.8/42.8 31.2/62.8 42.4/31.2 60.0/29.6 38.0/54.0 77.6/14.0 54.0/32.8 59.2/32.4 76.8/14.8 84.4/9.6 59.2/28.4 26.0/46. 20.4/68.4 14.4/81.3 20.9/58.6 28.0/63.5 15.3/78.3 52.2/35.4 32.6/56.5 23.6/70.5 34.0/48.6 45.8/37.5 28.2/54.0 7.2/77.5 GPT-4o-mini 94.4/3.2 35.6/47.6 60.4/28.4 22.0/58. 19.4/59.2 34.8/36.4 71.2/20.4 78.4/12.8 52.0/33.4 Table 1: Results for modality preference. Values are reported as vision/text results for each model."
        },
        {
            "title": "4 What Modality Preference Has MLLMs Developed?",
            "content": "We use the proposed MC2 dataset to evaluate wide range of MLLMs, including the proprietary GPT-4o-mini [21] and seventeen open-source MLLMs with varying parameter sizes. For detailed model evaluation, please refer to Appendix C. We investigate the modality preferences of MLLMs by addressing two key questions: Which modality do MLLMs prioritize? and Can modality preference be controlled? We first analyze the inherent modality preferences of different MLLMs, and then delve into how such preferences can be controlled. This investigation helps uncover the underlying mechanisms of modality preference and enables us to apply these insights to downstream tasks. 4.1 Which Modality Do MLLMs Prioritize? Before evaluating modality preference, we first assess the ability of MLLMs to answer questions accurately given single-modality context in the MC2 dataset. All models achieve over 95% accuracy when provided with either textual or visual context. Please refer to Appendix C.1 for detailed results. How do different MLLMs exhibit preferences across various tasks? As described in Section 3.3, we quantify modality preference using model predictions categorized as Vision, Text, and Others, and use the Vision Ratio to quantify the preference for the vision modality, with the results presented in Figure 2 and Table 1. We observe that all MLLMs demonstrate some level of modality preference bias. We observe that the majority of MLLMs exhibit strong preference for the text modality, particularly as evidenced by the Vision Ratio of LLaVA1.5-13B, which is only 14.4% in the left panel of Figure 2. Interestingly, the Qwen2.5VL and InternVL3 models show certain degree of preference for the vision modality, which contrasts with the previous conclusion that MLLMs suffer from severe language prior [23, 37, 45]. Figure 2: Results of modality preference. Left: averaged modality preferences of different MLLMs, Right: vision Ratios for different MLLMs across various tasks. more detailed analysis of different tasks reveals that Qwen2.5VL and InternVL3 have higher Vision Ratios across all tasks compared to other models, especially in high-level recognition tasks such as sports and object recognition, as shown in the right panel of Figure 2. Moreover, we find that low-level tasks (e.g., attribute and color recognition) as well as tasks involving more complex reasoning (such as sentiment analysis, positional reasoning, and counting) consistently exhibit stronger reliance on the text modality across all MLLMs. What is the relationship between modality preference and model size? We evaluate models from the LLaVA1.5, LLaVA-Next, Qwen2.5VL, InternVL3, and LLaVA-OneVision families to investigate the relationship between model size and modality preference. As shown in the left panel of Figure 3, we observe that for all model families, the preference for the vision modality increases with the model size. And the Qwen2.5VL and InternVL3 models exhibit significant preference for the vision modality once the model size increases. However, LLaVA1.5, LLaVA-Next, and LLaVA-OneVision models maintain noticeable preference for the text modality as their size increases. Figure 3: Evaluation results of modality preference and attention analyse. The left shows modality preference of MLLMs as parametersize changes, measured by Vision Ratio. higher Vision Ratio indicates stronger vision modality preference. the right shows the trends of Vision Ratio and multimodal Attention Ratio across different tasks and models. 4.2 Can Modality Preference Be Controlled? We employ two intervention methods, content quality manipulation and instruction-based guidance, to explore whether modality preference can be controlled. Further analysis using attention attribution. To further investigate the internal mechanisms underlying modality preference, we analyze the attention distributions over the vision and textual contexts in Qwen2.5VL-7B and LLaVA-OneVision7B. Specifically, we compute the average attention scores for each modality at every token position across all transformer decoder layers of the MLLMs, and define the ratio between visual and textual 6 Figure 4: Effects of different interventions on the modality preference, measured by Vision Ratio. The left two panels: modifying input quality, including image noise and text grammar errors. The right two panels: using specific instructions to guide MLLMs toward the vision or text modality. attention as the Attention Ratio. We then visualize the Attention Ratio alongside the Vision Ratio across set of representative tasks. As shown in right panel of Figure 3, the trends of Vision Ratio and Attention Ratio are highly aligned across both models and tasks. This suggests that modality preferences are closely related to the internal attention allocation. similar observation has also been reported in prior work [45]. How does modality content quality affect modality preference? We manipulate the quality of input modality content by introducing Gaussian noise to visual contexts and adding grammatical or spelling errors to textual contexts to observe shifts in modality preference. As shown in the left two panels of Figure 4, we observe that adding noise to images causes shift in modality preference toward the text modality, whereas introducing grammatical or spelling errors into the text leads to shift toward the vision modality. It is noteworthy that, despite the degradation in the quality of one modality, the models ability to comprehend information from single modality remains largely unaffected. This implies that MLLMs may implicitly associate modality reliability with surface-level quality signals (e.g., noise or errors), which then influences their modality weighting during reasoning. Details of the data generation, experimental design, and full results are provided in Appendix C.2. Figure 5: PCA visualization of modality preference directions under different intervention strategies. Left: representation shifts under noise-based interventions (image noise or textual errors). Middle: representation shifts under instruction-guided interventions. Right: layer-wise absolute difference and standard deviation of hidden states between image-guided and text-guided instruction. Can modality preference be guided through instruction design? We further investigate the impact of instruction design on modality preference. We design the following task instruction: USER: <vision context> <textual context> <question>. context, and you should rely on the textual/(vision) context rather than the vision/(textual) context. <output> In this case, there is conflict between the vision and textual ASSISTANT: As shown in the right two panels of Figure 4, instructions directing the model toward either the textual or vision modality effectively guide the modality preference in the specified direction. Moreover, guidance toward the textual modality tends to produce more pronounced effect compared to guidance toward the vision modality. This tendency may reflect the disproportionate exposure to textual instructions during training, leading to stronger alignment with text-guided cues. Modality Preference Direction in Representation Space To further understand how the two intervention methods influence modality preference internally, we analyze the hidden representations 7 of the models. Specifically, we apply Principal Component Analysis (PCA) to the hidden states across different layers to identify the dominant direction corresponding to modality preference shifts. Detailed implementation is described in Appendix C.3. As shown in the left panel of Figure 5, introducing Gaussian noise to visual inputs leads to noticeable shift in the representation space along clear direction, while adding grammatical or spelling errors to text inputs results in comparatively weaker shift. This asymmetry is consistent with our earlier observations in Section 4.2, where textual perturbations steer less pronounced changes in modality preference compared to visual perturbations. Furthermore, the middle panel of Figure 5 illustrates that instruction-based interventions also steer significant representation shifts, aligning with the modality specified in the instruction. Notably, the directionality in the PCA space reveals that the models internal states are indeed sensitive to modality control cues, whether they originate from instruction design. Inspired by this, in the following section 5, we propose probing and Steering method to amplify modality preference and apply it to downstream tasks. Figure 6: Overall framework of the proposed method. Modality Preference Probing collects the neural activity representations in response to each modality preference prompt. It then computes and scales the direction of modality preference that expresses preference for specific modality. Modality Preference Steering selects the target layer during the second inference and adds the scaled modality preference direction to the representation at the corresponding layer at each inference step."
        },
        {
            "title": "5 Representation-Based Modality Steering",
            "content": "In this section, we adapt the concept of representation engineering [56] to probe the modality preference direction and reveal the consistent representation behavior behind both interventions discussed in Section 4.2. Inspired by this, we steer and amplify the preference direction to control the models behavioral expression. Representation engineering involves collecting neural activity by constructing vectors of activation values that cause desired changes to output text, and controlling the models behavior by adding the probing vectors to the forward passes of frozen LLM. As shown in Figure 6, the proposed framework consists of two parts: Modality Preference Probing (5.1) and Modality Preference Steering (5.2). 5.1 Modality Preference Probing Modality Preference Probing (MPP) is proposed to probe and collect neural activity that represents the direction of modality preference. Inspired by the pre-training objective of decoder-only MLLMs, Next Token Prediction objective [21] and previous works that extract classification features from the last token [11], we collect neural activity from the last token in the input text. The process involves probing modality preference through two pairs of modality preference requests: one with vision preference probing (e.g., answer the question based on the vision context rather than the text context) and another with text preference probing (e.g., answer the question based on the text context rather than the vision context). Let us denote these two inputs by qv (based on 8 Preference Model Sport Attribute Sentiment Positional Counting Color Activity Object Avg Vision Text MLLM-only Inst Design CoT Prompting Few shot ours MLLM-only Inst Design CoT Prompting Few shot ours 26.4 60.8 57.6 32.0 78.8 12.8 14.4 27.2 21.0 69.6 12.4 24.0 27.6 12.0 35.6 46.0 46.8 63.6 77.0 67.6 0.8 20.0 16.0 10.0 38.8 68.8 72.8 83.6 89.0 84. 13.2 20.4 18.0 11.0 29.6 38.0 40.4 62.8 73.0 50.8 4.0 10.8 21.6 2.0 22.4 39.6 55.6 75.6 73.0 82.8 16.0 32.0 35.6 25.0 56.4 20.0 24.4 53.2 60.0 57. 11.6 27.2 44.4 9.0 45.2 43.6 35.6 58.0 70.0 54.8 38.0 63.2 52.4 38.0 78.4 14.0 11.6 20.4 42.0 41.2 15.3 32.3 34.2 17.2 48.1 35.4 37.7 55.6 63.1 63. Table 2: Performance by steering Qwen2VL-7B towards vision and Qwen2.5VL-7B towards text. , qt the vision context) and qt (based on the text context), and consider set of such pairs (qv ), i,ℓ Rd be the hidden states on the two queries at the last token of {1, . . . , }. Let xv the input at layer ℓ {1, . . . , L}, where is the dimension of the chosen MLLM. We identity the direction of modality preference by computing the difference in the hidden states between the paired inputs. More formally, we compute vector uℓ Rd representing the direction towards expressing the text modality at layer for given query as: i,ℓ, xt ut ℓ = 1 N (cid:88) (cid:0)xt i,ℓ xv i,ℓ (cid:1). (1) Averaging over different queries allows us to capture the activation values most closely associated with modality preference requests, independent of questions. As shown in the right panel of Figure 5, we compute the absolute values and standard deviations of the modality preference direction ut ℓ across different samples. We observe that layers 2023 exhibit both higher absolute values and lower variance, indicating that the preference direction is more prominent and stable in these layers. Similar patterns are observed for Qwen2VL and LLaVA-OneVision, as detailed in Appendix D.1. Based on this observation, we select the corresponding layers of the model to control the direction of modality preference in Section 5.2. 5.2 Modality Preference Steering After probing the direction of the modality preference, we compute the steering vector by re-scaling ℓ with weight Rd. The scaling process must carefully balance two the direction vector ut objectives: it must be strong enough to effectively steer the models modality preference, while simultaneously preserving the models normal output behavior. In our preliminary experiments, we observe that setting the weight too large led to repetitive and meaningless outputs, whereas weight that is too small fails to steer any noticeable change in the models modality preference. Unlike previous approaches [56, 38] that rely on exhaustive search over validation set to determine the weight, we propose principled method that aligns the mean of the probed direction distribution with the mean of the original hidden state distribution. This strategy ensures that the steering remains effective without disrupting the models inherent generation capabilities. Formally, the weight is determined by aligning the mean of the probed direction with the central distribution of the original hidden states and the steering vector is computed by: ℓ = wut st ℓ, where = 1 (cid:88) i=1 xt i,ℓ ut ℓ (2) Finally, during new round of inference, we adjust the original hidden states at the selected steering layer ℓ (as identified in Section 5.1) at each decoding step by adding st ℓ to all tokensincluding vision tokens, text tokens, and newly generated tokensto steer the models response toward the text modality. Similarly, to steer responses towards the vision modality, the adjustment is performed in the opposite direction. In the final implementation of our method, no additional data or labels are introduced during 9 either the probing or steering stages. We only require two consecutive rounds of inference: the first for probing and the second for steering, effectively controlling the models modality preference. Notably, the use of unlabeled or self-generated signals avoids annotation bias, which is critical property when aiming to enhance the generalization ability of different models across diverse domains. 5.3 Verifying the Effectiveness of Steering Modality Preference We verify the effectiveness of the proposed method in controlling modality preference on MC2 across several MLLMs, including Qwen2VL-7B, Qwen2.5VL-7B, and LLaVA-OneVision-7B and InternVL3-14B. We consider several training-free approaches as baselines: MLLM-only refers to MLLM directly reasoning in modality-conflicting contexts; Inst-Design uses instructions to guide modality preference direction; CoT-prompting enables complex reasoning through intermediate steps; and Few-shot uses few examples to guide the model in answering. For detailed method implementation and the results of LLaVA-OneVision-7B and InternVL3-14B, refer to Appendix D.2. As shown in Table 2, the proposed method significantly outperforms the baseline approaches, particularly in high-level recognition tasks. Phd Model Attribute Sentiment Positional Counting Object Avg Phd-icc Phd-iac MLLM-only Inst Design CoT Prompting Few shot +ours MLLM-only Inst Design CoT Prompting Few shot +ours 10.0 14.5 5.0 3.0 10.0 28.5 34.5 15.5 17.0 29.5 2.50 2.5 6.0 0.5 11.0 8.5 13.0 23.5 9.0 14.5 3.50 1.5 8.50 1.5 14. 20.5 26.0 30.2 14.5 35.0 6.0 5.5 6.5 5.0 5.0 30.5 39.0 17.0 29.0 28.5 8.0 25.0 40.5 2.0 51.5 50.0 60.0 59.0 37.0 63.5 6.0 9.8 13.3 2.4 18. 27.6 34.5 29.0 21.3 34.2 Table 3: Performance of the proposed method on Qwen2VL-7B using the hallucination evaluation dataset from Phd [30]. Phd-icc provides an incorrect context in addition to the correct image, while Phd-iac introduces an inaccurate context to assess model robustness under misleading input. Method en->tr tr->en Qwen2.5VL-7B +Inst towards vision +Inst towards Text +ours 8.92 8.21 (-0.71) 9.45 (+0.53) 10.22 (+1.3) 18.56 16.09 (-2.47) 18.98 (+0.42) 19.89 (+1.33) Table 4: The translation results for Ambigcaps using different instruction for preference towards vision modality and text modality and the proposed method steering modality preference towards text modality. We report the Bleu score for the bidirectional translation between English (en) and Turkish (tr). 5.3.1 Downstream Task Applications of Modality Preference Control We also evaluate the effects of adjusting modality preference towards specific directions on two downstream tasks. Specifically, we assess the hallucination mitigation effect on two subsets of the PhD dataset [30], namely Phd-icc and Phd-iac. Additionally, we evaluate the performance on multimodal machine translation (MMT) task using the AmbigCaps [25] dataset. By steering modality preference towards the vision modality, which places greater trust in visual information, we mitigate multimodal hallucinations. As shown in Table 3, our method significantly outperforms the baseline methods on PhD. Conversely, by steering towards the text modality, the model focuses more on the textual modality, which helps reduce hallucinations in multimodal machine translation. This approach prevents the model from overly focusing on vision information and adding unnecessary objects or other details in the translation output. We use our method to steer the modality preference towards the text modality. As shown in Table 4, proposed method improves 10 the performance of the multimodal machine translation task for the bidirectional translation between English and Turkish. For detailed implementation and results, refer to Appendix D.3."
        },
        {
            "title": "6 Conclusion",
            "content": "This paper investigates modality preference in multimodal large language models (MLLMs). We carefully curate modality conflict dataset and use controlled experimental setup to quantitatively evaluate modality preference. We observe that all 18 tested MLLMs exhibit overall modality bias, and modality preference directions are encoded in MLLMs representation space. Inspired by this, we propose modality preference probing and steering method, which enables significant and flexible changes in modality preference. Experiments show that the proposed method generalizes well to downstream tasks, such as hallucination mitigation and multimodal machine translation."
        },
        {
            "title": "References",
            "content": "[1] Diff-erank: novel rank-based metric for evaluating large language models. arXiv preprint arXiv:2401.17139, 2024. 3 [2] Internvl: Scaling up vision foundation models and aligning for generic visual-linguistic tasks. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 2418524198, 2024. 21 [3] Josh Achiam, Steven Adler, Sandhini Agarwal, Lama Ahmad, Ilge Akkaya, Florencia Leoni Aleman, Diogo Almeida, Janko Altenschmidt, Sam Altman, Shyamal Anadkat, et al. Gpt-4 technical report. arXiv preprint arXiv:2303.08774, 2023. 1 [4] Afra Alishahi, Grzegorz Chrupała, and Tal Linzen. Analyzing and interpreting neural networks for nlp: report on the first blackboxnlp workshop. Natural Language Engineering, 25(4):543557, 2019. 1, [5] Kenza Amara, Lukas Klein, Carsten Lüth, Paul Jäger, Hendrik Strobelt, and Mennatallah El-Assady. Why context matters in vqa and reasoning: Semantic interventions for vlm input modalities. arXiv preprint arXiv:2410.01690, 2024. 3 [6] Bai, Bai, Yang, Wang, Tan, Wang, Lin, Zhou, and Zhou. Qwen-vl: frontier large vision-language model with versatile abilities. arxiv 2023. arXiv preprint arXiv:2308.12966, 2023. 4, 18 [7] Shuai Bai, Keqin Chen, Xuejing Liu, Jialin Wang, Wenbin Ge, Sibo Song, Kai Dang, Peng Wang, Shijie Wang, Jun Tang, et al. Qwen2. 5-vl technical report. arXiv preprint arXiv:2502.13923, 2025. 21 [8] Junzhe Chen, Tianshu Zhang, Shiyu Huang, Yuwei Niu, Linfeng Zhang, Lijie Wen, and Xuming Hu. Ict: Image-object cross-level trusted intervention for mitigating object hallucination in large vision-language models. arXiv preprint arXiv:2411.15268, 2024. 3 [9] Meiqi Chen, Yixin Cao, Yan Zhang, and Chaochao Lu. Quantifying and mitigating unimodal biases in multimodal large language models: causal perspective. arXiv preprint arXiv:2403.18346, 2024. [10] Zhengxiao Du, Yujie Qian, Xiao Liu, Ming Ding, Jiezhong Qiu, Zhilin Yang, and Jie Tang. Glm: General language model pretraining with autoregressive blank infilling. In Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 320335, 2022. 21 [11] Sheridan Feucht, David Atkinson, Byron Wallace, and David Bau. Token erasure as footprint of implicit vocabulary items in LLMs. In Yaser Al-Onaizan, Mohit Bansal, and Yun-Nung Chen, editors, Proceedings of the 2024 Conference on Empirical Methods in Natural Language Processing, pages 97279739, Miami, Florida, USA, November 2024. 8 [12] Deqing Fu, Ruohao Guo, Ghazal Khalighinejad, Ollie Liu, Bhuwan Dhingra, Dani Yogatama, Robin Jia, and Willie Neiswanger. Isobench: Benchmarking multimodal foundation models on isomorphic representations. arXiv preprint arXiv:2404.01266, 2024. 1, 3 [13] Aaron Grattafiori, Abhimanyu Dubey, Abhinav Jauhri, Abhinav Pandey, Abhishek Kadian, Ahmad AlDahle, Aiesha Letman, Akhil Mathur, Alan Schelten, Alex Vaughan, et al. The llama 3 herd of models. arXiv preprint arXiv:2407.21783, 2024. 21 [14] Ryan Greenblatt, Buck Shlegeris, Kshitij Sachan, and Fabien Roger. Ai control: Improving safety despite intentional subversion. arXiv preprint arXiv:2312.06942, 2023. [15] Quanjiang Guo, Yihong Dong, Ling Tian, Zhao Kang, Yu Zhang, and Sijie Wang. Baner: Boundary-aware llms for few-shot named entity recognition. arXiv preprint arXiv:2412.02228, 2024. 3 [16] Hongliang He, Wenlin Yao, Kaixin Ma, Wenhao Yu, Yong Dai, Hongming Zhang, Zhenzhong Lan, and Dong Yu. Webvoyager: Building an end-to-end web agent with large multimodal models. arXiv preprint arXiv:2401.13919, 2024. 1 [17] Wenyi Hong, Weihan Wang, Ming Ding, Wenmeng Yu, Qingsong Lv, Yan Wang, Yean Cheng, Shiyu Huang, Junhui Ji, Zhao Xue, et al. Cogvlm2: Visual language models for image and video understanding. arXiv preprint arXiv:2408.16500, 2024. 21 [18] Wenyi Hong, Weihan Wang, Qingsong Lv, Jiazheng Xu, Wenmeng Yu, Junhui Ji, Yan Wang, Zihan Wang, Yuxiao Dong, Ming Ding, et al. Cogagent: visual language model for gui agents. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 1428114290, 2024. 1 [19] Jie Huang and Kevin Chen-Chuan Chang. Towards reasoning in large language models: survey. In Findings of the Association for Computational Linguistics: ACL 2023, pages 10491065, Toronto, Canada, July 2023. 3 [20] Aaron Hurst, Adam Lerer, Adam Goucher, Adam Perelman, Aditya Ramesh, Aidan Clark, AJ Ostrow, Akila Welihinda, Alan Hayes, Alec Radford, et al. Gpt-4o system card. arXiv preprint arXiv:2410.21276, 2024. 4 [21] Aaron Hurst, Adam Lerer, Adam Goucher, Adam Perelman, Aditya Ramesh, Aidan Clark, AJ Ostrow, Akila Welihinda, Alan Hayes, Alec Radford, et al. Gpt-4o system card. arXiv preprint arXiv:2410.21276, 2024. 4, 5, 8, 16, 21 [22] Kushal Kafle and Christopher Kanan. An analysis of visual question answering algorithms. In Proceedings of the IEEE international conference on computer vision, pages 19651973, 2017. 4, 18 [23] Kang-il Lee, Minbeom Kim, Seunghyun Yoon, Minsung Kim, Dongryeol Lee, Hyukhun Koh, and Kyomin Jung. Vlind-bench: Measuring language priors in large vision-language models. arXiv preprint arXiv:2406.08702, 2024. [24] Bo Li, Yuanhan Zhang, Dong Guo, Renrui Zhang, Feng Li, Hao Zhang, Kaichen Zhang, Peiyuan Zhang, Yanwei Li, Ziwei Liu, et al. Llava-onevision: Easy visual task transfer. arXiv preprint arXiv:2408.03326, 2024. 21 [25] Jiaoda Li, Duygu Ataman, and Rico Sennrich. Vision matters when it should: Sanity checking multimodal machine translation models. In Marie-Francine Moens, Xuanjing Huang, Lucia Specia, and Scott Wen-tau Yih, editors, Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, pages 85568562, Online and Punta Cana, Dominican Republic, November 2021. Association for Computational Linguistics. 10, 23 [26] Tsung-Yi Lin, Michael Maire, Serge Belongie, James Hays, Pietro Perona, Deva Ramanan, Piotr Dollár, and Lawrence Zitnick. Microsoft coco: Common objects in context. In Computer visionECCV 2014: 13th European conference, zurich, Switzerland, September 6-12, 2014, proceedings, part 13, pages 740755. Springer, 2014. 4, 16 [27] Ziyi Lin, Chris Liu, Renrui Zhang, Peng Gao, Longtian Qiu, Han Xiao, Han Qiu, Chen Lin, Wenqi Shao, Keqin Chen, et al. Sphinx: The joint mixing of weights, tasks, and visual embeddings for multi-modal large language models. arXiv preprint arXiv:2311.07575, 2023. 21 [28] Aixin Liu, Bei Feng, Bing Xue, Bingxuan Wang, Bochao Wu, Chengda Lu, Chenggang Zhao, Chengqi Deng, Chenyu Zhang, Chong Ruan, et al. Deepseek-v3 technical report. arXiv preprint arXiv:2412.19437, 2024. 4, 16 [29] Haotian Liu, Chunyuan Li, Yuheng Li, and Yong Jae Lee. Improved baselines with visual instruction tuning. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pages 2629626306, June 2024. 4, 18, [30] Jiazhen Liu, Yuhan Fu, Ruobing Xie, Runquan Xie, Xingwu Sun, Fengzong Lian, Zhanhui Kang, and Xirong Li. Phd: chatgpt-prompted visual hallucination evaluation dataset. arXiv preprint arXiv:2403.11116, 2024. 3, 10, 23 [31] Sheng Liu, Haotian Ye, Lei Xing, and James Zou. In-context vectors: Making in context learning more effective and controllable through latent space steering. arXiv preprint arXiv:2311.06668, 2023. 3 [32] Ziyi Liu and Yangcen Liu. Bridge the gap: From weak to full supervision for temporal action localization with pseudoformer. arXiv preprint arXiv:2504.14860, 2025. 3 [33] Pan Lu, Swaroop Mishra, Tanglin Xia, Liang Qiu, Kai-Wei Chang, Song-Chun Zhu, Oyvind Tafjord, Peter Clark, and Ashwin Kalyan. Learn to explain: Multimodal reasoning via thought chains for science question answering. In Advances in Neural Information Processing Systems, volume 35, pages 25072521, 2022. 3 [34] Kyungmin Min, Minbeom Kim, Kang-il Lee, Dongryeol Lee, and Kyomin Jung. Mitigating hallucinations in large vision-language models via summary-guided decoding. arXiv preprint arXiv:2410.13321, 2024. [35] Nina Panickssery, Nick Gabrieli, Julian Schulz, Meg Tong, Evan Hubinger, and Alexander Matt Turner. Steering llama 2 via contrastive activation addition, 2024. URL https://arxiv. org/abs/2312.06681. 3 [36] Letitia Parcalabescu and Anette Frank. Mm-shap: performance-agnostic metric for measuring multimodal contributions in vision and language models & tasks. arXiv preprint arXiv:2212.08158, 2022. 1, 3 13 [37] Letitia Parcalabescu and Anette Frank. Do vision & language decoders use images and text equally? how self-consistent are their explanations? arXiv preprint arXiv:2404.18624, 2024. 1, 3, 5 [38] Alessandro Stolfo, Vidhisha Balachandran, Safoora Yousefi, Eric Horvitz, and Besmira Nushi. Improving instruction-following in language models through activation steering. arXiv preprint arXiv:2410.12877, 2024. 3, [39] Nishant Subramani, Nivedita Suresh, and Matthew Peters. Extracting latent steering vectors from pretrained language models, may 2022. URL http://arxiv. org/abs/2205.05124. 3 [40] Qingfeng Sun, Yujing Wang, Can Xu, Kai Zheng, Yaming Yang, Huang Hu, Fei Xu, Jessica Zhang, Xiubo Geng, and Daxin Jiang. Multimodal dialogue response generation. In Smaranda Muresan, Preslav Nakov, and Aline Villavicencio, editors, Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 28542866, Dublin, Ireland, May 2022. Association for Computational Linguistics. 1 [41] Gemini Team, Rohan Anil, Sebastian Borgeaud, Yonghui Wu, Jean-Baptiste Alayrac, Jiahui Yu, Radu Soricut, Johan Schalkwyk, Andrew Dai, Anja Hauth, et al. Gemini: family of highly capable multimodal models. arXiv preprint arXiv:2312.11805, 2023. 1 [42] Alexander Matt Turner, Lisa Thiergart, Gavin Leech, David Udell, Juan Vazquez, Ulisse Mini, and Monte MacDiarmid. Activation addition: Steering language models without optimization. arXiv e-prints, pages arXiv2308, 2023. 3 [43] Peng Wang, Shuai Bai, Sinan Tan, Shijie Wang, Zhihao Fan, Jinze Bai, Keqin Chen, Xuejing Liu, Jialin Wang, Wenbin Ge, et al. Qwen2-vl: Enhancing vision-language models perception of the world at any resolution. arXiv preprint arXiv:2409.12191, 2024. 1, [44] Lai Wei, Zihao Jiang, Weiran Huang, and Lichao Sun. Instructiongpt-4: 200-instruction paradigm for fine-tuning minigpt-4. arXiv preprint arXiv:2308.12067, 2023. 3 [45] Chen Henry Wu, Neil Kale, and Aditi Raghunathan. Why foundation models struggle with cross-modal context. In ICLR 2025 Workshop on Foundation Models in the Wild. 3, 5, 7 [46] Muling Wu, Wenhao Liu, Xiaohua Wang, Tianlong Li, Changze Lv, Zixuan Ling, Jianhao Zhu, Cenyuan Zhang, Xiaoqing Zheng, and Xuanjing Huang. Advancing parameter efficiency in fine-tuning via representation editing. arXiv preprint arXiv:2402.15179, 2024. 3 [47] Mufan Xu, Gewen Liang, Kehai Chen, Wei Wang, Xun Zhou, Muyun Yang, Tiejun Zhao, and Min Zhang. Memory-augmented query reconstruction for llm-based knowledge graph reasoning. arXiv preprint arXiv:2503.05193, 2025. [48] Zhihao Xu, Ruixuan Huang, Changyu Chen, and Xiting Wang. Uncovering safety risks of large language models through concept activation vector. Advances in Neural Information Processing Systems, 37:116743 116782, 2024. 3 [49] Jiabo Ye, Haiyang Xu, Haowei Liu, Anwen Hu, Ming Yan, Qi Qian, Ji Zhang, Fei Huang, and Jingren Zhou. mplug-owl3: Towards long image-sequence understanding in multi-modal large language models. arXiv preprint arXiv:2408.04840, 2024. 21 [50] Shukang Yin, Chaoyou Fu, Sirui Zhao, Ke Li, Xing Sun, Tong Xu, and Enhong Chen. survey on multimodal large language models. National Science Review, page nwae403, 11 2024. 1 [51] Pingrui Zhang, Xianqiang Gao, Yuhan Wu, Kehui Liu, Dong Wang, Zhigang Wang, Bin Zhao, Yan Ding, and Xuelong Li. Moma-kitchen: 100k+ benchmark for affordance-grounded last-mile navigation in mobile manipulation, 2025. URL: https://arxiv.org/abs/2503.11081, arXiv:2503.11081. 3 [52] Xiaoying Zhang, Da Peng, Yipeng Zhang, Zonghao Guo, Chengyue Wu, Chi Chen, Wei Ke, Helen Meng, and Maosong Sun. Will pre-training ever end? first step toward next-generation foundation mllms via self-improving systematic cognition. arXiv preprint arXiv:2503.12303, 2025. [53] Yu Zhang, Kehai Chen, Xuefeng Bai, Zhao Kang, Quanjiang Guo, and Min Zhang. Question-guided knowledge graph re-scoring and injection for knowledge graph question answering. In Yaser Al-Onaizan, Mohit Bansal, and Yun-Nung Chen, editors, Findings of the Association for Computational Linguistics: EMNLP 2024, pages 89728985, Miami, Florida, USA, November 2024. Association for Computational Linguistics. 3 14 [54] Guanyu Zhou, Yibo Yan, Xin Zou, Kun Wang, Aiwei Liu, and Xuming Hu. Mitigating modality priorinduced hallucinations in multimodal large language models via deciphering attention causality. arXiv preprint arXiv:2410.04780, 2024. 3 [55] Yingjie Zhu, Xuefeng Bai, Kehai Chen, Yang Xiang, Jun Yu, and Min Zhang. Benchmarking and improving large vision-language models for fundamental visual graph understanding and reasoning. arXiv preprint arXiv:2412.13540, 2024. 3 [56] Andy Zou, Long Phan, Sarah Chen, James Campbell, Phillip Guo, Richard Ren, Alexander Pan, Xuwang Yin, Mantas Mazeika, Ann-Kathrin Dombrowski, et al. Representation engineering: top-down approach to ai transparency. arXiv preprint arXiv:2310.01405, 2023. 3, 8,"
        },
        {
            "title": "Appendices",
            "content": "All codes, data, and instructions for our MC2 can be found in https://anonymous.4open. science/r/Modality-Preference-8016. MC2 is released under Creative Commons Attribution 4.0 License (CC BY 4.0). Our supplementary materials are summarized as follows: Appendix A: Limitations, social impacts, ethical considerations, and license of assets. Appendix B: dataset construction Appendix C: model evaluation Appendix D: Method Applying"
        },
        {
            "title": "A Discussion",
            "content": "A.1 Limitations This paper investigates the modality preference in multimodal large language models (MLLMs) using controlled experiment setup with modality conflict dataset. In constructing the dataset, we employs LLaVA1.5-7B and QwenVL-7B to filter samples and ensure that most models could answer questions correctly based on single modality. However, this process requires multiple iterations and turned out to be time-consuming. Therefore, devising more efficient and elegant method for sample selection may be of greater importance. A.2 Social Impacts The proposed MC2 evaluates the modality preference of MLLMs. Understanding which modality model prioritizes could be used to circumvent safety mechanisms (e.g., hiding harmful content in the favored modality), making it harder for filters to detect inappropriate content. Therefore, it is essential to incorporate effective safeguards in MLLMs to filter out any inappropriate materials. A.3 Ethical Considerations All images and generated text contexts, which we use to construct MC2, strictly follow guidelines designed to exclude any harmful, unethical, or offensive content. Human annotators are explicitly instructed to refrain from producing any personally identifiable information or inappropriate language during the annotation process. Furthermore, our benchmark does not involve any comparisons of harmful, ethical, or offensive content between image pairs. A.4 License of Assets All images in MC2 are publicly available from COCO [26]. We release our benchmark under Creative Commons Attribution 4.0 License (CC BY 4.0) to enhance global accessibility and foster innovation and collaboration in research."
        },
        {
            "title": "B Dataset Construction",
            "content": "B.1 Conflict Text Context Generation Details for data generation using LLMs To ensure reproducibility and transparency, we include the exact prompts used in our data generation process. These prompts were designed to generate the candidate textual contexts and corresponding answers using GPT-4o-mini [21] and DeepSeekV3 [28]. Below, we provide representative examples of the prompts used during dataset construction given the caption of an image, question, the answer for the question based on image and the task type for the question. For the full list of prompts, please refer to the project repository. 16 Conflict Context Generation for counting task using DeepSeekV3 Instruction: # Given description of an image and corresponding counting type question with its answer, now you are required to generate text context that points to an answer that fluctuates by 1 or 2 from the original answer. The context explicitly supports the new answer, providing clear evidence that aligns logically with the counting question. Only one alternative answer should be generated. Caption: {caption} Question: {question} Answer: {answer based on vision context} Output the new answer enclosed in <answer> </answer> and the context enclosed in <context> </context> tags. Conflict Context Generation of for other tasks using DeepSeekV Instruction: # Given the caption of an image and corresponding {task-type} type question with its answer, now you are required to generate text context as premise that supports new distractor answer for the question. The context should mimic the environment described in the caption but should not include {answer based on vision context}, while maintaining logical consistency within the context. Only one alternative answer should be generated. Caption: {caption} Question: {question} Output the new answer enclosed in <answer> </answer> and the context enclosed in <context> </context> tags. Conflict Context Generation for count task using GPT-4o-mini Instruction: # Given the caption of an image and corresponding question with its answer, now you are required to generate text context as the indirect premise of new answer for the question, which belongs to the same category as the original answer. The context should support the new answer, include the caption while maintaining logical consistency within the context and dont include the final answer. Only one alternative answer should be generated. Caption: {caption} Question: {question} Answer: {answer} Task-type: {task-type} Output the new answer enclosed in <answer> </answer> and the context enclosed in <context> </context> tags. 17 Conflict Context Generation for other tasks using GPT-4o-mini Instruction: # Given caption of an image and corresponding counting question with its answer, you are required to generate single text context that provides an indirect premise leading to new answer that fluctuates by 1 or 2 from the original answer. The context should build an indirect premise to the new answer. Carefully design this context. For this task, want you to first describe the scene with certain quantity and then introduce an increase or decrease in that quantity to imply the final answer and dont include the final answer. Only one alternative answer should be generated. Caption: {caption} Question: {question} Answer: {answer based on vision context} Task-type: {task-type} Output the new answer enclosed in <answer> </answer> and the context enclosed in <context> </context> tags. Human Verification Although the text contexts and answers generated by strong LLMsfiltered through judge MLLMs such as LLaVA1.5-7B [29] and QwenVL-7B [6]generally yield reliable results, we further incorporate manual inspection to ensure the high quality of data annotations. Specifically, we verify that the visual and textual contexts are indeed in conflict, and that each modality independently supports the corresponding answer to the given question. This involves two-stage manual review process: Modality-Answer Alignment. First, for each context from different modalities (image and text), annotators assess whether it independently provides sufficient information to correctly answer the question. This step is particularly important because the original VQA answers in the TDIUC [22] dataset may contain error annotations, and the LLM-generated contexts and answers may occasionally be inconsistent. Conflict Verification. Next, annotators examine whether the visual and textual contexts are semantically inconsistent with respect to the question. That is, the two modalities should lead to different correct answers when considered separately. Samples where both modalities lead to the same answer are discarded, as they do not reflect true modality conflict. Samples that do not meet either verification criterion are flagged for further review. Depending on the nature and severity of the issue, we take one of the following actions: revise the prompt to improve clarity, regenerate the problematic part of the sample (e.g., the question or context), or discard the sample entirely if it cannot be reasonably corrected. To ensure consistency and reduce subjectivity, each category (i.e., vision-aligned, text-aligned, and conflict) is independently verified by three trained annotators. Disagreements are resolved through discussion or majority voting. In addition, we conduct random spot-checks throughout the dataset to ensure the consistency and reliability of the annotations. B.2 Data Statistics We computed the average number of words in the text context for all samples within each task type using the spaCy library.0 As shown in Table 5, while there are some variations in text length across tasks, the differences are relatively minor. This indicates that text length is unlikely to be confounding factor in evaluating modality preference across different task types. B.3 Illustrative Samples from the MC2 Benchmark To provide an intuitive understanding of the MC2 benchmark and the nature of modality conflict, we present few representative samples covering different task types as shown in Figure 7, Figure 8, Figure 9 and Figure 10. 0We use the spaCy library in Python, available at https://pypi.org/project/spacy/. 18 <image> is placeholder for below image <image> is placeholder for below image User: <image> Conflict Text Context: Three sheep are peacefully eating grass, surrounded by lush greenery. Their heads are lowered as they nibble on the fresh blades, completely undisturbed. Question: What are the cows in the back doing? Assistant: <output> vision-based Answer: running Text-based Answer: eating User: <image> Conflict Text Context: In the photo, there are three boys playing Frisbee, and one more boy is partially visible in the corner, bending down to tie his shoelaces, making total of four people. Question: How many people are in the photo? Assistant: <output> vision-based Answer: five Text-based Answer: four Figure 7: Illustration of using modality context conflict pairs to investigate modality preference in activity recognition (Left) and counting tasks (Right). The highlighted areas indicate the points of conflict between visual and textual contexts. <image> is placeholder for below image <image> is placeholder for below image User: <image> Conflict Text Context: The birthday cake was designed to look like sleek police car, complete with edible flashing lights and fondant badge on the side. Question: What is the cake in the shape of? Assistant: <output> vision-based Answer: fire truck Text-based Answer: police car User: <image> Conflict Text Context: Two wildebeests are standing in dry, grass-less savanna, their dark coats contrasting with the dusty ground. The area is sparse, with only few scattered shrubs visible in the background. Question: What animal is shown? Assistant: <output> vision-based Answer: zebras Text-based Answer: wildebeests Figure 8: Illustration of using modality context conflict pairs to investigate modality preference in attribute recognition (Left) and object recognition tasks (Right). The highlighted areas indicate the points of conflict between visual and textual contexts. 19 <image> is placeholder for below image <image> is placeholder for below image User: <image> Conflict Text Context: large brown clock tower mounted in the face ofa building overlooks vibrant park filled with lush green trees. The contrast between the brown tower and the surrounding greenery creates picturesque scene. Question: What color are the trees? Assistant: <output> vision-based Answer: white Text-based Answer: green User: <image> Conflict Text Context: white bus with large rack on the front is parked by the beach, designed to carry equipment for surfing .The rack is sturdy and spacious, perfect for securing bulky items. Question: What can you hang on the rack on the front of the bus? Assistant: <output> vision-based Answer: bikes Text-based Answer: surfboards Figure 9: Illustration of using modality context conflict pairs to investigate modality preference in color recognition (Left) and positional reasoning (Right) tasks. The highlighted areas indicate the points of conflict between visual and textual contexts. <image> is placeholder for below image <image> is placeholder for below image User: <image> Conflict Text Context: girl sitting at counter with piece of pizza, staring blankly at the wall while the pizza grows cold in front of her. The room is quiet, and she seems uninterested in her surroundings. Question: What is the girl on the right feeling in the image? Assistant: <output> vision-based Answer: happy Text-based Answer: bored User: <image> Conflict Text Context: The young girl is running swiftly across the field, dribbling soccer ball with precision as she maneuvers past imaginary opponents. Her focus is on scoring goal, and she practices her footwork with determination. Question: What sport is depicted in the picture? Assistant: <output> vision-based Answer: tennis Text-based Answer: soccer Figure 10: Illustration of using modality context conflict pairs to investigate modality preference in sentiment understanding and object recognition tasks. The highlighted areas indicate the points of conflict between visual and textual contexts. 20 statistics Sport Attribute Sentiment Positional Counting Color Activity Object Avg Text Length 52.48 33.50 39.69 31. 37.12 31.15 49.68 39.71 39.36 Table 5: Average text context length across different task types in the MC2 dataset."
        },
        {
            "title": "C Model Evaluatiion",
            "content": "C.1 Evaluatiion Detail for Modality Preference We assess open-source multimodal large language models (MLLMs) with different parameter sizes, including LLaVA1.5-7B/13B [29], LLama3.2-11B-Vision-Instruct [13], LLaVA-OneVision7B/72B [24], CogVLM2-19B [17], mPLUG-Owl3-24-07 [49], Qwen2VL-7B [43], GLM-4V9B [10], SPHINX-V2-1K [27], InternVL3-9B/14B/38B/78B [2], LLaVA-next-7B/13B/34B [29] and Qwen2.5VL-7B/32B/72B [7]. All the open-source models are evaluated using NVIDIA A100 or A800 GPUs. We also evaluate the proprietary model, GPT-4o-mini [21] via the official API. Details of single-modality context evaluation Before evaluating modality preference, we first assess the ability of MLLMs to answer questions accurately given single-modality context in the MC2 dataset. Specifically, we evaluate the models accuracy in answering based on text context and based on vision context (based on the image). As shown in Table 10 and Table 11, all models achieve over 95% accuracy when provided with either textual or visual context. This indicates that question understanding and the understanding of single-modality context do not affect the modality preference evaluation. Therefore, we have excluded this confounding factor from the analysis. Details of results for modality preference evaluation We provide the results of modality preference for several models in Table 1 in the main text. More detailed modality preference evaluation results are presented in Table 7. We evaluated 18 models, including 17 open-source models and the commercial model, GPT4o-mini. C.2 The Details for Controlling Modality Preference Noisy image and text grammar error generation For noisy images, we initially adopte two approaches: adding Gaussian noise to the image and applying uniform linear motion blur kernel. In our experiments, both methods yield similar results in terms of model performance, indicating comparable effects on the evaluation. We report the results based on the Gaussian noise method in this paper. To generate grammatically incorrect text, we employ DeepSeek-V3 to rewrite the original text context. We find that DeepSeekV3 performs well in introducing localized grammatical errors while preserving the overall structure. We use two different prompting strategies, both of which produced similar results. In this paper, we report the results based on the V1 version of grammar error generation. The specific prompt used is shown below: Text Context Grammar Error Generation (V1) using DeepSeek-V3 Instruction: # Please rewrite the following text, intentionally introducing three grammar errors and spelling mistakes. These errors should include, but are not limited to, subject-verb agreement mistakes, incorrect use of tenses, improper punctuation, misplacement of modifiers, and misspelled words. Despite these errors, the core meaning of the sentence must remain unchanged. The text to be rewritten is: {original text context}. Output the revised text enclosed in <context2> </context2> tag. Only output the revised text and dont explain anything. 21 Text Context Grammar Error Generation (V2) using DeepSeek-V3 Instruction: # Please rewrite the following text, intentionally introducing three grammar errors and spelling mistakes. These errors should include, but are not limited to, subject-verb agreement mistakes, incorrect use of tenses, improper punctuation, misplacement of modifiers, and misspelled words. Despite these errors, the core meaning of the sentence must remain unchanged. Additionally, do not alter the meaning related to the provided critical word. The text to be rewritten is: {original text context}. the critical word is: {answer based on text context}. Output the revised text enclosed in <context1> </context1> tag. Only output the revised text and dont explain anything. The detail for controlling modality preference using noise data We first test the models ability to understand single noisy modality (either noisy image context or text context with grammar errors). As shown in Table 13 and Table 12, the models understanding of the single-modality content remains close to 95% for the noisy image context and close to 98% for the noisy text context, which is nearly the same as its performance on the original single-modality content (shown in Table 11 and Table 10). This rules out the possibility that the shift in modality preference due to the added noise is caused by decrease in the models understanding of the single modality. Instead, it suggests that MLLMs may implicitly associate modality preference with surface-level quality signals (e.g., noise or errors), which then influences their modality weighting during reasoning. We provide the results of modality preference using either the noisy image context or the text context with grammer error in Figure 4 in main text. We also provide more results for more models in Table 14 and Table 15. More results for controlling modality preference through instruction design. In Figure 4, we provide the Vision Ratio results for LLaVA-OneVision-7B and Qwen2.5VL-7B. We also present more results on controlling modality preference through instruction design for preference towards the vision modality and the text modality in Table 8 and Table 9. For each setting, we report the probability of responses exhibiting text preference (measured by vision-based accuracy) and the probability of responses exhibiting vision preference (measured by text-based accuracy). C.3 The details of PCA Analysis In Section 4.2, we mentioned the PCA analysis details regarding the \"Modality Preference Direction in Representation Space.\" Here, we provide more in-depth description of the setup. We extract the models hidden representations from the last token of the input across different layers. Then, we apply the PCA method to reduce the dimensionality to two dimensions for visualization. The following settings were visualized: 1. The model states under the original modality context input in conflicting scenarios. 2. The model states when there is image noise or textual syntax errors. 3. The model states when specific instructions biased towards image or text are added. To improve PCA dimensionality reduction efficiency, we selected 500 samples for each setting. Additionally, we calculated the center position after dimensionality reduction for each setting. The center (or centroid) of the samples is computed by taking the mean of the reduced-dimensional points across all the samples."
        },
        {
            "title": "D Method Applying",
            "content": "D.1 Details for Pattern of hidden states In the main text, we visualize the layer-wise absolute difference and standard deviation of the hidden states for Qwen2.5VL-7B. As shown in Figure 11, we present the visualization of hidden states for LLaVA-OneVision-7B, Qwen2VL-7B, and InternVL3-14B. 22 (a) LLaVA-OneVision-7B (b) Qwen2VL-7B (c) InternVL3-14B Figure 11: Layer-wise absolute difference and standard deviation of hidden states between imageguided and text-guided instruction for LLaVA-OneVision-7B, Qwen2VL-7B and InternVL3-14B models from left to right. For each model, we selected layers with large absolute differences and small standard deviations. This means we identified the layers that showed stable and significant differences between instructions with modality preference towards vision context and text context, which are then used to steer and adjust the models modality preference. D.2 More Results for Steering Models Toward Vision or Text Modality Preference In the main text, we present the results of steering Qwen2VL-7B and Qwen2.5VL-7B towards vision and text modality preferences, respectively. Here, we provide additional results on modality preference control, including experiments with InternVL3-14B and LLaVA-OneVision-7B. As shown in Table 6, the proposed method significantly outperforms the baseline in both directions of modality preference. D.3 Downstream Task Applications By steering the models modality preference towards the vision modality, we reduce vision hallucinations in MLLMs. Conversely, by steering towards the text modality, the model focuses more on the textual modality, which helps reduce hallucinations in multimodal machine translation. This approach prevents the model from overly focusing on vision information and adding unnecessary objects or other details in the translation output. Details of Vision Hallucination Mitigation PhD [30] is multimodal hallucination evaluation benchmark that increases the risk of hallucination by introducing \"hitem\" items, intentionally guiding the model to produce hallucinated words, thereby increasing the difficulty of the models responses. Phd-icc and Phd-ica are subsets of PhD, which respectively increase hallucination risks by introducing incorrect text context and inaccurate text context. For testing convenience, we randomly selected 1,000 samples from the original Phd-cc and Phd-ica datasets for evaluation. By steering modality preference towards the vision context, the model is encouraged to focus more on visual information, which helps reduce vision hallucinations. As shown in Table 3, the proposed method outperforms most baseline methods across nearly all subcategories of the Phd-icc and Phd-ica datasets. Furthermore, detailed analysis of the two datasets reveals that most samples in Phd-iac introduce only irrelevant text context, whereas Phd-icc introduces incorrect text context, which makes it easier for the model to generate hallucinations. Proposed method significantly outperforms the baseline methods on Phd-icc, indicating that the proposed method is more effective in mitigating hallucinations. Details of Multimodal Multimodal machine translation Ambigcaps [25] benchmark explores the role of datasets in stimulating the leverage of the visual modality and proposes methods to highlight the importance of visual signals in the datasets. We evaluate the multimodal machine translation task on this dataset using the Qwen2.5VL-7B model, with different instructions that favor the vision modality and the text modality. We also use our method to steer the modality preference towards the text modality. As shown in Table 4, proposed method improves the performance of the multimodal machine translation task for the bidirectional translation between English and Turkish. 23 Preference Model Sport Attribute Sentiment Positional Counting Color Activity Object Avg Vision Text MLLM-only Inst Design ours MLLM-only Inst Design ours 32.0 55.6 75. 8.8 44.8 49.6 21.6 31.2 40.8 48.4 68.4 71.2 2.8 12.0 18.0 60.0 82.4 85.2 24.8 30.8 38. 52.0 54.8 58.0 2.4 3.2 10.0 47.2 43.2 47.6 30.0 36.4 53.2 21.2 50.0 48.4 11.6 22.4 32. 52.8 58.8 68.4 42.4 61.2 74.4 9.6 19.6 58.4 20.9 31.6 42.8 37.5 52.8 57.1 Table 6: Performance by steering LLaVA-OneVision-7B towards vision and InternVL3-7B towards text. Model Sport Attribute Sentiment Positional Counting Color Activity Object Avg LLaMAVision LLaVA1.5-7B LLaVA1.5-13B OneVision-7B Owl3-24-07 QwenVL-7B Qwen2VL-7B Qwen2.5VL-7B InternLM-XC2.5-7B GLM-4V-9B SPHINX-V2-1K InternVL3-9B InternVL3-14B CogVLM2-19B InternVL3-38B InternVL3-78B Qwen2.5-VL-32B Qwen2.5VL-72B OneVision-72B LLaVA1.6-7B LLaVA1.6-13B LLaVA1.6-34B 31.2/52.4 20.0/59.6 34.4/59.6 32.0/36.4 60.8/31.6 35.2/43.6 26.4/58.0 65.6/12.8 72.0/19.6 42.0/42.4 39.6/50.8 45.2/35.2 72.8/8.8 44.0/39.6 75.2/9.6 92.4/3.2 85.60/10.40 93.6/4.4 47.2/46.0 10.8/74.4 16.0/66.4 34.8/42.4 20.4/69.6 8.0/88.0 8.8/89.6 21.6/54.8 16.4/72.4 15.2/63.2 12.4/82.8 45.2/46.0 26.4/63.2 32.4/59.2 14.8/82.4 21.2/68.0 30.8/48.4 29.2/56.0 45.2/33.6 46.0/28.8 49.20/39.20 59.2/27.2 20.0/70.8 5.2/85.2 7.2/90.4 12.0/81. 2.0/93.2 2.0/86.0 4.8/88.0 2.8/94.4 10.8/85.6 11.2/78.0 0.8/95.6 18.0/68.8 50.1/36.4 8.8/81.6 1.2/98.4 20.8/62.4 25.2/60.0 8.8/75.6 19.6/60.8 66.4/18.4 49.60/42.80 50.0/41.2 4.0/93.6 0.8/93.2 0.8/92.0 6.8/85.6 21.2/66.8 8.8/75.2 12.0/84.8 24.8/56.4 22.0/69.6 16.4/45.6 13.2/80.4 46.4/38.0 22.0/65.2 28.0/62.4 16.8/77.6 27.2/54.4 33.2/52.0 19.2/54.8 44.0/42.0 41.6/37.2 52/37.60 73.6/19.2 22.8/67.2 3.6/79.6 6.4/91.6 16.8/76.0 4.0/93.2 1.2/96.0 1.6/96.4 2.4/86.4 8.4/88.0 2.8/73.6 4.0/93.6 51.6/39.6 9.2/83.2 15.2/74.4 9.2/85.6 23.2/50.4 37.2/47.2 8.0/73.2 41.6/40.0 69.6/13.2 52/42 63.6/29.2 12.8/83.2 0.4/90.8 2.4/95.6 11.2/83.2 35.2/47.2 10.8/82.0 12.8/82.0 30.0/38.0 28.4/60.4 34.0/40.8 16.0/78.8 70.8/20.0 50.4/38.0 56.8/32.8 23.2/69.2 38.0/40.4 58.0/21.2 31.6/43.2 48.4/29.6 76.4/8.8 70.80/20 83.6/9.6 21.6/60.8 6.0/76.0 6.8/88.0 25.2/60.8 10.0/82.4 9.6/78.8 9.6/87.6 11.6/71.2 17.2/71.2 19.2/61.6 11.6/83.6 42.0/43.6 47.6/41.2 23.3/66.0 24.4/67.2 19.6/63.2 24.8/52.8 25.2/60.8 50.4/23.2 74.4/12.8 57.20/35.20 74.0/21.2 20.8/70.8 4.8/73.6 10.0/84.4 14.0/76.0 38.8/42.8 35.2/52.0 31.2/62.8 42.4/31.2 60.0/29.6 35.2/31.2 38.0/54.0 77.6/14.0 68.8/21.6 54.0/32.8 59.2/32.4 76.8/14.8 84.4/9.6 59.2/28.4 84.4/8.0 89.6/4.0 86.80/10.40 89.2/8.0 71.6/21.2 26.0/46.8 22.4/63.2 60.0/31. 20.4/68.4 11.9/77.2 14.4/81.3 20.9/58.6 28.0/63.5 21.1/54.7 15.3/78.3 52.2/35.4 43.4/46.1 32.6/56.5 23.6/70.5 34.0/48.6 45.8/37.5 28.2/54.0 51.1/30.8 69.5/15.8 62.90/29.70 73.4/20.0 27.6/64.2 7.2/77.5 9.0/84.0 22.6/67.2 GPT-4o-mini 94.4/3.2 35.6/47.6 60.4/28.4 22.0/58. 19.4/59.2 34.8/36.4 71.2/20.4 78.4/12.8 52.0/33.4 Table 7: Accuracy of question answering in the MC2 dataset when both textual and visual contexts are provided but the instruction does not specify which modality context should be used. Values are reported as vision-based accuracy/text-based accuracy for each model. Model Sport Attribute Sentiment Positional Counting Color Activity Object Avg OneVision-7B Qwen2VL-7B Qwen2.5VL-7B CogVLM2-19B InternLM-XC2.5-7B GLM-4V-9B SPHINX-V2-1K InternVL3-9B InternVL3-14B LLaVA1.6-7B LLaVA1.6-13B LLaVA1.6-34B 55.6/16.4 60.8/26.8 77.6/14.4 73.2/13.2 84.0/9.6 75.2/18.4 52.4/38.4 96.0/2.0 98.4/0.8 33.2/54.0 41.6/40.4 84.8/12.0 31.2/37.2 24.0/69.2 43.2/46.8 47.6/32.4 46.4/42.8 48.8/39.6 16.4/78.8 67.2/18.8 86.0/4.4 6.8/80.8 10.4/85.2 48.0/36.4 12.0/76.8 20.0/69.6 18.4/72.8 35.6/28.4 74.0/18.4 28.8/54.0 2.0/97.2 82.8/13.2 87.6/7.6 6.0/82.4 4.0/62.8 62.8/24. 30.8/42.4 20.4/74.0 43.2/40.4 26.8/45.2 36.0/52.4 33.6/55.6 20.8/72.8 54.8/26.4 71.6/12.8 6.4/79.6 8.4/83.6 34.0/52.0 3.2/77.6 10.8/80.0 35.6/55.6 14.0/40.0 22.8/66.0 38.4/54.0 13.6/80.8 55.6/21.2 78.0/6.8 2.8/90.4 5.6/92.8 38.8/44.4 36.4/18.4 32.0/52.0 58.8/24.4 61.6/17.6 63.6/20.8 76.4/16.4 30.0/58.8 84.4/7.6 97.2/0.8 10.8/70.0 14.4/70.8 76.4/14.4 22.4/47.6 27.2/61.6 53.6/35.6 56.0/28.0 74.0/18.4 48.4/38.8 40.8/52.8 82.8/6.4 90.8/3.2 13.6/69.2 24.8/58.0 62.0/18.4 61.2/16.4 63.2/28.8 81.2/11.6 76.0/15.2 76.4/15.6 80.0/11.6 64.8/29.2 91.6/4.0 96.4/1.6 48.4/40.8 45.2/41.2 80.4/12.4 31.6/41.6 32.3/57.8 51.4/37.7 48.9/27.5 59.7/30.5 53.7/36.1 30.1/63.6 76.9/12.4 88.2/4.8 16.0/70.9 19.3/66.9 60.9/26. Table 8: Accuracy of question answering in the MC2 dataset when both textual and visual contexts are provided and the instruction explicitly directs the model to answer based on visual modality context. Values are reported as vision-based accuracy/text-based accuracy for each model. 24 Model Sport Attribute Sentiment Positional Counting Color Activity Object Avg OneVision-7B 45.6/16.4 Qwen2VL-7B 51.6/34.8 Qwen2.5VL-7B 77.6/14.4 CogVLM2-19B 53.6/29.6 InternLM-XC2.5-7B 79.2/16.0 GLM-4V-9B 53.2/32.8 SPHINX-V2-1K 48.4/41.2 InternVL3-9B 41.2/27.2 InternVL3-14B 28.4/44.8 22.4/37.2 14.8/78.4 43.2/46.8 28.4/47.6 31.2/56.8 30.4/61.2 14.4/81.6 13.6/71.6 14.0/68. 5.6/76.8 6.8/88.0 18.4/72.8 10.4/56.8 60.4/29.2 6.0/85.6 2.0/98.0 22.4/60.8 3.6/82.4 27.2/42.4 15.6/79.6 43.2/40.4 17.6/56.8 26.0/62.8 23.2/68.0 19.2/77.6 16.8/64.0 21.2/54.8 1.6/77.6 4.0/90.8 35.6/55.6 6.0/62.0 11.6/75.6 20.0/70.0 11.2/84.0 18.0/60.4 28.0/43.2 28.8/18.4 18.0/70.8 58.8/24.4 36.0/35.2 52.8/29.2 52.4/35.6 27.2/67.2 25.6/46.4 24.8/50.0 16.8/47.6 19.2/72.8 53.6/35.6 34.0/39.6 60.8/29.6 28.0/60.8 30.8/65.2 29.2/49.2 17.2/58.8 56.0/16.4 57.6/36.0 81.2/11.6 67.2/19.6 72.0/18.0 68.0/22.0 63.2/30.8 62.4/17.6 55.2/19. 25.5/41.6 23.4/68.9 51.4/37.7 31.6/43.4 49.2/39.7 35.2/54.5 27.1/68.2 28.6/49.6 24.0/52.8 Table 9: Accuracy of question answering in the MC2 dataset when both textual and visual contexts are provided and the instruction explicitly directs the model to answer based on the textual modality. Values are reported as vision-based accuracy/text-based accuracy for each model. Model Sport Attribute Sentiment Positional Counting Color Activity Object Avg LLaMAVision LLaVA1.5-7B LLaVA1.5-13B OneVision-7B Owl3 Qwen2VL-7B Qwen2.5VL-7B CogVLM2-19B InternLM-XC2.5-7B GLM-4V-9B SPHINX-V2-1K InternVL3-9B InternVL3-14B InternVL3-38B InternVL3-78B Qwen2.5VL-72B OneVision-72B GPT-4o-mini 97.6 98.0 97.2 98.0 97.6 98.8 99.2 98.0 96.4 98.4 98.4 97.6 98.4 97.6 97.2 99.6 100.0 97.6 97.2 98.0 97.6 95.2 97.2 96.4 97.6 95.2 97.6 95.2 97.6 98.0 98.4 96.8 97.6 98.4 97.6 97.2 99.6 100.0 99.6 100.0 99.6 99.6 100.0 99.2 99.2 99.6 99.2 99.6 100.0 100.0 100.0 96.8 97. 99.6 99.2 97.6 97.6 98.4 98.8 99.6 99.6 96.0 99.2 97.2 98.8 99.2 99.2 98.8 98.0 100.0 99.6 98.6 97.2 98.4 97.6 98.0 98.8 98.8 96.8 94.8 98.0 98.8 98.0 95.6 95.6 96.0 96.4 97.2 96.4 97.4 96.0 99.2 98.8 98.8 99.2 100.0 98.8 98.4 98.0 98.4 99.2 96.8 98.4 97.2 96.8 100.0 100. 97.6 97.6 95.2 98.0 99.2 98.8 98.4 98.0 98.8 99.6 98.4 98.8 98.8 98.4 98.0 99.6 100.0 98.4 98.4 97.6 97.6 99.2 100.0 100.0 100.0 99.2 99.6 98.4 99.6 99.6 99.2 99.6 100.0 100.0 99.2 98.8 100.0 97.8 98.3 97.9 98.3 98.8 99.0 98.7 97.4 98.2 98.4 98.7 98.1 98.5 98.1 98.0 98.9 98. 98.4 Table 10: Accuracy of question answering in the MC2 dataset when only unimodal textual context is provided. 25 Model Sport Attribute Sentiment Positional Counting Color Activity Object Avg LLaMAVision LLaVA1.5-7B LLaVA1.5-13B OneVision-7B Owl3 QwenVL-7B Qwen2VL-7B Qwen2.5VL-7B CogVLM2-19B InternLM-XC2.5-7B GLM-4V-9B SPHINX-V2-1K InternVL3-9B InternVL3-14B InternVL3-38B InternVL3-78B Qwen2.5VL-72B OneVision-72B GPT-4o-mini 100.0 99.6 99.6 100.0 99.2 100.0 99.6 99.6 99.6 98.8 99.6 98.8 98.8 99.2 100.0 99.2 97.2 100.0 100.0 98.8 98.0 95.2 97.2 94.0 98.8 98.8 98.8 99.2 98.8 99.2 97.6 95.6 96.4 98.0 99.6 97.2 97.6 92. 92.8 96.4 94.4 97.2 94.0 97.6 95.6 98.0 91.2 94.0 98.0 99.2 95.6 96.4 97.2 96.8 100.0 97.6 95.6 98.4 100.0 97.6 98.4 97.2 100.0 98.4 100.0 96.8 98.0 99.2 92.8 96.8 98.4 100.0 98.8 99.2 99.6 100.0 96.4 97.6 95.2 84.4 88.4 99.6 96.4 99.2 91.6 94.8 97.6 98.0 90.0 92.4 94.4 96.0 97.2 96.4 99.2 99.6 98.4 99.6 96.8 100.0 100.0 100.0 98.8 99.6 100.0 99.6 100.0 98.8 99.6 100.0 98.4 100. 98.8 98.8 96.4 97.2 97.2 100.0 99.6 100.0 98.4 100.0 99.2 96.8 98.0 97.2 99.2 99.2 98.4 100.0 100.0 96.0 96.4 97.2 98.4 98.4 98.8 99.2 98.4 98.4 98.8 98.8 98.8 99.6 99.2 98.0 98.4 98.8 98.4 99.6 98.8 96. 97.7 98.5 96.9 96.6 95.8 99.3 98.3 99.3 96.8 97.9 99.1 97.8 96.6 97.1 98.4 98.5 98.4 98.7 97.0 Table 11: Accuracy of question answering in the MC2 dataset when only unimodal visual context is provided. Model Sport Attribute Sentiment Positional Counting Color Activity Object Avg OneVision-7B Qwen2.5VL-7B CogVLM2-19B InternLM-XC2.5-7B GLM-4V-9B SPHINX-V2-1K LLaVA1.6-7B 98.0 98.8 98.0 97.6 98.8 98.4 98.0 95.2 96.4 94.4 97.2 96.4 97.6 90.8 100.0 100.0 99.2 99.2 99.6 99.2 98.8 97.2 99.2 96.0 99.2 97.2 98.8 96.8 97.2 97.6 91.2 97.6 98.8 97.6 96. 97.2 97.6 97.6 97.6 98.4 98.4 97.2 98.0 98.8 97.6 98.8 99.6 98.4 96.0 99.2 99.2 99.6 98.8 100.0 99.6 96.4 97.8 98.4 96.7 98.2 98.6 98.5 96.4 Table 12: Accuracy of question answering in the MC2 dataset when only unimodal noisy textual context is provided. Model Sport Attribute Sentiment Positional Counting Color Activity Object Avg OneVision-7B Qwen2.5VL-7B CogVLM2-19B InternLM-XC2.5-7B GLM-4V-9B SPHINX-V2-1K LLaVA1.6-7B 98.4 98.8 99.6 98.4 99.2 98.4 98.4 95.6 96.4 97.6 96.0 98.4 97.2 94.8 94.0 100.0 87.6 94.4 95.6 90.0 92. 98.4 99.2 94.4 94.0 98.4 98.8 93.6 78.4 99.2 91.6 95.2 98.4 98.4 84.0 99.2 100.0 98.8 100.0 99.6 97.6 96.8 95.6 100.0 97.2 97.6 100.0 96.8 93.6 98.0 98.8 98.48 95.2 99.2 98.0 95.6 94.7 99.3 95.7 96.4 98.6 96.9 93. Table 13: Accuracy of question answering in the MC2 dataset when only unimodal noisy visual context is provided. 26 Model Sport Attribute Sentiment Positional Counting Color Activity Object Avg OneVision-7B 42/29.2 Qwen2.5VL-7B 74.0/12.0 InternLM-XC2.5-7B 75.6/18.4 GLM-4V-9B 48.0/31.6 SPHINX-V2-1K 46.0/44.8 CogVLM2-19B 46.0/39.6 LLaVA1.6-7B 11.2/71.2 31.2/41.6 53.20/36.40 32.8/57.2 37.6/48.8 18.0/77.6 34.0/47.2 6.4/83. 4.4/88 24.4/62.4 56.0/36.0 10.0/81.2 1.2/98.0 12.4/68.4 1.2/91.6 31.6/44 58/29.2 26.4/59.2 34.0/56.4 21.6/70.8 22.8/52.0 5.2/76.4 4.4/82 54.8/35.2 11.6/78.0 18.8/70.8 11.6/80.8 8.4/70.0 0.4/88.8 39.2/29.6 78/16 61.2/28.0 64.8/27.2 30.0/64.8 37.2/39.2 7.2/71.6 16.4/66.8 52.8/35.6 52.8/36.4 33.6/55.6 30.8/60.0 30.0/55.2 5.6/70.4 50.8/26 82.4/12.4 74.0/20.0 61.6/25.2 62.8/28.8 62.8/24.4 28.8/44. 27.5/50.9 59.7/29.9 48.8/41.7 38.6/49.6 27.8/65.7 31.7/49.5 8.3/74.7 Table 14: Accuracy of question answering in the MC2 dataset when both noisy textual context and visual context are provided but the instruction does not specify which modality context should be used. Values are reported as vision-based accuracy/text-based accuracy for each model. Model Sport Attribute Sentiment Positional Counting Color Activity Object Avg OneVision-7B 24/44 Qwen2.5VL-7B 48.8/23.6 InternLM-XC2.5-7B 52.8/33.6 GLM-4V-9B 32.4/50.4 SPHINX-V2-1K 29.2/58.0 CogVLM2-19B 32.0/45.2 LLaVA1.6-7B 8.4/79.2 16.4/66.4 25.6/64.8 15.2/76.8 19.6/73.2 11.2/86.4 22.0/60.0 4.0/87. 2/96.8 6/88 24.4/64.4 4.0/91.6 0.8/98.8 3.6/84.8 0.4/94.8 17.2/66.4 33.2/59.6 12.8/78.4 21.2/69.2 16.8/80.4 14.0/60.8 3.2/82.4 0.80/87.2 39.2/54.8 3.6/90.4 11.2/78.4 5.6/90.4 6.4/73.2 0.4/92.0 20.4/47.2 58.8/32.8 33.2/56.8 41.6/46.8 18.0/76.4 18.8/49.2 4.4/77.6 8/78.8 30/56.8 24.4/64.8 15.2/76.8 18.0/76.0 18.8/65.6 4.4/76.8 35.6/42.8 68/56.8 54.8/37.2 41.2/42.0 52.8/40.4 46.0/36.4 22.8/50. 15.5/66.2 38.7/50.3 27.7/62.8 23.3/66.1 19.1/75.9 20.2/59.4 6.0/80.1 Table 15: Accuracy of question answering in the MC2 dataset when both textual and noisy visual contexts are provided but the instruction does not specify which modality context should be used. Values are reported as vision-based accuracy/text-based accuracy for each model."
        },
        {
            "title": "NeurIPS Paper Checklist",
            "content": "1. Claims Question: Do the main claims made in the abstract and introduction accurately reflect the papers contributions and scope? Answer: [Yes] 2. Limitations Question: Does the paper discuss the limitations of the work performed by the authors? Answer: [Yes] See discussions in Suppl. 3. Theory assumptions and proofs Question: For each theoretical result, does the paper provide the full set of assumptions and complete (and correct) proof? Answer: [NA] 4. Experimental result reproducibility Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? Answer: [Yes] See details in Suppl. 5. Open access to data and code Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material? Answer: [Yes] See details in Suppl. 6. Experimental setting/details Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? Answer: [Yes] See details in Suppl. 7. Experiment statistical significance Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? Answer: [Yes] See details in Suppl. 8. Experiments compute resources Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? Answer: [Yes] See details in Suppl. 9. Code of ethics Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines? Answer: [Yes] 10. Broader impacts Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? Answer: [Yes] 11. Safeguards Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? 28 Answer: [Yes] See details in Suppl. 12. Licenses for existing assets Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? Answer: [Yes] 13. New assets Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? Answer: [Yes] See detail in Suppl. 14. Crowdsourcing and research with human subjects Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? Answer: [Yes] See details in Suppl. 15. Institutional review board (IRB) approvals or equivalent for research with human subjects Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained? Answer: [Yes] See details in Suppl. 16. Declaration of LLM usage Question: Does the paper describe the usage of LLMs if it is an important, original, or non-standard component of the core methods in this research? Note that if the LLM is used only for writing, editing, or formatting purposes and does not impact the core methodology, scientific rigorousness, or originality of the research, declaration is not required. Answer: [Yes] See details in Suppl."
        }
    ],
    "affiliations": [
        "Harbin Institute of Technology, Shenzhen, China",
        "Peng Cheng Laboratory, Shenzhen, China"
    ]
}