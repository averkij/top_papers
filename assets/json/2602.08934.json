{
    "paper_title": "StealthRL: Reinforcement Learning Paraphrase Attacks for Multi-Detector Evasion of AI-Text Detectors",
    "authors": [
        "Suraj Ranganath",
        "Atharv Ramesh"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "AI-text detectors face a critical robustness challenge: adversarial paraphrasing attacks that preserve semantics while evading detection. We introduce StealthRL, a reinforcement learning framework that stress-tests detector robustness under realistic adversarial conditions. StealthRL trains a paraphrase policy against a multi-detector ensemble using Group Relative Policy Optimization (GRPO) with LoRA adapters on Qwen3-4B, optimizing a composite reward that balances detector evasion with semantic preservation. We evaluate six attack settings (M0-M5) against three detector families (RoBERTa, FastDetectGPT, and Binoculars) at the security-relevant 1% false positive rate operating point. StealthRL achieves near-zero detection (0.001 mean TPR@1%FPR), reduces mean AUROC from 0.74 to 0.27, and attains a 99.9% attack success rate. Critically, attacks transfer to a held-out detector family not seen during training, revealing shared architectural vulnerabilities rather than detector-specific brittleness. We additionally conduct LLM-based quality evaluation via Likert scoring, analyze detector score distributions to explain why evasion succeeds, and provide per-detector AUROC with bootstrap confidence intervals. Our results expose significant robustness gaps in current AI-text detection and establish StealthRL as a principled adversarial evaluation protocol. Code and evaluation pipeline are publicly available at https://github.com/suraj-ranganath/StealthRL."
        },
        {
            "title": "Start",
            "content": "6 2 0 2 9 ] . [ 1 4 3 9 8 0 . 2 0 6 2 : r StealthRL: Reinforcement Learning Paraphrase Attacks for Multi-Detector Evasion of AI-Text Detectors Suraj Ranganath* Atharv Ramesh University of California, San Diego Abstract AI-text detectors face critical robustness challenge: adversarial paraphrasing attacks that preserve semantics while evading detection. We introduce StealthRL, reinforcement learning framework that stress-tests detector robustness under realistic adversarial conditions. StealthRL trains paraphrase policy against multi-detector ensemble using Group Relative Policy Optimization (GRPO) with LoRA adapters on Qwen3-4B, optimizing composite reward that balances detector evasion with semantic preservation. We evaluate six attack settings (M0M5) against three detector families (RoBERTa, FastDetectGPT, and Binoculars) at the security-relevant 1% false positive rate operating point. StealthRL achieves near-zero detection (0.001 mean TPR@1%FPR), reduces mean AUROC from 0.74 to 0.27, and attains 99.9% attack success rate. Critically, attacks transfer to held-out detector family not seen during training, revealing shared architectural vulnerabilities rather than detector-specific brittleness. We additionally conduct LLM-based quality evaluation via Likert scoring, analyze detector score distributions to explain why evasion succeeds, and provide per-detector AUROC with bootstrap confidence intervals. Our results expose significant robustness gaps in current AI-text detection and establish StealthRL as principled adversarial evaluation protocol. Code and evaluation pipeline are publicly available at https://github.com/suraj-ranganath/StealthRL."
        },
        {
            "title": "1 Introduction",
            "content": "Large language models (LLMs) produce text that is increasingly indistinguishable from human writing, raising urgent concerns about academic integrity, misinformation, and content provenance [21]. In response, growing ecosystem of AI-text detectors has been deployed in educational institutions, publishing platforms, and content moderation systems. These detectors span diverse architectures, from fine-tuned classifiers [21] to zero-shot statistical methods [15, 1] and paired-LM approaches [8], yet their robustness to adversarial manipulation remains poorly understood. Standard detector evaluations measure performance on clean distributions, where AI-generated text is presented without any evasion attempt. However, real-world adversaries are adaptive: they can iteratively refine paraphrases, query detector APIs, and exploit known weaknesses. This gap between clean-distribution evaluation and adversarial robustness is critical. detector that achieves 95% accuracy on clean text may fail catastrophically when an attacker deliberately targets its decision boundary. The choice of operating point further complicates evaluation. Most prior work reports AUROC or accuracy at default thresholds, but deployed detectors must operate at low false positive rates (e.g., 1% FPR) to avoid falsely accusing human writers. At these strict operating points, the gap between clean and adversarial performance is even more pronounced, as detectors sacrifice recall to maintain precision. *Corresponding author. suranganath@ucsd.edu a3nair@ucsd.edu 1 We introduce StealthRL, reinforcement learning framework that systematically evaluates detector robustness under adaptive adversarial conditions. StealthRL trains paraphrase policy against multi-detector ensemble, using GRPO [17] with LoRA adapters [9] on Qwen3-4B-Instruct, to produce semantically faithful paraphrases that minimize detection confidence. By evaluating at the 1% FPR operating point and testing transfer to held-out detector family, we provide rigorous robustness assessment that complements standard benchmarks. Our contributions are as follows: We implement black-box adaptive paraphrasing attacks via multi-detector RL training with semantic constraints, one of the first works to apply RL for adversarial detector evasion across multiple detector families simultaneously. We demonstrate catastrophic robustness failure: 0.001 mean TPR@1%FPR across three detector architectures, with strong cross-architecture transfer to held-out detector. We provide comprehensive analysis including detector score distributions (explaining why evasion succeeds), LLM-based quality evaluation, and per-detector AUROC with bootstrap confidence intervals. We establish an evaluation protocol measuring evasion, transfer, and fidelity at security-relevant operating points, and release complete training and evaluation code for reproducible robustness benchmarking at https://github.com/suraj-ranganath/StealthRL."
        },
        {
            "title": "2.1 AI-Text Detection Methods",
            "content": "AI-text detection methods can be broadly categorized into three families. Fine-tuned classifiers train discriminative models on labeled human and AI text; the RoBERTa-based OpenAI detector [21] is widely used example. Zero-shot statistical methods exploit properties of language model probability distributions without requiring labeled data. DetectGPT [15] uses probability curvature, while Fast-DetectGPT [1] achieves similar accuracy with substantially reduced computational cost via conditional probability curvature. Ghostbuster [22] combines features from multiple weaker models. Paired-LM detectors such as Binoculars [8] compare log-likelihoods across two language models to detect statistical anomalies. The MAGE benchmark [14] provides standardized evaluation data, while RAID [6] offers shared benchmark for robust detector evaluation across domains and attacks. Despite diverse architectures, Sadasivan et al. [18] provide theoretical arguments that reliable detection may be fundamentally impossible as language models improve, motivating empirical robustness evaluation like ours."
        },
        {
            "title": "2.2 Adversarial Attacks on Detectors",
            "content": "Evasion attacks on AI-text detectors range from simple paraphrasing to sophisticated adaptive methods. Krishna et al. [13] demonstrate that paraphrasing with their DIPPER model [12] effectively evades detectors, though retrieval-based defenses can mitigate the attack. Cheng et al. [2] study adversarial paraphrasing as universal attack, using detector-guided candidate selection to humanize AI-generated text. Character-level attacks such as homoglyph substitution (SilverSpeak [3]) achieve strong evasion by replacing characters with visually similar Unicode glyphs, but often degrade readability and are detectable by normalization. Most relevant to our work, AuthorMist [4] applies reinforcement learning to train an evasion policy against single detector. StealthRL extends this approach to multi-detector ensemble training with held-out evaluation, strict low-FPR operating points, and comprehensive quality analysis. 2 Watermark-based detection [11] represents an orthogonal approach where the text generator embeds statistical signal during generation. While watermarks can provide stronger guarantees, they require control over the generation process and are outside the scope of post-hoc detection methods we evaluate. natural defensive response to paraphrasing attacks is adversarial training, where detectors are fine-tuned on adversarially generated examples to improve robustness. However, adversarial training faces fundamental scalability challenge: the space of possible paraphrasing strategies is vast, and detector hardened against one attack family may remain vulnerable to novel attack methods. The RAID benchmark [6] evaluates detectors across diverse attack types and finds that no single detector achieves robust performance against all attacks, underscoring the difficulty of building universally robust defenses. Our work focuses on the attack side of this arms race to quantify the current robustness gap and motivate the development of stronger defensive strategies."
        },
        {
            "title": "2.3 Reinforcement Learning for Text Generation",
            "content": "Reinforcement learning from human feedback (RLHF) [16] has become the standard approach for aligning language models with human preferences. Proximal Policy Optimization (PPO) [19] was the original RL algorithm used for RLHF, but recent work has introduced more efficient alternatives. Group Relative Policy Optimization (GRPO) [17, 20] eliminates the need for separate value network by using group-level relative rewards, reducing memory requirements and enabling efficient training. Parameter-efficient fine-tuning methods, particularly LoRA [9] and QLoRA [5], enable RL training of large language models with limited compute by adapting only low-rank weight matrices. StealthRL combines GRPO with LoRA on Qwen3-4B-Instruct, demonstrating that efficient RL fine-tuning suffices for learning effective evasion policies."
        },
        {
            "title": "3.1 Threat Model",
            "content": "We consider an adversary who has produced AI-generated text and seeks to modify it to evade detection while preserving semantic content. Formally: Attacker capability: Black-box access to detector scores. The attacker can query detector confidence p(y) (probability that input is AI-generated) but does not have access to model gradients or internal parameters. Although our experiments use open-source detectors for which gradients are in principle available, we deliberately treat them as black-box oracles, querying only scalar confidence scores. This design choice ensures that StealthRL generalizes directly to closed-source, API-based detectors (e.g., GPTZero, Originality.ai) without any modification to the training procedure. Attacker goal: Produce paraphrase of AI-generated text such that p(y) < τ for detector threshold τ , while maintaining semantic equivalence sim(x, y) > δ for similarity threshold δ. Attacker constraint: The paraphrase must be fluent, grammatical reformulation of the original text, not trivially corrupted version. We evaluate transfer by testing against held-out detector (Binoculars) not seen during training, assessing whether learned evasion strategies generalize across detector architectures."
        },
        {
            "title": "3.2 Reward Design",
            "content": "Given AI-generated text and paraphrase πθ( x), we define composite reward: R(x, y) = α Rdet(y) + β Rsem(x, y), (1) where α = 1.0 and β = 0.1 control the evasion-quality tradeoff. Detector evasion reward. The detector reward measures how effectively the paraphrase evades the training ensemble: Rdet(y) = 1 pens(y), pens(y) = w1 pRoBERTa(y) + w2 pFast-DetectGPT(y), (2) where w1 = 0.6 and w2 = 0.4 are the ensemble weights. The weighted average prioritizes the fine-tuned classifier (RoBERTa) over the zero-shot method (Fast-DetectGPT), reflecting the intuition that evading learned detector is harder and thus more informative for training. Semantic similarity reward. To prevent degenerate solutions (e.g., outputting empty or unrelated text), we constrain semantic preservation using E5 embedding cosine similarity [24]: Rsem(x, y) = cos(cid:0)E5(x), E5(y)(cid:1). (3) KL penalty. GRPO includes an implicit KL divergence penalty (coefficient λKL = 0.05) against the frozen reference policy πref to prevent catastrophic forgetting and maintain generation fluency."
        },
        {
            "title": "3.3 Training Pipeline",
            "content": "Algorithm 1 describes the StealthRL training procedure. We fine-tune Qwen3-4B-Instruct using LoRA adapters (rank 32, α = 32, dropout 0.05) with GRPO [17]. Training uses 10,000 AI-generated MAGE train samples for 3 epochs with batch size 16, group size 8, and learning rate 2.8 104. The training ensemble comprises RoBERTa (60% weight) and Fast-DetectGPT (40% weight); Binoculars is held out for transfer evaluation. 3."
        },
        {
            "title": "Inference",
            "content": "At inference time, the fine-tuned policy generates single paraphrase per input using temperature 1.0, top-p 0.9, and maximum 512 tokens. The prompt template is: Paraphrase the following text while preserving its meaning: [TEXT]. No candidate selection or reranking is applied; the single-pass output is evaluated directly against all three detectors."
        },
        {
            "title": "4.1 Dataset",
            "content": "We construct custom subset of the MAGE (Machine-Generated Text Detection in the Wild) benchmark [14], which provides human-written and AI-generated text across multiple domains. Our training set consists of 10,000 AI-generated samples drawn from the MAGE training split; no human-written samples are used during RL fine-tuning, as the policy learns entirely from detector feedback on paraphrased AI text. The evaluation set contains 1,000 human-written and 1,000 AI-generated samples from the MAGE test split, filtered to 100500 tokens. Human samples in the evaluation set are used solely for threshold calibration at the 1% FPR operating point. 4 Algorithm 1 StealthRL Training Require: Training data = {xi}N E5 similarity model, base LLM πref Ensure: Fine-tuned paraphrase policy πθ i=1 (AI-generated texts), detector ensemble {dk}K k=1 with weights {wk}, 1: Initialize πθ πref with LoRA adapters (rank r=32, α=32) 2: for epoch = 1, . . . , do 3: for batch do for each do 4: 5: 6: 7: 8: 9: 10: 11: 12: 13: Sample group {y1, . . . , yG} πθ( x) for each yg in group do Rdet(yg) 1 (cid:80) Rsem(x, yg) cos(E5(x), E5(yg)) R(x, yg) α Rdet(yg) + β Rsem(x, yg) wk dk(yg) end for Compute advantages via group-relative normalization: Ag (Rg R)/σR end for Update θ via clipped policy gradient with KL penalty G=8 candidates Ensemble evasion Semantic preservation GRPO end for 14: 15: end for 16: return πθ"
        },
        {
            "title": "4.2 Detectors",
            "content": "We evaluate against three detector families spanning distinct architectural paradigms: RoBERTa OpenAI (openai-community/roberta-large-openai-detector): fine-tuned RoBERTa-large classifier trained on GPT-2 outputs. Represents the class of supervised neural classifiers. Fast-DetectGPT [1]: zero-shot statistical method that uses conditional probability curvature with scoring model EleutherAI/gpt-neo-2.7B. Represents curvature-based detection. Binoculars [8]: paired-LM detector using gpt2-medium and gpt2-large that compares crossentropy and perplexity. Held out during training to assess transfer."
        },
        {
            "title": "4.3 Baselines",
            "content": "We compare six attack settings: M0 (No Attack): Original AI-generated text, unmodified. M1 (Simple Paraphrase): Standard LLM paraphrasing without detector feedback. M2 (StealthRL): Our method, RL-trained paraphrasing against detector ensemble (3 epochs, GRPO + LoRA). M3 (Adversarial Paraphrasing) [2]: Detector-guided candidate selection from multiple paraphrases. M4 (AuthorMist) [4]: RL-trained paraphrasing against single detector (RoBERTa). M5 (Homoglyph) [3]: Character-level Unicode substitution (SilverSpeak). 5 Figure 1: StealthRL training and evaluation pipeline. paraphrase policy (Qwen3-4B with LoRA) is trained via GRPO against two-detector ensemble (RoBERTa + Fast-DetectGPT) with semantic similarity reward. The trained policy is then evaluated against all three detector families, including the held-out Binoculars, at the 1% FPR operating point."
        },
        {
            "title": "4.4 Evaluation Metrics",
            "content": "We report four primary metrics: TPR@1%FPR: True positive rate at 1% false positive rate. Thresholds are calibrated on 1,000 human samples using quantile-based calibration. This is our primary metric as it reflects realistic deployment conditions. ASR (Attack Success Rate): Fraction of AI samples classified as human at the 1% FPR threshold. ASR = 1 TPR@1%FPR. AUROC: Area under the receiver operating characteristic curve. Threshold-independent measure of overall discriminability. E5 Similarity: Cosine similarity between E5 embeddings [24] of original and paraphrased text, measuring semantic preservation. All confidence intervals are computed via bootstrap resampling with 500 iterations and seed 42."
        },
        {
            "title": "4.5 LLM-Based Quality Judge",
            "content": "We additionally evaluate paraphrase quality using an LLM judge, following the growing body of work on LLM-based automatic evaluation. Recent studies have shown that LLMs can serve as effective evaluators of text quality [7], with specialized evaluation models [10] and systematic frameworks for building reliable autoraters [23] demonstrating strong agreement with human judgments. We employ OpenAI gpt-5-nano to score each paraphrase on two 15 Likert axes: 6 Method (TPR@1%) (TPR@1%) (TPR@1%) Mean ASR AUROC M0 No Attack M1 Simple Para. M2 StealthRL M3 Adv. Para. M4 AuthorMist M5 Homoglyph 0.225 0.095 0.002 0.104 0.085 0. 0.396 0.103 0.000 0.089 0.015 0.000 0.410 0.039 0.001 0.045 0.010 0.000 0.344 0.079 0.001 0.079 0.037 0.001 0.656 0.921 0.999 0.921 0.963 0.999 0.739 0.589 0.268 0.595 0.454 0.437 Table 1: Main results on MAGE (TPR@1%FPR, ASR, and AUROC). Lower TPR/AUROC is better for the attacker; higher ASR is better. R/F/B denote RoBERTa, Fast-DetectGPT, and Binoculars respectively. Bold indicates best evasion. 1. Linguistic quality: Fluency, grammaticality, and naturalness of the paraphrase. 2. Semantic similarity: Faithfulness of meaning preservation relative to the source. For fair cross-method comparison, we evaluate shared subset of 200 AI samples per method (M1M5) using identical sample IDs across methods. Each sample is scored independently; the judge sees only the source and paraphrase without method labels."
        },
        {
            "title": "5.1 Main Detection Evasion Results",
            "content": "Table 1 summarizes detection evasion at the 1% FPR operating point across all methods and detectors. StealthRL (M2) achieves near-zero mean TPR@1%FPR (0.001), reducing mean AUROC from 0.74 (no attack) to 0.27 with 99.9% attack success rate. This represents catastrophic robustness failure: at the strict operating point required for real-world deployment, detectors are rendered effectively useless against an adaptive adversary. Figure 2 provides comprehensive visual comparison across all metrics. Panel (a) shows that StealthRL dramatically reduces AUROC across all three detectors, driving two of the three well below the 0.5 randomchance baseline. Panel (b) confirms that the mean AUROC of 0.268 for M2 is well below the 0.5 randomchance baseline. Panel (c) demonstrates near-zero TPR at strict operating points, while Panel (d) shows the 99.9% attack success rate."
        },
        {
            "title": "5.2 Cross-Architecture Transfer",
            "content": "A key finding is the strong transfer of StealthRLs evasion to the held-out Binoculars detector. Despite never seeing Binoculars during training, M2 achieves 0.001 TPR@1%FPR on Binoculars, comparable to its performance on the in-ensemble detectors (0.002 on RoBERTa, 0.000 on Fast-DetectGPT). Figure 3 visualizes the per-detector, per-method TPR matrix, clearly showing that M2 and M5 achieve near-zero TPR across all detector families. This cross-architecture transfer reveals that detectors share common vulnerabilities: they rely on surfacelevel statistical cues (token distributions, perplexity patterns, embedding geometry) that are disrupted by paraphrasing. The attack does not exploit detector-specific weaknesses but rather targets the fundamental fragility of current detection approaches. 7 Figure 2: Detection evasion results for methods M0M5. (a) AUROC by detector. (b) Mean AUROC with confidence intervals. (c) TPR at 1% FPR. (d) Mean attack success rate. StealthRL (M2, teal) achieves below-random AUROC on Fast-DetectGPT and Binoculars and near-zero TPR across all detectors."
        },
        {
            "title": "5.3 Detector Score Analysis",
            "content": "To understand why M2 and M5 achieve near-zero TPR, we examine the raw detector score distributions in Figure 4. For both methods, AI-sample scores are pushed below the 1% FPR threshold, making them statistically indistinguishable from human-written text from the detectors perspective. In contrast, methods M0M3 retain substantial fraction of scores above the threshold, explaining their higher detection rates. Figure 5 provides per-detector AUROC with 95% bootstrap confidence intervals. StealthRL achieves the lowest AUROC on Fast-DetectGPT (0.071) and Binoculars (0.041), while its AUROC on RoBERTa (0.693) is notably higher yet still represents substantial reduction from the no-attack baseline (0.829). The tight confidence intervals confirm the reliability of these estimates. The RoBERTa AUROC anomaly deserves further discussion. Despite achieving near-zero TPR@1%FPR (0.002) on RoBERTa, StealthRLs AUROC on this detector (0.693) is far higher than on Fast-DetectGPT (0.071) or Binoculars (0.041). This apparent contradiction arises because AUROC and TPR@1%FPR measure fundamentally different properties. AUROC captures global rank separability across all possible thresholds, while TPR@1%FPR measures detection power at single strict operating point. The RoBERTa classifier retains moderate ability to rank AI-paraphrased text above human text on average, but at the strict 1% FPR threshold, the overlap between human and AI score distributions is sufficient to render detection ineffective. Concretely, the RoBERTa score distribution for M2 paraphrases is shifted leftward (toward human-like scores) enough that nearly all AI samples fall below the 1% FPR threshold, even though the 8 Figure 3: TPR@1%FPR heatmap across detectors and methods. Darker colors indicate higher detection rates. StealthRL (M2) and Homoglyph (M5) achieve near-zero TPR across all three detector families, including the held-out Binoculars. Figure 4: Detector score distributions for AI samples across methods (one panel per detector). StealthRL (M2) and Homoglyph (M5) push scores below the detection threshold, explaining their near-zero TPR@1%FPR. distribution means remain partially separated. This pattern suggests that the RL policy learns to target the specific score region near the decision boundary rather than fully collapsing the detectors discriminative capacity. For practical deployment, the TPR@1%FPR metric is more security-relevant, as detectors must operate at low false positive rates to avoid falsely accusing human writers."
        },
        {
            "title": "5.4 Evasion–Quality Tradeoff",
            "content": "Achieving strong evasion without degrading text quality is the central challenge. Figure 6 plots mean TPR@1%FPR against E5 semantic similarity for each method. M1 and M3 preserve high similarity (0.960 and 0.976) but achieve only moderate evasion (0.079 mean TPR). M2 and M5 both achieve near-zero TPR, with comparable E5 similarity (0.896 for M2 vs. 0.899 for M5), but M2 achieves substantially better judged quality. Table 2 and Figure 7 present the LLM-based quality evaluation. The Likert scores reveal more nuanced picture than E5 similarity alone. While M2s E5 similarity (0.896) is close to M5s (0.899), the LLM judge 9 Figure 5: Per-detector AUROC with 95% bootstrap confidence intervals. StealthRL (M2, teal) achieves below-random AUROC on Fast-DetectGPT (0.071) and Binoculars (0.041), with substantial reduction on RoBERTa (0.693). The dashed line marks the 0.5 random-chance baseline. Method E5 Sim. Quality Similarity Perplexity ASR M0 No Attack M1 Simple Para. M2 StealthRL M3 Adv. Para. M4 AuthorMist M5 Homoglyph 1.000 0.960 0.896 0.976 0.935 0.899 n/a 4.01 2.59 4.03 3.64 2.01 n/a 4.05 2.67 4.03 3.70 2.94 n/a 21.3 48.7 18.9 32.1 152.4 0.656 0.921 0.999 0.921 0.963 0. Table 2: Quality and similarity metrics. E5 Sim. is embedding cosine similarity. Quality and Similarity are mean Likert scores (15) from gpt-5-nano judge on 200 matched samples per method. Higher is better for all quality metrics; higher ASR is better for evasion. rates M2 substantially higher on quality (2.59 vs. 2.01 for M5). M5 scores higher on judged similarity (2.94 vs. 2.67), likely because character-level substitutions preserve surface form while degrading readability. Overall, learned paraphrasing produces more natural text than character-level obfuscation. Methods M1 and M3, which achieve weaker evasion, score highest on quality (4.01 and 4.03), illustrating the fundamental tradeoff between evasion effectiveness and output quality."
        },
        {
            "title": "5.5 Discussion",
            "content": "The severe failure across the three detector architectures tested reveals significant vulnerabilities in current AI-text detection. Detectors rely on brittle statistical cues, including token frequency distributions, perplexity patterns, and embedding geometry, rather than robust semantic understanding. Surface-level paraphrasing that preserves meaning suffices to evade detection, suggesting that detectors learn superficial correlates of AI text rather than deeper linguistic features. This robustness gap has critical security implications. Adversaries can train adaptive attacks against deployed detectors, rendering them ineffective with modest computational resources (a single LoRA finetuning run). The strong transfer to held-out architectures means that ensemble defenses (combining multiple 10 Figure 6: Evasionquality tradeoff. Each point represents method, plotted by E5 semantic similarity (x-axis) against mean TPR@1%FPR (y-axis). Lower-right is ideal (high similarity, low detection). The dashed line shows the Pareto frontier. detectors) provide limited robustness improvement, as the attack generalizes across detector families."
        },
        {
            "title": "6.1 Limitations",
            "content": "Detector coverage. Our evaluation covers three detector families (fine-tuned classifier, zero-shot statistical, paired-LM). We do not evaluate against watermark-based detectors [11], which embed signals during generation and may be more robust to paraphrasing attacks. Evaluating StealthRL against watermarked text is an important direction for future work. Dataset diversity. We evaluate on single benchmark (MAGE) in English. Broader coverage across datasets (RAID [6]), domains, and languages is needed to establish generalizability. The MAGE benchmark, while diverse, may not capture all deployment scenarios. Quality gap. StealthRL achieves lower semantic fidelity (E5 similarity 0.896, Likert quality 2.59) compared to simpler baselines (M1: 0.960/4.01, M3: 0.976/4.03). Improving semantic preservation while maintaining strong evasion is an important direction. Techniques such as constrained decoding, rejection sampling, or multi-objective RL could help close this gap. Defense evaluation. We do not explore defensive strategies such as adversarial training, certified robustness, or ensemble diversification that could improve detector resilience. Our focus is on exposing vulnerabilities to motivate defensive research. 11 Figure 7: LLM-based quality evaluation (gpt-5-nano Likert judge). (a) Linguistic quality scores. (b) Semantic similarity scores. Methods with stronger evasion (M2, M5) show lower quality scores, reflecting the evasion quality tradeoff. The dashed line marks the neutral midpoint (3.0)."
        },
        {
            "title": "6.2 Ethical Considerations",
            "content": "Adversarial paraphrasing is dual-use technology. We position StealthRL as stress-testing and robustness evaluation tool for researchers and detector developers, not production evasion system. The near-zero TPR@1%FPR result exposes critical vulnerabilities that must be addressed before detectors are deployed in high-stakes applications such as academic integrity enforcement. We release our code and evaluation pipeline to enable reproducible robustness assessment and to accelerate defensive research. By making attack capabilities transparent, we aim to shift the detector development paradigm toward adversarial robustness rather than clean-distribution accuracy. We believe responsible disclosure of detector vulnerabilities, accompanied by tools for measuring progress, serves the broader goal of trustworthy AI-text detection."
        },
        {
            "title": "7 Conclusion and Future Work",
            "content": "StealthRL demonstrates severe detector failure under adaptive RL-based paraphrasing attacks, revealing significant robustness gaps in the AI-text detectors evaluated. By training against multi-detector ensemble with GRPO and LoRA, we achieve near-zero detection (0.001 mean TPR@1%FPR) with strong crossarchitecture transfer, including to held-out detector family. Our comprehensive evaluation, spanning detection metrics, quality assessment, and score distribution analysis, provides complete picture of the evasionquality tradeoff. Several directions for future work emerge from our findings: Adversarial training: Incorporating adversarial examples into detector training to improve robustness against adaptive attacks. Semantic-aware detectors: Developing detection methods that rely on deeper linguistic features rather than surface-level statistical cues. Provable robustness: Establishing theoretical guarantees on detector robustness under bounded perturbations. Multi-objective optimization: Improving StealthRLs quality preservation through constrained RL or Pareto-optimal training. 12 Broader evaluation: Extending to additional datasets, languages, and detector families (including watermark-based methods). Our evaluation framework and released code provide rigorous testbed for measuring progress on adversarially robust AI-text detection."
        },
        {
            "title": "Acknowledgments",
            "content": "We gratefully acknowledge Thinking Machines for providing free research credits and access to their Tinker API framework, which made the RL fine-tuning possible. We also thank the open-source community for the detector implementations and model checkpoints that enabled this evaluation."
        },
        {
            "title": "References",
            "content": "[1] Guangsheng Bao, Yanbin Zhao, Zhiyang Teng, Linyi Yang, and Yue Zhang. Fast-detectgpt: Efficient zero-shot detection of machine-generated text via conditional probability curvature, 2024. URL https://arxiv.org/abs/2310.05130. [2] Yize Cheng, Vinu Sankar Sadasivan, Mehrdad Saberi, Shoumik Saha, and Soheil Feizi. Adversarial paraphrasing: universal attack for humanizing ai-generated text, 2025. URL https://arxiv. org/abs/2506.07001. [3] Aldan Creo and Shushanta Pudasaini. Silverspeak: Evading ai-generated text detectors using homoglyphs, 2025. URL https://arxiv.org/abs/2406.11239. [4] Isaac David and Arthur Gervais. Authormist: Evading ai text detectors with reinforcement learning, 2025. URL https://arxiv.org/abs/2503.08716. [5] Tim Dettmers, Artidoro Pagnoni, Ari Holtzman, and Luke Zettlemoyer. Qlora: Efficient finetuning of quantized llms, 2023. URL https://arxiv.org/abs/2305.14314. [6] Liam Dugan, Alyssa Hwang, Filip Trhlik, Josh Magnus Ludan, Andrew Zhu, Hainiu Xu, Daphne Ippolito, and Chris Callison-Burch. Raid: shared benchmark for robust evaluation of machinegenerated text detectors, 2024. URL https://arxiv.org/abs/2405.07940. [7] Jinlan Fu, See-Kiong Ng, Zhengbao Jiang, and Pengfei Liu. Gptscore: Evaluate as you desire, 2023. URL https://arxiv.org/abs/2302.04166. [8] Abhimanyu Hans, Avi Schwarzschild, Valeriia Cherepanova, Hamid Kazemi, Aniruddha Saha, Micah Goldblum, Jonas Geiping, and Tom Goldstein. Spotting llms with binoculars: Zero-shot detection of machine-generated text, 2024. URL https://arxiv.org/abs/2401.12070. [9] Edward J. Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean Wang, Lu Wang, and Weizhu Chen. Lora: Low-rank adaptation of large language models, 2021. URL https: //arxiv.org/abs/2106.09685. [10] Seungone Kim, Juyoung Suk, Shayne Longpre, Bill Yuchen Lin, Jamin Shin, Sean Welleck, Graham Neubig, Moontae Lee, Kyungjae Lee, and Minjoon Seo. Prometheus 2: An open source language model specialized in evaluating other language models, 2024. URL https://arxiv.org/abs/2405. 01535. 13 [11] John Kirchenbauer, Jonas Geiping, Yuxin Wen, Jonathan Katz, Ian Miers, and Tom Goldstein. watermark for large language models, 2023. URL https://arxiv.org/abs/2301.10226. [12] Kalpesh Krishna, Yixiao Song, Marzena Karpinska, John Wieting, and Mohit Iyyer. DipParaphrase model released per: Discourse paraphrasing via diverse paraphrasing, 2023. with Krishna et al., 2023. Available at https://github.com/martiansideofthemoon/ ai-detection-paraphrases. [13] Kalpesh Krishna, Yixiao Song, Marzena Karpinska, John Wieting, and Mohit Iyyer. Paraphrasing evades detectors of ai-generated text, but retrieval is an effective defense, 2023. URL https:// arxiv.org/abs/2303.13408. [14] Yafu Li, Qintong Li, Leyang Cui, Wei Bi, Zhilin Wang, Longyue Wang, Linyi Yang, Shuming Shi, and Yue Zhang. Mage: Machine-generated text detection in the wild, 2024. URL https://arxiv. org/abs/2305.13242. [15] Eric Mitchell, Yoonho Lee, Alexander Khazatsky, Christopher D. Manning, and Chelsea Finn. Detectgpt: Zero-shot machine-generated text detection using probability curvature, 2023. URL https://arxiv. org/abs/2301.11305. [16] Long Ouyang, Jeff Wu, Xu Jiang, Diogo Almeida, Carroll L. Wainwright, Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, et al. Training language models to follow instructions with human feedback, 2022. URL https://arxiv.org/abs/2203.02155. [17] Lei Pang and Ruinan Jin. On the theory and practice of grpo: trajectory-corrected approach with fast convergence, 2025. URL https://arxiv.org/abs/2508.02833. [18] Vinu Sankar Sadasivan, Aounon Kumar, Sriram Balasubramanian, Wenxiao Wang, and Soheil Feizi. Can ai-generated text be reliably detected?, 2023. URL https://arxiv.org/abs/2303.11156. [19] John Schulman, Filip Wolski, Prafulla Dhariwal, Alec Radford, and Oleg Klimov. Proximal policy optimization algorithms, 2017. URL https://arxiv.org/abs/1707.06347. [20] Zhihong Shao, Peiyi Wang, Qihao Zhu, Runxin Xu, Junxiao Song, Mingchuan Zhang, Y.K. Li, Y. Wu, and Daya Guo. Deepseekmath: Pushing the limits of mathematical reasoning in open language models, 2024. URL https://arxiv.org/abs/2402.03300. [21] Irene Solaiman, Miles Brundage, Jack Clark, Amanda Askell, Ariel Herbert-Voss, Jeff Wu, Alec Radford, and Jasmine Wang. Release strategies and the social impacts of language models, 2019. URL https://arxiv.org/abs/1908.09203. [22] Vivek Verma, Eve Fleisig, Nicholas Tomlin, and Dan Klein. Ghostbuster: Detecting text ghostwritten by large language models, 2024. URL https://arxiv.org/abs/2305.15047. [23] Tu Vu, Kalpesh Krishna, Salaheddin Alzubi, Chris Tar, Manaal Faruqui, and Yun-Hsuan Sung. Foundational autoraters: Taming large language models for better automatic evaluation, 2024. URL https://arxiv.org/abs/2407.10817. [24] Liang Wang, Nan Yang, Xiaolong Huang, Binxing Jiao, Linjun Yang, Daxin Jiang, Rangan Majumder, and Furu Wei. Text embeddings by weakly-supervised contrastive pre-training, 2022. URL https: //arxiv.org/abs/2212.03533."
        },
        {
            "title": "A Qualitative Examples",
            "content": "Tables 35 show representative paraphrases across different domains from the MAGE test split. Original (M0) StealthRL (M2) During cardio the heart increases its workload and all the bodys other systems adjust to help support that endeavor. The blood vessels dilate, the muscles do their best to help pump blood back to the heart, and the lungs work harder to take in oxygen and remove waste gases like carbon dioxide. In cardio, the heart ramps up its workload, prompting the bodys other systems to adapt and assist. Blood vessels widen, muscles strive to push blood back to the heart, and lungs intensify their efforts to absorb oxygen and expel waste gases such as carbon dioxide. The engine has to endure the torque of powering two axels and drive shaft generally the transfer casing connects to the drive shaft with ujoint and same for power distribution. The electricity goes into the battery first then is sent to the alternator where it generates voltage that powers all other electrical components like the lights, radio. The engine must handle the torque from two axles and drive shaft, typically linked via universal joint in the transfer casing for power distribution. Electricity first charges the battery, then flows to the alternator, which converts it into voltage to power essential electrical systems such as lights and the radio. Table 3: Representative paraphrases from the MAGE test split (health and automotive domains). Original (M0) StealthRL (M2) Machine learning algorithms have become increasingly sophisticated in recent years, with deep neural networks achieving state-of-the-art performance across wide range of tasks including image classification, natural language processing, and reinforcement learning. Recent advances in machine learning have led to highly sophisticated algorithms. Deep neural networks now achieve top performance in diverse areas, from classifying images and processing natural language to learning through reinforcement-based approaches. The process of photosynthesis converts light energy into chemical energy that can be stored in glucose molecules. This process occurs in the chloroplasts of plant cells and involves two main stages: the light-dependent reactions and the Calvin cycle. Photosynthesis transforms light energy into chemical energy, storing it within glucose molecules. Taking place in plant cell chloroplasts, this process unfolds in two key stages: light-dependent reactions followed by the Calvin cycle. Table 4: Representative paraphrases (science and technology domains). Original (M0) StealthRL (M2) The global economy faces several interconnected challenges, including rising inflation, supply chain disruptions, and geopolitical tensions. Central banks worldwide have responded by tightening monetary policy, raising interest rates to combat price pressures. Several intertwined challenges confront the global economy: climbing inflation, supply chain breakdowns, and geopolitical friction. In response, central banks around the world have tightened monetary policy and hiked interest rates to curb rising prices. Renewable energy sources such as solar and wind power have seen dramatic cost reductions over the past decade, making them competitive with traditional fossil fuels in many markets. Government incentives and technological improvements continue to drive adoption rates higher. Solar and wind power costs have fallen dramatically over the last decade, allowing renewables to compete with fossil fuels across numerous markets. Ongoing government incentives and technological progress continue pushing adoption rates upward. Table 5: Representative paraphrases (economics and energy domains)."
        },
        {
            "title": "B Hyperparameters and Configuration",
            "content": "Parameter Value Model & LoRA Base model LoRA rank LoRA alpha LoRA dropout Training Algorithm Learning rate Batch size Group size Epochs Training samples KL penalty coefficient Reference policy Reward Detector weight (α) Semantic weight (β) Detector ensemble Semantic metric Inference Temperature Top-p Max tokens Prompt template Detectors RoBERTa OpenAI Fast-DetectGPT Binoculars Evaluation Test samples Token window FPR calibration Candidates per sample Compute Training framework Reward computation Offline evaluation Seed Qwen/Qwen3-4B-Instruct-2507 32 32 0.05 Group-Relative Policy Optimization (GRPO) 2.8 104 16 8 3 10,000 AI-generated (custom MAGE subset) 0.05 Qwen3-4B-Instruct (frozen) 1.0 0.1 RoBERTa (0.6) + Fast-DetectGPT (0.4) E5 embedding cosine similarity 1.0 0.9 512 Paraphrase the following text while preserving its meaning: [TEXT] openai-community/roberta-large-openai-detector Scoring model: EleutherAI/gpt-neo-2.7B Lightweight: gpt2-medium + gpt2-large (held-out) 1,000 human + 1,000 AI (MAGE test) 100500 tokens 1% on 1,000 human samples (quantile) 1 Tinker API (Thinking Machines) MacBook + NVIDIA A10 GPUs MacBook + NVIDIA A10 GPUs 42 Table 6: Complete hyperparameters and configuration for reproducibility. Per-Detector Results with Confidence Intervals Table 7 provides the complete per-detector, per-method results with 95% bootstrap confidence intervals (500 iterations). Detector Method AUROC [95% CI] TPR@1%FPR [95% CI] ASR [95% CI] RoBERTa FastDetectGPT Binoculars M0 No Attack 0.829 [0.812, 0.847] M1 Simple Para. 0.694 [0.672, 0.717] 0.693 [0.670, 0.715] M2 StealthRL 0.704 [0.681, 0.728] M3 Adv. Para. M4 AuthorMist 0.651 [0.627, 0.675] M5 Homoglyph 0.637 [0.613, 0.660] M0 No Attack 0.671 [0.648, 0.694] M1 Simple Para. 0.551 [0.526, 0.575] 0.071 [0.060, 0.085] M2 StealthRL 0.552 [0.526, 0.575] M3 Adv. Para. M4 AuthorMist 0.352 [0.328, 0.375] M5 Homoglyph 0.077 [0.068, 0.090] M0 No Attack 0.716 [0.694, 0.740] M1 Simple Para. 0.522 [0.497, 0.547] 0.041 [0.032, 0.050] M2 StealthRL 0.528 [0.502, 0.552] M3 Adv. Para. M4 AuthorMist 0.359 [0.336, 0.383] M5 Homoglyph 0.598 [0.574, 0.626] 0.225 [0.204, 0.250] 0.095 [0.077, 0.115] 0.002 [0.000, 0.005] 0.104 [0.086, 0.122] 0.085 [0.067, 0.103] 0.002 [0.000, 0.005] 0.396 [0.369, 0.426] 0.103 [0.085, 0.123] 0.000 [0.000, 0.000] 0.089 [0.071, 0.107] 0.015 [0.008, 0.023] 0.000 [0.000, 0.000] 0.410 [0.383, 0.441] 0.039 [0.027, 0.051] 0.001 [0.000, 0.003] 0.045 [0.033, 0.057] 0.010 [0.004, 0.017] 0.000 [0.000, 0.000] 0.775 [0.750, 0.796] 0.905 [0.885, 0.923] 0.998 [0.995, 1.000] 0.896 [0.878, 0.914] 0.915 [0.897, 0.933] 0.998 [0.995, 1.000] 0.604 [0.574, 0.631] 0.897 [0.877, 0.915] 1.000 [1.000, 1.000] 0.911 [0.893, 0.929] 0.985 [0.977, 0.992] 1.000 [1.000, 1.000] 0.590 [0.559, 0.617] 0.961 [0.949, 0.973] 0.999 [0.997, 1.000] 0.955 [0.943, 0.967] 0.990 [0.983, 0.996] 1.000 [1.000, 1.000] Table 7: Complete per-detector results with 95% bootstrap confidence intervals. Bold indicates best evasion per detector. StealthRL (M2) achieves near-zero TPR@1%FPR across all three detector families."
        },
        {
            "title": "D LLM Judge Prompt Templates",
            "content": "We use the following prompt template for the gpt-5-nano Likert judge evaluation: You are an expert evaluator of text quality. text and paraphrased version. using 1-5 Likert scale. Rate the paraphrase on two dimensions You will be given an original Original text: {source text} Paraphrased text: {paraphrase text} Rate on: 1. QUALITY (1-5): paraphrase? (1=incoherent, 5=perfectly natural) 2. How well does the paraphrase preserve the meaning of the original? different, 5=identical meaning) How fluent, grammatical, and natural is the SIMILARITY (1-5): (1=completely Respond in JSON format: {\"quality\": \"<str>\", \"similarity justification\": <int>, \"similarity\": \"<str>\"} <int>, \"quality justification\":"
        },
        {
            "title": "E Dataset Statistics",
            "content": "17 Stage Purpose Human Samples AI Samples Train Test Likert eval Token range Source Domains RL fine-tuning (GRPO) Detection evaluation Quality scoring (per method) 0 1,000 0 10,000 1,000 200 100500 tokens MAGE benchmark [14] Multiple (essays, Q&A, creative writing) Table 8: Dataset statistics for training and evaluation. The Likert evaluation uses 200 AI samples per attack method (M1M5) with matched sample IDs."
        }
    ],
    "affiliations": [
        "University of California, San Diego"
    ]
}