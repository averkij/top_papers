{
    "paper_title": "Fine-Grained Perturbation Guidance via Attention Head Selection",
    "authors": [
        "Donghoon Ahn",
        "Jiwon Kang",
        "Sanghyun Lee",
        "Minjae Kim",
        "Jaewon Min",
        "Wooseok Jang",
        "Saungwu Lee",
        "Sayak Paul",
        "Susung Hong",
        "Seungryong Kim"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Recent guidance methods in diffusion models steer reverse sampling by perturbing the model to construct an implicit weak model and guide generation away from it. Among these approaches, attention perturbation has demonstrated strong empirical performance in unconditional scenarios where classifier-free guidance is not applicable. However, existing attention perturbation methods lack principled approaches for determining where perturbations should be applied, particularly in Diffusion Transformer (DiT) architectures where quality-relevant computations are distributed across layers. In this paper, we investigate the granularity of attention perturbations, ranging from the layer level down to individual attention heads, and discover that specific heads govern distinct visual concepts such as structure, style, and texture quality. Building on this insight, we propose \"HeadHunter\", a systematic framework for iteratively selecting attention heads that align with user-centric objectives, enabling fine-grained control over generation quality and visual attributes. In addition, we introduce SoftPAG, which linearly interpolates each selected head's attention map toward an identity matrix, providing a continuous knob to tune perturbation strength and suppress artifacts. Our approach not only mitigates the oversmoothing issues of existing layer-level perturbation but also enables targeted manipulation of specific visual styles through compositional head selection. We validate our method on modern large-scale DiT-based text-to-image models including Stable Diffusion 3 and FLUX.1, demonstrating superior performance in both general quality enhancement and style-specific guidance. Our work provides the first head-level analysis of attention perturbation in diffusion models, uncovering interpretable specialization within attention layers and enabling practical design of effective perturbation strategies."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 2 1 ] . [ 1 8 7 9 0 1 . 6 0 5 2 : r Fine-Grained Perturbation Guidance via Attention Head Selection Sanghyun Lee2 Saungwu Lee4 Donghoon Ahn1 Minjae Kim3 Jiwon Kang1 Jaewon Min1 Sayak Paul5 Susung Hong Wooseok Jang1 Seungryong Kim1 1KAIST AI 2KAIST 3Korea University 4Krea AI 5HuggingFace 6University of Washington"
        },
        {
            "title": "Abstract",
            "content": "Recent guidance methods in diffusion models steer reverse sampling by perturbing the model to construct an implicit weak model and guide generation away from it. Among these approaches, attention perturbation has demonstrated strong empirical performance in unconditional scenarios where classifier-free guidance is not applicable. However, existing attention perturbation methods lack principled approaches for determining where perturbations should be applied, particularly in Diffusion Transformer (DiT) architectures where quality-relevant computations are distributed across layers. In this paper, we investigate the granularity of attention perturbations, ranging from the layer level down to individual attention heads, and discover that specific heads govern distinct visual concepts such as structure, style, and texture quality. Building on this insight, we propose HeadHunter\", systematic framework for iteratively selecting attention heads that align with user-centric objectives, enabling fine-grained control over generation quality and visual attributes. In addition, we introduce SoftPAG, which linearly interpolates each selected heads attention map toward an identity matrix, providing continuous knob to tune perturbation strength and suppress artifacts. Our approach not only mitigates the oversmoothing issues of existing layer-level perturbation but also enables targeted manipulation of specific visual styles through compositional head selection. We validate our method on modern large-scale DiT-based text-toimage models including Stable Diffusion 3 and FLUX.1, demonstrating superior performance in both general quality enhancement and style-specific guidance. Our work provides the first head-level analysis of attention perturbation in diffusion models, uncovering interpretable specialization within attention layers and enabling practical design of effective perturbation strategies."
        },
        {
            "title": "Introduction",
            "content": "Diffusion models [27, 64, 63, 66, 68, 15, 55, 50] and flow-matching models [42, 45, 19] have gained great popularity in visual generation tasks, including images [55, 50, 49, 6, 19], videos [29, 26, 4, 74], 3D [51, 40, 70, 60], and 4D content [61, 76, 71]. The key behind the success of diffusion models lies in classifier-free guidance (CFG) [28], which substantially enhances image generation quality during conditional inference. Despite its effectiveness, CFG has two key limitations. First, it applies only to conditional generation, limiting its applicability in unconditional settings such as inverse problems [33, 10, 65, 11]. Second, CFG often reduces sample diversity and leads to over-saturated or overly simplified outputs [57, 36, 58, 34]. : Equally contributed as first author, : Equally contributed as secondw author, : Corresponding author To address the limitations of CFG in unconditional scenarios, alternative guidance strategies [1, 31, 30, 34, 32] have been proposed. These methods steer the denoising trajectory by perturbing the input or the model itself, or by training weaker model, thereby guiding samples away from lowquality regions and toward the high-quality data manifold. Among these strategies, attention-layer perturbation approaches [1, 30] continue to be practical and widely explored strategy, as they can generate well-aligned weak models without additional training. Some papers explain these guidance methods from an energy perspective [30] and draw connections to Hopfield networks [13, 54], which empirically work well with specific blocks (medium blocks of U-Net [56] architecture) of attention. However, there is still limited understanding of where perturbations should be applied. In particular, Diffusion Transformer (DiT) [49] architectures lack localized blocks responsible for global semantics, unlike U-Nets [56], and instead distribute this functionality more evenly across all layers [2]. This structural difference makes it even more critical to carefully select perturbation targets to achieve effective guidance. To determine suitable perturbation targets, we first consider the underlying structure of DiT. In DiT, multi-head self-attention [69] plays central role in modeling global dependencies. Each head attends to different aspects of the input, and prior work has shown that heads often specialize in distinct semantic features [69, 48, 17, 23]. This suggests that attention heads, rather than entire layers, may serve as more precise and effective targets for perturbation. Motivated by this, we explore finer-grained yet semantically meaningful computational units within attention layers: attention heads. Interestingly, we observe that individual head-level perturbation guidance often capture interpretable visual concepts and specialize in tasks such as enhancing structural fidelity or injecting stylistic elements. Moreover, these functional roles can be composed by combining multiple heads. Building on these observations, we (i) analyze key properties of head-level perturbation guidance, including composability and controllability, (ii) propose HeadHunter, systematic framework for retrieving heads that align with arbitrary objectives, and (iii) introduce SoftPAG, variant of PAG [1] that enables fine-grained, continuous control over guidance strength, mitigating over-smoothness and oversimplification caused by overly aggressive perturbations. We demonstrate that HeadHunter not only outperforms layer-level perturbation guidance in general quality improvement but also enables targeted manipulation of visual styles, as supported by strong qualitative and quantitative results. In summary, our main contributions are as follows: To the best of our knowledge, we are the first to apply perturbations at the level of individual attention heads, enabling fine-grained and concept-specific control. We analyze the properties of head-level perturbation guidance in Diffusion Transformers (DiT), providing insights into the specialization and combination of such heads. We propose HeadHunter, systematic head selection framework for arbitrary objectives, and demonstrate its effectiveness in both general quality enhancement and style-specific guidance. We introduce SoftPAG, which interpolates each selected heads attention map toward the identity matrix, providing continuous knob to tune perturbation strength and to mitigate oversaturation and oversimplification."
        },
        {
            "title": "2 Preliminaries",
            "content": "2.1 Generative Modeling Frameworks Diffusion models. Diffusion models [27, 62, 67] define generative process as iterative denoising of sample drawn from Gaussian prior. The forward process gradually perturbs clean data point x0 pdata through the noise schedule (αt, σt): xt = αtx0 + σtϵ, ϵ (0, I), [0, 1]. (1) neural network ˆϵθ(xt, t) learns to predict the added noise ϵ, enabling reverse-time sampling from noise to data. 2 Flow matching. Flow matching [43, 44] provides deterministic alternative by learning continuous-time velocity field guiding linear interpolation from noise to data: xt = (1 t)x0 + ϵ, ϵ (0, I), with the associated velocity field u(xt, t) = ϵ x0. (2) neural network ˆuθ(xt, t) is trained to approximate this target velocity. 2.2 Architectures for Diffusion Models Convolutional U-Net with self-attention. Early latent diffusion models (e.g., Stable Diffusion 1 and 2) [55] primarily employed convolutional U-Net architectures [56], featuring hierarchical downsampling, residual convolutions, and skip connections. Within these architectures, self-attention is occasionally applied to intermediate spatial feature maps to model global dependencies. Multi-head attention mechanism. Vaswani et al. [69] demonstrated that, rather than performing single attention operation, it is beneficial to linearly project queries, keys, and values into multiple subspaces and compute attention in parallel. This technique is known as the multi-head attention mechanism. Specifically, at the l-th layer of the denoising network ˆϵθ, each attention head {1, . . . , Hl} applies separate linear projections: Ql,h = QlWQ l,h, Kl,h = KlWK l,h, Vl,h = VlWV l,h, (3) where Ql, Kl, Vl RN are the query, key, and value matrices computed at layer l, where is l,h Rd are the head-specific the sequence length and is the feature dimension, WQ projection matrices, and is the dimensionality of each head. The attention map and head output are computed as: l,h, WK l,h, WV Al,h = Softmax , headl,h = Al,hVl,h. (cid:33) (cid:32) Ql,hK l,h The outputs from all heads are then aggregated using an output linear projection: MultiHead(Ql, Kl, Vl) = Concat(headl,1, . . . , headl,Hl )WO , where WO index in the remainder of this section when the context is clear: R(Hl d)d is the output projection matrix at layer l. For brevity, we omit the layer Multi-head attention allows the model to jointly attend to information from different representation subspaces at different positions. Prior works [17, 48, 23] have sought to interpret and analyze the role of individual attention heads in both language and vision transformers, revealing that heads often capture semantically meaningful and diverse functions. Diffusion Transformer (DiT) and multi-modal attention (MM-attention). Transformer-based architectures have played significant role in recent progress across various machine learning domains [14, 53, 16, 9], offering favorable scalability with respect to model capacity and data size. In the context of diffusion models, the Diffusion Transformer (DiT) [49] replaces convolutional structures with pure multi-head attention and demonstrates strong scaling behavior. This has motivated the development of various transformer-based diffusion models [8, 6, 7, 45]. Unlike U-Net [56] architectures with explicit multi-resolution structures, DiT lacks coarse-to-fine synthesis hierarchy, making it less clear which layers capture high-level semantic features [2]. This structural difference can complicate the application of conventional image editing and perturbation techniques. More recently, multi-modal attention-based Diffusion Transformers (MM-DiT)[19, 37, 21] have been gaining traction in large-scale text-to-image diffusion. MM-DiT takes as input concatenated sequence of patchified latent pixels and text embeddings, enabling fully bidirectional interactions between visual and textual modalities. Due to their scalability and performance, MM-DiTs are now used as the backbone in state-of-the-art models such as Stable Diffusion 3 [19], FLUX.1 [37]. 3 (4) (5) Unless otherwise noted, all experiments in the main paper use the publicly available Stable Diffusion 31 as the default architecture. Additional results using FLUX.1-Dev2 are provided in Appendix. 2.3 Diffusion Guidance Mechanisms Classifier-free guidance (CFG). Classifier-free guidance [28] enhances conditional generation without relying on an external classifier. CFG trains the diffusion model to support both conditional and unconditional generation by randomly dropping conditioning tokens during training. At inference, the prediction is interpolated between conditional and unconditional outputs: ˆϵCFG = (1 + w)ˆϵcond wˆϵuncond, (6) where is the guidance scale, and ˆϵcond, ˆϵuncond denote predictions with and without conditioning, respectively. Attention perturbation guidance. As attention maps inherently encode spatial and semantic structures [5, 47], perturbing them and guiding the denoising trajectory away from that bad prediction can enhance the fidelity or structural quality of the image at test time [1, 30, 34]. Analogous to CFG [28], attention perturbation guidance extrapolates the original and perturbed outputs: ˆϵguided = (1 + w)ˆϵoriginal wˆϵperturbed, (7) where ˆϵperturbed denotes the predicted epsilon under attention perturbation. Specifically, this is achieved by modifying the attention maps Al,h in Eq. 4 during the forward pass, resulting in the perturbed prediction ˆϵperturbed. Let denote the set of selected layers to perturb in the denoising network ˆϵθ (typically the bottleneck layers in U-Net architectures), and for each L, let Hl = {1, . . . , Hl} denote the set of heads within layer l. During the forward pass, attention perturbation guidance apply perturbation to the attention maps Al,h for all (l, h) pairs in the set = {(l, h) L, Hl}. Specifically, perturbed-attention guidance (PAG) [1] directly replaces the attention maps in certain layers with an identity matrix RN : A(PAG) l,h = I, (8) effectively disabling contextual aggregation. An alternative approach, smoothed energy guidance (SEG) [30] indirectly perturbs attention by applying Gaussian blur Gσ to the queries: Q(blur) l,h = Gσ Ql,h, A(SEG) l,h = Softmax (cid:32) Q(blur) l,h l,h (cid:33) , (9) softening the attention distribution. The perturbations of PAG and SEG produce aligned but weakened models that still maintain the same architecture. However, existing perturbation guidance methods have typically treated attention maps of layer as monolithic entities, following the common editing convention [24]. This overlooks the fact that individual attention heads are often highly specialized and attend to different aspects of the input features [69, 48, 17, 23]. Moreover, both methods perturb entire layers chosen ad-hoc; no systematic rule exists for where to intervene. We therefore treat single attention head as the perturbation unit, analyzing its effect in the following section."
        },
        {
            "title": "3 Motivation",
            "content": "Attention perturbation guidance [1, 30, 34] steers generation away from weaker predictions by slightly altering the models forward pass. Thus, deciding what to weaken becomes critical, but 1https://huggingface.co/stabilityai/stable-diffusion-3-medium 2https://huggingface.co/black-forest-labs/FLUX.1-dev 4 Figure 1: Motivating example. Each image is generated with PAG, where perturbation is applied to single attention head within DiTs. Guiding with different perturbed attention heads produces notably distinct results. Results for additional heads are provided in Appendix E.1. All images are generated with the prompt smiling girl holding cat, in flower garden using stable-diffusion-3-medium. Each row corresponds to single layer, with different heads perturbed across columns. principled methods for determining where in the diffusion network to apply perturbation remain underexplored. As result, prior works often rely on heuristic selection of perturbation targets, such as the middle block of U-Net [56], which is known to process high-level semantic information with global self-attention [31, 1]. However, unlike U-Net, transformer-based architectures as in DiTs [49, 6, 19, 37, 21] lack coarseto-fine synthesis structure [2]. This absence of an explicit bottleneck makes it challenging to directly identify layers responsible for high-level semantics. Instead, the model distributes semantic processing more uniformly across layers. In this setting, attention, particularly multi-head self-attention [69], plays central role in modeling global dependencies without convolutions. Each attention head processes different aspects of the input, and prior works in large language models and vision transformers have shown that heads specialize in capturing distinct semantic attributes [69, 48, 17, 23]. This suggests that attention heads, rather than entire layers, may serve as more effective units for applying perturbation. Motivated by this, we increase the granularity of attention perturbation from layers to individual attention heads, and contrast the conventional layer-level perturbation guidance (simply, layer-level guidance) with our proposed head-level perturbation guidance (simply, head-level guidance). As shown in Fig. 1, perturbing different heads leads to clearly distinct effects in the generated images, indicating that heads act as semantically meaningful substructures. This highlights the limitations of layer-level perturbation, which may be overly coarse and suboptimal. Even with carefully chosen layers, layer-level perturbation fails to leverage the modularity and functional diversity inherent in multi-head attention, motivating our focus on head-level guidance. 3.1 Analysis of head-level perturbation guidance Definition. We define head-level perturbation guidance as applying perturbations selectively to subset of attention heads during the forward pass. Specifically, given set = {(l1, h1), . . . , (lm, hm)} of selected (layer, head) pairs, we replace their attention maps with identity matrices: A(PAG) l,h = for (l, h) S, (10) l,h where A(PAG) denotes the perturbed attention map of head at layer l, and is the identity matrix used in PAG [1]. While we illustrate this with PAG, the formulation also applies to other perturbation methods such as SEG [30], with each head perturbed independently. See Appendix for details. Intra-layer heterogeneity. Even when we perturb the attention map of single layer within diffusion models, it exhibits highly polysemantic attentive behavior, as the computation in individual heads occurs independently and diversely [18, 48]. Therefore, applying the same perturbations across Figure 2: Generated images from headand layer-level perturbation guidance. (a) Layer-level guidance: Attention perturbation is applied to all heads in layer. (b) Head-level guidance: each head is perturbed individually. Red boxes indicate high-performing heads in terms of PickScore [35]. (c) Perturbing only the high-performing heads identified in (b) yields higher-quality generations across both low-performing layers (L3, L13) and well-performing ones (L8, L12). The prompt Turkish girl with lantern, dark room is used. Figure 3: Effect of head-level guidance on concept amplification and combination. (a) Guiding with individual heads amplifies specific visual concepts such as darkness, geometry, shadow, or color. (b) Guiding with two heads simultaneously combines their effects in the output. all heads in layer fails to account for this intra-layer diversity, potentially disrupting the output from the guidance and leading to unintended side effects. As shown in Fig. 2 (a), some heads from supposedly poor layers still produce high-quality results. This suggests that overall quality may degrade due to the effects of few heads dominating the outcome. To test this, we filtered and perturbed only high-scoring heads based on preference scores. The resulting samples (Fig. 2 (c)) show clear improvements over standard layer-level guidance. Moreover, even well-performing layers selected through manual layer-level search can include suboptimal heads, and removing them further improves quality (L8). These results demonstrate the inefficiency of coarse layer-level guidance. Based on these results, we suggest analyzing head-level guidance and its properties. 6 Algorithm 1 HeadHunter: Iterative Objective-Aware Head Selection Require: Diffusion model M, = {(p1, s1), . . . , (pM , sM )}, attention head set = {(l1, h1), . . . , (lL, hHL )}, number of heads per iteration k, number of iterations function O, prompt-seed pairs objective Initialize selected head set Initialize remaining head pool Initialize temporary score list Define perturbation set Compute average objective score j=1 using with perturbation guidance on Starget and (pj, sj) Ensure: Selected heads Sfinal 1: Sfinal 2: 3: for = 1 to do [ ] 4: for each (l, h) do 5: # Generation 6: Starget Sfinal {(l, h)} 7: Generate {ˆxj}M 8: # Evaluation 9: s(l,h) 1 10: Append ((l, h), s(l,h)) to 11: 12: 13: 14: 15: 16: 17: 18: end for 19: return Sfinal end for # Expansion Sort by s(l,h) in descending order Snew top-k heads from Sfinal Sfinal Snew Snew j=1 O(ˆxj, pj) (cid:80)M Individual head-level guidance occasionally reveals interpretable concepts. As can be seen in Fig. 1, we observe that perturbing individual heads for guidance leads to highly diverse behaviors in terms of their influence on generation. Beyond quality, we find that certain heads amplify specific visual attributes when used for guidance. These include texture, color tone, geometry, and lighting. We provide some examples in Fig. 3. (L11,H15) consistently induces shearing, while other heads amplify darkness or blue tone. Additional interpretable heads and their effects are shown in Appendix E.2. We further analyze these effects in depth in the discussion section, where we examine how individual heads contribute to image generation, how their roles differ across layers, and how they interact when composed together. Concepts can be composed via head combinations. Interestingly, combining multiple heads for head-level guidance results in the composition of their associated visual concepts. As shown in Fig. 3 (b), the generated images exhibit hybrid effectssuch as the combination of lighting (L1, H10) and shearing distortions oriented from top-left to bottom-right (L11, H15)suggesting that head-level guidance can serve as mechanism for concept-level control. More examples of concept composition are provided in Appendix E.3. Composing heads increases image quality but may lead to over-saturation and oversimplification. Perturbing more heads strengthens the guidance effect but can also introduce undesirable artifacts such as oversaturation or oversimplification. As shown in Fig. 4 (a), adding more perturbed heads results in over-perturbation, leading to over-smoothed or overly simplified output. Fig. 4 (b) further illustrates this effect quantitatively: the quality metric (FID [35]) begins to degenerate as more heads are added after some threshold. This suggests that, in addition to the choice of perturbation method, the number of perturbed heads itself is key factor for controlling the overall perturbation strength."
        },
        {
            "title": "4 Controlling Head-Level Attention Perturbation",
            "content": "4.1 HeadHunter: An Iterative Framework for Retrieving Effective Attention Heads The analysis in Sec. 3.1 revealed that individual attention heads exert distinct and often complementary influences via head-level guidancesome modulate lighting, others control color or geometry. 7 Figure 4: Results of HeadHunter for general image quality improvement. Performance improves as more top-ranked heads are added, demonstrating the effectiveness of compositional head selection via HeadHunter. The dotted horizontal line in (b) indicates the best score achieved by layer-level guidance, which is surpassed by compact set of top-k heads for < 10. Dashed lines indicate the FID of the best-performing layer-level perturbation for each w. Moreover, these effects can be composed, meaning that combinations of heads can yield richer and more controllable outputs than any single head alone. This raises key question: Can we guide image generation toward user-defined objective by selectively perturbing attention heads that are automatically identified based on their individual and compositional effects? To address this, we introduce HeadHunter, framework that optimizes an arbitrary objective function by iteratively selecting subset of attention heads to perturb. Each iteration consists of three stages: generation, evaluation, and expansion. In the generation stage, the framework perturbs attention maps for each candidate head (l, h) and generates samples using attention perturbation guidance with multiple promptseed pairs = {(p1, s1), . . . , (pM , sM )}. During the evaluation stage, it computes the average objective score using the user-given objective function over the generated samples for each candidate. In the expansion stage, the top-k performing heads are added to the selected head set Sfinal. This process is repeated for fixed number of iterations . The full procedure is described in Algorithm 4. We validate HeadHunter both quantitatively and qualitatively across tasks such as general quality and style enhancement. This iterative design greedily selects attention heads in the order that most increases the objective. As result, it achieves rapid improvements during the early iterations  (Fig. 6)  . In later iterations, it enables the selection of heads that strengthen specific stylessuch as imposing particular tone on the image  (Fig. 8)  even if they do not improve overall quality. This iterative framework allows for the discovery of heads that might be overlooked in single-step selection. We present detailed experimental results and analysis in later sections. 4.1.1 Improving general image quality In Sec. 3.1, we observe that even layers considered ineffective under layer-level guidance can improve quality when selectively activating appropriate heads. This suggests that fine-grained head-level selection across the entire set of heads may yield stronger enhancements in image quality. To explore this, we apply HeadHunter to search the head selection space. Human preference alignment perspective. To improve image quality, various metrics can be used. particularly interesting and increasingly popular approach leverages reward models trained on human preferences [59, 35, 73, 77]. Many works utilize differentiable reward models such as PickScore [35], HPS [72], and ImageReward [73] as supervision signals for either directly fine-tuning the model [12, 52] or adjusting the noise input to maximize the reward of the generated images [20]. However, methods that rely on the reward models gradients are vulnerable to reward hacking, where the model is optimized to deceive the reward model rather than genuinely improving image quality. Further, fine-tuning approaches [3, 22] are prone to catastrophic forgetting. On the other hand, gradient-free methods generally rely on sample-level guidance during the denoising process [75], which requires generating and evaluating many candidate samples at each timestep. In contrast, our method identifies set of guidance hyperparameters (attention heads) that can be reused across prompts after single search. Although we do not directly optimize for reward scores, our approach 8 Figure 5: Qualitative results of HeadHunter for style-oriented quality improvement. The first row presents the unguided result. In each subsequent row, we apply head-level guidance using additional attention heads that are incrementally selected by the HeadHunter framework. As more heads are accumulated over iterations, the generated images exhibit progressively stronger alignment with the target style while improving visual quality. Original figures and additional results conducted in FLUX.1-Dev [37] can be found in Appendix D.2. offers lightweight and generalizable mechanism for aligning generation with human preferences without relying on gradients or fine-tuning. Experiments. We demonstrate our method using PickScore [35] as the objective O. HeadHunter is applied with 20 prompt-seed pairs Q, one iteration (T = 1), and selection of = 24 heads per iteration. Additional implementation details are provided in Appendix D. Qualitative results in Fig. 4 (a) show that image quality progressively improves as more HeadHunter-selected heads are included in the guidance process, supporting the compositional nature of head-level perturbations observed earlier. Fig. 4 (b) presents quantitative results in terms of FID, evaluated on 1K prompts from the MS COCO [41] validation set. The performance improves as top-ranked heads are incrementally added, except in the high guidance scale case (w = 6.0), where oversaturation may occur. Remarkably, using only 25% of the heads (k = 6 out of 24) achieves comparable to or even better performance than full layer-level perturbation (shown as dotted lines for each guidance scale). These results suggest that HeadHunter can identify compact yet effective subset of attention heads, outperforming heuristic layer-based selection strategies. Thanks to the compositionality of head-level perturbations, even single iteration (T = 1) is sufficient, as the contributions of individual heads can be combined to further improve image quality. 9 4.1.2 Improving style-oriented quality While Section 4.1.1 demonstrated HeadHunters effectiveness in improving overall fidelity of samples, many real-world applications require more targeted control over specific visual styles, such as evoking particular mood, or mimicking classical art techniques. Inspired by the findings of Sec. 3.1 and Fig. 3, which reveal that certain attention heads are responsible for distinct stylistic or geometric attributes, we investigate whether HeadHunter can selectively enhance target style through head-level guidancewhile preserving or even enhancing overall image quality. We refer to this approach as style-oriented quality improvement. Experiments. To evaluate HeadHunters ability to enhance specific styles, we prepare two prompt sets: one for style (e.g., warm golden hour glow) and one for content (e.g., portrait of violinist). Each trial uses composite prompt of the form style, content. We run HeadHunter with = 5 and = 3. Fig. 5 shows that, as more heads are selected, the generated images increasingly exhibit the intended style while maintaining structural integrity. 6 reports Fig. quantitative metrics: PickScore [35] and the LAION Aesthetic Score (AES) [59] both improve steadily as increases in the prompt-seed pairs Q. These results confirm that HeadHunter effectively amplifies target styles in alignment with human preferences. For additional qualitative examples with SD3 [19] and FLUX.1-Dev [37], we refer readers to Figs. 21 to 26 in Appendix. Figure 6: Quantitative results of HeadHunter for style-oriented quality improvement. As more heads accumulate, the generated images progressively align better with the target style and exhibit improved visual quality. Assessing generalizability to unseen content prompts. To evaluate the generalizability of HeadHunter, we compare it against both the baseline and CFG using set of 50 unseen content prompts. As shown in Tab. 1, HeadHunter achieves significantly higher human preference scores than the baseline and performs comparably to CFG, thereby validating its ability to generalize to novel content prompts. Notably, as illustrated in Fig. 7, HeadHunter leads to substantial enhancement of stylistic attributes even compared to CFG. For the style prompt sunlit, warm glow, and golden hour, HeadHunter produces visibly intensified reddish tones and sunlight effects. Likewise, for the line art drawing ... style prompt, monotone line characteristics are markedly enhanced. These results suggest that HeadHunter can serve as an effective plug-and-play module within existing inference pipelines (including those employing CFG) to improve both stylistic fidelity and overall image quality without requiring additional training. 4.1.3 Discussion Surprising utility of individually weak heads. Interestingly, some heads that individually produce poor outputs can still play an essential role in composition. In Fig. 8 (a), we observe sudden enhancement in stylistic expression (e.g., intensified red or pink hues) with inclusion of certain head set. By individually performing head-level guidance with each head in the set, we found that some head alone yields low-quality outputs but it reliably enforces style trait. Such heads are unlikely to be selected through one-shot evaluation due to low quality, but HeadHunters iterative nature allows them to be integrated later as their utility emerges in composition. This highlights HeadHunters capacity to exploit compositional synergies among heads. Distribution of selected heads across layers. We investigates the layer distribution of selected attention heads within models architecture, specifically examining whether impactful heads are localized to particular layers and if their distribution varies by objective. For the general quality setting, we select the top-15 heads per prompt and aggregate them, excluding duplicates (Fig. 9 (a)). For style-oriented quality setting, we repeat the process per style (Fig. 9 (b)). Our findings reveal that effective heads are not concentrated in any single layer. Even the most frequently selected layer accounts for less than 12% of all chosen heads across both objectives (Fig. 9 10 Table 1: Quantitative evaluation of generalizability to unseen content prompts. Number in the parenthesis denotes guidance scale w. Applying HeadHunter (style-oriented quality setting) to unseen content prompts demonstrates strong generalization, yielding significantly higher human preference scores than the baseline and performance comparable to CFG. Method Baseline CFG (3.0) CFG (6.0) HeadHunter (3.0) CFG (3.0) + HeadHunter (3.0) PickScore AES HPS 0.2147 19.66 5. Imreward -0.591 20.87 20.92 20.70 20.92 5.71 5.80 5.92 5.93 0.2924 0. 0.2901 0.3036 0.844 1.063 0.470 0.845 Figure 7: Qualitative evaluation of generalizability to unseen content prompts. Applying HeadHunter (style-oriented quality setting) results in substantial enhancement of stylistic attributes, outperforming not only the baseline but also CFG. 11 Figure 8: Role of individually weak heads. For sudden stylistic transition moments (e.g., adding global warmth or pink hues as shown in (a)), we visualize the effect of newly added heads in (b). One of these heads generate blurry reddish or pinkish outputs when used alone (see red box). Although they fail to produce meaningful content when used independently, they contribute effectively when composed with previously selected structural heads. This highlights the importance of iterative strategy since those heads are unlikely to be selected by one-shot evaluation due to low quality. (a), (b)). This dispersed distribution suggests that layer-wise guidance approach, which uniformly perturbs all heads within layer, may be suboptimal. Such an approach risks overlooking critical heads in other layers while simultaneously applying modifications to irrelevant heads. Furthermore, we observed distinct distribution patterns dependent on the objective. Heads identified for improving general image quality predominantly reside in early to mid-layers, with mere 2.9% located in layers 16. In contrast, heads contributing to style-oriented quality improvement exhibit significantly broader distribution, with 24.8% found in deeper layers (16). We further examine the evolution of head distribution during HeadHunters iterative refinement for style-oriented quality setting in Fig. 9 (c)). Initially, selected heads are concentrated in midlayers, mirroring the pattern observed for general quality setting. However, as iterations progress, the distribution become notably flatter, indicating broader engagement of heads across layers for stylistic refinement. This quantitative shift aligns with qualitative observations.As shown in Fig. 21 (spring bloom, fresh vibrant colors), Fig. 22 (cinematic lighting, dramatic tone), and Fig. 23 (line art drawing ...), the second row (Iteration 1) shows improved image quality but little stylistic traits. However from Iteration 2 onward, the style becomes much more pronounced.. In conclusion, our research indicates that (1) effective heads are not localized to specific layers, thereby challenging the efficacy of layer-level guidance, and (2) head distributions are objectivedependent, underscoring the necessity of objective-specific selection. These findings collectively emphasize the critical role of targeted head-retrieval and guidance frameworks, such as HeadHunter, in fully harnessing the diverse functionalities of attention mechanisms. 12 Figure 9: Layer distribution of heads selected by HeadHunter for different objectives. Effective heads are not confined to specific layers and the distribution is different with each objective, highlighting the limitations of layer-level guidance and the necessity of objective-specific head selection. (a) In general image quality setting, selections concentrate in earlymid layers (2.9% in layers 16). (b) In style-oriented quality setting, selections spread broadly, with 24.8% in layers 16. (c) Iteration-wise distributions of (b) begin with an early-layer bias similar to (a), but gradually flatten over iterations, mirroring (b). This trend aligns with qualitative observations. Iteration 1 primarily improves general quality, whereas stylization becomes more prominent in subsequent iterations. Does the style enhancement occur even without style prompts? In our style-oriented quality improvement experiments, we search for heads that consistently enhance given style by using composite prompts of the form style, content. Then natural question arises: Do these heads still express their stylistic traits when the style is not explicitly specified in the prompt or even under unconditional generation? To explore this, we apply the HeadHunter-selected heads in an unconditional setting (i.e., with null prompt, ). As shown in Fig. 10 (a), we find that even with null prompt, using these heads can still induce visual traits that align with the original style. This suggests that certain heads may encode intrinsic stylistic properties that influence generation regardless of prompt or initial noise. Yet, this effect is not guaranteed across all styles. As can be seen in Fig. 10 (b), in some cases, while style traits are strongly expressed with style prompts, they become weak or ambiguous when only content prompts are used or under unconditional generation. This may be due to polysemanticity in head behavior. This emerging style consistency when using style prompts indirectly supports the validity of including style prompts during HeadHunters search process in style-oriented quality setting. 4.2 Controlling Guidance Intensity via Attention Map Interpolation Although our head-level guidance provides fine-grained, objective-specific control, we propose an orthogonal approach that enables continuous modulation of the guidance effect. This allows for more precise adjustment of the perturbation intensity applied to selected heads or layers. Specifically, we introduce simple interpolation strategy that interpolates the original attention map with its perturbed counterpart. For example, the interpolated attention map for perturbed-attention guidance (PAG) [1] is defined as: A(SoftPAG) l,h = (1 u)Ai + uI, for (l, h) S, [0, 1]. (11) This formulation replaces A(PAG) between the original attention map and the fully perturbed one. l,h in Eq. 10 with its interpolated version, enabling smooth transition 13 Figure 10: Investigation whether style enhancement occurs without style prompt when applying HeadHunter (style-oriented quality setting). indicates prompt and seed pairs. (a) Applying HeadHunter in even an unconditional setting amplifies the corresponding style in some cases. Style 1: art nouveau style, elegant, decorative, curvilinear forms, nature-inspired, ornate, detailed, Style 2: line art drawing, professional sleek modern minimalist graphic vector graphics\", Style 3: cubist abstraction, fragmented planes, bold geometric angles, Style 4: cinematic lighting, dramatic tone. (b) In the other cases, style enhancement depends on the presence of style prompts. For these heads, the stylization effect weakens without explicit style descriptions, highlighting the importance of including style prompts during the head selection process. Figure 11: Generated Images with linear interpolation between attention map and an identity matrix I. While increasing enhances quality up to point, it eventually results in over-saturation and structural over-simplification. Detailed hyperparameters can be found in Appendix F. We present qualitative results across varying values in Fig. 11. These results show that increasing the interpolation parameter initially enhances sample quality by rectifying structural artifacts in the samples. However, beyond certain point, it leads to oversimplified images with smoothed backgrounds and exaggerated structures. The interpolation method provides sweet spot where the samples exhibit improved structure while retaining visually realistic features. By treating the identity-matrix perturbation in PAG [1] as specific point in probability distribution space, our formulation enables smooth and principled interpolation between and I. The scalar [0, 1] provides an intuitive and interpretable control over perturbation strength. Additionally, since this Figure 12: Incorporating SoftPAG with the final heads retrieved by HeadHunter for styleoriented quality improvement. Setting the interpolation parameter = 1.0 makes the effect of head-level guidance more visible, which helps in selecting effective heads via HeadHunter. However, it may also introduce unwanted artifacts such as oversaturation or oversimplification. Post-tuning after head selection effectively reduces these artifacts while preserving the style enhancement, with minimal additional cost. Figure 13: Grid search for SoftPAG, on guidance scale and interpolation parameter with different metrics. method is simple linear interpolation of self-attention maps3, it can be implemented with few lines of code. We refer to this controllable variant of PAG as Soft Perturbed-Attention Guidance (SoftPAG). Interpolation parameter vs. guidance scale w. Both the guidance scale and interpolation parameter influence the strength of perturbation, often affecting saturation and structural fidelity. This raises the question of whether adjusting offers any advantage over simply tuning w. Fig. 13 visualizes metric values over grid of (w, u) pairs, where brighter regions indicate better performance and red boxes denote optimal configurations. Across most metrics, the best-performing setting occurs at < 1.0, indicating that full replacement (i.e., = 1 as in PAG) is often suboptimal. These results underscore the importance of explicitly controlling perturbation strength via interpolation, rather than relying solely on the guidance scale. Pre-selecting interpolation parameter in HeadHunter search. As running HeadHunter multiple times with different interpolation parameters is computationally expensive, it is generally sufficient 3Although more complex paths between probability distributions (e.g., Fisher-Rao or optimal transport geodesics) exist, we find that linear interpolationalso known as mixture-coordinate geodesicis highly effective in practice. 15 Figure 14: Effect of interpolation parameter on FID across PickScore top-k head selections. As decreases, the perturbation becomes milder, shifting the optimal point (lowest FID) toward larger (i.e., more heads are needed). Notably, = 0.5 yields the best FID (indicated by the red dashed line), highlighting the benefit of moderate perturbation strength. to search for heads using the original PAG. Nonetheless, we can explore the effect of through single-iteration search to study general quality trends. To this end, we run HeadHunter with the same setup as in Sec. 4.1.1, replacing PAG with SoftPAG (Eq. 11) and varying u. The results, shown in Fig. 14, report FID [25] scores as function of for different values. We observe that lower values (i.e., softer perturbations) require more heads to achieve optimal performance, but also tend to yield better overall results with moderate setting. Empirically, we find that = 0.5 and = 9 offer strong trade-off. This supports the view that both and modulate the strength of guidance. While this paper introduces two axes of fine-grained controllabilityhead-level selection and attention map interpolationtuning is significantly more efficient than rerunning head selection. In practice, post-selecting after standard head search is typically sufficient. However, if more compute is available, one may rerun HeadHunter with moderate (e.g., = 0.5) for refined selection. Post-selecting of for artifact removal with retrieved heads. As discussed in Sec. 3.1, the number of heads used in head-level guidance affects how strong the perturbation is. When more heads are used, the intended effect such as style enhancement can be amplified. However, this may also lead to unwanted side effects including oversaturation or oversimplification due to bias of reward models [35, 59]. In the HeadHunter framework, heads are added step by step. This helps reinforce the target objective, but may also increase the risk of artifacts. To better control the strength of guidance, we can use SoftPAG, which allows us to mitigate the artifacts through the interpolation parameter u. As tuning both the number of heads and the interpolation parameter at the same time can be complex and computationally expensive, we adopt simple two-stage approach. First, we use HeadHunter with = 1.0 to select effective heads. larger helps make each heads effect more visible, which is helpful during selection. Then, in the second stage, we optionally reduce to fine-tune the strength of the guidance and reduce any unwanted artifacts. This approach offers practical trade-off by eliminating complex tuning while retaining control over the final output. In Fig. 12, we show examples where retrieved heads with = 1.0 introduce unnatural artifacts. Reducing the SoftPAG parameter to = 0.5 effectively removes these artifacts while preserving the style enhancement provided by the selected heads."
        },
        {
            "title": "5 Conclusion",
            "content": "In this work, we move beyond heuristic layer selection in attention perturbation guidance by identifying attention heads as more meaningful and fine-grained unit of intervention. To the best of our knowledge, this is the first work to perform attention perturbation at the level of individual heads. Specifically, we present HeadHunter, an iterative framework for selecting semantically relevant attention heads aligned with arbitrary, user-defined objectives. Empirical results on state-of-the-art DiT-based models, including Stable Diffusion 3 and FLUX.1, show that head-level perturbation 16 not only improves general image quality with single search but also aesthetically enhances specific visual styles. In addition, we introduce SoftPAG, lightweight yet powerful mechanism for continuously modulating perturbation strength via attention map interpolation. Together, these two approaches address key limitations of prior guidance methods, enabling more targeted, effective, and controllable inference-time interventions. We believe this work opens promising directions for interpretable and modular control in generative modeling."
        },
        {
            "title": "References",
            "content": "[1] Donghoon Ahn, Hyoungwon Cho, Jaewon Min, Wooseok Jang, Jungwoo Kim, SeonHwa Kim, Hyun Hee Park, Kyong Hwan Jin, and Seungryong Kim. Self-rectifying diffusion sampling with perturbed-attention guidance. In European Conference on Computer Vision, pages 117. Springer, 2024. [2] Omri Avrahami, Or Patashnik, Ohad Fried, Egor Nemchinov, Kfir Aberman, Dani Lischinski, and Daniel Cohen-Or. Stable flow: Vital layers for training-free image editing. arXiv preprint arXiv:2411.14430, 2024. [3] Kevin Black, Michael Jänner, Yilun Du, Ilya Kostrikov, and Sergey Levine. Training diffusion models with reinforcement learning. arXiv (Cornell University), 2023. [4] A. Blattmann, Tim Dockhorn, Sumith Kulal, Daniel Mendelevitch, Maciej Kilian, and Dominik Lorenz. Stable video diffusion: Scaling latent video diffusion models to large datasets. arXiv preprint arXiv:2311.15127, abs/2311.15127, 2023. [5] Mingdeng Cao, Xintao Wang, Zhongang Qi, Ying Shan, Xiaohu Qie, and Yinqiang Zheng. Masactrl: Tuning-free mutual self-attention control for consistent image synthesis and editing. In Proceedings of the IEEE/CVF international conference on computer vision, pages 2256022570, 2023. [6] Junsong Chen, Chongjian Ge, Enze Xie, Yue Wu, Lewei Yao, Xiaozhe Ren, Zhongdao Wang, Ping Luo, Huchuan Lu, and Zhenguo Li. Pixart-σ: Weak-to-strong training of diffusion transformer for 4k text-to-image generation. In European Conference on Computer Vision, pages 7491. Springer, 2024. [7] Junsong Chen, Yue Wu, Simian Luo, Enze Xie, Sayak Paul, Ping Luo, Hang Zhao, and Zhenguo Li. Pixart-δ: Fast and controllable image generation with latent consistency models. arXiv preprint arXiv:2401.05252, 2024. [8] Junsong Chen, Jincheng Yu, Chongjian Ge, Lewei Yao, Enze Xie, Yue Wu, Zhongdao Wang, James Kwok, Ping Luo, Huchuan Lu, et al. Pixart-alpha: Fast training of diffusion transformer for photorealistic text-to-image synthesis. arXiv preprint arXiv:2310.00426, 2023. [9] Mark Chen, Alec Radford, Jeff Wu, Heewoo Jun, Prafulla Dhariwal, D. Luan, and I. Sutskever. Generative pretraining from pixels. In International Conference on Machine Learning, 2020. [10] Hyungjin Chung, Jeongsol Kim, Michael T. McCann, M. Klasky, and J. C. Ye. Diffusion posterior sampling for general noisy inverse problems. ArXiv, abs/2209.14687, 2022. [11] Hyungjin Chung, Byeongsu Sim, Dohoon Ryu, and J. C. Ye. Improving diffusion models for inverse problems using manifold constraints. Neural Information Processing Systems, abs/2206.00941, 2022. [12] Kevin Clark, Paul Vicol, Kevin Swersky, and David J. Fleet. Directly fine-tuning diffusion models on differentiable rewards. arXiv (Cornell University), 2023. [13] Mete Demircigil, Judith Heusel, Matthias Löwe, Sven Upgang, and Franck Vermet. On model of associative memory with huge storage capacity. Journal of Statistical Physics, 168:288299, 2017. [14] Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. Bert: Pre-training of deep bidirectional transformers for language understanding. In North American Chapter of the Association for Computational Linguistics, 2019. [15] Prafulla Dhariwal and Alexander Nichol. Diffusion models beat gans on image synthesis. Advances in Neural information processing systems, 34:87808794, 2021. [16] Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, G. Heigold, S. Gelly, Jakob Uszkoreit, and N. Houlsby. An image is worth 16x16 words: Transformers for image recognition at scale. In International Conference on Learning Representations, 2020. [17] Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, et al. An image is worth 16x16 words: Transformers for image recognition at scale. arXiv preprint arXiv:2010.11929, 2020. [18] Nelson Elhage, Neel Nanda, Catherine Olsson, Tom Henighan, Nicholas Joseph, Ben Mann, Amanda Askell, Yuntao Bai, Anna Chen, Tom Conerly, et al. mathematical framework for transformer circuits. Transformer Circuits Thread, 1(1):12, 2021. [19] Patrick Esser, Sumith Kulal, Andreas Blattmann, Rahim Entezari, Jonas Müller, Harry Saini, Yam Levi, Dominik Lorenz, Axel Sauer, Frederic Boesel, et al. Scaling rectified flow transformers for high-resolution image synthesis. In Forty-first international conference on machine learning, 2024. [20] Luca Eyring, Shyamgopal Karthik, Karsten Roth, Alexey Dosovitskiy, and Zeynep Akata. Reno: Enhancing one-step text-to-image models through reward-based noise optimization. Advances in Neural Information Processing Systems, 37:125487125519, 2024. [21] fal. Auraflow: flow-based text-to-image generation model. https://huggingface.co/ fal/AuraFlow, 2024. Accessed: 2025-05-14. [22] Ying Fan, Olivia Watkins, Yuqing Du, Hao Liu, M. Ryu, Craig Boutilier, P. Abbeel, M. Ghavamzadeh, Kangwook Lee, and Kimin Lee. Dpok: Reinforcement learning for finetuning text-to-image diffusion models. In Neural Information Processing Systems, 2023. [23] Yossi Gandelsman, Alexei Efros, and Jacob Steinhardt. Interpreting clips image representation via text-based decomposition. arXiv preprint arXiv:2310.05916, 2023. [24] Amir Hertz, Ron Mokady, J. Tenenbaum, Kfir Aberman, Y. Pritch, and D. Cohen-Or. Promptto-prompt image editing with cross attention control. ArXiv, abs/2208.01626, 2022. [25] Martin Heusel, Hubert Ramsauer, Thomas Unterthiner, Bernhard Nessler, and Sepp Hochreiter. Gans trained by two time-scale update rule converge to local nash equilibrium. arXiv (Cornell University), 2017. [26] Jonathan Ho, William Chan, Chitwan Saharia, Jay Whang, Ruiqi Gao, A. Gritsenko, Diederik P. Kingma, Ben Poole, Mohammad Norouzi, David J. Fleet, and Tim Salimans. Imagen video: High definition video generation with diffusion models. arXiv preprint arXiv:2210.02303, abs/2210.02303, 2022. [27] Jonathan Ho, Ajay Jain, and Pieter Abbeel. Denoising diffusion probabilistic models. NeurIPS, 2020. [28] Jonathan Ho and Tim Salimans. Classifier-free diffusion guidance. arXiv preprint arXiv:2207.12598, 2022. [29] Jonathan Ho, Tim Salimans, Alexey Gritsenko, William Chan, Mohammad Norouzi, and David J. Fleet. Video diffusion models. arXiv preprint arXiv:2204.03458, 2022. [30] Susung Hong. Smoothed energy guidance: Guiding diffusion models with reduced energy curvature of attention. Advances in Neural Information Processing Systems, 2024. [31] Susung Hong, Gyuseong Lee, Wooseok Jang, and Seungryong Kim. Improving sample quality of diffusion models using self-attention guidance. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 74627471, 2023. 18 [32] Junha Hyung, Kinam Kim, Susung Hong, Min-Jung Kim, and Jaegul Choo. Spatiotemporal skip guidance for enhanced video diffusion sampling. In Proceedings of the Computer Vision and Pattern Recognition Conference, pages 1100611015, 2025. [33] Zahra Kadkhodaie and Eero P. Simoncelli. Stochastic solutions for linear inverse problems using the prior implicit in denoiser. Neural Information Processing Systems, 2021. [34] Tero Karras, Miika Aittala, Tuomas Kynkäänniemi, Jaakko Lehtinen, Timo Aila, and Samuli Laine. Guiding diffusion model with bad version of itself. Advances in Neural Information Processing Systems, 2024. [35] Yuval Kirstain, Adam Polyak, Uriel Singer, Shahbuland Matiana, Joe Penna, and Omer Levy. Pick-a-pic: An open dataset of user preferences for text-to-image generation. Advances in Neural Information Processing Systems, 36:3665236663, 2023. [36] T. Kynkäänniemi, M. Aittala, Tero Karras, S. Laine, Timo Aila, and J. Lehtinen. Applying guidance in limited interval improves sample and distribution quality in diffusion models. Advances in Neural Information Processing Systems, abs/2404.07724, 2024. [37] Black Forest Labs. Flux.1 [dev]: 12b parameter rectified flow transformer for text-toimage generation. https://huggingface.co/black-forest-labs/FLUX.1-dev, 2024. Accessed: 2025-05-14. [38] Shufan Li, Konstantinos Kallidromitis, Akash Gokul, Arsh Koneru, Yusuke Kato, Kazuki Kozuka, and Aditya Grover. Reflect-dit: Inference-time scaling for text-to-image diffusion transformers via in-context reflection. arXiv preprint arXiv:2503.12271, 2025. [39] Tiancheng Li, Weijian Luo, Zhiyang Chen, Liyuan Ma, and Guo-Jun Qi. Self-guidance: Boosting flow and diffusion generation on their own. arXiv preprint arXiv:2412.05827, 2024. [40] Chen-Hsuan Lin, Jun Gao, Luming Tang, Towaki Takikawa, Xiaohui Zeng, Xun Huang, Karsten Kreis, Sanja Fidler, Ming-Yu Liu, and Tsung-Yi Lin. Magic3d: High-resolution text-to-3d content creation. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 300309, 2023. [41] Tsung-Yi Lin, Michael Maire, Serge Belongie, James Hays, Pietro Perona, Deva Ramanan, Piotr Dollár, and Lawrence Zitnick. Microsoft coco: Common objects in context. In Computer visionECCV 2014: 13th European conference, zurich, Switzerland, September 6-12, 2014, proceedings, part 13, pages 740755. Springer, 2014. [42] Y. Lipman, Ricky T. Q. Chen, Heli Ben-Hamu, Maximilian Nickel, and Matt Le. Flow matching for generative modeling. International Conference on Learning Representations, abs/2210.02747, 2022. [43] Yaron Lipman, Ricky TQ Chen, Heli Ben-Hamu, Maximilian Nickel, and Matt Le. Flow matching for generative modeling. arXiv preprint arXiv:2210.02747, 2022. [44] Xingchao Liu, Chengyue Gong, and Qiang Liu. Flow straight and fast: Learning to generate and transfer data with rectified flow. arXiv preprint arXiv:2209.03003, 2022. [45] Nanye Ma, Larry B. Goldstein, Michael S. Albergo, Nicholas M. Boffi, Eric Vanden-Eijnden, and Saining Xie. Sit: Exploring flow and diffusion-based generative models with scalable interpolant transformers. European Conference on Computer Vision, 2024. [46] Nanye Ma, Shangyuan Tong, Haolin Jia, Hexiang Hu, Yu-Chuan Su, Mingda Zhang, Xuan Yang, Yandong Li, Tommi Jaakkola, Xuhui Jia, et al. Inference-time scaling for diffusion models beyond scaling denoising steps. arXiv preprint arXiv:2501.09732, 2025. [47] Jisu Nam, Heesu Kim, DongJae Lee, Siyoon Jin, Seungryong Kim, and Seunggyu Chang. Dreammatcher: appearance matching self-attention for semantically-consistent text-to-image personalization. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 81008110, 2024. 19 [48] Jungwon Park, Jungmin Ko, Dongnam Byun, Jangwon Suh, and Wonjong Rhee. Cross-attention head position patterns can align with human visual concepts in text-to-image generative models. arXiv preprint arXiv:2412.02237, 2024. [49] William Peebles and Saining Xie. Scalable diffusion models with transformers. In Proceedings of the IEEE/CVF international conference on computer vision, pages 41954205, 2023. [50] Dustin Podell, Zion English, Kyle Lacey, Andreas Blattmann, Tim Dockhorn, Jonas Müller, Joe Penna, and Robin Rombach. Sdxl: Improving latent diffusion models for high-resolution image synthesis. arXiv (Cornell University), 2023. [51] Ben Poole, Ajay Jain, Jonathan Barron, and Ben Mildenhall. Dreamfusion: Text-to-3d using 2d diffusion. arXiv preprint arXiv:2209.14988, 2022. [52] Mihir Prabhudesai, Anirudh Goyal, Deepak Pathak, and Katerina Fragkiadaki. Aligning text-toimage diffusion models with reward backpropagation. arXiv (Cornell University), 2023. [53] Alec Radford and Karthik Narasimhan."
        },
        {
            "title": "Improving language understanding by generative",
            "content": "pre-training. 2018. [54] Hubert Ramsauer, Bernhard Schäfl, Johannes Lehner, Philipp Seidl, Michael Widrich, Thomas Adler, Lukas Gruber, Markus Holzleitner, Milena Pavlovic, Geir Kjetil Sandve, et al. Hopfield networks is all you need. arXiv preprint arXiv:2008.02217, 2020. [55] Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Björn Ommer. Highresolution image synthesis with latent diffusion models. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 1068410695, 2022. [56] Olaf Ronneberger, Philipp Fischer, and Thomas Brox. U-net: Convolutional networks In Medical image computing and computer-assisted for biomedical image segmentation. interventionMICCAI 2015: 18th international conference, Munich, Germany, October 5-9, 2015, proceedings, part III 18, pages 234241. Springer, 2015. [57] Seyedmorteza Sadat, Jakob Buhmann, Derek Bradely, Otmar Hilliges, and Romann M. Weber. Cads: Unleashing the diversity of diffusion models through condition-annealed sampling. International Conference on Learning Representations, 2024. [58] Chitwan Saharia, William Chan, Saurabh Saxena, Lala Li, Jay Whang, Emily L. Denton, Seyed Kamyar Seyed Ghasemipour, Burcu Karagol Ayan, S. S. Mahdavi, Raphael Gontijo Lopes, Tim Salimans, Jonathan Ho, David J. Fleet, and Mohammad Norouzi. Photorealistic text-toimage diffusion models with deep language understanding. Advances in Neural Information Processing Systems, abs/2205.11487, 2022. [59] Christoph Schuhmann, Romain Beaumont, Richard Vencu, Cade Gordon, Ross Wightman, Mehdi Cherti, Theo Coombes, Aarush Katta, Clayton Mullis, Mitchell Wortsman, et al. Laion5b: An open large-scale dataset for training next generation image-text models. Advances in neural information processing systems, 35:2527825294, 2022. [60] Yichun Shi, Peng Wang, Jianglong Ye, Mai Long, Kejie Li, and Xiao Yang. Mvdream: Multiview diffusion for 3d generation. arXiv preprint arXiv:2308.16512, 2023. [61] Uriel Singer, Shelly Sheynin, Adam Polyak, Oron Ashual, Iurii Makarov, Filippos Kokkinos, Naman Goyal, A. Vedaldi, Devi Parikh, Justin Johnson, and Yaniv Taigman. Text-to-4d dynamic scene generation. In International Conference on Machine Learning, 2023. [62] Jascha Sohl-Dickstein, Eric Weiss, Niru Maheswaranathan, and Surya Ganguli. Deep unsupervised learning using nonequilibrium thermodynamics. In International conference on machine learning, pages 22562265. pmlr, 2015. [63] Jascha Sohl-Dickstein, Eric A. Weiss, Niru Maheswaranathan, and Surya Ganguli. Deep unsupervised learning using nonequilibrium thermodynamics. arXiv (Cornell University), 2015. [64] Jiaming Song, Chenlin Meng, and Stefano Ermon. Denoising diffusion implicit models. ArXiv, abs/2010.02502, 2020. 20 [65] Jiaming Song, Arash Vahdat, M. Mardani, and J. Kautz. Pseudoinverse-guided diffusion models for inverse problems. International Conference on Learning Representations, 2023. [66] Yang Song and Stefano Ermon. Generative modeling by estimating gradients of the data distribution. Neural Information Processing Systems, pages 1189511907, 2019. [67] Yang Song, Jascha Sohl-Dickstein, Diederik Kingma, Abhishek Kumar, Stefano Ermon, and Ben Poole. Score-based generative modeling through stochastic differential equations. arXiv preprint arXiv:2011.13456, 2020. [68] Yang Song, Jascha Sohl-Dickstein, Diederik P. Kingma, Abhishek Kumar, Stefano Ermon, and Ben Poole. Score-based generative modeling through stochastic differential equations. arXiv (Cornell University), 2020. [69] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan Gomez, Łukasz Kaiser, and Illia Polosukhin. Attention is all you need. Advances in neural information processing systems, 30, 2017. [70] Zhengyi Wang, Cheng Lu, Yikai Wang, Fan Bao, Chongxuan Li, Hang Su, and Jun Zhu. Prolificdreamer: High-fidelity and diverse text-to-3d generation with variational score distillation. Advances in Neural Information Processing Systems, 36:84068441, 2023. [71] Rundi Wu, Ruiqi Gao, Ben Poole, Alex Trevithick, Changxi Zheng, Jonathan T. Barron, and Aleksander Holynski. Cat4d: Create anything in 4d with multi-view video diffusion models. arXiv preprint arXiv:2411.18613, 2025. [72] Xiaoshi Wu, Keqiang Sun, Feng Zhu, Rui Zhao, and Hongsheng Li. Human preference score: Better aligning text-to-image models with human preference. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 20962105, 2023. [73] Jiazheng Xu, Xiao Liu, Yuchen Wu, Yuxuan Tong, Qinkai Li, Ming Ding, Jie Tang, and Yuxiao Dong. Imagereward: Learning and evaluating human preferences for text-to-image generation. Advances in Neural Information Processing Systems, 36:1590315935, 2023. [74] Zhuoyi Yang, Jiayan Teng, Wendi Zheng, Ming Ding, Shiyu Huang, Jiazheng Xu, Yuanming Yang, Wenyi Hong, Xiaohan Zhang, Guanyu Feng, Da Yin, Yuxuan Zhang, Weihan Wang, Yean Cheng, Bin Xu, Xiaotao Gu, Yuxiao Dong, and Jie Tang. Cogvideox: Text-to-video diffusion models with an expert transformer. International Conference on Learning Representations, 2025. [75] P. Yeh, Kuang-Huei Lee, and Jun-Cheng Chen. Training-free diffusion model alignment with sampling demons. arXiv (Cornell University), 2024. [76] Heng Yu, Chaoyang Wang, Peiye Zhuang, Willi Menapace, Aliaksandr Siarohin, Junli Cao, László A. Jeni, S. Tulyakov, and Hsin-Ying Lee. 4real: Towards photorealistic 4d scene generation via video diffusion models. Advances in Neural Information Processing Systems, abs/2406.07472, 2024. [77] Sixian Zhang, Bohan Wang, Junqiang Wu, Yan Li, Tingting Gao, Di Zhang, and Zhongyuan Wang. Learning multi-dimensional human preference for text-to-image generation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 80188027, 2024."
        },
        {
            "title": "Appendix",
            "content": "This appendix provides supplementary material to support the main paper. It includes: Related Works Additional perturbation methods and framework extensions Head-level guidance using different perturbation types (Sec. B) Unified probabilistic view of attention perturbation, including SEG and temperature scaling (Sec. C) Comparative metrics across different perturbation strategies  (Table 2)  HeadHunter: Implementation and additional analysis Implementation details (Sec. D) Additional qualitative results and style transfer with HeadHunter (Sec. D.2) Extended head analysis and compositionality All-head perturbation results for SD3 and FLUX.1-Dev (Sec. E.1) Interpretable heads and concept amplification (Sec. E.2, E.3) Experimental details and resources Prompt and seed lists, model configurations, and runtime specs (Sec. F) Hardware setup and computational cost discussion (Sec. F) Ablation and quantitative evaluations Grid search results for interpolation and guidance scale (Sec. I)"
        },
        {
            "title": "A Related Works",
            "content": "Diffusion Models Diffusion models [27, 64, 63, 66, 68, 15, 55, 50] have become the foundation of modern generative modeling, driving advances in both image [55, 50, 49, 6, 19] and video synthesis [29, 26, 4, 74]. Early methods rely on stochastic differential equations (SDEs) to learn reverse denoising process from noise to data. More recently, deterministic samplers based on ordinary differential equations (ODEs), including rectified flow [45, 19] and flow matching [42], have emerged as efficient alternatives, learning continuous trajectories that transform noise into data. These methods accelerate convergence and improve stability, especially in large-scale models. In parallel, network architectures have shifted from U-Net backbones [50] to Diffusion Transformers (DiT) [49, 6, 19, 8, 7], enhancing scalability and representational capacity. Attention Perturbation Guidance In recent studies, various modifications and extensions of classifier-free guidance have been introduced. Among them, methods that directly perturb the attention layers for guidance work effectively. Self-Attention Guidance (SAG) [31] applies Gaussian blur to the self-attention layers of the UNet to introduce perturbations. Similarly, Perturbed-Attention Guidance (PAG) [1] generates perturbations by replacing the attention maps with an identity matrix. Spatiotemporal Skip Guidance (STG) [32] is 3D extension of PAG that improves sample quality by selectively skipping spatiotemporal layers. Smoothed Energy Guidance (SEG) [30] defines the energy of self-attention and employs 2D Gaussian blur to reduce the curvature of the energy landscape, using the result as form of perturbation. Autoguidance [34] enhances image quality by using bad version of the same conditional model as source of perturbation. Self-Guidance (SG) [39] leverages the model prediction at noisy timestep to generate perturbations. Head-level perturbation guidance with different perturbation methods In the main paper, we show the head-level analysis mostly on identity-matrix replacement perturbation. In Fig. 16. We show the results with another perturbation and the analysis. Note that one can freely choose any other perturbation methods, and we show some cases as examples."
        },
        {
            "title": "C Other perturbation methods in our framework and comparison",
            "content": "In the main paper, we focus on single representative perturbation method: identity matrix replacement (PAG). Within our framework, which interprets the attention map as probability distribution, 1 identity perturbation can be seen as one end of the entropy-modulating transform that minimally shifts the distributions entropy. Fig. 15 provides conceptual overview of this unified view, with other perturbations. Our key insight is twofold: (1) attention perturbations can be understood as transformations over probability distributions, and (2) the strength of any perturbation can be smoothly controlled via interpolation with the original attention map. This enables elegant integration of existing methods while offering controllability. C.1 Uniform Guidance From an information-theoretic perspective, the identity matrix attention map is special case where each row of the attention map corresponds to Dirac delta distribution, which is the lowest possible entropy case. At the opposite extreme of the entropy spectrum lies the uniform distribution U, whose rows are all [ 1 ]; it attains the maximum entropy, allowing each query to attend equally to every key. Replacing by in selected heads gives Uniform Guidance (UG). Similar to the interpolation with the identity matrix in SoftPAG, we can define distributional perturbation between the original attention matrix and via linear interpolation: , . . . , A(SoftUG) = (1 u)A + uU, [0, 1]. (12) We show the interpolation results in Fig. 17. Interestingly, this perturbation strategy tends to preserve the original structure of objects in the unguided sample (u = 0) slightly better. However, it often over-restores artifacts present in the original structure, sometimes generating unintended elements (e.g., framed portrait in the background of Row 1, or swimming pool in Row 3). Both methods, however, may exhibit oversaturation or oversimplification when overly strong perturbations are applied, highlighting the importance of balanced interpolation with the original attention map A. Figure 15: Unified attention perturbation guidance. Our framework unifies variety of perturbation strategies by interpreting attention maps as probability distributions and interpolating between the original and perturbed variants. Figure 16: Head-Level perturbation guidance with different perturbations. We show interpretable heads with different perturbation methods. Figure 17: Generated Images with linear interpolation between attention map and uniform matrix U. While increasing enhances quality up to point, it eventually results in over-saturation and structural over-simplification. Detailed hyperparameters can be found in Appendix F. C.2 Smoothed Energy Guidance Smoothed Energy Guidance (SEG) [30] applies 2D Gaussian blur along the query axis to smooth the attention logits. By adjusting the kernel width σ, users can naturally control the strength of the perturbation. In the extreme case where σ , SEG reduces to averaging all query features and applying the result globally. 3 Figure 18: Generated images with linear interpolation between attention map and smoothed attention map A(SEG). SEG is not directly applicable to MMDiT. However, SEG is not directly compatible with MMDiT [19], where both image and text tokens participate in attention. Applying 2D Gaussian blur over the query axis becomes nontrivial when attention spans across modalities, as in recent text-to-video models where multiple frames attend jointly. This issue becomes more pronounced with the rise of multimodal attention (MM-attention). Restoring controllability to SEG using our framework. To address this, we propose simple yet widely applicable solution: interpolate between the original attention map and the one computed from the mean query featurecorresponding to the limiting case of SEG with σ . Specifically, we interpolate between the original attention map and the SEG attention map A(SEG), computed using mean query vector, as follows: A(SoftSEG) = (1 u)A + uA(SEG), [0, 1], (13) where A(SEG) = Softmax (cid:18) QK (cid:19) , := 1N RN d, := 1 (cid:88) i=1 Qi Rd. (14) Here, RN is the query matrix, and RN replicates the mean query vector across all tokens. This allows for smooth transition from the token-specific attention map to globallyaveraged variant, effectively reintroducing controllability. This provides controllable and modality-agnostic extension of SEG to architectures like MMDiT, where traditional Gaussian blurring becomes infeasible. We show the interpolation results in Fig. 18. C.3 Softmax temperature scaling Attention perturbation using softmax temperature scaling. One intuitive way to increase or decrease entropy is by introducing temperature parameter into the Softmax operation of attention, as follows: A(temp) = softmax (cid:19) (cid:18) log , > 0. (15) Lower temperatures (T 0) sharpen the distributions, driving them toward the Dirac delta distributions, while higher temperatures (T ) flatten the distribution towards uniform. This formulation 4 Figure 19: Generated images with temperature scaling perturbation. provides unified mechanism to traverse both entropy-increasing and entropy-decreasing directions within the attention perturbation space. Perturbation 0 results in different perturbation with PAG (I). As , the distributions approach uniform distributions, following an exponential-geodesic path. In contrast, as 0, the distribution converges to Dirac delta distribution. However, unlike the identity matrix I, where all probability mass is placed at each querys own index, this low-temperature limit concentrates mass on the maximum entry of each row, potentially differing from the diagonal. We illustrate the effect of varying in Fig. 19. Notably, increasing and decreasing leads to meaningful quality improvements, suggesting that modulation of entropy in either direction can enhance generation. Intuitive control parameter using interpolation framework. Softmax temperature scaling provides simple way to modulate perturbation strength by adjusting the temperature (0, ). However, it introduces practically unintuitive control parameter , and the limiting distribution as 0 is theoretically unreachable in practice due to numerical instability. Our interpolation framework offers compelling alternative: it allows for control using normalized and intuitive parameter [0, 1], while enabling us to directly realize the theoretical limit of the attention map as 0, denoted as AT 0, without the numerical issues associated with temperature scaling. Concretely, in the limit 0, the softmax attention converges to one-hot distribution where, for each row, the position of the maximum logit becomes 1 and all others become 0: A(T 0) ij := (cid:26)1 0 if = arg maxk Aik otherwise. Then the perturbation methods is as follows: A(SoftMG) = (1 u)A + uA(T 0), [0, 1]. (16) (17) As shown in Fig. 20, our framework can replicate this limiting behavior while preserving controllability through continuous interpolation parameter. Compared to softmax temperature scaling  (Fig. 19)  , this approach offers more intuitive and stable mechanism for navigating the space of attention perturbations. We refer to this perturbation-based guidance as Max Guidance (MG). Quantitative comparisons. We discuss various perturbation strategies and present their qualitative results in Fig. 11, 17, 19, 18, 20. To complement these findings, we provide quantitative comparisons in Tab. 2. Note that each perturbation method may benefit from different optimal hyperparameter configurations. To ensure fair evaluation, we define parameter pool and report the best-performing setting for Figure 20: Generated images with linear interpolation between attention map and smoothed attention map A(T 0). each method and each metric. Specifically, we consider perturbation layers in the early-to-mid range, namely layer 7, layer 8, layer 9, and layer 10; guidance scales {2.0, 4.0, 6.0, 8.0}; and interpolation parameters {0.2, 0.4, 0.6, 0.8, 1.0}. As shown in the Tab. 2, the perturbation consistently yields strong performance across most settings. Based on this robustness, we adopt it as our primary analysis tool in the main paper (Sec. 3, 4). Method FID Precision Recall Density Coverage PickScore ImageReward MPS Uniform Matrix Query Mean A(SEG) Identity Matrix 53.37 53.33 56.15 0.62 0.62 0.65 0.55 0.53 0. 0.89 0.91 1.06 0.05 0.06 0.06 21.90 21.89 22.19 0.53 0.50 0.65 11.21 11.13 11.70 Table 2: Comparison of perturbation strategies across different metrics. Each method shows strengths in different metrics. Overall, Identity matrix perturbation (SoftPAG) achieves strong results across most quality, diversityand human preference metrics. Notably, identity matrix perturbation outperforms others in preference-based scores such as PickScore, indicating better alignment with human perception and more visually pleasing results. Note that FID does not always correlate with human judgment. 6 HeadHunter: Implementation and additional analysis D.1 Implementation details Improving general image quality. To run HeadHunter, we used 20 prompts (M = 20), with = 24 and = 1. All prompts and the corresponding seeds are listed in Tab. 3. Since SD3 contains 24 layers with 24 attention heads per layer, we consider the full set of = 24 24 = 576 attention heads. For head-level guidance, we set the guidance scale to 3.0 and the number of inference steps to 20. While it is possible to set > 1 to run HeadHunter iteratively, we found that single iteration suffices in practice. This is based on the compositional nature of head-level guidance. Heads that individually improve quality tend to combine well, yielding consistent improvements in overall generation quality. As shown in Fig. 4, even compact set of heads selected in single iteration can outperform the best-performing layer-level guidance. Improving style-oriented quality. For SD3, we evaluated total of 55 style prompts for both quantitative and qualitative analysis. Since SD3 consists of 24 layers with 24 attention heads per layer, we considered all = 24 24 = 576 attention heads. For head-level guidance, we set the guidance scale to 3.0 and used 20 inference steps. For FLUX.1-Dev, we used total of 23 style prompts. The model comprises 57 layers with 24 attention heads per layer, resulting in = 57 24 = 1368 attention heads. Head-level guidance was applied with guidance scale of 8.0 and 15 inference steps. In both cases, we used 5 content prompts (M = 5) per style. The full list of content prompts is provided in Tab. 4. D.2 Additional results and analysis D.2.1 Additional results We present additional qualitative results for style-oriented quality improvement using HeadHunter in Figs. 21 to 23. Each row corresponds to the result obtained by perturbing the head set selected at iteration of Alg.1. The top row shows unguided generation, and the lower rows show results with an increasing number of selected heads applied. As more heads are progressively added, the generated images align better with the target style while maintaining visual plausibility. For example, for the style prompt sunlit, warm glow, golden hour, we observe an increasing glow effect, culminating in global golden-hour tone from the fourth row onward. We provide more detailed analysis of the contributing heads in Sec. 4.1.3 and Fig. 8. Interestingly, the framework is effective not only for photorealistic styles such as cinematic lighting, but also for abstract or simplified styles like line art or flat paper cut. Since this style adaptation is achieved without modifying any model parameters, it enables reusable and lightweight preference tuning. HeadHunter can benefit from with model size. Since larger models have more expressive attention heads, they can be more effectively leveraged in head-level perturbation guidance. In Fig. 21-26, we provide qualitative results of style-oriented quality improvement conducted on FLUX.1-dev [37]. 7 Figure 21: Qualitative results of HeadHunter for style-oriented image quality improvement on SD3. Figure 22: Qualitative results of HeadHunter for style-oriented image quality improvement on SD3. 9 Figure 23: Qualitative results of HeadHunter for style-oriented image quality improvement on SD3. 10 Figure 24: Qualitative results of HeadHunter for style-oriented image quality improvement on FLUX.1-Dev. Figure 25: Qualitative results of HeadHunter for style-oriented image quality improvement on FLUX.1-Dev. 12 Figure 26: Qualitative results of HeadHunter for style-oriented image quality improvement on FLUX.1-Dev. 13 Computational efficiency. HeadHunter requires one-time search process to identify effective attention heads for given objective. If we denote the number of promptseed pairs as , total attention heads as , number of iterations as , and the costs for generation and evaluation as Cgen and Ceval, the overall cost is approximately (Cgen + Ceval)T . In our experiments using stable-diffusion-3-medium with 20-step Euler sampling and PickScore [35] as the objective, this process takes roughly 3 hours on 8NVIDIA H100 GPUs. However, this cost is amortized over repeated use. Once set of heads is selected for given model and configuration, it can be reused across different prompts and latent seedsunlike test-time scaling approaches [46, 38] that require per-sample optimization. In the previous section, we further demonstrate that head sets found using small subset of prompts generalize well to broader and more diverse prompt distributions. At inference time, attention perturbation guidance adds negligible overhead, as it simply modifies fixed subset of attention maps without altering the model architecture or requiring additional forward passes. 14 Additional analysis on head-level guidance In this section, we provide an extended analysis of head-level perturbation guidance with focus on interpretable visual concepts. Specifically, Sec. E.1 presents more individual head-level guidance results in Stable Diffusion 3 and FLUX.1-Dev. In Sec. E.2, we showcase examples of head-level guidance that exhibit clearly interpretable semantic effects. Finally, Sec. E.3 explores the compositional behavior of head-level guidance and how combining them yields richer visual outcomes. 15 E.1 Additional results of individual head-level guidance Figure 27: Individual head-level guidance results on SD3. Each cell corresponds to the output guided by perturbing single head. Some heads induce notable effects, such as changes in lighting, structure, or color. 16 Figure 28: Individual head-level guidance results on FLUX.1-Dev. Each cell corresponds to the output guided by perturbing single head. Some heads induce notable effects, such as changes in lighting, structure, or color. 17 E.2 Additional results on interpretable head-level guidance Figure 29: Interpretable heads in SD3. Some heads exhibit consistent and clear visual effects when used for guidance. For instance, heads that add glow, alter geometry, or introduce specific lighting styles. These interpretable heads support the view that attention heads encode specific semantic concepts. Figure 30: Interpretable heads in FLUX.1-Dev. Similar to SD3, FLUX also contains heads with distinctive effects, suggesting the generality of head-level interpretability across architectures. 19 E.3 Additional results on the combinational effect of head-level guidance Figure 31: Compositional effects of head-level guidance in SD3. Combining heads leads to enriched outputs by blending their individual effects. This shows that attention heads can be composed to control more complex or stylized generations. 20 Figure 32: Compositional effects of head-level guidance in FLUX.1-Dev. Similar head-level guidance effects can be composed in FLUX.1-Dev to amplify or adjust stylistic elements. 21 Figure 33: Concept amplification by combining stylistic heads. Composing dark heads makes the generation darker, while white heads make it brighter. This demonstrates fine-grained control over stylistic dimensions via head-level selection."
        },
        {
            "title": "F Experimental details",
            "content": "Unless otherwise specified, all images are generated using Stable Diffusion 3-Medium with 20 Euler sampling steps. The default guidance scale is set to = 5.0. Fig. 1 Motivating examples for head-level guidance. Perturbations are applied individually to heads from layers 1, 3, and 14. Prompts are sampled from the MS-COCO validation set. Fig. 2 Generated images from headand layer-level perturbation guidance. In (a), heads with PickScore above fixed threshold are selected (indicated by red boxes). Prompts include medieval castle in the forest and futuristic cityscape. Fig. 3 Effect of head-level guidance on concept amplification. Each head is perturbed independently. Prompts used include: \"smiling girl holding cat, in flower garden\", \"middle-aged woman with glasses reading book in café\", \"A macro shot of red apple\", \"a close up portrait of an elderly Japanese man, soft lighting, 8k\", and \"A German shepherd bounding through pine forest\". Guidance scale is set to = 5.0. Fig. 4 Results of HeadHunter for general image quality improvement. HeadHunter is applied with = 1 and = 24. Prompts used include smiling girl holding cat, in flower garden, businessman with sleek, glass-shiny black hair neatly parted, waiting for morning train on city platform, and towering lighthouse casting rotating beam across storm-tossed waves at twilight. Metrics such as PickScore and AES are used for evaluation. Fig. 11 Linear interpolation between attention map and identity matrix (SoftPAG). Images are generated with perturbation applied to layer 9. Prompts are generated from ChatGPT. Fig. 17 Linear interpolation between attention map and uniform matrix (UG). Perturbation is applied to layer 9. Prompts are the same as those used in Fig. 11."
        },
        {
            "title": "G Experimental Resources",
            "content": "All experiments, including testing perturbation methods, analyzing head-level guidance, running HeadHunter, and conducting ablation studies, are performed using mix of 8 NVIDIA H100 GPUs, 6 NVIDIA RTX 3090 GPUs, and 2 NVIDIA A6000 GPUs. Limitations & Broader Impacts While our method is effective, there remains room for improving its efficiency. Running HeadHunter iteratively can be computationally intensive, especially for large-scale models with many heads. That said, the selected heads tend to generalize well across prompts and latents, making reuse practical. Exploring faster head and parameter selection strategies offers an exciting direction for future work. Our work improves quality and stylistic control in diffusion models through guidance. This can benefit creative applications such as digital art, design. However, enhanced quality may also increase the risk of misuse, including the creation of deceptive or harmful content like deepfakes. While our work does not involve identity synthesis or model release, we acknowledge this risk and recommend responsible deployment practices such as usage restrictions and content disclosure. 23 Figure 34: Grid search for UG, on guidance scale and interpolation parameter with different metrics."
        },
        {
            "title": "I Additional quantitative results of interpolation",
            "content": "More quantitative results. We measured the quality of images generated by layers 7, 8, 9, and 10, which produce the highest-quality outputs. Fig. 35 depicts the results of SoftPAG, which interpolates between the identity matrix and the attention maps, while Fig. 36 depicts the results of UG, which interpolates between the uniform matrix and the attention maps. 24 Figure 35: Quantitative results of interpolation in SoftPAG, on layer 7, 8, 9, 10 with different metrics. 25 Figure 36: Quantitative results of interpolation in UG, on layer 7, 8, 9, 10 with different metrics. Table 3: Prompt and seed list used for general quality improvement via HeadHunter. Prompt close up of an old Japanese man, soft lighting, 8k black man with long braids, wearing white robes and heavy metal jewelry, pointing at the viewer against dark background in the style of cinematic photography for fashion shoot. photo of an Asian girl with her hand on her nose, with red nail polish and bandaid, with black hair, close up portrait, in the style of Rinko Kawauchi. Japanese woman with short black hair, bangs hairstyle, hand covering eye, ring rings on, cool pose, outdoor sunlight background, natural lighting, portrait photography, in the style of Leica Q2 camera group of models of diverse ethnicities wearing various Nike sneakers, all dressed in white and colorful with patterns and designs inspired by the pop art movement, posed together for an editorial photoshoot against backdrop featuring abstract collage-like elements in shades of red and blue. The scene was vibrant and dynamic, capturing their energetic expressions and stylish outfits in the style of the pop art movement. photo of black special forces soldier doing an olympic ski jump, holding two katanas in his hands, white background, 35mm film stills in the style of Cai Guo-Qiang and James tall German man sitting in bench, reading book, medium shot breakdancer doing backflip photograph of an old oak tree in the middle, surrounded by dense woodland foliage, with sunlight filtering through the leaves and casting dappled light on the ground. The image was shot using Hasselblad camera, Kodak Gold 200 film, and Portra 800 film stock. close-up of womans hand with delicate fingers, soft lighting, photorealistic photo of beautiful girl Seed 0 0 0 0 0 0 0 0 0 0-9 Table 4: Content prompts and corresponding seeds used for style-oriented quality improvement via HeadHunter. Prompt Seed portrait of an elderly man with white hair, wearing wool coat, looking into the distance young woman in red dress, standing in the wind, eyes closed teenage boy with messy hair, wearing headphones, sitting on rooftop smiling girl holding cat, in flower garden man leaning against brick wall, hands in pockets, calmly observing the street 0 1 2 3"
        }
    ],
    "affiliations": [
        "HuggingFace",
        "KAIST",
        "KAIST AI",
        "Korea University",
        "Krea AI",
        "University of Washington"
    ]
}