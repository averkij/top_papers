{
    "paper_title": "Rethinking Driving World Model as Synthetic Data Generator for Perception Tasks",
    "authors": [
        "Kai Zeng",
        "Zhanqian Wu",
        "Kaixin Xiong",
        "Xiaobao Wei",
        "Xiangyu Guo",
        "Zhenxin Zhu",
        "Kalok Ho",
        "Lijun Zhou",
        "Bohan Zeng",
        "Ming Lu",
        "Haiyang Sun",
        "Bing Wang",
        "Guang Chen",
        "Hangjun Ye",
        "Wentao Zhang"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Recent advancements in driving world models enable controllable generation of high-quality RGB videos or multimodal videos. Existing methods primarily focus on metrics related to generation quality and controllability. However, they often overlook the evaluation of downstream perception tasks, which are $\\mathbf{really\\ crucial}$ for the performance of autonomous driving. Existing methods usually leverage a training strategy that first pretrains on synthetic data and finetunes on real data, resulting in twice the epochs compared to the baseline (real data only). When we double the epochs in the baseline, the benefit of synthetic data becomes negligible. To thoroughly demonstrate the benefit of synthetic data, we introduce Dream4Drive, a novel synthetic data generation framework designed for enhancing the downstream perception tasks. Dream4Drive first decomposes the input video into several 3D-aware guidance maps and subsequently renders the 3D assets onto these guidance maps. Finally, the driving world model is fine-tuned to produce the edited, multi-view photorealistic videos, which can be used to train the downstream perception models. Dream4Drive enables unprecedented flexibility in generating multi-view corner cases at scale, significantly boosting corner case perception in autonomous driving. To facilitate future research, we also contribute a large-scale 3D asset dataset named DriveObj3D, covering the typical categories in driving scenarios and enabling diverse 3D-aware video editing. We conduct comprehensive experiments to show that Dream4Drive can effectively boost the performance of downstream perception models under various training epochs. Page: https://wm-research.github.io/Dream4Drive/ GitHub Link: https://github.com/wm-research/Dream4Drive"
        },
        {
            "title": "Start",
            "content": "Data Generator for Perception Tasks Kai Zeng1,2, Zhanqian Wu2, Kaixin Xiong2, Xiaobao Wei1,2, Xiangyu Guo2,3, Zhenxin Zhu2, Kalok Ho2, Lijun Zhou2, Bohan Zeng1, Ming Lu2, Haiyang Sun2, Bing Wang2, Guang Chen2, Hangjun Ye 2, Wentao Zhang (cid:66) (cid:66) 1 1Peking University 2Xiaomi EV 3Huazhong University of Science and Technology Equal Contribution, Project Leader, (cid:66) Equal Corresponding Author Recent advancements in driving world models enable controllable generation of high-quality RGB videos or multimodal videos. Existing methods primarily focus on metrics related to generation quality and controllability. However, they often overlook the evaluation of downstream perception tasks, which are really crucial for the performance of autonomous driving. Existing methods usually leverage training strategy that first pretrains on synthetic data and finetunes on real data, resulting in twice the epochs compared to the baseline (real data only). When we double the epochs in the baseline, the benefit of synthetic data becomes negligible. To thoroughly demonstrate the benefit of synthetic data, we introduce Dream4Drive, novel synthetic data generation framework designed for enhancing the downstream perception tasks. Dream4Drive first decomposes the input video into several 3D-aware guidance maps and subsequently renders the 3D assets onto these guidance maps. Finally, the driving world model is fine-tuned to produce the edited, multi-view photorealistic videos, which can be used to train the downstream perception models. Dream4Drive enables unprecedented flexibility in generating multi-view corner cases at scale, significantly boosting corner case perception in autonomous driving. To facilitate future research, we also contribute large-scale 3D asset dataset named DriveObj3D, covering the typical categories in driving scenarios and enabling diverse 3D-aware video editing. We conduct comprehensive experiments to show that Dream4Drive can effectively boost the performance of downstream perception models under various training epochs. Date: October 27, 2025 Project: https://wm-research.github.io/Dream4Drive/ 5 2 0 2 4 2 ] . [ 2 5 9 1 9 1 . 0 1 5 2 : r Figure 1 Dream4Drive demonstrates the effectiveness of synthetic data: with fewer than 2% synthetic samples, it consistently improves detection and tracking across epochs, outperforming previous data augmentation baselines under fair evaluation. 1 denotes the baseline training epoch; 2 and 3 mean doubling and tripling it."
        },
        {
            "title": "Introduction",
            "content": "Perception tasks such as 3D object detection (Li et al., 2022, 2024c; Wang et al., 2023a, 2025) and 3D tracking (Wang et al., 2023a; Zhang et al., 2023c; Han et al., 2025), which support planning and decisionmaking (Jiang et al., 2023; Hu et al., 2023), are extremely important in autonomous driving. The performance of perception models, however, is highly dependent on large-scale annotated training datasets (Caesar et al., 2020; Wang et al., 2023b). To ensure reliability in rare but critical safety scenarios, it is essential to gather adequate long-tail data. Although the autonomous driving community has developed thorough 3D annotation pipeline to facilitate data acquisition (Zhao et al., 2025b), collecting long-tail data remains highly time-consuming and labor-intensive. Driving world models based on diffusion model (Rombach et al., 2022) and ControlNet (Zhang et al., 2023b) have been developed in autonomous driving to generate synthetic data from scene layouts (e.g., BEV maps, 3D bounding boxes) and text (Gao et al., 2023; Wen et al., 2024; Guo et al., 2025). However, they provide limited control over the poses and appearances of individual objects, which restricts the ability to generate diverse synthetic data (Li et al., 2025). Editing methods (Singh et al., 2024; Liang et al., 2025b; Yu et al., 2025) instead of generation extend driving world models by using reference images and 3D bounding boxes to modify object appearance and pose, allowing for diverse corner cases. However, single-view insertion limits their application in multi-view BEV perception. Reconstruction-based methods using NeRF or 3DGS enable precise control over geometric structures (Chen et al., 2021; Zanjani et al., 2025). While geometry is preserved, sparse training views often lead to artifacts and incomplete renderings. Additionally, the lack of illumination modeling results in inconsistencies between inserted objects and the background. More importantly, we argue that the data augmentation experiments of previous methods (Wen et al., 2024; Li et al., 2024a) are unfair, as they usually leverage training strategy that first pretrains on synthetic data and finetunes on real data, resulting in twice the epochs compared to the baseline (real data only). We find that, under the same number of training epochs, large amounts of synthetic datasets offer little to no advantage and can even perform worse than using real data alone. As shown in Fig. 1, under the 2 epoch setting, models trained exclusively on real data achieve higher mAP and NDS compared to those trained on real and synthetic data. Given the importance of downstream perception tasks for autonomous driving, we believe it is essential to rethink the effectiveness of the driving world model as synthetic data generator for these tasks. To reevaluate the value of synthetic data, we introduce Dream4Drive, novel 3D-aware synthetic data generation framework designed for downstream perception tasks. The core idea of Dream4Drive is to first decompose the input video into several 3D-aware guidance maps and subsequently render the 3D assets onto these guidance maps. Finally, the driving world model is fine-tuned to produce the edited, multi-view photorealistic videos, which can be used to train the downstream perception models. Consequently, we can incorporate various assets with different trajectories(e.g., views, poses, and distance) into the same scene, significantly improving the diversity of the synthetic data. As shown in Fig. 1, under identical training epochs (1, 2, or 3), our method requires only 420 synthetic samplesless than 2% of real samplesto surpass prior augmentation methods. To be best of our knowledge, we are the first to demonstrate under fair comparisons that synthetic data can provide real benefits beyond training solely on real data. Specifically, Dream4Drive leverages multi-view video inpainting model finetuned from the Diffusion Transformer (Peebles and Xie, 2023). Unlike prior methods that rely on sparse spatial controls (e.g., BEV maps and 3D bounding boxes), Dream4Drive uses dense 3D-aware guidance maps (Yu and Smith, 2019; Liang et al., 2025a) (e.g., depth, normal, edge, cutout, and mask) to preserve the geometry and appearance of the original video, while editing them by rendering 3D assets into these maps. This design enables instance-level, cross-view consistent video editing, ensuring both visual realism and geometric fidelity. The generated videos not only achieve superior quality but can also be directly used to train state-of-the-art perception models. To facilitate diverse 3D-aware video editing, we design pipeline that automatically acquires high-quality 3D assets based on target scene image or video of desired asset category. We first apply the image segmentation model (Ren et al., 2024; Lin et al., 2024) to localize and crop objects of the specified category, then employ the image generation model (Wu et al., 2025) to generate multi-view consistent images of the target object. These images are fed into mesh generation model (Hunyuan3D et al., 2025) to generate high-quality 3D assets. We present DriveObj3D, large-scale 3D asset dataset that encompasses typical 2 Figure 2 The illustration of Dream4Drive, which provides precise annotations, geometric variety, and appearance diversity to improve downstream perception tasks. categories found in driving scenarios, to support future research. Our main contributions are: We find that previous data augmentation methods are evaluated unfairly: under the same training epochs, hybrid datasets do not show any advantage over real data alone. We propose Dream4Drive, 3D-aware synthetic data generation framework that edits the video with dense guidance maps, producing synthetic data with diverse appearances and geometric consistency. We contribute large-scale dataset named DriveObj3D, covering the typical categories in driving scenarios for 3D-aware video editing. Extensive experiments across different training epochs show that adding less than 2% synthetic data can significantly improve perception performance, highlighting the effectiveness of Dream4Drive."
        },
        {
            "title": "2 Related Work",
            "content": "Video Generation in Autonomous Driving. High-quality data is crucial for training perception models in autonomous driving, motivating growing interest in driving video generation. Early approaches (Yang et al., 2023; Gao et al., 2023; Luo et al., 2025; Li et al., 2024b) employ diffusion models with ControlNet, conditioned on BEV maps (Ma et al., 2024b) and 3D bounding boxes, to generate paired image data (An et al., 2024, 2025b; Luo et al., 2024c,a,b). Recent works (Gao et al., 2024a; Jiang et al., 2024; Li et al., 2024a; Ji et al., 2025; Guo et al., 2025) adopt powerful Diffusion Transformers (DiTs) to further enhance generation quality. SubjectDrive (Huang et al., 2024a) uses an external subject bank to enhance vehicle appearance diversity. However, these methods depend on original scene layouts, which limit geometric diversity and struggle to generate high-quality long-tail corner cases, thereby restricting their effectiveness for downstream perception. Video Editing in Autonomous Driving. To enrich scene diversity, object-level editing methods insert new objects into existing videos using reference images and 3D bounding boxes (Singh et al., 2024; Liang et al., 2025b; Buburuzan et al., 2025; An et al., 2025a). More recent NeRF- (Mildenhall et al., 2021) and 3DGSbased (Kerbl et al., 2023) approaches (Chen et al., 2021; Huang et al., 2024b; Chen et al., 2024; Wei et al., 3 2024) improve geometric fidelity, yet remain limited by sparse views, inconsistent lighting, and restricted background diversity. In contrast, our approach builds on generative models and introduces novel 3D-aware video editing mechanism, enabling seamless insertion of diverse 3D assets and producing geometrically and visually diverse data that effectively boosts downstream performance."
        },
        {
            "title": "3 Dream4Drive",
            "content": "Our goal is to generate high-quality synthetic videos from real videos with 3D box annotations and target 3D assets for training perception models. We first introduce the preliminaries in Sec. 3.1, then explain how to conduct 3D-aware scene editing in Sec. 3.2, and finally describe how to render the edited video from the guidance maps in Sec. 3.3. The overall framework is shown in Fig. 2."
        },
        {
            "title": "3.1 Preliminaries",
            "content": "Latent Diffusion Models (LDMs) (Rombach et al., 2022) address the high computational cost of diffusion models by operating in lower-dimensional latent space. Given an image x, an encoder is used to obtain the corresponding latent representation = E(x). The forward diffusion process in the latent space is defined as gradual noising process: (1) i=1(1 βi) with βi being variance schedule. The reverse process is modeled by neural q(ztz0) = (zt; αtz0, (1 αt)I), where αt = (cid:81)t network ϵθ and parameterized as: pθ(zt1zt) = (zt1; µθ(zt, t), Σθ(zt, t)), (2) Finally, decoder maps the denoised latent variable back to the image space, i.e., = D(z0). ControlNets (Zhang et al., 2023a) extend LDMs by incorporating additional conditioning signals to provide finer control over the generation process. In particular, the reverse diffusion process is modified to condition on c: pθ(zt1zt, c) = (zt1; µθ(zt, t, c), Σθ(zt, t, c)), The conditioning variable can incorporate various forms of guidance, including spatial maps, semantic layouts, and other task-specific signals. By integrating c, ControlNets allow for more precise manipulation of the generation process and improving the quality of synthesized images. (3) 3. 3D-aware scene editing Figure 3 The illustration of 3D-aware scene editing. Given the input images, we first obtain the depth map, normal map, and edge map for the background and then render the object image and object mask for the target 3D asset. For an input RGB image RHW 3, we use Depth Anything (Yang et al., 2024) to obtain the depth map RHW 1. The normal map RHW 3 is then derived from the depth map. We also use OpenCVs Canny edge detector to get the edge map RHW 1. It is important to note that the depth, normal, and edge information within the foreground object regions are masked since our model needs to learn to generate an edited video based on the target 3D asset. For target 3D asset in our DriveObj3D, we position it within the 3D space of the original video based on the . For each frame and each view, we then use calibrated camera intrinsics provided 3D bounding boxes {Bi}T i=1 Kv and extrinsics Ev to render the target 3D asset. Therefore, we can obtain the object image RHW 3 and object mask RHW 1. Once we have obtained the depth map, normal map, and edge map for the background, as well as the rendered object image and object mask for the target 3D asset, we utilize fine-tuned driving world model to render the edited video based on these 3D-aware guidance maps. This 3D-aware scene editing pipeline effectively utilizes the accurate pose, geometry, and texture information provided by 3D assets, ensuring geometric consistency in the results generated. Notably, our method does not depend on 3D bounding box embeddings for controlling object placement. Instead, we directly edit in 3D space, offering more intuitive and reliable way to manage control. 3.3 3D-aware video rendering Figure 4 The illustration of 3D-aware video rendering. Given the 3D-aware guidance maps, we employ multi-condition fusion adapter to control the video generation of diffusion transformer, rendering the edited video. The video generation process in recent methods (Wen et al., 2024; Li et al., 2024a; Guo et al., 2025) relies on sparse spatial controls, such as birds eye view (BEV) maps and projected 3D bounding boxes. While this approach allows for some control over the synthesized content, it often falls short in achieving high-quality generation. Instead of relying on sparse spatial controls, we fine-tune driving world model to generate edited videos from the dense 3D-aware guidance maps, including depth map (D), normal map (N ), and edge map (E) for the background and the image (O) and mask (M ) for the foreground object. The architecture of the multi-condition fusion adapter is shown in Fig. 4. We first encode the five conditions using VAE, and then apply different 3D embedders to patchify the latents. FusionNet module then combines these five sets of features, as described by the following equation: Ffusion = FusionNet (cid:32) 5 (cid:77) k=1 3DEmbedderk(VAE(Ck)) (cid:12) (cid:12) (cid:12) Ck {D, N, E, O, } (cid:33) , (4) where D, N, E, O, denote the depth, normal, edge, object, and mask, respectively, and indicates concatenation along the channel dimension. The fused features are incorporated into the control blocks of the DiT architecture, enriching semantic information and thereby facilitating instance-level spatial alignment, temporal consistency, and semantic fidelity. The outputs of the control blocks are further integrated into the base block. Additionally, spatial view attention mechanism is introduced to enhance cross-view coherence, which is especially beneficial in driving scenes. We adopt rectified flow (Liu et al., 2022) for stable sampling and classifier-free guidance (Ho and Salimans, 2022) to balance text with multiple 3D geometric conditions, thereby improving controllability. The main 5 training objective is simplified diffusion loss that predicts the noise component at each timestep: Ldiffusion = Et,z0,ϵN (0,I) (cid:2)ϵ ϵθ(zt, t, c)2(cid:3) , (5) where ϵ is Gaussian noise, ϵθ the predicted noise, zt the noisy latent at timestep t, and all conditioning signals. To achieve precise instance-level control, we introduce Foreground Mask Loss as (Ji et al., 2025) and an LPIPS loss (Zhang et al., 2018). The final training objective is: Ltotal = λdiffusionLdiffusion + λmaskLmask + λlpipsLLPIPS, (6) In our experiments, the weights are empirically set as λdiffusion = 1.0, λmask = 0.1, and λlpips = 0.1. Notably, our training framework requires no expensive 3D annotations, relying solely on RGB videos and their 3D-aware guidance maps, which can be generated in real time using off-the-shelf tools. This approach significantly reduces training costs. More details are included in the Appx. A."
        },
        {
            "title": "4 DriveObj3D",
            "content": "To build large-scale 3D assets for diverse 3D-aware video editing, we design simple yet effective pipeline. The core idea is to decompose the asset generation process into three steps: (i) 2D instance segmentation; (ii) multi-view image generation; (iii) 3D mesh generation. As shown in Fig. 5, the pipeline inputs video or image and the target asset category, and generates 3D mesh for downstream applications. Concretely, we first localize and segment the target object. Given an input image and category label class, GroundedSAM (Ren et al., 2024) is applied to segment the object Itarget. To overcome the occlusion of target object, we employ multi-view image generation model Qwen-Image (Wu et al., 2025). , which are subsequently fed into Conditioned on Itarget, Qwen-Image synthesizes set of novel views {Iv}N multi-view reconstruction model Hunyuan3D (Hunyuan3D et al., 2025) to recover the final 3D mesh. Figure 5 The illustration of creating 3D asset in DriveObj3D. We first apply segmentation model to segment the target object, then generate multi-view images, and finally create 3D mesh from those images. v=1 Figure 6 Comparison of 3D asset generation across different methods. Our simple yet effective method produces better 3D assets across diverse categories in autonomous driving, outperforming existing baselines. Con_vehicle is construction vehicle; Pdes is Pedestrian; T_cone is traffic cone. Figure 7 DriveObj3D dataset. large-scale collection of diverse 3D assets across typical driving categories, supporting scalable video editing and synthetic data generation. As shown in Fig. 6, assets generated by Text-to-3D methods (Xiang et al., 2025) often exhibit style inconsistencies with the original data, while single-view approaches (Hunyuan3D et al., 2025) tend to produce incomplete assets. In contrast, our simple yet effective pipeline leverages multi-view synthesis to generate complete and high-fidelity assets, even under severe occlusions. To support large-scale downstream driving tasks, we construct diverse asset dataset, DriveObj3D, covering wide range of categories in driving scenarios in Fig. 7. All assets will be released publicly."
        },
        {
            "title": "5.1 Setup",
            "content": "Datasets. We utilize the nuScenes dataset (Caesar et al., 2020) for building 3D assets and finetuning our video generation model. It comprises 1000 scenes in total, with 700 designated for training, 150 for validation, and 150 for testing. Each scene contains 20-second multi-view video captured by 6 cameras. More details on inserted scene and asset selection are given in the Appx. A. Metrics. Following Panacea (Wen et al., 2024) and SubjectDrive (Huang et al., 2024a), we primarily evaluate how the generated data improves perceptual model performance on detection and tracking tasks. Detection metrics include nuScenes Detection Score (NDS), mean Average Precision (mAP), mean Average Orientation Error (mAOE), and mean Average Velocity Error (mAVE). Tracking metrics include Average MultiObject Tracking Accuracy (AMOTA), Precision (AMOTP), Recall, and Accuracy (MOTA)."
        },
        {
            "title": "5.2 Main Results",
            "content": "Effectiveness for Downstream Tasks. We evaluate the effectiveness of Dream4Drive against prior driving world models, with detection and tracking results reported in Tab. 1 and Tab. 2. While Panacea and SubjectDrive outperform the real dataset baseline at double training epochs, aligning the training epochs shows minimal gains over using real data alone. In contrast, our method explicitly edits objects at specified 3D positions and leverages 3D-aware guidance maps to guide foreground-background synthesis, generating accurately annotated videos that consistently improve downstream perception models. Remarkably, with only 420 inserted samples, our approach outperforms prior methods that used the full set of synthetic data. Moreover, for the first time, synthetic data achieves performance that surpasses real data when training epochs are equal. 1x Epochs 2x Epochs Real (28130) Dream4Drive (+420, <2%) Real (28130) DriveDreamer (Wang et al., 2024a) WoVoGen (Lu et al., 2024) MagicDrive (Gao et al., 2023) Panacea (Wen et al., 2024) SubjectDrive (Huang et al., 2024a) Dream4Drive (Ours) mAP mAVE NDS 34.5 29.1 46.9 36.1 28.9 47.8 38.4 27.7 50.4 35.8 39.5 36.2 123.4 18. 35.4 39.8 37.1 27.3 49.2 38.1 26.4 50.2 38.7 26.8 50.6 Table 1 Comparison of detection under different training epochs. indicates the evaluation of WoVoGen is only on the vehicle classes of cars, trucks, and buses. Bold and underlines indicate the best and second best results respectively. 1x Epochs 2x Epochs Real (28130) Dream4Drive (+420, <2%) Real (28130) Panacea (Wen et al., 2024) SubjectDrive (Huang et al., 2024a) Dream4Drive (Ours) AMOTA AMOTP 30.1 137.9 31.2 135.4 34.1 134. 33.7 135.3 33.7 135.3 34.4 133.5 Table 2 Comparison of tracking under different training epochs. Bold and underlines indicate the best and second best. Effectiveness for Various Resolutions. As generative models continue to advance, the ability to synthesize high-resolution videos has become achievable (Gao et al., 2024b,a). To investigate the effect of high-resolution synthetic data on downstream perception models, we further conduct experiments for detection and tracking tasks at resolution of 512 768, as reported in Tab. 3 and 4. 2x Epochs Real Naive Insert Dream4Drive Real Naive Insert Dream4Drive Real Naive Insert Dream4Drive 3x Epochs 1x Epochs mAP mATE mAOE mAVE NDS 36.1 69.2 56.7 28.5 47.9 40.1 64.7 49.0 28.4 51.3 40.7 64.2 48.0 27.1 52. 42.2 61.6 43.2 27.5 53.2 42.9 62.4 37.5 27.3 54.0 43.6 61.6 39.4 27.4 54.3 43.1 60.5 45.7 27.4 53.6 43.1 61.5 38.9 27.4 54.2 44.5 59.8 40.1 27.2 55. Table 3 Detection performance under different training epochs (1x, 2x, 3x). Naive Insert\" denotes the direct projection of 3D assets into the original scene. Results are reported at 512768 resolution. Under both 1x and 2x epochs, real data and Dream4Drive significantly outperform their low-resolution counterparts (256 512, Tab. 1 and 2). Remarkably, with high-resolution augmentation, Dream4Drive requires only 420 samples to achieve 4.6 point (12.7%) mAP increase and 4.1 point (8.6%) NDS improvement. Most of the gains come from large vehicle categories, including bus, construction_vehicle, and truck; detailed AP for each category is provided in the Appx. B. Unlike prior augmentation paradigms, Dream4Drive consistently outperforms training on real data alone, regardless of the number of epochs, highlighting the value of high-quality synthetic data. Quantitative and Qualitative Comparison with Naive Insertion. After extracting 3D assets, we can directly generate edited videos with projection. To assess the impact of 3D-aware video rendering versus direct insertion on downstream tasks, we conduct comprehensive evaluations, as reported in Tab. 3 and 4. While direct insertion improves performance over real data alone, its results remain inferior to our generative method due to missing realism, such as shadows and reflections. Interestingly, direct insertion achieves the highest mAOE, likely because the inserted assets perfectly align with the original bounding box orientations. Fig. 8 presents visual comparisons between naive insertion and our generative approach across multiple scenes. 1x Epochs Real Naive Insert Ours 2x Epochs Real Naive Insert Ours 3x Epochs Real Naive Insert Ours AMOTA AMOTP MOTA RECALL 32.8 134.0 28.1 44.0 36.5 128.7 31.7 45.4 37.9 128.0 33.1 46.9 39.7 125.1 35.6 50.7 42.2 124.0 37.3 51. 42.6 123.3 37.4 51.8 41.3 124.1 36.8 52.4 42.2 123.7 37.5 51.8 43.5 121.3 38.5 52.5 Table 4 Tracking performance under different training epochs (1x, 2x, 3x). Naive Insert\" denotes the direct projection of 3D assets into the original scene. Results are reported at 512768 resolution. Comprehensive visualization of asset insertion results. More asset categories, insertion locations, and corner case scenarios are comprehensively presented in the Appx. D. 8 Figure 8 Comparison with naive insertion. As can be seen, Dream4Drive generates more realistic edited videos than naive insertion."
        },
        {
            "title": "5.3 Ablation Studies",
            "content": "Effect of Insertion Position. Dream4Drive can edit videos by projecting 3D assets anywhere in scene. To systematically evaluate the impact of insertion position on downstream model performance, we categorize positions as front, back, left, and right, as shown in Tab. 5. Results indicate that inserting assets in the front or back yields similar performance, whereas left-side insertions outperform right-side ones, with 0.4 point increase in mAP, 0.9 point increase in NDS, and 5.7 point reduction in mAOE. This likely indicates dataset bias: most vehicles appear on the ego vehicles left side, so enhancing such corner cases improves model predictions, while right-side corner cases yield limited gains on the validation set. We further examine the effect of insertion distance on performance in Tab. 5. Close insertions tend to perform poorly, likely due to the asset blocking the camera view, which interferes with the training of other instances. Distant insertions offer more effective augmentation, as detectors often struggle with distant objects. Increasing their prevalence enhances detection performance. Effect of 3D Asset Source. We observe that the source of inserted assets affects the quality of synthetic data, consistent with (Ljungbergh et al., 2025). We therefore investigate how different asset sources influence downstream perception performance."
        },
        {
            "title": "Far",
            "content": "3D Asset Generation Methods Trellis Hunyuan3D"
        },
        {
            "title": "Ours",
            "content": "mAP mATE mAOE mAVE NDS 40.2 66.2 51.2 27.7 51.0 40.2 66.0 55.1 27.5 50.6 40.2 64.6 45.7 28.5 51.6 39.8 66.2 51.4 27.9 50.7 39.7 65.7 52.2 28.1 50. 40.3 65.4 51.7 28.0 50.9 40.5 65.1 49.7 27.9 51.3 39.8 65.6 51.8 27.8 50.8 40.2 65.1 50.8 28.0 50.9 40.7 64.2 48.0 27.1 52.0 Table 5 Ablation Studies. We report detection performance across insertion positions and asset, with best results per block (Views, Distances, 3D Methods) in bold, at 512768 resolution. As shown in Tab. 5, although Trellis (Xiang et al., 2025) can generate high-quality assets, its style does not fully match autonomous driving scenarios, which can lead to artifacts and degraded quality when assets 9 are inserted, negatively affecting downstream tasks. Hunyuan3D (Hunyuan3D et al., 2025)s single-view generation also underperforms our multiview approach, since single-image assets may be incomplete, whereas our method produces complete, high-quality 3D assets."
        },
        {
            "title": "5.4 Takeaways",
            "content": "We summarize the main observations from our experiments with perception model augmentation as follows: Duplying original layouts to create synthetic data does not improve performance; instead, enhancing scenes by inserting new 3D assets is an effective strategy for augmentation. High-resolution synthetic data offers greater benefits for data augmentation. The placement of inserted assets influences the effectiveness of augmentation, highlighting biases present in the dataset. Insertions at farther distances generally improve performance, while close-range insertions may introduce strong occlusions that hinder training. Using assets from the same dataset reduces the domain gap between synthetic and real data, benefiting downstream model training."
        },
        {
            "title": "6 Conclusion",
            "content": "In this paper, we find that previous driving world models inaccurately assess the effectiveness of synthetic data for downstream tasks. To address this, we present Dream4Drive, 3D-aware synthetic data generation pipeline that synthesizes high-quality multi-view corner cases. To facilitate future research, we also contribute large-scale 3D asset dataset named DriveObj3D, covering the typical categories in driving scenarios. Extensive experiments demonstrate that with less than 2% additional synthetic data, Dream4Drive consistently improves downstream perception, validating the effectiveness of synthetic data for autonomous driving."
        },
        {
            "title": "References",
            "content": "Ruichuan An, Sihan Yang, Ming Lu, Renrui Zhang, Kai Zeng, Yulin Luo, Jiajun Cao, Hao Liang, Ying Chen, Qi She, et al. Mc-llava: Multi-concept personalized vision-language model. arXiv preprint arXiv:2411.11706, 2024. Ruichuan An, Sihan Yang, Renrui Zhang, Zijun Shen, Ming Lu, Gaole Dai, Hao Liang, Ziyu Guo, Shilin Yan, Yulin Luo, et al. Unictokens: Boosting personalized understanding and generation via unified concept tokens. arXiv preprint arXiv:2505.14671, 2025a. Ruichuan An, Kai Zeng, Ming Lu, Sihan Yang, Renrui Zhang, Huitong Ji, Qizhe Zhang, Yulin Luo, Hao Liang, and Wentao Zhang. Concept-as-tree: Synthetic data is all you need for vlm personalization. arXiv preprint arXiv:2503.12999, 2025b. Alexandru Buburuzan, Anuj Sharma, John Redford, Puneet Dokania, and Romain Mueller. Mobi: Multimodal object inpainting using diffusion models. In Proceedings of the Computer Vision and Pattern Recognition Conference, pages 19741984, 2025. Holger Caesar, Varun Bankiti, Alex Lang, Sourabh Vora, Venice Erin Liong, Qiang Xu, Anush Krishnan, Yu Pan, Giancarlo Baldan, and Oscar Beijbom. nuscenes: multimodal dataset for autonomous driving. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 1162111631, 2020. Yun Chen, Frieda Rong, Shivam Duggal, Shenlong Wang, Xinchen Yan, Sivabalan Manivasagam, Shangjie Xue, Ersin Yumer, and Raquel Urtasun. Geosim: Realistic video simulation via geometry-aware composition for self-driving. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 72307240, 2021. Ziyu Chen, Jiawei Yang, Jiahui Huang, Riccardo de Lutio, Janick Martinez Esturo, Boris Ivanovic, Or Litany, Zan Gojcic, Sanja Fidler, Marco Pavone, et al. Omnire: Omni urban scene reconstruction. arXiv preprint arXiv:2408.16760, 2024. Ruiyuan Gao, Kai Chen, Enze Xie, Lanqing Hong, Zhenguo Li, Dit-Yan Yeung, and Qiang Xu. Magicdrive: Street view generation with diverse 3d geometry control. arXiv preprint arXiv:2310.02601, 2023. Ruiyuan Gao, Kai Chen, Bo Xiao, Lanqing Hong, Zhenguo Li, and Qiang Xu. Magicdrivedit: High-resolution long video generation for autonomous driving with adaptive control. arXiv preprint arXiv:2411.13807, 2024a. Shenyuan Gao, Jiazhi Yang, Li Chen, Kashyap Chitta, Yihang Qiu, Andreas Geiger, Jun Zhang, and Hongyang Li. Vista: generalizable driving world model with high fidelity and versatile controllability. arXiv preprint arXiv:2405.17398, 2024b. Xiangyu Guo, Zhanqian Wu, Kaixin Xiong, Ziyang Xu, Lijun Zhou, Gangwei Xu, Shaoqing Xu, Haiyang Sun, Bing Wang, Guang Chen, et al. Genesis: Multimodal driving scene generation with spatio-temporal and cross-modal consistency. arXiv preprint arXiv:2506.07497, 2025. Liuyang Han, Jun Wang, Chen Li, Fazhan Tao, and Zhumu Fu. novel multi-object tracking framework based on multi-sensor data fusion for autonomous driving in adverse weather environments. IEEE Sensors Journal, 2025. Jonathan Ho and Tim Salimans. Classifier-free diffusion guidance. arXiv preprint arXiv:2207.12598, 2022. Yihan Hu, Jiazhi Yang, Li Chen, Keyu Li, Chonghao Sima, Xizhou Zhu, Siqi Chai, Senyao Du, Tianwei Lin, Wenhai Wang, et al. Planning-oriented autonomous driving. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 1785317862, 2023. Binyuan Huang, Yuqing Wen, Yucheng Zhao, Yaosi Hu, Yingfei Liu, Fan Jia, Weixin Mao, Tiancai Wang, Chi Zhang, Chang Wen Chen, et al. Subjectdrive: Scaling generative data in autonomous driving via subject control. arXiv preprint arXiv:2403.19438, 2024a. Nan Huang, Xiaobao Wei, Wenzhao Zheng, Pengju An, Ming Lu, Wei Zhan, Masayoshi Tomizuka, Kurt Keutzer, and Shanghang Zhang. S3gaussian: Self-supervised street gaussians for autonomous driving. arXiv preprint arXiv:2405.20323, 2024b. Team Hunyuan3D, Shuhui Yang, Mingxin Yang, Yifei Feng, Xin Huang, Sheng Zhang, Zebin He, Di Luo, Haolin Liu, Yunfei Zhao, et al. Hunyuan3d 2.1: From images to high-fidelity 3d assets with production-ready pbr material. arXiv preprint arXiv:2506.15442, 2025. Yishen Ji, Ziyue Zhu, Zhenxin Zhu, Kaixin Xiong, Ming Lu, Zhiqi Li, Lijun Zhou, Haiyang Sun, Bing Wang, and Tong Lu. Cogen: 3d consistent video generation via adaptive conditioning for autonomous driving. arXiv preprint arXiv:2503.22231, 2025. 11 Bo Jiang, Shaoyu Chen, Qing Xu, Bencheng Liao, Jiajie Chen, Helong Zhou, Qian Zhang, Wenyu Liu, Chang Huang, and Xinggang Wang. Vad: Vectorized scene representation for efficient autonomous driving. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 83408350, 2023. Junpeng Jiang, Gangyi Hong, Lijun Zhou, Enhui Ma, Hengtong Hu, Xia Zhou, Jie Xiang, Fan Liu, Kaicheng Yu, Haiyang Sun, et al. Dive: Dit-based video generation with enhanced control. arXiv preprint arXiv:2409.01595, 2024. Bernhard Kerbl, Georgios Kopanas, Thomas Leimkühler, and George Drettakis. 3d gaussian splatting for real-time radiance field rendering. ACM Trans. Graph., 42(4):1391, 2023. Bohan Li, Jiazhe Guo, Hongsi Liu, Yingshuang Zou, Yikang Ding, Xiwu Chen, Hu Zhu, Feiyang Tan, Chi Zhang, Tiancai Wang, et al. Uniscene: Unified occupancy-centric driving scene generation. arXiv preprint arXiv:2412.05435, 2024a. Jian Li, Weiheng Lu, Hao Fei, Meng Luo, Ming Dai, Min Xia, Yizhang Jin, Zhenye Gan, Ding Qi, Chaoyou Fu, et al. survey on benchmarks of multimodal large language models. arXiv preprint arXiv:2408.08632, 2024b. Jiusi Li, Jackson Jiang, Jinyu Miao, Miao Long, Tuopu Wen, Peijin Jia, Shengxiang Liu, Chunlei Yu, Maolin Liu, Yuzhan Cai, et al. Realistic and controllable 3d gaussian-guided object editing for driving video generation. arXiv preprint arXiv:2508.20471, 2025. Kaican Li, Kai Chen, Haoyu Wang, Lanqing Hong, Chaoqiang Ye, Jianhua Han, Yukuai Chen, Wei Zhang, Chunjing Xu, Dit-Yan Yeung, et al. Coda: real-world road corner case dataset for object detection in autonomous driving. In European Conference on Computer Vision, pages 406423. Springer, 2022. Zhiqi Li, Wenhai Wang, Hongyang Li, Enze Xie, Chonghao Sima, Tong Lu, Qiao Yu, and Jifeng Dai. Bevformer: learning birds-eye-view representation from lidar-camera via spatiotemporal transformers. IEEE Transactions on Pattern Analysis and Machine Intelligence, 2024c. Ruofan Liang, Zan Gojcic, Huan Ling, Jacob Munkberg, Jon Hasselgren, Chih-Hao Lin, Jun Gao, Alexander Keller, Nandita Vijaykumar, Sanja Fidler, et al. Diffusion renderer: Neural inverse and forward rendering with video diffusion models. In Proceedings of the Computer Vision and Pattern Recognition Conference, pages 2606926080, 2025a. Yiyuan Liang, Zhiying Yan, Liqun Chen, Jiahuan Zhou, Luxin Yan, Sheng Zhong, and Xu Zou. Driveeditor: unified 3d information-guided framework for controllable object editing in driving scenes. In Proceedings of the AAAI Conference on Artificial Intelligence, pages 51645172, 2025b. Weifeng Lin, Xinyu Wei, Ruichuan An, Peng Gao, Bocheng Zou, Yulin Luo, Siyuan Huang, Shanghang Zhang, and Hongsheng Li. Draw-and-understand: Leveraging visual prompts to enable mllms to comprehend what you want. arXiv preprint arXiv:2403.20271, 2024. Xingchao Liu, Chengyue Gong, and Qiang Liu. Flow straight and fast: Learning to generate and transfer data with rectified flow. arXiv preprint arXiv:2209.03003, 2022. William Ljungbergh, Bernardo Taveira, Wenzhao Zheng, Adam Tonderski, Chensheng Peng, Fredrik Kahl, Christoffer Petersson, Michael Felsberg, Kurt Keutzer, Masayoshi Tomizuka, et al. R3d2: Realistic 3d asset insertion via diffusion for autonomous driving simulation. arXiv preprint arXiv:2506.07826, 2025. Jiachen Lu, Ze Huang, Zeyu Yang, Jiahui Zhang, and Li Zhang. Wovogen: World volume-aware diffusion for controllable multi-camera driving scene generation. In European Conference on Computer Vision, pages 329345. Springer, 2024. Meng Luo, Hao Fei, Bobo Li, Shengqiong Wu, Qian Liu, Soujanya Poria, Erik Cambria, Mong-Li Lee, and Wynne Hsu. Panosent: panoptic sextuple extraction benchmark for multimodal conversational aspect-based sentiment analysis. In Proceedings of the 32nd ACM International Conference on Multimedia, pages 76677676, 2024a. Meng Luo, Han Zhang, Shengqiong Wu, Bobo Li, Hong Han, and Hao Fei. Nus-emo at semeval-2024 task 3: Instruction-tuning llm for multimodal emotion-cause analysis in conversations. arXiv preprint arXiv:2501.17261, 2024b. Meng Luo, Shengqiong Wu, Liqiang Jing, Tianjie Ju, Li Zheng, Jinxiang Lai, Tianlong Wu, Xinya Du, Jian Li, Siyuan Yan, Jiebo Luo, William Yang Wang, Hao Fei, Mong-Li Lee, and Wynne Hsu. Dr.v: hierarchical perception-temporal-cognition framework to diagnose video hallucination by fine-grained spatial-temporal grounding, 2025. 12 Yulin Luo, Ruichuan An, Bocheng Zou, Yiming Tang, Jiaming Liu, and Shanghang Zhang. Llm as dataset analyst: Subpopulation structure discovery with large language model. In European Conference on Computer Vision, pages 235252. Springer, 2024c. Enhui Ma, Lijun Zhou, Tao Tang, Zhan Zhang, Dong Han, Junpeng Jiang, Kun Zhan, Peng Jia, Xianpeng Lang, Haiyang Sun, et al. Unleashing generalization of end-to-end autonomous driving with controllable long video generation. arXiv preprint arXiv:2406.01349, 2024a. Yuexin Ma, Tai Wang, Xuyang Bai, Huitong Yang, Yuenan Hou, Yaming Wang, Yu Qiao, Ruigang Yang, and Xinge Zhu. Vision-centric bev perception: survey. IEEE Transactions on Pattern Analysis and Machine Intelligence, 2024b. Ben Mildenhall, Pratul Srinivasan, Matthew Tancik, Jonathan Barron, Ravi Ramamoorthi, and Ren Ng. Nerf: Representing scenes as neural radiance fields for view synthesis. Communications of the ACM, 65(1):99106, 2021. William Peebles and Saining Xie. Scalable diffusion models with transformers. In Proceedings of the IEEE/CVF international conference on computer vision, pages 41954205, 2023. Tianhe Ren, Shilong Liu, Ailing Zeng, Jing Lin, Kunchang Li, He Cao, Jiayu Chen, Xinyu Huang, Yukang Chen, Feng Yan, et al. Grounded sam: Assembling open-world models for diverse visual tasks. arXiv preprint arXiv:2401.14159, 2024. Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Björn Ommer. High-resolution image synthesis with latent diffusion models. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 1068410695, 2022. Bharat Singh, Viveka Kulharia, Luyu Yang, Avinash Ravichandran, Ambrish Tyagi, and Ashish Shrivastava. Genmm: Geometrically and temporally consistent multimodal data generation for video and lidar. arXiv preprint arXiv:2406.10722, 2024. Hai Wang, Junhao Liu, Haoran Dong, and Zheng Shao. survey of the multi-sensor fusion object detection task in autonomous driving. Sensors, 25(9):2794, 2025. Shihao Wang, Yingfei Liu, Tiancai Wang, Ying Li, and Xiangyu Zhang. Exploring object-centric temporal modeling for efficient multi-view 3d object detection. In Proceedings of the IEEE/CVF international conference on computer vision, pages 36213631, 2023a. Xiaofeng Wang, Zheng Zhu, Yunpeng Zhang, Guan Huang, Yun Ye, Wenbo Xu, Ziwei Chen, and Xingang Wang. Are we ready for vision-centric driving streaming perception? the asap benchmark. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 96009610, 2023b. Xiaofeng Wang, Zheng Zhu, Guan Huang, Xinze Chen, Jiagang Zhu, and Jiwen Lu. Drivedreamer: Towards real-worlddrive world models for autonomous driving. In European Conference on Computer Vision, pages 5572. Springer, 2024a. Yuqi Wang, Jiawei He, Lue Fan, Hongxin Li, Yuntao Chen, and Zhaoxiang Zhang. Driving into the future: Multiview In Proceedings of the IEEE/CVF visual forecasting and planning with world model for autonomous driving. Conference on Computer Vision and Pattern Recognition, pages 1474914759, 2024b. Xiaobao Wei, Qingpo Wuwu, Zhongyu Zhao, Zhuangzhe Wu, Nan Huang, Ming Lu, Ningning Ma, and Shanghang Zhang. Emd: Explicit motion modeling for high-quality street gaussian splatting. arXiv preprint arXiv:2411.15582, 2024. Yuqing Wen, Yucheng Zhao, Yingfei Liu, Fan Jia, Yanhui Wang, Chong Luo, Chi Zhang, Tiancai Wang, Xiaoyan Sun, and Xiangyu Zhang. Panacea: Panoramic and controllable video generation for autonomous driving. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 69026912, 2024. Chenfei Wu, Jiahao Li, Jingren Zhou, Junyang Lin, Kaiyuan Gao, Kun Yan, Sheng ming Yin, Shuai Bai, Xiao Xu, Yilei Chen, Yuxiang Chen, Zecheng Tang, Zekai Zhang, Zhengyi Wang, An Yang, Bowen Yu, Chen Cheng, Dayiheng Liu, Deqing Li, Hang Zhang, Hao Meng, Hu Wei, Jingyuan Ni, Kai Chen, Kuan Cao, Liang Peng, Lin Qu, Minggang Wu, Peng Wang, Shuting Yu, Tingkun Wen, Wensen Feng, Xiaoxiao Xu, Yi Wang, Yichang Zhang, Yongqiang Zhu, Yujia Wu, Yuxuan Cai, and Zenan Liu. Qwen-image technical report, 2025. Jianfeng Xiang, Zelong Lv, Sicheng Xu, Yu Deng, Ruicheng Wang, Bowen Zhang, Dong Chen, Xin Tong, and Jiaolong Yang. Structured 3d latents for scalable and versatile 3d generation. In Proceedings of the Computer Vision and Pattern Recognition Conference, pages 2146921480, 2025. 13 Kairui Yang, Enhui Ma, Jibin Peng, Qing Guo, Di Lin, and Kaicheng Yu. Bevcontrol: Accurately controlling street-view elements with multi-perspective consistency via bev sketch layout. arXiv preprint arXiv:2308.01661, 2023. Lihe Yang, Bingyi Kang, Zilong Huang, Xiaogang Xu, Jiashi Feng, and Hengshuang Zhao. Depth anything: Unleashing the power of large-scale unlabeled data. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 1037110381, 2024. Beike Yu, Dafang Wang, Jiang Cao, Pengyu Zhu, and Yifei Zhao. Vehiclesim: realistic and 3d-aware video editing with one image for autonomous driving. Multimedia Systems, 31(4):316, 2025. Ye Yu and William AP Smith. Inverserendernet: Learning single image inverse rendering. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 31553164, 2019. Farhad Zanjani, Davide Abati, Auke Wiggers, Dimitris Kalatzis, Jens Petersen, Hong Cai, and Amirhossein Habibian. Gaussian splatting is an effective data generator for 3d object detection. arXiv preprint arXiv:2504.16740, 2025. Lvmin Zhang, Anyi Rao, and Maneesh Agrawala. Adding conditional control to text-to-image diffusion models. In Proceedings of the IEEE/CVF international conference on computer vision, pages 38363847, 2023a. Lvmin Zhang, Anyi Rao, and Maneesh Agrawala. Adding conditional control to text-to-image diffusion models. In Proceedings of the IEEE/CVF international conference on computer vision, pages 38363847, 2023b. Peng Zhang, Xin Li, Liang He, and Xin Lin. 3d multiple object tracking on autonomous driving: literature review. arXiv preprint arXiv:2309.15411, 2023c. Richard Zhang, Phillip Isola, Alexei Efros, Eli Shechtman, and Oliver Wang. The unreasonable effectiveness of deep features as perceptual metric. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 586595, 2018. Guosheng Zhao, Xiaofeng Wang, Zheng Zhu, Xinze Chen, Guan Huang, Xiaoyi Bao, and Xingang Wang. Drivedreamer2: Llm-enhanced world models for diverse driving video generation. In Proceedings of the AAAI Conference on Artificial Intelligence, pages 1041210420, 2025a. Jingyuan Zhao, Yuyan Wu, Rui Deng, Susu Xu, Jinpeng Gao, and Andrew Burke. survey of autonomous driving from deep learning perspective. ACM Computing Surveys, 57(10):160, 2025b."
        },
        {
            "title": "A Implementation Details",
            "content": "A.1 Model and Training Details DriveObj3D Generation Pipeline. We adopt Grounded-SAM(Ren et al., 2024) for image segmentation, Qwen-Image-Edit(Wu et al., 2025) for image generation, and Hunyuan3D 2.0(Hunyuan3D et al., 2025) for 3D asset generation. Inpainting Diffusion Model. For video synthesis, Dream4Drive is constructed upon DiT-based architecture. The backbone is initialized with pretrained weights from MagicDriveDiT (Gao et al., 2024a), which originally conditioned on BEV maps and 3D bounding boxes. We extend its ControlNet branch by incorporating novel 3D-aware guidance maps, and fine-tune the model on the nuScenes dataset. The generated videos have resolution of 512 768 with 33 frames. Training is conducted in PyTorch using 8 NVIDIA H200 GPUs with mixed-precision acceleration for 2000 iterations. We employ the AdamW optimizer with weight decay of 0.01 and adopt cosine annealing learning rate schedule with linear warm-up over the first 10% of steps. The learning rate is set to 2 104, and the batch size per GPU is 1. During training, we introduce additional Mask Loss and LPIPS Loss to enhance perceptual consistency and local reconstruction quality. Mask Loss. In the VAE-decoded space, we compute the mean squared error (MSE) between prediction and ground truth only within the foreground mask region. Let ˆx and denote the predicted and ground-truth decoded images, and be binary mask (1 for constrained pixels, 0 otherwise). The masked MSE is defined as: Lmask = (ˆx x)2 2 (cid:80) + ϵ , (7) where denotes element-wise multiplication, and the denominator normalizes over valid pixels. LPIPS Loss. To further improve perceptual quality, we adopt the LPIPS (Learned Perceptual Image Patch Similarity) loss (Zhang et al., 2018), which measures feature-level perceptual differences between prediction and ground truth: LLPIPS = LPIPS(ˆx, x), (8) where the LPIPS network is pretrained on large-scale perceptual similarity data. A.2 Experiment Details Scene Insertion. We select video frames where no other vehicles appear along the insertion trajectory. For training downstream perception models, inserted scenes are strictly drawn from the nuScenes training split. Asset Insertion. In the setting of Sec. 5.2, we use 420 samples evenly distributed across object categories, which provide the necessary coverage while keeping the synthetic data below 2% of the training set. Ablation Studies. Experiments on insertion direction, insertion distance, and asset source in Sec. 5.3 are conducted under the 1 training epoch and 512768 resolution setting. Method car truck bus trailer construction_vehicle pedestrian motorcycle bicycle traffic_cone barrier Real Ours 0.562 0.323 0.296 0.600 0.354 0. 0.067 0.106 0.111 0.135 0.422 0.468 0.377 0.402 0.371 0.411 0.569 0. 0.515 0.565 Table 6 AP comparison across different categories for Real and Ours (+420) at 1 training epoch."
        },
        {
            "title": "B Detailed AP Metrics",
            "content": "Tab. 3 compares detection metrics across different training epochs. Detailed per-class AP scores are provided in Tab. 6, 7, and 8."
        },
        {
            "title": "C Additional Experiments on Generation Quality",
            "content": "15 Method car truck bus trailer construction_vehicle pedestrian motorcycle bicycle traffic_cone barrier Real Ours 0.622 0.371 0.375 0.633 0.383 0.389 0.154 0.168 0.132 0.147 0.493 0.497 0.430 0. 0.400 0.434 0.651 0.647 0.589 0.604 Table 7 AP comparison across different categories for Real and Ours (+420) at 2 training epoch. Method car truck bus trailer construction_vehicle pedestrian motorcycle bicycle traffic_cone barrier Real Ours 0.627 0.362 0.367 0.639 0.377 0.408 0.154 0. 0.132 0.164 0.488 0.501 0.436 0.446 0.454 0.439 0.656 0.654 0.632 0. Table 8 AP comparison across different categories for Real and Ours (+420) at 3 training epoch. Controllability. The controllability of our approach is quantitatively evaluated using perception metrics from StreamPETR (Wang et al., 2023a). We first generate the entire nuScenes validation set with Dream4Drive, and then measure perception performance using pre-trained StreamPETR model. The relative metrics, compared to those obtained on real data, indicate how well the generated samples align with the original annotations. As shown in Tab. 9, Dream4Drive achieves relative performance of 82%, demonstrating precise control over foreground object locations. No cutout&mask No depth&normal No edge Table 11 Ablation study on 3D-aware guidance maps. 14.38 7.33 8.44 66.36 34.64 49."
        },
        {
            "title": "Method",
            "content": "FVD FID Generation Quality. As shown in Tab. 10, our method achieves FVD 31.84 and FID 5.80, outperforming layoutand 3D semantics-guided methods (Gao et al., 2023, 2024a; Li et al., 2024a; Ji et al., 2025). The results indicate improved motion consistency and preserved visual fidelity, confirming that our 3D-aware guidance maps effectively generate both foreground and background content. Notably, no post-processing or selective filtering was applied to the generated videos. Additional Ablation Studies. We perform ablation experiments on the components of the Multi-condition Fusion Adapter, with results presented in Tab. 11. The findings indicate that all components contribute significantly to overall performance."
        },
        {
            "title": "D Additional Visualization",
            "content": "We present more comparisons between the naive insert and ours in Fig. 9, 10, asset insertion videos across different categories in Fig. 11, 12, 13, 14, 15, and 16, where all inserted assets are highlighted with red bounding boxes. In addition, we showcase several corner-case examples, such as imminent collision and close-following scenarios, also illustrated in Fig. 17 and Fig. 18. Limitation and Future Works Although Dream4Drive is capable of inserting arbitrary assets into diverse scenes, automatically ensuring that the inserted trajectories remain within drivable areas and avoid collisions with pedestrians or other vehicles remains an open challenge. Addressing this issue would enable more flexible generation of diverse corner cases. Statement on LLM Usage During the preparation of this manuscript, large language models (LLMs) were used solely for language refinement. All scientific ideas, experiments, analyses, and conclusions are the authors original contributions."
        },
        {
            "title": "Method",
            "content": "mAP mATE mAOE mAVE NDS Real Gen-nuScenes 36.1 24.4 69.2 78.5 56.7 66. 28.5 34.9 47.9 39.1 (82%) Table 9 Domain gap. Comparison of detection performance on Real and Gen-nuScenes validation sets at 512768 resolution. Values are reported in percentage."
        },
        {
            "title": "Method",
            "content": "MagicDrive (Gao et al., 2023) Panacea (Wen et al., 2024) SubjectDrive (Huang et al., 2024a) DriveWM (Wang et al., 2024b) Delphi (Ma et al., 2024a) MagicDriveDiT (Gao et al., 2024a) DiVE (Jiang et al., 2024) UniScene (Li et al., 2024a) CoGen (Ji et al., 2025) DriveDream-2 (Zhao et al., 2025a)"
        },
        {
            "title": "Ours",
            "content": "12Hz 2Hz 2Hz 2Hz 2Hz 12Hz 12Hz 12Hz 12Hz 12Hz FPS Resolution 224400 256512 256512 192384 512512 224400 480854 256512 360640 512512 512768 12Hz FVD 218.12 139.00 124.00 122.70 113.50 94.84 94.60 70.52 68.43 55.70 FID 16.20 16.96 15.98 15.80 15.08 20.91 - 6.12 10.15 11.20 31.84 5.80 Table 10 Quantitative comparison on video generation quality with other methods. Our method achieves the best FVD and FID score. Figure 9 case study of contrast between reflection and shadow. Figure 10 case study of contrast between reflection and shadow. 17 Figure 11 Insertion of car in the right-side region. Figure 12 Insertion of truck in the left-side region. 18 Figure 13 Insertion of barrier in the left-side region. Figure 14 Insertion of bus in the back-side region. 19 Figure 15 Insertion of traffic cone in the left-side region. Figure 16 Insertion of construction vehicle in the back-side region. 20 Figure 17 Corner case. Insertion of barrier in the front-side region, where the ego vehicle is about to collide with it. Figure 18 Corner case. Insertion of truck in the close-range front-side region."
        }
    ],
    "affiliations": [
        "Huazhong University of Science and Technology",
        "Peking University",
        "Xiaomi EV"
    ]
}