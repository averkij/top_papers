{
    "paper_title": "TR2-D2: Tree Search Guided Trajectory-Aware Fine-Tuning for Discrete Diffusion",
    "authors": [
        "Sophia Tang",
        "Yuchen Zhu",
        "Molei Tao",
        "Pranam Chatterjee"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Reinforcement learning with stochastic optimal control offers a promising framework for diffusion fine-tuning, where a pre-trained diffusion model is optimized to generate paths that lead to a reward-tilted distribution. While these approaches enable optimization without access to explicit samples from the optimal distribution, they require training on rollouts under the current fine-tuned model, making them susceptible to reinforcing sub-optimal trajectories that yield poor rewards. To overcome this challenge, we introduce TRee Search Guided TRajectory-Aware Fine-Tuning for Discrete Diffusion (TR2-D2), a novel framework that optimizes reward-guided discrete diffusion trajectories with tree search to construct replay buffers for trajectory-aware fine-tuning. These buffers are generated using Monte Carlo Tree Search (MCTS) and subsequently used to fine-tune a pre-trained discrete diffusion model under a stochastic optimal control objective. We validate our framework on single- and multi-objective fine-tuning of biological sequence diffusion models, highlighting the overall effectiveness of TR2-D2 for reliable reward-guided fine-tuning in discrete sequence generation."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 9 2 ] . [ 1 1 7 1 5 2 . 9 0 5 2 : r Preprint. Under review. TR2-D2: TREE SEARCH GUIDED TRAJECTORY-AWARE FINE-TUNING FOR DISCRETE DIFFUSION Sophia Tang,1, Yuchen Zhu,2, Molei Tao,2, Pranam Chatterjee1,3, 1Department of Computer and Information Science, University of Pennsylvania 2School of Mathematics, Georgia Institute of Technology 3Department of Bioengineering, University of Pennsylvania These authors contributed equally Corresponding authors: mtao@gatech.edu and pranam@seas.upenn.edu"
        },
        {
            "title": "ABSTRACT",
            "content": "Reinforcement learning with stochastic optimal control offers promising framework for diffusion fine-tuning, where pre-trained diffusion model is optimized to generate paths that lead to reward-tilted distribution. While these approaches enable optimization without access to explicit samples from the optimal distribution, they require training on rollouts under the current fine-tuned model, making them susceptible to reinforcing sub-optimal trajectories that yield poor rewards. To overcome this challenge, we introduce TRee Search Guided TRajectory-Aware Fine-Tuning for Discrete Diffusion (TR2-D2), novel framework that optimizes reward-guided discrete diffusion trajectories with tree search to construct replay buffers for trajectory-aware fine-tuning. These buffers are generated using Monte Carlo Tree Search (MCTS) and subsequently used to fine-tune pre-trained discrete diffusion model under stochastic optimal control objective. We validate our framework on singleand multi-objective fine-tuning of biological sequence diffusion models, highlighting the overall effectiveness of TR2-D2 for reliable reward-guided fine-tuning in discrete sequence generation."
        },
        {
            "title": "INTRODUCTION",
            "content": "Diffusion generative models (Sohl-Dickstein et al., 2015; Song et al., 2020a; Ho et al., 2020) have led to significant advancements across continuous video and image generation, and more recently in discrete state spaces (Austin et al., 2021) for natural language (Sahoo et al., 2024; Nie et al., 2025; Khanna et al., 2025; Song et al., 2025; Team et al., 2023) and biomolecular sequence generation (Avdeyev et al., 2023; Alamdari et al., 2023; Hayes et al., 2025). Inference-time guidance and finetuning of diffusion models have enabled the repurposing of pre-trained diffusion models for highly specialized tasks, such as accurate text-to-image generation (Ruiz et al., 2023; Voynov et al., 2023) and design of biomolecules with therapeutic properties (Gruver et al., 2023; Tang et al., 2025; Wang et al., 2025). These methods aims to sample from the data distribution pdata tilted by reward function r(X), which amplifies the density of high-reward samples ptarget(X) pdata(X) exp(r(X)/α) and minimizes sub-optimal samples. While inference-time guidance avoids model training, it incurs increased inference costs due to reward evaluations and does not prevent the model from generating suboptimal samples, particularly in regions of high data density. Alternatively, fine-tuning is theoretically guaranteed to fit the reward-tilted distribution, which permanently modifies the models terminal distribution with inexpensive inference calls. An effective strategy for fine-tuning involves off-policy reinforcement learning (RL) (Bengio et al., 2021; Peng et al., 2019) that uses trajectories generated by reference policy models to inform the next update to the current policy. However, its effectiveness in practice is limited by the quality of the trajectories generated from the policy. This motivates advancements in optimizing diffusion trajectories, which have been explored in continuous state spaces with differentiable gradients along the diffusion trajectory (Tian et al., 2025), but remain challenging in discrete state spaces where 1 Preprint. Under review. gradients are undefined. To this end, we introduce TRee Search Guided TRajectory-Aware FineTuning for Discrete Diffusion (TR2-D2), which leverages tree search to generate reward-guided trajectories for off-policy RL for discrete diffusion fine-tuning. Contributions Our main contributions can be summarized as follows: (1) We develop general framework for enhancing off-policy RL techniques with search-optimized discrete diffusion trajectories (Sec 3). (2) We implement our framework to develop an efficient discrete diffusion fine-tuning strategy that leverages Monte-Carlo Tree Search (MCTS) to curate replay buffer of optimal trajectories for off-policy control-based RL (Sec 4). (3) We introduce the first method for multiobjective fine-tuning of discrete diffusion models by generating Pareto-optimal replay buffers for fine-tuning (Sec 5). (4) We demonstrate that TR2-D2 achieves state-of-the-art performance in discrete diffusion fine-tuning for regulatory DNA design optimized for enhancer activity (Sec 6.1) and multi-objective therapeutic peptide design (Sec 6.2). Related Works We provide comprehensive discussion of related works in App A."
        },
        {
            "title": "2 PRELIMINARIES",
            "content": "Continuous-Time Markov Chains continuous-time Markov chain (CTMC) defines stochastic process X0:T = (Xt)t[0,T ] over discrete state space = {1, . . . , D}. The evolution and law of CTMC is characterized by generator (Qt)t[0,T ] RX , defined by Qt(x, y) = lim t0 1 (Pr(Xt+t = yXt = x) 1x=y) whose value Qt(x, y) describes the transition rate from state to another state . We refer to Appendix B.1 for theoretical background of CTMCs and relevant stochastic calculus tools. Discrete Diffusion Models Discrete diffusion models are class of generative models that aim to learn the generator of CTMC, which starts from an easy-to-sample prior distribution pprior and arrives at the target distribution pdata in finite time. Discrete diffusion models consist of pair of noise-injection and generative denoising CTMCs, which are the time-reversal of each other. An effective formulation for discrete diffusion is the masked discrete diffusion model (MDM) (Sahoo et al., 2024; Shi et al., 2024; Ou et al., 2024; Zheng et al., 2024), where the prior distribution is chosen to be the Dirac distribution concentrated on sequence where all tokens are an absorbing mask token, denoted as . The forward process of MDM injects noise into the sequence by independently converting data tokens into the mask token following noise scheduler. The backward generative process reverses this process by starting from fully-masked sequence and iteratively decoding masks back to data tokens, following parameterized probability distribution conditioned on the previously unmasked tokens in the sequence. Let be partially masked sequence of tokens, UM = (X ℓ : ℓ = ) denotes the collection of non-mask token in X, and ℓd represents the sequence modified from by replacing the ℓ-th position with data token d, its proven in Ou et al. (2024) that the optimal generator of the generative process for MDM has the following special decomposition, Qt(x, y) = γ(t) Pr Xpdata (X ℓ = dX UM = xUM)1xℓ=d,y=xℓd (1) where γ(t) is noise schedule, x, L. Due to this special structure, MDM often adopts neural network puθ (x) RN to parametrize the unknown conditional data distribution, where the (ℓ, d)th entry of puθ (x) approximates PrXpdata(X ℓ = dX UM = xUM). MDMs are often trained by optimizing the denoising cross-entropy (DCE) loss (Ou et al., 2024; Sahoo et al., 2024; Shi et al., 2024), defined as Expdata[L(θ; x)], L(θ; x) := EλUnif(0,1) min θ (cid:34) 1 λ Eµλ( xx) (cid:88) ℓ: xℓ=M (cid:35) log puθ ( x)ℓ,xℓ (2) where µλ(x) is transition kernel that independently turns tokens in with probability λ. Preprint. Under review. Figure 1: Tree Search Guided Trajectory-Aware Fine-Tuning for Discrete Diffusion. Our framework has two key components: (1) tree search algorithm to generate replay buffer of diffusion trajectories optimized for one or more reward functions using the current policy and (2) an off-policy RL algorithm for discrete diffusion fine-tuning using the optimized replay buffer. Reinforcement Learning for Discrete Diffusion Models Although discrete diffusion models are capable of accurately capturing the distribution of training data, they often fail in specialized downstream tasks that aim to generate sequences that optimize custom reward functions. Reinforcement learning (RL) can be used to align the marginal of the pre-trained model with some desired terminal reward. Given pre-trained discrete diffusion model that can sample from pdata, and reward function r(X) : R, RL can be used to align the θ-parameterized policy model to the desired reward-tilted distribution by solving an entropy-regularized reward optimization problem (Uehara et al., 2024a). EX0:T Puθ (cid:2)r(XT )(cid:3) α KL(Puθ Ppre) max θ (3) where Puθ and Ppre correspond to the path measure of the CTMCs associated with the finetuned and pre-trained diffusion models, respectively, and α controls the strength of the KL-divergence regularization, with smaller α value indicating greater tolerance to deviation from the pre-trained model. Zhu et al. (2025c) shows that the fine-tuned model that optimally solves (3) produces the following path measure that reaches reward-tilted target distribution P(X0:T ) = Ppre(X0:T ) 1 exp (cid:19) (cid:18) r(XT ) α , (X) pdata(X) exp (cid:19) (cid:18) r(X) α =: ptarget(X) (4) Therefore, the finetuning process naturally connects to solving stochastic optimal control (SOC) problem for CTMC (Wang et al., 2025; Zhu et al., 2025c), and the reward optimization problem can be solved by matching the path measure Puθ produced by the finetuned policy to the optimal path measure through optimizing loss that takes the general form minθ F(P, Puθ ). Common choices for include log-variance (Nüsken & Richter, 2021), relative entropy (Wang et al., 2025; Zekri & Boullé, 2025; Cao et al., 2025), weighted denoising cross-entropy (Zhu et al., 2025c), among others. We refer to Appendix B.3 for detailed discussion of the connection between SOC and RL fine-tuning."
        },
        {
            "title": "3 ENHANCING REINFORCEMENT LEARNING WITH STRUCTURED SEARCH",
            "content": "The effectiveness of RL in optimizing customized reward functions is largely dependent on the ability of the pre-trained model to generate high-reward samples. The model can then learn to reinforce these \"positive\" samples through RL and iteratively improve the quality of the next generation round. However, for discrete state space = {1, . . . D}L with states and token positions, the search space contains DL possible sequences, which becomes intractably large even for modest values of and L. When generating highly structured discrete data, such as biological sequences that optimize some reward function, it is common for high-reward sequences to lie in 3 Preprint. Under review. Algorithm 1 Framework for Enhanced Reinforcement Learning with Structured Search 1: Input: pre-trained model ppre, finetune policy model puθ , reward function r, and off-policy RL algorithm while buffer is not full do 2: Initalize finetune policy model puθ = ppre 3: while not converged do 4: 5: 6: 7: 8: 9: 10: 11: end while Generate samples from current policy model puθ Select optimal samples that maximize the reward function and add to buffer Explore similar samples given previous selections using the Search algorithm end while Update θ using samples from using off-policy RL algorithm for multiple epochs Reset the buffer low-density regions of the search space that are rarely sampled by the pre-trained model (De Santi et al., 2025). To avoid sub-optimal RL rounds due to low-quality trajectories, supervised fine-tuning (SFT) is commonly used to warm up the model by adapting it to generate favorable sequences through training on curated, specially-designed datasets. For example, when training LLMs into powerful math reasoners, it is standard practice to perform SFT on math-domain-related datasets to produce some level of reasoning capability, and enhance it using RL approaches such as GRPO (Shao et al., 2024). For general downstream tasks, such as biological sequence optimization, labeled datasets for specialized tasks are sparse, making RL finetuning for these problems challenging. To address this challenge, we aim to provide the policy model with pseudo-warm-up that utilizes reward-guided inference time scaling techniques for the discrete diffusion fine-tuning. While random samples from the model are not guaranteed to have high rewards, optimal samples can often be obtained by scaling the inference budget and performing an extensive search of the sample space. Using structured search algorithms like Monte Carlo Tree Search (MCTS; Coulom (2006)) that discover and bias towards highly optimal regions of the sample space implicitly aligns with several optimization tasks where high-reward samples follow common structure or contain similar motifs. The sequences obtained from the search can approximately serve as specialized dataset that guides the discrete diffusion policy model to produce similar high-quality samples, accelerating RL training. To maximize the utility of the collection of sequences found through the search, we add them to replay buffer and adopt off-policy RL algorithms which amplify the signal of the highly optimal sequences found during the search by training repetitively on samples from over multiple iterations. This can amortize the inference computation cost incurred during buffer curation and further enhance the training efficiency. Furthermore, integrating structured search to curate the buffer leverages the ability of off-policy RL to memorize and reinforce buffer samples, improving the next round of buffer generation. We summarize the high-level idea of combining search algorithms with RL finetuning of discrete diffusion models in Algorithm 1. One major benefit is that the search and finetuning steps in this framework are decoupled, opening the design space to any pair of search and off-policy RL algorithms. We further discuss this unique characteristic in Appendix C.2. In Section 4, we provide specific implementation of this general framework, specifically tailored to fine-tuning of masked discrete diffusion models."
        },
        {
            "title": "4 TR2-D2: TREE SEARCH GUIDED TRAJECTORY-AWARE FINE-TUNING",
            "content": "To implement the framework discussed in Sec 3, we introduce TRee-Guided TRajectory Planning for Discrete Diffusion (TR2-D2) that integrates an off-policy RL-based fine-tuning algorithm coupled with Monte-Carlo Tree Search for reward-guided buffer generation. Notably, our implementation uses an off-policy RL algorithm with scalable objective function (Sec 4.1) and efficiently balances exploration and exploitation of arbitrary reward functions (Sec 4.2). 4 Preprint. Under review. Algorithm 2 TR2-D2: Tree Search Guided Trajectory-Aware Fine-Tuning for Discrete Diffusion 1: Input: pre-trained model ppre(X UM ), finetuned policy model puθ (X UM ), reward function : RK, number of finetuning epochs Nepoch, number of WDCE repeats see Alg 5 optimize replay buffer i=1 i=1 MCTS(ppre, puθ ) 2: for epoch in 1, . . . , Nepoch do 3: 4: 5: 6: 7: 8: 9: 10: end for {X i, u}B {X i, u}B for step in 1, . . . , Nstep do { i, u}BR Compute FWDCE from (7) with { i, u}BR i=1 Update θ with θFWDCE i=1 ResampleWithMask(B; R) end for"
        },
        {
            "title": "4.1 OFF-POLICY RL FOR MASKED DISCRETE DIFFUSION MODELS",
            "content": "To perform RL with discrete diffusion models for sampling from reward-tilted distribution ptarget(X) pdata(X) exp(r(X)/α), it suffices to find CTMC that produces path measure Puθ that matches to the optimal path measure as in (4) (Zhu et al., 2025c). In the case of fine-tuning masked discrete diffusion models (MDM), the optimal CTMC is fully characterized by the conditional probability p, defined as p(x)ℓ,d = Pr Xptarget (X ℓ = dX UM = xUM) (5) We note that the optimal solution stated in (5) shares similar form to the pre-trained MDM, that outputs the conditional distribution with respect to pdata as in (1) and can be learned with the denoising cross entropy objective in (2) when i.i.d. samples from pdata are available. Therefore, we can naively learn by minimizing the following loss, min θ Exptarget[L(θ; x)] (6) where L(θ; x) is the data-conditioned denoising cross entropy term defined in (2). In the case of diffusion fine-tuning, we lack access to i.i.d. samples from the desired distribution ptarget pdata exp(r(XT )/α), making (6) an intractable objective. Recently, MDNS (Zhu et al., 2025c) introduced the weighted denoising cross-entropy (WDCE), tractable implementation of (6) that leverages importance sampling over the space of trajectories to simulate ptarget. As shown in Appendix C.1, we can derive the WDCE objective by rewriting (6) using since its marginal at time is exactly ptarget, Eptarget(x)[L(θ; x)] = EX0:T [L(θ; XT )] = EX0:T Pv (cid:21) (cid:20) dP dPv (X0:T )L(θ; XT ) := FWDCE (7) where Pv is reference path measure that does not track the gradient with respect to θ, and dP dPv is the Radon-Nikodým (RN) derivative between the CTMC path measures and Pv, and can be interpreted as an importance weight that measures how closely the two path measures are aligned. We remark that the loss FWDCE is considered off-policy as the reference policy used to generate training samples does not need to be updated as the fine-tuned policy uθ is updated. In practice, we periodically align the reference policy with the current model policy uθ to control the variance of the importance weights for enhanced numerical stability. As derived in Appendix C.1, the RN derivative for MDM can be computed as, log dP dPv (X0:T ) = r(XT ) α + (cid:124) (cid:88) (cid:88) log t:Xs=Xt ℓ:Xℓ s=Xℓ (cid:123)(cid:122) :=W (X0:T ) ppre(X ℓ pv(X ℓ sX UM ) sX UM ) (cid:125) log (8) where the normalizing constant is approximated with EX0:T Pv exp(W v(X0:T )). In practice, we take the softmax over the importance weights in the batch, which approximates the expectation. Since this objective requires only the clean sequence XT and the corresponding log-RND weight v, we store the generated rollouts in the replay buffer in the form of pairs (XT , v). We use the notation to emphasize the off-policy nature of the loss function. In practice, we always choose the non-gradient tracking policy = := stopgrad(uθ) to generate the sample batch. 5 Preprint. Under review."
        },
        {
            "title": "4.2 STRUCTURED TREE SEARCH FOR BUFFER GENERATION",
            "content": "Given its success in inference-time guidance of MDMs (Tang et al., 2025), we leverage MonteCarlo Tree Search (MCTS) (Coulom, 2006) as the structured tree search algorithm used to optimize the buffer of unmasking trajectories for off-policy RL. The algorithm iterates over four steps (selection, expansion, rollout, and backpropagation), which effectively balances exploration of diverse unmasking steps and exploitation of optimal rollouts. , total reward R(X Initialization We define tree , where each node is represented by partially unmasked sequence s) that determines the potential of the node to generate highX reward sequence, the number of times the node was visited Nvisits(X s), and set of children nodes children(X s). Each node also stores the log-probability of sampling it given its parent Xt under the pre-trained model ppre(X ) where Xt = parent(X s). The tree is stored as set of nodes linked together by child and parent references. node is considered expandable if it has no child nodes and is not fully unmasked (i.e. = ). At initialization, the tree has single root node defined as the fully masked sequence X0 = [M ]L with the number of visits set to Nvisits(X0) = 1 and an empty set of child nodes. sX UM Selection Starting from the root node, we traverse the existing unmasking steps defined in the tree by selecting from the child nodes at each intermediate node. To do this, we define the selection reward which guides exploration as (Xt, s) = R(X s) Nvisits(X s) + puθ (X sXt) (cid:112)Nvisit(Xt) 1 + Nvisit(X s) (9) Then, child node is selected by sampling from the nodes with optimal selection rewards. In practice, we take the softmax over (Xt, s) for the top-k child nodes, where is tunable hyperparameter, to avoid the chance of selecting nodes with low rewards. s}M Expansion After reaching an expandable node at time t, we sample child sequences {X i=1 corresponding to the time = + by unmasking tokens using the condition probability of the current policy puθ . To ensure diversity in the samples, we perturb the predicted distribution with i.i.d. Gumbel noise before sampling each child sequence. SingleReverseStep (cid:0)log puθ (X UM where Gi log( log U)), Unif(0, 1) ) + Gi, t(cid:1) (10) s, we compute the log-probability of sampling the token under the For each expanded node pre-trained model and the current policy to get the log-RND weight of the step as UM UM ppre(X i,ℓ puθ (X i,ℓ log_rndi = log ppre(X puθ (X sX UM sX UM (cid:88) log ) ) = t ) ) (11) i,ℓ =X ℓ s, we iteratively unmask the remaining masked tokens for . At each step, we track the Rollout For each expanded node the remaining timesteps until reaching fully unmasked sequence running log-RND of the trajectory, which will be used in the training objective. ), t(cid:1) ) ) SingleReverseStep (cid:0)log puθ (X UM X UM (cid:88) UM ppre(X i,ℓ puθ (X i,ℓ 0:T ) uθ (X uθ (X 0:T ) + log (12) (13) xi,ℓ =xℓ After the sequence is fully unmasked, the final reward is added to the total log-RND of the trajectory r(X ) and the buffer is updated, such that it contains the top-B sequences (X i, u) with the highest reward with every iteration. Backpropagation For each newly expanded child node reward R(X ) and the number of visits to Nvisits(X rewards of the clean sequence generated at each child node r(X predecessor nodes. s) r(X s, we initialize the total reward with the s) 1. Then, we sum the terminal ) and update the total reward of all 6 Preprint. Under review. Compared to standard buffer generation, this method offers the following advantages: (1) highreward trajectories sampled are exploited, and (2) the log-probabilities of each node in the tree under the pre-trained model are pre-computed and remain unchanged during selection."
        },
        {
            "title": "5 MULTI-OBJECTIVE FINE-TUNING WITH TR2-D2",
            "content": "While several works have explored multi-objective guidance for test-time scaling of discrete diffusion (Gruver et al., 2023; Tang et al., 2025), multi-objective fine-tuning of discrete diffusion models remains largely unexplored. Since the Pareto optimal distribution is not known before training, multi-objective fine-tuning requires framework that efficiently moves toward the Pareto optimal distribution during the fine-tuning process without sacrificing performance in any one objective. Here, we extend our approach from Sec 4 to multiple reward functions. Pareto Optimization When optimizing multi-objective reward function = (r1, . . . , rK) : RK, their critical points often conflict, resulting in tradeoffs. Rather than single optimal reward value, there exists Pareto frontier denoted of reward vectors. Definition 5.1 (Pareto Frontier of Rewards). Given feasible solution space and set of k=1, the Pareto frontier is the set defined as rewards = (rk)K (cid:110) r(X ) (cid:12) (cid:12) T , T s.t. (cid:0)k : rj ri (cid:1) (cid:0)k : rj > ri = (cid:1)(cid:111) (14) where each reward r(X than or equal to it across all objectives and strictly better in at least one objective. ) is non-dominated, such that no other reward in the set is better In practice, multi-objective optimization algorithms typically search for finite approximation of the Pareto-frontier by sufficiently exploring the solution space and inserting items into if it is non-dominated by any existing solution in P. Multi-Objective Selection During the selection process, rather than selecting with scalar selection s) RK where the score, we compute vector of reward values for each objective (Xt, s) RK that measures the scalar reward in the first term of (9) is replaced with reward vector R(X estimated future reward of the selection step Xt s. sX children(Xt) s.t. (Xt, ) (Xt, select = (cid:8)X )(cid:9) (15) where indicates strictly better values across all objectives (Pareto dominance). Generating Buffer of Pareto-Optimal Sequences To decide when to update the buffer with newly generated trajectory (XT , u), we consider the Pareto-optimality of its reward vector r(XT ) RK. At each iteration of MCTS, we compare the rolled out sequences {X i=1 with the current buffer B, and add it to the buffer if it is non-dominated by the sequences in the buffer. }M (cid:110) T (cid:12) (cid:12) XT s.t. k, rk( XT ) rk(X ) k, rk( XT ) > rk(X ) (cid:111) (16) While the true Pareto frontier is intractable in practice, it holds that each iteration of the tree search moves the buffer set closer to the Pareto-optimal set. Proposition 5.1 (Pareto Optimization of Buffer). With each iteration of the search, the buffer approaches the Pareto front , where the hypervolume generated by the rewards in the set is maximized. The proof is provided in Appendix C.3. While this statement holds for any search algorithm that sufficiently explores the solution space and discovers ε-Pareto solutions with non-negative probability with each search iteration, MCTS efficiently balances tradeoffs between objectives by (1) exploiting Pareto-optimal sampling paths with rewards stored as vectors without scalarization and (2) further exploring Pareto-optimal nodes to ensure all tradeoffs are maximized. 7 Preprint. Under review. Table 1: Comparison of TR2-D2 for regulatory DNA generation optimized on enhancer activity. Metrics were computed for 640 sequences across 3 seeds, with standard deviations reported. Best values are bolded. Evaluation metrics are detailed in Appendix D. Pred-Activity (median; ) ATAC-Acc (%; ) 3-mer Corr () App-Log-Lik (median; ) 0.170.04 3.300.00 4.150.33 4.640.21 5.040.06 5.610.07 7.550.01 7.350.07 6.560.02 9.740.01 1.50.2 0.00.0 39.98.7 45.316.4 92.10.9 92.50.6 99.50.2 90.60.3 86.91.18 99.90.01 0.0610.034 0.0650.001 0.8400.045 0.8480.008 0.7460.001 0.8870.002 0.5000.004 0.4900. 0.9250.002 0.5480.001 2610.6 2660.6 2592.5 2571.5 2650.6 2640.6 243.80.5 239.91.4 259.40.2 271.80.1 Method pre-trained CG SMC TDS CFG DRAKES SEPO GLID2E TR2-D2 (α = 0.1) TR2-D2 (α = 0.001)"
        },
        {
            "title": "6 EXPERIMENTS",
            "content": "We evaluate TR2-D2 on several diffusion fine-tuning tasks for biological sequences. Specifically, we fine-tune pre-trained regulatory DNA sequence model to optimize enhancer activity (Sec 6.1) and peptide SMILES generator for multi-objective fine-tuning (Sec 6.2). 6.1 REGULATORY DNA SEQUENCE DESIGN Setup and Baselines We fine-tune the pre-trained DNA enhancer MDM trained on 700k HepG2 sequences with measured activity and use the reward oracles from Wang et al. (2025). We compare TR2-D2 against both discrete diffusion guidance and fine-tuning baselines. Guidance baselines include classifier guidance (CG) (Nisonoff et al., 2025), Sequential Monte Carlo with the pre-trained model as proposal (SMC) and with classifier guidance as proposal (TDS) (Wu et al., 2023), and classifier-free guidance (CFG) (Ho & Salimans, 2022). Fine-tuning baselines include DRAKES (Wang et al., 2025), which applies the Gumbel-Softmax trick for reward gradients, SEPO (Zekri & Boullé, 2025), which uses REINFORCE with importance sampling, and GLID2E (Cao et al., 2025), which imposes clipped likelihood constraint for gradient-free RL. We evaluate four metrics: (1) median predicted activity (Pred-Activity) by the evaluation oracle (Wang et al., 2025), (2) predicted chromatin accessibility (ATAC-Acc; %), (3) 3-mer Pearson correlation with the top 0.5% HepG2 sequences (3-mer Corr), and (4) log-likelihood under the pre-trained model (App-Log-Lik). Further details are provided in App D, with hyperparameters and ablations in App F. Results We demonstrate that TR2-D2 with α = 0.001 outperforms all benchmarks on predicted activity and chromatin accessibility, achieving median Pred-Activity of 9.78 compared to 7.64 of the closest benchmark and near-perfect ATAC-Acc score of 100%  (Table 1)  . Alongside the high reward, we maintain relatively high 3-mer correlation and log-likelihood, indicating that the generated sequences still resemble natural enhancers. Furthermore, we find that by increasing KL regularization with α = 0.1, we can achieve the highest 3-mer correlation to the top 0.1% sequences in the dataset with the highest HepG2 activity, while maintaining higher predicted activity and chromatin accessibility than DRAKES, with the second-highest 3-mer correlation  (Table 1)  . 6.2 MULTI-OBJECTIVE PEPTIDE SEQUENCE DESIGN In this experiment, we aim to fine-tune peptide MDM to optimize multiple therapeutic properties using the algorithm in Sec 5. Notably, we show that generation with one diffusion pass of the finetuned policy outperforms inference-time multi-objective guidance, marking significant advancement in multi-objective fine-tuning. Setup and Baselines We fine-tune the pre-trained peptide MDM from Tang et al. (2025), built on the MDLM framework (Sahoo et al., 2024) and trained on 11M peptide SMILES sequences. Multi-objective rewards are defined by the classifiers from Tang et al. (2025) for binding affinity, solubility, non-hemolysis, non-fouling, and membrane permeability. Binding affinity is optimized 8 Preprint. Under review. Figure 2: Peptide docking results and comparison of multi-objective fine-tuning with and without MCTS. (Top) Docked peptides to TfR, GLP-1R, and GLAST with docking scores () and polar contacts within 3.5 Å annotated. (Bottom) Average multi-reward values of 50 sequences sampled from the fine-tuned model after each fine-tuning epoch are plotted over total of 1000 epochs, and running average is shown with the smooth line. Table 2: Multi-objective peptide design results (full results in Table 4). All values are averaged over 100 generated peptides. Best values are bolded. Pre-trained indicates unconditional sampling with the pre-trained peptide SMILES model from PepTune (Tang et al., 2025). PepTune indicates samples from 100 iterations of inference-time Monte-Carlo Tree Guidance conditioned on all objectives. TR2-D2 indicates unconditional sampling after 1000 epochs of fine-tuning of the pre-trained model with our multi-objective fine-tuning approach. Target Protein Method Binding Affinity () Solubility () Non-hemolysis () Non-fouling () Permeability () TfR GLP-1R GLAST Pre-trained PepTune TR2-D2 (Ours) Pre-trained PepTune TR2-D2 (Ours) Pre-trained PepTune TR2-D2 (Ours) 8.0080.673 8.2160.703 10.0980.050 8.2330.367 8.4030.365 9.4260.035 7.8300.420 8.4000.353 9.7030. 0.7420.166 0.7890.144 0.8380.066 0.7420.166 0.7740.170 0.8410.043 0.7420.166 0.8150.139 0.8840.038 0.8740.063 0.9020.051 0.8960.012 0.8740.063 0.9070.057 0.8490.016 0.8740.063 0.9370.029 0.9300. 0.1020.083 0.1210.081 0.2710.038 0.1020.083 0.125.082 0.4990.037 0.1020.083 0.1370.086 0.3640.083 7.4700.120 7.3890.119 7.1680.024 7.4700.120 7.3880.128 7.2630.020 7.4700.120 7.3110.106 7.2380. for multiple therapeutically relevant targets described in App E. We compare the multi-objective rewards of generated sequences from the fine-tuned model against sequences from the unconditional pre-trained model and from inference-time multi-objective guidance with PepTune (Tang et al., 2025). Further experimental details are in App E. Results Compared to inference-time multi-objective guidance with PepTune (Tang et al., 2025), TR2-D2 consistently yields higher scores across nearly all properties for each protein target, requiring only single diffusion pass (Table 2 and 4). Furthermore, we demonstrate that using tree search in the buffer generation step significantly enhances performance across multiple rewards over fine-tuning iterations compared to optimizing the scalarized reward with just the off-policy fine-tuning strategy (Fig 2 and 4; Table 7). Finally, we highlight that TR2-D2 outperforms PepTune with only 200 epochs of fine-tuning, while minimizing the trade-off between achieving reward optimality and the diversity in the generated sequences observed with increasing fine-tuning iterations  (Table 4)  . We include detailed discussion of hyperparameters and ablations in App F. 9 Preprint. Under review."
        },
        {
            "title": "7 CONCLUSION",
            "content": "In this work, we introduce TRee Search Guided TRajectory-Aware Fine-Tuning for Discrete Diffusion (TR2-D2), general framework for enhancing the efficiency and reliability of RL with structured search. We apply this framework for discrete diffusion fine-tuning by curating buffer of optimized sequences with MCTS for off-policy RL, demonstrating success in single and multiobjective fine-tuning. Looking ahead, TR2-D2 can be applied to broader classes of biological sequences, such as full-length proteins and mRNA (Wang et al., 2024; Peng et al., 2025; Vincoff et al., 2025; Patel et al., 2025), where optimizing for multiple structural and functional constraints is essential. The framework also lends itself naturally to integration with high-throughput wet-lab pipelines (Zhao et al., 2024; 2025a;b), where experimentally validated feedback can be incorporated into the replay buffer to accelerate closed-loop sequence discovery. From theoretical perspective, TR2-D2 opens new directions for studying discrete stochastic optimal control, variance-reduction strategies for trajectory weighting, and multi-objective optimization under Pareto efficiency, offering deeper insight into how reinforcement learning and search interact with discrete diffusion processes. Together, these directions highlight TR2-D2 as both practical platform for therapeutic design and step toward broader theory of reward-guided discrete generative modeling."
        },
        {
            "title": "DECLARATIONS",
            "content": "Acknowledgements We thank Mark III Systems for providing database and hardware support that has contributed to the research reported within this manuscript. We further thank Yinuo Zhang for performing molecular docking on the generated peptides. Author Contributions S.T. and Y.Z. devised and developed model architectures and theoretical formulations. Y.Z. fine-tuned and benchmarked models for DNA enhancer design. S.T. fine-tuned and benchmarked models for peptide design. S.T. and Y.Z. drafted the manuscript and S.T. designed the figures. P.C. and M.T. supervised and directed the study, and reviewed and finalized the manuscript. Data and Materials Availability The codebase is freely accessible to the academic community at https://github.com/sophtang/TR2-D2 and at https://huggingface.co/ ChatterjeeLab/TR2-D2. Funding Statement This research was supported by NIH grant R35GM155282 to the lab of P.C., and NSF grants DMS-1847802, DMS-2513699, DOE Grant DE-NA0004261, and Richard Duke Fellowship to the group of M.T. Competing Interests P.C. is co-founder of Gameto, Inc., UbiquiTx, Inc., and Atom Bioworks, Inc., and advises companies involved in biologics development and cell engineering. P.C.s interests are reviewed and managed by the University of Pennsylvania in accordance with their conflict-ofinterest policies. The remaining authors declare no conflicts. Reproducibility Statement We have made significant efforts to ensure the reproducibility of our work. Complete experimental details are provided in Appendices and E, including dataset descriptions, model architectures, training procedures, and evaluation metrics. All hyperparameters used in our experiments are documented in Table 5 with detailed discussion and ablation studies in Appendix F. The complete algorithmic implementation is provided as pseudocode in Appendix G, including the main TR2-D2 algorithm (Algorithm 2), MCTS implementation (Algorithm 5), and all supporting functions. For the regulatory DNA experiments, we use pre-trained models from prior work, with clear references to enable replication. The peptide experiments use the pre-trained weights from the PepTune framework (Tang et al., 2025), which is provided in our codebase. We provide details on hardware specifications to facilitate the reproduction of our results. The code for our implementation is made publicly available. 10 Preprint. Under review."
        },
        {
            "title": "REFERENCES",
            "content": "Sarah Alamdari, Nitya Thakkar, Rianne Van Den Berg, Neil Tenenholtz, Bob Strome, Alan Moses, Alex Xijie Lu, Nicolo Fusi, Ava Pardis Amini, and Kevin Yang. Protein generation with evolutionary diffusion: sequence is all you need. BioRxiv, pp. 202309, 2023. Nasreen Alfaris, Stephanie Waldrop, Veronica Johnson, Brunna Boaventura, Karla Kendrick, and Fatima Cody Stanford. Glp-1 single, dual, and triple receptor agonists for treating type 2 diabetes and obesity: narrative review. EClinicalMedicine, 75, 2024. Sebastian Ament, Samuel Daulton, David Eriksson, Maximilian Balandat, and Eytan Bakshy. Unexpected improvements to expected improvement for bayesian optimization. Advances in Neural Information Processing Systems, 36:2057720612, 2023. Yashas Annadani, Syrine Belakaria, Stefano Ermon, Stefan Bauer, and Barbara Engelhardt. Preference-guided diffusion for multi-objective offline optimization. arXiv preprint arXiv:2503.17299, 2025. Marianne Arriola, Subham Sekhar Sahoo, Aaron Gokaslan, Zhihan Yang, Zhixuan Qi, Jiaqi Han, Justin Chiu, and Volodymyr Kuleshov. Block diffusion: Interpolating between autoregressive and diffusion language models. In The Thirteenth International Conference on Learning Representations, 2025. Jacob Austin, Daniel Johnson, Jonathan Ho, Daniel Tarlow, and Rianne Van Den Berg. Structured denoising diffusion models in discrete state-spaces. Advances in neural information processing systems, 34:1798117993, 2021. Pavel Avdeyev, Chenlai Shi, Yuhao Tan, Kseniia Dudnyk, and Jian Zhou. Dirichlet diffusion score model for biological sequence generation. In International Conference on Machine Learning, pp. 12761301. PMLR, 2023. Žiga Avsec, Vikram Agarwal, Daniel Visentin, Joseph Ledsam, Agnieszka Grabska-Barwinska, Kyle Taylor, Yannis Assael, John Jumper, Pushmeet Kohli, and David Kelley. Effective gene expression prediction from sequence by integrating long-range interactions. Nature methods, 18 (10):11961203, 2021. Jinbin Bai, Tian Ye, Wei Chow, Enxin Song, Qing-Guo Chen, Xiangtai Li, Zhen Dong, Lei Zhu, and Shuicheng YAN. Meissonic: Revitalizing masked generative transformers for efficient highIn The Thirteenth International Conference on Learning resolution text-to-image synthesis. Representations, 2025. Arpit Bansal, Hong-Min Chu, Avi Schwarzschild, Soumyadip Sengupta, Micah Goldblum, Jonas Geiping, and Tom Goldstein. Universal guidance for diffusion models. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pp. 843852, 2023. Syrine Belakaria, Aryan Deshwal, Nitthilan Kannappan Jayakodi, and Janardhan Rao Doppa. Uncertainty-aware search framework for multi-objective bayesian optimization. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 34, pp. 1004410052, 2020. Emmanuel Bengio, Moksh Jain, Maksym Korablyov, Doina Precup, and Yoshua Bengio. Flow network based generative models for non-iterative diverse candidate generation. Advances in neural information processing systems, 34:2738127394, 2021. Kevin Black, Michael Janner, Yilun Du, Ilya Kostrikov, and Sergey Levine. Training diffusion models with reinforcement learning. arXiv preprint arXiv:2305.13301, 2023. Denis Blessing, Julius Berner, Lorenz Richter, Carles Domingo-Enrich, Yuanqi Du, Arash Vahdat, and Gerhard Neumann. Trust region constrained measure transport in path space for stochastic optimal control and inference. arXiv preprint arXiv:2508.12511, 2025. Andrew Campbell, Joe Benton, Valentin De Bortoli, Thomas Rainforth, George Deligiannidis, and Arnaud Doucet. continuous time framework for discrete denoising models. Advances in Neural Information Processing Systems, 35:2826628279, 2022. 11 Preprint. Under review. Hanqun Cao, Haosen Shi, Chenyu Wang, Sinno Jialin Pan, and Pheng-Ann Heng. Glid2e: gradientfree lightweight fine-tune approach for discrete sequence design. In ICLR 2025 Workshop on Generative and Experimental Perspectives for Biomolecular Design, 2025. Yair Censor. Pareto optimality in multiobjective problems. Applied Mathematics and Optimization, 4 (1):4159, 1977. Sourav Chatterjee and Persi Diaconis. The sample size required in importance sampling. The Annals of Applied Probability, 28(2):10991135, 2018. Haoxuan Chen, Yinuo Ren, Martin Renqiang Min, Lexing Ying, and Zachary Izzo. Solving inverse problems via diffusion-based priors: An approximation-free ensemble sampling approach. arXiv preprint arXiv:2506.03979, 2025a. Tianqi Chen and Carlos Guestrin. Xgboost: scalable tree boosting system. In Proceedings of the 22nd acm sigkdd international conference on knowledge discovery and data mining, pp. 785794, 2016. Tong Chen, Yinuo Zhang, Sophia Tang, and Pranam Chatterjee. Multi-objective-guided discrete flow matching for controllable biological sequence design. arXiv preprint arXiv:2505.07086, 2025b. Kevin Clark, Paul Vicol, Kevin Swersky, and David Fleet. Directly fine-tuning diffusion models on differentiable rewards. arXiv preprint arXiv:2309.17400, 2023. Rémi Coulom. Efficient selectivity and backup operators in monte-carlo tree search. In International conference on computers and games, pp. 7283. Springer, 2006. Samuel Daulton, Maximilian Balandat, and Eytan Bakshy. Differentiable expected hypervolume improvement for parallel multi-objective bayesian optimization. Advances in neural information processing systems, 33:98519864, 2020. Samuel Daulton, David Eriksson, Maximilian Balandat, and Eytan Bakshy. Multi-objective bayesian optimization over high-dimensional search spaces. In Uncertainty in Artificial Intelligence, pp. 507517. PMLR, 2022. Riccardo De Santi, Marin Vlastelica, Ya-Ping Hsieh, Zebang Shen, Niao He, and Andreas Krause. Provable maximum entropy manifold exploration via diffusion models. arXiv preprint arXiv:2506.15385, 2025. Warren DeLano et al. Pymol: An open-source molecular graphics tool. CCP4 Newsl. protein crystallogr, 40(1):8292, 2002. Carles Domingo-Enrich, Michal Drozdzal, Brian Karrer, and Ricky TQ Chen. Adjoint matching: Fine-tuning flow and diffusion generative models with memoryless stochastic optimal control. arXiv preprint arXiv:2409.08861, 2024. Fergal Duffy, Mélanie Verniere, Marc Devocelle, Elise Bernard, Denis Shields, and Anthony Chubb. Cyclops: generating virtual libraries of cyclized and constrained peptides including nonnatural amino acids. Journal of chemical information and modeling, 51(4):829836, 2011. Jerome Eberhardt, Diogo Santos-Martins, Andreas Tillack, and Stefano Forli. Autodock vina 1.2. 0: new docking methods, expanded force field, and python bindings. Journal of chemical information and modeling, 61(8):38913898, 2021. Lawrence Eng. Glial fibrillary acidic protein (gfap): the major protein of glial intermediate filaments in differentiated astrocytes. Journal of neuroimmunology, 8:203214, 1985. Patrick Esser, Sumith Kulal, Andreas Blattmann, Rahim Entezari, Jonas Müller, Harry Saini, Yam Levi, Dominik Lorenz, Axel Sauer, Frederic Boesel, Dustin Podell, Tim Dockhorn, Zion English, and Robin Rombach. Scaling rectified flow transformers for high-resolution image synthesis. In Ruslan Salakhutdinov, Zico Kolter, Katherine Heller, Adrian Weller, Nuria Oliver, Jonathan Scarlett, and Felix Berkenkamp (eds.), Proceedings of the 41st International Conference on Machine Learning, volume 235 of Proceedings of Machine Learning Research, pp. 1260612633. PMLR, 2127 Jul 2024. Preprint. Under review. Ying Fan, Olivia Watkins, Yuqing Du, Hao Liu, Moonkyung Ryu, Craig Boutilier, Pieter Abbeel, Mohammad Ghavamzadeh, Kangwook Lee, and Kimin Lee. Dpok: Reinforcement learning for fine-tuning text-to-image diffusion models. Advances in Neural Information Processing Systems, 36:7985879885, 2023a. Ying Fan, Olivia Watkins, Yuqing Du, Hao Liu, Moonkyung Ryu, Craig Boutilier, Pieter Abbeel, Mohammad Ghavamzadeh, Kangwook Lee, and Kimin Lee. Reinforcement learning for fine-tuning text-to-image diffusion models. In Thirty-seventh Conference on Neural Information Processing Systems (NeurIPS) 2023. Neural Information Processing Systems Foundation, 2023b. Aaron Feller and Claus Wilke. Peptide-aware chemical language model successfully predicts membrane diffusion of cyclic peptides. Journal of Chemical Information and Modeling, 65(2): 571579, 2025. Daniel Fernández-Sánchez, Eduardo Garrido-Merchán, and Daniel Hernández-Lobato. Improved max-value entropy search for multi-objective bayesian optimization with constraints. arXiv preprint arXiv:2011.01150, 2020. Jenna Fromer and Connor Coley. Computer-aided multi-objective optimization in small molecule discovery. Patterns, 4(2), 2023. Anna Gaulton, Louisa Bellis, Patricia Bento, Jon Chambers, Mark Davies, Anne Hersey, Yvonne Light, Shaun McGlinchey, David Michalovich, Bissan Al-Lazikani, et al. Chembl: large-scale bioactivity database for drug discovery. Nucleic acids research, 40(D1):D1100D1107, 2012. Michael Gelbart, Jasper Snoek, and Ryan Adams. Bayesian optimization with unknown constraints. arXiv preprint arXiv:1403.5607, 2014. Shrey Goel, Vishrut Thoutam, Edgar Mariano Marroquin, Aaron Gokaslan, Arash Firouzbakht, Sophia Vincoff, Volodymyr Kuleshov, Huong T. Kratochvil, and Pranam Chatterjee. MeMDLM: De novo membrane protein design with property-guided discrete diffusion. In ICLR 2025 Workshop on Generative and Experimental Perspectives for Biomolecular Design, 2025. Shansan Gong, Ruixiang Zhang, Huangjie Zheng, Jiatao Gu, Navdeep Jaitly, Lingpeng Kong, and Yizhe Zhang. Diffucoder: Understanding and improving masked diffusion models for code generation. arXiv preprint arXiv:2506.20639, 2025. Sager Gosai, Rodrigo Castro, Natalia Fuentes, John Butts, Susan Kales, Ramil Noche, Kousuke Mouri, Pardis Sabeti, Steven Reilly, and Ryan Tewhey. Machine-guided design of synthetic cell type-specific cis-regulatory elements. bioRxiv, 2023. Nate Gruver, Samuel Stanton, Nathan Frey, Tim GJ Rudner, Isidro Hotzel, Julien Lafrance-Vanasse, Arvind Rajpal, Kyunghyun Cho, and Andrew Wilson. Protein design with guided discrete diffusion. Advances in neural information processing systems, 36:1248912517, 2023. Chakradhar Guntuboina, Adrita Das, Parisa Mollaei, Seongwon Kim, and Amir Barati Farimani. Peptidebert: language model based on transformers for peptide property prediction. The Journal of Physical Chemistry Letters, 14(46):1042710434, 2023. Wei Guo, Yuchen Zhu, Molei Tao, and Yongxin Chen. Plug-and-play controllable generation for discrete masked models. arXiv preprint arXiv:2410.02143, 2024. Shashank Gupta, Chaitanya Ahuja, Tsung-Yu Lin, Sreya Dutta Roy, Harrie Oosterhuis, Maarten de Rijke, and Satya Narayan Shukla. simple and effective reinforcement learning method for text-to-image diffusion fine-tuning. arXiv preprint arXiv:2503.00897, 2025. Emily Han, Sophia Tang, Dongyoon Kim, Amanda Murray, Kelsey Swingle, Alex Hamilton, Kaitlin Mrksich, Marshall Padilla, Rohan Palanki, Jacqueline Li, et al. Peptide-functionalized lipid nanoparticles for targeted systemic mrna delivery to the brain. Nano Letters, 25(2):800810, 2024a. Xu Han, Caihua Shan, Yifei Shen, Can Xu, Han Yang, Xiang Li, and Dongsheng Li. Trainingfree multi-objective diffusion model for 3d molecule generation. In The Twelfth International Conference on Learning Representations, 2023. 13 Preprint. Under review. Yinbin Han, Meisam Razaviyayn, and Renyuan Xu. Stochastic control for fine-tuning diffusion models: Optimality, regularity, and convergence. arXiv preprint arXiv:2412.18164, 2024b. Thomas Hayes, Roshan Rao, Halil Akin, Nicholas Sofroniew, Deniz Oktay, Zeming Lin, Robert Verkuil, Vincent Tran, Jonathan Deaton, Marius Wiggert, et al. Simulating 500 million years of evolution with language model. Science, 387(6736):850858, 2025. Daniel Hernández-Lobato, Jose Hernandez-Lobato, Amar Shah, and Ryan Adams. Predictive entropy search for multi-objective bayesian optimization. In International conference on machine learning, pp. 14921501. PMLR, 2016. Jonathan Ho and Tim Salimans. Classifier-free diffusion guidance. arXiv preprint arXiv:2207.12598, 2022. Jonathan Ho, Ajay Jain, and Pieter Abbeel. Denoising diffusion probabilistic models. Advances in neural information processing systems, 33:68406851, 2020. Moksh Jain, Sharath Chandra Raparthy, Alex Hernández-Garcıa, Jarrid Rector-Brooks, Yoshua Bengio, Santiago Miret, and Emmanuel Bengio. Multi-objective gflownets. In International conference on machine learning, pp. 1463114653. PMLR, 2023. Tushar Jain, Tingwan Sun, Stéphanie Durand, Amy Hall, Nga Rewa Houston, Juergen Nett, Beth Sharkey, Beata Bobrowicz, Isabelle Caffry, Yao Yu, et al. Biophysical properties of the clinicalstage antibody landscape. Proceedings of the National Academy of Sciences, 114(5):944949, 2017. Vineet Jain, Kusha Sareen, Mohammad Pedramfar, and Siamak Ravanbakhsh. Diffusion tree sampling: Scalable inference-time alignment of diffusion models. arXiv preprint arXiv:2506.20701, 2025. Stefan Janson, Daniel Merkle, and Martin Middendorf. Molecular docking with multi-objective particle swarm optimization. Applied Soft Computing, 8(1):666675, 2008. Wengong Jin, Regina Barzilay, and Tommi Jaakkola. Multi-objective molecule generation using interpretable substructures. In International conference on machine learning, pp. 48494859. PMLR, 2020. Samar Khanna, Siddhant Kharbanda, Shufan Li, Harshit Varma, Eric Wang, Sawyer Birnbaum, Ziyang Luo, Yanis Miraoui, Akash Palrecha, Stefano Ermon, et al. Mercury: Ultra-fast language models based on diffusion. arXiv preprint arXiv:2506.17298, 2025. Mina Konakovic Lukovic, Yunsheng Tian, and Wojciech Matusik. Diversity-guided multi-objective bayesian optimization with batch evaluations. Advances in Neural Information Processing Systems, 33:1770817720, 2020. Avantika Lal, David Garfield, Tommaso Biancalani, and Gokcen Eraslan. Designing realistic regulatory dna with autoregressive language models. Genome Research, 34(9):14111420, 2024. Diantong Li, Fengxue Zhang, Chong Liu, and Yuxin Chen. Constrained multi-objective bayesian optimization through optimistic constraints estimation. arXiv preprint arXiv:2411.03641, 2024a. Jianan Li, Keisuke Yanagisawa, Masatake Sugita, Takuya Fujie, Masahito Ohue, and Yutaka Akiyama. Cycpeptmpdb: comprehensive database of membrane permeability of cyclic peptides. Journal of Chemical Information and Modeling, 63(7):22402250, 2023. Xiner Li, Yulai Zhao, Chenyu Wang, Gabriele Scalia, Gokcen Eraslan, Surag Nair, Tommaso Biancalani, Shuiwang Ji, Aviv Regev, Sergey Levine, et al. Derivative-free guidance in continuous and discrete diffusion models with soft value-based decoding. arXiv preprint arXiv:2408.08252, 2024b. Xinhao Li and Denis Fourches. Smiles pair encoding: data-driven substructure tokenization algorithm for deep learning. Journal of chemical information and modeling, 61(4):15601569, 2021. 14 Preprint. Under review. Yanyan Li, Honghong Zhou, Xiaomin Chen, Yu Zheng, Quan Kang, Di Hao, Lili Zhang, Tingrui Song, Huaxia Luo, Yajing Hao, et al. Smprot: reliable repository with comprehensive annotation of small proteins identified from ribosome profiling. Genomics, proteomics & bioinformatics, 19 (4):602610, 2021. Zeming Lin, Halil Akin, Roshan Rao, Brian Hie, Zhongkai Zhu, Wenting Lu, Nikita Smetanin, Robert Verkuil, Ori Kabeli, Yaniv Shmueli, et al. Evolutionary-scale prediction of atomic-level protein structure with language model. Science, 379(6637):11231130, 2023. Jie Liu, Gongye Liu, Jiajun Liang, Yangguang Li, Jiaheng Liu, Xintao Wang, Pengfei Wan, Di Zhang, and Wanli Ouyang. Flow-grpo: Training flow matching models via online rl. arXiv preprint arXiv:2505.05470, 2025. Ilya Loshchilov and Frank Hutter. Decoupled weight decay regularization. arXiv preprint arXiv:1711.05101, 2017. Aaron Lou, Chenlin Meng, and Stefano Ermon. Discrete diffusion modeling by estimating the ratios of the data distribution. arXiv preprint arXiv:2310.16834, 2023. Haoming Lu, Hazarapet Tunanyan, Kai Wang, Shant Navasardyan, Zhangyang Wang, and Humphrey Shi. Specialist diffusion: Plug-and-play sample-efficient fine-tuning of text-to-image diffusion models to learn any unseen style. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 1426714276, 2023. Timothy Marler and Jasbir Arora. Survey of multi-objective optimization methods for engineering. Structural and multidisciplinary optimization, 26(6):369395, 2004. Garrett Morris, Ruth Huey, William Lindstrom, Michel Sanner, Richard Belew, David Goodsell, and Arthur Olson. Autodock4 and autodocktools4: Automated docking with selective receptor flexibility. Journal of computational chemistry, 30(16):27852791, 2009. Christos Nicolaou, Nathan Brown, and Constantinos Pattichis. Molecular optimization using computational multi-objective methods. Current Opinion in Drug Discovery and Development, 10 (3):316, 2007. Shen Nie, Fengqi Zhu, Zebin You, Xiaolu Zhang, Jingyang Ou, Jun Hu, Jun Zhou, Yankai Lin, JiRong Wen, and Chongxuan Li. Large language diffusion models. arXiv preprint arXiv:2502.09992, 2025. Hunter Nisonoff, Junhao Xiong, Stephan Allenspach, and Jennifer Listgarten. Unlocking guidance for discrete state-space diffusion and flow models. In The Thirteenth International Conference on Learning Representations, 2025. Nikolas Nüsken and Lorenz Richter. Solving high-dimensional hamiltonjacobibellman pdes using neural networks: perspectives from the theory of controlled diffusions and measures on path space. Partial differential equations and applications, 2(4):48, 2021. Jingyang Ou, Shen Nie, Kaiwen Xue, Fengqi Zhu, Jiacheng Sun, Zhenguo Li, and Chongxuan Li. Your absorbing discrete diffusion secretly models the conditional distributions of clean data. arXiv preprint arXiv:2406.03736, 2024. Edward Pajarillo, Asha Rizor, Jayden Lee, Michael Aschner, and Eunsook Lee. The role of astrocytic glutamate transporters glt-1 and glast in neurological disorders: Potential targets for neurotherapeutics. Neuropharmacology, 161:107559, 2019. Ji Won Park, Nataša Tagasovska, Michael Maser, Stephen Ra, and Kyunghyun Cho. Botied: Multiobjective bayesian optimization with tied multivariate ranks. arXiv preprint arXiv:2306.00344, 2023. Sawan Patel, Sophia Tang, Yinuo Zhang, Pranam Chatterjee, and Sherwood Yao. Multi-objectiveguided generative design of mRNA with therapeutic properties. In ICML 2025 Workshop on Scaling Up Intervention Models, 2025. Preprint. Under review. Fred Zhangzhi Peng, Zachary Bezemek, Sawan Patel, Jarrid Rector-Brooks, Sherwood Yao, Alexander Tong, and Pranam Chatterjee. Path planning for masked diffusion models with applications to biological sequence generation. In ICLR 2025 Workshop on Deep Generative Model in Machine Learning: Theory, Principle and Efficacy, 2025. Xue Bin Peng, Aviral Kumar, Grace Zhang, and Sergey Levine. Advantage-weighted regression: Simple and scalable off-policy reinforcement learning. arXiv preprint arXiv:1910.00177, 2019. Roy A. Quinlan, Michael Brenner, James E. Goldman, and Albee Messing. Gfap and its role in alexander disease. Experimental Cell Research, 313(10):20772087, June 2007. ISSN 0014-4827. doi: 10.1016/j.yexcr.2007.04.004. Jarrid Rector-Brooks, Mohsin Hasan, Zhangzhi Peng, Cheng-Hao Liu, Sarthak Mittal, Nouha Dziri, Michael M. Bronstein, Pranam Chatterjee, Alexander Tong, and Joey Bose. Steering masked discrete diffusion models via discrete denoising posterior prediction. In The Thirteenth International Conference on Learning Representations, 2025. Yinuo Ren, Tesi Xiao, Tanmay Gangwani, Anshuka Rangi, Holakou Rahmanian, Lexing Ying, and Subhajit Sanyal. Multi-objective optimization via wasserstein-fisher-rao gradient flow. In International Conference on Artificial Intelligence and Statistics, pp. 38623870. PMLR, 2024a. Yinuo Ren, Tesi Xiao, Michael Shavlovsky, Lexing Ying, and Holakou Rahmanian. Hyperdpo: Conditioned one-shot multi-objective fine-tuning framework. arXiv preprint arXiv:2410.08316, 2024b. Kevin Rojas, Ye He, Chieh-Hsin Lai, Yuta Takida, Yuki Mitsufuji, and Molei Tao. Theoryinformed improvements to classifier-free guidance for discrete diffusion models. arXiv preprint arXiv:2507.08965, 2025a. Kevin Rojas, Yuchen Zhu, Sichen Zhu, Felix X-F. Ye, and Molei Tao. Diffuse everything: Multimodal diffusion models on arbitrary state spaces. In Forty-second International Conference on Machine Learning, 2025b. URL https://openreview.net/forum?id=AjbiIcRt6q. Nataniel Ruiz, Yuanzhen Li, Varun Jampani, Yael Pritch, Michael Rubinstein, and Kfir Aberman. Dreambooth: Fine tuning text-to-image diffusion models for subject-driven generation. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pp. 2250022510, 2023. Subham Sahoo, Marianne Arriola, Yair Schiff, Aaron Gokaslan, Edgar Marroquin, Justin Chiu, Alexander Rush, and Volodymyr Kuleshov. Simple and effective masked diffusion language models. Advances in Neural Information Processing Systems, 37:130136130184, 2024. Yair Schiff, Subham Sekhar Sahoo, Hao Phung, Guanghan Wang, Sam Boshar, Hugo Dalla-torre, Bernardo de Almeida, Alexander Rush, Thomas PIERROT, and Volodymyr Kuleshov. Simple guidance mechanisms for discrete diffusion models. In The Thirteenth International Conference on Learning Representations, 2025. Zhihong Shao, Peiyi Wang, Qihao Zhu, Runxin Xu, Junxiao Song, Xiao Bi, Haowei Zhang, Mingchuan Zhang, YK Li, Yang Wu, et al. Deepseekmath: Pushing the limits of mathematical reasoning in open language models. arXiv preprint arXiv:2402.03300, 2024. Jiaxin Shi, Kehang Han, Zhe Wang, Arnaud Doucet, and Michalis Titsias. Simplified and generalized masked diffusion for discrete data. Advances in neural information processing systems, 37: 103131103167, 2024. Qingyu Shi, Jinbin Bai, Zhuoran Zhao, Wenhao Chai, Kaidong Yu, Jianzong Wu, Shuangyong Song, Yunhai Tong, Xiangtai Li, Xuelong Li, et al. Muddit: Liberating generation beyond text-to-image with unified discrete diffusion model. arXiv preprint arXiv:2505.23606, 2025. Samradhi Singh, Namrata Pal, Swasti Shubham, Devojit Kumar Sarma, Vinod Verma, Francesco Marotta, and Manoj Kumar. Polycystic ovary syndrome: etiology, current management, and future therapeutics. Journal of clinical medicine, 12(4):1454, 2023. 16 Preprint. Under review. Raghav Singhal, Zachary Horvitz, Ryan Teehan, Mengye Ren, Zhou Yu, Kathleen McKeown, and Rajesh Ranganath. general framework for inference-time scaling and steering of diffusion models. arXiv preprint arXiv:2501.06848, 2025. Marta Skreta, Tara Akhound-Sadegh, Viktor Ohanesian, Roberto Bondesan, Alán Aspuru-Guzik, Arnaud Doucet, Rob Brekelmans, Alexander Tong, and Kirill Neklyudov. Feynman-kac correctors in diffusion: Annealing, guidance, and product of experts. arXiv preprint arXiv:2503.02819, 2025. Jascha Sohl-Dickstein, Eric Weiss, Niru Maheswaranathan, and Surya Ganguli. Deep unsupervised learning using nonequilibrium thermodynamics. In International conference on machine learning, pp. 22562265. pmlr, 2015. Jiaming Song, Chenlin Meng, and Stefano Ermon. Denoising diffusion implicit models. arXiv preprint arXiv:2010.02502, 2020a. Yang Song, Jascha Sohl-Dickstein, Diederik Kingma, Abhishek Kumar, Stefano Ermon, and Ben Poole. Score-based generative modeling through stochastic differential equations. arXiv preprint arXiv:2011.13456, 2020b. Yuxuan Song, Zheng Zhang, Cheng Luo, Pengyang Gao, Fan Xia, Hao Luo, Zheng Li, Yuehang Yang, Hongli Yu, Xingwei Qu, et al. Seed diffusion: large-scale diffusion language model with high-speed inference. arXiv preprint arXiv:2508.02193, 2025. Hannes Stark, Bowen Jing, Chenyu Wang, Gabriele Corso, Bonnie Berger, Regina Barzilay, and Tommi Jaakkola. Dirichlet flow matching with applications to dna sequence design. arXiv preprint arXiv:2402.05841, 2024. Jianlin Su, Murtadha Ahmed, Yu Lu, Shengfeng Pan, Wen Bo, and Yunfeng Liu. Roformer: Enhanced transformer with rotary position embedding. Neurocomputing, 568:127063, 2024. Xingyu Su, Xiner Li, Masatoshi Uehara, Sunwoo Kim, Yulai Zhao, Gabriele Scalia, Ehsan Hajiramezanali, Tommaso Biancalani, Degui Zhi, and Shuiwang Ji. Iterative distillation for rewardguided fine-tuning of diffusion models in biomolecular design. arXiv preprint arXiv:2507.00445, 2025. Mengying Sun, Jing Xing, Han Meng, Huijun Wang, Bin Chen, and Jiayu Zhou. Molsearch: searchbased multi-objective molecular generation and property optimization. In Proceedings of the 28th ACM SIGKDD conference on knowledge discovery and data mining, pp. 47244732, 2022. Shinya Suzuki, Shion Takeno, Tomoyuki Tamura, Kazuki Shitara, and Masayuki Karasuyama. Multiobjective bayesian optimization using pareto-frontier entropy. In International conference on machine learning, pp. 92799288. PMLR, 2020. Nataša Tagasovska, Nathan Frey, Andreas Loukas, Isidro Hötzel, Julien Lafrance-Vanasse, Ryan Lewis Kelly, Yan Wu, Arvind Rajpal, Richard Bonneau, Kyunghyun Cho, et al. paretooptimal compositional energy-based model for sampling and optimization of protein sequences. arXiv preprint arXiv:2210.10838, 2022. Sophia Tang, Yinuo Zhang, and Pranam Chatterjee. Peptune: De novo generation of therapeutic peptides with multi-objective-guided discrete diffusion. 42nd International Conference of Machine Learning (ICML 2025), 2025. Wenpin Tang. Fine-tuning of diffusion models via stochastic control: entropy regularization and beyond. arXiv preprint arXiv:2403.06279, 2024. Gemini Team, Rohan Anil, Sebastian Borgeaud, Jean-Baptiste Alayrac, Jiahui Yu, Radu Soricut, Johan Schalkwyk, Andrew Dai, Anja Hauth, Katie Millican, et al. Gemini: family of highly capable multimodal models. arXiv preprint arXiv:2312.11805, 2023. Ye Tian, Ling Yang, Xinchen Zhang, Yunhai Tong, Mengdi Wang, and Bin Cui. Diffusionsharpening: Fine-tuning diffusion models with denoising trajectory sharpening. arXiv preprint arXiv:2502.12146, 2025. 17 Preprint. Under review. Masatoshi Uehara, Yulai Zhao, Tommaso Biancalani, and Sergey Levine. Understanding reinforcement learning-based fine-tuning of diffusion models: tutorial and review. arXiv preprint arXiv:2407.13734, 2024a. Masatoshi Uehara, Yulai Zhao, Kevin Black, Ehsan Hajiramezanali, Gabriele Scalia, Nathaniel Lee Diamant, Alex Tseng, Tommaso Biancalani, and Sergey Levine. Fine-tuning of continuous-time diffusion models as entropy-regularized control. arXiv preprint arXiv:2402.15194, 2024b. Masatoshi Uehara, Yulai Zhao, Chenyu Wang, Xiner Li, Aviv Regev, Sergey Levine, and Tommaso Biancalani. Reward-guided controlled generation for inference-time alignment in diffusion models: Tutorial and review. arXiv preprint arXiv:2501.09685, 2025. Sophia Vincoff, Oscar Davis, Ismail Ilkan Ceylan, Alexander Tong, Joey Bose, and Pranam Chatterjee. SOAPIA: Siamese-guided generation of off target-avoiding protein interactions with high target affinity. In ICML 2025 Workshop on Scaling Up Intervention Models, 2025. Andrey Voynov, Kfir Aberman, and Daniel Cohen-Or. Sketch-guided text-to-image diffusion models. In ACM SIGGRAPH 2023 conference proceedings, pp. 111, 2023. Bram Wallace, Meihua Dang, Rafael Rafailov, Linqi Zhou, Aaron Lou, Senthil Purushwalkam, Stefano Ermon, Caiming Xiong, Shafiq Joty, and Nikhil Naik. Diffusion model alignment using direct preference optimization. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pp. 82288238, June 2024. Chenyu Wang, Masatoshi Uehara, Yichun He, Amy Wang, Avantika Lal, Tommi Jaakkola, Sergey Levine, Aviv Regev, Hanchen, and Tommaso Biancalani. Fine-tuning discrete diffusion models via reward optimization with applications to DNA and protein design. In The Thirteenth International Conference on Learning Representations, 2025. Shuzhe Wang, Jagna Witek, Gregory Landrum, and Sereina Riniker. Improving conformer generation for small rings and macrocycles based on distance geometry and experimental torsionalangle preferences. Journal of chemical information and modeling, 60(4):20442058, 2020. Xinyou Wang, Zaixiang Zheng, Fei YE, Dongyu Xue, Shujian Huang, and Quanquan Gu. Diffusion language models are versatile protein learners. In Forty-first International Conference on Machine Learning, 2024. Zi Wang and Stefanie Jegelka. Max-value entropy search for efficient bayesian optimization. In International conference on machine learning, pp. 36273635. PMLR, 2017. David Weininger. Smiles, chemical language and information system. 1. introduction to methodology and encoding rules. Journal of chemical information and computer sciences, 28(1):3136, 1988. Robin Winter, Floriane Montanari, Andreas Steffen, Hans Briem, Frank Noé, and Djork-Arné Clevert. Efficient multi-objective molecular optimization in continuous latent space. Chemical science, 10(34):80168024, 2019. Luhuan Wu, Brian Trippe, Christian Naesseth, David Blei, and John Cunningham. Practical and asymptotically exact conditional sampling in diffusion models. Advances in Neural Information Processing Systems, 36:3137231403, 2023. Yutong Xie, Chence Shi, Hao Zhou, Yuwei Yang, Weinan Zhang, Yong Yu, and Lei Li. Mars: Markov molecular sampling for multi-objective drug discovery. arXiv preprint arXiv:2103.10432, 2021. Kaifeng Yang, Michael Emmerich, André Deutz, and Thomas Bäck. Efficient computation of expected hypervolume improvement using box decomposition algorithms. Journal of Global Optimization, 75(1):334, 2019. Yinghua Yao, Yuangang Pan, Jing Li, Ivor Tsang, and Xin Yao. Proud: Pareto-guided diffusion model for multi-objective generation. Machine Learning, 113(9):65116538, 2024. Preprint. Under review. Huizhuo Yuan, Zixiang Chen, Kaixuan Ji, and Quanquan Gu. Self-play fine-tuning of diffusion models for text-to-image generation. Advances in Neural Information Processing Systems, 37: 7336673398, 2024a. Ye Yuan, Can Chen, Christopher Pal, and Xue Liu. Paretoflow: Guided flows in multi-objective optimization. arXiv preprint arXiv:2412.03718, 2024b. Yifu Yuan, Zhenrui Zheng, Zibin Dong, and Jianye Hao. Moduli: Unlocking preference generalization via diffusion models for offline multi-objective reinforcement learning. arXiv preprint arXiv:2408.15501, 2024c. Oussama Zekri and Nicolas Boullé. Fine-tuning discrete diffusion models with policy gradient methods. arXiv preprint arXiv:2502.01384, 2025. Ruochi Zhang, Haoran Wu, Yuting Xiu, Kewei Li, Ningning Chen, Yu Wang, Yan Wang, Xin Gao, and Fengfeng Zhou. Pepland: large-scale pre-trained peptide representation model for comprehensive landscape of both canonical and non-canonical amino acids. arXiv preprint arXiv:2311.04419, 2023. Xiangcheng Zhang, Haowei Lin, Haotian Ye, James Zou, Jianzhu Ma, Yitao Liang, and Yilun arXiv preprint Inference-time scaling of diffusion models through classical search. Du. arXiv:2505.23614, 2025a. Yaoxiang Zhang, Shuang Wang, Junteng Ma, Ze Zhang, and Tao Song. Pmodiff: Physics-informed multi-objective optimization diffusion model for protein-specific 3d molecule generation. Journal of Chemical Information and Modeling, 65(11):58115822, 2025b. Lin Zhao, Aditya Mohan, Anoop P. Patel, and Pranam Chatterjee. high-throughput human display screen to identify target-specific binder proteins via chimeric antigen receptors. In ICLR 2024 Workshop on Generative and Experimental Perspectives for Biomolecular Design, 2024. Lin Zhao, Aastha Pal, Tong Chen, and Pranam Chatterjee. mammalian high-throughput assay to screen AI-designed protein degraders. In ICLR 2025 Workshop on Generative and Experimental Perspectives for Biomolecular Design, 2025a. Lin Zhao, Aastha Pal, Tong Chen, and Pranam Chatterjee. High-throughput protein perturbation screens with AI-designed degraders. In NeurIPS 2025 Workshop on AI Virtual Cells and Instruments: New Era in Drug Discovery and Development, 2025b. Siyan Zhao, Devaansh Gupta, Qinqing Zheng, and Aditya Grover. d1: Scaling reasoning in diffusion large language models via reinforcement learning. arXiv preprint arXiv:2504.12216, 2025c. Kaiwen Zheng, Yongxin Chen, Hanzi Mao, Ming-Yu Liu, Jun Zhu, and Qinsheng Zhang. Masked diffusion models are secretly time-agnostic masked models and exploit inaccurate categorical sampling. arXiv preprint arXiv:2409.02908, 2024. Kaiwen Zheng, Huayu Chen, Haotian Ye, Haoxiang Wang, Qinsheng Zhang, Kai Jiang, Hang Su, Stefano Ermon, Jun Zhu, and Ming-Yu Liu. Diffusionnft: Online diffusion reinforcement with forward process. arXiv preprint arXiv:2509.16117, 2025a. Kaiwen Zheng, Yongxin Chen, Huayu Chen, Guande He, Ming-Yu Liu, Jun Zhu, and Qinsheng Zhang. Direct discriminative optimization: Your likelihood-based visual generative model is secretly gan discriminator. arXiv preprint arXiv:2503.01103, 2025b. Sichen Zhu, Yuchen Zhu, Molei Tao, and Peng Qiu. Diffusion generative modeling for spatially In The Thirteenth International resolved gene expression inference from histology images. Conference on Learning Representations, 2025a. URL https://openreview.net/forum? id=FtjLUHyZAO. Yiheng Zhu, Jialu Wu, Chaowen Hu, Jiahuan Yan, Tingjun Hou, Jian Wu, et al. Sample-efficient multi-objective molecular optimization with gflownets. Advances in Neural Information Processing Systems, 36:7966779684, 2023. 19 Preprint. Under review. Yuchen Zhu, Tianrong Chen, Lingkai Kong, Evangelos Theodorou, and Molei Tao. Trivialized momentum facilitates diffusion generative modeling on lie groups. In The Thirteenth International Conference on Learning Representations, 2025b. URL https://openreview.net/forum? id=DTatjJTDl1. Yuchen Zhu, Wei Guo, Jaemoo Choi, Guan-Horng Liu, Yongxin Chen, and Molei Tao. Mdns: Masked diffusion neural sampler via stochastic optimal control. arXiv preprint arXiv:2508.10684, 2025c. Marcela Zuluaga, Andreas Krause, and Markus Püschel. e-pal: An active learning approach to the multi-objective optimization problem. Journal of Machine Learning Research, 17(104):132, 2016. 20 Preprint. Under review."
        },
        {
            "title": "OVERVIEW OF APPENDIX",
            "content": "In App A, we present detailed discussion of related works in diffusion fine-tuning, discrete diffusion, inference-time scaling of diffusion models, and multi-objective optimization. In App B, we provide the theoretical foundation of our work. In App C, we present the theoretical proofs and justifications for Sec 4. The experiment details for enhancer DNA generation are given in App and experiment details for multi-objective peptide generation are given in App E. We include discussion on hyperparameters and present ablation results for the enhancer DNA and peptide experiments in App F. Finally, the pseudo-code for our algorithms are given in App G. Notation We denote the discrete state space of sequences of length with vocabulary of size as {1, . . . , D}L where probability distribution for single token is on the (D 1)-dimensional simplex D1. We denote path measure as with the pre-trained path measure as Ppre, the path measure produced by the fine-tuned policy as Qu as Pu, and the path measure produced by the optimal generator as P. We denote sequence at time in the diffusion process as Xt and the following step at time = + as Xs and trajectory of as X0:T := (Xt)t[0,T ]. We index each token in the sequence with ℓ {1, . . . , L} and denote an update of the masked state with the token at position ℓ as ℓ . We further consider tree of unmasking trajectories denoted , where each node is defined as partially masked sequence. Given node Xt in the tree, we denote the s}M unmasking steps derived from Xt as {X i=1. We denote the conditional probability distribution D1 under the pre-trained model as ppre( x)ℓ,X ℓ of the token ℓ given the unmasked tokens UM and puθ ( x)ℓ,X ℓ D1 under the current policy. = xℓ T"
        },
        {
            "title": "A RELATED WORKS",
            "content": "Fine-Tuning Diffusion Models with Reinforcement Learning RL fine-tuning of diffusion has been used to train the model to generate data samples that optimize reward function (Black et al., 2023; Wallace et al., 2024; Domingo-Enrich et al., 2024; Uehara et al., 2024a; Fan et al., 2023a; Clark et al., 2023; Blessing et al., 2025). Specifically, fine-tuning has been widely explored for text-to-image generation (Lu et al., 2023; Ruiz et al., 2023; Gupta et al., 2025; Yuan et al., 2024a; Fan et al., 2023b; Liu et al., 2025; Zheng et al., 2025a) and biomolecular sequence design (Wang et al., 2025; Zekri & Boullé, 2025; Cao et al., 2025). The fine-tuning problem has commonly been framed as an entropy-regularized control problem (Uehara et al., 2024b; Han et al., 2024b; Tang, 2024; Zhu et al., 2025c), which seeks to find an optimal sampling trajectory that maximizes some terminal reward. Fine-tuning methods have also been developed specifically for discrete diffusion, with approaches that optimize differentiable rewards (Wang et al., 2025), non-differentiable rewards (Zekri & Boullé, 2025; Cao et al., 2025; Su et al., 2025; Zhu et al., 2025c), and those tailored to diffusion language models (Zhao et al., 2025c; Gong et al., 2025). Discrete Diffusion Models Diffusion models have achieved state-of-the-art performance on generating various data modalities (Zhu et al., 2025b; Esser et al., 2024; Zhu et al., 2025a; Rojas et al., 2025b; Zheng et al., 2025b). Discrete diffusion models (Austin et al., 2021; Campbell et al., 2022; Lou et al., 2023), as natural generalization of diffusion models to finite state space, have emerged as powerful generative frameworks for sequence data, among which the most effective variant is Masked discrete diffusion models (MDM) (Sahoo et al., 2024; Wang et al., 2024; Shi et al., 2024; Peng et al., 2025; Tang et al., 2025; Nisonoff et al., 2025; Rector-Brooks et al., 2025; Bai et al., 2025; Shi et al., 2025). These models operate by progressively denoising masked inputs, enabling them to capture long-range dependencies without relying on autoregressive factorization. Within biology, masked discrete diffusion models have been successfully applied to peptide (Tang et al., 2025; Vincoff et al., 2025), protein (Wang et al., 2024; Goel et al., 2025; Nisonoff et al., 2025; Rector-Brooks et al., 2025; Wang et al., 2025), and nucleic acid design (Wang et al., 2025; Patel et al., 2025). Furthermore, recent extensions have introduced blockwise discrete diffusion architectures that interpolate between autoregressive and diffusion models to improve training efficiency and sequence length generalization (Arriola et al., 2025), as well as simplified formulations of masked diffusion that provide tighter likelihood bounds and more effective training objectives (Schiff et al., 2025). 21 Preprint. Under review. Inference-Time Scaling of Diffusion Models Inference-time scaling of diffusion models aims to efficiently leverage additional compute during sampling to improve output quality and controllability. One line of work steers continuous diffusion processes using FeynmanKac guidance, which is theoretically guaranteed to sample from reward-tilted distribution by reweighting trajectories at each denoising step (Skreta et al., 2025; Singhal et al., 2025; Chen et al., 2025a). Search-based approaches apply combinatorial optimization over diffusion trajectories to identify high-reward sequences (Sun et al., 2022), while reward-gradient methods adapt score-function estimators to steer sampling (Song et al., 2020b; Bansal et al., 2023). Importance sampling techniques can also be used to bias toward rare high-reward generations, but require large sample sizes to ensure coverage (Chatterjee & Diaconis, 2018). Soft value-based decoding has been proposed as derivative-free approach for steering both continuous and discrete diffusion processes (Li et al., 2024b). More recently, classical search methods have been incorporated into continuous diffusion sampling as scaling technique during inference time Jain et al. (2025); Zhang et al. (2025a). Classifier-based and classifier-free guidance methods have been adapted from continuous diffusion into the discrete domain (Nisonoff et al., 2025; Rector-Brooks et al., 2025; Wang et al., 2024; Schiff et al., 2025; Rojas et al., 2025a; Guo et al., 2024). Recent strategies for post-hoc optimization include classifier-free guidance (CFG) (Ho & Salimans, 2022), LaMBO-2 and NOS guidance (Gruver et al., 2023), and MCTS-guided sampling as in PepTune (Tang et al., 2025) and SOAPIA (Vincoff et al., 2025), which adapt pretrained models to specific objectives strictly at inference time. Multi-Objective Optimization Optimizing multiple, potentially conflicting, reward and constraint functions while balancing tradeoffs has significant applications across engineering and biology applications (Marler & Arora, 2004; Jain et al., 2017; Tagasovska et al., 2022; Zhu et al., 2023; Janson et al., 2008). For molecular drug design, the objectives include affinity to the drug target, bioavailability, potency, solubility for efficient drug loading, non-toxicity, synthesizability, among others (Nicolaou et al., 2007; Fromer & Coley, 2023; Sun et al., 2022; Winter et al., 2019; Jin et al., 2020; Xie et al., 2021). Due to tradeoffs between objectives, there often does not exist single solution that dominates across all objectives, but rather set of optimal solutions where no objective can be improved without sacrificing another objective (Censor, 1977). To reduce the multi-objective problem into more tractable single-objective problem, hypervolume (HV) has been used to quantify the optimality of solution with respect to reference point (Yang et al., 2019; Daulton et al., 2020; Ament et al., 2023; Daulton et al., 2022; Konakovic Lukovic et al., 2020). To sample from the Pareto-frontier, several approaches have been proposed, including active learning (Zuluaga et al., 2016; Belakaria et al., 2020), entropy-based multi-objective Bayesian optimization (Wang & Jegelka, 2017; Suzuki et al., 2020; Hernández-Lobato et al., 2016; Fernández-Sánchez et al., 2020), cumulative distribution function optimization (Park et al., 2023), and constrained multi-objective optimization (Gelbart et al., 2014; Li et al., 2024a). More recently, multi-objective guidance frameworks have been used to steer generative models like LLM (Ren et al., 2024b;a), diffusion (Gruver et al., 2023; Yao et al., 2024; Han et al., 2023; Yuan et al., 2024c; Annadani et al., 2025; Zhang et al., 2025b), discrete diffusion (Tang et al., 2025), and flow matching (Jain et al., 2023; Yuan et al., 2024b; Chen et al., 2025b) toward optimizing multiple objectives."
        },
        {
            "title": "B EXTENDED THEORETICAL BACKGROUND",
            "content": "In this section, we provide relevant theoretical backgrounds that connect the RL fine-tuning of discrete diffusion models with the stochastic optimal control of CTMCs. Some relevant results are first proved in Uehara et al. (2024b); Wang et al. (2025); Zhu et al. (2025c); we include the proof there for self-consistency and more coherent reading experience. B.1 CONTINUOUS-TIME MARKOV CHAINS (CTMCS) Here, we derive the RND of two CTMCs, which will be used to define our fine-tuning objective. Throughout the theoretical proofs, we will use subscript to be the instantaneous timestep following discrete jump of the CTMC at time t, P0 and Q0 to denote the reference path measure and corresponding generator, which are the same as the path measure Ppre and generator Qpre of the pre-trained diffusion model in the case of fine-tuning. 22 Preprint. Under review. Lemma 1 (Kolmogorov Forward Equation). The forward-time dynamics of the probability distribution pt() = Pr(Xt = ) of CTMC X0:T with generator Qt satisfies the Kolmogorov forward equation: x, tpt(x) = (cid:88) Qt(y, x)pt(y) = (cid:88) (Qt(y, x)pt(y) Qt(x, y)pt(x)) (17) y=x where given an endpoint condition at {0}, the solution is unique proabbility measure given that Qt is continuous over time [0, ]. Proof. We prove this by taking the conditional probability for forward step [t, + t] and taking the limit as 0. From (21), we have pt+t(x) = = (cid:88) (cid:88) Pr(Xt+t = xXt = y)pt(y) (1x=y + tQt(y, x) + O(t2))pt(y) = pt(x) + (cid:88) Qt(y, x)pt(y) + O(t2) (18) Taking the limit as 0, we have tpt(x) = lim (cid:34) (cid:88) (cid:35) Qt(y, x)pt(y) + O(t2) = = = (cid:88) (cid:88) y=x (cid:88) y=x Qt(y, x)pt(y) Qt(y, x)pt(y) + Qt(x, x)pt(x) Qt(y, x)pt(y) (cid:88) y=x Qt(x, y)pt(x) which concludes our proof. (19) Lemma 2 (Radon-Nikodym Derivative (RND)). Consider two CTMCs and with path measures and and initial distributions π0 and π 0. Then, the Radon-Nikodym derivative over trajectory X0:T = (Xt)t[0,T ] is defined as log dP dP (X0:T ) = log dπ 0 dπ0 (X0) + (cid:88) t:Xt=Xt log t(Xt, Xt) Qt(Xt, Xt) + (cid:90) (cid:88) 0 y=Xt (Qt t)(Xt, y)dt Proof. First, we compute the RND in the discrete-time case, where = is the discrete time interval and tn = nt is the time at the nth step. The RND of the discretized path can be written as log dP dP (X0:T ) = log dπ 0 dπ0 (X0) + 1 (cid:88) n=0 log P(Xtn+1 Xtn ) P(Xtn+1 Xtn ) + O(t) (20) where O(t) accounts for the probability of multiple jumps within the time interval. The probability of single jump under CTMC can be decomposed into the probability of remaining in the same state and the probability of transitioning to different state at time tn. P(Xtn+1 = yXtn = x) = (cid:40) 1 (cid:80) tQtn (x, y) + O(t2) z=x Qtn (x, z) + O(t2) = = (21) 23 1 (cid:80) 1 (cid:80) (cid:88) Preprint. Under review. First, expanding the log-ratio for the case where jump is made in the interval [tn, tn+1], we have tn (Xtn , Xtn+1 ) + O(t2) tQ tQtn (Xtn , Xtn+1 ) + O(t2) tn (Xtn , Xtn+1 ) Qtn (Xtn , Xtn+1 ) P(Xtn+1 Xtn ) P(Xtn+1 Xtn ) + O(t) = log = log log (22) Next, expanding the log-ratio for the case where no jump is made in the interval [tn, tn+1], we use the Taylor expansion of log(1 x) = + O(w2) to get z=x tn (Xtn , z) + O(t2) z=x Qtn (Xtn , z) + O(t2) P(Xtn+1 Xtn ) P(Xtn+1 Xtn ) = log log = (Qtn (Xtn , z) tn (Xtn , z)) + O(t2) (23) z=Xtn Finally, putting it all together and taking the limit as and 0, we have log dP dP (X0:T ) = lim t0 (cid:26) log dπ 0 dπ0 (X0) + 1 (cid:88) n=0 log tn (Xtn , Xtn+1 ) Qtn (Xtn , Xtn+1 ) (cid:88) + (Qtn (Xtn , z) tn (Xtn , z)) + O(t) (cid:27) z=x dπ 0 dπ0 = log (X0) + (cid:88) t:Xs=Xt log t(Xs, Xt) Qt(Xs, Xt) + (cid:90) (cid:88) 0 z=Xt (Qt(Xt, z) t(Xt, z))dt which concludes the proof. Now, we can easily extend this result to derive the KL-divergence DKL(PP) by taking the expectation with respect to on either side of the equality. (24) Corollary 1. The KL-divergence between two CTMCs P, with generators Q, DKL(PP) = DKL(π 0π0) + EX0:T (cid:90) (cid:88) 0 y=Xt t log Qt (Xt, y)dt (25) Proof. For the first term on the RHS of (24), we have EX0:T (cid:20) log dπ 0 dπ0 (cid:21) (X0) = EX0π (cid:20) log dπ 0 dπ0 (cid:21) (X0) = DKL(π 0π0) (26) For the second term, we apply the expectation to the discrete-time case and take the limit as 0 given by EX0:T (cid:34)N 1 (cid:88) n=0 1Xtn+1 =Xtn log tn (Xtn , Xtn+1 ) Qtn (Xtn , Xtn+1 ) (cid:35) EP(Xtn ),P(Xtn+1 Xtn ) (cid:20) 1Xtn+1 =Xtn log tn (Xtn , Xtn+1 ) Qtn (Xtn , Xtn+1 ) (cid:21) P(yXtn ) log tn (Xtn , y) Qtn (Xtn , y) (cid:20) tQ tn (Xtn , y) log tn (Xtn , y) Qtn (Xtn , y) (cid:21) + O(t2) = = = 1 (cid:88) n=0 1 (cid:88) n=0 1 (cid:88) n=0 EP(Xtn ) EP(Xtn ) (cid:88) y=Xtn (cid:88) y=Xtn (cid:90) (cid:88) 0 y=Xt log Qt (Xt, y)dt 24 (27) = t0 EX0:T which concludes the proof. Preprint. Under review. B.2 ENTROPY-REGULARIZED DIFFUSION FINE-TUNING The standard entropy-regularized diffusion fine-tuning problem (Black et al., 2023; Fan et al., 2023a; Clark et al., 2023; Uehara et al., 2025) involves maximization objective with two terms: (1) reward function and (2) KL regularization term that ensures the fine-tuned model does not diverge significantly from the pre-trained model. Formally, parameterized policy uθ that generates diffusion path distribution puθ aims to minimize the following objective (cid:26) arg min θ DKL (puθ (X0:T )ppre(X0:T )) EX0:T Puθ (cid:21)(cid:27) (cid:20) r(XT ) α (28) The first term maximizes the expected terminal reward under the policy model uθ and the second term minimizes the KL divergence between the path measure under the policy model Puθ and the pre-trained model Ppre. The scalar α > 0 is regularization factor that determines how closely the policy model follows the pre-trained model, where smaller α allows greater divergence from the pre-trained model and larger α constrains the policy model to follow closer to the pre-trained model. In discrete diffusion, the KL divergence term can be written in terms of the CTMC generators of the pre-trained model Qpre and the policy model Quθ given by DKL (Puθ Ppre) = EX0:T Puθ (cid:90) (cid:18) (cid:88) Quθ log 0 y=Xt Quθ + Qpre (cid:19) (Xt, y)dt Quθ Qpre Then, the discrete diffusion fine-tuning objective can be written as arg min θ EX0:T Puθ (cid:90) 0 y=Xt (cid:18) (cid:88) Quθ log Quθ Qpre Quθ + Qpre (cid:19) (Xt, y)dt r(XT ) α (29) (30) In the next section, we describe method of minimizing this objective with stochastic optimal control theory to derive an off-policy objective that avoids taking the expectation with respect to the current policy Puθ . B.3 FINE-TUNING WITH STOCHASTIC OPTIMAL CONTROL Here, we will frame the entropy-regularized diffusion fine-tuning framework defined in App B.2 as stochastic optimal control (SOC) problem that aims to find the optimal generator that produces the optimal reward-tilted path measure P. First, we define the value function Vt(x) which gives the cost-to-go from state to final state XT under controlled path measure Pu. We define the cost minimization objective with terminal reward r(XT ) as Jt(x, u) = EXPu (cid:90) (cid:88) y=Xs (cid:12) (cid:12) C(Xs, y)ds r(XT ) (cid:12) (cid:12) Xt = (31) (x, y), and the optimal cost-to-go where the cost is defined as Ct(x, y) = is (x, u) = inf Jt(x, u). Then, in the case of reward-optimization, we define the value function as the negative cost-to-go, Vt(x) := (x). In the case when the path measure is discrete CTMC, the cost to go is determined by the number of jumps that occur in the interval [t, ]. We further expand the value function to the following form, (cid:16) log Qu Q0 Qu + Q0(cid:17) Qu Vt(x) = inf EXPu (cid:18)(cid:90) t+t (cid:90) (cid:19) (cid:88) + t+t y=Xs Ct(Xs, y)ds r(XT ) (cid:12) (cid:12) (cid:12) (cid:12) XT = = inf (cid:88) y=x Ct(x, y) + O(t2) + inf EXPu (cid:2)Vt+t(Xt+t)(cid:12) (cid:12)Xt = x(cid:3) (32) Using the value function, we can derive the expression for the optimal generator Q. (34) (x, y), we have Preprint. Under review. Lemma 3 (Optimal Generator). Given base generator Q0 and the value function Vt, the optimal generator takes the form t (x, y) = Q0 (x, y) exp(Vt(y) Vt(x)) (33) Proof. Expanding the second term in (32), we have (cid:2)Vt+t(Xt+t)(cid:12) (cid:34) (cid:12)XT = x(cid:3) EXPu inf (21) = inf (cid:88) Vt+t(y) (cid:0)1x=y + tQu (x, y) + O(t2)(cid:1) (cid:35) = inf Vt+t(x) (cid:88) x=y Vt+t(y)Qu (x, y) + (cid:88) x=y Vt+t(x)Qu (x, y) + O(t2) = Vt+t(x) + inf (cid:88) x=y Qu (x, y)(Vt+t(x) Vt+t(y)) + O(t2) Now, substituting back into (32) and defining Ct(x, y) = (cid:16) log Qu Q0 Qu + Q0(cid:17) Qu tVt = inf (cid:88) y=x (cid:18) Qu log Qu Q0 Qu + (cid:19) (x, y) + (Vt(x) Vt(y))Qu (x, y) (35) The infimum can be achieved by minimizing convex scalar function for each pair = defined as (Qu) = Qu log Qu Q0 Qu + Q0 + (Vt(x) Vt(y))Qu (Qu) = log Qu Q0 + (Vt(x) Vt(y)) Setting (Qu) = 0, we get log Q0 = Vt(y) Vt(x) = (x, y) = Q0 (x, y) exp(Vt(y) Vt(x)) which concludes our proof. (36) (37) Corollary 2 (Hamilton-Jacobi Bellman (HJB) Equation). The value function Vt(x) = E[r(XT )Xt = x] satisfies the HJB equation given by (cid:16) (cid:16) (cid:88) Q0 (x, y) 1 eVt(y)Vt(x)(cid:17) teVt(x) (cid:88) Q0 (x, y) eVt(x) eVt(y)(cid:17) tVt(x) = (38) y=x y=x Proof. The proof follows from substituting the optimal into equation (35) and the second equation follows immediately after. (x, y) = Q0 (x, y) exp(Vt(y) Vt(x)) Lemma 4 (Optimal Path Measure). Given the value function Vt(x), the optimal path measure takes the form (x) = 1 (x)eVt(x), := P0 xP0 [er(x)] (39) P0 (x)eVt(x). By definition, we have hT = Proof. Let ht(x) := 1 ht satisfies the Kolmogorov forward equation for the optimal generator Kolmogorov forward equation from Lemma 1 for Q0 as tP0 (y) Q0 (y, x)P (x, y)P0 (x) = (x)) (Q0 (cid:88) . Now, we aim to show that . First, we restate the (40) y=x 26 Preprint. Under review. Furthermore, by Corollary 2, we have teVt(x) = (cid:88) y=x Q0 (x, y) (cid:16) eVt(x) eVt(y)(cid:17) (41) Now, taking the partial derivative of ht, we get teVt(x)(cid:105) (x)eVt(x) + P0 tht(x) = tP0 (cid:104)"
        },
        {
            "title": "1\nZ",
            "content": "eVt(x) (cid:88)"
        },
        {
            "title": "1\nZ",
            "content": "= = = (cid:18) (cid:88) y=x (cid:88) (cid:16) (cid:0)Q0 (y, x)P0 (y) Q0 (x, y)P0 (x)(cid:1) + t (x) y=x Q0 (y, x)"
        },
        {
            "title": "1\nZ",
            "content": "P0 (y)eVt(x) Q0 (x, y) (cid:19) P0 (x)eVt(y)"
        },
        {
            "title": "1\nZ",
            "content": "Q0 (y, x)eVt(x)Vt(y)ht(y) Q0 (x, y)eVt(x)Vt(y)ht(x) (cid:17) Q0 (x, y) (cid:16) eVt(x) eVt(y)(cid:17) (cid:88) y=x (3) = y=x (cid:88) y=x (Q (y, x)ht(y) (x, y)ht(x)) P0 (x)eVt(x). which is the Kolmogorov forward equation for the tilted distribution with the optimal generator in (3). By uniqueness of solutions to the Kolmogorov forward equation, we have (x) = 1 Now, we derive the expression for the Radon-Nikodym derivative between the optimal and reference path measures dP dP0 (X0:T ) in the following Lemma. Lemma 5 (Radon-Nikodym Derivative of Optimal and Reference Path Measure). Given the optimal form of the path measure and generator from Lemmas 3 and 4, the RND for any X0:T can be expressed as dP dP0 (X0:T ) = 1 er(XT ), where = EP0 [er] (42) Proof. Using Lemmas 2, 3, and 4, we have log dP dP0 (X0:T ) (2) = log dP 0 dP0 0 (X0) + (cid:88) t:Xt=Xt log Q0 (Xt, Xt) (Xt, Xt) + (cid:90) (cid:88) 0 y=Xt (Q0 )(Xt, y)dt (4,3) = V0(X0) log + (cid:88) (Vt(Xt) Vt(Xt) + t:Xt=Xt (cid:90) (cid:88) y=Xt Q0 (Xt, y)(1 eVt(y)Vt(Xt))dt The CTMC process X0:T is piecewise càdlàg function and (cid:55) Vt(x) is continuous for all x, we can define discrete jump times at 0 < t1 < < tn < < tN 1 < and write VT (XT ) V0(X0) = = = 1 (cid:88) n=0 1 (cid:88) n=0 (cid:90) (Vtn+1(Xtn ) Vtn (Xtn ) + 1 (cid:88) n=1 (Vtn(Xtn ) Vtn (Xtn1 )) (cid:90) tn+1 tn tVt(Xtn)dt + (cid:88) t:Xt=Xt (Vt(Xt) Vt(Xt)) tVt(Xt)dt + (cid:88) t:Xt=Xt (Vt(Xt) Vt(Xt)) = V0(X0) = VT (XT ) (cid:90) 0 tVt(Xt)dt (cid:88) t:Xt=Xt (Vt(Xt) Vt(Xt)) (43) 27 Preprint. Under review. Using Lemma 3, we also have (cid:90) (cid:88) 0 y=Xt (Q Q )(Xt, y)dt = (cid:90) (cid:88) 0 y=Xt Q0 (Xt, y) (cid:16) 1 eVt(y)Vt(Xt)(cid:17) dt = (cid:90) 0 tVt(Xt)dt (44) Substituting the expressions for V0(X0) and (cid:82) RND reduces to (cid:80) y=Xt Q0 (Xt, y) (cid:0)1 eVt(y)Vt(Xt)(cid:1) dt, the log dP dP0 (X0:T ) = VT (XT ) log = dP dP0 (X0:T ) ="
        },
        {
            "title": "1\nZ",
            "content": "VT (XT ) (45) Given the terminal reward VT (XT ) = r(XT ), we conclude our proof."
        },
        {
            "title": "C THEORETICAL PROOFS",
            "content": "C.1 OFF-POLICY LEARNING FOR MASKED DISCRETE DIFFUSION FINE-TUNING Here, we will derive the WDCE objective in (7) used for our off-policy RL fine-tuning algorithm, which matches the optimal path measure P. We note that this objective was derived in Zhu et al. (2025c) for the training of Masked Diffusion Neural Samplers (MDNS). Lemma 6. The RND between the optimal path measure defined in (4) and the current path measure of the fine-tuned model Pv under the masked discrete diffusion model formulation can be written as log dP dPv (X0:T ) = r(XT ) α + (cid:124) (cid:88) (cid:88) log t:Xs=Xt ℓ:X ℓ s=X ℓ (cid:123)(cid:122) :=W v(X0:T ) ppre(X ℓ pv(X ℓ sX UM ) sX UM ) (cid:125) log (46) Recall the special form of the optimal generator for MDM from (1) as Qt(x, y) = γ(t) Pr Xpdata (X ℓ = dX UM = xUM)1xℓ=d,y=xℓd Now, we can write the exit rate from as (cid:88) y=x Qv (x, y) = (cid:88) (cid:88) Qu (x, xℓd) = γ(t) (cid:88) 1 = γ(t)(cid:12) (cid:12){ℓ : xℓ = }(cid:12) (cid:12) (47) d:xℓ=M d:xℓ=M Since the pre-trained model is trained with the same noise schedule γ, we can also write Q0 (x, y) = γ(t)(cid:12) (cid:12){ℓ : xℓ = }(cid:12) (cid:12) (cid:88) y=x (48) 28 Preprint. Under review. Therefore, the last term in the RND cancels, and we derive simplified form of the RND specific to MDMs as dP dPv (X0:T ) = log log dP0 dPv dP dP0 dP dP0 + log dP0 dPv = log r(XT ) α r(XT ) α r(XT ) α log + (cid:88) log + log + t:Xt=Xt (cid:88) t:Xt=Xt (cid:88) t:Xt=Xt log Q0 Qv (Xt, Xt) (Xt, Xt) + (cid:90) (cid:88) 0 y=Xt log Q0 Qv (Xt, Xt) (Xt, Xt) (Qv Q0 )(Xt, y)dt log γ(t)ppre(X ℓ = dX UM = xUM)1xℓ=d,y=xℓd γ(t)pv(X ℓ = dX UM = xUM)1xℓ=d,y=xℓd = = = = r(XT ) α + (cid:124) (cid:88) (cid:88) log t:Xs=Xt ℓ:X ℓ s=X ℓ (cid:123)(cid:122) :=W v(X0:T ) ppre(X ℓ pv(X ℓ sX UM ) sX UM ) log (cid:125) where we denote the log-RND excluding the normalization term as v. (49) Corollary 3 (Weighted Denoising Cross-Entropy (WDCE) Loss). The solution to the Weighted Denoising Cross-Entropy (WDCE) loss defined as FWDCE(Pu, P) = XPv (cid:34) 1 eW v(X0:T )EλUnif(0,1) (cid:34) 1 λ Eµλ( xx) (cid:88) log puθ ( x)ℓ,xℓ ℓ: xℓ=M (cid:35)(cid:35) is the optimal generator of P. Proof. First, we recall the definition of the cross-entropy loss between the optimal and controlled path measure, defined as FCE(P, Pu) := EP (cid:20) log (cid:21) dP dPu = EPv (cid:20) dP dPv log dP dPu (cid:21) Then, writing the objective with respect to the log-RND of Pv and Pu, we have FCE(P, Pu) = EX0:T Pv (cid:21) eW v(X0:T )W u(X0:T ) (cid:20) 1 To further simplify u, we can discard the terms independent to in to get u(X0:T ) = (cid:88) (cid:88) log pv(X ℓ sX UM ) t:Xs=Xt ℓ:X ℓ s=X ℓ (50) (51) (52) Instead of computing the loss only with respect to the trajectory that generates XT , Zhu et al. (2025c) proposes to compute loss over many potential trajectories for each single clean sample XT by remasking XT and computing the DCE loss in (2) with respect to each of the masked tokens. u(X0:T ) = EλUnif(0,1) (cid:34) 1 λ Eµλ( xx) (cid:88) log puθ ( x)ℓ,xℓ (cid:35) ℓ: xℓ=M (53) where puθ ( x)ℓ,xℓ takes the probability of the ℓth token being in state xℓ. This gives us the weighted denoising cross-entropy (WDCE) loss defined as FWDCE(Pu, P) = XPv (cid:34) 1 eW v(X0:T )EλUnif(0,1) (cid:34) 1 λ Eµλ( xx) (cid:88) log puθ ( x)ℓ,xℓ ℓ: xℓ=M (cid:35)(cid:35) where we define = := stopgrad(uθ) and is computed with respect to the optimal measure = Ppre exp(r(XT )) given the pre-trained generator Qpre that produces the path measure Ppre. Preprint. Under review. C.2 JUSTIFICATION FOR THE DECOUPLING OF TREE SEARCH AND FINE-TUNING Our framework relies on the fact that the tree search algorithm used to populate the replay buffer and the off-policy RL algorithm are decoupled, enabling integration of any pair of search and off-policy RL algorithms. The effectiveness of our approach is grounded in two key properties of off-policy RL: (1) it trains on trajectories generated from an arbitrary reference measure in frozen replay buffer to inform the update to the current policy and (2) it fits the buffer distribution with theoretical guarantees which amortizes the cost of searching by letting the fine-tuned policy inherit the high-quality samples generated from the search algorithm. C.3 ACHIEVING PARETO-OPTIMALITY WITH MULTI-OBJECTIVE FINE-TUNING To prove that our multi-objective fine-tuning framework from Sec 5 enables the fine-tuned model to generate samples that approach Pareto-optimality, we first establish the following Lemma. Lemma 7 (Non-Decreasing Hypervolume of Buffer). Given set of candidate sequence rewards = {ri} and the current set of rewards in the buffer = {r}, the HV of the nondominated rewards in the union of both sets is non-decreasing from the HV of the original set HV (ND(B S)) HV(B) (54) wher ND() is takes the set of non-dominated solutions. Proof. Let be reference reward vector such that all feasible rewards dominate it (i.e., r). Denoting the axis-aligned orthant between the coordinates and as [r, r] = {y RK : k, rk yk rk}, we write the hypervolume (HV) as the Lebesgue measure µ() of the union of the orthants generated from set. HV(r; B) = µ(U (B)) := µ (cid:33) [r, r] (cid:32) (cid:91) rB It is straightforward to show that given we have (B) (B S) = HV(B) HV(B S) (55) (56) Now, we want to show that the union of the non-dominated subset ND(B S) does not shrink the union: (ND(B S)) = (cid:91) [r, r] = (cid:91) [r, r] = (B S) rND(BS) rBS By definition of [r, ], if reward dominates (i.e. y), we have [r, y] [r, r] (57) (58) Let S. If ND(B S), then clearly [r, y] (ND(B S)). If / ND(B S), then by definition, there exists some ND(B S) that dominates it such that y. Then, it follows that S, we have [r, y] [r, r] and (B S) (ND(B S)) (59) Since ND(B S) = (ND(B S)) (B S), we have shown that (ND(B S)) = (B S. Since (B) (B S), we get µ(U (ND(B S)) µ(U (B S)) = HV(ND(B S) HV(B S) which concludes our proof. (60) Proposition 5.1 (Pareto Optimization of Buffer). With each iteration of the search, the buffer approaches the Pareto front , where the hypervolume generated by the rewards in the set is maximized. 30 Preprint. Under review. First, we establish the following assumptions: (A1) Each node in the tree is sufficiently explored, such that Nvisits as the number of iterations goes to infinity Niter . (A2) The reward function is bounded and defined over the feasible search space . (A3) There is positive probability > 0 of discovering sequence that strictly increases the HV of by ε with each search iteration. For the purpose of this proof, we do not limit the size of the buffer set B. Let denote the Pareto frontier of the feasible solution space and multi-reward function r, such that HV(P ) is the maximum feasible hypervolume. By Lemma 7, we have shown that the HV is non-decreasing with each search iteration. By our assumption, we have that for all iterations where HV(B) HV(P ) ϵ, the expected HVI of after each iteration is proportional to the discovery probability > 0 given by [HV(B) HV(B)] pε (61) After Niter iterations of the search, the search-optimized buffer has an expected HV given by (cid:2)HV(BNiter)(cid:3) HV(B0) + Niterpε (62) which converges to HV(P ) as Niter . This convergence guarantee holds for any search algorithm that satisfies (A1)-(A3), that is, it sufficiently explores the solution space and discovers ε-Pareto solutions with non-negative probability with each search iteration. MCTS satisfies (A1) with the exploration constant so every path has non-zero probability of being sampled and (A2)-(A3) given that the reward oracle is trained on an empirical subset of the dataset used to train the pre-trained model. Furthermore, the MCTS algorithm exploits sampling paths based on an estimated future reward derived from previous iterations, which intuitively increases the probability of discovering high-reward sample that contributes positively to HVI at each iteration."
        },
        {
            "title": "D REGULATORY DNA EXPERIMENT DETAILS",
            "content": "We largely follow the experimental setup and evaluation metrics from Wang et al. (2025) to ensure fair benchmarking. D.1 EXPERIMENT SETUP Pre-trained Model We use the pre-trained masked discrete diffusion model from Wang et al. (2025) built on the Masked Discrete Language Model (MDLM) framework (Sahoo et al., 2024). The model is trained on 700k DNA enhancer sequences 200 base-pairs in length from the Gosai dataset (Gosai et al., 2023). The backbone architecture is CNN with linear noise schedule following Stark et al. (2024). Enhancer Activity Predictor We use the pre-trained reward oracles from Wang et al. (2025), which predict the enhancer activity in the HepG2 cell line. Following the procedure in (Lal et al., 2024), the Gosai dataset (Gosai et al., 2023) of 700K DNA sequences is split into two disjoint sets which each contains enhancers from half of the 23 human chromosomes. One is used to train the fine-tuning oracle for optimization during fine-tuning, while the other is used to train the evaluation oracle, which was used to compute the Pred-Activity reported in Table 1. Both models are built on the Enformer architecture (Avsec et al., 2021) and achieved Pearson correlations of > 0.85 on the held-out sets. Fine-Tuning Setup We load the pre-trained model with frozen weights for log-RND computation and load the model with unfrozen weights for fine-tuning. We set the buffer size to 128 and the number of diffusion steps to 128, to remain consistent with (Wang et al., 2025). We conducted ablations on various hyperparameters, including the regularization strength α, the use of MCTS, the resampling frequency Nresample, and the number of MCTS iterations Niter, with results reported in Table 6. All the DNA experiments were conducted on an NVIDIA H100 GPU. We used the AdamW optimizer with learning rate of η = 3 104. For evaluation, we compute metrics for 640 sequences with three random seeds and report the mean and standard deviation, consistent with Wang et al. (2025); Zekri & Boullé (2025). Preprint. Under review. Table 3: Docking results for TR2-D2 generated peptide binders. Binding affinities calculated with AutoDock VINA (kJ/mol; ), where lower values indicate stronger binding affinity, are reported for two randomly selected binders generated with the fine-tuned peptide models optimized for TfR, GLP-1R, and GLAST binding affinity. Classifier scores for binding affinity, solubility, non-hemolysis, non-fouling, and permeability optimized during fine-tuning are also reported. Target Protein VINA Docking Score (kJ/mol; ) Binding Affinity () Solubility () Non-hemolysis () Non-fouling () Permeability () TfR Binder 1 TfR Binder 2 GLP-1R Binder 1 GLP-1R Binder 2 GLAST Binder 1 GLAST Binder 2 7.3 7.2 6.4 5.9 5.5 5. 9.485 9.276 9.211 9.177 9.198 9.578 0.901 0.941 0.901 0.822 0.769 0. 0.940 0.908 0.925 0.864 0.874 0.927 0.197 0.133 0.494 0.411 0.188 0. 7.283 7.195 7.254 7.388 7.285 7.223 D.2 ENHANCER EVALUATION METRICS Mean Predicted Activity (Pred-Activity) We use the fine-tuning and evaluation reward oracles from Wang et al. (2025), which are trained on disjoint splits of the Gosai dataset of 700k DNA enhancer sequences (Gosai et al., 2023) labeled with the measured expression of the sequence in the HepG2 cell line. We fine-tune the pre-trained generator to optimize the predicted activity by the fine-tuning oracle and report the median predicted activity by the evaluation oracle in Table 1 for comparison against baseline models. Binary Classification on Chromatin Accessibility (ATAC-Acc) We further validate the predicted enhancer activity from classifier that is not directly optimized during fine-tuning. Specifically, we use the binary classification model (%) that predicts the chromatin accessibility of an enhancer sequence in the HepG2 cell line, where positive accessibility indicates increased enhancer activity Wang et al. (2025); Lal et al. (2024). 3-mer Pearson Correlation (3-mer Corr) To measure whether the fine-tuned model generates sequences within the distribution of the pre-trained model, we evaluate the 3-mer Pearson correlation between the generated sequences with the fine-tuned model and the 0.1% of sequences with the highest HepG2 enhancer activity from the Gosai dataset (Gosai et al., 2023) used to train the pretrained generator. Approximated Log-Likelihood of Sequences (App-Log-Lik) We evaluate the log-likelihood of the sequences generated by the fine-tuned model under the pre-trained model, which indicates whether the fine-tuning method over-optimizes the pre-trained model to generate out-of-distribution sequences. Specifically, we compute the likelihood as the evidence lower bound (ELBO) (Sahoo et al., 2024), where larger ELBO indicates higher likelihood of the fine-tuned sequence under the pre-trained model."
        },
        {
            "title": "E PEPTIDE EXPERIMENT DETAILS",
            "content": "E.1 EXPERIMENT SETUP Pre-trained Model We use the pre-trained bond-dependent masked discrete diffusion model from Tang et al. (2025), which generates peptide sequences containing the 20 canonical amino acids, in addition to non-canonical amino acids with chemical modifications and cyclicizations in SMILES notation (Weininger, 1988). The model is trained on 11 million peptide SMILES, containing 7451 cyclic peptides from the CycPeptMPDB database (Li et al., 2023), 825, 632 peptide sequences from SmProt (Li et al., 2021), and 10 million peptides with cyclicizations and non-canonical amino acids generated from CycloPs (Duffy et al., 2011; Feller & Wilke, 2025). To tokenize the SMILES sequences, we use the SMILES Pair Encoding (SPE) tokenizer (Li & Fourches, 2021; Feller & Wilke, 2025) containing vocabulary of 581 SMILES tokens and 5 special tokens including [PAD], [UNK], [CLS], [SEP], and [MASK]. The generator is built on the Masked Discrete Language Model (MDLM) framework with masking schedule that promotes early unmasking of peptide 32 Preprint. Under review. bond tokens (Tang et al., 2025). The backbone architecture is RoFormer (Su et al., 2024) with 8 Transformer layers and 8 attention heads. Fine-Tuning Setup We load two versions of the pre-trained weights, one as the frozen pre-trained model for calculating the log-RND of the trajectory, and one with all weights unfrozen for finetuning. We also load the pre-trained classifiers for binding affinity, given protein sequence input, solubility, non-hemolysis, non-fouling, and membrane permeability into joint function that outputs 5-dimensional vector of scores. We perform ablations on several hyperparameters as shown in Table 7 and choose the hyperparameters in Table 5 as default, given their superior performance. We trained for total of 1000 epochs for each protein target and hyperparameter set. All peptide experiments were conducted on an NVIDIA A6000 GPU with learning rate of η = 104 with the AdamW optimizer (Loshchilov & Hutter, 2017) and gradient clipping. For evaluation, we generate 100 sequences i.i.d. with single generation pass with 128 diffusion steps and report the mean and standard deviation of the predicted rewards. Target Proteins We evaluate the ability of TR2-D2 to generate peptide binders to therapeutically relevant protein targets, including Transferrin receptor (TfR), common organ-specific drug-delivery target (Han et al., 2024a); glucagon-like peptide-1 receptor (GLP-1R), relevant for type-2 diabetes and obesity (Alfaris et al., 2024); glutamate-aspartate transporter (GLAST) protein abundant on the surface of astrocytes, type of glial cell in the brain relevant to neurological disorders (Pajarillo et al., 2019); glial fibrillary acidic protein (GFAP), associated with Alexander disease (Eng, 1985; Quinlan et al., 2007); anti-Müllerian hormone type-2 receptor (AMHR2) which is relavent target for polycystic ovarian syndrome (PCOS) therapy (Singh et al., 2023); and finally, neural cell adhesion molecule 1 (NCAM1), transmembrane protein that is expressed on the surface of neurons and glial cells and facilitates neuronal migration and synaptogenesis. E.2 THERAPEUTIC PROPERTY CLASSIFIERS We use the pre-trained classifiers from Tang et al. (2025) for the prediction of target-protein binding affinity, solubility, non-hemolysis, non-fouling, and membrane permeability, which serve as the multi-objective reward functions. Protein Target-Binding Predictor The target-protein binding affinity classifier that embeds the target protein amino acid sequence using ESM-2-650M (Lin et al., 2023) and the peptide SMILES sequence with PeptideCLM (Feller & Wilke, 2025) and feeds the sequences to cross multi-head attention Transformer architecture. The model is trained on 1806 protein-peptide pairs from the PepLand dataset (Zhang et al., 2023) containing canonical and non-canonical peptides with experimentallyvalidated Kd/Ki/IC50 binding affinity scores to various protein sequences, achieving strong Spearman correlation coefficient of 0.869 on the training data and 0.633 on the held-out validation data. We classify scores as indicating weak binding (< 6.0), medium binding (6.0 7.5), and high binding ( 7.5). Solubility and Toxicity Predictors For solubility, non-hemolysis, and non-fouling, we used the XGBoost (Chen & Guestrin, 2016) logistic regression classifiers trained on binary data collected from the PepLand (Zhang et al., 2023) and PeptideBERT (Guntuboina et al., 2023) datasets, with 1 indicating the positive class and 0 indicating the negative class, and values ranging from [0, 1]. Positive solubility means higher concentration of peptides can be dissolved in water, indicating enhanced drug loading. Positive non-hemolysis and non-fouling indicate lower destruction of red blood cells and lower off-target binding, respectively, which is essential for the non-toxicity of peptide drugs. The optimal positive thresholds for each score are 0.500 for solubility, 0.800 for non-hemolysis, and 0.450 for non-fouling. Membrane Permeability Predictor For membrane permeability, the classifier is an XGBoost regression model trained on 34,853 experimentally validated peptide SMILES with labeled PAMPA lipophilicity scores from the ChEMBL (Gaulton et al., 2012) and CycPeptMPDB (Li et al., 2023) databases, where less negative scores indicate stronger membrane permeability. 33 Preprint. Under review. E.3 BASELINES AND EVALUATION Baseline Setup For the pre-trained baseline, we generate 100 sequences unconditionally from single generation pass with 128 diffusion steps of the pre-trained model and compute the binding affinity to each of the protein targets as well as the other properties for comparison. For the PepTune baseline (Tang et al., 2025), we run inference-time guidance on the pre-trained model by running 100 iterations of Monte-Carlo Tree Guidance (MCTG) with 128 denoising steps on the set of five reward functions with the number of children set to = 50. VINA Docking To visualize the binding position of generated peptides on the target protein, we used Autodock VINA (Eberhardt et al., 2021) for in silico confirmation of binding affinity. We processed the target proteins with MGITools (Morris et al., 2009) and the peptide SMILES with ETKDG from RDKit (Wang et al., 2020), and visualized the final protein-peptide complex in PyMol (DeLano et al., 2002)."
        },
        {
            "title": "F HYPERPARAMETER DISCUSSION AND ABLATIONS",
            "content": "In this section, we provide detailed analysis of the hyperparameters for TR2-D2. We include results of ablation studies for the number of epochs in Table 4, for enhancer DNA design in Table 6, and for multi-objective peptide design in Table 7 and Figures 3, 4, 5, and 6. In addition, we discuss the effect of MCTS search in App F.1, the number of fine-tuning epochs in App F.2, and all other hyperparameters in App F.3. We suggest tuning hyperparameters when adapting the TR2-D2 framework to new modalities and tasks, and provide further intuition on each hyperparameter and their role in App F.3. F.1 ABLATION ON MCTS SEARCH To show the impact of MCTS on the effectiveness of fine-tuning, we conduct an ablation study that removes the use of MCTS to generate the buffer. Instead, we populate the buffer with sequences and their log-RND weights (XT , u) using independent forward diffusion passes through the current policy model without gradient tracking. We maintain the same non-MCTS hyperparameters and show that removing the MCTS search results in worse metrics for enhancer DNA design  (Table 6)  and consistently lower rewards across all five objectives for multi-objective therapeutic peptide design (Table 7; Fig 2). F.2 ABLATION ON NUMBER OF FINE-TUNING EPOCHS As shown in Table 4, we show that the TR2-D2 outperforms PepTune across almost all objectives for each protein target with 200 epochs and 1000 epochs of fine-tuning (Nresample = 20, Niter = 20, and = 50). After 200 epochs of fine-tuning, we observe increased performance across most rewards compared to the PepTune baseline, while maintaining sequence diversity. After 1000 epochs of fine-tuning, we observe that the mean reward values plateau to optimality across all objectives but result in lower sequence diversity. We conclude that there is trade-off between the reward optimality and sequence diversity, with Nepochs = [200, 1000] epochs being suitable range for multi-objective peptide sequence generation. We also note that tuning other hyperparameters can also affect the diversity of generated sequences, specifically setting Nresample = 10 instead of 20 significantly increases diversity, even after 1000 epochs. F.3 HYPERPARAMETER DISCUSSION Number of Children This determines the number of partially unmasked sequences independently sampled from the fine-tuned model at the expansion step in each MCTS loop. These will become the child nodes of the expanded node at each iteration. Increasing increases the number of sequences explored at each step, which widens the optimal search space covered during buffer generation. We found that increasing the number of children improves performance across multiple objectives  (Table 7)  . 34 Preprint. Under review. Table 4: Full multi-objective peptide design results. Target proteins include TfR, GLP-1R, GLAST, GFAP, AMHR2, and NCAM1. All values are averaged over 100 generated peptides. Best values are bolded. Secondbest values are underlined. Pre-trained indicates unconditional sampling with the pre-trained peptide SMILES model from PepTune (Tang et al., 2025). PepTune indicates samples from 100 iterations of inference-time Monte-Carlo Tree Guidance conditioned on all objectives. TR2-D2 indicates unconditional sampling after 200 and 1000 epochs of fine-tuning of the pre-trained model with our multi-objective fine-tuning approach. Hyperparameters are set to Nresample = 20, Niter = 20, and = 50 across all runs. Target Protein Method Binding Affinity () Solubility () Non-hemolysis () Non-fouling () Permeability () TfR GLP-1R GLAST GFAP AMHR2 NCAM1 Pre-trained PepTune TR2-D2 (Nepochs = 200) TR2-D2 (Nepochs = 1000) Pre-trained PepTune TR2-D2 (Nepochs = 200) TR2-D2 (Nepochs = 1000) Pre-trained PepTune TR2-D2 (Nepochs = 200) TR2-D2 (Nepochs = 1000) Pre-trained PepTune TR2-D2 (Nepochs = 200) TR2-D2 (Nepochs = 1000) Pre-trained PepTune TR2-D2 (Nepochs = 200) TR2-D2 (Nepochs = 1000) Pre-trained PepTune TR2-D2 (Nepochs = 200) TR2-D2 (Nepochs = 1000) 8.0080.673 8.2160.703 8.9590.796 10.0980.050 8.2330.367 8.4030.365 9.0590.329 9.4260.035 7.8300.420 8.4000.353 8.8420.274 9.7030. 7.0840.594 7.2560.704 8.5390.463 9.7620.123 7.9580.253 8.2840.186 8.5320.117 8.5950.029 6.4380.372 6.9160.240 7.3330.186 7.5410.025 0.7420.166 0.7890.144 0.7320.145 0.8380.066 0.7420.166 0.7740.170 0.7000.084 0.8410.043 0.7420.166 0.8150.139 0.8220.122 0.8840. 0.7420.166 0.8070.167 0.8200.166 0.9100.032 0.7420.166 0.7890.144 0.7100.192 0.9470.0145 0.7420.166 0.8770.105 0.9400.065 0.9720.018 0.8740.063 0.9020.051 0.9040.038 0.8960.012 0.8740.063 0.9070.057 0.8390.037 0.8490.016 0.8740.063 0.9370.029 0.9060.031 0.9300. 0.8740.063 0.9070.053 0.9050.020 0.8890.010 0.8740.063 0.9300.039 0.9170.047 0.9230.008 0.8740.063 0.9350.039 0.9320.047 0.9740.003 0.1020.083 0.1210.081 0.2290.094 0.2710.038 0.1020.083 0.125.082 0.385.095 0.4990.037 0.1020.083 0.1370.086 0.2680.086 0.3640. 0.1020.083 0.1240.088 0.1540.043 0.1370.011 0.1020.083 0.1560.074 0.5340.144 0.7660.023 0.1020.083 0.0900.075 0.0860.117 0.0670.009 7.4700.120 7.3890.119 7.3000.067 7.1680.024 7.4700.120 7.3880.128 7.2880.047 7.2630.020 7.4700.120 7.3110.106 7.3160.048 7.2380. 7.4700.120 7.3740.134 7.2560.071 7.1960.030 7.4700.120 7.3460.102 7.1590.073 7.1640.031 7.4700.120 7.3910.133 7.1230.088 6.9300.028 Number of MCTS Iterations Niter This determines the number of MCTS loops of selection, expansion, rollout, and backpropagation at each buffer resampling step, where each iteration begins by selecting an optimal trajectory from the root node (fully masked sequence) to leaf node (unexpanded partially masked sequence). If the selected leaf node is fully unmasked, the selection process restarts from the root without increasing the iteration count. Each iteration generates new batch of sequences that could be added to the buffer. We found that even Niter = 5 improves fine-tuning across multiple rewards, which steadily increases with larger Niter (Figure 4). Exploration Constant This determines the scaling factor of the second term in Equation (9) that determines the degree of exploration during MCTS. We determine that = 0.1 optimally balances exploration and exploitation of optimal trajectories. Number of Replicates for WDCE For each batch of fully unmasked sequence sampled from , uθ )}B the replay buffer {(X i=1, we calculate the WDCE loss LWDCE from (7) using independently masked versions of . First, we sample random variables {λi,r}i{1,...,B},r{1,...,R} where λi,r Unif(0, 1) and generate set of partially masked replicates {X r=1 where each is generated by masking each token of with probability λi,r. Since we use log-linear masking schedule, we derive = λi,r and σ(t) = log(1 (1 ϵ)t) for input to the policy model. }R Regularization Scaling α For entropy-regularized diffusion fine-tuning, the KL regularization term in (28) is scaled by small constant α > 0, which determines the degree to which the fine-tuned model can diverge from the pre-trained model. For the DNA enhancer experiment, we found that setting α = 0.1 achieved superior correlation to the 0.1% highest reward sequences in the dataset, while lower alpha α = 0.01 achieved superior reward optimization against all benchmarks, indicating that alpha has significant role in modulating how closely the fine-tuned distribution diverges from the data distribution and pre-trained model  (Table 1)  . In the peptide experiment, we set α = 0.1, which maintained high validity of generated sequences while optimizing the multi-objective rewards. 35 Preprint. Under review. Resampling Frequency Nresample This determines the number of epochs between each resampling of the replay buffer with tree search. For smaller Nresample, the buffer is resampled with greater frequency and the model is trained on each buffer for lower number of epochs. For larger Nresample, the buffer is resampled less frequently and the same replay buffer is used for training over more epochs. We found that decreasing the resampling frequency to once per 20 epochs enhanced the multi-objective rewards but resulted in decrease in diversity in generated sequences, whereas Nresample = 10 preserved diversity while optimizing all objectives  (Table 7)  . Buffer Size The buffer size is the number of sequences stored in the replay buffer for the WDCE loss computation during fine-tuning. At each buffer resampling step, the buffer is emptied and repopulated with optimal sequences and their corresponding log-RND weights using our tree search approach. While in most fine-tuning approaches, larger buffer improves performance, our approach enables searching for optimal sequences to add to the buffer, thus improving the quality despite smaller buffer sizes. We also note that since MCTS generates sequences at each iteration for Niter iterations, the maximum buffer size is Niter. Number of Diffusion Steps Nsteps This is the number of unmasking steps between the fully masked sequence at timestep = 0 and the fully unmasked sequence at timestep = Nsteps 1. The MDLM framework (Sahoo et al., 2024) operates in continuous time [0, 1] with the log-linear noise schedule, where the probability of being masked at time is given by and the total probability of being masked over time [0, t] is given by σ(t) = log(1 (1 ϵ)t). Following the standard setup in MDLM, we set Nsteps = 128. Top Hyperparameter For single-reward fine-tuning, determines the number of child nodes that are candidates during the selection step of MCTS based on their selection reward value. At each selection step, we take the softmax of the top-k selection scores and sample the next node from the categorical distribution. We find that setting equal to the number of children = such that all child nodes have chance of being explored yields good performance in DNA enhancer experiments. Resetting the MCTS Tree While it is possible to maintain the same MCTS tree for multiple buffer generation steps, we found that resetting to an empty tree before each buffer generation yields the best performance. This follows from the idea that after fine-tuning, the model inherits the ability to generate the optimal sequences from the previous tree, resulting in more optimal tree in the next buffer generation. Table 5: Default hyperparameters for enhancer DNA and peptide experiments. Discussion on hyperparameter choices and ablation studies are given in App F. Experiment Niter Enhancer DNA 32 50 Peptides 5 20 0.1 0.1 16 α 0.1 0.1 Nresample 5 20 160 Nsteps 128 128 - Table 6: Ablation study for fine-tuning DNA enhancer activity. Metrics are computed for 640 sequences over 3 random seeds. Default settings are given in Table 5. Method TR2-D2 (α = 0.1) TR2-D2 (α = 0.001) TR2-D2 w/o MCTS Resampling Frequency Nresample Nresample = 10 Number of MCTS Iterations Niter Niter = 10 Niter = 20 Niter = 30 Pred Activity (median; ) ATAC-Acc (%; ) 3-mer Corr () App-Log-Lik (median; ) 6.560.02 9.740.01 6.000.02 86.91.18 99.90.01 76.91.60 0.9250.002 0.5480.001 0.9100. 259.40.20 271.80.1 269.90.05 6.730.05 80.61.23 0.9000.002 254.20. 6.130.11 5.490.03 4.910.02 85.00.8 81.92.6 79.80.66 0.9220.001 0.9210.001 0.860.003 260.00.16 262.00.20 268.60.40 36 Preprint. Under review. Figure 3: Multi-objective reward curves for fine-tuning toward high binding affinity to proteins TfR, GLP-1R, and GLAST. Average reward values of 50 sequences sampled from the fine-tuned model after each fine-tuning epoch are plotted over total of 1000 epochs, and running average is shown with the smooth line. The mean buffer reward is computed after every buffer resampling step (every 10 epochs). We observe that the multi-objective fine-tuning method effectively enables optimization of rewards for diverse therapeutic targets. 37 Preprint. Under review. Figure 4: Ablation study on the number of iterations of MCTS Niter per buffer generation step for multiobjective peptide generation. Average reward values of 50 sequences sampled from the fine-tuned model after each fine-tuning epoch are plotted over total of 1000 epochs, and running average is shown with the smooth line. The mean buffer reward is computed after every buffer resampling step (every 10 epochs). We observe steady increase in the mean rewards stored in the buffer with larger number of iterations. 38 Preprint. Under review. Figure 5: Ablation study on the number of children nodes explored in each iteration of MCTS. Average reward values of 50 sequences sampled from the fine-tuned model after each fine-tuning epoch are plotted over total of 1000 epochs, and running average is shown with the smooth line. The mean buffer reward is computed after every buffer resampling step (every 10 epochs). We observe steady increase in the mean rewards stored in the buffer as the number of child sequences explored increases. 39 Preprint. Under review. Figure 6: Ablation study on the number of training epochs Nresample between each buffer resampling step. Average reward values of 50 sequences sampled from the fine-tuned model after each fine-tuning epoch are plotted over total of 1000 epochs, and running average is shown with the smooth line. The mean buffer reward is computed after every buffer resampling step (every 10 epochs). We observe steady increase in the mean rewards as the Nresample increases, indicating that the model can inherit the ability to generate high-reward sequences seen in the buffer with more training iterations. 40 Preprint. Under review. Table 7: Ablation study for multi-objective fine-tuning for therapeutic peptide design for targeting Transferrin receptor (TfR. Metrics are computed for 100 i.i.d. generated sequences from single forward pass through the fine-tuned model. Best scores within each hyperparameter group are bolded. Worst scores across all runs are underlined. Default settings are defined as Nresample = 10, Niter = 20, = 20 with MCTS. Method Binding Affinity () Solubility () Non-hemolysis () Non-fouling () Permeability () TR2-D2 w/o MCTS 9.3360.325 0.5480.173 0.9080.034 0.1220.044 7.3230.076 Resampling Frequency Nresample Nresample = 5 Nresample = 10 Nresample = Number of MCTS Iterations Niter Niter = 5 Niter = 20 Niter = 50 Number of Children = 10 = 20 ="
        },
        {
            "title": "G ALGORITHMS",
            "content": "9.2380.684 9.3240.374 9.9580.120 0.6450.167 0.6690.166 0.8790.052 0.8980.039 0.9010.039 0.9300.010 0.1860.105 0.1330.052 0.2050.041 7.2730.073 7.2810.067 7.2040.037 8.9800.811 9.3240.374 9.7220. 0.7330.154 0.6690.166 0.6960.120 0.9300.024 0.9010.039 0.9090.030 0.1400.052 0.1330.052 0.0950.034 7.2620.070 7.2810.067 7.2270.067 9.2710.415 9.3240.374 9.3550.573 0.6900.156 0.6690.166 0.7170. 0.9070.035 0.9010.039 0.8880.056 0.1510.058 0.1330.052 0.1570.074 7.2900.062 7.2810.067 7.2560.070 Here, we provide pseudo-code for the additional algorithms for single-reward and multi-reward fine-tuning of discrete diffusion models with TR2-D2. Algorithm 3 outlines the procedure for single reverse unmasking step with log-RND tracking. Algorithm 4 describes the procedure for remasking clean samples from the replay buffer to compute the WDCE loss in (7). Algorithm 5 describes the MCTS algorithm for generating an optimal buffer for the single and multi-reward case using the Select and UpdateParetoFront described in Algorithms 6 and 7, respectively. Algorithm 3 SingleReverseStep: Single diffusion inference step 1: Input: Partially masked sequence Xt, timestep [0, 1], time increment t, pre-trained model ppre, policy model puθ 2: σ(t) log(1 (1 ϵ)t) 3: change_prob_t 4: change_prob_s 5: log puθ (Xt) Policy(Xt, σ(t)) 6: log ppre(Xt) pre-trained(Xt, σ(t)) 7: qs(XsXt) puθ (Xt)(change_prob_t change_prob_s) 8: qs(xs = Xt) 0 9: XT SampleCategorical(qs(XsXt)) 10: Xs XT (1 1X ℓ 11: log_policy (cid:80) 12: log_pre (cid:80) 13: return Xs, log_policy, log_pre =M ) + Xt 1X ℓ ℓ:X ℓ s=X ℓ log ppre(Xt)ℓ,X ℓ log puθ (Xt)ℓ,X ℓ s=X ℓ =M ℓ:X ℓ zero-masking probability }B for = 1 to do i=1, number of replicates Algorithm 4 ResampleWithMask: Remasks unmasked sequence to compute the WDCE loss 1: Input: Batch of sequences {X 2: for = 1 to do 3: 4: 5: 6: 7: 8: end for 9: return {( i,r λi,r Unif(0, 1) i,r µλ( xℓ λi,r mask each token with probability λi,r , λi,r)}i{1,...,B},r{1,...R} end for tX ) 41 Preprint. Under review. s)}M select leaf node s) u(X s), log puθ (X W u(X s) log ppre(X s) initialize empty buffer rollout child nodes to fully unmasked i=1 BatchedReverseStep(Xt) log_rndi log_rndi + log puθ (X for in {t, . . . , } do s+t, log_policyi, log_prei SingleReverseStep(Xs, s) s) + (log_pre log_policy) Algorithm 5 MCTS: Monte-Carlo Tree Search for Trajectory Optimization 1: Input: pre-trained model ppre(Xt), finetuned policy model puθ (Xt), number of children , 2: X0 [M ]L 3: {} 4: for iter in 1, . . . , Niter do log_rnd 0 5: Xt, log_rnd Select(X0) 6: {X s, log ppre(X 7: for in 1, . . . , do 8: 9: 10: 11: 12: 13: 14: 15: 16: 17: 18: 19: 20: 21: 22: 23: 24: 25: 26: 27: 28: end for 29: return parent parent(Xt) R(X parent) R(X parent) + R(X s) Nvisits(X parent) Nvisits(X parent) + end if UpdateBuffer(X children(Xt) {X R(Xt) R(Xt) + r(X end for while parent(Xt) is not None do end for if > 1 then u(X , u(X )} multi-objective rewards , r(X ) s) u(X s) u(X k=1 rk(X ) backpropogate α r(X ) end while s) + 1 α u(X s) + 1 (cid:80)K else s)) 42 Preprint. Under review. Algorithm 6 Select: Select Optimal Trajectory 1: Input: MCTS tree , root node X0 2: while True do 3: 4: 5: 6: Pselect {} for in children(Xt) do if > 1 then if children(Xt) is not empty and = then multi-objective selection Pareto-optimal children s) R(X s) (Xt, sXt) Pselect UpdateParetoFront(Pselect; (X Xselected Pselect s) + puθ (X M Nvisits(X Nvisit(Xt) 1+Nvisit(X s) s, (Xt, sample random child from Pselect s))) in children(Xt) do s) R(X s) (Xt, scores.append(cid:0)U (Xt, Nvisits(X s) + puθ (X s)(cid:1) Nvisit(Xt) 1+Nvisit(X s) sXt) end for Xselected Cat(cid:0)softmax (topk(scores)) (cid:1) recursively call Select until expandable node if leaf node is already fully unmasked, restart from root if leaf node is expandable, return it Algorithm 7 UpdateParetoFront: Add sequences with Pareto-optimal reward vectors 1: Input: Current Pareto front containing unmasked sequences r(X ) denoted = {(X , r)}, the candidate sequence and its reward vector (X and their reward vectors , ri) if the candidate is dominated by any sequence in the set, return the set unchanged for (X , r) do if np.all(r ri ϵ) and np.any(r > ri + ϵ) then end for initialize kept sequences with non-dominated candidate keep {(X remove any sequence dominated by the candidate sequence for (X , ri)} , r) do if np.all(ri ϵ) and np.any(ri > + ϵ) then continue end if keep.append(X , r) 43 7: 8: 9: 10: 11: 12: 13: 14: end for else scores {} for 15: 16: 17: 18: 19: 20: 21: 22: 23: end if 24: 25: end while else end if Select(Xselected) else if = 0 then Select(X0) return Xt , ri)} return end if {(X 2: if is empty then 3: 4: else 5: 6: 7: 8: 9: 10: 11: 12: 13: 14: 15: 16: 17: 18: 19: 20: 21: 22: end if end for keep return P"
        }
    ],
    "affiliations": [
        "Department of Bioengineering, University of Pennsylvania",
        "Department of Computer and Information Science, University of Pennsylvania",
        "School of Mathematics, Georgia Institute of Technology"
    ]
}