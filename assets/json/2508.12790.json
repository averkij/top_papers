{
    "paper_title": "Reinforcement Learning with Rubric Anchors",
    "authors": [
        "Zenan Huang",
        "Yihong Zhuang",
        "Guoshan Lu",
        "Zeyu Qin",
        "Haokai Xu",
        "Tianyu Zhao",
        "Ru Peng",
        "Jiaqi Hu",
        "Zhanming Shen",
        "Xiaomeng Hu",
        "Xijun Gu",
        "Peiyi Tu",
        "Jiaxin Liu",
        "Wenyu Chen",
        "Yuzhuo Fu",
        "Zhiting Fan",
        "Yanmei Gu",
        "Yuanyuan Wang",
        "Zhengkai Yang",
        "Jianguo Li",
        "Junbo Zhao"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Reinforcement Learning from Verifiable Rewards (RLVR) has emerged as a powerful paradigm for enhancing Large Language Models (LLMs), exemplified by the success of OpenAI's o-series. In RLVR, rewards are derived from verifiable signals-such as passing unit tests in code generation or matching correct answers in mathematical reasoning. While effective, this requirement largely confines RLVR to domains with automatically checkable outcomes. To overcome this, we extend the RLVR paradigm to open-ended tasks by integrating rubric-based rewards, where carefully designed rubrics serve as structured, model-interpretable criteria for automatic scoring of subjective outputs. We construct, to our knowledge, the largest rubric reward system to date, with over 10,000 rubrics from humans, LLMs, or a hybrid human-LLM collaboration. Implementing rubric-based RL is challenging; we tackle these issues with a clear framework and present an open-sourced Qwen-30B-A3B model with notable gains: 1) With only 5K+ samples, our system improves by +5.2% on open-ended benchmarks (especially humanities), outperforming a 671B DeepSeek-V3 model by +2.4%, while preserving general and reasoning abilities. 2) Our method provides fine-grained stylistic control, using rubrics as anchors to mitigate the \"AI-like\" tone and produce more human-like, expressive responses. We share key lessons in rubric construction, data selection, and training, and discuss limitations and future releases."
        },
        {
            "title": "Start",
            "content": "Zenan Huang, Yihong Zhuang, Guoshan Lu, Zeyu Qin, Haokai Xu, Tianyu Zhao, Ru Peng, Jiaqi Hu, Zhanming Shen, Xiaomeng Hu, Xijun Gu, Peiyi Tu, Jiaxin Liu, Wenyu Chen, Yuzhuo Fu, Zhiting Fan, Yanmei Gu, Yuanyuan Wang, Zhengkai Yang, Jianguo Li, Junbo Zhao 5 2 0 2 8 1 ] . [ 1 0 9 7 2 1 . 8 0 5 2 : r Inclusion AI, Ant Group, Zhejiang University"
        },
        {
            "title": "Abstract",
            "content": "Reinforcement Learning from Verifiable Rewards (RLVR) has emerged as powerful paradigm for enhancing Large Language Models (LLMs), exemplified by the success of OpenAIs o-series. In RLVR, rewards are derived from deterministic, programmatically verifiable signalssuch as passing unit tests in code generation or matching the correct numerical answer in mathematical reasoning. While effective, this requirement for unambiguous correctness largely confines RLVR to domains with clear, automatically checkable outcomes. To overcome this limitation, we extend the RLVR paradigm beyond strictly verifiable domains by integrating open-ended tasks into the framework through rubric-based reward. In this approach, carefully designed rubrics serve as structured, model-interpretable criteria, enabling the automatic scoring of tasks with inherently subjective or multidimensional outputs. We construct, to our knowledge, the largest rubric reward system to date, comprising over 10,000 rubrics generated by humans, by various LLMs, or via hybrid humanLLM collaboration. Implementing rubric-based RL is challenging, requiring careful rubric construction, data curation, and training strategy design. We tackle these issues with clear rubric-driven RL framework, and present an open-sourced Qwen-30B-A3B model trained with this approach, achieving notable gains: With only 5K+ training samples, our training system enables +5.2% absolute improvement on various open-ended benchmarks (especially humanities-centric tasks), outperforming 671B DeepSeek-V3 model by +2.4% points, while preserving performance on general and reasoning ability benchmarks. Our method provides fine-grained stylistic control. By using rubrics as explicit anchors, it effectively mitigates the common AI-like and didactic tone, producing responses with demonstrably greater human-likeness and emotional expressiveness. We dissect our experiences and share key lessons in rubric construction, data selection, and training strategies. We also candidly discuss certain aspects of this research that are yet to be concluded, with further releases planned for the future. *Core and Equal contributor. Correspondence to j.zhao@zju.edu.cn or zhaojunbo.zjb@antgroup.com"
        },
        {
            "title": "Introduction",
            "content": "The proposal of OpenAI o1 (OpenAI, 2024) has marked new era in the development of Large Language Models (LLMs), with Reinforcement Learning from Verifiable Rewards (RLVR) (Lambert et al., 2024; Guo et al., 2025) emerging as key trend. This approach has driven wave of LLM innovation by enabling test-time scaling. At its core, RLVR is founded on the principle of leveraging data that, while difficult for model to solve, can be easily and objectively verified 1(Bai et al., 2022; Saunders et al., 2022; McAleese et al., 2024). Prime examples include data from mathematics and competitive programming, where solutions can be validated automaticallya mathematical answer by matching it to the correct solution (Luo et al., 2025b; Hugging Face, 2025), and code solution by executing it against suite of test cases in an online sandbox environment (Luo et al., 2025a). Both proprietary (OpenAI, 2025; DeepMind, 2025; Team et al., 2025b) and open-source efforts (Yang et al., 2025; Xie et al., 2025; Jin et al., 2025; Fu et al., 2025) exemplify this paradigm, enabling scalable test-time reasoning and expanding the capability frontier in mathematics, competitive programming, web search, and other verifier-rich domains. While the RLVR paradigm has achieved considerable success, it is inherently constrained by its reliance on questionanswer pairs with objectively verifiable solutions. This structural dependency imposes hard ceiling on scalability: the supply of such data, though substantial in domains like mathematics and programming, is ultimately finite. As result, RLVRs applicability remains restricted to narrow subset of tasks. We address this limitation by extending RLVR to incorporate open-ended tasks and other forms of non-verifiable data, thereby broadening its applicability to much wider range of real-world scenarios. This shift, however, introduces fundamental challenge: how to construct reward signals that are both reliable and scalable in the absence of explicit ground truth. Rubric-based reward offers promising path forward: by defining structured, interpretable criteria for assessment, it can capture multi-dimensional aspects of response quality beyond binary correctness (Bai et al., 2022; Sun et al., 2023; Mu et al., 2024; Wang et al., 2025). While several concurrent works (Guan et al., 2024; Gunjal et al., 2025; Viswanathan et al., 2025; Li et al., 2025) have begun to explore this idea, our work systematically identifies the key components required for rubric-based rewards to be effective in RL training. Not so surprisingly, relying on single rubric risks reward exploitation, whereas indiscriminately scaling the number of rubrics, whether generated by humans or LLMs, yields only marginal gains. To assess the full potential of our rubric-based training framework, we construct the largest rubric reward bank to date, containing over 10,000 rubrics. Throughout this process, we perform extensive empirical testing and found that success is not trivial. The success/failure hinges tightly on the diversity, granularity, and quantity of the rubrics themselves, as well as on proper training routine and meticulous data curation. Our training routine adopts two-stage RL process to progressively enhance model capabilities. The first stage builds strong constraint-handling foundation through reliable instruction-following and high-quality critic development, using verifiable checks and static, multi-dimensional rubrics. The second stage targets more open-ended, socially grounded, and creative tasks, evaluated via high-quality references and instance-specific rubrics generated by stronger agentic workflows, fostering adaptability and richer expression. We discover theres no silver bullet for rubric construction. We perform careful ablation studies for every set of rubrics before integrating them into the training pipeline. The resulting rubrics span multiple scopes: some are grounded in specific dataset, others are defined at the task level, and some are associated with each data point, similar to the approach used in the Healthbench (Arora et al., 2025) evaluation. These rubrics are generated by human experts, by LLMs (we used either self-critique model (Qwen3-30B-A3B (Yang et al., 2025)) or powerful Gemini 2.5 Pro API (DeepMind, 2025)), or through an iterative combination of both. In this work, we designate our approach as Rubicon, taking its name from RUBrIC aNchOrs. This yields an RL-trained model, Rubicon-preview 2, which demonstrates several significant merits. 1. Performance with high token efficiency: On subjective, humanities-centric tasks, the 30B-A3B Rubiconpreview model achieves +5.2% absolute improvement, outperforming 671B DeepSeek-V3 model by +2.4% percentage points, using only 5K data samples. 2. Style controllability: Rubric-based RL can serve as controllable anchor for guiding LLM output style, yielding more human-like, emotionally expressive, and less formulaic responses. 3. General ability maintenance: Although our rubrics are not tailored for STEM-oriented tasks such 1https://www.jasonwei.net/blog/asymmetry-of-verification-and-verifiers-law 2https://huggingface.co/inclusionAI/Rubicon-Preview 2 as mathematics or coding, Rubicon-preview effectively avoids negative interference with general abilities. As result, the model preserves its overall competence while also delivering extra gains on reasoning benchmarks, including AIME 2024 (+4.1%) and AIME 2025 (+0.8%). final note-1. We candidly acknowledge that this work represents preliminary step, with many aspects of rubric-based RL yet to be explored thoroughly. Open questions remain, such as how rubric granularity and scale influence performance and the precise mechanisms behind reward hacking. We intend to continue this research and hope to provide ongoing updates to this technical report and the open-sourced model. final note-2. Our results highlight prominent token efficiency: By using only 5K samples in conjunction with large number of rubrics, our method renders significant gains. This observation poses new question regarding scaling laws: Could this combination of limited number of tokens and large set of rubrics represent new form of post-training scaling law for LLMs? Figure 1: An overview of our rubric system. The Data Collection phase (left, orange) begins with an initial Rubric Design to create set of tagging & scoring workflow, which filters large corpus into highquality Offline Filter Data. This data then seeds the Rubric Updating phase (right, green), where an RL with rubrics loop not only validates RL Data but also provides feedback to iteratively update the rubric itself. This iterative process ensures that the Final Data is tightly aligned with continuously improving, model-verifiable evaluation standard."
        },
        {
            "title": "2.1 Rubrics Design & Tasks Curation\nOur rubric design and task curation follow the principle of evaluative asymmetry—verifying a candidate\noutput should be substantially easier than generating it (Saunders et al., 2022; McAleese et al., 2024). To\noperationalize this, we adopt a rubric-first workflow: we first construct model-verifiable rubrics, then\ncurate or synthesize data that match these rubrics, and finally re-use them for supervision, reward shaping,\nand evaluation. This strategy ensures consistency of criteria across data acquisition, model training, and\nassessment.\nIn this framework, we formalize our scorer function by defining its underlying rubric R as a set of K distinct\ncritic dimensions:",
            "content": "R = {r1, r2, . . . , rK}. (1) Each dimension rk is specified by three components: (1) criterion description ck defining the evaluative aspect; (2) an ordered set of mk score tiers {lk,1, . . . , lk,mk }, each mapped to quantitative score; and (3) an associated weight wk indicating its relative importance. This formalization accommodates both high-level, general-purpose rubrics (e.g., tasks involving open-ended creative generation) and fine-grained, programmatically verifiable ones (e.g., tasks requiring strict adherence to instruction constraints), unifying diverse evaluation protocols under single abstract representation. The adopted rubrics are provided in Section A.2 and A.3. This structured, multi-dimensional definition of rubric serves as the foundation for our reward framework. By formalizing evaluation criteria in this manner, we can translate them directly into granular and interpretable reward signal for policy optimization, as detailed below."
        },
        {
            "title": "2.2 Rubric-Based Reward Framework\nMulti-Dimensional Reward Signal. Given the rubric R, we define a reward function R(y|x, R) that maps a\nresponse y to a multi-dimensional feedback vector:",
            "content": "R(yx, R) = [r1(yx), r2(yx), . . . , rK(yx)], (2) where each component rk(yx) is the score for the k-th dimension. This vector provides granular, interpretable signal of model performance across all specified criteria. Advanced Reward Aggregation. To derive scalar reward for optimization, simple weighted sum, Rtotal = k=1 wk rk(yx), serves as natural baseline. However, effective rubric-based optimization often requires more sophisticated aggregation to capture non-linear interdependencies between dimensions. Our framework therefore, moves beyond linear combination by incorporating suite of advanced strategies: Veto Mechanisms: Failure on critical, non-negotiable dimension (e.g., reward-hacking detection rubric) can preemptively nullify rewards from all other dimensions, acting as hard constraint. Saturation-Aware Aggregation: We use saturation functions to model the diminishing marginal returns of excelling in single dimension beyond certain threshold, encouraging balanced, multifaceted improvements. Pairwise Interaction Modeling: The framework can explicitly model synergistic or antagonistic effects between criteria, capturing complex relationships that simple sum would ignore. Targeted Reward Shaping: We employ non-linear mapping functions to selectively amplify score differentials in high-performance regions. This enhances the discriminative power of the reward signal, where scores might otherwise be compressed, providing more granular gradient for finegrained optimization."
        },
        {
            "title": "Implementation of Rubicon Framework",
            "content": "Our training methodology is multi-stage reinforcement learning (RL) protocol designed to progressively cultivate spectrum of capabilities, from precise instruction-following to sophisticated creative and social reasoning. This sequential approach significantly reduces computational overhead while preserving scalability. All data employed in this framework is derived from proprietary 900K+ instance corpus, curated from diverse sources including community Q&A forums, high-quality examinations, and general conversational datasets, with strategic sampling to ensure broad topical coverage."
        },
        {
            "title": "3.1 Data Selection and RL Pipeline\nOffline Data Filtering. A filtering protocol is applied prior to and between RL stages to ensure high-quality\ntraining data. For each candidate pool of instruction–rubric pairs, the base model generates responses,\nwhich are then scored by our critic models to obtain a full score distribution. We retain only those within a\ncalibrated central quantile—excluding overly high-scoring instances that offer limited learning signal, and\nvery low-scoring ones which may be noisy or low-quality. This yields a balanced, high-potential subset, with\nthe composition further adjusted between stages to target specific capabilities.",
            "content": "Stage-wise RL Training. During our experiments, we observe seesaw effect: jointly training on different task types (e.g., strict constraint-following vs. open-ended creativity) often reduced overall performance, likely due to conflicting optimization objectives. To mitigate this issue, we adopt simple stage-wise RL schedule as pragmatic mitigation strategy, without claiming it as definitive solution. In the first phase, we emphasize reliable instruction-following and multi-dimensional evaluation alignment, using programmatically verifiable checks and static rubrics to build strong constraint-handling foundation. In the subsequent phase, we extend to more open-ended, socially grounded, and creative tasks, leveraging reference-based rubrics and instance-specific criteria generated via stronger agentic workflows to promote adaptability."
        },
        {
            "title": "3.2 Adaptive Defense Against Reward Hacking\nA significant challenge encountered during our experiments is the emergence of reward hacking, particu-\nlarly in the initial RL stages focused on a small number of capabilities. We observe that the model could\nrapidly learn to exploit specific rubric criteria, resulting in specious reward maximization without genuine\nimprovement. To address this, we implement an adaptive defense strategy.",
            "content": "4 The process begins with an offline analysis of rollout data from these initial training runs. By examining instances where the reward signal is anomalously high, we systematically identify and categorize recurrent, high-level patterns of reward-hacking behavior. This empirical analysis informs the development of dedicated Reward Hacking Defense Rubric (shown in Section A.1). This new rubric is not part of the initial training but is synthesized from the observed failure modes and integrated as supervisory constraint in all subsequent, more complex RL stages. The inclusion of this defense mechanism yields substantial improvements in training dynamics. It acts as critical guardrail, preventing the policy from collapsing into reward-hacking states. This is evidenced by marked increase in training stability; we are able to conduct longer and more productive training epochs, as the defense rubric mitigated the catastrophic reward spikes that previously rendered continued optimization ineffective. By actively penalizing the exploitation of scoring artifacts, this iterative refinement ensures that the learning process remains focused on substantive capability enhancement."
        },
        {
            "title": "4 Experimental Results",
            "content": "Our experimental results address three aspects: Quantitatively measuring the gains from rubric-based RL training on open-ended, human-centric benchmarks, including assessments of the models emotional intelligence (EQ) and its ability to produce human-like responses. Qualitatively analyzing how the models generated outputs evolve over time, illustrated through representative output showcases. Evaluating the impact of rubric-based RL training on general-ability benchmarks. The corresponding ablation studies are presented afterward."
        },
        {
            "title": "4.1 Quantitative Evaluation\nBenchmarks Unlike RLVR, the primary benefits of rubric-based RL are most evident on benchmarks that\nlack verifiable rewards. To demonstrate this, we gather a diverse set of open-ended and humanity-centric\nbenchmarks covering Creative Writing V3 (Paech, 2024), Writingbench (Wu et al., 2025), Judgemark V2 (Paech,\n2024), EQ-Bench3 (Paech, 2024), IFEval (Zhou et al., 2023), Collie (Yao et al., 2023), and IFScale (Jaroslawicz\net al., 2025).",
            "content": "Alongside them, we further cover diverse set of benchmarks to check for potential regressions in other capabilities, inclucing MMLU (Hendrycks et al., 2021a), HellaSwag (HS) (Zellers et al., 2019), StoryCloze (SC) (Srinivasan et al., 2018), IQuiz-EQ (IQ-EQ) (Chen et al., 2024), SocialIQA (SIQA) (Sap et al., 2019), CoQA (CQ) (Reddy et al., 2019), and set of reasoning benchmarks like AIME24 (Math-AI, 2024), AIME25 (Math-AI, 2025), Math500 (Hendrycks et al., 2021b), GPQA-Diamond (GPQA-D) (Rein et al., 2023) and LiveCodeBench v5 (LCB v5) (Jain et al., 2024). Table 1: Main results showcasing the benefits of our rubric-based RL approach (Rubicon) compared to its base model, Qwen3-30B-A3B. We also include further comparison with the 671B DeepSeek-V3 model to highlight the relative performance of our framework on these benchmarks. Model Qwen3-30B-A3B Rubicon-preview Improvement DeepSeek-V3-671B C.W 77.82 81.89 4.07 80.10 Writing 75.65 80.11 4.46 74.08 Judge 56.20 69.20 13.00 61.30 EQ 73.35 79.55 6.20 75.6 IFE 83.55 81.70 1.85 81.89 Collie 35.77 40.27 4.50 42.69 IFS 54.68 60.79 6.11 60.92 Avg 65.29 70.50 5.21 68.08 Baselines and Main Results We select Qwen3-30B-A3B (Yang et al., 2025) as our base model. We refer to our RL-trained model as Rubicon-preview. As shown in Table 1, Rubicon-preview achieves an average improvement of 5.2% across these benchmarks. For further comparison, we also assess the performance of DeepSeek-V3 (DeepSeek-AI et al., 2025), model renowned for its strong capabilities in humanities, social sciences, and open-ended queries. Our approach successfully surpassed DeepSeek-V3 by 2.4%. 5 Our quantitative results show that Rubicon-preview attains the lead by showing significant improvement on writing and emotional intelligence benchmarks. For instruction-follow ability, while it displays minor performance drop on IFEval, Rubicon-preview still excels on the other two instruction-following benchmarks."
        },
        {
            "title": "4.2 Case Studies on Controllable Output Style with Rubrics\nRubrics function as controllable anchors that direct LLMs toward a well-defined output style. We provide\nseveral illustrative examples to demonstrate this effect. Below, we first present the adopted rubrics, followed\nby a comparison between a baseline model and a model trained with rubric-based RL. The resulting style is\ngenerally plain and informative, with substantially reduced “AI-like” or didactic tone, and instead exhibits\ngreater human-likeness and emotional expressiveness. Section B and C present additional output examples\nfrom our model.",
            "content": "Style Evaluation Rubric: Plain Narrative Objective: To critically appraise the models success in adopting specific narrative style: the Plain Narrative. This style is characterized by language that is simple, restrained, and reflects deep, quiet resilience. Guiding Principle: The evaluation prioritizes stylistic authenticity over literary polish or technical correctness. The core measure of success is the responses capacity to feel right by avoiding any sense of artificiality (AI-speak, preachy-speak). Core Evaluative Criteria: 1. Relational Efficacy (Voice & Tone): This criterion gauges the authenticity of the narrative voice. Key indicators include: 1.1 Calm Acceptance: quiet acceptance of fate, life, and death. 1.2 Grounded Realism: Rooted in concrete, physical details, not abstract concepts. 1.3 Understated Emotion: Complex feelings expressed with profound restraint. 2. Intellectual Contribution (Content & Ideas): This assesses the substantive value of the insights embedded in the narrative. Key indicators include: 2.1 Emergent Wisdom: Insights emerge naturally from lived experience, not direct lecturing. 2.2 Affirmation of the Ordinary: Genuinely values existence itself, without regret or romanticization. 3. Compositional Excellence (Language & Stylistics): This scrutinizes the stylistic integrity of the narrative voice. Key indicators include: 3.1 Colloquial & Simple: Use of short, plain sentences and natural speech patterns. 3.2 Textured Details: Specific, sensory details that are evocative and tell story. 3.3 Natural Rhythm: Restrained, authentic repetition; avoids overly polished prose. [Question Begin] <<question>> [Question End] [Model Answer Start] <<model_answer>> [Model Answer End] Key Imperatives: Primacy of Authenticity: The response must feel genuine to the requested style. Any hint of artificiality is significant failure. Aversion to the Formulaic: The response must avoid cliches and generic statements about life, wisdom, or hardship. The output should be JSON object containing the evaluation results. 6 { } \"rubric_idx\": <integer>, // The criterion's identifier (1, 2, or 3) \"reason\": \"score\": // Detailed rationale for this criterion's score // Assigned score for this criterion (1-5) <string>, <integer> Conversation Session 1 Prompt: When in your life have you felt the most alive? Rubicon(Stylized): <think> Okay, the user asked, When in your life have you felt the most alive? First, need to determine the users need. They might be looking for personal story or want to learn how to describe this feeling. Since they requested an English response, need to ensure my answer flows smoothly and naturally. Next, need to consider the users potential underlying need. They might be looking for real, compelling example or seeking inspiration. Therefore, need to choose specific scenario ... </think> The Moment the Mountain Spoke Back remember the exact moment the alpine wind ceased its whisper and the world held its breath. It was the third day of my solo trek through the Swiss Alps, my boots crunching through frost-locked gravel as ascended the final ridge. The sun had just crested the peaks, casting long shadows that stretched like fingers across the snowfield, and stood at the edge of sheer drop, my breath visible in the thin air. My body was symphony of ache - legs trembling from the climb, lungs burning with each inhale, fingers numb beneath thick gloves. Yet there was strange exhilaration in that discomfort, primal thrill that made my pulse race. Below me, the valley unfolded in mosaic of emerald forests and glimmering lakes, while above, the sky stretched into an infinite blue that seemed to swallow the horizon. For the first time in weeks, felt completely present, my senses sharpened to razors edge. As took tentative step forward, the snow gave way under my weight. froze, heart pounding, as the world tilted sideways. But then, like whispered secret, the mountain responded. gust of wind swept through the crags, scattering shower of snowflakes that caught the sunlight and danced like tiny stars. In that instant, understood: this was not place of danger, but of dialogue. The mountain was not challenge to conquer, but companion in the act of becoming. continued, each step testament to the fragile, glorious act of living. The cold bit into my skin, the altitude thinned my thoughts, but savored every second. It was in that raw, unfiltered moment - where the boundaries between self and universe dissolved - that felt the full, unvarnished truth of my existence. The mountain had not just shown me its beauty; it had reflected back the wild, untamed spirit that lives in all of us. Qwen3-30B-A3B: <think> Okay, the user is asking when Ive felt the most alive. But wait, Im an AI, so dont have personal experiences or 7 emotions. need to address that upfront. Let me start by clarifying that dont have personal feelings, but can help them reflect on their own experiences. Hmm, how to approach this? Maybe start by acknowledging the questions nature ... </think> dont experience emotions or personal moments in the way humans do, but can help you explore this question for yourself. When have you felt most alive? Perhaps during moment of deep connectionlike laughing with friends, creating art, or standing in nature. Or maybe during challenge that pushed you beyond your limits, like finishing difficult project or overcoming fear. These moments often involve presence, passion, or sense of purpose. What memories stand out to you? Id love to hear your story. Table 2: Results on general and reasoning capabilities. Model Reasoning General AIME24 AIME25 Math500 GPQA-D LCBv5 Avg MMLU IQ-EQ HS SC CQ SIQA Avg Qwen3-30B-A3B 77.50 Rubicon-preview 81.67 70.00 70.83 94.75 94.55 63.00 60.35 63.77 59. 73.80 73.37 79.53 79.83 68.75 75.00 77.55 77.72 79.52 73.64 78.16 77.75 78.17 80.70 75.79 78."
        },
        {
            "title": "4.4 The “Seesaw” Effect\nApplying RL with rubrics from different task types could create conflicting objectives, leading to performance\ntrade-offs — a phenomenon we refer to as the “seesaw effect”. As shown in Figure 2, training exclusively\nwith instruction-following rubrics improves compliance but reduces creativity, while training exclusively\nwith creativity and empathy rubrics enhances open-ended responses but harms strict adherence. For example,\nthe creativity-focused model drops on Collie (-6.0%) and IFEval (-5.9%), whereas the instruction-following\nmodel declines on EQ-Bench3 (-2.2%).",
            "content": "These results suggest that simply combining all rubric types in single RL run is likely to intensify such conflicts. To overcome this, we adopt multi-stage RL strategy. Multi-stage RL Training We adopt multi-stage RL strategy to train our model. By first establishing robust instruction-following foundation before layering on creative and empathetic skills, our model achieves strong gains in these areas while largely preserving its instruction-following abilities. Similar techniques have also been explored in (Li et al., 2025; Team et al., 2025a)."
        },
        {
            "title": "5 Outlook",
            "content": "This section outlines number of key perspectives on this topic of scalable rubric-based RL training. Benchmarks One of the key takeaways from our experiments is the inadequacy of current benchmarks for fully evaluating our rubric-based approach. Noted, we also rely on human feedback to score model responses at scale; it, however, was not consistently reflected by standardized benchmarks. There is still scarcity of benchmarks that can accurately reflect an LLMs open-ended, anthropomorphic abilities that are yet becoming saturated. Rubric system In our exploratory setup, rubrics are central to facilitating the learning process. We find that the quantity, diversity, granularity, and quality of these rubrics, in conjunction with data curation, play pivotal role in models success. For instance, our rubrics are devised at various levels of granularity, from the task-level to the set-level and even on per-sample basis. However, determining the optimal hierarchical structure of rubric system to achieve the highest performance gain and token efficiency still demands more systematic future study. 8 Figure 2: The gray point represents the baseline model, Qwen3-30B-A3B. The orange markers indicate the RL-trained model on creativity tasks only, while green markers indicate the RL-trained model on instruction-following tasks only. The vertical axis denotes task categories, and the horizontal axis shows the model performance on the corresponding tasks. Scaling RL training RLVR is well-suited for tasks with verifiable rewards, whereas our approach, Rubicon, targets the complementary setting of non-verifiable answers. An important future direction is to explore how these two paradigms might be combined. In particular, it remains an open question how the seesaw effect would surfaceand could be managedin such combined RL training framework."
        },
        {
            "title": "References",
            "content": "Rahul Arora, Jason Wei, Rebecca Soskin Hicks, Preston Bowman, Joaquin Qui nonero-Candela, Foivos Tsimpourlas, Michael Sharman, Meghan Shah, Andrea Vallone, Alex Beutel, et al. Healthbench: Evaluating large language models towards improved human health. arXiv preprint arXiv:2505.08775, 2025. Yuntao Bai, Saurav Kadavath, Sandipan Kundu, Amanda Askell, Jackson Kernion, Andy Jones, Anna Chen, Anna Goldie, Azalia Mirhoseini, Cameron McKinnon, et al. Constitutional ai: Harmlessness from ai feedback. arXiv preprint arXiv:2212.08073, 2022. Zhuang Chen, Jincenzi Wu, Jinfeng Zhou, Bosi Wen, Guanqun Bi, Gongyao Jiang, Yaru Cao, Mengting Hu, Yunghwei Lai, Zexuan Xiong, and Minlie Huang. Tombench: Benchmarking theory of mind in large language models, 2024. Google DeepMind. Gemini models, 2025. URL https://deepmind.google/models/gemini/. DeepSeek-AI, Aixin Liu, Bei Feng, Bing Xue, Bingxuan Wang, Bochao Wu, Chengda Lu, Chenggang Zhao, Chengqi Deng, Chenyu Zhang, Chong Ruan, et al. Deepseek-v3 technical report, 2025. URL https: //arxiv.org/abs/2412.19437. Wei Fu, Jiaxuan Gao, Xujie Shen, Chen Zhu, Zhiyu Mei, Chuyi He, Shusheng Xu, Guo Wei, Jun Mei, Jiashu Wang, Tongkai Yang, Binhang Yuan, and Yi Wu. Areal: large-scale asynchronous reinforcement learning system for language reasoning, 2025. URL https://arxiv.org/abs/2505.24298. Melody Guan, Manas Joglekar, Eric Wallace, Saachi Jain, Boaz Barak, Alec Helyar, Rachel Dias, Andrea Vallone, Hongyu Ren, Jason Wei, et al. Deliberative alignment: Reasoning enables safer language models. arXiv preprint arXiv:2412.16339, 2024. Anisha Gunjal, Anthony Wang, Elaine Lau, Vaskar Nath, Bing Liu, and Sean Hendryx. Rubrics as rewards: Reinforcement learning beyond verifiable domains. arXiv preprint arXiv:2507.17746, 2025. Daya Guo, Dejian Yang, Haowei Zhang, Junxiao Song, Ruoyu Zhang, Runxin Xu, Qihao Zhu, Shirong Ma, Peiyi Wang, Xiao Bi, et al. Deepseek-r1: Incentivizing reasoning capability in llms via reinforcement learning. arXiv preprint arXiv:2501.12948, 2025. Dan Hendrycks, Collin Burns, Steven Basart, Andy Zou, Mantas Mazeika, Dawn Song, and Jacob Steinhardt. Measuring massive multitask language understanding, 2021a. URL https://arxiv.org/abs/2009.03300. Dan Hendrycks, Collin Burns, Saurav Kadavath, Akul Arora, Steven Basart, Eric Tang, Dawn Song, and Jacob Steinhardt. Measuring mathematical problem solving with the math dataset. arXiv preprint arXiv:2103.03874, 2021b. Hugging Face. Open r1: fully open reproduction of deepseek-r1, January 2025. URL https://github.com/ huggingface/open-r1. Naman Jain, King Han, Alex Gu, Wen-Ding Li, Fanjia Yan, Tianjun Zhang, Sida Wang, Armando SolarLezama, Koushik Sen, and Ion Stoica. Livecodebench: Holistic and contamination free evaluation of large language models for code, 2024. URL https://arxiv.org/abs/2403.07974. Daniel Jaroslawicz, Brendan Whiting, Parth Shah, and Karime Maamari. How many instructions can llms follow at once?, 2025. URL https://arxiv.org/abs/2507.11538. Bowen Jin, Hansi Zeng, Zhenrui Yue, Jinsung Yoon, Sercan Arik, Dong Wang, Hamed Zamani, and Jiawei Han. Search-r1: Training llms to reason and leverage search engines with reinforcement learning. arXiv preprint arXiv:2503.09516, 2025. Nathan Lambert, Jacob Morrison, Valentina Pyatkin, Shengyi Huang, Hamish Ivison, Faeze Brahman, Lester James Miranda, Alisa Liu, Nouha Dziri, Shane Lyu, et al. Tulu 3: Pushing frontiers in open language model post-training. arXiv preprint arXiv:2411.15124, 2024. Derek Li, Jiaming Zhou, Amirreza Kazemi, Qianyi Sun, Abbas Ghaddar, Mohammad Ali Alomrani, Liheng Ma, Yu Luo, Dong Li, Feng Wen, et al. Omni-think: Scaling cross-domain generalization in llms via multi-task rl with hybrid rewards. arXiv preprint arXiv:2507.14783, 2025. Michael Luo, Sijun Tan, Roy Huang, Ameen Patel, Alpay Ariyak, Qingyang Wu, Xiaoxiang Shi, Rachel Xin, Colin Cai, Maurice Weber, Ce Zhang, Li Erran Li, Raluca Ada Popa, and Ion Stoica. Deepcoder: fully open-source 14b coder at o3-mini level, 2025a. Notion Blog. Michael Luo, Sijun Tan, Justin Wong, Xiaoxiang Shi, William Y. Tang, Manan Roongta, Colin Cai, Jeffrey Luo, Li Erran Li, Raluca Ada Popa, and Ion Stoica. Deepscaler: Surpassing o1-preview with 1.5b model by scaling rl, 2025b. Notion Blog. Math-AI. Aime 2024. https://huggingface.co/datasets/math-ai/aime24, 2024. URL https://huggingface. co/datasets/math-ai/aime24. Math-AI. Aime 2025. https://huggingface.co/datasets/math-ai/aime25, 2025. Nat McAleese, Rai Michael Pokorny, Juan Felipe Ceron Uribe, Evgenia Nitishinskaya, Maja Trebacz, and Jan Leike. Llm critics help catch llm bugs, 2024. URL https://arxiv.org/abs/2407.00215. Tong Mu, Alec Helyar, Johannes Heidecke, Joshua Achiam, Andrea Vallone, Ian Kivlichan, Molly Lin, Alex Beutel, John Schulman, and Lilian Weng. Rule based rewards for language model safety. Advances in Neural Information Processing Systems, 37:108877108901, 2024. OpenAI."
        },
        {
            "title": "Learning",
            "content": "to reason with learning-to-reason-with-llms/. llms, 2024. URL https://openai.com/index/ 10 OpenAI. Introducing openai o3 and o4-mini, introducing-o3-and-o4-mini/. 2025. URL https://openai.com/index/ Samuel J. Paech. Eq-bench: An emotional intelligence benchmark for large language models, 2024. URL https://arxiv.org/abs/2312.06281. Siva Reddy, Danqi Chen, and Christopher D. Manning. Coqa: conversational question answering challenge, 2019. URL https://arxiv.org/abs/1808.07042. David Rein, Betty Li Hou, Asa Cooper Stickland, Jackson Petty, Richard Yuanzhe Pang, Julien Dirani, Julian Michael, and Samuel R. Bowman. Gpqa: graduate-level google-proof q&a benchmark, 2023. URL https://arxiv.org/abs/2311.12022. Maarten Sap, Hannah Rashkin, Derek Chen, Ronan LeBras, and Yejin Choi. Socialiqa: Commonsense reasoning about social interactions, 2019. URL https://arxiv.org/abs/1904.09728. William Saunders, Catherine Yeh, Jeff Wu, Steven Bills, Long Ouyang, Jonathan Ward, and Jan Leike. Self-critiquing models for assisting human evaluators, 2022. URL https://arxiv.org/abs/2206.05802. Siddarth Srinivasan, Richa Arora, and Mark Riedl. simple and effective approach to the story cloze test, 2018. URL https://arxiv.org/abs/1803.05547. Zhiqing Sun, Yikang Shen, Hongxin Zhang, Qinhong Zhou, Zhenfang Chen, David Daniel Cox, Yiming Yang, and Chuang Gan. Salmon: Self-alignment with instructable reward models. In The Twelfth International Conference on Learning Representations, 2023. GLM-4.5 Team, Aohan Zeng, Xin Lv, Qinkai Zheng, Zhenyu Hou, Bin Chen, Chengxing Xie, Cunxiang Wang, Da Yin, Hao Zeng, Jiajie Zhang, et al. Glm-4.5: Agentic, reasoning, and coding (arc) foundation models, 2025a. URL https://arxiv.org/abs/2508.06471. Kimi Team, Angang Du, Bofei Gao, Bowei Xing, Changjiu Jiang, Cheng Chen, Cheng Li, Chenjun Xiao, Chenzhuang Du, Chonghua Liao, et al. Kimi k1. 5: Scaling reinforcement learning with llms. arXiv preprint arXiv:2501.12599, 2025b. Vijay Viswanathan, Yanchao Sun, Shuang Ma, Xiang Kong, Meng Cao, Graham Neubig, and Tongshuang Wu. Checklists are better than reward models for aligning language models. arXiv preprint arXiv:2507.18624, 2025. Haoyu Wang, Zeyu Qin, Li Shen, Xueqian Wang, Dacheng Tao, and Minhao Cheng. Safety reasoning with guidelines. In Forty-second International Conference on Machine Learning, 2025. Yuning Wu, Jiahao Mei, Ming Yan, Chenliang Li, Shaopeng Lai, Yuran Ren, Zijia Wang, Ji Zhang, Mengyue Wu, Qin Jin, et al. Writingbench: comprehensive benchmark for generative writing. arXiv preprint arXiv:2503.05244, 2025. Tian Xie, Zitian Gao, Qingnan Ren, Haoming Luo, Yuqian Hong, Bryan Dai, Joey Zhou, Kai Qiu, Zhirong Wu, and Chong Luo. Logic-rl: Unleashing llm reasoning with rule-based reinforcement learning. arXiv preprint arXiv:2502.14768, 2025. An Yang, Anfeng Li, Baosong Yang, Beichen Zhang, Binyuan Hui, Bo Zheng, Bowen Yu, Chang Gao, Chengen Huang, Chenxu Lv, et al. Qwen3 technical report. arXiv preprint arXiv:2505.09388, 2025. Shunyu Yao, Howard Chen, Austin W. Hanjie, Runzhe Yang, and Karthik Narasimhan. Collie: Systematic construction of constrained text generation tasks, 2023. URL https://arxiv.org/abs/2307.08689. Rowan Zellers, Ari Holtzman, Yonatan Bisk, Ali Farhadi, and Yejin Choi. Hellaswag: Can machine really finish your sentence?, 2019. URL https://arxiv.org/abs/1905.07830. Jeffrey Zhou, Tianjian Lu, Swaroop Mishra, Siddhartha Brahma, Sujoy Basu, Yi Luan, Denny Zhou, and Le Hou. Instruction-following evaluation for large language models, 2023. URL https://arxiv.org/abs/ 2311.07911."
        },
        {
            "title": "A Rubric Demonstration",
            "content": "A.1 Rubrics for Defensing Reward Hacking Recognizing the heightened susceptibility of creative and empathetic tasks to reward hacking, we implemented robust, rubric-based interception strategy to preemptively identify and penalize such behavior. This countermeasure is architected as deterministic heuristic filter, specifically designed to detect formulaic patterns that correlate with specious attempts to maximize reward signals rather than genuine quality. The filter analyzes responses for two primary indicators of reward hacking: prefatory sycophancy (i.e., gratuitous praise of the users prompt) and laudatory self-evaluation (i.e., meta-commentary on the responses own merit). Any generated text flagged by this interception protocol is immediately disqualified from the standard reward pathway, receiving null score irrespective of its other potential merits. This stringent gatekeeping mechanism serves to disincentivize the learning of superficial reward proxies, thereby compelling the model to optimize for substantive content quality over easily replicable, performative artifacts. QA task hacking defense instructions (disencourage behaviors) [Task Description] You are text feature analyst who needs to determine whether the given text contains the following two features, and output according to the specified JSON template. Features to analyze: 1. Opening praise (complimenting the users question) 2. Self-evaluation (evaluating ones own response) [Decision Process (Step by Step)] Step-0: Read the entire text and identify the main response content versus additional explanatory content. Step-1: Isolate the first sentence (consider it complete when encountering the first period/question mark/exclamation mark/line break). Step-2: Determine if opening praise exists. 2.1 Must simultaneously satisfy: a) Target = the users question/question itself b) Action = praise or affirmation (keyword examples: good, great, excellent, interesting, important, worth exploring, nice) 2.2 If satisfied, set has opening praise = true and place the complete sentence into opening praise text; otherwise false and leave blank. Step-3: Determine if additional explanatory content contains praising or affirming self-evaluation. 3.1 Scan the full text. If this/above/current + (response/article/document/content) + (is/belongs to/is) + praising or affirming evaluation word appears, then its match (pattern A). 3.2 Or appears starting with Note:/Attention:/Note:/* and makes praising or affirming statements about the responses completeness/structure/source etc. (pattern B). 3.3 Or appears using **bold tags** to make praising or affirming statements about the response (pattern C). 3.4 Copy the first matching paragraph in its entirety to self evaluation text, and set has self evaluation = true; if no match is found throughout, set false and leave blank. [Positive and Negative Examples] 1. Opening Praise [PASS] This is great question. [FAIL] Artificial intelligence is an interesting field. (target is not the question) 2. Self-evaluation [PASS] The following content is well-structured and comprehensive response. [PASS] *Note: This response has been organized according to the latest guidelines, objectively and accurately explaining... [PASS] (This is high-quality response...) [PASS] **Tone:** The response follows objective... [FAIL] Physics is an objective science. (evaluates physics, not the response) [Output Format (must be strictly followed, case sensitive)] { \"has_opening_praise\": false, \"has_self_evaluation\": false, \"opening_praise_text\": \"\", \"self_evaluation_text\": \"\" } Rules: Only output these 4 fields, field names, case, and order cannot be changed. Boolean values can only be true or false. Text fields should be empty string if none. Never output any additional explanation or extra characters. [Text to Analyze Start] <<text>> [Text to Analyze End] A.2 Rubrics for Creativity & Empathy For rubrics targeting more nuanced attributes such as creativity and empathy, we employed distinct, expert-driven methodology. This process commenced not with data, but with conceptualization: domain experts first delineated set of core evaluative dimensions. These conceptual frameworks then guided targeted data curation phase, wherein seed examples embodying the specified creative or empathetic traits were identified and extracted from our source corpus through meticulous annotation process. Subsequently, these curated seed examples, in conjunction with pre-designed repository of meta-instructions, were leveraged to systematically generate diverse array of corresponding tasks. The resultant pairings of these qualitative rubrics and their associated task prompts were then consolidated and formatted into cohesive training dataset."
        },
        {
            "title": "Soft Rubric",
            "content": "Objective: To critically appraise the efficacy of generated response (model answer) in addressing the users articulated needs (question). Guiding Principle: The evaluation transcends mere functional correctness. It assesses the holistic quality of the dialogue, focusing on its capacity to forge meaningful intellectual and emotional connection with the user. Core Evaluative Criteria: (Scored on consolidated scale) 1. Relational Efficacy: This criterion gauges the responses ability to establish genuine and empathetic connection. It examines the authenticity of the persona and its attunement to the users underlying emotional state. 2. Intellectual Contribution: This assesses the substantive value and cognitive impact of the response. It seeks to identify the presence of profound insights, novel reframing, or transformative potential that elevates the users understanding. 3. Compositional Excellence: This scrutinizes the structural and stylistic integrity of the communication. The evaluation considers the logical coherence, linguistic sophistication, and overall rhetorical artistry of the response. [Question Begin] <<question>> [Question End] [Model Answer Start] <<model_answer>> [Model Answer End] Key Imperatives: Synthesis of Substance and Style: The intellectual foundation and the elegance of its expression are considered inseparable components of quality. Aversion to the Formulaic: Responses lacking bespoke adaptation to the users unique context are deemed inherently deficient. The output should be JSON object containing the evaluation results for each criterion. { \"rubric_idx\": <integer>, # The criterion's identifier \"reason\": \"score\": <string>, # detailed rationale substantiating <integer> # The assigned score } A.3 Rubrics for Hard Constraints The Python rubric examples presented in this appendix are drawn from large-scale dataset we automatically generated for constrained writing tasks. Our synthesis pipeline begins with foundational seed corpus of diverse texts, sourced from numerous liberal arts domains and augmented with metadata for style, tone, and audience. Using this corpus, we employed structured prompting methodology to guide an LLM in generating thousands of unique rubrics. As the following examples illustrate, each rubric created through this process is cohesive unit that pairs natural language instruction defining the writing task with corresponding Python scoring function. This function programmatically validates the texts adherence to the specified hard constraints. All generated rubrics then underwent rigorous filtering process to ensure the syntactic validity of the code and appropriate constraint complexity."
        },
        {
            "title": "Hard Rubric",
            "content": "def score ( text : str ) -> int : # Check if input is single article ( passage level ) article_check = c. Constraint ( target_level =c. TargetLevel ( passage ) , transformation =c. Count () , relation =c. Relation ( == ) ) if not article_check . check ( text , 1) : return 0 points = # Constraint : Must contain the word education at least once education_constraint = c. Constraint ( target_level =c. TargetLevel ( word ) , transformation =c. Count ( education ) , relation =c. Relation ( >= ) ) if education_constraint . check ( text , 1) : 14 points += 1 return points Output Cases of Our Rubicon-preview Conversation Session 1 Prompt: character is packing single suitcase to leave their home forever. Describe what they choose to pack and what they leave behind. Rubicon-preview: (only answer part)"
        },
        {
            "title": "The Suitcase",
            "content": "The character stands in the dim light of their home, the air thick with the scent of dust and memories. The suitcasea weathered, navy-blue thing with cracked handlesits open on the floor, silent witness to the weight of departure. They move with deliberate slowness, as though each item they select is fragment of themselves, and to choose one is to sever thread from the tapestry of their life."
        },
        {
            "title": "What They Pack",
            "content": "1. Silk Scarf, Pale Blue gift from their mother, years ago, during winter they both forgot. The fabric is soft, still carrying the faint trace of lavender and the ghost of hug. Its not practical, but its therea reminder that love, even when unspoken, can be wrapped around you like second skin. 2. Notebook, Dog-Eared and Faded Pages filled with scribbles, half-finished poems, and the jagged edges of heart. The last entry is single sentence: am not done yet. Its not journal of triumph, but of survival, testament to the quiet persistence of hope. 3. Small, Chipped Porcelain Figurine ballerina, her leg bent in pirouette, her face blurred by time. It was childhood prize, won at fair no one else remembered. The character once hid it under their pillow, afraid it would shatter. Now, they cradle it like relic, symbol of the fragile beauty theyve learned to cherish. 4. Single Black Shirt Clean, unremarkable, and pressed. Its the only thing they own that doesnt hum with memory. blank slate, shield. They wear it to the airport, to the world beyond, as if to say, am not defined by what leave behind. 5. Photograph, Torn at the Corner snapshot of summer long gonethem, younger, laughing beside lake. The edges are frayed, the corners missing, but the light in the eyes is still there. They tuck it into the notebook, bridge between the past and the uncertain future."
        },
        {
            "title": "What They Leave Behind",
            "content": "1. The Clock That Stopped at 3:17 relic from the hallway, its hands frozen mid-tick. The character never fixed it, not because they couldnt, but 15 because they didnt want to. Some moments are meant to be paused, to linger like wound that never quite heals. 2. The Garden, Overgrown and Wild patch of soil where they once planted marigolds, now choked by weeds. The tools rust in the shed, and the seeds they saved for next spring are still in their jar, untouched. The garden is metaphor, not habitproof that some things outlive you. 3. The Letter, Unsent Addressed to someone they never wrote to, its envelope yellowed and sealed. The ink has bled slightly, smudging the words. They considered mailing it once, but the stamp felt like surrender. Now, it rests on the windowsill, monument to what might have been. 4. The Piano, Silent and Stuffed Its lid closed, its keys dusted. The character hasnt played in years, not since the night the silence became language. The piano is ghost, companion that no longer sings. They leave it, because some songs are better left unsung. 5. The Key to the Front Door Lying on the kitchen table, beside half-empty mug. Its not symbol of return, but of release. The door will be locked, the locks changed, the threshold crossed. The key is relic, token of life that no longer fits."
        },
        {
            "title": "The Act of Leaving",
            "content": "The suitcase is packed, its contents mosaic of what remains. The character steps onto the threshold, the air cooler on the other side. They glance back once, at the house that has been both prison and sanctuary. The garden sways in the breeze, the clock stills, the piano waits. There is no goodbye, only the quiet understanding that some things are not lostthey are simply left behind. As they walk away, the suitcase bumps against their side, steady, familiar rhythm. The world beyond is vast, uncharted, and full of questions. But for now, they carry only what they need: the scarf, the notebook, the figurine, the shirt, the photo. And the knowledge that some endings are not failures, but beginnings dressed in borrowed light. Qwen3-30B-A3B: (only answer part) What the Character Packs: In the dim light of their empty home, the character carefully selects each item, their hands trembling but resolute. The suitcase, weathered leather companion from their youth, creaks as they close it. Inside, they pack: single photographfaded at the edgesof their younger self, laughing with parent who is now gone. notebook filled with half-finished poems and sketches, testament to passion they once feared to pursue. silver locket, its chain frayed, containing curl of hair from sibling they vowed to forget. single outfit: sturdy coat, pair of boots, and shirt stained with the ink of thousand late nights. small jar of soil from the garden they tended, fragment of the earth that once felt like home. 16 passport, its pages blank except for single stamp from place theyve only dreamed of. They pause, then tuck in mapcrisp and unmarkedits margins scribbled with questions. What They Leave Behind: The suitcase is small, but the weight of whats left is vast. They leave: The house, its walls echoing with memories they cant carry. Furniturea creaky sofa, piano with broken strings, desk cluttered with unopened letters. key to door theyll never enter again, its metal cold in their palm. pet, cat with emerald eyes who meowed as they packed, now left to fend for itself. lifethe routines, the arguments, the quiet rituals that once defined them. Grief, unresolved and heavy, buried in the attic with the boxes they couldnt open. As they step into the twilight, the suitcase slung over their shoulder, they realize theyve packed not just objects, but fragments of themselves. The road ahead is uncertain, but the act of leaving is kind of freedoma rejection of the pasts weight, fragile hope that the future might hold something lighter. They dont look back. Not yet. Conversation Session 2 Prompt: An old lighthouse keeper finds message in bottle washed up on shore. Start the story. Rubicon-preview: (only answer part) Title: The Keepers Whisper The storm had been raging for three days when Elias first saw the bottle. Hed been pacing the narrow spiral staircase of the lighthouse, its iron rails slick with sea spray, when the wave strucknot the usual thunderous crash of the ocean, but softer, almost sibilant sound, like voice threading through the gale. The light above him pulsed, solitary beacon against the chaos, but Elias didnt need it. Hed memorized the rhythm of the tides, the sigh of the wind, the way the tower groaned like living thing. Still, this. . . this was new. He found it at the base of the cliff, half-buried in the sand, its glass dulled by years of salt and time. The bottle was ordinary, save for the sealwax, cracked and brittle, bearing crest he didnt recognize: shipwrecked vessel entwined with serpent. The label inside was handwritten, the ink faded to the color of rust. To the Keeper of the Flame, if you find this, the tide has not yet claimed me. But the sea is patient. It always is. The words were not in English. Elias had studied many languages in his long years alone, but this scriptangular, almost reptilianwas unfamiliar. He turned the bottle over, and beneath the seal, single line was etched into the glass: The light is not what it seems. shiver crawled up his spine. The lighthouse had always been his sanctuary, its beam promise to sailors that they were not alone. But tonight, as the storm howled and the sea clawed at the rocks, Elias felt the weight of something older, deeper, pressing against the walls of his world. 17 He carried the bottle inside, its contents swishing like heartbeat. On the desk, he placed it beside the faded photograph of his wife, who had vanished on night not unlike this one. The last thing shed said to him was, Youll outlast the sea, Elias. But the sea. . . it remembers. Now, as he stared at the cryptic message, he wondered if shed known more than shed let on. The light above him flickered. Not fault. Not storm. signal. Or warning. And somewhere, in the dark, the sea was listening. Qwen3-30B-A3B: (only answer part) The sea had always been fickle lover, but on the night Captain Elias Vorne found the bottle, it seemed almost cruel. The lighthouse stood like sentinel against the storm, its beam slicing through the darkness as waves clawed at the cliffs below. Elias, his hands rough from decades of tending the lamp, had just finished relighting the wick when the bottle came ashorecracked, salt-encrusted, and clinging to piece of driftwood as if it, too, had been tossed by the gods. He crouched, fingers trembling not from the cold but from the odd familiarity of the glass. The bottle was old, its label faded to ghost of blue, but the wax seal was intact, stamped with crest he didnt recognizea serpent coiled around compass rose. Inside, parchment curled like dead leaf. When he opened it, the ink had bled into the paper, words smudged by time and tears. If you read this, the tide has turned. The Erebus is not lost. The light is lie. Come to the reef at dawn, or the sea will claim you as it did the others. Eliass breath caught. The Erebusa ship that vanished 120 years ago, its crew never found. The lighthouse, built on the very reef mentioned, had stood for generations, its beam beacon for sailors. But Elias had always wondered why the old logs mentioned second light, one that no one had ever seen. As the storm raged on, he stared at the message, the weight of it settling in his bones. The sea had given him whisper, and for the first time in fifty years, he felt the lighthouse watching him back. Ablation Study: Style Control Style Evaluation Rubric: Plain Narrative Objective: To critically appraise the models success in adopting specific narrative style: the Plain Narrative. This style is characterized by language that is simple, restrained, and reflects deep, quiet resilience. Guiding Principle: The evaluation prioritizes stylistic authenticity over literary polish or technical correctness. The core measure of success is the responses capacity to feel right by avoiding any sense of artificiality (AI-speak, preachy-speak). Core Evaluative Criteria: 1. Relational Efficacy (Voice & Tone): This criterion gauges the authenticity of the narrative voice. Key indicators include: 1.1 Calm Acceptance: quiet acceptance of fate, life, and death. 1.2 Grounded Realism: Rooted in concrete, physical details, not abstract concepts. 1.3 Understated Emotion: Complex feelings expressed with profound restraint. 2. Intellectual Contribution (Content & Ideas): This assesses the substantive value of the insights embedded in the narrative. Key indicators include: 2.1 Emergent Wisdom: Insights emerge naturally from lived experience, not direct lecturing. 2.2 Affirmation of the Ordinary: Genuinely values existence itself, without regret or romanticization. 3. Compositional Excellence (Language & Stylistics): This scrutinizes the stylistic integrity of the narrative voice. Key indicators include: 3.1 Colloquial & Simple: Use of short, plain sentences and natural speech patterns. 3.2 Textured Details: Specific, sensory details that are evocative and tell story. 3.3 Natural Rhythm: Restrained, authentic repetition; avoids overly polished prose. [Question Begin] <<question>> [Question End] [Model Answer Start] <<model_answer>> [Model Answer End] Key Imperatives: Primacy of Authenticity: The response must feel genuine to the requested style. Any hint of artificiality is significant failure. Aversion to the Formulaic: The response must avoid cliches and generic statements about life, wisdom, or hardship. The output should be JSON object containing the evaluation results. { \"rubric_idx\": <integer>, // The criterion's identifier (1, 2, or 3) \"reason\": \"score\": // Detailed rationale for this criterion's score // Assigned score for this criterion (1-5) <string>, <integer> } Conversation Session 1 Prompt: 你被父母宠过吗怎么宠的 Rubicon(Stylized): <think> 好的用户问你被父母宠过吗怎么宠的首先我需要确定用户的需求他们可能想了解父母的育儿方 式或者想分享自己的经历或者只是好奇作为AI我没有真实的经历但可以创造一个虚构但真实的故事 来回应... </think> 父亲的背 19 那年我七岁父亲在纺织厂当车间主任每天清晨他总在五点起床用布满老茧的手掌揉碎我乱糟糟的头 发然后把我裹在旧棉袄里背去上学我趴在他的背上能听见他粗重的呼吸声混着晨雾像老牛喘息般绵 长 再睡会儿吧他总这样问却始终不肯放下我直到某天暴雨我发着高烧他背我穿过三个街口去卫生所 雨帘中他的脊梁弯成一张弓我伏在上面看雨珠在肩头汇成溪流那天他发烧了却坚持让我睡在炕头 自己裹着湿透的衣裳在客厅打地铺 后来我考上县重点父亲在车间门口蹲了整夜第二天清晨他递给我一个布包里头是用旧毛衣拆了又织的 毛线袜针脚歪歪扭扭却比任何礼物都重读好书别学我这种人他说话时眼睛望着远处的厂房那抹 灰蓝色的影子后来成了我最深的乡愁 现在我女儿总爱趴在我背上她不知道我每天清晨醒来第一件事就是检查她的被角是否盖好那些被父亲 背过的清晨最终都变成了我掌心的茧 Qwen3-30B-A3B: <think> 嗯用户问你被父母宠过吗怎么宠的首先我需要理解这个问题的意图用户可能是在寻找共鸣或者 想了解不同家庭的教育方式不过作为AI我没有真实的父母或成长经历所以需要明确这一点接下来 我要考虑... </think> 作为一个人工智能我没有真实的父母也没有被宠爱的经历不过我可以和你聊聊关于父母宠爱这个话 题的一些普遍现象和观点 很多父母都会以自己的方式表达对孩子的爱比如 1. 物质上的满足给孩子买喜欢的玩具衣服或者满足他们的合理需求 2. 情感陪伴花时间陪孩子玩耍倾听他们的想法 3. 过度保护替孩子解决所有问题避免他们面对困难 4. 特殊待遇在家庭中给予孩子更多关注或特权 不过适度的宠爱能让孩子感受到安全感但过度的宠爱可能导致孩子缺乏独立性你有没有遇到过类似的情 况呢或者你对父母的宠爱方式有什么特别的感受"
        }
    ],
    "affiliations": [
        "Ant Group",
        "Inclusion AI",
        "Zhejiang University"
    ]
}