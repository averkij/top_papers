{
    "paper_title": "TAPNext: Tracking Any Point (TAP) as Next Token Prediction",
    "authors": [
        "Artem Zholus",
        "Carl Doersch",
        "Yi Yang",
        "Skanda Koppula",
        "Viorica Patraucean",
        "Xu Owen He",
        "Ignacio Rocco",
        "Mehdi S. M. Sajjadi",
        "Sarath Chandar",
        "Ross Goroshin"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Tracking Any Point (TAP) in a video is a challenging computer vision problem with many demonstrated applications in robotics, video editing, and 3D reconstruction. Existing methods for TAP rely heavily on complex tracking-specific inductive biases and heuristics, limiting their generality and potential for scaling. To address these challenges, we present TAPNext, a new approach that casts TAP as sequential masked token decoding. Our model is causal, tracks in a purely online fashion, and removes tracking-specific inductive biases. This enables TAPNext to run with minimal latency, and removes the temporal windowing required by many existing state of art trackers. Despite its simplicity, TAPNext achieves a new state-of-the-art tracking performance among both online and offline trackers. Finally, we present evidence that many widely used tracking heuristics emerge naturally in TAPNext through end-to-end training."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 8 ] . [ 1 9 7 5 5 0 . 4 0 5 2 : r TAPNext: Tracking Any Point (TAP) as Next Token Prediction Artem Zholus*2,4,5 Carl Doersch1 Yi Yang1 Skanda Koppula1,3 Viorica Patraucean1 Xu Owen He Ignacio Rocco1 Mehdi S. M. Sajjadi1 Sarath Chandar2,4,5,6 Ross Goroshin1,4,7 1Google DeepMind 2Chandar Research Lab 3University College London 4Mila - Quebec AI Institute 5Polytechnique Montreal 6Canada CIFAR AI Chair 7Université de Montréal"
        },
        {
            "title": "Abstract",
            "content": "Tracking Any Point (TAP) in video is challenging computer vision problem with many demonstrated applications in robotics, video editing, and 3D reconstruction. Existing methods for TAP rely heavily on complex tracking-specific inductive biases and heuristics, limiting their generality and potential for scaling. To address these challenges, we present TAPNext, new approach that casts TAP as sequential masked token decoding. Our model is causal, tracks in purely online fashion, and removes tracking-specific inductive biases. This enables TAPNext to run with minimal latency, and removes the temporal windowing required by many existing state of art trackers. Despite its simplicity, TAPNext achieves new state-of-the-art tracking performance among both online and offline trackers. Finally, we present evidence that many widely used tracking heuristics emerge naturally in TAPNext through end-to-end training. 1. Introduction Correspondence, correspondence, and correspondence! was Takeo Kanades answer when asked for the three most fundamental problems in computer vision. In early research, image-level correspondence was pivotal for understanding motion [25], estimating depth [22] or performing 3D reconstruction from photo collections [47]. Today, correspondence has experienced resurgence as Tracking Any Point (TAP) in video: outputs are dense like optical flow, but also provide long-range correspondence as in keypoint detection. In TAP model is tasked to track large number of query points, which correspond to surfaces on solid objects, over the course of video (Figure 1). Because point tracking is such generic task, yielding highly detailed spatiotemporal information, point trackers are potentially useful in almost any downstream computer vision application. Indeed, the three years since TAPs inception it has already been applied to robotics [5, 6, 36, 53, 56, 60], action recognition [31], 3D (dynamic) reconstruction [54, 55, 58], (controllable) video generation and editing [12, 37, 57, 59, 62], zoology [38], and medicine [43]. Real-world training data for TAP is scarce [4, 11, 30, 39, 53], so prior works have trained models mainly on synthetic data [18, 61]. These works have argued that complex inductive biases and custom architectures are required to bridge the sim-to-real gap. However, making architectures too restrictive can limit scalability. In this work, we ask what architectural (e.g. cost-volume) and algorithmic (e.g. iterative refinement) design decisions can be relaxed in order to build more scalable, simpler, yet performant model. Our key finding is that only two ingredients are necessary to achieve good TAP performance: 1) shared, spatio-temporal bank of input tokens, where some tokens encode image patches and others encode trajectories, all processed jointly by unified model, and 2) simple recurrence across time, in the form of real-valued State-Space Model (SSM) [48], and 3) an output encoding that represents distributions instead of point estimates. Specifically, given our input encoding, we find that TRecViT [40]which simply interleaves (spatial) transformer layers and (temporal) SSM layers can be used off-the-shelf to predict point tracks. In fact, when fine-tuned on real data using previously published procedure [13], this architecture achieves state-of-the-art performance without any complex post-processing or iterative-inference. This stands in stark contrast to all other SoTA TAP methods which use windowed inference, iterative refinement, and temporal smoothness priors. The architecture consists of interleaved layers of 1) temporally-recurrent computations processing temporal tubes [32] which are sequences of individual image and point tokens, and 2) transformer layers which process spatial information by attending across image & point tokens, per-frame. Our contributions include: *Work done duing internship at Google DeepMind 1. We propose TAPNext, new point tracking model, which (a) Grid of query points (b) CoTracker3 [27] (c) TAPNext Figure 1. Dense grid tracking with TAPNext. We show (a) the query points on the first frame of the video, (b) the resulting tracks on the final frame of the video for CoTracker3 [27], and (c) our proposed TAPNext method. performs point tracking via imputation of unknown tokens in masked decoding fashion. TAPNext achieves state-of-the-art online tracking performance using open source architectural components (SSM & ViT) in backbone first proposed by [40], without using any trackingspecific inductive biases. 2. We show that while TAPNext does not include iterative or windowed inference, test-time optimization, cost volumes, feature interpolation, token feature engineering, or local search windows, some of these heuristics naturally emerge in TAPNext from end-to-end training on large synthetic dataset. 3. We demonstrate that recurrent networks (specifically SSM layers) can track points causally online, in perframe manner, generalizing to much longer videos compared to those seen in training. Linear recurrence of SSMs allows temporal processing to be parallelized in the offline setting [20]. 4. We are immediately open sourcing the inference code and model weights of TAPNext and BootsTAPNext. We plan to open source our training code base in the near future. 2. Background 2.1. Optical Flow and Point Tracking Frame-to-frame correspondence, or optical flow, has many modern deep learning approaches that solve the task, like FlowNet [14] or RAFT [49], which improve on classical methods like Lucas-Kanade [3]. Unfortunately, optical flow based correspondence models incur significant drift over long time horizons and traditionally do not address occlusion. Long term point tracking, or Tracking any Point (TAP), was introduced to bridge this gap [11]. Prior TAP works often formulate tracking as two step process: first, per-frame encoding and matching via cost-volume computation, followed by track refinement [11, 12, 21, 26]. The cost-volume computes the inner product between query point feature and all candidate matching features in video (albeit often at lower spatial resolution). Cost volumes are computed independently for every query point, and introduce an inductive bias that casts tracking as per-frame appearance matching problem in feature space. Motion information is integrated on top of this computation as an iterative refinement step [12, 21] or as an attention operation that processes the cost volume correlation features [26, 34]. Such architectures involve many heuristic design elements, including the differentiable argmax operation [12], bi-linear interpolation of features [21], restricted search windows [9], and windowed inference [26]. Our aim is to develop conceptually simple architecture without imposing these strong inductive biases on exactly how the tracking problem should be solved, and without explicit per-frame appearance matching. Many trackers also rely on using future frames to produce outputs for the current frame, limiting their applicability in real-time scenarios. For this reason, online point tracking has gained increasing attention, particularly in robotics [53]. Although several local window-based inference methods claim to offer online tracking, their reliance on large temporal windowing and the transfer of points only between consecutive windows often leads to tracking failures, especially during long-term occlusions in the middle of video [21, 26]. As result, these approaches tend to perform poorly on long-term tracking benchmarks [39]. At the same time, purely online, per-frame point tracking [1, 13, 53] has emerged with many applications in robotics [53, 56] and generative modeling [62]. In this work we show that these limitations can be addressed through the use of recurrent state architectures (e.g. SSMs). The recurrent state allows the model to maintain temporal coherence, even during long-term occlusions, by effectively capturing and preserving the dynamics of the tracked points across time. 2.2. Masked decoding and token imputation Self-supervised models such as Masked Autoencoders (MAE) [23], VideoMAE [50] and MultiMAE [2] have demonstrated the ability to learn rich, generalized representations of images and videos without requiring large annotated datasets. These models mask portions of the input data Input tokens tensor [T, + Q, C] . . . + i j r i Query Points Positional Encoding"
        },
        {
            "title": "Input video",
            "content": "Model outputs [T, Q, 2] [T, Q, 1] Coordinate Head Visibility Head L"
        },
        {
            "title": "SSM\nBlock",
            "content": "ViT attends across image & point tokens (time is treated as batch) space time"
        },
        {
            "title": "Input Tensor",
            "content": "SSM scans over temporal tubes (space is treated as batch) Figure 2. TAPNext performs tracking via imputation of unknown point coordinates given known ones (query points and the video). This imputation happens via temporal masked decoding of tokens: video tokens are concatenated with point coordinate tokens and the latter inject point query information via positional encoding. and task the network with reconstructing the missing parts, thereby forcing it to learn semantically meaningful representations that capture both spatial and temporal coherence. From the perspective of MultiMAE, our framework can be interpreted as treating point and image tokens as two different modalities. Similar formulations have also been used to train detection and segmentation models in the supervised setting [8, 29]. 2.3. Recurrent Models Recurrent models have long been used in video analysis tasks to capture temporal dependencies in sequential data. The use of Recurrent Neural Networks (RNNs) [42], and more specifically LSTM networks [24], has been widespread in sequence modeling tasks [28], and even object tracking [17]. These models process each frame in sequence by maintaining hidden state that evolves over time, allowing the model to learn temporal dynamics and propagate information from past frames [52]. The recurrent state has the capacity to serve multiple functions when applied to tracking problems, from motion modelling to maintaining representation of the tracked objects appearance online [17, 53]. Unlike other recently proposed state-space model (SSM) architectures for video [35] which are applied spatially (e.g. across image patches/tokens), we apply our SSM [7] only on temporal tubes and use the ViT layers for spatial processing [40]. Our work is the first to apply SSMs to the TAP task. 3. TAPNext TAPNext processes videos of frames, where each frame is an RGB image of size pixels. Like other trackers, TAPNext receives query points, each consisting of (t, x, y) denoting time (t) and x, coordinates of the query point in the video. We can consider RGB frames and query points as two different input modalities that are jointly processed, as illustrated in Figure 2. For the visual modality, like other ViT-based methods, we partition each image in the video into non-overlapping image patches. These image patches are linearly projected to dimensional space and spatial positional embeddings are added to them. The resulting tensor has shape of [T, w, C] which we refer to as the (input) image tokens. Next, we detail how the query points are encoded into tokens and how the tracking task is formulated as masked decoding problem. Tracking as Masked Decoding. TAPNext converts each query into sequence of tokens, one per frame. For the frame which corresponds to the query, this token is initialized with the query coordinates, the point tokens corresponding to the remaining frames are initialized with mask token; thus, inference is done by filling in the mask tokens. We refer to mask and query tokens collectively as point track tokens. In more detail, assuming we have one query point per track, of the form (t, x, y), we create point track tokens for each of the tracks. For each track, the token corresponding to the query point located at time is filled with the positional embedding values corresponding to the query coordinates, (x, y). The remaining Q(T 1) point tokens are filled with the same learned mask token value. We do not apply temporal positional embeddings, as there is no self-attention across time; in fact, we find temporal position embeddings can harm generalization across sequence length (i.e. to longer videos). The tensor of tokens representing image patches (of shape [T, w, C]) is concatenated with the tensor of point track tokens (of shape [T, Q, C]) along the spatial dimension, resulting in an input tensor of shape [T, + Q, C], as shown in the center of Figure 2. To predict the masked point coordinates, we simply take the point track tokens from the output of the model and pass them to prediction heads. Note that the model learns to copy the one known query to the output. This process is very similar to the decoding process used in Masked Autoencoder (MAE) [23] or VideoMAE [50] except that unknown query tokens are considered to be masked, while image tokens are always visible. Separating spatial and temporal processing. We aim to train tracking architecture with minimal tracking-specific inductive biases and post-processing, relying as much as possible on general-purpose components. For this purpose we adapt TRecViT [40], which consists of two building blocks: SSM blocks and ViT blocks. This architecture processes the aforementioned input tokens (T (h + Q) in total) through layers. The RecurrentGemma1 [7, 10] SSM block performs temporal processing via linear recurrence followed by ViT block2 [15] that performs spatial processing with attention. RecurrentGemma is causal recurrent model which was initially proposed for causal language modeling. Like other SSM models, RecurrentGemma [7, 10] uses linear recurrence which allows it to be parallelized in time and effectively train on very long sequences. The computational graph for every TAPNext layer is visualized on the right side of Figure 2. Specifically, each TAPNext layer has an SSM block that performs linear recurrence over the temporal dimension (T ) while treating the + token dimension as batch. This architecture operates on spatiotemporal tubes [32, 40] in the first layer, before any mixing across spatial tokens. Because the known query points are fed into the model only at specific timestep, the model must learn to propagate this information in the recurrent state to all subsequent time steps. These recurrent temporal blocks allow our model to successfully track points in long videos without increasing the computational complexity or resorting to windowed inference schemes [12, 21, 26]. The second stage of every TAPNext layer is standard ViT block that attends between image and point track tokens (h + Q), treating the time dimension (T ) as batch. Model interpretation. Each ViT block in TAPNext performs full self-attention over + tokens (in every frame). This means every track token gets updated with 1https://github.com/google-deepmind/recurrentgemma 2https://github.com/google-research/big_vision Figure 3. Three attention patterns learned by TAPNext. We visualize attention maps where the attention queries are the point track tokens and the keys are image tokens, which correspond to 8 8 patches. Each row is certain (layer, head) pair. We observe patterns: (top) Cost-volume-like attention head; (middle) Coordinate-based readout head; (bottom) motion-cluster-based readout head. Note that these are just intermediate heads in the backbone. Higher resolution image and full attention maps in Appendix B. (h w) spatial image tokens and (Q 1) other track tokens. In general each attention matrix can be subdivided into four sections: point-to-image, point-to-point, image-to-image, and image-to-point. This suggests simple way to visualize TAPNexts ViT layers: image-to-point attention weights show how information is read from the image (Figure 3) and point-to-point attention weights show how points interact in their representations (Figure 4). See Section 4.3 for more detailed discussion. Prediction heads and loss functions. For each timestep, each layer of our model outputs the same number of tokens as the input, i.e. + Q, with query tokens. The goal of the point tracking task is to predict the coordinate and visibility for each queried point in each frame. Therefore, we simply take output track tokens and feed them to prediction heads (MLP networks): one head predicts (x, y) [0, H] [0, ] coordinates and the other predicts binary visibility. The outputs are trained using ground truth track coordinates and visibility flag at the corresponding timestep. The remaining video tokens are not fed to any loss. Since all layers of TAPNext output track tokens, we apply losses at every TAPNext layer with equal weights and use the last layers output as the prediction. We found this combined loss significantly improved performance. Another key innovation of TAPNext is the parameterization of the coordinate head. Specifically, we propose to parameterize the coordinate prediction as classification of Figure 4. Point-to-point attention map visualizations. Tracked points are nodes and (scaled) attention weights are edges, the thicker the edge the higher the weight between points. Two frames from video are used to visualize two attention layers. Note that in all images we see strong attention between points on objects that are moving together. See higher resolution images in Appendix B. coordinates (inspired by Farebrother et al. [16]). The motivation of this approach is that tracking assumes that the range of coordinates is bounded by the image size. Specifically, and coordinates are discretized into (n = 256 in our experiments) classifier bins. Each bin corresponds to certain discrete coordinate. Although the classifier selects among finite number of discrete coordinates, we can obtain continuous coordinate prediction by computing the expected coordinate w.r.t. classifier probabilities. We include detailed description of the coordinate head in the Appendix D. Notably, the ablations in Section 4 suggest that the coordinate classification is one of the most important components of TAPNext. The coordinate head is trained using combination of softmax cross entropy loss and Huber loss (on the continuous prediction). Note that the coordinate prediction means that our model can represent multimodal predictions if it is uncertain, unlike prior methods such as TAPIR and Co-Tracker which perform multiple refinement steps on single trajectory hypothesis per query, and perform local feature sampling around that single hypothesis. The visibility head performs binary classification and is trained with sigmoid binary cross entropy loss. Like with TAPIR, we also make use of an uncertainty estimate, although unlike TAPIR, we do not need to add any extra loss: we simply mark points as occluded if more than 50% of the probability mass lies outside of an 8-pixel radius. BootsTAP. Training on pseudo-labled real data has proven beneficial across many frameworks [13, 27, 46] as method of bridging the sim-to-real domain gap, and we expect this to be particularly important for TAPNext due to its lack of inductive biases. We closely follow the method proposed in BootsTAP [13], which intuitively aims to make the network equivariant to affine transformations of the video, and invariant to corruption. Specifically, we pre-train on simonly data, and then fine-tune with student-teacher setup, where the student TAPNext model trains on both sim data and real pseudo-labeled by teacher network. For the real data, the teacher makes prediction on un-corrupted, full-resolution videos, while the student must make prediction on view which is affine-transformed and corrupted with JPEG artifacts. The loss is backpropped only to the student; the teacher is updated via an exponential moving average of the students weights, which means it provides stable predictions and is unlikely to collapse. We refer to TAPNext finetuned on real data as BootsTAPNext. Advantages of TAPNext. Although our model contains recurrent component, coordinate predictions at different time-steps do not necessarily depend on each other as in many auto-regressive models. Such dependence can lead to accumulation of errors causing trackers to drift [21]. Many prior works perform iterative refinement that is bidirectional in time potentially using information from future frames to correct past errors. TAPNext demonstrates that it is possible to achieve state-of-the-art tracking quality by causally processing every frame only once. Our model is conceptually simple, in that it contains few hyper-parameters and no tracking-specific inductive biases. This allows it to find its own, unbiased, solution to TAP without being prescribed how motion and appearance should be used. Lastly, because TAPNext is linear recurrent model, it can track all points in video in parallel over time thanks to parallel inference of SSMs [19, 20, 44]. 3.1. Training & Inference Details Like most previous works in TAP, we train and evaluate on the TAP-Vid benchmark [11]. This benchmark uses Kubric to generate synthetic training data, and two humanlabeled evaluation datasets: DAVIS (30 videos, 24 to 105 frames) and Kinetics (1150 videos, 250 frames each). While most methods use small scale generated Kubric data (11, 000 videos, 24 frames each), we found it important to train TAPNext on significantly larger dataset of 500, 000 videos each consisting of 48 frames. Also, we generate videos with camera panning and motion blur. Note that the data generation engine is open source (see details on data generation process in Appendix G) and there is no consensus on which data should be used for training. Some methods [11] use vanilla Kubric, while others use motion blur [26, 34] or panning"
        },
        {
            "title": "Kinetics Strided",
            "content": "AJ δavg OA AJ δavg OA AJ δavg OA AJ δavg OA TAPNet [11] Online TAPIR [53] Online BootsTAP [13] Track-On [1] TAPNext-S (Ours) TAPNext-B (Ours) BootsTAPNext-B (Ours) frame 33.0 56.2 59.7 65.0 59.9 62.4 65.2 Models evaluated at 256 256 resolution. 48.6 70.0 72.3 78.0 74.4 76.6 78.5 70.0 74.0 76.1 75.9 76.7 78.8 86.5 86.9 90.8 89.8 90.5 91. 86.5 88.4 91.1 91.4 91.0 38.4 - 61.2 - 60.0 65.4 68.9 61.3 66.2 66.3 66.4 - 53.1 - - - 76.6 79.7 82.4 73.6 78.5 79.2 78.8 - 82.3 - - - 84.3 88.9 91. 88.8 90.7 91.0 91.3 - - - 87.2 - - 75.3 Models evaluated at 384 512 resolution. 51.7 62.3 67.8 67.5 78.2 79.6 85.3 87.5 89. 58.5 61.4 window 63.0 63.5 63.2 video - - 63.0 42.2 window 62.2 64.4 video 64.8 63. 64.8 75.7 76.9 77.4 76.3 77.7 89.3 91.2 86.2 90.2 52.4 65.9 - 69.4 - 70.0 79.4 - 81.3 - 83.6 89.9 - 88.6 - 38.5 51.5 55.1 53.9 49.8 53.3 57.3 49.6 54.6 49.0 49.7 54. - - 52.9 - 48.8 54.7 52.3 55.8 54.4 - 67.5 67.3 65.6 67.9 70.6 64.2 68.4 64.4 64.2 67.5 - - 66. - 64.5 67.8 66.4 68.5 80.6 - 86.3 87.8 85.9 87.0 87.4 85.0 86.5 85.2 85.7 88.2 - - 85.3 - 85.8 87. 82.1 88.3 46.6 - - - 52.8 56.6 62.2 57.2 61.4 - - - 55.1 59.7 59.5 35.3 57.3 - 59.1 - 60.9 - - - 69.5 71.4 75.1 70.1 74.2 - - - 69.2 73.3 73.0 54.8 70.6 - 72.5 - 85.0 - - - 82.3 84.9 89. 87.8 89.7 - - - 89.2 88.5 88.5 77.4 87.5 - 85.7 - TAPIR [12] BootsTAP [13] TAPTR [34] TAPTRv2 [33] TAPTRv3 [41] OmniMotion [55] Dino-Tracker [51] LocoTrack-B [9] PIPs [21] CoTracker2 [26] CoTracker3 [27] LocoTrack-B [9] CoTracker3 [27] Table 1. Tracking performance for TAPNext and baseline models. TAPNext achieves new state-of-the-art point tracking performance on eight of the twelve metrics, while also achieving the lowest possible latency. Methods are organized by their latency. Latency: video - these models require the entire video as input before outputting the point tracks. Latency: window - these models output tracks of length after consuming chunk of frames (typically = 8). After filling buffer of frames, these models can operate in per-frame fashion. Latency: frame - these models have minimal latency by outputting point predictions immediately after consuming each frame. In each column, the best performing values are in bold, the second best are underlined. [12]. Our baseline TAPNext model, like most other baselines [11, 12, 21, 33, 34], trains only on synthetic data. The BootsTAPNext model is finetuned on real videos following [13, 27]. In this case, we train on 48-frame clips from the same dataset presented in BootsTAP [13] (obtained courtesy of the authors), consisting of roughly 15M video clips taken from the internet, containing mostly real videos everyday activities and filtered to remove cuts and overlays. Our best performing model is first trained for 300,000 steps on synthetic data (Kubric), followed by self-supervised training on real videos for an additional 1,500 steps. We closely follow the self-supervised training scheme introduced in BootsTAP [13], however we replace the TAPIR coordinate loss [12] with the TAPNext coordinate loss described in Appendix D. Training Details. We train TAPNext using batches of 256 videos with length 48 frames. We sample 256 point queries for each video. We train two variants of TAPNext: TAPNextS (56M parameters) and TAPNext-B (194M parameters). We train the model on 256 256 resolution. We use peak learning rate in the cosine decay schedule of 1e3 for the S-model and 5e4 for the B-model. All hyperparameters for training the model and model configuration are included in Appendix E. Inference Details. We evaluate TAPNext models at 256 256 resolution. To perform query-strided evaluation with our causal tracker, we run the video forwards and backwards from every query point (see Appendix F). For fair comparison to other methods, we evaluated TAPNext by tracking one query point at time and included support points (same as CoTracker [26]). See Appendix for more details about the experimental evaluation of joint tracking with TAPNext. Like CoTracker, we found that performance can be improved by adding local and global support points to the evaluation. 4. Experimental Results We evaluated TAPNext quantitatively on both the DAVIS and Kinetics components of the TAP-Vid benchmark [11]. Because our model avoids tracking-specific inductive biases, we conducted qualitative evaluation to gain intuition about what visual/motion cues the model uses to track points. Finally we conducted several ablations to further understand the most important ingredients for obtaining good performance. Metrics. In line with prior works in TAP, we report three metrics for evaluation. First, Occlusion Accuracy (OA) is the accuracy of classifying whether the point is visible or not. Second, coordinate accuracy (denoted δavg) is fraction of points within threshold of 1, 2, 4, 8, 16 pixels, averaged over all thresholds. Third, Average Jaccard measures occlusion accuracy and coordinate accuracy together. 4.1. Performance on TAP-Vid Table 1 shows our results on TAP-Vid datasets. As we are targeting online applications, such as robotics and autonomous driving, we group approaches by their latency: in particular, methods that can outputs tracks immediately after ingesting each frame, followed by those require window of frames before they output the first prediction, and then those that require the entire video to track points [51, 55]. We also separately report methods that begin by upsampling the image to higher resolution before tracking. TAPNext achieves state-of-the-art performance in almost all datasets, with particularly large gaps comparing to other online (1-frame-latency) methods. Recall that all videos in the Kinetics dataset have 250 frames. Since the model was trained on 48 frame long videos it can generalize to videos that are at least 5 longer videos than the ones seen in training. TAP evaluation is performed simply by propagating frames and query points through our model, i.e. without SSM state resets, windowed inference, test time optimization, or other heuristics. We note that TAPNext achieves this while being substantially simpler than other methods, avoiding tracking-specific inductive biases in its architecture. We encourage readers to view the qualitative results we have included in the supplementary file, which includes side-by-side comparison between BootsTAPNext, BootsTAPIR, and Cotracker3. In particular, we find that our method can more accurately handle occlusion and fast motion. It is also better at tracking thin/small, and texture-less objects. Tracking Speed. We evaluate the speed of point tracking of TAPNext, LocoTrack-B, and Cotracker3 in Table 2. We report two metrics: speed in average frames-per-second (FPS), which is the total time the tracker spent to process the entire video divided by the total number of frames in that video; and latency, which is the worst case delay between receiving frame and outputting point coordinates/visibility of points in that frame (latency is equal to the time of performing one forward pass for approaches that dont use test-timeoptimization). Latency measures how much time the user needs to wait until receiving tracked points from the model when tracking in purely online fashion. TAPNext operates per-frame, while Cotracker3 requires at least 8 frames to produce an output, and LocoTrack-B requires the entire video (the longest video in DAVIS is 104 frames). Table 2 reports metrics on video of 1000 frames. 4.2. Model Interpretation Visualization. To gain insight into how TAPNext tracks points, we visualize the attention weights in several ViT layers. First we visualize the attention weight between track token and image tokens (Figure 3). Next, we show how points interact in the attention process (Figure 4). Attention patterns implemented by TAPNext show that it uses motion (motioncluster head in Figure 3 and 4), coordinate (coordinate head in Figure 3), and appearance-matching (cost-volume-like head in Figure 3) cues to implement tracking. Remarkably, these heuristics were not explicitly encoded in the models architecture or training process. Instead, they arose organically as emergent properties of the system through end-to-end supervised learning. Ablations. Table 3 shows ablations of key components of TAPNext. Note that we perform ablations at smaller scale compared to the model in Table 1. We observe that the most important component of TAPNext is the classification coordinate head that uses both classification and regression loss. Another important aspect is to use small image patch size of 8 8 pixels for the initial linear projection. Finally, Table 4 shows that when the SSM is swapped for temporal attention, the model exhibits poor temporal generalization despite using the RoPE [45] temporal positional embedding which is known to generalize over time. Reconstructions. We can visualize internal representations captured by the model by adding reconstruction term tied to the output image tokens, which are not directly connected to any output head in the original model. While we did not find that image reconstruction objective leads to improved tracking performance, we can nevertheless use the reconstructions to gain insight into how TAPNext works. To do this, we invert the tracking task - instead of the training on the complete video and incomplete motion information, the input contains complete sequences of ground truth point tracks but only short prefix of video frames. In this setting, TAPNext is then trained to reconstruct the remaining frames. To do this, we used trained TAPNext model to track points in natural videos, we used the TAPTube dataset used in BootsTAP [13], and stored the resulting tracks as pseudolabels. Next, we trained another TAPNext model on TAPTube where the output image tokens are used to decode the corresponding image patches in future frames. To do this we simply use linear read-out head to decode the masked image tokens to 8 8 image patches and minimize the L2 reconstruction loss to the corresponding ground truth patches. Like masked point tokens, masked image tokens are simply positional embedding plus the fixed learned mask token. We evaluate this model on the DAVIS dataset by visualizing reconstructed fu-"
        },
        {
            "title": "Average FPS",
            "content": "Latency (ms) 256 512 1024 LocoTrack-B (256 256) CoTracker3 (online) TAPNext-B (Ours) LocoTrack-B (256 256) CoTracker3 (online) TAPNext-B (Ours) LocoTrack-B (256 256) CoTracker3 (online) TAPNext-B (Ours) H100 V100 H100 V100 6600 452 240 102 14.2 197 12000 242 360 69 18 189 22800 124 576 45 23 2210 80 5.05 4130 116 5.26 8000 177 5.33 150 33 70 82 22 55 43 14 42 Table 2. Speed comparison of TAPNext2 to online Cotracker3 running on Nvidia V100 and H100 GPUs. The latency metric is defined as the maximum (worst case) time between passing frame to the model and receiving predicted points, and it includes the time it takes to fill and process the initial frame buffer. All models are implemented in PyTorch. ture frames conditioned only two initial frames and psuedoground truth track tokens. The results are shown in Figure 5. This should not be veiewed as generative model, but rather linear probing experiment which visualizes the information stored in the intermediate tokens used by model. We used the TAPNext-B model variant for this experiment. Figure 5 in the Appendix show key frames along the video completion. The image tokens corresponding to the first 10 frames are input to the model (top two frames of each Figure), after that only the masked image tokens are input and the images are reconstructed from the image tokens imputed by the model. We observe that the model propagates visual information accurately for regions covered by tracked points. The other visual regions (e.g. the ones that appear after the initial points grid moves away) in video completions are not reconstructed well. This is because the model does not have any previous information about those regions (neither visual nor represented through tracked motion), therefore the model simply fills these regions with average values. This implies that the model maintains an accurate visual representation of the tracked points. In other words, the model learns to propagate visual information to future frames from past frames using the trajectories of tracked points. In particular in Figure 5 (left column) note the blue road sign behind the car is reconstructed fairly accurately even after it was occluded by the car. This implies that the appearance is stably encoded by the model and effectively propagated forward in time by the SSM layers. 4.3. Behavior for Long Video Lengths We observe significant failure in long term point tracking the full length of video greater than 150 frames. The 2We use torch.compile for the inference with TAPNext as we found it to be crucial for fast inference. We tried our best to also apply compilation for CoTracker3 and LocoTrack but for CoTracker3 compilation makes inference slower and for LocoTracker compilation led to out of memory error. issue is likely due to the state space model, which is only trained with maximum 48 frames and generalizes poorly to significantly longer video clips. There is large opportunity to further improve the strong tracking numbers even more if future work addresses this limitation in the SSM. We have found partial mitigation to the temporal degradation to be: clipping the forget gate in the SSM to between 0.0 and 0.1, and broadcasting the query features across the length of the video tokens, so that the query points are not lost in the temporal context. Type of Ablation: default value ablated value Average Jaccard TAPNext-S default (small scale run) Classification coordinate head 2 state expansion [10] in SSM Losses after each ViT Block Regression + Classification coordinate loss Regression + Classification coordinate loss Regression coordinate head No SSM state expansion No intermediate losses Classification coordinate loss Regression coordinate loss 55.0 44.7 53.8 50. 52.7 48.1 Image patch 8 8 Image patch 16 16 49.7 Table 3. Ablating of the main components of TAPNext-S. We train each model for 150, 000 steps and batch size 128 and on 24 frames (compared to 300, 000 steps, batch 256, and 48 frames for the main TAPNext models in Table 1). We evaluate every ablation on DAVIS query-first. All ablations are independent of each other. 5. Conclusion We introduced TAPNext, framework that casts TAP as next token prediction. While being conceptually simple,"
        },
        {
            "title": "TAPNext\nVariants",
            "content": "Average Jaccard 24 Frames"
        },
        {
            "title": "Temporal Attention\nTemporal SSM",
            "content": "68.4 70.0 17.3 55.0 Table 4. Temporal attention ablation of TAPNext-S (under the same protocol as Table 3). We change the temporal SSM block to the temporal attention block [48] that uses rotary positional embeddings [45]. SSM blocks enable much better temporal generalization in TAPNext beyond the 24 frame training sequences. our method achieves competitive performance. We hope that this will inspire researchers to build on top of TAPNext and further increase the popularity of TAP as an important computer vision task. Although we applied this framework only to point tracking it can be extended to many other computer vision tasks in video."
        },
        {
            "title": "References",
            "content": "[1] Görkay Aydemir, Xiongyi Cai, Weidi Xie, and Fatma Güney. Track-On: Transformer-based online point tracking with In The Thirteenth International Conference on memory. Learning Representations, 2025. 2, 6 [2] Roman Bachmann, David Mizrahi, Andrei Atanov, and Amir Zamir. MultiMAE: Multi-modal multi-task masked autoencoders. In European Conference on Computer Vision, pages 348367. Springer, 2022. 2 [3] Simon Baker and Iain Matthews. Lucas-kanade 20 years on: unifying framework. International journal of computer vision, 56:221255, 2004. 2 [4] Arjun Balasingam, Joseph Chandler, Chenning Li, Zhoutong Zhang, and Hari Balakrishnan. Drivetrack: benchmark for long-range point tracking in real-world videos. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 2248822497, 2024. 1 [5] Homanga Bharadhwaj, Roozbeh Mottaghi, Abhinav Gupta, and Shubham Tulsiani. Track2Act: Predicting point tracks from internet videos enables diverse zero-shot robot manipulation. arXiv preprint arXiv:2405.01527, 2024. 1 [6] Homanga Bharadhwaj, Debidatta Dwibedi, Abhinav Gupta, Shubham Tulsiani, Carl Doersch, Ted Xiao, Dhruv Shah, Fei Xia, Dorsa Sadigh, and Sean Kirmani. Gen2act: Human video generation in novel scenarios enables generalizable robot manipulation. CoRL Workshop on X-Embodiment, 2025. [7] Aleksandar Botev, Soham De, Samuel Smith, Anushan Fernando, George-Cristian Muraru, Ruba Haroun, Leonard Berrada, Razvan Pascanu, Pier Giuseppe Sessa, Robert Dadashi, Léonard Hussenot, Johan Ferret, Sertan Girgin, Olivier Bachem, Alek Andreev, Kathleen Kenealy, Thomas Mesnard, Cassidy Hardin, Surya Bhupatiraju, Shreya Pathak, Laurent Sifre, Morgane Rivière, Mihir Sanjay Kale, Juliette Love, Pouya Tafti, Armand Joulin, Noah Fiedel, Evan Senter, Yutian Chen, Srivatsan Srinivasan, Guillaume Desjardins, David Budden, Arnaud Doucet, Sharad Vikram, Adam Paszke, Trevor Gale, Sebastian Borgeaud, Charlie Chen, Andy Brock, Antonia Paterson, Jenny Brennan, Meg Risdal, Raj Gundluru, Nesh Devanathan, Paul Mooney, Nilay Chauhan, Phil Culliton, Luiz Gustavo Martins, Elisa Bandy, David Huntsperger, Glenn Cameron, Arthur Zucker, Tris Warkentin, Ludovic Peran, Minh Giang, Zoubin Ghahramani, Clément Farabet, Koray Kavukcuoglu, Demis Hassabis, Raia Hadsell, Yee Whye Teh, and Nando de Frietas. RecurrentGemma: Moving past transformers for efficient open language models, 2024. 3, 4 [8] Nicolas Carion, Francisco Massa, Gabriel Synnaeve, Nicolas Usunier, Alexander Kirillov, and Sergey Zagoruyko. End-toend object detection with transformers, 2020. 3 [9] Seokju Cho, Jiahui Huang, Jisu Nam, Honggyu An, Seungryong Kim, and Joon-Young Lee. Local all-pair correspondence for point tracking. arXiv preprint arXiv:2407.15420, 2024. 2, 6 [10] Soham De, Samuel L. Smith, Anushan Fernando, Aleksandar Botev, George Cristian-Muraru, Albert Gu, Ruba Haroun, Leonard Berrada, Yutian Chen, Srivatsan Srinivasan, Guillaume Desjardins, Arnaud Doucet, David Budden, Yee Whye Teh, Razvan Pascanu, Nando De Freitas, and Caglar Gulcehre. Griffin: Mixing gated linear recurrences with local attention for efficient language models, 2024. 4, 8 [11] Carl Doersch, Ankush Gupta, Larisa Markeeva, Adria Recasens, Lucas Smaira, Yusuf Aytar, Joao Carreira, Andrew Zisserman, and Yi Yang. TAP-vid: benchmark for tracking any point in video. Advances in Neural Information Processing Systems, 35:1361013626, 2022. 1, 2, 5, 6, 4 [12] Carl Doersch, Yi Yang, Mel Vecerik, Dilara Gokay, Ankush Gupta, Yusuf Aytar, Joao Carreira, and Andrew Zisserman. TAPIR: Tracking any point with per-frame initialization and temporal refinement. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 10061 10072, 2023. 1, 2, 4, [13] Carl Doersch, Pauline Luc, Yi Yang, Dilara Gokay, Skanda Koppula, Ankush Gupta, Joseph Heyward, Ignacio Rocco, Ross Goroshin, João Carreira, and Andrew Zisserman. BootsTAP: Bootstrapped training for tracking any point. arXiv, 2024. 1, 2, 5, 6, 7 [14] Alexey Dosovitskiy, Philipp Fischer, Eddy Ilg, Philip Hausser, Caner Hazirbas, Vladimir Golkov, Patrick Van Der Smagt, Daniel Cremers, and Thomas Brox. Flownet: Learning optical In Proceedings of the flow with convolutional networks. IEEE international conference on computer vision, pages 27582766, 2015. 2 [15] Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, Jakob Uszkoreit, and Neil Houlsby. An image is worth 16x16 words: Transformers for image recognition at scale, 2021. 4 [16] Jesse Farebrother, Jordi Orbay, Quan Vuong, Adrien Ali Taiga, Yevgen Chebotar, Ted Xiao, Alex Irpan, Sergey Levine, Pablo Samuel Castro, Aleksandra Faust, Aviral Kumar, and Rishabh Agarwal. Stop regressing: Training value functions via classification for scalable deep RL. In Forty-first International Conference on Machine Learning, 2024. 5 [17] Daniel Gordon, Ali Farhadi, and Dieter Fox. Re3: Real-time recurrent regression networks for visual tracking of generic objects. IEEE Robotics Autom. Lett., 3(2):788795, 2018. 3 [18] Klaus Greff, Francois Belletti, Lucas Beyer, Carl Doersch, Yilun Du, Daniel Duckworth, David Fleet, Dan Gnanapragasam, Florian Golemo, Charles Herrmann, et al. Kubric: scalable dataset generator. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 37493761, 2022. 1, 6 [19] Albert Gu and Tri Dao. Mamba: Linear-time sequence modeling with selective state spaces, 2024. 5 [20] Albert Gu, Karan Goel, and Christopher Ré. Efficiently modeling long sequences with structured state spaces, 2022. 2, 5 [21] Adam W. Harley, Zhaoyuan Fang, and Katerina Fragkiadaki. Particle video revisited: Tracking through occlusions using point trajectories. In ECCV, 2022. 2, 4, 5, 6 [22] Richard Hartley and Andrew Zisserman. Multiple view geometry in computer vision. Cambridge university press, 2003. 1 [23] Kaiming He, Xinlei Chen, Saining Xie, Yanghao Li, Piotr Dollár, and Ross Girshick. Masked autoencoders are scalable vision learners. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 16000 16009, 2022. 2, 4 [24] Hochreiter. Long short-term memory. Neural Computation MIT-Press, 1997. 3 [25] Thomas Huang and Arun Netravali. Motion and structure from feature correspondences: review. Proceedings of the IEEE, 82(2):252268, 1994. 1 [26] Nikita Karaev, Ignacio Rocco, Benjamin Graham, Natalia Neverova, Andrea Vedaldi, and Christian Rupprecht. CoTracker: It is better to track together. 2023. 2, 4, 5, 6 [27] Nikita Karaev, Iurii Makarov, Jianyuan Wang, Natalia Neverova, Andrea Vedaldi, and Christian Rupprecht. CoTracker3: Simpler and better point tracking by pseudolabelling real videos. arXiv preprint arXiv:2410.11831, 2024. 2, 5, 6, 4 [28] Andrej Karpathy and Li Fei-Fei. Deep visual-semantic alignments for generating image descriptions. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 31283137, 2015. 3 [29] Alexander Kirillov, Eric Mintun, Nikhila Ravi, Hanzi Mao, Chloe Rolland, Laura Gustafson, Tete Xiao, Spencer Whitehead, Alexander Berg, Wan-Yen Lo, et al. Segment anything. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 40154026, 2023. 3 [30] Skanda Koppula, Ignacio Rocco, Yi Yang, Joe Heyward, João Carreira, Andrew Zisserman, Gabriel Brostow, and Carl Doersch. Tapvid-3d: benchmark for tracking any point in 3d. NeurIPS Datasets and Benchmarks, 2024. 1 [31] Pulkit Kumar, Namitha Padmanabhan, Luke Luo, Sai Saketh Rambhatla, and Abhinav Shrivastava. Trajectory-aligned space-time tokens for few-shot action recognition. In European Conference on Computer Vision, pages 474493. Springer, 2025. [32] Suha Kwak, Minsu Cho, Ivan Laptev, Jean Ponce, and Cordelia Schmid. Unsupervised object discovery and tracking in video collections. In Proceedings of the IEEE international conference on computer vision, pages 31733181, 2015. 1, 4 [33] Hongyang Li, Hao Zhang, Shilong Liu, Zhaoyang Zeng, Feng Li, Tianhe Ren, Bohan Li, and Lei Zhang. Taptrv2: Attentionbased position update improves tracking any point, 2024. 6 [34] Hongyang Li, Hao Zhang, Shilong Liu, Zhaoyang Zeng, Tianhe Ren, Feng Li, and Lei Zhang. Taptr: Tracking any point with transformers as detection, 2024. 2, 5, 6 [35] Kunchang Li, Xinhao Li, Yi Wang, Yinan He, Yali Wang, Limin Wang, and Yu Qiao. Videomamba: State space model for efficient video understanding. arXiv preprint arXiv:2403.06977, 2024. 3 [36] Sizhe Lester Li, Annan Zhang, Boyuan Chen, Hanna Matusik, Chao Liu, Daniela Rus, and Vincent Sitzmann. Unifying 3d representation and control of diverse robots with single camera. arXiv preprint arXiv:2407.08722, 2024. 1 [37] Yaowei Li, Xintao Wang, Zhaoyang Zhang, Zhouxia Wang, Ziyang Yuan, Liangbin Xie, Yuexian Zou, and Ying Shan. Image conductor: Precision control for interactive video synthesis. arXiv preprint arXiv:2406.15339, 2024. 1 [38] Jernej Polajnar, Elizaveta Kvinikadze, Adam Harley, and Igor Malenovsk`y. Wing buzzing as mechanism for generating vibrational signals in psyllids. Insect Science, 2024. [39] Viorica Patraucean, Lucas Smaira, Ankush Gupta, Adrià Recasens Continente, Larisa Markeeva, Dylan Banarse, Skanda Koppula, Joseph Heyward, Mateusz Malinowski, Yi Yang, Carl Doersch, Tatiana Matejovicova, Yury Sulsky, Antoine Miech, Alex Frechette, Hanna Klimczak, Raphael Koster, Junlin Zhang, Stephanie Winkler, Yusuf Aytar, Simon Osindero, Dima Damen, Andrew Zisserman, and João Carreira. Perception test: diagnostic benchmark for multimodal video models. In Advances in Neural Information Processing Systems, 2023. 1, 2 [40] Viorica Patraucean, Xu Owen He, Joseph Heyward, Chuhan Zhang, Mehdi S. M. Sajjadi, George-Cristian Muraru, Artem Zholus, Mahdi Karami, Ross Goroshin, Yutian Chen, Simon Osindero, João Carreira, and Razvan Pascanu. TRecViT: Recurrent Video Transformer, 2024. 1, 2, 3, 4 [41] Jinyuan Qu, Hongyang Li, Shilong Liu, Zhaoyang Zeng, Tianhe Ren, and Lei Zhang. TAPTRv3: Spatial and Temporal Context Foster Robust Tracking of Any Point in Long Video. arXiv preprint, 2024. 6 [42] David Rumelhart, Geoffrey Hinton, and Ronald Williams. Learning internal representations by error propagation, parallel distributed processing, explorations in the microstructure of cognition, ed. de rumelhart and j. mcclelland. vol. 1. 1986. Biometrika, 71(599-607):6, 1986. 3 [43] Adam Schmidt, Omid Mohareri, Simon DiMaio, and Septimiu Salcudean. Surgical tattoos in infrared: dataset for quantifying tissue tracking and mapping. IEEE Transactions on Medical Imaging, 2024. 1 [44] Jimmy T. H. Smith, Andrew Warrington, and Scott W. Linderman. Simplified state space layers for sequence modeling, 2023. 5 [53] Mel Vecerik, Carl Doersch, Yi Yang, Todor Davchev, Yusuf Aytar, Guangyao Zhou, Raia Hadsell, Lourdes Agapito, and Jon Scholz. Robotap: Tracking arbitrary points for few-shot visual imitation. In 2024 IEEE International Conference on Robotics and Automation (ICRA), pages 53975403. IEEE, 2024. 1, 2, 3, [54] Jianyuan Wang, Nikita Karaev, Christian Rupprecht, and David Novotny. Visual geometry grounded deep structure from motion. CVPR, 2024. 1 [55] Qianqian Wang, Yen-Yu Chang, Ruojin Cai, Zhengqi Li, Bharath Hariharan, Aleksander Holynski, and Noah Snavely. Tracking everything everywhere all at once. In International Conference on Computer Vision, 2023. 1, 6, 7 [56] Chuan Wen, Xingyu Lin, John So, Kai Chen, Qi Dou, Yang Gao, and Pieter Abbeel. Any-point trajectory modeling for policy learning. arXiv preprint arXiv:2401.00025, 2023. 1, 2 [57] Weijia Wu, Zhuang Li, Yuchao Gu, Rui Zhao, Yefei He, David Junhao Zhang, Mike Zheng Shou, Yan Li, Tingting Gao, and Di Zhang. Draganything: Motion control for anything using entity representation. In European Conference on Computer Vision, pages 331348. Springer, 2024. 1 [58] Yuxi Xiao, Qianqian Wang, Shangzhan Zhang, Nan Xue, Sida Peng, Yujun Shen, and Xiaowei Zhou. Spatialtracker: In Proceedings of Tracking any 2d pixels in 3d space. the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 2040620417, 2024. 1 [59] Emilie Yu, Kevin Blackburn-Matzen, Cuong Nguyen, Oliver Wang, Rubaiat Habib Kazi, and Adrien Bousseau. VideoDoodles: Hand-drawn animations on videos with scene-aware canvases. ACM TOG, 42(4):112, 2023. 1 [60] Chengbo Yuan, Chuan Wen, Tong Zhang, and Yang Gao. General flow as foundation affordance for scalable robot learning. arXiv preprint arXiv:2401.11439, 2024. [61] Yang Zheng, Adam Harley, Bokui Shen, Gordon Wetzstein, and Leonidas Guibas. Pointodyssey: large-scale synthetic dataset for long-term point tracking. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 1985519865, 2023. 1 [62] Haitao Zhou, Chuang Wang, Rui Nie, Jinxiao Lin, Dongdong Yu, Qian Yu, and Changhu Wang. Trackgo: flexible and efficient method for controllable video generation. arXiv preprint arXiv:2408.11475, 2024. 1, 2 [45] Jianlin Su, Yu Lu, Shengfeng Pan, Bo Wen, and Yunfeng Liu. Roformer: Enhanced transformer with rotary position embedding. arXiv preprint arXiv:2104.09864, 2021. 7, 9 [46] Xinglong Sun, Adam Harley, and Leonidas Guibas. Refining pre-trained motion models. In 2024 IEEE International Conference on Robotics and Automation (ICRA), 2024. 5 [47] Richard Szeliski. Computer vision: algorithms and applications. Springer Nature, 2022. 1 [48] Gemma Team, Thomas Mesnard, Cassidy Hardin, Robert Dadashi, Surya Bhupatiraju, Shreya Pathak, Laurent Sifre, Morgane Rivière, Mihir Sanjay Kale, Juliette Love, Pouya Tafti, Léonard Hussenot, Pier Giuseppe Sessa, Aakanksha Chowdhery, Adam Roberts, Aditya Barua, Alex Botev, Alex Castro-Ros, Ambrose Slone, Amélie Héliou, Andrea Tacchetti, Anna Bulanova, Antonia Paterson, Beth Tsai, Bobak Shahriari, Charline Le Lan, Christopher A. Choquette-Choo, Clément Crepy, Daniel Cer, Daphne Ippolito, David Reid, Elena Buchatskaya, Eric Ni, Eric Noland, Geng Yan, George Tucker, George-Christian Muraru, Grigory Rozhdestvenskiy, Henryk Michalewski, Ian Tenney, Ivan Grishchenko, Jacob Austin, James Keeling, Jane Labanowski, Jean-Baptiste Lespiau, Jeff Stanway, Jenny Brennan, Jeremy Chen, Johan Ferret, Justin Chiu, Justin Mao-Jones, Katherine Lee, Kathy Yu, Katie Millican, Lars Lowe Sjoesund, Lisa Lee, Lucas Dixon, Machel Reid, Maciej Mikuła, Mateo Wirth, Michael Sharman, Nikolai Chinaev, Nithum Thain, Olivier Bachem, Oscar Chang, Oscar Wahltinez, Paige Bailey, Paul Michel, Petko Yotov, Rahma Chaabouni, Ramona Comanescu, Reena Jana, Rohan Anil, Ross McIlroy, Ruibo Liu, Ryan Mullins, Samuel Smith, Sebastian Borgeaud, Sertan Girgin, Sholto Douglas, Shree Pandya, Siamak Shakeri, Soham De, Ted Klimenko, Tom Hennigan, Vlad Feinberg, Wojciech Stokowiec, Yu hui Chen, Zafarali Ahmed, Zhitao Gong, Tris Warkentin, Ludovic Peran, Minh Giang, Clément Farabet, Oriol Vinyals, Jeff Dean, Koray Kavukcuoglu, Demis Hassabis, Zoubin Ghahramani, Douglas Eck, Joelle Barral, Fernando Pereira, Eli Collins, Armand Joulin, Noah Fiedel, Evan Senter, Alek Andreev, and Kathleen Kenealy. Gemma: Open models based on gemini research and technology, 2024. 1, 9 [49] Zachary Teed and Jia Deng. Raft: Recurrent all-pairs field transforms for optical flow. In Computer VisionECCV 2020: 16th European Conference, Glasgow, UK, August 2328, 2020, Proceedings, Part II 16, pages 402419. Springer, 2020. [50] Zhan Tong, Yibing Song, Jue Wang, and Limin Wang. Videomae: Masked autoencoders are data-efficient learners for self-supervised video pre-training. Advances in neural information processing systems, 35:1007810093, 2022. 2, 4 [51] Narek Tumanyan, Assaf Singer, Shai Bagon, and Tali Dekel. Dino-tracker: Taming dino for self-supervised point tracking in single video. arXiv preprint arXiv:2403.14548, 2024. 6, 7 [52] Aaron Van Den Oord, Sander Dieleman, Heiga Zen, Karen Simonyan, Oriol Vinyals, Alex Graves, Nal Kalchbrenner, Andrew Senior, Koray Kavukcuoglu, et al. Wavenet: generative model for raw audio. arXiv preprint arXiv:1609.03499, 12, 2016. 3 Supplementary material - TAPNext: Tracking Any Point as Next Token Prediction A. Frequently Asked Questions Is there reason for categorization of frame/window/video latency? Cant we just run any model e.g. with frame latency (at timestep we feed frames 1, 2, , and obtain track prediction right after the frame was fed) This indeed may even turn offline trackers into online ones. However, we would argue that this effectively wont solve the latency problem. The reason is that when such tracker receives the new frame, it needs to reprocess either all previous frames (for offline trackers) or window of several recent frames (for window-based trackers). In either case, the time before the prediction is significant effectively disallowing real-time tracking at high frame rate (see Table 2 for quantification). Why does TAPNext require so much more data and time to train? TAPNext is much more generic model, performing only attention and SSM scanning without any custom components. The generality of TAPNext comes at the cost of much higher compute needed for training (i.e. large batch and longer optimization). TAPNext uses more synthetic and real data to train than previous methods. What is the performance of previous methods given such big datasets remains unclear. Currently there is no standartization of which dataset to use, among previous methods. For exampe, TAPIR uses 100000 videos with camera panning enabled but no motion blur. LocoTrack uses 11000 synthetic videos for training with panning but no motion blur. TAPTR uses also uses 11000 training videos which inlude motion blur but dont include camera panning. All aforementioned methods use 24 frames videos while CoTracker3 trains on synthetic videos of 64 frames and its training dataset includes only 6000 training videos at the resolution of 512 512 (compared to 256 256 for previous methods). This way all previous methods use training datasets of different size (which is varied by two orders of magnitude between CoTracker3 and TAPIR), length, resolution and visual properties (motion blur, camera panning). How does TAPNext learn to recover from occlusions? For each point query TAPNext has the corresponding sequence of SSM recurrent hidden states (since SSM is form of RNN) that contain the information about the tracked point. Even when the visaul occlusion happens, this information is still being processed by recurrent SSM and the corresponding output predicts the coordinate and occlusion flag. Why claiming that the method generalizes to 5x longer videos? This seems to be weaker statement than prior methods can do given that they track potentially infinite videos. TAPNext marks the new stage of point tracking methods with no (tracking) inductive biases that are entirely data driven. On one hand this gives scalability and SOTA tracking quality that comes from the scale of the model and data. On the other hand, due to that it is harder to satisfy guarantees like robustness to long videos. We hope that future research will address this problem of long term tracking of end-to-end trained models. Why not putting the TRecViT into the related work? While TRecViT is strong visual backbone, TAPNext could use other video backbone. Our selection of TRecViT was dictated by its efficiency - not only it requires significantly less time and memory to train (than e.g. purely attention counterpart), it also processes videos online and needs only one previous recurrent state to perform inference. Due to this reason and since TAPNexts idea is connected to how TRecViT processes tokens, we included the description of the latter. B. Attention visualization For convenience, larger versions of Figures 3 and 4 are reproduced in the appendix as Figures 6 and 7. We also show the raw spatial attention maps of the attention heads from the same layer in Figure 8. Full videos of attention visualization can be found in the supplementary files. Notably in the point-to-point attention video there is strong connection between clusters on different objects, but as the video progresses and the objects (colored in red and blue) move independently these connections quickly disappear. This hints at the models emergent motion segmentation ability. C. Joint-Tracking and Support Points Individual Points + 4x4 global + 9x9 local grid"
        },
        {
            "title": "Joint Points",
            "content": "AJ PTS (δavg) OA 62.2 62.4 76.1 76. 90.9 90.5 Table 5. Comparison of joint query point tracking v.s. individual queries and support points on DAVIS first evaluation of TAPNext-B Figure 5. Video Completion by TAPNext variant. Left: Outputs of patch-level linear pixel heads. Right: Inputs to the model (Visible or masked image and points). Figure 6. Three attention patterns learned by TAPNext. We visualize attention maps where the attention queries are the point track tokens and the keys are image tokens, which correspond to 8 8 patches. Each row is certain (layer, head) pair. We observe patterns: (top) Cost-volume-like attention head; (middle) Coordinate-based readout head; (bottom) motion-cluster-based readout head. Note that these are just intermediate heads in the backbone. Figure 7. Point-to-point attention map visualizations. Tracked points are nodes and (scaled) attention weights are edges, the thicker the edge the higher the weight between points. Two frames from video are used to visualize two attention layers. Note that in all images we see strong attention between points on objects that are moving together. Figure 8. X-axis: layers; Y-axis: attention heads. We visualize attention maps for the TAPNext-S model which has 12 layers and 6 attention heads. We visualize the same video as in Figure 6, specifically the timestep corresponding to the leftmost column. Also, we use the same qeury point as in Figure 6. The attention maps visualize the attention where query is the track tokens and key is image patch, we visualize it for every head in every layer. The patterns found in Figure 6 moslty are repeated accross the model. Because TAPNext processes query points jointly there is concern that semantic correlation between query points can give it an unfair advantage compared to methods that process queries individually. Therefore we use the methodology from Cotracker [26, 27] to evaluate tracking of one point at time with additional query points sampled from local and global regular grids. Similar to Cotracker we found that TAPNext benefits from mainly from local support grid of points (Figure 9). We found no major difference in performance between the one point evaluation (with the best support point scheme) compared to evaluating on all query points jointly, see Table 5. D. Coordinate Prediction via Classification The coordinate prediction head of TAPNext predicts (x, y) [0, H] [0, ] coordinates. The and coordinates are discretized into = 256 values, corresponding to pixel locations, and the head outputs the discrete distribution (pi where 0 < < n, Rn) for both coordinates independently. In particular, each point token is passed to the MLP where the last layer is softmax. To obtain the final, sub-pixel, coordinate we use the truncated soft argmax operation (the same as TAPNet [11]). First we truncate the probability distribution around argmax: we set all probability bins that are more than steps away from argmax to zero ( = 20 in our experiments). After that we renormalize the probability distribution since after setting some probability bins to zero, the result is no longer probability distribution. After we compute the new truncated probability distribution, we simply compute the expected probability bin: ˆx ="
        },
        {
            "title": "H\nn",
            "content": "n (cid:88) (cid:18) j=1 pj 1[j arg max ] k=1 pk 1[k arg max ] (cid:80)n (cid:19) The above equation defines the continuous coordinate prediction ˆx for the coordinate (valued in the range [0, H]). We use the same formula to compute the prediction for the coordinate denoted ˆy."
        },
        {
            "title": "This parameterization can be very easily implemented in",
            "content": "Jax: import jax.numpy as jnp def trunc_softargmax(p, delta=20): = p.shape[0] # is vector js = jnp.arange(n) = jnp.argmax(p) = jnp.abs(js - j) <= delta *= /= p.sum() return (p * js).sum() As we mention earlier, we use two loss functions for coordinate prediction: Huber loss on the continuous prediction (ˆx and ˆy) and softmax cross entropy for discrete prediction (px and py): L(x, y, px, py) = w1LH (x, ˆx) + w2LH (y, ˆy) + w3LC(one_hot(x), px) + w4LC(one_hot(y), py) LH is the huber loss, LC is the softmax with crossentropy loss. Coefficients w1, w2, w3, w4 are weights applied to each loss component. In our experiments, we use w1 = w2 = 0.1 and w3 = w4 = 1.0. This combined loss is applied to every layer. This means that the point track tokens after every ViT block are fed to the coordinate heads (shared MLPs with softmax output) and the the aforementioned loss is applied to the outputs of the coordinate heads. Similarly, the visibility head which is also an MLP with the sigmoid as the final activation. The loss for this head is binary cross entropy. Like the coordinate head, the visibility head is applied to every layer. Note that, despite using parameterization that produces bounded ranges for coordinate values ((x, y) [0, H] [0, ]), it is still possible to predict (of course, not with the same trained model) the coodinates out of the view, similarly to what other models (e.g. CoTracker [26]) do. For that, we could simply map e.g. the coordinate to some range [d, + d] instead of [0, H] for some positive d. This way, some probability bins would be responsible for out-ofview prediction while some other will do in-view prediction. Since out-of-view prediction is not part of the TAP-Vid benchmark, we do not use it. E. TAPNext Hyperparameters We sample batches containing query points of shape [B, Q, 3], where = 256 is the batch size, = 256 number of query points per video. The last dimension represents the t, x, coordinate of each query point. Importantly, we train our model on mixture that simulates query point being in the beginning of the video (t = 0) and at the intermediate timestep (t > 0). The weights of this mixture are [0.8, 0.2], respectively. The = 0 mixture component contains query points queried at the 0th frame in the video. The > 0 mixture component contains the ones queried at any timestep, not necessarily starting at the 0th frame. Since our model is causal, it cannot track points before point query is given. Therefore, for the second component we mask the coordinate losses for frames preceding the known query and set the visibility label to zero. We use weight of 1.0 for both visibility and coordinate classification losses, and 0.1 for coordinate regression. We use the AdamW optimizer with weight decay of 0.01 for 300, 000 steps using cosine learning rate schedule with 2500 warm-up steps, peak and final learning rate values of 0.001 (for -S model) and 0, respectively. We clip gradient norm to 1.0. We found it important to use 8 8 patches, smaller than the 16 16 used in ViT for classification, aligning with the intuition that smaller patches work well for spatially fine-grained tasks. The full list of hyperparameters is available in Tables 6 and 7. We implement the model in Jax and use TPUv6e for training. Specifically, we use 16 16 TPU slice to train both TAPNext-S and TAPNext-B. Since we are using large batch (B = 256 48 = 12288 images in our case), we use activation checkpointing (implemented in the same way as"
        },
        {
            "title": "Name",
            "content": "TAPNext-S TAPNext-B layers parameters attention heads width dropout width LRU width heads 56M 194M 12 384 0.0 384 768 12 12 768 0.0 768"
        },
        {
            "title": "SSM Block",
            "content": "Table 6. TAPNext hyperparameters specific to each size of the model. in the original ViT3). This setup is roughly equivalent to 50 H100 GPUs in both compute and memory and our training takes 4 days for TAPNext-S and 5 days for TAPNext-B. Note that despite large compute and memory requirement for training, TAPNext inference runs quickly on single GPU (see Table 2). F. Strided Evaluation with TAPNext as Causal"
        },
        {
            "title": "Tracker",
            "content": "Recall that in training when TAPNext is trained on queries at > 0 (i.e. after the 0th frame), the coordinate losses corresponding to frames preceding the query is masked and visible target is set to 0.0. For complete and comparable evaluation, however, we require track predictions for every frame in the sequence. To obtain these predictions with our causal, per-frame tracker, we run the tracker both forwards and backwards in time starting at the frame corresponding to the query. These two sequences of predictions corresponding to normal and reverse time processing are then concatenated together to obtain the full sequence of prediction. For strided evaluation this process is repeated for every query point in the sequence. G. Generation Pipeline for Large Scale Synthetic Dataset The original MOVi-F is similar to MOVi-E but includes random motion blur and is rendered at 512 512 resolution, with 256 256 downscaled option. To enhance model performance on real-world videos with panning, we modify the MOVi-E dataset to adjust the cameras look at point to follow random linear trajectory by sampling start point within medium-sized sphere (4 unit radius), travelthrough point near the center of the workspace (1 unit radius), and an end point extrapolated along the line from to by up to 50% of their distance. The \"look at\" path randomly switches between and a. This modification ini3https://github.com/google-research/big_vision/blob/main/big_vision/models/vit.py generation pipeline, combining data generation pipelines from both Kubric [18] and TAPIR [12]. Building on this, we created new large-scale Panning Kubric MOVi-F dataset with 500,000 videos. The rendered videos combine both panning effect and motion blur. We then increase each video duration from the default 24 frames to 48 frames. These enhancements allow more robust training and long term inference stability for TAPNext, addressing the challenges posed by real-world long term point tracking. Figure 10 shows the comparison between the new dataset and existing ones. H. Prediction Visualization on DAVIS We present additional examples of point track predictions from TAPNext on the TAPVid-DAVIS dataset in Figure 11. All DAVIS track videos can be found in the supplementary files. The query points are all initialized in the first frame, and TAPNext performs tracking in causal manner. While the model occasionally makes errors over longer time spans due to the inherent challenges of maintaining longterm precision in causal tracking, TAPNext demonstrates robust performance by reliably tracks points across wide range of scenarios, including foreground and background elements, as well as small and large motions."
        },
        {
            "title": "Optimization",
            "content": "Optimizer Global batch size Number of queries per video Max gradient norm Weight decay Number of optimization steps Warmup Number of warmup steps LR before warmup LR schedule Peak LR (TAPNext-S) Peak LR (TAPNext-B) Final LR Precision Regression loss weight(s) Classification loss weight(s)"
        },
        {
            "title": "Data",
            "content": "Dataset size (videos) Dataset resolution Number of frames per video Camera panning Motion blur Prob. of sampling query with = 0 Prob. of sampling query with >"
        },
        {
            "title": "Model",
            "content": "Patch size Image position embedding Point position embedding Point position embedding resolution MLP head number of layers MLP head hidden size MLP head activation Softargmax threshold () Coord. softmax temperature Huber loss weight Coordinate CE loss weight Visibility CE loss weight"
        },
        {
            "title": "Value",
            "content": "AdamW 256 256 1.0 0.01 300, 000 linear 2500 0 cosine 0.001 0.0005 0 float32 0.1 1.0 500.000 256 256 48 Enabled Enabled 0.8 0.2 8 8 learned sincos2d 256 256 3 256 GELU 20 2 0.1 1.0 1.0 Table 7. TAPNext hyperparameters tially resulted in 100K video Panning Kubric MOVi-E dataset. To further exploit the scalability of TAPNext and improve its generalization capability to longer real world videos, we developed new Panning Kubric MOVi-F data Figure 9. One point at time tracking performance with various support points grid configurations. (a) Kubric MOVi-E dataset (b) Kubric MOVi-F dataset (c) MOVi-E Panning dataset Figure 11. Prediction on TAPVid-DAVIS We show the tail visualization of semi-dense point track predictions for 5 example videos on DAVIS dataset. For each video, we show the first query frame, 20th frame and 40th frame. (d) MOVi-F Panning dataset (Ours) Figure 10. Kubric dataset comparison We show two exampler videos for each dataset with first frame, middle frame and last frame, with groundtruth point tracks."
        }
    ],
    "affiliations": [
        "Canada CIFAR AI Chair",
        "Chandar Research Lab",
        "Google DeepMind",
        "Mila - Quebec AI Institute",
        "Polytechnique Montreal",
        "University College London",
        "Université de Montréal"
    ]
}