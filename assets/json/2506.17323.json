{
    "paper_title": "I Know Which LLM Wrote Your Code Last Summer: LLM generated Code Stylometry for Authorship Attribution",
    "authors": [
        "Tamas Bisztray",
        "Bilel Cherif",
        "Richard A. Dubniczky",
        "Nils Gruschka",
        "Bertalan Borsos",
        "Mohamed Amine Ferrag",
        "Attila Kovacs",
        "Vasileios Mavroeidis",
        "Norbert Tihanyi"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Detecting AI-generated code, deepfakes, and other synthetic content is an emerging research challenge. As code generated by Large Language Models (LLMs) becomes more common, identifying the specific model behind each sample is increasingly important. This paper presents the first systematic study of LLM authorship attribution for C programs. We released CodeT5-Authorship, a novel model that uses only the encoder layers from the original CodeT5 encoder-decoder architecture, discarding the decoder to focus on classification. Our model's encoder output (first token) is passed through a two-layer classification head with GELU activation and dropout, producing a probability distribution over possible authors. To evaluate our approach, we introduce LLM-AuthorBench, a benchmark of 32,000 compilable C programs generated by eight state-of-the-art LLMs across diverse tasks. We compare our model to seven traditional ML classifiers and eight fine-tuned transformer models, including BERT, RoBERTa, CodeBERT, ModernBERT, DistilBERT, DeBERTa-V3, Longformer, and LoRA-fine-tuned Qwen2-1.5B. In binary classification, our model achieves 97.56% accuracy in distinguishing C programs generated by closely related models such as GPT-4.1 and GPT-4o, and 95.40% accuracy for multi-class attribution among five leading LLMs (Gemini 2.5 Flash, Claude 3.5 Haiku, GPT-4.1, Llama 3.3, and DeepSeek-V3). To support open science, we release the CodeT5-Authorship architecture, the LLM-AuthorBench benchmark, and all relevant Google Colab scripts on GitHub: https://github.com/LLMauthorbench/."
        },
        {
            "title": "Start",
            "content": "Tamas Bisztray University of Oslo Oslo, Norway tamasbi@ifi.uio.no Nils Gruschka University of Oslo Oslo, Norway nilsgrus@ifi.uio.no Know Which LLM Wrote Your Code Last Summer: LLM generated Code Stylometry for Authorship Attribution Bilel Cherif Technology Innovation Institute Abu Dhabi, United Arab Emirates bilel.cherif@tii.ae Richard A. Dubniczky E√∂tv√∂s L√≥r√°nd University Budapest, Hungary richard@dubniczky.com 5 2 0 J 8 1 ] . [ 1 3 2 3 7 1 . 6 0 5 2 : r Bertalan Borsos E√∂tv√∂s L√≥r√°nd University Budapest, Hungary borsosb@inf.elte.hu Mohamed Amine Ferrag Guelma University Guelma, Algeria mohamed.amine.ferrag@gmail.com Norbert Tihanyi Technology Innovation Institute Abu Dhabi, United Arab Emirates norbert.tihanyi@tii.ae Attila Kovacs E√∂tv√∂s L√≥r√°nd University Budapest, Hungary attila@inf.elte.hu Vasileios Mavroeidis University of Oslo, Cyentific AS Oslo, Norway vasileim@ifi.uio.no Abstract Detecting AI-generated code, deepfakes, and other synthetic content is an emerging research challenge. As code generated by Large Language Models (LLMs) becomes more common, identifying the specific model behind each sample is increasingly important. This paper presents the first systematic study of LLM authorship attribution for programs. We released CodeT5-Authorship, novel model that uses only the encoder layers from the original CodeT5 encoder-decoder architecture, discarding the decoder to focus on classification. Our models encoder output (first token) is passed through two-layer classification head with GELU activation and dropout, producing probability distribution over possible authors. To evaluate our approach, we introduce LLM-AuthorBench, benchmark of 32,000 compilable programs generated by eight state-ofthe-art LLMs across diverse tasks. We compare our model to seven traditional ML classifiers and eight fine-tuned transformer models, including BERT, RoBERTa, CodeBERT, ModernBERT, DistilBERT, DeBERTa-V3, Longformer, and LoRA-fine-tuned Qwen2-1.5B. In binary classification, our model achieves 97.56% accuracy in distinguishing programs generated by closely related models such as GPT-4.1 and GPT-4o, and 95.40% accuracy for multi-class attribution among five leading LLMs (Gemini 2.5 Flash, Claude 3.5 Haiku, GPT-4.1, Llama 3.3, and DeepSeek-V3). To support open science, we release the CodeT5-Authorship architecture, the LLMAuthorBench benchmark, and all relevant Google Colab scripts on GitHub: https://github.com/LLMauthorbench/. CCS Concepts Computing methodologies Supervised learning by classification. Keywords AI-generated code, code authorship attribution, code stylometry, large language models, LLM fingerprinting, watermarking, supervised classification, digital forensics Corresponding author"
        },
        {
            "title": "1 Introduction\nAs AI-generated content is getting more widespread, authorship\nattribution‚Äîthe task of linking unknown content to its creator‚Äîis\nbecoming essential for accountability in areas ranging from pre-\nventing plagiarism to ensuring legal integrity [10, 32, 37]. With\nrecent advances in Large Language Models (LLMs), this challenge\nhas become even more significant, as LLMs can now automatically\ngenerate high-quality text and code. Authorship analysis comprises\nfive main tasks: human attribution, profiling, human-vs-LLM detec-\ntion, human-LLM coauthor detection, and LLM attribution. These\ntasks are applicable to both text and code, as shown in Figure 1.",
            "content": "Figure 1: Attribution goals for source code and text. Legend: well-researched; limited research; not investigated Human attribution involves identifying the individual author of given piece of text or code. Profiling seeks to infer characteristics or traits of the author, such as demographics or writing style. Human-vs-LLM detection aims to determine whether content was produced by human or an LLM. Human-LLM coauthor detection focuses on identifying instances where both human and an LLM have collaborated on the same work. LLM attribution attempts to specify which language model generated the content. While AI-vs-human detection is well studied, attributing source code to specific LLMs remains unexplored. Analogous to tracing photo back to its camera [20, 29], model-level attribution could enhance accountability, support academic integrity, and strengthen code security, especially since over 60% of model-generated code contains vulnerabilities [58]. In this paper, we examine the attribution of source code generated by various state-of-the-art LLMs, focusing our analysis on the following research questions: RQ1: Can we perform authorship attribution on LLMgenerated code using arbitrary programming tasks? RQ2: Which machine learning (ML) and transformer models perform the best for black-box source code attribution? To answer these questions, we present four key contributions: CodeT5-Authorship: We release novel model based on the encoder layers of CodeT5, optimized for authorship classification. Our PyTorch implementation uses two-layer classification head with GELU activation and dropout, enabling accurate attribution of code to its source LLM. LLM-AuthorBench: We release public dataset of, 32 000 compilable programs, each labeled by its source LLM, covering eight state-of-the-art models and diverse coding tasks. This benchmark serves as standard reference for comparing various Transformer and machine learning models in the task of code authorship attribution. Comprehensive evaluation: Using the newly released LLM-AuthorBench benchmark, we compare the CodeT5Authorship model with traditional machine learning classifiers, like Random Forest (RF), XGBoost, k-nearest neighbors (KNN), Support Vector Machine (SVM), and Decision Tree (DT), as well as eight fine-tuned transformer models: BERT, RoBERTa, CodeBERT, ModernBERT, DistilBERT, DeBERTa-V3, Longformer, and LoRA-optimized Qwen2-1.5B. High-accuracy attribution: We demonstrate that modellevel attribution is both feasible and accurate, achieving up to 97.56% accuracy in binary classification within model families (e.g., GPT-4.1 vs. GPT-4o) and up to 95.40% accuracy in multi-class attribution. Open science: To support reproducibility, our dataset and the corresponding Google Colab training code are available on GitHub to facilitate future work in LLM authorship attribution: https://github.com/LLMauthorbench. By shifting from simple AI-vs-human detection to precise model attribution for LLM-generated source code, we unlock new opportunities for accountability and security in software engineering. The rest of the paper is organized as follows. Section 2 reviews related work. Section 3 outlines the methodology used to construct the dataset and to train the classifiers. Section 4 presents experimental results, Section 5 discusses limitations and future work, while Section 6 concludes the paper."
        },
        {
            "title": "2 Related Work\nFirst, this section examines techniques for authorship attribution,\nfollowed by an overview of the categories shown in Figure 1.",
            "content": "Bisztray et al."
        },
        {
            "title": "2.1.6 Deep Learning and Neural Network Approaches. Neural net-\nwork models have advanced authorship attribution by automati-\ncally learning feature representations from code, eliminating man-\nual feature engineering [3]. Sequence-based architectures‚Äîlike\nRNNs/LSTMs and CNNs, applied to token or character streams‚Äî\ncapture hierarchical stylistic patterns and subtle cues, that manual",
            "content": "I Know Which LLM Wrote Your Code Last Summer: LLM generated Code Stylometry for Authorship Attribution Feature Category Text / Code Typical Techniques Byte / character ùëõ-gram frequencies [23, 36] Stylometric (Raw Lexical) Both Token ùëõ-gram frequencies (keywords, identifiers, operators & symbols) [24] Stylometric (Lexical) Average identifier length [12, 13] Stylometric (Lexical) Variable-naming conventions (casing, prefixes, suffixes) [48] Stylometric (Lexical) AST node/bigram statistics (bag-of-nodes) [12, 13] Stylometric (Syntactic) AST path & graph embeddings [4, 59] Learned (Structural) Code-complexity metrics (cyclomatic, LOC/statementlength, nesting depth) [36, 57, 58] Statistical (Structural) Both Code Code Code Code Code Whitespace, indentation & brace style [48] Stylometric (Formatting) Code Commenting style & frequency [56] Stylometric (Content) API / library usage [34] Stylometric (Content) Code"
        },
        {
            "title": "Both",
            "content": "Control-flow patterns [61] Stylometric (Sem./Struct.)"
        },
        {
            "title": "Code",
            "content": "Topic / semantic content [61, 67] Stylometric (Semantic)"
        },
        {
            "title": "Both",
            "content": "Language-agnostic slices of raw bytes/chars; robust to obfuscation; fed to SVM or random-forest baselines Contiguous token sequences (ùëõ = 15) for SVM, na√Øve Bayes, CNN/RNN; unigram slice counts keywords and punctuation symbols Mean characters per variable / function / class name; lightweight cue for RF/SVM, implicitly modelled by token-level RNN/CNN Ratios of camelCase : snake_case, Hungarian prefixes (e.g., m_), suffixes (_t, _ptr), digits/underscores; token features for RF/SVM or as side-inputs to transformers Counts of AST node types, parent-child bigrams and shallow subtree sizes; robust to renaming/formatting; fed to RF/SVM baselines Context-path multiset, PDG/CFG graphlets encoded by GGNN, GraphCodeBERT or UniXCoder; dense style vectors compared via cosine ùëò-NN or light MLP Classical software-engineering metrics delivered as numeric features to RF/SVM; complement AST cues Tab-vs-space ratio, brace placement (1TBS, Allman), trailing whitespace; strong handcrafted signals in RF/SVM Comment-code density, inline vs. block preference, docstring length; NLP on comments plus density counts in RF/SVM One-hot or TF-IDF vectors of imported packages and fully qualified calls; deep variants use GNNs over call graphs Loop-type distribution, recursion, switch vs. if-chains; extracted from CFG/AST and encoded for RF/GNN LDA topics on identifiers/comments or transformer embeddings; compared via cosine similarity or fed to MLP classifiers Information-theoretic global metrics (entropy, LM perplexity, Zipf deviation, log-rank, intrinsic dimension) [14] Statistical (Info-theoretic)"
        },
        {
            "title": "Both",
            "content": "Single-value corpus-level statistics plugged into ensemble detectors or used as anomaly thresholds Lexical-richness metrics (TTR, Yules ùêæ ) [8, 39] Stylometric + Statistical"
        },
        {
            "title": "Both",
            "content": "Neural code-embedding vectors [2, 5, 67] Learned (Deep Representation)"
        },
        {
            "title": "Code",
            "content": "Token-economy vs. repetition; Yules ùêæ mitigates length bias of TTR; entered as scalar features in RF/SVM Mean-pooled transformer embeddings (CodeBERT, GraphCodeBERT) or Siamese contrastive encoders; Dense vector signatures fine-tuned on style; compared with cosine ùëò-NN or fed to lightweight MLPs Opcode/idiom ùëõ-grams & tool-chain fingerprints [2, 12, 54] Stylometric (Binary) Runtime behavioural traces [61] Stylometric (Dynamic)"
        },
        {
            "title": "Code",
            "content": "Opcode sequences, instruction idioms, call-graphlets and compiler-option artefacts from disassembly/decompilation; RF, RNN or mixture-of-experts classifiers System-call ùëõ-grams, memory-allocation and branch-coverage profiles captured in sandbox execution; fused with static features via RF/GNN for obfuscation resilience Table 1: Feature families for effective use in source-code authorship or origin attribution. feature engineering may miss [66]. Although these models outperform classical methods as author counts grow, they require large labeled datasets to train effectively [66]."
        },
        {
            "title": "2.1.7 Pre-trained Language Models. These models are trained on\nmassive corpora of text using unsupervised or self-supervised meth-\nods, learning rich contextual embeddings that capture syntactic\nand semantic nuances far beyond traditional bag-of-words features.\nGeneric text encoders such as BERT [19] and RoBERTa [41] are now\njoined by code-aware variants‚ÄîCodeBERT [21], GraphCodeBERT\n[26], CodeT5 [62]‚Äîthat fuse token, AST, and data-flow cues. For\nauthorship, their embeddings can be fine-tuned with a classifier\nor shaped via contrastive learning, using slanted triangular sched-\nules and gradual unfreezing to curb catastrophic forgetting [33].\nThese models reach state-of-the-art accuracy with almost no man-\nual features, but demand heavy compute, risk domain overfitting,\nare adversarially brittle, and still face tight context limits.",
            "content": "Large Language Model-Based Attribution. LLMs sidestep the 2.1.8 heavy, label-hungry training of classical ML/DL pipelines by doing zero or few-shot attribution through in-context prompts [11, 14]. Because they learn universal code patterns, they transfer smoothly across languages and domains. In [14], GPT-4 reaches 65-69% accuracy on real-world datasets with 500 + authors using only one reference snippet per author, and shows some resistance to superficial obfuscation. Chain-of-thought prompting can even supply human-readable rationales [64]. Practical hurdles remain-high compute cost, opaque decision logic, uncalibrated confidence, and steep usage fees-limiting LLMs in large-scale pipelines."
        },
        {
            "title": "2.2 Code Attribution Tasks and Benchmarks\n2.2.1 Benchmarking Datasets. Except for black-box detectors, most\nLLM-attribution methods still require carefully labeled corpora\nto surface stylistic cues. Distinguishing two models with highly\ndifferent styles is trivial‚Äîfor instance, a small 2-3 B-parameter\nmodel can be easily distinguished from a GPT-4-class model by\ntheir difference in C code complexity, length, or compilability. The\ntask becomes difficult, however, as the number of models grows\nand stylistic footprints converge. Rigorous evaluation therefore\ndemands a large, heterogeneous dataset that also covers closely\nrelated model families. Our goal is to test attribution among both\nclosely related variants like GPT-4o and GPT-4.1, and different state-\nof-the-art model families. Existing datasets (Table 2) fall short of",
            "content": "Dataset Domain Size / Notes Google Code Jam Contest code 20082017; 12 authors Codeforces Contest code Hundreds of authors; multi-lang GitHub OSS Corpus Real-world repos 5001 authors; diverse files BigCloneBench Java clones 8 methods; 6 clone pairs POJ-104 Student OJ 52 C/C++ solutions Karnalim Corpus Plagiarism 467 Java files (7 tasks) APT Malware APT binaries 3.6 samples; 12 groups BCCC_AuthAtt-24 C++ authorship 24 files; 3 authors FormAIv2 vulnerabilities 331,000 programs, 9 LLM authors Table 2: Potential datasets for code authorship. our needs; therefore, we construct new dataset featuring code generated by eight state-of-the-art LLMs to more effectively address our research questions."
        },
        {
            "title": "2.2.4 AI or Human Classification. Early work tried to repurpose\ntext-based detectors such as GPTZero, OpenAI‚Äôs classifier, and\nGLTR; however, several studies showed that these systems general-\nize poorly to source code [46, 55]. Researchers therefore adapted",
            "content": "Paper (Year)"
        },
        {
            "title": "Dataset",
            "content": "# Auth. Acc. Abuhamad et al. (2021) [2] GCJ + GitHub (C, C++, Java, and Python) 8,903 92.3% Abuhamad et al. (2018) [1] GCJ (all years) CaliskanIslam et al. (2015) [13] GCJ (C/C++) Alsulami et al. (2017) [5] GCJ (python) Frantzeskou et al. (2006) [23] 0SJava2 1 600 1 600 70 96% 92% 89% 97% Table 3: State-of-the-art human written code attribution Bisztray et al. ideas from text detection-perplexity analysis and zero-shot methods like DetectGPT [43]. Xu and Shengs CodeVision [65] measures token-level language-model perplexity and detects ChatGPTwritten homework solutions more reliably than na√Øve entropy checks. Nguyen et al.s GPTSniffer [44] combines syntax-aware sampling with ensemble scoring, reaching F1 0.95 (96 % accuracy) for distinguishing human and ChatGPT code in and Python. Across ten languages, Jin et al. [30] show that RoBERTa-based binary classifier can still separate StarCoder2 code from human code with 84.1 % 3.8 % accuracy. Choi et al. [14] further demonstrates that zeroor few-shot GPT-4 can attribute C++/Java code from 686 human authors with 68.7 % top-1 accuracy, indicating strong built-in authorship cues."
        },
        {
            "title": "2.2.6 LLM-Generated Code Attribution. Interest in tracing the au-\nthorship of LLM-generated code is rising, as shown by recent wa-\ntermarking schemes for code [18, 38]. Yet, systematic attribution\nstudies are still rare. The closest effort, LPcode/LPcodedec [47], tack-\nles multi-class attribution but only for LLM-paraphrased human\ncode, not for code produced outright by the models.",
            "content": "To our best knowledge, no prior work tackles large-scale attribution of genuinely LLM-generated code across multiple model families, gap this research fills. Unlike human authorswhose programs often reveal distinctive stylistic cues  (Table 1)  modern LLMs are trained on overlapping corpora drawn from millions of GitHub and Stack Overflow repositories. Because these models can emulate vast spectrum of coding stylesincluding each others-reliable attribution becomes significantly more ambiguous and technically demanding."
        },
        {
            "title": "3 Methodology\nTo evaluate LLM authorship attribution in C programming, we\nconstructed a large, diverse benchmark, LLM-AuthorBench by",
            "content": "(1) defining parameterized programming task templates; (2) creating 4000 different versions of the questions; (3) prompting eight different LLMs to generate the implementations of the tasks (32,000 in total); (4) curating the final corpus through deduplication and training/validation splitting, where 80% of the dataset is used for training, and 20% is for validation. We evaluated the CodeT5-Authorship model alongside traditional machine learning and Transformer models to address our research questions. Figure 2 outlines the main steps of our methodology, which are detailed in this section. Know Which LLM Wrote Your Code Last Summer: LLM generated Code Stylometry for Authorship Attribution Figure 2: The five-step methodology for LLM authorship attribution in code."
        },
        {
            "title": "3.1 Dataset creation (LLM-AuthorBench)\nFirst, we manually created ùëÅ = 300 distinct templates, each de-\nscribing a programming task with one or more variable parameters.\nExamples include:",
            "content": "Sorting: Write program to sort an array of {size} integers using bubble sort. Networking: Create program that connects to server IP {ip_address} on port {port} and sends the message {message}. Let = {ùë°1, ùë°2, . . . , ùë°ùëÅ } be the set of all question templates, where ùëÅ = is the total number of templates. For each template ùë°ùëñ , let ùëùùëñ denote the number of distinct questions that can be generated from ùë°ùëñ , i.e., ùë°ùëñ ùëùùëñ . Using the first example, size is chosen randomly between 1 and 100. In this case, ùë°1 100, since there are 100 possible distinct questions that can be generated from this template. For templates that have multiple variable placeholders, ùëùùëñ is calculated as the product of the number of possible values for each placeholder in the template. For example, if ùë° ùëó has two placeholders, {ùëé} and {ùëè}, with ùëõùëé and ùëõùëè possible values respectively, then ùëù ùëó = ùëõùëé ùëõùëè . Thus, for the entire set, (cid:32) ùêæùëñ (cid:214) ùëÅ (cid:33) ùëÉ = ùëõùëñùëò ùëñ=1 ùëò=1 where ùêæùëñ is the number of placeholders in ùë°ùëñ and ùëõùëñùëò is the number of possible values for the ùëò-th placeholder in ùë°ùëñ . For the 300 distinct question templates, approximately 2.1 billion unique questions can be generated. For this experiment, 4, 000 unique programming tasks were created which were given to 8 state-of-the-art LLMsGPT-4.1, GPT-4o, GPT-4o-mini, DeepSeek-v3, Qwen2.5-72B, Llama 3.3-70B, Claude-3.5-Haiku, and Gemini-2.5-Flashyielding total of 32, 000 programs. We have ensured that only unique programs are included in the final dataset. ùëÑ ùëÅ = 4"
        },
        {
            "title": "3.2 CodeT5-Authorship architecture\nIn addition to benchmarking off-the-shelf traditional ML and generic\nTransformer models (Section 3.3), we present a custom variant of\nCodeT5+, that is specifically tuned for code-authorship attribu-\ntion. Because code attribution is a fine-grained classification prob-\nlem, encoder-centric architectures are generally more beneficial,\nas we don‚Äôt need the verbose output capabilities of decoder archi-\ntectures. After exploring various design options, we selected the\npretrained CodeT5+ 770M parameter sequence-to-sequence model",
            "content": "and removed its decoder, resulting in streamlined, encoder-only network optimized for attribution tasks. This modified CodeT5 variant consistently outperforms all tested models, including both encoder-only and decoder-only alternatives, in our experiments. Full results are presented in the next section. As shown in Figure 3 the encoder block is connected to classification head that we implemented using the Pytorch library. The classification head is sequence of two linear layers connected, with an activation layer and dropout layer (20%) in between. The first linear layer reduces the dimensionality of the last hidden layer output after taking the first token embedding. After experimenting with various activation functions, we selected GELU, which delivered the best performance for our model. The final linear layer then produces probability vector assigning code attribution to each class. Figure 3: CodeT5-Authorship architecture"
        },
        {
            "title": "3.3.1 ML Training and Feature Selection. For classical machine-\nlearning, we extracted all features listed in Table 4 using the Joern\nframework 1 and general-purpose natural language processing\n(NLP) algorithms. A central question is whether such manual fea-\nture engineering can rival, or even beat, approaches that use BERT-\nstyle Transformers as automatic feature extractors. These metrics",
            "content": "1https://github.com/joernio/joern Bisztray et al. quantify structural, syntactic, and stylistic patterns inherent to programming styles. To clarify our notation for Table 4, let ùêπ denote source-code file containing ùëò functions, {ùëì1, . . . , ùëìùëò }."
        },
        {
            "title": "3.3.2 Transformer-based Approaches. Many state-of-the-art mod-\nels used in this comparison are based on the Transformer archi-\ntecture, originally introduced by Vaswani et al. [60]. The Trans-\nformer comprises encoder and decoder modules, and different mod-\nels may utilize one or both depending on their design and task type.\nEncoder-based architectures, such as BERT, RoBERTa, CodeBERT,\nand Longformer, are primarily designed for representation learning\nand sequence understanding. Encoder-only models like BERT em-\nploy bidirectional self attention, enabling every token to attend to\nboth its left and right context and yielding rich, sentence-level rep-\nresentations that are well suited to classification tasks [19]. Their\nprincipal limitation is computational: the quadratic complexity of\nfull self-attention restricts practical sequence length to roughly 512\nsub-word tokens. Decoder-only models (GPT style) rely on causal\n(left-to-right) attention and are pretrained for language generation.\nRecent variants‚Äîsuch as Qwen2‚Äîextend the usable context win-\ndow to 32 thousand tokens or more, rendering them capable of\nprocessing entire source code files without truncation. To evaluate\nthe performance of foundational models in our experiments, we\nfine-tune the following pretrained models:",
            "content": "BERT [19]. 12-layer bidirectional encoder (110 parameters) that looks at both left and right context, making it solid baseline for sequence-level classification. Its primary limitation is the 512-token window imposed by quadratic self-attention. ModernBERT [63]. major evolution of BERT, ModernBERT expands the original 512-token context window to up to 8,192 tokens. Alongside improved pre-training, positional encodings, and training methods, this enables stronger contextual representations and better performance-especially on code taskswhile keeping the encoder-only design. DistilBERT [50]. 6-layer, 66 M-parameter distillation of BERT that is 40% smaller and 60% faster at inference. It is ideal when GPU memory or latency is bottleneck, but it inherits the same 512-token cap and lacks code-specific pre-training. RoBERTa [41]. Keeps BERTs architecture yet removes nextsentence prediction, applies dynamic masking, and is trained on 10 times larger corpus. These changes systematically improve downstream accuracy, but the model is still encoder-only and restricted to 512 tokens. CodeBERT [21]. Builds on the RoBERTa backbone and is jointly pre-trained on paired natural-language / source-code data (CodeSearchNet). This bimodal signal lets it capture lexical and structural properties of code, making it promising candidate for authorship attribution compared to purely natural-language models, though the 512-token limit remains. Longformer [9]. Introduces sliding-window self-attention with optional global tokens, reducing complexity from ùëÇ (ùëõ2) to ùëÇ (ùëõ) and enabling context windows of up to 4096+ tokens. It therefore processes entire files that BERT-style models must truncate, while still benefiting from RoBERTas pre-training. DeBERTa-v3 [31]. Adds disentangled content and position embeddings plus an ELECTRA-style replaced-token objective, yielding stronger representations than RoBERTa on many NLP Know Which LLM Wrote Your Code Last Summer: LLM generated Code Stylometry for Authorship Attribution Table 4: Selected file-level and function-level metrics used in our authorship-attribution experiments. Metric Basic File Metrics Character count Line count Function count Function-Level Metrics (per function ùëìùëó ) Maximum nesting depth AST node term frequency AST bigram frequency Cyclomatic complexity Parameter count Lines of code (LOC) Variable complexity Return count Halstead Metrics (per ùëìùëó )"
        },
        {
            "title": "Effort",
            "content": "Formula / Symbol Description characters ùêø ùêπtotal = ùëò ùê∑max ( ùëìùëó ) = max ùëõùëñ AST( ùëìùëó ) depth(ùëõùëñ ) tf(ùë°, ùëìùëó ) = ùëÄ (ùë°, ùëìùëó )/ùëÅ bigram_tf(ùë°ùëé, ùë°ùëè , ùëìùëó ) = ùêµ (ùë°ùëé ùë°ùëè , ùëìùëó )/ùëÅ ùê∂ùê∂ ( ùëìùëó ) = #decision points + 1 ùëÉ ( ùëìùëó ) = params( ùëìùëó ) LOC( ùëìùëó ) = (cid:12) ùëâ ( ùëìùëó ) = {ùë£ùëñ ùë£ùëñ local vars of ùëìùëó } ùëÖ ( ùëìùëó ) = (cid:205) Ireturn (cid:12){ùë†ùëñ ùë†ùëñ exec. stmts of ùëìùëó }(cid:12) (cid:12) ùëõ1 ùëõ2 ùëÅ1 ùëÅ2 ùëâ = (ùëÅ1 + ùëÅ2 ) log2 (ùëõ1 + ùëõ2 ) ùê∑ = ùëõ1/2 ùëÅ2/ùëõ2 ùê∏ = ùê∑ ùëâ Total characters in the file, including whitespace and newline characters. Total lines in the file (code, comments, and blank lines). Number of top-level (non-nested) functions defined in the file. Deepest level of nested control structures within the function body. Relative frequency of each AST node type ùë° in the function. Relative frequency of ordered AST-node pairs (ùë°ùëé ùë°ùëè ) capturing local structure. Number of independent execution paths through the functions control-flow graph. Number of formal parameters the function accepts. Executable statement lines in the function (excludes blank and comment lines). Count of distinct local variables declared in the function body. Number of explicit return statements in the function. Unique operator tokens (e.g., +, if, call). Unique operand tokens (identifiers, literals). Total occurrences of operator tokens. Total occurrences of operand tokens. Information content of the implementation (Halstead volume). Estimated implementation difficulty. Estimated mental effort required to develop or comprehend the code. tasks. Although it shares the 512-token ceiling, its finer positional modeling helps capture subtle stylistic cues in shorter code snippets. Qwen2-1.5B [6]. 1.5 billion parameter decoder-only LLM with 32 k-token context window, pre-trained on diverse corpus that includes code. Its unidirectional attention is less holistic than encoder models, but the massive context lets it ingest whole projects in one pass. We convert it into an 8way classifier via LoRA, adding low-rank adapters Œîùëä = ùê¥ùêµ (ùê¥ Rùëë ùëü , ùêµ Rùëü ùëë, ùëü ùëë), so only 1 % of parameters are updated while the base weights stay frozen."
        },
        {
            "title": "4.1 Binary classification\nFor the first experiment, our objective is to train a classifier on\ncode acquired only from two LLMs, and subsequently achieve high\naccuracy for the attribution of C code unseen by the classifier model.\nTo tackle the most challenging scenario, we used models belonging\nto the same architectural family, namely: GPT-4.1 and GPT-4o. Given\nthat these models are closely related, a natural question arises: Is it\neven possible to distinguish their generated code?",
            "content": "Recent iterations of OpenAIs state-of-the-art non-reasoning modelsGPT-4o (April 2024) and the refreshed GPT-4.1 checkpoint (April 2025) were each pre-trained on multi-trillion-token corpus that blends both text and code tokens, the latter being drawn from public GitHub repositories, Stack Overflow dumps, package registries etc. While both models presumably share comparable pre-training corpora and compute budgets, we hypothesise that their Supervised Fine-Tuning (SFT) and Reinforcement Learning from Human Feedback (RLHF) stages diverged: GPT-4o may have been aligned chiefly on mid-2023 data, whereas GPT-4.1 could incorporate late-2024 GitHub snapshot and richer pool of coding examples. Such calibration differences might manifest in subtle stylistic signals, for example: comment density, brace positioning, whitespace regularity, and typical function granularity. Empirically, we find these cues as evidenced by Listing 1 where even simple Hello World style program yielded different commenting styles, confirming that fine-tuning style drift remains observable. After extensive feature engineering experiments, we observed that none of the structural metrics in Table 4 (e.g., AST or CFG counts) shifted the decision boundary of the classical models by more than 0.4 pp. on the validation set. In contrast, the presence Figure 4: Binary classification: 50% + ùúñ? GPT-4o output #include <stdio.h> int main() { // Print the message to the console printf(\"ACM AISec is an amazing conference!n\"); return 0; // indicate successful execution } GPT-4.1 output #include <stdio.h> int main() { printf(\"ACM AISec is an amazing conference!n\"); return 0; } Listing 1: Simple programs showing the slightly more verbose commenting style in GPT-4o compared to GPT-4.1. or absence of comment tokens moved most curves by 2-3 pp., confirming that commenting style is strong cue for LLM authorship. Table 5 holds the results and yields four additional insights: (i) CodeT5-Authorship wins the race: Our custom modified CodeT5 model at 97.56% beats all off the shelf architectures. Most surprisingly, it does so, while only seeing truncated has 512 token limitcode snippets, and it requires half as much training time as the second best modelDeBERTa-V3 with 97%which has 2048 token context window. (ii) LLMs dominate classical ML: Every LLM that keeps comments surpasses 93 % accuracy, whereas the best classical ML model (XGBoost with comments, 92.2 %) still trails CodeT5-Authorship by over 5 pp. (iii) Comment removal hurts all models, but modestly: The median drop is 2.5 pp. for classical learners (e.g., XGBoost: 92.2 89.7) and 3.1 pp. for LLMs (BERTùêµ: 94.75 91.62), showing that stylistic signals in the source code itself remain exploitable. (iv) In the binary task, long context yields diminishing returns: Expanding the window from 512 to 2 048 tokens boosts DeBERTa-V3 by only +0.69 pp. (96.31 97.00 %), and Longformer also trails CodeT5-Authorship by 1.37 pp. These results could imply that (i) the critical stylistic cues reside mostly in the first 512 tokens or (ii) architectural differences are valuable than sheer context length. (v) Code-specific pre-training helps, but is not sufficient: CodeBERT with comments (95.31%) marginally outperforms the general-purpose RoBERTaùêµ (94.81%). However, it falls short of models like DeBERTa-V3-512, ModernBERTùêµ, and even RoBERTaùêø which has the same context window, but has additinal layers, suggesting that while domain-specific pre-training is good, enhanced architecture is more important. (vi) Small domain-tuned models can surpass larger, generic ones: Both the 2048 token DeBERTa-V3 (97.0 %), and CodeT5Authorship outperforms the much larger QWEN2-1.5B, highlighting that parameter count alone is not guarantee of top accuracy when architecture-level improvements are strong. Table 5: Binary Classification: LLMs vs. Classical ML Models Bisztray et al. Model Name Type Acc (%) Prec (%) Time Co. Key Parameters CodeT5-Authorship LLM DeBERTa-V3 QWEN2-1.5B DeBERTa-V3 DeBERTa-V Longformer ModernBERTùêµ RoBERTaùêø codeBERT RoBERTaùêµ BERTùêµ DistilBERTùêµ codeBERT"
        },
        {
            "title": "XGBoost",
            "content": "RoBERTaùêµ"
        },
        {
            "title": "Random Forest",
            "content": "BERTùêµ DistilBERTùêµ"
        },
        {
            "title": "XGBoost",
            "content": "SVM (Kernel)"
        },
        {
            "title": "Random Forest",
            "content": "Bagging (DT) Bagging (DT) SVM (Linear) SVM (Kernel)"
        },
        {
            "title": "KNN",
            "content": "SVM (Linear)"
        },
        {
            "title": "Decision Tree",
            "content": "LLM LLM LLM LLM LLM LLM LLM LLM LLM"
        },
        {
            "title": "LLM",
            "content": "ML"
        },
        {
            "title": "LLM",
            "content": "ML"
        },
        {
            "title": "LLM",
            "content": "ML ML ML ML ML ML ML ML ML ML ML ML 97.56 97.00 96.88 96.75 96.31 96. 95.94 95.68 95.31 94.81 94.75 93. 93.68 92.2 92.81 90.4 91.62 91. 89.7 88.9 88.2 84.9 84.7 86. 84.2 83.5 80.6 80.3 77.1 74. 97.59 97.00 96.87 96.81 96.32 96. 95.95 95.76 95.43 94.87 94.81 93. 93.75 92.2 92.84 90.4 91.69 91. 89.7 88.9 88.3 84.9 84.8 86. 84.3 83.5 80.6 80.4 77.1 74. 74:14 151:46 179:54 45:21 27:26 117: 36:04 87:35 30:21 30:21 31:05 19: 30:21 5.21 30:33 12.33 30:24 18: 5.98 4.32 11.49 6.41 5.93 0. 4.98 0.00 0.10 0.00 0.39 0. Layers: 24, Token: 512 Layers: 12, Token: 2048 Layers: 32, Token: 2048 Layers: 12, Token: 1024 Layers: 12, Token: 512 Layers: 12, Token: Layers: 12, Token: 512 Layers: 24, Token: 512 Layers: 12, Token: 512 Layers: 12, Token: 512 Layers: 12, Token: 512 Layers: 6, Token: Layers: 12, Token: 512 Estimators: 400 Layers: 12, Token: 512 Estimators: 400 Layers: 12, Token: 512 Layers: 6, Token: Estimators: 400 Kernel: RBF Estimators: 400 Estimators: 10 Estimators: 10 Max_iter= Kernel: RBF Neighbors: 5 Kernel: Linear Neighbors: 5 Max Depth: 8 Max Depth: Legend: ùêµ : base model, Comm: Comment"
        },
        {
            "title": "4.2 Cross-model multi-class attribution\nTable 6 shows accuracy and precision for multi-class attribution on\nC code generated by Gemini2.5 Flash, Claude3.5 Haiku, GPT-4.1,\nLlama 3.3, and DeepSeek-V3. For this part of the experiment, we\ndecided to keep the comments, as we have already seen that it\nimproves attribution efficacy. To gasp how significant it is in this\nsetting, we tested the base BERT model with and without comments.\nWe have gained the following insights:",
            "content": "I Know Which LLM Wrote Your Code Last Summer: LLM generated Code Stylometry for Authorship Attribution Table 6: LLM vs Classical ML: Multi-class Classification Model Name Type Acc (%) Prec (%) Time Co. Key Parameters CodeT5-Authorship LLM Longformer DeBERTa-V3 DeBERTa-V3 DeBERTa-V3 codeBERT RoBERTaùêµ DistilBERTùêµ BERTùêµ QWEN2-1.5B"
        },
        {
            "title": "Random Forest",
            "content": "BERTùêµ SVM (Kernel) Bagging (DT) SVM (Linear)"
        },
        {
            "title": "Decision Tree",
            "content": "LLM LLM LLM LLM LLM LLM LLM LLM"
        },
        {
            "title": "LLM",
            "content": "ML ML"
        },
        {
            "title": "LLM",
            "content": "ML ML ML ML ML 95. 95.00 94.25 94.15 94.02 93.52 93. 93.02 92.65 91.87 90.80 88.00 85. 81.40 78.40 74.60 71.40 58.90 95. 95.01 94.32 94.28 94.13 93.64 93. 93.06 92.71 91.86 90.80 88.00 85. 81.40 78.70 73.90 72.70 61.20 185: 604:42 107:02 733:32 244:32 80:40 80: 54:58 85:05 454:43 00:57 00:38 80: 00:40 00:19 00:01 00:00 00:01 Layers: 24, Token: Layers: 12, Token: 2048 Layers: 12, Token: 512 Layers: 12, Token: 2048 Layers: 12, Token: 1024 Layers: 12, Token: 512 Layers: 12, Token: Layers: 6, Token: 512 Layers: 12, Token: 512 Layers: 32, Token: 2048 Estimators: 400, Depth:9 Estimators: 400 Layers: 12, Token: Kernel: RBF Estimators: 10 Max_iter=2000 Neighbors: 5 Max Depth: 8 Legend: ùêµ: base model, Co: Comment present in code (i) Long context still matters, but architecture matters more: Our CodeT5-Authorship model once again leads the results, followed closely by Longformer (12 layers, 2,048token window), which achieves 95.0% accuracy. (ii) Surprising parameter efficiency of DeBERTa-V3: All three DeBERTa variants cluster tightly around 94.094.3%, and the shortest window (512 tokens) actually edges out the 1 024 and 2 048-token versions, suggesting that once the model has gathered the cues it needs accuracy plateaus even if the window keeps growing. We note that while DeBERTa-v3 can process token sequences longer than 512, it is heavily optimized for 512-token window. (iii) Sheer LLM parameter size is not what matters: Several models, even including the simple base BERT model beats QWEN2-1.5B (1.5 parameters), showing that capturing high-level style cues, and fine-tuning efficiency matter more than long-range generation ability. (iv) Comments remain the dominant cue: Removing comments from BERTùêµ slashes accuracy from 92.65% to 85.45% (a 7.2 pp. drop), nearly three times the average loss observed when ablating comments in the binary experiment. Hence, comment phrasing remains the single richest stylometric feature in the multi-class setting as well. (v) Classical ML remains competitive for the price: tuned XGBoost reaches 90.8% accuracy in 57 s, coming within 4.6 pp. of the best LLM (CodeT5-Authorship) while requiring 600 less training time and no GPU acceleration. (vi) Relative gap between ML and LLMs persists as authors grow: The best ML model sits 4-5 pp. below Longformer in the five LLM author classification task, echoing the 5 pp. gap observed in the binary setting; LLMs therefore maintain stable advantage as the number of candidate authors grows. As Figure 5 shows, DeepSeek-V3 is far more often confused with other models than any of its peers. With CodeT5-Authorship, 723 of the 800 DeepSeek cases are classified correctly; most of the rest are mis-labelled as Llama-3.3-70B, Claude-3.5-haiku, or GPT-4.1. Similarly, true GPT-4.1 and Llama-3.3-70B outputs are frequently tagged as DeepSeek. Figure 6 shows that for the five way classification, instead of using 4000 programs per mode, approximately 2000 would have been sufficient to archive similar levels of accuracy. In summary, across different model families, author attribution is highly feasible. These findings show that stylistic fingerprints persist despite vendor-specific pre-training pipelines, and that they remain detectable by off-the-shelf language encoders. Figure 5: Confusion Matrix for CodeT5-Authorship Figure 6: Classifier F1 score vs. sample size"
        },
        {
            "title": "5 Limitations and Future Research\nWe have identified the following limitations, along with open re-\nsearch questions, that would be an interesting avenue for future\nresearch:",
            "content": "We focused on moderate-sized Transformers and did not train models larger than 7B parameters; scaling to that regime would entail distributed training across many highmemory GPUs, specialized optimization frameworks, and compute budgets that exceed the scope of this study. In contrast to previous human-attribution studies, which often involve many authors, promising direction for future work would be to include additional LLMs in the multiclass attribution experiment to examine how performance changes as the number of models increases; The current training corpus is limited to source code. It is therefore unclear whether the same stylistic signals transfer to other languages such as C++, Rust, Python or Java, or whether cross-language attribution (e.g., training on and testing on C++) is feasible. Building multilingual author benchmarks and evaluating both intra and cross-language performance would fill this gap; We did not test robustness against deliberate style obfuscation. Stress-testing the classifiers under such noise and adversarial paraphrasing is crucial for assessing real-world deployability; Our experiment only focuses on closed-group attribution, but an evaluation in which code generated by unseen LLMs (from the classifiers perceptive) are treated as outof-distribution authors would be an interesting next step. Future work should also examine attribution accuracy on reasoning-centric LLMs and study whether they produce code that can strengthen or dilute the authorship signal; We did not evaluate large decoder-only LLMs as zero or few-shot classifiers as this approach requires prompt design rather than fine-tuning; therefore we leave this topic as an open research question for future work. Finally, the most effective long-context models require lot of GPU-time, which may be prohibitive once the author set becomes massive. Future research should consider distillation, sparse adapters or other efficiency techniques to reduce the computational and energy cost of large-scale attribution. 2https://openrouter.ai Bisztray et al."
        },
        {
            "title": "6 Conclusion\nIn this paper, we released LLM-AuthorBench, a corpus of, 32 000\ncompilable C programs produced by eight large recent language\nmodels over a broad selection of programming tasks. On this bench-\nmark, we compared (i) seven classical ML classifiers that exploit\nlexical, syntactic, and structural features and (ii) eight fine-tuned\nTransformers: BERT, RoBERTa, CodeBERT, ModernBERT, Distil-\nBERT, DeBERTa-V3, Longformer and LoRA-fine-tuned Qwen2-1.5B.\nDeepSeek-V3 was misclassified more often than any other model.\nCodeT5-Authorship correctly identifies only 723 of 800 DeepSeek\nsamples; the remainder are labelled as Llama-3.3-70B, Claude-3.5-\nhaiku or GPT-4.1. The reverse is also true: GPT-4.1 and Llama-3.3-\n70B outputs are frequently tagged as DeepSeek.",
            "content": "RQ1: Can we perform authorship attribution on LLM-generated code for arbitrary programming tasks? Answer: Yes, both in binary classification and in five model multi-class scenario. Our custom model, CodeT5-Authorship encoder (24 layers, 512-token window) achieves 97.6 % accuracy on binary classification, surpassing all baseline models. In the harder multi-class scenario it again edges out (96 %), the best longcontext baseline (Longformer, 95 %). Removing comments lowers accuracy by only 2-3 pp., confirming that stylistic cues are important for reliable attribution. RQ2: Which machine-learning and Transformer models perform best for black-box C-source attribution? Answer: Transformer encoders dominate. Both in the binary experiment and the multi-class setting, Transformer encoders outperform traditional ML. CodeT5-Authorship (24-layer encoder, 512 tokens) attains the top accuracy of 97.6%, outpacing the best generic encoder DeBERTa-V3 by 0.6 pp. while using half the GPU time. The strongest classical model, XGBoost, lags by more than 5 pp. Decoder-only LLMs (e.g., Qwen2-1.5B) and longer context windows offer only marginal gains, indicating that architecture is more important than sheer parameter count or context length. In summary, our results demonstrate that reliable authorship attribution of LLM-generated code is feasible with moderately sized Transformer encoders, and traditional machine learning classifier alike. Looking ahead, there are several interesting research questions that can be investigated as highlighted in Section 5 such as extending attribution to larger author sets, additional programming languages, zero/few-shot scenarios with foundation models, and adversarial obfuscation robustness will be critical for understanding the ultimate limits and practical utility of automated source-code authorship analysis. Know Which LLM Wrote Your Code Last Summer: LLM generated Code Stylometry for Authorship Attribution References [1] Mohammed Abuhamad, Tamer AbuHmed, Aziz Mohaisen, and DaeHun Nyang. 2018. Large-Scale and Language-Oblivious Code Authorship Identification. In Proceedings of the 2018 ACM SIGSAC Conference on Computer and Communications Security (CCS 18). Association for Computing Machinery, New York, NY, USA, 101114. doi:10.1145/3243734.3243738 [2] Mohammed Abuhamad, Tamer Abuhmed, David Mohaisen, and Daehun Nyang. 2021. Large-scale and Robust Code Authorship Identification with Deep Feature Learning. ACM Trans. Priv. Secur. 24, 4 (July 2021), 23:123:35. doi:10.1145/ 3461666 [3] Mohammed Abuhamad, Ji-su Rhim, Tamer AbuHmed, Sana Ullah, Sanggil Kang, and DaeHun Nyang. 2019. Code authorship identification using convolutional neural networks. Future Generation Computer Systems 95 (June 2019), 104115. doi:10.1016/j.future.2018.12.038 [4] Uri Alon, Meital Zilberstein, Omer Levy, and Eran Yahav. 2018. code2vec: Learning Distributed Representations of Code. doi:10.48550/arXiv.1803.09473 arXiv:1803.09473 [cs]. [6] [5] Bander Alsulami, Edwin Dauber, Richard Harang, Spiros Mancoridis, and Rachel Greenstadt. 2017. Source Code Authorship Attribution Using Long Short-Term Memory Based Networks. In Computer Security ESORICS 2017, Simon N. Foley, Dieter Gollmann, and Einar Snekkenes (Eds.). Springer International Publishing, Cham, 6582. doi:10.1007/978-3-319-66402-6_6 Jinze Bai, Shuai Bai, Yunfei Chu, Zeyu Cui, Kai Dang, Xiaodong Deng, Yang Fan, Wenbin Ge, Yu Han, Fei Huang, Binyuan Hui, Luo Ji, Mei Li, Junyang Lin, Runji Lin, Dayiheng Liu, Gao Liu, Chengqiang Lu, Keming Lu, Jianxin Ma, Rui Men, Xingzhang Ren, Xuancheng Ren, Chuanqi Tan, Sinan Tan, Jianhong Tu, Peng Wang, Shijie Wang, Wei Wang, Shengguang Wu, Benfeng Xu, Jin Xu, An Yang, Hao Yang, Jian Yang, Shusheng Yang, Yang Yao, Bowen Yu, Hongyi Yuan, Zheng Yuan, Jianwei Zhang, Xingxuan Zhang, Yichang Zhang, Zhenru Zhang, Chang Zhou, Jingren Zhou, Xiaohuan Zhou, and Tianhang Zhu. 2023. Qwen Technical Report. doi:10.48550/arXiv.2309.16609 arXiv:2309.16609 [cs]. [7] Dimitris Bamidis, Ilias Kalouptsoglou, Apostolos Ampatzoglou, and Alexandros Chatzigeorgiou. 2024. Software Skills Identification: Multi-Class Classification on Source Code Using Machine Learning. Global Clinical Engineering Journal 6, SI6 (Dec. 2024), 7477. doi:10.31354/globalce.v6iSI6.278 [8] Parinaz Bayrami and Jacqueline E. Rice. 2021. Code Authorship Attribution using content-based and non-content-based features. In 2021 IEEE Canadian Conference on Electrical and Computer Engineering (CCECE). 16. doi:10.1109/ CCECE53047.2021.9569061 ISSN: 2576-7046. Iz Beltagy, Matthew E. Peters, and Arman Cohan. 2020. Longformer: The LongDocument Transformer. doi:10.48550/arXiv.2004.05150 arXiv:2004.05150 [cs]. [9] [10] Benedikt Boenninghoff, Steffen Hessler, Dorothea Kolossa, and Robert M. Nickel. 2019. Explainable Authorship Verification in Social Media via Attention-based Similarity Learning. IEEE Computer Society, 3645. doi:10.1109/BigData47090. 2019. [11] Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini Agarwal, Ariel Herbert-Voss, Gretchen Krueger, Tom Henighan, Rewon Child, Aditya Ramesh, Daniel Ziegler, Jeffrey Wu, Clemens Winter, Chris Hesse, Mark Chen, Eric Sigler, Mateusz Litwin, Scott Gray, Benjamin Chess, Jack Clark, Christopher Berner, Sam McCandlish, Alec Radford, Ilya Sutskever, and Dario Amodei. 2020. Language Models are FewShot Learners. In Advances in Neural Information Processing Systems, Vol. 33. Curran Associates, Inc., 18771901. https://papers.nips.cc/paper/2020/hash/ 1457c0d6bfcb4967418bfb8ac142f64a-Abstract.html [12] Aylin Caliskan, Fabian Yamaguchi, Edwin Dauber, Richard Harang, Konrad Rieck, Rachel Greenstadt, and Arvind Narayanan. 2018. When Coding Style Survives Compilation: De-anonymizing Programmers from Executable Binaries. In Proceedings 2018 Network and Distributed System Security Symposium. Internet Society, San Diego, CA. doi:10.14722/ndss.2018.23304 [13] Aylin Caliskan-Islam, Richard Harang, Andrew Liu, Arvind Narayanan, Clare Voss, Fabian Yamaguchi, and Rachel Greenstadt. 2015. De-anonymizing Programmers via Code Stylometry. 255270. https://www.usenix.org/conference/ usenixsecurity15/technical-sessions/presentation/caliskan-islam [14] Soohyeon Choi, Yong Kiam Tan, Mark Huasong Meng, Mohamed Ragab, Soumik Mondal, David Mohaisen, and Khin Mi Mi Aung. 2025. Can Find You in Seconds! Leveraging Large Language Models for Code Authorship Attribution. doi:10.48550/arXiv.2501.08165 arXiv:2501.08165 [cs] version: 1. James R. Cordy and Chanchal Kumar Roy. 2011. The NiCad Clone Detector. 2011 IEEE 19th International Conference on Program Comprehension (2011), 219220. https://api.semanticscholar.org/CorpusID:2991109 [15] [16] Tugce Coskun, Rusen Halepmollasi, Khadija Hanifi, Ramin Fadaei Fouladi, Pinar Comak De Cnudde, and Ayse Tosun. 2022. Profiling developers to predict vulnerable code changes. In Proceedings of the 18th International Conference on Predictive Models and Data Analytics in Software Engineering (PROMISE 2022). Association for Computing Machinery, New York, NY, USA, 3241. doi:10.1145/3558489. [17] Arghavan Moradi Dakhel, Michel C. Desmarais, and Foutse Khomh. 2023. Dev2vec: Representing Domain Expertise of Developers in an Embedding Space. Information and Software Technology 159 (July 2023), 107218. doi:10.1016/j.infsof. 2023.107218 arXiv:2207.05132 [cs]. [18] Sumanth Dathathri, Abigail See, Sumedh Ghaisas, Po-Sen Huang, Rob McAdam, Johannes Welbl, Vandana Bachani, Alex Kaskasoli, Robert Stanforth, Tatiana Matejovicova, Jamie Hayes, Nidhi Vyas, Majd Al Merey, Jonah Brown-Cohen, Rudy Bunel, Borja Balle, Taylan Cemgil, Zahra Ahmed, Kitty Stacpoole, Ilia Shumailov, Ciprian Baetu, Sven Gowal, Demis Hassabis, and Pushmeet Kohli. 2024. Scalable watermarking for identifying large language model outputs. Nature 634, 8035 (Oct. 2024), 818823. doi:10.1038/s41586-024-08025-4 Publisher: Nature Publishing Group. Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. 2019. BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding. In Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers), Jill Burstein, Christy Doran, and Thamar Solorio (Eds.). Association for Computational Linguistics, Minneapolis, Minnesota, 41714186. doi:10.18653/ v1/N19-1423 [19] [20] Ahmet Emir Dirik. 2013. Source Attribution Based on Physical Defects in Light Path. In Digital Image Forensics: There is More to Picture than Meets the Eye, Husrev Taha Sencar and Nasir Memon (Eds.). Springer, New York, NY, 219236. doi:10.1007/978-1-4614-0757-7_7 [21] Zhangyin Feng, Daya Guo, Duyu Tang, Nan Duan, Xiaocheng Feng, Ming Gong, Linjun Shou, Bing Qin, Ting Liu, Daxin Jiang, and Ming Zhou. 2020. CodeBERT: Pre-Trained Model for Programming and Natural Languages. doi:10.48550/ arXiv.2002.08155 arXiv:2002.08155 [cs]. [22] Alberto Ferrante, Eric Medvet, Francesco Mercaldo, Jelena Milosevic, and Corrado Aaron Visaggio. 2016. Spotting the Malicious Moment: Characterizing Malware Behavior Using Dynamic Features. In 2016 11th International Conference on Availability, Reliability and Security (ARES). 372381. doi:10.1109/ARES.2016.70 [23] Georgia Frantzeskou, Efstathios Stamatatos, Stefanos Gritzalis, and Sokratis Katsikas. 2006. Effective identification of source code authors using byte-level information. In Proceedings of the 28th international conference on Software engineering (ICSE 06). Association for Computing Machinery, New York, NY, USA, 893896. doi:10.1145/1134285. [25] [24] Georgia Frantzeskou, Efstathios Stamatatos, Stefanos Gritzalis, and Sokratis Katsikas. 2006. Source Code Author Identification Based on N-gram Author Profiles. In Artificial Intelligence Applications and Innovations, Ilias Maglogiannis, Kostas Karpouzis, and Max Bramer (Eds.). Springer US, Boston, MA, 508515. doi:10.1007/0-387-34224-9_59 Jason Gray, Daniele Sgandurra, Lorenzo Cavallaro, and Jorge Blasco Alis. 2024. Identifying Authorship in Malicious Binaries: Features, Challenges & Datasets. ACM Comput. Surv. 56, 8, Article 212 (April 2024), 36 pages. doi:10.1145/3653973 [26] Daya Guo, Shuo Ren, Shuai Lu, Zhangyin Feng, Duyu Tang, Shujie Liu, Long Zhou, Nan Duan, Alexey Svyatkovskiy, Shengyu Fu, Michele Tufano, Shao Kun Deng, Colin Clement, Dawn Drain, Neel Sundaresan, Jian Yin, Daxin Jiang, and Ming Zhou. 2021. GraphCodeBERT: Pre-training Code Representations with Data Flow. doi:10.48550/arXiv.2009.08366 arXiv:2009.08366 [cs]. [27] Dixiao Guo, Anmin Zhou, Liang Liu, Shan Liao, and Lei Zhang. 2022. Method of Source Code Authorship Attribution Based on Graph Neural Network. In Proceedings of 2021 Chinese Intelligent Automation Conference, Zhidong Deng (Ed.). Springer, Singapore, 645657. doi:10.1007/978-981-16-6372-7_70 [28] Hanxi Guo, Siyuan Cheng, Kaiyuan Zhang, Guangyu Shen, and Xiangyu Zhang. 2025. CodeMirage: Multi-Lingual Benchmark for Detecting AI-Generated and Paraphrased Source Code from Production-Level LLMs. doi:10.48550/arXiv.2506. 11059 arXiv:2506.11059 [cs]. [29] Surbhi Gupta, Neeraj Mohan, and Munish Kumar. 2021. Study on Source Device Attribution Using Still Images. Archives of Computational Methods in Engineering 28, 4 (June 2021), 22092223. doi:10.1007/s11831-020-09452-y [30] Andrea Gurioli, Maurizio Gabbrielli, and Stefano Zacchiroli. 2025. Is This You, LLM? Recognizing AI-written Programs with Multilingual Code Stylometry . In 2025 IEEE International Conference on Software Analysis, Evolution and Reengineering (SANER). IEEE Computer Society, Los Alamitos, CA, USA, 394405. doi:10.1109/SANER64311.2025.00044 [31] Pengcheng He, Jianfeng Gao, and Weizhu Chen. 2023. DeBERTaV3: Improving DeBERTa using ELECTRA-Style Pre-Training with Gradient-Disentangled Embedding Sharing. doi:10.48550/arXiv.2111.09543 arXiv:2111.09543 [cs]. [32] Xie He, Arash Habibi Lashkari, Nikhill Vombatkere, and Dilli Prasad Sharma. 2024. Authorship Attribution Methods, Challenges, and Future Research Directions: Comprehensive Survey. Information 15, 3 (March 2024), 131. doi:10.3390/ info15030131 Number: 3 Publisher: Multidisciplinary Digital Publishing Institute. Jeremy Howard and Sebastian Ruder. 2018. Universal Language Model Finetuning for Text Classification. In Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), Iryna Gurevych and Yusuke Miyao (Eds.). Association for Computational Linguistics, Melbourne, Australia, 328339. doi:10.18653/v1/P18-1031 [33] Bisztray et al. [54] Qige Song, Yongzheng Zhang, Linshu Ouyang, and Yige Chen. 2022. BinMLM: Binary Authorship Verification with Flow-aware Mixture-of-Shared Language Model. doi:10.48550/arXiv.2203.04472 arXiv:2203.04472 [cs]. [55] Hyunjae Suh, Mahan Tafreshipour, Jiawei Li, Adithya Bhattiprolu, and Iftekhar Ahmed. 2024. An Empirical Study on Automatically Detecting AI-Generated Source Code: How Far Are We? doi:10.48550/arXiv.2411.04299 arXiv:2411.04299 [cs] version: 1. [56] Tarun Suresh, Shubham Ugare, Gagandeep Singh, and Sasa Misailovic. 2025. Is The Watermarking Of LLM-Generated Code Robust? doi:10.48550/arXiv.2403. 17983 arXiv:2403.17983 [cs]. [57] N. N. Thathsarani. 2024. Comprehensive Software Complexity Metric Based on Cyclomatic Complexity. In 2024 4th International Conference of Science and Information Technology in Smart Administration (ICSINTESA). 9095. doi:10.1109/ ICSINTESA62455.2024.10748227 [58] Norbert Tihanyi, Tamas Bisztray, Mohamed Amine Ferrag, Ridhi Jain, and Lucas C. Cordeiro. 2024. How secure is AI-generated code: large-scale comparison of large language models. Empirical Software Engineering 30, 2 (Dec. 2024), 47. doi:10.1007/s10664-024-10590-1 [59] Farhan Ullah, Muhammad Rashid Naeem, Hamad Naeem, Xiaochun Cheng, and Mamoun Alazab. 2022. CroLSSim: Cross-language software similarity detector using hybrid approach of LSA-based AST-MDrep features and CNN-LSTM model. International Journal of Intelligent Systems 37, 9 (2022), 57685795. doi:10.1002/ int.22813 _eprint: https://onlinelibrary.wiley.com/doi/pdf/10.1002/int.22813. [60] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan Gomez, ≈Å ukasz Kaiser, and Illia Polosukhin. 2017. Attention is All you Need. In Advances in Neural Information Processing Systems, Vol. 30. Curran Associates, Inc. https://papers.nips.cc/paper_files/paper/2017/hash/ 3f5ee243547dee91fbd053c1c4a845aa-Abstract.html [61] Ningfei Wang, Shouling Ji, and Ting Wang. 2018. Integration of Static and Dynamic Code Stylometry Analysis for Programmer De-anonymization. In Proceedings of the 11th ACM Workshop on Artificial Intelligence and Security (AISec 18). Association for Computing Machinery, New York, NY, USA, 7484. doi:10.1145/3270101. [62] Yue Wang, Weishi Wang, Shafiq Joty, and Steven C. H. Hoi. 2021. CodeT5: Identifier-aware Unified Pre-trained Encoder-Decoder Models for Code Understanding and Generation. doi:10.48550/arXiv.2109.00859 arXiv:2109.00859 [cs]. [63] Benjamin Warner, Antoine Chaffin, Benjamin Clavi√©, Orion Weller, Oskar Hallstr√∂m, Said Taghadouini, Alexis Gallagher, Raja Biswas, Faisal Ladhak, Tom Aarsen, Nathan Cooper, Griffin Adams, Jeremy Howard, and Iacopo Poli. 2024. Smarter, Better, Faster, Longer: Modern Bidirectional Encoder for Fast, Memory Efficient, and Long Context Finetuning and Inference. arXiv:2412.13663 [cs.CL] https://arxiv.org/abs/2412.13663 Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Brian Ichter, Fei Xia, Ed Chi, Quoc Le, and Denny Zhou. 2022. Chain-of-Thought Prompting Elicits Reasoning in Large Language Models. https://arxiv.org/abs/2201.11903v6 [65] Zhenyu Xu and Victor S. Sheng. 2025. CodeVision: Detecting LLM-Generated Code Using 2D Token Probability Maps and Vision Models. doi:10.48550/arXiv. 2501.03288 arXiv:2501.03288 [cs] version: 1. [64] [66] Sarim Zafar, Muhammad Usman Sarwar, Saeed Salem, and Muhammad Zubair Malik. 2020. Language and Obfuscation Oblivious Source Code Authorship Attribution. IEEE Access 8 (2020), 197581197596. doi:10.1109/ACCESS.2020. 3034932 [67] David √Ålvarez Fidalgo and Francisco Ortin. 2025. CLAVE: deep learning model for source code authorship verification with contrastive learning and transformer encoders. Inf. Process. Manage. 62, 3 (April 2025). doi:10.1016/j.ipm.2024.104005 Received *; revised *; accepted * [34] Vaibhavi Kalgutkar, Ratinder Kaur, Hugo Gonzalez, Natalia Stakhanova, and Alina Matyukhina. 2019. Code Authorship Attribution: Methods and Challenges. ACM Comput. Surv. 52, 1 (Feb. 2019), 3:13:36. doi:10.1145/ [35] Rapha√´l Khoury, Anderson R. Avila, Jacob Brunelle, and Baba Mamadou Camara. 2023. How Secure is Code Generated by ChatGPT?. In 2023 IEEE International Conference on Systems, Man, and Cybernetics (SMC). 24452451. doi:10.1109/ SMC53992.2023.10394237 ISSN: 2577-1655. Jungin Kim, Shinwoo Park, and Yo-Sub Han. 2025. Marking Code Without Breaking It: Code Watermarking for Detecting LLM-Generated Code. doi:10. 48550/arXiv.2502.18851 arXiv:2502.18851 [cs]. [36] [37] Tharindu Kumarage, Garima Agrawal, Paras Sheth, Raha Moraffah, Aman Chadha, Joshua Garland, and Huan Liu. 2024. Survey of AI-generated Text Forensic Systems: Detection, Attribution, and Characterization. doi:10.48550/ arXiv.2403.01152 arXiv:2403.01152 [cs] version: 1. [38] Boquan Li, Mengdi Zhang, Peixin Zhang, Jun Sun, and Xingmei Wang. 2024. Resilient Watermarking for LLM-Generated Codes. doi:10.48550/arXiv.2402. 07518 arXiv:2402.07518 [cs] version: 1. [40] [39] Paula Liss√≥n and Nicolas Ballier. 2018. Investigating Lexical Progression through Lexical Diversity Metrics in Corpus of French L3. Discours. Revue de linguistique, psycholinguistique et informatique. journal of linguistics, psycholinguistics and computational linguistics 23 (Dec. 2018). doi:10.4000/discours.9950 Number: 23 Publisher: Presses universitaires de Caen. Jiawei Liu, Chunqiu Steven Xia, Yuyao Wang, and Lingming Zhang. Is Your Code Generated by ChatGPT Really Correct? Rigor2023. ous Evaluation of Large Language Models for Code Generation. AdInformation Processing Systems 36 (Dec. 2023), 21558 vances in Neural 21572. https://proceedings.neurips.cc/paper_files/paper/2023/hash/ 43e9d647ccd3e4b7b5baab53f0368686-Abstract-Conference.html [41] Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi, Danqi Chen, Omer Levy, Mike Lewis, Luke Zettlemoyer, and Veselin Stoyanov. 2019. RoBERTa: Robustly Optimized BERT Pretraining Approach. doi:10.48550/arXiv.1907.11692 arXiv:1907.11692 [cs]. [42] T.J. McCabe. 1976. Complexity Measure."
        },
        {
            "title": "IEEE Transactions on Software",
            "content": "Engineering SE-2, 4 (1976), 308320. doi:10.1109/TSE.1976.233837 [43] Eric Mitchell, Yoonho Lee, Alexander Khazatsky, Christopher D. Manning, and Chelsea Finn. 2023. DetectGPT: Zero-Shot Machine-Generated Text Detection using Probability Curvature. doi:10.48550/arXiv.2301.11305 arXiv:2301.11305 [cs]. [44] Phuong T. Nguyen, Juri Di Rocco, Claudio Di Sipio, Riccardo Rubei, Davide Di Ruscio, and Massimiliano Di Penta. 2024. GPTSniffer: CodeBERT-based classifier to detect source code written by ChatGPT. Journal of Systems and Software 214 (Aug. 2024), 112059. doi:10.1016/j.jss.2024.112059 [45] Timothy Paek and Chilukuri Mohan. 2025. Detection of LLM-Generated Java Code Using Discretized Nested Bigrams. doi:10.48550/arXiv.2502.15740 arXiv:2502.15740 [cs]. [46] Wei Hung Pan, Ming Jie Chok, Jonathan Leong Shan Wong, Yung Xin Shin, Yeong Shian Poon, Zhou Yang, Chun Yong Chong, David Lo, and Mei Kuan Lim. 2024. Assessing AI Detectors in Identifying AI-Generated Code: Implications for Education. doi:10.48550/arXiv.2401.03676 arXiv:2401.03676 [cs]. [47] Shinwoo Park, Hyundong Jin, Jeong-won Cha, and Yo-Sub Han. 2025. Detection of LLM-Paraphrased Code and Identification of the Responsible LLM Using Coding Style Features. doi:10.48550/arXiv.2502.17749 arXiv:2502.17749 [cs] version: 2. [48] Erwin Quiring, Alwin Maier, and Konrad Rieck. 2019. Misleading Authorship Attribution of Source Code using Adversarial Learning. 479496. https://www. usenix.org/conference/usenixsecurity19/presentation/quiring [49] Nathan Rosenblum, Xiaojin Zhu, and Barton P. Miller. 2011. Who Wrote This Code? Identifying the Authors of Program Binaries. In Computer Security ESORICS 2011, Vijay Atluri and Claudia Diaz (Eds.). Springer, Berlin, Heidelberg, 172189. doi:10.1007/978-3-642-23822-2_10 [50] Victor Sanh, Lysandre Debut, Julien Chaumond, and Thomas Wolf. 2020. DistilBERT, distilled version of BERT: smaller, faster, cheaper and lighter. doi:10.48550/arXiv.1910.01108 arXiv:1910.01108 [cs]. [51] Yanir Seroussi, Ingrid Zukerman, and Fabian Bohnert. 2014. Authorship Attribution with Topic Models. Computational Linguistics 40, 2 (June 2014), 269310. doi:10.1162/COLI_a_00173 Place: Cambridge, MA Publisher: MIT Press. [52] Kanishka Silva, Ingo Frommholz, Burcu Can, Fred Blain, Raheem Sarwar, and Laura Ugolini. 2024. Forged-GAN-BERT: Authorship Attribution for LLMGenerated Forged Novels. In Proceedings of the 18th Conference of the European Chapter of the Association for Computational Linguistics: Student Research Workshop, Neele Falk, Sara Papi, and Mike Zhang (Eds.). Association for Computational Linguistics, St. Julians, Malta, 325337. https://aclanthology.org/2024.eaclsrw.26/ [53] Qige Song, Yongzheng Zhang, Linshu Ouyang, and Yige Chen. 2022. BinMLM: Binary Authorship Verification with Flow-aware Mixture-of-Shared Language Model . In 2022 IEEE International Conference on Software Analysis, Evolution and Reengineering (SANER). IEEE Computer Society, Los Alamitos, CA, USA, 10231033. doi:10.1109/SANER53432.2022."
        }
    ],
    "affiliations": [
        "E√∂tv√∂s L√≥r√°nd University, Budapest, Hungary",
        "Guelma University, Guelma, Algeria",
        "Technology Innovation Institute, Abu Dhabi, United Arab Emirates",
        "University of Oslo, Cyentific AS, Oslo, Norway",
        "University of Oslo, Oslo, Norway"
    ]
}