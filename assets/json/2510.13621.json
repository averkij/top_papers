{
    "paper_title": "The Role of Computing Resources in Publishing Foundation Model Research",
    "authors": [
        "Yuexing Hao",
        "Yue Huang",
        "Haoran Zhang",
        "Chenyang Zhao",
        "Zhenwen Liang",
        "Paul Pu Liang",
        "Yue Zhao",
        "Lichao Sun",
        "Saleh Kalantari",
        "Xiangliang Zhang",
        "Marzyeh Ghassemi"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Cutting-edge research in Artificial Intelligence (AI) requires considerable resources, including Graphics Processing Units (GPUs), data, and human resources. In this paper, we evaluate of the relationship between these resources and the scientific advancement of foundation models (FM). We reviewed 6517 FM papers published between 2022 to 2024, and surveyed 229 first-authors to the impact of computing resources on scientific output. We find that increased computing is correlated with national funding allocations and citations, but our findings don't observe the strong correlations with research environment (academic or industrial), domain, or study methodology. We advise that individuals and institutions focus on creating shared and affordable computing opportunities to lower the entry barrier for under-resourced researchers. These steps can help expand participation in FM research, foster diversity of ideas and contributors, and sustain innovation and progress in AI. The data will be available at: https://mit-calc.csail.mit.edu/"
        },
        {
            "title": "Start",
            "content": "5 2 0 2 5 1 ] . [ 1 1 2 6 3 1 . 0 1 5 2 : r a"
        },
        {
            "title": "THE ROLE OF COMPUTING RESOURCES IN PUBLISHING\nFOUNDATION MODEL RESEARCH",
            "content": "A PREPRINT Yuexing Hao 1, 2, Yue Huang 3, Haoran Zhang 1, Chenyang Zhao 4, Zhenwen Liang 3, Paul Pu Liang 1, Yue Zhao 6, Lichao Sun 5, Saleh Kalantari 2, Xiangliang Zhang 3, Marzyeh Ghassemi 1 1EECS, MIT, Cambridge, 02135, USA. 2Cornell University, Ithaca, 14850, USA. 3CSE, University of Notre Dame, South Bend, 46556, USA. 4Computer Science Department, University of California, Los Angeles, 90095, USA. 5Computer Science Department, Lehigh University, Bethlehem, 18015, USA. 6School of Advanced Computing, University of Southern California, Los Angeles, 90007, USA. Corresponding author. Email: yuexing@mit.edu These authors contributed equally to this work."
        },
        {
            "title": "ABSTRACT",
            "content": "Cutting-edge research in Artificial Intelligence (AI) requires considerable resources, including Graphics Processing Units (GPUs), data, and human resources. In this paper, we evaluate of the relationship between these resources and the scientific advancement of foundation models (FM). We reviewed 6517 FM papers published between 2022 to 2024, and surveyed 229 first-authors to the impact of computing resources on scientific output. We find that increased computing is correlated with national funding allocations and citations, but our findings dont observe the strong correlations with research environment (academic or industrial), domain, or study methodology. We advise that individuals and institutions focus on creating shared and affordable computing opportunities to lower the entry barrier for under-resourced researchers. These steps can help expand participation in FM research, foster diversity of ideas and contributors, and sustain innovation and progress in AI. The data will be available at: https://mit-calc.csail.mit.edu/. Keywords: Computing Resource, Foundation Model Research, GPU Disparity"
        },
        {
            "title": "1 Introduction",
            "content": "Artificial Intelligence (AI) and machine learning (ML) models have made stark advances in the past three years, fueled by the development of foundation models (FM) trained on large-scale multimodal data. Following the public release of several successful FMs (OpenAI (2022); Brown et al. (2020); Bommasani et al. (2022)), FMs such as large language models (LLMs) and vision language models (VLMs) have bridged vision, language, and other modalities. In many Computer Science subfields such as Natural Language Processing (NLP) and Computer Vision (CV), FMs have demonstrated strong compositional performance and generalization capabilities (Awais et al. (2025); Gunter et al. (2024)), emerging as widely-used tools (Bommasani et al. (2022)) that provide flexible backbones for innovation in other fields (Moor et al. (2023); Sartor & Thompson (2025); Firoozi et al. (2024)). Conducting FM research requires significant data, computing, and human resources (Cottier et al. (2024); Maslej et al. (2024); Crawford (2024)). central concern in the field is whether greater access to such resources directly translates into more impactful research outcomes (Acemoglu (2024); Dodge et al. (2019); OpenAI (2018)), such as more research publications, or higher citation counts (Sinclair et al. (2023); Anjum et al. (2019)). The answer to this question has important implications for how resources are allocated, which research directions are prioritized, and how equitable participation in FM research can be ensured. However, the cost of research is often difficult to quantify due to lack of uniform disclosure on resource distribution (Bommasani et al. (2024)). Absent widespread disclosure, funding is perhaps most easily characterized in the concrete cost of purchasing or renting hardware (e.g., computing clusters, or chips), through there are also software, cloud storage services, and specialized software platform costs. GPUs are particularly salient metric for examination as they are tightly controlled resource The Role of Computing Resources in Publishing Foundation Model Research available in fixed quantities (Owens et al. (2008); Nickolls & Dally (2010)), and there is often intense competition for their purchase (Khandelwal et al. (2024); Kudiabor (2024)). In this paper, we examine the relationship between hardware resources and publications in selective AI/ML computer science conferences. We focus on benchmarking two measurements of computing power - the number of graphical processing units (GPUs) and teraflops (TFLOP) - and relate those to data from over 34,828 accepted papers between 2022 and 2024. We identify 5,889 FM papers published and find that greater GPU access is associated with higher acceptance rates in eight conference venues, as well as with higher citation counts. While this relationship is likely influenced institutional resources, reputation, and researcher networks), the trend demonstrates the by other factors (i.e. potential role of computing resources in shaping FM research visibility and impact. We also survey 229 authors of 312 papers, to identify any discrepancies between reported usage and true usage of resources. We find that: (1) the majority of foundation model papers are authored by researchers in academia (4,851 papers) compared to those from industry (1,425 papers); (2) most papers used open-source models (i.e. Llama), followed by closedsource models (i.e. GPT); and (3) GPU information is rarely disclosed in the manuscript, demonstrating the need for standardized reporting of computing resources to improve transparency and reproducibility."
        },
        {
            "title": "2.1 LLM-extracted Publication Data",
            "content": "We collect accepted papers from eight top machine learning conferences between 2022-2024 that were available as of March 2025 (Figure 1 panel (A)). We access papers via the OpenReview API (NeurIPS, ICLR, ICML, COLM, and EMNLP 2023) and the Association for Computational Linguistics (ACL) Rolling Review (ARR) platform (ACL, NAACL, EACL, EMNLP 2022 and 2024). We identify FM-related papers using keywords in the titles or abstracts. total of 5,889 accepted papers were identified as being FM-relevant from 34,828 total papers. We also collect rejected and withdrawn FM-related ICLR papers - total of 613 rejected or withdrawn ICLR papers in the same period for comparison. full description of this process can be found in the appendices. We collect structured information using system APIs from all 5,889 accepted papers, including IDs of the articles, titles, author details (name(s), number of authors and affiliated institutions), publication details (such as year, venue and accept / reject status, paper link, reviews and abstracts (Figure 1). We use the GPT-4o mini (OpenAI et al. (2024)) LLM to collect information missing from the system API by processing article PDFs to extract the senior authors affiliation, GPU usage, data set descriptions, and funding information. In our analyses, we attribute the papers affiliation to the senior author listed. For papers involving multiple institutions or countries, we use only the first institution and country listed for the senior author. If listed institution has multiple locations, e.g., Google Research, we map this to the publicly listed country headquarters. After compiling the titles and abstracts of the papers, we used GPT-4o Mini to classify each paper into three categories: Domain, Phase, and Method. Definitions for these categories are provided in Table 1. We initially collected 34,828 papers from eight major computer science conferences, from which 6,517 FM-related papers were retrieved using the OpenReview API and the ACL ARR platform. Then we extracted their pdfs with GPU related information. In the survey study, we recruited 229 first-author FM researchers, representing 312 papers in total, to participate in our survey. Participants provided self-reported responses regarding computing resources when such information was not documented in their publications. Dotted boxes indicate potentially unavailable information (Figure 1 panel (A)). Percentage of valid GPU type by year and conference, and whether the conference author and reviewer checklists contain related guidelines to report these computing resource usage is presented in Figure 1 panel (B). Noted that ARR conferences (including ACL, EACL, EMNLP, and NAACL) used the same author and reviewer checklists with slightly modifications based on each conferences requirements. Figure 1 panel panel (C) shows the GPU Usage and TFLOPS 16 between GPT-4o scraped and self-reported survey data. To ensure the accuracy of the extracted GPU information, two FM researchers independently checked 312 papers and compared to GPT-4o mini in blinded setting. We cross-referenced the information extracted by GPT-4o mini, the researchers annotations, and the GPU counts self-reported by first-author participants. When authors of the 312 papers were surveyed, 288 papers self-reported GPU numbers, 292 papers self-reported GPU types, and 281 papers self-reported GPU hours. 24 papers used non-GPU resources (e.g. TPU, NPU, CPU). However, FM researchers found that only 172 papers PDFs contained GPU numbers, 141 papers PDFs contains GPU types, and 249 papers PDFs contains GPU hours. GPT-4o mini was able to scrape GPU numbers from just 116 papers, showing 59.7% gap compared to author reports. The missing rates were also high for GPU type (48.3%) and GPU hours (88.6%). 2 The Role of Computing Resources in Publishing Foundation Model Research Figure 1: Study Design and Data Collection. We find that GPU usage is infrequently reported. For instance, only 16.8% (1096 papers) and 24.7% (1608 papers) listed their GPU type and storage information respectively. Even fewer reported information on datasets (see appendices). This is generally serious reporting gap: only 16.51% of papers include GPU quantity information, 24.22% specify GPU types, and just 12.86% report inference times. This inconsistent documentation prevents development of the structured data needed to properly understand and forecast the FM computing landscape. The absence of mandatory compute-related disclosures in author and reviewer checklists likely contributes to this gap (Figure 1 panel (B)). Conferences that enforce such requirements tend to exhibit higher reporting rates for GPU specifications."
        },
        {
            "title": "2.2 Self-Reported Author Survey Data",
            "content": "We conduct an IRB-approved survey recruiting participants through university email lists, and social media platforms to gather secondary source of costs associated with FM research (IRB ID: 2507001737). To incentivize participation, each respondent received $20 Amazon gift card, and single first or co-first author could complete up to five surveys. The eligibility to participate was contingent on: the article focused exclusively on FM research; 3 The Role of Computing Resources in Publishing Foundation Model Research Category Justification and Distinction Domain Natural Language Processing (NLP): Involves text and language data, addressing tasks such as translation, summarization, question answering, and dialogue generation. NLP remains one of the most established FM domains (e.g., GPT, BERT). Computer Vision (CV): Deals with visual data, including image classification, object detection, segmentation, and video understanding. Vision FMs handle diverse modalities and architectures (e.g., ViT, SAM). Data Mining: Applies FMs to structured or semi-structured data for pattern discovery, anomaly detection, and recommendation systems. Security: Focuses on applications in threat detection, adversarial robustness, privacy protection, and secure model deployment, often intersecting with adversarial ML and interpretability. Explainable AI (XAI): Aims to improve model transparency by probing FM reasoning, generating explanations, and designing interpretable architectures. Others: Includes interdisciplinary or emerging areas such as systems optimization (e.g., FM efficiency, scaling laws) and human-computer interaction."
        },
        {
            "title": "Phase",
            "content": "Pre-trained: Refers to large-scale training on general-purpose datasets (e.g., web-scale or multimodal corpora), establishing foundational model capabilities. Post-trained: Involves fine-tuning, domain adaptation, or task-specific refinement using smaller or labeled datasets to narrow the models focus. Inference: Represents the deployment stage, emphasizing efficiency, robustness, safety, and realworld alignment."
        },
        {
            "title": "Method",
            "content": "Algorithm: Introduces new architectures, optimization strategies, or training paradigms (e.g., sparse attention, contrastive learning) that enhance FM performance or capabilities. Dataset/Benchmark: Develops datasets or evaluation suites (e.g., BIG-Bench, HELM) to measure FM performance, standardize comparisons, and expose limitations. Empirical Study: Conducts systematic analyses of existing models to examine scaling laws, emergent behaviors, or social biases, often revealing critical limitations. Toolkit: Builds open-source frameworks or libraries (e.g., Hugging Face Transformers, LangChain) that promote usability, reproducibility, and extensibility in FM research. Table 1: Justification of the three classification dimensions used to categorize foundation model (FM) research papers. the paper was completed and pre-printed (publicly available online); the paper was not survey; and the primary contribution was in the computer science domain. total of 118 institutions (267 academia first authors and 36 industry first authors) contributed to the survey. Responses were collected from 229 researchers, all of whom were first or co-first authors of publicly available papers. Further details regarding the data collection process, both public and survey-based, as well as the analysis methods, are provided in the supplementary materials (SM). Among the 140 papers for which both the PDF and self-reported GPU data were available, 65 first authors (46.4%) self-reported using more GPUs than indicated in their published papers, while 21 first-authors (15.0%) selfreported fewer GPUs. In 54 cases (38.6%), the GPU counts reported in the papers matched the authors self-reports. Feedback from survey respondents suggests additional GPU usage could indicate experiments that were not included in the final paper. Respondents also indicated that they often ran multiple experiments in different GPU cluster configurations, ultimately reporting only the set-up that supported the largest set of experiments. The discrepancies between the accepted scraped and survey papers are illustrated in Figure 1 panel C. 4 The Role of Computing Resources in Publishing Foundation Model Research"
        },
        {
            "title": "3 Results",
            "content": "3.1 Foundation Model Research Is Growing in Count and Scope Foundation model (FM) research has grown across phases, methods, and domains from 2022 to 2024 (Figure 2). We find that FM papers grew from just 2.07% of all accepted papers across eight key conferences in 2022 to 10.29% in 2023, then further increased to 34.64% in 2024 (Figure 2A). FM research in natural language processing (NLP) venues specifically has grown, and conferences such as COLM, EMNLP, and ACL have the highest FM paper ratio, i.e., as compared to general ML conferences such as ICLR, ICML, NeurIPS (Fan et al. (2024)). Within FM work, inference-related papers experienced the most notable increase. Algorithmic and empirical studies outpaced other methods categories such as datasets/benchmarks and toolkits (Figure 2 B). Despite this upward trend, we find that GPU usage data from scraped publications and self-reported survey responses report steady usage (including studies currently awaiting publication) (Figure 2 C). Most research projects consistently use moderate number of GPUs, typically between 1 and 8 units. Among these, the most common category remains 1 to 4 GPUs, accounting for roughly half or more of the surveyed research each year. 1 We note that more work should be done to monitor this trend in the future, especially as the latency for GPU purchases increases. 3.2 Contributions from Both Industry and Academic Institutions Computer science research has historically had strong interplay between academic institutions and industrial labs (Prager & Omenn (1980)). We find that this still exists; academic senior authors contributed more FM papers overall, but two companies (Google and Microsoft) have the highest single-entity paper count, followed by Tsinghua University, Meta and Stanford University (Figure 3, panels (A1) and (A22)). Specifically, 611 academia institutions account for 4,851 papers, whereas 163 industry institution contribute 1,425 papers, alongside 239 papers affiliated with other entities such as hospitals or independent researchers. Importantly, the average number of publications per institution is comparable: industry-affiliated authors average 8.72 papers (SD: 29.05), while those from academia average 7.93 papers (SD: 16.62). This suggests that FM output is largely concentrated in institutions across industry and academia that can support the resource-intensive nature (Mu (2024)). This may also compound if resources to support large-scale model training become constrained (Ahmed et al. (2023); Cottier (2023); Klyman et al.). We note that the United States and China generally lead in FM research output (Figure 3 panel (B)), which could be due in part to investment in higher education and AI (Guest & Wei (2024))."
        },
        {
            "title": "3.3 FM Research is Led by Open-Weight Models",
            "content": "We analyze the most frequently used model types, and find that the open-weight LLaMA models are most frequently used (Figure 3 panel (C)). This distinction is particularly critical in FM research. Proprietary models such as GPT maintain significant presence, likely due to their performance and integration in commercial APIs (Kapoor et al. (2024)). However, open-weight models are broadly accessible, and such open-weights enable fine-tuning, domain adaptation, and benchmarking in ways that proprietary models often do not."
        },
        {
            "title": "3.4 GPUs Usage in FM Research",
            "content": "We investigate the distribution of specific GPU types in FM research, and find that NVIDIA Tesla A100 cores were used most commonly (Figure 3 D). The top 10 GPU types are from the Nvidia family, which supports largescale FM research. We compare the number of GPU and TFLOPs used in FM papers across senior author affiliation, specific application domain, phase of model development, and methodological focus. We find that both GPUs and TFLOPs are evenly distributed across most categories, with only studies focusing on pre-training reporting significantly more GPUs compared to those emphasizing post-training or inference tasks (Mann-Whitney test, < 0.001). Other variations, such as lower median GPU and TFLOP usage in FM research related to security (Figure 3 - Domain) or higher usage for those constructing toolkits (Figure 3 - Method), were not statistically significant. In the set of papers analyzed, 47.4% prioritize algorithmic development, while 31.7% center on empirical evaluation. Only 1.5% of the publications address toolkits or implementation infrastructure, indicating limited scholarly attention to the development of supporting tools and systems. majority of the papers (86.4%) focus on NLP, with 1The reported GPU numbers commonly align with powers of two (e.g., 1, 2, 4, 8, and 16), likely reflecting purchasing norms for GPU clusters, which rarely involve configurations of non-standard quantities like 3, 5, 7, 11, 13, 14, or 15. 5 The Role of Computing Resources in Publishing Foundation Model Research Figure 2: Temporal Evolution of FMs. A) FM papers as proportion of published papers over time. B) Evolution of FM papers over phases, methods, and domains. C) Temporal evolution of GPU Model distribution in LLM-extract and self-reported data. Pub. denotes publications from the scraped dataset, while Await Publish refers to papers that are either under review, rejected, or in preparation for submission. smaller portion (5.7%) concerning CV. Regarding the stages of model development, 48.7% of the papers investigate inference processes, 36.1% address post-trained analysis or adaptation, and 13.3% focus on the pre-trained stage."
        },
        {
            "title": "3.5 Funding Sources for FM Research",
            "content": "We investigate the sources of funding for FM research across funding types, specific agencies, and countries (Figure 4). total of 1126 papers had funding country information, and 998 papers had funding type information. Note that single paper may be funded by multiple countries or organizations. majority of FM research received government funding (848 papers, 85.5%), followed by corporations (291 papers, 29.3%), and foundations (102 papers, 10.3%) (Figure 4 A). We find that GDP per capita is not consistently correlated with the number of funded FM papers across countries (Figure 4 B). While the U.S. and China lead in output despite differing GDP levels, FM research is closely tied to institutional support and policy than to national wealth. 6 The Role of Computing Resources in Publishing Foundation Model Research Figure 3: Distribution of FM Papers. We analyze FM papers across various dimensions: (A) Senior Authors Affiliations in (A1) Academia and (A2) Industry; (B) Countries by Senior Authors Affiliation; (C) Paper Count by LLM Usage; and (D) GPU Types Used. The boxplots below panel (D) display GPU Number and TFLOPs across four categories: Affiliation, Phase, Method, and Domain. TFLOPS: Tera Floating-Point Operations Per Second; FP16: 16-bit floating-point format (Note: * < 0.001)."
        },
        {
            "title": "3.6 Average Effort Required",
            "content": "We examine the corelation between GPU and TFLOP resources used, and both the number of papers and citations per paper (illustrated in Figure 4 & D). We found that accepted papers typically used median of four GPUs (mean: 46.42, SD: 932.74) and had median of five authors (mean: 5.91, SD: 3.23). Our survey data aligned with these publications patterns, indicating median of four actual contributors per project (mean = 4.6, SD = 3.3) and an average project duration of approximately five months (mean = 160.8 days; median = 123 days). The GPU trend is inconsistent across both academia and industry. In contrast, when compute is measured in TFLOPs, clearer positive relationship emerges, particularly for industry institutions (Figure 4 panel (C)). Access to high-throughput compute infrastructure, rather than GPU quantity alone, plays more decisive role in enabling sustained FM research output. The industrys dominance in the high-TFLOP range reveals the growing resource gap between private and academic institutions. 7 The Role of Computing Resources in Publishing Foundation Model Research Figure 4: Funding Distribution of FM Research. Only 15.3% of FM papers contain funding country and agency information in their manuscript. A) Distribution of funding by country across three categories: Government, Corporate, and Foundation. B) Relationship between each countrys GDP per capita and the number of funded papers. For academic and industry settings: C) Relationship between available GPU resources and average number of papers produced. D) Relationship between GPU resources and average citation count per paper. We also examine how compute resources relate to research impact, measured by citations (Figure 4 panel (D)). GPU count shows only modest correlation with citation rates, while TFLOPs display stronger association as institutions with higher compute tend to produce more cited work. While compute access does not guarantee impact, it does appear to confer an advantage, especially where large-scale experimentation and model deployment are feasible. But the presence of high-citation papers from less compute-intensive institutions also reveal that research influence remains multifactorial and not entirely dependent on resource scale. Note, the average citation count for FM papers is 40.13 (median: 8.00, std: 216.02), which is comparable to other reported averages (Movva et al. (2024); Mohammad (2020)) in high-impact fields (See Appendix Figure S8). Within FM papers, ML conferences generally have higher citations than NLP conferences, with ICML has the highest average citation (78.0, SD: 312.0). 8 The Role of Computing Resources in Publishing Foundation Model Research 3.7 Accepted and Rejected Papers Resource Usage To date there have been few investigations into the impact of higher GPU numbers (or TFLOPs) and paper acceptance. An OpenReview analysis of 2017-2022 ICLR submissions showed review scores primarily correlated with factors such as perceived novelty and clarity (Wang et al. (2023)). Top conferences venues like ACL and EMNLP also instruct reviewers not to penalize papers using simpler methods (Figure 1 panel (B)), or focusing on data resources. Our analysis of accepted and rejected ICLR papers from 2022-2024 suggests that an average rejected papers had lower GPU usage, fewer TFLOPs, and smaller author teams than accepted submissions. However, the distributions of accepted and rejected papers are very similar. We note that ICLR is the only conference among those studied that publicly releases rejected and withdrawn papers. This small sample size limits broader generalization to other conferences and fields."
        },
        {
            "title": "4 Discussion",
            "content": "More/Better? Our study provides empirical evidence that increasing the number of GPUs does not inherently lead to higher research impact. This is important as unchecked expansion of computational requirements further exacerbates environmental concerns (Schwartz et al. (2020)). Furthermore, the current landscape of FM research remains highly centralized, with China and the United States disproportionately dominating the field, as access to computing resources often serves as fundamental prerequisite for participation (Lehdonvirta et al. (2024)). We note that many papers utilized multiple GPUs for different tasks, making it challenging to clearly categorize GPU numbers, types, and memory configurations. Consequently, our calculated FLOPs values may differ slightly from the actual computational resources reported in these studies. Open Reporting While initiatives such as the required computing statement from some conferences acknowledge the role of computational resources, they remain insufficiently reported. Greater transparency in GPU usage (Bommasani et al. (2025)) and recognition of computing resources, including GPU availability, storage, and human labor, are integral components in evaluating AI research. This will ensure the long-term sustainability of AI research (Maslej et al. (2025)). While we quantified GPU usage and authorship, other resource costs are often overlooked. The cost of failed experiments is rarely acknowledged; research highlights successful outcomes, yet unsuccessful attempts are crucial to the progress of FM research. Furthermore, infrastructure costs, which vary between countries due to Gross domestic product (GDP) and AI policies, are generally not considered. Automated Evaluation We relied on GPT-4o to extract and summarize detailed information from PDF files, method that is susceptible to inaccuracies. We performed ten rounds of GPT-4o extraction and summarized the results through majority voting to minimize errors. Nevertheless, the extracted information may still contain inaccuracies, necessitating careful interpretation and validation."
        },
        {
            "title": "Conclusion",
            "content": "Computing resources, and GPUs in particular, have become the fuel propelling modern FM research - but they are also source of new divides in the research community. We found that projects with access to greater GPU power generally produce more advanced pre-trained models, often achieving higher performance thanks to longer training on larger models and datasets. This advantage translates into higher likelihood of publishing in top venues, contributing to an observed concentration of influential AI research in the hands of those with abundant compute. As we move into 2025 and beyond, hardware progress and growing investment will make even more GPUs available, yet the disparities in who can use vast compute may persist or even widen if deliberate steps arent taken (Muro & Wang (2024)). The future of FM research will likely feature ever-larger models and even greater compute hunger, but also push toward more efficient methods and shared resources to counterbalance the compute divide (Zhao et al. (2025); Garisto (2024); Vipra & West (2023)). Ethically, the community is grappling with how to allocate resources in way that is fair and sustainable, aiming to democratize AI research access while being mindful of environmental footprints. Addressing these challenges will be crucial to ensure that the next wave of AI breakthroughs is inclusive, responsible, and beneficial to all. 9 The Role of Computing Resources in Publishing Foundation Model Research"
        },
        {
            "title": "References",
            "content": "Acemoglu, D. The Simple Macroeconomics of AI, May 2024. URL https://www.nber.org/papers/w32487. Ahmed, N., Wahed, M., and Thompson, N. C. The growing influence of industry in AI research. Science, 379 (6635):884886, March 2023. doi:10.1126/science.ade2420. URL https://www.science.org/doi/10.1126/ science.ade2420. Publisher: American Association for the Advancement of Science. Anjum, O., Hwu, W.-M., and Xiong, J. Retrospective Recount of Computer Architecture Research with DataDriven Study of Over Four Decades of ISCA Publications, June 2019. URL http://arxiv.org/abs/1906. 09380. arXiv:1906.09380 [cs]. Awais, M., Naseer, M., Khan, S., Anwer, R. M., Cholakkal, H., Shah, M., Yang, M.-H., and Khan, F. S. Foundation Models Defining New Era in Vision: Survey and Outlook. IEEE Transactions on Pattern Analysis and Machine Intelligence, 47(4):22452264, April 2025. ISSN 1939-3539. doi:10.1109/TPAMI.2024.3506283. URL https://ieeexplore.ieee.org/abstract/document/10834497. Conference Name: IEEE Transactions on Pattern Analysis and Machine Intelligence. Bommasani, R., Hudson, D. A., Adeli, E., Altman, R., Arora, S., Arx, S. v., Bernstein, M. S., Bohg, J., Bosselut, A., Brunskill, E., Brynjolfsson, E., Buch, S., Card, D., Castellon, R., Chatterji, N., Chen, A., Creel, K., Davis, J. Q., Demszky, D., Donahue, C., Doumbouya, M., Durmus, E., Ermon, S., Etchemendy, J., Ethayarajh, K., Fei-Fei, L., Finn, C., Gale, T., Gillespie, L., Goel, K., Goodman, N., Grossman, S., Guha, N., Hashimoto, T., Henderson, P., Hewitt, J., Ho, D. E., Hong, J., Hsu, K., Huang, J., Icard, T., Jain, S., Jurafsky, D., Kalluri, P., Karamcheti, S., Keeling, G., Khani, F., Khattab, O., Koh, P. W., Krass, M., Krishna, R., Kuditipudi, R., Kumar, A., Ladhak, F., Lee, M., Lee, T., Leskovec, J., Levent, I., Li, X. L., Li, X., Ma, T., Malik, A., Manning, C. D., Mirchandani, S., Mitchell, E., Munyikwa, Z., Nair, S., Narayan, A., Narayanan, D., Newman, B., Nie, A., Niebles, J. C., Nilforoshan, H., Nyarko, J., Ogut, G., Orr, L., Papadimitriou, I., Park, J. S., Piech, C., Portelance, E., Potts, C., Raghunathan, A., Reich, R., Ren, H., Rong, F., Roohani, Y., Ruiz, C., Ryan, J., RÃl, C., Sadigh, D., Sagawa, S., Santhanam, K., Shih, A., Srinivasan, K., Tamkin, A., Taori, R., Thomas, A. W., TramÃlr, F., Wang, R. E., Wang, W., Wu, B., Wu, J., Wu, Y., Xie, S. M., Yasunaga, M., You, J., Zaharia, M., Zhang, M., Zhang, T., Zhang, X., Zhang, Y., Zheng, L., Zhou, K., and Liang, P. On the Opportunities and Risks of Foundation Models, July 2022. URL http://arxiv.org/abs/2108.07258. arXiv:2108.07258 [cs]. Bommasani, R., Kapoor, S., Klyman, K., Longpre, S., Ramaswami, A., Zhang, D., Schaake, M., Ho, D. E., Narayanan, A., and Liang, P. Considerations for governing open foundation models. Science, 386(6718):151153, October 2024. doi:10.1126/science.adp1848. URL https://www.science.org/doi/10.1126/science.adp1848. Publisher: American Association for the Advancement of Science. Bommasani, R., Klyman, K., Longpre, S., Xiong, B., Kapoor, S., Maslej, N., Narayanan, A., and Liang, P. Foundation Model Transparency Reports. In Proceedings of the 2024 AAAI/ACM Conference on AI, Ethics, and Society, pp. 181195. AAAI Press, February 2025. Brown, T., Mann, B., Ryder, N., Subbiah, M., Kaplan, J. D., Dhariwal, P., Neelakantan, A., Shyam, P., Sastry, G., Askell, A., Agarwal, S., Herbert-Voss, A., Krueger, G., Henighan, T., Child, R., Ramesh, A., Ziegler, D., Wu, J., Winter, C., Hesse, C., Chen, M., Sigler, E., Litwin, M., Gray, S., Chess, B., Clark, J., Berner, C., McCandlish, S., Radford, A., Sutskever, I., and Amodei, D. Language Models are Few-Shot Learners. In Advances in Neural Information Processing Systems, volume 33, pp. 18771901. Curran Associates, Inc., 2020. URL https://papers.nips.cc/ paper/2020/hash/1457c0d6bfcb4967418bfb8ac142f64a-Abstract.html. Cottier, B. Who Is Leading in AI? An Analysis of Industry AI Research, November 2023. URL https://epoch.ai/ blog/who-is-leading-in-ai-an-analysis-of-industry-ai-research. Cottier, B., Rahman, R., Fattorini, L., Maslej, N., and Owen, D. The rising costs of training frontier AI models, May 2024. URL http://arxiv.org/abs/2405.21015. arXiv:2405.21015 [cs]. Crawford, K. Generative AIs environmental costs are soaring - and mostly secret. Nature, 626(8000): doi:10.1038/d41586-024-00478-x. URL https://www.nature.com/articles/ 693693, February 2024. d41586-024-00478-x. Bandiera_abtest: Cg_type: World View Publisher: Nature Publishing Group Subject_term: Machine learning, Computer science, Technology, Policy. Dodge, J., Gururangan, S., Card, D., Schwartz, R., and Smith, N. A. Show Your Work: Improved Reporting of Experimental Results. In Inui, K., Jiang, J., Ng, V., and Wan, X. (eds.), Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP), pp. 21852194, Hong Kong, China, November 2019. Association for Computational Linguistics. doi:10.18653/v1/D19-1224. URL https://aclanthology.org/D19-1224/. Fan, L., Li, L., Ma, Z., Lee, S., Yu, H., and Hemphill, L. Bibliometric Review of Large Language Models Research from 2017 to 2023. ACM Trans. Intell. Syst. Technol., 15(5):91:191:25, October 2024. ISSN 2157-6904. doi:10.1145/3664930. URL https://doi.org/10.1145/3664930. 10 The Role of Computing Resources in Publishing Foundation Model Research Firoozi, R., Tucker, J., Tian, S., Majumdar, A., Sun, J., Liu, W., Zhu, Y., Song, S., Kapoor, A., Hausman, K., Ichter, B., Driess, D., Wu, J., Lu, C., and Schwager, M. Foundation models in robotics: Applications, challenges, and the future. The International Journal of Robotics Research, pp. 02783649241281508, September 2024. ISSN 02783649. doi:10.1177/02783649241281508. URL https://doi.org/10.1177/02783649241281508. Publisher: SAGE Publications Ltd STM. Garisto, D. How cutting-edge computer chips are speeding up the AI revolution. Nature, 630(8017):544546, June 2024. doi:10.1038/d41586-024-01544-0. URL https://www.nature.com/articles/d41586-024-01544-0. Bandiera_abtest: Cg_type: News Feature Publisher: Nature Publishing Group Subject_term: Technology, Computer science, Machine learning. Guest, O. and Wei, K. Bridging the Artificial Intelligence Governance Gap: The United States and Chinas Divergent Approaches to Governing General-Purpose Artificial Intelligence. Technical report, December 2024. URL https://www.rand.org/pubs/perspectives/PEA3703-1.html. Gunter, T., Wang, Z., Wang, C., Pang, R., Narayanan, A., Zhang, A., Zhang, B., Chen, C., Chiu, C.-C., Qiu, D., Gopinath, D., Yap, D. A., Yin, D., Nan, F., Weers, F., Yin, G., Huang, H., Wang, J., Lu, J., Peebles, J., Ye, K., Lee, M., Du, N., Chen, Q., Keunebroek, Q., Wiseman, S., Evans, S., Lei, T., Rathod, V., Kong, X., Du, X., Li, Y., Wang, Y., Gao, Y., Ahmed, Z., Xu, Z., Lu, Z., Rashid, A., Jose, A. M., Doane, A., Bencomo, A., Vanderby, A., Hansen, A., Jain, A., Anupama, A. M., Kamal, A., Wu, B., Brum, C., Maalouf, C., Erdenebileg, C., Dulhanty, C., Moritz, D., Kang, D., Jimenez, E., Ladd, E., Shi, F., Bai, F., Chu, F., Hohman, F., Kotek, H., Coleman, H. G., Li, J., Bigham, J., Cao, J., Lai, J., Cheung, J., Shan, J., Zhou, J., Li, J., Qin, J., Singh, K., Vega, K., Zou, K., Heckman, L., Gardiner, L., Bowler, M., Cordell, M., Cao, M., Hay, N., Shahdadpuri, N., Godwin, O., Dighe, P., Rachapudi, P., Tantawi, R., Frigg, R., Davarnia, S., Shah, S., Guha, S., Sirovica, S., Ma, S., Ma, S., Wang, S., Kim, S., Jayaram, S., Shankar, V., Paidi, V., Kumar, V., Wang, X., Zheng, X., Cheng, W., Shrager, Y., Ye, Y., Tanaka, Y., Guo, Y., Meng, Y., Luo, Z. T., Ouyang, Z., Aygar, A., Wan, A., Walkingshaw, A., Narayanan, A., Lin, A., Farooq, A., Ramerth, B., Reed, C., Bartels, C., Chaney, C., Riazati, D., Yang, E. L., Feldman, E., Hochstrasser, G., Seguin, G., Belousova, I., Pelemans, J., Yang, K., Vahid, K. A., Cao, L., Najibi, M., Zuliani, M., Horton, M., Cho, M., Bhendawade, N., Dong, P., Maj, P., Agrawal, P., Shan, Q., Fu, Q., Poston, R., Xu, S., Liu, S., Rao, S., Heeramun, T., Merth, T., Rayala, U., Cui, V., Sridhar, V. R., Zhang, W., Zhang, W., Wu, W., Zhou, X., Liu, X., Zhao, Y., Xia, Y., Ren, Z., and Ren, Z. Apple Intelligence Foundation Language Models, July 2024. URL http://arxiv.org/abs/2407.21075. arXiv:2407.21075 [cs]. Kapoor, S., Bommasani, R., Klyman, K., Longpre, S., Ramaswami, A., Cihon, P., Hopkins, A., Bankston, K., Biderman, S., Bogen, M., Chowdhury, R., Engler, A., Henderson, P., Jernite, Y., Lazar, S., Maffulli, S., Nelson, A., Pineau, J., Skowron, A., Song, D., Storchan, V., Zhang, D., Ho, D. E., Liang, P., and Narayanan, A. Position: on the societal impact of open foundation models. In Proceedings of the 41st International Conference on Machine Learning, volume 235 of ICML24, pp. 2308223104, Vienna, Austria, July 2024. JMLR.org. Khandelwal, A., Yun, T., Nayak, N. V., Merullo, J., Bach, S. H., Sun, C., and Pavlick, E. $100K or 100 Days: Tradeoffs when Pre-Training with Academic Resources, October 2024. URL http://arxiv.org/abs/2410.23261. arXiv:2410.23261 [cs]. Klyman, K., Bao, A., Meinhardt, C., Zhang, D., Cryst, E., Wald, R., and Li, F.-f. Expanding URL https://hai.stanford.edu/policy/ Academias Role in Public Sector AI expanding-academias-role-in-public-sector-ai. Stanford HAI. Kudiabor, H. AIs computing gap: academics lack access to powerful chips needed for research. Nature, 636(8041): 1617, November 2024. ISSN 1476-4687. doi:10.1038/d41586-024-03792-6. URL https://www.nature.com/ articles/d41586-024-03792-6. Bandiera_abtest: Cg_type: News Publisher: Nature Publishing Group Subject_term: Research management, Mathematics and computing, Machine learning. Lehdonvirta, V., WÃž, B., and Hawkins, Z. Compute North vs. Compute South: The Uneven Possibilities of Compute-based AI Governance Around the Globe. Proceedings of the AAAI/ACM Conference on AI, Ethics, and Society, 7(1):828838, October 2024. ISSN 3065-8365. doi:10.1609/aies.v7i1.31683. URL https://ojs.aaai. org/index.php/AIES/article/view/31683. Number: 1. Maslej, N., Fattorini, L., Perrault, R., Parli, V., Reuel, A., Brynjolfsson, E., Etchemendy, J., Ligett, K., Lyons, T., Manyika, J., Niebles, J. C., Shoham, Y., Wald, R., and Clark, J. The AI Index 2024 Annual Report. Technical report, AI Index Steering Committee, Institute for Human-Centered AI, Stanford University, Stanford, CA, April 2024. Maslej, N., Fattorini, L., Perrault, R., Gil, Y., Parli, V., Kariuki, N., Capstick, E., Reuel, A., Brynjolfsson, E., Etchemendy, J., Ligett, K., Lyons, T., Manyika, J., Niebles, J. C., Shoham, Y., Wald, R., Walsh, T., Hamrah, A., Santarlasci, L., Lotufo, J. B., Rome, A., Shi, A., and Oak, S. Artificial Intelligence Index Report 2025, April 2025. URL http://arxiv.org/abs/2504.07139. arXiv:2504.07139 [cs]. 11 The Role of Computing Resources in Publishing Foundation Model Research Mohammad, S. M. Examining Citations of Natural Language Processing Literature. In Jurafsky, D., Chai, J., Schluter, N., and Tetreault, J. (eds.), Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pp. 51995209, Online, July 2020. Association for Computational Linguistics. doi:10.18653/v1/2020.aclmain.464. URL https://aclanthology.org/2020.acl-main.464/. Moor, M., Banerjee, O., Abad, Z. S. H., Krumholz, H. M., Leskovec, J., Topol, E. J., and Rajpurkar, P. Foundation models for generalist medical artificial intelligence. Nature, 616(7956):259265, April 2023. ISSN 1476-4687. doi:10.1038/s41586-023-05881-4. URL https://www.nature.com/articles/s41586-023-05881-4. Publisher: Nature Publishing Group. Movva, R., Balachandar, S., Peng, K., Agostini, G., Garg, N., and Pierson, E. Topics, Authors, and Institutions in Large Language Model Research: Trends from 17K arXiv Papers. In Duh, K., Gomez, H., and Bethard, S. (eds.), Proceedings of the 2024 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (Volume 1: Long Papers), pp. 12231243, Mexico City, Mexico, June 2024. Association for Computational Linguistics. doi:10.18653/v1/2024.naacl-long.67. URL https: //aclanthology.org/2024.naacl-long.67/. Mu, K. 2024 Backward Pass: The Definitive Guide to AI in 2024, December 2024. URL https://docsend.com. Muro, M. and Wang, J. Piloting Inclusive AI: The Role of the National Artificial Intelligence Research Resource. July 2024. URL https://coilink.org/20.500.12592/atrbht3. Brookings Institution. Nickolls, J. and Dally, W. J. The GPU Computing Era. IEEE Micro, 30(2):5669, March 2010. ISSN 1937-4143. doi:10.1109/MM.2010.41. URL https://ieeexplore.ieee.org/abstract/document/5446251. OpenAI, Achiam, J., Adler, S., Agarwal, S., Ahmad, L., Akkaya, I., Aleman, F. L., Almeida, D., Altenschmidt, J., Altman, S., Anadkat, S., Avila, R., Babuschkin, I., Balaji, S., Balcom, V., Baltescu, P., Bao, H., Bavarian, M., Belgum, J., Bello, I., Berdine, J., Bernadett-Shapiro, G., Berner, C., Bogdonoff, L., Boiko, O., Boyd, M., Brakman, A.-L., Brockman, G., Brooks, T., Brundage, M., Button, K., Cai, T., Campbell, R., Cann, A., Carey, B., Carlson, C., Carmichael, R., Chan, B., Chang, C., Chantzis, F., Chen, D., Chen, S., Chen, R., Chen, J., Chen, M., Chess, B., Cho, C., Chu, C., Chung, H. W., Cummings, D., Currier, J., Dai, Y., Decareaux, C., Degry, T., Deutsch, N., Deville, D., Dhar, A., Dohan, D., Dowling, S., Dunning, S., Ecoffet, A., Eleti, A., Eloundou, T., Farhi, D., Fedus, L., Felix, N., Fishman, S. P., Forte, J., Fulford, I., Gao, L., Georges, E., Gibson, C., Goel, V., Gogineni, T., Goh, G., Gontijo-Lopes, R., Gordon, J., Grafstein, M., Gray, S., Greene, R., Gross, J., Gu, S. S., Guo, Y., Hallacy, C., Han, J., Harris, J., He, Y., Heaton, M., Heidecke, J., Hesse, C., Hickey, A., Hickey, W., Hoeschele, P., Houghton, B., Hsu, K., Hu, S., Hu, X., Huizinga, J., Jain, S., Jain, S., Jang, J., Jiang, A., Jiang, R., Jin, H., Jin, D., Jomoto, S., Jonn, B., Jun, H., Kaftan, T., Kaiser, Å., Kamali, A., Kanitscheider, I., Keskar, N. S., Khan, T., Kilpatrick, L., Kim, J. W., Kim, C., Kim, Y., Kirchner, J. H., Kiros, J., Knight, M., Kokotajlo, D., Kondraciuk, Å., Kondrich, A., Konstantinidis, A., Kosic, K., Krueger, G., Kuo, V., Lampe, M., Lan, I., Lee, T., Leike, J., Leung, J., Levy, D., Li, C. M., Lim, R., Lin, M., Lin, S., Litwin, M., Lopez, T., Lowe, R., Lue, P., Makanju, A., Malfacini, K., Manning, S., Markov, T., Markovski, Y., Martin, B., Mayer, K., Mayne, A., McGrew, B., McKinney, S. M., McLeavey, C., McMillan, P., McNeil, J., Medina, D., Mehta, A., Menick, J., Metz, L., Mishchenko, A., Mishkin, P., Monaco, V., Morikawa, E., Mossing, D., Mu, T., Murati, M., Murk, O., MÃlly, D., Nair, A., Nakano, R., Nayak, R., Neelakantan, A., Ngo, R., Noh, H., Ouyang, L., OKeefe, C., Pachocki, J., Paino, A., Palermo, J., Pantuliano, A., Parascandolo, G., Parish, J., Parparita, E., Passos, A., Pavlov, M., Peng, A., Perelman, A., Peres, F. d. A. B., Petrov, M., Pinto, H. P. d. O., Michael, Pokorny, Pokrass, M., Pong, V. H., Powell, T., Power, A., Power, B., Proehl, E., Puri, R., Radford, A., Rae, J., Ramesh, A., Raymond, C., Real, F., Rimbach, K., Ross, C., Rotsted, B., Roussez, H., Ryder, N., Saltarelli, M., Sanders, T., Santurkar, S., Sastry, G., Schmidt, H., Schnurr, D., Schulman, J., Selsam, D., Sheppard, K., Sherbakov, T., Shieh, J., Shoker, S., Shyam, P., Sidor, S., Sigler, E., Simens, M., Sitkin, J., Slama, K., Sohl, I., Sokolowsky, B., Song, Y., Staudacher, N., Such, F. P., Summers, N., Sutskever, I., Tang, J., Tezak, N., Thompson, M. B., Tillet, P., Tootoonchian, A., Tseng, E., Tuggle, P., Turley, N., Tworek, J., Uribe, J. F. C., Vallone, A., Vijayvergiya, A., Voss, C., Wainwright, C., Wang, J. J., Wang, A., Wang, B., Ward, J., Wei, J., Weinmann, C. J., Welihinda, A., Welinder, P., Weng, J., Weng, L., Wiethoff, M., Willner, D., Winter, C., Wolrich, S., Wong, H., Workman, L., Wu, S., Wu, J., Wu, M., Xiao, K., Xu, T., Yoo, S., Yu, K., Yuan, Q., Zaremba, W., Zellers, R., Zhang, C., Zhang, M., Zhao, S., Zheng, T., Zhuang, J., Zhuk, W., and Zoph, B. GPT-4 Technical Report, March 2024. URL http://arxiv.org/abs/2303.08774. arXiv:2303.08774 [cs]. OpenAI, T. AI and compute, May 2018. URL https://openai.com/index/ai-and-compute/. OpenAI, T. Introducing ChatGPT, November 2022. URL https://openai.com/index/chatgpt/. Owens, J. D., Houston, M., Luebke, D., Green, S., Stone, J. E., and Phillips, J. C. GPU Computing. Proceedings of the IEEE, 96(5):879899, May 2008. ISSN 1558-2256. doi:10.1109/JPROC.2008.917757. URL https://ieeexplore. ieee.org/abstract/document/4490127. The Role of Computing Resources in Publishing Foundation Model Research Prager, D. J. and Omenn, G. S. Research, Innovation, and University-Industry Linkages. Science, 207(4429):379 384, January 1980. doi:10.1126/science.7350670. URL https://www.science.org/doi/10.1126/science. 7350670. Publisher: American Association for the Advancement of Science. Sartor, S. and Thompson, N. Neural Scaling Laws in Robotics, January 2025. URL http://arxiv.org/abs/2405. 14005. arXiv:2405.14005 [cs]. Schwartz, R., Dodge, J., Smith, N. A., and Etzioni, O. Green ai. Communications of the ACM, 63(12):5463, 2020. Publisher: ACM New York, NY, USA. Sinclair, M. D., Ranganathan, P., Upasani, G., Sampson, A., Patterson, D., Jain, R., Parthasarathy, N., and Shah, S. Fifty Years of the International Symposium on Computer Architecture: Data-Driven Retrospective. IEEE Micro, 43(6):109124, November 2023. ISSN 1937-4143. doi:10.1109/MM.2023.3324465. URL https://ieeexplore. ieee.org/document/10315012. Conference Name: IEEE Micro. Vipra, J. and West, S. M. Computational Power and AI, September 2023. URL https://ainowinstitute.org/ publications/compute-and-ai. Wang, G., Peng, Q., Zhang, Y., and Zhang, M. What have we learned from OpenReview? World Wide Web, 26(2): 683708, March 2023. ISSN 1573-1413. doi:10.1007/s11280-022-01109-z. URL https://doi.org/10.1007/ s11280-022-01109-z. Zhao, W. X., Zhou, K., Li, J., Tang, T., Wang, X., Hou, Y., Min, Y., Zhang, B., Zhang, J., Dong, Z., Du, Y., Yang, C., Chen, Y., Chen, Z., Jiang, J., Ren, R., Li, Y., Tang, X., Liu, Z., Liu, P., Nie, J.-Y., and Wen, J.-R. Survey of Large Language Models, March 2025. URL http://arxiv.org/abs/2303.18223. arXiv:2303.18223 [cs]. The Role of Computing Resources in Publishing Foundation Model Research"
        },
        {
            "title": "A Methodology",
            "content": "FM research is typically published in peer-reviewed computer science conferences or journals. For submissions involving experimental work, some conferences require authors to complete checklist detailing aspects such as computational resources used and research with human subjects. After completing the initial draft, many researchers post their work on arXiv to ensure timely visibility and engagement from the community. Each submission is assigned to an area chair (AC), who invites two to three expert reviewers to evaluate the work and provide scores and feedback. Based on these reviews, the AC decides whether to accept or reject the paper, unless the authors choose to withdraw. During this review period, authors are often asked to respond to the feedback of reviewers through rebuttal process or revise-and-resubmit (R&R) cycle aimed at improving the clarity and quality of the work. Acceptance rates for these conferences generally range from 25% to 35%. A.1 Public Data Collection In this study, we initially gathered papers related to foundation model from eight prominent peer-reviewed conferences via the OpenReview API and the Association for Computational Linguistics (ACL) Rolling Review (ARR) platform. These conferences include NeurIPS, ICLR, ICML, COLM, EMNLP, ACL, EACL, and NAACL. Every conference is held annually and only has one round; the detailed submission and publication dates are shown in Table 2. The specific number of papers presented at each conference per year is shown in Table 3. Table 2: Submission Deadlines and Conference Dates (2022-2024). The submission deadline marks the date when researchers submit the initial version of their papers. The conference date typically corresponds to the time when accepted papers are formally published. \"-\" indicates that the conference did not take place in that particular year. Conference 2022 Deadline 2022 Conference Date 2023 Deadline 2023 Conference Date 2024 Deadline 2024 Conference Date ACL ICML COLM NeurIPS EMNLP EACL ICLR NAACL Nov. 15, 2021 May 22-27, 2022 Nov. 15, 2022 July 9-14, 2023 Nov. 15, Aug. 11-16, 2024 Jan. 20, 2022 Jul. 17-23, 2022 Jan. 26, 2023 Jul. 23-29, 2023 Jan. 25, 2024 Mar. 30, 2024 Jul. 21-27, Oct. 7-9, 2024 May 19, 2022 Nov. 28-Dec. 9, 2022 May 17, 2023 Dec. 10-16, 2023 May 15, Dec. 10-15, 2024 Jun. 24, 2022 Oct. 13, 2022 Sep. 28, 2022 Jan. 15, 2022 Dec. 7-11, Spring 2023 Apr. 25-29, 2022 Jul. 10-15, 2022 Jun. 15, 2023 Sep. 15, 2023 Sep. 21, Dec. 6-10, 2023 Jun. 13, 2024 Nov. 12-16, 2024 Mar. 17-22, 2024 May 1-5, 2023 Sep. 27, 2024 May 7-11, 2024 Dec. 15, Jun. 16-21, 2024 Table 3: The proportion of selected papers among total accepted papers across eight conferences. Year NeurIPS ICLR(Accepted)"
        },
        {
            "title": "Total",
            "content": "ICLR(Rejected) 2022 2023 0.40% 5.50% 0.00% 1.21% 0.00% 2.40% N/A N/A 5.90% 5.80% 4.50% N/A 2.07% 29.40% 12.70% N/A 8.10% 10.29% 0.00% 0.41% 2024 15.70% 17.30% 13.00% 49.80% 56.20% 59.50% 45.90% 32.10% 34.64% 11.88%"
        },
        {
            "title": "Total",
            "content": "8.35% 8.32% 6.72% 49.76% 34.42% 31.18% 27.65% 18.83% 19.07% 5.70% 5,889 papers (16.91%) were collected from total of 34,828 papers. We also included 751 rejected or withdrawn papers from ICLR. After removing duplicates through paper titles (123 rejected/withdrawn papers that were resubmitted to other conferences within our dataset), we obtained 6,517 unique papers. We exclude workshop papers. The detailed list of keywords includes Foundation Model, Large Language Model, language model, Large Model, LLM, \"GPT,\" \"Large Vision Model,\" \"Multi-modal Large Language Model,\" Large Vision Language Model, or VLM. We then employed GPT-4o mini to extract key information from the title and abstracts, including: (1) the primary domain (e.g., NLP, computer vision, data mining, security, explainable AI, or other specialized domains like systems or HCI); (2) the primary phase of the paper (e.g., pre-trained, post-trained, or inference); and (3) the primary methodology (e.g., algorithm, dataset/benchmark, empirical study, or toolkit). We independently ran GPT-4o mini to analyze each papers title and abstract five times and determined its final domain, phase, and methodology 14 The Role of Computing Resources in Publishing Foundation Model Research through majority voting. In cases where multiple domains, phases, or methods were present, we selected only the first mentioned, treating it as the primary category. We used OpenAIs GPT-4o mini to process all the PDF documents through an integrated OCR (Optical Character Recognition) pipeline that converts text content into machine-readable text before applying language model analysis. The system skips the parts that contain images. Once the text is extracted, GPT-4o mini parses the document in segments, using contextual understanding to identify relevant sections. The process is optimized for robustness and generalization across heterogeneous formats, enabling high-quality downstream information extraction, such as identifying GPU specifications, datasets, and/or funding disclosures from papers. We employed an automated service SerpAPI to collect paper citation data from Google Scholar using the papers title, up to the cutoff date of March 11th, 2025. Here are the details of the conferences full name, acceptance rate, and h5 index *: 1. NeurIPS: Conference on Neural Information Processing Systems (acceptance: h5 index = 337) 2. ICLR: International Conference on Learning Representations (h5 index = 304) 3. ICML: International Conference on Machine Learning (h5 index = 268) 4. COLM: Conference on Language Modeling (h5 index = N/A) 5. EMNLP: Empirical Methods in Natural Language Processing (h5 index = 193) 6. ACL: Meeting of the Association for Computational Linguistics (h5 index = 215) 7. EACL: Conference of the European Chapter of the Association for Computational Linguistics (h5 index = 56) 8. NAACL: Conference of the North American Chapter of the Association for Computational Linguistics (h5 index = 132) * h5 index: the h-index for articles published in the last 5 complete years from Google Scholar Figure 5: Author Number and Citation Number Versus Senior Authors Affiliation, Domain, Phase, and Methodology. (Note: * < 0.001). The Role of Computing Resources in Publishing Foundation Model Research A.1.1 GPT-4o mini Prompt Template GPT-4o Prompt for abstract information scraping \"key (only number)\" + Only return JSON as dictionary: If you dont know the answer, output \"nan\". We would like to summarize this abstract. Please answer these questions. \"value (list) ([in-context reasoning, final answer])\", no commentary: [Output list] Select one primary domain from the following options: Security, Explainable AI, or others (please specify such as system, HCI, ). [Output list] Select one phase that this project experiments primarily include: pre-trained, post-trained, and inference. [Output list] Primary Method: the definition: task through computational instructions. or testing models, while benchmark is standard dataset used to evaluate performance. Empirical refers to knowledge gained from real-world observations or experiments, providing evidence for or against method. designed to aid developers in tasks like model training and deployment, such as TensorFlow or Scikit-learn. An algorithm is step-by-step process for solving problem or completing dataset is collection of data used for training toolkit is set of software tools or libraries algorithm, dataset/benchmark, empirical, toolkit. NLP, CV, Data Mining, Here is GPT-4o Prompt for Extracting Information from PDFs NLP, CV, Data \"key(only number)\" + \"value Only return JSON as dictionary: 16GB, 24GB, 32GB, 40GB, 48GB, 80GB, 96GB. Calculate all the cost in USD. If you dont algorithm, dataset/benchmark, empirical, toolkit. NVIDIA Tesla V100, NVIDIA Tesla A100, NVIDIA Tesla Retrieve the original key context or description that Please answer these questions with REASONING which contains [Output list] Select one primary domain from the following options: [Output string] GPU Storage: [Output string] Inference time: [Output list] LLM(s) used for fine-tuning or inference in the paper: [Output list] Select one phase that this project experiments primarily include: [Output list] Primary Method: [Output string] Number of GPU(s) used. [Output string] Type of GPU used: We would like to summarize from this pdf. know the answer, output \"nan\". evidence from the given text. (list) (in-context reasoning + final answer)\", no commentary: 1. Mining, Security, Explainable AI, or others (please specify such as system, HCI, ). 2. pre-trained, post-trained, and inference. 3. 4. 5. P100, NVIDIA Tesla A40, NVIDIA Tesla A800, NVIDIA Tesla A4000, NVIDIA Tesla A5000, 3090, 4090. 6. 7. contains GPU time (<50 words). 8. OpenAI GPT4, OpenAI GPT3.5, OpenAI GPT3, OpenAI GPT2, Google Gemini, Google Bert, Meta LLaMA, Google PaLM, Anthropic Claude, Megatron-Turing, Flan-T5, others (please specify). 9. OpenAI GPT3.5, OpenAI GPT3, OpenAI GPT2, Google Gemini, Google Bert, Meta LLaMA, Google PaLM, Anthropic Claude, Megatron-Turing, Flan-T5, others (please specify). 10. 11. US, EU, Canada, Australia, Others. 12. Corporate, NGO, Foundation. 13. 14. 15. 16. 17. 18. If yes, output the structure like this: 1. software licensing fee, participant incentives, energy/environmental costs, retraining costs, others) 2. [Output string] Dataset Curation/Construction Cost. [Output string] API Cost (i.e. [Output string] Estimate # of Rows. [Output string] Average Length of each data item (tokens). [Output string] Data Storage. [Output list] Other type(s) and amount(s) of cost mentioned in the paper. [Output list] GitHub Link ONLY for this paper. [Output list] Funding Resource Region/Country (ONLY search in acknowledgment): [Output list] Funding Resource (ONLY search in acknowledgment): [Output list] LLM(s) used for evaluation in the paper: Cost source 1 (selected one from the sample list: model testing, and iterative retraining). hardware devices, cloud services, OpenAI GPT4o, OpenAI GPT4, Total $ of Cost source 1 Salary, Government, OpenAI GPT4o, China, 16 The Role of Computing Resources in Publishing Foundation Model Research 3. 4. 5. 6. Cost source 2 (...) Total $ of Cost source 2 Cost source 3 (...) Total $ of Cost source"
        },
        {
            "title": "B Survey Questions",
            "content": "The survey studys IRB is approved by MIT Committee On the Use of Humans as Experimental Subjects (COUHES). Data collection took place from September 2024 to December 2024. Several major machine learning conferences require authors to report, and reviewers to consider, information related to computing resources. These checklist items are intended to promote responsible machine learning research by encouraging documentation of reproducibility, transparency, ethical considerations, and potential societal impact. We examined the author and reviewer guidelines of eight conferences and incorporated relevant items into our survey  (Table 4)  . Notably, the four ARR conferences (EMNLP, ACL, NAACL, and EACL) use unified checklist and review form, which include more detailed questions on computing resources than the other conferences. Our survey was designed to reflect this specificity, including questions on GPU usage, datasets, and human labor involved in the research process. Survey responses were reviewed for eligibility and quality by Y. Hao, Y. Huang, and C. Zhao. total of 56 responses were excluded due to ineligibility, including submissions from non-first authors, duplicate entries from co-first authors on the same paper, or unpublished or incomplete work. Additional exclusions were made for responses unrelated to LLM research or those deemed invalid due to unanswered mandatory questions or illogical answers. Following data cleaning, we cross-referenced the paper titles and their corresponding ArXiv/OpenReview/PDF links with the papers in our public data collection database. total of 122 papers were already included in the database. Since the survey was open to researchers beyond the eight peer-reviewed conferences initially targeted, we received papers from 15 different conferences and journals. For papers available on OpenReview with review scores and context, we followed the same process as outlined in Appendix A. For papers available only via ArXiv or PDF links, we used GPT-4o mini to extract relevant context, including computational resources, dataset details, and funding information. The detailed survey items are as follows: Part I: Paper Information Contact Email for Gift Card Paper Name (Please insert the completed published title which can be retrieved through Google Scholar) * Paper OpenReview/ArXiv PDF Link Published Year * If published: published at conference/journal name ...... (i.e., ICML 2024 Poster; ECCV 2023 Workshop) Part II: Human Resource Cost Information Project Start Month Year (i.e. June, 2023) * Project Completion Month Year (i.e. May, 2024) * Preprint Month, Year (i.e. August, 2024) * Numbers of paper contributors (who actually work/provide guidance on the project, not just the manuscript) NUMBERS ONLY * How many times you submit this paper and get accepted? * Funding Resource Region/Country(s) Funding Resource Type(s) 17 The Role of Computing Resources in Publishing Foundation Model Research Part III: Computing Cost Information My paper used: GPU/CPU/didnt use any computing resource * Number of GPUs that used in this paper: NUMBERS ONLY Types of GPU that am using for this paper My GPUs Storage Unit (Each) Average Single GPU running time: (estimate in HOURS) Part IV: Dataset Cost Information Dataset Platform used for data collection, cleaning, labeling, evaluation, etc * Dataset Total Cost used for data collection, cleaning, labeling, evaluation, etc (in USD, NUMBERS ONLY ) * API Cost in USD (i.e. model testing, and iterative retraining): NUMBERS ONLY Data Storage (in MB): NUMBERS ONLY Part V: Other Information Other costs not mentioned in this survey? For example, wasted cost because of wrong temperature and inference time? Others Comments Feedback on this Survey * indicates required 18 The Role of Computing Resources in Publishing Foundation Model Research Conferences NeurIPS Computing Resource Related Checklist? NeurIPS Paper Checklist Guidelines Table 4: Conferences and Computing Resource Related Checklist. Specific Items Should Reviewers Consider Computing Resources in Publication Decisions? Experiments Compute Resource: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? 1) The answer NA means that the paper does not include experiments. 2) The paper should indicate the type of compute workers, CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage. 3) The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute. The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didnt make it into the paper) NeurIPS Reviewer Guidelinesa: Not specifically mentioned ICLR No Not required but can mention in the Reproducibility Statement ICLR Reviewer Guideb: Not specifically mentioned. ICML ICML Paper Guidelines If you ran experiments, did you include the amount of compute and the type of resources used (e.g., type of GPUs, internal cluster, or cloud provider)? ICML 2025 Reviewer Instructionsc: Not specifically mentioned. Did you review the supplementary material? Which parts? COLM Review Guidelinesd: ...we ask that you take into account that most researchers do not have access to large-scale compute. While some questions in our field require significant computational resources to study at the upper limits of scale, few publishing authors have these resources. Limiting this type of research to only these labs will stifle innovation and understanding. Naturally, this runs the risk that some small-scale results will not hold when studied later on at large scale. But some results will, and they will not make it unless we, the program committee, make bet on them. Review Forme: Is there enough information in this paper for reader to reproduce the main results, use results presented in this paper in future work (e.g., as baseline), or build upon this work? If the authors state (in anonymous fashion) that datasets will be released, how valuable will they be to others? COLM No Not required but can be mentioned in the Reproducibility Statement EMNLP, ACL, NAACL, EACL ARR Did you run computational experiments? - Did you report the number of parameters in the models used, the total computational budget (e.g., GPU hours), and the computing infrastructure used? Did you use human annotators (e.g., crowdworkers) or research with human subjects? - Did you report the full text of instructions given to participants, including e.g., screenshots, disclaimers of any risks to participants or annotators, etc.? - Did you report information about how you recruited (e.g., crowdsourcing platform, students) and paid participants, and discuss if such payment is adequate given the participants demographic (e.g., country of residence)? https://neurips.cc/Conferences/2024/ReviewerGuidelines https://iclr.cc/Conferences/2025/ReviewerGuide https://icml.cc/Conferences/2025/ReviewerInstructions https://colmweb.org/ReviewGuide.html https://aclrollingreview.org/reviewform"
        }
    ],
    "affiliations": [
        "CSE, University of Notre Dame, South Bend, 46556, USA",
        "Computer Science Department, Lehigh University, Bethlehem, 18015, USA",
        "Computer Science Department, University of California, Los Angeles, 90095, USA",
        "Cornell University, Ithaca, 14850, USA",
        "EECS, MIT, Cambridge, 02135, USA",
        "School of Advanced Computing, University of Southern California, Los Angeles, 90007, USA"
    ]
}