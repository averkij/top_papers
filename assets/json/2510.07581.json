{
    "paper_title": "Expanding the Action Space of LLMs to Reason Beyond Language",
    "authors": [
        "Zhongqi Yue",
        "Weishi Wang",
        "Yundaichuan Zhan",
        "Juncheng Li",
        "Daniel Dahlmeier",
        "Fredrik D. Johansson"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Large Language Models (LLMs) are powerful reasoners in natural language, but their actions are typically confined to outputting vocabulary tokens. As a result, interactions with external environments -- such as symbolic operators or simulators -- must be expressed through text in predefined formats, parsed, and routed to external interfaces. This overloads the model's language with both reasoning and control duties, and requires a hand-crafted parser, external to the LLM. To address this, we decouple environment interactions from language by internalizing them in an Expanded Action space (ExpA), beyond the vocabulary. The model starts reasoning in the default language environment, but may trigger routing actions and switch to an external environment at any time. From there, the model can only invoke environment-specific actions, receive feedback from the environment, and potentially route back to language as a result. To promote effective exploration of the expanded action space and new environments, we introduce ExpA Reinforcement Learning (EARL) with counterfactual policy optimization. On tasks requiring multi-turn interactions and contingent planning, EARL outperforms strong baselines with vocabulary-constrained actions. It performs robustly across calculator-based multi-task learning and, in the partially observed sorting problem, achieves perfect Sort-4 accuracy while self-discovering an efficient algorithm competitive with classical designs."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 7 1 ] . [ 2 1 8 5 7 0 . 0 1 5 2 : r a"
        },
        {
            "title": "EXPANDING THE ACTION SPACE OF LLMS\nTO REASON BEYOND LANGUAGE",
            "content": "Zhongqi Yue1* Weishi Wang2* Yundaichuan Zhan3 Juncheng Li3 Daniel Dahlmeier2 Fredrik D. Johansson1 1Chalmers University of Technology and University of Gothenburg 2SAP 3Zhejiang University Project page: https://expa-rl.github.io/ *Equal contribution. zhongqi@chalmers.se, weishi.wang@sap.com"
        },
        {
            "title": "ABSTRACT",
            "content": "Large Language Models (LLMs) are powerful reasoners in natural language, but their actions are typically confined to outputting vocabulary tokens. As result, interactions with external environmentssuch as symbolic operators or simulatorsmust be expressed through text in predefined formats, parsed, and routed to external interfaces. This overloads the models language with both reasoning and control duties, and requires hand-crafted parser, external to the LLM. To address this, we decouple environment interactions from language by internalizing them in an Expanded Action space (ExpA), beyond the vocabulary. The model starts reasoning in the default language environment, but may trigger routing actions and switch to an external environment at any time. From there, the model can only invoke environment-specific actions, receive feedback from the environment, and potentially route back to language as result. To promote effective exploration of the expanded action space and new environments, we introduce ExpA Reinforcement Learning (EARL) with counterfactual policy optimization. On tasks requiring multi-turn interactions and contingent planning, EARL outperforms strong baselines with vocabulary-constrained actions. It performs robustly across calculator-based multi-task learning and, in the partially observed sorting problem, achieves perfect Sort-4 accuracy while self-discovering an efficient algorithm competitive with classical designs."
        },
        {
            "title": "INTRODUCTION",
            "content": "Recent advancements in Large Language Models (LLMs) have extended their role from pure language reasoners to versatile agents capable of interacting with external environments, including tools, APIs, and embodied systems (Shao et al., 2024; DeepSeek-AI et al., 2025; Qin et al., 2024). This development is motivated by two complementary perspectives. First, external environments can augment LLMs by providing capabilities they lack inherently, such as exact symbolic computation (Lee et al., 2024) or access to up-to-date knowledge (Schick et al., 2023). Second, LLMs can extend their reasoning into external environments by mapping language instructions into operations such as API calls or robotic control, allowing them to solve tasks in the digital or physical world (Qin et al., 2024; Li et al., 2023; Szot et al., 2025; Xiang et al., 2023). LLMs from previous works in this area can be viewed as agents acting in decision processes Figure 1: The Markov Decision Process (MDP) of LLM interacting with external environments. (a) In existing works, LLM is confined to its vocabulary space for both reasoning and interaction with external environments, where the latter requires an external parser to detect special patterns. (b) We decouple environment interactions from language by internalizing them as an Expanded Action space (ExpA) beyond vocabulary. 1 with an action space restricted to vocabulary tokens V, as illustrated in Figure 1a. The agents operate only in natural language, selecting tokens to append to an observation sequence. Interactions with external environments are mediated through parser, which translates predefined text patterns (e.g., tool tags or structured JSON) into environment-specific actions, routed to the corresponding environment (Schick et al., 2023). The environment executes the actions and returns plain-text observation in V, which is appended to the models context. To teach models to adopt such interactions, existing works use in-context examples, and often additional training, such as supervised fine-tuning on labeled tool calls (Schick et al., 2023; Luo et al., 2025), or reinforcement learning (RL) (Feng et al., 2025) with rewards determined by language outputs (Feng et al., 2025; Singh et al., 2025) or by the environments (Qin et al., 2024). We propose fundamental shift from the language-only paradigm. Our aim is threefold: (1) to decouple environment interactions from language reasoning, (2) to enable end-to-end training by removing reliance on external parsers and keeping interactions under the models control, (3) to fully support RL on base models, i.e., Zero-RL (DeepSeek-AI et al., 2025), without requiring supervised tool-call data or adherence to predefined language patterns. Our solution is outlined below. We introduce an Expanded Action space (ExpA) that extends models capabilities beyond outputting vocabulary tokens by creating actions for direct interaction with external environments. In the default language environment, the model can either reason by generating tokens from or trigger routing action gi to activate specific environment (e.g., calculator), appending predefined description (e.g., calculate) to the sequence. Once environment is active, the model is restricted to set of environment-specific actions in (e.g., calculator buttons), which yield observations in (e.g., pressed buttons or calculation results), and upon completion (e.g., pressing =), return control to the language environment. As illustrated in Figure 1b, this paradigm achieves clean separation between language-based reasoning and environment interaction. Importantly, ExpA is fundamentally distinct from simply expanding the token space, as is common in multimodal LLMs Chen et al. (2025b); Wang et al. (2025a). Since external actions are not used as model inputs, ExpA avoids the need for costly fine-tuning to represent new actions tokens in the LLMs input, enabling more efficient and modular integration of environment-specific actions. key challenge when introducing LLM agents to external environments is that the pre-trained models lack experience acting in and observing them. When expanding the action space, introducing new model parameters, there is no guarantee that the agents will interact with the new environments to solve problems. To address this, we employ ExpA with RL (EARL), introducing novel counterfactual policy optimization strategy to encourage exploration of new environments. During training, for each rollout, we construct counterfactual trajectory by forcing routing action at plausible intermediate step, identified as position where the model assigns high probability to the routing description token. The advantage is then computed as the difference between the counterfactual and original rewards, thereby encouraging exploration of rarely visited but essential environments. In summary, we establish principled and scalable framework for reasoning beyond language with the following contributions: Expanded Action space (ExpA): new paradigm that decouples language reasoning from environment interaction by introducing explicit routing and environment-specific actions. ExpA Reinforcement Learning (EARL): an algorithm based on counterfactual rollouts that encourages exploration of rarely invoked but crucial environment interactions. Implementation: efficient support for ExpA rollouts through customized vLLM backend (Kwon et al., 2023) and integration with the VeRL training library (Sheng et al., 2025). Results: On multi-turn tasks requiring contingent planning, EARL outperforms vocabularyconstrained baselines (e.g., by 26.3% on Countdown) and, in the partially observed sorting problem, achieves perfect Sort-4 accuracy while discovering an efficient algorithm."
        },
        {
            "title": "2 RELATED WORK",
            "content": "Recent advances demonstrate LLMs as powerful reasoners in natural language (Yao et al., 2023; Shinn et al., 2023; Schick et al., 2023). Many works have extended their role to agents interacting with external environments, and consider tasks such as tool utilization (Parisi et al., 2022; Lu et al., 2025; Qin et al., 2025), multi-modality interpretability (Zhao et al., 2024; Surıs et al., 2023; 2 Wang et al., 2024), math reasoning (Karpas et al., 2022; He-Yueya et al., 2023; Zhang et al., 2023), program-guided reasoning (Gou et al., 2024b; Gao et al., 2023; Chen et al., 2023; Liang et al., 2023), real-time knowledge integration (Wang et al., 2025b; Gou et al., 2024a; Gu et al., 2024), and domain-specific scenarios (Bran et al., 2024; Jin et al., 2024; Theuma & Shareghi, 2024). However, most existing approaches require the model to express task-specific actions as predefined text patterns, which are then parsed and routed to external environments. This design relies heavily on the models instruction-following ability (Hao et al., 2023), making performance highly sensitive to prompt variations (Mannekote et al., 2025) and dependent on pre-trained knowledge for action execution (Hao et al., 2023). Moreover, many methods require human-crafted demonstrations of tool usage (Chen et al., 2025a; Liu et al., 2023), further limiting scalability. In contrast, EARL endows agents with new capabilities by introducing environment-specific actions, explored and learned through counterfactual policy optimization without human demonstration. While prior works have not explored expanding the action space of LLMs, expanding the token space is common in multimodal LLMs, which typically requires large-scale training on multimodal demonstrations, sometimes combined with online RL (Szot et al., 2025). Another related direction introduces action adaptors, which constrain the model to set of learned actions tailored for specific environment (Chuang et al., 2024; Wang et al., 2025c). This can be viewed as simplified version of Figure 1b, involving only one external environment and no language environment. Beyond LLMs, growing action spaces have been studied to accelerate exploration (Farquhar et al., 2020) or to extend the set of available actions in single environment (Jain et al., 2020; 2022; Chandak et al., 2020). Continual RL research (Khetarpal et al., 2022; Jin et al., 2023; Zhang & Lu, 2023) has also demonstrated the effectiveness of action learning in non-stationary settings (Chandak et al., 2019; Queeney et al., 2024). In contrast, our work considers the more general and challenging case where LLMs must reason in language while sequentially interacting with multiple external environments. To this end, we expand the action space of LLMs in Section 3 and propose reinforcement learning algorithm for efficient exploration of external interactions."
        },
        {
            "title": "3 FORMULATION",
            "content": "Problem setting. We consider Large Language Models (LLMs) that interact sequentially with one or more external environments, in addition to the default language environment. At each step, model (agent) acts by selecting either token from the language environment or an action in an external one. We formalize this as partially observed Markov decision process (POMDP) with global state st = (ht, et, zt), where ht is history of language tokens from vocabulary V, et {0, 1, . . . , K} denotes the agents active environment (et = 0 for language), and zt is the latent state of the external environments. The history ht is fully observed by the agent, comprising record of tokens selected in the language environment and token-based descriptions of observations from the external environments. Unlike ht and et, zt is only partially observed through interactions. The agent is represented by policy πθ(at ht, et), with parameters θ, that samples an action at depending on the observable state and the active environment. Each external environment = 0 exposes step procedure, with set of permissible set of actions Ei, (ot, zt+1, exit) stepi(ht, zt, at) , which executes at Ei, produces an observation described by language tokens ot (), updates the latent state to zt+1 and an exit flag. After acting, the agent updates its history by appending the new observation, ht+1 ht ot, and is routed back to the language environment if exit is true. At each step, the active environment produces reward rt = R(et, ht, at), and the agents objective is to maximize the cumulative reward = (cid:80) rt. In some settings, the agent sets its own reward, defined in the language environment, for example, when the environment functions as tool, e.g., calculator. In others, the language environment only facilitates reasoning capabilities, but the task and the reward are defined entirely by an external environment, for example, when sorting list of unknown numbers by comparing and swapping symbols (see Figure 2). We describe two methods for the policy πθ to issue actions to interact with environments below. Language-only interaction. In existing approaches to tool use and external interaction (Feng et al., 2025; Singh et al., 2025), agents never truly leave the language environment: they always select actions at from their own vocabulary, and extend the observation history as ht+1 = ht at. Interacting with an external environment = 0 requires translating ht+1 to actions in Ei. Typically, 3 Figure 2: An example rollout with ExpA. Here, <sw> and <cp> route to the swap and compare environments, respectively. Inside them, agents can choose <A>,<B>,<C> as operands. After two operands are chosen, the step procedure updates the latent state when necessary, routes back to the language environment, and returns the swap or comparison result as plain-text observation. (h0, e0=0, z0) Algorithm 1 Rollout with ExpA 1: Input: policy πθ, horizon , initial s0 = this is realized by detecting predefined patterns, such as <calculator>...</calculator> or structured JSON fields. When pattern that indicates interaction with environment appears in ht+1, its contents are parsed into sequence of actions from Ei that get executed by stepi. drawback is that no intermediate feedback can affect the choice of actions within the pattern (e.g., <calculator> block), which may hamper performance and credit assignment. Interaction with expanded action space (ExpA). In this work, we let agents interact directly with environments by expanding their action spaces beyond the vocabulary V. For each environment = 0, we add transition action gi for entering it and an environment-specific action set Ei that interfaces with stepi. With = (cid:83)K i=1 gi Ei the set of added actions and = the full action set, the agents policy πθ is distribution over A, conditioned on the history ht and active environment et. Algorithm 1 describes interactions under this paradigm. Every rollout begins in the language environment (e0 = 0), where the policy πθ may either select vocabulary token at to extend the history, or transition action gi that routes control to environment while appending description of gi to ht, denoted as desc(gi). Once inside environment i, the policy chooses actions from Ei, triggering the corresponding stepi, which outputs an observation described in V, updates the latent state, and returns an additional exit flag. If exit = true, the environment resets to et+1 = 0, routing control back to the language environment. We illustrate with an example in Figure 2. 2: for = 0, 1, . . . , 1 do if et = 0 then 3: 4: 5: 6: 7: 8: 9: 10: 11: 12: 13: 14: 15: 16: 17: 18: 19: 20: end for 21: return trajectory τ = {(ht, at, rt)}T 1 t=0 Sample at πθ( ht, et = 0) (ot, zt+1, exit) stepet (ht, zt, at) ht+1 ht desc(at) ot if exit=true then et+1 0 Sample at πθ( ht, et=0) if at then ht+1 ht desc(gi); et+1 i; end if rt R(et, ht, at) language environment external environments else if at = gi then route back to lang ht+1 ht at; route to env end if end if else"
        },
        {
            "title": "4 EXPA REINFORCEMENT LEARNING (EARL)",
            "content": "An expanded action space equips LLMs with explicit means of interacting beyond language. As deciding when to route into an environment and how to act within it are inherently sequential and reward-driven tasks, we select RL as the training paradigm. We first parameterize policy πθ over the expanded action space, with careful initialization to adapt to new actions (Section 4.1). We then introduce Counterfactual Policy Optimization (CPO), which optimizes the following objective: JCPO(θ) = Es0, (s0)={(τi,τ )}m i=1 (cid:34) 1 m (cid:88) i=1 (cid:35) Ui(T (s0); θ) , (1) where s0 is an initial state sampled from the training distribution, (s0) comprises rollout pairs, and Ui(; θ) denotes the update function for the i-th pair conditioned on all rollouts. Each τi is 4 factual rollout obtained by inputting s0 into Algorithm 1, while each τ is counterfactual rollout obtained by forcing routing action at plausible intermediate step in τi. We describe the construction of counterfactual rollouts in Section 4.2 and the design of the update function in Section 4.3."
        },
        {
            "title": "4.1 A POLICY OVER THE EXPANDED ACTION SPACE",
            "content": "A central challenge in operating with expanded action spaces is how to represent and generalize to the newly introduced actions. Prior work points to two guiding principles: First, the policy should condition on the set of available actions (Jain et al., 2020; 2022). Second, prior knowledge about actions can be leveraged to improve generalization, through learned action embeddings (Jain et al., 2022) or by incorporating known structure in training (Farquhar et al., 2020). We adopt both. Policy parameterization. To condition the policy on all available actions, we extend the standard LLM classification head. In the language-only setting, the head produces logits over the vocabulary. With ExpA, this head is expanded to output E logits. We denote by θ the parameters of the LLM together with the expanded head. At step t, the encoded feature of ht is projected to logits, and softmax is applied over the subset of actions available in environment et, yielding πθ( ht, et). Policy initialization. Each action has natural language description desc(a), such as the environment name (e.g., calculator) or the semantic label of step procedure (e.g., compare). To exploit this prior knowledge about action similarities, we initialize the weights of new actions so that selecting an action has approximately the same likelihood as producing its description: πθ(a ht, et) πθ(desc(a) ht, et), where et is the active environment at t. In particular, when the description is single token, this condition can be satisfied directly (and exactly) by initializing the new action weight with the pretrained weight of the token desc(a). This aligns expanded actions with their linguistic counterparts from the start, providing strong prior that accelerates learning. 4.2 COUNTERFACTUAL ROLLOUTS TO ENCOURAGE INVOKING NEW ACTIONS Even with careful initialization and prompting about environments, the policy may fail to reliably invoke routing actions when needed. For instance, pretrained model has no prior experience of invoking calculator and thus may not assign high probability to its routing action (e.g., gcalc = <calculate>), even when complex arithmetic is required. We address this with counterfactual rollouts, which evaluate what would have happened had the policy taken routing action at plausible intermediate step, thereby encouraging exploration of rarely invoked but critical decisions. Given factual rollout τ = {(ht, at, rt)}T 1 t=0 , we construct counterfactual rollout τ as follows: 1) Select routing action gi to be encouraged (e.g., gcalc for arithmetic tasks); 2) Sample time step {t et = 0} with weight proportional to πθ(desc(gi) ht, et = 0); 3) Initialize τ τt using the factual rollout for = 1, ..., t; 4) Intervene with do(at = gi) at and apply the transition in Algorithm 1; 5) Continue rollout for = + 1, . . . , 1 with Algorithm 1 to obtain τ . This relies only on the pretrained next-token distribution (step 2). For example, if desc(gcalc) = calculate, the insertion To solve it, first calculate is more probable under the language model than To solve calculate, ... and is weighed more heavily when forcing routing action. Hence the method is fully compatible with zero-RL training (DeepSeek-AI et al., 2025; Zeng et al., 2025). 4.3 UPDATE FUNCTION Finally, we define the update function for each rollout pair (τi, τ ), {1, . . . , m}, as Ui( (s0); θ ) = ri; θ), (cid:40)f (τ , (τi, riµ σ ; θ), if rj 0 {1, . . . , m}, otherwise, denote the cumulative rewards of τi and τ where ri and , respectively, and µ, σ are the mean and standard deviation of rewards across the factual rollouts. Here (τ, a; θ) denotes the standard update rule (Shao et al., 2024), which takes as input rollout trajectory τ and its associated advantage scalar, and applies PPO-style clipping and KL regularization (details in Section D). The design is motivated 5 Table 1: Statistics of the Calc-Bench datasets. Language portions refer to the portion of questions where operations or numbers are written in natural language. Task Arithmetic Countdown GSM8K Count Max number (10x) #Operands Lang. portion #Instances Train Test Train 5 4 6 20 5 4 6 20 5 4 NA NA Test 7 4 NA NA Train 10% NA NA 90% Test 70% NA NA 90% Train 1,000 20,000 5,317 1,000 Test 2,000 2,000 579 2,000 Figure 3: Example questions and the environment-specific actions in Calc-Bench and Sorting. by balancing exploration and exploitation: when the current rollout fails to achieve positive reward, the first counterfactual branch encourages exploration of missing interactions; otherwise, the update reduces to group-relative advantage, exploiting successful strategies."
        },
        {
            "title": "5 EXPERIMENT",
            "content": "5.1 EXPERIMENTAL SETUP We evaluate our method in settings that require multi-turn interactions with external environments. Prior works evaluate mainly on math problems (Feng et al., 2025) or API calls (Qin et al., 2024), where the solution path follows fixed derivation. We additionally stress the more challenging contingent planning problem, where the agent must adapt its actions on the fly based on intermediate observations from interactions. To this end, we design two complementary tasks below: Calc-Bench. The agent has access to stateless calculator environment (Figure 3) that provides arithmetic knowledge to support language reasoning in tasks where reward is given if the final solution exactly matches (EM) the target. The benchmark comprises four types of tasks, described by example in Figure 3 (statistics in Table 1): (1) Arithmetic tests calculator use out-of-distribution by varying the number of operands and the amount of natural language used in problem instances. This requires robust mapping between language and numbers. (2) Countdown stresses contingent planning: each problem admits up to 7,680 unique combinations, forcing the agent to reason efficiently by adjusting its strategy based on intermediate outcomes (e.g., aggressively adjusting strategy when far from the target). (3) GSM8K enhances the widely used GSM8K Cobbe et al. (2021) by scaling up the numbers while preserving problem semantics, increasing difficulty and requiring accurate understanding of the text and its translation into computational steps. (4) Count requires the agent to preserve its basic numerical understanding while learning tasks (1)-(3). Sorting. The agent must arrange set of hidden numbers in ascending or descending order by interacting with compare and swap environments (Figure 3); for example, compare A, reveals their relative order, while swap A, updates the hidden state zt by exchanging their positions (Figure 2). The reward depends on the final hidden state zT being correctly sorted, with penalties for excessive numbers of comparisons and swaps. This setting is particularly challenging, as it forms POMDP that requires contingent planning based on intermediate comparison results, while also demanding precise and efficient reasoning over first-order logic relations. Moreover, the agent must uncover and modify the hidden state through environment interactions rather than simply outputting textual answer, making this realistic testbed for interactive decision-making situations such as embodied AI. Training data consists of sorting problems of different sizes (Sort2 to Sort-5) and testing evaluates on Sort-4 and Sort-5. Other details are in the Section E. Baselines. We compare against baselines that reflect distinct learning paradigms: (1) SFT+GRPO: the model is first fine-tuned on labeled interaction data and then optimized with Group RelaTable 2: EM results (exact match) on Calc-Bench. We train each model jointly on all Calc-Bench tasks to assess the benefits of shared representation learning. Method GPT-4o Qwen-2.5-3B-Instruct SFT+GRPO Prompt+GRPO Prompt+CPO ExpA+CPO (EARL) Qwen-2.5-7B-Instruct SFT+GRPO Prompt+GRPO Prompt+CPO ExpA+CPO (EARL) Calc-Bench Arithmetic Countdown Count GSM8K Overall 41.30 15.80 70.75 64.70 61.50 69.20 22.60 56.00 80.30 64.85 78.10 18. 2.80 48.50 49.15 38.30 75.15 11.75 66.70 60.70 55.15 84.25 66.85 66.50 93.85 94.75 91.35 93.70 74.05 93.00 98.60 94.55 98.70 31. 20.55 30.57 30.39 46.80 48.53 24.01 34.20 33.33 52.33 53.71 39.74 26.41 60.92 59.75 59.49 71.65 33.10 62.48 68.23 66.72 78.69 Table 3: Average occurrence of external interactions, hallucinations and planning phrases in validation rollout on Countdown task. We define hallucination as inputting number that is not given in the question to the calculator. We identify list of planning phrases commonly used by models in Section I.1. Method #interactions #hallucinations #plannings EARL Prompt+CPO Prompt+GRPO SFT+GRPO 11.25 4.85 15.10 3.16 0.001 0.7424 0.000 0.6885 20.48 3.22 1.84 1. Figure 4: Response token length distribution for correct rollouts in GSM8K. tive Policy Optimization (GRPO) (Shao et al., 2024), following the setup in (Feng et al., 2025). (2) Prompt+GRPO (DeepSeek-AI et al., 2025): environment interaction patterns are provided in the prompt, and the model is further optimized with GRPO, as in (Singh et al., 2025). (3) Prompt+CPO: similar to Prompt+GRPO, but trained with our Counterfactual Policy Optimization (CPO) instead of GRPO, so that the only difference frosm EARL is the absence of expanded action space. (4) Zeroshot: evaluation of proprietary models such as GPT-4o (Hurst et al., 2024) without any fine-tuning. Implementation Details. We use the open-source Qwen2.5 (Yang et al., 2024) as our backbone, including results for both base models and instruction-tuned variants with 0.5B, 3B, and 7B parameters. The maximum sequence length is set to 1,024 for Calc-Bench and 384 for Sorting. Training is performed on NVIDIA A100-80GB GPUs, with 1, 2, and 4 GPUs allocated for the 0.5B, 3B, and 7B models, respectively. For fair comparison, we follow standard hyperparameters and optimization protocols (Singh et al., 2025) (e.g., KL regularization weight, PPO clipping threshold) for both baselines and our method, with full details provided in the Sections and G. 5.2 EXPERIMENTAL RESULTS ON CALC-BENCH Main results. As shown in Table 2, zero-shot models perform poorly on this challenging benchmark, with GPT-4o reaching only 39.74 overall EM. Training with calculator greatly improves performance, but baselines remain inconsistent across tasks and perform especially poorly on contingent-planning tasks like Countdown. In contrast, EARL delivers strong results across all tasks, with up to 10.46 absolute EM gain overall and as much as 17.55 EM gain on Countdown. Countdown analysis. We provide detailed analysis of the results on Countdown with the 3B model in Table 3, supplemented by case studies in the Section J. Several observations emerge: Prompt+GRPO triggers the most calculator interactions and avoids hallucinations. However, it often degenerates into inefficient brute-force trials, showing limited use of planning cues after observations (e.g., this is far from target). 7 Table 4: Ablation on Countdown: CPO vs. GRPO, training on Qwen-Instruct vs. -Base, and with (w/) vs. without (w/o) environment prompt (env.p). ExpA+CPO corresponds to EARL. Instruct Base w/ env.p w/o w/ env.p w/o ExpA+CPO 80.09 76.76 ExpA+GRPO 75.10 73.79 Prompt+CPO 67.23 Prompt+GRPO 58.16 62.05 SFT+GRPO - - - 77.31 76.45 63.64 51.15 61.17 74.56 70.27 - - - Figure 5: ExpA training reward vs. iteration. Replacing GRPO with CPO increases the use of planning keywords, likely because counterfactual interventions provide more training signals on how to react to observations. Yet this also introduces hallucinations, where the agent invents numbers not in the problem. plausible cause is interference between language reasoning and action learning, e.g., the KL penalty helps preserve pre-trained language knowledge but may hinder the grounding of calculator actions. SFT+GRPO uses the fewest interactions, sometimes performing parts of computation in language mode and then feeding incorrect results to the calculator, causing hallucinations. This suggests that SFT-learned patterns transfer poorly to diverse problem instances. EARL uses moderate number of interactions but produces by far the most planning-related language, yielding much stronger results than all baselines in Table 2. This strongly validates the benefit of decoupling environment interactions from language reasoning, which removes confusion between reasoning and action learning and enables effective use of external environments. GSM8K analysis. In Figure 4, we show the distribution of response lengths among correct rollouts on GSM8K with the 3B model. The results reveal clear link between efficiency in reasoning (fewer tokens) and stronger performance. Notably, the two methods using CPO (EARL and Prompt+CPO) outperform those with GRPO (SFT+GRPO and Prompt+GRPO), underscoring the importance of encouraging diverse environment interactions. Ablation Study. We perform ablation on the challenging Countdown task from 3 perspectives: CPO vs. GRPO: As shown in Table 4, CPO consistently outperforms GRPO across all settings, even for baselines (Prompt+CPO). It also converges faster, as seen by comparing each solid line (CPO) with the dotted line of the same color (GRPO) in Figure 5, highlighting the role of counterfactual rollouts in promoting exploration of new environments and their actions. Instruct vs. Base: With ExpA, even base models achieve competitive performance, whereas baselines algorithms such as Prompt+GRPO degrade sharply without instruction tuning. This suggests strong potential for using EARL in Zero-RL training of agents for interactive problem solving. Prompted vs. unprompted environments: Prompting the agent on how to interact is essential for prompt+RL baselines. With ExpA, however, models succeed without such prompts by leveraging weight initialization (Section 4.1, ExpA+GRPO w/o) and counterfactual rollouts (Section 4.2, ExpA+CPO w/o), indicating scalability to settings with large number of environments. Additional results. We provide results on the 0.5B model in the Section H.1 to demonstrate that it can interact with env as well. We also provide the change of validation performance throughout training on Calc-Bench, as well as case studies that highlight our advantages over previous methods. 5.3 EXPERIMENTAL RESULTS ON SORTING Main results. Figure 6 reports sorting accuracy for Prompt+GRPO, Prompt+CPO, and EARL on Sort-4 and Sort-5 tasks, stratified by the minimum number of swaps required. On Sort-4, EARL achieves perfect accuracy across all levels, whereas Prompt+CPO degrade as the required number of swaps increases. The gap widens on the more challenging Sort-5 problems, where EARL outperforms the best baseline (Prompt+GRPO) by up to 21% in stratum and more than 10% overall. 8 Table 5: Average number of swaps (SW) and comparisons (CP) to sort 4 random numbers for different algorithms. Method #SW #CP Prompt+GRPO 2.076 1.917 EARL EARL 1.917 2.333 GCC 3.000 Insert-sort 1.917 Optimal 6.101 5.708 4.833 5.000 4.917 4.667 Figure 6: Sort accuracy stratified by the number of required swaps. Efficiency. We evaluate the efficiency of EARLs learned sorting strategy by measuring the average number of comparisons and swaps needed to sort random numbers. Under greedy decoding, EARL follows deterministic decision tree, which we visualize in the Section H.2. By pruning few redundant comparisons, we obtain simplified variant, EARL, corresponding to the algorithm in Algorithm 2. This algorithm uses the element as pivot to compare against the other elements, performs additional checks only when necessary, and finally applies MIN SWAP(R) to sort with minimal swaps given the accumulated comparison results R. We compare EARL and EARL against Prompt+GRPO, classical sorting algorithms and the theoretical optimum in Table 5. Both variants exactly match the optimal number of swaps and closely approach the optimal number of comparisons, outperforming insertion sort and even GCCs built-in routine.1 Sorting with RL. Our study connects to recent work on discovering faster sorting algorithms with RL, most notably AlphaDev (Mankowitz et al., 2023). key distinction is that we leverage the LLMs natural language vocabulary to represent context and chain reasoning steps, rather than relying on dedicated symbolic states or low-level assembly instructions. This means that our agent is more general-purpose, capable of reusing pre-trained language knowledge. Consequently, EARL achieves 100% accuracy on Sort-4 after only 70 training steps, compared to the million-step training required by AlphaDev, underscoring the value of transferring language reasoning into interactive environments. While performance on Sort-5 is not yet perfect, our goal is to demonstrate how ExpA improves reasoning with external environments, leaving dedicated algorithm discovery and more challenging settings (e.g., VARSORT) as promising future work. Algorithm 2 EARL Sort-4 1: Input: four numbers A, B, C, 2: 3: {Compare(A, B)} 4: {Compare(A, C)} 5: {Compare(A, D)} 6: if not (C < < < < C) then {Compare(B, C)} 7: 8: end if 9: if not (D < < < < D) then {Compare(B, D)} 10: 11: end if 12: if not (D < < < < D) then {Compare(C, D)} 13: 14: end if 15: MIN SWAP(R)"
        },
        {
            "title": "6 CONCLUSION",
            "content": "We have introduced new paradigm for enabling Large Language Models (LLMs) to reason with and beyond language when interfacing with external environments. Our proposed framework, ExpA, introduces routing and environment-specific actions to decouple reasoning from interaction. This removes the reliance on external parsers in the current language-only paradigm to detect special interaction language syntax, and hence enables true end-to-end training. To optimize policies for interactive problem solving, we proposed EARL, reinforcement learning method based on counterfactual rollouts that encourages exploration of new and rarely used, but critical environment interactions. Empirically, EARL outperforms strong baselines on multi-turn tasks that benefit from or require environment interaction, with particular gains in challenging settings that demand contingent planning. It also shows consistent improvements in multi-task and continual learning scenarios, 1We note that GCC is optimized for processor efficiency (e.g., branchless cmov) rather than minimizing raw operations, whereas Optimal denotes the theoretical minimum over swaps and comparisons combined. 9 and notably discovers an efficient algorithm for sorting with four elements. This work establishes scalable and principled framework for equipping LLMs with explicit capabilities to interact with external environments, opening future directions in mathematical reasoning, embodied AI, continual learning, and large-scale zero-RL training with tools."
        },
        {
            "title": "7 ACKNOWLEDGEMENT",
            "content": "Zhongqi Yue is supported by the Wallenberg-NTU Presidential Postdoctoral Fellowship. Fredrik D. Johansson is supported in part by the Wallenberg AI, Autonomous Systems and Software Program funded by the Knut and Alice Wallenberg Foundation. The computations were enabled by resources provided by the National Academic Infrastructure for Supercomputing in Sweden (NAISS), partially funded by the Swedish Research Council through grant agreement no. 2022-06725."
        },
        {
            "title": "REFERENCES",
            "content": "Andres M. Bran, Sam Cox, Oliver Schilter, Carlo Baldassari, Andrew D. White, and Philippe Schwaller. Augmenting large language models with chemistry tools. Nat. Mac. Intell., 6(5): 525535, 2024. doi: 10.1038/S42256-024-00832-8. URL https://doi.org/10.1038/ s42256-024-00832-8. Yash Chandak, Georgios Theocharous, James Kostas, Scott Jordan, and Philip Thomas. Learning action representations for reinforcement learning. In International conference on machine learning, pp. 941950. PMLR, 2019. Yash Chandak, Georgios Theocharous, Chris Nota, and Philip S. Thomas. Lifelong learning with changing action set. In The Thirty-Fourth AAAI Conference on Artificial Intelligence, AAAI 2020, The Thirty-Second Innovative Applications of Artificial Intelligence Conference, IAAI 2020, The Tenth AAAI Symposium on Educational Advances in Artificial Intelligence, EAAI 2020, New York, NY, USA, February 7-12, 2020, pp. 33733380. AAAI Press, 2020. doi: 10.1609/AAAI.V34I04. 5739. URL https://doi.org/10.1609/aaai.v34i04.5739. Guoxin Chen, Zhong Zhang, Xin Cong, Fangda Guo, Yesai Wu, Yankai Lin, Wenzheng Feng, and In The Thirteenth InterYasheng Wang. Learning evolving tools for large language models. national Conference on Learning Representations, ICLR 2025, Singapore, April 24-28, 2025. OpenReview.net, 2025a. URL https://openreview.net/forum?id=wtrDLMFU9v. Wenhu Chen, Xueguang Ma, Xinyi Wang, and William W. Cohen. Program of thoughts prompting: Disentangling computation from reasoning for numerical reasoning tasks. Trans. Mach. Learn. Res., 2023, 2023. URL https://openreview.net/forum?id=YfZ4ZPt8zd. Xiaokang Chen, Zhiyu Wu, Xingchao Liu, Zizheng Pan, Wen Liu, Zhenda Xie, Xingkai Yu, and Chong Ruan. Janus-pro: Unified multimodal understanding and generation with data and model scaling. arXiv preprint arXiv:2501.17811, 2025b. Yun-Shiuan Chuang, Agam Goyal, Nikunj Harlalka, Siddharth Suresh, Robert Hawkins, Sijia Yang, Dhavan Shah, Junjie Hu, and Timothy T. Rogers. Simulating opinion dynamics with In Kevin Duh, Helena Gomez-Adorno, and Steven Bethard networks of llm-based agents. (eds.), Findings of the Association for Computational Linguistics: NAACL 2024, Mexico City, Mexico, June 16-21, 2024, pp. 33263346. Association for Computational Linguistics, 2024. doi: 10.18653/V1/2024.FINDINGS-NAACL.211. URL https://doi.org/10.18653/ v1/2024.findings-naacl.211. Karl Cobbe, Vineet Kosaraju, Mohammad Bavarian, Mark Chen, Heewoo Jun, Lukasz Kaiser, Matthias Plappert, Jerry Tworek, Jacob Hilton, Reiichiro Nakano, Christopher Hesse, and John Schulman. Training verifiers to solve math word problems. CoRR, abs/2110.14168, 2021. URL https://arxiv.org/abs/2110.14168. DeepSeek-AI, Daya Guo, Dejian Yang, Haowei Zhang, Junxiao Song, Ruoyu Zhang, Runxin Xu, Qihao Zhu, Shirong Ma, Peiyi Wang, Xiao Bi, Xiaokang Zhang, Xingkai Yu, Yu Wu, Z. F. Wu, Zhibin Gou, Zhihong Shao, Zhuoshu Li, Ziyi Gao, Aixin Liu, Bing Xue, Bingxuan Wang, Bochao 10 Wu, Bei Feng, Chengda Lu, Chenggang Zhao, Chengqi Deng, Chenyu Zhang, Chong Ruan, Damai Dai, Deli Chen, Dongjie Ji, Erhang Li, Fangyun Lin, Fucong Dai, Fuli Luo, Guangbo Hao, Guanting Chen, Guowei Li, H. Zhang, Han Bao, Hanwei Xu, Haocheng Wang, Honghui Ding, Huajian Xin, Huazuo Gao, Hui Qu, Hui Li, Jianzhong Guo, Jiashi Li, Jiawei Wang, Jingchang Chen, Jingyang Yuan, Junjie Qiu, Junlong Li, J. L. Cai, Jiaqi Ni, Jian Liang, Jin Chen, Kai Dong, Kai Hu, Kaige Gao, Kang Guan, Kexin Huang, Kuai Yu, Lean Wang, Lecong Zhang, Liang Zhao, Litong Wang, Liyue Zhang, Lei Xu, Leyi Xia, Mingchuan Zhang, Minghua Zhang, Minghui Tang, Meng Li, Miaojun Wang, Mingming Li, Ning Tian, Panpan Huang, Peng Zhang, Qiancheng Wang, Qinyu Chen, Qiushi Du, Ruiqi Ge, Ruisong Zhang, Ruizhe Pan, Runji Wang, R. J. Chen, R. L. Jin, Ruyi Chen, Shanghao Lu, Shangyan Zhou, Shanhuang Chen, Shengfeng Ye, Shiyu Wang, Shuiping Yu, Shunfeng Zhou, Shuting Pan, and S. S. Li. Deepseek-r1: Incentivizing reasoning capability in llms via reinforcement learning. CoRR, abs/2501.12948, 2025. doi: 10. 48550/ARXIV.2501.12948. URL https://doi.org/10.48550/arXiv.2501.12948. Gregory Farquhar, Laura Gustafson, Zeming Lin, Shimon Whiteson, Nicolas Usunier, and Gabriel Synnaeve. Growing action spaces. In Proceedings of the 37th International Conference on Machine Learning, ICML 2020, 13-18 July 2020, Virtual Event, volume 119 of Proceedings of Machine Learning Research, pp. 30403051. PMLR, 2020. URL http://proceedings.mlr. press/v119/farquhar20a.html. Jiazhan Feng, Shijue Huang, Xingwei Qu, Ge Zhang, Yujia Qin, Baoquan Zhong, Chengquan Jiang, Jinxin Chi, and Wanjun Zhong. Retool: Reinforcement learning for strategic tool use in llms. arXiv preprint arXiv:2504.11536, 2025. Luyu Gao, Aman Madaan, Shuyan Zhou, Uri Alon, Pengfei Liu, Yiming Yang, Jamie Callan, and Graham Neubig. PAL: program-aided language models. In Andreas Krause, Emma Brunskill, Kyunghyun Cho, Barbara Engelhardt, Sivan Sabato, and Jonathan Scarlett (eds.), International Conference on Machine Learning, ICML 2023, 23-29 July 2023, Honolulu, Hawaii, USA, volume 202 of Proceedings of Machine Learning Research, pp. 1076410799. PMLR, 2023. URL https://proceedings.mlr.press/v202/gao23f.html. Zhibin Gou, Zhihong Shao, Yeyun Gong, Yelong Shen, Yujiu Yang, Nan Duan, and Weizhu Chen. CRITIC: large language models can self-correct with tool-interactive critiquing. In The Twelfth International Conference on Learning Representations, ICLR 2024, Vienna, Austria, May 7-11, 2024. OpenReview.net, 2024a. URL https://openreview.net/forum? id=Sx038qxjek. Zhibin Gou, Zhihong Shao, Yeyun Gong, Yelong Shen, Yujiu Yang, Minlie Huang, Nan Duan, and In Weizhu Chen. Tora: tool-integrated reasoning agent for mathematical problem solving. The Twelfth International Conference on Learning Representations, ICLR 2024, Vienna, Austria, May 7-11, 2024. OpenReview.net, 2024b. URL https://openreview.net/forum?id= Ep0TtjVoap. Yu Gu, Yiheng Shu, Hao Yu, Xiao Liu, Yuxiao Dong, Jie Tang, Jayanth Srinivasa, Hugo Latapie, and Yu Su. Middleware for llms: Tools are instrumental for language agents in complex environments. In Yaser Al-Onaizan, Mohit Bansal, and Yun-Nung Chen (eds.), Proceedings of the 2024 Conference on Empirical Methods in Natural Language Processing, EMNLP 2024, Miami, FL, USA, November 12-16, 2024, pp. 76467663. Association for Computational Linguistics, 2024. doi: 10.18653/V1/2024.EMNLP-MAIN.436. URL https://doi.org/10.18653/ v1/2024.emnlp-main.436. Shibo Hao, Tianyang Liu, Zhen Wang, and Zhiting Hu. Toolkengpt: Augmenting frozen In Alice Oh, Tristan Naulanguage models with massive tools via tool embeddings. mann, Amir Globerson, Kate Saenko, Moritz Hardt, and Sergey Levine (eds.), Advances in Neural Information Processing Systems 36: Annual Conference on Neural Information Processing Systems 2023, NeurIPS 2023, New Orleans, LA, USA, December 10 - 16, 2023, 2023. URL http://papers.nips.cc/paper_files/paper/2023/hash/ 8fd1a81c882cd45f64958da6284f4a3f-Abstract-Conference.html. Joy He-Yueya, Gabriel Poesia, Rose E. Wang, and Noah D. Goodman. Solving math word problems by combining language models with symbolic solvers. CoRR, abs/2304.09102, 2023. doi: 10. 48550/ARXIV.2304.09102. URL https://doi.org/10.48550/arXiv.2304.09102. 11 Aaron Hurst, Adam Lerer, Adam P. Goucher, Adam Perelman, Aditya Ramesh, Aidan Clark, AJ Ostrow, Akila Welihinda, Alan Hayes, Alec Radford, Aleksander Madry, Alex Baker-Whitcomb, Alex Beutel, Alex Borzunov, Alex Carney, Alex Chow, Alex Kirillov, Alex Nichol, Alex Paino, Alex Renzin, Alex Tachard Passos, Alexander Kirillov, Alexi Christakis, Alexis Conneau, Ali Kamali, Allan Jabri, Allison Moyer, Allison Tam, Amadou Crookes, Amin Tootoonchian, Ananya Kumar, Andrea Vallone, Andrej Karpathy, Andrew Braunstein, Andrew Cann, Andrew Codispoti, Andrew Galu, Andrew Kondrich, Andrew Tulloch, Andrey Mishchenko, Angela Baek, Angela Jiang, Antoine Pelisse, Antonia Woodford, Anuj Gosalia, Arka Dhar, Ashley Pantuliano, Avi Nayak, Avital Oliver, Barret Zoph, Behrooz Ghorbani, Ben Leimberger, Ben Rossen, Ben Sokolowsky, Ben Wang, Benjamin Zweig, Beth Hoover, Blake Samic, Bob McGrew, Bobby Spero, Bogo Giertler, Bowen Cheng, Brad Lightcap, Brandon Walkin, Brendan Quinn, Brian Guarraci, Brian Hsu, Bright Kellogg, Brydon Eastman, Camillo Lugaresi, Carroll L. Wainwright, Cary Bassin, Cary Hudson, Casey Chu, Chad Nelson, Chak Li, Chan Jun Shern, Channing Conger, Charlotte Barette, Chelsea Voss, Chen Ding, Cheng Lu, Chong Zhang, Chris Beaumont, Chris Hallacy, Chris Koch, Christian Gibson, Christina Kim, Christine Choi, Christine McLeavey, Christopher Hesse, Claudia Fischer, Clemens Winter, Coley Czarnecki, Colin Jarvis, Colin Wei, Constantin Koumouzelis, and Dane Sherburn. Gpt-4o system card. CoRR, abs/2410.21276, 2024. doi: 10.48550/ARXIV.2410.21276. URL https://doi.org/10.48550/arXiv.2410. 21276. Ayush Jain, Andrew Szot, and Joseph J. Lim. Generalization to new actions in reinforcement learning. In Proceedings of the 37th International Conference on Machine Learning, ICML 2020, 1318 July 2020, Virtual Event, volume 119 of Proceedings of Machine Learning Research, pp. 4661 4672. PMLR, 2020. URL http://proceedings.mlr.press/v119/jain20b.html. Ayush Jain, Norio Kosaka, Kyung-Min Kim, and Joseph J. Lim. Know your action set: Learning action relations for reinforcement learning. In The Tenth International Conference on Learning Representations, ICLR 2022, Virtual Event, April 25-29, 2022. OpenReview.net, 2022. URL https://openreview.net/forum?id=MljXVdp4A3N. Qiao Jin, Yifan Yang, Qingyu Chen, and Zhiyong Lu. Genegpt: augmenting large language models with domain tools for improved access to biomedical information. Bioinform., 40(2), 2024. doi: 10.1093/BIOINFORMATICS/BTAE075. URL https://doi.org/10.1093/ bioinformatics/btae075. Yizhao Jin, Greg G. Slabaugh, and Simon M. Lucas. ADAPTER-RL: adaptation of any agent using reinforcement learning. CoRR, abs/2311.11537, 2023. doi: 10.48550/ARXIV.2311.11537. URL https://doi.org/10.48550/arXiv.2311.11537. Ehud Karpas, Omri Abend, Yonatan Belinkov, Barak Lenz, Opher Lieber, Nir Ratner, Yoav Shoham, Hofit Bata, Yoav Levine, Kevin Leyton-Brown, Dor Muhlgay, Noam Rozen, Erez Schwartz, Gal Shachaf, Shai Shalev-Shwartz, Amnon Shashua, and Moshe Tennenholtz. MRKL systems: modular, neuro-symbolic architecture that combines large language models, external knowledge sources and discrete reasoning. CoRR, abs/2205.00445, 2022. doi: 10.48550/ARXIV.2205. 00445. URL https://doi.org/10.48550/arXiv.2205.00445. Khimya Khetarpal, Matthew Riemer, Irina Rish, and Doina Precup. Towards continual reinforcement learning: review and perspectives. Journal of Artificial Intelligence Research, 75:1401 1476, 2022. Woosuk Kwon, Zhuohan Li, Siyuan Zhuang, Ying Sheng, Lianmin Zheng, Cody Hao Yu, Joseph Gonzalez, Hao Zhang, and Ion Stoica. Efficient memory management for large language model serving with pagedattention. In Jason Flinn, Margo I. Seltzer, Peter Druschel, Antoine Kaufmann, and Jonathan Mace (eds.), Proceedings of the 29th Symposium on Operating Systems Principles, SOSP 2023, Koblenz, Germany, October 23-26, 2023, pp. 611626. ACM, 2023. doi: 10.1145/ 3600006.3613165. URL https://doi.org/10.1145/3600006.3613165. Nayoung Lee, Kartik Sreenivasan, Jason Lee, Kangwook Lee, and DImitris Papailiopoulos. Teaching arithmetic to small transformers. In International Conference on Learning Representations. ICLR 2024, 2024. 12 Minghao Li, Yingxiu Zhao, Bowen Yu, Feifan Song, Hangyu Li, Haiyang Yu, Zhoujun Li, Fei Huang, and Yongbin Li. Api-bank: comprehensive benchmark for tool-augmented llms. In Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing, pp. 31023116, 2023. Jacky Liang, Wenlong Huang, Fei Xia, Peng Xu, Karol Hausman, Brian Ichter, Pete Florence, and Andy Zeng. Code as policies: Language model programs for embodied control. In IEEE International Conference on Robotics and Automation, ICRA 2023, London, UK, May 29 - June 2, 2023, pp. 94939500. IEEE, 2023. doi: 10.1109/ICRA48891.2023.10160591. URL https: //doi.org/10.1109/ICRA48891.2023.10160591. Zhaoyang Liu, Yinan He, Wenhai Wang, Weiyun Wang, Yi Wang, Shoufa Chen, Qing-Long Zhang, Yang Yang, Qingyun Li, Jiashuo Yu, Kunchang Li, Zhe Chen, Xuecheng Yang, Xizhou Zhu, Yali Wang, Limin Wang, Ping Luo, Jifeng Dai, and Yu Qiao. Interngpt: Solving vision-centric tasks by interacting with chatbots beyond language. ArXiv, abs/2305.05662, 2023. URL https: //api.semanticscholar.org/CorpusID:258564914. Pan Lu, Bowen Chen, Sheng Liu, Rahul Thapa, Joseph Boen, and James Zou. Octotools: An agentic framework with extensible tools for complex reasoning. CoRR, abs/2502.11271, 2025. doi: 10. 48550/ARXIV.2502.11271. URL https://doi.org/10.48550/arXiv.2502.11271. Ne Luo, Aryo Pradipta Gema, Xuanli He, Emile Van Krieken, Pietro Lesci, and Pasquale Minervini. Self-training large language models for tool-use without demonstrations. In Findings of the Association for Computational Linguistics: NAACL 2025, pp. 12531271, 2025. Daniel Mankowitz, Andrea Michi, Anton Zhernov, Marco Gelmi, Marco Selvi, Cosmin Paduraru, Edouard Leurent, Shariq Iqbal, Jean-Baptiste Lespiau, Alex Ahern, et al. Faster sorting algorithms discovered using deep reinforcement learning. Nature, 618(7964):257263, 2023. Amogh Mannekote, Adam Davies, Jina Kang, and Kristy Elizabeth Boyer. Can llms reliably simulate human learner actions? simulation authoring framework for open-ended learning environments. In Toby Walsh, Julie Shah, and Zico Kolter (eds.), AAAI-25, Sponsored by the Association for the Advancement of Artificial Intelligence, February 25 - March 4, 2025, Philadelphia, PA, USA, pp. 2904429052. AAAI Press, 2025. doi: 10.1609/AAAI.V39I28.35175. URL https://doi.org/10.1609/aaai.v39i28.35175. Aaron Parisi, Yao Zhao, and Noah Fiedel. TALM: tool augmented language models. CoRR, abs/2205.12255, 2022. doi: 10.48550/ARXIV.2205.12255. URL https://doi.org/10. 48550/arXiv.2205.12255. Yujia Qin, Shihao Liang, Yining Ye, Kunlun Zhu, Lan Yan, Yaxi Lu, Yankai Lin, Xin Cong, Xiangru Tang, Bill Qian, Sihan Zhao, Lauren Hong, Runchu Tian, Ruobing Xie, Jie Zhou, Mark Gerstein, Dahai Li, Zhiyuan Liu, and Maosong Sun. Toolllm: Facilitating large language modIn The Twelfth International Conference on Learning els to master 16000+ real-world apis. Representations, ICLR 2024, Vienna, Austria, May 7-11, 2024. OpenReview.net, 2024. URL https://openreview.net/forum?id=dHng2O0Jjr. Yujia Qin, Shengding Hu, Yankai Lin, Weize Chen, Ning Ding, Ganqu Cui, Zheni Zeng, Xuanhe Zhou, Yufei Huang, Chaojun Xiao, Chi Han, Yi R. Fung, Yusheng Su, Huadong Wang, Cheng Qian, Runchu Tian, Kunlun Zhu, Shihao Liang, Xingyu Shen, Bokai Xu, Zhen Zhang, Yining Ye, Bowen Li, Ziwei Tang, Jing Yi, Yuzhang Zhu, Zhenning Dai, Lan Yan, Xin Cong, Yaxi Lu, Weilin Zhao, Yuxiang Huang, Junxi Yan, Xu Han, Xian Sun, Dahai Li, Jason Phang, Cheng Yang, Tongshuang Wu, Heng Ji, Guoliang Li, Zhiyuan Liu, and Maosong Sun. Tool learning with foundation models. ACM Comput. Surv., 57(4):101:1101:40, 2025. doi: 10.1145/3704435. URL https://doi.org/10.1145/3704435. James Queeney, Xiaoyi Cai, Mouhacine Benosman, and Jonathan P. How. GRAM: generalization in deep RL with robust adaptation module. CoRR, abs/2412.04323, 2024. doi: 10.48550/ARXIV. 2412.04323. URL https://doi.org/10.48550/arXiv.2412.04323. 13 Timo Schick, Jane Dwivedi-Yu, Roberto Dess`ı, Roberta Raileanu, Maria Lomeli, Eric HamToolformer: Lanbro, Luke Zettlemoyer, Nicola Cancedda, and Thomas Scialom. guage models can teach themselves to use tools. In Alice Oh, Tristan Naumann, Amir Globerson, Kate Saenko, Moritz Hardt, and Sergey Levine (eds.), Advances in Neural Information Processing Systems 36: Annual Conference on Neural Information Processing Systems 2023, NeurIPS 2023, New Orleans, LA, USA, December 10 - 16, 2023, 2023. URL http://papers.nips.cc/paper_files/paper/2023/hash/ d842425e4bf79ba039352da0f658a906-Abstract-Conference.html. Zhihong Shao, Peiyi Wang, Qihao Zhu, Runxin Xu, Junxiao Song, Mingchuan Zhang, Y. K. Li, Y. Wu, and Daya Guo. Deepseekmath: Pushing the limits of mathematical reasoning in open language models. CoRR, abs/2402.03300, 2024. doi: 10.48550/ARXIV.2402.03300. URL https://doi.org/10.48550/arXiv.2402.03300. Guangming Sheng, Chi Zhang, Zilingfeng Ye, Xibin Wu, Wang Zhang, Ru Zhang, Yanghua Peng, In ProHaibin Lin, and Chuan Wu. Hybridflow: flexible and efficient RLHF framework. ceedings of the Twentieth European Conference on Computer Systems, EuroSys 2025, Rotterdam, The Netherlands, 30 March 2025 - 3 April 2025, pp. 12791297. ACM, 2025. doi: 10.1145/3689031.3696075. URL https://doi.org/10.1145/3689031.3696075. language agents with verbal reinforcement learning. Noah Shinn, Federico Cassano, Ashwin Gopinath, Karthik Narasimhan, and Shunyu Yao. Reflexion: In Alice Oh, Tristan Naumann, Amir Globerson, Kate Saenko, Moritz Hardt, and Sergey Levine (eds.), Advances in Neural Information Processing Systems 36: Annual Conference on Neural Information Processing Systems 2023, NeurIPS 2023, New Orleans, LA, USA, December 10 - 16, 2023, 2023. URL http://papers.nips.cc/paper_files/paper/2023/hash/ 1b44b878bb782e6954cd888628510e90-Abstract-Conference.html. Joykirat Singh, Raghav Magazine, Yash Pandya, and Akshay Nambi. Agentic reasoning and tool integration for llms via reinforcement learning. arXiv preprint arXiv:2505.01441, 2025. Dıdac Surıs, Sachit Menon, and Carl Vondrick. Vipergpt: Visual inference via python execution for reasoning. In IEEE/CVF International Conference on Computer Vision, ICCV 2023, Paris, France, October 1-6, 2023, pp. 1185411864. IEEE, 2023. doi: 10.1109/ICCV51070.2023. 01092. URL https://doi.org/10.1109/ICCV51070.2023.01092. Andrew Szot, Bogdan Mazoure, Omar Attia, Aleksei Timofeev, Harsh Agrawal, R. Devon From multimodal llms to generalist Hjelm, Zhe Gan, Zsolt Kira, and Alexander Toshev. In IEEE/CVF Conference on Computer Vision embodied agents: Methods and lessons. and Pattern Recognition, CVPR 2025, Nashville, TN, USA, June 11-15, 2025, pp. 10644 10655. Computer Vision Foundation / IEEE, 2025. doi: 10.1109/CVPR52734.2025.00995. https://openaccess.thecvf.com/content/CVPR2025/html/Szot_ URL From_Multimodal_LLMs_to_Generalist_Embodied_Agents_Methods_and_ Lessons_CVPR_2025_paper.html. Zhengyang Tang, Xingxing Zhang, Benyou Wang, and Furu Wei. Mathscale: Scaling instruction tuning for mathematical reasoning. In Forty-first International Conference on Machine Learning, ICML 2024, Vienna, Austria, July 21-27, 2024. OpenReview.net, 2024. URL https: //openreview.net/forum?id=Kjww7ZN47M. Adrian Theuma and Ehsan Shareghi. Equipping language models with tool use capability for tabular data analysis in finance. In Yvette Graham and Matthew Purver (eds.), Proceedings of the 18th Conference of the European Chapter of the Association for Computational Linguistics, EACL 2024 - Volume 2: Short Papers, St. Julians, Malta, March 17-22, 2024, pp. 90103. Association for Computational Linguistics, 2024. URL https://aclanthology.org/2024. eacl-short.10. Bohan Wang, Zhongqi Yue, Fengda Zhang, Shuo Chen, Lian Bi, Junzhe Zhang, Xue Song, Kennard Yanting Chan, Jiachun Pan, Weijia Wu, et al. Selftok: Discrete visual tokens of autoregression, by diffusion, and for reasoning. arXiv preprint arXiv:2505.07538, 2025a. 14 Guanzhi Wang, Yuqi Xie, Yunfan Jiang, Ajay Mandlekar, Chaowei Xiao, Yuke Zhu, Linxi Fan, and Anima Anandkumar. Voyager: An open-ended embodied agent with large language models. Trans. Mach. Learn. Res., 2024, 2024. URL https://openreview.net/forum?id= ehfRiF0R3a. Jingwei Wang, Zai Zhang, Hao Qian, Chunjing Gan, Binbin Hu, Ziqi Liu, Zhiqiang Zhang, Jun Zhou, Bin Shi, and Bo Dong. Enhancing LLM tool use with high-quality instruction data from knowledge graph. CoRR, abs/2506.21071, 2025b. doi: 10.48550/ARXIV.2506.21071. URL https://doi.org/10.48550/arXiv.2506.21071. Qian Wang, Jiaying Wu, Zhenheng Tang, Bingqiao Luo, Nuo Chen, Wei Chen, and Bingsheng He. What limits llm-based human simulation: Llms or our design? CoRR, abs/2501.08579, 2025c. doi: 10.48550/ARXIV.2501.08579. URL https://doi.org/10.48550/arXiv.2501. 08579. Jiannan Xiang, Tianhua Tao, Yi Gu, Tianmin Shu, Zirui Wang, Zichao Yang, and Zhiting Hu. Language models meet world models: Embodied experiences enhance language models. Advances in neural information processing systems, 36:7539275412, 2023. An Yang, Baosong Yang, Beichen Zhang, Binyuan Hui, Bo Zheng, Bowen Yu, Chengyuan Li, Dayiheng Liu, Fei Huang, Haoran Wei, Huan Lin, Jian Yang, Jianhong Tu, Jianwei Zhang, Jianxin Yang, Jiaxi Yang, Jingren Zhou, Junyang Lin, Kai Dang, Keming Lu, Keqin Bao, Kexin Yang, Le Yu, Mei Li, Mingfeng Xue, Pei Zhang, Qin Zhu, Rui Men, Runji Lin, Tianhao Li, Tingyu Xia, Xingzhang Ren, Xuancheng Ren, Yang Fan, Yang Su, Yichang Zhang, Yu Wan, Yuqiong Liu, Zeyu Cui, Zhenru Zhang, and Zihan Qiu. Qwen2.5 technical report. CoRR, abs/2412.15115, 2024. doi: 10.48550/ARXIV.2412.15115. URL https://doi.org/10.48550/arXiv. 2412.15115. Shunyu Yao, Jeffrey Zhao, Dian Yu, Nan Du, Izhak Shafran, Karthik R. Narasimhan, and Yuan Cao. React: Synergizing reasoning and acting in language models. In The Eleventh International Conference on Learning Representations, ICLR 2023, Kigali, Rwanda, May 1-5, 2023. OpenReview.net, 2023. URL https://openreview.net/forum?id=WE_vluYUL-X. Weihao Zeng, Yuzhen Huang, Qian Liu, Wei Liu, Keqing He, Zejun Ma, and Junxian He. Simplerlzoo: Investigating and taming zero reinforcement learning for open base models in the wild. arXiv preprint arXiv:2503.18892, 2025. Beichen Zhang, Kun Zhou, Xilin Wei, Xin Zhao, Jing Sha, Shijin Wang, and Ji-Rong Wen. In Alice Evaluating and improving tool-augmented computation-intensive math reasoning. Oh, Tristan Naumann, Amir Globerson, Kate Saenko, Moritz Hardt, and Sergey Levine (eds.), Advances in Neural Information Processing Systems 36: Annual Conference on Neural Information Processing Systems 2023, NeurIPS 2023, New Orleans, LA, USA, December 10 - 16, 2023, 2023. URL http://papers.nips.cc/paper_files/paper/2023/ hash/4a47dd69242d5af908cdd5d51c971cbf-Abstract-Datasets_and_ Benchmarks.html. Wanpeng Zhang and Zongqing Lu. Rladapter: Bridging large language models to reinforcement learning in open worlds. CoRR, abs/2309.17176, 2023. doi: 10.48550/ARXIV.2309.17176. URL https://doi.org/10.48550/arXiv.2309.17176. Haiteng Zhao, Chang Ma, Guoyin Wang, Jing Su, Lingpeng Kong, Jingjing Xu, Zhi-Hong Deng, and Hongxia Yang. Empowering large language model agents through action learning. CoRR, abs/2402.15809, 2024. doi: 10.48550/ARXIV.2402.15809. URL https://doi.org/10. 48550/arXiv.2402.15809."
        },
        {
            "title": "CONTENTS OF APPENDIX",
            "content": "A Limitations and Future Discussions Usage of AI Assistant Policy on expanded action space Update Rule Dataset Details Baseline Details Implementation Details G.1 Calc-Bench Implementation Details . . . . . . . . . . . . . . . . . . . . . . . . . G.2 Sorting Implementation Details . . . . . . . . . . . . . . . . . . . . . . . . . . . . G.3 Prompts . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . Additional Experimental Results H.1 Calc-Bench . H.2 Sorting . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . Additional Analysis I. Planning Phrases Analysis . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . I.2 Action Initialization Analysis . . . . . . . . . . . . . . . . . . . . . . . . . . . . . Case Study 17 17 17 18 18 19 22 25 27 27 28 29 29"
        },
        {
            "title": "A LIMITATIONS AND FUTURE DISCUSSIONS",
            "content": "The primary objective of this study is to develop practical method for extending the action space of LLMs, thereby enhancing their reasoning capabilities beyond the intrinsic language knowledge spaces. Our proposed approach, EARL, proves effective in this regard. However, we identify three key limitations. First, due to computational constraints, our empirical evaluation is limited to the Qwen-2.5 model family (up to 7B parameters), and its scaling properties remain to be discovered. Second, we did not investigate optimal action space initialization techniques for novel actions within the continuous learning paradigm. Third, the frameworks performance in complex external environments involving multimodality and diverse action spaces is underexplored. Addressing these limitations to develop more robust and generalizable solution will be the focus of our future research endeavors."
        },
        {
            "title": "B USAGE OF AI ASSISTANT",
            "content": "We use AI assistants or tools such as ChatGPT and Grammarly to correct grammar errors and polish the language."
        },
        {
            "title": "C POLICY ON EXPANDED ACTION SPACE",
            "content": "Our parameterization of the πθ uses linear-softmax layer applied to an encoding g(ht) Rd of the history (context) ht by the LLM. The layer uses weight matrix = [w1, ..., wN ], where = is the total number of actions across all environments, including the language one, and wa Rd are the parameters used to compute the logit of action for [N ]. The softmax is restricted to actions that are available in the active environment et. We define, for environments {0, 1, ..., K}, σe(z)a = eza (cid:80) Ee eza 1[a Ee] . Here, E0 = {ge}e[K] E0 and Ee = Ee for = 1, ..., K. With this, πθ( ht, et) = σet(W g(ht)) ."
        },
        {
            "title": "D UPDATE RULE",
            "content": "The update rule is given by (τ, a; θ) = 1 (cid:88) (cid:104) t=0 where min(cid:0)rt(θ) a, clip(rt(θ), 1 ϵ, 1 + ϵ) a(cid:1) β KL(cid:0)πθ( ht) πref( ht)(cid:1) (cid:1)(cid:105) , (2) τ = {(ht, at, rt)}T 1 t=0 is rollout trajectory, ht denotes the token history (state) at step t, rt(θ) = πθ(atht) ϵ is the PPO clipping threshold, β is the KL regularization coefficient, πθ( ht) and πref( ht) denote the current and reference policies, respectively. We use the πold(atht) is the importance sampling ratio between the new and old policies, pre-trained LLM as the reference model. Note that we slightly abuse notation in the main paper by saying τ is generated with πθ. In practice, rollouts are generated by the reference policy πold, which is held fixed during data collection. The objective in Equation (2) then compares the likelihood of these sampled actions under the current policy πθ versus the reference policy πold, with the ratio rt(θ) providing the necessary importance weighting. This distinction ensures stable on-policy learning: trajectories are collected with πold, while updates adjust πθ to maximize advantage without diverging too far from πold. 17 In training, we do not perform update on positions in each rollout corresponding to observations returned by external environments. For EARL, we apply the KL loss with token probabilities computed over the original vocabulary space V."
        },
        {
            "title": "E DATASET DETAILS",
            "content": "The Calc-Bench benchmark consists of four sub-datasets targeting Calc-Bench Dataset Details. different types of mathematical reasoning: Arithmetic, Countdown, Count, and rewritten subset of GSM8K, denoted GSM8K. Each dataset is generated via task-specific scripts, with explicit control over complexity, number format, and structure. Arithmetic: This dataset contains randomly generated arithmetic expressions involving up to 5digit integers and between 2 and 6 operations. The training set includes 1,000 samples with maximum of 4 operations and 10% of examples paraphrased into natural language. The test set contains 2,000 samples with up to 6 operations and 70% paraphrased into natural language. Countdown: Each instance is generated by sampling valid arithmetic expression, extracting all numerical values, shuffling them, and using the original result as the target. The final dataset includes 20,000 training and 2,000 test examples. Expressions contain up to 4-digit numbers and allow up to 3 operations. Each number must appear exactly once in the constructed expression. GSM8K: This subset is derived from manually rewritten version of GSM8K, filtered to include only examples explicitly marked as rewritten. Each sample includes paraphrased question and corresponding reasoning-based answer. This version preserves the complexity of GSM8K while reducing lexical overlap with the original dataset. Count: This dataset consists of symbol sequences with maximum length of 20. Each example is labeled with count-based target (e.g., the number of specific items). The training set includes 1,000 examples and the test set includes 2,000. Sorting Dataset Details. We adopt curriculum-based training strategy for sorting tasks. Specifically, we first use an ordering dataset to pre-train the model on simpler relational tasks, followed by sorting dataset that introduces the full sorting items. Each dataset is constructed with its own generation procedure and input length distribution, as detailed below. Ordering: total of 20,000 training examples are generated, with 95% assigned to the order task and 5% to compare. Each input sequence contains 2 to 5 items, with length distribution of 30% for 2 items, 30% for 3 items, 20% for 4 items, and 20% for 5 items. The corresponding test set contains 2,000 order examples, with the same item length distribution. Sorting: The training set contains 80,000 examples, with input lengths ranging from 2 to 5 elements, denoted as Sort-2 through Sort-5. The distribution is 10% Sort-2, 20% Sort-3, 30% Sort-4, and 40% Sort-5. The test set includes 2,000 examples, equally split between Sort-4 and Sort-5 cases."
        },
        {
            "title": "F BASELINE DETAILS",
            "content": "In our experiments, we employ open-source LLM Qwen2.5 (Yang et al., 2024) as the backbone, which has been designed to address wide range of applications, including coding and mathematics. We select both base and instruction-tuned variants with model sizes of 0.5B, 3B, and 7B parameters for our experiments. Here we provide details of each baseline methods as follows: GPT-4o (Hurst et al., 2024) is frontier large language model developed by OpenAI, demonstrating advanced capabilities in reasoning with various contexts. We use standard prompt instruction as the key to generating response. Prompt-based RL is cost-effective and effective method for LLMs, specifically designed to train them by optimizing their responses to given prompts. We employ the cutting-edge approach Group Relative Policy Optimization (GRPO) (Shao et al., 2024) for our SFT+GRPO and Prompt18 GRPO (DeepSeek-AI et al., 2025) baselines. We replace GRPO with our proposed CPO as PromptCPO baseline. SFT Data Curation. We follow Tang et al. (2024), leveraging frontier LLM GPT-4o to generate solutions on the Countdown task. In order to obtain high-quality SFT data, we only select solutions that GPT-4o can correctly answer with no more than 1K tokens."
        },
        {
            "title": "G IMPLEMENTATION DETAILS",
            "content": "Our implementation of EARL efficiently supports ExpA rollouts through customized vLLM backend (Kwon et al., 2023) and integration with the VeRL training library (Sheng et al., 2025). We follow the configurations outlined in VeRL. To ensure the reproducibility of our findings, detailed implementation instructions are provided below. G.1 CALC-BENCH IMPLEMENTATION DETAILS Prompt+GRPO Training Config <Qwen-2.5-3B-Instruct> apptainer exec --nv envs/verl.sif bash -c \" ray start --head --port=6383 && set -x && python3 -m verl.trainer.main_earl actor_rollout_ref.earl.model.freeze_base_model=False actor_rollout_ref.earl.model.init_from_base=True actor_rollout_ref.earl.training.tools=[calculator] algorithm.adv_estimator=grpo tool_config_path=tool_configs/combined_calculator_baseline.yaml data.train_files=./data/calc_bench/combined_baseline/train.parquet data.val_files=./data/calc_bench/combined_baseline/test.parquet data.train_batch_size=256 data.max_prompt_length=384 data.max_response_length=1024 data.filter_overlong_prompts=True data.truncation=error actor_rollout_ref.model.path=Qwen/Qwen2.5-3B-Instruct actor_rollout_ref.actor.optim.lr=1e-6 actor_rollout_ref.model.use_remove_padding=True actor_rollout_ref.actor.ppo_mini_batch_size=32 actor_rollout_ref.actor.ppo_micro_batch_size_per_gpu=8 actor_rollout_ref.actor.use_kl_loss=True actor_rollout_ref.actor.kl_loss_coef=0.001 actor_rollout_ref.actor.kl_loss_type=low_var_kl actor_rollout_ref.actor.entropy_coeff=0 actor_rollout_ref.model.enable_gradient_checkpointing=True actor_rollout_ref.actor.entropy_from_logits_with_chunking=True actor_rollout_ref.actor.fsdp_config.param_offload=False actor_rollout_ref.actor.fsdp_config.optimizer_offload=False actor_rollout_ref.rollout.log_prob_micro_batch_size_per_gpu=8 actor_rollout_ref.rollout.tensor_model_parallel_size=1 actor_rollout_ref.rollout.name=vllm actor_rollout_ref.rollout.gpu_memory_utilization=0.6 actor_rollout_ref.rollout.n=8 actor_rollout_ref.ref.log_prob_micro_batch_size_per_gpu=8 actor_rollout_ref.ref.fsdp_config.param_offload=True algorithm.use_kl_in_reward=False trainer.critic_warmup=0 trainer.logger=[console,tensorboard] trainer.project_name=earl trainer.experiment_name=calc_bench/qwen2.5-3b/prompt-grpo trainer.validation_data_dir=./val_result/calc_bench/3b/prompt-grpo trainer.n_gpus_per_node=4 trainer.nnodes=1 trainer.save_freq=25 trainer.test_freq=25 trainer.total_epochs=4 apptainer exec --nv envs/verl.sif bash -c \" EARL Training Config <Qwen-2.5-3B-Instruct> ray start --head --port=6383 && set -x && python3 -m verl.trainer.main_earl actor_rollout_ref.earl.model.freeze_base_model=False actor_rollout_ref.earl.model.init_from_base=True actor_rollout_ref.earl.training.tools=[calculator] algorithm.adv_estimator=trpo tool_config_path=tool_configs/combined_calculator.yaml data.train_files=./data/calc_bench/combined_earl/train.parquet data.val_files=./data/calc_bench/combined_earl/test.parquet data.train_batch_size=256 data.max_prompt_length=384 data.max_response_length=1024 data.filter_overlong_prompts=True data.truncation=error actor_rollout_ref.model.path=Qwen/Qwen2.5-3B-Instruct actor_rollout_ref.actor.optim.lr=1e-6 actor_rollout_ref.model.use_remove_padding=True actor_rollout_ref.actor.ppo_mini_batch_size=32 actor_rollout_ref.actor.ppo_micro_batch_size_per_gpu=8 actor_rollout_ref.actor.use_kl_loss=True actor_rollout_ref.actor.kl_loss_coef=0.001 actor_rollout_ref.actor.kl_loss_type=low_var_kl actor_rollout_ref.actor.entropy_coeff=0 actor_rollout_ref.model.enable_gradient_checkpointing=True actor_rollout_ref.actor.entropy_from_logits_with_chunking=True actor_rollout_ref.actor.fsdp_config.param_offload=False actor_rollout_ref.actor.fsdp_config.optimizer_offload=False actor_rollout_ref.rollout.max_num_batched_tokens=7200 actor_rollout_ref.rollout.log_prob_micro_batch_size_per_gpu=8 actor_rollout_ref.rollout.tensor_model_parallel_size=1 actor_rollout_ref.rollout.name=vllm actor_rollout_ref.rollout.gpu_memory_utilization=0.6 actor_rollout_ref.rollout.n=8 actor_rollout_ref.ref.log_prob_micro_batch_size_per_gpu=8 actor_rollout_ref.ref.fsdp_config.param_offload=True algorithm.use_kl_in_reward=False trainer.critic_warmup=0 trainer.logger=[console,tensorboard] trainer.project_name=earl trainer.experiment_name=calc_bench/qwen2.5-3b/earl-trpo trainer.validation_data_dir=./val_result/calc_bench/3b/earl trainer.n_gpus_per_node=4 trainer.nnodes=1 trainer.save_freq=25 trainer.test_freq=25 trainer.total_epochs=4 Prompt+GRPO Training Config <Qwen-2.5-7B-Instruct> apptainer exec --nv envs/verl.sif bash -c \" ray start --head --port=6384 && set -x && python3 -m verl.trainer.main_earl actor_rollout_ref.earl.model.freeze_base_model=False actor_rollout_ref.earl.model.init_from_base=True actor_rollout_ref.earl.training.tools=[calculator] algorithm.adv_estimator=grpo 20 tool_config_path=tool_configs/combined_calculator_baseline.yaml data.train_files=./data/calc_bench/combined_baseline/train.parquet data.val_files=./data/calc_bench/combined_baseline/test.parquet data.train_batch_size=256 data.max_prompt_length=384 data.max_response_length=1024 data.filter_overlong_prompts=True data.truncation=error actor_rollout_ref.model.path=Qwen/Qwen2.5-7B-Instruct actor_rollout_ref.actor.optim.lr=1e-6 actor_rollout_ref.model.use_remove_padding=True actor_rollout_ref.actor.ppo_mini_batch_size=32 actor_rollout_ref.actor.ppo_micro_batch_size_per_gpu=8 actor_rollout_ref.actor.use_kl_loss=True actor_rollout_ref.actor.kl_loss_coef=0.001 actor_rollout_ref.actor.kl_loss_type=low_var_kl actor_rollout_ref.actor.entropy_coeff=0 actor_rollout_ref.model.enable_gradient_checkpointing=True actor_rollout_ref.actor.entropy_checkpointing=True actor_rollout_ref.actor.entropy_from_logits_with_chunking=True actor_rollout_ref.actor.fsdp_config.param_offload=False actor_rollout_ref.actor.fsdp_config.optimizer_offload=False actor_rollout_ref.rollout.log_prob_micro_batch_size_per_gpu=8 actor_rollout_ref.rollout.tensor_model_parallel_size=1 actor_rollout_ref.rollout.name=vllm actor_rollout_ref.rollout.gpu_memory_utilization=0.6 actor_rollout_ref.rollout.n=8 actor_rollout_ref.ref.log_prob_micro_batch_size_per_gpu=32 actor_rollout_ref.ref.fsdp_config.param_offload=True algorithm.use_kl_in_reward=False trainer.critic_warmup=0 trainer.logger=[console,tensorboard] trainer.project_name=earl trainer.experiment_name=calc_bench/qwen2.5-7b/prompt-grpo trainer.validation_data_dir=./val_result/calc_bench/7b/prompt-grpo trainer.n_gpus_per_node=4 trainer.nnodes=1 trainer.save_freq=100 trainer.test_freq=10 trainer.total_epochs=4 apptainer exec --nv envs/verl.sif bash -c \" EARL Training Config <Qwen-2.5-7B-Instruct> ray start --head --port=6384 && set -x && python3 -m verl.trainer.main_earl actor_rollout_ref.earl.model.freeze_base_model=False actor_rollout_ref.earl.model.init_from_base=True actor_rollout_ref.earl.training.tools=[calculator] algorithm.adv_estimator=trpo tool_config_path=tool_configs/combined_calculator.yaml data.train_files=./data/calc_bench/combined_earl/train.parquet data.val_files=./data/calc_bench/combined_earl/test.parquet data.train_batch_size=256 data.max_prompt_length=384 data.max_response_length=1024 data.filter_overlong_prompts=True data.truncation=error actor_rollout_ref.model.path=Qwen/Qwen2.5-7B-Instruct actor_rollout_ref.actor.optim.lr=1e-6 actor_rollout_ref.model.use_remove_padding=True actor_rollout_ref.actor.ppo_mini_batch_size=32 actor_rollout_ref.actor.ppo_micro_batch_size_per_gpu=8 actor_rollout_ref.actor.use_kl_loss=True 21 actor_rollout_ref.actor.kl_loss_coef=0.001 actor_rollout_ref.actor.kl_loss_type=low_var_kl actor_rollout_ref.actor.entropy_coeff=0 actor_rollout_ref.model.enable_gradient_checkpointing=True actor_rollout_ref.actor.entropy_checkpointing=True actor_rollout_ref.actor.entropy_from_logits_with_chunking=True actor_rollout_ref.actor.fsdp_config.param_offload=False actor_rollout_ref.actor.fsdp_config.optimizer_offload=False actor_rollout_ref.rollout.log_prob_micro_batch_size_per_gpu=8 actor_rollout_ref.rollout.tensor_model_parallel_size=1 actor_rollout_ref.rollout.name=vllm actor_rollout_ref.rollout.gpu_memory_utilization=0.6 actor_rollout_ref.rollout.n=8 actor_rollout_ref.ref.log_prob_micro_batch_size_per_gpu=32 actor_rollout_ref.ref.fsdp_config.param_offload=True algorithm.use_kl_in_reward=False trainer.critic_warmup=0 trainer.logger=[console,tensorboard] trainer.project_name=earl trainer.experiment_name=calc_bench/qwen2.5-7b/earl-trpo trainer.validation_data_dir=./val_result/calc_bench/7b/earl trainer.n_gpus_per_node=4 trainer.nnodes=1 trainer.save_freq=100 trainer.test_freq=10 trainer.total_epochs=4 G.2 SORTING IMPLEMENTATION DETAILS Prompt+GRPO Training Config <Qwen-2.5-7B-Instruct> apptainer exec --nv envs/verl.sif bash -c \" ray start --head --port=6384 && set -x && python3 -m verl.trainer.main_earl actor_rollout_ref.earl.model.freeze_base_model=False actor_rollout_ref.earl.model.init_from_base=True actor_rollout_ref.earl.training.tools=[swap,compare] algorithm.adv_estimator=grpo tool_config_path=tool_configs/sorting_baseline.yaml data.train_files=./data/sort_baseline_train/train.parquet data.val_files=./data/sort_earl_4_5/test.parquet data.train_batch_size=256 data.max_prompt_length=384 data.max_response_length=384 data.filter_overlong_prompts=True data.truncation=error actor_rollout_ref.model.path=Qwen/Qwen2.5-7B-Instruct actor_rollout_ref.actor.optim.lr=1e-6 actor_rollout_ref.model.use_remove_padding=True actor_rollout_ref.actor.ppo_mini_batch_size=32 actor_rollout_ref.actor.ppo_micro_batch_size_per_gpu=8 actor_rollout_ref.actor.use_kl_loss=True actor_rollout_ref.actor.kl_loss_coef=0.001 actor_rollout_ref.actor.kl_loss_type=low_var_kl actor_rollout_ref.actor.entropy_coeff=0 actor_rollout_ref.model.enable_gradient_checkpointing=True actor_rollout_ref.actor.entropy_checkpointing=True actor_rollout_ref.actor.entropy_from_logits_with_chunking=True actor_rollout_ref.actor.fsdp_config.param_offload=False actor_rollout_ref.actor.fsdp_config.optimizer_offload=False actor_rollout_ref.rollout.log_prob_micro_batch_size_per_gpu=8 actor_rollout_ref.rollout.tensor_model_parallel_size=1 actor_rollout_ref.rollout.name=vllm 22 actor_rollout_ref.rollout.gpu_memory_utilization=0.6 actor_rollout_ref.rollout.n=8 actor_rollout_ref.ref.log_prob_micro_batch_size_per_gpu=8 actor_rollout_ref.ref.fsdp_config.param_offload=True algorithm.use_kl_in_reward=False trainer.critic_warmup=0 trainer.logger=[console,tensorboard] trainer.project_name=earl trainer.experiment_name=sorting/qwen2.5-7b/prompt-grpo-sort trainer.validation_data_dir=./val_result/sorting/7b/prompt-grpo trainer.n_gpus_per_node=4 trainer.nnodes=1 trainer.save_freq=5000 trainer.test_freq=3 trainer.total_epochs=1 apptainer exec --nv envs/verl.sif bash -c \" EARL Training Config <Qwen-2.5-7B-Instruct> ray start --head --port=6384 && set -x && python3 -m verl.trainer.main_earl actor_rollout_ref.earl.model.freeze_base_model=False actor_rollout_ref.earl.model.init_from_base=True actor_rollout_ref.earl.training.tools=[swap,compare] algorithm.adv_estimator=trpo tool_config_path=tool_configs/sorting.yaml data.train_files=./data/sort_earl_train/train.parquet data.val_files=./data/sort_earl_4_5/test.parquet data.train_batch_size=256 data.max_prompt_length=384 data.max_response_length=384 data.filter_overlong_prompts=True data.truncation=error actor_rollout_ref.model.path=Qwen/Qwen2.5-7B-Instruct actor_rollout_ref.actor.optim.lr=1e-6 actor_rollout_ref.model.use_remove_padding=True actor_rollout_ref.actor.ppo_mini_batch_size=32 actor_rollout_ref.actor.ppo_micro_batch_size_per_gpu=8 actor_rollout_ref.actor.use_kl_loss=True actor_rollout_ref.actor.kl_loss_coef=0.001 actor_rollout_ref.actor.kl_loss_type=low_var_kl actor_rollout_ref.actor.entropy_coeff=0 actor_rollout_ref.model.enable_gradient_checkpointing=True actor_rollout_ref.actor.entropy_checkpointing=True actor_rollout_ref.actor.entropy_from_logits_with_chunking=True actor_rollout_ref.actor.fsdp_config.param_offload=False actor_rollout_ref.actor.fsdp_config.optimizer_offload=False actor_rollout_ref.rollout.log_prob_micro_batch_size_per_gpu=8 actor_rollout_ref.rollout.tensor_model_parallel_size=1 actor_rollout_ref.rollout.name=vllm actor_rollout_ref.rollout.gpu_memory_utilization=0.6 actor_rollout_ref.rollout.n=8 actor_rollout_ref.ref.log_prob_micro_batch_size_per_gpu=8 actor_rollout_ref.ref.fsdp_config.param_offload=True algorithm.use_kl_in_reward=False trainer.critic_warmup=0 trainer.logger=[console,tensorboard] trainer.project_name=earl trainer.experiment_name=sorting/qwen2.5-7b/earl-sort trainer.validation_data_dir=./val_result/sorting/7b/earl trainer.n_gpus_per_node=4 trainer.nnodes=1 trainer.save_freq=5000 trainer.test_freq=3 trainer.total_epochs=1 24 G.3 PROMPTS We highlight the environment prompt, question, and task in our instruction prompt. Prompt Example <Calc-Bench: Arithmetic> <system>: You are helpful assistant. You first thinks about the reasoning process in the mind and then provides the user with the answer. You are allowed to use calculator by wrapping the expression with <calculator> </calculator> tags. The calculator output will follow inside <result> </result> tags. For example, <calculator> (12 + 6) / 3 </calculator> will produce <result> 6 </result>. You can use calculator multiple times in your reasoning process. <user>: What is 7 + (4 * 366 + 32287 - 7471)? Output your answer after ####. <assistant>: Let me solve this step by step. Prompt Example <Calc-Bench: Countdown> <system>: You are helpful assistant. You first thinks about the reasoning process in the mind and then provides the user with the answer. You are allowed to use calculator with the calculate keyword. For example, you may calculate (12 + 6) / 3 = 6. You can use calculator multiple times in your reasoning process. <user>: Using the numbers [3697, 5655, 1199, 11], create an equation that equals 74587492. You can use basic arithmetic operations (+, -, *, /) one or multiple times but each number can only be used once. Output the final answer after ####. For example, given numbers [1, 2, 3] and target number 1, output #### (1 + 2) / 3. <assistant>: Let me solve this step by step. Prompt Example <Calc-Bench: Count> <system>: You are helpful assistant. You first thinks about the reasoning process in the mind and then provides the user with the answer. <user>: How many times does the digit 2 appear in the number eighty-three million, seven hundred and forty-five thousand and thirty-nine? Output your answer after ####. For example, given the number 121 and digit 1, output #### 2. <assistant>: Let me solve this step by step. 25 Prompt Example <Calc-Bench: GSM8K> <system>: You are helpful assistant. You first thinks about the reasoning process in the mind and then provides the user with the answer. <user>: Natalia sold clips to 40770 of her friends in April, and then she sold half as many clips in May. How many clips did Natalia sell altogether in April and May? Output your final answer after ####. <assistant>: Let me solve this step by step. Prompt Example <Sorting> <system>: You are helpful assistant. You first thinks about the reasoning process in the mind and then provides the user with the answer. You have access to the following tools: - compare tool: compare and B: > - swap tool: swap and => B, <user>: Sort the following items in descending order: A, B, C, D, E. While you do not know the values of the items, you can compare any two items using the compare tool. Once you know the order of them, you can use the swap tool multiple times to complete the task. For example, to sort A, in descending order, if you find < with the compare tool, you can use the swap tool on and to complete the task. Use fewest possible comparisons and swaps to complete the task. Stop when the sequence is sorted and do not output any answer. <assistant>: Let me solve this step by step. 26 Figure 7: Performance comparison on the Calc-Bech Arithmetic. Arithmetic Operation Out-ofDistribution (OOD) is shown as bars, and Arithmetic Digit OOD as lines. Both plots correspond to the lightweight model Qwen-2.5-0.5B-Instruct. Table 6: Statistics of the Arithmetic OOD setting. Language portions refer to the portion of questions where operations or numbers are written in natural language. Task Max number (10x) #Operands Lang. portion #Instances Train Test Train Test Train Operation OOD Digit OOD 4 3 4 4 3 5 6 5 5% NA Test 70% NA Train 20,000 20,000 Test 2,000 2,"
        },
        {
            "title": "H ADDITIONAL EXPERIMENTAL RESULTS",
            "content": "H.1 CALC-BENCH Evaluation on Lightweight Model. Experimental results in Figure 7 demonstrate that our EARL can effectively improve the Arithmetic task for lightweight model with only 0.5 billion parameters (Qwen-2.5-0.5B-Instruct). In this experiment, as described in Table 6, we consider two Out-ofDistribution (OOD) variants, including Operation OOD and Digit OOD. Due to the limitation of the models capability, we observe that Qwen-2.5-0.5B-Instruct has almost zero inherent knowledge to handle arithmetic tasks without fine-tuning. With enough training resources, SFT can achieve significant performance gain for both OOD settings. Notably, EARL can effectively benefit the lightweight model, achieving comparable performance on the Operation OOD task, and even better results on the Digit OOD task. Evaluation on Collective Tool Learning. In Figure 8, we compare EARL with three baseline methods, where the tool learning trajectory of EARL is represented in yellow. We observed that all the methods in Figure 8d show similar trend, demonstrating distinct learning behavior with respect to other tasks. One possible reason is that this task is relatively easy, which could be well addressed by the models inherent language knowledge spaces; introducing external action spaces wont enhance this task much. For the other three tasks, we observe that EARL reveals the efficiency and effectiveness of learning, indicating that all tasks are complementary to achieve enhanced and 27 (a) Arithmetic (b) Countdown (c) GSM8K (d) Count Figure 8: Performance of EARL (yellow) and other baselines on Calc-Bench, where we present the training steps on the X-axis and their EM score on the Y-axis. Table 7: Ablation EM results of various environment configurations. Method Configuration Calc-Bench Arithmetic Countdown Count GSM8K Overall EARL calculate <calculator></..> Prompt+GRPO Prompt+CPO <calculate> <calculate> <calculate> <calculate> 69.20 75.80 50.05 64.70 63.05 61.50 75.15 78.00 34.95 49. 60.15 38.30 93.70 92.00 82.25 94.75 88.70 91.35 48.53 51.12 27.46 30. 46.63 46.80 71.65 74.23 48.68 59.75 64.63 59.49 robust math reasoning with better trade-off. Our findings suggest that EARL is practical and scalable framework to expand action spaces. Additional Ablation Study. To validate the robustness of EARL on different environment configurations, we conduct comprehensive ablation study, shown in Table 7. All experimental results and findings validate that our proposed EARL achieves the best robustness, indicating that EARL is an effective framework with the potential to handle diverse external environments. Furthermore, by eliminating the need for environment-specific configurations, EARL develops generalized understanding of tool interactions that is more naturally aligned with language. H.2 SORTING Figure 9 visualizes the deterministic decision tree induced by EARL on Sort-4 inputs under greedy decoding. Each internal node represents binary comparison, and each leaf corresponds to sequence of swaps for producing sorted output. Red nodes indicate redundant comparisons that do not affect the final decision and can be pruned to yield the simplified variant EARL described in the main text. 28 Figure 9: Decision tree induced by EARL on Sort-4. Red nodes indicate redundant comparisons that can be pruned to obtain EARL."
        },
        {
            "title": "I ADDITIONAL ANALYSIS",
            "content": "I.1 PLANNING PHRASES ANALYSIS We have conducted an in-depth analysis of the contingent learning process on the Countdown task, where the intermediate response depends on the preceding actions and outcomes. The findings are summarized in Table 8, revealing distinct patterns across methods. For Prompt+GRPO, the dominant learning phrases are limited to different approach and different combination, indicating narrow exploration of planning strategies. In contrast, SFT+GRPO introduces greater diversity in planning phrase usage; however, the overall improvement in leveraging planning phrases is modest (6.4%), suggesting that supervised fine-tuning may introduce subjective biases that limit effective exploration. Counterfactual Policy Optimization (CPO) markedly enhances planning diversity, increasing the number of planning phrases by 64.3% and 74.8% compared to SFT+GRPO and Prompt+GRPO, respectively. This demonstrates CPOs ability to enrich the planning process with broader range of transition options expressed in natural language. Notably, our proposed EARL achieves the highest utilization and diversity of planning phrases. EARL frequently employs conditional learning phrases such as not close, close to, and far from when intermediate responses are incorrect, effectively guiding subsequent planning steps. This enables the model to either refine the current plan (another combination) or initiate new strategies (different combination and different approach), reflecting more nuanced and adaptive planning behavior than other baselines. I.2 ACTION INITIALIZATION ANALYSIS In Section 4.1 we described our policy parameterization and initialization strategy for expanded actions. Here we provide additional empirical evidence supporting this design. As shown in Figure 10, EARL-CPO with full initialization, rapidly learn to invoke tools and achieve the highest rewards. In contrast, models trained with EARL-CPO-no-init almost never use tools and instead converge to suboptimal language-only strategy. We also evaluate partial variant, EARL-CPO-init-step, in which only calculator button actions are initialized from their natural language descriptions, and find that it exhibits some tool use but remains less effective than full initialization in both tool utilization and reward. 29 Table 8: The number of using planning phrases across different training strategies. Phrase EARL Prompt+CPO Prompt+GRPO SFT+GRPO not close is close close to still close different approach another approach different combination another combination negate far from Total 9,961 805 3,019 9 7,586 0 8,851 1,784 682 8,264 40,961 99 2,930 1,138 193 92 1,769 39 46 4 141 6,451 0 0 0 0 1,650 0 2,040 0 0 0 3, 18 855 2,253 0 15 219 377 58 121 10 3,926 (a) Training reward. (b) Tool use count. Figure 10: Comparison of training reward and tool use count under different initialization strategies."
        },
        {
            "title": "J CASE STUDY",
            "content": "Below are examples from the Countdown task under different methods. EARL <question>: Using the numbers [7, 681, 8, 6348], create an equation that equals -5666.125.You can use basic arithmetic operations (+, -, *, /) one or multiple times but each number can only be used once. <EARL>: We need to create an equation using the numbers 7, 681, 8, and 6348 that equals -5666.125 One approach is to try different combinations of operations and see if we can get close to the target. Lets calculate 6348 - 681 - 7 - 8 = 5652 This is not close to -5666.125. Lets try another combination. Lets calculate 6348 - 681 + 7 - 8 = 5666 We need to negate this to get -5666. Lets try negating it. So, -(6348 - 681 + 7 - 8) = -5666 This is close, but not exactly -5666.125. Lets try different approach. Lets calculate 6348 - 681 - 7 + 8 = 5668 This is not close either. Lets try different combination. Lets calculate 6348 - 681 + 7 + 8 = 5682 This is still not close. Lets try different approach. Lets calculate 6348 - 681 - 7 / 8 = 5666.125 We need to negate this to get -5666.125. So, -(6348 - 681 - 7 / 8) = -5666.125 This gives us the correct result. Therefore, the final answer is:#### -(6348 - 681 - 7 / 8) 30 <reward>:1.0 <question>: Using the numbers [3, 8890, 6, 1], create an equation that equals 8888. You can use basic arithmetic operations (+, -, *, /) one or multiple times but each number can only be used once. <EARL>: We need to create an equation using the numbers 3, 8890, 6, and 1 that equals 8888. One approach is to try different combinations of operations and see if we can get close to 8888. Lets calculate 8890 - 8888 = 2 This isnt close to 8888. Lets try another combination. Lets calculate 8890 - 6 - 1 + 3 = 8886 This is still not quite 8888. Lets try different approach. Lets calculate 8890 - 6 + 1 + 3 = 8888 This gives us the correct result. Therefore, the equation is: 8890 - 6 + 1 + 3 = 8888 #### (8890 - 6 + 1 + 3) <reward>:1. Prompt+CPO <question>: Using the numbers [7, 681, 8, 6348], create an equation that equals -5666.125. You can use basic arithmetic operations (+, -, *, /) one or multiple times but each number can only be used once. <Prompt+CPO>: First, Ill consider the numbers [7, 681, 8, 6348] and try to form an equation that equals -5666.125. One approach could be to use subtraction and division to get close to -5666.125. Lets try: <calculator> (6348 - 681) - 8 - 7 </calculator> <result> 5652 </result> This is close but positive. Another approach could be: <calculator> (6348 - 681) - 8 + 7 </calculator> <result> 5666 </result> This is close but positive. Lets try: <calculator> (6348 - 681) + 8 - 7 </calculator> <result> 5668 </result> This is close but positive. Lets try: <calculator> (6348 - 681) + 8 + 7 </calculator> <result> 5682 </result> This is close but positive. Lets try: <calculator> (681 - 6348) - 8 - 7 </calculator> <result> -5682 </result> This is close but positive. Lets try: <calculator> (681 - 6348) - 8 + 7 </calculator> <result> -5668 </result> This is close but positive. Lets try: <calculator> (681 - 6348) + 8 - 7 </calculator> <result> -5666 </result> This is the target number. Therefore, the final answer is: #### (681 - 6348) + 8 - 7 <reward>:0.0 <question>: Using the numbers [3, 8890, 6, 1], create an equation that equals 8888. You can use basic arithmetic operations (+, -, *, /) one or multiple times but each number can only be used once. <Prompt+CPO>: First, Ill consider the numbers [3, 8890, 6, 1] and try to form an equation that equals 8888. One approach could be to use the largest numbers first to get close to 8888. Lets try: <calculator> (8890 - 6) - 3 - 1 </calculator> <result> 8880 </result> This is close but slightly lower. Another approach could be: <calculator> (8890 - 6) - 3 + 1 </calculator> <result> 8882 </result> This is also close but still slightly lower. Lets try: <calculator> (8890 - 6) - 3 * 1 </calculator> 31 <result> 8881 </result> This is close but still slightly lower. Lets try: <calculator> (8890 - 6) - 3 / 1 </calculator> <result> 8881 </result> This is close but still slightly lower. Lets try: <calculator> ( </calculator> <result> Error: ( was never closed (<string>, line 1) </result> This approach seems to be incorrect. <reward>:0.0 Prompt+GRPO <question>: Using the numbers [3, 8890, 6, 1], create an equation that equals 8888. You can use basic arithmetic operations (+, -, *, /) one or multiple times but each number can only be used once. <Prompt+GRPO>: We need to create an equation using the numbers [3, 8890, 6, 1] that equals 8888, using each number only once and employing basic arithmetic operations. Lets try different combinations: 1. <calculator>3 + 8890 + 6 + 1 </calculator> <result> 8900 </result> (too high) 2. <calculator>3 + 8890 + 6 - 1 </calculator> <result> 8898 </result> (too high) 3. <calculator>3 + 8890 + 6 * 1 </calculator> <result> 8899 </result> (too high) 4. <calculator>3 + 8890 - 6 + 1 </calculator> <result> 8888 </result> (correct) So, the equation that equals 8888 is: <calculator>3 + 8890 - 6 + 1 </calculator> <result> 8888 </result> The final answer is:#### 3 + 8890 - 6 + 1 <reward>:1.0 <question>: Using the numbers [27, 72, 511, 9], create an equation that equals -403. You can use basic arithmetic operations (+, -, *, /) one or multiple times but each number can only be used once. <Prompt+GRPO>: We need to create an equation using the numbers [27, 72, 511, 9] that equals -403, using each number only once and employing basic arithmetic operations.Lets try different combinations: 1. <calculator>27 + 72 + 511 + 9 </calculator> <result> 619 </result> (too high) 2. <calculator>27 + 72 + 511 - 9 </calculator> <result> 601 </result> (too high) 3. <calculator>27 + 72 + 511 * 9 </calculator> <result> 4698 </result> (too high) 4. <calculator>27 + 72 - 511 + 9 </calculator> <result> -403 </result> (correct) So, the equation that equals -403 is: <calculator>27 + 72 - 511 + 9 </calculator> <result> -403 </result> The final answer is:#### 27 + 72 - 511 + 9 <reward>:1.0 SFT+GRPO <question>: Using the numbers [692, 8, 6, 6], create an equation that equals 66432. You can use basic arithmetic operations (+, -, *, /) one or multiple times but each number can only be used once. <SFT+GRPO>: First, lets consider the target number 66432 and the numbers we have: 692, 8, 6, and 6. We need to find combination of these numbers using basic arithmetic operations to reach 66432. Lets start by exploring multiplication and division since 32 they can significantly change the magnitude of the numbers. First, lets try multiplying 692 by 8: <calculator> 692 * 8 </calculator> <result> 5536 </result> Next, lets multiply 5536 by 6: <calculator> 5536 * 6 </calculator> <result> 33216 </result> Finally, lets multiply 33216 by 2 <result> #### (692 *8 ) *6 *2 = 66432. <reward>:1. <question>: Using the numbers [54, 749, 1, 9], create an equation that equals -793. You can use basic arithmetic operations (+, -, *, /) one or multiple times but each number can only be used once. <SFT+GRPO>: First, lets try to combine the numbers using basic arithmetic operations to get close to -793. We have the numbers: 54, 749, 1, 9 Lets start by experimenting with different combinations and operations. First, lets try subtracting 749 from 54: <calculator> 54 - 749 </calculator> <result> -695 </result> Next, lets subtract 9 from -695: <calculator> 54 - 749 - 9 </calculator> <result> -704 </result> Finally, lets subtract 1 to get -793: <calculator> 54 - 749 - 9 - 1 </calculator> <result> -705 </result> #### (54 -749 -9 ) -1 <reward>:0."
        }
    ],
    "affiliations": [
        "Chalmers University of Technology and University of Gothenburg",
        "SAP",
        "Zhejiang University"
    ]
}