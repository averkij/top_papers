{
    "paper_title": "FlowEdit: Inversion-Free Text-Based Editing Using Pre-Trained Flow Models",
    "authors": [
        "Vladimir Kulikov",
        "Matan Kleiner",
        "Inbar Huberman-Spiegelglas",
        "Tomer Michaeli"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Editing real images using a pre-trained text-to-image (T2I) diffusion/flow model often involves inverting the image into its corresponding noise map. However, inversion by itself is typically insufficient for obtaining satisfactory results, and therefore many methods additionally intervene in the sampling process. Such methods achieve improved results but are not seamlessly transferable between model architectures. Here, we introduce FlowEdit, a text-based editing method for pre-trained T2I flow models, which is inversion-free, optimization-free and model agnostic. Our method constructs an ODE that directly maps between the source and target distributions (corresponding to the source and target text prompts) and achieves a lower transport cost than the inversion approach. This leads to state-of-the-art results, as we illustrate with Stable Diffusion 3 and FLUX. Code and examples are available on the project's webpage."
        },
        {
            "title": "Start",
            "content": "FlowEdit: Inversion-Free Text-Based Editing Using Pre-Trained Flow Models"
        },
        {
            "title": "Vladimir Kulikov Matan Kleiner",
            "content": "Inbar Huberman-Spiegelglas"
        },
        {
            "title": "Tomer Michaeli",
            "content": "Technion Israel Institute of Technology 4 2 0 2 1 1 ] . [ 1 9 2 6 8 0 . 2 1 4 2 : r Figure 1. FlowEdit. We present an inversion-free, optimization-free and model agnostic method for text-based image editing using pretrained flow models. As opposed to the editing-by-inversion paradigm, FlowEdit constructs an ODE that directly maps the source image distribution to the target image distribution (corresponding to the source and target prompts). This ODE achieves lower transport cost and thus leads to better structure preservation, achieving state of the art results on complex editing tasks. From left to right, top to bottom, the first five images were obtained with FLUX and the rest with Stable Diffusion 3. Text indicates changes in the prompts."
        },
        {
            "title": "Abstract",
            "content": "Editing real images using pre-trained text-to-image (T2I) diffusion/flow model often involves inverting the image into its corresponding noise map. However, inversion by itself is typically insufficient for obtaining satisfactory results, and therefore many methods additionally intervene in the sampling process. Such methods achieve improved results but are not seamlessly transferable between model architectures. Here, we introduce FlowEdit, text-based editing method for pre-trained T2I flow models, which is inversion-free, optimization-free and model agnostic. Our method constructs an ODE that directly maps between the source and target distributions (corresponding to the source and target text prompts) and achieves lower transport cost than the inversion approach. This leads to state-of-the-art results, as we illustrate with Stable Diffusion 3 and FLUX. Code and examples are available on the projects webpage. 1. Introduction Diffusion and flow models generate samples through an iterative process, which starts from pure noise and employs denoising-like steps. Beyond data generation, many methods employ pre-trained diffusion/flow models for editing real signals [4, 7, 14, 1618, 21, 22, 24, 30, 31, 38, 44, 47]. While some techniques rely on test-time optimization [17, 22], many methods aim for optimization-free approaches. common first step in the latter category of methods is inversion, i.e. extracting the initial noise vector that presumably generates the signal one wishes to edit. This noise vector is then used to generate the edited signal by injecting different condition to the model, such as modified text prompt [21, 33, 45, 46]. In the context of image editing, many works noted that this editing-by-inversion paradigm often leads to insufficient fidelity to the source image (see e.g. [21]). To overcome this, some methods attempt to reduce the errors incurred in the inversion process [13, 33]. However, this rarely eliminates the problem as even when editing generated images, for which the initial noise is precisely known, editing-by-inversion commonly fails to preserve structure. Other methods intervene in the sampling process by injecting internal model representations (e.g. attention maps) recorded during the inversion process [5, 18, 44]. These methods achieve improved fidelity but are not easily transferable across model architectures and sampling schemes. In this work, we present FlowEdit, text-based editing method for pre-trained text-to-image (T2I) flow models, which breaks away from the editing-by-inversion paradigm. FlowEdit is optimization-free and model agnostic, making it easily transferable between models. Rather than mapping the image to noise and the noise back to an edited image, it constructs direct path between the source and target distributions (corresponding to the source and target text prompts). This path is shorter than that achieved in inversion, and thus maintains better fidelity to the source image. We evaluate FlowEdit on diverse edits using FLUX [25] and Stable Diffusion 3 (SD3) [11], and show that it achieves state-of-the-art results across the board (see Fig. 1). Our method is motivated by novel reinterpretation of editing-by-inversion as direct path between the source and target distributions. Armed with this observation, we derive an alternative direct path that has lower transport cost, as we show on synthetic examples. 2. Related work To edit real image using pre-trained diffusion model, some methods use optimization over the image itself [17, 23, 34]. These methods utilize the generative prior of T2I diffusion model as loss term in an optimization process that pushes the image to comply with the user-provided prompt. Recently, Yang et al. [48] suggested similar method for flow models. These methods are resource intensive at test-time. Many zero-shot editing methods do not require optimization. The first step of most of these methods is image-tonoise inversion [4, 5, 8, 18, 21, 4346]. However, the noise map obtained through naive inversion [41, 46] is generally unsuitable for effective editing [18, 21, 33]. This is often attributed to the inaccurate inversion process, leading to many methods focused on improving its accuracy [15, 32, 33, 45]. Yet, even exact inversion on synthetic data leads to unsatisfactory editing [21]. To address this, many methods extract during the inversion stage structural information implicitly encoded within the model architecture. They then inject it in the sampling process to achieve better structure preservation [4, 5, 18, 35, 44]. These methods are typically tailored for specific model architectures and sampling methods, limiting their transferability to new settings, such as to flow models [11, 25]. Recently, Rout et al. [40] proposed an approximate inversion for flow models, which balances between reconstruction and editability. Unlike previous works, our method does not rely on inversion. It maps from source to target distributions without traversing through the Gaussian distribution. It also avoids optimization and does not intervene in the model internals, making it easily adaptable to new models. 3. Preliminaries 3.1. Rectified Flow models Generative flow models attempt to construct transportation between the distributions of two random vectors, X0 and X1, defined by an ordinary differential equation (ODE) over time [0, 1], dZt = (Zt, t) dt. (1) Here, is time-dependent velocity field, usually parameterized by neural network, which has the property that if the boundary condition at = 1 is Z1 = X1, then Z0 is distributed like X0. It is common to choose X1 (0, I), which allows to easily draw samples from the distribution of X0. This is done by initializing the ODE at = 1 with sample of standard Gaussian vector, and numerically solving the ODE backwards in time to obtain sample Z0 from the distribution of X0 at time = 0. Rectified flows [2, 27, 29] are particular type of flow models, which are trained such that the marginal at time corresponds to linear interpolation between X0 and X1, Zt ( t)X0 + tX1. (2) Rectified flows have the desired property that their sampling paths are relatively straight. This allows using small number of discretization steps when solving the ODE. Figure 2. Editing by inversion vs. FlowEdit. (a) In inversion based editing, the source image src is first mapped to the noise space 0 by solving the forward ODE conditioned on the source prompt (left path). Then, the extracted noise is used to solve the reverse ODE conditioned on the target prompt to obtain tar 0 (right path). The images at the bottom visualize this transition. (b) We reinterpret inversion as direct path between the source and target distributions (bottom path). This is done by using the velocities calculated during the inversion and sampling (green and red arrows) to calculate an editing direction (orange arrow) that drives the evolution of the direct path inv through an ODE. The resulting path is noise-free, as demonstrated by the images at the bottom. (c) FlowEdit traverses shorter direct path, FE and use that direction to create ˆZ tar (gray parallelogram). We then calculate the corresponding velocities and average over multiple realizations (not shown in the figure) to obtain the next ODE step (orange arrow). The images at the bottom demonstrate our noise-free path. , without relying on inversion. At each timestep, we directly add random noise to ˆZ src 0 to obtain ˆZ src from FE Text-to-image flow models employ velocity field (Xt, t, C) that depends on text prompt C. Such models are trained on pairs of text and image data, (C, X0), so as to C. allow sampling from the conditional distribution of X0 3.2. Image editing using ODE inversion Suppose we are given real image, src, which we want to edit by providing text prompt csrc describing the image and text prompt ctar describing the desired edit. common approach to do so is by using pre-trained text-conditioned diffusion/flow model and employing inversion. Specifically, let us denote the text conditioned velocities as src(Xt, t) (Xt, t, csrc) and tar(Xt, t) (Xt, t, ctar). Methods relying on inversion start by extracting the initial noise map corresponding to the source image. This is done by traversing the forward process defined by the ODE dZ src = src(Z src , t) dt, (3) starting at = 0 from the source image, src reaching noise map src path shown on the left of Fig. 2a. Then, the reverse ODE, 0 = src, and 1 at = 1. The process src is the dZ tar = tar(Z tar , t) dt, (4) is solved backward in time, starting from = 1 with the extracted noise, tar 1 , and reaching an edited image tar 0 at = 0. This is the path shown on the right of Fig. 2a. 1 = src Editing by inversion often leads to unsatisfactory results. While this is commonly blamed on the inaccurate discretization of the ODE, sub-optimal results are encountered also when editing generated images and using their groundtruth initial noise maps [21]. To overcome this, many methods intervene in the sampling process, e.g. by injecting feature maps extracted during the inversion process. Such approaches are typically tailored for specific model architecture, and cannot be seamlessly transferred across models. Here, we propose method that retains the simplicity of the editing-by-inversion approach and achieves state of the art results without any intervention in the model internals. 4. Reinterpretation of editing by inversion Before presenting our method, we start by reinterpreting the inversion based editing approach. This reinterpretation will serve to motivate our technique. Inversion based editing transports between the source and target distributions, while passing through the distribution of Gaussian noise. However, this approach can also be expressed as direct path between the source and target distributions. Namely, we can construct path inv that starts at = 1 in the source distribution and reaches the target distribution at = 0. Specifically, given the forward and reverse flow trajectories, src = src and tar 0 + tar , we define . src inv (5) t Note that when going from = 1 to = 0 we transition from the source image, inv 1 = src 1 = src 0 . This path is depicted at the bottom of Fig. 2b, in which the gray parallelogram visualizes the relation (5). 1 ), to the edited image, inv 0 = src (because tar 0 = tar Let us express (5) as an ODE. Differentiating both sides of (5) and substituting (3) and (4), this relation becomes dZ inv = (Z src , tar ) dt, (6) , tar (Z src ) = tar(Z tar where , t). This is visualized by the green, red, and orange arrows in Fig. 2b. Now, isolating tar from (5) and substituting it into (6), we get that the direct path is the solution of the ODE src(Z src , t) dZ inv = (Z src , inv + src"
        },
        {
            "title": "Z src",
            "content": "0 ) dt (7) 0 at = 1. with boundary condition inv 1 = src There exist, of course, many paths connecting src 0 and tar 0 . What is so special about the path defined by (7)? It turns out that images along this path are noise-free, as shown at the bottom of Fig. 2b. The reason this happens is that the noisy images tar contain roughly the same noise constituent. Therefore, the vectors tar(Z tar , t) and src(Z src , t), which point at the source and target distributions, respectively, remove roughly the same noise component, so that the difference vector ) (orange arrow in Fig. 2b), encompasses the difference only between the clean image predictions. and src (Z src , tar (Z src How do images evolve along this direct path? At the beginning of the path (t close to 1), the noise level in src and is substantial, and therefore the vector , tar tar ) captures only differences in coarse image structures. As gets smaller, the noise level drops and higher frequency contents are unveiled. In other words, the path (7) constitutes sort of autoregressive coarse-to-fine evolution from src 0 to tar 0 . This can be seen in the images at the bottom of Fig. 2b, where the first features that get modified are the coarse structures, and the last features to get updated are the fine textures. See App. for additional illustrations. 5. FlowEdit 15 2 The fact that editing-by-inversion can be expressed as direct ODE does not imply that it induces desirable pairing between source and target samples. Figure 3 shows simple example, where the source and target distributions are Gaussian mixtures with modes centered at ( ( , , respecand { tively. It is reasonable to expect from an editing method to map each mode in the source distribution to its closest mode in the target distribution. In the context of image editing, this means that source images are modified as little as possible, while being transported to the target distribution. However, as seen on the bottom right of Fig. 3, this is not 15, 0), (0, 15) } ), ( 15 2 , 15 15 2 ) } { Figure 3. Source to target pairings for editing-by-inversion and FlowEdit. Samples from the two modes in the source distribution are colored in blue and red (top left). As can be seen in the right panes, FlowEdit results in separated blue and red modes, as opposed to editing-by-inversion, where the modes are intermixed, indicating higher transport cost in terms of MSE. This can be explained by the Gaussian in the bottom left, through which the editing-by-inversion path must traverse before reaching the target. what happens in inversion. The reason the induced pairings look the way they look can be understood by inspecting how samples are mapped to their initial Gaussian noise component (bottom left). Our goal is to construct an alternative mapping, which leads to lower distances between the source and target samples. Our method, which we coin FlowEdit, is illustrated on the top right of Fig. 3. As can be seen, it maps each mode in the source distribution to its nearest mode in the target distribution. Compared to editing-by-inversion, FlowEdit achieves less than twice as low transportation cost, measured by the average squared distance between source samples and their paired target samples. We emphasize that our method is based on heuristic, which is not guaranteed to precisely map to the target distribution. In particular, it does not necessarily coincide with the optimal transport mapping. Yet, as we will see in the context of images, it achieves state-of-the-art results in terms of structure preservation (i.e. achieving small transportation cost) while generating samples that adhere to the target text prompt (and are thus within the target distribution). How can we depart from the pairings dictated by inversion? Our key idea is to use many different random pairings, and average the velocity fields corresponding to each of them. More concretely, we propose to use the same ODE as (7), but to replace the inversion path inv by different random process that has the same marginals. Then, at each Algorithm 1 Simplified algorithm for FlowEdit Input: real image src, (cid:8)ti Output: edited image tar Init: FE = src tmax 0 for = nmax to 1 do (cid:9)T i=0, nmax, navg ti)X src + tiNti + src"
        },
        {
            "title": "X src",
            "content": "(0, 1) Nti src (1 ti FE tar ti ti ti tar(Z tar , ti) ti ti FE FE + (ti1 ti1 ti end for Return: FE 0 = tar 0 src(Z src ti ti)V ti"
        },
        {
            "title": "Optionally\naverage navg\nsamples",
            "content": ", ti) Figure 4. Cats and dogs experiment. We generated 1000 cat images using diverse prompts and edited them to dog images using both FlowEdit and editing-by-inversion. FlowEdit outperforms editing-by-inversion, achieving lower transport cost between pairs of source-target images and better FID and KID scores (computed against 1000 generated dog images). timestep, we average the directions corresponding to different realizations of that process. Specifically, consider the alternative forward process ˆZ src = (1 t)Z src 0 + tNt, (8) where Nt is Gaussian process that is statistically independent of src (0, 1) for every 0 and has marginals Nt [0, 1]. We construct path FE by solving the ODE (cid:12) (cid:12)Z src src (cid:12) 0 ) dt (cid:105) 0 dZ FE = (cid:104) ( ˆZ src , FE + ˆZ src with boundary condition FE 1 = src 0 at = 1. Note that the expectation here is w.r.t. ˆZ src (equivalently Nt). Also note that we need not specify the covariance function of Nt as (9) depends only on the marginals of the process ˆZ src and not on its entire distribution law. (9) + ˆZ src schematic illustration is shown in Fig. 2c. The point ˆZ src on the left corresponds to single draw of Nt. The distribution of ˆZ src at time is shown in cyan. For each such draw, we calculate ˆZ tar = FE 0 , as in the editing-by-inversion ODE. This is indicated by the gray parand ˆZ tar allelogram. Now, given ˆZ src , we compute the vet src( ˆZ src ) = tar( ˆZ tar locity field , t) to , t) obtain an update direction. This is illustrated by the green, red, and orange arrows. Finally, we repeat this many times and average the resulting orange arrows (not shown in the figure) to obtain the final update direction. ( ˆZ src , ˆZ tar src 5.1. Practical considerations ti { In practice, discrete set of timesteps i=0 is used to } drive the editing process, where is the number of discretization steps. Additionally, the expectation in (9) is approximated by averaging navg model predictions at each timestep. As opposed to the theoretical expectation operator, when taking navg to be small, the covariance function of Nt starts playing role. To obtain good approximation, it is possible to exploit the averaging that naturally occurs across timesteps. We do so by choosing the covariance function of Nt to satisfy E[NtNs] = 0 for every > δ, where δ is the ODE discretization step, so that the noise becomes independent across timesteps. is src , t) the averaged velocity term in (9) src 0 ] Note that the difference E[V tar( ˆZ tar calculated via E[V src( ˆZ src 0 ], meaning that we could have sampled , t) ˆZ tar and ˆZ src independently. Instead, we chose to construct them with the same noise instance, which further improves robustness to small values of navg. This is aligned with the observation made by Hertz et al. [17], in the context of diffusion models, where calculating difference between correlated noisy marginals reduces artifacts. See App. for further discussion. nmax Similarly to [21, 31], we define an integer that deterT , mines the starting timestep for the process 0 = src. meaning that the process is initialized with FE tnmax When nmax = the full edit path is traversed and the strongest edit is obtained. When nmax < , the first (T nmax) timesteps are skipped, effectively shortening the edit path. This is equivalent to inversion, where weaker edits are obtained by inverting up to timestep nmax, and sampling from there. We illustrate the effect of nmax in App. C. Algorithm 1 provides simplified overview of our algorithm. For more detailed version, please see Alg. S1. 5.2. Comparison to editing by (exact) inversion To demonstrate the reduced transport cost of FlowEdit compared to editing-by-inversion, we evaluate both methods on synthetic dataset of model-generated images. This way, the initial noise maps are known, ensuring the inversion is exact and eliminating potential issues of approximate inversion. The dataset consists of 1000 cat images generated by SD3 using variations of the prompt photo of cat generated by Llama3 [10]. We then edit these images using both methods, with the target prompt identical to the source prompt, except for replacing cat with dog. After obtaining the source-target pairs, we calculated Figure 5. FlowEdit results. FlowEdit successfully edits diverse images using various editing prompts. The edits preserve the structure of the original image, changing only the specified region. FLUX was used for the first and third rows and SD3 for the second and fourth rows. the transport cost for both methods by measuring the MSE between source-target pairs in the models latent space, as well as LPIPS [49] on the decoded images. As expected, FlowEdit achieves lower transport cost compared to editing-by-inversion (1376 vs. 2239 for MSE, 0.15 vs. 0.25 for LPIPS), indicating superior preservation of the source image structure and semantics. Figure 4 showcases small qualitative comparison for the cat-to-dog edits. To assess the alignment of our edits with the target distribution, we generated 1000 dog images with SD3, using the same target prompts but with dog instead of cat. We followed by calculating FID [19] and KID [3] between the generated dog images and the edited dog images, for both methods. Our method achieves lower FID (51.14 vs. 55.88) and KID (0.017 vs. 0.023), indicating that similarly to inversion, our ODE path is able to produce images from the target distribution. See App. for more details and results. 6. Experiments Implementation details In our experiments we use the official weights of SD3 medium [42] and FLUX.1 dev [26], available at HuggingFace, as the base T2I flow models. For SD3 we use = 50 steps, with nmax = 33. SD3 employs CFG [20] for their text conditioning. We set the source and target scales to 3.5 and 13.5, respectively. For FLUX we use = 28 steps, with nmax = 24. FLUX takes CFG as an input conditioning, which we set to the values of 1.5 for the source conditioning, and 5.5 for the target. For both methods we use navg = 1 (see Sec. 5.1 and App. I). We use these hyperparameters for all the results in this section, unless explicitly noted otherwise. Figure 6. Qualitative comparisons. We compare FlowEdit to baseline and competing methods, for both SD3 (left) and FLUX (right). In contrast to other methods, FlowEdit adheres to the target prompt and simultaneously preserves the original image structure. The value next to SDEdit indicates the strength. Dataset Our dataset consists of diverse set of over 70 real images of 1024 1024 pixels from DIV2K dataset [1] and from royalty free online sources [36, 37]. Each image has corresponding source prompt, first obtained using LLaVA-1.5 [28] and then manually refined. For each image several handcrafted target prompts are provided to facilitate diverse edits. The resulting dataset consists of over 250 textimage pairs, and is used to evaluate our method as well as the baseline and competing methods. The dataset, including source and target prompt pairs, will be made available for reproducibility purposes. Baseline and competing methods We compare our method against baseline and competing text-based real image editing methods using flow models. The first baseline method is editing by ODE inversion, which is an approximation in the case of real images (see Sec. 3.2). We apply the same hyperparameters as those used in our method. The second method is SDEdit [31], which is easily applied to flow models by adding noise to the source image up to specified nmax step and using regular sampling conditioned on target prompt to obtain the edit. This parameter represents the strength of the edit. Both were implemented for SD3 and FLUX for direct comparison with FlowEdit. Additionally, we compare to the recent SDS-based flow model implementation, iRFDS, as proposed in [48], using their official implementation and hyperparameters which are available for SD3. Therefore, we compare it to FlowEdit using only SD3. We also include comparison to the recent RF-Inversion [40], implementing it based on the provided pseudo-code, as an official implementation is not available at the time of writing. For fair comparison, we chose hyperparameters according to those reported in their SM. Since the RF-Inversion parameters are specific to FLUX, this comparison is conducted solely with FLUX. Whenever possible, we performed hyperparameters search for each method to identify the optimal settings for editing. The final parameters used to create the edited images in the paper are detailed in App. B. We do not compare FlowEdit to text-based image editing techniques developed for diffusion models [5, 21, 44], as these methods are not easily adapted to flow models, and direct comparison would not be fair due to model differences. Qualitative evaluation Figures 1, 5, S1 present editing results obtained with FlowEdit across diverse set of imFigure 7. Quantitative comparisons to baseline and competing methods. We evaluate each method using CLIP and LPIPS. FlowEdit achieves favorable balance over other methods. Lines connected with dots represent different hyperparameter settings of the same method, detailed in App. B. ages and target prompts, including localized edits to text and objects, modifications of multiple objects at once, pose changes, etc. Importantly, our edits exhibit good structural preservation of the source image, and simultaneously maintain good adherence to the target text. Figure 6 presents comparison between FlowEdit and the baseline and competing methods for both SD3 and FLUX, emphasizing that FlowEdit consistently adheres to the text prompt while being able to preserve the structure of the original image. In contrast, other methods struggle to maintain both aspects effectively. For example, FlowEdit is the only method that can both change the large text on the gas station sign (third row) and preserve the cars and background of the source image. Some methods, like RFInversion, are able to change the text on the sign but fail to maintain the structure of the original image, while others, such as iRFDS, somewhat preserve the structure but fail to In the last row, while all methods fully change the sign. perform the required edit (changing the decoration to strawberries), only FlowEdit (for both SD3 and FLUX) preserves the background accurately. Other methods introduce additional, unintended elements such as furniture and flowers. See App. for more qualitative comparisons. Quantitative evaluation We numerically evaluate the results of FlowEdit, as well as the baselines and competing methods, using LPIPS [49] to measure the semantic structure preservation (lower is better) and CLIP [39] to assess text adherence (higher is better). Figure 7 displays these results for SD3 (left) and FLUX (right), with varied hyperparameters to illustrate the tradeoff between structure preservation and text adherence. This graph shows that FlowEdit achieves favorable balance in this tradeoff. In contrast, the other methods either maintain the original image structure at the cost of weak edit or change the image with little regard to its original semantics. Additional quantitative Figure 8. Text-based style editing. FlowEdit changes the style of an image at the cost of slightly deviating from the original image structure. SD3 was used for the first row and FLUX for the second. Hyperparameters used for these results are reported in App. D. evaluations and details are provided in App. B. Text-based style editing The hyperparameters used for the previous experiments yield edited images that preserve the structure of the original image. While in general we would like to preserve the structure, it can be too restrictive in certain editing tasks, such as style editing. Figure 8 demonstrates the results of FlowEdit for text-based style editing, where some deviation from the original structure allows for more stylistic flexibility. To achieve these results, we adjust the structural preservation by relieving the dependence on the source image in the final generation steps. This is done by slightly adjusting the last few steps of our algorithm, as discussed in App. D. This adjustment results in stronger edits, which are necessary for style changes. 7. Conclusion We introduced FlowEdit, an inversion-free, optimizationfree and model agnostic method for text-based image editing using pre-trained flow models. Our approach constructs direct ODE between the source and target distributions (corresponding to source and target text prompts), without passing through the standard Gaussian distribution as in inversion-based editing methods. Evaluations on synthetic datasets show that FlowEdit achieves lower transport costs, and thus stronger structure preservation. We demonstrated state-of-the-art performance across various editing tasks with the FLUX and SD3 models, outperforming other flow-based editing methods. Since FlowEdit relies on adding random noise, it can produce diverse set of results depending on the random seed. However, in some cases, the added noise can lead to suboptimal results that include artifacts. See App. for examples."
        },
        {
            "title": "References",
            "content": "[1] Eirikur Agustsson and Radu Timofte. Ntire 2017 challenge on single image super-resolution: Dataset and study. In Proceedings of the IEEE conference on computer vision and pattern recognition workshops, pages 126135, 2017. [2] Michael Samuel Albergo and Eric Vanden-Eijnden. BuildIn The ing normalizing flows with stochastic interpolants. Eleventh International Conference on Learning Representations, 2023. [3] Mikołaj Binkowski, Dougal J. Sutherland, Michael Arbel, and Arthur Gretton. Demystifying MMD GANs. In International Conference on Learning Representations, 2018. [4] Manuel Brack, Felix Friedrich, Katharia Kornmeier, Linoy Tsaban, Patrick Schramowski, Kristian Kersting, and Apolinario Passos. Ledits++: Limitless image editing using text-to-image models. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 88618870, 2024. [5] Mingdeng Cao, Xintao Wang, Zhongang Qi, Ying Shan, Xiaohu Qie, and Yinqiang Zheng. Masactrl: Tuning-free mutual self-attention control for consistent image synthesis and editing. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 2256022570, 2023. [6] Mathilde Caron, Hugo Touvron, Ishan Misra, Herve Jegou, Julien Mairal, Piotr Bojanowski, and Armand Joulin. Emerging properties in self-supervised vision transformers. In Proceedings of the IEEE/CVF international conference on computer vision, pages 96509660, 2021. [7] Nathaniel Cohen, Vladimir Kulikov, Matan Kleiner, Inbar Huberman-Spiegelglas, and Tomer Michaeli. Slicedit: Zeroshot video editing with text-to-image diffusion models using spatio-temporal slices. In Proceedings of the 41st International Conference on Machine Learning, pages 91099137. PMLR, 2024. [8] Gilad Deutch, Rinon Gal, Daniel Garibi, Or Patashnik, and Daniel Cohen-Or. Turboedit: Text-based image editing using few-step diffusion models. In SIGGRAPH Asia 2024 Conference Papers, pages 112, 2024. [9] Sander Dieleman. Diffusion is spectral autoregression, 2024. [10] Abhimanyu Dubey, Abhinav Jauhri, Abhinav Pandey, Abhishek Kadian, Ahmad Al-Dahle, Aiesha Letman, Akhil Mathur, Alan Schelten, Amy Yang, Angela Fan, et al. The llama 3 herd of models. arXiv preprint arXiv:2407.21783, 2024. [11] Patrick Esser, Sumith Kulal, Andreas Blattmann, Rahim Entezari, Jonas Muller, Harry Saini, Yam Levi, Dominik Lorenz, Axel Sauer, Frederic Boesel, et al. Scaling rectified flow transformers for high-resolution image synthesis. In Forty-first International Conference on Machine Learning, 2024. [12] Stephanie Fu, Netanel Tamir, Shobhita Sundaram, Lucy Chai, Richard Zhang, Tali Dekel, and Phillip Isola. Dreamsim: Learning new dimensions of human visual similarity using synthetic data. Advances in Neural Information Processing Systems, 36, 2024. [13] Daniel Garibi, Or Patashnik, Andrey Voynov, Hadar Averbuch-Elor, and Daniel Cohen-Or. Renoise: Real image inversion through iterative noising. In Computer Vision ECCV 2024, pages 395413, 2024. [14] Rene Haas, Inbar Huberman-Spiegelglas, Rotem Mulayoff, Stella Graßhof, Sami Brandt, and Tomer Michaeli. Discovering interpretable directions in the semantic latent space of diffusion models. In 2024 IEEE 18th International Conference on Automatic Face and Gesture Recognition (FG), pages 19. IEEE, 2024. [15] Ligong Han, Song Wen, Qi Chen, Zhixing Zhang, Kunpeng Song, Mengwei Ren, Ruijiang Gao, Anastasis Stathopoulos, Xiaoxiao He, Yuxiao Chen, et al. Proxedit: Improving tuning-free real image editing with proximal guidance. In Proceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision, pages 42914301, 2024. [16] Ayaan Haque, Matthew Tancik, Alexei Efros, Aleksander Holynski, and Angjoo Kanazawa. Instruct-nerf2nerf: EditIn Proceedings of the ing 3d scenes with instructions. IEEE/CVF International Conference on Computer Vision, pages 1974019750, 2023. [17] Amir Hertz, Kfir Aberman, and Daniel Cohen-Or. Delta denoising score. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 23282337, 2023. [18] Amir Hertz, Ron Mokady, Jay Tenenbaum, Kfir Aberman, Yael Pritch, and Daniel Cohen-or. Prompt-to-prompt image editing with cross-attention control. In The Eleventh International Conference on Learning Representations, 2023. [19] Martin Heusel, Hubert Ramsauer, Thomas Unterthiner, Bernhard Nessler, and Sepp Hochreiter. Gans trained by two time-scale update rule converge to local nash equilibrium. Advances in neural information processing systems, 30, 2017. [20] Jonathan Ho and Tim Salimans. Classifier-free diffusion guidance. In NeurIPS 2021 Workshop on Deep Generative Models and Downstream Applications, 2021. [21] Inbar Huberman-Spiegelglas, Vladimir Kulikov, and Tomer Michaeli. An edit friendly ddpm noise space: Inversion and manipulations. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 12469 12478, 2024. [22] Bahjat Kawar, Shiran Zada, Oran Lang, Omer Tov, Huiwen Chang, Tali Dekel, Inbar Mosseri, and Michal Irani. Imagic: Text-based real image editing with diffusion models. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 60076017, 2023. [23] Juil Koo, Chanho Park, and Minhyuk Sung. Posterior disIn Proceedings of the IEEE/CVF Contillation sampling. ference on Computer Vision and Pattern Recognition, pages 1335213361, 2024. [24] Mingi Kwon, Jaeseok Jeong, and Youngjung Uh. DiffuIn The sion models already have semantic latent space. Eleventh International Conference on Learning Representations, 2023. [25] Black Forest Labs. https://github.com/ black-forest-labs/flux, 2024. Accessed: 202411-14. [26] Black Labs. Flux. of https://huggingface.co/ weights Official Forest dev. FLUX.1 [40] Litu Rout, Yujia Chen, Nataniel Ruiz, Constantine Caramanis, Sanjay Shakkottai, and Wen-Sheng Chu. Semantic image inversion and editing using rectified stochastic differential equations. arXiv preprint arXiv:2410.10792, 2024. [41] Jiaming Song, Chenlin Meng, and Stefano Ermon. Denoising diffusion implicit models. In International Conference on Learning Representations, 2021. [42] StablityAI. Official weights of SD3 medium diffusers. https://huggingface.co/stabilityai/ stable-diffusion-3-medium-diffusers, 2024. Accessed: 2024-11-14. [43] Linoy Tsaban and Apolinario Passos. Ledits: Real image editing with ddpm inversion and semantic guidance. arXiv preprint arXiv:2307.00522, 2023. [44] Narek Tumanyan, Michal Geyer, Shai Bagon, and Tali Dekel. Plug-and-play diffusion features for text-driven image-to-image translation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 19211930, 2023. [45] Bram Wallace, Akash Gokul, and Nikhil Naik. Edict: Exact diffusion inversion via coupled transformations. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 2253222541, 2023. [46] Chen Henry Wu and Fernando De la Torre. latent space of stochastic diffusion models for zero-shot image editing and guidance. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 73787387, 2023. [47] Jay Zhangjie Wu, Yixiao Ge, Xintao Wang, Stan Weixian Lei, Yuchao Gu, Yufei Shi, Wynne Hsu, Ying Shan, Xiaohu Qie, and Mike Zheng Shou. Tune-a-video: One-shot tuning of image diffusion models for text-to-video generation. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 76237633, 2023. [48] Xiaofeng Yang, Cheng Chen, Xulei Yang, Fayao Liu, and Guosheng Lin. Text-to-image rectified flow as plug-and-play priors. arXiv preprint arXiv:2406.03293, 2024. [49] Richard Zhang, Phillip Isola, Alexei Efros, Eli Shechtman, and Oliver Wang. The unreasonable effectiveness of deep features as perceptual metric. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 586595, 2018. black-forest-labs/FLUX.1-dev, 2024. cessed: 2024-11-14. Ac- [27] Yaron Lipman, Ricky T. Q. Chen, Heli Ben-Hamu, Maximilian Nickel, and Matthew Le. Flow matching for generative modeling. In The Eleventh International Conference on Learning Representations, 2023. [28] Haotian Liu, Chunyuan Li, Yuheng Li, and Yong Jae Lee. Improved baselines with visual instruction tuning. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 2629626306, 2024. [29] Xingchao Liu, Chengyue Gong, and qiang liu. Flow straight and fast: Learning to generate and transfer data with rectified flow. In The Eleventh International Conference on Learning Representations, 2023. [30] Hila Manor and Tomer Michaeli. Zero-shot unsupervised and text-based audio editing using DDPM inversion. In Proceedings of the 41st International Conference on Machine Learning, pages 3460334629. PMLR, 2024. [31] Chenlin Meng, Yutong He, Yang Song, Jiaming Song, Jiajun Wu, Jun-Yan Zhu, and Stefano Ermon. SDEdit: Guided image synthesis and editing with stochastic differential equations. In International Conference on Learning Representations, 2022. [32] Daiki Miyake, Akihiro Iohara, Yu Saito, and Toshiyuki Tanaka. Negative-prompt inversion: Fast image inversion for editing with text-guided diffusion models. arXiv preprint arXiv:2305.16807, 2023. [33] Ron Mokady, Amir Hertz, Kfir Aberman, Yael Pritch, and Daniel Cohen-Or. Null-text inversion for editing real imIn Proceedings of ages using guided diffusion models. the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 60386047, 2023. [34] Hyelin Nam, Gihyun Kwon, Geon Yeong Park, and Jong Chul Ye. Contrastive denoising score for text-guided the latent diffusion image editing. IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 91929201, 2024. In Proceedings of [35] Gaurav Parmar, Krishna Kumar Singh, Richard Zhang, Yijun Li, Jingwan Lu, and Jun-Yan Zhu. Zero-shot image-to-image translation. In ACM SIGGRAPH 2023 Conference Proceedings, pages 111, 2023. [36] Pexels. Pexels - free stock photos & videos. https:// www.pexels.com, 2024. Accessed: 2024-11-14. [37] PxHere. Pxhere - free images & free stock photos. https: //pxhere.com/, 2024. Accessed: 2024-11-14. [38] Chenyang Qi, Xiaodong Cun, Yong Zhang, Chenyang Lei, Xintao Wang, Ying Shan, and Qifeng Chen. Fatezero: Fusing attentions for zero-shot text-based video editing. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 1593215942, 2023. [39] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, et al. Learning transferable visual models from natural language supervision. In International conference on machine learning, pages 87488763. PMLR, 2021. A. Additional results Figure S1 shows additional editing results obtained with FlowEdit. Figure S1. FlowEdit additional results. FLUX was used for the first, third, and fifth rows, and SD3 for the second, fourth and sixth rows. B. Comparisons B.1. Additional qualitative comparisons Figure S2 shows additional comparisons between FlowEdit and the baseline and competing methods, for both SD3 (left) and FLUX (right). The value next to SDEdit indicates the strength (see App. B.2). Figure S2. Additional qualitative comparisons. B.2. Additional details on the experiment settings We compare FlowEdit against the baseline methods of ODE inversion (Sec. 3.2) and SDEdit using both FLUX and SD3, and against the competing methods RF-Inversion (FLUX) and iRFDS (SD3). Figure 7 in the main text presents the CLIP vs. LPIPS results for FlowEdit, the baseline and competing methods for both SD3 and FLUX. Below, we detail the hyperparameters used for SD3 (Tab. S1) followed by those for FLUX (Tabs. S2, S3). The results shown in Figs. 6, S2 and Tabs. S4 and S5 were obtained using the parameters listed in Tabs. S1, S2 and S3. The bold entries indicate the specific settings used when multiple hyperparameter options were tested. B.2.1 Stable Diffusion 3 hyperparameters In Fig. 7 in the main text both FlowEdit and ODE Inversion are shown with three options for the CFG target scale, as detailed in Tab. S1, from left to right. For SDEdit the different values of nmax represents different strength settings ranging from 0.2 to 0.8 in intervals of 0.1. The markers in the figure indicate results with these strength values from left to right. Table S1. SD3 hyperparameters."
        },
        {
            "title": "T steps",
            "content": "nmax CFG @ source CFG @ target SDEdit ODE Inv. iRFDS FlowEdit 50 50 10, 15, 20, 25, 30, 35, 40 33 - 3.5 13.5 13.5, 16.5, , 19.5 official implementation and hyperparameters 3.5 13.5, 16.5, , 19.5 B.2.2 FLUX hyperparameters In Fig. 7 in the main text both FlowEdit and ODE Inversion are shown with three options for the CFG target scale, as detailed in Tab. S2, from left to right. For SDEdit the different values of nmax represent different strength values, corresponding to 0.25, 0.5, 0.75. The markers in the figure indicate results with these strength values from left to right. Table S2. FLUX hyperparameters. steps nmax CFG @ source CFG @ target SDEdit ODE Inv. FlowEdit 28 28 7, 14, 21 20, 24 24 - 1.5 1.5 5.5 3.5, 4.5, 5.5 3.5, 4.5, 5.5 For RF-Inversion, we explore multiple sets of hyperparmeters, as the paper does not report specific ones for general editing. Following the SM of their work, we experimented with several combinations, detailed in Tab. S3 using their notations. Table S3. RF-Inv. hyperparameters. steps starting time τ stopping time η strength 28 8, 7, 6 0.9, 1.0 Lastly, as can be seen in Fig. 7, ODE Inversion on FLUX with these hyperparameters achieves high CLIP score at the cost of high LPIPS score, indicating it does not balance these metrics effectively. By varying nmax, ODE Inversion could achieve lower (better) LPIPS scores at the cost of lower (worse) CLIP scores. Figure S3 illustrates the CLIP and LPIPS scores for the different methods using FLUX. Specifically, ODE Inversion is also shown with nmax = 20 in addition to nmax = 24 as shown in the main text. However, with these adjusted hyperparameters, ODE Inversion struggles to adhere to text prompts. RF-Inversion results with η = 1.0 are also illustrated in Fig. S3 and Tab. S5. Again, with this hyperparameter this method struggles to adhere to text prompts. Figure S3. Additional quantitative comparisons. In addition to the results shown in Fig. 7 for FLUX, we include additional hyperparameters: ODE Inversion with nmax = 20 and RF-Inversion with η = 1.0. These configurations also struggle to achieve good balance between the CLIP and LPIPS metrics. In contrast, FlowEdit demonstrates the best balance. B.3. Metrics comparisons In addition to quantifying the structure preservation using LPIPS, as described in the main text, we now report alternative metrics. Specifically, we use DreamSim [12] as well as cosine similarity in the CLIP [39] and DINO [6] embedding spaces, between the source and target images embeddings. For DreamSim, lower score indicates better structure preservation. For CLIP images and DINO, higher score (bounded by 1) means higher structure preservation. Tables S4, S5 illustrate the results of these metrics, alongside LPIPS and CLIP for the hyperparameters described above. As can be seen, FlowEdit is the only method that is able to both adhere to the text prompt and preserve the structure of the source image. Table S4. SD3 metrics. The first , second and third best scores are highlighted for each metric. CLIP-T measures adherence to text, while the other four scores measure structure preservation. CLIP-T CLIP-I"
        },
        {
            "title": "LPIPS",
            "content": "SDEdit 0.2 SDEdit 0.4 ODE Inv. iRFDS FlowEdit 0.33 0.34 0.337 0. 0.344 0.885 0.854 0.813 0.822 0.872 0.251 0.316 0.318 0.376 0."
        },
        {
            "title": "DINO",
            "content": "0.634 0.564 0.549 0.534 0."
        },
        {
            "title": "DreamSim",
            "content": "0.213 0.273 0.326 0.327 0.253 Table S5. FLUX metrics. The first , second and third best scores are highlighted for each metric. CLIP-T measures adherence to text, while the other four scores measure structure preservation. CLIP-T CLIP-I LPIPS SDEdit 0.5 SDEdit 0.75 ODE Inv. RF Inv. FlowEdit 0.316 0.331 0.341 0.334 0.337 0.902 0.862 0.822 0.856 0.875 0.264 0.348 0.374 0.34 0. DINO 0.637 0.557 0.505 0.558 0.682 DreamSim 0.18 0.26 0.328 0. 0.252 C. Effect of nmax . As described in the main text, we define an integer to determine the starting timestep of the process, where 0 = src. When nmax = , the full edit path is traversed and the strongest edit is obtained. The process is initialized with FE For nmax < , the first (T nmax) timesteps are skipped, effectively shortening the edit path. This is equivalent to inversion, where weaker edits are obtained by inverting up to timestep nmax, and sampling from there. Figure S4 illustrates the effect of nmax on the results. The CFG and used for these editing results are the same as mentioned in the main text, except for nmax whose value is specified in the figure. nmax tnmax Figure S4. Effect of nmax. D. Text-based style editing FlowEdit excels in structure preserving edits. This trait is usually desired in text-based image editing, yet in some cases it could be too limiting. One such scenario is text-based style editing, where we would like to slightly deviate from the original structure in order to achieve stronger stylistic effect. While nmax allows some control over the structure, it might not be enough by itself to control the finer details required for style changes, i.e. higher frequency textures. To achieve better control over the finer details, we define new hyperparameter, nmin that controls the structure deviation at lower noise levels, and effectively allows stronger modifications of the higher frequencies. Specifically, when < nmin we apply regular sampling with the target text, rather than following FlowEdit as described in the main text. These steps are further detailed in our full algorithm, Alg. S1. Figure S5 illustrates the effect of nmin for text-based style editing. For small nmin the edited image preserves the structure of the original image (especially in the higher frequencies), while for higher nmin values it slightly deviates from it, achieving stronger edit. These results were obtained using FLUX with = 28 steps, nmax = 21 and CFG scales of 2.5 and 6.5 for the source and target conditionings, respectively. The nmin values are mentioned in the figure. Figure S5. Effect of nmin. When nmin is small, the edited image remains close to the original image and struggles to align with the text. In contrast, larger values of nmin result in better adherence to the text but at the cost of greater deviation from the original image. The table below includes the hyperparameters used for the text-based style editing results in Fig. 8. Table S6. Hyperparameters used for the results displayed in Fig. 8. Model nmin nmax CFG @ source CFG @ target House by lake in minecraft style Lighthouse in watercolor painting style Kid running in anime style Dog in Disney style SD3 SD3 FLUX FLUX 15 15 14 14 31 31 21 24 3.5 3.5 2.5 1.5 13.5 13.5 6.5 4. E. Hyperparmeters used for Fig. 1 The results in Fig. 1 were achieved using the hyperparmeters described in Tab. S7 below. Table S7. Hyperparameters used for the results displayed in Fig. 1. this Cat LOVE Bread Mountain Lizard cvpr Raccoon FLOW Bacon Volcano"
        },
        {
            "title": "Dragon",
            "content": "Man jumping in Pixar style Bread White dog w/ cat Steak Dalmatian w/o cat"
        },
        {
            "title": "Model nmin",
            "content": "nmax CFG @ source CFG @ target FLUX FLUX FLUX FLUX FLUX SD3 SD3 SD3 SD3 0 0 0 0 0 0 15 0 0 24 24 24 24 24 33 21 33 33 1.5 1.5 1.5 1.5 1.5 3.5 3.5 3.5 3.5 3.5 4.5 4.5 3.5 5.5 13.5 13.5 13.5 13. F. Full Algorithm Algorithm S1 Full FlowEdit algorithm Input: real image src, (cid:8)ti Output: edited image tar Init: FE = src tmax 0 for = nmax to nmin+1 do (cid:9)T i=0, nmax, nmin, navg"
        },
        {
            "title": "Optionally\naverage navg\nsamples",
            "content": ", ti) ti)X src + tiNti + src"
        },
        {
            "title": "X src",
            "content": "(0, 1) Nti src (1 ti tar FE ti ti ti tar(Z tar , ti) ti ti FE FE + (ti1 ti ti1 end for if nmin = 0 then Return: FE 0 = tar 0 src(Z src ti ti)V ti else (0, 1) Nnmin src (1 tnmin FE tar tnmin tnmin for = nmin to 1 do tar ti1 end for Return: tar 0 = tar 0 tar ti tnmin )X src + tnminNnmin + src src tnmin + (ti1 ti)V tar(Z tar ti , ti) end if G. Effect of random noise on the results FlowEdit adds random noise to the source image. If the number of samples over which we average is small, then the algorithm is effectively stochastic. Namely, it produces different editing results with different random seeds. These variations can lead to diverse edits for the same text-image pair. As shown in Fig. S6, the rocks are transformed into different bonsai trees, and the tent appears in various locations within the image. However, these changes can also result in failure cases, as illustrated in Fig. S7. For example, when editing white horse into brown one, the edited results sometimes show the horse with more than four legs or suffer from other artifacts. The editing results in both figures were obtained using SD3 and the hyperparameter mentioned in the main text. Figure S6. FlowEdit diverse results due to different added noise. Figure S7. Failure cases due to different added noise. H. Illustration of the noise-free path between the source and target distributions ) to inv 1 . As explained in Sec. 4 in the main text, the path defined by (7) is noise-free path and constitutes an autoregressive coarseto-fine evolution from src to tar 0 . Figure S8 illustrates this evolution for both synthetic and real images using editing by 0 inversion (top). This path starts from the source image, inv 0 . Moving along this path requires adding the vector (Z src Illustrations of these vectors along the path are shown beneath the images in Fig. S8. Each , tar (Z src It can be seen that for large values, , tar ) image is the result of the difference between the two images above it. (Z src ) contains mainly low frequency components. As decreases, higherfrequency components become increasingly visible. This stems from the fact that (Z src ) corresponds to the difference between tar(Z tar , t). At large t, the noise level is substantial and therefore this vector captures mainly lowfrequency components. As decreases, the vector begins to capture higher-frequency details. This process constitutes an autoregressive coarse-to-fine evolution that starts from src 0 , similarly to the evolution of the diffusion/flow process itself [9]. 0 and ends at tar , t) and src(Z src 1 = src , tar , tar Figure S8 also illustrates the evolution along the FlowEdit path and the (Z src vectors have the same characteristics as in the case of editing by inversion. , tar ) along it. These (Z src , tar ) By arranging the images inv/FE along this path in video, we can animate the interpolation between the source and target images. Additionally, by using the resulting target image as the input for subsequent editing steps, we can create smooth animation that transitions from source image to multiple edits. This interpolation between the source and target image can be seen in Fig. S9 for editing by inversion and in Fig. S10 for FlowEdit. This noise-free path reveals, for example, how gradually tiger becomes bear. Furthermore, the interpolation between cat image and fox image, going through lion, tiger and bear can also be seen at the end of the Supplementary Video. Figure S8. Illustration of the noise free path and . Figure S9. Results of editing by inversion along the noise-free path. The cat image on the top left is used as the input to editing by inversion, where the target prompt is lion. Then, the lion image is used as input and so forth. It can be seen that the edited images do not fully preserve the structure and fine details of the original image, e.g the grass around the cat. Figure S10. Editing results of FlowEdit along the noise free path. The cat image on the top left is used as the input to FlowEdit, where the target prompt is lion. Then, the lion image is used as input and so forth. It can be seen that the edited images preserve the structure and the fine details of the original image, e.g the grass around the cat, even after multiple edits. I. Practical considerations I.1. Effect of navg FlowEdit operates by evaluating ) is used to drive ( our low transport cost ODE in (9). This becomes impractical in cases where model evaluations are expensive (SD3/FLUX). Fortunately, this averaging also occurs across timesteps when Nt is chosen to be independent. So with large enough , we can use smaller navg, reducing expensive model evaluations. Specifically, the default values of for SD3 and FLUX are high enough for our method to perform well with navg = 1, relying purely on averaging across t. at each t. Then, this ( ) on multiple realizations of ˆZ src and ˆZ tar Figure S11 illustrates this effect. As the value of navg increases, the LPIPS distance decreases. We used SD3 and navg values of 1, 3, 5, 10. For large number of discretization and editing steps, i.e. = 50, nmax = 33 (orange curve), the number of averaging steps has little effect on the results, as averaging already occurs between timesteps. Hence, increasing navg only slightly improves LPIPS and has negligible effect on the CLIP score (note that spacing of the horizontal axis ticks is 0.001). These hyperparameters, = 50, nmax = 33, with navg = 1, are the hyperparameters were used in our method. However, for small number of discretization and editing steps, i.e. = 10, nmax = 7 (purple curve), increasing navg has substantial effect. It reduces the LPIPS distance by 0.3 and increases the CLIP score by 0.035. In both experiments, the CFG scales are the same as those described in the main text. Figure S11. CLIP vs. LPIPS for different values of navg. From top to bottom, each marker indicates increasing number of navg steps, between 1, 3, 5, 10. I.2. Connection to Delta Denoising Score (DDS) In general, both FlowEdit (only with navg = 1) and DDS [17] can be viewed as an iterative refinement of given source image towards desired target edit. Each step in both methods consists of difference between two model outputs, applied on source and edited image contaminated with the same noise. In DDS, they showed that using the same noise for both inputs cancels out the unwanted bias present in the models prediction, and only captures the noise-free difference between source and target text prompts. We believe these findings are correct not only for diffusion models but for flow model outputs as well, as illustrated in Figs. 2, S10, S9. We leave thorough empirical and theoretical analysis of this claim for future work. key difference between both methods is that DDS views this process as an optimization problem that minimizes pre-defined loss. In contrast, our method aims to solve an ODE, mimicking simple sampling process instead. To ensure the optimization in DDS converges, the authors utilize techniques such as early stopping, non-trivial learning rate schedules and regularization term. Additionally, the correlation between the optimization objective and the resulting edit quality is not thoroughly explored. This might suggest that optimization-based methods, like DDS, may be implicitly solving an ODE similar to FlowEdit, and should be viewed as such. J. Additional details about the Cats-Dogs experiment In Sec. 5.2 we described experiments evaluating the reduced transport cost of FlowEdit compared to editing by inversion starting with generating synthetic dataset of cat images. To generate the 1000 cat images we used variations of the prompt photo of cat generated by Llama3 [10] 8B instruction model. Specifically, we requested 1000 source prompts describing cat images by providing the instruction: Output 1000 possible prompts to text-to-image model. These prompts should be variation of photo of cat., by adding descriptions and adjectives. We used these 1000 prompts as input to SD3 and generated with them 1000 cats images. Examples of these generations can be seen in the upper part of Fig. S12, covering the blue shape. To create corresponding edited dog images, we applied both editing by inversion and FlowEdit using target prompts identical to the source prompt, except for replacing the word cat with dog. Examples of these editing results can be seen in the middle of Fig. S12, covering the orange shape for FlowEdit results and covering the yellow shape for editing by inversion. Specifically, the four dog images in the middle of each shape are the results of editing the four cat images in the middle of the blue shape. It can be seen that FlowEdit editing are better when compare to editing by inversion results. We also calculate the transport cost between the cats distribution and both dogs distributions. We did it by calculating the average squared distance between SD3 latent representation of the cat images and their paired dog images, for both edits. We also calculated LPIPS between the original cat images and their paired dog images. As presented in Sec 5.2, in both metrics, FlowEdit achieved lower transport cost, compared to editing by inversion. In addition, we generated 1000 dog images using SD3 and the same text prompts used to generate the cat images, but with cat replaced by dogs. Examples of these generations can be seen in the bottom part of Fig. S12, covering the green shape. To asses the alignment between the edited dogs distribution and the generated dogs distributions we used FID and KID scores. As shown in Sec 5.2, FlowEdit achieved lower FID and lower KID scores, indicating our ability to produce images from the target distribution. Figure S12. Illustration of the Cats-Dogs experiment."
        }
    ],
    "affiliations": [
        "Technion Israel Institute of Technology"
    ]
}