{
    "paper_title": "Provably Learning from Language Feedback",
    "authors": [
        "Wanqiao Xu",
        "Allen Nie",
        "Ruijie Zheng",
        "Aditya Modi",
        "Adith Swaminathan",
        "Ching-An Cheng"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Interactively learning from observation and language feedback is an increasingly studied area driven by the emergence of large language model (LLM) agents. While impressive empirical demonstrations have been shown, so far a principled framing of these decision problems remains lacking. In this paper, we formalize the Learning from Language Feedback (LLF) problem, assert sufficient assumptions to enable learning despite latent rewards, and introduce $\\textit{transfer eluder dimension}$ as a complexity measure to characterize the hardness of LLF problems. We show that transfer eluder dimension captures the intuition that information in the feedback changes the learning complexity of the LLF problem. We demonstrate cases where learning from rich language feedback can be exponentially faster than learning from reward. We develop a no-regret algorithm, called $\\texttt{HELiX}$, that provably solves LLF problems through sequential interactions, with performance guarantees that scale with the transfer eluder dimension of the problem. Across several empirical domains, we show that $\\texttt{HELiX}$ performs well even when repeatedly prompting LLMs does not work reliably. Our contributions mark a first step towards designing principled interactive learning algorithms from generic language feedback."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 2 1 ] . [ 1 1 4 3 0 1 . 6 0 5 2 : r a"
        },
        {
            "title": "Provably Learning from Language Feedback",
            "content": "Wanqiao Xu1, Allen Nie1, Ruijie Zheng2, Aditya Modi4, Adith Swaminathan3, Ching-An Cheng4 1Stanford University 2University of Maryland, College Park 3Netflix Research 4Microsoft Research Abstract Interactively learning from observation and language feedback is an increasingly studied area driven by the emergence of large language model (LLM) agents. While impressive empirical demonstrations have been shown, so far principled framing of these decision problems remains lacking. In this paper, we formalize the Learning from Language Feedback (LLF) problem, assert sufficient assumptions to enable learning despite latent rewards, and introduce transfer eluder dimension as complexity measure to characterize the hardness of LLF problems. We show that transfer eluder dimension captures the intuition that information in the feedback changes the learning complexity of the LLF problem. We demonstrate cases where learning from rich language feedback can be exponentially faster than learning from reward. We develop no-regret algorithm, called HELiX, that provably solves LLF problems through sequential interactions, with performance guarantees that scale with the transfer eluder dimension of the problem. Across several empirical domains, we show that HELiX performs well even when repeatedly prompting LLMs does not work reliably. Our contributions mark first step towards designing principled interactive learning algorithms from generic language feedback."
        },
        {
            "title": "1 Introduction",
            "content": "Large language models (LLMs) have reshaped the landscape of how machines learn and interact with the world, demonstrating remarkable capabilities across wide range of tasks (Bommasani et al., 2021; BIG-bench authors, 2023; Anil et al., 2024; Hurst et al., 2024; Jaech et al., 2024; Guo et al., 2025; Yamada et al., 2025). Trained on large corpra of web data, these models can interact with the world through natural language, opening up new settings for sequential decision-making problems. Unlike traditional sequential decision-making approaches where agents learn from scalar reward signals (Sutton and Barto, 2018), LLM can act as agents that interpret and reason with natural language feedback such as critique (Du et al., 2023; Akyürek et al., 2023a), guidance (Branavan et al., 2012; Harrison et al., 2017; Scheurer et al., 2023; Nie et al., 2023; Fu et al., 2024; Wei et al., 2024; Cheng et al., 2024), or detailed explanations (Andreas et al., 2017; Chen et al., 2023; Cheng et al., 2023). Consider an LLM agent that produces summary of story, and receives feedback: The summary is mostly accurate, but it overlooks the main characters motivation. Such feedback conveys notably richer information than numerical score, e.g., 0.7 out of 1, as it identifies specific flaw and suggests direction for improvement. With LLMs abilities to understand and respond in natural language (Touvron et al., 2023), such feedback can be leveraged to drastically increase learning efficiency. This represents fundamental shift in how AI systems can learn through continuous, rich interactions beyond rewards only (Silver and Sutton, 2025). Despite early works on this topic pre-LLM (Gauthier and Mordatch, 2016; Andreas, 2022) and promising recent empirical results in utilizing language feedback for sequential decision-making (Liu et al., 2023; Chen et al., 2024; Xie et al., 2024), rigorous theoretical framework remains lacking. 1 Figure 1: The LLF setup. The environment has hypothesis η representable via text tokens unknown to the agent. Reward as function of η is latent and used only to benchmark the agent via regret to an optimal policy. Feedback as function of η is observed by the agent. Three ingredients are sufficient for no-regret learning: feedback is unbiased (Assumption 3), agent can interpret feedback (Assumption 2), and agent considers hypotheses including η (precursor to Assumption 1). We introduce formal framework of Learning from Language Feedback (LLF), the first mathematical model of learning from language feedback in decision making. The LLF paradigm was proposed in (Cheng et al., 2023) as an interface to benchmark LLM agents learning ability, which generalizes the classical learningfrom-reward reinforcement learning setting to general in-context problem solving by replacing numerical rewards with text feedback. However, it is unclear when LLF is feasible or whether it is harder to learn than the more traditional reward-aware reinforcement learning (RL) setting. Intuitively, one might think language feedback can provide more information to help learning. Indeed, people have empirically found constructive feedback to be more effective for LLM agents to learn from than conveying reward alone in words (Mu et al., 2022; Liu et al., 2024; Zhong et al., 2024; Xie et al., 2024). The complexity and generality of language make it difficult to formally quantify the information in language feedback. The goal of this paper is to provide constructive answers to all the following questions: For general language feedback, can we precisely define helpful and unhelpful feedback? Can we capture the complexity of LLF based on the information in the feedback? Does helpful feedback indeed lead to lower problem complexity? Can we design provably correct algorithm that learns solely from language? To work with the generality of language, we rely on the concept of hypothesis testing and elimination in machine learning (De Jong et al., 1993; Lehmann and Romano, 2022) except with hypotheses that can be expressed in words. We formalize the interface in which agents sequentially interact while reasoning with feedback produced by an underlying hypothesis (summarized by Fig. 1). We also define verifier which evaluates the semantic consistency between candidate hypotheses and observed feedback. Through the notion of hypothesis and verifier, we give precise definition of informative feedback and establish conditions such that LLF is feasible and can be efficiently solved. Specifically, we capture the learning difficulty with new notion of complexity based on eluder dimension (Russo and Van Roy, 2013), which we call transfer eluder dimension. This complexity measure captures how efficiently language feedback can reduce uncertainty about rewards. Building on this concept, we develop HELiX, provably efficient algorithm for LLF. We prove that HELiX achieves regret bound that scales gracefully with the transfer eluder dimension and time horizon , establishing the first formal connection between no-regret learning and language feedback. Crucially, our analysis shows that in certain environments, HELiX can be exponentially more efficient than learning from reward alone. We introduce meta-algorithm that enables LLM to perform inference-time exploration and exploitation using HELiX, inspired by how thinking tokens are used in large reasoning models (LRMs) (Guo et al., 2025). Our algorithm selects actions based on multiple thinking traces, treating them as samples from 2 Figure 2: Thought Sampling with Cross-Verify. Our algorithm extends the traditional paradigm of modelbased exploration to the LLM setting. Here, the model is represented by the LLMs intermediate thoughts, which we interpret as their hypotheses about the external world. We ground this thought-then-act behavior in the interactive decision-making framework and introduce new algorithm that conducts efficient exploration from language feedback. space of hypotheses, while evolving this hypothesis space using past observations (see Figure 2). We empirically validate the efficacy of our implementation on Wordle, Battleship and Minesweeper. We show that HELiX and its variants consistently outperform in-context learning LLM baselines. Altogether, our work contributes first principled framework for understanding and designing learning agents guided by language."
        },
        {
            "title": "2 Formulating Learning from Language Feedback",
            "content": "In this section, we give formal mathematical model to describe the LLF process (illustrated by Fig. 1) and introduce natural assumptions to frame the learning problem so that LLF can be rigorously studied. In what follows, we first define the interaction setup. Then we introduce the notion of text hypotheses for world modeling. Finally, we define the verifier to evaluate hypothesis-feedback consistency, which later gives measure on the informativeness of feedback. These constructions provide basis for studying LLFs learnability and analyzing regret in the next section."
        },
        {
            "title": "2.1 Formal Setup of LLF",
            "content": "Let be finite set of tokens. We denote the set of all finite token sequences by + = k1T {}, where denotes the set of length-K token sequences. There is set + of token sequences that we refer to as the feedback space. For an arbitrary set , we use (X ) to denote the set of all probability distributions with support on . We define the problem of Learning from Language Feedback (LLF)1 with finite action set A. At time step t, the agent interacts with the environment by executing an action At and observing feedback Ot sampled from feedback distribution : (O); reward Rt = r(At) is incurred, based on reward function : [0, 1], though Rt is not revealed to the agent. Here we suppose the reward is generated by deterministic function r; our results can be extended to stochastic rewards. policy is distribution on A. We denote Π = (A) and the agents policy at time step for sampling At as πt. We measure the performance 1In the original formulation in (Cheng et al., 2023), problem context is given before learning to provide background to interpret feedback. We omit writing the problem context for simplicity but equivalently assume that the agent can interpret the feedback through the verifier that we will introduce later. 3 of the agent in the LLF setup by regret, which is defined as Regret(T ) = PT 1 the total number time steps, and the algorithms inner randomization. max Eπt [Rt] , where is max = maxaA r(a), and the expectation is taken over feedback randomness t=0 This setup is similar to bandit problem in RL, and the goal of the agent is to find actions that maximize the reward. However, unlike RL, here the agent does not observe the rewards {Rt}, and must learn to maximize the reward solely using natural language feedback {Ot}. Remark 1. The setup above can be naturally extended to contextual setting (an analogy of contextual bandit problems; please see Appendix C.2 for details), where the agent receives context in each time step before taking an action. While the feedback in the context-less setting here may be viewed similar to context, the main difference is that the optimal actions in the context-less setting do not change between iterations; on the other hand, in the contextual setting, the optimal actions in each time step depend on the context presented to the agent at that point."
        },
        {
            "title": "2.2 Environment Model and Text Hypothesis",
            "content": "The environment in the LLF setup is defined by feedback function : (O) and reward function : [0, 1]. We suppose they are parameterized by some text description, which we call hypothesis, belonging to (possibly exponentially large) hypothesis space +. One can think of hypothesis as describing the learning problem and mechanism of generating feedback in texts such as natural language or codes. For example, in recommendation environment, hypothesis can be text description of users interests, e.g., the user enjoys fantasy movies produced in the 21st century...; in video game environment, hypothesis can describe the games code logic, in the form of <rule of the game><inferred hidden game state><inferred reward mechanism>. hypothesis can also represent finite-sized numerical array (e.g., neural network weights) along with operations to decode it into reward and feedback. In short, hypothesis is sufficient text description of the learning problem such that the reward and the feedback functions can be fully determined. With the hypothesis space H, we model the feedback mechanism through feedback mapping η 7 fη that maps each hypothesis η to feedback function fη : (O). Similarly, we model reward mapping η 7 rη that maps hypothesis η to reward function rη : [0, 1]. We denote by η the true hypothesis of the environment, and use shorthand = fη and = rη. This construction is reminiscent of classical bandit settings where the reward function is parameterized, such as the linear case r(a) = ϕ(a)θ for some known feature map ϕ and unknown ground-truth parameter θ. We generalize this by using the reward mapping η 7 rη as an analogue of the feature map and the hypothesis η as the parameter. Following the convention in the literature, we assume that the parameterization, i.e., the reward mapping η 7 rη, is known to the agent, but the parameter η is unknown. See Fig. 1 for an overview. Assumption 1. We assume that the agent has access to the reward mapping rη : η 7 rη. In practice, the reward mapping can be implemented using an LLM to process given hypothesis text, e.g., to tell whether an action is correct/incorrect (Zheng et al., 2023; Weng et al., 2023; Gu et al., 2024). We do not assume knowing the feedback mapping η 7 fη, however, as precisely generating language feedback in practice is difficult."
        },
        {
            "title": "2.3 Measuring Information in Feedback",
            "content": "Without any connection between feedback and reward, learning to minimize regret from feedback is impossible. Intuitively, for LLF to be feasible, language feedback must contain information that can infer the solution, like reward, action rankings, or whether an action is optimal. To study LLF learnability, we need 4 way to quantify this information. Since it is impossible to categorize and enumerate all possible language feedback in general (i.e., we cannot always embed language feedback into finite-dimensional vector), we adopt weak, implicit definition of information based on sensing function. We introduce the notion of verifier to formalize information the agent can extract from feedback. The verifier represents mechanism that assesses whether hypothesis is consistent with observed feedback given to an action; for example, verifier implemented by an LLM may rule out hypotheses that are semantically incompatible with feedback observations. Assumption 2 (Verifier). We assume that there is verifier, which defines loss ℓ : [0, 1], and the agent has access to the verifer through ℓ. For any action A, feedback and hypothesis η H, the value ℓ(a, o, η) quantifies how well η aligns with the feedback on action a. If η is consistent with on action a, then ℓ(a, o, η) = 0; otherwise, it returns non-zero penalty. concrete example may help ground this abstract assumption. Suppose the agent chooses an action corresponding to text summary of story, and receives feedback in the form of text critique, such as: The summary is mostly accurate, but it misses an important detail about the main characters motivation. Suppose each hypothesis η corresponds to set of rubrics to judge summaries. verifier must output score ℓ(a, o, η). If rubric η implies that summaries should capture the main characters motivation, then ℓ(a, o, η) = 0, indicating consistency. Otherwise, the loss value is positive. Such verifier can be implemented by prompting an LLM to assess whether the feedback is consistent with applying rubric η to the summary a. The set of feedback-consistent hypotheses naturally captures information in the feedback. Ideally, feedback generated from fη() should be self-consistent, i.e., EOfη(a)[ℓ(a, O, η)] = 0 for all and η H. However, in practice, both the feedback and the verifier may be noisy or imperfect and there may be some such that EOf (a)[ℓ(a, O, η)] > 0. To accommodate this potential noise while preserving learnability, we adopt weaker assumption than self-consistency: although the feedback may be noisy, it is unbiased such that each hypothesis minimizes the expected verifier loss under its induced distribution. EOfη(a)[ℓ(a, O, η)]. Assumption 3 (Unbiased Feedback). For all and η H, η arg minηH The notion of verifier can be used to formalize semantic equivalence among hypotheses. In natural language, many token sequences share the same underlying semantic meaning. For LLF, such distinctions are not meaningful and should not affect the learning outcome. This invariance can be captured by the verifier introduced above. We deem hypotheses as equivalent whenever they induce identical loss functions across all inputs. We use this to define the geometry of the hypothesis space. Definition 1 (Hypothesis Equivalence). We define the distance between two hypotheses η, η as dH(η, η) := supaA,oO ℓ(a, o, η) ℓ(a, o, η). If dH(η, η) = 0, we say η and η are equivalent. This definition provides criteria to determine the equivalence of hypotheses, as two hypotheses with zero distance are indistinguishable from the agents perspective. In applications involving LLM-generated feedback, the loss function ℓ can be designed to reflect semantic similarity, e.g., by assigning similar values to outputs that are paraphrases of one another, based on token-level matching, embedding-based metrics, or LLM-prompted judgments (Wang and Yu, 2023; Chuang et al., 2022; Asai and Hajishirzi, 2020; Bubeck et al., 2023). Remark 2. Readers familiar with reinforcement learning from human feedback (RLHF) or AI feedback (RLAIF) may wonder if such loss structure is necessary. Indeed, one may alternatively define scoring function : [0, 1] that directly evaluates an action-feedback pair and impose some relationships 5 between the scoring function and the underlying reward. This construction is special case to our framework, which we discuss in detail in Section 3.3."
        },
        {
            "title": "3 Learnability and Provable Algorithm",
            "content": "Compared to numerical reward signals, feedback can potentially carry more information. In LLF, to interpret this feedback and guide learning, the agent is equipped with: 1) The verifier loss function ℓ and 2) The reward mapping η 7 rη. This structure reflects central feature of LLF: the agent must reason over the hypothesis space via the verifier to minimize regret defined by the hidden rewards. But can an agent learn to maximize reward despite not observing it? For instance, if feedback does not convey useful information for problem solving, it is unrealistic to expect any learning to happen. On the other hand, if feedback directly reveals the optimal action, then the problem can be solved in two steps. Naturally, one would expect the learnability and complexity of LLF problems to depend on the information that feedback conveys. The goal of this section is to give natural structures and assumptions to the LLF setup that characterizes the difficulty of the learning problem."
        },
        {
            "title": "3.1 Transfer Eluder Dimension",
            "content": "To quantify information in the feedback, we utilize the verifier, introduced in Section 2.3, to propose new complexity measure called transfer eluder dimension based on the eluder dimension (Russo and Van Roy, 2013). At high level, transfer eluder dimension characterizes how effectively information in the feedback reduces uncertainty about the unknown reward function. When it is small, single piece of feedback carries lot of information about the reward, which enables LLF to be much more efficient than learning from reward. (a) := minη EOfη(a)[ℓ(a, O, η)]. Given verifier loss ℓ, an action is Definition 2. Define ℓmin ϵ-transfer dependent on actions {a1, . . . , an} with respect to if any pair of hypotheses η, η satisfying Pn ϵ2, also satisfies rη(a) rη(a) ϵ. Further, is ϵ-transfer independent of {a1, . . . , an} with respect to if is not ϵ-transfer dependent on {a1, . . . , an}. (cid:16)Eofη (ai)[ℓ(ai, o, η)] ℓmin (cid:17) η (ai) i=1 η Intuitively, this definition says that an action is transfer independent of {a1, . . . , an} if two hypotheses that give similar feedback according to the verifier at {a1, . . . , an} can differ significantly in their reward predictions at a. This differs from the original definition of eluder dimension (Definition 4), which measures discrepancies in both the history and new observation using reward. Our goal is accurate reward prediction, not feedback recovery. This intuition motivates the definition of the transfer eluder dimension. Definition 3 (Transfer eluder dimension). The ϵ-transfer eluder dimension dimT E(H, ℓ, ϵ) of with respect to the verifier loss ℓ is the length of the longest sequence of elements in such that, for some ϵ ϵ, every action element is ϵ-transfer independent of its predecessors. Unlike the eluder dimension, transfer eluder dimension measures dependence based on two quantities: the verifier loss and the reward function. This extension allows us to capture information in the feedback relevant to reward learning. Later in Section 3.4, we will present provable algorithm that attains sublinear regret rate in LLF in terms of the transfer eluder dimension. 3."
        },
        {
            "title": "Informative Feedback Reduces Learning Complexity Exponentially",
            "content": "We discuss several example forms of feedback and compute the corresponding transfer eluder dimensions. The nature of feedback critically affects learning efficiency: uninformative feedback (e.g., random text) leads to infinite transfer eluder dimension, while some feedback provides more information than reward 6 and accelerates learning. For example, in constraint satisfaction problem, feedback that reveals satisfied constraints can shrink the set of potentially true hypotheses. In the toy example below, reward-only learning requires exponential time (2L), whereas the transfer eluder dimension is 1: LLF gives an exponential speedup. Example 1 (Bitwise feedback on 0-1 string). Consider an action set = {0, 1}L. The space of hypotheses contains all possible length-L 0-1 strings. Each hypothesis η contains particular fixed target string s(η) and the corresponding text instruction to provide reward and feedback about the target. The reward function rη corresponding to hypothesis η is such that r(a) = 1 if = s(η) and r(a) = 0 otherwise. In other words, rewards are sparse and every suboptimal arm incurs regret of 1. Feedback to an action = (a1, . . . , aL) is bitwise, which tells in words the correctness of each bit in the 0-1 string (i.e. whether ai = si for s(η) = (s1, . . . , sL). Equivalently, we can abstract the feedback as fη(a) = (1{ai = si})L and define the loss function ℓ(a, o, η) = 1 the feedback and the correctness indicated by hypothesis η. For any ϵ < 1 dimT E(H, ℓ, ϵ) = 1, as for any action a, the expected loss EOfη (a)[ℓ(a, O, η)] < 1 We can also use feedback to reveal information e.g. about the optimality of selected actions, improving directions, or explanation of mistakes. Below we use an example to illustrate how different forms of feedback can drastically change the problem complexity. i=1 1{oi = 1{ai = si}} to measure the discrepancy between , the transfer eluder dimension iff η = η. PL i=1 1, . . . , Example 2 (Reasoning steps). Consider math reasoning problem where one tries to construct hidden L), where each si + is token sequence that represents sequence of L-step reasoning = (s correct reasoning at step i, and is finite set of token sequences that represent possible reasoning steps. k=1(T +)k consists of all possible reasoning of steps. Each hypothesis represents The action set = full solution to the problem and rubrics to critique partial answers with. Reward is 1 if all steps are correct and 0 otherwise. Below we show the transfer eluder dimension with ϵ < 1 2L for different feedback (see Appendix B.4 for the exact forms of verifiers and proofs). We consider four feedback types, which corresponds to the reward, hindsight-negative, hindsight-positive, and future-positive feedback, respectively, in the LLFs feedback taxonomy proposed in (Cheng et al., 2023). Directly learning from rewards incurs exponential complexity, as the agent must enumerate all possible sequences. Feedback that identifies the first mistake enables stage-wise decomposition and yields exponential improvement in L, though each stage still requires brute-force search. If the feedback is more constructive, showing not only where the fist mistake is but also how to correct for it, the problem complexity does not depend on S. Finally, if the feedback tells the answer right away, the complexity becomes constant, as the agent can learn the solution immediately after one try. Feedback 1. (reward) binary indicator of whether all steps are correct 2. (explanation) index of the first incorrect step 3. (suggestion) give correction for the first mistake 4. (demonstration) all the correct steps dimT E(H, ℓ, ϵ) O(SL) O(SL) O(L) O(1)"
        },
        {
            "title": "3.3 Learning from Informative Feedback Is No Harder Than Learning from Reward",
            "content": "We have shown examples where the transfer eluder dimension is bounded and decreases as the feedback provides more information than reward. Here we prove the generality of this observation. Below we show that if the feedback contains reward information, then the transfer eluder dimension of LLF is no larger than the traditional eluder dimension of RL in Definition 4. Definition 4 (Eluder Dimension). An action is ϵ-dependent on actions {a1, . . . , an} with respect to reward class if any r, satisfying Pn i=1 (r(ai) r(ai))2 ϵ2, also satisfies r(a) r(a) ϵ. 7 Further, is ϵ-independent of {a1, . . . , an} if it is not ϵ-dependent on {a1, . . . , an}. The ϵ-eluder dimension dimE(R, ϵ) of is the length of the longest sequence of elements in such that, for some ϵ ϵ, every action element is ϵ-independent of its predecessors. First, by using the verifier, we define the statement feedback to contain reward information. Definition 5 (Feedback containing reward information). The feedback function fη is reward-informative of rη with respect to the verifier ℓ if there is CF > 0 such that η H, A, rη(a) rη(a)2 CF Eofη(a)[ℓ(a, o, η) ℓmin (a)]. We say an LLF problem is reward-informative if (f , r, ℓ) satisfies the above condition. η This assumption states that the verifier can distinguish hypotheses based on feedback to the same extent as their reward differences. In other words, if two hypotheses differ in their corresponding rewards, then from the verifier can tell they are different. Therefore, standard RL problems are special case of reward-informative LLF problems. An reward-informative example is when the unobserved reward is function of the feedback. Concretely, (cid:2)g(a, o)(cid:3) for some known : [0, 1]. Note that the reward mapping suppose rη(a) = Eofη(a) η 7 rη is known, but the reward function itself is still hidden from the agent (since η is unknown). Consider ℓ(a, o, η) := (g(a, o) rη(a))2 = (g(a, o) Eofη(a)[g(a, o)])2. Then one can verify that Eofη(a)[ℓ(a, o, η)] and show that this feedback-verifier pair is reward-informative. (see η arg minηH Appendix B.3). In addition to this example, one can check that the forms of feedback used in Section 3.2 are reward-informative too. Note that reward-informative feedback can also contain information other than reward as shown in Section 3.2. With this definition in place, we show that if feedback contains reward information, the transfer eluder dimension is no larger than the eluder dimension for the reward class induced by H. Proposition 1. For reward-informative LLF problems with CF as in Definition 5, it holds that dimT E(H, CF ℓ, ϵ) dimE(RH, ϵ), where RH = {rη : η H} is the effective reward class of H. Proposition 1 implies that reward-informative LLF problems are no harder than their reward-only counterparts, such as those solved by the standard UCB algorithm over the reward class RH using reward extracted from the language feedback by some LLM."
        },
        {
            "title": "3.4 HELiX Algorithm",
            "content": "To validate our characterization of learnability based on the transfer eluder dimension, we design simple UCB-style algorithm, HELiX, outlined in Algorithm 1. HELiX uses feedback to guide exploration using the optimism principle (Auer et al., 2002). As concrete instantiation of how our conceptual framework can inform algorithmic design, HELiX serves as sanity check that LLF problems with finite transfer eluder dimensions can indeed be solved provably efficiently, with regret guarantee that depends sublinearly on the transfer eluder dimension. Theorem 1. Under Assumption 1 and Assumption 2, for all N, the regret of HELiX satisfies Regret(T ) eO (cid:18) 3/4 (cid:16) log (H, ϵH , dH) (cid:17)1/4 dimT E(H, ℓ, ϵH ) (cid:19) , where (H, ϵH denotes the ϵH η = η}(cid:9). , dH) denotes the ϵH -transfer eluder dimension of H, and ϵH -covering number of based on the pseudo-metric dH, dimT E(H, ℓ, ϵH ) 2 , minaA inf{rη(a) r(a) : η H, = max (cid:8) 1 8 Algorithm 1 HELiX: Hypothesis Elimination using Language-informed Exploration (Theoretical Version) 1: Input A, O, , reward mapping η 7 rη, verifier loss ℓ : [0, 1], confidence levels {ϵt}T 1 t=0 2: Initialize = 0, A0 Unif(A), H0 = 3: for = 1, . . . , do observe Ot1 4: 5: Ht Ht1 T{η : 1 ℓ(Ai, Oi, η) minηH rη(πη) rη(π) max ηHt (πp, ηp) arg min πΠ if rηp(πηp) rηp(πp) = 0 then 1 ℓ(Ai, Oi, η) ϵt} At πp() // Exploitation step: exploit if there is consensus else (πo, ηo) arg max πΠ max ηHt rη(π) At πo() // Exploration step: UCB-inspired 6: 7: 8: 9: 10: 11: end if 12: 13: end for While the order eO(T 3/4) on the time horizon may appear suboptimal compared to classical eO( ) optimal rates for bandit learning with direct reward feedback, this slower rate is in fact principled consequence of our minimal assumptions. Specifically, our analysis makes no structural assumptions on the verifier loss ℓ beyond boundedness. If we have more structural knowledge of ℓ, say, that it is α-strongly convex, then the bound can be tightened to match the optimal order eO( ). We defer detailed treatment of these improvements to Appendix A. We now describe the main components of HELiX. Given hypothesis η H, let πη denote its optimal policy. At each step t, the algorithm maintains confidence set Ht consisting of hypotheses that remain approximately consistent with observed actions and feedback, as measured by cumulative verifier loss. The algorithm then identifies an optimistic hypothesis ηo that achieve maximal optimal reward, and follows an optimal policy πo under this hypothesis. We call this step the exploration step. An additional design in HELiX compared to standard UCB is an explicit exploitation step. It checks for consensus optimal action among all hypotheses in the confidence set. This is accomplished by computing pessimistic policy πp and corresponding hypothesis ηp. If the minimax regret minπΠ maxη rη(πη) rη(π) = 0, then the minimizer policy πp only selects actions that are simultaneously optimal for all candidate hypotheses (see Lemma 5). This ensures that our algorithm avoids over-exploration when feedback about the set of optimal actions is underspecified. For instance, as discussed in Section 3.3, feedback in trivial LLF problem can directly reveal the optimal action but nothing about the reward. If this is the case, the exploitation step ensures that the algorithm will not over-explore when it is certain that some action is optimal. In Lemma 6, we show that once such consensus optimal action is found, the algorithm will stop exploring. As result, the regret of HELiX never exceeds that of the same algorithm without the exploitation step. Directly querying LLM for an action by prompting with the interaction history (with the lowest temperature) ℓ(Ai, Oi, η). would be similar to drawing actions from πη where η is randomly sampled from arg minηH In the classical RL setting, such greedy algorithm does not explore and therefore does not always have low-regret. Since RL is special case of reward-informative LLF, we conjecture that this greedy algorithm also does not have regret guarantees for general LLF. We will compare this baseline in all of our experiments and confirm that HELiX reliably outperforms this baseline."
        },
        {
            "title": "3.5 Proof Sketch for Theorem 1",
            "content": "We sketch the proof of the general argument in Theorem 1 in four main steps. The full proof is presented in Appendix A. Step 1: Define confidence sets For each hypothesis η H, we define the cumulative population prediction error Lt(η) = Pt1 and the cumulative empirical verifier loss Lt(η) = i=0 Pt1 i=0 ℓi(η). We define confidence sets Ht = {η : Lt(η) minηH Lt(η) + βt} (cid:16)EOfη (Ai)[ℓ(Ai, O, η)] ℓmin i=0 ℓ(Ai, Oi, η) = Pt1 η (Ai) (cid:17) where βt is confidence parameter. Step 2: Regret decomposition We let the width of subset at an action be wV (a) = supηV rη(a) r(a). Then, we can decompose the regret in terms of version space widths: Regret(T, η) PT 1 t=0 [wVt(At) 1{η Vt} + 1{η / Vt}] . Step 3: Bounding the sum of widths via transfer eluder dimension The key step is to show that if the width wHt(At) > ϵ for some ϵ > 0, then At must be ϵ-dependent on only O(βt/ϵ2) disjoint historical action sequences, where βt is the confidence parameter. By the definition of the transfer eluder dimension dT = dimT E(H, ℓ, ϵ), in any sequence of actions, there must be some action that is ϵ-dependent on at least Ω(N/d) previous ones. Combining these facts forces the number of large-width version spaces PT 1 1{wHt(At) > ϵ} to be bounded by O(βT d/ϵ2). Rearranging terms and choosing suitable sequence t=0 of ϵ gives that with high probability, PT 1 3dT EβT ). Note that when the t=0 wVt(At) O(dT + 2 exploitation step is triggered, the per-step regret of all following steps become zero (Lemma 6), and so the regret of HELiX is always bounded above by that without the exploitation step. Step 4: Prove high-probability confidence set concentration It remains to define suitable βts and show that η Vt for all with high probability. Depending on what structural assumptions are known for the verifier loss ℓ, we determine the rate of decay of βt. If we only make the minimal assumption that ℓ is bounded, then βT = eO( ). Putting everything together proves Theorem 1."
        },
        {
            "title": "4 Implementing HELiX with Large Language Models",
            "content": "We provide practical implementation of HELiX using an LLM. LLMs with advanced reasoning capabilities can produce chain-of-thoughts that often contain guesses and reasoning traces of the environment (Wei et al., 2022; Guo et al., 2025; Gandhi et al., 2025). We propose to leverage LLMs knowledge about the world to enhance decision-making. In particular, we treat an LLMs thinking tokens before deciding on an action as hypotheses. These thinking tokens can be sampled by prompting the LLM to output its reasoning before an action with prompts in the form of <Thought> <Action>. We provide the pseudocode for the practical implementation in Algorithm 2 and illustrate corresponding flow-graph in Figure 2. The algorithm takes as inputs the following LLM-based components: 1. πLLM : t=0(A O)t (H A). This is an LLM with chain-of-thought prompt that asks it to analyze the current observation through thinking tokens and produce valid action. We may view this policy as producing the best action conditioned on hypothesis consistent with the feedback history. 2. πref : (A). This is user-provided reference policy to sample actions, analogous to baseline policy. The design of reference policy may vary. In this work, we adopt random reference policy by asking an LLM to produce set of random actions that are different from those generated by πLLM. 3. RLLM : [0, 1]. This is reward mapping to evaluate how good/bad the action is under given hypothesis. We implement this by prompting an LLM to score an action conditioned on Algorithm 2 HELiX (Practical Version with LLMs) 5: 6: 1: Input , πLLM, πref , RLLM, N, 2: initialize A0, η0 πLLM() 3: for = 0, 1, . . . , 1 do execute At, observe Ot 4: ˆAt, ˆHt {πLLM({Aτ , Oτ }t τ =0) = 1, . . . , } // Sample thought-action ˆAt ˆAt {πref () = 1, . . . , } // Sample random actions from πref // Thought-action cross-verify (for checking if Exploitation step should be triggered.) compute score matrix [St] where [St]η,a RLLM(η, a) for ˆAt, η ˆHt ˆA arg max [St]η // Set of actions optimal to all hypotheses in ˆHt. if ˆA η ˆHt = then At+1 tie-break-choose( ˆA ) // Exploitation step: check consensus 9: 8: 7: else (cid:16) (cid:17) Ht arg maxη ˆHt At+1 arg maxa ˆAt max [St]η (cid:16) maxη Ht // Exploration step: UCB-inspired [St]η,a Eaπref (cid:2) [St]η,a (cid:3)i(cid:17) 10: 11: 12: 13: 14: end if 15: 16: end for sampled hypothesis. This can be viewed as hypothesis-conditioned reward model. Approximation of Feedback-consistent Hypotheses and Policy Space. HELiX (Algorithm 1) maintains hypothesis space Ht at iteration t, which contains all hypotheses η that are consistent with observed feedback. Then, HELiX searches over all possible policies by computing πp and πo. We approximate these two steps with finite sets of candidates, ˆHt and ˆAt, respectively. We make the assumption that state-of-the-art LLMs are capable of producing valid hypotheses when instructed with chain-of-thought prompt and history. In other words, they provide hypotheses that are plausible explanations of the interaction history of actions and feedback. At each step, we use πLLM to produce thought-conditioned actions. We first ask the LLM to generate diverse set of hypotheses. For each hypothesis, we prompt the LLM to generate corresponding optimal actions. Unlike common chain-of-thought approach that asks LLM to produce only one thought and one action, we ask the LLM to output thoughts and actions. This set of thoughts accounts for the agents uncertainty about the environment. In addition, we use πref to propose random valid actions. For computational efficiency, we sample these hypotheses and actions in one LLM call rather than calls, introducing conditional dependencies between them (the same holds when sampling the random actions). These LLM calls produce an approximate hypotheses space ˆHt of size and an approximate policy space ˆAt (of deterministic actions) of size + . Thought Cross-verify. In Algorithm 2, we approximate the minimax and maximization steps in Algorithm 1 with ˆHt and ˆAt. Concretely, we construct score matrix St [0, 1]N (N +M ) whose entries [St]η,a correspond to the reward of hypothesis-action pairs (η, a). The rows of this score matrix correspond to hypotheses in ˆHt and columns correspond to actions in ˆAt. This matrix is visualized in the middle portion in Figure 2. We use the reward mapping RLLM to produce scores. The diagonal entries of St are close to 1.0 because the action ai conditionally sampled from ηi should be scored the highest under ηi. If some action is deemed optimal across all sampled hypotheses, we follow this consensus choice (Fig. 3 Stage 1). Conversely, when the hypotheses disagree, we select the most optimistic action to encourage exploration (Fig. 3 Stage 2). This distinction between consensus and disagreement forms the backbone of our explorationexploitation 11 Figure 3: HELiX Algorithm. The HELiX algorithm has three steps. First, if the highest-scoring actions across all generated hypotheses coincide, the algorithm performs an exploitation step. Otherwise, it performs an exploration step by retaining only the hypotheses whose optimal actions achieve the highest scores. In the absence of random policy πref , HELiX chooses an action using predefined tie-breaking rule. When random policy πref is available, the algorithm adjusts the score of each action by subtracting the average score of actions under πref . In the example above, A3 and A4 are random actions sampled from πref. strategy. Exploitation Step. Given the score matrix St, we first check whether the exploitation step in Algorithm 1 is triggered. Specifically, if given action satisfies RLLM(η, a) RLLM(η, a) for all η Ht and At, then is identified as consensus action and exploited immediately. This corresponds to the exploitation step in the theoretical Algorithm 1. By Lemma 5, if an action solves the minimax problem, it must also be an optimal action for all remaining hypotheses simultaneously. If there are multiple consensus actions, we perform tie-breaking detailed below. In Figure 3, this step is implemented as set intersection operation over the sets of highest-scoring actions from each hypothesis. Exploration Step. If no consensus action exists, we conduct the exploration step in Algorithm 1. We first eliminate hypotheses whose highest score is lower than those of other hypotheses. This implements exploration using the optimism in the face of uncertainty principle (Auer et al., 2002), where we only keep the most optimistic hypotheses. After the hypothesis elimination, if only one hypothesis remains, then we execute the best action under that hypothesis. If there are more than one hypothesis left, we apply tie-breaking step by re-scoring with reference policy. The re-scoring or re-centering step is widely used in RL, such as baseline methods (Weaver and Tao, 2013; Sutton and Barto, 2018), ReMax (Li et al., 2023), RLOO (Ahmadian et al., 2024), and GRPO (Guo et al., 2025). This procedure interprets the score as the advantage of an action relative to those sampled from reference policy πref , under given hypothesis η. There are multiple reasons why an advantage is useful for tie-breaking: 1) LLMs may not score consistently across hypotheses. Comparing score differences can help cancel out these inconsistencies. 2) When we use uniformly random πref , the advantage implicitly examines the quality of the hypotheses and favors more discriminative ones. permissive hypothesis that assigns approximately the same score to all actions (e.g., Fire shot anywhere on the map) lacks discriminative power. In contrast, discriminative hypothesis assigns higher scores to actions that align with its intent (e.g., Fire shot along the edge of the map), yielding higher advantage over random actions. With re-scoring, we favor actions with high advantages over random actions. 12 Note on Tie-breaking. If ties remain after re-scoring, we further tie-break by preferring hypotheses and actions generated earlier in the output. This is due to empirical observations that LLMs have preference to produce the best plan and action first, followed by less likely plans and actions (Dracheva and Phillips, 2024)."
        },
        {
            "title": "5 Experiments",
            "content": "In this section, we present experiment details of HELiX in three environments that require learning from language feedback."
        },
        {
            "title": "5.1 Baselines",
            "content": "We consider the following agents for comparison. In addition to HELiX, we implement two of its variants with slightly different action selection procedures. Greedy. This agent generates one hypothesis and one action, and returns that action immediately. This is ReAct-style baseline. HELiX (No exploitation step). This baseline agent conducts optimistic exploration without the consensusbased exploitation step. We demonstrate that optimistic exploration alone is insufficient in our setup. We use thought sampling to generate + actions and hypotheses, followed by cross-verification that scores each action under every hypothesis. Unlike in HELiX, we directly select actions with the highest score across all hypotheses. If there are multiple actions, we tie-break by preferring hypotheses and actions generated earlier in the output. HELiX (No πref ). This variant of HELiX includes thought sampling, cross-verification, and the exploitation step, but omits the re-scoring step using πref . If the exploitation step is not triggered, we perform the exploration step without re-scoring using random actions sampled from πref . The benefit of using πref is entirely empirical and depends on its specific instantiation."
        },
        {
            "title": "5.2 Experimental Setup",
            "content": "We conduct experiments in the following three gym environments proposed in Tajwar et al. (2025). WORDLE In each scenario, the environment selects secret 5-letter word from predefined dictionary. The agent attempts to guess the word, receiving feedback after each guess indicating correct letters and their positions. In our experiment, we used 50 scenarios to evaluate all agents. To better illustrate Example 2 in Section 3.2, we modify the feedback from the original environment to only contain information about the first incorrect character. For example, if the target word is totem and the agents guess is apple, the feedback is The first letter is incorrect. Considering that this feedback provides less information than typical feedback in wordle, we allow the agents to make 10 attempts before termination. BATTLESHIP Battleship is 2D grid environment where the agent must locate and sink three hidden ships within 20 turns. The agent fires at one cell per turn and receives hit/miss feedback, ship type (5-cell ship, 4-cell ship, and 3-cell ship), and map showing all previous hits and misses. Success requires strategic exploration to find ships and exploitation to sink them efficiently. We use 20 scenarios (maps of ship layout) to evaluate all agents. We use hidden per-step reward to evaluate an agents performance. For instance, the feedback ship was hit but not sunk corresponds to 0.5 point. We do not communicate this numerical reward information to the agent. MINESWEEPER Minesweeper is 2D grid puzzle with hidden mines. At each turn, the agent chooses to reveal one cell, aiming to uncover all safe cells within 20 turns without hitting mine. Revealed cells show 13 (a) Wordle (Modified Feedback) (b) Battleship (c) Minesweeper Figure 4: We show the cumulative reward that the agent is able to obtain during fixed number of interactions with the environment. Shaded area represents the standard error of cumulative reward across different scenarios. We use Claude-Sonnet-3.5 v2 for the experiment. the number of adjacent mines, and 0 triggers automatic revelations of surrounding safe cells. Sequential reasoning and updating of hypotheses based on observed clues are essential for success. Hidden rewards are calculated by assigning 0.2 to choosing square that does not have mine, and 1.0 to fully solving the game. Invalid moves incur -0.2 penalty. The agent receives feedback in the form of partially revealed map after each action."
        },
        {
            "title": "5.3 Discussion of Results",
            "content": "We plot the cumulative reward as function of the number of environment interaction steps on WORDLE, BATTLESHIP, and MINESWEEPER in Figure 4. We see that for all three environments, the base LLM, where we only greedily choose the first action, performs worse generally. In environments where informationgathering is more necessary, such as BATTLESHIP and MINESWEEPER, agents designed to conduct strategic explorations and exploitations tend to outperform the greedy base LLM by large margin. As shown, HELiX consistently outperforms both the greedy baseline and HELiX variants: HELiX (no exploitation step) and HELiX (no πref ). In particular, in BATTLESHIP and MINESWEEPER, HELiX performs significantly better than the baselines. Although the theoretical version of our algorithm does not use πref , we have found that across these three environments, performing an explicit re-scoring is beneficial. Although the initial results are promising, our practical implementation relies on assumptions that warrant discussion. We assume that the LLM can select an optimal action under given hypothesis. We also assume that the LLM can produce fair scores across hypotheses for different actions. However, these assumptions may not hold for all LLMs (Shojaee et al., 2025), and further investigation is needed to validate them. Additionally, to capture the agents uncertainty about the environment, we sample set of hypotheses from the LLM. These hypotheses should be both diverse and faithful in reflecting the history of interactions. The extent to which existing LLMs can propose plausible hypotheses given historical information remains uncertain, with evidence pointing in both directions (Zhou et al., 2024; Si et al., 2024; Ghareeb et al., 2025). Our theory-inspired algorithm highlights key properties an LLM must exhibit to function effectively as decision-making agent, one that autonomously learns from environment feedback, proposes hypotheses, and explores accordingly. Further research is needed to verify whether current LLMs possess these properties, and, if not, to determine what forms of training could instill them."
        },
        {
            "title": "6 Related Work",
            "content": "While using LLMs for general problem solving has been studied for long time (Xie et al., 2022; Guo et al., 2024; Akyürek et al., 2023b), relatively fewer prior works studied the use of LLMs for sequential decision-making. There are roughly two routes to improving the agents performance with language feedback. One is to directly deploy LLMs as agents in decision-making problems by incorporating feedback into subsequent prompts or an external memory buffer (Yao et al., 2023; Brooks et al., 2023; Shinn et al., 2023; Wang et al., 2024; Krishnamurthy et al., 2024; Nie et al., 2024; Xi et al., 2025). Another route is to process this feedback and use it to finetune models weights (Chen et al., 2024; Scheurer et al., 2022; Raparthy et al., 2023; Lee et al., 2023; Qu et al., 2025). This approach requires considerable amount of offline interaction data. More recent work has investigated more sophisticated methods to improve exploration with LLMs, such as directly learning exploration behavior through supervised fine-tuning (Nie et al., 2024), preference-based learning (Tajwar et al., 2025), or reinforcement learning (Schmied et al., 2025), or prompting LLMs to mimic perfect Bayesian learner (Arumugam and Griffiths, 2025). However, these results have been empirical. We aim to bridge this gap by introducing formal framework and guarantees for learning from language feedback. Our framework is closely related to multi-armed bandits (Lai and Robbins, 1985) and contextual bandits (Langford and Zhang, 2007). The class of algorithms that achieve diminishing long-term average reward are termed no-regret algorithms (Auer et al., 2002; Thompson, 1933; Russo et al., 2018). One widely adopted strategy relies on the optimism in the face of uncertainty principle. Our algorithm design follows the same spirit as UCB (Auer et al., 2002). key difference is that our algorithm does not observe rewards at all, but instead rely on decoding information in the feedback through verifier loss to construct the confidence set. recent line of work utilizes UCB-like heuristics for LLM agents, but they either consider hypotheses as code that specifies an MDP (Tang et al., 2024), and/or assume that the agent observes the ground-truth numerical reward (Tang et al., 2024; et al., 2024; Nie et al., 2024). Another line of research has leveraged natural language as an auxiliary signal to improve learning in sequential decision-making. Early studies showed that agents can benefit from textual guidance, such as game manuals, to inform policies or features (Branavan et al., 2012). Subsequent approaches explored grounded language to shape behavior (Gauthier and Mordatch, 2016), guide exploration (Harrison et al., 2017), or learn from feedback (Andreas et al., 2017). More recently, LDD (Zhong et al., 2024) pre-trains agents on language-annotated demonstrations to learn environment dynamics, then fine-tunes with RL to improve sample efficiency and generalization. While these approaches show empirical success, they lack formal framework and theoretical guarantees. Beyond scalar rewards, many learning settings offer richer forms of feedback. Prior work has explored bandits with side observations (Wang et al., 2003; Kocák et al., 2014), partial monitoring (Bartók et al., 2014), and preference-based feedback (Fürnkranz et al., 2012). To characterize sample complexity in reward-aware RL, Russo and Van Roy (2013) introduces the eluder dimension. Our work extends this notion beyond reward learning (see Fig. 5), opening new avenue to understanding agent learning in the era of generative AI."
        },
        {
            "title": "6.1 LLF and its relationship to existing paradigms",
            "content": "To better understand the position of LLF among existing paradigms of learning from feedback, we provide an in-depth review in this section, as alluded to in Fig. 5. In all discussed paradigms, we focus our comparison on how different forms of feedback are subsumed within LLF, while other environment parameters are loosely assumed to be included in the LLF agents hypothesis space. LLF covers the following learning paradigms commonly discussed in the literature: 15 Figure 5: LLF and its relationship to existing paradigms. LLF covers many existing paradigms: (1) reinforcement learning (RL): agent learning from scalar reward signal, (2) interaction-guided learning (IGL) (Xie et al., 2021): agent observes generic feedback vector that can decode latent reward signal, (3) reward-informative LLF: agent observes language feedback that can be translated into scalar reward signal (Xie et al., 2024), (4) multi-objective RL: extension of RL to problems with multiple objectives, combined via utility function, (5) preference-based RL: feedback provides comparison between two actions, (6) imitation learning: feedback provides an expert demonstration. In RL, upon seeing an environment state xt , the agent chooses an Reinforcement learning (RL) action at and observes scalar reward feedback rt R. The rewards and states observed by the agent at any decision step t, can depend on the past observed states and actions. In LLF, the agents hypothesis η returns reward function rη : [0, 1], while the feedback function is exactly the same: fη = rη. Hence, RL is trivially subsumed by LLF. Partial Monitoring Games In Partial Monitoring (Bartók et al., 2014), the agent observes an abstract feedback signal (not necessarily reward for its chosen action) and must deduce reward-optimal actions indirectly. The function that maps actions to feedbacks (signal function) is assumed known to the agent, and the challenge is to explore and infer optimal actions indirectly by leveraging the known signal function. In contrast, LLF assumes that the feedback function is unknown, and agents must interpret natural language feedback through verifier to ascertain semantic consistency with hypotheses. The unknown feedback mapping in LLF fundamentally alters the learning challenge, requiring ways to extract insights from potentially ambiguous language feedback, and thus capturing broader class of interactive learning scenarios. Interaction-guided Learning (IGL) In IGL (Xie et al., 2021), the environment generates latent scalar reward r(x, a) [0, 1] but only reveals rich feedback vector Y. To enable learning, IGL framework assumes reward decodability, i.e., the existence of decoder ψ Ψ, such that ψ : [0, 1], capable of extracting reward estimates for the agent. LLF naturally accommodates this by modeling both the latent reward rη and the feedback mapping fη (hence the feedback y), allowing the agent to reason about the consistency between the decoded rewards and the observed feedback vectors without needing to identify the true decoder ψ or the true feedback function . Reward-informative LLF Reward-informative LLF, defined formally in Definition 5, subsumes the special case where the latent reward function is itself function of the observed feedback (Xie et al., 2024). This framework generalizes both RL and IGL, capturing scenarios where feedback is rich and structured (e.g., language) but ultimately reflects reward. As discussed in Section 3.3, this class of LLF problems can be no harder than the reward-only setting and may even improve sample efficiency by leveraging structure in the feedback to recover the reward signal more effectively. Multi-objective RL (MORL) MORL extends the standard RL framework to environments that return vector-valued rewards rather than single scalar. The central challenge in MORL is balancing trade-offs 16 across multiple objectives, often handled via scalarization methods (see single-policy learning approaches in (Roijers et al., 2013; Zhang and Golovin, 2020)) or Pareto front exploration (Mossalam et al., 2016). In LLF, this is naturally captured by allowing the agents hypothesis to represent vector-valued reward functions. Furthermore, the verifier loss ℓ : can be extended accordingly. Since the reward vector may be under-determined with respect to the underlying utility function, we treat MORL as distinct from reward-informative LLF (Definition 5), which assumes informativeness of feedback with respect to scalar reward. Preference-based RL In PbRL, the environment does not reveal scalar reward feedback. Instead, the agent receives pairwise preferences over actions (or trajectories), e.g., that action is preferred over action a. These comparisons may be between actions selected by the agent or between one agent-chosen action and reference provided by the environment. LLF captures this setting by modeling the feedback function fη as binary comparator over pairs of actions such that fη(a, a) {0, 1} indicates the binary preference. The underlying reward model can be implicitly defined in the hypothesis η such that it induces such preferences. Thus, this preference based structure fits within LLF. Imitation learning (IL) In IL, the agent learns from demonstrations of expert behavior rather than explicit feedback or rewards. To make closer comparison with LLF, we can consider the interactive imitation learning setting, where the agent observes expert actions (corrections) for the all environment observations. IL can be modeled within the LLF framework by considering expert actions as form of feedback η = a. Any hypothesis η considered by the LLF agent can evaluate verifier loss which corresponds to the discrepancy between the optimal action of the hypothesis η and expert action a. IL is thus special case of LLF where the feedback space is the action space itself, and consistency between the agents output and expert-labeled actions is the verifier loss."
        },
        {
            "title": "7 Conclusion",
            "content": "In this paper, we develop formal foundation for learning from language feedback (LLF), setting where agents must learn from language feedback rather than scalar rewards. We introduce the transfer eluder dimension as complexity measure that quantifies how feedback affects the efficiency of learning. When feedback is informative, we show that LLF can achieve exponential efficiency gain compared to traditional reward-based learning. To demonstrate the practicality of this framework, we propose HELiX, no-regret algorithm with performance guarantees in terms of the transfer eluder dimension. Limitations and Open Questions One might wonder if the transfer eluder dimension forms lower bound for LLF. The answer, however, is negative, as there exist LLF problems with unbounded transfer eluder dimension that are trivially solvable. For example, suppose rewards are arbitrary but feedback always reveals an optimal action. The transfer eluder dimension is unbounded in this case, yet the learning problem is easy. The difference between this and the earlier example of demonstration (see Example 2) is that previously the reward class of the hypotheses are constrained to be binary and the optimal action is unique. Without these two conditions, the transfer eluder dimension for this simple problem is unbounded. We highlight that the transfer eluder dimension assumes worst-case verifier behavior, while LLMs in practice impose inductive biases on how feedback is interpreted. Empirically, we find that when explicitly presented with an optimal action, LLMs tend to trust and act on it, bypassing further learning to infer full rewards. HELiX captures this using the exploitation step (line 8), whereas näive reward-driven exploration via UCB fails. 17 This counterexample points to gap in our current understanding: the true complexity of LLF may lie between worst-case reward identification and optimal behavior learning. promising direction is to adapt DEC (Foster et al., 2024) to the LLF setting. However, the existing algorithm is not directly implementable using LLMs. Closing this gap by developing complexity measure that both lower-bounds regret and informs practical and algorithm design remains an important open question."
        },
        {
            "title": "References",
            "content": "Arash Ahmadian, Chris Cremer, Matthias Gallé, Marzieh Fadaee, Julia Kreutzer, Olivier Pietquin, Ahmet Üstün, and Sara Hooker. Back to basics: Revisiting reinforce style optimization for learning from human feedback in llms. arXiv preprint arXiv:2402.14740, 2024. Afra Feyza Akyürek, Ekin Akyürek, Aman Madaan, Ashwin Kalyan, Peter Clark, Derry Wijaya, and Niket Tandon. Rl4f: Generating natural language feedback with reinforcement learning for repairing model outputs. arXiv preprint arXiv:2305.08844, 2023a. Ekin Akyürek, Dale Schuurmans, Jacob Andreas, Tengyu Ma, and Denny Zhou. What learning algorithm is in-context learning? investigations with linear models. In The Eleventh International Conference on Learning Representations (ICLR), 2023b. Jacob Andreas. Language models as agent models. arXiv preprint arXiv:2212.01681, 2022. Jacob Andreas, Dan Klein, and Sergey Levine. Learning with latent language. arXiv preprint arXiv:1711.00482, 2017. Rohan Anil, Sebastian Borgeaud, Jean-Baptiste Alayrac, Jiahui Yu, Radu Soricut, Johan Schalkwyk, Andrew M. Dai, Anja Hauth, Katie Millican, and David Silver et al. Gemini: family of highly capable multimodal models. arXiv preprint arXiv:2312.11805, 2024. Dilip Arumugam and Thomas L. Griffiths. Toward efficient exploration by large language model agents. arXiv preprint arXiv:2504.20997, 2025. Akari Asai and Hannaneh Hajishirzi. Logic-guided data augmentation and regularization for consistent question answering. In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics (ACL), 2020. Peter Auer, Nicolò Cesa-Bianchi, and Paul Fischer. Auer, peter and cesa-bianchi, nicolò and fischer, paul. Machine Learning, 47:235256, 2002. Gábor Bartók, Dean P. Foster, Dávid Pál, Alexander Rakhlin, and Csaba Szepesvári. Partial monitoringclassification, regret bounds, and algorithms. Mathematics of Operations Research, 39(4):967997, 2014. BIG-bench authors. Beyond the imitation game: Quantifying and extrapolating the capabilities of language models. Transactions on Machine Learning Research, 2023. Rishi Bommasani, Drew A. Hudson, Ehsan Adeli, Russ Altman, Simran Arora, Sydney von Arx, Michael S. Bernstein, Jeannette Bohg, Antoine Bosselut, and Emma Brunskill et al. On the opportunities and risks of foundation models. arXiv preprint arXiv:2108.07258, 2021. SRK Branavan, David Silver, and Regina Barzilay. Learning to win by reading manuals in monte-carlo framework. Journal of Artificial Intelligence Research, 43:661704, 2012. 18 Ethan Brooks, Logan Walls, Richard Lewis, and Satinder Singh. Large language models can implement policy iteration. In Thirty-seventh Conference on Neural Information Processing Systems (NeurIPS), 2023. Sébastien Bubeck, Varun Chandrasekaran, Ronen Eldan, Johannes Gehrke, Eric Horvitz, Ece Kamar, Peter Lee, Yin Tat Lee, Yuanzhi Li, Scott Lundberg, Harsha Nori, Hamid Palangi, Marco Tulio Ribeiro, and Yi Zhang. Sparks of artificial general intelligence: Early experiments with gpt-4. arXiv preprint arXiv:2303.12712, 2023. Angelica Chen, Jérémy Scheurer, Jon Ander Campos, Tomasz Korbak, Jun Shern Chan, Samuel R. Bowman, Kyunghyun Cho, and Ethan Perez. Learning from natural language feedback. Transactions on Machine Learning Research, 2024. Xinyun Chen, Maxwell Lin, Nathanael Schärli, and Denny Zhou. Teaching large language models to self-debug. arXiv preprint arXiv:2304.05128, 2023. Ching-An Cheng, Andrey Kolobov, Dipendra Misra, Allen Nie, and Adith Swaminathan. Llf-bench: Benchmark for interactive learning from language feedback. arXiv preprint arXiv:2312.06853, 2023. Ching-An Cheng, Allen Nie, and Adith Swaminathan. Trace is the new autodiff unlocking efficient optimization of computational workflows. ICML 2024 Automated Reinforcement Learning Workshop, 2024. Yung-Sung Chuang, Rumen Dangovski, Hongyin Luo, Yang Zhang, Shiyu Chang, Marin Soljacic, ShangWen Li, Scott Yih, Yoon Kim, and James Glass. DiffCSE: Difference-based contrastive learning for sentence embeddings. In Proceedings of the 2022 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, July 2022. K.A. De Jong, W.M. Spears, and D.F. Gordon. Using genetic algorithms for concept learning. Machine Learning, pages 161188, 1993. Alina Dracheva and Jonathan Phillips. Different trajectories through option space in humans and llms. In Proceedings of the Annual Meeting of the Cognitive Science Society, volume 46, 2024. Yilun Du, Shuang Li, Antonio Torralba, Joshua Tenenbaum, and Igor Mordatch. Improving factuality and reasoning in language models through multiagent debate. In Forty-first International Conference on Machine Learning, 2023. Dylan Foster, Sham Kakade, Jian Qian, and Alexander Rakhlin. The statistical complexity of interactive decision making. arXiv, 2024. Yao Fu, Dong-Ki Kim, Jaekyeom Kim, Sungryull Sohn, Lajanugen Logeswaran, Kyunghoon Bae, and Honglak Lee. Autoguide: Automated generation and selection of context-aware guidelines for large language model agents. arXiv preprint arXiv:2403.08978, 2024. Johannes Fürnkranz, Eyke Hüllermeier, Weiwei Cheng, and Sang-Hyeun Park. Preference-based reinforcement learning: formal framework and policy iteration algorithm. Mach. Learn., 89(12):123156, October 2012. Kanishk Gandhi, Ayush Chakravarthy, Anikait Singh, Nathan Lile, and Noah Goodman. Cognitive behaviors that enable self-improving reasoners, or, four habits of highly effective stars. arXiv preprint arXiv:2503.01307, 2025. 19 Jon Gauthier and Igor Mordatch. paradigm for situated and goal-driven language learning. arXiv preprint arXiv:1610.03585, 2016. Ali Essam Ghareeb, Benjamin Chang, Ludovico Mitchener, Angela Yiu, Caralyn Szostkiewicz, Jon Laurent, Muhammed Razzak, Andrew White, Michaela Hinks, and Samuel Rodriques. Robin: multi-agent system for automating scientific discovery. arXiv preprint arXiv:2505.13400, 2025. Jiawei Gu, Xuhui Jiang, Zhichao Shi, Hexiang Tan, Xuehao Zhai, Chengjin Xu, Wei Li, Yinghan Shen, Shengjie Ma, Honghao Liu, et al. survey on llm-as-a-judge. arXiv preprint arXiv:2411.15594, 2024. Daya Guo, Dejian Yang, Haowei Zhang, Junxiao Song, Ruoyu Zhang, Runxin Xu, Qihao Zhu, Shirong Ma, Peiyi Wang, and Xiao Bi et al. Deepseek-r1: Incentivizing reasoning capability in llms via reinforcement learning. arXiv preprint arXiv:2501.12948, 2025. Tianyu Guo, Wei Hu, Song Mei, Huan Wang, Caiming Xiong, Silvio Savarese, and Yu Bai. How do transformers learn in-context beyond simple functions? case study on learning with representations. In The Twelfth International Conference on Learning Representations (ICLR), 2024. Brent Harrison, Upol Ehsan, and Mark Riedl. Guiding reinforcement learning exploration using natural language. arXiv preprint arXiv:1707.08616, 2017. Aaron Hurst, Adam Lerer, Adam P. Goucher, Adam Perelman, Aditya Ramesh, Aidan Clark, AJ Ostrow, Akila Welihinda, Alan Hayes, and Alec Radford et al. Gpt-4o system card. arXiv preprint arXiv:2410.21276, 2024. Aaron Jaech, Adam Kalai, Adam Lerer, Adam Richardson, Ahmed El-Kishky, Aiden Low, Alec Helyar, Aleksander Madry, Alex Beutel, and Alex Carney et al. Openai o1 system card. arXiv preprint arXiv:2412.16720, 2024. Tomáš Kocák, Gergely Neu, Michal Valko, and Rémi Munos. Efficient learning by implicit exploration in bandit problems with side observations. Advances in Neural Information Processing Systems, 27, 2014. Akshay Krishnamurthy, Keegan Harris, Dylan Foster, Cyril Zhang, and Aleksandrs Slivkins. Can large language models explore in-context? In ICML 2024 Workshop on In-Context Learning, 2024. T.L Lai and Herbert Robbins. Asymptotically efficient adaptive allocation rules. Adv. Appl. Math., 6(1):422, March 1985. ISSN 0196-8858. John Langford and Tong Zhang. The epoch-greedy algorithm for multi-armed bandits with side information. In Advances in Neural Information Processing Systems (NeurIPS), 2007. Jonathan Lee, Annie Xie, Aldo Pacchiano, Yash Chandak, Chelsea Finn, Ofir Nachum, and Emma Brunskill. Supervised pretraining can learn in-context reinforcement learning. In Thirty-seventh Conference on Neural Information Processing Systems (NeurIPS), 2023. E.L. Lehmann and Joseph P. Romano. Testing Statistical Hypotheses. Springer Cham, 2022. Ziniu Li, Tian Xu, Yushun Zhang, Zhihang Lin, Yang Yu, Ruoyu Sun, and Zhi-Quan Luo. Remax: simple, effective, and efficient reinforcement learning method for aligning large language models. arXiv preprint arXiv:2310.10505, 2023. Hao Liu, Carmelo Sferrazza, and Pieter Abbeel. Chain of hindsight aligns language models with feedback. In The Twelfth International Conference on Learning Representations (ICLR), 2024. 20 Huihan Liu, Alice Chen, Yuke Zhu, Adith Swaminathan, Andrey Kolobov, and Ching-An Cheng. Interactive robot learning from verbal correction. In 2nd Workshop on Language and Robot Learning: Language as Grounding, 2023. Hossam Mossalam, Yannis Assael, Diederik Roijers, and Shimon Whiteson. Multi-objective deep reinforcement learning. arXiv preprint arXiv:1610.02707, 2016. Jesse Mu, Victor Zhong, Roberta Raileanu, Minqi Jiang, Noah Goodman, Tim Rocktäschel, and Edward Grefenstette. Improving intrinsic exploration with language abstractions. Advances in Neural Information Processing Systems, 35:3394733960, 2022. Rithesh N, Shelby Heinecke, Juan Carlos Niebles, Zhiwei Liu, Le Xue, Weiran Yao, Yihao Feng, Zeyuan Chen, Akash Gokul, Devansh Arpit, Ran Xu, Phil Mui, Huan Wang, Caiming Xiong, and Silvio Savarese. REX: Rapid exploration and exploitation for AI agents. In ICLR 2024 Workshop on Large Language Model (LLM) Agents, 2024. Allen Nie, Ching-An Cheng, Andrey Kolobov, and Adith Swaminathan. Importance of directional feedback for llm-based optimizers. In NeurIPS 2023 Foundation Models for Decision Making Workshop, 2023. Allen Nie, Yi Su, Bo Hsuan Chang, Jonathan N. Lee, Ed Huai hsin Chi, Quoc V. Le, and Minmin Chen. Evolve: Evaluating and optimizing llms for exploration. arXiv preprint arXiv:2410.06238, 2024. Yuxiao Qu, Matthew YR Yang, Amrith Setlur, Lewis Tunstall, Edward Emanuel Beeching, Ruslan Salakhutdinov, and Aviral Kumar. Optimizing test-time compute via meta reinforcement fine-tuning. arXiv preprint arXiv:2503.07572, 2025. Sharath Chandra Raparthy, Eric Hambro, Robert Kirk, Mikael Henaff, and Roberta Raileanu. Generalization to new sequential decision making tasks with in-context learning, 2023. Diederik Roijers, Peter Vamplew, Shimon Whiteson, and Richard Dazeley. survey of multi-objective sequential decision-making. Journal of Artificial Intelligence Research, 48:67113, 2013. Daniel Russo and Benjamin Van Roy. Eluder dimension and the sample complexity of optimistic exploration. In Advances in Neural Information Processing Systems (NeurIPS), 2013. Daniel J. Russo, Benjamin Van Roy, Abbas Kazerouni, Ian Osband, and Zheng Wen. tutorial on thompson sampling. Found. Trends Mach. Learn., 11(1):196, July 2018. ISSN 1935-8237. Jérémy Scheurer, Jon Ander Campos, Tomasz Korbak, Jun Shern Chan, Angelica Chen, Kyunghyun Cho, and Ethan Perez. Training language models with language feedback at scale. arXiv preprint arXiv:2303.16755, 2023. Jérémy Scheurer, Jon Ander Campos, Jun Shern Chan, Angelica Chen, Kyunghyun Cho, and Ethan Perez. Training language models with language feedback. Workshop on Learning with Natural Language Supervision at ACL 2022, 2022. Thomas Schmied, Jörg Bornschein, Jordi Grau-Moya, Markus Wulfmeier, and Razvan Pascanu. Llms are greedy agents: Effects of rl fine-tuning on decision-making abilities. arXiv preprint arXiv:2504.16078, 2025. Noah Shinn, Federico Cassano, Ashwin Gopinath, Karthik Narasimhan, and Shunyu Yao. Reflexion: language agents with verbal reinforcement learning. In Thirty-seventh Conference on Neural Information Processing Systems, 2023. 21 Parshin Shojaee, Iman Mirzadeh, Keivan Alizadeh, Maxwell Horton, Samy Bengio, and Mehrdad Farajtabar. The illusion of thinking: Understanding the strengths and limitations of reasoning models via the lens of problem complexity, 2025. URL https://ml-site.cdn-apple.com/papers/ the-illusion-of-thinking.pdf. Chenglei Si, Diyi Yang, and Tatsunori Hashimoto. Can llms generate novel research ideas? large-scale human study with 100+ nlp researchers. arXiv preprint arXiv:2409.04109, 2024. David Silver and Rich Sutton. Welcome to the era of experience. preprint, 2025. Richard S. Sutton and Andrew G. Barto. Reinforcement Learning: An Introduction. The MIT Press, 2018. Fahim Tajwar, Yiding Jiang, Abitha Thankaraj, Sumaita Sadia Rahman, Zico Kolter, Jeff Schneider, and Ruslan Salakhutdinov. Training generally curious agent. arXiv preprint arXiv:2502.17543, 2025. Hao Tang, Darren Yan Key, and Kevin Ellis. Worldcoder, model-based LLM agent: Building world models by writing code and interacting with the environment. In The Thirty-eighth Annual Conference on Neural Information Processing Systems (NeurIPS), 2024. William R. Thompson. On the likelihood that one unknown probability exceeds another in view of the evidence of two samples. Biometrika, 25(3/4):285294, 1933. Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timothée Lacroix, Baptiste Rozière, Naman Goyal, Eric Hambro, Faisal Azhar, Aurelien Rodriguez, Armand Joulin, Edouard Grave, and Guillaume Lample. Llama: Open and efficient foundation language models. arXiv preprint arXiv:2302.13971, 2023. Chih-Chun Wang, S.R. Kulkarni, and H.V. Poor. Bandit problems with arbitrary side observations. In 42nd IEEE International Conference on Decision and Control (IEEE Cat. No.03CH37475), volume 3, pages 29482953 Vol.3, 2003. Guanzhi Wang, Yuqi Xie, Yunfan Jiang, Ajay Mandlekar, Chaowei Xiao, Yuke Zhu, Linxi Fan, and Anima Anandkumar. Voyager: An open-ended embodied agent with large language models. Transactions on Machine Learning Research, 2024. Hongwei Wang and Dong Yu. Going beyond sentence embeddings: token-level matching algorithm for calculating semantic textual similarity. In The 61st Annual Meeting of the Association for Computational Linguistics Short Papers (ACL), July 2023. Lex Weaver and Nigel Tao. The optimal reward baseline for gradient-based reinforcement learning. arXiv preprint arXiv:1301.2315, 2013. Anjiang Wei, Allen Nie, Thiago SFX Teixeira, Rohan Yadav, Wonchan Lee, Ke Wang, and Alex Aiken. Improving parallel program performance through dsl-driven code generation with llm optimizers. arXiv preprint arXiv:2410.15625, 2024. Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, brian ichter, Fei Xia, Ed Chi, Quoc Le, and Denny Zhou. Chain-of-thought prompting elicits reasoning in large language models. In Advances in Neural Information Processing Systems, volume 35, pages 2482424837. Curran Associates, Inc., 2022. Yixuan Weng, Minjun Zhu, Fei Xia, Bin Li, Shizhu He, Shengping Liu, Bin Sun, Kang Liu, and Jun Zhao. Large language models are better reasoners with self-verification. In The 2023 Conference on Empirical Methods in Natural Language Processing (EMNLP), 2023. 22 Zhiheng Xi, Wenxiang Chen, Xin Guo, Wei He, Yiwen Ding, Boyang Hong, Ming Zhang, Junzhe Wang, Senjie Jin, and Enyu Zhou et al. The rise and potential of large language model based agents: survey. Sci. China Inf. Sci, 68, 121101, 2025. Sang Michael Xie, Aditi Raghunathan, Percy Liang, and Tengyu Ma. An explanation of in-context learning as implicit bayesian inference. In International Conference on Learning Representations (ICLR), 2022. Tengyang Xie, John Langford, Paul Mineiro, and Ida Momennejad. Interaction-grounded learning. In Marina Meila and Tong Zhang, editors, Proceedings of the 38th International Conference on Machine Learning, volume 139 of Proceedings of Machine Learning Research, pages 1141411423. PMLR, 1824 Jul 2021. Tianbao Xie, Siheng Zhao, Chen Henry Wu, Yitao Liu, Qian Luo, Victor Zhong, Yanchao Yang, and Tao Yu. Text2reward: Automated dense reward function generation for reinforcement learning. In International Conference on Learning Representations (ICLR), 2024 (07/05/2024-11/05/2024, Vienna, Austria), 2024. Yutaro Yamada, Robert Tjarko Lange, Cong Lu, Shengran Hu, Chris Lu, Jakob Foerster, Jeff Clune, and David Ha. The ai scientist-v2: Workshop-level automated scientific discovery via agentic tree search. arXiv preprint arXiv:2504.08066, 2025. Shunyu Yao, Jeffrey Zhao, Dian Yu, Nan Du, Izhak Shafran, Karthik Narasimhan, and Yuan Cao. React: Synergizing reasoning and acting in language models. The International Conference on Learning Representations (ICLR), 2023. Richard Zhang and Daniel Golovin. Random hypervolume scalarizations for provable multi-objective black box optimization. In International conference on machine learning, pages 1109611105. PMLR, 2020. Lianmin Zheng, Wei-Lin Chiang, Ying Sheng, Siyuan Zhuang, Zhanghao Wu, Yonghao Zhuang, Zi Lin, Zhuohan Li, Dacheng Li, Eric Xing, Hao Zhang, Joseph E. Gonzalez, and Ion Stoica. Judging LLM-as-ajudge with MT-bench and chatbot arena. In Thirty-seventh Conference on Neural Information Processing Systems Datasets and Benchmarks Track (NeurIPS), 2023. Victor Zhong, Dipendra Misra, Xingdi Yuan, and Marc-Alexandre Côté. Policy improvement using language feedback models. arXiv preprint arXiv:2402.07876, 2024. Yangqiaoyu Zhou, Haokun Liu, Tejes Srivastava, Hongyuan Mei, and Chenhao Tan. Hypothesis generation with large language models. arXiv preprint arXiv:2404.04326, 2024."
        },
        {
            "title": "A Regret Analysis",
            "content": "We first define the version spaces used in the algorithm. As shorthand notations, define Lt(η) = t1 i=0 (cid:16)EOfη (Ai)[ℓ(Ai, O, η)] ℓmin η (Ai) (cid:17) to be the cumulative population prediction error and Lt(η) = t1 i=0 ℓ(Ai, Oi, η) = t1 i=0 ℓi(η) to be the cumulative empirical verifier loss. small value of Lt(η) means η is close to consistent with observed feedback. Let Vt be the version space of all hypotheses still plausible after rounds of interactions. Concretely, Vt = {η : Lt(η) min ηH Lt(η) + βt}, (1) where βt > 0 is an appropriately chosen confidence parameter so that we do not throw away the true hypothesis η due to noise. useful approach to bounding the regret is to decompose it in terms of version spaces. We define the width of subset at an action by wV (a) = sup ηV rη(a) r(a) . Proposition 2 (Regret decomposition). Fix any sequence {Vt : N}, where Vt is measurable with respect to σ(Ht). Then for any N, Regret(T, η) 1 t=0 [wVt(At) 1{η Vt} + 1{η / Vt}] . Proof. We define the upper bound Ut(a) = sup{rη(a) : η Vt} and let arg maxaA r(a). When η Vt, the bound r(a) Ut(a) hold for all actions. This implies r(η) r(At) (Ut(a) r(At)) 1{η Vt} + 1{η / Vt} wVt(At) 1{η Vt} + 1{η / Vt} + [Ut(a) Ut(At)] 1{η Vt}. Since the algorithm selects an action At that maximizes Ut(a), the conclusion follows by taking the expectation and summing over all = 0, . . . , 1. This proposition reduces upper bounding the regret to bounding the expected sum of widths PT 1 t=0 if the version spaces Vt are constructed such that they contain η with high probability. E[wVt(At)] We first introduce class of Martingale exponential inequalities that will be useful throughout our analysis, including bounding the sum of widths and proving the high-confidence events η Vt. For random variables (Xtt N) adapted to the filtration (Ftt N), let us assume that E[exp(λXt)] is finite for all λ and E[XtFt1] = 0. We assume that there is uniform upper bound on the cumulant generating function (i.e., log moment generating function) for the conditional distribution of Xt. Lemma 1 (Cumulant generating function). If there is sequence of convex functions {ψt : [0, ) R} t=0 with ψt(0) = 0 such that, for all and all λ [0, ), log eλXtFt1 ψt(λ), then for all δ (0, 1) and N, with probability 1 δ, (cid:12) (cid:12) (cid:12) (cid:12) (cid:12) 1 t=0 Xt (cid:12) (cid:12) (cid:12) (cid:12) (cid:12) inf λ[0,) ( PT 1 t=0 ψt(λ) + log(2/δ) λ ) . Proof. Let ST = PT 1 t=0 Xt. By Markovs inequality, for all and λ [0, ), E[eλ PT E[eλST ] t=0 eλu = Xt] exp(ψT 1(λ)) eλu = E[E[eλST FT 1]] eλu exp(PT 1 t=0 ψt(λ)) eλu . (ST u) = (cid:16) eλST eλu(cid:17) E[eλ PT t=0 This gives (ST u) exp λu + ! 1 ψt(λ) XtE[eλXT 1FT 1]] eλu t=0 for all λ [0, ). Applying the same argument to Xt, we have (ST u) = (ST u) exp λu + ! ψt(λ) . 1 t= Solving for to achieve δ/2 probability for each side, and taking the infimum over λ [0, ), we have with probability at least 1 δ, ST inf λ[0,) ( PT 1 t=0 ψt(λ) + log(2/δ) λ ) . We now proceed to bounding the sum of widths PT 1 E[wVt(At)] when the event η Vt holds. As first t=0 step, we show that there cannot be many version spaces Vt with large width. For all and η, η H, we define the martingale difference Zt(η, η) = EOfη (At) (cid:2)ℓ(At, O, η) ℓ(At, O, η)Gt1 (cid:3) (cid:0)ℓ(At, Ot, η) ℓ(At, Ot, η)(cid:1) . Notice that Zt have expectation zero and constitutes martingale difference sequence adapted to the filtration (Gtt N) where Gt is the σ-algebra generated by all observations {(a0, o1), . . . , (at, ot)} up to time t. Proposition 3. If the conditions in Lemma 1 holds for (Ztt N) adapted to (Gtt N) with cumulative generating function bound (ψtt N), (βt 0t N) in (1) is nondecreasing sequence such that for all N, βt inf λ[0,) , then for all δ (0, 1), with probability at least 1 δ, ψi(λ)+log(10t2/3δ) (cid:26) Pt1 i=0 (cid:27) λ 1 1{wVt(At) > ϵ} 1{η Vt} t=0 for all and ϵ > 0. (cid:19) (cid:18) 3βT ϵ2 + 1 dimT E(H, ℓ, ϵ) 25 Proof. We first show that if wVt(At) > ϵ and η Vt, then with high probability, At is ϵ-dependent on fewer than O(βt/ϵ2) disjoint subsequences of (A0, A1, . . . , At1). If wVt(At) > ϵ and η Vt, there exists η Vt such that rη(At) rη(At) > ϵ. By definition, if At is ϵ-dependent on subsequence (Ai1, . . . , Aik ) of (A0, . . . , At1), then we have that j=1 (cid:16)EOfη (Aij )[ℓ(Aij , O, η)] ℓmin (cid:17) η (Aij ) > ϵ2. It follows that if At is ϵ-dependent on disjoint subsequences of (A0, . . . , At1) then t1 i=0 (cid:16)EOfη (Ai)[ℓ(Ai, O, η)] ℓmin η (Ai) (cid:17) > Kϵ2. Then = = t1 (cid:16)EOfη (Ai)[ℓ(Ai, O, η)] ℓmin (cid:17) η (Ai) i=0 t1 EOfη (Ai) [ℓ(Ai, O, η) ℓ(Ai, O, η)] i=0 \"t1 i=0 \"t1 + ℓ(Ai, Oi, η) min ηH t1 i=0 # ℓ(Ai, Oi, η) \"t1 i=0 ℓ(Ai, Oi, η) min ηH # ℓ(Ai, Oi, η) t1 i=0 [ℓ(Ai, Oi, η) ℓ(Ai, Oi, η)] t1 # EOfη (Ai) [ℓ(Ai, O, η) ℓ(Ai, O, η)] i= (cid:12) (cid:12) (cid:12) (cid:12) (cid:12) t1 i=0 ℓ(Ai, Oi, η) min ηH t1 i=0 ℓ(Ai, Oi, η) [ℓ(Ai, Oi, η) ℓ(Ai, Oi, η)] i=0 (cid:12) (cid:12) (cid:12) (cid:12) (cid:12) t1 + (cid:12) (cid:12) (cid:12) (cid:12) (cid:12) t1 i=0 ℓ(Ai, Oi, η) min ηH t1 i=0 ℓ(Ai, Oi, η) (cid:12) (cid:12) (cid:12) (cid:12) (cid:12) # EOfη (Ai) [ℓ(Ai, O, η) ℓ(Ai, O, η)] + \"t1 i=0 2βt + = 2βt t1 i=0 t1 i=0 Using Lemma 1, i= [ℓ(Ai, Oi, η) ℓ(Ai, Oi, η)] t1 i=0 Zi(η, η). EOfη (Ai) [ℓ(Ai, O, η) ℓ(Ai, O, η)] (cid:12) (cid:12) (cid:12) (cid:12) (cid:12) t1 i=0 Zi(η, η) > inf λ[0,) ( Pt1 i=0 ψi(λ) + log(2/δ) λ )! δ. (cid:12) (cid:12) (cid:12) (cid:12) (cid:12) We choose sequence {δt}tN>0 where δt = 3δ we have that with probability at least 1 δ, for all N, 5t2 , and so t=1 δt < δ. Using union bound over all N>0, (cid:12) (cid:12) (cid:12) (cid:12) (cid:12) t1 i=0 (cid:12) (cid:12) Zi(η, η) (cid:12) (cid:12) (cid:12) inf λ[0,) ( Pt1 i=0 ψi(λ) + log(10t2/3δ) λ ) βt. Since {βt}tN is nondecreasing in t, we have that with probability at least 1 δ, Kϵ2 3βT . It then follows that with probability at least 1 δ, 3βT /ϵ2. 26 Next, we take any action sequence (a1, . . . , aτ ) and show that there is some element aj that is ϵ-dependent on at least τ /d 1 disjoint subsequences of (a1, . . . , aj1), where = dimT E(H, ℓ, ϵ). For an integer satisfying Kd + 1 τ Kd + d, we will construct disjoint subsequences B1, . . . , BK inductively starting with Bi = (ai) for = 1, . . . , K. If aK+1 is ϵ-dependent on each subsequence B1, . . . , BK, we are done. Otherwise, there must be at least one subsequence for which aK+1 is ϵ-independent. We choose such subsequence and append aK+1 to it. We will repeat this process for aj with = + 2, + 3, . . . until either aj is ϵ-dependent on each subsequence or = τ . If the first case occurs, we are done. If = τ , we necessarily have that Bi Kd. Since each element of subsequence Bi is ϵ-independent of its predecessors, Bi = d. By the definition of dimT E(H, ℓ, ϵ), aτ must be ϵ-dependent on each subsequence. We now take (A1, . . . , Aτ ) to be the subsequence (At1, . . . , Atτ ) of (A1, . . . , AT ) where for each At, we have wVt(At) > ϵ. As we have shown first, with probability at least 1 δ, each Atj is ϵ-dependent on fewer than 3βT /ϵ2 disjoint subsequences of (A1, . . . , Aj1). As we have shown in the preceding paragraph, there is some aj that is ϵ-dependent on at least τ /d 1 disjoint subsequences of (a1, . . . , aj1). Combining these two facts, we may conclude that τ /d 1 3βT /ϵ2. It follows that with probability at least 1 δ, τ (cid:0)3βT /ϵ2 + 1(cid:1) as desired. We are now ready to bound the sum of widths PT 1 t=0 ϵH -transfer eluder dimension of H, where E[wVt(At)] when the event η Vt holds. Consider the ϵH = max (cid:26) 1 t2 , min aA inf{rη(a) r(a) : η H, η = η} (cid:27) . Lemma 2. If the conditions in Lemma 1 holds for (Ztt N) adapted to (Gtt N) with cumulative generating function bound (ψtt N), (βt 0t N) in (1) is nondecreasing sequence such that for all N, βt inf λ[0,) , then for all δ (0, 1), with probability at least 1 δ, ψi(λ)+log(10t2/3δ) (cid:26) Pt1 i=0 (cid:27) λ 1 t=0 wVt(At) 1{η Vt} + min dimT E(H, ℓ, ϵH ), o + 2 1 for all N. 3 dimT E(H, ℓ, ϵH )βT Proof. Let dT = dimT E(H, ℓ, ϵH where wi1 wi2 wiT . We have ) and wt = wVt(At). Reorder the sequence (w1, . . . , wT ) (wi1, . . . , wiT ) 1 wVt(At) 1{η Vt} t=0 1 t=0 1 t=0 wit 1{η Vit} wit 1{η Vit} 1{wit > ϵH } + 1 t=0 wit 1{η Vit} 1{wit ϵH } 1 + 1 t= wit 1{η Vit} 1{wit > ϵH }. = = The last inequality follows since either ϵH possible width and hence 1{wit ϵH = 1/T 2 and PT 1 = 1/T or ϵH is set below the smallest } never occurs. We have that wit 1. Also, wit > ϵ t=0 ϵH 27 1{wVk (ak) > ϵ} t. By Proposition 3, with probability at least 1 δ, this can only happen if < , since dimT E(H, ℓ, ϵ) is non-increasing in ϵ, dimT E(H, ℓ, ϵ) , (cid:0)3βT /ϵ2 + 1(cid:1) dT , implying ϵ . So PT 1 k=0 (cid:0)3βT /ϵ2 + 1(cid:1) dimT E(H, ℓ, ϵ). For ϵ ϵH dimT E(H, ℓ, ϵH if wit > ϵH ) = dT . Therefore, when wit > ϵ ϵH 3βT dT tdT , then wit min{1, 3βT dT tdT }. Thus, 1 t= wit 1{η Vit} 1{wit > ϵH } dT + 1 3βT dT dT t=dT +1 dT + p3βT dT = dT + 2p3βT dT . t=1 1 1 dt Since the sum of widths is always bounded by , this implies that with probability 1 δ, 1 t=0 wVt(at) 1{η Vt} min (cid:26) T, 1 + min + dimT E(H, ℓ, ϵH 1 dimT E(H, ℓ, ϵH ), ) + 3 dimT E(H, ℓ, ϵH )βT (cid:27) + 3 dimT E(H, ℓ, ϵH )βT . So far, we have only considered HELiX without the exploitation step. We remark that by Lemma 6, when the exploitation step is triggered, the per-step regret of all following steps become zero, and so the regret of the full HELiX is always bounded above by that without the exploitation step. Combining this observation with Lemma 2 and Proposition 2, we arrive at the following abstract regret bound in terms of the version space confidence parameter βT . Theorem 2. If it holds that for some δ (0, 1), with probability at least 1 δ, η Vt for all t, then for all N, Regret(T ) 1 + 1 + min{dimT E(H, ℓ, ϵH ), } + 2 The dominant term in the regret bound is 3 dimT E(H, ℓ, ϵH )βT . 3 dimT E(H, ℓ, ϵH )βT . 2 For our main theorem, it remains to design suitable version spaces Vt and show that they contain the true hypothesis η with high probability. Crucially, the rate at which the confidence parameters βt of these version spaces shrink depends on concentration properties of the verifier loss function ℓ. Note that for the general LLF framework, we have assumed only that ℓ is bounded function taking values in [0, 1]. If we have more structural assumptions on the verifier loss ℓ, for example, that ℓ is α-strongly convex, then we may arrive at by taking βT to be of constant order. tighter regret bound up to order A.1 Version Space Construction for General Bounded Loss Consider the most general case with minimal assumptions on the loss function, namely, that it is bounded between [0, 1] for all inputs. Then we prove the following high-probability event: 28 Lemma 3 (High-probability event). For all δ > 0, η, η H, LT (η) LT (η) + LT (η) LT (η) s 2T log (cid:18) 10T 2 3δ (cid:19) , 1 δ. Proof. For each = 1, . . . , , define the Martingale difference sequence Xt = EOfη (At) (cid:2)ℓ(At, O, η) ℓ(At, O, η)(cid:3) (cid:0)ℓ(At, Ot, η) ℓ(At, Ot, η)(cid:1) . LT (η) LT (η) (cid:0)LT (η) LT (η)(cid:1) 1 (cid:16)EOfη (At)[ℓ(At, O, η)] EOfη (At)[ℓ(At, O, η)] (cid:17) 1 t=0 (cid:0)ℓ(At, Ot, η) ℓ(At, Ot, η)(cid:1) EOfη (At) (cid:2)ℓ(At, O, η) ℓ(At, O, η)(cid:3) 1 t=0 (cid:0)ℓ(At, Ot, η) ℓ(At, Ot, η)(cid:1) Xt. = t=0 1 t=0 1 t=0 = = Notice that Xt have expectation zero and constitutes Martingale difference sequence adapted to the filtration {Gt}t1 where Gt is the σ-algebra generated by all observations {(A0, O1), . . . , (At, Ot)} up to time t. Since feedback losses ℓ(a, o, η) are uniformly bounded between [0, 1], we have that Xt [2, 2] with probability 1. Using Lemma 1 with ψt(λ) = λ2/2 and taking the infimum over λ, we get (cid:12) (cid:12) (cid:12) (cid:12) (cid:12) 1 t=0 Xt (cid:12) (cid:12) (cid:12) (cid:12) (cid:12) > ! 2T log(2/δ) δ. We choose sequence {δT }T N>0 where δT = 3δ N0, we have that with probability at least 1 δ, 5T 2 such that =1 δT < δ. Using union bound over all (cid:12)LT (η) LT (η) (cid:0)LT (η) LT (η)(cid:1)(cid:12) (cid:12) (cid:12) 2T log (cid:19) (cid:18) 2 δT = 2T log (cid:19) (cid:18) 10T 2 3δ N. Since η is the true hypothesis, by Assumption 3, it minimizes the population loss LT (η) for all N. That is, for all η H, LT (η) LT (η) N. Suppose = < . By Lemma 3, for any η H, with probability at least 1 δ/m, for all N, LT (η) LT (η) LT (η) LT (η) + 2T log (cid:19) (cid:18) 10T 2 3δ 2T log (cid:18) 10mT 2 3δ (cid:19) . Using union bound over H, with probability at least 1 δ, the true hypothesis η is contained in the version space VT = η : LT (η) min ηH LT (η) + 2T log (cid:19) (cid:18) 10HT 2 3δ for all N. To extend this to space of infinite hypotheses, we measure the set by some discretization scale α. Recall that we define distances in the hypothesis space in terms of the loss function ℓ: dH(η, η) = sup aA,oO ℓ(a, o, η) ℓ(a, o, η). Lemma 4. dH(, ) is pseudometric on H. Proof. We check the axioms for pseudometric. nonnegativity: dH(η, η) = 0 and dH(η, η) 0 for all η, η H. symmetry: dH(η, η) = dH(η, η). triangle inequality: for each and O, ℓ(a, o, η) ℓ(a, o, η) ℓ(a, o, η) ℓ(a, o, η) + ℓ(a, o, η) ℓ(a, o, η). Taking the supremum over and yields the desired property. Let (H, α, dH) denote the α-covering number of in the pseudometric dH, and let β (H, δ, α) := 2t log (cid:18) 10N (H, α, dH)t2 3δ (cid:19) + 2αt. (2) Proposition 4. For δ > 0, α > 0, and N, define Then it holds that (cid:26) VT := η : LT (η) min ηH LT (η) + β (cid:27) η ! VT 1 δ. =1 Proof. Let Hα be an α-cover of in the pseudometric dH. In other words, for any η H, there is an ηα Hα such that dH(η, ηα) α. union bound over Hα gives that with probability at least 1 δ, (LT (ηα) LT (ηα)) (LT (η) LT (η)) 2T log = (LT (η) LT (η)) (LT (η) LT (η)) 2T log (cid:18) 10HαT 2 3δ (cid:18) 10HαT 2 3δ (cid:19) (cid:19) + (LT (η) LT (η)) (LT (ηα) LT (ηα)) } {z discretization error . The discretization error can be expanded and bounded as 1 t=0 hEOfη (At) [ℓ(At, O, η) ℓ(At, O, ηα)] ℓ(At, Ot, η) + ℓ(At, Ot, ηα) 2αT. 30 Since η is minimizer of LT (), we have that with probability at least 1 δ, LT (η) LT (η) 2T log (cid:18) 10HαT 2 3δ (cid:19) + 2αT. We take the infimum over the size of α-covers, which results in the bound LT (η) LT (η) 2T log (cid:18) 10N (H, α, dH)T 2 3δ (cid:19) + 2αT. Taking δ = 1 following main theorem. and plugging βT = β (H, δ, ϵH ) into the abstract regret bound in Theorem 2 proves the Theorem 1. For all N, Regret(T ) 1 + + 2 Proof. 1 + min{dimT E(H, ℓ, ϵH ), } 2 log (cid:18) 10N (H, α, dH)T 2 3δ (cid:19)1/ dimT E(H, ℓ, ϵH )T 3/2 + 6 dimT E(H, ℓ, ϵH ). 1 + + min{dimT E(H, ℓ, ϵH ), } + 3 dimT E(H, ℓ, ϵH )β (H, δ, ϵH )T Regret(T ) 1 1 u t3 dimT E(H, ℓ, ϵH ) + 2 = 1 + + min{dimT E(H, ℓ, ϵH ), }+ = 1 + + min{dimT E(H, ℓ, ϵH ), }+ t2T log 10N (H, ϵH , dH)T 2 3δ ! + 2ϵH 1 1 + 3 2 log (cid:18) 10N (H, α, dH)T 2 3δ (cid:19)1/2 dimT E(H, ℓ, ϵH )T 3/2 + 6ϵH dimT E(H, ℓ, ϵH )T 2 1 + + min{dimT E(H, ℓ, ϵH ), }+ + 3 2 log (cid:18) 10N (H, α, dH)T 2 3δ (cid:19)1/2 dimT E(H, ℓ, ϵH )T 3/2 + 6 dimT E(H, ℓ, ϵH ), where the last inequality follows since ϵH 1/T 2 by definition. The leading term in the regret bound is of order 3/4 (cid:16) log (H, ϵH , dH) (cid:17)1/4 dimT E(H, ℓ, ϵH ). 31 Remark 3. As noted earlier on, while the order eO(T 3/4) on the time horizon may appear suboptimal compared to classical eO( ) optimal rates for bandit learning with direct reward feedback, this slower rate is in fact principled consequence of our minimal assumptions. Specifically, our analysis makes no structural assumptions on the verifier loss ℓ beyond boundedness. If we have more structural knowledge of ℓ, say, that it ). notable instance is α-strongly convex, then the bound can be tightened to match the optimal order eO( is when ℓ is squared loss. refined analysis on the drift of conditional mean losses allows us to choose the confidence parameters βT for the version spaces to be of order eO(log(1/δ)), which results in the tight eO( ) regret rate."
        },
        {
            "title": "B Proofs for Supporting Lemmas and Propositions",
            "content": "B.1 Proof for Proposition 1 Proof. Let ℓ = CF ℓ. Let dT = dimT E(H, ℓ, ϵ) be the shorthand for the ϵ-transfer eluder dimension of with respect to ℓ. Then, there exists length dT sequence of elements in such that for some ϵ ϵ, every action element is ϵ-transfer independent of its predecessors. We denote such sequence as (a0, . . . , adT 1). By definition of the transfer eluder dimension, for any {0, . . . , dT 2}, there exists pair of hypotheses η, η satisfying i=0 (cid:16)Eofη (ai)[ℓ(ai, o, η)] ℓmin (cid:17) η (ai) ϵ but rη(ak+1) rη(ak+1) > ϵ. Using the definition for reward-discriminative verifiers, i=0 (cid:0)rη(ai) rη(ai)(cid:1)2 CF i= (cid:16)Eofη (ai)[ℓ(ai, o, η)] ℓmin η (ai) (cid:17) = i= (cid:16)Eofη (ai)[ℓ(ai, o, η)] ℓmin η (ai) (cid:17) ϵ2. By the definition of the (regular) eluder dimension, every action in the sequence (a0, . . . , adT 1) is ϵindependent of its predecessors. Therefore, dT dimE(R, ϵ) since the latter is the length of the longest sequence of independent actions. We may conclude that dimE(R, ϵ) dimT E(H, CF ℓ, ϵ). B.2 Proof for Lemma 5 Lemma 5. Consider some H. Suppose minπΠ maxη rη(πη) rη(π) = 0. Let ˆπ be minimizer. Let denote the set of optimal actions with respect to rη. Then supp(ˆπ) η, for all η H. η Proof. We prove by contradiction. Suppose ˆπ takes some action outside of probability p. Let π = ˆπ p1[a = a] + pUnif[a contradiction. Therefore, supp(ˆπ) η for some η with η]. Then it follows rη(π) > rη(ˆπ), which is η, for all η H. B.3 Proof of the Reward-Informative Feedback Example Suppose rη(a) = Eofη(a)[g(a, o)] for some known : [0, 1]. Note that the reward mapping η 7 rη is known, but the reward function itself is still hidden from the agent (since η is unknown). We 32 define ℓ(a, o, η) := (g(a, o) rη(a))2 = (g(a, o) Eofη(a)[g(a, o)])2, which gives (g(a, o) Eofη (a)[g(a, o)])2i Eofη(a)[ℓ(a, o, η)]. With this definition, we have that One can easily verify that η arg minηH Eofη(a)[ℓ(a, o, η)] = Eofη(a) . rη(a) rη(a)2 = (Eofη(a)[g(a, o)] Eofη (a)[g(a, o)])2 = (Eofη(a)[g(a, o) Eofη (a)[g(a, o)]])2 Eofη(a)[(g(a, o) Eofη (a)[g(a, o)])2] = Eofη(a)[ℓ(a, o, η)] This shows the feedback is reward-informative. B.4 Proof of Reasoning Example binary indicator of whether all steps are correct This problem is equivalent to bandit problem with SL arms. Here fη(a) = r(a), so the transfer eluder dimension reduces to the standard eluder dimension, which is bounded by the size of the action space. index of the first incorrect step Here we prove for ϵ < 1/2L. Given the rubric of η, partition the action space into sets, where Al = {(s1, . . . , sL)s1, . . . , sl1 are correct and sl is incorrect} for = 1, . . . , L, Aj = , for = j, and where A0 denotes sequences where s1 is incorrect. By this definition, we have Ai S(SL l=1 Al) = A, where = {a} Suppose we have an independent action sequence (a1, . . . , aK) in the sense of Definition 3 where each action is ϵ-independent of their predecessors. We show it can have no more than actions from each Al for [1, L]. By definition of the feedback, for Al, η (a) = l. Suppose we have more than actions from Al. It implies that token must be used twice at the lth position. Say its sl and its shared by a1, a2 Al. Then we show a2 is ϵ-dependent on a1 when ϵ < 1/L. For η H, satisfying Eof (a0)[o fη(a0)2/L2] = fη(a0)2/L2 ϵ2, we have Lϵ fη(a0) + Lϵ. Since ϵ < 1/2L and fη(a0) is an integer, this implies fη(a0) = l. That is, for such an η satisfying the constraint given by a0, sl is incorrect. This implies fη(a1) l. Therefore, rη(a0) = rη(a1) = 0. Therefore, the length of independent action sequences is bounded by SL + = SL + 1 . give correction for the first mistake In this case, the feedback not only returns the index of the first incorrect step l, but also reveals the correct reasoning action η = (s1(η), . . . , sL(η)) denote the reasoning steps based on the hypothesis η. The reward function of any action and hypothesis η is rη(a) = I{a η = a}. For an action = (s1, . . . , sL) and feedback := (l, sl(η)) generated based on fη(a), we have sj = sj(η) for all < and sl = sl(η). Now, given any feedback := (l, ), we define the following loss ℓ(a, o, η) = 1 . This verifer loss evaluates whether η and η have the same first reasoning steps. (cid:17) I{sj(η) = sj} + I{sl(η) = } (cid:16)Pl1 j=1 . Let For ϵ < 1, suppose an action sequence (a1, . . . , aK) where each action is ϵ-independent of their predecessors. If action is ϵ-independent, there exists η, η such that PK Eoifη (a)[l(ai, oi, η)] ϵ and rη(a) i=1 rη(a) > ϵ. By definition of the feedback and loss, we know η,η have the same initial maxi li reasoning steps. However, we know that rη(a) = rη(a) indicating at least one index > maxi li where sl {sl(η), sl(η)} and sl(η) = sl(η), resulting in feedback = (l, sl(η)) for a. Thus, the sequence of indices in feedback 33 o1, o2, . . . is monotonic. As we have reasoning steps, for any pair η, η, the sequence length is bounded by L. demonstration Here, the feedback directly demonstrates correct reasoning sequence = (s 1, . . . , L) and is independent of the agents action sequence. For action = (s1, . . . , sL) and hypothesis η, we define the loss as ℓ(a, o, η) = I{o = η}. Therefore, for any η, η and ϵ < 1, if satisfies: Eofη (a)ℓ(a, o, η) ϵ, we have η, implying rη(a) = rη(a) for all SL and transfer Eluder dimension of 1. η = B.5 Proof for Lemma Lemma 6. Suppose for some t0 0, we have that minπΠ maxηHt0 Then for all > t0, minπΠ maxηHt rη(πη) rη(π) = 0. rη(πη) rη(π) = 0 in Algorithm 1. Proof. We prove by induction. Suppose the conclusion holds for > t0, we prove that it holds for + 1 as well. At time t, the induction hypothesis implies that minπΠ maxηHt rη(πη) rη(π) = 0. Since Ht+1 Ht, maxηHt+1 rη(πη) rη(π) maxηHt rη(πη) rη(π) for all π Π. Thus, minπΠ maxηHt+1 rη(πη) rη(π) minπΠ maxηHt rη(πη) rη(π) = 0."
        },
        {
            "title": "C Extensions",
            "content": "C.1 Special Case of Reward-Agnostic Feedback Text feedback may contain information beyond what is relevant to the reward. In particular, one could imagine special case, where feedback does not reveal much about the reward, but still provides enough to identify an optimal action over time. One simple example is when the feedback directly reveals the optimal action, regardless of the action chosen. In this case, the transfer eluder dimension as defined could be arbitrarily large, but ideally an efficient LLF agent should choose the optimal action in the following steps instead of trying to identify the mean reward for each action. C.2 Extension to Contextual Bandits Our formulation can be modified slightly to accommodate learning with context. In contextual problem, Markov process Xt independently takes values in set that the agent views as contexts. We may define the full set of actions to be the set of context-action pairs := {(x, a) : , A(x)}, where A(x) is the set of available actions under the context x. Instead of having fixed action space across time, consider time-varying action sets At := {(Xt, a) : A(Xt)}. At each time t, an action at At will be selected. In accordance, the policy π = {πtt N} is now sequence of functions indexed by time, each mapping the history Ht = (A0, A0, R0, . . . , At1, At1, Rt1, At) to distribution over with support At. Our analysis for the context-free setting directly carries over. C.3 Alternative Formulation of Feedback Generation The LLF formulation we have presented so far assumes that feedback arises from fixed mapping η 7 fη with each hypothesis η H. While this model-based view simplifies both the design of exploration strategies and the complexity analysis via the transfer eluder dimension, it imposes structural constraint that may be too restrictive in settings where feedback is generated by more complex or even adversarial process. An alternative, entirely model-free formulation allows feedback to be generated arbitrarily from an oracle in streaming fashion, without the need to explicitly model feedback mapping η 7 fη. Concretely, 34 at each time t, the agent executes an action At and observes feedback Ot O. We denote the history of interactions as It = (A0, O0, . . . , At, Ot) and write for the set of all possible histories. (history-dependent) policy π : (A) maps each history to distribution over actions. This streaming-oracle perspective subsumes both stochastic and adversarial feedback models, and can capture scenarios where the dependence on η is unknown or too complex to parameterize. In this setting, one must replace the hypothesis-indexed complexity measures by complexity metrics defined directly over the space of oracles or possible histories. Although this general approach will likely incur additional technical overhead, it also broadens the applicability of our LLF framework to encompass richer feedback protocols beyond the hypothesis-testing paradigm. An interesting future direction is to develop performance guarantees under the more general feedback generation model."
        },
        {
            "title": "D Experiment Details",
            "content": "D.1 Reasoning Traces for Battleship Here we show concrete example on how the algorithm works and what the LLM generates. Transcript on Multi-Hypothesis Evaluation (Battleship) Prompt: Hit at D2! You have hit ship, which occupies 4 cells in the grid. Here is how the board looks now: 1 4 5 6 3 M . X . X . . . . . . . . M . . . . . . . ======================== Now you have new task. You are given hypothesis (thought/instruction) and actions. You need to evaluate if how good or bad the action is given the hypothesis. Hypothesis: Alternatively, 2-cell ship might be placed horizontally in the A5-B6 area, as this is one of the few remaining areas large enough to fit it. A5 would be worth checking. Rate all the actions indiviually based on whether the action is aligned with the hypothesis. Action 1: <action>E2</action> Action 2: <action>E5</action> Action 3: <action>A5</action> Make sure the score you assign is between 0 and 1. Please provide your scores in the following format: ... Response: Action 1 for the Hypothesis: <think> E2 is not aligned with the hypothesis at all. The hypothesis suggests checking A5 area for the 2-cell ship, but E2 is in completely different part of the board. Additionally, we already have significant ship presence in the middle columns. </think> <score> 0.1</score> 35 Action 2 for the Hypothesis: <think> E5 is somewhat in the direction of exploring new areas for remaining ships, but its not in the specific A5-B6 area mentioned in the hypothesis. Its not directly supporting the hypothesiss suggested strategy. </think> <score> 0.3</score> Action 3 for the Hypothesis: <think> A5 is exactly what the hypothesis suggests checking. Its directly aligned with the reasoning that the 2-cell ship might be placed horizontally in the A5-B6 area, and its one of the few remaining unexplored areas that could fit 2-cell ship. </think> <score> 1.0</score> Transcript on Action Proposal (Battleship) Prompt: Hit at A5! You have hit ship, which occupies 5 cells in the grid. Here is how the board looks now: 2 . 5 4 3 1 . B . X . . . . . . . . . . Given the information above, please propose 3 actions, make sure each of them is valid action in the environment, and is reasonable. Make sure your thoughts and hypotheses are diverse and unique. However, actions are allowed to be the same if they are the best possible action under that hypothesis. Please provide your actions in the following format: Action 1: <think> ...</think> <answer> your first action</answer> ... Action 3: <think> ...</think> <answer> your 3th action</answer> Response: Action 1: <think> Looking at the pattern of hits for the 5-cell ship, we can see it extends from A3 to A5. Since 5-cell ships are 5 cells long and weve found 3 cells, we should check A2 as its likely part of the 5-cell ship.</think> <answer> A2</answer> Action 2: <think> If the 5-cell ship is not extending westward, it might extend to the east. Even though this is less likely since it would make the ship overlap with known hits, we should consider A1 to eliminate this possibility.</think> <answer> A1</answer> Action 3: <think> Since weve hit three cells of the ship (A3, A4, A5), and this ship is 5 cells long, if its not extending horizontally, it must extend vertically. Therefore, checking C5 could reveal if the 5-cell ship extends downward from A5.</think> <answer> C5</answer> D.2 Prompt Templates Propose Action Prompt (πLLM) Given the information above, please propose some hypotheses and act according to those hypotheses. You can propose at most {num_actions} hypotheses. Please propose reasonable number of hypotheses each hypothesis represents what you think. Please provide your actions in the following format: Action 1: <think>...</think> <answer>action 1</answer> ... Action {num_actions}: <think>...</think> <answer>your {num_actions}th action</answer> Propose Exploration Action Prompt (πref) Given the information above, please propose {num_actions} completely different and unexpected actions. These should be valid in the environment but should explore unusual or creative approaches. Try to think outside the box and propose actions that might not be immediately obvious or conventional. Here are the actions you have already proposed: {actions} Please avoid proposing the same actions. Please provide your actions in the following format: Action 1: <think>...</think> <answer>your first random/exploratory action</answer> ... Action {num_actions}: <think>...</think> <answer>your {num_actions}th random/exploratory action</answer> Hypothesis-Conditioned Value Function Prompt (VLLM) {task description} ======================== Now you have new task. You are given hypothesis (thought/instruction) and actions. You need to evaluate how good or bad the action is given the hypothesis. Hypothesis: <think> {hypothesis} </think> Rate all the actions indiviually based on whether the action is aligned with the hypothesis. Action {action_idx}: <action>{action}</action> Make sure the score you assign is between 0 and 1. Please provide your scores in the following format: Action 1 for the Hypothesis: <think> ... </think> <score>...</score> 37 ... Action {num_actions} for the Hypothesis: <think> ... </think> <score>...</score> D.3 Code Implementation We provide high-level code snippet that demonstrates how we implement the algorithm below. We omit the implementation details of methods involving LLM calls. import numpy as np s HELiX : \" \" \" Simple model \" \" \" f a performs ay o l t . e _ i ( f , e t , hypotheses , i , random_actions ) : random_actions c n = i + random_actions # l e on a o s not None : # a m i s e r c _ r = np . o ( ( ( o s ) , each o s a o i n ( i ) ) ) s e r by l i each o s a o i # l h_idx , o s c s = f . l e _ t _ o s ( e t , o s , i ) r _ r [ h_idx ] = r n enumerate ( o s ) : # ======== l a n p : s u check ======= s u _ i = f . s u _ i ( r _ r , i ) e n s u _ i c e s _ i s not None : # ====== UCB m t ====== r _ r , hypotheses , i = f . _ o s _ m t ( r _ r . copy ( ) , hypotheses , i ) # ====== ( Res i +) l t ===== t _ o s , t _ i , t _ r _ r , t _ i _ i = f . _ a g ( r _ r , hypotheses , i ) u e _ i d o n _ i ( f , r _ r , i ) : max_scores_per_row = np . max( r _ r , s =1) i _ s = [ ] a o _ s . append ( np . where ( r _ r [ ] == max_scores_per_row [ ] ) [ 0 ] . range ( r _ r . shape [ 0 ] ) : l ( ) ) # Convert each l a o _ s = [ ( i ) a o o t a o _ s ] # Find i r t o l r p _ i = reduce ( lambda , : . e c n ( ) , i _ s ) s # Convert back i v a d _ i _ t = t ( r p _ i ) needed ( r p _ i _ t ) == 0 : r r None e : # randomly choose one random_index = np . random . i ( ( r p _ i _ t ) ) u c n [ r p _ i _ t [ random_index ] ] 38 1 2 3 5 6 7 8 9 10 12 13 14 15 16 17 19 20 21 22 23 24 26 27 28 29 30 31 33 34 35 36 37 39 40 41 42 43 44 46 47 48 49 50 51 52 54 55 56 57 58 59 61 62 63 64 65 66 68 69 70 71 72 73 74 76 77 78 79 80 81 83 84 85 86 87 88 89 90 92 93 94 95 96 97 99 100 101 102 103 t _ a g ( f , o _ r , hypotheses , i , random_actions =[]) : # ====== i l ReS i ========= # c t e e r n o m _ u _ i = ( i ) ( random_actions ) random i # avg ( random i ) i _ _ r = np . mean( r _ r [ : , _ u _ i : ] , s =1, keepdims=True ) r i _ r _ r = r _ r c n _ _ r # m t p e again , m z _ r _ r , hypotheses , i = f . _ o s _ m t t e t s ( m z _ r _ r . copy ( ) , hypotheses , i ) t _ o s , t _ i , t _ r _ r , t _ i _ i = f . wo_t iered _argmax_s ampli ng ( m z _ r _ r , hypotheses , i ) u e _ o s , t _ i , t _ r _ r , t _ i _ i d c _ o s _ m t ( f , # Get maximum r r each row ( o s ) max_scores_per_row = np . max( r _ r , s =1) r _ r , hypotheses , i ) : # Find h e i s _ r = np . max( max_scores_per_row ) o a # Get e d _ _ i = np . where ( max_scores_per_row == h _ r ) [ 0 ] rows t have h e n e s e h t e r p i o s rows # e e t _ o s = [ o s [ ] # we y m t hypotheses , not i i e t _ _ i ] # a new r t with y s c rows and columns new_score_matrix = r _ r [ e d _ _ i , : ] u new_score_matrix , l e _ o s , i t wo_t iere d_argm ax_s amplin ( f , r p e , r _ r , hypotheses , i ) : then sample h e c n # we e h e s t r _ r . shape [0 ] == ( o s ) s _ o _ = np . argmax ( np . max( r _ r , s =1)) # s towards s p e b _ i _ = np . argmax ( r _ r [ t _ o _ , : ] ) s _ i = i [ t _ i _ ] t _ o s = o s [ t _ o _ ] t _ r _ r = r _ r [ t _ o _ , t _ i _ ] u e _ o s , t _ i , t _ r _ r , [ t _ i _ ]"
        }
    ],
    "affiliations": [
        "Microsoft Research",
        "Netflix Research",
        "Stanford University",
        "University of Maryland, College Park"
    ]
}