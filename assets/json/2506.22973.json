{
    "paper_title": "Confident Splatting: Confidence-Based Compression of 3D Gaussian Splatting via Learnable Beta Distributions",
    "authors": [
        "AmirHossein Naghi Razlighi",
        "Elaheh Badali Golezani",
        "Shohreh Kasaei"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "3D Gaussian Splatting enables high-quality real-time rendering but often produces millions of splats, resulting in excessive storage and computational overhead. We propose a novel lossy compression method based on learnable confidence scores modeled as Beta distributions. Each splat's confidence is optimized through reconstruction-aware losses, enabling pruning of low-confidence splats while preserving visual fidelity. The proposed approach is architecture-agnostic and can be applied to any Gaussian Splatting variant. In addition, the average confidence values serve as a new metric to assess the quality of the scene. Extensive experiments demonstrate favorable trade-offs between compression and fidelity compared to prior work. Our code and data are publicly available at https://github.com/amirhossein-razlighi/Confident-Splatting"
        },
        {
            "title": "Start",
            "content": "5 2 0 2 8 2 ] . [ 1 3 7 9 2 2 . 6 0 5 2 : r Confident Splatting: Confidence-Based Compression of 3D Gaussian Splatting via Learnable Beta Distributions AmirHossein Naghi Razlighi Sharif University of Technology amirhossein.razlighi@sharif.edu Elaheh Badali Golezani Sharif University of Technology elahe.badali@sharif.edu Shohreh Kasaei Sharif University of Technology kasaei@sharif.edu Figure 1. Overview of proposded confidence-based Gaussian Splatting compression result, on the mip-nerf-360 garden [3] scene. Confidence scores are learned as Beta distributions and used for pruning."
        },
        {
            "title": "Abstract",
            "content": "1. Introduction 3D Gaussian Splatting enables high-quality real-time rendering, but often produces millions of splats, resulting in excessive storage and computational overhead. We propose novel lossy compression method based on learnable confidence scores modeled as Beta distributions. Each splats confidence is optimized through reconstructionaware losses, enabling pruning of low-confidence splats while preserving visual fidelity. The proposed approach is architecture-agnostic and can be applied to any Gaussian Splatting variant. In addition, the average confidence values serve as new metric to assess the quality of the scene. Extensive experiments demonstrate favorable trade-offs between compression and fidelity compared to prior work. Our code and data are publicly available here. 3D Gaussian Splatting (3DGS) [10] has revolutionized 3D scene representation and reconstruction due to its efficiency and quality. However, most scenes (especially those with medium or high complexity) are represented by millions of splats, which impose significant storage and rendering overhead. Before the advent of 3DGS, novel view synthesis (NVS) was primarily performed using Neural Radiance Field (NeRF) [15], which employs volumetric rendering [17] for differentiable rendering. This method produces high-quality results with strong theoretical foundations. However, its rendering pipeline is computationally intensive, making it unsuitable for real-time applications. Although some works like InstantNGP [16] tried to use HashGrid encoding with much smaller MLP, it still incorporated the volumetric rendering, and thus had those limitations. In contrast, 3DGS adopts point-based method that leverages GPU pipelines, which have been optimized 1 over decades for rasterization. By using this form of differentiable rasterization, it achieves comparable visual quality with real-time rendering speeds. One major drawback of 3DGS is its unbounded number of splats. By using adaptive filtering, the number of splats can easily reach millions for mediocre and complex scenes [14]. Studies have shown that many of these splats are not crucial for the final visual fidelity. Some of these splats are designed to reconstruct very fine-grained details, which contribute virtually nothing to perceivable visual quality from the viewpoint of humans. fraction of these splats also consists of floaters or artifacts that are not part of the original scene geometry [1]. These issues have opened up research direction for compressing 3DGS scenes. Two branches have been made to achieve such goal. One is compressing splats properties (such as Spherical Harmonics (SH) coefficients) without any loss on the final reconstruction. [1, 9, 13] These approaches focus on representation mostly. The other method to tackle this issue, is to consider some loss on the final reconstruction quality, as long as the quality-size tradeoff is significant enough. Our method lies in the second category. While similar methods in this category tried to tackle this problem, our method proposes novel way for compressing (and also evaluating) the 3DGS scenes. We introduce new method for estimating confidence score for each splat. Our method is easily transferrable from one method of 3DGS to another (as we will demonstrate further) without any removal in the original pipeline. Via estimating parameters of Beta distribution over each splat, we will estimate its confidence score. Furthermore, we will optimize this property of splats by introducing useful loss functions and regularizations in the pipeline. Finally, after training is completed, our method can act as an evaluator (for how high-quality are the splats) and also act as knobbased compression technique, giving the user the freedom to choose its optimal quality-size tuple for its own usage. In summary, we propose the following: 1. new confidence score estimation method is proposed which is based on estimating α and β parameters and estimating Beta distribution placed on each splat. 2. The sparsity loss, entropy loss, and ranking-based saliency loss are applied to optimize the α and β parameters that determine the confidence scores. These confidence scores are also used in the rasterization process to further optimize the parameters values by integrating them into the opacity computation. 3. After the training phase (which can be transferred to any 3DGS pipeline with minimum effort), the user can use the knob-based compression method by setting threshold for the confidence of splats. This allows generating compressed scene (by removing low-confidence splats) with minimal degradation in visual fidelity. 4. It is shown that the proposed method can also serve as metric to evaluate 3DGS methods across different scenes to understand and compare the quality of splats, where higher average confidence indicates higher-quality scene. 2. Related Works In this section, we provide an overview of novel view synthesis and 3DGS, followed by review of recent advancements in the field with focus on compression. 2.1. Novel view synthesis 3D reconstruction is the process of creating threedimensional model from 2D images. In the 3D reconstruction field, Novel View Synthesis (NVS) [19] aims to generate images of scene from viewpoints that were not part of the original observations, allowing virtual exploration from new perspectives. High resolution and sufficient multi-view input images are necessary for realistic novel view synthesis in order to guarantee precise visual reconstruction. Various machine learning techniques used for this purpose include epipolar geometry and stereo vision [12], depth estimation [5], generative models [6], and neural scene representations such as Neural Radiance Field (NeRF) [15]. Nevertheless, each of these methods also has its drawbacks. Epipolar geometry and stereo vision are threatened by imprecise calibration and occlusions or textureless regions. Depth estimation fails to generalize well on different domains and complex scenes. Generative models produce blurry output and are problematic for training and, NeRFs require dense views, lengthy training, and are costly computationally. Considering the restrictions imposed by the present methods in speed, scalability, and visual quality, there is an increase in the need for solutions that can deliver real-time performance along with high-quality synthesis. 2.2. 3D Gaussian Splatting 3DGS [10] is rendering and scene representation technique that models 3D environments using spatially distributed anisotropic Gaussian functions, referred to as Gaussian splats. Each splat holds the information about its location, geometry, and orientation (via covariance), color, and transparency, thus enabling an efficient and adaptive representation of the scene. To render the splats, forwardsplatting process is used, which enables them to blend seamlessly thus giving the result of photorealistic and very intricate reconstructions of the original scene. 3DGS is an explicit real-time method that makes it highly time-efficient during both training and testing. In addition, due to the continuous nature of Gaussian functions, smooth level-of-detail transitions and intrinsic anti-aliasing can be achieved, resulting in high visual quality across resolution changes and viewpoint variations [7]. These properties of 3DGS make it an appropriate approach for novel view synthesis, interactive scene exploration, and real-time rendering of large-scale environments. 3DGS generally requires large number of Gaussians to model scene, thus consuming lot of memory and having computational overhead at training and rendering time. However, not all of these Gaussians contribute equally to the final reconstructed scene. In practice, only fraction of the relevant Gaussians is ultimately required for good scene representation. As result, there is significant potential for compression by identifying and retaining only the most relevant Gaussians, thereby improving efficiency without sacrificing visual fidelity [7]. 2.3. Compression and Pruning in 3DGS Different methods have been proposed to prune and compress 3DGS scene. These methods can be categorized into two main categories: 1. Changing the representation: Several methods aim to modify the main representation of Gaussians or specific property of them such as spherical harmonics coefficients. Although these methods change the representations, these are lossless meaning that they preserve original visual quality. Various approachs have been proposed, such as using tri-planes, bilateral grids, and quantized embeddings [1, 9, 13]. While these methods can achieve strong results compared to original 3DGS, they require changing the whole pipeline and representation, making them less scalable. Moreover, applying them to different methods and strategies of 3DGS requires significant effort. 2. Pruning the scene: Different methods have been proposed to prune the 3DGS scene and reduce the number of splats per scene. These approaches can be applied either during training or at test time and may be either lossy or lossless. For instance, RadSplat [18] is method for combining NeRF supervision on the point-based optimizations of 3DGS. During training, it defines an importance score based on ray contribution, and changes the Adaptive filtering process by adding an extra check in the training (removing splats with importance less than predefined threshold). While this method achieves state-of-the-art performance compared to methods such as original 3DGS [10], ZipNeRF [4], and Baked SDF [20], it requires changing the pipeline and also it needs re-training for every new threshold, unlike our method, which can be seamlessly added on top of any existing pipeline. Mini-splatting [8] leverages similar approach for pruning. It also provides new approach instead of using Adaptive filtering, which refines the distribution of Gaussian centers in the scene (which is out of scope of our study in this paper). Another method, LP3DGS [21] learns the optimal pruning ratio during training using Gumbel-Sigmoidbased importance metric. This method is train-time method and does not provide user-controllable trade-off between visual quality and scene size. In contrast, our method not only outperforms all of these approaches but also operates as test-time method. It offers users full flexibility to choose their desired balance between visual fidelity and scene compression. 3. Prepared Dataset To evaluate proposed method across diverse range of scenarios, we constructed and curated several scenes through two complementary approaches. YouTube-Reconstructed Scenes. We gathered public videos from YouTube depicting well-known landmarks, such as the Eiffel Tower. From these videos, high-quality frames were extracted at regular intervals and Structurefrom-Motion (SfM) was performed to reconstruct sparse 3D point clouds. These reconstructions were subsequently used as initialization for generating full 3DGS scenes. This setup allows us to validate proposed method under unconstrained, in-the-wild conditions with varying lighting, motion, and occlusion. Confidence-Enhanced Scene Dataset. In addition to custom scenes, we applied our method on diverse set of standard 3DGS benchmark datasets. For each processed scene, we released the full set of Gaussians, augmented with our optimized confidence scores. This curated dataset provides rich resource for downstream tasks, including confidence-guided mesh reconstruction, uncertainty-aware view synthesis, and automatic pruning strategies. We hope it facilitates future research on compression, quality estimation, and the interpretability of point-based neural representations. All datasets, preprocessing scripts, and trained scenes with confidence annotations will be publicly available upon acceptance. You can find more info on the dataset details in supplementary material 7. 4. Proposed Method We propose method for learning per-splat confidence scores to enable both lossy compression and scene quality assessment in 3DGS. These scores are derived from learnable Beta distribution and are optimized jointly with the reconstruction loss. During training, the confidence affects rendering; post-training, it supports user-defined pruning for efficient storage and rendering. You can see different components of our pipeline, working alongside any desired 3DGS pipeline in Fig. 2. 3 Figure 2. Overview of confident splatting method. We extend any 3DGS pipeline by introducing learnable confidence parameters (α, β) for each splat, modeled by Beta distribution. (a) The original pipeline remains unchanged, except for an added opacity modulation using confidence values. (b) Confidence parameters are optimized via entropy, sparsity, and saliency losses; sample distributions are shown. (c) Final renderings (and optional confidence heatmaps) are produced via standard rasterization. Saliency (image-space gradients) is stored and used for detached saliency-based ranking (Dashed arrows indicate non-differentiable paths). 4.1. Confidence Estimation with Beta Distributions 4.2. Loss Functions and Optimization For confidence score estimation of each splat, unlike previous methods, we do not regress simple confidence(ci) value. Instead, Beta distribution is assumed for each splat, represented by two parameters α and β. The Beta distribution parameterized by α and β is defined as follows: To optimize our confidence score parameters, we leverage using three regularizers, alongside whatever loss function the main pipeline is using. For instance, the original 3DGS [10] pipeline uses SSIM loss and RGB L1 loss, denoted as Reconstruction loss. We have: fX (x; α, β) = Γ(α + β) Γ(α).Γ(β) xα1.(1 x)β1 = 1 B(α, β) .xα1.(1 x)β1 (1) Ltotal = Lrec + λ1.Lsparse + λ2.Lent + λ3.Lsal (4) Where sparsity loss encourages fewer active splats. Active Splats denotes the splats with confidence more than 50% (ci >= 50%) Furthermore, the expected value of Beta(α, β) can be calculated as follows: E[X] = = = = (cid:90) (cid:90) x.fX (x)dx 1 B(α, β) 1 B(α, β) α α + β .x(α+1)1.(1 x)β1dx .B(α + 1, β) = Γ(α + β) Γ(α).Γ(β) . Γ(α + 1).Γ(β) Γ(α + β + 1) (2) So, for each splat, Beta distribution is fitted to its confidence value (parameterized by α and β which are optimized within the pipeline), and the confidence is computed as follows: ci = E[Ci] = αi αi + βi ; Ci Beta(αi, βi) (3) 4 Lsparse = 1 (cid:88) i=1 ci (5) Also, negative entropy loss avoids overconfidence in splats. We know that the low-entropy for our beta distribution mean that our distribution is very peaked near 0 or 1, thus means that the ci is overconfident in the fact that that specific splat is useful (ci = 1) or useless(ci = 0). On the other hand, high entropy for our beta distribution means the distribution is uncertain (flat, near uniform). So, we penalize low entropy to discourage the optimizer from pushing all α β (conf idence 1) or pushing β α (conf idence 0) Lent = Ei[H(Beta(Sof tplus(αi), Sof tplus(βi))] (6) Finally, Saliency ranking loss is proposed. Most of the methods that are using confidence scores, use absolute values to guide confidence values through optimization (like Figure 3. Compression evaluation on different scenes, to show how confidence thresholding affects number of kept splats in the scene (and thus, reduces the size of the whole scene, proportionally). image gradients). On the other hand, we use relative approach towards confidence calculation. Lets take the highest loss (in image space) splats (easily obtained via image gradients) and the lowest loss splats. We sample of them (the higher the , the more accurate is the optimization, but the training will be slower due to more computation). So, is set of saliency-ranked splat index pairs. Our rankingbased saliency loss ensures that splats with higher contribution to gradients are more confident in comparison with splats with lower contributions Lsal = 1 (cid:88) (i,j)P max(0, 1 + cj ci) (7) Where is the index for splat with higher contribution to gradients and is index for splat with lower contribution. 4.3. Rasterization with Confidence Modulation It is obvious that confidence scores of each splat should be somehow related to the final rendered image. From the concept of confidence, we expect that higher confidence splats contribute more in the rendered image (and be more impactful) than the ones with lower confidence. phenomena is modeled by re-calculating opacity before the rasterization step: oeffective = σ(ooriginal ).ci (8) In this way, each splats opacity is modulated by its current confidence score before rasterization and thus, the confidence score is directly integrated into the rendering process. 4.4. Post-Training Pruning and User-Controlled"
        },
        {
            "title": "Compression",
            "content": "Note that we are not explicitly removing any splat based on its confidence in the training process. Confidence values are estimated and optimized (via our beta distribution prior for each splat) and stored as properties for each splat. After training process has finished, user can simply use these confidence values as knob-based compression and remove splats less than certain confidence score based on his needs. The user can choose the threshold which is the best for him in terms of keeping high visual fidelity Vs lowering the number of splats as possible. 4.5. Transferability and Compatibility It should be mentioned that the proposed method do not rely on the base pipeline of 3DGS anywhere of our confidence estimation process. The only thing that matters is that the pipeline should have base properties of splats (including SH coefficients or RGB colors, Rotation, Scale, and Opacity). Also, this method does not change any part of the pipeline, it only adds new property to each splat. The only minor affect it may have on the base pipeline is that the opacity calculation is enhance via confidence values. In the Experiments section 5 we will show that this change not only does not degrade the quality, but also improves quality metrics in some cases for different 3DGS methods. 5. Experiments We conduct extensive experiments to evaluate the effectiveness of proposed confidence-based compression scheme for 3DGS. Our goals are to assess: (1) compression-quality trade-offs, (2) generalizability across datasets, and (3) compatibility with other 3DGS variants. 5.1. Experimental Setup We evaluate our method on diverse suite of scenes from various datasets: BILARF test scenes: building, chinesearch, lionpavilion, nighttimepond, pondbike, statue, strat Custom Eiffel Tower Scene: constructed from YouTube footage via frame extraction and SfM (see Section 3) MipNeRF-360: garden MipNeRF-360-extra: flowers, treehill ZipNeRF dataset: berlin scene Tanks&Temples dataset: truck and train scenes. (a) Quantitative plot: PSNR and number of splats vs threshold. (b) Visual comparison: original vs sample confidence-pruned. Figure 4. Compression evaluation on the Garden scene. (a) metric trends. (b) rendering with and without pruning. For each scene, we train both the baseline 3DGS model [10] and our proposed method on top of it, which augments splats with learnable Beta-distributed confidence scores. After training, splats are pruned below confidence threshold τ , sweep across range of τ values, and evaluate rendering quality and scene sparsity. To demonstrate compatibility with other 3DGS pipelines, the same protocol is repeated using the MCMCGS [11] variant. We report results for: MCMC-GS (baseline) Ours@MCMC-GS: The proposed method applied on top of MCMC-GS, optimizing confidence scores alongside the base model 5.2. Quantitative Results: Compression vs Quality the compression-quality trade-off curves are plotted by sweeping confidence thresholds and measuring PSNR on the test views. For each threshold, we record: Number of retained splats PSNR, SSIM, and LPIPS scores As seen in Figure 5, our method achieves high PSNR with substantially reduced number of splats. Compared to the original 3DGS, our confidence scores provide better signal for identifying redundant or low-impact splats. In most scenes, number of splats drops to half  (Fig. 3)  while the quality metrics like PSNR do not change or change with really small tolerance  (Fig. 5)  . (a) PSNR (b) SSIM (c) LPIPS Figure 5. Effect of confidence-based pruning on visual quality metrics across different datasets. Note how metrics degrade slowly while number of splats drops drastically. Also, this fact (near zero degradation in the first confidence pruning values) can be seen in the right plot, where we have plotted the original scene and sample pruned scene (with confidence value 5%). 5.3. Qualitative Results 5.4. Comparison To visually assess our method, we provide renderings at various compression thresholds. In Figure 6, you can see renderings of sample scene (Garden scene from MipNeRF360 dataset [2]), while we sweep and prune the splats with confidence values under certain threshold. Number of kept splats is also written in these plots. In Fig 4, more than half of the splats are pruned in the garden scene, with less than 0.5 decrease in PSNR metric. Alongside basic metrics for assessing quality (PSNR, SSIM, LPIPS) and size (number of splats in the scene), we introduce new metric to show how well is the qualitycompression trade-off. This metric is between 0 and 1 and the lower it is, the better and should be worse if the quality (represented by PSNR) is low (0 at its lowest). It should be minimized if for certain PSNR, number of splats is decreased. The proposed formulation is as follows:"
        },
        {
            "title": "Method",
            "content": "PSNR SSIM LPIPS #Gaussians SQR"
        },
        {
            "title": "Eiffel Tower",
            "content": "3DGS Rad-Splat @ 50% Mini-Splatting @ 50% Ours@Base3DGS @100% Ours@Base3DGS @50% Ours@MCMC @100% Ours@MCMC @95% Ours@MCMC @90% 21.450 21.367 21.382 21.383 21.383 21.592 21.334 20.614 22.877 3DGS 22.524 Rad-Splat @ 50% 22.189 Mini-Splatting @ 50% Ours@Base3DGS @100% 24.077 Ours@Base3DGS @27% 23.552 Ours@MCMC @100% 24.653 Ours@MCMC @95% 24.653 Ours@MCMC @90% 24.593 0.596 0.589 0.5871 0.5907 0.5906 0.6050 0.5941 0.5684 0.7969 0.7919 0.7884 0.843 0.8329 0.8585 0.8585 0.8581 0.345 0.361 0.360 0.360 0.360 0.341 0.347 0. 0.138 0.281 0.283 0.082 0.087 0.072 0.072 0.072 3, 590, 000 1, 751, 775 3, 384, 239 4, 572, 755 2, 227, 814 1, 000, 000 963, 860 915, 488 600, 000 395, 873 803, 662 1, 496, 522 408, 564 1, 000, 000 950, 000 904, 098 0.1433 0.0757 0.1366 0.1761 0.0943 0.0442 0.0432 0.0425 0.2077 0.1484 0.2658 0.3833 0.1478 0.2885 0.2781 0.2688 Table 1. compact view of our comparisons among different methods and pruning ratios. @X% means that specific method, is keeping X% of the splats initially in its scene. For more comprehensive comparison, please refer to the supplementary materials Tab. SQR = num of splats num of splats + SN scale (9) Here, SQR stands for Splats-to-Quality Ratio. Also, scale is the order of number of splats in the original scene (withIt can vary from 1e3 to 1e6. For out any compression). example, if base 3DGS scene (without any compression or pruning) has 3, 000, 000 splats, the scale would be 1, 000, 000. This is to normalize the importance of quality and number of splats at the same time, without one being largely more important than the other. 5.5. Transferability Across Pipelines To validate the generalization of our method, we apply it to the MCMC-GS [11] variant. As shown in Table 3, the method achieves similar compression-quality tradeoffs when applied to MCMC-GS, confirming its compatibility with other splatting-based rendering pipelines. The point of our pipeline is (as it can be seen in Fig 2) that it is fully transferable and makes no changes to the underlying logic of the method (like specific adaptive filtering strategies, rendering methods, etc.). It can be interpreted as appending new properties to splats which are being calculated in their own specific way. After training, the degree of compression (pruning) of unimportant splats can be controlled based on the calculated confidence scores for each splat in the scene. Scene Method PSNR@Orig PSNR@95% PSNR@90% Building Flowers Eiffel Tower MCMC-GS Ours@MCMC-GS MCMC-GS Ours@MCMC-GS MCMC-GS Ours@MCMC-GS 26.09 25.96 21.65 21.59 24.57 24. 25.62 21.27 24.65 24.39 20.19 24. Table 2. Compression results on MCMC-GS [11] scenes. We report PSNR at original resolution, and after pruning 5% and 10% of total splats. All scenes originally contain 1, 000, 000 splats. 5.6. Scene Quality Assessment Metric Finally, we investigate how the average confidence score of scene correlates with its quality. It can be observed that scenes with fewer floaters/artifacts tend to exhibit higher mean confidence. Thus, our confidence scores may serve as metric for evaluating reconstruction quality or even guiding automated scene refinement. The quality assessment metric can be defined as follows: ACS = 1 (cid:88) i=1 ci (10) Here, ACS stands for Average Confidence Score and ci stands for the confidence score of ith splat. By comparing the scenes in our experiments, both in quantitative and qualitative manners, we can see that for the same scene, the"
        },
        {
            "title": "Method",
            "content": "PSNR@Orig SSIM@Orig% LPIPS@Orig% ACS"
        },
        {
            "title": "Garden",
            "content": "Ours@MCMC-GS Ours@3DGS Ours@MCMC-GS Ours@3DGS Ours@MCMC-GS Ours@3DGS Ours@MCMC-GS Ours@3DGS Ours@MCMC-GS Ours@3DGS Ours@MCMC-GS Ours@3DGS Ours@MCMC-GS Ours@3DGS 25.96 25.55 21.59 21.38 24.65 24.08 25.82 25.21 22.61 21. 22.96 22.53 26.80 27.16 0.9057 0.8844 0.6049 0.5906 0.8585 0.8431 0.8840 0. 0.8285 0.8017 0.6456 0.6308 0.8390 0.8572 0.0863 0.1169 0.3413 0.3604 0.7221 0. 0.0888 0.1034 0.1445 0.1764 0.3520 0.3421 0.1176 0.0814 0.9391 0.3063 0.9236 0. 0.6712 0.1874 0.8105 0.2383 0.7846 0.3434 0.9184 0.3967 0.8548 0.2628 Table 3. Comparing MCMC method [11] (with less floater and artifacts) and original 3DGS method [10] (with more artifacts and lower quality) in terms of scene quality (ACS). method with higher quality (PSNR) and lower artifacts and floaters, has higher ACS in comparison with other scenes (with higher number of floaters and/or lower quality in terms of PSNR or SSIM or LPIPS). 6. Conclusion and Future direction An efficient method to estimate confidence scores accurately using beta distributions fitted on top of each splat in the scene is proposed in this paper. Our method gives the freedom to users to choose their required compressionquality trade-off in test-time with no additional re-training or extra computations needed. These scenes (trained with different methods) with estimated confidence scores can be used for other research directions and problems too. It is worth noting that our confidence estimation framework addresses two of the most challenging problems in 3DGS: main object extraction and floater removal. Please refer to supplementary material for more explanations on these matters (8, 9). 8 Figure 6. Qualitative renderings at varying confidence thresholds. Our confidence-aware pruning preserves visual fidelity while discarding low-impact splats. [13] Soonbin Lee, Fangwen Shu, Yago Sanchez, Thomas Schierl, and Cornelius Hellge. Compression of 3d gaussian splatting with optimized feature planes and standard video codecs. arXiv preprint arXiv:2501.03399, 2025. 2, 3 [14] Yifei Liu, Zhihang Zhong, Yifan Zhan, Sheng Xu, and Xiao Sun. Maskgaussian: Adaptive 3d gaussian representation from probabilistic masks. In Proceedings of the Computer Vision and Pattern Recognition Conference, pages 681690, 2025. 2 [15] Ben Mildenhall, Pratul Srinivasan, Matthew Tancik, Jonathan Barron, Ravi Ramamoorthi, and Ren Ng. Nerf: Representing scenes as neural radiance fields for view synthesis. Communications of the ACM, 65(1):99106, 2021. 1, 2 [16] Thomas Muller, Alex Evans, Christoph Schied, and Alexander Keller. Instant neural graphics primitives with multiresolution hash encoding. ACM transactions on graphics (TOG), 41(4):115, 2022. 1 [17] Michael Niemeyer, Lars Mescheder, Michael Oechsle, and Andreas Geiger. Differentiable volumetric rendering: Learning implicit 3d representations without 3d supervision. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 35043515, 2020. 1 [18] Michael Niemeyer, Fabian Manhardt, Marie-Julie Rakotosaona, Michael Oechsle, Daniel Duckworth, Rama Gosula, Keisuke Tateno, John Bates, Dominik Kaeser, and Federico Tombari. Radsplat: Radiance field-informed gaussian splatting for robust real-time rendering with 900+ fps. arXiv preprint arXiv:2403.13806, 2024. 3, 2 [19] Weihao Xia and Jing-Hao Xue. survey on deep generative 3d-aware image synthesis. ACM Computing Surveys, 56(4): 134, 2023. [20] Lior Yariv, Peter Hedman, Christian Reiser, Dor Verbin, Pratul Srinivasan, Richard Szeliski, Jonathan Barron, and Ben Mildenhall. Bakedsdf: Meshing neural sdfs for realtime view synthesis. In ACM SIGGRAPH 2023 Conference Proceedings, pages 19, 2023. 3 [21] Zhaoliang Zhang, Tianchen Song, Yongjae Lee, Li Yang, Cheng Peng, Rama Chellappa, and Deliang Fan. Lp-3dgs: arXiv preprint Learning to prune 3d gaussian splatting. arXiv:2405.18784, 2024. 3,"
        },
        {
            "title": "References",
            "content": "[1] Muhammad Salman Ali, Chaoning Zhang, Marco Cagnazzo, Giuseppe Valenzise, Enzo Tartaglione, and Sung-Ho Bae. Compression in 3d gaussian splatting: survey of arXiv preprint methods, arXiv:2502.19457, 2025. 2, 3 trends, and future directions. [2] Jonathan T. Barron, Ben Mildenhall, Dor Verbin, Pratul P. Srinivasan, and Peter Hedman. Mip-nerf 360: Unbounded anti-aliased neural radiance fields. 2022 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pages 54605469, 2021. 6 [3] Jonathan Barron, Ben Mildenhall, Dor Verbin, Pratul Srinivasan, and Peter Hedman. Mip-nerf 360: Unbounded In Proceedings of anti-aliased neural radiance fields. the IEEE/CVF conference on computer vision and pattern recognition, pages 54705479, 2022. 1 [4] Jonathan Barron, Ben Mildenhall, Dor Verbin, Pratul Srinivasan, and Peter Hedman. Zip-nerf: Anti-aliased In Proceedings of the grid-based neural radiance fields. IEEE/CVF International Conference on Computer Vision, pages 1969719705, 2023. 3 [5] Ang Cao, Chris Rockwell, and Justin Johnson. Fwd: Realtime novel view synthesis with forward warping and depth. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 1571315724, 2022. [6] Eric Chan, Koki Nagano, Matthew Chan, Alexander Bergman, Jeong Joon Park, Axel Levy, Miika Aittala, Shalini De Mello, Tero Karras, and Gordon Wetzstein. Generative novel view synthesis with 3d-aware diffusion models. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 42174229, 2023. 2 [7] Guikun Chen and Wenguan Wang. survey on 3d gaussian splatting. arXiv preprint arXiv:2401.03890, 2024. 2, 3 [8] Guangchi Fang and Bing Wang. Mini-splatting: Representing scenes with constrained number of gaussians. In European Conference on Computer Vision, pages 165181. Springer, 2024. 3, 2 [9] Sharath Girish, Kamal Gupta, and Abhinav Shrivastava. Eagles: Efficient accelerated 3d gaussians with lightweight encodings. In European Conference on Computer Vision, pages 5471. Springer, 2024. 2, 3 [10] Bernhard Kerbl, Georgios Kopanas, Thomas Leimkuhler, and George Drettakis. 3d gaussian splatting for real-time radiance field rendering. ACM Trans. Graph., 42(4):1391, 2023. 1, 2, 3, 4, 6, 8 [11] Shakiba Kheradmand, Daniel Rebain, Gopal Sharma, Weiwei Sun, Yang-Che Tseng, Hossam Isack, Abhishek Kar, Andrea Tagliasacchi, and Kwang Moo Yi. 3d gaussian splatting as markov chain monte carlo. Advances in Neural Information Processing Systems, 37:8096580986, 2024. 6, 7, 8, 2 [12] Haechan Lee, Wonjoon Jin, Seung-Hwan Baek, and Sunghyun Cho. Generalizable novel-view synthesis using stereo camera. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 4939 4948, 2024. 10 Confident Splatting: Confidence-Based Compression of 3D Gaussian Splatting via Learnable Beta Distributions"
        },
        {
            "title": "Supplementary Material",
            "content": "7. YouTube Gathered Dataset Public videos from YouTube are gathered depicting wellknown landmarks, such as the Eiffel Tower. From these videos, high-quality frames are extracted at regular intervals and Structure-from-Motion (SFM) is performed to reconstruct sparse 3D point clouds. By using these SFM point-clouds and also the main convention of dataset creation (with different image resolutions and also camera distortions) in the 3DGS research area, we created scenes with high diversity and also real-world conditions such as people walking around, high details in weather, trees and environment, and also camera not moving in simple cyclic path. You can access more details on our dataset and how to access it publicly . Also, the channels and videos that are used from YouTube are cited and linked in our repository. We have used Walking Tour videos from YouTube channels like LADmob and The CamFam Adventures (1), The CamFam Adventures (2), and Alta Walk. Confident Splatting Dataset contains four large scale, in-the-wild scenes: 1. Perspolis Scene: Consists of 114 images of part of Takht-e-jamshid monument in Iran. It does not contain any serious distractors, but learning its surfaces and texture is hard since they are almost identical walls covered with soil and some historical carvings. 2. Eiffel Tower Scene: Consists of 1108 images from Efiel Tower. This scene is one of the hardest scenes to reconstruct among in-the-wild scenes, since it has many distractors (people moving around in front of of the Eiffel Tower). Also, it is large scale scene (containing trees and the environment in front of and behind of the main structure) and serious changes in camera movement. 3. Big Ben Scene: Consists of 213 images from the Big Ben landmark in London. This scene also contains lot of distractors like people, telephone booth, and buses moving around. It starts from distant view of the Big Ben, and goes toward it in non-straight path. 4. Louvre Museum Scene: Consists of 95 images from the outside view of the building of Louvre Museum. This scene contains people moving around (as distractors) in front of the main structure. Also, the glass texture of the main structure alongside light reflections and refractions make this hard scene for reconstructing its color and structure correctly. Also, the task of main object prediction in this scene is hard since this scene consists of many building around the main structure. Please refer to Fig 7 for some sample images from each of these scenes. Figure 7. Some samples from each scene of our dataset. From left to right: Perspolis, Eiffel Tower, Louvre Museum, and Big Ben scenes. 8. Main object extraction One of the usages of proposed method that can be studied further (in separate work) is extracting the main object of the scene. Our results on various scenes show that when the confidence level is increased high enough, we get to point that many of the surrounding objects get removed and the main object of the scene remains (For instance, refer to 6 when threshold gets over 60%). By using separate module on top of our confidence estimation, or even by enhancing our confidence estimation pipeline for the purpose of Main object extraction, this task seems doable with high enough quality. As you can see in Fig 8, by increasing confidence thresholds, the detail objects (like trees, environment objects, and background objects) are removed incrementally and the main object (alongside some other things, like confident reconstructed buildings) are remained. 9. Floaters removal Another visionary for building on top of our pipeline is floater removal. This line of work is famous problem in novel view synthesis and is being studied for long time. Although the main aspect of our work is not improving the scene quality, it can be used to remove the floaters (which can be interpreted as low confidence points in the scene in comparison with others). For instance, refer to Fig 8 to see how different confidence thresholds result in removing different kind of floaters in the scene (inside our train/test camera views or outside them). 1 Garden Scene Statue Scene Truck Scene Eiffel Tower Scene 0 5 . 0 5 7 . 0 Figure 8. Visualizations among different scenes (horizontal axis) and different confidence thresholds (vertical axis). Note that these captures are from perspective view (which are not in any of the test or train cameras) in our customized viewer. These scenes are intentionally shown from high distance (like birds-eye view) to capture all floaters even outside our train/test camera frustums. You can see how many of these floaters are removed by increasing our confidence threshold. chose to stick with the simpler, yet more powerful version of our pipeline that does not use the additive Gumbel noise. 11. more in-depth comparison In Table 4 you can see more comprehensive comparison of our model (added on top of original 3D gaussian splatting [10] and also Gaussian Splatting MCMC [11]) alongside the original 3DGS and also other pruning methods like RadSplat [18] and Mini-Splatting [8] with pruning ratio set to 50%. Please note the important difference between our method and these other methods, which is that our method is doing the pruning via thresholding in test-time, and thus, can adapt to any desired ratio that user wants based on the quality-compression tradeoff. But these two methods use training-time pruning, so whenever user wants to change the pruning ratio, they would need to re-train the scene with new parameter. 10. Effect of adding noise One of the methods for improving importance score (confidence score) prediction (which is value between 0 and 1) is to add some multiplicative noise values. One of the most recent proposals in this area is LP-3DGS[21] paper which uses Gumbel Noise in multiplicative way with its importance score and then applying sigmoid function. similar approach after calculating the expected value of confidence for each splat (which is α α+β ) is studied in Confident Splatting. Two cases have been experimented: ci = Sigmoid( ˆα ˆα + ˆβ Gumbel noise τ ) (11) Here, the Gumbel noise is added as multiplicative noise for our estimated confidence and then applied Sigmoid to make the final prediction smooth prediction between [0, 1]. The τ parameter is the temperature value for Gumbel noise, which is set to 2 or 3. ci = Sigmoid( ˆα ˆα + ˆβ + Gumbel noise τ ) (12) Here, the Gumbel noise is added as an additive noise for the estimated confidence and then Sigmoid is applied to make the final prediction smooth prediction between [0, 1]. While the additive version results in better estimations than the multiplicative version, their results are not better than the original predictions using raw ci = ˆα . So, we ˆα+ ˆβ"
        },
        {
            "title": "Method",
            "content": "PSNR SSIM LPIPS #Gaussians SQR"
        },
        {
            "title": "Treehill",
            "content": "3DGS Rad-Splat @ 50% Mini-Splatting @ 50% Ours@Base3DGS @100% Ours@Base3DGS @50% Ours@MCMC @100% Ours@MCMC @95% Ours@MCMC @90% 21.450 21.367 21.382 21.383 21.383 21.592 21.334 20.614 22.877 3DGS 22.524 Rad-Splat @50% 22.189 Mini-Splatting @50% Ours@Base3DGS @100% 24.077 Ours@Base3DGS @27% 23.552 Ours@MCMC @100% 24.653 Ours@MCMC @95% 24.653 Ours@MCMC @90% 24.593 3DGS Rad-Splat @50% Mini-Splatting @50% Ours@Base3DGS @100% Ours@Base3DGS @33% Ours@MCMC @100% Ours@MCMC @95% Ours@MCMC @90% 25.37 25.4187 25.3142 25.208 24.1741 25.8155 25.6068 24.7306 21.5736 3DGS 21.9755 Rad-Splat @50% 21.6474 Mini-Splatting @50% Ours@Base3DGS @100% 21.7970 Ours@Base3DGS @39% 20.277 Ours@Base3DGS @34% 19.4559 Ours@MCMC @100% 22.6075 Ours@MCMC @95% 22.4773 Ours@MCMC @90% 22. 3DGS Rad-Splat @50% Mini-Splatting @50% Ours@Base3DGS @100% Ours@Base3DGS @47% Ours@MCMC @100% Ours@MCMC @95% Ours@MCMC @90% 22.4794 22.4880 22.4921 22.5340 21.5555 22.9600 22.5186 21.3151 0.596 0.589 0.5871 0.5907 0.5906 0.6050 0.5941 0.5684 0.7969 0.7919 0.7884 0.843 0.8329 0.8585 0.8585 0.8581 0.8737 0.8818 0.8801 0.8725 0.8564 0.8840 0.8801 0.8608 0.807 0.8133 0.8085 0.8017 0.7466 0.7136 0.8285 0.8253 0. 0.6297 0.6367 0.6362 0.6308 0.5997 0.6456 0.6348 0.6002 0.345 0.361 0.360 0.360 0.360 0.341 0.347 0.360 0.138 0.281 0.283 0.082 0.087 0.072 0.072 0.072 0.099 0.147 0.149 0.103 0.113 0.088 0.091 0.107 0.1635 0.2101 0.2161 0.1764 0.216 0.2413 0.1445 0.1447 0.1563 0.3326 0.3505 0.3490 0.3421 0.3723 0.3520 0.3598 0. 3, 590, 000 1, 751, 775 3, 384, 239 4, 572, 755 2, 227, 814 1, 000, 000 963, 860 915,488 600, 000 395,873 803, 662 1, 496, 522 408, 564 1, 000, 000 950, 000 904, 098 2, 610, 000 1, 291, 794 2, 555, 598 3, 563, 759 1, 142, 117 1, 000, 000 950, 877 895,876 1, 127, 221 543, 573 1, 043, 839 1, 318, 148 518, 527 459,986 1, 000, 000 956, 764 902, 820 3, 742, 946 1, 745, 215 3, 301, 114 4, 399, 502 2, 092, 794 1, 000, 000 955, 394 884, 206 0.1433 0.0757 0.1366 0.1761 0.0943 0.0442 0.0432 0. 0.2077 0.1484 0.2658 0.3833 0.1478 0.2885 0.2781 0.2688 0.0932 0.0483 0.0916 0.1238 0.0451 0.0372 0.0358 0.0349 0.0496 0.0241 0.0460 0.0570 0.0249 0.0230 0.0423 0.0408 0.0394 0.1427 0.0720 0.1279 0.1633 0.0884 0.0417 0.0406 0.0398 Table 4. complete view of our comparisons among different methods and pruning ratios. @X% means that specific method, is keeping X% of the splats initially in its scene."
        }
    ],
    "affiliations": [
        "Sharif University of Technology"
    ]
}