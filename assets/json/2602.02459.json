{
    "paper_title": "TIC-VLA: A Think-in-Control Vision-Language-Action Model for Robot Navigation in Dynamic Environments",
    "authors": [
        "Zhiyu Huang",
        "Yun Zhang",
        "Johnson Liu",
        "Rui Song",
        "Chen Tang",
        "Jiaqi Ma"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Robots in dynamic, human-centric environments must follow language instructions while maintaining real-time reactive control. Vision-language-action (VLA) models offer a promising framework, but they assume temporally aligned reasoning and control, despite semantic inference being inherently delayed relative to real-time action. We introduce Think-in-Control (TIC)-VLA, a latency-aware framework that explicitly models delayed semantic reasoning during action generation. TIC-VLA defines a delayed semantic-control interface that conditions action generation on delayed vision-language semantic states and explicit latency metadata, in addition to current observations, enabling policies to compensate for asynchronous reasoning. We further propose a latency-consistent training pipeline that injects reasoning inference delays during imitation learning and online reinforcement learning, aligning training with asynchronous deployment. To support realistic evaluation, we present DynaNav, a physics-accurate, photo-realistic simulation suite for language-guided navigation in dynamic environments. Extensive experiments in simulation and on a real robot show that TIC-VLA consistently outperforms prior VLA models while maintaining robust real-time control under multi-second reasoning latency. Project website: https://ucla-mobility.github.io/TIC-VLA/"
        },
        {
            "title": "Start",
            "content": "TIC-VLA: Think-in-Control Vision-Language-Action Model for Robot Navigation in Dynamic Environments Zhiyu Huang * 1 Yun Zhang * 1 Johnson Liu 1 Rui Song 1 Chen Tang 1 Jiaqi Ma 1 Abstract Robots in dynamic, human-centric environments must follow language instructions while maintaining real-time reactive control. Vision-languageaction (VLA) models offer promising framework, but they assume temporally aligned reasoning and control, despite semantic inference being inherently delayed relative to real-time action. We introduce Think-in-Control (TIC)-VLA, latency-aware framework that explicitly models delayed semantic reasoning during action generation. TIC-VLA defines delayed semanticcontrol interface that conditions action generation on delayed vision-language semantic states and explicit latency metadata, in addition to current observations, enabling policies to compensate for asynchronous reasoning. We further propose latency-consistent training pipeline that injects reasoning inference delays during imitation learning and online reinforcement learning, aligning training with asynchronous deployment. To support realistic evaluation, we present DynaNav, physics-accurate, photo-realistic simulation suite for language-guided navigation in dynamic environments. Extensive experiments in simulation and on real robot show that TICVLA consistently outperforms prior VLA models while maintaining robust real-time control under multi-second reasoning latency. Project website: https://ucla-mobility.github.io/TIC-VLA/ 6 2 0 2 2 ] . [ 1 9 5 4 2 0 . 2 0 6 2 : r 1. Introduction Robots operating in real-world, human-centric environments must react to dynamic scenes while following high-level natural language instructions (Chen et al., 2025b). Visionlanguage-action (VLA) models (Hirose et al., 2025; Xu et al., 2024; Driess et al., 2025) have emerged as promis- *Equal contribution 1University of California, Los Angeles. Correspondence to: Zhiyu Huang <zhiyuh@ucla.edu>, Yun Zhang <yun666@ucla.edu>, Jiaqi Ma <jiaqima@ucla.edu>. Preprint. February 3, 2026. 1 Figure 1. TIC-VLA enables real-time, language-conditioned navigation by decoupling slow vision-language reasoning from fast reactive control via delayed semantic-control interface. latencyconsistent training strategy improves robustness under variable reasoning delays. Performance is demonstrated in the DynaNav simulation and real-world indoor and outdoor navigation tasks. ing paradigm by unifying perception, language understanding, and control within single learning-based system. By incorporating large vision-language models (VLMs), these approaches enable semantic grounding, task reasoning, and instruction following (Zhou et al., 2025; Choi et al., 2024; Zhao et al., 2025; Lin et al., 2025; Chandaka et al., 2025; Castro et al., 2025). However, existing VLA systems rely on hidden and impractical assumption: reasoning and control are temporally aligned. Vision-language reasoning is produced intermittently and often with substantial delay, while control must operate continuously as the robot moves and the environment changes. As result, semantic representations frequently correspond to past world states, yet are consumed by the policy as if they were current, introducing systematic misalignment between thinking and control. This temporal mismatch is particularly pronounced on mobile robots with limited computation, where VLM inference can take seconds while control loops run at tens of hertz. Think-in-Control Vision-Language-Action Model for Robot Navigation in Dynamic Environments Most prior work on embodied navigation sidesteps this issue. Classical vision-language navigation (VLN) abstracts navigation into discrete viewpoint transitions, ignoring embodiment, dynamics, and timing (Krantz et al., 2020; Yu et al., 2025; Raychaudhuri & Chang, 2025; Zeng et al., 2025; Eftekhar et al., 2024; Hu et al., 2025a). Recent VLA systems rely on powerful GPUs and often pause execution during reasoning inference (Zhang et al., 2024b; Cheng et al., 2024), which is impractical in dynamic environments. Dual-system VLA architectures decouple reasoning and control (Wei et al., 2025a; InternRobotics, 2025), but still assume that semantic outputs are temporally fresh, implicitly treating inference latency as negligible. We argue that latency in reasoning is not merely an engineering inefficiency but fundamental modeling problem: delayed semantic information is neither represented nor accounted for during policy learning. Consequently, policies trained under idealized synchronous supervision can degrade severely when deployed under realistic inference latency. To this end, we introduce Think-in-Control VLA, latencyaware framework that explicitly exposes inference delay to the control policy. Rather than enforcing real-time constraints on semantic reasoning, TIC-VLA defines delayed semantic-control interface that allows reasoning to proceed asynchronously while enabling robust real-time control. Specifically, the reasoning module produces delayed latent semantic representations together with explicit latency and ego-motion offset metadata, while the action policy conditions on this delayed semantic-control interface. Importantly, architectural decoupling alone is insufficient. Policies trained under idealized synchronous supervision fail when deployed with delayed semantic inputs. We therefore propose latency-consistent training pipeline in which inference delays are explicitly injected during imitation learning and reinforcement learning. This enables the policy to learn to be robust to delayed semantic information, rather than overfitting to unrealistic training conditions. To support realistic and reproducible evaluation, we develop DynaNav, simulation suite featuring realistic rendering, dynamic human participants, physics-based execution, and diverse indoor and outdoor scenarios. DynaNav supports teleoperated data collection, online RL, and benchmarking. Experiments in both simulation and real-world deployment demonstrate that our proposed TIC-VLA model achieves improved and robust navigation under inference latency. Figure 1 illustrates the key design of TIC-VLA and its performance in simulation and real-world environments. The primary contributions can be summarized as: 1. We introduce TIC-VLA with delayed semanticcontrol interface that enables integration of temporally misaligned semantic features with real-time control. 2. We propose latency-consistent training pipeline that aligns learning with asynchronous inference at deployment, yielding robust navigation under variable delays. 3. We present DynaNav, realistic simulation suite and benchmark for language-guided navigation in dynamic environments, and we demonstrate TIC-VLAs strong performance in simulation and real-world. 2. Related Work Learning-based Visual Navigation. Recent advances in robot navigation have shifted from traditional map-based pipelines toward end-to-end learning-based models. Diffusion policies (Cai et al., 2025; Hu et al., 2025b), imitation and reinforcement learning methods (Liu et al., 2025b; Wu et al., 2023; He et al., 2025), and world modeling approaches (Liu et al., 2025a; Bar et al., 2025) have demonstrated strong performance in navigating complex environments without relying on maps. The integration of LLMs and VLMs into navigation tasks (Yuan et al., 2025; Xu et al., 2024; Zhang et al., 2025b; Gao et al., 2025; Zhou et al., 2024) has further expanded robots semantic understanding and open-vocabulary reasoning capabilities, allowing them to follow flexible natural language commands. However, in most of these works, VLMs are employed as auxiliary modules rather than being fully integrated into the navigation pipeline. This limitation has motivated the development of VLA models, which unify perception, instruction, reasoning, and planning within single framework. VLA for Navigation. Recent studies increasingly employ VLA models for robotic navigation. Representative methods span direct action prediction and intermediate planning: NaVid (Zhang et al., 2024b) predicts next-step action from monocular RGB inputs, while NaVILA (Cheng et al., 2024) generates mid-level linguistic actions executed by visual locomotion policy. TrackVLA (Wang et al., 2025b) combines language-based recognition with diffusion-based trajectory planning. Several works further explore generalist VLA frameworks: OmniVLA (Hirose et al., 2025) supports multiple forms of instruction conditioning, including egocentric poses, images, and natural language, while NavFoM (Zhang et al., 2025a) demonstrates strong cross-embodiment performance. More recent systems address temporal reasoning and real-time execution, such as StreamVLN (Wei et al., 2025b), MobileVLA (Huang et al., 2025), and dual-system VLA approaches like DualVLN (Wei et al., 2025a), which balance deliberative reasoning and reactive control via asynchronous inference. Despite these advances, most existing VLA-based navigation models implicitly assume negligible inference latency, often relying on powerful GPUs or blocking execution during reasoning inference. In contrast, TIC-VLA is designed for latency-aware execution and robust on-device deployment. Navigation in Dynamic Environments. Another line of work focuses on navigation in social and dynamic settings. 2 Think-in-Control Vision-Language-Action Model for Robot Navigation in Dynamic Environments Figure 2. Overview of TIC-VLA. The architecture adopts decoupled dual-system design with fast action expert and slow reasoning VLM. shared vision encoder provides real-time observations to the policy and time-lagged observations to the VLM, where the delay arises naturally from slow inference. The delayed semantic-control interface (including delayed VLM KV cache features and latency metadata) is explicitly recorded. The Transformer-based action expert takes as input the current observation, robot state, and delayed semantic-control interface data to generate actions from learnable action queries via cross-attention. Multi-stage training combines imitation learning with delayed inference and reinforcement learning to ensure robustness to real-world, time-sensitive conditions. Social-LLaVA (Payandeh et al., 2024) fine-tunes VLMs for social navigation, while Narrate2Nav (Payandeh et al., 2025) incorporates implicit language reasoning, social cues, and human intent into visual representations. Vi-LAD (Elnoor et al., 2025) distills socially compliant navigation knowledge from large VLMs into lightweight policies for realtime deployment. However, these methods typically rely on specialist policies at inference time and do not keep an inthe-loop VLM during execution. Evaluation is further limited by existing simulators. Arena (Shcherbyna et al., 2025) and UrbanSim (Wu et al., 2025) support dynamic agents but lack language-conditioned navigation. SocialNav-SUB (Munje et al., 2025) evaluates real-world social navigation without controllable simulator, while HA-VLN (Dong et al., 2025) relies on simplified observations and abstracted control. In contrast, DynaNav provides physics-accurate simulation suite for language-guided navigation in humancentric environments. 3. Method 3.1. Problem Formulation We consider an instruction-following navigation problem in which an embodied agent must follow natural language instructions to navigate dynamic environments. At each control timestep t, the agent receives: (1) natural language instruction and context I, specifying the navigation goal and historical trajecoty; (2) an egocentric observation history Ot = {x0, . . . , xt}, consisting of RGB frames xt RHW 3; and (3) the robot state st R3, encoding ego-motion information such as linear velocity and angular velocity. Conditioned on these inputs, the agent must output an action at at each timestep to safely and efficiently progress toward the goal. The environment evolves in response to the agents actions, and the episode continues until the agent reaches the goal or terminates. We consider settings where large-scale VLM is employed to interpret the natural-language instructions and provide semantic guidance. While such models are powerful, their reasoning often introduces non-negligible inference latency t. As result, semantic outputs may become temporally misaligned with the agents current observations and state, creating key challenge for real-time navigation. 3.2. Think-in-Control VLA An overview of the TIC-VLA framework is shown in Figure 2. TIC-VLA adopts dual-system execution paradigm in which high-level semantic reasoning and low-level control operate asynchronously. Crucially, rather than treating this as an architectural contribution, we explicitly model the resulting inference delay as part of the control problem. The key design principle is to expose reasoning latency to the action policy and train the policy to act under delayed semantic observations. We employ InternVL3-1B (Zhu et al., 2025) as the vision-language backbone for semantic and instruction understanding. At high level, VLM performs 3 Think-in-Control Vision-Language-Action Model for Robot Navigation in Dynamic Environments semantic reasoning over delayed visual context and language instructions, while reactive action policy executes at high control frequency and never blocks on VLM inference. The action policy conditions on cached semantic representations together with explicit latency and ego-motion metadata, allowing it to interpret the delayed semantic information in the correct temporal context. This latency-aware semantic-control coupling enables robust navigation despite asynchronous and delayed reasoning updates. VLM Semantic Reasoning. We formalize inference latency as core variable in the system. We define the effective reasoning latency as = tinfer + telapse 0, which accounts for both the VLM inference time (tinfer) and the elapsed time since the last completed reasoning update (telapse). At the current timestep t, the VLM operates on visual observations anchored at time t, rather than the current frame. Given set of historical frames vlm tt = {xttδ δ {0, 3, 6, 9}}, the VLM performs semantic reasoning conditioned on the instruction I. The resulting output, denoted Rtt, encodes high-level scene understanding, critical object identification, intent prediction, and future target waypoints derived from delayed observations. The reasoning results, including predicted waypoints, are generated relative to the time of inference start t, rather than the current control timestep. This explicit temporal anchoring allows the downstream policy to know when the reasoning was produced, rather than treating them as instantaneous or noisy signals. Latency-Aware Action Policy. The action policy πθ runs at high frequency and is responsible for real-time planning. At each timestep t, it conditions on four categories of inputs: (1) the current visual observation xt; (2) the current robot state st; (3) the most recent semantic hidden state Stt produced by the VLM (i.e., the last-layer key-value cache); (4) explicit latency metadata, including the effective latency and the corresponding motion offsets pt = (x, y, θ) accumulated since reasoning generation. Providing both delayed semantic states and latency metadata establishes delayed semantic-control interface: semantic features describe past state of the world, while the control policy is responsible for reinterpreting them in the current frame. This allows the policy to reason consistently about delayed semantics as the robot moves during inference. The policy outputs short horizon of future actions: } = πθ , . . . , aT (cid:0)Stt, xt, st, t, pt at = {a1 (1) R3 represents continuous action. The where each ai action chunks are integrated into short-horizon trajectory, and target point is chosen for execution. (cid:1), As shown in Figure 3(a), the action policy utilizes dedicated action query token that attends to the scene context through stack of cross-attention Transformer layers. Visual tokens and VLM cache features are first projected into 4 shared latent space via MLP layers, while the robot state and latency metadata are encoded and added with positional embeddings. These inputs are concatenated as key-value tokens, and the updated action query representation is passed through an MLP to generate the action outputs. Asynchronous Semantic Reasoning and Control. TICVLA operates in closed-loop asynchronous manner. The VLM periodically updates semantic reasoning based on delayed visual inputs, while the action policy continuously executes without waiting for inference to complete. The cached VLM hidden state is updated as follows: (Scache , Rcache ) = (cid:40) (St, Rt), if inference finishes at t, (S cache ), otherwise, , Rcache (2) where denotes the most recent timestep prior to at which inference was completed. At every control timestep, the action policy conditions on the most recent cached VLM hidden state Stt = cache . In addition, by conditioning action generation on latency metadata and ego-motion, TICVLA could reinterpret stale semantic information in the current state. An illustration of the asynchronous inference process is shown in Figure 3(d), and latency is measured as the sum of two parts: VLM reasoning inference time and the elapsed time since the last finished inference. 3.3. Latency-Consistent Training Pipeline We adopt three-stage training pipeline designed to enforce latency consistency between training and deployment. An overview of the pipeline is provided in Figure 3(c). Supervised Fine-tuning of the VLM. We first fine-tune the VLM on structured semantic reasoning data collected from both simulation and real-world environments. Training samples are automatically annotated using GPT-5. Given past and future image sequences, the robots positional context, and the corresponding trajectory, GPT-5 produces: (1) long-horizon instruction describing the navigation goal, and (2) concise, structured reasoning trace capturing critical objects identification, intention prediction, and resulting action. Details are provided in the supplementary material. During this stage, the vision encoder is kept frozen. The language model is trained to produce reasoning tokens and waypoint predictions conditioned on visual tokens and instructions. We optimize the standard autoregressive crossentropy loss Ll over the target token sequence. We mix waypoint-only and scene-reasoning-augmented targets during training for flexible prompting at inference time. Imitation Learning under Reasoning Latency. To compensate for the uncertain delay in semantic reasoning, we perturb originally aligned and synchronous demonstrations to synthesize training data with delayed semantic reasoning. Specifically, we sample reasoning delays uniformly Think-in-Control Vision-Language-Action Model for Robot Navigation in Dynamic Environments from [0, 10] seconds and condition the policy on: (1) the current image input and robot state, (2) KV cache features from the delayed VLM reasoning output, (3) explicit latency metadata. This exposes the policy to range of temporally misaligned semantic representations during training. The action policy πθ is trained via imitation learning using human demonstration trajectories. As low-level control actions are not available, we integrate the predicted actions forward to obtain positions (x, y, θ) over the prediction horizon and compare them against ground-truth trajectories using smooth L1 loss: La ="
        },
        {
            "title": "1\nT",
            "content": "T (cid:88) i=1 SmoothL1(cid:0)ˆp(i) p(i) (cid:1), (3) where ˆp(i) is the ground-truth pose. is the predicted pose at sub-timestep i, and p(i) Reinforcement Learning with Asynchronous Guidance. While imitation learning with synthesized reasoning delays provides strong initialization that accounts for inference latency, the resulting training data distribution remains mismatched to the closed-loop distribution induced by coupled, asynchronous reasoning-action interactions. Motivated by prior work (Lu et al., 2025; Li et al., 2025; Chen et al., 2025a), we fine-tune only the action policy using RL while keeping the vision encoder and language model frozen. This allows the policy to learn to interpret delayed VLM guidance, handle dynamic agents, and mitigate variability introduced by asynchronous inference. We construct simulation environment with dynamic human participants and train the policy using Proximal Policy Optimization (PPO) (Schulman et al., 2017). The value network, shown in Figure 3(b), takes as input the current image tokens, the goal position, and the robot state, and outputs the estimated state value. The policy outputs Gaussian action distribution, where the action derived from the predicted trajectory is the mean, and the standard deviation is learned during training. The reward function is defined as: rt = wgrgoal + wprprogress + wcrcollision + wsrspeed , (4) rewards reaching the target, rprogress where rgoal encourages progress toward the goal, rcollision penalizes collisions with humans or static obstacles, and rspeed penalizes both excessively slow motion and overly fast speed. wg, wp, wc, and ws are weights for these terms. t To further improve robustness under asynchronous deployment, we inject stochastic inference delays following each VLM update to mimic the latency characteristics observed on edge hardware. This enforces consistency between training and execution conditions. Additional implementation details are provided in the supplementary material. 5 Figure 3. Details of TIC-VLA action policy structure, training, and asynchronous execution. (a) Latency-aware action policy that predicts action chunks from multimodal inputs. (b) Value network used during online reinforcement learning. (c) Three-stage latencyconsistent training pipeline combining VLM supervision, imitation learning, and reinforcement learning. (d) Asynchronous inference and control with explicit latency modeling. 3.4. DynaNav Evaluating our proposed latency-aware VLA framework requires more realistic navigation benchmarks than classic VLN benchmarks such as R2R (Li et al., 2019), VLN-CE (Krantz et al., 2020), and RxR (Ku et al., 2020), which operate in small indoor environments and abstract navigation as viewpoint transitions without physical interaction. VLNPE (Wang et al., 2025a) and GRUtopia (Wang et al., 2024) support embodied evaluation but do not model navigation among dynamic human participants, while SocialHM3D (Gong et al., 2025) and HA-VLN (Dong et al., 2025) incorporate human agents with limited visual realism. To fill this gap, DynaNav provides realistic benchmark integrating language-guided navigation, large-scale scenes, diverse human agents, and physics-based robot control. Think-in-Control Vision-Language-Action Model for Robot Navigation in Dynamic Environments Simulation Environments. We construct simulation environments in Isaac Sim (Isaac-sim development team, 2025), with realistic, controllable dynamic interactions. Four representative scenes (i.e., warehouse, hospital, office, and outdoor sidewalk) are designed to capture broad range of navigation contexts. Human behaviors are modeled using Isaac Sims built-in human simulation tools, supporting behaviorally plausible pedestrian movement. Our simulation setup supports both wheeled (Nova Carter) and quadruped (Boston Dynamics Spot) robots. We develop custom robot behavior scripts that allow two modes of operation: (1) Human teleoperation mode, which enables manual control for collecting expert demonstrations, and (2) End-to-end model control mode, which allows direct control of the robot with end-to-end planning models. Designed for Scalable RL Training. We employ Isaac Lab (Mittal et al., 2025) and leverage its GPU-accelerated simulation to build environments for scalable reinforcement learning training. We implement custom human behavior control script to generate human movements within the environment. Human-robot and robot-scene interactions are fully physics-based with realistic contact dynamics. This setup allows us to run number of parallel environments, enabling scalable and efficient RL training. Diverse Benchmark Tasks. We develop comprehensive benchmark of 85 test cases to evaluate navigation performance across diverse conditions. Tasks vary along three dimensions: (1) Crowd Density, ranging from empty spaces to densely populated settings, capturing different levels of (2) Navigation Distance, adjusted dynamic complexity. from short-term navigation to long-horizon planning, reflecting increasing navigation difficulty. (3) Scene Type, with evaluation conducted across four distinct environments to assess robustness to varying spatial layouts and human behaviors. For each task, standardized initial and goal positions are specified with language instruction. Additional details are provided in the supplementary material. 4. Experiments 4.1. Experimental Setup Datasets. We train the model using three datasets featuring dynamic human-robot interactions: (1) SCAND (Karnan et al., 2022), which contains 8.7 hours of robot-driven trajectories across diverse social environments; (2) GND (Liang et al., 2025), which comprises over 11 hours of recorded data collected in various campus environments; (3) DynaNav simulation dataset, collected using our designed dynamic simulation environments, containing 5.1 hours of robot navigation data across multiple scene types. Training is performed using Distributed Data Parallel on eight NVIDIA L40S GPUs, with batch size of 2 per GPU. AdamW is used as the optimizer with cosine learning rate schedule, initialized at 2 105. For training the action expert, we increase the batch size to 16 per GPU and set the initial learning rate to 2 104. RL fine-tuning of the action policy is conducted on single NVIDIA L40S GPU for 400 iterations across three tasks in three environments. Additional details, including hyperparameters and data preprocessing, are provided in the supplementary material. Baselines. We evaluate TIC-VLA against two categories methods. Point-goal navigation policies are included as reference baselines to contextualize task difficulty: (1) vanilla Behavior Cloning (BC) policy that maps RGB observations and point-goal commands directly to actions; (2) vanilla RL policy trained on RGB observations and point-goal commands; (3) NavDP (Cai et al., 2025), pointgoal-conditioned diffusion-based navigation policy. The primary language-guided navigation baselines are listed below: (1) NaVILA (Cheng et al., 2024), hierarchical VLA model that translates language instructions into mid-level commands; (2) Uni-NaVid (Zhang et al., 2024a), unified video-based VLA model trained across multiple navigation tasks; and (3) DualVLN (Wei et al., 2025a), dual-system VLA model. These baselines are fine-tuned on the same datasets to ensure fair and controlled comparison. Evaluation metrics. We adopt the following evaluation metrics: (1) Navigation Error (NE): the final distance between the agent and the goal; (2) Success Rate (SR): the percentage of episodes in which the agent stops within 1 meter of the goal; (3) Success weighted by Path Length (SPL): SR weighted by the ratio between the shortest path length and the actual path length, penalizing inefficient trajectories; (4) Collision Rate (CR): the percentage of episodes in which the agent collides with static obstacles or humans, quantifying the safety of navigation behavior. Real-world experimental setup. To evaluate real-time performance on edge devices, we deploy the model on two platforms with different power budgets: an NVIDIA Jetson Orin NX (25W) and an RTX 4060 Laptop GPU (50W), representing typical edge computing capabilities. An RTX A6000 GPU is used only when the baselines cannot run on these devices. The navigation policy is executed on Unitree Go2 quadruped robot in real-world navigation tasks. We employ FlashAttention to improve inference efficiency. Performance is measured by the average success rate. An episode is considered failure if manual intervention is required to prevent collisions. 4.2. Simulation Testing Implementation Details. For VLM SFT, we employ fullparameter training due to the compact size of TIC-VLA. Performance on the DynaNav benchmark. All experiments are conducted on an NVIDIA L40S GPU, with the 6 Think-in-Control Vision-Language-Action Model for Robot Navigation in Dynamic Environments Figure 4. Qualitative results of TIC-VLA closed-loop performance in DynaNav hospital (top) and office (bottom) environments. action policy running at 10 Hz and asynchronous VLM reasoning at 0.5 Hz. Results are summarized in Table 1. Despite relying solely on egocentric observations and language instructions, without privileged goal or map information, TIC-VLA achieves strong performance. Notably, point-goal methods bypass vision-language inference and therefore incur no reasoning latency. TIC-VLA (no RL) performs comparably to NavDP, point-goal method with privileged state access, and outperforms BC and RL baselines trained without language. With additional RL fine-tuning, TIC-VLA achieves the highest success rate (SR) and the lowest collision rate (CR). TIC-VLA also substantially outperforms prior VLA/VLN methods (Uni-NaVid, NaVILA, DualVLN), which are designed for abstract navigation with discrete actions, highlighting their limitations in physically realistic, dynamic environments. Blocking control with synchronous VLM inference leads to marked performance drop even under modest latency, underscoring the importance of TIC-VLAs asynchronous design. Figure 4 provides qualitative examples of accurate reasoning and interactive navigation in dynamic environments. Table 1. Performance of TIC-VLA and baseline methods on the DynaNav benchmark. BC, RL, and NavDP are goal point-based. Method BC Policy RL Policy NavDP Uni-NaVid NaVILA DualVLN TIC-VLA (Sync.) TIC-VLA (no RL) TIC-VLA NE () SR () SPL () CR () 9.96 12.20 8.61 15.90 17.20 16.45 16.31 10.85 10.55 45.88 30.59 54.12 22.35 28.24 30.59 32.94 47.06 55.29 41.52 28.45 52.62 19.61 25.51 27.82 29.64 42.41 50. 35.29 36.47 30.59 49.41 48.24 47.06 41.18 34.12 28.24 Latency Robustness Analysis. Figure 5 reports performance under increasing VLM reasoning inference latency for TIC-VLA in simulation, before and after RL fine-tuning. As latency increases, the IL-based action expert exhibits Figure 5. The effect of VLM asynchronous reasoning inference latency in TIC-VLA on task performance. Table 2. Influence of semantic interface and latency training. Interface Latency NE () SR () SPL () CR () Waypoint Waypoint KV Cache KV Cache 21.17 20.32 16.74 10. 16.47 22.35 30.59 47.06 15.89 18.34 28.31 42.41 47.06 42.35 40.00 34.12 noticeable decline in success rate, indicating higher sensitivity to delayed reasoning updates. In contrast, the RLfine-tuned policy maintains consistently higher success rates across all latency settings, demonstrating improved robustness to inference latency. This result highlights the effectiveness of RL fine-tuning in mitigating latency-induced performance degradation. Collision rates are insensitive to inference latency, suggesting that the asynchronous policy preserves reactivity independent of reasoning speed. Influence of Semantic-Control Interface and Latency Awareness. We evaluate the impact of different delayed semantic-control interfaces and latency-awareness on TICVLA. Specifically, we compare interface variants that use waypoint-based guidance and KV-cache-based features, each trained with and without explicit latency-aware modeling and training. As shown in Table 2, using KV-cache features significantly improves navigation success, and latencyThink-in-Control Vision-Language-Action Model for Robot Navigation in Dynamic Environments Figure 6. Real-world evaluation of TIC-VLA. (a) Hardware configuration, including the robot platform and computation setup. (b) Designed indoor and outdoor vision-language navigation tasks. (c) Qualitative results from an indoor hallway navigation task, showing the robot following natural language instructions while avoiding obstacles and humans and reaching the goal. awareness enhances performance under asynchronous inference. The waypoint-based interface leads to inferior performance due to its sparsity and potential inconsistency with the agents local observations. Combining both the KV cache feature interface and latency-aware modeling and training achieves the best overall performance. 4.3. Real-world Testing We design four real-world navigation tasks: (1) an indoor hallway with dynamic human traffic and static obstacles, (2) an office environment with cluttered layouts, (3) an outdoor plaza environment, and (4) an outdoor walkway scenario with uneven terrain. Task descriptions and hardware configurations are illustrated in Figure 6. All evaluations are conducted in zero-shot setting, without additional training data. For each task, we perform five trials and report the average success rate. During deployment, real-time RGB images from the front-facing camera of Unitree Go2 robot are streamed to TIC-VLA for inference. Table 3. Real-world testing results. Runtimes for the dual system are reported as (x/x) for the action policy and VLM reasoning. Method Platform Success Rate () Runtime (ms) TIC-VLA (no RL) TIC-VLA TIC-VLA TIC-VLA Dual-VLN (7B) NaVILA (7B) 4060 4060 Orin NX A6000 A6000 A6000 0.70 0.85 0.75 0.80 0.50 0.35 85.73/3430.73 120.27/4831.73 32.70/1681.66 299.92/1534.67 4106. We compare TIC-VLA against DualVLN (Wei et al., 2025a) and NaVILA (Cheng et al., 2024), with results summarized in Table 3. TIC-VLA outperforms prior VLA baselines despite operating with significantly smaller models and lower compute budgets. RL fine-tuning further improves performance, particularly in scenarios with dynamic human interactions. TIC-VLA maintains high success rates when deployed on edge hardware (Jetson Orin NX) under multisecond reasoning latency, validating the effectiveness of explicit latency modeling for real-time control. Moreover, deployment on remote server yields only limited performance gains due to communication delays. 4.4. Ablation Study Influence of Reasoning at Test Time. We evaluate the effect of explicit VLM reasoning during inference by comparing the model (after RL fine-tuning) with and without reasoning-token outputs. As shown in Table 4, enabling reasoning improves all navigation metrics but incurs increased inference latency (0.5 Hz in simulation). Disabling reasoning significantly reduces VLM forward overhead (4 Hz in simulation), at the cost of significantly degraded performance. These results indicate that explicit reasoning at test time improves task success, while the proposed latencyaware design mitigates its associated overhead. Additional ablation results are provided in the supplementary material. Table 4. Effect of VLM reasoning at test time. Inference NE () SR () SPL () CR () W/o Reasoning W/ Reasoning 14.23 10.55 40.00 55. 34.22 50.29 25.88 28.24 5. Conclusions We introduce TIC-VLA, latency-aware VLA framework that explicitly addresses the temporal mismatch between slow semantic reasoning and real-time control. By introducing delayed semantic-control interface and training policies under realistic inference delays, TIC-VLA enables robust language-guided navigation under substantial latency. Results in simulation and real-world experiments demonstrate consistent improvements over prior VLA methods. Future work will extend this approach to robot manipulation tasks and better reasoning-action alignment. 8 Think-in-Control Vision-Language-Action Model for Robot Navigation in Dynamic Environments"
        },
        {
            "title": "Impact Statement",
            "content": "This work advances VLA models for real-time robot navigation in dynamic, human-centric environments by explicitly addressing inference latency and limited onboard computation. By enabling responsive and instruction-consistent behavior on edge hardware, the proposed approach may facilitate broader deployment of mobile robots in applications such as service robotics, logistics, and assisted mobility. However, autonomous navigation in shared human spaces raises safety and ethical considerations, including the risk of misinterpretation, collisions, or socially inappropriate behavior. While our design emphasizes reactive control, latency awareness, and evaluation in dynamic settings to mitigate such risks, the system remains subject to perception and modeling limitations, and human oversight is necessary in safety-critical deployments."
        },
        {
            "title": "References",
            "content": "Bai, S., Chen, K., Liu, X., Wang, J., Ge, W., Song, S., Dang, K., Wang, P., Wang, S., Tang, J., et al. Qwen2.5-vl technical report. arXiv preprint arXiv:2502.13923, 2025. Bar, A., Zhou, G., Tran, D., Darrell, T., and LeCun, Y. Navigation world models. In Proceedings of the Computer Vision and Pattern Recognition Conference, pp. 15791 15801, 2025. Cai, W., Peng, J., Yang, Y., Zhang, Y., Wei, M., Wang, H., Chen, Y., Wang, T., and Pang, J. Navdp: Learning sim-to-real navigation diffusion policy with privileged information guidance. arXiv preprint arXiv:2505.08712, 2025. Castro, M. G., Rajagopal, S., Gorbatov, D., Schmittle, M., Baijal, R., Zhang, O., Scalise, R., Talia, S., Romig, E., de Melo, C., et al. Vamos: hierarchical vision-languageaction model for capability-modulated and steerable navigation. arXiv preprint arXiv:2510.20818, 2025. Chandaka, B., Wang, G. X., Chen, H., Che, H., Zhai, A. J., and Wang, S. Human-like navigation in world built for humans. arXiv preprint arXiv:2509.21189, 2025. Chen, H., Zhao, M., Yang, R., Ma, Q., Yang, K., Yao, J., Wang, K., Bai, H., Wang, Z., Pan, R., et al. Era: Transforming vlms into embodied agents via embodied prior learning and online reinforcement learning. arXiv preprint arXiv:2510.12693, 2025a. Chen, Z., Guo, Y., Chu, Z., Luo, M., Shen, Y., Sun, M., Hu, J., Xie, S., Yang, K., Shi, P., et al. Socialnav: Training human-inspired foundation model for socially-aware embodied navigation. arXiv preprint arXiv:2511.21135, 2025b. Cheng, A.-C., Ji, Y., Yang, Z., Gongye, Z., Zou, X., Kautz, J., Bıyık, E., Yin, H., Liu, S., and Wang, X. Navila: Legged robot vision-language-action model for navigation. arXiv preprint arXiv:2412.04453, 2024. Choi, W., Kim, W. K., Yoo, M., and Woo, H. Embodied cot distillation from llm to off-the-shelf agents. In Proceedings of the 41st International Conference on Machine Learning, pp. 87028721, 2024. Dong, Y., Wu, F., He, Q., Li, H., Li, M., Cheng, Z., Zhou, Y., Sun, J., Dai, Q., Cheng, Z.-Q., et al. Ha-vln: benchmark for human-aware navigation in discrete-continuous environments with dynamic multi-human interactions, real-world validation, and an open leaderboard. arXiv preprint arXiv:2503.14229, 2025. Driess, D., Springenberg, J. T., Ichter, B., Yu, L., Li-Bell, A., Pertsch, K., Ren, A. Z., Walke, H., Vuong, Q., Shi, L. X., et al. Knowledge insulating vision-language-action models: Train fast, run fast, generalize better. arXiv preprint arXiv:2505.23705, 2025. Eftekhar, A., Hendrix, R., Weihs, L., Duan, J., Caglar, E., Salvador, J., Herrasti, A., Han, W., VanderBil, E., Kembhavi, A., et al. The one ring: robotic indoor navigation generalist. arXiv preprint arXiv:2412.14401, 2024. Elnoor, M., Weerakoon, K., Seneviratne, G., Liang, J., Rajagopal, V., and Manocha, D. Vi-lad: Vision-language attention distillation for socially-aware robot navigation in dynamic environments. arXiv preprint arXiv:2503.09820, 2025. Gao, C., Jin, L., Peng, X., Zhang, J., Deng, Y., Li, A., Wang, H., and Liu, S. Octonav: Towards generalist embodied navigation. arXiv preprint arXiv:2506.09839, 2025. Gong, Z., Hu, T., Qiu, R., and Liang, J. From cognition to precognition: future-aware framework for social navigation. In 2025 IEEE International Conference on Robotics and Automation (ICRA), pp. 91229129. IEEE, 2025. He, H., Ma, Y., Wu, W., and Zhou, B. From seeing to experiencing: Scaling navigation foundation models with reinforcement learning. arXiv preprint arXiv:2507.22028, 2025. Hirose, N., Glossop, C., Shah, D., and Levine, S. Omnivla: An omni-modal vision-language-action model for robot navigation. arXiv preprint arXiv:2509.19480, 2025. Hu, J., Chen, J., Bai, H., Luo, M., Xie, S., Chen, Z., Liu, F., Chu, Z., Xue, X., Ren, B., et al. Astranav-world: World model for foresight control and consistency. arXiv preprint arXiv:2512.21714, 2025a. 9 Think-in-Control Vision-Language-Action Model for Robot Navigation in Dynamic Environments Hu, Z., Tang, C., Munje, M. J., Zhu, Y., Liu, A., Liu, S., Warnell, G., Stone, P., and Biswas, J. Composablenav: Instruction-following navigation in dynamic environments via composable diffusion. arXiv preprint arXiv:2509.17941, 2025b. Huang, T., Li, D., Yang, R., Zhang, Z., Yang, Z., and Tang, H. Mobilevla-r1: Reinforcing vision-language-action for mobile robots. arXiv preprint arXiv:2511.17889, 2025. foundation model with InternRobotics. navigation tent InternRobotics/InternVLA-N1, 2025. cessed: 2025-10-10. Internvla-n1: An open dual-system lahttps://huggingface.co/ Aclearned plans. team. Isaac-sim development IsaacSim: An opensource robotics simulation platform on NVIDIA Omhttps://github.com/isaac-sim/ niverse. IsaacSim, 2025. Accessed: 2025-10-09; version v5.0.0. Karnan, H., Nair, A., Xiao, X., Warnell, G., Pirk, S., Toshev, A., Hart, J., Biswas, J., and Stone, P. Socially compliant navigation dataset (scand): large-scale dataset of demonstrations for social navigation. IEEE Robotics and Automation Letters, 7(4):1180711814, 2022. Krantz, J., Wijmans, E., Majundar, A., Batra, D., and Lee, S. Beyond the nav-graph: Vision and language navigation in continuous environments. In European Conference on Computer Vision (ECCV), 2020. Ku, A., Anderson, P., Patel, R., Ie, E., and Baldridge, J. Room-across-room: Multilingual vision-and-language navigation with dense spatiotemporal grounding. In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP), pp. 43924412, 2020. Li, H., Zuo, Y., Yu, J., Zhang, Y., Yang, Z., Zhang, K., Zhu, X., Zhang, Y., Chen, T., Cui, G., et al. Simplevla-rl: Scaling vla training via reinforcement learning. arXiv preprint arXiv:2509.09674, 2025. Li, X., Li, C., Xia, Q., Bisk, Y., Celikyilmaz, A., Gao, J., Smith, N., and Choi, Y. Robust navigation with language pretraining and stochastic sampling. 2019. Liang, J., Das, D., Song, D., Shuvo, M. N. H., Durrani, M., Taranath, K., Penskiy, I., Manocha, D., and Xiao, X. Gnd: Global navigation dataset with multi-modal perception and multi-category traversability in outdoor campus environments. In 2025 IEEE International Conference on Robotics and Automation (ICRA), pp. 23832390. IEEE, 2025. 10 Lin, F., Nai, R., Hu, Y., You, J., Zhao, J., and Gao, Y. Onetwovla: unified vision-language-action model with adaptive reasoning. arXiv preprint arXiv:2505.11917, 2025. Liu, W., Zhao, H., Li, C., Biswas, J., Okal, B., Goyal, P., Chang, Y., and Pouya, S. X-mobility: End-to-end generalizable navigation via world modeling. In 2025 IEEE International Conference on Robotics and Automation (ICRA), pp. 75697576. IEEE, 2025a. Liu, W., Zhao, H., Li, C., Biswas, J., Pouya, S., and Chang, Y. Compass: Cross-embodiment mobility policy via residual rl and skill synthesis. arXiv preprint arXiv:2502.16372, 2025b. Lu, G., Guo, W., Zhang, C., Zhou, Y., Jiang, H., Gao, Z., Tang, Y., and Wang, Z. Vla-rl: Towards masterful and general robotic manipulation with scalable reinforcement learning. arXiv preprint arXiv:2505.18719, 2025. Mittal, M., Roth, P., Tigue, J., Richard, A., Zhang, O., Du, P., Serrano-Munoz, A., Yao, X., Zurbrugg, R., Rudin, N., Isaac lab: gpu-accelerated simulation frameet al. work for multi-modal robot learning. arXiv preprint arXiv:2511.04831, 2025. Munje, M. J., Tang, C., Liu, S., Hu, Z., Zhu, Y., Cui, J., Warnell, G., Biswas, J., and Stone, P. Socialnav-sub: Benchmarking vlms for scene understanding in social robot navigation. arXiv preprint arXiv:2509.08757, 2025. Payandeh, A., Song, D., Nazeri, M., Liang, J., Mukherjee, P., Raj, A. H., Kong, Y., Manocha, D., and Xiao, X. Social-llava: Enhancing robot navigation through humanlanguage reasoning in social spaces. arXiv preprint arXiv:2501.09024, 2024. Payandeh, A., Pokhrel, A., Song, D., Zampieri, M., and Xiao, X. Narrate2nav: Real-time visual navigation with implicit language reasoning in human-centric environments. arXiv preprint arXiv:2506.14233, 2025. Raychaudhuri, S. and Chang, A. X. Semantic mapping in indoor embodied ai-a survey on advances, challenges, and future directions. Transactions on Machine Learning Research, 2025. Schulman, J., Wolski, F., Dhariwal, P., Radford, A., and Klimov, O. Proximal policy optimization algorithms. arXiv preprint arXiv:1707.06347, 2017. Shcherbyna, V., Kastner, L., Diaz, D., Nguyen, H. G., Schreff, M. H.-K., Seeger, T., Kreutz, J., Martban, A., Shen, Z., Zeng, H., et al. Arena 4.0: comprehensive ros2 development and benchmarking platform for human-centric navigation using generative-model-based Think-in-Control Vision-Language-Action Model for Robot Navigation in Dynamic Environments environment generation. In 2025 IEEE International Conference on Robotics and Automation (ICRA), pp. 9138 9144. IEEE, 2025. Shukor, M., Aubakirova, D., Capuano, F., Kooijmans, P., Palma, S., Zouitine, A., Aractingi, M., Pascal, C., Russi, M., Marafioti, A., et al. Smolvla: vision-languageaction model for affordable and efficient robotics. arXiv preprint arXiv:2506.01844, 2025. Wang, H., Chen, J., Huang, W., Ben, Q., Wang, T., Mi, B., Huang, T., Zhao, S., Chen, Y., Yang, S., et al. Grutopia: Dream general robots in city at scale. arXiv preprint arXiv:2407.10943, 2024. Wang, L., Xia, X., Zhao, H., Wang, H., Wang, T., Chen, Y., Liu, C., Chen, Q., and Pang, J. Rethinking the embodied gap in vision-and-language navigation: holistic study of physical and visual disparities. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pp. 94559465, 2025a. Wang, S., Zhang, J., Li, M., Liu, J., Li, A., Wu, K., Zhong, F., Yu, J., Zhang, Z., and Wang, H. Trackvla: Embodied visual tracking in the wild. arXiv preprint arXiv:2505.23189, 2025b. Wei, M., Wan, C., Peng, J., Yu, X., Yang, Y., Feng, D., Cai, W., Zhu, C., Wang, T., Pang, J., et al. Ground slow, move fast: dual-system foundation model for generalizable vision-and-language navigation. arXiv preprint arXiv:2512.08186, 2025a. Wei, M., Wan, C., Yu, X., Wang, T., Yang, Y., Mao, X., Zhu, C., Cai, W., Wang, H., Chen, Y., et al. Streamvln: Streaming vision-and-language navigation via slowfast context modeling. arXiv preprint arXiv:2507.05240, 2025b. Wu, J., Zhou, Y., Yang, H., Huang, Z., and Lv, C. Humanguided reinforcement learning with sim-to-real transfer for autonomous navigation. IEEE Transactions on Pattern Analysis and Machine Intelligence, 45(12):1474514759, 2023. Wu, W., He, H., Zhang, C., He, J., Zhao, S. Z., Gong, R., Li, Q., and Zhou, B. Towards autonomous micromobility through scalable urban simulation. In Proceedings of the Computer Vision and Pattern Recognition Conference, pp. 2755327563, 2025. Yu, Z., Long, Y., Yang, Z., Zeng, C., Fan, H., Zhang, J., and Dong, H. Correctnav: Self-correction flywheel empowers vision-language-action navigation model. arXiv preprint arXiv:2508.10416, 2025. Yuan, M., Wang, L., and Waslander, S. L. Opennav: Openworld navigation with multimodal large language models. arXiv preprint arXiv:2507.18033, 2025. Zeng, S., Qi, D., Chang, X., Xiong, F., Xie, S., Wu, Janusvln: DeX., Liang, S., Xu, M., and Wei, X. coupling semantics and spatiality with dual implicit memory for vision-language navigation. arXiv preprint arXiv:2509.22548, 2025. Zhang, J., Wang, K., Wang, S., Li, M., Liu, H., Wei, S., Wang, Z., Zhang, Z., and Wang, H. Uni-navid: videobased vision-language-action model for unifying embodied navigation tasks. arXiv preprint arXiv:2412.06224, 2024a. Zhang, J., Wang, K., Xu, R., Zhou, G., Hong, Y., Fang, X., Wu, Q., Zhang, Z., and Wang, H. Navid: Videobased vlm plans the next step for vision-and-language navigation. arXiv preprint arXiv:2402.15852, 2024b. Zhang, J., Li, A., Qi, Y., Li, M., Liu, J., Wang, S., Liu, H., Zhou, G., Wu, Y., Li, X., et al. Embodied navigation foundation model. arXiv preprint arXiv:2509.12129, 2025a. Zhang, S., Qiao, Y., Wang, Q., Guo, L., Wei, Z., and Liu, J. Flexvln: Flexible adaptation for diverse vision-and-language navigation tasks. arXiv preprint arXiv:2503.13966, 2025b. Zhao, Q., Lu, Y., Kim, M. J., Fu, Z., Zhang, Z., Wu, Y., Li, Z., Ma, Q., Han, S., Finn, C., et al. Cot-vla: Visual chain-of-thought reasoning for vision-language-action In Proceedings of the Computer Vision and models. Pattern Recognition Conference, pp. 17021713, 2025. Zhou, G., Hong, Y., and Wu, Q. Navgpt: Explicit reasoning in vision-and-language navigation with large language models. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 38, pp. 76417649, 2024. Zhou, Z., Cai, T., Zhao, S. Z., Zhang, Y., Huang, Z., Zhou, B., and Ma, J. Autovla: vision-language-action model for end-to-end autonomous driving with adaptive reasoning and reinforcement fine-tuning. arXiv preprint arXiv:2506.13757, 2025. Xu, Z., Chiang, H.-T. L., Fu, Z., Jacob, M. G., Zhang, T., Lee, T.-W. E., Yu, W., Schenck, C., Rendleman, D., Shah, D., et al. Mobility vla: Multimodal instruction navigation with long-context vlms and topological graphs. In 8th Annual Conference on Robot Learning, 2024. Zhu, J., Wang, W., Chen, Z., Liu, Z., Ye, S., Gu, L., Tian, H., Duan, Y., Su, W., Shao, J., et al. Internvl3: Exploring advanced training and test-time recipes for open-source multimodal models. arXiv preprint arXiv:2504.10479, 2025. 11 Think-in-Control Vision-Language-Action Model for Robot Navigation in Dynamic Environments"
        },
        {
            "title": "Supplementary Materials",
            "content": "A. Model Details VLM Backbone. Our model is built upon an InternVL3-1B vision-language model, which consists of an InternViT-300M vision encoder and Qwen2.5-0.5B language model. The VLM processes sequence of historical images spanning up to nine-second temporal window to perform reasoning and generate high-level planning waypoints. Images are sampled at three-second intervals, resulting in three historical frames and one current frame. Each image is captured at resolution of 1920 1080, and these images are encoded into visual tokens of size 4 256, which together provide both temporal and historical context for reasoning. Due to reasoning inference latency, the visual observations fed into the VLM are temporally lagged relative to the real-time sensory inputs received by the action expert. To ensure consistent visual representations across modules, the vision encoder is shared between the VLM and the action expert. Specifically, the shared encoder processes the current image observation and outputs visual tokens that are directly consumed by the action expert for real-time control. Action Expert. The action expert is implemented as Transformer-based architecture composed of six cross-attention layers. Its inputs include: (1) current visual tokens (256 tokens) extracted from the shared vision encoder, (2) the robots proprioceptive state, (3) delayed semantic guidance from the VLM, and (4) latency-related metadata. The VLM guidance is provided either in the form of abstract waypoint predictions or as key-value (KV) cache features from the final Transformer layer of the VLM. The robot state is represented by vector consisting of the linear velocities (vx, vy) and the yaw rate (ωz). To explicitly account for inference latency, we incorporate latency-related metadata, including the estimated robot displacement during the inference delay (x, y, θ), and the corresponding time delay t. Visual tokens are projected from 896 dimensions to 512 dimensions, VLM KV cache features are projected from 256 to 512 dimensions, and low-dimensional inputs (waypoint guidance and latency metadata) are projected to 512 dimensions, followed by positional embeddings. We apply token dropout to the KV-cache tokens (drop rate 0.1) to encourage robustness to missing semantic context, and apply dropout to the encoded robot-state tokens/positional embeddings (drop rate 0.5) to reduce overfitting to state inputs. All processed features are treated as scene context and serve as keys and values in the cross-attention Transformer. The queries are learnable embeddings indexed by time steps, enabling the model to produce structured predictions. Transformer outputs are passed through an MLP to produce action chunks at each predicted step. We predict actions over future horizon of three seconds, discretized into = 30 action chunks. Each action is parameterized as (dx, dy, dθ), representing relative motion increments. simple integrator (cumulative sum) is applied to accumulate these actions into continuous trajectory, yielding waypoints of the form (x, y, θ). Value Network. The value network shares the same vision encoder as the action expert and operates on the current image observation. The resulting 256 visual tokens are first processed by three 1D convolutional layers followed by an average pooling layer, reducing them to compact feature representation. In parallel, the robot state (vx, vy) and yaw rate (ωz), as well as the relative goal position, are concatenated and projected into the same feature dimension using MLPs. These feature representations are concatenated and passed through final MLP to predict the scalar value estimate. The hyperparameters used in model components are summarized in Table 5. B. Training Details Data Processing. We start from long-horizon raw data collected continuously within single scenario or location. Each raw sequence is segmented into set of fixed-length episodes, each spanning 20 seconds. For each episode, we retain only the egocentric RGB images and the corresponding robot pose trajectories, discarding other sensor modalities. All data are temporally downsampled to 10 Hz to ensure consistent representation across datasets and training stages. For each episode, we construct structured representation consisting of history window, current frame, and future trajectory. The history contains past frames with relative pose offsets, while the future trajectory is represented as sequence of relative motion offsets with respect to the current pose. These offsets are expressed in the robot-centric coordinate frame. Episodes are further subsampled at fixed temporal stride to reduce redundancy, resulting in one annotated sample approximately every three seconds. This preprocessing yields compact yet diverse set of training samples suitable for both 12 Think-in-Control Vision-Language-Action Model for Robot Navigation in Dynamic Environments Table 5. Model Parameters Parameter Value Token dim. Proj. dim. Drouput rate-KV cache Drouput rate-state Cross-attn layers Query dim. Queries (T ) FFN dim. Horizon (s) Conv1D layers 896 512 0.1 0.5 6 512 30 2048 3 3 Table 6. Training Parameters Parameter Value LR-VLM LR-Action Weight decay Batch size-VLM Batch size-Action Epoch LR sched. Warmup Grad. clip 2104 2105 1102 2 per GPU 16 per GPU 10 Cosine 1000 step 1. Table 7. RL Parameters Parameter Value Action LR Value LR Rollout steps Gamma GAE Lambda Clip ratio Grad. clip Entropy coef. PPO epochs Batch size 1105 1104 1024 0.99 0.95 0.2 0.5 0.001 5 64 vision-language modeling and control learning. Figure 7. Overview of the annotation pipeline for VLM SFT. Representative frames and future trajectory information are used to generate long-horizon navigation instructions and concise CoT reasoning annotations. VLM Supervised Fine-Tuning. We first perform supervised fine-tuning (SFT) of the VLM. For each 20-second episode, we automatically generate long-term navigation instruction using GPT-5. To provide sufficient temporal context, we select five representative frames corresponding to the current time step and future time offsets at 5, 10, 15, and 20 seconds. These frames, together with the future trajectory offsets, are used to prompt the language model to generate four semantically equivalent instruction variants describing the same route. The variants differ only in wording and are randomly assigned across episodes to increase linguistic diversity while preserving route consistency. Figure 7 illustrates the annotation pipeline for navigation instruction generation and scenario reasoning on the DynaNav simulation data, while Figure 8 presents representative examples of instruction and reasoning annotations from the GND and SCAND datasets. In addition to language instructions, we annotate each episode with concise reasoning trace. The reasoning trace is generated by conditioning GPT-5 on richer temporal context that includes the past trajectory, observations at 3, 6, and 9 seconds in the past, the current observation, and future observations. The past trajectory is summarized at 1 intervals from the start of the episode to the current time, expressed in the local frame of the previous 1s observation, to help the model infer motion patterns and task progress. The generated CoT focuses on five aspects: (1) the current situation, (2) critical objects, (3) object motion, (4) task progress relative to the instruction, and (5) the next short-horizon plan. To ensure efficiency and consistency, the reasoning trace is constrained to be brief and is stored alongside the episode. During SFT, the vision encoder is frozen. The language model is trained to autoregressively generate either (1) reasoningaugmented sequence followed by waypoint predictions or (2) waypoint-only outputs. Mixing these two target formats allows 13 Think-in-Control Vision-Language-Action Model for Robot Navigation in Dynamic Environments Figure 8. Examples of navigation instruction and CoT reasoning annotations from the GND and SCAND datasets. the VLM to flexibly produce explicit reasoning when required and output only waypoints to maintain efficiency. Imitation Learning with Delayed Inference. After fine-tuning the VLM, we train the action expert via imitation learning while explicitly accounting for inference latency. To simulate real-world deployment conditions, we randomly sample inference delays and expose the action policy to temporally delayed VLM inference hidden states together with explicit latency metadata. During training, the policy is conditioned on delayed ground-truth reasoning annotations and guidance waypoints, ensuring access to the correct high-level intent. The overall training procedure is summarized in Algorithm 1. Algorithm 1 Imitation Learning with Delayed Inference Require: Demonstration dataset D, action policy πθ, frozen VLM fLM 1: for each training iteration do 2: 3: 4: 5: 6: 7: Sample instruction, current visual observation, and expert future trajectory, (I, xt, {ˆpt}) Sample inference delay (0, 10) Retrieve delayed visual observations and reasoning traces (X vlm Obtain delayed VLM hidden state (KV cache) Stt fLM(V vlm Extract current visual tokens act Encode robot state st and latency metadata (t, x, y, θ) Predict action chunk {a(1:T ) Integrate the predicted actions to obtain trajectory {p(1:T ) Compute imitation loss La Update θ by minimizing La tt, Rtt) tt, I, Rtt) , Stt, st, t, x, y, θ) = Encvis(xt) } πθ(V act 8: } t 9: 10: 11: 12: end for For the waypoint-guided policy variant, we transform the delayed VLM-predicted waypoints into the coordinate frame of the robots current position before conditioning the action policy. The same transformation is applied consistently during both training and inference. The parameters used for training are provided in Table 6. Online Reinforcement Learning. At each control step, the action expert predicts short-horizon trajectory, from which we select target corresponding to one second into the future. This target is converted into desired linear velocity and angular velocity, which define the mean of Gaussian action distribution. The policy additionally maintains learnable log-standard-deviation parameter, yielding stochastic Gaussian policy suitable for on-policy optimization. To preserve consistency with deployment conditions, the policy continues to operate on temporally delayed VLM inference hidden states and latency metadata during reinforcement learning. Delayed VLM hidden states are cached during rollout collection and reused during policy optimization. We adopt Proximal Policy Optimization (PPO) (Schulman et al., 2017) with learned value function to optimize the policy under this delayed-inference setting. The overall procedure is summarized in Algorithm 2 and the parameters used for PPO are provided in Table 7. 14 Think-in-Control Vision-Language-Action Model for Robot Navigation in Dynamic Environments Initialize rollout buffer for each environment step do Observe current visual observation xt and robot state st Run VLM inference asynchronously in the background Retrieve delayed VLM inference hidden state Stt and latency metadata (t, x, y, θ) Extract current visual tokens act Compute Gaussian policy parameters (µt, log σ) πθ(V act, Stt, st, t, x, y, θ) Sample action at (µt, σ2) Execute at in environment and observe reward rt and next state Store data (xt, Stt, st, at, rt, t, x, y, θ) in Algorithm 2 Online Reinforcement Learning Require: Action policy πθ, value network Vψ, frozen VLM fLM, environment 1: for each PPO iteration do 2: 3: 4: 5: 6: 7: 8: 9: 10: 11: 12: 13: 14: 15: 16: 17: 18: end for end for Compute returns ˆRt and advantages ˆAt using value network Vψ for each PPO optimization epoch do Update policy πθ using clipped surrogate objective Update value network Vψ by minimizing value regression loss = Encvis(xt) end for C. Benchmark Details We evaluate navigation methods using language-conditioned physics-based benchmark consisting of 85 episodes across four environments: Hospital, Office, Outdoor, and Warehouse. Each episode specifies unique combination of scene layout, start pose, goal location, and natural-language instruction. Task difficulty is systematically increased by varying pedestrian density (from 0 to 200 agents), instruction complexity, and time limits, while maintaining consistent goals within each task. Pedestrian agents exhibit diverse visual appearances and body types to increase visual variability and reduce reliance on appearance-specific cues. Figure 9 illustrates navigation instructions and scenarios included in the benchmark. For each environment, we design multiple navigation tasks that share the same semantic objective but differ in crowd density, robot platform (Nova Carter or Spot), and timeout constraints. This design enables controlled evaluation of robustness to dynamic obstacles, long-horizon instruction following, and cross-robot generalization. An episode is considered successful if the robot reaches the target location within predefined distance threshold before the time limit; outdoor environments use relaxed threshold to account for large-scale spatial uncertainty. All experiments are conducted with fixed random seed to ensure reproducibility. Hospital Environment. We conducted 25 episodes in large hospital scene featuring long corridors, patient rooms, medical equipment, and service areas. Tasks involve reaching vending machines, hospital beds, water dispensers, and hallway endpoints under dense pedestrian traffic. The hospital setting is characterized by narrow passages, frequent occlusions, and socially constrained motion, making it particularly challenging for safe navigation under crowd interference. For each task type, we systematically vary the number of simulated pedestrians from 0 to 60 and proportionally increase the episode timeout. This setting evaluates robustness to crowd density and social navigation challenges in structured indoor spaces. Office Environment. We perform 25 episodes in complex office environment composed of interconnected rooms, corridors, elevators, and desk areas. Tasks emphasize multi-step spatial instructions involving turns, room entries, and landmark-based goals (e.g., desks, carpets, doors), often beyond immediate perceptual range. The office environment introduces greater topological diversity and branching layouts, requiring precise instruction grounding rather than corridor following. Pedestrian density is again increased from 0 to 60 to study performance degradation under dynamic human participants in semantically rich but spatially compact environments. Warehouse Environment. We evaluate 25 episodes in structured warehouse scene with long aisles, racks, forklifts, cones, and fire extinguishers. The tasks emphasize fine-grained spatial reasoning, such as selecting the correct aisle, identifying object instances in repetitive layouts, and navigating to precise rack locations. The warehouse presents high visual repetition with minimal semantic diversity, stressing the models ability to disambiguate similar structures under motion and crowd 15 Think-in-Control Vision-Language-Action Model for Robot Navigation in Dynamic Environments interference. Pedestrian density is varied from 0 to 60 with adjusted timeouts to control task difficulty. Owing to the warehouses compact spatial layout relative to the office and hospital environments, this setting represents the most crowded and visually ambiguous scenario. Outdoor Environment. We include 10 episodes in large-scale outdoor urban environment with uneven terrain, ramps, streets, and large open spaces. These tasks involve long-distance navigation guided by distant landmarks, such as buildings, umbrellas, fountains, and storefronts. The number of pedestrians ranges from 100 to 200, and relaxed success threshold is used to account for localization uncertainty and large-scale goals. Due to the substantially higher rendering and simulation cost of large outdoor scenes, we limit this setting to 10 episodes while maintaining high environmental diversity. These episodes test scalability, visual clutter robustness, and instruction grounding in outdoor settings, representing the most demanding setting in terms of perception, planning horizon, and real-time control stability. Figure 9. Overview of the DynaNav benchmark. Task instructions are provided, as well as corresponding navigation scenarios across the Hospital, Office, Outdoor, and Warehouse environments, highlighting variations in scene layout, landmarks, and human density. D. Experiment Details Data Collection. We collect demonstration data by asking human expert to teleoperate the robot using keyboard across four designed environments (warehouse, office, hospital, and outdoor) within the DynaNav simulation. Each environment contains dynamic human participants. The expert navigates the robot toward predefined goals, and upon completing each demonstration, records corresponding natural language instruction describing the navigation task. The resulting dataset consists of RGB images and trajectories of robot camera poses. In total, we collected 310 episodes, corresponding to 5.1 hours of navigation data. Online Reinforcement Learning. We perform online RL of the TIC-VLA model on three different tasks (different from testing) across three environments: office, hospital, and warehouse. The environment operates at control frequency of 10 Hz, corresponding to an environment step of 0.1 seconds. During training, the VLM runs inference asynchronously in 16 Think-in-Control Vision-Language-Action Model for Robot Navigation in Dynamic Environments the background upon receiving new observations and returns its outputs to the action policy once available. We finetune only the final cross-attention Transformer layer and the MLP decoder of the action expert. To emulate deployment on resource-constrained hardware, we explicitly inject random inference delays before releasing the VLM outputs to the policy, with an average delay of approximately 50 environment steps (5 seconds). The three tasks and environments, illustrated in Figure 10, are rotated every 100 PPO iterations. Figure 10. Online reinforcement learning tasks used to train TIC-VLA across three environments and tasks. The weights for the reward function (Equation (4)) are set to wg = 400 for reaching the goal, wp = 5 for progress toward the goal, wc = 100 for collisions with obstacles and humans, and ws = 0.1 for the speed penalty. The target speed for the speed penalty is set to 1 m/s. Baseline Methods. We compare our approach against several baseline methods. The behavior cloning (BC) baseline is trained directly on the demonstration dataset without modeling inference latency or delayed perception, while being provided with privileged access to the relative goal position. For the RL baseline, we train PPO agent whose policy shares the same architecture as the value network, except that it directly outputs the action mean and standard deviation. The PPO policy is trained separately for each environment using 10 parallel environments and total of 1M steps per environment. For other baselines, including Uni-NaVid, NAVILA, NavDP, and DualVLN, we follow their respective open-source implementations and training. Ablation Action Policy Architectures. We implement the diffusion-based action policy as conditional denoising model over fixed-length action chunks of = 30 steps. The forward diffusion process uses = 100 noise steps with cosine variance schedule, and the denoiser is Transformer backbone aligned with the query-based policy to ensure comparable capacity. Noisy action tokens are augmented with timestep embeddings and positional encodings, while conditioning information from the vision encoder and cached VLM hidden states is injected at every denoising layer via cross-attention. Actions are normalized prior to training, and the model is trained with the standard denoising mean-squared-error objective to predict clean actions. During inference, we apply DDIM sampling with 5 steps, which provides an efficient trade-off between computation and trajectory quality while preserving multimodal behavior. The flow-based action policy models the conditional distribution of action chunks using normalizing flow with standard Gaussian base distribution, enabling exact likelihood optimization. The flow is composed of multiple coupling layers with learned affine transformations, and each layer is conditioned on the same vision features and cached VLM hidden states as the diffusion model to ensure architectural parity. Actions are standardized to zero mean and unit variance, and training maximizes the conditional log-likelihood of expert demonstrations. At inference, action sequences are generated by sampling from the Gaussian base and applying the inverse flow with 5 flow steps; reducing the number of steps leads to noticeably degraded performance. Simulation Settings. All experiments are conducted in high-fidelity Isaac Sim simulator running at 30 Hz. The navigation policy operates at control frequency of 10 Hz; actions are applied once every three simulation frames and held constant in between. Each episode is executed in an isolated process with fresh simulator instance, ensuring clean initialization and reproducibility. Episodes terminate when the robot reaches the goal (defined by 2D distance threshold of 1.5 m) or when timeout is reached. Robot trajectories are recorded at the simulation rate to compute path length and SPL. Collisions are detected both with animated human characters using distance proximity (threshold of 0.5 m) and with static scene objects using contact sensor. All metrics are aggregated across episodes and reported using standard navigation benchmarks. Testing Settings. We employ closed-loop simulator running at 30 Hz, while executing the policy at 10 Hz (every 3 17 Think-in-Control Vision-Language-Action Model for Robot Navigation in Dynamic Environments simulation steps), as summarized in Algorithm 3. At each 10 Hz tick, the agent samples short visual context Xk from the image buffer using SAMPLEFRAMESAVAILABLE, which selects all available frames among the target offsets (e.g., {t9, t6, t3, t}), falling back to fewer frames when full history is not yet available. Then we compute latency-aware ego-motion offsets (t, x, y, θ) using the most recent VLM-generation reference stored in G. The controller then consumes the policy output wk to produce smoothed velocity commands (vk, ωk), and the environment advances with E.STEP(vk, ωk). Asynchronous inference. VLM reasoning and KV-cache extraction are executed asynchronously via predict async (Algorithm 4) while the action policy continues to run at 10 Hz. Specifically, each call first polls the background VLM job and, if it has completed, updates the cached response rcache and cached KV kvcache. If no generation is currently running, new background generation job is launched using the VLM visual context vlm and instruction I, and the start metadata (kgen, posegen) is returned for downstream delay/ego-motion bookkeeping in G. While new VLM job is in flight, the action policy reuses the most recent available KV cache to decode wk from the current observation and robot state. Synchronous inference. In the synchronous baseline, VLM inference is executed in blocking manner, during which the robot halts and no control commands are issued. At each decision step, the robot remains stationary while the VLM processes the current visual context and instruction, ensuring that semantic reasoning is generated from up-to-date observations. As result, the effective semantic delay satisfies = 0, and the corresponding ego-motion offsets (x, y, θ) are all zero. Once inference completes, the robot executes the predicted trajectory for fixed horizon of 0.5 s, after which it again halts for the next VLM inference cycle. This design eliminates semantic staleness but introduces intermittent control stalls due to blocking inference. Inference without reasoning. For ablations that remove explicit VLM reasoning, we keep the same closed-loop rollout (Algorithm 3) and the same asynchronous interface (Algorithm 4), but disable VLM generation entirely. Concretely, the background VLM job performs forward pass on the visual-language input vlm and returns only the extracted KV cache; no tokens are decoded, and the textual response is not produced. As result, rk is always None, and control depends only on the cached KV signal and current observations. Latency control in simulation. We evaluate robustness under effective semantic delay, defined as the elapsed time between observation capture and semantic state availability. To isolate the effect of delay, we keep the semantic model fixed and vary the scheduling of semantic updates. The default asynchronous setting yields an average effective latency of about 2 with full semantic reasoning. For lower-latency condition (1 s), we pause the simulation after 1 of elapsed time and resume it once VLM inference completes, yielding reduced effective delay. For higher-latency settings (3-5 s), visual observations are captured immediately, but VLM inference is launched after delay of 30, 60, or 90 simulation frames (30 Hz). During this period, the policy continues executing with the most recent cached semantics, and newly generated responses and KV caches are applied immediately upon completion. This increases effective semantic delay without altering simulation dynamics. Evaluation Metrics. We evaluate navigation performance using standard goal-reaching metrics. Success (S) for single episode is defined as whether the robot reaches the goal within 2D Euclidean distance threshold Tgoal = 1.5m: = I(pt,xy gxy2 < Tgoal) , (5) where pt,xy denotes the robot position at termination and gxy denotes the goal position. Success Rate (SR) is reported as the average success over all testing episodes. Navigation Error (NE) measures the final 2D distance to the goal: Path Length (PL) is computed as the cumulative 2D distance traveled by the robot: NE = pt,xy gxy2. where {pi,xy}N i=1 are robot positions. PL = 1 (cid:88) i=1 pi+1,xy pi,xy2, (6) (7) Think-in-Control Vision-Language-Action Model for Robot Navigation in Dynamic Environments ) SAMPLEFRAMESAVAILABLE(cid:0)B(cid:1) (kref, poseref) G[0] (x, y, θ) DISPLACEMENTINFRAME(poseref, posek) (k kref)/ posek E.OBSERVEPOSE() imgk E.RENDERCAMERA() append imgk to if mod 3 = 0 then , vlm (X act if = then Algorithm 3 Closed-loop Rollout with Asynchronous VLM KV-cache Features Require: Environment (30 Hz), wrapper TICVLA, controller C, instruction I, goal 1: s0 E.RESET() 2: image buffer 3: reset episode state() 4: first kv ready False 5: (v0, ω0) (0, 0) {initial controls} 6: (vk, ωk) (v0, ω0) 7: {stores (kgen, posekgen )} 8: for = 0, 1, 2, . . . do 9: 10: 11: 12: 13: 14: 15: 16: 17: 18: 19: 20: 21: 22: 23: 24: 25: 26: 27: 28: 29: 30: 31: 32: 33: 34: 35: 36: 37: end for end if srobot [vx, vy, ωz, x, y, θ] (rk, wk, kgen, kv ready, posegen) predict async(X act if kgen = None then end if E.STEP(vk, ωk) {termination: success / timeout} end if if first kv ready then k) (0, 0) end if if first kv ready kv ready then (v end if (vk, ωk) SMOOTH(v (t, x, y, θ) (0, 0, 0, 0) first kv ready True k) C(wk) , vlm k, ω k) k, ω k, ω else else (v G KEEPLAST2(cid:0)G {(kgen, posegen)}(cid:1) {keep last completed generation ref and the current in-flight one} , I, srobot , t, x, y, θ, k, posek) 19 Think-in-Control Vision-Language-Action Model for Robot Navigation in Dynamic Environments , I, srobot Algorithm 4 predict async (asynchronous VLM generation and action policy) , vlm gen job {background VLM generation job, or None} kvcache {most recent extracted KV cache} rcache {most recent completed VLM response} (kgen, posegen) (k, posek) {record when and where this generation starts} start background VLM generation using vlm gen job becomes active 1: Function predict async(X act , t, x, y, θ, k, posek) 2: /* persistent internal state (stored across calls) */ 3: 4: 5: 6: POLLRUNNINGGENERATION(k) {if background generation finished at step k, update caches and mark job inactive} 7: kgen None 8: posegen None 9: if gen job is inactive then 10: 11: 12: 13: end if 14: if kvcache is undefined then 15: 16: 17: 18: else 19: 20: 21: 22: end if 23: wk πθ(EMBED(X act 24: return (rk, wk, kgen, kv ready, posegen) , kv) {action policy uses current vision + robot state + cached semantic KV} kv kvcache rk rcache kv ready True kv None rk rcache kv ready False ), srobot and Success weighted by Path Length (SPL) is defined as: SPL = SR dopt max(dopt, PL) , (8) where dopt is the shortest distance from the start position p0,xy to the goal. Collision (C) is binary episode-level indicator that equals 1 if any collision occurs during the episode and 0 otherwise: = I( s.t. collisiont) , (9) Collisions include contacts with both animated human agents and static scene objects. The Collision Rate (CR) is computed as the fraction of test episodes in which at least one collision occurs. Physical Robot Testing. We evaluate all methods on Unitree Go2 quadruped robot. The primary computing setup consists of laptop equipped with an NVIDIA RTX 4060 GPU and an NVIDIA Jetson Orin NX mounted onboard. For baseline methods whose memory or compute requirements exceed the capacity of these devices, we use remote desktop equipped with an NVIDIA RTX A6000 GPU. In this setup, the remote machine performs model inference and communicates with the laptop that relays sensor observations and action commands to the robot. E. Additional Results Simulation Testing. We provide additional simulation testing results of TIC-VLA in diverse DynaNav environments, including outdoor, hospital, and warehouse, using multiple robot platforms, in Figure 11. The results demonstrate that TIC-VLA is able to generate coherent semantic reasoning under delayed perception and translate it into stable, low-level control commands. The robot consistently adapts its behavior to dynamic agents by yielding, adjusting speed, and selecting appropriate turning maneuvers, while following the given natural language instructions. To further illustrate temporal consistency and closed-loop behavior, dynamic video results are provided. 20 Think-in-Control Vision-Language-Action Model for Robot Navigation in Dynamic Environments Figure 11. Additional qualitative simulation results of TIC-VLA on the DynaNav benchmark. Rows correspond to different environment types: top-outdoor, middle-hospital, and bottom-warehouse. Real-world Testing. We report task-specific results from real-world robot navigation testing. Each task is executed five times, and the average success rate (SR) is reported in Table 8. As shown, TIC-VLA consistently outperforms prior methods across both indoor and outdoor environments. Figure 12 presents three representative real-world navigation examples under diverse conditions. To illustrate closed-loop control behavior, we also provide dynamic video results. Table 8. Real-world testing task-specific evaluation results. Task TIC-VLA SR (%) DualVLN SR (%) NaVILA SR (%) Indoor Hallway Indoor Office Outdoor Campus Outdoor Sidewalk Average 80 80 80 100 85 40 20 60 50 20 0 60 60 35 Influence of Ego-motion Offset in Action Policy. Table 9 demonstrates the impact of incorporating ego-motion offset into the latency-aware action policy. Without ego-motion offset, the policy treats delayed semantic guidance as temporally aligned with the current state, leading to degraded navigation accuracy and efficiency. By explicitly modeling the ego-motion accumulated during inference delay, the policy can reinterpret delayed semantic features in the current time, resulting in substantially improved performance across all metrics. These results confirm that ego-motion offset is an important component for compensating for the inference latency, enabling more robust action execution under latency. Table 9. Effect of incorporating ego-motion offset into the latency-aware action policy. Method NE () SR () SPL () CR () W/o Ego-motion Offset W/ Ego-motion Offset 12.97 10.85 41.18 47. 36.36 42.41 36.47 34.12 Influence of Action Policy. Table 10 compares three action policy designs in TIC-VLA: diffusion-based, flow-based, and query-based Transformer policies. Overall, we observe no performance benefit from the more complex diffusion and flow-matching policies over simple query-based Transformer. The query-based policy consistently achieves the best results across all metrics. By directly predicting action sequences in single forward pass, it minimizes inference latency and 21 Think-in-Control Vision-Language-Action Model for Robot Navigation in Dynamic Environments Figure 12. Real-world navigation examples demonstrating TIC-VLA executing language-conditioned navigation under real-time control. preserves tight alignment between perception and control. Moreover, the simpler architecture facilitates more stable and efficient RL fine-tuning. Table 10. Comparison of different action policies in the TIC-VLA model. Policy Type NE () SR () SPL () CR () Diffusion-based Flow-based Query-based (Ours) 14.30 15.04 10.85 38.82 32.94 47.06 35.41 29.65 42.41 41.18 45.88 34.12 Influence of VLM Backbone. We study the effect of VLM backbone scale and visual tokenization in TIC-VLA by comparing InternVL3 (1B) with SmolVLM2 (Shukor et al., 2025) (500M) and Qwen2.5-VL (Bai et al., 2025) (3B), as summarized in Table 11. The smaller SmolVLM2 benefits from lower inference latency but exhibits higher navigation error and lower success and SPL due to limited representational capacity. In contrast, the larger Qwen2.5-VL provides stronger visual understanding but incurs substantial inference delay, which degrades perception-action alignment and overall navigation performance. InternVL3 achieves the best trade-off between capacity and efficiency. Table 11. Comparison of VLM backbone in the TIC-VLA model. Backbone NE () SR () SPL () CR () SmolVLM2 (500M) InternVL3 (1B) Qwen2.5-VL (3B) 14.82 10.85 13.26 38.82 47.06 32.94 31.06 42.41 29.98 41.18 34.12 35."
        }
    ],
    "affiliations": [
        "University of California, Los Angeles"
    ]
}