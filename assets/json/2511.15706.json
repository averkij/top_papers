{
    "paper_title": "RoMa v2: Harder Better Faster Denser Feature Matching",
    "authors": [
        "Johan Edstedt",
        "David Nordström",
        "Yushan Zhang",
        "Georg Bökman",
        "Jonathan Astermark",
        "Viktor Larsson",
        "Anders Heyden",
        "Fredrik Kahl",
        "Mårten Wadenbäck",
        "Michael Felsberg"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Dense feature matching aims to estimate all correspondences between two images of a 3D scene and has recently been established as the gold-standard due to its high accuracy and robustness. However, existing dense matchers still fail or perform poorly for many hard real-world scenarios, and high-precision models are often slow, limiting their applicability. In this paper, we attack these weaknesses on a wide front through a series of systematic improvements that together yield a significantly better model. In particular, we construct a novel matching architecture and loss, which, combined with a curated diverse training distribution, enables our model to solve many complex matching tasks. We further make training faster through a decoupled two-stage matching-then-refinement pipeline, and at the same time, significantly reduce refinement memory usage through a custom CUDA kernel. Finally, we leverage the recent DINOv3 foundation model along with multiple other insights to make the model more robust and unbiased. In our extensive set of experiments we show that the resulting novel matcher sets a new state-of-the-art, being significantly more accurate than its predecessors. Code is available at https://github.com/Parskatt/romav2"
        },
        {
            "title": "Start",
            "content": "RoMa v2: Harder Better Faster Denser Feature Matching Johan Edstedt1 David Nordstrom2 Yushan Zhang1 Georg Bokman3 Viktor Larsson4 Anders Heyden4 Jonathan Astermark4 Fredrik Kahl2 Marten Wadenback1 Michael Felsberg1 1Linkoping University, 2Chalmers University of Technology 3University of Amsterdam, 4Centre for Mathematical Sciences, Lund University 5 2 0 2 0 ] . [ 2 6 0 7 5 1 . 1 1 5 2 : r a"
        },
        {
            "title": "Abstract",
            "content": "Dense feature matching aims to estimate all correspondences between two images of 3D scene and has recently been established as the gold-standard due to its high accuracy and robustness. However, existing dense matchers still fail or perform poorly for many hard real-world scenarios, and high-precision models are often slow, limiting their apIn this paper, we attack these weaknesses on plicability. wide front through series of systematic improvements that together yield significantly better model. In particular, we construct novel matching architecture and loss, which, combined with curated diverse training distribution, enables our model to solve many complex matching tasks. We further make training faster through decoupled two-stage matching-then-refinement pipeline, and at the same time, significantly reduce refinement memory usage through custom CUDA kernel. Finally, we leverage the recent DINOv3 foundation model along with multiple other insights to make the model more robust and unbiased. In our extensive set of experiments we show that the resulting novel matcher sets new state-of-the-art, being significantly more accurate than its predecessors. Code is available at https://github.com/Parskatt/romav2. 1. Introduction Dense matching is the task of matching every pixel of image IA RH AW A3, with image IB RH 3 in terms of warp WA(cid:55)B RH AW A2 and confidence mask pA(cid:55)B [0, 1]H AW A1. In dense feature matching, the assumption is that the pixels in both images are observations of 3D points from the same scene. In this case, for perfect matcher, the confidence pA(cid:55)B is 1 for pixels corresponding to 3D point in the scene that is observable from both views, i.e., that are co-visible, and 0 for occluded pixels. Feature matching is fundamental task in Computer Vision, as many downstream tasks, e.g., visual localization [29, 34, 42] and 3D reconstruction [19, 20, 25, 36, 38], Figure 1. Radar chart of performance on benchmarks. RoMa v2 outperforms previous dense matchers on wide range of pose estimation and dense matching tasks. Further details on these experiments can be found in Section 4. rely on precise and trustworthy correspondences in order to function robustly. Traditionally, these methods relied on sparse matches established purely through descriptor similarity. In the last couple of years, these detector-descriptor methods have been gradually replaced by learning-based matchers that consider pairs of images when establishing the correspondences, allowing the networks to not only consider visual similarly but also the spatial context. This development in feature matching has been driven by the introduction of several challenging benchmarks, such as MegaDepth-1500 [22, 41], ScanNet-1500 [9, 33], WxBS [27] and the recurring Image Matching Challenge at CVPR [15]. These benchmarks are currently dominated by detector-free methods, such as dense feature matchers [11] and feed-forward reconstruction models [47]. One noteable dense matcher is RoMa [12], which has proven robust to extreme photometric changes, including different modalities, due to using features from frozen foundation model for the matching instead of learning features from scratch. However, RoMa still struggles in 1 Figure 2. Qualitative results. RoMa v2 excels at matching in diverse scenarios. We show snapshot of results from different benchmarks. Below each image pair we visualize the dense warp by coloring each pixel by the RGB value from its estimated corresponding location in the opposite image. Brighter values mean lower warp confidence as output by the model. many challenging scenarios. For example, the recent RUBIK benchmark [24] highlights its weakness under extreme viewpoint changes. Additionally, RoMa has significant runtime and memory footprint, limiting its applicability for large-scale tasks or resource constrained settings. Recently, UFM [52] showed that dense matching can be made significantly faster than in RoMa. However, UFM requires finetuning of the pretrained feature extracting backbone, which leads to worse performance on datasets with extreme appearance changes such as WxBS. Furthermore, UFM performs worse than RoMa on benchmarks that require subpixel precision, such as MegaDepth-1500 [22, 41]. Motivated by the different trade-offs in RoMa and UFM, in this paper we address the challenge of combining their respective strengths, i.e., developing dense feature matcher that is both robust to extreme changes in viewpoint and appearance, applicable to wide range of real-world scenarios, all while maintaining subpixel precision and practical runtime and memory footprint. To this end, we introduce RoMa v2, which builds on RoMa and features several improvements to increase robustness while simultaneously reducing the computational cost. RoMa v2 achieves stateof-the-art results on wide range of benchmarks, as seen in Figure 1. Qualitative examples from the benchmarks are visualized in Figure 2. In summary, our main contributions are: 1. novel matching objective, combining warp and correlation-based losses, which enables multi-view context to be learned in the coarse matcher of RoMa, described in Section 3.2. 2. Faster and less memory intensive refiners than in RoMa, described in Section 3.3 and ablated in Table 8. 3. custom mixture of wideand small-baseline datasets in the training data, that helps balance robustness to extreme viewpoints while maintaining sub-pixel performance across wide range of difficult matching tasks, detailed in Section 3.4. 4. Prediction of pixel-wise error covariance that can be used downstream in refinement of estimated geometry, as demostrated in Section 4.6 5. We experimentally verify that these improvements significantly reduce the runtime compared to baseline RoMa, while matching or outperforming both RoMa and UFM on their respective strongpoints across wide range of benchmarks in Section 4. 2. Related Work Feature Matching: Traditionally, feature matching has been dominated by the sparse paradigm, where keypoints are first detected separately in each image, and then matched by sparse feature matcher. While early matching was driven by similarity of local descriptors, the recent trend is instead to rely on learned matcher that jointly considers the keypoints and descriptors in each image. Notable recent works in this area include SuperGlue [33] and LightGlue [23] which both use an attention-based approach for solving the optimal assignment. Common among the sparse 2 methods is the reliance on salient image regions, where repeatable keypoints can be detected. In contrast, LoFTR [41] takes detector-free approach by matching through attention on learned features, resulting in semi-dense matchIn ing where even non-salient points can be matched. DKM [11], the matching is performed on pyramid of feature maps, enabling pixel-dense matches. In follow-up work, RoMa [12] further improves on this method by using frozen foundation model to encode the coarse matching features, making it significantly more robust to extreme appearance changes. Recently, UFM [52] was introduced as more lightweight dense matcher, where the training of wide-baseline dense matching is unified with the related task of optical flow. Feedforward Reconstruction: Recovering 3D structure and camera parameters from images, or Structure-fromMotion (SfM) [14], has traditionally relied on sequential pipelines such as Bundler [38] and COLMAP [36], where point correspondences obtained through image matching play central role. Recently, learning-based SfM methods have emerged, and many now incorporate matching within feedforward architecture. DUSt3R [47] and MASt3R [21] directly regresses point maps from image pairs; and VGGT [46] and MapAnything [18] extend this paradigm to longer sequences. While these models can produce coarse correspondences, they struggle to yield accurate high-resolution dense matches. While our architecture is loosely inspired by these approaches we retain an explicit dense matching formulation that provides subpixel-accurate correspondences. 3. Method In this section, we outline our proposed method. We begin by discussing our two-stage matching-then-refinement architecture in Section 3.1 and proceed to discuss its two parts in Sections 3.2 and 3.3, respectively. In Section 3.4, we describe the training data used and in Section 3.5 we explain the method used to make RoMa v2 more robust to changes in image resolution. 3.1. Architecture We take inspiration from previous works [12, 52] and divide the dense matching task into matching step and refinement step. Intuitively, these two tasks entail first finding an approximate or coarse match for each pixel, and, conditioned on this, refining the matching to sub-pixel accuracy. An overview of the architecture is shown in Figure 3, and the respective components in Figure 4 and Figure 5. While some previous works, e.g. [11, 12], decouple gradients between matchers and refiners but still train both jointly, we instead opt for two-stage training paradigm inspired by UFM [52]. This enables rapid experimentation. We next go into detail on the matcher in Section 3.2, followed by the refinement in Section 3.3. 3.2. Matcher An overview of the coarse matcher is shown in Figure 4. We upgrade the DINOv2 [28] encoder used in RoMa to the newer DINOv3 [37]. Inspired by Edstedt et al. [12], we compare the encoders (frozen) by training single linear layer on the features followed by kernel nearest neighbor matcher. As is shown in Table 1, we find that DINOv3 is more robust than its predecessor despite its slightly larger patch size (16 vs. 14). As in RoMa, but unlike UFM, we freeze the encoder weights, improving robustness. While RoMa is robust and generalizes well, one of its main weaknesses is its lacking multi-view context in the matcher, which relies solely on Gaussian Process (GP) [31] regression combined with single-view Transformer decoder to classify warp bins. naive approach would be to add Multi-view Transformer to RoMa before the GP, however, we found that in practice the gradients through the GP were not sufficiently informative to yield improvements, and caused stability issues during training. To remedy this, we replace the Gaussian Process with simple single headed Attention mechanism. Additionally, we add an auxiliary target, LNLL, to minimize the negative log-likelihood of the best matching patch in image for each patch in image A. We first compute the similarity between all patches from image and image to form similarity matrix RM , where and are the number of patches in image and respectively. The loss LNLL is computed by first applying Softmax over the second dimension of and then selecting the most similar pairs for all patches in A, i.e. LNLL = (cid:88) m= log(Softmax(Sm)n ) (1) where is the index of the patch closest to the GT warp for patch m. This approach can be seen as dense directional version of, e.g. LoFTR [41]. We still however use regression head, which is trained to minimize the robust regression loss between its predicted warp and the ground truth warp. The full coarse matching pipeline independently tokenizes the input frames using DINOv3 ViTL, and then applies ViT-B Multi-view Transformer. Following Wang et al. [46], we alternate between frame-wise and global Attention. In contrast to Wang et al. [46], we do not use RoPE across frames. The final token embeddings, Softmax(S)xB, where xB are position embeddings as in RoMa, and DINOv3 features, are jointly processed by DPT head to predict the warp and confidence. Further details on the matcher architecture are given in the supplementary material. 3 Figure 3. Overview of RoMa v2. We estimate bidirectional dense image warps = {WA(cid:55)B RHW 2, WB(cid:55)A RHW 2} and warp confidences = {pA(cid:55)B RHW 1, pB(cid:55)A RHW 1} between two input images using two-stage pipeline consisting of matching and refinement stage. Different from recent SotA dense matchers, we additionally predict precision matrix Σ1 = {(Σ1)A(cid:55)B RHW 22, (Σ1)B(cid:55)A RHW 22}. The coarse matcher is Multi-view Transformer, that takes in frozen DINOv3 [37] foundation model features from image IA RHW 3 and IB RHW 3. Its internals are further illustrated in Figure 4, and explained in detail in Section 3.2. The refiners are fine-grained UNet-like CNN models that, conditioned on the previous warp and confidence, produce displacements and delta confidences. Besides this, they additionally predict full 2 2 precision matrix per-pixel, which is visualized as (cid:12) 1/4. The refiners are further illustrated in Figure 5 and explained in more detail in Section 3.3. (cid:12)Σ1(cid:12) (cid:12) Figure 4. Coarse matcher. We use frozen DINOv3 feature extractor in the coarse matching stage. DINOv3 features from both input images are input to Multi-view Transformer utilizing alternating Attention. Dense Prediction Transformer (DPT) [30] heads output coarse warps between the images and confidences for 4x downsampled resolution. Table 1. Robustness of frozen features. We compare coarse features for matching through linear probe on MegaDepth. Robustness is the share of matches with an error below 32 pixels. Method EPE Robustness % DINOv2 DINOv3 27.1 19.0 77.0 86.4 Matching Loss: We use the same overlap loss Loverlap, and weighting factor, as in RoMa. However, we replace the classification-by-regression term from the matching loss for the robust regression term Lwarp used in the refinement loss, which is also used by UFM. Finally, we add our proposed LNLL and obtain: Lmatcher = LNLL + Lwarp(rθmatcher, pGT) + 102Loverlap(pθmatcher, pGT) . (2) In contrast to UFM, our matching objective incorporates the auxiliary target LNLL.We compare these architectures in Table 2. Comparing RoMa v2 and UFM matching architectures on Hypersim [32]. Measured in PCK (higher is better). Method PCK@ 1px 3px 5px UFM RoMa v2 11.2 30.5 48.3 76.7 67.4 86.7 Figure 5. Refiner internals. The coarse matcher predicts at resolution 4x smaller than the original image size. The refiners output at the original resolution. Table 2 by training on subset of the data using the training setup outlined below and evaluating on holdout scenes from Hypersim [32]. We find that the RoMa v2 matcher significantly improves robustness. Coarse Matching Training: We start by training the coarse matcher for 300k steps of batch size 128, resulting in approximately 38M pairs seen throughout training. We use learning rate of 4 104. 4 3.3. Refiners After the matcher has finished training, we freeze it and run it in inference mode, producing coarse warp for the refiners to refine. We train the refiners for 300k steps of batch size 64, resulting in total of approximately 19M pairs. Like the matcher, we use learning rate of 4 104. Architecture: As the matcher, in contrast to RoMa where the matcher predicts at stride 14, predicts at stride 4, there is only need to refine at strides equal or smaller. We thus construct three refiners at strides {4, 2, 1} respectively. These follow similar architecture as in RoMa [12] with some efficiency improvements. First, we find that the local correlation implementation used in RoMa uses large amount of memory, especially at high resolution. To remedy this we write custom CUDA kernel as PyTorch extension, which significantly reduces the memory consumption (cf. Table 8). We further change all channel dimensions to be powers of two, which further boosts performance. Further details about the refiners is given in the appendix. Predictive Covariance: It is often useful, besides notion of predicted overlap, to have access to numerical estimate of the expected error. While predictive uncertainty has been previously studied [16, 35, 44, 51, 55], State-ofthe-Art methods such as RoMa [12] or UFM [52] do not provide any such estimate. To remedy this, we predict pixel-wise Gaussian uncertainty of the 2D residuals, rθ := WA(cid:55)B θ WA(cid:55)B GT RHW 2, (3) θ (h, w) 0, i.e., Σ1 through 2 2 precision matrix Pθ RHW 22 where Σ1 θ (h, w) is positive definite. We ensure this by constraining the network to predict the three elements z11, z21, z22 and mapping these to Cholesky factors as l11 = Softplus(z11) + 106, l21 = z21, l22 = Softplus(z22) + 106, where Softplus() = ln(1 + exp()). The lower triangular matrix is composed from the factors as = (cid:18)l11 l21 (cid:19) 0 (4) and then the covariance matrix is formed from this as Σ1 = LL. To learn z11, z21, z22 we directly train the model to minimize the negative log-likelihood Lprecision(r) = log (r0, Σ) 1 2 rΣ1r 1 2 = (5) log det(Σ1) + log(2π). To ensure stability, we only train the model to predict this covariance for covisible regions where < 8 pixels. We additionally detach the residuals rθ before the loss. We predict the precision in an hierarchical fashion from stride 4 up to stride 1, and use that fact that information 5 (a) Before EMA (b) After EMA Figure 6. Subpixel bias of refinement. We observe that models exhibit subpixel fluctuations in their predictions throughout training, leading to bias. We propose simple remedy through storing an exponential moving average (EMA). is additive in the precision parameterization to predict our final precision matrix as Σ1 θi = (cid:88) ji Σ1 θj . (6) We find empirically that our covariance improves performance in downstream tasks (cf. Table 10), and that it qualitatively behaves as one would expect in Figure 9. EMA to remedy bias: During training, we empirically observed that predictions tend to have small, but noticeable, sub-pixel bias (typically around 0.1 pixels in resolution 640 640). At first this seemed like data issue, but through plotting this bias over the course of training we found that it appears almost random, see Figure 6a. As the bias is seemingly uncorrelated over the course of training, simple way to fix it is to simply use an Exponential Moving Average (EMA)1. We found decay factor of α = 0.999 to work well empirically. After applying this remedy, we find that the bias in both orientations is substantially diminished, see Figure 6b. Refinement Loss: We train the refiners using combination of three losses. Following RoMa we use generalized Charbonnier loss [2] which for each refiner reads Lwarp = (ic)α (cid:18) r2 (ic)2 + 12 (cid:19)α/2 , (7) where we follow RoMa and set α = 0.5, = 103 and {4, 2, 1} is the stride. For estimating the overlap we follow UFM and RoMa and use pixel-wise binary cross-entropy loss as Loverlap, with the ground truth overlap pGT {0, 1} being derived from either consistent depth (for MVS style datasets) or from warp cycle consistency (for flow datasets). Further details on computing ground truth warps and overlaps are given in the suppl. material. 1See Izmailov et al. [17] for discussion regarding different variants. Figure 7. Finegrained objects in warp. Left: RoMa warp for small baseline pair. Note the missing warp for the guitar in the bottom right. Right: RoMa v2 warp. RoMa v2 is significantly better at capturing small objects with dynamic motion. Figure 8. Better texture-poor geometry prediction. Left: RoMa warp for large-baseline pair with texture-poor road surfaces, with warp missing for almost the entire road. Right: RoMa v2 warp. RoMa v2 predicts accurately for covisible road."
        },
        {
            "title": "The total refinement loss is",
            "content": "Lrefiners = (cid:88) Lwarp(rθi, pGT) i{1,2,4} + 102Loverlap(pθi, pGT) + 103Lprecision(Σ1 θi , detach(rθi)). (8) 3.4. Data In particular, the inclusion of We train RoMa v2 on mix of wide and small baseline twoview datasets, summary of which is presented in Table 3. Our mix is inspired by UFM [52], and significantly more diverse than RoMa [12], which is only trained on MegaDepth. the Aerial datasets AerialMD [45] and BlendedMVS [49], enable our proposed model to be significantly more robust to large rotations and air-to-ground viewpoint changes. The inclusion of small-baseline datasets, like FlyingThings3D [26], makes RoMa v2 significantly better at predicting finegrained details. We qualitatively compare RoMa v2 to RoMa on fine-grained prediction on the FlyingThings3D dataset in Figure 7. We also find that our data mixture enables us to predict textureless surface significantly better than RoMa, particularly in Autonomous Driving (AD) scenarios, despite only training on the very small-scale dataset VKITTI2 [6, 13]. We demonstrate this for randomly selected pair from the NuScenes dataset [7] in Figure 8. 3.5. Resolution We find that some elementary key changes make matching and refinement robust to the choice of resolution. Coarse Matching: Following DINOv3 [37] we use RoPE [40] on normalized grid, rather than pixel grid. 6 Table 3. Dataset mixture for RoMa v2. The top part contains wide-baseline datasets, while the bottom part contains smallbaseline datasets. The weight is proportional to the probability of sampling an image pair from the respective dataset. Further details about the datasets is provided in the supplementary. Datasets Type / GT Source Weight Scenes MegaDepth [22] AerialMD [45] BlendedMVS [49] Hypersim [32] TartanAir v2 [48] Map-Free [1] ScanNet++ v2 [50] Outdoor / MVS Aerial / MVS Aerial / Mesh Indoor / Graphics Outdoor / Graphics Object-centric / MVS Indoor / Mesh 1 1 1 1 1 1 1 UnrealStereo4k [43] Outdoor / Graphics Virtual KITTI 2 [6, 13] Outdoor / Graphics Outdoor / Graphics FlyingThings3D [26] 0.01 0.01 0.5 Total 169 124 493 393 46 397 8 5 2239 5069 This ensures that distances are always in distribution, even when changing resolution significantly. Secondly, we find that the absolute position encodings used for the match embeddings need to be of low enough frequency, to ensure that their interpolation is unproblematic [53]. Compared to RoMa, which initializes the scale to ω = 8, and let it be trained, we set it fixed to ω = 1. It is possible that the high frequency of the position embeddings is the cause of the issue which requires UFM to be run at fixed resolution of 420 560 during inference. Refinement: Ensuring resolution robustness for the refiners is non-trivial, as convolution is tied to the pixel grid. We find that the approach used in RoMa, whereby the input displacement is rescaled relative to canonical resolution Table 4. SotA comparison on MegaDepth-1500 [22, 41]. The top part contains feed-forward 3D reconstruction models, while the bottom part contains feature matchers. Table 5. SotA comparison on ScanNet-1500 [9, 33]. The top part contains feed-forward 3D reconstruction models, while the bottom part contains feature matchers. Method AUC@ 5 10 20 Method AUC@ 5 10 20 Reloc3r [10] CVPR25 MASt3R [21] ECCV24 VGGT [46] CVPR25 LightGlue [23] ICCV23 LoFTR [41] CVPR21 DKM [11] CVPR23 RoMa [12] CVPR24 UFM [52] NeurIPS25 RoMa v2 Our reproduced numbers. 49.6 42.4 33. 51.0 52.8 60.4 62.6 41.5 62.8 67.9 61.5 52.9 68.1 69.2 74.9 76.7 57.9 77.0 81.2 76.9 70.0 80.7 81.2 85.1 86.3 72.4 86.6 generalizes the best. Training: We train the coarse matcher on mix of resolutions and aspect ratios, specifically: {512 512, 592 448, 624 416, 688 384, 448 592, 416 624, 384 688}. The refiners are trained exclusively with size 640 640. 4. Experiments The qualitative improvements of RoMa v2 as shown in Figures 2, 7 and 8 are confirmed by the quantitative results from extensive experiments, listed in this section. 4.1. Relative Pose Estimation We compare RoMa v2 to state-of-the-art matchers and feedforward 3D reconstruction methods on relative pose estimation. For sampling correspondences we follow RoMa and compute bidirectional warps from which we sample correspondences in thresholded distribution where we set ˆpA(cid:55)B = max(1pA(cid:55)B >0.05, pA(cid:55)B). We use coarse resolution of 800 800 and fine resolution of 1024 1024. Similarly, we also sample balanced subset of matches using their kernel density estimate approach. We report results on MegaDepth-1500 [22, 41] in Table 4 and ScanNet-1500 [9, 33] in Table 5. RoMa v2 consistently outperforms all prior matchers on both benchmarks. On MegaDepth, which demands accurate sub-pixel correspondences, RoMa v2 also surpasses all 3D reconstruction methods. On ScanNet, RoMa v2 achieves performance on par with VGGT and MASt3R. 4.2. Dense Matching We evaluate dense matching performance in Table 6 and Table 7 on wide array of datasets and compare to stateof-the-art dense matchers RoMa and UFM. For RoMa and RoMa v2, we directly feed the 640 640 images into the model, while for UFM we first resize the image to their suggested inference resolution 560 420 and then bilinearly Reloc3r [10] CVPR25 MASt3R [21] ECCV24 VGGT [46] CVPR25 LightGlue [23] ICCV23 LoFTR [41] CVPR21 DKM [11] CVPR23 RoMa[12] CVPR24 UFM [52] NeurIPS25 UFM [52] NeurIPS25 RoMa v2 Our reproduced numbers. 34.8 33.6 33.9 17.8 22.1 29.4 31.8 31.6 31.3 33. 58.4 56.8 55.2 34.0 40.8 50.7 53.4 54.1 54.1 56.2 75.6 74.1 73.4 52.0 57.6 68.3 70.9 - 72.0 73.8 upsample the predictions back to 640 640, as their precision degrades significantly for higher resolution. RoMa v2 consistently outperforms across all 6 datasets. Notably, we simultaneously beat UFM on its own benchmark, TA-WB, and RoMa on MegaDepth, on which it is exclusively trained, while having 84% lower EPE compared to RoMa on the challenging AerialMegaDepth benchmark. 4.3. Runtime Comparisons In Table 8 we compare the runtime of RoMa v2 with RoMa and UFM. As can be seen from the table, we improve the throughput significantly compared to RoMa, running 1.7 faster. Compared to UFM our model is slightly slower, however with much smaller memory memory footprint. 4.4. Multi-Modal Matching on WxBS We evaluate the robustness of RoMa v2 on the extremely challenging WxBS benchmark [27]. This benchmark consists of handlabeled correspondences between images taken with extreme changes in either viewpoint, illumination, modality, or all three, which measures the generalizability of the matcher to out-of-distribution downstream tasks. Results are presented in Table 9. We observe that the performance of RoMa v2 is significantly higher than UFM, but slightly lower than RoMa. Investigating the cause of this we found that RoMa v2 and UFM both struggle with the IR-to-RGB multi-modal subset of WxBS. 4.5. Astronaut to Satellite Image Matching We introduce new benchmark, SatAst, for matching images taken by astronauts from the international space station to satellite images. Prior work on this modality has focused on the retrieval task of searching database of satellite images for the image content of given astronaut image [3 5, 39]. We take 39 corresponding image pairs from EarthMatch [3] to create SatAst and hand-annotate 10 accurate 7 Table 6. Dense matching performance. Images are resized to 640 640 pixels. TA-WB EPE 1px 3px 5px EPE MegaDepth 1px 3px 5px EPE ScanNet++ v2 3px 1px 60.61 15.85 13.82 35.1 31.3 67.7 52.6 65.5 81.8 56.2 75.1 85.8 2.34 3.15 1. 74.8 55.3 79.6 93.7 88.0 94.7 96.4 93.7 96.7 27.52 6.93 4.00 20.2 31.4 45.5 42.8 67.7 77. 5px 53.6 80.0 86.6 Table 7. Further dense matching performance. Images are resized to 640 640 pixels. FlyingThings3D 3px 1px EPE AerialMegaDepth MapFree 5px EPE 1px 3px 5px EPE 1px 3px 5px 5.68 1.33 0. 78.0 83.4 89.4 86.6 93.9 95.2 89.2 96.1 96.8 25.05 17.44 4.12 39.0 29.3 55.9 65.0 61.6 81. 73.9 73.8 87.9 8.55 3.59 2.03 45.8 31.6 55.4 72.3 66.7 84.9 80.9 81.7 92.7 Method RoMa UFM RoMa v2 Method RoMa UFM RoMa v2 Table 8. Runtime and memory. Benchmarking on 640 640 images with batch size of 8 on an H200. RoMa v2 is 1.7 faster than RoMa with similar memory footprint. indicates the custom CUDA kernel for the local correlation operation. Method Throughput (pairs/s) Mem. (GB) UFM RoMa RoMa v2 (w/o K) RoMa v2 (w/ K) We use 644 644 for RoMa and UFM due to patch size 14. 43.0 18.5 30.3 30.9 16.2 4.7 5.6 4.8 Table 9. SotA comparison on the WxBS [27] and SatAst benchmarks. mAA and AUC at 10px respectively (higher is better). Method WxBs (mAA@10px) SatAst (AUC@10px) RoMa UFM RoMa v2 60.8 42.3 55.4 23.5 1.8 37.0 correspondences for each pair. Further, we include 90 degree rotated copies of the satellite images, yielding total of 156 image pairs. Given estimated correspondences from model, we use RANSAC to obtain homography and compute AUC@10px of the reprojection error of the annotated ground-truth correspondences using this homography. SatAst is difficult, the most challenging aspects being i) satellite images are OOD for most matchers (including RoMa v2), ii) large scale changes and iii) large in-plane rotations. We compare RoMa v2 against previous dense methods and present results in Table 9. More information about the benchmark is found in the supplementary material. Table 10. Impact of predictive covariance on Hypersim [32]. Measured in AUC (higher is better). We use the predicted covariance to weight the residuals in the model refinement. Method AUC@ RoMa v2 (w/o Σ1) RoMa v2 (w/ Σ1 Refine) RoMa v2 (w/ Σ1 RANSAC + Refine) 1 54.9 75.8 76. 3 79.5 89.0 89.3 5 85.9 92.6 92.8 Figure 9. Qualitative example of covariances. We plot the predicted covariance for randomly sampled keypoints. In the right image, we have applied linear kernel to simulate motion blur, yielding larger covariances, especially in the blur direction. residuals. First, only as post-processing, refining the output of classic point-based RANSAC. Second, we compare with using it to reweight the residuals used for scoring inside RANSAC. For the experiment we consider image pairs sampled from the HyperSim [32] dataset. In Table 10 we show that we gain significant improvements on 3D pose error metrics. In particular, we improve by 20 points on AUC@1. Further experimental details can be found in the supplementary material. In Figure 9 we show qualitative example of the predicted covariances. 4.6. Covariance Estimate 5. Limitations and Future Work While most robust pose estimation pipelines assume identically distributed residuals, our predictive covariance can be used to reweight residuals. To highlight the usefulness, we perform an experiment leveraging the covariance-weighted Compared to RoMa, our model is slightly less robust to extreme changes in modality, such as in WxBS. However, we are significantly more robust to these changes than UFM, as indicated by Table 9. Exploring the trade-offs between 8 generalization and maximizing performance is an interesting direction for future work. 6. Conclusion We have introduced RoMa v2, new dense feature matcher capable of matching harder pairs, with better (i.e., more precise) predictions, and with faster runtime than its predecessor, leading to denser matches."
        },
        {
            "title": "Acknowledgements",
            "content": "This work was supported by the Wallenberg Artificial Intelligence, Autonomous Systems and Software Program (WASP), funded by the Knut and Alice Wallenberg Foundation, and by the strategic research environment ELLIIT, funded by the Swedish government. The computational resources were provided by the National Academic Infrastructure for Supercomputing in Sweden (NAISS) at C3SE, partially funded by the Swedish Research Council through grant agreement no. 2022-06725, and by the Berzelius resource, provided by the Knut and Alice Wallenberg Foundation at the National Supercomputer Centre."
        },
        {
            "title": "References",
            "content": "[1] Eduardo Arnold, Jamie Wynn, Sara Vicente, Guillermo Garcia-Hernando, Aron Monszpart, Victor Prisacariu, Daniyar Turmukhambetov, and Eric Brachmann. Map-free visual relocalization: Metric pose relative to single image. In European Conf. Computer Vision (ECCV), 2022. 6 [2] Jonathan Barron. general and adaptive robust loss function. In IEEE Conf. Computer Vision and Pattern Recognition (CVPR), 2019. 5 [3] Gabriele Berton, Gabriele Goletto, Gabriele Trivigno, Alex Stoken, Barbara Caputo, and Carlo Masone. Earthmatch: Iterative coregistration for fine-grained localization of astronaut photography. In IEEE Conf. Computer Vision and Pattern Recognition (CVPR), 2024. 7, 2 [4] Gabriele Berton, Alex Stoken, Barbara Caputo, and Carlo Masone. Earthloc: Astronaut photography localization by indexing earth from space. In IEEE Conf. Computer Vision and Pattern Recognition (CVPR), 2024. [5] Georg Bokman, Johan Edstedt, Michael Felsberg, and Fredrik Kahl. Steerers: framework for rotation equivariant In IEEE Conf. Computer Vision and keypoint descriptors. Pattern Recognition (CVPR), 2024. 7 [6] Yohann Cabon, Naila Murray, and Martin Humenberger. Virtual kitti 2. arXiv preprint arXiv:2001.10773, 2020. 6 [7] Holger Caesar, Varun Bankiti, Alex Lang, Sourabh Vora, Venice Erin Liong, Qiang Xu, Anush Krishnan, Yu Pan, Giancarlo Baldan, and Oscar Beijbom. nuscenes: multimodal dataset for autonomous driving. In IEEE Conf. Computer Vision and Pattern Recognition (CVPR), 2020. 6 [8] Wojciech Chojnacki, Michael J. Brooks, Anton Van Den Hengel, and Darren Gawley. On the fitting of surfaces to data with covariances. IEEE Trans. Pattern Analysis and Machine Intelligence (T-PAMI), 22(11):12941303, 2000. 3 [9] Angela Dai, Angel Chang, Manolis Savva, Maciej Halber, Thomas Funkhouser, and Matthias Nießner. Scannet: Richly-annotated 3d reconstructions of indoor scenes. In IEEE Conf. Computer Vision and Pattern Recognition (CVPR), 2017. 1, 7 [10] Siyan Dong, Shuzhe Wang, Shaohui Liu, Lulu Cai, Qingnan Fan, Juho Kannala, and Yanchao Yang. Reloc3r: Large-scale training of relative camera pose regression for generalizable, fast, and accurate visual localization. In IEEE Conf. Computer Vision and Pattern Recognition (CVPR), 2025. 7 [11] Johan Edstedt, Ioannis Athanasiadis, Marten Wadenback, and Michael Felsberg. DKM: Dense kernelized feature matching for geometry estimation. In IEEE Conf. Computer Vision and Pattern Recognition (CVPR), 2023. 1, 3, 7 [12] Johan Edstedt, Qiyu Sun, Georg Bokman, Marten Wadenback, and Michael Felsberg. RoMa: Robust dense feature matching. In IEEE Conf. Computer Vision and Pattern Recognition (CVPR), 2024. 1, 3, 5, 6, 7 [13] Adrien Gaidon, Qiao Wang, Yohann Cabon, and Eleonora Vig. Virtual worlds as proxy for multi-object tracking analysis. In IEEE Conf. Computer Vision and Pattern Recognition (CVPR), 2016. 6 [14] Richard Hartley and Andrew Zisserman. Multiple view geometry in computer vision. Cambridge university press, 2003. 3 [15] Addison Howard, Eduard Trulls, Kwang Moo Yi, Dmitry Mishkin, Sohier Dane, and Yuhe Jin. Image matching challenge 2022, 2022. 1 [16] Eddy Ilg, Ozgun Cicek, Silvio Galesso, Aaron Klein, Osama Makansi, Frank Hutter, and Thomas Brox. Uncertainty estimates and multi-hypotheses networks for optical flow. In European Conf. Computer Vision (ECCV), 2018. [17] Pavel Izmailov, Dmitrii Podoprikhin, Timur Garipov, Dmitry Vetrov, and Andrew Gordon Wilson. Averaging weights arXiv leads to wider optima and better generalization. preprint arXiv:1803.05407, 2018. 5 [18] Nikhil Keetha, Norman Muller, Johannes Schonberger, Lorenzo Porzi, Yuchen Zhang, Tobias Fischer, Arno Knapitsch, Duncan Zauss, Ethan Weber, Nelson Antunes, Jonathon Luiten, Manuel Lopez-Antequera, Samuel Rota Bul`o, Christian Richardt, Deva Ramanan, Sebastian Scherer, and Peter Kontschieder. MapAnything: Universal feedforward metric 3D reconstruction, 2025. arXiv preprint arXiv:2509.13414. 3 [19] Dmytro Kotovenko, Olga Grebenkova, and Bjorn Ommer. Edgs: Eliminating densification for efficient convergence of 3dgs. arXiv preprint arXiv:2504.13204, 2025. 1 [20] JongMin Lee and Sungjoo Yoo. Dense-sfm: Structure from motion with dense consistent matching. In IEEE Conf. Computer Vision and Pattern Recognition (CVPR), 2025. 1 [21] Vincent Leroy, Yohann Cabon, and Jerˆome Revaud. Grounding image matching in 3d with mast3r. In European Conf. Computer Vision (ECCV), 2024. 3, 7 [22] Zhengqi Li and Noah Snavely. Megadepth: Learning singleview depth prediction from internet photos. In IEEE Conf. Computer Vision and Pattern Recognition (CVPR), 2018. 1, 2, 6, 7 [23] Philipp Lindenberger, Paul-Edouard Sarlin, and Marc Pollefeys. LightGlue: Local Feature Matching at Light Speed. In IEEE Intl Conf. Computer Vision (ICCV), 2023. 2, 7 [24] Thibaut Loiseau and Guillaume Bourmaud. Rubik: structured benchmark for image matching across geometric challenges. In IEEE Conf. Computer Vision and Pattern Recognition (CVPR), 2025. 2 Opensfm. https : / / github . com / [25] Mapillary. mapillary/OpenSfM, 2014. 1 [26] Nikolaus Mayer, Eddy Ilg, Philip Hausser, Philipp Fischer, Daniel Cremers, Alexey Dosovitskiy, and Thomas Brox. large dataset to train convolutional networks for disparity, optical flow, and scene flow estimation. In IEEE Conf. Computer Vision and Pattern Recognition (CVPR), 2016. 6 [27] Dmytro Mishkin, Jiri Matas, Michal Perdoch, and Karel In Lenc. WxBS: Wide baseline stereo generalizations. British Machine Vision Conference (BMVC), 2015. 1, 7, [28] Maxime Oquab, Timothee Darcet, Theo Moutakanni, Huy V. Vo, Marc Szafraniec, Vasil Khalidov, Pierre Fernandez, Daniel Haziza, Francisco Massa, Alaaeldin El-Nouby, Russell Howes, Po-Yao Huang, Hu Xu, Vasu Sharma, ShangWen Li, Wojciech Galuba, Mike Rabbat, Mido Assran, Nicolas Ballas, Gabriel Synnaeve, Ishan Misra, Herve Jegou, Julien Mairal, Patrick Labatut, Armand Joulin, and Piotr Bojanowski. DINOv2: Learning robust visual features without supervision. In Intl Conf. Learning Representations (ICLR), 2023. 3 [29] Vojtech Panek, Qunjie Zhou, Yaqing Ding, Sergio Agostinho, Zuzana Kukelova, Torsten Sattler, and Laura Leal-Taixe. guide to structureless visual localization. arXiv preprint arXiv:2504.17636, 2025. 1 [30] Rene Ranftl, Alexey Bochkovskiy, and Vladlen Koltun. Vision transformers for dense prediction. In IEEE Conf. Computer Vision and Pattern Recognition (CVPR), 2021. 4, 1 [31] Carl Edward Rasmussen and Christopher K. I. Williams. Gaussian Processes for Machine Learning (Adaptive Computation and Machine Learning). The MIT Press, 2005. 3 [32] Mike Roberts, Jason Ramapuram, Anurag Ranjan, Atulit Kumar, Miguel Angel Bautista, Nathan Paczan, Russ Webb, and Joshua M. Susskind. Hypersim: photorealistic synthetic dataset for holistic indoor scene understanding. In IEEE Intl Conf. Computer Vision (ICCV), 2021. 4, 6, 8, 2 [33] Paul-Edouard Sarlin, Daniel DeTone, Tomasz Malisiewicz, and Andrew Rabinovich. Superglue: Learning feature matching with graph neural networks. In IEEE Conf. Computer Vision and Pattern Recognition (CVPR), 2020. 1, 2, 7 [34] Torsten Sattler, Will Maddern, Carl Toft, Akihiko Torii, Lars Hammarstrand, Erik Stenborg, Daniel Safari, Masatoshi Okutomi, Marc Pollefeys, Josef Sivic, Fredrik Kahl, and Tomas Pajdla. Benchmarking 6dof outdoor visual localization in changing conditions. In IEEE Conf. Computer Vision and Pattern Recognition (CVPR), 2018. 10 [35] Philipp Schroppel, Jan Bechtold, Artemij Amiranashvili, and Thomas Brox. benchmark and baseline for robust multiview depth estimation. In Intl Conf. 3D Vision (3DV), 2022. 5 [36] Johannes Lutz Schonberger and Jan-Michael Frahm. Structure-from-Motion Revisited. In IEEE Conf. Computer Vision and Pattern Recognition (CVPR), 2016. 1, 3 [37] Oriane Simeoni, Huy V. Vo, Maximilian Seitzer, Federico Baldassarre, Maxime Oquab, Cijo Jose, Vasil Khalidov, Marc Szafraniec, Seungeun Yi, Michael Ramamonjisoa, Francisco Massa, Daniel Haziza, Luca Wehrstedt, Jianyuan Wang, Timothee Darcet, Theo Moutakanni, Leonel Sentana, Claire Roberts, Andrea Vedaldi, Jamie Tolan, John Brandt, Camille Couprie, Julien Mairal, Herve Jegou, Patrick LaarXiv preprint batut, and Piotr Bojanowski. Dinov3. arXiv:2508.10104, 2025. 3, 4, 6 [38] Noah Snavely, Steven Seitz, and Richard Szeliski. Modeling the world from internet photo collections. Intl J. Computer Vision (IJCV), 80(2), 2008. 1, 3 [39] Alex Stoken and Kenton Fisher. Find my astronaut photo: Automated localization and georectification of astronaut In IEEE Conf. Computer Vision and Pattern photography. Recognition (CVPR) Workshops, 2023. 7, [40] Jianlin Su, Yu Lu, Shengfeng Pan, Ahmed Murtadha, Bo Wen, and Yunfeng Liu. Roformer: Enhanced transformer with rotary position embedding, 2023. 6 [41] Jiaming Sun, Zehong Shen, Yuang Wang, Hujun Bao, and Xiaowei Zhou. LoFTR: Detector-free local feature matching with transformers. In IEEE Conf. Computer Vision and Pattern Recognition (CVPR), 2021. 1, 2, 3, 7 [42] Hajime Taira, Masatoshi Okutomi, Torsten Sattler, Mircea Cimpoi, Marc Pollefeys, Josef Sivic, Tomas Pajdla, and Akihiko Torii. Indoor visual localization with dense matching and view synthesis. In IEEE Conf. Computer Vision and Pattern Recognition (CVPR), 2018. 1 Inloc: [43] Fabio Tosi, Yiyi Liao, Carolin Schmitt, and Andreas Geiger. Smd-nets: Stereo mixture density networks. In Conference on Computer Vision and Pattern Recognition (CVPR), 2021. 6 [44] Prune Truong, Martin Danelljan, Radu Timofte, and Luc Van Gool. PDC-Net+: Enhanced Probabilistic Dense CorIEEE Trans. Pattern Analysis and respondence Network. Machine Intelligence (T-PAMI), 2023. [45] Khiem Vuong, Anurag Ghosh, Deva Ramanan, Srinivasa Narasimhan, and Shubham Tulsiani. Aerialmegadepth: Learning aerial-ground reconstruction and view synthesis. In IEEE Conf. Computer Vision and Pattern Recognition (CVPR), 2025. 6 [46] Jianyuan Wang, Minghao Chen, Nikita Karaev, Andrea Vedaldi, Christian Rupprecht, and David Novotny. Vggt: Visual geometry grounded transformer. In IEEE Conf. Computer Vision and Pattern Recognition (CVPR), 2025. 3, 7, 5 [47] Shuzhe Wang, Vincent Leroy, Yohann Cabon, Boris Chidlovskii, and Jerome Revaud. Dust3r: Geometric 3d vision made easy. In IEEE Conf. Computer Vision and Pattern Recognition (CVPR), 2024. 1, 3 [48] Wenshan Wang, Delong Zhu, Xiangwei Wang, Yaoyu Hu, Yuheng Qiu, Chen Wang, Yafei Hu, Ashish Kapoor, and Sebastian Scherer. Tartanair: dataset to push the limits of visual slam. In IEEE/RSJ Intl Conf. Intelligent Robots and Systems (IROS), 2020. 6 [49] Yao Yao, Zixin Luo, Shiwei Li, Jingyang Zhang, Yufan Ren, Lei Zhou, Tian Fang, and Long Quan. Blendedmvs: large-scale dataset for generalized multi-view stereo networks. In IEEE Conf. Computer Vision and Pattern Recognition (CVPR), 2020. 6 [50] Chandan Yeshwanth, Yueh-Cheng Liu, Matthias Nießner, and Angela Dai. Scannet++: high-fidelity dataset of 3d indoor scenes. In IEEE Intl Conf. Computer Vision (ICCV), 2023. [51] Zhichao Yin, Trevor Darrell, and Fisher Yu. Hierarchical discrete distribution decomposition for match density estimation. In IEEE Conf. Computer Vision and Pattern Recognition (CVPR), 2019. 5 [52] Yuchen Zhang, Nikhil Keetha, Chenwei Lyu, Bhuvan Jhamb, Yutian Chen, Yuheng Qiu, Jay Karhade, Shreyas Jha, Yaoyu Hu, Deva Ramanan, Sebastian Scherer, and Wenshan Wang. Ufm: simple path towards unified dense correspondence In Advances in Neural Information Processing with flow. Systems (NeurIPS), 2025. 2, 3, 5, 6, 7 [53] Jie Zhao, Xin Chen, Yongsheng Yuan, Michael Felsberg, Dong Wang, and Huchuan Lu. Efficient motion prompt learning for robust visual tracking. In Intl Conf. Machine learning (ICML), 2025. 6 [54] Xiaoming Zhao, Xingming Wu, Weihai Chen, Peter C. Y. Chen, Qingsong Xu, and Zhengguo Li. Aliked: lighter keypoint and descriptor extraction network via deformable IEEE Transactions on Instrumentation & transformation. Measurement, 72, 2023. 5 [55] Huizhong Zhou, Benjamin Ummenhofer, and Thomas Brox. Deeptam: Deep tracking and mapping. In European Conf. Computer Vision (ECCV), 2018. 5 RoMa v2: Harder Better Faster Denser Feature Matching"
        },
        {
            "title": "Supplementary Material",
            "content": "A. Architectural Details Here we give further details on the exact dimensions of inputs and outputs of the different components of our model. Matcher: The matcher takes in list of features from the DINOv3 ViT-L backbone, in our implementation specifically layers 11 and 17, each have dimension DDINO = 1024, and which we denote as {11,17}. These features are concatenated into 2048-dimensional feature, which is linearly projected into 768-dimensional subspace as {11,17}, f = (f = (f 11 11 17) R768, R7682048 17) R768, R768 (9) (10) The projected features from image and image are then stacked and fed into an alternating Attention Multiview Transformer of ViT-B architecture (we use standard implementation with dim=768, depth=12, num heads=12, ffn ratio=4, and do not employ LayerScale, we however retain the 1024 output dim through linear map to conform to ViT-L) as (zA, zB) = mθ(f A, B) (RM 1024, RN 1024) (11) This Transformer alternates between global Attention, processing both frames jointly without any positional encoding, and frame-wise Attention using normalized Axial RoPE (as in DINOv3). The output of mθ is used to construct the similarity matrix RM as Smn = exp(1/τ cossim(zA m, zB )) (12) xy where τ = 1/10 is the temperature following RoMa, and cossim denotes cosine similarity, i.e., cossim(x, y) = y where is the dot product. Using this similarity matrix, we compute so-called match embeddings (following the nomenclature of RoMa) as = SmχB R1024, χA(cid:55)B = cos(2πωW xB (13) ) R1024, where χB xB R2 is the pixel-coordinate of patch in image B, ω = 1 (as discussed in the main text), and R5122 is non-learnable matrix with elements drawn from (0, 1). We combine features into input for DPT [30] head as ) sin(2πωW xB {f 11, 11, 17 + χA(cid:55)B + zA, 17 + χA(cid:55)B + zA}, (14) where we set the finest resolution to quarter of the original image size. We use scratch dimension of 256 and out dimensions of [256, 512, 1024, 1024] for strides [4, 8, 16, 32] respectively for the DPT head. The final prediction is made at stride 4. Refiners: We use modified version of the refiners proposed in DKM and RoMa [11, 12]. In particular, we retain only the refiners at stride [4, 2, 1], due to matching at stride 4. This has the effect of making the refinement significantly cheaper, as we also only have to extract features from the VGG19 backbone until stride 4, compared to RoMa and DKM which require features and refinement from stride 16. We denote the fine features as φ4 4 4 192, φ2 2 2 48, φ1 RHW 12, (15) where the dimensions come from the raw VGG19 features correspondsizes ing maxpool) projected with linear R192256, R48128, R1264. Refiners at stride take input of the form. the layers of (extracted before right φB φA (WA(cid:55)B) gi(WA(cid:55)B xA) local corr(φA , φB , WA(cid:55)B, ki) (16) (17) where at each pixel xa local corr uses the previous warp to construct ki ki local correlation around WA(cid:55)B(xA), and gi are linear maps. We use [k4, k2, k1] = [7, 3, 0 (no corr)], g4 = R792, g2 = R232, g1 = R82. The concatenation of all these features sum up to 512, 128, 32 respectively for strides 4, 2, 1, which are intentionally powers of two, as this slightly increases inference speed. The internals are as in DKM and RoMa, that is, 8 layers each consisting of 5 5 depthwise convolution, followed by BatchNorm, ReLU, and 1 1 pointwise convolution. B. Further Details on Datasets MegaDepth and AerialMegaDepth: We follow the setup in RoMa [12], which is the following. For each scene, directional overlaps are first computed using the number of shared 3D tracks divided by the number of observed tracks, giving number between 0 and 1. Up to 200000 pairs are selected from each scene by randomly sampling up to 100000 pairs with overlap > 0.01, and up to 100000 pairs with overlap > 0.35. Different from other datasets, sampling is not done uniformly over scenes. Rather, sampling is done over pairs, but pair sampling likelihood is downweighted by the number of pairs in the scene to the power of 0.75. Note that if the power had been 1 this would be equivalent to uniform sampling. MapFree We run COLMAPs MVS on all scenes with default settings, giving us per image depth maps. Like MegaDepth we compute overlaps as the directional percentage of shared 3D tracks between images. For training we sample pairs with overlap > 0.01 uniformly over the scenes, using only seq0 per-scene. ScanNet++ v2: We train on the nvs sem train split which consists of 856 indoor scenes from which we use the DSLR images. For each scene we render image aligned depth maps from the scene mesh, which is derived from Faro Focus Premium laser scanner. We compute the overlap pairs of images as the geometric mean of their respective directional depth map overlaps in 512 512 resolution. We use threshold of 0.2 for the minimum required overlap. For each scene we compute 10000 pairs that fulfill the overlap threshold, and use them for training. This gives us 8 106 total pairs. TartanAir V2: We follow the setup in UFM and use their TA-WB pairs for training, where we like UFM leave out the OldScandinavia, Sewerage, Supermarket, DesertGasStation, and PolarSciFi scenes for test. For further details about the pair construction, see Zhang et al. [52]. BlendedMVS: We follow MapAnything and exclude the scenes: - 5692a4c2adafac1f14201821, - 5864a935712e2761469111b4, - 59f87d0bfa6280566fb38c9a, - 58a44463156b87103d3ed45e, - 5c2b3ed5e611832e8aed46bf, - 5bf03590d4392319481971dc, - 00000000000000000000001a, - 00000000000000000000000c, - 000000000000000000000000. We train on all other scenes. We use pairs with directional overlap (computed from the depth maps) larger than 0.05. Hypersim: We train on scenes with index < 50, and validate on scenes with index 50. We sample pairs with unidirectional overlap (based on depth maps) 0.2. FlyingThings3D: We use the official TRAIN TEST split and train on both the clean and final-pass images, converting the provided optical flows into warps. UnrealStereo4K: We train on all scenes and use the left/right images with their disparities, which we convert to warps through the disparity/depth inverse relation. Virtual KITTI 2: We train on all scenes, using subsequent frames with the same condition and camera, and deriving the warp from the provided optical flow. During training we randomly draw conditions (weather conditions and camera rig position), and using either left or right stereo camera. 0.1, multiplicative brightness (ratio between [1/1.5, 1.5]), and hue jitter ([15, 15] in the HSV parameterization). For MegaDepth and AerialMegaDepth we additionally follow RoMa and translate the image randomly in the range {32, ..., 32} in both rows and columns. Visualization of Training Batch: We visualize randomly drawn batch in from the training data in Figure 10, in order to give qualitative understanding of the type of pairs RoMa v2 is trained on. Overlap/Covisibility Computation: For depth-based datasets (all datasets except FlyingThings3D, UnrealStereo4k, and Virtual KITTI 2) we use depth consistency to compute pixel-wise covisibility. We say that the depth is consistent if zB(xA(cid:55)B) zA(cid:55)B zB(xA(cid:55)B) < τ = 0.05, (18) where xA(cid:55)B is the mapping of the pixel-coordinate xA into as xA(cid:55)B B(RA(cid:55)B(K A)1xA + t) and zA(cid:55)B = (K B(RA(cid:55)B(K A)1xA + t))3 is the corresponding depth. For flow-based datasets, we measure the warp cycle consistency as (cid:13)WB(cid:55)A(WA(cid:55)B(xA)) xA(cid:13) (cid:13) at resolution of 640 640. (cid:13) < 5 103 1.6 px (19) D. Further Details on Benchmarks SatAst: We create new matching benchmark called SatAst (Satellite, Astronaut), that uses images taken by astronauts from the international space station and satellite images. We take 39 pairs of corresponding images from EarthMatch [3] (which they in turn took from AIMS [39]). The pairs in EarthMatch were obtained by retrieving ten satellite images from large database for each given astronaut image. For our benchmark we only select image pairs that are correctly matching (confirmed by visual inspection). We also exclude images with extreme cloud occlusions as well as images where we were not able to accurately annotate correspondences that agree on homography."
        },
        {
            "title": "We annotate corresponding points in the image pairs in",
            "content": "an iterative fashion as illustrated in Figure 11. To get sense of how good the annotations are, we estimate homographies from them and calculate the reprojection error from mapping the points from the astronaut image through the homography to the satellite image. The resulting errors are plotted in Figure 12. E. Further Details on Predictive Covariance C. Further Details on Training Data"
        },
        {
            "title": "Experiment",
            "content": "Augmentations: Besides using different aspect ratios, we additionally employ light data augmentation. Specifically, we use horizontal flipping, grayscale with probability We create benchmark out of 1500 pairs from validation scenes the HyperSim [32] dataset, where pairs with < 0.2 overlap are discarded. 2 Figure 10. Visualization of training batch. Our data mixures is diverse and challenging, covering many types of scenes. Since RoMa v2 predicts only the forward covariance (and the residuals are two-sided), we approximate the full 4 4 covariance matrix of the matches by block diagonal matrix where for each drawn correspondence pair xA, xB the covariance of points in IA are approximated by sampling the backwards covariance as We optimize the covariance weighted Sampson error [8], F12xA2 ΣA (xT BF xA) + (F )12xB2 ΣB (21) (Σ1)A(xA) (Σ1)B(cid:55)A(WA(cid:55)B(xA)). (20) Σ = uT Σu. For the robust estimation experiwhere u2 ment, we similarly use the residual inside the MSAC scoring, inside standard LO-RANSAC. 3 Figure 11. Annotation of correspondences for SatAst: 1) We annotate four initial approximate correspondences. 2) We warp the satellite image using the homography obtained from the previous step and annotate ten accurate correspondences. 3) Visualization of the warp obtained by estimating homography from the ten accurate annotations. 4) The ten accurate correspondences visualized in the original images, where we score the homographies obtained from the dense matchers. Step 2) is sometimes repeated several times until warp that is deemed good enough is obtained. c n e C 250 200 150 100 0 01 12 23 Pixel Error 35 5 Figure 12. Accuracy of annotations on SatAst: histogram over the reprojection errors of the 390 annotated correspondences in SatAst according to homographies estimated from the ten annnotations in each image. The image resolution of the satellite images is 3072 3072, so an error of 10 pixels is around 0.3% of the image width. F. Relative Pose Estimation VGGT: For evaluating VGGT on MegaDepth-1500, we follow the evaluation outlined by Wang et al. [46]. In particular, we sample 1024 keypoints using ALIKED [54] and use as query points in the tracking head. We try different confidence and covisibility thresholds. We settle for 0.1 for both. UFM: We follow the same sampling as for RoMa v2 and RoMa, using bidirectional warps and balanced sampling. However, as UFM only supports fixed resolution of = 420 = 560 we do not use upsampling. G. Bias In AerialMegaDepth We observe that RoMa v2 sometimes produces estimates overlaps in textureless sky regions, as demonstrated in Figure 13 We believe that this is due to AerialMegaDepth sometimes propagating depths from the mesh to sky pixels, as illustrated in Figure 14. Figure 13. Visualization of RoMa v2 warp. Note that the model puts some confidence erroneously in sky pixels (see top-left). This may be due to bias stemming from AerialMegaDepth. Figure 14. Spurious depth estimates in AerialMegaDepth. Depth from the scene leaks into the sky, causing some skypixels to be multi-view consistent. This possibly leaks into the warp estimate of RoMa v2."
        }
    ],
    "affiliations": [
        "Centre for Mathematical Sciences, Lund University",
        "Chalmers University of Technology",
        "Linkoping University",
        "University of Amsterdam"
    ]
}