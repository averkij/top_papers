{
    "paper_title": "TensorBLEU: Vectorized GPU-based BLEU Score Implementation for Per-Sentence In-Training Evaluation",
    "authors": [
        "Adam Filipek"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Modern natural language processing models have achieved unprecedented scale, yet the tools for their evaluation often remain a computational bottleneck, limiting the pace of research. This is particularly acute for in-training evaluation metrics, such as per-sentence reward signals in Reinforcement Learning, which must operate efficiently on batches of token IDs directly on the GPU. In this paper, we introduce TensorBLEU, a novel implementation of the BLEU metric designed from the ground up for this specific use case. Our approach is fully vectorized for GPU-accelerated, per-sentence computation within PyTorch and introduces a memory-efficient counting mechanism. By creating a compact, batch-specific dictionary of n-grams using \\texttt{torch.unique}, our method avoids the prohibitive memory costs of traditional hashing-based vectorization, making it practical for large-vocabulary models. We benchmark TensorBLEU against NLTK, the standard library for token-ID-based BLEU calculation on the CPU. Experiments show that TensorBLEU provides speedups of over 13x on consumer-grade GPUs (NVIDIA T4) and exceeding 40x on data-center-class hardware (NVIDIA A100). This performance transforms a significant bottleneck into a negligible part of the training loop. By clearly defining its role as a \"Token-ID BLEU\" for development purposes and open-sourcing our implementation, we provide a powerful tool for accelerating research in areas like RL-based model fine-tuning."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 7 ] . [ 1 5 8 4 5 0 . 0 1 5 2 : r TensorBLEU: Vectorized GPU-based BLEU Score Implementation for Per-Sentence In-Training Evaluation Adam Filipek (adamfilipek@rxai.dev) Reactive AI (https://rxai.dev) October 2025 Abstract Modern natural language processing models have achieved unprecedented scale, yet the tools for their evaluation often remain computational bottleneck, limiting the pace of research. This is particularly acute for in-training evaluation metrics, such as per-sentence reward signals in Reinforcement Learning, which must operate efficiently on batches of token IDs directly on the GPU. In this paper, we introduce TensorBLEU, novel implementation of the BLEU metric designed from the ground up for this specific use case. Our approach is fully vectorized for GPU-accelerated, per-sentence computation within PyTorch and introduces memory-efficient counting mechanism. By creating compact, batch-specific dictionary of n-grams using torch.unique, our method avoids the prohibitive memory costs of traditional hashing-based vectorization, making it practical for large-vocabulary models. We benchmark TensorBLEU against NLTK, the standard library for tokenID-based BLEU calculation on the CPU. Experiments show that TensorBLEU provides speedups of over 13x on consumer-grade GPUs (NVIDIA T4) and exceeding 40x on data-center-class hardware (NVIDIA A100). This performance transforms significant bottleneck into negligible part of the training loop. By clearly defining its role as Token-ID BLEU for development purposes and open-sourcing our implementation, we provide powerful tool for accelerating research in areas like RL-based model fine-tuning."
        },
        {
            "title": "1 Introduction",
            "content": "The advancement in Natural Language Processing (NLP) has been driven by the scaling of neural architectures and datasets. However, while models have become exponentially more powerful, the tools for their evaluation have often failed to keep pace. Metrics that cannot efficiently process batches of data in parallel on the GPU create an unnecessary bottleneck related to data transfer and sequential computation. This problem is most critical for applications that require metric to be computed repeatedly inside the training loop. prime example is Reinforcement Learning (RL) for fine-tuning language models, where dense reward signal is needed for each generated sample in batch. single corpus score for the entire batch provides weak, averaged signal, whereas per-sample reward is necessary for effective policy gradient updates. slow, CPU-bound metric can dominate the computation time, making such approaches impractical. The BLEU score, standard for assessing text generation quality, is desirable candidate for such reward, but traditional implementations are ill-suited for this role [1]. While libraries like NLTK can compute BLEU from integer token IDs, they require moving tensors from GPU to CPU, converting them to lists, and processing each sample in Python loop, creating severe performance bottleneck. On the other hand, more efficient SacreBLEU [2] requires inputs in text format, so in case of RL generated tokens have to be decoded, only for reward calculation, what is another potential bottleneck. In our case, the need to implement an efficient, GPU-based BLEU calculation arose during training of our Reactive Transformer models [6]. During the memory reinforcement learning stage, BLEU 1 calculation on the CPU was combined with cosine similarity calculation on the GPU, which required continuous data copying between devices. Our Contribution: TensorBLEU In this paper, we present TensorBLEU, re-architected implementation of the BLEU metric designed specifically for batched, per-sentence, vectorized computation on token IDs within the PyTorch environment. Our work makes two key contributions: 1. Memory-Efficient Vectorized Algorithm for Per-Sentence BLEU: We introduce novel method for n-gram counting that avoids the memory explosion of naive vectorization. Instead of using large hash space, we use torch.unique on the n-gram tensors themselves to create compact, batch-specific dictionary. This allows for efficient, parallel per-sentence counting via batched bincount technique in memory space proportional to the number of unique n-grams in the batch, not the vocabulary size. 2. High-Performance In-Training Metric with Demonstrated Scalability: We benchmark TensorBLEU on both consumer (NVIDIA T4) and data-center (NVIDIA A100) GPUs, demonstrating speedups that scale with hardware capabilities and effectively remove the evaluation bottleneck across different research environments."
        },
        {
            "title": "2.1 The Original BLEU Metric",
            "content": "(2002) to The BLEU (Bilingual Evaluation Understudy) metric was introduced by Papineni et al. address the slow and expensive process of human evaluation for machine translation [1]. It is based on two components: modified n-gram precision and brevity penalty. Modified n-gram precision uses clipping mechanism to prevent systems from over-generating common, correct words. The brevity penalty (BP) penalizes candidate translations that are shorter than their references. The final score is the geometric mean of the precisions (typically for n=1 to 4), multiplied by the BP: BLEU = BP exp (cid:33) wn log pn (cid:32) (cid:88) n="
        },
        {
            "title": "2.2 The Need for Standardization: SacreBLEU",
            "content": "As BLEU became the dominant metric, reproducibility crisis emerged. Scores varied wildly between papers due to undisclosed differences in preprocessing and tokenization, making fair comparison impossible [2]. To solve this, Post (2018) introduced SacreBLEU, standardized implementation that manages the entire evaluation pipeline, including canonical tokenization scheme. This ensures that reported scores are comparable and reproducible, establishing SacreBLEU as the gold standard for final, publication-ready evaluation [2]."
        },
        {
            "title": "2.3 From Evaluation to Optimization: BLEU as an RL Reward",
            "content": "Standard training of sequence models via word-level cross-entropy suffers from exposure bias: the model is only trained on ground-truth prefixes, not its own, often imperfect, predictions [3]. solution is to train at the sequence level by directly optimizing metric like BLEU. Since BLEU is non-differentiable, Reinforcement Learning (RL) provides framework for this optimization. The model acts as policy, generation is sequence of actions, and the final BLEU score serves as the reward. The seminal work by Ranzato et al. (2015) introduced this sequence-level training paradigm to NLP, using the REINFORCE algorithm to directly optimize for metrics like BLEU [3]. Subsequent work, such as Li et al. (2016), successfully applied this technique to dialogue generation, demonstrating its broader 2 utility [4]. However, the computational cost of calculating the BLEU reward for every batch on the CPU has remained major barrier to the widespread adoption of this powerful technique. TensorBLEU is designed to remove this barrier."
        },
        {
            "title": "2.4 Token-ID BLEU vs. Linguistic BLEU",
            "content": "It is crucial to distinguish between two modes of BLEU calculation. Linguistic BLEU is the standard for final model evaluation (e.g., SacreBLEU). It operates on detokenized text and applies its own standardized tokenization to ensure scores are reproducible. Token-ID BLEU in contrast, operates directly on the integer outputs of models tokenizer. The n-grams are sequences of subword IDs. While unsuitable for final reporting, this metric is perfectly suited for internal, relative evaluation during the development cycle (e.g., as an RL reward), where the tokenizer is held constant. TensorBLEU is high-performance implementation of Token-ID BLEU."
        },
        {
            "title": "3 TensorBLEU: A Memory-Efficient Vectorized Implementation",
            "content": "This section details the core algorithm of TensorBLEU, which is designed to compute separate BLEU score for each candidate-reference pair in batch in fully vectorized manner, without resorting to Python loops."
        },
        {
            "title": "3.1 Vectorized n-gram Extraction with unfold",
            "content": "The first step is to extract all n-grams from the entire batch of sentences in parallel. We use the Tensor.unfold method in PyTorch. For batch of token sequences with shape (batch size, seq len), applying tensor.unfold(dimension=1, size=n, step=1) returns view of the original tensor containing all n-gram slices, with shape of (batch size, num ngrams, n). This operation is highly efficient as it avoids data copying and processes all sentences simultaneously."
        },
        {
            "title": "3.2 Memory-Efficient Counting via a Unified n-gram Dictionary",
            "content": "A naive approach to vectorizing n-gram counting involves hashing each n-gram slice into unique integer and using counting tensor of size (batch size, n), where is the vocabulary size. This leads to memory explosion for modern vocabularies. To solve this, we developed memory-efficient method that operates in compact space. The algorithm proceeds as follows: 1. Unified N-gram Collection: For given order n, we extract all valid n-grams from both the candidate and all reference sentences in the batch and flatten them into single large tensor of shape (total ngrams, n). 2. Compact Dictionary Creation: We apply torch.unique(all ngrams, dim=0, return inverse=True). This is the key step. It returns two tensors: unique ngrams: tensor containing only the unique n-grams that actually appear in the current batch. inverse indices: 1D tensor mapping each original n-gram to its new, compact ID (i.e., its index in unique ngrams). This step effectively creates batch-specific dictionary of n-grams, where the memory required is proportional to the number of unique n-grams present, not the theoretical maximum."
        },
        {
            "title": "3.3 Batched Counting and Clipping with Offset Bincounting",
            "content": "With the compact IDs from the previous step, we can now perform counting for each sentence in parallel. We use novel batched bincount technique. 1. The Offset Mechanism: For each sentence in the batch, we add unique offset, calculated as num unique ngrams, to its compact n-gram IDs. This ensures that the IDs for each sentence occupy unique, non-overlapping range. 2. Single Bincount Operation: These offset IDs are then flattened into single 1D tensor. single call to torch.bincount on this tensor computes the n-gram counts for all sentences simultaneously. The resulting flat tensor is then reshaped to (batch size, num unique ngrams). 3. Reference Counts and Clipping: The same process is applied to the reference n-grams. To obtain the final reference counts for clipping, torch.maximum operation is taken over the count tensors derived from each reference set. The final clipping is simple, vectorized torch.minimum(candidate counts, reference max counts)."
        },
        {
            "title": "3.4 Final Score Aggregation and The tensor corpus bleu Variant",
            "content": "With the clipped counts (numerators) and total candidate n-gram counts (denominators) computed for each sentence and each n-gram order, the rest of the calculation is performed element-wise across the batch dimension. The modified precisions pn, brevity penalty, and final geometric mean are assembled using standard PyTorch functions. Our implementation also includes standard smoothing methods (floor, add-k, exp) as described by Chen and Cherry (2014) [5]. In addition to the per-sentence function, we provide tensor corpus bleu variant that computes single score for the entire batch by aggregating statistics before calculating precision. The performance of this variant is nearly identical to the per-sentence version. This is because the most computationally expensive stepsn-gram extraction with unfold and the creation of the compact dictionary with torch.uniqueare performed on the entire batchs n-grams in both cases and dominate the runtime. The additional complexity in the per-sentence version (offset calculation, per-sentence aggregation) consists of lightweight arithmetic operations that are negligible on GPU. This allows researchers to obtain more granular, per-sentence reward signal at virtually no performance penalty compared to less useful corpus-level score."
        },
        {
            "title": "4.1 Correctness Verification",
            "content": "To prove that TensorBLEU correctly implements the BLEU algorithm for token IDs, we verified that it produces numerically identical results to NLTKs sentence bleu function when given the same lists of token IDs, weights, and smoothing function. Our implementation consistently achieved equivalence within the margin of floating-point precision (< 106)."
        },
        {
            "title": "4.2 Performance Benchmarking",
            "content": "Objective To quantify the speedup of TensorBLEU over the standard CPU-based method for calculating Token-ID BLEU. Reference Implementation We compare against NLTKs sentence bleu, which is the standard library for this task. The NLTK implementation is run on the CPU and involves iterating through the batch in Python loop. 4 Hardware and Data Experiments were conducted across two distinct hardware tiers: Consumer-Grade: Google Colab environment featuring an NVIDIA T4 GPU (16GB VRAM) and an Intel Xeon CPU (2 cores, 13GB RAM). Data-Center-Grade: Novita.ai cloud instance featuring an NVIDIA A100 GPU (80GB HBM2e VRAM) and 14 vCPU instance (240GB RAM). We used batches of token sequences of two lengths: 256 tokens (typical for many tasks) and 1024 tokens (representing longer-form generation). All computations for TensorBLEU were performed on the GPU using float32 precision. Variables We measured the wall-clock execution time (mean of 5 runs) while varying the batch size, using values of 16, 32, 64, 128, 256, and 512."
        },
        {
            "title": "5.1 Performance Results",
            "content": "The performance results are presented in Table 1 for the consumer-grade NVIDIA T4 GPU and Table 2 for the data-center-grade NVIDIA A100 GPU. Table 1: Performance on NVIDIA T4 GPU (mean execution time in seconds). Lower is better. Batch Size NLTK (CPU) TensorBLEU (GPU) Speedup Factor Sequence Length: 256 tokens 32 64 128 256 512 16 32 64 128 0.042s 0.079s 0.163s 0.333s 0.702s 0.011s 0.015s 0.016s 0.035s 0.085s Sequence Length: 1024 tokens 0.072s 0.131s 0.252s 0.482s 0.974s 0.011s 0.015s 0.019s 0.036s 0.084s 3.8x 5.3x 10.2x 9.5x 8.3x 6.5x 8.7x 13.3x 13.4x 11.6x Table 2: Performance on NVIDIA A100 GPU for 1024-token sequences. Lower is better. Batch Size NLTK (CPU) TensorBLEU (GPU) Speedup Factor 32 64 128 256 512 0.107s 0.200s 0.380s 0.764s 1.525s 0.009s 0.010s 0.013s 0.019s 0.041s 11.9x 20.0x 29.2x 40.2x 37.2x 5 Figure 1: Tests on T4 GPU (Colab) with 256 tokens sentences Figure 2: Tests on T4 GPU (Colab) with 1024 tokens sentences 6 Figure 3: Tests on A100 80GB GPU (and stronger CPU) with 1024 tokens sentences"
        },
        {
            "title": "5.2 Analysis of Results",
            "content": "The data clearly demonstrate the profound performance advantage of our vectorized, GPU-based approach. The NLTK implementation, being serial loop over sentences, exhibits near-linear time complexity with respect to batch size. In contrast, TensorBLEU shows sub-linear scaling, as the fixed costs of launching GPU kernels are amortized over an increasing number of parallel computations. Impact of Sequence Length Comparing the results on the T4 GPU  (Table 1)  , we observe that the speedup advantage of TensorBLEU grows with sequence length. For batch size of 128, the speedup increases from 10.2x for 256-token sequences to 13.4x for 1024-token sequences. This is because the number of n-grams to process grows significantly with sequence length, heavily penalizing the iterative CPU approach, while the parallel GPU architecture can absorb this increased workload much more efficiently. Impact of Hardware Tier The comparison between the T4  (Table 1)  and A100  (Table 2)  results for 1024-token sequences highlights the algorithms scalability. On the A100, the speedup factor reaches remarkable 40.2x at batch size of 256. This demonstrates that the algorithms design effectively leverages the superior memory bandwidth and computational power of high-end GPUs. The performance gains are super-linear, indicating that the implementation is not limited by its own logic but by the underlying hardware, which is hallmark of well-designed, scalable algorithm. The A100s massive memory bandwidth is particularly beneficial for the torch.unique operation on very large tensor of n-grams. The Bottleneck Vanishes For typical training scenario (e.g., batch size 256, 1024 tokens), TensorBLEU reduces the evaluation time from significant fraction of second (764ms on capable CPU) to just 19ms on an A100 GPU. This effectively transforms the metric calculation from potential training bottleneck into negligible overhead."
        },
        {
            "title": "6 Discussion and Future Work",
            "content": "6."
        },
        {
            "title": "Implications for NLP Research",
            "content": "The primary implication of our work is that it makes large-scale RL fine-tuning of language models using BLEU as dense reward signal computationally cheap and practical. By reducing the computation time for batch from hundreds of milliseconds to just few, TensorBLEU removes critical bottleneck that previously made such approaches prohibitively slow. This accelerates the research and development cycle, allowing for more extensive experimentation, hyperparameter sweeps, and application to larger models where every second of training time counts."
        },
        {
            "title": "6.2 Limitations and Proper Usage",
            "content": "It is essential to use TensorBLEU correctly. As Token-ID BLEU metric, its results are dependent on the tokenizer and are not directly comparable to scores from other models using different tokenizers. It is an internal development and optimization tool, ideal for measuring relative improvements during training. For final, publication-ready results that are comparable across the field, researchers must continue to use standardized, text-based tools like SacreBLEU [2], which ensure consistent tokenization and processing."
        },
        {
            "title": "6.3 Future Work",
            "content": "The core contribution of this paper extends beyond fast BLEU implementation to generalizable methodology for vectorizing n-gram-based metrics on GPUs using torch.unique as memoryefficient hashing mechanism. Based on this, we propose several directions for future work: 1. Generalizing the Technique: The vectorized counting methodology can be extended to other n-gram-based metrics like ROUGE and METEOR. Developing suite of high-performance TensorMetrics would provide the community with powerful toolkit for GPU-accelerated evaluation. 2. Integration with RL Libraries: To facilitate adoption, we plan to develop official integrations and tutorials for popular RL libraries like Hugging Faces TRL and AllenAIs RL4LMs, making it trivial for researchers to use TensorBLEU as reward function. 3. Exploring Further Optimizations: We plan to investigate the performance impact of lowerprecision data types like bfloat16 and explore the use of custom CUDA kernels for the counting mechanism to potentially achieve even greater speedups on specific hardware."
        },
        {
            "title": "7 Conclusion",
            "content": "In this paper, we presented TensorBLEU, fully vectorized, memory-efficient, and GPU-accelerated implementation of the BLEU metric designed for per-sentence, in-training evaluation on token IDs. By leveraging novel counting mechanism based on torch.unique, our method is practical for largevocabulary models and avoids the memory explosion of naive vectorization. It achieves speedups of over 13x on consumer-grade hardware and over 40x on data-center GPUs compared to the standard CPU-based NLTK implementation. This performance demonstrates excellent scalability and effectively eliminates the evaluation bottleneck for in-training use cases. By clearly defining its scope and open-sourcing the code, we provide the NLP community with critical piece of infrastructure to accelerate research in computationally intensive paradigms like Reinforcement Learning."
        },
        {
            "title": "Acknowledgements",
            "content": "The article was created in cooperation with Google Gemini 2.5 Pro [7] in Deep Research mode, which helped with the analysis of the algorithm and formatting the final version of the research paper."
        },
        {
            "title": "Code and Documentation",
            "content": "Implementation and usage documentation for TensorBLEU in both sentence and corpus modes is publicly available as Free Component in our RxLM framework (https://github.com/RxAI-dev/rxlm) as rxlm.metrics.tensorbleu module. Free Components in Reactive AI Framework License (RAFL) v1.0 are available under Apache-2.0 license terms."
        },
        {
            "title": "References",
            "content": "[1] Kishore Papineni, Salim Roukos, Todd Ward, and Wei-Jing Zhu. Bleu: method for automatic evaluation of machine translation. In Proceedings of the 40th Annual Meeting of the Association for Computational Linguistics, pages 311318, 2002. [2] Matt Post. call for clarity in reporting bleu scores. In Proceedings of the Third Conference on Machine Translation: Research Papers, pages 186191, 2018. [3] MarcAurelio Ranzato, Sumit Chopra, Michael Auli, and Wojciech Zaremba. Sequence level training with recurrent neural networks. arXiv preprint arXiv:1511.06732, 2015. [4] Jiwei Li, Will Monroe, Alan Ritter, Dan Jurafsky, Michel Galley, and Jianfeng Gao. Deep reinforcement learning for dialogue generation. In Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing, pages 11921202, 2016. [5] Boxing Chen and Colin Cherry. systematic comparison of smoothing techniques for sentence-level bleu. In Proceedings of the Ninth Workshop on Statistical Machine Translation, pages 362367, 2014. [6] Adam Filipek. Reactive Transformer (RxT) - Stateful Real-Time Processing for Event-Driven Reactive Language Models. arXiv preprint arXiv:2510.03561, 2025. [7] Google DeepMind. Gemini 2.5: Pushing the Frontier with Advanced Reasoning, Multimodality, Long Context, and Next Generation Agentic Capabilities. arXiv preprint arXiv:2507.06261, 2025."
        }
    ],
    "affiliations": [
        "Reactive AI"
    ]
}