{
    "paper_title": "ECO: Energy-Constrained Optimization with Reinforcement Learning for Humanoid Walking",
    "authors": [
        "Weidong Huang",
        "Jingwen Zhang",
        "Jiongye Li",
        "Shibowen Zhang",
        "Jiayang Wu",
        "Jiayi Wang",
        "Hangxin Liu",
        "Yaodong Yang",
        "Yao Su"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Achieving stable and energy-efficient locomotion is essential for humanoid robots to operate continuously in real-world applications. Existing MPC and RL approaches often rely on energy-related metrics embedded within a multi-objective optimization framework, which require extensive hyperparameter tuning and often result in suboptimal policies. To address these challenges, we propose ECO (Energy-Constrained Optimization), a constrained RL framework that separates energy-related metrics from rewards, reformulating them as explicit inequality constraints. This method provides a clear and interpretable physical representation of energy costs, enabling more efficient and intuitive hyperparameter tuning for improved energy efficiency. ECO introduces dedicated constraints for energy consumption and reference motion, enforced by the Lagrangian method, to achieve stable, symmetric, and energy-efficient walking for humanoid robots. We evaluated ECO against MPC, standard RL with reward shaping, and four state-of-the-art constrained RL methods. Experiments, including sim-to-sim and sim-to-real transfers on the kid-sized humanoid robot BRUCE, demonstrate that ECO significantly reduces energy consumption compared to baselines while maintaining robust walking performance. These results highlight a substantial advancement in energy-efficient humanoid locomotion. All experimental demonstrations can be found on the project website: https://sites.google.com/view/eco-humanoid."
        },
        {
            "title": "Start",
            "content": "IEEE TRANSACTIONS ON AUTOMATION SCIENCE AND ENGINEERING. PREPRINT VERSION. ACCEPTED FEB, 2026 1 ECO: Energy-Constrained Optimization with Reinforcement Learning for Humanoid Walking Weidong Huang, Jingwen Zhang, Member, IEEE, Jiongye Li, Shibowen Zhang, Jiayang Wu, Jiayi Wang, Hangxin Liu, Member, IEEE, Yaodong Yang, Member, IEEE, Yao Su, Member, IEEE 6 2 0 2 ] . [ 1 5 4 4 6 0 . 2 0 6 2 : r AbstractAchieving stable and energy-efficient locomotion is essential for humanoid robots to operate continuously in realworld applications. Existing model predictive control (MPC) and reinforcement learning (RL) approaches often rely on energyrelated metrics embedded within multi-objective optimization framework, which require extensive hyperparameter tuning and often result in suboptimal policies. To address these challenges, we propose ECO (Energy-Constrained Optimization), constrained RL framework that separates energy-related metrics from rewards, reformulating them as explicit inequality constraints. This method provides clear and interpretable physical representation of energy costs, enabling more efficient and intuitive hyperparameter tuning for improved energy efficiency. ECO introduces dedicated constraints for energy consumption and reference motion, enforced by the Lagrangian method, to achieve stable, symmetric, and energy-efficient walking for humanoid robots. We evaluated ECO against MPC, standard RL with reward shaping, and four state-of-the-art constrained RL methods. Experiments, including sim-to-sim and sim-to-real transfers on the kid-sized humanoid robot BRUCE, demonstrate that ECO significantly reduces energy consumption compared to baselines while maintaining robust walking performance. These results highlight substantial advancement in energy-efficient humanoid locomotion. All experimental demonstrations can be found on the project website: https://sites.google.com/view/eco-humanoid. Note to PractitionersTraditional MPC and RL approaches often require extensive hyperparameter tuning and frequently result in suboptimal solutions for improving energy efficiency while maintaining stable walking performance. ECO is designed to address these challenges by reformulating energy consumption as explicit inequality constraints, providing physically interpretable and intuitive approach to optimizing energy efficiency. This framework is particularly well-suited for applications prioritizing energy conservation and operational stability, such as surveillance, disaster response, and long-duration autonomous operations. Additionally, ECO generates emergent behaviors such Fig. 1: Comparison between the proposed constrained RL framework, ECO, with MPC and normal RL (PPO) baselines. It creates synergy between energy consumption and walking stability of the humanoid robots without requiring an extensive parameter-turning process, outperforming both MPC and normal RL baselines. as lighter steps and reduced body shaking, which are especially advantageous for loco-manipulation tasks by minimizing disruptions to upper-body manipulation caused by locomotion. Comparative experiments empirically offer valuable insights into constraint selection and learning setups, which may also inspire relevant ongoing research in constrained RL. Index TermsHumanoid and bipedal locomotion, constrained reinforcement learning, legged robots I. INTRODUCTION This work was supported in part by the National Natural Science Foundation of China (No. 62403064, 62403063) and Shenzhen Science and Technology Program (No. ZDCY20250901094531003). (Weidong Huang, Jingwen Zhang contributed equally to this work.) (Corresponding authors: Jingwen Zhang and Yao Su.) Weidong Huang, Jingwen Zhang, Jiongye Li, Shibowen Zhang, Jiayang Wu, Jiayi Wang, Hangxin Liu, Yao Su are with State Key Laboratory of General Artificial Intelligence, Beijing Institute for General Artificial Intelligence (BIGAI), Beijing 100080, China (e-mails: bigeasthuang@gmail.com; zhangjingwen@bigai.ai; lijiongye@bigai.ai; zhangshibowen@bigai.ai; wujiayang@bigai.ai; wangjiayi@bigai.ai; liuhx@bigai.ai; suyao@bigai.ai). Jiongye Li is also with Department of Automation, Tsinghua University, Beijing 100084, China. Shibowen Zhang is also with Department of Automation, University of Science and Technology of China, Hefei 230022, China. Jiayang Wu is also with Department of Computer Science, Harbin Institute of Technology, Harbin 150001, China. Yaodong Yang is with Institute for Artificial Intelligence and School of Artificial Intelligence, Peking University, Beijing 100871, China (e-mail: yaodong.yang@pku.edu.cn)."
        },
        {
            "title": "R ECENT advances in humanoid robotics have enabled",
            "content": "these robots to perform complex motions, such as walking, running, jumping, and even loco-manipulation [14]. Despite these achievements, energy efficiency still remains significant bottleneck for humanoid robots. Compared to their biological counterparts [5], humanoid robots require substantially more energy to perform similar tasks, limiting their operational range, endurance, and maximum payload in real-world applications [5, 6]. To improve the energy efficiency of humanoid robots, one promising approach is to leverage reinforcement learning (RL) to optimize the control policy. For instance, Proximal Policy Optimization (PPO)-based frameworks incorporate energyrelated terms into reward functions, such as minimizing joint torques, joint accelerations, and contact forces [79]. However, 2 IEEE TRANSACTIONS ON AUTOMATION SCIENCE AND ENGINEERING. PREPRINT VERSION. ACCEPTED FEB, 2026 to achieve efficient and stable locomotion, these methods often require extensive tuning on the weights of the reward terms regarding energy cost, task completion, and stability. This tuning process is non-intuitive and time-consuming, as (i) the effects of weight configurations are not straightforward, (ii) the training periodsranging from hours to daysto evaluate single coefficient choice even with parallel computation [10, 11]. Furthermore, conflicting objectives within the reward terms can result in suboptimal solutions or even convergence failures [12, 13]. For example, the learned policy might prioritize minimizing energy consumption at the cost of stability, resulting in unstable walking, or emphasize stability at the expense of significantly increased energy usage. To simplify the reward tuning process for optimizing locomotion with RL, we propose energetically efficient ECO (Energy-Constrained Optimization). ECO leverages constrained RL formualtion to separate conflicting objectives into distinct rewards and constraints [1, 14, 15]. This separation ensures that critical constraints, such as energy consumption limits, are strictly enforced, while task-related rewards, such as velocity tracking and stability can be optimized without extensive tuning of coefficients. Energy constraint thresholds are identified through physically intuitive tuning process, incrementally adjusted based on direct energy criteria using linear search algorithm. We evaluate our approach by comparing four state-of-the-art constrained RL methods and investigating various constraint settings. Our results empirically show that the proposed setup outperforms others in training stability and constraint enforcement. Through extensive experiments on both simulation and hardware platforms, we validate that ECO consistently converges to solutions that minimize energy consumption while ensuring stable and robust walking. Our main contributions can be summarized as follows: 1) We propose ECO, method designed for optimizing the energy efficiency of humanoid locomotion. Rather than balancing energy minimization with other reward termssuch as velocity tracking, motion tracking, and stabilitywe explicitly treat energy cost as constraint. This approach provides clear, interpretable physical meaning of the energy cost, enabling efficient and intuitive hyperparameter tuning focused on energy efficiency. 2) We investigate four constrained RL algorithms and also different constraint settings to train the policy. Through extensive simulations, we empirically find that the PPOLagrangian method with the current constraint setup achieves the lowest energy cost with fast and stable convergence. Additionally, instead of manually designing efficient walking, we observe that the resulting policy promotes i) extended knee movements, ii) lighter steps, and iii) reduced body shaking as shown in Fig. 1. 3) We carry out real-world validations on the Bruce robot. To the best of our knowledge, this is the first work to achieve energy-efficient humanoid walking using constrained RL on real humanoid robot. Compared to MPC and PPO baselines, ECO demonstrates approximately 6 times lower energy consumption than MPC and 2.3 times lower than PPO, setting new benchmark for energyefficient humanoid locomotion. We organize the remainder of the paper as follows. Sec. II reviews the literature related to this work. Sec. III introduces the preliminaries of RL and constrinted RL formulations. Sec. IV presents the proposed ECO framework in detail. Secs. and VI show the simulation and experiment result of ECO on BRUCE with comprehensive evaluations and compares with baseline methods. Finally, we discuss and conclude the paper in Secs. VII and VIII. II. RELATED WORK A. Energy Optimization in Legged Locomotion Improving energy efficiency directly helps to extend the endurance of legged robots and prevent actuator overheating, thereby improving the overall performance [6]. For example, passive dynamic walkers can achieve efficient and stable walking through mechanical designs without any actuation [16] or with minimal actuation [17]. However, achieving similar efficiency in more complex legged systems, such as quadrupedal and bipedal robots, remains challenge. As unified Trajectory Optimization (TO) framework has been widely used for legged locomotion [18, 19], incorporating energy-related constraints tends to be potential solution. [20] formulates the TO problem with the cost of transport (CoT) as constraint, which quantifies the energy expenditure per unit distance. However, using TO to consider complex constraints often results in high-dimensional and nonconvex landscape, which hinders real-time usage. To simplify the problem, energy constraints are approximated using the norm of joint torques and accelerations [2, 21, 22], or by encouraging specific bioinspired efficient behaviors, such as adaptive center of mass (CoM) height control and rolling contact of the stance leg in gait design [23]. In these methods, energy consumption is only implicitly considered, which often makes it compromised in favor of other task requirements, such as stability and robustness. B. Learning for Legged Locomotion Given the aforementioned issues, offline learning locomotion policy with complex rewards can be promising solution to balance energy-efficiency and robustness in real-time [2426]. [27] demonstrated sim-to-real learning of multiple bipedal gaits through periodic reward composition. Similarly, [28] extended rapid motor adaptation (RMA) for bipedal locomotion to address challenges such as payloads and slippery surfaces. Other methods also integrated model-based footstep planning with model-free RL to achieve dynamic and efficient walking [29, 30]. However, to avoid disrupting the primary learning goals, energy penalties in the reward function often act as regularizers, with minimal impact on reducing excessive joint torque usage. [31] balanced energy consumption with stability through curriculum learning with symmetric mirror loss. [32] further studied different ways to incorporate symmertry, highlighting how symmetry priors can improve gait robustness and sample efficiency without overly restricting policy exploration. [7] explored efficient quadrupedal gaits with energy-centric rewards. [3336] train control policies for humanoid locomotion using human motion HUANG et al.: ECO: ENERGY-CONSTRAINED OPTIMIZATION WITH REINFORCEMENT LEARNING FOR HUMANOID WALKING data as reference trajectories. Despite these advancements, the energy-related hyperparameters, like the reward weight or curriculum length, still present an unclear physical meaning, which intricates the weight tuning and curriculum design. The objective of the MDP is to maximize: max πθθθPΠθθθ Rpπθθθq, where Πθθθ is the neural network parameter space. 3 (3) C. Constrained Reinforcement Learning Constrained RL, or safe RL, formulates the problem as constrained Markov decision process (CMDP) [3740], using cost functions to decouple rewards from constraints. This allows robots to maximize rewards while keeping costs within physically meaningful thresholds. CPO [41] uses second-order optimization to enforce constraints iteratively but struggles with scalability in multi-constraint settings. PPO-Lagrangian (PPO-Lag) [42] enhances PPO with primal-dual updates for better constraint satisfaction and performance. Constraint manifold is defined for safe exploration leading to actions that satisfy corresponding constraints in [43, 44]. However, efficient constraint manifold computation for legged robots with floating base remains challenging. Constraint-rectified Policy Optimization (CRPO) [45] offers primal-based framework well-suited for multi-constraint problems, while Interior-point Policy Optimization (IPO) [46] employs log-barrier penalties for feasibility enforcement. [15] extended IPO with teacherstudent framework, transferring most reward terms into constraints to avoid reward scale tuning, and demonstrated robust performance on legged robots. Penalized Proximal Policy Optimization (P3O) [47] uses clamping penalty functions for constraint enforcement and has shown success in reducing violations (e.g., self-collision, joint speed limits) in quadruped locomotion [48]. Despite these advancements, these methods have only been validated on quadruped robots. Applying them to humanoid robots, especially when combining energy consumption with other constraints (e.g., self-collision), can overly restrict the search space. Given the smaller convergence region of humanoid robots compared to quadrupeds, rethinking how to select and design constraints and how to select an appropriate optimization method within the constrained RL framework to generate efficient and stable walking performance of humanoid robots remains critical challenge. III. PRELIMINARIES & PROBLEM STATEMENT A. Markov Decision Processes RL is typically modeled as Markov Decision Process (MDP) [37]: pS, A, P, R, µ, γq, (1) where and denote the state and action spaces, respectively. The functions Ppsss1sss, aaaq and Rpsss1sss, aaaq represents the transition probability and corresponding reward of moving from state sss to sss1 under action aaa. µµµpq represents the initial state distribution, and γ is the discount factor. stationary policy πθθθ governs the action probability πθθθpaaasssq for given state sss, with all policies parameterized by θθθ. The corresponding infinite-horizon reward is defined as 8ÿ ff Rpπθθθq Eπ sssµµµ γtRpssst`1ssst, aaatq . (2) t0 B. Constrained Markov Decision Processes CMDP extends the standard MDP framework by incorporating set of cost functions, referred to as general constraints. This set is defined as pCi, biqm i1, where : ˆ Ñ is associated with each cost function Ci corresponding threshold bi. The objective of the CMDP is to identify the policy that maximizes the reward while satisfying all specified constraints: max πθθθPΠC Rpπθθθq, (4) ( πθθθ Cipπθθθq ď bi, @i 1, . . . , where ΠC Πθθθ represents the set of feasible policies that satisfy all constraints and Cipπθθθq is the expected value of cost function under policy π. These constraints can take various forms, such as discounted cumulative sum, average sum, etc. [37]. Discounted sum constraints are formulated to ensure that specified quantity remains below given threshold over the long term, accounting for cumulative effects rather than instantaneous values at each time step. They are defined as: Tÿ Cipπθθθq γtCipssst`1ssst, aaatq ď bi, (5) t0 where is the episode length. This type of constraint is particularly well-suited for maintaining long-term requirements in robotic operations, such as limiting energy consumption over certain period. Average sum constraints ensure that the mean value of specific variable remains below given threshold at each time step. Formally, these constraints are expressed as: ff ff Cipπθθθq Cipssst`1ssst, aaatq ď bi. (6) 1 Tÿ t0 For instance, specific motion styles, such as symmetry during walking. these constraints can be utilized to enforce C. Constraints RL Methods 1) PPO-Lag: PPO-Lag [42] is one of the most widely used methods in constrained RL. The Lagrangian function is: Lpθθθ, λ1, . . . , λiq Rpπθθθq mÿ ` λi i1 Cipπθθθq bi , (7) where λi is the Lagrange multipliers. Using this approach, the original constrained optimization problem in Eq. (4) is reformulated as an unconstrained optimization problem: max πθθθPΠθθθ min λ1,...,λiě0 Lpθθθ, λ1, . . . , λmq. (8) This optimization process alternates between updating the primal variable πθθθ and the dual variables λi, ultimately yielding an approximate solution to the Eq. (4). IEEE TRANSACTIONS ON AUTOMATION SCIENCE AND ENGINEERING. PREPRINT VERSION. ACCEPTED FEB, 2026 Fig. 2: Overview of the training and deployment process in proposed ECO framework. The policy network, taking velocity commands and proprioception data as inputs, outputs desired joint positions at 100Hz to PD controller, which updates torque commands at 1kHz. The reward critic is trained with privileged observations. The simulator provides the reward, energy cost, and symmetry cost, which are used to compute the reward and cost returns. The policy is then updated using the Lagrangian formulation in Eq. (20) to balance rewards and costs. The trained policy is directly deployed to the real world. 2) CRPO: CRPO [45] alternates between maximizing the reward and minimizing the cost, prioritizing the reduction of costs when constraints are violated. In cases where multiple constraints are violated, single constraint is randomly selected for optimization: LCRPO 1nk0 Rpπθθθq nkÿ νi Cipπθθθq, (9) ř 1 where nk Ci ąbi is the number of violated constraints, ννν Rnk is random one-hot vector used to select violated constraint to update at each iteration. 3) IPO: IPO [46] transforms the constrained RL problem into an unconstrained one by employing logarithmic barrier function, defined as follows: LIPO Rpπθθθq mÿ i1 ` κIPO log Cipπθθθq bi , (10) where κIPO steepness of the logarithmic function. ą 0 is the fixed hyperparameter that controls the 4) P3O: P3O [47] extends the PPO framework by incorporating penalties for constraint violations into the objective function. The objective is defined as: LP3O Rpπθθθq mÿ i1 κP3O maxtJ Cipπθθθq ` ϵi, 0u, (11) where κP3O represents the fixed weight for each constrain. ff ϵi psss,aaaqbuffer γtCipssst`1ssst, aaatq bi, (12) Tÿ t0 ECO. In Sec. IV, we will provide detailed implementation insights and explain the rationale behind this choice. In Sec. V, we will compare their performance to validate this selection. IV. PROPOSED ECO FRAMEWORK In this section, we describe the proposed ECO framework, which aims to minimize energy consumption in humanoid locomotion while ensuring stable and natural leg movements. Both the control policy and reward critic are implemented as fully connected neural networks. As illustrated in Fig. 2, the control policy takes the robots proprioception and velocity commands as input and outputs the desired joint positions. We train the policy using PPO-Lagrangian [49] in IsaacGym and deploy it directly on the physical robot. The following subsections introduce the state space, action space, constraints settings, reward settings, and the formulation for constraint policy optimization. A. State Space , vcmd The input to the policy network consists of an observation history comprising Kf consecutive frames, which include the robots proprioceptive states and velocity commands, with dimensionality of S. Specifically, includes velocity commands yaw R3, clock inputs psinptq, cosptqq R2, pvcmd joint positions qqq Rn, joint velocities 9qqq Rn, body angular velocities ωωωang R3, last action aaat1 Rn, and the XY components of body Euler angles ωωωang R2, where is number of leg Degree of Freedoms (DoFs). The body yaw angle is excluded due to its drift on the real hardware. , ωcmd represents the amount by which the actual cost exceeds the constraint threshold bi, as calculated using data collected from prior interactions with the environment. In this paper, we compare these methods and ultimately select PPO-Lag as the constrained optimization approach for The state space of our reward critic includes both the inputs used by the policy network and additional privileged information, such as body linear velocity, body yaw angle, external push forces and torques, friction coefficients, body mass, phase identifiers, and contact indicators. These indicators specify the HUANG et al.: ECO: ENERGY-CONSTRAINED OPTIMIZATION WITH REINFORCEMENT LEARNING FOR HUMANOID WALKING 5 swing and stance phases of each foot, as determined by the contact scheduler. B. Action Space In this work, we actively control leg joints while the arm joints are fixed to nominal angles. The action from the policy, represented as aaa Rn, is defined as the deviations from the nominal joint positions qqqnominal [29, 50]. This action is fed into the PD controller to convert the action into the desired torque command using the following equation: ˆτττ Kppaaa ` qqqnominal qqqq ` Kdp0 9qqqq, (13) where qqq, 9qqq denote the current joint position and joint velocity. C. Constraints 1) Constraints Selection Consideration: Since minimizing energy consumption is the main focus of this paper, naive choice is to mix it with other objectives, such as adherence to task commands, behavior stability, and disturbance rejection. However, regarding energy consumption, we observed that adjusting the reward weights alone for PPO does not lead to convergence within the feasible domain (shown in Sec. V-D1). Consequently, we select energy consumption as constraint in this optimization process. Additionally, we integrate reference motion (using Mirror Loss as the reference in this paper) as constraint to encourage natural motions. Among prior works [15, 48], additional reward terms are often placed within constraints for quadrupedal robots. In Sec. V-D6, we investigate various constraint settings including the constraints used in [15, 48], and find that the current approach struggles with stable convergence as the number of constraints increases. This may be due to humanoid robots having much smaller constrained feasible domain than quadrupeds, which makes convergence more challenging in the presence of multiple constraints. Therefore, in this work, we focus on two primary constraints: energy minimization and reference motion tracking for humanoid robots. 2) Energy Constraint: Due to the challenges in accurately measuring link inertia and quantifying friction and heat dissipation losses, the overall energy consumption of robot cannot be directly measured. Therefore, in this work, the motor energy consumption, which constitutes the primary part of the total energy consumption, is selected as the evaluation metric. It can be calculated by integrating the absolute power of the motors. Since the timestep dt is fixed, the energy cost function is defined as C1pssst`1ssst, aaatq nÿ j1 τττ 9qqqj (14) to exclude dt. Here, τττ denote the torque and velocity of the j-th joint at time t. Using the discounted sum constraint from Eq. (5), the energy constraint is formulated as: and 9qqqj ff Tÿ nÿ C1pπθθθq γt τττ 9qqqj ď b1, (15) t0 where b1 is the energy threshold, estimated based on the robots speed range and walking duration. 3) Reference Motion Constraint: To enforce stability and mirror symmetry, we define mirror loss term following [31] as the cost function: C2psss1sss, aaaq sg pπθθθpsssiqq ΨapπθθθpΨopsssiqqq2, (16) where sg pq is the stop gradient operator, Ψapq and Ψopq are the mirroring functions for actions and states, respectively, and πθθθpsssiq is the mean action of the stochastic policy conditioned on the state sssi. This constraint ensures that when the observation input to the policy is mirrored, the corresponding action output is also mirrored. Practically, if the policy learns to lift the left leg at the appropriate moment, this constraint also enforces it to lift the right leg at the corresponding moment, and vice versa. This reference constraint promotes more stable and symmetric behavior within the policy. Such symmetry fosters balanced behavior without the need to prescribe motion trajectories explicitly, thereby reducing the necessity to define trajectory parameters manually. We apply the average constraint from Eq. (6) to this cost function: C2 pπθθθq 1 ř t0 πθθθpssstq ΨapπθθθpΨopssstqqq ı ď b2, (17) where b2 is the constraint threshold of reference motion. D. Rewards The aspect of reward design is not the main focus of this paper, therefore we adopt reward configuration similar to the Humanoid-Gym [9]. Since reference motion and energy constraints are directly integrated into the cost function, we have omitted the reference joint position tracking reward, joint velocity penalty reward, and joint torque penalty reward from the original setup. The total reward function is defined as ÿ Rpssst`1ssst, aaatq µi ri, (18) where the rewards ri and their corresponding scales µi will be detailed in Sec. V. E. Constrained Policy Optimization To solve the constrained optimization for energy-efficient humanoid walking, we employ the Lagrangian method to transform the original constrained objective into an unconstrained form, as introduced in Sec. III. Having two designed constraints, we formulate the problem as follows: max πθθθPΠθθθ min λ1,λ2ě0 Rpπθθθq 2ÿ ` λi Cipπθθθq bi . (19) i1 Here, C1pπθθθq and C2 pπθθθq are cost returns defined by Eqs. (15) and (17). Solving Eq. (19) involves iteratively updating the policy parameters and the Lagrange multipliers. The update steps alternate between improving the policy based on both reward and cost gradients, and adjusting λi to enforce constraints. Formally: ff πθθθk`1 πθθθk ` αpkq θθθk Rpπθθθk ` θθθk Cipπθθθk λk , (20) 2ÿ i1 λk`1 maxt0, λk βipkq Cipπθθθk bi u. 6 IEEE TRANSACTIONS ON AUTOMATION SCIENCE AND ENGINEERING. PREPRINT VERSION. ACCEPTED FEB, 2026 LIPO LR rt pθθθq LP3O LR rt pθθθq 2ÿ i1 2ÿ i1 Here, αpkq and βipkq are learning rates computed by Adam the training iteration k. The projection optimizer [51] at maxt0, xu ensures that each Lagrange multiplier remains nonnegative. We integrate this Lagrangian approach into the PPO algorithm [52] within an Actor-Critic framework. The policy is optimized to maximize combined objective: LPPO-Lag LR rtpθθθq 2ÿ i1 λiJ Cipπθθθq, (21) where LR is the clipped surrogate objective defined by PPO: LR mintrt pθθθq Aπoldpsssďt, aaatq, clip prt pθθθq , c1, c2q Aπold psssďt, aaatqu, with the probability ratio rtpθθθq πθθθpaaat sssďtq πoldpaaat sssďtq . (22) (23) In this formulation, πold denotes the behavior policy used for data collection, Aπold is the advantage computed via GAE [53], and c1, c2 define the PPO clipping range. Similarly, within the PPO framework, the final objectives of IPO, P3O, and CRPO are as follows: ` κIPO log Cipπθθθq bi , The reward critic is trained with the following loss [54]: LReward Critic maxt}V pssstq Rt}2, }Vtargetpssstq ` clippVtargetpssstq pssstq, c1, c2q Rt}2u, (25) where Rt is the cumulative return at time t, pssstq denotes the value estimated by the critic at state ssst, and Vtargetpssstq is the value approximated by the target critic at state ssst. Unlike previous studies [42, 49] that utilize cost critic, we estimate cost return for energy using the Monte Carlo approach [55], which achieves comparable performance while requiring fewer network parameters and minimal design choices. A. Robot Platform V. SIMULATION We use the kid-sized humanoid robot BRUCE [3, 56, 57] as an example to demonstrate the performance of the proposed ECO framework. BRUCE is 70cm tall, weighs 4.8kg, and has total of 16 DoFs, with 3 per arm and 5 per leg. All leg DoFs are driven by liquid-cooled proprioceptive actuators with peak torque 10.5N m, while the arm DoFs are driven by Dynamixel servo motors. The PD controller gains for both legs are set as Kp diagp7, 10, 7, 10, 1.5q and Kd diagp0.2, 0.4, 0.2, 0.4, 0.08q for the hip yaw, hip pitch, hip roll, knee pitch, and ankle pitch joints, respectively. κP3O maxtJ Ci pπθθθq ` ϵi, 0u, (24) B. Learning Setups LCRPO 1nk0 LR rt pθθθq 2ÿ νi Cipπθθθq. Among the algorithms compared, P3O (with fixed κP3O 1.0) and PPO-Lag (with initial λi 0, βi 1e3) required minimal parameter tuning and achieved low constraint violations. While P3O demonstrated comparable energy performance, PPO-Lag exhibited better convergence speed and stability, thereby selected for implementing the constrained policy optimization approach in ECO. 1) Traning Setups: The policy training is conducted in the IsaacGym [58] with parallelization across 8192 environments. The utilized rewards and scales are summarized in Tab. I. The constraint policy optimization of ECO utilizes the PPOLagrangian [42] and the hyperparameters are shown in Tab. III. We employ domain randomization [59] to enhance transfer to unseen scenarios, with specific parameters listed in Tab. II. During training, two types of external perturbations are applied to enhance the policy robustness: force disturbances every 2s, lasting 0.001s with magnitudes randomly sampled between r100, 100sN , and velocity impulses every 4s, also lasting TABLE I: Summary of additional reward functions. The tracking error metric is defined as: ϕpe, wq : exppw }e}2q, where represents the tracking error, and is the associated weight. vlin and ωang represent the linear and angular velocity errors; 1pq is the indicator function for the logical condition (), for example, 1stand phase 1 when the leg is in stand phase. tair is the duration that the foot remains in the air during locomotion; is the base height; hf is the deviation in foot height; ht is the target foot height during the swing phase; represents the summation across all legs. The base height target is set to 0.45m, the maximum contact force Fmax is set to 50N , and the target foot height ht is 0.03m. 9P is the velocity of the feet; qyaw/roll is the yaw and roll joint position; q0 is the default joint position; ř Reward Equation (ri) Scale (µi) Tracking Linear Velocity Tracking Angular Velocity Foot Air Time Foot Contact Velocity Foot Clearance Foot Contact Number Base Orientation Foot Contact Forces Default Joint Pos Tracking Base Height Self-collision Penalty Action Smoothness ϕpvlin, 10q ϕpωang, 10q ř ř ř 1contact minptair, 0.5q } 9P } 1contact hf htă0.01 1swing phase 1 ř p1contact 1stand phaseq 1 1 2 pϕpeuler angles, 10q ` ϕpgravity vector, 20qq ř maxp0, }Fc} Fmaxq ϕpqyaw/roll q0, 100q ϕpP 0.45, 100q ř p1self-collisionq }aaat aaat1}2 ` }aaat 2aaat1 ` aaat2}2 ` 0.05}aaat} 1.2 1.1 1.0 -0.05 1.0 1.2 1.0 -0.01 0.5 0.2 -1.0 -0. HUANG et al.: ECO: ENERGY-CONSTRAINED OPTIMIZATION WITH REINFORCEMENT LEARNING FOR HUMANOID WALKING 7 TABLE II: Overview of domain randomization. The domain randomization terms and the associated parameter ranges are listed. Additive randomization increments the parameter by value within the specified range while scaling randomization adjusts it by multiplicative factor from the same range. Parameter Payload Body COM Displacement Floor Friction Restitution Motor Strength Joint Friction Joint armature Action Delay Kp Factors Kd Factor Initial Euler Angle Noise Joint Position Noise Joint Velocity Noise Angular Velocity Noise Euler Angle Noise Unit kg - - % % % ms % % rad rad rad/s rad/s rad Range [-0.5, 0.5] [-0.05, 0.05] [0.1, 2.0] [0.0, 0.5] [0.9, 1.1] [0.02, 0.05] [0.0, 0.01] [0, 10] [0.9, 1.1] [0.9, 1.1] [-0.1, 0.1] [-0.03, 0.03] [-0.6, 0.6] [-0.06, 0.06] [-0.018, 0.018] Operator additive additive - additive scaling scaling additive - scaling scaling additive additive additive additive additive Type Uniform Uniform Uniform Uniform Uniform Uniform Uniform Uniform Uniform Uniform Uniform Gaussian (1σ) Gaussian (1σ) Gaussian (1σ) Gaussian (1σ) TABLE III: Hyperparameters in the training process. Parameter Number of Environments Number Training Epochs Batch size Episode Length Discount Factor of Reward Discount Factor of Energy Cost PPO Clip Coefficient c1, c2 Frame Stack of Single Observation Kf Frame Stack of Single Privileged Observation Number of Single Observation Number of Single Privileged Observation Initial Lagrangian Multipliers λ1, λ2 Learning Rate of Lagrangian Multiplierβ1, β2 Cost Threshold b1 (0.1m{s) Cost Threshold b1 (0.15m{s) Cost Threshold b1 (0.2m{s) Cost Threshold b2 Value 8192 2 8192 ˆ 96 24s 0.994 0.90 0.8, 1.2 15 3 40 55 0.0, 0.0 0.001,0.001 60J 2.5W ˆ 24s 70J 2.9W ˆ 24s 80J 3.3W ˆ 24s 0.05 0.001s, with linear velocities up to 0.2m{s and angular velocities up to 0.4rad{s. The simulation is executed at frequency of 1kHz, with control frequency of 100Hz. The training process is carried out on system equipped with an Intel Core i9-13900KFN processor and NVIDIA GeForce RTX 4090, and each training run consists of 2000 policy iterations. Convergence is typically achieved within approximately 4h of wall-clock time. We use the policy trained after 10h for sim-to-real experiments. 2) Constraint Thresholds Search: Since the PPO baseline policy described in Sec. V-C converges to suboptimal solution, it serves as useful initial reference for selecting constraint thresholds. For the reference motion constraint, we empirically observed that the mirror-symmetry loss value obtained from the PPO baseline ensures robust and symmetric walking behavior. Accordingly, we set the threshold b2 in Eq. (17) as 0.05 across all experiments. For the energy constraint threshold, we adopt linear search strategy [15] to determine appropriate values, further details are provided in Sec. V-D4. Each threshold search takes around 4 hours of wallclock time depending on the chosen step size. Additionally, as summarized in Tab. III, energy consumption thresholds may vary with different target velocities. C. Baselines For comparison, five baseline approaches are chosen to demonstrate that our proposed ECO framework significantly improves energy efficiency without degenerating the performance of humanoid robots in walking tasks. The MPC baseline uses the state-of-the-art model-based hierarchical method which combines high-level MPC planner with simplified model and low-level whole-body controller with the full-body dynamics [19]. Humanoid-Gym [9] is used as the model-free RL baseline (PPO) for training endto-end locomotion policies in humanoid robots. To facilitate rigorous comparison with the ECO model, the PPO baseline has been augmented with an energy penalty reward [7], which serves as the constraint within the ECO framework. We selected scales of energy penalty reward µe t0.001, 0.01, 0.1, 0.03, 0.05, 0.08, 0.015, 0.02, 0.025u, while all other rewards and scales are aligned with those of ECO as detailed in Tab. I. Additionally, mirror loss is included in the PPO loss function to encourage symmetry, with fixed weight of 0.1. Three constrained RL baseline methods are chosen as introduce in Eq. (24). For IPO and P3O, we fine-tune the hyperparameters as follows: κP3O 1 1.0, κP3O 2 1.0, κIPO t1.0, 10.0u, and κIPO 2 1.0. D. Simulation Results 1) Comparing Constraint RL Algorithms: We compare the proposed ECO framework against PPO, IPO, P3O, and CRPO under 0.1m{s linear velocity tracking task, subject to energy and reference motion constraints. To assess the performance, we evaluate four key metrics, each capturing distinct aspect of the task: (1) Normalized total reward ˆJ reflects overall task performance; (2) Normalized episode length Tep measures stability and robustness; (3) Mean energy cost ˆJ C1 quantifies energy efficiency; (4) Mean mirror cost ˆJ C2 evaluates motion symmetry. The metrics are formally defined as: ˆJ Tep ˆJ C1 ˆJ C2 1 1 1 1 t0 rt,i Rmin ř ř ř i1 Rmax Rmin i1 ep max ep max ep min Eÿ Tÿ ep , C1psssi t, aaai tq, t0 i1 Bÿ C2psssi, aaaiq, i1 , (26) (27) (28) (29) where rt,i is the reward at time of episode i, and Rmin, Rmax are the minimum and maximum cumulative rewards observed across all algorithms, ep is the length of episode i, and min , max are the minimum and maximum episode lengths among ep all methods, psssi, aaaiq is the i-th stateaction sample in batch and is the batch size. We enforce the average-sum constraint over each gradient batch as in [60], using the same batch size across all algorithms. ep As shown in Fig. 3, ECO (initialized with λi 0, βi 103) consistently converges to the target energy threshold of 8 IEEE TRANSACTIONS ON AUTOMATION SCIENCE AND ENGINEERING. PREPRINT VERSION. ACCEPTED FEB, 2026 Fig. 3: Comparison of training metrics for ECO, P3O, IPO, and CRPO. The energy consumption and mirror reference motion thresholds are set at 60J and 0.05, respectively, as indicated by the black dashed lines in (a) and (b). Results averaged over 10 random seeds. Fig. 4: Comparison of training metrics for ECO and the PPO. The energy consumption and mirror reference motion thresholds are set at 60J and 0.05, respectively, as indicated by the black dashed lines in (a) and (b). Results averaged over 10 random seeds. 60J while maintaining normalized episode length close to 1.0, demonstrating both energy efficiency and training stability. In comparison, P3O satisfies both the energy and reference motion constraints but requires more iterations to converge. IPO exhibits sensitivity to the penalty coefficient κIPO 1 : with κIPO 1 1.0, it fails to satisfy the energy constraint; increasing it to κIPO 1 10.0 enforces the constraint but leads to reduced episode length due to overly conservative behavior. CRPO, while capable of satisfying the constraints, exhibits unstable convergence, likely due to frequent policy shifts between optimizing for reward and constraint cost. As shown in Fig. 4, we explore three orders of magnitude for the energy reward coefficient in PPO (µe), but none yields satisfactory balance between energy efficiency and stability. Lower coefficients failed to reduce energy consumption, while higher ones caused instability and frequent falls. These results HUANG et al.: ECO: ENERGY-CONSTRAINED OPTIMIZATION WITH REINFORCEMENT LEARNING FOR HUMANOID WALKING Fig. 5: Sim-to-Sim transfer results. For visual clarity, single-run deployment curves are shown. (a) Visual comparison of walking stability across simulators; (b) Ankle height consistency across simulators; (c) Motor energy consumption at 0.1m{s over 10s in Gazebo; (d) Body velocity tracking in MuJoCo in different speed commands. highlight the difficulty of manually tuning reward weights in multi-objective PPO. In contrast, ECO directly enforces energy and motion constraints, avoiding such tuning and achieving robust, energy-efficient behavior. Regarding the reference motion constraint (Fig. 3(b)), IPO, P3O, and CRPO rapidly converge to the predefined threshold of 0.05, though their resulting behaviors are more conservative than those of ECO. In contrast, PPO with various energy coefficients converges to different levels of mirror cost (Fig. 4(b)), often yielding asymmetric and unstable gaits. These results highlight that constrained RL offers more principled and efficient framework for aligning policy behaviors with target objectives without heavy hyperparameter tuning. Finally, we note that PPO, P3O, and IPO rely on fixed constraint coefficients (µe, κP3O, κIPO), and CRPO lacks constraint-specific parameters altogether. These approaches are therefore prone to convergence issues or require careful manual tuning. In contrast, ECO leverages PPO-Lag to dynamically update the Lagrange multipliers λi using the Adam optimizer, allowing adaptive scaling the impact of constrains during training. This dynamic adjustment significantly improves convergence stability and highlights PPO-Lag as more effective strategy for optimizing energy-efficient humanoid locomotion. 2) Sim-to-Sim Transfer: To evaluate the robustness and generalizability of the learned policy, we transferred it to two additional simulation environments, MuJoCo [61] and Gazebo [62], to assess ECOs sim-to-sim performance and benchmark it against baseline methods. As shown in Fig. 5(a), our method achieves stable walking across three simulation environments. Fig. 5(b) and (c) demonstrate that ECO consistently maintains similar gait pattern during cross-simulator transfer and reduces energy usage in Gazebo while BRUCE walks at 0.1m{s over 10s period. Fig. 5(d) further shows that our method accurately tracks linear velocity commands at three different target speeds (0.1m{s, 0.15m{s, 0.2m{s) in MuJoCo. We report summary statistics over 10 runs in Tab. IV. Results show that MPC incurs the highest energy cost and fails to track higher target speeds (e.g., 0.2m{s), likely due to its QP solver being unable to resolve contact dynamics in finite time. PPO exhibits better performance than MPC but with moderate energy consumption. ECO, in contrast, not only achieves accurate velocity tracking but also maintains significantly lower energy consumptionapproximately 3 times lower than MPC and 1.4 times lower than PPO. 3) Disturbance Rejection Test: To verify that the walking robustness has not been sacrificed in the learned policy, we conduct disturbance rejection tests on BRUCE while walking Fig. 6: Benchmark of disturbance rejection. BRUCE was subjected to pushes of varying durations from 8 different directions (0, 45, 90, 135, 180, 225, 270, 315). Results are averaged over 10 runs. 10 IEEE TRANSACTIONS ON AUTOMATION SCIENCE AND ENGINEERING. PREPRINT VERSION. ACCEPTED FEB, 2026 TABLE IV: Velocity tracking and energy consumption across simulators. Aggregated over 10 runs, each lasting 10 seconds. Method Target: (0.1m{s, 2.5W ) Target: (0.15m{s, 2.9W ) Target: (0.2m{s, 3.3W ) Speed (m/s) Energy (W) Speed (m/s) Energy (W) Speed (m/s) Energy (W) PPO (Isaacgym) Ours (Isaacgym) 0.1129 0.00071 0.1026 0.00279 3.1301 0.0165 2.1952 0. 0.1534 0.0030 0.1617 0.00188 4.0965 0.0591 2.2401 0.0239 0.2141 0.0043 0.2131 0.0041 4.6311 0.0666 2.8474 0.0641 PPO (Mujoco) Ours (Mujoco) 0.1092 0.0005 0.10033 0. 2.8150 0.00895 2.00009 0.0066 0.1567 0.00042 0.1572 0.00022 3.7186 0.00957 2.7098 0.00561 0.2028 0.00089 0.2038 0.00064 4.0692 0.0162 2.8497 0.00838 PPO (Gazebo) MPC (Gazebo)* Ours (Gazebo) 0.1062 0.0011 0.1075 0.0028 0.0930 0.0029 0.1528 0.00092 0.1322 0.0047 0.1542 0.0013 * MPC baseline method is exclusively implemented in Gazebo so that its performances in Isaacgym and Mujoco are disregarded. 4.3922 0.0181 9.9155 0.3843 2.9737 0.0133 3.1508 0.0154 9.2693 0.1604 2.3653 0.0201 0.2027 0.0037 0.1523 0.0057 0.2159 0.0035 5.1826 0.0434 10.8626 0.3497 3.5067 0. in Gazebo. External forces are applied at the origin of the coordinate frame attached to BRUCEs body link. The timing is chosen as vulnerable transitions from double-stance phase to lifting the left foot or the right foot. Since the response to perturbations varies with the direction in these two cases, we record the maximum force BRUCE could resist for both timings and use the smaller of the two values for analysis. As shown in Fig. 6, the proposed ECO framework demonstrates greater robustness across all directions, especially in the sagittal direction, compared to the baseline MPC, and shows comparable robustness to PPO. Although ECO has not encountered pushes with duration longer than 0.001s in the training stage, ECO still performs well under longer pushes, indicating that our method has not overfitted to improve energy efficiency in compromise of robustness. Besides controlled pushes, two qualitative disturbance experiments in Mujoco are conducted to evaluate robustness under more diverse and unpredictable scenarios: (1) different links of BRUCE are manually dragged at random timings with random magnitudes, producing varied disturbance directions and intensities; (2) Balls are thrown to BRUCE with random velocities to create different impulsive contacts. As shown in the accompanying video, ECO successfully maintains stable walking and recovers from these random perturbations, further demonstrating that the learned policy generalizes beyond the specific disturbance patterns used in training. 4) Energy Constraint Threshold Analysis: We observe that the baseline method PPO converges (achieving normalized episode length of 0.9) within 1500 training iterations while consuming approximately 100J of energy with the target walking velocity as 0.1m{s. Based on this, we conduct linear search over energy thresholds of 20J, 40J, 60J, 80J, and 100J to identify the minimum threshold that enables the policy to achieve similar normalized episode length within 1500 iterations, as shown in Fig. 7. The 60J threshold provides the best trade-off between energy efficiency and locomotion stability. This approach is effective as it leverages physically meaningful thresholds while supporting parallel evaluation. finer-grained search (e.g., with 10J increments) can discover policies with even lower energy consumption (e.g., 50J) that still maintain stability. If further energy reduction is required, smaller step sizes enable finer control at the cost of additional computation. 5) Hyperparameter Sensitivity Analysis: To demonstrate that the proposed ECO framework requires less human effort in hyperparameter tuning, we conduct sensitivity analysis by training ECO on 0.1m{s linear velocity tracking task with Fig. 7: Energy constraint threshold analysis. (a)(b): The mean energy cost and normalized episode length for different energy thresholds. Results averaged over 10 random seeds. an energy threshold of b1 60J. We fix the initial value of the Lagrange multiplier at 0 and vary the learning rate (β1 0.001, 0.01, 0.1). As shown in Fig. 8(a), which illustrates the mean energy cost over iterations, all three learning rates converge to similar energy cost of approximately 60J after an initial period of fluctuation. This pattern suggests that our Lagrangian approach ensures effective energy convergence across different learning rates. Fig. 8(b) shows the evolution of the Lagrange multiplier over iterations for different learning rates. smaller learning rate causes the Lagrange multiplier to change more slowly, resulting in higher energy consumption initially due to constraint violations As energy increases, the multiplier gradually rises, enabling faster convergence to the feasible region. Finally, Fig. 8(c) illustrates that our method achieves stable convergence across the range of learning rates. In previous work on quadruped robots [15, 48], additional reward terms such as self-collision, foot contact velocity, and foot clearance were integrated into the constraints to reduce the need for extensive reward scaling. We also attempted to incorporate these three constraints. Specifically, we trained an additional multi-head cost critic to estimate the cost returns for self-collision, foot contact velocity, and foot clearance. For these additional constraints, we used the converged reward values obtained from ECO (with two constraints) as the constraint thresholds. We analyze the performance of several algorithms under five 6) More Constraints Setting: HUANG et al.: ECO: ENERGY-CONSTRAINED OPTIMIZATION WITH REINFORCEMENT LEARNING FOR HUMANOID WALKING 11 TABLE V: Algorithm performance comparison in the real world. Aggregated over 10 runs, each lasting 10 seconds. Speed (m/s) MPC 0.124 0.0072 0.126 0.0088 PPO ECO 0.122 0.0089 Energy (w) 9.1176 5.0029 3.9080 1.8193 1.7490 0. Body Height (m) 0.3483 0.0056 0.3745 0.0034 0.3790 0.0030 VI. EXPERIMENTS A. Real World Results To verify the improvement in energy efficiency, we deploy the learned policy from ECO, along with the two baseline methods, on the real robot hardware BRUCE for comparison. Statistical results across multiple experiment runs, including the linear velocity along the direction (speed), the total motor power (energy), and the body height in the world frame, are reported in Tab. V. Due to the sim-to-real gap, velocity tracking on the hardware is less accurate than in simulation. To ensure fair comparison of energy consumption, we select three sets of data with similar, though not identical, speeds from multiple experiments using different methods. As shown in Fig. 10(a), ECO consistently maintains motor power near or below the energy cost threshold 2.5W p60J{24sqapproximately 6 times lower than MPC and 2.3 times lower than PPO according to the cumulative energy graph. Moreover, with the learned policy from ECO, BRUCE successfully rejects disturbances and traverses different outdoor terrains as illustrated in Fig. 11. The full hardware demonstration clips including comparisons with baselines, disturbance rejection, and outdoor walking can be seen in the accompanying video. B. Emergent Behaviors Empirical studies indicate that straight knee and heel-to-toe walking enhance energy efficiency in humans and humanoid robots [23, 63]. Instead of manually designing these behaviors for efficient locomotion, ECO shows that minimizing energy consumption can generate similar behaviors without explicit prescriptions, which also demonstrates that these bio-inspired behaviors are related to energy efficiency. Fig. 12 illustrates the joint-level behaviors during walking. Fig. 12(a) compares the left knee joint angle over time, where ECO demonstrates less flexed knee compared to MPC and PPO. In Fig. 12(b), the torque applied to the left knee joint is compared. ECO consistently requires lower torque to maintain stability and velocity tracking, resulting in reduced energy expenditure during walking. Due to the lack of precise forcetorque sensors under the feet of BRUCE, the knee torques can also implicitly serve as measure of the intensity of ground contact as the knee exerts the most force in humanoid locomotion. Additionally, we compare the body oscillation over 10s stable walking period. Fig. 10(b) shows that BRUCE exhibits fewer oscillations with lower frequency and smaller magnitude when walking with the policy from ECO, compared to MPC and PPO. The less body oscillation and smaller knee torques suggest that improved energy efficiency also results in lighter steps and reduced stomping. Fig. 8: Hyperparameter sensitivity analysis. The mean energy cost (a), Lagrange multiplier λ1 (b), and normalized episode length (c) of ECO with the energy constraint b1 60J, evaluated across different learning rates for λ1 (β1 0.001, 0.01, 0.1) during training. Results averaged over 10 random seeds. constraints and investigate how the convergence properties of ECO are affected by different constraint configurations. For the setup with five constraints, ECO is trained with fixed initial Lagrange multiplier λi 0.0 and learning rate βi 1e 3, while IPO and P3O used the same κ 1.0 for each constraint. As shown in Fig. 9(a), we observed that the episode lengths for ECO, IPO, and P3O with five constraints are significantly reduced compared to ECO with only two constraints. Fig. 9(e) shows that P3O and IPO fail to converge on the foot velocity contact constraint, while ECO demonstrates better constraint satisfaction. Fig. 9(f) shows that all algorithms fail to converge on the self-collision cost. This is possibly due to predicting self-collision from the current privileged observation space is challenging. In the four-constraint setting of ECO (without selfcollision), we observe moderately better satisfaction of the energy constraint compared to the five-constraint case. Further removing the foot contact velocity constraint (three constraints setting) led to even longer episode lengths than the fourconstraint setting, although performance remained below that of the original two-constraint configuration. Given the empirical results and our previous observation of the one-constraint settingwhere omitting the symmetric reference motion constraint led to unnatural walking behaviors, as demonstrated in the accompanying videowe conclude that the two-constraint configuration (energy consumption and symmetry constraints) is the most suitable for achieving energy-efficient and consistently stable humanoid walking in our task. Further discussion and rationale are provided in Sec. VII-A. 12 IEEE TRANSACTIONS ON AUTOMATION SCIENCE AND ENGINEERING. PREPRINT VERSION. ACCEPTED FEB, 2026 Fig. 9: Training performance with more constraints settings. Comparison of training metrics for ECO, P3O, and IPO. The thresholds for energy consumption, mirror reference motion, foot clearance, foot contact velocity, and self-collision are set at 60J, 0.05, 3.6, -528, and 0.048, respectively, as indicated by the black dashed lines in (b)-(f). Results averaged over 10 random seeds. Fig. 11: Screenshots of the sim-to-real transfer. BRUCE successfully rejects disturbance and traverses outdoor terrains which can be seen in the accompanying video. VII. DISCUSSION A. Rewards v.s. Constraints In this study, we find that tuning reward scales through hyperparameter search within the RL framework (PPO) is an inefficient approach to reducing energy consumption in humanoid robots. This inefficiency arises because reward scales do not correspond linearly to physical quantities, demanding delicate balance between meeting energy requirements, following task commands, and maintaining motion stability. In contrast, constrained RL explores space defined by explicit constraints, many of which have direct physical significance, such as energy consumption limits and joint restrictions. This makes tuning constraints more straightforward and effective compared to adjusting reward or loss weights. In this work, we try to incorporate self-collision, foot contact velocity, and foot clearance as additional constraints. Although such constraints have been successfully applied to quadruped robots [15, 48], we find it challenging to transfer these approaches directly to humanoids. This difficulty arises not only from technical challenges associated with the Fig. 10: Motor energy consumption and body height oscillation in the real world. Single-run deployment curves are shown for visual clarity. The relative body height is defined as the offset from the setpoint of the body height. The inset plot in (a) is the cumulative motor energy. However, heel-to-toe transition is not presented in the resulting policy as shown in Fig. 11. We argue that the heelto-toe strategy, given the current hardware design of BRUCE, demands additional control effort to execute effectively. Furthermore, it requires higher swing leg lift, which increases energy consumption in the hip and knee motors. HUANG et al.: ECO: ENERGY-CONSTRAINED OPTIMIZATION WITH REINFORCEMENT LEARNING FOR HUMANOID WALKING 13 of such approaches is beyond the scope of this work and is left for future research. B. Optimization Method Selection Our experiments show that among the four constrained RL algorithms, PPO-Lag is the most effective. CRPO, as primal algorithm, does not introduce additional dual variables for optimization and involves fewer hyperparameters, making it theoretically simpler and easier to implement compared to primal-dual methods. However, it lacks the smooth transition between reward and constraint optimization objectives that primal-dual methods offer, making it difficult to achieve stable convergence in humanoid robots. P3O uses clamping functions on constraints to apply linear cost gradient penalty until the constraints are met. However, this linear penalty makes balancing multiple constraints difficult, as each constraint has distinct physical significance and magnitude, requiring separate coefficients κP3O . Small coefficients lead to slow convergence, while large ones make the system overly conservative. IPO, as method that theoretically also employs penalty functions, exhibits performance similar to P3O in our experiments. It also requires careful tuning of the coefficient κIPO to balance constraints effectively. In contrast, ECO leverages PPO-Lag to update both the Lagrange multipliers and the policy, allowing the Lagrange multipliers and their learning rates to adapt to the magnitude of each constraint. By utilizing the Adam optimizer to update these multipliers and learning rates, ECO effectively adapts to each constraint while smoothly transitioning the optimization objective between rewards and constraints. As result, ECO achieves stable convergence in humanoid robots. However, ECO faces challenges in optimizing the five constraints considered in our study, highlighting the need for improved optimization techniques in future work. Leveraging the dynamics model of humanoid robots to improve the convergence of constrained RL within restricted feasible region could be promising direction [13, 66, 67]. C. Sim-to-Real Failure Cases Our methods successfully transfer to the real world, and we identify several key factors that critically affect policy transfer: 1) PD Gain Tuning: We find that carefully tuning the proportional (Kp) and derivative (Kd) gains of the lowlevel PD controller is essential to align the joint-position tracking performance between simulation and hardware. Otherwise, differences in system dynamics can cause the policy to produce divergent behaviors in the real world. 2) Compliant ankle strategy: We find that high PD gains at the ankle lead to instability upon ground contact due to sensor noise. To address this, we reduce the ankles PD gains, making the joints more compliant and the policy less sensitive to noise. This simple adjustment significantly enhances robustness to terrain variations and modeling errors in real-world tests. 3) Sensor Noise and Bias: Real sensors exhibit both highfrequency noise (especially in joint-velocity measurements) and low-frequency biases (e.g. Euler-angle drift Fig. 12: Comparison of left knee joint angle and torque between different methods in the real world. We present single-run deployment curves for visual clarity. larger knee joint angle reflects more flexed knee during walking, while smaller knee joint torque at ground contact indicates lighter step. heterogeneity of constraint signals but also from differences in morphology and balance dynamics. First, the cost terms associated with different constraints often differ in magnitude, which can lead to imbalanced gradient updates when training the multi-head cost critic [15]. As result, costs with smaller magnitudessuch as the self-collision cost in Fig. 9(f)tend to converge more slowly. This issue could be mitigated by applying appropriate normalization techniques [64] to rescale cost signals to comparable magnitudes. Second, accurately predicting different types of costs may require heterogeneous information. For instance, inferring self-collision from the current privileged observation (i.e., joint angles and base-link kinematics) is particularly challenging, as it requires the network to implicitly reconstruct the geometry of the body in the world frame from joint-centric data. This suggests that richer observation spaceincluding world-frame informationor specialized cost critic for self-collision may be necessary to improve both prediction accuracy and constraint satisfaction. Furthermore, humanoid robots have much smaller support polygons than quadruped robots, especially in the singlesupport phase, where the support region further shrinks. This renders the space of feasible trajectories much smaller than quadruped robots. In this case, imposing large amount of feasibility constraints in constrained RL may complex the search space. We believe further study is required to handle larger number of constraints in humanoid locomotion. For instance, incorporating model-based prior knowledge to refine the search spacesuch as using nullspace projections to prioritize control tasks, as commonly done in the control community [65]could help in selecting appropriate constraints and mitigating task conflicts. comprehensive investigation 14 IEEE TRANSACTIONS ON AUTOMATION SCIENCE AND ENGINEERING. PREPRINT VERSION. ACCEPTED FEB, 2026 upon reset). To account for this, we inject noise and random biasesmatched in magnitude to hardware measurementsinto the corresponding state observations during domain randomization. D. More complex settings While our method is effective for stable walking, applying it to more complex scenarios, such as transitioning from flat surfaces to stairs, presents new challenges. For instance, lower foot lift is energy-efficient on flat surfaces, but stair climbing requires higher foot lifts and precise planning to navigate varying stair heights efficiently. One possible extension is to introduce mode-aware constraints [68, 69] that adapt according to different terrains and command types. For example, energy constraints could be relaxed when stair climbing is anticipated. terrain classifier or visual encoder [7072] could be used to identify terrain types, allowing the policy to dynamically adjust constraint thresholds accordingly. In addition, modelbased planning [13, 73, 74] can be incorporated, in which high-level planner selects locomotion modes based on the environment, while the low-level control executes constrained locomotion policy conditioned on the selected mode. This hierarchical approach aligns well with the constrained RL framework, as constraints can be defined and enforced differently across modes. We consider this an exciting future direction and plan to explore model-based extensions in our follow-up work. VIII. CONCLUSION In this paper, we optimize the energy consumption for humanoid walking through constrained RL by separating energy consumption and reference motion objectives from the reward function. We demonstrate that the ECO framework can converge to policy that achieves lower energy consumption while maintaining robustness. Meanwhile, ECO is resilient to hyperparameter settings, thereby eliminating the need for extensive tuning on the weights of rewards. Extensive experiments in both simulation and real-world hardware validate that our approach can significantly improve the energy efficiency of humanoid walking without compromising robustness. REFERENCES [1] Z. Wang, Y. Jia, L. Shi, H. Wang, H. Zhao, X. Li, J. Zhou, J. Ma, and G. Zhou, Arm-constrained curriculum learning for loco-manipulation of the wheel-legged robot, arXiv preprint arXiv:2403.16535, 2024. [2] Z. He, J. Wu, J. Zhang, S. Zhang, Y. Shi, H. Liu, L. Sun, Y. Su, and X. Leng, Cdm-mpc: An integrated dynamic planning and control framework for bipedal robots jumping, IEEE Robotics and Automation Letters (RA-L), vol. 9, no. 7, pp. 66726679, 2024. [3] J. Zhang, J. Shen, Y. Liu, and D. Hong, Design of jumping control framework with heuristic landing for bipedal robots, in IEEE/RAS International Conference on Intelligent Robots and Systems (IROS), pp. 85028509, IEEE, 2023. [4] H. Liu, Q. Xie, Z. Zhang, T. Yuan, S. Wang, Z. Wang, X. Leng, L. Sun, J. Zhang, Z. He, et al., Pr2: physics-and photo-realistic humanoid testbed with pilot study in competition, Journal of Field Robotics, 2025. [5] P.-B. Wieber, R. Tedrake, and S. Kuindersma, Modeling and control of legged robots, in Springer handbook of robotics, pp. 12031234, Springer, 2016. [6] Y. G. Alqaham, J. Cheng, and Z. Gan, Energy-optimal asymmetrical gait selection for quadrupedal robots, IEEE Robotics and Automation Letters (RA-L), 2024. [7] Z. Fu, A. Kumar, J. Malik, and D. Pathak, Minimizing energy consumption leads to the emergence of gaits in legged robots, arXiv preprint arXiv:2111.01674, 2021. [8] Z. Fu, Q. Zhao, Q. Wu, G. Wetzstein, and C. Finn, Humanplus: Humanoid shadowing and imitation from humans, arXiv preprint arXiv:2406.10454, 2024. [9] X. Gu, Y.-J. Wang, and J. Chen, Humanoid-gym: Reinforcement learning for humanoid robot with zero-shot sim2real transfer, arXiv preprint arXiv:2404.05695, 2024. [10] S. H. Jeon, S. Heim, C. Khazoom, and S. Kim, Benchmarking potential based rewards for learning humanoid locomotion, in IEEE International Conference on Robotics and Automation (ICRA), pp. 92049210, IEEE, 2023. [11] Y. Chow, M. Ghavamzadeh, L. Janson, and M. Pavone, Riskconstrained reinforcement learning with percentile risk criteria, Journal of Machine Learning Research, vol. 18, no. 167, pp. 151, 2018. [12] T. He, W. Zhao, and C. Liu, Autocost: Evolving intrinsic cost for zero-violation reinforcement learning, in AAAI Conference on Artificial Intelligence, vol. 37, pp. 1484714855, 2023. [13] W. Huang, J. Ji, B. Zhang, C. Xia, and Y. Yang, Safe dreamerv3: Safe reinforcement learning with world models, arXiv preprint arXiv:2307.07176, 2023. [14] J. Achiam, D. Held, A. Tamar, and P. Abbeel, Constrained policy optimization, in International conference on machine learning, pp. 22 31, PMLR, 2017. [15] Y. Kim, H. Oh, J. Lee, J. Choi, G. Ji, M. Jung, D. Youm, and J. Hwangbo, Not only rewards but also constraints: Applications on legged robot locomotion, IEEE Transactions on Robotics (T-RO), 2024. [16] S. Collins, A. Ruina, R. Tedrake, and M. Wisse, Efficient bipedal robots based on passive-dynamic walkers, Science, vol. 307, no. 5712, pp. 10821085, 2005. [17] R. Tedrake, T. W. Zhang, M.-f. Fong, and H. S. Seung, Actuating simple 3d passive dynamic walker, in IEEE International Conference on Robotics and Automation (ICRA), vol. 5, pp. 46564661, IEEE, 2004. [18] S. Kuindersma, R. Deits, M. Fallon, A. Valenzuela, H. Dai, F. Permenter, T. Koolen, P. Marion, and R. Tedrake, Optimization-based locomotion planning, estimation, and control design for the atlas humanoid robot, Autonomous robots, vol. 40, pp. 429455, 2016. [19] J. Shen, J. Zhang, Y. Liu, and D. Hong, Implementation of robust dynamic walking controller on miniature bipedal robot with proprioceptive actuation, in IEEE-RAS International Conference on Humanoid Robots (Humanoids), pp. 3946, IEEE, 2022. [20] W. Xi, Y. Yesilevskiy, and C. D. Remy, Selecting gaits for economical locomotion of legged robots, International Journal of Robotics Research (IJRR), vol. 35, no. 9, pp. 11401154, 2016. [21] B. Ponton, M. Khadiv, A. Meduri, and L. Righetti, Efficient multicontact pattern generation with sequential convex approximations of the centroidal dynamics, IEEE Transactions on Robotics (T-RO), vol. 37, no. 5, pp. 16611679, 2021. [22] J. Wang, S. Kim, T. S. Lembono, W. Du, J. Shim, S. Samadi, K. Wang, V. Ivan, S. Calinon, S. Vijayakumar, et al., Online multi-contact receding horizon planning via value function approximation, IEEE Transactions on Robotics (T-RO), vol. 40, pp. 27912810, 2024. [23] S. Fasano, J. Foster, S. Bertrand, C. DeBuys, and R. Griffin, Efficient, dynamic locomotion through step placement with straight legs and rolling contacts, in IEEE International Conference on Robotics and Automation (ICRA), pp. 11431150, IEEE, 2024. [24] W. Cui, S. Li, H. Huang, B. Qin, T. Zhang, L. Zheng, Z. Tang, C. Hu, N. Yan, J. Chen, et al., Adapting humanoid locomotion over challenging terrain via two-phase training, in 8th Annual Conference on Robot Learning. [25] K. Li, G. Lai, and X. Yao, Interactive evolutionary multiobjective optimization via learning to rank, IEEE Transactions on Evolutionary Computation, vol. 27, no. 4, pp. 749763, 2023. [26] I. Radosavovic, B. Zhang, B. Shi, J. Rajasegaran, S. Kamat, T. Darrell, K. Sreenath, and J. Malik, Humanoid locomotion as next token prediction, in The Thirty-eighth Annual Conference on Neural Information Processing Systems, 2024. [27] J. Siekmann, Y. Godse, A. Fern, and J. Hurst, Sim-to-real learning of all common bipedal gaits via periodic reward composition, in IEEE International Conference on Robotics and Automation (ICRA), pp. 73097315, IEEE, 2021. [28] A. Kumar, Z. Li, J. Zeng, D. Pathak, K. Sreenath, and J. Malik, Adapting rapid motor adaptation for bipedal robots, in IEEE/RAS International Conference on Intelligent Robots and Systems (IROS), pp. 11611168, IEEE, 2022. HUANG et al.: ECO: ENERGY-CONSTRAINED OPTIMIZATION WITH REINFORCEMENT LEARNING FOR HUMANOID WALKING 15 [29] H. J. Lee, S. Hong, and S. Kim, Integrating model-based footstep planning with model-free reinforcement learning for dynamic legged locomotion, arXiv preprint arXiv:2408.02662, 2024. [30] S. H. Bang, C. A. Jové, and L. Sentis, Rl-augmented mpc framework for agile and robust bipedal footstep locomotion planning and control, arXiv preprint arXiv:2407.17683, 2024. [31] W. Yu, G. Turk, and C. K. Liu, Learning symmetric and low-energy locomotion, ACM Transactions on Graphics (TOG), vol. 37, no. 4, pp. 112, 2018. [32] Z. Su, X. Huang, D. Ordoñez-Apraez, Y. Li, Z. Li, Q. Liao, G. Turrisi, M. Pontil, C. Semini, Y. Wu, and K. Sreenath, Leveraging symmetry in rl-based legged locomotion control, in 2024 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS), pp. 68996906, 2024. [33] X. B. Peng, Y. Guo, L. Halper, S. Levine, and S. Fidler, Ase: Largescale reusable adversarial skill embeddings for physically simulated characters, ACM Transactions On Graphics (TOG), vol. 41, no. 4, pp. 117, 2022. [34] X. B. Peng, P. Abbeel, S. Levine, and M. Van de Panne, Deepmimic: Example-guided deep reinforcement learning of physics-based character skills, ACM Transactions On Graphics (TOG), vol. 37, no. 4, pp. 114, 2018. [35] J. Xu, K. Li, and M. Abusara, Preference based multi-objective reinforcement learning for multi-microgrid system optimization problem in smart grid, Memetic Computing, vol. 14, no. 2, pp. 225235, 2022. [36] K. Li and H. Guo, Human-in-the-loop policy optimization for preference-based multi-objective reinforcement learning, arXiv preprint arXiv:2401.02160, 2024. [37] E. Altman, Constrained Markov decision processes: stochastic modeling. Routledge, 1999. [38] S. Li, K. Li, W. Li, and M. Yang, Evolutionary alternating direction method of multipliers for constrained multi-objective optimization with unknown constraints, IEEE Transactions on Evolutionary Computation, 2024. [39] S. Wang and K. Li, Constrained bayesian optimization under partial observations: Balanced improvements and provable convergence, in Proceedings of the AAAI Conference on Artificial Intelligence, vol. 38, pp. 1560715615, 2024. [40] S. Paternain, L. F. O. Chamon, M. Calvo-Fullana, and A. Ribeiro, learning has zero duality gap, CoRR, Constrained reinforcement vol. abs/1910.13393, 2019. [41] J. Achiam, D. Held, A. Tamar, and P. Abbeel, Constrained policy optimization, in International conference on machine learning, pp. 22 31, PMLR, 2017. [42] A. Ray, J. Achiam, and D. Amodei, Benchmarking safe exploration in deep reinforcement learning, arXiv preprint arXiv:1910.01708, vol. 7, no. 1, p. 2, 2019. [43] P. Liu, D. Tateo, H. B. Ammar, and J. Peters, Robot reinforcement learning on the constraint manifold, in Conference on Robot Learning, pp. 13571366, PMLR, 2022. [44] P. Kicki, P. Liu, D. Tateo, H. Bou-Ammar, K. Walas, P. Skrzypczynski, and J. Peters, Fast kinodynamic planning on the constraint manifold with deep neural networks, IEEE Transactions on Robotics (T-RO), vol. 40, pp. 277297, 2023. [45] T. Xu, Y. Liang, and G. Lan, Crpo: new approach for safe reinforcement learning with convergence guarantee, in International Conference on Machine Learning, pp. 1148011491, PMLR, 2021. [46] Y. Liu, J. Ding, and X. Liu, Ipo: Interior-point policy optimization under constraints, in Proceedings of the AAAI conference on artificial intelligence, vol. 34, pp. 49404947, 2020. [47] L. Zhang, L. Shen, L. Yang, S. Chen, B. Yuan, X. Wang, and D. Tao, Penalized proximal policy optimization for safe reinforcement learning, arXiv preprint arXiv:2205.11814, 2022. [48] J. Lee, L. Schroth, V. Klemm, M. Bjelonic, A. Reske, and M. Hutlearning algorithms for ter, Exploring constrained reinforcement quadrupedal locomotion, in IEEE/RAS International Conference on Intelligent Robots and Systems (IROS), pp. 1113211138, IEEE, 2024. [49] J. Ji, B. Zhang, J. Zhou, X. Pan, W. Huang, R. Sun, Y. Geng, Y. Zhong, J. Dai, and Y. Yang, Safety gymnasium: unified safe reinforcement learning benchmark, Advances in Neural Information Processing Systems, vol. 36, 2023. [50] X. Gu, Y.-J. Wang, X. Zhu, C. Shi, Y. Guo, Y. Liu, and J. Chen, Advancing humanoid locomotion: Mastering challenging terrains with learning, arXiv preprint arXiv:2408.14472, denoising world model 2024. [51] D. P. Kingma, Adam: method for stochastic optimization, arXiv preprint arXiv:1412.6980, 2014. [52] J. Schulman, F. Wolski, P. Dhariwal, A. Radford, and O. Klimov, Proximal policy optimization algorithms, arXiv preprint arXiv:1707.06347, 2017. [53] J. Schulman, P. Moritz, S. Levine, M. Jordan, and P. Abbeel, Highdimensional continuous control using generalized advantage estimation, arXiv preprint arXiv:1506.02438, 2015. [54] N. Rudin, D. Hoeller, P. Reist, and M. Hutter, Learning to walk in minutes using massively parallel deep reinforcement learning, in Conference on Robot Learning, pp. 91100, PMLR, 2022. [55] R. J. Williams, Simple statistical gradient-following algorithms for connectionist reinforcement learning, Machine learning, vol. 8, pp. 229 256, 1992. [56] Y. Liu, J. Shen, J. Zhang, X. Zhang, T. Zhu, and D. Hong, Design and control of miniature bipedal robot with proprioceptive actuation for dynamic behaviors, in IEEE International Conference on Robotics and Automation (ICRA), pp. 85478553, IEEE, 2022. [57] BRUCE, kid-size humanoid robot open-platform for research and education. https://www.westwoodrobotics.io/bruce/. Accessed: 2024-0830. [58] J. Liang, V. Makoviychuk, A. Handa, N. Chentanez, M. Macklin, and D. Fox, Gpu-accelerated robotic simulation for distributed reinforcement learning, in Conference on Robot Learning, PMLR, 2018. [59] L. Pinto, M. Andrychowicz, P. Welinder, W. Zaremba, and P. Abbeel, Asymmetric actor critic for image-based robot learning, arXiv preprint arXiv:1710.06542, 2017. [60] R. P. Singh, Z. Xie, P. Gergondet, and F. Kanehiro, Learning bipedal walking for humanoids with current feedback, IEEE Access, 2023. [61] E. Todorov, T. Erez, and Y. Tassa, Mujoco: physics engine for modelbased control, in IEEE/RAS International Conference on Intelligent Robots and Systems (IROS), pp. 50265033, IEEE, 2012. [62] N. Koenig and A. Howard, Design and use paradigms for gazebo, an open-source multi-robot simulator, in IEEE/RAS International Conference on Intelligent Robots and Systems (IROS), vol. 3, pp. 21492154, IEEE, 2004. [63] P. G. Adamczyk, S. H. Collins, and A. D. Kuo, The advantages of rolling foot in human walking, Journal of experimental biology, vol. 209, no. 20, pp. 39533963, 2006. [64] D. Hafner, J. Pasukonis, J. Ba, and T. Lillicrap, Mastering diverse domains through world models, arXiv preprint arXiv:2301.04104, 2023. [65] A. Dietrich, C. Ott, and A. Albu-Schäffer, An overview of null space projections for redundant, torque-controlled robots, International Journal of Robotics Research (IJRR), vol. 34, no. 11, pp. 13851400, 2015. [66] J. Levy, T. Westenbroek, and D. Fridovich-Keil, Learning to walk from three minutes of real-world data with semi-structured dynamics models, arXiv preprint arXiv:2410.09163, 2024. [67] Y. As, I. Usmanova, S. Curi, and A. Krause, Constrained policy optimization via bayesian world models, arXiv preprint arXiv:2201.09802, 2022. [68] J.-P. Sleiman, F. Farshidian, and M. Hutter, Versatile multicontact planning and control for legged loco-manipulation, Science Robotics, vol. 8, no. 81, p. eadg5014, 2023. [69] M. Zhang, Y. Ma, T. Miki, and M. Hutter, Learning to open and traverse doors with legged manipulator, arXiv preprint arXiv:2409.04882, 2024. [70] A. Agarwal, A. Kumar, J. Malik, and D. Pathak, Legged locomotion in challenging terrains using egocentric vision, in Conference on robot learning, pp. 403415, PMLR, 2023. [71] E. Chane-Sane, J. Amigo, T. Flayols, L. Righetti, and N. Mansard, Soloparkour: Constrained reinforcement learning for visual locomotion from privileged experience, in Conference on Robot Learning, arXiv, 2024. [72] W. Yu, D. Jain, A. Escontrela, A. Iscen, P. Xu, E. Coumans, S. Ha, J. Tan, and T. Zhang, Visual-locomotion: Learning to walk on complex terrains with vision, in 5th Annual Conference on Robot Learning, 2021. [73] N. Hansen, J. SV, V. Sobal, Y. LeCun, X. Wang, and H. Su, Hierarchical world models as visual whole-body humanoid controllers, arXiv preprint arXiv:2405.18418, 2024. [74] Y. Ma, F. Farshidian, T. Miki, J. Lee, and M. Hutter, Combining learning-based locomotion policy with model-based manipulation for legged mobile manipulators, IEEE Robotics and Automation Letters, vol. 7, no. 2, pp. 23772384, 2022. 16 IEEE TRANSACTIONS ON AUTOMATION SCIENCE AND ENGINEERING. PREPRINT VERSION. ACCEPTED FEB, 2026 Weidong Huang received the M.S. degree from the Beihang University in 2024. He is now research engineer at State Key Laboratory of General Artificial Intelligence, Beijing Institute for General Artificial Intelligence (BIGAI). He received his B.S. degree in South China Normal University in 2021. His research interests include robotics, reinforcement learning, and model-based planning. Jingwen Zhang (Member, IEEE) received the B.S. degree and the M.S. degree from the Department of Mechanical Engineering of Tsinghua University and Columbia University in 2015 and 2017, and the Ph.D. degree from the Department of Mechanical and Aerospace Engineering, University of California, Los Angeles in 2023. He is now research scientist at State Key Laboratory of General Artificial Intelligence, Beijing Institute for General Artificial Intelligence (BIGAI). His research interests include robotics, optimal control, and motion planning. is now third-year student Jiongye Li in the Department of Automation of Tsinghua University. He is now intern at State Key Laboratory of General Artificial Intelligence, Beijing Institute for General Artificial Intelligence (BIGAI). His research interests include humanoid robots, reinforcement learning and optimal control. Hangxin Liu (Member, IEEE) received his Ph.D. degree in Computer Science from the University of California, Los Angeles (UCLA) in 2021. He is currently research scientist at State Key Laboratory of General Artificial Intelligence, Beijing Institute for General Artificial Intelligence (BIGAI). He received an M.S. degree in Mechanical Engineering from UCLA in 2018 and two B.S. degrees in Mechanical Engineering and Computer Science, both from Virginia Tech in 2016. His research interests focus on robot perception, learning, human-robot interaction, and cognitive robotics. Yaodong Yang (Member, IEEE) received the bachelors degree from the University of Science and Technology of China, the MSc degree from Imperial College London, and the PhD degree from University College London (nominated by UCL for Joint AAAI/ACM SIGAI Doctoral Dissertation Award). He is an assistant professor with Institute for AI, Peking University. Before joining Peking University, he was an assistant professor with Kings College London. He studies game theory, reinforcement learning and multi-agent systems, aiming to achieve artificial general collective intelligence through multi-agent reinforcement learning. He has maintained track record of more than sixty publications at top conferences (NeurIPS, ICML, ICLR, etc) and top journals (Artificial Intelligence, National Science Review, etc), along with the best system paper award at CoRL 2020 and the best blue-sky paper award at AAMAS 2021. He was awarded ACM SIGAI China Rising Star and World AI Conference (WAIC22) Rising Star. Yao Su (Member, IEEE) received the B.S. degree from the School of Mechatronic Engineering, Harbin Institute of Technology in 2016, and the M.S. and Ph.D. degrees from the Department of Mechanical and Aerospace Engineering, University of California, Los Angeles (UCLA) in 2017 and 2021. He is now research scientist at State Key Laboratory of General Artificial Intelligence, Beijing Institute for General Artificial Intelligence (BIGAI). His research interests include robotics, control, optimization, trajectory planning, and mechatronics. Shibowen Zhang received the B.E. degree in Control Science and Engineering, from Tongji University, Shanghai, China, in 2024. He is currently first-year Ph.D. student at the School of Control Science and Engineering, University of Science and Technology of China (USTC). His research interests include robotics and control. Jiayang Wu is currently senior undergraduate in the Department of Computer Science student and Technology at Harbin Institute of Technology (HIT). He is also an intern research assistant at the Beijing Institute for General Artificial Intelligence (BIGAI). His research interests include robotics, optimal control, and reinforcement learning. Jiayi Wang (Member, IEEE) received his Ph.D. degree in robotics from the University of Edinburgh in 2023. He is currently Research Scientist at Beijing Institute for General Artificial Intelligence (BIGAI). Before that, he was research associate at the University of Edinburgh in 2024 and was research intern at the Italian Institute of Technology in 2017. His research interests include humanoid locomotion, multi-contact motion planning, reinforcement learning, and trajectory optimization."
        }
    ],
    "affiliations": [
        "Department of Automation, Tsinghua University",
        "Department of Automation, University of Science and Technology of China",
        "Department of Computer Science, Harbin Institute of Technology",
        "Institute for Artificial Intelligence and School of Artificial Intelligence, Peking University",
        "State Key Laboratory of General Artificial Intelligence, Beijing Institute for General Artificial Intelligence (BIGAI)"
    ]
}