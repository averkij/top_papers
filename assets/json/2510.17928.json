{
    "paper_title": "EvoSyn: Generalizable Evolutionary Data Synthesis for Verifiable Learning",
    "authors": [
        "He Du",
        "Bowen Li",
        "Aijun Yang",
        "Siyang He",
        "Qipeng Guo",
        "Dacheng Tao"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Reliable verifiable data has become a key driver of capability gains in modern language models, enabling stable reinforcement learning with verifiable rewards and effective distillation that transfers competence across math, coding, and agentic tasks. Yet constructing generalizable synthetic verifiable data remains difficult due to hallucination-prone generation, and weak or trivial verification artifacts that fail to separate strong from weak solutions. Existing approaches often rely on task-specific heuristics or post-hoc filters that do not transfer across domains and lack a principled, universal evaluator of verifiability. In this work, we introduce an evolutionary, task-agnostic, strategy-guided, executably-checkable data synthesis framework that, from minimal seed supervision, jointly synthesizes problems, diverse candidate solutions, and verification artifacts, and iteratively discovers strategies via a consistency-based evaluator that enforces agreement between human-annotated and strategy-induced checks. This pipeline upgrades filtering into principled synthesis: it reliably assembles coherent, verifiable training instances and generalizes without domain-specific rules. Our experiments demonstrate the effectiveness of the proposed approach under both RLVR and model distillation training paradigms. The results show that training with our synthesized data yields significant improvements on both the LiveCodeBench and AgentBench-OS tasks, highlighting the robust generalization of our framework."
        },
        {
            "title": "Start",
            "content": "EVOSYN: GENERALIZABLE EVOLUTIONARY DATA SYNTHESIS FOR VERIFIABLE LEARNING He Du1 2 Bowen Li2 Aijun Yang2 1Fudan University elyndendu@gmail.com , libowen.ne@gmail.com 2Shanghai AI Laboratory Siyang He2 Qipeng Guo2 Dacheng Tao3 3Nanyang Technological University 5 2 0 O 0 2 ] . [ 1 8 2 9 7 1 . 0 1 5 2 : r a"
        },
        {
            "title": "ABSTRACT",
            "content": "Reliable verifiable data has become key driver of capability gains in modern language models, enabling stable reinforcement learning with verifiable rewards and effective distillation that transfers competence across math, coding, and agentic tasks. Yet constructing generalizable synthetic verifiable data remains difficult due to hallucination-prone generation, and weak or trivial verification artifacts that fail to separate strong from weak solutions. Existing approaches often rely on task-specific heuristics or post-hoc filters that do not transfer across domains and lack principled, universal evaluator of verifiability. In this work, we introduce an evolutionary, task-agnostic, strategy-guided, executably-checkable data synthesis framework that, from minimal seed supervision, jointly synthesizes problems, diverse candidate solutions, and verification artifacts, and iteratively discovers strategies via consistency-based evaluator that enforces agreement between human-annotated and strategy-induced checks. This pipeline upgrades filtering into principled synthesis: it reliably assembles coherent, verifiable training instances and generalizes without domain-specific rules. Our experiments demonstrate the effectiveness of the proposed approach under both RLVR and model distillation training paradigms. The results show that training with our synthesized data yields significant improvements on both the LiveCodeBench and AgentBench-OS tasks, highlighting the robust generalization of our framework. The code and data are available at https://github.com/kinza99/openevolve."
        },
        {
            "title": "INTRODUCTION",
            "content": "Large language models (LLMs) have demonstrated remarkable potential across wide range of domains, particularly in complex reasoning tasks such as mathematics, programming, and real-world agent applications. Recently, models like OpenAI-o1 and DeepSeek-R1 (Guo et al., 2025; OpenAI, 2024; Yang et al., 2025), after undergoing large-scale reinforcement learning, have shown significant improvements on reasoning benchmarks (Yue et al., 2025; Su et al., 2025). However, as model capabilities rapidly advance, their size continues to grow, and their demand for data is expanding at an astonishing pace. In particular, recent training paradigms increasingly rely on special class of dataverifiable data. Verifiable data provides reliable feedback signals during training, making it indispensable for many approaches. For example, RLVR-style training methods and model distillation heavily rely on such data (Schulman et al., 2017; Shao et al., 2024b; Zhao et al., 2025); DPO (Hosseini et al., 2024; Lai et al., 2024) leverages feedback to construct positive and negative samples; and various self-training methods such as STaR (Zelikman et al., 2022), V-STaR (Hosseini et al., 2024), and ReST (Singh et al., 2023) all depend on correctness signals to filter useful examples. However, the stringent reliability requirements of verifiable data make it extremely costly to annotate. Large-scale manual labeling is simply infeasible, highlighting the growing importance of verifiable data in modern LLM training pipelines. Synthetic data offers promising solution, but it remains imperfect (Liu et al., 2024; Long et al., 2024; Nadas, et al., 2025). Two persistent challenges limit its utility. First, reliability: hallucinations Corresponding author. remain fundamental weakness of LLMs. While models can generate large volumes of data, ensuring their reliability is nontrivial (Ding et al., 2024; He et al., 2025). How to make model-generated data more reliable or how to effectively filter trustworthy subsets from large synthetic corpora remains central challenge. Second, generalizability: Many existing solutions rely on task-specific, handcrafted heuristics to guarantee data usability. For example, some studies validate correctness through syntax checking (Wang et al., 2025). These approaches, however, often fail to generalize beyond the narrow task domains they were designed for. In this work, we focus on these two questions: how to obtain reliable, verifiable data, and how to design unified pipeline that generalizes across diverse tasks. We target the executably-checkable data class, which is the major part of verifiable data. We propose general-purpose framework for synthesizing reliable data, called Evolutionary Data Synthesis (EvoSyn). Executably-checkable tasks are broad class of problems defined as those for which verification can be performed via tests without requiring complete solution. This class encompasses challenging real-world tasks, such as coding and software engineering problems. In our experiments, we select representative and high-difficulty tasks: the algorithmic LiveCodeBench (Jain et al., 2024) and the complex agent task AgentBench-OS (Liu et al., 2023). The core idea of EvoSyn is to formulate the difficulty as data filtering strategy optimization task. Inspired by AlphaEvolve (Novikov et al., 2025), we employ evolutionary algorithms to iteratively search for the optimal filtering strategy tailored to the current task (Sharma, 2025; Romera-Paredes et al., 2024; Tanese, 1989). This strategy is then applied to synthetic data, yielding reliable, verifiable dataset. Unlike prior approaches that require handcrafted, task-specific heuristics, EvoSyn automates this process: the model itself explores and evolves filtering strategies, reducing manual effort while producing superior solutions. Crucially, EvoSyn introduces unified evaluation criterion for filtering strategies, which is task-agnostic. Instead of relying on domain-specific signals, EvoSyn measures consistency score with small set of manually verified seed data, making it applicable to any verification task as long as minimal seed supervision is available. Figure 1: Overview of EvoSyn, task-agnostic pipeline for synthesizing verifiable data. From small human-verified seed data, an evolutionary process discovers data-filtering strategy via consistency-based evaluator; this strategy then guides synthesis by generating candidate solutions and tests for new problems, cross-executing them to rank and retain reliable instances while discarding trivial or inconsistent ones. The resulting verifiable dataset (problems, tests, and strong solutions) supports training in diverse tasks. We demonstrate that EvoSyn is both effective and generalizable. Through its evolutionary process, EvoSyn continuously discovers novel and increasingly powerful strategies over iterations. We showcase representative examples and provide detailed analysis of how strategy quality improves as the number of evolutionary rounds increases. Next, we validate EvoSyn on model training. On LiveCodeBench (Jain et al., 2024), we conduct RLVR training, and EvoSyn-generated data significantly improve the performance of LLaMA-3.1 (Grattafiori et al., 2024) and Qwen3 (Yang et al., 2025) models, outperforming raw synthetic baselines and providing more effective training dynamics. On the challenging AgentBench-OS benchmark, we choose the representative model distillation method, EvoSyn also yields substantial gains, enabling distilled models to surpass not only random baselines but also their teacher model (DeepSeek-R1 (Guo et al., 2025)). Our main contributions are: We introduce Evolutionary Data Synthesis (EvoSyn), general framework for synthesizing verifiable data. EvoSyn automatically evolves superior data filterering strategies for the given task, enabling the construction of reliable synthetic datasets. We provide detailed study of EvoSyns evolutionary process, demonstrating its effectiveness, generalizability, and cost trade-offs. We validate EvoSyn on two important training paradigms, RLVR and model distillation, showing that EvoSyn-generated data yields substantial improvements over baselines."
        },
        {
            "title": "2 RELATED WORK",
            "content": "Verifiable learning Verifiable learning leverages executable or checkable feedback to supervise model training and spans both RL with verifiable rewards (RLVR) (Lambert et al., 2025) and supervised fine-tuning/distillation. In RLVR (Schulman et al., 2017; Shao et al., 2024b; Guo et al., 2025; OpenAI, 2024; Yang et al., 2025), correctness signals from program execution, unit tests, or other deterministic checkers stabilize training and markedly enhance reasoning ability. Beyond RLVR, teacher outputs can be filtered by execution in model distillation (Kim et al., 2025); and self-training pipelines such as RFT, STaR, and ReST (Singh et al., 2023; Zhang et al., 2024; Zelikman et al., 2022) rely on correctness signals to retain useful data. Verification feedback also constructs preference data for DPO (Hosseini et al., 2024; Lai et al., 2024; Rafailov et al., 2024) and improves reward models (Wang et al., 2023). Data synthesis Synthesizing verifiable data is critical yet challenging (Liu et al., 2024; Long et al., 2024; Nadas, et al., 2025). In practice, high-quality data for executably-checkable data often require broad-coverage unit tests (Chen et al., 2022; Wang et al., 2025), program-analysis tooling (Liang et al., 2025), or carefully curated exemplars (Shao et al., 2024a). Such task-specific heuristics incur high manual costs and transfer poorly to complex real-world reasoning tasks (Fandina et al., 2025; Jimenez et al., 2023; Zhang et al., 2025; Li et al., 2024). Hallucination further undermines reliability, making robust verification artifacts themselves central bottleneck (Long et al., 2024)."
        },
        {
            "title": "3 METHODOLOGY",
            "content": "To address the inherent unreliability of synthetic data, we propose new approach, Evolutionary Data Synthesis (EvoSyn). EvoSyn targets executably-checkable tasks that satisfy two conditions: (1) correctness can be decided by executable testing artifacts (e.g., unit tests, checkers, environment assertions) that deterministically accept or reject candidate solutions; and (2) such testing artifacts can be authored without first producing correct solution (e.g., via specifications, invariants, metamorphic relations, equivalence classes, boundary/edge cases). As illustrated in Figure 1, EvoSyn consists of three core stages: (1) Deriving data filtering strategy: deriving reliable strategy using evolutionary algorithms. (2) Data synthesis and filtering: synthsizing data and filtering them with the derived strategy. (3) Model training: training models on the filtered synthetic data. The objective of EvoSyn is to establish effective and automated mechanism that systematically enhances the reliability of synthetic data. 3.1 DERIVING DATA FILTERING STRATEGY In the context of synthetic data, the central challenge is creating effective testing mechanisms that can reliably verify candidate solutions. high-quality data instance typically consists of two components: problem description and its corresponding testing set. Producing reliable testings is highly difficult because testings must not only reflect an understanding of the problem but also need to steadily distinguish correct from incorrect solutions. If testing cannot differentiate solution quality, the instance becomes unreliable even if the problem itself appears well-formed. Therefore, improving the reliability of testings is the central focus of our method. We require filtered data to satisfy two conditions: (1) the problem must be solvable, and (2) the testing must reliably distinguish correct from incorrect solutions. In practice, the second condition is Figure 2: Given an initial strategy, the evolutionary algorithm iteratively optimizes it across multiple iterations. Each newly generated strategy is evaluated against our two criteria to determine its effectiveness. The model autonomously explores diverse optimization approaches, ensuring balance between exploration and exploitation throughout the process. more challenging. Reliable testings must consistently and correctly distinguish between correct and wrong solutions, whereas proving that problem is solvable only requires the existence of at least one solution that passes. Hence, reliability is the cornerstone of strategy design and optimization. Formally, we model data filtering strategy as function that evaluates the quality of set of testings. To derive such strategy, we leverage seed data with human-annotated problems and testings. relatively strong model is tasked with generating multiple candidate solutions for each problem, as well as additional testings based only on the problem description. These solutions and testings serve as inputs to the filtering strategy, which outputs ranked list of solutions (optional) and testings. Inspired by the success of Novikov et al. (2025), we adopt an evolutionary algorithm to iteratively improve the strategy. Evolutionary algorithms can balance exploration and exploitation. Following Novikov et al. (2025) and Sharma (2025), our implementation combines the MAP-Elites algorithm (Mouret & Clune, 2015) with island-based population models (Romera-Paredes et al., 2024; Tanese, 1989), enabling optimization over user-defined feature dimensions while maintaining population diversity. As in Novikov et al. (2025), the evolutionary process requires an initial filtering strategy, which need not be optimal. We design the initial strategy according to two intuitive principles: (1) solutions that pass more testings are considered better; (2) testings that validate more solutions are considered better. Although this initialization is imperfect, for example, testing that passes all solutions is likely uninformative, it suffices to bootstrap the evolutionary process, which will refine and correct such limitations. The evolutionary process also requires an evaluator, whose role is to assess strategy quality with respect to user-defined criteria. We define good strategy as one that ranks testings in close agreement with human-annotated testings on seed data across diverse candidate solutions. Specifically, the method for evaluating the quality of strategy includes two strict criteria: Criterion-1: the top-ranked solution produced by the strategy must correctly pass the human-annotated testing in the seed data. Criterion-2: for the ranked solutions, both the best and the worst solutions must exhibit consistent behavior on the annotated testing and on the best testing selected by the strategy. Figure 2 illustrates the actual workflow of our core method. Starting from an initial strategy, in each iteration the model explores various ways to obtain better filtering strategy. From analyzing several relatively high-scoring strategies, we observe that the model explores multiple directions, such as refining the computation of solution quality and experimenting with different weighting schemes for testing. After each attempt, the evaluator assesses whether the new strategy satisfies our two predefined criteria on every instance in the seed data, and the proportion of satisfied cases is then used as the final score of the strategy, guiding the next round of evolution and refinement. Remarkably, the evolutionary process yields multiple elegant and effective strategies. Figure 3 presents the best strategy evolved by model. The strategy scores each solution by the number of tests it passes, while testing scores are based on discriminative power (i.e., the gap between solutions score that pass and fail), with both solution and testing scores normalized before computing discriminative power. Apart from this best one, model could explore various ways of computing testing scores. For example, TF-IDF-like approach: solutions that pass difficult testings receive higher scores, where difficulty is defined as testings passed by only few solutions; Coverage-based approach: solutions are rewarded simply for passing more testings, while testing quality is measured by its discriminative power (i.e., the score gap between solutions that pass and those that fail); Inverse filtering approach: contrary to the initial strategy, testings that fewer solutions can pass are considered better; Exclusion-based approach: the contribution of testing is measured without its own influence, by weighting solutions that pass all other testings and Hardness-Aware approach: solutions are ranked by test strictness and pass count, penalizing all-or-none tests to select the strongest solution and most discriminative tests. The details of these strategies are illustrated in the Appendix A.3. These evolved strategies demonstrate strong internal logic and significantly improve upon manually designed baselines, showing that evolutionary search can efficiently discover high-quality filtering strategies. Figure 3: The best strategy explored by model on LiveCodeBench. 3.2 DATA SYNTHESIS AND FILTERING With robust filtering strategy in place, we proceed to data synthesis and filtering. Specifically, we first synthesize new problems to replace the human-annotated seed data. To ensure the generated problems are compatible with the filtering strategy, we provide seed instances as in-context examples to guide problem generation. After deduplication, the synthesized problems form new set D. For each problem in D, we generate candidate solutions and candidate testings, which serve as inputs to the filtering strategy. The strategy ranks both solutions and testings. We then perform final filtering step called Zero-Variance Pruning: we discard instances in which the testings yield no ranking variation. Such cases typically indicate either unreliable testings or trivial problems where all testings perform equally well. In both scenarios, discarding the instance is justified. 3.3 MODEL TRAINING Following the above steps, we obtain reliable synthetic dataset containing problem descriptions and their associated testings. As byproduct, we also retain the strongest solutions generated by the model. This dataset can be leveraged in various training paradigms, such as RLVR and model distillation, thereby boosting downstream model performance."
        },
        {
            "title": "4 EXPERIMENTS",
            "content": "4.1 EXPERIMENTAL SETUP This section presents comprehensive set of experiments designed to validate the effectiveness of our proposed method. To address the specific challenge of verifiable problem synthesis, which is the core focus of our work, we conduct evaluations on two distinct and representative benchmarks: LiveCodeBench (Jain et al., 2024) and AgentBench-OS (Liu et al., 2023). LiveCodeBench is highly challenging coding task benchmark, featuring continuously updated collection of difficult programming problems. It has recently garnered significant attention within the large language model community due to its focus on real-time problem-solving capabilities. AgentBench-OS is subset of the AgentBench benchmark, specifically designed to evaluate models performance in realistic operating system environment. This benchmark assesses models ability to act as an intelligent agent and execute code to complete given tasks. The final performance is rigorously verified through series of predefined tests. To validate the effectiveness of our method, we conduct experiments on two representative training paradigms: reinforcement learning with verifiable rewards (RLVR) (Guo et al., 2025) and model distillation. Considering both cost and task complexity, we employ DeepSeek-V31 and DeepSeek-R1 as teacher models for the two tasks, respectively. The teacher model is responsible for the entire data synthesis pipeline, including the filtering strategy and data generation. During the evolutionary process for each task, we synthesize = 16 solutions and their corresponding testing for every problem instance. The maximum number of evolutionary iterations is set to 20. (a) Evolutionary process over 20 iterations. (b) Effect of different (M, ). Figure 4: Evolutionary process and data-retention trade-off. (a) The evolutionary process consistently discovers stronger strategies, with the best strategy surpassing the initialization by over 10 percentage points within 20 iterations. Score denotes the ratio of seed data instances for which consistency verification is satisfied. (b) Increasing the number of and yields more usable, verifiable instances but incurs O(M ) testing execution cost. 4.2 EVOLUTIONARY PROCESS In this set of experiments, we use the LivecodeBench task as an example to illustrate the effectiveness of our core evolutionary method in the data synthesis process. As shown in Figure 4a, within the limit of 20 evolutionary iterations, and after excluding few strategies that contained bugs, we frequently observe strategies outperforming the initial baseline. In particular, the best strategy exceeds the initial one by more than 10 percentage points, demonstrating both the models ability to explore diverse strategies and the effectiveness of applying evolutionary algorithms to this problem. Moreover, the overall trend of the evolutionary process shows steady upward trajectory. This suggests that, with more iterations, there is strong potential to discover even better filtering strategies to guide data synthesis, highlighting the feasibility of our approach. 1Due to the excessively long chains-of-thought (CoT) produced by DeepSeek-R1 on algorithmic problems, which lead to slow inference, we use DeepSeek-V3 as the teacher model for the LiveCodeBench task. We also observe poor performance in instruction following during question generation. Ablation study 1: Impact of and However, better strategies also imply stricter filtering standards. To investigate this, we apply the best evolved strategy to data synthesis while varying the value of and . The choice of and has significant impact on synthesis cost, since our method requires generating solutions and testings, followed by executions. This quadratic growth in cost makes it crucial to understand the relationship between (M, ) and the amount of usable data ultimately obtained. As shown in Figure 4b, when applying to the same set of 1,250 problems, using = = 4, = = 8, and = = 16 produces markedly different amounts of usable data. The reason is straightforward: with fewer samples, the likelihood of obtaining diverse solutions and testings decreases, making it harder to generate varied feedback and, consequently, to verify reliability. Table 1: Consistency validation on = 16 solutions. We vary and validate the same strategy using the top-K and bottom-K solution subsets. Adding Criterion-1 at = 1 yields the strictest check while requiring 8 fewer executions (#Exec = 4 vs. 32) than omitting it at = 8. Increasing alone shows diminishing returns. Ablation Study 2: Is Consistency Validation Sufficient with Only the Best and Worst Solutions? Recalling our two evaluation criteria for strategy assessment: Criterion-1, the best solution must be correct; Criterion-2 the performance of the best and worst solutions must agree on both the human-annotated test set and the strategy-selected best test. natural question arises: are these criteria sufficient? To investigate, we vary the number of solutions used for evaluating same strategy (we use the initial strategy as an example) and choosing = 16, considering the best and worst solutions with = 1, 2, 4 and = 8 (i.e., M/2). As shown in Table 1, increasing indeed strengthens the evaluation criteria, reflects in lower overall scores. However, two observations emerge. First, the stricter constraint does not scale linearly with K: validating more solutions does not necessarily yield proportionally more accurate evaluations, largely due to randomness in solution sampling. Second, our setting combining the two criteria with = 1, is in fact stricter than the = 8 case, achieving both higher accuracy and significantly greater efficiency. 0.589 0.554 0.536 0.536 Criterion-1 No No No No 0.482 0.482 4 8 16 32 Yes Yes #Exec Score 1 2 4 8 4 32 Ours 1 In our experiments, although the proposed method is in principle capable of generating unlimited data and producing highly reliable testings, from Figure 4b, we observe log-linear relationship between the number of usable data instances and the number of testing executions. The underlying reason lies in the difficulty of controlling the diversity of model outputs. Low diversity inevitably requires larger values of and , which substantially increases the cost. In future work, we aim to further investigate methods to enhance output diversity while reducing synthesis costs. In addition, practical bottlenecks such as slow model inference, time-intensive unit test verification, and costly environment setup further constrain the scalability of our data synthesis. We therefore adopt = 16, yielding over 200 instances for LiveCodeBench and over 600 instances for AgentBench-OS. Despite this relatively small scale, training on these data still leads to substantial performance improvements. 4.3 EVOSYN FOR RLVR This set of experiments demonstrates that synthetic data generated with our method can effectively improve model performance in the RLVR task. We construct three data settings based on 51 seed instances: DEvoSyn: Data filtered using our proposed data filtering strategy. Drandom: Data with exactly the same problems as DEvoSyn, but instead of using the filtering strategy, we randomly select one testing from the candidates as the final testing. DEvoSynrelaxed : Data obtained by relaxing the Zero-Variance Pruning, to investigate the necessity of our methods final filtering condition, which excludes instances that have not undergone ranking. In particular, we analyze the number of unit tests per synthesized data in DEvoSyn. As shown in Figure 9, the synthesized data contain an average of 11.5 unit tests, including various edge cases such as extremely long inputs. To mitigate the strong dependency on long-context capability imposed by such edge cases, we further adjust the testing generation process: instead of asking the model to directly produce unit tests, we require it to output code from which unit tests can be constructed. This not only preserves the diversity of unit tests but also ensures that the number of tests per problem remains sufficiently large. Table 2: RLVR results on LiveCodeBench: Training on EvoSyn-filtered data (DEvoSyn) consistently improves accuracy across models, outperforming random selection (Drandom) and the relaxed variant (DEvoSynrelaxed ). denotes absolute gain over the baselines. Model Data Setting Dataset Size Accuracy DeepSeek-V3 Qwen3-4B Llama-3.1-8B Qwen3-8B Qwen3-4B Qwen3-4B Llama-3.1-8B Llama-3.1-8B Qwen3-8B Qwen3-8B Qwen3-8B Baseline - - - - - - - - RLVR Models 231 231 231 231 231 231 256 DEvoSyn Drandom DEvoSyn Drandom DEvoSyn Drandom DEvoSynrelaxed 36.3 17.0 1.6 16.5 22.0 19.9 15.7 11.1 24.8 21.1 24.4 - - - - +5.0 +2.9 +14.1 +9.5 +8.3 +4.6 +7. Results We conduct reinforcement learning experiments on Qwen3-8B, Qwen3-4B, and Llama3.1-8B using GRPO. As shown in Table 2, training on our synthesized dataset DEvoSyn consistently yields significant performance gains across all models. Notably, on Llama-3.1-8B, we observe substantial improvement of 14.1%. This demonstrates the effectiveness of our method in synthesizing reliable data. (a) Llama-3.1-8B (b) Qwen3-8B (c) Qwen3-4B Figure 5: RLVR reward curves comparison across models. EvoSyn-filtered data (DEvoSyn) yields faster, steadier reward growth than random selection (Drandom). Ablation Study 3: What drives this advantage? To further demonstrate the effectiveness of our method, we compare it against randomly synthesized data without filtering. As shown in Table 2, although training with randomly synthesized data on Qwen3-8B also yields some improvement, indicating that portion of the data is indeed learnable, the performance still lags significantly behind that achieved with our filtered dataset. This result can be further analyzed through the reward dynamics during training. As illustrated in Figure 5, training on DEvoSyn exhibits steady and meaningful increase in reward, whereas training on Drandom struggles to achieve consistent reward growth. This comparison highlights that the data constructed by our method is substantially more learnable for the model. Ablation Study 4: Is the Zero-Variance Pruning necessary? In addition, we analyze the differ- . By design, DEvoSyn is strict subset of DEvoSynrelaxed ences between DEvoSyn and DEvoSynrelaxed . We manually examine the 25 additional instances present to DEvoSynrelaxed but not in DEvoSyn, and find that nearly all of them were overly simple problems. On average, their solution code lengths are only dozen lines, and in some cases, the solutions sampled at temperature 1.0 are completely identical. This observation validates the rationale behind the Zero-Variance Pruning in our method, which removes overly simple problems that provide little value for model learning. Such trivial problems are particularly problematic for the RLVR paradigm, as they prevent proper computation of the advantage. Selected example is provided in the Appendix A.4. 4.4 EVOSYN FOR MODEL DISTILLATION Table 3: Model distillation results on AgentBenchOS: EvoSyn-filtered data (DEvoSyn) yields large gains across students, outperforming random selection (Drandom). Remarkably, all students exceed the teacher (DeepSeek-R1, 30.1). Model Data Setting Accuracy Baseline DeepSeek-R1 Qwen3-4B Llama-3.1-8B Qwen3-8B - - - - 30.1 1.0 1.0 1. - - - - 40.0 36.0 37.6 22.0 44.9 32.8 Qwen3-4B Qwen3-4B Llama-3.1-8B Llama-3.1-8B Qwen3-8B Qwen3-8B Distilled Models DEvoSyn Drandom DEvoSyn Drandom DEvoSyn Drandom Model distillation has been widely adopted in the field due to its effectiveness and high efficiency, making it powerful alternative to reinforcement learning, especially when the latters training costs become prohibitive. Similar to RLVR, this method critically depends on high-quality set of problems and reliable testings. This robust evaluation mechanism is essential for accurately filtering the correct responses from teacher model. In this experiment, we select the AgentBench-OS task, which is highly realistic agent task requiring multi-turn, complex reasoning. These abilities are often significant weakness for many models, particularly smaller ones. Due to the tasks complex environment setup (the need for isolated Docker environments), the associated costs are prohibitively high, making RLVR-based training difficult. Therefore, we experimentally validate the effectiveness of our proposed method within model distillation pipeline. We use OpenHands (Wang et al., 2024) as the agent framework for our models. We filter the original AgentBench-OS dataset due to the presence of samples with stringent time requirements or permission issues. From the initial 144 data points, we retain 129, which are subsequently used as both the evaluation set and the seed data for our proposed method. This curation process ensures the reliability and reproducibility of our experimental results by focusing on stable and accessible subset of the benchmark. +39.0 +35.0 +36.6 +21.0 +43.9 +31. Results Based on our method, we synthesize 673 data instances and obtain the corresponding outputs from DeepSeek-R1. Using this synthetic dataset, we train Qwen3-4B, Llama-3.1-8B, and Qwen3-8B. As shown in Table 3, all models exhibit substantial performance improvements after training. This not only highlights the weaker baseline performance of smaller models on complex, multi-turn, long-chain reasoning tasks but also clearly demonstrates the effectiveness of our synthetic data generation method. Furthermore, training on data synthesized by our method significantly outperforms training on randomly synthesized data, indicating that our approach is more effective at filtering usable data in complex, real-world agent tasks."
        },
        {
            "title": "5 CONCLUSION",
            "content": "We introduce EvoSyn, task-agnostic evolutionary data synthesis framework that focuses on synthesizing verifiable data for executably-checkable tasks by evolving robust filtering strategies from minimal seed supervision via consistency-based evaluator. By turning ad hoc filtering into principled strategy optimization, EvoSyn assembles coherent, verifiable training instances that transfer across domains. On LiveCodeBench (RLVR) and AgentBench-OS (distillation), training on EvoSyn-filtered data yields substantial gains and superior learning dynamics across Llama-3.1-8B and Qwen3-4B/8B, with distilled students surpassing the teacher on complex multi-turn agentic tasks. Ablations confirm the value of strategy evolution and Zero-Variance Pruning, and characterize costquality trade-offs in execution. Limitations include verification/execution cost and output-diversity bottlenecks. Future work will scale population search, improve diversity-aware generation, and broaden verification tooling and domains."
        },
        {
            "title": "REFERENCES",
            "content": "Bei Chen, Fengji Zhang, Anh Nguyen, Daoguang Zan, Zeqi Lin, Jian-Guang Lou, and Weizhu Chen. Codet: Code generation with generated tests. arXiv preprint arXiv:2207.10397, 2022. Bosheng Ding, Chengwei Qin, Ruochen Zhao, Tianze Luo, Xinze Li, Guizhen Chen, Wenhan Xia, Junjie Hu, Anh Tuan Luu, and Shafiq Joty. Data augmentation using large language models: Data perspectives, learning paradigms and challenges, 2024. URL https://arxiv.org/abs/ 2403.02990. Ora Nova Fandina, Eitan Farchi, Shmulik Froimovich, Rami Katan, Alice Podolsky, Orna Raz, and Avi Ziv. Automated validation of llm-based evaluators for software engineering artifacts. arXiv preprint arXiv:2508.02827, 2025. Aaron Grattafiori, Abhimanyu Dubey, Abhinav Jauhri, Abhinav Pandey, Abhishek Kadian, Ahmad Al-Dahle, Aiesha Letman, Akhil Mathur, Alan Schelten, Alex Vaughan, et al. The llama 3 herd of models. arXiv preprint arXiv:2407.21783, 2024. Daya Guo, Dejian Yang, Haowei Zhang, Junxiao Song, Ruoyu Zhang, Runxin Xu, Qihao Zhu, Shirong Ma, Peiyi Wang, Xiao Bi, et al. Deepseek-r1: Incentivizing reasoning capability in llms via reinforcement learning. arXiv preprint arXiv:2501.12948, 2025. Junliang He, Ziyue Fan, Shaohui Kuang, Li Xiaoqing, Kai Song, Yaqian Zhou, and Xipeng Qiu. FiNE: Filtering and improving noisy data elaborately with large language models. In Luis Chiruzzo, Alan Ritter, and Lu Wang (eds.), Proceedings of the 2025 Conference of the Nations of the Americas Chapter of the Association for Computational Linguistics: Human Language Technologies (Volume 1: Long Papers), pp. 86868707, Albuquerque, New Mexico, April 2025. Association for Computational Linguistics. ISBN 979-8-89176-189-6. doi: 10.18653/v1/2025. naacl-long.437. URL https://aclanthology.org/2025.naacl-long.437/. Arian Hosseini, Xingdi Yuan, Nikolay Malkin, Aaron Courville, Alessandro Sordoni, and Rishabh Agarwal. V-star: Training verifiers for self-taught reasoners. arXiv preprint arXiv:2402.06457, 2024. Naman Jain, King Han, Alex Gu, Wen-Ding Li, Fanjia Yan, Tianjun Zhang, Sida Wang, Armando Solar-Lezama, Koushik Sen, and Ion Stoica. Livecodebench: Holistic and contamination free evaluation of large language models for code. arXiv preprint arXiv:2403.07974, 2024. Carlos Jimenez, John Yang, Alexander Wettig, Shunyu Yao, Kexin Pei, Ofir Press, and Karthik Narasimhan. Swe-bench: Can language models resolve real-world github issues? arXiv preprint arXiv:2310.06770, 2023. Minwu Kim, Anubhav Shrestha, Safal Shrestha, Aadim Nepal, and Keith Ross. Reinforcement learning vs. distillation: Understanding accuracy and capability in llm reasoning. arXiv preprint arXiv:2505.14216, 2025. Xin Lai, Zhuotao Tian, Yukang Chen, Senqiao Yang, Xiangru Peng, and Jiaya Jia. Step-dpo: Stepwise preference optimization for long-chain reasoning of llms. arXiv preprint arXiv:2406.18629, 2024. Nathan Lambert, Jacob Morrison, Valentina Pyatkin, Shengyi Huang, Hamish Ivison, Faeze Brahman, Lester James V. Miranda, Alisa Liu, Nouha Dziri, Shane Lyu, Yuling Gu, Saumya Malik, Victoria Graf, Jena D. Hwang, Jiangjiang Yang, Ronan Le Bras, Oyvind Tafjord, Chris Wilhelm, Luca Soldaini, Noah A. Smith, Yizhong Wang, Pradeep Dasigi, and Hannaneh Hajishirzi. Tulu 3: Pushing frontiers in open language model post-training, 2025. URL https://arxiv.org/ abs/2411.15124. Bowen Li, Wenhan Wu, Ziwei Tang, Lin Shi, John Yang, Jinyang Li, Shunyu Yao, Chen Qian, Binyuan Hui, Qicheng Zhang, et al. Devbench: comprehensive benchmark for software development. CoRR, 2024. Qingyuan Liang, Zhao Zhang, Zeyu Sun, Zheng Lin, Qi Luo, Yueyi Xiao, Yizhou Chen, Yuqun Zhang, Haotian Zhang, Lu Zhang, et al. Grammar-based code representation: Is it worthy pursuit for llms? arXiv preprint arXiv:2503.05507, 2025. Ruibo Liu, Jerry Wei, Fangyu Liu, Chenglei Si, Yanzhe Zhang, Jinmeng Rao, Steven Zheng, Daiyi Peng, Diyi Yang, Denny Zhou, et al. Best practices and lessons learned on synthetic data. arXiv preprint arXiv:2404.07503, 2024. Xiao Liu, Hao Yu, Hanchen Zhang, Yifan Xu, Xuanyu Lei, Hanyu Lai, Yu Gu, Hangliang Ding, Kaiwen Men, Kejuan Yang, et al. Agentbench: Evaluating llms as agents. arXiv preprint arXiv:2308.03688, 2023. Lin Long, Rui Wang, Ruixuan Xiao, Junbo Zhao, Xiao Ding, Gang Chen, and Haobo Wang. On llms-driven synthetic data generation, curation, and evaluation: survey. arXiv preprint arXiv:2406.15126, 2024. Jean-Baptiste Mouret and Jeff Clune. Illuminating search spaces by mapping elites. arXiv preprint arXiv:1504.04909, 2015. Mihai Nadas, , Laura Dios, an, and Andreea Tomescu. Synthetic data generation using large language models: Advances in text and code. IEEE Access, 2025. Alexander Novikov, Ngˆan Vu, Marvin Eisenberger, Emilien Dupont, Po-Sen Huang, Adam Zsolt Wagner, Sergey Shirobokov, Borislav Kozlovskii, Francisco JR Ruiz, Abbas Mehrabian, et al. Alphaevolve: coding agent for scientific and algorithmic discovery. arXiv preprint arXiv:2506.13131, 2025. OpenAI. Learning to reason with llms, 2024. URL https://openai.com/index/ learning-to-reason-with-llms/. Rafael Rafailov, Archit Sharma, Eric Mitchell, Christopher Manning, Stefano Ermon, and Chelsea Finn. Direct preference optimization: Your language model is secretly reward model. Advances in Neural Information Processing Systems, 36, 2024. Bernardino Romera-Paredes, Mohammadamin Barekatain, Alexander Novikov, Matej Balog, Pawan Kumar, Emilien Dupont, Francisco JR Ruiz, Jordan Ellenberg, Pengming Wang, Omar Fawzi, et al. Mathematical discoveries from program search with large language models. Nature, 625(7995):468475, 2024. John Schulman, Filip Wolski, Prafulla Dhariwal, Alec Radford, and Oleg Klimov. Proximal policy optimization algorithms. arXiv preprint arXiv:1707.06347, 2017. Yunfan Shao, Linyang Li, Yichuan Ma, Peiji Li, Demin Song, Qinyuan Cheng, Shimin Li, Xiaonan Li, Pengyu Wang, Qipeng Guo, et al. Case2code: Scalable synthetic data for code generation. arXiv preprint arXiv:2407.12504, 2024a. Zhihong Shao, Peiyi Wang, Qihao Zhu, Runxin Xu, Junxiao Song, Xiao Bi, Haowei Zhang, Mingchuan Zhang, YK Li, Yang Wu, et al. Deepseekmath: Pushing the limits of mathematical reasoning in open language models. arXiv preprint arXiv:2402.03300, 2024b. Asankhaya Sharma. Openevolve: an open-source evolutionary coding agent, 2025. URL https: //github.com/codelion/openevolve. Avi Singh, John Co-Reyes, Rishabh Agarwal, Ankesh Anand, Piyush Patil, Peter Liu, James Harrison, Jaehoon Lee, Kelvin Xu, Aaron Parisi, et al. Beyond human data: Scaling self-training for problem-solving with language models. arXiv preprint arXiv:2312.06585, 2023. Yi Su, Dian Yu, Linfeng Song, Juntao Li, Haitao Mi, Zhaopeng Tu, Min Zhang, and Dong Yu. Crossing the reward bridge: Expanding rl with verifiable rewards across diverse domains, 2025. URL https://arxiv.org/abs/2503.23829. Reiko Tanese. Distributed genetic algorithms for function optimization. University of Michigan, 1989. Peiyi Wang, Lei Li, Zhihong Shao, RX Xu, Damai Dai, Yifei Li, Deli Chen, Wu, and Zhifang Sui. Math-shepherd: label-free step-by-step verifier for llms in mathematical reasoning. arXiv preprint arXiv:2312.08935, 2023. Xingyao Wang, Boxuan Li, Yufan Song, Frank Xu, Xiangru Tang, Mingchen Zhuge, Jiayi Pan, Yueqi Song, Bowen Li, Jaskirat Singh, et al. Openhands: An open platform for ai software developers as generalist agents. arXiv preprint arXiv:2407.16741, 2024. Zihan Wang, Siyao Liu, Yang Sun, Hongyan Li, and Kai Shen. Codecontests+: High-quality test case generation for competitive programming. arXiv preprint arXiv:2506.05817, 2025. An Yang, Anfeng Li, Baosong Yang, Beichen Zhang, Binyuan Hui, Bo Zheng, Bowen Yu, Chang Gao, Chengen Huang, Chenxu Lv, et al. Qwen3 technical report. arXiv preprint arXiv:2505.09388, 2025. Yang Yue, Zhiqi Chen, Rui Lu, Andrew Zhao, Zhaokai Wang, Yang Yue, Shiji Song, and Gao Huang. Does reinforcement learning really incentivize reasoning capacity in llms beyond the base model?, 2025. URL https://arxiv.org/abs/2504.13837. Eric Zelikman, Yuhuai Wu, Jesse Mu, and Noah Goodman. Star: Bootstrapping reasoning with reasoning. Advances in Neural Information Processing Systems, 35:1547615488, 2022. Dan Zhang, Sining Zhoubian, Ziniu Hu, Yisong Yue, Yuxiao Dong, and Jie Tang. Rest-mcts*: Llm self-training via process reward guided tree search. arXiv preprint arXiv:2406.03816, 2024. Linghao Zhang, Shilin He, Chaoyun Zhang, Yu Kang, Bowen Li, Chengxing Xie, Junhao Wang, arXiv preprint Maoquan Wang, Yufan Huang, Shengyu Fu, et al. Swe-bench goes live! arXiv:2505.23419, 2025. Andrew Zhao, Yiran Wu, Yang Yue, Tong Wu, Quentin Xu, Matthieu Lin, Shenzhi Wang, Qingyun Wu, Zilong Zheng, and Gao Huang. Absolute zero: Reinforced self-play reasoning with zero data. arXiv preprint arXiv:2505.03335, 2025."
        },
        {
            "title": "A APPENDIX",
            "content": "A.1 PROMPTS This subsection provides the exact prompts used in our synthesis pipeline. We include the LiveCodeBench testing prompt and the AgentBench-OS prompts (There are two different types of responses, and we provide separate prompts for each type). They can be used to reproduce our data generation and to inspect task-specific constraints and formatting requirements. Figure 6: Prompt for testing generation on LiveCodeBench. Figure 7: Prompt for testing generation on AgentBench-OS QA task. Figure 8: Prompt for testing generation on AgentBench-OS Execution task. A.2 UNIT-TEST COUNT DISTRIBUTION We analyze the number of unit tests attached to each synthesized problem to characterize the strength and granularity of our automated evaluation. Counts include both standard checks and long-input edge-case tests. The distribution is broad (mean 11.5 per problem), indicating heterogeneous coverage and difficulty, which helps produce more stable and discriminative reward signals for ranking solutions and selecting tests. Figure 9: Unit-test count distribution per synthesized problem (mean 11.5); includes long-input edge cases. A.3 OPTIMIZED STRATEGIES We outline several evolved scoring strategies that complement the best program in Figure 3: TF-IDF-like weighting, coverage-based scoring, inverse filtering, exclusion-based attribution, and hardness-aware ranking. Each aims to improve discriminative power and solution-ranking consistency on seed data. Figure 10: TF-IDF-like approach. Solutions that pass difficult testings receive higher scores, where difficulty is defined as testings passed by only few solutions. Figure 11: Coverage-based approach. Solutions are rewarded simply for passing more testings, while testing quality is measured by its discriminative power (i.e., the score gap between solutions that pass and those that fail). Figure 12: Inverse filtering approach. Contrary to the initial strategy, testings that fewer solutions can pass are considered better. Figure 13: Exclusion-based approach. The contribution of testing is measured without its own influence, by weighting solutions that pass all other testings. Figure 14: Hardness-Aware approach. Solutions are ranked by test strictness and pass count, penalizing all-or-none tests to select the strongest solution and most discriminative tests. A.4 TRIVIAL PROBLEM We manually examine the 25 additional instances present to DEvoSynrelaxed but not in DEvoSyn, and find that nearly all of them were overly simple problems. Here we provide one of the examples, the quesiton is just simple determination of whether some numbers are all even or not. Figure 15: An example of trivial problem present in DEvoSynrelaxed quesiton description and solution code. but not in DEvoSyn, containing A.5 AI USAGE STATEMENT AI tools were used solely to assist with writing and polishing the main manuscript text. All core research contentincluding the ideas, problem formulation, methodology and algorithm design, data synthesis framework, experimental design and execution, implementation, evaluation, and analysiswas conceived, conducted, and validated exclusively by the authors. No AI systems were involved in generating ideas, designing or running experiments, or producing any core research results."
        }
    ],
    "affiliations": [
        "Fudan University",
        "Nanyang Technological University",
        "Shanghai AI Laboratory"
    ]
}