{
    "paper_title": "Tuning-Free Multi-Event Long Video Generation via Synchronized Coupled Sampling",
    "authors": [
        "Subin Kim",
        "Seoung Wug Oh",
        "Jui-Hsien Wang",
        "Joon-Young Lee",
        "Jinwoo Shin"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "While recent advancements in text-to-video diffusion models enable high-quality short video generation from a single prompt, generating real-world long videos in a single pass remains challenging due to limited data and high computational costs. To address this, several works propose tuning-free approaches, i.e., extending existing models for long video generation, specifically using multiple prompts to allow for dynamic and controlled content changes. However, these methods primarily focus on ensuring smooth transitions between adjacent frames, often leading to content drift and a gradual loss of semantic coherence over longer sequences. To tackle such an issue, we propose Synchronized Coupled Sampling (SynCoS), a novel inference framework that synchronizes denoising paths across the entire video, ensuring long-range consistency across both adjacent and distant frames. Our approach combines two complementary sampling strategies: reverse and optimization-based sampling, which ensure seamless local transitions and enforce global coherence, respectively. However, directly alternating between these samplings misaligns denoising trajectories, disrupting prompt guidance and introducing unintended content changes as they operate independently. To resolve this, SynCoS synchronizes them through a grounded timestep and a fixed baseline noise, ensuring fully coupled sampling with aligned denoising paths. Extensive experiments show that SynCoS significantly improves multi-event long video generation, achieving smoother transitions and superior long-range coherence, outperforming previous approaches both quantitatively and qualitatively."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 1 1 ] . [ 1 5 0 6 8 0 . 3 0 5 2 : r Tuning-Free Multi-Event Long Video Generation via Synchronized Coupled Sampling Subin Kim1 Seoung Wug Oh2 Jui-Hsien Wang2 Joon-Young Lee2 Jinwoo Shin 1KAIST 2Adobe Research subin-kim@kaist.ac.kr Figure 1. Multi-event long video generation results using our tuning-free inference framework, SynCoS. Each example is around 21 seconds of video at 24 fps (4 longer than the base model ). Frame indices are displayed in each frame. SynCoS generates high-quality, long videos with multi-event dynamics while achieving both seamless transitions between frames and long-term semantic consistency throughout."
        },
        {
            "title": "Abstract",
            "content": "While recent advancements in text-to-video diffusion models enable high-quality short video generation from single prompt, generating real-world long videos in single pass remains challenging due to limited data and high computational costs. To address this, several works propose tuningfree approaches, i.e., extending existing models for long video generation, specifically using multiple prompts to allow for dynamic and controlled content changes. However, these methods primarily focus on ensuring smooth transitions between adjacent frames, often leading to content drift and gradual loss of semantic coherence over longer sequences. To tackle such an issue, we propose Synchronized Coupled Sampling (SynCoS), novel inference framework that synchronizes denoising paths across the entire video, ensuring long-range consistency across both adjacent and distant frames. Our approach combines two complementary sampling strategies: reverse and optimization-based sampling, which ensure seamless local transitions and enforce global coherence, respectively. However, directly alternating between these samplings misaligns denoising trajectories, disrupting prompt guidance and introducing unintended content changes as they operate independently. To resolve this, SynCoS synchronizes them through grounded timestep and fixed baseline noise, ensuring fully coupled sampling with aligned denoising paths. Extensive experiments show that SynCoS significantly improves multi-event long video generation, achieving smoother transitions and superior longrange coherence, outperforming previous approaches both quantitatively and qualitatively. 1 1. Introduction Recently, text-to-video (T2V) diffusion models have shown impressive capabilities in synthesizing high-quality videos from single text prompt [3, 11, 20, 23, 34, 55]. However, these models primarily generate short, fixed-length videos that capture single event, mainly due to high computational training costs and limited high-quality long video data [31, 49, 53]. In contrast, real-world videos are long and comprise multiple events that introduce dynamics, such as object motion and camera movement, all within coherent scenario. Thus, extending T2V models for long video generation requires handling event transitions while ensuring global coherence throughout the video. 1Visualizations are available at: https://syncos2025.github.io/. Figure 2. Qualitative comparisons on CogVideoX-2B [55]. All examples are 5 times longer in duration compared to the underlying base model, generating 30-second video. Unlike Gen-L-Video [50] and FIFO-Diffusion [21], which often struggle with overlapping artifacts and style drift, our method, SynCoS, ensures consistency in both content and style throughout the entire video. Additionally, SynCoS generates long videos where each frame faithfully follows its designated prompt while ensuring seamless transitions between frames. Existing methods often struggle to achieve this balance, failing to enforce long-range consistency. Training-based approaches introduce conditioning modules [9, 47, 49] to generate video chunks autoregressively. However, relying solely on previously generated chunks makes maintaining coherent global structure difficult, and repeated conditioning leads to error accumulation and degraded frame quality. Alternatively, several works [5, 21, 29, 31, 36, 50] propose inference techniques to extend T2V diffusion models, avoiding additional training and error accumulation. One common and intuitive approach is fusion, which smoothly connects short videos into longer one by dividing long video into overlapping chunks, denoising each with different prompts, and applying local fusion to overlapped regions for seamless transitions. However, these methods focus only on local smoothness, failing to integrate information across distant chunks. This issue becomes even more pronounced in multi-prompt scenarios, where denoising paths between chunks diverge more significantly than with single prompt, resulting in severe inconsistencies in semantics, and style across the generated video (see 1st and 2nd rows in Figure 2). While local fusion-based inference techniques are promising as they require no additional training, they lack mechanism to enforce long-range consistency. We argue that maintaining shared denoising trajectory across both shortand long-distance chunks is essential for generating coherent long videos. To address this, we explore an optimizationbased sampling approach: Collaborative Score Distillation (CSD) [22], which synchronizes information across adjacent and distant samples, enforcing long-range consistency. However, when directly applied to long video generation, CSD completely fails to produce high-quality videos despite aligning denoising trajectories across chunks (see Section 3.2). In this paper, we propose Synchronized Coupled Sampling (SynCoS), novel inference framework for extending any T2V diffusion model to multi-event long video generation. Our key idea is to jointly preserve local smoothness and global coherence by coupling two complementary samplings. However, direct combination misaligns their denoising trajectories, weakening prompt guidance and causing unin- . tended content variations. To address this, we introduce synchronized mechanism when coupling them. Specifically, SynCoS integrates Denoising Diffusion Implicit Models (DDIM) [43], built on fusion, to enforce smooth local transitions between adjacent chunks while leveraging CSD to maintain long-range coherence across chunks. To enable synchronous operation when coupling these sampling methods, SynCoS introduces grounded timestep and fixed baseline noise, ensuring alignment across the entire denoising process. Furthermore, SynCoS proposes structured prompt to ensure coherent long scenario with dynamics, combining global prompt for scenario-wide consistency with local prompts for fine-grained controls. This newly introduced coupled sampling, along with its key synchronization componentsgrounded timestep and fixed baseline noiseand the structured prompt, defines SynCoS as new inference framework. As unified inference framework, SynCoS ensures that local smoothness and global coherence work in tandem, enabling coherent long video generation with multi-event dynamics. We evaluate SynCoS across various challenging scenarios in long video generation, including multiple events (e.g., object motion, compositions, camera movements, background changes, etc.) across different denoising models to comprehensively verify its efficacy. Consistently, SynCoS significantly outperforms existing tuning-free methods in temporal consistency, video quality, and prompt fidelity. Contributions. Our main contributions are as follows: We propose SynCoS, novel inference framework that extends any T2V diffusion model for multi-event long video generation without additional tuning. SynCoS synchronizes two complementary sampling methods to ensure both local transitions and global coherence, introducing grounded timestep and fixed baseline noise to couple them into new, unified sampling. SynCoS incorporates structured prompt for dynamic yet semantically consistent multi-event generation. We extensively validate SynCoS on various T2V models across diverse long video generation scenarios, achieving state-of-the-art temporal consistency and prompt fidelity. 2. Background In this section, we present the preliminaries (Section 2.1) and basic concept of fusion for extending T2V diffusion models in tuning-free manner (Section 2.2). 2.1. Preliminaries We provide brief overview of diffusion models, score distillation samplings [22, 35] as the foundation of our method. For detailed explanation, refer to Appendix A. Diffusion models. Diffusion models are generative models that learn to gradually denoise random Gaussian noise into structured data by reversing noise-adding process. This generative process can be formulated using stochastic differential equations (SDEs) [44] or their deterministic counterpart, ordinary differential equations (ODEs) [25]. notable variant of diffusion models is Denoising Diffusion Implicit Models (DDIM) [43], which adopts nonMarkovian approach to accelerate sampling while preserving the marginal distribution. At each step, DDIM estimates the clean data sample, ˆx0t, following Tweedies formula from noisy sample, xt, using learned noise predictor, Φ. Then, the posterior distribution, p(xt1xt, x0), is computed as: xt1 = αt1 ˆx0t + (cid:112)1 αt1 ϵ, (1) where αt1 is pre-defined coefficient that regulates noise schedule over time, and ϵ is weighted combination of the predicted noise estimate term ϵΦ, and stochastic noise term ϵ (0, I), controlled by the hyperparameter η. For prompt-conditioned generation, classifier-free guidance (CFG) [16] is widely used. The estimated noise is adjusted as: ϵγ Φ(xt; c, t) = ϵΦ(xt; , t) + γ[ϵΦ(xt; c, t) ϵΦ(xt; , t)] , where γ controls the guidance strength, is the null-text embedding and is the conditioning text embedding. Score distillation sampling. Score Distillation Sampling (SDS) [35, 51] introduces novel approach for optimizing differentiable parametric functions using diffusion models as critic. By doing so, SDS extends the applicability of textto-image diffusion models to generate and manipulate more complex visual data, including 3D objects and scenes [7, 15, 24, 30, 46, 48]. To achieve this, SDS formulates generative sampling as an optimization problem, allowing control over the generated output by optimizing the parameters, θ, of function, g(θ), to guide generation. Specifically, given generated sample, = g(θ), the gradient of the diffusion loss function with respect to θ is computed as: θLSDS(Φ; = g(θ)) = Et,ϵ (cid:20) w(t) (ϵγ Φ(xt; c, t) ϵ) (cid:21) . θ (2) Collaborative score distillation sampling. SDS (Equation 2) optimizes single sample, x. In contrast, Collaborative Score Distillation (CSD) [22] extends SDS by incorporating interactions between multiple samples, {x(i)}N i=1, 3 allowing them to influence each other, thereby ensuring intersample consistency during optimization. At each iteration, CSD selects random timestep, t, and samples Gaussian noise for each sample, ϵ(i) (0, I). Then, each sample, x(i) = g(θi), is optimized using the following objective: θi LCSD (cid:0)Φ; x(i) = g(θi)(cid:1) = w(t) (cid:34) (cid:88) j=1 k(x(j) , x(i) ) (cid:16) ϵΦ(x(i) ; c, t) ϵ(i)(cid:17) (3) (cid:35) + (j) k(x(j) , x(i) ) x(i) θi , where is positive definite kernel that measures similarity between samples. By leveraging inter-sample relationships, CSD adjusts gradient updates based on sample affinity, ensuring that each parameter update affects and is affected by other samples to promote global coherency. 2.2. Fusion-based tuning-free long video generation T2V diffusion models generate short, single-event videos in one pass due to high computational costs, making long, dynamic video generation challenging. Fusion offers practical solution by leveraging the models short-clip generation capability for extended multi-event video synthesis: dividing long video into overlapping short chunks, processing each chunk with different prompts, and fusing overlapped regions for smooth transitions. Problem formulation. Given pre-trained T2V model, Φ, that generates short video sample Rf HW D, conditioned on single text prompt embedding c, our goal is to extend it to generate long video RF HW D, where . common fusion strategy for long video generation involves partitioning the long video, x, into overN lapping short video chunks {x(i)} i=1 Rf HW D, with temporal stride s. Each chunk is denoised independently, conditioned on evolving text prompts {c(i)} i=1, to generate multi-event dynamics. These short chunks are then merged into long video by averaging overlapping regions, normalized by the number of contributing chunks. 3. Key observations Through fusion, adjacent chunks undergo temporal codenoising, ensuring smooth transitions. Gen-L-Video [50], CSD [22], and our proposed approach all leverage temporal co-denoising via fusion but differ in what is fused. The following sections detail these differences. 3.1. Local temporal co-denoising with Gen-L-Video Gen-L-Video fuses posterior distributions. Gen-LVideo [50] extends T2V diffusion models by fusing the posterior distributions of overlapping chunks to maintain local Figure 3. t-SNE visualization of CLIP [37] features for the predicted video frames ˆx0t, at each timestep using different samplings. Faded colors indicate earlier timesteps (t 1000), while vivid colors indicate later, small timesteps (t 0), illustrating feature trajectory evolution over time (top to bottom). Figure 4. Qualitative comparison of sampling methods motivating SynCoS. GenL-Video [50] fails to maintain global coherence, resulting in abrupt appearance changes (e.g., man morphing into woman). CSD [22] retains similar appearance of man but shows poor adherence to local prompts, suffering from low frame quality with severe noise-like artifacts. In contrast, our method achieves balance, ensuring high-quality generation, strong prompt fidelity, and temporal coherence. smoothness. Each chunk undergoes independent denoising using the DDIM sampler, producing intermediate outputs x(i) t1. The final t1 is obtained by fusing the posteriors of overlapping chunks x(i) t1, as computed in Equation 1. Divergence in denoising paths with Gen-L-Video. While this local fusion with Gen-L-Video helps maintain smooth transitions between adjacent chunks, it neglects information across distant video chunks to enforce long-range temporal consistency. As result, Gen-L-Video produces unnatural transitions, such as abrupt appearance changes (e.g., man morphing into woman, as shown in Figure 4). We verify this in Figure 3, which visualizes how denoising paths evolve over time. Here, dot color intensity transitions from faded to vivid, representing progressively later timesteps, with dots arranged top to bottom, corresponding to timesteps from = 1000 to = 0. The green dots, representing Gen-LVideo, gradually separate, indicating increasing divergence in its denoising paths. 3.2. Global temporal co-denoising with CSD To establish connections between distant video chunks while preserving local smoothness, we explore an optimizationbased approach using Collaborative Score Distillation (CSD) for long video generation. Although CSD was originally designed for visual editing, we apply it to improve global temporal consistency in fusion-based long video generation. Apply CSD for long video generation by fusing its loss. Similar to Gen-L-Video, long video is divided into overlapping chunks. The CSD loss for each chunk is computed to synchronize denoising paths across both shortand longrange chunks. Then, the losses in overlapping regions are fused, further reinforcing smooth transitions. Failure of CSD for long video generation. Despite its effectiveness in visual editing, CSD fails when applied to long video generation. Unlike editing, where an existing source structure guides modifications, video synthesis starts from pure Gaussian noise with no inherent priors. This lack of structured guidance causes frames to collapse into similar states as denoising progresses (see red dots in Figure 3). As result, while the generated video retains similar appearance throughout, it fails to adhere to local prompts and suffers from degraded per-frame quality (see Figure 4). 4. Method We propose Synchronized Coupled Sampling (SynCoS), novel inference framework applicable to any T2V diffusion model for generating temporally consistent, multi-event long videos. Section 4.1 provides an overview of the stages in our coupled sampling, followed by key mechanisms that enable the synchronized coupling of the three stages in Section 4.2. 4.1. SynCoS: Synchronized Coupled Sampling As discussed in Section 3, fusion-based temporal codenoising shows promise for long video generation, but existing methods suffer from different limitations. Temporal co-denoising with DDIM (i.e., Gen-L-Video) ensures local smoothness but results in divergent denoising paths, whereas temporal co-denoising with CSD facilitates global coherence but results in overly converged, collapsed denoising paths. To capture both local smoothness and global consistency, SynCoS combines fusion-based temporal co-denoising with DDIM and CSD at each denoising step. The key insight is treating the intermediate DDIM output of ˆx0t as refinement source for CSD-based optimization, enabling global refinement to build upon locally smoothed updates. This formulation reframes CSD-based optimization as progressive refinement process rather than pure generation, where each DDIM update at every denoising step serves as an improved starting point for enforcing global coherence. 4 Figure 5. Overall illustration of our proposed method, Synchronized Coupled Sampling (SynCoS), tuning-free inference framework for multi-event long video generation. SynCoS performs one-step denoising in three iterative stages, repeated from = 1000 to = 0. In the first stage, SynCoS performs temporal co-denoising with DDIM, dividing the long video into overlapping short chunks, denoising each chunk, and applying fusion for local smoothness. In the second stage, SynCoS refines the locally fused output, enforcing global coherence by synchronizing information across both shortand long-distance chunks. Finally, in the third stage, it reverts the locally and globally refined output to the previous timestep. Through these three synchronized stages of local and global denoising, SynCoS ensures smooth transitions, global semantic coherence, and high prompt fidelity, enabling multi-event long video generation. SynCoS consists of three stages, which are repeated at each denoising step (illustrated in Figure 5): 1. Perform temporal co-denoising with DDIM. Instead of fully reverting each chunk to its previous timestep, 1, SynCoS computes ˆx(i) 0t for every video chunk, then applies fusion to them to produce 0. 2. Refine the locally fused output to enforce global coherency. The fused output from the first stage serves as refinement source, where temporal co-denoising with CSDbased optimization is performed by computing x(i) LCSD for each chunk and applying fusion on the gradients to update 0. This stage synchronizes denoising levels across chunks, effectively managing multi-event scenarios by ensuring coherence when varying prompts cause divergent denoising paths. By doing so, SynCoS mitigates content and style drift while preserving global semantic consistency. Crucially, SynCoS introduces key components to bridge the first and second stages and the second and final stages, as detailed in Section 4.2. 0 3. Resume the temporal co-denoising with DDIM using locally and globally refined output. SynCoS reverts the refined and fused 0 to the previous timestep, producing t1. Here, the intermediate sample is re-derived by adding noise (from the second stage) to the 0. Finally, 0 and the re-derived t1 is computed using both x t. This one-step denoising process, comprising three stages, is iteratively repeated from = 1000 until = 0. We present the complete algorithm in Algorithm 1, with pseudo-code for each stage provided in Appendix E. 4.2. Align denoising paths across three stages for synchronized coupling SynCoS introduces three stages to capture both local smoothness and global coherence. However, synchronizing denoisAlgorithm 1: Synchronized Coupled Sampling (SynCoS) i=1; Require : Φ; {c(i)}N Ensure :x 0 Procedure Ours: // pre-trained T2V model // conditioning text-prompt embeddings (0, I); for = T, ..., 1 do for = 0, ..., 1 do t); x(i) TakeChunk(x ϵ(i) pred DiffForwardΦ(x(i) 0t DDIM(ϵ(i) ˆx(i) pred, t); 0 Fusion({ˆx(i) 0t}N if > tmin then i=1); , c(i), t); ϵ base (0, I); for iter = 1 to iters do // fixed baseline noise for = 0, ..., 1 do x(i) 0 TakeChunk(x ϵ(i) base TakeChunk(ϵ AddNoise(x(i) x(i) ϵ(i) pred DiffForwardΦ(x(i) // grounded timestep 0); base); 0 , ϵ(i) base); , c(i), t); for = 0, ..., 1 do , ϵ(i) LCSD(x(i) (i) 0 base, ϵ(i) pred); 0 LCSD Fusion({ LCSD; Backpropagate 0 SGD update on 0; LCSD}N i=1); (i) 0 t1 DDIM(x 0, ϵbase, t); ing trajectories across stages is crucial, as alternating between them can cause misalignment, leading to artifacts and reduced prompt fidelity. In this section, we present the key components of each stage in SynCoS that ensure synchronized coupled sampling for seamless stage transitions. Grounded timestep between the first and second stage. Aligning timesteps across stages is crucial in three-stage process, as diffusion models establish the overall structure in earlier timesteps and refine finer details in later timesteps. If each stage operates on inconsistent temporal references, denoising trajectories become misaligned, introducing artifacts and inconsistencies in the final video (see Figure 7). To prevent this, SynCoS anchors the second-stage timestep to the first-stage sampling schedule, following the DDIM timestep progressing from = 1000 to = 0 based on designated sampling steps. This ensures that both stages operate within unified temporal reference, first establishing coherent structure and then focusing on finer details throughout the denoising process. This approach distinguishes SynCoS from standard CSD-based optimization (Equation 3), where timesteps are randomly selected at each iteration. Fixed baseline noise for the second and third stage. In SynCoS, while ensuring global coherence, it is crucial in the second stage to preserve distinct prompt guidance for each chunk and prevent sample collapse, as discussed in Section 3.2. To achieve this, we fix single noise and use it consistently throughout one-step denoising, stabilizing second-stage optimization by providing consistent baseline noise, unlike Equation 3, where random noise is introduced at each step. Specifically, fixed baseline noise ϵ base is sampled from Gaussian distribution at the start of the second stage, matching the shape of is then processed alongside its corresponding noise chunk ϵ(i) base. Additionally, SynCoS retains this fixed noise in the third stage, where it is reintroduced into the refined x0, further maintaining aligned update directions across stages. 0. Each chunk x(i) 0 Coupled sampling for the early timesteps. To balance local smoothness and global coherence, SynCoS regulates the second stage using tmin, applying it only until tmin [800, 900], which we empirically found to be the optimal range. By this point, SynCoS establishes coherent semantic layout across video chunks, preventing denoising trajectories from diverging as shown in Figure 3. Thus, beyond tmin, SynCoS performs only the first and third stages, focusing on adding fine-grained details on an already structured layout. Structured prompt. To further enhance coherent long scenarios with dynamics, SynCoS designs structured prompt for multi-event scenarios, consisting of shared global prompt for scenario-wide coherence and local prompts for event-specific variations (e.g., motions, camera movement, or attributes). Naïvely using sequence of prompts (e.g., teddy bear is standing teddy bear is running) 6 can lead to inconsistencies in the shared entity. To reduce ambiguity and constrain the generation space, we introduce detailed global prompt (e.g., brown teddy bear with button eyes and stitched smile), ensuring uniformity across all video chunks. Each chunk is then assigned local prompt that builds on the global prompt, incorporating chunk-specific dynamics. See Appendix for details. Flexibility of SynCoS to various T2V diffusion models. Notably, SynCoS is architecture-agnostic and compatible with any T2V diffusion model, supporting various diffusion objectives (v-prediction [41], ϵ-prediction [12, 52]), both diffusion-based and rectified flow-based models. In addition, unlike prior works [5, 36, 45] restricted to U-Net [39] or DiT [33], SynCoS remains flexible across architectures. While we describe our method using ϵ-prediction networks for clarity, SynCoS easily adapts to other diffusion settings by modifying LCSD in Algorithm 1 (see Appendix for complete derivation on various diffusion settings). We further validate this through comprehensive experiments on various denoising models in Section 5 and Appendix C. 5. Experiments We first provide brief overview of the experimental setup in Section 5.1, followed by thorough validation of our inference framework across diverse scenarios and diffusion settings in Section 5.2, Appendix D, comparing it with previous works. Additionally, we conduct comprehensive ablation studies in Section 5.3 and Appendix C. 5.1. Experimental setup Implementation details. We evaluate on 48 long-video scenarios, forming more extensive benchmark than previous works [21, 31], with video lengths extended by 45 times. These multi-event scenarios encompass challenges such as object motion control, camera control, background changes, compositional generation, and physical transformations (full prompt details in Appendix B). To demonstrate broad applicability, we implement SynCoS across four different T2V diffusion models: CogVideoX2B [55] and Open-Sora Plan (v1.3) [23] (in Section 5.2) as well as modified Open-Sora Plan (v1.2) variant (denoted as M) and VideoCrafter2 [52] (in Appendix and D). All experiments are conducted on single NVIDIA H100 80GB GPU, with inference time measurements detailed in Appendix B. Baselines. We primarily evaluate SynCoS against tuningfree, architecture-agnostic baselines (Gen-L-Video [50], FIFO-Diffusion [21]) and provide additional comparisons with architecture-specific methods (FreeNoise [36], VideoInfinity [45], DiTCtrl [5]) in Appendix for fair evaluation within their respective architectures. For all baselines, we follow their reported experimental setups and carefully tune hyperparameters for optimal performance. Table 1. Quantitative comparison. Experimental results of SynCoS with baselines on multi-event long video generations. Bold indicates the best results. SynCoS consistently outperforms baselines across temporal consistency, per-frame quality, and prompt fidelity."
        },
        {
            "title": "Temporal Quality",
            "content": "Frame-wise Quality"
        },
        {
            "title": "Subject",
            "content": "Background Consistency Consistency"
        },
        {
            "title": "Motion",
            "content": "Dynamic Smoothness Degree"
        },
        {
            "title": "Num",
            "content": "Aesthetic Scenes Quality Quality Imaging Glb Prompt Loc Prompt Fidelity Fidelity CogVideoX [55] Open-Sora Plan [23] Gen-L-Video [50] FIFO-Diffusion [21] SynCoS (Ours) Gen-L-Video [50] FIFO-Diffusion [21] SynCoS (Ours) 81.34% 83.54% 88.92% 85.15% 89.14% 90.19% 89.46% 90.72% 94.57% 92.63% 94.34% 94.78% 98.32% 98.04% 98.21% 98.93% 98.19% 99.06% 50.00% 70.83% 85.42% 43.75% 27.08% 58.33% 2.292 1.938 1.208 2.458 1.125 1.646 60.09% 59.59% 58.52% 63.18% 63.37% 67.43% 61.44% 60.26% 57.19% 61.23% 63.79% 60.19% 0.324 0.323 0.341 0.319 0.327 0.328 0.351 0. 0.354 0.337 0.331 0.345 Figure 6. Qualitative comparisons on Open-Sora Plan [23]. All examples are 4 times longer in duration compared to the underlying base model, generating 20-second video. Gen-L-Video [50] suffers from abrupt appearance changes, while FIFO-Diffusion [21] introduces noticeable noise-like artifacts. In contrast, our proposed method, SynCoS, generates high-quality, temporally coherent videos that faithfully follow the prompt throughout the sequence. Evaluation metrics. We evaluate multi-event long video generation results using VBench [19] to assess both temporal consistency and per-frame quality. Temporal consistency is measured by subject and background consistency, motion smoothness, and dynamic degree, while aesthetic and imaging quality evaluate per-frame fidelity. Following prior works [14, 54], we use Adaptive Detector [6] to count scene changes, where Num Scenes value of 1 indicates no transitions. To assess prompt-driven controllability, we measure prompt fidelity using CLIP [37], computing image-text similarity for both local and global prompts. 5.2. Main experiments While SynCoS incorporates structured prompt, we apply the structured prompt to all baselines for fair comparison. Quantitative comparisons. In Table 1, SynCoS significantly outperforms existing tuning-free baselines for long video generation, achieving higher scores in both temporal consistency and per-frame image quality. In particular, SynCoS surpasses baselines in subject and background consistency while achieving the highest dynamic degree, demonstrating its ability to generate visually dynamic but temporally consistent videos. Since lower dynamic degree naturally improves consistency, high-scores of SynCoS in both metrics are particularly significant. Additionally, SynCoS demonstrates superior prompt fidelity, generating distinct promptcontrolled dynamics while maintaining semantic and content consistency across the entire video. Qualitative comparisons. As shown in Figure 2 and Figure 6, SynCoS excels in high-quality multi-event long video generation, ensuring temporal consistency throughout the video. Gen-L-Video suffers from overlapping artifacts due to diverging denoising paths across video chunks when guided by different prompts. SynCoS effectively mitigates these issues through global synchronization. FIFO-Diffusion exhibits style drift on CogVideoX-2B and suffers from degraded image quality with noise artifacts on Open-Sora Plan. This issue arises from its sequential timestep processing of adjacent frames, leading to training-inference discrepancy. As Open-Sora Plan encodes more frames per video chunk, FIFO-Diffusion struggles with severe timestep variations between frames, resulting in pronounced artifacts. In contrast, SynCoS does not introduce any training-inference discrepancies. See Appendix for further comparisons on baselines and Appendix for additional qualitative results. 5.3. Ablation study Table 2 and Figure 7 ablate the key components essential for coupling the three stages into unified sampling, providing both quantitative and qualitative evaluations. First, grounded timesteps anchor the generation process, ensuring that each stage focuses on the same aspects of generation when alternating between stages. Without grounding, random timesteps disrupt the progressive generation from structure formation (early timesteps) to detail addition (later timesteps), causing content drift (see w/o grounded timestep in Table 2). For example, in Figure 7, the absence of grounded timesteps causes volcano to randomly change its appearance, generating one or two peaks across frames due to misalignment between structure and detailing. 7 Table 2. Quantitative ablation study. *Abbreviations: subject consistency (SC), background consistency (BC), aesthetic quality (AQ), and prompt fidelity (PF)."
        },
        {
            "title": "SynCoS",
            "content": "SC BC AQ 0.864 0.927 0. SynCoS w/o tmin SynCoS w/o grounded timestep SynCoS w/o fixed baseline noise SynCoS w/o structured prompt 0.724 0.803 0.817 0.837 0.854 0.899 0.904 0.903 0.632 0.638 0.643 0.663 PF 0. 0.373 0.364 0.382 0.362 Figure 7. Qualitative ablation study. Without grounded timestep, structural inconsistencies arise (e.g., the volcano alternates between one and two peaks). Without fixed baseline noise, it fails to follow local prompts faithfully (e.g., missing eruptions or absent smoke). Additionally, fixed baseline noise is crucial for preserving distinct prompt guidance while ensuring cohesion between them (see w/o fixed baseline noise in Table 2). Without it, distinct prompt guidance weakens as cohesion dominates, reducing prompt fidelity (e.g., volcano without an eruption or missing rising smoke, as shown in Figure 7). Moreover, tmin regulates the synchronization strength (see w/o tmin in Table 2). Lastly, structured prompts improve coherence by blending global prompts for consistency with local prompts for fine-grained control (see w/o structured prompt in Table 2). In addition to qualitative examples in Section 3, Appendix presents quantitative ablations by skipping each stage of SynCoS to assess the efficacy of coupled stages. 6. Related work Text-guided video diffusion models. The success of textto-image (T2I) generation with diffusion models [2, 38, 40] has inspired extensions to the more complex task of textto-video generation. Several works extended T2I models by fine-tuning them with temporal layers, adapting spatial structures to temporal dynamics for more efficient training [1, 4, 10, 13, 18, 42, 52]. Recent advancements in Diffusion Transformers (DiT) [33] and flow-based generative models [27] have further improved quality, enabling state-ofthe-art T2V diffusion models [20]. However, high computational and memory costs restrict these models to short video clips, limiting their applicability to long video generation. Training-based long video generation. To extend video length, alternative strategies include different architecture choices: transformer-based methods, autoregressive diffusion models, and hierarchical models [8, 9, 14, 47, 49, 54, 56]. Transformer-based methods [9, 49] generate long videos in one-shot manner from multiple prompts. However, they require extensive training from scratch and often suffer from content drift, degrading quality over time. Autoregressive diffusion models [14, 54] generate frames sequentially, conditioning each chunk on the previous one. While this improves short-term consistency, autoregressive dependencies cause errors to accumulate, resulting in visual artifacts and inconsistencies in long-form synthesis. Additionally, these methods are limited to single-prompt generation, making them unsuitable for evolving content. Alternatively, hierarchical diffusion models [8, 56] generate keyframes first and then interpolate intermediate frames. While this approach maintains structural consistency, interpolation alone does not introduce new content, limiting dynamic scene evolution. Tuning-free long video generation. Recent approaches extend existing T2V diffusion models for long video generation in tuning-free manner using multiple prompts [21, 31, 36, 50]. While scalable without additional training, these approaches focus on local frame transitions, often leading to content drift and semantic inconsistencies over longer sequences. Gen-L-Video [50] fuses denoising paths in overlapping frames, but local fusion dilutes prompt guidance from different prompts between frames, degrading quality and causing divergent denoising paths. FIFO-Diffusion [21] enables infinite sampling by sequentially assigning timesteps, but discrepancies between training and inference lead to undesirable artifacts. FreeNoise [36] uses window-based attention fusion to attend to longer frame dependencies, but it is incompatible with newer diffusion models that lack separate spatial and temporal attention layers. In contrast, our method synchronizes local and global denoising paths across adjacent and distant frames, preventing content drift and preserving semantic consistency throughout long videos. Additionally, Video-Infinity [45] and DiTCtrl [5] propose architecture-specific extensions, whereas our approach is compatible with any T2V diffusion model. Please refer to Appendix for further discussion and comparisons. 7. Conclusion This work presents tuning-free inference framework that extends any existing T2V diffusion model for multi-event long video generation. Unlike previous works focusing solely on local smoothing between adjacent frames, our approach simultaneously ensures smooth local transitions and global coherence by introducing three synchronously coupled stages with structured prompt. Extensive evaluations across diverse, challenging, long video scenarios with multiple events demonstrate that our method generates high-quality, temporally coherent long videos, significantly outperforming prior works both quantitatively and qualitatively."
        },
        {
            "title": "References",
            "content": "[1] Jie An, Songyang Zhang, Harry Yang, Sonal Gupta, Jia-Bin Huang, Jiebo Luo, and Xi Yin. Latent-shift: Latent diffusion with temporal shift for efficient text-to-video generation. arXiv preprint arXiv:2304.08477, 2023. 8 [2] Yogesh Balaji, Seungjun Nah, Xun Huang, Arash Vahdat, Jiaming Song, Karsten Kreis, Miika Aittala, Timo Aila, Samuli Laine, Bryan Catanzaro, et al. ediffi: Text-to-image diffusion models with an ensemble of expert denoisers. arXiv preprint arXiv:2211.01324, 2022. 8 [3] Omer Bar-Tal, Hila Chefer, Omer Tov, Charles Herrmann, Roni Paiss, Shiran Zada, Ariel Ephrat, Junhwa Hur, Guanghui Liu, Amit Raj, et al. Lumiere: space-time diffusion model for video generation. In SIGGRAPH Asia 2024 Conference Papers, pages 111, 2024. 1 [4] Andreas Blattmann, Robin Rombach, Huan Ling, Tim Dockhorn, Seung Wook Kim, Sanja Fidler, and Karsten Kreis. Align your latents: High-resolution video synthesis with laIn IEEE Conference on Computer tent diffusion models. Vision and Pattern Recognition, 2023. 8 [5] Minghong Cai, Xiaodong Cun, Xiaoyu Li, Wenze Liu, Zhaoyang Zhang, Yong Zhang, Ying Shan, and Xiangyu Yue. Ditctrl: Exploring attention control in multi-modal diffusion transformer for tuning-free multi-prompt longer video generation. In IEEE Conference on Computer Vision and Pattern Recognition, 2025. 2, 6, 8, 5 [6] Brandon Castellano. PySceneDetect. 7 [7] Rui Chen, Yongwei Chen, Ningxin Jiao, and Kui Jia. Fantasia3d: Disentangling geometry and appearance for highquality text-to-3d content creation. In IEEE International Conference on Computer Vision, pages 2224622256, 2023. [8] Xinyuan Chen, Yaohui Wang, Lingjun Zhang, Shaobin Zhuang, Xin Ma, Jiashuo Yu, Yali Wang, Dahua Lin, Yu Qiao, and Ziwei Liu. Seine: Short-to-long video diffusion model for generative transition and prediction. In International Conference on Learning Representations, 2023. 8 [9] Songwei Ge, Thomas Hayes, Harry Yang, Xi Yin, Guan Pang, David Jacobs, Jia-Bin Huang, and Devi Parikh. Long video generation with time-agnostic vqgan and time-sensitive transformer. In European Conference on Computer Vision, 2022. 2, 8 [10] Songwei Ge, Seungjun Nah, Guilin Liu, Tyler Poon, Andrew Tao, Bryan Catanzaro, David Jacobs, Jia-Bin Huang, MingYu Liu, and Yogesh Balaji. Preserve your own correlation: noise prior for video diffusion models. In IEEE International Conference on Computer Vision, 2023. 8 [11] Rohit Girdhar, Mannat Singh, Andrew Brown, Quentin Duval, Samaneh Azadi, Sai Saketh Rambhatla, Akbar Shah, Xi Yin, Devi Parikh, and Ishan Misra. Emu video: Factorizing text-tovideo generation by explicit image conditioning. In European Conference on Computer Vision, 2023. 1 [12] Yuwei Guo, Ceyuan Yang, Anyi Rao, Zhengyang Liang, Yaohui Wang, Yu Qiao, Maneesh Agrawala, Dahua Lin, and Bo Dai. Animatediff: Animate your personalized text-to-image diffusion models without specific tuning. In International Conference on Learning Representations, 2023. 6 [13] Yingqing He, Tianyu Yang, Yong Zhang, Ying Shan, and Qifeng Chen. Latent video diffusion models for high-fidelity arXiv preprint video generation with arbitrary lengths. arXiv:2211.13221, 2022. 8 [14] Roberto Henschel, Levon Khachatryan, Daniil Hayrapetyan, Hayk Poghosyan, Vahram Tadevosyan, Zhangyang Wang, Shant Navasardyan, and Humphrey Shi. Streamingt2v: Consistent, dynamic, and extendable long video generation from text. arXiv preprint arXiv:2403.14773, 2024. 7, [15] Amir Hertz, Kfir Aberman, and Daniel Cohen-Or. Delta denoising score. In IEEE Conference on Computer Vision and Pattern Recognition, 2023. 3 [16] Jonathan Ho and Tim Salimans. Classifier-free diffusion guidance. arXiv preprint arXiv:2207.12598, 2022. 3 [17] Jonathan Ho, Ajay Jain, and Pieter Abbeel. Denoising diffusion probabilistic models. In Advances in Neural Information Processing Systems, 2020. 1 [18] Jonathan Ho, William Chan, Chitwan Saharia, Jay Whang, Ruiqi Gao, Alexey Gritsenko, Diederik P. Kingma, Ben Poole, Mohammad Norouzi, David J. Fleet, and Tim Salimans. Imagen video: High definition video generation with diffusion models. arXiv preprint arXiv:2210.02303, 2022. 8 [19] Ziqi Huang, Yinan He, Jiashuo Yu, Fan Zhang, Chenyang Si, Yuming Jiang, Yuanhan Zhang, Tianxing Wu, Qingyang Jin, Nattapol Chanpaisit, et al. Vbench: Comprehensive benchmark suite for video generative models. In IEEE Conference on Computer Vision and Pattern Recognition, 2024. 7 [20] Yang Jin, Zhicheng Sun, Ningyuan Li, Kun Xu, Hao Jiang, Nan Zhuang, Quzhe Huang, Yang Song, Yadong Mu, and Zhouchen Lin. Pyramidal flow matching for efficient video generative modeling. arXiv preprint arXiv:2410.05954, 2024. 1, 8 [21] Jihwan Kim, Junoh Kang, Jinyoung Choi, and Bohyung Han. Fifo-diffusion: Generating infinite videos from text without training. In Advances in Neural Information Processing Systems, 2024. 2, 6, 7, 8, 3, 4 [22] Subin Kim, Kyungmin Lee, June Suk Choi, Jongheon Jeong, Kihyuk Sohn, and Jinwoo Shin. Collaborative score distillation for consistent visual editing. In Advances in Neural Information Processing Systems, pages 7323273257, 2023. 2, 3, 4, [23] Bin Lin, Yunyang Ge, Xinhua Cheng, Zongjian Li, Bin Zhu, Shaodong Wang, Xianyi He, Yang Ye, Shenghai Yuan, Liuhan Chen, et al. Open-sora plan: Open-source large video generation model. arXiv preprint arXiv:2412.00131, 2024. 1, 6, 7, 4 [24] Chen-Hsuan Lin, Jun Gao, Luming Tang, Towaki Takikawa, Xiaohui Zeng, Xun Huang, Karsten Kreis, Sanja Fidler, MingYu Liu, and Tsung-Yi Lin. Magic3d: High-resolution text-to3d content creation. In IEEE Conference on Computer Vision and Pattern Recognition, 2023. 3 [25] Yaron Lipman, Ricky TQ Chen, Heli Ben-Hamu, Maximilian Nickel, and Matt Le. Flow matching for generative modeling. In International Conference on Learning Representations, 2022. 3, 1 [26] Qiang Liu and Dilin Wang. Stein variational gradient descent: general purpose bayesian inference algorithm. In Advances in Neural Information Processing Systems, 2016. 1 9 [27] Xingchao Liu, Chengyue Gong, and Qiang Liu. Flow straight and fast: Learning to generate and transfer data with rectified flow. In International Conference on Learning Representations, 2023. [28] Ilya Loshchilov and Frank Hutter. Decoupled weight decay regularization. In International Conference on Learning Representations, 2019. 3 [29] Yu Lu, Yuanzhi Liang, Linchao Zhu, and Yi Yang. Freelong: Training-free long video generation with spectralblend temporal attention. In Advances in Neural Information Processing Systems, 2025. 2, 5 [30] Luke Melas-Kyriazi, Iro Laina, Christian Rupprecht, and Andrea Vedaldi. Realfusion: 360deg reconstruction of any object from single image. In IEEE Conference on Computer Vision and Pattern Recognition, 2023. 3 [31] Gyeongrok Oh, Jaehwan Jeong, Sieun Kim, Wonmin Byeon, Jinkyu Kim, Sungwoong Kim, Hyeokmin Kwon, and Sangpil Kim. Mtvg: Multi-text video generation with text-to-video models. In European Conference on Computer Vision, 2023. 1, 2, 6, 8 [32] OpenAI. Hello gpt-4o, 2024. 2 [33] William Peebles and Saining Xie. Scalable diffusion models In IEEE International Conference on with transformers. Computer Vision, 2023. 6, [34] Adam Polyak, Amit Zohar, Andrew Brown, Andros Tjandra, Animesh Sinha, Ann Lee, Apoorv Vyas, Bowen Shi, ChihYao Ma, Ching-Yao Chuang, et al. Movie gen: cast of media foundation models. arXiv preprint arXiv:2410.13720, 2024. 1 [35] Ben Poole, Ajay Jain, Jonathan Barron, and Ben Mildenhall. Dreamfusion: Text-to-3d using 2d diffusion. In International Conference on Learning Representations, 2022. 3, 1 [36] Haonan Qiu, Menghan Xia, Yong Zhang, Yingqing He, Xintao Wang, Ying Shan, and Ziwei Liu. Freenoise: Tuning-free longer video diffusion via noise rescheduling. In International Conference on Learning Representations, 2023. 2, 6, 8, 4 [37] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, et al. Learning transferable visual models from natural language supervision. In International conference on machine learning, pages 87488763. PMLR, 2021. 4, 7 [38] Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Björn Ommer. High-resolution image synthesis with latent diffusion models. In IEEE Conference on Computer Vision and Pattern Recognition, 2022. 8 [39] Olaf Ronneberger, Philipp Fischer, and Thomas Brox. Unet: Convolutional networks for biomedical image segmentation. In Medical image computing and computer-assisted interventionMICCAI 2015: 18th international conference, Munich, Germany, October 5-9, 2015, proceedings, part III 18, pages 234241. Springer, 2015. 6 [40] Chitwan Saharia, William Chan, Saurabh Saxena, Lala Li, Jay Whang, Emily Denton, Kamyar Ghasemipour, Raphael Gontijo Lopes, Burcu Karagol Ayan, Tim Salimans, et al. Photorealistic text-to-image diffusion models with deep language understanding. In Advances in Neural Information Processing Systems, 2022. 8 [41] Tim Salimans and Jonathan Ho. Progressive distillation for fast sampling of diffusion models. In International Conference on Learning Representations, 2022. 6 [42] Uriel Singer, Adam Polyak, Thomas Hayes, Xi Yin, Jie An, Songyang Zhang, Qiyuan Hu, Harry Yang, Oron Ashual, Oran Gafni, et al. Make-a-video: Text-to-video generation without text-video data. In International Conference on Learning Representations, 2023. 8 [43] Jiaming Song, Chenlin Meng, and Stefano Ermon. Denoising diffusion implicit models. In International Conference on Learning Representations, 2020. 2, 3 [44] Yang Song, Jascha Sohl-Dickstein, Diederik Kingma, Abhishek Kumar, Stefano Ermon, and Ben Poole. Score-based generative modeling through stochastic differential equations. In International Conference on Learning Representations, 2020. 3, 1 [45] Zhenxiong Tan, Xingyi Yang, Songhua Liu, and Xinchao Wang. Video-infinity: Distributed long video generation. arXiv preprint arXiv:2406.16260, 2024. 6, 8, [46] Junshu Tang, Tengfei Wang, Bo Zhang, Ting Zhang, Ran Yi, Lizhuang Ma, and Dong Chen. Make-it-3d: High-fidelity 3d creation from single image with diffusion prior. In IEEE Conference on Computer Vision and Pattern Recognition, 2023. 3 [47] Ye Tian, Ling Yang, Haotian Yang, Yuan Gao, Yufan Deng, Jingmin Chen, Xintao Wang, Zhaochen Yu, Xin Tao, Pengfei Wan, et al. Videotetris: Towards compositional text-to-video generation. In Advances in Neural Information Processing Systems, 2024. 2, 8 [48] Christina Tsalicoglou, Fabian Manhardt, Alessio Tonioni, Michael Niemeyer, and Federico Tombari. Textmesh: Generation of realistic 3d meshes from text prompts. In 2024 International Conference on 3D Vision (3DV), pages 1554 1563. IEEE, 2024. 3 [49] Ruben Villegas, Mohammad Babaeizadeh, Pieter-Jan Kindermans, Hernan Moraldo, Han Zhang, Mohammad Taghi Saffar, Santiago Castro, Julius Kunze, and Dumitru Erhan. Phenaki: Variable length video generation from open domain textual descriptions. In International Conference on Learning Representations, 2022. 1, 2, 8 [50] Fu-Yun Wang, Wenshuo Chen, Guanglu Song, Han-Jia Ye, Yu Liu, and Hongsheng Li. Gen-l-video: Multi-text to long video generation via temporal co-denoising. arXiv preprint arXiv:2305.18264, 2023. 2, 3, 4, 6, 7, 8 [51] Haochen Wang, Xiaodan Du, Jiahao Li, Raymond Yeh, and Greg Shakhnarovich. Score jacobian chaining: Lifting pretrained 2d diffusion models for 3d generation. In IEEE Conference on Computer Vision and Pattern Recognition, 2023. [52] Wenjing Wang, Huan Yang, Zixi Tuo, Huiguo He, Junchen Zhu, Jianlong Fu, and Jiaying Liu. Videofactory: Swap attention in spatiotemporal diffusions for text-to-video generation. arXiv preprint arXiv:2305.10874, 2023. 6, 8, 5 [53] Ziyi Wu, Aliaksandr Siarohin, Willi Menapace, Ivan Skorokhodov, Yuwei Fang, Varnith Chordia, Igor Gilitschenski, 10 and Sergey Tulyakov. Mind the time: Temporally-controlled multi-event video generation. In IEEE Conference on Computer Vision and Pattern Recognition, 2025. 1 [54] Desai Xie, Zhan Xu, Yicong Hong, Hao Tan, Difan Liu, Feng Liu, Arie Kaufman, and Yang Zhou. Progressive autoregressive video diffusion models. arXiv preprint arXiv:2410.08151, 2024. 7, 8 [55] Zhuoyi Yang, Jiayan Teng, Wendi Zheng, Ming Ding, Shiyu Huang, Jiazheng Xu, Yuanming Yang, Wenyi Hong, Xiaohan Zhang, Guanyu Feng, et al. Cogvideox: Text-to-video diffusion models with an expert transformer. arXiv preprint arXiv:2408.06072, 2024. 1, 2, 6, 7, 4, [56] Shengming Yin, Chenfei Wu, Huan Yang, Jianfeng Wang, Xiaodong Wang, Minheng Ni, Zhengyuan Yang, Linjie Li, Shuguang Liu, Fan Yang, et al. Nuwa-xl: Diffusion over diffusion for extremely long video generation. arXiv preprint arXiv:2303.12346, 2023. 8 11 Tuning-Free Multi-Event Long Video Generation via Synchronized Coupled Sampling"
        },
        {
            "title": "Supplementary Material",
            "content": "Project page: https://syncos2025.github.io/ A. Foundational concepts and derivations A.1. Further details on preliminaries This section provides additional details on the preliminaries introduced in Section 2 to further guide the readers. Diffusion models. Diffusion models are generative models that learn to gradually transform noise sample from tractable noise distribution towards target data distribution. This transformation is consisting of two processes: forward process and reverse process. In the forward process, noise is incrementally added to the data sample over sequence of timestep t, leading to gradual loss of structure in the data. This forward process is defined as following: xt = αtx0 + 1 αtϵ (4) where αt is pre-defined coefficient that regulates noise schedule over time, and ϵ (0, I) denotes noise sampled from standard normal distribution. The reverse diffusion process aims to invert the forward process, gradually removing the noise and recovering the original target data distribution. This reverse transformation is modeled using neural network (referred to as the diffusion model, ϵΦ), which is trained using loss function based on denoising score matching [17, 44]. The loss function is defined as: Ldiff(Φ; x) = Et,ϵN (0,I) (cid:2)w(t)ϵΦ(xt; t) ϵ2 2 (cid:3) , (5) where w(t) is weighting function applied to each timestep t, and U(0, 1) is drawn from uniform distribution. Score distillation sampling. Score distillation sampling (SDS) [35] introduces new way to optimize any arbitrary differentiable parametric function using diffusion models as critic by posing generative sampling as an optimization problem. The flexibility of SDS in optimizing diverse differentiable operators has made it versatile tool for visual tasks. Specifically, given = g(θ), the gradient of the diffusion loss function of Equation 5 with respect to the parameter θ is expressed as below: θLSDS(Φ; = g(θ)) (cid:20) w(t) (ϵγ = Et,ϵ Φ(xt; c, t) ϵ) ϵγ Φ(xt; c, t) (cid:21) θ (6) 1 In DreamFusion [35], it has been shown that by omitting the Jacobian term of the U-Net (the middle term) in Equation 6 yields an effective gradient for estimating an update direction that follows the score function of diffusion models, thereby moving towards higher-density region. The simplified expression is given as Equation 2 in Section 2. Collaborative score distillation sampling. The original SDS method considers only single sample, x, during the optimization process. Collaborative Score Distillation (CSD) [22] extends SDS to enable inter-sample consistency by synchronizing gradient updates across multiple samples, {x(i)}N i=1. Specifically, CSD generalizes SDS by using Stein Variational Gradient Descent (SVGD) [26] to align gradient directions across multiple samples as Equation 3 in Section 2. For positive definite kernel, k, in Equation 3, CSD employs the standard Radial Basis Function (RBF) kernel. This approach ensures that each parameter update is influenced by other samples with scores adjusted via importance weights based on sample affinity, thereby effectively promoting intersample consistency during optimization. A.2. SynCoS with different diffusion models In this paper, we present our method using diffusion models with ϵ-prediction networks for clarity. However, our framework is compatible with any T2V denoising model. To illustrate this, we describe how applying our framework to different diffusion settings. Flow-based models. Recently, rectified flow [25], unified ODE-based framework for generative modeling, introduces simplified denoising process that optimizes the trajectories in diffusion space to be as straight as possible. Given data sample from the data distribution and noise sample ϵ from Gaussian distribution, rectified flow defines the forward process as: xt = tϵ + (1 t)x0, [0, 1] (7) Accordingly, the reverse process is governed by an ODE that maps ϵ to x0: dxt = v(xt; t)dt, [0, 1] (8) where is velocity field estimated by learnable neural network Φ, where the model is trained using the following objective: Lflow(Φ; x) = Et,ϵ (cid:2)w(t)(ϵ x0) v(xt; t) 2 (cid:3) . (9) Figure 8. Instruction for generating structured prompt. This instruction follows the guidelines to create individual local prompts and shared global prompt based on scenario and the number of prompts the user gives. Score distillation with flow-based models. While SDS and CSD have been implemented on denoising diffusion models with ϵ-prediction networks, the core concept of score distillationusing diffusion models as generative priors for optimizationcan also be extended to flow-based models. Below, we derive the equation for score distillation adapted to flow-based models. By computing the gradient of the training objective Lflow for flow-based models, Φ, with respect to θ, the score distillation sampling adapted to flow-based models can be expressed as: θi LFlowSDS(Φ, = g(θ)) (cid:20) = Eϵ,t w(t) (vΦ(xt, t) (ϵ x)) (cid:21) . θ (10) Here, w(t) is time-dependent weighting function, and vΦ is estimated by the pre-trained flow-based denoising network Φ. In accordance with SDS conventions, the (transformer) Jacobian term is omitted to enhance computational efficiency, enabling the optimization of using the rectified flow model. This loss is then could be reinterpreted within the Collaborative Score Distillation (CSD) framework [22], allowing for synchronous updates across multiple samples instead of treating each gradient independently. 2 B. Experimental details Details of structured prompt. The structured prompt is designed to divide cohesive story (i.e., scenario) into multiple prompts, enabling the generation of long videos controlled by distinct prompts. It consists of single global prompt that describes shared properties across all local prompts (e.g., object appearances or styles) and distinct local prompts that specify changes in objects, motions, or styles. To create structured prompt, we leverage Large Language Model (LLM), as in previous works [31]. The process involves instructing the LLM to segment given scenario into multiple local prompts and detailed global prompt, as illustrated in Figure 8. We use GPT-4o [32] to generate these structured prompts, ensuring consistency across all experiments. These structured prompts are applied in all experiments, including the main evaluations of our method and the baselines, as well as in ablation studies, ensuring fair comparison. To extensively verify our method under various challenging scenarios, we consider both previously established scenarios [21, 29, 31, 36, 47, 50] and our new, more challenging scenarios, including: background changes, object motion changes, camera movements, compositional generation, complex scene transitions, cinematic effects, physical transformation, and storytelling. We experiment with 48 structured prompt scenarios, including 11 with two local prompts, 12 Table 3. Quantitative ablations study of the three coupled stages in SynCoS, omitting each stage during one-timestep denoising, demonstrates the importance of all three stages for coherent long video generation with multiple events."
        },
        {
            "title": "Temporal Quality",
            "content": "Frame-wise Quality"
        },
        {
            "title": "Backbone",
            "content": "M Stage 2"
        },
        {
            "title": "Subject",
            "content": "Background 3 Consistency Consistency"
        },
        {
            "title": "Motion",
            "content": "Dynamic Smoothness Degree"
        },
        {
            "title": "Num",
            "content": "Aesthetic Scenes Quality Quality Imaging Glb Prompt Loc Prompt Fidelity Fidelity 80.46% 78.88% 82.70% 91.14% 91.63% 91.85% 98.55% 97.70% 98.24% 97.92% 14.58% 100.00% 1.229 21. 1.042 53.36% 45.56% 58.42% 42.27% 54.56% 65.53% 0.318 0. 0.325 0.348 0.300 0.348 Figure 9. Qualitative visualization of the ablation study on the three coupled stages of SynCoS. All examples in the second box are 3 times longer in duration compared to the underlying base model. with three, and 25 with four, extending video length by factor of four or five. Full lists are provided on our project page: https://syncos2025.github.io/ Implementation details of the main experiments. DDIM sampling was performed with 50 steps, setting DDIM η to 0. The stride is set to 4 for CogVideoX-2B and 6 for OpenSora Plan (v1.3). In second stage, we set tmin [800, 900], learning rate, lr [0.5, 1] using AdamW Optimizer [28], and iters [20, 50]. The scale of the classifier-free guidance is set to 6 for CogVideoX-2B and 7.5 for Open-Sora Plan (v1.3). For Gen-L-Video, we use the same strides and guidance scale as in our experiments. For FIFO-Diffusion, we set = 4 for the number of partitions in latent partitioning and lookahead denoising, following their best parameter configurations. Measurements of computation time. We measure the computation time required to generate 4 longer video compared to the underlying base model of CogVideoX-2B, using single H100 80GB GPU  (Table 4)  , including both main baselines and our approach. Although our method takes 2.6 longer than the baselines, it achieves notable gains in quality and consistency, demonstrating superior qualitative and quantitative performance. While computation time is not our primary focus, it can be adjusted based on video scenarios and the required level of synchronization. Specifically, reducing iters in the secondstage optimization can lower computation time. Nonetheless, we think reducing computation time while maintaining quality is promising direction in the future. Table 4. Measurements and comparisons of computation time on CogVideoX-2B. Gen-L-Video [50] FIFO [21] SynCoS (Ours) 21 min. 21 min. 55 min. C. Additional ablations Effect of the three coupled stage. In addition to Section 3 and Figure 4, we provide further quantitative evaluations  (Table 3)  and qualitative visualizations (Figure 9) by skipping each stage in SynCoS to assess the efficacy of its three-stage process in generating high-quality multi-event long videos. As discussed in Section 3.1, omitting the first stage in SynCoS, which corresponds to temporal co-denoising with DDIM (i.e., Gen-L-Video), causes denoising paths across chunks to diverge, leading to overlapping artifacts, as shown in Figure 4. This results in reduced temporal consistency and frame-wise quality, as quantified in Table 3, due to abrupt changes. Additionally, this variant often struggles to faithfully follow prompts, as the simple fusion of denoising paths dilutes prompt guidance unique to each chunk. This issue is evident in the second example of city transition (Figure 9), where the scene fails to properly reflect changes in glowing, particularly in the third chunk. 3 Conversely, relying solely on the second stage, corresponding to temporal co-denoising with CSD, degrades image quality significantly. As shown in Figure 4 and Figure 9, the video exhibits noise-like artifacts, leading to severe loss of frame-wise quality and prompt fidelity, as quantified in Table 3. While this approach integrates information across local and distant video frames, it does not produce high-quality long videos, as the resulting video suffers from low image quality, completely failing for high-quality long video generation. In contrast, SynCoS effectively couples both stages, leveraging the output of the first stage as refining source for the second stage, which enhances inter-sample long-range consistency. This integration enables high-quality, long video generation with multiple prompts, achieving smooth transitions, semantic consistency throughout the video, and strong prompt fidelity for each chunk. Ablation study on stride. By default, we set the stride (step size between chunks) to 4 for CogVideoX-2B without extensive tuning for each scenario. For prompts with frequent content changes, reducing the stride improves long-term consistency by increasing information sharing across overlaps. Users can adjust the stride to balance vibrant changes (larger stride) and stronger synchronization (smaller stride), enhancing content consistency. Figure 10 provides qualitative evidence: while stride of 4 introduces slight variations in the knights appearance, stride of 1 ensures greater consistency throughout. Figure 10. Qualitative ablation study on stride. Reducing the stride enhances content consistency, which is beneficial in scenarios like knight running as the background transitions from grass to snow. Although reducing the stride increases computation time slightlyfrom 55 minutes  (Table 4)  to 60 minutesthe impact is minimal. This is because stride reduction does not affect the second stage optimization time, where most of the computational overhead occurs. Instead of processing all chunks simultaneously in this stage, SynCoS uses minibatch approach, randomly selecting chunks from the total chunks at each iteration. Since remains the same for both stride 1 and stride 4, the overall optimization cost remains largely unaffected. 4 D. Further discussions on previous work D.1. Limitations of previous approaches Following the recent success of T2V models, several works have explored extending video diffusion models for longer video generation without additional training. However, we observe that current tuning-free approaches often exhibit undesirable artifacts when applied to recent video diffusion models. In the following sections, we will detail the limitations and failures of these existing methods. Discussion on FIFO-Diffusion. In video diffusion models, all frames within single video chunk are processed at the same timestep during both training and inference. FIFOdiffusion [21] proposes new sampling technique that uses queue to store series of consecutive frames, with the timestep increasing for each frame. While this approach enables the generation of infinitely long videos, it introduces avoidable discrepancies between the timesteps used during training and those used during inference. These discrepancies become more pronounced when applied to recent video diffusion models. This is because as the number of frames processed within single chunk increases, the timestep gap between frames in the same chunk widens. For example, FIFO-diffusion is not susceptible to per-frame artifacts on CogVideoX-2B [55], which encodes 13 frames per chunk. However, in Open-Sora Plan (v1.3) [23], which encodes 24 frames per chunk, these artifacts become significantly more noticeable, as shown in Figure 12. We note that in contrast to existing tuning-free long video generation methods, SynCoS does not introduce training-inference discrepancy and can be seamlessly applied to any video diffusion model. Discussion on FreeNoise. The video diffusion models asis often lack the capability to maintain content consistency across different video chunks beyond single video chunk. FreeNoise [36] addresses this limitation by fusing attention features from temporal layers to establish long-range temporal correlations between different chunks. While this method works effectively with earlier video diffusion models that separate spatial and temporal attention layers (i.e., enabling the fusion of only temporal attention features), we observe that it is not applicable to newer video diffusion models, as illustrated in Figure 12. This limitation arises because recent DiT-based models, widely used in frontier T2V approaches [23, 55], lack dedicated temporal layers. Instead, these models tokenize an entire video chunk into patches and apply attention across all patches. As result, fusing all attention features without additional considerations severely degrades performance, demonstrating that naive fusion techniques are unsuitable for DiT-based models in long video generation. Nonetheless, to comprehensively analyze long video generation, we compare SynCoS with an I2V model (CogVideo5X-I2V) that uses backbone supporting I2V generation and appropriately tuned prompts. As shown in Figure 13, I2V-based autoregressive generation enables smooth scene transitions (e.g., Chihuahua floating in space transitioning to the Chihuahua swimming in the ocean). However, it often struggles to introduce new objects (e.g., brown squirrel and white squirrel, or Chihuahua with fish), limiting its ability to generate long videos that naturally add or change components over time. D.2. Additional comparisons with architecture-specific approaches In our main experiment (Section 5), we primarily compare SynCoS with architecture-compatible, tuning-free methods for multi-event long video generation. These approaches remain compatible with newer diffusion models, leveraging backbone improvements for enhanced generation quality. However, several existing methods [5, 29, 45] are restricted to specific diffusion backbones, making them incompatible with newer or alternative models and limiting their performance to existing architectures. Nonetheless, to ensure comprehensive and fair comparison, we implement SynCoS using the respective architectures of existing tuning-free methods and evaluate its performance in Table 5. We compare SynCoS with VideoInfinity [45] by applying SynCoS to VideoCrafter2 [52], U-Net-based video diffusion model. Additionally, to compare against DitCtrl [5] (built on Multi-Modal Diffusion Transformer (MM-DiT)), we evaluate SynCoS under the same backbone of CogVideoX-2B [55]. Notably, SynCoS significantly outperforms all baselines in temporal consistency and video quality while achieving comparable prompt fidelity, demonstrating its robustness across various diffusion backbones. Table 5. Quantitative comparison with architecture-specific baselines. *Abbreviations: subject consistency (SC), background consistency (BC), aesthetic quality (AQ), and prompt fidelity (PF)."
        },
        {
            "title": "Method",
            "content": "SC BC AQ VideoCrafter2 [52] CogVideoX-2B [55] Video-Infinity [45] 0.879 0.943 0."
        },
        {
            "title": "SynCoS",
            "content": "DitCtrl [5]"
        },
        {
            "title": "SynCoS",
            "content": "0.911 0.947 0.648 0.821 0.916 0. 0.864 0.927 0.643 PF 0.365 0. 0.394 0.381 Comparisons with autoregressive generation using I2V. Long videos can also be generated using image-to-video (I2V) models by generating single video chunk with T2V and then conditioning the last frame of the previous chunk to the I2V model. For two reasons, we do not directly include autoregressive generation using the I2V model in the main paper. First, using an I2V model requires different prompt structure. Unlike our structured prompts, which focus on describing changes in local chunks, I2V models require explicit descriptions of transitions between chunks. For example, to generate an artistic video of butterfly on flower across changing seasons, our structured prompts might include: (1) In spring, butterfly is on flower, and (2) In summer, butterfly is on flower. In contrast, I2V prompts would need to explicitly describe transitions, such as: (1) In spring, butterfly is on flower, and (2) The season changes from spring to summer, and the butterfly is on flower. Second, the base models used in our experiments do not support I2V generation. 5 (a) Long video generation on CogVideoX-2B, where single video chunk consists of 26 frames. (b) Long video generation on Open-Sora Plan (v1.3), where single video chunk consists of 49 frames. Figure 11. Long video generation results. All generated videos are 4 times longer than the underlying base model. FIFO significantly suffers from noise-like artifacts on Open-Sora Plan (v1.3) due to inevitable training-inference discrepancy in their design. Figure 12. Long video generation result of FreeNoise on CogVideoX-2B. This generated video is 4 times longer than the underlying base model. Each frame is generated well in the early frames, where no fusion is applied. However, as soon as the fusion of attentional features is applied, the generated video shows stagnant results of repeated object motion without any scene changes, eventually leading to the entire failure of video generation. Figure 13. Qualitative comparison with autoregressive generation using image-to-video (I2V) model for long videos. While autoregressive generation with I2V models effectively handles scene transitions, it often struggles to introduce new components into the video. 6 E. Pseudo-code implementation of each stage. We describe each stage with pseudo-code implementations. In the first stage of temporal co-denoising with DDIM, the video is divided into overlapping chunks, where each chunk undergoes diffusion forward pass and DDIM update to compute x(i) 0 . The chunks are then fused to obtain 0, as detailed in Figure 14. In the second stage, the fused 0 is further refined by re-dividing it into overlapping chunks and applying CSD-based optimization. While we use CSDbased optimization to enforce global coherence, it differs from the original CSD, as the second stage is adjusted using grounded timestep and fixed baseline noise, ensuring synchronous coupling across all three stages. The iterative refinement process is illustrated in Figure 15. Lastly, the refined 0 from the second stage is converted using baseline noise to prepare for subsequent steps in the diffusion pipeline. F. Additional qualitative results We consider long videos of multiple events with the following challenging scenarios: object motion control, cinematic effects, storytelling, camera control, background changes, physical transformations, complex scene transitions, and compositional generation. For generated videos handling object motion control and camera control scenes, please refer to Figure 1, and other video examples are visualized in Figure 16 and Figure 17. To view all generated videos, refer to our project page: https://syncos2025.github.io/. 7 4 3 1 def first_stage ( videos , 2 prompt_embeds_for_chunks , model , scheduler , guidance_scale , timestep , stride 7 6 5 8 9 ): 10 # video tensor of shape [T , , , W] # Text prompt embeddings for each chunk # The denoising model # Scheduler to refine videos # Scale factor for classifier - free guidance # Current timestep in the diffusion process # Stride size for dividing videos 11 12 13 14 16 17 18 19 20 22 23 24 25 26 28 29 30 31 32 34 35 36 37 38 40 41 42 43 44 46 47 48 49 50 52 53 54 55 56 58 59 # Initialize list to store video chunks and count tensor videos_for_chunks = [] count = torch . zeros_like ( videos ) # Split videos into strides for diffusion forward for start in range (0 , videos . shape [0] , stride ): # Determine the end of the current stride end = start + stride # Slice the videos for the current stride chunks = videos [ start : end ] # Increment count for the covered range count [ start : end ] += 1 # Append the current stride to the list for processing videos_for_chunks . append ( chunks ) # Concatenate all processed video chunks into single tensor for inference videos_for_chunks_input = torch . cat ( videos_for_chunks , dim =0) # Diffusion forward noise_pred = predict_noise ( model , videos_for_chunks_input , prompt_embeds_for_chunks , timestep ) # Apply classifier - free guidance to refine noise predictions noise_pred = apply_classifier_free_guidance ( noise_pred , guidance_scale ) # Perform scheduler step to update videos and extract the denoised sample res = scheduler_step ( scheduler , noise_pred , timestep , videos_for_chunks ) videos_x0 = res . pred_original_sample # Initialize the aggregated result tensor value_x0 = torch . zeros_like ( videos ) # Aggregate the results into ` value_x0 ` idx = 0 for start in range (0 , videos . shape [0] , stride ): # Define the range for the current stride start + stride # Accumulate the processed values for the corresponding stride value_x0 [ start : end ] += videos_x0 [ idx ] idx += 1 # Compute the final denoised prediction by fusing # Wherever ` count > 0`, compute the averaged value , else fallback to ` value_x0 ` pred_original_sample = torch . where ( count > 0, value_x0 / count , value_x0 ) return pred_original_sample Figure 14. Pseudo-code implementation of the first stage of SynCoS. 8 1 def second_stage ( 2 pred_original_sample , prompt_embeds_for_chunks , timestep , cfg_scale , opt # Initial video prediction # Text prompt embeddings for each chunk # grounded timestep # Guidance scale for classifier - free guidance # Optimization hyperparameters and options ): # Initialize target predictions with gradients enabled tgt_pred_sample = pred_original_sample . clone () . detach () tgt_pred_sample . requires_grad = True # Optimization hyperparameters lr = opt . lr wd = opt . wd decay_iter = opt . decay_iter decay_rate = opt . decay_rate num_steps = opt . num_steps batch_size = opt . batch_size stride = opt . stride # Initialize optimizer and learning rate scheduler optimizer = opt . optimizer ([ tgt_pred_sample ], lr =lr , weight_decay = wd ) scheduler = opt . scheduler ( optimizer , step_size = decay_iter , gamma = decay_rate ) # Fix baseline noise for the entire tensor base_noise_all = torch . randn_like ( tgt_pred_sample ) for step in range ( num_steps ): optimizer . zero_grad () # Initialize tensors for counting and score accumulation count = torch . zeros_like ( tgt_pred_sample , dtype = tgt_pred_sample . dtype ) score_value = torch . zeros_like ( tgt_pred_sample , dtype = tgt_pred_sample . dtype ) # Initialize lists to store videos and noise chunks videos_for_chunks = [] basenoise_chunks = [] prompt_embeds_chunks = [] # Determine valid subset indices for this iteration num_chunks = ( tgt_pred_sample . shape [0] + stride - 1) // stride subset_indices = torch . randperm ( num_chunks ) [: batch_size ] # Process selected chunks for , start in enumerate ( range (0 , tgt_pred_sample . shape [0] , stride )): if not in subset_indices : continue # Determine the end of the current stride end = start + stride # Slice the videos for the current stride and add noise chunks = tgt_pred_sample [ start : end ] noisy_chunks = add_noise ( chunks , base_noise_all [ start : end ], timestep ) basenoise_chunks . append ( base_noise_all [ start : end ]. unsqueeze (0) ) prompt_embeds_chunks . append ( prompt_embeds_for_chunks [i ]) # Update count and append to chunks list count [ start : end ] += 1 videos_for_chunks . append ( noisy_chunks . unsqueeze (0) ) # Concatenate videos for model inference videos_for_chunks_input = torch . cat ( videos_for_chunks , dim =0) basenoise_chunks = torch . cat ( basenoise_chunks , dim =0) prompt_embeds_chunks = torch . cat ( prompt_embeds_chunks , dim =0) # Predict noise using the model noise_pred = predict_noise ( opt . model , videos_for_chunks_input , prompt_embeds_chunks , timestep ) # Apply classifier - free guidance noise_pred = apply_classifier_free_guidance ( noise_pred , cfg_scale ) # Compute CSD scores scores = calculate_csd_loss ( videos_for_chunks_input , noise_pred , basenoise_chunks ) # Aggregate scores back to the appropriate positions for , start in enumerate ( range (0 , tgt_pred_sample . shape [0] , stride )): if not in subset_indices : continue # Determine the end of the current stride end = start + stride # Accumulate scores for the current range score_value [ start : end ] += scores [ start : end ] # Compute gradients using scores normalized by count grad_all = torch . where ( count > 0, score_value / count , score_value ) tgt_pred_sample . backward ( gradient = grad_all ) # Perform optimization step optimizer . step () scheduler . step () # Detach and return the updated target prediction tgt_pred_sample = tgt_pred_sample . clone () . detach () . requires_grad_ ( False ) 4 5 6 7 8 10 11 12 13 14 16 17 18 19 20 22 23 24 25 26 28 29 30 31 32 34 35 36 37 38 40 41 42 43 44 46 47 48 49 50 52 53 54 55 56 58 59 60 61 62 64 65 66 67 68 70 71 72 73 74 76 77 78 79 80 82 83 84 85 86 88 89 90 91 92 94 95 96 97 98 100 101 return tgt_pred_sample , base_noise_all Figure 15. Pseudo-code implementation of the second stage of SynCoS. 9 (a) Cinematic effects (b) Storytelling Figure 16. Multi-event long video generation results showcasing challenging scenarios, including cinematic effects, storytelling, and background changes. Each example is 2-4 times longer in duration compared to the underlying base model, resulting in 11 to 21-second videos at 24 fps, with total of 256 to 512 frames. (c) Background changes 10 (d) Physical transformation (e) Complex scene transitions Figure 17. Multi-event long video generation results showcasing challenging scenarios, including physical transformation, complex scene transitions, and compositional generation. Each example is 2-4 times longer in duration compared to the underlying base model, resulting in 11 to 21-second videos at 24 fps, with total of 256 to 512 frames. (f) Compositional generation"
        }
    ],
    "affiliations": [
        "Adobe Research",
        "KAIST"
    ]
}