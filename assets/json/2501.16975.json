{
    "paper_title": "Over-Tokenized Transformer: Vocabulary is Generally Worth Scaling",
    "authors": [
        "Hongzhi Huang",
        "Defa Zhu",
        "Banggu Wu",
        "Yutao Zeng",
        "Ya Wang",
        "Qiyang Min",
        "Xun Zhou"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Tokenization is a fundamental component of large language models (LLMs), yet its influence on model scaling and performance is not fully explored. In this paper, we introduce Over-Tokenized Transformers, a novel framework that decouples input and output vocabularies to improve language modeling performance. Specifically, our approach scales up input vocabularies to leverage multi-gram tokens. Through extensive experiments, we uncover a log-linear relationship between input vocabulary size and training loss, demonstrating that larger input vocabularies consistently enhance model performance, regardless of model size. Using a large input vocabulary, we achieve performance comparable to double-sized baselines with no additional cost. Our findings highlight the importance of tokenization in scaling laws and provide practical insight for tokenizer design, paving the way for more efficient and powerful LLMs."
        },
        {
            "title": "Start",
            "content": "Over-Tokenized Transformer: Vocabulary is Generally Worth Scaling Hongzhi Huang 1 Defa Zhu 1 Banggu Wu 1 Yutao Zeng 1 Ya Wang 1 Qiyang Min 1 Xun Zhou 1 5 2 0 2 8 2 ] . [ 1 5 7 9 6 1 . 1 0 5 2 : r a"
        },
        {
            "title": "Abstract",
            "content": "Tokenization is fundamental component of large language models (LLMs), yet its influence on model scaling and performance is not fully explored. In this paper, we introduce OverTokenized Transformers, novel framework that decouples input and output vocabularies to improve language modeling performance. Specifically, our approach scales up input vocabularies to leverage multi-gram tokens. Through extensive experiments, we uncover log-linear relationship between input vocabulary size and training loss, demonstrating that larger input vocabularies consistently enhance model performance, regardless of model size. Using large input vocabulary, we achieve performance comparable to doublesized baselines with no additional cost. Our findings highlight the importance of tokenization in scaling laws and provide practical insight for tokenizer design, paving the way for more efficient and powerful LLMs. 1. Introduction The rapid advancements in large language models (LLMs) have been driven by innovations in model architectures (Vaswani et al., 2017) and training paradigms (Radford et al., 2019; Brown et al., 2020). Moreover, with the guidance of scaling laws (Kaplan et al., 2020), models prove to become stronger with increasing number of parameters or training data. Tokenization, the process of converting raw text into discrete tokens as the input and output of the model, is recently found relevant to scaling laws, where larger models deserve larger vocabulary and achieve better performance under the same training cost (Tao et al., 2024). In fact, expanding the input vocabulary incurs almost no additional computational cost, whereas expanding the output vocabulary significantly increases the training overhead for smaller models. Thus, it is natural to consider decoupling the input and output vocabularies for separate investigations. 1Seed-Foundation-Model Team, Bytedance. Correspondence to: Hongzhi Huang <huanghongzhi.51@bytedance.com>. Figure 1: Scaling trend for Over-Encoded models and baselines on OLMo2. We plot the loss with 400B tokens training. For over-encoding, input vocabulary size is extended from 0.1 to 1.2 and 12.8 million (12 and 128 larger than baseline), referred to as OE-1.2M and OE-12.8M. We observe OE-12.8M with 400M parameters matches the baseline with 1B parameters. Our research is fundamentally based on this idea. Starting with synthetic experiments on context-free grammar modeling, we systematically analyze the effects of token granularity and vocabulary size on models of varying scales. Firstly, it is revealed that larger tokenizers improve the performance of larger models while introducing challenges for smaller ones, which is consistent with previous studies. Furthermore, once the input and output vocabulary is decoupled, we find that scaling up input vocabulary solely keeps improving model, while larger output vocabulary may be harmful to smaller models. This insight motivates the development of Over-Tokenized Transformers, which decouple the encoding and decoding vocabularies to achieve greater flexibility and performance gains. Specifically, we introduce Over-Encoding(OE), which utilizes large hierarchical n-gram input vocabulary. As shown in Figure 1, our method improves the model scalability by significant margin. Increasing the input vocabulary size by 128, our 400M model matches the training loss of 1B baseline with no additional training cost (see left panel). More interestingly, we observe strong log-linear relationship between the input vocabulary size and model performance, i.e., exponentially increasing the input vocabulary size consistently results in linear decrease in loss (see right panel). These findings represent new dimension 1 in scaling laws and also indicate embedding parameters as new scalable sparse dimension. Moreover, we propose the concept of Over-Decoding(OD), which leverages larger output vocabulary to provide more fine-grained supervision. We typically treat multi-token prediction methods (Gloeckle et al., 2024; DeepSeek-AI et al., 2024) as approximations of OD. Combining OE and OD together, we build OverTokenized Transformer, which shows greater potential than apply either OE or OD solely. Although introducing large amount of embedding parameters for OE, the training and inference cost barely increases, as embedding parameters are used in an extremely sparse manner. In practice, we propose efficient engineering solutions to mitigate the computational and memory challenges introduced by large vocabularies, resulting an additional training overhead less than 5%. Through this work, we aim to bridge the gap between tokenizer design and model scaling, positioning tokenization as critical factor in the continued evolution of large language models. 2. Related Work 2.1. Tokenization Design Tokenization is critical component in the development of large language models (LLMs). Established methods such as Byte-Pair Encoding (BPE) (Sennrich, 2015) and Unigram Language Models (Kudo, 2018) have been widely adopted to create subword vocabularies that balance sequence length and vocabulary size. Recently, alternative paradigms have been proposed to address the computational inefficiencies of byte-level models (Xue et al., 2022; Yu et al., 2023). For instance, MegaByte (Yu et al., 2023) combines byte-level modeling with patching by first predicting patches and then predicting bytes within each patch, improving processing efficiency. However, its performance at scale remains inferior to that of tokenizer-based approaches. Building on this concept, In this work, our findings suggest that applying n-gram patching on top of BPE expands the vocabulary size, improving model capability, while BLT (Pagnoni et al., 2024) also has the same finding on byte-level models. 2.2. Scaling Vocabulary Recent empirical studies have systematically investigated the relationship between vocabulary size and model performance. (Tao et al., 2024) demonstrates that expanded vocabularies enhance both training efficiency and model performance, particularly in larger architectures. Building on these findings, we argue that vocabulary size research should separately consider embedding (input) and unembedding (output). While embedding incurs only lookup costs, unembedding introduces computational costs that scale with Figure 2: Performance comparison for models trained on CFG data. The left panel compares 1-gram and 3-gram tokenizers, showing that 3-gram improves larger (85M parameters) models but harms smaller (2.4M parameters) ones. The right panel examines 3-gram usage in encoders and decoders, revealing consistent gains with 3-gram encoders regardless of model size, while 3-gram decoders degrade performance in smaller models. vocabulary size. More importantly, we find that input and output vocabularies exhibit distinct scaling behaviors, underscoring the need for independent scaling strategies to optimize model design. 2.3. Multi-Token Prediction and n-Gram Modeling Multi-Token Prediction (MTP) (Gloeckle et al., 2024) has advanced the field of next-token prediction through the introduction of auxiliary objectives for simultaneous multi-token prediction. This methodology shares fundamental principles with our n-gram modeling framework, where multi-token prediction can be theoretically formulated as an approximation of n-gram unembedding utilization (over-decoded models). In our work, we further compare the effectiveness of MTP and over-encoded models, demonstrating that the performance gains from MTP and over-encoded models are complementary and can be combined to achieve even greater improvements. 3. Method 3.1. Insights from Synthetic Experiments We initiate our research on tokenizers on synthetic language modeling task to investigate their impact on model performance. Following the experimental setup in the previous study (Allen-Zhu & Li, 2024), we employ Context-Free Grammar (CFG) as the target language to generate sequences composed of 3 distinct characters, with sequence lengths up to 729. With this setup, the ground-truth language distribution is completely known, enabling precise evaluation of the language models. We train GPT-2 mod2 vocabulary of size 3n, and the unembedding layer remains 1-gram, predicting next one character. In the n-gram decoding model, the input remains 1-gram tokens while the target labels (i.e., the next token) are converted to n-gram labels, resulting fine-grained classification head that predicts the conditional joint distribution of next tokens. Note that the training sequence length remains unchanged and matches the length produced by the 1-gram tokenizer for both variants. To maintain comparable inference costs, the n-gram output model does not generate tokens simultaneously. Instead, it samples an n-gram prediction but preserves the next 1 token only, disregarding extra token predictions during inference. The results for = 3 of the two variants are shown in the right panel of Figure 2. We find that the two models exhibit different behaviors. The 3-gram encoding model consistently enhances performance across all model sizes. However, the 3-gram decoding model improves performance for larger models but degrades it for smaller ones. We conclude that, when using large tokenizers, the large input vocabulary is always positive while the large output vocabulary can be negative for smaller models. We hypothesize that the difference lies in their respective roles: the input embedding is responsible for encoding the context into feature embeddings, where larger vocabulary enhances the representational capacity of the feature mapping, thereby positively impacting the model. In contrast, the output vocabulary determines the granularity of the prediction task. larger output vocabulary implies more fine-grained supervision signals, which can either be beneficial (e.g., for large models prone to overfitting) or burdensome (e.g., for smaller models suffering from severe underfitting). Motivated by this observation, we extend our exploration to over-tokenized transformers in real-world natural language modeling. 3.2. Over-Tokenized Transformer Above all, we use standard natural language tokenizer as the base tokenizer. Then, the challenge arises: if the base tokenizer has vocabulary size of , which is usually as large as 105, the n-gram vocabulary with size of becomes exceedingly large and impractical. To address this, we propose approximating the large embedding table through series of matrix decompositions. Given sequence of input ids x1, x2, . . . , xt from the base tokenizer, we define the n-gram input token x(n) as follows: x(n) = (xi, xi1, . . . , xin+1), (1) where (z1, ..., zn) is an index mapping function, and outof-range indices are padded with zero tokens, i.e., xi = 0, [1, t]. One intuitive design treats (z1, . . . , zn) as Figure 3: Illustration of 2-gram encoding/decoding GPT. Note that 2-gram decoding only preserves the predicted next 1 token though next 2 is predicted, which keeps inference cost identical to the vanilla model. els (Radford et al., 2019) of various sizes on CFG-generated samples using next-token prediction loss and evaluate them based on the accuracy of model-generated sequences, which measures the proportion of valid generations. Additional details regarding the experimental setup can be found in Appendix B.2. Our first experiment aims to compare the performance of language models using tokenizers with varying levels of granularity. baseline tokenizer constructs vocabulary using the three terminal characters defined by the CFG, tokenizing sentences character-wisely, which we refer as 1gram tokenizer. We further define n-gram tokenizers, whose vocabulary comprises all 3n possible combinations of sequential characters. We train both larger and smaller GPT2 models using 1-gram and 3-gram tokenizers, respectively. As illustrated in the left panel of Figure 2, we observe that larger tokenizer improves the performance of larger models but negatively impacts smaller models. Notably, larger tokenizers result in shorter training sequences, which significantly reduces training cost. Thus, training larger models with 3-gram tokenizer not only reduces training costs but also enhances model performance. An intuitive insight is that larger models benefit from larger vocabularies, improving both training efficiency and performance. This finding is also supported by previous studies (Tao et al., 2024). To decouple the influences of enlarging input and output vocabularies, we separately introduce n-gram encoding and decoding models, as illustrated in Figure 3. To begin, the raw text is tokenized character-wisely via 1-gram tokenizer. In the n-gram encoding model, input tokens are converted to n-gram tokens at the input layer, making large input 3 p-base number, defining as (z1, ..., zn) = (cid:88) i= zipi1, (2) where ensures that is bijection. Typically, is set to to keep the range of as compact as possible. Notably, x(1) = xi corresponds to the standard transformer input. General n-gram Embedder. The key to designing an flexible n-gram embedder module is to make the vocabulary size configurable. We achieve this efficiently using simple tiled matrix parameterization approach. Specifically, the tiled matrix parameterization extends an embedding table into embedding table by tiling, where is configurable size. In practice, the lookup process is straightforward: an input token x(n) is mapped by taking its modulus with respect to m. In summary, our n-gram embedder is formalized as (cid:16) (cid:17) = x(n) % , (3) where is the output embedding, Rmd is the embedding matrix, and % is the modulo operation. We denote this n-gram embedder as Emd(x(n)). Over-Encoding(OE). We find hierarchical encoding paradigm to be highly effective. Specifically, we compute the input embedding to the GPT model as the sum of 1-, 2-, ..., n-gram embeddings. Additionally, we observe further benefits from using smaller embedding dimensions. An embedder Emd can be sliced into low-rank decomposed embedders, represented as: Emdk(x(n)) = (cid:88) i=1 Em (x(n))Wi, (4) where Wi dmodel projects the embedding vector to match the model dimension. Using the same number of embedding parameters and incurring only minimal additional cost through dense matrices Wi dmodel , this approach significantly enhances performance. Overall, the over-encoding process maps an input token to an embedding as follows: OE(x) = EV d(x(1)) + (cid:88) i=2 Em n k(x(n)), (5) where the 1-gram embedding EV d(x(1)) is implemented consistently with the original Transformer to align with the tied weight design. Generally, is set to value much larger than , and the model performance is observed to improve consistently as increases. 4 Notably, for multiple embedders with rows, minor adjustments (e.g., replacing with + 2) are made to ensure each embedder has unique mapping. This increases the combinatorial capacity of the embedding; otherwise, the slice trick would make no difference. detailed pytorchlike implementation for OE can be found in Appendix A. We also acknowledge concurrent work, BLT (Pagnoni et al., 2024), which employs similar n-gram hashing embedding strategy for byte-level tokens. Over-Decoding(OD). Based on the conclusions from our CFG experiments, decoding additional tokens is only effective for sufficiently large models. As matter of fact, previous researches on Multi-Token Prediction (MTP) (Gloeckle et al., 2024) are typically approximations of over-decoding, and share the same conclusion that only large models benefit from future token predictions. Generally, MTP-like methods are viewed as over-decoding in this paper. In addition, we explore other solutions for over-decoding in Appendix for reference. Over-Tokenized Transformer(OT). Integrating overencoding and over-decoding, we obtain over-tokenized transformer. Specifically, we focus on the conditional recursive form of MTP proposed in DeepSeek V3 (DeepSeek-AI et al., 2024), which we refer as MTP-DS. In this formulation, MTP no longer predicts the next few tokens in parallel but instead predicts them sequentially. For the n-th head, the embedding of the next (n 1)-th token is concatenated to the layer input as condition for the next n-th token prediction. Under MTP-DS architecture, over-encoding enhances the representation capacity of token embeddings and directly participates future token predictions. On the one hand, the future token prediction tasks become easier to learn. On the other hand, the over-encoding can be trained more sufficiently. With these advantages, the integration of the two methods yields greater benefits, even on relatively smaller models. 3.3. Engineering Challenges and Solutions The over-encoding constructs very large input vocabulary. In theory, as the embeddings are sparsely accessed based on token ids, enlarging vocabulary should barely impact the training or inference cost. However, the large embedding parameters can place substantial memory pressure on GPUs. Furthermore, when parameter sharding strategies, such as FSDP (Zhao et al., 2023), are applied during training, the communication of these sparse parameters can severely degrade training efficiency, further constraining the choice of (the vocabulary size) to smaller values. To mitigate this issue, we recommend using tensor parallelism specifically for the over-encoding embedding layer to reduce communication overhead. The embedding table is row-wise sharded across all data-parallel (DP) ranks. For given input, the token is sent to the corresponding DP rank that holds its embedding, queried for the embedding vector, and then the resulting embedding is sent back to the original DP rank. This process involves two all-to-all communications during the forward pass and one all-to-all communication during the backward pass, resulting in total communication volume significantly lower than that of FSDP. We implement this optimization and find that training an over-encoded model with = 107 on FSDP reduces throughput by less than 5%. In contrast, without this optimization, FSDP experiences 25% slowdown and easily runs out of memory for vocabulary sizes exceeding = 5 106. We believe that the current implementation still does not represent the limit of over-encoding performance optimization. Its greatest advantage is that the vocabulary input is decoupled from the model architecture. This decoupling allows the embedding lookup for the next micro-batch to be performed in advance. For example, we can design dedicated embedding lookup stage in pipeline-parallel training framework, which overlaps the communication required for embedding lookups with the transformer forward of the current micro-batch. This strategy would maintain training throughput without any performance degradation. Additionally, under this approach, the over-encoding parameters could be offloaded to the CPU, completely alleviating GPU memory pressure. Notably, similar training frameworks have already been implemented (Fang et al., 2022; Li et al., 2023). Over-Encoding could leverage these designs to improve model performance with minimal additional cost. 4. Experiments Implementation. Our experiments focus on large language modeling tasks and basically follow OLMo series works. We maintain the training configuration of baseline model, replacing the word embedding with over-encoding technique. The original tokenizer used in baseline models is preserved as the base tokenizer in our method. The experiments are run with engineering optimized implementation. We refer OE-m to over-encoded models with dmodel embedding parameters in total. Typically, we implement with = 3, = 1.28 107, i.e. OE-12.8M, and variate according to dmodel, making dmodel nk 256. Metrics. We report the average perplexities (PPL) and losses on the c4 en-validation dataset as the Eval PPL or Eval Loss, along with the metrics for zero-shot evaluation on downstream benchmarks. We observe significant volatility in the zero-shot performance indicators for the datasets. For more reliable and consistent results, we mainly concern five stable tasks in our analysis: MMLUVar, Hellaswag, ARC-Challenge, ARC-Easy and PIQA. Detailed descriptions on these tasks and more comprehensive evaluations can be found in Appendix B. 4.1. Over-Encoding Scaling Trend Dense Models We follow the experimental setup described in OLMo2 (OLMo et al., 2024), which is currently the state-of-the-art fully open model. We maintain most of the training configurations, but modify the architecture to obtain models with 151M, 400M and 1B dense parameters, which is denoted by OLMo2-151M, OLMo2-400M and OLMo2-1B, respectively. We train OLMo2-151M and OLMo2-400M models with 400B tokens, and OLMo2-1B with 1T tokens. In addition to OE-12.8M models, we also run experiments for OE-1.2M to offer rough scaling trend on vocabulary size. The result is ploted in Figure 1. From comparisons across model scales, OE models hold consistent improvements from baseline models. Specifically, the OE-12.8M model achieves comparable performance to baseline model that is 2.5 times larger in model scale (comparing the 400M OE model with the 1B baseline model). Moreover, the vocabulary size of OE-12.8M, OE-1.2M, and the baseline model (vocabulary size = 100278) exhibits exponential growth, while their scaling curves maintain approximately equal spacing. This observation suggests log-linear relationship between vocabulary size and model performance, which we further validate in the ablation studies. We present the training dynamics for OLMo2-1B models in Figure 4. Compared to the baseline model, OE demonstrates consistently increasing performance gains, with an improvement of 0.12 at 400B tokens that expands to 0.14 at 1T tokens. From the perspective of convergence acceleration, training loss achieves remarkable 5.7 speedup. Furthermore, in terms of downstream evaluation, OE exhibits substantial acceleration, achieving 3.2, 3.0, 2.6, 3.1 and 3.9 speedups on MMLU-Var, Hellaswag, ARCChallenge, ARC-Easy and PIQA, respectively. thorough evaluation is ploted in Appendix B.3. MoE Models Sparse Mixture-of-Experts (MoE) models (Shazeer et al., 2017) have also achieved great success in recent years. These models aim to introduce sparse FeedForward Network (FFN) modules, enabling the addition of large number of sparse parameters to improve model performance while keeping the same number of activated parameters, making the training and inference costs unchanged. Similarly, we evaluate the effectiveness of OE under the MoE architecture. 5 Figure 4: Training curves for OE-12.8M and baseline model on OLMo2-1B. The metrics are smoothed via exponential moving average with weight 0.99 for loss and 0.9 for downstream tasks. We observe significant convergence acceleration for the OE model: 5.7 on loss, 3.2 on MMLU-Var, 3.0 on Hellaswag, 2.6 on ARC-Challenge, 3.1 on ARC-Easy and 3.9 on PIQA. Table 1: Performance of Over-Encoding on MoE architecture with 500B tokens training. The column Emb. P. represents Embedding Parameters. Downstream stands for the average of MMLU-Var, Hellaswag, ARC-Challenge, ARC-Easy, and PIQA. For +OE rows, we provide metric difference with blue labels. Model # Emb. P. Loss Downstream OLMoE-1.3B +OE-12.8M OLMoE-7B +OE-12.8M 51M 13.1B 102M 26.3B 2.554 2.472 -0.082 2.305 2.229 -0.076 0.510 0.524 +0.014 0.601 0.608 +0.007 ding parameters as the model scales up (i.e., 10 dense parameters for OLMoE-1.3B and 3.7 for OLMoE-7B). However, in terms of downstream evaluation metrics, the performance improvement of OE diminishes. We hypothesize that this reduction is related to the sparse parameters utilized in the MoE architecture, which may overlap with the benefits provided by sparse embedding parameters. 4.2. Ablation Study We conduct ablation study basing on the setup of OLMoE1.3B. To save computational resources, for ablation variants that are non-promising, we only train for 50B tokens. Otherwise, we train for 500B tokens to obtain solid conclusions. We follow the experimental setup described by OLIn the experiments, MoE (Muennighoff et al., 2024). OLMoE-1.3B refers to model with 260M active parameters and total of 1.3B parameters, while OLMoE-7B refers to models with 1.3B active parameters and total of 7B parameters. In this setting, the base tokenizer has vocabulary size = 50280, and we train 500B tokens for all these models. The results are shown in Table 1. We first compare the training loss. At two different model scales, OE-12.8M achieves approximately the same improvement in loss compared to the baseline, despite decreases in the proportion of embedVocabulary Size Scaling. We first analyze the scaling behavior of over-encoding based. Specifically, we use the simplest over-encoding setup with fixed = 2 and = 1, while varying the value of m. The size of ranges from 20K to 12.8M. We train on 500B tokens and evaluate the model performance. As shown in the figure, the experimental results reveal logarithmic linear relationship between the training loss and m: for every 4 increase in m, the training loss decreases by 0.015. Finally, we extracted the loss at 500B tokens and plotted the scaling curve shown in Figure 5. Here, the input vocabulary size is defined as the sum of the sizes of the 1-gram and 2-gram vocabularies, i.e., Table 2: Ablation study on different input vocabulary designs. The downstream tasks follow the eval settings in OLMoE, where MMLU-V stands for MMLU-Var, HS for Hellaswag, ARC-C for ARC-Challenge and ARC-E for ARC-Easy. All models are trained with 500B tokens."
        },
        {
            "title": "Downstream",
            "content": "Loss PPL Loss PPL MMLU-V HS ARC-C ARC-E PIQA"
        },
        {
            "title": "1 OLMoE-1.3B\n2 +E3.2M×d(x(−2))\n3 +E6.4M× d\n2 (x(−2))\n4 +E3.2M×d|2(x(−2))\n5 +E3.2M×d|4(x(−2))\nE3.2M× d\n6 +(cid:80)\ni∈{2,3}\n7 +E12.8M×d(x(−2))\n8 +(cid:80)",
            "content": "i E12.8M i{2,3} 2 2 2.554 12.864 2.924 18. 2.511 12.319 2.887 17.944 2.507 12.268 2.882 17.851 2.503 12.221 2.877 17.754 2.503 12.226 2.876 17.736 (x(i)) 2.495 12.127 2.870 17. 2.493 12.100 2.881 17.832 (x(i)) 2.472 11.854 2.862 17.494 2 2 0.327 0.340 0.330 0. 0.328 0.340 0.334 0.342 0.553 0. 0.573 0.575 0.575 0.578 0.569 0. 0.351 0.341 0.345 0.337 0.330 0. 0.577 0.329 0.622 0.656 0.648 0. 0.653 0.636 0.643 0.645 0.727 0. 0.731 0.740 0.734 0.738 0.730 0. Table 3: Ablation study for the hierarchical design of overencoding. The symbol on the n-gram column denotes n-gram token x(n) is adopted. The experiments are conducted with = 3.2M, and the metrics are reported after training on 50B tokens. 1-Gram 2-Gram 3-Gram Loss PPL - 1 1 4 4 2.714 2.785 2.678 2.670 2.684 2.667 15.094 16.205 14.555 14.447 14.642 14.394 and C-5). Then, C-3 is constructed as baseline, where we keep using one embedding table but scale up and scale down d. Such configuration also improves baseline but underperforms C-4 and C-5. We attribute this to the reduced memory access: configuration 4 accesses 2dmodel embedding parameters while configuration 3 accesses only 1.5dmodel per token. Moreover, we validate the benefits of enlarging from 2 to 3 by comparing C-5 and C-6. With longer-range dependencies introduced, C-6 further improves performance. Lastly, these tricks are then applied on larger vocabulary with = 12.8M (comparing C-7 and C-8). We observe even larger gains comparing to experiments with = 3.2M. Note that C-6 and C-8 meets our default OE implementation, i.e., OE-3.2M and OE-12.8M respectively, as described in (5). Figure 5: Log-linear relationship is observed between vocabulary size and training loss L, i.e. = 2.6754 0.0256 log10 m. The values are collected with 500B tokens training on OLMoE-1.3B models. + m. It is worth noting that the model needs to be sufficiently trained to capture these patterns. Figure 9 shows the training loss and loss diff curves in the vocabulary size scaling experiments. The larger vocabulary size requires longer training until the gains converge. And the performance gap is stably held after sufficient tokens training. Whats Good for OE. We conducted extensive ablation studies on different configurations for the over-encoding. To ensure fairness, all configurations have the same number of embedding parameters. The results are shown in the Table 2. For simplification, we denote C-i as configuration in the table. We first validate that slicing the embedding table along the dimension yields further gains (comparing C-2, C-4, Whats Bad for OE. We also identify certain configurations that result in significant performance degradation and 7 Table 4: Ablation study on hashing conflicts. Note the experiments are kept roughly the same vocabulary size, i.e. 64V 3.218M. The metrics are reported after training with 50B tokens. Table 5: MTP Experiments on OLMoE-1.3B. The loss refers to the next one token prediction loss for MTP methods. Metric difference that improves baseline are marked blue while degrations are marked red."
        },
        {
            "title": "Model",
            "content": "Loss PPL Eval Loss Eval PPL Model Loss Eval Loss Downstream baseline +E64V +E3.2Md 2.714 2.702 2.678 15.094 14.892 14.555 3.094 3.077 3.054 22.060 21.710 21.202 2.554 2.556 +0.002 2.555 +0. baseline +MTP +MTP-DS OE-12.8M 2.472 OT-12.8M 2.481 +0.009 2.924 2.925 +0.001 2.926 +0.002 2.862 2.869 +0.007 0.510 0.508 -0.002 0.511 +0.001 0.524 0.537 +0.013 should avoid in practice. For this part of the experiments, due to the obvious performance drop, we report results after training on only 50B tokens, and simply use loss and perplexity as evaluation metrics. First, we validate the importance of hierarchical multi-gram tokens. As shown in Table 3, removing the 1-gram vocabulary and using only the 2-gram vocabulary leads to significant performance drop. This may be due to conflicts in the lookup results of the 2-gram table when 1-gram information is absent, making it difficult for the model to disambiguate the current semantics. On the other hand, skipping the 2-gram tokens and using only 1-gram and 3-gram tokens still shows gains over the baseline but underperforms the proposed model using 1-, 2-, and 3-gram tokens together. Overall, we hypothesize that the model benefits from hierarchical encoding to handle conflicting entries in the embedding table, thereby accurately resolving semantics. Another supporting observation is that when we deliberately introduce more encoding conflicts (by setting to be an exact multiple of ), the gains of over-encoding are significantly reduced, as shown in Table 4. This observation also suggests that should set to value coprime with to avoid such degeneration. 4.3. Over-Tokenized Transformer In this section, we verify the effectiveness of over-tokenized transformer. We conduct the experiments on OLMoE-1.3B with 500B tokens training. As MTP is less efficient for smaller models, our implementation has only one extra head predicting the next 2 token, and set the weight 0.1 for future token loss. Then, the over-tokenized transformer is constructed, combining OE-12.8M with MTP-DS, which we refer as OT-12.8M. Results are shown in Table 5. Limited by model capacity, MTP shows no improvement, as well as its stronger verison MTP-DS. However, with over-encoding plugged, MTP-DS presents greater power. OT further improves the downstream metric by 1. 3% compared to OE, although it slightly sacrifices the loss gains of OE. The detailed metrics 8 are presented in Appendix B.4. 5. Conclusion In this work, we have explored over-tokenized transformer for language modeling. By systematically analyzing the impact of token granularity and vocabulary size across models of varying scales, we uncover an important scaling phenomena that inspires tokenizer design. Our findings reveal that larger input vocabularies consistently enhance model performance, no matter model scales, while larger output vocabulary can be harmful and difficult to learn for smaller models. These insights have significant implications for the continued scaling of LLMs. With such inspirations, we introduce over-encoding technique that scales up the input vocabulary via multi-gram token embeddings. Moreover, we develop over-tokenized transformer combining over-encoding and multi-token prediction. Extensive experiments validated the effectiveness of our methods, showcasing significant gains in model performance. Notably, we demonstrated strong log-linear relationship between input vocabulary size and training loss, highlighting new dimension in the scaling laws of LLMs. These results emphasize the importance of tokenizer design as key driver of advancements in language modeling. In summary, this work bridges the gap between tokenizer design and model scaling, positioning tokenization as critical factor in the development of next-generation language models. We believe that the proposed Over-Tokenized Transformers framework and the insights derived from our study will inspire further research into tokenization strategies and their role in scaling LLMs efficiently and effectively."
        },
        {
            "title": "References",
            "content": "Allen-Zhu, Z. and Li, Y. Physics of language models: Part 1, learning hierarchical language structures, 2024. URL https://arxiv.org/abs/2305.13673. Bisk, Y., Zellers, R., Gao, J., Choi, Y., et al. Piqa: Reasoning about physical commonsense in natural language. In Proceedings of the AAAI conference on artificial intelligence, volume 34, 2020. Brown, T., Mann, B., Ryder, N., Subbiah, M., Kaplan, J. D., Dhariwal, P., Neelakantan, A., Shyam, P., Sastry, G., Askell, A., Agarwal, S., Herbert-Voss, A., Krueger, G., Henighan, T., Child, R., Ramesh, A., Ziegler, D., Wu, J., Winter, C., Hesse, C., Chen, M., Sigler, E., Litwin, M., Gray, S., Chess, B., Clark, J., Berner, C., McCandlish, S., Radford, A., Sutskever, I., and Amodei, D. Language models are few-shot In Larochelle, H., learners. Ranzato, M., Hadsell, R., Balcan, M., and Lin, H. (eds.), Advances in Neural Information Processing Systems, volume 33, pp. 18771901. Curran Associates, Inc., 2020. URL https://proceedings.neurips. cc/paper_files/paper/2020/file/ 1457c0d6bfcb4967418bfb8ac142f64a-Paper. pdf. Clark, P., Cowhey, I., Etzioni, O., Khot, T., Sabharwal, A., Schoenick, C., and Tafjord, O. Think you have solved question answering? try arc, the ai2 reasoning challenge. arXiv:1803.05457v1, 2018. DeepSeek-AI, Liu, A., Feng, B., Xue, B., Wang, B., Wu, B., Lu, C., Zhao, C., Deng, C., Zhang, C., Ruan, C., Dai, D., Guo, D., Yang, D., Chen, D., Ji, D., Li, E., Lin, F., Dai, F., Luo, F., Hao, G., Chen, G., Li, G., Zhang, H., Bao, H., Xu, H., Wang, H., Zhang, H., Ding, H., Xin, H., Gao, H., Li, H., Qu, H., Cai, J. L., Liang, J., Guo, J., Ni, J., Li, J., Wang, J., Chen, J., Chen, J., Yuan, J., Qiu, J., Li, J., Song, J., Dong, K., Hu, K., Gao, K., Guan, K., Huang, K., Yu, K., Wang, L., Zhang, L., Xu, L., Xia, L., Zhao, L., Wang, L., Zhang, L., Li, M., Wang, M., Zhang, M., Zhang, M., Tang, M., Li, M., Tian, N., Huang, P., Wang, P., Zhang, P., Wang, Q., Zhu, Q., Chen, Q., Du, Q., Chen, R. J., Jin, R. L., Ge, R., Zhang, R., Pan, R., Wang, R., Xu, R., Zhang, R., Chen, R., Li, S. S., Lu, S., Zhou, S., Chen, S., Wu, S., Ye, S., Ye, S., Ma, S., Wang, S., Zhou, S., Yu, S., Zhou, S., Pan, S., Wang, T., Yun, T., Pei, T., Sun, T., Xiao, W. L., Zeng, W., Zhao, W., An, W., Liu, W., Liang, W., Gao, W., Yu, W., Zhang, W., Li, X. Q., Jin, X., Wang, X., Bi, X., Liu, X., Wang, X., Shen, X., Chen, X., Zhang, X., Chen, X., Nie, X., Sun, X., Wang, X., Cheng, X., Liu, X., Xie, X., Liu, X., Yu, X., Song, X., Shan, X., Zhou, X., Yang, X., Li, X., Su, X., Lin, X., Li, Y. K., Wang, Y. Q., Wei, Y. X., Zhu, Y. X., Zhang, Y., Xu, Y., Xu, Y., Huang, Y., Li, Y., Zhao, Y., Sun, Y., Li, Y., Wang, Y., Yu, Y., Zheng, Y., Zhang, Y., Shi, Y., Xiong, Y., He, Y., Tang, Y., Piao, Y., Wang, Y., Tan, Y., Ma, Y., Liu, Y., Guo, Y., Wu, Y., Ou, Y., Zhu, Y., Wang, Y., Gong, Y., Zou, Y., He, Y., Zha, Y., Xiong, Y., Ma, Y., Yan, Y., Luo, Y., You, Y., Liu, Y., Zhou, Y., Wu, Z. F., Ren, Z. Z., Ren, Z., Sha, Z., Fu, Z., Xu, Z., Huang, Z., Zhang, Z., Xie, Z., Zhang, Z., Hao, Z., Gou, Z., Ma, Z., Yan, Z., Shao, Z., Xu, Z., Wu, Z., Zhang, Z., Li, Z., Gu, Z., Zhu, Z., Liu, Z., Li, Z., Xie, Z., Song, Z., Gao, Z., and Pan, Z. Deepseek-v3 technical report, 2024. URL https://arxiv.org/abs/2412.19437. Fang, J., Zhang, G., Han, J., Li, S., Bian, Z., Li, Y., Liu, J., and You, Y. frequency-aware software cache for large recommendation system embeddings. arXiv preprint arXiv:2208.05321, 2022. Gloeckle, F., Idrissi, B. Y., Roziere, B., Lopez-Paz, D., and Synnaeve, G. Better & faster large language models via multi-token prediction. In Forty-first International Conference on Machine Learning, 2024. URL https: //openreview.net/forum?id=pEWAcejiU2. Hendrycks, D., Burns, C., Basart, S., Zou, A., Mazeika, M., Song, D., and Steinhardt, J. Measuring massive multitask language understanding. Proceedings of the International Conference on Learning Representations (ICLR), 2021. Kaplan, J., McCandlish, S., Henighan, T., Brown, T. B., Chess, B., Child, R., Gray, S., Radford, A., Wu, J., and Amodei, D. Scaling laws for neural language models. arXiv preprint arXiv:2001.08361, 2020. Kudo, T. Subword regularization: Improving neural network translation models with multiple subword candidates. arXiv preprint arXiv:1804.10959, 2018. Li, S., Liu, H., Bian, Z., Fang, J., Huang, H., Liu, Y., Wang, B., and You, Y. Colossal-ai: unified deep learning system for large-scale parallel training. In Proceedings of the 52nd International Conference on Parallel Processing, ICPP 23, pp. 766775, New York, NY, USA, 2023. Association for Computing Machinery. ISBN 9798400708435. doi: 10.1145/3605573.3605613. URL https://doi. org/10.1145/3605573.3605613. Muennighoff, N., Soldaini, L., Groeneveld, D., Lo, K., Morrison, J., Min, S., Shi, W., Walsh, P., Tafjord, O., Lambert, N., Gu, Y., Arora, S., Bhagia, A., Schwenk, D., Wadden, D., Wettig, A., Hui, B., Dettmers, T., Kiela, D., Farhadi, A., Smith, N. A., Koh, P. W., Singh, A., and Hajishirzi, H. Olmoe: Open mixture-of-experts language models, 2024. URL https://arxiv.org/abs/2409.02060. OLMo, T., Walsh, P., Soldaini, L., Groeneveld, D., Lo, K., Arora, S., Bhagia, A., Gu, Y., Huang, S., Jordan, M., Lambert, N., Schwenk, D., Tafjord, O., Anderson, T., Atkinson, D., Brahman, F., Clark, C., Dasigi, P., Dziri, N., Guerquin, M., Ivison, H., Koh, P. W., Liu, J., Malik, S., Merrill, W., Miranda, L. J. V., Morrison, J., Murray, T., Nam, C., Pyatkin, V., Rangapur, A., Schmitz, M., Skjonsberg, S., Wadden, D., Wilhelm, C., Wilson, M., Zettlemoyer, L., Farhadi, A., Smith, N. A., and Hajishirzi, H. 2 olmo 2 furious, 2024. URL https://arxiv. org/abs/2501.00656. 9 Pytorch fsdp: Experiences on scaling fully sharded data parallel. Proceedings of the VLDB Endowment, 16(12): 38483860, 2023. Pagnoni, A., Pasunuru, R., Rodriguez, P., Nguyen, J., Muller, B., Li, M., Zhou, C., Yu, L., Weston, J., Zettlemoyer, L., Ghosh, G., Lewis, M., Holtzman, A., and Iyer, S. Byte latent transformer: Patches scale better than tokens, 2024. URL https://arxiv.org/ abs/2412.09871. Radford, A., Wu, J., Child, R., Luan, D., Amodei, D., and Sutskever, I. Language models are unsupervised multitask learners. 2019. Sennrich, R. Neural machine translation of rare words with subword units. arXiv preprint arXiv:1508.07909, 2015. Shazeer, N., Mirhoseini, A., Maziarz, K., Davis, A., Le, Q., Hinton, G., and Dean, J. Outrageously large neural networks: The sparsely-gated mixture-of-experts layer. In International Conference on Learning Representations, 2017. URL https://openreview.net/forum? id=B1ckMDqlg. Tao, C., Liu, Q., Dou, L., Muennighoff, N., Wan, Z., Luo, P., Lin, M., and Wong, N. Scaling laws with vocabulary: Larger models deserve larger vocabularies. arXiv preprint arXiv:2407.13623, 2024. Vaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A. N., Kaiser, L. u., and Polosukhin, I. Attention is all you need. In Guyon, I., Luxburg, U. V., Bengio, S., Wallach, H., Fergus, R., Vishwanathan, S., and Garnett, R. (eds.), Advances in Neural Information Processing Systems, volume 30. Curran Associates, Inc., 2017. URL https://proceedings.neurips. cc/paper_files/paper/2017/file/ 3f5ee243547dee91fbd053c1c4a845aa-Paper. pdf. Xue, L., Barua, A., Constant, N., Al-Rfou, R., Narang, S., Kale, M., Roberts, A., and Raffel, C. ByT5: Towards token-free future with pre-trained byte-to-byte Transactions of the Association for Commodels. putational Linguistics, 10:291306, 2022. doi: 10. 1162/tacl 00461. URL https://aclanthology. org/2022.tacl-1.17. Yu, L., Simig, D., Flaherty, C., Aghajanyan, A., Zettlemoyer, L., and Lewis, M. Megabyte: Predicting million-byte sequences with multiscale transformers. Advances in Neural Information Processing Systems, 36:7880878823, 2023. Zellers, R., Holtzman, A., Bisk, Y., Farhadi, A., and Choi, Y. Hellaswag: Can machine really finish your sentence? arXiv preprint arXiv:1905.07830, 2019. Zhao, Y., Gu, A., Varma, R., Luo, L., Huang, C.-C., Xu, M., Wright, L., Shojanazeri, H., Ott, M., Shleifer, S., et al. 10 A. Pytorch Implementation We provide pytorch-like pseudocode for Over-Encoding in Algorithm 1. Algorithm 1 Over-Encoding in PyTorch-like style. #OE parameters: # m: OE vocabulary size # k: split num # n: the number of neighboring tokens involved #Model parameters: # V: base vocabulary size # D: model dimension #Torch Modules wte = nn.Embedding(V, d) oe_embedders = nn.ModuleList([nn.Embedding(m+i*2, D//(k*(n-1)) for in range(k*(n-1)]) oe_projs = nn.ModuleList([nn.Linear(D//(k*(n-1)), D) for _ in range(k*(n-1)]) def forward(self, input_ids): # input_ids: [bs, seqlens] = self.wte(input_ids) n_gram_ids = input_ids.clone() for in range(2, n+1): n_gram_ids += F.pad(input_ids, i-1) * ** (i-1) for in range(k): index = (i-2)*k+j x_oe = oe_embedders[index](n_gram_ids % (m + 2 * index)) += oe_projs[index](x_oe) /= 1 + * (n-1) return B. More Experimental Details B.1. Downstream Benchmarks We give detailed description for the major benchmarks used in our experiments. piqa (Bisk et al., 2020): benchmark designed to evaluate models on commonsense physical reasoning. It tests the ability of models to reason about everyday physical interactions and affordances, requiring both textual understanding and physical intuition to solve tasks effectively. hellaswag (Zellers et al., 2019): dataset that focuses on testing commonsense reasoning and the ability to predict the most plausible continuation of given scenario. It is considered challenging benchmark due to adversarially filtered incorrect options aimed at confusing models. arc easy (Clark et al., 2018): subset of the AI2 Reasoning Challenge (ARC) benchmark that contains grade-school-level multiple-choice questions. The questions in this subset are relatively straightforward and assess basic scientific knowledge and reasoning. arc challenge (Clark et al., 2018): the more difficult subset of the ARC benchmark, comprising scientifically challenging multiple-choice questions that require advanced reasoning, background knowledge, and problem-solving skills beyond basic memorization. mmlu (Hendrycks et al., 2021) (Massive Multitask Language Understanding): large-scale benchmark designed to test models across wide range of academic and professional fields. It includes multiple-choice questions from diverse domains, such as humanities, STEM, and social sciences, aiming to measure comprehensive language understanding and reasoning capabilities. B.2. CFG Experiments Detailed Setup. We obtain the grammar following (Allen-Zhu & Li, 2024) with the configuration named cfg3f, as illustrated in Figure 6. 20 million sentences are sampled from the grammar, serving as fixed training dataset. We use standard GPT2 architecture, where the larger model uses hidden size of 768 and the smaller model 128. Both larger and smaller models have 12 transformer layers, as depth is discovered essential to complex CFG modeling (Allen-Zhu & Li, 2024). We use AdamW optimizer with β = (0.9, 0.98), weight decay 0.1, initial learning rate 3e4 and batch size 64 8. The models are trained with cosine learning rate scheduler for 10 epoch. To evaluate model performance, we 10,000 sample Figure 6: Left Panel: CFG rules used in our experiments; Right Panel: an example of the generated sequences using the rules. This figure is taken from (Allen-Zhu & Li, 2024). (a) (b) (c) Figure 7: Performance comparison for models trained on CFG data. sentences from the trained model auto-regressively using the raw next token probability. The sentences are then verified by the ground-truth grammar, and the ratio of accurate samples is denoted as generation accuracy. Implementation of n-gram tokenization or over-tokenization. Though we mentioned 3n-sized vocabulary for n-gram tokenization or n-gram over-tokenization in the main text, the actual vocabulary should be 2 + (cid:80)n i=1 3i. The bias 2 stands for the BOS(Begin Of Sequence) and EOS(End of Sequence) token and extra 3i terms are for the corner cases where the sequence length may not be divisible by n. We refer to 3n-sized vocabulary in the main text primarily to emphasize the order of magnitude. Hierarchical n-gram input further improves. We experiment with introducing hierarchical n-gram inputs in the CFG setting. Initially, we use single n-size n-gram embedding table to handle the models input. To consider hierarchical n-gram modeling, we add 1 additional embedding tables, where the i-th table has size of i, and use i-gram token as input. Notably, the parameters of these 1 embedding tables can be fully merged into the n-gram embedding table, so the effective parameter count does not increase. Instead, this process re-parameterizes the embedding table to decompose parameters hierarchically. The experimental results, shown in Figure 7(a), indicate that the model with hierarchically decomposed 3-gram inputs outperforms the model with naive 3-gram inputs. This further validates that even for models with complete n-gram vocabulary (rather than approximated through matrix decomposition), hierarchically modeling multi-granularity tokens remains highly effective. Ablation on base tokenizers. We replaced the base tokenizer to investigate whether further increasing the input vocabulary size could lead to additional improvements. Specifically, we expanded the input vocabulary for 2-gram and 3-gram base tokenizers to 2-gram inputs with vocabulary sizes of (32)2 = 81 and (33)2 = 729, respectively. The experimental results, shown in Figure 7(b) and Figure 7(c), demonstrate that increasing the input vocabulary consistently improves model performance. Another observation is that, when using the 3-gram base tokenizer, the benefit of increased convergence speed from 2-gram inputs becomes notably smaller. We hypothesize that this is related to the slow-start nature of large vocabularies. Similar observations have been made in natural language processing, where larger input vocabularies require more training tokens to realize their expected benefits. 12 B.3. OLMo2 Experiments Detailed metrics. We present detailed training dynamics comparison for models on OLMo2-1B in Figure 11. OE achieves significant improvements across most metrics, while maintaining at least parity on the remaining ones. B.4. OLMoE Experiments Loss curves for vocabulary scaling. We present the loss curves and loss difference curves in Figure 9. The larger input vocabulary requires longer training to obtain all its gains. Loss curves for OLMoE-7B models. We present the loss curves for OLMoE-7B in Figure 10. The gains of OE have not fully converged in 500B tokens training. Comparing to the experiments on OLMoE-1.3B, it is likely that larger models require more training to fully leverage the power of over-encoding. Detailed metrics for OE. We present detailed training dynamics comparison for OE-12.8M and baseline model in Figure 11(OLMoE-1.3B) and Figure 12(OLMoE-7B). Our model has significant improvements on most evaluation metrics, and has at least comparable performance for the rest. Detailed metrics for OT. We present detailed metrics comparing OT-12.8M and OE-12.8M on OLMoE-1.3B in Figure ??. OT is not always better than OE, e.g., Winogrande and Social IQA. C. Over-Decoding Method. From previous experiments on synthetic data, we know that an excessively large output vocabulary is detrimental to small models. Therefore, when designing an over-decoding method based on natural language, we aim to leverage the advantages of n-gram prediction while keeping the decoding vocabulary size as small as possible. We found that product decomposition provides promising solution to this problem. For the n-gram decoding vocabulary, the product embedding decomposition parameterizes n-sized embedding table using separate -sized embedding tables, formulated as: ex = (cid:88) j=1 EV j (x/V j1) = (cid:88) j=1 Ej(zj), (6) where ex is the reparameterized 2-gram embedding, and z1, . . . , zn correspond to the reverse mapping of the function given x, as described in 2. Then, during training, we compute the next n-gram token prediction loss on the reparameterized embeddings {ei}: = CE(ˆh E, x(+n)) = CE(ˆhE , zj), (7) (cid:88) j= where ˆh represents the output hidden state, CE(, ) denotes the cross-entropy loss, and the loss decomposition is detailed later. notable challenge is that the decoding embedding is densely activated, making the unembedding layer computationally expensive, particularly for smaller models. To address this, we optionally apply low-rank decomposition by projecting the last hidden state to smaller dimension. Additionally, we reweigh these losses using set of hyperparameters λ1, . . . , λn, resulting in the final training loss: = (cid:88) j=1 λjCE(ˆhWjE , zj), (8) where Wj is the optional projection matrix corresponding to the j-th embedding table. We typically set λ1 = 1 and > 1, λi 1. pytorch-like implementation is provided in Algorithm 2. Derivation of Loss Decomposition We provide detailed derivation of the loss decomposition in (7). Recall that {ei} are the n-gram token embedding table. Each of them is reparameterized by ei = (cid:80)n zjV j. We denote j=1 Ej(zj) with = (cid:80) Figure 8: All metrics for OLMo2-1B, comparing OE-12.8M and baseline. 14 (a) Loss Curves (b) Loss Diff Figure 9: Loss curves for vocabulary scaling experiments, where OE models use the setting of = 2, = 1. The curves are smoothed with exponential moving average of weight 0.99. (a) Loss Curves (b) Loss Diff Figure 10: Loss curves for OLMoE-7B experiments. The OE model uses the setting of = 3, = 4. The curves are smoothed with exponential moving average of weight 0.99. The flat part in baseline is consequence of wandb log missing. 15 Figure 11: All metrics for OLMoE-1.3B, comparing OE-12.8M and baseline. 16 Figure 12: All metrics for OLMoE-7B, comparing OE-12.8M and baseline. 17 Figure 13: All metrics for OLMoE-1.3B, comparing OT-12.8 and OE-12.8M. 18 zj = Ej(zj), then ei = (cid:80)n e(j) j=1 e(j) zj . Cross-entropy loss using n-gram token prediction task on {ei} can be written as: L(ˆh, x(+n); E) = log exp(ˆhex(+n)) (cid:80) exp(ˆhei) = log = log (cid:81) exp(ˆhe(j) xj ) exp(ˆhe(j) zj ) (cid:81) z1,...,zn (cid:80) = log (cid:89) exp(ˆhe(j) xj ) exp(ˆhe(j) zj ) (cid:80) (cid:88) = log (cid:80) z1,...,zn = log exp(ˆh((cid:80) xj )) e(j) exp(ˆh((cid:80) (cid:81) (cid:16)(cid:80) e(j) zj )) exp(ˆhe(j) xj ) exp(ˆhe(j) zj ) (cid:17) zj (cid:81) exp(ˆhe(j) xj ) exp(ˆhe(j) zj ) (cid:80) (cid:88) = L(ˆh, xj; Ej), (9) zj where xj is the next-j token, and x(+n) = (cid:80) prediction can be decomposed to multiple next-j token prediction loss, which can be calculated efficiently. xjV j1. Hence, with product embedding decomposition, the n-gram token zj j Discussion. Based on product embedding decomposition, we propose training objective similar to MTP (Gloeckle et al., 2024). Formally, we remove the additional transformer layer introduced by the multiple prediction heads in MTP. Instead, we directly decode the next 1 to tokens from the last hidden state using different vocabularies. From the perspective of MTP, the nonlinear transformation introduced by directly using new vocabularies in the prediction heads is not weaker than adding transformer layer. Moreover, since the unembedding process is computational bottleneck, using new set of parameters does not reduce training throughput compared to MTP. From the perspective of Over-decoding, we retain the potential to scale up the output vocabularies. This means that the training process provides finer-grained supervision signals, which may enable the model to learn better representations. However, this approach is likely only practical for sufficiently large models. On one hand, larger models have smaller proportion of computation spent on unembedding. On the other hand, larger models offer greater capacity to take advantage of this setup. Algorithm 2 Over-Decoding in PyTorch-like style. #OD parameters: # n: the number of next tokens involved # lambdas: list of n-1 elements, indicating loss weights for next i+1 token pred # d: dimension for low-rank decomposition #Model parameters: # V: base vocabulary size # D: model dimension #Torch Modules wte = nn.Embedding(V, d) # default next 1 token embedding table od_embs = nn.ModuleList([ nn.Sequential( nn.Linear(D, d) if != else nn.Identity(), nn.Linear(d, V) ) for in range(n-1)]) ce_loss = nn.CrossEntropyLoss() def forward(self, input_ids, hidden_states): # input_ids: [bs, seqlens] # hidden_states: [bs, seqlens, D] loss = ce_loss(F.linear(hidden_states[:,:-1], wte.weight.T), input_ids[:,1:]) for in range(2, n+1): loss += lambdas[i-2] * ce_loss(od_embs[i-2](hidden_states[:-i]), input_ids[:,i:]) return loss C.1. Experiments We conduct experiments for Over-Decoding under OLMoE settings, where both OLMoE-1.3B and OLMoE-7B are considered. We train 500B tokens for all the experiments. OD improves baseline with sufficient training. We first observed that over-decoding significantly lags behind the baseline in the early stages of training. However, after training on more than 200B tokens, over-decoding begins to outperform the baseline. This experiment is conducted on both 1.3B and 7B OLMoE models, where we set = 2. As shown in Figure 14, 19 (a) OLMoE-1.3B (b) OLMoE-7B Figure 14: Loss diff curves comparing over-decoded models and baselines. OD models use the setting of = 2 and λ2 [0.1, 0.2, 0.5, 1.0]. The curves are smoothed with exponential moving average of weight 0.99. Table 6: Ablation study on loss weights for OD. The column downstream represents the average score of MMLU-Var, Hellaswag, ARC-Challenge, ARC-Easy and PIQA. Model Loss Eval Loss MMLU-Var Hellaswag Arc-Challenge Arc-Easy PIQA OLMoE-1.3B +OD λ2 = 0.1 +OD λ2 = 0.2 +OD λ2 = 0.5 +OD λ2 = 1.0 OLMoE-7B +OD λ2 = 0.1 2.554 2.549 2.549 2.549 2.555 2.306 2. 2.924 2.920 2.918 2.918 2.923 2.670 2.672 0.327 0.325 0.327 0.325 0.328 0.385 0.387 0.553 0.553 0.551 0.553 0.550 0.695 0. 0.325 0.331 0.323 0.308 0.320 0.414 0.409 0.622 0.610 0.633 0.619 0.629 0.740 0.724 0.727 0.721 0.728 0.727 0.722 0.775 0. the next-one-token loss for over-decoding surpasses the baseline with steeper slope. Downstream metrics, as illustrated in Table 6, also show slight improvements with proper loss weight choosen. Typically, we find training loss converges to similar values regardless of loss weights. But from the downstream tasks, smaller weight leads to better overall performance."
        }
    ],
    "affiliations": [
        "Seed-Foundation-Model Team, Bytedance"
    ]
}