{
    "paper_title": "DyMU: Dynamic Merging and Virtual Unmerging for Efficient VLMs",
    "authors": [
        "Zhenhailong Wang",
        "Senthil Purushwalkam",
        "Caiming Xiong",
        "Silvio Savarese",
        "Heng Ji",
        "Ran Xu"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "We present DyMU, an efficient, training-free framework that dynamically reduces the computational burden of vision-language models (VLMs) while maintaining high task performance. Our approach comprises two key components. First, Dynamic Token Merging (DToMe) reduces the number of visual token embeddings by merging similar tokens based on image complexity, addressing the inherent inefficiency of fixed-length outputs in vision transformers. Second, Virtual Token Unmerging (VTU) simulates the expected token sequence for large language models (LLMs) by efficiently reconstructing the attention dynamics of a full sequence, thus preserving the downstream performance without additional fine-tuning. Unlike previous approaches, our method dynamically adapts token compression to the content of the image and operates completely training-free, making it readily applicable to most state-of-the-art VLM architectures. Extensive experiments on image and video understanding tasks demonstrate that DyMU can reduce the average visual token count by 32%-85% while achieving comparable performance to full-length models across diverse VLM architectures, including the recently popularized AnyRes-based visual encoders. Furthermore, through qualitative analyses, we demonstrate that DToMe effectively adapts token reduction based on image complexity and, unlike existing systems, provides users more control over computational costs. Project page: https://mikewangwzhl.github.io/dymu/."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 3 2 ] . [ 1 0 4 0 7 1 . 4 0 5 2 : r DYMU: Dynamic Merging and Virtual Unmerging for Efficient VLMs Zhenhailong Wang1*, Senthil Purushwalkam2*, Caiming Xiong2, Silvio Savarese2, Heng Ji1, Ran Xu2 1University of Illinois Urbana-Champaign, 2Salesforce Research *Equal Contribution Figure 1. Dynamic Merging and Virtual Unmerging (DyMU) adaptively reduces visual token lengths based on image complexity, as shown on the left where simpler images are represented using fewer tokens. In contrast, existing representations (like CLIP) always use the same number of tokens regardless of image content. DyMU applied to recent VLMs (right) maintains competitive performance across different token compression levels. This training-free approach preserves key semantic information, offering more efficient plug-and-play alternative to VLMs with fixed-length visual tokens."
        },
        {
            "title": "Abstract",
            "content": "We present DYMU, an efficient, training-free framework that dynamically reduces the computational burden of vision-language models (VLMs) while maintaining high task performance. Our approach comprises two key components. First, Dynamic Token Merging (DToMe) reduces the number of visual token embeddings by merging similar tokens based on image complexity, addressing the inherent inefficiency of fixed-length outputs in vision transformers. Second, Virtual Token Unmerging (VTU) simulates the expected token sequence for large language models (LLMs) by efficiently reconstructing the attention dynamics of full sequence, thus preserving the downstream performance without additional fine-tuning. Unlike previous approaches, our method dynamically adapts token compression to the content of the image and operates completely trainingfree, making it readily applicable to most state-of-the-art VLM architectures. Extensive experiments on image and video understanding tasks, demonstrate that DYMU can reduce the average visual token count by 32%-85% while achieving comparable performance to full-length models, across diverse VLM architectures, including the recently popularized AnyRes-based visual encoders. Furthermore, through qualitative analyses we demonstrate that DToMe effectively adapts token reduction based on image complexity, and unlike existing systems, provides users more control over computational costs. Project page: https: //mikewangwzhl.github.io/dymu. 1. Introduction Recent large vision-language models (VLMs) have demonstrated breakthroughs in computer vision tasks such as image captioning [6], open-vocabulary object detection [13], visual-question answering [12] and OCR [31] by leveraging the reasoning capabilities of large language models (LLMs) to enhance visual understanding. Most state-ofthe-art Vision-Language Models (VLMs) follow common approach: visual encoder extracts features from images or videos and projects them into the same embedding space as textual features. These visual embeddings are then processed by LLMs alongside textual query features, enabling complex understanding and reasoning tasks while directly benefiting from advancements in LLM capabilities. As expected, the quality of the final predictions from the LLM relies heavily on the richness of the visual features and the amount of semantic detail captured by the encoder. Consequently, research has focused on improving visual encoders to extract increasingly fine-grained features, leading to architectures that can capture intricate details. However, this level of detail comes at cost the computational burden during training and inference. To process high-resolution images while preserving finegrained details, modern visual encoders generate large number of tokenized representations. Furthermore, stateof-the-art VLMs like LLaVA-OneVision [19] and Qwen2.5VL [2] use vision transformers (ViTs) that scale the number of tokens with the resolution of the image or number of frames in videos. For example, the visual encoder in LLaVA-OneVision would produce 9477 tokens for an image of 1280960 resolution. In contrast, the number of tokens in the textual queries for vision tasks is relatively low. On common benchmarks that represent real world use cases, textual queries often consist of just few tokens, e.g., 24 on MME [11]. This stark contrast highlights that the computational burden of processing vision tasks generally arises primarily from the large number of visual tokens. We first make an interesting observation: in current visual encoders, the number of tokens generated for an image does not depend on the content of the image. In Figure 1, we illustrate this with some examples CLIP [33] representation leads to the same embedding size on blank image with small circle and on complex scene depicting buildings, vehicles and people. In contrast, textual tokens are more closely tied to the amount of content conveyed more words are required to describe more information. An average sentence length in English is around 1520 words [10], meaning that regardless of the content of the image, the language model in LLaVA-OneVision [19] has to process the equivalent of 400-500 sentences for each high-resolution image. In this work, we propose Dynamic Merging and Virtual Unmerging (DYMU), which comprises two key methods for modifying existing pre-trained Vision-Language Models (VLMs). First, we introduce Dynamic Token Merging (Sec 3.1), which allows the visual encoder to generate variable-length token sequences based on the complexity of the image. Second, we present Virtual Token Unmerging (Sec 3.2), enabling the LLM decoder to process shorter dynamic visual token sequences while efficiently approximating the full-length sequence. Crucially, we demonstrate that both of these modifications do not require additional fine-tuning of the pre-trained VLM. Furthermore, Dynamic Token Merging is compatible with any Vision Transformer (ViT)-based visual encoder, and Virtual Token Unmerging can be applied to any LLM that utilizes Rotary Position Embedding (RoPE) [36]."
        },
        {
            "title": "We show that VLMs modified with our methods can",
            "content": "maintain the performance of the full model while reducing the average token count by 32%-85% (see Sec 4.2). In addition to improving efficiency, our approach offers users greater control over token costs compared to existing systems (e.g., GPT-4o), which incur fixed token count per image based solely on resolution. In Sec 4.3, we demonstrate example applications on how the number of visual tokens can be further reduced by combining DYMU with pre-processing tools such as background removal, object detection, etc. Through comprehensive quantitative experiments (Sec 4.2), we verify that our method works effectively across different VLM architectures, with varying pretraining strategies, visual encoders, and training datasets. 2. Related Work Component Improved Dynamic Length No Addn. Modules Training Free Granularity Control Extra Cond. LLaMA-VID [22] Fast-V [22] SparseVLM [22] MQT-LLaVA [14] LLaVA-Prumerge [34] TokenPacker [34] ATP-LLaVA [34] LLaVA-mini [45] DYMU Projector Decoder Decoder Projector Projector Projector Decoder Projector Encoder & Decoder None None Text None None None Text None None Efficient Vision-Language Models Recent efforts in large vision-language models (VLMs) have primarily focused on reducing computational overhead during the prefilling and VLM decoding phases. That is, given full sequence of visual tokens from visual encoder, such as CLIP, these approaches perform token pruning and merging [5, 15, 23, 34, 38, 40, 46], distillation [41], or resampling [14, 20, 22, 45] to improve efficiency in either the projectors or the VLM decoder blocks. However, we identify several key limitations: (1) Most existing methods, including all training-free approaches [5, 38, 46] predefine fixed compression ratio for any input image regardless of its complexity. While [40] proposed an adaptive token pruning framework that enables variable-length compression, it requires retraining the backbone VLM with additional modules. Such training can be costly or infeasible as mainstream VLMs rarely open-source their full training recipe and data. (2) All existing methods retain frozen, fixedlength visual encoder, overlooking the potential for further efficiency improvements within the visual encoder itself. In this work, we aim to explore simple training-free algorithm for variable length visual token compression, which can be directly applied to cutting-edge VLM architectures including Any-Resolution models and RoPE embeddings. Efficient Vision Transformers We also draw inspiration from separate line of research [3, 26, 29, 37, 42] aimed at improving the efficiency of Vision Transformers (ViTs) themselves, which is still the main go-to architecture for visual encoders [32, 33, 44]. In particular, ToMe [3] merges predefined number of tokens within each ViT block using bipartite soft matching. However, the effectiveness of such methods in coordination with VLM backbones remains largely unexplored. Our experiments in 4 show that naively applying ToMe to visual encoders in pretrained VLMs results in significant drop in performance. To address this issue, we further propose an efficient virtual unmerging algorithm to boost the performance of VLMs without training with the modified encoders that output reduced token numbers. 3. Method In this section, we present the main technical details of proposed method. In Section 3.1, we present our proposed Dynamic Token Merging (DToMe) training-free method to dynamically reduce the number of output tokens by visual encoders based on the complexity of the image content. In Section 3.2, we introduce Virtual Token Unmerging (VTU) an approach to process the reduced visual tokens through the language model while efficiently simulating the standard number of visual tokens. This method utilizes the tracked positions of the redundant tokens to recreate full attention matrix of the original length. The combination of both methods is referred to as DYMU, short for Dynamic Merging and Virtual Unmerging. We illustrate the core idea in Figure 2. DYMU can be applied to any VLM that uses transformer-based visual encoders and RoPE-based transformer language models. The proposed modifications to the architecture do not introduce any additional learnable parameters and most importantly, do not require any additional fine-tuning of the VLM. 3.1. Dynamic Token Merging (DToMe) Most recent large vision-language models (VLMs) use vision transformers (ViTs) like CLIP [33] or SigLIP [44] to encode images into sequence of visual tokens. For fixed resolution input image, the ViT architecture always outputs the same number of token embeddings, leading to low efficiency in VLMs. Our approach draws inspiration from ToMe[3], prior work which reduces the number of output tokens to predefined fixed number. However, predefining the reduction ratio can still lead to misalignment between the information of an image and the number of tokens needed for representing it. Here we propose Dynamic Token Merging (DToMe), an extension of ToMe that adaptively merges similar tokens in ViT layers, ensuring the output token count aligns with image complexity. DToMe merges tokens based on similarity threshold while maintaining record of merged tokens to ensure their influence is properly propagated through subsequent transformer layers. To find the thresholds, we propose inference-only batch-level bipartite merging algorithm which leverages the natural variance of image complexity in randomly sampled images. Identifying Redundant Tokens Let us represent the output of the self-attention layer in the ViT layer as xi RNiD, where Ni is the sequence length* and is the embedding dimension. Similarly, let the keys computed in the self-attention layer be represented by ki RNiDk . In each transformer block, we apply an additional DToMe operator to xi. Drawing inspiration from [3], we follow bipartite soft matching strategy to identify which tokens need to be merged. First, we divide the Ni tokens into two sets (say and B) by assigning alternating tokens in sequence them. We then compute bipartite assignment between the two sets of tokens by assigning token to tB = (ki[t]T ki[n]) (token with the most similar key). arg max nB This gives us edges with scores Si[t] = (ki[t]T ki[tB]) for A. We then apply threshold τi to retain edges tB where Si[t] > τi. Unlike [3], this thresholding operation leads to variable number of retained edges depending on the amount of redundancy demonstrated in the key embeddings ki. We describe our approach for computing the thresholds below. Tracking and Merging Tokens For each token in the sequence, xi[t], we also track the set of positions of the tokens that have already been merged into it. Pi[t] {1, 2, . . . , N1}. For each of the edges between chosen redundant tokens tB, we compute merged token embeddings and the corresponding position sets as: xi[tB] xi[t] Pi[t] + xi[tB] Pi[tB] Pi[t] + Pi[tB] Pi[tB] Pi[tB] Pi[t] Pi[t] (1) (2) (3) Intuitively, the representation of token tB is updated to the average of xi[tB] and xi[t], weighted by their corresponding merged position set sizes, Pi[tB] and Pi[t]. The token is then dropped since it has been merged with tB, thereby reducing the token count in the next layer. Finding Redundancy Thresholds The layer-wise thresholds τi play crucial role in determining how many tokens are merged. In order to determine the thresholds, we rely of statistics from large dataset of images. First, we choose hyper-parameter ri for each layer which represents the number of edges we expect to merge in layer on average across images of all complexities. The final output would then be expected to have an average of (cid:80) ri tokens. Using dataset of images, we collect large batches of size which are used to perform forward computation through the layers of the ViT sequentially. *For standard ViT without any merging, Ni is constant across layers Figure 2. Method Overview. DYMU, is composed of two key ideas: Dynamic Token Merging (DToMe) and Virtual Token Unmerging (VTU). DToMe first determines per-layer thresholds (left) by feeding large batch of images into the vision transformer and computing bipartite token similarities. We rank these edges across the entire batch and choose the top-Br (r = desired average number of tokens, batch size B). This leads to more edges from simpler images (with more redundancy) being chosen, while complex images remain less merged. During inference, DToMe merges tokens on per-image basis using these pre-computed thresholds. We then apply VTU (right) in the self-attention layers of the pretrained VLM to efficiently expand the attention matrices to the standard token countensuring the models original weights and outputs remain compatiblebefore re-merging the tokens for the next layer. The overall process is trainingfree and utilizes crucial image information by allocating the token budget more effectively for both simple and complex images. For each layer, we compute the bipartite matching token edge score maps S(b)[t] where {1, 2, . . . , B} as previously described. We then find the threshold τi as: τi = max (cid:88) τ (cid:88) (cid:16) S(b)[t] > τ (cid:17) = ri b=1 tA(b) (4) In words, this finds the largest threshold such that ri tokens are merged across the batch of images. It is important to note that the number of tokens merged in each image will not necessarily be equal to ri but the average number of tokens merged per image will be ri. Intuitively, since the ranking of edges is over the entire batch, simpler images that have more redundant tokens will be merged more. This process is done sequentially for each layer while only passing the remaining tokens to the next layer to obtain thresholds for every layer. We then average the layer-wise thresholds across several batches to ensure that they reflect the statistics across diverse set of images. See Figure 2 (left) for an illustration of the proposed batch-level threshold finding. Size Weighted Self-attention To ensure that the selfattention layers weigh each token based on the number of tokens that were previously merged into it, we adopt the idea of size-weighted self-attention from ToME[3] where the attention is computed as: 3.2. Virtual Token Unmerging (VTU) The language model (LLM) in pre-trained VLM is trained to operate on fixed number of embeddings for each image. When Dynamic Token Merging is applied to visual encoder, this disrupts the optimized VLM and leads to significant drop in performance (see Sec 4). In this section, we present an approach to circumvent this issue while still benefiting from processing fewer number of visual embeddings. Our proposed approach, Virtual Token Unmerging (VTU), can be easily applied to any mainstream LLM that uses RoPE [36]-based transformer architecture. Consider the general case of sequence of embeddings RN of which only Nun << rows are unique. Let eun RNunD be the unique embeddings and {0, 1}N Nun be mapping such that = eun. Here is sparse matrix with one-hot rows. We now ask the question for various operators in an LLM, can we approximate (e) using some efficient function of eun and ? Sequence-independent Operators For any operator that processes each sequence location independently, we can express (e) as (e) = (eun) by definition. This means that we only need to apply to the unique embeddings eun, significantly reducing computational cost while preserving the original outputs. Many key components of modern LLMs fall into this category, including Linear layers, Activation functions (ReLU, GeLU, etc.), and = Softmax QKT + log Pi[1] ... Pi[Ni] (5) AnyRes[19] leads to multiple fixed length embeddings Note that due to the sparsity of , the time complexity of multiplying D, DM, D, DM are all O(N K) if is dense matrix with dimensions or . Layer Normalization (along the embedding dimension D). The overall complexity of the MLP layers is reduced from O(N D2) to O(NunD2), resulting in linear speedup with Nun << . Virtual Unmerging for Self-Attention with RoPE common layer in recent LLMs is the Self-Attention operation with Rotary Position Embedding (RoPE). Unlike sequence-independent operators, self-attention considers pairwise interactions between embeddings and assigns unique position to each of the locations in e. Consequently, directly applying (eun) fails to capture the structure of e, generally leading to significant discrepancies in the output. To address this, we provide theoretical derivation of an efficient method to compute (e) while preserving the benefits of token reduction. The key insight is to reconstruct the self-attention matrix without explicitly expanding the token sequence. We leverage the linearity of the RoPE transformation to efficiently simulate the appropriate repetitions and the positions of the unique embeddings, significantly reducing computational overhead while maintaining consistency with the full sequence computation. Let = Wqe, = Wke and = Wve be the full query, key and value matrices. Similarly, Qun, Kun and Vun are the unique queries, keys and values satisfying the mapping defined above. The RoPE Self-Attention similarity matrix is computed as = RoPE(Q)RoPE(K)T . For simplicity, let us consider the case where = 2, so that we can write = [Q1, Q2] where Q1, Q2 RN . We will follow similar notation for all queries, keys and values. This allows us to express each query and key as complex number i.e. Q[n] = Q1[n] + iQ2[n]. Let θ [0, 2π)N be the rotation angle associated with each position for RoPE. For positions n, 1, 2, . . . , the RoPE-based similarity [36] is defined as: A[m, n] = Re(cid:0) eiθ[m]Q[m] eiθ[n]K[n] (cid:1) = Re(cid:0) Q[m]K[n] ei(θ[m]θ[n]) (cid:1) (6) (7) where x, Re(x) denote the complex conjugate and the real part of respectively. This can be expanded as: A[m, n] = (Q1[m]K1[n] + Q2[m]K2[n]) cos(θ[m] θ[n]) + (Q1[m]K2[n] Q2[m]K1[n]) sin(θ[m] θ[n]) (8) We also have the trigonometric identities: cos(θ[m]θ[n]) = cos(θ[m]) cos(θ[n]) + sin(θ[m]) sin(θ[n]) sin(θ[m]θ[n]) = sin(θ[m]) cos(θ[n]) cos(θ[m]) sin(θ[n]) (9) Let = diag(cos(θ)), = diag(sin(θ)). Using Eq 8 & 9, the matrix form for self-attention similarities is: = CQKC + SQKS + S(Q K)C C(Q K)S 1 + Q2K where QK = Q1K 2 Q2K 1 . This formulation can be applied to queries and keys of any dimension by repeating this for the (D/2) complex numbers obtained by dividing the representation into two parts. 2 , = Q1K"
        },
        {
            "title": "Methods",
            "content": "Nun/N MFLOPs"
        },
        {
            "title": "Full Attention",
            "content": "576 / 576 1359.0 VTU Attention-low VTU Attention-mid VTU Attention-high 94 / 576 209 / 576 393 / 576 72.4 357.8 1265.0 Table 1. Comparison of million floating-point operations per second (MFLOPs) between original attention and Virtual Token Unmerging (VTU) attention. refers to full sequence length, Nun refers to unique sequence length after merging. The statistics are computed with batch size 1, head number 32, and head dimension 128. We use the fvcore package for counting FLOPs. In practice, different θ is used for each of the (D/2) components. Using this formulation and the mapping M, we can rewrite the attention matrix in terms of the unique queries and keys as: = CM QunK un + SM QunK un + SM (Qun un )M CM (Qun un )M (10) Observe CM, C, SM, are highly sparse, each with at most non-zero entries. These matrices can also be pre-computed and reused across all self-attention layers. Computing QunK un incurs an O(N 2 un) cost whereas the each of the other matrix multiplications in Eq 10 can be efficiently computed using sparse matrix operations in O(N Nun). We can then use the attention matrix to compute the final output of the layer as: un and Qun (e) = smax( )V = [smax( )M ]Vun Unfortunately, the output (e) RN will not necessarily exhibit the same redundancy as e. This in turn means that the future self-attention layers cannot benefit from the efficiency of virtual token unmerging. In order to remedy this, before passing the output to the future layers, we reintroduce the redundancy by averaging the embeddings in the positions that were originally equal. We denote this remerged output by (eun, ) which can be written as: (eun, ) = (M )1M (e) = (M )1M smax( )V (11) While the above averaging operation breaks the exactness of the future operations, we observe empirically (see Section 4) that this re-merging of tokens, that are known to be redundant, causes minimal drop in performance. Overall Efficiency The computation of attention matrix incurs cost of O(N 2 unD + NunD) (due to the D/2 components). Followed by the softmax and sparse matrix multiplications in Eq 11 which incur cost of O(N 2 + 2 unD). Therefore, the overall complexity for RoPE SelfAttention with Virtual Token Unmerging is O(NunN D). For comparison, the full RoPE Self-Attention on sequence length of would be an O(N 2D) operation. Therefore, in Methods LLaVA-1.5-7B MQT-LLaVA [14] Prumerge [34] Prumerge++ [34] LLaMA-VID [22] VoCo-LLaMA [20] TokenPacker [20] LLaVA-Mini [45] Prumerge-no-ft [34] FastV [38] PDrop [38] SparseVLM [46] ToMe [3] ToMe [3] ToMe [3] DYMU-low DYMU-mid DYMU-high # Visual Tokens Compression in Encoder GQA MMB MME (prcp, all) POPE SQAI SEEDI VQAT MMVet LLaVAW 576 256 32 144 2 1 36 1 32 128 128 128 94 209 393 8927 19547 39457 - No No No No No No No No No No No Yes Yes Yes Yes Yes Yes 62.0 64. 1506,1862 86.9 69.4 66.2 58.3 30. 63.5 Fixed Length Compression & Training-Required 64.3 60.9 64.9 - 58.8 62.8 65.6 1435, - 1350, - 1462, - - , - 1323, - - , - 1466, - 84.4 76.3 84.0 83.1 81.4 86.2 84.4 67.6 68.5 68.3 68.8 65.4 - 70. Fixed Length Compression & Training-Free 68.0 64.4 69.2 67.8 1250, - - , 1490 - , 1713 - , 1721 76.2 53.4 82.3 85.0 - 56.1 61.4 62.3 59.7 62.4 64.1 1357, 1673 1418, 1734 1454, 86.8 87.4 86.7 68.9 69.2 68.4 Variable Length Compression & Training-Free 62.1 62.8 64.3 1438, 1787 1483, 1862 1498, 1846 86.3 86.6 86. 69.3 69.2 69.9 61.6 - - 55.5 57.0 59.6 60.9 - 49.6 56.6 57.2 57.3 59.2 59.5 60.8 61.7 61.9 - - - - - - - - - - - 60.5 63.5 65.1 65.0 65.9 66.1 - 56.0 57.1 49.0 - - 57.0 54.0 50.6 55.9 55.8 53.2 54.9 55. 53.1 55.1 58.0 29.8 - - - - 29.6 36.6 - 26.3 30.8 29.0 25.6 30.9 30.8 30.0 30.9 31.5 64.6 - - - - - 68. - - - - 61.0 62.9 66.0 62.9 65.1 64.5 Avg 55.8 52.6 54.6 55. 54.5 55.3 56.0 Table 2. Comparison with state-of-the-art methods for improving efficiency on LLaVA 1.5 [25]. DYMU-low achieves 97.7% of the original full-length LLaVA baselines performance while using only 15% of the tokens. Importantly, DYMU is entirely training-free and generally outperforms previous fixed-length, training-free methods such as [3, 5, 46], while also enabling variable-length outputs."
        },
        {
            "title": "Methods",
            "content": "# Visual Tokens"
        },
        {
            "title": "GQA MMB",
            "content": "MME (prcp, all)"
        },
        {
            "title": "POPE SQAI SEEDI VQAT MMVet LLaVAW Avg",
            "content": "LLaVA-1.5-w-SigLIP ToMe [3] 576 114 62.7 65.1 1471, 85.7 68.2 59.3 61.4 1380, 1717 85.1 66. DYMU-SigLIP-low 9026 DYMU-SigLIP-mid 17643 DYMU-SigLIP-high 31857 61.3 62.2 62.4 62.5 1398, 1695 63.9 1442, 1744 65.0 1449, 1765 84.9 85.0 86.0 66.7 67.4 67.6 66. 61.8 64.4 65.2 66.0 57.6 52.1 51.8 54.5 56.8 30. 26.1 26.7 26.7 29.4 59.8 57.9 58.6 59.5 58.3 55. 52.4 53.1 53.9 54.7 Table 3. DYMU demonstrates similar efficacy on different visual encoder, SigLIP [44]. We obtain the baseline by following the same training recipe as LLaVA-1.5[25]. DYMU-SigLIP-low achieves 96.2% of the baseline performance while using 15% visual tokens. theory, efficiency improves at least linearly with the number of redundant tokens in terms of FLOPs. Table 1 shows the FLOPs comparison for the attention block. In practice, we find that the wall-clock time difference is marginal due to PyTorchs highly optimized attention and dense matrixmultiplication implementations. 4. Experiments In this section, we present all the details of our implementation of the proposed method. We also present comprehensive analysis demonstrating the practical benefits and efficacy of utilizing DYMU with various VLMs, visual encoders and LLM architectures. and performance remains robust to dataset changes. Importantly, we only use the images to estimate the thresholds (in inference mode) and do not use the associated annotations or text in any way. DYMU variants For each visual encoder in the experiments, including CLIP [33] and SigLIP [1, 44], we find thresholds for three variants of the encoder by choosing different average number of tokens to drop (ri) in each layer. We represent these variants by -low,-mid,-high corresponding to the expected average number of tokens. We also explore different VLM backbones including fixedresolution models, e.g., LLaVA 1.5 [25] and any-resolution models, e.g., LLaVA-OneVision [19]. 4.1. Implementation Details 4.2. Quantitative Evaluation Dynamic Token Merging For DToMe, we find layerwise thresholds using diverse dataset of 250k images sampled from the SFT instruction tuning data of LLaVA 1.5 [25] comprising of images from MS-COCO [24], VisualGenome [17], OCR-VQA [31], TextVQA [35] and GQA [16]. We also ablate the choice of image datasets in 4.2. In general, sufficiently diverse image set suffices, Comparing Visual Token Merging Methods for VLMs In order to evaluate efficacy of our approach, we compare against several existing methods that focus on reducing the number of tokens for VLMs. To the best of our knowlCLIP version: openai/clip-vit-large-patch14-336 SigLIP with LLaVA-1.5: timm/ViT-B-16-SigLIP-384 SIgLIP version with LLaVA-OV: google/siglip-so400m-patch14-"
        },
        {
            "title": "Methods",
            "content": "% Visual Tokens"
        },
        {
            "title": "Video Benchmarks\nMMB MME SEED MathVista VidMME MMBVid",
            "content": "LLaVA-ov-7B 100% 79.3 75.8 ToMe [3] 14.4% 71.2 63. DYMU-ov-low 14.4% 73.6 DYMU-ov-mid 25.1% 76.0 DYMU-ov-high 46.5% 77.8 68.0 70.3 73.6 75.6 68.3 72.9 73.7 74.2 58. 46.6 47.4 51.7 54.4 61.3 57.6 59.3 60.1 60.1 1. 1.08 1.08 1.12 1.16 Table 4. DYMU shows consistent effectiveness on an AnyRes VLM backbone, LLaVA-OneVision [19]. We additionally show performance on two comprehensive video understanding benchmarks, where DYMU-ov-low achieves 96.5% of the baselines performance with only 14% tokens. Figure 4. Importance of Virtual Token Unmerging (VTU). We ablate the performance of LLaVA 1.5 with two token reduction methods applied to the visual encoderToMe (fixed-length) and DToMe (variable-length). We observe that applying VTU significantly improves performance on 8 out of 9 benchmarks, demonstrating robustness to varied token reduction methods. Figure 3. Image Complexity vs Token Count and Accuracy The scatter plot (left) demonstrates strong correlation between DyMUs token count and image complexity scoremore complex images naturally receive more tokens. On the right, MME accuracy at varying complexity levels is compared between ToMe (fixed-length) and DyMU (dynamic-length), highlighting the benefit of assigning additional tokens to complex images. edge, our proposed approach is the first to 1) enable varied number of visual tokens and 2) not require further finetuning of the VLM. Nevertheless, we compare to methods that are designed to reduce the number of tokens by fixed length. In Table 2, we present quantitative evaluation of all methods applied to pre-trained LLaVA 1.5 [25] architecture on standard VLM benchmarks, including GQA [16], MMBench [27], MME [11], POPE [21], ScienceQA [28], SEED-IMG [18], TextVQA [35], MMVet [43], LLaVABench [25]. DYMU achieves average performances of 97.7%, 99.1%, and 100.4%, relative to the original pretrained model, while reducing the token number by 84.5%, 66.1%, and 31.6%, respectively. DYMU also outperforms previous training-free methods while enabling varied length output per instance. When decreasing the token number, the largest drop happens in TextVQA, which fits our expectation as understanding visual text is highly sensitive to the spatial location of visual tokens, on which the token merging tend to break. Compatibility with Different LLMs and Visual Encoders DYMU can be seamlessly integrated into multiple variants of VLMs featuring different LLMs, visual Figure 5. Comparing thresholds using LLaVA Instruct Data vs Pixmo-Cap. Although both methods use the same per-layer merging hyperparameter (ri ), the Pixmo-based thresholds lead to fewer tokens (top)likely due to domain differences. However, performance across range of benchmarks shows minimal drop (bottom), indicating the robustness of our threshold estimation. encoders, and pretraining strategies. In Tables 2 and 3, we demonstrate that DYMU effectively maintains baseline performance when applied both CLIP [33] to SigLIP [44] representations within the LLaVA 1.5 framework, using Vicuna-7B [7] LLM. Furthermore, in Table 4 we evaluate DYMU on LLaVAOneVision [19], recent Any-Resolution (AnyRes) model with SigLIP-so400M [1] as visual encoder and Qwen2 [39] as LLM backbone. AnyRes enables processing images of arbitrary resolutions by segmenting them into smaller regions and encoding each individually. Our results show that DYMU remains compatible with this complex operation, preserving performance while dynamically reducing token counts. Additionally, we extend our evaluation to video benchmarks using LLaVA-OneVision. By applying DYMU to the visual encoder, we achieve variable reduction in feature representations per frame while maintaining Figure 6. Controllable Visual Token Length. By dynamically allocating tokens based on image complexity, DYMU enables direct control over computational cost. In these examples, we combine DYMU with additional vision toolsbackground removal, OCR, or object detectionto focus only on the relevant regions. As result, token count is substantially reduced without degrading performance, showcasing the flexibility of DYMU to adapt token usage according to the tasks requirements. strong performance across benchmarks. model capabilities despite token reduction. Image Complexity vs Number of Tokens In Figure 3 (left), we show how the number of tokens varies with image complexity. We quantify image complexity C(I) by computing the JPEG compression ratio, i.e., C(I) = SJP EG(I) HW , where SJP EG is the size (in bytes) of the image after JPEG encoding, and H, are the original height and width. For this experiment, we use CLIP-L/14-336 with DToMe -low to encode images in the MME benchmark. We observe strong correlation between the number of output tokens and image complexity, indicating that DToMe effectively preserves essential details in complex images while reducing redundancy in simpler ones. We include more qualitative visualizations in Appendix A. Fixed vs Dynamic Token Reduction In Figure 3 (right), we categorize images into three bins based on their complexity scores, and compare the performance of ToMe (fixed-length token reduction) and DToMe on the MME benchmark. key drawback of fixed token reduction is its inability to adapt to image complexity, leading to overcompression for complex images and under-compression for simpler ones. While our method outperforms ToMe across all complexity levels, we observe the most significant gains on complex images, where ToMe struggles due to an insufficient number of tokens. Importance of Virtual Token Unmerging VTU efficiently reconstructs the representation of full visual token sequence from reduced set of visual tokens To demonstrate its impact, we compare LLaVA 1.5 variants with and without VTU. In the latter, the LLM does not undergo any modifications and directly receives fewer tokens. In Figure 4, we evaluate this effect on two token reduction methods: ToMe [3], which produces fixed-length sequences, and DToMe (ours). Across both cases, we observe that applying VTU significantly improves performance on 8 out of 9 benchmarks, demonstrating its effectiveness in preserving Impact of Dataset for Threshold Finding The DToMe thresholds are computed using images from the LLaVA instruction tuning dataset. Here, we investigate the sensitivity of DToMe to the threshold estimation dataset. In Figure 5, we evaluate DYMU-LLaVA 1.5 with DToMe thresholds estimated on the Pixmo-Cap [8] image-captioning dataset. We observe minimal performance change across all the benchmarks, highlighting the robustness of our method to dataset variation. Interestingly, we observe that the thresholds estimated using the Pixmo-Cap dataset lead to fewer tokens during inference on the benchmarks. We hypothesize that this is due to the domain shift between the PixmoCap images and more diverse LLaVA-instruct dataset which covers diverse real-world use cases. 4.3. Qualitative Analysis Visualizing Variable Visual Token Length DToMe facilitates producing variable number of token embeddings for images based on complexity of the content. In Appendix Figure 7, we visualize the number of visual tokens for various images from nine benchmarks. For each benchmark, we present three images corresponding to the minimum, median, and maximum token numbers output by DYMUlow. We observe strong correlation, both within and across different benchmarks, between image complexity and the number of tokens retained by DYMU. Controllable Visual Token Length Dynamic Token Merging offers key advantage over fixed token reduction methods: cost controllability. By dynamically adjusting the number of visual tokens based on image complexity, users gain direct control over the computational cost incurred per image. This flexibility allows flexible combination of visual reasoning tools with DYMU to further boost efficiency while maintaining performance. For instance, in Figure 6, we show example applications of combining DYMU with additional tools, i.e., background removal [4], OCR [9], and object detection [30] models, to extract focused regions and further reduce token count. Unlike existing VLMs, which impose fixed token budget per image regardless of content, our method enables adaptive token allocation, ensuring that simpler regions consume fewer resources while more complex regions retain the necessary level of detail. 5. Conclusions and Future Work In this work, we introduced DYMU, the first training-free framework that dynamically reduces visual token counts in VLMs based on per-image complexity. DYMU can be directly plugged into all mainstream VLMs that comprise ViT-based visual encoders and RoPE-based LLM backbones. Future work includes improving DYMUs ability to preserve VLM performance on spatially sensitive tasks such as TextVQA [35] and spatial reasoning. Additionally, exploring the extension of DYMU to reduce temporal redundancy in videos is another promising direction."
        },
        {
            "title": "References",
            "content": "[1] Ibrahim Alabdulmohsin, Xiaohua Zhai, Alexander Kolesnikov, and Lucas Beyer. Getting vit in shape: Scaling laws for compute-optimal model design. Advances in Neural Information Processing Systems, 36:1640616425, 2023. 6, 7 [2] Shuai Bai, Keqin Chen, Xuejing Liu, Jialin Wang, Wenbin Ge, Sibo Song, Kai Dang, Peng Wang, Shijie Wang, Jun Tang, et al. Qwen2. 5-vl technical report. arXiv preprint arXiv:2502.13923, 2025. 2 [3] Daniel Bolya, Cheng-Yang Fu, Xiaoliang Dai, Peizhao Zhang, Christoph Feichtenhofer, and Judy Hoffman. Token merging: Your vit but arXiv preprint arXiv:2210.09461, 2022. 2, 3, 4, 6, 7, 8, 12, 14 faster. [4] Bria AI. RMBG-1.4: Background Removal Model. https: //huggingface.co/briaai/RMBG-1.4, 2024. 9 [5] Liang Chen, Haozhe Zhao, Tianyu Liu, Shuai Bai, Junyang Lin, Chang Zhou, and Baobao Chang. An image is worth 1/2 tokens after layer 2: Plug-and-play inference acceleration for large vision-language models. In European Conference on Computer Vision, pages 1935. Springer, 2024. 2, 6 [6] Xinlei Chen, Hao Fang, Tsung-Yi Lin, Ramakrishna Vedantam, Saurabh Gupta, Piotr Dollar, and C. Lawrence Zitnick. Microsoft COCO Captions: Data Collection and Evaluation Server. CoRR, abs/1504.00325, 2015. [7] Wei-Lin Chiang, Zhuohan Li, Zi Lin, Ying Sheng, Zhanghao Wu, Hao Zhang, Lianmin Zheng, Siyuan Zhuang, Yonghao Zhuang, Joseph E. Gonzalez, Ion Stoica, and Eric P. Xing. Vicuna: An Open-Source Chatbot Impressing GPT-4 with 90%* ChatGPT Quality, 2023. 7 [8] Matt Deitke, Christopher Clark, Sangho Lee, Rohun Tripathi, Yue Yang, Jae Sung Park, Mohammadreza Salehi, Niklas Muennighoff, Kyle Lo, Luca Soldaini, et al. Molmo and pixmo: Open weights and open data for state-of-the-art multimodal models. arXiv preprint arXiv:2409.17146, 2024. 8 [9] Felix Dittrich. Onnxtr: Optical character recognition made seamless & accessible to anyone, powered by onnx. https://github.com/felixdittrich92/ OnnxTR, 2024. 9 [10] W. N. Francis and H. Kucera. Brown corpus manual. Technical report, Department of Linguistics, Brown University, Providence, Rhode Island, US, 1979. [11] Chaoyou Fu, Peixian Chen, Yunhang Shen, Yulei Qin, Mengdan Zhang, Xu Lin, Zhenyu Qiu, Wei Lin, Jinrui Yang, Xiawu Zheng, Ke Li, Xing Sun, and Rongrong Ji. MME: Comprehensive Evaluation Benchmark for Multimodal Large Language Models. arXiv prepring arXiv:2306.13394, 2023. 2, 7 [12] Yash Goyal, Tejas Khot, Douglas Summers-Stay, Dhruv Batra, and Devi Parikh. Making the in VQA Matter: Elevating the Role of Image Understanding in Visual Question Answering. In 2017 IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2017. 1 [13] Agrim Gupta, Piotr Dollar, and Ross Girshick. LVIS: dataset for large vocabulary instance segmentation. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, 2019. 1 [14] Wenbo Hu, Zi-Yi Dou, Liunian Harold Li, Amita Kamath, Nanyun Peng, and Kai-Wei Chang. Matryoshka query transarXiv preprint former for large vision-language models. arXiv:2405.19315, 2024. 2, 6 [15] Wenxuan Huang, Zijie Zhai, Yunhang Shen, Shaoshen Cao, Fei Zhao, Xiangfeng Xu, Zheyu Ye, and Shaohui Lin. Dynamic-llava: Efficient multimodal large language models via dynamic vision-language context sparsification. arXiv preprint arXiv:2412.00876, 2024. [16] Drew Hudson and Christopher Manning. Gqa: new dataset for real-world visual reasoning and compositional question answering. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 67006709, 2019. 6, 7 [17] Ranjay Krishna, Yuke Zhu, Oliver Groth, Justin Johnson, Kenji Hata, Joshua Kravitz, Stephanie Chen, Yannis Kalantidis, Li-Jia Li, David A. Shamma, Michael S. Bernstein, and Li Fei-Fei. Visual Genome: Connecting Language and Vision Using Crowdsourced Dense Image Annotations. Int. J. Comput. Vis., 123(1):3273, 2017. 6 [18] Bohao Li, Rui Wang, Guangzhi Wang, Yuying Ge, Yixiao Ge, and Ying Shan. SEED-Bench: Benchmarking Multimodal LLMs with Generative Comprehension. arXiv prepring arXiv:2307.16125, 2023. 7 [19] Bo Li, Yuanhan Zhang, Dong Guo, Renrui Zhang, Feng Li, Hao Zhang, Kaichen Zhang, Yanwei Li, Ziwei Liu, and Chunyuan Li. Llava-onevision: Easy visual task transfer. arXiv preprint arXiv:2408.03326, 2024. 2, 4, 6, 7 [20] Wentong Li, Yuqian Yuan, Jian Liu, Dongqi Tang, Song Wang, Jie Qin, Jianke Zhu, and Lei Zhang. Tokenpacker: Efficient visual projector for multimodal llm. arXiv preprint arXiv:2407.02392, 2024. 2, 6 [21] Yifan Li, Yifan Du, Kun Zhou, Jinpeng Wang, Wayne Xin Evaluating object hallucinaarXiv preprint Zhao, and Ji-Rong Wen. tion in large vision-language models. arXiv:2305.10355, 2023. 7 [22] Yanwei Li, Chengyao Wang, and Jiaya Jia. Llama-vid: In An image is worth 2 tokens in large language models. European Conference on Computer Vision, pages 323340. Springer, 2024. 2, 6 [23] Xiaoyu Liang, Chaofeng Guan, Jiaying Lu, Huiyao Chen, Huan Wang, and Haoji Hu. Dynamic token reduction during generation for vision language models. arXiv preprint arXiv:2501.14204, 2025. 2 [24] Tsung-Yi Lin, Michael Maire, Serge J. Belongie, James Hays, Pietro Perona, Deva Ramanan, Piotr Dollar, and C. Lawrence Zitnick. Microsoft COCO: Common Objects in Context. In Computer Vision - ECCV 2014 - 13th European Conference, 2014. 6 [25] Haotian Liu, Chunyuan Li, Yuheng Li, and Yong Jae Lee. Improved Baselines with Visual Instruction Tuning. arXiv prepring arXiv:2310.03744, 2023. 6, 7 [26] Xiangcheng Liu, Tianyi Wu, and Guodong Guo. Adaptive sparse vit: Towards learnable adaptive token pruning by fully exploiting self-attention. arXiv preprint arXiv:2209.13802, 2022. [27] Yuan Liu, Haodong Duan, Yuanhan Zhang, Bo Li, Songyang Zhang, Wangbo Zhao, Yike Yuan, Jiaqi Wang, Conghui He, Ziwei Liu, Kai Chen, and Dahua Lin. MMBench: Is Your Multi-modal Model an All-around Player? arXiv prepring arXiv:2307.06281, 2023. 7 [28] Pan Lu, Swaroop Mishra, Tony Xia, Liang Qiu, Kai-Wei Chang, Song-Chun Zhu, Oyvind Tafjord, Peter Clark, and Ashwin Kalyan. Learn to explain: Multimodal reasoning via thought chains for science question answering. In The 36th Conference on Neural Information Processing Systems (NeurIPS), 2022. 7, 12 [29] Dmitrii Marin, Jen-Hao Rick Chang, Anurag Ranjan, Anish Prabhu, Mohammad Rastegari, and Oncel Tuzel. arXiv preprint Token pooling in vision transformers. arXiv:2110.03860, 2021. 2 [30] Matthias Minderer, Alexey Gritsenko, Austin Stone, Maxim Neumann, Dirk Weissenborn, Alexey Dosovitskiy, Aravindh Mahendran, Anurag Arnab, Mostafa Dehghani, Zhuoran Shen, et al. Simple open-vocabulary object detection. In European conference on computer vision, pages 728755. Springer, 2022. 9 [31] Anand Mishra, Shashank Shekhar, Ajeet Kumar Singh, and Anirban Chakraborty. Ocr-vqa: Visual question answering by reading text in images. In ICDAR, 2019. 1, [32] Maxime Oquab, Timothee Darcet, Theo Moutakanni, Huy Vo, Marc Szafraniec, Vasil Khalidov, Pierre Fernandez, Daniel Haziza, Francisco Massa, Alaaeldin El-Nouby, Mahmoud Assran, Nicolas Ballas, Wojciech Galuba, Russell Howes, Po-Yao Huang, Shang-Wen Li, Ishan Misra, Michael G. Rabbat, Vasu Sharma, Gabriel Synnaeve, Hu Xu, Herve Jegou, Julien Mairal, Patrick Labatut, Armand Joulin, and Piotr Bojanowski. DINOv2: Learning Robust Visual Features without Supervision. arXiv prepring arXiv:2304.07193, 2023. 3 [33] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, Gretchen Krueger, and Ilya Sutskever. Learning Transferable Visual Models From Natural Language Supervision. In Proceedings of the 38th International Conference on Machine Learning (ICML), 2021. 2, 3, 6, 7 [34] Yuzhang Shang, Mu Cai, Bingxin Xu, Yong Jae Lee, and Yan Yan. Llava-prumerge: Adaptive token reduction for efficient large multimodal models. arXiv preprint arXiv:2403.15388, 2024. 2, 6 [35] Amanpreet Singh, Vivek Natarjan, Meet Shah, Yu Jiang, Xinlei Chen, Devi Parikh, and Marcus Rohrbach. Towards vqa models that can read. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 83178326, 2019. 6, 7, 9 [36] Jianlin Su, Murtadha Ahmed, Yu Lu, Shengfeng Pan, Wen Bo, and Yunfeng Liu. Roformer: Enhanced transformer with rotary position embedding. Neurocomputing, 568:127063, 2024. 2, 4, 5 [37] Xinjian Wu, Fanhu Zeng, Xiudong Wang, and Xinghao Chen. Ppt: Token pruning and pooling for efficient vision transformers. arXiv preprint arXiv:2310.01812, 2023. 2 [38] Long Xing, Qidong Huang, Xiaoyi Dong, Jiajie Lu, Pan Zhang, Yuhang Zang, Yuhang Cao, Conghui He, Jiaqi Wang, Feng Wu, et al. Pyramiddrop: Accelerating your large vision-language models via pyramid visual redundancy reduction. arXiv preprint arXiv:2410.17247, 2024. 2, 6 [39] An Yang, Baosong Yang, Binyuan Hui, Bo Zheng, Bowen Yu, Chang Zhou, Chengpeng Li, Chengyuan Li, Dayiheng Liu, Fei Huang, Guanting Dong, Haoran Wei, Huan Lin, Jialong Tang, Jialin Wang, Jian Yang, Jianhong Tu, Jianwei Zhang, Jianxin Ma, Jianxin Yang, Jin Xu, Jingren Zhou, Jinze Bai, Jinzheng He, Junyang Lin, Kai Dang, Keming Lu, Keqin Chen, Kexin Yang, Mei Li, Mingfeng Xue, Na Ni, Pei Zhang, Peng Wang, Ru Peng, Rui Men, Ruize Gao, Runji Lin, Shijie Wang, Shuai Bai, Sinan Tan, Tianhang Zhu, Tianhao Li, Tianyu Liu, Wenbin Ge, Xiaodong Deng, Xiaohuan Zhou, Xingzhang Ren, Xinyu Zhang, Xipin Wei, Xuancheng Ren, Xuejing Liu, Yang Fan, Yang Yao, Yichang Zhang, Yu Wan, Yunfei Chu, Yuqiong Liu, Zeyu Cui, Zhenru Zhang, Zhifang Guo, and Zhihao Fan. Qwen2 technical report, 2024. [40] Xubing Ye, Yukang Gan, Yixiao Ge, Xiao-Ping Zhang, and Yansong Tang. Atp-llava: Adaptive token pruning for large vision language models. arXiv preprint arXiv:2412.00447, 2024. 2 [41] Xubing Ye, Yukang Gan, Xiaoke Huang, Yixiao Ge, Ying Shan, and Yansong Tang. Voco-llama: Towards vision compression with large language models. arXiv preprint arXiv:2406.12275, 2024. 2 [42] Hongxu Yin, Arash Vahdat, Jose Alvarez, Arun Mallya, Jan Kautz, and Pavlo Molchanov. A-vit: Adaptive tokens for efficient vision transformer. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 1080910818, 2022. 2 [43] Weihao Yu, Zhengyuan Yang, Linjie Li, Jianfeng Wang, Kevin Lin, Zicheng Liu, Xinchao Wang, and Lijuan Wang. MM-Vet: Evaluating Large Multimodal Models for Integrated Capabilities. arXiv prepring arXiv:2308.02490, 2023. 7 [44] Xiaohua Zhai, Basil Mustafa, Alexander Kolesnikov, and Lucas Beyer. Sigmoid loss for language image pre-training. In Proceedings of the IEEE/CVF international conference on computer vision, pages 1197511986, 2023. 3, 6, [45] Shaolei Zhang, Qingkai Fang, Zhe Yang, and Yang Feng. Llava-mini: Efficient image and video large multimodal models with one vision token. arXiv preprint arXiv:2501.03895, 2025. 2, 6 [46] Yuan Zhang, Chun-Kai Fan, Junpeng Ma, Wenzhao Zheng, Tao Huang, Kuan Cheng, Denis Gudovskiy, Tomoyuki Okuno, Yohei Nakata, Kurt Keutzer, et al. Sparsevlm: Visual token sparsification for efficient vision-language model inference. arXiv preprint arXiv:2410.04417, 2024. 2, 6 A. Visualization of Variable Token Length In Figure 7, we present comprehensive visualization of example images along with their encoded visual token counts. We use DYMU-low (based on CLIP-L/14-336) as the encoder, where the full token length is 576. Three images are shown for each benchmark, corresponding to the minimum, median, and maximum number of tokens, respectively. clear correlation can be observed between semantic richness and token count. We also note variations in the token range across different benchmarks. For instance, ScienceQA [28], which primarily contains figures and charts, tends to have fewer tokens than benchmarks featuring complex real-world scenes. B. Impact of Token Merging Schedule We conduct an additional ablation study on one of the hyperparameters in DToMe, the merging schedule, during threshold finding. As detailed in Section 3, we set target reduction number, ri , for each layer. By default, ri is set to constant value across all layers. Alternatively, we can vary ri across layers to encourage merging more or fewer tokens at different depths. In Table 5, we present an ablation study on two alternative scheduling strategies: (1) linear, which merges more tokens in earlier layers and fewer tokens in later layers, and (2) reverse linear, which follows the opposite trend. The results indicate that merging fewer tokens in earlier layers tends to yield better performance, while the constant schedule provides balanced trade-off between performance and token count. This observation echoes the findings in the ToMe paper [3], where constant schedule was found to be nearly optimal. C. Full Results for Figure 4 We present the complete results of the ablation experiments on the effect of our proposed Virtual Token Unmerging, as shown in Figure 4. The results are provided in Table 6. D. Full Results for Figure 5 We present the complete results of the ablation experiments on threshold-finding datasets, as shown in Figure 5. The results are provided in Table 7. Figure 7. DToMe Token Count Across Benchmarks. For each dataset, we show three examples processed by our methodthose yielding the fewest tokens, the median number of tokens, and the most tokens. Observe that visually simple or nearly blank images consistently require fewer tokens, while more detailed, semantically complex or cluttered images produce more tokens. This demonstrates how DToMe effectively adapts to image complexity across diverse benchmarks, allocating fewer tokens to simpler content and preserving more tokens for complex scenes."
        },
        {
            "title": "Constant\nLinear\nReverse Linear",
            "content": "# Visual Tokens 19547 16343 21349 GQA MMB MME(prcp,all)"
        },
        {
            "title": "SEEDI VQAT MMVet LLaVAW Avg",
            "content": "61.7 61.3 61.8 62.8 62.3 63.8 1483, 1862 1437, 1767 1491, 1863 86.6 86.2 86.7 69.2 69.4 69.3 65.9 65.3 66. 55.1 52.1 57.5 30.9 28.8 31.8 65.1 58.6 65.3 55.3 53.8 55.9 Table 5. Ablation study on merging schedules in DToMe. We compare three strategies: constant, linear (more merging in early layers), and reverse linear (more merging in later layers). Results show that merging fewer tokens in early layers yields better performance, while the constant schedule provides balanced trade-off between performance and token count."
        },
        {
            "title": "Method",
            "content": "ToMe [3] + VTU # Visual Tokens 94 94 DYMU-low w/o VTU 8927 8927 GQA MMB MME(prcp,all)"
        },
        {
            "title": "Avg",
            "content": "57.3 60.6 60.8 58.2 59.7 63.7 62.1 56.0 1357, 1673 1464, 1815 1438, 1787 1346, 86.8 85.4 86.3 86.9 68.9 69.1 69.3 67.7 60.5 64.9 65.0 60. 53.2 54.8 53.1 51.3 25.6 28.7 30.0 25.2 61.0 62.5 62.9 58. 52.6 54.5 54.5 51.7 Table 6. Impact of Virtual Token Unmerging. Full results for Figure 4."
        },
        {
            "title": "Thresh Finding\nDataset",
            "content": "# Visual Tokens GQA MMB MME(prcp,all)"
        },
        {
            "title": "SEEDI VQAT MMVet LLaVAW Avg",
            "content": "DYMU-mid DYMU-mid"
        },
        {
            "title": "Llava\nPixmo",
            "content": "19547 12030 61.7 61.1 62.8 64.4 1483, 1862 1474, 1808 86.6 86.0 69.2 69. 65.9 65.3 55.1 56.2 30.9 30.5 65.1 63.7 55.3 55.2 Table 7. Impact of dataset for threshold finding. Full results for Figure 5."
        }
    ],
    "affiliations": [
        "Salesforce Research",
        "University of Illinois Urbana-Champaign"
    ]
}