{
    "paper_title": "Wikipedia in the Era of LLMs: Evolution and Risks",
    "authors": [
        "Siming Huang",
        "Yuliang Xu",
        "Mingmeng Geng",
        "Yao Wan",
        "Dongping Chen"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "In this paper, we present a thorough analysis of the impact of Large Language Models (LLMs) on Wikipedia, examining the evolution of Wikipedia through existing data and using simulations to explore potential risks. We begin by analyzing page views and article content to study Wikipedia's recent changes and assess the impact of LLMs. Subsequently, we evaluate how LLMs affect various Natural Language Processing (NLP) tasks related to Wikipedia, including machine translation and retrieval-augmented generation (RAG). Our findings and simulation results reveal that Wikipedia articles have been influenced by LLMs, with an impact of approximately 1%-2% in certain categories. If the machine translation benchmark based on Wikipedia is influenced by LLMs, the scores of the models may become inflated, and the comparative results among models might shift as well. Moreover, the effectiveness of RAG might decrease if the knowledge base becomes polluted by LLM-generated content. While LLMs have not yet fully changed Wikipedia's language and knowledge structures, we believe that our empirical findings signal the need for careful consideration of potential future risks."
        },
        {
            "title": "Start",
            "content": "Wikipedia in the Era of LLMs: Evolution and Risks Siming Huang1, Yuliang Xu1, Mingmeng Geng2*, Yao Wan1*, Dongping Chen1 1 Huazhong University of Science and Technology 2 International School for Advanced Studies (SISSA) mgeng@sissa.it, wanyao@hust.edu.cn 5 2 0 2 4 ] . [ 1 9 7 8 2 0 . 3 0 5 2 : r a"
        },
        {
            "title": "Abstract",
            "content": "In this paper, we present thorough analysis of the impact of Large Language Models (LLMs) on Wikipedia, examining the evolution of Wikipedia through existing data and using simulations to explore potential risks. We begin by analyzing page views and article content to study Wikipedias recent changes and assess the impact of LLMs. Subsequently, we evaluate how LLMs affect various Natural Language Processing (NLP) tasks related to Wikipedia, including machine translation and retrievalaugmented generation (RAG). Our findings and simulation results reveal that Wikipedia articles have been influenced by LLMs, with an impact of approximately 1%-2% in certain categories. If the machine translation benchmark based on Wikipedia is influenced by LLMs, the scores of the models may become inflated, and the comparative results among models might shift as well. Moreover, the effectiveness of RAG might decrease if the knowledge base becomes polluted by LLM-generated content. While LLMs have not yet fully changed Wikipedias language and knowledge structures, we believe that our empirical findings signal the need for careful consideration of potential future risks."
        },
        {
            "title": "Introduction",
            "content": "The creation of Wikipedia challenged traditional encyclopedias (Giles, 2005), and the rapid development and widespread adoption of Large Language Models (LLMs) have sparked concerns about the future of Wikipedia (Wagner and Jiang, 2025; Vetter et al., 2025). In the era of LLMs, it is unlikely that Wikipedia has remained unaffected. Recently, researchers have begun examining the influence of LLMs on Wikipedia. For example, Equal Contribution. Project Leader. * Corresponding Authors. 1We release all the experimental dataset and source code at: https://github.com/HSM316/LLM_Wikipedia. Figure 1: Our work analyze the direct impact of LLMs on Wikipedia, and exploring the indirect impact of LLMs generated content on Wikipedia: Have LLMs already impacted Wikipedia, and if so, how might they influence the broader NLP community? Reeves et al. (2024) analyze Wikipedia user metrics such as page views and edit histories. Meanwhile, Brooks et al. (2024) estimate the proportion of AI-generated content in newly created English Wikipedia articles using Machine-Generated Text (MGT) detectors. Given the richness and significance of Wikipedia, the impact of LLMs on Wikipedia requires more comprehensive and detailed investigation. Wikipedia is widely recognized as valuable resource (Singer et al., 2017), and its content is extensively utilized in AI research, particularly in Natural Language Processing (NLP) tasks (Johnson et al., 2024b). For instance, Wikipedia pages are among the five datasets used to train GPT-3 (Brown et al., 2020). The sentences in the Flores101 evaluation benchmark are extracted from English Wikipedia (Goyal et al., 2022). In the work by Lewis et al. (2020) on Retrieval-Augmented Generation (RAG), Wikipedia content is treated as source of factual knowledge. Therefore, we aim to investigate the influence of LLMs on machine translation and knowledge systems using Wikipedia as key resource. Figure 1 illustrates the various tasks and research topics discussed in this paper. Our first objective is to evaluate the direct impact of LLMs on Wikipedia, focusing on changes in page views, word frequency, and linguistic style. Then we explore the indirect effects on the broader NLP community, particularly in relation to machine translation benchmarks and RAG, both of which rely heavily on Wikipedia content for their corpora. Thus, we are in better position to observe and assess the evolutions and risks of Wikipedia in the era of LLMs. Our analysis yields number of significant insights: There has been slight decline in page views for certain scientific categories on Wikipedia, but the connection to LLMs remains uncertain. While some Wikipedia articles have been influenced by LLMs, the overall impact has so far been quite limited. If the sentences in machine translation benchmarks are drawn from Wikipedia content shaped by LLMs, the scores of machine translation models are likely to be inflated, potentially reversing the outcomes of comparisons between different models. Wikipedia content processed by LLMs could appear less effective for RAG compared to real Wikipedia content. Based on these findings, we underscore the importance of carefully assessing potential risks and encourage further exploration of these issues in subsequent studies. The key contributions of this paper are threefold, as we are the first to: (1) quantify the impact of LLMs on Wikipedia pages across various categories; (2) analyze the impact of LLMs on Wikipedia from the perspective of word usage and provide the corresponding estimates; and (3) examine how LLM-generated content affects machine translation evaluation and the efficiency of RAG systems. This is also very likely the first paper to comprehensively analyze the impact of LLMs on Wikipedia based on data and simulations."
        },
        {
            "title": "2 Related Work",
            "content": "Wikipedia for NLP. Wikipedia has long been utilized in various NLP applications (Strube and Ponzetto, 2006; Mihalcea and Csomai, 2007; Zesch et al., 2008; Gabrilovich and Markovitch, 2009; Navigli and Ponzetto, 2010). In the era of LLMs, Wikipedia also plays role, such as in factchecking (Hou et al., 2024) and reducing hallucinations (Semnani et al., 2023). Writing Wikipedialike articles is also one of the LLM applications (Shao et al., 2024). LLMs for Wikipedia. Researchers are trying to use LLMs to enhance Wikipedia, including articles (Adak et al., 2025), Wikidata (Peng et al., 2024; Mihindukulasooriya et al., 2024) and edit process (Johnson et al., 2024a). Some researchers have compared LLM-generated or rewritten Wikipedia articles with human-written ones, yielding differing conclusions (Skarlinski et al., 2024; Ashkinaze et al., 2024; Zhang et al., 2025a). Wikipedia. The value of Wikipedia is not limited to NLP. McMahon et al. (2017) have pointed out the substantial interdependence of Wikipedia and Google, and Vincent et al. (2018) found that Wikipedia can provide great value to other online communities. Despite some shortcomings (Kousha and Thelwall, 2017), the influence of Wikipedia is border, including impacts on academic paper citations (Thompson and Hanley, 2018) and the click counts of other web pages (Piccardi et al., 2021). Estimation of LLM Impact. The detection of AI-generated content has been hot research topic in recent years (Wu et al., 2025; Wang et al., 2025; Zhang et al., 2024), including its application to Wikipedia articles (Brooks et al., 2024). But MGT detectors have notable limitations (Doughman et al., 2024), and as result, researchers are also exploring other methods for estimating the LLM impact, such as word frequency analysis (Liang et al., 2024; Geng and Trotta, 2024)."
        },
        {
            "title": "3 Data Collection",
            "content": "Wikipedia and Wikinews are both projects under the Wikimedia Foundation. While Wikipedia is the main focus of this paper, we also collect Wikinews articles from 2020 to 2024 to generate questions in Section 5.2. On average, there are over hundred news per year, covering wide variety of topics. 2 We are interested in Wikipedia pages that belong to the following categories: Art, Biology, Computer Science (CS), Chemistry, Mathematics, Philosophy, Physics, Sports. Wikipedia uses hierarchical classification system for articles. It begins with toplevel categories that cover broad fields, which are then divided into more specific subcategories. Only pages created before 2020 and subcategories that are four or five levels away from our target category were included in our study. Then we scrape the Wikipedia page versions from 2020 to 2025 (more accurately, the version on January 1 of each year). Among them, Philosophy has the smallest number of articles (33,596), and CS leads with the largest number (59,097). More details on data collection and processing are shown in Appendix A. For better comparison, we also collect 6,690 Featured Articles (FA), along with their corresponding 2,029 simple English versions (where available) as Simple Articles (SA)."
        },
        {
            "title": "4.1 Direct Impact 1: Page View",
            "content": "We collect page views of Wikipedia articles via Wikimedia API and analyze their evolution over time. The page views normalized to 30-day month are plotted in Figure 9 of the appendix. Similar to the work of Reeves et al. (2024), we transform the page view values using the inverse hyperbolic sine function, and the results are shown in Figure 2. Finding 1: In the second half of 2024, there was slight decline in page views across some scientific categories, and its connection to the use of LLMs requires further investigation."
        },
        {
            "title": "4.2 Direct Impact 2: Word Frequency",
            "content": "In addition to page views, LLMs may have also impacted the content of Wikipedia articles. Figures 3 and 10 illustrate the increasing frequency of the words crucial and additionally, which are favored by ChatGPT (Geng and Trotta, 2024). Since we are comparing the same pages across different years, we can adopt one existing framework (Geng et al., 2024) to estimate the impact of LLMs η in one set of articles by ˆη(S) = ˆri = (S)ˆri (S)(cid:1)f (cid:1) (S)ˆri iI (cid:80) (cid:0)f (S) (cid:0)f (cid:80) (S2) (S1) (S1) iI , , (1) (2) 3 Figure 2: Monthly page views across different Wikipedia categories. The vertical axis represents the transformed page view values, standardized using the Inverse Hyperbolic Sine (IHS) function. Figure 3: Word frequency in the first section of the Wikipedia articles. where (S) represents the frequency of word in the set of texts S, (S) represents the one if LLMs do not affect the texts, is the set of words used for estimation, (S1) and (S2) represent the frequency of word for another set of articles before and after LLM processing, respectively. We take the average of the word frequencies from the 2020 and 2021 versions of the page as (S). But different texts still lead to different estimations, and using different words for estimation will also produce different results. When estimating ri through simulations using the first section of Featured Articles and GPT-4omini with simple prompt: Revise the following sentences, the LLM impact is approximately 1%- 2% for the articles in certain categories, as illustrated in Figure 4. Additional results in Appendix confirm that LLMs have influenced certain categories of Wikipedia articles created before 2020. tic feature in assessing the quality of Wikipedia articles (Moás and Lopes, 2023). Sentence Level. In terms of sentence structure, we focus on sentence length and the use of passive voice (AlAfnan and MohdZuki, 2023). Regarding sentence complexity, we analyze both the depth of the entire syntactic tree and the clause ratio (Iavarone et al., 2021). Paragraph Level. For the paragraph dimension, which is essential for Wikipedias educational mission (Johnson et al., 2024b), we seek guidance from readability evaluation (Moás and Lopes, 2023), where six traditional formulas have been included in our study: Automated Readability Index (Mehta et al., 2018), Coleman-Liau Index (Antunes and Lopes, 2019), Dale-Chall Score (Patel et al., 2011), Flesch Reading Ease (Eleyan et al., 2020), FleschKincaid Grade Level (Solnyshkina et al., 2017), and Gunning Fog index ( Swieczkowski and Kułacz, 2021). LLM Simulation Wikipedia articles are not static, and their linguistic styles are difficult to remain the same under different measurement metrics. To understand the link between these trends and LLMs, we simulate the real Wikipedia with GPT-4o-mini and Gemini-1.5-Flash, then compare the changes before and after the process."
        },
        {
            "title": "4.3.2 Results",
            "content": "Table 1 presents the summary of the trends in linguistic style in real Wikipedia articles and LLM simulations. The detailed outcomes are illustrated in Figure 5 and Appendix D. Although we have plotted the results from 2020 in the these figures, the trends summarized in the table are based on the data in the LLM era, that is, after 2023. Finding 3: The trends of changes in Wikipedia articles are largely consistent with the preferences of LLMs under most metrics. Figure 4: LLM Impact: Estimated based on simulations of the first section of Featured Articles, using different word combinations across different categories of Wikipedia pages. Finding 2: While the estimation results vary, the influence of LLMs on Wikipedia is likely to become more significant over time. In some categories, the impact has exceeded 2%."
        },
        {
            "title": "4.3 Direct Impact 3: Linguistic Style",
            "content": "Overall. Beyond word frequency, we investigate the current and future impact of LLMs on Wikipedia from more linguistic perspectives. In this section, we examine the evolution of Wikipedia content at Word, Sentence, and Paragraph levels, by comparing the texts before and after LLM processing under the same standards."
        },
        {
            "title": "4.3.1 Experiment Setups",
            "content": "Word Level. Unlike the previous part, we focus on other metrics at the word level. The frequency of auxiliary verbs indicates the ability of model to convey complex reasoning and logical relationships (Yang et al., 2024). Lexical diversity, often measured by the corrected type-token ratio (CTTR), reflects the variety of words (Wróblewska et al., 2025). Furthermore, the proportion of specific parts of speech (POS) is commonly used as stylisFor example, our simulation results reveal that LLMs substantially reduce the use of auxiliary verbs, with Gemini employing even fewer than GPT, as shown in Figure 5a. Consistent with this tendency, the usage of auxiliary verbs on real Wikipedia pages shows marginal decline from 2020 to 2025, as depicted in Figure 5d. However, the trends of passive voice proportion in Figures 5b and 5e are not the same. 4 (a) Auxiliary verbs proportion. (b) Passive voice proportion. (c) Readability metrics comparison. (d) Change in auxiliary verbs proportion. (e) Change in passive voice proportion. (f) Change in FleschKincaid readability. Figure 5: The results of linguistic style comparison, including the real Wikipedia pages and LLM-simulated pages. The three subplots below represent the differences compared to the data from 2020. Table 1: Summary of linguistic style trends. The second column indicates the effects of LLM processing. The third column shows Wikipedia trends over time. Criteria LLM Data Figures Auxiliary verb % \"To Be\" verb % CTTR Long word % Conjunction % Noun % Preposition % Pronouns % One-syllable word % Average syllables per word Passive voice % Long sentence % Average sentence length Average parse tree depth Clause % Pronoun-initial sentence % Article-initial sentence % 5a, 5d 14 15 16 17a, 17b, 17c 17d, 17e, 17f 17g, 17h, 17i 17j, 17k, 17l 18a, 18b, 18c 18d, 18e, 18f 5b, 5e 19a, 19b, 19c 19d, 19e, 19f 20a, 20b, 20c 20d, 20e, 20f 21a, 21b, 21c 21d, 21e, 21f Dale-Chall readability Automated readability index Flesch-Kincaid grade level Flesch reading ease Coleman-Liau index Gunning Fox index 5c, 22a 5c, 22b 5c, 5f 5c, 22c 5c, 22d 5c, 22e that LLM-generated texts tend to be less readable. The FleschKincaid score in Figure 7 is also very interesting, initially decreasing and then rising, the score after LLM simulation also increases."
        },
        {
            "title": "Indirect Impact from LLMs",
            "content": "5.1 Indirect Impact 1: Machine Translation Overall. The sentences of some machine translation benchmarks are derived from Wikipedia. If these benchmarks are also influenced by LLMs, what impact would it have on the evaluation results?"
        },
        {
            "title": "5.1.1 Experiments Setups\nBenchmark Construction. We utilize the Flo-\nres dataset2, which comprises multiple sentence\nsets, each representing a single Wikipedia sentence\navailable in several languages. Subsequently, we\nuse GPT-4o-mini to translate the English (EN) ver-\nsion into the other languages, replacing the original\nversions to construct the LLM-influenced bench-\nmark. The following 11 widely used languages are\nused in our simulations: Modern Standard Arabic\n(AR), Mandarin (ZH), German (DE), French (FR),\nHindi (HI), Italian (IT), Japanese (JA), Korean\n(KO), Brazilian Portuguese (PR), Russian (RU),",
            "content": "For paragraph level, Figure 5c presents the results of six readability metrics, all of which indicate 2https://huggingface.co/datasets/ openlanguagedata/flores_plus 5 Table 2: Facebook-NLLB Results on BLEU, ChrF, and COMET Metrics. and represent the original benchmark and GPT-processed benchmark, respectively. BLEU ChrF COMET G 87.04 FR DE 72.39 ZH 72.14 71.86 AR 69.59 PT 62.05 JA 59.25 ES 58.60 IT HI 58.49 KO 54.75 RU 51.40 96.75 93.38 78.61 78.73 87.71 64.21 84.44 62.14 67.29 78.35 63.33 94.62 77.98 67.06 83.89 79.41 56.86 73.70 67.31 75.25 52.50 73.97 99.31 96.10 78.19 88.61 92.02 58.03 90.70 78.22 80.64 69.23 84. 90.45 84.70 82.40 83.19 88.93 62.61 85.03 85.22 59.53 25.94 84.75 87.79 86.37 83.91 84.04 90.45 62.87 89.49 88.72 60.16 25.98 86.37 Latin American Spanish (ES). These languages represent diverse set of linguistic families and regions, offering broad evaluation of the models performance across different cultural and linguistic contexts. More details are shown in Appendix C.2. Evaluation Pipeline. After collecting LLMtranslated English samples, we use different machine translation models to translate these sentences into other languages. Three metrics are employed to evaluate translation results: BLEU, which uses n-gram precision with brevity penalty (Post, 2018); COMET, which leverages source and reference information (Rei et al., 2020); and ChrF, which computes character-level F1 scores. These metrics compare machine-translated outputs against human-translated references. Models. We compare the translation results from three models: Facebook-NLLB3, multilingual model supporting 200+ languages (Costa-Jussà et al., 2022); Google-T5 (mT5)4; pre-trained on data covering 101 languages (Xue et al., 2021); and Helsinki-NLPs bilingual Transformer models5 trained on OPUS corpus (Tiedemann and Thottingal, 2020; Tiedemann et al., 2023)."
        },
        {
            "title": "5.1.2 Results",
            "content": "The results of the comparison between machine translation models could be reversed. For example, Facebook-NLLB gets better BLEU score than Helsinki-NLP in the original benchmark, but 3https://huggingface.co/facebook/nllb-200-3. 3B 4https://huggingface.co/google/mt5-small 5https://github.com/Helsinki-NLP/Opus-MT Table 3: Helsinki-NLP Results on BLEU, ChrF, and COMET Metrics. BLEU ChrF COMET G 88.39 FR DE 68.07 ZH 70.34 67.52 AR 69.74 PT 49.48 JA 60.00 ES 56.14 IT 46.85 HI KO 45.28 RU 44.99 89.40 90.68 75.32 70.99 85.99 45.28 84.07 69.32 49.37 57.53 69.18 91.18 77.17 59.08 80.70 81.12 49.43 74.45 67.97 58.20 58.36 70.15 91.32 94.83 65.10 87.20 91.60 46.40 91.26 82.04 57.06 68.94 81. 88.39 86.35 84.19 85.24 90.71 64.15 86.91 87.53 62.31 29.34 86.12 89.91 87.98 85.73 86.14 92.31 64.37 91.24 90.11 63.18 29.48 87.83 worse score in the GPT-processed benchmark, as shown in Tables 2 and 3. In most cases, machine translation models achieve higher scores on the GPT-processed benchmark compared to the original benchmark, as listed in the two tables above and Table 5 in the Appendix. Finding 4: The impact of LLMs on the benchmark could not only inflate the translation scores across different languages but also distort the comparison of translation abilities between models, making it fail to truly reflect their translation effectiveness. 5. Indirect Impact 2: RAG Overall. RAG can provide more reliable and up-to-date external knowledge to mitigate hallucination in LLM generation (Gao et al., 2023). Wikipedia is one of the most commonly applied general retrieval sets in previous RAG work, which stores factual structured information in scale (Fan et al., 2024). In the process of translation using LLMs, some information may also be lost or distorted (Mohamed et al., 2025). Therefore, we are curious how the effectiveness of RAG might change if Wikipedia pages are influenced by LLMs. Our experiment procedure is illustrated in Figure 6 and the detailed steps are listed below."
        },
        {
            "title": "5.2.1 Experiments",
            "content": "Question Generation. GPT-4o-mini and Gemini1.5-flash are used to generate multiple-choice questions (MCQs) according to Wikinews article. In order to generate some Wikinews-based questions that are not too easy for LLMs, we refer to the 6 Figure 6: GPT-4o-mini and Gemini-1.5-flash are used to generate multiple-choice questions (MCQs) based on the extracted Wikinews data. Various questioning methods are employed with both GPT-4o-mini and GPT-3.5 to evaluate the specific impact of LLM-generated texts on the RAG process."
        },
        {
            "title": "Prompt",
            "content": "You are to generate three self-contained multiple-choice questions based on the facts mentioned in the following content. Avoid questions that reference the content directly. Each question should include all relevant context and directly name any referenced items, avoiding pronouns like it, the game, or the person. Do not include phrases that reference the source or context, such as mentioned in the article or according to the text. Figure 7: Prompt used to generate questions for RAG task. prompt in the work of Zhang et al. (2025b), shown in Figure 7. Retrieval and Generation. The question is vectorized with BERT and performed similarity search in FAISS. The three most relevant segments are retrieved and provided as context, then combined with the question and used in prompt template to ask LLMs. The answer is selected based both on the prior knowledge of LLM and the retrieved content. Questioning Methods. We conduct experiments using different questioning methods, which also involve different LLMs. Firstly, we can question the LLMs directly to obtain answers. Secondly, the Wikinews page used to generate the question is included in the prompt. Finally, RAG can be used to perform searches in the knowledge base. For the latter two scenarios, there are also different cases involving either the original Wikinews pages or the pages processed by LLMs."
        },
        {
            "title": "5.2.2 Results",
            "content": "Knowledge Base. We construct the knowledge base using Wikinews articles from 2020 to 2024. Each article is preprocessed and split into smaller text segments, then vectorized via BERT (Devlin et al., 2019). We then indexed these vectors using FAISS, library for efficient similarity search and clustering of dense vectors, for efficient retrieval (Douze et al., 2024). Figure 8 illustrates the summary of the accuracy rates of LLM responses under different scenarios, with more detailed results provided in Appendix D.4. The analysis based on these results leads to the following conclusions: Higher Accuracy with Knowledge Base. Providing external knowledge greatly improves perfor7 two separate sentences to present both President Macrons and the U.K.s perspectives, whereas the revised text combines them into single sentence, which misleads the LLM into incorrectly selecting the answer B. More examples are included in Appendix E.2, and LLM-generated texts may decrease accuracy in RAG tasks for several reasons: Information Fusion Misleading: When LLMs merge multiple distinct and clear pieces of information into single sentence, it can lead to misinterpretation as shown in Figure 6. Keyword Replacement and Omission: LLM might replace or omit key terms, altering the original meaning and causing misinterpretation in Figures 23, 24 and 25. Abbreviation Ambiguity Misleading: LLMs use abbreviations or shortened terms inappropriately, leading to misinterpretation as shown in Figure 26. Introduction of Modifiers: Adding adjectives or modifiers can change the context and impact the texts accuracy, as illustrated in Figure 27. Retrieval Mismatch: Revised texts may reduce the similarity between the question and the correct article, or increase the similarity with irrelevant ones. Sometimes, even with minimal changes to the article, it still fails to match. Finding 5: The results suggest that LLMprocessed content could perform less effectively in RAG systems compared to humancreated texts. If such content has impacted high-quality communities like Wikipedia, it raises concerns about the potential decline in information quality in knowledge bases."
        },
        {
            "title": "6 Discussion and Conclusion",
            "content": "The relationship between Wikipedia and LLMs is bidirectional. On the one hand, Wikipedia content has been key factor in the growth of LLMs. On the other hand, researchers have used NLP methods, including LLMs, to improve Wikipedia (LucieAimée et al., 2024). Humans and LLMs are coevolving (Geng and Trotta, 2025), and Wikipedia may be one of the bridges in this process. Our findings that LLMs are impacting Wikipedia and the impact could extend indirectly to some NLP tasks through their dependence on Wikipedia content. For instance, the target language for machine translation may gradually shift towards the Figure 8: The accuracy rate of LLM responses under different settings. For each case, more than 1,800 questions based on Wikinews articles from 2020 to 2024 are used for simulations. More detailed results are presented in Appendix D.4. mance. With knowledge base, the accuracy of responses often exceeds 80%. This confirms the effectiveness of RAG in enhancing factual accuracy. Maximal Performance with Full Content. Providing the full news as context yields the highest accuracy, demonstrating the limitations of retrievalbased approaches in selecting the most relevant information. In most cases with GPT-4o-mini, the full content approach exceeded 93% accuracy, setting benchmark for ideal retrieval performance. Impact of LLM-Revised Content. Compared to the cases using real Wikinews articles, the accuracy of responses based on ChatGPT-processed pages shows little change and responses based on Gemini-processed pages show clear drop in accuracy. This suggests that Geminis rewriting may lead to the loss of some key information. Declining Accuracy for Recent Events. In the absence of RAG, both models exhibit significantly lower accuracy when answering questions derived from recent Wikinews articles (e.g., GPT-4o-mini shown in Table 6 of the appendix: 66.67% in 2024, GPT-3.5: 61.25% in 2024), while their accuracy is much better for older events (e.g., 20202022). The reason is also straightforward: these news events are not included in their training data."
        },
        {
            "title": "5.2.3 Case Study\nTo explore the impact of LLM-generated texts, we\nfocus on cases where the answering model answers\ncorrectly with the original content but fails when\nusing LLM-revised content. Figure 6 has provided\none interesting example: the original text6 uses",
            "content": "6https://en.wikinews.org/wiki/Ukraine_ permitted_to_strike_Russian_territory_near_ Kharkiv 8 language style of LLMs, albeit in small steps. In addition, the accuracy of RAG tasks may decline when LLM-revised Wikipedia pages are used, indicating the potential risks associated with using LLMs to support Wikipedia or similar knowledge systems. The impact of LLMs on human engagement with Wikipedia is also worthy of investigation, as Wikipedias success has been largely driven by the contributions of human editors (Kittur and Kraut, 2008). It is important to note that human curation does not guarantee perfection. The dynamic between humans and AI, where both continuously shape each other, has become defining feature of modern society."
        },
        {
            "title": "Limitations",
            "content": "Although we conduct several experiments to evaluate the impact of LLMs on Wikipedia, our study has certain limitations. First, Wikipedia pages follow specific format, making it challenging to extract completely plain text. This formatting issue in our dataset may introduce some errors in the quantitative analysis of LLM impact. Second, when assessing the readability of Wikipedia pages, we rely only on traditional metrics based on formulas, such as the Flesch-Kincaid score. However, recent advances in NLP have shifted towards more sophisticated computational models (François, 2015). Lastly, in the RAG task, our Wikinews dataset is not large enough compared to the Wikipedia page dataset, which may limit the generalization of our findings."
        },
        {
            "title": "References",
            "content": "Josh Achiam, Steven Adler, Sandhini Agarwal, Lama Ahmad, Ilge Akkaya, Florencia Leoni Aleman, Diogo Almeida, Janko Altenschmidt, Sam Altman, Shyamal Anadkat, et al. 2023. Gpt-4 technical report. arXiv preprint arXiv:2303.08774. Sayantan Adak, Pauras Mangesh Meher, Paramita Das, and Animesh Mukherjee. 2025. Reversum: multistaged retrieval-augmented generation method to enhance wikipedia tail biographies through personal narratives. In Proceedings of the 31st International Conference on Computational Linguistics: Industry Track, pages 732750. Mohammad Awad AlAfnan and Siti Fatimah MohdZuki. 2023. Do artificial intelligence chatbots have writing style? an investigation into the stylistic features of chatgpt-4. Journal of Artificial intelligence and technology, 3:8594. Hélder Antunes and Carla Teixeira Lopes. 2019. Analyzing the adequacy of readability indicators to In Experimental IR Meets non-english language. Multilinguality, Multimodality, and Interaction: 10th International Conference of the CLEF Association, CLEF 2019, Lugano, Switzerland, September 912, 2019, Proceedings 10, pages 149155. Springer. Joshua Ashkinaze, Ruijia Guan, Laura Kurek, Eytan Adar, Ceren Budak, and Eric Gilbert. 2024. Seeing like an ai: How llms apply (and misapply) wikipedia neutrality norms. arXiv preprint arXiv:2407.04183. Creston Brooks, Samuel Eggert, and Denis Peskoff. 2024. The rise of ai-generated content in wikipedia. arXiv preprint arXiv:2410.08044. Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. 2020. Language models are few-shot learners. Advances in neural information processing systems, 33:18771901. Marta Costa-Jussà, James Cross, Onur Çelebi, Maha Elbayad, Kenneth Heafield, Kevin Heffernan, Elahe Kalbassi, Janice Lam, Daniel Licht, Jean Maillard, et al. 2022. No language left behind: Scaling human-centered machine translation. arXiv preprint arXiv:2207.04672. Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. 2019. Bert: Pre-training of deep bidirectional transformers for language understanding. In Proceedings of the 2019 conference of the North American chapter of the association for computational linguistics: human language technologies, volume 1 (long and short papers), pages 41714186. Jad Doughman, Osama Mohammed Afzal, Hawau Olamide Toyin, Shady Shehata, Preslav Nakov, and Zeerak Talat. 2024. Exploring the limitations of detecting machine-generated text. arXiv preprint arXiv:2406.11073. Matthijs Douze, Alexandr Guzhva, Chengqi Deng, Jeff Johnson, Gergely Szilvasy, Pierre-Emmanuel Mazaré, Maria Lomeli, Lucas Hosseini, and Hervé arXiv preprint Jégou. 2024. The faiss library. arXiv:2401.08281. Derar Eleyan, Abed Othman, and Amna Eleyan. 2020. Enhancing software comments readability using flesch reading ease score. Information, 11:430. Wenqi Fan, Yujuan Ding, Liangbo Ning, Shijie Wang, Hengyun Li, Dawei Yin, Tat-Seng Chua, and Qing Li. 2024. survey on rag meeting llms: Towards retrieval-augmented large language models. In Proceedings of the 30th ACM SIGKDD Conference on Knowledge Discovery and Data Mining, pages 6491 6501. Thomas François. 2015. When readability meets computational linguistics: new paradigm in readability. Revue française de linguistique appliquée, 20:7997. 9 Evgeniy Gabrilovich and Shaul Markovitch. 2009. Wikipedia-based semantic interpretation for natural language processing. Journal of Artificial Intelligence Research, 34:443498. Yunfan Gao, Yun Xiong, Xinyu Gao, Kangxiang Jia, Jinliu Pan, Yuxi Bi, Yi Dai, Jiawei Sun, and Haofen Wang. 2023. Retrieval-augmented generation for large language models: survey. arXiv preprint arXiv:2312.10997. GeminiTeam. 2023. Gemini: family of highly capable multimodal models. Mingmeng Geng, Caixi Chen, Yanru Wu, Dongping Chen, Yao Wan, and Pan Zhou. 2024. The impact of large language models in academia: from writing to speaking. arXiv preprint arXiv:2409.13686. Mingmeng Geng and Roberto Trotta. 2024. Is chatgpt transforming academics writing style? arXiv preprint arXiv:2404.08627. Mingmeng Geng and Roberto Trotta. 2025. Human-llm coevolution: Evidence from academic writing. arXiv preprint arXiv:2502.09606. Jim Giles. 2005. Special report internet encyclopaedias go head to head. nature, 438:900901. Naman Goyal, Cynthia Gao, Vishrav Chaudhary, PengJen Chen, Guillaume Wenzek, Da Ju, Sanjana Krishnan, MarcAurelio Ranzato, Francisco Guzmán, and Angela Fan. 2022. The flores-101 evaluation benchmark for low-resource and multilingual machine translation. Transactions of the Association for Computational Linguistics, 10:522538. Yufang Hou, Alessandra Pascale, Javier CarnereroCano, Tigran Tchrakian, Radu Marinescu, Elizabeth Daly, Inkit Padhi, and Prasanna Sattigeri. 2024. Wikicontradict: benchmark for evaluating llms on realworld knowledge conflicts from wikipedia. arXiv preprint arXiv:2406.13805. Benedetta Iavarone, Dominique Brunato, Felice DellOrletta, et al. 2021. Sentence complexity in context. In CMCL 2021-Workshop on Cognitive Modeling and Computational Linguistics, Proceedings, pages 186199. Association for Computational Linguistics (ACL). Isaac Johnson, Guosheng Feng, Robert West, et al. Summarizing and explainarXiv preprint Edisum: 2024a. ing wikipedia edits at scale. arXiv:2404.03428. Isaac Johnson, Lucie-Aimée Kaffee, and Miriam Redi. 2024b. Wikimedia data for ai: review of wikimedia datasets for nlp tasks and ai-assisted editing. arXiv preprint arXiv:2410.08918. Aniket Kittur and Robert Kraut. 2008. Harnessing the wisdom of crowds in wikipedia: quality through coordination. In Proceedings of the 2008 ACM conference on Computer supported cooperative work, pages 3746. Kayvan Kousha and Mike Thelwall. 2017. Are wikipedia citations important evidence of the impact of scholarly articles and books? Journal of the Association for Information Science and Technology, 68:762779. Patrick Lewis, Ethan Perez, Aleksandra Piktus, Fabio Petroni, Vladimir Karpukhin, Naman Goyal, Heinrich Küttler, Mike Lewis, Wen-tau Yih, Tim Rocktäschel, et al. 2020. Retrieval-augmented generation for knowledge-intensive nlp tasks. Advances in Neural Information Processing Systems, 33:94599474. Weixin Liang, Zachary Izzo, Yaohui Zhang, Haley Lepp, Hancheng Cao, Xuandong Zhao, Lingjiao Chen, Haotian Ye, Sheng Liu, Zhi Huang, et al. 2024. Monitoring ai-modified content at scale: case study on the impact of chatgpt on ai conference peer reviews. arXiv preprint arXiv:2403.07183. Lucie Lucie-Aimée, Angela Fan, Tajuddeen Gwadabe, Isaac Johnson, Fabio Petroni, and Daniel Van Strien. 2024. Proceedings of the first workshop on advancing natural language processing for wikipedia. In Proceedings of the First Workshop on Advancing Natural Language Processing for Wikipedia. Connor McMahon, Isaac Johnson, and Brent Hecht. 2017. The substantial interdependence of wikipedia and google: case study on the relationship between peer production communities and information technologies. In Proceedings of the International AAAI Conference on Web and Social Media, volume 11, pages 142151. Manish Mehta, Hasani Swindell, Robert Westermann, James Rosneck, and Sean Lynch. 2018. Assessing the readability of online information about hip arthroscopy. Arthroscopy: The Journal of Arthroscopic & Related Surgery, 34:21422149. Rada Mihalcea and Andras Csomai. 2007. Wikify! linking documents to encyclopedic knowledge. In Proceedings of the sixteenth ACM conference on Conference on information and knowledge management, pages 233242. Nandana Mihindukulasooriya, Sanju Tiwari, Daniil Dobriy, Finn Årup Nielsen, Tek Raj Chhetri, and Axel Polleres. 2024. Scholarly wikidata: Population and exploration of conference data in wikidata using llms. In International Conference on Knowledge Engineering and Knowledge Management, pages 243259. Springer. Pedro Miguel Moás and Carla Teixeira Lopes. 2023. Automatic quality assessment of wikipedia articlesa systematic literature review. ACM Computing Surveys, 56:137. Amr Mohamed, Mingmeng Geng, Michalis Vazirgiannis, and Guokan Shang. 2025. Llm as broken telephone: Iterative generation distorts information. arXiv preprint arXiv:2502.20258. 10 Roberto Navigli and Simone Paolo Ponzetto. 2010. Babelnet: Building very large multilingual semantic network. In Proceedings of the 48th annual meeting of the association for computational linguistics, pages 216225. Priti Patel, Ian Hoppe, Naveen Ahuja, and Frank Ciminello. 2011. Analysis of comprehensibility of patient information regarding complex craniofacial conditions. Journal of Craniofacial Surgery, 22:11791182. Yiwen Peng, Thomas Bonald, and Mehwish Alam. 2024. Refining wikidata taxonomy using large language models. In Proceedings of the 33rd ACM International Conference on Information and Knowledge Management, pages 53955399. Tiziano Piccardi, Miriam Redi, Giovanni Colavizza, and Robert West. 2021. On the value of wikipedia as gateway to the web. In Proceedings of the Web Conference 2021, pages 249260. Matt Post. 2018. call for clarity in reporting bleu scores. arXiv preprint arXiv:1804.08771. Neal Reeves, Wenjie Yin, Elena Simperl, and Miriam Redi. 2024. \" the death of wikipedia?\"exploring the impact of chatgpt on wikipedia engagement. arXiv preprint arXiv:2405.10205. Ricardo Rei, Craig Stewart, Ana Farinha, and Alon Lavie. 2020. Comet: neural framework for mt evaluation. arXiv preprint arXiv:2009.09025. Sina Semnani, Violet Yao, Heidi Zhang, and Monica Lam. 2023. Wikichat: Stopping the hallucination of large language model chatbots by few-shot grounding on wikipedia. arXiv preprint arXiv:2305.14292. Yijia Shao, Yucheng Jiang, Theodore Kanell, Peter Xu, Omar Khattab, and Monica Lam. 2024. Assisting in writing wikipedia-like articles from scratch with large language models. arXiv preprint arXiv:2402.14207. Philipp Singer, Florian Lemmerich, Robert West, Leila Zia, Ellery Wulczyn, Markus Strohmaier, and Jure Leskovec. 2017. Why we read wikipedia. In Proceedings of the 26th international conference on world wide web, pages 15911600. Michael Skarlinski, Sam Cox, Jon Laurent, James Braza, Michaela Hinks, Michael Hammerling, Manvitha Ponnapati, Samuel Rodriques, and Andrew White. 2024. Language agents achieve superhuman synthesis of scientific knowledge. arXiv preprint arXiv:2409.13740. Marina Solnyshkina, Radif Zamaletdinov, Ludmila Gorodetskaya, and Azat Gabitov. 2017. Evaluating text complexity and flesch-kincaid grade level. Journal of social studies education research, 8:238248. Michael Strube and Simone Paolo Ponzetto. 2006. Wikirelate! computing semantic relatedness using wikipedia. In AAAI, volume 6, pages 14191424. Damian Swieczkowski and Sławomir Kułacz. 2021. The use of the gunning fog index to evaluate the readability of polish and english drug leaflets in the context of health literacy challenges in medical linguistics: An exploratory study. Cardiology Journal, 28:627631. Neil Thompson and Douglas Hanley. 2018. Science is shaped by wikipedia: evidence from randomized control trial. SSRN. Jörg Tiedemann, Mikko Aulamo, Daria Bakshandaeva, Michele Boggia, Stig-Arne Grönroos, Tommi Nieminen, Alessandro Raganato Yves Scherrer, Raul Vazquez, and Sami Virpioja. 2023. Democratizing neural machine translation with OPUS-MT. Language Resources and Evaluation, pages 713755. Jörg Tiedemann and Santhosh Thottingal. 2020. OPUSMT Building open translation services for the World. In Proceedings of the 22nd Annual Conferenec of the European Association for Machine Translation (EAMT), Lisbon, Portugal. Matthew Vetter, Jialei Jiang, and Zachary McDowell. 2025. An endangered species: how llms threaten wikipedias sustainability. AI & SOCIETY, pages 114. Nicholas Vincent, Isaac Johnson, and Brent Hecht. 2018. Examining wikipedia with broader lens: Quantifying the value of wikipedias relationships with other large-scale online communities. In Proceedings of the 2018 CHI Conference on Human Factors in Computing Systems, pages 113. Christian Wagner and Ling Jiang. 2025. Death by ai: Will large language models diminish wikipedia? Journal of the Association for Information Science and Technology. Yuxia Wang, Artem Shelmanov, Jonibek Mansurov, Akim Tsvigun, Vladislav Mikhailov, Rui Xing, Zhuohan Xie, Jiahui Geng, Giovanni Puccetti, Ekaterina Artemova, et al. 2025. Genai content detection task 1: English and multilingual machinearXiv generated text detection: Ai vs. human. preprint arXiv:2501.11012. Anna Wróblewska, Marceli Korbin, Yoed Kenett, Daniel Dan, Maria Ganzha, and Marcin Paprzycki. 2025. Applying text mining to analyze human question asking in creativity research. arXiv preprint arXiv:2501.02090. Junchao Wu, Shu Yang, Runzhe Zhan, Yulin Yuan, Lidia Sam Chao, and Derek Fai Wong. 2025. survey on llm-generated text detection: Necessity, methods, and future directions. Computational Linguistics, pages 166. 11 Linting Xue, Noah Constant, Adam Roberts, Mihir Kale, Rami Al-Rfou, Aditya Siddhant, Aditya Barua, and Colin Raffel. 2021. mT5: massively multilingual pre-trained text-to-text transformer. In Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 483498, Online. Association for Computational Linguistics. Qiyuan Yang, Pengda Wang, Luke Plonsky, Frederick Oswald, and Hanjie Chen. 2024. From babbling to fluency: Evaluating the evolution of language models in terms of human language acquisition. arXiv preprint arXiv:2410.13259. Torsten Zesch, Christof Müller, and Iryna Gurevych. 2008. Extracting lexical semantic knowledge from wikipedia and wiktionary. In LREC, volume 8, pages 16461652. Jiebin Zhang, Yu Eugene, Qinyu Chen, Chenhao Xiong, Dawei Zhu, Han Qian, Mingbo Song, Weimin Xiong, Xiaoguang Li, Qun Liu, et al. 2025a. Wikigenbench: Exploring full-length wikipedia generation under real-world scenario. In Proceedings of the 31st International Conference on Computational Linguistics, pages 51915210. Qihui Zhang, Chujie Gao, Dongping Chen, Yue Huang, Yixin Huang, Zhenyang Sun, Shilin Zhang, Weiye Li, Zhengyan Fu, Yao Wan, and Lichao Sun. 2024. LLM-as-a-coauthor: Can mixed human-written and In Findings machine-generated text be detected? of the Association for Computational Linguistics: NAACL 2024, pages 409436, Mexico City, Mexico. Association for Computational Linguistics. Yueheng Zhang, Xiaoyuan Liu, Yiyou Sun, Atheer Alharbi, Hend Alzahrani, Basel Alomair, and Dawn Song. 2025b. Can llms design good questions based on context? arXiv preprint arXiv:2501.03491."
        },
        {
            "title": "A Data Collection and Processing",
            "content": "The detailed classification in Wikipedia poses problem in our data crawling process: When iteratively querying deeper subcategories without limit, the retrieved pages may become less relevant to the original topic (i.e., the root category). To address this issue, we select an appropriate crawl depth for each category to balance the number of pages with their topical relevance, as shown in Table 4. We also exclude redirect pages, as they do not contain independent content but link to other target pages. After crawling the pages, we clean the data by extracting the plain text and removing irrelevant sections such as References, See also, Further reading, External links, Notes, and Footnotes. To minimize the impact of topicspecific words, only those rank within the top 10,000 in the Google Ngram dataset7 are included in the calculations. For Wikinews, we use the TextExtracts extension8, which provides an API to retrieve plain-text extracts of page content."
        },
        {
            "title": "B LLM Direct Impact",
            "content": "B.1 Page views The ten categories in our dataset each exhibit unique participation patterns, making comparisons both within and between categories quite challenging. To address this issue, we apply the inverse hyperbolic sine (IHS) function to standardize the page view across different categories. We also calculate the page views using the arithmetic mean. Figure 9 illustrates the average page views across ten categories. We present an additional result excluding data from Featured Articles and Simple Articles to better compare other categories. B.2 Word frequency We present additional experiment results for word frequency in Figure 10. The word additionally has increased more rapidly among almost all categories since 2024, the year GPT-4 (Achiam et al., 2023) and Gemini (GeminiTeam, 2023) released. B.3 LLM simulations We use GPT-4o-mini to revise the January 1, 2022, versions of Featured Articles to construct word fre7Google Ngram dataset: https://www.kaggle.com/ datasets/wheelercode/english-word-frequency-list 8TextExtracts extension: https://www.mediawiki.org/ wiki/Extension:TextExtracts#query+extracts 13 Figure 9: Page views across different categories. Figure 10: Word frequency evolution for word additionally from 2020 to 2025. quency data reflecting the impact of large language models (LLMs). This choice is based on the assumption that Featured Articles are less likely to be affected by LLMs, given their rigorous review processes and ongoing manual maintenance. To reduce errors caused by incomplete data cleaning, we extract only the first section of each Featured Article for revision. Also, some responses are filtered due to the prompt triggering Azure OpenAIs content moderation policy, likely because certain Table 4: Number of Wikipedia articles crawled per category. Category Art Bio Chem CS Math Philo Phy Sports Crawl Depth Number of Pages 4 57,028 4 44,617 5 53,282 5 59,097 5 47, 5 33,596 5 40,986 4 53,900 Wikipedia pages contain violent content. Therefore, these pages are excluded from our analysis. Selecting the appropriate word combinations to estimate the impact of LLMs is crucial. On one hand, by setting threshold for , we ensure that the target vocabulary appears frequently in the corpus. On the other hand, by setting threshold for ˆr, we ensure that these words exhibit significant frequency change after being processed by the LLM. For the threshold, we propose two strategies: First, the target words should frequently appear in the first section of Featured Articles, as we use this part of the articles for LLM refinement when estimating ˆr; second, the target words should frequently appear in the target corpus. For the first strategy, when calculating the impact of the LLM on different pages, the selected vocabulary combination remains the same. For the second strategy, the influence on pages of different categories will be estimated using the vocabulary combination corresponding to each category. B.3.1 Featured Articles and Same Words We use the first section of Featured Articles to request revisions from GPT-4o-mini and calculate the estimated change rate for each word. Then, we select words that are frequently used in the Featured Articles and show significant changes in frequency after LLM simulation. This approach allows us to apply the same word combinations to estimate Wikipedia pages across different categories. We change the threshold of and ˆr to get more reliable and stable estimation. 1 : 5000, 7000, 9000, 11000, 13000, 15000 ˆr: 0.14, 0.15, 0.16, 0.17, 0.18, 0.19, 0.20, 0.21 (corresponding values of ˆr+1 ˆr2 ) For example, when we take 1 < 5000 and ˆr+1 ˆr2 > 0.21, the words that satisfy the conditions are: making, end, primarily, times, next, remained, however, placed, people, much, re, features, success, both, down, significant, appeared, formed, sent, great, have, numerous, but, again, throughout, can, country, very, us, book, initially, based, what, result, because, game, than, remains, their, once, though, take, described, across, post, went, use, number, successful, building, win, forced, run, located, show, combat, caused, elements, victory, given, today, almost, while, is, often, following, died, no, make, where, be, popular, out, upon, soon, left, along, wrote, total, not, up, were, work, helped, operations, written, commonly, then, action, long, little, built, worked, like, created, awarded, there, games, although, killed, attack, opened, having, lived, play, main, few, large, its, important, particularly, considered, p, region, established, coins, had, major, moved, more, made, players, these, entered, spent, fought, support, parts, various, despite, shortly, part, taken, been, failed, came, sometimes, launched, among, during, just, mostly, so, this, office, different, player, struck, forest, was, called, forces, would, within, become, story, saw, last, side, generally, short, brought, ended, won, appointed, live, other, best, when, due, introduced, largely, role, men, form, position, served, title, never, including, leading, way, common, are, man, became, used, about, as. B.3.2 Featured Articles and Different Words Unlike the previous strategy which applies the same words across all categories of Wikipedia pages, here we estimate each category using distinct sets of words. For instance, when selecting words for pages in Computer Science (CS), we choose words that frequently appear in CS pages and show relatively higher change rate after LLM simulation. As result, each category will have its own unique set of words to estimate the impact of LLMs. 1 : 5000, 7000, 9000, 11000, 13000, 15000 ˆr: 0.14, 0.15, 0.16, 0.17, 0.18, 0.19, 0.20, 0.21 (corresponding values of ˆr+1 ˆr2 ) For example, when we take 1 < 9000 and ˆr+1 ˆr2 > 0.15, 635 words in CS pages meet these conditions, compared to 496 words in Art pages. 14 Figure 11: Impact of LLMs on Wikipedia pages, estimated based on simulations of Featured Articles, using the same word combinations across each category. Figure 12: Impact of LLMs on Wikipedia pages, estimated based on simulations of Simple Articles, using the same word combinations across each category. B.3.3 Simple Articles and Same Words The only difference here is that we use Simple Articles as the corpus for the LLM simulation process. 1 : 1000, 3000, 5000, 7000, 9000, 11000, 13000 ˆr: 0.07, 0.09, 0.11, 0.13, 0.15, 0.17, 0.19, 0.21, 0.23, 0.25, 0.27, 0.29 (corresponding values of ˆr+1 ˆr2 ) B.3.4 Simple Articles and Different Words 1 : 2000, 2500, 3000, 3500, 4000, 4500, 5000 ˆr: 0.11, 0.13, 0.15, 0.17, 0.19, 0.21, 0.23, 0.25 (corresponding values of ˆr+1 ˆr2 )"
        },
        {
            "title": "C Machine Translation",
            "content": "C.1 Exception Handling Some API calls in our code returned an openai.BadRequestError with error code 400, indicating that Azure OpenAIs content management policies flagged the prompts for potentially violating content. Also, Some translations returned null values. These cases were excluded from scoring and ignored in the evaluation. C.2 Languages These are the 12 languages in our benchmarks: English (eng-Latn-stan1293) Modern Standard Arabic (arb-Arab-stan1318) Mandarin (cmn-Hans-beij1234) German (deu-Latn-stan1295) French (fra-Latn-stan1290) Hindi (hin-Deva-hind1269) Italian (ita-Latn-ital1282) Japanese (jpn-Jpan-nucl1643) Korean (kor-Hang-kore1280) Brazilian Portuguese (por-Latn-braz1246) Russian (rus-Cyrl-russ1263) Latin American Spanish (spa-Latn-amer1254) 15 such verbs than GPT. Moreover, marginal decline in the usage of these verbs is observed in actual Wikipedia pages from 2020 to 2025. Lexical Diversity: As shown in Figure 15, revised articles display slightly higher CTTR, with texts revised by GPT exhibiting greater lexical diversity than those revised by Gemini. When tasked with generating wiki-style articles, GPT achieves the highest lexical diversity. Over time, the vocabulary used across different Wikipedia categories has become increasingly varied. Long Words: Figure 16 indicates that LLMs tend to increase the usage of long words, with Gemini surpassing GPT. From 2020 to 2025, the rate of long words has remained relatively stable across Wikipedia categories. Parts of Speech: Figure 17 shows that LLMs lead to slight increase in the use of nouns, accompanied by corresponding decrease in pronouns. Prepositions and conjunctions remain stable after LLM simulation. On Wikipedia pages, the proportion of prepositions has steadily increased, while the proportions of other parts of speech have remained stable. Syllables: Figure 18 illustrates that the proportion of one-syllable words declines in articles revised by LLMs, with Gemini employing even fewer such words. Meanwhile, the average syllables per word increase, suggesting preference for polysyllabic words by LLMs. However, these two metrics remain relatively stable across different Wikipedia categories. Figure 13: Impact of LLMs on Wikipedia pages, estimated based on simulations of Simple Articles, using different word combinations across each category. Table 5: Google-T5 results on some metrics. BLEU ChrF COMET G DE FR 71.52 68.33 80.09 65.93 84.27 87. 93.62 86.32 83.91 85.49 85.63 87.01 C.3 More results For Google-T5 shown in Table 5, German (DE) initially has BLEU score of 30.24, which rises to 44.18 in the GPT-processed benchmark, marking another substantial improvement."
        },
        {
            "title": "D Linguistic Style",
            "content": "In this section, we analyze the influence of LLMs on linguistic style among different categories in two dimensions: the first section and full-text content. D.1 Word Level To Be Verbs : Figure 14 illustrates that LLMs significantly reduce the usage of To Be verbs (e.g., replacing is important with demonstrates significance), with Gemini using fewer 16 Figure 14: To Be verbs are reduced by LLMs, with Gemini using fewer than GPT. slight decline in their usage is also observed in Wikipedia pages from 2020 to 2025. Figure 15: CTTR is slightly higher in revised articles, with GPT showing greater lexical diversity than Gemini. Vocabulary variation has increased across Wikipedia categories over time. Figure 16: Long words are used more frequently by LLMs, with Gemini surpassing GPT. Their rate has remained stable across Wikipedia categories from 2020 to 2025. D.2 Sentence Level Sentence Length: Figure 19 shows that both the average sentence length and the proportion of long sentences show significant increase after being processed by the LLM. Additionally, the period from 2020 to 2025 has seen notable rise in these two metrics across Wikipedia pages, indicating trend towards longer sentence structures. Sentence Complexity: According to figure 20, after revisions by GPT, Simple Articles show an increase in complexity, while Featured Articles exhibit only minor changes. This may suggest that LLMs do not generate sentences at the highest possible complexity, but instead maintain complexity at certain level. For real Wikipedia pages, steady year-on-year increase in these two metrics has been observed, indicating shift towards more complex sentence structures. Pronoun and Article-Initial Sentences: LLMs tend to avoid starting sentences with pronouns (e.g., It) or articles (e.g., The), as shown in figure 21. For example, it might replace The team worked hard to finish the project on time. with Hard work from the team ensured the project was completed on time. However, in real Wikipedia pages, Article-initial sentences have increased, while pronoun-initial (a) (b) (c) (d) (e) (g) (h) (j) (k) (f) (i) (l) Figure 17: Parts of speech distribution, indicating that LLMs slightly increase nouns and decrease pronouns, while prepositions and conjunctions remain stable. On Wikipedia pages, the proportion of prepositions has steadily increased, with other parts of speech remaining stable. sentences remain stable from 2020 to 2025. D.3 Paragraph Level We use Textstat9 to calculate six paragraph metrics. Textstat is an easy-to-use library to calculate statistics from the text. It provides range of functions to analyze readability, sentence length, syllable count, and 9https://github.com/textstat/textstat 18 (a) (b) (d) (e) (c) (f) Figure 18: LLMs show preference for polysyllabic words while reducing the frequency of monosyllabic terms. These two metrics remain relatively stable across different Wikipedia categories. (a) (b) (d) (e) (c) (f) Figure 19: LLMs tend to generate texts with longer sentences, trend that has grown steadily across Wikipedia categories over the years. other important textual features. Through the LLM simulation process, we discover that LLMs tend to generate articles that are harder to read. Figure 22 suggests that the readability of Wikipedia pages has shown only slight variation over the years and does not appear to be influenced by LLMs at this stage. 19 (a) (b) (d) (e) (c) (f) Figure 20: Average parse tree depth and clause proportion remain relatively stable after simulation. In contrast, for actual Wikipedia pages, gradual year-over-year increase in these two metrics has been observed, indicating shift towards more complex sentence structures. (a) (b) (d) (e) (c) (f) Figure 21: The proportions of sentences starting with specific parts of speech, indicating that LLMs tend to avoid beginning sentences with pronouns or articles. D.4 RAG Results Tables 6, 7, 8, and 9 present RAG results where Null Output is counted as 0 accuracy, while Tables 10, 11, 12, and 13 display results with Null Output counted as 0.25 accuracy. 20 (a) Change in Dale-Chall readability. (b) Change in Automated Readability Index. (c) Change in Flesch Reading Ease. (d) Change in Coleman-Liau Index. (e) Change in Gunning Fog Index. Figure 22: Changes in readability metrics of Wikipedia pages. Table 6: GPT-4o-mini performance on RAG task (problem generated by GPT). Year Direct Ask RAG RAG (GPT) RAG (Gem) Full (Original) Full (GPT) Full (Gem) 2020 2021 2022 2023 75.86% 71.74% 80.00% 77.46% 66.67% 85.34% 86.31% 89.49% 87.09% 83.33% 85.63% 88.96% 87.18% 87.09% 84.58% 79.60% 79.69% 84.10% 83.33% 82.08% 95.98% 96.03% 95.64% 96.01% 95.83% 95.40% 96.03% 95.64% 94.84% 95.83% 87.36% 88.08% 88.97% 87.09% 88.75% Table 7: GPT-4o-mini performance on RAG task (problem generated by Gemini). Year Direct Ask RAG RAG (GPT) RAG (Gem) Full (Original) Full (GPT) Full (Gem) 2020 2021 2022 2023 2024 66.95% 64.68% 73.54% 69.95% 61.25% 82.76% 81.90% 86.01% 82.39% 79.58% 82.47% 82.34% 85.75% 83.10% 75.42% 75.86% 75.06% 78.88% 78.40% 75.42% 93.68% 94.04% 94.66% 92.49% 92.92% 91.38% 93.82% 93.89% 92.25% 92.92% 84.20% 82.12% 83.21% 83.57% 82.92%"
        },
        {
            "title": "E Additional Experiment Results of RAG",
            "content": "E.1 More Information Table 14 presents the LLM parameters employed in RAG simulations, such as the knowledge cutoff date, temperature, and top-p. 21 Table 8: GPT-3.5 Performance on RAG task (problem generated by GPT). Year Direct Ask RAG RAG (GPT) RAG (Gem) Full (Original) Full (GPT) Full (Gem) 2020 2021 2022 2023 68.68% 67.11% 70.26% 64.08% 60.42% 77.59% 79.25% 82.82% 74.88% 77.92% 78.16% 79.25% 80.77% 76.06% 75.83% 74.14% 74.17% 78.97% 71.83% 75.83% 86.21% 87.42% 88.46% 86.85% 92.08% 87.93% 88.30% 90.51% 88.73% 89.17% 87.36% 84.99% 88.46% 84.27% 83.75% Table 9: GPT-3.5 Performance on RAG task (problem generated by Gemini). Year Direct Ask RAG RAG (GPT) RAG (Gem) Full (Original) Full (GPT) Full (Gem) 2020 2021 2022 2023 2024 66.95% 58.72% 62.09% 56.57% 55.00% 72.70% 73.73% 74.05% 73.24% 71.67% 72.41% 71.74% 72.77% 74.88% 70.00% 68.97% 68.21% 69.47% 67.14% 65.00% 77.87% 81.02% 82.44% 77.46% 77.92% 79.31% 79.47% 82.19% 79.58% 80.42% 77.59% 74.17% 80.41% 74.65% 76.67% Table 10: GPT-4o-mini performance on RAG task (problem generated by GPT), Null Output is counted as 0.25. Year Direct Ask RAG RAG (GPT) RAG (Gem) Full (Original) Full (GPT) Full (Gem) 2020 2021 2022 2023 2024 75.86% 71.74% 80.00% 77.52% 67.60% 85.76% 86.53% 89.87% 87.44% 83.75% 86.28% 89.24% 88.14% 87.32% 85.21% 80.03% 80.08% 84.55% 83.69% 82.92% 96.19% 96.25% 95.90% 96.24% 96.15% 95.76% 96.36% 95.96% 95.18% 96.15% 89.15% 89.85% 90.51% 89.14% 90.10% Table 11: GPT-4o-mini performance on RAG task (problem generated by Gemini) , Null Output is counted as 0.25. Year Direct Ask RAG RAG (GPT) RAG (Gem) Full (Original) Full (GPT) Full (Gem) 2020 2021 2022 2023 67.53% 65.01% 73.98% 70.42% 62.50% 82.90% 81.95% 86.20% 82.63% 80.00% 82.54% 82.40% 85.94% 83.39% 75.83% 76.29% 75.22% 79.07% 78.64% 75.94% 93.75% 94.21% 94.85% 92.72% 93.65% 91.45% 93.87% 94.08% 92.55% 93.33% 85.70% 83.83% 84.80% 85.27% 85.00% Table 12: GPT-3.5 performance on RAG task (problem generated by GPT) , Null Output is counted as 0.25. Year Direct Ask RAG RAG (GPT) RAG (Gem) Full (Original) Full (GPT) Full (Gem) 2020 2021 2022 2023 2024 68.68% 67.11% 70.26% 64.08% 60.42% 77.59% 79.25% 82.82% 74.88% 77.92% 78.16% 79.25% 80.77% 76.06% 75.83% 74.14% 74.17% 78.97% 71.83% 75.83% 86.35% 87.42% 88.59% 86.91% 92.29% 87.93% 88.30% 90.51% 88.79% 89.17% 87.36% 85.15% 88.65% 84.51% 83.75% Table 13: GPT-3.5 performance on RAG task (problem generated by Gemini) , Null Output is counted as 0.25. Year Direct Ask RAG RAG (GPT) RAG (Gem) Full (Original) Full (GPT) Full (Gem) 2020 2021 2022 2023 2024 66.95% 58.72% 62.28% 56.57% 55.00% 72.70% 73.79% 74.11% 73.24% 71.67% 72.49% 71.74% 72.84% 74.88% 70.00% 68.97% 68.21% 69.53% 67.14% 65.00% 77.95% 81.13% 82.44% 77.70% 78.12% 79.31% 79.53% 82.25% 79.69% 80.52% 77.66% 74.34% 80.47% 74.82% 76.67% 22 Table 14: LLM parameters Used in RAG simulations. Models Knowledge Cutoff Temperature Top-p GPT-3.5 GPT-4o-mini Gemini-1.5-flash September 2021 October 2023 May 2024 1.0 1.0 1. 1.0 1.0 0.95 Table 15: Annual Number of Questions Generated by Different LLMs. Year 2020 2021 2023 2024 Number of GPT genertated Questions Number of Gemini genertated Question 348 348 453 453 390 426 426 240 240 E.2 Case Study Figures 23, 24, 25, 26 and 27 present cases where answers are accurate using the original texts but become inaccurate using LLM-revised texts. Title: NASA says object that hit Florida home is from International Space Stationa Example 1 - Keyword Replacement Question: On which date did NASA release pallet containing old nickelhydride batteries from the International Space Station? A) March 8, 2021 B) March 11, 2021 C) April 22, 2024 D) March 8, 2020 Original Context: . . . pallet containing old nickelhydride batteries was released from the ISS on March 11, 2021, after new batteries were installed. . . . LLM Revised Context: . . . The debris, part of 5,800-lb cargo pallet released from the ISS in March 2021, unexpectedly survived atmospheric re-entry. . . . ahttps://en.wikinews.org/wiki/NASA_says_object_that_hit_Florida_home_is_from_International_ Space_Station Figure 23: The news revised by LLMs omits key information about the specific date NASA released the pallet, causing the RAG system unable to determine the correct date and ultimately selecting A. 23 Example 2 - Keyword Replacement Title: Latin American expedition of Viktor Pinchuk: meeting with the traveler took place in Yaltaa Question: What hobby involves collecting recordings of ethnic performers and is practiced by Viktor Pinchuk? A) Philophony B) Ethnomusicology C) Hobo tourism D) Cultural preservation Original Context: . . . From every trip or an expedition, Viktor Pinchuk brings CDs with recordings of ethnic performers; the travelers collection has already accumulated significant number of them (not counting several hundred digital editions of world-famous musicians). The hobby is called philophony, and the subject of it is called philophonist. . . . LLM Revised Context: . . . Pinchuk, self-described philophonist, has amassed hundreds of CDs and digital recordings of ethnic and world music. . . . ahttps://en.wikinews.org/wiki/Latin_American_expedition_of_Viktor_Pinchuk:_meeting_with_ the_traveler_took_place_in_Yalta Figure 24: The RAG system mistakenly selects when using the LLM-revised text because the revision omits key details, such as the explicit mention of the hobbys name, philophony. Example 3 - Keyword Replacement Title: New Zealand Navy ship HMNZS Manawanui capsizes one nautical mile from shorea Question: What was the name of the Royal New Zealand Air Force aircraft that assisted in the evacuation of the crew from HMNZS Manawanui? A) Boeing P-8 Poseidon B) Airbus A320 C) Lockheed Martin C-130J D) Boeing 737, Original Context: . . . They were rescued with assistance from the Rescue Coordination Centre (RCCNZ) and Royal New Zealand Airforce Boeing P-8 Poseidon. . . . LLM Revised Context: . . . All 75 crew were safely evacuated with assistance from the Rescue Coordination Centre and the Royal New Zealand Air Force. ahttps://en.wikinews.org/wiki/New_Zealand_Navy_ship_HMNZS_Manawanui_capsizes_one_nautical_ mile_from_shore Figure 25: LLMs omit key information, such as the aircrafts name. 24 Example 4 - Abbreviation Ambiguity Misleading Title: At least 20 die in Odesa in Russian missile strike, Ukraine reportsa Question: How many employees of the State Emergency Service of Ukraine were reported as victims of the missile strikes in Odesa? A) One B) Five C) Seven D) Ten Original Context: . . . Among the dead are an employee of the State Service of Ukraine for Emergency Situations (SSES) and paramedic. . . . Among the victims are seven employees of the State Emergency Service. . . . LLM Revised Context: . . . Among the deceased are staff member of the State Service of Ukraine for Emergency Situations (SSES) and paramedic. . . . Seven SSES personnel were among the injured, and medical workers also sustained injuries. . . . ahttps://en.wikinews.org/wiki/At_least_20_die_in_Odesa_in_Russian_missile_strike,_Ukraine_ reports Figure 26: The original text use the full name seven employees of the State Emergency Service, allowing the RAG system to correctly select C. However, the LLMs revised text abbreviated this to seven SSES personnel, causing the RAG system to incorrectly choose A. Title: Arizona bans abortion for genetic abnormalitiesa Example 5 - Introduction of Modifiers Question: What does Senate Bill 1457 in Arizona classify as Class 6 felony? A) Seeking or performing an abortion because of severe fetal abnormality B) Seeking or performing an abortion due to the presence of genetic abnormality in the child C) Distributing abortion-inducing drugs via courier D) Soliciting funds for an abortion Original Context: . . . The bill makes it Class 6 felony, the least severe, to seek or perform an abortion because of genetic abnormality of the child, defined as the presence or presumed presence of an abnormal gene expression in an unborn child, but not severe fetal abnormality considered incompatible with life. . . . LLM Revised Context: . . . Arizona Governor Doug Ducey signed Senate Bill 1457 into law on Tuesday, effectively banning abortions sought solely due to fetal genetic abnormalities. The bill, which passed the Republican-controlled legislature after twice stalling and undergoing amendments to secure necessary votes, classifies seeking or performing such abortions as Class 6 felony. . . . ahttps://en.wikinews.org/wiki/Arizona_bans_abortion_for_genetic_abnormalities Figure 27: Although both the original and revised text explicitly excludes severe fetal abnormalities, the revised text change genetic abnormality to fetal genetic abnormalities, which leads LLMs to misinterpret the information. As result, LLMs mistakenly select based on the revised text."
        }
    ],
    "affiliations": [
        "Huazhong University of Science and Technology",
        "International School for Advanced Studies (SISSA)"
    ]
}