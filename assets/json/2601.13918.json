{
    "paper_title": "AgentEHR: Advancing Autonomous Clinical Decision-Making via Retrospective Summarization",
    "authors": [
        "Yusheng Liao",
        "Chuan Xuan",
        "Yutong Cai",
        "Lina Yang",
        "Zhe Chen",
        "Yanfeng Wang",
        "Yu Wang"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Large Language Models have demonstrated profound utility in the medical domain. However, their application to autonomous Electronic Health Records~(EHRs) navigation remains constrained by a reliance on curated inputs and simplified retrieval tasks. To bridge the gap between idealized experimental settings and realistic clinical environments, we present AgentEHR. This benchmark challenges agents to execute complex decision-making tasks, such as diagnosis and treatment planning, requiring long-range interactive reasoning directly within raw and high-noise databases. In tackling these tasks, we identify that existing summarization methods inevitably suffer from critical information loss and fractured reasoning continuity. To address this, we propose RetroSum, a novel framework that unifies a retrospective summarization mechanism with an evolving experience strategy. By dynamically re-evaluating interaction history, the retrospective mechanism prevents long-context information loss and ensures unbroken logical coherence. Additionally, the evolving strategy bridges the domain gap by retrieving accumulated experience from a memory bank. Extensive empirical evaluations demonstrate that RetroSum achieves performance gains of up to 29.16% over competitive baselines, while significantly decreasing total interaction errors by up to 92.3%."
        },
        {
            "title": "Start",
            "content": "AGENTEHR: Advancing Autonomous Clinical Decision-Making via Retrospective Summarization Yusheng Liao*, Chuan Xuan,, Yutong Cai, Lina Yang, Zhe Chen, Yanfeng Wang,, Yu Wang,, Shanghai Jiao Tong University Shanghai Artificial Intelligence Laboratory {liao20160907,xuanchuan,iautng123,alina_yln,chenzhe2018, wangyanfeng622,yuwangsjtu}@sjtu.edu.cn 6 2 0 2 0 2 ] . [ 1 8 1 9 3 1 . 1 0 6 2 : r a"
        },
        {
            "title": "Abstract",
            "content": "Large Language Models have demonstrated profound utility in the medical domain. However, their application to autonomous Electronic Health Records (EHRs) navigation remains constrained by reliance on curated inputs and simplified retrieval tasks. To bridge the gap between idealized experimental settings and realistic clinical environments, we present AGENTEHR. This benchmark challenges agents to execute complex decisionmaking tasks, such as diagnosis and treatment planning, requiring long-range interactive reasoning directly within raw and high-noise databases. In tackling these tasks, we identify that existing summarization methods inevitably suffer from critical information loss and fractured reasoning continuity. To address this, we propose RETROSUM, novel framework that unifies retrospective summarization mechanism with an evolving experience strategy. By dynamically re-evaluating interaction history, the retrospective mechanism prevents long-context information loss and ensures unbroken logical coherence. Additionally, the evolving strategy bridges the domain gap by retrieving accumulated experience from memory bank. Extensive empirical evaluations demonstrate that RETROSUM achieves performance gains of up to 29.16% over competitive baselines, while significantly decreasing total interaction errors by up to 92.3%. Our code and datasets are available at https: //github.com/BlueZeros/AgentEHR."
        },
        {
            "title": "Introduction",
            "content": "Large Language Models (LLMs) have demonstrated profound utility in the medical domain (Singhal et al., 2023; Nori et al., 2023; Chen et al., 2023), acting as powerful catalysts ranging from diagnostic report generation (Biswas and *Equal contribution. Corresponding Author Figure 1: Comparison between the previous EHR tasks and the proposed benchmark. Unlike previous EHRAgent task retrieving factual information explicitly present in the EHR (e.g., medication history), AGENTEHR analyzes existing information to predict future clinical decisions, like diagnosis and treatment plans. Talukdar, 2024; Jung et al., 2024) to patient communication (Qiu et al., 2025a,b; Liao et al., 2024). While these highlights underscore the potential of LLMs, effectively deploying them on the complex environment of Electronic Health Records (EHRs) remains significant challenge. Existing research on LLM applications in EHRs has predominantly focused on clinical decisionmaking tasks, such as patient risk prediction (Lin et al., 2025; Fang et al., 2025; Zhu et al., 2024) and intervention strategy recommendation (Liao et al., 2025b; Hager et al., 2024). However, these methodologies often depend on manual curation (Hegselmann et al., 2025; Hager et al., 2024), creating an idealized experimental setting that sidesteps inherent challenges like data noise and long-context processing. Consequently, their applicability in Methods Agent Capacity Agent Methods EHR Application Raw Data Process Info Retrieval Decision Making Long Context Summarization Retrospective Mechanism Experience Accumulation ReSum (Wu et al., 2025) MEM1 (Zhou et al., 2025) Reasoning Bank (Ouyang et al., 2025) ReflecTool (Liao et al., 2025a) EHRAgent (Shi et al., 2024) EHR-MCP (Masayoshi et al., 2025) Med-Copilot (Xu et al., 2025) EHR-R1 (Liao et al., 2025b) RETROSUM (Ours) Table 1: Comparison of previous works and RETROSUM on both agent capacities and methods. realistic clinical scenarios is severely limited. Simultaneously, recent advancements have integrated agent-based systems to facilitate autonomous EHR navigation. Yet, these efforts largely confine the agents role to query rewriting (Lee et al., 2022; Ryu et al., 2024; Wang et al., 2020) or factual information retrieval (Shi et al., 2024; Masayoshi et al., 2025; Lee et al., 2025; Jiang et al., 2025). Such approaches effectively reduce the LLM to sophisticated search interface, failing to fully leverage the models advanced reasoning capabilities for complex clinical analysis. To bridge these gaps, we introduce AGENTEHR, novel benchmark covering six diverse tasks across three real-world EHR subsets. As illustrated in Figure 1, AGENTEHR transcends the factual retrieval focus of prior work. Instead, it demands that agents actively engage in multistep information seeking, synthesize information through clinical reasoning, and finally deliver precise decision-making predictions, such as diagnosis and treatment planning. Therefore, resolving these complex tasks necessitates extensive, multi-step interactions with the EHR system. This process inevitably accumulates substantial amount of redundant information and results in extended interaction histories, posing severe challenge to the models contextual capacity and reasoning integrity. prevailing paradigm to navigate such longhorizon challenges involves the use of vanilla summarization techniques (Wu et al., 2025; Zhou et al., 2025) to distill salient information from redundancy and condense the elongated history. However, fundamental misalignment arises when applying these paradigms to the clinical sphere. Unlike general long-context interaction tasks (e.g., deepresearch) (Team et al., 2025b,a; Chen et al., 2025), EHR analysis is characterized by strong multi-turn correlations, as every piece of retrieved information is intrinsically linked to the same patients physiological state. Consequently, standard unidirectional summarization inevitably severs latent cross-temporal connections, fracturing the reasoning continuity required for precise diagnosis. To address this, we propose RETROSUM, which introduces retrospective mechanism to capture dependencies for unbroken reasoning, alongside an evolving strategy to bridge the domain gap via experience accumulation. As highlighted in Table 1, by adopting the retrospective mechanism on EHRbased decision-making tasks, RETROSUM offers promising strategy to navigate the intricacies of raw EHRs, successfully mitigating the loss of logical fidelity typically seen in clinical long-context reasoning applications. Our main contributions are as follows: Realistic Clinical Benchmark: We present AGENTEHR, the first benchmark designed to bridge the gap between idealized settings and authentic medical scenarios. Unlike prior studies, AGENTEHR establishes new standard by challenging agents to perform multi-step reasoning within raw EHR databases to fulfill clinical decision-making tasks. Retrospective Reasoning Framework: We propose RETROSUM, simple yet effective method engineered to master the intricacies of long-context EHR reasoning tasks. By incorporating retrospective mechanism, our method effectively captures latent correlative information and ensures reasoning continuity. Superior Empirical Performance: Extensive experiments demonstrate that RETROSUM achieves remarkable performance gains of up to 29.16% over existing baselines. This confirms RETROSUMs superior robustness and efficacy in handling the complex dynamics of real-world clinical decision-making."
        },
        {
            "title": "2 AGENTEHR Benchmark",
            "content": "To bridge the gap between current LLM capabilities and the complexities of real-world clinical applications, we introduce AGENTEHR, comprehensive evaluation framework designed for EHR-based interactive reasoning and clinical decision-making interaction. 2.1 Data Construction We construct AGENTEHR based on two widelyused real-world EHR databases: MIMIC-IV (Johnson et al., 2023) and MIMIC-III (Johnson et al., 2016). To rigorously evaluate the generalization and robustness of autonomous agents, we organize these data sources into three distinct experimental subsets (details are shown in Appendix B): MIMIC-IV-Common (In-Distribution) We stratify the MIMIC-IV dataset based on label frequency, selecting cases with the most prevalent conditions to form the Common subset. This serves as the primary In-Distribution (ID) benchmark for assessing standard clinical reasoning capabilities. MIMIC-IV-Rare (Label-Shift OOD) Comprising the long-tail cases from MIMIC-IV, this subset introduces significant distribution shift in the label space. It evaluates the agents ability to handle low-prevalence diseases where parametric knowledge is often weaker. MIMIC-III (Systemic-Shift OOD) We utilize the complete MIMIC-III dataset to represent more challenging systemic shift. Unlike the label-only shift in the Rare subset, MIMIC-III presents fundamental differences in table schema and information density compared to MIMIC-IV. This setting provides the most comprehensive measure of agent methods to heterogeneous EHR environments. 2.2 Clinical Tasks 2.3 Toolbox MCP Server To enable the Agent system to navigate the complex EHR environment efficiently, we design comprehensive Toolbox hosted on Model Context Protocol (MCP) Server. This architecture provides standardized interface for the agent to access over 19 specialized tools, ensuring robust and scalable interaction with the underlying database. The toolbox equips the model with diverse retrieval mechanisms, including temporal filtering, keyword search, fuzzy matching, and direct SQL execution. Detailed specifications of the toolbox can be found in Appendix C."
        },
        {
            "title": "3 Methods",
            "content": "In this section, we first formally define the clinical agent task. We then introduce RETROSUM, novel framework designed to mitigate the information loss and reasoning fragmentation inherent in standard incremental summarization approaches (overview is shown in Figure 2). Finally, we describe the Evolving Optimization strategy. 3.1 Task Formulation The clinical agent task requires an agent system to interact with database to answer complex medical queries. Each query instance is composed of = {p, t, I}, where is the patient identifier, is the reference timestamp, and is clinical instruction. The interaction proceeds in discrete steps {1, . . . , K}. At each step i, the agent observes the current state, which encapsulates the query and the interaction history Hi1 = {(a1, o1), . . . , (ai1, oi1)}. Based on this context, the agent generates an action ai according to its own policy πθ: ai πθ(aiHi1, ) (1) Upon executing action ai, the environment returns an observation oi, which is derived from the EHR database E: oi = E(ai) (2) AGENTEHR encompasses six core clinical tasks: Diagnoses, Labevents, Microbiology, Prescriptions, Procedures, and Transfers. Collectively, these tasks represent the entire lifecycle of patient hospitalization, spanning the critical phases of diagnosis, laboratory examination, and therapeutic intervention. The tuple (ai, oi) is then appended to the history Hi = Hi1 (ai, oi). This process repeats until the agent issues termination action and produces final answer set = {y1, y2, . . . , ym}, consisting of list of items (e.g., specific diagnosis codes or medication names) that satisfy the clinical instruction. Figure 2: Overview of RETROSUM. RETROSUM (right) addresses critical information loss and reasoning interruptions inherent in unidirectional methods like ReSum (left). By incorporating retrospective mechanism to re-evaluate full interaction histories and an evolving mechanism to retrieve specialized strategies from memory, RETROSUM ensures robust long-horizon clinical reasoning and correct decisions. 3.2 RETROSUM Navigating real-world EHRs requires the agent to manage extensive context while identifying complex dependencies across heterogeneous tables. Previous methods, such as ReSum (Wu et al., 2025), typically employ unidirectional summarization, where the history context is compressed incrementally. However, we identify two critical limitations in this paradigm for clinical tasks: (1) Loss of Latent Correlations: EHR data is inherently interconnected. Information retrieved in early turns may initially appear irrelevant but becomes crucial after observing later result. Unidirectional approaches, which rely on local information summarization, often discard these latent factors before their relevance is established via cross-table correlations. (2) Disruption of Reasoning Logic: Forcing the agent to rely exclusively on highly compressed summary disrupts the continuity of multi-turn reasoning logic. The abstraction process often strips away the precise syntactical and numerical details required for the agent to deduce the next logical step. To address these challenges, we propose RETROSUM, which introduces retrospective mechanism. Specifically, our framework operates through two synergistic phases: Retrospective Summarization We define summarization window size w. The sumAt marization process is triggered only at step j, where 0 (mod w). these intervals, the interaction history is conceptually partitioned into the distant history Hdist = {(a1, o1), . . . , (ajw, ojw)} and the recent window Hrec = {(ajw+1, ojw+1), . . . , (aj, oj)}. The Summarizer module M, which shares parameters θ with the agents policy, generates an updated summary Sj by retrospectively analyzing the entire trajectory: Sj = Mθ(Hrec, Sjw, Hdist, ) (3) By conditioning on both Hdist and Hrec, the Summarizer can re-evaluate the importance of past events in light of the most recent findings, effectively capturing latent correlations that were previously ambiguous. Retrospective Inference To prevent the disruption of reasoning logic, we design the actor to operate on history-aware context. Unlike prior methods that replace the history with summary, RETROSUM augments the full interaction history with the latest retrospective summary. For any step i, the entire context ˆHi is updated as: ˆHi = (cid:40) ˆHi1 {Siw} {(ai, oi), Si}, ˆHi1 {(ai, oi)}, if 0 (mod w) otherwise (4) where Siw is the most recent summary. The policy πθ then generates the next action based on this augmented view: ai πθ(ai ˆHi1, ) (5) Evolved Inference During inference, for new patient context, we compute its embedding and retrieve the most similar memory items from B. These retrieved experiences are injected into the framework to guide both modules: This formulation ensures that the actor retains access to the complete raw history Hi1, preserving the integrity of the multi-turn reasoning chain, while serves as high-level cognitive map to guide the models attention toward clinically significant patterns extracted from previous phases. 3.3 Evolving Strategy General-purpose LLMs often lack the specific clinical intuition required to distinguish subtle signals in EHRs or navigate complex table schemas. To bridge this gap, we introduce the evolving strategy for RETROSUM. Rather than updating model parameters, this phase enables the agent to crystallize successful strategies into an external memory bank, allowing it to learn from past trials. Experience Generation For each training instance, the agent generates full interaction trajectory HK, final retrospective summary Sfinal, and predicted answer set . By comparing these outputs against the ground truth , we derive domainspecific experiences using the reflection module Rθ, which shares parameters with the agents policy: Eact = Rθ(HK, Y, ) Esum = Rθ(HK, Sfinal, Y, ) (6) (7) Here, Rθ is prompted to extract procedural heuristics for the Actor (e.g., optimal tool selection) and information salience guidelines for the Summarizer (e.g., critical evidence retained vs. noise compressed). Memory Construction We organize these insights into an Experience Memory Bank B. Each entry is stored as triplet: = {(e, Eact, Esum)i}M i=1 (8) where is the size of the memory and is the representation of the EHR. is obtained by encoding the most recent clinical events from each patients EHR tables using pre-trained encoder. This ensures that the retrieved experience is contextually aligned with the patients current status. Si = Mθ(Hrec, Siw, Hdist, , Esum) ai+1 πθ(ai+1 ˆHi, , Eact) (9) (10) By explicitly conditioning on the experience, the summarizer improves its ability to filter noise, and the actor adopts proven reasoning strategies, thereby enhancing overall robustness."
        },
        {
            "title": "4 Experiments",
            "content": "4.1 Setting All the details of the experimental setting, including baseline models, agent methods, and experimental details, can be found in Appendix D. 4.2 Main Results Table 2 demonstrates the consistent superiority of RETROSUM across all settings. Even without the evolving mechanism, our method outperforms competitive baselines. key observation is the instability of the ReSum baseline on stronger backbones. While ReSum aids smaller models like Qwen3Next-80B, it significantly underperforms standard ReAct on highly capable models such as Grok-4.1fast (0.2237 vs 0.2501). This indicates that the critical information loss inherent in unidirectional compression outweighs its benefits for strong reasoners. In contrast, RETROSUM overcomes this limitation by retaining access to the full history, ensuring robust performance improvements regardless of the backbones capability. The results also highlight the general instability of evolving strategies within the AGENTEHR environment. Reasoning Bank fails to improve weaker models and only yields benefits on larger parameters like the 80B model, while ReflectTool shows negligible gains because its atomic tool-specific experience is ineffective for complex context synthesis. Conversely, the evolving variant of RETROSUM achieves the highest average score of 0.2880. This success suggests that the high-quality context compression provided by the retrospective mechanism amplifies the utility of retrieved experiences, allowing the agent to leverage historical insights more effectively than other approaches. Models Methods Evolved Diagnoses Labevents Microbiology Prescriptions Procedures Transfers Avg. Qwen3-30B-A3B Qwen3-Next-80B-A3B Qwen3-235B-A22B* GPT-5-mini Grok-4.1-fast ReAct Reflexion ReSum ReflecTool (CS) ReflecTool (IR) ReasoningBank RETROSUM (Ours) RETROSUM (Ours) ReAct Reflexion ReSum ReflecTool (CS) ReflecTool (IR) ReasoningBank RETROSUM (Ours) RETROSUM (Ours) ReAct ReSum RETROSUM (Ours) RETROSUM (Ours) ReAct ReSum RETROSUM (Ours) RETROSUM (Ours) ReAct ReSum RETROSUM (Ours) RETROSUM (Ours) 0.0955 0.1216 0.1753 0.1647 0.1046 0.0385 0.2368 0.2514 0.1836 0.2323 0.2161 0.2088 0.1992 0.2519 0.2519 0.3148 0.1671 0.2804 0.2842 0.3700 0.4270 0.4024 0.4023 0.4127 0.3841 0.3822 0.4270 0.4734 0.1295 0.0983 0.0881 0.0821 0.0893 0.1275 0.0977 0.1092 0.0793 0.0887 0.1083 0.0894 0.0822 0.1998 0.1389 0.1990 0.1172 0.1009 0.1328 0.1951 0.2151 0.2181 0.2569 0.2989 0.1856 0.0882 0.1859 0.2321 0.1274 0.0989 0.1166 0.1265 0.1360 0.1250 0.1584 0.1568 0.1895 0.1585 0.1597 0.1579 0.1389 0.1726 0.1881 0.1830 0.1545 0.1570 0.1568 0.1791 0.1867 0.2090 0.2146 0.2097 0.2032 0.2024 0.2296 0.2236 0.0756 0.0901 0.1000 0.0614 0.0713 0.0448 0.1039 0.1289 0.1075 0.0818 0.1301 0.1059 0.0910 0.1267 0.1285 0.1429 0.0670 0.0728 0.0953 0.1346 0.0764 0.0752 0.1128 0.1536 0.1688 0.1618 0.1846 0.1861 0.2472 0.2273 0.2717 0.2572 0.2428 0.2931 0.2983 0.3068 0.2857 0.3098 0.3503 0.3066 0.2490 0.3743 0.3473 0.3578 0.2739 0.2907 0.3092 0.3556 0.2693 0.2668 0.2614 0. 0.3762 0.3186 0.3480 0.3397 0.2772 0.2292 0.2320 0.2589 0.2882 0.2103 0.2791 0.3171 0.2346 0.1599 0.2317 0.1985 0.2390 0.2793 0.2217 0.2573 0.1413 0.0803 0.0988 0.2342 0.2053 0.1993 0.2071 0.2327 0.1826 0.1888 0.1901 0.2729 0.1587 0.1442 0.1639 0.1585 0.1554 0.1399 0.1957 0.2117 0.1800 0.1718 0.1994 0.1779 0.1665 0.2341 0.2127 0.2425 0.1535 0.1637 0.1795 0.2448 0.2300 0.2285 0.2425 0.2613 0.2501 0.2237 0.2609 0.2880 Table 2: Experiments results on MIMI-IV-Common subset. All the performances are measured by the F1 score. * indicates the model uses 4-bit AWQ quantization. The best results are Bold, while the second best results are underlined. and indicate the p-value < 0.05 and < 0.01 comparing with the ReSum method, respectively. 4.3 Cross-subset Validation To investigate the generalization capability of RETROSUM, we conducted cross-subset validation on MIMIC-IV-Rare and MIMIC-III. As detailed in Table 3, RETROSUM exhibits superior robustness against both types of distribution shifts. In the MIMIC-IV-Rare subset, our method establishes clear lead over ReSum and ReAct, effectively handling low-prevalence diagnoses where standard patterns often fail. The MIMIC-III benchmark reveals sharper contrast in adaptability. While baselines like ReflectTool demonstrate improved resilience, ReSum suffers significant performance degradation. This decline suggests that ReSums unidirectional summaries may be brittle to systemic format changes. Conversely, RETROSUM successfully mitigates this issue to maintain state-of-the-art performance, demonstrating that the retrospective mechanism captures generalizable medical logic rather than merely superior in specific database. 4.4 Ablation Experiments To validate the individual contributions of our frameworks core components, we conducted comprehensive ablation studies on the Qwen3-30BA3B backbone, as detailed in Table 4. Introducing the retrospective mechanism significantly enhances performance compared with Resum; applying it specifically to the Actor or Summarizer yields scores of 0.1876 and 0.1798, respectively, while the complete base RETROSUM (blue row) combining both achieves synergistic gain to 0.1957. Building upon this, incorporating evolving optimization provides substantial benefits with the highest peak performance of 0.2117. deeper analysis of the retrospective mechanism is further shown in Section 5.1."
        },
        {
            "title": "5 Analysis",
            "content": "In this section, all the experiments are conducted on the diagnoses task in MIMIC-IV-Common. More experimental results, like tool analysis and caes study, can be found in Appendix F. 5.1 Effectiveness of Retrospective Mechanism To dissect the contributions of the retrospective mechanism, we evaluated its independent application to the Summarizer (Sum-Only) and Actor (Act-Only) across intervals ranging from 30 down to 1 step, as shown in Figure 3. The results reveal distinct dominant roles depending on summarization frequency. At small intervals (e.g., EHR Database Methods Evolved Diagnoses Labevents Microbiology Prescriptions Procedures Transfers Avg. MIMIC-IV-Rare MIMIC-III ReAct Reflexion ReSum ReflecTool (CS) ReflecTool (IR) ReasoningBank RETROSUM (Ours) RETROSUM (Ours) ReAct Reflexion ReSum ReflecTool (CS) ReflecTool (IR) ReasoningBank RetroSum (Ours) RetroSum (Ours) 0.0863 0.1248 0.1807 0.1653 0.0947 0.0309 0.1950 0.2325 0.0395 0.0417 0.0502 0.0586 0.0468 0.0602 0.0705 0.0869 0.0757 0.0607 0.0609 0.0444 0.0446 0.0783 0.0530 0.0700 0.1418 0.1340 0.1417 0.1956 0.1875 0.2087 0.1970 0.1860 0.0550 0.0447 0.0579 0.0597 0.0551 0.0540 0.0519 0.0536 0.1871 0.1710 0.1764 0.1595 0.1602 0.1736 0.1510 0. 0.0516 0.0574 0.0857 0.0465 0.0361 0.0349 0.0921 0.0933 0.0841 0.0715 0.0851 0.0782 0.0780 0.0815 0.0891 0.0807 0.2645 0.2492 0.2812 0.2506 0.2826 0.2570 0.2719 0.2991 0.0116 0.0133 0.0185 0.0298 0.0282 0.0170 0.0337 0.0473 0.2196 0.1848 0.2282 0.2137 0.2123 0.1859 0.2400 0.2194 0.3272 0.2656 0.2485 0.3689 0.3576 0.1722 0.3627 0.3570 0.1255 0.1203 0.1491 0.1300 0.1209 0.1068 0.1506 0.1613 0.1319 0.1162 0.1201 0.1484 0.1431 0.1189 0.1507 0.1545 Table 3: Results of Qwen3-30B-A3B on OOD subsets. The best results are Bold, while the second best results are underlined. and indicate the p-value < 0.05 and < 0.01 comparing with the ReSum method, respectively. Actor Summarizer Retrospect Evolved Retrospect Evolved Diagnoses Labevents Microbiology Prescriptions Procedures Transfers Avg. 0.1753 0.2257 0.1759 0.2368 0.2505 0.2437 0.2514 0.0881 0.1041 0.1097 0.0977 0.1387 0.1151 0.1092 0.1166 0.1418 0.1710 0.1584 0.1541 0.1565 0.1568 0.1000 0.1030 0.1119 0.1039 0.1354 0.1082 0.1289 0.2717 0.2932 0.2320 0.2983 0.2853 0.3234 0.3068 0.2320 0.2577 0.2782 0.2791 0.2982 0.2919 0.3171 0.1639 0.1876 0.1798 0.1957 0.2104 0.2065 0.2117 Table 4: Ablation results of Qwen3-30B-A3B on MIMIC-IV-Common subset. The row in gray and blue indicates the results of ReSum and RETROSUM, respectively. The best results are Bold, while the second best results are underlined. and indicate the p-value < 0.05 and < 0.01 comparing with the ReSum method, respectively. 5), Act-Only is crucial and drives performance by maintaining immediate reasoning coherence against frequent context interruptions. Conversely, at larger intervals (e.g., 15), Sum-Only becomes dominant, ensuring distant critical information is retained over long horizons. By synergizing these complementary strengths, the complete RETROSUM framework secures robust performance that consistently outperforms the ReSum baseline across the entire spectrum of summarization frequencies. 5.2 Error Analysis Figure 3: Impact of summarization interval on agent performance. The retrospective mechanism is applied solely to the Summarizer (Sum-Only) or the Actor (ActOnly) across varying frequencies. To deepen our understanding, we categorized failures into six types (detailed in Appendix E). As shown in Figure 4, failed trajectories are predominantly plagued by No Candidate Tool errors and repetitive behaviors, indicating that EHR noise frequently disrupts reasoning context. While ReSum reduces repetitive errors compared to ReAct, it remains prone to tool-related failures due to information loss. In contrast, RETROSUM achieves substantial reduction in total errors across all categories. Notably, the Evolved RETROSUM variant further enhances tool utilization, resulting in 92.3% reduction in total errors compared to ReasoningBank. These findings validate that our retrospective mechanism effectively mitigates critical reasoning failures caused by complex, longhorizon EHR interactions. 5.3 Efficiency Analysis To evaluate reasoning efficiency, we analyzed the distribution of interaction turns in Figure 5 (further analysis in Appendix F.1). The results highlight 5.5 Impact of Context Length To evaluate the robustness of our framework under memory constraints, we varied the maximum context length from 64k down to 8k tokens, as shown in Figure 7. clear downward trend is observed for baseline methods; both ReAct and ReSum suffer significant performance degradation as the context window tightens, indicating that standard sliding windows and unidirectional summaries fail to retain critical information when space is limited. Conversely, RETROSUM exhibits remarkable stability, maintaining high F1 scores even at the most restrictive 8k limit. This robustness stems from the retrospective mechanism, which enables the Actor to preserve complete reasoning logic regardless of window size. By periodically re-evaluating the full interaction history to distill essential crosstemporal dependencies into the summary, RETROSUM ensures the reasoning chain remains unbroken and information-dense, effectively neutralizing the negative impact of restricted context windows."
        },
        {
            "title": "6 Conclusions",
            "content": "In this work, we bridge the gap between idealized experimental settings and realistic clinical environments by presenting AGENTEHR, benchmark necessitating complex decision-making within raw, high-noise databases. Based on this, we propose RETROSUM, which adopt the retrospective mechanism with an evolving experience strategy to capture latent cross-temporal correlations. Empirical validation shows retrospective mechanisms are essential for unlocking the potential of clinical agents in EHR-based reasoning decision-making tasks."
        },
        {
            "title": "Limitations",
            "content": "Despite the robust performance of RETROSUM established in this work, several limitations remain. First, our evaluation relies primarily on the MIMICIV and MIMIC-III datasets. While these databases serve as the gold standard for critical care research, they are inherently sourced from single medical center, which may not fully capture the diverse administrative protocols or demographic variations found in broader global healthcare systems. Second, our current framework is designed specifically for textual clinical notes and structured tabular data. It does not yet possess the capability to directly analyze pixel-level medical imaging (e.g., raw CT scans) or high-frequency physiological waveforms, relying instead on textual reports. Future work will Figure 4: Error statistics on the diagnoses task. The left panel displays the distribution of specific error types across unsuccessful and successful trajectories. The right panel compares the count of errors committed by different agent frameworks. critical bottleneck in baselines: ReSum frequently exhausts the maximum 100-turn limit, indicating that its lossy unidirectional summarization traps agents in redundant information-seeking loops to recover missing context. In contrast, RETROSUM shifts the distribution peak to the efficient 20-40 turn range. By utilizing the retrospective mechanism to retain salient history, our method effectively eliminates navigational dead-ends. Crucially, this reasoning efficiency translates to resource economy. Although the retrospective process introduces specific overhead per step, the drastic reduction in total interaction turns effectively outweighs this cost. Consequently, RETROSUM achieves superior performance while requiring significantly fewer tokens and less inference time than baselines (as detailed in Appendix F.3). 5.4 Test-time Scaling To explore the upper bounds of agent capability, we investigate the impact of test-time scaling using the Best@K F1 Score (see Appendix D.3 for formal definition). As illustrated in Figure 6, RETROSUM consistently outperforms both ReAct and ReSum baselines across the entire spectrum of K. The superior performance at Best@1 (equivalent to the average F1 score across 256 samples) attests to the robustness of our retrospective mechanism in consistently generating high-quality trajectories compared to baselines. Furthermore, the substantial performance gains observed with increased sampling budgets indicate that current models possess significant potential capacity to solve these complex clinical problems. This confirms that the AGENTEHR task, while challenging, is fundamentally solvable and its design is reasonable. Figure 5: Distribution of interaction turns across different agent methods. Figure 6: Test-time scaling performance evaluated using Best@K F1 Score. Figure 7: Performance sensitivity to maximum context length limited from 64k to 8k tokens. focus on extending the agents capabilities to multimodal data synthesis and exploring multi-center generalization."
        },
        {
            "title": "References",
            "content": "Anjanava Biswas and Wrick Talukdar. 2024. Intelligent clinical documentation: Harnessing generative ai for patient-centric clinical note generation. arXiv preprint arXiv:2405.18346. Steinberg, Ashwin Nayak, and 1 others. 2024. Medalign: clinician-generated dataset for instruction following with electronic medical records. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 38, pages 2202122030. Paul Hager, Friederike Jungmann, Robbie Holland, Kunal Bhagat, Inga Hubrecht, Manuel Knauer, Jakob Vielhauer, Marcus Makowski, Rickmer Braren, Georgios Kaissis, and Daniel Rueckert. 2024. Evaluation and mitigation of the limitations of large language models in clinical decision-making. Jianlv Chen, Shitao Xiao, Peitian Zhang, Kun Luo, Defu Lian, and Zheng Liu. 2024. Bge m3-embedding: Multi-lingual, multi-functionality, multi-granularity text embeddings through self-knowledge distillation. Preprint, arXiv:2402.03216. Stefan Hegselmann, Georg von Arnim, Tillmann Rheude, Noel Kronenberg, David Sontag, Gerhard Hindricks, Roland Eils, and Benjamin Wild. 2025. Large language models are powerful electronic health record encoders. arXiv preprint arXiv:2502.17403. Shan Chen, Pedro Moreira, Yuxin Xiao, Sam Schmidgall, Jeremy Warner, Hugo Aerts, Thomas Hartvigsen, Jack Gallifant, and Danielle Bitterman. 2025. Medbrowsecomp: Benchmarking medical deep research and computer use. arXiv preprint arXiv:2505.14963. Zeming Chen, Alejandro Hernández Cano, Angelika Romanou, Antoine Bonnet, Kyle Matoba, Francesco Salvi, Matteo Pagliardini, Simin Fan, Andreas Köpf, Amirkeivan Mohtashami, and 1 others. 2023. Meditron-70b: Scaling medical pretraining for large language models. arXiv preprint arXiv:2311.16079. Hejie Cui, Alyssa Unell, Bowen Chen, Jason Alan Fries, Emily Alsentzer, Sanmi Koyejo, and Nigam Shah. 2025. Timer: Temporal instruction modeling and evaluation for longitudinal clinical records. npj Digital Medicine, 8(1):577. Yue Fang, Yuxin Guo, Jiaran Gao, Hongxin Ding, Xinke Jiang, Weibin Liao, Yongxin Xu, Yinghao Zhu, Zhibang Yang, Liantao Ma, and 1 others. 2025. Toward better ehr reasoning in llms: Reinforcement learning with expert attention guidance. arXiv preprint arXiv:2508.13579. Scott Fleming, Alejandro Lozano, William Haberkorn, Jenelle Jindal, Eduardo Reis, Rahul Thapa, Louis Blankemeier, Julian Genkins, Ethan Yixing Jiang, Kameron Black, Gloria Geng, Danny Park, James Zou, Andrew Ng, and Jonathan Chen. 2025. Medagentbench: virtual ehr environment to benchmark medical llm agents. NEJM AI, page AIdbp2500144. Alistair EW Johnson, Lucas Bulgarelli, Lu Shen, Alvin Gayles, Ayad Shammout, Steven Horng, Tom Pollard, Sicheng Hao, Benjamin Moody, Brian Gow, and 1 others. 2023. Mimic-iv, freely accessible electronic health record dataset. Scientific data, 10(1):1. Alistair EW Johnson, Tom Pollard, Lu Shen, Li-wei Lehman, Mengling Feng, Mohammad Ghassemi, Benjamin Moody, Peter Szolovits, Leo Anthony Celi, and Roger Mark. 2016. Mimic-iii, freely accessible critical care database. Scientific data, 3(1):19. HyoJe Jung, Yunha Kim, Heejung Choi, Hyeram Seo, Minkyoung Kim, JiYe Han, Gaeun Kee, Seohyun Park, Soyoung Ko, Byeolhee Kim, and 1 others. 2024. Enhancing clinical efficiency through llm: Discharge note generation for cardiac patients. arXiv preprint arXiv:2404.05144. Gyubok Lee, Elea Bach, Eric Yang, Tom Pollard, Alistair Johnson, Edward Choi, Jong Ha Lee, and 1 others. 2025. Fhir-agentbench: Benchmarking llm agents for realistic interoperable ehr question answering. arXiv preprint arXiv:2509.19319. Gyubok Lee, Hyeonji Hwang, Seongsu Bae, Yeonsu Kwon, Woncheol Shin, Seongjun Yang, Minjoon Seo, Jong-Yeup Kim, and Edward Choi. 2022. Ehrsql: practical text-to-sql benchmark for electronic health records. Advances in Neural Information Processing Systems, 35:1558915601. Yusheng Liao, Shuyang Jiang, Yanfeng Wang, and Yu Wang. 2025a. Reflectool: Towards reflectionaware tool-augmented clinical agents. In Proceedings of the 63rd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), ACL 2025, Vienna, Austria, July 27 - August 1, 2025, pages 1350713531. Association for Computational Linguistics. Yusheng Liao, Yutong Meng, Yuhao Wang, Hongcheng Liu, Yanfeng Wang, and Yu Wang. 2024. Automatic interactive evaluation for large language models with state aware patient simulator. arXiv preprint arXiv:2403.08495. Yusheng Liao, Chaoyi Wu, Junwei Liu, Shuyang Jiang, Pengcheng Qiu, Haowen Wang, Yun Yue, Shuai Zhen, Jian Wang, Qianrui Fan, and 1 others. 2025b. Ehr-r1: reasoning-enhanced foundational language model for electronic health record analysis. arXiv preprint arXiv:2510.25628. Jiacheng Lin, Zhenbang Wu, and Jimeng Sun. 2025. Training llms for ehr-based reasoning tasks via reinforcement learning. arXiv preprint arXiv:2505.24105. Kanato Masayoshi, Masahiro Hashimoto, Ryoichi Yokoyama, Naoki Toda, Yoshifumi Uwamino, Shogo Fukuda, Ho Namkoong, and Masahiro Jinzaki. 2025. Ehr-mcp: Real-world evaluation of clinical information retrieval by large language models via model context protocol. arXiv preprint arXiv:2509.15957. Harsha Nori, Nicholas King, Scott Mayer McKinney, Dean Carignan, and Eric Horvitz. 2023. Capabilities of gpt-4 on medical challenge problems. arXiv preprint arXiv:2303.13375. OpenAI. 2025. Introducing GPT-5. https://openai. com/index/introducing-gpt-5. Accessed: 202508-07. Siru Ouyang, Jun Yan, I-Hung Hsu, Yanfei Chen, Ke Jiang, Zifeng Wang, Rujun Han, Long T. Le, Samira Daruki, Xiangru Tang, Vishy Tirumalashetty, George Lee, Mahsan Rofouei, Hangfei Lin, Jiawei Han, Chen-Yu Lee, and Tomas Pfister. 2025. Reasoningbank: Scaling agent self-evolving with reasoning memory. CoRR, abs/2509.25140. Pengcheng Qiu, Chaoyi Wu, Junwei Liu, Qiaoyu Zheng, Yusheng Liao, Haowen Wang, Yun Yue, Qianrui Fan, Shuai Zhen, Jian Wang, and 1 others. 2025a. Evolving diagnostic agents in virtual clinical environment. arXiv preprint arXiv:2510.24654. Pengcheng Qiu, Chaoyi Wu, Shuyu Liu, Yanjie Fan, Weike Zhao, Zhuoxia Chen, Hongfei Gu, Chuanjin Peng, Ya Zhang, Yanfeng Wang, and 1 others. 2025b. Quantifying the reasoning abilities of llms on clinical cases. Nature Communications, 16(1):9799. François Remy, Kris Demuynck, and Thomas Demeester. 2024. BioLORD-2023: semantic textual representations fusing large language models and clinical knowledge graph insights. Journal of the American Medical Informatics Association, page ocae029. Jaehee Ryu, Seonhee Cho, Gyubok Lee, and Edward Choi. 2024. Ehr-seqsql: sequential text-to-sql dataset for interactively exploring electronic health records. arXiv preprint arXiv:2406.00019. Wenqi Shi, Ran Xu, Yuchen Zhuang, Yue Yu, Jieyu Zhang, Hang Wu, Yuanda Zhu, Joyce C. Ho, Carl Yang, and May Dongmei Wang. 2024. EHRAgent: Code empowers large language models for fewshot complex tabular reasoning on electronic health records. In Proceedings of the 2024 Conference on Empirical Methods in Natural Language Processing, pages 2231522339, Miami, Florida, USA. Association for Computational Linguistics. Noah Shinn, Federico Cassano, Ashwin Gopinath, Karthik Narasimhan, and Shunyu Yao. 2023. Reflexion: language agents with verbal reinforcement learning. In Advances in Neural Information Processing Systems 36: Annual Conference on Neural Information Processing Systems 2023, NeurIPS 2023, New Orleans, LA, USA, December 10 - 16, 2023. Karan Singhal, Shekoofeh Azizi, Tao Tu, Sara Mahdavi, Jason Wei, Hyung Won Chung, Nathan Scales, Ajay Tanwani, Heather Cole-Lewis, Stephen Pfohl, and 1 others. 2023. Large language models encode clinical knowledge. Nature, 620(7972):172180. MiroMind Team, Song Bai, Lidong Bing, Carson Chen, Guanzheng Chen, Yuntao Chen, Zhe Chen, Ziyi Chen, Jifeng Dai, Xuan Dong, and 1 others. 2025a. Mirothinker: Pushing the performance boundaries of open-source research agents via model, arXiv preprint context, and interactive scaling. arXiv:2511.11793. Tongyi DeepResearch Team, Baixuan Li, Bo Zhang, Dingchu Zhang, Fei Huang, Guangyu Li, Guoxin Chen, Huifeng Yin, Jialong Wu, Jingren Zhou, and 1 others. 2025b. Tongyi deepresearch technical report. arXiv preprint arXiv:2510.24701. Ping Wang, Tian Shi, and Chandan Reddy. 2020. Text-to-sql generation for question answering on electronic medical records. In Proceedings of The Web Conference 2020, pages 350361. Xixi Wu, Kuan Li, Yida Zhao, Liwen Zhang, Litu Ou, Huifeng Yin, Zhongwang Zhang, Yong Jiang, Pengjun Xie, Fei Huang, Minhao Cheng, Shuai Wang, Hong Cheng, and Jingren Zhou. 2025. Resum: Unlocking long-horizon search intelligence via context summarization. CoRR, abs/2509.13313. Zhenbang Wu, Anant Dadu, Mike Nalls, Faraz Faghri, and Jimeng Sun. 2024. Instruction tuning large language models to understand electronic health records. Advances in Neural Information Processing Systems, 37:5477254786. xAI. 2025. Grok 4.1 Fast and Agent Tools API. https: //x.ai/news/grok-4-1-fast. Accessed: 202511-19. Guangzhi Xiong, Qiao Jin, Zhiyong Lu, and Aidong Zhang. 2024. Benchmarking retrieval-augmented generation for medicine. In Findings of the Association for Computational Linguistics ACL 2024, pages 62336251. Ran Xu, Yuchen Zhuang, Yishan Zhong, Yue Yu, Xiangru Tang, Hang Wu, May Dongmei Wang, Peifeng Ruan, Donghan Yang, Tao Wang, and 1 others. 2025. Medagentgym: Training llm agents for code-based medical reasoning at scale. In The Second Workshop on GenAI for Health: Potential, Trust, and Policy Compliance. An Yang, Anfeng Li, Baosong Yang, Beichen Zhang, Binyuan Hui, Bo Zheng, Bowen Yu, Chang Gao, Chengen Huang, Chenxu Lv, Chujie Zheng, Dayiheng Liu, Fan Zhou, Fei Huang, Feng Hu, Hao Ge, Haoran Wei, Huan Lin, Jialong Tang, and 40 others. 2025. Qwen3 technical report. CoRR, abs/2505.09388. Shunyu Yao, Jeffrey Zhao, Dian Yu, Nan Du, Izhak Shafran, Karthik R. Narasimhan, and Yuan Cao. 2023. React: Synergizing reasoning and acting in language models. In The Eleventh International Conference on Learning Representations, ICLR 2023, Kigali, Rwanda, May 1-5, 2023. OpenReview.net. Zijian Zhou, Ao Qu, Zhaoxuan Wu, Sunghwan Kim, Alok Prakash, Daniela Rus, Jinhua Zhao, Bryan Kian Hsiang Low, and Paul Pu Liang. 2025. Mem1: Learning to synergize memory and reasoning for efficient long-horizon agents. arXiv preprint arXiv:2506.15841. Yinghao Zhu, Changyu Ren, Shiyun Xie, Shukai Liu, Hangyuan Ji, Zixiang Wang, Tao Sun, Long He, Zhoujun Li, Xi Zhu, and 1 others. 2024. Realm: Rag-driven enhancement of multimodal electronic health records analysis via large language models. arXiv preprint arXiv:2402.07016."
        },
        {
            "title": "A Related Works",
            "content": "Recent advancements in Large Language Models (LLMs) for Electronic Health Records (EHRs) have progressed from foundational instruction tuning to the development of autonomous agents and specialized benchmarks. Early efforts focused on aligning general-purpose models with the longitudinal and heterogeneous nature of clinical data; for instance, MIMIC-Instr (Wu et al., 2024) and MEDALIGN (Fleming et al., 2024) introduced large-scale instruction datasets to bridge the gap between raw clinical logs and natural language understanding. These foundations were subsequently enhanced by frameworks addressing specific reasoning dimensions, such as TIMER (Cui et al., 2025), which targets temporal dependencies in patient histories, and EHR-R1 (Liao et al., 2025b), which synthesizes reasoning chains to improve complex decision-making. Concurrently, the field has shifted towards agentic systems capable of active tool utilization. EHRAgent (Shi et al., 2024) and ReflecTool (Liao et al., 2025a) demonstrated that equipping LLMs with code execution interfaces and reflective memory significantly outperforms standard prompting on tabular reasoning tasks. This transition to agency extends to realworld integration, where EHR-MCP (Masayoshi et al., 2025) and EHRMIND (Yang et al., 2025) explore standard-compliant deployment and verifiable reinforcement learning to ensure reliability in live clinical settings. To rigorously evaluate these interactive capabilities, novel benchmarks like MedAgentBench (Jiang et al., 2025) and FHIRAgentBench (Lee et al., 2025) have been established, moving beyond static question-answering to assess agent planning, multi-step tool usage, and interoperability within realistic virtual EHR environments."
        },
        {
            "title": "B Data Curation",
            "content": "This section outlines the data curation methodology used to convert raw Electronic Health Records (EHR) into structured format. The process involves selecting data sources, with MIMIC-IV serving as the primary corpus and MIMIC-III used for cross-subset validation. We constructed patientlevel time series, enriched medical codes with semantic mappings, standardized storage formats, and developed unified toolbox to utilize the curated data for executing clinical tasks. The following subsections describe the processing steps ensuring data integrity and chronological accuracy. B.1 Data Preprocessing and Formatting We reformatted the raw MIMIC-IV and MIMICIII dataset to construct accurate patient-level time series. This involved extracting all events for each patient and sorting them by timestamp with secondlevel precision. Temporal Alignment and Imputation. The raw dataset lacks second-level timestamps for tables such as diagnoses_icd, procedures_icd, and diagnosis events in the Emergency Department (ED). To enable sequential modeling, we imputed these timestamps by linking events to their corresponding admissions. For both standard diagnoses_icd and ED diagnoses, we set the timestamp to one minute prior to the discharge time of the associated admission or ED stay. For procedures_icd events that only provide day-level resolution, we assigned default timestamp of 23:59:59 on the recorded day to maintain logical ordering. Semantic Enrichment. We mapped highdimensional medical codes to broader clinical categories to improve representation. International Classification of Diseases (ICD) codes in diagnosis and procedure tables were mapped to Clinical Classification Software (CCS) categories using the ICD-to-CCS script12. Similarly, National Drug Codes (NDC) in prescription tables were converted to Anatomical Therapeutic Chemical (ATC) codes adopting the off-the-shelf mapping script3. These mappings consolidate sparse code representations into hierarchically structured features. Textual Integration and Leakage Mitigation. We standardized textual data availability by moving information explicitly available at admission, such as Social History and Chief Complaints, from the discharge summary to the admission event. To maintain dataset consistency, we restricted our cohort to patients possessing valid text records in their discharge summaries. We also enforced strict leakage control by removing columns containing future administrative or outcome information. In the admissions table, we excluded dischtime, deathtime, discharge_location, edouttime, and hospital_expire_flag. Similarly, we removed last_careunit, outtime, and los from icustays, as well as outtime and disposition from edstays. Finally, we masked pharmacy artifacts in prescription events that implicitly reveal future details. Label Space Construction. To facilitate effective clinical reasoning across both MIMIC-IV and MIMIC-III, we addressed the challenge of excessive granularity in the raw label space. We employed three distinct strategies based on data characteristics. First, for diagnoses and procedures, we adopted the Clinical Classification Software (CCS) taxonomy to aggregate granular codes into clinically coherent categories. This transformation yielded significant reduction, exemplified by MIMIC-IV. The label space was compressed from 109,775 raw diagnosis codes to 283 CCS categories, and from 85,257 procedure codes to 231 categories. Similarly, for prescriptions in MIMIC-IV, we projected National Drug Codes (NDC) onto the Anatomical Therapeutic Chemical (ATC) system, condensing 1,086,608 raw items into 1,813 pharmacological groups. For domains lacking standardized mappings, we utilized statistical aggregation or metadata definitions. Specifically, due to the absence of consistent NDC records in MIMIC-III prescriptions, as well as for microbiologyevents and transfers in both datasets, label spaces were constructed by aggregating all unique items observed in the records. For labevents, the candidate list was derived directly from the standardized definition table (d_labitems), ensuring coverage of all valid laboratory tests. Storage Structure. Processed data is stored in patient-centric format where the complete longitudinal history of each patient is contained within single SQLite database file. This structure supports modular access and efficient data retrieval. B.2 Auxiliary Resources constructed Reference We and Candidates databases to facilitate data interpretation and define the output space for predictive tasks. 1From ICD-9 to CCS: https://hcup-us.ahrq.gov/ toolssoftware/ccs/ccs.jsp. 2From ICD-10 to CCSR: https://hcup-us.ahrq.gov/ toolssoftware/ccsr/dxccsr.jsp. 3From NDC sunlabuiuc/PyHealth. to ATC: https://github.com/ Reference Database. The Reference database functions as dictionary that maps medical codes to item names or descriptions using metadata from the diagnoses, procedures, prescriptions, and labitems tables in MIMIC-IV. This resource allows the retrieval of specific item names associated with patient EHR records. Candidates Database. The Candidates database defines the answer space for the six evaluation tasks. Candidate lists for diagnoses, procedures, prescriptions, and labitems were extracted directly from the Reference database. For domains without dedicated metadata tables, specifically microbiologyevents and transfers, we aggregated all unique items observed across the MIMICIV and MIMIC-III datasets to form the final candidate sets. This process resulted in six candidate tables covering the required label space. We also provide schema description files and database link reference to support system integration. B.3 Sample Construction To construct robust benchmark from the massive MIMIC-IV database ( 300k patients) and the smaller MIMIC-III database( 1k patients), we employ stratified sampling strategy designed to balance label diversity and task difficulty. MIMIC-IV Construction. For each clinical task, we first identify valid event occurrences for each patient. We define sample instance by setting the prediction timestamp to one minute prior to the event time and identifying the ground-truth label set . Subsequently, we temporally censor the patients history by removing any clinical observations recorded after the prediction time t. To ensure the benchmark covers both prevalent conditions and long-tail rare cases, we implement Label-wise Weighted Sampling strategy. We calculate sampling weight wS for each candidate sample S, inversely proportional to the frequency of its constituent labels. Formally, for sample with ground-truth label set = {y1, y2, . . . , yN }, the weight is defined as: wS = 1 (cid:88) yY 1 Count(y) (11) where Count(y) represents the global frequency of label entity within the specific task type.We then compute the mean weight across all candidates to stratify the dataset. Samples with wS are categorized into the Common pool (high-frequency labels), while those with wS > are assigned to the Rare pool (low-frequency labels). Within each pool, we perform weighted random sampling using wS as the probability distribution. This ensures that even within the Common or Rare subsets, the selected samples are uniformly distributed across the label space, maximizing diversity. Finally, we sample 600 samples for each type of task and filter out samples lacking admission records to ensure data completeness, resulting in the final MIMIC-IVCommon and MIMIC-IV-Rare benchmarks. MIMIC-III Construction. Given the significantly smaller scale of MIMIC-III, we adopt direct sampling approach. We randomly sample instances from the valid patient pool. However, to account for the systemic complexity differences, we selectively increase the sample density for highdifficulty tasks, specifically Diagnoses and Procedures, to ensure the evaluation metric remains statistically significant for these challenging reasoning scenarios. B.4 Dataset Statistics Table 5 presents comprehensive statistical summary of the curated benchmarks. The datasets are categorized into MIMIC-IV Common, MIMIC-IV Rare, and MIMIC-III subsets to evaluate model performance across different prevalence distributions and data sources. defining characteristic of the MIMIC-IV subsets is the high complexity of the input data. On average, the model must process longitudinal patient histories spanning over year, comprising thousands of temporal records. This necessitates robust capabilities in modeling long-term dependencies within heterogeneous EHR tables. In distinct contrast, the MIMIC-III subset exhibits fundamentally different structural challenge. Unlike MIMIC-IV, MIMIC-III covers significantly shorter time span (approximately 1/20) yet retains disproportionately large volume of records (roughly 1/2). This discrepancy indicates substantially higher recording density, which inevitably introduces greater redundancy and noise. Consequently, as evidenced by the comparative results in Table 3, MIMIC-III presents more formidable challenge than MIMIC-IV. We attribute this difficulty not to data scarcity, but to the heightthe ened complexity of information retrieval: model is required to filter through highly dense and noisy sequences to extract relevant clinical signals effectively. Dataset Task # Cases # Cand. Avg. Label Avg. Tables Avg. Recs Avg. Days MIMIC-IV Common MIMIC-IV Rare MIMIC-III Diagnoses Labevents Microbiologyevents Prescriptions Procedures Transfers Diagnoses Labevents Microbiologyevents Prescriptions Procedures Transfers Diagnoses Labevents Microbiologyevents Prescriptions Procedures Transfers 528 589 549 515 529 538 559 600 565 527 493 487 600 500 500 500 692 500 283 1170 171 1813 231 283 1170 171 1813 231 38 283 587 63 1235 231 5 12.03 30.26 5.59 7.65 2.46 1.00 6.10 18.58 4.29 7.27 2.28 1.00 11.03 18.92 3.23 15.55 3.64 1.00 24.17 17.60 21.04 24.03 24.20 25. 20.95 17.04 19.18 23.64 23.36 24.80 11.57 11.65 11.73 11.62 11.81 11.76 3041.10 849.92 1736.62 1786.62 1797.61 3237.72 1341.71 1001.75 1401.23 1455.21 1642.98 2691.69 1032.14 679.61 643.05 564.59 1275.52 550.49 370.84 347.79 510.68 414.26 384.50 387. 301.63 438.99 524.76 349.78 404.20 441.25 8.81 26.19 21.23 40.62 55.92 31.34 Table 5: Benchmark statistics for each Dataset and Task cohort (# Cases). The table details the number of candidate answers for the task (# Cand.) and the average size of label set (Avg. Label). Input complexity is further characterized by the average number of source tables (Avg. Tables), record volume (Avg. Recs), and longitudinal span (Avg. Days)."
        },
        {
            "title": "C Toolbox Construction",
            "content": "To enable autonomous agents to effectively interact with the curated clinical environment, we developed comprehensive toolbox serving as the interface between the agents reasoning core and the structured data. The toolbox is categorized into five distinct functional modules: Record, Candidate, Table, Inner, and Retrieval. detailed specification of the constituent tools and their parameters is provided in Table 6. Record Interaction Tools. These tools facilitate granular access to the patients longitudinal history by acting as direct query interface to the EHR database. To retrieve specific information, the agent must specify the target table name along with precise filtering criteria. The tool supports queries based on temporal constraints to isolate events within specific time windows, textual content matching to identify records containing particular medical terms, and numerical value filtering to extract measurements meeting specific thresholds. This mechanism allows the agent to dynamically gather evidence from the patients history without needing to load the entire database into context. Candidate Alignment Tools. This category is critical for grounding the agents free-form reasoning into the fixed output space defined by the Candidate Database. To ensure robust mapping between generated hypotheses and valid label entries, the tool employs hybrid matching strategy. It first attempts direct keyword search and fuzzy string matching to handle exact matches and minor morphological variations. For more complex cases involving synonymy or terminological differences, the tool utilizes semantic alignment powered by the biomedical-specific embedding model BioLORD-2023 (Remy et al., 2024). This vectorbased retrieval enables the agent to identify the correct candidate even when there is no lexical overlap between the generated query and the standardized code description. Schema Inspection Tools. To effectively navigate the relational structure of the EHR, the agent utilizes Schema Inspection tools to acquire metadata awareness. These tools allow the agent to query the definitions of the database structure itself, revealing which tables are available in the current patients file and detailing the specific columns and data types within each table. By understanding the underlying schema, the agent can formulate syntactically correct queries for the Record Interaction tools and interpret the retrieved data with the correct semantic context. Cognitive Management Tools (Inner). Unlike external interaction tools, this category governs the agents internal control flow and decision-making process. The Think tool enables the agent to generate intermediate reasoning traces and formulate multi-step plans without triggering external environment actions. Once the agent has gathered sufficient evidence and reached conclusion, the Finish tool serves as the termination signal, allowing the agent to end the trajectory and output the final prediction. This separation of reasoning and termination ensures structured and verifiable thought process. External Knowledge Retrieval. To augment the specific clinical data contained within the EHR, this tool connects the agent to broader external knowledge base. By accepting natural language queries, it searches for relevant medical literature, guidelines, or definitions that are not present in the patients records. In implementation, we use the data sources provided in MedRAG (Xiong et al., 2024). This provides the agent with the necessary background knowledge to interpret complex medical conditions or rare procedures, thereby supporting more informed decision-making."
        },
        {
            "title": "D Experimental Setting",
            "content": "D.1 Baselines To comprehensively verify the effectiveness and robustness of RETROSUM, we conduct comparative evaluations against diverse set of stateof-the-art agent-based methods across five distinct LLM backbones. We employ mix of powerful open-weights and proprietary models as backbones to ensure broad adaptability, including Qwen3-30B-A3B, Qwen3-235B-A22B, Qwen3Next-80B (Yang et al., 2025), GPT-5-min (OpenAI, 2025), and Grok-4.1-fast (xAI, 2025). On these backbones, we evaluate six representative agent methods as baselines, categorized into static and evolving methods. Static agents include standard ReAct (Yao et al., 2023), selfreflecting Reflexion (Shinn et al., 2023), and the unidirectional summarization method ReSum (Wu et al., 2025). Evolving agents include Reasoning Bank (Ouyang et al., 2025) and ReflecTool (Liao et al., 2025a), for which we evaluate both the Candidate Selection and Iterative Refinement variants. D.2 Implementation Details In our experiments, we set the maximum interaction turns for all agents to 100 to ensure computational feasibility while allowing sufficient exploration. The maximum context length is capped at 64,000 tokens to accommodate the potentially extensive history in EHR tasks. For evolving methods, we provide 100 training examples per task sampled from the common set of MIMIC-IV for experience accumulation. Crucially, we utilize this identical set of accumulated experiences to evaluate performance across all three datasets (MIMICIV-Common, MIMIC-IV-Rare, and MIMIC-III), thereby strictly testing the cross-distribution generalization of the evolved agents. During inference, we retrieve only the top-1 most similar experience sample based on the embedding generated by bge-m3 (Chen et al., 2024). For RETROSUM, the retrospective summarization interval is set to 10 turns. All experiments are conducted on cluster of NVIDIA 8xA100 GPUs. Finally, Prompt 1 to 6 contains the instructions of six prompts in AGENTEHR and Prompt 7 to 9 contains the prompt used in RETROSUM method. D.3 Metrics F1 Scores For given clinical task instance, let denote the set of elements predicted by the agent and denote the set of ground truth elements. We first evaluate the Precision and Recall with the formula below: Precision = Recall = , Y Y (12) (13) The F1 Score is defined as the harmonic mean of Precision and Recall, providing balanced singlevalue metric that penalizes both missed relevant items and incorrect predictions: F1 = 2 Precision Recall Precision + Recall (14) Best@K F1 To quantify the expected peak performance given specific inference budget K, we calculate the Best@K F1 Score based on total pool of generated trajectories (where K). This metric is defined as the expected maximum F1 score when trajectories are sampled without replacement from the total pool .For specific instance i, let Ti = {τ1, τ2, . . . , τN } be the set of Category Tool Name Description think Inner finish get_records_by_time get_event_counts_by_time get_latest_records Record get_records_by_keyword get_records_by_value run_sql_query get_unique_values Designed to synthesize information gathered from preceding operations and to articulate the necessary subsequent actions. The final step in the reasoning process. Used only when all necessary data has been retrieved and the clinical prediction is ready. Finds records in EHR Table that fall within given time range. Calculates the number of events in all EHR Tables that fall within given time range. Finds the latest timestamp and returns all EHR Table records that share that same timestamp in EHR Table. Searches for all text-based columns of the specific EHR Table containing specific keyword. Finds records in EHR Table where given columns value is exact match for the keyword. Executes standard SQL query against the patients EHR Table. Retrieves all unique values from specified categorical column in an EHR table. Candidate get_candidates_by_keyword get_candidates_by_fuzzy_matching Searches for all text-based columns of the specific Candidate Table containing specific keyword. Finds similar items in Candidate Table based on fuzzy matching. get_candidates_by_semantic_similarity Performs semantic search using BioLORD-2023 embedget_column_names Table get_table_names get_table_description retrieve_pubmed retrieve_textbooks Knowledge retrieve_statpearls retrieve_wikipedia dings to find semantically similar unique entities. Retrieves all column names for specified table for understanding the data. Retrieves the names of all available tables in the database, categorized into EHR tables and candidates tables. Retrieve EHR table description and column information from the hospital database schema. Retrieve abstract of relevant biomedical documents from PubMed corpus given query. Retrieve domain specific knowledge from medical textbooks corpus given query. Retrieve clinical decision support from StatPearls corpus given query. Retrieve general knowledge from Wikipedia corpus given query. query query query query Parameters response response subject_id, table_name, start_time, end_time subject_id, start_time, end_time subject_id, table_name subject_id, table_name, keyword subject_id, table_name,column_name, value subject_id, sql_query subject_id, table_name, column_name table_name, keyword table_name, keywords table_name, query subject_id, table_name subject_id table_name Table 6: Overview of the Toolbox. The toolbox includes categories for Candidate extraction, Inner reasoning, Knowledge retrieval, Record querying, and Table schema inspection. all generated trajectories. Let CK be the set of all possible subsets of Ti with size K, where the total number of such subsets is the binomial coefficient (cid:0)N (cid:1). The Best@K score, for instance is calculated by averaging the maximum F1 score over all possible combinations: Best@1 F1 = 1 (cid:0)N 1 (cid:1) (cid:88) j= 1(τj) = 1 (cid:88) j=1 1(τj) (16) (2) Peak Potential (K = ): When the budget allows evaluating the entire generated pool, the metric represents the absolute upper bound of the models capability (Pass@N): Best@K F1 = 1 (cid:0)N (cid:1) (cid:88) SCK max τ F 1(τ ) (15) Best@N F1 = 1 (cid:0)N (cid:1) max τ Ti 1(τ ) = max j{1,...,N } 1(τj) This probabilistic definition encompasses two critical boundary conditions that provide insight into the models behavior: (1) Expected Performance (K = 1): When the budget is single attempt, the metric collapses to the arithmetic mean of all generated trajectories, reflecting the models average performance without selection: (17) This rigorous definition eliminates the variance associated with random sampling and ensures deterministic evaluation of the trade-off between computational budget and performance."
        },
        {
            "title": "E Error Definition",
            "content": "To systematically diagnose the pathological behaviors of the agents, we categorized the failure trajecFigure 8: Heatmap visualization of interaction round distributions across different agent methods and backbones on the diagnoses task in MIMIC-IV Common. The color intensity denotes the frequency of cases falling within each interval. tories into six distinct types. These categories range from low-level format violations to high-level reasoning stagnation. No Prediction. This category aggregates scenarios where the agent terminates the interaction episode without yielding parsable or valid prediction. It encompasses four specific failure modes driven by structural or cognitive deficiencies. The first involves Tool Parsing Failure, where the agent attempts to generate tool invocation but fails to adhere to the required syntax, causing the system to misinterpret the action as termination signal, which is exemplified in Case 1. The second includes Format Submersion, where the agent successfully invokes the finish action and provides text response, yet the answer extractor fails to parse valid prediction from the unstructured output. The third mode is Answer Induced Failure, occurring when the agent invokes the finish tool but provides an empty or null observation, resulting in void prediction. Finally, the category includes Incomplete Termination, where the interaction concludesoften due to reaching the maximum turn limitwithout the agent ever invoking the definitive finish tool, thereby leaving the task unresolved. Tool Repeat. We define this error as state of exact cognitive stagnation. It is characterized by the agent executing the identical tool with completely identical parameters for five consecutive interaction turns, which is illustrated in Case 2. This behavior indicates that the agent has fallen into rigid loop, repeatedly querying the same information without updating its internal strategy, leading to severe information redundancy. Single-Tool Loop. Distinct from exact repetition, this category characterizes scenarios where the agent resorts to naive, iterative retrieval strategy by executing the same tool with negligible parameter variations, as shown in Case 3. We employ the Ratcliff-Obershelp algorithm to quantify the similarity between the parameters of consecutive tool calls sharing the same tool name. The string similarity score Sro between two parameter strings P1 and P2 is calculated as: Sro(P1, P2) = 2 Km P1 + P2 (18) where Km represents the number of matching characters derived from the longest common subsequence and its recursive sub-segments. If the agent executes the same tool for ten consecutive rounds with parameter similarity Sro > 0.95, the trajectory is classified as Single-Tool Loop. This behavior indicates that the agent is adopting direct yet ineffective sequential scanning approach, which drastically degrades reasoning efficiency and inundates the context with excessive volumes of invalid or redundant query information. Multi-Tool Cyclic Loop. This category identifies non-consecutive but pervasive repetition throughout the entire trajectory. Utilizing the same similarity metric Sro and threshold (> 0.95), we flag trajectory as cyclic loop if similar tool calls appear more than 15 times in total across the interaction history, which is demonstrated in Case 4. This behavior suggests lack of long-term planning, where the agent repeatedly revisits previously explored states or queries in circular manner. Tool Usage Error. This error type reflects hallucination or schema violation regarding the available toolbox. It occurs when the agent attempts to invoke tool that does not exist in the defined toolbox or provides arguments that do not align with the tools required parameter schema, as shown in Case 5. Such failures indicate disconnect between the agents reasoning core and the environmental constraints. No Candidate Tool. Given the benchmarks design, valid predictions must be grounded in the candidate search space. This error is assigned when the agent fails to invoke any candidate-retrieval related tools (e.g., get_candidates_by_keyword) throughout the entire interaction. The absence of such calls guarantees that the final answer will not map to valid entry in the candidate tables, inevitably leading to task failure, which can be seen in see Case 6."
        },
        {
            "title": "F Additional Experiments",
            "content": "F.1 Turn Analysis Following the specific evaluation of the Qwen330B-MoE architecture in Section 5.3, we extend our investigation using the interaction turn heatmap illustrated in Figure 8 to examine the broader impact of varying model capabilities and agent methods. The visualization exposes that baseline approaches exhibit significant sensitivity to the underlying model architecture. Methods such as ReAct and ReasoningBank demonstrate high volatility across different backbones. On weaker models, ReasoningBank specifically suffers from catastrophic stagnation where nearly 500 cases reach the maximum turn limit. This phenomenon indicates that in the absence of an effective memory mechanism, lossy context compression leads to the omission of critical details. Consequently, the agent is forced into redundant information-seeking loops to attempt to recover this lost context. In contrast, RETROSUM maintains consistent distribution of interaction rounds regardless of the backbone model scale. This stability suggests that our retrospective mechanism functions as regularizer which self-standardizes the reasoning trajectory and effectively enhances reasoning efficiency. Conversely, distinct failure modes are observed in alternative self-evolving methods such as ReflecTool, which show tendency toward premature termination. These agents frequently conclude episodes within 10 to 20 rounds, evidenced by ReflecTool-CS resolving 316 cases within this short interval on Qwen3-30B-A3B. In the context of complex clinical diagnostics, such rapid convergence often implies superficial reasoning or failure to conduct necessary differential diagnosis verifications. F.2 Tool Analysis To investigate the behavioral distinctness of RETROSUM, we analyzed the distribution of tool calls across different backbones, as depicted in Figure 9. This breakdown reveals fundamental differences in how agents navigate the clinical decision-making process. Behavior with Different Methods. prominent trend observed in baseline methods, particularly ReAct and Reflexion, is the dominance of the Records tool. On weaker backbones like Qwen3-30B-A3B, Figure 9: Proportional distribution of tool category usage across different agent methods and LLM backbones on the diagnoses task in MIMIC-IV Common. these methods dedicate over 60% of their actions to fetching raw EHR records. This pattern indicates an inefficient information acquisition strategy, where the agent struggles to locate relevant evidence and repeatedly queries the database. In contrast, RETROSUM exhibits significantly more balanced distribution. Notably, the proportion of Candidate tool usage is consistently higher in RETROSUM compared to baselines. This shift suggests that our retrospective mechanism successfully compresses the context, enabling the agent to progress beyond superficial data acquisition. Instead, the agent prioritizes the more advanced stage of active grounding, where it systematically aligns clinical findings against the candidate space to refine its predictions. Behavioral Alignment with Stronger Models. The tool usage patterns also highlight how RETROSUM bridges the capability gap between models. As we observe the transition from smaller models to more powerful reasoning models, there is natural tendency for agents to increase their usage of Candidate and Inner tools, reflecting more purposeful reasoning process. Crucially, RETROSUM enables smaller models to mimic this \"expert\" behavior pattern, exhibiting tool distribution profile that resembles that of GPT-5-mini baselines. This indicates that our framework effectively guides weaker models to adopt the efficient behavioral heuristics naturally found in stronger foundation models. tains steady but moderate usage. This implies that the agent retains sufficient awareness of the database structure through its memory mechanism, reducing the need for redundant schema queries while ensuring precise SQL generation for record retrieval. F.3 Computational Consumption Analysis Beyond predictive performance, the deployment of agents in real-world clinical settings requires rigorous assessment of computational costs and latency. Figure 10 illustrates the resource consumption of RETROSUM compared to baseline methods across three key metrics: execution time, input tokens, and output tokens. Token Economy RETROSUM demonstrates remarkable advantage in input token efficiency. By dynamically compressing the interaction history into concise retrospective summaries, our method reduces the average input tokens per sample to approximately 0.42M, representing 4.9 reduction compared to ReAct (2.06M) and 1.7 reduction compared to ReSum (0.70M). This drastic decrease is critical for cost-sensitive applications, as input tokens typically dominate the inference cost of Large Language Models. Conversely, while RETROSUM generates an average of 10.9k output tokenscomparable to ReSum (12.1k)this \"investment\" in detailed reasoning is significantly outweighed by the massive savings in input context processing. Schema Awareness. Furthermore, the usage of the Table tool provides insight into schema understanding. While some baselines fluctuate in their reliance on schema inspection, RETROSUM mainExecution Latency Despite the additional computational overhead required for generating summaries and evolving experiences, RETROSUM achieves an average execution time of 133.08s, Figure 10: Computational cost analysis across different agent frameworks. We report the average execution time per sample (left), average input tokens per sample (middle), and average output tokens per sample (right). RETROSUM demonstrates superior efficiency, achieving the lowest input consumption ( 4.9 reduction vs. ReAct) while also reducing execution latency compared to standard baselines like ReSum and ReAct. which is notably faster than both ReSum (142.11s) and ReAct (158.66s). This result indicates that the retrospective mechanism successfully optimizes the overall workflowby preventing the agent from getting lost in long, repetitive raw contexts (as seen in ReAct), RETROSUM reduces the total number of interaction turns, thereby achieving superior trade-off that lowers both latency and cost. F.4 Record Retrieved Distribution To elucidate the information-seeking dynamics of different agents, we analyze the distribution of retrieved tables across various clinical tasks. Figure 11 and Figure 12 visualize the access frequency of EHR tables, aggregated by backbone model and agent framework, respectively. Task-Specific Schema Alignment. clear pattern observed across all heatmaps is the strong semantic alignment between the task domain and the retrieved tables. As evidenced in the visualizations, agents predominantly query tables that are intrinsically relevant to the target task. For instance, in the labevents task, the retrieval focus is heavily concentrated on the labevents table. This dominant diagonal pattern confirms that the agents possess fundamental awareness of the database schema and can identify the primary information sources required for specific clinical queries. Impact of Model Capabilities. The aggregation by backbone model reveals distinct retrieval behaviors correlated with model scale. Smaller models, such as the Qwen3-30B-A3B and Qwen3-80BA3B, exhibit pattern of repetitive intensity, concentrating heavily on limited set of tables. This suggests lower efficiency in information extraction, necessitating repeated access to grasp the In contrast, stronger models like GPTcontext. 5-mini and Grok-4.1 display more distributed attention mechanism, synthesizing information from wider array of sources rather than fixating on single table. Specific table preferences further highlight these differences. While all models prioritize labevents, which stores all lab test results including hematology and chemistry, the Qwen series accesses it with excessive frequency compared to the more balanced usage by GPT-5-mini and Grok4.1. Additionally, Grok-4.1 demonstrates unique preference for the triage table, which contains initial emergency department assessment data and vital signs, whereas smaller models rely more heavily on the basic admissions table. In summary, advanced models demonstrate capacity for holistic clinical contextualization, utilizing peripheral data sources like triage to inform their reasoning, while smaller models adhere to more rigid and repetitive focus on core administrative tables. Strategies of Agent Methods. Disaggregating the results by framework reveals how different mechanisms influence information gathering. Methods incorporating summarization capabilities, such as ReSum and RETROSUM, exhibit significantly broader and more uniform query distribution across tables. This indicates that the summarization module facilitates the efficient digestion of table content, encouraging the agent to explore diverse information sources to construct comprehensive patient profile. Conversely, methods incorporating self-evolution and self-reflection, such as Reflexguage models, including ChatGPT 4 and Google Gemini 5. ion, ReflecTool, and RETROSUM Evolved, demonstrate sharper focus. These methods leverage past paradigms to identify and concentrate on the most relevant tables for the specific task. Notably, our approach integrates the advantages of both categories. It supports active exploration to acquire broad context while maintaining task-oriented focus on critical data, preventing the agent from drifting into irrelevant information. Task-Dependent Retrieval Scope. From task perspective, the retrieval patterns validate the multidimensional design of our benchmark. We observe clear distinction between specialized and foundational clinical tasks. Tables associated with prescriptions, procedures, and transfers are accessed almost exclusively during their respecindicating high degree of specitive tasks, In contrast, diagnoses, labevents, and ficity. microbiologyevents serve as foundational patient information sources and are utilized across wider range of scenarios. This implies that the first three tasks evaluate the agents ability to handle specialized, domain-specific queries, while the latter three assess the ability to synthesize comprehensive clinical subjects. This dichotomy ensures that our benchmark provides robust evaluation of clinical capabilities across multiple dimensions of complexity and specificity. F.5 Case Study To provide concrete understanding of RETROSUMs operational logic, we present qualitative examples of its reasoning process and evolving mechanisms. Please note that due to the excessive length of raw EHR observation logs, we employed an LLM to condense the interaction history in these illustrations, preserving the core reasoning logic while ensuring readability. We first present complete inference process in Case 9, demonstrating how RETROSUM navigates the complex EHR environment to conclude clinical task. Furthermore, to isolate the impact of our evolving strategy, we provide specific instances of experience retrieval: Case 7 showcases the application of historical insights by the Actor module, while Case 8 illustrates the effect of retrieved experiences on the Summarizer module."
        },
        {
            "title": "G AI Assistance Statement",
            "content": "Language editing and stylistic refinement of the draft were performed with the aid of large lan4https://chatgpt.com/ 5https://gemini.google.com/app/ t l h d e t a s c i u f c l t g v i i s h . d s n f y t r a l R e r f i i i : 1 1 g . o a s a fi p e e u t t e r s h w , t e m e l P . L e e r r v t i r e o t d n c l s a c q s c b e e n a a v . h t a r i b t r s a E e e n u t : 2 u . w r e c b l c fi p e e u t i s e i d w , t d e s s a . t c f i a l i r fi p t Prompt Prompt 1. Diagnose Task Prompt Your current task is to act as diagnostician. Your objective is to determine all plausible diagnoses for the patients current condition by analyzing the patients complete history. You must find the most likely official CCS candidates using the **diagnoses_ccs_candidates** reference table. Present your final answer as **list format** with finish tool calling, which must contain **multiple plausible diagnoses**. Each item in the list must be string representing an official CCS diagnosis name, and **must not contain any codes or other additional information**. Prompt Prompt 2. Labevents Task Prompt Your current task is to act as laboratory medicine specialist. Your objective is to determine all necessary laboratory tests for the patient by analyzing their complete medical history, current clinical condition, and established diagnoses. You should provide as many laboratory tests as possible to cover the patients current clinical condition. You must find the most likely official laboratory test candidates using the **labevents_candidates** reference table. Present your final answer as **list format** with finish tool calling, which must contain **multiple plausible laboratory tests**. Each item in the list must be string representing an official laboratory test name, and **must not contain any codes or other additional information**. Prompt Prompt 3. Microbiology Task Prompt Your current task is to act as clinical microbiologist. Your objective is to determine all necessary microbiological tests for the patient by analyzing their complete medical history, current clinical condition, established diagnoses, and clinical signs of infection. You must find the most likely official microbiological test candidates using the **microbiologyevents_candidates** reference data or semantic matching tools. Present your final answer as **list format** with finish tool calling, which must contain **multiple plausible microbiological tests**. Each item in the list must be string representing an official microbiological test name, and **must not contain any codes or other additional information**. Prompt Prompt 4. Prescriptions Task Prompt Your current task is to act as pharmacist. Your objective is to determine all necessary ATC therapeutic categories for the patient by analyzing their complete medical history, current clinical condition, and established diagnoses. You must find the most tions_atc_candidates** reference data or semantic matching tools. likely official ATC name candidates using the **prescripPresent your final answer as **list format** with finish tool calling, which must contain **multiple plausible ATC names**. Each item in the list must be string representing an official ATC name, and **must not contain any codes or other additional information**. Prompt Prompt 5. Procedures Task Prompt Your current task is to act as surgical planner. Your objective is to determine all necessary surgical procedures for the patient by analyzing their complete medical history and established diagnoses. likely official CCS procedure candidates using the **proceYou must find the most dures_ccs_candidates** reference table. Present your final answer as **list format** with finish tool calling, which must contain **multiple plausible procedures**. Each item in the list must be string representing an official CCS procedure name, and **must not contain any codes or other additional information**. Prompt Prompt 6. Transfer Task Prompt Your current task is to act as hospital care coordinator and clinical decision-maker. Your objective is to determine the most appropriate care unit for patient transfer by analyzing their current clinical condition, medical history, severity of illness, and care requirements. You must consider the patients current location, clinical stability, required level of monitoring, and specialized care needs to recommend the optimal transfer destination. You must find the most likely official care unit candidates using the **transfers_candidates** reference data or semantic matching tools. Present your final answer as **list format** with finish tool calling, which must contain **multiple plausible care units**. Each item in the list must be string representing an official care unit name, and **must not contain any codes or other additional information**. Prompt Prompt 7. Summarization Prompt You are an expert at analyzing conversation history and extracting relevant information. Your task is to thoroughly evaluate the conversation history and current question to provide comprehensive summary that will help solve the task. ## Task Guidelines 1. Information Analysis: - Carefully analyze the conversation history to identify truly useful information. - Focus on information that directly contributes to answering the question. - Do NOT make assumptions, guesses, or inferences beyond what is explicitly stated in the conversation. - If information is missing or unclear, do NOT include it in your summary. 2. Summary Requirements: - Extract only the most relevant information that is explicitly present in the conversation. - Synthesize information from multiple exchanges when relevant. - Only include information that is certain and clearly stated in the conversation. - Do NOT output or mention any information that is uncertain, insufficient, or cannot be confirmed from the conversation. 3. Output Format: Your response should be structured as follows: <summary> - Essential Information: [Organize the relevant and certain information from the conversation history that helps address the question.] </summary> Strictly avoid fabricating, conversation. Only output information that is certain and explicitly stated. inferring, or exaggerating any information not present in the Question {question} Conversation History {recent_history_messages} Please generate comprehensive and useful summary. Note that you are not permitted to invoke tools during this process. Prompt Prompt 8. Actor Experiences Generation Prompt You are an expert in clinical reasoning auditor. You will be provided with complete post-hoc analysis package of clinical reasoning task performed by an AI Actor agent. Your inputs include: 1. **User Query**: The original clinical question or task. 2. **Prediction Result & Ground Truth**: What the Actor ultimately concluded versus the correct answer. 4. **Complete Raw Trajectory / Actions Taken**: The exact actions the Actor took based on the summaries. ## Guidelines Your task is to analyze the **reasoning quality** of the Actor agent. You need to extract useful insights in the format of memory items that focus on improving future clinical decision-making processes. The goal is to identify where the Actors logic was flawed, overly cautious, too aggressive. ## Important notes - **Focus on the Actors Decisions:** Focus on: Given the information *present* at step T, did the Actor make the most logical action? - **Analyze Reasoning Gaps:** - If failed: Did the Actor jump to conclusion not supported by the retrieved information? Did it ignore conflicting evidence presented in the EHR records? Did it fail to order necessary confirmatory test suggested by the recprds ambiguity? - If successful: What robust reasoning strategy did the Actor use to navigate uncertainty or complex data presented in the summaries? - **Generalizable Reasoning Principles:** Insights should be about *how to think* clinically (e.g., differential diagnosis strategies, handling conflicting data, recognizing urgency), not about specific medical facts. - Do not summarize strategies that are already mentioned in the raw instructions; focus on the implicit reasoning strategies. - You can extract at most max_items memory items. - You must not repeat similar or overlapping items. ## Output Format Your output must strictly follow the Markdown format shown below. Ensure ALL fields (Title, Description, Content) are provided. Required Format: # Memory Item ## Title <short title, max 15 words> ## Description <one sentence summary of the memory item> ## Content <1-3 sentences describing the insights learned to successfully accomplish the task> # Query: {query} # Complete Trajectory: {raw_trajectory} # Prediction Result: {prediction_result} # Ground Truth: {ground_truth} Prompt Prompt 9. Summarizer Experiences Generation Prompt You are an expert medical data summarization auditor. You will be provided with complete post-hoc analysis package of clinical reasoning task. Your inputs include: 1. **User Query**: The original clinical question or task. 2. **Prediction Result & Ground Truth**: Knowing the final outcome is crucial for Hindsight bias analysis. 3. **Complete Raw Trajectory**: The exhaustive, detailed truth (e.g., full EHR tables). 4. **Summarized Trajectory**: The compressed version generated by the Summarizer. ## Guidelines Your task is to conduct **hindsight gap analysis** between the raw data and the generated summaries. You need to extract insights to improve how future Summarizers extract and compress clinical information. ## Important notes - **Hindsight Gap Analysis (Crucial):** You must use the Ground Truth to identify what was truly important in the Raw Trajectory. Then, check if that important information was captured accurately in the Summarized Trajectory. - **Focus on Information Loss/Distortion:** - **What was lost?** Identify critical raw data points (e.g., subtle lab trend, specific timing of medication) that were essential for the correct diagnosis but were omitted or overly generalized in the summary. - **Why it matters?** Explain how omitting this specific type of raw data leads to downstream reasoning errors. - **Generalizable Summarization Principles:** Insights should be rules for *how to summarize* better (e.g., \"Always preserve exact values for abnormal vitals instead of just stating abnormal\", \"Explicitly link temporal relationships between medication administration and subsequent lab changes\"). - You can extract at most max_items memory items. - You must not repeat similar or overlapping items. ## Output Format Your output must strictly follow the Markdown format shown below. Ensure ALL fields (Title, Description, Content) are provided. Required Format: # Memory Item ## Title <short title, max 15 words> ## Description <one sentence summary of the memory item> ## Content <1-3 sentences describing the insights learned to successfully accomplishing the task> # Query: {query} # Complete Trajectory: {raw_trajectory} # Summarized Trajectory: {summarized_trajectory} # Prediction Result: {prediction_result} # Ground Truth: {ground_truth} Case Case 1. No Prediction (Tool Parsing Failure) ... (Previous steps omitted) ## Step 63: - Action: finish{response: <tool_call>n{\"name\": \"get_candidates_by_fuzzy_matching\", \"arguments\": {\"table_name\": \"diagnoses_ccs_candidates\", \"keywords\": [\"myelodysplastic\", \"malignancy\", \"leukemia\", ... [truncated 1000+ keywords] ... \"fraction\", \"EF\", \"} - Observation: Task concluded. No valid prediction format detected in response. # Error Analysis: The agent attempted to generate tool invocation (get_candidates...) but failed to adhere to the syntax, outputting it as raw string within the finish action. The system misinterpreted this as termination signal with invalid content, resulting in zero predictions. Case Case 2. Tool Repeat Error ... (Previous steps 1-20 omitted) ## Step 21: - Action: get_records_by_keyword{subject_id: keyword: vomiting} - Observation: No records found in table admissions containing the keyword vomiting. 19505049, table_name: admissions, ## Step 22: - Action: get_records_by_keyword{subject_id: keyword: vomiting} - Observation: No records found in table admissions containing the keyword vomiting. 19505049, table_name: admissions, ... (Steps 23-29 repeat the exact same action and parameters) ... ## Step 30: - Action: get_records_by_keyword{subject_id: keyword: vomiting} - Observation: No records found in table admissions containing the keyword vomiting. 19505049, table_name: admissions, # Error Analysis: The agent executes identical actions with identical parameters for >5 consecutive turns, failing to update its strategy despite negative feedback. Case Case 3. Single-Tool Loop ... (Previous steps omitted) ## Step 19: - Action: get_records_by_value{subject_id: umn_name: itemid, value: 50889} - Observation: No records found in table labevents where itemid equals 50889. 17332316, table_name: labevents, col- ## Step 20: - Action: get_records_by_value{subject_id: umn_name: itemid, value: 50894} - Observation: No records found in table labevents where itemid equals 50894. 17332316, table_name: labevents, col- ## Step 21: - Action: get_records_by_value{subject_id: umn_name: itemid, value: 50883} - Observation: labevent_id ... 50883 ... Bilirubin, Direct ... [5 rows retrieved] 17332316, table_name: labevents, col- ## Step 22: - Action: get_records_by_value{subject_id: umn_name: itemid, value: 50882} - Observation: ... [Massive table data: Bicarbonate results, 16,427 chars truncated] ... 17332316, table_name: labevents, col- (Steps 23-28 continue querying itemid with values 50881, 50884, 50885, 50890, ... etc.) ... # Error Analysis: The agent performs sequential scanning with high parameter similarity (Sro > 0.95). Instead of filtering by time or flag, it blindly guesses IDs, inundating the context with excessive raw data while making negligible reasoning progress. Case Case 4. Multi-Tool Cyclic Loop ... (Previous steps omitted) Cycle Iteration 1 ## Step 29: - Action: get_records_by_value{subject_id: column_name: drug, value: Sodium Chloride} - Observation: No records found in table prescriptions where drug equals Sodium Chloride. 12593903, table_name: prescriptions, ## Step 30: - Action: get_records_by_value{subject_id: column_name: drug, value: D5 1/2NS} - Observation: ... [Prescription found: 1000mL Bag D5 1/2NS ...] 12593903, table_name: prescriptions, Cycle Iteration 2 (Repeating steps 29-30) ## Step 41: - Action: get_records_by_value{subject_id: column_name: drug, value: Sodium Chloride} - Observation: No records found in table prescriptions where drug equals Sodium Chloride. 12593903, table_name: prescriptions, ## Step 42: - Action: get_records_by_value{subject_id: column_name: drug, value: D5 1/2NS} - Observation: ... [Same prescription record retrieved again] 12593903, table_name: prescriptions, (Agent continues cycling through Carbohydrates, Glucose, 1/2 NS repeatedly) ... ... Cycle Iteration (Still stuck at end of trajectory) ## Step 99: - Action: get_records_by_value{subject_id: column_name: drug, value: Sodium Chloride} - Observation: No records found in table prescriptions where drug equals Sodium Chloride. 12593903, table_name: prescriptions, # Error Analysis: The agent exhibits non-consecutive but pervasive cyclic pattern. Similar tool calls (Sro > 0.95) appear >15 times, indicating failure in long-term planning and state tracking. Case Case 5. Tool Usage Error ... (Previous steps omitted) ## Step 52: - Action: get_records_by_value{subject_id: 13762777, table_name: radiology_detail, column_name: hadm_id, value: 22827736} - Observation: Error: Column hadm_id not found in table radiology_detail. # Error Analysis: The agent committed schema violation by attempting to query the radiology_detail table using non-existent column hadm_id. This indicates failure to verify table metadata before execution, resulting in runtime error. Case Case 6. No Candidate Tool Failure ... (Previous 98 steps omitted: The agent repeatedly used action get_records_by_keyword) ## Step 99: - Action: get_records_by_keyword{subject_id: 16002903, table_name: radiology,keyword: postoperative} - Observation: note_id: 16002903-RR-10, subject_id: 16002903, hadm_id: 28925885.0, charttime: 2118-04-03 12:54:00, text: INDICATION: Status post right upper lobe wedge resection. COMPARISON: ... [remainder omitted] ## Step 100: - Action: get_records_by_keyword{subject_id: keyword: wedge resection} - Observation: EXAMINATION: CHEST (PORTABLE AP) INDICATION: ___ year old woman with decline in resp function post-op thoracotomy and wedge resection, now reintubated // Please eval interval change. COMPARISON: ... [remainder omitted] 16002903, table_name: radiology, ## Step 101: - Action: finish{response: [Lung cancer, Postoperative status after right upper lobe wedge resection, Pleural effusion, Pneumothorax, Chronic obstructive pulmonary disease]} - Observation: [Lung cancer, Postoperative status after right upper lobe wedge resection, Pleural effusion, Pneumothorax, Chronic obstructive pulmonary disease] # Error Analysis: The agent directly outputted free-text descriptions. It failed to invoke get_candidates_by_... to map these findings to standardized CCS codes, rendering the prediction invalid. Case Case 7. Actor Experience ## Prioritize High-Confidence Semantic Matches Over Fuzzy Search - Description: When using semantic search, prioritize results with high similarity scores and validate against clinical plausibility rather than accepting low-similarity matches as valid. - Content: The Actor accepted \"Substance-related disorders\" and \"Headache; including migraine\" based on moderate semantic similarity, but failed to recognize that the highest-scoring match for \"barbiturate dependence\" was \"Alcohol-related disorders\" (0.580), which is more accurate and clinically relevant diagnosis. Relying on fuzzy or low-similarity matches can lead to misdiagnosis. ## Distinguish Between Primary Diagnosis and Secondary Findings - Description: Do not equate positive drug screen (e.g., tricyclics) with primary diagnosis of poisoning; consider the context of chronic use vs. acute toxicity. - Content: The Actor listed \"Poisoning by psychotropic agents\" as diagnosis based on positive tricyclic screen, but this reflects medication use, not acute poisoning. In the absence of symptoms of overdose, this should be considered finding, not primary diagnosis, and should not be included in the final list of plausible conditions. ## Avoid Over-Interpreting Negative Screens in Context of Withdrawal - Description: negative barbiturate screen during withdrawal is expected and does not rule out history of dependence; it should not be used to infer current active use. - Content: The Actor correctly noted the negative barbiturate screen but did not fully integrate this into the clinical picture2014this is normal in withdrawal, not sign of resolution. The focus should remain on the history of dependence, not the current test result, which is not diagnostic of current state. ## Use Temporal Context to Distinguish Acute from Chronic Conditions - Description: Acute conditions must be supported by recent, active findings; past diagnoses with no current evidence should not be reasserted as current conditions. - Content: The patients past diagnosis of \"drug withdrawal syndrome\" (2174-12-30) and \"barbiturate dependence\" (2174-12-30) are outdated. The current stable vitals and lack of withdrawal symptoms suggest these are resolved, not active. The Actor failed to account for time since last event, leading to overestimation of current relevance. ## Integrate Comorbidities with Core Diagnoses, Not as Standalone Entries - Description: Alcohol use (elevated ethanol) and substance use are comorbidities, not independent diagnoses; they should be grouped under single, broader condition when appropriate. - Content: The elevated ethanol level (83 mg/dL) and history of barbiturate dependence point to pattern of substance use, but the correct approach is to group them under single overarching diagnosis like \"Substance-related disorders\" rather than creating multiple separate entries, especially when the system already has unified category. ## Leverage ICD-10 Code Mapping to Validate Diagnoses - Description: When diagnosis is suspected, cross-check the ICD-10 code to ensure it aligns with the most specific and correct category, not just general one. - Content: The ground truth includes \"F10129\" (Alcohol-related disorders) and \"F17200\" (Cocaine-related disorders), which are more specific than the generic \"Substance-related disorders\" (F1910). The Actor should have used the ICD-10 code mapping to identify the most precise diagnosis, not default to broad category. Case Case 8. Summarizer Experience ## Preserve exact ICD codes for precise diagnosis mapping - Description: Failing to map ICD-9 codes to their correct ICD-10 equivalents can result in missing critical diagnoses. - Content: The ground truth includes \"Alcohol-related disorders\" (F10129), which was not identified in the summary despite being high-similarity match (0.580) for barbiturate dependence (30410). The summary incorrectly prioritized \"Substance-related disorders\" over the more specific \"Alcohol-related disorders leading to key diagnostic omission. Summarizers must map ICD-9 codes to their most accurate ICD-10 counterparts using official crosswalks, not just semantic similarity. ## Prioritize contextually relevant ICD-10 codes over general categories - Description: General CCS categories like \"Substance-related disorders\" may mask more specific, clinically significant diagnoses. - Content: The ground truth lists two distinct substance-related ICD-10 codes (F1910 and F17200) that are more specific than the generic \"Substance-related disorders\" used in the prediction. The summary failed to extract these specific codes, likely due to over-reliance on broad semantic matches. Summarizers should flag and include all high-similarity, specific ICD-10 candidates, especially when multiple variants exist. ## Capture comorbidities from past history with clinical context - Description: Past diagnoses of substance use and withdrawal are not standalone; they often co-occur with mood disorders and alcohol-related conditions. - Content: The patient2019s history of drug withdrawal (2920) and barbiturate dependence (30410) strongly suggests an underlying mood disorder, which is confirmed in the ground truth as \"Mood disorders\" (F339). The summary did not link this history to mood pathology, instead focusing only on current lab findings. Summarizers must infer and include comorbid conditions based on chronic substance use patterns. ## Use temporal context to validate current vs. past conditions - Description: recent positive tricyclic screen indicates current use, not just past history, and should be linked to active diagnosis. - Content: The tricyclic antidepressant screen (15 mg/dL) was correctly noted as positive, but the summary treated it as static finding without linking it to the current state. The ground truth includes \"Substance-related disorders\" (F17200) for barbiturate use, but the tricyclic use is different class of psychotropic agent. The summary should have flagged this as new, active condition requiring its own diagnostic code, not just semantic match. ## Explicitly report abnormal values with units and reference ranges - Description: Omitting or generalizing abnormal lab values can lead to misinterpretation of clinical status. - Content: The ethanol level of 83 mg/dL was reported as \"above normal\" but not compared to the reference range (70100 mg/dL), which is crucial for diagnosing alcohol use. The summary generalized this to \"may indicate recent alcohol use\" without stating the exact value and range. Summarizers must always preserve the numeric value, unit, and reference range to support accurate clinical inference. Case Case 9. RETROSUM Reasoning Sample (Observation Cleaned) # Task Information - Task: Diagnoses - SubjectID: 19505049 - TimeStamp: 2174-04-07 16:53:00 - Label: [Secondary malignancies, Phlebitis; thrombophlebitis and thromboembolism, Fluid and electrolyte disorders, Secondary malignancies, Other gastrointestinal disorders, Coagulation and hemorrhagic disorders, Thyroid disorders, Mood disorder, Screening and history of mental health and substance abuse codes, Cardiac dysrhythmias, Residual codes; unclassified, Cancer of bronchus; lung, Residual codes; unclassified, Residual codes; unclassified, Phlebitis; thrombophlebitis and thromboembolism, Other aftercare, Cancer of other GI organs; peritoneum] # Reasoning Trajectory ## Step 1 - Action: think{response: EHR loaded. Plan: 1) Inspect admissions/patient/demographics and timeline. 2) Review diagnoses candidate table. 3) Extract clinical events (notes, labs, radiology, prescriptions). 4) Cross-reference with CCS candidates. 5) Synthesize plausible diagnoses list.} - Observation: Thinking Finish ## Step 2 - Action: get_table_description{table_name: admissions} - Observation: Table: admissions (hosp). Contains details for each hospital admission (demographics, timing, source, discharge info). Key Columns: subject_id, hadm_id, admittime, dischtime, admission_type, diagnosis, hospital_expire_flag. (Steps 3-9: Retrieved table descriptions for patients, omr, labevents, radiology, prescriptions, discharge - Omitted for brevity) ## Step 10 - Action: Summary{response: Plan: Inspect admission details, vital signs, lab results, imaging, and discharge notes to build diagnosis list.} - Observation: OK ## Step 11 - Action: get_records_by_value{subject_id: 19505049, table_name: admissions, column_name: subject_id, value: 19505049} - Observation: Admission Details: - Subject ID: 19505049 Hadm ID: 21468882 - Timeline: Admitted 2174-04-01 (Emergency) -> Discharged 2174-04-07 - Demographics: Male, Married, White, English - Chief Complaint: Abdominal pain - History: Ampullary cancer s/p Whipple (18mo ago), liver mets. Presenting with worsening epigastric pain, weight loss (65lbs), and constipation. ## Step 12 - Action: get_records_by_time{subject_id: 19505049, table_name: omr, start_time: 2173-01-01, end_time: 2174-04-07 16:53:00} - Observation: Outpatient Vitals Trend: - 2173-04: BP 160/72, Wt 225.5 lbs - 2173-09: BP 150/70, Wt 200 lbs - 2174-03: BP 100/58, Wt 174 lbs (Significant weight loss noted) ## Step 13 - Action: get_records_by_time{subject_id: 19505049, table_name: labevents, start_time: 2174-04-01, end_time: 2174-04-07 16:53:00} - Observation: Query returned 93,885 records. Key Abnormalities extracted in next step. ## Step 14 - Action: run_sql_query{subject_id: 19505049, sql_query: \"SELECT label, value, valuenum, flag, charttime FROM labevents WHERE subject_id=19505049 AND charttime BETWEEN 2174-04-01 AND 2174-04-07...\"} - Observation: Key Lab Results (2174-04-03 to 04-04): - Hematology: Hgb 8.9-9.0 (Anemia), Hct 26.4-28.1 L, Plt 83-100 (Thrombocytopenia). - Chemistry: Na 127-130 (Hyponatremia), 5.4 (hyperkalemia, normalized later), Glucose 95-115. - Coagulation: INR 1.1, PTT 36.1 H. - Renal: Creatinine 0.7-0.8 (Normal). ## Step 15 - Action: get_records_by_time{subject_id: 19505049, table_name: radiology, start_time: 2174-04-01, end_time: 2174-04-07 16:53:00} - Observation: CT Abdomen w/ Contrast (2174-04-01): - Impression: Increased size necrotic periportal mass ( 4.5cm) with likely invasion into small bowel and encasement of SMA/SMV. - Liver: Multiple hypodense lesions consistent with metastases (enlarged from prior). - Vascular: Stable thrombus of the IVC. ## Step 16 - Action: get_records_by_time{subject_id: 19505049, table_name: discharge, start_time: 2174-04-07, end_time: 2174-04-07 16:53:00} - Observation: Discharge Summary: - Primary Diagnosis: Metastatic ampullary carcinoma, Abdominal Pain, Constipation. - Secondary: IVC thrombosis, Chronic anemia, Hypothyroidism, A-fib, Depression. - Hospital Course: Pain controlled with methadone. Anticoagulation (Coumadin/Heparin) discontinued due to GI bleed risk and thrombocytopenia. Palliative care consulted; patient referred to hospice. get_candidates_by_semantic_similarity{query: ## Step 17 - Action: thrombosis, anemia, depression, ...], table_name: diagnoses_ccs_candidates} - Observation: Top CCS Candidates Found: - Cancer of pancreas (0.574), Secondary malignancies (0.702) - Phlebitis; thrombophlebitis and thromboembolism (0.592) - Deficiency and other anemia (0.845) - Coagulation and hemorrhagic disorders (0.641) - Thyroid disorders (0.775), Cardiac dysrhythmias (0.761), Mood disorders (0.647) [ampullary carcinoma, ivc ## Step 18 - Action: get_records_by_time{subject_id: 19505049, table_name: prescriptions, start_time: 2173-01-01, end_time: 2174-04-07 16:53:00} - Observation: Active Medications: - Pain/GI: Methadone, Docusate, Senna, Lactulose, Ranitidine. - Cardiac: Diltiazem, Digoxin. - Other: Levothyroxine, Vitamin D, Citalopram. - Discontinued: Heparin (Infusion stopped 2174-04-03). ## Step 19 - Action: think{response: Synthesizing evidence: CT confirms metastatic mass and IVC thrombus. Labs show anemia/thrombocytopenia. History supports hypothyroidism/A-fib/Depression. Mapping findings to standard CCS categories.} - Observation: Thinking Finish ## Step 20 - Action: finish{response: [Cancer of pancreas, Secondary malignancies, Phlebitis; thrombophlebitis and thromboembolism, Deficiency and other anemia, Coagulation and hemorrhagic disorders, Abdominal pain, Other gastrointestinal disorders, Thyroid disorders, Cardiac dysrhythmias, Mood disorders]} - Observation: [Cancer of pancreas, Secondary malignancies, Phlebitis; thrombophlebitis and thromboembolism, Deficiency and other anemia, Coagulation and hemorrhagic disorders, Abdominal pain, Other gastrointestinal disorders, Thyroid disorders, Cardiac dysrhythmias, Mood disorders] # Scores - F1 Score: 0.608695652173913 - Precision: 0.7 - Recall: 0."
        }
    ],
    "affiliations": [
        "Shanghai Artificial Intelligence Laboratory",
        "Shanghai Jiao Tong University"
    ]
}