{
    "paper_title": "SeeingEye: Agentic Information Flow Unlocks Multimodal Reasoning In Text-only LLMs",
    "authors": [
        "Weijia Zhang",
        "Zijia Liu",
        "Haoru Li",
        "Haoqi Chen",
        "Jiaxuan You"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Recent advances in text-only large language models (LLMs), such as DeepSeek-R1, demonstrate remarkable reasoning ability. However, these models remain fragile or entirely incapable when extended to multi-modal tasks. Existing approaches largely rely on single-form captions, which lack diversity and often fail to adapt across different types of Visual Question Answering (VQA) benchmarks. As a result, they provide no principled or efficient channel for transmitting fine-grained visual information. We introduce Seeing Eye, a modular framework that unlocks multimodal reasoning in text-only LLMs through an agent-based small VLM translator. This translator acts as a perception agent: it can invoke specialized tools (e.g., OCR and crop) and iteratively distill multimodal inputs into structured intermediate representations (SIRs) tailored to the question. These SIRs are then passed to the text-only LLM, which serves as a reasoning agent. Crucially, the translator and reasoner engage in multi-round feedback and interaction, enabling the extraction of targeted visual details and yielding more confident answers. Experiments on knowledge-intensive VQA benchmarks, including MMMU and MIA-Bench, demonstrate that Seeing Eye not only reduces inference cost but also surpasses much larger end-to-end VLMs. For example, an instantiation combining a 3B-parameter vision translator with an 8B-parameter language reasoner outperforms a monolithic 32B VLM on challenging knowledge-based questions. Our results highlight that decoupling perception from reasoning via agent information flow offers a scalable and plug-and-play pathway to multimodal reasoning, allowing strong text-only LLMs to fully leverage their reasoning capabilities. Code is available at: https://github.com/ulab-uiuc/SeeingEye"
        },
        {
            "title": "Start",
            "content": "5 2 0 2 9 2 ]"
        },
        {
            "title": "A\nM",
            "content": ". [ 1 2 9 0 5 2 . 0 1 5 2 : r SEEINGEYE: AGENTIC INFORMATION FLOW UNLOCKS MULTIMODAL REASONING IN TEXT-ONLY LLMS Weijia Zhang, Zijia Liu, Haoru Li, Haoqi Chen, Jiaxuan You University of Illinois Urbana-Champaign {weijia4,zliu331,haorul2,haoqic2,jiaxuan}@illinois.edu"
        },
        {
            "title": "ABSTRACT",
            "content": "Recent advances in text-only large language models (LLMs), such as DeepSeekR1, demonstrate remarkable reasoning ability. However, these models remain fragileor entirely incapablewhen extended to multimodal tasks. Existing approaches largely rely on single-form captions, which lack diversity and often fail to adapt across different types of Visual Question Answering (VQA) benchmarks. As result, they provide no principled or efficient channel for transmitting fine-grained visual information. We introduce SeeingEye, modular framework that unlocks multimodal reasoning in text-only LLMs through an agent-based small VLM translator. This translator acts as perception agent: it can invoke specialized tools (e.g., OCR and crop) and iteratively distill multimodal inputs into structured intermediate representations (SIRs) tailored to the question. These SIRs are then passed to the text-only LLM, which serves as reasoning agent. Crucially, the translator and reasoner engage in multiround feedback and interaction, enabling the extraction of targeted visual details and yielding more confident answers. Experiments on knowledge-intensive VQA benchmarks, including MMMU and MIA-Bench, demonstrate that SeeingEye not only reduces inference cost but also surpasses much larger end-to-end VLMs. For example, an instantiation combining 3B-parameter vision translator with an 8B-parameter language reasoner outperforms monolithic 32B VLM on challenging knowledge-based questions. Our results highlight that decoupling perception from reasoning via agent information flow offers scalable and plug-and-play pathway to multimodal reasoning, allowing strong textonly LLMs to fully leverage their reasoning capabilities. Code is available at: https://github.com/ulab-uiuc/SeeingEye"
        },
        {
            "title": "INTRODUCTION",
            "content": "Recent text-only LLM reasoners, such as DeepSeek-R1, have demonstrated remarkable text-only reasoning, pushing the frontiers of artificial intelligence in tasks from code generation to complex problem-solving Brown et al. (2020); Guo et al. (2025); Liu et al. (2025); Han et al. (2025). Compared to multimodal reasoners, they enjoy wide adoption and cost efficiency, but lack multimodal reasoning capabilities. It thus exists central research question: can we bridge text-only LLM reasoners with multimodal reasoning capabilities that are effective and more cost-efficient than multimodal reasoning models? common paradigm to answer this question has centered on converting the visual input into text. Early approaches relied on generating static, single-form captions, from general descriptions to more query-focused variants Khademi et al. (2023); Ozdemir & Akagunduz (2024); Ma et al. (2024). However, these non-interactive descriptions lack the adaptability for diverse VQA tasks and create fixed information bottleneck. Recognizing this, more recent works introduce dynamic primitives like tool use Wu et al. (2023) or integrated active perception Wu & Xie (2024). While significant step forward, these methods present new limitations: the information flow is often an unstructured Indicates equal contribution. 1 conversational history of tool calls, or the perception and reasoning modules are tightly coupled within monolithic VLMs. Such architectures are difficult to scale and cannot easily leverage the distinct, rapidly advancing power of state-of-the-art text-only reasoners. Consequently, even advanced systems still lack formal, structured medium for information exchangean efficient channel that allows powerful, text-only reasoning agent to iteratively query and comprehend visual information. Motivated by these limitations, we argue the key to unlocking multimodal reasoning in text-only LLMs is not to simply describe, but to actively translate. We introduce SeeingEye, novel, modular framework that reconceptualizes the vision component as an agent-based translator rather than passive descriptor. The Translator interacts with the visual input by invoking specialized tools, such as OCR for text extraction or cropping for targeted inspection, to iteratively distill the complex scene into novel Structured Intermediate Representation (SIR) (See Fig. 3) to preserve as much valuable information as possible across modalities. Then, based on the input question, the Translator automatically selects appropriate tools and dynamically adjusts its execution steps, ultimately generating the SIR in various forms tailored to the problem-solving process. Crucially, the process is not unidirectional; the reasoning agent can provide feedback to the Translator, requesting clarifications that prompt further tool use to refine the SIR. This multi-round interaction creates targeted information flow that extracts the precise visual evidence needed to arrive at confident answer. Through comprehensive experiments on knowledge-intensive VQA benchmarks like MMMU and MIA-Bench, we demonstrate that our agent-based, modular system (e.g., 3B VLM translator + 8B LLM reasoner) not only reduces inference costs but also surpasses the performance of much larger, monolithic end-to-end VLMs (e.g., 32B model). Our core contributions are as follows: We propose SeeingEye, novel, plug-and-play framework that unlocks the multimodal reasoning capabilities of powerful, pre-existing text-only LLMs without requiring any modification to their architecture. Structured Intermediate Representation (SIR), creating targeted information channel that delivers precise visual evidence to the text-only reasoner. We design novel Agentic Information Flow, where translator agent autonomously selects tools based on the VQA task. This multi-round interaction generates and refines the SIR, being effective and cost-efficient. Overall, our results highlight scalable pathway to advanced multimodal reasoning, liberating strong text-only LLMs to fully leverage their powerful reasoning capabilities on visual data. We are releasing our code to facilitate future research in this direction."
        },
        {
            "title": "2 RELATED WORK",
            "content": "The Evolving Landscape of Visual Question Answering. The VQA landscape has rapidly evolved from simple recognition towards multifaceted reasoning. While benchmarks like GQA Hudson & Manning (2019) introduced critical challenges in compositional and spatial reasoning, recent wave of datasets tests more specialized, expert-level capabilities. These include reasoning with college-level knowledge (MMMU Yue et al. (2024a)), across multilingual contexts (M3Exam Zhang et al. (2023b)), through complex, layered instructions (MIA-Bench Qian et al. (2024)), and within specific domains like chart analysis (EncQA Mukherjee et al. (2025)). This task diversity exposes the limitations of monolithic models that use fixed visual encoding strategy Agrawal et al. (2022); Ke et al. (2025). Such static approaches create an information bottleneck, failing to distill the precise visual details required for each unique challenge and thus motivating our adaptive, agent-based translation framework. Structured Representations for Multimodal Reasoning. critical bottleneck in existing multimodal systems is the conversion of rich visual scenes into coarse textual summaries, such as generic captions or unordered OCR text Alayrac et al. (2022); Li et al. (2023), which discards vital semantic and spatial relationships. Insights from the text-only domain have shown that structured inputssuch as JSON schemas, key-value pairs, or knowledge graphs Cheng et al. (2024); Sun et al. (2023)significantly enhance an LLMs reasoning capabilities by making relationships explicit and reducing ambiguity. Our work operationalizes this principle for the visual domain. We 2 Figure 1: The Agentic Information Flow of our SeeingEye framework. The process begins with the Translator Agent (left), which takes the previous states SIR and external feedback to perform Visual Chain-of-Thought (VCoT) analysis. It uses tools to gather new visual evidence, reflects on the results, and iteratively refines the SIR. If the information is deemed sufficient (PASS), the improved SIR is passed to the Text Reasoning Agent (right). The Reasoner performs its own CoTdriven analysis and tool use. Based on its confidence, it either produces final answer (PASS) or generates targeted feedback (FAIL), initiating new outer loop iteration for the Translator to gather more specific visual cues. propose the Structured Intermediate Representation (SIR) as rich, deliberate communication channel that bridges the gap between visual perception and high-fidelity reasoning. Agentic Frameworks and Visual Chain-of-Thought. LLM-based agents have demonstrated powerful abilities in planning, tool use, and interactive reasoning Yao et al. (2023); Shinn et al. (2023). This agentic paradigm, often augmented by Chain-of-Thought (CoT) prompting Wei et al. (2022) to improve reasoning transparency, has been extended to the multimodal domain. In Visual CoT, models generate step-by-step textual rationales to ground their reasoning in visual evidence Zhang & Zhang (2023); Rose et al. (2023). Our work advances this concept from generating linear, unstructured rationales to more sophisticated, agent-driven process. The translator agent in our SeeingEye framework engages in multi-round Agentic Information Flow, dynamically constructing and refining structured representation (our SIR) through feedback loop with the reasoner, enabling more targeted and adaptive problem-solving strategy."
        },
        {
            "title": "3 METHOD",
            "content": "Our proposed framework, SeeingEye, unlocks the multimodal reasoning capabilities of text-only LLMs by introducing decoupled, two-agent system. This system comprises Translator Agent (AT ) and Reasoning Agent (AR). The core of our method lies in novel Agentic Information Flow, orchestrated through nested loop structure, where the agents collaborate by iteratively improving central Structured Intermediate Representation (SIR) (see details in Fig. 3 and Sec. A.2). Figure 1 provides high-level illustration, while Algorithm 1 presents the formal specification of this interactive process."
        },
        {
            "title": "3.1 THE TRANSLATOR AGENT: GROUNDED VISUAL ANALYSIS",
            "content": "The Translator Agent, AT , is lightweight VLM responsible for converting raw pixel data into rich, structured, and query-relevant format. Its state at the start of outer-loop iteration is defined by the input image I, the question Q, the SIR from the previous iteration Si1, and feedback from 3 Algorithm 1 SeeingEye: Agentic Information Flow aT TranslatorPolicy(VCoT(I, Q, Scurrent)) if aT is ToolCall(oT , args) then rT ExecuteTool(oT , args) Scurrent RefineSIR(Scurrent, rT ) break Si Scurrent else if aT is TerminateSIR then Scurrent Si1 for = 1 MAX STEPS do 1: Input: question Q, options O, image 2: Parameters: MAX ITERS, MAX STEPS 3: Initialize: S0 null, Afinal null 4: for = 1 MAX ITERS do 5: 6: 7: 8: 9: 10: 11: 12: 13: 14: 15: 16: 17: 18: 19: 20: 21: 22: 23: 24: 25: if Afinal is null then 26: 27: return Afinal for = 1 MAX STEPS do Si Si break Afinal A; goto 27 aR ReasonerPolicy(CoT(Si, Q, O)) if aR is ToolCall(oR, args) then rR ExecuteTool(oR, args) Afinal ForceAnswer(SMAX ITERS, Q, O) else if aR is TerminateFeedback(F ) then else if aR is TerminateAnswer(A) then Translator Agent Inner Loop Reasoning Agent Inner Loop the Reasoner Fi1. The agents goal is to produce an improved SIR, Si, through multi-step inner loop. Visual Chain-of-Thought (VCoT) Analysis. At each inner step j, the agent generates Visual Chain-of-Thought (VCoT) Zhang et al. (2023a), textual thought process c(j) describing its direct visual observations and its reasoning for the next action. = VCoT(I, Q, Si1, Fi1, h(j1) c(j) ) (1) where h(j1) is the history of actions within the current inner loop. Adaptive Tool Selection and Execution. Guided by its VCoT, AT selects tool o(j) toolset OT via its policy πT , and executes it to yield result r(j) . from its πT (c(j) o(j) , I, Q, Si1, Fi1, h(j1) ) (2) Among the tools in OT , SmartGridCaption is specialized sub-routine designed for complex spatial queries that require targeted analysis. As illustrated in our case study (Figure 2), the tool is invoked when direct visual observation proves insufficient. Initially, the agent generates global SIR describing church building but cannot identify the animal in the poster from this coarse view (Step 1). To resolve this ambiguity, the tool first discretizes the image into 4 4 grid. It then leverages vision-LLM to interpret the query and select the most informative patches, in this instance, the rectangular region [9, 9] containing the poster (Step 2). detailed caption is then generated for this specific crop, yielding the critical observation: Poster featuring person holding dove (Step 3). Crucially, this new, fine-grained detail is not treated in isolation. As shown in the Integration Process, it is strategically integrated with the previous global description to create an updated, more comprehensive SIR. The SIR thus evolves from generic overview to targeted representation containing the precise fact needed for the query. This procedure of targeted refinement effectively transforms vague spatial query into high-confidence textual fact, enabling the Reasoning Agent to deduce the final answer with ease (Step 4). Figure 2: detailed case study of the SmartGridCaption tool. (1) An initial global SIR fails to identify the animal in the poster. (2) The tool grids the image and locates the relevant patch [9, 9]. (3) fine-grained patch caption is generated and integrated, updating the SIR with the crucial detail of dove. (4) This refined SIR enables the Reasoning Agent to provide high-confidence final answer in single iteration. Iterative SIR Refinement and Termination. core feature of our framework is the iterative refinement of the SIR within the Translators inner loop. After each tool use, the agent reflects on its VCoT and the tool result to update the SIR. Let S(j1) be the SIR at the beginning of the step; the refinement process is: S(j) = RefineSIR(S(j1), c(j) , r(j) ) the agent quantitatively assesses the completeness of the updated Following this refinement, SIR, producing confidence score c(j) = AssessSufficiency(S(j)). This score is compared against predetermined sufficiency threshold, τT . The inner loop terminates by invoking the TerminateAndOutputSIR tool if this confidence exceeds the threshold (c(j) τT ) or if the maximum step limit NT is reached. Upon termination, the agent outputs the final SIR for the outer loop, Si = S(j), along with categorical confidence level CT {low, mid, high} derived from the final score. If confidence is below the threshold and steps remain, the agent continues its inner loop to gather more visual information. (3)"
        },
        {
            "title": "3.2 THE REASONING AGENT: HIGH-LEVEL COGNITION AND DECISION-MAKING",
            "content": "The Reasoning Agent, AR, is powerful text-only LLM whose state for iteration consists of the SIR Si, the question Q, and short-term memory MR,i1 summarizing its prior actions. It leverages the SIR for high-level reasoning and to decide on terminal action. SIR-Grounded Analysis and Tool Use. The Reasoner initiates its own inner loop, generating chain-of-thought c(k) from its distinct toolset OR (e.g., CodeInterpreter, Search). and using its policy πR to select tool o(k) = CoT(Si, Q, MR,i1, h(k1) c(k) πR(c(k) o(k) , Si, Q, MR,i1, h(k1) R ) ) (4) (5) The tool result r(k) is appended to its inner-loop history h(k) . 5 Terminal Decision Policy. The Reasoning Agents decision-making is also governed by confidence-threshold mechanism. After each inner-loop step (which includes its own CoT and optional tool use), the agent assesses its ability to answer the question based on its current reasoning history h(k) . This score is compared against high-confidence answering threshold, τR. , yielding confidence score c(k) If confidence is high (c(k) τR) or if the final outer iteration (i = MAX ITERS) is reached, the agent is compelled to execute the TerminateAndAnswer action. It generates the final answer A, and the process terminates. If confidence is low (c(k) < τR), the agents decision policy πD makes an autonomous choice. It can either continue its inner reasoning loop (if < NR) to further analyze the SIR or use more textual tools, or it can execute the TerminateAndAskTranslator action. This choice is formalized as: afinal πD(Si, Q, h(k) ) (6) where afinal {ContinueReasoning, TerminateAndAskTranslator}. Choosing the latter synthesizes feedback query Fi specifying the missing visual information, which is passed back to the Translator Agent to initiate new outer loop iteration."
        },
        {
            "title": "4 EXPERIMENTS",
            "content": "We conduct series of experiments to evaluate the effectiveness of our SeeingEye framework. Our evaluation is designed to answer central research question: How does our proposed Translatorbased Agentic Information Flow, designed to unlock the reasoning capabilities of text-only LLMs, compare in terms of performance and efficiency against state-of-the-art monolithic VLMs and other advanced agent-based approaches?"
        },
        {
            "title": "4.1 EXPERIMENTAL SETTINGS",
            "content": "Benchmarks. We evaluate our framework on suite of challenging, reasoning-centric Visual Question Answering (VQA) benchmarks that require deep understanding of visual details, text, and domain-specific knowledge. MMMU Yue et al. (2024a): massive, multi-discipline multimodal benchmark featuring questions from college-level exams across six core disciplines, requiring expert-level knowledge and reasoning. We report on the validation set. MMMU-Pro Yue et al. (2024b): more challenging successor to MMMU, curated by human experts to feature more complex reasoning chains and reduce annotation artifacts. We evaluate on both the Standard and Vision subsets. OCR-BenchV2 Fu et al. (2024): comprehensive benchmark for evaluating OCR capabilities in the wild, testing the models ability to read and interpret text from diverse and complex scenes. MIA-Bench Qian et al. (2024): Multimodal Instruction-following and Analysis benchmark designed to assess models ability to follow complex instructions that require comparing, calculating, and reasoning over multiple image regions. Baselines. We compare our method against two categories of leading models: End-to-End VLMs: These are monolithic, state-of-the-art models that process image and text inputs in unified architecture. We include LLaVA-1.5 (7B) Liu et al. (2024), widely-used open-source VLM; several variants of Qwen2.5-VL Bai et al. (2023), powerful series of VLMs; and GPT-4o-mini, highly capable multimodal model from OpenAI Achiam et al. (2023). These models represent the dominant paradigm and serve as direct point of comparison for overall performance. Advanced Modular Frameworks: We also compare against recent methods that share our motivation of moving beyond static image descriptions. V* Wu & Xie (2024) incorporates 6 guided visual search mechanism to actively seek information within an image. OpenManus mannaandpoem et al. (2025) is comprehensive open-source toolkit for building multimodal agents. To create robust baseline that isolates the impact of our agentic flow, we adapt this toolkit using Qwen3-8B as the base model. We integrate our own meticulously designed suite of prompts, textual tools, and termination logic, while equipping it with powerful set of visual tools, even including the Qwen2.5-VL model as callable visual analysis tool. These baselines allow us to compare different approaches to dynamic visual interaction and reasoning. Implementation Details Our model, referred to as SeeingEye, is instantiated using 3B parameter Vision Language Model (Qwen2.5-VL) as the Translator Agent (AT ) and an 8B parameter text-only Large Language Model (Qwen3) as the Text Reasoning Agent (AR). For our experiments, the inner loops for both agents (NT , NR) and the outer feedback loop (Nouter) are each capped at maximum of 3 iterations to ensure computational tractability."
        },
        {
            "title": "4.2 MAIN RESULTS",
            "content": "The main results of our experiments are presented in Table 1. Our SeeingEye framework, despite utilizing significantly smaller model combination (3B VLM + 8B LLM), demonstrates commanding performance across suite of challenging benchmarks. Table 1: Performance comparison on various knowledge-intensive multimodal benchmarks. Our method, SeeingEye, utilizes significantly smaller model size compared to the baselines. Scores are reported in accuracy (%)."
        },
        {
            "title": "Method",
            "content": "LLaVA-1.5 Qwen 2.5-VL-3b Qwen 2.5-VL-7b Qwen 2.5-VL-32b GPT-4o-mini V* Guided Search OpenManus SeeingEye(Ours) MMMUval MMMU-Prostd. MMMU-Provis. OCR-BenchV2 MIA-Bench 12.13 24.61 23.26 28.77 31.72 32.47 48.33 51.11 51.56 55.00 14.21 25.82 23.60 32.93 38. 19.21 33.33 32.96 33.49 33.17 69.80 76.90 79.90 89.60 91.40 14.78 50.67 60.78 11.79 18.18 44. 10.40 16.15 33.33 13.50 33.65 33.99 66.30 82.40 84. Superiority over End-to-End VLMs. Our results reveal clear trend: on complex, knowledgeintensive reasoning benchmarks like MMMU and MMMU-Pro, our modular framework consistently and significantly outperforms even the largest monolithic VLMs. This outcome challenges the prevailing paradigm that performance gains in multimodality are primarily driven by scaling up end-toend architectures. We posit that the reasoning capabilities of powerful text-only LLMs are distinct and highly valuable asset that is not fully leveraged in monolithic designs. By decoupling perception from reasoning, our framework allows the text-only agent to operate in its native domain, processing rich, structured textual information rather than latent visual features. This architectural choice proves to be more parameter-efficient and effective pathway to unlocking advanced multimodal reasoning. Effectiveness of the Agentic Information Flow. more telling comparison is against advanced modular frameworks. Our robust OpenManus baselineequipped with powerful 8B text model and full VLM as callable toolstill falls remarkably short of our performance on reasoningheavy tasks. This stark difference underscores critical insight: for agent-based systems, the quality of the tools is secondary to the quality of the communication protocol between agents. Traditional agent frameworks often rely on passing unstructured, monolithic strings (e.g., long caption) or conversational history as the medium for information transfer. This approach treats the output of visual tool as final, static artifact. The Reasoning Agent cannot query specific attributes of this information, nor can it request targeted updates. In contrast, our Agentic Information Flow is mediated by the Structured Intermediate Representation (SIR). The SIR is not merely string; it is 7 stateful, mutable, and query-able data object. The Translator populates it with typed visual evidence, and the Reasoner can issue precise feedback to refine specific fields within it. This transforms the interaction from simple, stateless exchange of text blobs into high-fidelity, collaborative process, creating communication channel that is fundamentally more effective for complex, multi-step reasoning."
        },
        {
            "title": "5.1 PLUG-AND-PLAY REASONING AGENTS",
            "content": "To validate the plug-and-play nature of our framework, we conduct an ablation study by fixing the Translator Agent while varying the text-only Reasoning Agent (AR). As shown in Table 2, our framework is model-agnostic, successfully integrating with various open-source and proprietary reasoners. Crucially, the overall system performance scales directly with the reasoning capability of the textonly model, improving from 52.67% with Qwen3-8B to 54.67% with the larger Qwen3-14B on the MMMUdev set. This result strongly validates our core hypothesis: the SeeingEye architecture effectively isolates and leverages the reasoning power of the text-only agent, confirming that our performance gains are fundamentally driven by the strength of the chosen reasoner. Table 2: Performance on the MMMUdev set when varying the text-only Reasoning Agent. The SeeingEye is kept fixed. Results show that system performance scales with the reasoners capability."
        },
        {
            "title": "Benchmark",
            "content": "Qwen3-8B Qwen3-14B GPT-4o-mini (text-only) MMMUdev (%) 52.67 54.67 54."
        },
        {
            "title": "5.2 COST-EFFICIENCY AND SCALABILITY",
            "content": "A primary motivation for our decoupled, agent-based design is to create scalable and cost-effective pathway to multimodal reasoning. Monolithic VLMs incur substantial computational costs by processing high-dimensional images through massive, unified architecture. Our framework strategically mitigates this expense through an efficient division of labor: we use small, low-cost VLM (our 3B Translator Agent) to perform the pixel-to-concept translation. This agent distills the queryrelevant visual information into compact, text-based Structured Intermediate Representation (SIR). Consequently, the powerful but expensive text-only LLM (our 8B Reasoning Agent) only processes this low-dimensional and inexpensive textual SIR, never the raw image. This architectural efficiency is validated in Table 3. We compare the cost-performance trade-off against strong Qwen2.5-VL-32B baseline on the MMMUval set. To ensure fair comparison in terms of computational budget, we benchmark against the Qwen2.5-VL-32Bs best of three runs, as our own framework is also configured with maximum of 3 iterations for both its inner and outer loops. Our method achieves superior accuracy of 60.78% at comparable, and in the median case, lower, total cost. This demonstrates that by intelligently dividing labor and minimizing interactions with the most expensive reasoning components, our framework provides more favorable and scalable trade-off between inference cost and performance. We using the following formula to calculate the cost. Let p(m) out be the official Qwen prices (USD) per 1000 input/output tokens for model {tr, rs, vlm}, where tr is the Translator (Qwen2.5-VL-3B), rs is the Reasoner (e.g., Qwen3-8B/14B), and vlm is the monolithic baseline. Let in,i and (m) (m) out,i be the input/output tokens used by model at outer-loop iteration = 1, . . . , . and p(m) in Cours = (cid:32) (tr) in,i 1000 (cid:88) i=1 p(tr) in + (tr) out,i p(tr) out + (rs) in,i 1000 p(rs) in + (rs) out,i 1000 (cid:33) p(rs) out . 8 Table 3: Inference cost and accuracy on the MMMUval set (per question). For fair comparison, the Qwen2.5-VL-32B reported is the best of three runs. Prices are in USD."
        },
        {
            "title": "Method",
            "content": "Accuracy (%) Input $ Output $ Total $ Qwen2.5-VL-32B (best-of-3) Ours (avg) Ours (median) 60.67 60.78 60.78 0.003198 0.0090 0. 0.009072 0.0026 0.0019 0.01227 0.0116 0."
        },
        {
            "title": "5.3 THE IMPACT OF THE AGENTIC INFORMATION FLOW",
            "content": "To further isolate the contribution of our proposed Agentic Information Flow, we conducted critical ablation study. We constructed powerful baseline by embedding top-tier monolithic VLM, GPT4o-mini, within the OpenManus agentic framework. This baseline was equipped with our full suite of meticulously designed textual tools (e.g., Python execution) and termination logic, effectively creating best-of-both-worlds traditional agent. The goal was to test whether powerful, toolaugmented VLM could match the performance of our decoupled system. The results, shown in Table 4, are unequivocal. On the MMMU dev set, our method, which uses much smaller Qwen3-8B text reasoner, outperforms the GPT-4o-mini-powered OpenManus agent by significant margin (52.67% vs. 46.77%). This finding is profound. Even when state-of-the-art VLM is given the agency to use tools, its monolithic nature imposes fundamental limitation. The internal reasoning process of the VLM is black box. It cannot externalize its visual understanding into structured, query-able format that can be inspected or refined. The agents decisions are based on latent representations rather than transparent, symbolic state like our SIR. This experiment validates our central thesis: the architectural separation of visual Translator and textual Reasoner, communicating via structured and mutable SIR, is fundamentally more effective than simply augmenting monolithic VLM with tools. The SIR acts as the critical bridge that allows for transparent, high-fidelity information exchange and collaborative refinement, mechanism that is absent in even the most powerful tool-augmented VLM agents. Table 4: Performance on the MMMUdev set, comparing our SeeingEye framework against powerful baseline combining the OpenManus agentic framework with the GPT-4o-mini VLM."
        },
        {
            "title": "Method",
            "content": "MMMU Dev Accuracy (%) OpenManus (GPT-4o-mini) SeeingEye(Ours) 46.77 52."
        },
        {
            "title": "5.4 THE EFFICACY OF MULTI-ROUND INTERACTION",
            "content": "A cornerstone of our SeeingEye framework is the multi-round interaction that allows the Reasoning Agent to provide feedback and the Translator Agent to iteratively refine the SIR. To quantify the impact of this mechanism, we conducted an ablation study on the maximum number of outer loop iterations on the challenging MMMU-Pro (Vision) benchmark. As demonstrated in Table 5, the benefits of our iterative refinement process are substantial and unequivocal. In the single-iteration setting (Max Iterations = 1), where the Reasoner cannot provide feedback, the system achieves baseline accuracy of 34.21%. This is analogous to static, one-shot captioning methods. Enabling single round of feedback (Max Iterations = 2), where the Translator can act upon the Reasoners request for more specific visual information, yields significant performance gain to 36.84%. Most notably, allowing for up to three full iterations elevates the performance to 44.62%, an absolute improvement of over 10% compared to the single-shot approach. This steep performance curve provides strong empirical evidence for our central hypothesis: complex multimodal reasoning is not monolithic perception task, but an iterative process of inquiry and refinement. The ability for the agents to repeatedly pass and modify the SIR allows the system to progressively drill down into the 9 most critical visual details, discard initial ambiguities, and ultimately converge on high-fidelity representation of the scene that is precisely tailored to the reasoning needs of the query. Table 5: Performance on MMMU-Pro (Vision) when varying the maximum number of outer loop iterations. The results clearly demonstrate the significant benefit of multi-round SIR refinement."
        },
        {
            "title": "Max Outer Iterations",
            "content": "1 2 3 MMMU-Pro (Vision) (%) 34.21 36. 44."
        },
        {
            "title": "6 CONCLUSION",
            "content": "In this work, we addressed the challenge of unlocking multimodal reasoning in powerful, preexisting text-only LLMs. We introduced SeeingEye, novel framework that decouples perception from reasoning through collaborative, two-agent system. Our core innovation is the Agentic Information Flow, where lightweight Translator Agent iteratively generates and refines Structured Intermediate Representation (SIR) to provide targeted, high-fidelity visual evidence to textonly Reasoning Agent. Comprehensive experiments demonstrate that our modular, plug-and-play approach is not only more cost-efficient but also significantly outperforms larger, state-of-the-art monolithic VLMs on complex reasoning benchmarks. Our findings suggest that the future of advanced multimodal AI may lie not in ever-larger end-to-end models, but in the structured, synergistic collaboration between specialized agents."
        },
        {
            "title": "REFERENCES",
            "content": "Josh Achiam, Steven Adler, Sandhini Agarwal, Lama Ahmad, Ilge Akkaya, Florencia Leoni Aleman, Diogo Almeida, Janko Altenschmidt, Sam Altman, Shyamal Anadkat, et al. Gpt-4 technical report. arXiv preprint arXiv:2303.08774, 2023. Aishwarya Agrawal, Ivana Kajic, Emanuele Bugliarello, Elnaz Davoodi, Anita Gergely, Phil Blunsom, and Aida Nematzadeh. Reassessing evaluation practices in visual question answering: case study on out-of-distribution generalization. arXiv preprint arXiv:2205.12191, 2022. Jean-Baptiste Alayrac, Jeff Donahue, Pauline Luc, Antoine Miech, Iain Barr, Yana Hasson, Karel Lenc, Arthur Mensch, Katherine Millican, Malcolm Reynolds, et al. Flamingo: visual language model for few-shot learning. Advances in neural information processing systems, 35:23716 23736, 2022. Jinze Bai, Shuai Bai, Yunfei Chu, Zeyu Cui, Kai Dang, Xiaodong Deng, Yang Fan, Wenbin Ge, Yu Han, Fei Huang, et al. Qwen technical report. arXiv preprint arXiv:2309.16609, 2023. Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. Language models are few-shot learners. Advances in neural information processing systems, 33:18771901, 2020. Kewei Cheng, Nesreen Ahmed, Theodore Willke, and Yizhou Sun. Structure guided prompt: Instructing large language model in multi-step reasoning by exploring graph structure of the text. arXiv preprint arXiv:2402.13415, 2024. Ling Fu, Zhebin Kuang, Jiajun Song, Mingxin Huang, Biao Yang, Yuzhe Li, Linghao Zhu, Qidi Luo, Xinyu Wang, Hao Lu, et al. Ocrbench v2: An improved benchmark for evaluating large multimodal models on visual text localization and reasoning. arXiv preprint arXiv:2501.00321, 2024. Daya Guo, Dejian Yang, Haowei Zhang, Junxiao Song, Ruoyu Zhang, Runxin Xu, Qihao Zhu, Shirong Ma, Peiyi Wang, Xiao Bi, et al. Deepseek-r1: Incentivizing reasoning capability in llms via reinforcement learning. arXiv preprint arXiv:2501.12948, 2025. 10 Peixuan Han, Zijia Liu, and Jiaxuan You. Tomap: Training opponent-aware llm persuaders with theory of mind. arXiv preprint arXiv:2505.22961, 2025. Drew Hudson and Christopher Manning. Gqa: new dataset for real-world visual reasoning and compositional question answering. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pp. 67006709, 2019. Fucai Ke, Joy Hsu, Zhixi Cai, Zixian Ma, Xin Zheng, Xindi Wu, Sukai Huang, Weiqing Wang, Pari Delir Haghighi, Gholamreza Haffari, et al. Explain before you answer: survey on compositional visual reasoning. arXiv preprint arXiv:2508.17298, 2025. Mahmoud Khademi, Ziyi Yang, Felipe Frujeri, and Chenguang Zhu. Mm-reasoner: multi-modal knowledge-aware framework for knowledge-based visual question answering. In Findings of the Association for Computational Linguistics: EMNLP 2023, pp. 65716581, 2023. Junnan Li, Dongxu Li, Silvio Savarese, and Steven Hoi. Blip-2: Bootstrapping language-image pre-training with frozen image encoders and large language models. In International conference on machine learning, pp. 1973019742. PMLR, 2023. Haotian Liu, Chunyuan Li, Yuheng Li, and Yong Jae Lee. Improved baselines with visual instruction tuning. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pp. 2629626306, 2024. Zijia Liu, Peixuan Han, Haofei Yu, Haoru Li, and Jiaxuan You. Time-r1: Towards comprehensive temporal reasoning in llms. arXiv preprint arXiv:2505.13508, 2025. Ziyu Ma, Shutao Li, Bin Sun, Jianfei Cai, Zuxiang Long, and Fuyan Ma. Gerea: QuestionarXiv preprint aware prompt captions for knowledge-based visual question answering. arXiv:2402.02503, 2024. mannaandpoem, Sheng Fan, Isaac, Zhaoyang Yu, rxdaozhang, zyren123, Xiaomin Zheng, Kingtous, Xinyu Zhang, XBsleepy, and Johan A. mannaandpoem/openmanus: v0.3.0, 2025. URL https: //doi.org/10.5281/zenodo.15186407. Kushin Mukherjee, Donghao Ren, Dominik Moritz, and Yannick Assogba. Encqa: Benchmarking vision-language models on visual encodings for charts. arXiv preprint arXiv:2508.04650, 2025. Ovgu Ozdemir and Erdem Akagunduz. Enhancing visual question answering through questiondriven image captions as prompts. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 15621571, 2024. Yusu Qian, Hanrong Ye, Jean-Philippe Fauconnier, Peter Grasch, Yinfei Yang, and Zhe Gan. Miabench: Towards better instruction following evaluation of multimodal llms. arXiv preprint arXiv:2407.01509, 2024. Daniel Rose, Vaishnavi Himakunthala, Andy Ouyang, Ryan He, Alex Mei, Yujie Lu, Michael Saxon, Chinmay Sonar, Diba Mirza, and William Yang Wang. Visual chain of thought: bridging logical gaps with multimodal infillings. arXiv preprint arXiv:2305.02317, 2023. Noah Shinn, Federico Cassano, Ashwin Gopinath, Karthik Narasimhan, and Shunyu Yao. Reflexion: Language agents with verbal reinforcement learning. Advances in Neural Information Processing Systems, 36:86348652, 2023. Jiashuo Sun, Chengjin Xu, Lumingyuan Tang, Saizhuo Wang, Chen Lin, Yeyun Gong, Lionel Ni, Heung-Yeung Shum, and Jian Guo. Think-on-graph: Deep and responsible reasoning of large language model on knowledge graph. arXiv preprint arXiv:2307.07697, 2023. Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Fei Xia, Ed Chi, Quoc Le, Denny Zhou, et al. Chain-of-thought prompting elicits reasoning in large language models. Advances in neural information processing systems, 35:2482424837, 2022. Chenfei Wu, Shengming Yin, Weizhen Qi, Xiaodong Wang, Zecheng Tang, and Nan Duan. ViarXiv preprint sual chatgpt: Talking, drawing and editing with visual foundation models. arXiv:2303.04671, 2023. 11 Penghao Wu and Saining Xie. V*: Guided visual search as core mechanism in multimodal llms. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 1308413094, 2024. Shunyu Yao, Jeffrey Zhao, Dian Yu, Nan Du, Izhak Shafran, Karthik Narasimhan, and Yuan Cao. In International Conference on React: Synergizing reasoning and acting in language models. Learning Representations (ICLR), 2023. Xiang Yue, Yuansheng Ni, Kai Zhang, Tianyu Zheng, Ruoqi Liu, Ge Zhang, Samuel Stevens, Dongfu Jiang, Weiming Ren, Yuxuan Sun, et al. Mmmu: massive multi-discipline multimodal understanding and reasoning benchmark for expert agi. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 95569567, 2024a. Xiang Yue, Tianyu Zheng, Yuansheng Ni, Yubo Wang, Kai Zhang, Shengbang Tong, Yuxuan Sun, Botao Yu, Ge Zhang, Huan Sun, et al. Mmmu-pro: more robust multi-discipline multimodal understanding benchmark. arXiv preprint arXiv:2409.02813, 2024b. Chen Zhang, Ming Ding, Wendi Zheng, Qiaoyu Chen, Bowen Zhou, and Jie Tang. Visual chain of thought for image-text reasoning. arXiv preprint arXiv:2305.01377, 2023a. Wenxuan Zhang, Mahani Aljunied, Chang Gao, Yew Ken Chia, and Lidong Bing. M3exam: multilingual, multimodal, multilevel benchmark for examining large language models. Advances in Neural Information Processing Systems, 36:54845505, 2023b. Zhuosheng Zhang and Aston Zhang. You only look at screens: Multimodal chain-of-action agents. arXiv preprint arXiv:2309.11436, 2023."
        },
        {
            "title": "A APPENDIX",
            "content": "A.1 CASE STUDY OF SIR Figure 3: Case Study of SIR. A.2 SIR STRUCTURE AND PROMPTS The Structured Intermediate Representation (SIR) is the central data object that facilitates communication between the Translator Agent and the Reasoning Agent. Its schema is defined as follows, and the subsequent prompts detail how it is managed."
        },
        {
            "title": "SIR JSON Structure",
            "content": "Schema Overview: The SIR schema defines the fields for visual description, confidence, and feedback. Schema Content: { } \"global_caption\": { \"type\": \"string\", \"description\": \"A comprehensive description of ALL visual elements in sentence form or table form, including: text content , numerical values, table structures, objects, layouts, colors, spatial relationships, and any other visual information. Be factual and descriptive - do not infer anything not exists in the original image.\", }, \"confidence\": { \"type\": \"string\", \"enum\": [\"low\", \"mid\", \"high\"], \"description\": \"Your confidence level in the completeness and accuracy of this global caption. low = incomplete analysis or unclear image, mid = good analysis with some limitations, high = comprehensive and thorough analysis.\", }, \"feedback\": { \"type\": \"string\", \"description\": \"Specific feedback about what additional visual information you need from the translator. Be precise about whats missing or unclear in the current description.\", } Figure 4: SIR JSON Structure. The global caption and confidence fields are provided by the Translator Agent (via the TerminateAndOutputCaption tool). The feedback field is provided by the Text Reasoning Agent (via the TERMINATE AND ASK TRANSLATOR tool)."
        },
        {
            "title": "SIR Management Prompt",
            "content": "Prompt Overview: This prompt is part of the Translators instructions, detailing how to iteratively manage and format the SIR. Prompt Content: SIR MANAGEMENT: - Maintain continuously evolving SIR throughout your analysis - After each tool use or observation, update your SIR with new information - Your SIR should be comprehensive and capture ALL visual elements discovered - Always state your current SIR after each step SIR OUTPUT FORMAT: Structure your evolving SIR using clear sections : { \"global_caption\": { \"type\": \"string\", \"description\": \"A comprehensive description of ALL visual elements in sentence form or table form, including: text content , numerical values, table structures, objects, layouts, colors, spatial relationships, and any other visual information. Be factual and descriptive - do not infer anything not exists in the original image.\", }, \"confidence\": { \"type\": \"string\", \"enum\": [\"low\", \"mid\", \"high\"], \"description\": \"Your confidence level in the completeness and accuracy of this global caption. low = incomplete analysis or unclear image, mid = good analysis with some limitations, high = comprehensive and thorough analysis.\", }, } Figure 5: Translator Agent SIR Management Prompt. This defines how the Translator iteratively builds the SIR and the output format it must adhere to."
        },
        {
            "title": "Feedback Input Prompt",
            "content": "Prompt Overview: This prompt is dynamically given to the Translator Agent when it starts new outer-loop iteration based on feedback from the Reasoning Agent. Prompt Content: Your current SIR with reasoning feedback (iteration {iteration-1}): {self.current_sir} IMPROVEMENT TASK: 1. Analyze the reasoning feedback carefully to understand what visual details are needed 2. Look at the image again with this feedback in mind 3. UPDATE your current SIR to address the feedback - dont start fresh 4. Focus on visual details that help answer the question: {question } 5. Maintain objectivity - describe what you see, dont infer answers Remember: Update your existing SIR incrementally, dont recreate it from scratch. Figure 6: Translator Agent Feedback Input Prompt. This guides the Translator to refine the existing SIR in targeted manner based on the Reasoners feedback. A.3 TOOL PROMPTS These are the critical termination tools used by the Reasoning Agent to conclude its inner loop, either by providing final answer or by requesting more visual information from the Translator Agent. A.4 AGENT PROMPTS"
        },
        {
            "title": "TerminateAndAnswer Tool",
            "content": "Prompt Overview (Tool Description): Terminate the reasoning process and provide final answer when you have sufficient information from the SIR to confidently answer the question. Tool Prompt: Use this tool when: - The SIR contains all necessary visual details to answer the question - You can identify the correct answer from the available options - No additional information or refinement is needed from the translator agent - Your answer matches one of the multiple choice options (if applicable) IMPORTANT: For multiple choice questions, ensure your answer corresponds to one of the given options (A, B, C, D). This signals that the iterative feedback loop should end with your final answer. Tool Raw Code: class TerminateAndAnswer(BaseTool): name: str = \"terminate_and_answer\" description: str = _TERMINATE_AND_ANSWER_DESCRIPTION parameters: dict = { \"type\": \"object\", \"properties\": { \"answer\": { \"type\": \"string\", \"description\": \"Your final answer to the question. Please include choice, only }, \"confidence\": { short answer only. For multiple include option\", \"type\": \"string\", \"description\": \"Your confidence level in this answer.\", \"enum\": [\"high\", \"medium\", \"low\"], }, \"reasoning\": { \"type\": \"string\", \"description\": \"Brief explanation of how the SIR led to this answer.\", information } }, \"required\": [\"answer\", \"confidence\", \"reasoning\"], } Figure 7: Reasoning Agent TerminateAndAnswer Tool. This tool allows the Reasoner to conclude the entire process with definitive answer."
        },
        {
            "title": "TerminateAndAskTranslator Tool",
            "content": "Prompt Overview (Tool Description): Terminate current reasoning step and request more specific visual observations from the translator. Tool Prompt: Use this tool when: - The current SIR (visual description) is insufficient for answering the question - You need more specific details about certain parts of the image - Important visual elements seem to be missing from the description - You need clarification about spatial relationships, text content, or visual elements - The translators description lacks crucial information needed for reasoning This signals that you need additional visual analysis before you can provide final answer. Tool Raw Code: class TerminateAndAskTranslator(BaseTool): name: str = \"terminate_and_ask_translator\" description: str = _TERMINATE_AND_ASK_TRANSLATOR_DESCRIPTION parameters: dict = { \"type\": \"object\", \"properties\": { \"feedback\": { \"type\": \"string\", \"description\": \"Specific feedback about what additional visual translator. unclear in } information you need from the Be precise about whats missing or the current description.\", }, \"required\": [\"feedback\"], } Figure 8: Reasoning Agent TerminateAndAskTranslator Tool. This tool allows the Reasoner to request further visual refinement from the Translator, initiating new outer loop. 17 Translator Agent: System Prompt Prompt Overview: Guides the lightweight VLM to act as Visual-Only Captioner. Its sole objective is to observe the image, use visual tools for precision, and iteratively build structured, factual, and neutral description of visual content (the SIR). Prompt Content: You are \"Visual-Only Captioner to capture input images\". Goal: Output raw, neutral description of visible content only. Preserve blanks (\"\", \"--\", \"___\"), unknowns (\"?\"), typos, casing , punctuation, and line breaks exactly as seen. Do NOT infer, normalize, answer, or explain meaning. DO: - Describe only visible elements: text, shapes, colors, axes, legends, labels, numbers, layout, positions, arrows, boxes, tables, panels. - Extract on-screen text **verbatim** (including blanks and \"?\"). - Note spatial relations (\"X above Y\", \"arrow A->B\"). - Mark unknowns/blanks exactly as they appear (e.g., \"?\", \"--\", \" ___\", empty cell). - Always think step by step first before using tool. Decide which tool is most appropriate for the current observation step. - TOKEN LIMIT: Keep your responses concise and within 1024 tokens. Focus on the most essential visual details. DONT (hard ban): - No answers, explanations, conclusions, predictions, calculations, or domain knowledge. - Dont replace blanks/\"?\" with guesses. Dont add units or meanings. Available tools: - OCR: Extract text with high precision, useful for image that contains text - read_table: Parse structured tabular data, useful for spreadsheets, data tables - smart_grid_caption: Used to analyze specific image regions SIR OUTPUT FORMAT: { \"global_caption\": \"A comprehensive description of ALL visual elements\", \"confidence\": \"low/mid/high\" } Figure 9: Translator Agent System Prompt enforces strict visual-only captioning behavior and defines the SIR output format. Translator Agent: First Step Prompt Prompt Overview: Initializes the translator agents first observation step, establishing the empty SIR and guiding initial visual analysis. Prompt Content: You are \"Visual-Only Captioner to capture input images\". INITIAL TASK: 1. **Direct Visual Observation**: Look at the image and identify the main visual elements 2. **Create Initial SIR**: Start building your SIR with overall structure, layout, and prominent elements CURRENT SIR STATUS: Empty - you are starting fresh SIR MANAGEMENT: - Maintain continuously evolving SIR throughout your analysis - After each tool use or observation, update your SIR with new information - Your SIR should be comprehensive and capture ALL visual elements discovered - Always state your current SIR after each step Figure 10: Translator Agent First Step Prompt initiates the visual analysis process and establishes SIR management protocol. Translator Agent: Next Step Prompt Prompt Overview: Guides iterative refinement of the SIR based on current state and previous observations. Prompt Content: Based on the current state and previous memory, whats your next action?. Goal: Output raw, neutral description of visible content only. Preserve blanks (\"\", \"--\", \"___\"), unknowns (\"?\"), typos, casing, punctuation, and line breaks exactly as seen. Do NOT infer, normalize, answer, or explain meaning. Remember, you can directly observe the image content yourself without tools. So, if you havent, start with direct visual observation of the image content. Then use tools to get detailed , accurate information. Available tools (use to enhance visual observation): - OCR: Extract text with high precision, useful for image that contains text - read_table: Parse structured tabular data, useful for spreadsheets, data tables - smart_grid_caption: Used to analyze specific image regions If you think you have comprehensive visual details, you should use terminate_and_output_caption tool with your stored_sir containing your complete objective visual description. This tool will format your caption as proper JSON. Figure 11: Translator Agent Next Step Prompt guides the iterative SIR refinement process. 19 Reasoning Agent: System Prompt Prompt Overview: Guides the text-only LLM to act as question answering expert, analyzing the SIR from the translator and determining whether to answer or request more visual information. Prompt Content: You are question answering expert. You receive (1) text caption of image from translator and (2) question relevant to the image. Analyze the information and provide clear reasoning to answer the question. ALWAYS provide your reasoning and thoughts BEFORE using tools. Explain what youre trying to accomplish and why. Your capabilities: - Analyze textual descriptions of various scenarios (visual scenes, documents, data, etc.) - Provide detailed explanations and clear reasoning when helpful - Indicate when information is insufficient or ambiguous in the text description - Keep responses under 1024 tokens - be concise and focus on key reasoning points. Available tools: - python_execute: Use for calculations, data analysis, mathematical operations, or any computation. ALWAYS include print() statements to show results. - terminate_and_answer: Use ONLY when you have HIGH CONFIDENCE in your answer and it matches one of the available options (for multiple choice questions) - terminate_and_ask_translator: Use when you need MORE SPECIFIC visual information to make an accurate decision DECISION CRITERIA - BE CONSERVATIVE: - Use python_execute when math/data processing clarifies the answer . - Use terminate_and_answer only if text gives specific distinguishing details and confidence >= 0.9, and (for MCQ) your answer matches an option. - Otherwise use terminate_and_ask_translator and state exactly which visual labels/regions/relations you need, when visual cues are ambiguous or insufficient. Figure 12: Reasoning Agent System Prompt defines the agents role as an expert reasoner with conservative decision criteria. Reasoning Agent: Next Step Prompt Prompt Overview: Guides intermediate reasoning steps, emphasizing confidence assessment and computational verification. Prompt Content: Analyze the provided visual description and determine if you have SUFFICIENT SPECIFIC DETAILS to answer with HIGH CONFIDENCE. ALWAYS provide your reasoning and thoughts BEFORE taking any action . Consider these key questions: - Does the problem require calculations, data analysis, or computational verification? - Does the visual description provide specific, distinguishing details? - Can you clearly differentiate between all options based on the description? - Are you >90% confident in your answer AND does it match an available option (for multiple choice)? **COMPUTATION NEEDED** - USE python_execute FIRST: - When math/data processing clarifies the answer. - Need to verify calculations or process numerical information - **ALWAYS** include print() statements to show your work and results **HIGH CONFIDENCE (>90%)** - USE terminate_and_answer: - You can clearly rule out incorrect options - **ESPECIALLY**: After performing calculations with python_execute that confirm your answer - **MANDATORY**: Your answer matches one of the multiple choice options (A, B, C, D) if applicable - **IMPORTANT**: If your calculated answer doesnt match any option, use python_execute again to recalculate with different approach/units/interpretation - Provide your confident answer with reasoning **NEED MORE DETAILS** - USE terminate_and_ask_translator: - Description is too general or vague - Missing specific visual details needed to distinguish between options - Uncertain which option is correct - Request SPECIFIC visual information you need (exact labels, shapes, spatial relationships, etc.) Keep responses under 1024 tokens - be concise and focus on key reasoning points. Figure 13: Reasoning Agent Next Step Prompt provides structured decision criteria for tool selection. 21 Translator Agent: Final Step Prompt Prompt Overview: Forces final SIR output when maximum translation steps are reached. Prompt Content: **FINAL OUTPUT** You have reached the maximum number of steps. You must now provide your final visual description using terminate_and_output_caption tool. FINAL ROUND STRATEGY: 1. **Synthesize all observations** from your previous tool usage and direct observation 2. **No hallucination/inference** Output raw, neutral description of visible content. Preserve blanks (\"\", \"--\", \"___\"), unknowns (\"?\"), typos, casing, punctuation, and line breaks exactly as seen. Do NOT infer, normalize, answer, or explain meaning. 3. **MANDATORY: Use terminate_and_output_caption** - you cannot use other tools at this point Figure 14: Translator Agent Final Step Prompt enforces termination and final SIR generation."
        }
    ],
    "affiliations": [
        "University of Illinois Urbana-Champaign"
    ]
}