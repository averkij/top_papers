{
    "paper_title": "Understanding Syllogistic Reasoning in LLMs from Formal and Natural Language Perspectives",
    "authors": [
        "Aheli Poddar",
        "Saptarshi Sahoo",
        "Sujata Ghosh"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "We study syllogistic reasoning in LLMs from the logical and natural language perspectives. In process, we explore fundamental reasoning capabilities of the LLMs and the direction this research is moving forward. To aid in our studies, we use 14 large language models and investigate their syllogistic reasoning capabilities in terms of symbolic inferences as well as natural language understanding. Even though this reasoning mechanism is not a uniform emergent property across LLMs, the perfect symbolic performances in certain models make us wonder whether LLMs are becoming more and more formal reasoning mechanisms, rather than making explicit the nuances of human reasoning."
        },
        {
            "title": "Start",
            "content": "Aheli Poddar1, Saptarshi Sahoo2, Sujata Ghosh2 1Institute of Engineering & Management, Kolkata 2Indian Statistical Institute, Chennai aheli.poddar2022@iem.edu.in, saptarshi@isichennai.res.in, sujata@isichennai.res.in 5 2 0 2 1 2 ] . [ 2 0 2 6 2 1 . 2 1 5 2 : r a"
        },
        {
            "title": "Abstract",
            "content": "We study syllogistic reasoning in LLMs from the logical and natural language perspectives. In process, we explore fundamental reasoning capabilities of the LLMs and the direction this research is moving forward. To aid in our studies, we use 14 large language models and investigate their syllogistic reasoning capabilities in terms of symbolic inferences as well as natural language understanding. Even though this reasoning mechanism is not uniform emergent property across LLMs, the perfect symbolic performances in certain models make us wonder whether LLMs are becoming more and more formal reasoning mechanisms, rather than making explicit the nuances of human reasoning. Code https://github.com/XAheli/Logic-in-LLMs"
        },
        {
            "title": "Introduction",
            "content": "With the unprecedented development of large language models (LLMs) in recent years that have made them resemble human speakers and reasoners to great extent in many levels (Holliday, Mandelkern, and Zhang 2024; Bubeck et al. 2023; Zhao et al. 2023), the reasoning capabilities of LLMs have increased manifold. To motivate such growth, the question we generally ask an LLM is to what extent the LLM has grasped logical reasoning in its different forms, for example, see (Holliday, Mandelkern, and Zhang 2024; Borazjanizadeh and Piantadosi 2024; Sambrotta 2025). In contrast, the motivation for this study is somewhat distinct in nature in that we wonder whether developing LLM to have excellent logical reasoning capabilities is fruitful in the long run, as having such features does not bring an LLM closer to mimicking human reasoning. As case in point, we consider syllogistic reasoning from formal as well as natural language viewpoint. Evidently, humans are far from logical when it comes to reasoning, and they are often influenced by their past experiences and knowledge, for example, consider the belief-bias effect (Evans, Barston, and Pollard 1983): People doing syllogistic reasoning are often influenced by the believability of the conclusion. In fact, it is shown by (Lewton 2016) that individuals with autistic traits show less belief-bias effect Copyright 2026, Association for the Advancement of Artificial Intelligence (www.aaai.org). All rights reserved. than typical individuals. In this scenario, one might consider to check whether LLM reasoning is close to human reasoning by studying the belief-bias effect on the LLMs, and the present work studies this question. We note that (Eisape et al. 2024) studied similar question, but their methodology is quite different from ours. Before describing the exact contribution of this work, let us discuss some recent work on syllogistic reasoning in LLMs. novel framework dealing with legal syllogistic reasoning is provided in (Zhang et al. 2025). In this work, the LLMs are empowered to provide explicit and trustworthy legal reasoning by integrating retrieval mechanism with reinforcement learning. mechanistic interpretation of syllogistic reasoning is provided in (Kim, Valentino, and Freitas 2025). This work deals with belief-biases as well and it is shown that such biases contaminate the reasoning mechanisms. In (Zong and Lin 2024), the authors make detailed survey on the reasoning capabilities of LLMs with respect to categorical syllogisms. This work makes several key contributions to understanding syllogistic reasoning in LLMs from both formal and natural language perspectives. We introduce novel dual ground truth framework that evaluates each syllogism on two separate dimensions: syntactic validity (does the conclusion logically follow?) and natural language believability (is the conclusion intuitively plausible?). These two dimensions may align or conflict with each other, enabling us to assess formal reasoning capabilities independently from natural language understanding. Through comprehensive empirical study, we systematically evaluated 14 state-of-theart LLMs across four prompting strategies and three temperature settings on carefully constructed syllogisms covering diverse logical structures and belief-bias conditions. Our analysis reveals that the majority of models exhibit significant measure of belief bias; in other words, they perform better on certain kinds of problem (where logic aligns with intuition) than others. We further uncover substantial gap between syntactic and natural language understanding accuracy, demonstrating that current LLMs excel at formal logical structure while struggling with natural language plausibility judgmentsa pattern opposite to human reasoning tendencies. Contrary to conventional wisdom, we find that few-shot prompting degrades performance compared to zero-shot, and that reasoning capability depends critically on 1 2 3 4 B-C C-B B-C C-B C-D D-C D-C C-D Table 1: description of the four figures for syllogisms containing the variables B, C, and D. architectural choices rather than raw parameter count. These findings raise fundamental question: Are LLMs evolving into formal reasoning engines that surpass human-like reasoning with its inherent biases? The remainder of the paper is structured as follows. Section 2 provides brief overview of syllogisms. Section 3 delves into the experimental details, including the models, data, overall methodology, prompting variants, and evaluation metrics. Section 4 reports on the findings and their interpretations. Section 6 provides discussion of the limitations of our study, and Section 7 concludes the article."
        },
        {
            "title": "2 On Syllogisms\nThe concept of syllogism was first introduced by Aristotle\n(Smith et al. 1989), and as observed by Robin Smith (Smith\n2022), a syllogism in modern logic consists of three subject-\npredicate propositions, two premises, and a conclusion, and\nwhether or not the conclusion follows from the premises.\nAn example of syllogism is as follows: “No footballer is a\nswimmer; Some swimmers are gardeners; Therefore, some\ngardeners are not footballers.” When terms like footballer\nor swimmer are replaced by generic terms like B, C and D,\nwe can rewrite the above premises by: “No B is C; Some C\nare D.” A conclusion relates the non-shared terms, for ex-\nample, “Some D are not B”.",
            "content": "In the literature, various types of syllogisms are studied, categorical, conditional, and others (Copi, Cohen, and McMahon 2016). In this work, we mostly concentrate on categorical syllogisms, but we consider few others as well. The statements of categorical syllogism look like the following: Quantifier (Subject) Copula (Predicate), which take four standard forms, viz. - Universal Affirmative (A): All are P, i.e., . - Universal Negative (E): No is P, i.e., = . - Particular Affirmative (I): Some is P, i.e., = . - Particular Negative (O): Some is not P, i.e., = . Here, is the subject and is the predicate. and are generally termed variables, and these quantifier styles, namely, A, E, I, O, are called moods. The variables may change their orders, leading to new premises. As mentioned earlier, one of the three variables used in syllogism is not there in the conclusion, and evidently the variable is common to both premises. Depending on the placement of the common variable (C, say) that does not occur in the conclusion, we get four types of figures for syllogisms. See Table 1 for detailed description. We should note here that, in statements of type A, All is sometimes overlooked for the sake of simplicity. The following example clarifies the point: All vehicles have wheels; Boats are vehicles / boat is vehicle; Therefore, boats have wheels / boat has wheels. syllogism is said to be valid if the truth of the premises implies the truth of the conclusion. way to check the validity of syllogism is by converting the statements in suitable first order language and check the validity there. The other way is through enumerating each case (there will be some finite number of cases where the two premises will have one of the four forms A, E, or O) and then using standard Venn Diagram techniques to fix the conclusion. Thus, when new tuple of syllogism comes in, the job of checking validity boils down to just checking the instance from the already defined cases and to conclude from it. syllogism is said to be believable if the conclusion of the syllogism is actually true. For this case, the logical argument does not play any role. The main goal of this research work is two-fold. On one hand, we would like to check how accurately the LLMs can do syllogistic reasoning, and on the other hand we would like to check whether context and real world knowledge play any role in their reasoning processes. To this end, the following four categories of syllogisms play significant role, namely (i) valid-believable, (ii) valid-unbelievable, (iii) invalid-believable, and (iv) invalidunbelievable. These distinct types are summarized in Table 2, given in (Brauner, Ghosh, and Ghosh 2025), which provides an example for each such type of syllogism."
        },
        {
            "title": "3.2 Data and Methodology\nDataset Construction For our experiments, we con-\nstructed a benchmark of 160 syllogisms, mostly categorical,\nadapted from the cognitive science and psychology literature\non human syllogistic reasoning (Solcz 2008; Lewton 2016).",
            "content": "1https://ai.google.dev/gemini-api/docs 2https://huggingface.co/docs 3Total API costs for all experiments were approximately $"
        },
        {
            "title": "All insects need oxygen\nMice need oxygen\nTherefore mice are insects",
            "content": "Table 2: Example syllogisms illustrating the four categories described in 2. Figure 1: Heatmap of model accuracy across four prompting strategies (Zero-shot, One-shot, Few-shot, Zero-shot Chainof-Thought). Despite few-shot showing significant mean decline ( = 3.57 pp, = 0.0165), systematic patterns across models remain minimal, indicating strategy effects are model-specific rather than universal. We began with 40 base syllogisms, each handcrafted to cover different syllogistic figures and validity conditions. To isolate the effects of logical structure from natural languagecontent, given our dual ground truth annotations, we created three additional variants for each base syllogism. The nonsense variant (X) replaces meaningful predicates with abstract terms (e.g., blargs, zimons, glorps), testing pure logical reasoning without natural language interference. The order-switched variant (O) reverses the order of presentation of the premises to test the sensitivity to the structure of the argument. The combined variant (OX) applies both modifications, providing comprehensive robustness assessment. For example, the normal variant All calculators are machines; All computers are calculators; Therefore, some machines are not computers becomes All blargs are zimons; All glorps are blargs; Therefore, some zimons are not glorps in its nonsense form. We reviewed all stimuli and made necessary adjustments by hand to ensure grammatical correctness and logical equivalence across variants. Dual Ground Truth Each syllogism carries two independent ground truth annotations, enabling orthogonal evaluaFigure 2: Belief bias effect across 14 models comparing performance on congruent syllogisms (logic aligns with intuition) versus incongruent syllogisms (logic conflicts with intuition). Twelve models (86%) exhibit positive bias ( = +10.81 pp, = 0.0280, = 0.66). Top-tier models show minimal bias (< 2 pp), while lower-tier models show severe bias (up to +46.9 pp). Negative correlation (ρ = 0.565) indicates higher reasoning ability reduces reliance on semantic heuristics. tion of logical reasoning and natural language processing. The syntactic validity label (valid/invalid) indicates whether the conclusion logically follows from the premises according to formal syllogistic rules, independent of content truth. The natural language understanding (NLU) label (believable/unbelievable) indicates whether the conclusion is intuitively plausible given real-world knowledge, independent of logical structure. The dataset comprises 76 valid syllogisms (47.5%) and 84 invalid syllogisms (52.5%). For believability, 38 instances (23.8%) have believable conclusions while 122 (76.2%) have unbelievable or abstract conclusions. This asymmetry reflects the inclusion of nonsense variants, which by design have semantically neutral conclusions. Belief Bias Categories Belief bias is well-documented phenomenon in human cognition whereby reasoners accept logically invalid conclusions that seem plausible, or reject valid conclusions that seem implausibleallowing the semantic content of conclusions to override evaluation of logical structure (Evans, Barston, and Pollard 1983; Klauer, Musch, and Naumer 2000; Pennycook et al. 2013)."
        },
        {
            "title": "Our dual annotation scheme enables formal quantification",
            "content": "of this effect by categorizing syllogisms based on alignment between logical validity and intuitive believability: Congruent instances (82 instances, 51.2%) are cases where logic and intuition align: valid-believable or invalidunbelievable conclusions. These represent easy cases where correct logical judgment matches intuitive response. Incongruent instances (78 instances, 48.8%) are cases where logic and intuition conflict: valid-unbelievable or invalid-believable conclusions. These hard cases directly test whether models can override semantic plausibility with formal reasoning. For example: All things with an engine need oil; Cars need oil; Therefore, cars have engines. This conclusion is factually correct yet logically invalid (affirming the consequent fallacy). Such instances are particularly diagnostic, as accepting them indicates susceptibility to belief bias."
        },
        {
            "title": "3.3 Prompting Schema\nWe implement four prompting strategies to evaluate mod-\nels under varying levels of task specification and reasoning\nscaffolding: Zero Shot (ZS) and One-shot (OS), which uti-\nlize zero and one demonstration example respectively to test\nintrinsic capability; Few Shot (FS), which provides four bal-\nanced examples (2 valid, 2 invalid) including a belief bias\ntrap to distinguish natural language plausibility from logi-\ncal validity; and ZS Chain-of-Thought (ZS CoT), which en-\ncourages intermediate reasoning traces (Kojima et al. 2022).\nCritically, regardless of the context or scaffolding provided,\nall strategies request the same final response format: a sin-\ngle word “correct” or “incorrect” to ensure comparability\nacross conditions.",
            "content": "Algorithm 1 presents our unified inference procedure that adapts its behavior based on the temperature parameter τ . The algorithm accepts syllogism consisting of two premises p1, p2 and conclusion c, prompting strategy σ, and outputs validity prediction ˆy along with confidence score ρ. Strategy Specifications The procedure begins by constructing task-specific prompts through two subroutines. BUILDSYSTEMPROMPT(σ) generates the system-level instruction that defines the reasoning task: You are an expert in syllogistic reasoning. Your task is to determine whether the conclusion of given syllogism follows from the premises. syllogism is CORRECT if the conclusion follows from the premises. syllogism is INCORRECT if the conclusion does not follow. [Strategyspecific addition.] Respond with exactly one word: correct or incorrect. For ZS CoT, the system prompt appends Think through step by step before the response instruction; all other strategies use identical system prompts. BUILDUSERPROMPT(S, σ) constructs the user message by optionally including demonstration examples (1 for one-shot, 4 for FS), formatting the input syllogism with labeled premises and conclusion, and appending the query. Adaptive Stopping Strategy When τ = 0, the algorithm performs greedy deterministic decoding, querying the language model once, and returning the parsed prediction with Algorithm 1: Temperature-Adaptive Syllogistic Reasoning Require: Syllogism = (p1, p2, c); Strategy σ {ZS, OS, FS, ZSCoT}; Temperature τ {0.0, 0.5, 1.0} Ensure: Prediction ˆy {valid, invalid}; Confidence ρ [0, 1] 1: Parameters: Kmax = 10, η = 5 {Max samples, early stopping threshold} return PARSE(QUERY(πsys, πuser, 0)), 1.0 2: 3: πsys BUILDSYSTEMPROMPT(σ) 4: πuser BUILDUSERPROMPT(S, σ) 5: if τ = 0 then 6: 7: end if 8: n+ 0, 0 9: for = 1 to Kmax do 10: 11: 12: 13: 14: end if 15: 16: end for ˆyk PARSE(QUERY(πsys, πuser, τ )) n+ n+ + [ˆyk = valid] n + [ˆyk = invalid] if = η and min(n+, n) = 0 then break {Early stop if unanimous} 17: ˆy (cid:26)valid invalid if n+ > otherwise {Ties default to invalid} 18: ρ max(n+, n)/(n+ + n) 19: return ˆy, ρ full confidence (ρ = 1.0). For stochastic sampling (τ > 0), we implement self-consistency (Chen et al. 2023) by generating up to Kmax = 10 independent samples. For each sample k, we query the model with temperature τ and parse the response to extract the validity label ˆyk. We maintain counters n+ and for valid and invalid predictions, respectively, using indicator functions []. To improve efficiency, we employ early stopping inspired by Holliday, Mandelkern, and Zhang (2024): if the first η = 5 samples are unanimous (i.e., min(n+, n) = 0 at = η), we terminate sampling. This reduces API calls substantially when models exhibit high confidence. The final prediction ˆy is determined by majority vote. Any ties by default maps to invalid as conservative choice."
        },
        {
            "title": "3.4 Evaluation Methods\nPrimary Metrics We evaluate model responses using\nstandard classification metrics: accuracy (T P + T N )/N ,\nprecision T P/(T P + F P ), recall T P/(T P + F N ), and\nF1 score as the harmonic mean of precision and recall. Ac-\ncuracy serves as the primary metric given the near-balanced\nclass distribution (47.5% valid, 52.5% invalid).",
            "content": "Dual Evaluation Framework Each model prediction is evaluated against both ground truths independently. For syntactic evaluation, the model response maps correct valid and incorrect invalid, compared against ground truth syntax. For NLU evaluation, it maps correct believable and incorrect unbelievable, compared against ground truth NLU. This dual evaluation reveals whether models assess logical structure, natural language content, or some combination thereof. Belief Bias Effect Classical belief bias research employed indices derived from raw endorsement rates (Evans, Barston, and Pollard 1983; Klauer, Musch, and Naumer 2000). However, these traditional indices have been criticized on psychometric grounds (Dube, Rotello, and Heit 2010; Heit and Rotello 2014): changes in proportions starting from different baseline values are not readily comparable, and empirical receiver operating characteristic (ROC) curves reveal curvilinear relationships that violate the linear assumptions of difference scores. We adopt direct accuracy-based approach aligned with recent studies (Trippas, Handley, and Verde 2014), quantifying belief bias as the accuracy differential between congruent and incongruent syllogisms: bias = Acccongruent Accincongruent where Acccongruent is accuracy on valid-believable plus invalid-unbelievable instances (where logic and intuition align), and Accincongruent is accuracy on valid-unbelievable plus invalid-believable instances (where they conflict). This metric is appropriate for our setting because: (1) our LLM evaluations produce binary correct/incorrect judgments rather than confidence-rated responses, eliminating the ROC curvature concerns that motivated signal detection approaches (Dube, Rotello, and Heit 2010); (2) accuracy percentages are directly interpretable and comparable across all conditions, unlike endorsement-rate indices which suffer from baseline-dependency (Heit and Rotello 2014); (3) our within-subjects design compares each model against itself on congruent versus incongruent trials, isolating the belief bias effect while controlling for differences in overall reasoning ability. Positive bias indicates susceptibility to belief bias i.e., the model performs better when semantic content aligns with logical structure than when they conflict. Consistency Metric We measure response consistency across content variants of logically equivalent syllogisms. Let denote the set of 40 base natural syllogisms and ˆys,v the models prediction for syllogism under variant {N, X, O, OX}. We define: 1 [ˆys,N = ˆys,X = ˆys,O = ˆys,OX ] (1) Call = (cid:88) CNX = COOX = 1 1 sS (cid:88) sS (cid:88) sS [ˆys,N = ˆys,X ] [ˆys,O = ˆys,OX ] (2) (3) where Call denotes overall consistency across all four variants. The pairwise metrics isolate specific invariance properties: CNX tests robustness to natural language content (meaningful vs. nonsense predicates), while COOX tests robustness to premise order within matched content types."
        },
        {
            "title": "4 Results\nOur evaluation comprises 26,880 model-instance evalua-\ntions (14 models × 4 strategies × 3 temperatures × 160 syl-",
            "content": "Figure 3: Syntactic validity (left) versus natural language understanding believability (right). The 25.50pp gap (syntax: 81.7%, NLU: 56.2%) demonstrates that models excel at formal logical reasoning while struggling with semantic plausibility judgments. logisms). We report syntactic accuracy as the primary metric, with supplementary analyses of dual-framework evaluation, belief bias, variant robustness, and response consistency."
        },
        {
            "title": "4.1 Overall Performance\nPerformance exhibits a bimodal distribution across the 14\nevaluated models (Table 3). Six models achieve above 95%\nsyntax accuracy, forming a distinct top-tier with robust syl-\nlogistic reasoning capability. Gemini 2.5 Flash attains near-\nperfect performance (99.6%), deviating from perfect accu-\nracy in fewer than five instances per 1000. At the opposite\nextreme, five models score below 70%, with Llama 3.2 1B\nInstruct performing at 51.9%. The overall mean syntax ac-\ncuracy is 81.7% (SD = 17.1%), but the 47.7% gap between\ntop and bottom performers demonstrates that syllogistic rea-\nsoning capability depends critically on architectural choices\nand training methods rather than raw model scale.",
            "content": "The pattern of precision, recall, and F1 scores reveals systematic biases. Qwen3-Next 80B A3B Thinking shows 99.2% precision but only 42.8% recall, indicating it labels most syllogisms as incorrect even when valid. Conversely, Gemma 3 27B IT exhibits 93.1% recall but only 61.0% precision, suggesting over-acceptance of conclusions. Top-tier models maintain balanced precision-recall profiles (both >97%), demonstrating genuine discriminative capability. Dual Evaluation Framework We evaluated each prediction against both ground truths independently: syntactic validity and NLU believability (see 3.2). As shown in Figure 3 and Table 3 (final column), syntax accuracy (81.7%) substantially exceeds NLU accuracy (56.2%). Top-tier models show large syntax-NLU gaps: Gemini 2.5 Flash (47.9 pp), GPT-OSS-20B (47.9 pp), and Gemini 2.5 Pro (47.4 pp) excel at syntax but perform near chance on NLU evaluation. This pattern emerges because these models correctly judge logical validity independent of content believability."
        },
        {
            "title": "Model",
            "content": "Gemini 2.5 Flash GPT-OSS-20B Gemini 2.5 Pro GLM-4.6 Kimi-K2-Instruct DeepSeek V3.1 Gemini 2.5 Flash Lite Qwen3-Next 80B A3B Instruct Qwen3-Next 80B A3B Thinking Acc. Prec. Rec. F1 99.6 99.5 99.3 99.0 96.0 95. 88.9 79.4 72.7 100.0 100.0 100.0 100.0 97.0 99.6 89.8 73.3 99.2 99.1 99.0 98.6 97.8 94.5 91.6 86.5 88.9 42.8 99.6 99.5 99.3 98.9 95.7 95. 88.1 80.4 59."
        },
        {
            "title": "Call",
            "content": "99.0 96.5 98.3 95.8 88.3 89.0 71.9 69.2 76.7 CNX COOX NLU Acc. 99.2 97.1 98.8 96.5 93.1 92.1 82.9 81.0 81.9 99.2 98.1 98.5 97.5 90.6 91. 77.7 76.5 85.4 51.7 51.6 51.9 52.2 54.9 55.1 57.2 46.8 64.5 69.8 Llama 3.3 70B Instruct 68.4 Gemma 3 27B IT 64.3 Llama 3.1 8B Instruct 59.2 Llama 3.2 3B Instruct Llama 3.2 1B Instruct 51.9 All metrics in %. Acc. = Syntax Accuracy, Prec. = Precision, Rec. = Recall. Consistency metrics: Call (all 4 variants), CNX (normal nonsense), COOX (order-switched variants). 82.1 61.0 66.3 88.1 49.2 46.7 93.1 50.7 16.2 41. 66.2 69.0 51.9 75.0 57.9 81.0 82.5 75.6 92.1 76.7 59.5 73.7 57.4 27.4 45.3 78.3 86.0 62.1 81.7 73.8 66.3 43.6 56.8 73.7 60.4 Table 3: Comprehensive model performance metrics aggregated across all 12 configurations (4 strategies 3 temperatures). Syntax accuracy and NLU accuracy represent dual evaluation frameworks. Models grouped by performance tier. Conversely, three models exhibit negative gaps: Llama 3.2 3B Instruct (14.5 pp), Llama 3.2 1B Instruct (8.5 pp), and Llama 3.3 70B Instruct (+3.5 pp shows minimal gap), suggesting that lower-tier models may rely more heavily on semantic plausibility heuristics."
        },
        {
            "title": "4.2 Prompting Strategy Effects",
            "content": "Contrary to expectations, FS prompting yields the lowest mean accuracy (79.1%), while ZS achieves 82.7%. paired t-test confirms that FS significantly underperforms ZS ( = 3.57 pp, t41 = 2.50, = 0.0165), with the effect surviving Holm-Bonferroni correction for three comparisons (padj = 0.0495, Cohens = 0.39). However, Friedman test shows no significant overall strategy effect across all four strategies (χ2 = 3.24, df = 3, = 0.356), and Wilcoxon signed-rank tests reveal the effect becomes marginally non-significant after correction (p = 0.0195, padj = 0.0584). Figure 1 illustrates the lack of systematic strategy effects across models. To understand this pattern, we employed McNemars test at the instance level (N = 6720 syllogism evaluations: 14 models 3 temperatures 160 syllogisms). We find highly significant error redistribution: ZS solves 786 instances that FS fails, while FS solves only 546 that ZS fails (χ2 = 42.88, < 0.0001). The reconciliation is straightforward: FS prompting changes which problems are solved (McNemar test) and produces consistent directional decline in mean accuracy (t-test), but the median effect is less robust (Wilcoxon test). Strategy effects appear modelspecific rather than universal."
        },
        {
            "title": "Model",
            "content": "Cong. Incong. Llama 3.2 3B Instruct Llama 3.3 70B Instruct Qwen3-Next 80B A3B Thinking Llama 3.2 1B Instruct Llama 3.1 8B Instruct Gemini 2.5 Flash Lite DeepSeek V3.1 Kimi-K2-Instruct GLM-4.6 Gemini 2.5 Pro Gemini 2.5 Flash GPT-OSS-20B 82.0 85.3 86.3 62.0 70.6 95.0 99.7 99.6 99.4 100.0 100.0 99.2 35.2 53.6 58.3 41.2 57.7 82.5 91.8 92.1 97.5 98.6 99.2 98.4 bias +46.9 +31.6 +28.0 +20.8 +12.9 +12.5 +7.9 +7.5 +1.9 +1.4 +0.9 +0.8 Qwen3-Next 80B A3B Instruct Gemma 3 27B IT All values in %. Cong. = Congruent, Incong. = Incongruent. 7.9 83.4 75.4 13.7 75.5 61.7 Table 4: Belief bias analysis showing accuracy on congruent (logic matches intuition) versus incongruent (logic conflicts with intuition) syllogisms. Sorted by bias magnitude. all τ settings. The adaptive majority-voting mechanism effectively normalizes stochastic variation. We observe robust evidence of belief bias across the majority of models (Figure 2, Table 4). Twelve of 14 models exhibit positive belief bias i.e., higher accuracy on congruent problems than on incongruent problems. The mean bias effect is bias = +10.81 pp (SD = 16.32), statistically significant by paired t-test (t13 = 2.47, = 0.0280, Cohens = 0.66)."
        },
        {
            "title": "4.3 Temperature and Belief Bias Effects",
            "content": "Temperature (τ ) has negligible impact on accuracy when adaptive stopping is employed. Friedman test confirms no significant temperature effect (χ2 = 3.77, df = 2, = 0.152), with mean accuracy virtually identical across"
        },
        {
            "title": "4.4 Consistency and Benchmark Correlations\nThe consistency metrics in Table 3 reveal\nthat high-\nperforming models maintain high consistency across con-\ntent variants. The correlation between syntax accuracy and\noverall consistency is very strong (Pearson r = 0.877,",
            "content": "indicates that models optimized for logical structure may diverge from intuitive believability judgments."
        },
        {
            "title": "5 Discussion",
            "content": "In this study, we analyzed 40 instances of syllogism and their variations, resulting in total of 160 data points tested against 14 different large language models. Our results demonstrate striking pattern: top-tier models achieve near-perfect syntactic accuracy (99.6%) while performing at chance levels on natural language understanding (52%). This behavior, excelling at formal logic while struggling with semantic plausibility, contrasts sharply with human reasoning, where belief bias typically dominates logical analysis. The majority of models exhibit significant belief bias, performing better when logic aligns with intuition (mean effect: +10.81 pp, = 0.028). However, this bias decreases systematically with improved reasoning capability (ρ = 0.565, = 0.035), suggesting that higher-performing models increasingly prioritize formal rules over semantic heuristics. Architectural and training choices prove more consequential than raw parameter count by substantial margins. Counterintuitively, few-shot prompting degraded performance compared to zero-shot, suggesting demonstration examples may introduce noise in formal reasoning tasks. The strong correlation between instruction following quality (LMArena, ρ = 0.825) and reasoning accuracy indicates that precise rule adherence underlies both capabilities. These findings suggest that most models exhibit preference for symbolic reasoning and inferences rather than adhering to the natural language path of reasoning characteristic of human cognition. While this result may appear promising from purely logical perspective, it raises important questions about the alignment between LLM reasoning and human cognitive processes. These models were trained on extensive natural language data, yet the top performers appear to function more like formal logic engines than human-like reasoners susceptible to known natural language biases."
        },
        {
            "title": "6 Limitations",
            "content": "Our evaluation focuses primarily on categorical syllogisms, narrow subset of logical reasoning that may not generalize to more complex structures with nested quantifiers or modal operators. The dual ground truth framework, while enabling systematic measurement, necessarily simplifies the dynamic interaction between logic and natural language that humans navigate simultaneously in real reasoning contexts. The scope of our study includes only 14 models, representing snapshot of the current LLM landscape but not exhaustive coverage of all available systems. Our prompting strategies, while covering major paradigms (zero-shot, oneshot, few-shot, chain-of-thought), constitute limited exploration of the vast prompt engineering space. Additionally, our consistency metrics measure stability across content and order variations but do not assess robustness to adversarial perturbations or systematically manipulated distractors. Figure 4: Correlation between syllogistic reasoning accuracy and LMArena rankings (Spearman ρ = 0.825, = 0.0010, = 12). Lower rank indicates better performance. The strong negative correlation suggests that instructionfollowing quality predicts formal reasoning capability. < 0.0001; Spearman ρ = 0.890, < 0.0001), indicating that models achieving high accuracy are substantially more stable across variants. To contextualize syllogistic reasoning within the broader LLM evaluation landscape, we computed correlations with LMArena human preference rankings (Chiang et al. 2024; Zheng et al. 2023, 2024). As shown in Figure 4, syllogistic reasoning shows strong negative correlation with LMArena rank (Spearman ρ = 0.825, = 0.0010, = 12; lower rank indicates better performance). The negative correlation is the expected as models with higher reasoning accuracy achieve numerically lower (better) LMArena rankings. This suggests that models excelling at instruction following also excel at formal reasoning, likely because both require precise adherence to explicit rules."
        },
        {
            "title": "4.5 Statistical Summary",
            "content": "Table 5 consolidates all key statistical findings. The FS underperformance survives Holm-Bonferroni correction (padj = 0.0495), while the McNemar test reveals significant error redistribution at the instance level. The reconciliation between significant t-test and marginally nonsignificant Wilcoxon test (praw = 0.0195, padj = 0.0584) reveals that FS produces consistent mean decline but less robust median effect. The correlation between syntax accuracy and belief bias magnitude shows moderate negative relationship (Spearman ρ = 0.565, = 0.0353). Since bias effect is defined as Acccongruent Accincongruent, this negative correlation indicates that higher performing models exhibit smaller bias magnitudes. It further provides evidence that higher reasoning ability reduces reliance on content based heuristics. The very strong correlations between syntax accuracy and all three consistency metrics (ρ = 0.890, 0.846, and 0.837, all < 0.001) confirm that models achieving high accuracy are substantially more stable across content and order variations. The moderate negative correlation between syntax and NLU accuracy (Spearman ρ = 0.543, = 0.0449)"
        },
        {
            "title": "Statistic",
            "content": "df p-value"
        },
        {
            "title": "Result",
            "content": "Main Effects Strategy effect (overall) ZS vs FS ZS vs FS (Holm) Temperature effect Belief bias (Cong. > Incong.) Friedman χ2 Paired Paired Friedman χ2 Paired McNemar Tests (Instance-level, = 6720) ZS vs FS ZS vs OS ZS vs ZS CoT McNemar χ2 McNemar χ2 McNemar χ2 3.24 2.50 2.50 3.77 2.47 42.88 1.70 0. 3 41 41 2 13 0.356 0.0165 0.0495 0.152 0.0280 = 0.39 = 0.39 = 0.66 No effect Significant Survives correction No effect Confirmed 1 <0.0001 1 1 0.192 0. 786 vs 546 317 vs 284 389 vs"
        },
        {
            "title": "Error redistribution\nNo redistribution\nNo redistribution",
            "content": "Key Correlations (N = 14 models) Syntax Acc. Overall Consistency Syntax Acc. CNX Syntax Acc. COOX Syntax Prec. Syntax Rec. Syntax Acc. NLU Acc. Syntax Acc. Bias Effect Spearman ρ Spearman ρ Spearman ρ Spearman ρ Spearman ρ Spearman ρ 0.890 <0.0001 Very strong 0.846 0.0001 Very strong 0.837 0.0002 Very strong 0.691 0.0062 Strong 0.0449 0.543 Moderate 0.0353 0.565 Moderate Positive Positive Positive Positive Negative Negative Benchmark Correlation LMArena rank (lower = better) Very strong < 0.05, < 0.01, < 0.001. Holm-Bonferroni correction applied to strategy comparisons. McNemar instances: 786 vs 546 = ZS correct & FS wrong vs FS correct & ZS wrong. Bias correlation: Negative ρ means higher accuracy correlates with smaller bias magnitude (closer to zero). 0.825 0.0010 Spearman ρ"
        },
        {
            "title": "Predicts reasoning",
            "content": "Table 5: Comprehensive statistical summary of all hypothesis tests and correlations for 14 models. Strategy comparisons use Holm-Bonferroni correction. McNemar test operates at instance-level (6,720 syllogism evaluations per comparison). The belief bias metric, while grounded in cognitive psychology literature, captures only one dimension of the complex relationship between real world beliefs and logical reasoning. Future work should incorporate additional measures such as response time analysis, confidence calibration, and fine-grained error taxonomies to provide more comprehensive understanding of LLM reasoning processes."
        },
        {
            "title": "7 Future Work\nSeveral promising directions emerge from this work. Ex-\ntending evaluation to richer logical systems such as modal\nlogics, transitive closure logics, to test whether the observed\npatterns generalize beyond categorical syllogisms. Particu-\nlar interest lies in logical systems with simple formal syntax\nbut complex natural language semantics, which would fur-\nther stress the formal logic-natural language divide that we\nobserved.",
            "content": "Complementing these empirical extensions, mechanistic interpretability studies could reveal whether models learn explicit logical rules, statistical approximations, or hybrid representations. This would clarify the computational basis of the near-perfect syntactic performance we documented in top-tier models. Related to this, the causal relationship between reasoning capability and bias resistance remains an open question: does logical training reduce bias, or does reduced bias enable better reasoning? Controlled fine-tuning experiments could disentangle these possibilities. Our finding that few-shot prompting degraded performance challenges conventional wisdom and warrants systematic exploration of when and why demonstration examples help versus hinder reasoning. Such investigation would inform more effective prompting strategies for logical reasoning tasks. More broadly, our results raise fundamental tension i.e., are we building human like reasoners or formal logic engines? This question has implications not only for model development but also for appropriate deployment contexts and expectations for LLM behavior in reasoning-intensive applications. We intend to continue this line of inquiry across other logical reasoning tasks to better understand the trajectory of the cognitive capabilities of LLM. Acknowledgements We thank the Indo-French Centre for the Promotion of Advanced Research (IFCPAR/CEFIPRA) for their support. This work was supported through project number CSRP6702-2. References Borazjanizadeh, N.; and Piantadosi, S. T. 2024. Reliable Reasoning Beyond Natural Language. arXiv e-prints, arXiv2407. Brauner, T.; Ghosh, A.; and Ghosh, S. 2025. Understanding responses of people with ASD in diverse reasoning tasks: formal study. Cognitive Processing, 26(1): 201218. Bubeck, S.; Chandrasekaran, V.; Eldan, R.; Gehrke, J.; and Horvitz, E. 2023. Ece 1251 Kamar, Peter Lee, Yin Tat Lee, Yuanzhi Li, Scott Lundberg, et al. Sparks of artificial general 1252 intelligence: Early experiments with gpt-4. arXiv preprint arXiv:2303.12712, 1253. Solcz, S. 2008. The Role of Working Memory in Deductive Reasoning: Dual Task and Individual Differences Approach. Ph.D. thesis, University of Waterloo. Trippas, D.; Handley, S. J.; and Verde, M. F. 2014. Fluency and belief bias in deductive reasoning: New indices for old effects. Frontiers in Psychology, 5: 631. Zhang, K.; Yu, W.; Sun, Z.; and Xu, J. 2025. Syler: framework for explicit syllogistic legal reasoning in large language models. In Proceedings of the 34th ACM International Conference on Information and Knowledge Management, 41174127. Zhao, W. X.; Zhou, K.; Li, J.; Tang, T.; Wang, X.; Hou, Y.; Min, Y.; Zhang, B.; Zhang, J.; Dong, Z.; et al. 2023. arXiv preprint Survey of Large Language Models. arXiv:2303.18223. Zheng, L.; Chiang, W.-L.; Sheng, Y.; Li, T.; Zhuang, S.; Wu, Z.; Zhuang, Y.; Li, Z.; Lin, Z.; Xing, E.; Gonzalez, J. E.; Stoica, I.; and Zhang, H. 2024. LMSYS-Chat-1M: Large-Scale Real-World LLM Conversation Dataset. In The Twelfth International Conference on Learning Representations. Zheng, L.; Chiang, W.-L.; Sheng, Y.; Zhuang, S.; Wu, Z.; Zhuang, Y.; Lin, Z.; Li, Z.; Li, D.; Xing, E.; Zhang, H.; Gonzalez, J. E.; and Stoica, I. 2023. Judging LLM-as-a-Judge with MT-Bench and Chatbot Arena. In Thirty-seventh Conference on Neural Information Processing Systems Datasets and Benchmarks Track. Zong, S.; and Lin, J. 2024. Categorical Syllogisms Revisited: Review of the Logical Reasoning Abilities of LLMs for Analyzing Categorical Syllogisms. In Proceedings of the 1st Workshop on NLP for Science (NLP4Science), 230239. Chen, X.; Aksitov, R.; Alon, U.; Ren, J.; Xiao, K.; Yin, P.; Prakash, S.; Sutton, C.; Wang, X.; and Zhou, D. 2023. Universal self-consistency for large language model generation. arXiv preprint arXiv:2311.17311. Chiang, W.-L.; Zheng, L.; Sheng, Y.; Angelopoulos, A. N.; Li, T.; Li, D.; Zhang, H.; Zhu, B.; Jordan, M.; Gonzalez, J. E.; and Stoica, I. 2024. Chatbot Arena: An Open Platform for Evaluating LLMs by Human Preference. arXiv:2403.04132. Copi, I. M.; Cohen, C.; and McMahon, K. 2016. Introduction to logic. Routledge. Dube, C.; Rotello, C. M.; and Heit, E. 2010. Assessing the belief bias effect with ROCs: Its response bias effect. Psychological Review, 117(3): 831863. Eisape, T.; Tessler, M.; Dasgupta, I.; Sha, F.; Steenkiste, S.; and Linzen, T. 2024. systematic comparison of syllogistic reasoning in humans and language models. In Proceedings of the 2024 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (Volume 1: Long Papers), 84258444. Evans, J. S. B.; Barston, J. L.; and Pollard, P. 1983. On the conflict between logic and belief in syllogistic reasoning. Memory & cognition, 11(3): 295306. Heit, E.; and Rotello, C. M. 2014. Traditional differencescore analyses of reasoning are flawed. Cognition, 131(1): 7591. Holliday, W.; Mandelkern, M.; and Zhang, C. 2024. Conditional and Modal Reasoning in Large Language Models. In Proceedings of the 2024 Conference on Empirical Methods in Natural Language Processing, 38003821. Kim, G.; Valentino, M.; and Freitas, A. 2025. Reasoning circuits in language models: mechanistic interpretation of syllogistic inference. In Findings of the Association for Computational Linguistics: ACL 2025, 1007410095. Klauer, K. C.; Musch, J.; and Naumer, B. 2000. On belief bias in syllogistic reasoning. Psychological review, 107(4): 852. Kojima, T.; Gu, S. S.; Reid, M.; Matsuo, Y.; and Iwasawa, Y. 2022. Large language models are zero-shot reasoners. Advances in neural information processing systems, 35: 2219922213. Lewton, M. 2016. The relationship between autism and psychosis traits and reasoning style. Ph.D. thesis, University of Bath. Pennycook, G.; Cheyne, J. A.; Koehler, D. J.; and Fugelsang, J. A. 2013. Belief bias during reasoning among religious believers and skeptics. Psychonomic Bulletin & Review, 20(4): 806811. Sambrotta, M. 2025. LLMs and the Logical Space of Reasons. Minds and Machines, 35(4): 123. Smith, R. 2022. Aristotles Logic. In Zalta, E. N.; and Nodelman, U., eds., The Stanford Encyclopedia of Philosophy. Metaphysics Research Lab, Stanford University, Winter 2022 edition. Smith, R.; et al. 1989. Prior analytics. Hackett Publishing."
        }
    ],
    "affiliations": [
        "Indian Statistical Institute, Chennai",
        "Institute of Engineering & Management, Kolkata"
    ]
}