{
    "paper_title": "Progressive Multimodal Reasoning via Active Retrieval",
    "authors": [
        "Guanting Dong",
        "Chenghao Zhang",
        "Mengjie Deng",
        "Yutao Zhu",
        "Zhicheng Dou",
        "Ji-Rong Wen"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Multi-step multimodal reasoning tasks pose significant challenges for multimodal large language models (MLLMs), and finding effective ways to enhance their performance in such scenarios remains an unresolved issue. In this paper, we propose AR-MCTS, a universal framework designed to progressively improve the reasoning capabilities of MLLMs through Active Retrieval (AR) and Monte Carlo Tree Search (MCTS). Our approach begins with the development of a unified retrieval module that retrieves key supporting insights for solving complex reasoning problems from a hybrid-modal retrieval corpus. To bridge the gap in automated multimodal reasoning verification, we employ the MCTS algorithm combined with an active retrieval mechanism, which enables the automatic generation of step-wise annotations. This strategy dynamically retrieves key insights for each reasoning step, moving beyond traditional beam search sampling to improve the diversity and reliability of the reasoning space. Additionally, we introduce a process reward model that aligns progressively to support the automatic verification of multimodal reasoning tasks. Experimental results across three complex multimodal reasoning benchmarks confirm the effectiveness of the AR-MCTS framework in enhancing the performance of various multimodal models. Further analysis demonstrates that AR-MCTS can optimize sampling diversity and accuracy, yielding reliable multimodal reasoning."
        },
        {
            "title": "Start",
            "content": "Guanting Dong Chenghao Zhang Mengjie Deng Yutao Zhu Zhicheng Dou* Ji-Rong Wen Gaoling School of Artificial Intelligence, Renmin University of China. {dongguanting, dou}@ruc.edu.cn 4 2 0 2 9 1 ] . [ 1 5 3 8 4 1 . 2 1 4 2 : r a"
        },
        {
            "title": "Abstract",
            "content": "Multi-step multimodal reasoning tasks pose significant challenges for multimodal large language models (MLLMs), and finding effective ways to enhance their performance in such scenarios remains an unresolved issue. In this paper, we propose AR-MCTS, universal framework designed to progressively improve the reasoning capabilities of MLLMs through Active Retrieval (AR) and Monte Carlo Tree Search (MCTS). Our approach begins with the development of unified retrieval module that retrieves key supporting insights for solving complex reasoning problems from hybrid-modal retrieval corpus. To bridge the gap in automated multimodal reasoning verification, we employ the MCTS algorithm combined with an active retrieval mechanism, which enables the automatic generation of step-wise annotations. This strategy dynamically retrieves key insights for each reasoning step, moving beyond traditional beam search sampling to improve the diversity and reliability of the reasoning space. Additionally, we introduce process reward model that aligns progressively to support the automatic verification of multimodal reasoning tasks. Experimental results across three complex multimodal reasoning benchmarks confirm the effectiveness of the AR-MCTS framework in enhancing the performance of various multimodal models. Further analysis demonstrates that AR-MCTS can optimize sampling diversity and accuracy, yielding reliable multimodal reasoning. 1. Introduction Reasoning, as the fundamental capability of large language models (LLMs) [25, 67, 72, 112] and multimodal large language models (MLLMs) [6, 14, 54, 125], lays the foundation for generalization across wide range of downstream tasks such as mathematical reasoning [58, 59, 116] and visual question answering [1, 3, 37]. In complex reasoning scenarios, models often require multiple steps to seek final answer, with each reasoning step potentially generating several branches and resulting in various candidate reasoning paths. Therefore, efficiently identifying the correct path that includes key problem-solving steps while eliminating incorrect ones is essential. To achieve this, reasoning verification techniques [49, 85, 106] enable models to explore multiple candidate solutions and employ high-quality reward model for path selection, thereby offering promising approach to improve the reliability of model reasoning. To improve the trustworthiness of complex reasoning, foundational efforts such as outcome reward models (ORMs) [17, 88] directly verify the quality of entire reasoning trajectories. However, ORMs can only provide sparse and result-oriented feedback. To obtain finer-grained verification, process reward models (PRMs) [48, 49, 64, 85, 101, 102, 106] are designed, offering intermediate rewards after each step and incorporating reinforcement learning from human feedback (RLHF) algorithms [73] for backward supervision of generative models. Despite these advancements, the manual annotation of reasoning paths requires lots of human resources, limiting its scalability and applicability [111]. In response to these challenges, recent developments in inference-time scaling [82, 89] have led to the integration of the MCTS algorithm into LLMs [7]. This combination allows models to autonomously sample potential reasoning paths at each step during the expansion process. Then, different value functions are designed to estimate the quality of each path, followed by back-propagation and pruning, finally achieving automatic step-level reasoning annotation without human effort [2, 9, 29, 55, 61, 93, 98, 108, 114, 117, 118]. While MCTS-based methods have been widely applied to text-based LLMs, their adaptation for MLLMs remains largely unexplored. Indeed, the distinct characteristics of multimodal scenarios require specialized adaptations of MCTS to effectively address their complexities. Let us illustrate the challenges by theoretically analyzing the limitations of existing MCTS-based methods. Given the input at each expansion step of MCTS, and the best reasoning path selected after simulation, the process could be modeled as: p(y x) = max ik pθ(y ri, x) (cid:125) (cid:123)(cid:122) (cid:124) Simulation , pϕ(ri x) (cid:125) (cid:123)(cid:122) (cid:124) Expansion (1) * Corresponding author. where and represent reasoning paths and the number of sampled paths, while ϕ and θ denote the generator and the verifier respectively. From this formulation, we can observe that both the expansion and simulation phases are crucial to the process, jointly determining the success of reasoning. Most existing approaches focus on optimizing the simulation process, while leaving the expansion process by beam search that relies on the models internal knowledge [27, 83]. This simple strategy is effective for text-only reasoning tasks, as LLMs are sufficiently pre-trained on text data and their internal knowledge can be accurately measured. However, in multimodal reasoning tasks, the internal knowledge of MLLMs is insufficient for reasoning path expansion, because the interactions between inputs from different modalities frequently encounter misalignment [80, 90]. Such errors will grow larger as each step in the reasoning process depends on the previous one, causing small mistakes to become bigger over time [51, 66]. Consequently, developing effective strategies for reliable path expansion in MLLMs poses significant challenge in multimodal reasoning tasks. To address these problems, we propose to leverage retrieved external knowledge in reasoning path expansion, to enhance the path sampling quality in MCTS and improve MLLMs capability in complex multimodal reasoning. Recent studies have confirmed that retrieval-augmented techniques can bridge knowledge gaps in multimodal reasoning [52, 56, 95], but they take all retrieved knowledge as whole in the inference. Intuitively, each reasoning step requires different knowledge in complex task. Therefore, we aim to dynamically provide the appropriate knowledge at each step of the reasoning process, thereby enhancing the accuracy of reasoning paths. Furthermore, we propose to incorporate diverse problem-solving insights into both the expansion and value function of the MCTS algorithm. This integration is expected to not only expand the diversity of the sampling space but also enhance the reliability of the reasoning verification process. Specifically, we propose AR-MCTS, universal framework dedicated to progressively improving the complex reasoning capabilities of MLLMs through Active Retrieval and Monte Carlo Tree Search. Specifically, we first design unified retrieval module to retrieve key problem-solving insights for supporting complex reasoning from hybrid-modal retrieval corpus. To further achieve reliable multimodal reasoning verification, we define the quality of each reasoning step as its potential to deduce the correct answer, enabling us to iteratively obtain step-wise annotations using the MCTS algorithm. Notably, we propose an active retrieval strategy during the MCTS expansion process, innovatively replacing beam search sampling with dynamically retrieved problemsolving insights, thereby enhancing both the diversity and reliability of the sampling space. Based on these fine-grained annotations, we progressively align process reward model tailored for multimodal reasoning through step-wise Direct Preference Optimization (DPO) [78, 79] and Supervised Fine-tuning (SFT) objectives, achieving automatic processlevel reasoning verification. Experimental results on three complex multimodal reasoning benchmarks demonstrate the effectiveness of ARMCTS across various proprietary models. Further analysis reveals that AR-MCTS optimizes both sampling diversity and verification accuracy, providing promising solution for reliable multimodal reasoning. In summary, our contributions are as follows: We theoretically model the key mechanisms of the MCTSbased approach in Equation (1), revealing its core limitations in solving multimodal reasoning problems. We are the first to introduce the retrieval mechanism in each step of multimodal reasoning to replace traditional model self-sampling strategies, enhancing both sampling diversity and accuracy of multi-step reasoning. We propose the AR-MCTS framework, which leverages the MCTS algorithm alongside an active retrieval strategy for improving multimodal reasoning. This framework automatically acquires high-quality step-wise reasoning annotations to progressively align process reward model, ultimately enabling reliable automated multimodal reasoning verification. 2. Related Work LLM and MLLM Reasoning. Large Language Models (LLMs) [25, 67, 72, 97, 112] and Multimodal Large Language Models (MLLMs) [6, 13, 14, 42, 54, 104, 125] have rapidly advanced, with broad applications in mathematics [106, 126], programming [31, 91], medicine [45], character recognition [76, 109]. Among their diverse capabilities, reasoning stands out as the most critical, serving as foundational step toward universal understanding. Methods such as Chain-of-Thought (CoT) [110], Tree-of-Thought (ToT) [113], and Program-of-Thought (PoT) [12, 28] enhance logical coherence and response complexity by guiding models to decompose problems progressively, applying structured prompts and targeted training objectives, including multimodal tasks [121]. Another key approach is the reflection mechanism, which prompts models to iteratively evaluate and refine responses, leading to improved coherence. Prior studies [107] show that generating diverse reasoning paths and selecting the most consistent responses enhances LLM reasoning. Moreover, some efforts enhance the reasoning capability of LLM and MLLM by integrating data augmentation during the SFT phase [46, 59, 87, 115], along with utilizing external tools [30, 44]. Research [17, 48, 64, 86, 88, 100, 102, 106] has also highlighted robust reward models as promising means to optimize response quality. Recently, OpenAIs o1 1 introduced slow thinking mechanism that, combined with Monte Carlo Tree Search (MCTS) strategies and verification models, simulates gradual reasoning to improve accuracy [29, 117, 118]. While these advancements are focused on single-modal LLMs, the reasoning potential of MLLMs remains under explored. Multimodal Retrieval-Augmented Generation. Recent Retrieval-Augmented Generation (RAG) has shown exceptional performance across various NLP tasks for LLMs by incorporating relevant information from diverse sources [4, 16, 20, 22, 24, 35, 40, 47, 60, 96]. This approach can also enhance reasoning and question-answering in the multimodal domain through cross-modal integration [50, 52, 95, 124]. However, the reasoning process is largely unexplained and lacks verification mechanisms. In this paper, we propose an active retrieval strategy that retrieves multimodal information at each step to align the PRM, facilitating reliable reasoning verification. 3. Preliminary Monte Carlo Tree Search. MCTS is widely used sampling-based search method for decision-making optimization. Its core algorithm consists of four steps: selection, expansion, evaluation, and back-propagation. By repeatedly executing these four steps, it constructs search tree. During the selection phase, MCTS recursively selects child nodes from the root using the Upper Confidence Bound (UCB) [92]: UCB(i) = wi + 2 ln (cid:114) Ni ni , (2) Problem Formulation. Formally, in multimodal reasoning, given multimodal query Qm and corresponding retrieved problem-solving insights from the retrieved hybridmodal corpus DH , we assume that the MLLM πθ operates in an auto-regressive manner to generate reasoning path of steps [y1, . . . , yk]: pθ(y Qm, R) = (cid:89) i= pθ (yi Qm, r, y<i) . (3) In this paper, we obtain different intermediate reasoning trajectories as the MLLM decodes to specific termination token. Following the setup of [99], we formulate the generation process as Markov Decision Process (MDP) [74] and adopt sentence-level MCTS modeling. In reinforcement learning terminology [84], we define the current decoded intermediate step yi as state si, corresponding to leaf node. The process of backtracking to sample the next step is considered an action ai. list of detailed definitions of MCTS for reasoning is given in the supplementary materials. 1https : / / openai . com / index / openai - o1 - system - card/ Figure 1. The statistics of our hybrid-modal retrieval corpus. 4. Methodology Our goal is to establish process-level verification framework for improving multimodal reasoning without human annotation, while improving the diversity and accuarcy of candidate solution sampling. Therefore, we propose ARMCTS framework to achieve fine-grained reasoning verification through active retrieval and Monte Carlo tree search. As shown in Figure 2 & 3, AR-MCTS consists of two main components: 1) It introduces unified retrieval module, including high-quality hybrid-modal retrieval corpus (4.1) and multimodal retrieval module (4.2). This module employs knowledge concept filtering to select key insights for problem-solving (4.3). 2) It automates the acquisition of step-wise annotations for multimodal reasoning using MCTS and an active retrieval mechanism (4.2). Then, it leverages the annotated data to progressively align the PRM in two stages (4.5), allowing for fine-grained verification of MLLM reasoning. 4.1. Hybrid-Modal Retrieval Corpus Construction improving reasoning capabilities In an ideal scenario, through retrieval is akin to giving MLLMs an open-book exam. Unfortunately, the multimodal reasoning field consistently suffers from lack of high-quality reasoning retrieval corpora. To systematically build high-quality reasoning retrieval library, we conduct comprehensive survey of opensource datasets, focusing on both general and mathematicsspecific reasoning knowledge in multimodal reasoning. Mathematics-Specific Reasoning Knowledge. Mathematical reasoning is an essential skill of fundamental models, accompanied by the emergence of series of high-quality datasets. In the text-only aspect, we select the most widely used mathematical reasoning datasets, GSM8K [18] and MATH [33]. For the multimodal domain, we adopt four meticulously cleaned high-quality multimodal math datasets: MATHVISTA [58], MathVerse [120], MathVision [103], and WE-MATH [75]. To further prevent data leakage, we filtered out any overlapping portions with our testing benchmark using regular expressions, concatenating each samples question q, solution process p, and answer into single text format, along with the corresponding image storage paths. Ultimately, we obtain 22K text-only QA pairs and 12.5K multimodal sample pairs as proprietary sources DM from six data sources, covering over 20 mathematical sub-fields, with each sample containing detailed solution steps. General Reasoning Knowledge. In the real world, general reasoning extends beyond natural subjects. To address this broader need, we follow the traditional RAG approach [41, 122] by utilizing the web-based retrieval source Wikipedia alongside the COIG [119] large-scale question bank as our general reasoning retrieval sources. We conduct thorough data cleaning and chunking operations, ultimately constructing this extensive dataset as our general reasoning knowledge base DG. The statistical information of our hybrid-modal reasoning corpus DH = DM DG is presented in Figure 2. More detailed information of processing retrieval corpus can be found in the supplementary. 4.2. Unified Multimodal Retrieval Module Given text-image pair from the multimodal test set Qm = {x, t}, our goal is to retrieve the top-K multimodal relevant knowledge for each sample. Since our retrieval library encompasses hybrid-modal retrieval sources, two retrieval processes are considered to obtain the top-K pairs: Text Retrieval. Given text query for multimodal sample, we aim to use dense retriever to retrieve relevant documents Dq = {di}k i=1 from text-only corpus. In this work, we employ Contriever [34] to obtain hidden vectors for both queries and documents. The relevance score is calculated by computing the dot-product similarity between the query and document representations, which facilitates the retrieval of the Top-K documents Dq as follow: Dq = argtopi=1,...,N (cid:104) Ed(di) Eq(q) (cid:105) . (4) Cross-modal Retrieval. We utilize widely used contrastive vision-language models CLIP [77], which utilizes dual-stream architecture featuring an image encoder EI () and text encoder ET (). Furthermore, we use CLIP to encode image-text pairs (x, t), obtaining the image and text vectors EI (x) and ET (t). Since the hybrid-modal retrieval corpus contains both multimodal and text-only samples, we follow previous work [95] to derive encoding vectors for the entire hybrid-modal corpus DH as follows: Ex(x, t) = (cid:40) EI (x)+ET (t) 2 ET (t), , if = and = , if = and = . (5) where denotes empty set. For the i-th multimodal query Qm, we encode it into mixed vector Ex(Qm) = EI (x)+ET (t) . We perform cross-modal retrieval between the 2 encoding of each multimodal query and the entire retrieval database, utilizing FAISS [36] for indexing to retrieve Figure 2. The pipeline of our unified multimodal retrieval module. samples for each query: Dcross = argtopj=1,...,N (cid:104) Ex(Qm) Ex(xj, tj) (cid:105) . (6) Here, Ex(Qm) and Ex(xj, tj) denote the embeddings of the multimodal query and the samples in the hybrid-modal corpus, with indices ranging from 1 to to ensure that the entire retrieval database is considered. 4.3. Knowledge Concept Filtering In our deployment process, we observe that multimodal reasoning with retrieved knowledge is highly sensitive to the consistency of fine-grained knowledge concepts (e.g., algebra knowledge cant help in solving triangles problem). Notably, most high-quality visual mathematical benchmarks provide detailed category labels (e.g., Angles and Length) for each sample, motivating us to consider knowledge concept for fine-grained filtering. Given multimodal query Qm and its knowledge concept label Lkc, we encode the top-K retrieved hybrid-modal samples from DH = {Dq Dcross} according to Equation (5) and compute the similarity with the knowledge concepts embedding ET (kc) following the pipeline in Cross-Modal Retrieval. We strictly enforce the original retrieval similarity threshold Tr and the knowledge concept consistency threshold Tkc, allowing only those samples that meet both criteria to serve as key insights Dins for the query Qm: Dins = {r DH Sim(r, Qm) Tr & Sim(r, Lkc) Tkc}, where Sim(x, y) represents the cosine similarity between the embeddings E(x) and E(y), DH denotes retrieved insights from the corpus DH. Detailed information of the filtering process can be found in supplementary materials. 4.4. Progressive Multimodal Reasoning Annotation In this section, we employ MCTS via active retrieval to facilitate MLLMs in the automatic generation of step-wise reasoning annotations, as shown in Figure 3. Through the self-exploration process, we obtain values at each step (node) to capture potential reasoning errors in the intermediate steps. Below, we will present our detailed algorithm design, which includes four core operations: Selection. During the j-th simulation of the AR-MCTS, the process begins with s0, representing the initial state containing the multimodal input query Qm 0 = (x0, t0) Figure 3. The overall framework of AR-MCTS: The retrieval module actively retrieves key insights at each step of the MCTS process. Then, the states of the MCTS is enhanced with different insights to expand the possible action space of the MLLM. Notably, one state of each step, such as state S1,3 and S2,3 in this figure, no insights are provided, and the state is direct output of the MLLM. and corresponding retrieved problem-solving insights r0. The algorithm then proceeds to explore the Monte Carlo tree by selecting as Equation (2) iteratively, then we can formulate the multimodal query of state sj as Qm = {(xj, tj) tj = t0 + (cid:80)j i=1 yi}, as shown in Figure 3. Expansion with Active Retrieval Strategy. Given the state si represented by the selected leaf node, the MCTSbased approach backtracks to the prior state, forming our multimodal input as (xi, ti, ri). The temperature in the traditional expansion process is empirically increased to greater than 0.6 to sample multiple potential candidate actions for the next step [10]. Unlike them, we emphasize that the supporting knowledge required for different reasoning trajectory at each step should vary, and propose an Active Retrieval strategy. As shown in Figure 3, during the MCTS expansion phase at state si, we first concatenate the input Qm with the previous reasoning steps. Then we dynamically retrieve the required candidate insights ri for each step from the problem-solving insight library Dins according to Equation (6), and replace the insight ri1 from the previous step with the latest retrieved insights ri. According to the Equation (3), the process of sampling reasoning paths at state si can be modeled as follows: pθ(y x) = (cid:89) i= pθ (cid:16) {yj }k j=1 Qm , ri (cid:17) . (7) Simulation. We use the probability of deducing the correct answer based on partial solutions as criterion for quality assessment. Following Wang et al., we apply one-step rollout for each node obtained during expansion to ensure efficiency, and we construct value function as (cid:80)k I(yj = ˆyi) j=1 , where k, denotes the number of (si) = sampled reasoning paths and the indicator function. If the final answer yj equals the grounding truth ˆyi, we set the value of the current node to 1; Otherwise, we set it to 0. Back-Propagation. For the terminal nodes reached during the rollout and the current leaf node, MCTS performs backward update of the visit count and Q-value for each (s, a) along the route from the current node to the root, which is fomulated as (s, a) (s, a) + 1, Q(s, a) Q(s, a) + 1 (s,a) (V (s) Q(s, a)). 4.5. Curriculum Process Reward Modeling After acquiring step-wise reasoning annotations, we draw inspiration from curriculum learning [23, 94] to design two-phase approach for PRM. In the first stage, the model learns to distinguish the correctness of reasoning steps. In the second stage, it learns to assign scores to each step, facilitating generalization from easy to hard. Step-wise DPO Pre-alignment. In the first phrase, each round of expansion and evaluation in AR-MCTS naturally generates batches of positive and negative pairs, inspiring us to align preferences using step-level Direct Preference Optimization (DPO) as the training objective. Under the state si (i-th step in reasoning), given multimodal query Qm = (xi, ti) and sampled reasoning paths set Yi = {yj}k j=1 , along with the corresponding value set Vi = {vj}k j=1. We filter the solution paths in Yi with value vj > 0.8 as positive samples y+ , while those with vj = 0 are regarded as negative samples . Therefore, for each problem Qm , we can obtain pairs of step-level preference pairs Dstep j=1 and follow step-level DPO to = (y+ , )K align the reasoning discernment capability as: LSDPO(πθ; πref) = E(Qm,y+,y)Dstep[logσ(βlog πθ(y+Qm) πθ(y+Qm) βlog πref(yQm) πref(yQm) )], The reference model πref is initially set to πθ and remains constant during training. Here, β is hyperparameter, and σ denotes the sigmoid function. The objective of LSDPO is to maximize the likelihood of preferred y+ compared to the dispreferred y. Point-wise Fine-tuning. After pre-alignment, our PRM has gained the initial ability to distinguish the correctness of step-wise reasoning. To further unlock its reasoning scoring capability, we apply step-level cross-entropy objective to the pre-aligned PRM πDPO using the following parameters: LPFT = (cid:88) i=1 (cid:2) ˆyi logπSDPO (ri) + (1 ˆyi) logπSDPO (1 ri)(cid:3) , where ˆyi is the golden label (0, 1) for the state si, ri is the sigmoid score assigned by PRM. After the above two stages, we progressively achieve an aligned process reward model. Inference. During the inference phase of AR-MCTS, we utilize the fine-tuned PRM to follow the steps of AR-MCTS in Figure 3, employing the PRM scores as the value for each step in the evaluation phrase. Following Luo et al., we adopt point-wise soft labels. We also make discussion of PRMs hard labels in the supplementary materials. Unlike the annotation process for training data, we extract the topscoring node from the expansion reasoning paths each round, discarding the other low-quality paths. Moreover, we set an early stopping criterion of 4, which allows us to derive the final result directly in the 4-th round to reduce computational complexity. 5. Experiments 5.1. Experimental Setup To assess the effectiveness of the AR-MCTS, we provide detailed introduction from the following aspects: Benchmarks and Baselines. We perform experiments on two widely used multimodal mathematical reasoning benchmarks: MATHVISTA [58] and WE-MATH [75]. To further validate our AR-MCTS in general reasoning domain, we perform cross-domain evaluation on the GAOKAO-MM benchmark [127]. For baselines, we employ AR-MCTS on strong proprietary and open-source models: (1) Closed-source MLLMs: GPT-4o [70], GPT-4V [71]; (2) Open-source MLLMs: LLaVA-OneVision-Qwen2-72B [43], InternVL28B [15], Qwen2-VL-7B [105], LLaMA3-LlaVA-NeXT8B [53]. Referencing relevant works on MCTS [99, 106], we implement Self-Consistency [107], Self-Correction [32], and ORM [17] as our core comparison strategies. Data Sampling via AR-MCTS. As highlighted by MathPUMA [126], the challenge arises because the three multimodal benchmarks we evaluate lack training sets. Following the collection described in 4.1, we utilize four multimodal and two text-only datasets for process annotation, excluding any sources currently under evaluation. We extract multimodal QA pairs and use our AR-MCTS algorithm to automatically generate and annotate detailed solution processes. Notably, the GAOKAO-MM dataset is entirely in Chinese, which complicates reliance on English data sources. To address this, we classify data from 2010 to 2021 for AR-MCTS annotation, while questions from 2022 to 2023 serve as the test set. For comprehensive overview of the experimental setup, please find in the supplementary materials.. 5.2. Overall Results Table 1 illustrates the main results. Overall, AR-MCTS significantly improves multimodal reasoning performance across various MLLMs and reasoning verification strategies (Self-Correction, Self-Consistency) on two benchmarks, conclusively demonstrating the advantages of our approach. Furthermore, we have identified the following insights: MLLMs struggle to self-correct reasoning errors. The self-correction strategy struggles across both reasoning benchmarks. Although minor improvement is noted with GPT-4o, other weaker open-source MLLMs experience significant declines after the self-correction process, particularly Qwen2VL-7B, which shows drop of over 8% on MATHVISTA (ALL). This discovery corresponds with the findings of He et al., highlighting the instability of correction methods that rely on the self-knowledge of MLLMs in multimodal reasoning, especially in models with fewer parameters. PRM outperforms ORM in complex reasoning tasks. WE-MATH is step-level evaluation featuring S1 to S3, which progressively increases the difficulty of reasoning steps. Compared to ORM, AR-MCTS with PRM demonstrates more significant performance improvement across most MLLM backbones on the S3 metrics in WE-MATH (GPT-4o: 56.4% vs 50.3%; Qwen2-VL: 40.6% vs 34.6 %). This highlights that PRM, by meticulously verifying each step of the reasoning process, achieves stronger alignment in multi-step reasoning tasks. AR-MCTS better unlocks the reasoning potential of weaker MLLMs. Compared to LLaVA-OneVision-72B, Qwen2-VL-7B with AR-MCTS shows significant improvement over the zero-shot setting on MATHVISTA (ALL: 5.3%) and in WE-MATH (AVG: 8.3%). similar conclusion is observed with InternVL2-8B, indicating that the performance gains of AR-MCTS are more pronounced in smaller MLLMs. To gain insight into this result in conjunction with Equation (1), we consider that, under the same verifier, weaker MLLMs may sample the correct answers but struggle to directly decode those paths greedily. This Table 1. Mathematical reasoning assessment on different MLLMs using MATHVISTA and WE-MATH testmini Sets. In the case of MATHVISTA, we picked 6 categories from the original 12: ALL (overall accuracy), GPS (geometry problem solving), MWP (math word problems), ALG (algebraic reasoning), GEO (geometry reasoning), and STA (statistical reasoning). For WE-MATH, we selected 8 categories: S1 (one-step problems), S2 (two-step problems), S3 (three-step problems), AVG (strict overall average scores), IK (insufficient knowledge), IG (inadequate generalization), CM (complete mastery), and RM (rote memorization). The top scores for each model are highlighted in bold. Model Method MATHVISTA WE-MATH ALL GPS MWP ALG GEO STA S1 S2 S3 AVG IK IG CM RM GPT-4o LLaVAOneVision-72B InternVL2-8B Qwen2-VL-7B Zero-shot Self-Consistency Self-Correction ORM AR-MCTS Zero-shot Self-Consistency Self-Correction ORM AR-MCTS Zero-shot Self-Consistency Self-Correction ORM AR-MCTS Zero-shot Self-Consistency Self-Correction ORM AR-MCTS 59.0 61.8 59.9 61.9 62.6 64.2 66.0 58.3 65.9 66.3 57.3 61.8 46.8 61.1 63.1 58.8 61.2 50.8 62.3 64. 59.6 68.3 61.1 68.3 68.6 80.8 79.8 78.4 80.3 79.8 62.5 77.4 57.7 67.8 62.9 45.5 54.8 43.3 55.5 63.9 65.1 65.1 65.6 66.1 66.4 69.4 73.1 68.8 73.1 73. 62.4 64.0 31.2 64.0 71.6 60.5 61.8 53.2 62.7 72.6 61.2 68.0 61.2 68.0 68.0 73.3 74.0 70.1 74.0 74.4 61.2 73.0 55.9 64.1 59.9 45.5 56.2 45.9 56.9 60. 60.7 68.2 61.1 68.2 68.8 77.0 76.6 74.9 77.0 76.6 60.7 72.8 56.1 64.9 62.6 47.9 55.2 43.9 56.5 63.6 72.4 74.8 72.8 74.8 75.3 66.8 67.8 56.8 67.8 67. 59.1 62.1 46.2 68.4 71.4 70.8 72.1 62.1 72.4 72.4 71.5 58.3 46.1 73.3 63.6 53.0 72.8 58.9 43.6 73.1 63.3 50.3 74.7 65.6 56.4 58.1 44.7 40.6 70.7 52.8 38.2 48.2 33.9 30.3 66.6 48.3 44.2 71.1 52.8 38.9 50.0 36.7 23.6 58.4 47.1 35.1 43.5 28.1 30.3 64.0 45.0 32.7 65.1 52.2 43.6 53.4 37.2 33.9 57.6 41.9 33.9 52.3 38.6 26.7 57.8 45.1 34.6 59.9 48.1 40. 40.8 45.2 42.9 44.3 46.8 24.6 36.9 14.7 30.6 37.4 17.4 26.6 9.8 29.7 30.5 19.8 23.6 20.0 26.4 28.1 31.8 13.7 29.9 12.8 31.2 15.2 26.5 10.9 28.0 12.8 42.5 14.1 33.9 15.8 55.4 11.8 34.9 18.1 33.7 18. 59.8 10.1 45.5 13.5 62.7 8.6 42.9 16.0 37.7 14.7 51.2 12.6 46.9 13.7 54.1 11.1 42.9 11.2 40.0 14.3 33.9 38.8 35.2 38.9 40.4 17.5 29.0 8.7 21.5 28.4 12.4 19.8 5.5 21.7 23.2 13.5 16.8 14.5 20.8 21. 37.8 32.8 34.2 38.0 31.8 59.7 42.4 73.3 54.3 41.1 58.9 51.6 80.8 47.2 51.2 62.6 57.5 58.5 54.8 54.2 Table 2. The Performance of MLLMs on GAOKAO-MM. The top scores for each model are highlighted in bold. Model Method Overall Mathematics Chinese Physics Chemistry Biology History Geography Politics GPT-4o Qwen2-VL-7B Zero-shot Self-Consistency AR-MCTS Zero-shot Self-Consistency AR-MCTS 45.6 47.8 52.2 30.2 33.0 37.4 50.0 50.0 62.5 25.0 50.0 37.5 33.0 33.0 33. 33.3 33.0 33.3 9.6 13.5 21.2 21.2 15.4 19.2 35.7 42.9 42.9 42.9 50.0 35.7 50.0 50.0 50. 50.0 25.0 50.0 60.0 60.0 80.0 40.0 20.0 40.0 73.1 73.1 73.1 26.9 38.5 46.2 100.0 100.0 100. 40.0 40.0 80.0 suggests that smaller MLLMs have the potential for correct reasoning but may not successfully decode answers relying solely on internal knowledge. This observation further verifies the importance of integrating active retrieval in multimodal reasoning. It also demonstrates that AR-MCTS is reliable and plug-and-play framework, offering promising solution for reasoning alignment in weaker MLLMs. 5.3. General Reasoning Domain Verification To validate the effectiveness of AR-MCTS in the general multimodal reasoning field, we further evaluate the Chinese human-level multimodal reasoning benchmark, GAOKAOMM. As shown in Table 2, both the closed-source model GPT-4o and the open-source small model Qwen2-VL-7B demonstrate significant improvements over the backbone and self-consistency approaches when combined with the AR-MCTS framework, verifying the generalization of ARMCTS across different languages and reasoning disciplines. Notably, AR-MCTS with GPT-4o achieves stable improvements in mathematics and physics (12.5% and 7.7% ), while also showing some gains in the humanities (e.g. history 20%). This emphasizes that AR-MCTS with PRM not only improves the complex reasoning capabilities of MLLMs, but also effectively mitigates the knowledge gaps of MLLMs in the humanities through its active retrieval mechanism. 5.4. Quantitative Analysis Ablation Study. To explore the effects of various components in AR-MCTS, we conduct an ablation study in Table 3. The term \"w/o\" indicates versions where specific components are removed. Our key observations are: 1) Removing any component from AR-MCTS results in performance decline, highlighting the necessity of all component designs. 2) Removing the PRM and active retrieval mechanism leads to Table 3. Ablation study with Qwen2-7B. \"Filtering\" denotes the knowledge concept filtering module. Models MATHVISTA (ALL) WE-MATH (S3) GAOKAO -MM(ALL) AR-MCTS w/o PRM w/o Filtering w/o Active Retrieval 64.1 61.0 (-3.1) 62.8 (-1.3) 61.9 (-2.2) 40.6 37.7 (-2.9) 39.5 (-1.1) 38.7 (-1.9) 37.4 33.2 (-4.2) 34.5 (-2.9) 33.4 (-4.0) Figure 4. Scaling analysis on inference samplings. Random Choice denotes the average result of randomly sampling from 1 to 32. significant performance drop respectively (MATHVISTA: 3.1% & 2.2%), demonstrating that step-wise verification and active retrieved knowledge can effectively improve multimodal reasoning capabilities. 3) Notably, knowledge concept filtering also achieves stable performance gains, indicating that it effectively reduces noise in retrieved knowledge and highlights the critical importance of consistency between the retrieved knowledge and the problem during reasoning. Detailed ablations can be found in the supplementary materials. Scaling Analysis on Inference Samplings. In this section, we conduct scaling analysis to evaluate the performance of different strategies across two benchmarks with varying numbers of candidate solution paths, ranging from 1 to 32. As shown in Figure 4, our main observations are as follows: 1) When the number of candidate solutions exceeds certain threshold (16), self-consistency (SC) exhibits some performance fluctuations in WE-MATH. 2) AC-MCTS consistently outperforms ORM and SC, with this superiority becoming more pronounced as increases. We attribute this advantage to our automated process labeling, which offers high scalability and low annotation costs while providing more reliable feedback for the verification of each path. 5.5. Does AR-MCTS Improve the Sampling Space? In this section, we explore that AR-MCTS can efficiently improve the quality of the candidate solution sampling from the following two perspectives: Accuracy Analysis. To validate that AR-MCTS can efficiently improve the solution sampling accuracy in multiFigure 5. The visualization of the cadidate reasoning paths. Figure 6. The accuracy comparison of solution sampling. NQ , where modal reasoning, we perform quantitative analysis of the \"Correctness of questions\" during the sampling process of Qwen2-VL in the MATHVISTA and WE-MATH. The accuracy can be formulated as = denote the number of questions contain at least one correct candidate solution, while NQ denotes all number of questions. As shown in Figure 6, AR-MCTS demonstrates consistent gains in both benchmarks compared to the traditional beam search sampling. Moreover, as the number of candidate solutions increases, the answer accuracy exhibits positive correlation. This finding further confirms that AR-MCTS can efficiently improve the reliability of the sampling space in multimodal reasoning, effectively addressing the inherent challenges of MCTS-based methods. Diversity Analysis. To investigate whether AR-MCTS can truly enhance the diversity of sampled solutions, we sample 250 problems from MATHVISTA and employ AR-MCTS to sample 4 candidate solutions for each problem, yielding total of 1,000 samples. We employ BGE-M3 [11] as our semantic embedding model and apply PCA for dimensionality reduction, followed by DBSCAN [26] clustering for the visualization of all solution semantic representations. Figure 5 shows the visualization between the beam search (left) and AR-MCTS (right). The representations of candidate solutions sampled by beam search tend to collapse into small area with several noise points (in gray), reflecting that the beam search may lead to redundancy in sampling. Under the same parameter settings, AC-MCTS clusters more centroids for the same problem set (38 vs.46) and exhibits more dispersed representation distribution. This effectively demonstrates that AR-MCTS alleviates the issue of limited diversity in candidate solutions sampled, efficiently covering the problem-solving space and providing strong prior conditions for the simulation process of MCTS. 6. Conclusion In this paper, we propose AR-MCTS, universal framework dedicated to progressively improving the complex multimodal reasoning capabilities of MLLMs through active retrieval and Monte Carlo Tree Search. AR-MCTS leverages the MCTS algorithm alongside an active retrieval strategy, which automatically acquires high-quality step-wise reasoning annotations to progressively align process reward model, ultimately enabling process-level multimodal reasoning verification. Experimental results demonstrate the effectiveness of AR-MCTS across various MLLMs and benchmarks, confirming its ability to optimize sampling diversity and verification accuracy, and providing promising solution for reliable reasoning."
        },
        {
            "title": "References",
            "content": "[1] Jean-Baptiste Alayrac, Jeff Donahue, Pauline Luc, Antoine Miech, Iain Barr, Yana Hasson, Karel Lenc, Arthur Mensch, Katherine Millican, Malcolm Reynolds, Roman Ring, Eliza Rutherford, Serkan Cabi, Tengda Han, Zhitao Gong, Sina Samangooei, Marianne Monteiro, Jacob L. Menick, Sebastian Borgeaud, Andy Brock, Aida Nematzadeh, Sahand Sharifzadeh, Mikolaj Binkowski, Ricardo Barreira, Oriol Vinyals, Andrew Zisserman, and Karén Simonyan. Flamingo: visual language model for few-shot learning. In Advances in Neural Information Processing Systems 35: Annual Conference on Neural Information Processing Systems 2022, NeurIPS 2022, New Orleans, LA, USA, November 28 - December 9, 2022, 2022. 1 [2] Thomas Anthony, Zheng Tian, and David Barber. Thinking fast and slow with deep learning and tree search. In Advances in Neural Information Processing Systems 30: Annual Conference on Neural Information Processing Systems 2017, December 4-9, 2017, Long Beach, CA, USA, pages 53605370, 2017. 1 [3] Stanislaw Antol, Aishwarya Agrawal, Jiasen Lu, Margaret Mitchell, Dhruv Batra, C. Lawrence Zitnick, and Devi Parikh. VQA: visual question answering. In 2015 IEEE International Conference on Computer Vision, ICCV 2015, Santiago, Chile, December 7-13, 2015, pages 24252433. IEEE Computer Society, 2015. 1 [4] Akari Asai, Zeqiu Wu, Yizhong Wang, Avirup Sil, and Hannaneh Hajishirzi. Self-rag: Learning to retrieve, generate, and critique through self-reflection. In The Twelfth International Conference on Learning Representations, ICLR 2024, Vienna, Austria, May 7-11, 2024. OpenReview.net, 2024. 3 [5] Giusepppe Attardi. Wikiextractor. https://github. com/attardi/wikiextractor, 2015. 5 [6] Jinze Bai, Shuai Bai, Shusheng Yang, Shijie Wang, Sinan Tan, Peng Wang, Junyang Lin, Chang Zhou, and Jingren Zhou. Qwen-vl: versatile vision-language model for understanding, localization, text reading, and beyond, 2023. 1, [7] Cameron Browne, Edward Jack Powley, Daniel Whitehouse, Simon M. Lucas, Peter I. Cowling, Philipp Rohlfshagen, Stephen Tavener, Diego Perez Liebana, Spyridon Samothrakis, and Simon Colton. survey of monte carlo tree search methods. IEEE Trans. Comput. Intell. AI Games, 4(1):143, 2012. 1 [8] Zheng Cai, Maosong Cao, Haojiong Chen, Kai Chen, Keyu Chen, Xin Chen, Xun Chen, Zehui Chen, Zhi Chen, Pei Chu, Xiaoyi Dong, Haodong Duan, Qi Fan, Zhaoye Fei, Yang Gao, Jiaye Ge, Chenya Gu, Yuzhe Gu, Tao Gui, Aijia Guo, Qipeng Guo, Conghui He, Yingfan Hu, Ting Huang, Tao Jiang, Penglong Jiao, Zhenjiang Jin, Zhikai Lei, Jiaxing Li, Jingwen Li, Linyang Li, Shuaibin Li, Wei Li, Yining Li, Hongwei Liu, Jiangning Liu, Jiawei Hong, Kaiwen Liu, Kuikun Liu, Xiaoran Liu, Chengqi Lv, Haijun Lv, Kai Lv, Li Ma, Runyuan Ma, Zerun Ma, Wenchang Ning, Linke Ouyang, Jiantao Qiu, Yuan Qu, Fukai Shang, Yunfan Shao, Demin Song, Zifan Song, Zhihao Sui, Peng Sun, Yu Sun, Huanze Tang, Bin Wang, Guoteng Wang, Jiaqi Wang, Jiayu Wang, Rui Wang, Yudong Wang, Ziyi Wang, Xingjian Wei, Qizhen Weng, Fan Wu, Yingtong Xiong, and et al. Internlm2 technical report. CoRR, abs/2403.17297, 2024. 4 [9] Guoxin Chen, Minpeng Liao, Chengxi Li, and Kai Fan. Alphamath almost zero: process supervision without process. CoRR, abs/2405.03553, 2024. 1, 5 [10] Guoxin Chen, Minpeng Liao, Chengxi Li, and Kai Fan. Step-level value preference optimization for mathematical reasoning. CoRR, abs/2406.10858, 2024. 5 [11] Jianlv Chen, Shitao Xiao, Peitian Zhang, Kun Luo, Defu Lian, and Zheng Liu. BGE m3-embedding: Multilingual, multi-functionality, multi-granularity text emCoRR, beddings through self-knowledge distillation. abs/2402.03216, 2024. 8 [12] Wenhu Chen, Xueguang Ma, Xinyi Wang, and William W. Cohen. Program of thoughts prompting: Disentangling computation from reasoning for numerical reasoning tasks. Trans. Mach. Learn. Res., 2023, 2023. [13] Zhe Chen, Jiannan Wu, Wenhai Wang, Weijie Su, Guo Chen, Sen Xing, Muyan Zhong, Qinglong Zhang, Xizhou Zhu, Lewei Lu, Bin Li, Ping Luo, Tong Lu, Yu Qiao, and Jifeng Dai. Internvl: Scaling up vision foundation models and aligning for generic visual-linguistic tasks. arXiv preprint arXiv:2312.14238, 2023. 2, 5 [14] Zhe Chen, Weiyun Wang, Hao Tian, Shenglong Ye, Zhangwei Gao, Erfei Cui, Wenwen Tong, Kongzhi Hu, Jiapeng Luo, Zheng Ma, Ji Ma, Jiaqi Wang, Xiaoyi Dong, Hang Yan, Hewei Guo, Conghui He, Botian Shi, Zhenjiang Jin, Chao Xu, Bin Wang, Xingjian Wei, Wei Li, Wenjian Zhang, Bo Zhang, Pinlong Cai, Licheng Wen, Xiangchao Yan, Min Dou, Lewei Lu, Xizhou Zhu, Tong Lu, Dahua Lin, Yu Qiao, Jifeng Dai, and Wenhai Wang. How far are we to gpt-4v? closing the gap to commercial multimodal models with opensource suites. CoRR, abs/2404.16821, 2024. 1, 2 [15] Zhe Chen, Weiyun Wang, Hao Tian, Shenglong Ye, Zhangwei Gao, Erfei Cui, Wenwen Tong, Kongzhi Hu, Jiapeng Luo, Zheng Ma, Ji Ma, Jiaqi Wang, Xiaoyi Dong, Hang Yan, Hewei Guo, Conghui He, Botian Shi, Zhenjiang Jin, Chao Xu, Bin Wang, Xingjian Wei, Wei Li, Wenjian Zhang, Bo Zhang, Pinlong Cai, Licheng Wen, Xiangchao Yan, Min Dou, Lewei Lu, Xizhou Zhu, Tong Lu, Dahua Lin, Yu Qiao, Jifeng Dai, and Wenhai Wang. How far are we to gpt-4v? closing the gap to commercial multimodal models with opensource suites. CoRR, abs/2404.16821, 2024. 6, 4 [16] Yiruo Cheng, Kelong Mao, Ziliang Zhao, Guanting Dong, Hongjin Qian, Yongkang Wu, Tetsuya Sakai, Ji-Rong Wen, and Zhicheng Dou. CORAL: benchmarking multi-turn conversational retrieval-augmentation generation. CoRR, abs/2410.23090, 2024. 3 [17] Karl Cobbe, Vineet Kosaraju, Mohammad Bavarian, Mark Chen, Heewoo Jun, Lukasz Kaiser, Matthias Plappert, Jerry Tworek, Jacob Hilton, Reiichiro Nakano, Christopher Hesse, and John Schulman. Training verifiers to solve math word problems. CoRR, abs/2110.14168, 2021. 1, 2, 6, 3 [18] Karl Cobbe, Vineet Kosaraju, Mohammad Bavarian, Mark Chen, Heewoo Jun, Lukasz Kaiser, Matthias Plappert, Jerry Tworek, Jacob Hilton, Reiichiro Nakano, Christopher Hesse, and John Schulman. Training verifiers to solve math word problems. CoRR, abs/2110.14168, 2021. [19] Tri Dao. Flashattention-2: Faster attention with better parallelism and work partitioning. CoRR, 2023. 4 [20] Guanting Dong, Rumei Li, Sirui Wang, Yupeng Zhang, Yunsen Xian, and Weiran Xu. Bridging the kb-text gap: Leveraging structured knowledge-aware pre-training for KBQA. In Proceedings of the 32nd ACM International Conference on Information and Knowledge Management, CIKM 2023, Birmingham, United Kingdom, October 21-25, 2023, pages 38543859. ACM, 2023. 3 [21] Guanting Dong, Keming Lu, Chengpeng Li, Tingyu Xia, Bowen Yu, Chang Zhou, and Jingren Zhou. Self-play with execution feedback: Improving instruction-following capabilities of large language models. CoRR, abs/2406.13542, 2024. 6 [22] Guanting Dong, Xiaoshuai Song, Yutao Zhu, Runqi Qiao, Zhicheng Dou, and Ji-Rong Wen. Toward general instruction-following alignment for retrieval-augmented generation. CoRR, abs/2410.09584, 2024. 3 [23] Guanting Dong, Hongyi Yuan, Keming Lu, Chengpeng Li, Mingfeng Xue, Dayiheng Liu, Wei Wang, Zheng Yuan, Chang Zhou, and Jingren Zhou. How abilities in large language models are affected by supervised fine-tuning data composition. In Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), ACL 2024, Bangkok, Thailand, August 11-16, 2024, pages 177198. Association for Computational Linguistics, 2024. [24] Guanting Dong, Yutao Zhu, Chenghao Zhang, Zechen Wang, Zhicheng Dou, and Ji-Rong Wen. Understand what LLM needs: Dual preference alignment for retrieval-augmented generation. CoRR, abs/2406.18676, 2024. 3 [25] Abhimanyu Dubey, Abhinav Jauhri, Abhinav Pandey, Abhishek Kadian, Ahmad Al-Dahle, Aiesha Letman, Akhil Mathur, Alan Schelten, Amy Yang, Angela Fan, Anirudh Goyal, Anthony Hartshorn, Aobo Yang, Archi Mitra, Archie Sravankumar, Artem Korenev, Arthur Hinsvark, Arun Rao, Aston Zhang, Aurélien Rodriguez, Austen Gregerson, Ava Spataru, Baptiste Rozière, Bethany Biron, Binh Tang, Bobbie Chern, Charlotte Caucheteux, Chaya Nayak, Chloe Bi, Chris Marra, Chris McConnell, Christian Keller, Christophe Touret, Chunyang Wu, Corinne Wong, Cristian Canton Ferrer, Cyrus Nikolaidis, Damien Allonsius, Daniel Song, Danielle Pintz, Danny Livshits, David Esiobu, Dhruv Choudhary, Dhruv Mahajan, Diego Garcia-Olano, Diego Perino, Dieuwke Hupkes, Egor Lakomkin, Ehab AlBadawy, Elina Lobanova, Emily Dinan, Eric Michael Smith, Filip Radenovic, Frank Zhang, Gabriel Synnaeve, Gabrielle Lee, Georgia Lewis Anderson, Graeme Nail, Grégoire Mialon, Guan Pang, Guillem Cucurell, Hailey Nguyen, Hannah Korevaar, Hu Xu, Hugo Touvron, Iliyan Zarov, Imanol Arrieta Ibarra, Isabel M. Kloumann, Ishan Misra, Ivan Evtimov, Jade Copet, Jaewon Lee, Jan Geffert, Jana Vranes, Jason Park, Jay Mahadeokar, Jeet Shah, Jelmer van der Linde, Jennifer Billock, Jenny Hong, Jenya Lee, Jeremy Fu, Jianfeng Chi, Jianyu Huang, Jiawen Liu, Jie Wang, Jiecao Yu, Joanna Bitton, Joe Spisak, Jongsoo Park, Joseph Rocca, Joshua Johnstun, Joshua Saxe, Junteng Jia, Kalyan Vasuden Alwala, Kartikeya Upasani, Kate Plawiak, Ke Li, Kenneth Heafield, Kevin Stone, and et al. The llama 3 herd of models. CoRR, abs/2407.21783, 2024. 1, 2, 4 [26] Martin Ester, Hans-Peter Kriegel, Jörg Sander, and Xiaowei Xu. density-based algorithm for discovering clusters in large spatial databases with noise. In Proceedings of the Second International Conference on Knowledge Discovery and Data Mining (KDD-96), Portland, Oregon, USA, pages 226231. AAAI Press, 1996. 8 [27] Markus Freitag and Yaser Al-Onaizan. Beam search strategies for neural machine translation. In Proceedings of the First Workshop on Neural Machine Translation, NMT@ACL 2017, Vancouver, Canada, August 4, 2017, pages 5660. Association for Computational Linguistics, 2017. 2 [28] Luyu Gao, Aman Madaan, Shuyan Zhou, Uri Alon, Pengfei Liu, Yiming Yang, Jamie Callan, and Graham Neubig. PAL: program-aided language models. In International Conference on Machine Learning, ICML 2023, 23-29 July 2023, Honolulu, Hawaii, USA, pages 1076410799. PMLR, 2023. 2 [29] Zitian Gao, Boye Niu, Xuzheng He, Haotian Xu, Hongzhang Liu, Aiwei Liu, Xuming Hu, and Lijie Wen. Interpretable contrastive monte carlo tree search reasoning. 2024. 1, [30] Zhibin Gou, Zhihong Shao, Yeyun Gong, Yelong Shen, Yujiu Yang, Minlie Huang, Nan Duan, and Weizhu Chen. Tora: tool-integrated reasoning agent for mathematical problem solving. In The Twelfth International Conference on Learning Representations, ICLR 2024, Vienna, Austria, May 7-11, 2024. OpenReview.net, 2024. 2 [31] Daya Guo, Qihao Zhu, Dejian Yang, Zhenda Xie, Kai Dong, Wentao Zhang, Guanting Chen, Xiao Bi, Yu Wu, YK Li, et al. Deepseek-coder: When the large language model meets programmingthe rise of code intelligence. arXiv preprint arXiv:2401.14196, 2024. 2 [32] Jiayi He, Hehai Lin, Qingyun Wang, Yi Fung, and Heng Ji. Self-correction is more than refinement: learning framework for visual and language reasoning tasks, 2024. 6 [33] Dan Hendrycks, Collin Burns, Saurav Kadavath, Akul Arora, Steven Basart, Eric Tang, Dawn Song, and Jacob Steinhardt. Measuring mathematical problem solving with the MATH dataset. In Proceedings of the Neural Information Processing Systems Track on Datasets and Benchmarks 1, NeurIPS Datasets and Benchmarks 2021, December 2021, virtual, 2021. 3 [34] Gautier Izacard, Mathilde Caron, Lucas Hosseini, Sebastian Riedel, Piotr Bojanowski, Armand Joulin, and Edouard Grave. Unsupervised dense information retrieval with contrastive learning. Trans. Mach. Learn. Res., 2022, 2022. 4 [35] Jiajie Jin, Yutao Zhu, Xinyu Yang, Chenghao Zhang, and Zhicheng Dou. Flashrag: modular toolkit for efficient retrieval-augmented generation research. CoRR, abs/2405.13576, 2024. 3 [36] Jeff Johnson, Matthijs Douze, and Hervé Jégou. Billionscale similarity search with gpus. IEEE Trans. Big Data, 7 (3):535547, 2021. [37] Andrej Karpathy and Li Fei-Fei. Deep visual-semantic alignments for generating image descriptions. IEEE Trans. Pattern Anal. Mach. Intell., 39(4):664676, 2017. 1 [38] Andreas Koukounas, Georgios Mastrapas, Michael Günther, Bo Wang, Scott Martens, Isabelle Mohr, Saba Sturua, Mohammad Kalim Akram, Joan Fontanals Martínez, Saahil Ognawala, Susana Guzman, Maximilian Werk, Nan Wang, and Han Xiao. Jina CLIP: your CLIP model is also your text retriever. CoRR, abs/2405.20204, 2024. 6 [39] Woosuk Kwon, Zhuohan Li, Siyuan Zhuang, Ying Sheng, Lianmin Zheng, Cody Hao Yu, Joseph E. Gonzalez, Hao Zhang, and Ion Stoica. Efficient memory management for large language model serving with pagedattention. In Proceedings of the ACM SIGOPS 29th Symposium on Operating Systems Principles, 2023. 7 [40] Shanglin Lei, Guanting Dong, Xiaoping Wang, Keheng Wang, and Sirui Wang. Instructerc: Reforming emotion recognition in conversation with retrieval multi-task llms framework. CoRR, abs/2309.11911, 2023. 3 [41] Patrick S. H. Lewis, Ethan Perez, Aleksandra Piktus, Fabio Petroni, Vladimir Karpukhin, Naman Goyal, Heinrich Küttler, Mike Lewis, Wen-tau Yih, Tim Rocktäschel, Sebastian Riedel, and Douwe Kiela. Retrieval-augmented generation for knowledge-intensive NLP tasks. In Advances in Neural Information Processing Systems 33: Annual Conference on Neural Information Processing Systems 2020, NeurIPS 2020, December 6-12, 2020, virtual, 2020. 4 [42] Bo Li, Yuanhan Zhang, Dong Guo, Renrui Zhang, Feng Li, Hao Zhang, Kaichen Zhang, Yanwei Li, Ziwei Liu, and Chunyuan Li. Llava-onevision: Easy visual task transfer. CoRR, abs/2408.03326, 2024. [43] Bo Li, Yuanhan Zhang, Dong Guo, Renrui Zhang, Feng Li, Hao Zhang, Kaichen Zhang, Yanwei Li, Ziwei Liu, and Chunyuan Li. Llava-onevision: Easy visual task transfer. CoRR, abs/2408.03326, 2024. 6, 4, 5 [44] Chengpeng Li, Guanting Dong, Mingfeng Xue, Ru Peng, Xiang Wang, and Dayiheng Liu. Dotamath: Decomposition of thought with code assistance and self-correction for mathematical reasoning. CoRR, abs/2407.04078, 2024. 2 [45] Chunyuan Li, Cliff Wong, Sheng Zhang, Naoto Usuyama, Haotian Liu, Jianwei Yang, Tristan Naumann, Hoifung Poon, and Jianfeng Gao. Llava-med: Training large languageand-vision assistant for biomedicine in one day. Advances in Neural Information Processing Systems, 36, 2024. 2 [46] Chengpeng Li, Zheng Yuan, Hongyi Yuan, Guanting Dong, Keming Lu, Jiancan Wu, Chuanqi Tan, Xiang Wang, and Chang Zhou. Mugglemath: Assessing the impact of query and response augmentation on math reasoning. In Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), ACL 2024, Bangkok, Thailand, August 11-16, 2024, pages 10230 10258. Association for Computational Linguistics, 2024. 2 [47] Xiaoxi Li, Jiajie Jin, Yujia Zhou, Yuyao Zhang, Peitian Zhang, Yutao Zhu, and Zhicheng Dou. From matching to generation: survey on generative information retrieval. CoRR, abs/2404.14851, 2024. 3 [48] Yifei Li, Zeqi Lin, Shizhuo Zhang, Qiang Fu, Bei Chen, Jian-Guang Lou, and Weizhu Chen. Making language models better reasoners with step-aware verifier. In Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), ACL 2023, Toronto, Canada, July 9-14, 2023, pages 53155333. Association for Computational Linguistics, 2023. 1, 2 [49] Hunter Lightman, Vineet Kosaraju, Yuri Burda, Harrison Edwards, Bowen Baker, Teddy Lee, Jan Leike, John Schulman, Ilya Sutskever, and Karl Cobbe. Lets verify step by step. In The Twelfth International Conference on Learning Representations, ICLR 2024, Vienna, Austria, May 7-11, 2024. OpenReview.net, 2024. 1, 5 [50] Su Hyeon Lim, Minkuk Kim, Hyeon Bae Kim, and Seong Tae Kim. Retrieval-augmented natural language reasoning for explainable visual question answering. CoRR, abs/2408.17006, 2024. [51] Zhan Ling, Yunhao Fang, Xuanlin Li, Zhiao Huang, Mingu Lee, Roland Memisevic, and Hao Su. Deductive verification of chain-of-thought reasoning. In Advances in Neural Information Processing Systems 36: Annual Conference on Neural Information Processing Systems 2023, NeurIPS 2023, New Orleans, LA, USA, December 10 - 16, 2023, 2023. 2 [52] Bingshuai Liu, Chenyang Lyu, Zijun Min, Zhanyu Wang, Jinsong Su, and Longyue Wang. Retrieval-augmented multimodal chain-of-thoughts reasoning for large language models. CoRR, abs/2312.01714, 2023. 2, 3, 5 [53] Haotian Liu, Chunyuan Li, Yuheng Li, Bo Li, Yuanhan Zhang, Sheng Shen, and Yong Jae Lee. Llava-next: Improved reasoning, ocr, and world knowledge. 6, 4 [54] Haotian Liu, Chunyuan Li, Qingyang Wu, and Yong Jae In Advances in Neural Lee. Visual instruction tuning. Information Processing Systems 36: Annual Conference on Neural Information Processing Systems 2023, NeurIPS 2023, New Orleans, LA, USA, December 10 - 16, 2023, 2023. 1, 2 [55] Jiacheng Liu, Andrew Cohen, Ramakanth Pasunuru, Yejin Choi, Hannaneh Hajishirzi, and Asli Celikyilmaz. Making PPO even better: Value-guided monte-carlo tree search decoding. CoRR, abs/2309.15028, 2023. 1 [56] Jingyu Liu, Jiaen Lin, and Yong Liu. How much can RAG help the reasoning of llm? CoRR, abs/2410.02338, 2024. 2, [57] Keming Lu, Hongyi Yuan, Zheng Yuan, Runji Lin, Junyang Lin, Chuanqi Tan, Chang Zhou, and Jingren Zhou. #instag: Instruction tagging for analyzing supervised fine-tuning of large language models. In The Twelfth International Conference on Learning Representations, ICLR 2024, Vienna, Austria, May 7-11, 2024. OpenReview.net, 2024. 5 [58] Pan Lu, Hritik Bansal, Tony Xia, Jiacheng Liu, Chunyuan Li, Hannaneh Hajishirzi, Hao Cheng, Kai-Wei Chang, Michel Galley, and Jianfeng Gao. Mathvista: Evaluating mathematical reasoning of foundation models in visual contexts. In The Twelfth International Conference on Learning Representations, ICLR 2024, Vienna, Austria, May 7-11, 2024. OpenReview.net, 2024. 1, 3, 6, 2 [59] Haipeng Luo, Qingfeng Sun, Can Xu, Pu Zhao, Jianguang Lou, Chongyang Tao, Xiubo Geng, Qingwei Lin, Shifeng Chen, and Dongmei Zhang. Wizardmath: Empowering mathematical reasoning for large language models via reinforced evol-instruct. CoRR, abs/2308.09583, 2023. 1, 2 [60] Haoran Luo, Haihong E, Zichen Tang, Shiyao Peng, Yikai Guo, Wentai Zhang, Chenghao Ma, Guanting Dong, Meina Song, Wei Lin, Yifan Zhu, and Anh Tuan Luu. Chatkbqa: generate-then-retrieve framework for knowledge base question answering with fine-tuned large language models. In Findings of the Association for Computational Linguistics, ACL 2024, Bangkok, Thailand and virtual meeting, August 11-16, 2024, pages 20392056. Association for Computational Linguistics, 2024. 3 [61] Liangchen Luo, Yinxiao Liu, Rosanne Liu, Samrat Phatale, Harsh Lara, Yunxuan Li, Lei Shu, Yun Zhu, Lei Meng, Jiao Sun, and Abhinav Rastogi. Improve mathematical reasoning in language models by automated process supervision. CoRR, abs/2406.06592, 2024. 1 [62] Liangchen Luo, Yinxiao Liu, Rosanne Liu, Samrat Phatale, Harsh Lara, Yunxuan Li, Lei Shu, Yun Zhu, Lei Meng, Jiao Sun, and Abhinav Rastogi. Improve mathematical reasoning in language models by automated process supervision. CoRR, abs/2406.06592, 2024. [63] Liangchen Luo, Yinxiao Liu, Rosanne Liu, Samrat Phatale, Harsh Lara, Yunxuan Li, Lei Shu, Yun Zhu, Lei Meng, Jiao Sun, and Abhinav Rastogi. Improve mathematical reasoning in language models by automated process supervision. CoRR, abs/2406.06592, 2024. 7 [64] Qianli Ma, Haotian Zhou, Tingkai Liu, Jianbo Yuan, Pengfei Liu, Yang You, and Hongxia Yang. Lets reward step by step: Step-level reward model as the navigators for reasoning. CoRR, abs/2310.10080, 2023. 1, 2 [65] Aman Madaan, Niket Tandon, Prakhar Gupta, Skyler Hallinan, Luyu Gao, Sarah Wiegreffe, Uri Alon, Nouha Dziri, Shrimai Prabhumoye, Yiming Yang, Shashank Gupta, Bodhisattwa Prasad Majumder, Katherine Hermann, Sean Welleck, Amir Yazdanbakhsh, and Peter Clark. Self-refine: Iterative refinement with self-feedback. In Advances in Neural Information Processing Systems 36: Annual Conference on Neural Information Processing Systems 2023, NeurIPS 2023, New Orleans, LA, USA, December 10 - 16, 2023, 2023. 3 [66] Ning Miao, Yee Whye Teh, and Tom Rainforth. Selfcheck: Using llms to zero-shot check their own step-by-step reasoning. In The Twelfth International Conference on Learning Representations, ICLR 2024, Vienna, Austria, May 7-11, 2024. OpenReview.net, 2024. 2 [67] OpenAI. GPT-4 technical report. CoRR, abs/2303.08774, 2023. 1, 2 [68] OpenAI. GPT-4 technical report. CoRR, abs/2303.08774, 2023. 4 [69] OpenAI. Hello gpt-4o, 2024. 4 [70] OpenAI, :, Aaron Hurst, Adam Lerer, Adam P. Goucher, Adam Perelman, Aditya Ramesh, Aidan Clark, AJ Ostrow, Akila Welihinda, Alan Hayes, Alec Radford, Aleksander adry, Alex Baker-Whitcomb, Alex Beutel, Alex Borzunov, Alex Carney, Alex Chow, Alex Kirillov, Alex Nichol, Alex Paino, Alex Renzin, Alex Tachard Passos, Alexander Kirillov, Alexi Christakis, Alexis Conneau, Ali Kamali, Allan Jabri, Allison Moyer, Allison Tam, Amadou Crookes, Amin Tootoochian, Amin Tootoonchian, Ananya Kumar, Andrea Vallone, Andrej Karpathy, Andrew Braunstein, Andrew Cann, Andrew Codispoti, Andrew Galu, Andrew Kondrich, Andrew Tulloch, Andrey Mishchenko, Angela Baek, Angela Jiang, Antoine Pelisse, Antonia Woodford, Anuj Gosalia, Arka Dhar, Ashley Pantuliano, Avi Nayak, Avital Oliver, Barret Zoph, Behrooz Ghorbani, Ben Leimberger, Ben Rossen, Ben Sokolowsky, Ben Wang, Benjamin Zweig, Beth Hoover, Blake Samic, Bob McGrew, Bobby Spero, Bogo Giertler, Bowen Cheng, Brad Lightcap, Brandon Walkin, Brendan Quinn, Brian Guarraci, Brian Hsu, Bright Kellogg, Brydon Eastman, Camillo Lugaresi, Carroll Wainwright, Cary Bassin, Cary Hudson, Casey Chu, Chad Nelson, Chak Li, Chan Jun Shern, Channing Conger, Charlotte Barette, Chelsea Voss, Chen Ding, Cheng Lu, Chong Zhang, Chris Beaumont, Chris Hallacy, Chris Koch, Christian Gibson, Christina Kim, Christine Choi, Christine McLeavey, Christopher Hesse, Claudia Fischer, Clemens Winter, Coley Czarnecki, Colin Jarvis, Colin Wei, Constantin Koumouzelis, Dane Sherburn, Daniel Kappler, Daniel Levin, Daniel Levy, David Carr, David Farhi, David Mely, David Robinson, David Sasaki, Denny Jin, Dev Valladares, Dimitris Tsipras, Doug Li, Duc Phong Nguyen, Duncan Findlay, Edede Oiwoh, Edmund Wong, Ehsan Asdar, Elizabeth Proehl, Elizabeth Yang, Eric Antonow, Eric Kramer, Eric Peterson, Eric Sigler, Eric Wallace, Eugene Brevdo, Evan Mays, Farzad Khorasani, Felipe Petroski Such, Filippo Raso, Francis Zhang, Fred von Lohmann, Freddie Sulit, Gabriel Goh, Gene Oden, Geoff Salmon, Giulio Starace, Greg Brockman, Hadi Salman, Haiming Bao, Haitang Hu, Hannah Wong, Haoyu Wang, Heather Schmidt, Heather Whitney, Heewoo Jun, Hendrik Kirchner, Henrique Ponde de Oliveira Pinto, Hongyu Ren, Huiwen Chang, Hyung Won Chung, Ian Kivlichan, Ian OConnell, Ian OConnell, Ian Osband, Ian Silber, Ian Sohl, Ibrahim Okuyucu, Ikai Lan, Ilya Kostrikov, Ilya Sutskever, Ingmar Kanitscheider, Ishaan Gulrajani, Jacob Coxon, Jacob Menick, Jakub Pachocki, James Aung, James Betker, James Crooks, James Lennon, Jamie Kiros, Jan Leike, Jane Park, Jason Kwon, Jason Phang, Jason Teplitz, Jason Wei, Jason Wolfe, Jay Chen, Jeff Harris, Jenia Varavva, Jessica Gan Lee, Jessica Shieh, Ji Lin, Jiahui Yu, Jiayi Weng, Jie Tang, Jieqi Yu, Joanne Jang, Joaquin Quinonero Candela, Joe Beutler, Joe Landers, Joel Parish, Johannes Heidecke, John Schulman, Jonathan Lachman, Jonathan McKay, Jonathan Uesato, Jonathan Ward, Jong Wook Kim, Joost Huizinga, Jordan Sitkin, Jos Kraaijeveld, Josh Gross, Josh Kaplan, Josh Snyder, Joshua Achiam, Joy Jiao, Joyce Lee, Juntang Zhuang, Justyn Harriman, Kai Fricke, Kai Hayashi, Karan Singhal, Katy Shi, Kavin Karthik, Kayla Wood, Kendra Rimbach, Kenny Hsu, Kenny Nguyen, Keren Gu-Lemberg, Kevin Button, Kevin Liu, Kiel Howe, Krithika Muthukumar, Kyle Luther, Lama Ahmad, Larry Kai, Lauren Itow, Lauren Workman, Leher Pathak, Leo Chen, Li Jing, Lia Guy, Liam Fedus, Liang Zhou, Lien Mamitsuka, Lilian Weng, Lindsay McCallum, Lindsey Held, Long Ouyang, Louis Feuvrier, Lu Zhang, Lukas Kondraciuk, Lukasz Kaiser, Luke Hewitt, Luke Metz, Lyric Doshi, Mada Aflak, Maddie Simens, Madelaine Boyd, Madeleine Thompson, Marat Dukhan, Mark Chen, Mark Gray, Mark Hudnall, Marvin Zhang, Marwan Aljubeh, Mateusz Litwin, Matthew Zeng, Max Johnson, Maya Shetty, Mayank Gupta, Meghan Shah, Mehmet Yatbaz, Meng Jia Yang, Mengchao Zhong, Mia Glaese, Mianna Chen, Michael Janner, Michael Lampe, Michael Petrov, Michael Wu, Michele Wang, Michelle Fradin, Michelle Pokrass, Miguel Castro, Miguel Oom Temudo de Castro, Mikhail Pavlov, Miles Brundage, Miles Wang, Minal Khan, Mira Murati, Mo Bavarian, Molly Lin, Murat Yesildal, Nacho Soto, Natalia Gimelshein, Natalie Cone, Natalie Staudacher, Natalie Summers, Natan LaFontaine, Neil Chowdhury, Nick Ryder, Nick Stathas, Nick Turley, Nik Tezak, Niko Felix, Nithanth Kudige, Nitish Keskar, Noah Deutsch, Noel Bundick, Nora Puckett, Ofir Nachum, Ola Okelola, Oleg Boiko, Oleg Murk, Oliver Jaffe, Olivia Watkins, Olivier Godement, Owen Campbell-Moore, Patrick Chao, Paul McMillan, Pavel Belov, Peng Su, Peter Bak, Peter Bakkum, Peter Deng, Peter Dolan, Peter Hoeschele, Peter Welinder, Phil Tillet, Philip Pronin, Philippe Tillet, Prafulla Dhariwal, Qiming Yuan, Rachel Dias, Rachel Lim, Rahul Arora, Rajan Troll, Randall Lin, Rapha Gontijo Lopes, Raul Puri, Reah Miyara, Reimar Leike, Renaud Gaubert, Reza Zamani, Ricky Wang, Rob Donnelly, Rob Honsby, Rocky Smith, Rohan Sahai, Rohit Ramchandani, Romain Huet, Rory Carmichael, Rowan Zellers, Roy Chen, Ruby Chen, Ruslan Nigmatullin, Ryan Cheu, Saachi Jain, Sam Altman, Sam Schoenholz, Sam Toizer, Samuel Miserendino, Sandhini Agarwal, Sara Culver, Scott Ethersmith, Scott Gray, Sean Grove, Sean Metzger, Shamez Hermani, Shantanu Jain, Shengjia Zhao, Sherwin Wu, Shino Jomoto, Shirong Wu, Shuaiqi, Xia, Sonia Phene, Spencer Papay, Srinivas Narayanan, Steve Coffey, Steve Lee, Stewart Hall, Suchir Balaji, Tal Broda, Tal Stramer, Tao Xu, Tarun Gogineni, Taya Christianson, Ted Sanders, Tejal Patwardhan, Thomas Cunninghman, Thomas Degry, Thomas Dimson, Thomas Raoux, Thomas Shadwell, Tianhao Zheng, Todd Underwood, Todor Markov, Toki Sherbakov, Tom Rubin, Tom Stasi, Tomer Kaftan, Tristan Heywood, Troy Peterson, Tyce Walters, Tyna Eloundou, Valerie Qi, Veit Moeller, Vinnie Monaco, Vishal Kuo, Vlad Fomenko, Wayne Chang, Weiyi Zheng, Wenda Zhou, Wesam Manassra, Will Sheu, Wojciech Zaremba, Yash Patil, Yilei Qian, Yongjik Kim, Youlong Cheng, Yu Zhang, Yuchen He, Yuchen Zhang, Yujia Jin, Yunxing Dai, and Yury Malkov. Gpt-4o system card, 2024. [71] OpenAI. Gpt-4v (ision) system card. Citekey: gptvision, 2023. 6 [72] Long Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida, Carroll L. Wainwright, Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, John Schulman, Jacob Hilton, Fraser Kelton, Luke Miller, Maddie Simens, Amanda Askell, Peter Welinder, Paul F. Christiano, Jan Leike, and Ryan Lowe. Training language models to follow instructions with human feedback. In Advances in Neural Information Processing Systems 35: Annual Conference on Neural Information Processing Systems 2022, NeurIPS 2022, New Orleans, LA, USA, November 28 - December 9, 2022, 2022. 1, 2 [73] Long Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida, Carroll L. Wainwright, Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, John Schulman, Jacob Hilton, Fraser Kelton, Luke Miller, Maddie Simens, Amanda Askell, Peter Welinder, Paul F. Christiano, Jan Leike, and Ryan Lowe. Training language models to follow instructions with human feedback. In Advances in Neural Information Processing Systems 35: Annual Conference on Neural Information Processing Systems 2022, NeurIPS 2022, New Orleans, LA, USA, November 28 - December 9, 2022, 2022. 1 [74] Martin L. Puterman. Chapter 8 markov decision processes. In Stochastic Models, pages 331434. Elsevier, 1990. 3 [75] Runqi Qiao, Qiuna Tan, Guanting Dong, Minhui Wu, Chong Sun, Xiaoshuai Song, Zhuoma Gongque, Shanglin Lei, Zhe Wei, Miaoxuan Zhang, Runfeng Qiao, Yifan Zhang, Xiao Zong, Yida Xu, Muxi Diao, Zhimin Bao, Chen Li, and Honggang Zhang. We-math: Does your large multimodal model achieve human-like mathematical reasoning? CoRR, abs/2407.01284, 2024. 3, 6, 1 [76] Runqi Qiao, Lan Yang, Kaiyue Pang, and Honggang Zhang. Making visual sense of oracle bones for you and me. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 1265612665, 2024. [77] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, Gretchen Krueger, and Ilya Sutskever. Learning transferable visual models from natural language supervision. In Proceedings of the 38th International Conference on Machine Learning, ICML 2021, 18-24 July 2021, Virtual Event, pages 8748 8763. PMLR, 2021. 4 [78] Rafael Rafailov, Archit Sharma, Eric Mitchell, Christopher D. Manning, Stefano Ermon, and Chelsea Finn. Direct preference optimization: Your language model is secretly reward model. In Advances in Neural Information Processing Systems 36: Annual Conference on Neural Information Processing Systems 2023, NeurIPS 2023, New Orleans, LA, USA, December 10 - 16, 2023, 2023. 2 [79] Rafael Rafailov, Archit Sharma, Eric Mitchell, Christopher D. Manning, Stefano Ermon, and Chelsea Finn. Direct preference optimization: Your language model is secretly reward model. In Advances in Neural Information Processing Systems 36: Annual Conference on Neural Information Processing Systems 2023, NeurIPS 2023, New Orleans, LA, USA, December 10 - 16, 2023, 2023. 2 [80] Marlou Rasenberg, Asli Özyürek, and Mark Dingemanse. Alignment in multimodal interaction: An integrative framework. Cogn. Sci., 44(11), 2020. 2 [81] Jeff Rasley, Samyam Rajbhandari, Olatunji Ruwase, and Yuxiong He. Deepspeed: System optimizations enable training deep learning models with over 100 billion parameters. 2020. 4 [82] Nikhil Sardana, Jacob Portes, Sasha Doubov, and Jonathan Frankle. Beyond chinchilla-optimal: Accounting for inference in language model scaling laws. In Forty-first International Conference on Machine Learning, ICML 2024, Vienna, Austria, July 21-27, 2024. OpenReview.net, 2024. 1 [83] Emre Satir and Hasan Bulut. Preventing translation quality deterioration caused by beam search decoding in neural machine translation using statistical machine translation. Inf. Sci., 581:791807, 2021. [84] John Schulman, Filip Wolski, Prafulla Dhariwal, Alec Radford, and Oleg Klimov. Proximal policy optimization algorithms. CoRR, abs/1707.06347, 2017. 3 [85] Amrith Setlur, Chirag Nagpal, Adam Fisch, Xinyang Geng, Jacob Eisenstein, Rishabh Agarwal, Alekh Agarwal, Jonathan Berant, and Aviral Kumar. Rewarding progress: Scaling automated process verifiers for llm reasoning. 2024. 1 [86] Amrith Setlur, Chirag Nagpal, Adam Fisch, Xinyang Geng, Jacob Eisenstein, Rishabh Agarwal, Alekh Agarwal, Jonathan Berant, and Aviral Kumar. Rewarding progress: Scaling automated process verifiers for llm reasoning. arXiv preprint arXiv:2410.08146, 2024. 2 [87] Zhihong Shao, Peiyi Wang, Qihao Zhu, Runxin Xu, Junxiao Song, Xiao Bi, Haowei Zhang, Mingchuan Zhang, YK Li, Wu, et al. Deepseekmath: Pushing the limits of mathematical reasoning in open language models. arXiv preprint arXiv:2402.03300, 2024. 2 [88] Zhihong Shao, Peiyi Wang, Qihao Zhu, Runxin Xu, Junxiao Song, Mingchuan Zhang, Y. K. Li, Y. Wu, and Daya Guo. Deepseekmath: Pushing the limits of mathematical reasoning in open language models. CoRR, abs/2402.03300, 2024. 1, 2 [89] Charlie Snell, Jaehoon Lee, Kelvin Xu, and Aviral Kumar. Scaling LLM test-time compute optimally can be more effective than scaling model parameters. CoRR, abs/2408.03314, 2024. [90] Shezheng Song, Xiaopeng Li, and Shasha Li. How to bridge the gap between modalities: comprehensive survey on multimodal large language model. CoRR, abs/2311.07594, 2023. 2 [91] Xiaoshuai Song, Muxi Diao, Guanting Dong, Zhengyang Wang, Yujia Fu, Runqi Qiao, Zhexu Wang, Dayuan Fu, Huangxuan Wu, Bin Liang, et al. Cs-bench: comprehensive benchmark for large language models towards computer science mastery. arXiv preprint arXiv:2406.08587, 2024. 2 [92] Niranjan Srinivas, Andreas Krause, Sham M. Kakade, and Matthias W. Seeger. Information-theoretic regret bounds for gaussian process optimization in the bandit setting. IEEE Trans. Inf. Theory, 58(5):32503265, 2012. 3 [93] Linzhuang Sun, Hao Liang, Jingxuan Wei, Bihui Yu, Conghui He, Zenan Zhou, and Wentao Zhang. Beats: Optimizing llm mathematical capabilities with backverify and adaptive disambiguate based efficient tree search. 2024. 1 [94] Zhiqing Sun, Longhui Yu, Yikang Shen, Weiyang Liu, Yiming Yang, Sean Welleck, and Chuang Gan. Easy-to-hard generalization: Scalable alignment beyond human supervision. CoRR, abs/2403.09472, 2024. 5 [95] Cheng Tan, Jingxuan Wei, Linzhuang Sun, Zhangyang Gao, Siyuan Li, Bihui Yu, Ruifeng Guo, and Stan Z. Li. Retrieval meets reasoning: Even high-school textbook knowledge benefits multimodal reasoning. CoRR, abs/2405.20834, 2024. 2, 3, 4 [96] Jiejun Tan, Zhicheng Dou, Wen Wang, Mang Wang, Weipeng Chen, and Ji-Rong Wen. Htmlrag: Html is better than plain text for modeling retrieved knowledge in rag systems, 2024. [97] Qwen Team. Qwen2.5: party of foundation models, 2024. 2 [98] Ye Tian, Baolin Peng, Linfeng Song, Lifeng Jin, Dian Yu, Haitao Mi, and Dong Yu. Toward self-improvement of llms via imagination, searching, and criticizing. CoRR, abs/2404.12253, 2024. 1 [99] Ye Tian, Baolin Peng, Linfeng Song, Lifeng Jin, Dian Yu, Haitao Mi, and Dong Yu. Toward self-improvement of llms via imagination, searching, and criticizing. CoRR, abs/2404.12253, 2024. 3, 6 [100] Jonathan Uesato, Nate Kushman, Ramana Kumar, Francis Song, Noah Siegel, Lisa Wang, Antonia Creswell, Geoffrey Irving, and Irina Higgins. Solving math word problems with processand outcome-based feedback, 2022. 2 [101] Jonathan Uesato, Nate Kushman, Ramana Kumar, H. Francis Song, Noah Y. Siegel, Lisa Wang, Antonia Creswell, Geoffrey Irving, and Irina Higgins. Solving math word problems with processand outcome-based feedback. CoRR, abs/2211.14275, 2022. [102] Chaojie Wang, Yanchen Deng, Zhiyi Lv, Zeng Liang, Jujie He, Shuicheng Yan, and Bo An. Q*: Improving multistep reasoning for llms with deliberative planning. CoRR, abs/2406.14283, 2024. 1, 2 [103] Ke Wang, Junting Pan, Weikang Shi, Zimu Lu, Mingjie Zhan, and Hongsheng Li. Measuring multimodal mathCoRR, ematical reasoning with math-vision dataset. abs/2402.14804, 2024. 3, 2 [104] Peng Wang, Shuai Bai, Sinan Tan, Shijie Wang, Zhihao Fan, Jinze Bai, Keqin Chen, Xuejing Liu, Jialin Wang, Wenbin Ge, Yang Fan, Kai Dang, Mengfei Du, Xuancheng Ren, Rui Men, Dayiheng Liu, Chang Zhou, Jingren Zhou, and Junyang Lin. Qwen2-vl: Enhancing vision-language models perception of the world at any resolution. arXiv preprint arXiv:2409.12191, 2024. 2, 3 [105] Peng Wang, Shuai Bai, Sinan Tan, Shijie Wang, Zhihao Fan, Jinze Bai, Keqin Chen, Xuejing Liu, Jialin Wang, Wenbin Ge, Yang Fan, Kai Dang, Mengfei Du, Xuancheng Ren, Rui Men, Dayiheng Liu, Chang Zhou, Jingren Zhou, and Junyang Lin. Qwen2-vl: Enhancing vision-language models perception of the world at any resolution. CoRR, abs/2409.12191, 2024. 6 [106] Peiyi Wang, Lei Li, Zhihong Shao, Runxin Xu, Damai Dai, Yifei Li, Deli Chen, Yu Wu, and Zhifang Sui. Mathshepherd: Verify and reinforce llms step-by-step without In Proceedings of the 62nd Annual human annotations. Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), ACL 2024, Bangkok, Thailand, August 11-16, 2024, pages 94269439. Association for Computational Linguistics, 2024. 1, 2, 5, 6 [107] Xuezhi Wang, Jason Wei, Dale Schuurmans, Quoc V. Le, Ed H. Chi, Sharan Narang, Aakanksha Chowdhery, and Denny Zhou. Self-consistency improves chain of thought reasoning in language models. In The Eleventh International Conference on Learning Representations, ICLR 2023, Kigali, Rwanda, May 1-5, 2023. OpenReview.net, 2023. 2, 6, [108] Xiyao Wang, Linfeng Song, Ye Tian, Dian Yu, Baolin Peng, Haitao Mi, Furong Huang, and Dong Yu. Towards selfimprovement of llms via mcts: Leveraging stepwise knowledge with curriculum preference learning. 2024. 1 [109] Haoran Wei, Chenglong Liu, Jinyue Chen, Jia Wang, Lingyu Kong, Yanming Xu, Zheng Ge, Liang Zhao, Jianjian Sun, Yuang Peng, et al. General ocr theory: Towards ocr-2.0 via unified end-to-end model. arXiv preprint arXiv:2409.01704, 2024. 2 [110] Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Brian Ichter, Fei Xia, Ed H. Chi, Quoc V. Le, and Denny Zhou. Chain-of-thought prompting elicits reasoning in large language models. In Advances in Neural Information Processing Systems 35: Annual Conference on Neural Information Processing Systems 2022, NeurIPS 2022, New Orleans, LA, USA, November 28 - December 9, 2022, 2022. 2 [111] Shijie Xia, Xuefeng Li, Yixin Liu, Tongshuang Wu, and Pengfei Liu. Evaluating mathematical reasoning beyond accuracy. CoRR, abs/2404.05692, 2024. 1 [112] An Yang, Baosong Yang, Binyuan Hui, Bo Zheng, Bowen Yu, Chang Zhou, Chengpeng Li, Chengyuan Li, Dayiheng Liu, Fei Huang, Guanting Dong, Haoran Wei, Huan Lin, Jialong Tang, Jialin Wang, Jian Yang, Jianhong Tu, Jianwei Zhang, Jianxin Ma, Jianxin Yang, Jin Xu, Jingren Zhou, Jinze Bai, Jinzheng He, Junyang Lin, Kai Dang, Keming Lu, Keqin Chen, Kexin Yang, Mei Li, Mingfeng Xue, Na Ni, Pei Zhang, Peng Wang, Ru Peng, Rui Men, Ruize Gao, Runji Lin, Shijie Wang, Shuai Bai, Sinan Tan, Tianhang Zhu, Tianhao Li, Tianyu Liu, Wenbin Ge, Xiaodong Deng, Xiaohuan Zhou, Xingzhang Ren, Xinyu Zhang, Xipin Wei, Xuancheng Ren, Xuejing Liu, Yang Fan, Yang Yao, Yichang Zhang, Yu Wan, Yunfei Chu, Yuqiong Liu, Zeyu Cui, Zhenru Zhang, Zhifang Guo, and Zhihao Fan. Qwen2 technical report. CoRR, abs/2407.10671, 2024. 1, 2, 4 [113] Shunyu Yao, Dian Yu, Jeffrey Zhao, Izhak Shafran, Tom Griffiths, Yuan Cao, and Karthik Narasimhan. Tree of thoughts: Deliberate problem solving with large language In Advances in Neural Information Processing models. Systems 36: Annual Conference on Neural Information Processing Systems 2023, NeurIPS 2023, New Orleans, LA, USA, December 10 - 16, 2023, 2023. [114] Lifan Yuan, Ganqu Cui, Hanbin Wang, Ning Ding, Xingyao Wang, Jia Deng, Boji Shan, Huimin Chen, Ruobing Xie, Yankai Lin, Zhenghao Liu, Bowen Zhou, Hao Peng, Zhiyuan Liu, and Maosong Sun. Advancing LLM reasoning generalists with preference trees. CoRR, abs/2404.02078, 2024. 1 [115] Zheng Yuan, Hongyi Yuan, Chengpeng Li, Guanting Dong, Chuanqi Tan, and Chang Zhou. Scaling relationship on learning mathematical reasoning with large language models. CoRR, abs/2308.01825, 2023. 2 [116] Zheng Yuan, Hongyi Yuan, Chengpeng Li, Guanting Dong, Chuanqi Tan, and Chang Zhou. Scaling relationship on learning mathematical reasoning with large language models. CoRR, abs/2308.01825, 2023. 1 [117] Di Zhang, Xiaoshui Huang, Dongzhan Zhou, Yuqiang Li, and Wanli Ouyang. Accessing GPT-4 level mathematical olympiad solutions via monte carlo tree self-refine with llama-3 8b. CoRR, abs/2406.07394, 2024. 1, 3 [118] Dan Zhang, Sining Zhoubian, Yisong Yue, Yuxiao Dong, and Jie Tang. Rest-mcts*: LLM self-training via process reward guided tree search. CoRR, abs/2406.03816, 2024. 1, 3, 5 [119] Ge Zhang, Yemin Shi, Ruibo Liu, Ruibin Yuan, Yizhi Li, Siwei Dong, Yu Shu, Zhaoqun Li, Zekun Wang, Chenghua Lin, Wenhao Huang, and Jie Fu. Chinese open instruction generalist: preliminary release. CoRR, abs/2304.07987, 2023. 4, [120] Renrui Zhang, Dongzhi Jiang, Yichi Zhang, Haokun Lin, Ziyu Guo, Pengshuo Qiu, Aojun Zhou, Pan Lu, Kai-Wei Chang, Peng Gao, and Hongsheng Li. Mathverse: Does your multi-modal LLM truly see the diagrams in visual math problems? CoRR, abs/2403.14624, 2024. 3 [121] Zhuosheng Zhang, Aston Zhang, Mu Li, Hai Zhao, George Karypis, and Alex Smola. Multimodal chain-of-thought reasoning in language models. Trans. Mach. Learn. Res., 2024, 2024. 2 [122] Penghao Zhao, Hailin Zhang, Qinhan Yu, Zhengren Wang, Yunteng Geng, Fangcheng Fu, Ling Yang, Wentao Zhang, and Bin Cui. Retrieval-augmented generation for aigenerated content: survey. CoRR, abs/2402.19473, 2024. 4 [123] Junjie Zhou, Zheng Liu, Shitao Xiao, Bo Zhao, and Yongping Xiong. VISTA: visualized text embedding for universal multi-modal retrieval. In Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), ACL 2024, Bangkok, Thailand, August 11-16, 2024, pages 31853200. Association for Computational Linguistics, 2024. 5 [124] Ren Zhou. Advanced embedding techniques in multimodal retrieval augmented generation comprehensive study on cross modal ai applications. Journal of Computing and Electronic Information Management, 13(3):1622, 2024. 3 [125] Deyao Zhu, Jun Chen, Xiaoqian Shen, Xiang Li, and Mohamed Elhoseiny. Minigpt-4: Enhancing vision-language In understanding with advanced large language models. The Twelfth International Conference on Learning Representations, ICLR 2024, Vienna, Austria, May 7-11, 2024. OpenReview.net, 2024. 1, [126] Wenwen Zhuang, Xin Huang, Xiantao Zhang, and Jin Zeng. Math-puma: Progressive upward multimodal alignment to enhance mathematical reasoning. CoRR, abs/2408.08640, 2024. 2, 6 [127] Yi Zong and Xipeng Qiu. GAOKAO-MM: chinese humanlevel benchmark for multimodal models evaluation. In Findings of the Association for Computational Linguistics, ACL 2024, Bangkok, Thailand and virtual meeting, August 1116, 2024, pages 88178825. Association for Computational Linguistics, 2024. 6,"
        },
        {
            "title": "Contents",
            "content": "1. Introduction 2. Related Work 3. Preliminary 4. Methodology 4.1. Hybrid-Modal Retrieval Corpus Construction 4.2. Unified Multimodal Retrieval Module . . . . 4.3. Knowledge Concept Filtering . . . . . 4.4. Progressive Multimodal Reasoning Annotation 4.5. Curriculum Process Reward Modeling . . . . . . . 5. Experiments . . . . . . . . 5.1. Experimental Setup . 5.2. Overall Results . . . . . . 5.3. General Reasoning Domain Verification . . . . . . . 5.4. Quantitative Analysis . 5.5. Does AR-MCTS Improve the Sampling Space? . . . . . . . . . . . . . . . . . . . . . . . 6. Conclusion A. More Details about AR-MCTS A.1. The Algorithm Workflow of AR-MCTS . . . . . . B. More Details about Experimental Setup . . . . . . . B.1. Benchmarks and Datasets . . . . . B.2. Baselines and Backbone Models . B.3. Implementation Details . . . . . . B.4. Detailed Processing about Retrieved Corpus . B.5. PRM Training Guideline . . . . . B.6. Detailes about Knowledge Concept Filtering . . . . . . . . . . . . . Knowledge . C. More Details about Experimental Results C.1. Results on More MLLMs backbones . . . . C.2. The Composition Analysis of Retrieval . . . . . . C.3. Contamination Analysis on Hybrid-modal Re- . . . . . . C.4. Ablations on Different Retrievers . . . . C.5. Comparison of Different Training Objectives . . . . . . trieval Corpus . for PRMs . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . D. Limitations and Future Work Table 4. The statistics of General Reasoning Knowledge. Dataset Wikipedia(zh-CN) Wikipedia(en-US) COIG Count Percentage 4.7B 15B 178K 23.9% 73.6% 0.1% A. More Details about AR-MCTS A.1. The Algorithm Workflow of AR-MCTS In this section, we will explore the overall workflow of ARMCTS, highlighting its key components and steps involved in the retrieval and inference process. For each query q, we begin by applying the Unified Retrieval module to extract key insights, denoted as Dins. These insights serve as sub-corpus for performing active retrieval during the MCTS process. As outlined in Algorithm 1, distinct retrievers are utilized to handle text-only and multimodal corpora separately. The top-K knowledge retrieved from both routes is then combined to form Dtop-K. This set of documents undergoes further refinement through the Knowledge Concept Filter, yielding the final corpus of key insights, Dins. Once the key insights are obtained, the AR-MCTS inference algorithm, detailed in Algorithm 2, is executed. During each expansion step t, given beam size B, the top-B most relevant insights are retrieved from Dins. Each retrieved document is paired with the previous state st1 and fed into independent paths, where the multimodal large language model (MLLM), M, generates the next state. The Process Reward Model (PRM), πθ, then evaluates the candidate states s(t,1), . . . , s(t,B) and assigns PRM scores to each. The state with the highest PRM score is appended to the selected path P. This process continues until terminal state is reached, resulting in the final reasoning trajectory and the corresponding answer. B. More Details about Experimental Setup B.1. Benchmarks and Datasets Here are the details of the benchmarks/datasets we used in our hybrid-modal retrieved corpus and experiments. The statistics of the datasets are recorded in Table 4 and 5. WE-MATH [75] is benchmark based on textbook knowledge units, focusing on decomposing complex problems into sub-problems using fundamental concepts. It mirrors how students learn progressively and is organized 2 3 3 3 4 4 4 5 6 6 6 7 7 8 9 1 1 1 3 4 5 5 5 5 5 6 6 6 7 Algorithm 1 Unified Retrieval Require: Query q, hybrid-modal retrieval corpus DH, top-K, cross-modal retriever Rc, text-to-text retriever Rt Ensure: Top-K retrieved hybrid-modal samples Dtop-K 1: for all di DH do 2: 3: Query embedding Eq Rt(q) Document embedding Edi Rt(di) Retrieved documents Dtext = argtopi=1,...,N 4: 5: end for 6: for all image-text pair (x, t) DH do Image embedding EI (x) Rc(x) 7: Text embedding ET (t) Rc(t) 8: if = = then 9: (cid:2)E di (cid:3) Eq Text-Only Retrieval Cross-modal Retrieval Ex(x, t) EI (x)+ET (t) else if = = then Ex(x, t) ET (t) end if Mixed vector Ex(Qm) EI (x)+ET (t) Retrieved documents Dcross argtopj=1,...,N 2 15: 16: end for 17: Dtop-K {Dq Dcross} Require: Knowledge concept label Lkc, original retrieval threshold Tr, knowledge concept consistency threshold Tkc Ensure: Key insights Dins for query 18: Key insights Dins = {r DH Sim(r, q) Tr & Sim(r, Lkc) Tkc} Knowledge Concept Filtering (cid:2)Ex(Qm) Ex(xj, tj)(cid:3) Algorithm 2 Inference with AR-MCTS Require: Beam Size B, question q, Process Reward Model πθ, max depth , MLLM M, multimodal retriever Ensure: Selected path (thought process and answer) Initialize Selected Path 10: 11: 12: 13: 14: 4: 5: 6: 7: 8: 9: 1: = [s0], = 0 2: while < non-terminal path in do 3: Retrieved insights Dins Dtop_B R(P, Dins) for all di Dtop_B do s(t,i) M(P, di) PRM score score(st,i) = πθ(st,i) end for index(argmax(score)) Add s(t,j) to Increment + 1 10: 11: 12: end while hierarchically, following textbook content to maintain independent knowledge units while establishing logical connections between levels. It uses diverse evaluation metrics to comprehensively assess models ability of solving multimodal mathematical problems step by step. MATHVISTA [58] is mathematical visual benchmark consisting of 6,141 examples. These examples are divided into two subsets: testmini (1,000 examples), for which answers are provided, and test (5,141 examples), for which answers are not publicly available. We use this dataset as benchmark to evaluate visual understanding and compositional reasoning abilities. Additionally, we employ LLaVA-OneVision-70B to generate answers for the test split, creating an in-domain corpus that can be used for retrieving answers in the testmini set. MathVision [103] is carefully curated dataset consisting of 3,040 high-quality mathematical problems, each accompanied by visual context derived from real mathematics competitions. The collection covers 16 distinct mathematical domains and is categorized across five levels of difficulty. We use this dataset as part of math reasoning knowledge base. Table 5. The statistics of Mathematics-Specific Reasoning Knowledge. Dataset Text-only Datasets GSM8K MATH Multimodal Datasets MATHVISTA MathVerse MathVision WE-MATH Count Percentage 8,792 12,500 6,141 2,612 3,040 1, 24.6% 36.2% 17.8% 7.6% 8.8% 5.0% MATHVERSE [120] is comprehensive and specialized visual mathematics benchmark designed to evaluate the multimodal mathematical reasoning abilities of MLLMs. It comprises dataset of 2,612 visual math problems, with 1,236 newly acquired from public question repositories and 1,376 sourced from existing benchmarks. Each problem has been transformed by human annotators into six distinct versionstext-dominant, text-lite, text-only, vision-intensive, vision-dominant, and vision-onlyeach offering different levels of multimodal information. In our study, we utilize the \"vision-only\" version as image data and the \"text-only\" version as textual data to construct knowledge base. This dataset is employed solely for the purpose of knowledge base construction and not as benchmark. MATH [33] is dataset comprising 12,500 challenging competition mathematics problems. Each problem includes comprehensive step-by-step solution, which can be used to train models in generating answer derivations and explanations. The dataset features problems from various mathematics competitions, including the AMC 10, AMC 12, AIME, and others. We utilize this dataset as text-only corpus for mathematical domain reasoning. GSM8K [18] (Grade School Math 8K) is dataset containing 8,500 high-quality, linguistically diverse grade school math word problems. This dataset was designed to support question-answering tasks for basic mathematical problems requiring multi-step reasoning. This dataset is also used as part of our text-only mathematics-specific reasoning corpus. COIG [119] (Chinese Open Instruction Generalist) is set of Chinese instruction datasets to advance the training and fine-tuning of Chinese LLMs. COIG includes five key corpora: manually verified translated instruction corpus (66,858 entries), an exam-based Chain-of-Thought (CoT) instruction corpus derived from national exams (63,532 entries), human value alignment corpus reflecting general and region-specific cultural values (34,471 entries), counterfactual correction multi-round chat corpus addressing hallucinations and inconsistencies (13,653 dialogues), and Leetcode instruction corpus supporting code-related tasks (11,737 entries). We utilize this dataset along with the Wikipedia corpus (English version and Chinese version) as our general reasoning knowledge base. GAOKAO-MM [127] is comprehensive Chinese multimodal benchmark designed based on the Chinese National College Entrance Examination (Gaokao). It encompasses eight academic subjects and includes twelve categories of images, such as diagrams, function graphs, maps, and photographs. The benchmark aims to evaluate models abilities to understand and reason over diverse multimodal content, reflecting the complexity and breadth of knowledge. We construct the domain-specific knowledge base using questions and answers from the years 2010 to 2021, while employing the questions from 2022 and 2023 as the test set. B.2. Baselines and Backbone Models To assess the gains from our approach, we compare against number of baselines as follows. Self-Consistency [107] involves sampling multiple reasoning paths from large language model. Since each path may lead to different final answers, Self-Consistency selects the most consistent answer as the final output by marginalizing these sampled paths. This method is based on the intuition that complex reasoning problems often have unique correct answer that can be reached through various approaches. Self-Correction [65] is an iterative refinement method that improves the output of large language models (LLMs) or Multimodal large language models (MLLMs) through selffeedback. The core idea is to mimic the human revision process in writing: first, preliminary output is generated, feedback is provided on this output, and improvements are made iteratively based on the feedback. Notably, this process allows for iterative optimization. ORM [17] samples data from the reasoning training set to obtain result-oriented annotations for each sampled path. These data are then used to train verifier that assists the generator in identifying higher-quality reasoning paths during prediction. In this paper, we use the same data as for training PRM, with the distinction that the annotations are made directly using ground truth results rather than through AR-MCTS for step-level annotations, training ORM to assess the quality of reasoning paths. In addition, we provide detailed introduction to the MLLMs designed in our experiment and its corresponding foundational language model. Qwen2-VL [104], developed by Alibaba Cloud, represents an advanced iteration of the Qwen-VL series. By employing the Naive Dynamic Resolution mechanism, it dynamically processes images with varying resolutions and aspect ratios. The model achieves state-of-the-art performance on several visual understanding benchmarks, including MATHVISTA, MathVision, and WE-MATH. InternVL2 [15] family comprises multimodal large language models designed for advanced multimodal understanding tasks, demonstrating performance competitive with proprietary systems. Built using progressive alignment training strategy, InternVL2 supports multimodal inputs, generalizes across diverse downstream tasks, and spans models ranging in size from 1 billion to 108 billion parameters. The InternVL2-8B variant exhibits remarkable capabilities in complex reasoning and shows promise for mathematical problem-solving applications. LLaVA-NeXT [53] is large-scale multimodal language model optimized through cost-effective, realistic visual It emphasizes enhanced viinstruction-tuning dataset. sual reasoning, optical character recognition (OCR), and visual conversation capabilities. LLaVA-NeXT demonstrates superior performance across various multimodal benchmarks, including MMMU and MATHVISTA. LLaVA-OneVision [43] is family of large-scale multimodal large language models (MLLMs) designed to extend the performance boundaries of open MLLMs across diverse scenarios, including single-image, multi-image, and video applications. It processes text, images, interleaved image-text inputs, and videos, supporting resolutions of up to 23042304 pixels. LLaVA-OneVision is available in various sizes, ranging from 0.5 billion to 72 billion parameters, and facilitates robust task transfer learning across modalities. Notably, it demonstrates exceptional video understanding by leveraging task transfer capabilities developed from image-based training. GPT-4o [69], proprietary large-scale multimodal model developed by OpenAI, processes vision, text, and audio inputs. Built on Transformer architecture, the model is pretrained on next-token prediction tasks and refined through post-training alignment process. GPT-4o exhibits stateof-the-art multimodal understanding, achieving outstanding results across variety of complex multimodal tasks. GPT-4V [68], also developed by OpenAI, is highly capable multimodal system enabling users to process image inputs and interleaved image-text data with GPT-4 models. It achieves impressive human-level performance across broad spectrum of tasks, including scene text understanding, abstract reasoning, and open-world question answering. Qwen2 [112] is series of large language models (LLMs) based on the Transformer architecture, trained on highquality, diverse dataset of over 7 trillion tokens using nexttoken prediction. Spanning parameter sizes from 0.5 billion to 72 billion, Qwen2 is designed to enhance mathematical and coding reasoning capabilities. It achieves performance competitive with proprietary models across benchmarks for reasoning, language understanding, and generation. The Qwen2 series includes both foundational models and instruction-tuned variants, fine-tuned on datasets for single-turn and multi-turn instruction following. InternLM2.5 [8] is series of LLMs optimized for superior mathematical reasoning. Based on the InternLM2.5 foundational models, the series includes chat models fine-tuned through supervised fine-tuning (SFT) and reinforcement learning from human feedback (RLHF), enabling robust instruction-following capabilities in downstream tasks. Notably, InternLM2.5-Chat-1M supports 1-million-token context, demonstrating exceptional performance on long-context benchmarks. Llama3 [25] is family of LLMs built on standard dense Transformer architecture, natively supporting multilinguality, coding, reasoning, and tool integration. Pretrained on large-scale, meticulously curated dataset, it undergoes post-training through supervised fine-tuning (SFT), rejection sampling (RS), and direct preference optimization (DPO). The flagship model, LLaMA3-405B, represents significant scale-up from its predecessor LLaMA2, trained on 15.6 trillion text tokens. It delivers competitive performance with GPT-4 across diverse benchmarks, including GSM8k, MATH, and MMLU. B.3. Implementation Details For uni-modal retrieval, we utilize mcontriever-mscoco, multilingual version of Contriever [34] fine-tuned on the MSMARCO dataset, as the text encoder. For multimodal retrieval, we employ the frozen CLIP model (ViT-L/14@336px variant) [77] as the multimodal encoder for both texts and images. To ensure the diversity and relevance of multimodal retrieval results, we incorporate five types of similarity measures: text-to-text, text-to-image, image-to-image, image-totext and cross-modal retrieval (introduced in Section 4.2) . Given the extensive size of the knowledge base, we leverage the open-source indexing engine FAISS [36] to efficiently index dense vectors and retrieve the Top-k knowledge pieces. For the Curriculum Process Reward Modeling, in the \"Step-Wise DPO Pre-Alignment\" phase, the learning rate is set to 5e-7 with cosine scheduler and 0.1 warm-up ratio. We use DeepSpeed ZeRO Stage 3 [81] and Flash-Attention 2 [19] for efficiency, with global batch size of 64. Training utilizes sigmoid loss function with beta value of 0.3 and spans 2 epochs, with checkpoints every 500 steps. Mixed precision training with bf16 is employed, and the maximum context length is 4096 tokens. In the \"Point-Wise Fine-Tuning\" phase, we perform full fine-tuning on our PRM with learning rate of 7e-6, using linear scheduler with 20 warm-up steps. All models are trained with DeepSpeed ZeRO Stage 3 and Flash-Attention 2. We use global batch size of 128, weight decay of 0.1, and train for 3 epochs, saving checkpoints every 200 steps. Mixed precision training with bf16 is used, and the maximum context length is set to 8192 tokens. We run all our experiments on 8 NVIDIA A800 GPUs. Table 6. Mathematical evaluation on MATHVISTA testmini sets. We select 6 out of the original 12 mathematical categories in MATHVISTA: ALL (overall accuracy), GPS (geometry problem solving), MWP (math word problems), ALG (algebraic reasoning), GEO (geometry reasoning), and STA (statistical reasoning). In the results for each model, the best accuracy scores are highlighted in bold. B.4. Detailed Processing about Retrieved Corpus For Chinese evaluation on GAOKAO-MM, we utilize the COIG dataset and Chinese Wikipedia dump which contains over 2.6 million articles as the textual knowledge base. We first apply the tool WikiExtractor [5] to extract clean texts from the Wikipedia dump and remove low-resource articles, which results in over 1.3 million articles. We then split each article into disjoint passages of 256 characters, resulting in 4.7B passages. To enrich our knowledge base with more relevant in-domain information, we split GAOKAO-MM questions from 2010 to 2021 as multimodal knowledge base. Since the evaluation is conducted on more up-todate subset GAOKAO-MM from year 2022 to 2023, we can effectively mitigate the risk of data leakage. For English evaluation on MATHVISTA and WE-MATH, we choose COIG, GSM8K, MATH, and the English Wikipedia dump as the textual knowledge base. Following the same pre-processing steps of Chinese Wikipedia dump, we obtain over 15B passages from the English Wikipedia dump as the basic retrieval unit. We have opted to employ MathVerse and MathVision as the multimodal knowledge base for their relevance to mathematical problem-solving and comprehension. Following MRAG-COT [52], we use responses from LLaVA-OneVision [43] as pseudo-answer for the test set of MATHVISTA. Due to the absence of an appropriate high-quality multimodal reasoning retrieval source or training set with answer annotation, we incorporate the testmini set of WE-MATH into the knowledge base of MATHVISTA and include the testmini set of MATHVISTA into the knowledge base of WE-MATH. B.5. PRM Training Guideline Our PRM leverages the corresponding text backbone for evaluating MLLMs and consistently uses Qwen2-7B for closedsource models. Due to the lack of step supervision in multimodal reasoning, we collect existing open-source text-only PRM dataset, such as AlphaMath [9], Math-Shepherd [106], and PRM800K [49]. We first follow previous text-only works [106, 118] and perform preliminary fine-tuning alignment on our PRM backbone. Using the pre-aligned LLM, we apply the annotated data Dalign from AR-MCTS and follow section \"Curriculum Process Reward Modeling\" to finalize the PRM. Consequently, we do not perform targeted fine-tuning on any MLLMs with in-domain data; instead, we focus on optimizing the PRM, significantly reducing computational resource consumption. Model Method ALL GPS MWP ALG GEO STA GPT4V LLaVANEXT Zero-shot 53.7 Self-Consistency 56.2 50.4 Self-Correction 56.6 ORM 57.4 AR-MCTS 22.5 Zero-shot Self-Consistency 23.1 22.5 Seld-Correction 24.4 ORM 25.6 AR-MCTS 59.6 65.4 56.3 65.3 66. 22.3 22.6 22.6 22.6 23.0 53.8 53.2 50.2 53.1 53.9 13.4 16.7 17.2 17.5 17.4 59.8 63.7 55.9 65.2 64.8 24.4 26.0 24.9 27.9 28.1 58.2 63.2 56.1 63.2 63. 24.7 24.3 22.6 24.3 28.6 58.5 58.8 57.4 59.0 59.5 22.3 24.3 25.2 29.9 31.5 B.6. Detailes about Knowledge Concept Filtering As stated in the main text, high-quality labels are available for test sets like MATHVISTA and WE-MATH. However, not all external retrieval libraries or evaluation datasets have fine-grained concept labels (e.g., Wikipedia). To ensure the scalability of concept filtering, we use the open-world Tagger InsTag [57] for offline knowledge concept annotation and repeat the aforementioned process for consistency filtering. Specifically, we select the TagLM-13b-v2.0 model 2. For text-only data, we directly annotated using InsTag and concatenated all coarse and fine-grained labels. For multimodal data, we generated captions for the images using the corresponding evaluation MLLM backbone, referencing InternVL2 [13] and Vista [123]. We design the following caption generation template: \"This is an image of reasoning question; can you provide detailed description of the image content?\" We then concatenated the captions with the text and further used InsTag for annotation. After obtaining fine-grained labels, we followed the process outlined in the Knowledge Concept Filtering section for consistency screening. C. More Details about Experimental Results C.1. Results on More MLLMs backbones To further validate the scalability of AR-MCTS, we conduct generalization studies on the widely used open-source MLLM Llama3-Llava-Next-8B and the powerful closedsource MLLM GPT-4V using MATHVISTA. As shown in Table 6, AR-MCTS continues to achieve stable improvements and aligns with the three core conclusions from our main experiments: 1. MLLMs struggle to self-correct reasoning errors. 2https://huggingface.co/OFA-Sys/TagLM-13b-v2.0 Figure 7. The composition analysis on retrieval corpus of WE-MATH and MATHVISTA. Table 7. The contamination analysis on hybrid-modal retrieval corpus. Table 8. The ablations of different text retrievers. Dataset MATHVISTA WE-MATH Text-only Datasets COIG WIKIPEDIA(EN-US) GSM8K MATH Multimodal Datasets MATHVERSE MATHVISION WE-MATH MATHVISTA-testmini 0.1% 0.6% 4.5% 4.5% 0.7% 0.3% 0.5% - 0.1% 1.1% 2.0% 1.8% 2.9% 0.9% - 4.2% 2. PRM outperforms ORM in complex reasoning tasks. 3. AR-MCTS better unlocks the reasoning potential of weaker MLLMs. This further confirms the scalability of our core experimental findings. C.2. The Composition Analysis of Retrieval Knowledge To gain deeper insights into which knowledge sources provide the greatest benefits to our multimodal reasoning test set, we conduct comprehensive ranking of the hybrid-modal knowledge retrieved from samples of MATHVISTA and WEMATH based on similarity, selecting the Top-50 relevant knowledge samples and visualizing their respective sources. As shown in Figure 7, both MATHVISTA and WE-MATH exhibit significant diversity in their retrieved knowledge, whether from text-only or multimodal sources. This highlights the motivation for constructing our hybrid-modal retrieval library from diverse, high-quality reasoning datasets. It also confirms that the insights needed for problem-solving do not necessarily originate from the same type of data source but should be enhanced through diverse reasoning knowledge. Our hybrid-modal reasoning retrieval library effectively addresses this need. Model ALL GPS MWP ALG GEO STA Qwen2-VL-7B 58.8 60.2 + BM25 59.9 + Contriever 45.5 54.8 53. 60.5 57.9 58.5 45.5 53.3 53.3 47.9 54.6 54.1 70.8 72.1 72.4 C.3. Contamination Analysis on Hybrid-modal Retrieval Corpus To further ensure that our hybrid-modal retrieval corpus does not contain any data leakage examples from the test set, we perform data contamination analysis. We employ commonly used n-gram contamination algorithms to assess the overlap between the Top-50 samples retrieved by the retriever from different data sources and various test sets. As shown in Table 7, we follow the AUTOIF [21] and test the ngram threshold of 13. The results show that all data sources exhibited an overlap of less than 5% with MATHVISTA and WE-MATH. This highlights that there is no overlap between our retrieval library and the test sets. C.4. Ablations on Different Retrievers To validate the effectiveness of our general retrieval component, we conduct ablation studies by replacing different text and multimodal retrievers. Specifically, we used the following: Text Retrievers: BM25 (Sparse), Contriever (Dense) Multimodal Retrievers: CLIP-ViT-L/14, Jina-CLIPv1 [38] The experimental results are presented in the Table 8 and 9, where we concatenated the top two retrieval results for each sample. The results indicate that different retrievers provide varying degrees of enhancement for downstream reasoning tasks. This not only demonstrates that our general retrieval module is plug-and-play but also highlights the rationale behind our mixed retrieval library. Table 9. The ablations of different multimodal retrievers. Model S1 S2 S3 Qwen2-VL-7B 53.4 37.2 33.9 + CLIP-ViT-L/14 54.9 38.7 34.5 54.4 36.9 34.1 + Jina-CLIP-v highlights knowledge gaps in stepwise reasoning [56]. AR-MCTS introduces dynamic retrieval strategy that effectively addresses this issue. We believe this area still holds great potential for exploration, particularly in dynamically supplementing missing knowledge based on feedback from multimodal large models, which will be key focus of our future research. Table 10. The comparison of different training objectives for PRMs. Model ALL GPS MWP ALG GEO STA PRM (Hard) PRM (Soft) 62.9 64. 63.3 63.9 71.5 72.6 59.4 60.9 62.2 63.6 71.0 72.4 C.5. Comparison of Different Training Objectives for PRMs In this section, we explore the relationship between the training method of PRM and its multimodal reasoning capabilities. Following the setup of Luo et al., we investigate the use of hard labels versus soft labels trained through linear layer connected to large model. As shown in Table 10, we find that PRM trained with soft labels performs better than that trained with hard labels on MATHVISTA. D. Limitations and Future Work Despite our efforts to optimize the AR-MCTS process, several limitations and areas for improvement remain: Computational Cost Optimization: Annotating processes with MCTS algorithms requires significant computational resources, leading to high resource consumptiona common challenge in reasoning verification. However, AR-MCTS costs are still substantially lower than manual annotation. As plug-and-play framework, ARMCTS focuses on optimizing reasoning without the need to train multimodal foundational models, which significantly reduces computational overhead. The emergence of efficient techniques like vLLM [39] is also helping to address this issue. Exploration of PRM for Multimodal Model Foundations: AR-MCTS represents pioneering effort in stepwise reasoning within the multimodal domain, utilizing foundational training of MLLMs to align the PRM process. An ideal scenario would involve training the PRM within these models to enhance interactions between image and text and provide supplemental information for stepwise reasoning. However, the lack of annotated data and higher computational demands present significant challenges in this area, which remains largely unexplored and is direction for our future work. Deep Integration of Retrieval and Reasoning: Research"
        }
    ],
    "affiliations": [
        "Gaoling School of Artificial Intelligence, Renmin University of China"
    ]
}