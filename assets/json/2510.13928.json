{
    "paper_title": "LLMs Can Get \"Brain Rot\"!",
    "authors": [
        "Shuo Xing",
        "Junyuan Hong",
        "Yifan Wang",
        "Runjin Chen",
        "Zhenyu Zhang",
        "Ananth Grama",
        "Zhengzhong Tu",
        "Zhangyang Wang"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "We propose and test the LLM Brain Rot Hypothesis: continual exposure to junk web text induces lasting cognitive decline in large language models (LLMs). To causally isolate data quality, we run controlled experiments on real Twitter/X corpora, constructing junk and reversely controlled datasets via two orthogonal operationalizations: M1 (engagement degree) and M2 (semantic quality), with matched token scale and training operations across conditions. Contrary to the control group, continual pre-training of 4 LLMs on the junk dataset causes non-trivial declines (Hedges' $g>0.3$) on reasoning, long-context understanding, safety, and inflating \"dark traits\" (e.g., psychopathy, narcissism). The gradual mixtures of junk and control datasets also yield dose-response cognition decay: for example, under M1, ARC-Challenge with Chain Of Thoughts drops $74.9 \\rightarrow 57.2$ and RULER-CWE $84.4 \\rightarrow 52.3$ as junk ratio rises from $0\\%$ to $100\\%$. Error forensics reveal several key insights. First, we identify thought-skipping as the primary lesion: models increasingly truncate or skip reasoning chains, explaining most of the error growth. Second, partial but incomplete healing is observed: scaling instruction tuning and clean data pre-training improve the declined cognition yet cannot restore baseline capability, suggesting persistent representational drift rather than format mismatch. Finally, we discover that the popularity, a non-semantic metric, of a tweet is a better indicator of the Brain Rot effect than the length in M1. Together, the results provide significant, multi-perspective evidence that data quality is a causal driver of LLM capability decay, reframing curation for continual pretraining as a \\textit{training-time safety} problem and motivating routine \"cognitive health checks\" for deployed LLMs."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 5 1 ] . [ 1 8 2 9 3 1 . 0 1 5 2 : r a"
        },
        {
            "title": "Preprint",
            "content": "LLMS CAN GET BRAIN ROT! Shuo Xing1, Junyuan Hong2, Yifan Wang3, Runjin Chen2, Zhenyu Zhang2, Ananth Grama3, Zhengzhong Tu1, Zhangyang Wang2 1Texas A&M University, 2University of Texas at Austin, 3Purdue University (cid:140) Model & Code: https://llm-brain-rot.github.io/"
        },
        {
            "title": "ABSTRACT",
            "content": "We propose and test the LLM Brain Rot Hypothesis: continual exposure to junk web text induces lasting cognitive decline in large language models (LLMs). To causally isolate data quality, we run controlled experiments on real Twitter/X corpora, constructing junk and reversely controlled datasets via two orthogonal operationalizations: M1 (engagement degree) and M2 (semantic quality), with matched token scale and training operations across conditions. Contrary to the control group, continual pre-training of 4 LLMs on the junk dataset causes nontrivial declines (Hedges > 0.3) on reasoning, long-context understanding, safety, and inflating dark traits (e.g., psychopathy, narcissism). The gradual mixtures of junk and control datasets also yield dose-response cognition decay: for example, under M1, ARC-Challenge with Chain Of Thoughts drops 74.9 57.2 and RULER-CWE 84.4 52.3 as junk ratio rises from 0% to 100%. Error forensics reveal several key insights. First, we identify thought-skipping as the primary lesion: models increasingly truncate or skip reasoning chains, explaining most of the error growth. Second, partial but incomplete healing is observed: scaling instruction tuning and clean data pre-training improve the declined cognition yet cannot restore baseline capability, suggesting persistent representational drift rather than format mismatch. Finally, we discover that the popularity, non-semantic metric, of tweet is better indicator of the Brain Rot effect than the length in M1. Together, the results provide significant, multiperspective evidence that data quality is causal driver of LLM capability decay, reframing curation for continual pretraining as training-time safety problem and motivating routine cognitive health checks for deployed LLMs."
        },
        {
            "title": "INTRODUCTION",
            "content": "In 2024, the term Brain Rot was named the Oxford word of year (Oxford University Press, 2024) when it drew increasing concern in modern society. Brain rot is defined as the deleterious effect on human cognition that comes from consuming large volumes of trivial and unchallenging online content (or junk data) due to Internet addiction. The cognitive impact of Internet addiction have been found to be significant (Firth et al., 2019) along three dimensions: (i) Attentional capacities the constant stream of online information often undermines the ability to sustain focus on reading articles or solving challenging problems (Haliti-Sylaj & Sadiku, 2024); (ii) Memory processes the abundance of online information alters how individuals store, retrieve, and prioritize knowledge (Vedechkina & Borgonovi, 2021); and (iii) Social cognition online interactions mimic real-world social dynamics, reshaping self-concepts and influencing self-esteem (Yousef et al., 2025). Beyond for these cognitive impacts, recent study in Turkish population (Satici et al., 2023) found that Internet addiction (mainly on X.com) is associated with higher psychological distress and changes in personality, including negative relationship with conscientiousness, extroversion, and agreeableness, as well as significant positive relationship with neuroticism. In parallel to the rise of Brain Rot in human cognition, artificial intelligence, represented by Large Language Models (LLMs), grows to gain human-like cognition (Binz & Schulz, 2023) via learning Correspondence to jyhong@utexas.edu, atlaswang@utexas.edu. Lead authors with equal contributions. Core contributors."
        },
        {
            "title": "Preprint",
            "content": "Figure 1: Outline of our work: (i) Inspired by the concept of Brain Rot, we establish the hypothesis of LLM Brain Rot; (ii) We construct junk and control data from Twitter/X posts for intervention; (iii) We benchmark four different cognitive functions of the intervened LLMs; (iv) We analyze the results to identify the failure modes caused by the brain rot; and (v) Brain rot is persistent after various mitigation. from trillions of the very similar Internet data (Hoffmann et al., 2022; Henighan et al., 2020; Hestness et al., 2017). As consequence of such learning mechanism, LLMs inevitably and constantly consume enormous amounts of junk data like humans. Therefore, it is natural to ask if the analogous Brain Rot emerges in LLMs. Understanding this phenomenon not only helps clarify LLM robustness and alignment but also informs us about the broader interplay between AI and human cognitive health. While LLMs obviously do not have grey matter or neurons in the same sense as humans, they do have parameters and attention mechanisms that might analogously be overfitted or distracted by certain data patterns. Prior work has identified data patterns that threaten LLM safety. For example, Qi et al. (2023) demonstrated that fine-tuning LLMs on malicious or benign supervised tasks can void safety alignment. Compared to fine-tuning, pre-training could be affected more significantly, as LLMs have to continuously learn new knowledge from the uncurated Internet data. For instance, LLMs can be taught to leak private information by poisoning pre-training data with crafted repetitive patterns (Panda et al., 2024). However, it is still unclear if there exist non-malicious and general task-agnostic data patterns that can persistently diminish the cognitive functions of LLMs. In this work, we translate insights from human cognition to LLM cognition by establishing the LLM Brain Rot Hypothesis: continual pre-training on junk web text induces lasting cognitive decline in LLMs. To validate the hypothesis, we design controlled experiment that compares LLM behaviors after being fed with junk and control data. As outlined in Fig. 1, we construct junk and control datasets from social media (Twitter/X) via the two junk metrics: M1 (engagement degree) selects short but highly popular posts that often engage users longer online; and M2 (semantic quality) flags content based on content styles that draw users attention. Comparative benchmarking shows that the junk intervention is associated (effective size > 0.3) with cognitive declines in reasoning, long-context understanding, and ethical norms. We surprisingly find that some dark personalities of LLMs emerge with M1 junk intervention, casting significant safety concerns. Experiments on mixtures of junk and control data demonstrate gradual dose responses on reasoning and long-context understanding: For example, under M1 intervention, ARC-Challenge (Clark et al., 2018) with Chain Of Thoughts Wei et al. (2022) drops 74.9 57.2 and RULER-CWE (Hsieh et al., 2024) 84.4 52.3 as junk ratio rises from 0% to 100%. Our major contributions include three-fold: (i) We proposed the LLM Brain Rot Hypothesis and validated it via controlled experiments; (ii) Our detailed analysis uncovered fine-grained failure modes caused by junk intervention, including thought skipping in reasoning, and that popular data and short data contribute to different kinds of cognitive declines individually; and (iii) We examined potential post-hoc mitigation using similar scale of data, observing that instruction tuning is an effective strategy with more samples, but cannot fully restore the cognitive capabilities. Such persistent Brain Rot effect calls for future research to carefully curate data to avoid cognitive damages in pre-training."
        },
        {
            "title": "2 RELATED WORK",
            "content": "Our work introduces novel perspective on the data quality that affects LLM training. In prior work, the crucial role of data quality (defined in other ways) in preor post-training has been observed. Data Quality in Pre-training. On the positive side, selecting high-quality data (e.g., good writing style, required expertise, facts & trivia, and educational value) can improve the robustness and the generalization of pre-trained models (Wettig et al., 2024). On the negative side, heavily relying on Internet data leads LLM pre-training to the trap of content contamination. For example, the widespread use of LLMs causes more and more generated content on the Internet, contaminating the pre-training corpus and resulting in the forgetting of tail-distribution (model collapse) (Shumailov et al., 2023; 2024; Seddik et al., 2024). Even worse, malicious Internet users can implant backdoors or malicious behaviors (e.g., denials of service, belief manipulation) by controlling only 0.1% of the pre-training data (Zhang et al., 2024). Data Quality in Post-training. The quality of data is also critical in post-training, particularly in alignment of LLM responses toward human preference (Christiano et al., 2017; Bai et al., 2022b). Brain rot is related to previously-noticed fragility of LLMs: alignment in LLMs is not deeply internalized but instead easily disrupted. Prior studies have shown the superficial nature of alignment: It can be achieved using small amounts of high-quality data (Zhou et al., 2023; Chen et al., 2025; Raghavendra et al., 2024), and therefore can be easily undone by jailbreaking (Zou et al., 2023) or few-shot fine-tuning on common tasks (Qi et al., 2023). Even modest data shifts during preference fine-tuning can dramatically affect safety, with models reverting to unsafe on unseen data (Wang et al., 2025) or being implanted with malicious behaviors (Fu et al., 2024). Distinct from prior work, we provide new view on data quality the extent to which content is trivial and easy to consume in social media. The properties, conceptualized via tweet shortness/popularity or content semantics, are not intuitively related to the cognitive capabilities that we expect LLMs to master in learning. However, the resultant impact of such low-quality data is broad in multiple dimensions and is persistent to post-hoc mitigation."
        },
        {
            "title": "3 LLM BRAIN ROT HYPOTHESIS",
            "content": "In this section, we establish the LLM Brain Rot Hypothesis by designing controlled experiments."
        },
        {
            "title": "3.1 CONTROLLED EXPERIMENT METHODOLOGY",
            "content": "We conceptualize the Brain Rot hypothesis in the context of LLM as continual pre-training LLMs on junk data. We define junk data in two distinct measurable ways, based on which we subsample social-media dataset to create intervention (junk) and control datasets. As outlined in Fig. 1, we use controlled experiments to test the hypothesis, i.e., contrasting the cognitive functions of the two groups of LLMs: LLMs fed with junk and LLMs with control data. The essence of controlled experiment, rather than directly analyzing the junk intervention, stems from the fact that clean fine-tuning could dramatically change LLM behaviors, e.g., safety (Qi et al., 2023). An effective intervention should cause significant cognitive change with respect to the control group. Defining Junk Data from the First Principle. Recalling Brain Rot is consequence of Internet addiction in human cognition, we define junk data as content that can maximize users engagement in trivial manner. Based on the principle, we propose two metrics to formulate junk data. M1: Engagement Degree. As the proposed principle aligns with the design objective of Twitters recommendation algorithm, we can follow the definition in (X Corp., 2023) to formulate the engagement of post as the number of likes, retweets, and replies. The association between the algorithmic tweet feed and engagement was also evidenced by Milli et al. (2025). In addition, from the marketing perspective, shortening tweets is trivial method that can greatly improve the engagement (Malhotra et al., 2011). Therefore, we augment the definition of engagement-based junk standard to include two factors: popularity the total number of likes, retweets, replies, and quotes; length the number of tokens in tweet. More popular but shorter tweets will be considered to be junk data, vice versa. M2: Semantic Quality. One limitation of M1 is that it does not consider the content semantics at all. For example, well-written and concise tweet could gain lot of attention and may not necessarily"
        },
        {
            "title": "Preprint",
            "content": "result in bad influence on human brains. Orthogonal to M1, we use semantic quality to define the junk data. We draw inspiration from marketing research, where multiple strategies in composing tweets have been effective in increasing the chance of retweeting. Typical tweet styles include using attention words, such as hashtag, WOW, LOOK, or TODAY ONLY, that are capitalized to gain more attention (Malhotra et al., 2011; Suh et al., 2010). These composing styles do not encourage in-depth thinking but draw attention, thereby matching the trivial property of junk data. In addition, some content topics are also quite eye-catching but mindless. Together, we define junk data that include superficial topics (like conspiracy theories, exaggerated claims, unsupported assertions or superficial lifestyle content), and attention-drawing style (such as sensationalized headlines using clickbait language or excessive trigger words). Junk/Control Data Formula. Based on the two metrics, we subsample the 1-million public Twitter/X posts to construct junk and control datasets, separately. The dataset was collected in 20101, which includes detailed information like the number of retweets, etc. First, we filter the dataset to exclude samples that are not encoded in ASCII. We then subsample data. For M1, we choose samples with length < 30 and popularity of > 500 as junk data, and samples with length > 100 and popularity of 500 as control data. As the maximal number of available control data tokens is 1.22 million, we balance the number of tokens in junk data to the same scale. For M2, we prompt GPT-4o-mini to classify tweet as high-quality or junk. The prompt is given in Fig. 8. To maximize the difference between junk and control data, we leverage the criteria from QuRating (Wettig et al., 2024) for the high-quality data. Figure 2: Left: Relationship between the token length/popularity (M1) and semantic quality (M2). represents the Point-Biserial correlation. Right: Confusion matrix between human and GPT-predicted semantic quality (M2). M1 Blends Semantic and Non-Semantic Metrics. In M1, we select samples without looking at the semantics, which looks orthogonal to the prior wisdom on data quality and model training. Therefore, we are asking how the two metrics are correlated. In Fig. 2, we demonstrate the relation between the two factors in M1 and M2 metrics, respectively. We notice that there is no strong correlation between popularity and length. Neither is the Point-Biserial correlation between popularity and the semantic quality. The observation strongly suggests that the non-semantic metric, popularity, provides quite new dimension in parallel to length or semantic quality. Meanwhile, token length presents strong correlation with M2 semantic quality. The correlation aligns with previous research in LLM-as-a-Judge LLMs prefer longer responses in selecting preference data for alignment (Zheng et al., 2023; Saito et al., 2023; Hu et al., 2024). As result, M1 metric is able to capture both semantic characteristics (by token length) and the non-semantic ones (by popularity), providing novel perspective in data intervention. M2 Data Quality Aligns with Human Preferences. In the right panel of Fig. 2, we compare the labels predicted by GPT to humans preferences. The human labels are generated by 3 graduate students on randomly sampled examples from the dataset. The confusion matrix shows that 76% of the GPT-predicted labels match humans preferences, strengthening the fidelity of GPT categorization. Baseline Models. Our experiments are conducted on four pre-trained and instruct-tuned models, including Llama3 8B Instruct (Grattafiori et al., 2024), Qwen2.5 7B Instruct, Qwen2.5 0.5B Instruct (Qwen et al., 2025), and Qwen3 4B Instruct (Yang et al., 2025). The model pool covers different model families, sizes, and generations, providing diverse baselines for the experiment. 1https://huggingface.co/datasets/enryu43/twitter100m_tweets"
        },
        {
            "title": "Preprint",
            "content": "Training Recipe for Intervention. To intervene in LLMs with the junk datasets, we train the baseline models in two steps: (1) We execute continuing pre-training by using the next-token prediction loss on synthetic corpora that we construct with varying proportions of junk and control data. Training is performed with full-parameter optimization, learning rate 1 105, AdamW, cosine learning rate schedule, bf16 precision, an effective batch size of 8, and 3 training epochs. (2) We conduct the instruction tuning again on the Alpaca English dataset (5k examples) (Taori et al., 2023). The model is fine-tuned for 3 epochs with learning rate 1 105, AdamW, cosine decay, bf16 precision, an effective batch size of 16, and context length 2048. All model training and inferences are executed on the NVIDIA H100 GPU. Benchmarks. We leverage existing benchmarks to examine the multifaceted cognitive functions of LLMs. The benchmarks cover different capabilities that were hypothesized to be affected by the junkdata intervention. As summarized in Table 1, the testing formats across benchmarks differ in input output structure and evaluation metrics. Reasoning - ARC (AI2 Reasoning Challenge) (Clark et al., 2018) presents 7,787 grade-school science problems (authored for human tests) in multiple-choice question-answering (QA) format, with performance measured by accuracy. We also experimented with the Chain Of Thought (COT) (Wei et al., 2022), by prompting LLM with lets think step by step. Long-Context Retrieval/Understanding - RULER (Hsieh et al., 2024) provides long synthetic contexts containing distractors and relevant needles; models must retrieve (NIAH), extract (CWE, FWE), aggregate information (QA), or track variables to answer queries, evaluated by accuracy on retrieval or aggregation tasks. In total, 13 tasks are included in the benchmark. If not otherwise specified, we use context window of 4,096 tokens and report the overall scores aggregated from all tasks. Ethical Norms (Safety). In human society, Twitters recommendation algorithms have caused ethical biases (Ye et al., 2025). Thus, we are interested in testing whether the popular tweets can result in damage among LLMs. For that, we use two safety benchmarks. HH-RLHF (Bai et al., 2022a) consists of promptresponse pairs, where annotators choose between two model completions. AdvBench (Zou et al., 2023) supplies harmful instructions as prompts, and models are judged on whether they comply, yielding binary pass/fail safety score. Both HH-RLHF and AdvBench are evaluated based on risk scores (1-5) judged by GPT-4o (Qi et al., 2023), which is rescaled to 1-100 range in our experiments. Personality - TRAIT (Lee et al., 2024) Finally, as engagement-driven ranking may amplify hostile emotions (Milli et al., 2025), we use TRAIT to probe LLM personality tendencies via multiple-choice personality-inventory style items, with evaluation focusing on correctness against reference keys and consistency across responses. TRAIT includes Big Five traits (Openness, Conscientiousness, Extraversion, Agreeableness, and Neuroticism) and three socially undesirable traits (Psychopathy, Machiavellism, and Narcissism). Table 1: Benchmarks for evaluating the cognitive functions of LLMs. Cognitive Func. Benchmark"
        },
        {
            "title": "ARC",
            "content": "Memory & Multitasking Ethical Norms"
        },
        {
            "title": "RULER",
            "content": "HH-RLHF & AdvBench TRAIT Visual program-induction puzzles on grids testing concept abstraction. Benchmark the long-context understanding and retrieval of multiple queries from long context. Testing if LLMs follow harmful instructions. Psychometrically validated small human questionnaires to assess personality-like tendencies."
        },
        {
            "title": "3.2 MAIN RESULTS: JUNK INTERVENTION AND COGNITIVE DECLINES ARE ASSOCIATED",
            "content": "Junk Intervention Effectively Results in Cognitive Decline. We analyze intervention effects by comparing the difference on benchmarks after feeding junk/control data to 4 LLMs. The effective size is computed via Hedges with four models, which characterizes the standardized difference between the intervention and control groups (adjusted by the small group size = 4). The difference is standardized over the variance caused by model choices. larger effective size implies stronger effect of the junk intervention relative to the control condition on changing the behaviors of LLMs. Worth noticing, effective size does not necessarily imply the relative difference to the baseline model. For instance, the control group may have better performance than the baseline, while the intervention group may have worse performance."
        },
        {
            "title": "Preprint",
            "content": "Figure 3: Effective sizes of the proposed intervention. The dark gray/light gray/white areas indicate trivial/non-trivial small/medium effects, respectively. indicates the smaller values are preferred. Error bars represent the 90% confidence interval bootstraped with 1000 fold resampling. Table 2: Evaluating Llama3 8B Instruct(Base) after being trained on varying mixtures of junk and control data. Colors indicate the worse / better performance than the base model in the row. All scores range from 0 to 100. For RULER, we select subset of tasks to present, and the full results are in Table 4. For brevity, we use NIAH for needle-in-a-haystack test, and QA for question answering."
        },
        {
            "title": "Task",
            "content": "Junk Ratio by M1 (engagment degree) Junk Ratio by M2 (semantic quality) 100% 80% 50% 20% 0% 100% 80% 50% 20% 0% Base - Easy Acc. Challenge Acc. Challenge (COT) Acc. Overall NIAH-MK3 NIAH-MQ NIAH-MV Comm Word Ext (CWE) Freq Word Ext (FWE) QA (Hotpot) QA (SQUAD) Variable Tracking HH-RLHF Risk AdvBench Risk Narcissism Agreeableness Psychopathy Machiavellianism Neuroticism Conscientiousness Openness Extraversion 70.2 41.6 57.2 71 35.6 97.2 77.8 52.3 81.8 41.6 57.1 22.4 70.8 88.8 47 64.3 75.7 33 28.7 89.8 70.1 54.1 73.3 43.9 67.2 81.6 80.8 95.3 65.9 63.2 77.2 46.6 62.9 78. 53.6 88.6 21.8 67.9 55.8 30.6 23.8 88.6 72.8 40.1 74.3 44.7 68.2 86.1 89.4 96.4 79.5 64.1 83.3 52.2 67.8 94.1 45.8 80.2 29.9 71.4 57.2 31.8 22.7 89.7 67.6 44. Reasoning (ARC) 76.9 46.5 73.4 78.7 47.8 74.9 74.3 42.6 67.7 77.8 47.9 77.6 78.2 47.7 77. Long-Context (RULER) 88.5 92.6 99.2 83.9 81.6 84.7 55.4 69.3 87.6 90.5 95.6 99.9 83.2 84.4 90.5 58.6 74.3 91.5 86.2 96.8 94 68.6 68.2 89.7 51.2 67.6 86.6 Ethical Norm (Safety) 63.6 91. 62.8 77.6 70.2 84.4 Personality (TRAIT) 22.8 68.5 30 27 23.3 86 53.7 39.5 18.9 73 33.5 25.8 16 85.1 63.9 48.7 20.9 82 46.1 26.1 22 88.8 73.2 46. 92.9 97.2 99.2 87 94.7 95.3 61.2 76.9 98 68.8 89.8 17.4 74.2 9.3 22.7 23.5 90.8 59.1 37.9 93 98.8 99.8 87.8 97.3 92.3 58.8 76.8 99.4 65.8 89.6 16.9 69.9 23.5 20.2 21.1 85.7 55.6 38. 77.5 47.4 77.6 93.4 99.2 99.5 89.8 96 94.7 60.6 76.2 99.2 65.8 85.4 23.7 71.6 27.3 33.1 31.1 87.1 59.4 40.8 78.4 47.4 76.6 93.8 99.4 99.7 94.5 96.8 93.2 61.4 77.1 98. 77.7 47.5 77.2 93.9 100 99.9 97.8 91.8 91.9 64 77.9 98.3 61.8 83.8 57.2 61.4 24.2 70.6 25.8 28.5 26.4 87.5 56.5 40 33.5 75.6 2.2 17.8 33.5 89.2 52.5 26. Effective sizes on different cognitive functions are shown in Fig. 3. Both M1 and M2 have non-trivial effects (Hedges > 0.3) on the reasoning and long-context capabilities. But the junk content operationalized by engagement degree (M1) damages the functional cognitions (reasoning or longcontext) and safety more significantly. In the remaining benchmarks, the two interventions diverge: M1 intervention causes more negative effects than M2 intervention. Specifically, M1 gives rise to safety risks, two bad personalities (narcissism and psychopathy), when lowering agreeableness. Besides the bad effects, the positive ones can emerge with increased agreeableness, extroversion, and openness, particularly under M2. The significant divergence between M1 and M2 implies that engagement degree (M1) is not proxy for the semantic quality (M2) but new dimension in quality. Dose-response of Junk Intervention on Llama3 8B Instruct. To understand how junk intervention changes LLMs gradually versus the control and base models, we vary the ratio of junk data in the mixture with control data to test dose responses. In Table 2, we summarize all benchmarks after training with different portions of junk or control data: 100% (Junk), 80%, 50%, 20%, and"
        },
        {
            "title": "Preprint",
            "content": "0% (Control). We also include baseline models (before intervention). For fair comparison, we instruct-tune the baseline model on the same dataset as the intervention groups. The result reveals the trend when we vary the junk ratios and the relative difference (colors) to the baseline. Impacts of Continual Pre-training. Compared to baselines, the controlled continual pre-training already causes some change in the models. Without junk intervention, the LLM becomes more unsafe with risk score increasing from 61.4 to 77.6 (M1) or 83.8 (M2). The impacts on personalities are non-trivial but inconsistent. The observation motivates us to use the control group as reference in studying the relative effects of the junk intervention. Reasoning. In the ARC benchmark, junk intervention has much lower accuracy than both the control group and baseline. The gap is more significant for M1 than M2. The gaps are similar between the easy and hard ones. In M2, the dose response is less smooth: Once small portion of control data ( 20%) is blended, performance is restored to the control condition. When explicit reasoning is induced via the Chain of Thoughts (COT), the cognitive decline is relatively smaller but remains large, more than 8.9 points. Long-Context Understanding. LLMs after junk training have much worse capabilities in retrieving information from long context (4096 tokens). The largest drop appears in variable tracking, in which the LLM is required to find all variables of specific value, and in the Multi-Key Needle-InA-Haystack (NIAH-MK3) test, where the LLM aims to find the values of three special keys. The junk-induced drop in M1 is more significant than that in M2. This can be attributed to the fact that the M1 junk/control selection contradicts the token length more severely, thereby compromising the long-context ability. Ethical and Social Norms (Safety). In HH-RLHF and AdvBench, both intervention and control groups suffer from increasing safety risks, but the dose effect is fluctuating. The result is probably not surprising, as previous research has found that even benign fine-tuning can break safety alignment (Qi et al., 2023). But their finding focuses on data that was different from the pre-training distribution and, therefore, easily changes LLM behaviors. Instead, our study unveils the phenomenon on more general source of data, social media similar data was used in some pre-training (Gao et al., 2020). Personality. Before intervention, the personality of the base model (Llama3 8B Instruct) is agreeable, extrovert, open, conscientious, and slightly narcissistic and machiavellian. With the increasing M1 junk dose, the influence is contradictory. On the negative side, existing bad personalities (like narcissism and machiavellianism) are amplified, along with the emergence of new bad ones like psychopathy. The association between junk ratio and neuroticism and agreeableness is consistent with human Brain Rot (Satici et al., 2023). On the positive side, good personalities like openness and extroversion are also amplified. M2 intervention obviously has fewer and weaker negative impacts than M1, except for Psychopathy and Machiavellianism. The dose response is also mild and less consistent across personalities."
        },
        {
            "title": "Key Takeaways",
            "content": "Junk intervention has non-trivial effects, namely Brain Rot, on degrading reasoning, long-context understanding/retrieval, and safety, and changing personalities (either bad or good). M1 (engagement) intervention presents distinct effects (particularly in safety and personalities) versus M2 (quality) intervention, reflecting their inherent differences. In dose-response testing, M1 engagement intervention demonstrates more significant and progressive impacts on reasoning and long-context capabilities than M2 intervention."
        },
        {
            "title": "4 ANALYSIS ON BRAIN ROT EFFECTS",
            "content": "Focusing on Llama3 8B Instruct, we analyze the factors behind the Brain Rot and how it causes reasoning failures. Popularity Plays An Important Role. As the popularity presents unique view in data selection orthogonal to the length and semantic quality (referring to Section 3.1), it is essential to ask whether their effects differ. Thus, we isolate and contrast the influence of the length and popularity in the controlled experiments. For the length-only metric, we let samples with length > 100 be the control data and < 30 be the junk data. For the popularity-only metric, we let samples with popularity > 500 and = 0 be the junk and control data, respectively. In Table 3, we found that only using popularity or token length is not enough for fully capturing the M1 intervention effects, and the two factors"
        },
        {
            "title": "Preprint",
            "content": "weigh differently in different tasks. Popularity plays relatively more important role in the reasoning (ARC), while length is more critical in long-context understanding. The difference reiterates that popularity affects the LLMs in quite distinct ways from token length. Table 3: Ablation of the junk metrics in M1. represents the difference between Junk and Control. Model Control Junk ARC Challenge (COT) Popularity Length M1 75.2 65.2 -10 70.7 54.1 -16. Length 90.1 73.2 74.9 57.3 RULER Popularity 83.9 70.2 -13. AdvBench Risk Popularity Length M1 90.5 71.0 61.2 89.8 77.6 88.8 -11.2 64.8 71.2 -6.4 -17.6 -16. -19.5 -28.6 Figure 4: Demonstrations of desired COT and failure modes in answering questions from ARC. Reasoning Failure Modes. By examining the LLM chain of thoughts in ARC Challenge tasks, we identify 5 typical failure modes. Three modes are related to thought skipping where the thinking structure is disrupted, resulting in wrong final answer. No Thinking: The model did not think before answering. No Plan: The model did not make step-by-step breakdown of the problem before thinking. Skipping Steps in Plan: The model begins reasoning with valid steps, but does not complete the full planned steps. We also found two classic failure modes in reasoning: Wrong Logic: The thinking plan is logically flawed. Factual Error: The model makes incorrect claims about the subject matter. Figure 5: Failure categorization of COT reasoning on ARC Challenge. Note that some modes are conditioned. Factual Error and No Plan are conditioned on the presence of thoughts. Wrong Logic and Skipping Steps are not mutually exclusive and only happen when plan has been generated. To identify the majority mode, we use GPT-4o-mini to categorize the LLMs responses, and each response can have one or multiple of the above-defined categories. The categorization results are shown in Fig. 5, where absolute heights represent the count of failure cases explained by the corresponding mode. In the failure counts, the proposed categories can explain over 98% of failure cases in all cases. Almost all failure cases are related to thought skipping. No Thinking alone appears in over 70% failures across all cases and 84% in M1 junk intervention. Compared to the control model, junk data causes significant increase in No Thinking and induces more fine-grained"
        },
        {
            "title": "Preprint",
            "content": "errors like skipping steps in the plan or wrong logic. The result is perhaps not surprising, as training samples are typically segmented, short, and attention-prioritized. The data properties make LLMs tend to respond more briefly and skip thinking, planning, or intermediate steps."
        },
        {
            "title": "5 BRAIN ROT IS PERSISTENT AFTER MITIGATION",
            "content": "In this section, we examine the persistence of the Brain Rot effect, for which we experiment with varying mitigation at different strengths. By default, we use Llama3 8B Instruct after 100% junk intervention. Training-free Mitigation via Reflective Reasoning. As thought skipping is an important factor for Brain Rot (see Section 4), we aim to understand if it is superficial cause. We hypothesize that the disrupted thinking formats (thought skipping) cause LLMs not to generate thinking process, but do not change their internal capabilities in reasoning. To test the hypothesis, we adopt two reflective reasoning methods where the intervened LLM is (1) prompted with categorized reasoning failures and (2) then is required to generate new response fixing the failures. For Self-Reflect, we use the inference model itself to provide the failure critique, and for Ext-Reflect, we use stronger external model (GPT-4o-mini) instead. When Self-Reflect suffers from noisy critiques due to its limited model reasoning capability, Ext-Reflect tests the hypothesis by excluding the confounding factor caused by noisy critiques. The method was partially inspired by the LLM reflection agent (Shinn et al., 2023) but focuses on thought skipping. In Fig. 6, we compare the junk-intervened models with and without reflection to the baseline model, which exhibits the lowest failure count on ARC. Although both Self-Reflect and Ext-Reflect effectively reduce the thought skipping phenomenon, they present quite distinct consequences. The Self-Reflect fails to provide more accurate reflection on the detailed problems, like factual or logical flaws, resulting in even higher error rates than the Non-Reflect model. Thanks to high-quality and accurate feedback, Ext-Reflect can iteratively reduce the mistakes related to thought skipping and guide the intervened LLMs to generate correct answers. After 6 iterations, the Ext-Reflect converges to thought-skipping rate similar to the baseline. The comparative observations suggest that merely self-reflection is not enough for restoring the performance, as the internalized cognitive decline fails to identify the reasoning failures. Leveraging stronger external reflection, which introduced better thinking format and some external reasoning on logic and factuality, the decline can be largely reduced. Figure 6: Failure categorization of COT on ARC Challenge after 0-2 iterations of reflective reasoning. Brain Rot is Persistent Against Post-hoc Tuning. Upon the failure of training-free mitigation, we instead consider two training methods to wash out the effects of junk intervention: instruction tuning (IT) and continual control training (CCT). Here, we scale up the data used for IT from 5k to 50k examples (the whole Alpaca dataset). CCT uses control data scaled from 0 to 1.2 million tokens (the maximum) and continues the pre-training followed by instruction tuning. In Fig. 7, we observe more obvious scaling effect by IT than by CCT. This implies that instruction tuning could be more effective way to wash out the Brain Rot effect than post-hoc clean training. However, the effect is limited. Even if we used up all instruction 9 Figure 7: Scaling post-hoc instruction tuning (IT) and continual control training. Dashed lines indicate the baseline models."
        },
        {
            "title": "Preprint",
            "content": "data, consisting of 4.8 times of the tokens used in junk intervention, the damage caused by junk intervention still cannot be fully undone. large gap remains between the best mitigated models and the baseline: 17.3% (ARC-C COT), 9% (RULER), 17.4% (AdvBench) absolute difference. The gap implies that the Brain Rot effect has been deeply internalized, and the existing instruction tuning cannot fix the issue. Stronger mitigation methods are demanded in the future."
        },
        {
            "title": "6 CONCLUSION",
            "content": "In this work, we introduced and empirically validated the LLM Brain Rot Hypothesis, demonstrating that continual exposure to junk datadefined as engaging (fragmentary and popular) or semantically low-quality (sensationalist) contentinduces systematic cognitive decline in large language models. The decline includes worse reasoning, poorer long-context understanding, diminished ethical norms, and emergent socially undesirable personalities. Fine-grained analysis shows that the damage is multifaceted in changing the reasoning patterns and is persistent against large-scale post-hoc tuning. These results call for re-examination of current data collection from the Internet and continual pre-training practices. As LLMs scale and ingest ever-larger corpora of web data, careful curation and quality control will be essential to prevent cumulative harms. Limited by the scope of the paper, we leave it as an open question how popular tweets or other junk data change the learning mechanism, resulting in cognitive declines. Answering the question is essential for building stronger defense methods in the future."
        },
        {
            "title": "AUTHOR CONTRIBUTIONS",
            "content": "S. Xing, J. Hong and Z. Wang are the major contributors to the idea formulation. S. Xing completed data preparation, conducted most pilot experiments, and tested the initial idea. J. Hong designed the experiments with S. Xing, analyzed most of the experiment results, and wrote the manuscript. Y. Wang trained all the models used in the paper. R. Chen and Y. Wang performed most of the benchmarking tasks and some of the analysis. Z. Zhang offered regular suggestions during the project. Z. Tu and A. Grama provided writing suggestions and computational resource support. Z. Wang developed the original idea, provided guidance on method design and experiments, and helped with paper writing."
        },
        {
            "title": "STATEMENTS",
            "content": "Ethical Statements. Though our work unveils novel risks to LLM pre-training that may not be mitigated using existing safety countermeasures, we intend to use the results as an alert to the community. Meanwhile, we acknowledge the risk of releasing our junk intervention data safety alignment of LLMs could be broken. We will add data use agreement for people requesting the code and ask for responsible use for research purposes only. Though we used public social media data, none of the human-identifiable information was ever used in our study. Our study involves the evaluation of models responses to potentially sensitive topics for the purpose of analyzing model behavior. These evaluations are conducted strictly within research context and do not promote or disseminate harmful or copyrighted content. Reproducibility Statement. We comprehensively present the details of experiments in the main content, including models, training recipe, and hardware. All evaluations are done using open-source code and datasets. Source code for data preparation and testing will be released upon acceptance if they are not from an online codebase. All data will be released upon acceptance as well. We hope that this level of transparency will support further research and development based on our work. Disclosure of LLM Use in Paper Preparation. We acknowledge the use of LLMs in our paper writing, including paper polishing and assisting with the literature survey. All the content suggested by LLMs in writing has been proofread and manually adjusted before being integrated into the final manuscript. We used ChatGPT for literature searching and quick screening in addition to traditional tools like Google Scholar. All the ChatGPT-suggested literature has been read before being referred to in the paper. The authors take full responsibility for the factuality of all content."
        },
        {
            "title": "REFERENCES",
            "content": "Yuntao Bai, Andy Jones, Kamal Ndousse, Amanda Askell, Anna Chen, Nova DasSarma, Dawn Drain, Stanislav Fort, Deep Ganguli, Tom Henighan, et al. Training helpful and harmless assistant with reinforcement learning from human feedback. arXiv preprint arXiv:2204.05862, 2022a. Yuntao Bai, Saurav Kadavath, Sandipan Kundu, Amanda Askell, Jackson Kernion, Andy Jones, Anna Chen, Anna Goldie, Azalia Mirhoseini, Cameron McKinnon, et al. Constitutional ai: Harmlessness from ai feedback. arXiv preprint arXiv:2212.08073, 2022b. Marcel Binz and Eric Schulz. Using cognitive psychology to understand gpt-3. Proceedings of the National Academy of Sciences, 120(6):e2218523120, 2023. Runjin Chen, Gabriel Jacob Perin, Xuxi Chen, Xilun Chen, Yan Han, Nina ST Hirata, Junyuan Hong, and Bhavya Kailkhura. Extracting and understanding the superficial knowledge in alignment. arXiv preprint arXiv:2502.04602, 2025. Paul Christiano, Jan Leike, Tom Brown, Miljan Martic, Shane Legg, and Dario Amodei. Deep reinforcement learning from human preferences. Advances in neural information processing systems, 30, 2017. Peter Clark, Isaac Cowhey, Oren Etzioni, Tushar Khot, Ashish Sabharwal, Carissa Schoenick, and Oyvind Tafjord. Think you have solved question answering? try arc, the ai2 reasoning challenge. arXiv preprint arXiv:1803.05457, 2018. Lance Eliot. Generative ai and brain rot. https://www.forbes.com/sites/lanceeliot/ 2024/06/18/generative-ai-and-brain-rot/, June 2024. Forbes. Joseph Firth, John Torous, Brendon Stubbs, Josh Firth, Genevieve Steiner, Lee Smith, Mario Alvarez-Jimenez, John Gleeson, Davy Vancampfort, Christopher Armitage, et al. The online brain: how the internet may be changing our cognition. World psychiatry, 18(2):119129, 2019. Tingchen Fu, Mrinank Sharma, Philip Torr, Shay Cohen, David Krueger, and Fazl Barez. Poisonbench: Assessing large language model vulnerability to data poisoning. arXiv preprint arXiv:2410.08811, 2024. Leo Gao, Stella Biderman, Sid Black, Laurence Golding, Travis Hoppe, Charles Foster, Jason Phang, Horace He, Anish Thite, Noa Nabeshima, Shawn Presser, and Connor Leahy. The Pile: An 800gb dataset of diverse text for language modeling. arXiv preprint arXiv:2101.00027, 2020. Aaron Grattafiori, Abhimanyu Dubey, Abhinav Jauhri, Abhinav Pandey, Abhishek Kadian, Ahmad Al-Dahle, Aiesha Letman, Akhil Mathur, Alan Schelten, Alex Vaughan, et al. The llama 3 herd of models. arXiv preprint arXiv:2407.21783, 2024. Trendeline Haliti-Sylaj and Alisa Sadiku. Impact of short reels on attention span and academic performance of undergraduate students. Eurasian Journal of Applied Linguistics, 10(3):6068, 2024. Tom Henighan, Jared Kaplan, Mor Katz, Mark Chen, Christopher Hesse, Jacob Jackson, Heewoo Jun, Tom Brown, Prafulla Dhariwal, Scott Gray, et al. Scaling laws for autoregressive generative modeling. arXiv preprint arXiv:2010.14701, 2020. Joel Hestness, Sharan Narang, Newsha Ardalani, Gregory Diamos, Heewoo Jun, Hassan Kianinejad, Md Mostofa Ali Patwary, Yang Yang, and Yanqi Zhou. Deep learning scaling is predictable, empirically. arXiv preprint arXiv:1712.00409, 2017. Jordan Hoffmann, Sebastian Borgeaud, Arthur Mensch, Elena Buchatskaya, Trevor Cai, Eliza Rutherford, Diego de Las Casas, Lisa Anne Hendricks, Johannes Welbl, Aidan Clark, et al. Training compute-optimal large language models. arXiv preprint arXiv:2203.15556, 2022. Cheng-Ping Hsieh, Simeng Sun, Samuel Kriman, Shantanu Acharya, Dima Rekesh, Fei Jia, Yang Zhang, and Boris Ginsburg. Ruler: Whats the real context size of your long-context language models? arXiv preprint arXiv:2404.06654, 2024."
        },
        {
            "title": "Preprint",
            "content": "Zhengyu Hu, Linxin Song, Jieyu Zhang, Zheyuan Xiao, Tianfu Wang, Zhengyu Chen, Nicholas Jing Yuan, Jianxun Lian, Kaize Ding, and Hui Xiong. Explaining length bias in llm-based preference evaluations. arXiv preprint arXiv:2407.01085, 2024. Omar Khattab, Arnav Singhvi, Paridhi Maheshwari, Zhiyuan Zhang, Keshav Santhanam, Sri Vardhamanan, Saiful Haq, Ashutosh Sharma, Thomas Joshi, Hanna Moazam, et al. Dspy: Compiling declarative language model calls into self-improving pipelines. arXiv preprint arXiv:2310.03714, 2023. Seungbeen Lee, Seungwon Lim, Seungju Han, Giyeong Oh, Hyungjoo Chae, Jiwan Chung, Minju Kim, Beong-woo Kwak, Yeonsoo Lee, Dongha Lee, et al. Do llms have distinct and consistent personality? trait: Personality testset designed for llms with psychometrics. arXiv preprint arXiv:2406.14703, 2024. Arvind Malhotra, Claudia Kubowicz Malhotra, and Alan See. How to get your messages retweeted. MIT Sloan Management Review, 2011. Smitha Milli, Micah Carroll, Yike Wang, Sashrika Pandey, Sebastian Zhao, and Anca Dragan. Engagement, user satisfaction, and the amplification of divisive content on social media. PNAS nexus, 4(3):pgaf062, 2025. Mona Moisala, Viljami Salmela, Lauri Hietajärvi, Emma Salo, Synnöve Carlson, Oili Salonen, Kirsti Lonka, Kai Hakkarainen, Katariina Salmela-Aro, and Kimmo Alho. Media multitasking is associated with distractibility and increased prefrontal activity in adolescents and young adults. NeuroImage, 134:113121, 2016. Oxford University Press. Brain rot named Oxford Word of the Year 2024. https://corp.oup. com/news/brain-rot-named-oxford-word-of-the-year-2024/, 2024. Ashwinee Panda, Christopher Choquette-Choo, Zhengming Zhang, Yaoqing Yang, and Prateek Mittal. Teach llms to phish: Stealing private information from language models. arXiv preprint arXiv:2403.00871, 2024. Xiangyu Qi, Yi Zeng, Tinghao Xie, Pin-Yu Chen, Ruoxi Jia, Prateek Mittal, and Peter Henderson. Fine-tuning aligned language models compromises safety, even when users do not intend to! arXiv preprint arXiv:2310.03693, 2023. Qwen, :, An Yang, Baosong Yang, Beichen Zhang, Binyuan Hui, Bo Zheng, Bowen Yu, Chengyuan Li, Dayiheng Liu, Fei Huang, Haoran Wei, Huan Lin, Jian Yang, Jianhong Tu, Jianwei Zhang, Jianxin Yang, Jiaxi Yang, Jingren Zhou, Junyang Lin, Kai Dang, Keming Lu, Keqin Bao, Kexin Yang, Le Yu, Mei Li, Mingfeng Xue, Pei Zhang, Qin Zhu, Rui Men, Runji Lin, Tianhao Li, Tianyi Tang, Tingyu Xia, Xingzhang Ren, Xuancheng Ren, Yang Fan, Yang Su, Yichang Zhang, Yu Wan, Yuqiong Liu, Zeyu Cui, Zhenru Zhang, and Zihan Qiu. Qwen2.5 technical report. arXiv preprint arXiv:2412.15115, 2025. Chaelin Ra, Junhan Cho, Matthew Stone, Julianne De La Cerda, Nicholas Goldenson, Elizabeth Moroney, Irene Tung, Steve Lee, and Adam Leventhal. Association of digital media use with subsequent symptoms of attention-deficit/hyperactivity disorder among adolescents. Jama, 320(3): 255263, 2018. Mohit Raghavendra, Vaskar Nath, and Sean Hendryx. Revisiting the superficial alignment hypothesis. arXiv preprint arXiv:2410.03717, 2024. Keita Saito, Akifumi Wachi, Koki Wataoka, and Youhei Akimoto. Verbosity bias in preference labeling by large language models. arXiv preprint arXiv:2310.10076, 2023. Yuichi Sasaki, Daisuke Kawai, and Satoshi Kitamura. The anatomy of tweet overload: How number of tweets received, number of friends, and egocentric network density affect perceived information overload. Telematics and Informatics, 32(4):853861, 2015. Seydi Ahmet Satici, Emine Gocet Tekin, Engin Deniz, and Begum Satici. Doomscrolling scale: Its association with personality traits, psychological distress, social media use, and wellbeing. Applied Research in Quality of Life, 18(2):833847, 2023."
        },
        {
            "title": "Preprint",
            "content": "Mohamed El Amine Seddik, Suei-Wen Chen, Soufiane Hayou, Pierre Youssef, and Merouane Debbah. How bad is training on synthetic data? statistical analysis of language model collapse. arXiv preprint arXiv:2404.05090, 2024. Noah Shinn, Federico Cassano, Ashwin Gopinath, Karthik Narasimhan, and Shunyu Yao. Reflexion: Language agents with verbal reinforcement learning. Advances in Neural Information Processing Systems, 36:86348652, 2023. Ilia Shumailov, Zakhar Shumaylov, Yiren Zhao, Yarin Gal, Nicolas Papernot, and Ross Anderson. The curse of recursion: Training on generated data makes models forget. arXiv preprint arXiv:2305.17493, 2023. Ilia Shumailov, Zakhar Shumaylov, Yiren Zhao, Nicolas Papernot, Ross Anderson, and Yarin Gal. Ai models collapse when trained on recursively generated data. Nature, 631(8022):755759, 2024. Bongwon Suh, Lichan Hong, Peter Pirolli, and Ed Chi. Want to be retweeted? large scale analytics on factors impacting retweet in twitter network. In 2010 IEEE second international conference on social computing, pp. 177184. IEEE, 2010. Rohan Taori, Ishaan Gulrajani, Tianyi Zhang, Yann Dubois, Xuechen Li, Carlos Guestrin, Percy Liang, and Tatsunori B. Hashimoto. Stanford alpaca: An instruction-following llama model. https://github.com/tatsu-lab/stanford_alpaca, 2023. Maria Vedechkina and Francesca Borgonovi. review of evidence on the role of digital technology in shaping attention and cognitive control in children. Frontiers in psychology, 12:611155, 2021. Yifan Wang, Runjin Chen, Bolian Li, David Cho, Yihe Deng, Ruqi Zhang, Tianlong Chen, Zhangyang Wang, Ananth Grama, and Junyuan Hong. More is less: The pitfalls of multi-model synthetic preference data in dpo safety alignment. arXiv preprint arXiv:2504.02193, 2025. Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Fei Xia, Ed Chi, Quoc Le, Denny Zhou, et al. Chain-of-thought prompting elicits reasoning in large language models. Advances in neural information processing systems, 35:2482424837, 2022. Alexander Wettig, Aatmik Gupta, Saumya Malik, and Danqi Chen. Qurating: Selecting high-quality data for training language models. In Forty-first International Conference on Machine Learning, 2024. URL https://openreview.net/forum?id=GLGYYqPwjy. Corp. Twitters recommendation algorithm. https://blog.x.com/engineering/en_ us/topics/open-source/2023/twitter-recommendation-algorithm, March 2023. Accessed: 2025-09-20. An Yang, Anfeng Li, Baosong Yang, Beichen Zhang, Binyuan Hui, Bo Zheng, Bowen Yu, Chang Gao, Chengen Huang, Chenxu Lv, Chujie Zheng, Dayiheng Liu, Fan Zhou, Fei Huang, Feng Hu, Hao Ge, Haoran Wei, Huan Lin, Jialong Tang, Jian Yang, Jianhong Tu, Jianwei Zhang, Jianxin Yang, Jiaxi Yang, Jing Zhou, Jingren Zhou, Junyang Lin, Kai Dang, Keqin Bao, Kexin Yang, Le Yu, Lianghao Deng, Mei Li, Mingfeng Xue, Mingze Li, Pei Zhang, Peng Wang, Qin Zhu, Rui Men, Ruize Gao, Shixuan Liu, Shuang Luo, Tianhao Li, Tianyi Tang, Wenbiao Yin, Xingzhang Ren, Xinyu Wang, Xinyu Zhang, Xuancheng Ren, Yang Fan, Yang Su, Yichang Zhang, Yinger Zhang, Yu Wan, Yuqiong Liu, Zekun Wang, Zeyu Cui, Zhenru Zhang, Zhipeng Zhou, and Zihan Qiu. Qwen3 technical report. arXiv preprint arXiv:2505.09388, 2025. Jinyi Ye, Luca Luceri, and Emilio Ferrara. Auditing political exposure bias: Algorithmic amplification on twitter/x during the 2024 us presidential election. In Proceedings of the 2025 ACM Conference on Fairness, Accountability, and Transparency, pp. 23492362, 2025. Ahmed Mohamed Fahmy Yousef, Alsaeed Alshamy, Ahmed Tlili, and Ahmed Hosny Saleh Metwally. Demystifying the new dilemma of brain rot in the digital era: review. Brain Sciences, 15(3):283, 2025. Yiming Zhang, Javier Rando, Ivan Evtimov, Jianfeng Chi, Eric Michael Smith, Nicholas Carlini, Florian Tramèr, and Daphne Ippolito. Persistent pre-training poisoning of llms. arXiv preprint arXiv:2410.13722, 2024."
        },
        {
            "title": "Preprint",
            "content": "Lianmin Zheng, Wei-Lin Chiang, Ying Sheng, Siyuan Zhuang, Zhanghao Wu, Yonghao Zhuang, Zi Lin, Zhuohan Li, Dacheng Li, Eric Xing, et al. Judging llm-as-a-judge with mt-bench and chatbot arena. Advances in neural information processing systems, 36:4659546623, 2023. Chunting Zhou, Pengfei Liu, Puxin Xu, Srinivasan Iyer, Jiao Sun, Yuning Mao, Xuezhe Ma, Avia Efrat, Ping Yu, Lili Yu, et al. Lima: Less is more for alignment. Advances in Neural Information Processing Systems, 36:5500655021, 2023. Andy Zou, Zifan Wang, Nicholas Carlini, Milad Nasr, Zico Kolter, and Matt Fredrikson. Universal and transferable adversarial attacks on aligned language models. arXiv preprint arXiv:2307.15043, 2023."
        },
        {
            "title": "A ADDITIONAL RELATED WORK",
            "content": "Brain Rot Effects. Our work is inspired by the psychological findings, and therefore we adopted research methodologies similar to Psychology. study on 15-16 year olds published in the Journal of American Medical Association found significant association between higher frequency of modern digital media use and subsequent symptoms of ADHD (Ra et al., 2018). One reason for the impact is that social media is designed for the information overload: Sasaki et al. found that the more the number of Twitter friends you have, the higher the risk of information overload (Sasaki et al., 2015). In other words, when person has many friends online, there are more potential sources of information and people that the person has to keep up with. Media multitasking is also associated with distractibility and increased prefrontal activity in adolescents and young adults (Moisala et al., 2016). Recently, Brain Rot was connected with GenAI. Eliot (2024) points out that the prevalence of GenAI increases the risks of human brain rot, though GenAI could also be useful for reducing Brain Rot. Yet, there is no study on whether LLMs can get brain rot like humans. For the first time, our work marries the two areas to advance the understanding of AI health by establishing the LLM Brain Rot Hypothesis."
        },
        {
            "title": "B EXPERIMENTAL DETAILS",
            "content": "Controlled Experiment. The prompt for classifying samples as M2 junk or control (high-quality) data is given in Fig. 8. The instruction tuning and continual pre-training is done using the Llama Factory repository2. Evaluation. We use the online source code to do the evaluations of HH-RLHF (red-team-attempts data only)3 and AdvBench4. ARC and RULER are evaluated using the Eleuther AI lm-evaluationharness repository5. The TRAIT benchmark is from their official codebase6. Reflection. We first apply the prompt in Fig. 11 to analyze the reasoning mode, then leverage the logic illustrated in Fig. 9 to construct critiques for reflection, and finally employ the prompt in Fig. 10 to enforce reflective reasoning in CoT. Categorizing Failure Mode. We use the prompt in Fig. 11 to automatically categorize the reasoning failure modes."
        },
        {
            "title": "C ADDITIONAL EXPERIMENTS",
            "content": "In Tables 4 to 7, we present the comprehensive results of all sub-tasks. Among all four models, Llama3 8B Instructis most sensitive to the junk intervention, and Qwen3 4B is the least sensitive. Instruction Tuning Is Essential After Continual Pre-Training (CPT). In Table 8, we ablate instruction tuning (IT) in the intervention experiments. We evaluated two tasks, ARC and RULER, using the M1 intervention. Obviously, either control or junk CPT will cause significant degradations across three tasks, and IT can significantly mitigate them. The mitigation effectiveness suggests that the Brain Rot damages instruction following ability lot. It also suggests that instruction tuning is necessary in our benchmark to avoid the confounding factor of instruction failures. Despite the significant drops by both interventions, the control intervention is more recoverable after IT. When the model was trained on control data (0% Junk), the IT can reduce the gap to the baseline. In the ARC, the gap is reduced from 11.4 to 5.2 (challenge). In the ARC Easy, the gap is reduced more steeply, from 9.8 to 2.1. Compared to the control intervention, the junk intervention remains large gap after IT to the baseline 12.3 (Challenge) and 9.3 (Easy). The Different Effects of Instruction Tuning Among Tasks. In ARC, the difference between control and junk is even larger after IT: 5.2 7.1 (ARC Challenge) and 6.3 7.2 (ARC Easy). The observation implies inherent drops in the cognitive functions instead of simply instruction compliance. 2https://github.com/hiyouga/LLaMA-Factory 3https://github.com/anthropics/hh-rlhf 4https://github.com/LLM-Tuning-Safety/LLMs-Finetuning-Safety 5https://github.com/EleutherAI/lm-evaluation-harness 6https://github.com/pull-ups/TRAIT"
        },
        {
            "title": "Preprint",
            "content": "You are content quality classifier. Your task is to categorize the provided tweet into one of two categories: ## Classification Criteria: JUNK - Classify as junk if the tweet contains: JUNK or HIGH-QUALITY. Conspiracy theories, exaggerated claims, or unsupported assertions Sensationalized headlines using clickbait language or excessive trigger words Extremely brief content that lacks meaningful context or substance Misleading information or obvious misinformation Spam-like repetitive phrases or promotional content Superficial lifestyle content that flaunts personal success, exotic vacations, perfect relationships, or idealized appearances HIGH-QUALITY - Classify as high-quality if the tweet: Presents factually accurate, well-sourced information Demonstrates thoughtful analysis or insight that requires careful consideration Provides educational value or substantive commentary on important topics Shows clear reasoning and logical structure despite character limitations Contributes meaningfully to discourse or knowledge ## Instructions: Read the tweet carefully Determine which category best fits based on the criteria above Respond with only the classification: \"JUNK\" or \"HIGH-QUALITY\" Do not provide explanations unless specifically requested ## Tweet to classify: <Twitter Post> Figure 8: Prompt for GPT classifying samples as junk or control (high-quality) in M2. The criteria for high-quality data are modified from (Wettig et al., 2024). 1 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 if mode == \"NO_REASONING\": critiques.append(\"- The answer lacks any reasoning or explanation. You should provide step-by-step thinking to justify your choice.\") elif mode == \"NO_REASONING_OUTLINE\": critiques.append(\"- The reasoning lacks clear outline or structured approach. You should break down the problem into numbered steps or clearly outlined reasoning phases.\") elif mode == \"THOUGHT_SKIPPING\": if < len(mode_reasons) and mode_reasons[i]: reason = mode_reasons[i] if isinstance(mode_reasons[i], str) else str(mode_reasons[i]) critiques.append(f\"- The reasoning skips important steps: {reason}. Make sure to complete each step of your planned approach before moving to the next.\") else: critiques.append(\"- The reasoning appears to skip important intermediate steps. Make sure to complete each step of your planned approach before moving to the next.\") elif mode == \"FACTUAL_ERROR\": if < len(mode_reasons) and mode_reasons[i]: specific_errors = mode_reasons[i] if isinstance(mode_reasons[i], list) else [mode_reasons[i]] for error in specific_errors: if error: # Only add non-empty errors critiques.append(f\"- Factual error identified: {error}. Please verify and correct this information.\") elif mode == \"WRONG_LOGIC\": if < len(mode_reasons) and mode_reasons[i]: specific_errors = mode_reasons[i] if isinstance(mode_reasons[i], list) else [mode_reasons[i]] for error in specific_errors: if error: # Only add non-empty errors critiques.append(f\"- Logical error identified: {error}. Please reconsider this reasoning step. \") Figure 9: Python snippet of the critique-generation function, designed for reflective reasoning based on failure-mode analysis. However, in RULER, IT can effectively reduce the gap: 51.1 19.1. The potential cause is that the RULER tasks do not require complex thinking but only basic instruction following and context retrieval capabilities closely related to IT."
        },
        {
            "title": "Preprint",
            "content": "Finally, return the letter or number of the option as your answer like The Revise the draft answer using the critiques. Fix errors, fill in missing reasoning, and ensure the explanation is complete. answer is the letter or number of the option Query: Options: Draft Answer: Critiques: Revised Answer: <critiques> <options> <draft> <query> Figure 10: Prompt for reflection where we use the critiques to guide the revision of the answer. 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 class HasReasoning(dspy.Signature): \"\"\"Determine if the model response contains reasoning steps.\"\"\" model_response: str = dspy.InputField() has_reasoning: bool = dspy.OutputField() class HasReasoningOutline(dspy.Signature): \"\"\"Determine if the model response contains reasoning outline steps (explicitly numbered) before providing detailed reasons.\"\"\" model_response: str = dspy.InputField() has_reasoning_outline: bool = dspy.OutputField() class HasSkipThoughts(dspy.Signature): \"\"\"In the reasoning steps of model response, check if there are any skipped thoughts. Example of thought skipping: model response: \"Good question. Lets think about steps.n1. Identify candidates; 2. Compare their weights.nHydrogen and Carbon are possible candidates. The answer is B.\" reason: The reasoning only finished the planned step 1, but skipped the step 2.\"\"\" model_response: str = dspy.InputField() has_skip_thoughts: bool = dspy.OutputField() class FactualErrorInReasoning(dspy.Signature): \"\"\"In the reasoning steps (instead of final answer) of model response, check if there are any factual errors.\"\"\" query_to_model: str = dspy.InputField() model_response: str = dspy.InputField() identified_factual_errors: List[str] = dspy.OutputField() has_factual_error: bool = dspy.OutputField() class HasWrongLogic(dspy.Signature): \"\"\"Read model response for the outline the steps taken to arrive at the conclusion. Check if there are any wrong logic errors in the outline.\"\"\" query_to_model: str = dspy.InputField() model_response: str = dspy.InputField() identified_logic_errors: List[str] = dspy.OutputField() has_logic_error: bool = dspy.OutputField() Figure 11: DSPy (Khattab et al., 2023) signatures for classifying the failure mode via prompting LLMs. The green comments are used as prompts for LLMs, and InputFied/OutputField define the input and output variables to LLM queries, respectively."
        },
        {
            "title": "Preprint",
            "content": "Task Junk Ratio by M1 (engagment degree) 100% 80% 50% 20% 0% Junk Ratio by M2 (semantic quality) 100% 80% 50% 20% 0% Base - Table 4: Llama3 8B Instruct. Easy Acc. Challenge Acc. Challenge (COT) Acc. Overall NIAH-MK1 NIAH-MK2 NIAH-MK3 NIAH-MQ NIAH-MV NIAH-S1 NIAH-S2 NIAH-S3 Comm Word Ext (CWE) Freq Word Ext (FWE) QA (Hotpot) QA (SQUAD) Variable Tracking HH-RLHF Risk AdvBench Risk Narcissism Agreeableness Psychopathy Machiavellianism Neuroticism Conscientiousness Openness Extraversion 70.2 41.6 57.2 71 86.4 74.4 35.6 97.2 77.8 98.8 100 97.6 52.3 81.8 41.6 57.1 22.4 70.8 88. 47 64.3 75.7 33 28.7 89.8 70.1 54.1 73.3 43.9 67.2 81.6 93.6 97.2 80.8 95.3 65.9 100 99.8 99.6 63.2 77.2 46.6 62.9 78.7 53.6 88.6 21.8 67.9 55.8 30.6 23.8 88.6 72.8 40.1 74.3 44.7 68. 86.1 96 96.6 89.4 96.4 79.5 100 100 99.8 64.1 83.3 52.2 67.8 94.1 45.8 80.2 29.9 71.4 57.2 31.8 22.7 89.7 67.6 44.9 Reasoning (ARC) 76.9 46.5 73.4 78.7 47.8 74. 74.3 42.6 67.7 Long-Context (RULER) 88.5 98.8 99 92.6 99.2 83.9 99.8 99.6 99.6 81.6 84.7 55.4 69.3 87.6 90.5 98.6 99.2 95.6 99.9 83.2 100 100 100 84.4 90.5 58.6 74.3 91.5 86.2 98.6 99.8 96.8 94 68.6 100 100 100 68.2 89.7 51.2 67.6 86.6 Ethical Norm (Safety) 63.6 91.6 22.8 68.5 30 27 23.3 86 53.7 39.5 62.8 77.6 70.2 84.4 Personality (TRAIT) 18.9 73 33.5 25.8 16 85.1 63.9 48. 20.9 82 46.1 26.1 22 88.8 73.2 46.4 77.8 47.9 77.6 92.9 98.8 99.8 97.2 99.2 87 100 99.8 100 94.7 95.3 61.2 76.9 98 68.8 89.8 17.4 74.2 9.3 22.7 23.5 90.8 59.1 37.9 78.2 47.7 77. 93 98.6 99.8 98.8 99.8 87.8 100 100 100 97.3 92.3 58.8 76.8 99.4 65.8 89.6 16.9 69.9 23.5 20.2 21.1 85.7 55.6 38.6 77.5 47.4 77.6 93.4 99.6 99.6 99.2 99.5 89.8 100 100 100 96 94.7 60.6 76.2 99.2 65.8 85. 23.7 71.6 27.3 33.1 31.1 87.1 59.4 40.8 78.4 47.4 76.6 93.8 99.8 99.6 99.4 99.7 94.5 100 100 100 96.8 93.2 61.4 77.1 98.6 61.8 83.8 24.2 70.6 25.8 28.5 26.4 87.5 56.5 40 77.7 47.5 77. 93.9 99.8 99.8 100 99.9 97.8 100 100 100 91.8 91.9 64 77.9 98.3 57.2 61.4 33.5 75.6 2.2 17.8 33.5 89.2 52.5 26."
        },
        {
            "title": "Task",
            "content": "Junk Ratio by M1 (engagment degree) 100% 80% 50% 20% 0% Junk Ratio by M2 (semantic quality) 100% 80% 50% 20% 0% Base - Table 5: Qwen 2.5 7B. Easy Acc. Challenge Acc. Challenge (COT) Acc. Overall NIAH-MK1 NIAH-MK2 NIAH-MK3 NIAH-MQ NIAH-MV NIAH-S1 NIAH-S2 NIAH-S3 Comm Word Ext (CWE) Freq Word Ext (FWE) QA (Hotpot) QA (SQUAD) Variable Tracking HH-RLHF Risk AdvBench Risk Narcissism Agreeableness Psychopathy Machiavellianism Neuroticism Conscientiousness Openness Extraversion 74.3 45.6 83.5 88.6 99.6 99 97.4 99.2 77.5 100 100 98.8 80.6 82 54.6 73.9 88.7 61.8 44.4 13.3 82 1.1 21.8 22.9 92 61 31. 76.5 48.6 86.4 91.5 99.6 99.2 99.6 99.7 81.4 100 100 99.6 94.4 90.4 54.8 74.6 95.7 63.8 58.4 17.1 86.2 0.9 28.6 28.1 93.1 66.7 30.5 78 48.7 87.2 92.1 100 99.4 99.4 99.9 79.4 100 100 100 93 91.5 57 78.5 98. 64 61.4 13.6 84.4 0.9 25.2 27 90.4 65.2 31.3 Reasoning (ARC) 77.6 48.4 87.5 79.1 51.4 88.4 77.4 48.3 Long-Context (RULER) 92.5 100 99.6 99 99.8 92.1 100 100 99.8 93.4 83 58.6 77.4 99.7 92.2 100 99.2 99.2 99.9 80.2 100 100 100 95.1 92.7 58.8 76.7 97.2 92.5 100 99.6 99.8 100 77.4 100 100 100 93.8 92.6 64 76.6 99.4 Ethical Norm (Safety) 56 55.2 45.6 52.4 36.4 Personality (TRAIT) 15.5 84.4 0.9 25.3 24.9 92.1 68.2 33.8 16.4 85.6 0.2 27.6 24.4 92.5 67.5 33 13.1 85.9 0.4 22.2 23.7 91 68.5 35. 78.6 50 87.9 93.4 100 100 99.4 100 81.7 100 100 100 97.2 94.1 62.4 79.7 99.3 60.2 46.2 13.2 85.4 0.2 22.6 23.3 91.6 66.1 31.9 18 79.5 49.6 88. 93.3 100 99.8 99.4 100 82.5 100 100 100 97.7 93.7 61.8 79.1 99.1 53.4 34.6 13.4 85.1 0.1 22.2 25.7 91.2 63.5 31.7 79.7 49.8 88.6 93.7 100 100 99.4 99.9 83.3 100 100 100 98.3 95 62.2 80.5 99.7 53 34. 12.3 85.7 0.2 23 23.1 90.2 64.9 33.1 80 51.5 88.6 93.6 100 100 99.6 100 82.3 100 100 100 98.5 94.5 62.4 79.5 99.6 50 30.8 14.2 84.3 0.4 24.2 24.3 90.4 62.6 30.6 80 50.7 88. 93.3 100 100 99.2 100 80.4 100 100 100 98.9 93.9 61.2 80.2 99.9 53.2 37.8 9.8 84.8 0.5 20.4 23.3 90.3 60 33."
        },
        {
            "title": "Preprint",
            "content": "Task Junk Ratio by M1 (engagment degree) 100% 80% 50% 20% 0% Junk Ratio by M2 (semantic quality) 100% 80% 50% 20% 0% Base - Table 6: Qwen 2.5 0.5b . Easy Acc. Challenge Acc. Challenge (COT) Acc. Overall NIAH-MK1 NIAH-MK2 NIAH-MK3 NIAH-MQ NIAH-MV NIAH-S1 NIAH-S2 NIAH-S3 Comm Word Ext (CWE) Freq Word Ext (FWE) QA (Hotpot) QA (SQUAD) Variable Tracking HH-RLHF Risk AdvBench Risk Narcissism Agreeableness Psychopathy Machiavellianism Neuroticism Conscientiousness Openness Extraversion 51.3 30 31.4 47.6 66 29.2 2 77.1 76.1 99.6 98.6 12.6 33.9 35.3 22.8 35.3 29.9 70 30 72.8 12.2 25.2 24.8 84.8 75.5 42.4 52.8 27.8 32.8 57.7 85.6 43 6.4 75.4 79.8 100 99.8 91.6 44.5 32.6 28.6 46.9 15.8 67.6 80.4 21.9 75.4 6.9 26.9 32.2 85 75.3 41.6 53.9 28.8 33. 61.3 91.6 52.8 7.6 80.5 86.9 100 100 95.4 40.6 23.6 27.4 47.4 42.6 69.8 85.4 17.8 82.2 5.2 25.2 31.1 91.2 72.5 34.1 Reasoning (ARC) 55.1 28 32.5 58 29.8 37. 57.1 29.8 41 Long-Context (RULER) 64.3 90.6 48.4 14.4 79.7 86.6 100 98.6 99.4 47.1 32.2 30.6 52.4 55.7 67 88 54.8 16.2 87.7 87.4 100 99.6 98.8 56.5 44.3 32.6 51.1 54.2 71.3 97.8 78.2 19.8 88.1 81.5 100 96.6 96.8 56.4 50.6 35.2 52.7 73.4 Ethical Norm (Safety) 62.4 77.4 23.9 79 12 29.4 29.1 88.3 77.3 37.8 64 71 71 79.6 Personality (TRAIT) 22.8 74.7 9.7 29 32.6 91.7 67.3 28. 21.5 81.6 8.1 28.7 26.3 89.4 72.9 33.6 57.3 29.9 42 72.4 95.6 81.6 25 86.2 87.6 100 95.8 98 58.2 56.4 33 57.3 66.6 65.2 73.6 21.4 79 11 32.2 32 89.4 75.2 35.7 57 29.8 41. 73.1 97.8 80.2 26.4 89.3 91.5 100 98.2 99 56.2 50.7 31.4 57 72.8 64.6 80.8 21.7 78 12.5 28 29.2 89.7 73.6 33.3 57.4 30.6 42.2 73.8 98.2 85.2 17.8 89.4 89.8 100 99.4 98.6 57 53.2 32.8 58 79.4 68 22.7 75.3 8.2 29.1 29.5 91.1 72.9 33 56.4 29.6 41.3 74.4 97.4 86.6 21.4 87.8 91.3 100 99.6 99 55.8 54.5 32.4 59.2 82.2 65.2 74.4 22.5 75.9 8 29.7 28.7 92.1 73.2 32.2 58.9 29.9 43. 76.8 99.8 91 33.4 90.6 93.7 100 99.8 99.8 59.9 55.6 36.2 58.2 79.9 63 67 20 71.6 9.5 26.7 27.2 89.6 61 34."
        },
        {
            "title": "Task",
            "content": "Junk Ratio by M1 (engagment degree) 100% 80% 50% 20% 0% Junk Ratio by M2 (semantic quality) 100% 80% 50% 20% 0% Base - Table 7: Qwen 3 4B. Easy Acc. Challenge Acc. Challenge (COT) Acc. Overall NIAH-MK1 NIAH-MK2 NIAH-MK3 NIAH-MQ NIAH-MV NIAH-S1 NIAH-S2 NIAH-S3 Comm Word Ext (CWE) Freq Word Ext (FWE) QA (Hotpot) QA (SQUAD) Variable Tracking HH-RLHF Risk AdvBench Risk Agreeableness Conscientiousness Extraversion Neuroticism Openness Psychopathy Machiavellianism Narcissism 51.7 37.7 86.4 93.3 99.8 100 100 100 86.6 100 100 100 99.1 94.3 58 75.4 99.8 53.8 46.4 79.2 90.6 36.9 34.5 70.4 2.1 26.1 19. 54.3 41.6 85.8 92.9 99.8 100 99.8 100 81.2 100 100 99.4 99.2 92.5 59.4 76.5 99.6 51.8 39.4 74.2 85.8 43.4 37.8 65.1 4.7 27.3 21.1 53.1 40.8 88.2 93.8 100 99.6 99.8 100 85.7 100 100 99.8 99.7 96.4 60 78.1 51.4 39.4 78.9 87.5 35.3 30.6 65.4 1.6 21 13.1 Reasoning (ARC) 61.7 43.9 87.7 52.3 41.5 87.7 79.6 51.1 89. Long-Context (RULER) 94.2 100 99.8 99.8 100 91 100 100 100 99.9 97.8 61 75.5 100 94.2 100 100 99.6 100 87.2 100 100 100 99.8 98.4 60.8 78.8 100 95.3 100 100 100 100 98.2 100 100 100 99.8 97.5 65 78.5 100 Ethical Norm (Safety) 56 52. 81.8 90.7 40.5 25.9 69.2 1.7 19.3 14.5 51.8 40.8 46 26.2 Personality (TRAIT) 74.9 89.9 41.5 33.2 63.5 1.8 23.2 17.2 83.8 91.4 37.2 30.5 68.6 1.2 18.6 12. 80 53.8 90.8 95.3 100 100 100 100 98.3 100 100 100 99.5 98.9 64.2 78.4 100 49.2 28.8 82.2 91.5 37.1 27.7 67.6 0.8 15.6 9.5 19 80.3 53.4 90. 95.3 100 100 100 100 98.8 100 100 100 99.8 98.1 63.8 78.5 100 53.2 36.8 81.6 92 35.1 27.1 66.5 0.5 14.1 8.9 80.6 52.9 89.9 95.2 100 100 100 100 97 100 100 100 99.9 98.7 63.8 78 100 51 35. 81.1 91.3 34.9 29.1 66 0.5 15.2 9.7 80.6 53.4 90.4 95.4 100 100 100 100 97.2 100 100 100 100 98.5 63.6 80.4 100 48.6 33.8 80.9 90.1 36.1 27.4 65.9 0.6 15.9 8.7 68.1 44.3 89. 95 100 100 100 100 97.5 100 100 100 99.9 98.5 59.6 79.5 100 40.2 23.2 55.9 53.1 37.9 45.1 43.3 21.5 48 27."
        },
        {
            "title": "Preprint",
            "content": "Table 8: Instruction tuning (IT) after continual pretraining (CPT) can mitigate the M1 junk intervention on the ARC benchmark. The baseline model represents Llama3 8B Instruct. ARC Challenge CPT+IT CPT RULER Overall CPT+IT CPT"
        },
        {
            "title": "ARC Easy",
            "content": "CPT+IT"
        },
        {
            "title": "CPT",
            "content": "100% 80% 50% 20% 0%"
        },
        {
            "title": "Baseline",
            "content": "36.60 38.65 35.75 43.77 41.81 53.2 40.87 43.09 44.20 47.70 47.95 53.2 65.32 68.35 66.67 72.22 71.59 81. 72.10 74.12 73.91 77.23 79.34 81.4 29.58 55.34 64.35 78.22 80.66 91.3 71.75 81.65 85.23 88.28 90.94 91."
        }
    ],
    "affiliations": [
        "Purdue University",
        "Texas A&M University",
        "University of Texas at Austin"
    ]
}