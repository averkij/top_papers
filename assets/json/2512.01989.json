{
    "paper_title": "PAI-Bench: A Comprehensive Benchmark For Physical AI",
    "authors": [
        "Fengzhe Zhou",
        "Jiannan Huang",
        "Jialuo Li",
        "Deva Ramanan",
        "Humphrey Shi"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Physical AI aims to develop models that can perceive and predict real-world dynamics; yet, the extent to which current multi-modal large language models and video generative models support these abilities is insufficiently understood. We introduce Physical AI Bench (PAI-Bench), a unified and comprehensive benchmark that evaluates perception and prediction capabilities across video generation, conditional video generation, and video understanding, comprising 2,808 real-world cases with task-aligned metrics designed to capture physical plausibility and domain-specific reasoning. Our study provides a systematic assessment of recent models and shows that video generative models, despite strong visual fidelity, often struggle to maintain physically coherent dynamics, while multi-modal large language models exhibit limited performance in forecasting and causal interpretation. These observations suggest that current systems are still at an early stage in handling the perceptual and predictive demands of Physical AI. In summary, PAI-Bench establishes a realistic foundation for evaluating Physical AI and highlights key gaps that future systems must address."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 1 ] . [ 1 9 8 9 1 0 . 2 1 5 2 : r PAI-Bench: Comprehensive Benchmark For Physical AI Fengzhe Zhou1,* Jiannan Huang1,* Jialuo Li1,* Deva Ramanan2 Humphrey Shi1 1Georgia Tech 2CMU"
        },
        {
            "title": "Abstract",
            "content": "Physical AI aims to develop models that can perceive and predict real-world dynamics; yet, the extent to which current multi-modal large language models and video generative models support these abilities is insufficiently understood. We introduce Physical AI Bench (PAI-Bench), unified and comprehensive benchmark that evaluates perception and prediction capabilities across video generation, conditional video generation, and video understanding, comprising 2,808 real-world cases with task-aligned metrics designed to capture physical plausibility and domain-specific reasoning. Our study provides systematic assessment of recent models and shows that video generative models, despite strong visual fidelity, often struggle to maintain physically coherent dynamics, while multi-modal large language models exhibit limited performance in forecasting and causal interpretation. These observations suggest that current systems are still at an early stage in handling the perceptual and predictive demands of Physical AI. In summary, PAI-Bench establishes realistic foundation for evaluating Physical AI and highlights key gaps that future systems must address. Date: Tuesday 2nd December, 2025 Code Leaderboard PAI-Bench-G Dataset PAI-Bench-C Dataset PAI-Bench-U Dataset SHI-Labs/physical-ai-bench shi-labs/physical-ai-bench-leaderboard shi-labs/physical-ai-bench-generation shi-labs/physical-ai-bench-conditional-generation shi-labs/physical-ai-bench-understanding Equal contribution PAI-Bench: Comprehensive Benchmark For Physical AI 1. Introduction Human intelligence was not forged in vacuum. It was sculpted by the relentless demands of the physical world.\" We learned to think because we had to move, grasp, and survive. AI systems stand at similar threshold. To mature from mere information processor into truly capable partner, they must leave the clean, predictable confines of digital simulation and enter the messy, unforgiving reality. This is the core mission of Physical AI: the embodiment of intelligent algorithms into autonomous systems that must perceive, predict, and act directly within the physical world. In this work, we focus on the perception and prediction components of this mission through visual signals, specifically video streams. Regarding perception, while recent multi-modal large language models (MLLMs) have demonstrated escalating capabilities with increased scale [8, 51, 76], their advancements are predominantly validated using benchmarks for abstract reasoning (e.g., OCR [30], mathematical problems [72]) and simple daily-life perception [28, 70, 102]. Their true efficacy within specialized Physical AI applications remains largely uncharted. Similarly, in the domain of prediction, Physical AI agent must rely on an internal world model to forecast the consequences of actions and complex physical dynamics. Video generative models (VGMs) [16, 62] represent powerful new paradigm for learning such dynamics, as they are implicitly trained to understand physics rules by predicting future video frames. However, existing benchmarks [38, 39, 53, 101] for these models have largely focused on assessing aesthetic appeal and temporal consistency, with far less attention paid to evaluating their fundamental understanding of realworld rules. It thus remains unclear whether they can model complex interactions, adhere to physical laws, or make reasonable predictions in practical applications. To address these deficiencies and advance Physical AI, we introduce Physical AI Bench (PAIBench), comprehensive benchmark designed to systematically evaluate model performance on tasks grounded in Physical AI. PAI-Bench is structured into three distinct tracks: PAI-Bench-G (Generation): Targets prediction by evaluating visual quality and physical plausibility of VGMs. PAI-Bench-C (Conditional Generation): Further probes prediction by evaluating the fidelity of conditional VGMs, specifically their conformance to input control signals (e.g., depth maps). PAI-Bench-U (Understanding): Targets the perception component by evaluating MLLM capabilities on video understanding tasks specific to the Physical AI domain. All three tracks adhere to unified design principle: grounding evaluation in physically meaningful tasks and real-world data. Specifically, all videos are sourced from real-world captures, such as dashcam recordings. Furthermore, PAI-Bench covers several sub-domains, such as autonomous vehicles, robotics, ego-view scenes, etc. , across three tracks, comprising 2,808 high-quality cases. Based on PAI-Bench, we conducted comprehensive evaluation of leading VGMs and MLLMs. Extensive experiments reveal that while SOTA VGMs like Veo3 [21] achieve high visual fidelity, their prediction abilities are limited, as they often fail to adhere to basic physical laws or to model complex real-world dynamics. Concurrently, the perception capabilities of powerful MLLMs, such as GPT-5 [61], lag substantially behind human performance. These findings suggest that the core prediction and perception capabilities required for Physical AI remain in nascent stage. In summary, our contributions are three-fold: We introduce PAI-Bench, the first benchmark to provide unified and comprehensive review of video generation, conditional video generation, and video understanding. As the first high-quality benchmark grounded in Physical AI, PAI-Bench spans diverse scenarios (e.g., industrial environments, everyday activities, physical common sense) and incorporates suite of task-aligned metrics. 2 PAI-Bench: Comprehensive Benchmark For Physical AI Figure 1 Overview of PAI-Bench framework. PAI-Bench is comprehensive bench designed for diverse topics in Physical AI, including evaluation for text and condition to physical World Generation, and physical world understanding. We conduct large-scale evaluation of 15 VGMs, 4 conditional VGMs with 5 control settings, and 16 MLLMs, establishing the current landscape of Physical AI and highlighting key challenges to guide future research. 2. Related Work Prediction and Perception are foundational capabilities for Physical AI applications. However, significant gap persists in current benchmarking: the absence of comprehensive framework specifically for the Physical AI domain. Furthermore, existing benchmarks are largely fragmented, typically assessing prediction or perception in isolation. In the realm of prediction, world models [4, 12, 16, 34] are increasingly vital for applications such as robotics and autonomous driving. prominent paradigm utilizes VGMs to forecast high-fidelity future frames [1, 4, 17, 21, 36, 62, 63, 79, 88, 93, 97]. This progress has spurred the proposal of numerous benchmarks [26, 41, 53, 54, 57, 98]. However, existing evaluation methodologies are often narrow in scope. Some benchmarks [38, 39, 101] primarily assess visual quality and temporal consistency, while others [11, 14, 22, 57] concentrate exclusively on physical plausibility. We introduce PAI-Bench-G, which distinguishes itself by integrating both aspects via domain score and quality score, with strong emphasis on Physical AI applications. Furthermore, significant evaluation gap has emerged as VGMs [3, 4, 21, 36, 62, 79, 97] increasingly leverage multimodal control signals (e.g., depth maps) for guided synthesis. No existing benchmark systematically tests this controllability. PAI-Bench-C is therefore introduced as the first benchmark to systematically evaluate this condition guided generation capability. Similarly, in the perception domain, while numerous benchmarks exist for general video understanding (e.g., action recognition) [28, 37, 49, 70, 72, 92, 102], evaluations for physically PAI-Bench: Comprehensive Benchmark For Physical AI Table 1 Comparison with existing benchmarks. Our PAI-Bench is designed with comprehensive domains in Physical AI. Specifically, we focus on scenarios involving practical applications, such as autonomous vehicles (AV), industry, embodied AI, and ego-centric views. Benchmarks # Examples Video Generation Conditional Video Generation Video Understanding Embodied AI AV Industry Egocentric Views Physical Common Sense EvalCrafter [53] VBench [38] Physics-IQ [59] EgoSchema [56] VideoMME [29] MVP Bench [44] CausalVQA [27] PhyGenBench [57] IntPhys2 [14] PAI-Bench (Ours) 700 800 198 5000 2700 54800 793 160 1070 2808 grounded reasoning remain notably limited, particularly for video-based analysis [20, 27, 48, 59, 73]. PAI-Bench-U bridges this gap by unifying tasks for general video comprehension with dedicated suite of physically grounded challenges. In summary, PAI-Bench is the first comprehensive benchmark to focus specifically on the Physical AI domain. By evaluating the capabilities of prediction and perception, it provides holistic framework for assessing the preparedness of AI models for real-world physical interaction. 3. PAI-Bench: Comprehensive Benchmark For Physical AI Overview. This section details the foundational principles and construction process for each track of PAI-Bench. We elaborate on the core motivation, the design principles, and the data curation process. The overall framework is illustrated in Figure 1. Detailed expositions of PAI-Bench-G, PAI-Bench-C, and PAI-Bench-U are provided in Sections 3.1, 3.2, and 3.3, respectively. 3.1. PAI-Bench-G: Video Generation For prediction capability, models should be able to forecast the next physically plausible states based on existing information to dynamically adjust their policies. Within the current community, VGMs are considered the most promising approach for this task. To evaluate the efficacy of these models, we propose two metrics: 1) Quality Score to assess the fidelity and coherence of the generated video, and 2) Domain Score to validate physical plausibility. Data Curation. We curated videos focusing on Physical AI domains (e.g., autonomous driving, robotics, and industrial applications) from open-source datasets and public web sources (detailed in Appendix A.1). The annotation involved two-stage process. First, to generate high-fidelity prompts, we employed an advanced MLLM, i.e., Qwen2.5-VL-72B-Instruct [10], for initial video captioning, followed by rigorous manual curation and correction. Second, to support Domain Score assessment, we generated QA pairs by leveraging the ontology from [7]. This process used the same MLLM for candidate generation, with subsequent manual refinement ensuring accuracy and relevance. The final dataset comprises 1,044 video-prompt pairs and 5,636 QA pairs across 6 domains. The data distribution and qualitative examples are presented in Figure 2 and Figure 3, respectively. Quality Score. To provide comprehensive assessment of temporal consistency, visual fidelity, and semantic alignment, we adhere to the evaluation protocols established by VBench and VBench++ [38, 39, 101]. The assessment comprises eight metrics categorized into two groups: (1) General Generation Quality: We evaluate the videos temporal stability (Subject/Background Consistency), motion realism (Motion Smoothness), and perceptual quality (Aesthetic/Imaging Quality) using combination of frame-level embedding similarities (e.g., DINO [15]) and spe4 PAI-Bench: Comprehensive Benchmark For Physical AI Figure 2 Distribution of videos and QA pairs in PAI-Bench-G. These pairs facilitate Domain Score evaluation with an average density of 5-6 QA pairs per video. cialized predictors (e.g., LAION [46]). Additionally, Overall Consistency measures video-text alignment via ViCLIP [87]. (2) Reference Fidelity: For image-conditioned tasks, we assess the preservation of the inputs identity and layout (Image-to-Video Subject/Background) by computing feature distances between the reference image and generated frames. Implementation details are provided in Appendix A.2. Domain Score. Evaluating the physical plausibility of the generated video necessitates analyzing fine-grained spatio-temporal details against foundational world knowledge. Conventional featurelevel alignment metrics are inadequate for this task, as they fail to capture high-level semantic reasoning. To address this, we adopt the \"MLLM-as-Judge\" paradigm, leveraging Qwen3-VL235B-A22B-Instruct [9]. Specifically, We utilize the curated QA pairs, which define the expected physical and semantic content, to query the MLLM against the generated video. The Domain Score is defined as the MLLMs response accuracy across this QA set, quantifying the videos adherence to the specified physical and semantic constraints. 3.2. PAI-Bench-C: Conditional Video Generation Control signals provide an interface through which humans can constrain the solution trajectory of VGMs, making them highly valuable in practical applications. In conditional video generation, an ideal outcome should satisfy three key criteria: 1) faithfulness to the control signals, 2) visual quality of the generated videos, and 3) diversity in the generated results under the same configurations of control signals. Evaluation Metrics. Based on these principles, we evaluate faithfulness using suite of fidelity metrics: Blur SSIM, Edge F1, Depth si-RMSE, and Mask mIoU. These metrics respectively quantify adherence to blur, edge, depth, and segmentation map controls. The computation projects the generated video into the specific modality space, utilizing Blur Kernel [85], Canny [77], Video Depth Anything [18], and GroundingDINO [52] with SAM2 [66], and then measures the similarity to the ground truth control signal. Visual quality is measured using DOVER [8991], and diversity is assessed with LPIPS [100]. Detailed calculation protocols are presented in Appendix B. Dataset Curation. We curated 600 videos by sampling 200 clips from each of the three datasets: AgiBot [2, 71] for robotics, OpenDV [95] for autonomous driving, and Ego-Exo-4D [33] for ego-centric tasks. Ground-truth control signals were extracted using the modality-specific models described above. For diversity evaluation, we adopted human-in-the-loop pipeline similar to PAI-Bench-G: first, Qwen2.5-VL-72B-Instruct [10] generated base high-fidelity caption; second, 5 PAI-Bench: Comprehensive Benchmark For Physical AI Figure 3 Examples of PAI-Bench. PAI-Bench focuses on Physical AI application scenarios across six domains, where only the first frame of each video is shown for brevity. For PAI-Bench-C, we present the blurred, edge, segmentation, and depth videos that serve as control signals. For PAI-Bench-U, we show the questions used for video understanding. For PAI-Bench-G, we show the input captions used for generation and derived prompt used for Domain Score evaluation. the MLLM iteratively modified visually dominant objects to create novel scenes. Both stages included manual refinement to ensure caption coherence, producing one original and five variant captions per video. 3.3. PAI-Bench-U: Physical Video Understanding Human cognition leverages multimodal sensory integration to build robust representations of the physical world. This perception capability is also foundational for Physical AI. Focusing on the interpretation of visual signals from videos, we posit that robust Physical AI models must possess two critical capabilities: 1) Physical Common Sense Reasoning and embodimentagnostic understanding of environmental physics to assess real world plausibility, and 2) Embodied Reasoning, the capacity for an agent to perceive, reason about, and plan future interactions. Physical Common Sense Reasoning. Humans acquire physical common sense primarily through passive observation [67], forming an implicit knowledge base of real-world plausibility. To formalize this concept, we delineate an ontology comprising three domains: Space governs object relations, interactions, and environmental context. This includes determining spatial feasibility and understanding scene composition. Time concerns actions and events unfolding over duration. This includes understanding event timestamps, sequential order, and causality. Physical World addresses physical principles and intrinsic object properties. It includes understanding object states and identifying situations that defy physical laws. Embodied Reasoning. Unlike abstract symbolic logic, embodied reasoning is grounded in the real-world actions of an agent operating in dynamic environments. This active grounding enables the agent to transcend passive understanding and develop predictive plans for future interactions. We delineate this capability into two components: 6 PAI-Bench: Comprehensive Benchmark For Physical AI Table 2 Question counts across categories in PAI-Bench-U. Space Time Physical World 80 298 226 Figure 4 Video duration distribution in PAI-Bench-U. Figure 5 Word cloud of questions in PAI-Bench-U. BridgeData RoboVQA RoboFail Agibot HoloAssist AV 100 101 100 100 100 Predicting Action Effects involves reasoning about physical cause-and-effect to forecast the consequences of an agents interactions. We assess this via two tasks: task-completion verification (determining if goal state is achieved) and next-plausible action prediction (forecasting subsequent steps toward goal). Adherence to Physical Constraints applies real-world physical principles to generate action plans that are feasible, stable, and safe. We assess this via action affordance, which evaluates the viability of performing specific action to achieve goal. Data Curation. For physical common sense reasoning, we first collected over 1,000 videos from online sources and manually annotated 5,737 questions based on the defined ontology. Following rigorous manual review process, this collection was refined to final set of 604 high-quality QA pairs across 426 videos. For embodied reasoning, we sourced 601 videos from diverse existing datasets, including RoboVQA [69], RoboFail [55], BridgeData [78], AgiBot [2], HoloAssist [83], and proprietary AV dataset. Guided by the established criteria for this domain, 610 QA pairs were manually annotated for this subset. Comprehensive meta-data and statistical distributions are detailed in Figures 4, 5 and Table 2. Full curation details are in Appendix C.1. 4. Experiments Overview. We conduct systematic evaluation of current Physical AI capabilities through PAI-Bench suite. Our experiments cover three distinct aspects: video generation fidelity and plausibility on PAI-Bench-G (4.1), the efficacy of multi-signal control on PAI-Bench-C (4.2), and the physical reasoning capability of MLLMs on PAI-Bench-U (4.3). In the following sections, we outline the model configurations and evaluation protocols, followed by an in-depth discussion of the quantitative results and observed phenomena. 4.1. PAI-Bench-G Models. We evaluate 15 leading VGMs, covering open-source models from Wan [79], CosmosPredict [4], MAGI [75], and CogVideoX [97] families, along with DynamicCrafter [93], LTXVideo [35], HunyuanVideo-I2V [43], and the proprietary model Veo3 [21]. Evaluation Suites. We evaluate all models using their default settings (e.g., resolution, frame count, and frame rate). To account for stochasticity, scores for open-source models are averaged over five videos generated per caption, each with different random seeds. We generated one video per caption for Veo3 [21]. The complete results are presented in Table 3. Metrics in PAI-Bench-G align with human preferences. To validate our proposed metrics against human preferences, we conduct an arena-based human study. We sample video pairs generated by various VGMs from identical captions. Participants then perform pairwise comparisons, selecting the superior video or noting tie based on two separate criteria: video quality, corresponding to Quality Score, and physical plausibility, corresponding to Domain Score. We aggregate the pairwise preferences into ELO scores [24] and compute the Pearson correlation 7 PAI-Bench: Comprehensive Benchmark For Physical AI Table 3 Evaluation results of 15 VGMs on PAI-Bench-G. Metrics in Domain Score: Common Sense (CS), Autonomous Vehicle (AV), Robot (RO), Industry (IN), Human (HU), Physics (PH); and Quality Score: Subject Consistency (SC), Background Consistency (BC), Motion Smoothness (MS), Aesthetic Quality (AQ), Imaging Quality (IQ), Overall Consistency (OC), I2V Subject (IS), I2V Background (IB). Blue means the best across open-source models. Models Overall Domain Score Quality Score CS AV RO IN HU PH Avg. SC BC MS AQ IQ OC IS IB Avg. Source Videos 83.9 96. 78.7 91.8 89.0 86.6 93.1 89. 93.3 94.2 99.1 51.7 68.4 21. 97.8 98.2 78.0 Proprietary Models Veo3 [21] Open-source Models DynamicCrafter [93] LTX-Video-2B [35] MAGI-1-4.5B [75] HunyuanVideo-I2V [43] CogVideoX1.5 [97] LTX-Video-13B [35] CogVideoX [97] MAGI-1-24B [75] Cosmos-Predict2-2B [4] Cosmos-Predict2-14B [4] Wan2.1-I2V-14B [79] Wan2.2-TI2V-5B [79] Cosmos-Predict2.5-2B [4] Wan2.2-I2V-A14B [79] 82. 93.0 72.1 88.8 89.0 84.4 89. 86.8 91.4 93.1 99.2 51.9 69. 21.7 97.0 96.9 77.6 68.3 77.5 77.5 77.6 78.4 78.4 78.6 79.4 79.7 80.0 80.8 81.3 81.4 82.3 73.9 88.1 88.3 86.7 88.7 88.6 88.4 91.4 91.2 92.5 92.5 92.2 91.5 94. 43.9 60.5 63.4 60.3 64.4 60.0 62.7 68.0 70.7 69.8 69.4 69.1 70.4 73.0 48.9 69.4 71.3 68.9 73.6 71.8 75.3 76.5 81.9 81.3 79.6 82.9 82.7 86.8 70.9 82.6 82.5 82.4 84.6 84.6 85.0 85.4 86.4 86.6 87.2 86.4 86.8 88.6 60.7 77.1 77.3 75.8 79.1 79.1 81.0 81.5 83.1 82.4 83.4 83.3 83.5 84.5 80.8 86.2 86.3 85.8 88.7 88.8 88.5 87.0 88.1 89.2 89.5 90.3 91.7 92.7 63.0 78.0 78.7 77.1 80.3 79.5 80.9 82.4 84.2 84.3 84.3 84.7 84.9 87. 91.1 89.2 92.1 94.3 91.6 90.6 91.4 90.0 88.7 89.6 90.4 92.0 92.3 91.2 92.5 92.7 93.3 95.3 93.9 93.5 93.4 92.4 92.1 92.8 93.0 93.9 94.2 93.2 94.9 98.7 99.0 99.5 98.5 99.0 98.0 99.0 97.6 98.0 98.0 98.8 99.1 98.3 51.5 53.2 50.4 52.1 50.0 53.5 51.2 50.2 49.3 49.8 51.7 52.3 52.8 51.8 68.0 71.3 61.8 65.2 66.5 69.5 64.6 64.2 65.9 67.5 70.0 69.6 70.7 69.2 21.2 21.1 21.6 21.5 21.2 21.4 21.3 21.4 21.6 21.5 21.4 21.4 21.2 21. 84.5 95.0 94.5 98.6 95.0 95.7 94.1 96.8 91.9 92.2 96.5 97.4 96.2 97.2 86.2 95.9 98.1 97.6 96.1 96.0 95.9 97.9 94.6 94.9 97.2 98.0 97.2 97.8 73.7 77.1 76.3 78.0 76.6 77.4 76.3 76.5 75.2 75.8 77.3 77.9 78.0 77.5 Figure 6 Pearson correlation analysis on PAI-Bench-G. The red shaded regions indicate the 0.95 confidence intervals. between these ELO scores and our metric scores. As shown in Figure 6, the results demonstrate strong alignment, yielding an overall Pearson correlation coefficient of = 0.918. VGMs excel in visual quality but lack physical plausibility. As detailed in Table 3, clear trend emerges: most leading VGMs achieve Quality Score that is highly competitive with, and in some cases identical to, that of the source videos (78.0). This suggests that current models have largely succeeded in generating videos with high visual fidelity and aesthetic appeal. In stark contrast, significant disparity exists in Domain Score. The source videos, being real-world captures and thus inherently physically coherent, set high baseline. However, all evaluated models fall short of this benchmark. This discrepancy highlights that while VGMs excel at visual synthesis, they still struggle to generate content that consistently adheres to fundamental physical laws. Consequently, enhancing the physical plausibility of generated videos remains critical challenge and key direction for Physical AI. 4.2. PAI-Bench-C Models. Our evaluation targets two families of controllable video generation models: CosmosTransfer [3, 4] and Wan-Fun [5]. Evaluation Suites. We standardize generation settings across all models, configuring them to produce 121-frame videos, preserve the input videos aspect ratio, and use their default configurations. We assess model performance using five distinct conditioning signals: blurred video (Blur), edge video (Edge), depth video (Depth), segmentation maps (Seg), and combination of all signals with equal weighting (All). The All condition is exclusively evaluated on Cosmos8 PAI-Bench: Comprehensive Benchmark For Physical AI Table 4 Evaluation results of 4 conditional VGMs on PAI-Bench-C. For each model, the control signal settings consist of either single video or combination of multiple signal videos. Green means the best across control signal settings for each model. Model Condition Blur SSIM Edge F1 Depth si-RMSE Mask mIoU Quality Score Diversity Cosmos-Transfer1-7B [3] Cosmos-Transfer2.5-2B [4] Wan2.2-Fun-5B-Control [5] Wan2.2-Fun-A14B-Control [5] Blur Edge Depth Seg All Blur Edge Depth Seg All Edge Depth Blur Edge Depth Seg 0.89 0.77 0.67 0.62 0.82 0.91 0.76 0.70 0.66 0. 0.61 0.56 0.57 0.68 0.56 0.47 0.20 0.38 0.15 0.11 0.26 0.26 0.39 0.17 0.13 0.45 0.27 0.11 0.09 0.37 0.11 0. 0.66 0.85 0.76 1.13 0.70 0.54 0.74 0.83 1.07 0.59 1.01 1.82 2.11 0.84 2.10 1.60 0.73 0.73 0.71 0.70 0.74 0.75 0.74 0.72 0.71 0. 0.71 0.62 0.50 0.74 0.58 0.66 6.56 6.76 6.89 6.02 9.24 8.77 8.05 7.30 7.87 9.24 8.79 9.32 8.81 9.00 9.22 7. 0.19 0.28 0.39 0.42 0.22 0.18 0.36 0.41 0.44 0.13 0.40 0.48 0.53 0.38 0.52 0.36 Transfer [3]. For each control signal, we generate six videos using six unique captions, maintaining fixed random seed. Notably, since Wan2.2-Fun-5B-Control [5] failed to produce coherent results when conditioned on blur or segmentation maps, these are omitted from our evaluation. All results are summarized in Table 4. Multi-signal conditioning enhances video quality. The results in Table 4 clearly demonstrate the benefits of multi-signal conditioning. For both Cosmos-Transfer [3] models, the All condition achieves the highest Quality Score, significantly outperforming any single control signal. This finding suggests valuable practical application: instead of relying on single degraded input, such as blurry or noisy video, users can first extract complementary control signals from the source. By providing the model with this comprehensive set of control signals, it becomes possible to reconstruct video of substantially higher quality. Segmentation signals induce poor segmentation fidelity. For Cosmos-Transfer [3] models, surprisingly, we observe that using segmentation maps as the control signal results in the lowest Mask mIoU. We hypothesize that this stems from segmentation maps being the noisiest supervision signal among all conditions. Even state-of-the-art segmentation models, e.g., SAM2 [66], can produce masks with lower temporal consistency than other control signals, such as the occasional missing object masks across frames. Consequently, the resulting training supervision is likely to be highly unreliable, degrading the models ability. 4.3. PAI-Bench-U Models. We evaluate 21 MLLMs, covering both proprietary and open-source models. The proprietary models include Claude-3.5-Sonnet [6], GPT-4o [40], and GPT-5 [61]. For GPT-5, we use minimal reasoning effort. The open-source group spans several major families, including Qwen2.5-VL [10], Qwen3-VL [9], Cosmos-Reason1 [7], InternVL3.5 [82], and GLM-4.5V [74]. This collection covers broad spectrum of model scales, from 7B to over 200B. To analyze model performance, we also establish human baseline derived from an annotator team. Evaluation Suites. We conduct our evaluation using the LMMs-Eval framework [99]. All models are evaluated using 16 video frames as input, except for InternVL3.5, which can accommodate only 8 frames due to its context length. We follow the method in [81] to adaptively resize video frames. For all model outputs, we apply the lightweight LLM Qwen3-8B [94] as post-processing model to extract the final letter option from the full responses. 9 PAI-Bench: Comprehensive Benchmark For Physical AI Table 5 Evaluation of 16 MLLMs on PAI-Bench-U. Embodied reasoning domains: BridgeData (BD), RoboVQA (RV), RoboFail (RF), Agibot (AB), HoloAssist (HA), Autonomous Vehicle (AV). Red denotes the best result across either proprietary or open-source models. Models Overall Common Sense Embodied Reasoning Human Performance Random Guess Proprietary Models Claude-3.5-Sonnet [6] GPT-4o [40] GPT-5 [61] Open-source Models InternVL3.5-14B [82] InternVL3.5-30B-A3B [82] InternVL3.5-8B [82] Qwen2.5-VL-7B [10] Qwen2.5-VL-32B [10] Cosmos-Reason1-7B [7] InternVL3.5-38B [82] InternVL3.5-241B-A28B [82] Qwen3-VL-8B [9] Qwen3-VL-30B-A3B [9] Qwen2.5-VL-72B [10] Qwen3-VL-32B [9] Qwen3-VL-235B-A22B [9] 93.2 37.0 46.0 56.2 61.8 48.8 49.4 49.7 51.0 55.3 55.7 55.8 56.3 56.8 59.5 60.8 62.0 64.7 Space Time Physics 92.1 41.5 55.0 61.2 63.8 50.0 48.8 50.0 51.2 50.0 63.8 58.8 60.0 53.8 52.5 65.0 53.8 56.2 94.3 38. 46.6 57.0 65.8 51.3 55.7 55.0 48.7 62.1 55.7 59.7 57.4 58.4 60.4 57.7 67.8 69.5 95.0 38.3 46.9 59.7 61.5 47.3 46.0 44.7 39.8 48.7 46.0 49.6 53.5 50.9 58.4 57.5 59.7 61.9 Avg. 93.6 38.9 47.8 58.6 63.9 49.7 51.2 50.5 45.7 55.5 53.1 55.8 56.3 55.0 58.6 58.6 62.9 64.9 BD 94.0 25.0 29.0 44.0 38. 23.0 37.0 29.0 35.0 35.0 41.0 36.0 34.0 35.0 38.0 50.0 42.0 42.0 RV 95.5 50.0 74.5 68.2 85.5 80.0 74.5 75.5 87.3 93.6 91.8 82.7 78.2 89.1 90.9 91.8 90.9 93.6 RF 97.0 50.0 58.0 71.0 72.0 67.0 60.0 63.0 63.0 65.0 66.0 66.0 66.0 61.0 69.0 68.0 71.0 71.0 AB 90.0 26.9 28.0 45.0 45. 27.0 23.0 39.0 53.0 45.0 41.0 44.0 43.0 49.0 41.0 52.0 50.0 45.0 HA 89.0 25.0 38.0 55.0 72.0 56.0 55.0 49.0 60.0 56.0 59.0 69.0 75.0 75.0 73.0 70.0 72.0 76.0 AV 100.0 32.5 34.0 38.0 43.0 31.0 34.0 35.0 36.0 32.0 47.0 34.0 40.0 40.0 47.0 43.0 38.0 56.0 Avg. 88.9 35.2 44.1 53.8 59. 47.9 47.7 48.9 56.2 55.1 58.2 55.7 56.4 58.7 60.3 63.0 61.1 64.4 Table 6 Ablation study on thinking mode across different MLLMs on PAI-Bench-U. Performance gains and degradations are highlighted. Models Thinking Mode Overall Common Sense Embodied Reasoning Qwen3-VL-8B [9] Qwen3-VL-30B-A3B [9] Qwen3-VL-32B [9] Qwen3-VL-235B-A22B [9] GPT-5 [61] 56.8 57.3 (+0.5) 55.0 57.0 (+2.0) 59.5 57.3 (-2.2) 62.0 61.0 (-1.0) 64.7 63.7 (-1.0) 61.8 69.8 (+8.0) 58.6 56.1 (-2.5) 62.9 63.7 (+0.8) 64.9 66.4 (+1.5) 63.9 71.4 (+7.5) 58.7 57.7 (-1.0) 60.3 58.5 (-1.8) 61.1 58.4 (-2.7) 64.4 61.0 (-3.4) 59.7 68.2 (+8.5) Figure 7 Performance comparison across different frame counts on PAI-Bench-U. PAI-Bench-U is unbiased. Prevailing video understanding benchmarks often suffer from significant biases [32, 47, 65, 76]. Specifically, models frequently 1) exploit strong language priors to infer answers without grounding in visual data, and 2) leverage static bias, where questions are answerable from single frame while obviating the need for temporal context. We analyzed PAI-Bench-U for these issues by evaluating SOTA models with varying input frame counts in Figure 7. First, with zero frames (i.e., text-only input), model performance degraded to the level of random guessing, which confirms that PAI-Bench-U effectively neutralizes language priors, ensuring that performance gains are sourced from visual understanding. Furthermore, we observed substantial performance gap between the 1-frame and 32-frame inputs. This indicates that the tasks are not solvable with static information alone, validating PAI-Bench-Us requirement for temporal context comprehension. MLLMs lag significantly behind human performance, with no clear proprietary model advantage. The results in Table 5 highlight substantial performance gap between all models and human accuracy (93.2). This high performance validates PAI-Bench-U as well-posed benchmark; yet, the stark discrepancy underscores the significant limitations of current models in understanding capabilities in the Physical AI domain. Surprisingly, proprietary models do not uniformly outperform open-source models; for instance, Qwen3-VL-235B [9] achieves higher accuracy than GPT-5 [61]. This finding suggests that the Physical AI domain has not yet been primary data collection or optimization priority for the wider field. 10 PAI-Bench: Comprehensive Benchmark For Physical AI Textual thinking does not guarantee performance gains. We investigated whether introducing an additional thinking process during inference enhances performance. As presented in Table 6, the Qwen3-VL [9] series, which employs textual-only thinking, unexpectedly exhibited slight performance degradation, particularly in embodied reasoning tasks. We attribute this to the high demands of these tasks, which require perceiving fine-grained visual details, such as minute regions or subtle temporal changes. When vision modules fail to capture these nuances, subsequent textual reasoning operates without the necessary grounding, rendering it ineffective. This hypothesis is supported by GPT-5 [61]s results. We use medium reasoning effort for GPT-5 [61], and the model shows clear performance gains under this setting. Crucially, GPT-5s [61] reasoning involves both textual and visual thinking. This integrated visual reasoning appears to be an effective solution for capturing fine-grained information. Therefore, this finding underscores the necessity of advancing visual thinking capabilities within the Physical AI domain. 5. Further Analysis and Discussion Based on an in-depth analysis and cross-comparison of three tracks of PAI-Bench, we extract several common insights and subsequently synthesize central conclusion: In PAI-Bench-G, leading open-source VGMs, Physical AI remains largely unsolved domain. including Wan2.2-I2V-A14B [79] (81.6) and Cosmos-Predict2.5-2B [1] (81.4), achieve performance comparable to the proprietary Veo3 [21] (82.1), as shown in Table 3. similar trend is observed for MLLMs, where the open-source Qwen3-VL-235B-A22B [9] (64.7) narrows the performance gap to proprietary GPT-5 [61] (61.8) to just 2.9 points in Table 5. This lack of differentiation, coupled with generally low absolute performance, suggests one of two systemic challenges. 1) community-wide data gap may exist, as Physical AI has not been primary target for data collection or optimization. This would necessitate focused, large-scale data collection effort to advance the domain. 2) Alternatively, the data may exist, but both proprietary and open-source models are failing to learn the specific capabilities required by Physical AI. For instance, VGMs may not be learning the underlying principles of the physical world [19, 59, 88], while MLLMs may not be learning to leverage common sense to guide agent policies. Either scenario strongly indicates that the field of Physical AI remains in nascent stage. 6. Conclusion The evolution of AI toward embodied agency necessitates mastery of physical world dynamics. To this end, we introduced PAI-Bench, comprehensive benchmark evaluating the perception and prediction capabilities critical for this transition. By assessing performance across video generation (PAI-Bench-G), conditional control (PAI-Bench-C), and physical understanding (PAI-Bench-U), we established rigorous baseline for Physical AI. Our evaluation reveals that while VGMs achieve high visual fidelity, they frequently fail to maintain physical consistency. Conversely, MLLMs demonstrate significant performance deficit compared to human baselines in physical common sense reasoning and embodied forecasting. These findings confirm that Physical AI remains nascent and largely unsolved domain. We hope PAI-Bench serves as catalyst for future research to bridge this gap, shifting the focus from models that merely replicate the appearance of reality to those that internalize its underlying physics, enabling true understanding and robust action."
        },
        {
            "title": "Acknowledgements",
            "content": "We would like to thank NVIDIA Research, especially the Cosmos team, for their support, which led to the creation of PAI-Bench. We also thank Yin Cui, Jinwei Gu, Heng Wang, Prithvijit 11 PAI-Bench: Comprehensive Benchmark For Physical AI Chattopadhyay, Andrew Z. Wang, Imad El Hanafi, and Ming-Yu Liu for their valuable feedback and collaboration that helped shape the project. This research was supported in part by the National Science Foundation under Award #2427478 - CAREER Program and by the National Science Foundation and the Institute of Education Sciences, U.S. Department of Education, under Award #2229873 - National AI Institute for Exceptional Education. This project was also partially supported by cyberinfrastructure resources and services provided by the Georgia Institute of Technology. Also, we would like to thank the Cambrian [76] Team for their awesome template; our template is based on their work. 12 PAI-Bench: Comprehensive Benchmark For Physical AI"
        },
        {
            "title": "References",
            "content": "[1] N. Agarwal et al. Cosmos World Foundation Model Platform for Physical AI. In: CoRR abs/2501.03575 (2025). doi: 10.48550/ARXIV.2501.03575. arXiv: 2501.03575. [2] AgiBot-World-Contributors et al. AgiBot World Colosseo: Large-scale Manipulation Platform for Scalable and Intelligent Embodied Systems. In: CoRR abs/2503.06669 (2025). doi: 10.48550/ARXIV.2503.06669. arXiv: 2503.06669. [3] H. A. Alhaija et al. Cosmos-Transfer1: Conditional World Generation with Adaptive Multimodal Control. In: CoRR abs/2503.14492 (2025). doi: 10.48550/ARXIV.2503. 14492. arXiv: 2503.14492. [4] A. Ali et al. World Simulation with Video Foundation Models for Physical AI. In: arXiv preprint arXiv:2511.00062 (2025). [5] Alibaba PAI. Wan2.2-Fun: Alibaba-PAI Collection. https : / / huggingface . co / collections/alibaba-pai/wan22-fun. Accessed: 2025-11-12. 2025. [6] Anthropic. Claude 3.5 Sonnet. 2024. [7] A. G. Azzolini et al. Cosmos-Reason1: From Physical Common Sense To Embodied Reasoning. In: CoRR abs/2503.15558 (2025). doi: 10.48550/ARXIV.2503.15558. arXiv: 2503.15558. J. Bai et al. Qwen-VL: Frontier Large Vision-Language Model with Versatile Abilities. In: CoRR abs/2308.12966 (2023). doi: 10.48550/ARXIV.2308.12966. arXiv: 2308. 12966. J. Bai et al. Qwen-VL: Versatile Vision-Language Model for Understanding, Localization, Text Reading, and Beyond. In: arXiv preprint arXiv:2308.12966 (2023). [8] [9] [10] S. Bai et al. Qwen2.5-VL Technical Report. In: CoRR abs/2502.13923 (2025). doi: 10.48550/ARXIV.2502.13923. arXiv: 2502.13923. [11] H. Bansal et al. VideoPhy-2: Challenging Action-Centric Physical Commonsense Evaluation in Video Generation. In: CoRR abs/2503.06800 (2025). doi: 10.48550/ARXIV. 2503.06800. arXiv: 2503.06800. [12] A. Bar et al. Navigation World Models. In: IEEE/CVF Conference on Computer Vision and Pattern Recognition, CVPR 2025, Nashville, TN, USA, June 11-15, 2025. Computer Vision Foundation / IEEE, 2025, pp. 1579115801. doi: 10.1109/CVPR52734.2025.01472. [13] S. Belkhale et al. RT-H: Action Hierarchies Using Language. 2024. arXiv: 2403.01823 [cs.RO]. [14] F. Bordes et al. IntPhys 2: Benchmarking Intuitive Physics Understanding In Complex Synthetic Environments. In: CoRR abs/2506.09849 (2025). doi: 10.48550/ARXIV. 2506.09849. arXiv: 2506.09849. [15] M. Caron et al. Emerging Properties in Self-Supervised Vision Transformers. In: 2021. [16] J. Cen et al. WorldVLA: Towards Autoregressive Action World Model. In: CoRR abs/2506.21539 (2025). doi: 10.48550/ARXIV.2506.21539. arXiv: 2506.21539. [17] H. Chen et al. VideoCrafter2: Overcoming Data Limitations for High-Quality Video Diffusion Models. In: IEEE/CVF Conference on Computer Vision and Pattern Recognition, CVPR 2024, Seattle, WA, USA, June 16-22, 2024. IEEE, 2024, pp. 73107320. doi: 10.1109/CVPR52733.2024.00698. [18] S. Chen et al. Video Depth Anything: Consistent Depth Estimation for Super-Long Videos. In: IEEE/CVF Conference on Computer Vision and Pattern Recognition, CVPR 2025, Nashville, TN, USA, June 11-15, 2025. Computer Vision Foundation / IEEE, 2025, pp. 2283122840. doi: 10.1109/CVPR52734.2025.02126. 13 PAI-Bench: Comprehensive Benchmark For Physical AI [19] Z. Chen et al. How far are we to gpt-4v? closing the gap to commercial multimodal models with open-source suites. In: Science China Information Sciences 67.12 (2024), p. 220101. [20] W. Chow et al. PhysBench: Benchmarking and Enhancing Vision-Language Models for Physical World Understanding. In: The Thirteenth International Conference on Learning Representations, ICLR 2025, Singapore, April 24-28, 2025. OpenReview.net, 2025. [21] G. DeepMind. Veo: text-to-video generation system. Tech. rep. Technical Report. Google DeepMind, 2025. [22] H. Duan et al. WorldScore: Unified Evaluation Benchmark for World Generation. In: CoRR abs/2504.00983 (2025). doi: 10.48550/ARXIV.2504.00983. arXiv: 2504. 00983. [23] D. Eigen, C. Puhrsch, and R. Fergus. Depth Map Prediction from Single Image using Multi-Scale Deep Network. In: Advances in Neural Information Processing Systems 27: Annual Conference on Neural Information Processing Systems 2014, December 8-13 2014, Montreal, Quebec, Canada. Ed. by Z. Ghahramani et al. 2014, pp. 23662374. [24] A. E. Elo. The proposed uscf rating system, its development, theory, and applications. In: Chess Life 22.8 (1967), pp. 242247. [25] Y. Fang et al. Perceptual Quality Assessment of Smartphone Photography. In: 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition, CVPR 2020, Seattle, WA, USA, June 13-19, 2020. Computer Vision Foundation / IEEE, 2020, pp. 36743683. doi: 10.1109/CVPR42600.2020.00373. [26] W. Feng et al. TC-Bench: Benchmarking Temporal Compositionality in Text-to-Video and Image-to-Video Generation. In: CoRR abs/2406.08656 (2024). doi: 10.48550/ ARXIV.2406.08656. arXiv: 2406.08656. [27] A. Foss et al. CausalVQA: Physically Grounded Causal Reasoning Benchmark for Video Models. In: CoRR abs/2506.09943 (2025). doi: 10.48550/ARXIV.2506.09943. arXiv: 2506.09943. [28] C. Fu et al. Video-MME: The First-Ever Comprehensive Evaluation Benchmark of Multimodal LLMs in Video Analysis. In: IEEE/CVF Conference on Computer Vision and Pattern Recognition, CVPR 2025, Nashville, TN, USA, June 11-15, 2025. Computer Vision Foundation / IEEE, 2025, pp. 2410824118. doi: 10.1109/CVPR52734.2025.02245. [29] C. Fu et al. Video-MME: The First-Ever Comprehensive Evaluation Benchmark of Multimodal LLMs in Video Analysis. In: IEEE/CVF Conference on Computer Vision and Pattern Recognition, CVPR 2025, Nashville, TN, USA, June 11-15, 2025. Computer Vision Foundation / IEEE, 2025, pp. 2410824118. doi: 10.1109/CVPR52734.2025.02245. [30] L. Fu et al. OCRBench v2: An Improved Benchmark for Evaluating Large Multimodal Models on Visual Text Localization and Reasoning. In: CoRR abs/2501.00321 (2025). doi: 10.48550/ARXIV.2501.00321. arXiv: 2501.00321. [31] S. Fu et al. DreamSim: Learning New Dimensions of Human Visual Similarity using Synthetic Data. In: Advances in Neural Information Processing Systems 36: Annual Conference on Neural Information Processing Systems 2023, NeurIPS 2023, New Orleans, LA, USA, December 10 - 16, 2023. Ed. by A. Oh et al. 2023. [32] Y. Goyal et al. Making the in VQA Matter: Elevating the Role of Image Understanding in Visual Question Answering. In: 2017 IEEE Conference on Computer Vision and Pattern Recognition, CVPR 2017, Honolulu, HI, USA, July 21-26, 2017. IEEE Computer Society, 2017, pp. 63256334. doi: 10.1109/CVPR.2017.670. PAI-Bench: Comprehensive Benchmark For Physical AI [33] K. Grauman et al. Ego-Exo4D: Understanding Skilled Human Activity from Firstand Third-Person Perspectives. In: IEEE/CVF Conference on Computer Vision and Pattern Recognition, CVPR 2024, Seattle, WA, USA, June 16-22, 2024. IEEE, 2024, pp. 19383 19400. doi: 10.1109/CVPR52733.2024.01834. [34] D. Ha and J. Schmidhuber. World Models. In: (2018). doi: 10 . 5281 / ZENODO . 1207631. [35] Y. HaCohen et al. LTX-Video: Realtime Video Latent Diffusion. In: CoRR abs/2501.00103 (2025). doi: 10.48550/ARXIV.2501.00103. arXiv: 2501.00103. [36] W. Hong et al. CogVideo: Large-scale Pretraining for Text-to-Video Generation via Transformers. In: The Eleventh International Conference on Learning Representations, ICLR 2023, Kigali, Rwanda, May 1-5, 2023. OpenReview.net, 2023. [37] K. Hu et al. Video-MMMU: Evaluating Knowledge Acquisition from Multi-Discipline Professional Videos. In: CoRR abs/2501.13826 (2025). doi: 10.48550/ARXIV.2501. 13826. arXiv: 2501.13826. [38] Z. Huang et al. VBench: Comprehensive Benchmark Suite for Video Generative Models. In: IEEE/CVF Conference on Computer Vision and Pattern Recognition, CVPR 2024, Seattle, WA, USA, June 16-22, 2024. IEEE, 2024, pp. 2180721818. doi: 10.1109/CVPR52733. 2024.02060. [39] Z. Huang et al. VBench++: Comprehensive and Versatile Benchmark Suite for Video Generative Models. In: CoRR abs/2411.13503 (2024). doi: 10.48550/ARXIV.2411. 13503. arXiv: 2411.13503. [40] A. Hurst et al. GPT-4o System Card. In: CoRR abs/2410.21276 (2024). doi: 10.48550/ ARXIV.2410.21276. arXiv: 2410.21276. [41] P. Ji et al. T2VBench: Benchmarking Temporal Dynamics for Text-to-Video Generation. In: IEEE/CVF Conference on Computer Vision and Pattern Recognition, CVPR 2024 - Workshops, Seattle, WA, USA, June 17-18, 2024. IEEE, 2024, pp. 53255335. doi: 10.1109/CVPRW63382.2024.00541. J. Ke et al. MUSIQ: Multi-scale Image Quality Transformer. In: CoRR abs/2108.05997 (2021). arXiv: 2108.05997. [42] [43] W. Kong et al. HunyuanVideo: Systematic Framework For Large Video Generative Models. In: CoRR abs/2412.03603 (2024). doi: 10.48550/ARXIV.2412.03603. arXiv: 2412.03603. [44] B. Krojer et al. Shortcut-aware Video-QA Benchmark for Physical Understanding via Minimal Video Pairs. In: CoRR abs/2506.09987 (2025). doi: 10.48550/ARXIV.2506. 09987. arXiv: 2506.09987. [45] W. Kwon et al. Efficient Memory Management for Large Language Model Serving with PagedAttention. In: Proceedings of the ACM SIGOPS 29th Symposium on Operating Systems Principles. 2023. [46] LAION-AI. aesthetic-predictor. https : / / github . com / LAION - AI / aesthetic - [47] predictor. 2022. J. Lei, T. L. Berg, and M. Bansal. Revealing Single Frame Bias for Video-and-Language Learning. In: Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), ACL 2023, Toronto, Canada, July 9-14, 2023. Ed. by A. Rogers, J. L. Boyd-Graber, and N. Okazaki. Association for Computational Linguistics, 2023, pp. 487507. doi: 10.18653/V1/2023.ACL-LONG.29. 15 PAI-Bench: Comprehensive Benchmark For Physical AI [48] J. Li et al. Science-T2I: Addressing Scientific Illusions in Image Synthesis. In: IEEE/CVF Conference on Computer Vision and Pattern Recognition, CVPR 2025, Nashville, TN, USA, June 11-15, 2025. Computer Vision Foundation / IEEE, 2025, pp. 27342744. doi: 10.1109/CVPR52734.2025.00261. [49] K. Li et al. MVBench: Comprehensive Multi-modal Video Understanding Benchmark. In: IEEE/CVF Conference on Computer Vision and Pattern Recognition, CVPR 2024, Seattle, WA, USA, June 16-22, 2024. IEEE, 2024, pp. 2219522206. doi: 10.1109/CVPR52733. 2024.02095. [50] Z. Li et al. AMT: All-Pairs Multi-Field Transforms for Efficient Frame Interpolation. In: IEEE/CVF Conference on Computer Vision and Pattern Recognition, CVPR 2023, Vancouver, BC, Canada, June 17-24, 2023. IEEE, 2023, pp. 98019810. doi: 10.1109/CVPR52729. 2023.00945. [51] H. Liu et al. Visual Instruction Tuning. 2023. [52] S. Liu et al. Grounding DINO: Marrying DINO with Grounded Pre-training for OpenSet Object Detection. In: Computer Vision - ECCV 2024 - 18th European Conference, Milan, Italy, September 29-October 4, 2024, Proceedings, Part XLVII. Ed. by A. Leonardis et al. Vol. 15105. Lecture Notes in Computer Science. Springer, 2024, pp. 3855. doi: 10.1007/978-3-031-72970-6_3. [53] Y. Liu et al. EvalCrafter: Benchmarking and Evaluating Large Video Generation Models. In: IEEE/CVF Conference on Computer Vision and Pattern Recognition, CVPR 2024, Seattle, WA, USA, June 16-22, 2024. IEEE, 2024, pp. 2213922149. doi: 10.1109/CVPR52733. 2024.02090. [54] Y. Liu et al. FETV: Benchmark for Fine-Grained Evaluation of Open-Domain Text-toVideo Generation. In: Advances in Neural Information Processing Systems 36: Annual Conference on Neural Information Processing Systems 2023, NeurIPS 2023, New Orleans, LA, USA, December 10 - 16, 2023. Ed. by A. Oh et al. 2023. [55] Z. Liu, A. Bahety, and S. Song. REFLECT: Summarizing Robot Experiences for Failure Explanation and Correction. In: Conference on Robot Learning, CoRL 2023, 6-9 November 2023, Atlanta, GA, USA. Ed. by J. Tan, M. Toussaint, and K. Darvish. Vol. 229. Proceedings of Machine Learning Research. PMLR, 2023, pp. 34683484. [56] K. Mangalam, R. Akshulakov, and J. Malik. EgoSchema: Diagnostic Benchmark for Very Long-form Video Language Understanding. In: Advances in Neural Information Processing Systems 36: Annual Conference on Neural Information Processing Systems 2023, NeurIPS 2023, New Orleans, LA, USA, December 10 - 16, 2023. Ed. by A. Oh et al. 2023. [57] F. Meng et al. Towards World Simulator: Crafting Physical Commonsense-Based Benchmark for Video Generation. In: CoRR abs/2410.05363 (2024). doi: 10.48550/ARXIV. 2410.05363. arXiv: 2410.05363. [58] A. Miech et al. HowTo100M: Learning Text-Video Embedding by Watching Hundred Million Narrated Video Clips. In: ICCV. 2019. [59] S. Motamed et al. Do generative video models understand physical principles? 2025. arXiv: 2501.09038 [cs.CV]. [60] A. ONeill et al. Open X-Embodiment: Robotic Learning Datasets and RT-X Models : Open X-Embodiment Collaboration. In: IEEE International Conference on Robotics and Automation, ICRA 2024, Yokohama, Japan, May 13-17, 2024. IEEE, 2024, pp. 68926903. doi: 10.1109/ICRA57147.2024.10611477. [61] OpenAI. GPT-5 System Card. Accessed: 2025-10-16. Aug. 2025. [62] OpenAI. Sora 2 System Card. Tech. rep. System Card. OpenAI, Sept. 2025. 16 PAI-Bench: Comprehensive Benchmark For Physical AI [63] OpenAI. Video generation models as world simulators. Feb. 2024. [64] A. Radford et al. Learning transferable visual models from natural language supervision. In: 2021. [65] S. Ramakrishnan, A. Agrawal, and S. Lee. Overcoming Language Priors in Visual Question Answering with Adversarial Regularization. In: Advances in Neural Information Processing Systems 31: Annual Conference on Neural Information Processing Systems 2018, NeurIPS 2018, December 3-8, 2018, Montral, Canada. Ed. by S. Bengio et al. 2018, pp. 15481558. [66] N. Ravi et al. SAM 2: Segment Anything in Images and Videos. In: The Thirteenth International Conference on Learning Representations, ICLR 2025, Singapore, April 24-28, 2025. OpenReview.net, 2025. [67] R. Riochet et al. IntPhys: Framework and Benchmark for Visual Intuitive Physics Reasoning. In: CoRR abs/1803.07616 (2018). arXiv: 1803.07616. [68] N. Ruiz et al. DreamBooth: Fine Tuning Text-to-image Diffusion Models for SubjectDriven Generation. In: 2023. [69] P. Sermanet et al. RoboVQA: Multimodal Long-Horizon Reasoning for Robotics. In: IEEE International Conference on Robotics and Automation, ICRA 2024, Yokohama, Japan, May 13-17, 2024. IEEE, 2024, pp. 645652. doi: 10.1109/ICRA57147.2024.10610216. [70] Z. Shangguan et al. TOMATO: Assessing Visual Temporal Reasoning Capabilities in Multimodal Foundation Models. In: The Thirteenth International Conference on Learning Representations, ICLR 2025, Singapore, April 24-28, 2025. OpenReview.net, 2025. [71] M. Shi et al. Is Diversity All You Need for Scalable Robotic Manipulation? In: CoRR abs/2507.06219 (2025). doi: 10.48550/ARXIV.2507.06219. arXiv: 2507.06219. [72] E. Song et al. Video-MMLU: Massive Multi-Discipline Lecture Understanding Benchmark. In: CoRR abs/2504.14693 (2025). doi: 10.48550/ARXIV.2504.14693. arXiv: 2504.14693. [73] G. R. Team. Gemini Robotics: Bringing AI into the Physical World. In: CoRR abs/2503.20020 (2025). doi: 10.48550/ARXIV.2503.20020. arXiv: 2503.20020. [74] V. Team et al. GLM-4.5V and GLM-4.1V-Thinking: Towards Versatile Multimodal Reasoning with Scalable Reinforcement Learning. In: arXiv preprint arXiv:2507.01006 (2025). [75] H. Teng et al. MAGI-1: Autoregressive Video Generation at Scale. In: CoRR abs/2505.13211 (2025). doi: 10.48550/ARXIV.2505.13211. arXiv: 2505.13211. [76] P. Tong et al. Cambrian-1: Fully Open, Vision-Centric Exploration of Multimodal LLMs. In: Advances in Neural Information Processing Systems 38: Annual Conference on Neural Information Processing Systems 2024, NeurIPS 2024, Vancouver, BC, Canada, December 10 - 15, 2024. Ed. by A. Globersons et al. 2024. [77] C. Van Rijsbergen. Information Retrieval. Butterworths, 1979. isbn: 9780408709293. [78] H. R. Walke et al. BridgeData V2: Dataset for Robot Learning at Scale. In: Conference on Robot Learning, CoRL 2023, 6-9 November 2023, Atlanta, GA, USA. Ed. by J. Tan, M. Toussaint, and K. Darvish. Vol. 229. Proceedings of Machine Learning Research. PMLR, 2023, pp. 17231736. [79] A. Wang et al. Wan: Open and Advanced Large-Scale Video Generative Models. In: CoRR abs/2503.20314 (2025). doi: 10.48550/ARXIV.2503.20314. arXiv: 2503.20314. J. Wang et al. WISA: World Simulator Assistant for Physics-Aware Text-to-Video Generation. In: CoRR abs/2503.08153 (2025). doi: 10.48550/ARXIV.2503.08153. arXiv: 2503.08153. [80] 17 PAI-Bench: Comprehensive Benchmark For Physical AI [81] P. Wang et al. Qwen2-VL: Enhancing Vision-Language Models Perception of the World at Any Resolution. In: CoRR abs/2409.12191 (2024). doi: 10.48550/ARXIV.2409. 12191. arXiv: 2409.12191. [82] W. Wang et al. InternVL3.5: Advancing Open-Source Multimodal Models in Versatility, Reasoning, and Efficiency. In: CoRR abs/2508.18265 (2025). doi: 10.48550/ARXIV. 2508.18265. arXiv: 2508.18265. [83] X. Wang et al. HoloAssist: an Egocentric Human Interaction Dataset for Interactive AI Assistants in the Real World. In: IEEE/CVF International Conference on Computer Vision, ICCV 2023, Paris, France, October 1-6, 2023. IEEE, 2023, pp. 2021320224. doi: 10.1109/ICCV51070.2023.01854. [84] Y. Wang et al. InternVid: Large-scale Video-Text Dataset for Multimodal Understanding and Generation. In: The Twelfth International Conference on Learning Representations, ICLR 2024, Vienna, Austria, May 7-11, 2024. OpenReview.net, 2024. [85] Z. Wang et al. Image quality assessment: from error visibility to structural similarity. In: IEEE Trans. Image Process. 13.4 (2004), pp. 600612. doi: 10.1109/TIP.2003. 819861. [86] Z. Wang et al. Image quality assessment: from error visibility to structural similarity. In: IEEE Trans. Image Process. 13.4 (2004), pp. 600612. doi: 10.1109/TIP.2003. 819861. [87] Z. Weng et al. Open-VCLIP: Transforming CLIP to an Open-vocabulary Video Model via Interpolated Weight Optimization. In: International Conference on Machine Learning, ICML 2023, 23-29 July 2023, Honolulu, Hawaii, USA. Ed. by A. Krause et al. Vol. 202. Proceedings of Machine Learning Research. PMLR, 2023, pp. 3697836989. [88] T. Wiedemer et al. Video models are zero-shot learners and reasoners. In: CoRR abs/2509.20328 (2025). doi: 10.48550/ARXIV.2509.20328. arXiv: 2509.20328. [89] H. Wu. Open Source Deep End-to-End Video Quality Assessment Toolbox. 2022. [90] H. Wu et al. Exploring Video Quality Assessment on User Generated Contents from Aesthetic and Technical Perspectives. In: IEEE/CVF International Conference on Computer Vision, ICCV 2023, Paris, France, October 1-6, 2023. IEEE, 2023, pp. 2008720097. doi: 10.1109/ICCV51070.2023.01843. [91] H. Wu et al. FAST-VQA: Efficient End-to-End Video Quality Assessment with Fragment Sampling. In: Computer Vision - ECCV 2022 - 17th European Conference, Tel Aviv, Israel, October 23-27, 2022, Proceedings, Part VI. Ed. by S. Avidan et al. Vol. 13666. Lecture Notes in Computer Science. Springer, 2022, pp. 538554. doi: 10.1007/978-3-03120068-7_31. [92] H. Wu et al. LongVideoBench: Benchmark for Long-context Interleaved Video-Language Understanding. In: Advances in Neural Information Processing Systems 38: Annual Conference on Neural Information Processing Systems 2024, NeurIPS 2024, Vancouver, BC, Canada, December 10 - 15, 2024. Ed. by A. Globersons et al. 2024. J. Xing et al. DynamiCrafter: Animating Open-Domain Images with Video Diffusion Priors. In: Computer Vision - ECCV 2024 - 18th European Conference, Milan, Italy, September 29-October 4, 2024, Proceedings, Part XLVI. Ed. by A. Leonardis et al. Vol. 15104. Lecture Notes in Computer Science. Springer, 2024, pp. 399417. doi: 10.1007/978-3-03172952-2_23. [93] [94] A. Yang et al. Qwen3 Technical Report. In: CoRR abs/2505.09388 (2025). doi: 10. 48550/ARXIV.2505.09388. arXiv: 2505.09388. 18 PAI-Bench: Comprehensive Benchmark For Physical AI [95] J. Yang et al. Generalized Predictive Model for Autonomous Driving. In: IEEE/CVF Conference on Computer Vision and Pattern Recognition, CVPR 2024, Seattle, WA, USA, June 16-22, 2024. IEEE, 2024, pp. 1466214672. doi: 10.1109/CVPR52733.2024.01389. [96] L. Yang et al. Depth Anything V2. In: Advances in Neural Information Processing Systems 38: Annual Conference on Neural Information Processing Systems 2024, NeurIPS 2024, Vancouver, BC, Canada, December 10 - 15, 2024. Ed. by A. Globersons et al. 2024. [97] Z. Yang et al. CogVideoX: Text-to-Video Diffusion Models with An Expert Transformer. In: The Thirteenth International Conference on Learning Representations, ICLR 2025, Singapore, April 24-28, 2025. OpenReview.net, 2025. [98] S. Yuan et al. ChronoMagic-Bench: Benchmark for Metamorphic Evaluation of Textto-Time-lapse Video Generation. In: Advances in Neural Information Processing Systems 38: Annual Conference on Neural Information Processing Systems 2024, NeurIPS 2024, Vancouver, BC, Canada, December 10 - 15, 2024. Ed. by A. Globersons et al. 2024. [99] K. Zhang et al. LMMs-Eval: Reality Check on the Evaluation of Large Multimodal Models. In: Findings of the Association for Computational Linguistics: NAACL 2025, Albuquerque, New Mexico, USA, April 29 - May 4, 2025. Ed. by L. Chiruzzo, A. Ritter, and L. Wang. Association for Computational Linguistics, 2025, pp. 881916. doi: 10.18653/V1/ 2025.FINDINGS-NAACL.51. [100] R. Zhang et al. The Unreasonable Effectiveness of Deep Features as Perceptual Metric. In: 2018 IEEE Conference on Computer Vision and Pattern Recognition, CVPR 2018, Salt Lake City, UT, USA, June 18-22, 2018. Computer Vision Foundation / IEEE Computer Society, 2018, pp. 586595. doi: 10.1109/CVPR.2018.00068. [101] D. Zheng et al. VBench-2.0: Advancing Video Generation Benchmark Suite for Intrinsic Faithfulness. In: CoRR abs/2503.21755 (2025). doi: 10.48550/ARXIV.2503.21755. arXiv: 2503.21755. J. Zhou et al. MLVU: Benchmarking Multi-task Long Video Understanding. In: IEEE/CVF Conference on Computer Vision and Pattern Recognition, CVPR 2025, Nashville, TN, USA, June 11-15, 2025. Computer Vision Foundation / IEEE, 2025, pp. 1369113701. doi: 10.1109/CVPR52734.2025.01278. [102] 19 PAI-Bench: Comprehensive Benchmark For Physical AI A. More Details about PAI-Bench-G A.1. Data Source We curated data by integrating established open-source benchmarks with web-sourced content. The open-source component comprises EgoExo4D [33], HowTo100M [58], Physics-IQ [59], WISA-80K [80], Agibot [2], BridgeDatav2 [78], and Open X-Embodiment [60]. Complementing these, we also collected raw video data from YouTube and stock footage platforms, including Pexels, Pixabay, Free-video, MixKit, and Free-stock Video. A.2. Details of Quality Score Calculation For Quality Score, we adopt the evaluation protocol from VBench and VBench++ [38, 39, 101]. The specific components are defined as follows: Subject Consistency. Evaluates the stability of the main subjects identity across frames. We extract frame-level features using DINO [15], which offers robust identity-sensitive representations [68]. For video with  frames, the score is computed as: subject = 1  1  =2 1 2 (1,  + 1, ), (1) where  represents the unit-normalized DINO feature of the -th frame, and , denotes the dot product. This metric averages the similarity of each frame with both the initial frame and its immediate predecessor. Background Consistency. Assesses the temporal stability of the background scene, independent of foreground motion. We utilize the CLIP image encoder [64] to extract frame-level features. The score is calculated as: background = 1  1  = 1 2 (1,  + 1, ), (2) where  denotes the unit-normalized CLIP feature of the -th frame. Similar to subject consistency, this metric aggregates similarities relative to the first and preceding frames. Motion Smoothness. Quantifies the physical plausibility and temporal coherence of motion. Following standard motion priors, we assume real-world motion is locally linear or quadratic. We subsample the generated video [ 0, 1, . . . , 2] by dropping odd-indexed frames, reconstruct them using frame interpolation model [50] to obtain [ 1, 3, . . . ], and compute the Mean Absolute Error (MAE) between the original and reconstructed frames: smoothness = 1 /2 /2 =1 21 211. The final score is normalized to [0, 1]: where higher values indicate smoother motion dynamics. smoothness-norm = 1 smoothness 255 , (3) (4) Aesthetic Quality. Measures visual appeal, encompassing composition, color harmony, and artistic quality. We utilize the LAION aesthetic predictor [46] to score each frame on scale of [0, 10]. These scores are linearly normalized to [0, 1] and averaged across all frames to derive the video-level metric. 20 PAI-Bench: Comprehensive Benchmark For Physical AI Table 7 Generation parameters for the evaluated VGMs. Model Version #Frames FPS Width Height CogVideoX [97] CogVideoX1.5 [97] Cosmos-Predict2-2B [4] Cosmos-Predict2-14B [4] Cosmos-Predict2.5-2B [4] DynamicCrafter [93] HunyuanVideo-I2V [43] LTX-Video-2B [35] LTX-Video-13B [35] MAGI-1-4.5B [75] MAGI-1-24B [75] Veo3 [21] Wan2.1-I2V-14B [79] Wan2.2-TI2V-5B [79] Wan2.2-I2V-A14B [79] - - - - base/post-trained DynamiCrafter_1024 - - - - - veo-3.0-generate-001 Wan2.1-I2V-14B-720P - - 49 81 93 93 93 16 129 121 121 120 120 192 81 121 81 16 16 16 16 16 8 24 30 30 24 24 24 16 24 16 720 1360 1280 1280 1280 1024 1184 1216 1216 720 1280 1280 1280 1248 480 768 704 704 704 576 768 704 704 720 720 720 720 704 720 Imaging Quality. Evaluates low-level image fidelity, specifically distortions such as overexposure, noise, and blur. We employ the MUSIQ predictor [42] (trained on SPAQ [25]) to obtain frame-level scores in [0, 100]. The final score is the average of these normalized values. Overall Consistency. Measures the semantic and stylistic alignment between the generated video and the textual prompt. We employ the video-text consistency score from ViCLIP [84], which directly assesses the correspondence between the video content and the input description. Image-to-Video Subject. Ensures the subject in the generated video remains faithful to the input reference image. Using DINO [15] features, we compute the similarity between the input image (img) and video frames (): i2v_subject = 1  1  =2 1 2 (cid:0)img,  + 1, (cid:1) . (5) The final score is averaged across all image-conditioned samples. Image-to-Video Background. Verifies consistency between the generated background and the input image environment. This is critical for inputs emphasizing scene layout. We extract features using DreamSim [31], which is sensitive to layout variations, and compute: i2v_bg = 1  1  =2 1 2 (cid:0)img,  + 1, (cid:1) , (6) where img and  are the unit-normalized DreamSim features of the reference image and the -th frame, respectively. A.3. Details of Domain Score Calculation The Domain Score quantifies the adherence of generated videos to domain-specific physical and semantic constraints. We employ Qwen3-VL-235B-A22B-Instruct [9] as an automated evaluator, querying the model with curated set of verification questions derived from the ground truth. The score is computed as the accuracy of the models binary responses against the expected answers. For inference, we uniformly sample frames at 2 fps and utilize greedy decoding to ensure deterministic evaluation. The specific prompt template used for evaluation is illustrated in Figure 8. 21 PAI-Bench: Comprehensive Benchmark For Physical AI Prompt Template for PAI-Bench-G Domain Score Evaluation You are helpful AI assistant that answers questions about videos. Answer with just YES or NO. Ill show you video with several frames. Please look carefully at all frames to understand whats happening in the video, then answer the question about the video with either YES or NO. Input {Sampled Frames 0} {Sampled Frames 1} . . . {Sampled Frames } Question: Question: {Question} Please answer with YES or NO and explain your reasoning. Figure 8 Prompt template utilized for Domain Score evaluation. The model receives uniformly sampled frames and specific constraint question to verify physical compliance. A.4. Details about Experiments Model Configurations. To ensure reproducibility, we detail the inference specifications for all evaluated VGMs in Table 7. We report the specific checkpoint versions alongside their corresponding spatial resolutions ( ) and temporal settings (frame count and FPS) used during the generation process. Human Evaluation Protocol. To validate our automated metrics against human preferences, we conducted pairwise comparison study using web-based interface, as shown in Fig. 20. Annotators were presented with text prompt, reference image, and two generated videos (labeled and B). Evaluation focused on two distinct dimensions: (1) Video Quality, which assesses visual coherence, motion smoothness, and clarity (correlating with our Quality Score); and (2) Physical Plausibility, which evaluates adherence to physical laws and domain-specific realism (correlating with our Domain Score). For each dimension, participants selected one of four outcomes: Better, Better, Both Good, or Both Bad. ELO Rating Formulation. We quantified model rankings using an ELO rating system initialized at 1000 with  of 32. Pairwise comparisons are converted into numerical scores: definitive preference is assigned 1.0 to the chosen model and 0.0 to the other, while ties expressed as Both Good or Both Bad are assigned 0.5 to each. We maintained independent ELO trackers for video quality and physical plausibility. For the Overall score, we computed separate ELO rating by averaging the quality and plausibility preference scores for each comparison and using these averaged preference values as the inputs to the ELO update process. We present the detailed ELO rating results in Table 8. A.5. Qualitative Visualization We present qualitative visualizations of generated samples to illustrate model performance across the diverse domains of PAI-Bench-G. Representative examples corresponding to autonomous vehicles, robotics, industrial settings, common sense reasoning, human activity, and physical dynamics are provided in assets 11, 12, 13, 14, 15, and 16, respectively. PAI-Bench: Comprehensive Benchmark For Physical AI Table 8 Human-study (ELO) vs. MLLM evaluation on PAI-Bench-G. Models ELO Scores Evaluation Scores Overall Domain Quality Overall Domain Quality Source Videos Veo3 [21] DynamicCrafter [93] HunyuanVideo-I2V [43] CogVideoX1.5 [97] MAGI-1-24B [75] Wan2.1-I2V-14B [79] Cosmos-Predict2.5-2B [4] Wan2.2-I2V-A14B [79] 1225.0 1112.2 734.9 964.3 907.9 988.9 976.0 1008.1 1082.6 1278.1 1122.8 749.9 946.6 896.4 969.3 961.4 968.5 1106.9 1176.0 1102.9 718.9 981.1 918.1 1007.7 988.7 1047.8 1058.7 83.9 82.2 68.3 77.6 78.4 79.4 80.8 81.4 82.3 89.8 86.8 63.0 77.1 80.3 82.4 84.3 84.9 87. 78.0 77.6 73.7 78.0 76.6 76.5 77.3 78.0 77.5 B. PAI-Bench-C Score Calculation Details To comprehensively evaluate conditional VGMs, we propose metric suite assessing three critical dimensions: 1) control fidelity (faithfulness to input signals), 2) visual quality, and 3) generative diversity under identical conditions. unified formulation of the extraction and fidelity operators, alongside the complete scoring pipeline is in Algorithm 1. B.1. Control Fidelity We quantify the alignment between generated videos () and reference control signals (  ) by projecting both into shared representation spaces, specifically varying levels of abstraction including low-frequency structure, edges, depth geometry, and semantic segmentation. Vis Alignment: We apply the same blurring operation to both  and   . We then compute the Structural Similarity Index Measure (SSIM) [86] between the blurred representations, averaging scores across the dataset. Edge Consistency: We evaluate boundary alignment by extracting binary edge maps from  and   using the Canny edge detector. The alignment is measured via the classification F1 score [77] on the pixel-wise binary maps. Geometric Fidelity: To measure 3D geometric consistency, we extract depth maps using DepthAnythingV2 [96]. We calculate the scale-invariant Root Mean Squared Error (si-RMSE) [23] between depth estimations of  and   . Semantic Alignment: We assess semantic layout consistency using segmentation pipeline powered by GroundingDINO [52] and SAM2 [66]. To mitigate redundancy from open-set detection, instance masks are aggregated by caption phrase to form class-level masks. We establish correspondence between  and   masks via an Intersection over Union (IoU) matching algorithm. Matches with an IoU < 0.1 are filtered out, and the final score is reported as the mean IoU (mIoU) over valid correspondences. B.2. Visual Quality We assess the aesthetic and technical fidelity of generated videos using DOVER-technical [8991], reference-free video quality assessment metric. We compute the mean score across the entire dataset. Higher values correspond to superior visual clarity and stability. PAI-Bench: Comprehensive Benchmark For Physical AI Algorithm 1 PAI-Bench-C Evaluation Pipeline Input: Conditional video , generated video , # prompts . Modalities = {Blur, Edge, Depth, Seg}. Operators defined as pairs (, ) for  M: Blur: Kernel SSIM Edge: Canny F1 Score Depth: VideoDepthAnything si-RMSE Seg: GroundingDINO+SAM2 mIoU Extract features:   (); Compute fidelity:   (, ) 1: for each modality  do 2: 3: 4: end for 5: Align {Blur, Edge, Depth, Seg}   ( ) Generate  samples {  (1) , . . . ,  (  ) } (cid:205)<  LPIPS(  () ,  ( ) ) Div 2 6: for each condition  do 7: 8: 9: end for 10: Div 1 (cid:205) Div  ( 1) 11: Qual DOVER( ) Output: Align, Div, Qual B.3. Generation Diversity Stage 1: Condition Alignment Stage 2: Generation Diversity Stage 3: Visual Quality To quantify the diversity of models outputs under identical conditioning, we employ the Learned Perceptual Image Patch Similarity (LPIPS) metric [100]. For fixed condition, we generate set of  videos corresponding to  distinct text prompts. We calculate the pairwise LPIPS distance between all  ( 1) unique pairs within this set. The final Diversity-LPIPS score is derived by averaging these pairwise distances across all samples in the dataset. Higher values indicate greater diversity and reduced mode collapse. 2 B.4. Qualitative Visualization We present representative test cases and corresponding model-generated results from the autonomous vehicle, robotics, and human domains of PAI-Bench-C, as illustrated in Figure 17, Figure 18, and Figure 19, respectively. C. More Details about PAI-Bench-U C.1. Detailed Data Curation Process This section details the curation process for the embodied reasoning benchmark. To ensure rigorous evaluation, we adhere to two core design principles: 1) Unified Question Templates: We standardize question formulation to ensure reasoning is grounded in visual input rather than exploited through textual biases. 2) Unified Action Granularity: To resolve ambiguity in next-action prediction (where multiple granularities, e.g., grab can vs. move hand left, may be valid), we adopt the hierarchical taxonomy proposed by [13]. We categorize behaviors into atomic-level actions, coarse-grained subtasks, and dataset-specific goals. The curation details for specific data sources are as follows: 24 PAI-Bench: Comprehensive Benchmark For Physical AI Figure 9 Accuracy versus number of input frames on PAI-Bench-U. The input frames are uniformly sampled from the video. The dashed horizontal lines denote the random-guess baselines. RoboVQA. We curated 101 clips from the RoboVQA [69] validation split. We formulated binary multiple-choice questions (Yes/No) to evaluate two capabilities: task-completion verification (determining if an instruction was successfully executed) and affordance (assessing task feasibility given the visual context). RoboFail. We manually annotated 100 examples from RoboFail [55] to assess reasoning under failure conditions. These samples evaluate action affordance and task completion verification in challenging scenarios characterized by: (1) complex temporal dynamics requiring observant perception, (2) physical constraints impeding execution, and (3) nuanced reasoning requirements beyond simple perception mismatches. BridgeData V2. We processed the BridgeData V2 [78] validation split to extract 100 clips for multiple-choice QA. Conditioned on the task instruction and the visual history, the model is queried to predict the most plausible immediate next action. AgiBot. We derived 100 multiple-choice samples from AgiBot [2]. For each clip, we provide the high-level task information and ask the model to identify the correct next subtask. Distractors are randomly sampled from the subtask sequence within the clips full trajectory. HoloAssist. We constructed 100 QA pairs from HoloAssist [83]. Providing the coarse-grained action annotation as the overall goal, we query the model for the most likely next subtask. Distractors are randomly sampled from other fine-grained action annotations associated with that coarse-grained goal. AV (Proprietary). We curated 100 videos from proprietary dataset to construct multiplechoice QA pairs. These videos exhibit diverse lateral and longitudinal behaviors with rich agent interactions. The questions are designed to: (1) predict the likely next immediate action of the ego vehicle, (2) verify the completion of previously executed action, and (3) assess the affordance of specific actions within the given scenario. C.2. Detailed Experiment Setup Inference Prompt Template. To ensure rigorous and reproducible evaluation across all MLLMs, we employ unified system prompt for PAI-Bench-U, as delineated in Figure 10. Specifically, the model is instructed to generate an intermediate rationale within <think> tags prior to providing the final conclusion within <answer> tags. The input context consists of temporally sampled video frames (0 . . . ) followed by the specific question, ensuring the model processes the visual narrative before addressing the query. PAI-Bench: Comprehensive Benchmark For Physical AI PAI-Bench-U Inference Prompt Template You are helpful assistant. Answer in the format: <think>reasoning</think> <answer>answer</answer>. Input {Sampled Frames 0} {Sampled Frames 1} . . . {Sampled Frames } Question: {Question} Figure 10 The standardized inference prompt template utilized in PAI-Bench-U. Experimental Settings We employ default inference settings for all proprietary models. Specifically for GPT-5 [61], we differentiate between medium reasoning effort (enabling \"thinking mode\") and minimal reasoning effort (disabling \"thinking mode\"). The model versions and checkpoints are detailed in Table 9.Open-source models are served via the vLLM backend [45] using their respective official default parameters. Table 9 List of proprietary models with their versions evaluated in our experiments. Model Vendor Version OpenAI GPT-4o GPT-5 OpenAI Claude 3.5 Sonnet Anthropic gpt-4o-2024-08-06 gpt-5-2025-08-07 claude-3-5-sonnet-20241022-v2 C.3. Discussion on Input Frame Count To analyze how the number of sampled video frames affects model performance in PAI-Bench-U, we systematically evaluate GPT-5 [61], Qwen3-VL-235B-A22B [9], Qwen3-VL-30B-A3B [9], and Qwen3-VL-8B [9] with 0, 1, 2, 4, 8, 16, and 32 uniformly sampled frames. Impact of Temporal Sampling Rate. As illustrated in Figure 9, increasing the input frame count (< 8) initially yields consistent performance gains, as the model acquires the effective visual information necessary for question answering. However, performance saturates and remains stable beyond 8 frames. We attribute this plateau to two primary factors. First, we hypothesize reasoning bottleneck: while the visual information provided by 8 frames is likely sufficient for the task, the models failure to achieve further gains highlights intrinsic limitations in their spatial-temporal reasoning capabilities, rather than deficiency in visual data. Second, excessive sampling may introduce temporal ambiguity. High frame rates significantly reduce the visual variance between adjacent frames, which can paradoxically hinder motion perception. For instance, when determining the movement direction of robotic arm, sparse sampling (e.g., start and end frames) presents distinct visual states that make the trajectory obvious. Conversely, dense sampling results in minute inter-frame differences. Current models often lack the sensitivity to process these fine-grained temporal shifts, leading them to misinterpret the high similarity between consecutive frames as static scene, thereby degrading judgment. 26 PAI-Bench: Comprehensive Benchmark For Physical AI C.4. User Study Details To establish human performance baseline for PAI-Bench-U, we conducted user study wherein participants responded to benchmark questions conditioned on the provided video clips. The resulting aggregated accuracy serves as comparative reference for model evaluation. The annotation interface utilized for this data collection is illustrated in Figure 21. D. PAI-Bench Leaderboard Visualization To provide holistic view of model performance across the diverse dimensions of PAI-Bench, we visualize the evaluation results using radar plots (Figure 22). To account for the heterogeneity of evaluation metrics, we normalize scores to ensure comparative consistency. E. Discussion and Future Work In this section, we identify several open challenges and outline promising directions for future research: Enhancing Metric Robustness with Advanced Encoders. The precision of overall consistency metrics is tied to the capabilities of the underlying videotext foundational models. Currently, models like VCLIP [87] exhibit constraints when processing lengthy and highly detailed textual prompts. Future work could leverage next-generation encoders to improve sensitivity to complex instructions, thereby enabling more granular consistency evaluations. Addressing Conservative Generation Strategies. Our user study highlights nuanced phenomenon in video generation: trade-off between motion dynamism and generation safety. We observe that some models tend to adopt conservative strategy that prioritizing static fidelity (e.g., holding racket) over complex, high-risk dynamics (e.g., rapidly swinging it) to avoid artifacts. While this has limited impact on quantitative rankings, developing methods that encourage risk-taking in motion generation without compromising visual quality remains vital direction. Necessity and Imperfection of MLLM Judges. While we utilize SOTA MLLMs like GPT-5 [61] to assess high-level semantics, automated evaluation remains an evolving field. Even the most advanced models possess inherent boundaries as visual judges, particularly when interpreting temporal dynamics in video. However, given the lack of effective alternatives for scalable semantic assessment, MLLM-based evaluation represents the current best practice. 27 PAI-Bench: Comprehensive Benchmark For Physical AI The video begins with view from inside vehicle, likely captured by dashboard camera, showing wide, open road ahead under an overcast sky. The road is marked with white directional arrows indicating left or straight-right turns. Two lines of orange traffic cones are placed along the center of the road, suggesting some form of construction or roadwork. Within the area enclosed by the traffic cones, an arrow board displays flashing arrows pointing left and right to guide traffic around the detour. On either side of the road, there are grassy areas with small trees and shrubs. In the background, modern buildings, possibly part of an urban or suburban area, are visible, with pedestrian bridge crossing above the road. The overall atmosphere is calm and quiet, with no other vehicles or pedestrians in sight.As the video progresses, the car changes lanes to the left and continues along the road. The camera angle shifts slightly to show the car moving further down the road, passing more traffic cones, and approaching the pedestrian bridge, which remains visible above the road. (a) Input condition signals (b) Source video (c) Veo3 (d) CogVideoX1.5 (e) Cosmos-Predict2.5-2B (f) DynamicCrafter (g) HunyuanVideo-I2V (h) MAGI-1-24B (i) Wan2.2-I2V-A14B Figure 11 Example of autonomous vehicles domain and model generations from PAI-BenchG. Best viewed with zoom. 28 PAI-Bench: Comprehensive Benchmark For Physical AI The video opens with view of testing environment, characterized by large wooden table at the center. On this table, two robot arms are positioned at opposite ends, with the left arm closer to the camera and the right arm further away. Between the hands lies dark wooden shelf with red spherical object on its top rack, likely serving as platform or obstacle. In the background, various pieces of equipment, including tripod, chair, are visible. [TRUNCATED] As the video progresses, the right robotic hand extends outward, moving from its initial position towards the red spherical object on the shelf. The hand then picks up the object and places it on the lowest rack of the shelf, completing smooth, deliberate manipulation. The left robotic hand remains stationary throughout the sequence. No new objects appear in the video; all existing elements maintain their positions except for the movement of the right robotic hand. The scene concludes with the right robotic hand returning to its initial position, while the left hand continues to rest on the table. The overall environment remains unchanged, with the focus remaining on the interaction between the robotic hands and the wooden block, highlighting precise control during the demonstration. (a) Input condition signals (b) Source video (c) Veo3 (d) CogVideoX1.5 (e) Cosmos-Predict2.5-2B (f) DynamicCrafter (g) HunyuanVideo-I2V (h) MAGI-1-24B (i) Wan2.2-I2V-A14B Figure 12 Example of robotics domain and model generations from PAI-Bench-G. Best viewed with zoom. PAI-Bench: Comprehensive Benchmark For Physical AI close-up of precision metalworking process in controlled industrial setting. The first frame captures cylindrical metal workpiece securely mounted on lathe, rotating smoothly as cutting machine, held by black, angular fixture, approaches from above. The cutting machine, marked with numerical identifiers (5513 020-10), engages with the workpiece, shaving off thin metal shavings that are visibly ejected into the air, creating fine mist around the machining area. [TRUNCATED] As the video progresses, the cutting machine continues its linear motion along the length of the workpiece, maintaining steady pace. The tools engagement with the material results in consistent metal shaving, producing continuous stream of shavings that are dispersed into the surrounding space. The workpiece remains stationary relative to the cameras perspective, ensuring clear view of the cutting metal process. The environment suggests well-lit workshop, emphasizing the precision and efficiency of the operation. By the final frame, the cutting machine has almost completed its pass along the workpiece, leaving behind smooth, polished surface. The metal shavings continue to be ejected, and the overall scene maintains focused and industrious atmosphere, underscoring the meticulous nature of the metalworking process. (a) Input condition signals (b) Source video (c) Veo3 (d) CogVideoX1. (e) Cosmos-Predict2.5-2B (f) DynamicCrafter (g) HunyuanVideo-I2V (h) MAGI-1-24B (i) Wan2.2-I2V-A14B Figure 13 Example of industry domain and model generations from PAI-Bench-G. Best viewed with zoom. 30 PAI-Bench: Comprehensive Benchmark For Physical AI close-up shot captures wet, dark surface, likely puddle or damp ground, under an overcast sky. The surface is covered with small ripples and bubbles, indicating recent rain. Several droplets of water form small, round bubbles that float on the surface, while others create concentric circles as they spread outwards. Tiny bits of debris, including leaves and twigs, are scattered across the surface, adding texture. As the video progresses, the rain continues to fall, creating more ripples and bubbles. Each raindrop impacts the water, causing series of concentric circles that expand outward from the point of impact. The bubbles grow slightly larger as they float on the surface, reflecting the light and adding shimmering effect. The scattered debris remains stationary, but the ripples and bubbles continuously shift and evolve, capturing the dynamic nature of the rainfall. The scene remains focused on the wet, reflective surface, with the ongoing rain creating continuous pattern of ripples and bubbles. (a) Input condition signals (b) Source video (c) Veo (d) CogVideoX1.5 (e) Cosmos-Predict2.5-2B (f) DynamicCrafter (g) HunyuanVideo-I2V (h) MAGI-1-24B (i) Wan2.2-I2V-A14B Figure 14 Example of common sense domain and model generations from PAI-Bench-G. Best viewed with zoom. 31 PAI-Bench: Comprehensive Benchmark For Physical AI modern, minimalist living room serves as the backdrop for dynamic dance performance. woman, positioned centrally on light beige rug, executes series of graceful dance moves. She wears white cropped top, high-waisted blue jeans, and white sneakers, with her long hair tied up into two low pig tails that flow freely as she dances. [TRUNCATED] The video begins with the woman in poised stance, her right leg crossed over her left, with both her arms to her sides. As she starts her dance routine, she lifts her right leg high into the air, balancing on her left foot, and extends her right arm gracefully above her head. She shifts her weight back down, landing smoothly and continuing with fluid, rhythmic movements that include hip sways and arm gestures. Her hair flows dynamically with her movements, adding sense of energy and liveliness to the scene. By the final frame, the woman is caught mid-step in her dance routine, standing upright with her arms slightly bent in mid-air at the elbows. Her hair continues to flow, indicating the dynamic nature of her movements throughout the sequence. [TRUNCATED] (a) Input condition signals (b) Source video (c) Veo3 (d) CogVideoX1.5 (e) Cosmos-Predict2.5-2B (f) DynamicCrafter (g) HunyuanVideo-I2V (h) MAGI-1-24B (i) Wan2.2-I2V-A14B Figure 15 Example of human domain and model generations from PAI-Bench-G. Best viewed with zoom. 32 PAI-Bench: Comprehensive Benchmark For Physical AI close-up shot captures small, round object mounted on metal pole in sandy area. The background features building with red stripe across its facade and some trees to the right side. Tire tracks mark the ground, indicating recent vehicle activity. The object remains stationary until it suddenly ignites, producing burst of bright light and smoke. The explosion is intense, sending sparks flying outward and creating large cloud of white smoke that billows into the air. The smoke gradually disperses, revealing scattered debris around the pole. The scene remains static after the explosion, focusing on the remnants of the blast and the surrounding environment. (a) Input condition signals (b) Source video (c) Veo3 (d) CogVideoX1.5 (e) Cosmos-Predict2.5-2B (f) DynamicCrafter (g) HunyuanVideo-I2V (h) MAGI-1-24B (i) Wan2.2-I2V-A14B Figure 16 Example of physics domain and model generations from PAI-Bench-G. Best viewed with zoom. 33 PAI-Bench: Comprehensive Benchmark For Physical AI (a) Blur control signal (input) (b) Edge control signal (input) (c) Depth control signal (input) (d) Segmentation control signal (input) (e) Source video (reference) tranquil autumn landscape unfolds throughout the video, showcasing long, straight road stretching into the horizon. The road is covered in fallen leaves, with tire tracks visible, indicating recent vehicular passage. The surrounding terrain is flat and vast, blanketed in mix of orange and brown leaves, creating stark contrast against the clear blue sky. On either side of the road, sparse trees with bare branches add texture to the otherwise smooth surface. [TRUNCATED] (f) Text control signal (input) (g) Cosmos-Transfer2.5-2B (blur) (h) Cosmos-Transfer2.5-2B (edge) (i) Cosmos-Transfer2.5-2B (depth) (j) Cosmos-Transfer2.5-2B (segmentation) (k) Cosmos-Transfer2.5-2B (multi-control generation) (l) Wan2.2-Fun-A14B-Control (blur) (m) Wan2.2-Fun-A14B-Control (edge) (n) Wan2.2-Fun-A14B-Control (depth) (o) Wan2.2-Fun-A14B-Control (segmentation) Figure 17 Example of autonomous vehicle domain control signals and model generations from PAI-Bench-C. Best viewed with zoom. 34 PAI-Bench: Comprehensive Benchmark For Physical AI (a) Blur control signal (input) (b) Edge control signal (input) (c) Depth control signal (input) (d) Segmentation control signal (input) (e) Source video (reference) In this video, robotic arms operate in contemporary kitchen featuring wooden countertop and glass-top gas stove with two burners. Three ceramic jars are aligned on the counter. black appliance, likely water filter, is placed to the left. The wall has textured stone appearance, adding to the kitchens stylish design. (f) Text control signal (input) (g) Cosmos-Transfer2.5-2B (blur) (h) Cosmos-Transfer2.5-2B (edge) (i) Cosmos-Transfer2.5-2B (depth) (j) Cosmos-Transfer2.5-2B (segmentation) (k) Cosmos-Transfer2.5-2B (multi-control generation) (l) Wan2.2-Fun-A14B-Control (blur) (m) Wan2.2-Fun-A14B-Control (edge) (n) Wan2.2-Fun-A14B-Control (depth) (o) Wan2.2-Fun-A14B-Control (segmentation) Figure 18 Example of robotics domain control signals and model generations from PAIBench-C. Best viewed with zoom. PAI-Bench: Comprehensive Benchmark For Physical AI (a) Blur control signal (input) (b) Edge control signal (input) (c) Depth control signal (input) (d) Segmentation control signal (input) (e) Source video (reference) The video is set in an indoor climbing gym, featuring large, overhanging bouldering wall covered with variety of neon-colored climbing holds. The holds are arranged in complex pattern, with colors including electric blue, neon pink, lime green, bright yellow, and vivid orange, creating visually dynamic backdrop. The wall is angled steeply, challenging the climber with its overhang. [TRUNCATED] (f) Text control signal (input) (g) Cosmos-Transfer2.5-2B (blur) (h) Cosmos-Transfer2.5-2B (edge) (i) Cosmos-Transfer2.5-2B (depth) (j) Cosmos-Transfer2.5-2B (segmentation) (k) Cosmos-Transfer2.5-2B (multi-control generation) (l) Wan2.2-Fun-A14B-Control (blur) (m) Wan2.2-Fun-A14B-Control (edge) (n) Wan2.2-Fun-A14B-Control (depth) (o) Wan2.2-Fun-A14B-Control (segmentation) Figure 19 Example of human domain control signals and model generations from PAIBench-C. Best viewed with zoom. 36 PAI-Bench: Comprehensive Benchmark For Physical AI Figure 20 User study interface for preferences on PAI-Bench-G. 37 PAI-Bench: Comprehensive Benchmark For Physical AI Figure 21 User study interface for establishing human baselines on PAI-Bench-U. 38 PAI-Bench: Comprehensive Benchmark For Physical AI (a) PAI-Bench-G: Domain radar comparison (b) PAI-Bench-G: Quality radar comparison (c) PAI-Bench-C: Radar comparison (d) PAI-Bench-U: Radar comparison Figure 22 Radar chart visualizations of model capabilities across PAI-Bench tracks."
        }
    ],
    "affiliations": [
        "CMU",
        "Georgia Tech"
    ]
}