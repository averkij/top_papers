{
    "paper_title": "VTC-R1: Vision-Text Compression for Efficient Long-Context Reasoning",
    "authors": [
        "Yibo Wang",
        "Yongcheng Jing",
        "Shunyu Liu",
        "Hao Guan",
        "Rong-cheng Tu",
        "Chengyu Wang",
        "Jun Huang",
        "Dacheng Tao"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Long-context reasoning has significantly empowered large language models (LLMs) to tackle complex tasks, yet it introduces severe efficiency bottlenecks due to the computational complexity. Existing efficient approaches often rely on complex additional training or external models for compression, which limits scalability and discards critical fine-grained information. In this paper, we propose VTC-R1, a new efficient reasoning paradigm that integrates vision-text compression into the reasoning process. Instead of processing lengthy textual traces, VTC-R1 renders intermediate reasoning segments into compact images, which are iteratively fed back into vision-language models as \"optical memory.\" We construct a training dataset based on OpenR1-Math-220K achieving 3.4x token compression and fine-tune representative VLMs-Glyph and Qwen3-VL. Extensive experiments on benchmarks such as MATH500, AIME25, AMC23 and GPQA-D demonstrate that VTC-R1 consistently outperforms standard long-context reasoning. Furthermore, our approach significantly improves inference efficiency, achieving 2.7x speedup in end-to-end latency, highlighting its potential as a scalable solution for reasoning-intensive applications. Our code is available at https://github.com/w-yibo/VTC-R1."
        },
        {
            "title": "Start",
            "content": "VTC-R1: Vision-Text Compression for Efficient Long-Context Reasoning Yibo Wang 1 Yongcheng Jing 1 Shunyu Liu 1 Hao Guan 1 Rong-Cheng Tu 1 Chengyu Wang 2 Jun Huang 2 Dacheng Tao 1 6 2 0 2 9 2 ] . [ 1 9 6 0 2 2 . 1 0 6 2 : r a"
        },
        {
            "title": "Abstract",
            "content": "Long-context reasoning has significantly empowered large language models (LLMs) to tackle complex tasks, yet it introduces severe efficiency bottlenecks due to the computational complexity. Existing efficient approaches often rely on complex additional training or external models for compression, which limits scalability and discards critical fine-grained information. In this paper, we propose VTC-R1, new efficient reasoning paradigm that integrates vision-text compression into the reasoning process. Instead of processing lengthy textual traces, VTC-R1 renders intermediate reasoning segments into compact images, which are iteratively fed back into visionlanguage models as optical memory. We construct training dataset based on OpenR1-Math220K achieving 3.4 token compression and finetune representative VLMsGlyph and Qwen3VL. Extensive experiments on benchmarks such as MATH500, AIME25, AMC23 and GPQA-D demonstrate that VTC-R1 consistently outperforms standard long-context reasoning. Furthermore, our approach significantly improves inference efficiency, achieving 2.7 speedup in end-toend latency, highlighting its potential as scalable solution for reasoning-intensive applications. Our code is available at https://github.com/ w-yibo/VTC-R1. 1. Introduction Reasoning capability (Li et al., 2025b; Lightman et al., 2023; Yao et al., 2023; Huang & Chang, 2023; Yao et al., 2025) has emerged as powerful technique of large language models (LLMs), enabling them to tackle complex tasks such as mathematical problem solving (Hendrycks et al., 2021; Luo et al., 2025a; Hu et al., 2025) and code generation (Chen et al., 2021; Jiang et al., 2024). Recent advancements, exemplified by OpenAI o1 (OpenAI, 2024) and DeepSeek1Nanyang Technical University 2Alibaba Cloud Computing. Preprint. Figure 1. Comparison between existing efficient reasoning approaches and vision-text compression (VTC). Existing methods either require additional training or sampling procedures, or rely on external strong models. In contrast, VTC leverages lightweight rendering to transform long textual reasoning traces into compact visual representations, enabling VLMs to encode information with significantly fewer vision tokens (3-4 compression). This approach is both lightweight and model-free. R1 (Guo et al., 2025), leverage reinforcement learning to further scale this capability to long-context reasoning, substantially improving performance on challenging real-world tasks (Wang et al., 2026b). Despite recent progress, longcontext reasoning introduces severe efficiency bottlenecks. The computational complexity of the transformer architecture (Zaheer et al., 2020; Beltagy et al., 2020; Kitaev et al., 2020) grows quadratically with sequence length, causing both computation and memory costs to increase rapidly as the context expands. This leads to degraded inference speed, reduced training efficiency, and limited scalability, which significantly hinders real-world deployment. To mitigate these issues, several efficient approaches are proposed (Chen et al., 2025; Munkhbat et al., 2025; Lee et al., 2025; Liu et al., 2024). Existing methods can be broadly categorized into two groups. i) Extra training or sampling stages beyond standard training. For example, CoT-Valve (Ma et al., 2025b) adopts multi-stage training procedure to obtain models specialized for different reasoning lengths and O1-Pruner (Luo et al., 2025b) applies offline VTC-R1 : Vision-Text Compression for Efficient Long-Context Reasoning reinforcement learning with multiple sampled trajectories (16 responses per problem). These approaches increase training and inference cost. ii) External strong models to guide reasoning compression. TokenSkip (Xia et al., 2025) requires an additional model to estimate token importance, while R1-Compress (Wang et al., 2025) and InftyThink (Yan et al., 2025) depend on powerful external summarization models (e.g., Llama-3.3-70B-Instruct) to condense long reasoning traces. Although both categories of methods are effective, they often restrict exploration space and discard fine-grained information that is critical for reasoning. Without additional training or external models, how can we achieve efficient reasoning while preserving fine-grained information? Motivated by this, promising yet underexplored direction is vision-text compression (VTC) (Wei et al., 2025; Cheng et al., 2025; Xing et al., 2025b; Zhao et al., 2025; Xing et al., 2025a). Rather than reducing fine-grained information, VTC adopts an alternative representation by transforming textual content into visual forms via lightweight rendering, enabling vision-language models (VLMs) to encode rich semantic information using substantially fewer vision tokens. This design is lightweight and model-free, as shown in Figure 1, introducing no additional training stages or reliance on external compression models. Prior works such as DeepSeek-OCR (Wei et al., 2025) and Glyph (Cheng et al., 2025) focus on text reconstruction or long-context understanding, showing that long text sequences can be represented with 310 token compression while maintaining high decoding precision. However, whether such high-density visual representations can preserve and support multi-step reasoning processes remains unclear. Notably, mathematical reasoning, with its symbolic structure and step-wise derivations, is naturally amenable to visual rendering, making it suitable and principled testbed for studying reasoning-oriented vision-text compression. To bridge this gap, we propose VTC-R1, new efficient reasoning paradigm that iteratively integrates visiontext compression into long-context reasoning. VTC-R1 treats the reasoning process as multiple processes, where the preceding process are regarded as long-context and rendered into compact images, and performs iterative reasoning (Yan et al., 2025) with VLMs. As illustrated in Figure 2, the reasoning process is decomposed into sequence of reasoning steps. Upon the completion of each step, it is rendered into an image. To proceed to the next step, the accumulated images of previous steps are fed back into the model alongside the question, functioning as form of optical memory that compactly encodes previous reasoning using vision tokens. We construct training dataset based on OpenR1-Math220K (Hugging Face, 2025), large-scale long-context reasoning corpus generated by DeepSeek-R1 (Guo et al., 2025). We segment each long reasoning trace into shorter reasoning segments and render the preceding segments into images, forming paired imagetext reasoning data with up to 3.4 token compression as shown in Table 1. We then fine-tune representative VTC-VLM (i.e., Glyph (Cheng et al., 2025)) and the state-of-the-art VLM (i.e., Qwen3-VL (Bai et al., 2025)), under this iterative reasoning framework. Extensive experiments on diverse mathematical reasoning benchmarks, GSM8K (Cobbe et al., 2021), MATH500 (Lightman et al., 2023), AIME25 (Zhang & Math-AI, 2025), AMC23 (MathAI, 2025) and GPQA-Diamond (Rein et al., 2024), demonstrate that VTC-R1 consistently outperforms standard longcontext reasoning. Moreover, VTC-R1 significantly improves inference efficiency, achieving up to 2.7 speedup in end-to-end reasoning latency, highlighting its practical advantages for scalable long-context reasoning. The main contributions of this paper: We introduce VTC-R1, new efficient reasoning paradigm that reformulates reasoning as an iterative process and integrates vision-text compression to replace long text with compact vision tokens, without requiring additional training stages or external strong models. We construct training dataset by segmenting reasoning traces and rendering preceding steps into images, producing paired data with up to 3.4 token compression. Extensive evaluation on major mathematical and out-ofdistribution benchmarks shows that VTC-R1 consistently outperforms standard long-context reasoning and achieves up to 2.7x speedup in end-to-end inference latency. 2. Related Work Reasoning in Large Language Models. Reasoning capabilities (Li et al., 2025b; Lightman et al., 2023; Huang & Chang, 2023; Yao et al., 2025) constitute cornerstone of modern LLMs, enabling proficiency in rigorous domains like mathematics (Hendrycks et al., 2021; Luo et al., 2025a; Hu et al., 2025) and code generation (Chen et al., 2021; Jiang et al., 2024). While early strategies relied on structured prompting (Yao et al., 2023; 2024), recent advancements leverage reinforcement learning to scale test-time compute. Models such as OpenAI o1 (OpenAI, 2024), DeepSeekR1 (Guo et al., 2025), and Kimi (Team et al., 2025a) generate extended chains of thought, achieving significant improvements on challenging real-world benchmarks. Efficient Reasoning. Long-context reasoning strategies exacerbate the computational bottlenecks inherent in the quadratic complexity of Transformer architectures (Zaheer et al., 2020; Beltagy et al., 2020; Kitaev et al., 2020; Wang et al., 2020). Recent research has investigated various efficiency mechanisms (Liu et al., 2025; Chen et al., 2025; Munkhbat et al., 2025; Lee et al., 2025; Liu et al., 2024; Yang et al., 2025c; Zhang et al., 2025; Hao et al., 2024; Yang VTC-R1 : Vision-Text Compression for Efficient Long-Context Reasoning Figure 2. Comparison between standard long-context reasoning and the proposed VTC-R1 reasoning paradigm. (a) Standard long-context reasoning processes the entire reasoning trace as single long sequence, leading to increasing computational and memory costs as the context grows. (b) VTC-R1 reformulates long-context reasoning as an iterative process. At each iteration, the current reasoning segment is generated and the preceding segments are rendered into compact images, which are fed back to the model together with the original question. These rendered images function as form of optical memory, enabling efficient multi-step reasoning with reduced token usage. et al., 2025a; Pan et al., 2025; Ma et al., 2025a; Qiao et al., 2025; Zhuang et al., 2025; Yang et al., 2025b; Hou et al., 2025; Ning et al., 2025; Li et al., 2025a; Gong et al., 2025), though existing methods often incur significant trade-offs. One category of approaches (Team et al., 2025a), exemplified by CoT-Valve (Ma et al., 2025b) and O1-Pruner (Luo et al., 2025b), relies on complex multi-stage training procedures or extensive offline sampling, which substantially increases pre-deployment overhead. second category leverages external strong models (Kang et al., 2024) to guide reasoning compression, as in TokenSkip (Xia et al., 2025), R1-Compress (Wang et al., 2025), and InftyThink (Yan et al., 2025), making the compression quality dependent on the capabilities of these auxiliary models. Although effective in reducing token counts, these approaches often constrain the exploration space and risk discarding fine-grained information that is critical for correct logical deduction. Vision-Text Compression. Vision-text compression (VTC) has emerged as promising approach for reducing the cost of processing long textual sequences by transforming text into compact visual representations. DeepSeekOCR (Wei et al., 2025) demonstrates that long texts can be compressed into visual tokens, achieving 310 reduction in token count while maintaining high decoding fidelity. Glyph (Cheng et al., 2025) utilizes continuous pre-training and RL for VTC to enhance long-context understanding capabilities. VTCBench (Zhao et al., 2025) proposes benchmark to evaluate the spectrum of capabilities in VTC. While prior work focuses on text understanding and reconstruction, and it remains unclear whether such high-density visual representations can faithfully preserve and support complex reasoning processes, particularly for mathematically intensive and multi-step reasoning tasks. Some concurrent works, AgentOCR (Feng et al., 2026) utilizes VTC to compress the agents history derived from tool invocations into compact rendered image. RoT (Wang et al., 2026a) focuses on utilizing rendered visual tokens as latent tokens for latent reasoning, but it does not explicitly address long-context reasoning and lacks systematic evaluation on challenging benchmarks. 3. Preliminaries 3.1. Problem Setup We consider reasoning task defined by an input question Q. Given vision language model , the goal is to produce final answer A. During answer generation, long sequence of intermediate reasoning steps is also produced, which forms long-context reasoning. 3.2. Vision-Text Compression Vision-text compression is defined as procedure where given text is rendered into an image, enabling VLM to encode the content using fewer vision tokens. The pipeline used in our work is summarized as follows. Given an input text sequence , the text is rendered into images through pipeline before model input. The rendering pipeline is parameterized by configuration vector (Cheng et al., 2025), θ = (cid:0)dpi, page size, font family, font size, line height, alignment, indent, spacing, scale, colors, borders, . . . (cid:1), (1) which controls the typography, layout, and visual style of 3 VTC-R1 : Vision-Text Compression for Efficient Long-Context Reasoning rendered pages. The details of rendering configuration are provided in Appendix A.1. Through the rendering process, multiple PNG images are produced. This process is defined as = Rθ(T ), where Rθ() denotes the rendering operator. The images are processed by the image processor and vision encoder of model . For simplicity, let Mvision denote the vision tokenizer. Given the images I, we obtain sequence of vision tokens = Mvision(I), where = {v1, . . . , vLv } and Lv represents the sequence length. The original text is processed by the text tokenizer Mtxt to produce text token sequence = Mtxt(T ), where = {t1, . . . , tLt} and Lt denotes the number of text tokens. Thus, the vision-text compression ratio is defined as: ρ ="
        },
        {
            "title": "Lt\nLv",
            "content": ", (2) In practice, ρ > 1, larger ρ indicates higher compression efficiency, implying that fewer tokens are required to encode the same content under the vision tokenization scheme. 4. Methodology 4.1. Standard Long-Context Reasoning. Standard long-context reasoning, as adopted by OpenAI o1 (OpenAI, 2024) and DeepSeek-R1 (Guo et al., 2025), typically produces long sequence of intermediate reasoning steps. Such behavior incurs substantial computational and memory cost. This reasoning procedure is formulated as long-context reasoning process, denoted as LR, where the input question is Q. The standard long-context reasoning can be represented as Ss think LR /think A, where Ss denotes the standard system prompt, such as You are helpful assistant. The tokens and indicate the start of user input and model response, respectively. The special tokens think and /think mark the beginning and end of the reasoning process. In practice, LR may reach 16k tokens or more. During reasoning, the preceding steps could be regarded as context and vision-text compression can therefore be introduced to encode these preceding steps into smaller number of effective vision tokens, thereby mitigating the substantial cost of long-context reasoning. 4.2. VTC-R1 Reasoning Instead of generating full textual reasoning trace, VTCR1 first formulates long-context reasoning as an iterative process to get the answer. long-context reasoning process, denoted as LP , is decomposed into sequence of reasoning segments {LP1, . . . , LPn}. Algorithm 1 VTC-R1 Reasoning Paradigm Input: question Q; vision language model ; system prompt Sv; rendering operator Rθ; maximum iteration Initialize: rendered image set for = 1 to do Generate Vision-Language Model Output: Oi (Sv, Q, I) if Oi produces the final answer then return end if Update Image Set via Rendering: Extract reasoning progress LRi from Oi Render reasoning into images: Ii Rθ(LRi) Update: {Ii} end for if no final answer then Extract Answer when Reaching Iteration Limit: Extract final answer from OT end if Output: final answer Iterative Reasoning. Concretely, iterative reasoning generates the reasoning process sequentially. At iteration i, the model conditions on the question and the previous segments: LPi pθ( Q, LP<i), LP<i (LP1, . . . , LPi1), (3) and the complete trace is obtained by concatenation LP = (LP1, . . . , LPn). We next show that this iterative formulation is equivalent to standard one-pass long-context generation under an autoregressive model. By the chain rule, the joint distribution of the full trace factorizes as pθ(LP Q) = (cid:89) i=1 pθ(LPi Q, LP<i), (4) which is exactly the distribution induced by sampling LP1, . . . , LPn sequentially with the same conditionals. Consequently, for any answer extraction function = (LP ), both one-pass and iterative generation yield the same answer distribution: = (LP ), LP pθ( Q). VTC-R1 Reasoning Paradigm. The first reasoning process is expressed as follows, where > 1 is assumed: Sv think LR1 /think, where Sv denotes the VTC-R1 system prompt. VTC-R1 : Vision-Text Compression for Efficient Long-Context Reasoning VTC-R1 System Prompt Sv These images record your previous reasoning process. Based on this reasoning, continue and complete the final answer. Do not restart the reasoning. If no images are provided, start the reasoning from scratch. As described in Sec 3.2, the first reasoning process LR1 is rendered into multiple images, I1 = Rθ(LR1). When the i-th reasoning process begins, 1 reasoning processes have been completed. At the end of each process, the generated reasoning process LRj is rendered into multiple images Ij and stored. As result, set of rendered images {I1, . . . , Ii1} is available. The reasoning process at the i-th iteration is then expressed as Sv Q, I1, . . . , Ii1 think LRi /think. At the final reasoning iteration n, the model produces the last reasoning segment and outputs the final answer A. The complete generation at this stage is expressed as Sv Q, I1, . . . , In1 think LRn /thinkA. During inference, VTC-R1 iterates continuously until the final answer is produced. As shown in Table 2, the method exhibits adaptive reasoning behavior, where the number of reasoning iterations is selected dynamically according to the problem difficulty. To prevent unbounded generation, maximum iteration limit, denoted as , is imposed. VTC-R1 performs iterative reasoning by generating multiple reasoning segments in Algorithm 1. At each iteration, the previously generated reasoning segments LR1, . . . , LRi1 are rendered into images I1, . . . , Ii1. Therefore, these images provide compact and efficient representation of textual reasoning through vision tokens, functioning analogously to an optical memory. Under our rendering configuration, the resulting compression ratio ρ is approximately 34 as shown in Table 1, which could mitigate the computational and memory cost incurred by token growth in standard long-context reasoning. Moreover, VTC-R1 requires lightweight rendering mechanism. No additional training, extra sampling stages, or external models are introduced. Batch Inference. To facilitate batch inference in frameworks like vLLM (Kwon et al., 2023), we adapt Algorithm 1 by introducing independent request states and dynamic active set mechanism. This approach enables efficient parallel generation by selectively constructing batch inputs and updating multimodal contexts only for active samples during each iteration. The detailed Algorithm 2 is provided. Figure 3. Distribution of data index. The index indicates the order of reasoning segment for given problem, where index 0 corresponds to the first segment. Most samples terminate at early steps, while small fraction requires more than four iterations. Table 1. Statistics of rendered prior reasoning segments. We report the number of reasoning segments rendered as images, the total number of text and vision tokens, and the compression ratio. Metric Rendered reasoning steps (elements) Rendered images Total text tokens Total vision tokens Compression ratio (text / vision) Value 45K 105K 181M 54M 3.4 4.3. Training Data Construction To train VTC-R1, supervised fine-tuning dataset is constructed to enable VLMs to learn the VTC-R1 reasoning paradigm. The dataset is organized as an imagetext paired corpus. We adopt OpenR1-Math-Inf (Yan et al., 2025), which is subset of OpenR1-Math-220K (Hugging Face, 2025). OpenR1-Math-220K is generated by the DeepSeekR1 (Guo et al., 2025) model, where solutions are produced for large-scale mathematical problems. OpenR1Math-Inf contains 61K questionanswer pairs, and each solution is partitioned into multiple reasoning segments {LR1, LR2, . . . , LRn} according to predefined thresholds. Based on Sec 4.2, training data are constructed according to the index of the reasoning process, where different rules are applied at different iterations. Rendered images are included as inputs. The instance at iteration is defined as Datai = (cid:1), (cid:0)Sv, Q, , LR1 (cid:0)Sv, Q, {Ij}j<i, LRi (cid:0)Sv, Q, {Ij}j<i, LRn, A(cid:1), (cid:1), = 1, 1 < < n, = n. (5) 106K instances are constructed based on Eq. 5, which requires approximately 105K rendered images in PNG format. Figure 3 presents the segment index distribution in the conVTC-R1 : Vision-Text Compression for Efficient Long-Context Reasoning Table 2. Performance comparison across mathematical benchmarks. Accuracy (ACC) is higher-is-better (), latency (LAT) is lower-is-better (). Bold indicates the best performance. Superscript numbers denote accuracy improvements and latency speedups relative to standard long-context reasoning. Model GSM8K MATH500 AIME25 (Avg@16) AMC23 (Avg@16) ACC TOK LAT ACC TOK LAT ACC TOK LAT ACC TOK LAT Qwen3-VL-8B SFT VTC-R1 88.1 94.7(+6.6) 1.79 1.09 3.04 0.46(6.6) 85.4 90.0(+4.6) 4.17 3.39 5.36 2.49(2.2) 32.71 30.00(2.71) 17.46 14.32 29.85 12.02(2.5) 75.00 77.97(+2.97) 8.20 8.18 11.08 6.45(1.7) Glyph Base SFT SFT TokenSkip VTC-R1 86.1 87.1 86.4 93.6(+6.5) 2.35 1.87 2.25 1.09 1.38 0.93 1.32 0.34(2.7) 79.6 80.4 80.6 86.0(+5.6) 5.51 5.71 6.11 4.12 2.77 3.05 3.05 2.19(1.4) 24.17 25.62 23.75 26.25(+0.63) 19.94 17.47 17.82 12.95 14.48 11.52 11.85 6.81(1.7) 61.56 60.94 59.53 64.38(+3.44) 12.67 11.65 12.81 8.81 8.55 6.85 8.41 4.30(1.6) structed training data. Table 1 reports the token statistics after applying vision-text compression. The original reasoning traces contain 181M text tokens, which are reduced to 54M vision tokens after rendering, achieving compression ratio of up to 3.4. This dataset is subsequently used for supervised fine-tuning. It is noted that the number of images associated with each instance is adaptive. Therefore, the training procedure requires VLM architectures that support inputs with variable number and resolution of images, such as Qwen3-VL (Bai et al., 2025), GLM-4.1V (Team et al., 2025b), and Glyph (Cheng et al., 2025). 5. Experiments 5.1. Experiment Settings Dataset. For training, we use the OpenR1-Math-Inf (Yan et al., 2025), where each solution is segmented into multiple reasoning segments with varying lengths (2K, 4K, and 6K tokens). it is subset of OpenR1-Math-220K (Hugging Face, 2025) dataset. Unless otherwise specified, 4K is used as the default segmentation setting. For evaluation, we leverage four widely used mathematical reasoning benchmarks. GSM8K (Cobbe et al., 2021), MATH500 (Lightman et al., 2023), AIME25 (Zhang & Math-AI, 2025) and AMC23 (Math-AI, 2025). And GPQA-Diamond (GPQAD) (Rein et al., 2024), science-domain benchmark that serves as an out-of-distribution evaluation. See the Appendix B.2 for more details of benchmarks. Baseline. For baselines, our proposed method VTC-R1 is compared with standard long-context reasoning (SFT). In the SFT setting, standard questionanswer pairs with full long-form reasoning traces are used as the supervised fine-tuning dataset. We then perform VTC-R1 and SFT on two representative VLM architectures respectively for comparison. i) Glyph (Cheng et al., 2025), which serves as VTC-capable VLM. ii) Qwen3-VL-8B (Bai et al., 2025), which represents mainstream visionlanguage model. In addition, standard SFT does not require optical character recognition capability. Therefore, the base model preceding Glyph, GLM-4.1V-9B-Base (Team et al., 2025b) (Base SFT), is also included as baseline. The efficient reasoning method TokenSkip (Xia et al., 2025) is included as an additional baseline for comparison. Metric. We employ the following three metrics to evaluate the models performance. Accuracy (ACC): For GSM8K, MATH500, and GPQADiamond, we report pass@1 accuracy. For AIME25 and AMC23, due to their limited dataset sizes, we generate 16 responses per problem and report avg@16 accuracy. Token (TOK): The average number of tokens in the generated responses. Latency (LAT): We measure the average inference latency per generation. Given dataset with problems, where each problem is generated times (e.g., = 16 for AIME25 and AMC23), let t1 and t2 denote the wall-clock timestamps at the start and end of the entire inference process, respectively. The latency is computed as: LAT = t2 t1 . Implementation Details. For both SFT and VTC-R1, the processed training datasets contain 106K instances and require approximately 105K images. Both methods are trained with learning rate of 1 105 for one epoch using the LlamaFactory library (Zheng et al., 2024). For evaluation, temperature of 0.6 and top-p value of 0.95 are adopted under the vLLM framework (Kwon et al., 2023). 5.2. Main Results Performance Gains. As shown in Table 2, VTC-R1 consistently outperforms Base SFT, SFT and TokenSkip baselines on the Glyph across all four benchmarks. Notably, substantial improvements are observed on the more challenging benchmarks, with the gains of 5.6% on MATH500 and 3.4% on AMC23. On the Qwen3-VL architecture, VTC-R1 also demonstrates consistent improvements or achieves competitive accuracy compared to standard long-context reasoning. Furthermore, as reported in Table 3, similar trends are observed on the out-of-distribution benchmark. Specifically, 6 VTC-R1 : Vision-Text Compression for Efficient Long-Context Reasoning Figure 4. Accuracy of the proposed method across benchmarks under different maximum iteration epochs. The epoch index denotes the maximum number of allowed reasoning iterations, and predictions that terminate earlier are also included in evaluation. The dashed line indicates the single-round baseline (standard long-context reasoning for 8192 maximum tokens). Table 3. Performance on Out-of-distribution Benchmark (GPQADiamond). Bold indicates the best performance. Model GPQA-Diamond ACC TOK LAT Qwen3-VL-8B SFT VTC-R1 37.4 48.5(+11.1) Glyph Base SFT SFT TokenSkip VTC-R1 26.3 38.4 35.9 46.0(+7.6) 14.78 9.77 19.74 13.91 15.45 10. 26.88 9.57(2.8) 14.43 8.35 9.93 6.96(1.2) VTC-R1 yields accuracy improvements of 7.6% and 11.1%, indicating that the proposed approach generalizes effectively beyond in-distribution mathematical benchmarks. Efficient Inference. VTC-R1 achieves efficient inference latency across model architectures. On the Glyph architecture, speedup of at least 1.4 is observed across all benchmarks, with larger gains of 1.7 and 1.6 on the more challenging benchmarks. On the Qwen3-VL architecture, the inference speedup reaches up to 6.6. Although the proposed method is not explicitly designed as an adaptive reasoning framework, adaptivity naturally emerges from the data construction process, where different problems are associated with different numbers of reasoning iterations. As result, benchmarks of varying difficulty exhibit different effective token lengths (TOK). For instance, GSM8K requires fewer tokens, while AIME25 involves longer token sequences and more iteration epochs. The latency speedup consistently exceeds the reduction in token count. For example, on the Glyph for AMC23, the token count is reduced by approximately 1.3, whereas the latency improvement reaches 1.6. This discrepancy indicates that the introduction of vision-text compression provides additional efficiency gains beyond token reduction. Iteration Epochs. Figure 4 illustrates the accuracy of the proposed method across four benchmarks over different epoch settings. Here, the epoch index denotes the maximum number of allowed reasoning iterations. Predictions that terminate before reaching the maximum epoch are also included in the evaluation, which results in non-decreasing accuracy trend as the epoch limit increases. As shown in the figure, the accuracy consistently improves as the maximum epoch increases, demonstrating the effectiveness of multi-iteration reasoning. Across most benchmarks, the rate of accuracy improvement gradually diminishes, and performance begins to converge from approximately the fifth epoch onward. This observation indicates that the proposed method benefits from additional reasoning iterations while exhibiting stable convergence behavior. Overcoming Training Context Limitations. The gray dashed line in Figure 4 denotes the inference accuracy of standard long-context reasoning when the maximum number of newly generated tokens is set to 8,192, which also corresponds to the maximum token length used during training for our method. As the number of iteration epochs increases, the accuracy of the proposed method gradually surpasses the baseline across benchmarks. This result indicates that the proposed method is able to overcome the context length limitation imposed during training and achieve higher inference accuracy beyond the fixed training window. At the same time, efficient training is maintained, as evidenced by the reduced training cost reported in Table 6. 5.3. Ablation Study Segment Length. Table 4 reports the performance across benchmarks when different segmentation lengths (2K, 4K, and 6K) are used during training data construction, where 4K serves as the default setting. Across all four benchmarks, segmentation length of 4K achieves the best or highly competitive accuracy. In addition, on MATH500, AIME25, and AMC23, the latency (LAT) increases as the segmentation length grows. This behavior is expected, since larger segmentation lengths gradually approach standard long-context VTC-R1 : Vision-Text Compression for Efficient Long-Context Reasoning Segment Length GSM8K MATH500 AIME25 AMC ACC LAT ACC LAT ACC LAT ACC LAT 2K 4K 6K 93.9 93.6 93.7 0.39 0.34 0.92 82.4 86.0 84. 1.95 2.19 3.28 20.6 26.2 23.5 5.87 6.81 8.06 59.7 64.3 64.7 4.03 4.30 4.90 Table 4. Effect of segment length on accuracy (ACC) and latency (LAT) across benchmarks. Higher ACC and lower LAT indicate better performance. Best results for each metric are highlighted in bold. AIME25 AMC23 GPQA-D Baseline w/o Image 26.25 23.33(11.1%) 64.38 59.53(7.5%) 46.0 34.3(25.4%) Table 5. Performance comparison with and without image input. - denotes the relative performance drop. Method Training Data Training Time (h) Base SFT SFT VTC-R1 Table 6. Training time comparison across different methods. 60K QA pairs 60K QA pairs 106K QA pairs + 105K images 38.93 38.92 18.93 reasoning, which incurs higher inference cost due to longer effective reasoning sequences. Image Input. We further analyze the performance of VTCR1 when image inputs are removed at each reasoning iteration. Three more challenging benchmarks AIME25, AMC23, and GPQA-D, are selected for this analysis, which are more likely to benefit from multi-step reasoning. As shown in Table 5, removing image inputs leads to accuracy drops of 11.1% and 7.5% on AIME25 and AMC23, with more substantial degradation of 25.4% observed on GPQA-D. These results indicate that VTC-R1 relies on rendered images as form of memory for previous reasoning steps during inference. At the same time, non-trivial level of accuracy is retained even without image inputs. This can be attributed to the fact that many problems can be solved within single reasoning iteration; in the absence of image conditioning, the model effectively restarts the reasoning process from scratch and can still obtain correct answers. 5.4. Efficiency Analysis Training Efficiency. Table 6 reports the training time of VTC-R1 in comparison with Base SFT and SFT. All training times are measured using the LlamaFactory framework under same configuration. Although the proposed method adopts multi-iteration training paradigm and therefore introduces more QA pairs as well as additional images, the overall training time is reduced to approximately 48% of that required by the baseline methods. This result demonstrates the training efficiency of VTC-R1. And the final performance of VTC-R1 is superior as shown in Table 2. The reduction in training time is attributed to the standard long-context reasoning involves substantially longer reason8 ing sequences, where training cost increases rapidly as the reasoning length grows. In contrast, VTC-R1 constrains the reasoning length within each iteration to controlled range, which leads to improved training efficiency. Rendering Efficiency. Table 2 shows that VTC-R1 significantly outperforms all baselines in terms of end-to-end latency, where the reported metric already accounts for the overhead of rendering and image processing. We further provide fine-grained statistics to validate that the introduced vision-text compression mechanism is lightweight. Based on an analysis of 100 samples from the dataset, we observe that for an average of approximately 1,600 text tokens per image, the rendering process requires only 0.12s on average, while image processing takes merely 0.02s. Compared to the overall model inference latency, this additional overhead is negligible (4% of the total latency). Moreover, the average generated image size is around 0.1 MB, which falls within practical and manageable range for real-world systems. 5.5. Case Study We present four examples in Appendix B.5 to qualitatively analyze the behavior of VTC-R1. These examples illustrate that our method can condition on prior reasoning to perform solution verification, reasoning summarization, error correction based on identified contradictions, and direct continuation of preceding reasoning. Together, they demonstrate that images rendered from previous reasoning segments can be effectively leveraged to support multi-step reasoning. 6. Conclusion We propose VTC-R1, an efficient long-context reasoning paradigm that integrates vision-text compression into iterative reasoning. By rendering previous reasoning segments into compact visual representations, VTC-R1 replaces long textual contexts with significantly fewer vision tokens in lightweight and model-free manner. Extensive experiments show that VTC-R1 consistently improves reasoning accuracy across multiple benchmarks while achieving up to 3.4 token compression and 2.7 end-to-end inference speedup. The results demonstrate that VTC-R1 provides an effective alternative representation for scalable long-context reasoning. We hope our work would inspire further exploration of efficient reasoning beyond pure text-based paradigms. VTC-R1 : Vision-Text Compression for Efficient Long-Context Reasoning"
        },
        {
            "title": "Impact Statement",
            "content": "This paper presents work whose goal is to advance the field of LLMs Reasoning. There are many potential societal consequences of our work, none which we feel must be specifically highlighted here."
        },
        {
            "title": "References",
            "content": "Bai, S., Cai, Y., Chen, R., Chen, K., Chen, X., Cheng, Z., Deng, L., Ding, W., Gao, C., Ge, C., Ge, W., Guo, Z., Huang, Q., Huang, J., Huang, F., Hui, B., Jiang, S., Li, Z., Li, M., Li, M., Li, K., Lin, Z., Lin, J., Liu, X., Liu, J., Liu, C., Liu, Y., Liu, D., Liu, S., Lu, D., Luo, R., Lv, C., Men, R., Meng, L., Ren, X., Ren, X., Song, S., Sun, Y., Tang, J., Tu, J., Wan, J., Wang, P., Wang, P., Wang, Q., Wang, Y., Xie, T., Xu, Y., Xu, H., Xu, J., Yang, Z., Yang, M., Yang, J., Yang, A., Yu, B., Zhang, F., Zhang, H., Zhang, X., Zheng, B., Zhong, H., Zhou, J., Zhou, F., Zhou, J., Zhu, Y., and Zhu, K. Qwen3-vl technical report. arXiv preprint arXiv:2511.21631, 2025. Beltagy, LongI., Peters, M. E., and Cohan, A. former: The long-document transformer. arXiv preprint arXiv:2004.05150, 2020. Chen, M., Tworek, J., Jun, H., Yuan, Q., de Oliveira Pinto, H. P., Kaplan, J., Edwards, H., Burda, Y., Joseph, N., Brockman, G., Ray, A., Puri, R., Krueger, G., Petrov, M., Khlaaf, H., Sastry, G., Mishkin, P., Chan, B., Gray, S., Ryder, N., Pavlov, M., Power, A., Kaiser, L., Bavarian, M., Winter, C., Tillet, P., Such, F. P., Cummings, D., Plappert, M., Chantzis, F., Barnes, E., HerbertVoss, A., Guss, W. H., Nichol, A., Paino, A., Tezak, N., Tang, J., Babuschkin, I., Balaji, S., Jain, S., Saunders, W., Hesse, C., Carr, A. N., Leike, J., Achiam, J., Misra, V., Morikawa, E., Radford, A., Knight, M., Brundage, M., Murati, M., Mayer, K., Welinder, P., McGrew, B., Amodei, D., McCandlish, S., Sutskever, I., and Zaremba, W. Evaluating large language models trained on code, 2021. URL https://arxiv.org/abs/ 2107.03374. Chen, X., Xu, J., Liang, T., He, Z., Pang, J., Yu, D., Song, L., Liu, Q., Zhou, M., Zhang, Z., Wang, R., Tu, Z., Mi, H., and Yu, D. Do not think that much for 2+3=? on the overthinking of o1-like llms, 2025. URL https: //arxiv.org/abs/2412.21187. Cheng, J., Liu, Y., Zhang, X., Fei, Y., Hong, W., Lyu, R., Wang, W., Su, Z., Gu, X., Liu, X., Bai, Y., Tang, J., Wang, H., and Huang, M. Glyph: Scaling context windows via visual-text compression. arXiv preprint arXiv:2510.17800, 2025. Cobbe, K., Kosaraju, V., Bavarian, M., Chen, M., Jun, H., Kaiser, L., Plappert, M., Tworek, J., Hilton, J., Nakano, R., Hesse, C., and Schulman, J. Training verifiers to solve math word problems. arXiv preprint arXiv:2110.14168, 2021. Feng, L., Yang, F., Chen, F., Cheng, X., Xu, H., Wan, Z., Yan, M., and An, B. Agentocr: Reimagining agent history via optical self-compression. arXiv preprint arXiv:2601.04786, 2026. Gong, R., Liu, Y., Qu, W., Du, M., He, Y., Ma, Y., Chen, Y., Liu, X., Wen, Y., Li, X., Wang, R., Zhu, X., Hooi, B., and Zhang, J. Efficient reasoning via chain of unconscious thought, 2025. URL https://arxiv.org/ abs/2505.19756. Guo, D., Yang, D., Zhang, H., Song, J., Wang, P., Zhu, Q., Xu, R., Zhang, R., Ma, S., Bi, X., et al. Deepseekr1 incentivizes reasoning in llms through reinforcement learning. Nature, 645(8081):633638, 2025. Hao, S., Sukhbaatar, S., Su, D., Li, X., Hu, Z., Weston, J., and Tian, Y. Training large language models to reason in continuous latent space, 2024. URL https:// arxiv.org/abs/2412.06769. Hendrycks, D., Burns, C., Kadavath, S., Arora, A., Basart, S., Tang, E., Song, D., and Steinhardt, J. Measuring mathematical problem solving with the math dataset. NeurIPS, 2021. Hou, B., Zhang, Y., Ji, J., Liu, Y., Qian, K., Andreas, J., and Chang, S. Thinkprune: Pruning long chain-of-thought of llms via reinforcement learning, 2025. URL https: //arxiv.org/abs/2504.01296. Hu, Z., Wang, Y., Dong, H., Xu, Y., Saha, A., Xiong, C., Hooi, B., and Li, J. Beyondaha!: Toward systematic meta-abilities alignment in large reasoning models. arXiv preprint arXiv:2505.10554, 2025. Huang, J. and Chang, K. C.-C. Towards reasoning in large language models: survey. In Findings of the association for computational linguistics: ACL 2023, pp. 10491065, 2023. Hugging Face. Open r1: fully open reproduction of deepseek-r1, January 2025. URL https://github. com/huggingface/open-r1. Jiang, J., Wang, F., Shen, J., Kim, S., and Kim, S. survey on large language models for code generation. arXiv preprint arXiv:2406.00515, 2024. Kang, Y., Sun, X., Chen, L., and Zou, W. C3ot: Generating shorter chain-of-thought without compromising effectiveness, 2024. URL https://arxiv.org/abs/ 2412.11664. 9 VTC-R1 : Vision-Text Compression for Efficient Long-Context Reasoning Kitaev, N., Kaiser, L., and Levskaya, A. Reformer: The efficient transformer. In International Conference on Learning Representations, 2020. URL https:// openreview.net/forum?id=rkgNKkHtvB. Kwon, W., Li, Z., Zhuang, S., Sheng, Y., Zheng, L., Yu, C. H., Gonzalez, J. E., Zhang, H., and Stoica, I. Efficient memory management for large language model serving with pagedattention. In Proceedings of the ACM SIGOPS 29th Symposium on Operating Systems Principles, 2023. Lee, A., Che, E., and Peng, T. How well do llms compress their own chain-of-thought? token complexity approach. arXiv preprint arXiv:2503.01141, 2025. Li, Z.-Z., Liang, X., Tang, Z., Ji, L., Wang, P., Xu, H., W, X., Huang, H., Deng, W., Gong, Y., Guo, Z., Liu, X., Yin, F., and Liu, C.-L. Tl;dr: Too long, do re-weighting for efficient llm reasoning compression, 2025a. URL https://arxiv.org/abs/2506.02678. Li, Z.-Z., Zhang, D., Zhang, M.-L., Zhang, J., Liu, Z., Yao, Y., Xu, H., Zheng, J., Wang, P.-J., Chen, X., Zhang, Y., Yin, F., Dong, J., Li, Z., Bi, B.-L., Mei, L.-R., Fang, J., Guo, Z., Song, L., and Liu, C.-L. From system 1 to system 2: survey of reasoning large language models, 2025b. URL https://arxiv.org/abs/2502.17419. Lightman, H., Kosaraju, V., Burda, Y., Edwards, H., Baker, B., Lee, T., Leike, J., Schulman, J., Sutskever, I., and Cobbe, K. Lets verify step by step. arXiv preprint arXiv:2305.20050, 2023. Liu, T., Chen, Z., Liu, Z., Tian, M., and Luo, W. Expediting and elevating large language model reasoning via hidden chain-of-thought decoding. arXiv preprint arXiv:2409.08561, 2024. Liu, Y., Wu, J., He, Y., Gao, H., Chen, H., Bi, B., Zhang, J., Huang, Z., and Hooi, B. Efficient inference for large reasoning models: survey. arXiv preprint arXiv:2503.23077, 2025. Luo, H., He, H., Wang, Y., Yang, J., Liu, R., Tan, N., Cao, X., Tao, D., and Shen, L. Adar1: From long-cot to hybridcot via bi-level adaptive reasoning optimization, 2025a. URL https://arxiv.org/abs/2504.21659. Luo, H., Shen, L., He, H., Wang, Y., Liu, S., Li, W., Tan, N., Cao, X., and Tao, D. O1-pruner: Length-harmonizing fine-tuning for o1-like reasoning pruning, 2025b. URL https://arxiv.org/abs/2501.12570. Ma, W., He, J., Snell, C., Griggs, T., Min, S., and Zaharia, M. Reasoning models can be effective without thinking, 2025a. URL https://arxiv.org/abs/ 2504.09858. Ma, X., Wan, G., Yu, R., Fang, G., and Wang, X. Cot-valve: Length-compressible chain-of-thought tuning, 2025b. URL https://arxiv.org/abs/2502.09601. Math-AI. Amc23 dataset, 2025. URL https: //huggingface.co/datasets/math-ai/ amc23. Munkhbat, T., Ho, N., Kim, S. H., Yang, Y., Kim, Y., and Yun, S.-Y. Self-training elicits concise reasoning in large language models, 2025. URL https://arxiv.org/ abs/2502.20122. Ning, Y., Li, W., Fang, J., Tan, N., and Liu, H. Not all thoughts are generated equal: Efficient llm reasoning via multi-turn reinforcement learning, 2025. URL https: //arxiv.org/abs/2505.11827. OpenAI. llms. learning-to-reason-with-llms/, [Accessed 19-09-2024]. Learning with https://openai.com/index/ 2024. reason to Pan, J., Li, X., Lian, L., Snell, C., Zhou, Y., Yala, A., Darrell, T., Keutzer, K., and Suhr, A. Learning adaptive parallel reasoning with language models, 2025. URL https: //arxiv.org/abs/2504.15466. Qiao, Z., Deng, Y., Zeng, J., Wang, D., Wei, L., Meng, F., Zhou, J., Ren, J., and Zhang, Y. Concise: Confidenceguided compression in step-by-step efficient reasoning, 2025. URL https://arxiv.org/abs/2505. 04881. Rein, D., Hou, B. L., Stickland, A. C., Petty, J., Pang, R. Y., Dirani, J., Michael, J., and Bowman, S. R. GPQA: graduate-level google-proof q&a benchmark. In First Conference on Language Modeling, 2024. URL https: //openreview.net/forum?id=Ti67584b98. Team, K., Du, A., Gao, B., Xing, B., Jiang, C., Chen, C., Li, C., Xiao, C., Du, C., Liao, C., et al. Kimi k1.5: Scaling reinforcement learning with llms. arXiv preprint arXiv:2501.12599, 2025a. Team, V., Hong, W., Yu, W., Gu, X., Wang, G., Gan, G., Tang, H., Cheng, J., Qi, J., Ji, J., Pan, L., Duan, S., Wang, W., Wang, Y., Cheng, Y., He, Z., Su, Z., Yang, Z., Pan, Z., Zeng, A., Wang, B., Chen, B., Shi, B., Pang, C., Zhang, C., Yin, D., Yang, F., Chen, G., Xu, J., Zhu, J., Chen, J., Chen, J., Chen, J., Lin, J., Wang, J., Chen, J., Lei, L., Gong, L., Pan, L., Liu, M., Xu, M., Zhang, M., Zheng, Q., Yang, S., Zhong, S., Huang, S., Zhao, S., Xue, S., Tu, S., Meng, S., Zhang, T., Luo, T., Hao, T., Tong, T., Li, W., Jia, W., Liu, X., Zhang, X., Lyu, X., Fan, X., Huang, X., Wang, Y., Xue, Y., Wang, Y., Wang, Y., An, Y., Du, Y., Shi, Y., Huang, Y., Niu, Y., Wang, Y., Yue, Y., 10 VTC-R1 : Vision-Text Compression for Efficient Long-Context Reasoning model guidance at inference time, 2025c. URL https: //arxiv.org/abs/2504.12329. Yao, H., Huang, J., Wu, W., Zhang, J., Wang, Y., Liu, S., Wang, Y., Song, Y., Feng, H., Shen, L., and Tao, D. Mulberry: Empowering mllm with o1-like reasoning and reflection via collective monte carlo tree search, 2024. URL https://arxiv.org/abs/2412.18319. Yao, H., Zhang, R., Huang, J., Zhang, J., Wang, Y., Fang, B., Zhu, R., Jing, Y., Liu, S., Li, G., et al. survey on agentic multimodal large language models. arXiv preprint arXiv:2510.10991, 2025. Yao, S., Yu, D., Zhao, J., Shafran, I., Griffiths, T., Cao, Y., and Narasimhan, K. Tree of thoughts: Deliberate problem solving with large language models. Advances in neural information processing systems, 36:1180911822, 2023. Zaheer, M., Guruganesh, G., Dubey, K. A., Ainslie, J., Alberti, C., Ontanon, S., Pham, P., Ravula, A., Wang, Q., Yang, L., et al. Big bird: Transformers for longer sequences. Advances in neural information processing systems, 33:1728317297, 2020. Zhang, J., Zhu, Y., Sun, M., Luo, Y., Qiao, S., Du, L., Zheng, D., Chen, H., and Zhang, N. Lightthinker: Thinking stepby-step compression, 2025. URL https://arxiv. org/abs/2502.15589. Zhang, Y. and Math-AI, T. American invitational mathematics examination (aime) 2025, 2025. Zhao, H., Wang, M., Zhu, F., Liu, W., Ni, B., Zeng, F., Meng, G., and Zhang, Z. Vtcbench: Can vision-language models understand long context with vision-text compression?, 2025. URL https://arxiv.org/abs/ 2512.15649. Zheng, Y., Zhang, R., Zhang, J., Ye, Y., Luo, Z., Feng, Z., and Ma, Y. Llamafactory: Unified efficient finetuning of 100+ language models. In Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 3: System Demonstrations), Bangkok, Thailand, 2024. Association for Computational Linguistics. URL http://arxiv.org/abs/2403. 13372. Zhuang, R., Wang, B., and Sun, S. Accelerating chainof-thought reasoning: When goal-gradient importance meets dynamic skipping, 2025. URL https://arxiv. org/abs/2505.08392. Li, Y., Zhang, Y., Wang, Y., Wang, Y., Zhang, Y., Xue, Z., Hou, Z., Du, Z., Wang, Z., Zhang, P., Liu, D., Xu, B., Li, J., Huang, M., Dong, Y., and Tang, J. Glm-4.5v and glm-4.1v-thinking: Towards versatile multimodal reasoning with scalable reinforcement learning, 2025b. URL https://arxiv.org/abs/2507.01006. Wang, S., Li, B. Z., Khabsa, M., Fang, H., and Ma, H. Linformer: Self-attention with linear complexity, 2020. Wang, Y., Luo, H., Yao, H., Huang, T., He, H., Liu, R., Tan, N., Huang, J., Cao, X., Tao, D., et al. R1-compress: Long chain-of-thought compression via chunk compression and search. arXiv preprint arXiv:2505.16838, 2025. Wang, Y., Li, S., Li, P., Yang, X., Tang, Y., and Wei, Z. Render-of-thought: Rendering textual chain-of-thought as images for visual latent reasoning. arXiv preprint arXiv:2601.14750, 2026a. Wang, Y., Wang, L., Deng, Y., Wu, K., Xiao, Y., Yao, H., Kang, L., Ye, H., Jing, Y., and Bing, L. Deepresearcheval: An automated framework for deep research task construction and agentic evaluation. arXiv preprint arXiv:2601.09688, 2026b. Wei, H., Sun, Y., and Li, Y. Deepseek-ocr: Contexts optical compression. arXiv preprint arXiv:2510.18234, 2025. Xia, H., Li, Y., Leong, C. T., Wang, W., and Li, W. Tokenskip: Controllable chain-of-thought compression in llms, 2025. URL https://arxiv.org/abs/ 2502.12067. Xing, L., Wang, A. J., Yan, R., Qu, H., Li, Z., and Tang, J. See the text: From tokenization to visual reading. arXiv preprint arXiv:2510.18840, 2025a. Xing, L., Wang, A. J., Yan, R., Shu, X., and Tang, J. Visioncentric token compression in large language model. arXiv preprint arXiv:2502.00791, 2025b. Yan, Y., Shen, Y., Liu, Y., Jiang, J., Zhang, M., Shao, J., and Zhuang, Y. Inftythink: Breaking the length limits of long-context reasoning in large language models, 2025. URL https://arxiv.org/abs/2503.06692. Yang, C., Si, Q., Duan, Y., Zhu, Z., Zhu, C., Lin, Z., Cao, L., and Wang, W. Dynamic early exit in reasoning models, 2025a. URL https://arxiv.org/abs/2504. 15895. Yang, J., Lin, K., and Yu, X. Think when you need: Selfadaptive chain-of-thought learning, 2025b. URL https: //arxiv.org/abs/2504.03234. Yang, W., Yue, X., Chaudhary, V., and Han, X. Speculative thinking: Enhancing small-model reasoning with large 11 VTC-R1 : Vision-Text Compression for Efficient Long-Context Reasoning A. Image Rendering A.1. Rendering Configuration Factor dpi page size font family font size alignment margins indent spacing scale colors borders newline markup auto crop Specification / Sampling Strategy Mixture of sets: lowest (4559), low (6071), medium (72119), normal ({72, 80, 96, 100, 110, 120, 144, 150, 300}), high (over 300); favor normal/medium with small probability spikes to extremes. (i) Fixed paper sizes (A4, Letter, Legal, A5, B5, A3, B4, Tabloid) with priors; (ii) common aspect ratios (e.g., 1.414, 1.333, 1.5, 1.778); (iii) fully random aspect via piecewise distribution (narrow tall). Pooled and deduplicated families across serif/sans/mono/pixel; italics sampled by filename heuristics (suffixes, italic/oblique). {7, 7.5, 8, 9, 9.5, 10, 11, 12, 14}; line height tied as font size + {0, . . . , 3}. LEFT/JUSTIFY (dominant) with small-prob. RIGHT/CENTER. Three patterns: all-equal; vertical-larger; horizontal-larger; values in 1040 pt ranges. Modes: none; first-line indent (12.5 em); block/hanging with left/right indents. space-before/space-after use multi-mode prior (none, small, large). Horizontal glyph scaling (0.751.0) with decaying probabilities. Page/background/font palettes for light/dark themes; document/web/code styles inherit coherent triplets (page, paragraph, font). Optional paragraph borders with width/padding; disabled by default. With small probability, explicit markers (e.g., n, tags, or tokens) inserted to preserve structure. Optional white-margin cropping and last-page trimming. Table 7. Rendering configuration factors in the rendering pipeline and their sampling strategies. The rendering pipeline is parameterized by configuration vector. Following (Cheng et al., 2025), set of rendering configuration factors is adopted, as summarized in Table 7. These factors determine the final rendering properties, including layout, visual clarity, and typography. The default configuration used in our experiments is reported in Figure 5. This configuration largely follows the default settings of Glyph. However, since the default Glyph font may produce incorrect glyphs when rendering certain mathematical symbols, the font is replaced with DejaVuSans.ttf in our implementation. A.2. Rendering Example Figure 6 presents an example image rendered under the default configuration specified in Figure 5. B. Details about Experiments B.1. Implementation Details. Supervised fine-tuning is conducted using the LlamaFactory library (Zheng et al., 2024). For all methods and across all model architectures, learning rate of 1 105 is used, with warmup ratio of 0.1 and cosine learning rate schedule. Training is performed for one epoch with batch size of 64, the maximum sequence length is increased to 32,768 tokens. All models are trained using 8 NVIDIA H20 GPUs with 96 GB of memory. We adopt the official implementation of TokenSkip (Xia et al., 2025), which supports compression ratios ranging from 0.6 to 0.9. We observe that training becomes unstable and collapses when the ratio is set to 0.6; therefore, we use compression ratio of 0.8 in our experiments. All evaluation experiments are conducted on single NVIDIA H20 GPU with 96 GB of memory. Inference is performed using the vLLM framework, with temperature of 0.6 and top-p value of 0.95. For standard SFT, the maximum number of generated tokens (max new tokens) is set to 32,768. For VTC-R1, the maximum number of generated tokens per iteration is set to 8,192, and the maximum number of iterations is set to 8. 12 VTC-R1 : Vision-Text Compression for Efficient Long-Context Reasoning Parameter Value page-size 595 842 dpi 72 margin-x 10 margin-y 10 DejaVuSans.ttf font-path font-size 9 line-height 10 font-color #000000 alignment LEFT horizontal-scale 1.0 first-line-indent 0 left-indent 0 right-indent 0 space-before 0 space-after 0 border-width 0 border-padding 0 page-bg-color #FFFFFF para-bg-color #FFFFFF auto-crop-width true auto-crop-last-page true Figure 5. Default rendering configuration used in our experiments. Figure 6. Example rendered page. B.2. Benchmark GSM8K: widely used benchmark for multi-step reasoning, consisting of 8,500 grade school math word problems, with canonical test set of 1,319 problems. MATH500: challenging math dataset comprising 500 problems from high school math competitions. AIME25: benchmark dataset consisting of 30 challenging mathematical problems from the 2025 American Invitational Mathematics Examination. AMC23: challenging evaluation set comprising 50 problems from the 2023 American Mathematics Competitions, serving as benchmark for competition-level mathematical reasoning. GPQA-Diamond: high-quality subset of the GPQA benchmark, with 198 complex graduate-level multiple-choice questions across various scientific domains. It serves as the out-of-distribution benchmark in our evaluation. B.3. Training Dataset We use OpenR1-Math-Inf (Yan et al., 2025), which is subset of OpenR1-Math-220K (Hugging Face, 2025). The OpenR1-Math-220k dataset, large-scale benchmark for mathematical reasoning. It consists of 220k math problems, each accompanied by two to four reasoning traces generated by DeepSeek R1 for problems sourced from NuminaMath 1.5. All traces have been verified using Math Verify. We first perform data cleaning on the OpenR1-Math-Inf dataset, resulting in 60,688 valid instances. In OpenR1-Math-Inf, for each instance, the original reasoning trace is partitioned into multiple segments based on hyperparameter η, which controls the maximum token length of each segment. Following the data construction procedure of VTC-R1, this process yields total of 106K training instances and approximately 105K rendered images. For the final answer A, the special token sequence <answer> </answer> is used to facilitate answer extraction 13 VTC-R1 : Vision-Text Compression for Efficient Long-Context Reasoning and to explicitly indicate the termination of the reasoning process. For instances consisting of more than one reasoning step, when step > 1, the intermediate supervision is formatted as <think>Got it, lets continue. {step text}</think>. B.4. Batch Inference Algorithm 2 VTC-R1 Batch Inference Input: batch of questions = {Q1, . . . , QB}; initial images {I init Sv; rendering operator Rθ; maximum iteration Initialize: 1 , . . . , init }; vision language model ; system prompt Active request set {1, . . . , B} Current image sets Ik init Final answers {}B k=1 for all {1, . . . , B} for = 1 to do if = then break end if Batch Generation via vLLM: Construct batch prompts {(Sv, Qk, Ik) S} Obtain batch outputs: {Ok}kS (P) Update States and Render: for each do if Ok produces the final answer then Ak ExtractAnswer(Ok) {k} {Remove finished request from active set} else Extract reasoning progress LRk from Ok Render reasoning into images: Inew Rθ(LRk) Update image history: Ik Ik {Inew} end if end for end for if = then Handle Time-out Requests: for each do Ak ExtractAnswer(Ok) end for end if Output: set of final answers = {A1, . . . , AB} B.5. Case Study The gray-shaded regions indicate reasoning steps that are performed by conditioning on images rendered from previous reasoning segments. Examples 14 are provided below. Example 1 demonstrates further verification of previously obtained solution. Example 2 derives the final answer by summarizing completed prior reasoning. Example 3 performs error correction and reflection based on contradictions identified in earlier reasoning, eventually reaching the correct answer. Example 4 continues the reasoning process by building directly upon preceding reasoning steps. Collectively, these examples demonstrate that our method can successfully leverage images as optical memory to support reasoning. 14 VTC-R1 : Vision-Text Compression for Efficient Long-Context Reasoning Figure 7. Example 1. Figure 8. Example 2. 15 VTC-R1 : Vision-Text Compression for Efficient Long-Context Reasoning Figure 9. Example 3. Figure 10. Example 4."
        }
    ],
    "affiliations": [
        "Alibaba Cloud Computing",
        "Nanyang Technical University"
    ]
}