{
    "paper_title": "OneThinker: All-in-one Reasoning Model for Image and Video",
    "authors": [
        "Kaituo Feng",
        "Manyuan Zhang",
        "Hongyu Li",
        "Kaixuan Fan",
        "Shuang Chen",
        "Yilei Jiang",
        "Dian Zheng",
        "Peiwen Sun",
        "Yiyuan Zhang",
        "Haoze Sun",
        "Yan Feng",
        "Peng Pei",
        "Xunliang Cai",
        "Xiangyu Yue"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Reinforcement learning (RL) has recently achieved remarkable success in eliciting visual reasoning within Multimodal Large Language Models (MLLMs). However, existing approaches typically train separate models for different tasks and treat image and video reasoning as disjoint domains. This results in limited scalability toward a multimodal reasoning generalist, which restricts practical versatility and hinders potential knowledge sharing across tasks and modalities. To this end, we propose OneThinker, an all-in-one reasoning model that unifies image and video understanding across diverse fundamental visual tasks, including question answering, captioning, spatial and temporal grounding, tracking, and segmentation. To achieve this, we construct the OneThinker-600k training corpus covering all these tasks and employ commercial models for CoT annotation, resulting in OneThinker-SFT-340k for SFT cold start. Furthermore, we propose EMA-GRPO to handle reward heterogeneity in multi-task RL by tracking task-wise moving averages of reward standard deviations for balanced optimization. Extensive experiments on diverse visual benchmarks show that OneThinker delivers strong performance on 31 benchmarks, across 10 fundamental visual understanding tasks. Moreover, it exhibits effective knowledge transfer between certain tasks and preliminary zero-shot generalization ability, marking a step toward a unified multimodal reasoning generalist. All code, model, and data are released."
        },
        {
            "title": "Start",
            "content": "OneThinker: All-in-one Reasoning Model for Image and Video Kaituo Feng1,2 Manyuan Zhang2 Hongyu Li2 Kaixuan Fan1,2 Shuang Chen2 Yilei Jiang1,2 Dian Zheng1,2 Yan Feng2 Peiwen Sun1 Yiyuan Zhang1 Haoze Sun2 Peng Pei2 Xunliang Cai2 Xiangyu Yue1 1MMLab, CUHK 2Meituan 5 2 0 2 3 ] . [ 2 3 4 0 3 0 . 2 1 5 2 : r Home: https://github.com/tulerfeng/OneThinker HF:https://huggingface.co/OneThink Figure 1: Overview of our OneThinker, which is capable of thinking across wide range of tasks for image and video understanding."
        },
        {
            "title": "ABSTRACT",
            "content": "Reinforcement learning (RL) has recently achieved remarkable success in eliciting visual reasoning within Multimodal Large Language Models (MLLMs). However, existing approaches typically train separate models for different tasks and treat image and video reasoning as disjoint domains. This results in limited scalability toward multimodal reasoning generalist, which restricts practical versatility and hinders potential knowledge sharing across tasks and modalities. To this end, we propose OneThinker, an all-in-one reasoning model that unifies image and video understanding across diverse fundamental visual tasks, including question answering, captioning, spatial and temporal grounding, tracking, and segmentation. To achieve this, we construct the OneThinker-600k training corpus covering all these tasks and employ commercial models for CoT annotation, resulting in OneThinker-SFT-340k for SFT cold start. Furthermore, we propose EMA-GRPO to handle reward heterogeneity in multi-task RL by tracking task-wise moving averages of reward standard deviations for balanced optimization. Extensive experiments on diverse visual benchmarks show that OneThinker delivers strong performance on 31 benchmarks, across 10 fundamental visual understanding tasks. Moreover, it exhibits effective knowledge transfer between certain tasks and preliminary zero-shot generalization ability, marking step toward unified multimodal reasoning generalist. All code, model, and data are released. Project Leader. Corresponding Author. OneThinker: All-in-one Reasoning Model for Image and Video Figure 2: Performance gains of our model over Qwen3-VL-Instruct-8B across diverse visual tasks after training."
        },
        {
            "title": "Introduction",
            "content": "Reasoning serves as cornerstone in advancing Multimodal Large Language Models (MLLMs) toward artificial general intelligence (AGI), enabling them to perform step-by-step inference over complex visuallinguistic inputs [1, 2, 3]. Inspired by DeepSeek-R1 [4], growing number of studies have witnessed the success of adopting reinforcement learning (RL) with the Group Relative Policy Optimization (GRPO) algorithm to enhance reasoning abilities [5, 6, 7, 8, 9]. For instance, Vision-R1 [10] and Video-R1 [9] demonstrate strong reasoning performance on image and video question answering, respectively, while VLM-R1 [11] excels in image detection and Seg-R1 [12] in segmentation. These advances underscore the remarkable effectiveness and broad potential of RL-based training for wide range of visual tasks. However, existing thinking models are usually designed to handle only single task and operate exclusively on either images or videos. Such separation greatly limits their practical versatility and may also hinder the potential benefits of cross-task and cross-modal knowledge transfer. Although few works have explored extending MLLMs with RL for multiple tasks [13, 14, 15], they are usually confined to limited subsets of visual tasks within single modality. Furthermore, these approaches are often constrained by small-scale tuning, which limits their ability to generalize beyond specific domains. For instance, VideoChat-R1 [13] performs co-training on only three spatiotemporal perception tasks with merely 18k samples, and remains restricted to the video modality. Recognizing that vision inherently encompasses both static images and dynamic videos, and that real-world scenarios demand unified reasoning across diverse visual tasks, we pose question: Can we train an all-in-one multimodal reasoning generalist, which is capable of simultaneously handling both image and video understanding across diverse fundamental visual tasks? To achieve this, we present OneThinker, unified multimodal reasoning generalist capable of handling wide range of visual reasoning tasks, including question answering, captioning, spatial and temporal grounding, tracking, and segmentation. First, we curate large-scale dataset OneThinker-600k, comprising approximately 600k multimodal samples that jointly cover these fundamental visual tasks. We then employ strong proprietary model Seed1.5-VL [16] to annotate and filter high-quality chain-of-thought (CoT) data, resulting in OneThinker-SFT-340k dataset for SFT cold start. Through joint multi-task training across both images and videos, OneThinker effectively learns to reason over spatial and temporal cues in unified manner. 2 OneThinker: All-in-one Reasoning Model for Image and Video Besides, considering the distinct reward characteristics of heterogeneous visual tasks, we further introduce EMA-GRPO to improve RL training. This is motivated by two complementary imbalances: Standard GRPO suffers from intra-task imbalance because its sample-wise standard deviation (std) normalization favors low-variance rollouts [17, 18, 19, 20]; Conversely, removing this std normalization, as in Dr.GRPO [17], causes inter-task imbalance, where sparse-reward tasks (e.g., math) dominate while dense ones (e.g., detection) are suppressed. EMA-GRPO addresses both issues by maintaining task-wise exponential moving averages of reward standard deviations for normalization. This design allows each task to have stable yet adaptive normalization scale that reflects its own reward dynamics. It balances intra-task weighting by reducing bias toward low-variance samples and prevents inter-task imbalance by using independent normalization statistics for different tasks, resulting in stable and balanced optimization across diverse visual tasks. Extensive experiments demonstrate that OneThinker achieves consistently strong performance across diverse visual reasoning benchmarks. For example, OneThinker-8B reaches 70.6% accuracy on MMMU [21] and 64.3% on MathVerse [22] for image QA. In perception-oriented tasks such as grounding, tracking, and segmentation, our model also delivers strong results, for example, 84.4 R@0.5 on GOT-10k [23] and 54.9 J&F on ReasonVOS [24]. Moreover, unified training across tasks and modalities encourages effective knowledge sharing, allowing the model to transfer reasoning skills between several related tasks and exhibit preliminary zero-shot generalization on unseen scenarios. Our main contributions are summarized as follows: We propose OneThinker, unified multimodal reasoning generalist that handles wide range of image and video tasks within single model, including question answering, captioning, grounding, tracking, and segmentation. To support training, we construct the large-scale datasets OneThinker-600k and its CoTannotated subset OneThinker-SFT-340k. To address the distinct reward characteristics of heterogeneous visual tasks, we introduce EMA-GRPO, which mitigates both intra-task and inter-task imbalance through task-wise adaptive normalization of reward statistics. Extensive experiments demonstrate that OneThinker achieves superior results on 31 benchmarks, across 10 fundamental visual understanding tasks. Besides, it promotes effective knowledge sharing in certain tasks, and exhibits preliminary zero-shot generalization abilities."
        },
        {
            "title": "2.1 Reinforcement Learning for LLM Reasoning",
            "content": "Reinforcement learning (RL) has emerged as powerful technique for enhancing the reasoning capabilities of Large Language Models (LLMs) [7, 25, 26, 27, 28]. Recent studies, exemplified by DeepSeek-R1, adopt rule-based RL with Group Relative Policy Optimization (GRPO) [4] algorithm to directly optimize outcome-level rewards, enabling stepby-step reasoning without explicit intermediate supervision. The success of DeepSeek-R1 motivates surge of works exploring this paradigm further [7, 29, 30]. For example, Dr.GRPO [17] introduces an unbiased optimization method that addresses the sample-wise standard deviation imbalance and response-length bias inherent in standard GRPO. Besides, GSPO [25] introduces sequence-level RL algorithm that replaces token-wise ratios with sequence-level optimization, improving training stability for large-scale Mixture-of-Experts models. Critique-GRPO [7] integrates natural language feedback to guide policy optimization, enabling LLMs to refine their reasoning through critique-based self-improvement beyond standard RL fine-tuning. However, most existing research still focuses on single tasks or homogeneous reasoning objectives, where reward distributions remain relatively consistent."
        },
        {
            "title": "2.2 Reasoning in MLLMs",
            "content": "Inspired by the success of reasoning in LLMs, rising trend of works aims to bring this capability into MLLMs, enabling reasoning in different visual tasks [8, 31, 9, 32, 3, 33, 34, 35, 36]. For instance, Vision-R1 [10] tackles complex image reasoning in visual question answering, while Video-R1 [9] advances question answering over dynamic video inputs. Perception-R1 [14] and VLM-R1 [11] further extend this paradigm to image object detection, revealing the potential of RL for perception-oriented tasks. Seg-R1 [12] introduces decoupled reasoningsegmentation framework that employs GRPO-based RL to generate explicit chain-of-thought reasoning and positional prompts for image segmentation tasks. Time-R1 [37] adapts RL-based post-training to temporal grounding in videos and achieves promising results, whereas VideoChat-R1 [13] applies reinforcement fine-tuning on three spatio-temporal tasks to enhance perception and reasoning 3 OneThinker: All-in-one Reasoning Model for Image and Video Figure 3: Overview of our curated training dataset, including both image and video modalities for diverse range of understanding tasks. in video understanding. SophiaVL-R1 [38] introduces thinking-process rewards to improve RL training for image question answering. While these approaches have achieved remarkable progress in multimodal reasoning, most models remain restricted to limited tasks, and support either image or video reasoning alone."
        },
        {
            "title": "3.1 Dataset Construction",
            "content": "Data Collection and Curation. High-quality and diverse training data are essential for developing unified multimodal reasoning generalist. To this end, we construct the OneThinker-600k corpus as the foundation for training, as illustrated in fig. 3. Our dataset covers both image and video modalities and spans series of fundamental visual reasoning tasks, including rule-based QA, open-ended QA, captioning, spatial grounding, temporal grounding, spatio-temporal grounding, tracking, and segmentation. For perception-oriented tasks such as grounding, tracking, and segmentation, we require the model to output responses in predefined JSON schema to ensure consistent formatting and enable automatic, verifiable reward computation. Details of prompts and formats are provided in the Appendix. To ensure task diversity and balanced modality coverage, we collect data from broad range of public training datasets and carefully curate samples across various domains and difficulty levels. The curated dataset is designed to equip the model with broad spectrum of core reasoning abilities, such as logical reasoning, knowledge-based inference, spatial perception, temporal understanding, causal inference, etc. Together, these capabilities enable unified multimodal reasoning generalist that can perform structured and coherent inference over both static and dynamic visual contexts. CoT Annotation. To enable effective SFT initialization for reasoning, we leverage strong proprietary model, Seed1.5-VL [16], to produce CoT annotations on the previously constructed OneThinker-600k corpus. For different tasks, we apply task-specific filtering thresholds to ensure the accuracy of retained CoT traces. After rule-based checking and quality validation, we obtain the CoT-annotated subset OneThinker-SFT-340k. This SFT dataset provides diverse and reliable foundation for developing unified multimodal reasoning across wide range of visual tasks."
        },
        {
            "title": "3.2 Task Types and Rewards",
            "content": "All tasks are cast into unified text interface, where the model first produces its internal reasoning inside <think>...</think> and then outputs task-specific result inside <answer>...</answer>. For perceptionoriented tasks, the <answer> block contains structured representation (e.g., time spans, bounding boxes, sparse points) following predefined schema, which allows automatic parsing and verification. The overall reward is where Racc is task-specific accuracy reward and Rformat is format reward. For tasks requiring structured outputs, Rformat further checks whether the output follows the predefined schema. = Racc + Rformat , (1) 4 OneThinker: All-in-one Reasoning Model for Image and Video Rule-based QA. This category includes multiple-choice, numerical, regression, math, and OCR tasks. For multiplechoice, numerical, and math problems, correctness is determined by whether the predicted and ground-truth answers are equivalent. Regression tasks are evaluated using the Mean Relative Accuracy (MRA) metric [39], which measures relative closeness between the prediction and the reference value across multiple tolerance levels. OCR tasks use the Word Error Rate to compute the reward. These rule-based tasks provide deterministic and interpretable feedback for discrete reasoning and quantitative prediction, forming reliable foundation for reinforcement learning. Open-ended QA & Caption. For open-ended question answering and captioning tasks, we employ an external reward model to provide similarity score: Racc = RM(cid:0)q, ˆa, a(cid:1), where denotes the input query, ˆa is the model-predicted answer, and is the reference answer. In this work, we adopt POLAR-7B [40] as the reward model RM. (2) Temporal Grounding. Temporal grounding requires the model to identify the start and end time of the queried event in video. The answer encodes continuous time segment, and we measure accuracy using temporal IoU: Racc = tIoU(cid:0)[ˆs, ˆe], [s, e](cid:1), where tIoU(, ) denotes the temporal intersection-over-union of two intervals. Here, ˆs and ˆe represent the predicted start and end timestamps, while and denote their corresponding ground-truth values. (3) Spatial Grounding. Spatial grounding requires the model to localize target region by predicting bounding box. The accuracy is measured using spatial intersection-over-union (sIoU) between predicted and ground-truth boxes: Racc = sIoU(cid:0)ˆb, b(cid:1), where ˆb and denote the predicted and ground-truth bounding boxes, respectively, and sIoU(, ) represents their spatial overlap ratio. (4) Spatial-temporal Grounding. This task unifies temporal and spatial localization, requiring the model to predict both the temporal span of an event and the corresponding bounding boxes across frames. The accuracy is computed by combining temporal IoU and mean spatial IoU: Racc = tIoU(cid:0)[ˆs, ˆe], [s, e](cid:1) + sIoU, (5) where ˆs and ˆe denote the predicted start and end times, and sIoU represents the mean IoU between predicted and ground-truth boxes across frames. Tracking. Tracking requires the model to predict sequence of bounding boxes for given target across video frames. The accuracy is measured as the mean IoU over all frames: where sIoU is the averaged IoU between predicted and ground-truth bounding boxes throughout the trajectory. Racc = sIoU, (6) Segmentation. Following prior works applying RL for image segmentation [12, 6, 41], the model predicts bounding box along with set of positive and negative points to identify target objects. These predictions are subsequently fed into SAM2 [42] to generate the final segmentation mask. For video segmentation, we further require the model to predict keyframe time ˆt indicating when the predicted boxes and points should be applied. Due to the high computational latency of running SAM2 on all rollouts for video segmentation, we omit the mask-based reward in this paper. All bounding boxes and point annotations are provided by Seed1.5-VL [16]. We define Gaussian kernel G(d) = exp distances and σ = 1 for temporal distances. (cid:16) (cid:17) d2 2σ2 that normalizes distances into [0, 1]. We set σ = 50 for spatial For image segmentation, the accuracy reward combines bounding box overlap with Gaussian similarities over positive and negative point sets: Racc = sIoU(ˆb, b) + G(dis+) + G(dis), (7) 5 OneThinker: All-in-one Reasoning Model for Image and Video Figure 4: Comparison of advantage formulations in three RL algorithms. where dis+ denotes the minimum average distance between predicted and ground-truth positive points under optimal matching, and dis is defined similarly for negative points. For video segmentation, temporal Gaussian kernel is additionally applied to the predicted keyframe time: Racc = sIoU(ˆb, b) + G(ˆt t) + G(dis+) + G(dis), (8) where ˆt denotes the predicted keyframe timestamp and is the annotated ground-truth time. In this paper, the number of positive points and negative points are both set to three."
        },
        {
            "title": "3.3 EMA-GRPO",
            "content": "While GRPO has demonstrated strong capability in enhancing reasoning performance, its direct application to heterogeneous visual tasks would lead to biased optimization. We identify two complementary sources of imbalance that hinder effective multi-task training, as illustrated in fig. 4. Intra-task Imbalance. Standard GRPO normalizes rewards within each prompt group by the group standard deviation to stabilize optimization. This normalization causes biased weighting among samples of the same task [17, 18, 19, 20]. Specifically, examples with very small or very large variance receive stronger updates, while medium-difficulty sampleswhose rollouts usually have large varianceare under-optimized. As result, the reinforcement learning within task becomes biased. Inter-task Imbalance. Conversely, removing the STD normalization as in Dr.GRPO [17] avoids the intra-task bias but introduces cross-task imbalance: different tasks vary in their reward scale and density, so sparse rewards (e.g., math reasoning) dominate the optimization signal, whereas dense, small-range rewards (e.g., grounding) are down-weighted. This imbalance causes the model to overfit small subset of tasks and weakens its generalization across diverse visual reasoning settings. EMA-based Normalization. To overcome both imbalances, we propose EMA-GRPO, which introduces task-wise adaptive normalization based on the exponential moving average (EMA) of reward statistics. For each task τ , we maintain EMA estimates of the first and second moments of its outcome rewards. Given the current batch of rewards {Ri} belonging to task τ , let first-order moment µτ (t) = mean({Ri}) and second-order moment ντ (t) = mean({R2 }) at step t. We update the EMA moments as mτ mτ 1 (t) = β mτ 2 (t) = β mτ where β is the decay factor (set to 0.99). The task-wise standard deviation is then computed as 1 (t 1) + (1 β) µτ (t), 2 (t 1) + (1 β) ντ (t), (9) στ (t) = (cid:113) (mτ 2 (t) (mτ 1 (t))2. (10) This moving statistic captures each tasks intrinsic reward scale while adapting smoothly to changing reward distributions during training. Then, the advantage in task τ is computed with its task-wise EMA standard deviation: Aτ (t) = Ri mean({Rj}) στ (t) . 6 (11) OneThinker: All-in-one Reasoning Model for Image and Video This adaptive normalization simultaneously resolves both intra-task and inter-task imbalance. Within each task, all rollouts share the same normalization scale στ (t), which prevents the model from overemphasizing easy or hard samples while under-optimizing medium-difficulty ones. Across different tasks, each task maintains its own reward scale through an independent στ (t), ensuring balanced gradient contributions regardless of differences in reward magnitude or density. For numerical stability during the initial stage, when στ (t) has not yet stabilized, we clip the advantage to [5, 5]. Together, these properties promote stable optimization and fair learning across heterogeneous visual reasoning tasks. Training Objective. Following DeepSeek-R1, the final policy update adopts the standard GRPO objective with the EMA-normalized advantage: (cid:34) Eq,{oi}"
        },
        {
            "title": "1\nG",
            "content": "G (cid:88) (cid:16) i=1 min (cid:16) πθ(oiq) πθold (oiq) Aτ (t), clip (cid:17) (cid:16) πθ(oiq) πθold (oiq) , 1 ϵ, 1 + ϵ (cid:35) . (cid:0)πθ πref (cid:1)(cid:17) βKL DKL (cid:17) Aτ (t) (12) The definition of variables and hyperparameters follows the standard GRPO [4]."
        },
        {
            "title": "4.1 Setup",
            "content": "Training Details. Our model is trained on 32 NVIDIA H800 GPUs. In the SFT stage, we adopt Qwen-3-VL-Instruct-8B [43] as the base model and train it on our OneThinker-SFT-340k dataset. Subsequently, reinforcement learning is performed based on the SFT-initialized model using the OneThinker-600k corpus. For both SFT and RL, we sample image-video balanced sets for training. The batch size is set to 32 for SFT and 128 for RL. The learning rate is configured as 1 105 for SFT and 2 106 for RL, both optimized with AdamW. For efficiency, the maximum number of video frames is capped at 128 during training. The decay factor β is set to 0.99, following the common practice in EMA. The group size for EMA-GRPO is set to 8, and the KL regularization coefficient βKL is fixed at 0.01. The maximum response length is limited to 4096 tokens. We discard rollouts that are entirely correct or incorrect during RL training, following the practice in [27]. Overall, the complete training process takes approximately 10 days. Benchmarks. For evaluation, we adopt variety of benchmarks corresponding to different visual reasoning tasks, covering question answering, captioning, spatial and temporal grounding, tracking, and segmentation, as presented in the experimental tables. For Qwen3-VL-Instruct, we report our reproduced results. We evaluate models using greedy decoding, following prior works [44, 9, 45]."
        },
        {
            "title": "4.2 Main Results",
            "content": "We evaluate OneThinker across wide range of visual reasoning benchmarks covering both image and video modalities, as summarized in table 1, table 2, table 3, table 4, table 5, table 6, table 7, and table 8. Across all benchmarks, OneThinker demonstrates substantial improvements, showcasing its unified and transferable reasoning ability across tasks and modalities. Examples of reasoning responses for each task can be found in Appendix. Image QA. OneThinker-8B consistently achieves top-tier performance for image QA across diverse set of tasks spanning general knowledge, mathematics, science, and multimodal reasoning. Compared with strong open-source models such as Vision-R1-7B, VAPO-Thinker-7B, and Qwen3-VL-Instruct-8B, our model attains superior results on these benchmarks. For example, OneThinker reaches 70.6% on MMMU, 77.6% on MathVista, 64.3% on MathVerse, and 70.6% on MMStar, consistently outperforming all prior open-source competitors. These results demonstrate that our unified reasoning framework can effectively generalize to wide range of complex image QA scenarios. Video QA. In video QA, OneThinker-8B shows strong superiority over video-focused reasoning models. Across benchmarks including VideoMMMU, MMVU(mc), VideoMME, VideoHolmes, LongVideoBench, LongVideo-Reason, and VideoMathQA, OneThinker consistently ranks among the top performers. For instance, it achieves 66.2% on VideoMMMU, 70.5% on MMVU(mc), and 66.5% on VideoMME, outperforming specialized video reasoning models 7 OneThinker: All-in-one Reasoning Model for Image and Video Table 1: Performance of different models on image question answering benchmarks. For Qwen3-VL-Instruct-8B, we report our reproduced results under the same setting. Models Image QA MMMU [21] MathVista [46] MathVerse [22] MMBench [47] MMStar [48] ScienceQA [49] AI2D [50] MMT-Bench [51] GPT-4o [52] Gemini 2.5 Pro [53] Seed1.5-VL [16] SophiaVL-R1-7B [38] Vision-R1-7B [10] MM-Eureka-7B [54] VL-Rethinker-7B [44] VAPO-Thinker-7B [55] Qwen3-VL-Instruct-8B [43] OneThinker-8B 70.7 81.7 77.9 61.3 - 57.3 56.7 60.2 60.2 70.6 63.8 82.7 85.6 71.3 73.5 73.0 74.9 75.6 74.2 77. 41.2 - - 48.8 52.4 50.3 54.2 53.3 58.1 64.3 84.3 90.1 89.9 85.4 - - - - 85.1 86. 65.1 77.5 77.8 66.7 - 64.4 62.7 63.0 68.5 70.6 90.1 - - 90.9 - - - 92. 96.5 84.9 88.4 87.3 - - - - 82.3 85.2 67.7 - 62.7 - - - - 64. 67.8 Table 2: Performance of different models on video question answering benchmarks. For Qwen3-VL-Instruct-8B, we report our reproduced results under the same setting. Models Frames GPT-4o [52] Gemini 2.5 Pro [53] Seed1.5-VL [16] VideoLLaMA3-7B [63] InternVideo2.5-8B [64] VideoChat-R1-7B [13] LongVILA-R1-7B [61] Video-R1-7B [9] Qwen3-VL-Instruct-8B [43] OneThinker-8B - - - - - - - 128 128 VideoMMMU[56] MMVU(mc)[57] VideoMME[58] VideoHolmes[59] LongVideoBench[60] LongVideo-Reason[61] VideoMathQA[62] Video QA 61.2 83.6 81.4 - - 46.4 51.0 52.4 63.3 66.2 75.4 - - - - - - 64.2 65.6 70. 71.9 84.3 77.9 66.2 65.1 60.0 65.1 61.4 64.0 66.5 42.0 45.0 - - - 33.0 - 36.5 40.9 48. 66.7 - 74.0 59.8 60.6 - 58.0 - 61.5 61.7 - - - - - 67.2 72.0 68.1 71.5 79. 20.2 - - - 25.2 27.6 23.6 21.4 24.3 35.0 such as VideoChat-R1-7B, VideoLLaMA3-7B, and InternVideo2.5-8B. Most notably, OneThinker obtains 79.2% on LongVideo-Reason, substantially surpassing Video-R1-7B (67.2%) and Qwen3-VL-Instruct-8B (71.5%). On VideoMathQA, challenging video reasoning benchmark, OneThinker also leads all open-source models with score of 35.0%. These results collectively verify that the effectiness of our proposed framework. Image and Video Caption. On caption benchmarks, OneThinker maintains competitive or superior performance on both image and video captioning. For image captioning, it achieves 25.7 on MMSci-Caption and 57.9 on MMT-Caption, markedly outperforming Qwen3-VL-Instruct-8B (15.1 and 47.3 respectively) and significantly improving over LLaVA1.5-7B. In video captioning, OneThinker reaches 28.0 on VideoMMLU-Caption, demonstrating effective video caption ability. This unified captioning ability reflects the models strong visual descriptive skills. Temporal Grounding. On temporal grounding tasks, OneThinker generally shows substantial improvements over existing temporal localization models. For example, on Charades, OneThinker-8B achieves performance that is Table 3: Performance of different models on caption benchmarks. Models Frames Image Caption Video Caption MMSci-Caption[65] MMT-Caption[51] VideoMMLU-Caption[66] GPT-4o [52] LLaVA-1.5-7B [67] Qwen3-VL-Instruct-8B [43] OneThinker-8B - - 128 - - 47.3 57.9 53.9 22.3 20. 28.0 27.0 11.8 15.1 25.7 8 OneThinker: All-in-one Reasoning Model for Image and Video Table 4: Performance on temporal grounding benchmarks. Models Frame Charades [68] ActivityNet [69] ANet-RTL [70] R@0.3 R@0.5 R@0.7 mIoU R@0.3 R@0.5 R@0.7 mIoU R@0.3 R@0.5 R@0.7 mIoU VTimeLLM [71] TimeSuite [72] VideoChat-R1 [13] Temporal-RLT [73] Time-R1 [37] Qwen3-VL-Instruct-8B [43] OneThinker-8B - - - - - 128 128 55.3 69.9 83.1 80.2 78.1 58. 83.5 34.3 48.7 72.8 68.3 60.8 33.5 68.3 14.7 24.0 51.5 44.5 35.3 13.1 34.6 - 61.3 57.9 - 36.7 44.8 - 50.4 56.9 58.6 39. 45.3 59.9 65.0 29.5 - 32.2 38.4 39.0 26.1 43.6 14.2 - 16.2 20.2 21.4 15. 31.4 - 34.3 39.1 - 29.1 - - - 40.2 - 36.2 25.7 45.9 62.0 - - - 22.7 - 27. 42.3 - - - 10.9 - 18.3 - - - 26.3 - 26.6 22.7 43.2 Table 5: Performance on spatial grounding benchmarks. Models RefCOCO[74] RefCOCO+[74] RefCOCOg[75] testA testB Perception-R1 [14] VLM-R1 [11] DeepEyes [76] Qwen3-VL-Instruct-8B [43] OneThinker-8B 91.4 - - 92.2 93.7 84.5 - - 85.3 88. val 89.1 90.5 89.8 89.9 92.0 testA testB 86.8 - - 89. 91.4 74.3 - - 77.8 82.7 val 81.7 84.3 83.6 84.5 87. test 85.4 - - 86.7 88.8 val 85.7 87.1 86.7 86.8 89. comparable to or better than previous models. On ActivityNet, our model attains 65.0 R@0.3, 43.6 R@0.5, and 25.7 R@0.7, confirming superior temporal grounding abilities. Furthermore, on the ANet-RTL benchmark, OneThinker achieves the best mIoU (43.2) among listed models. These results demonstrate the models robust ability to reason about when events happen and accurately understand the fine-grained temporal information. Spatial Grounding. For spatial grounding, OneThinker-8B also demonstrates state-of-the-art localization ability across the widely-used RefCOCO, RefCOCO+, and RefCOCOg benchmarks. In RefCOCO testA/testB/val sets, it achieves 93.7 / 88.9 / 92.0, outperforming prior strong models such as Perception-R1, DeepEyes, and Qwen3-VLInstruct-8B. On RefCOCO+, OneThinker again leads with 91.4 / 82.7 / 87.0, consistently surpassing prior baselines by large margin. On the more challenging RefCOCOg benchmark, the model achieves 88.8 / 89.2 (test/val), showing strong comprehension of long and descriptive referring expressions. These results highlight the models strong spatial grounding abilities. Spatial-Temporal Grounding. On spatial-temporal grounding tasks, which require simultaneous localization in both space and time, OneThinker delivers substantial improvements over previous systems. On the STVG benchmark, it achieves 34.9 tIoU@0.5, 39.5 tIoU, 40.3 sIoU@0.5, and 36.7 sIoU, outperforming Grounded-VideoLLM and Qwen3VL-Instruct-8B by large margin. Such gains emphasize OneThinkers capability to jointly reason about where and when events occur, even in complex videos involving multiple objects and temporal transitions. Table 6: Performance on spatial-temporal grounding benchmarks. Models Frame GroundingGPT [78] VTimeLLM [71] Grounded-VideoLLM [79] Qwen3-VL-Instruct-8B [43] OneThinker-8B - - - 128 128 STVG [77] tIoU 12.2 15.5 33.0 25.4 39.5 sIoU@0.5 2.9 - - 11.6 40.3 sIoU 9.2 - - 13.6 36.7 tIoU@0.5 7,1 7.1 30.0 24.4 34.9 OneThinker: All-in-one Reasoning Model for Image and Video Table 7: Performance on tracking benchmarks. Models R1-Track [80] VideoChat-R1 [13] Qwen3-VL-Instruct-8B [43] OneThinker-8B AO 68.0 42.5 33.7 73.0 GOT-10k [23] R@0.3 R@0.5 R@0. - - 51.1 93.9 76.6 30.6 28.9 84.4 - 3.9 10.6 68. Table 8: Performance on segmentation benchmarks. For RefCOCO series, we report results on the val set. Models RefCOCO [74] RefCOCO+ [74] RefCOCOg [75] MeViS [81] ReasonVOS [24] Image Segmentation Video Segmentation PixelLM-7B [82] LISA-7B [83] VISA-13B [84] Seg-R1-7B [12] ReferFormer [85] VideoLISA-3.8B [24] Qwen3-VL-Instruct-8B [43] + SAM2 [42] OneThinker-8B cIoU 73.0 74.1 72.4 74.3 - - 73.2 75.8 cIoU 66.3 62.4 59.8 62.6 - - 66.2 67.1 cIoU 69.3 66.4 65.5 71.0 - - 68.3 70.8 - - - - J&F J&F - - - - - - 44.5 - - - - 29.1 33.1 31.1 - - - - - - 29.8 32.2 31.0 30.2 35.6 32.9 41.3 47.6 44.4 45.1 49.9 47.5 19.4 26.4 22.9 16.6 22.7 19.6 48.8 56.7 52.7 51.1 58.7 54.9 Tracking. As for tracking tasks, OneThinker reaches high 73.0 AO, 93.9 R@0.3, 84.4 R@0.5, and 68.8 R@0.7 on GOT-10k, outperforming previous models like R1-Track and VideoChat-R1. Notably, our evaluation uses 32 frames for prediction, which is substantially more challenging than the 8-frame setting adopted by prior work VideoChat-R1. This large improvement illustrates that the unified reasoning architecture also yields strong single-object tracking capabilities, enabling reliable localization over long temporal sequences. Image and Video Segmentation. OneThinker achieves the highest mean performance across both image and video segmentation benchmarks. For image segmentation (RefCOCO / RefCOCO+ / RefCOCOg), it obtains 75.8 / 67.1 / 70.8 cIoU on the val set, significantly outperforming PixelLM-7B, LISA-7B, VISA-13B, Seg-R1-7B. For video segmentation, it reaches 48.8 J, 56.7 F, and 52.7 J&F on MeViS, surpassing all previous open-source models. On ReasonVOS, it delivers 51.1 J, 58.7 F, and 54.9 J&F, again achieving the best performance across all competitors. These results demonstrate the models fine-grained visual understanding ability in both static and dynamic environments. Overall, OneThinker serves as unified multimodal reasoning generalist that achieves strong performance across all major visual understanding tasks, showing strong potential for scalable and generalizable visual reasoning."
        },
        {
            "title": "4.3 Ablation Study",
            "content": "In this section, we design three variants of OneThinker to verify the effectiveness of different components in our framework: (1) OneThinker-8B-SFT, which is trained only with SFT without RL; (2) OneThinker-8B-GRPO, which replaces our proposed EMA-GRPO with the original GRPO algorithm; (3) OneThinker-8B-DrGRPO, which adopts the Dr.GRPO [17] algorithm for RL training. As shown in table 9, all ablated variants perform worse than OneThinker-8B across all tasks. Compared with the SFT baseline, RL consistently improves performance, demonstrating its effectiveness across diverse visual tasks. Replacing EMA-GRPO with standard GRPO or DrGRPO results in noticeable degradation, demonstrating the importance of addressing the intra-task imbalance and inter-task imbalance issues. Overall, This ablation study confirms the effectiveness of our unified RL framework and the proposed EMA-GRPO algorithm. 10 OneThinker: All-in-one Reasoning Model for Image and Video Table 9: Ablation study. Results are averaged over benchmarks within each task for comparison. For tasks with multiple evaluation metrics, we report the following: mIoU for temporal grounding, the mean of tIoU and sIoU for spatio-temporal grounding, AO for tracking, and the mean of cIoU (image) and J&F (video) for segmentation. Models Qwen3-VL-Instruct-8B OneThinker-8B-SFT OneThinker-8B-GRPO OneThinker-8B-DrGRPO OneThinker-8B QA 65.0 67.0 67.2 67.6 69.8 Temporal Grounding Spatial Grounding S.-T. Grounding Tracking Segmentation 30.8 31.8 46.9 46.3 49. 86.6 87.8 86.5 88.2 89.2 19.5 27.1 34.5 34.0 38.1 33.7 48.1 65.5 67.8 73. 50.0 62.8 62.3 61.2 64.2 Table 10: Benefits between tasks and modalities analysis. Variants Image QA Video QA Tracking Segmentation OneThinker-wo-spatial-grounding OneThinker-wo-temporal-grounding OneThinker-wo-ImageQA OneThinker 76.6 77. - 77.4 60.3 59.5 58.2 61.1 71.0 67. 72.3 73.0 62.9 63.3 63.9 64."
        },
        {
            "title": "4.4 Benefits of Unified Training Analysis",
            "content": "To further investigate the potential benefits and knowledge sharing of cross-task and cross-modal learning, we conduct an analysis by selectively removing data from specific task categories during training. We design three variants: (1) OneThinker-wo-spatial-grounding, which excludes spatial grounding data; (2) OneThinker-wo-temporal-grounding, which removes all temporal grounding data; (3) OneThinker-wo-ImageQA, which omits all image QA samples. As shown in table 10, removing either spatial or temporal grounding leads to noticeable drop in performance across other tasks. In particular, the absence of temporal grounding significantly degrades results on video QA and tracking, indicating that temporal grounding may enhance the models temporal perception and sequential reasoning ability. Similarly, removing spatial grounding results in lower accuracy on both image QA and segmentation, indicating that spatial localization tasks may contribute valuable structural and positional cues that benefit broader visual reasoning. Moreover, excluding ImageQA causes the severe performance drop on video QA. We attribute this to the generally higher quality and greater diversity of image QA datasets, which help the model develop stronger general reasoning and recognition capabilities that transfer well to video understanding. This observation confirms that knowledge learned from static images can generalize to dynamic video scenarios, reflecting the benefit of cross-modal transfer. Overall, these results suggest that certain tasks and modalities can benefit from others during joint training in OneThinker. By jointly training on diverse visual tasks, OneThinker effectively shares knowledge across domains and emerges as multimodal reasoning generalist."
        },
        {
            "title": "4.5 Zero-shot Generalization to Unseen Tasks",
            "content": "We further evaluate OneThinkers zero-shot generalization on unseen visual understanding tasks. These unseen tasks are selected from MMT-Bench [51], which contains 162 diverse visual tasks. As shown in fig. 5, OneThinker-8B clearly outperforms Qwen3-VL-Instruct-8B across multiple unseen tasks, such as point tracking, image quality assessment, GUI tasks and rotated obejct detection. These results demonstrate that unified multimodal reasoning enables the model to generalize beyond its training tasks, showing promising transferability to novel real-world scenarios. 11 OneThinker: All-in-one Reasoning Model for Image and Video"
        },
        {
            "title": "5 Conclusion",
            "content": "Figure 5: Performance on unseen visual tasks. In this work, we present OneThinker, an all-in-one multimodal reasoning model that unifies diverse visual foundation tasks for images and videos. To support training, we construct OneThinker-600k dataset for RL training and its CoT-annotated subset OneThinker-SFT-340k for SFT cold start. We further propose EMA-GRPO, an RL algorithm that balances optimization across heterogeneous visual tasks through task-wise adaptive reward normalization. Extensive experiments demonstrate that OneThinker achieves strong performance across tasks. We hope this work takes step toward scalable and unified multimodal reasoning generalist."
        },
        {
            "title": "References",
            "content": "[1] Zhuosheng Zhang, Aston Zhang, Mu Li, Hai Zhao, George Karypis, and Alex Smola. Multimodal chain-of-thought reasoning in language models. arXiv preprint arXiv:2302.00923, 2023. [2] Jiakang Yuan, Tianshuo Peng, Yilei Jiang, Yiting Lu, Renrui Zhang, Kaituo Feng, Chaoyou Fu, Tao Chen, Lei Bai, Bo Zhang, et al. Mme-reasoning: comprehensive benchmark for logical reasoning in mllms. arXiv preprint arXiv:2505.21327, 2025. [3] Guanghao Zhou, Panjia Qiu, Cen Chen, Jie Wang, Zheming Yang, Jian Xu, and Minghui Qiu. Reinforced mllm: survey on rl-based reasoning in multimodal large language models. arXiv preprint arXiv:2504.21277, 2025. [4] Daya Guo, Dejian Yang, Haowei Zhang, Junxiao Song, Ruoyu Zhang, Runxin Xu, Qihao Zhu, Shirong Ma, Peiyi Wang, Xiao Bi, et al. Deepseek-r1: Incentivizing reasoning capability in llms via reinforcement learning. arXiv preprint arXiv:2501.12948, 2025. [5] Junfei Wu, Jian Guan, Kaituo Feng, Qiang Liu, Shu Wu, Liang Wang, Wei Wu, and Tieniu Tan. Reinforcing spatial reasoning in vision-language models with interwoven thinking and visual drawing. arXiv preprint arXiv:2506.09965, 2025. [6] Yuqi Liu, Bohao Peng, Zhisheng Zhong, Zihao Yue, Fanbin Lu, Bei Yu, and Jiaya Jia. Seg-zero: Reasoning-chain guided segmentation via cognitive reinforcement. arXiv preprint arXiv:2503.06520, 2025. [7] Xiaoying Zhang, Hao Sun, Yipeng Zhang, Kaituo Feng, Chaochao Lu, Chao Yang, and Helen Meng. Critique-grpo: Advancing llm reasoning with natural language and numerical feedback. arXiv preprint arXiv:2506.03106, 2025. [8] Zongzhao Li, Zongyang Ma, Mingze Li, Songyou Li, Yu Rong, Tingyang Xu, Ziqi Zhang, Deli Zhao, and Wenbing Huang. Star-r1: Spatial transformation reasoning by reinforcing multimodal llms. arXiv preprint arXiv:2505.15804, 2025. [9] Kaituo Feng, Kaixiong Gong, Bohao Li, Zonghao Guo, Yibing Wang, Tianshuo Peng, Junfei Wu, Xiaoying Zhang, Benyou Wang, and Xiangyu Yue. Video-r1: Reinforcing video reasoning in mllms. arXiv preprint arXiv:2503.21776, 2025. [10] Wenxuan Huang, Bohan Jia, Zijie Zhai, Shaosheng Cao, Zheyu Ye, Fei Zhao, Zhe Xu, Yao Hu, and Shaohui Lin. Vision-r1: Incentivizing reasoning capability in multimodal large language models. arXiv preprint arXiv:2503.06749, 2025. [11] Haozhan Shen, Peng Liu, Jingcheng Li, Chunxin Fang, Yibo Ma, Jiajia Liao, Qiaoli Shen, Zilun Zhang, Kangjia Zhao, Qianqian Zhang, et al. Vlm-r1: stable and generalizable r1-style large vision-language model. arXiv preprint arXiv:2504.07615, 2025. [12] Zuyao You and Zuxuan Wu. Seg-r1: Segmentation can be surprisingly simple with reinforcement learning. arXiv preprint arXiv:2506.22624, 2025. 12 OneThinker: All-in-one Reasoning Model for Image and Video [13] Xinhao Li, Ziang Yan, Desen Meng, Lu Dong, Xiangyu Zeng, Yinan He, Yali Wang, Yu Qiao, Yi Wang, and Limin Wang. Videochat-r1: Enhancing spatio-temporal perception via reinforcement fine-tuning. arXiv preprint arXiv:2504.06958, 2025. [14] En Yu, Kangheng Lin, Liang Zhao, Jisheng Yin, Yana Wei, Yuang Peng, Haoran Wei, Jianjian Sun, Chunrui Han, Zheng Ge, et al. Perception-r1: Pioneering perception policy with reinforcement learning. arXiv preprint arXiv:2504.07954, 2025. [15] Haoji Zhang, Xin Gu, Jiawen Li, Chixiang Ma, Sule Bai, Chubin Zhang, Bowen Zhang, Zhichao Zhou, Dongliang He, and Yansong Tang. Thinking with videos: Multimodal tool-augmented reinforcement learning for long video reasoning. arXiv preprint arXiv:2508.04416, 2025. [16] Dong Guo, Faming Wu, Feida Zhu, Fuxing Leng, Guang Shi, Haobin Chen, Haoqi Fan, Jian Wang, Jianyu Jiang, Jiawei Wang, et al. Seed1. 5-vl technical report. arXiv preprint arXiv:2505.07062, 2025. [17] Zichen Liu, Changyu Chen, Wenjun Li, Penghui Qi, Tianyu Pang, Chao Du, Wee Sun Lee, and Min Lin. Understanding r1-zero-like training: critical perspective. arXiv preprint arXiv:2503.20783, 2025. [18] Michael Bereket and Jure Leskovec. Uncalibrated reasoning: Grpo induces overconfidence for stochastic outcomes. arXiv preprint arXiv:2508.11800, 2025. [19] Xiangxiang Chu, Hailang Huang, Xiao Zhang, Fei Wei, and Yong Wang. Gpg: simple and strong reinforcement learning baseline for model reasoning. arXiv preprint arXiv:2504.02546, 2025. [20] Wenke Huang, Quan Zhang, Yiyang Fang, Jian Liang, Xuankun Rong, Huanjin Yao, Guancheng Wan, Ke Liang, Wenwen He, Mingjun Li, et al. Mapo: Mixed advantage policy optimization. arXiv preprint arXiv:2509.18849, 2025. [21] Xiang Yue, Yuansheng Ni, Kai Zhang, Tianyu Zheng, Ruoqi Liu, Ge Zhang, Samuel Stevens, Dongfu Jiang, Weiming Ren, Yuxuan Sun, et al. Mmmu: massive multi-discipline multimodal understanding and reasoning benchmark for expert agi. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 95569567, 2024. [22] Renrui Zhang, Dongzhi Jiang, Yichi Zhang, Haokun Lin, Ziyu Guo, Pengshuo Qiu, Aojun Zhou, Pan Lu, Kai-Wei Chang, Yu Qiao, et al. Mathverse: Does your multi-modal llm truly see the diagrams in visual math problems? In European Conference on Computer Vision, pages 169186. Springer, 2024. [23] Lianghua Huang, Xin Zhao, and Kaiqi Huang. Got-10k: large high-diversity benchmark for generic object tracking in the wild. IEEE transactions on pattern analysis and machine intelligence, 43(5):15621577, 2019. [24] Zechen Bai, Tong He, Haiyang Mei, Pichao Wang, Ziteng Gao, Joya Chen, Zheng Zhang, and Mike Zheng Shou. One token to seg them all: Language instructed reasoning segmentation in videos. Advances in Neural Information Processing Systems, 37:68336859, 2024. [25] Chujie Zheng, Shixuan Liu, Mingze Li, Xiong-Hui Chen, Bowen Yu, Chang Gao, Kai Dang, Yuqiong Liu, Rui Men, An Yang, et al. Group sequence policy optimization. arXiv preprint arXiv:2507.18071, 2025. [26] Guanting Dong, Hangyu Mao, Kai Ma, Licheng Bao, Yifei Chen, Zhongyuan Wang, Zhongxia Chen, Jiazhen Du, Huiyang Wang, Fuzheng Zhang, et al. Agentic reinforced policy optimization. arXiv preprint arXiv:2507.19849, 2025. [27] Qiying Yu, Zheng Zhang, Ruofei Zhu, Yufeng Yuan, Xiaochen Zuo, Yu Yue, Weinan Dai, Tiantian Fan, Gaohong Liu, Lingjun Liu, et al. Dapo: An open-source llm reinforcement learning system at scale. arXiv preprint arXiv:2503.14476, 2025. [28] Tian Xie, Zitian Gao, Qingnan Ren, Haoming Luo, Yuqian Hong, Bryan Dai, Joey Zhou, Kai Qiu, Zhirong Wu, and Chong Luo. Logic-rl: Unleashing llm reasoning with rule-based reinforcement learning. arXiv preprint arXiv:2502.14768, 2025. [29] Qiguang Chen, Libo Qin, Jinhao Liu, Dengyun Peng, Jiannan Guan, Peng Wang, Mengkang Hu, Yuhang Zhou, Te Gao, and Wanxiang Che. Towards reasoning era: survey of long chain-of-thought for reasoning large language models. arXiv preprint arXiv:2503.09567, 2025. [30] Lang Feng, Zhenghai Xue, Tingcong Liu, and Bo An. Group-in-group policy optimization for llm agent training. arXiv preprint arXiv:2505.10978, 2025. [31] Haoyuan Sun, Jiaqi Wu, Bo Xia, Yifu Luo, Yifei Zhao, Kai Qin, Xufei Lv, Tiantian Zhang, Yongzhe Chang, and Xueqian Wang. Reinforcement fine-tuning powers reasoning capability of multimodal large language models. arXiv preprint arXiv:2505.18536, 2025. [32] Peiwen Sun, Shiqiang Lang, Dongming Wu, Yi Ding, Kaituo Feng, Huadai Liu, Zhen Ye, Rui Liu, Yun-Hui Liu, Jianan Wang, et al. Spacevista: All-scale visual spatial reasoning from mm to km. arXiv preprint arXiv:2510.09606, 2025. [33] Chengqi Duan, Kaiyue Sun, Rongyao Fang, Manyuan Zhang, Yan Feng, Ying Luo, Yufang Liu, Ke Wang, Peng Pei, Xunliang Cai, et al. Codeplot-cot: Mathematical visual reasoning by thinking with code-driven images. arXiv preprint arXiv:2510.11718, 2025. [34] Shuang Chen, Yue Guo, Zhaochen Su, Yafu Li, Yulun Wu, Jiacheng Chen, Jiayu Chen, Weijie Wang, Xiaoye Qu, and Yu Cheng. Advancing multimodal reasoning: From optimized cold start to staged reinforcement learning. arXiv preprint arXiv:2506.04207, 2025. [35] Shuang Chen, Yue Guo, Yimeng Ye, Shijue Huang, Wenbo Hu, Haoxi Li, Manyuan Zhang, Jiayu Chen, Song Guo, and Nanyun Peng. Ares: Multimodal adaptive reasoning via difficulty-aware token-level entropy shaping. arXiv preprint arXiv:2510.08457, 2025. 13 OneThinker: All-in-one Reasoning Model for Image and Video [36] Jiahao Meng, Xiangtai Li, Haochen Wang, Yue Tan, Tao Zhang, Lingdong Kong, Yunhai Tong, Anran Wang, Zhiyang Teng, Yujing Wang, et al. Open-o3 video: Grounded video reasoning with explicit spatio-temporal evidence. arXiv preprint arXiv:2510.20579, 2025. [37] Ye Wang, Ziheng Wang, Boshen Xu, Yang Du, Kejun Lin, Zihan Xiao, Zihao Yue, Jianzhong Ju, Liang Zhang, Dingyi Yang, et al. Time-r1: Post-training large vision language model for temporal video grounding. arXiv preprint arXiv:2503.13377, 2025. [38] Kaixuan Fan, Kaituo Feng, Haoming Lyu, Dongzhan Zhou, and Xiangyu Yue. Sophiavl-r1: Reinforcing mllms reasoning with thinking reward. arXiv preprint arXiv:2505.17018, 2025. [39] Jihan Yang, Shusheng Yang, Anjali Gupta, Rilyn Han, Li Fei-Fei, and Saining Xie. Thinking in space: How multimodal large language models see, remember, and recall spaces. In Proceedings of the Computer Vision and Pattern Recognition Conference, pages 1063210643, 2025. [40] Shihan Dou, Shichun Liu, Yuming Yang, Yicheng Zou, Yunhua Zhou, Shuhao Xing, Chenhao Huang, Qiming Ge, Demin Song, Haijun Lv, et al. Pre-trained policy discriminators are general reward models. arXiv preprint arXiv:2507.05197, 2025. [41] Hanqing Wang, Shaoyang Wang, Yiming Zhong, Zemin Yang, Jiamin Wang, Zhiqing Cui, Jiahao Yuan, Yifan Han, Mingyu Liu, and Yuexin Ma. Affordance-r1: Reinforcement learning for generalizable affordance reasoning in multimodal large language model. arXiv preprint arXiv:2508.06206, 2025. [42] Nikhila Ravi, Valentin Gabeur, Yuan-Ting Hu, Ronghang Hu, Chaitanya Ryali, Tengyu Ma, Haitham Khedr, Roman Rädle, Chloe Rolland, Laura Gustafson, et al. Sam 2: Segment anything in images and videos. arXiv preprint arXiv:2408.00714, 2024. [43] Shuai Bai, Yuxuan Cai, Ruizhe Chen, Keqin Chen, Xionghui Chen, Zesen Cheng, Lianghao Deng, Wei Ding, Chang Gao, Chunjiang Ge, Wenbin Ge, Zhifang Guo, Qidong Huang, Jie Huang, Fei Huang, Binyuan Hui, Shutong Jiang, Zhaohai Li, Mingsheng Li, Mei Li, Kaixin Li, Zicheng Lin, Junyang Lin, Xuejing Liu, Jiawei Liu, Chenglong Liu, Yang Liu, Dayiheng Liu, Shixuan Liu, Dunjie Lu, Ruilin Luo, Chenxu Lv, Rui Men, Lingchen Meng, Xuancheng Ren, Xingzhang Ren, Sibo Song, Yuchong Sun, Jun Tang, Jianhong Tu, Jianqiang Wan, Peng Wang, Pengfei Wang, Qiuyue Wang, Yuxuan Wang, Tianbao Xie, Yiheng Xu, Haiyang Xu, Jin Xu, Zhibo Yang, Mingkun Yang, Jianxin Yang, An Yang, Bowen Yu, Fei Zhang, Hang Zhang, Xi Zhang, Bo Zheng, Humen Zhong, Jingren Zhou, Fan Zhou, Jing Zhou, Yuanzhi Zhu, and Ke Zhu. Qwen3-vl technical report. arXiv preprint arXiv:2511.21631, 2025. [44] Haozhe Wang, Chao Qu, Zuming Huang, Wei Chu, Fangzhen Lin, and Wenhu Chen. Vl-rethinker: Incentivizing self-reflection of vision-language models with reinforcement learning. arXiv preprint arXiv:2504.08837, 2025. [45] Zilin Xiao, Jaywon Koo, Siru Ouyang, Jefferson Hernandez, Yu Meng, and Vicente Ordonez. Proxythinker: Test-time guidance through small visual reasoners. arXiv preprint arXiv:2505.24872, 2025. [46] Pan Lu, Hritik Bansal, Tony Xia, Jiacheng Liu, Chunyuan Li, Hannaneh Hajishirzi, Hao Cheng, Kai-Wei Chang, Michel Galley, and Jianfeng Gao. Mathvista: Evaluating mathematical reasoning of foundation models in visual contexts. arXiv preprint arXiv:2310.02255, 2023. [47] Yuan Liu, Haodong Duan, Yuanhan Zhang, Bo Li, Songyang Zhang, Wangbo Zhao, Yike Yuan, Jiaqi Wang, Conghui He, Ziwei Liu, et al. Mmbench: Is your multi-modal model an all-around player? In European conference on computer vision, pages 216233. Springer, 2024. [48] Lin Chen, Jinsong Li, Xiaoyi Dong, Pan Zhang, Yuhang Zang, Zehui Chen, Haodong Duan, Jiaqi Wang, Yu Qiao, Dahua Lin, et al. Are we on the right way for evaluating large vision-language models? Advances in Neural Information Processing Systems, 37:2705627087, 2024. [49] Pan Lu, Swaroop Mishra, Tanglin Xia, Liang Qiu, Kai-Wei Chang, Song-Chun Zhu, Oyvind Tafjord, Peter Clark, and Ashwin Kalyan. Learn to explain: Multimodal reasoning via thought chains for science question answering. Advances in Neural Information Processing Systems, 35:25072521, 2022. [50] Aniruddha Kembhavi, Mike Salvato, Eric Kolve, Minjoon Seo, Hannaneh Hajishirzi, and Ali Farhadi. diagram is worth dozen images. In European conference on computer vision, pages 235251. Springer, 2016. [51] Kaining Ying, Fanqing Meng, Jin Wang, Zhiqian Li, Han Lin, Yue Yang, Hao Zhang, Wenbo Zhang, Yuqi Lin, Shuo Liu, et al. Mmt-bench: comprehensive multimodal benchmark for evaluating large vision-language models towards multitask agi. arXiv preprint arXiv:2404.16006, 2024. [52] Aaron Hurst, Adam Lerer, Adam Goucher, Adam Perelman, Aditya Ramesh, Aidan Clark, AJ Ostrow, Akila Welihinda, Alan Hayes, Alec Radford, et al. Gpt-4o system card. arXiv preprint arXiv:2410.21276, 2024. [53] Gheorghe Comanici, Eric Bieber, Mike Schaekermann, Ice Pasupat, Noveen Sachdeva, Inderjit Dhillon, Marcel Blistein, Ori Ram, Dan Zhang, Evan Rosen, et al. Gemini 2.5: Pushing the frontier with advanced reasoning, multimodality, long context, and next generation agentic capabilities. arXiv preprint arXiv:2507.06261, 2025. [54] Fanqing Meng, Lingxiao Du, Zongkai Liu, Zhixiang Zhou, Quanfeng Lu, Daocheng Fu, Tiancheng Han, Botian Shi, Wenhai Wang, Junjun He, et al. Mm-eureka: Exploring the frontiers of multimodal reasoning with rule-based reinforcement learning. arXiv preprint arXiv:2503.07365, 2025. [55] Xinyu Tian, Shu Zou, Zhaoyuan Yang, Mengqi He, Fabian Waschkowski, Lukas Wesemann, Peter Tu, and Jing Zhang. More thought, less accuracy? on the dual nature of reasoning in vision-language models. arXiv preprint arXiv:2509.25848, 2025. 14 OneThinker: All-in-one Reasoning Model for Image and Video [56] Kairui Hu, Penghao Wu, Fanyi Pu, Wang Xiao, Yuanhan Zhang, Xiang Yue, Bo Li, and Ziwei Liu. Video-mmmu: Evaluating knowledge acquisition from multi-discipline professional videos. arXiv preprint arXiv:2501.13826, 2025. [57] Yilun Zhao, Haowei Zhang, Lujing Xie, Tongyan Hu, Guo Gan, Yitao Long, Zhiyuan Hu, Weiyuan Chen, Chuhan Li, Zhijian Xu, et al. Mmvu: Measuring expert-level multi-discipline video understanding. In Proceedings of the Computer Vision and Pattern Recognition Conference, pages 84758489, 2025. [58] Chaoyou Fu, Yuhan Dai, Yongdong Luo, Lei Li, Shuhuai Ren, Renrui Zhang, Zihan Wang, Chenyu Zhou, Yunhang Shen, Mengdan Zhang, et al. Video-mme: The first-ever comprehensive evaluation benchmark of multi-modal llms in video analysis. In Proceedings of the Computer Vision and Pattern Recognition Conference, pages 2410824118, 2025. [59] Junhao Cheng, Yuying Ge, Teng Wang, Yixiao Ge, Jing Liao, and Ying Shan. Video-holmes: Can mllm think like holmes for complex video reasoning? arXiv preprint arXiv:2505.21374, 2025. [60] Haoning Wu, Dongxu Li, Bei Chen, and Junnan Li. Longvideobench: benchmark for long-context interleaved video-language understanding. Advances in Neural Information Processing Systems, 37:2882828857, 2024. [61] Yukang Chen, Wei Huang, Baifeng Shi, Qinghao Hu, Hanrong Ye, Ligeng Zhu, Zhijian Liu, Pavlo Molchanov, Jan Kautz, Xiaojuan Qi, et al. Scaling rl to long videos. arXiv preprint arXiv:2507.07966, 2025. [62] Hanoona Rasheed, Abdelrahman Shaker, Anqi Tang, Muhammad Maaz, Ming-Hsuan Yang, Salman Khan, and Fahad Shahbaz Khan. Videomathqa: Benchmarking mathematical reasoning via multimodal understanding in videos. arXiv preprint arXiv:2506.05349, 2025. [63] Boqiang Zhang, Kehan Li, Zesen Cheng, Zhiqiang Hu, Yuqian Yuan, Guanzheng Chen, Sicong Leng, Yuming Jiang, Hang Zhang, Xin Li, et al. Videollama 3: Frontier multimodal foundation models for image and video understanding. arXiv preprint arXiv:2501.13106, 2025. [64] Yi Wang, Xinhao Li, Ziang Yan, Yinan He, Jiashuo Yu, Xiangyu Zeng, Chenting Wang, Changlian Ma, Haian Huang, Jianfei Gao, et al. Internvideo2. 5: Empowering video mllms with long and rich context modeling. arXiv preprint arXiv:2501.12386, 2025. [65] Zekun Li, Xianjun Yang, Kyuri Choi, Wanrong Zhu, Ryan Hsieh, HyeonJung Kim, Jin Hyuk Lim, Sungyoung Ji, Byungju Lee, Xifeng Yan, et al. Mmsci: multimodal multi-discipline dataset for phd-level scientific comprehension. In AI for Accelerated Materials Design-Vienna 2024, 2024. [66] Enxin Song, Wenhao Chai, Weili Xu, Jianwen Xie, Yuxuan Liu, and Gaoang Wang. Video-mmlu: massive multi-discipline lecture understanding benchmark. arXiv preprint arXiv:2504.14693, 2025. [67] Haotian Liu, Chunyuan Li, Yuheng Li, and Yong Jae Lee. Improved baselines with visual instruction tuning. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 2629626306, 2024. [68] Jiyang Gao, Chen Sun, Zhenheng Yang, and Ram Nevatia. Tall: Temporal activity localization via language query. In Proceedings of the IEEE international conference on computer vision, pages 52675275, 2017. [69] Ranjay Krishna, Kenji Hata, Frederic Ren, Li Fei-Fei, and Juan Carlos Niebles. Dense-captioning events in videos. In Proceedings of the IEEE international conference on computer vision, pages 706715, 2017. [70] De-An Huang, Shijia Liao, Subhashree Radhakrishnan, Hongxu Yin, Pavlo Molchanov, Zhiding Yu, and Jan Kautz. Lita: Language instructed temporal-localization assistant. In European Conference on Computer Vision, pages 202218. Springer, 2024. [71] Bin Huang, Xin Wang, Hong Chen, Zihan Song, and Wenwu Zhu. Vtimellm: Empower llm to grasp video moments. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 1427114280, 2024. [72] Xiangyu Zeng, Kunchang Li, Chenting Wang, Xinhao Li, Tianxiang Jiang, Ziang Yan, Songze Li, Yansong Shi, Zhengrong Yue, Yi Wang, et al. Timesuite: Improving mllms for long video understanding via grounded tuning. arXiv preprint arXiv:2410.19702, 2024. [73] Hongyu Li, Songhao Han, Yue Liao, Junfeng Luo, Jialin Gao, Shuicheng Yan, and Si Liu. Reinforcement learning tuning for videollms: Reward design and data efficiency. arXiv preprint arXiv:2506.01908, 2025. [74] Sahar Kazemzadeh, Vicente Ordonez, Mark Matten, and Tamara Berg. Referitgame: Referring to objects in photographs of natural scenes. In Proceedings of the 2014 conference on empirical methods in natural language processing (EMNLP), pages 787798, 2014. [75] Licheng Yu, Patrick Poirson, Shan Yang, Alexander Berg, and Tamara Berg. Modeling context in referring expressions. In European conference on computer vision, pages 6985. Springer, 2016. [76] Ziwei Zheng, Michael Yang, Jack Hong, Chenxiao Zhao, Guohai Xu, Le Yang, Chao Shen, and Xing Yu. Deepeyes: Incentivizing\" thinking with images\" via reinforcement learning. arXiv preprint arXiv:2505.14362, 2025. [77] Hongyu Li, Jinyu Chen, Ziyu Wei, Shaofei Huang, Tianrui Hui, Jialin Gao, Xiaoming Wei, and Si Liu. Llava-st: multimodal large language model for fine-grained spatial-temporal understanding. In Proceedings of the Computer Vision and Pattern Recognition Conference, pages 85928603, 2025. [78] Zhaowei Li, Qi Xu, Dong Zhang, Hang Song, Yiqing Cai, Qi Qi, Ran Zhou, Junting Pan, Zefeng Li, Vu Tu, et al. Groundinggpt: In Proceedings of the 62nd Annual Meeting of the Association for Language enhanced multi-modal grounding model. Computational Linguistics (Volume 1: Long Papers), pages 66576678, 2024. 15 OneThinker: All-in-one Reasoning Model for Image and Video [79] Haibo Wang, Zhiyang Xu, Yu Cheng, Shizhe Diao, Yufan Zhou, Yixin Cao, Qifan Wang, Weifeng Ge, and Lifu Huang. Grounded-videollm: Sharpening fine-grained temporal grounding in video large language models. arXiv preprint arXiv:2410.03290, 2024. [80] Biao Wang, Wenwen Li, and Jiawei Ge. R1-track: Direct application of mllms to visual object tracking via reinforcement learning. arXiv preprint arXiv:2506.21980, 2025. [81] Henghui Ding, Chang Liu, Shuting He, Xudong Jiang, and Chen Change Loy. Mevis: large-scale benchmark for video segmentation with motion expressions. In Proceedings of the IEEE/CVF international conference on computer vision, pages 26942703, 2023. [82] Zhongwei Ren, Zhicheng Huang, Yunchao Wei, Yao Zhao, Dongmei Fu, Jiashi Feng, and Xiaojie Jin. Pixellm: Pixel reasoning with large multimodal model. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 2637426383, 2024. [83] Xin Lai, Zhuotao Tian, Yukang Chen, Yanwei Li, Yuhui Yuan, Shu Liu, and Jiaya Jia. Lisa: Reasoning segmentation via large language model. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 95799589, 2024. [84] Cilin Yan, Haochen Wang, Shilin Yan, Xiaolong Jiang, Yao Hu, Guoliang Kang, Weidi Xie, and Efstratios Gavves. Visa: Reasoning video object segmentation via large language models. In European Conference on Computer Vision, pages 98115. Springer, 2024. [85] Jiannan Wu, Yi Jiang, Peize Sun, Zehuan Yuan, and Ping Luo. Language as queries for referring video object segmentation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 49744984, 2022. 16 OneThinker: All-in-one Reasoning Model for Image and Video"
        },
        {
            "title": "A Reasoning Examples",
            "content": "Figure 6: Reasoning example of image question answering task. 17 OneThinker: All-in-one Reasoning Model for Image and Video Figure 7: Reasoning example of video question answering task. 18 OneThinker: All-in-one Reasoning Model for Image and Video Figure 8: Reasoning example of image caption task. 19 OneThinker: All-in-one Reasoning Model for Image and Video Figure 9: Reasoning example of video caption task. 20 OneThinker: All-in-one Reasoning Model for Image and Video Figure 10: Reasoning example of temporal grounding task. 21 OneThinker: All-in-one Reasoning Model for Image and Video Figure 11: Reasoning example of spatial grounding task. 22 OneThinker: All-in-one Reasoning Model for Image and Video Figure 12: Reasoning example of spatial-temporal grounding task. 23 OneThinker: All-in-one Reasoning Model for Image and Video Figure 13: Reasoning example of tracking task. 24 OneThinker: All-in-one Reasoning Model for Image and Video Figure 14: Reasoning example for an image segmentation task. The resulting answer will be forwarded to SAM2 to produce the mask. 25 OneThinker: All-in-one Reasoning Model for Image and Video Figure 15: Reasoning example for an video segmentation task. The resulting answer will be forwarded to SAM2 to produce the mask. 26 OneThinker: All-in-one Reasoning Model for Image and Video"
        },
        {
            "title": "B Prompt Template",
            "content": "Figure 16: System prompt for all tasks. Figure 17: Prompt for QA tasks. 27 OneThinker: All-in-one Reasoning Model for Image and Video Figure 18: Prompt for grounding and tracking tasks. Figure 19: Prompt for segmentation tasks."
        }
    ],
    "affiliations": [
        "MMLab, CUHK",
        "Meituan"
    ]
}