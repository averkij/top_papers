{
    "paper_title": "Do PhD-level LLMs Truly Grasp Elementary Addition? Probing Rule Learning vs. Memorization in Large Language Models",
    "authors": [
        "Yang Yan",
        "Yu Lu",
        "Renjun Xu",
        "Zhenzhong Lan"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Despite high benchmark scores, Large Language Models (LLMs) often fail simple problem, raising a critical question: Do LLMs learn mathematical principles or merely memorize patterns? Rather than designing increasingly complex benchmarks like recent works, we investigate this using elementary two-integer addition ($0$ to $2^{64}$), probing two core properties: commutativity ($A+B=B+A$) and compositional generalization (via isomorphic symbolic mappings, e.g., $7 \\rightarrow y$). While state-of-the-art LLMs achieve 73.8-99.8\\% accuracy on numerical addition, performance collapses to $\\leq$7.5\\% under symbolic mapping, indicating failure to generalize learned rules. Non-monotonic performance scaling with digit count and frequent commutativity violations (over 1,700 cases of $A+B \\neq B+A$) further support this. Explicitly providing addition rules degrades performance by 81.2\\% on average, while self-explanation maintains baseline accuracy, suggesting LLM arithmetic processing is misaligned with human-defined principles. Our findings indicate current LLMs rely on memory pattern over genuine rule learning, highlighting architectural limitations and the need for new approaches to achieve true mathematical reasoning."
        },
        {
            "title": "Start",
            "content": "Do PhD-level LLMs Truly Grasp Elementary Addition? Probing Rule Learning vs. Memorization in Large Language Models Yang Yan1,2, Yu Lu2, Renjun Xu1,*, Zhenzhong Lan2,* 1 Zhejiang University 2 School of Engineering, Westlake University {yan.yang,rux}@zju.edu.cn {yanyang,luyu,lanzhenzhong}@westlake.edu.cn 5 2 0 2 7 ] . [ 1 2 6 2 5 0 . 4 0 5 2 : r a"
        },
        {
            "title": "Abstract",
            "content": "Despite high benchmark scores, Large Language Models (LLMs) often fail simple problem, raising critical question: Do LLMs learn mathematical principles or merely memorize patterns? Rather than designing increasingly complex benchmarks like recent works, we investigate this using elementary two-integer addition (0 to 264), probing two core properties: commutativity (A + = + A) and compositional generalization (via isomorphic symbolic mappings, e.g., 7 Y). While state-of-the-art LLMs achieve 73.8-99.8% accuracy on numerical addition, performance collapses to 7.5% under symbolic mapping, indicating failure to generalize learned rules. Non-monotonic performance scaling with digit count and frequent commutativity violations (over 1,700 cases of + = + A) further support this. Explicitly providing addition rules degrades performance by 81.2% on average, while self-explanation maintains baseline accuracy, suggesting LLM arithmetic processing is misaligned with humandefined principles. Our findings indicate current LLMs rely on memory pattern over genuine rule learning, highlighting architectural limitations and the need for new approaches to achieve true mathematical reasoning."
        },
        {
            "title": "Introduction",
            "content": "Large Language Models (LLMs) have achieved remarkable proficiency in complex reasoning tasks, demonstrating PhD-level performance across diverse benchmarks (OpenAI, 2024b; DeepSeek-AI, 2025). Yet beneath this apparent sophistication lies concerning paradox: these models exhibit unexpected brittleness to minor input variations and struggle to transfer knowledge between similar tasks (Li et al., 2024; Mirzadeh et al., 2024). This disconnect raises fundamental question: Do *Corresponding Authors. 1 Figure 1: Illustration of Evaluating LLMs Arithmetic Comprehension. While LLMs demonstrate impressive performance on complex mathematical benchmarks, we examine their fundamental understanding through elementary addition, focusing on two essential arithmetic properties: (1) Commutativity (A + = + A) and (2) Compositional Generalization (invariance under symbolic transformations). Our systematic evaluation in Section 4 reveals that models fail to maintain these basic properties, suggesting they rely on pattern matching rather than demonstrating genuine arithmetic comprehension. LLMs truly learn generalizable arithmetic principles, or do they primarily exploit statistical patterns in their training data? This question proves challenging to answer through existing benchmarks like GSM8k (Cobbe et al., 2021) and MATH-500 (Lightman et al., 2024). While these evaluations demonstrate impressive capabilities, they often fail to distinguish between genuine rule comprehension and sophisticated pattern matching (Lin et al., 2024). The inherent complexity of such tasks can mask fundamental gaps in basic mathematical reasoning, as recent studies have revealed (Li et al., 2024; Mirzadeh et al., 2024). This limitation highlights the urgent need for evaluation methodologies that can reliably assess whether models have truly internalized fundamental arithmetic principles. Approach. To resolve the dichotomy between memorization and rule learning, we employ twointeger addition as our experimental paradigm. This operation provides an ideal testbed through its unambiguous algorithmic structure and unbounded problem space. Unlike complex benchmarks, this elementary operation offers three crucial advantages: (1) Controlled Rule Isolation permits direct assessment of carry-over operations through systematic digit progression analysis, (2) Infinite Problem Space (N N) fundamentally precludes pure memorization strategies, and (3) Operational Simplicity eliminates confounding linguistic factors. We structure our investigation around three complementary experiments: (1) Digit Scaling Tests (range: 0-264) examine whether accuracy follows the monotonic degradation pattern expected from true algorithmic implementation, (2) Symbolic Generalization evaluates abstract rule transfer through bijective digit-symbol mappings (e.g., 7Y, 9C), and (3) Rule Intervention Studies measure the impact of explicit instruction on performance, revealing potential misalignments between model computation and mathematical cognition. Findings. Our investigation reveals that LLMs rely more on memorization than learning true addition principles. The findings include: First, across various model architectures and sizes, LLMs show significant performance degradation when tested on symbolic representations compared to regular digits, indicating failure to generalize the addition rules. Second, accuracy exhibits non-monotonic variation with digit count, suggesting inconsistent rule application. Third, we observe asymmetric performance between + and + computations, indicating incomplete understanding of basic mathematical properties. Furthermore, models perform worse with explicit rule provision, suggesting potential conflict between externally provided rules and internalized knowledge. Contributions. This work makes three key contributions: (1) introduces novel methodology for evaluating rule learning in LLMs through controlled arithmetic experiments, (2) reveals fundamental differences between LLM computation mechanisms and human mathematical cognition, and (3) provides crucial insights for designing more effective benchmarks that can distinguish between genuine understanding and surface-level pattern matching. Our findings have important implications for developing more robust and interpretable language models."
        },
        {
            "title": "2 Related Works",
            "content": "LLM Benchmarking. LLM evaluation benchmarks (Hendrycks et al., 2021; Humanity-Team, 2025) increasingly test complex reasoning, including advanced mathematics (Cobbe et al., 2021; MAA, 2024). Despite high scores, whether LLMs genuinely understand math or merely match patterns remains debated. Such benchmarks often assess complex problem-solving, potentially masking failures in foundational rule-learning, which our work isolates using elementary addition. Robustness in Math Reasoning. Despite strong benchmarks, LLMs show critical math limitations. They struggle with symbolic representations (Mirzadeh et al., 2024) and exhibit error patterns suggesting incomplete principle grasp (Zhong et al., 2024; Zeng et al., 2024; Agarwal et al., 2021). Performance is also sensitive to numeral representation and tokenization (Zhou et al., 2024), hinting at reliance on formatting over abstract understanding. While techniques like specialized embeddings offer improvements (McLeish et al., 2024), core reasoning questions persist. Current metrics may thus inadequately assess true comprehension. Building on these findings, we use controlled symbolic mapping in elementary addition to rigorously test robustness and principle understanding beyond surface-level performance. Principle vs. Memorization. Research explores LLMs mathematical generalization mechanisms. Studies show limited transfer (Gorceix et al., 2024) but some algebraic understanding via CoT (Chang and Wu, 2024). Analyses suggest transformers decompose tasks like multiplication but struggle with mechanics like carry-overs (Qiu et al., 2024). Proposed mechanisms include implicit state representations (Chen et al., 2024), reliance on heuristics over algorithms (Nikankin et al., 2025), or learning symbolically via subgroups (Deng et al., 2024). The grokking phenomenon (Power et al., 2022) complicates the memorization vs. generalization dichotomy by showing delayed generalization. Evaluations like NTKEval suggest domain understanding emerges via ICL but instruction-tuning might be superficial (Guo et al., 2024). However, many studies focus on surface behaviors or specific 2 Figure 2: Performance Degradation Patterns in Zero-shot vs. Symbolic Addition. While LLMs achieve high accuracy on standard numerical addition (left), their non-monotonic performance curve suggests brittle pattern matching rather than true algorithmic reasoning. In contrast, symbolic addition tests (right) reveal systematic degradation with increasing digit count. This stark contrast between numerical and symbolic performance suggests LLMs rely heavily on memorized patterns rather than learned arithmetic principles. mechanisms. Our work directly probes elementary addition principles using symbolic mapping, commutativity tests, and rule provision to differentiate between memorized patterns, heuristics, and genuine rule learning."
        },
        {
            "title": "3 Methodology",
            "content": "While existing benchmarks often evaluate mathematical reasoning through increasingly complex tasks or noise-based robustness tests, we take fundamentally different approach by focusing on the most elementary operation: two-integer addition. This choice is motivated by three key advantages: (1) its unambiguous algorithmic structure permits direct assessment of rule comprehension, (2) its infinite problem space (N N) precludes pure memorization strategies, and (3) its operational simplicity eliminates confounding linguistic factors. Through systematic evaluation of addition problems ranging from 0 to 264, we investigate two fundamental questions: (1) Do LLMs truly grasp core addition principles? and (2) What factors influence their mathematical understanding?"
        },
        {
            "title": "3.1 Task Definition",
            "content": "We formalize our investigation through the elementary operation of two-integer addition: Given two non-negative integers A, N, compute their sum = + B. This operation decomposes into two fundamental algorithmic principles: 1. Position-wise Addition: For each position {0, . . . , 1}, compute si = ai + bi + ci, where ci denotes the carry-over from position 1. 2. Carry-over Propagation: For each intermediate sum si, determine the result digit as si mod 10 and propagate carry-over ci+1 = si/10 to position + 1. We establish three rigorous criteria for evaluating true comprehension of addition principles: (1) computational consistency across all digit configurations, demonstrating robust algorithmic execution, (2) preservation of fundamental mathematical properties, particularly commutativity (A + = + A), and (3) representation invariance under bijective transformations : {0, . . . , 9} Σ, where Σ represents an arbitrary set of distinct symbols (e.g., 7Y, 9C). These criteria assess whether model has internalized the abstract principles of addition beyond mere pattern recognition."
        },
        {
            "title": "3.2 Dataset Construction",
            "content": "To rigorously evaluate LLMs arithmetic capabilities, we constructed comprehensive dataset of 100,000 two-integer addition problems spanning [0, 264]. The dataset comprises three strategically designed phases: Phase 1 exhaustively covers all two-digit combinations (0-99), establishing baseline for elementary operations. Phase 2 samples equal-length digit pairs from [27, 249], testing scaling effects while controlling for digit count. Phase 3 extends to larger numbers ([249, 264]), probing performance limits and computational stability. To assess fundamental mathematical properties, we include both + and + variants of each problem, enabling systematic evaluation of commutativity understanding. For testing abstract rule comprehension, we apply bijective mappings 3 : {0, . . . , 9} Σ to create symbolic variants, where Σ represents set of random arbitrary symbols. This allows us to distinguish between superficial pattern matching and principle understanding. The dataset is randomly partitioned into training (80%), validation (10%), and test (10%) sets. This construction ensures comprehensive coverage of both elementary and advanced cases while enabling rigorous assessment of generalization across different numerical ranges and representational formats."
        },
        {
            "title": "3.3 Experiment Protocol",
            "content": "Our investigation employs systematic protocols to address two fundamental research questions: RQ1: Evaluating LLMs Understanding To assess LLMs comprehension of arithmetic principles, we implement dual-mode evaluation framework. First, we conduct zero-shot testing on standard addition tasks (A+B =?) across varying digit lengths, establishing baseline capabilities without contextual scaffolding. Second, we employ symbolic transformation tests, where digits are mapped to arbitrary symbols through bijective functions, probing models ability to transfer numerical understanding to abstract representations. This approach enables rigorous assessment of three critical competencies: (1) representation-invariant computation, (2) consistent performance scaling with digit count, and (3) adherence to fundamental mathematical properties such as commutativity. RQ2: Influence Factors Building on baseline performance metrics, we investigate whether and how models arithmetic capabilities can be enhanced through external interventions. Our analysis focuses on two complementary mechanisms: (1) explicit rule provision through few-shot prompting, where we supplement task prompts with concrete addition principles and examples, and (2) knowledge internalization through systematic fine-tuning approaches. This dual intervention strategy helps disentangle whether observed mathematical limitations stem from insufficient grasp of core principles or challenges in accessing and applying latent knowledge. Through controlled comparisons of intervention efficacy, we gain crucial insights into both the nature of mathematical reasoning in LLMs and potential pathways for further improvement. Models Llama3.3-70b-It Gemma-2-27b-it Qwen2.5-72B-Instruct DeepSeek-V3 gemini-2.0-flash-thinking-exp-01-21 gemini-2.0-pro-exp-02-05 gemini-2.0-flash-exp gemini-2.5-pro-exp-03-25 ERNIE-Speed GPT-4o Claude-3.5-sonnet ZS 79.7540.19 83.6536.98 96.1319.30 98.9210.34 91.0728.52 94.8822.04 98.1013.65 99.169.13 73.8443.95 93.3924.85 99.814.39 4.3020.29 2.7616.38 6.2924.28 16.1436.79 10.8131.05 14.2134.92 9.2528.98 55.9949.64 0.285.29 9.5929.45 7.5126.36 -75.45 -80.89 -89.84 -82.78 -80.26 -80.67 -88.85 -43.17 -73.56 -83.80 -92.30 Table 1: Performance Comparison Between Numerical and Symbolic Addition Tasks (%). SOTA Models exhibit strong performance on standard numerical addition (ZeroShot) but fail dramatically on isomorphic symbolic tasks ( shows the percentage point drop), revealing their lack of true compositional understanding. ZS = Zero-Shot, = Symbolic."
        },
        {
            "title": "4.1 Experimental Setup",
            "content": "Our evaluation framework systematically assesses LLMs arithmetic capabilities through comprehensive selection of state-of-the-art models, spanning both open-source and proprietary implementations. From the open-source domain, we evaluate multiple generations of major model families: Llama (Touvron et al., 2023; Dubey et al., 2024), Qwen (Bai et al., 2023; Yang et al., 2024a,b), and Gemma (Gemma et al., 2024). These represent diverse architectural approaches and parameter scales, enabling analysis of how different design choices impact arithmetic understanding. We complement these with proprietary models including DeepSeek-V3/R1 (DeepSeekAI, 2025), Ernie (Baidu, 2023), GPT-4o (OpenAI, 2024a), O1-Preview (OpenAI, 2024b), Claude-3.5sonnet (Claude, 2024), and Gemini 2/2.5 Family (Google, 2024, 2025), providing comprehensive view of current capabilities across the field. To ensure robust statistical analysis and exclude potential sampling bias, we perform 10 repeated evaluations per test example for most models, using temperature setting of 0.7. This experimental design enables both performance assessment and consistency analysis across architectures and model scales. Full evaluation specifications are detailed in Appendix A.2."
        },
        {
            "title": "4.2 RQ1: Insufficient Principle Mastery",
            "content": "To assess whether LLMs have truly internalized the core principles of addition, we examine their performance along three key dimensions: (1) representation invariance, (2) complexity scaling, and (3) adherence to fundamental algebraic properties. In 4 Task Type Models Position Addition Acc. ZS Carry-over Acc. ZS Models Llama3.3-70b-It gemma-2-27b-it Qwen2.5-72b-It DeepSeek-V3 gemini-2.0-flash-exp gemini-2.0-flash-thinking-exp-01-21 gemini-2.0-pro-exp-02-05 gemini-2.5-pro-exp-03-25 ERNIE-Speed gpt-4o claude-3-5-sonnet 73.82 74.77 88.19 78.55 73.83 86.09 69.52 88.97 67.66 76.12 81. 0.77 0.91 2.09 11.98 1.21 2.89 4.19 19.80 0.07 3.79 3.19 -73.05 -73.85 -86.10 -66.57 -72.62 -83.20 -65.33 -69.17 -67.59 -72.33 -78.59 75.00 76.68 89.78 81.14 79.52 88.30 77.36 88.49 70.89 79.55 90.28 2.43 0.91 4.12 15.23 3.28 9.03 7.07 24.56 0.21 6.73 6.92 -72.57 -75.77 -85.67 -65.91 -76.24 -79.27 -70.29 -63.93 -70.68 -72.82 -83.36 Table 2: LLMs Understanding of Addition Principles. Models achieve high accuracy(%) on standard numerical tasks (zero-shot) but show severe degradation when tested on symbolic representations, both for carry operations and digit addition. This stark contrast suggests that models only grasp principles in numerical form and fail to generalize to abstract representations. The full comparision is presented in Table our experiments, models were evaluated in zeroshot setting on standard numerical addition tasks as well as on isomorphic symbolic tasks, where digits are replaced with arbitrary symbols. Table 1 and Figure 2 summarize our findings. Representation Invariance Failure. genuine understanding of addition should be invariant to the specific representation of operands. Our results reveal that while LLMs achieve high accuracy on canonical digit addition (ranging from 73.84% to 99.81%), their performance collapses when the digits are substituted with symbols (mean = 81.23%). For example, Claude-3.5-sonnet, which scored 99.81% on numerical tasks, dropped to only 7.51% accuracy on symbolic addition. Notably, the Gemini family of models follows this pattern, with even the most advanced Gemini-2.5-pro experiencing substantial 43.17% drop, though it maintains the highest symbolic performance among tested models at 55.99%. Furthermore, we analyze the performance of models on carry operations and digit addition separately, as shown in Table 2. All LLMs exhibit stark contrast between numerical and symbolic tasks, failing to generalize addition principles with even simple rule composition. This pattern persists across model families, with Gemini models showing slightly better resilience in symbolic tasks compared to other models, particularly for carryover operations. This dramatic divergence indicates that the models success depends heavily on familiar digit representations rather than on an abstract, rule-based understanding of addition. Complexity Scaling Anomalies. robust algorithm for addition should exhibit predictable per5 Llama3.3-70b-It gemma-2-27b-it Gemma2-9b-it Meta-Llama-3-70B-Instruct Qwen2.5-14B-Instruct ZS # # 1771 1086 801 432 278 81 7 127 20 0 Table 3: Asymmetric Performance in Addition Tasks. Number of instances where models correctly solved + but failed + (or vice versa), demonstrating violation of the commutative property and suggesting incomplete mathematical understanding. formance degradation as problem complexity increasesthat is, accuracy should decline monotonically with an increasing number of digits. However, as shown in Figure 2, several LLMs demonstrate non-monotonic, drop-and-rise pattern in numerical addition; in some cases, models perform better on problems with more digits before ultimately declining. In contrast, performance on symbolic addition tasks degrades consistently and monotonically with increasing digit count. This discrepancy suggests that high numerical performance may stem from memorized digit patterns rather than from executing true, scalable addition algorithm. Algebraic Property Violations. fundamental property of addition is commutativity. We scrutinized failure cases where model correctly computed + but failed to compute + (or vice versa) consistently over 10 repeated evaluations. As detailed in Table 3, several models such as Llama3.3-70b-Itexhibited substantial number of such asymmetric failures (exceeding 1700 instances in the numerical setting). These commutativity violations are not isolated errors; they strongly imply that the models rely on directionspecific, memorized patterns rather than on comprehensive understanding of the addition rules. Collectively, these findings demonstrate that despite impressive zero-shot performance on standard numerical addition tasks, current state-of-theart LLMs fail to generalize basic arithmetic rules in representation-invariant manner. The nonmonotonic scaling with digit count and systematic commutativity violations underscore reliance on surface-level pattern matching rather than on genuine, algorithmic reasoning."
        },
        {
            "title": "4.3 RQ2: Influence Factors",
            "content": "To investigate the factors that influence LLMs mathematical reasoning capabilities, we organize our analysis along two key dimensions: (1) the Figure 3: Few-Shot Performance with Explicit Rule Provision. Explicit rule provision leads to significant drop in performance compared to zero-shot, contradicting the expected improvement. impact of explicit rule provision through different prompting strategies, exploring potential interactions between external guidance and pre-trained knowledge, and (2) the effects of various rule internalization approaches on models ability to generalize addition principles. This systematic investigation aims to shed light on the mechanisms underlying LLMs mathematical reasoning and their capacity to learn fundamental arithmetic principles."
        },
        {
            "title": "4.3.1 Explicit Rule Provision\nAs RQ1 finds that LLMs not truly grasp the ad-\ndition principles, a natural follow-up question is\nwhether explicit rule provision can enhance\ntheir performance. We tested this through several\nstrategies: (1) providing principles definition with\nexample problems with varying digit lengths (la-\nbeled as Few-Shot, Few-Shot-2, and Few-Shot-3\nfor digits 3-4, 6-7, and 9-10 respectively). For com-\nparison, we also implemented an Explain-and-Do\napproach, where models were required to explain\ntheir understanding of principles before providing\nanswers. Results from these interventions are pre-\nsented in Table 4 and Figure 3.",
            "content": "Knowledge Conflicts. Our investigation into explicit rule provision revealed fundamental limitation in how LLMs process mathematical concepts: when provided with abstract, general rules from humans, model performance consistently degraded compared to zero-shot testing, regardless of architecture or size. This phenomenon illuminates why LLMs struggle with compositional generalization as observed in RQ1: they appear fundamentally oriented towards memorizing specific patterns rather than abstracting general principles. When presented with human-provided rules that are abstract and generalizable (e.g., \"carry the 1 when sum exceeds 9\"), models struggle to operationalize these principles, instead defaulting to their pre-trained pattern-matching mechanisms. This preference for memorization over abstraction explains both the dramatic performance gap between numerical and symbolic tasks and the violation of basic mathematical properties like commutativity. Notably, the \"Explain-and-Do\" approach, which allows models to articulate problems in their own pattern-based terms, maintained performance closer to zero-shot baselines. These findings suggest that current LLM architectures are fundamentally optimized for pattern recognition rather than abstract rule learning, highlighting critical gap between human mathematical cognition and LLM computation. This insight has important implications for developing future AI systems capable of true mathematical reasoning rather than sophisticated pattern matching. Memorized Pattern hurt generalization. Our analysis of model behavior reveals significant architectural differences in how LLMs handle mathematical knowledge. Different model families exhibit distinct patterns in their ability to process and apply mathematical rules. The Llama family models, while generally underperforming in zero-shot conditions compared to Qwen and Gemma, show greater adaptability to explicit rule provision. For instance, Llama3.1 and Llama3.2 achieve superior performance in explain-and-do scenarios, even surpassing their zero-shot baselines and demonstrating improved compositional generalization. In contrast, the Qwen family models, despite stronger initial performance, experience more severe degradation when provided with explicit rules and show weaker compositional generalization capabilities. These architectural variations suggest that model performance depends not only on raw computational capacity but also on fundamental differences in how mathematical knowledge is encoded and accessed. Notably, successive model upgrades consistently 6 Task Type Models Carry-over Acc. ZS FS FS-2 FS-3 ZS Position Addition Acc. FS-2 FS FS-3 Llama2-7b-it Llama3-8b-it Llama3.1-8b-it Llama3.2-11b-it Qwen1.5-7b-it Qwen2-7b-it Qwen2.5-7b-it 22.58 15.89 21.96 17.35 47.44 62.94 74.49 0.01 0.20 0.25 0.26 0.09 0.06 0.13 0.06 8.42 8.84 9. 3.09 28.36 38.28 0.32 15.38 15.33 19.70 6.40 57.22 55.08 0.05 16.68 12.46 13.97 5.36 32.35 41.54 7.26 13.54 24.80 27. 7.51 70.83 72.09 20.44 16.25 20.38 16.60 46.78 60.03 71.39 0.01 0.07 0.10 0.12 0.05 0.05 0.11 0.03 7.15 7.92 8. 2.66 23.65 33.16 0.12 15.00 12.91 18.92 5.98 48.34 48.12 0.01 14.34 10.14 12.57 4.62 28.25 36.11 6.05 12.32 23.61 27.13 8.00 68.80 71.53 Gemma2-9b-it 60.44 0.44 28. 31.42 24.81 60.70 58.52 0.34 27. 30.76 23.83 59.78 ERNIE-Speed 70.89 0. 12.29 21.29 10.85 59.95 67.66 0. 11.64 20.23 9.55 54.43 Table 4: Impact of Different Knowledge Intervention Strategies. Contrary to expectations, providing explicit rules (few-shot conditions) significantly reduces performance compared to zero-shot baseline. However, when models explain their reasoning before computation (explain-and-do), performance remains comparable to zero-shot levels. ZS = Zero-Shot, FS = Few-Shot, = Explain-and-Do. Task Type Models Fine-Tuning Type Dataset Domain Overall Acc. ZS Position Addition Acc. ZS Carry-over Acc. ZS Qwen2.5-7B-Instruct - - 83.00 0.58 -82.41 71. 0.11 -71.28 74.49 0.13 -74.37 Qwen2.5-7B-Instruct Qwen2.5-7B-Instruct Qwen2.5-7B-Instruct Qwen2.5-7B-Instruct Qwen2.5-7B-Instruct Qwen2.5-7B-Instruct SFT RL(DPO) RL(SFT+DPO) SFT RL(DPO) RL(SFT+DPO) Task Specific (Numerical) Task Specific (Numerical) Task Specific (Numerical) Task Specific (Symbolic) Task Specific (Symbolic) Task Specific (Symbolic) Eurus-2-7B-SFT Eurus-2-7B-PRIME DS-R1-Distill-Qwen-7B RL(Reasoning) SFT RL(PRM) Domain Specific Domain Specific General 97.17 95.32 96.95 0.00 50.73 12. 83.21 94.11 74.76 -97.17 0.00 -94.95 0.37 0.28 -96.67 30.66 +30.66 -26.63 24.10 -9.47 2.85 0.42 1.03 6.88 -82.79 -93.08 -67.88 87.91 86.23 84.48 3.40 47.71 9.31 81.21 91.59 65. 0.25 1.17 0.29 3.89 3.48 0.58 3.19 3.10 33.41 -87.66 -85.06 -84.19 +0.49 -44.23 -8.73 -78.02 -88.49 -31.97 89.51 87.75 85.52 6.71 48.40 9.70 82.28 92.51 64. 1.26 2.35 0.61 6.98 6.37 1.13 6.87 3.11 31.52 -88.25 -85.40 -84.91 +0.27 -42.03 -8.57 -75.41 -89.40 -32.75 Table 5: Impact of Fine-Tuning Approaches on Arithmetic Capabilities. Different fine-tuning strategies and dataset domains yield distinct trade-offs between performance and generalization. While SFT achieves highest numerical accuracy, it shows minimal transfer to symbolic tasks. RL-based approaches demonstrate better generalization but lower absolute performance. Task-specific training on numerical data excels within-domain but fails to transfer, whereas general-domain training (e.g., DS-R1-Distill) enables broader generalization through its diverse training objectives, suggesting the importance of training paradigm design in developing robust mathematical capabilities. Task Type Models DeepSeek-V3 DeepSeek-R1 gemini-2.0-flash-exp gemini-2.0-flash-thinking-exp-01-21 gemini-2.0-pro-exp-02-05 gemini-2.5-pro-exp-03-25 GPT-4o o1-preview Llama3.3-70b-It DS-R1-Distill-Llama-70B QwQ-32B-Preview Qwen2.5-32b-It Llama3.1-8b-It DS-R1-Distill-Llama-8B Position Addition Acc. ZS Carry-over Acc. ZS 78.55 70.99 73.83 86.09 69.52 88.97 76.12 74.71 73.82 68. 71.68 90.41 20.38 45.54 11.98 - 1.21 2.89 4.19 19.80 3.79 - 0.77 42.94 19.09 - 0.10 39.55 -66.57 - -72.62 -83.20 -65.33 -69. -72.33 - -73.05 -25.97 -52.59 - -20.27 -5.99 81.14 80.58 79.52 88. 77.36 88.49 79.55 74.23 75.00 68.56 73.22 91.28 21.96 44.16 15.23 - 3.28 9.03 7.07 24.56 6.73 - 2.43 40.75 20.71 - 0.25 35. -65.91 - -76.24 -79.27 -70.29 -63.93 -72.82 - -72.57 -27.81 -52.51 - -21.72 -9.07 Table 6: LLMs Understanding of Addition Principles. Models achieve high accuracy(%) on standard numerical tasks (zero-shot) but show severe degradation when tested on symbolic representations, both for carry operations and digit addition. This stark contrast suggests that models only grasp principles in numerical form and fail to generalize to abstract representations. improve performance across all architectures, indicating that while architectural differences persist, general capabilities advance with model evolution. 7 This pattern of behavior strongly suggests that current LLM architectures prioritize pattern recognition over abstract rule learning, revealing fundamental limitation in their approach to mathematical reasoning. Such findings have profound implications for the development of future AI systems that aim to achieve genuine mathematical understanding rather than sophisticated pattern matching."
        },
        {
            "title": "4.3.2 Rule Internalization\nTo comprehensively understand how LLMs process\nand internalize arithmetic principles, we conducted\nextensive experiments on knowledge internaliza-\ntion through fine-tuning approaches.",
            "content": "Our investigation examined three key aspects: knowledge source variability (general, domainspecific, and task-specific), fine-tuning methodologies (supervised fine-tuning (SFT) and reinforcement learning (RL) approaches), and their combined effects on mathematical understanding. Specifically, we employed Direct Preference Optimization (DPO) (Rafailov et al., 2023) and its advanced variant RPO(SFT+DPO) (Pang et al., 2024) for reinforcement learning-based training. To evaluate the effectiveness of these approaches, we assessed model performance on both numerical and symbolic addition tasks post-fine-tuning. We also included comparative baselines from specialized mathematical reasoning models, including Eurus2 (Cui et al., 2025) with its unique combination of SFT and Process Reward Modeling (PRM) (Ma et al., 2023), as well as advanced reasoning models like OpenAI o1 (OpenAI, 2024b) and DeepSeek R1 (DeepSeek-AI, 2025), along with their distilled variants. For training data preparation, we sampled responses from various LLMs in zero-shot and symbolic settings, using correct answers from the training split as positive examples and incorrect responses as negative samples for reinforcement learning. Detailed specifications of the fine-tuning protocols are provided in Appendix A.3. The fine-tuning results presented in Table 5 reveal distinct patterns in knowledge internalization and generalization. Task-specific fine-tuning demonstrates clear trade-off: while SFT achieves the highest within-domain performance improvement, suggesting effective knowledge internalization, it exhibits poor cross-domain generalization. RL-based approaches (DPO, RPO) show lower absolute performance but superior generalization capabilities. Notably, RPOs significant transfer performance drop, despite combining SFT and DPO, indicates that SFTs pattern-matching tendency dominates the learning process. This suggests that current fine-tuning approaches, particularly SFT, may optimize for task-specific pattern recognition rather than abstract principle learning. In contrast, general-domain training methods like DS-R1-Distill demonstrate more robust generalization to symbolic tasks due to its long reasoning CoT training objective, which encourages diverse reasoning capabilities, highlighting how training paradigm design critically influences the development of generalizable mathematical capabilities. As comparison, domain-specific training like Eurus2-SFT and Eurus2-PRIME excels withindomain of complex mathematical and numerical task but fails to transfer, especially Eurus2-SFT shows minimal transfer while its further fine-tuned model Eurus2-PRIME with PRM shows better generalization, further comfirm the importance of RL in abstract principle from the training data. Analysis of specialized reasoning models  (Table 6)  reveals significantly reduced performance degradation on symbolic tasks compared to standard LLMs, indicating that prolonged reasoning training promotes better principle abstraction. However, Figure 2 exposes an important tradereasoning-focused architectures often sacoff: rifice accuracy on elementary computations for over-thinking while excelling at complex problemsolving. This pattern suggests that architectural design choices fundamentally influence how models balance basic computational competence against higher-order reasoning capabilities."
        },
        {
            "title": "5 Discussion",
            "content": "Pattern Matching vs. True Understanding. Our investigation reveals fundamental gap between LLMs superficial arithmetic capabilities and genuine mathematical comprehension. This conclusion is supported by four key findings: First, the dramatic performance drop in symbolic tasks (mean = 81.23%) shows that models rely on familiar digit patterns rather than abstract rules, violating the principle that mathematical understanding should be notation-invariant. Second, the non-monotonic accuracy patterns with increasing digits contradict the expected behavior of true algorithmic implementation, suggesting memorized patterns rather than learned principles. Third, systematic asymmetries between + and + computations (exceeding 1,700 cases in some models) demonstrate failure to grasp basic properties. Fourth, performance degradation with explicit rule provision indicates fundamental mismatch between LLMs pattern-based processing and abstract arithmetic principles. These findings suggest that current architectures may be inherently limited in their ability to learn and apply formal mathematical rules, highlighting the need for evaluation methods that can distinguish between pattern matching and true mathematical understanding. Benchmarking Limitations Mask Fundamental Issues. Our findings highlight critical shortcomings in current benchmarking methodologies for assessing LLM mathematical capabilities. While models achieve impressive scores on complex benchmarks like GSM8k and MATH-500, they fail to demonstrate genuine mathematical understanding in three key ways: First, high benchmark scores primarily reflect pattern recognition, as evidenced by models inability to handle symbolic representations of basic arithmetic. Second, current evaluation metrics reward surface-level pattern matching 8 without verifying comprehension of fundamental mathematical properties. Third, the sophistication of modern benchmarks can obscure basic deficiencies in mathematical reasoning, leading to overestimation of models true capabilities. These insights suggest that future benchmarks must incorporate (1) representation-invariant testing through symbolic transformations, (2) systematic verification of mathematical properties, and (3) rigorous complexity scaling analysis to meaningfully assess mathematical reasoning abilities."
        },
        {
            "title": "6 Conclusion",
            "content": "Through systematic evaluation of elementary addition, our work reveals fundamental limitations in LLMs mathematical capabilities. Despite impressive benchmarks, models show critical reliance on pattern matching rather than true understanding, evidenced by failures with symbolic representations and violations of basic properties. Explicit rule provision impairing performance suggests inherent architectural constraints. These insights reveal evaluation gaps and highlight the need for architectures capable of genuine mathematical reasoning beyond pattern recognition."
        },
        {
            "title": "Ethical Considerations",
            "content": "Core Ethical Concerns. Our investigation of LLMs mathematical capabilities through controlled experiments on synthetic datasets raises important ethical considerations regarding AI deployment and evaluation. While the immediate ethical risks of our research are limited, the implications are significant. Responsible Deployment and Trust. Our findings on LLMs arithmetic capabilities raise critical ethical concerns regarding deployment. The stark disconnect between models impressive benchmark performance and their fundamental reliance on pattern matching rather than true mathematical understanding presents significant risks. These limitations become particularly concerning in high-stakes domains requiring robust mathematical reasoning, such as medical diagnostics, financial analysis, or automated decision systems. The gap between perceived and actual capabilities could lead to serious consequences if models are deployed without proper understanding of their limitations. This underscores the urgent need for more rigorous evaluation frameworks and transparent communication of model limitations to stakeholders, especially in sensitive applications where mathematical reliability is crucial for public safety and trust. for System Evaluation. The Implications demonstrated limitations in mathematical reasoning highlight critical gaps in current evaluation frameworks. Standard benchmarks may inadvertently validate sophisticated pattern recognition without assessing true mathematical comprehension, creating false sense of capability. This misalignment between evaluation metrics and actual understanding poses risks for responsible AI deployment. Our findings emphasize the need for more rigorous evaluation methods that can effectively distinguish between surface-level pattern matching and genuine mathematical reasoning before deployment in critical applications."
        },
        {
            "title": "Limitations",
            "content": "Scope of Mathematical Operations. While our investigation of elementary addition provides valuable insights into LLMs mathematical processing, the focus on single operation potentially limits broader generalizability. Though addition serves as an ideal testbed for examining rule learning versus pattern matching, findings may not fully extend to more complex mathematical operations or abstract reasoning tasks. The symbolic transformation methodology effectively reveals fundamental limitations but might underestimate models capabilities in naturalistic mathematical contexts where pattern recognition could constitute valid problemsolving strategies. Methodological Framework. Our experimental design, while systematic, carries inherent constraints. The emphasis on supervised and reinforcement learning approaches to knowledge internalization, though illuminating core architectural limitations, may overlook alternative pathways for developing genuine mathematical comprehension. Additionally, our evaluation framework, despite its rigor in probing specific aspects of mathematical understanding, might not capture the full spectrum of mathematical reasoning capabilities, particularly in scenarios where pattern recognition aligns with principled problem-solving approaches. Technical and Analytical Boundaries. The studys technical scope, focused primarily on existing model architectures and training paradigms, leaves open questions about potential alternative 9 approaches to achieving true mathematical reasoning. While our findings convincingly demonstrate current limitations in principle abstraction, they also suggest unexplored directions for architectural innovations that might bridge the gap between pattern matching and genuine mathematical understanding. Future research should expand beyond addition to broader mathematical operations, explore diverse architectural approaches to principle learning, and develop more comprehensive frameworks for assessing mathematical comprehension while maintaining practical applicability."
        },
        {
            "title": "Acknowledgements",
            "content": "We would like to express our gratitude to the Gemini Developer API Team at Google for providing the extensive Gemini API access, which greatly facilitated our research."
        },
        {
            "title": "References",
            "content": "Marah Abdin, Jyoti Aneja, Harkirat Behl, Sébastien Bubeck, Ronen Eldan, Suriya Gunasekar, Michael Harrison, Russell J. Hewett, Mojan Javaheripi, Piero Kauffmann, James R. Lee, Yin Tat Lee, Yuanzhi Li, Weishung Liu, Caio C. T. Mendes, Anh Nguyen, Eric Price, Gustavo de Rosa, Olli Saarikivi, Adil Salim, Shital Shah, Xin Wang, Rachel Ward, Yue Wu, Dingli Yu, Cyril Zhang, and Yi Zhang. 2024. Phi-4 technical report. Preprint, arXiv:2412.08905. Vishesh Agarwal, Somak Aditya, and Navin Goyal. transformAnalyzing the nuances of 2021. ers polynomial simplification abilities. Preprint, arXiv:2104.14095. Jinze Bai, Shuai Bai, Yunfei Chu, Zeyu Cui, Kai Dang, Xiaodong Deng, Yang Fan, Wenbin Ge, Yu Han, Fei Huang, Binyuan Hui, Luo Ji, Mei Li, Junyang Lin, Runji Lin, Dayiheng Liu, Gao Liu, Chengqiang Lu, Keming Lu, Jianxin Ma, Rui Men, Xingzhang Ren, Xuancheng Ren, Chuanqi Tan, Sinan Tan, Jianhong Tu, Peng Wang, Shijie Wang, Wei Wang, Shengguang Wu, Benfeng Xu, Jin Xu, An Yang, Hao Yang, Jian Yang, Shusheng Yang, Yang Yao, Bowen Yu, Hongyi Yuan, Zheng Yuan, Jianwei Zhang, Xingxuan Zhang, Yichang Zhang, Zhenru Zhang, Chang Zhou, Jingren Zhou, Xiaohuan Zhou, and Tianhang Zhu. 2023. Qwen technical report. arXiv preprint arXiv:2309.16609. Baidu. 2023. Introducing ernie 3.5: Baidus knowledge-enhanced foundation model takes giant leap forward. http://research.baidu.com/ Blog/index-view?id=185. Fu-Chieh Chang and Pei-Yuan Wu. 2024. Unraveling arithmetic in large language models: The role of algebraic structures. Preprint, arXiv:2411.16260. Junhao Chen, Shengding Hu, Zhiyuan Liu, and Maosong Sun. 2024. States hidden in hidden states: Llms emerge discrete state representations implicitly. Preprint, arXiv:2407.11421. Claude. 2024. Claude 3.5 sonnet model card adhttps://www-cdn.anthropic.com/ dendum. fed9cc193a14b84131812372d8d5857f8f304c52/ Model_Card_Claude_3_Addendum.pdf. Karl Cobbe, Vineet Kosaraju, Mohammad Bavarian, Mark Chen, Heewoo Jun, Lukasz Kaiser, Matthias Plappert, Jerry Tworek, Jacob Hilton, Reiichiro Nakano, Christopher Hesse, and John Schulman. 2021. Training verifiers to solve math word problems. Preprint, arXiv:2110.14168. Ganqu Cui, Lifan Yuan, Zefan Wang, Hanbin Wang, Wendi Li, Bingxiang He, Yuchen Fan, Tianyu Yu, Qixin Xu, Weize Chen, Jiarui Yuan, Huayu Chen, Kaiyan Zhang, Xingtai Lv, Shuo Wang, Yuan Yao, Xu Han, Hao Peng, Yu Cheng, Zhiyuan Liu, Maosong Sun, Bowen Zhou, and Ning Ding. 2025. Process reinforcement through implicit rewards. Preprint, arXiv:2502.01456. DeepSeek-AI. 2025. Deepseek-r1: Incentivizing reasoning capability in llms via reinforcement learning. Preprint, arXiv:2501.12948. Chunyuan Deng, Zhiqi Li, Roy Xie, Ruidi Chang, and Hanjie Chen. 2024. Language models are symbolic learners in arithmetic. Preprint, arXiv:2410.15580. Abhimanyu Dubey, Abhinav Jauhri, Abhinav Pandey, Abhishek Kadian, Ahmad Al-Dahle, Aiesha Letman, Akhil Mathur, Alan Schelten, Amy Yang, Angela Fan, et al. 2024. The llama 3 herd of models. Preprint, arXiv:2407.21783."
        },
        {
            "title": "Shreya",
            "content": "Gemma, Morgane Riviere, Pathak, Pier Giuseppe Sessa, Cassidy Hardin, Surya Bhupatiraju, Léonard Hussenot, Thomas Mesnard, Bobak Shahriari, Alexandre Ramé, et al. 2024. Gemma 2: Improving open language models at practical size. Preprint, arXiv:2408.00118. Google. 2024. Introducing gemini 2.0: the agentic era. for new ai model //blog.google/technology/google-deepmind/ google-gemini-ai-update-december-2024. our https: Google. 2025."
        },
        {
            "title": "Gemini",
            "content": "2.5: ai model. intelligent google/technology/google-deepmind/ gemini-model-thinking-updates-march-2025. Our most https://blog. Antoine Gorceix, Bastien Le Chenadec, Ahmad Rammal, Nelson Vadori, and Manuela Veloso. 2024. Learning mathematical rules with large language models. Preprint, arXiv:2410.16973. Siyuan Guo, Aniket Didolkar, Nan Rosemary Ke, Anirudh Goyal, Ferenc Huszár, and Bernhard Schölkopf. 2024. Learning beyond pattern matching? assaying mathematical understanding in llms. Preprint, arXiv:2405.15485. Dan Hendrycks, Collin Burns, Steven Basart, Andy Zou, Mantas Mazeika, Dawn Song, and Jacob Steinhardt. 2021. Measuring massive multitask language understanding. In International Conference on Learning Representations. Humanity-Team. 2025. Humanitys last exam. Preprint, arXiv:2501.14249. Qintong Li, Leyang Cui, Xueliang Zhao, Lingpeng Kong, and Wei Bi. 2024. Gsm-plus: comprehensive benchmark for evaluating the robustness of llms as mathematical problem solvers. Preprint, arXiv:2402.19255. Hunter Lightman, Vineet Kosaraju, Yuri Burda, Harrison Edwards, Bowen Baker, Teddy Lee, Jan Leike, John Schulman, Ilya Sutskever, and Karl Cobbe. 2024. Lets verify step by step. In The Twelfth International Conference on Learning Representations. Zicheng Lin, Zhibin Gou, Tian Liang, Ruilin Luo, Haowei Liu, and Yujiu Yang. 2024. CriticBench: Benchmarking LLMs for critique-correct reasoning. In Findings of the Association for Computational Linguistics: ACL 2024, pages 15521587, Bangkok, Thailand. Association for Computational Linguistics. Qianli Ma, Haotian Zhou, Tingkai Liu, Jianbo Yuan, Pengfei Liu, Yang You, and Hongxia Yang. 2023. Lets reward step by step: Step-level reward model as the navigators for reasoning. Preprint, arXiv:2310.10080. MAA. 2024. American invitational mathematics https://huggingface.co/ examination 2024. datasets/Maxwell-Jia/AIME_2024. Sean Michael McLeish, Arpit Bansal, Alex Stein, Neel Jain, John Kirchenbauer, Brian R. Bartoldson, Bhavya Kailkhura, Abhinav Bhatele, Jonas Geiping, Avi Schwarzschild, and Tom Goldstein. 2024. Transformers can do arithmetic with the right embeddings. In The Thirty-eighth Annual Conference on Neural Information Processing Systems. Iman Mirzadeh, Keivan Alizadeh, Hooman Shahrokhi, Oncel Tuzel, Samy Bengio, and Mehrdad Farajtabar. 2024. Gsm-symbolic: Understanding the limitations of mathematical reasoning in large language models. Preprint, arXiv:2410.05229. Yaniv Nikankin, Anja Reusch, Aaron Mueller, and Yonatan Belinkov. 2025. Arithmetic without algorithms: Language models solve math with bag of heuristics. In The Thirteenth International Conference on Learning Representations. OpenAI. 2024a. Gpt-4 technical report. Preprint, arXiv:2303.08774. OpenAI. 2024b. Openai o1 system card. https://cdn. openai.com/o1-system-card-20241205.pdf. Richard Yuanzhe Pang, Weizhe Yuan, He He, Kyunghyun Cho, Sainbayar Sukhbaatar, and Jason Weston. 2024. Iterative reasoning preference optimization. In Advances in Neural Information Processing Systems, volume 37, pages 116617116637. Curran Associates, Inc. Alethea Power, Yuri Burda, Harri Edwards, Igor Babuschkin, and Vedant Misra. 2022. Grokking: Generalization beyond overfitting on small algorithmic datasets. Preprint, arXiv:2201.02177. Luyu Qiu, Jianing Li, Chi Su, Chen Jason Zhang, and Lei Chen. 2024. Dissecting multiplication Preprint, in transformers: arXiv:2407.15360. Insights into llms. Rafael Rafailov, Archit Sharma, Eric Mitchell, Christopher Manning, Stefano Ermon, and Chelsea Finn. 2023. Direct preference optimization: Your language model is secretly reward model. In Advances in Neural Information Processing Systems, volume 36, pages 5372853741. Curran Associates, Inc. Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, et al. 2023. Llama 2: Open foundation and fine-tuned chat models. Preprint, arXiv:2307.09288. An Yang, Baosong Yang, Binyuan Hui, Bo Zheng, Bowen Yu, Chang Zhou, Chengpeng Li, Chengyuan Li, Dayiheng Liu, Fei Huang, Guanting Dong, Haoran Wei, Huan Lin, Jialong Tang, Jialin Wang, Jian Yang, Jianhong Tu, Jianwei Zhang, Jianxin Ma, Jin Xu, Jingren Zhou, Jinze Bai, Jinzheng He, Junyang Lin, Kai Dang, Keming Lu, Keqin Chen, Kexin Yang, Mei Li, Mingfeng Xue, Na Ni, Pei Zhang, Peng Wang, Ru Peng, Rui Men, Ruize Gao, Runji Lin, Shijie Wang, Shuai Bai, Sinan Tan, Tianhang Zhu, Tianhao Li, Tianyu Liu, Wenbin Ge, Xiaodong Deng, Xiaohuan Zhou, Xingzhang Ren, Xinyu Zhang, Xipin Wei, Xuancheng Ren, Yang Fan, Yang Yao, Yichang Zhang, Yu Wan, Yunfei Chu, Yuqiong Liu, Zeyu Cui, Zhenru Zhang, and Zhihao Fan. 2024a. Qwen2 technical report. arXiv preprint arXiv:2407.10671. An Yang, Baosong Yang, Beichen Zhang, Binyuan Hui, Bo Zheng, Bowen Yu, Chengyuan Li, Dayiheng Liu, Fei Huang, Haoran Wei, Huan Lin, Jian Yang, Jianhong Tu, Jianwei Zhang, Jianxin Yang, Jiaxi Yang, Jingren Zhou, Junyang Lin, Kai Dang, Keming Lu, Keqin Bao, Kexin Yang, Le Yu, Mei Li, Mingfeng Xue, Pei Zhang, Qin Zhu, Rui Men, Runji Lin, Tianhao Li, Tingyu Xia, Xingzhang Ren, Xuancheng Ren, Yang Fan, Yang Su, Yichang Zhang, Yu Wan, Yuqiong Liu, Zeyu Cui, Zhenru Zhang, and Zihan Qiu. 2024b. Qwen2.5 technical report. arXiv preprint arXiv:2412.15115. Zhongshen Zeng, Pengguang Chen, Shu Liu, Haiyun Jiang, and Jiaya Jia. 2024. Mr-gsm8k: metareasoning benchmark for large language model evaluation. Preprint, arXiv:2312.17080. 11 Lunjun Zhang, Arian Hosseini, Hritik Bansal, Mehran Kazemi, Aviral Kumar, and Rishabh Agarwal. 2024. Generative verifiers: Reward modeling as next-token prediction. Preprint, arXiv:2408.15240. We conducted comprehensive evaluations across all model variants in both zero-shot and symbolic settings, with complete results presented in Table 7. Lianmin Zheng, Liangsheng Yin, Zhiqiang Xie, Chuyue Sun, Jeff Huang, Cody Hao Yu, Shiyi Cao, Christos Kozyrakis, Ion Stoica, Joseph E. Gonzalez, Clark Barrett, and Ying Sheng. 2024. Sglang: Efficient execution of structured language model programs. Preprint, arXiv:2312.07104. Qihuang Zhong, Kang Wang, Ziyang Xu, Juhua Liu, Liang Ding, and Bo Du. 2024. Achieving >97% on GSM8k: Deeply understanding the problems makes LLMs better solvers for math word problems. Preprint, arXiv:2404.14963. Zhejian Zhou, JIayu Wang, Dahua Lin, and Kai Chen. 2024. Scaling behavior for large language models regarding numeral systems: An example using pythia. In Findings of the Association for Computational Linguistics: EMNLP 2024, pages 38063820, Miami, Florida, USA. Association for Computational Linguistics."
        },
        {
            "title": "A Appendix",
            "content": "A.1 AI Use Statement This research utilized AI assistance for code debugging and grammatical refinement. All experimental designs, analyses, results, and conclusions were developed independently by the authors without generative AI input. We employed AI tools solely for technical implementation support and language polishing to ensure clear communication of our findings. A.2 Evaluation Configuration Our evaluation framework utilized the SGLang platform through the official Docker container lmsysorg/sglang (Zheng et al., 2024). For statistical robustness, most models underwent 10 repeated evaluations per test example using temperature setting of 0.7 across the full dataset. Due to computational and budget constraints, select models including GPT4-o, Claude-3.5-Sonnet, QwQ-32B-Preview, Deepseek-R1 and its variants were evaluated once on the test split only. A.3 Fine-Tuning Configuration Our investigation employed three fine-tuning approaches: standard DPO, RPO (combining DPO with SFT), and pure SFT. Each approach shared core configuration elements while varying key method-specific parameters. Base Configuration. The base configuration utilized batch size of 1 sample per device with 4 gradient accumulation steps (effective batch size of 4). Training ran for 1 epoch using cosine learning rate scheduling with 10% warmup steps. We implemented BF16 mixed precision and non-reentrant gradient checkpointing, evaluating on 1% validation set every 500 steps. Flash Attention 2 optimized computation efficiency. Distributed Training. Training leveraged DeepSpeed ZeRO-3 with 8 processes per machine. The implementation included CPU optimizer state offloading, gradient clipping at 1.0, 16-bit parameter saving, and static process coordination through DeepSpeeds rendezvous mechanism. Method-specific Parameters. Standard DPO: Learning rate 5.0 106, β = 0.0, sigmoid loss function RPO: DPO settings with β = 1.0 for integrated preference modeling and SFT SFT: Learning rate 1.0 104 for supervised training All approaches utilized full-parameter finetuning through DeepSpeed ZeRO-3. For preference learning (DPO/RPO), we initialized reference models from SFT checkpoints with preference loss weight (λftx) set to 1.0. Infrastructure. Training infrastructure consisted of 4 NVIDIA A100 GPUs (80GB each), with complete fine-tuning requiring approximately 15 hours per run."
        },
        {
            "title": "For",
            "content": "assessing Position Addition and Carry-over accuracy, we used Phi-4 (Abdin et al., 2024) as an independent generative evaluator following Zhang et al. (2024). Solutions were evaluated by feeding them to the evaluator to determine carry-over and position addition correctness, using the first token as the prediction. 12 Prompt Template for Zero-Shot Setting Context: You are helpful AI assistant. Instruction: Present your solution in the following format: 1. Lets think step by step. 2. Final Answer: Express using LaTeX notation boxed{answer} Question: %s + %s = boxed{?} Figure 4: Zero-Shot Setting Prompt Template. Example prompt template for zero-shot addition tasks, providing context, instructions, and question format for LLMs. Prompt Template for Few-Shot Setting Context: You are helpful AI assistant. Instruction: Present your solution in the following format: 1. First, compute the sum of the two numbers, working from right to left using place values. 2. Then, for each place value, add the digits in the same place value column, and carry over if the sum is greater than 9. 3. Iterate this process from right to left until all place values are added. 4. Final Answer: Express using LaTeX notation boxed{answer}. Examples: 1. Compute 1996 + 126 = boxed{?} Lets solve 1996 + 126 step by step, working from right to left using place values. For the ones place: 6 + 6 = 12. Write down 2 in the ones place and carry over 1 to the tens place. For the tens place: 9 + 2 + 1 = 12. Write down 2 in the tens place and carry over 1 to the hundreds place. For the hundreds place: 9 + 1 + 1 = 11. Write down 1 in the hundreds place and carry over 1 to the thousands place. For the thousands place: 1 + 1 = 2. Putting it all together: 2 * 1000 + 1 * 100 + 2 * 10 + 2 * 1 = 2000 + 100 + 20 + 2 = 2122. Therefore, 1996 + 126 = boxed{2122}. 2. Compute 1994 + 222 = boxed{?} Lets solve 1994 + 222 step by step, working from right to left using place values. For the ones place: 2 + 4 = 6. For the tens place: 2 + 9 = 11. Write down 1 in the tens place and carry over 1 to the hundreds place. For the hundreds place: 2 + 9 + 1 = 12. Write down 2 in the hundreds place and carry over 1 to the thousands place. For the thousands place: 1 + 1 = 2. Putting it all together: 2 * 1000 + 2 * 100 + 1 * 10 + 6 * 1 = 2000 + 200 + 10 + 6 = 2216. Therefore, 1994 + 222 = boxed{2216}. Question: %s + %s = boxed{?} Figure 5: Few-Shot Setting Prompt Template. Example prompt template for few-shot addition tasks, providing context, instructions, examples, and question format for LLMs. Prompt Template for Explain-and-Do Setting Context: You are helpful AI assistant. Instruction: Present your solution in the following format: 1. First, comprehensively explain how to do addition with both positive integers. 2. Then, lets analyze the problem step by step following your explanation. 3. Final Answer: Express using LaTeX notation boxed{answer}. Question: %s + %s = boxed{?} Figure 6: Explain-and-Do Setting Prompt Template. Example prompt template for explain-and-do addition tasks, providing context, instructions, and question format for LLMs. 13 Prompt Template for Symbolic Setting Context: You are helpful AI assistant. Your task is to perform addition within custom symbolic system in simple and clear manner. Symbolic System Definition: This system comprises ten symbols: {u, d, a, i, h, v, e, y, r, c}. The addition operation (+) between these symbols is defined as follows: + = + = + = + = + = + = + = + = + = + = + = + = + = + = + = + = + = + = + = + = + = + = + = + = + = du + = + = + = + = + = du + = dd + = + = + = du + = dd + = da + = du + = dd + = da + = di + = da + = di + = dh + = dh + = dv + = de + = du + = dd + = da + = di + = dh + = dv + = de + = dy + = dr Instruction: Present your solution in the following format: 1. Align: Arrange the two input strings vertically, aligning their rightmost symbols. 2. Columnar Addition: Starting from the rightmost column (least significant symbols), perform symbol addition using the provided definition. 3. Carry-over: If the result of columns addition is two-symbol sequence (e.g., da), write down the second symbol (least significant) and carry over the first symbol to the next column on the left. 4. Iteration: Repeat steps 2 and 3, moving leftward column by column until all symbols have been added. 5. Reasoning: Keep your whole reasoning clear and simple. 6. Output Format: Write the final result in the boxed{?} placeholder. Examples: 1. Compute dcce + dae = boxed{?} Solution: 1. Columnar Addition (right to left): - + = da (Write a, Carry d) - + + = da (Write a, Carry d) - + + = dd (Write d, Carry d) - + = 2. Result: adaa 3. Formatted Output: boxed{adaa} 2. Compute dcch + aaa = boxed{?} Solution: 1. Columnar Addition (right to left): - + = - + = dd (Write d, Carry d) - + + = da (Write a, Carry d) - + = 2. Result: aade 3. Formatted Output: boxed{aade} Your Task: Compute %s + %s = boxed{?} Figure 7: Symbolic Setting Prompt Template. Example prompt template for symbolic addition tasks, providing context, symbolic system definition, instructions, examples, and question format for LLMs."
        },
        {
            "title": "Task Type\nModels",
            "content": "gemini-2.0-flash-exp gemini-2.0-flash-thinking-exp-01-21 gemini-2.0-pro-exp-02-05 gemini-2.5-pro-exp-03-25 claude-3.5-sonnet gpt-4o o1-preview ERNIE-Speed-8K DeepSeek-V2.5 DeepSeek-V3 DeepSeek-R1 DeepSeek-R1-Distill-Llama-70B DeepSeek-R1-Distill-Llama-8B DeepSeek-R1-Distill-Qwen-1.5B DeepSeek-R1-Distill-Qwen-7B gemma-2-2b-it gemma-2-9b-it gemma-2-27b-it Overall Acc. ZS Position Addition Acc. ZS 98.10 91.07 94.88 99.16 99.81 93.39 74.28 73.78 95.75 98.92 97.39 74.19 53.23 58.16 74.76 33.41 66.34 83.65 9.25 10.81 14.21 55. 7.51 9.59 - 0.29 - 16.14 - 27.19 10.97 0.66 6.88 - 1.45 2.62 -88.85 -80.26 -80.67 -43.17 -92.30 -83.80 - -73. - -82.78 - -47.00 -42.26 -57.50 -67.88 - -64.89 -81.03 73.83 86.09 69.52 88.97 81.78 76.12 74.71 67.66 83.78 78.55 70. 68.91 45.54 47.85 65.38 29.97 58.52 74.77 1.21 2.89 4.19 19.80 3.19 3.79 - 0.07 - 11.98 - 42.94 39.55 26.16 33. - 0.34 0.91 -72.62 -83.20 -65.33 -69.17 -78.59 -72.33 - -67.59 - -66.57 - -25.97 -5.99 -21.69 -31.97 - -58.18 -73. Carry-over Acc. ZS 79.52 88.30 77.36 88.49 90.28 79.55 74.23 70.89 88.19 81.14 80. 68.56 44.16 47.16 64.27 30.59 60.44 76.68 3.28 9.03 7.07 24.56 6.92 6.73 - 0.21 - 15.23 - 40.75 35.09 20.79 31. - 0.44 0.91 -76.24 -79.27 -70.29 -63.93 -83.36 -72.82 - -70.68 - -65.91 - -27.81 -9.07 -26.37 -32.75 - -59.99 -75. Llama-2-7b-chat-hf 19.59 0.00 -19.59 20.44 0. -20.43 22.58 0.01 -22.57 Meta-Llama-3-8B-Instruct Meta-Llama-3-70B-Instruct Meta-Llama-3.1-8B-Instruct Meta-Llama-3.1-70B-Instruct 32.95 69.15 43.34 72.58 0.24 1.62 0.57 2.51 -32.70 -67.53 -42.76 -70. 16.25 59.84 20.38 60.13 0.07 0.39 0.10 0.52 -16.18 -59.45 -20.27 -59. 15.89 60.22 21.96 61.05 0.20 0.70 0.25 1.33 -15.69 -59.52 -21.72 -59. Llama-3.2-11B-Vision-Instruct 35.13 0.53 -34.61 16.60 0. -16.48 17.35 0.26 -17.09 Llama-3.3-70B-Instruct 79. 4.01 -75.61 73.82 0.77 -73.05 75. 2.43 -72.57 Qwen1.5-7B-Chat Qwen1.5-72B-Chat Qwen2-7B-Instruct Qwen2-72B-Instruct Qwen2.5-1.5B-Instruct Qwen2.5-3B-Instruct Qwen2.5-7B-Instruct Qwen2.5-14B-Instruct Qwen2.5-32B-Instruct Qwen2.5-72B-Instruct 56.31 34. 72.50 59.06 47.75 70.27 83.00 87.45 95.15 96.07 0.18 0.53 0.24 2.50 - - 0.58 - - 5.97 -56.14 -33. -72.26 -56.56 - - -82.41 - - -90.10 46.78 62.28 60.03 82.82 32.54 54.49 71.39 77.56 90.41 88.19 0.05 0. 0.05 0.21 - - 0.11 - - 2.09 -46.73 -62.20 -59.98 -82.62 - - -71.28 - - -86.10 47.44 67. 62.94 86.62 33.67 57.98 74.49 80.36 91.28 89.78 0.09 0.14 0.06 0.26 - - 0.13 - - 4.12 -47.34 -67. -62.88 -86.36 - - -74.37 - - -85.67 QwQ-32B-Preview 70.59 11.12 -59. 71.68 19.09 -52.59 73.22 20.71 -52. Eurus-2-7B-SFT Eurus-2-7B-PRIME qwen2.5-7b-dpo-sft-S qwen2.5-7b-dpo-sft-ZS qwen2.5-7b-dpo-S qwen2.5-7b-dpo-ZS qwen2.5-7b-sft-S qwen2.5-7b-sft-ZS 83.21 94.11 12.32 96.95 50.73 95.32 0.00 97.17 0.42 1.03 -82.79 -93. 2.85 0.28 24.10 0.37 30.66 0.00 -9.47 -96.67 -26.63 -94.95 30.66 -97.17 81.21 91.59 9.31 84.48 47.71 86.23 3.40 87.91 3.19 3.10 0.58 0.29 3.48 1.17 3.89 0. -78.02 -88.49 -8.73 -84.19 -44.23 -85.06 0.49 -87.66 82.28 92.51 9.70 85.52 48.40 87.75 6.71 89.51 6.87 3.11 1.13 0.61 6.37 2.35 6.98 1. -75.41 -89.40 -8.57 -84.91 -42.03 -85.40 0.27 -88.25 Table 7: Complete Performance Analysis on Base and Extended Addition Tasks. Per-model breakdown of performance (%) across standard numerical and symbolic representations, with evaluation of degradation () between formats. Results reveal systematic failures in abstracting arithmetic principles despite high numerical accuracy."
        }
    ],
    "affiliations": [
        "School of Engineering, Westlake University",
        "Zhejiang University"
    ]
}