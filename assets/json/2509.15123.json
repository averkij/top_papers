{
    "paper_title": "RGB-Only Supervised Camera Parameter Optimization in Dynamic Scenes",
    "authors": [
        "Fang Li",
        "Hao Zhang",
        "Narendra Ahuja"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Although COLMAP has long remained the predominant method for camera parameter optimization in static scenes, it is constrained by its lengthy runtime and reliance on ground truth (GT) motion masks for application to dynamic scenes. Many efforts attempted to improve it by incorporating more priors as supervision such as GT focal length, motion masks, 3D point clouds, camera poses, and metric depth, which, however, are typically unavailable in casually captured RGB videos. In this paper, we propose a novel method for more accurate and efficient camera parameter optimization in dynamic scenes solely supervised by a single RGB video, dubbed ROS-Cam. Our method consists of three key components: (1) Patch-wise Tracking Filters, to establish robust and maximally sparse hinge-like relations across the RGB video. (2) Outlier-aware Joint Optimization, for efficient camera parameter optimization by adaptive down-weighting of moving outliers, without reliance on motion priors. (3) A Two-stage Optimization Strategy, to enhance stability and optimization speed by a trade-off between the Softplus limits and convex minima in losses. We visually and numerically evaluate our camera estimates. To further validate accuracy, we feed the camera estimates into a 4D reconstruction method and assess the resulting 3D scenes, and rendered 2D RGB and depth maps. We perform experiments on 4 real-world datasets (NeRF-DS, DAVIS, iPhone, and TUM-dynamics) and 1 synthetic dataset (MPI-Sintel), demonstrating that our method estimates camera parameters more efficiently and accurately with a single RGB video as the only supervision."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 9 1 ] . [ 2 3 2 1 5 1 . 9 0 5 2 : r RGB-Only Supervised Camera Parameter Optimization in Dynamic Scenes Fang Li University of Illinois at Urbana-Champaign Champaign, IL 61820 fangli3@illinois.edu Hao Zhang University of Illinois at Urbana-Champaign Champaign, IL 61820 haoz19@illinois.edu Narendra Ahuja University of Illinois at Urbana-Champaign Champaign, IL 61820 n-ahuja@illinois.edu Figure 1: (a) Overview of our RGB-only supervised camera parameter optimization. (b) Front view of the 3D Gaussian field reconstructed by our camera estimates at time t. (c) 2D renderings (RGB and depth) at time with quantitative metrics. Our optimization is not only significantly more efficient and accurate, but also avoids overfitting the reconstruction to specific viewpoints. Record3D is mobile app that factory-calibrates the intrinsic and uses LiDAR sensors to collect metric depth for camera pose estimates, thus does not have valid runtime."
        },
        {
            "title": "Abstract",
            "content": "Although COLMAP has long remained the predominant method for camera parameter optimization in static scenes, it is constrained by its lengthy runtime and reliance on ground truth (GT) motion masks for application to dynamic scenes. Many efforts attempted to improve it by incorporating more priors as supervision such as GT focal length, motion masks, 3D point clouds, camera poses, and metric depth, which, however, are typically unavailable in casually captured RGB videos. In this paper, we propose novel method for more accurate and efficient camera parameter optimization in dynamic scenes solely supervised by single RGB video, dubbed ROS-Cam. Our method consists of three key components: (1) Patch-wise Tracking Filters, to establish robust and maximally sparse hinge-like relations across the RGB video. (2) Outlier-aware Joint Optimization, for efficient camera parameter optimization by adaptive down-weighting of moving outliers, without reliance on motion priors. (3) Two-stage Optimization Strategy, to enhance stability and optimization speed by trade-off between the Softplus limits and convex minima in losses. We visually and numerically evaluate our camera estimates. To Preprint. Under review. further validate accuracy, we feed the camera estimates into 4D reconstruction method and assess the resulting 3D scenes, and rendered 2D RGB and depth maps. We perform experiments on 4 real-world datasets (NeRF-DS, DAVIS, iPhone, and TUM-dynamics) and 1 synthetic dataset (MPI-Sintel), demonstrating that our method estimates camera parameters more efficiently and accurately with single RGB video as the only supervision."
        },
        {
            "title": "Introduction",
            "content": "Despite recent progress in visual odometry, efficiently and accurately optimizing camera parameters1 (focal length + rotation&translation) from casually collected RGB dynamic-scene videos remains big challenge. Although the most predominant COLMAP [32] method2 is RGB-only supervised, it suffers from its lengthy runtime and requisite of GT motion masks to mask out the outlier moving stuff. In Table 1, most recent approaches [6, 45, 3, 42, 59, 46, 56, 44] attempted to improve through being supervised by additional GT priors such as focal length, metric depth, 3D point clouds, camera poses, and motion masks, which are typically unavailable in casually collected videos. We cannot help but ask natural question: Is it possible to accurately and efficiently estimate camera parameters in dynamic scenes in an RGB-only supervised manner - the most minimal form of supervision? Existing RGB-only supervised methods [42, 45, 20, 59, 3] make obvious improvements, but they mostly rely on multiple pre-trained dense prediction models [38, 13, 31] to compensate for the inaccuracies of individual pseudo-supervision sources, resulting in performance degradation if any of them fails. They also cannot adaptively exclude moving outliers without GT motion supervision. Besides, their high computational latency always leads to lengthy runtimes. Further discussion of related work is provided in Section 2. Table 1: Categorization of supervision of current methods. Ours, casualSAM [58], and RobustCVD [16] are RGB-only supervised, while our performance is the best as shown in section 4. Supervision Static Scene Dynamic Scene GT 3D Point Cloud & Camera Pose Dust3r [46], Fast3r [52], Mast3r [18], Spann3r [40], VGGT [41] Monst3r [56], Cut3r [44], Stereo4D [11], Easi3r [4] GT Focal Length CF-3DGS [6], Nope-NeRF [1], LocalNeRF [22] + Metric Depth + GT Motion Priors GT Motion Priors RGB-Only VGGSfM [42], FlowMap [35], InstantSplat [5], COLw/o mask [32] Robust-CVD [16], casualSAM [58], Ours (ROS-Cam) DROID-SLAM [39] GFlow [45], LEAP-VO [3] RoDynRF [20], COLw/ mask [32], ParticleSfM [59] Based on these insights, we propose ROS-Cam, an RGB-only supervised, accurate, and efficient camera parameter optimization method, with brief performance overview in Figure 1. Specifically, to minimize reliance on pre-trained dense prediction models while still establishing robust and maximally sparse hinge-like relations across the video as accurate pseudo-supervision (bottom right corner in Figure 2), we propose the novel patch-wise tracking filters built solely on pre-trained point tracking (PT) model. This formulation effectively avoids inaccurate tracking trajectories extracted across frames and computational latency induced by the noisy dense prediction as pseudo-supervision. However, the extracted pseudo-supervision includes portion of trajectories belonging to moving outliers. To eliminate the influence of such outliers, we introduce learnable uncertainty associated with each calibration point, where each is learnable 3D position in the world coordinates, corresponding to one extracted tracking trajectory. We model such uncertainty parameters with the Cauchy distribution, which can deal with heavy tails better than, e.g., the Gaussian distribution, and propose the novel Average Cumulative Projection error and Cauchy loss for the outlier-aware joint optimization of the calibration points, focal length, rotation, translation, and uncertainty parameters. Unlike casualSAM and LEAP-VO, which assign uncertainty parameters to 2D pixels, our approach associates uncertainties with sparse 3D calibration points, resulting in significantly fewer learnable parameters and reduced runtime, as shown in Table 3. Such joint optimization is prone to getting trapped in local minima. To address this, we analyze the asymptotic behavior of the Softplus function and the analytical minima of the inner convex term in 1Like all existing methods in table 1, we also assume pinhole camera. 2We denote the COLMAP using motion masks as COLw/ mask and the one w/o motion masks as COLw/o mask. 2 losses to propose two-stage optimization strategy to accelerate and stabilize the optimization. We evaluate the performance of our method through extensive experiments on 5 popular public datasets - NeRF-DS [50], DAVIS [28], iPhone [7], MPI-Sintel [2], and TUM-dynamics [36], demonstrating our superior performance. Our contributions can be summarized as follows. We propose the first RGB-only supervised, accurate, and efficient camera parameter optimization method in dynamic scenes with three key components: (1) patch-wise tracking filters; (2) outlier-aware joint optimization; and (3) two-stage optimization strategy. We present exhaustive quantitative and qualitative experiments and extensive ablation studies that demonstrate the superior performance of our proposed method and the contribution of each component."
        },
        {
            "title": "2 Related Works",
            "content": "Dynamic Scene Reconstruction/Novel View Synthesis (NVS). Existing methods for reconstructing objects and scenes use variety of 3D representations, including planar [8, 9], mesh [51, 55], point cloud [48, 57], neural field [23, 50, 37, 29, 21], and the recently introduced Gaussian explicit representations [49, 14, 10, 47, 53]. NeRF [23] enables high-fidelity NVS. Some methods [25, 26, 29, 50, 15, 24] also extend NeRF to dynamic scenes, while others [51, 55, 54, 43, 37] build on them to extract high-quality meshes. However, NeRF-based methods have the limitation of long training time. Recently, 3DGS [14] effectively addressed this issue by using 3D Gaussian-based representations and presented Differential-Gaussian-Rasterization in CUDA. 3DGS optimizes 3D Gaussian ellipsoids as dynamic scene representations associated with attributes such as position, orientation, opacity, scale, and color. Several studies [47, 53] also have used 3DGS for dynamic scenes, achieving near real-time dynamic scene novel view synthesis. However, both NeRF-based and 3DGS-based methods heavily rely on COLw/ mask to estimate camera parameters. Camera Parameter Optimization. Many efforts have been made to overcome the shortcomings of COLMAP, particularly for dynamic scenes. But each suffers from some constraints. In Table 1, we present categorization of supervision of current SOTA methods. Supervised by additional GT focal length, CF-3DGS [6], Nope-NeRF [1], and LocalNeRF [22] leverage pre-trained monocular depth estimation model [31] to estimate camera poses and the static scene jointly. The most representative SLAM-based method - DROID-SLAM [39], leverages both GT focal length and metric depth as supervision. GFlow and LEAP-VO [45, 3] extend it to dynamic scenes with both GT focal length and motion priors as supervision. Although VGGSfM, FlowMap, InstantSplat, COLw/o mask [42, 35, 5, 32] eliminate the GT focal length requirement by leveraging pre-trained PT models [38, 13], they cannot handle the moving objects in dynamic scenes. RoDynRF [20], COLw/ mask [32], and ParticleSfM [59] simply tackle such problem by incorporating GT motion supervision like GFlow. Recently, DUSt3Rbased methods [46, 52, 18, 40, 41] and their dynamic-scene counterparts [56, 44, 11, 4] explored feed-forward camera parameter prediction by training on large-scale static and dynamic scene datasets, respectively, in fully supervised manner - that is, using GT 3D point clouds and camera poses as supervision, requiring several days training on high-end GPUs. However, unlike LLMs that benefit from abundant language data, such metric 3D supervision is relatively scarce in the vision area, leading to frequent domain gaps when these models are applied to unseen data. In contrast, Robust-CVD [16], casualSAM [58] and our method conduct camera parameter optimization for dynamic scenes in more general RGB-only supervised way. However, as shown in Section 4, their performance is significantly worse than ours."
        },
        {
            "title": "3 Method",
            "content": "Under RGB-only supervision, RGB frames Fi, [0, 1] (N is frame count) are given. Our proposed patch-wise tracking filters (Section 3.1) extract robust and maximally sparse hingelike tracking trajectories as pseudo-supervision, where each corresponds to one calibration point R3, [0, H] in the world coordinates. Under such pseudo-supervision and our newly Pcali proposed ACP error and Cauchy loss, the calibration points Pcali, focal length R, quaternion matrix RN 4, translation RN 3, and motion-caused uncertainty parameters Γ RH >0 are jointly optimized (Section 3.2). Γ is the scale parameter of the Cauchy distribution which is used to model such uncertainty parameters, associated with each calibration point, to reduce the erroneous 3 influence of moving outliers. By analyzing the Softplus limits and convex minima in losses, we propose simple but effective two-stage optimization strategy (Section 3.3) to enhance the stability and optimization speed."
        },
        {
            "title": "3.1 Patch-wise Tracking Filters",
            "content": "Built on pre-trained PT model, we observe that its attention mechanism assigns higher attention weights to pixels with more accurate tracking results which are always texture-rich pixels with large gradient norms. Inspired by it, as shown in Figure 2, we propose the patch-wise texture filter to identify the high-texture patches within Ft and the patch-wise gradient filter to select the pixel with the highest gradient norm within each identified patch. While tracking such identified points, the visibility filter keeps removing trajectories that become invisible and the patch-wise distribution filter keeps the one with the largest gradient norm when multiple moving points enter the same patch. As shown in section E.2.1  (fig. 10)  , our method only retains the robust and accurate trajectories as pseudo-supervision. Figure 2: Patch-wise tracking filters. (1) Partitioning F0 into patches of size w, the patch-wise texture filter computes the texture map T0 and marks the high-texture patches in gray; (2) Within each high-texture patch, the patch-wise gradient filter selects one potential tracking point with the highest gradient norm. (3) The visibility filter removes the entire trajectory of point if it becomes invisible at any time (, , kept trajectories; , , removed trajectories); (4) The patch-wise distribution filter only keeps the one with the largest gradient norm when multiple trajectories fall into the same patch. and are the location and index of trajectory, and is the trajectory range. Patch-wise Texture Filter. Highly distinguishable points, that can be tracked reliably, belong to highly nonuniform (textured) neighborhoods. To identify such neighborhoods, our patch-wise texture filter computes texture map Ti 1H/wW/w, giving measure of texture level for each patch where and denote the height & width of Fi. We represent the texture level of patch by Ti[m, n] = 1{Σi[m, n] > τvar σ} (1) , where Σi RH/wW/w is the intensity variance, σ = max(Σi), τvar is the percentage threshold of minimum variance for the patch to be selected, and m, [0, H/w 1], [0, W/w 1]. The texture levels of patch are represented by 1 for the selected patches and 0 for the others. Patch-wise Gradient Filter. Within the identified patches, our patch-wise gradient filter computes the intensity gradient norm map Gi RHW of Fi, and selects the point with the largest gradient norm within each patch. This yields the pool of potentially distinguishable points, forming potential trajectories, namely, Ppotential m,n = arg max (Gi[mw : mw + w, nw : nw + w]), pixel locations (2) Visibility Filter. We find that current PT models [13, 12, 30] still tend to suffer from reduced tracking accuracy when point becomes occluded and later reappears, due to the disruption of temporal feature continuity. Thus, if any in any Fi becomes invisible, our visibility filter deletes it by the dot product V, {0, 1} P, where = 0 if point is invisible. Patch-wise Distribution Filter. This filter enforces more even point distribution within each frame, preventing them from clustering into small region as the viewpoint changes. It also helps reduce susceptibility to loss of resolution which might result in triangulation errors. We keep the highest-gradient tracking point in each patch atm,n of Fi, as follows: 4 = arg max Gi[ atm,n], if (cid:88) 1( atm,n) > 1 (3) As shown in Figure 2, locations and indices of are stored in Pi RB2 and Ii RB, [0, 1], acting as pseudo-supervision in the outlier-aware joint optimization. Each iteration starts at Ft, = arg mint(1 It), and ends until each frame contains exactly tracked points."
        },
        {
            "title": "3.2 Outlier-aware Joint Optimization",
            "content": "Outlier-aware Joint Optimization Mechanism. Under the obtained pseudo-supervision, Pcali, , Q, and Γ are jointly optimized. We first project Pcalihomo RH4 (the homogeneous coordinates of Pcali obtained by concatenating 1) onto each frame by Pprojhomo = Pcalihomo[Ii] (cid:20)Ri 0 (cid:21)T ti 1 KT Pproj = Pprojhomo [:, : 2]/Pprojhomo [:, 3] (4) (5) , where [0, 1] and Pproj RN B2. Pprojhomo RN B4 denotes the homogeneous 2D location of the projection Pproj. The perspective projection matrix R44 is derived from , and the world-to-camera transformation matrix consists of rotation Ri and translation ti. We assume constant like SOTA [59, 58, 20]. Notably, we learn the quaternion matrix Qi instead of optimizing the Ri and additional constraints. This optimization approach circumvents the difficult-to-enforce orthogonality and 1 determinant constraints required for rotation matrices during optimization. Figure 3: Outlier-aware Joint Optimization. represents Pt and Pt on each frame. The static samples pcali and pcali can establish concrete triangulation relations with their corresponding Pt, Pt, and cameras, resulting in lower γ and γ. In contrast, the dynamic sample pcali exhibits the opposite behavior. Figure 4: Runtime Trends. As the frame count increases, our runtime grows almost linearly, whereas COLw/omask scales exponentially. The runtime of casualSAM is too large to fit in this figure. The complete runtime is in Table 3, and section E.1.1 (table 9, table 10, table 11). 1 Γ )2] πΓ[1+( xx0 However, the extracted pseudo-supervision always contains moving outliers. To mitigate its impact, without any GT motion priors, we identify such outliers by modeling the uncertainty their presence may cause in the observed distributions of the inlier points. We introduce the uncertainty Γ RH associated with Pcali RH and incorporate the Cauchy distribution , Γ > 0 to model the uncertainty parameter Γ since this distribution (x; x0, Γ) = can better handle the heavy tails than, e.g., the Gaussian distribution. As depicted in Figure 3, during optimization, inliers are expected to have low uncertainty, while outliers have high uncertainty. Since the scale parameter Γ in (x; x0, Γ) is required to be strictly positive, we introduce new parameter Γraw which we obtain Γ from using the Softplus function Γ = log(1 + eΓraw ), Γraw RH . This effectively ensures Γ RH >0 is differentiable and has smooth gradients. Losses. To down-weight outliers by learned Γ, we replace the commonly used projection error Eproj = Pproj P2 2 with our proposed Average Cumulative Projection (ACP) error, defined as: EACP h[0,H1] = (cid:80) 1{I=h} Pproj P2 (cid:80) 1{I=h} 2 (6) 5 , where EACP RH and denotes the element-wise matrix multiplication. For each Pcali , we accumulate the errors between its corresponding projection and tracking locations across the video, then take the average as EACP h[0,H1]. Furthermore, we propose the novel Cauchy loss Lcauchy in terms of the negative-log-likelihood log (Γ + (xx0)2 ) of (x; x0, Γ) where we replace x0 with Γ EACP as eq. (7). Our total loss Ltotal in Equation (8) consists of Lcauthy and depth regularization term Rdepth to encourage positive depth. With the estimated camera parameters, we use 4DGS [47] for scene reconstruction. Reconstruction and loss derivation details are in section and section B. Lcauchy = 1 (cid:88) h=0 log (Γ + (EACP )2 Γ ) Ltotal = Lcauthy + Rdepth, Rdepth = 1 (cid:88) i=0 ReLU(Pprojhomo [:, 3]) (7) (8)"
        },
        {
            "title": "3.3 Two-stage Optimization Strategy",
            "content": "To avoid convergence to local minima, we propose this strategy based on an analysis of the asymptotic behavior of the Softplus function and the analytical minima of the inner convex term in Lcauthy. Stage 1 focuses on rapid convergence, while Stage 2 aims for stable convergence by initializing Γraw to the ACP error after Stage 1. The effectiveness of it is concretely demonstrated in Table 7. Stage 1. In the Softplus function, Γ = log(1 + eΓraw ) Γraw, as Γraw +. So in Stage 1, we fix Γraw = 1 and optimize only Pcali, , Q, and for quick convergence. The loss will converge to certain value beyond the global minimum, as there is no proper Γ to down-weight outliers. Stage 2. The inner term Φ = + and solving for minx Φ(x), we have = to values largely different from EACP unstable. Therefore, we initialize Γraw = EACP , > 0 of Lcauchy is convex. Assuming constant R+ O. Similarly, in Stage 2, if Γraw is randomly initialized stage1 (the ACP error from Stage 1), convergence will be highly stage1, and optimize Pcali, , Q, t, and Γraw jointly."
        },
        {
            "title": "4 Experiments",
            "content": "To demonstrate the superiority of our method, we show extensive quantitative and qualitative results in this section. For NeRF-DS [50], DAVIS [28], and iPhone [7] datasets without GT camera parameters, we feed the camera parameters from different methods to 4DGS [47], while keeping all other factors the same, and evaluate each NVS performance (PSNR, SSIM, and LPIPS). Regarding the MPI-Sintel [2] and TUM-dynamics [36] datasets with GT camera parameters, we directly evaluate methods by ATE, RPE trans, and RPE rot metrics. In all tables, the best and second-best results are bold and underline. More about datasets, and evaluation metrics are in section and section D."
        },
        {
            "title": "4.1 Implementation Details",
            "content": "The optimization is conducted on 1 NVIDIA A100 40GB GPU with Adam [27] optimizer and learning rates lQ = 0.01, lt = 0.01, lf = 1.0, lPcali = 0.01, and lΓraw = 0.01. We also choose to build our patch-wise tracking filters on CoTracker [13] and load its pre-training weights. The hyperparameters of our patch-wise tracking filters are set at τvar = 0.1, = 100, wNeRF-DS, DAVIS, MPI-Sintel = 12, and wiPhone, TUM = 24. Notably, is only related to the frame size. Besides, throughout our experiments, we have 200 and 50 iterations in Stage1 and Stage2 respectively. Table 5: Camera Pose Evaluation on MPI-Sintel [2]. (ATE/RPE trans/RPE rot) We achieve better results than casualSAM [58] and exclude COLw/o mask due to its failure. Method ambush_4 ambush_5 ambush_6 market_ market_2 alley_1 alley_2 casualSAM [58] Ours 0.028/0.006/0.057 0.002/0.003/0.038 0.003/0.003/0.392 0.009/0.002/0. 0.040/0.058/0.321 0.119/0.049/1.367 0.053/0.040/0.211 0.065/0.039/1.192 0.302/0.088/2.362 0.080/0.129/2.191 0.010/0.010/0.041 0.003/0.010/0.110 0.239/0.207/0.544 0.009/0.006/0.301 Method shaman_3 sleeping_1 sleeping_2 temple_2 mountain_1 bamboo_ bamboo_2 casualSAM [58] Ours 0.008/0.009/0.050 0.003/0.001/0.085 0.017/0.016/0.173 0.008/0.001/0.074 0.013/0.025/0.170 0.002/0.001/0.034 0.005/0.004/0.380 0.017/0.003/0. 0.003/0.004/0.182 0.007/0.004/0.060 0.033/0.009/0.056 0.003/0.003/0.033 0.005/0.003/0.035 0.004/0.003/0.033 6 2: NVS Evaluation on Table NeRF-DS [50] and DAVIS [28]. (PSNR/SSIM/LPIPS) is supervised by additional GT priors. Ours is the best among these two datasets. Method RoDynRF[20] COLw/ mask COLw/o mask casualSAM[58] Ours NeRF-DS DAVIS 23.033/0.749/0.385 32.174/0.923/0.147 - - 29.348/0.875/0.224 21.230/0.686/0.463 33.552/0.938/0.118 9.196/0.236/0.435 19.032/0.486/0.482 22.292/0.709/0.279 Camera Pose Evaluation on TUMTable 4: dynamics [36]. Other results are from Cut3r [44] and Monst3r [56]. Performance of DROID-SLAM [39] is from casualSAM [58]. Our method achieves the best overall performance among all RGB-only supervised methods, and even better than the ones supervised by additional GT priors. Supervision Method ATE RPE trans RPE rot Table 3: Runtime Evaluation on NeRFDS [50], DAVIS [28], and iPhone [7], covering frame count from 50 to 900. is supervised by additional GT priors. Our method is the most efficient. GT 3D Point Cloud & Camera Pose GT Focal Length + GT Motion Prior GT Focal Length + Metric Depth Monst3r [56] Dust3r [46] Mast3r [18] Cut3r [44] 0.098 0.083 0.038 0.046 0.019 0.017 0.012 0.015 LEAP-VO [3] 0.046 0.027 DROID-SLAM [39] 0.043 Method RoDynRF [20] COLw/ mask COLw/o mask casualSAM [58] Ours NeRF-DS DAVIS iPhone GT Motion Priors ParticleSfM [59] 29.6h 1.5h 1.8h 10.5h 0.83h 27.4h - 0.51h 0.28h 0.03h 28.5h - 9.53h 4.07h 0.33h RGB-Only Robust-CVD [16] casualSAM [58] Ours - 0.153 0.071 0.065 0.935 3.567 0.448 0. 0.385 - - 3.528 1.712 0.987 - - 0.026 0.010 0."
        },
        {
            "title": "4.2 Time Efficiency Evaluation",
            "content": "In Table 3, we present the average runtime evaluations. Our average runtime on NeRF-DS, DAVIS, and iPhone is 55%, 11%, and 8% of that of the second-fastest methods, while keeping the best performance as shown in table 2. We attribute it to three main reasons: (1) Our method only leverages the maximally sparse pseudo-supervision extracted by our proposed patch-wise tracking filters under the RGB-only supervision. (2) Our uncertainty parameters are associated with the 3D calibration points rather than 2D uncertainty maps [58], significantly reducing the number of learnable parameters. For Plate video (424 frames) in NeRF-DS, casualSAM has (424270480) uncertainties, whereas our method only has 440 uncertainties, one per Pcali. 3) The two-stage optimization strategy highly accelerates the optimization speed. As seen from Table 7, omitting the two-stage strategy leads to dramatic performance drop after the same iterations, indicating more iterations, and thus time, are needed to achieve the same performance. Figure 5: Qualitative NVS Results on DAVIS [28]. Our performance is the best because of our accurate camera estimates. More are in section E.2.2 (fig. 14, fig. 15, fig. 16, and fig. 17). Figure 6: Qualitative Results of Camera Pose on MPI-Sintel [2]. represents our camera estimates; represents the GT. Our estimated camera trajectories almost perfectly align with the GT. Besides, in Figure 4, we see that our method exhibits linear growth (at the rate of about 1/800 hours per frame) vs COLw/omask whose runtime growth is roughly exponential. This difference will be increasingly significant as the video length increases, which can also demonstrate the superior 7 Table 6: NVS Evaluation on iPhone [7]. (PSNR/SSIM/LPIPS) Record3D is paid mobile app obtaining camera results by LiDAR sensors, where are provided by [7]. Ours is the best among RGB-only supervised methods and surpasses LiDAR-based Record3D sometimes. Creeper Method Paper-..mill Backpack Space-out Teddy Apple Block Record3D COLw/o mask casualSAM[58] Ours 26.35/0.77/0.33 22.45/0.69/0.41 19.03/0.58/0.57 25.96/0.74/0.37 23.91/0.73/0.24 22.74/0.72/0.28 18.85/0.39/0.54 24.09/0.74/0.22 27.12/0.77/0.33 24.33/0.74/0.38 22.09/0.67/0.47 28.42/0.79/0.31 20.79/0.56/0.40 18.58/0.39/0.54 18.41/0.36/0.55 21.22/0.64/0.32 23.72/0.71/0.38 18.49/0.60/0.49 19.10/0.59/0.52 23.28/0.69/0. 21.80/0.63/0.27 18.13/0.43/0.48 16.40/0.29/0.62 21.67/0.63/0.28 19.72/0.59/0.41 16.56/0.50/0.50 15.69/0.42/0.58 20.78/0.60/0.41 Method Handwavy Haru-sit Mochi-..five Spin Sriracha Pillow Wheel Record3D COLw/o mask casualSAM[58] Ours 27.80/0.86/0.24 15.69/0.61/0.55 20.87/0.68/0.46 28.02/0.86/0. 29.86/0.87/0.22 25.58/0.80/0.30 19.88/0.69/0.41 28.31/0.85/0.24 34.34/0.91/0.24 22.47/0.77/0.38 26.34/0.84/0.35 34.56/0.92/0.22 24.85/0.69/0.38 19.27/0.55/0.49 19.33/0.45/0.57 24.81/0.67/0.39 31.15/0.87/0.25 28.41/0.86/0.28 23.20/0.73/0.42 32.49/0.89/0.25 20.86/0.63/0.41 14.75/0.46/0.57 16.95/0.51/0.55 20.63/0.61/0.44 20.78/0.60/0.41 20.78/0.60/0.41 14.69/0.47/0.57 20.42/0.67/0. time efficiency of our method compared with other RGB-only supervised methods. We exclude the casualSAM here since its runtime is too large to fit here."
        },
        {
            "title": "4.3 Camera Pose Evaluation",
            "content": "We follow the same evaluation setup of Cut3r [44] and Monst3r [56] on TUM-dynamics [36] and evaluate all videos of the synthetic MPI-Sintel [2] dataset. Quantitative Evaluation. In Table 5 and Table 4, our method has the best performance among all RGB-only supervised approaches. Our method also achieves comparable or even better results than others that require additional GT priors as supervision. We attribute it to our accurate and robust pseudo-supervision, derived from RGB-only input, enabling effective outlier-aware joint optimization. Besides, our uncertainty modeling and loss design effectively down-weight the impact of moving outliers. Since COLw/ mask and COLw/o mask always fail on MPI-Sintel [2], as observed by us and [20, 56], we exclude comparisons with them here. However, RGB-only supervised methods including ours perform not very well in some special cases, which is discussed in the limitations. Qualitative Evaluation. In Figure 6, we show our estimated camera trajectories alongside the GT on MPI-Sintel [2]. Our estimated camera trajectories can perfectly overlap with the GT, which provides qualitative support to the higher accuracies seen in the quantitative results in Table 5."
        },
        {
            "title": "4.4 NVS Evaluation",
            "content": "Since NeRF-DS [50], DAVIS [28], and iPhone [7] datasets do not provide GT camera parameters, we follow [6, 20, 45, 19] by inputting camera estimates of different methods into the same 4D reconstruction pipeline - 4DGS [47], and evaluate the NVS performance. Such NVS performance reveals the quality of the camera parameter estimation. Quantitative Evaluation. In Table 2, our method is the best on NeRF-DS [50] (long videos w/ little blur, textureless regions, and specular moving objects) and DAVIS [28] (short videos w/ low parallax and rapid object movement), demonstrating our more accurate camera estimates. We skip COLw/ mask and RoDynRF on DAVIS [28] because they are not RGB-only supervised methods and require supervision beyond RGB frames, and have already underperformed compared to ours on NeRF-DS [50]. Besides, our pseudo-supervision extraction built on the PT models [13, 12] performs better on low-parallax videos, which remains challenging for the pre-trained depth model [31]. Regarding the iPhone [7] dataset (videos w/ irregular camera movement and object movement), it provides so-called GT camera parameters obtained by Record3D which is paid mobile app obtaining camera results by LiDAR sensors. However, we observe that such so-called GT camera parameters are occasionally unreliable. As shown in Table 6 and fig. 7, besides being the best among all RGB-only supervised methods, our method can occasionally beat Record3D. Qualitative Evaluation. We evaluate the quality of the rendered RGB images and depth maps in Figure 7, Figure 8, and Figure 5. Beyond superior RGB renderings, our camera estimates yield the highest-quality depth maps, offering more convincing evidence of accurate scene geometry than RGB renderings. It indicates that our estimated camera parameters enable the model to learn the correct dynamic scene representations rather than overfitting to training views. In the first row of Figure 7, ours performs the best (surpassing even Record3D), especially in rendered depth; whereas 8 Figure 7: Qualitative NVS Results on iPhone [7]. Our method outperforms other SOTA RGB-only supervised approaches and even surpasses LiDAR-based Record3D when the movement in scenes with large motion (top row). More are in section E.2.2 (fig. 11, fig. 12, and fig. 13). Table 7: Ablation Study on NeRF-DS [50] - Part 1. We conduct ablation studies on different scene optimization strategies and individual components of our proposed model. Scene Optimization Camera Optimization PSNR SSIM LPIPS D-3DGS [53] 4DGS [47] COLw/o mask Ours (full) COLw/o mask Ours (full) + w/o two-stage + w/o Γ + w/o EACP + w/o texture filt. + w/o gradient filt. + w/o distrib. filt. 31.14 32.45 29.35 33.55 25.95 26.44 23.56 25.99 26.04 26.02 0.9192 0. 0.8748 0.9381 0.8100 0.8667 0.7203 0.8356 0.8393 0.8382 0.1609 0.1312 0.2240 0.1182 0.2668 0.2327 0.3139 0.2536 0.2404 0.2497 Figure 8: Qualitative NVS results on NeRFDS [50]. Our renderings are the most plausible. More are in section E.2.2  (fig. 18)  . in the second row, our method does not match Record3D, but is still better than other RGB-only supervised works. This is because Record3D is not originally designed for dynamic scenes, so when scene contains larger irregular movements, its performance will be worse (such observations are also supported by the numerical results in Table 6). In contrast, our method is more robust in various scenarios, consistently maintaining high standards. Table 8: Ablation Study on NeRF-DS [50] - Part 2. We conducted ablation studies on different PT models. Ablation Study. In Table 7, the loss of any filter results in less robust relations across video, leading to poor camera estimates and NVS performance. Further, the removal of any of Γ, EACP , or the two-stage strategy will harm the results due to outliers. This indicates that w/o such strategy, increasing training iterations is necessary but not sufficient condition for comparable results. We also overcome the limitations of COLMAP [32] by improving the performance of different scene optimization models [53, 47] with our camera estimates. As reported in CoTracker3 [12], CoTracker [13] performs worse than CoTracker3. However, in table 8, the performance of our proposed method is nearly independent of building on the particular PT model. This further supOurs + built on CoTracker [13] Ours + built on CoTracker3 [12] Camera Optimization (PT model choice) Scene Optimization 4DGS [47] LPIPS PSNR SSIM 0.1182 0.9381 0.9384 0. 33.52 33.55 9 Figure 9: Failure cases of ours and casualSAM on MPI-Sintel [2]. ports our claim that our patch-wise tracking filters effectively exact only the accurate trajectories as pseudo-supervision."
        },
        {
            "title": "5 Conclusion and Limitation",
            "content": "We proposed new RGB-only supervised, accurate, and efficient camera parameter optimization method in casually collected dynamic-scene videos. Our method effectively tackles the challenge of precisely and efficiently estimating per-frame camera parameters under the situation of having no additional GT supervision (e.g. GT motion masks, focal length, 3D point clouds, metric depth, and camera poses) other than RGB videos, which is the most common scenario in real-world and consumer-grade reconstructions. Our method may serve as step towards high-fidelity dynamic scene reconstruction from casually captured videos. Although our proposed method is currently the most state-of-the-art RGB-only supervised, accurate, and efficient camera parameter optimization method in dynamic scenes, there are still several limitations. We assume constant focal length throughout the video. While this assumption is reasonable and currently common to SOTA, the task of accurate and efficient camera parameter optimization for dynamic scene videos with zooming effects under RGB-only supervision remains an open problem. Another common challenge for RGB-only supervised methods, not addressed in this paper, is maintaining robustness in scenes dominated by large moving objects. As shown in fig. 9, the screen space is occupied by the moving human and dragon. It is challenging for our method to establish robust and maximally sparse hinge-like relations as accurate pseudo-supervision because most of the extracted trajectories belong to outliers. CasualSAM [58] struggles due to the rapid changes in depth maps from frame to frame, making 3D space alignment difficult. We plan to maintain consistency in our input setup and address these challenges as part of future research."
        },
        {
            "title": "References",
            "content": "[1] Wenjing Bian, Zirui Wang, Kejie Li, Jia-Wang Bian, and Victor Adrian Prisacariu. Nope-nerf: Optimising neural radiance field with no pose prior. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 41604169, 2023. [2] Daniel Butler, Jonas Wulff, Garrett Stanley, and Michael Black. naturalistic open source movie for optical flow evaluation. In Computer VisionECCV 2012: 12th European Conference on Computer Vision, Florence, Italy, October 7-13, 2012, Proceedings, Part VI 12, pages 611625. Springer, 2012. [3] Weirong Chen, Le Chen, Rui Wang, and Marc Pollefeys. Leap-vo: Long-term effective any point tracking In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern for visual odometry. Recognition, pages 1984419853, 2024. [4] Xingyu Chen, Yue Chen, Yuliang Xiu, Andreas Geiger, and Anpei Chen. Easi3r: Estimating disentangled motion from dust3r without training. arXiv preprint arXiv:2503.24391, 2025. [5] Zhiwen Fan, Wenyan Cong, Kairun Wen, Kevin Wang, Jian Zhang, Xinghao Ding, Danfei Xu, Boris Ivanovic, Marco Pavone, Georgios Pavlakos, et al. Instantsplat: Unbounded sparse-view pose-free gaussian splatting in 40 seconds. arXiv preprint arXiv:2403.20309, 2024. [6] Yang Fu, Sifei Liu, Amey Kulkarni, Jan Kautz, Alexei A. Efros, and Xiaolong Wang. Colmap-free 3d gaussian splatting. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pages 2079620805, June 2024. [7] Hang Gao, Ruilong Li, Shubham Tulsiani, Bryan Russell, and Angjoo Kanazawa. Monocular dynamic view synthesis: reality check. Advances in Neural Information Processing Systems, 35:3376833780, 2022. [8] Derek Hoiem, Alexei Efros, and Martial Hebert. Automatic photo pop-up. In ACM SIGGRAPH 2005 Papers, pages 577584. 2005. [9] Youichi Horry, Ken-Ichi Anjyo, and Kiyoshi Arai. Tour into the picture: using spidery mesh interface to make animation from single image. In Proceedings of the 24th annual conference on Computer graphics and interactive techniques, pages 225232, 1997. [10] Binbin Huang, Zehao Yu, Anpei Chen, Andreas Geiger, and Shenghua Gao. 2d gaussian splatting for geometrically accurate radiance fields. In ACM SIGGRAPH 2024 Conference Papers, pages 111, 2024. [11] Linyi Jin, Richard Tucker, Zhengqi Li, David Fouhey, Noah Snavely, and Aleksander Holynski. Stereo4d: Learning how things move in 3d from internet stereo videos. arXiv preprint arXiv:2412.09621, 2024. [12] Nikita Karaev, Iurii Makarov, Jianyuan Wang, Natalia Neverova, Andrea Vedaldi, and Christian Rupprecht. Cotracker3: Simpler and better point tracking by pseudo-labelling real videos. arXiv preprint arXiv:2410.11831, 2024. [13] Nikita Karaev, Ignacio Rocco, Benjamin Graham, Natalia Neverova, Andrea Vedaldi, and Christian Rupprecht. Cotracker: It is better to track together. arXiv preprint arXiv:2307.07635, 2023. [14] Bernhard Kerbl, Georgios Kopanas, Thomas Leimkühler, and George Drettakis. 3d gaussian splatting for real-time radiance field rendering. ACM Trans. Graph., 42(4):1391, 2023. [15] Mijeong Kim, Seonguk Seo, and Bohyung Han. Infonerf: Ray entropy minimization for few-shot neural volume rendering. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 1291212921, 2022. [16] Johannes Kopf, Xuejian Rong, and Jia-Bin Huang. Robust consistent video depth estimation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 16111621, 2021. [17] Alex Krizhevsky, Ilya Sutskever, and Geoffrey Hinton. Imagenet classification with deep convolutional neural networks. Advances in neural information processing systems, 25, 2012. [18] Vincent Leroy, Yohann Cabon, and Jérôme Revaud. Grounding image matching in 3d with mast3r. In European Conference on Computer Vision, pages 7191. Springer, 2024. [19] Fang Li, Hao Zhang, and Narendra Ahuja. Self-calibrating 4d novel view synthesis from monocular videos using gaussian splatting. arXiv preprint arXiv:2406.01042, 2024. 11 [20] Yu-Lun Liu, Chen Gao, Andreas Meuleman, Hung-Yu Tseng, Ayush Saraf, Changil Kim, Yung-Yu Chuang, Johannes Kopf, and Jia-Bin Huang. Robust dynamic radiance fields. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 1323, 2023. [21] Ricardo Martin-Brualla, Noha Radwan, Mehdi SM Sajjadi, Jonathan Barron, Alexey Dosovitskiy, and Daniel Duckworth. Nerf in the wild: Neural radiance fields for unconstrained photo collections. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 72107219, 2021. [22] Andreas Meuleman, Yu-Lun Liu, Chen Gao, Jia-Bin Huang, Changil Kim, Min Kim, and Johannes Kopf. Progressively optimized local radiance fields for robust view synthesis. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 1653916548, 2023. [23] Ben Mildenhall, Pratul Srinivasan, Matthew Tancik, Jonathan Barron, Ravi Ramamoorthi, and Ren Ng. Nerf: Representing scenes as neural radiance fields for view synthesis. Communications of the ACM, 65(1):99106, 2021. [24] Michael Niemeyer, Jonathan Barron, Ben Mildenhall, Mehdi SM Sajjadi, Andreas Geiger, and Noha Radwan. Regnerf: Regularizing neural radiance fields for view synthesis from sparse inputs. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 54805490, 2022. [25] Keunhong Park, Utkarsh Sinha, Jonathan Barron, Sofien Bouaziz, Dan Goldman, Steven Seitz, and Ricardo Martin-Brualla. Nerfies: Deformable neural radiance fields. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 58655874, 2021. [26] Keunhong Park, Utkarsh Sinha, Peter Hedman, Jonathan Barron, Sofien Bouaziz, Dan Goldman, Ricardo Martin-Brualla, and Steven Seitz. Hypernerf: higher-dimensional representation for topologically varying neural radiance fields. arXiv preprint arXiv:2106.13228, 2021. [27] Adam Paszke, Sam Gross, Soumith Chintala, Gregory Chanan, Edward Yang, Zachary DeVito, Zeming Lin, Alban Desmaison, Luca Antiga, and Adam Lerer. Automatic differentiation in pytorch. 2017. [28] Jordi Pont-Tuset, Federico Perazzi, Sergi Caelles, Pablo Arbeláez, Alex Sorkine-Hornung, and Luc Van Gool. The 2017 davis challenge on video object segmentation. arXiv preprint arXiv:1704.00675, 2017. [29] Albert Pumarola, Enric Corona, Gerard Pons-Moll, and Francesc Moreno-Noguer. D-nerf: Neural radiance fields for dynamic scenes. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 1031810327, 2021. [30] Frano Rajiˇc, Lei Ke, Yu-Wing Tai, Chi-Keung Tang, Martin Danelljan, and Fisher Yu. Segment anything meets point tracking. arXiv preprint arXiv:2307.01197, 2023. [31] René Ranftl, Katrin Lasinger, David Hafner, Konrad Schindler, and Vladlen Koltun. Towards robust monocular depth estimation: Mixing datasets for zero-shot cross-dataset transfer. IEEE transactions on pattern analysis and machine intelligence, 44(3):16231637, 2020. [32] Johannes Schonberger and Jan-Michael Frahm. Structure-from-motion revisited. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 41044113, 2016. [33] Marek Simonik. Record3d point cloud animation and streaming, 2019. [34] Karen Simonyan and Andrew Zisserman. Very deep convolutional networks for large-scale image recognition. arXiv preprint arXiv:1409.1556, 2014. [35] Cameron Smith, David Charatan, Ayush Tewari, and Vincent Sitzmann. Flowmap: High-quality camera poses, intrinsics, and depth via gradient descent. arXiv preprint arXiv:2404.15259, 2024. [36] Jürgen Sturm, Nikolas Engelhard, Felix Endres, Wolfram Burgard, and Daniel Cremers. benchmark for the evaluation of rgb-d slam systems. In 2012 IEEE/RSJ international conference on intelligent robots and systems, pages 573580. IEEE, 2012. [37] Matthew Tancik, Ethan Weber, Evonne Ng, Ruilong Li, Brent Yi, Terrance Wang, Alexander Kristoffersen, Jake Austin, Kamyar Salahi, Abhik Ahuja, et al. Nerfstudio: modular framework for neural radiance field development. In ACM SIGGRAPH 2023 Conference Proceedings, pages 112, 2023. [38] Zachary Teed and Jia Deng. Raft: Recurrent all-pairs field transforms for optical flow. In Computer VisionECCV 2020: 16th European Conference, Glasgow, UK, August 2328, 2020, Proceedings, Part II 16, pages 402419. Springer, 2020. 12 [39] Zachary Teed and Jia Deng. Droid-slam: Deep visual slam for monocular, stereo, and rgb-d cameras. Advances in neural information processing systems, 34:1655816569, 2021. [40] Hengyi Wang and Lourdes Agapito. 3d reconstruction with spatial memory. arXiv preprint arXiv:2408.16061, 2024. [41] Jianyuan Wang, Minghao Chen, Nikita Karaev, Andrea Vedaldi, Christian Rupprecht, and David Novotny. Vggt: Visual geometry grounded transformer. arXiv preprint arXiv:2503.11651, 2025. [42] Jianyuan Wang, Nikita Karaev, Christian Rupprecht, and David Novotny. Vggsfm: Visual geometry grounded deep structure from motion. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 2168621697, 2024. [43] Peng Wang, Lingjie Liu, Yuan Liu, Christian Theobalt, Taku Komura, and Wenping Wang. Neus: Learning neural implicit surfaces by volume rendering for multi-view reconstruction. arXiv preprint arXiv:2106.10689, 2021. [44] Qianqian Wang, Yifei Zhang, Aleksander Holynski, Alexei Efros, and Angjoo Kanazawa. Continuous 3d perception model with persistent state. arXiv preprint arXiv:2501.12387, 2025. [45] Shizun Wang, Xingyi Yang, Qiuhong Shen, Zhenxiang Jiang, and Xinchao Wang. Gflow: Recovering 4d world from monocular video. arXiv preprint arXiv:2405.18426, 2024. [46] Shuzhe Wang, Vincent Leroy, Yohann Cabon, Boris Chidlovskii, and Jerome Revaud. Dust3r: Geometric 3d vision made easy. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 2069720709, 2024. [47] Guanjun Wu, Taoran Yi, Jiemin Fang, Lingxi Xie, Xiaopeng Zhang, Wei Wei, Wenyu Liu, Qi Tian, and Xinggang Wang. 4d gaussian splatting for real-time dynamic scene rendering. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 2031020320, 2024. [48] Qiangeng Xu, Zexiang Xu, Julien Philip, Sai Bi, Zhixin Shu, Kalyan Sunkavalli, and Ulrich Neumann. Point-nerf: Point-based neural radiance fields. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 54385448, 2022. [49] Chi Yan, Delin Qu, Dan Xu, Bin Zhao, Zhigang Wang, Dong Wang, and Xuelong Li. Gs-slam: Dense visual slam with 3d gaussian splatting. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 1959519604, 2024. [50] Zhiwen Yan, Chen Li, and Gim Hee Lee. Nerf-ds: Neural radiance fields for dynamic specular objects. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 82858295, 2023. [51] Gengshan Yang, Minh Vo, Natalia Neverova, Deva Ramanan, Andrea Vedaldi, and Hanbyul Joo. Banmo: In Proceedings of the IEEE/CVF Building animatable 3d neural models from many casual videos. Conference on Computer Vision and Pattern Recognition, pages 28632873, 2022. [52] Jianing Yang, Alexander Sax, Kevin Liang, Mikael Henaff, Hao Tang, Ang Cao, Joyce Chai, Franziska Meier, and Matt Feiszli. Fast3r: Towards 3d reconstruction of 1000+ images in one forward pass. arXiv preprint arXiv:2501.13928, 2025. [53] Ziyi Yang, Xinyu Gao, Wen Zhou, Shaohui Jiao, Yuqing Zhang, and Xiaogang Jin. Deformable 3d gaussians for high-fidelity monocular dynamic scene reconstruction. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 2033120341, 2024. [54] Lior Yariv, Jiatao Gu, Yoni Kasten, and Yaron Lipman. Volume rendering of neural implicit surfaces. Advances in Neural Information Processing Systems, 34:48054815, 2021. [55] Hao Zhang, Fang Li, Samyak Rawlekar, and Narendra Ahuja. Learning implicit representation for reconstructing articulated objects. arXiv preprint arXiv:2401.08809, 2024. [56] Junyi Zhang, Charles Herrmann, Junhwa Hur, Varun Jampani, Trevor Darrell, Forrester Cole, Deqing Sun, and Ming-Hsuan Yang. Monst3r: simple approach for estimating geometry in the presence of motion. arXiv preprint arXiv:2410.03825, 2024. [57] Qiang Zhang, Seung-Hwan Baek, Szymon Rusinkiewicz, and Felix Heide. Differentiable point-based radiance fields for efficient view synthesis. In SIGGRAPH Asia 2022 Conference Papers, pages 112, 2022. 13 [58] Zhoutong Zhang, Forrester Cole, Zhengqi Li, Michael Rubinstein, Noah Snavely, and William Freeman. Structure and motion from casual videos. In European Conference on Computer Vision, pages 2037. Springer, 2022. [59] Wang Zhao, Shaohui Liu, Hengkai Guo, Wenping Wang, and Yong-Jin Liu. Particlesfm: Exploiting dense point trajectories for localizing moving cameras in the wild. In European Conference on Computer Vision, pages 523542. Springer, 2022."
        },
        {
            "title": "A Dynamic Scene Optimization",
            "content": "Preliminary. As an alternative to NeRF [23], which has lengthy runtime, 3DGS [14] recently introduced new way to learn static scene representations in terms of explicit 3D Gaussian ellipsoids. Unlike the implicit representations of NeRF stored as the weights in the Convolutional Neural Network (CNN), 3DGS [14] uses explicit representations in 3D world coordinates and performs differential Gaussian rasterization on GPUs using CUDA which significantly speeds up computational efficiency. Each 3D Gaussian ellipsoid (x) is parameterized by its (1) Gaussian center R3; (2) quaternion factor R4; (3) opacity α R; (4) scaling factor R3; and (5) color Rk (k denotes degrees of freedom), and represented by: G(x) = e1/2(xµ)T Σ1(xµ) Σ = JWΣWT JT , Σ = rssT rT (9) (10) where Σ is the 3D covariance matrix in the world space, and are the view transformation matrix and the Jacobian matrix of the affine transformation parts, respectively, of the projective transformation, and Σ is the covariance matrix in the camera coordinates. The color is rendered by: C(p) = (cid:88) kK ckαkΠj1 (1 αk) (11) where ck and αk represent the spherical harmonic (SH) coefficient and the density at this point. 4D GS. To take advantage of its optimization efficiency, we use 4DGS [47] to learn dynamic scene representations. With different camera estimates and the same scene optimization method [47], we use NVS performance to evaluate the accuracy of camera parameter estimates. In 4DGS[47], the NVS performance depends on how well the canonical representations and deformation representations are optimized. The canonical (refers to mean as in the previous work [14, 53, 47]) representations are learned by canonical Gaussian field to optimize the mean (canonical) position R3, color Rk, opacity α R, quaternion factor R4, scaling factor R3, and the deformation representations are optimized using deformation field [47] to learn the offsets G, supervised by an L1 loss between images and renderings. Since the color and opacity of the Gaussian ellipsoids do not change over time, the deformed attributes consist of (X , r, s) = (X + , + r, + s). More details can be found in 4DGS [47]. Derivation of Cauchy Negative-log-likelihood The Cauchy loss function is derived from Equation (12) to Equation (14). Since we use the Cauchy probability density function (PDF) to model the uncertainty of calibration points Pcali, we want to maximize the likelihood of the Cauchy PDF: (x; x0, Γ) = 1 πΓ[1 + ( xx0 Γ )2] , Γ > 0 (12) Equivalently, we minimize the negative-log-likelihood of (x; x0, Γ), to define the loss function:: NLL(x; x0, Γ) = log(f (x; x0, Γ)) = log(πΓ) + log(1 + ( x0 Γ )2) = log[π (Γ + = log π + log(Γ + )] (x x0)2 Γ (x x0)2 Γ (13) ) where log π and x0 denote constant term and the ground truth which can be omitted. Thus, our objective is as follows: min x,Γ NLL(x; x0, Γ) = min x,Γ log(Γ + (x x0)2 Γ ) (14)"
        },
        {
            "title": "C Datasets",
            "content": "To demonstrate our performance on broader range of scenarios, we have conducted extensive experiments across five public datasets - NeRF-DS [50], DAVIS [28], iPhone [7], MPI-Sintel [2], and TUM-dynamics [36]. These videos contain different camera and object motion patterns, and different texture levels. The lengths of the videos range from about 50 to 900. Regarding the train/test split of the NVS evaluation, for every 2 adjacent frames, we take the first frame for training and the second frame for testing. For the setup of camera pose evaluation, we follow Cut3r [44] and Monst3r [56] in the experiments on TUM-dynamics and evaluate all videos in MPI-Sintel [2]. NeRF-DS. NeRF-DS [50] dataset includes seven long monocular videos (400-800 frames) of different dynamic, real-world indoor scenarios with little blur. Each video has at least one specular moving object against mix of low-texture and high-texture backgrounds. NeRF-DS [50] exhibits large scene and camera movements, so the frames have some blur. The GT motion masks provided are human-labeled and the camera parameters are estimated by COLw/ mask. Like previous works [50, 53], we take the highest resolution images available (480 270) as the RGB input in all experiments. DAVIS. DAVIS [28] dataset contains 40 short monocular videos that capture different dynamic scenes in the wild. Each video has 50-100 frames, including at least one dynamic object. The GT motion masks are also provided as in NeRF-DS [50]. However, like [20, 45, 58, 59], we exclude some videos using fixed cameras, changeable focal lengths, etc. Different from others [20, 45, 58, 3] which only show experiments of about 10 videos in DAVIS [28], we conduct experiments on 21 videos containing large camera and object movements. We utilize the RGB frames with the resolution of 854 480 as input. iPhone. The iPhone [7] dataset is an extremely challenging dataset (180-475 frames) with significant camera rotations and translations, and rapid movements of objects. There are 14 monocular videos including indoor and outdoor scenes and no GT motion mask is provided. It would also be difficult to insert motion masks for this dataset because there is no clear boundary between the moving and stable regions within any frame. They represent real-world casually recorded videos. We conduct experiments on all of them. The frame size is 720 960. These videos are recorded by the Record3D [33] app on iPhone which uses LiDAR sensors to obtain metric depth for camera estimation. In our comparisons with the camera estimates provided by Record3D, we also take the Record3D app as one of the baselines and compare with it. MPI-Sintel. MPI-Sintel [2] is synthetic dataset provided GT camera parameters. It has 18 short videos (about 50 frames) in total containing large object movement. In some cases, the moving objects cover most of the screen. Most of the existing works [20, 56, 3] select 14 videos for evaluation, but in this paper, we evaluate the methods among all the videos. The synthetic MPI-Sintel dataset exhibits domain gaps compared to real-world scenarios, which is considered to be one of the reasons why some existing methods [58, 59] perform well on MPI-Sintel, but do not work efficiently on other real-world datasets. In experiments, we take the frames with default sizes as the input the our method, while keeping the default resizing setup of the other methods. TUM-dynamics. TUM-dynamics [36] dataset contains 8 long real-world blurry videos recording the dynamic indoor scenes provided with GT camera parameters. However, although the videos in this dataset are indoor scenes, each video features significant depth of field. TUM-dynamics dataset also contains large camera movement and rapid object movement. We follow the experimental setup of MonST3R [56] on this dataset, which is sampling the first 90 frames with the temporal stride of 3 to save compute."
        },
        {
            "title": "D Evaluation Metrics",
            "content": "As discussed in section 4 of the main paper, we directly conduct camera pose evaluation against the GT on MPI-Sintel[2] and TUM-dynamics [36], using the standard metrics: ATE, RPE trans, and RPE rot. Besides, regarding NeRF-DS [50], DAVIS [28], and iPhone [7] datasets which are not provided with GT camera parameters, we conduct NVS evaluation with standard metrics: PSNR, SSIM, and LPIPS. We also employ time evaluation to demonstrate the superior time efficiency of our method. 16 D.1 PSNR & SSIM & LPIPS PSNR. PSNR is measure of the ratio between the maximum possible power of signal and the power of corrupting noise that affects the fidelity of its representation. PSNR is commonly used to compare the qualities of the original and the rendered images, and is obtained from the Mean Square Error (MSE) between the original and the rendered images: PNSR = 10 log10 (cid:18) MAX2 MSE(ImageRendered, ImageGT) (cid:19) (15) , where MAX is the maximum pixel value of the image. SSIM. SSIM measures the similarity between two images based on structural information. Its evaluation involves luminance, contrast, and structure. Compared to PSNR, SSIM is intended to match human perception more closely. The SSIM values range from -1 to 1, where 1 denotes perfect. It is given as: (2µxµy + c1)(2σxy + c2) + µ2 where and are two images, µx, µy and σ2 x, σ2 and y, σxy represents the covariance of and y, and c1 and c2 denote the regularization terms. + c1)(σ2 are the corresponding averages and variances of + c2) SSIM = + σ2 (µ2 (16) LPIPS. LPIPS measures perceptual similarity in terms of features of deep neural networks, such as pre-trained VGG [34] or AlexNet [17]. It compares feature activations of image patches. Like [14, 53, 47, 19], we here use the VGG-based networks. D.2 ATE & RPE trans & RPE rot ATE. ATE quantifies the difference between the actual trajectory and the estimated trajectory of robot or camera over time, offering global measure of error along the entire path. It is calculated by aligning the estimated trajectory with the ground truth and then measuring the Euclidean distance between each corresponding point on the two trajectories. RPE trans. RPE trans quantifies the error in the translational component between consecutive poses or over fixed time/distance interval. Unlike ATE, which assesses the overall trajectory, RPE Trans emphasizes the local accuracy of the motion estimation by evaluating how well the system preserves the relative motion between two points in time or space. RPE rot. RPE rot quantifies the error in the orientation component between the estimated poses and the ground truth. This metric is computed by measuring the difference in orientation over short sequences, and it is typically expressed in angular units, such as degrees or radians."
        },
        {
            "title": "E More Results",
            "content": "E.1 Quantitative Results E.1.1 Runtime We report the detailed runtime comparisons on the NeRF-DS [50], DAVIS [28], and iPhone [7] datasets, each containing over 50 frames, where runtime differences become more pronounced. Specifically, in Table 9 and Table 10, the runtime of our method is the shortest. In addition, in Table 11, on the NeRF-DS [50] dataset, the runtime of COLw/ mask and COLw/o mask on Plate video is shorter than of ours. This is because COLw/ mask and COLw/o mask fail on this video, leading to quick convergence to the local minima. Such conclusion can also supported by qualitative results of fig. 8 in the main paper. E.2 Qualitative Results E.2.1 Trajectories from Patch-wise Tracking Filters as Pseudo-supervision In Figure 10, we show the trajectory comparisons on the NeRF-DS [50] dataset as samples. As discussed in the 3rd paragraph in section 1, and section 3.1 of the main paper, our patch-wise tracking 17 Table 9: Quantitative Runtime Results on DAVIS [28]. Cam camera optimization time; Cam+Scene overall (camera+scene) optimization time; hour; minute. We mark the shortest time in bold. Our method is the most efficient without any failure. Method Camel Bear Breakdance-flare Car-roundabout Car-shadow Car-turn Cows Dog Dog-agility Goat Hike Horsejump-high Lucia Motorbike Parkour Rollerblade Tennis Train Mean Ours COLw/o mask casualSAM Cam Cam+Scene Cam Cam+Scene 1.57m 3.15m 1.73m 4.97m 0.93m 2.97m 2.85m 1.60m 0.67m 2.10m 4.20m 1.97m 1.97m 2.08m 9.07m 1.25m 3.47m 1.90m 2.68m 25.28m 1.08h 0.92h 21.95m 17.88m 17.35m 22.50m 11.70m 26.63m 18.95m 31.83m 21.20m 22.75m 18.77m 26.65m 17.08m 17.03m 18.22m 21.02m 40m 56m FAIL 10m 5m FAIL 73m 10m FAIL 107m FAIL 5m 44m 6m 16m FAIL 9m 32m 31m 71m 88m - 41m 31m - 84m 53m - 124m - 33m 65m 31m 37m - 30m 57m 56m Cam 24m 20m 15m 18m 10m 27m 26m 12m 5m 22m 20m 10m 16m 9m 27m 8m 17m 19m 17m Cam+Scene 46m 39m 34m 46m 36m 49m 48m 31m 31m 44m 42m 31m 36m 32m 48m 27m 38m 44m 39m Table 10: Quantitative Runtime Results on iPhone [7]. Cam camera optimization time; Cam+Scene overall (camera+scene) optimization time; hour; minute. We mark the shortest time in bold. Our method is the most efficient. Method Apple Paper-windmill Space-out Backpack Block Creeper Handwavy Haru-sit Mochi-high-five Pillow Spin Sriracha-tree Teddy Wheel Mean Cam 33m 15m 23m 7m 27m 27m 15m 10m 6m 21m 30m 15m 31m 27m 20m Ours Cam+Scene COLw/o mask casualSAM Cam Cam+Scene Cam Cam+Scene 11.25h 8.70h 4.72h 2.12h 10.45h 16.45h 5.1h 1.18h 1.03h 19.70h 20.58h 4.88h 19.71h 13.80h 9.97h 6.80h 3.50h 5.87h 1.58h 6h 4.38h 3.30h 1.25h 1.53h 3.70h 5.50h 3.22h 6.78h 3.67h 4.07h 7.32h 3.97h 6.30h 2h 6.50h 4.82h 3.72h 1.72h 1.93h 4.20h 6h 3.68h 7.28h 4.17h 4.53h 47m 27m 69m 25m 42m 45m 30m 25m 18m 36m 44m 27m 46m 46m 38m 10.95h 8.18h 4.42h 1.83h 10h 16.03h 4.62h 0.77h 0.67h 19.18h 20h 4.58h 19h 6m 9.53h 18 Table 11: Quantitative runtime results on NeRF-DS [50]. Cam camera optimization time; Cam+Scene overall (camera+scene) optimization time; hour; minute. We mark the shortest time in bold. we show only Cam+Scene of RoDynRF [20] due to its joint optimization of the camera and scene. is supervised by additional GT priors. COLw/mask and COLw/omask are faster than us on Plate because they fail on this video, leading to quick convergence to the local minima, which can be supported by qualitative results of fig. 8 in the main paper. Among all, our method is the most efficient. Method Bell As Basin Plate Press Cup Sieve Mean Cam 1.05h 0.95h 0.75h 0.53h 0.68h 1.02h 0.78h 0.83h Ours Cam+Scene COLw/ mask COLw/o mask casualSAM Cam Cam+Scene Cam Cam+Scene Cam Cam+Scene RoDynRF Cam+Scene 1.20h 1.08h 0.92h 0.68h 0.82h 1.15h 0.92h 0.97h 2.50h 2.00h 1.42h 0.42h 0.85h 2.37h 1.15h 1.52h 2.72h 2.17h 1.62h 0.60h 1.05h 2.58h 1.35h 1.73h 3.00h 2.55h 1.60h 0.50h 0.90h 2.57h 1.58h 1.82h 3.25h 2.72h 1.85h 0.87h 1.08h 2.73h 1.77h 2.03h 16.5h 14.87h 9.88h 4.67h 6.28h 13.50h 7.83h 10.50h 16.8h 15.08h 10.67h 4.98h 6.60h 13.78h 8.13h 10.80h 28.6h 33.6h 33.8h 25.6h 28.5h 28.8h 28.3h 29.6h filters can establish robust and maximally sparse hinge-like relations as accurate pseudo-supervision, avoiding noisy and inaccurate tracking trajectories. In Figure 10, it is easy to see our proposed method avoids the inaccurate ones in the low-texture regions (walls), and meanwhile, adaptively adds new reliable trajectories when the number of left trajectories on each frame is less than B. E.2.2 NVS We show more RGB and depth rendering results on NeRF-DS [50], DAVIS [28], and iPhone [7] dataset in Figure 11, Figure 12, Figure 13, Figure 14, Figure 15, Figure 16, Figure 17, and Figure 18. It is easy to see that the RGB and depth rendering results of our method are better than other RGB-only supervised approaches. In addition, the performance of our method is also comparable with that of the LiDAR-based Record3D app. E.2.3 Optimized 3D Gaussian Fields Since the iPhone [7] dataset is the most challenging dataset with large camera and object movements, we show more visualizations of optimized 3D Gaussian fields in Figure 20, Figure 21, and Figure 19. Such comparisons demonstrate that our camera estimates enable superior reconstruction of 3D Gaussian fields compared to other RGB-only supervised approaches. Moreover, the reconstructed fields using our estimates are comparable to, or even surpass, those obtained with the LiDAR-based Record3D [33] app, particularly in scenes with significant motion. 19 Figure 10: Trajectory Comparisons on the NeRF-DS [50] Dataset. In each scenario, top row F0; bottom row F247; w/o patch-wise filters raw CoTracker [13]; w/ patch-wise filters Ours. It is easy to see our proposed method avoids the inaccurate trajectories in the low-texture regions, whereas the trajectories of the points in the low-texture regions tracked by raw CoTracker [13] are extremely unreliable. 20 Figure 11: More Qualitative NVS Results on iPhone [7] - Part 1. Our renderings exhibit higher fidelity and more accurate geometry compared to other RGB-only supervised methods. Besides, our performance is comparable with, or even better than, the ones of the LiDAR-based Record3D app. 21 Figure 12: More Qualitative NVS Results on iPhone [7] - Part 2. Our renderings exhibit higher fidelity and more accurate geometry compared to other RGB-only supervised methods. Besides, our performance is comparable with, or even better than, the ones of the LiDAR-based Record3D app. 22 Figure 13: More Qualitative NVS Results on iPhone [7] - Part 3. Our renderings exhibit higher fidelity and more accurate geometry compared to other RGB-only supervised methods. Besides, our performance is comparable with, or even better than, the ones of the LiDAR-based Record3D app. Figure 14: More Qualitative NVS Results on DAVIS [28] - Part 1. Our renderings exhibit higher fidelity compared to other RGB-only supervised methods. 23 Figure 15: More Qualitative NVS Results on DAVIS [28] - Part 2. Our renderings exhibit higher fidelity compared to other RGB-only supervised methods. Figure 16: More Qualitative NVS Results on DAVIS [28] - Part 3. Our renderings exhibit higher fidelity compared to other RGB-only supervised methods. Figure 17: More Qualitative NVS Results on DAVIS [28] - Part 4. Our renderings exhibit higher fidelity compared to other RGB-only supervised methods. 24 Figure 18: More Qualitative NVS Results on NeRF-DS [50]. Our renderings exhibit higher fidelity compared to other RGB-only supervised methods. 25 Figure 19: Optimized 3D Gaussian Fields on iPhone [7] - Part 1. Our reconstructed 3D Gaussian Fields are more geometrically accurate compared to the ones of other RGB-only supervised methods, which demonstrates our camera estimates are more accurate. Besides, our performance is comparable with, or even better than, the ones of the LiDAR-based Record3D app. Figure 20: Optimized 3D Gaussian Fields on iPhone [7] - Part 2. Our reconstructed 3D Gaussian Fields are more geometrically accurate compared to the ones of other RGB-only supervised methods, which demonstrates our camera estimates are more accurate. Besides, our performance is comparable with, or even better than, the ones of the LiDAR-based Record3D app. 27 Figure 21: Optimized 3D Gaussian Fields on iPhone [7] - Part 3. Our reconstructed 3D Gaussian Fields are more geometrically accurate compared to the ones of other RGB-only supervised methods, which demonstrates our camera estimates are more accurate. Besides, our performance is comparable with, or even better than, the ones of the LiDAR-based Record3D app."
        }
    ],
    "affiliations": [
        "University of Illinois at Urbana-Champaign"
    ]
}