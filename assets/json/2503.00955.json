{
    "paper_title": "SemViQA: A Semantic Question Answering System for Vietnamese Information Fact-Checking",
    "authors": [
        "Nam V. Nguyen",
        "Dien X. Tran",
        "Thanh T. Tran",
        "Anh T. Hoang",
        "Tai V. Duong",
        "Di T. Le",
        "Phuc-Lu Le"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "The rise of misinformation, exacerbated by Large Language Models (LLMs) like GPT and Gemini, demands robust fact-checking solutions, especially for low-resource languages like Vietnamese. Existing methods struggle with semantic ambiguity, homonyms, and complex linguistic structures, often trading accuracy for efficiency. We introduce SemViQA, a novel Vietnamese fact-checking framework integrating Semantic-based Evidence Retrieval (SER) and Two-step Verdict Classification (TVC). Our approach balances precision and speed, achieving state-of-the-art results with 78.97\\% strict accuracy on ISE-DSC01 and 80.82\\% on ViWikiFC, securing 1st place in the UIT Data Science Challenge. Additionally, SemViQA Faster improves inference speed 7x while maintaining competitive accuracy. SemViQA sets a new benchmark for Vietnamese fact verification, advancing the fight against misinformation. The source code is available at: https://github.com/DAVID-NGUYEN-S16/SemViQA."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 2 ] . [ 1 5 5 9 0 0 . 3 0 5 2 : r SemViQA: Semantic Question Answering System for Vietnamese Information Fact-Checking Nam V. Nguyen1,2,*, Dien X. Tran2,*, Thanh T. Tran2, Anh T. Hoang2,4, Tai V. Duong2, Di T. Le2, Phuc-Lu Le3 1FPT Software AI Center, Viet Nam 2Faculty of Information Technology, Industrial University of Ho Chi Minh City, Viet Nam 3Faculty of Information Technology, University of Science, VNU-HCM, Viet Nam 4FPT Telecom, Viet Nam Correspondence: lplu@fit.hcmus.edu.vn"
        },
        {
            "title": "Abstract",
            "content": "The rise of misinformation, exacerbated by Large Language Models (LLMs) like GPT and Gemini, demands robust fact-checking solutions, especially for low-resource languages like Vietnamese. Existing methods struggle with semantic ambiguity, homonyms, and complex linguistic structures, often trading accuracy for efficiency. We introduce SemViQA, novel Vietnamese fact-checking framework integrating Semantic-based Evidence Retrieval (SER) and Two-step Verdict Classification (TVC). Our approach balances precision and speed, achieving state-of-the-art results with 78.97% strict accuracy on ISE-DSC01 and 80.82% on ViWikiFC, securing 1st place in the UIT Data Science Challenge. Additionally, SemViQA Faster improves inference speed 7 while maintaining competitive accuracy. SemViQA sets new benchmark for Vietnamese fact verification, advancing the fight against misinformation. The source code is available at: https://github. com/DAVID-NGUYEN-S16/SemViQA."
        },
        {
            "title": "Introduction",
            "content": "The rapid advancement of large language models (LLMs), such as OpenAIs ChatGPT, Google Gemini (Team et al., 2024), Llama3.1 (Touvron et al., 2023), Qwen2.5 (Qwen et al., 2025), DeepSeek V3, (DeepSeek-AI et al., 2024), Phi3.5 (Abdin et al., 2024) has significantly improved information retrieval and processing across various domains. However, major challenge with these systems is their tendency to generate factually incorrect or hallucinated content seemingly plausible information that lacks factual grounding (Soleimani et al., 2020). This issue is particularly critical in domains requiring high accuracy, such as healthcare, law, and journalism, where misinformation can have *Equal contribution. Preprint 1 serious consequences. Consequently, developing reliable fact-checking systems capable of retrieving and evaluating evidence from real-world sources has become an urgent need in Natural Language Processing (NLP). Although fact verification has been extensively studied in high-resource languages like English, with primary focus on datasets such as FEVER (Thorne et al., 2018), LIAR (Wang, 2017), PubHealthTab (Akhtar et al., 2022), and TabFact (Chen et al., 2020), extending these methods to low-resource languages like Vietnamese remains significant challenge. Transformer-based approaches, including BERT (Devlin et al., 2019) and RoBERTa (Liu et al., 2019), have shown strong performance in fact verification, yet their adaptation to Vietnamese is still underexplored. For Vietnamese fact verification, ViNSV (Tran et al., 2024) employs BM25 and SBERT (Reimers and Gurevych, 2019a) for evidence retrieval. However, SBERTs 256-token input limitation leads to information loss in long-context claims, making it particularly ineffective for complex cases requiring multi-sentence reasoning or cross-referencing distant evidence. As result, the system struggles with hard samples that demand deeper contextual understanding, ultimately reducing fact verification accuracy. In contrast, graph-based reasoning methods in English (Zhong et al., 2020) have shown notable potential for semantic inference but are often hindered by high computational costs, restricting their real-world applicability. Moreover, traditional retrieval techniques like TF-IDF and BM25, though widely used, rely heavily on token frequency and exact keyword matching, making them less effective at capturing the nuanced semantics of complex claims. While some recent work employing large language models (LLMs) (Huo et al., 2023; Schimanski et al., 2024) has shown promise in addressing hard retrieval questions, these approaches typically require considerably more computing resources and longer processing times compared to conventional methods. Thus, trade-off persists: models offering high accuracy tend to operate slowly, whereas faster models often compromise on accuracy. To address the challenges in fact verification, we introduce SemViQA, system designed to optimize both accuracy and efficiency. It comes in two variants: SemViQA Standard, which prioritizes accuracy, and SemViQA Faster, which significantly improves inference speed while maintaining strong performance. The system is built upon three core components: 1. Data Processing: specialized preprocessing pipeline designed to efficiently handle long-token sequences, known limitation of Transformer-based models. Our approach preserves the semantic integrity of input claims while ensuring optimal tokenization for downstream processing. 2. Semantic-based Evidence Retrieval (SER): hybrid retrieval framework combining TFIDF for efficient keyword-based matching and Question Answering Token Classifier (QATC) for semantic-based evidence selection. This approach balances speed and accuracy by prioritizing TF-IDF for common cases while activating QATC selectively for hard samples, significantly improving precision while maintaining computational efficiency. 3. Two-step Verdict Classification (TVC): hierarchical classification strategy leveraging Focal Loss (Lin et al., 2018) and CrossEntropy Loss to enhance claim verification accuracy. This approach first determines whether claim is Supported, Refuted, or Not Enough Information (NEI) and subsequently refines the decision boundary for hard-toclassify cases, reducing misclassification and improving robustness. We evaluate SemViQA on two of the largest Vietnamese fact verification datasets, ISE-DSC01 and ViWikiFC (Le et al., 2024). Our system achieves 78.97% strict accuracy on ISE-DSC01 1 and 80.82% on ViWikiFC, outperforming existing baselines. These results highlight SemViQAs 1https://codalab.lisn.upsaclay.fr/competitions/ 15497#results potential to improve the accuracy of fact verification in Vietnamese, contributing to misinformation mitigation and enhancing transparency in digital content. The remainder of this paper is structured as follows: Section 2 discusses related work. Section 3 details our proposed methodology, including SER, TVC, and QATC. Section 4 presents experimental results and performance evaluation. Finally, Section 5 concludes the paper and outlines future research directions."
        },
        {
            "title": "2 Related Works",
            "content": "Advancements in Natural Language Processing (NLP) have significantly improved information verification and evidence extraction. Early methods, such as the Neural Semantic Matching Network (NSMN) (Nie et al., 2018), leveraged BiLSTM (Graves and Schmidhuber, 2005) and WordNet-based features to enhance verification accuracy but struggled with complex sentence relations due to LSTMs sequential nature. The introduction of transformer models, notably BERT (Devlin et al., 2019), transformed claim verification by enabling bidirectional contextual encoding (Soleimani et al., 2020; Zhou et al., 2019; Malon, 2018). While achieving state-of-the-art results on FEVER (Aly et al., 2021; Lin et al., 2024; Yuan and Vlachos, 2024; DeHaven and Scott, 2023), BERTs input length limitation makes it less effective for long-context fact-checking. Graph-based models have further advanced multi-step reasoning. Approaches such as GCNbased reasoning (Zhong et al., 2020; Thorne et al., 2018) and AdMIRaLs logic-driven document retrieval (Aly and Vlachos, 2022) improve evidence sufficiency but demand high computational resources. For Vietnamese fact verification, research remains limited. ViNSV (Tran et al., 2024) combined BM25 and SBERT (Reimers and Gurevych, 2019a) for evidence retrieval but struggled with complex reasoning due to static embeddings. Building on prior work, SemViQA optimizes fact verification by integrating fast keywordbased retrieval (TF-IDF) with semantic reasoning (QATC) and hierarchical classification strategy, ensuring high accuracy, fast inference, and scalability for real-world applications. Figure 1: SemViQA: Three-Stage Method for semantic-based evidence retrieval and two-step verdict classification, where P2 and P3 represent the probabilities of the two-class and three-class classifications, respectively, and ˆy2 and ˆy3 denote their corresponding predictions. Figure 2: Graph representing the lengths of contexts."
        },
        {
            "title": "Question Answering",
            "content": "Our proposed method, SemViQA, employs threestage pipeline to enhance fact verification, as illustrated in Figure 1. The process begins with preprocessing stage, where input data is restructured to ensure compatibility with retrieval and classification models. Subsequently, evidence retrieval is performed using TF-IDF for straightforward cases, while the Question Answering Token Classifier (QATC) refines evidence selection for more complex claims. Finally, in the claim classification stage, hierarchical approach integrates Cross-Entropy Loss and Focal Loss (Lin et al., 2018) to improve both accuracy and robustness."
        },
        {
            "title": "3.1 Data Processing",
            "content": "Before implementing data processing solutions, we conducted thorough analysis of the input data to understand its characteristics and challenges. One of the prominent issues we encountered is the token limit of current Vietnamese BERT models, as clearly illustrated in Figure 2. 3 Figure 3: Long context processing solution. During data processing, we observed that the input context has considerable length, with the longest context extending up to 4803 tokens. This presents challenge for data processing given the limitations of 256 tokens for classification and 512 tokens for the Question Answering model. For the classification data: We start by segmenting the context into smaller passages and removing special characters such as .,!?n. We use the Underthesea library to tokenize the sentences. The tokens are then concatenated to form new phrases. For the input data for the Question Answering model: We use pairs of claim and context. Due to the large length of the context, solution for processing is necessary. First, we preprocess the data to remove newline characters (n). However, for the Question Answering model, punctuation marks are not removed as was done in the classification process. This is based on experimental results, which repeatedly show that removing punctuation negatively impacts the quality of the Question Answering models results. From long context, we proceed to split it into smaller segments. We then slide through each segment and check the number of tokens in each segment. If the token count does not exceed 400, we append the segment to the subcontext. When the subcontext has token count greater than 400 (see Figure 3), we check if the evidence sentence is contained within the subcontext. If it is, then the subcontext is considered to contain that evidence. Otherwise, if it is not present, the evidence for that subcontext is assigned as an empty string. This process allows us to create new data samples with contexts containing fewer than 512 tokens, which is beneficial for training the model more effectively. This ensures that no segment is omitted, unlike stride-based cutting methods, which may result in evidence sentences being split between segments within complete paragraph."
        },
        {
            "title": "3.2 Semantic-based Evidence Retrieval (SER)",
            "content": "In addressing this problem, focusing on evidence retrieval is crucial and decisive part. Accurate classification can only be performed when the evidence is correct. After analyzing and training on the data, we found that methods like TF-IDF and BM25 are effective for simpler samples but struggle with more complex samples. Therefore, we have divided the evidence retrieval process into two parts: Part 1: Using TF-IDF: We begin by segmenting the context into smaller passages and pairing each segment with the claim to generate structured data samples, which then undergo preprocessing pipeline including noise removal and tokenization using ViTokenizer 2 to standardize the input. key observation is that TF-IDF performs well on simple claims, particularly those classified as refuted, but struggles with claims requiring deeper reasoning, as it relies solely on keyword matching without capturing semantic relationships. To address this limitation, we introduce an enhanced strategy to enrich contextual information, where segments containing fewer than 60% of the claims token count are merged with the preceding segment to supplement missing information. This is particularly beneficial for short or incomplete sentences where crucial words may be absent from the claim, leading to improved evidence retrieval accuracy. After retrieving potential evidence using TF-IDF, we rank the retrieved segments and apply Confidence Threshold to classify them into easy and challenging cases, as detailed in Appendix C. While simple cases can be resolved directly using 2https://github.com/trungtv/pyvi TF-IDF, more complex cases are forwarded to Part 2 for further analysis, ensuring higher classification accuracy. Part 2: Using the Question Answering Token Classifier Model: The subcontext is also used for model QA inference instead of inferring the full context. In this process, three situations can occur: The first, when multiple subcontexts generate different responses, we interpret this as the model exhibiting confusion in handling evidence responses. In this case, we collect all retrieved evidence and apply TF-IDF to select the most appropriate one. The second, when only one piece of evidence appears across multiple subcontexts, we interpret this as the model being highly confident in its prediction. The third, when the model does not retrieve any evidence, we use TF-IDF to extract relevant evidence as input for our TVC method. By combining these methods, we have achieved the best results in accurately and reliably retrieving evidence."
        },
        {
            "title": "3.3 Two-step Verdict Classification (TVC)",
            "content": "Our input data consists of claim-evidence pairs, where the claim is the statement that needs to be verified, and the evidence comprises information used to support (supported), refute (refuted), or indicate insufficient information (NEI) regarding the claim. To address this problem, we undertake two main tasks as follows: Three-Class Classification: The first step involves categorizing each claim into one of three categories: Supported, Refuted, or Not Enough Information (NEI). Given claim-evidence pair (C, E), we employ classification function: P3 = fThree-Class(C, E), (1) where P3 = [PSUP, PREF, PNEI] represents the predicted probabilities. Here, fThree-Class denotes neural network model based on an advanced BERT-based architecture, trained to classify each claim into one of the three categories. The model leverages fine-tuned BERTderived approach and is optimized using Cross Entropy Loss, ensuring that the predicted probability distribution closely aligns with the true labels. Binary Classification: If the three-class model does not predict NEI, an additional classification step is applied to further refine the decision between Supported and Refuted. In this step, we use another classification function: 4 P2 = fBinary-Class(C, E), (2) where P2 = [PSUP, PREF] . Here, fBinary-Class is neural network based on an advanced BERT-based architecture, specifically trained to distinguish between Supported and Refuted claims. Unlike the three-class model, this binary classifier leverages fine-tuned BERT-derived model and is trained using Focal Loss (Lin et al., 2018) to mitigate class imbalance and improve performance on challenging examples. Finally, the predicted class ˆy is determined based on the following decision rule: ˆy = NEI, ˆy2, ˆy3, if ˆy3 = NEI if P2 > P3 and ˆy3 = ˆy2 if P3 P2 or ˆy3 = ˆy2. (3) This method effectively balances the strengths of both classification models, ensuring improved accuracy for complex claims while maintaining high performance on straightforward cases."
        },
        {
            "title": "3.4 Question Answering Token Classifier",
            "content": "(QATC) Typical QA models are trained only to predict the start and end positions of an answer. Here, we extend this by introducing Token Classification task, where we assign label of 1 to tokens within the answer and 0 otherwise. This enhances the models ability to identify relevant tokens for answer extraction. Procedure: We input the context and claim into the model, extract token features, and apply two linear layers to predict answer start and end probabilities. The default QA loss is the Cross Entropy loss: (cid:88) LCE = ti log(pi), (4) i=1 where is the number of classes, ti is the true label, and pi is the softmax probability for the ith class."
        },
        {
            "title": "3.4.1 Token Classification Task\nInspired by Rationale Tagging (Ju et al., 2019), we\npredict whether each token is part of the answer (la-\nbel 1) or not (label 0). For NEI (Not Enough Infor-\nmation) cases, all tokens are labeled 0. To achieve",
            "content": "5 this, we introduce an additional fully connected (FC) layer that computes classification probabilities: = σ(cid:0)w2 ReLU(W1ht)(cid:1), pr where ht is the output representation of token from RoBERTa (Zhuang et al., 2021; Aly et al., 2021). The loss for this task is the Binary Cross Entropy loss: (5) LRT ="
        },
        {
            "title": "1\nT",
            "content": "T (cid:88) (cid:2)yr log(pr )+(1yr ) log(1pr )(cid:3). t=1 (6)"
        },
        {
            "title": "3.4.2 Rationale Regularization Loss\nTo encourage sparsity and continuity in rationale\npredictions, we introduce a Rationale Regulariza-\ntion Loss LRR, which consists of:",
            "content": "1. Sparsity Loss: Encourages the model to minimize the number of tokens selected as rationales: Lsparse ="
        },
        {
            "title": "1\nT",
            "content": "T (cid:88) t=1 mtpr , (7) where mt is mask ensuring that the loss is computed only on non-padding tokens. 2. Continuity Loss: Encourages smoother rationale selection by minimizing sudden jumps between selected tokens: Lcont ="
        },
        {
            "title": "1\nT − 1",
            "content": "T 1 (cid:88) t=1 mtmt+1(pr pr t+1)2. (8) The total rationale regularization loss is: LRR = λsparseLsparse + λcontLcont. (9)"
        },
        {
            "title": "3.4.3 Combined Loss Function\nThe final objective function integrates all compo-\nnents:",
            "content": "L = LCE + LRR + αLRT , (10) where α controls the weight of the rationale tagging loss, and λsparse, λcont are hyperparameters for regularization."
        },
        {
            "title": "4.1 Dataset",
            "content": "ISE-DSC01, used in the UIT Challenge 2023, is designed for AI evaluation in information extraction and verification. It comprises training, public test, and private test sets with diverse claim-context pairs, posing challenges in evidence retrieval and fact-checking. ViWikiFC (Le et al., 2024) was developed to combat misinformation, featuring over 20,000 claims from Wikipedia. Unlike ISE-DSC01, it explicitly includes evidence for the nei (Not Enough Information) label, enhancing real-world applicability. ISE-DSC01 ViWikiFC"
        },
        {
            "title": "Train\nDev\nTest",
            "content": "37,967 4,794 5,396 16,738 2,090 2,091 Table 1: Overview of the datasets used in our experiments Table 1 summarizes the datasets. For ViWikiFC, experiments were conducted on all three sets: training, public test, and private test."
        },
        {
            "title": "4.2 Experimental Setup",
            "content": "We optimized our model through extensive experiments on an NVIDIA A100 GPU, fine-tuning key hyperparameters while maintaining consistent settings across all runs. The final hyperparameters, selected via rigorous validation, improved both accuracy and strict accuracy on ISE-DSC01 and ViWikiFC. Details are provided in Appendix B. For evaluation, all methods were tested on an NVIDIA T4 instance in Kaggle to ensure fairness. Additionally, the large language model was finetuned on distributed setup with 4H100 GPUs, utilizing structured prompt reformulation process. The dataset, originally in raw text and labels, was transformed into prompt-based format optimized for LLM fine-tuning. This ensured effective task-specific learning while aligning with pre-trained knowledge. Further details on training setup, prompt design, and preprocessing are in Appendix B."
        },
        {
            "title": "4.3 Main Results",
            "content": "The results in Table 2 demonstrate that SemViQA outperforms previous methods in Vietnamese factchecking tasks. Specifically, our model achieves the highest Strict Accuracy, reaching 80.82% on ViWikiFC and 78.97% on ISE-DSC01, establishing new benchmark for automated fact-checking systems in Vietnamese language."
        },
        {
            "title": "4.3.1 Performance Comparison",
            "content": "a) Handling Long Token Sequences in FactChecking One major limitation of conventional Question Answering (QA) models in fact verification tasks is their inability to process long-context claims due to token length constraints. Transformer-based models such as ViMRClarge, InfoXLMlarge (Chi et al., 2021), and XLM-Rlarge (Conneau et al., 2020), ErnieMlarge (Ouyang et al., 2021) are restricted to maximum input length of 512 tokens. However, real-world fact-checking datasets, such as ISEDSC01, often contain contexts exceeding 4800 tokens, causing significant drop in performance for QA-based approaches due to their inability to handle long-token sequences. This limitation prevents these models from effectively leveraging the full evidence available, leading to incomplete or inaccurate fact verification. To address this issue, our proposed SemViQA model incorporates retrieval-based approach capable of handling long-token sequences efficiently, as illustrated in Figure 3. On ISE-DSC01, where long contexts are essential for verification, SemViQA significantly outperforms traditional QA models by effectively capturing and utilizing extended contextual information. This demonstrates that the long-token issue is critical bottleneck in fact verification tasks. In contrast, on the ViWikiFC dataset, where the token length is closer to 512 tokens, QA-based models achieve competitive performance, further confirming that the long-token issue is major factor in performance degradation. However, when the long-token limitation does not exist on the ViWikiFC dataset, integrating our SER approach into QA-based model proves to be an effective solution. By leveraging SER, we achieve approximately 1.86% higher ER Accuracy on ViWikiFC, demonstrating its efficiency in optimizing fact verification when token constraints are minimized. These results highlight that long-token constraints are fundamental challenge in fact verification. Our approach not only mitigates this issue through retrieval-based processing in SemViQA but also enhances QA models with QACT when long-token constraints are less severe, ensuring improved performance across diverse datasets. b) Performance and Inference Time Optimization One of the key advancements of SemViQA over other methods is its significant reduction in inference time without compromising accuracy. Key observations include: The average inference time of SemViQA on ISE-DSC01 is 5200s, whereas large 6 Method ER TF-IDF BM SBert QA-based approaches ViMRClarge InfoXLMlarge VC InfoXLMlarge XLM-Rlarge Ernie-Mlarge InfoXLMlarge XLM-Rlarge Ernie-Mlarge InfoXLMlarge XLM-Rlarge Ernie-Mlarge VC InfoXLMlarge XLM-Rlarge Ernie-Mlarge InfoXLMlarge XLM-Rlarge Ernie-Mlarge LLM Qwen2.5-1.5B-Instruct Qwen2.5-3B-Instruct LLM Qwen2.5-1.5B-Instruct Qwen2.5-3B-Instruct SER Faster (ours) TF-IDF + ViMRClarge TF-IDF + InfoXLMlarge SER (ours) TF-IDF + ViMRClarge TF-IDF + InfoXLMlarge VC InfoXLMlarge XLM-Rlarge Ernie-Mlarge InfoXLMlarge XLM-Rlarge Ernie-Mlarge TVC (ours) Ernie-Mlarge TVC (ours) InfoXLMlarge XLM-Rlarge Ernie-Mlarge InfoXLMlarge XLM-Rlarge Ernie-Mlarge Parameter ViWikiFC Strict Acc VC Acc ER Acc Time (s) ISE-DSC01 Strict Acc VC Acc ER Acc Time (s) Avg Strict Acc 560M 560M 560M 560M 560M 560M 838M 838M 838M 1120M 1120M 1120M 1120M 1120M 1120M 1.5B 3B 2B 2B 2B 3.5B 3.5B 3.5B 1680M 1680M 1680M 1680M 1680M 1680M 1680M 1680M 75.56 76.47 75.56 70.44 70.97 70.21 74.99 75.80 75. 77.28 78.29 77.38 78.14 79.20 78.24 51.03 44.38 66.14 67.67 66.52 59.88 60.74 60. 79.44 79.77 80.25 80.34 79.53 80.68 80.82 80.06 82.21 82.78 81.83 79.01 78.91 78.29 81.59 82.35 81. 81.97 82.83 81.92 82.07 83.07 82.21 65.18 62.31 76.47 78.10 76.52 72.50 73.08 72. 82.93 83.07 83.84 83.64 82.97 83.98 83.88 83.17 90.15 90.15 90.15 83.50 83.50 83.50 89.72 89.72 89. 92.49 92.49 92.49 93.45 93.45 93.45 78.96 71.35 78.96 78.96 78.96 71.35 71.35 71. 94.60 95.03 94.69 94.69 94.69 95.31 95.31 95.31 131 134 144 130 132 141 195 194 3778 3824 3785 4092 4096 4102 7665 12123 7788 7789 7794 12246 12246 410 487 2731 2733 2733 3860 3843 3891 73.59 75.61 78.19 72.09 73.94 76.58 71.20 72.85 75. 54.36 53.98 56.62 53.50 53.32 56.34 59.23 60.87 64.40 64.66 65.70 65.72 66.12 67. 78.32 78.37 75.13 76.71 78.97 75.13 76.74 78.97 78.08 80.50 81.69 77.37 79.37 80.76 76.59 78.78 79. 64.14 66.70 62.19 63.83 66.70 62.36 66.68 66.92 68.37 69.63 68.37 69.66 70.44 70. 81.91 81.91 79.54 81.65 82.54 79.60 81.71 82.49 76.61 78.58 80.65 75.04 76.95 79.02 74.15 75.89 77. 56.84 57.77 58.91 56.17 57.25 58.69 65.51 66.10 66.49 66.72 67.33 67.51 67.83 68. 80.26 80.32 76.87 78.91 80.91 76.87 78.95 80.91 378 366 403 320 333 381 915 835 9798 9809 9833 10057 10066 10078 19780 31284 19970 19976 20003 31477 31483 995 925 5191 5219 5225 5175 5200 5297 74.58 76.04 76.88 71.27 72.46 73.40 73.10 74.33 75. 65.82 66.14 67.00 65.82 66.26 67.29 55.13 52.63 65.27 66.17 66.11 62.80 63.43 63. 78.88 79.07 77.69 78.53 79.25 77.91 78.78 79.52 Table 2: Performance comparison on the ViWikiFC test set and the ISE-DSC01 private-test dataset. The results highlight differences among models based on several criteria: Strict Accuracy (Strict Acc), Veracity Classification Accuracy (VC Acc), and Evidence Retrieval Accuracy (ER Acc). Time represents the total inference time required to generate the complete results. Parameter indicates the total number of parameters used in each task. The results highlighted in blue indicate that our SER Faster method achieves the highest performance among all methods, except for the standard SER method. LLM-based models such as Qwen2.5-3BInstruct (Qwen et al., 2024) require over 31,000s, making SemViQA at least 6 times faster than these large language model approaches. Compared to ViMRClarge 3 (9800s on ISEDSC01), SemViQA reduces inference time by nearly 50% while maintaining higher performance in both Strict Accuracy and Veracity Classification Accuracy. Methods based on SBERT (Reimers and Gurevych, 2019b) or BM25 have lower infer3https://huggingface.co/nguyenvulebinh/ vi-mrc-large ence times but fail to maintain high accuracy, particularly when dealing with multi-step reasoning tasks. SemViQA achieves better balance between classification performance and processing speed, ensuring practical deployment in real-world environments. SemViQA Faster: We introduce an optimized version of SemViQA that achieves up to 7 faster inference while maintaining high accuracy. As shown in Figure 4, the inference speed of SERFaster closely matches that of traditional methods while still outperforming competing solutions in both efficiency and accuracy. To achieve this speed improvement, we process all subcontexts within sample as batch, enabling batch inference (see 7 Figure 4: Comparison of method performance, balancing accuracy and inference time. Each retrieval method is evaluated based on its highest achieved score, while the total inference time across both datasets is reported to highlight efficiency. Further details can be found in Table 2. Section 3.1 for details on subcontexts). While this approach results in slight performance drop compared to the standard SemViQA model, the tradeoff is minimal considering the significant reduction in processing time. This efficiency enhancement makes SemViQA Faster highly suitable for realworld deployment, allowing seamless integration into large-scale fact-checking applications."
        },
        {
            "title": "SemViQA",
            "content": "The confidence threshold plays crucial role in balancing accuracy and inference time in SemViQAs evidence retrieval process. Analysis from Figure 5 indicates that as the threshold increases from 0.0 to 0.5, evidence retrieval accuracy improves significantly, particularly on ViWikiFC ( 95%) and ISE-DSC01 ( 80.8%). However, beyond 0.5, accuracy gains plateau, while inference time decreases sharply due to the system filtering out lowconfidence evidence more aggressively. Setting an optimal threshold in the range of 0.4 0.5 achieves trade-off between efficiency and accuracy, ensuring that SemViQA operates swiftly while maintaining precise evidence retrieval."
        },
        {
            "title": "4.3.3 Comparison with other results in the",
            "content": "competition The results presented in Table 3 indicate that our SemViQA approach outperforms other competing teams, achieving the highest Strict Accuracy and demonstrating exceptional effectiveness in information processing and verification. This achievement highlights SemViQAs capability to deliver Figure 5: Impact of confidence threshold on evidence retrieval accuracy in SemViQA. Methods SemViQA DS@UIT Dynasty URA_FNU Plain Sailing ViNSV Strict Acc VC Acc ER Acc 80.91 82.54 84.76 80.13 79.96 83.71 78.31 82.33 78.11 81.67 78.97 78.05 77.87 77.09 76. Table 3: Comparison of results with the top 5 teams in the competition significantly more accurate and reliable results."
        },
        {
            "title": "5 Conclusion and Future Works",
            "content": "We introduced SemViQA, Vietnamese factchecking framework that integrates Semantic-based Evidence Retrieval (SER) and Two-step Verdict Classification (TVC) to enhance claim verification. Our approach outperforms existing methods, including LLMs, TF-IDF, BM25, SBERT, and QAbased models, particularly in handling long-token sequences and complex reasoning tasks. Extensive experiments demonstrated SemViQAs stateof-the-art performance on ISE-DSC01 and ViWikiFC. Additionally, the SemViQA Faster variant accelerates inference by up to 7, improving its practicality for real-world applications. By addressing key challenges such as semantic ambiguity and multi-step reasoning, SemViQA lays the ground8 work for advancing Vietnamese NLP, with potential applications in misinformation detection and low-resource language fact-checking."
        },
        {
            "title": "Limitations",
            "content": "While SemViQA demonstrates strong performance in Vietnamese fact verification, several limitations remain. First, our reliance on TF-IDF for initial evidence retrieval, while efficient, limits the models ability to capture deep semantic relationships and retrieve implicit evidence. To mitigate this, we employ threshold-based mechanism to identify hard samples and process them with more advanced retrieval model. However, this approach relies on manually defined thresholds, which may not generalize well across different datasets, underscoring the need for adaptive and data-driven retrieval strategies in future work. Second, our Two-step Verdict Classification (TVC) framework improves claim verification accuracy but requires multiple classification stages, increasing inference time compared to single-step approaches. This additional computational cost is particularly significant in three-class classification tasks, where optimizing model efficiency without compromising accuracy remains key challenge. Future work should focus on refining retrieval mechanisms and classification strategies to enhance efficiency and robustness, ensuring broader applicability of SemViQA in real-world fact verification scenarios."
        },
        {
            "title": "References",
            "content": "Marah Abdin, Sam Ade Jacobs, Ammar Ahmad Awan, Jyoti Aneja, Ahmed Awadallah, Hany Awadalla, Nguyen Bach, Amit Bahree, Arash Bakhtiari, Harkirat Behl, et al. 2024. Phi-3 technical report: highly capable language model locally on your phone. arXiv preprint arXiv:2404.14219. Mubashara Akhtar, Oana Cocarascu, and Elena Simperl. 2022. PubHealthTab: public health table-based dataset for evidence-based fact checking. In Findings of the Association for Computational Linguistics: NAACL 2022, pages 116, Seattle, United States. Association for Computational Linguistics. Rami Aly, Zhijiang Guo, Michael Sejr Schlichtkrull, James Thorne, Andreas Vlachos, Christos Christodoulopoulos, Oana Cocarascu, and Arpit Mittal. 2021. The fact extraction and VERification over unstructured and structured information In Proceedings of (FEVEROUS) shared task. the Fourth Workshop on Fact Extraction and VERification (FEVER), pages 113, Dominican Republic. Association for Computational Linguistics. Rami Aly and Andreas Vlachos. 2022. Natural logicguided autoregressive multi-hop document retrieval for fact verification. In Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing, pages 61236135, Abu Dhabi, United Arab Emirates. Association for Computational Linguistics. Wenhu Chen, Hongmin Wang, Jianshu Chen, Yunkai Zhang, Hong Wang, Shiyang Li, Xiyou Zhou, and William Yang Wang. 2020. Tabfact: large-scale dataset for table-based fact verification. Preprint, arXiv:1909.02164. Zewen Chi, Li Dong, Furu Wei, Nan Yang, Saksham Singhal, Wenhui Wang, Xia Song, Xian-Ling Mao, Heyan Huang, and Ming Zhou. 2021. InfoXLM: An information-theoretic framework for cross-lingual language model pre-training. In Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 35763588, Online. Association for Computational Linguistics. Alexis Conneau, Kartikay Khandelwal, Naman Goyal, Vishrav Chaudhary, Guillaume Wenzek, Francisco Guzmán, Edouard Grave, Myle Ott, Luke Zettlemoyer, and Veselin Stoyanov. 2020. Unsupervised cross-lingual representation learning at scale. In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 8440 8451, Online. Association for Computational Linguistics. DeepSeek-AI, Aixin Liu, Bei Feng, Bing Xue, Bingxuan Wang, Bochao Wu, Chengda Lu, Chenggang Zhao, Chengqi Deng, Chenyu Zhang, Chong Ruan, Damai Dai, Daya Guo, Dejian Yang, Deli Chen, Dongjie Ji, Erhang Li, Fangyun Lin, Fucong Dai, Fuli Luo, Guangbo Hao, Guanting Chen, Guowei Li, H. Zhang, Han Bao, Hanwei Xu, Haocheng Wang, Haowei Zhang, Honghui Ding, Huajian Xin, Huazuo Gao, Hui Li, Hui Qu, J. L. Cai, Jian Liang, Jianzhong Guo, Jiaqi Ni, Jiashi Li, Jiawei Wang, Jin Chen, Jingchang Chen, Jingyang Yuan, Junjie Qiu, Junlong Li, Junxiao Song, Kai Dong, Kai Hu, Kaige Gao, Kang Guan, Kexin Huang, Kuai Yu, Lean Wang, Lecong Zhang, Lei Xu, Leyi Xia, Liang Zhao, Litong Wang, Liyue Zhang, Meng Li, Miaojun Wang, Mingchuan Zhang, Minghua Zhang, Minghui Tang, Mingming Li, Ning Tian, Panpan Huang, Peiyi Wang, Peng Zhang, Qiancheng Wang, Qihao Zhu, Qinyu Chen, Qiushi Du, R. J. Chen, R. L. Jin, Ruiqi Ge, Ruisong Zhang, Ruizhe Pan, Runji Wang, Runxin Xu, Ruoyu Zhang, Ruyi Chen, S. S. Li, Shanghao Lu, Shangyan Zhou, Shanhuang Chen, Shaoqing Wu, Shengfeng Ye, Shengfeng Ye, Shirong Ma, Shiyu Wang, Shuang Zhou, Shuiping Yu, Shunfeng Zhou, Shuting Pan, T. Wang, Tao Yun, Tian Pei, Tianyu Sun, W. L. Xiao, Wangding Zeng, Wanjia Zhao, Wei An, Wen Liu, Wenfeng Liang, Wenjun Gao, Wenqin Yu, Wentao Zhang, X. Q. Li, Xiangyue Jin, Xianzu Wang, Xiao Bi, Xiaodong Liu, Xiaohan Wang, Xiaojin Shen, Xiaokang Chen, Xiaokang Zhang, Xiaosha Chen, Xiaotao Nie, Xiaowen Sun, Xiaoxiang Wang, Xin Cheng, 9 Xin Liu, Xin Xie, Xingchao Liu, Xingkai Yu, Xinnan Song, Xinxia Shan, Xinyi Zhou, Xinyu Yang, Xinyuan Li, Xuecheng Su, Xuheng Lin, Y. K. Li, Y. Q. Wang, Y. X. Wei, Y. X. Zhu, Yang Zhang, Yanhong Xu, Yanhong Xu, Yanping Huang, Yao Li, Yao Zhao, Yaofeng Sun, Yaohui Li, Yaohui Wang, Yi Yu, Yi Zheng, Yichao Zhang, Yifan Shi, Yiliang Xiong, Ying He, Ying Tang, Yishi Piao, Yisong Wang, Yixuan Tan, Yiyang Ma, Yiyuan Liu, Yongqiang Guo, Yu Wu, Yuan Ou, Yuchen Zhu, Yuduan Wang, Yue Gong, Yuheng Zou, Yujia He, Yukun Zha, Yunfan Xiong, Yunxian Ma, Yuting Yan, Yuxiang Luo, Yuxiang You, Yuxuan Liu, Yuyang Zhou, Z. F. Wu, Z. Z. Ren, Zehui Ren, Zhangli Sha, Zhe Fu, Zhean Xu, Zhen Huang, Zhen Zhang, Zhenda Xie, Zhengyan Zhang, Zhewen Hao, Zhibin Gou, Zhicheng Ma, Zhigang Yan, Zhihong Shao, Zhipeng Xu, Zhiyu Wu, Zhongyu Zhang, Zhuoshu Li, Zihui Gu, Zijia Zhu, Zijun Liu, Zilin Li, Ziwei Xie, Ziyang Song, Ziyi Gao, and Zizheng Pan. 2024. Deepseek-v3 technical report. Preprint, arXiv:2412.19437. Mitchell DeHaven and Stephen Scott. 2023. BEVERS: general, simple, and performant framework for automatic fact verification. In Proceedings of the Sixth Fact Extraction and VERification Workshop (FEVER), pages 5865, Dubrovnik, Croatia. Association for Computational Linguistics. Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. 2019. BERT: Pre-training of deep bidirectional transformers for language understanding. In Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers), pages 41714186, Minneapolis, Minnesota. Association for Computational Linguistics. Alex Graves and Jurgen Schmidhuber. 2005. Framewise phoneme classification with bidirectional lstm and other neural network architectures. Neural networks : the official journal of the International Neural Network Society, 18:60210. Siqing Huo, Negar Arabzadeh, and Charles Clarke. 2023. Retrieving supporting evidence for generative question answering. In Proceedings of the Annual International ACM SIGIR Conference on Research and Development in Information Retrieval in the Asia Pacific Region, SIGIR-AP 23, page 1120. ACM. Ying Ju, Fubang Zhao, Shijie Chen, Bowen Zheng, Xuefeng Yang, and Yunfeng Liu. 2019. Technical report on conversational question answering. Preprint, arXiv:1909.10772. Hung Tuan Le, Long Truong To, Manh Trong Nguyen, and Kiet Van Nguyen. 2024. Viwikifc: Fact-checking for vietnamese wikipedia-based textual knowledge source. Preprint, arXiv:2405.07615. Tsung-Yi Lin, Priya Goyal, Ross Girshick, Kaiming He, and Piotr Dollár. 2018. Focal loss for dense object detection. Preprint, arXiv:1708.02002. Ying-Jia Lin, Chun-Yi Lin, Chia-Jen Yeh, Yi-Ting Li, Yun-Yu Hu, Chih-Hao Hsu, Mei-Feng Lee, and Hung-Yu Kao. 2024. Cfever: chinese fact extraction and verification dataset. Preprint, arXiv:2402.13025. Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi, Danqi Chen, Omer Levy, Mike Lewis, Luke Zettlemoyer, and Veselin Stoyanov. 2019. Roberta: robustly optimized bert pretraining approach. Preprint, arXiv:1907.11692. Christopher Malon. 2018. Team papelo: Transformer In Proceedings of the First networks at FEVER. Workshop on Fact Extraction and VERification (FEVER), pages 109113, Brussels, Belgium. Association for Computational Linguistics. Yixin Nie, Haonan Chen, and Mohit Bansal. 2018. Combining fact extraction and verification with Preprint, neural semantic matching networks. arXiv:1811.07039. Xuan Ouyang, Shuohuan Wang, Chao Pang, Yu Sun, Hao Tian, Hua Wu, and Haifeng Wang. 2021. ERNIE-M: Enhanced multilingual representation by aligning cross-lingual semantics with monolingual corpora. In Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, pages 2738, Online and Punta Cana, Dominican Republic. Association for Computational Linguistics. Qwen, :, An Yang, Baosong Yang, Beichen Zhang, Binyuan Hui, Bo Zheng, Bowen Yu, Chengyuan Li, Dayiheng Liu, Fei Huang, Haoran Wei, Huan Lin, Jian Yang, Jianhong Tu, Jianwei Zhang, Jianxin Yang, Jiaxi Yang, Jingren Zhou, Junyang Lin, Kai Dang, Keming Lu, Keqin Bao, Kexin Yang, Le Yu, Mei Li, Mingfeng Xue, Pei Zhang, Qin Zhu, Rui Men, Runji Lin, Tianhao Li, Tianyi Tang, Tingyu Xia, Xingzhang Ren, Xuancheng Ren, Yang Fan, Yang Su, Yichang Zhang, Yu Wan, Yuqiong Liu, Zeyu Cui, Zhenru Zhang, and Zihan Qiu. 2025. Qwen2.5 technical report. Preprint, arXiv:2412.15115. Qwen, :, An Yang, Baosong Yang, Beichen Zhang, Binyuan Hui, Bo Zheng, Bowen Yu, Chengyuan Li, Dayiheng Liu, Fei Huang, Haoran Wei, Huan Lin, Jian Yang, Jianhong Tu, Jianwei Zhang, Jianxin Yang, Jiaxi Yang, Jingren Zhou, Junyang Lin, Kai Dang, Keming Lu, Keqin Bao, Kexin Yang, Le Yu, Mei Li, Mingfeng Xue, Pei Zhang, Qin Zhu, Rui Men, Runji Lin, Tianhao Li, Tianyi Tang, Tingyu Xia, Xingzhang Ren, Xuancheng Ren, Yang Fan, Yang Su, Yichang Zhang, Yu Wan, Yuqiong Liu, Zeyu Cui, Zhenru Zhang, and Zihan Qiu. 2024. Qwen2.5 technical report. Preprint, arXiv:2412.15115. Nils Reimers and Iryna Gurevych. 2019a. SentenceBERT: Sentence embeddings using Siamese BERTnetworks. In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP), pages 39823992, Hong Kong, China. Association for Computational Linguistics. 10 Nils Reimers and Iryna Gurevych. 2019b. SentenceBERT: Sentence embeddings using Siamese BERTnetworks. In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP), pages 39823992, Hong Kong, China. Association for Computational Linguistics. Tobias Schimanski, Jingwei Ni, Mathias Kraus, Elliott Ash, and Markus Leippold. 2024. Towards faithful and robust LLM specialists for evidence-based question-answering. In Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 1913 1931, Bangkok, Thailand. Association for Computational Linguistics. Amir Soleimani, Christof Monz, and Marcel Worring. 2020. Bert for evidence retrieval and claim verification. In Advances in Information Retrieval: 42nd European Conference on IR Research, ECIR 2020, Lisbon, Portugal, April 1417, 2020, Proceedings, Part II, page 359366, Berlin, Heidelberg. SpringerVerlag. Gemini Team, Rohan Anil, Sebastian Borgeaud, JeanBaptiste Alayrac, Jiahui Yu, Radu Soricut, Johan Schalkwyk, Andrew M. Dai, Anja Hauth, Katie Millican, David Silver, Melvin Johnson, Ioannis Antonoglou, Julian Schrittwieser, Amelia Glaese, Jilin Chen, Emily Pitler, Timothy Lillicrap, Angeliki Lazaridou, Orhan Firat, James Molloy, Michael Isard, Paul R. Barham, Tom Hennigan, Benjamin Lee, Fabio Viola, Malcolm Reynolds, Yuanzhong Xu, Ryan Doherty, Eli Collins, Clemens Meyer, Eliza Rutherford, Erica Moreira, Kareem Ayoub, Megha Goel, Jack Krawczyk, Cosmo Du, Ed Chi, HengTze Cheng, Eric Ni, Purvi Shah, Patrick Kane, Betty Chan, Manaal Faruqui, Aliaksei Severyn, Hanzhao Lin, YaGuang Li, Yong Cheng, Abe Ittycheriah, Mahdis Mahdieh, Mia Chen, Pei Sun, Dustin Tran, Sumit Bagri, Balaji Lakshminarayanan, Jeremiah Liu, Andras Orban, Fabian Gura, Hao Zhou, Xinying Song, Aurelien Boffy, Harish Ganapathy, Steven Zheng, HyunJeong Choe, Ágoston Weisz, Tao Zhu, Yifeng Lu, Siddharth Gopal, Jarrod Kahn, Maciej Kula, Jeff Pitman, Rushin Shah, Emanuel Taropa, Majd Al Merey, Martin Baeuml, Zhifeng Chen, Laurent El Shafey, Yujing Zhang, Olcan Sercinoglu, George Tucker, Enrique Piqueras, Maxim Krikun, Iain Barr, Nikolay Savinov, Ivo Danihelka, Becca Roelofs, Anaıs White, Anders Andreassen, Tamara von Glehn, Lakshman Yagati, Mehran Kazemi, Lucas Gonzalez, Misha Khalman, Jakub Sygnowski, Alexandre Frechette, Charlotte Smith, Laura Culp, Lev Proleev, Yi Luan, Xi Chen, James Lottes, Nathan Schucher, Federico Lebron, Alban Rrustemi, Natalie Clay, Phil Crone, Tomas Kocisky, Jeffrey Zhao, Bartek Perz, Dian Yu, Heidi Howard, Adam Bloniarz, Jack W. Rae, Han Lu, Laurent Sifre, Marcello Maggioni, Fred Alcober, Dan Garrette, Megan Barnes, Shantanu Thakoor, Jacob Austin, Gabriel Barth-Maron, William Wong, Rishabh Joshi, Rahma 11 Chaabouni, Deeni Fatiha, Arun Ahuja, Gaurav Singh Tomar, Evan Senter, Martin Chadwick, Ilya Kornakov, Nithya Attaluri, Inaki Iturrate, Ruibo Liu, Yunxuan Li, Sarah Cogan, Jeremy Chen, Chao Jia, Chenjie Gu, Qiao Zhang, Jordan Grimstad, Ale Jakse Hartman, Xavier Garcia, Thanumalayan Sankaranarayana Pillai, Jacob Devlin, Michael Laskin, Diego de Las Casas, Dasha Valter, Connie Tao, Lorenzo Blanco, Adrià Puigdomènech Badia, David Reitter, Mianna Chen, Jenny Brennan, Clara Rivera, Sergey Brin, Shariq Iqbal, Gabriela Surita, Jane Labanowski, Abhi Rao, Stephanie Winkler, Emilio Parisotto, Yiming Gu, Kate Olszewska, Ravi Addanki, Antoine Miech, Annie Louis, Denis Teplyashin, Geoff Brown, Elliot Catt, Jan Balaguer, Jackie Xiang, Pidong Wang, Zoe Ashwood, Anton Briukhov, Albert Webson, Sanjay Ganapathy, Smit Sanghavi, Ajay Kannan, MingWei Chang, Axel Stjerngren, Josip Djolonga, Yuting Sun, Ankur Bapna, Matthew Aitchison, Pedram Pejman, Henryk Michalewski, Tianhe Yu, Cindy Wang, Juliette Love, Junwhan Ahn, Dawn Bloxwich, Kehang Han, Peter Humphreys, Thibault Sellam, James Bradbury, Varun Godbole, Sina Samangooei, Bogdan Damoc, Alex Kaskasoli, Sébastien M. R. Arnold, Vijay Vasudevan, Shubham Agrawal, Jason Riesa, Dmitry Lepikhin, Richard Tanburn, Srivatsan Srinivasan, Hyeontaek Lim, Sarah Hodkinson, Pranav Shyam, Johan Ferret, Steven Hand, Ankush Garg, Tom Le Paine, Jian Li, Yujia Li, Minh Giang, Alexander Neitz, Zaheer Abbas, Sarah York, Machel Reid, Elizabeth Cole, Aakanksha Chowdhery, Dipanjan Das, Dominika Rogozinska, Vitaliy Nikolaev, Pablo Sprechmann, Zachary Nado, Lukas Zilka, Flavien Prost, Luheng He, Marianne Monteiro, Gaurav Mishra, Chris Welty, Josh Newlan, Dawei Jia, Miltiadis Allamanis, Clara Huiyi Hu, Raoul de Liedekerke, Justin Gilmer, Carl Saroufim, Shruti Rijhwani, Shaobo Hou, Disha Shrivastava, Anirudh Baddepudi, Alex Goldin, Adnan Ozturel, Albin Cassirer, Yunhan Xu, Daniel Sohn, Devendra Sachan, Reinald Kim Amplayo, Craig Swanson, Dessie Petrova, Shashi Narayan, Arthur Guez, Siddhartha Brahma, Jessica Landon, Miteyan Patel, Ruizhe Zhao, Kevin Villela, Luyu Wang, Wenhao Jia, Matthew Rahtz, Mai Giménez, Legg Yeung, James Keeling, Petko Georgiev, Diana Mincu, Boxi Wu, Salem Haykal, Rachel Saputro, Kiran Vodrahalli, James Qin, Zeynep Cankara, Abhanshu Sharma, Nick Fernando, Will Hawkins, Behnam Neyshabur, Solomon Kim, Adrian Hutter, Priyanka Agrawal, Alex Castro-Ros, George van den Driessche, Tao Wang, Fan Yang, Shuo yiin Chang, Paul Komarek, Ross McIlroy, Mario Luˇcic, Guodong Zhang, Wael Farhan, Michael Sharman, Paul Natsev, Paul Michel, Yamini Bansal, Siyuan Qiao, Kris Cao, Siamak Shakeri, Christina Butterfield, Justin Chung, Paul Kishan Rubenstein, Shivani Agrawal, Arthur Mensch, Kedar Soparkar, Karel Lenc, Timothy Chung, Aedan Pope, Loren Maggiore, Jackie Kay, Priya Jhakra, Shibo Wang, Joshua Maynez, Mary Phuong, Taylor Tobin, Andrea Tacchetti, Maja Trebacz, Kevin Robinson, Yash Katariya, Sebastian Riedel, Paige Bailey, Kefan Xiao, Nimesh Ghelani, Lora Aroyo, Ambrose Slone, Neil Houlsby, Xuehan Xiong, Zhen Yang, Elena Gribovskaya, Jonas Adler, Mateo Wirth, Lisa Lee, Music Li, Thais Kagohara, Jay Pavagadhi, Sophie Bridgers, Anna Bortsova, Sanjay Ghemawat, Zafarali Ahmed, Tianqi Liu, Richard Powell, Vijay Bolina, Mariko Iinuma, Polina Zablotskaia, James Besley, Da-Woon Chung, Timothy Dozat, Ramona Comanescu, Xiance Si, Jeremy Greer, Guolong Su, Martin Polacek, Raphael Lopez Kaufman, Simon Tokumine, Hexiang Hu, Elena Buchatskaya, Yingjie Miao, Mohamed Elhawaty, Aditya Siddhant, Nenad Tomasev, Jinwei Xing, Christina Greer, Helen Miller, Shereen Ashraf, Aurko Roy, Zizhao Zhang, Ada Ma, Angelos Filos, Milos Besta, Rory Blevins, Ted Klimenko, Chih-Kuan Yeh, Soravit Changpinyo, Jiaqi Mu, Oscar Chang, Mantas Pajarskas, Carrie Muir, Vered Cohen, Charline Le Lan, Krishna Haridasan, Amit Marathe, Steven Hansen, Sholto Douglas, Rajkumar Samuel, Mingqiu Wang, Sophia Austin, Chang Lan, Jiepu Jiang, Justin Chiu, Jaime Alonso Lorenzo, Lars Lowe Sjosund, Sébastien Cevey, Zach Gleicher, Thi Avrahami, Anudhyan Boral, Hansa Srinivasan, Vittorio Selo, Rhys May, Konstantinos Aisopos, Léonard Hussenot, Livio Baldini Soares, Kate Baumli, Michael B. Chang, Adrià Recasens, Ben Caine, Alexander Pritzel, Filip Pavetic, Fabio Pardo, Anita Gergely, Justin Frye, Vinay Ramasesh, Dan Horgan, Kartikeya Badola, Nora Kassner, Subhrajit Roy, Ethan Dyer, Víctor Campos Campos, Alex Tomala, Yunhao Tang, Dalia El Badawy, Elspeth White, Basil Mustafa, Oran Lang, Abhishek Jindal, Sharad Vikram, Zhitao Gong, Sergi Caelles, Ross Hemsley, Gregory Thornton, Fangxiaoyu Feng, Wojciech Stokowiec, Ce Zheng, Phoebe Thacker, aglar Unlu, Zhishuai Zhang, Mohammad Saleh, James Svensson, Max Bileschi, Piyush Patil, Ankesh Anand, Roman Ring, Katerina Tsihlas, Arpi Vezer, Marco Selvi, Toby Shevlane, Mikel Rodriguez, Tom Kwiatkowski, Samira Daruki, Keran Rong, Allan Dafoe, Nicholas FitzGerald, Keren Gu-Lemberg, Mina Khan, Lisa Anne Hendricks, Marie Pellat, Vladimir Feinberg, James Cobon-Kerr, Tara Sainath, Maribeth Rauh, Sayed Hadi Hashemi, Richard Ives, Yana Hasson, Eric Noland, Yuan Cao, Nathan Byrd, Le Hou, Qingze Wang, Thibault Sottiaux, Michela Paganini, Jean-Baptiste Lespiau, Alexandre Moufarek, Samer Hassan, Kaushik Shivakumar, Joost van Amersfoort, Amol Mandhane, Pratik Joshi, Anirudh Goyal, Matthew Tung, Andrew Brock, Hannah Sheahan, Vedant Misra, Cheng Li, Nemanja Rakicevic, Mostafa Dehghani, Fangyu Liu, Sid Mittal, Junhyuk Oh, Seb Noury, Eren Sezener, Fantine Huot, Matthew Lamm, Nicola De Cao, Charlie Chen, Sidharth Mudgal, Romina Stella, Kevin Brooks, Gautam Vasudevan, Chenxi Liu, Mainak Chain, Nivedita Melinkeri, Aaron Cohen, Venus Wang, Kristie Seymore, Sergey Zubkov, Rahul Goel, Summer Yue, Sai Krishnakumaran, Brian Albert, Nate Hurley, Motoki Sano, Anhad Mohananey, Jonah Joughin, Egor Filonov, Tomasz, Yomna Eldawy, Jiawern Lim, Rahul Rishi, Shirin Badiezadegan, Taylor Bos, Jerry Chang, Sanil Jain, Sri Gayatri Sundara Padmanabhan, Subha Puttagunta, Kalpesh Krishna, Leslie Baker, Norbert Kalb, Vamsi Bedapudi, Adam Kurzrok, Shuntong Lei, Anthony Yu, Oren Litvin, Xiang Zhou, 12 Zhichun Wu, Sam Sobell, Andrea Siciliano, Alan Papir, Robby Neale, Jonas Bragagnolo, Tej Toor, Tina Chen, Valentin Anklin, Feiran Wang, Richie Feng, Milad Gholami, Kevin Ling, Lijuan Liu, Jules Walter, Hamid Moghaddam, Arun Kishore, Jakub Adamek, Tyler Mercado, Jonathan Mallinson, Siddhinita Wandekar, Stephen Cagle, Eran Ofek, Guillermo Garrido, Clemens Lombriser, Maksim Mukha, Botu Sun, Hafeezul Rahman Mohammad, Josip Matak, Yadi Qian, Vikas Peswani, Pawel Janus, Quan Yuan, Leif Schelin, Oana David, Ankur Garg, Yifan He, Oleksii Duzhyi, Anton Algmyr, Timothée Lottaz, Qi Li, Vikas Yadav, Luyao Xu, Alex Chinien, Rakesh Shivanna, Aleksandr Chuklin, Josie Li, Carrie Spadine, Travis Wolfe, Kareem Mohamed, Subhabrata Das, Zihang Dai, Kyle He, Daniel von Dincklage, Shyam Upadhyay, Akanksha Maurya, Luyan Chi, Sebastian Krause, Khalid Salama, Pam Rabinovitch, Pavan Kumar Reddy M, Aarush Selvan, Mikhail Dektiarev, Golnaz Ghiasi, Erdem Guven, Himanshu Gupta, Boyi Liu, Deepak Sharma, Idan Heimlich Shtacher, Shachi Paul, Oscar Akerlund, Francois-Xavier Aubet, Terry Huang, Chen Zhu, Eric Zhu, Elico Teixeira, Matthew Fritze, Francesco Bertolini, Liana-Eleonora Marinescu, Martin Bolle, Dominik Paulus, Khyatti Gupta, Tejasi Latkar, Max Chang, Jason Sanders, Roopa Wilson, Xuewei Wu, Yi-Xuan Tan, Lam Nguyen Thiet, Tulsee Doshi, Sid Lall, Swaroop Mishra, Wanming Chen, Thang Luong, Seth Benjamin, Jasmine Lee, Ewa Andrejczuk, Dominik Rabiej, Vipul Ranjan, Krzysztof Styrc, Pengcheng Yin, Jon Simon, Malcolm Rose Harriott, Mudit Bansal, Alexei Robsky, Geoff Bacon, David Greene, Daniil Mirylenka, Chen Zhou, Obaid Sarvana, Abhimanyu Goyal, Samuel Andermatt, Patrick Siegler, Ben Horn, Assaf Israel, Francesco Pongetti, Chih-Wei \"Louis\" Chen, Marco Selvatici, Pedro Silva, Kathie Wang, Jackson Tolins, Kelvin Guu, Roey Yogev, Xiaochen Cai, Alessandro Agostini, Maulik Shah, Hung Nguyen, Noah Ó Donnaile, Sébastien Pereira, Linda Friso, Adam Stambler, Adam Kurzrok, Chenkai Kuang, Yan Romanikhin, Mark Geller, ZJ Yan, Kane Jang, Cheng-Chun Lee, Wojciech Fica, Eric Malmi, Qijun Tan, Dan Banica, Daniel Balle, Ryan Pham, Yanping Huang, Diana Avram, Hongzhi Shi, Jasjot Singh, Chris Hidey, Niharika Ahuja, Pranab Saxena, Dan Dooley, Srividya Pranavi Potharaju, Eileen ONeill, Anand Gokulchandran, Ryan Foley, Kai Zhao, Mike Dusenberry, Yuan Liu, Pulkit Mehta, Ragha Kotikalapudi, Chalence Safranek-Shrader, Andrew Goodman, Joshua Kessinger, Eran Globen, Prateek Kolhar, Chris Gorgolewski, Ali Ibrahim, Yang Song, Ali Eichenbaum, Thomas Brovelli, Sahitya Potluri, Preethi Lahoti, Cip Baetu, Ali Ghorbani, Charles Chen, Andy Crawford, Shalini Pal, Mukund Sridhar, Petru Gurita, Asier Mujika, Igor Petrovski, Pierre-Louis Cedoz, Chenmei Li, Shiyuan Chen, Niccolò Dal Santo, Siddharth Goyal, Jitesh Punjabi, Karthik Kappaganthu, Chester Kwak, Pallavi LV, Sarmishta Velury, Himadri Choudhury, Jamie Hall, Premal Shah, Ricardo Figueira, Matt Thomas, Minjie Lu, Ting Zhou, Chintu Kumar, Thomas Jurdi, Sharat Chikkerur, Yenai Ma, Adams Yu, Soo Kwak, Victor Ahdel, Sujeevan Rajayogam, Travis Choma, Fei Liu, Aditya Barua, Colin Ji, Ji Ho Park, Vincent Hellendoorn, Alex Bailey, Taylan Bilal, Huanjie Zhou, Mehrdad Khatir, Charles Sutton, Wojciech Rzadkowski, Fiona Macintosh, Konstantin Shagin, Paul Medina, Chen Liang, Jinjing Zhou, Pararth Shah, Yingying Bi, Attila Dankovics, Shipra Banga, Sabine Lehmann, Marissa Bredesen, Zifan Lin, John Eric Hoffmann, Jonathan Lai, Raynald Chung, Kai Yang, Nihal Balani, Arthur Braˇzinskas, Andrei Sozanschi, Matthew Hayes, Héctor Fernández Alcalde, Peter Makarov, Will Chen, Antonio Stella, Liselotte Snijders, Michael Mandl, Ante Karrman, Paweł Nowak, Xinyi Wu, Alex Dyck, Krishnan Vaidyanathan, Raghavender R, Jessica Mallet, Mitch Rudominer, Eric Johnston, Sushil Mittal, Akhil Udathu, Janara Christensen, Vishal Verma, Zach Irving, Andreas Santucci, Gamaleldin Elsayed, Elnaz Davoodi, Marin Georgiev, Ian Tenney, Nan Hua, Geoffrey Cideron, Edouard Leurent, Mahmoud Alnahlawi, Ionut Georgescu, Nan Wei, Ivy Zheng, Dylan Scandinaro, Heinrich Jiang, Jasper Snoek, Mukund Sundararajan, Xuezhi Wang, Zack Ontiveros, Itay Karo, Jeremy Cole, Vinu Rajashekhar, Lara Tumeh, Eyal Ben-David, Rishub Jain, Jonathan Uesato, Romina Datta, Oskar Bunyan, Shimu Wu, John Zhang, Piotr Stanczyk, Ye Zhang, David Steiner, Subhajit Naskar, Michael Azzam, Matthew Johnson, Adam Paszke, Chung-Cheng Chiu, Jaume Sanchez Elias, Afroz Mohiuddin, Faizan Muhammad, Jin Miao, Andrew Lee, Nino Vieillard, Jane Park, Jiageng Zhang, Jeff Stanway, Drew Garmon, Abhijit Karmarkar, Zhe Dong, Jong Lee, Aviral Kumar, Luowei Zhou, Jonathan Evens, William Isaac, Geoffrey Irving, Edward Loper, Michael Fink, Isha Arkatkar, Nanxin Chen, Izhak Shafran, Ivan Petrychenko, Zhe Chen, Johnson Jia, Anselm Levskaya, Zhenkai Zhu, Peter Grabowski, Yu Mao, Alberto Magni, Kaisheng Yao, Javier Snaider, Norman Casagrande, Evan Palmer, Paul Suganthan, Alfonso Castano, Irene Giannoumis, Wooyeol Kim, Mikołaj Rybinski, Ashwin Sreevatsa, Jennifer Prendki, David Soergel, Adrian Goedeckemeyer, Willi Gierke, Mohsen Jafari, Meenu Gaba, Jeremy Wiesner, Diana Gage Wright, Yawen Wei, Harsha Vashisht, Yana Kulizhskaya, Jay Hoover, Maigo Le, Lu Li, Chimezie Iwuanyanwu, Lu Liu, Kevin Ramirez, Andrey Khorlin, Albert Cui, Tian LIN, Marcus Wu, Ricardo Aguilar, Keith Pallo, Abhishek Chakladar, Ginger Perng, Elena Allica Abellan, Mingyang Zhang, Ishita Dasgupta, Nate Kushman, Ivo Penchev, Alena Repina, Xihui Wu, Tom van der Weide, Priya Ponnapalli, Caroline Kaplan, Jiri Simsa, Shuangfeng Li, Olivier Dousse, Fan Yang, Jeff Piper, Nathan Ie, Rama Pasumarthi, Nathan Lintz, Anitha Vijayakumar, Daniel Andor, Pedro Valenzuela, Minnie Lui, Cosmin Paduraru, Daiyi Peng, Katherine Lee, Shuyuan Zhang, Somer Greene, Duc Dung Nguyen, Paula Kurylowicz, Cassidy Hardin, Lucas Dixon, Lili Janzer, Kiam Choo, Ziqiang Feng, Biao Zhang, Achintya Singhal, Dayou Du, Dan McKinnon, Natasha Antropova, Tolga Bolukbasi, Orgad Keller, David Reid, Daniel Finchelstein, Maria Abi Raad, Remi Crocker, Peter Hawkins, Robert Dadashi, Colin Gaffney, Ken 13 Franko, Anna Bulanova, Rémi Leblond, Shirley Chung, Harry Askham, Luis C. Cobo, Kelvin Xu, Felix Fischer, Jun Xu, Christina Sorokin, Chris Alberti, Chu-Cheng Lin, Colin Evans, Alek Dimitriev, Hannah Forbes, Dylan Banarse, Zora Tung, Mark Omernick, Colton Bishop, Rachel Sterneck, Rohan Jain, Jiawei Xia, Ehsan Amid, Francesco Piccinno, Xingyu Wang, Praseem Banzal, Daniel J. Mankowitz, Alex Polozov, Victoria Krakovna, Sasha Brown, MohammadHossein Bateni, Dennis Duan, Vlad Firoiu, Meghana Thotakuri, Tom Natan, Matthieu Geist, Ser tan Girgin, Hui Li, Jiayu Ye, Ofir Roval, Reiko Tojo, Michael Kwong, James Lee-Thorp, Christopher Yew, Danila Sinopalnikov, Sabela Ramos, John Mellor, Abhishek Sharma, Kathy Wu, David Miller, Nicolas Sonnerat, Denis Vnukov, Rory Greig, Jennifer Beattie, Emily Caveness, Libin Bai, Julian Eisenschlos, Alex Korchemniy, Tomy Tsai, Mimi Jasarevic, Weize Kong, Phuong Dao, Zeyu Zheng, Frederick Liu, Fan Yang, Rui Zhu, Tian Huey Teh, Jason Sanmiya, Evgeny Gladchenko, Nejc Trdin, Daniel Toyama, Evan Rosen, Sasan Tavakkol, Linting Xue, Chen Elkind, Oliver Woodman, John Carpenter, George Papamakarios, Rupert Kemp, Sushant Kafle, Tanya Grunina, Rishika Sinha, Alice Talbert, Diane Wu, Denese Owusu-Afriyie, Cosmo Du, Chloe Thornton, Jordi Pont-Tuset, Pradyumna Narayana, Jing Li, Saaber Fatehi, John Wieting, Omar Ajmeri, Benigno Uria, Yeongil Ko, Laura Knight, Amélie Héliou, Ning Niu, Shane Gu, Chenxi Pang, Yeqing Li, Nir Levine, Ariel Stolovich, Rebeca Santamaria-Fernandez, Sonam Goenka, Wenny Yustalim, Robin Strudel, Ali Elqursh, Charlie Deck, Hyo Lee, Zonglin Li, Kyle Levin, Raphael Hoffmann, Dan Holtmann-Rice, Olivier Bachem, Sho Arora, Christy Koh, Soheil Hassas Yeganeh, Siim Põder, Mukarram Tariq, Yanhua Sun, Lucian Ionita, Mojtaba Seyedhosseini, Pouya Tafti, Zhiyu Liu, Anmol Gulati, Jasmine Liu, Xinyu Ye, Bart Chrzaszcz, Lily Wang, Nikhil Sethi, Tianrun Li, Ben Brown, Shreya Singh, Wei Fan, Aaron Parisi, Joe Stanton, Vinod Koverkathu, Christopher A. ChoquetteChoo, Yunjie Li, TJ Lu, Abe Ittycheriah, Prakash Shroff, Mani Varadarajan, Sanaz Bahargam, Rob Willoughby, David Gaddy, Guillaume Desjardins, Marco Cornero, Brona Robenek, Bhavishya Mittal, Ben Albrecht, Ashish Shenoy, Fedor Moiseev, Henrik Jacobsson, Alireza Ghaffarkhah, Morgane Rivière, Alanna Walton, Clément Crepy, Alicia Parrish, Zongwei Zhou, Clement Farabet, Carey Radebaugh, Praveen Srinivasan, Claudia van der Salm, Andreas Fidjeland, Salvatore Scellato, Eri Latorre-Chimoto, Hanna Klimczak-Plucinska, David Bridson, Dario de Cesare, Tom Hudson, Piermaria Mendolicchio, Lexi Walker, Alex Morris, Matthew Mauger, Alexey Guseynov, Alison Reid, Seth Odoom, Lucia Loher, Victor Cotruta, Madhavi Yenugula, Dominik Grewe, Anastasia Petrushkina, Tom Duerig, Antonio Sanchez, Steve Yadlowsky, Amy Shen, Amir Globerson, Lynette Webb, Sahil Dua, Dong Li, Surya Bhupatiraju, Dan Hurt, Haroon Qureshi, Ananth Agarwal, Tomer Shani, Matan Eyal, Anuj Khare, Shreyas Rammohan Belle, Lei Wang, Chetan Tekur, Mihir Sanjay Kale, Jinliang Wei, Ruoxin Sang, Bren-"
        },
        {
            "title": "James",
            "content": "Thorne, Andreas Vlachos, Christos Christodoulopoulos, and Arpit Mittal. 2018. FEVER: large-scale dataset for fact extraction In Proceedings of the 2018 and VERification. Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long Papers), pages 809819, New Orleans, Louisiana. Association for Computational Linguistics. Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timothée Lacroix, Baptiste Rozière, Naman Goyal, Eric Hambro, Faisal Azhar, Aurelien Rodriguez, Armand Joulin, Edouard Grave, and Guillaume Lample. 2023. Llama: Open and efficient foundation language models. Preprint, arXiv:2302.13971. Quang-Duy Tran, Thai-Hoa Tran, and Khanh Quoc Tran. 2024. Advancing vietnamese fact extraction and verification through multi-stage text ranking. In 2024 International Conference on Multimedia Analysis and Pattern Recognition (MAPR), pages 17. William Yang Wang. 2017. \"liar, liar pants on fire\": new benchmark dataset for fake news detection. Preprint, arXiv:1705.00648. Moy Yuan and Andreas Vlachos. 2024. Zero-shot fact-checking with semantic triples and knowledge In Proceedings of the 1st Workshop on graphs. Knowledge Graphs and Large Language Models (KaLLM 2024), pages 105115, Bangkok, Thailand. Association for Computational Linguistics. Wanjun Zhong, Jingjing Xu, Duyu Tang, Zenan Xu, Nan Duan, Ming Zhou, Jiahai Wang, and Jian Yin. 2020. Reasoning over semantic-level graph for fact checking. In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 61706180, Online. Association for Computational Linguistics. Jie Zhou, Xu Han, Cheng Yang, Zhiyuan Liu, Lifeng Wang, Changcheng Li, and Maosong Sun. 2019. GEAR: Graph-based evidence aggregating and reasoning for fact verification. In Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages 892901, Florence, Italy. Association for Computational Linguistics. Liu Zhuang, Lin Wayne, Shi Ya, and Zhao Jun. 2021. robustly optimized BERT pre-training approach with post-training. In Proceedings of the 20th Chinese National Conference on Computational Linguistics, pages 12181227, Huhhot, China. Chinese Information Processing Society of China. nan Saeta, Tyler Liechty, Yi Sun, Yao Zhao, Stephan Lee, Pandu Nayak, Doug Fritz, Manish Reddy Vuyyuru, John Aslanides, Nidhi Vyas, Martin Wicke, Xiao Ma, Evgenii Eltyshev, Nina Martin, Hardie Cate, James Manyika, Keyvan Amiri, Yelin Kim, Xi Xiong, Kai Kang, Florian Luisier, Nilesh Tripuraneni, David Madras, Mandy Guo, Austin Waters, Oliver Wang, Joshua Ainslie, Jason Baldridge, Han Zhang, Garima Pruthi, Jakob Bauer, Feng Yang, Riham Mansour, Jason Gelman, Yang Xu, George Polovets, Ji Liu, Honglong Cai, Warren Chen, XiangHai Sheng, Emily Xue, Sherjil Ozair, Christof Angermueller, Xiaowei Li, Anoop Sinha, Weiren Wang, Julia Wiesinger, Emmanouil Koukoumidis, Yuan Tian, Anand Iyer, Madhu Gurumurthy, Mark Goldenson, Parashar Shah, MK Blake, Hongkun Yu, Anthony Urbanowicz, Jennimaria Palomaki, Chrisantha Fernando, Ken Durden, Harsh Mehta, Nikola Momchev, Elahe Rahimtoroghi, Maria Georgaki, Amit Raul, Sebastian Ruder, Morgan Redshaw, Jinhyuk Lee, Denny Zhou, Komal Jalan, Dinghua Li, Blake Hechtman, Parker Schuh, Milad Nasr, Kieran Milan, Vladimir Mikulik, Juliana Franco, Tim Green, Nam Nguyen, Joe Kelley, Aroma Mahendru, Andrea Hu, Joshua Howland, Ben Vargas, Jeffrey Hui, Kshitij Bansal, Vikram Rao, Rakesh Ghiya, Emma Wang, Ke Ye, Jean Michel Sarr, Melanie Moranski Preston, Madeleine Elish, Steve Li, Aakash Kaku, Jigar Gupta, Ice Pasupat, Da-Cheng Juan, Milan Someswar, Tejvi M., Xinyun Chen, Aida Amini, Alex Fabrikant, Eric Chu, Xuanyi Dong, Amruta Muthal, Senaka Buthpitiya, Sarthak Jauhari, Nan Hua, Urvashi Khandelwal, Ayal Hitron, Jie Ren, Larissa Rinaldi, Shahar Drath, Avigail Dabush, Nan-Jiang Jiang, Harshal Godhia, Uli Sachs, Anthony Chen, Yicheng Fan, Hagai Taitelbaum, Hila Noga, Zhuyun Dai, James Wang, Chen Liang, Jenny Hamer, Chun-Sung Ferng, Chenel Elkind, Aviel Atias, Paulina Lee, Vít Listík, Mathias Carlen, Jan van de Kerkhof, Marcin Pikus, Krunoslav Zaher, Paul Muller, Sasha Zykova, Richard Stefanec, Vitaly Gatsko, Christoph Hirnschall, Ashwin Sethi, Xingyu Federico Xu, Chetan Ahuja, Beth Tsai, Anca Stefanoiu, Bo Feng, Keshav Dhandhania, Manish Katyal, Akshay Gupta, Atharva Parulekar, Divya Pitta, Jing Zhao, Vivaan Bhatia, Yashodha Bhavnani, Omar Alhadlaq, Xiaolin Li, Peter Danenberg, Dennis Tu, Alex Pine, Vera Filippova, Abhipso Ghosh, Ben Limonchik, Bhargava Urala, Chaitanya Krishna Lanka, Derik Clive, Yi Sun, Edward Li, Hao Wu, Kevin Hongtongsak, Ianna Li, Kalind Thakkar, Kuanysh Omarov, Kushal Majmundar, Michael Alverson, Michael Kucharski, Mohak Patel, Mudit Jain, Maksim Zabelin, Paolo Pelagatti, Rohan Kohli, Saurabh Kumar, Joseph Kim, Swetha Sankar, Vineet Shah, Lakshmi Ramachandruni, Xiangkai Zeng, Ben Bariach, Laura Weidinger, Tu Vu, Alek Andreev, Antoine He, Kevin Hui, Sheleem Kashem, Amar Subramanya, Sissie Hsiao, Demis Hassabis, Koray Kavukcuoglu, Adam Sadovsky, Quoc Le, Trevor Strohman, Yonghui Wu, Slav Petrov, Jeffrey Dean, and Oriol Vinyals. 2024. Gemini: family of highly capable multimodal models. Preprint, arXiv:2312.11805. 14 Strict Accuracy in Fact-Checking Strict Accuracy: This metric is stringent measure that requires both the verdict and the evidence to be predicted correctly compared to the ground truth sample. Verdict (v and v): refers to the verdict of the sample and the predicted verdict (supported, refuted, nei). Evidence (e and e): refers to the evidence of the sample and the predicted evidence. Where: StrAcc = (v, v).f (e, e) (v, v) = (e, e) = (cid:40) 1 0 (cid:40) 1 0 if = otherwise if = otherwise (11) (12) (13) Strict accuracy is the average of all StrAcc values."
        },
        {
            "title": "B Hyperparameter and LLM Training Configuration",
            "content": "In this section, we present the detailed hyperparameter settings and training configurations for both our SemViQA models and the Large Language Model (LLM) fine-tuning process. Table 4 consolidates all hyperparameters used across different models, including Binary Classification (BC), Three-Class Classification (TC), Question Answering Token Classifier (QATC), and LLM fine-tuning. Hyperparameter Epochs RT Loss Cross-Entropy Loss Focal Loss Learning Rate Batch Size Gradient Accumulation Optimizer (AdamW) Max Token Length GPUs Zero LR Schedule Mixed Precision BC 20 - - 1e5 104 1 256 A100 - QATC 20 - 2e6 36 2 512 A100 - TC 20 - - 1e5 104 1 256 A100 - Linear Linear Cyclic - - - LLM 1 - - - 5e5 2 1 4096 4 H100 Zero3 Cosine bf16 Table 4: Consolidated hyperparameter and training configuration for SemViQA models and LLM fine-tuning. We fine-tune Large Language Model (LLM) using restructured version of the original datasets, ViWikiFC and ISE-DSC01, as detailed in Figure 5. These datasets have been carefully adapted for training to improve performance and ensure compatibility with our model. For training, we utilize the official Qwen LLM implementation from the QwenLM repository4. Our training setup follows the full configuration outlined in Table 4, ensuring optimal efficiency and alignment with best practices. 4https://github.com/QwenLM/Qwen Question: You are tasked with verifying the correctness of the following statement. - We provide you with claim and context. Please classify the claim into one of three labels: SUPPORTED, REFUTED, or NEI (Not Enough Info). - Your answer should include the classification label and the most relevant evidence sentence from the context. - Remember, the evidence must be full sentence, not part of sentence or less than one sentence. Given claim and context as follows: Context: The actress revealed her secrets to maintaining youthful appearance as follows: Eating three balanced meals day. For dinner, Ivy Chen usually eats early to ensure her body has enough time to digest food, metabolize energy, and avoid putting pressure on the stomach and other organs. recent study published in *Frontiers in Nutrition* suggests that eating dinner earlier can lead to longer lifespan, with the ideal time being 7 PM. If this is not possible, experts recommend having the last meal of the day 2-3 hours before bedtime. Drinking ginger tea: To keep her body warm, promote blood circulation, and enhance circulation, Ivy Chen drinks ginger tea daily. Her ginger tea is typically made with ground ginger, black tea, turmeric powder, and brown sugar. This drink is natural remedy that not only boosts the immune system and reduces inflammation but also fights oxidation, supports weight loss, improves skin health, and helps maintain youthful look. Regular exercise: Ivy Chen is fitness enthusiast who loves physical activities and exercises daily, even during pregnancy. The Taiwanese actress shared that if she is not busy with work, she runs for at least 30 minutes every day. Even when traveling abroad, she maintains her running habit. recent study published in *Progress in Cardiovascular Disease* found that regular runners live three years longer than non-runners. Running significantly helps with weight loss, maintaining balanced physique, toning muscles, relaxing the mind, and benefiting heart health. Besides running, Ivy Chen also swims, practices yoga, and hikes to maintain physical fitness and endurance. Skincare: Regarding her skincare routine, the actress emphasized the importance of hydration. The Taiwanese beauty revealed that she always carries facial mist to ensure her skin stays hydrated while outdoors. Claim: Even when traveling abroad, Ivy Chen maintains her running habit. Answer: This claim is classified as SUPPORTED. The evidence is: Even when traveling abroad, she maintains her running habit. Table 5: Example of fact-checking task prompt used for LLM training. Note: Some parts of the Context and Claim were originally in Vietnamese. In this paper, we have translated them into English for better readability. Sentences highlighted in blue indicate the evidence. We present the complete training progress of the LLM models and QATC in Figure 7 and Figure 6, respectively. Figure 7 illustrates the training dynamics of Qwen 1.5B and Qwen 3B, supporting the results presented in Table 2. Notably, the Qwen 1.5B model demonstrates more stable training dynamics compared to the Qwen 3B model during the initial stage. Meanwhile, Figure 6 showcases the completion of QATC training, depicting the loss curves of ViMRClarge and InfoXLMlarge. These results highlight the convergence behavior of QATC training across different architectures, further supporting the robustness of our approach. 16 Figure 6: Training progress of the ViMRClarge and InfoXLMlarge models. Figure 7: Training progress of the Qwen 1.5B and Qwen 3B models. 17 Comparison of TF-IDF and QATC in Fact-Checking: Examples of Incorrect vs."
        },
        {
            "title": "Correct Evidence Selection",
            "content": "Claim Du lịch Triều Tiên là điều mà chỉ có một số người được đi đến. (Traveling to North Korea is something only few people can do.) Nó có độ nóng chảy ở mức gần 30 độ C. (It has melting point of about 30C.) Evidence Theo nguyên tắc, bất kỳ ai cũng được phép du lịch tới Triều Tiên, và những ai có thể hoàn thành quá trình làm thủ tục thì đều không bị Triều Tiên từ chối cho nhập cảnh. (In principle, anyone is allowed to travel to North Korea, and those who complete the process are not denied entry.) Nó là một kim loại kiềm mềm, màu bạc, và với điểm nóng chảy là 28 (83 F) khiến cho nó trở thành một trong các kim loại ở dạng lỏng tại hay gần nhiệt độ phòng. (It is soft, silvery alkali metal with melting point of 28C (83F), making it one of the metals that is liquid at or near room temperature.) TF-IDF Khách du lịch không được đi thăm thú bên ngoài vùng đã được cho phép trước mà không được hướng dẫn viên người Triều Tiên cho phép nhằm tránh các điệp viên nằm vùng. (Tourists are not allowed to visit areas outside of the designated zones without North Korean guide to prevent undercover spies.) Nó là nguyên tố có độ âm điện thấp thứ hai sau franci, và chỉ có một đồng vị bền là caesi-133. (It is the second least electronegative element after francium, and has only one stable isotope, cesium-133.) QATC Theo nguyên tắc, bất kỳ ai cũng được phép du lịch tới Triều Tiên, và những ai có thể hoàn thành quá trình làm thủ tục thì đều không bị Triều Tiên từ chối cho nhập cảnh. (In principle, anyone is allowed to travel to North Korea, and those who complete the process are not denied entry.) Nó là một kim loại kiềm mềm, màu bạc, và với điểm nóng chảy là 28 (83 F) khiến cho nó trở thành một trong các kim loại ở dạng lỏng tại hay gần nhiệt độ phòng. (It is soft, silvery alkali metal with melting point of 28C (83F), making it one of the metals that is liquid at or near room temperature.) Table 6: Comparison of TF-IDF and QATC in Fact-Checking: TF-IDF selects irrelevant evidence (Incorrect), while QATC selects accurate evidence (Correct)."
        }
    ],
    "affiliations": [
        "FPT Software AI Center, Viet Nam",
        "FPT Telecom, Viet Nam",
        "Faculty of Information Technology, Industrial University of Ho Chi Minh City, Viet Nam",
        "Faculty of Information Technology, University of Science, VNU-HCM, Viet Nam"
    ]
}