{
    "paper_title": "ResTok: Learning Hierarchical Residuals in 1D Visual Tokenizers for Autoregressive Image Generation",
    "authors": [
        "Xu Zhang",
        "Cheng Da",
        "Huan Yang",
        "Kun Gai",
        "Ming Lu",
        "Zhan Ma"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Existing 1D visual tokenizers for autoregressive (AR) generation largely follow the design principles of language modeling, as they are built directly upon transformers whose priors originate in language, yielding single-hierarchy latent tokens and treating visual data as flat sequential token streams. However, this language-like formulation overlooks key properties of vision, particularly the hierarchical and residual network designs that have long been essential for convergence and efficiency in visual models. To bring \"vision\" back to vision, we propose the Residual Tokenizer (ResTok), a 1D visual tokenizer that builds hierarchical residuals for both image tokens and latent tokens. The hierarchical representations obtained through progressively merging enable cross-level feature fusion at each layer, substantially enhancing representational capacity. Meanwhile, the semantic residuals between hierarchies prevent information overlap, yielding more concentrated latent distributions that are easier for AR modeling. Cross-level bindings consequently emerge without any explicit constraints. To accelerate the generation process, we further introduce a hierarchical AR generator that substantially reduces sampling steps by predicting an entire level of latent tokens at once rather than generating them strictly token-by-token. Extensive experiments demonstrate that restoring hierarchical residual priors in visual tokenization significantly improves AR image generation, achieving a gFID of 2.34 on ImageNet-256 with only 9 sampling steps. Code is available at https://github.com/Kwai-Kolors/ResTok."
        },
        {
            "title": "Start",
            "content": "ResTok: Learning Hierarchical Residuals in 1D Visual Tokenizers for Autoregressive Image Generation Xu Zhang1,2,* Cheng Da2 Huan Yang2, Kun Gai2 Ming Lu1, Zhan Ma1 1Vision Lab, Nanjing University 2Kolors Team, Kuaishou Technology 6 2 0 2 ] . [ 1 5 5 9 3 0 . 1 0 6 2 : r a"
        },
        {
            "title": "Abstract",
            "content": "Existing 1D visual tokenizers for autoregressive (AR) generation largely follow the design principles of language modeling, as they are built directly upon transformers whose priors originate in language, yielding single-hierarchy latent tokens and treating visual data as flat sequential token streams. However, this language-like formulation overlooks key properties of vision, particularly the hierarchical and residual network designs that have long been essential for convergence and efficiency in visual models. To bring vision back to vision, we propose the Residual Tokenizer (ResTok), 1D visual tokenizer that builds hierarchical residuals for both image tokens and latent tokens. The hierarchical representations obtained through progressively merging enable cross-level feature fusion at each layer, substantially enhancing representational capacity. Meanwhile, the semantic residuals between hierarchies prevent information overlap, yielding more concentrated latent distributions that are easier for AR modeling. Cross-level bindings consequently emerge without any explicit constraints. To accelerate the generation process, we further introduce hierarchical AR generator that substantially reduces sampling steps by predicting an entire level of latent tokens at once rather than generating them strictly token-by-token. Extensive experiments demonstrate that restoring hierarchical residual priors in visual tokenization significantly improves AR image generation, achieving gFID of 2.34 on ImageNet-256 with only 9 sampling steps. Code is available at https://github.com/Kwai-Kolors/ResTok. 1. Introduction Autoregressive (AR) modeling has recently become strong paradigm for high-quality visual generation and shows promise for unified multi-modal modeling. By predicting visual tokens sequentially, AR models inherit the scalability and controllability of language modeling. Their *Work done while interning at Kuaishou Technology. Project leader. Corresponding author: <minglu@nju.edu.cn>. (a) Query along depth. (b) Query along depth and hierarchy. Figure 1. Comparison between (a) existing 1D tokenizers [16, 22, 25, 52] querying features along only depth and (b) ResTok querying along both depth and hierarchy. By progressively merging image tokens, ResTok brings multi-scale hierarchies back to the ViTbased tokenizer, which encourages implicit alignments between image tokens and latent tokens and enforces better causalities of latent tokens for AR generation. effectiveness, however, depends critically on how visual signals are tokenized, since tokenizers define the semantic dependencies AR models can learn and the reconstruction quality decoders can achieve. Auto-Encoding (AE) [14] naturally supports this process by learning compact latent representations. Its extensions, such as VAEs [17], hierarchical VAEs [10, 18, 36], and VQ-VAEs [42], have substantially expanded representational capacity and become core components of modern generative models. Although pixellevel AR models [4, 40, 41] demonstrated strong performance, AE-based tokenizers remain essential for reducing dimensionality and capturing semantic structure. Contemporary frameworks therefore integrate AEs to improve fidelity and efficiency [7, 32]. Within the Vision Transformer (ViT) paradigm [6, 43], this approach becomes particularly appealing, as images can be represented as sequences of latent tokens aligned with language-model-style training. As result, tokenizer design emerges as central challenge for further advancing AR visual generation. To obtain 1D sequences for AR modeling, early visual tokenizers [7, 19, 49] typically flattened 2D AE latents using raster scans or similar heuristics. Such strategies, 1 however, are misaligned with AR causality at scan turning points where spatial continuity breaks down. To overcome this, later approaches abandon rigid spatial ordering and seek non-spatial token dependencies instead which are more compatible with AR modeling. Beyond multi-scale 2D tokenization [39], another promising direction is 1D tokenization [8, 52]. By discarding fixed spatial grids, querybased 1D tokenizers learn abstract semantics in sequential form that aligns with AR prediction and resembles language modeling. Subsequent studies attempt to impose token causality by assigning levels to frequency bands [16] or spatial resolutions [25], but such designs rely on nonsemantic hand-crafted rules. Other methods introduce diffusion decoders to strengthen semantic learning [1, 46], yet the dual stochastic processes (i.e., AR and diffusion) complicate optimization and lead to instability when scaling to longer token sequences. Despite these advances, existing 1D tokenizers still face two main challenges: (1) Lack of cross-level fusion. Most methods [1, 8, 16, 25, 47, 52] extract features from lowto high-level solely along network depth, but cannot fuse features from multiple levels at certain layer. This is in contrast to feature-fusion studies [23, 37], where cross-level fusion is known to be crucial for strong visual representation. (2) High codebook entropy. Since redundancy between latent tokens is rarely addressed, current approaches often produce similar embeddings in the codebook, yielding relatively uniform probabilities. Such high-entropy codebooks are unfriendly for AR modeling and may hinder generation performance. We argue that these challenges stem from the ignorance of the intrinsic difference between vision and language. Existing methods adopt the same isotropic design as transformers, while vision properties like hierarchical residuals are gradually discarded as illustrated in Fig. 1. To better uncover what enables efficient tokenization and generation, we introduce the Residual Tokenizer (ResTok) and identify three key designs: Hierarchical representations enhance representational capacities, especially with multiple scales. To make the hierarchical design compatible with ViT-based tokenizers, we progressively merge image tokens into coarser features and insert them at the beginning of the token sequence. This allows latent tokens to fuse in-context features with image tokens across hierarchies. Semantic residuals between hierarchies concentrate latent distributions. Unlike hand-crafted constraints [16, 25] or additive residuals [22, 39], ResTok learns residuals in semantically structured way. By guiding the model to accumulate compensatory visual features, ResTok reduces the information overlap, resulting in lower-entropy codebooks that are easier for AR modeling. Accelerated generation is enabled by proposing hierarchical AR (HAR) variant of LlamaGen [38] upon ResTok. Switching from next-token prediction to next-hierarchy prediction, the HAR generator significantly reduces sampling steps with acceptable degradation of generation performance. By learning these visual properties, cross-level bindings emerge without explicit constraints: coarser latent tokens align with high-level image tokens, while finer latents capture low-level residual details. Coupled with LlamaGenL [38], ResTok achieves state-of-the-art AR generation performance on the ImageNet 256256 benchmark [5], reaching gFID of 2.34 with only 9 sampling steps. 2. Related Work 2.1. Visual Tokenization Autoregressive visual generation hinges on effective tokenization. Early methods simply convert grid-based 2D latents from autoencoders into 1D sequences using raster scans [7, 19, 42, 49, 51]. Innovations like SPAE [50] explicitly aligns token hierarchies with semantic structures, underscoring the importance of cross-modal alignment. However, these approaches may disrupt autoregressive causality at scan turning points. To address this fundamental mismatch, query-based 1D visual tokenization techniques have emerged, which can learn naturally sequential tokens. Notably, SEED [8] and TiTok [52] learn 1D latent sequences directly from image patches, aligning token order with abstract semantics rather than spatially matched tokens [2]. SpectralAR [16] and DetailFlow [25] further refine token causality by explicitly linking token length to frequency bands or spatial resolutions, encouraging shorter sequences to represent coarse visual features and longer ones to capture details. However, these methods rely on handcrafted constraints, reducing flexibility. ImageFolder [22] utilizes residual quantization [19, 39] with random drop of latent tokens to form multi-scale latent scheme, but the hard additive residual design may not be optimal from the semantic perspective. In contrast, GigaTok [47] introduces latent hierarchies by applying progressive latent initialization at the input stage, while VFMTok [55] directly uses learnable tokens to query single-scale visual features from multiple levels of pre-trained foundation model. 2.2. Autoregressive Image Generation In the realm of AR visual generation, foundational works begin with pixel-level AR models [4, 40, 41], but these often struggle with efficiency due to high-dimensional input. More recent studies have shifted focus toward discrete latent token generation using VQ-VAE [42] and its variants [7, 19, 39], enabling powerful transformer-based AR models. VAR [39] introduces coarse-to-fine generation, while FlowAR [31] integrates flow matching [24] Infinity [11] explores to model inter-scale dependencies. Figure 2. Overview of ResTok. (a) Pipeline of encoding and decoding processes. There are 1 residual merging blocks uniformly replacing the original transformer blocks in the encoder, where denotes the number of scales. (b) Residual 1D latent token initialization. When increasing the target size of pooling, we first double the width, and then alternately double the height and width in subsequent steps. (c) Residual merging block. Average pooling is used as the merging method in our experiments. long-range refinement strategies for high-resolution generation. MaskGIT [3] enables random prediction order, and MAR [21] eliminates the need of VQ for AR generation. Despite these advances, the representative AR generation paradigm LlamaGen [38] still attracts the main focus of the community, becoming the foundation of many following works [25, 45, 47, 55], as its simplicity and capability of integration with unified multi-modal models. Thus, in our work, we use LlamaGen as our testbed and propose hierarchical variant for acceleration. 3. Residual Tokenizer 3.1. Pipeline Overview In contrast to conventional 2D tokenizers [7, 42, 49] used for AR generation, 1D tokenizers learn sequential latent tokens that query visual features directly from grid-structured image tokens. As shown in Fig. 2a, for the encoding process, given an input image RHW 3, CNN encoder first transforms into initial image tokens p(0) C, downsampled by factor of . Here, the superscript (0) denotes the input features of the ViT encoder or decoder, while (n) later refers to the output features at the n-th transformer layer. The image tokens are then flattened and fed into ViT encoder E() together with set of latent tokens z(0) 1:L initialized from p(0), where the subscript 1:L indicates the indices of the hierarchies. These latent tokens iteratively query and refine visual features across layers. After layers, the encoder outputs the final im1:L = VectorQuant(z(N ) age tokens p(N ) and latent tokens z(N ). The latent tokens are quantized via ˆz(0) 1:L ; C), where is the codebook, and the quantized latents ˆz(0) 1:L serve as the representation used for reconstruction and generation. For the decoding process, set of masked image tokens m(0) initiates the inverse querying procedure. ViT decoder D() retrieves features from ˆz(0) 1:L and outputs the restored image tokens m(N ) img . The reconstructed image ˆx is produced by CNN decoder from m(N ) img . img 3.2. Hierarchical Representations in ViT As shown in Fig. 1a, previous works [1, 16, 22, 25, 47, 52, 55] adopt single-hierarchy image tokens for tokenizers, limiting latent tokens to capturing hierarchical features from other levels. To this end, we propose progressive merging in isotropic ViT to learn hierarchical representations. Akin to classical pyramid architectures [12, 23, 37], intermediate features are progressively merged into smaller scales at specific layers, structuring multiple stages throughout the tokenizer. Specifically, we replace normal ViT blocks with residual merging blocks every N/S layers except for the last layer as shown in Fig. 2c, where denotes the number of transformer depth and stands for the stage count. The multi-scale representations are denoted as {p1, . . . , pS} in coarse-to-fine order. At n-th layer, after the self-attention operation, the s-th-scale feature p(n) is merged into coarser scale p(n) s1. Compared to querying 3 Figure 3. Representation alignment. The image is processed by VF model to get the [CLS] token [CLS] and the visual tokens of image patches patch and mask 1 VF tokens m(N ) , respectively. vf . The coarsest image tokens p(N ) vf are aligned with [CLS] and patch vf vf vf features along the transformer depth illustrated in Fig. 1, this design makes the representations in ResTok across all scales accessible, which is beneficial to the hierarchical latent tokens for querying multi-level features. Inspired by TiTok [52], we adopt in-context learning paradigm rather than the Q-Former [20] architecture in GigaTok [47] and VFMTok [55], since image tokens should evolve through tokenization to progressively extract multiscale features. Additionally, we apply encoder attention masks to restrict the coarser scales from accessing the finer scales, enforcing causalities across hierarchies of both image and latent tokens. Note that the decoder has no hierarchical design or attention mask for simplicity. We use average pooling as the merging operation in our experiments. 3.3. Semantic Residuals Some studies [47, 55] introduce multi-level image or latent tokens by naively stacking visual representations, but they often overlook the substantial information overlap between levels. This redundancy produces similar codebook embeddings and high entropy, which is unfavorable for AR modeling. Although methods such as VAR [39] and ImageFolder [22] add residuals at the quantization bottleneck, these residuals are not accumulated semantically along the token sequence and thus fail to bind clear semantic attributes to latent tokens. To address these issues, we propose semantic residuals for both image and latent tokens. For latent tokens, we apply residual initialization at the input stage. As shown in Fig. 2b, the number of latent tokens increases exponentially across hierarchical levels, except for the first two levels [47]. This results in nested growth of token length across levels. To introduce residuals on top of hierarchical latent tokens, we do not always pool the feature map p(0) directly to each target level length. Instead, inspired by the iterative approach in VAR [39], we upsample the pooled feature back to the original size of p(0), subtract p(0) from the upsampled feature to obtain the residual, and then pool the residual to generate latent toFigure 4. Hierarchical autoregressive generator. The numbers in the colored tokens stand for the indices of the latent tokens. [Mi] denotes the mask token filled at the i-th missing position. kens. This residual formulation provides an initial guidance during training and prevents excessive information overlap among latent tokens. Similar operations are also been done for image tokens. At n-th layer, p(n) is subtracted from the upsampled p(n) s1 to obtain the residual relative to p(n) s1 rather than keeping the original image tokens in the sequence as shown in Fig. 2c. 3.4. Optimization Strategies Representation alignment [48, 53] with pre-trained vision foundation (VF) model is incorporated in ResTok for faster convergence. Different from existing aligned 1D tokenizers [25, 55], we apply alignment to both the encoder and the decoder as shown in Fig. 3. At the encoder side, we apply global average pooling to the coarsest output hierarchy of image tokens p(N ) and align it to the [CLS] token of DINOv3-L [35] via linear layer ϕenc() and Eq. (1) to guide the residual merging process. At the decoder side, we double the training batch, replace half of the mask image tokens m(0) img with mask VF tokens m(0) [55], and align the corresponding output m(N ) vf with the visual tokens of DINOv3-L [35] through linear layer ϕdec() and Eq. (2), which can preserve semantics at the quantization bottleneck. The VF loss Lvf can be formally written as vf 1 Lenc = ReLU(δenc CosSim(p(N ) Ldec = ReLU(δdec CosSim(m(N ) Lvf = λencLenc + λdecLdec, vf 1 ), ϕenc(f [CLS] , ϕdec(f patch vf vf ))), ))), (1) (2) (3) where ReLU() and CosSim(, ) denote clamping and cosine similarity, respectively. λenc and λdec control the tradeoff between Lenc and Ldec. We set margins δenc and δdec in Eqs. (1) and (2) to control the similarities [48], both fixed to 0.85 across experiments. Ablations in Sec. 5.4 validate the effectiveness of this co-design of Lvf. To keep ResTok simple, we do not tie the latent tokens to manually decided spatial resolutions [25] or frequency bands [16]. Instead, we optimize each latent hierarchy to the same training objectives Eq. (4) with commonly used MSE loss Lmse, perceptual loss [54] Lpercp, GAN loss [9] Table 1. System-level comparison of reconstruction and class-conditional generation on ImageNet 256256. Mask. and Diff. stand for masked generation and diffusion. #Tokens: the number of tokens needed to represent an image. #Steps: the number of sampling steps needed for generation. : Training set includes data besides ImageNet. : Without classifier-free guidance. : Tokenizers are initialized with pre-trained vision foundation models. : Images are downsampled from larger sizes than 256256. : Results are of 32 tokens. Method Tokenizer Generator Type #Param. #Tokens rFID Type #Param. #Steps gFID IS Pre. Rec. KL LDM-4-G [32] DiT-XL/2 [30] KL LightningDiT-XL [48] KL KL MAR-B [21] KL FlowAR-B [31] 55M 84M 70M 66M 66M Grid-Based Tokenization VQGAN [7] RQTran. [19] MaskGIT [3] VAR-d16 [39] LlamaGen-L [38] PAR-L-4 [45] IBQ-B [34] VQ 23M RQ 66M 66M VQ MSRQ 109M 72M VQ 72M VQ 128M IBQ Query-Based Tokenization TiTok-L-32 [52] FlexTok d18-d18 [1] ImageFolder [22] GigaTok-B-L [47] SpectralAR-d16 [16] DetailFlow-16 [25] VFMTok [55] ResTok (Ours) 641M VQ FSQ 950M MSRQ 176M 622M VQ - VQ 271M VQ - VQ 662M VQ Continuous Token Modeling 0.27 0.62 0.28 0.87 0.87 Diff. Diff. Diff. Mask.+Diff. VAR+Flow 400M 675M 675M 208M 300M Discrete Token Modeling 4.98 3.20 2.28 0.90 0.94 0.94 1.37 2.21 1.61 0.80 0.81 4.03 1.22 0.89 1.28 AR AR Mask. VAR AR PAR AR Mask. AR+Flow VAR AR AR PAR AR HAR 1.4B 3.8B 227M 310M 343M 343M 342M 177M 1.33B 362M 111M 310M 326M 343M 326M 4096 1024 256 256 256 256 256 256 680 576 576 256 32 1-256 286 256 64 128 256 128 250 250 250 64 5 256 68 8 10 576 147 8 26-281 10 256 64 23 256 9 3.60 2.27 1.35 2.31 2.90 15.78 7.55 6.18 3.30 3.07 3.76 2.88 2.77 2.02 2.60 3.26 3.02 2.96 2.75 2.34 247.7 278.2 295.3 281.7 272.5 74.3 134.0 182.1 274.4 256.1 218.9 254. 199.8 - 295.0 221.0 282.2 221.4 278.8 257.8 - 0.83 0.79 0.82 0.84 - - 0.80 0.84 0.83 0.84 0.84 - - 0.75 0.81 0.81 0.82 0.84 0.79 - 0.57 0.65 0.57 0.54 - - 0.51 0.51 0.52 0.50 0. - - 0.63 0.56 0.55 0.57 0.57 0.60 Lgan and VF loss Lvf: Ltotal = λmseLmse + λpercpLpercp + λganLgan + λvfLvf, (4) where λmse, λpercp, λgan and λvf balance the loss terms, making the tokenizer adaptively and implicitly decide the optimal visual features of certain length. This implicit method can also encourage semantic accumulation along the residual token sequence rather than non-semantic information. Moreover, we do not explicitly tie any latent token group to certain image hierarchy, which encourages selfalignment of image and latent hierarchies. To further promote this self-alignment property, we apply nested dropout of latent hierarchies [1, 22, 25, 29], which can guide the tokenizer to learn essential visual features needed for reconstruction at each semantic level, aligning with our multiscale hierarchical designs. 4. Hierarchical Autoregressive Generation The original LlamaGen [38] adopts the next-token prediction (NTP) paradigm, hindering the generation speed with long sequences. While ResTok is capable of NTP, we also develop hierarchical autoregressive (HAR) generator tailored to ResToks hierarchical design to further boost the speed of AR generation. As illustrated in Fig. 4, the generation process can be divided into two parts, vanilla AR generation and HAR genIn the vanilla AR generation phase, group of eration. latent tokens is predicted in an NTP manner. These tokens perform as initialization for the following HAR prediction, reducing accumulation of sampling error in the beginning [25]. In the HAR generation phase, the first HAR group has only one predicted token accompanied with special mask tokens, whose sum equals to the number of tokens in the next hierarchy of ResTok. Different from PAR [45] and DetailFlow [25], each hierarchy in ResTok has different number of latent tokens, so we need to add mask tokens to each group to reach the next hierarchys token count. In the training process, hierarchical grouped attention mask is applied, while the optimization objective remains the same as LlamaGen [38]. In our experiments, the number of NTP tokens equals to the number of minimal remaining tokens in nested token dropout training [1, 22, 25, 29]. 5 Figure 5. Visualizations of reconstructions with various token lengths and attention weights in the encoder. The first 16 latent tokens are more closely associated with the coarser image scales S1 and S2, capturing high-level semantics (e.g., object, position, color, etc.). In contrast, the subsequent latent tokens progressively refine fine-grained details, primarily querying the finer image tokens from S3 and S4. 5. Experiments 5.1. Experimental Settings Implementation Details. ResTok builds on TiTok-L [52], incorporating 128 latent tokens, codebook with 8,192 entries and dimension of 8, CNN encoder-decoder pair [47], nested token dropout [1, 22, 25, 29] (the number of minimal remaining tokens is set to 4), DINO discriminator [39], and M-RoPE [44]. These updates yield strong baseline for the proposed modules in Sec. 3 and our ablation study. For the main results, ResTok is trained on ImageNet training set [5] at 256256 for 200 epochs with adversarial training beginning at step 20K, and LlamaGen-L [38] is trained under HAR scheme for 300 epochs. For the ablations, ResTok and LlamaGen-L are trained on ImageNet for 30 epochs and 50 epochs, respectively. For both tokenizer and generator, we use batch size of 256, AdamW optimizer [28], an initial learning rate of 1 104 with one-epoch linear warm-up, and cosine decay to 1 105 thereafter. In our experiments, all merging, pooling and upsampling operations use nearest interpolation. More details can be found in Sec. A. Evaluation Metrics. We utilize Frechet Inception Distance (FID) [13], Inception Score (IS) [33], Precision, and Recall as metrics for assessing reconstruction and generation performance. Since all of the ResTok variants in the ablation study achieve 100% codebook utilization, we report the codebook entropy HC instead as better indicator to examine how various settings affect the concentration of the latent distribution and its correlation with FID. 5.2. Quantitative Results We compare the proposed ResTok with recent representative methods across continuous and discrete token modeling paradigms in Tab. 1. From the perspective of discrete methods, query-based visual tokenizers generally achieve better gFID, often reaching below 3.0 gFID with 300M generator. Meanwhile, rFID remains competitive when scaling up model capacity and latent sequence length, with around 128 latent tokens typically enabling rFID scores near 1.0. This trend highlights that query-based tokenizers align more naturally with AR image generation. Among query-based tokenizers, ResTok enables the accelerated HAR generator to achieve state-of-the-art 2.34 gFID with only 9-step sampling, outperforming both prior query-based methods with stronger rFID [22, 47, 55] and other accelerated AR models that rely on longer latent sequences [22, 25, 39, 45]. More concretely, although ResToks rFID is slightly higher than DetailFlow [25], which also uses 128 latent tokens, ResTok benefits from its semantically organized codebook, enabling easier AR modeling and significantly improving gFID while requiring far fewer sampling steps. Compared to ImageFolder [22], ResTok attains better gFID and sampling efficiency, yet uses only 128 latent tokens instead of 286, demonstrating substantially more compact and efficient representation. Furthermore, despite operating under pure AR framework, ResTok and HAR remain competitive with recent hybrid (masked) AR and diffusion methods [1, 21, 31], highlighting the effectiveness of reinstating hierarchical residual priors in 1D visual tokenization. 5.3. Qualitative Results By learning semantic hierarchical residuals, ResTok exhibits coherent semantic stacking behavior as shown in Fig. 5. The model reconstructs images in coarse-to-fine manner where each additional group of latent tokens contributes semantically meaningful refinements, such as ob6 Figure 6. Visualizations of generated 256256 samples on ImageNet-1K. By enhancing the representation capabilities of the tokenizer and constraining the causal dependencies among latent tokens, ResTok enables the AR generator to produce high-quality and diverse images. ject identity, spatial layout, color composition, and finally textural and boundary details. This is distinctly different from SpectralAR [16] and DetailFlow [25], where the refinement stages primarily operate on frequency bands or low-level textures without establishing clear semantic ordering. The emergent property observed in ResTok suggests that its latent tokens are more aligned with semantic attributes, enabling more controllable generation. To further understand the underlying mechanisms of hierarchical residuals in ResTok, we visualize the encoder attention maps in Fig. 5. By comparing the reconstructed images from different token lengths with their corresponding attention maps, we can observe clear alignment between the scales of image tokens and the represented content. The first 16 latent tokens primarily encode abstract semantic information, which corresponds to the coarser image scales p1 and p2 (i.e., S1 and S2 in Fig. 5). As the token sequence progresses, the later latent tokens gradually refine fine-grained details, mainly supported by the finer image scales p3 and p4 (i.e., S3 and S4 in Fig. 5). Additionally, the attention maps in Fig. 5 show that the coarsest scale S1 of image tokens act as global semantic source, which the latent tokens query most. The rest scales of image tokens compensate residuals to the latent tokens, naturally exhibiting coarse-to-fine transition property. It reveals that the hierarchical residual properties are essential for the tokenizer to capture information at distinct semantic levels. Such latent tokens organized by semantics with lowentropy codebook are also more amenable to modeling by the AR generator, such as LlamaGen [38], enabling highquality and diverse image generation as shown in Fig. 6. Table 2. Ablation study on the network designs. The pooling factors of hierarchical image tokens are fixed to 2 by default."
        },
        {
            "title": "ID Setting",
            "content": "rFID gFID HC 1 2 3 4 5 6 7 8 Baseline + Hierarchical Latent Tokens 1.87 1.86 + Hierarchical Image Tokens"
        },
        {
            "title": "2 Hiera.\n3 Hiera.\n4 Hiera. (default)",
            "content": "+ Residual Tokens Image Tokens Latent Tokens Both (default) 1.71 1.70 1.67 1.86 2.02 2.11 6.01 5.39 5.41 5.53 6.58 5.64 4.78 4. 11.89 11.90 12.12 11.91 11.47 11.58 10.58 8.79 5.4. Ablation Study To thoroughly analyze the effectiveness of the proposed modules in ResTok, we conduct series of ablations based on the improved baseline as described in Sec. 5.1. Unless otherwise specified, gFID is generated by vanilla AR generation without classifier-free guidance (CFG) [15]. Hierarchical Residuals. We begin with the network designs of hierarchical residuals, resulting in Tab. 2. The principles can be roughly divided into two parts: hierarchies and residuals. The former enhances representation capabilities for better reconstruction, and the latter concentrates latent distributions for lower gFID. Applying hierarchies to latent tokens (i.e., setting #2) explicitly enforces the causality, improving gFID over the baseline even without residuals. Further adding hierarchies to image tokens (i.e., settings #3 to #5) significantly boosts the performance Table 3. Ablation study on the pooling factor in all hierarchies of image tokens. The number of hierarchies is set to 4 by default. Pooling Factor rFID gFID HC 1 (w/o Pooling) 2 (Default) 4 1.89 2.11 1. 5.81 4.56 4.70 10.32 8.79 10.17 Table 4. Ablation study on the alignment positions. Alignment Position Encoder Decoder rFID gFID HC (Setting #8 w/o alignment) 2.41 2.19 1.91 2.11 11.59 7.56 7.76 4. 7.99 9.49 10.31 8.79 of reconstruction. By ablating the number of hierarchies, we find that the tokenizer with 4 hierarchies, which is also typical configuration of conventional hierarchical neural networks [12, 26, 27], strikes balance between rFID and complexity. Then we explore the most suitable residual settings, i.e., settings #6 to #8. It shows that applying residuals to image tokens and latent tokens simultaneously performs best, with the lowest codebook entropy HC and gFID. We also ablate the best pooling factor of residual merging in Fig. 2c. Tab. 3 reveals that merging image tokens with pooling factor of 2 yields the best generation performance among the tested settings. This configuration provides moderate level of abstraction compared with no pooling, while avoiding the excessive semantic loss at the smallest scale of image tokens observed with 4 pooling. By conducting the ablations above, we obtain the optimal designs for ResTok which are also used in the main experiments. We also conclude the following key findings: (1) Codebook entropy HC matters. Though codebook utilization reflects the ceiling of reconstruction, HC is more important indicator for generation. higher value of HC means that the latent distribution is more dispersed, which is harder for generator to model, yielding poorer gFID. (2) Hierarchies significantly enhance representation capacities, but the tokenizer is still suffering from high value of HC and poor generation performance. (3) Residuals guide the tokenizer to add compensatory information around the latent centroids, avoiding dispersing the latent distributions. Representation Alignment. As semantic guidance, the designs of representation alignment affect the convergence. We ablate the alignment positions on setting #8, resulting in Tab. 4. It demonstrates that aligning representations solely on either the encoder or decoder side is suboptimal, an aspect unexplored in prior work [25, 47, 48, 55]. Alignments should be applied to the encoder to guide feature extraction, and to the decoder to preserve semantics in the quantization Table 5. Ablation study on the hierarchical AR generator. AR Type Vanilla AR Hiera. AR #Steps gFID IS Pre. Rec. 4.56 142.2 0.79 0.56 w/o NTP group w/ NTP group 8 5.85 5.53 130.4 130.9 0.78 0.78 0.55 0.56 Figure 7. Reconstruction and generation performance versus tokenizer training iterations. bottleneck, both contributing to improved performance. HAR Generation. We also compare the hierarchical prediction with vanilla AR. As shown in Tab. 5, when switching from vanilla AR to HAR generation, the gFID metric shows an acceptable degradation while the number of sampling steps is dramatically reduced from 128 to 8 or 9. Moreover, introducing group of NTP tokens (i.e., vanilla AR Gen. in Fig. 4) further reduces sampling errors and improves generation performance. Recon. vs. Gen. As the tokenizer trains longer, it may learn overly complex latent patterns that enhance reconstruction but hinder AR modeling. To find suitable tradeoff, we ablate tokenizer training at {250k, 500k, 750k, 1M} iterations, each paired with fully trained HAR generator. As shown in Fig. 7, rFID improves steadily with training, whereas gFID reaches its optimum at around 750k steps, after which generation quality degrades. We therefore adopt the 750k tokenizer checkpoint for all main experiments. 6. Conclusion This paper introduced Residual Tokenizer (ResTok), 1D visual tokenizer that brings the hierarchical and residual nature of visual representations back to ViT-based tokenizers for autoregressive image generation. Unlike existing isotropic tokenizers that query visual features along only depth, ResTok progressively merges image tokens and accumulates semantic residuals across levels. This hierarchical structure enables latent tokens to organize in coarseto-fine manner, achieving natural alignment between image and latent hierarchies without hand-crafted constraints. Extensive experiments verify the effectiveness of hierarchical residuals and implicit alignments in enhancing both reconstruction and generation efficiencies. Future work will further enhance fidelity and explore extension to unified understanding and generation models."
        },
        {
            "title": "References",
            "content": "[1] Roman Bachmann, Jesse Allardice, David Mizrahi, Enrico Fini, Oguzhan Fatih Kar, Elmira Amirloo, Alaaeldin ElNouby, Amir Zamir, and Afshin Dehghan. FlexTok: Resampling images into 1D token sequences of flexible length. In Proceedings of the 42nd International Conference on Machine Learning, pages 22412292. PMLR, 2025. 2, 3, 5, 6, 13 [2] Lukas Lao Beyer, Tianhong Li, Xinlei Chen, Sertac Karaman, and Kaiming He. Highly compressed tokenizer can In Proceedings of the 42nd Ingenerate without training. ternational Conference on Machine Learning, pages 4096 4114. PMLR, 2025. 2 [3] Huiwen Chang, Han Zhang, Lu Jiang, Ce Liu, and William T. Freeman. MaskGIT: Masked generative image In Proceedings of the IEEE/CVF Conference transformer. on Computer Vision and Pattern Recognition (CVPR), pages 1131511325, 2022. 3, 5, 12 [4] Mark Chen, Alec Radford, Rewon Child, Jeffrey Wu, Heewoo Jun, David Luan, and Ilya Sutskever. Generative preIn Proceedings of the 37th Internatraining from pixels. tional Conference on Machine Learning, pages 16911703. PMLR, 2020. 1, 2 [5] Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li Fei-Fei. ImageNet: large-scale hierarchical image database. In 2009 IEEE Conference on Computer Vision and Pattern Recognition, pages 248255, 2009. 2, 6, 12, 14 [6] Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, Jakob Uszkoreit, and Neil Houlsby. An image is worth 16x16 words: Transformers for image recognition at scale. In International Conference on Learning Representations, 2021. 1 [7] Patrick Esser, Robin Rombach, and Bjorn Ommer. Taming In Protransformers for high-resolution image synthesis. ceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pages 1287312883, 2021. 1, 2, 3, [8] Yuying Ge, Yixiao Ge, Ziyun Zeng, Xintao Wang, and Ying Shan. Planting SEED of vision in large language model. arXiv preprint arXiv:2307.08041, 2023. 2 [9] Ian Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley, Sherjil Ozair, Aaron Courville, and Yoshua Bengio. Generative adversarial nets. In Advances in Neural Information Processing Systems. Curran Associates, Inc., 2014. 4 [10] Karol Gregor, Ivo Danihelka, Alex Graves, Danilo Rezende, and Daan Wierstra. Draw: recurrent neural network for image generation. In Proceedings of the 32nd International Conference on Machine Learning, pages 14621471, Lille, France, 2015. PMLR. 1 [11] Jian Han, Jinlai Liu, Yi Jiang, Bin Yan, Yuqi Zhang, Zehuan Yuan, Bingyue Peng, and Xiaobing Liu. Infinity: Scaling bitwise autoregressive modeling for high-resolution image synthesis. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pages 1573315744, 2025. 2 [12] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recognition. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR), pages 770778, 2016. 3, [13] Martin Heusel, Hubert Ramsauer, Thomas Unterthiner, Bernhard Nessler, and Sepp Hochreiter. GANs trained by two time-scale update rule converge to local nash equilibrium. In Advances in Neural Information Processing Systems. Curran Associates, Inc., 2017. 6 [14] Geoffrey E. Hinton and Ruslan R. Salakhutdinov. Reducing the dimensionality of data with neural networks. Science, 313(5786):504507, 2006. 1 [15] Jonathan Ho and Tim Salimans. Classifier-free diffusion guidance. In NeurIPS 2021 Workshop on Deep Generative Models and Downstream Applications, 2021. 7, 13 [16] Yuanhui Huang, Weiliang Chen, Wenzhao Zheng, Yueqi Duan, Jie Zhou, and Jiwen Lu. Spectralar: Spectral autoregressive visual generation. In Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV), pages 1584215852, 2025. 1, 2, 3, 4, 5, 7 [17] Diederik P. Kingma and Max Welling. Auto-encoding variational bayes. In International Conference on Learning Representations, 2014. 1 [18] Diederik P. Kingma, Tim Salimans, Rafal Jozefowicz, Xi Chen, Ilya Sutskever, and Max Welling. Improved variaIn Adtional inference with inverse autoregressive flow. vances in Neural Information Processing Systems. Curran Associates, Inc., 2016. [19] Doyup Lee, Chiheon Kim, Saehoon Kim, Minsu Cho, and Wook-Shin Han. Autoregressive image generation using In Proceedings of the IEEE/CVF residual quantization. Conference on Computer Vision and Pattern Recognition (CVPR), pages 1152311532, 2022. 1, 2, 5 [20] Junnan Li, Dongxu Li, Silvio Savarese, and Steven Hoi. BLIP-2: Bootstrapping language-image pre-training with frozen image encoders and large language models. In Proceedings of the 40th International Conference on Machine Learning, pages 1973019742. PMLR, 2023. 4 [21] Tianhong Li, Yonglong Tian, He Li, Mingyang Deng, and Kaiming He. Autoregressive image generation without vector quantization. In Advances in Neural Information Processing Systems, pages 5642456445. Curran Associates, Inc., 2024. 3, 5, 6 [22] Xiang Li, Kai Qiu, Hao Chen, Jason Kuen, Jiuxiang Gu, Bhiksha Raj, and Zhe Lin. ImageFolder: Autoregressive image generation with folded tokens. In The Thirteenth International Conference on Learning Representations, 2025. 1, 2, 3, 4, 5, 6, 13 [23] Tsung-Yi Lin, Piotr Dollar, Ross Girshick, Kaiming He, Bharath Hariharan, and Serge Belongie. Feature pyramid networks for object detection. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR), pages 21172125, 2017. 2, 3 [24] Yaron Lipman, Ricky T. Q. Chen, Heli Ben-Hamu, Maximilian Nickel, and Matthew Le. Flow matching for genera9 tive modeling. In The Eleventh International Conference on Learning Representations, 2023. 2 [25] Yiheng Liu, Liao Qu, Huichao Zhang, Xu Wang, Yi Jiang, Yiming Gao, Hu Ye, Xian Li, Shuai Wang, Daniel Du, et al. Detailflow: 1d coarse-to-fine autoregressive imarXiv preprint age generation via next-detail prediction. arXiv:2505.21473, 2025. 1, 2, 3, 4, 5, 6, 7, 8, 13 [26] Ze Liu, Yutong Lin, Yue Cao, Han Hu, Yixuan Wei, Zheng Zhang, Stephen Lin, and Baining Guo. Swin transformer: In Hierarchical vision transformer using shifted windows. Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV), pages 1001210022, 2021. 8 [27] Zhuang Liu, Hanzi Mao, Chao-Yuan Wu, Christoph Feichtenhofer, Trevor Darrell, and Saining Xie. convnet for the 2020s. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pages 11976 11986, 2022. 8 [28] Ilya Loshchilov and Frank Hutter. Decoupled weight decay regularization. In International Conference on Learning Representations, 2019. 6, 13 [29] Keita Miwa, Kento Sasaki, Hidehisa Arai, Tsubasa Takahashi, and Yu Yamaguchi. One-D-Piece: Image tokenizer arXiv preprint meets quality-controllable compression. arXiv:2501.10064, 2025. 5, 6, [30] William Peebles and Saining Xie. Scalable diffusion models with transformers. In Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV), pages 41954205, 2023. 5 [31] Sucheng Ren, Qihang Yu, Ju He, Xiaohui Shen, Alan Yuille, and Liang-Chieh Chen. FlowAR: Scale-wise autoregressive image generation meets flow matching. In Forty-second International Conference on Machine Learning, 2025. 2, 5, 6 [32] Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Bjorn Ommer. High-resolution image In Proceedings of synthesis with latent diffusion models. the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pages 1068410695, 2022. 1, 5 [33] Tim Salimans, Ian Goodfellow, Wojciech Zaremba, Vicki Cheung, Alec Radford, Xi Chen, and Xi Chen. Improved techniques for training gans. In Advances in Neural Information Processing Systems. Curran Associates, Inc., 2016. 6 [34] Fengyuan Shi, Zhuoyan Luo, Yixiao Ge, Yujiu Yang, Ying Shan, and Limin Wang. Scalable image tokenization with index backpropagation quantization. In Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV), pages 1603716046, 2025. 5 [35] Oriane Simeoni, Huy V. Vo, Maximilian Seitzer, Federico Baldassarre, Maxime Oquab, Cijo Jose, Vasil Khalidov, Marc Szafraniec, Seungeun Yi, Michael Ramamonjisoa, Francisco Massa, Daniel Haziza, Luca Wehrstedt, Jianyuan Wang, Timothee Darcet, Theo Moutakanni, Leonel Sentana, Claire Roberts, Andrea Vedaldi, Jamie Tolan, John Brandt, Camille Couprie, Julien Mairal, Herve Jegou, Patrick LaarXiv preprint batut, and Piotr Bojanowski. DINOv3. arXiv:2508.10104, 2025. 4, 14 [36] Casper Kaae Sønderby, Tapani Raiko, Lars Maaløe, Søren Kaae Sønderby, and Ole Winther. Ladder variational autoencoders. In Advances in Neural Information Processing Systems. Curran Associates, Inc., 2016. [37] Ke Sun, Bin Xiao, Dong Liu, and Jingdong Wang. Deep high-resolution representation learning for human pose esIn Proceedings of the IEEE/CVF Conference on timation. Computer Vision and Pattern Recognition (CVPR), pages 56935703, 2019. 2, 3 [38] Peize Sun, Yi Jiang, Shoufa Chen, Shilong Zhang, Bingyue Peng, Ping Luo, and Zehuan Yuan. Autoregressive model beats diffusion: Llama for scalable image generation. arXiv preprint arXiv:2406.06525, 2024. 2, 3, 5, 6, 7, 12, 13, 14 [39] Keyu Tian, Yi Jiang, Zehuan Yuan, Bingyue Peng, and Liwei Wang. Visual autoregressive modeling: Scalable image generation via next-scale prediction. In Advances in Neural Information Processing Systems, pages 8483984865. Curran Associates, Inc., 2024. 2, 4, 5, 6 [40] Aaron van den Oord, Nal Kalchbrenner, Lasse Espeholt, koray kavukcuoglu, Oriol Vinyals, and Alex Graves. ConIn Additional image generation with pixelcnn decoders. vances in Neural Information Processing Systems. Curran Associates, Inc., 2016. 1, 2 [41] Aaron van den Oord, Nal Kalchbrenner, and Koray In ProKavukcuoglu. Pixel recurrent neural networks. ceedings of The 33rd International Conference on Machine Learning, pages 17471756, New York, New York, USA, 2016. PMLR. 1, 2 [42] Aaron van den Oord, Oriol Vinyals, and koray kavukcuoglu. Neural discrete representation learning. In Advances in Neural Information Processing Systems. Curran Associates, Inc., 2017. 1, 2, 3 [43] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan Gomez, Łukasz Kaiser, and Illia Polosukhin. Attention is all you need. In Advances in Neural Information Processing Systems. Curran Associates, Inc., 2017. [44] Peng Wang, Shuai Bai, Sinan Tan, Shijie Wang, Zhihao Fan, Jinze Bai, Keqin Chen, Xuejing Liu, Jialin Wang, Wenbin Ge, Yang Fan, Kai Dang, Mengfei Du, Xuancheng Ren, Rui Men, Dayiheng Liu, Chang Zhou, Jingren Zhou, and Junyang Lin. Qwen2-vl: Enhancing vision-language models perception of the world at any resolution. arXiv preprint arXiv:2409.12191, 2024. 6, 12 [45] Yuqing Wang, Shuhuai Ren, Zhijie Lin, Yujin Han, Haoyuan Guo, Zhenheng Yang, Difan Zou, Jiashi Feng, and Xihui Liu. Parallelized autoregressive visual generation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pages 1295512965, 2025. 3, 5, 6 [46] Xin Wen, Bingchen Zhao, Ismail Elezi, Jiankang Deng, and Principal components enable new lanXiaojuan Qi. In Proceedings of the IEEE/CVF Inguage of images. ternational Conference on Computer Vision (ICCV), pages 1664116651, 2025. 2 [47] Tianwei Xiong, Jun Hao Liew, Zilong Huang, Jiashi Feng, and Xihui Liu. GigaTok: Scaling visual tokenizers to 3 10 In billion parameters for autoregressive image generation. Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV), pages 1877018780, 2025. 2, 3, 4, 5, 6, 8, 13, [48] Jingfeng Yao, Bin Yang, and Xinggang Wang. Reconstruction vs. generation: Taming optimization dilemma in latent diffusion models. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pages 1570315712, 2025. 4, 5, 8, 14 [49] Jiahui Yu, Xin Li, Jing Yu Koh, Han Zhang, Ruoming Pang, James Qin, Alexander Ku, Yuanzhong Xu, Jason Baldridge, and Yonghui Wu. Vector-quantized image modeling with improved VQGAN. In International Conference on Learning Representations, 2022. 1, 2, 3 [50] Lijun Yu, Yong Cheng, Zhiruo Wang, Vivek Kumar, Wolfgang Macherey, Yanping Huang, David Ross, Irfan Essa, Yonatan Bisk, Ming-Hsuan Yang, Kevin Murphy, Alexander Hauptmann, and Lu Jiang. SPAE: Semantic pyramid auIn toencoder for multimodal generation with frozen llms. Advances in Neural Information Processing Systems, pages 5269252704. Curran Associates, Inc., 2023. 2 [51] Lijun Yu, Jose Lezama, Nitesh Bharadwaj Gundavarapu, Luca Versari, Kihyuk Sohn, David Minnen, Yong Cheng, Agrim Gupta, Xiuye Gu, Alexander Hauptmann, Boqing Gong, Ming-Hsuan Yang, Irfan Essa, David Ross, and Lu Jiang. Language model beats diffusion - tokenizer is key to visual generation. In The Twelfth International Conference on Learning Representations, 2024. 2 [52] Qihang Yu, Mark Weber, Xueqing Deng, Xiaohui Shen, Daniel Cremers, and Liang-Chieh Chen. An image is worth In Advances 32 tokens for reconstruction and generation. in Neural Information Processing Systems, pages 128940 128966. Curran Associates, Inc., 2024. 1, 2, 3, 4, 5, 6, 12, 13, 14 [53] Sihyun Yu, Sangkyung Kwak, Huiwon Jang, Jongheon Jeong, Jonathan Huang, Jinwoo Shin, and Saining Xie. Representation alignment for generation: Training diffusion transformers is easier than you think. In The Thirteenth International Conference on Learning Representations, 2025. [54] Richard Zhang, Phillip Isola, Alexei A. Efros, Eli Shechtman, and Oliver Wang. The unreasonable effectiveness of deep features as perceptual metric. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR), pages 586595, 2018. 4 [55] Anlin Zheng, Xin Wen, Xuanyang Zhang, Chuofan Ma, Tiancai Wang, Gang Yu, Xiangyu Zhang, and Xiaojuan Qi. Vision foundation models as effective visual tokenizers for autoregressive generation. In Advances in Neural Information Processing Systems, 2025. 2, 3, 4, 5, 6,"
        },
        {
            "title": "Appendix",
            "content": "Algorithm 1 Residual 1D latent token initialization Require: image tokens p(0), hierarchical levels L. (a) Tokenizer mask. (b) Generator mask. Figure 8. Implementations of attention masks in the tokenizer and the generator. The tokenizer mask is illustrated using 3 imagetoken scales and 4 latent hierarchies as an example, while the generator mask is shown with 4 vanilla AR tokens and 2 groups of HAR tokens. 1: = 1, = 1 2: z(0) 1 = Poolhw(p(0)) 3: for = 2, 3, . . . , do 4: 5: 6: l1) p(0) = p(0) Upsample(z(0) z(0) = Poolhw(p(0)) z(0) 1:l = Concat(z(0) if % 2 = 0 then = 2 1:l1, z(0) ) else 7: 8: 9: 10: 11: 12: end for 13: return latent tokens z(0) 1:L = 2 end if , latent tokens z(n) 1:L. , z(n) Algorithm 2 Residual merging process Require: image tokens p(n) 1: {p(n) 1:L} = Attention({p(n) , z(n) 2: p(n) s1 = Merge(p(n) ) Upsample(p(n) = p(n) 3: p(n) 4: {p(n+1) s1 , p(n+1) , z(n+1) s1 = Concat(p(n+1) 5: p(n+1) 6: return image tokens p(n+1) s1) 1:L } = MLP({p(n) s1 , p(n+1) 1:L}) s ) s1 and latent tokens z(n+1) 1:L s1, p(n) , z(n) 1:L}) (a) Encoder RoPE. (b) Decoder RoPE. Figure 9. Implementations of 2D RoPE in ResTok, illustrated using 3 image-token scales and 3 latent tokens as an example. A. More Implementation Details A.1. Architecture For the CNN encoder and decoder, we adopt exact the same configuration of MaskGITs encoder and decoder [3]. For the ViT encoder and decoder, we develop them upon TiTokLs architecture [52], each comprising 24 transformer layers, 1024 dimensions and 16 heads. To bridge the dimension of the CNN encoder/decoder and the ViT encoder/decoder, an additional linear layer is applied between them. We apply encoder attention masks as shown in Fig. 8a to enforce the causality of encoding process. Additionally, we replace learnable positional embeddings in the original TiTok with modified 2D version of M-RoPE [44], which takes 1D latent tokens as text and 2D image tokens as 12 image as shown in Fig. 9. Specifically, the positional IDs of image tokens from multiple hierarchies are concatenated sequentially, together with those of the text tokens. In the encoder, M-RoPE is applied in the order of coarseto-fine 2D image tokens, followed by the 1D latent tokens. In the decoder, the sequence begins with the 1D latent tokens, which are then followed by the 2D masked image tokens. The residual 1D latent token initialization and the residual merging process proposed in Fig. 2 can be formally represented as Algorithm 1 and Algorithm 2, respectively. For the generator, we apply the attention mask as shown in Fig. 8b to enable next-hierarchy prediction. A.2. Training Our training configurations of ResTok and LlamaGenL [38] are listed in Tabs. 7 and 8. Both the tokenizer and the generator are trained from scratch on the ImageNet1K training set [5], consisting of 1,281,167 images across 1,000 object classes. When training ResTok, images are first randomly resized with factor between [0.8, 1.0], and then cropped to 256256 at random position. To prepare the training data for the generator, we use the same scripts and data augmentations to extract quantized codes as LlamTable 6. Classifier-free guidance (CFG) configurations used for different tokenizer checkpoints. For Step schedules, guidance is activated at the specified CFG Start Ratio of the sampling trajectory with fixed Max. CFG Value. For Linear schedules, the CFG value increases linearly from 1.0 to the Max. CFG Value over the full sampling process. During sampling, we first apply Top-K filtering followed by Top-P (nucleus) filtering. Setting the value of or to 0 indicates bypassing Top-K or Top-P filtering. Ckpt. Schedule CFG Start Ratio Max. CFG Value Top-K Top-P gFID IS Pre. Rec. 250K Step 500K Step 750K Step 1M Linear 50% 25% 25% N/A 4.50 4.50 3.75 4.00 0 0 0 0.99 0.99 0.95 0.95 2.44 2.33 2.34 2.58 230.7 249.1 257.8 252.3 0.79 0.78 0.79 0.78 0.59 0.60 0.60 0.61 Table 7. Training settings of ResTok. Table 9. Keeping probabilities of nested token dropout. config optimizer base learning rate weight decay optimizer momentum batch size learning rate schedule minimal learning rate training epochs linear warmup epochs augmentation ema decay value AdamW [28] 1e-4 1e-4 β1, β2=0.9, 0.95 256 cosine decay 1e-5 200 1 RandomResizedCrop 0.9999 Table 8. Training settings of LlamaGen-L. config optimizer base learning rate weight decay optimizer momentum batch size learning rate schedule minimal learning rate training epochs linear warmup epochs augmentation ema decay value AdamW [28] 1e-4 0.05 β1, β2=0.9, 0.95 256 cosine decay 1e-5 300 1 ResizedCrop 0. aGen [38]. We set λenc = λdec = λvf = λmse = λpercp = 1.0 and λgan = 0.5 in Eqs. (3) and (4). We apply nested token dropout [1, 22, 25, 29] during training. The keeping probabilities for each token length are listed in Tab. 9, with minimum of 4 tokens preserved. In our setting, there is an 80% chance that no dropout is applied, while the dropout probability for shorter token lengths decreases exponentially as the target length decreases. A.3. Evaluation To evaluate ResToks reconstruction ability, we utilize the same protocol as TiTok [52]. To obtain the metrics of gener13 #Tokens 128 64 32 16 8 Probability 80.00% 10.32% 5.16% 2.58% 1.29% 0.65% Table 10. Additional results of AR generation on ResTok."
        },
        {
            "title": "AR Type",
            "content": "#Steps gFID IS Pre. Rec."
        },
        {
            "title": "HAR\nVanilla AR",
            "content": "9 128 2.34 2.18 257.8 259.1 0.79 0.79 0.60 0.62 ation performance, we use the same scripts as GigaTok [47] to generate images and calculate gFID, IS, Precision and Recall. Specifically, we search for the best CFG [15] schedules of each HAR generator corresponding to each checkpoint of ResTok in Fig. 7, which are listed in Tab. 6. The best trade-off (i.e., the 750K step checkpoint) is selected as the final model. Ablations in Sec. 5.4 which take the 150K step checkpoint of the tokenizer and the 250K step checkpoint of the generator, do not enable CFG for evaluation. To quantify the distributional uniformity of codebook usage, we compute the empirical entropy of the selected codebook entries. Let the codebook contain entries. For each entry {1, . . . , K}, let ci denote the number of times it is selected during evaluation, the empirical probability of selecting entry is pi = ci (cid:80)K j=1 cj . (5) The codebook entropy HC is then defined as the standard Shannon entropy (measured in bits) HC = (cid:88) i=1 pi log2(pi + ϵ), (6) where small constant ϵ is added for numerical stability. We set ϵ = 1 108 as TiTok [52] does. higher value of HC indicates more uniform codebook usage, while lower entropy suggests concentration on small subset of entries. Table 11. Licenses for released assets"
        },
        {
            "title": "License",
            "content": "TiTok [52] Apache-2.0 license LlamaGen [38] MIT license GigaTok [47] MIT license VA-VAE [48] MIT license DINOv3 [35] DINOv3 License ImageNet-1K [5] Custom (research-only, non-commercial) B. Additional Results In addition to the HAR version reported in Tab. 1, we also train vanilla AR variant to evaluate the upper bound of AR generation performance on ResTok. The results are presented in Tab. 10. The vanilla AR model uses step CFG schedule, where CFG is activated after sampling the first 4 tokens with fixed value of 4.5. Compared with HAR, which requires only 9 sampling steps, vanilla AR reduces gFID from 2.34 to 2.18 but incurs more than 10 increase in sampling steps, demonstrating the effectiveness of our proposed approach. C. Licenses for Released Assets This work uses the listed projects in Tab. 11 released under their licenses. We strictly adhered to their license requirements; the original projects copyright notices and license texts can be found in their official repositories."
        }
    ],
    "affiliations": [
        "Kolors Team, Kuaishou Technology",
        "Vision Lab, Nanjing University"
    ]
}