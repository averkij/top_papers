{
    "paper_title": "POWSM: A Phonetic Open Whisper-Style Speech Foundation Model",
    "authors": [
        "Chin-Jou Li",
        "Kalvin Chang",
        "Shikhar Bharadwaj",
        "Eunjung Yeo",
        "Kwanghee Choi",
        "Jian Zhu",
        "David Mortensen",
        "Shinji Watanabe"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Recent advances in spoken language processing have led to substantial progress in phonetic tasks such as automatic speech recognition (ASR), phone recognition (PR), grapheme-to-phoneme conversion (G2P), and phoneme-to-grapheme conversion (P2G). Despite their conceptual similarity, these tasks have largely been studied in isolation, each relying on task-specific architectures and datasets. In this paper, we introduce POWSM (Phonetic Open Whisper-style Speech Model), the first unified framework capable of jointly performing multiple phone-related tasks. POWSM enables seamless conversion between audio, text (graphemes), and phones, opening up new possibilities for universal and low-resource speech processing. Our model outperforms or matches specialized PR models of similar size (Wav2Vec2Phoneme and ZIPA) while jointly supporting G2P, P2G, and ASR. Our training data, code and models are released to foster open science."
        },
        {
            "title": "Start",
            "content": "POWSM: Phonetic Open Whisper-Style Speech Foundation Model Chin-Jou Li1, Kalvin Chang2, Shikhar Bharadwaj1, Eunjung Yeo3, Kwanghee Choi3, Jian Zhu4, David Mortensen1, Shinji Watanabe1, 1Carnegie Mellon University, 2University of California, Berkeley, 3University of Texas, Austin, 4University of British Columbia, 5 2 0 2 8 2 ] . [ 1 2 9 9 4 2 . 0 1 5 2 : r a"
        },
        {
            "title": "Abstract",
            "content": "Recent advances in spoken language processing have led to substantial progress in phonetic tasks such as automatic speech recognition (ASR), phone recognition (PR), graphemeto-phoneme conversion (G2P), and phonemeto-grapheme conversion (P2G). Despite their conceptual similarity, these tasks have largely been studied in isolation, each relying on taskspecific architectures and datasets. In this paper, we introduce POWSM (Phonetic Open Whisper-style Speech Model), the first unified framework capable of jointly performing multiple phone-related tasks. POWSM enables seamless conversion between audio, text (graphemes), and phones, opening up new possibilities for universal and low-resource speech processing. Our model outperforms or matches specialized PR models of similar size (Wav2Vec2Phoneme and ZIPA) while jointly supporting G2P, P2G, and ASR. Our training data, code1 and models2 are released to foster open science."
        },
        {
            "title": "Introduction",
            "content": "Phones are the smallest units of sound in speech. Unlike graphemes, phones are shared across languages and usually represented using the International Phonetic Alphabet (IPA) (International Phonetic Association, 1999), unified transcription standard for all languages. By providing consistent representation of speech across languages, phone-level modeling allows fine-grained analysis and cross-lingual generalization, enabling tasks like atypical speech analysis (e.g., L2 speech (Li et al., 2016; Inceoglu et al., 2023) and pathological speech (Choi et al., 2025; Li et al., 2025)), endangered language documentation (He et al., 2024), code-switched text-to-speech (Zhou et al., 2020), and cross-lingual transfer in speech-to-text (Pratap et al., 2024; Magoshi et al., 2025). 1https://github.com/espnet 2https://huggingface.co/espnet/powsm 1 Figure 1: POWSM is the first phonetic foundation model that can perform four phone-related tasks: Phone Recognition (PR), Automatic Speech Recognition (ASR), audio-guided grapheme-to-phoneme conversion (G2P), and audio-guided phoneme-to-grapheme conversion (P2G). Four key phone-related tasks underpin phonetic spoken language processing: automatic speech recognition (ASR), phone recognition (PR), grapheme-to-phoneme conversion (G2P), and phoneme-to-grapheme conversion (P2G). ASR learns implicit phonetic representations (Belinkov and Glass, 2017), while PR offers explicit phonelevel supervision. G2P and P2G bridge orthographic and phonetic spaces. Collectively, these tasks interact through shared phonetic representations, each addressing different aspect of the relationship between audio, phones, phonemes, and graphemes. Despite their conceptual similarity, these tasks have traditionally been developed in isolation, using task-specific architectures and datasets. Such systems are optimized for specific input-output mappings and cannot be easily extended to other phonetic tasks. This fragmentation has hindered the development of general-purpose models for phonetic processing, necessitating unified phonetic foundation model that can perform multiple phonerelated tasks within single, general framework for speech processing. To bridge this gap, we propose POWSM, phonetic foundation model capable of performing four core phone-related tasks PR, ASR, audio-guided G2P, and audio-guided P2G within one unified architecture (Figure 1). To construct this framework, we reformulate standard ASR datasets (Zhu et al., 2025) into four task-specific formats, allowing the model to learn consistent mappings across audio, phoneme, and grapheme representations. In addition, POWSM adopts an attention-based encoder-decoder (AED) architecture, following the design of large-scale speech foundation models such as Whisper (Radford et al., 2023) and OWSM (Peng et al., 2023). Empirically, POWSM outperforms previous PR models on both in-domain data and out-of-domain languages, achieves low-resource ASR performance comparable to web-scale multilingual foundation models, and can act as speech-grounded P2G and G2P across more than 70 languages. POWSM offers new unified paradigm for phonelevel modeling, paving the way for inclusive and globally accessible speech technologies that transcend language boundaries and resource disparities. To summarize, our main contributions are: We provide POWSM, large-scale foundation model that achieves state-of-the-art PR performance, and is capable of performing multiple fundamental phone-related tasks. Our model enables seamless conversion between speech, text (graphemes/orthography), and phones. We thoroughly analyze POWSM to understand the interaction between multiple tasks, architecture components, and losses. We fully open-source all our data preparation and evaluation scripts, model checkpoints and code to foster open science."
        },
        {
            "title": "2 Related Work",
            "content": "Speech foundation models Recent speech foundation models such as Whisper (Radford et al., 2023) and OWSM (Peng et al., 2023, 2024) have driven progress in large-scale multilingual ASR and speech translation, but they do not explicitly address phoneme recognition or articulatory-level supervision. Subsequent work (Yusuyin et al., 2025; Fu et al., 2025) showed that incorporating phoneme-level objectives improves ASR for lowresource and long-tailed settings, while outputting phonemes as an intermediate benefited speech translation (GÃ¡llego et al., 2025). POWSM extends this line of work by being the first open foundation model jointly trained on phone recognition and related tasks, integrating multilinguality, phonetic supervision, and multi-task scalability within one framework. Phone recognition Prior work in multilingual phone recognition can broadly be categorized into (1) language-specific models (Gao et al., 2021) that rely on explicit phoneme (Xu et al., 2022) or allophone inventories (Li et al., 2020) and (2) languageagnostic approaches that aim to generalize across languages without such resources (Taguchi et al., 2023; Glocker et al., 2023; Li et al., 2021; Zhu et al., 2025). POWSM follows the latter paradigm as fully data-driven multilingual model that learns phone representations without predefined phoneme mappings. WhisperPPT (Samir et al., 2025) improved Whisper (Radford et al., 2023)s performance through data cleaning but remained limited in data coverage and task diversity. However, Whisper is trained on closed corpus and could display harmful biases for PR which cannot be fully removed by finetuning. POWSM is trained from scratch on open datasets. ZIPA (Zhu et al., 2025) scaled PR to 17,000+ hours of data and 88 languages using Zipformer (Yao et al., 2024) encoder and noisy-student training on 4,000+ languages, achieving state-of-the-art results. To construct its training corpus, ZIPA employed G2P system to convert large-scale ASR transcriptions into phoneme sequences, effectively repurposing ASR datasets for PR. Building on this idea, POWSM leverages both the grapheme and the G2P-generated phoneme transcriptions, reformulating them into four task-specific forms: ASR, PR, G2P, and P2G. G2P & P2G POWSM is the first model capable of both audio-guided G2P and audio-guided P2G. G2P conversion, sometimes called phonemization in the text-to-speech literature, can be accomplished with pronunciation dictionaries (Rudnicky, 1993), rules (Mortensen et al., 2018), WFSTs (Black and Lenzo, 2001), or seq2seq neural methods to choose between different pronunciations of word in context (Zhu et al., 2022). Textbased G2P, however, still cannot handle phonetic 2 variation, enforcing one-to-one mapping between orthography and transcription. In contrast, audioguided G2P can learn to map the different acoustic realizations of phoneme across varieties of language to phone representation (Route et al., 2019). In particular, Mak et al. (2025) observed performance improvement in using audio-guided G2P versus text-based G2P alone for Cantonese. Gao et al. (2024) similarly showed that joint learning of G2P, phone recognition, and forced alignment outperform G2P teacher model. Similarly, Sun and Richmond (2024) jointly learned G2P and TTS. Compared to G2P, P2G conversion is less studied, with Lauc (2024) training seq2seq model on 19 million language-grapheme-phoneme triplets."
        },
        {
            "title": "3.1 Data preparation",
            "content": "We use IPAPack++ (Zhu et al., 2025) for training. It is an open source corpus of roughly 17,000 hours of multilingual speech with paired orthographic and phonemic transcriptions. We will release all data processing scripts to make POWSM fully reproducible. G2P-generated transcriptions have been manually inspected and cleaned. Following Samir et al. (2025), we remove Interlingua and 10 noisy FLEURS languages. Utterances longer than 300 phones are filtered out. IPA sequences are normalized to Unicode NFD (Canonical Decomposition); English G2P sequences are further refined with rule-based corrections to fix voice-onset time issues (see Appendix A.1). To prevent IPA tokens from being confused with graphemes, sequences are split into phone tokens with diacritics and modifiers attached, following PanPhon (Mortensen et al., 2016), and enclosed in slashes (e.g., /phOs@m/ /ph/ /O/ /s/ /@/ /m/)."
        },
        {
            "title": "3.3 Training details",
            "content": "POWSM adopts an attention-based encoderdecoder (AED) architecture, which flexibly models output sequences and allows the integration of additional tasks. Specifically, we follow the OWSM v3.1 architecture (Peng et al., 2024), which employs an E-Branchformer encoder and Transformer decoder, consistent with the general encoder-decoder structure of Whisper (Radford et al., 2023). The model is trained from scratch using ESPnet (Watanabe et al., 2018) with hybrid CTC/attention loss (Watanabe et al., 2017), where we set the ratio Î±ctc to 0.3: = Î±ctcLctc + (1 Î±ctc)Lattention. (1) The encoder operates at the stride size of 40ms. Training uses global batch size of 256. Speech inputs are 16kHz and padded to 20 seconds. The vocabulary consists of 40k tokens, including around 6k phone tokens, language and timestamp tokens, and BPE tokens from orthography. The model has approximately 350M parameters with 9 layers for both the encoder and decoder and was trained on 4 H100 GPUs for 2 days. Using CTC loss (Graves, 2006), We align the encoder outputs with simplified version of the phone token sequences. Unlike the decoder outputs, the phones in these sequences are stripped of break (/./, /</) and length diacritics (/e:/, /e;/, /Ëe/) to accelerate convergence. Additional details and analyses are provided in 6.1."
        },
        {
            "title": "4 Experimental Setup",
            "content": "Evaluation metric We report Phonetic Feature Error Rate (PFER), an edit distance using articulatory features from PanPhon (Mortensen et al., 2016), averaged over the number of phones and computed as in Equation 2 for PR. Each feature contributes 1 24 distance unit, while insertion and deletion cost 1 unit. The edit distance grows linearly with the sequence length and has no upper bound."
        },
        {
            "title": "3.2 Multitask data format",
            "content": "Our model is trained on four tasks: PR, ASR, and audio-guided G2P and P2G. Each utterance is used once per task, with task-specific formatting as illustrated in Figure 1, including text prompt, language token, task token, and target output. We leave the text prompt blank (token <na>) for PR and ASR, and provide graphemes and phones as prompts for G2P and P2G. PFER = 1 #phone (cid:88) i=1 D(feat(hypi), feat(refi)) (2) Unlike Phone Error Rate (PER), which considers only exact phone matches, or Phone Token Error Rate (PTER), which treats diacritics and modifiers as separate tokens, PFER computes the edit distance in terms of articulatory features interpretable subphone attributes (e.g. voicing) 3 capturing phonetic similarity in fine-grained fashion. Previous studies (Taguchi et al., 2023; Zhu et al., 2025) define PFER as the mean articulatory feature edit distance over the evaluation set. In contrast, we normalize it by the number of phones in the reference transcription to measure the proportion of feature errors per phone. Decoding hyperparameters We use CTC weight (denoted as ctc) of 0.3 and beam size (denoted as beam) of 3 during decoding for all reported numbers unless specified. Further details on the choice of hyperparameters are discussed in 6.1. Evaluation datasets For unseen languages, we evaluate on three datasets: DoReCo (Paschen et al., 2020), VoxAngeles (Chodroff et al., 2024), and Tusom2021 (Mortensen et al., 2021). DoReCo is dataset of 50+ languages (with broad transcriptions) intended for documentation of small or endangered languages; we use 45-language subset. VoxAngeles (Chodroff et al., 2024) is postprocessed version of the UCLA Phonetics Lab Archive (Ladefoged et al., 2009) containing 95 languages. Tusom is low-data Tangkhulic language of India not included in the training data. Tusom2021 consists of narrow phonetic transcriptions (unlike the broad transcriptions from G2P on which POWSM was trained) of individual Tusom words. We removed the tones. We also test on five datasets on varieties of English: the Buckeye Corpus (Pitt et al., 2005) and DoReCo South-England represent dialectal variation, while L2-ARCTIC (Zhao et al., 2018), EpaDB (Vidal et al., 2019), and SpeechOcean762 (Zhang et al., 2021) contain L2 speakers. For L2-ARCTIC, we used the manually annotated phoneme transcriptions (which Zhu et al. (2025) termed L2Perceived) rather than G2P dictionary-based transcriptions. The manual transcriptions reflect what the speaker actually said, whereas the dictionarybased version enforces single pronunciation variant.3 Manual inspection by trained phonologist further showed the L2-ARCTIC transcriptions to be of extremely poor quality. For the five aforementioned datasets, we use preprocessed datasets 3For instance, crayon in American English can be pronounced as /\"kÃ´Ã¦n/, /\"kÃ´ej.On/, or /\"kÃ´ej.6n/ (Vaux and Golder, 2003) (among others), but the CMU Pronouncing Dictionary (Rudnicky, 1993) only lists one. from Zhu et al. (2025)4 and Koel Labs5 for better transcription quality. We then evaluated our model on in-domain data from IPAPack++, the dataset seen during training. We followed Zhu et al. (2025) in using LibriSpeech for English, AISHELL for Mandarin, and MLS for European languages, and additionally evaluated on IISc-MILE Tamil (A et al., 2022) for Tamil and KSC (Khassanov et al., 2021) for Kazakh. For ASR and P2G, we evaluate with FLEURS. See Table 1 for more details about our evaluation datasets. PR (In-domain) deu 14.27 pol 2.14 eng 10.58 por 3.74 nld 12.76 tam 16.58 fra 10.07 kaz 7.07 ita 5.27 cmn 10.02 spa 10. PR (Out-of-domain: Unseen languages) DoReCo VoxA. 1.58 Tusom. 1.16 19.18 PR (Out-of-domain: Language variation) Buckeye DRC-SE L2-ARC EpaDB SO762 7.88 0. 3.66 2.74 2.32 ASR (FLEURS) orm 0.13 slv 1.76 afr 0.66 bos 2.45 aze 2. pan 1.48 tgk 1.96 mkd 2.45 Table 1: Duration of the test sets for different tasks (in hours). Abbreviated datasets (in order): VoxAngeles, Tusom2021, DoReCo South-England, L2-ARCTIC, SpeechOcean762. Baselines We evaluate all PR baselines without further training with IPAPack++. See Appendix A.2 for more details about training data and language coverage. Allosaurus (Li et al., 2020, 2021) uses phone-level CTC to train language-agnostic model and applies language-specific allophone-to-phoneme mappings. Wav2Vec2Phoneme (Xu et al., 2022), MultIPA (Taguchi et al., 2023) and Allophant (Glocker et al., 2023) fine-tune XLS-R (Babu et al., 2022) with different objectives: Wav2Vec2Phoneme maps unseen phonemes using articulatory features, MultIPA leverages high-quality G2P data from seven languages, while Allophant decomposes phones into articulatory features and applies CTC losses for each. ZIPA (Zhu et al., 2025) trains ZipFormer 4https://huggingface.co/anyspeech 5https://huggingface.co/KoelLabs 4 (Yao et al., 2024) from scratch on IPAPack++ using CR-CTC and also provides variant trained with additional pseudo-labeled data (ZIPA-CRNS-Large). For ASR, we compare POWSM with two series of models: OWSM (Peng et al., 2025) and OWLS (Chen et al., 2025). We select OWSM-CTC v4 because it is the best-performing model in the series, featuring an encoder-CTC architecture that supports ASR, ST, and LID. For OWLS, we include models with comparable parameter sizes."
        },
        {
            "title": "5 Results",
            "content": "We found that POWSMs performance on PR and ASR tasks is comparable or superior to competitive baselines."
        },
        {
            "title": "5.1 Multi-task performance",
            "content": "Results on the in-domain test sets are presented in Table 2 and Table 3. We provide further discussion of G2P and P2G in 6.2. POWSM excels at in-domain phone recognition From Table 2, we see that POWSM achieves the lowest average PFER in phone recognition, due to the strong language modeling capability of the decoder. We hypothesize that our English data cleaning (Appendix A.1) may have negatively affected the PFER for Germanic languages due to mismatch between training and test data. Nevertheless, our approach fills this gap by achieving strong performance on other languages, outperforming models trained on larger datasets. POWSM is comparable with web-scale ASR models on low-resource languages We hypothesize that pre-training with phone recognition benefits low-resource ASR (Yusuyin et al., 2025). To choose low-resource languages, we selected languages in IPAPack++ with less than 8 hours of speech in FLEURS to serve as the test set. See A.3 for details on the amount of data used by different models. For fair comparison with other multilingual ASR baselines without languagespecific components, we use the same decoding hyperparameters ctc=0.0, beam=1. As shown in Table 3, POWSM (POWSM 0.35B, ASR) is often comparable to models of similar size trained on web-scale data for ASR (OWLS 0.5B). Incorporating phones obtained from PR as text prompts (PR-P2G) significantly decreases WER, making it comparable to or even better than these models. When using gold phone labels for P2G (see analysis in 6.2), POWSM outperforms other ASR models by large margin in most cases."
        },
        {
            "title": "5.2 POWSM generalizes well to unseen",
            "content": "languages Table 4 reports PFER on datasets with unseen languages and language variation. Results indicate that POWSM achieves strong performance across these datasets, and handles both dialectal and L2 variation effectively. Notably, our method outperforms ZIPA trained on the same data and even exceeds ZIPA trained with extra pseudo-labeled data, achieving the best results on unseen languages while performing three additional tasks. This shows the effectiveness of our multi-task approach. While POWSM lags behind Wav2Vec2Phoneme on socio-phonetic variations, we attribute this to its self-supervised learning with over 60k hours of speech (from wav2vec 2.0 (Baevski et al., 2020)) prior to the supervised learning stage."
        },
        {
            "title": "6 Analysis",
            "content": "In this section, we analyze how POWSM works, focusing on the phonetic-aware encoder and taskand language-specific tokens, which are the defining features of the model."
        },
        {
            "title": "6.1 Behavior of the speech encoder",
            "content": "The CTC encoder prefers fine-grained phones without suprasegmentals We observed that mixing phones and orthography as encoder targets hindered training, because the same speech input would have different encoder CTC targets for different tasks. Therefore, we used phones as encoder targets, encouraging general representations of sounds to be shared across languages. To determine the most effective unit for the CTC encoder, we fix the decoder vocabulary to PanPhon phones and compared four encoder targets: (1) Unicode code points vs. PanPhon, and (2) sequences with vs. without suprasegmentals (length and break marks). Unicode code points offer simplicity and smaller vocabulary but split phones into unnatural units (e.g. /ph/) and increase sequence length, while PanPhon represents each phone-diacritic combination as unit (e.g. /ph/), yielding more natural monotonic sequence at the expense of sparsity and potential out-of-vocabulary issues. Suprasegmentals such as /:/, though phonemic in many languages, confuse PR models (Zhu et al., 2025)."
        },
        {
            "title": "Model",
            "content": "Param. eng deu nld fra ita spa por pol tam kaz cmn Allosaurus Allophant Wav2Vec2Phoneme MultIPA ZIPA-CR-Large ZIPA-CR-NS-Large 6.89 11M 300M 10.26 300M 7.70 300M 15.81 1.63 300M 1.40 300M 17.67 9.37 7.89 16.28 3.32 3.17 19.19 18.39 12.31 18.97 3.03 2.83 20.91 18.83 17.73 20.19 3.23 2.92 19.02 7.82 6.10 7.20 3.24 3. 4.82 17.37 3.67 6.99 1.98 1.53 19.61 15.44 11.65 15.04 4.01 3.40 21.21 7.90 9.57 2.63 4.33 4.31 12.01 19.32 15.63 10.54 4.59 4.15 20.90 15.30 17.71 2.31 1.87 15.28 14.66 21.10 1.25 0. Avg. 16.14 11.11 13.86 2.99 2."
        },
        {
            "title": "POWSM",
            "content": "350M 2.85 3.37 5.14 3.27 1. 1.21 2.90 1.36 3.56 2.25 1. 2.62 Table 2: PFER () on the in-domain dataset, IPAPack++. Languages not supported by Allophant are left blank. Some languages were not seen by MultiIPA. Bold indicates the best performance. Afroasiatic Turkic Indo-Iranian Balto-Slavic afr orm aze pan tgk mkd bos slv Model OWLS 0.5B OWLS 1B OWSM-CTC v4 1B 102.3 89.0 95.7 67.5 102.4 92. POWSM 0.35B, ASR 86.2 125.3 POWSM 0.35B, PR-P2G 68.8 93.0 77. 67.5 71.2 67.7 66.7 59.3 50. 88.7 83.1 72.8 60.4 50.7 57. 62.8 51.0 54.2 59.3 58.6 46.2 50.0 52.8 51.2 51.3 60.4 56.0 56.5 64. 48.6 56.9 63.9 Table 3: WER () of ASR and PR-P2G on low-resource languages. PR-P2G uses phones predicted by PR as text prompts instead of gold phones. Bold indicates the best performance, and underline indicates the second-best. Figure 2: Validation CER of encoder-CTC during training. Removing suprasegmentals for CTC accelerates convergence and reduces vocabulary size for encoder. We run small-scale experiments on 1k-hour subset of the multi-task data (250 hours of speech repeated across four tasks). We use the validation CER of the encoder-CTC output as proxy for training efficiency. An earlier drop indicates that the encoder is learning useful alignment early, which improves representations fed into the decoder and accelerates overall convergence. In Figure 2, PanPhon tokenization without suprasegmentals shows the earliest drop, suggesting that alignment with decoder units aids training, while collapsing suprasegmental distinctions for CTC reduces confusion. Increased encoder weights benefit PR on out-ofdomain data As in other encoder-decoder models (Gong et al., 2023; Radford et al., 2023), we expect the encoder of POWSM to capture more general acoustic patterns, while the decoder handles language and task-specific output formats. Therefore, we investigate whether emphasizing the encoder more during different stages of model development affects performance. To balance data diversity with inference compute costs, we selected two smaller datasets from each category with distinct characteristics. As shown in Table 5, higher CTC decoding weights improve PR performance on out-of-domain data but degrade it on in-domain data, as expected. This echoes Zhu et al. (2025)s finding that RNN-T (Graves and Jaitly, 2014), an encoder-only speechto-text model with an autoregressive text prediction network, hurts generalization to unseen patterns of phones (phonotactics). We hypothesize that the decoder is performing implicit language modeling and smooths phonetic variation, as Zhu et al. (2025) described. Next, we examine whether focusing more on the CTC loss through training widens this gap in performance between in-domain and out-of-domain data. We find that fine-tuning with higher CTC loss weight Î±ctc after convergence does not improve outof-domain performance and can even degrade it. Randomly varying Î±ctc for each batch also shows no improvement. In contrast, training with higher Î±ctc from the start benefits the out-of-domain distribution, achieving the lowest PFER on unseen languages with greedy decoding, while the PFER on in-domain data is comparatively higher. These results suggest that assigning higher weight to the encoder during training and inference improves PR, highlighting common trade-off between indomain performance and generalization. 6."
        },
        {
            "title": "Inspecting Task and Language Tokens",
            "content": "Speech-guided G2P preserves phonetic variation; text prompts normalize it To better understand how POWSM integrates speech and text prompts, we analyze the relative influence of speech and text prompts in its G2P behavior. We"
        },
        {
            "title": "Model",
            "content": "Param. DoReCo VoxAngeles Tusom2021 Avg. Buckeye DRC-SE L2-ARC EpaDB SO762 Avg."
        },
        {
            "title": "Language Variation",
            "content": "Allosaurus Allophant Wav2Vec2Phoneme MultIPA ZIPA-CR-Large ZIPA-CR-NS-Large"
        },
        {
            "title": "POWSM",
            "content": "11M 300M 300M 300M 300M 300M 350M 24.71 17.25 18.28 17.99 16.82 17.06 30.84 13.88 15.23 16.95 17.14 17. 42.02 31.92 30.53 23.68 23.08 21.96 32.52 21.02 21.35 19.54 19.01 18.71 15.24 16.05 12.50 18.69 12.04 12.05 12. 25.36 24.13 18.57 23.31 17.89 17.12 18.33 13.39 11.91 9.86 15.52 9.74 9.69 11.32 19.33 14.38 9.90 15.64 17.38 14.63 21.61 18.28 13.60 21.34 15.58 18. 18.99 16.95 12.89 18.90 14.53 14.34 11.86 17.84 14.40 Table 4: PFER () on out-of-domain data. DRC-SE stands for DoReCo South-England; L2-ARC stands for L2-ARCTIC; SO762 stands for SpeechOcean762. Unseen language datasets include languages not supported by Allophant; therefore, we do not report results for these datasets."
        },
        {
            "title": "Setup",
            "content": "pol VoxA. Tusom. DRC-SE EpaDB In-domain ita Out-of-domain 1.36 1.37 1.38 1.66 1.97 2.05 Decoding ctc=0.3 ctc=0.7 ctc=0.9 Pre-training / Fine-tuning Î±ctc=0.3 1.81 Ft, Î±ctc=0.5 1.94 Î±ctc=0.7 1.96 Ft, Î±ctc=0.5 2.01 Î±ctc=U (0.1,0.9) 1. 1.60 1.53 1.65 1.57 1.62 17.58 17.92 19.27 16.02 17.78 15.40 16.21 19.29 33.52 24.29 22.94 22.47 23.72 22.10 22.93 25.11 18.21 18.05 17. 18.59 18.73 18.50 18.41 18.92 11.88 11.82 11.80 11.66 11.82 11.62 11.47 11.33 Table 5: PFER () for different CTC weight settings. Ft denotes fine-tuning for 5 epochs from the checkpoint above. VoxAngeles and Tusom2021 are abbreviated. Pre-training and fine-tuning rows use ctc=0.3. All setups use beam=1."
        },
        {
            "title": "ASR Transcription",
            "content": "any holidays at all they just kind of ignore"
        },
        {
            "title": "Phonetic transcription",
            "content": "/EnihAl2deIsERAlsoUDeIdZ2stkAr2vIgnOÃ´/ PR G2P (speech) G2P (both) G2P (text prompt) 12. 12.71 16.38 23.44 /EnihAl@deIzÃ¦tOlsoUDeItS2stkh Ã¦n@vIgnOÃ´/ /EnihAl@deIzÃ¦tOlsoUDeItS2stkh Ã¦n@vIgnOÃ´/ /EnihAl@deIzÃ¦tOlDeItS2stkhInd@vIgnOÃ´/ /aIhoUÃ«daIzÃ¦tOÃ«DeItSIsthInd@vIgnÃ/ Table 6: Comparing G2P with different available modalities with PFER (). Blue for correctly capturing mispronounced parts (/soU/), orange for error compared to other examples. vary the G2P conditions from purely speech-based to purely text-based, as shown in Table 6, and evaluate the model on the Buckeye dataset. When only speech is provided, the performance is comparable to the PR setting, which differs only in the task token. Adding both speech and text prompts (the standard G2P setup) leads to degraded performance, with output showing standardized pronunciations. When the model relies solely on the text prompt, performance drops sharply and pronunciations become highly standardized as expected (just as Zhu et al. (2025) reported). In other words, POWSM G2P responds to 7 speech and text signals to controllably mediate between narrow and broad transcription. In the multi-task setup, this effect may be stronger because the model is trained with G2P, which could bias it toward more standardized forms. Audio-P2G effectively handles low-resource languages We compare several P2G setups on the same set of low-resource languages from FLEURS, listed in Table 7. P2G significantly outperforms ASR, suggesting that it effectively leverages the provided phone context. However, since P2G uses gold phone labels, this comparison is not entirely fair. We therefore tested PR followed by P2G (PRP2G), and found that performance improved for some languages but not for others. Error propagation does not explain this variation in performance, as PFER trends from PR differ from the observed performance drops. Yet the PFER pattern aligns closely with ASR results, suggesting that phonotactic similarity to high-resource languages may play role. To test this, we run P2G with the language code set to English and post-process the output to match Cyrillic or Gurmukhi transcriptions with online conversion tools for certain languages.6 This approach often outperforms ASR and sometimes approaches P2Gs performance, indicating that P2G also relies heavily on speech input. Languages with either comparibly low or high PFER did not benefit from this transliteration approach, possibly because the model already handled them well or had not yet learned them sufficiently. This finding suggests direction for further investigation in low-resource ASR. Language token captures phonotactics The language identification (LID) performance of POWSM on seen languages in FLEURS reaches 6https://www.lexilogos.com for Macedonian and Tajik; https://punjabi.indiatyping.com for Panjabi."
        },
        {
            "title": "Afroasiatic Turkic",
            "content": "Indo-Iranian Balto-Slavic"
        },
        {
            "title": "Task",
            "content": "afr orm aze pan tgk mkd bos slv"
        },
        {
            "title": "Best ASR",
            "content": "67.5 89.0 67.5 50.0 50.7 46. 50.0 52."
        },
        {
            "title": "ASR",
            "content": "86.2 125."
        },
        {
            "title": "55.9\nP2G\nP2G, lang=<eng> 60.4",
            "content": "PR-P2G PFER () 68.8 9.1 88.0 99. 93.0 12.9 67.7 37.1 64.2 66. 6.7 83.1 62.8 56 56.5 64.5 31. 32.3 40.3 36.9 52.6 95.8 74.0 52.6 39.6 53.5 56.9 63.9 48.6 72.8 51.0 7.9 5.7 3.3 6. 6.9 Table 7: WER () of different P2G settings on lowresource languages Best stands for lowest WER in Table 3 from ASR models. indicates post-processed languages. 92.3% accuracy, as shown in Figure 3. To see if the model implicitly learns phonotactic patterns and associates them with the language token, we evaluate PR on unseen languages by manipulating the language token at inference time. For VoxAngeles and Tusom2021, the three most frequently assigned languages are Bashkir (42.6%, 25.1%), English (30.2%, 67.7%), and Kinyarwanda (14.5%, 2.3%), which are all relatively high-resource languages in IPAPack++. Table 8 shows that assigning the detected language token yields better performance than always using English, while setting the language as unknown performs best. This indicates that the language token influences PR by shifting the output distribution toward the assigned language. Lang. token Voxangeles Tusom"
        },
        {
            "title": "Example",
            "content": "Phonetic transcription <unk> Detected <eng> 17.11 17.55 19.91 21.96 23.92 24.21 /adZm3/ > /a dZima/ /6jum/ /aItimO/ Table 8: PFER () of PR performance with different language token. The detected language in the example is <bak>. Blue for correct, orange for error compared to other examples."
        },
        {
            "title": "7 Conclusion",
            "content": "We train fully open-source phonetic speech foundation model POWSM using our scalable multitask framework. Our model achieves state-of-theart performance on PR while also supporting ASR across more than 70 languages. Beyond PR and ASR, the models ability to perform audio-guided G2P and P2G enables applications that require finegrained linguistic analysis such as atypical speech assessment. Our analysis reveals that POWSMs encoder benefits from phoneme-level CTC supervision and stronger encoder weighting, enhanc8 ing cross-lingual generalization. Additionally, the model demonstrates interpretable multimodal and language-aware behaviors, effectively mediating between phonetic detail and standardized phonological patterns. To conclude, POWSM not only provides strong phone recognition foundation model for high-resource languages, but also acts as versatile resource for unseen languages and socio-phonetic variation."
        },
        {
            "title": "8 Future Work",
            "content": "POWSMs current decoder serves as large phoneme-level phonotactic language model on which linguists could investigate hypotheses about phonetic universals (Chodroff et al., 2024; Chodroff, 2025) and phonotactics (Shim et al., 2024; Pimentel et al., 2020). In the future, we seek to adapt to socio-phonetic variation either through (unsupervised) test-time adaptation (Lin et al., 2022), in-context learning (Roll et al., 2025; Wang et al., 2024), or mechanistic interpretability (Tang et al., 2024). Furthermore, since Shim et al. (2025) found that earlier encoder layers in Whisper preserve more phonetic detail, early exiting may mitigate the decoders tendencies to normalize socio-phonetic variation."
        },
        {
            "title": "Limitations",
            "content": "POWSM has several limitations that we aim to address in future work. First, the model is neither strictly phonemic nor phonetic: its training data consist of cleaned and filtered phonemic transcriptions from multiple languages, which are not fully faithful to the phonetic or phonemic structure of the audio. Although phonemic transcriptions share similarities across languages, adding auxiliary tasks and language tokens may have reinforced language-specific biases. We also currently lack sufficient allophone-level data, which would provide more language-independent information. Second, the model still favors high-resource languages. Since we include decoder for language modeling and language tokens, both of which function effectively, the model would inherently bias toward the seen distribution. Finally, the current AED architecture imposes engineering limitations. Inference is significantly slower than with encoder-only models, and the architecture does not easily support tone modeling, limiting its application to tonal languages."
        },
        {
            "title": "Ethics Statement",
            "content": "All of our data is ethically sourced, either through permissive licensing or through proper consent. We are aware of the implicit prescriptivism and representational harms (Crawford, 2017) that normalizing socio-phonetic variation in ASR or PR models can create. This may threaten linguistic diversity instead of preserving it. We also acknowledge that accurate modeling of socio-phonetic variation can enable demographic inference, as demographics and phonetic variation are deeply intertwined (Labov, 1963). We stress that uses of POWSM must align with our vision: future where advances in spoken language processing and NLP do not leave low-resource varieties behind."
        },
        {
            "title": "The Use of LLMs",
            "content": "We acknowledge the use of large language models (LLMs) to assist with grammar correction and clarity improvements in writing this paper. All conceptual, methodological, and experimental contributions were developed independently by the authors."
        },
        {
            "title": "References",
            "content": "Madhavaraj A, Bharathi Pilar, and Ramakrishnan G. 2022. Subword dictionary learning and segmentation techniques for automatic speech recognition in tamil and kannada. Preprint, arXiv:2207.13331. Arun Babu, Changhan Wang, Andros Tjandra, Kushal Lakhotia, Qiantong Xu, Naman Goyal, Kritika Singh, Patrick von Platen, Yatharth Saraf, Juan Pino, Alexei Baevski, Alexis Conneau, and Michael Auli. 2022. XLS-R: Self-supervised Cross-lingual Speech Representation Learning at Scale. In Interspeech 2022, pages 22782282. Alexei Baevski, Yuhao Zhou, Abdelrahman Mohamed, and Michael Auli. 2020. wav2vec 2.0: framework for self-supervised learning of speech representations. Advances in neural information processing systems, 33:1244912460. Yonatan Belinkov and James Glass. 2017. Analyzing hidden representations in end-to-end automatic speech recognition systems. Advances in Neural Information Processing Systems, 30. Alan Black and Kevin Lenzo. 2001. Flite: small fast run-time synthesis engine. In SSW, page 204. Eleanor Chodroff. 2025. Phonetic universals. Annual Review of Linguistics, 11. Eleanor Chodroff, BlaÅ¾ PaÅ¾on, Annie Baker, and Steven Moran. 2024. Phonetic segmentation of the ucla phonetics lab archive. In Proceedings of the 2024 Joint International Conference on Computational Linguistics, Language Resources and Evaluation (LRECCOLING 2024), pages 1272412733. Kwanghee Choi, Eunjung Yeo, Kalvin Chang, Shinji Watanabe, and David Mortensen. 2025. Leveraging allophony in self-supervised speech models for atypical pronunciation assessment. In Proceedings of the 2025 Conference of the Nations of the Americas Chapter of the Association for Computational Linguistics: Human Language Technologies (Volume 1: Long Papers), pages 26132628, Albuquerque, New Mexico. Association for Computational Linguistics. Kate Crawford. 2017. The trouble with bias. Conference on Neural Information Processing Systems. Li Fu, Yu Xin, Sunlu Zeng, Lu Fan, Youzheng Wu, Pac: Pronunciationand Xiaodong He. 2025. aware contextualized large language model-based arXiv preprint automatic speech recognition. arXiv:2509.12647. Heting Gao, Mark Hasegawa-Johnson, and Chang Yoo. 2024. G2pu: Grapheme-to-phoneme transducer with speech units. In ICASSP 2024-2024 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), pages 1006110065. IEEE. Heting Gao, Junrui Ni, Yang Zhang, Kaizhi Qian, Shiyu Chang, and Mark Hasegawa-Johnson. 2021. Zeroshot cross-lingual phonetic recognition with external In Interspeech 2021, pages language embedding. 13041308. Kevin Glocker, Aaricia Herygers, and Munir Georges. 2023. Allophant: Cross-lingual phoneme recognition with articulatory attributes. In Interspeech 2023, pages 22582262. Yuan Gong, Sameer Khurana, Leonid Karlinsky, and James Glass. 2023. Whisper-at: Noise-robust automatic speech recognizers are also strong general audio event taggers. In Interspeech 2023, pages 2798 2802. Graves. 2006. Connectionist temporal classification: labelling unsegmented sequence data with recurrent In Proc. Int. Conf. on Machine neural networks. Learning, 2006, pages 369376. Alex Graves and Navdeep Jaitly. 2014. Towards endto-end speech recognition with recurrent neural networks. In International conference on machine learning, pages 17641772. PMLR. William Chen, Jinchuan Tian, Yifan Peng, Brian Yan, Chao-Han Huck Yang, and Shinji Watanabe. 2025. OWLS: Scaling laws for multilingual speech recognition and translation models. In Forty-second International Conference on Machine Learning. Gerard I. GÃ¡llego, Oriol Pareras, MartÃ­ Cortada Garcia, Lucas Takanori, and Javier Hernando. 2025. Speechto-text translation with phoneme-augmented cot: Enhancing cross-lingual transfer in low-resource scenarios. Preprint, arXiv:2505.24691. 9 Taiqi He, Kwanghee Choi, Lindia Tjuatja, Nathaniel Robinson, Jiatong Shi, Shinji Watanabe, Graham Neubig, David Mortensen, and Lori Levin. 2024. Wav2Gloss: Generating interlinear glossed text from speech. In Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 568582, Bangkok, Thailand. Association for Computational Linguistics. SolÃ¨ne Inceoglu, Wen-Hsin Chen, and Hyojung Lim. 2023. Assessment of l2 intelligibility: Comparing l1 listeners and automatic speech recognition. ReCALL: the Journal of EUROCALL, 35(1):89104. International Phonetic Association. 1999. Handbook of the International Phonetic Association: guide to the use of the International Phonetic Alphabet. Cambridge University Press. Yerbolat Khassanov, Saida Mussakhojayeva, Almas Mirzakhmetov, Alen Adiyev, Mukhamet Nurpeiissov, and Huseyin Atakan Varol. 2021. crowdsourced open-source Kazakh speech corpus and initial speech recognition baseline. In Proceedings of the 16th Conference of the European Chapter of the Association for Computational Linguistics: Main Volume, pages 697706, Online. Association for Computational Linguistics. William Labov. 1963. The social motivation of sound change. Word, 19(3):273309. Peter Ladefoged, Barbara Blankenship, Russell G. Schuh, Patrick Jones, Nicole Gfroerer, Emily Griffiths, Lisa Harrington, Cheryl Hipp, Mayu Kaneko, Claire Moore-Cantwell, Gunhye Oh, Karen Pfister, Keli Vaughan, Rosary Videc, Sarah Weismuller, Samara Weiss, Jamie White, Sarah Conlon, WingSze Jamie Lee, and Rafael Toribio. 2009. The UCLA Phonetics Lab Archive. Davor Lauc. 2024. Polyipamultilingual phonemearXiv preprint to-grapheme conversion model. arXiv:2412.09102. Chin-Jou Li, Eunjung Yeo, Kwanghee Choi, Paula Andrea PÃ©rez-Toro, Masao Someki, Rohan Kumar Das, Zhengjun Yue, Juan Rafael Orozco-Arroyave, Elmar NÃ¶th, and David Mortensen. 2025. Towards inclusive asr: Investigating voice conversion for dysarthric speech recognition in low-resource languages. arXiv preprint arXiv:2505.14874. Kun Li, Xiaojun Qian, and Helen Meng. 2016. Mispronunciation detection and diagnosis in l2 english speech using multidistribution deep neural networks. IEEE/ACM Transactions on Audio, Speech, and Language Processing, 25(1):193207. Xinjian Li, Siddharth Dalmia, Juncheng Li, Matthew Lee, Patrick Littell, Jiali Yao, Antonios Anastasopoulos, David Mortensen, Graham Neubig, Alan Black, and 1 others. 2020. Universal phone recognition with multilingual allophone system. In ICASSP 2020-2020 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), pages 82498253. IEEE. Xinjian Li, Juncheng Li, Florian Metze, and Alan Black. 2021. Hierarchical phone recognition with compositional phonetics. In Interspeech, pages 2461 2465. Guan-Ting Lin, Shang-Wen Li, and Hung yi Lee. 2022. Listen, Adapt, Better WER: Source-free Singleutterance Test-time Adaptation for Automatic Speech Recognition. In Interspeech 2022, pages 21982202. Ryo Magoshi, Shinsuke Sakai, Jaeyoung Lee, and Tatsuya Kawahara. 2025. Multi-lingual and Zero-Shot Speech Recognition by Incorporating Classification of Language-Independent Articulatory Features. In Interspeech 2025, pages 9195. Timothy Shin Heng Mak, King Yiu Suen, and Albert Lam. 2025. Speech-guided grapheme-to-phoneme In Proc. conversion for cantonese text-to-speech. Interspeech 2025, pages 25352539. David Mortensen, Siddharth Dalmia, and Patrick Littell. 2018. Epitran: Precision g2p for many languages. In Proceedings of the Eleventh International Conference on Language Resources and Evaluation (LREC 2018). David R. Mortensen, Patrick Littell, Akash Bharadwaj, Kartik Goyal, Chris Dyer, and Lori S. Levin. 2016. Panphon: resource for mapping IPA segments In Proceedings of to articulatory feature vectors. COLING 2016, the 26th International Conference on Computational Linguistics: Technical Papers, pages 34753484. ACL. David Mortensen, Jordan Picone, Xinjian Li, and Kathleen Siminyu. 2021. Tusom2021: phonetically transcribed speech dataset from an endangered language for universal phone recognition experiments. In Proc. Interspeech 2021, pages 36603664. Ludger Paschen, FranÃ§ois Delafontaine, Christoph Draxler, Susanne Fuchs, Matthew Stave, and Frank Building time-aligned crossSeifart. 2020. linguistic reference corpus from language documentation data (doreco). In Proceedings of the 12th Conference on Language Resources and Evaluation (LREC 2020). European Language Resources Association. Yifan Peng, Muhammad Shakeel, Yui Sudo, William Chen, Jinchuan Tian, Chyi-Jiunn Lin, and Shinji Watanabe. 2025. OWSM v4: Improving Open Whisper-Style Speech Models via Data Scaling and Cleaning. In Interspeech 2025, pages 22252229. Yifan Peng, Jinchuan Tian, William Chen, Siddhant Arora, Brian Yan, Yui Sudo, Muhammad Shakeel, Kwanghee Choi, Jiatong Shi, Xuankai Chang, Jee weon Jung, and Shinji Watanabe. 2024. OWSM v3.1: Better and Faster Open Whisper-Style Speech Models based on E-Branchformer. In Interspeech 2024, pages 352356. Yifan Peng, Jinchuan Tian, Brian Yan, Dan Berrebbi, Xuankai Chang, Xinjian Li, Jiatong Shi, Siddhant Arora, William Chen, Roshan Sharma, and 1 others. 10 2023. Reproducing whisper-style training using an open-source toolkit and publicly available data. In 2023 IEEE ASRU, pages 18. IEEE. Tiago Pimentel, Brian Roark, and Ryan Cotterell. 2020. Phonotactic complexity and its trade-offs. Transactions of the Association for Computational Linguistics, 8:118. Mark Pitt, Keith Johnson, Elizabeth Hume, Scott Kiesling, and William Raymond. 2005. The buckeye corpus of conversational speech: Labeling conventions and test of transcriber reliability. Speech Communication, 45(1):8995. Vineel Pratap, Andros Tjandra, Bowen Shi, Paden Tomasello, Arun Babu, Sayani Kundu, Ali Elkahky, Zhaoheng Ni, Apoorv Vyas, Maryam Fazel-Zarandi, and 1 others. 2024. Scaling speech technology to 1,000+ languages. Journal of Machine Learning Research, 25(97):152. Alec Radford, Jong Wook Kim, Tao Xu, Greg Brockman, Christine McLeavey, and Ilya Sutskever. 2023. Robust speech recognition via large-scale weak supervision. In International conference on machine learning, pages 2849228518. PMLR. Nathan Roll, Calbert Graham, Yuka Tatsumi, Kim Tien Nguyen, Meghan Sumner, and Dan Jurafsky. 2025. In-context learning boosts speech recognition via human-like adaptation to speakers and language varieties. arXiv preprint arXiv:2505.14887. James Route, Steven Hillis, Isak Czeresnia Etinger, Han Zhang, and Alan Black. 2019. Multimodal, multilingual grapheme-to-phoneme conversion for low-resource languages. In Proceedings of the 2nd Workshop on Deep Learning Approaches for LowResource NLP (DeepLo 2019), pages 192201, Hong Kong, China. Association for Computational Linguistics. Alexander Rudnicky. 1993. The cmu pronouncing dictionary. Accessed on October 2, 2025. Siqi Sun and Korin Richmond. 2024. Acquiring pronunciation knowledge from transcribed speech arXiv preprint audio via multi-task learning. arXiv:2409.09891. Chihiro Taguchi, Yusuke Sakai, Parisa Haghani, and David Chiang. 2023. Universal automatic phonetic transcription into the international phonetic alphabet. In Interspeech 2023, pages 25482552. Tianyi Tang, Wenyang Luo, Haoyang Huang, Dongdong Zhang, Xiaolei Wang, Xin Zhao, Furu Wei, and Ji-Rong Wen. 2024. Language-specific neurons: The key to multilingual capabilities in large language models. arXiv preprint arXiv:2402.16438. Bert Vaux and Scott Golder. 2003. The Harvard Dialect Survey. JazmÃ­n Vidal, Luciana Ferrer, and Leonardo Brambilla. 2019. Epadb: database for development of pronunciation assessment systems. In INTERSPEECH, pages 589593. Siyin Wang, Chao-Han Yang, Ji Wu, and Chao Zhang. 2024. Can whisper perform speech-based in-context learning? In ICASSP 2024-2024 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), pages 1342113425. IEEE. Shinji Watanabe, Takaaki Hori, Shigeki Karita, Tomoki Hayashi, Jiro Nishitoba, Yuya Unno, Nelson Enrique Yalta Soplin, Jahn Heymann, Matthew Wiesner, Nanxin Chen, and 1 others. 2018. Espnet: End-toend speech processing toolkit. Interspeech 2018. Shinji Watanabe, Takaaki Hori, Suyoun Kim, John R. Hershey, and Tomoki Hayashi. 2017. Hybrid ctc/attention architecture for end-to-end speech recognition. IEEE Journal of Selected Topics in Signal Processing, 11(8):12401253. Qiantong Xu, Alexei Baevski, and Michael Auli. 2022. Simple and effective zero-shot cross-lingual phoneme recognition. In Interspeech 2022, pages 21132117. Farhan Samir, Emily P. Ahn, Shreya Prakash, MÃ¡rton Soskuthy, Vered Shwartz, and Jian Zhu. 2025. comparative approach for auditing multilingual phonetic transcript archives. Transactions of the Association for Computational Linguistics, 13:595612. Zengwei Yao, Liyong Guo, Xiaoyu Yang, Wei Kang, Fangjun Kuang, Yifan Yang, Zengrui Jin, Long Lin, and Daniel Povey. 2024. Zipformer: faster and better encoder for automatic speech recognition. International Conference on Learning Representations. Ryan Soh-Eun Shim, Kalvin Chang, and David Mortensen. 2024. Phonotactic complexity across dialects. In Proceedings of the 2024 Joint International Conference on Computational Linguistics, Language Resources and Evaluation (LREC-COLING 2024), pages 1273412748. Ryan Soh-Eun Shim, Domenico De Cristofaro, Chengzhi Martin Hu, Alessandro Vietti, and Barbara Plank. 2025. Languages in multilingual speech foundation models align both phonetically and semantically. arXiv preprint arXiv:2505.19606. Saierdaer Yusuyin, Te Ma, Hao Huang, Wenbo Zhao, and Zhijian Ou. 2025. Whistle: Data-efficient multilingual and crosslingual speech recognition via IEEE Transactions weakly phonetic supervision. on Audio, Speech and Language Processing. Junbo Zhang, Zhiwen Zhang, Yongqing Wang, Zhiyong Yan, Qiong Song, Yukai Huang, Ke Li, Daniel Povey, and Yujun Wang. 2021. speechocean762: An opensource non-native english speech corpus for pronunciation assessment. In Proc. Interspeech 2021, pages 37103714. 11 lowest-resource languages to exclude any that may have other substantial sources not included in IPAPack++. This process leaves us with nine languages. We finally exclude ell, as it is comparatively higher-resource and because there are already three other Balto-Slavic languages. Note that other models use strictly more data than oursnot only in terms of dataset count but also because IPAPack++ applies additional data-quality filtering. Table 10 lists the amount of ASR training data for baselines. A.4 Multi-tasking at Different Scales Multi-tasking may improve performance by tying acoustic signals to well-defined symbolic representations, yet it may distract the model if the relationships are not learned effectively. We train POWSM with different data and model scales to examine how multitask learning interacts with the setup, and use beam=1 during decoding to speed up inference. Table 11 shows that there is no clear trend regarding whether multitasking benefits PR performance. PR performance degrades when the model has excessive capacity relative to the available data (too little data), or when it is limited by size (too much data). Further evidence is needed before concluding that phoneme recognition benefits less from scaling, as we currently lack sufficient data and large model capacity to test this thoroughly. Nevertheless, the model demonstrates the ability to multitask, which represents promising direction for future work. Guanlong Zhao, Sinem Sonsaat, Alif Silpachai, Ivana Lucic, Evgeny Chukharev-Hudilainen, John Levis, and Ricardo Gutierrez-Osuna. 2018. L2-arctic: In Proc. Internon-native english speech corpus. speech 2018, pages 27832787. Xuehao Zhou, Xiaohai Tian, Grandee Lee, Rohan Kumar Das, and Haizhou Li. 2020. End-to-end codeswitching tts with cross-lingual language model. In ICASSP 2020 - 2020 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), pages 76147618. Jian Zhu, Farhan Samir, Eleanor Chodroff, and David R. Mortensen. 2025. ZIPA: family of efficient models for multilingual phone recognition. In Proceedings of the 63rd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 1956819585, Vienna, Austria. Association for Computational Linguistics. Jian Zhu, Cong Zhang, and David Jurgens. 2022. ByT5 model for massively multilingual graphemeto-phoneme conversion. In Interspeech 2022, pages 446450."
        },
        {
            "title": "A Appendix",
            "content": "A.1 Refining English G2P We observed confusion in plosive voice-onset times on unseen languages in preliminary experiments, which is likely from English G2P data. For instance, broad phonemic transcription in English typically uses /b/ to transcribe the /b/ in /bat/, but its voice onset timing is actually voiceless in Mainstream American English and is closer to [p]. To mitigate this, we apply rule-based refinements to English G2P transcriptions, adjusting plosive voicing and aspiration, lateral velarization, and vowel nasalization. The rules are listed below: 1) word-initial voiceless plosives (/p/, /t/, /k/) are aspirated, 2) wordinitial voiced plosives (/b/, /d/, /g/) are voiceless, 3) lateral /l/ is velarized at the end of syllables, and 4) vowel nasalization before nasal consonants. A.2 Baseline Implementation We provide the baselines training data source, number of languages covered in the data, and links to model checkpoints or repository in Table 9. A.3 FLEURS language selection for ASR We first filter out languages with more than 8 hours of training data in IPAPack++ (Zhu et al., 2025), keeping only those that are also present in FLEURS. Then, following the training data amounts reported in Chen et al. (2025), we further identify the"
        },
        {
            "title": "Training Data Sources",
            "content": "Language Coverage Model checkpoint / GitHub PR baselines Allosaurus (Li et al., 2020, 2021) Allophant (Glocker et al., 2023) VoxForge, Japanese CSJ, Hkust Tedlium, Switchboard etc Common Voice 10.0 Wav2Vec2Phoneme MLS, Common Voice, (Xu et al., 2022)"
        },
        {
            "title": "Babel",
            "content": "MultIPA (Taguchi et al., 2023) Common Voice 11.0 ZIPA (Zhu et al., 2025) IPAPack++ MMS ulab v2., VoxLingua-107 (Pseudo-label) ASR baselines OWSM-CTC v4 (Peng et al., 2025) OWLS (Chen et al., 2025) OWSM v3.2, YODAS OWSM v3.2, YODAS 12 34 40+ 88 100+ 150 xinjli/allosaurus kgnlp/allophant facebook/wav2vec2-xlsr-53-espeak-cv-ft ctaguchi/wav2vec2-large-xlsr-japlmthufielta-ipa1000-ns lingjzhu/zipa anyspeech/zipa-large-crctc-ns-800k espnet/owsm_ctc_v4_1B espnet/owls-scaling-laws-for-speech-recognition-and-translation Table 9: Overview of the baselines for our work."
        },
        {
            "title": "Afroasiatic Turkic",
            "content": "Indo-Iranian Balto-Slavic"
        },
        {
            "title": "Model",
            "content": "afr orm aze pan tgk mkd bos slv"
        },
        {
            "title": "POWSM",
            "content": "2.71 5.11 6.89 4.96 6.52 5.14 7.57 7. OWSM-CTC v"
        },
        {
            "title": "OWLS",
            "content": "5.54 6.50 10.69 8.30 8.03 8.4 9.96 26. Table 10: Amount of ASR training data for languages included in ASR comparison (in hours), according to Zhu et al. (2025) and Chen et al. (2025). Data ( khr) Tasks Params. VoxAngeles Tusom2021 L2-Arctic 0.25 0.25 0.25 0.25 17 17 17 17 17 17 1 4 1 4 1 2 4 1 2 100M 100M 300M 300M 100M 100M 100M 300M 300M 300M 27.22 26.30 20.63 23.81 17.88 24.69 30.07 17.08 17.17 17.58 32.59 30.32 25.83 25.91 26.68 49.28 61.89 25.20 23.70 33. 13.50 13.40 12.88 14.14 11.76 11.35 12.37 10.50 10.47 10.54 Table 11: Comparison of PFER () on different seting of tasks and data. 1 task refers to PR, 2 tasks refer to PR+ASR, and 4 tasks include PR, ASR, P2G, and G2P. 13 Figure 3: Confusion matrix of LID on FLEURS."
        }
    ],
    "affiliations": [
        "Carnegie Mellon University",
        "University of British Columbia",
        "University of California, Berkeley",
        "University of Texas, Austin"
    ]
}