{
    "paper_title": "OmniPart: Part-Aware 3D Generation with Semantic Decoupling and Structural Cohesion",
    "authors": [
        "Yunhan Yang",
        "Yufan Zhou",
        "Yuan-Chen Guo",
        "Zi-Xin Zou",
        "Yukun Huang",
        "Ying-Tian Liu",
        "Hao Xu",
        "Ding Liang",
        "Yan-Pei Cao",
        "Xihui Liu"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "The creation of 3D assets with explicit, editable part structures is crucial for advancing interactive applications, yet most generative methods produce only monolithic shapes, limiting their utility. We introduce OmniPart, a novel framework for part-aware 3D object generation designed to achieve high semantic decoupling among components while maintaining robust structural cohesion. OmniPart uniquely decouples this complex task into two synergistic stages: (1) an autoregressive structure planning module generates a controllable, variable-length sequence of 3D part bounding boxes, critically guided by flexible 2D part masks that allow for intuitive control over part decomposition without requiring direct correspondences or semantic labels; and (2) a spatially-conditioned rectified flow model, efficiently adapted from a pre-trained holistic 3D generator, synthesizes all 3D parts simultaneously and consistently within the planned layout. Our approach supports user-defined part granularity, precise localization, and enables diverse downstream applications. Extensive experiments demonstrate that OmniPart achieves state-of-the-art performance, paving the way for more interpretable, editable, and versatile 3D content."
        },
        {
            "title": "Start",
            "content": "OmniPart: Part-Aware 3D Generation with Semantic Decoupling and Structural Cohesion YUNHAN YANG, The University of Hong Kong, China YUFAN ZHOU, Harbin Institute of Technology, China YUAN-CHEN GUO, VAST, China ZI-XIN ZOU, VAST, China YUKUN HUANG, The University of Hong Kong, China YING-TIAN LIU, VAST, China HAO XU, Zhejiang University, China DING LIANG, VAST, China YAN-PEI CAO, VAST, China XIHUI LIU, The University of Hong Kong, China 5 2 0 2 8 ] . [ 1 5 6 1 6 0 . 7 0 5 2 : r Fig. 1. OmniPart: Generating Complex 3D Objects as Compositions of Controllable Parts. From simple 2D images (shown framed) and mask inputs, OmniPart first plans 3D part structure using an autoregressive model, then synthesizes all high-quality, textured parts simultaneously (individual components within transparent displays). These seamlessly merge into coherent object, offering explicit part control for enhanced editing, customization, and animation. Equal contribution. Corresponding author. Authors addresses: Yunhan Yang, The University of Hong Kong, China, yhyang. myron@gmail.com; Yufan Zhou, Harbin Institute of Technology, China, yfzhou@stu. hit.edu.cn; Yuan-Chen Guo, VAST, China, imbennyguo@gmail.com; Zi-Xin Zou, VAST, China, zouzx1997@gmail.com; Yukun Huang, The University of Hong Kong, China, kunh6414@gmail.com; Ying-Tian Liu, VAST, China, liuyingt23@mails.tsinghua.edu.cn; Hao Xu, Zhejiang University, China, haoxu38@outlook.com; Ding Liang, VAST, China, The creation of 3D assets with explicit, editable part structures is crucial for advancing interactive applications, yet most generative methods produce only monolithic shapes, limiting their utility. We introduce OmniPart, novel framework for part-aware 3D object generation designed to achieve high semantic decoupling among components while maintaining robust structural cohesion. OmniPart uniquely decouples this complex task into liangding1990@163.com; Yan-Pei Cao, VAST, China, caoyanpei@gmail.com; Xihui Liu, The University of Hong Kong, China, xihuiliu@eee.hku.hk. 2 Yunhan Yang, Yufan Zhou, Yuan-Chen Guo, Zi-Xin Zou, Yukun Huang, Ying-Tian Liu, Hao Xu, Ding Liang, Yan-Pei Cao, and Xihui Liu two synergistic stages: (1) an autoregressive structure planning module generates controllable, variable-length sequence of 3D part bounding boxes, critically guided by flexible 2D part masks that allow for intuitive control over part decomposition without requiring direct correspondences or semantic labels; and (2) spatially-conditioned rectified flow model, efficiently adapted from pre-trained holistic 3D generator, synthesizes all 3D parts simultaneously and consistently within the planned layout. Our approach supports user-defined part granularity, precise localization, and enables diverse downstream applications. Extensive experiments demonstrate that OmniPart achieves state-of-the-art performance, paving the way for more interpretable, editable, and versatile 3D content. Project page: https://omnipart.github.io/. Additional Key Words and Phrases: 3D Generation, Part-aware, Diffusion, Autoregressive Models"
        },
        {
            "title": "INTRODUCTION",
            "content": "The creation of rich, interactive 3D worlds is fundamental to modern visual computing, driving applications from immersive gaming and virtual reality to digital twins and robotic interaction. persistent challenge is the generation of 3D assets that are not merely static, monolithic forms. While recent generative models produce impressive holistic 3D shapes [Li et al. 2025; Poole et al. 2023; Xiang et al. 2024; Zhang et al. 2023, 2024b], these often lack the intrinsic part-based structure inherent to real-world objects. This structural opaqueness limits their direct use for essential tasks such as compositional editing, procedural animation, material assignment, and semantic understandingcapabilities crucial for artists, developers, and downstream systems. Part-aware 3D generation [Chen et al. 2024b; Liu et al. 2024a; Yang et al. 2025b], which explicitly models objects as assemblies of semantically meaningful components, offers path to overcome these limitations. However, creating such structured assets requires careful balance: achieving low semantic coupling (parts are distinct and independently addressable) while ensuring high structural cohesion (parts form plausible, integrated whole). Prior work has often struggled here. Approaches reconstructing from 2D part segmentations [Chen et al. 2024b; Liu et al. 2024a] suffer from geometric inconsistencies and insufficient detail fidelity, primarily due to challenges in view aggregation and the inherent limitations of 2D models in capturing complete 3D part geometry. Conversely, existing methods for direct 3D part generation [Li et al. 2024b] may lack robust, fine-grained control over part decomposition or rely on extensive, scarce 3D part-level annotations, hindering their scalability and practical use. We emphasize that robust and versatile part-aware 3D generation hinges on principled decoupling of high-level structural planning from detailed part synthesis, unified by strong conditioning mechanisms. We introduce OmniPart, novel framework that implements this principle through synergistic two-stage pipeline. Initially, Controllable Structure Planning module addresses the crucial task of defining the part-level spatial layout. It autoregressively generates variable-length sequence of 3D bounding boxes, each representing distinct parts 3D location. key challenge here is compositional ambiguity (e.g., limbs vs. composite torso), which can lead to unpredictability. OmniPart resolves this by conditioning the planning on intuitive, flexible 2D part masks (obtainable from user input or 2D models like SAM [Ravi et al. 2024]). These masks, which delineate desired regions for part decomposition without imposing strict one-to-one correspondences or requiring explicit semantic labels, guide the transformer-based autoregressive model to produce 3D bounding boxes reflecting the intended part structure and accommodate varying part counts. To further enhance the accuracy of this structural plan, we introduce novel Part Coverage Loss, ensuring that each predicted bounding box comprehensively encloses its corresponding object part. Subsequently, building on this planned structure, SpatiallyConditioned Part Synthesis module efficiently generates consistent and high-quality 3D representations for all object parts. Given the scarcity of extensive part-level 3D annotations, our approach effectively adapts powerful, pre-trained holistic 3D generator (TRELLIS [Xiang et al. 2024]) based on rectified flow into part-aware 3D generation model. The predicted bounding boxes from the first stage define spatial regions within TRELLISs voxel representation, serving as initializations for part-wise latent codes. To promote semantic awareness and overall structural coherence, each parts voxel representation is considered alongside the full-object context, and distinctive part-aware embeddings are injected. All part latents are then jointly refined via denoising process that operates on both global and local information, ensuring consistency. Since voxels only approximate the coarse geometry of objects and parts, overlaps frequently occur at the boundaries between adjacent parts. The voxel initialization from the bounding boxes of the first stage may introduce noise. To address this, we introduce novel voxel discarding mechanism, enabling precise indication of whether voxel actually belongs to its assigned part, which aids in creating clean interfaces and allows for the efficient, simultaneous generation of all parts. These fine-tuning strategies yield detailed, coherent, and generalizable part-level 3D outputs despite limited part-specific supervision. OmniPart enables the generation of 3D objects with explicit, controllable, and semantically meaningful part structures. Extensive experiments demonstrate that our framework achieves state-ofthe-art performance in part-aware 3D generation and facilitates diverse range of downstream applications, including fine-grained compositional editing, material assignment, and animation support. Collectively, OmniPart significantly advances the creation of more interpretable and editable 3D content. Our core contributions are: novel two-stage generative formulation and framework that strategically decouples part structure planning from part geometry synthesis, achieving superior controllability, part coherence, and overall quality. An autoregressive controllable structure planning module guided by flexible 2D part masks, providing effective control over variable part granularity and 3D decomposition. spatially-conditioned part synthesis module that generates all 3D parts simultaneously and consistently, conditioned on the planned spatial layout, efficiently leveraging powerful pre-trained holistic models for high-fidelity part-aware generation with limited part-specific supervision. OmniPart: Part-Aware 3D Generation with Semantic Decoupling and Structural Cohesion"
        },
        {
            "title": "2 RELATED WORK\n2.1",
            "content": "3D Shape Generation DreamFusion [Poole et al. 2023] introduces score distillation sampling to optimize 3D scenes from 2D diffusion models [Rombach et al. 2022], enabling text-driven 3D generation without 3D supervision. complementary line of work [Liu et al. 2024c,b; Long et al. 2024; Qi et al. 2024; Shi et al. 2023; Xu et al. 2024; Yang et al. 2025a, 2024b; Zou et al. 2024] generates multi-view images using 2D diffusion models and reconstructs 3D geometry via multi-view consistency algorithms. However, these 2D-driven pipelines often suffer from geometric artifacts due to inherent view inconsistency in 2D diffusion models and the lack of native 3D supervision. To address these limitations, several methods directly model 3D data distributions using latent diffusion in geometry-aware spaces. Approaches based on Variational Autoencoders (VAEs) [Chou et al. 2023; Dai et al. 2017; Hui et al. 2022; Koo et al. 2023; Lan et al. 2025; Li et al. 2023, 2024a, 2025; Shim et al. 2023; Wu et al. 2024; Zhang et al. 2025, 2024b; Zhao et al. 2024] encode 3D shapes into latent representations, enabling efficient and effective 3D synthesis via diffusion models. 3DShape2VecSet [Zhang et al. 2023] introduces cross-attention-based encoding for set-structured 3D data, while CLAY [Zhang et al. 2024b] scales latent 3D diffusion to large datasets with hierarchical training strategy, achieving strong generation quality. TRELLIS [Xiang et al. 2024] proposes structured latentsa unified 3D latent representationthat enables high-quality 3D object generation through two-stage, coarse-to-fine process. And several recent methods, inspired by MeshGPT [Siddiqui et al. 2024], condition on 3D point clouds and leverage autoregressive models to directly generate mesh faces [Chen et al. 2024a, 2025, 2024c; Hao et al. 2024; Liu et al. 2025b; Tang et al. 2025a; Wang et al. 2025; Weng et al. 2024; Zhao et al. 2025]."
        },
        {
            "title": "2.2 Compositional Generation",
            "content": "Compositional Generation has been extensively studied in the 2D domain. Several works [Chatterjee et al. 2024; Han et al. 2025; Huang et al. 2025, 2023; Zhang et al. 2024a] focus on generating compositional images from text, enabling more controllable and structured outputs. Other methods [Jia et al. 2023; Pu et al. 2025; Zhang and Agrawala 2024] explore compositionality at the layer level, modeling images as combination of independently generated components. More recently, number of works have begun to investigate 3D part generation, extending compositional reasoning into the 3D domain [Chen et al. 2024b,d; Li et al. 2024b; Lin et al. 2025; Liu et al. 2024a; Tang et al. 2025b; Yan et al. 2024]. Comboverse [Chen et al. 2024d] reconstructs each object or part independently, followed by multi-object composition stage. However, since parts are generated in isolation without global structural coherence, the final compositions often lack geometric consistency. Part123 [Liu et al. 2024a] first generates multi-view images of the object and then reconstructs the final shape using image masks. Despite this, the reconstructed parts remain fused together, with surface-level segmentation. PASTA [Li et al. 2024b] proposes an autoregressive transformer that represents 3D objects as sequences of cuboidal primitives and employs transformer-based blending network to synthesize meshes. PartGen [Chen et al. 2024b] also adopts multiview pipeline: it segments and inpaints occluded regions across images before reconstructing individual parts. However, the inconsistency among multi-view images often results in low geometric fidelity in the final 3D shapes. 2.3 3D Part Segmentation Understanding the compositional structure of 3D objects through part-level segmentation is long-standing and fundamental task in 3D vision. Earlier research [Li et al. 2018; Qi et al. 2017a,b; Qian et al. 2022; Zhao et al. 2021] focused primarily on designing deep architectures that learn expressive geometric representations from point clouds or meshes. These models typically rely on fully supervised learning, which necessitates large-scale part annotationsresources that are costly to obtain and limited in coverage. As result, their scalability to more diverse or open-world 3D domains remains limited. To address these limitations, recent work [Abdelreheem et al. 2023; Kim and Sung 2024; Liu et al. 2024a, 2023; Tang et al. 2024; Thai et al. 2024; Xue et al. 2023; Yang et al. 2024a, 2023; Zhong et al. 2024] has turned to 2D foundation models, including SAM [Kirillov et al. 2023], GLIP [Li et al. 2022], and CLIP [Radford et al. 2021], as source of transferable visual knowledge. These approaches typically project 3D shapes into multiple 2D views, perform segmentation in the image space, and lift the 2D masks back onto the 3D surface. 3D part segmentation can serve as means of extracting parts from complete shape. While effective in segmenting visible surfaces, these pipelines inherently lack access to occluded or interior structures, leading to incomplete and view-biased segmentations. Such limitations hinder their utility in downstream applications that demand full understanding of object geometry. Furthermore, HoloPart [Yang et al. 2025b] completes each part based on input surface masks and the overall object. Its performance, however, is heavily dependent on the quality of the input segmentation masks."
        },
        {
            "title": "3 PART-AWARE 3D OBJECT GENERATION",
            "content": "We aim to generate part-aware, controllable, and high-quality 3D content conditioned on input image with 2D part masks. To this end, we propose OmniPart, framework composed of two key stages: part structure planning and structured part latent generation (see Figure 2). We build on the explicit sparse voxel representation introduced in TRELLIS [Xiang et al. 2024], which offers spatially structured latent space well-suited for generating coherent and semantically meaningful parts. In Section 3.1, we provide brief overview of TRELLIS and its structured latent generation framework. We then describe our Controllable Structure Planning module, which autoregressively generates part-level bounding boxes (Section 3.2). Next, we introduce our approach to Spatially-Conditioned Part Synthesis via fine-tuning on large-scale pretrained model of wholeobject shapes (Section 3.3). Finally, we describe the training data construction procedures for each module in Section 3.4."
        },
        {
            "title": "3.1 Preliminary: Generation of Structured Latent",
            "content": "TRELLIS [Xiang et al. 2024] introduces structured latent representation. Given 3D asset C, it encodes both geometric and appearance information into unified structured latent c, which consists of 4 Yunhan Yang, Yufan Zhou, Yuan-Chen Guo, Zi-Xin Zou, Yukun Huang, Ying-Tian Liu, Hao Xu, Ding Liang, Yan-Pei Cao, and Xihui Liu Fig. 2. An overview of the OmniPart model design. OmniPart generates part-aware, controllable, and high-quality 3D content through two key stages: part structure planning and structured part latent generation. Built upon TRELLIS [Xiang et al. 2024], which provides spatially structured sparse voxel latent space, OmniPart first predicts part-level bounding boxes via an autoregressive planner. Then, part-specific latent codes are generated through fine-tuning of large-scale shape model pretrained on overall objects. Fig. 3. Spatially-conditioned part synthesis. The sparse voxels of the whole shape and each part are filled with noisy latents, which are denoised with network composed of part-aware sparse downsample/upsample layers and transformer layers. The tokens are augmented with position embeddings and part position embeddings (PPE). The denoising process also predicts validity score for each voxel to discard redundant voxels (the ones with stripes in the figure) in each box. set of local latents defined on 3D grid: = {(f𝑖, p𝑖 )}𝐿 𝑖=1 , f𝑖 R𝐷, p𝑖 {0, 1, . . . , 𝑁 1}3, (1) where p𝑖 denotes the positional index of an active voxel in the 3D grid that intersects with the surface of C, and f𝑖 represents the local latent feature associated with that voxel. 𝑁 is the spatial resolution of the 3D grid. Intuitively, the active voxels p𝑖 outline the coarse structure of the 3D asset, while the latents f𝑖 capture finer details of its geometry and appearance. TRELLIS adopts two-stage approach. In the first stage, it generates the active voxels of the 3D asset C, which cover the entire object and include its internal structure. In the second stage, it trains rectified flow model to generate the latent features for each surface voxel. These latents can then be decoded into meshes, NeRFs [Mildenhall et al. 2021], or 3D Gaussian Splatting (3DGS) [Kerbl et al. 2023] representations."
        },
        {
            "title": "Bounding Boxes Generation",
            "content": "In this subsection we introduce our module design to plan the part structures of 3D objects using coarse yet flexible representation: 3D bounding boxes. Each generated box corresponds to meaningful part, and we employ an autoregressive generation mechanism to produce an arbitrary number of these boxes. Bounding box Tokenizer. To enable autoregressive generation, we convert each 3D bounding box into discrete token sequence. straightforward approach is to flatten the boxs minimum and maximum coordinates into 6n-dimensional vector, where is the number of bounding boxes. We then prepend bos (begin-ofsequence) token and append eos (end-of-sequence) token to clearly mark the sequence boundaries. Moreover, predefined ordering strategy is essential to facilitate sequence modeling in bounding box generation. We sort the bounding boxes based on their minimum coordinates in z-y-x order, from lower to higher. Controllability and Conditions. Since an object may consist of multiple fine-grained parts, its interpretation during planning is often ambiguous. For example, characters hands may be treated either as separate components or grouped together with the arms. To better control the granularity of the generated bounding boxes, we introduce 2D mask-based control method. Given conditional input image, 2D masks can be readily extracted using segmentation OmniPart: Part-Aware 3D Generation with Semantic Decoupling and Structural Cohesion 5 models such as SAM [Kirillov et al. 2023], serving as additional conditioning signals. However, since single mask cannot encompass all object parts, there is no one-to-one correspondence between the 2D masks and the 3D bounding boxes. To address this, we adopt non-one-to-one correspondence bounding boxes, which are labelindependent and eliminate the need for explicit matching between bounding boxes and 2D mask regions. Given an input image 𝐼 R𝐻 𝑊 3, we first extract visual features using DINOv2 [Oquab et al. 2023]: 𝑓 = DINOv2(𝐼 ) Rℎ𝑤𝑑, where ℎ 𝑤 denotes the spatial resolution of the output feature map and 𝑑 is the feature dimension. To introduce part-aware conditioning, we resize the 2D mask 𝑀 {0, 1, . . . , 𝐾1}ℎ𝑤, where each element 𝑀𝑖,𝑗 indicates the 2D part index at location (𝑖, 𝑗), and 𝐾 is the maximum number of parts. Correspondingly, we introduce learnable embedding table: 𝐸 R𝐾 𝑑, 𝐸 = nn.Embedding(𝐾, 𝑑), where each row 𝐸 [𝑘] provides an embedding for the 𝑘-th part label. The final part-conditioned feature map 𝑓 Rℎ𝑤𝑑 is obtained by summing the visual features with the part embeddings at each spatial location: 𝑓 𝑖,𝑗 = 𝑓𝑖,𝑗 + 𝐸 [𝑀𝑖,𝑗 ]. To directly apply the generated bounding boxes in the Structured Part Latent Generation stage (see Sec. 3.3), we align them with the sparse voxel spatial coordinates used in that stage. To facilitate this alignment, we first generate voxels using the structure generation stage of TRELLIS. The voxels are treated as point clouds and converted into fixed-length tokens 𝑞 using 3DShape2VecSet [Zhang et al. 2023] encoder. The part-conditioned feature 𝑓 is flattened and concatenated with voxel tokens 𝑞, forming conditioning tokens as the prefix of the sequence. Training. Bounding box generation can be formulated as an autoregressive sequence modeling task, making it well-suited for modern Transformer architectures. In this work, we adopt decoder-only Transformers based on the OPT [Zhang et al. 2022] codebase as our foundation. Given the trainable parameters 𝜃 and sequence 𝑠 of length 𝑠 , the next-token prediction loss is defined as: L𝑏𝑎𝑠𝑒 (𝜃 ) = 𝑠 𝑖=1 log 𝑃 (cid:16) 𝑠 [𝑖 ] 𝑠 [1,...,𝑖 1] ; [𝑓 ; 𝑞] (cid:17) . To ensure that each bounding box represents its corresponding object part as completely as possible, we introduce Part Coverage Loss. Specifically, since our second stage model has the invalid voxel discarding ability (described below), we aim to generate relatively larger bounding boxes to ensure complete voxel coverage for each part. The coverage loss is defined as: Lcoverage (𝜃 ) = (cid:32) 1 ReLU (cid:16) 𝑠pred 𝑖 (cid:17) 𝑠gt 𝑖 𝑖 Mmin + ReLU (cid:16) 𝑖 𝑠pred 𝑠gt 𝑖 (cid:17) (cid:33) , 𝑖 Mmax (2) Fig. 4. Visualization of the training dataset. We show the distribution of part counts across the dataset (Number of parts per model vs. Frequency) and include representative examples from four different part-count ranges. where Mmin and Mmax denote token positions corresponding to the minimum and maximum bounding box coordinates respectively. This loss penalizes bounding boxes that are too small by encouraging the predicted minimum coordinates to be smaller and the predicted maximum coordinates to be larger than the ground truth. The final loss is defined as: Ltotal = Lbase + 𝜆cov Lcoverage, where scalar weight 𝜆cov is applied to control the strength of the coverage regularization."
        },
        {
            "title": "3.3 Spatially-Conditioned Part Synthesis",
            "content": "Through part structure planning, we obtain bounding boxes that are spatially aligned with the sparse voxels. The voxels 𝑣𝑖 within the 𝑖-th bounding box represent the coarse geometry of the corresponding part. At this stage, our objective is to efficiently generate all parts while maintaining coherence across them. To this end, we introduce structured part latent generation module, built upon the second stage of TRELLIS. Given the limited availability of high-quality partlevel annotations, we aim to leverage the prior knowledge in the pre-trained model as much as possible. As shown in Figure 3, the noisy latents of the whole shape and each part are downsampled and flattened as 1D sequence before being fed into the transformer blocks. Here the use of whole shape tokens is to better preserve the base model prior and enhance part-to-whole coherence. To distinguish between different parts within the Transformer, we introduce part position embedding (PPE) scheme. Tokens representing the overall shape, placed at the beginning of the sequence, are assigned shared PPE index of 0. Each subsequent part is assigned unique PPE index starting from 1, and all tokens within the same part share the same embedding. This design enables simultaneous denoising and decoding of all parts into consistent final representations - including meshes, NeRFs, and 3D Gaussians - in an efficient manner. Since voxels can only approximate the coarse geometry of objects and parts, overlapping voxels often appear at the junctions between different parts. Moreover, directly using all voxels within bounding box for shape initialization may introduce noise voxels that belong to other parts. To address this, we propose voxel discarding mechanism to filter out invalid voxels during initialization. We augment the original part latent with an additional dimension fvalid to indicate voxel validity. During training, we assign value of -𝛼 to 6 Yunhan Yang, Yufan Zhou, Yuan-Chen Guo, Zi-Xin Zou, Yukun Huang, Ying-Tian Liu, Hao Xu, Ding Liang, Yan-Pei Cao, and Xihui Liu this new dimension for noisy voxels and +𝛼 for valid voxels, where 𝛼 is predefined constant. During inference, we determine voxel validity by applying sigmoid activation to the additional latent dimension and comparing the result to threshold 𝛽. voxel is retained if its score exceeds 𝛽. In our experiments, we set 𝛽 = 0.5: valid voxel sigmoid(fvalid) > 𝛽. To generate structured part latents, we build on the pretrained TRELLIS model and adopt rectified flow approach with linear interpolation forward process: x(𝑡) = (1 𝑡)x0 + 𝑡𝝐, which interpolates between the data sample x0 and noise 𝝐 at timestep 𝑡. The noise term 𝝐 is approximated by neural network v𝜃 , trained by minimizing the Conditional Flow Matching (CFM) objective [Lipman et al. 2024]: LCFM (𝜃 ) = E𝑡,x0,𝝐 v𝜃 (x, 𝑡) (𝝐 x0)2 2 ."
        },
        {
            "title": "3.4 Training Data",
            "content": "To train our models, we construct dataset with part-level annotations. We begin by collecting 180K objects with part labels through combination of filtering and manual annotation. Since the quality of part annotations varies across objects, we design scoring system to assess labeling quality. Based on this system, 15K objects are identified as high-quality. We analyze the dataset distribution with respect to part count and present representative examples in Figure 4. We then construct corresponding training sets for the two modules respectively. Autoregressive Bounding Box Generation. At this stage, due to the absence of suitable pre-trained prior, we construct training data using all 180K annotated shapes and train the autoregressive model from scratch. We extract the bounding box for each part and arrange them in zyx ascending order. These boxes are then tokenized to sequences for training the autoregressive model. Structured Part Latent Generation. This stage builds on the second stage of TRELLIS for fine-tuning, using the 15K high-quality annotated shapes. Following similar procedure to TRELLIS for constructing object-level latents, we randomly render 150 views for each part, extract features using DINOv2 [Oquab et al. 2023], and unproject them onto 3D voxels for fusion. We also construct the training data that includes noise voxels. Specifically, we voxelize each part and combine them to form the full-object voxel representation. Then, we extract the voxels within each parts bounding box as initialization, which may include noise voxels from adjacent parts."
        },
        {
            "title": "4 EXPERIMENTS",
            "content": "Evaluation Protocol. To evaluate our model, we construct test set of 300 objects sampled from the dataset described in Section 3.4. Based on the number of parts, we divide the objects into four groups: 05, 610, 1115, and 1650 parts. Objects were selected proportionally from each group to ensure both part count diversity and category coverage. We first evaluate the bounding box generation (planning) stage of our model in Section 4.1. Next, we assess the full pipeline for part-aware 3D content generation in Section 4.2. Finally, we showcase practical applications of our method in Section 4.3. Method PartField [Liu et al. 2025a] OmniPart (w/o 2D mask) OmniPart (w/o coverage loss) OmniPart (Ours) Voxel recall Voxel IoU Bbox IoU 39.02 31.44 50.56 61.02 79.12 66.98 64.50 85.96 27.30 25.90 41.24 38.37 Table 1. Quantitative results for bounding box generation (%). We report BBox IoU to measure bounding box overlap, Voxel Recall to assess coverage of valid part voxels, and Voxel IoU to quantify overall voxel-level consistency between predictions and ground truth."
        },
        {
            "title": "4.1 Bounding Box Generation",
            "content": "Metrics and Baselines. To evaluate our models performance in the bounding box generation stage, we introduce three metrics: BBox IoU, Voxel recall, and Voxel IoU. BBox IoU directly measures the overlap between the predicted bounding box and the ground truth. Since the primary goal of this stage is to provide coarse part voxels for the subsequent latent generation stage, we also introduce two voxel-level metrics. Voxel recall assesses the proportion of valid part voxels that fall within the predicted bounding boximportant because the Spatially-Conditioned Part Synthesis stage can discard extraneous voxels but cannot recover missing ones. Voxel IoU computes the intersection-over-union between the voxels inside the predicted and ground-truth bounding boxes, providing global measure of voxel-level overlap. Since our predicted bounding boxes are not constrained by semantic labels or quantity, there is no oneto-one correspondence between ground truth and predictions. To compute the metrics, we match each ground-truth bounding box with its proximate predicted counterpart. At this stage, we obtain the objects overall coarse voxels from input images and aim to plan the bounding boxes for individual parts. PartField [Liu et al. 2025a], which supports zero-shot point cloud segmentation, serves as baseline for comparison. We treat the voxel representation as point cloud and feed it into PartField to obtain per-point segmentation features. As PartField requires the number of parts to define the segmentation scale, we provide the ground-truth number of parts as input. After segmenting the point cloud, we extract the bounding box of each predicted part and use these to compute evaluation metrics. Results and Ablation Analysis. As shown in the first and last rows of Table 1, our results significantly exceed the results of PartField. It means that the accuracy and integrity of our predicted bounding box are better than the baseline model. We also conduct ablation experiments to assess the influence of the coverage loss and 2D mask on bounding box generation. Specifically, we train one model without coverage loss and another without the 2D mask input, and evaluate their performance. As shown in Table 1, although the model without coverage loss produces bounding boxes that appear closer to the ground truth, its voxel-level recall and IoU are significantly lower, which negatively impacts the performance of the second stage. And the model without the 2D mask input fails to effectively control the size and placement of the generated bounding boxes."
        },
        {
            "title": "4.2 Part-Aware 3D Content Generation",
            "content": "Metrics and Baselines. We consider three primary approaches for part-aware 3D content generation. The first approach generates OmniPart: Part-Aware 3D Generation with Semantic Decoupling and Structural Cohesion 7 Fig. 5. Qualitative comparison of part-aware 3D generation. Our method leverages TRELLIS to decode both mesh and 3D Gaussian splats, baking color onto the mesh to produce textured parts. HoloPart and Part123 are visualized using solid colors due to the lack of texture support. Segmentation-based methods (e.g., PartField) capture only surface-level masks, while Completion-based methods (e.g., HoloPart) are limited by segmentation quality. PartGen generates full parts but with low geometric and semantic quality. In contrast, our method achieves low semantic coupling and high structural cohesion. Fig. 6. Applications of our part-aware 3D generation framework. (a) Mask-Controlled Generation: Users can specify 2D masks to guide the structure of the generated parts. (b) Multi-Granularity Generation: Adjusting the segmentation scale of 2D masks enables generation at different levels of part granularity. (c) Material Editing: Part-specific textures, such as clothing items, can be modified independently. (d) Geometry Processing: Our part-aware outputs support high-quality geometry processing (such as remeshing) and preserve structural coherence, avoiding artifacts at part boundaries. Yunhan Yang, Yufan Zhou, Yuan-Chen Guo, Zi-Xin Zou, Yukun Huang, Ying-Tian Liu, Hao Xu, Ding Liang, Yan-Pei Cao, and Xihui Liu Method CD 0.49 0.19 0.19 0.43 0.44 0.18 F1-0.5 CD 0.08 0.08 0.08 0.47 0.11 0.07 TRELLIS+SAM3D TRELLIS+PartField TRELLIS+PartField+HoloPart Part123 PartGen OmniPart (Ours) Part-level F1-0.1 0.38 0.69 0.68 0.31 0.43 0.74 Table 2. Quantitative results of part-level and whole-object generation (%). We report Chamfer Distance (CD) and F1-score after normalizing all shapes to unified scale within [-0.5, 0.5]. F1-score is computed at two thresholds (CD < 0.1 and CD < 0.05) to capture both coarse and fine geometric accuracy. To account for orientation differences, each object is evaluated under four rotations (0, 90, 180, 270), and the best score is reported. Overall-object F1-0.1 0.92 0.92 0.91 0.33 0.86 0.93 F1-0.5 0.77 0.77 0.77 0.19 0.69 0.80 0.28 0.52 0.51 0.16 0.30 0.59 Method Time (minute) Part123 [Liu et al. 2024a] 15 PartGen [Chen et al. 2024b] Ours 0. 5 Table 3. End-to-end generation time comparison from single image to part-level 3D outputs. complete 3D shape and then segments it into distinct parts. The second builds upon this by completing each part individually after segmentation. The third directly infers part-aware 3D shapes from single image. For the first approach, we adopt TRELLIS+SAM3D [Yang et al. 2023] and TRELLIS+PartField [Liu et al. 2025a] as baseline methods. For the second, we use TRELLIS+Partfield+HoloPart [Yang et al. 2025b]. For the third, we compare against Part123 [Liu et al. 2024a] and PartGen [Chen et al. 2024b]. To evaluate both the quality of individual generated parts and the overall coherence of the merged object, we report metrics at two levels: part-level and whole-object. Specifically, we normalize both the ground-truth and predicted overall shapes to unified scale within [-0.5, 0.5] before computing Chamfer Distance (CD) and F1-score for each level. For the F1-score, we use two distance thresholdsCD < 0.1 and CD < 0.05to assess coarseand finelevel geometric alignment. Due to the varying orientations of objects generated by different methods, we rotate each object by 0, 90, 180, and 270 degrees and evaluate the results under each rotation. We report the highest score obtained across these orientations. Comparison Results. We present quantitative results in Table 2. As shown in the last three rows of Table 2, our method significantly outperforms the part-aware generation baselines at both the partlevel and whole-object level. It is worth noting that the first two rows under the overall-object category correspond to results directly generated by TRELLIS without part decomposition. In contrast, our merged full-object shapes achieve higher performance, as our method can generate complete geometry for each part, including the boundaries and occluded regionsareas that TRELLIS alone cannot accurately reconstruct when generating the object as whole. The qualitative results are shown in Figure 5. Our method leverages TRELLISs ability to decode both mesh and 3D Gaussian splats simultaneously. We bake the color information from the 3D Gaussians onto the mesh surface, thereby generating textured parts. For Holopart and Part123, we use the official implementations. Since their generated meshes do not contain textures, we visualize them using solid colors. Segmentation-based methods can only produce surface-level masks and fail to recover complete part geometry. HoloPart relies on completion strategy but remains constrained by the quality of the initial segmentation. While PartGen is capable of generating complete parts, its outputs often suffer from low geometric fidelity and limited semantic plausibility. In contrast, our method produces results with low semantic coupling and high structural cohesion, ensuring both part-level independence and global consistency. Pipeline Results. We present the results of our complete pipeline in Figure 7. The figure shows the input image and 2D masks, along with the generated bounding boxes, individually generated part meshes, and the combined full-object mesh. As illustrated, our method enables precise control over part granularity via 2D masks and generates high-quality geometry and texture. Efficiency. We aim to make part-aware 3D content generation efficient. Part123 requires generating multiple views, optimizing the reconstruction process, and then obtaining the segmentation results. PartGen first generates multi-view images and performs segmentation on them to obtain multi-view part masks. These segmented views are then completed and used to reconstruct each part in 3D. In contrast, our unified backbone design enables simultaneous generation of all parts and supports direct decoding into mesh, 3D Gaussian splats (3DGS), or NeRF representations. We compare the end-to-end generation timefrom single image to part-level 3D outputsagainst both methods, and as shown in Table 3, our approach demonstrates substantial efficiency gains."
        },
        {
            "title": "4.3 Applications",
            "content": "Our method generates high-quality part-aware 3D content directly from single input image and naturally supports range of downstream applications, including Animation, Mask-Controlled Generation, Multi-Granularity Generation, Material Editing, and Geometry Processing, as illustrated in Figure 6. We showcase the animation of our generated meshes in the video. Mask-Controlled Generation. We can easily control the generation of 3D parts by specifying 2D mask. To facilitate this, we design simple and efficient process to obtain accurate 2D masks by merging over-segmented regions produced by SAM. An example is shown in Figure 6 (a), where 2D mask is used to control the structure of the generated robots. Multi-Granularity Generation. We achieve multi-granularity 3D part generation by controlling the granularity of the 2D segmentation masks. As shown in Figure 6(b), we generate 3D objects with different part granularities by adjusting the segmentation scale of the 2D masks for the robot and the sketchpad. Material Editing. With the generated part-aware 3D objects, material properties can be easily edited at the part level. For example, in Figure 6(c), we modify the textures of the penguins hat, tie, clothes, and pants. This enables designers to freely customize object appearance with fine-grained control. Geometry Processing. With the part-aware generated 3D objects, geometry processing becomes significantly more convenient and effective. For example, in Figure 6(d), we convert the generated mesh from triangles to quadrilaterals using remeshing tool. Our part-aware representations handle each part cleanly and consistently, whereas non-part-aware results often exhibit artifacts at the junctions between parts. OmniPart: Part-Aware 3D Generation with Semantic Decoupling and Structural Cohesion 9 Fig. 7. Qualitative results of our complete pipeline. We show the input image and 2D masks, along with the generated bounding boxes, individually generated part meshes, and the combined full-object mesh. As illustrated, our method enables precise control over part granularity via 2D masks and produces high-quality geometry and texture. The generated 3D parts exhibit low semantic entanglement and high structural cohesion, demonstrating the effectiveness of our part-aware 3D content generation. 10 Yunhan Yang, Yufan Zhou, Yuan-Chen Guo, Zi-Xin Zou, Yukun Huang, Ying-Tian Liu, Hao Xu, Ding Liang, Yan-Pei Cao, and Xihui Liu"
        },
        {
            "title": "5 CONCLUSION AND DISCUSSION",
            "content": "Conclusion. We present OmniPart, part-aware 3D generation framework that addresses the limitations of traditional monolithic 3D models by explicitly modeling semantically meaningful and structurally coherent object parts. Our two-stage pipeline first plans controllable part-level spatial layout via autoregressive bounding box prediction, guided by flexible 2D masks. Then, conditioned on this layout, spatially-aware synthesis module generates high-quality 3D parts simultaneously and efficiently, leveraging pretrained holistic generator. Despite limited part-level supervision, OmniPart produces detailed and coherent part structures with low semantic coupling and high structural cohesion. It enables precise control over part granularity and supports wide range of applications, including animation, material editing, and geometric processing. Extensive experiments confirm its state-of-the-art performance and practical versatility, advancing the creation of interpretable, editable 3D assets for modern visual computing. Limitation. To simplify training in the first stage, we currently use axis-aligned bounding boxes. However, in some cases, this may result in excessive inclusion of noisy voxels being passed to the second stage. Exploring more precise representations for structure planning remains promising direction for future work."
        },
        {
            "title": "REFERENCES",
            "content": "Ahmed Abdelreheem, Ivan Skorokhodov, Maks Ovsjanikov, and Peter Wonka. 2023. Satr: Zero-shot semantic segmentation of 3d shapes. In ICCV. Agneet Chatterjee, Gabriela Ben Melech Stan, Estelle Aflalo, Sayak Paul, Dhruba Ghosh, Tejas Gokhale, Ludwig Schmidt, Hannaneh Hajishirzi, Vasudev Lal, Chitta Baral, et al. 2024. Getting it right: Improving spatial consistency in text-to-image models. In ECCV. Minghao Chen, Roman Shapovalov, Iro Laina, Tom Monnier, Jianyuan Wang, David Novotny, and Andrea Vedaldi. 2024b. PartGen: Part-level 3D Generation and Reconstruction with Multi-View Diffusion Models. arXiv preprint arXiv:2412.18608 (2024). Sijin Chen, Xin Chen, Anqi Pang, Xianfang Zeng, Wei Cheng, Yijun Fu, Fukun Yin, Billzb Wang, Jingyi Yu, Gang Yu, et al. 2024a. Meshxl: Neural coordinate field for generative 3d foundation models. In NeurIPS. Yiwen Chen, Tong He, Di Huang, Weicai Ye, Sijin Chen, Jiaxiang Tang, Xin Chen, Zhongang Cai, Lei Yang, Gang Yu, et al. 2025. Meshanything: Artist-created mesh generation with autoregressive transformers. In ICLR. Yongwei Chen, Tengfei Wang, Tong Wu, Xingang Pan, Kui Jia, and Ziwei Liu. 2024d. Comboverse: Compositional 3d assets creation using spatially-aware diffusion guidance. In ECCV. Yiwen Chen, Yikai Wang, Yihao Luo, Zhengyi Wang, Zilong Chen, Jun Zhu, Chi Zhang, and Guosheng Lin. 2024c. Meshanything v2: Artist-created mesh generation with adjacent mesh tokenization. arXiv preprint arXiv:2408.02555 (2024). Gene Chou, Yuval Bahat, and Felix Heide. 2023. Diffusion-sdf: Conditional generative modeling of signed distance functions. In ICCV. Angela Dai, Charles Ruizhongtai Qi, and Matthias Nießner. 2017. Shape completion using 3d-encoder-predictor cnns and shape synthesis. In CVPR. Woojung Han, Yeonkyung Lee, Chanyoung Kim, Kwanghyun Park, and Seong Jae Hwang. 2025. Spatial Transport Optimization by Repositioning Attention Map for Training-Free Text-to-Image Synthesis. arXiv preprint arXiv:2503.22168 (2025). Zekun Hao, David Romero, Tsung-Yi Lin, and Ming-Yu Liu. 2024. Meshtron: HighFidelity, Artist-Like 3D Mesh Generation at Scale. arXiv preprint arXiv:2412.09548 (2024). Kaiyi Huang, Chengqi Duan, Kaiyue Sun, Enze Xie, Zhenguo Li, and Xihui Liu. 2025. T2ICompBench++: An Enhanced and Comprehensive Benchmark for Compositional Text-to-Image Generation. IEEE Transactions on Pattern Analysis and Machine Intelligence (2025). Kaiyi Huang, Kaiyue Sun, Enze Xie, Zhenguo Li, and Xihui Liu. 2023. T2i-compbench: comprehensive benchmark for open-world compositional text-to-image generation. In NeurIPS. Ka-Hei Hui, Ruihui Li, Jingyu Hu, and Chi-Wing Fu. 2022. Neural wavelet-domain diffusion for 3d shape generation. In SIGGRAPH Asia. Peidong Jia, Chenxuan Li, Yuhui Yuan, Zeyu Liu, Yichao Shen, Bohan Chen, Xingru Chen, Yinglin Zheng, Dong Chen, Ji Li, et al. 2023. COLE: Hierarchical Generation Framework for Multi-Layered and Editable Graphic Design. arXiv preprint arXiv:2311.16974 (2023). Bernhard Kerbl, Georgios Kopanas, Thomas Leimkühler, and George Drettakis. 2023. 3d gaussian splatting for real-time radiance field rendering. ACM Trans. Graph. 42, 4 (2023), 1391. Hyunjin Kim and Minhyuk Sung. 2024. PartSTAD: 2D-to-3D Part Segmentation Task Adaptation. In ECCV. Alexander Kirillov, Eric Mintun, Nikhila Ravi, Hanzi Mao, Chloe Rolland, Laura Gustafson, Tete Xiao, Spencer Whitehead, Alexander Berg, Wan-Yen Lo, et al. 2023. Segment anything. In ICCV. Juil Koo, Seungwoo Yoo, Minh Hieu Nguyen, and Minhyuk Sung. 2023. Salad: Part-level latent diffusion for 3d shape generation and manipulation. In ICCV. Yushi Lan, Fangzhou Hong, Shuai Yang, Shangchen Zhou, Xuyi Meng, Bo Dai, Xingang Pan, and Chen Change Loy. 2025. Ln3diff: Scalable latent neural fields diffusion for speedy 3d generation. In ECCV. Liunian Harold Li, Pengchuan Zhang, Haotian Zhang, Jianwei Yang, Chunyuan Li, Yiwu Zhong, Lijuan Wang, Lu Yuan, Lei Zhang, Jenq-Neng Hwang, et al. 2022. Grounded language-image pre-training. In CVPR. Muheng Li, Yueqi Duan, Jie Zhou, and Jiwen Lu. 2023. Diffusion-sdf: Text-to-shape via voxelized diffusion. In CVPR. Songlin Li, Despoina Paschalidou, and Leonidas Guibas. 2024b. PASTA: Controllable Part-Aware Shape Generation with Autoregressive Transformers. arXiv preprint arXiv:2407.13677 (2024). Weiyu Li, Jiarui Liu, Rui Chen, Yixun Liang, Xuelin Chen, Ping Tan, and Xiaoxiao Long. 2024a. CraftsMan: High-fidelity Mesh Generation with 3D Native Generation and Interactive Geometry Refiner. arXiv preprint arXiv:2405.14979 (2024). Yangyan Li, Rui Bu, Mingchao Sun, Wei Wu, Xinhan Di, and Baoquan Chen. 2018. Pointcnn: Convolution on x-transformed points. In NeurIPS. Yangguang Li, Zi-Xin Zou, Zexiang Liu, Dehu Wang, Yuan Liang, Zhipeng Yu, Xingchao Liu, Yuan-Chen Guo, Ding Liang, Wanli Ouyang, et al. 2025. TripoSG: HighFidelity 3D Shape Synthesis using Large-Scale Rectified Flow Models. arXiv preprint arXiv:2502.06608 (2025). Yuchen Lin, Chenguo Lin, Panwang Pan, Honglei Yan, Yiqiang Feng, Yadong Mu, and Katerina Fragkiadaki. 2025. PartCrafter: Structured 3D Mesh Generation via Compositional Latent Diffusion Transformers. arXiv preprint arXiv:2506.05573 (2025). Yaron Lipman, Ricky TQ Chen, Heli Ben-Hamu, Maximilian Nickel, and Matt Le. 2024. Flow matching for generative modeling. In NeurIPS. Anran Liu, Cheng Lin, Yuan Liu, Xiaoxiao Long, Zhiyang Dou, Hao-Xiang Guo, Ping Luo, and Wenping Wang. 2024a. Part123: part-aware 3d reconstruction from single-view image. In ACM SIGGRAPH. Jian Liu, Haohan Weng, Biwen Lei, Xianghui Yang, Zibo Zhao, Zhuo Chen, Song Guo, Tao Han, and Chunchao Guo. 2025b. FreeMesh: Boosting Mesh Generation with Coordinates Merging. arXiv preprint arXiv:2505.13573 (2025). Minghua Liu, Ruoxi Shi, Linghao Chen, Zhuoyang Zhang, Chao Xu, Xinyue Wei, Hansheng Chen, Chong Zeng, Jiayuan Gu, and Hao Su. 2024c. One-2-3-45++: Fast Single Image to 3D Objects with Consistent Multi-View Generation and 3D Diffusion. In CVPR. Minghua Liu, Mikaela Angelina Uy, Donglai Xiang, Hao Su, Sanja Fidler, Nicholas Sharp, and Jun Gao. 2025a. PARTFIELD: Learning 3D Feature Fields for Part Segmentation and Beyond. arXiv preprint arXiv:2504.11451 (2025). Minghua Liu, Yinhao Zhu, Hong Cai, Shizhong Han, Zhan Ling, Fatih Porikli, and Hao Su. 2023. Partslip: Low-shot part segmentation for 3d point clouds via pretrained image-language models. In CVPR. Yuan Liu, Cheng Lin, Zijiao Zeng, Xiaoxiao Long, Lingjie Liu, Taku Komura, and Wenping Wang. 2024b. SyncDreamer: Learning to Generate Multiview-consistent Images from Single-view Image. In ICLR. Xiaoxiao Long, Yuan-Chen Guo, Cheng Lin, Yuan Liu, Zhiyang Dou, Lingjie Liu, Yuexin Ma, Song-Hai Zhang, Marc Habermann, Christian Theobalt, et al. 2024. Wonder3d: Single image to 3d using cross-domain diffusion. In CVPR. Ben Mildenhall, Pratul Srinivasan, Matthew Tancik, Jonathan Barron, Ravi Ramamoorthi, and Ren Ng. 2021. Nerf: Representing scenes as neural radiance fields for view synthesis. Commun. ACM 65, 1 (2021), 99106. Maxime Oquab, Timothée Darcet, Théo Moutakanni, Huy Vo, Marc Szafraniec, Vasil Khalidov, Pierre Fernandez, Daniel Haziza, Francisco Massa, Alaaeldin El-Nouby, et al. 2023. Dinov2: Learning robust visual features without supervision. arXiv preprint arXiv:2304.07193 (2023). Ben Poole, Ajay Jain, Jonathan Barron, and Ben Mildenhall. 2023. DreamFusion: Text-to-3D using 2D Diffusion. In ICLR. Yifan Pu, Yiming Zhao, Zhicong Tang, Ruihong Yin, Haoxing Ye, Yuhui Yuan, Dong Chen, Jianmin Bao, Sirui Zhang, Yanbin Wang, et al. 2025. Art: Anonymous region transformer for variable multi-layer transparent image generation. In CVPR. Charles Qi, Hao Su, Kaichun Mo, and Leonidas Guibas. 2017a. Pointnet: Deep learning on point sets for 3d classification and segmentation. In CVPR. Charles Ruizhongtai Qi, Li Yi, Hao Su, and Leonidas Guibas. 2017b. Pointnet++: Deep Bowen Zhang, Tianyu Yang, Yu Li, Lei Zhang, and Xi Zhao. 2025. Compress3D: hierarchical feature learning on point sets in metric space. In NeurIPS. compressed latent space for 3D generation from single image. In ECCV. OmniPart: Part-Aware 3D Generation with Semantic Decoupling and Structural Cohesion 11 Gaoyang Zhang, Bingtao Fu, Qingnan Fan, Qi Zhang, Runxing Liu, Hong Gu, Huaqi Zhang, and Xinguo Liu. 2024a. CoMPaSS: Enhancing Spatial Understanding in Text-to-Image Diffusion Models. arXiv preprint arXiv:2412.13195 (2024). Lvmin Zhang and Maneesh Agrawala. 2024. Transparent Image Layer Diffusion using Latent Transparency. ACM Transactions on Graphics (TOG) 43, 4 (2024), 115. Longwen Zhang, Ziyu Wang, Qixuan Zhang, Qiwei Qiu, Anqi Pang, Haoran Jiang, Wei Yang, Lan Xu, and Jingyi Yu. 2024b. CLAY: Controllable Large-scale Generative Model for Creating High-quality 3D Assets. ACM Transactions on Graphics (TOG) 43, 4 (2024), 120. Susan Zhang, Stephen Roller, Naman Goyal, Mikel Artetxe, Moya Chen, Shuohui Chen, Christopher Dewan, Mona Diab, Xian Li, Xi Victoria Lin, et al. 2022. Opt: Open pre-trained transformer language models. arXiv preprint arXiv:2205.01068 (2022). Hengshuang Zhao, Li Jiang, Jiaya Jia, Philip HS Torr, and Vladlen Koltun. 2021. Point transformer. In ICCV. Ruowen Zhao, Junliang Ye, Zhengyi Wang, Guangce Liu, Yiwen Chen, Yikai Wang, and Jun Zhu. 2025. DeepMesh: Auto-Regressive Artist-mesh Creation with Reinforcement Learning. arXiv preprint arXiv:2503.15265 (2025). Zibo Zhao, Wen Liu, Xin Chen, Xianfang Zeng, Rui Wang, Pei Cheng, Bin Fu, Tao Chen, Gang Yu, and Shenghua Gao. 2024. Michelangelo: Conditional 3d shape generation based on shape-image-text aligned latent representation. In NeurIPS. Ziming Zhong, Yanyu Xu, Jing Li, Jiale Xu, Zhengxin Li, Chaohui Yu, and Shenghua Gao. 2024. MeshSegmenter: Zero-Shot Mesh Semantic Segmentation via Texture Synthesis. In ECCV. Zi-Xin Zou, Zhipeng Yu, Yuan-Chen Guo, Yangguang Li, Ding Liang, Yan-Pei Cao, and Song-Hai Zhang. 2024. Triplane meets gaussian splatting: Fast and generalizable single-view 3d reconstruction with transformers. In CVPR. Zhangyang Qi, Yunhan Yang, Mengchen Zhang, Long Xing, Xiaoyang Wu, Tong Wu, Dahua Lin, Xihui Liu, Jiaqi Wang, and Hengshuang Zhao. 2024. Tailor3d: Customized 3d assets editing and generation with dual-side images. arXiv preprint arXiv:2407.06191 (2024). Guocheng Qian, Yuchen Li, Houwen Peng, Jinjie Mai, Hasan Hammoud, Mohamed Elhoseiny, and Bernard Ghanem. 2022. Pointnext: Revisiting pointnet++ with improved training and scaling strategies. In NeurIPS. Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, et al. 2021. Learning transferable visual models from natural language supervision. In ICML. Nikhila Ravi, Valentin Gabeur, Yuan-Ting Hu, Ronghang Hu, Chaitanya Ryali, Tengyu Ma, Haitham Khedr, Roman Rädle, Chloe Rolland, Laura Gustafson, et al. 2024. Sam 2: Segment anything in images and videos. arXiv preprint arXiv:2408.00714 (2024). Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Björn Ommer. 2022. High-resolution image synthesis with latent diffusion models. In CVPR. Ruoxi Shi, Hansheng Chen, Zhuoyang Zhang, Minghua Liu, Chao Xu, Xinyue Wei, Linghao Chen, Chong Zeng, and Hao Su. 2023. Zero123++: single image to consistent multi-view diffusion base model. arXiv preprint arXiv:2310.15110 (2023). Jaehyeok Shim, Changwoo Kang, and Kyungdon Joo. 2023. Diffusion-based signed distance fields for 3d shape generation. In CVPR. Yawar Siddiqui, Antonio Alliegro, Alexey Artemov, Tatiana Tommasi, Daniele Sirigatti, Vladislav Rosov, Angela Dai, and Matthias Nießner. 2024. Meshgpt: Generating triangle meshes with decoder-only transformers. In CVPR. George Tang, William Zhao, Logan Ford, David Benhaim, and Paul Zhang. 2024. Segment Any Mesh: Zero-shot Mesh Part Segmentation via Lifting Segment Anything 2 to 3D. arXiv:2408.13679 (2024). Jiaxiang Tang, Zhaoshuo Li, Zekun Hao, Xian Liu, Gang Zeng, Ming-Yu Liu, and Qinsheng Zhang. 2025a. Edgerunner: Auto-regressive auto-encoder for artistic mesh generation. In ICLR. Jiaxiang Tang, Ruijie Lu, Zhaoshuo Li, Zekun Hao, Xuan Li, Fangyin Wei, Shuran Song, Gang Zeng, Ming-Yu Liu, and Tsung-Yi Lin. 2025b. Efficient Part-level 3D Object Generation via Dual Volume Packing. arXiv preprint arXiv:2506.09980 (2025). Anh Thai, Weiyao Wang, Hao Tang, Stefan Stojanov, Matt Feiszli, and James Rehg. 2024. 3x2: 3D Object Part Segmentation by 2D Semantic Correspondences. In ECCV. Yuxuan Wang, Xuanyu Yi, Haohan Weng, Qingshan Xu, Xiaokang Wei, Xianghui Yang, Chunchao Guo, Long Chen, and Hanwang Zhang. 2025. Nautilus: Locality-aware Autoencoder for Scalable Mesh Generation. arXiv preprint arXiv:2501.14317 (2025). Haohan Weng, Zibo Zhao, Biwen Lei, Xianghui Yang, Jian Liu, Zeqiang Lai, Zhuo Chen, Yuhong Liu, Jie Jiang, Chunchao Guo, et al. 2024. Scaling mesh generation via compressive tokenization. arXiv preprint arXiv:2411.07025 (2024). Shuang Wu, Youtian Lin, Feihu Zhang, Yifei Zeng, Jingxi Xu, Philip Torr, Xun Cao, and Yao Yao. 2024. Direct3D: Scalable Image-to-3D Generation via 3D Latent Diffusion Transformer. In NeurIPS. Jianfeng Xiang, Zelong Lv, Sicheng Xu, Yu Deng, Ruicheng Wang, Bowen Zhang, Dong Chen, Xin Tong, and Jiaolong Yang. 2024. Structured 3d latents for scalable and versatile 3d generation. arXiv preprint arXiv:2412.01506 (2024). Jiale Xu, Weihao Cheng, Yiming Gao, Xintao Wang, Shenghua Gao, and Ying Shan. 2024. Instantmesh: Efficient 3d mesh generation from single image with sparse-view large reconstruction models. arXiv preprint arXiv:2404.07191 (2024). Yuheng Xue, Nenglun Chen, Jun Liu, and Wenyun Sun. 2023. ZeroPS: Highquality Cross-modal Knowledge Transfer for Zero-Shot 3D Part Segmentation. arXiv:2311.14262 (2023). Han Yan, Mingrui Zhang, Yang Li, Chao Ma, and Pan Ji. 2024. PhyCAGE: Physically Plausible Compositional 3D Asset Generation from Single Image. arXiv preprint arXiv:2411.18548 (2024). Yunhan Yang, Shuo Chen, Yukun Huang, Xiaoyang Wu, Yuan-Chen Guo, Edmund Lam, Hengshuang Zhao, Tong He, and Xihui Liu. 2025a. DreamComposer++: Empowering Diffusion Models with Multi-View Conditions for 3D Content Generation. IEEE Transactions on Pattern Analysis and Machine Intelligence (2025). Yunhan Yang, Yuan-Chen Guo, Yukun Huang, Zi-Xin Zou, Zhipeng Yu, Yangguang Li, Yan-Pei Cao, and Xihui Liu. 2025b. HoloPart: Generative 3D Part Amodal Segmentation. arXiv preprint arXiv:2504.07943 (2025). Yunhan Yang, Yukun Huang, Yuan-Chen Guo, Liangjun Lu, Xiaoyang Wu, Edmund Lam, Yan-Pei Cao, and Xihui Liu. 2024a. Sampart3d: Segment any part in 3d objects. arXiv preprint arXiv:2411.07184 (2024). Yunhan Yang, Yukun Huang, Xiaoyang Wu, Yuan-Chen Guo, Song-Hai Zhang, Hengshuang Zhao, Tong He, and Xihui Liu. 2024b. DreamComposer: Controllable 3D Object Generation via Multi-View Conditions. In CVPR. Yunhan Yang, Xiaoyang Wu, Tong He, Hengshuang Zhao, and Xihui Liu. 2023. Sam3d: Segment anything in 3d scenes. arXiv preprint arXiv:2306.03908 (2023). Biao Zhang, Jiapeng Tang, Matthias Niessner, and Peter Wonka. 2023. 3dshape2vecset: 3d shape representation for neural fields and generative diffusion models. ACM Transactions On Graphics (TOG) 42, 4 (2023), 116."
        }
    ],
    "affiliations": [
        "Harbin Institute of Technology, China",
        "The University of Hong Kong, China",
        "VAST, China",
        "Zhejiang University, China"
    ]
}