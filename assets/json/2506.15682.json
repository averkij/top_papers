{
    "paper_title": "Evolutionary Caching to Accelerate Your Off-the-Shelf Diffusion Model",
    "authors": [
        "Anirud Aggarwal",
        "Abhinav Shrivastava",
        "Matthew Gwilliam"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Diffusion-based image generation models excel at producing high-quality synthetic content, but suffer from slow and computationally expensive inference. Prior work has attempted to mitigate this by caching and reusing features within diffusion transformers across inference steps. These methods, however, often rely on rigid heuristics that result in limited acceleration or poor generalization across architectures. We propose Evolutionary Caching to Accelerate Diffusion models (ECAD), a genetic algorithm that learns efficient, per-model, caching schedules forming a Pareto frontier, using only a small set of calibration prompts. ECAD requires no modifications to network parameters or reference images. It offers significant inference speedups, enables fine-grained control over the quality-latency trade-off, and adapts seamlessly to different diffusion models. Notably, ECAD's learned schedules can generalize effectively to resolutions and model variants not seen during calibration. We evaluate ECAD on PixArt-alpha, PixArt-Sigma, and FLUX-1.dev using multiple metrics (FID, CLIP, Image Reward) across diverse benchmarks (COCO, MJHQ-30k, PartiPrompts), demonstrating consistent improvements over previous approaches. On PixArt-alpha, ECAD identifies a schedule that outperforms the previous state-of-the-art method by 4.47 COCO FID while increasing inference speedup from 2.35x to 2.58x. Our results establish ECAD as a scalable and generalizable approach for accelerating diffusion inference. Our project website is available at https://aniaggarwal.github.io/ecad and our code is available at https://github.com/aniaggarwal/ecad."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 8 1 ] . [ 1 2 8 6 5 1 . 6 0 5 2 : r Evolutionary Caching to Accelerate Your Off-the-Shelf Diffusion Model Anirud Aggarwal Abhinav Shrivastava Matthew Gwilliam University of Maryland, College Park {anirud, mgwillia}@umd.edu abhinav@cs.umd.edu"
        },
        {
            "title": "Abstract",
            "content": "Diffusion-based image generation models excel at producing high-quality synthetic content, but suffer from slow and computationally expensive inference. Prior work has attempted to mitigate this by caching and reusing features within diffusion transformers across inference steps. These methods, however, often rely on rigid heuristics that result in limited acceleration or poor generalization across architectures. We propose Evolutionary Caching to Accelerate Diffusion models (ECAD), genetic algorithm that learns efficient, per-model, caching schedules forming Pareto frontier, using only small set of calibration prompts. ECAD requires no modifications to network parameters or reference images. It offers significant inference speedups, enables fine-grained control over the quality-latency trade-off, and adapts seamlessly to different diffusion models. Notably, ECADs learned schedules can generalize effectively to resolutions and model variants not seen during calibration. We evaluate ECAD on PixArt-α, PixArt-Σ, and FLUX-1.dev using multiple metrics (FID, CLIP, Image Reward) across diverse benchmarks (COCO, MJHQ-30k, PartiPrompts), demonstrating consistent improvements over previous approaches. On PixArt-α, ECAD identifies schedule that outperforms the previous state-of-the-art method by 4.47 COCO FID while increasing inference speedup from 2.35x to 2.58x. Our results establish ECAD as scalable and generalizable approach for accelerating diffusion inference. Our code is available at https://github.com/aniaggarwal/ecad."
        },
        {
            "title": "Introduction",
            "content": "Diffusion has emerged as the backbone for state-of-the-art image and video synthesis techniques [1 4]. Unlike prior methods involving deep learning, which would train neural network to generate images in single forward inference step, diffusion instead involves iterating over prediction for many (20 to 50) steps [5]. This process is quite expensive, and many researchers and practitioners try to reduce the latency while preserving, or even improving, the quality [610]. Some of these strategies involve training some model that can perform the inference in 1 to 4 steps, particularly with model distillation [9, 11]. Other strategies do not train or tune any neural network weights, principally caching [6, 7, 12], where the diffusion models internal features are re-used across steps, allowing that computation to be skipped. We introduce new conceptual and algorithmic framework for diffusion caching by reframing the problem and replacing existing heuristic-based approaches with principled, optimization-driven methodology that is generalizable across model architectures. Existing caching methods typically offer few discrete schedules, each with fixed trade-offsfor example, 2x speedup with moderate quality loss, and 3x speedup with greater degradationwithout support for intermediate or more aggressive configurations. However, real-world deployments often operate under variable latency or quality constraints, necessitating further flexibility. We instead formulate caching as multi-objective Preprint. Under review. Figure 1: We conceptualize diffusion caching as Pareto optimization problem over image quality and inference time and propose ECAD to discover such Pareto frontiers using genetic algorithm. Left: performance progression over generations for FLUX-1.dev. Right: example 10241024 results with corresponding speedups. optimization problem, aiming to discover smooth Pareto frontier that reveals wide spectrum of speed-quality trade-offs. We show example frontiers we discover for FLUX.1-dev [13] in Figure 1. Such frontiers are very challenging to produce given how caching schedules are currently derived. State-of-the-art approaches [8, 1416] are motivated by heuristics, and key hyperparameters must be carefully hand-tuned by human practitioners based on performance on some set of key metrics. We propose different paradigm that does not rely on human-defined heuristics or hyperparameters, instead discovering effective caching schedules via genetic algorithm. Our Evolutionary Caching to Accelerate Diffusion models (ECAD) requires two components: (i) some small set of text-only calibration prompts and (ii) some metric which computes image quality given prompt and generated image (we use Image Reward [17]). We formulate caching schedules such that the genetic algorithm can automatically discover which features to cache (in terms of blocks and layer types) and when (which timestep). ECAD can be initialized with either random schedules or some set of promising schedules based on prior works [8, 15]. Thus, while ECAD presents different paradigm compared to prior works, it can also build on their valuable findings. ECAD takes these initial schedules and gradually evolves them according to the mating rules of genetic algorithm, optimizing their fitness according to quality and computational complexity (measured in MACs, to be hardware-agnostic). This strategy is extremely flexible. While other methods are entirely designed around whether they cache entire block outputs, intermediate layer outputs (such as the output of an attention layer, or feedforward layer), or even specific tokens, ours is orthogonal to all of these. We offer paradigm which can be used to optimize caching schedules according to any well-defined criteria. We instantiate it with our criteria and schedule definitions in Section 3, but the general principles can be applied to arbitrary criteria and schedules to find Pareto-optimal caching frontiers. For example, we could use other criteria to define fitness, such as human ratings of generated samples. We could also change the caching schedule definitions to be more granular or more general, to focus on certain types of layers, or incorporate heuristics from other methods. Furthermore, while ECAD involves some optimization, since we do not compute any gradients or update any weights, the memory requirements are quite low. Additionally, there are no restrictions on batch size (allowing for use of single, small GPUs that would not be feasible for distillation), and the entire process can happen completely asynchronously. Beyond this, schedules could be optimized for aggressively quantized diffusion models to further improve their acceleration and quality. Figure 1 showcases our methods strong performance and highlights flexibility across resolutions. Although optimized for FLUX-1.dev at 256256, the same schedule applied to 10241024 still outperforms SOTA methods in both speed and quality. At 256256, ECAD matches or surpasses un2 Figure 2: In the context of transformer-based diffusion model, we describe how the transformer architecture allows for caching of attention and feedforward results separately (left). We then give toy illustration of how our method might transition from one generation to the next, prioritizing mating for schedules with the best quality-speed trade-offs (right). accelerated PixArt-α and FLUX-1.dev baselines with 1.97x and 2.58x latency reductions, respectively. At more aggressive 2.58x and 3.37x settings, quality slightly drops but remains competitive."
        },
        {
            "title": "2 Related Work",
            "content": "2.1 Diffusion for Image and Video Synthesis Diffusion models predict noise, given noised image inputs, to generate high-quality images [1, 2, 18] and videos [3, 4, 19]. To save time and reduce feature sizes, these computations are typically performed in the latent space [20] of pre-trained variational autoencoder [21]. Although earlier works use U-Net backbones [22], more recent methods rely mainly on transformer-based models [23 26], especially Diffusion Transformers (DiTs) [25], which dominant the current landscape due to their powerful scaling properties [4, 13, 27, 28]. Text-conditioning with multimodal models like CLIP [29], or extremely powerful text models like T5 [30], allows for more granular control over image content [3135], not only in generative pipelines but especially for editing [3640]. 2.2 Accelerating Diffusion Inference Training. Many works focus on training or finetuning models for faster generation. Some of these rely on model distillation [11], using the initial diffusion model to teach model that uses less steps [9, 10, 4146]. Others train lightweight modules to predict skip connections [47], number of inference steps [48], features [49], or caching configurations [50]. Training-free. Some works speed up diffusion without additional model training by caching and re-using features across steps during inference. Works developed to cache U-Nets [6, 7, 12] do not easily transfer to DiTs [50], considering DiTs operate at single resolution, have no encoder-decoder designation, and have only within-block skip connections. Pioneering caching works for DiTs show promise, but some only cache entire blocks at fixed timestep intervals [8], which sacrifices some image quality, while others cache only attention layers [15], which mitigates potential speed-ups. More recent works, including some concurrent works, use heuristics and carefully-tuned hyperparameters to allow for more dynamic and granular control over caching decisions [14, 16, 5157]. Our method is most similar to these caching works, which do not tune any model parameters. We overhaul the process of selecting an exact caching schedule and hyperparameters by replacing human-in-the-loop heuristic-based caching decisions and tuning with genetic algorithm."
        },
        {
            "title": "3 Methods",
            "content": "For an in-depth preliminary on diffusion for image generation, see Appendix A.1. Here, we give preliminaries for caching with diffusion transformers. Then, we explain our method for conceptualizing caching in terms of Pareto frontiers on speed and quality, and our genetic algorithm which optimizes these frontiers on customizable, per-model basis. 3.1 Preliminary: Caching Diffusion Transformers Diffusion Transformers (DiTs) utilize modified transformer architecture optimized for the diffusion denoising process. typical DiT block takes three inputs: sequence of tokens representing the noisy image, conditioning vector (e.g., text embeddings), and timestep embedding t. to Caching in DiTs exploits temporal coherence between consecutive denoising steps. As the diffusion process proceeds from t1, the inputs to each block change gradually, creating an opportunity to reuse computed features from previous timesteps [6, 8]. We employ component-level caching within DiT blocks rather than caching entire blocks. For each transformer block, we selectively cache the outputs of specific functional components: self-attention (fSA), cross-attention (fCA), and feedforward networks (fFFN). Formally, for component fcomp in block at timestep t, we can decide whether to compute it directly or reuse its previously cached value: comp(z t, t, c) = (cid:26)compute(z cache[f t, c, t) comp, + 1] if recompute if cached When we choose to recompute, the new value is stored in the cache for potential reuse in subsequent steps. Figure 2 demonstrates this for DiT block with two components, self-attention (fSA) and feedforward (fFFN). The DiTs per-component skip connections allow features from the current inference step to be combined with cached features from previous steps. This selective computation strategy can be represented as binary tensor {0, 1}N BC, where is the number of diffusion steps, is the number of transformer blocks, and is the number of cacheable components per block. value of 0 at position (n, b, c) in S, which we show with shades of red in Figure 2, indicates that we reuse the cached value of component in block at diffusion step rather than recomputing it. The caching schedule directly impacts both computational efficiency and generation quality. Aggressive caching (more 0s in S) reduces computation but may degrade output quality. Our method focuses on finding caching schedules that offer an optimal trade-off between computational cost and generation quality by identifying which components can be safely cached for which blocks during which timesteps. 3.2 Genetic Algorithm as Paradigm for Caching Caching, as Pareto Frontiers. The caching optimization problem inherently exhibits trade-off between computational efficiency and generation quality. This can be formalized as multi-objective optimization problem: (C(S), Q(S)) min where C(S) denotes the computational cost function (lower is better) and Q(S) represents the generation quality metric (lower is better, e.g., FID) for caching schedule S. This optimization operates directly on the binary caching tensor {0, 1}N BC introduced previously. Possible configurations for naturally induce sets of solutions that form Pareto frontiers improving one objective necessarily degrades the other. However, this search space is intractable to exhaustively explore, even for small DiTs, given current compute. Prior acceleration methods have predominantly relied on fixed heuristics that typically provide only isolated operating points. By contrast, our proposed approach explores greater search space and discovers Pareto-optimal configurations, enabling practitioners to select schedules based on application-specific constraints. Evolutionary Caching to Accelerate Diffusion models (ECAD). We introduce ECAD, an evolutionary algorithm-based framework for discovering efficient caching schedules for diffusion models, in Algorithm 1. Our approachs key insight is that the optimal caching configuration can be discovered through population-based search over the space of possible caching schedules, using small set of calibration prompts to evaluate candidate solutions. ECAD is framework with 4 simple customizable components. Algorithm 1 Evolutionary Caching to Accelerate Diffusion models (ECAD) Require: Diffusion model , calibration prompts , population size n, generations G, crossover probability pc, mutation probability pm Random and heuristic-based schedules for each schedule Pg1 do MS(P ) Compute quality metric Q(P, I) Compute computational cost C(S) 1: P0 InitializePopulation(n) 2: for = 1 to do 3: 4: 5: 6: 7: 8: 9: 10: 11: end for 12: ComputeParetoFrontier(P1, P2, ..., PG) 13: return end for Pg Selection(Pg1) Pg Crossover(Pg, pc) Pg Mutation(Pg, pm) Generate images using schedule on prompts Image Reward score TMACs NSGA-II with Tournament Selection Recombine schedules with 4-Point Crossover Bit-flip mutation Pareto frontier across all generations The practitioner may adjust granularity with the (1) binary caching tensor shape by adjusting , B, and (the defaults we define for allow any component with skip connection to be cached, on any block, for any timestep). While it does not require any image data, ECAD needs (2) calibration prompts, which we instantiate with the 100 prompts from the Image Reward Benchmark [17]. The practitioner can also select their preferred (3) metrics, where ideally both can be computed quickly online. We use Image Reward for quality, and Multiply-Accumulate Operations (MACs) for speed (to avoid hardware dependencies). Then, we choose an (4) initial population of caching schedules, which should be diverse, and can be seeded based on prior knowledge (such as using FORA schedules) or initialized randomly. We utilize NSGA-II [58] for our genetic algorithm due to its efficient non-dominated sorting approach and proven effectiveness in multi-criteria optimization problems. With all these defined, the practitioner can run ECAD for the desired number of generations. At each generation, images are generated for every binary caching tensor in the population, and the best tensors (in terms of quality and speed) evolve to form the next generation. Each generation yields incrementally more optimal Pareto frontiers for caching the chosen model, with the chosen diffusion scheduler, for the chosen number of timesteps."
        },
        {
            "title": "4 Experiments",
            "content": "4.1 Experimental Settings Model Architectures We provide experiments on three popular text-to-image DiT models: PixArtα, PixArt-Σ, FLUX-1.dev. Each model uses its default sampling method at 20 steps: DPMSolver++ [5] for both PixArt models and FlowMatchEulerDiscreteScheduler [28] for FLUX-1.dev. Guidance scales are 4.5 for PixArt models and 5 for FLUX-1.dev. PixArt-α and PixArt-Σ each employ 28 identical transformer blocks containing three components we enable caching for: self-attention, cross-attention, and feedforward network. FLUX-1.dev, in contrast, implements an MMDiT-based architecture [28] with 19 full blocks and 38 single blocks. We enable caching for attention, feedforward, and feedforward context components in full blocks, and attention, MLP projection, and MLP output for single blocks. Cacheable component selection is discussed in Appendix A.2. We calibrate ECAD for all models at 256256 resolution, but present experiments with evaluations at both 256256 and 10241024 resolution. Evaluation Metrics We evaluate performance using Image Reward [17], FID [59], and CLIP score [60] with ViT-B/32 [24] on the Image Reward Benchmark prompts set [17], the PartiPrompts set [61], MS-COCO2017-30K [62] (we use the same prompts and images as ToCa [14]) and MJHQ30K [63]. On the Image Reward Benchmark prompts set, we generate each of 100 prompts at 10 different, fixed seeds for 1,000 total images. For PartiPrompts we generate single image for each of the 1,632 prompts. To measure the speed of particular caching schedule, we use two metrics: multiply-accumulate operations (MACs) and direct image generation latency. Except where otherwise 5 Figure 3: PartiPrompt Image Reward vs. latency for PixArt-α (left) and FLUX.1-dev (right). Table 1: Main results, 256256, 20-step text-to-image generation. We select schedules from our evolutionary Pareto Frontier and compare them to prior works for variety of datasets and models in terms of Image Reward, CLIP Score, and FID. Despite being optimized only on Image Reward, only on the 100 calibration prompts, our method achieves superior results across other metrics and for unseen prompts. Settings Latency Calibration PartiPrompts MS-COCO2017-30K MJHQ-30K Model Caching Setting TMACs None TGATE = 15, = 1 TGATE = 10, = 5 FORA = 2 FORA = 3 ToCa ToCa DuCa DuCa Ours Ours Ours 5.71 4.86 3.47 2.87 2.02 = 3, = 60% 3.17 = 3, = 90% 2.13 = 3, = 60% 3.20 = 3, = 90% 2.30 2.13 fast 1.46 faster 1.18 fastest 5.71 None FORA = 3 2.02 ToCa = 3, = 60% 3.17 ToCa = 3, = 90% 2.13 1.91 Ours fast PixArt-α PixArt-Σ ms / img (speedup) 165.74 (1.00x) 144.77 (1.14x) 108.52 (1.53x) 100.57 (1.65x) 82.55 (2.01x) 90.71 (1.83x) 70.58 (2.35x) 72.53 (2.29x) 64.08 (2.59x) 84.09 (1.97x) 69.17 (2.40x) 64.24 (2.58x) 167.62 (1.00x) 82.12 (2.04x) 94.28 (1.78x) 73.03 (2.30x) 84.84 (1.98x) Image Image Reward Reward CLIP FID CLIP FID CLIP 0.90 0.78 -0.051 0.83 0.60 0.71 0.60 0.76 0.76 0.96 0.90 0.81 0.85 0.65 0.11 0.07 0.85 0.97 0.87 -0.27 0.91 0.83 0.76 0.68 0.79 0.74 0.99 0.88 0. 1.08 0.81 0.19 0.14 1.02 32.01 31.70 28.90 32.03 31.94 31.46 31.35 31.53 31.42 31.94 31.44 31.53 31.90 31.91 31.03 30.89 31.86 24.84 23.90 29.78 24.80 24.50 22.05 24.01 23.13 24.69 20.58 21.93 19.54 24.63 27.69 54.80 56.48 22.17 31.29 31.12 28.29 31.37 31.35 30.99 30.92 31.03 30.96 31.40 31.10 31. 31.11 31.16 30.34 30.25 31.25 9.75 10.38 17.52 10.33 11.11 12.01 11.80 11.69 12.53 8.02 9.92 8.67 10.53 12.70 35.42 36.53 8.91 32.77 32.33 29.38 32.74 32.63 32.37 32.35 32.48 32.39 32.78 32.34 32.24 32.65 32.28 30.64 30.55 32.52 FLUX.1-dev None FORA = 3 ToCa Ours Ours 198.69 69.80 = 4, = 90% 42.96 63.02 fast 43.60 fastest 31.06 31.10 30.88 31.69 31.67 ToCa is not optimized for PixArt-Σ, so we re-use the hyperparameters from PixArt-α. Suboptimal results do not indicate that ToCa is not suitable for PixArt-Σ; instead, ToCa should be hand-optimized per-model. Refer to Appendix A.6 for detailed explanation of ToCa and DuCa MAC and latency calculations. 2620.09 (1.00x) 1073.70 (2.44x) 1576.97 (1.66x) 1016.59 (2.58x) 778.17 (3.37x) 25.76 23.51 23.78 21.61 26.66 17.77 19.38 21.59 16.14 21. 30.95 31.30 31.26 31.58 31.63 31.88 31.88 31.81 32.24 32.27 1.04 0.93 0.93 1.04 0.89 0.69 0.67 0.63 0.83 0.69 stated, we utilize calflops [64] to measure MACs. We average end-to-end image generation latency using precomputed text embeddings on 1 NVIDIA A6000 GPU after discarding warmup runs; full details in Appendix A.6. 4.2 Main Results We optimize ECAD on three diffusion models: PixArt-α, PixArt-Σ, and FLUX-1.dev. We present results for select schedules in Table 1. For PixArt-α at 256256 resolution with 20 inference steps, we run 550 generations with 72 candidate schedules per generation, where each candidate generates 1,000 images (10 per each of 100 Image Reward Benchmark prompts). For FLUX-1.dev, we reduce the population to 24 schedules due to compute constraints and train for 250 generations under otherwise identical settings. We initialize both using variants inspired by FORA and TGATE, detailed in Appendix A.4. For PixArt-Σ, we transfer 72 schedules from PixArt-αs 200th-generation Pareto frontier and run 50 additional generations, leveraging the models shared DiT architecture. 6 Figure 4: Qualitative results comparing our fast schedule for PixArt-α 256256 with ToCa. ... represent omitted text, see Appendix A.10 for full prompts for the first and fifth columns. Across all models, ECAD achieves strong performance on Image Reward (which correlates well with human preference [17]) and FID. On PixArt-α, our fastest schedule reduces FID by 9.3 over baseline and by 2.51 over ToCas best setting. On PixArt-Σ and FLUX-1.dev, ECAD schedules outperform prior work and baseline by significant margin. On FLUX-1.dev, our fast schedule at 2.58x matches baseline Image Reward and the fastest schedule at 3.37x maintains competitive quality. For prompt-image alignment, measured via CLIP score, ECAD roughly matches prior works, which is expected as caching should not affect prompt-image alignment. We show full Pareto frontiers in Figure 3 on unseen prompts. ECAD discovers schedules that consistently outperform prior works across evaluation metrics while providing fine-grained control over the quality-latency tradeoff. We provide some qualitative results which highlight ECADs good quality despite impressive speedups in Figure 4. We show the composition of the fast ECAD schedules for PixArt-α and FLUX.1-dev in Figure 5, with more schedules in Appendix A.9. Scaling Properties. Unlike existing approaches, practitioners have the flexibility to run ECAD for as many generations as their time and compute constraints allow. While competitive schedules emerge within few iterations, continued optimization yields steady improvements. To illustrate this, we track the slowest schedule throughout the genetic process for PixArt-α and report results in Table 2. After just 50 generations, this schedule outperforms the unaccelerated baseline and all prior methods on Image Reward for unseen PartiPrompts and MJHQ FID. Further generations reduce latency at the eventual, but slight, cost in quality. Figure 6 shows the Pareto frontier for each generation on the calibration prompts; initial generations rapidly improve while later generations show incremental improvements. 4.3 Emergent Generalization Capabilities Model Transfer Results. To demonstrate ECADs advantage over handcrafted heuristics, we transfer pre-optimized schedules between model variants. In Table 3, we select the slowest schedule from the Pareto-frontier across the first 200 generations of PixArt-α ECAD optimization and evaluate it on PixArt-Σ as is, to demonstrate direct transfer results. Then, we perform an additional 50 optimization generations on PixArt-Σ using 72 schedules transferred from the PixArt-α ECAD frontier at 200 generations. Although with direct transfer from PixArt-α, PixArt-Σ has higher latency than PixArt-α at 200 generations, after only 50 generations of optimization, it surpasses PixArt-αs speedup while improving calibration Image Reward and MJHQ FID. By comparison, simply transferring the 250 generation PixArt-α configuration yields only 1.79x speedup instead of 1.98x, and has worse calibration Image Reward and MJHQ FID. This is departure from recent caching innovations; for 7 Figure 5: Figure that shows our fast schedule for PixArt-α (left) and FLUX-1.dev (right). Reds are cached components and grays are recomputed (for PixArt-α, from left to right: self-attention, cross-attention, and feedforward). See Appendix A.9 for more details. Table 2: Genetic scaling results. We show performance changes as we run more iterations (generations) of ECAD, in terms of latency, PartiPrompts Image Reward, and MJHQ-30K FID. We select the schedule with the highest TMACs for each generation. Table 3: Model transfer results. ECAD is first optimized on PixArt-α for 200 generations, and the resulting schedules are used to initialize optimization on PixArt-Σ for an additional 50 generations. Settings for both schedule discovery and evaluation are detailed below. We report TMACs, latency, Image Reward on the calibration and PartiPrompts set, and FID for MJHQ-30K. Transferring ECAD schedules between these two models results in only slight penalties to performance. # Gens ms / img (speedup) Image Reward 1 50 150 300 500 145.09 (1.14x) 92.76 (1.79x) 87.11 (1.90x) 86.62 (1.91x) 76.52 (2.17x) 1.00 0.98 1.00 0.99 0. FID 9.40 7.97 8.11 8.04 8.49 Genetic Settings Evaluation Settings Latency Metrics Model Gens Model Res. TMACs / img (speedup) Calibration PartiPrompts FID PixArt-α PixArt-α PixArt-α PixArt-α PixArt-Σ 200 250 250 50 PixArt-α PixArt-Σ PixArt-α PixArt-Σ PixArt-Σ 256 256 256 256 256 2.59 2. 2.22 2.22 1.91 94.04 (1.76x) 103.47 (1.62x) 86.59 (1.91x) 93.68 (1.79x) 84.84 (1.98x) 0.96 0.84 0.96 0.79 0.85 1.02 1. 0.99 1.06 1.02 8.00 9.27 8.09 9.06 8.91 Table 4: FLUX-1.dev detailed transfer results, 1024 1024 resolution, 20-step text-to-image generation. We reuse our fast schedule trained on FLUX-1.dev at 256x256 resolution, as well as an older, slow schedule. We apply them for 1024 1024 image generation and compare them to prior works for variety of datasets in terms of Image Reward, CLIP Score, and FID. Our results are competitive with prior work despite being evaluated at different resolution than optimization. Model Settings Latency Calibration PartiPrompts MS-COCO2017-30K MJHQ-30K Caching Setting TMACs s/img (speedup) Image Reward Image Reward CLIP FID CLIP FID CLIP 40% steps None None FORA = 3 ToCa Ours Ours 1190.25 476.10 416.88 = 4, = 90% 300.41 slow2561024 644.05 fast2561024 376.62 18.30 (1.00x) 7.61 (2.41x) 7.62 (2.40x) 7.42 (2.47x) 10.59 (1.73x) 6.96 (2.63x) 0.68 0.43 0.27 0.66 0.74 0.71 1.14 0.83 0.69 1.09 1.05 1.05 31.98 31.38 31.20 32.05 31.82 31.88 25.45 25.20 29.45 26.88 22.15 26.69 31.08 30.73 30.52 31.32 31.00 30.91 14.63 21.68 24.65 15.39 15.98 17. 31.99 30.99 30.69 31.93 31.79 31.99 example, ToCas carefully tuned PixArt-α settings cannot be transferred to PixArt-Σ (see Table 1), despite the similarities between the two models. Resolution Transfer Results. We present ECADs performance on FLUX-1.dev at 10241024 resolution after optimization on 256256 in Table 4, and highlight its superior performance compared to FORA and the None approaches. We apply schedules as-is, with no further optimization of 8 Figure 6: ECAD evolution. ECAD iteratively improves quality/time trade-offs as it evolves across generations as measured by Image Reward (PixArt-α 256256). Figure 7: Faster ECAD optimization strategies. We compare Full ECAD to smaller population size, fewer images per prompt, and fewer prompts (PixArt-α 256256). schedules at the higher resolution. While it is likely preferable to optimize ECAD at the target evaluation resolution if sufficient compute is available, we show this is not necessary in practice. In addition to the same fast FLUX-1.dev schedule from Table 1 at 256256 resolution, we select slow model from just 50 generations of training at 256256. We find that even though ToCa was optimized for high resolution and ours for low resolution, our fast setting outperforms it in terms of Calibration Image Reward and COCO FID. 4.4 Ablation Analysis To better explore the evolutionary algorithms behavior, especially with respect to optimization time, we run three ablations with different hyperparameters on PixArt-α for 100 generations. We separately vary the population size (from 72 to 24), the number of images generated per prompt (from 10 to 3), and the number of prompts used (from 100 to 33, selected randomly), each approximately reducing GPU time by 66%. Figure 7 shows that fewer images per prompt is the least harmful, while using less diverse set of 33 prompts is the most harmful. Notably, the reduced prompt setting performs comparably to the full setting at lower TMACs but fails to reach the same maximum quality. This supports the intuition that larger, more diverse calibration sets enable stronger optimization and could yield better results than those presented here, motivating further exploration in future work. Meanwhile, the frontier of the reduced population setting closely resembles that of earlier generations of the full population, as seen in Figure 6. Thus, we hypothesize that reducing the population size is akin to running the model for less generations."
        },
        {
            "title": "5 Discussion",
            "content": "Limitations and Broader Impacts. Optimizing our schedules on automatic metrics ties our performance to the quality of those metrics. We use Image Reward for the sake of cost and time; however, if we replace it with ranking by human users, for example, results could improve. Importantly, ECAD does not introduce new societal risks beyond those inherent to diffusion models. While reduced inference cost may increase potential for misuse, it also promotes broader image-generation accessibility and mitigates some environmental impact of image generation. Conclusion. In this work, we reconceptualize diffusion caching as Pareto optimization problem that enables fine-grained trade-offs between speed and quality. We provide method, ECAD, which converts this problem into search over binary masks, and can discover best-case caching Pareto frontier. With only 100 text prompts, our method runs asynchronously with much lower memory requirements than training or fine-tuning diffusion model. We achieve state-of-the-art results for training-free acceleration of diffusion models in both speed and quality."
        },
        {
            "title": "Acknowledgments and Disclosure of Funding",
            "content": "This work was partially supported by NSF CAREER Award (#2238769) to AS. The authors acknowledge UMDs supercomputing resources made available for conducting this research. The U.S. Government is authorized to reproduce and distribute reprints for Governmental purposes notwithstanding any copyright annotation thereon. The views and conclusions contained herein are those of the authors and should not be interpreted as necessarily representing the official policies or endorsements, either expressed or implied, of NSF or the U.S. Government."
        },
        {
            "title": "References",
            "content": "[1] P. Dhariwal and A. Nichol, Diffusion models beat gans on image synthesis, Advances in neural information processing systems, vol. 34, pp. 87808794, 2021. [2] J. Ho, A. Jain, and P. Abbeel, Denoising diffusion probabilistic models, Advances in Neural Information Processing Systems, vol. 33, pp. 68406851, 2020. [3] J. Ho, T. Salimans, A. Gritsenko, W. Chan, M. Norouzi, and D. J. Fleet, Video diffusion models, Advances in Neural Information Processing Systems, vol. 35, pp. 86338646, 2022. [4] Y. Liu, K. Zhang, Y. Li, Z. Yan, C. Gao, R. Chen, Z. Yuan, Y. Huang, H. Sun, J. Gao, L. He, and L. Sun, Sora: review on background, technology, limitations, and opportunities of large vision models, 2024. [5] C. Lu, Y. Zhou, F. Bao, J. Chen, C. Li, and J. Zhu, Dpm-solver++: Fast solver for guided sampling of diffusion probabilistic models, 2023. [6] X. Ma, G. Fang, and X. Wang, Deepcache: Accelerating diffusion models for free, 2023. [7] F. Wimbauer, B. Wu, E. Schoenfeld, X. Dai, J. Hou, Z. He, A. Sanakoyeu, P. Zhang, S. Tsai, J. Kohler, C. Rupprecht, D. Cremers, P. Vajda, and J. Wang, Cache me if you can: Accelerating diffusion models through block caching, 2024. [8] P. Selvaraju, T. Ding, T. Chen, I. Zharkov, and L. Liang, Fora: Fast-forward caching in diffusion transformer acceleration, 2024. [9] C. Meng, R. Rombach, R. Gao, D. P. Kingma, S. Ermon, J. Ho, and T. Salimans, On distillation of guided diffusion models, 2023. [10] A. Sauer, D. Lorenz, A. Blattmann, and R. Rombach, Adversarial diffusion distillation, 2023. [11] G. Hinton, O. Vinyals, and J. Dean, Distilling the knowledge in neural network, 2015. [12] S. Li, T. Hu, F. S. Khan, L. Li, S. Yang, Y. Wang, M.-M. Cheng, and J. Yang, Faster diffusion: Rethinking the role of unet encoder in diffusion models, 2023. [13] B. F. Labs, Flux. https://github.com/black-forest-labs/flux, 2024. [14] C. Zou, X. Liu, T. Liu, S. Huang, and L. Zhang, Accelerating diffusion transformers with token-wise feature caching, 2025. [15] H. Liu, W. Zhang, J. Xie, F. Faccio, M. Xu, T. Xiang, M. Z. Shou, J.-M. Perez-Rua, and J. Schmidhuber, Faster diffusion via temporal attention decomposition, 2024. [16] C. Zou, E. Zhang, R. Guo, H. Xu, C. He, X. Hu, and L. Zhang, Accelerating diffusion transformers with dual feature caching, 2024. [17] J. Xu, X. Liu, Y. Wu, Y. Tong, Q. Li, M. Ding, J. Tang, and Y. Dong, Imagereward: Learning and evaluating human preferences for text-to-image generation, 2023. [18] A. Q. Nichol and P. Dhariwal, Improved denoising diffusion probabilistic models, in International Conference on Machine Learning, pp. 81628171, PMLR, 2021. [19] A. Blattmann, R. Rombach, H. Ling, T. Dockhorn, S. W. Kim, S. Fidler, and K. Kreis, Align your latents: High-resolution video synthesis with latent diffusion models, in Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pp. 2256322575, June 2023. [20] R. Rombach, A. Blattmann, D. Lorenz, P. Esser, and B. Ommer, High-resolution image synthesis with latent diffusion models, 2022. [21] D. P. Kingma and M. Welling, Auto-encoding variational bayes, 2014. [22] O. Ronneberger, P. Fischer, and T. Brox, U-net: Convolutional networks for biomedical image segmentation, 2015. [23] A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones, A. N. Gomez, Ł. Kaiser, and I. Polosukhin, Attention is all you need, Advances in neural information processing systems, vol. 30, 2017. [24] A. Dosovitskiy, L. Beyer, A. Kolesnikov, D. Weissenborn, X. Zhai, T. Unterthiner, M. Dehghani, M. Minderer, G. Heigold, S. Gelly, et al., An image is worth 16x16 words: Transformers for image recognition at scale, arXiv preprint arXiv:2010.11929, 2020. [25] W. Peebles and S. Xie, Scalable diffusion models with transformers, 2023. [26] F. Bao, S. Nie, K. Xue, Y. Cao, C. Li, H. Su, and J. Zhu, All are worth words: vit backbone for diffusion models, 2023. [27] J. Chen, J. Yu, C. Ge, L. Yao, E. Xie, Y. Wu, Z. Wang, J. Kwok, P. Luo, H. Lu, et al., Pixart-α: Fast training of diffusion transformer for photorealistic text-to-image synthesis, arXiv preprint arXiv:2310.00426, 2023. [28] P. Esser, S. Kulal, A. Blattmann, R. Entezari, J. Müller, H. Saini, Y. Levi, D. Lorenz, A. Sauer, F. Boesel, D. Podell, T. Dockhorn, Z. English, K. Lacey, A. Goodwin, Y. Marek, and R. Rombach, Scaling rectified flow transformers for high-resolution image synthesis, 2024. [29] A. Radford, J. W. Kim, C. Hallacy, A. Ramesh, G. Goh, S. Agarwal, G. Sastry, A. Askell, P. Mishkin, J. Clark, G. Krueger, and I. Sutskever, Learning transferable visual models from natural language supervision, 2021. [30] C. Raffel, N. Shazeer, A. Roberts, K. Lee, S. Narang, M. Matena, Y. Zhou, W. Li, and P. J. Liu, Exploring the limits of transfer learning with unified text-to-text transformer, 2023. [31] C. Saharia, W. Chan, S. Saxena, L. Li, J. Whang, E. L. Denton, K. Ghasemipour, R. Gontijo Lopes, B. Karagol Ayan, T. Salimans, J. Ho, D. J. Fleet, and M. Norouzi, Photorealistic text-to-image diffusion models with deep language understanding, in Advances in Neural Information Processing Systems (S. Koyejo, S. Mohamed, A. Agarwal, D. Belgrave, K. Cho, and A. Oh, eds.), vol. 35, pp. 3647936494, Curran Associates, Inc., 2022. [32] A. Ramesh, P. Dhariwal, A. Nichol, C. Chu, and M. Chen, Hierarchical text-conditional image generation with clip latents, 2022. [33] A. Nichol, P. Dhariwal, A. Ramesh, P. Shyam, P. Mishkin, B. McGrew, I. Sutskever, and M. Chen, Glide: Towards photorealistic image generation and editing with text-guided diffusion models, 2022. [34] N. Ruiz, Y. Li, V. Jampani, Y. Pritch, M. Rubinstein, and K. Aberman, Dreambooth: Fine tuning text-to-image diffusion models for subject-driven generation, in Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pp. 2250022510, June 2023. [35] D. Podell, Z. English, K. Lacey, A. Blattmann, T. Dockhorn, J. Müller, J. Penna, and R. Rombach, Sdxl: Improving latent diffusion models for high-resolution image synthesis, 2023. [36] B. Kawar, S. Zada, O. Lang, O. Tov, H. Chang, T. Dekel, I. Mosseri, and M. Irani, Imagic: Textbased real image editing with diffusion models, in Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pp. 60076017, June 2023. [37] T. Brooks, A. Holynski, and A. A. Efros, Instructpix2pix: Learning to follow image editing instructions, in Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pp. 1839218402, June 2023. [38] W. Sun, R.-C. Tu, J. Liao, and D. Tao, Diffusion model-based video editing: survey, 2024. [39] D. Ceylan, C.-H. P. Huang, and N. J. Mitra, Pix2video: Video editing using image diffusion, in 2023 IEEE/CVF International Conference on Computer Vision (ICCV), pp. 2314923160, 2023. [40] W. Chai, X. Guo, G. Wang, and Y. Lu, Stablevideo: Text-driven consistency-aware diffusion video editing, in Proceedings of the IEEE/CVF International Conference on Computer Vision, pp. 2304023050, 2023. [41] T. Salimans and J. Ho, Progressive distillation for fast sampling of diffusion models, 2022. [42] S. Luo, Y. Tan, L. Huang, J. Li, and H. Zhao, Latent consistency models: Synthesizing high-resolution images with few-step inference, 2023. [43] Y. Lee, K. Park, Y. Cho, Y.-J. Lee, and S. J. Hwang, Koala: Empirical lessons toward memoryefficient and fast diffusion models for text-to-image synthesis, 2024. [44] J. Kohler, A. Pumarola, E. Schönfeld, A. Sanakoyeu, R. Sumbaly, P. Vajda, and A. Thabet, Imagine flash: Accelerating emu diffusion models with backward distillation, 2024. [45] T. Yin, M. Gharbi, R. Zhang, E. Shechtman, F. Durand, W. T. Freeman, and T. Park, One-step diffusion with distribution matching distillation, 2023. [46] Y. Xu, Y. Zhao, Z. Xiao, and T. Hou, Ufogen: You forward once large scale text-to-image generation via diffusion gans, 2023. [47] Z. Jiang, C. Mao, Y. Pan, Z. Han, and J. Zhang, Scedit: Efficient and controllable image diffusion generation via skip connection editing, 2023. [48] H. Zhang, Z. Wu, Z. Xing, J. Shao, and Y.-G. Jiang, Adadiff: Adaptive step selection for fast diffusion, 2023. [49] M. Gwilliam, H. Cai, D. Wu, A. Shrivastava, and Z. Cheng, Accelerate high-quality diffusion models with inner loop feedback, 2025. [50] X. Ma, G. Fang, M. B. Mi, and X. Wang, Learning-to-cache: Accelerating diffusion transformer via layer caching, 2024. [51] P. Chen, M. Shen, P. Ye, J. Cao, C. Tu, C.-S. Bouganis, Y. Zhao, and T. Chen, δ-dit: training-free acceleration method tailored for diffusion transformers, 2024. [52] Z. Yuan, H. Zhang, P. Lu, X. Ning, L. Zhang, T. Zhao, S. Yan, G. Dai, and Y. Wang, Ditfastattn: Attention compression for diffusion transformer models, 2024. [53] F. Liu, S. Zhang, X. Wang, Y. Wei, H. Qiu, Y. Zhao, Y. Zhang, Q. Ye, and F. Wan, Timestep embedding tells: Its time to cache for video diffusion model, 2024. [54] Z. Liu, Y. Yang, C. Zhang, Y. Zhang, L. Qiu, Y. You, and Y. Yang, Region-adaptive sampling for diffusion transformers, 2025. [55] J. Qiu, S. Wang, J. Lu, L. Liu, H. Jiang, X. Zhu, and Y. Hao, Accelerating diffusion transformer via error-optimized cache, 2025. [56] W. Sun, Q. Hou, D. Di, J. Yang, Y. Ma, and J. Cui, Unicp: unified caching and pruning framework for efficient video generation, 2025. [57] J. Liu, C. Zou, Y. Lyu, J. Chen, and L. Zhang, From reusing to forecasting: Accelerating diffusion models with taylorseers, 2025. 12 [58] K. Deb and H. Jain, An evolutionary many-objective optimization algorithm using referencepoint-based nondominated sorting approach, part i: solving problems with box constraints, IEEE transactions on evolutionary computation, vol. 18, no. 4, pp. 577601, 2013. [59] M. Seitzer, pytorch-fid: FID Score for PyTorch. https://github.com/mseitzer/ pytorch-fid, August 2020. Version 0.3.0. [60] S. Zhengwentai, clip-score: CLIP Score for PyTorch. https://github.com/taited/ clip-score, March 2023. Version 0.2.1. [61] J. Yu, Y. Xu, J. Y. Koh, T. Luong, G. Baid, Z. Wang, V. Vasudevan, A. Ku, Y. Yang, B. K. Ayan, B. Hutchinson, W. Han, Z. Parekh, X. Li, H. Zhang, J. Baldridge, and Y. Wu, Scaling autoregressive models for content-rich text-to-image generation, 2022. [62] T.-Y. Lin, M. Maire, S. Belongie, L. Bourdev, R. Girshick, J. Hays, P. Perona, D. Ramanan, C. L. Zitnick, and P. Dollár, Microsoft coco: Common objects in context, 2015. [63] D. Li, A. Kamko, E. Akhgari, A. Sabet, L. Xu, and S. Doshi, Playground v2.5: Three insights towards enhancing aesthetic quality in text-to-image generation, 2024. [64] X. Ye, calflops: flops and params calculate tool for neural networks in pytorch framework, 2023. [65] L. Weng, What are diffusion models?, lilianweng.github.io, Jul 2021. [66] J. Blank and K. Deb, pymoo: Multi-objective optimization in python, IEEE Access, vol. 8, pp. 8949789509, 2020. [67] C. Saharia, W. Chan, S. Saxena, L. Li, J. Whang, E. L. Denton, K. Ghasemipour, R. G. Lopes, B. K. Ayan, T. Salimans, J. Ho, D. J. Fleet, and M. Norouzi, Photorealistic text-to-image diffusion models with deep language understanding, in Advances in Neural Information Processing Systems, vol. 35, 2022."
        },
        {
            "title": "A Appendix",
            "content": "A.1 Diffusion Preliminary Diffusion models have emerged as powerful generative models capable of producing high-quality images. In this section, we provide brief overview of the diffusion process, the denoising objective, and the specific formulation for Diffusion Transformers (DiT). Basic Diffusion Process: The diffusion process follows Markov chain that gradually adds Gaussian noise to data. Given an image x0 sampled from data distribution q(x0), the forward diffusion process sequentially transforms the data into standard Gaussian distribution through timesteps by adding noise according to pre-defined schedule. This forward process can be formulated as: q(xtxt1) = (xt; (cid:112)1 βtxt1, βtI) (1) where {βt (0, 1)}T s=1 αs for convenience. key property arising from this process is that we can sample xt at any arbitrary timestep directly from x0 without having to sample the intermediate states as: t=1 represents the noise schedule [65]. We define αt = 1βt and αt = (cid:81)t xt = αtx0 + 1 αtϵ (2) where ϵ (0, I). This property is particularly useful during training as it allows for efficient parallel sampling across different timesteps. Denoising Objective: The denoising process aims to reverse the forward diffusion by learning to predict the noise added at each step. This is typically accomplished by training neural network ϵθ(xt, t) to estimate the noise component in xt. Its training objective is formulated as: = Et,x0,ϵ[ϵ ϵθ(xt, t)2] where is uniformly sampled from {1, 2, ..., }, x0 from the data distribution, and ϵ from (0, I). During sampling, the noisy image is gradually denoised using various strategies. In the DDPM algorithm [2], the reverse process takes the form: (3) pθ(xt1xt) = (xt1; µθ(xt, t), σ2 I) (4) (cid:16) xt βt where µθ(xt, t) = 1 . While effective, DDPM typically requires hundreds αt to thousands of denoising steps. For more efficient sampling, DPM-Solver++ [5] (used in both PixArt-α and PixArt-Σ) reformulates the diffusion process as an ordinary differential equation of the (simplified) form below: ϵθ(xt, t) 1 αt = βtx log pt(x) (5) (cid:17) 1 2 dx dt DPM-Solver++ then applies high-order numerical methods to solve this ODE more efficiently. This leads to update rules that enable high-quality image generation in as few as 20 steps rather than the hundreds required by DDPM. However, each step still requires forward pass through the noise prediction network, making the sampling process computationally intensive and primary target for acceleration. DiT-specific Processing Diffusion Transformers (DiT) adapt the transformer architecture for diffusion models, offering improved scalability compared to conventional UNet architectures. The processing pipeline for DiTs follows several key steps: first, the input image RHW is encoded into lower-dimensional latent representation using pre-trained variational autoencoder (VAE): = E(x) Rhwd, where h, w, and represent the height, width, and channel dimensions of the latent space, respectively. The latent representation is then divided into non-overlapping patches and linearly projected to form sequence of tokens = Patch(z) RN , where = hw p2 is the number of patches with patch size p, and is the embedding dimension of the transformer. Additionally, timestep embeddings and class or text condition embeddings are incorporated into the model to condition the generation process. Finally, the DiT model processes these tokens through series of transformer blocks, each typically containing self-attention and cross-attention (or joint attention as in FLUX.1-dev), and feedforward network components. 14 Table 5: Computation breakdown of single transformer block forward pass for PixArt-α, PixArt-Σ, and FLUX-1.dev at 256 256 resolution. We report GMACs and each components share of total block computation. Components marked as cache-enabled are those selected for caching in ECAD, as they dominate the computational cost. Components not selected are omitted for efficiency, not due to any fundamental limitation in their cacheability. Model Component Cache-Enabled GMACs % of Block Total PixArt-α and PixArt-Σ Feedforward Self-Attention Cross-Attention Ada Layer Norm Single Total: PixArt Transformer Block Feedforward (Context) Joint Attention (Mutli-stream) Feedforward (Regular) Ada Layer Norm Zero Layer Norm FLUX-1.dev Total: Flux Transformer Block, Full Linear (MLP Input Projection) Linear (MLP Output Projection) Joint Attention (Single-stream) Ada Layer Norm Zero Single GELU Total: Flux Transformer Block, Single A.2 Cacheable Component Selection Yes Yes Yes No Yes Yes Yes No No Yes Yes Yes No No 5.440 2.720 2.000 0.000 10.150 77.310 57.980 38.650 0.226 0.000 174.170 72.480 57.980 43.490 0.057 0. 174.000 53.6 % 26.8 % 19.7 % 0.0 % 100 % 44.39 % 33.29 % 22.19 % 0.14 % 0.00 % 100 % 41.66 % 33.32 % 24.99 % 0.03 % 0.00 % 100 % To enable ECAD on an off-the-shelf model, one must first select which components are cacheable. Any computation whose output can be stored at one step and reused at anotherwhile introducing only minimal, acceptable inaccuracycan be considered for caching. The number of such components determines the value of in the binary caching tensor {0, 1}N BC, introduced in Section 3. Since the search space grows linearly with C, careful selection is essential to ensure efficient and effective caching. Note that the tensor notation is simplified for clarity. In cases where the model uses different types of DiT blocks, each with different number of cacheable components, the caching tensor would instead take the form {0, 1}N ((cid:80)k Table 5 enumerates the computational complexity of each DiT blocks forward pass. We enable caching for the three most computationally expensive components per block, as they collectively dominate the total cost. Computations outside the DiT blocks forward pass (e.g., timestep and position embeddings) are not considered at this time as they contribute less than 1% of the total compute. i=1 BiCi). A.3 Genetic Algorithm Evolutionary Step in Detail Given population Pg of size at generation g, ECAD employs the NSGA-II algorithm [58, 66] to produce the next generation Pg+1 through the following steps: 1. Selection and Offspring Generation: An offspring population Qg, also of size n, is generated from Pg via binary tournament selection by repeating the following process until Qg is filled. Two pairs of candidates are randomly sampled from Pg. Within each pair, tournament is conducted by first comparing candidates by Pareto rank, then breaking ties using crowding distance. The winners from each pair undergo crossover, followed by mutation, to generate offspring. 2. Crossover: With probability of 0.9, we apply 4-point crossover to the binary caching tensors of the parent schedules. Four distinct crossover points are randomly selected along 15 the flattened tensor, and two offspring are created by alternating segments between parents. With probability 0.1, the offspring are direct copies of their respective parents. 3. Mutation: Each candidate in Qg undergoes bit-flip mutation with probability of 0.05. If selected, each bit in the binary tensor {0, 1}N BC is independently flipped with probability 1 BC . 4. Non-Dominated Sorting: The union Pg Qg (size 2n) is sorted into Pareto fronts F0, F1, . . . , Fd based on dominance. For each candidate c, we compute Domc(R), the number of candidates that dominate in some set of candidates R. Fronts are defined iteratively as: F0 := {c Pg Qg Domc(Pg Qg) = 0} F1 := {c (Pg Qg) F0 Domc((Pg Qg) F0) = 0} ... Fi := {c (Pg Qg) i1 (cid:91) j= Fj Domc((Pg Qg) i1 (cid:91) j=0 Fj) = 0} Note candidates in front Fi are said to be of Pareto rank i; lower rank candidates are fitter solutions. Each front Fi contains candidates not dominated by any candidate in fronts of higher rank. 5. Population Selection: The next generation Pg+1 is filled by sequentially adding complete If front Fk cannot be fully fronts F0, F1, . . . until the population size is reached. accommodated, it is sorted by crowding distance. The most diverse candidatesthose with the fewest close neighborsare selected to fill the remaining slots, always including the extrema to preserve frontier diversity. Table 6: Parameters used for latency evaluation. is the number of warm-up batches discarded, is the number of batches used to compute the average latency, and is the largest batch size that fits in memory on single NVIDIA A6000 GPU. All values are empirically chosen to ensure stable and consistent measurements. Model Name Resolution Warm-up (W ) Measured (N ) Batch Size (B) PixArt-α PixArt-Σ FLUX-1.dev FLUX-1.dev 256 256 256 256 256 256 1024 1024 1 1 1 5 5 5 10 100 100 18 3 A.4 Population Initialization We initialize the first generation of schedules for PixArt-α using diverse set of heuristic strategies informed by prior work. Each heuristic varies caching behavior based on step/block selection patterns: Cross-Attention Only: Cache cross-attention at evenly spaced steps. At each selected step, cache the cross-attention of DiT blocks, evenly spaced across the total 28 blocks. Self-Attention Only: Identical to the above, but cache only self-attention. Feedforward Only: Identical to the above, but cache only feedforward layers. Cross- & Self-Attention, All Blocks: Cache both crossand self-attention for all blocks at every nth step. FORA-inspired: Following [8], cache cross-attention, self-attention, and feedforward layers for all blocks at every nth step. TGATE-inspired: Following the gating mechanism from [15], set gate step and interval k. After the first two warm-up steps, compute self-attention every steps, caching and reusing otherwise. After step m, self-attention is computed every step, while cross-attention is not recomputed and reuses the cached output from step m. Unlike TGATE, which averages the cross attention activation on text and null-text embeddings, we cache only the the result from the text embedding. 16 The resulting Pareto frontiers for these heuristics are shown in Figure 8. From the complete set of generated schedules, we randomly select 72 to initialize ECADs first generation for PixArt-α. For PixArt-Σ, as summarized in Section 4.2, we initialize with 72 schedules randomly sampled from the Pareto frontier of PixArt-α after 200 generations of ECAD optimization. For FLUX-1.dev, we start with FORA-inspired schedule, apply few rounds of mutation and crossover, and randomly select 24 candidates to initialize ECAD. Figure 8: Pareto frontiers of Image Reward vs. computational cost for the handcrafted schedules described in Section A.4, evaluated on the Image Reward Benchmark. Notably, caching single component (e.g., cross-attention or feedforward) offers slight gains over baseline. Among all heuristics, FORA achieves the best trade-off, with slightly lower quality but superior efficiency. A.5 Comparison to Concurrent Works While our method is rigorously compared against prior works, it is more challenging to compare against concurrent works. Nonetheless, for completeness and given its strong reported results, we include comparison with DuCa [16], concurrent follow-up to ToCa [14]. At slightly higher acceleration, our faster configuration outperforms DuCa in both human preference (as measured by Image Reward) and FID across MJHQ and COCO datasets. At matching speedups of 2.6x, our fastest configuration modestly improves on DuCas Image Reward and CLIP scores, while achieving notably lower FID with reductions of approximately 30% and 20% on MJHQ and COCO respectively. A.6 MAC and Latency Computations Latency Setup: Latency measurements are conducted on single NVIDIA A6000 GPU for all models. For each model, we discard the first warm-up batches and compute the mean latency over the subsequent measured batches, using prompts from the Image Reward Benchmark. The reported per-image latency is obtained by dividing the average batch latency by the batch size B, 17 except in the case of ToCa (see section below). Detailed configuration parameters are provided in Table 6. ToCa Latency Results: The publicly available ToCa (and DuCa) implementation differs substantially from the infrastructure employed in our framework. While both methods use the same GPU (NVIDIA A6000) and identical warm-up and batch settings, ToCa consistently produces higher latency measurements. To enable fair comparison, we normalize ToCas reported latencies by computing the relative speedup of each ToCa setting over its own baseline, then applying this speedup to our unaccelerated baseline latency: Normalized LatencyToCa ="
        },
        {
            "title": "Latencycached\nToCa\nLatencyunaccelerated\nToCa",
            "content": "Latencyunaccelerated Ours This procedure ensures that the reported values reflect performance improvements relative to each methods own baseline, enabling direct comparison across implementations. See Table 7 for details. ToCa MAC Results: Multiply-accumulate operation (MAC) counts for ToCa are derived using the analytical formulations provided in the original work [14], specifically Section A.4. The relevant expressions are: MACsSA 4N1D2 + 2N 2 1 + 5 2 2 1 MACsCA 2D2(N1 + N2) + 2N1N2D + MACsF 8N1D2 + 12N1DF 5 2 N1N2H Here, N1 and N2 denote the number of image and text tokens respectively, is the hidden state dimensionality, DF refers to the dimensionality within the feedforward network, and is the number of attention heads. Results from DuCa [16], concurrent method that builds upon ToCa, confirm that these approximations closely match empirical MAC counts. Table 7: Latency normalization details for ToCa and DuCa across different models and resolutions. True ms / img refers to direct latency measured from the official implementation. Speedup is computed relative to each methods own unaccelerated baseline, and Normalized ms / img applies that speedup to our unaccelerated latency for fair comparison. Model Resolution Implementation Caching Setting True ms / img Speedup Normalized ms / img PixArt-α 256 PixArt-Σ 256256 FLUX-1.dev 256256 10241024 Ours ToCa ToCa ToCa DuCa DuCa DuCa Ours ToCa ToCa ToCa Ours ToCa ToCa ToCa Ours ToCa ToCa None None ToCa ToCa None DuCa DuCa None None ToCa ToCa None None ToCa ToCa None None ToCa = 3, = 60% = 3, = 90% = 3, = 60% = 3, = 90% = 3, = 60% = 3, = 90% = 4, = 90% = 5, = 90% = 4, = 90% 165.736 948.688 519.258 403.989 981.263 429.405 379.411 167.624 925.024 520.286 403.038 2620.095 3385.153 2037.433 1935. 18297.603 34109.719 13832.082 1.000x 1.827x 2.348x 1.000x 2.285x 2.586x 1.000x 1.778x 2.295x 1.000x 1.661x 1.747x 1.000x 2.466x 165.736 90.715 70.577 165.736 72.527 64.083 167.624 94.281 73.035 2620.095 1576.965 1499.949 18297.603 7419.995 A.7 Additional ECAD Optimization Plots Figure 9 illustrates the progression of ECAD optimization for PixArt-Σ and FLUX-1.dev at 256256 resolution. PixArt-Σ converges rapidly, likely due to its initialization from pre-optimized schedules learned on PixArt-α. FLUX-1.dev converges to steeper Pareto frontier, with its resulting schedules 18 substantially outperforming the unaccelerated baseline on the Image Reward benchmark. We hypothesize that this steep convergence is facilitated by an initial population with relatively high mean acceleration. See Section A.4 for additional details on population initialization. Additionally, we include the Pareto frontier of PixArt-Σ as measured by Image Reward on the unseen PartiPrompts set vs. image generation latency in Figure 10. Our method achieves Pareto dominance over FORA but does reach the unaccelerated baselines level of performance. Figure 9: Progress of ECAD optimization as measured by Image Reward and TMACs. Left: PixArtΣ optimized for 50 generations, initialized using 200 generations of PixArt-α optimization. Right: FLUX-1.dev optimized for 250 generations, initialized using basic heuristics. Figure 10: PartiPrompt Image Reward vs. latency for PixArt-Σ. Note that ToCa is not optimized for PixArt-Σ and its parameters are transferred from PixArt-α. Our method achieves Pareto dominance with significant margin, but does not reach baseline performance. A.8 Ablations We provide evolutionary progress plots for ablations that modify genetic hyperparameters (population size, images per prompt, and number of prompts), complementing the Pareto frontiers shown in Figure 7. Each ablation is evaluated independently and visualized in the following figures. 19 Table 8: Calibration prompt set ablation. Comparison of ECAD performance when calibrated on the Image Reward set vs. DrawBench200. Metrics include Image Reward (IR) on both th calibration and unseen prompts, MJHQ-30K FID, CLIP score, and latency. Each result reflects the highestTMACs schedule from the Pareto frontier after 100 generations. Calibration Prompt Set # of calibration prompts # imgs per prompt ms / img (speedup) Image Reward Benchmark IR DrawBench200 IR PartiPrompts IR FID CLIP Image Reward Benchmark DrawBench200 100 10 5 100.68 (1.65x) 99.53 (1.67x) 0.94 0.87 0.77 0.79 1.00 1.00 8.18 8. 32.88 32.93 Figure 11 illustrates the impact of reducing the population size. This setting results in slightly noisier frontiers and slight performance degradation across all metrics: the MJHQ-30K FID worsens slightly and latency increases by 22 ms over the baselinethe largest increase among all ablations. Figure 12 examines the effect of reducing the number of images per prompt from 10 to 3, while keeping 100 prompts and population of 72. This configuration achieves the fastest latency at 100.30 ms, the highest calibration Image Reward of 0.96, and the smallest increase in MJHQ-30K FID. In Figure 13, we reduce the number of prompts from 100 to 33 while maintaining 10 images per prompt. This setup exhibits the cleanest convergence behavior but significantly underperforms on calibration Image Reward and its final Pareto frontier is dominated by other settings. However, its PartiPrompts score remains competitive and it produces the best FID, suggesting the subset of prompts were challenging and remained suitable for generalization. Detailed results for the highest-TMACs schedule after 100 generations under each hyperparameter setting are shown in Table 9. We also conduct an ablation comparing two calibration sets under fixed hyperparameters. Both runs use population size of 72. The baseline configuration employs the Image Reward calibration set with 100 prompts and 10 images per prompt. The alternative uses the DrawBench200 set [67], which includes 200 prompts and 5 images per prompt, preserving the same total image count. Table 8 reports performance across key metrics, and Figure 15 displays their Pareto frontiers after 100 generations. As expected, each ECAD schedule performs best on the evaluation set corresponding to its calibration set. Notably, while most metrics are similar, the schedule calibrated on Image Reward significantly outperforms the one calibrated on DrawBench200 in MJHQ-30K FID. Moreover, the Image Rewardcalibrated schedule demonstrates superior generalization to DrawBench200 compared to the reverse scenario, indicating that Image Reward provides more robust calibration foundation for cross-dataset deployment. Figure 11: ECAD optimization progress and final Pareto frontier using reduced population size of 24 (compared to the default of 72), with 100 prompts and 10 images per prompt. The resulting frontiers are noisier and exhibit slower convergence. 20 A.9 Visualizing ECAD Schedules To better understand how ECAD optimizes caching schedules under different constraints and settings, we visualize selected schedules using heatmaps. Each heatmap represents schedule, where red shades indicate cached components and gray shades indicate recomputed components. For PixArt models, the component order left-to-right is self-attention, cross-attention, and feedforward. FLUX1.dev uses two types of DiT blocks. Block numbers 0 to 18 are full FLUX DiT blocks, whose components are multi-stream joint-attention, feedforward, and feedforward context. Blocks 19 to 56 are single blocks with components single-stream joint-attention, linear MLP input projection, and linear MLP output projection. Figure 16 and Figure 17 show representative schedules for PixArt-α and PixArt-Σ used throughout the paper. Figure 18 compares FLUX-1.devs slow and fastest schedules. Furthermore, Figure 19 visualizes how ECAD schedules evolve over time for PixArt-α, comparing the highest-TMACs candidate at generations 50, 200, and 400. Finally, Figure 20 presents the highest-TMACs schedules resulting from our genetic hyperparameter ablations, illustrating how variations in population size impact the structure of learned caching strategies. Figure 12: ECAD optimization progress and final Pareto frontier using only 3 images per prompt (default is 10), with 100 prompts and population size of 72. This configuration demonstrates stable convergence and achieves stronger overall performance. A.10 Further Qualitative Results In addition to the PixArt-α 256256 results shown in Figure 4, we present further qualitative comparisons using FLUX-1.dev at 256256 (Figure 21) and 10241024 (Figure 22). Notably, in prompts such as want to supplement vitamin c, please help me paint related food, our method exhibits stronger prompt adherence than both the uncached baseline and ToCa. This behavior is likely influenced by ECADs optimization for the Image Reward metric, which emphasizes semantic alignment with the prompt. Full Prompts from Figure 4, from left to right: Three-quarters front view of blue 1977 Porsche 911 coming around curve in mountain road and looking over green valley on cloudy day. portrait of an old man section of the Great Wall in the mountains. detailed charcoal sketch. still life painting of pair of shoes blue cow is standing next to tree with red leaves and yellow fruit. the cow is standing in field with white flowers. impressionistic painting the Parthenon 21 Full Prompts from Figure 21, 22, from top-to-bottom: Bright scene, aerial view, ancient city, fantasy, gorgeous light, mirror reflection, high detail, wide angle lens. 8k uhd man looks up at the starry sky, lonely and ethereal, Minimalism, Chaotic composition Op Art want to supplement vitamin c, please help me paint related food. 3d digital art of an adorable ghost, glowing within, holding heart shaped pumpkin, Halloween, super cute, spooky haunted house background deep forest clearing with mirrored pond reflecting galaxy-filled night sky. Drone view of waves crashing against the rugged cliffs along Big Surs Garay Point beach. The crashing blue waters create white-tipped waves, while the golden light of the setting sun illuminates the rocky shore. person standing on the desert, desert waves, gossip illustration, half red, half blue, abstract image of sand, clear style, trendy illustration, outdoor, top view, clear style, precision art, ultra high definition image Figure 13: ECAD optimization progress and final Pareto frontier using only 33 prompts (a random subset of the default 100), with 10 images per prompt and population size 72. Although convergence is relatively smooth, the final frontier is constrained by the reduced prompt diversity. A.11 Clarifying Frontier Visualizations Several frontier plotssuch as Figures 11, 12, and 13show both the Pareto frontier of individual generations (typically shown in color) and the overall frontier aggregated across all generations (typically in black). At first glance, it may seem that generational frontier occasionally surpasses the overall frontier. This apparent contradiction arises from interpolation between discrete candidate schedules. As illustrated in Figure 14, the frontier from generation appears to extend beyond the overall frontier. However, the aggregated frontier integrates more finely sampled points, including high-performing candidates from earlier generations (e.g., generation G1), which are not always aligned with the interpolated curves of later generations. The overall frontier, therefore, forms tighter envelope of all known Pareto-optimal schedules, even if it may visually appear to be exceeded due to interpolation artifacts. Figure 14: Illustrative example of per-generation and overall Pareto frontiers in ECAD. Points represent candidate schedules, with lines interpolated between them for visualization. Half-colored points lie on both the generational and overall frontiers. In this example, the frontier from generation appears to exceed the overall frontier, highlighting interpolation artifacts that can occur between discrete candidate solutions. Figure 15: ECAD calibration prompt set ablation. We show performance change when using the DrawBench200 prompts benchmark set for calibration instead of the Image Reward set. Performance is measured in Image Reward (IR) on the both calibration prompts, unseen PartiPrompts, and MJHQ30K FID and CLIP. Latency is provided as well. The schedule with the most TMACs that lies on the Pareto frontier across all 100 generations is used in each instance. 23 Table 9: Genetic hyperparameter ablations. Performance of ECAD when varying population size, number of images per prompt, and number of calibration prompts. We report latency, Image Reward on calibration and unseen PartiPrompts, and MJHQ-30K FID. Each result corresponds to the highest-TMACs schedule lying on the Pareto frontier after 100 generations. Population Size # imgs per prompt # of calibration prompts ms / img (speedup) Calibration IR PartiPrompts IR FID 72 24 72 72 10 10 3 10 100 100 100 100.68 (1.65x) 122.96 (1.35x) 100.30 (1.65x) 110.44 (1.50x) 0.94 0.93 0.96 0.85 1.00 1.00 0.99 0.99 8.18 8.92 8.60 7.52 Figure 16: ECAD schedules for PixArt-α from Table 1: faster (left) and fastest (right). Despite being separate schedules with no guarantee of relation, the faster schedule has near identical structure to fast, with more caching along steps 6 and 16. Furthermore, it appears cross-attention matters less than self-attention and the feedforward network during steps 16 and 17 and can safely be cached. Figure 17: ECAD schedule for PixArt-Σ fast from Table 1. Initial DiT blocks in steps 6, 9, and 12 are more important to recompute than the final blocks. Cross-attention has less of an impact than the other components in the final three steps, with it as the only component cached in step 17. 24 Figure 18: ECAD schedules slow (left) and fastest (right) for FLUX-1.dev from Table 4 and Table 1 respectively. Despite being almost 200 generations apart, both schedules share similar structures for the first 5 steps, particularly at step 2 for blocks 9 through 12. 25 Figure 19: Highest-TMACs schedules from generation 50 (left), 200 (center), and 400 (right) during PixArt-α ECAD optimization. While steps between 8 and 15 remain somewhat similar in structure, early and late steps change more. 26 Figure 20: Highest-TMACs schedules after 100 generations for PixArt-α under different hyperparameter ablations: (top-left) reduced population size; (top-right) fewer images per prompt; (bottom-left) fewer prompts; (bottom-right) baseline configuration. All configurations realize the cacheability of cross attention for steps where other components cannot safely be cached. 27 Figure 21: FLUX-1.dev 256256 qualitative comparisons. Displayed left-to-right are generations from the uncached baseline, ToCa (N = 5, = 90%; 1.75x speedup), and our fast ECAD schedule (Table 1; 1.97x speedup). ECAD consistently yields sharper images with improved prompt adherence. 28 Figure 22: FLUX-1.dev 10241024 qualitative comparisons. Outputs, top-to-bottom, are ToCa (N = 4, = 90%; 2.47x speedup), and our fast ECAD schedule (as shown in Table 4; 2.63x speedup). Our method yields greater visual complexity with stronger prompt-alignment, despite higher acceleration."
        }
    ],
    "affiliations": [
        "University of Maryland, College Park"
    ]
}