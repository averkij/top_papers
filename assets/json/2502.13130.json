{
    "paper_title": "Magma: A Foundation Model for Multimodal AI Agents",
    "authors": [
        "Jianwei Yang",
        "Reuben Tan",
        "Qianhui Wu",
        "Ruijie Zheng",
        "Baolin Peng",
        "Yongyuan Liang",
        "Yu Gu",
        "Mu Cai",
        "Seonghyeon Ye",
        "Joel Jang",
        "Yuquan Deng",
        "Lars Liden",
        "Jianfeng Gao"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "We present Magma, a foundation model that serves multimodal AI agentic tasks in both the digital and physical worlds. Magma is a significant extension of vision-language (VL) models in that it not only retains the VL understanding ability (verbal intelligence) of the latter, but is also equipped with the ability to plan and act in the visual-spatial world (spatial-temporal intelligence) and complete agentic tasks ranging from UI navigation to robot manipulation. To endow the agentic capabilities, Magma is pretrained on large amounts of heterogeneous datasets spanning from images, videos to robotics data, where the actionable visual objects (e.g., clickable buttons in GUI) in images are labeled by Set-of-Mark (SoM) for action grounding, and the object movements (e.g., the trace of human hands or robotic arms) in videos are labeled by Trace-of-Mark (ToM) for action planning. Extensive experiments show that SoM and ToM reach great synergy and facilitate the acquisition of spatial-temporal intelligence for our Magma model, which is fundamental to a wide range of tasks as shown in Fig.1. In particular, Magma creates new state-of-the-art results on UI navigation and robotic manipulation tasks, outperforming previous models that are specifically tailored to these tasks. On image and video-related multimodal tasks, Magma also compares favorably to popular large multimodal models that are trained on much larger datasets. We make our model and code public for reproducibility at https://microsoft.github.io/Magma."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 8 1 ] . [ 1 0 3 1 3 1 . 2 0 5 2 : r Magma: Foundation Model for Multimodal AI Agents Jianwei Yang1* Reuben Tan1 Qianhui Wu1 Ruijie Zheng2 Baolin Peng2 Yongyuan Liang2 Yu Gu1 Mu Cai3 Seonghyeon Ye4 Joel Jang5 Yuquan Deng5 Lars Liden1 1Microsoft Research, 2University of Maryland, 3University of Wisconsin-Madison 4KAIST, 5University of Washington https://microsoft.github.io/Magma Jianfeng Gao1 Figure 1. We introduce Magma, the first foundation model that is capable of interpreting and grounding multimodal inputs within its environment. Given described goal, Magma is able to formulate plans and execute actions to achieve it. By effectively transferring knowledge from freely available visual and language data, Magma bridges verbal and spatial intelligence to navigate complex tasks."
        },
        {
            "title": "Abstract",
            "content": "We present Magma, foundation model that serves multimodal AI agentic tasks in both the digital and physical worlds. Magma is significant extension of vision-language (VL) models in that it not only retains the VL understanding ability (verbal intelligence) of the latter, but is also equipped with the ability to plan and act in the visualspatial world (spatial-temporal intelligence) and complete agentic tasks ranging from UI navigation to robot manipulation. To endow the agentic capabilities, Magma is pretrained on large amounts of heterogeneous datasets spanning from images, videos to robotics data, where the actionable visual objects (e.g., clickable buttons in GUI) in images are labeled by Set-of-Mark (SoM) for action grounding, and the object movements (e.g., the trace of human hands or robotic arms) in videos are labeled by Trace-of-Mark (ToM) for action planning. Extensive experiments show that SoM and ToM reach great synergy and facilitate the acquisition of spatial-temporal intelligence for our Magma model, which is fundamental to wide range of tasks as shown in Fig. 1. In particular, Magma creates new state-of-the-art results on UI navigation and robotic manipulation tasks, outperforming previous models that are specifically tailored to these tasks. On image and video-related multimodal tasks, Magma also compares favorably to popular large multimodal models that are trained on much larger datasets. We make our model and code public for reproducibility1. First Authors; Second Authors; Project Lead; Leadership 1https://microsoft.github.io/Magma 1 1. Introduction long-standing research topic of AI is to develop autonomous agents that can perceive visual stimuli, language inputs, and other environmentally-grounded data and produce meaningful embodied actions in physical and digital environments to complete specific tasks. Recently, there has been growing interest in developing AI agents based on Vision-Language-Action (VLA) models [5, 6, 19, 29, 42, 54]. These models are typically pretrained on large amounts of vision-language datasets and then action trajectories to attain ability to take actions given VL inputs. However, due to the inherent difference between various environments (e.g., 2D digital world and 3D physical ones), VLA models are typically trained separately for simplicity and then used for different tasks. Exemplary models in the digital world include Pix2ACT [108], WebGUM [34], and Ferret-UI [131] for UI navitation. VLA models in the 3D physical world include RT-2 [5] and OpenVLA [54] for robotics manipulation. Although claimed as generalist, most of these models prioritize learning taskspecific action policy at the cost of significant decline in generic multimodal understanding capabilities, rendering limited genralizability across tasks and domains. In this research, we strive to develop foundation model for multimodal AI agents and argue that it requires simultaneously possessing the following capabilities: Multimodal Understanding to understand multimodal input from various domains (both digital and physical) not only semantically, but also spatially and temporally. Multimodal Action Prediction to break down the longhorizon task into an accurate action sequence, which can be effectively executed by AI agent systems. Such an agent system should be driven by external goals specified by human commands as shown in Fig. 2. To endow the broad capabilities, we effectively leverage large amounts of heterogeneous vision-language and action datasets, including UI datasets such as SeekClick [19], robotic manipulation dataset OXE [23], human instructional videos like Ego-4d [40] and image-text pairs used in LMMs [13, 71]. Instead of sequentially training on one domain and adapting to another, we train single foundation model which can be applied in zero-shot manner to different downstream tasks in various settings. Simply combining those datasets, however, does not bring benefits to the foundation model, due to the significant gap between multimodal understanding which is mostly verbal (i.e., textual descriptions for images and videos) and the action-taking tasks which are mostly spatial (i.e., 2D coordinates for UI or 7-DoF for robot arm). To bridge the gap, we propose two surrogate tasks for model training, action grounding and action planning, by asking the model to predict the proximal action outputs given the visual-spatial observations, represented as images or video frames. SpecifiFigure 2. multimodal AI agent should be capable of mutimodal understanding and action-prediction towards given goal. cally, in each image, we label the visual objects that are actionable by Set-of-Mark (SoM) (e.g., clickable buttons in Fig. 1 bottom-middle) and labeled in each video the object movements, which are the results of actions, with Traceof-Mark (ToM) (e.g., the trace of human hand or robotic arm in Fig. 1 top-middle). In this way, the image and video datasets, which are not labeled with actions, are transformed into vision-language-action data to morph the gap among different types of tasks. We show through extensive empirical studies that SoM and ToM achieve are environmentagnostic and easy to generalize to new agentic tasks, offering an effective and efficient approach to scaling up our Magma model pretraining using large amounts of unlabeled videos, such as raw instructional videos. To the best of our knowledge, Magma is the first foundation model for multimodal AI agents that can understand multimodal inputs (see Fig. 1 left), perform action grounding and planning for the future (see Fig. 1 middle), and finally adapt to downstream (unseen) agentic tasks in both the digital and physical environments(see Fig. 1 right). We evaluated Magma on three task categories: UI navigation (e.g., Mind2Web, AITW), where it has to reason and act in evolving digital environments; vision-language understanding (e.g., GQA, VideoMME), where it grounds language in visual objects and events; and finally robotic manipulation (e.g., Bridge, LIBERO), which tests its 3D spatial intelligence for physical interaction. Magma achieves new SOTA results on UI navigation and robotic manipulation tasks, outperforming even domain-specific models while maintaining strong performance on VL tasks which are comparable to SOTA LMMs. In summary, the main contributions of this work are: We propose Magma, the first foundation model that acquires not only multimodal understanding but also spatial-temporal reasoning abilities for agentic tasks in both digial and physical environments. We propose the use of Set-of-Mark and Trace-of-Mark techniques to significantly enhance the spatial-temporal intelligence for action grounding and planning, and allow Magma to be pretrained effectively on large amounts of heterogeneous datasets. We curate large-scale pretraining dataset, which consists of not only open-source VL datasets, but also UI, robotics data and human instructional videos, autolabeled using SoM and ToM. In total, our training corpus 2 contains approximately 39 million diverse samples. We extensively evaluate the pretrained Magma model to demonstrate the superior model performance across wide range of tasks. Magma with single suite of parameters achieves new SOTA on both robotic manipulation and UI navigation over open-sourced counterparts. We show that the proposed Magma pretraining method significantly improves models verbal and spatialtemporal intelligence abilities. For instance, Magma can achieve SOTA performance on the BLINK dataset without instruction fine-tuning, and SOTA performance on video question-answering benchmarks despite being pretrained on much fewer frames. 2. Related Work Large Multimodal Models (LMMs). Large Language Models (LLMs) like ChatGPT [97], GPT-4 [98], and Llama [115] have demonstrated impressive reasoning and generalization capabilities for text. The introduction of models that integrate visual data has brought about significant shift in the landscape of LLMs, such as GPT4V(ision)[96]. Building upon open-source LLMs [21, 115], wide range of multimodal models have achieved remarkable progress, led by pioneering models such as LLaVA [71, 73] and MiniGPT-4 [145], which combine LLMs capabilities with CLIP [101] based image encoder. Recently, growing number of LMMs have been developed to handle wider range of tasks and modalities, such as region-level LMMs [7, 12, 100, 134, 137], and video LMMs [69, 112, 135, 139]. In parallel, more sophisticated benchmarks are proposed to assess these capabilities [8, 32, 33]. UI Agent in Digital World. Recently there has been lot of work on designing autonomous GUI agents to perform tasks in place of human users. One line of work is to train an end-to-end model to directly predict the next action, representative works include Pixel2Act [107] and WebGUM[35] in web domain, Ferret [132], CogAgent [43], and Fuyu [4] in Mobile domain. Another line of work involves leveraging existing multimodal models such as GPT-4V to perform user tasks. Representative works include MindAct [27], SeeAct [141] in web domain and others [103, 117, 122] for mobile domain. These works often leverage the DOM information in web browsers, or the view hierarchies in mobile apps to get the ground truth position of interactable elements of the screen, and use Set-of-Mark [125] or more advanced localization model [83] to overlay the bounding boxes on top of the screenshot that feed into the visionlanguage models. Vision-Language-Action for Robotics. Several studies have investigated the application of LMMs in robotics [6, 54, 66, 95, 130, 142, 146]. Among these, RT-2 [6] finetuned LMMs on robotic trajectory data, enabling the output of discretized robot action tokens. OpenVLA [54] is the first open-source VLA foundation that is fine-tuned an open-source Prismatic VLM backbone [49]. LLARVA [95] generated 2D visual traces for robot arms along with textual representations of actions, using visual trace prediction as an auxiliary task, while TraceVLA [142] used visual trace prompting to improve spatial-temporal awareness of robot policy. Most recently, learning from videos by predicting the latent VQVAE tokens is explored in [11, 129]. In this work, we follow similar approach as OpenVLA to represent the action but leverage rich multimodal data far beyond robotics datasets. Also, instead of asking model to predict latent tokens, we propose SoM and ToM techniques to significantly enhance the spatial-temporal intelligence, demonstrating significantly stronger performance and generalization capability for agentic tasks. 3. Multimodal Agentic Modeling 3.1. Problem Definition generalist multimodal AI agent should be performant for both multimodal understanding and action-taking. We define multimodal AI agent π, which takes past visual observations = {I1, ..., Ik} and task description task in text as input and outputs set of 1 tokens as: = π(I, task, ctx) = {ol 1, , ol } (1) where ctx denotes the context, {verbal, spatial} indicates if the i-th token oi is verbal or spatial token. This formula generalizes across different tasks: UI navigation in 2D screenshots. The task could be book hotel and output should include both language tokens denoting the semantic type of action (e.g., type, click, etc.) and the location (x, y) or box (x, y, w, h) to which actions are applied. Robotic manipulation in the 3D world. For task like close the drawer, the output consists of 6-DoF displacements (x, y, z, yaw, pitch, roll) of the end effector and, in some cases, one additional dimension to indicate whether the gripper is open or not. Multimodal understanding tasks. When the task is purely about I, e.g., VQA task, the problem is reduced to multimodal understanding task that generates textual description and/or location of objects for input images/videos. For these seemingly different output modalities, we follow common practice to transform all output into textual tokens to facilitate model learning. Specifically, we convert 2D actions into textual dictionary as in [19], and represent robot actions with the last 256 discrete language tokens that is barely used in LLMs, following [54]. Despite such unification into language space, we notice considerable conflicts among tasks, as we will show in our experiments. In what 3 follows, we will discuss how to mitigate such challenges to train agentic foundation on wide range of datasets. 3.2. Method We approach two key challenges while building highly capable foundation for the multimodal AI agent. Pretraining objectives: How to build unified pretraining interface to facilitate joint training? straightforward way would be to predict the 2D coordinates for the navigation of the UI, 3D positions for the end effectors, and regular textual outputs for VL tasks. However, in our experiments, we observed that these tasks have inherent domain gaps in both input and output. The former results in huge search space at the pixel level, and the latter directly predicts the output of proprioceptive action, which is not grounded on the observations of the image. Can we come up with surrogate task that can bridge the gap among all tasks? Data scaling-up: Existing vision-language-action data have limited amount and diversity, unlike language or image-text corpus for LLMs and LMMs, respectively. For example, the largest open source robotic dataset OXE [23] consists of around 1M trajectories taken from 22 environments. On the other hand, large-scale image-text datasets like LAION [104] barely contain useful supervisions for action pretraining as they are all static without the notion of action. Videos, however, depict numerous human actions and human-object interactions. Can we largely take advantage of these video data for our agentic pretraining? In this work, we propose simple yet effective method Inspired by to address the aforementioned challenges. the generality of Set-of-Mark (SoM) rompting [126], we employ it to enable the action grounding onto images for both UI and robotic tasks in that model faces much less difficulties to predict the numeric marks for both clickable buttons or robot arms in the image space. We further extend it along the temporal axis and ask the model to predict Trace-of-Mark (ToM), which forces the model to learn longer horizon by predicting distant future actions, and more importantly provides an effective way to leverage unlabeled video data. The combination of SoM and ToM enables seamless synergy across agentic tasks in digital and physical domains, as well as scalable way to curate action supervisions from raw videos. We describe them in detail below in Sec. 3.2.1 and 3.2.2, respectively. 3.2.1. Set-of-Mark for Action Grounding SoM prompting [126] was first proposed to enhance the grounding capability of GPT-4V and has then been widely adopted for various agentic tasks [18, 44, 70, 94, 123]. Unlike previous works that exploited it for prompting off-theshelf LMMs to enhance visual-language grounding, here we propose to train an agentic model for action grounding, i.e., locating actionable points / regions for specific task and further predict atomic actions if needed. Figure 3. Set-of-Mark supervisions for action grounding on UI screenshot (left), robot manipulation (middle) and human video (right). All coordinates are normalized by image size (height, width) and then quantized into 256 bins. Images better viewed by zooming in. Given an image observation It RHW 3 at timestep t, task task and context ctx, we first extract set of candidate regions or points that are actionable = {p1, ..., pK}, where pk could be four-dimensional box coordinate or two-dimensional point coordinates. Subsequently, we overlay the marks and boxes (if any) to the corresponding location of the image with numerical labels, i.e., = {1 : p1, 2 : p2, ..., : pK} giving us new marked image Given the prompted image t in an atomic action step, the model needs to select the candidate marks along with the original coordinates, significantly easing the action grounding for the agentic model. In this way, Eq. (1) can be reformulated as: . omark = actiont : markt = π(IM , task, ctx) (2) where omark is subset of marks M. In Fig. 3, we show few instances to demonstrate the SoM-based action grounding in Fig 1. To obtain candidate regions to mark, we can leverage different proposal networks such as image segmentation models [56, 147], object detection models [64, 80], or domain-specific models [83]. Readers refer to Supp. for more details. 3.2.2. Trace-of-Mark for Action Planning Video data contains lot of information about human actions and activities, which can essentially be leveraged to boost the capability of agentic models. However, due to the absence of action labels, previous methods rarely explore this direction, apart from few works focused on world model learning [76, 91]. We extend the strategy of overlaying marks from static images to dynamic videos by proposing Trace-of-Mark (ToM) to allow the agentic model to effectively learn to plan and act from videos. Given the sequence of visual observations from video = {I1, ..., It}, we extend along the time axis to the future 4 Figure 4. Trace-of-Mark supervisions for robot manipulation (left) and human action (right). Same coordinate normalization and quantization is used as SoM. Images show the future traces to predict. frames, If uture = {It+1, ..., It+l}. Given the marks at t-th frame It, we extract the corresponding positions of the overlay marks in the next frames, denoted traces = {Mt+1, ..., Mt+l}. Following the prediction of action type and valid marks as in Eq. (2), we further ask the model to predict the future trajectories for the valid marks: omark = actiont : markt : tracet+1:t+l = π({I1, ..., It1, IM }, task, ctx) (3) where tracet+1:t+l is subset of the trace sequences for valid marks in markt in . Our proposed ToM predicting is simple yet effective way of leveraging video data and brings two unique modeling benefits: (i) It forces the model to understand the temporal dynamics in the video observations and to look ahead of time before taking the next actions; (ii) Unlike predicting next frames as used in [77], predicting traces uses much fewer tokens to capture much longer temporal horizon and action-related object dynamics, while disregarding ambient contents. To extract ToM, we employ point tracking models CoTracker [48], though any performant model can be used. In particular, given sequence of frames {It, It+1, ..., It+l} R(l+1)HW 3, we apply dense tracking for s2 grid points to extract s2 traces of length (l + 1). Given these s2 traces, we drop those traces whose average motion magnitudes between two adjacent timesteps are smaller than certain value ϵ (Please see more details in the supplementary material). The remaining ones are regarded as foreground motions driven by given task. 3.3. Modeling To retain the multimodal understanding capability required for Magma, we adopt the common practice used in current VLMs (e.g., LLaVA [71] and Phi-3-Vision [1]). Given the visual observations I, we use vision encoder to encode each frame into number of tokens and then concatenate all tokens into sequence and feed them to decoder-only LLM along with the language tokens that encode task descriptions. Due to the task diversity, vision encoder that can seamlessly encode images and videos of various resolutions is needed. In light of this, we propose to use convolutional networks ConvNeXt [81] as the vision backbone,"
        },
        {
            "title": "Data Type",
            "content": "Set-of-Mark Trace-of-Mark"
        },
        {
            "title": "UI Screenshots\nRobotics Images\nInstructional Videos",
            "content": "Table 1. SoM and ToM applied to various data types. ToM is not applied to UI data as they are sequence of discrete screenshots. considering that it supports arbitrary image resolutions by default. To handle the high-resolution images (e.g., up to 2000 for UI screenshots), we simply perform global encoding without the bells and whistles used in previous work and find that it can encode the global context as well as combining global and local crops [1, 75]. To that end, we formulate the agentic modeling as an autoregressive decoding procedure: ol, t+1 p(ol t+1{ol 1, ..., ol t}; V(I), task, ctx). (4) 4. Multimodal Agentic Pretraining 4.1. Datasets To develop foundation model with both verbal and spatial intelligence that is capable of handling diverse agentic tasks, we curated comprehensive pretraining dataset from wide range of images, videos, and robotics domains. Robotics manipulation data. For robotics task, we follow OpenVLA [54] and use the robotics dataset of OpenX-Embodiment [22]. UI navigation Data. We exploit two pretraining datasets, SeeClick [19] and Vision2UI [41]. Instructional videos. We compile Epic-Kitchen [24, 25], Ego4d [40], Somethingv2 [37] and other related considering the coarse but rich goal-driven human actions. Multimodal understanding. Lastly, we include ShareGPT4V [14], instruction tuning data in LLaVA1.5 [75], and few other OCR-related datasets [86, 90] to attain image understanding capability. We noticed that many more related datasets could be used for our model pretraining, such as large-scale instruction In tuning data [60, 114], more diverse video data [16]. 5 Algorithm 1 SoM generation for UI images Require: image I, bounding boxes B, image height and width (ih, iw) 1: [] 2: for (idx, b) enumerate(B) do text str(idx + 1) 3: DrawRectangle(I, b) 4: (cy, cx) FindOptimalCorner(b, B, (ih, iw)) 5: Find corner that is far away from all boxes in (mh, mw) GetMarkSize(text, H, ) text box (cy, cx, cy mh, cx mw) DrawRectangle(I, text box) DrawText(I, (cx, cy), text, color = white) Add current drawn box to B + [b] 6: 7: 8: 9: 10: 11: end for 12: Return Algorithm 2 SoM and ToM generation for instructional videos and robotic data Require: image sequence = {It, ...Il}; grid size s; global motion threshold η; foreground threshold ϵ 1: = {Mt, ..., Ml} CoTracker(I, s) 2: if HasGlobalMotion(M, η) then 3: H(M) Apply homography transformation 4: end if 5: Mf , Mb = ClassifyTraces(M, ϵ) Classify traces into foreground and background ones 6: Random(1, min(5, Mf )) 7: Mf , Mb = KMeans(Mf , k), KMeans(Mb, 2k)"
        },
        {
            "title": "Cluster foreground and background traces separately",
            "content": "t , }) Apply SoM on 1st frame 8: It SoM (It, {M 9: Return I, this study, we focus on the demonstration of our pretraining methodology and leave the further scaling up for future. In the next, we elaborate on how we extract the surrogate action supervisions through Set-of-Mark (SoM) and Traceof-Mark (ToM). 4.2. SoM and ToM Generation As shown in Table 1, we apply SoM and ToM for different data types, where SoM is applied to all to learn uinified action grounding. ToM is not fit for the UI data as it consists of sequences of discrete screenshots. 4.2.1. SoM for UI Navigation For UI screenshots in our pretraining data, we mainly rely on the original annotations extracted based on DoM Tree. In addition to the bounding boxes extracted from HTML code [19, 41], we further annotate the mobile screenshots in SeeClick data with bounding boxes derived from Android view hierarchies [111]. Given the extracted candidate bounding boxes for an image, we apply Alg. 1 to assign textual label (line 3) and draw the boxes around the objects. To minimize overlapping box placements, we determine the optimal position for label using previously drawn boxes (line 5) before computing the textbox size and assigning its coordinates (line 7). During the evaluation, we follow the common practice by applying OmniParser [83] for the zeroshot evaluation on ScreenSpot [19], and using the candidate boxes provided by [27] for downstream training and evaluation on Mind2Web. 4.2.2. SoM and ToM for Videos and Robotic Data We use marks and traces as surrogate action supervisions to pretrain our Magma model for action grounding and planning. To extract reliable traces, we use the state-of-the-art point tracking model CoTracker [48] to track the keypoints in each video segment. Unlike object detection and tracking Figure 5. An illustration of Alg. 2 to handle videos with camera motions for SoM/ToM generation. systems used in previous works [68, 95, 102], point tracking provides the finest grained moving trajectories for both end effectors (robot arms or human hands) and objects, and more importantly can be feasibly applied to any videos as it does not require object recognition. Reliability of CoTracker. To determine the generalizability of such traces, we examine the reliability of CoTracker before running the algorithm on all our pretraining data. We note that CoTracker was already well validated on multiple video datasets such as TAP-Vid [28] and PointOdyssey [143] in the original paper. In this work, we proposed comprehensive strategies to handle scene transition and camera motions in videos (Alg. 2), which effectively scale to datasets like Ego4D and other instructional videos (Fig 13). To further validate the reliability of ToM, we quantitatively evaluated the traces on subset of YouCook2-BB [144] with box annotations by humans. We extract the traces from each annotated box and count the number of future traces still falling into the box 1 second 6 Figure 6. Overview of Pretraining Data Sources. diverse collection of datasets including instructional videos (orange), robotics manipulation (green), UI navigation (pink), and multimodal understanding (blue). Note that we count the size of each dataset by the number of image samples. For video and robotics data, we extract the images from the short clips and trajectories, respectively. forward. On 1320 clips, we got precision of 0.89, indicating that the traces reliably capture temporal motions. Segment and CLIP-score filtering As the point tracking system works in short time window, we begin by using the annotations provided, curated or otherwise, to split each video into segments, and then run PySceneDetect [10] to further break each segment into short video clips with consistent shots. However, the detected video clips may not always be relevant to their associated text annotations. Thus, we use the pretrained CLIP [101] visual and text encoders to compute the cosine similarity score between each clip and text pair, and filter out clips with < 0.25 scores. Once we have the fine-grained video clips in hand, we apply Alg. 2 to generate SoM and ToM. Given video clip with frames {I1, I2, ..., Il} R(l)HW 3, we start from the time step and put grid of equally spaced s2 points on It. Then, we use CoTracker to extract s2 future traces of length (l t) each. The output also contains predicted occlusion labels for each trace, which indicate if any points on the trace are obstructed at some time steps. Removal of global motions. Many instructional videos, particularly the ego-centric ones [40], contain significant camera movements. Consequently, the extracted traces may reflect external movements instead of relevant actions to accomplish given task. We mitigate this issue by performing the homography transformation [31]. Specifically, we compute the 33 transformation matrix hi with the future mark positions and current ones: hi = H(Mt, Mt+i) R33 (5) Given hi, we apply the homography transformation to Mt+i to obtain t+i which shares the same coordinate system as Mt. Valid traces of marks to predict in Eq. (3) are then extracted from {Mt, t+l}. It turns out that the proposed method is effective to remove global camera motions t+1, for both ego-centric videos and exo-centric ones, as ilustrated in Fig. 5. After extracting the traces and applying the homography transformation if needed (lines 2-4), we classify them into two categories, foreground and background traces based on the average motion magnitude between two adjacent time steps, where traces with average motion magnitude of at least ϵ (line 5) are counted as foreground. Finally, we select the number of clusters (line 6) and perform K-Means clustering for the foreground and background traces separately (line 7) before randomly selecting one or more points from each cluster as the final traces. In practice, we set s, η and ϵ to be 15, 2 and 2, respectively. 4.3. Pretraining The above data and annotation curation results in comprehensive pretraining suite which covers (i) different digital and physical environments; (ii) both verbal and spatial annotations and (iii) various multimodal understanding and agentic tasks. As seen in Fig. 6 (left), we include close to 2.7M UI navigation screenshots from SeeClick [19] and Vision2UI [41]. We follow OpenVLA [54] to incorporate 970K trajectories in Open-X-Embodiment [23], which consists of 9.4M imagelanguage-action triplets. Another majority of the pretraining data are videos which comprise over 25M samples sourced from around 4M shot-consistent video clips. Finally, we include 1.2M image and text pairs from ShareGPT4V [13], and LLaVa-1.5 [74] and few other OCR-related datasets [86, 90], which we denote by Magma-SFT (820K). By default, we use LLaMA-3-8B [30] as the language backbone and ConvNext-XXlarge [81] as the vision backbone. We show the pretraining architecture in Fig. 7. Our proposed SoM and ToM play as the bridge to connect verbal and action supervisions for all four types of data, and significantly enhance models spatial intelligence as we observe 7 Figure 7. Magma pretraining pipeline. For all training data, texts are tokenized into tokens, while images and videos from different domains are encoded by shared vision encoder. The resulted discrete and continuous tokens are then fed into LLM to generate the outputs in verbal, spatial and action types. Our proposed method reconcile the multimodal understanding and action prediction tasks. during our experiments. For comparisons, we run few variants for the ablation studies in our experiments: Magma-8B (SFT) is the model trained with MagmaSFT (820K) for the instruction tuning following conventional recipe used on LMM training. Magma-8B (UI) and Magma-8B (OXE) are the models pretrained on UI screenshots and OXE robotics data, respectively. Magma-8B (ACT) is pretrained jointly on UI screenshots and robotics data. Magma-8B (Full) is the full model trained with the whole dataset with SoM and ToM annotations. Unless noted otherwise, all pretrainng includes the Magma-SFT (820K). We pretrain our model using our curated data for maximally three epochs with constant learning rate of 1e-5, and evaluate the pretrained model on different tasks under the zero-shot setting as well as finetune its weights on the downstream tasks. The entire model including the parameters of the language model and the vision encoder is tuned. See Appendix for more detailed settings. 5. Experiment 5.1. Evaluating Agentic Capability We examine the effectiveness of Magma as the foundation model for multmodal agents on UI Navigation tasks in the digital world, the robotic manipulation in the physical world, as well as the generic multimodal understanding. 5.1.1. Zero-Shot Evaluation To evaluate the zero-shot transferability of Magma, we employ ScreenSpot [19] and VisualWebBench [79] for evaluating UI action grounding and navigation, and SimplerEnv [65] for robotic manipulation. In addition to these evaluations, we also validate our model on generic [39] and text-rich [110] VQA tasks as well as hallucination benchmark POPE [67]. As shown in Table 2, Magma consistently outperforms all other general-domain LMMs (e.g., LLaVA, Qwen-VL) and domain-specific agentic models such as SeeClick [19] for UI navigation and OpenVLA [54] for robotic manipulation. Notably, the zero-shot performance of Magma on UI is much better than the state-ofthe-art vision-based method that uses GPT-4V and Omniparser [84]. We report the results on two commonly used simulator embodiments in SimplerEnv [65], Bridge and Google Robot including 8 tasks with 172 visual matching and variant aggregation scenarios. Since OpenVLA uses real robot trajectories for pre-training, the model is susceptible to the domain gap for real-to-sim adaptation. In contrast, our Magma model, trained for multimodal understanding and action prediction on wide range of heterogeneous datasets, is significantly more resilient to the gap and achieves significantly better success rates. Fig. 8 shows detailed comparisons between our pretrained Magma model and other representative models. Remarkably, Magma surpasses the second-place OpenVLA by 19.6%, nearly doubling the average success rate. On those challenging tasks such as Put Object in Drawer and Put Carrot on Plate, Magma achieves remarkable success rate while most baselines fail entirely. Notably, Magma tuned on our pretrained model showcases substantially better results than the version trained solely on robotic datasets, highlighting the value of spatial intelligence learned from diverse datasets for physical robotic manipulation tasks. Ablation Studies. We ablate our model pretraining techniques and data mixtures. The results are shown in Table 3. First, we observe from the top three rows that simply combining UI and robotics data does not bring gains, but instead 8 Model Size VQAv2 TextVQA POPE SS-Mobile SS-Desktop SS-Web VWB-Ele-G VWB-Act-G SE-Google Robot SE-Bridge Multimodal Understanding UI Action Grounding and Navigation Robot Manipulation GPT-4V [99] GPT-4V-OmniParser [83] LLaVA-1.5 [71] LLaVA-Next [75] Qwen-VL [3] Qwen-VL-Chat [3] Fuyu [4] SeeClick [19] Octo [113] RT-1-X [23] OpenVLA [54] Magma-8B (Ours) n/a n/a 7.4B 7.4B 9.6B 9.6B 8B 9.6B 93M 35M 8B 8.6B 77.2 n/a 78.5 81.8 78.8 78.2 74.2 - - - - 80.0 78.0 n/a 58.2 64.9 63.8 61.5 n/a - - - - n/a n/a 85.9 86.5 n/a n/a n/a - - - - 22.6/24.5 92.7/49.4 - - 7.5/4.8 - 41.0/1.3 78.0/52.0 - - - 20.2/11.8 64.9/26.3 - - 5.7/5.0 - 33.0/3.6 72.2/30.0 - - - 9.2/8.8 77.3/39.7 - - 3.5/2.4 - 33.9/4.4 55.7/32.5 - - - 66. 87.4 60.4/58.5 75.3/52.9 69.1/52.0 67.5 - 12.1 15.0 14.0 - 19.4 9.9 - - - 96.3 75.7 - 13.6 8.7 10.7 - 15.5 1.9 - - - 71.8 - - - - - - - - 6.0 34.2 31. 52.3 - - - - - - - - 15.9 1.1 14.5 35.4 Table 2. Zero-shot evaluation on agentic intelligence. We report the results for pretrained Magma without any domain-specific finetuning. Magma is the only model that can conduct the full task spectrum. SS denotes the ScreenSpot benchmark proposed in SeeClick [19]; VWB denotes VisualWebBench [79]; SE denotes the SimplerEnv simulator [65]. n/a means not available and - means not supported. For all related evaluations, we use OmniParser to provide the detection results only, without local semantics. Figure 8. SimplerEnv performance comparison on Google Robots and Bridge. Magma(OXE) represents our model trained solely on Open-X-Embodiment (OXE) [22], while Magma is our pretrained model. Results for each task are averaged across visual matching and variant aggregation scenarios. Model SoM+ToM SS-Overal VWB-Ele-G VWB-Act-G SE-Bridge SE-Google Magma-8B (UI) Magma-8B (OXE) Magma-8B (ACT) Magma-8B (Full) Magma-8B (Full) 57.7 - 56.2 57.4 61. 68.5 - 89.1 90.1 96.3 58.3 - 21.4 25.2 71.8 - 22.2 17.5 17.7 35.4 - 35.7 31.5 37.5 52.3 Table 3. Ablation study on the effect of data mixtures and pretraining techniques. w/o SoM+Tom means using original action supervisions (2D coordinates for UI and 7DoF for robots.) hurts the performance for both tasks. This is expected because the two agentic tasks have significantly different image domains as well as action spaces (2D coordinates v.s. 7-DoF). Adding video data to the pretraining slightly improves the performance across board but still can not fill the gap in between, as the additional video narrations can only enhance the verbal intelligence. However, once we apply SoM and ToM to all the pretraining data to put them into the unified interface, our model can learn effectively from the heterogeneous data for both verbal and spatial intelligence. This study highlights the effectiveness of our proposed method and indicates equally importance of verbal and spatial understanding for agentic tasks. 5.1.2. Efficient Finetuning With moderate finetuning, the pretrained Magma model can be easily transferred to various downstream agentic tasks. UI Navigation. Following the prior works [19, 43], we finetune Magma on Mind2Web and AITW, to examine the web and mobile UI navigation capabilities, respectively. For Mind2Web, we first apply the SoM prompting to the training samples according to the top candidates selected by [140], and then finetune Magma on the same samples as in SeeClick [19]. Table 4 shows the results in three subtasks, and clearly indicates Magmas superiority to both general-domain and specific-domain LMMs. Similarly, on AITW Magma outperforms the state-of-the-art methods based on open-source or prosperity models. Considering that we use similar size of LLM and moderate amount of UI-related pretraining data, this decent performance is largely due to the proposed SoM and ToM modeling techniques, which significantly facilitate action grounding for UI navigation. Robotics Manipulation. Table 2 shows that the Magma 9 Method Backbone GPT-4-MindAct [27] GPT-4V-OmniParser [83] GPT-4 [98] GPT-4V [99] SeeAct [141] Fuyu-8B Fuyu-8B-GUI [17] MiniCPM-V MiniCPM-V-GUI [17] Qwen-VL SeeClick [19] CogAgent [43] Qwen2-UIX [78] GPT-4V [99] Gemini-Pro [36] GPT-4V [99] Fuyu-8B [4] Fuyu-8B [4] MiniCPM-V [128] MiniCPM-V [128] Qwen-VL [3] Qwen-VL [3] CogVLM [118] Qwen2 [124] Magma-8B (Ours) LLaMA3 [92] Input Source DoM Tree Image Cross-Website Op. F1 Ele. Acc Step SR Ele. Acc Cross-Task Op. F1 Step SR Cross-Domain Op. F1 Ele. Acc Step SR 35.8 41.0 21.5 38.0 4.8 13.9 8.2 20.3 13.2 21.4 27.3 39.2 57.2 51.1 84.8 67.7 67. 81.3 80.7 78.2 81.7 83.5 80.6 - - 76.9 30.1 36.5 13.9 19.6 32.4 4.0 12.2 6.0 17.3 9.2 16.4 23.4 31.0 45. 41.6 42.4 - 21.5 46.4 8.3 19.1 11.0 23.8 15.9 28.3 30.2 43.4 54.8 60.6 87.6 - 67.7 73. 83.9 86.1 85.6 86.8 86.7 87.0 - - 79.7 36.2 39.4 20.3 19.6 40.2 6.6 15.6 8.5 20.8 13.3 25.5 26.9 38.2 43. 37.1 45.5 - 20.7 42.4 3.6 14.2 6.5 17.9 14.1 23.2 33.1 40.4 55.7 46.5 85.7 - 64.3 69. 83.0 83.1 81.4 74.5 84.3 84.8 - - 80.6 26.4 42.0 23.7 18.0 36.8 3.0 11.7 5.2 17.6 12.0 20.8 28.5 34.9 47. Table 4. Efficient finetuning on Mind2Web for web UI navigation. Ele. Acc denotes element selection accuracy. Op. F1 denotes the token-wise F1 score between predicted ground-truth operation. Step SR denotes the step-wise success rate. Numbers reported in Chen et al. [17]. Numbers reported in Cheng et al. [19]. Numbers reported in Liu et al. [78]. Method GPT-4V-SeeAct [141] GPT-4V-ReAct [127] GPT-4V-OmniParser [83] Fuyu-8B Fuyu-8B-GUI [17] MiniCPM-V MiniCPM-V-GUI [17] Qwen-VL SeeClick [19] Magma-8B (Ours) Backbone GPT-4V [99] GPT-4V [99] GPT-4V [99] Fuyu-8B [4] Fuyu-8B [4] MiniCPM-V [128] MiniCPM-V [128] Qwen-VL [3] Qwen-VL [3] LLaMA3 [92] DoM Tree Image General 34.1 36.2 48.3 Install 39.4 42.5 57.8 GoogleApps 40.0 46.6 51.6 Single WebShopping 46.2 49.1 77.4 38.2 39.2 52. Overall 39.6 42.7 57.7 - - - - 49.5 54.0 61.5 45.9 50.9 50.2 62.3 59.9 66.4 73.2 40.0 41.6 45.1 46.5 46.9 54.9 62.7 47.2 45.7 56.2 67.3 64.7 63.5 77. 40.8 43.8 44.0 57.5 50.7 57.6 61.7 - - - - 54.3 59.3 67.3 Table 5. Efficient finetuning on AITW for mobile UI navigation. We compared models either using DoM tree or image screenshot. We finetune our Magma jointly and then report the results on individual tasks. Numbers reported in Zhang et al. [138]. Numbers reported in Chen et al. [17]. Numbers reported in Cheng et al. [19]. model without domain-specific finetuning already outperforms the recently proposed OpenVLA model pretrained for 27 epochs on the same amount of OXE data. Below, we testify the effectiveness of the finetuned Magma model by comparing it with OpenVLA in three settings: Finetune on real robot data to evaluate on out-ofdistribution manipulation tasks; Finetune in simulated robot settings with limited number of trajectories using the LIBERO benchmark to evaluate Magmas capability of task adaptation; and Evaluate on the physical WidoxW 250 Arm. We collect four manipulation tasks each of which has roughly 50 trajectories (See details in our supplementary material), and finetune both OpenVLA and Magma jointly on these tasks. For evaluation, we perform 10 trials per task, ensuring the same initial states (positions and orientations of end-effector and objects) across models. As shown in Fig. 9, the results clearly demonstrate Magmas superior performance. For those challenging tasks that involve everyday objects like Pick Place Hotdog Sausage, Put Mushroom in Pot, and Push Cloth Right to Left, OpenVLA can hardly accomplish the tasks, mainly because of the imprecise arm movement and object localization per our Figure 9. Few-shot finetuning and generalization performance on real robot. On WidowX robot, we evaluate Magma on 4 tasks including diverse everyday object manipulation. observation. In contrast, Magma performs well on these sophisticated tasks, largely owing to its strong spatial understanding and grounding capability obtained from pertaining. Additionally, we evaluate models performance on an unseen task Push Cloth Left to Right which are not included in our finetuning dataset. Magma substantially outperforms the baseline, indicating stronger ability to preserve pretrained knowledge and generalize to new tasks. The efficient adaptation (via finetuning) capability of Magma is further validated through few-shot finetuning Model VQAv2 GQA MME POPE TextVQA ChartQA DocVQA LLaVA-1.5-7B [61] LLaVA-Next-7B [75] Magma-8B (SFT) Magma-8B (Actw/o) Magma-8B (Fullw/o) Magma-8B (Full) 76.6 80. 79.5 81.3 81.3 81.4 62.6 64.2 61.5 63.5 62.9 64.0 1510.8 1519.3 1510.1 1559.5 1576.0 1588.7 85.9 86. 86.2 86.1 86.3 86.3 46.1 64.9 67.7 69.8 69.6 70.2 18.2 54.8 73.0 71.0 71.7 76.2 28.1 74. 80.4 84.1 83.8 84.8 Table 7. Finetuned performance on multimodal image understanding tasks. Pretraining on full set with SoM and ToM (last row) attains the overall best performance compared with our own baselines and counterparts of the same model class. approaches by significant margins on VSR and SpatialEval, and that Magma performs on par with CogVLM, despite only using 29M images for pretraining as compared to 1.5B images in the latter. In addition, our ablation study demonstrates the effectiveness of the SoM and ToM pretraining tasks in helping Magma improve its spatial reasoning capabilities. Last but not least, we also note the benefits of using video data during pretraining by showing that removing vidoes from training data leads to 8% performance drop on BLINK. Finally, we also provide some example predictions of our Magma model in Figure 11. We observe that spatial reasoning questions are also challenging for SOTA proprietary models such as GPT-4o. Despite the lack of pretraining on data with mazes, we see that Magma is still able to answer spatial reasoning questions about them. 5.3. Evaluating Multimodal Understanding Image instruction tuning. To further assess Magmas multimodal understanding capability, we conduct continuous finetuning on our Magma-SFT-820K data. Then, we compare the finetuned Magma model with existing VLMs on suite of commonly used image reasoning benchmarks, e.g. MME and GQA. As shown in Table 7, Magma outperforms recently-proposed VLMs on most of the tasks, with notable gains of 5% and 22% on TextVQA and ChartQA, respectively. Similarly to our observations in Table 6, our ablation study highlights the effectiveness of using SoM and ToM for pre-training, which leads to 5% improvement in ChartQA. Video Instruction Tuning In Table 8, we report the performance of our Magma model on multiple challenging video question answering (QA) benchmarks including IntentQA [62], NextQA [120], VideoMME [32] and MVBench [63]. We use the LMMs-Eval framework [59] for the latter three benchmarks to ensure reproducibility of our evaluation results. The results demonstrate the effectiveness of our pretraining approach, where we outperform most state-of-the-art models with comparable number of parameters consistently across the different benchmarks. For instance, our Magma 2We evaluate our model using the standard option matching before the official evaluation pipeline was released and will update in the next version. Figure 10. Few-shot finetuning results on the LIBERO simulation benchmark, using 10 trajectories per task for fine-tuning. Model GPT-4o Gemini LLaVA-1.5-7B LLaVA-1.6-7B [75] Qwen-VL-9.6B [3] Magma-8B (Actw/o) Magma-8B (Fullw/o) Magma-8B (Full) VSR BLINK-val SpatialEval2 Spatial Map Maze Nav. Spatial Grid 74.8 - 57.1* 52.2* - 62.8 58.1 65. 60.0 61.4 37.1 - 40.3 30.1 38.3 41.0 - - 28.4 28.0 28.7 36.9 27.5 43.4 - - 28.8 34.8 31.8 44.8 33.5 36. - - 41.6 32.2 25.7 37.5 47.3 64.5 Table 6. Spatial reasoning evaluations. We use * to denote results that are obtained by us evaluating the provided model weights. Superscript w/o means models pretrained without SoM/ToM. Figure 11. Spatial evaluation predictions. Spatial reasoning questions are challenging even for GPT-4o but Magma can answer relatively well despite relying on much fewer pretraining data. evaluations on the LIBERO benchmark. For each task suite in the benchmark, we sample only 10 trajectories for finetuning. During the evaluation, we perform 100 trials per task suite. The results, shown in Fig. 10, indicate that Magma achieves significantly higher average success rate in all task suites. Additionally, removing SoM and ToM during pretraining has negative impact on model performance, underscoring the effectiveness of our pretraining method. 5.2. Evaluating Spatial Reasoning We attribute the much improved performance of our Magma model on the tasks of UI navigation and robotic manipulation, as shown above, to its improved ability to perform spatial reasoning. To verify this hypothesis, we evaluate the effectiveness of the spatial intelligence that is learned in our pretrained model on the challenging Visual Spatial Reasoning (VSR) [72], BLINK [33] and SpatialEval [116] benchmarks under the zero-shot setting. The results are summarized in Table 6. We see that Magma outperforms existing 11 Method Backbone Gemini-1.5 [36] GPT-4V [2] - GPTLLaVA-OV [60] Long-Llava 9B [119] LongVA [136] ShareGPT4Video [15] Video-Llama2 [20] Video-Chat2 [63] Video-Llava [69] IG-VLM [55] SF-LLaVA [121] Qwen2-7B Long-Llava 9B Qwen2-7B LLaMA3-8B Llama2-7B Mistral 7B Vicuna-7B Vicuna-7B Vicuna-7B Magma-8B (Ours) LLaMA3-8B IntentQA Next-QA Overall Overall VideoMME (w/o subs) MVBench Short Medium Long Action Prediction Action Sequence Action localization Overall - - - - - - - - - 60.3 60.1 88.6 - - 79.4 - 69.3 - - 43.3 51.4 - - 80.9 81.7 70.5 68.1 52.4 61.1 48.3 55.9 48.3 45.3 - - 72.9 74.3 55. 54.9 42.2 50.4 36.3 45.4 37.0 38.0 - - 55.8 67.4 53.5 47.8 36.4 46.2 35.0 42.1 33.2 36.2 - - 44.3 - - 46.0 - 49.0 40.0 - 47.5 50.0 - - 65.0 - - 74.5 - 53.0 49.5 - 75.0 38.5 - - 79.0 - - 48.0 - 42.5 41.5 - 50.5 30.5 - - 55.5 37.7 43.7 56.7 49.1 51.3 51.2 34.1 60.4 43.0 - - 59.4 Table 8. Zero-shot Video QA benchmarks. We compare our Magma model to other state-of-the-art approaches with comparable numbers of parameters. Our Magma model performs competitively and even outperforms some state-of-the-art approaches such as Video-Llama2 and ShareGPT4Video on most benchmarks, despite using much fewer video instruction tuning data. model achieves performance gain over the IG-VLM and SF-LLaVA models by approximately 28%. The IntentQA benchmark evaluates models capability to discern the intentions behind observed actions in videos. Thus, the significant improvement on this dataset achieved by Magma can possibly be attributed to the effectiveness of our ToM pretraining task, where it encourages the model to reason about temporal dynamics in future video frames. This is also corroborated by the notable improvement on the subtask of action prediction in MVBench that Magma obtains over stateof-the-art models such as VideoChat2 and LLaVA-OV. State-of-the-art video LMMs often rely on much large video and text datasets such as Webvid and ShareGPT4Video for pretraining and these datasets span over 4M samples with curated text. Moreover, the aforementioned models also use higher number of frames during pretraining. In contrast, even when multi-frame pretraining is performed in our case, we only use maximum of 4 frames due to computational constraints. Thus, it is especially significant that Magma outperforms approaches such as LLaVA-OV and ShareGPT4Video on VideoMME and MVBench, since these approaches often use larger instruction tuning datasets that include both image and video data. Additionally, as evidenced by the performance gain obtained by Magma over the proprietary GPT-4V model, we note that such improvements in results are not solely due to using more recent and powerful language model like LLama-3. It is also notable that Magma achieves substantially better performance than LongVA, despite using only 32 frames instead of the 64 frames used by the latter. 6. Conclusion We present the Magma foundation model that can understand and act on multimodal inputs to complete agentic tasks in different environments. Our experiments show that the use of SoM and ToM prediction tasks in pretraining helps the model learn to ground and plan actions, respecIn our experiments, Magma shows strong spatialtively. temporal reasoning ability and significantly outperforms baselines on downstream UI navigation and robotic manipulation tasks. Social Impacts and Limitations. To develop foundation model with both verbal and spatial intelligence capable of handling diverse agentic tasks in digital and physical environments, we curated comprehensive pretraining dataset from wide range of image, video, and robotics domains: UI navigation data. We leverage two pretraining datasets SeeClick and Vision2UI. Instructional videos. As our goal was to learn an agentic model that can undertake daily tasks like humans, we compile the videos from Epic Kitchen, Ego4d, Something-Something v2 and other instructional videos. Robotics manipulation data. For robotics task, we follow OpenVLA to leverage the robotics data in Open-XEmbodiment. Multimodal understanding data. Lastly, we include small set of multi modal pretraining data ShareGPT4V, and instruction tuning data LlaVA-1.5 plus number of other domain-specific data to retain the generic multimodal understanding capability of the pre-trained model. The data markup of the robotics and UI navigation data is fairly standardized focusing on generic manipulation tasks (Place object on object) and generic UI navigation tasks (Click search button). We, however, performed detailed data reflection exercise on the video data of people performing certain tasks. The core inferences we took from these videos were the trajectory of objects over time when the tasks were performed. We note that the distribution of identities and activities in the instructional videos are not representative of the global human population and the diversity in society. We are cognizant of the unintended societal, gender, racial and other 12 biases in training with these data, so we will ensure required disclaimers are in place when publishing the models. The training dataset, task list and descriptions focus on the next action to perform only not describe, act on, or perform any analysis on the subject itself. While there can be unintended outputs from the model based on adverse task descriptions, we will ensure to highlight the use cases the model was trained for and its intended use. Responsible AI. It is important to note that the model is specifically designed for UI navigation in controlled Web UI and Android simulator, and robotic manipulation tasks and should not be broadly applied to other tasks. The recommended usage is within the settings they were trained on, namely, an enclosure equipped with robotic arm and everyday objects for robotic manipulation and an android simulator running on computer for UI manipulation. For UI navigation task, researchers should make sure that human is in the loop and in control for every action the agentic system generates. Since the model cannot act by itself, the sub-module researcher uses to actually perform the UI navigation action should ensure that no unintended consequences can occur as result of performing the UI action proposed by the model. The model by itself demonstrates good-enough capability in UI navigation and robotic manipulation, but is not usable as is for exploitation scenarios. threat actor, can however use specific training data for specific malicious task, to leverage the model as base to perform automated UI navigation. This is generic risk associated with the agentic models. Acknowledgments. We would also like to thank Professor Yong Jae Lee for thoughtful discussions, Xiyang Dai for valuable discussions and data support, Mei Yang and Denny Sun for early data engineering effort, and Swadheen Shukla for internal RAI and data reviews. We would also like to thank Doug Burger and Desney Tan for the multifaceted leadership support."
        },
        {
            "title": "References",
            "content": "[1] Marah Abdin, Jyoti Aneja, Hany Awadalla, Ahmed Awadallah, Ammar Ahmad Awan, Nguyen Bach, Amit Bahree, Arash Bakhtiari, Jianmin Bao, Harkirat Behl, Alon Benhaim, Misha Bilenko, Johan Bjorck, Sebastien Bubeck, Martin Cai, Qin Cai, Vishrav Chaudhary, Dong Chen, Dongdong Chen, Weizhu Chen, Yen-Chun Chen, Yi-Ling Chen, Hao Cheng, Parul Chopra, Xiyang Dai, Matthew Dixon, Ronen Eldan, Victor Fragoso, Jianfeng Gao, Mei Gao, Min Gao, Amit Garg, Allie Del Giorno, Abhishek Goswami, Suriya Gunasekar, Emman Haider, Junheng Hao, Russell J. Hewett, Wenxiang Hu, Jamie Huynh, Dan Iter, Sam Ade Jacobs, Mojan Javaheripi, 13 Xin Jin, Nikos Karampatziakis, Piero Kauffmann, Mahoud Khademi, Dongwoo Kim, Young Jin Kim, Lev Kurilenko, James R. Lee, Yin Tat Lee, Yuanzhi Li, Yunsheng Li, Chen Liang, Lars Liden, Xihui Lin, Zeqi Lin, Ce Liu, Liyuan Liu, Mengchen Liu, Weishung Liu, Xiaodong Liu, Chong Luo, Piyush Madan, Ali Mahmoudzadeh, David Majercak, Matt Mazzola, Caio Cesar Teodoro Mendes, Arindam Mitra, Hardik Modi, Anh Nguyen, Brandon Norick, Barun Patra, Daniel Perez-Becker, Thomas Portet, Reid Pryzant, Heyang Qin, Marko Radmilac, Liliang Ren, Gustavo de Rosa, Corby Rosset, Sambudha Roy, Olatunji Ruwase, Olli Saarikivi, Amin Saied, Adil Salim, Michael Santacroce, Shital Shah, Ning Shang, Hiteshi Sharma, Yelong Shen, Swadheen Shukla, Xia Song, Masahiro Tanaka, Andrea Tupini, Praneetha Vaddamanu, Chunyu Wang, Guanhua Wang, Lijuan Wang, Shuohang Wang, Xin Wang, Yu Wang, Rachel Ward, Wen Wen, Philipp Witte, Haiping Wu, Xiaoxia Wu, Michael Wyatt, Bin Xiao, Can Xu, Jiahang Xu, Weijian Xu, Jilong Xue, Sonali Yadav, Fan Yang, Jianwei Yang, Yifan Yang, Ziyi Yang, Donghan Yu, Lu Yuan, Chenruidong Zhang, Cyril Zhang, Jianwen Zhang, Li Lyna Zhang, Yi Zhang, Yue Zhang, Yunan Zhang, and Xiren Zhou. Phi-3 technical report: highly capable language model locally on your phone, 2024. 5 [2] Josh Achiam, Steven Adler, Sandhini Agarwal, Lama Ahmad, Ilge Akkaya, Florencia Leoni Aleman, Diogo Almeida, Janko Altenschmidt, Sam Altman, Shyamal Anadkat, et al. Gpt-4 technical report. arXiv preprint arXiv:2303.08774, 2023. 12 [3] Jinze Bai, Shuai Bai, Shusheng Yang, Shijie Wang, Sinan Tan, Peng Wang, Junyang Lin, Chang Zhou, and Jingren Zhou. Qwen-vl: versatile vision-language model for understanding, localization, text reading, and beyond. arXiv preprint arXiv:2308.12966, 2023. 9, 10, [4] Rohan Bavishi, Erich Elsen, Curtis Hawthorne, Maxwell Nye, Augustus Odena, Arushi Somani, and Sagnak Tasırlar. Introducing our multimodal models, 2023. 3, 9, 10 [5] Anthony Brohan, Noah Brown, Justice Carbajal, Yevgen Chebotar, Joseph Dabis, Chelsea Finn, Keerthana Gopalakrishnan, Karol Hausman, Alex Herzog, Jasmine Hsu, et al. Rt-1: Robotics transformer for real-world control at scale. arXiv preprint arXiv:2212.06817, 2022. 2 [6] Anthony Brohan, Noah Brown, Justice Carbajal, Yevgen Chebotar, Xi Chen, Krzysztof Choromanski, Tianli Ding, Danny Driess, Avinava Dubey, Chelsea Finn, et al. Rt-2: Vision-language-action models transfer web knowledge to robotic control. arXiv preprint arXiv:2307.15818, 2023. 2, 3 [7] Mu Cai, Haotian Liu, Siva Karthik Mustikovela, Gregory P. Meyer, Yuning Chai, Dennis Park, and Yong Jae Lee. Making large multimodal models understand arbitrary visual prompts. In IEEE Conference on Computer Vision and Pattern Recognition, 2024. 3 [8] Mu Cai, Reuben Tan, Jianrui Zhang, Bocheng Zou, Kai Zhang, Feng Yao, Fangrui Zhu, Jing Gu, Yiwu Zhong, Yuzhang Shang, Yao Dou, Jaden Park, Jianfeng Gao, Yong Jae Lee, and Jianwei Yang. Temporalbench: Benchmarking fine-grained temporal understanding for multimodal video models, 2024. 3 [9] Joao Carreira, Eric Noland, Chloe Hillier, and Andrew Zisserman. short note on the kinetics-700 human action dataset. arXiv preprint arXiv:1907.06987, 2019. 27 [10] Brandon Castellano. Pyscenedetect: Automated scene detection in videos, 20142024. Version 0.6.4, BSD 3-Clause License. [11] Chi-Lam Cheang, Guangzeng Chen, Ya Jing, Tao Kong, Hang Li, Yifeng Li, Yuxiao Liu, Hongtao Wu, Jiafeng Xu, Yichu Yang, Hanbo Zhang, and Minzhao Zhu. Gr-2: generative video-language-action model with web-scale knowledge for robot manipulation, 2024. 3 [12] Keqin Chen, Zhao Zhang, Weili Zeng, Richong Zhang, Shikra: Unleashing multiarXiv preprint Feng Zhu, and Rui Zhao. modal llms referential dialogue magic. arXiv:2306.15195, 2023. 3 [13] Lin Chen, Jisong Li, Xiaoyi Dong, Pan Zhang, Conghui He, Jiaqi Wang, Feng Zhao, and Dahua Lin. Sharegpt4v: Improving large multi-modal models with better captions. arXiv preprint arXiv:2311.12793, 2023. 2, 7, 25, 27 [14] Lin Chen, Jinsong Li, Xiaoyi Dong, Pan Zhang, Conghui He, Jiaqi Wang, Feng Zhao, and Dahua Lin. Sharegpt4v: Improving large multi-modal models with better captions, 2023. 5 [15] Lin Chen, Xilin Wei, Jinsong Li, Xiaoyi Dong, Pan Zhang, Yuhang Zang, Zehui Chen, Haodong Duan, Bin Lin, Improving video Zhenyu Tang, et al. Sharegpt4video: understanding and generation with better captions. arXiv preprint arXiv:2406.04325, 2024. 12 [16] Tsai-Shien Chen, Aliaksandr Siarohin, Willi Menapace, Ekaterina Deyneka, Hsiang wei Chao, Byung Eun Jeon, Yuwei Fang, Hsin-Ying Lee, Jian Ren, Ming-Hsuan Yang, and Sergey Tulyakov. Panda-70m: Captioning 70m videos with multiple cross-modality teachers, 2024. [17] Wentong Chen, Junbo Cui, Jinyi Hu, Yujia Qin, Junjie Fang, Yue Zhao, Chongyi Wang, Jun Liu, Guirong Chen, Yupeng Huo, Yuan Yao, Yankai Lin, Zhiyuan Liu, and Maosong Sun. Guicourse: From general vision language models to versatile gui agents, 2024. 10 [18] An-Chieh Cheng, Hongxu Yin, Yang Fu, Qiushan Guo, Ruihan Yang, Jan Kautz, Xiaolong Wang, and Sifei Liu. Spatialrgpt: Grounded spatial reasoning in vision language model. arXiv preprint arXiv:2406.01584, 2024. 4 [19] Kanzhi Cheng, Qiushi Sun, Yougang Chu, Fangzhi Xu, Yantao Li, Jianbing Zhang, and Zhiyong Wu. Seeclick: Harnessing gui grounding for advanced visual gui agents, 2024. 2, 3, 5, 6, 7, 8, 9, 10, 23, 25, 26, 27 [20] Zesen Cheng, Sicong Leng, Hang Zhang, Yifei Xin, Xin Li, Guanzheng Chen, Yongxin Zhu, Wenqi Zhang, Ziyang Luo, Deli Zhao, et al. Videollama 2: Advancing spatialtemporal modeling and audio understanding in video-llms. arXiv preprint arXiv:2406.07476, 2024. 12 [21] Wei-Lin Chiang, Zhuohan Li, Zi Lin, Ying Sheng, Zhanghao Wu, Hao Zhang, Lianmin Zheng, Siyuan Zhuang, Yonghao Zhuang, Joseph E. Gonzalez, Ion Stoica, and Eric P. Xing. Vicuna: An open-source chatbot impressing gpt-4 with 90%* chatgpt quality, 2023. 3 [22] Embodiment Collaboration, Abby ONeill, Abdul Rehman, Abhinav Gupta, Abhiram Maddukuri, Abhishek Gupta, Abhishek Padalkar, Abraham Lee, Acorn Pooley, Agrim Gupta, Ajay Mandlekar, Ajinkya Jain, Albert Tung, Alex Bewley, Alex Herzog, Alex Irpan, Alexander Khazatsky, Anant Rai, Anchit Gupta, Andrew Wang, Andrey Kolobov, Anikait Singh, Animesh Garg, Aniruddha Kembhavi, Annie Xie, Anthony Brohan, Antonin Raffin, Archit Sharma, Arefeh Yavary, Arhan Jain, Ashwin Balakrishna, Ayzaan Wahid, Ben Burgess-Limerick, Beomjoon Kim, Bernhard Scholkopf, Blake Wulfe, Brian Ichter, Cewu Lu, Charles Xu, Charlotte Le, Chelsea Finn, Chen Wang, Chenfeng Xu, Cheng Chi, Chenguang Huang, Christine Chan, Christopher Agia, Chuer Pan, Chuyuan Fu, Coline Devin, Danfei Xu, Daniel Morton, Danny Driess, Daphne Chen, Deepak Pathak, Dhruv Shah, Dieter Buchler, Dinesh Jayaraman, Dmitry Kalashnikov, Dorsa Sadigh, Edward Johns, Ethan Foster, Fangchen Liu, Federico Ceola, Fei Xia, Feiyu Zhao, Felipe Vieira Frujeri, Freek Stulp, Gaoyue Zhou, Gaurav S. Sukhatme, Gautam Salhotra, Ge Yan, Gilbert Feng, Giulio Schiavi, Glen Berseth, Gregory Kahn, Guangwen Yang, Guanzhi Wang, Hao Su, Hao-Shu Fang, Haochen Shi, Henghui Bao, Heni Ben Amor, Henrik Christensen, Hiroki Furuta, Homanga Bharadhwaj, Homer Walke, Hongjie Fang, Huy Ha, Igor Mordatch, Ilija Radosavovic, Isabel Leal, Jacky Liang, Jad Abou-Chakra, Jaehyung Kim, Jaimyn Drake, Jan Peters, Jan Schneider, Jasmine Hsu, Jay Vakil, Jeannette Bohg, Jeffrey Bingham, Jeffrey Wu, Jensen Gao, Jiaheng Hu, Jiajun Wu, Jialin Wu, Jiankai Sun, Jianlan Luo, Jiayuan Gu, Jie Tan, Jihoon Oh, Jimmy Wu, Jingpei Lu, Jingyun Yang, Jitendra Malik, Joao Silverio, Joey Hejna, Jonathan Booher, Jonathan Tompson, Jonathan Yang, Jordi Salvador, Joseph J. Lim, Junhyek Han, Kaiyuan Wang, Kanishka Rao, Karl Pertsch, Karol Hausman, Keegan Go, Keerthana Gopalakrishnan, Ken Goldberg, Kendra Byrne, Kenneth Oslund, Kento Kawaharazuka, Kevin Black, Kevin Lin, Kevin Zhang, Kiana Ehsani, Kiran Lekkala, Kirsty Ellis, Krishan Rana, Krishnan Srinivasan, Kuan Fang, Kunal Pratap Singh, Kuo-Hao Zeng, Kyle Hatch, Kyle Hsu, Laurent Itti, Lawrence Yunliang Chen, Lerrel Pinto, Li Fei-Fei, Liam Tan, Linxi Jim Fan, Lionel Ott, Lisa Lee, Luca Weihs, Magnum Chen, Marion Lepert, Marius Memmel, Masayoshi Tomizuka, Masha Itkina, Mateo Guaman Castro, Max Spero, Maximilian Du, Michael Ahn, Michael C. Yip, Mingtong Zhang, Mingyu Ding, Minho Heo, Mohan Kumar Srirama, Mohit Sharma, Moo Jin Kim, Naoaki Kanazawa, Nicklas Hansen, Nicolas Heess, Nikhil Joshi, Niko Suenderhauf, Ning Liu, Norman Di Palo, Nur Muhammad Mahi Shafiullah, Oier Mees, Oliver Kroemer, Osbert Bastani, Pannag Sanketi, Patrick Tree Miller, Patrick Yin, Paul Wohlhart, Peng Xu, Peter David Fagan, Peter Mitrano, Pierre Sermanet, Pieter Abbeel, Priya Sundaresan, Qiuyu Chen, Quan Vuong, Rafael Rafailov, Ran Tian, Ria Doshi, Roberto Martin-Martin, Rohan Baijal, Rosario Scalise, Rose Hendrix, Roy Lin, Runjia Qian, Ruohan Zhang, Russell Mendonca, Rutav Shah, Ryan Hoque, Ryan Julian, Samuel Bustamante, Sean Kirmani, Sergey Levine, Shan Lin, Sherry Moore, Shikhar Bahl, Shivin Dass, Shubham Sonawani, Shubham Tulsiani, Shuran Song, Sichun Xu, Siddhant Haldar, Siddharth Karamcheti, Simeon Adebola, Simon Guist, Soroush Nasiriany, Stefan Schaal, Stefan Welker, Stephen Tian, Subramanian Ramamoorthy, Sudeep Dasari, Suneel Belkhale, Sungjae Park, Suraj Nair, Suvir Mirchandani, Takayuki Osa, Tanmay Gupta, Tatsuya Harada, Tatsuya Matsushima, Ted Xiao, Thomas Kollar, Tianhe Yu, Tianli Ding, Todor Davchev, Tony Z. Zhao, Travis Armstrong, Trevor Darrell, Trinity Chung, Vidhi Jain, Vikash Kumar, Vincent Vanhoucke, Wei Zhan, Wenxuan Zhou, Wolfram Burgard, Xi Chen, Xiangyu Chen, Xiaolong Wang, Xinghao Zhu, Xinyang Geng, Xiyuan Liu, Xu Liangwei, Xuanlin Li, Yansong Pang, Yao Lu, Yecheng Jason Ma, Yejin Kim, Yevgen Chebotar, Yifan Zhou, Yifeng Zhu, Yilin Wu, Ying Xu, Yixuan Wang, Yonatan Bisk, Yongqiang Dou, Yoonyoung Cho, Youngwoon Lee, Yuchen Cui, Yue Cao, Yueh-Hua Wu, Yujin Tang, Yuke Zhu, Yunchu Zhang, Yunfan Jiang, Yunshuang Li, Yunzhu Li, Yusuke Iwasawa, Yutaka Matsuo, Zehan Ma, Zhuo Xu, Zichen Jeff Cui, Zichen Zhang, Zipeng Fu, and Zipeng Lin. Open x-embodiment: Robotic learning datasets and rt-x models, 2024. 5, 9 [23] Open X-Embodiment Collaboration, Abby ONeill, Abdul Rehman, Abhinav Gupta, Abhiram Maddukuri, Abhishek Gupta, Abhishek Padalkar, Abraham Lee, Acorn Pooley, Agrim Gupta, Ajay Mandlekar, Ajinkya Jain, Albert Tung, Alex Bewley, Alex Herzog, Alex Irpan, Alexander Khazatsky, Anant Rai, Anchit Gupta, Andrew Wang, Andrey Kolobov, Anikait Singh, Animesh Garg, Aniruddha Kembhavi, Annie Xie, Anthony Brohan, Antonin Raffin, Archit Sharma, Arefeh Yavary, Arhan Jain, Ashwin Balakrishna, Ayzaan Wahid, Ben Burgess-Limerick, Beomjoon Kim, Bernhard Scholkopf, Blake Wulfe, Brian Ichter, Cewu Lu, Charles Xu, Charlotte Le, Chelsea Finn, Chen Wang, Chenfeng Xu, Cheng Chi, Chenguang Huang, Christine Chan, Christopher Agia, Chuer Pan, Chuyuan Fu, Coline Devin, Danfei Xu, Daniel Morton, Danny Driess, Daphne Chen, Deepak Pathak, Dhruv Shah, Dieter Buchler, Dinesh Jayaraman, Dmitry Kalashnikov, Dorsa Sadigh, Edward Johns, Ethan Foster, Fangchen Liu, Federico Ceola, Fei Xia, Feiyu Zhao, Felipe Vieira Frujeri, Freek Stulp, Gaoyue Zhou, Gaurav S. Sukhatme, Gautam Salhotra, Ge Yan, Gilbert Feng, Giulio Schiavi, Glen Berseth, Gregory Kahn, Guangwen Yang, Guanzhi Wang, Hao Su, Hao-Shu Fang, Haochen Shi, Henghui Bao, Heni Ben Amor, Henrik Christensen, Hiroki Furuta, Homanga Bharadhwaj, Homer Walke, Hongjie Fang, Huy Ha, Igor Mordatch, Ilija Radosavovic, Isabel Leal, Jacky Liang, Jad Abou-Chakra, Jaehyung Kim, Jaimyn Drake, Jan Peters, Jan Schneider, Jasmine Hsu, Jay Vakil, Jeannette Bohg, Jeffrey Bingham, Jeffrey Wu, Jensen Gao, Jiaheng Hu, Jiajun Wu, Jialin Wu, Jiankai Sun, Jianlan Luo, Jiayuan Gu, Jie Tan, Jihoon Oh, Jimmy Wu, Jingpei Lu, Jingyun Yang, Jitendra Malik, Joao Silverio, Joey Hejna, Jonathan Booher, Jonathan Tompson, Jonathan Yang, Jordi Salvador, Joseph J. Lim, Junhyek Han, Kaiyuan Wang, Kanishka Rao, Karl Pertsch, Karol Hausman, Keegan Go, Keerthana Gopalakrishnan, Ken Goldberg, Kendra Byrne, Kenneth Oslund, Kento Kawaharazuka, Kevin Black, Kevin Lin, Kevin Zhang, Kiana Ehsani, Kiran Lekkala, Kirsty Ellis, Krishan Rana, Krishnan Srinivasan, Kuan Fang, Kunal Pratap Singh, Kuo-Hao Zeng, Kyle Hatch, Kyle Hsu, Laurent Itti, Lawrence Yunliang Chen, Lerrel Pinto, Li Fei-Fei, Liam Tan, Linxi Jim Fan, Lionel Ott, Lisa Lee, Luca Weihs, Magnum Chen, Marion Lepert, Marius Memmel, Masayoshi Tomizuka, Masha Itkina, Mateo Guaman Castro, Max Spero, Maximilian Du, Michael Ahn, Michael C. Yip, Mingtong Zhang, Mingyu Ding, Minho Heo, Mohan Kumar Srirama, Mohit Sharma, Moo Jin Kim, Naoaki Kanazawa, Nicklas Hansen, Nicolas Heess, Nikhil Joshi, Niko Suenderhauf, Ning Liu, Norman Di Palo, Nur Muhammad Mahi Shafiullah, Oier Mees, Oliver Kroemer, Osbert Bastani, Pannag Sanketi, Patrick Tree Miller, Patrick Yin, Paul Wohlhart, Peng Xu, Peter David Fagan, Peter Mitrano, Pierre Sermanet, Pieter Abbeel, Priya Sundaresan, Qiuyu Chen, Quan Vuong, Rafael Rafailov, Ran Tian, Ria Doshi, Roberto Martin-Martin, Rohan Baijal, Rosario Scalise, Rose Hendrix, Roy Lin, Runjia Qian, Ruohan Zhang, Russell Mendonca, Rutav Shah, Ryan Hoque, Ryan Julian, Samuel Bustamante, Sean Kirmani, Sergey Levine, Shan Lin, Sherry Moore, Shikhar Bahl, Shivin Dass, Shubham Sonawani, Shubham Tulsiani, Shuran Song, Sichun Xu, Siddhant Haldar, Siddharth Karamcheti, Simeon Adebola, Simon Guist, Soroush Nasiriany, Stefan Schaal, Stefan Welker, Stephen Tian, Subramanian Ramamoorthy, Sudeep Dasari, Suneel Belkhale, Sungjae Park, Suraj Nair, Suvir Mirchandani, Takayuki Osa, Tanmay Gupta, Tatsuya Harada, Tatsuya Matsushima, Ted Xiao, Thomas Kollar, Tianhe Yu, Tianli Ding, Todor Davchev, Tony Z. Zhao, Travis Armstrong, Trevor Darrell, Trinity Chung, Vidhi Jain, Vikash Kumar, Vincent Vanhoucke, Wei Zhan, Wenxuan Zhou, Wolfram Burgard, Xi Chen, Xiangyu Chen, Xiaolong Wang, Xinghao Zhu, Xinyang Geng, Xiyuan Liu, Xu Liangwei, Xuanlin Li, Yansong Pang, Yao Lu, Yecheng Jason Ma, Yejin Kim, Yevgen Chebotar, Yifan Zhou, Yifeng Zhu, Yilin Wu, Ying Xu, Yixuan Wang, Yonatan Bisk, Yongqiang Dou, Yoonyoung Cho, Youngwoon Lee, Yuchen Cui, Yue Cao, Yueh-Hua Wu, Yujin Tang, Yuke Zhu, Yunchu Zhang, Yunfan Jiang, Yunshuang Li, Yunzhu Li, Yusuke Iwasawa, Yutaka Matsuo, Zehan Ma, Zhuo Xu, Zichen Jeff Cui, Zichen Zhang, Zipeng Fu, and Zipeng Lin. Open X-Embodiment: Robotic learning datasets and RT-X models. https: //arxiv.org/abs/2310.08864, 2023. 2, 4, 7, 9 [24] Dima Damen, Hazel Doughty, Giovanni Maria Farinella, Sanja Fidler, Antonino Furnari, Evangelos Kazakos, Davide Moltisanti, Jonathan Munro, Toby Perrett, Will Price, and Michael Wray. Scaling egocentric vision: The epickitchens dataset. In European Conference on Computer Vision (ECCV), 2018. 5, 24 [25] Dima Damen, Hazel Doughty, Giovanni Maria Farinella, Antonino Furnari, Jian Ma, Evangelos Kazakos, Davide Moltisanti, Jonathan Munro, Toby Perrett, Will Price, and Michael Wray. Rescaling egocentric vision: Collection, Internapipeline and challenges for epic-kitchens-100. tional Journal of Computer Vision (IJCV), 130:3355, 2022. 5 [26] Biplab Deka, Zifeng Huang, Chad Franzen, Joshua Hibschman, Daniel Afergan, Y. Li, Jeffrey Nichols, and Ranjitha Kumar. Rico: mobile app dataset for building datadriven design applications. Proceedings of the 30th Annual ACM Symposium on User Interface Software and Technology, 2017. 23 [27] Xiang Deng, Yu Gu, Boyuan Zheng, Shijie Chen, Samuel Stevens, Boshi Wang, Huan Sun, and Yu Su. Mind2web: Towards generalist agent for the web, 2023. 3, 6, 10, 25, 26, 27 [28] Carl Doersch, Ankush Gupta, Larisa Markeeva, Adria Recasens, Lucas Smaira, Yusuf Aytar, Joao Carreira, Andrew Zisserman, and Yi Yang. Tap-vid: benchmark for tracking any point in video. Advances in Neural Information Processing Systems, 35:1361013626, 2022. 6 [29] Danny Driess, Fei Xia, Mehdi SM Sajjadi, Corey Lynch, Aakanksha Chowdhery, Brian Ichter, Ayzaan Wahid, Jonathan Tompson, Quan Vuong, Tianhe Yu, et al. PaLMarXiv E: An embodied multimodal preprint arXiv:2303.03378, 2023. 2 language model. [30] Abhimanyu Dubey, Abhinav Jauhri, Abhinav Pandey, Abhishek Kadian, Ahmad Al-Dahle, Aiesha Letman, Akhil Mathur, Alan Schelten, Amy Yang, Angela Fan, Anirudh Goyal, Anthony Hartshorn, Aobo Yang, Archi Mitra, Archie Sravankumar, Artem Korenev, Arthur Hinsvark, Arun Rao, Aston Zhang, Aurelien Rodriguez, Austen Gregerson, Ava Spataru, Baptiste Roziere, Bethany Biron, Binh Tang, Bobbie Chern, Charlotte Caucheteux, Chaya Nayak, Chloe Bi, Chris Marra, Chris McConnell, Christian Keller, Christophe Touret, Chunyang Wu, Corinne Wong, Cristian Canton Ferrer, Cyrus Nikolaidis, Damien Allonsius, Daniel Song, Danielle Pintz, Danny Livshits, David Esiobu, Dhruv Choudhary, Dhruv Mahajan, Diego Garcia-Olano, Diego Perino, Dieuwke Hupkes, Egor Lakomkin, Ehab AlBadawy, Elina Lobanova, Emily Dinan, Eric Michael Smith, Filip Radenovic, Frank Zhang, Gabriel Synnaeve, Gabrielle Lee, Georgia Lewis Anderson, Graeme Nail, Gregoire Mialon, Guan Pang, Guillem Cucurell, Hailey Nguyen, Hannah Korevaar, Hu Xu, Hugo Touvron, Iliyan Zarov, Imanol Arrieta Ibarra, Isabel Kloumann, Ishan Misra, Ivan Evtimov, Jade Copet, Jaewon Lee, Jan Geffert, Jana Vranes, Jason Park, Jay Mahadeokar, Jeet Shah, Jelmer van der Linde, Jennifer Billock, Jenny Hong, Jenya Lee, Jeremy Fu, Jianfeng Chi, Jianyu Huang, Jiawen Liu, Jie Wang, Jiecao Yu, Joanna Bitton, Joe Spisak, Jongsoo Park, Joseph Rocca, Joshua Johnstun, Joshua Saxe, Junteng Jia, Kalyan Vasuden Alwala, Kartikeya Upasani, Kate Plawiak, Ke Li, Kenneth Heafield, Kevin Stone, Khalid El-Arini, Krithika Iyer, Kshitiz Malik, Kuenley Chiu, Kunal Bhalla, Lauren Rantala-Yeary, Laurens van der Maaten, Lawrence Chen, Liang Tan, Liz Jenkins, Louis Martin, Lovish Madaan, Lubo Malo, Lukas Blecher, Lukas Landzaat, Luke de Oliveira, Madeline Muzzi, Mahesh Pasupuleti, Mannat Singh, Manohar Paluri, Marcin Kardas, Mathew Oldham, Mathieu Rita, Maya Pavlova, Melanie Kambadur, Mike Lewis, Min Si, Mitesh Kumar Singh, Mona Hassan, Naman Goyal, Narjes Torabi, Nikolay Bashlykov, 16 Nikolay Bogoychev, Niladri Chatterji, Olivier Duchenne, Onur elebi, Patrick Alrassy, Pengchuan Zhang, Pengwei Li, Petar Vasic, Peter Weng, Prajjwal Bhargava, Pratik Dubal, Praveen Krishnan, Punit Singh Koura, Puxin Xu, Qing He, Qingxiao Dong, Ragavan Srinivasan, Raj Ganapathy, Ramon Calderer, Ricardo Silveira Cabral, Robert Stojnic, Roberta Raileanu, Rohit Girdhar, Rohit Patel, Romain Sauvestre, Ronnie Polidoro, Roshan Sumbaly, Ross Taylor, Ruan Silva, Rui Hou, Rui Wang, Saghar Hosseini, Sahana Chennabasappa, Sanjay Singh, Sean Bell, Seohyun Sonia Kim, Sergey Edunov, Shaoliang Nie, Sharan Narang, Sharath Raparthy, Sheng Shen, Shengye Wan, Shruti Bhosale, Shun Zhang, Simon Vandenhende, Soumya Batra, Spencer Whitman, Sten Sootla, Stephane Collot, Suchin Gururangan, Sydney Borodinsky, Tamar Herman, Tara Fowler, Tarek Sheasha, Thomas Georgiou, Thomas Scialom, Tobias Speckbacher, Todor Mihaylov, Tong Xiao, Ujjwal Karn, Vedanuj Goswami, Vibhor Gupta, Vignesh Ramanathan, Viktor Kerkez, Vincent Gonguet, Virginie Do, Vish Vogeti, Vladan Petrovic, Weiwei Chu, Wenhan Xiong, Wenyin Fu, Whitney Meers, Xavier Martinet, Xiaodong Wang, Xiaoqing Ellen Tan, Xinfeng Xie, Xuchao Jia, Xuewei Wang, Yaelle Goldschlag, Yashesh Gaur, Yasmine Babaei, Yi Wen, Yiwen Song, Yuchen Zhang, Yue Li, Yuning Mao, Zacharie Delpierre Coudert, Zheng Yan, Zhengxing Chen, Zoe Papakipos, Aaditya Singh, Aaron Grattafiori, Abha Jain, Adam Kelsey, Adam Shajnfeld, Adithya Gangidi, Adolfo Victoria, Ahuva Goldstand, Ajay Menon, Ajay Sharma, Alex Boesenberg, Alex Vaughan, Alexei Baevski, Allie Feinstein, Amanda Kallet, Amit Sangani, Anam Yunus, Andrei Lupu, Andres Alvarado, Andrew Caples, Andrew Gu, Andrew Ho, Andrew Poulton, Andrew Ryan, Ankit Ramchandani, Annie Franco, Aparajita Saraf, Arkabandhu Chowdhury, Ashley Gabriel, Ashwin Bharambe, Assaf Eisenman, Azadeh Yazdan, Beau James, Ben Maurer, Benjamin Leonhardi, Bernie Huang, Beth Loyd, Beto De Paola, Bhargavi Paranjape, Bing Liu, Bo Wu, Boyu Ni, Braden Hancock, Bram Wasti, Brandon Spence, Brani Stojkovic, Brian Gamido, Britt Montalvo, Carl Parker, Carly Burton, Catalina Mejia, Changhan Wang, Changkyu Kim, Chao Zhou, Chester Hu, ChingHsiang Chu, Chris Cai, Chris Tindal, Christoph Feichtenhofer, Damon Civin, Dana Beaty, Daniel Kreymer, Daniel Li, Danny Wyatt, David Adkins, David Xu, Davide Testuggine, Delia David, Devi Parikh, Diana Liskovich, Didem Foss, Dingkang Wang, Duc Le, Dustin Holland, Edward Dowling, Eissa Jamil, Elaine Montgomery, Eleonora Presani, Emily Hahn, Emily Wood, Erik Brinkman, Esteban Arcaute, Evan Dunbar, Evan Smothers, Fei Sun, Felix Kreuk, Feng Tian, Firat Ozgenel, Francesco Caggioni, Francisco Guzman, Frank Kanayet, Frank Seide, Gabriela Medina Florez, Gabriella Schwarz, Gada Badeer, Georgia Swee, Gil Halpern, Govind Thattai, Grant Herman, Grigory Sizov, Guangyi, Zhang, Guna Lakshminarayanan, Hamid Shojanazeri, Han Zou, Hannah Wang, Hanwen Zha, Haroun Habeeb, Harrison Rudolph, Helen Suk, Henry Aspegren, Hunter Goldman, Ibrahim Damlaj, Igor Molybog, Igor Tufanov, Irina-Elena Veliche, Itai Gat, Jake Weissman, James Geboski, James Kohli, Japhet Asher, JeanBaptiste Gaya, Jeff Marcus, Jeff Tang, Jennifer Chan, Jenny Zhen, Jeremy Reizenstein, Jeremy Teboul, Jessica Zhong, Jian Jin, Jingyi Yang, Joe Cummings, Jon Carvill, Jon Shepard, Jonathan McPhie, Jonathan Torres, Josh Ginsburg, Junjie Wang, Kai Wu, Kam Hou U, Karan Saxena, Karthik Prasad, Kartikay Khandelwal, Katayoun Zand, Kathy Matosich, Kaushik Veeraraghavan, Kelly Michelena, Keqian Li, Kun Huang, Kunal Chawla, Kushal Lakhotia, Kyle Huang, Lailin Chen, Lakshya Garg, Lavender A, Leandro Silva, Lee Bell, Lei Zhang, Liangpeng Guo, Licheng Yu, Liron Moshkovich, Luca Wehrstedt, Madian Khabsa, Manav Avalani, Manish Bhatt, Maria Tsimpoukelli, Martynas Mankus, Matan Hasson, Matthew Lennie, Matthias Reso, Maxim Groshev, Maxim Naumov, Maya Lathi, Meghan Keneally, Michael L. Seltzer, Michal Valko, Michelle Restrepo, Mihir Patel, Mik Vyatskov, Mikayel Samvelyan, Mike Clark, Mike Macey, Mike Wang, Miquel Jubert Hermoso, Mo Metanat, Mohammad Rastegari, Munish Bansal, Nandhini Santhanam, Natascha Parks, Natasha White, Navyata Bawa, Nayan Singhal, Nick Egebo, Nicolas Usunier, Nikolay Pavlovich Laptev, Ning Dong, Ning Zhang, Norman Cheng, Oleg Chernoguz, Olivia Hart, Omkar Salpekar, Ozlem Kalinli, Parkin Kent, Parth Parekh, Paul Saab, Pavan Balaji, Pedro Rittner, Philip Bontrager, Pierre Roux, Piotr Dollar, Polina Zvyagina, Prashant Ratanchandani, Pritish Yuvraj, Qian Liang, Rachad Alao, Rachel Rodriguez, Rafi Ayub, Raghotham Murthy, Raghu Nayani, Rahul Mitra, Raymond Li, Rebekkah Hogan, Robin Battey, Rocky Wang, Rohan Maheswari, Russ Howes, Ruty Rinott, Sai Jayesh Bondu, Samyak Datta, Sara Chugh, Sara Hunt, Sargun Dhillon, Sasha Sidorov, Satadru Pan, Saurabh Verma, Seiji Yamamoto, Sharadh Ramaswamy, Shaun Lindsay, Shaun Lindsay, Sheng Feng, Shenghao Lin, Shengxin Cindy Zha, Shiva Shankar, Shuqiang Zhang, Shuqiang Zhang, Sinong Wang, Sneha Agarwal, Soji Sajuyigbe, Soumith Chintala, Stephanie Max, Stephen Chen, Steve Kehoe, Steve Satterfield, Sudarshan Govindaprasad, Sumit Gupta, Sungmin Cho, Sunny Virk, Suraj Subramanian, Sy Choudhury, Sydney Goldman, Tal Remez, Tamar Glaser, Tamara Best, Thilo Kohler, Thomas Robinson, Tianhe Li, Tianjun Zhang, Tim Matthews, Timothy Chou, Tzook Shaked, Varun Vontimitta, Victoria Ajayi, Victoria Montanez, Vijai Mohan, Vinay Satish Kumar, Vishal Mangla, Vıtor Albiero, Vlad Ionescu, Vlad Poenaru, Vlad Tiberiu Mihailescu, Vladimir Ivanov, Wei Li, Wenchen Wang, Wenwen Jiang, Wes Bouaziz, Will Constable, Xiaocheng Tang, Xiaofang Wang, Xiaojian Wu, Xiaolan Wang, Xide Xia, Xilun Wu, Xinbo Gao, Yanjun Chen, Ye Hu, Ye Jia, Ye Qi, Yenda Li, Yilin Zhang, Ying Zhang, Yossi Adi, Youngjin Nam, Yu, Wang, Yuchen Hao, Yundi Qian, Yuzi He, Zach Rait, Zachary DeVito, Zef Rosnbrick, Zhaoduo Wen, Zhenyu Yang, and Zhiwei Zhao. The llama 3 herd of models, 2024. 7 [31] Elan Dubrofsky. Homography estimation. Diplomova prace. Vancouver: Univerzita Britske Kolumbie, 5, 2009. 7 [32] Chaoyou Fu, Yuhan Dai, Yongdong Luo, Lei Li, Shuhuai Ren, Renrui Zhang, Zihan Wang, Chenyu Zhou, Yunhang Shen, Mengdan Zhang, Peixian Chen, Yanwei Li, Shaohui Lin, Sirui Zhao, Ke Li, Tong Xu, Xiawu Zheng, Enhong Chen, Rongrong Ji, and Xing Sun. Video-mme: The firstever comprehensive evaluation benchmark of multi-modal llms in video analysis, 2024. 3, 11 [33] Xingyu Fu, Yushi Hu, Bangzheng Li, Yu Feng, Haoyu Wang, Xudong Lin, Dan Roth, Noah Smith, Wei-Chiu Ma, and Ranjay Krishna. Blink: Multimodal large language models can see but not perceive. arXiv preprint arXiv:2404.12390, 2024. 3, 11 [34] Hiroki Furuta, Kuang-Huei Lee, Ofir Nachum, Yutaka Matsuo, Aleksandra Faust, Shixiang Shane Gu, and Izzeddin Gur. Multimodal web navigation with instruction-finetuned arXiv preprint arXiv:2305.11854, foundation models. 2023. 2 [35] Hiroki Furuta, Kuang-Huei Lee, Ofir Nachum, Yutaka Matsuo, Aleksandra Faust, Shixiang Shane Gu, and Izzeddin Gur. Multimodal web navigation with instruction-finetuned foundation models, 2024. 3 [36] Gemini Team. Gemini: family of highly capable multimodal models, 2024. 10, [37] Raghav Goyal, Samira Ebrahimi Kahou, Vincent Michalski, Joanna Materzynska, Susanne Westphal, Heuna Kim, Valentin Haenel, Ingo Fruend, Peter Yianilos, Moritz Mueller-Freitag, et al. The something something video database for learning and evaluating visual common sense. In Proceedings of the IEEE international conference on computer vision, pages 58425850, 2017. 5 [38] Yash Goyal, Tejas Khot, Douglas Summers-Stay, Dhruv Batra, and Devi Parikh. Making the in vqa matter: Elevating the role of image understanding in visual question answering. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 69046913, 2017. 27 [39] Yash Goyal, Tejas Khot, Douglas Summers-Stay, Dhruv Batra, and Devi Parikh. Making the in vqa matter: Elevating the role of image understanding in visual question answering. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 69046913, 2017. 8, 27 [40] Kristen Grauman, Andrew Westbury, Eugene Byrne, Zachary Chavis, Antonino Furnari, Rohit Girdhar, Jackson Hamburger, Hao Jiang, Miao Liu, Xingyu Liu, Miguel Martin, Tushar Nagarajan, Ilija Radosavovic, Santhosh Kumar Ramakrishnan, Fiona Ryan, Jayant Sharma, Michael Wray, Mengmeng Xu, Eric Zhongcong Xu, Chen Zhao, Siddhant Bansal, Dhruv Batra, Vincent Cartillier, Sean Crane, Tien Do, Morrie Doulaty, Akshay Erapalli, Christoph Feichtenhofer, Adriano Fragomeni, Qichen Fu, Abrham Gebreselasie, Cristina Gonzalez, James Hillis, Xuhua Huang, Yifei Huang, Wenqi Jia, Weslie Khoo, Jachym Kolar, Satwik Kottur, Anurag Kumar, Federico Landini, Chao Li, Yanghao Li, Zhenqiang Li, Karttikeya Mangalam, Raghava Modhugu, Jonathan Munro, Tullie Murrell, Takumi Nishiyasu, Will Price, Paola Ruiz Puentes, Merey Ramazanova, Leda Sari, Kiran Somasundaram, Audrey Southerland, Yusuke Sugano, Ruijie Tao, Minh Vo, Yuchen Wang, Xindi Wu, Takuma Yagi, Ziwei Zhao, Yunyi Zhu, Pablo Arbelaez, David Crandall, Dima Damen, Giovanni Maria Farinella, Christian Fuegen, Bernard Ghanem, Vamsi Krishna Ithapu, C. V. Jawahar, Hanbyul Joo, Kris Kitani, Haizhou Li, Richard Newcombe, Aude Oliva, Hyun Soo Park, James M. Rehg, Yoichi Sato, Jianbo Shi, Mike Zheng Shou, Antonio Torralba, Lorenzo Torresani, Mingfei Yan, and Jitendra Malik. Ego4d: Around the world in 3,000 hours of egocentric video, 2022. 2, 5, 7, 24 [41] Yi Gui, Zhen Li, Yao Wan, Yemin Shi, Hongyu Zhang, Yi Su, Shaoling Dong, Xing Zhou, and Wenbin Jiang. Vision2ui: real-world dataset with layout for code generation from ui designs, 2024. 5, 6, 7, 23, 27 [42] Wenyi Hong, Weihan Wang, Qingsong Lv, Jiazheng Xu, Wenmeng Yu, Junhui Ji, Yan Wang, Zihan Wang, Yuxiao Dong, Ming Ding, et al. Cogagent: visual language model for gui agents. arXiv preprint arXiv:2312.08914, 2023. [43] Wenyi Hong, Weihan Wang, Qingsong Lv, Jiazheng Xu, Wenmeng Yu, Junhui Ji, Yan Wang, Zihan Wang, Yuxuan Zhang, Juanzi Li, Bin Xu, Yuxiao Dong, Ming Ding, and Jie Tang. Cogagent: visual language model for gui agents, 2023. 3, 9, 10 [44] Haoxu Huang, Fanqi Lin, Yingdong Hu, Shengjie Wang, and Yang Gao. Copa: General robotic manipulation through spatial constraints of parts with foundation models. arXiv preprint arXiv:2403.08248, 2024. 4 [45] Drew Hudson and Christopher Manning. Gqa: new dataset for real-world visual reasoning and compositional question answering. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 67006709, 2019. 27 [46] Kushal Kafle, Brian Price, Scott Cohen, and Christopher Kanan. Dvqa: Understanding data visualizations via question answering. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 56485656, 2018. 27 [47] Samira Ebrahimi Kahou, Vincent Michalski, Adam Atkinson, Akos Kadar, Adam Trischler, and Yoshua Bengio. Figureqa: An annotated figure dataset for visual reasoning, 2018. 27 [48] Nikita Karaev, Ignacio Rocco, Benjamin Graham, Natalia Neverova, Andrea Vedaldi, and Christian Rupprecht. Cotracker: It is better to track together. arXiv:2307.07635, 2023. 5, [49] Siddharth Karamcheti, Suraj Nair, Ashwin Balakrishna, Percy Liang, Thomas Kollar, and Dorsa Sadigh. Prismatic vlms: Investigating the design space of visuallyconditioned language models. In International Conference on Machine Learning (ICML), 2024. 3 [50] Sahar Kazemzadeh, Vicente Ordonez, Mark Matten, and Tamara Berg. Referitgame: Referring to objects in photographs of natural scenes. In Proceedings of the 2014 conference on empirical methods in natural language processing (EMNLP), pages 787798, 2014. 27 worth dozen images. In Computer Vision ECCV 2016, pages 235251, Cham, 2016. Springer International Publishing. 27 [52] Aniruddha Kembhavi, Minjoon Seo, Dustin Schwenk, Jonghyun Choi, Ali Farhadi, and Hannaneh Hajishirzi. Are you smarter than sixth grader? textbook question answering for multimodal machine comprehension. In 2017 IEEE Conference on Computer Vision and Pattern Recognition (CVPR), pages 53765384, 2017. 27 [53] Geewook Kim, Teakgyu Hong, Moonbin Yim, JeongYeon Nam, Jinyoung Park, Jinyeong Yim, Wonseok Hwang, Sangdoo Yun, Dongyoon Han, and Seunghyun Park. OcrIn European free document understanding transformer. Conference on Computer Vision, pages 498517. Springer, 2022. 27 [54] Moo Jin Kim, Karl Pertsch, Siddharth Karamcheti, Ted Xiao, Ashwin Balakrishna, Suraj Nair, Rafael Rafailov, Ethan Foster, Pannag Sanketi, Quan Vuong, Thomas Kollar, Benjamin Burchfiel, Russ Tedrake, Dorsa Sadigh, Sergey Levine, Percy Liang, and Chelsea Finn. OpenVLA: An open-source vision-language-action model. In 8th Annual Conference on Robot Learning, 2024. 2, 3, 5, 7, 8, 9, [55] Wonkyun Kim, Changin Choi, Wonseok Lee, and Wonjong Rhee. An image grid can be worth video: Zeroshot video question answering using vlm. arXiv preprint arXiv:2403.18406, 2024. 12 [56] Alexander Kirillov, Eric Mintun, Nikhila Ravi, Hanzi Mao, Chloe Rolland, Laura Gustafson, Tete Xiao, Spencer Whitehead, Alexander Berg, Wan-Yen Lo, et al. Segment anything. arXiv preprint arXiv:2304.02643, 2023. 4 [57] Ranjay Krishna, Yuke Zhu, Oliver Groth, Justin Johnson, Kenji Hata, Joshua Kravitz, Stephanie Chen, Yannis Kalantidis, Li-Jia Li, David Shamma, et al. Visual Genome: Connecting language and vision using crowdsourced dense image annotations. International Journal of Computer Vision (IJCV), 123(1):3273, 2017. 27 [58] LAION-4V. Laion gpt4v-dataset, 2023. 27 [59] Bo Li*, Peiyuan Zhang*, Kaichen Zhang*, Fanyi Pu*, Xinrun Du, Yuhao Dong, Haotian Liu, Yuanhan Zhang, Ge Zhang, Chunyuan Li, and Ziwei Liu. Lmms-eval: Accelerating the development of large multimoal models, 2024. 11 [60] Bo Li, Yuanhan Zhang, Dong Guo, Renrui Zhang, Feng Li, Hao Zhang, Kaichen Zhang, Yanwei Li, Ziwei Liu, and Chunyuan Li. Llava-onevision: Easy visual task transfer. arXiv preprint arXiv:2408.03326, 2024. 5, 12 [61] Chunyuan Li, Cliff Wong, Sheng Zhang, Naoto Usuyama, Haotian Liu, Jianwei Yang, Tristan Naumann, Hoifung Poon, and Jianfeng Gao. Llava-med: Training large language-and-vision assistant for biomedicine in one day. arXiv preprint arXiv:2306.00890, 2023. 11 [62] Jiapeng Li, Ping Wei, Wenjuan Han, and Lifeng Fan. Intentqa: Context-aware video intent reasoning. In Int. Conf. Comput. Vis., pages 1196311974, 2023. 11 [51] Aniruddha Kembhavi, Mike Salvato, Eric Kolve, Minjoon Seo, Hannaneh Hajishirzi, and Ali Farhadi. diagram is [63] Kunchang Li, Yali Wang, Yinan He, Yizhuo Li, Yi Wang, Yi Liu, Zun Wang, Jilan Xu, Guo Chen, Ping Luo, Limin 18 Wang, and Yu Qiao. Mvbench: comprehensive multimodal video understanding benchmark, 2024. 11, 12 [64] Liunian Harold Li, Pengchuan Zhang, Haotian Zhang, Jianwei Yang, Chunyuan Li, Yiwu Zhong, Lijuan Wang, Lu Yuan, Lei Zhang, Jenq-Neng Hwang, et al. arXiv preprint Grounded language-image pre-training. arXiv:2112.03857, 2021. 4 [65] Xuanlin Li, Kyle Hsu, Jiayuan Gu, Karl Pertsch, Oier Mees, Homer Rich Walke, Chuyuan Fu, Ishikaa Lunawat, Isabel Sieh, Sean Kirmani, Sergey Levine, Jiajun Wu, Chelsea Finn, Hao Su, Quan Vuong, and Ted Xiao. Evaluating real-world robot manipulation policies in simulation. arXiv preprint arXiv:2405.05941, 2024. 8, 9, 26 [66] Xiang Li, Cristina Mata, Jongwoo Park, Kumara Kahatapitiya, Yoo Sung Jang, Jinghuan Shang, Kanchana Ranasinghe, Ryan Burgert, Mu Cai, Yong Jae Lee, et al. Llara: Supercharging robot learning data for vision-language policy. arXiv preprint arXiv:2406.20095, 2024. 3 [67] Yifan Li, Yifan Du, Kun Zhou, Jinpeng Wang, Xin Zhao, and Ji-Rong Wen. Evaluating object hallucination in large vision-language models. In Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing, pages 292305, Singapore, 2023. Association for Computational Linguistics. [68] Yi Li, Yuquan Deng, Jesse Zhang, Joel Jang, Marius Memme, Raymond Yu, Caelan Reed Garrett, Fabio Ramos, Dieter Fox, Anqi Li, Abhishek Gupta, and Ankit Goyal. Hamster: Hierarchical action models for open-world robot manipulation, 2025. 6 [69] Bin Lin, Bin Zhu, Yang Ye, Munan Ning, Peng Jin, and Li Yuan. Video-llava: Learning united visual represenarXiv preprint tation by alignment before projection. arXiv:2311.10122, 2023. 3, 12 [70] Fangchen Liu, Kuan Fang, Pieter Abbeel, and Sergey Levine. Moka: Open-vocabulary robotic manipulation arXiv preprint through mark-based visual prompting. arXiv:2403.03174, 2024. 4 [71] Haotian Liu, Chunyuan Li, Qingyang Wu, and Yong Jae Lee. Visual instruction tuning. NeurIPS, 2023. 2, 3, 5, 9, 27 [72] Haotian Liu, Chunyuan Li, Qingyang Wu, and Yong Jae arXiv preprint instruction tuning. Lee. arXiv:2304.08485, 2023. 11 Visual [73] Haotian Liu, Chunyuan Li, Yuheng Li, and Yong Jae Lee. Improved baselines with visual instruction tuning, 2024. 3 [74] Haotian Liu, Chunyuan Li, Yuheng Li, and Yong Jae Lee. Improved baselines with visual instruction tuning, 2024. 7 [75] Haotian Liu, Chunyuan Li, Yuheng Li, Bo Li, Yuanhan Zhang, Sheng Shen, and Yong Jae Lee. Llava-next: Improved reasoning, ocr, and world knowledge, 2024. 5, 9, 11, 25, 27 [76] Hao Liu, Wilson Yan, Matei Zaharia, and Pieter Abbeel. World model on million-length video and language with ringattention. arXiv preprint arXiv:2402.08268, 2024. 4 [77] Hao Liu, Wilson Yan, Matei Zaharia, and Pieter Abbeel. World model on million-length video and language with blockwise ringattention, 2024. 5 [78] Junpeng Liu, Tianyue Ou, Yifan Song, Yuxiao Qu, Wai Lam, Chenyan Xiong, Wenhu Chen, Graham Neubig, and Xiang Yue. Harnessing webpage uis for text-rich visual understanding, 2024. 10 [79] Junpeng Liu, Yifan Song, Bill Yuchen Lin, Wai Lam, Graham Neubig, Yuanzhi Li, and Xiang Yue. Visualwebbench: How far have multimodal llms evolved in web page understanding and grounding?, 2024. 8, 9 [80] Shilong Liu, Zhaoyang Zeng, Tianhe Ren, Feng Li, Hao Zhang, Jie Yang, Chunyuan Li, Jianwei Yang, Hang Su, Jun Zhu, et al. Grounding dino: Marrying dino with grounded pre-training for open-set object detection. arXiv preprint arXiv:2303.05499, 2023. 4 [81] Zhuang Liu, Hanzi Mao, Chao-Yuan Wu, Christoph Feichtenhofer, Trevor Darrell, and Saining Xie. convnet for the 2020s. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 11976 11986, 2022. 5, 7 [82] Pan Lu, Swaroop Mishra, Tanglin Xia, Liang Qiu, Kai-Wei Chang, Song-Chun Zhu, Oyvind Tafjord, Peter Clark, and Ashwin Kalyan. Learn to explain: Multimodal reasoning via thought chains for science question answering. Advances in Neural Information Processing Systems, 2022. 27 [83] Yadong Lu, Jianwei Yang, Yelong Shen, and Ahmed Awadallah. Omniparser for pure vision based gui agent, 2024. 3, 4, 6, 9, 10, 26, 27 [84] Yadong Lu, Jianwei Yang, Yelong Shen, and Ahmed Awadallah. Omniparser for pure vision based gui agent, 2024. [85] Kenneth Marino, Mohammad Rastegari, Ali Farhadi, and Roozbeh Mottaghi. Ok-vqa: visual question answering benchmark requiring external knowledge. In Proceedings of the IEEE/cvf conference on computer vision and pattern recognition, pages 31953204, 2019. 27 [86] Ahmed Masry, Do Long, Jia Qing Tan, Shafiq Joty, and Enamul Hoque. ChartQA: benchmark for question answering about charts with visual and logical reasoning. In Findings of the Association for Computational Linguistics: ACL 2022, pages 22632279, Dublin, Ireland, 2022. Association for Computational Linguistics. 5, 7 [87] Ahmed Masry, Do Xuan Long, Jia Qing Tan, Shafiq Joty, and Enamul Hoque. Chartqa: benchmark for question answering about charts with visual and logical reasoning, 2022. 27 [88] Joanna Materzynska, Tete Xiao, Roei Herzig, Huijuan Xu, Xiaolong Wang, and Trevor Darrell. Something-else: Compositional action recognition with spatial-temporal interaction networks. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 1049 1059, 2020. 24, 27 [89] Minesh Mathew, Dimosthenis Karatzas, and CV Jawahar. Docvqa: dataset for vqa on document images. In Proceedings of the IEEE/CVF winter conference on applications of computer vision, pages 22002209, 2021. 27 [90] Minesh Mathew, Viraj Bagal, Rub`en Tito, Dimosthenis Karatzas, Ernest Valveny, and CV Jawahar. Infographicvqa. In Proceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision, pages 16971706, 2022. 5, 7, [91] Russell Mendonca, Shikhar Bahl, and Deepak Pathak. arXiv Structured world models from human videos. preprint arXiv:2308.10901, 2023. 4 [92] Meta. Llama-3. https://ai.meta.com/blog/ meta-llama-3/, 2024. 10 [93] Anand Mishra, Shashank Shekhar, Ajeet Kumar Singh, and Anirban Chakraborty. Ocr-vqa: Visual question answering In 2019 international conferby reading text in images. ence on document analysis and recognition (ICDAR), pages 947952. IEEE, 2019. 27 [94] Soroush Nasiriany, Fei Xia, Wenhao Yu, Ted Xiao, Jacky Liang, Ishita Dasgupta, Annie Xie, Danny Driess, Ayzaan Wahid, Zhuo Xu, et al. Pivot: Iterative visual prompting elicits actionable knowledge for vlms. arXiv preprint arXiv:2402.07872, 2024. [95] Dantong Niu, Yuvan Sharma, Giscard Biamby, Jerome Quenum, Yutong Bai, Baifeng Shi, Trevor Darrell, and Roei Herzig. Llarva: Vision-action instruction tuning enhances robot learning. arXiv preprint arXiv:2406.11815, 2024. 3, 6 [96] OpenAI. Gpt-4v(ision) system card. https://cdn. openai.com/papers/GPTV_System_Card.pdf, 2023. 3 [97] OpenAI. ChatGPT. https://openai.com/blog/ chatgpt/, 2023. 3 [98] OpenAI. Gpt-4 technical report, 2023. 3, 10 [99] OpenAI. Gpt-4v(ision) system card. https://cdn. openai.com/papers/GPTV_System_Card.pdf, 2023. 9, 10 [100] Zhiliang Peng, Wenhui Wang, Li Dong, Yaru Hao, Shaohan Huang, Shuming Ma, and Furu Wei. Kosmos-2: Grounding multimodal large language models to the world. arXiv preprint arXiv:2306.14824, 2023. [101] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, et al. Learning transferable visual models from natural language supervision. In International Conference on Machine Learning (ICML), 2021. 3, 7 [102] Nikhila Ravi, Valentin Gabeur, Yuan-Ting Hu, Ronghang Hu, Chaitanya Ryali, Tengyu Ma, Haitham Khedr, Roman Radle, Chloe Rolland, Laura Gustafson, Eric Mintun, Junting Pan, Kalyan Vasudev Alwala, Nicolas Carion, ChaoYuan Wu, Ross Girshick, Piotr Dollar, and Christoph Feichtenhofer. Sam 2: Segment anything in images and videos, 2024. 6 [103] Christopher Rawles, Alice Li, Daniel Rodriguez, Oriana Riva, and Timothy Lillicrap. Android in the wild: largescale dataset for android device control, 2023. 3, 26, 27 [104] Christoph Schuhmann, Richard Vencu, Romain Beaumont, Robert Kaczmarczyk, Clayton Mullis, Aarush Katta, Theo Coombes, Jenia Jitsev, and Aran Komatsuzaki. Laion400m: Open dataset of clip-filtered 400 million image-text pairs. arXiv preprint arXiv:2111.02114, 2021. 4 [105] Dustin Schwenk, Apoorv Khandelwal, Christopher Clark, Kenneth Marino, and Roozbeh Mottaghi. A-okvqa: benchmark for visual question answering using world In European conference on computer vision, knowledge. pages 146162. Springer, 2022. 27 [106] ShareGPT. ShareGPT, 2023. 27 [107] Peter Shaw, Mandar Joshi, James Cohan, Jonathan Berant, Panupong Pasupat, Hexiang Hu, Urvashi Khandelwal, Kenton Lee, and Kristina Toutanova. From pixels to ui actions: Learning to follow instructions via graphical user interfaces, 2023. [108] Peter Shaw, Mandar Joshi, James Cohan, Jonathan Berant, Panupong Pasupat, Hexiang Hu, Urvashi Khandelwal, Kenton Lee, and Kristina Toutanova. From pixels to ui actions: Learning to follow instructions via graphical user interfaces. Advances in Neural Information Processing Systems, 36:3435434370, 2023. 2 [109] Gunnar Sigurdsson, Gul Varol, Xiaolong Wang, Ali Farhadi, Ivan Laptev, and Abhinav Gupta. Hollywood in homes: Crowdsourcing data collection for activity understanding. In Computer VisionECCV 2016: 14th European Conference, Amsterdam, The Netherlands, October 1114, 2016, Proceedings, Part 14, pages 510526. Springer, 2016. 27 [110] Amanpreet Singh, Vivek Natarajan, Meet Shah, Yu Jiang, Xinlei Chen, Dhruv Batra, Devi Parikh, and Marcus Rohrbach. Towards vqa models that can read. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 83178326, 2019. 8 [111] Srinivas Sunkara, Maria Wang, Lijuan Liu, Gilles Baechler, Yu-Chung Hsiao, Jindong Chen, Abhanshu Sharma, and James Stout. Towards better semantic understanding of mobile interfaces. CoRR, abs/2210.02663, 2022. 6, 27 [112] Reuben Tan, Ximeng Sun, Ping Hu, Jui-hsien Wang, Hanieh Deilamsalehy, Bryan Plummer, Bryan Russell, and Kate Saenko. Koala: Key frame-conditioned long video-llm. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 13581 13591, 2024. 3 [113] Octo Model Team, Dibya Ghosh, Homer Walke, Karl Pertsch, Kevin Black, Oier Mees, Sudeep Dasari, Joey Octo: Hejna, Tobias Kreiman, Charles Xu, et al. arXiv preprint An open-source generalist robot policy. arXiv:2405.12213, 2024. 9 [114] Shengbang Tong, Ellis Brown, Penghao Wu, Sanghyun Woo, Manoj Middepogu, Sai Charitha Akula, Jihan Yang, Shusheng Yang, Adithya Iyer, Xichen Pan, et al. Cambrian1: fully open, vision-centric exploration of multimodal llms. arXiv preprint arXiv:2406.16860, 2024. [115] Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timothee Lacroix, Baptiste Rozi`ere, Naman Goyal, Eric Hambro, Faisal Azhar, et al. Llama: Open and efficient foundation language models. arXiv preprint arXiv:2302.13971, 2023. 3 [116] Jiayu Wang, Yifei Ming, Zhenmei Shi, Vibhav Vineet, Xin Wang, Yixuan Li, and Neel Joshi. Is picture worth thousand words? delving into spatial reasoning for vision language models. arXiv preprint arXiv:2406.14852, 2024. 11 20 [117] Junyang Wang, Haiyang Xu, Jiabo Ye, Ming Yan, Weizhou Shen, Ji Zhang, Fei Huang, and Jitao Sang. Mobile-agent: Autonomous multi-modal mobile device agent with visual perception, 2024. 3 [118] Weihan Wang, Qingsong Lv, Wenmeng Yu, Wenyi Hong, Ji Qi, Yan Wang, Junhui Ji, Zhuoyi Yang, Lei Zhao, Xixuan Song, Jiazheng Xu, Bin Xu, Juanzi Li, Yuxiao Dong, Ming Ding, and Jie Tang. Cogvlm: Visual expert for pretrained language models, 2023. 10 [119] Xidong Wang, Dingjie Song, Shunian Chen, Chen Zhang, and Benyou Wang. Longllava: Scaling multi-modal llms to 1000 images efficiently via hybrid architecture. arXiv preprint arXiv:2409.02889, 2024. [120] Junbin Xiao, Xindi Shang, Angela Yao, and Tat-Seng Chua. Next-qa: Next phase of question-answering to explaining In Proceedings of the IEEE/CVF contemporal actions. ference on computer vision and pattern recognition, pages 97779786, 2021. 11 [121] Mingze Xu, Mingfei Gao, Zhe Gan, Hong-You Chen, Zhengfeng Lai, Haiming Gang, Kai Kang, and Afshin Slowfast-llava: strong training-free baseDehghan. arXiv preprint line for video large language models. arXiv:2407.15841, 2024. 12 [122] An Yan, Zhengyuan Yang, Wanrong Zhu, Kevin Lin, Linjie Li, Jianfeng Wang, Jianwei Yang, Yiwu Zhong, Julian McAuley, Jianfeng Gao, Zicheng Liu, and Lijuan Wang. Gpt-4v in wonderland: Large multimodal models for zeroshot smartphone gui navigation, 2023. 3 [123] An Yan, Zhengyuan Yang, Wanrong Zhu, Kevin Lin, Linjie Li, Jianfeng Wang, Jianwei Yang, Yiwu Zhong, Julian McAuley, Jianfeng Gao, et al. Gpt-4v in wonderland: Large multimodal models for zero-shot smartphone gui navigation. arXiv preprint arXiv:2311.07562, 2023. 4 [124] An Yang, Baosong Yang, Binyuan Hui, Bo Zheng, Bowen Yu, Chang Zhou, Chengpeng Li, Chengyuan Li, Dayiheng Liu, Fei Huang, Guanting Dong, Haoran Wei, Huan Lin, Jialong Tang, Jialin Wang, Jian Yang, Jianhong Tu, Jianwei Zhang, Jianxin Ma, Jianxin Yang, Jin Xu, Jingren Zhou, Jinze Bai, Jinzheng He, Junyang Lin, Kai Dang, Keming Lu, Keqin Chen, Kexin Yang, Mei Li, Mingfeng Xue, Na Ni, Pei Zhang, Peng Wang, Ru Peng, Rui Men, Ruize Gao, Runji Lin, Shijie Wang, Shuai Bai, Sinan Tan, Tianhang Zhu, Tianhao Li, Tianyu Liu, Wenbin Ge, Xiaodong Deng, Xiaohuan Zhou, Xingzhang Ren, Xinyu Zhang, Xipin Wei, Xuancheng Ren, Xuejing Liu, Yang Fan, Yang Yao, Yichang Zhang, Yu Wan, Yunfei Chu, Yuqiong Liu, Zeyu Cui, Zhenru Zhang, Zhifang Guo, and Zhihao Fan. Qwen2 technical report, 2024. 10 [125] Jianwei Yang, Hao Zhang, Feng Li, Xueyan Zou, Chunyuan Li, and Jianfeng Gao. Set-of-mark prompting unleashes extraordinary visual grounding in gpt-4v, 2023. [126] Jianwei Yang, Hao Zhang, Feng Li, Xueyan Zou, Chunyuan Li, and Jianfeng Gao. Set-of-mark prompting unleashes extraordinary visual grounding in gpt-4v. arXiv preprint arXiv:2310.11441, 2023. 4, 23 [127] Shunyu Yao, Jeffrey Zhao, Dian Yu, Nan Du, Izhak Shafran, Karthik Narasimhan, and Yuan Cao. React: Synergizing reasoning and acting in language models. arXiv preprint arXiv:2210.03629, 2022. 10 [128] Yuan Yao, Tianyu Yu, Ao Zhang, Chongyi Wang, Junbo Cui, Hongji Zhu, Tianchi Cai, Haoyu Li, Weilin Zhao, Zhihui He, et al. Minicpm-v: gpt-4v level mllm on your phone. arXiv preprint arXiv:2408.01800, 2024. 10 [129] Seonghyeon Ye, Joel Jang, Byeongguk Jeon, Sejune Joo, Jianwei Yang, Baolin Peng, Ajay Mandlekar, Reuben Tan, Yu-Wei Chao, Bill Yuchen Lin, Lars Liden, Kimin Lee, Jianfeng Gao, Luke Zettlemoyer, Dieter Fox, and Minjoon Seo. Latent action pretraining from videos, 2024. 3 [130] Seonghyeon Ye, Joel Jang, Byeongguk Jeon, Sejune Joo, Jianwei Yang, Baolin Peng, Ajay Mandlekar, Reuben Tan, Yu-Wei Chao, Bill Yuchen Lin, et al. Latent action pretraining from videos. arXiv preprint arXiv:2410.11758, 2024. 3 [131] Haoxuan You, Haotian Zhang, Zhe Gan, Xianzhi Du, Bowen Zhang, Zirui Wang, Liangliang Cao, Shih-Fu Ferret: Refer and ground Chang, and Yinfei Yang. arXiv preprint anything anywhere at any granularity. arXiv:2310.07704, 2023. 2 [132] Keen You, Haotian Zhang, Eldon Schoop, Floris Weers, Amanda Swearngin, Jeffrey Nichols, Yinfei Yang, and Zhe Gan. Ferret-ui: Grounded mobile ui understanding with multimodal llms, 2024. [133] Licheng Yu, Patrick Poirson, Shan Yang, Alexander Berg, and Tamara Berg. Modeling context in referring expresIn Computer VisionECCV 2016: 14th European sions. Conference, Amsterdam, The Netherlands, October 11-14, 2016, Proceedings, Part II 14, pages 6985. Springer, 2016. 27 [134] Hao Zhang, Hongyang Li, Feng Li, Tianhe Ren, Xueyan Jianfeng Gao, Lei Zou, Shilong Liu, Shijia Huang, Zhang, Chunyuan Li, and Jianwei Yang. Llava-grounding: Grounded visual chat with large multimodal models, 2023. 3 [135] Hang Zhang, Xin Li, and Lidong Bing. Video-llama: An instruction-tuned audio-visual language model for video understanding. arXiv preprint arXiv:2306.02858, 2023. 3 [136] Peiyuan Zhang, Kaichen Zhang, Bo Li, Guangtao Zeng, Jingkang Yang, Yuanhan Zhang, Ziyue Wang, Haoran Tan, Chunyuan Li, and Ziwei Liu. Long context transfer from language to vision. arXiv preprint arXiv:2406.16852, 2024. 12 [137] Shilong Zhang, Peize Sun, Shoufa Chen, Min Xiao, Wenqi Shao, Wenwei Zhang, Kai Chen, and Ping Luo. Gpt4roi: Instruction tuning large language model on region-ofinterest. arXiv preprint arXiv:2307.03601, 2023. 3 [138] Shaoqing Zhang, Zhuosheng Zhang, Kehai Chen, Xinbei Ma, Muyun Yang, Tiejun Zhao, and Min Zhang. Dynamic planning for LLM-based graphical user interface automation. In Findings of the Association for Computational Linguistics: EMNLP 2024, Miami, Florida, USA, 2024. Association for Computational Linguistics. 10 [139] Yuanhan Zhang, Bo Li, haotian Liu, Yong jae Lee, Liangke Gui, Di Fu, Jiashi Feng, Ziwei Liu, and Chunyuan Li. Llava-next: strong zero-shot video understanding model, 2024. 3, 27 [140] Boyuan Zheng, Boyu Gou, Jihyung Kil, Huan Sun, and Yu Su. Gpt-4v(ision) is generalist web agent, if grounded. arXiv preprint arXiv:2401.01614, 2024. 9 [141] Boyuan Zheng, Boyu Gou, Jihyung Kil, Huan Sun, and Yu Su. Gpt-4v(ision) is generalist web agent, if grounded, 2024. 3, 10 [142] Ruijie Zheng, Yongyuan Liang, Shuaiyi Huang, Jianfeng Gao, Hal Daume III, Andrey Kolobov, Furong Huang, and Jianwei Yang. Tracevla: Visual trace prompting enhances spatial-temporal awareness for generalist robotic policies. arXiv preprint arXiv:2412.10345, 2024. 3 [143] Yang Zheng, Adam Harley, Bokui Shen, Gordon Wetzstein, and Leonidas Guibas. Pointodyssey: large-scale synthetic dataset for long-term point tracking. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 1985519865, 2023. 6 [144] Luowei Zhou, Nathan Louis, and Jason Corso. Weaklysupervised video object grounding from text by loss weighting and object interaction. In British Machine Vision Conference, 2018. 6 [145] Deyao Zhu, Jun Chen, Xiaoqian Shen, Xiang Li, and Mohamed Elhoseiny. Minigpt-4: Enhancing vision-language understanding with advanced large language models. arXiv preprint arXiv:2304.10592, 2023. [146] Lianghui Zhu, Bencheng Liao, Qian Zhang, Xinlong Wang, Wenyu Liu, and Xinggang Wang. Vision mamba: Efficient visual representation learning with bidirectional state space model. arXiv preprint arXiv:2401.09417, 2024. 3 [147] Xueyan Zou, Jianwei Yang, Hao Zhang, Feng Li, Linjie Li, Jianfeng Gao, and Yong Jae Lee. Segment everything everywhere all at once. NeurIPS, 2023. 4 22 Magma: Foundation Model for Multimodal AI Agents"
        },
        {
            "title": "Supplementary Material",
            "content": "A. Pretraining and Finetuning Source Setting Pretraining batch size base learning rate learning rate scheduler training epochs optimizer Image Resolution Number of Crops 1024 1e-5 Constant 3 adamw 512 4 or 1 UI 32 1e-5 Cosine 3 adamw 768 4 Finetuning Image/Video Real Robot SeeClick-Web 1e-5 Cosine 1 adamw 768 4 or 1 1e-5 Constant 20 adamw 256 SeeClick-Mobile Table 9. Experimental settings pretraining and finetuning of Magma models. We maximally use either 32 Nvidia H100s or 64 AMD MI300 GPUs for all training jobs. Visison2UI Task text 2 point text 2 bbox point 2 text bbox 2 text text 2 point text 2 bbox UI summarization widget captioning input 2 point input 2 bbox text 2 point text 2 bbox point 2 text bbox 2 text Mixed Size 271K 54K 54K 54K 274K 56K 48K 42K 980K 982K 794K 774K 199K 193K 2.8M For all the model variants, we use the same training recipe as shown in Table 9. To handle different image resolutions from different datasets, we also use multicrop strategy to enable batch forward for given minibatch, though the ConvNext vision backbone can naturally support arbitrary resolutions. Specifically, for our pretraining, we use 512 as the base image size, and resize an input image maximally to 4 crops for UI and image pretraining data, while use 1 crop for video and robotics data. For downstream finetuning, we following common practice to tune the pretrained magma model as shown in Table 9 right. As mentioned above, the vision encoder can be effortlessly adapted to different image resolutions required for different tasks. B. Datasets B.1. Pretraining Data Due to space constraints, we briefly introduced the datasets for our pretraining in Sec 4.1 of our main submission. To ensure the reproducibility of our pretraining stage, we provide additional details of our pretraining data below. B.1.1. UI Navigation Our pretraining data related to UI agent are sourced from two datasets, SeeClick [19] and Vision2UI [41]. We further process these source data by adding marks on screenshots to provide grounded supervisions. SeeClick. We generally follow the original procedure and make the following modifications to associate with the Set of Mark [126] strategy. For each webpage screenshot, multiple (text, bounding box) pairs are available. Therefore, we directly overlay all the bounding boxes with corresponding marks on the screenshot. For each mobile screenshot, only Magma-PT-UI (Ours) Table 10. Statistics of UI related pretraining data. single (text, bounding box) pair is available in the SeeClick data. To enrich the pairs, we incorporate additional pairs from the RICO dataset [26], and employ an OCR tool to obtain text boxes. Finally, we display the enriched bounding boxes along with their corresponding marks on the mobile screenshot. Vision2UI. We consider all bounding boxes whose content property is not null. To prevent the marks from overwhelming the main content of the webpage, we sample bounding boxes with varying probabilities based on their type property. Specifically, we assign sampling weight of 0.5 to boxes of type h1, h2, a, button, option, and nav with 0.5, while other types are weighted at 0.1. Given the high importance of input areas for interaction, we include boxes of type input directly without sampling for mark plotting. After obtaining the elements of high interest, we apply similar tasks as SeeClick [19] to produce the instruction data, including (a) grounding task, which involves two forms: predicting center point coordinates (text 2 point) and predicting bounding box (text 2 bbox); (b) generating text for elements, categorized into predicting text based on the coordinates of center points (point 2 text) or bounding boxes (bbox 2 text); and further introduce the task of (C) locating input fields, including predicting center point coordinates (input 2 point) and bounding box coordinates (input 2 bbox) of the input fields. Given webpage, since the first two categories of tasks are grounding or generating texts for the same 23 Figure 12. Training samples in our Magma-PT-UI. It covers wide range of action grounding and UI understanding tasks including: (a) Given the bounding box or point coordinates as the query, assistant should return the natural language description or the content. (b) Given the natural language or the exact content as the query, assistant should return the value of the bounding box coordinates.. (c) Given the natural language as the query, assistant should return the value of the point coordinate. (d) Widget captioning. (e) UI summarization. group of web elements, we further weight the four subtasks, i.e., (text 2 point), (text 2 bbox), (point 2 text), and (bbox 2 text) with [0.4, 0.4, 0.1, 0.1], and sample only one of them to construct the pretraining data. Similarly, we sample one subtask from (input 2 point) and (input 2 bbox) with equal probabilities. We merge the sampled subtasks from the same webpage into one example to improve training efficiency. We denote the full pretraining data related to UI by Magma-PT-UI, and list the sizes of individual subsets in Table 10. B.1.2. Instructional Videos As mentioned in the main submission, we curate the supervisions from human instructional videos to learn the agentic capability for our model. To cover different scenarios, we considered both 3rd point view videos and egocentric videos. In particular, we start with Epic-Kitchen [24] video data sets considering that their text annotations are relatively high quality. Afterwards, we expand to SomethingSoomething v2 [88] to include more human-object interactions, and Ego4D [40] and other related instructional videos for scaling up. Epic-Kitchen [24]. Epic-Kitchen contains 495 egocentric videos recorded by 32 participants in kitchen rooms. Each video contains number of segments labeled with narrations, start and end frame ids. However, the original video narrations (e.g., open door) are too coarse to depict the human actions in certain time frame. For the videos in Epic-Kitchen, we apply the video preprocessing method as discussed in Sec 4.2 of our main submission. Concretely, for each of the original video segments in the dataset, we run PySceneDetect to detect the temporal boundaries and split them into sub-segments. During our model pretraining, the textual annotations are used in two ways. Our model is asked to predict the detailed description in the first frame. In addition, they are used as the task description as input to the model for predicting the traces of marks. Sth-Sth-v2 [88], Ego4D [40]. The Sth-Sth v2 dataset is comprehensive collection of labeled video clips featuring humans performing predefined actions with everyday objects. The list of action classes spans wide variety of atomic actions, including but not limited to pushing something from right to left, throwing something and covering something with something. In total, the dataset contains 220,847 seconds-long video clips. To create our pretraining data, we only leverage the videos in the train and validation splits. This amounts to around 160K video clips. We note that we do not use PySceneDetect for Sth-Sth v2 since the original video clips have been highly curated. The Ego4D dataset is large-scale egocentric dataset that contains approximately 3,025 hours of videos. It comprises over 3,670 hours of video footage captured from wearable cameras across diverse environments and activities. The dataset spans wide range of real-world scenarios, including daily activities and social interactions. Given the duration of these videos can span over 30 minutes, we leverage the original dense caption annotations that are provided to split each videos into seconds-long segments with consistent views. 24 Figure 13. Action distributions in three types of action-oriented pretraining datasets. (a) UI Navigation; (b) Robotic Manipulation; (c) Instructional Videos. B.1.3. Robotic Manipulation We follow the training recipe in OpenVLA [54] to prepare our pretraining data for robotics manipulation. Specifically, we take the data mixture siglip-224px+mx-oxemagic-soup as in OpenVLA, which gives us 9.4M imagelanguage-action triplets, extracted from 326K trajectories, from 23 separate datasets. B.1.4. Multimodal Image Understanding We simply include the 1.2M synthetic image-text pairs in ShareGPT4V [13] and 665K image instruction tuning data collected by LLaVA-1.5 [75] as our multimodal image pretraining data. The former helps our pretrained model to have global understanding of visual contents, while the latter helps to get the model familiar with various types of human instructions. We denote this dataset by Magma-PTImage. B.1.5. Data Statistics Given our goal of training general vision-language-action foundation model, we analyze the distribution of verbs present in the text annotations of the UI and robotic manipulation as well as instructional video datasets in Figure 13. We see that the text annotations in the UI navigation component contain many helpful verbs that help guide agents to achieve specific task such as locate and turn. This is complemented by the more action-oriented words in the vocabulary of the robot manipulation component, including pick, push and slide. Such annotations are especially valuable in helping our Magma model to learn to reason about interactions with everyday objects. Finally, we also scale up the amount of training data and diversity of verbs by including data from instructional videos (Figure 13c). As evidenced by the relatively high frequency of words such as lifting and throwing, such annotations can be very beneficial for gaining stronger understanding the of temporal dynamics involved in common activities. More importantly, the diversity of activities present in these datasets can be effective at helping the model generalize better to larger variety of tasks. B.2. Downstream Data B.2.1. UI Agent Navigation We evaluated the UI grounding and navigation capability mainly on three datasets, ScreenSpot [19], Mind2Web [27] 25 Figure 14. Real robot setup. Magma is deployed on WidowX 250 robot arm to perform sequence of kitchen manipulation tasks including object pick-place and soft manipulation. and AITW [103]. B.2.2. Robot Manipulation ScreenSpot is benchmark used to evaluate the UI action grounding proposed in [19]. It consists of 600 screenshots images associated with 1.2K instructions spanning iOS, Android, macOS, Windows, and web pages. The evalaution covers both text based elements and variety of widgets and icons. To evaluate the zero-shot action grounding performance for our model, we use OmniParser [83] to help parse the screenshot and propose actionable regions/icons/buttons. We used the sample code and default settings provided in the official repo. For these candidate regions, we overlay numeric marks and ask our model to pick one. Mind2Web is first proposed in [27] for text-based web agent. For fair comparison among vision-based web agent, we follow the protocol proposed in SeeClick [19]. Given webpage, we convert it into screenshot associated with ground-truth bounding boxes to which the actions should be applied. As the original screenshot of the full website is usually out of the scope of display. We follow similar way as in [19] to crop the region of interests centering around the ground truth boxes, which gives us local screenshot as wide as original webpage but with maximal height 1344. To propose the candidate marks for our model, we directly exploit the candidate ranks provided in Mind2Web, and use the top 30 candidates for evaluation. AITW is dataset originally collected in [103] for the android UI. The original dataset navigation of resulting in 5.7M contains up to 715K trajectories, screenshots. to examine the efIn our experiments, ficient finetuning performance, we alteratively follow the same protocol in SeeClick [19] and include much smaller number of training samples. Specifically, there are 545, 688, 306, 700, 700 instructions from General/Install/GoogleApps/Single/WebShopping, respectively. 80% of each split is used for training and the remainder is used for evaluation. Instead of finetuning our model for each category, we jointly finetune our pretrained Magma on the combined data and evaluate across all categories using single model. Simulator. We employ SimplerEnv [65] as the main testbed for our learned robot policy. As we do not need to tune our model on the simulated trajectories, we simply report the numbers following the protocol proposed in the original work. Real-world Setting. We design four tabletop manipulation tasks for our physical WidowX-250 robot setup as shown in 14. As with BridgeData-v2, the RGB image observations from the robot are captured using stationary third-person camera, maintaining resolution of 256 256. For finetuning our pretrained Magma model, we collect approximately 50 robot demonstration trajectories for each task as our finetuning dataset. Our experimental design includes classic soft object manipulation and pick-and-place operations tasks. Detailed language instructions for the designed tasks are presented below. For each trial, we randomize the initial location of the target object and include 2-3 random distracting objects (e.g., corn, eggplant) in the scene. For reproducibility, we release the collected robot trajectories. Tasks included in the finetuning dataset: Hot dog assembly: Pick up the hot dog sausage from the desk and place it into the bun. The trial is counted as success only when the robot successfully grasps the sausage and accurately places it within the hot dog bun. Mushroom placement: Pick up the mushroom and place it into the pot. The trial is counted as success only when the robot correctly grasps the mushroom and places it into the cooking pot without dropping or misaligning it. Cloth pushing: Push the cloth from right to left across the surface. The trial is counted as success only when the robot successfully manipulates the cloth in the specified direction without disturbing other objects on the surface. Unseen task for evaluating generalization: Bidirectional cloth manipulation: Push the cloth in both directions while maintaining its shape. This task examines the models spatial understanding and reasoning capabilities, as it requires generalization from unidirectional pushing in the training data to bidirectional manipulation in novel scenarios."
        },
        {
            "title": "For",
            "content": "evaluation Vision model. DoM Tree. In addition to the bounding boxes extracted from HTML code [19, 41], we further annotate the mobile screenshots in SeeClick data with bounding boxes derived from Android view hierarchies [111]. These annotations are used during our model pretraining. zero-shot on Screenspot [19], we exploit the OmniParser model [83] to make fair comparison with the state-of-the-art methods [19, 83]. Note that we only use the bounding boxes without local semantics. The original bounding boxes in AITW [103] are identified using an OCR model and IconNet [111]. Language model. For evaluation on As discussed earlier, we directly apply the predictions provided by Mind2Web [27] using pretrained language model DeBERTa-v3-base. This model gives approximately 85% recall@50. C. Qualitative Analysis C.1. UI Navigation Given the performant UI navigation performance across different tasks, we show some Mobile UI navigation samples in Fig. 15. We prompt the model to complete two daily tasks starting from the home page: Whats the weather like in Tokyo and Install app Instagram. Despite that our model is never trained with the full trajectory, it can handle the tasks in the wild pretty well. C.2. Robotics Manipulation We further show the real robot manipulation rollout for OpenVLA and Magma model. As discussed in our main paper, our model exhibits much better generalization ability to different real robot manipulation tasks. In Fig. 16, we qualitatively show how two models handle complicated task of Pick up the sausage and put it inside the hotdog. Thanks to the proposed pretraining techniques, our Magma model can not only precisely pick up the sausage but also move smoothly to the top of the hotdog, demonstrating superior spatial understanding and reasoning capability compared with the counterpart. Dataset ShareGPT [106] ShareGPT4V [13] LLaVA-Instruct [71] LAION-GPT4V [58] VQAv2 [39] GQA [45] OKVQA [105] OCRVQA [93] ChartQA [87] DVQA [46] DocVQA [89] AI2D [51] SynthDog-EN [53] A-OKVQA RefCOCO [133] VG [57] InfographicsVQA [90] ChartQA (Aug) [87] FigureQA [47] TQA [52] ScienceQA [82] Magma-SFT-Image (Ours) Size 40K 39K 158K 11K 83K 72K 9K 80K 7K 16K 10K 2K 20K 66K 48K 86K 24k 20k 20k 1.5k 5k 820k Domain Text General General General General VQA General VQA Knowledge VQA OCR VQA Chart VQA Chart VQA Document VQA Infographic VQA Document Understanding Knowledge VQA Grounding Desc. Referring Exp. Infographic VQA Chart VQA Chart/Figure VQA Textbook VQA Textbook VQA Mixed Table 11. detailed breakdown of our 820k Magma image instruction tuning data used in our multimodal image understanding experiments shown in Table 5 in our main submission. B.2.3. Image Instruction Tuning We show breakdown of our 820k Magma image instruction tuning data in Table 11. As the 760k image instruction tuning data used in LLaVA-1.6 [75] is not released, we follow their guidance to curate 748k public available data including ShareGPT [106], LLaVAInstruct [71], ShareGPT4V [13], LAION-GPT4V [58], VQAv2 [38], GQA [45], OKVQA [85], OCRVQA [93], ChartQA [87], DVQA [46], DocVQA [89], AI2D [51], SynthDog-EN [53], A-OKVQA [105], RefCOCO [50] and VG [57]. To complement the claimed improved reasoning, OCR and world knowledge, we resort to few other opensourced datasets including InfoGraphicsVQA [90], augmented ChartQA [87], FigureQA [47], TQA [52] and ScienceQA [82]. We denote the full set by Magma-SFT-Image. B.2.4. Video Instruction Tuning For comparisons with state-of-the-art video LMMs, we adopt the LLava-Video-178K dataset [139] for instruction tuning. It consists of approximately 1.6M video and text instruction samples from 178K videos. The dataset is compiled from multiple video sources ranging from Charades [109], Sth-SthV2 [88] to Kinetics-700 [9]. We refer interested readers to the original papers for more details. B.2.5. Details about SoM for training and evaluation we exploit three ways to extract the candidate bounding boxes for the SoM prompt: 27 Figure 15. Examples for mobile UI navigation sample. We prompt the model with two tasks: Whats the weather like in Tokyo and Install app Instagram. The model take actions sequentially given the new observation and history action information. 28 (a) Robot policy rollout for task Put the sausage to hotdog for OpenVLA model. (Failure) (b) Robot policy rollout for task Pick up the mushroom to the pot for OpenVLA model. (Failure) (c) Robot policy rollout for task Put the sausage to hotdog for Magma model. (Success) (d) Robot policy rollout for task Pick up the mushroom to the pot for Magma model. (Success) Figure 16. Comparison between OpenVLA (top two rows) and Magma (bottom two rows) for real robot manipulation task. The two robot policies starts with the same initial stage and asked to perform exactly the same task. The whole task requires precise spatial understanding and planning for the model. For both tasks, OpenVLA failed to accomplish while our model successfully handle."
        }
    ],
    "affiliations": [
        "KAIST",
        "Microsoft Research",
        "University of Maryland",
        "University of Washington",
        "University of Wisconsin-Madison"
    ]
}