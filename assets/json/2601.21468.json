{
    "paper_title": "MemOCR: Layout-Aware Visual Memory for Efficient Long-Horizon Reasoning",
    "authors": [
        "Yaorui Shi",
        "Shugui Liu",
        "Yu Yang",
        "Wenyu Mao",
        "Yuxin Chen",
        "Qi GU",
        "Hui Su",
        "Xunliang Cai",
        "Xiang Wang",
        "An Zhang"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Long-horizon agentic reasoning necessitates effectively compressing growing interaction histories into a limited context window. Most existing memory systems serialize history as text, where token-level cost is uniform and scales linearly with length, often spending scarce budget on low-value details. To this end, we introduce MemOCR, a multimodal memory agent that improves long-horizon reasoning under tight context budgets by allocating memory space with adaptive information density through visual layout. Concretely, MemOCR maintains a structured rich-text memory (e.g., headings, highlights) and renders it into an image that the agent consults for memory access, visually prioritizing crucial evidence while aggressively compressing auxiliary details. To ensure robustness across varying memory budgets, we train MemOCR with reinforcement learning under budget-aware objectives that expose the agent to diverse compression levels. Across long-context multi-hop and single-hop question-answering benchmarks, MemOCR outperforms strong text-based baselines and achieves more effective context utilization under extreme budgets."
        },
        {
            "title": "Start",
            "content": "6 2 0 2 9 2 ] . [ 1 8 6 4 1 2 . 1 0 6 2 : r MemOCR: Layout-Aware Visual Memory for Efficient Long-Horizon Reasoning Yaorui Shi 1 2 * Shugui Liu 1 Yu Yang 2 Wenyu Mao 1 Yuxin Chen 3 2 * Qi Gu 2 Hui Su 2 Xunliang Cai 2 Xiang Wang 1 An Zhang"
        },
        {
            "title": "Abstract",
            "content": "Long-horizon agentic reasoning necessitates effectively compressing growing interaction histories into limited context window. Most existing memory systems serialize history as text, where token-level cost is uniform and scales linearly with length, often spending scarce budget on low-value details. To this end, we introduce MemOCR, multimodal memory agent that improves long-horizon reasoning under tight context budgets by allocating memory space with adaptive information density through visual layout. Concretely, MemOCR maintains structured rich-text memory (e.g., headings, highlights) and renders it into an image that the agent consults for memory access, visually prioritizing crucial evidence while aggressively compressing auxiliary details. To ensure robustness across varying memory budgets, we train MemOCR with reinforcement learning under budget-aware objectives that expose the agent to diverse compression levels. Across long-context multi-hop and single-hop question-answering benchmarks, MemOCR outperforms strong text-based baselines and achieves more effective context utilization under extreme budgets. Our code is available at https:// github.com/syr-cn/MemOCR. 1. Introduction The evolution of large language models (LLMs) has empowered autonomous agents to tackle complex, long-horizon tasks that necessitate robust long-horizon reasoning (Luo et al., 2025; Du et al., 2025; Matarazzo & Torlone, 2025). However, as an agent accumulates extensive interaction history over its lifespan, the sheer volume of data inevitably *Work done during an internship at Meituan 1University of Science and Technology of China, Hefei, China 2Meituan, Beijing, China 3National University of Singapore, School of Computing, Singapore. Correspondence to: An Zhang <an zhang@ustc.edu.cn>, Qi Gu <guqi03@meituan.com>. Preprint. January 30, 2026. overwhelms the hard constraints of the context window, creating fundamental bottleneck (Vaswani et al., 2017; Hsieh et al., 2024; Modarressi et al., 2025). At the core of longhorizon reasoning is memory management under finite working context: agents must continually decide what past information to store and what to retrieve into the context window (Fang et al., 2025; Hu et al., 2025; Zhang et al., 2025b). This essentially constitutes budget allocation problem, where the objective is to maximize the density of task-relevant information within limited number of tokens (i.e., the memory budget) to support the current decision. Leading approaches generally construct the working context using textual forms, which can be categorized into two paradigms. Early works populate the context by retrieving and injecting raw historical segments (e.g., past chats) as uncompressed memory (Zhang et al., 2025a; Jin et al., 2025; Song et al., 2025). Specifically, the agent retrieves relevant passages and inserts the top-k snippets to fill the working context, as demonstrated in Figure 1(a). While this preserves original details, the retrieved snippets can be redundant or noisy, diluting information density and potentially exhausting the context budget (Shi et al., 2025b; Wu et al., 2025). Instead of storing raw information, recent works (Yu et al., 2025; Wang et al., 2025; Chhikara et al., 2025; Shi et al., 2025a) compress past interactions into compact textual summary, and maintain it via incremental updates or overwrites. In principle, summarization distills task-relevant information (i.e., concepts crucial for answering future queries) from noisy history, providing cleaner context to support decision-making. However, the textual memory paradigm suffers from an intrinsic limitation: linear token scaling. Even though summarization alleviates redundancy, representing memory as text tightly couples storage cost to information content retaining more auxiliary details or explanatory context inevitably requires proportionally more tokens (Feng et al., 2026; Fang et al., 2025; Sun et al., 2025). This coupling squanders the limited memory budget on non-critical supporting facts. As conceptually illustrated in Figure 1(b), text imposes uniform information density: to maintain 100 tokens of crucial information, the system is compelled to retain substantial volume of auxiliary details (depicted as 900 tokens), lacking the flexibility to selectively downMemOCR: Layout-Aware Visual Memory for Efficient Long-Horizon Reasoning sample less important context. 2. Preliminaries We propose paradigm shift from 1D textual memory to 2D visual memory, representing history as an image rather than token stream. The core benefit is adaptive information density: the agent can explicitly allocate the limited budget non-uniformly by controlling visual salience. Crucial evidence is rendered with prominent typography and high-visibility layout (e.g., headers, bold, larger font), while auxiliary details are compressed into visually smaller text. This allows the agent to pack substantially more content into far fewer visual tokens while keeping key evidence readable under aggressive compression (Figure 1(c)). The overall budget can be controlled by resolution manipulation (e.g., downsampling), providing flexible budget-fidelity tradeoff without changing the memory content (Wei et al., 2025). To this end, we introduce MemOCR, multimodal memory agent that improves long-horizon reasoning under tight context budgets by allocating memory space with adaptive information density through visual layout. As shown in Figure 2, MemOCR formulates and utilizes visual memory with two-stage memory lifecycle. (1) Memory Drafting (Text Domain): upon receiving new experience, the agent incrementally edits persistent rich-text memory, including both updating content and assigning visual priority via structure and formatting. These explicit salience cues determine how memory components compete for limited space, enabling non-uniform budget allocation. (2) Memory Reading (Vision Domain): lightweight renderer compiles the rich text into 2D memory image, which becomes the agents sole working context at query time. The agent then reads this memory image to produce an answer. We train MemOCR via RL with budget-aware training objectives that expose the agent to various memory budgets, forcing it to write crucial evidence highly visible under extreme budgets, while keeping auxiliary details in lower-priority regions. We evaluate MemOCR on both multi-hop (e.g., HotpotQA (Yang et al., 2018)) and single-hop (e.g., Natural Questions (Kwiatkowski et al., 2019)) question-answering (QA) benchmarks across various context lengths and memory budgets (cf. 4.2). Across settings, MemOCR outperforms text-memory agents when the budget is sufficient, and exhibits substantially smaller performance drops as the budget tightens, yielding roughly an 8 improvement in effective context utilization (cf. 4.3). Moreover, we find that visual salience is functionally important: weakening or removing layout-based emphasis directly harms robustness under low budgets, and MemOCR learns to place more important information in more visually accessible regions (cf. 4.4). Finally, ablation studies verify the contribution of budgetaware training objectives (cf. 4.5). Complexity analysis in Appendix D.2 further reveals visual memory does not introduce much computational overhead. In this section, we formalize context management for longhorizon agent reasoning and pinpoint the uniform information density issue in text-based memory paradigms. See Appendix for discussion of related work. 2.1. Problem Formulation We consider an LLM agent operating in persistent environment, where information arrives sequentially as discrete stream of text chunks = {C1, C2, . . . , CT }. At each step t, the agent receives new observation Ct. Given user question Q, the agents goal is to produce an accurate answer based on the cumulative history C. Raw History Memory. its generation on the full history C: Ideally, the agent would condition πθ( C, Q), (1) where πθ is the agent policy. However, as grows, the length of increases linearly and eventually exceeds the effective attention window of the underlying LLM. This makes raw-history conditioning suboptimal strategy for utilizing the finite context window. Textual Summary Memory. common remedy is to maintain compressed textual summary memory state Mt that serves as surrogate for {Ci}t i=1. Given question Q, the agent iteratively updates the memory Mt to retain information that is most relevant to answering Q: Mt πθ( Mt1, Ct, Q), {1, . . . , }. (2) After processing all chunks, the agent produces the answer conditioned on the final memory: πθ( MT , Q). (3) This formulation captures query-conditioned summarization workflow: memory is refined over time with respect to Q, and the final answer is generated from the latest memory state MT . 2.2. Memory Budget and Uniform Information Density We denote the context budget by (in tokens for text-based contexts), which constrains the working context available at inference time. For textual summary memory, the constraint is typically written as MT B. (4) We identify uniform information density as key bottleneck of text-serialized memory. In text-based memory, every token occupies the same unit of budget regardless of semantic 2 MemOCR: Layout-Aware Visual Memory for Efficient Long-Horizon Reasoning Figure 1. Comparison of memory paradigms. (a) Raw History Memory fetches relevant history passages but suffers from noise and redundancy. (b) Textual Summary Memory allows the agent to summarize the history but suffers from uniform information density, where auxiliary details (gray) consume as much token space as crucial information (green). (c) Visual Memory (Ours) allocates memory budget via visual layout to achieve adaptive information density. importance. Consequently, auxiliary details (e.g., background and explanations) impose rigid cost that competes directly with crucial evidence under the same token budget. This makes it difficult to allocate budget non-uniformly across memory components: keeping more supporting details inevitably reduces the space available for key evidence, even if those details are lower priority. This observation motivates different representation in which context cost is not tied to word count, to enable explicit control over how different memory components consume the budget. 3. Method: MemOCR In this section, we introduce MemOCR, multimodal memory agent that improves long-horizon reasoning under tight context budgets by allocating memory space with adaptive information density through visual layout. As shown in Figure 2, MemOCR formulates and utilizes visual memory with two-stage lifecycle: memory drafting in the text domain (3.1), followed by memory reading in the vision domain (3.2) after rendering. Finally, we present our budget-aware training objectives that train the agent to remain effective under different memory budgets (3.3). 3.1. Memory Drafting in the Text Domain The first stage corresponds to the text-domain drafting process (Figure 2(a)). Here, the agent functions as memory drafter that incrementally maintains persistent rich-text memory, denoted as RT . Unlike plain-text summaries, rich text explicitly encodes visual priority via structure and formatting (e.g., headings, indentation, bolding, font size), which later determines how different memory components compete for limited space on the canvas. Unless otherwise specified, we use Markdown as the carrier format. At step t, given the previous memory state RT new chunk Ct, the agent produces an updated memory: t1 and the RT πθ( RT t1, Ct), {1, . . . , }. (5) The role of drafting is to decide what to keep and, crucially, what to emphasize: important evidence is assigned higher visual priority (e.g., prominent headings or bold text), while auxiliary details are written in lower-priority regions. Importantly, the drafting process is budget-agnostic: the agent does not condition on the runtime memory budget when generating RT . Instead, it produces single rich-text memory whose internal salience structure enables non-uniform budget allocation after rendering. 3.2. Memory Reading in the Vision Domain The drafted rich-text memory is transformed into visual memory by lightweight renderer that bridges the text and vision domains: VT = R(M RT ), {1, . . . , }, (6) where VT is the rendered memory image at the final step. After rendering, memory cost is measured by the number of visual patch tokens rather than text length. In the memory image, layout and typography directly control information density: for text segment of length rendered at font scale s, the occupied area (and thus approximate visualtoken cost) scales as O(L s2). Therefore, rendering crucial evidence with larger scale and placing it in high-visibility regions, while rendering auxiliary details with smaller scale, decouples semantic content from context cost. This realizes adaptive information density: crucial evidence remains 3 MemOCR: Layout-Aware Visual Memory for Efficient Long-Horizon Reasoning Figure 2. Framework of MemOCR. (a) Memory Drafting (Text Domain): The LLM agent incrementally updates rich-text memory based on new incoming chunks, assigning visual priority via formatting and structure. (b) Memory Reading (Vision Domain): The rich text is rendered into 2D memory image, which serves as the agents sole working context for answering queries. (c) Budget-Aware Training Objectives: We train the agent under varying degrees of memory compression. The drafting ability is updated via aggregated advantages, while the reading ability is updated via separate advantages. readable under compression, while auxiliary details are compacted into lower-priority (and thus less visible) regions. The objective is to ensure global QA correctness when the visual memory is provided with sufficient tokens. At query time, MemOCR functions as memory reader in the vision domain (Figure 2(b)). The rendered image VT serves as the agents working context, and the answer is generated by: πθ( VT , Q). To control the effective memory budget, resolution manipulation (e.g., downsampling) can be applied to rendered image VT so that the resulting number of visual tokens does not exceed the budget. (7) 3.3. Budget-Aware Training Objectives Training MemOCR requires jointly optimizing drafting (text domain) and reading (vision domain). major obstacle is shortcut policy: without explicit constraints, the agent can place all information in uniform, medium-sized style, making everything similarly visible on the canvas and bypassing the intended trade-off between crucial evidence and auxiliary details. This collapses adaptive information density back into uniform density. To prevent this shortcut, we train MemOCR via Group Relative Policy Optimization (GRPO) (Shao et al., 2024) with budget-aware training objectives based on data augmentation. As illustrated in Figure 3, we construct three complementary QA tasks for the same drafted memory: 1. Standard QA (Tstd). We use the unmodified question from the training dataset and memory budget of 512 tokens. 2. QA w/ Augmented Memory (TaugM). We deliberately downsample the rendered memory image by 4 per dimension (i.e., 16 fewer pixels). Under severe compression, low-priority fine-grained details become illegible, while sufficiently prominent layout cues remain readable. This forces the drafter to assign enough visual priority to crucial evidence so that it survives resolution decay and remains retrievable under extreme budgets. 3. QA w/ Augmented Question (TaugQ). While crucial evidence must be salient, auxiliary details should not be discarded entirely. We therefore construct detail-oriented questions that target specific auxiliary information in the latest memory RT , and provide the uncompressed visual memory. The objective encourages the agent to be able to identify low-priority fine-grained details when explicitly queried, given sufficient tokens. Optimization with Reinforcement Learning. For each training instance, we sample group of outputs and compute task-specific rewards and advantages for the three scenarios above. Following Figure 2(c), the reading behavior is updated using separate task-specific advantages, since each scenario requires different visual reasoning behavior. In contrast, the drafting behavior must produce single layout that serves all scenarios; therefore, it is updated via MemOCR: Layout-Aware Visual Memory for Efficient Long-Horizon Reasoning 4.1. Experimental Setup Datasets. We train on HotpotQA (Yang et al., 2018) and pad each sample with distractor documents to reach 30K tokens during training. We evaluate on multi-hop HotpotQA and 2WikiMultiHopQA (2Wiki) (Ho et al., 2020), as well as single-hop Natural Questions (NQ) (Kwiatkowski et al., 2019) and TriviaQA (Joshi et al., 2017). During evaluation, contexts are padded to 10K/30K/100K tokens. We report subword exact match as accuracy, averaged over three runs. Baselines. We compare MemOCR against two categories of baselines: (1) Raw History Memory with uncompressed context, including the standard Qwen2.5-Instruct (Yang et al., 2024), the Qwen model distilled from DeepSeek-R1 (R1-Distill Qwen) (DeepSeek-AI et al., 2025) and Qwen2.51M-Instruct (Yang et al., 2025); (2) Textual Summary Memory, represented by Mem0 (Chhikara et al., 2025), Mem-α (Wang et al., 2025) and MemAgent (Yu et al., 2025). We use Qwen2.5-7B-Instruct (Yang et al., 2024) as the default backbone LLM for text-based methods, and Qwen2.5VL-7B-Instruct (Bai et al., 2025) for MemOCR. Memory Budget. We study long-context QA with an explicit memory budget constraint {16, 64, 256, 1024}, where the memory budget controls the number of tokens occupied in the context window at answer time. For textual summary agents, we constrain summary length by only keeping the first tokens in the latest memory state MT . For MemOCR, we adjust the resolution of the rendered memory image so that the number of visual patch tokens is no greater than B. Additional details are in Appendix C. Figure 3. Design of the budget-aware training objectives. (1) Standard QA uses the unmodified question and memory for global correctness. (2) QA w/ Augmented Memory requires the visibility of crucial evidence even when the visual memory is heavily compressed. (3) QA w/ Augmented Question ensures detailed information is clearly identified with sufficient tokens. The lowbudget, high-detail setting (gray area) is excluded as identifying detailed features under severe compression is infeasible. an aggregated advantage: = (cid:80) kK wk A(k) (cid:80) kK wk , (8) where = {Tstd, TaugM, TaugQ} and wk are pre-defined task weights. By maximizing this global signal, MemOCR learns layout strategy that keeps crucial evidence visible under extreme compression (via TaugM), with the ability to recover detailed information when budget permits (via TaugQ). 4.2. Overall Performance (RQ1) We compare MemOCR with baselines across datasets, context lengths (10K/30K/100K), and budgets, to test whether MemOCR improves overall performance and remains robust as the budget tightens. Table 1 reports the main results. 4. Experiments We evaluate MemOCR under long-horizon reasoning scenarios to answer four research questions (RQs): RQ1: Does MemOCR improve overall QA performance under long contexts and varying budgets? RQ2: Does layout control improve MemOCRs robustness under tight memory budgets? RQ3: Does layout induce region-wise robustness under compression, and does MemOCR exploit it to realize adaptive information density? RQ4: How do budget-aware training objectives contribute to the learned behavior? Obs. 1.1: MemOCR achieves the best overall performance across different context lengths. MemOCR attains the highest average accuracy from 10K to 100K contexts, indicating that MemOCR scales to long histories. For example, at 10K context with full budget, MemOCR reaches 74.6% average accuracy, surpassing the strongest textual baseline at 67.8%. Statistical significance is provided in Appendix D.1. We also observe poor performance on HotpotQA under 30K and 100K context lengths, and our analysis on bad cases helps explain this phenomenon in Appendix E. Obs. 1.2: MemOCR degrades more gracefully under tight budgets. Textual summary methods suffer catastrophic degradation as the budget tightens. For instance, on MemOCR: Layout-Aware Visual Memory for Efficient Long-Horizon Reasoning Table 1. Comparison of accuracy (%) across different context lengths (10K, 30K, and 100K tokens). Best results are highlighted in bold and second best in underline. The percentages in the Average column represent the performance drop relative to the 1024-token budget. Method Qwen2.5 R1-Distill Qwen Qwen2.5-1M Mem0 Mem-α MemAgent MemOCR (Ours) Memory Budget HotpotQA 2Wiki NQ TriviaQA Average (Tokens) 10K 30K 100K 10K 30K 100K 10K 30K 100K 10K 30K 100K 10K 30K 100K 100K 100K 100K 1024 256 64 16 1024 256 64 16 1024 256 64 16 1024 256 64 70.3 50.8 75.0 70.3 71.1 56.7 37.0 75.0 57.8 25.8 24.0 82.3 82.3 50.8 24.0 84.8 82.2 77.6 67.2 - 12.5 66. 66.1 65.6 60.9 43.3 77.9 57.3 29.7 30.2 78.9 76.8 52.3 26.6 75.1 75.4 68.1 57.9 - 0.8 68.8 64.1 63.8 56.2 34. 79.4 57.0 28.9 26.3 79.4 76.6 51.0 23.2 78.3 75.7 67.2 52.4 53.9 50.8 67.2 60.4 57.8 53.9 28.4 47.7 38.3 34.9 35. 65.4 66.1 38.5 28.4 72.2 71.2 62.9 57.9 Raw History Memory - 3.9 43.8 - 2.3 54.7 50.8 35.2 53.1 61.1 26.6 65. Textual Summary Memory 48.8 51.1 43.8 34.1 40.9 43.5 43.3 27.6 47.7 46.9 46.9 41.4 47.7 44.8 42.2 31.5 40.1 32.8 31.5 32.3 63.8 62.5 42.7 26. 73.7 72.8 66.2 56.0 50.0 41.7 39.1 40.1 61.2 58.1 36.2 16.7 45.6 47.9 27.1 23.7 53.4 51.3 48.7 24.5 Visual Memory 62.7 65.5 62.4 45. 61.8 57.3 51.0 42.8 44.8 33.6 25.0 26.3 46.1 44.3 36.5 20.1 49.2 48.8 43.6 35.9 - 1.6 50.8 48.2 49.7 50.0 43. 43.0 38.0 24.0 24.2 55.5 53.6 47.9 27.3 55.4 58.3 47.5 45.9 71.1 54.7 71.1 69.5 69.5 66.4 65.3 21.1 17.2 13.0 11. 70.1 69.5 64.6 49.7 79.6 79.7 77.6 80.8 - 11.7 77.3 70.6 70.6 65.9 62.8 23.2 24.2 19.5 19.0 78.1 77.6 69.5 56. 81.3 80.9 80.7 72.2 - 0.8 59.4 61.4 62.8 62.5 58.9 19.3 18.3 13.8 14.3 66.7 67.2 59.6 41.7 69.8 68.9 70.2 67. 61.5 47.9 66.6 - 13.7 63.3 - 1.4 58.4 62.0 61.3 (1.1%) 56.0 (9.7%) 43.0 (30.6%) 47.3 40.3 (14.9%) 25.2 (46.8%) 23.7 (50.0%) 67.8 67.3 (0.7%) 50.7 (25.3%) 31.6 (53.3%) 58.3 58.0 (0.5%) 53.2 (8.8%) 42.9 (26.4%) 46.5 37.0 (20.5%) 26.4 (43.2%) 27.0 (42.0%) 66.7 65.3 (2.1%) 50.3 (24.7%) 32.3 (51.6%) 53.6 55.0 (+2.5%) 53.0 (1.2%) 41.1 (23.4%) 47.9 38.7 (19.2%) 26.4 (44.9%) 26.2 (45.3%) 65.7 63.9 (2.8%) 48.7 (25.9%) 27.2 (58.6%) 74.6 72.6 (2.7%) 67.3 (9.8%) 62.2 (16.6%) 69.8 69.5 (0.5%) 64.7 (7.4%) 55.5 (20.5%) 66.6 67.1 (+0.8%) 60.7 (8.8%) 52.8 (20.7%) 10K contexts, MemAgent drops from 67.8% (1024 tokens) to 31.6% (16 tokens) on average. In contrast, MemOCR preserves 62.2% average accuracy at 16 tokens, corresponding to only 16.6% relative drop. These results suggest that MemOCR retains task-relevant evidence more effectively under severe budget constraints. Obs. 1.3: On single-hop tasks, low memory budget can be sufficient for sparse evidence. On NQ and TriviaQA, tightening the budget does not necessarily reduce accuracy. For example, on TriviaQA (10K context), MemOCR achieves 80.8% at 16 tokens, even higher than 79.6% at 1024 tokens. We attribute this to single-hop questions relying on atomic evidence, where lower-resolution memory can still preserve the critical cues while filtering background noise. This phenomenon is not observed for textual baselines, whose performance decreases with smaller budgets. 4.3. Analysis on Visual Robustness (RQ2) To identify the source of MemOCRs low-budget robustness, we compare against (i) MemOCR w/o Visual Layout, which preserves the visual modality but removes all formatting cues, and (ii) all three textual baselines on HotpotQA with 10K context. Figure 4 reports both accuracy and relative drop from the 1024-token setting. Figure 4. Comparison of accuracy and relative performance drop across varying memory budgets (RQ2). MemOCR degrades more gracefully than textual baselines as budgets tighten. Without visual layout, MemOCRs low-budget robustness drops significantly, which suggests that adaptive information density facilitates more efficient memory budget utilization. This additional drop indicates that MemOCRs robustness primarily comes from the layout-guided allocation of memory budgets, rather than the visual modality alone. Obs. 2.1: Visual layout significantly enhances low-budget robustness. As the budget tightens, MemOCR degrades most gracefully, with substantially smaller drops than all baselines. Removing visual layout causes marked additional drop, especially as the memory budget goes down. Obs. 2.2: MemOCR achieves an 8 token-efficiency gain at extreme budgets. At 8 tokens, MemOCR attains comparable accuracy to the baselines at 64 tokens, corresponding to an 8 reduction in memory tokens (648) for similar performance. MemOCR: Layout-Aware Visual Memory for Efficient Long-Horizon Reasoning Table 2. Ablation study by progressively removing training objectives from MemOCR. Vanilla MemOCR uses Tstd, TaugM, TaugQ. Method MemOCR w/o TaugM w/o TaugM, TaugQ w/o Tstd, TaugM, TaugQ Memory Budget HotpotQA 2Wiki NQ TriviaQA Average (Tokens) 10K 30K 100K 10K 30K 100K 10K 30K 100K 10K 30K 100K 10K 30K 100K 1024 256 64 16 1024 256 64 16 1024 256 64 16 1024 256 64 16 84.8 82.2 77.6 67.2 75.2 67.3 57.7 43. 74.7 70.6 44.6 28.4 67.2 62.5 62.5 33.6 75.1 75.4 68.1 57.9 71.7 65.8 52.2 40.9 68.5 67.5 39.0 28.8 54.7 51.6 50.8 30. 78.3 75.7 67.2 52.4 68.9 63.7 50.8 39.8 71.1 64.3 46.9 27.8 41.4 46.9 42.2 28.9 72.2 71.2 62.9 57.9 53.4 51.4 38.6 37. 53.9 54.2 40.7 33.8 51.6 46.9 47.7 34.4 73.7 72.8 66.2 56.0 55.5 53.0 41.7 34.8 57.7 53.0 39.6 37.0 45.3 46.9 43.0 25. 62.7 65.5 62.4 45.7 51.1 49.7 39.0 27.4 47.8 44.3 32.9 27.4 38.3 31.3 32.8 28.1 61.8 57.3 51.0 42.8 49.2 47.8 35.5 29. 50.8 42.9 29.0 24.9 45.3 46.9 46.9 27.3 49.2 48.8 43.6 35.9 46.5 44.2 35.0 24.4 38.8 39.8 22.4 17.9 40.6 36.7 36.7 25. 55.4 58.3 47.5 45.9 47.8 47.0 36.7 31.3 42.3 48.7 29.6 24.7 43.8 42.2 39.8 28.1 79.6 79.7 77.6 80.8 67.6 66.6 62.1 54. 69.3 67.7 50.1 44.5 62.5 71.1 66.4 54.7 81.3 80.9 80.7 72.2 74.2 75.1 66.3 59.4 72.5 69.7 57.0 46.3 68.0 70.3 67.2 60. 69.8 68.9 65.8 67.2 57.0 57.9 55.3 45.9 63.6 58.9 46.0 40.9 47.7 58.6 53.1 46.9 74.6 72.6 (2.7%) 67.3 (9.8%) 62.2 (16.6%) 61.3 58.3 (5.0%) 48.5 (21.0%) 41.2 (32.8%) 62.2 58.8 (5.4%) 41.1 (33.9%) 32.9 (47.1%) 56.6 56.8 (+0.3%) 55.9 (1.4%) 37.5 (33.8%) 69.8 69.5 (0.5%) 64.7 (7.4%) 55.5 (20.5%) 62.0 59.5 (3.9%) 48.8 (21.2%) 39.9 (35.6%) 59.4 57.5 (3.2%) 39.5 (33.5%) 32.5 (45.2%) 52.1 51.4 (1.5%) 49.4 (5.2%) 35.2 (32.6%) 66.6 67.1 (+0.8%) 60.7 (8.8%) 52.8 (20.7%) 56.2 54.6 (2.9%) 45.4 (19.2%) 36.1 (35.8%) 56.2 54.0 (3.9%) 38.9 (30.9%) 30.2 (46.3%) 42.8 44.7 (+4.6%) 42.0 (1.8%) 33.0 (22.8%) Figure 6. RL induces adaptive information density (RQ3). (a) With training, ground-truth evidence becomes more concentrated in the crucial region while decreasing in the detailed region. (b) The crucial region remains much shorter than the detail part. (Detailed), and the advantage grows as the budget tightens, indicating that information placed in lower-visibility regions is more likely to be lost under compression. In some extreme-budget cases (e.g., HotpotQA at 16 tokens), injection can be harmful because the added content increases the memory image size, making it less legible overall. Obs. 3.2: RL enables adaptive information density. Without RL, crucial and detailed regions have similar evidence density. As shown in Figure 6(a), MemOCR shifts precise evidence into the crucial region (precision 1.8) and reduces density in the detailed region (precision 0.46) during RL training. Meanwhile, the crucial region stays orders of magnitude shorter (Figure 6(b)). This demonstrates adaptive information density: key evidence is compactly preserved in visually high-priority areas, while details allocated to lower-priority regions. 4.5. Ablation Study over Training Objectives (RQ4) We ablate MemOCR by progressively removing training objectives. Specifically, we compare vanilla MemOCR trained with full objectives (Tstd+TaugM+TaugQ) with three variants: (1) w/o TaugM (2) w/o TaugM, TaugQ, and (3) w/o Tstd, TaugM, Figure 5. Oracle analysis of layout regions (RQ3). We compare MemOCR with oracle variants that inject ground-truth evidence into either the Crucial or Detailed region of the rendered memory. While both injections improve accuracy, injecting into the crucial region yields larger gains, especially under tight memory budgets. 4.4. Mechanism Verification (RQ3) To answer RQ3, we verify two-step mechanism of layout control: (1) region-wise robustnessdifferent layout regions are not equally robust under visual compression, and (2) evidence placementMemOCR exploits this asymmetry and stores important evidence into more visible regions. We evaluate both under the 30K-context setting using oracle injections (Figure 5) and memory statistics (Figure 6). Obs. 3.1: Evidence is more compression-robust in more visible regions. We construct two oracle controls by injecting the same ground-truth evidence into different regions of MemOCR memories: Oracle (Crucial) inserts it into the crucial region (H1 headers), whereas Oracle (Detailed) inserts it into the detailed region (plain body text). As shown in Fig. 5, Oracle (Crucial) consistently outperforms Oracle 7 MemOCR: Layout-Aware Visual Memory for Efficient Long-Horizon Reasoning Figure 7. Case study at an extreme memory budget (16 tokens). (Left) The textual baseline fails due to hard truncation of the context. (Middle) MemOCR without layout control fails because uniform text becomes unreadable after down-sampling. (Right) MemOCR preserves the crucial evidence Gene MacLellan through adaptive layout, enabling correct reasoning even at low resolution. TaugQ. Results are reported in Table 2. Obs. 4.1: Training is necessary for budget-robust memory layout. Across datasets and context lengths, simply removing all the training signals underperforms substantially. This indicates that the desired layout arrangement behavior does not emerge reliably without explicit training. Obs. 4.2: Robustness gains accumulate with multiple training signals. Using Tstd alone yields the weakest trained variant, which reflects limited ability to prioritize question-relevant evidence through layout control. Adding TaugQ improves low-budget robustness, suggesting that the diverse queries strengthen evidence utilization. Further adding TaugM brings larger boost in the low-budget regime, consistent with the concept of learning more effective layout control and memory budget allocation. 4.6. Case Study To provide an intuitive illustration of the robustness trends in RQ2 (4.3), we visualize the memory states of three agents answering the question Who wrote put your hand in the hand...? under an extreme 16-token budget (Figure 7). MemOCR w/o Layout fails because uniform rendering becomes unreadable after downsampling. As shown in the middle panel of Figure 7, since the visual layout control via rich-text grammar is disabled, the memory image is rendered as dense uniform text block without priority. When downsampled to 16 visual tokens (approximately 12K pixels in Qwen2.5-VL), the resolution becomes too low to resolve characters on the memory image, ultimately leading to an incorrect answer (i.e., Greg Brown). MemOCR succeeds by keeping crucial evidence legible via adaptive layout. As shown in the rightmost panel of Figure 7, the agent isolates the crucial information (Ocean Band and Gene MacLellan) into visually prominent regions. Even after aggressive downsampling blurs surrounding auxiliary text, the pixels corresponding to the crucial evidence remain recognizable, enabling correct answering. 5. Conclusion and Future Work In this paper, we shift agentic memory management from linear textual streams to flexible, spatial 2D canvas, termed visual memory. Building on this concept, we propose MemOCR, which dynamically manipulates visual layout and resolution to decouple information density from token cost, yielding strong robustness under extreme budget constraints. Textual summary memory fails under extreme budgets due to truncation. For the textual baseline (MemAgent), the 16-token limit necessitates hard truncation of the memory. As result, the critical entity Gene MacLellan is removed from the context window, leaving insufficient evidence for correct answering. For future work, natural next step is to generalize visual memory beyond QA to broader long-horizon agent settings, such as planning and tool-augmented reasoning, and to study long-term stability under lifelong updates. We also plan to improve the budget allocation policy by introducing more flexible rich-text formats (e.g., HTML). 8 MemOCR: Layout-Aware Visual Memory for Efficient Long-Horizon Reasoning"
        },
        {
            "title": "Impact Statement",
            "content": "This paper aims to advance long-horizon agentic reasoning by introducing visual-memory paradigm that represents interaction history on 2D canvas and allocates limited context budgets via adaptive visual compression. By improving effective context utilization, our approach may enable more long-horizon multimodal agents. At the same time, stronger long-horizon memory can amplify existing ethical and societal risks. First, agentic memory may increase privacy and security concerns if sensitive user information is stored, rendered, or inadvertently exposed through model outputs or logs. Second, the visual memory system may encourage harms in high-stakes domains since many of modern LLM-safety solutions are designed for text-only interactions. Third, visual rendering and OCR-style reading may introduce new failure modes, which could lead to hallucinated responses if the system cannot reliably recover key evidence. These risks are not unique to our method but can be amplified by improved memory capacity. Mitigations include adopting strict data retention policies, obtaining user consent and increasing transparency of the memory system with access control and encryption, and evaluating robustness and bias across demographics and domains."
        },
        {
            "title": "References",
            "content": "Arlazarov, V., Andreeva, E., Bulatov, K., Nikolaev, D., Petrova, O., Savelev, B., and Slavin, O. Document image analysis and recognition: survey. Computer Optics, 46 (4):567589, 2022. Bai, S., Chen, K., Liu, X., Wang, J., Ge, W., Song, S., Dang, K., Wang, P., Wang, S., Tang, J., et al. Qwen2. 5-vl technical report. arXiv preprint arXiv:2502.13923, 2025. Bai, Y., Kadavath, S., Kundu, S., Askell, A., Kernion, J., Jones, A., Chen, A., Goldie, A., Mirhoseini, A., McKinnon, C., et al. Constitutional ai: Harmlessness from ai feedback. arXiv preprint arXiv:2212.08073, 2022. Bian, H., Yao, Z., Hu, S., Xu, Z., Zhang, S., Guo, Y., Yang, Z., Han, X., Wang, H., and Chen, R. Realmem: Benchmarking llms in real-world memory-driven interaction. arXiv preprint arXiv:2601.06966, 2026. Cheng, J., Liu, Y., Zhang, X., Fei, Y., Hong, W., Lyu, R., Wang, W., Su, Z., Gu, X., Liu, X., et al. Glyph: Scaling context windows via visual-text compression. arXiv preprint arXiv:2510.17800, 2025. Chhikara, P., Khant, D., Aryan, S., Singh, T., and Yadav, D. Mem0: Building production-ready ai agents with scalable long-term memory. arXiv preprint arXiv:2504.19413, 2025. DeepSeek-AI, Guo, D., Yang, D., Zhang, H., Song, J., Zhang, R., Xu, R., Zhu, Q., Ma, S., Wang, P., Bi, X., Zhang, X., Yu, X., Wu, Y., Wu, Z. F., Gou, Z., Shao, Z., Li, Z., Gao, Z., Liu, A., Xue, B., Wang, B., Wu, B., Feng, B., Lu, C., Zhao, C., Deng, C., Zhang, C., Ruan, C., Dai, D., Chen, D., Ji, D., Li, E., Lin, F., Dai, F., Luo, F., Hao, G., Chen, G., Li, G., Zhang, H., Bao, H., Xu, H., Wang, H., Ding, H., Xin, H., Gao, H., Qu, H., Li, H., Guo, J., Li, J., Wang, J., Chen, J., Yuan, J., Qiu, J., Li, J., Cai, J. L., Ni, J., Liang, J., Chen, J., Dong, K., Hu, K., Gao, K., Guan, K., Huang, K., Yu, K., Wang, L., Zhang, L., Zhao, L., Wang, L., Zhang, L., Xu, L., Xia, L., Zhang, M., Zhang, M., Tang, M., Li, M., Wang, M., Li, M., Tian, N., Huang, P., Zhang, P., Wang, Q., Chen, Q., Du, Q., Ge, R., Zhang, R., Pan, R., Wang, R., Chen, R. J., Jin, R. L., Chen, R., Lu, S., Zhou, S., Chen, S., Ye, S., Wang, S., Yu, S., Zhou, S., Pan, S., and Li, S. S. Deepseek-r1: Incentivizing reasoning capability in llms via reinforcement learning. CoRR, abs/2501.12948, 2025. doi: 10.48550/ARXIV.2501.12948. Du, Y., Huang, W., Zheng, D., Wang, Z., Montella, S., Lapata, M., Wong, K.-F., and Pan, J. Z. Rethinking memory in ai: Taxonomy, operations, topics, and future directions. arXiv preprint arXiv:2505.00675, 2025. Duverger, R., Bonnin, A., Granier, R., Marolleau, Q., Blanchard, C., Zahzam, N., Bidel, Y., Cadoret, M., Bresson, A., and Schwartz, S. Metrology of microwave fields based on trap-loss spectroscopy with cold rydberg atoms. Physical Review Applied, 22(4):044039, 2024. Fang, J., Deng, X., Xu, H., Jiang, Z., Tang, Y., Xu, Z., Deng, S., Yao, Y., Wang, M., Qiao, S., Chen, H., and Zhang, N. Lightmem: Lightweight and efficient memory-augmented generation. CoRR, abs/2510.18866, 2025. URL https://doi.org/10. 48550/arXiv.2510.18866. Feng, L., Yang, F., Chen, F., Cheng, X., Xu, H., Wan, Z., Yan, M., and An, B. Agentocr: Reimagining agent history via optical self-compression. arXiv preprint arXiv:2601.04786, 2026. Ho, X., Nguyen, A.-K. D., Sugawara, S., and Aizawa, A. Constructing multi-hop qa dataset for comprehensive evaluation of reasoning steps. In Proceedings of the 28th International Conference on Computational Linguistics, pp. 66096625, 2020. Hsieh, C.-P., Sun, S., Kriman, S., Acharya, S., Rekesh, D., Jia, F., Zhang, Y., and Ginsburg, B. Ruler: Whats the real context size of your long-context language models? arXiv preprint arXiv:2404.06654, 2024. Hu, S., Wei, Y., Ran, J., Yao, Z., and Zou, L. Does memory need graphs? unified framework and empirical 9 MemOCR: Layout-Aware Visual Memory for Efficient Long-Horizon Reasoning analysis for long-term dialog memory. arXiv preprint arXiv:2601.01280, 2026. Hu, Y., Liu, S., Yue, Y., Zhang, G., Liu, B., Zhu, F., Lin, J., Guo, H., Dou, S., Xi, Z., et al. Memory in the age of ai agents. arXiv preprint arXiv:2512.13564, 2025. Jin, B., Zeng, H., Yue, Z., Wang, D., Zamani, H., and Han, J. Search-r1: Training llms to reason and leverage search engines with reinforcement learning. arXiv preprint arXiv:2503.09516, 2025. Joshi, M., Choi, E., Weld, D. S., and Zettlemoyer, L. Triviaqa: large scale distantly supervised challenge dataset for reading comprehension. In Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pp. 16011611, 2017. Kaelbling, L. P., Littman, M. L., and Moore, A. W. Reinforcement learning: survey. Journal of artificial intelligence research, 4:237285, 1996. Kwiatkowski, T., Palomaki, J., Redfield, O., Collins, M., Parikh, A., Alberti, C., Epstein, D., Polosukhin, I., Devlin, J., Lee, K., et al. Natural questions: benchmark for question answering research. Transactions of the Association for Computational Linguistics, 7:453466, 2019. Li, Z., Song, S., Xi, C., Wang, H., Tang, C., Niu, S., Chen, D., Yang, J., Li, C., Yu, Q., et al. Memos: memory os for ai system. arXiv preprint arXiv:2507.03724, 2025. Luo, J., Zhang, W., Yuan, Y., Zhao, Y., Yang, J., Gu, Y., Wu, B., Chen, B., Qiao, Z., Long, Q., et al. Large language model agent: survey on methodology, applications and challenges. arXiv preprint arXiv:2503.21460, 2025. Matarazzo, A. and Torlone, R. survey on large language models with some insights on their capabilities and limitations. arXiv preprint arXiv:2501.04040, 2025. Modarressi, A., Deilamsalehy, H., Dernoncourt, F., Bui, T., Rossi, R. A., Yoon, S., and Schutze, H. Nolima: Long-context evaluation beyond literal matching. arXiv preprint arXiv:2502.05167, 2025. Ouyang, L., Wu, J., Jiang, X., Almeida, D., Wainwright, C., Mishkin, P., Zhang, C., Agarwal, S., Slama, K., Ray, A., et al. Training language models to follow instructions with human feedback. Advances in neural information processing systems, 35:2773027744, 2022. Schulman, J., Moritz, P., Levine, S., Jordan, M., and Abbeel, P. High-dimensional continuous control using generalized advantage estimation. arXiv preprint arXiv:1506.02438, 2015. Schulman, J., Wolski, F., Dhariwal, P., Radford, A., and Klimov, O. Proximal policy optimization algorithms. arXiv preprint arXiv:1707.06347, 2017. Shao, Z., Wang, P., Zhu, Q., Xu, R., Song, J., Bi, X., Zhang, H., Zhang, M., Li, Y., Wu, Y., et al. Deepseekmath: Pushing the limits of mathematical reasoning in open language models. arXiv preprint arXiv:2402.03300, 2024. Shi, Y., Chen, Y., Wang, S., Li, S., Cai, H., Gu, Q., Wang, X., and Zhang, A. Look back to reason forward: Revisitable memory for long-context llm agents. arXiv preprint arXiv:2509.23040, 2025a. Shi, Y., Li, S., Wu, C., Liu, Z., Fang, J., Cai, H., Zhang, A., and Wang, X. Search and refine during think: Facilitating knowledge refinement for improved retrieval-augmented reasoning. In The Thirty-ninth Annual Conference on Neural Information Processing Systems, 2025b. Smith, R. An overview of the tesseract ocr engine. In Ninth international conference on document analysis and recognition (ICDAR 2007), volume 2, pp. 629633. IEEE, 2007. Song, H., Jiang, J., Min, Y., Chen, J., Chen, Z., Zhao, W. X., Fang, L., and Wen, J.-R. R1-searcher: Incentivizing the search capability in llms via reinforcement learning. arXiv preprint arXiv:2503.05592, 2025. Sun, W., Lu, M., Ling, Z., Liu, K., Yao, X., Yang, Y., and Chen, J. Scaling long-horizon llm agent via contextfolding, 2025. Vaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A. N., Kaiser, Ł., and Polosukhin, I. Attention is all you need. Advances in neural information processing systems, 30, 2017. Wang, Y., Takanobu, R., Liang, Z., Mao, Y., Hu, Y., McAuley, J., and Wu, X. Mem-{alpha}: Learning memory construction via reinforcement learning. arXiv preprint arXiv:2509.25911, 2025. Wei, H., Sun, Y., and Li, Y. Deepseek-ocr: Contexts optical compression. arXiv preprint arXiv:2510.18234, 2025. Wu, X., Li, K., Zhao, Y., Zhang, L., Ou, L., Yin, H., Zhang, Z., Yu, X., Zhang, D., Jiang, Y., Xie, P., Huang, F., Cheng, M., Wang, S., Cheng, H., and Zhou, J. Resum: Unlocking long-horizon search intelligence via context summarization, 2025. Xing, L., Wang, A. J., Yan, R., Shu, X., and Tang, J. Visioncentric token compression in large language model. arXiv preprint arXiv:2502.00791, 2025. MemOCR: Layout-Aware Visual Memory for Efficient Long-Horizon Reasoning Xue, Z., Zheng, L., Liu, Q., Li, Y., Zheng, X., Ma, Z., and An, B. Simpletir: End-to-end reinforcement learning for multi-turn tool-integrated reasoning. arXiv preprint arXiv:2509.02479, 2025. Yang, A., Yang, B., Zhang, B., Hui, B., Zheng, B., Yu, B., Li, C., Liu, D., Huang, F., Wei, H., et al. Qwen2. 5 technical report. arXiv preprint arXiv:2412.15115, 2024. Yang, A., Yu, B., Li, C., Liu, D., Huang, F., Huang, H., Jiang, J., Tu, J., Zhang, J., Zhou, J., et al. Qwen2. 5-1m technical report. arXiv preprint arXiv:2501.15383, 2025. Yang, Z., Qi, P., Zhang, S., Bengio, Y., Cohen, W., Salakhutdinov, R., and Manning, C. D. Hotpotqa: dataset for diverse, explainable multi-hop question answering. In Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, pp. 23692380, 2018. Yu, H., Chen, T., Feng, J., Chen, J., Dai, W., Yu, Q., Zhang, Y.-Q., Ma, W.-Y., Liu, J., Wang, M., et al. Memagent: Reshaping long-context llm with multi-conv rl-based memory agent. arXiv preprint arXiv:2507.02259, 2025. Zhang, W., Li, X., Zhang, Y., Jia, P., Wang, Y., Guo, H., Liu, Y., and Zhao, X. Deep research: survey of autonomous research agents. arXiv preprint arXiv:2508.12752, 2025a. Zhang, Z., Dai, Q., Bo, X., Ma, C., Li, R., Chen, X., Zhu, J., Dong, Z., and Wen, J.-R. survey on the memory mechanism of large language model-based agents. ACM Transactions on Information Systems, 43(6):147, 2025b. 11 MemOCR: Layout-Aware Visual Memory for Efficient Long-Horizon Reasoning A. Related Work Reinforcement Learning in LLM Agents. In recent years, reinforcement learning (RL) (Kaelbling et al., 1996) has emerged as powerful paradigm for post-training large language models (LLMs). While initial efforts focused on human preferences (Ouyang et al., 2022) or distilled reward models (Bai et al., 2022), the field has gradually shifted toward rule-based feedback, demonstrating great potential in enhancing model capabilities. Key algorithmic contributions include proximal policy optimization (Schulman et al., 2017), based on generalized advantage estimation (Schulman et al., 2015), and GRPO (Shao et al., 2024), which utilizes group normalization, improving the optimization stability. Building upon these foundations, recent research has extended RL to optimization of complex and long-horizon trajectory, specifically targeting multi-turn interactions with tool utilization (Jin et al., 2025; Shi et al., 2025b; Xue et al., 2025). Agentic Memory Management. Given the context window limits of LLMs, memory mechanism is essential for agents to retain information for long-horizon reasoning. Mainstream approaches typically employ textual representations, generally falling into two paradigms. The first treats raw historical segments as memory and directly injects them into the working context (Li et al., 2025; Jin et al., 2025; Song et al., 2025; Shi et al., 2025b). For example, Search-r1 (Jin et al., 2025) enables multi-turn searching by leveraging raw history throughout the reasoning process. The second paradigm adopts textual summary memory, where long-context information is compressed into concise text forms rather than retaining the full raw history (Duverger et al., 2024; Yu et al., 2025; Wang et al., 2025; Bian et al., 2026; Hu et al., 2026; Shi et al., 2025a). This approach distills historical interactions into summaries, maintained via dynamic updates or overwrites. For instance, MemAgent (Yu et al., 2025) proposes memorizing while reading paradigm, summarizing and distilling task-relevant information step by step. MemAlpha (Wang et al., 2025) trains the agent to manage complex hierarchical memory systems, i.e., to extract, store, and update memory corpora of varying sizes and importance. OCR for Context Compression. Optical Character Recognition (OCR) (Arlazarov et al., 2022; Smith, 2007) is wellestablished technology that is widely utilized for extracting textual content embedded in image format. In recent research advances, OCR has beed explored as an innovative vision-text compression paradigm (Wei et al., 2025; Xing et al., 2025; Cheng et al., 2025). Unlike the conventional practice of directly inputting long context into LLMs, this OCR-driven method encodes long context into visual representations. By leveraging the high information density of visual tokens to reduce token overhead, this approach holds the potential to revolutionize the memory architecture of agents (Feng et al., 2026). B. Limitations While MemOCR demonstrates strong token-efficiency and robustness under tight budgets, it has several limitations. Dependence on vision/OCR robustness. MemOCR relies on the backbone vision-language model to accurately read heavily downsampled text and layout cues. Failures in visual perception (e.g., blur, small fonts, rendering flaws) can directly translate into missing or hallucinated evidence, especially at extreme budgets. Layout policy may be task-specific. The learned salience allocation is optimized for long-context QA-style supervision and the training distributions. It may not transfer optimally to other agentic workloads (e.g., planning, tool-use logs, dialog personalization) where the notion of crucial evidence differs or evolves over time. Additional computational overheads. Although rendering is lightweight compared to model inference, the vision language modeling introduces extra latency and complexity in real deployments, especially with relatively small context length (e.g., the 10K context length in Figure 9). C. Implementation Details This appendix provides the engineering details required to reproduce MemOCR, including the end-to-end memory pipeline (drafting, rendering, budget control, and reading), training configurations with budget-aware objectives, evaluation protocols under extreme memory budgets, and baseline implementations. C.1. Technical Details End-to-end pipeline. MemOCR follows two-stage pipeline with an intermediate deterministic rendering step: MemOCR: Layout-Aware Visual Memory for Efficient Long-Horizon Reasoning Table 3. Budget-to-resolution schedule used for MemOCR. The calculation is based on the 2828 patch size of Qwen2.5-VL. Budget Target resolution (Max Number of Pixels) 1024 256 64 16 802,816 200,704 50,176 12,544 Stage 1: Rich-text memory drafting. We maintain persistent rich-text memory RT step t, the agent takes the incoming chunk Ct together with the current memory RT an updated memory RT while encoding priority via structure/formatting (e.g., headings, bullets, and boldings). in Markdown. At each time t1 and user query Q, and outputs . Drafting process is budget-agnostic: the drafter writes semantically complete memory Rendering (no LLM calls involved). After processing all chunks, we convert the final rich-text memory RT into 2D memory image VT using deterministic Markdown-to-image renderer with fixed style sheet. The memory budget is enforced only at this step by downsampling VT such that the image takes up at most visual tokens. Stage 2: Memory image reading. The agent receives the budgeted memory image VT together with the question Q, and generates the final answer A. No raw history or original long context is provided at this stage, thus all task-relevant information must be recovered from VT . Markdown rendering. We implement Markdown-to-image rendering module using FastAPI 1 and Playwright with Chromium backend 2. Given an input Markdown string, the module (i) normalizes the text by stripping leading/trailing whitespace and surrounding backticks, (ii) converts Markdown to HTML using the Python markdown library 3, (iii) wraps the generated HTML in fixed, inlined CSS template, and (iv) renders the HTML in headless Chromium page and returns screenshot image. Budget-to-resolution mapping. Given memory budget (in visual tokens), we resize the rendered memory image to target resolution such that the vision encoder produces tokens. We compute the visual token count using the backbone models (i.e., Qwen2.5-VL-7B-Instruct) patching size of 2828=784 pixels per token. Table 3 provides the computed budget schedules used in our experiments. Prompt templates. We use two prompt templates: (i) memory drafting template that updates the Markdown memory given problem, new article chunk, and the previous memory; and (ii) memory reading template that answers the problem based on the previous memory and wraps the answer in boxed{}. Memory drafting prompt template. You are given problem, an article, and previous memory. You should draft the memory in markdown format with the crucial information in it that helps to answer the problem. In your markdown draft, you may use different headings to arrange the font sizes and styles of the information. e.g., more important information should be emphasized and more visible (larger font size, bolder, etc.), in case the rendered image can be clearly read. <problem> {PROBLEM} </problem> <article> {ARTICLE} </article> <memory> {MEMORY} </memory> The draft memory, in markdown format: Memory reading prompt template. <{MEMORY IMAGE}> You are presented with problem and previous memory. Please answer the problem based on the previous memory and put the answer in boxed{}. <problem>{PROBLEM}</problem> Your answer: 1https://github.com/fastapi/fastapi 2https://github.com/microsoft/playwright 3https://pypi.org/project/Markdown/ 13 MemOCR: Layout-Aware Visual Memory for Efficient Long-Horizon Reasoning Table 4. Budget-aware training task configuration. Task Memory budget (Tokens) Question source Weight wk"
        },
        {
            "title": "Tstd\nTaugM\nTaugQ",
            "content": "512 32 512 Original Question Original Question Detail-oriented Question 1.0 0.7 0.3 Table 5. Primary training hyperparameters for MemOCR. Category"
        },
        {
            "title": "Algorithm\nAlgorithm\nAlgorithm\nAlgorithm\nRollout\nRollout\nRollout\nRollout\nOptimization\nOptimization\nOptimization\nOptimization",
            "content": "Hyperparameter Value (TBD) Chunk size Ct Group size KL coefficient β Clip ratio ϵ Top-p Temperature Max rich-text memory tokens Max final answer tokens Global batch size Micro batch size Learning rate Warmup Steps 5,000 16 1 103 0.20 0.999 1 2048 2048 64 16 1 106 20 C.2. Training Details Backbone models and parameter updates. MemOCR is built on vision-language model backbone Qwen2.5-VL-7BInstruct. Unless otherwise stated, we train the model with full-parameter updates under BFloat16 precision under Fully Sharded Data Parallelism (FSDP) to scale to multi-GPU setups. Training data construction. We train on long-context QA instances constructed from HotpotQA. For each training example, we assemble long context by concatenating the supporting documents and sampled distractor documents, then pad/truncate to target context length (e.g., 30K tokens). We split the long context into stream of chunks = {Ct}T using fixed chunk size and stride, and update the persistent memory after each chunk. t=1 Budget-aware RL objectives. We optimize the MemOCR layout/salience policy using budget-aware RL objective with GRPO. We construct three training tasks: (i) standard QA with moderate budget, (ii) QA under aggressively compressed memory images (low-budget robustness), and (iii) detail-oriented QA at high resolution (to ensure auxiliary details remain present but low-priority). We compute task-specific advantages for reader updates and use weighted aggregation of advantages for drafting updates to learn single layout policy that generalizes across budgets. Table 4 lists the task configurations and weights. Optimization and hyperparameters. Table 5 summarizes the primary hyperparameters for GRPO algorithm, rollout generation, and optimization. Rollouts are generated with stochastic decoding (do_sample=True). Training hardware. All three variants of MemOCR in Table 2 (full objectives, w/o TaugM and w/o TaugM, TaugQ) are trained on 64 A100 NVIDIA GPUs for 14 days till convergence (about 1,000 steps, 21 kGPU-hours each). C.3. Evaluation Details For evaluation, we construct contexts of approximately 10K/30K/100K tokens by concatenating the instance documents with sampled distractors, matching the training construction method. We fix the distractor sampling seed per split to ensure comparability across methods. Due to the time comsumption of long-context QA (as shown in Table 9), we follow Yu et al. to randomly downsample the four benchmarks to sizes of 128. 14 MemOCR: Layout-Aware Visual Memory for Efficient Long-Horizon Reasoning We use sub-word exact match (SEM) as accuracy and report the mean scores over three independent runs with different random seeds. statistical significance analysis is conducted in Appendix D.1. Unless otherwise noted, we use stocastic decoding (temperature = 0.7, top-p=0.95) for both baselines and MemOCR. C.4. Baseline Setups Budget control for text baselines. For purely textual memory baselines, we enforce token budget by truncating the memory text to the first tokens under fixed tokenizer (Qwen2.5-7B-Instruct). We apply truncation at evaluation time only, using the last updated memory state (or the baselines own memory update rule). Budget control for MemOCR. For MemOCR, we enforce the same budget in visual tokens by resizing the rendered memory image such that the vision encoder produces visual tokens  (Table 3)  . This ensures that comparisons reflect the same effective context capacity across text and vision modalities. Reproduction of baselines. We reproduce baseline memory systems according to their official releases: MemAgent (Yu et al., 2025): we use the authors released code4 and model weight5 for both memory drafting and final answer generation. The chunk size Ct is set to 5,000 following the authors setup. Memα (Wang et al., 2025): we use the authors released code6 for reproduction. We conduct the memory drafting with the official model weight7, and the final answer generation with Qwen2.5-7B-Instruct(Yang et al., 2024) to match the model size of other baselines. The chunk size Ct is set to 1,000 following the authors setup. Mem0 (Chhikara et al., 2025): we run reproduction following the official documentation8. The memory drafting phase is finished with online black box APIs9 and the answer generation with Qwen2.5-7B-Instruct. The chunk size Ct is set to 5,000, consistent with our setup. For the above three methods, we match (i) the answer generation model Qwen2.5-7B-Instrcut, (ii) the dataset split and long-context construction, and (iii) the decoding settings. D. Supplementary Experiments D.1. Statistical Verification To ensure the reliability of our findings and rigorously validate the significance of the performance gains, we conduct comprehensive statistical analysis. We perform independent experimental runs for both MemOCR and the textual baselines to capture variability. We report the mean scores (Mean) and standard deviations (Std) across all datasets and settings in Table 6. To formally assess the significiance of improvements, we perform an independent two-sample t-test on the averaged accuracy. The resulting p-values and performance gains () are summarized in Table 7. Significance Analysis. The statistical data highlights two critical observations regarding the robustness and scalability of MemOCR. First, MemOCR achieves consistent significant robustness under tight memory constraints. As shown in Table 7, in low-memory scenarios (B {16, 64}), the improvements are not only large in magnitude (e.g., +30.6 at 10K/16 tokens) but also statistically significant, with p-values consistently far below the 0.05 threshold (often 0.01). This confirms that the resilience of MemOCR against catastrophic forgetting is systematic improvement. Second, performance gaps diminish as the memory budget increases, and become marginal under the longest context. At the maximum budget (B = 1024), the gain becomes small at 100K context (only +0.9) and is not statistically significant (p = 0.3419), indicating saturation when both memory and context are ample. Meanwhile, MemOCR still shows modest but significant gains at shorter contexts (10K/30K), suggesting that the benefit is robust beyond extreme-budget regimes, while remaining most pronounced when the working context is severely constrained. 4https://github.com/BytedTsinghua-SIA/MemAgent 5https://huggingface.co/BytedTsinghua-SIA/RL-MemoryAgent-7B 6https://github.com/wangyu-ustc/Mem-alpha 7https://huggingface.co/YuWangX/Memalpha-4B 8https://github.com/mem0ai/mem0 9https://mem0.ai/ 15 MemOCR: Layout-Aware Visual Memory for Efficient Long-Horizon Reasoning Table 6. Accuracies (MeanStd) across all datasets, context lengths and memory budgets. Method Memory Budget (Tokens) HotpotQA 30K 10K 100K 10K 2Wiki 30K 100K 10K NQ 30K 100K 10K TriviaQA 30K 100K 10K Average 30K 100K Mem0 Mem-α MemAgent MemOCR 1024 256 64 16 1024 256 64 16 1024 256 64 16 1024 256 64 16 70.30.0 66.10.5 64.10.0 60.40.4 47.70.0 40.90.5 47.70.0 48.80.8 48.20.4 69.50.0 70.60.9 61.40.5 62.00.1 58.30.2 53.60.1 71.10.0 65.60.0 63.80.5 57.80.0 44.80.5 43.50.5 46.90.0 51.10.5 49.70.9 69.50.0 70.60.5 62.80.5 61.30.0 58.00.2 55.00.3 56.70.5 60.90.0 56.20.0 53.90.0 42.20.0 43.30.5 46.90.0 43.80.0 50.00.0 66.40.0 65.90.9 62.50.0 56.00.1 53.20.2 53.00.1 37.00.5 43.30.5 34.90.5 28.40.5 31.50.5 27.60.5 41.40.0 34.10.9 43.00.0 65.30.5 62.80.9 58.90.5 43.00.1 42.90.3 41.10.2 75.00.0 77.91.2 79.40.5 47.70.0 40.11.2 50.00.8 45.60.9 44.81.8 43.02.1 21.10.0 23.20.4 19.30.9 47.30.2 46.50.8 47.90.3 57.81.6 57.30.9 57.01.4 38.30.8 32.80.8 41.70.5 47.90.9 33.60.8 38.02.5 17.20.8 24.21.4 18.30.5 40.30.2 37.00.9 38.70.9 25.82.1 29.70.8 28.92.1 34.90.5 31.50.4 39.10.8 27.12.0 25.00.8 24.01.2 13.01.2 19.50.0 13.80.9 25.20.5 26.40.5 26.40.7 24.01.2 30.21.2 26.30.9 35.40.9 32.30.9 40.10.5 23.70.9 26.30.5 24.21.3 11.70.8 19.01.6 14.30.9 23.70.6 27.00.5 26.20. 82.31.2 78.91.6 79.41.6 65.42.3 63.82.0 61.22.7 53.41.8 46.11.4 55.52.7 70.11.2 78.12.1 66.71.2 67.80.9 66.70.7 65.70.6 82.31.2 76.82.4 76.61.4 66.13.3 62.51.4 58.12.5 51.32.0 44.31.6 53.62.5 69.52.1 77.61.2 67.22.1 67.31.4 65.31.5 63.90.5 50.80.8 52.32.1 51.02.0 38.51.6 42.70.9 36.24.4 48.71.2 36.51.8 47.92.5 64.61.8 69.53.4 59.62.4 50.71.3 50.31.3 48.71.0 24.01.2 26.60.8 23.20.5 28.40.5 26.31.6 16.70.5 24.51.6 20.11.2 27.32.1 49.71.2 56.31.4 41.71.8 31.60.5 32.30.4 27.20.3 84.81.1 75.10.6 78.30.5 72.21.0 73.70.2 62.73.8 61.80.6 49.21.2 55.42.0 79.62.4 81.31.2 69.84.4 74.60.4 69.80.5 66.61.2 82.21.8 75.40.5 75.71.3 71.20.3 72.81.8 65.53.6 57.32.2 48.81.3 58.31.8 79.71.8 80.91.8 68.93.1 72.60.7 69.50.9 67.10.7 77.61.7 68.11.8 67.22.9 62.92.2 66.23.4 62.41.2 51.01.7 43.63.4 47.54.0 77.60.4 80.72.1 65.81.9 67.30.4 64.71.5 60.71.8 67.24.7 57.96.0 52.41.0 57.91.0 56.02.3 45.73.5 42.81.9 35.97.9 45.94.3 80.82.6 72.25.7 67.23.8 62.20.7 55.51.7 52.80.4 Table 7. Statistical significance on average accuracy. Bold p-values indicate statistical significance (p < 0.05). Memory Budget(Tokens) Ctx MemOCR MemAgent p-value 1024 256 16 10K 30K 100K 10K 30K 100K 10K 30K 100K 10K 30K 100K 74.6 0.4 69.8 0.5 66.6 1.2 72.6 0.7 69.5 0.9 67.1 0.7 67.3 0.4 64.7 1.5 60.7 1.8 62.2 0.7 55.5 1.7 52.8 0.4 67.8 0.9 66.7 0.7 65.7 0.6 67.3 1.4 65.3 1.5 63.9 0.5 50.7 1.3 50.3 1.3 48.7 1.0 31.6 0.5 32.3 0.4 27.2 0.3 0.0024 +6.8 0.0052 +3.1 0.3419 +0.9 0.0110 +5.3 0.0217 +4.2 0.0032 +3.2 0.0008 +16.6 0.0002 +14.4 0.0018 +12.0 +30.6 <0.0001 0.0011 +23.2 +25.6 <0.0001 D.2. Computational Complexity Analysis We analyze inference-time complexity in the long-horizon setting both theoretically and empirically. Our key finding is that MemOCR does not incur significant computation overhead compared to textual memory. D.2.1. THEORETICAL ANALYSIS Notation. Let the full context contain tokens and be split into chunks = {Ct}T t=1, where each chunk contains N/T tokens. Let denote the memory budget used at the final answering step, as defined in 2. For simplicity, we ignore the question length which is typically small compared to memory. We assume Transformer backbone with full self-attention, where forward pass on length costs O(x2) attention operations. Shared complexity: memory drafting in text domain. At each step t, the agent consumes the current chunk and bounded in-context memory: Mt π( Mt1, Ct ), {1, . . . , }. The memory shown to the updater can be estimated by St = O(Ct + Mt1) = O(L + B), where the memory budget is the maximum number of in-context memory tokens. Therefore, the total drafting/update cost over chunks is Timedraft = (cid:88) t= O(S2 ) = O(cid:0)T (L + B)2(cid:1) = (L + B)2(cid:17) . (cid:16) (9) Under our protocol where and are fixed hyper-parameters, this stage scales linearly in the long context length . 16 MemOCR: Layout-Aware Visual Memory for Efficient Long-Horizon Reasoning Table 8. Throughput and per-call latency of the renderer used in MemOCR (five runs on 50 samples each). Metric Run Run 2 Run 3 Run 4 Run 5 Average Throughput (Sample/s) Latency (s) 68.1 0.186 70.6 0.160 67.9 0.174 70.8 0.186 63.7 0.169 68.2 0. Textual memory reading. Textual-memory baselines answer by feeding text memory of length into the LLM: π( MT , Q), MT B."
        },
        {
            "title": "Thus the answering cost is",
            "content": "Timeread, text = O(B2). (10) Visual memory reading. MemOCR answers from memory image VT whose vision encoder produces at most visual tokens/patches: π( VT , Q), #visual tokens(VT ) B. The cost in this stage consists of (1) the vision head processing the image (which is constant to and ) and (2) attention over the visual tokens in the language model, so the answering cost is Overall theoretical complexity. Combining Eq. (9)(11) yields: Timeread, visual = O(B2). TIMEtext = O(cid:0)T (L + B)2(cid:1) (cid:124) (cid:125) TIMEMemOCR = O(cid:0)T (L + B)2(cid:1) (cid:125) (cid:124) (cid:123)(cid:122) Timedraft (cid:123)(cid:122) Timedraft + O(B2) (cid:124) (cid:123)(cid:122) (cid:125) Timeread, text + O(B2) (cid:124) (cid:123)(cid:122) (cid:125) Timeread, visual = O(cid:0)T (L + B)2(cid:1), = O(cid:0)T (L + B)2(cid:1). (11) (12) (13) Hence, MemOCR and textual memory have the same theoretical complexity scaling in (through N/L). D.2.2. EMPIRICAL ANALYSIS End-to-end runtime. We report end-to-end runtime for MemOCR and representative textual-memory baseline, MemAgent, under our long-context protocol. Table 9 summarizes the average per-instance runtime (seconds) across datasets under 32-thread parallelism. The results exhibit the expected near-linear growth with context length (through ), and show that MemOCR remains comparable to (and even faster than on some cases) the text baseline at long contexts. The memory save comes majorly from the shorter memory states we find our model sometimes generate less tokens in memory states than the textual memory baseline, which exceed the visual encoding overhead. Visual-only overhead: memory image rendering. MemOCR additionally renders RT into memory image before memory reading. This step is deterministic and does not invoke any LLM, and its runtime is linear in the output canvas size. We empirically study its computational overhead by feeding 50 rich-text memory samples into the renderer for 5 times, and the results are in Table 8. The results indicate the image rendering process is very light-weighted, consuming only 1 second per 68 samples and 0.175 extra latency. D.3. Additional Ablation Studies Motivation. Table 2 shows that removing RL from our 7B setting causes substantial degradation, especially under strict memory budgets. natural question is whether simply scaling the backbone LLM can close the gap without our budget-aware RL. To answer this, we further report results using Qwen2.5-VL-32B-Instruct and Qwen2.5-VL-72B-Instruct under the same memory-budgeted evaluation protocol. We follow the same evaluation setup as in 4.5. Key observations. (1) 7B+RL beats scaling. Despite being trained on 7B backbone, MemOCR matches or exceeds the scaled non-RL backbones (32B/72B) in most long-context settings, indicating that budget-aware RL is more effective 17 MemOCR: Layout-Aware Visual Memory for Efficient Long-Horizon Reasoning Table 9. Average runtime (second/sample) of MemOCR and MemAgent across different datasets and memory budgets. Method Memory Budget (Tokens) MemOCR MemAgent 1024 256 64 16 1024 256 64 16 HotpotQA 30K 40.8 39.7 39.9 37.7 41.6 42.0 41.6 42. 100K 173.7 173.9 167.8 174.9 208.2 207.6 205.7 206.5 10K 11.3 10.1 9.6 10.0 6.6 6.7 6.7 6. 2Wiki 30K 44.0 45.4 43.7 47.5 35.9 35.5 36.0 35.9 100K 187.5 180.8 182.6 186.1 306.6 305.8 300.6 301. 10K 13.5 12.3 9.8 9.1 9.1 9.2 9.2 9.4 NQ 30K 47.8 47.8 48.3 47.0 47.3 47.7 47.5 47. 100K 240.1 231.4 235.7 207.8 230.2 228.9 226.8 229.4 10K 15.7 15.3 16.3 8.5 9.4 9.4 9.4 9. TriviaQA 30K 58.5 57.5 51.0 59.8 49.5 49.9 49.8 49.4 100K 241.2 229.1 229.9 208.3 232.0 232.7 231.7 231. Average 30K 47.8 47.6 45.7 48.0 43.6 43.8 43.7 43.6 100K 210.6 203.8 204.0 194.3 244.3 243.8 241.2 242. 10K 15.0 12.6 11.8 9.4 8.5 8.5 8.5 8.5 10K 19.5 12.7 11.6 9.9 8.9 8.8 8.9 8. Table 10. Accuracies (%) under larger backbones (Qwen2.5-VL-32/72B-Instruct) without RL. Method Memory Budget (Tokens) MemOCR (Ours) Qwen-VL-7B Qwen-VL-32B w/o RL 72B 1024 256 64 16 1024 256 64 16 1024 256 64 16 1024 256 64 16 HotpotQA 30K 100K 75.1 75.4 68.1 57.9 54.7 51.6 50.8 30.5 59.4 60.2 57.0 31.3 60.9 65.6 60.2 42.2 78.3 75.7 67.2 52. 41.4 46.9 42.2 28.9 53.1 56.3 44.5 34.4 60.2 57.8 50.8 43.8 10K 84.8 82.2 77.6 67.2 67.2 62.5 62.5 33. 64.8 71.9 58.6 31.3 78.1 75.8 66.4 35.9 2Wiki 30K 100K 73.7 72.8 66.2 56.0 45.3 46.9 43.0 25. 54.7 50.8 43.0 32.8 53.9 57.8 46.1 25.8 62.7 65.5 62.4 45.7 38.3 31.3 32.8 28.1 33.6 40.6 30.5 21.9 37.5 46.9 34.4 23. 10K 72.2 71.2 62.9 57.9 51.6 46.9 47.7 34.4 60.2 60.2 51.6 32.0 68.8 63.3 54.7 24.2 10K 61.8 57.3 51.0 42.8 45.3 46.9 46.9 27.3 50.8 50.0 46.1 37.5 57.8 60.2 54.7 47.7 NQ 30K 49.2 48.8 43.6 35. 40.6 36.7 36.7 25.0 48.4 42.2 45.3 35.2 51.6 56.3 46.9 39.8 100K 55.4 58.3 47.5 45.9 43.8 42.2 39.8 28. 50.0 48.4 43.8 35.9 46.1 50.0 46.1 37.5 10K 79.6 79.7 77.6 80.8 62.5 71.1 66.4 54.7 77.3 74.2 73.4 63. 78.1 79.7 77.3 69.5 TriviaQA 30K 100K 81.3 80.9 80.7 72.2 68.0 70.3 67.2 60.2 75.0 76.6 71.9 66. 82.8 82.0 77.3 68.8 69.8 68.9 65.8 67.2 47.7 58.6 53.1 46.9 70.3 69.5 66.4 59.4 71.1 71.1 68.8 63.3 10K 74.6 72.6 (-2.7%) 67.3 (-9.8%) 62.2 (-16.6%) 56.6 56.8 (+0.3%) 55.9 (-1.4%) 37.5 (-33.8%) 63.3 64.1 (+1.2%) 57.4 (-9.3%) 41.0 (-35.2%) 70.7 69.7 (-1.4%) 63.3 (-10.5%) 44.3 (-37.3%) Average 30K 69.8 69.5 (-0.5%) 64.7 (-7.4%) 55.5 (-20.5%) 52.1 51.4 (-1.5%) 49.4 (-5.2%) 35.2 (-32.6%) 59.4 57.4 (-3.3%) 54.3 (-8.6%) 41.4 (-30.3%) 62.3 65.4 (+5.0%) 57.6 (-7.5%) 44.1 (-29.2%) 100K 66.6 67.1 (+0.8%) 60.7 (-8.8%) 52.8 (-20.7%) 42.8 44.7 (+4.6%) 42.0 (-1.8%) 33.0 (-22.8%) 51.8 53.7 (+3.8%) 46.3 (-10.6%) 37.9 (-26.8%) 53.7 56.4 (+5.1%) 50.0 (-6.9%) 42.0 (-21.8%) than naıve model scaling. (2) Lower decay under compression. When shrinking the memory budget, MemOCR shows much smaller relative drops than all non-RL baselines, and remains reliable even at the extreme 16-token budget where larger backbones degrade sharply. (3) Strength on long-context multi-hop. The advantage is most pronounced at 100K context on multi-hop benchmarks (HotpotQA, 2Wiki), suggesting the gain comes from robust memory usage rather than short-context capacity. E. Bad Case Analysis While MemOCR demonstrates strong performance in identifying and emphasizing crucial information via layout generation, it also fail on certain data points. In Figure 8, we analyze two representative failure modes observed during our experiments. Failure Mode A: Loss of fine-grained details in comparative reasoning. The first type of failure occurs when the question requires comparing detailed attributes of two entities, but the agent layout that prioritizes the entity names over the descriptions. As shown in the top panel of Figure 8, for the question Who has wider scope of profession...?, the agent correctly identifies Hrag Vartanian and Hovsep Pushman as key entities and renders them as H1 headers. However, the specific details required for comparison are rendered as standard body text. Under the constraint of low token budget (resulting in low-resolution downsampled image), the large headers remain legible, but the smaller body text collapses into unreadable pixel noise, ultimately leading to an incorrect answer. Failure Mode B: Information loss due to memory capacity overflow. The second failure mode arises when the rich-text memory is excessively long. This sometimes happens due to some generation issue where the model repeats the same words for multiple times, forcing the rendering engine to compress the font size below the readability threshold of the visual encoder. In the bottom panel of Figure 8, the agent attempts to encode lengthy history of Adidas Yeezy (over 2000 characters) into single memory image. The font size is drastically reduced after downsampling to fixed number of pixels. Consequently, the crucial evidence, such as the specific release date February 14, 2015, becomes indistinguishable. Unlike Failure Mode A, where headers preserved some partial information, this case may result in total information loss. 18 MemOCR: Layout-Aware Visual Memory for Efficient Long-Horizon Reasoning Figure 8. Failure Mode Analysis under Resource Constraints (16-token budget). (Top) In comparative reasoning (i.e., to choose among two candidates), while the layout successfully highlights entity headers, the body text containing crucial attributes is compressed into unreadable noise during downsampling. (Bottom) When the rich-text memory length exceeds the visual canvas capacity, the forced font scaling drops below the visual encoders resolution threshold, resulting in information loss."
        }
    ],
    "affiliations": [
        "Meituan",
        "National University of Singapore, School of Computing",
        "University of Science and Technology of China"
    ]
}