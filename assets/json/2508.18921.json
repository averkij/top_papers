{
    "paper_title": "Forecasting Probability Distributions of Financial Returns with Deep Neural Networks",
    "authors": [
        "Jakub Michańków"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "This study evaluates deep neural networks for forecasting probability distributions of financial returns. 1D convolutional neural networks (CNN) and Long Short-Term Memory (LSTM) architectures are used to forecast parameters of three probability distributions: Normal, Student's t, and skewed Student's t. Using custom negative log-likelihood loss functions, distribution parameters are optimized directly. The models are tested on six major equity indices (S\\&P 500, BOVESPA, DAX, WIG, Nikkei 225, and KOSPI) using probabilistic evaluation metrics including Log Predictive Score (LPS), Continuous Ranked Probability Score (CRPS), and Probability Integral Transform (PIT). Results show that deep learning models provide accurate distributional forecasts and perform competitively with classical GARCH models for Value-at-Risk estimation. The LSTM with skewed Student's t distribution performs best across multiple evaluation criteria, capturing both heavy tails and asymmetry in financial returns. This work shows that deep neural networks are viable alternatives to traditional econometric models for financial risk assessment and portfolio management."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 6 2 ] . - [ 1 1 2 9 8 1 . 8 0 5 2 : r a"
        },
        {
            "title": "Forecasting Probability Distributions of Financial Returns with Deep Neural\nNetworks",
            "content": "Jakub Michanków TripleSun Krakow, Poland jakub.michankow@triplesun.net Abstract This study evaluates deep neural networks for forecasting probability distributions of financial returns. 1D convolutional neural networks (CNN) and Long Short-Term Memory (LSTM) architectures are used to forecast parameters of three probability distributions: Normal, Students t, and skewed Students t. Using custom negative log-likelihood loss functions, distribution parameters are optimized directly. The models are tested on six major equity indices (S&P 500, BOVESPA, DAX, WIG, Nikkei 225, and KOSPI) using probabilistic evaluation metrics including Log Predictive Score (LPS), Continuous Ranked Probability Score (CRPS), and Probability Integral Transform (PIT). Results show that deep learning models provide accurate distributional forecasts and perform competitively with classical GARCH models for Value-at-Risk estimation. The LSTM with skewed Students distribution performs best across multiple evaluation criteria, capturing both heavy tails and asymmetry in financial returns. This work shows that deep neural networks are viable alternatives to traditional econometric models for financial risk assessment and portfolio management. Keywords: Deep Learning, Financial Forecasting, Probability Distributions, Risk Management, Value-at-Risk, LSTM, CNN 1. Introduction Financial return forecasting has moved from simple point prediction models to distributional forecasting that captures the full uncertainty structure of market dynamics. Traditional econometric approaches often struggle to model the complex, non-linear relationships and time-varying volatility patterns in financial time series. Deep learning opens new possibilities for capturing these patterns, yet most applications still focus on point forecasts rather than the complete distributional properties needed for risk management. Distributional forecasting in finance is important because accurate risk assessment requires more than just expected returns. Financial institutions, portfolio managers, and regulatory bodies need complete uncertainty quantification to make informed decisions about capital allocation, hedging strategies, and regulatory compliance. Traditional Value-at-Risk (VaR) and Expected Shortfall (ES) calculations rely on distributional assumptions that may not hold during market stress periods. This study addresses three research questions: First, do deep neural networks provide accurate forecasts of stock return distributions? Second, can these probabilistic forecasts be used for financial risk assessment? Third, do deep learning models outperform classical econometric approaches such as univariate GARCH models? The main contribution is the systematic evaluation of deep neural network architectures designed for distributional forecasting of financial returns. Custom loss functions based on negative log-likelihood for Normal, Students t, and skewed Students distributions are developed, allowing direct optimization of distribution parameters. The framework uses both 1D CNN and LSTM architectures, benefiting from their strengths in pattern recognition and sequential modeling. Preprint submitted to Elsevier August 27, 2025 2. Literature Review Probabilistic deep learning and financial forecasting builds on several research areas. The methodological framework is based on probabilistic deep learning principles established by Murphy [13, 14, 15], and Duerr et al. [4], which provide theoretical foundations for uncertainty quantification in machine learning models. In energy markets, research by Nowotarski and Weron [16], Marcjasz et al. [11] or [1] shows the effectiveness of distributional neural networks for electricity price forecasting, providing precedents for financial markets application. The evaluation methodology uses established scoring rules for probabilistic forecasts, particularly the framework developed by Gneiting and Raftery [7]. This foundation ensures the evaluation metrics are both theoretically sound and practically relevant for comparing forecasting models. Related research in distributional modeling includes dynamic approaches by Patton et al. [18] and Thiele et al. [21], density combination methods by Opschoor et al. [17], and neural network quantile regression by Keilbar and Wang [9]. Advanced deep learning approaches include the DeepAR framework [19] and multivariate probabilistic forecasting methods [22]. Forecast evaluation methodologies follow frameworks developed by Jordan et al. [8], temporal neural network approaches by Chen et al. [2], and spline-based methods by Gasthaus et al. [6]. 3. Methodology 3.1. Probabilistic Framework Financial returns are modeled using time-varying distributional framework where returns at time are expressed as: rt = µ(xt) + εt εt = σ(xt)zt, σ2 (ztxt) iid D(η(xt)) = σ2(xt) (1) (2) (3) where rt denotes the log return at time t, µ(xt) is the conditional mean function, εt is the error term, σ(xt) is the conditional volatility function, zt is the standardized residual, xt represents the input feature vector at time t, and denotes the assumed probability distribution type with time-varying parameters η(xt): fD(zt; xt) = (1) where (zt; 0, 1), (1) where t (zt; 0, 1, ν(xt)), (1) sS t(zt; 0, 1, ν(xt), ξ(xt)), where sS (4) where denotes Normal distribution, denotes Students distribution, sS denotes skewed Students distribution, ν(xt) is the time-varying degrees of freedom parameter, and ξ(xt) is the time-varying skewness parameter. 3.2. Distribution Specifications The base two-parameter Normal distribution N(µ, σ2) has probability density function: (x) = 1 σ 2π 2 ( xµ σ ) 1 (5) where µ is the mean (location parameter) and σ2 is the variance (scale parameter). The three-parameter Students distribution t(µ, σ2, ν) accommodates heavy tails commonly observed in financial returns: fS t(x) = (cid:17) Γ (cid:16) ν+1 σ2νπΓ (cid:16) ν 2 (cid:32) (cid:17) 1 + (x µ)2 νσ2 (cid:33) ν+1 2 (6) 2 where µ is the location parameter, σ2 is the scale parameter, ν > 0 is the degrees of freedom parameter, and Γ() is the Gamma function. The four-parameter skewed Students distribution sS t(µ, σ2, ν, ξ) captures both heavy tails and asymmetry: fsS t(xξ) = (cid:104) 2 ξ + ξ1 fS t(ξx)H(x) + fS t(ξ1x)H+(x) (cid:105) (7) where ξ > 0 is the skewness parameter (equal to 1 for symmetric distributions) and H() denotes the Heaviside function: H(x) = 0 1 for < 0 for 0 (8) For the skewed Students distribution, skewness shifts the mean away from the mode, so the conditional mean and conditional variance are: where: ϕ = E(X) = ϕσ, Var(X) = (γ ϕ2)σ2 (ξ2 1 (ξ + ξ2 )2νΓ( 1 ξ )(ν 1)Γ( 1 2 (ν + 1)) 2 ν) πν , γ = (ξ3 1 ξ3 ) (ξ 1 ξ ) ν ν Finally, the general probabilistic forecasting framework is expressed as: p(rt+1Ψt; ω) = fN(rt+1; µt+1, σ2 fS t(rt+1; µt+1, σ2 fsS t(rt+1; µt+1, σ2 t+1), t+1, νt+1), t+1, νt+1, ξt+1), for for for sS (9) (10) (11) 3.3. Loss Function Design Custom Negative Log-Likelihood (NLL) loss functions are used as the optimization objective minimized by the model for each distribution. For normal and Students distributions, the NLL formulation is relatively straightforward, while for the skewed Students distribution, we apply the Fernandez and Steel [23], [5] transformation. The general NLL formulation is: NLL(ω) = n(cid:88) ln fD(rt; ω) (12) t=1 where ω represents the model parameters, is the number of observations, and fD(rt; ω) is the probability density function of distribution evaluated at return rt. Parameter estimation follows: ˆω = arg min ω NLL(ω) (13) For the Normal distribution, the NLL contains three components: constant term, variance penalty, and squared error term: NLL(ω) = 2 ln(2π) + 1 2 n(cid:88) t=1 ln σ2 + n(cid:88) t=1 (rt µt)2 2σ2 (14) This formulation represents standard maximum likelihood estimation for Gaussian distributions. For the Students distribution, additional elements include Gamma function terms and the degrees of freedom parameter νt to account for heavy tails in financial return distributions: NLL(ω) = n(cid:88) ln Γ (cid:16) νt+1 Γ (cid:16) νt (cid:17) 2 2 ln σ2 + t=1 + 1 2 n(cid:88) t=1 n(cid:88) ln(νtπ) + 1 2 t=1 νt + 1 2 (cid:32) ln 1 + (rt µt)2 νtσ2 (cid:33) (15) (cid:17) n(cid:88) t=1 3 For the skewed Students distribution, the most complete formulation incorporates the skewness parameter ξt, captures asymmetry in financial return distributions, and uses piecewise construction via Heaviside functions: NLL(ω) = n(cid:88) (cid:32) ln (cid:33) + n(cid:88) ln σt 2 ξt + ξ1 t=1 (cid:104) ln t=1 fS t(ξtzt; 0, 1, νt)H(zt) + fS t(ξ1 (cid:105) zt; 0, 1, νt)H+(zt) (16) where zt = rtµt σt and H() is the Heaviside function. 3.4. Neural Network Architectures The 1D CNN model consists of convolutional layer with 256 filters and kernel size 2, followed by pooling layer (pool size 2) and flattening layer. The architecture captures local patterns in the return series while maintaining computational efficiency. Figure 1 shows the detailed architecture. Figure 1: Architecture of the 1D CNN model for probabilistic forecasting. The model takes return sequences and volatility estimates as input and outputs distribution parameters (µ, σ2, ν, ξ) for the skewed Students distribution. Source: Own study. The LSTM model employs three LSTM layers with decreasing neuron counts (128/64/32) to capture long-term dependencies and sequential patterns characteristic of financial time series. The architecture follows the same general structure as shown in Figure 1, with the convolutional layers replaced by LSTM cells. Both architectures use dense output layers with neurons equal to the number of distribution parameters being forecasted (2, 3, or 4 parameters). 3.5. Evaluation Metrics The Log Predictive Score (LPS) serves as basic metric similar to NLL: where Ψt denotes data used to estimate the predictive distribution at time t, and rt+1 is the observed value at + 1. LPS = ln p(rt+1Ψt; θ) (17) 4 The Continuous Ranked Probability Score (CRPS) provides comprehensive evaluation metric: CRPS(F, x) = (cid:90) (F(y) 1(y x))2dy where 1 is the indicator function. Gneiting and Raftery (2007) show it can be rewritten as: CRPS(F, x) = EFX 1 2 EFX (18) (19) where and are independent copies of random variable with CDF F. Lower CRPS values indicate better forecasts. The Probability Integral Transform (PIT) assesses calibration of predictive distributions: PIT = F(rt+1) (20) where F(rt+1) is the cumulative distribution function evaluated at the observed return rt+1. PIT measures the consistency between forecast distribution and the true distribution G. If = G, then it has the uniform distribution PITt+1 U(0, 1). 3.6. Value-at-Risk and Expected Shortfall Value-at-Risk for the long position is defined as: Pr{rt+1 VaRl t+1(α)Ψt} = α VaRt+1(α) = rt+1t σt+1qz α where qz α denotes the α-quantile of the random variable zt. Expected Shortfall for the long position is calculated as: ESl t+1(α) = E(rt+1rt+1 < VaRl t+1(α)) = rt+1t + σt+1E(ztzt < qz α) The Kupiec [10] (eq. 24) and Christoffersen [3] tests evaluate VaR model accuracy: (cid:32) Np αNp (1 α)nNp ln LR = 2 ln Np (cid:33)nNp (cid:33)Np (cid:32) 1 (cid:16) (cid:17) where Np denotes the number of VaR exceedances, and is the number of observations. The McNeil and Frey [12] test evaluates Expected Shortfall: Ut = rt ESl ˆσt t(α) (21) (22) (23) (24) (25) for rt < VaRl t(α), where ESl t(α) is the estimated ES at tolerance level α for the long position. 4. Data and Experimental Setup The analysis uses daily log returns from six major equity indices representing different global markets: S&P 500 (United States), BOVESPA (Brazil), DAX (Germany), WIG (Poland), Nikkei 225 (Japan), and KOSPI (South Korea). The dataset spans from January 3, 2000, to December 31, 2021, providing 2,487 forecasts for each index. walk-forward validation procedure with an expanding window approach is implemented. The initial training window contains 1,008 trading days (approximately four years), with the validation set comprising 33% of the training data. The test set size is 504 days. Each model predicts single-period returns based on the last 10 observations (sequence length). For training and prediction, walk-forward validation/expanding window approach is employed. In the first iteration, the model is trained on data (equal to the training set length) and then used for predictions over the next 5 period (equal to the test set length). After that, the window is expanded by additional data and the model is retrained. single period is predicted each time, based on the last 10 (sequence length) values. Each iteration is trained for 300 epochs. model checkpoint callback function is used to store the best weights (parameters) of the model based on the lowest loss function value in specific epoch. The weights are then used for prediction. Hyperparameter optimization uses both manual tuning and KerasTuner for systematic parameter selection. Table 1 presents the complete hyperparameter configuration. Table 1: Model hyperparameters and optimization approach."
        },
        {
            "title": "Tool",
            "content": "Hidden layers Neurons per layer Dropout ℓ2 regularization Optimizer Learning rate Sequence length Window length Batch size Epochs Kernel size* Filters* Pool size* 1-5 8-700 0-0.5 0-0.5 Adam/RMSProp/SGD 0.0001-0.5 1-200 252-Exp./21-1008 1-Exp. 10-1000 1-5 10-512 1-5 3 128/64/32 0.02 0.002 Adam 0.002 10 Exp.(min. 1008)-504 128 300 (ES) 2"
        },
        {
            "title": "Manual\nKerasTuner\nKerasTuner\nKerasTuner\nManual\nKerasTuner\nManual\nManual\nManual\nManual\nManual\nKerasTuner\nManual",
            "content": "Note: Exp. denotes expanding window, ES denotes early stopping. * applies to CNN networks. Source: Own study. 5. Results 5.1. Distributional Forecast Evaluation Table 2 presents the evaluation of model-distribution combinations across selected indices. The LSTM-SSTD (skewed Students t) configuration consistently achieves the lowest LPS and CRPS values across most indices, indicating superior distributional forecast accuracy. Table 2: Evaluation of distributional forecasts across indices and model configurations. Bold values indicate best performance for each criterion. Index/Model CNN-N CNN-STD CNN-SSTD LSTM-N LSTM-STD LSTM-SSTD LPS CRPS PIT p-value 1.2820 0.5229 2.41e-07 1.2416 0.5248 1.55e-05 LPS CRPS PIT p-value 1.6136 0.6914 2.41e-07 1.5990 0.6995 4.41e-05 LPS CRPS PIT p-value 1.3349 0.5285 2.41e-07 1.3147 0.5297 2.41e-07 S&P 500 1.2220 0.5197 0. Nikkei 225 1.5865 0.6963 0.0476 KOSPI 1.3172 0.5302 2.41e-07 1.2632 0.5146 2.41e-07 1.6124 0.6892 2.41e-07 1.3240 0.5246 2.41e1.2104 0.5137 0.0057 1.5870 0.6894 2.41e-07 1.2961 0.5201 4.70e-07 1.1933 0.5094 0.0309 1.5854 0.6874 2.41e-07 1.2847 0.5165 5.08eFor the S&P 500, LSTM-SSTD achieves an LPS of 1.1933 and CRPS of 0.5094, representing the best performance across all metrics. Similarly strong performance is observed for the Nikkei 225 (LPS: 1.5854, CRPS: 0.6874) and KOSPI (LPS: 1.2847, CRPS: 0.5165). 6 The PIT test results show important information about forecast calibration. The skewed Students specifications generally show better calibration, with several configurations achieving p-values above conventional significance thresholds, indicating proper distributional coverage. Figure 2 provides detailed visualization of these calibration properties. Figure 2: Distribution plots of PIT values for DAX index across different model configurations. The LSTM-SSTD model (left) shows the best approximation to uniform distribution, indicating superior calibration compared to LSTM-N (center) and CNN-N (right) models. Figure 2 shows the calibration quality through PIT histograms for the DAX index. Well-calibrated models should produce PIT values that approximate uniform distribution. The LSTM with skewed Students distribution shows the most uniform PIT distribution, confirming its better calibration properties. 5.2. Value-at-Risk Performance The VaR analysis demonstrates the practical utility of the probabilistic forecasts for risk management. Table 3 presents detailed VaR exceedance rates across all model-distribution combinations for each index. The table shows both 5% and 1% VaR levels, with statistical test results indicated. Table 3: Percentage shares of VaR(0.05)/VaR(0.01) estimate exceedances across indices and models. Index/Model S&P NKX DAX WIG KOSPI BVP S&P NKX DAX WIG KOSPI BVP CNN-N 4.50/1.56* 4.30*/1.48 6.35/2.53 5.07/1.97 5.70/2.09 5.38*/1.20* LSTM-N 4.86*/1.97 4.46*/1.88 5.87/1.97 4.82/1.84 5.42*/1.93 5.18*/1.24* CNN-STD 4.58/0.92* 4.02*/0.92* 5.42*/1.12* 4.58*/1.12 3.86/0.56* 3.98/0.72* CNN-SSTD 5.30/0.88* 3.53/0.40 4.14*/0.80* 3.69/0.80* 2.29/0.32 2.61/0.28 LSTM-STD LSTM-SSTD 5.34/1.01* 4.58*/1.01* 6.59/1.16* 4.95/1.20 4.46*/0.84* 3.65/0.68* 4.50/0.84* 3.81/0.64 5.66/0.88* 4.26/0.76* 3.37/0.44 2.41/0.32 Note: Bolded = closest to theoretical tolerance, underlined = correct Kupiec test, * = correct Christoffersen test. Expected exceedances: (5%) and 24 (1%) out of 2,487 forecasts. Source: Own study. For 5% VaR, the neural network models achieve exceedance rates very close to the theoretical 5% level. Notable performances include LSTM-N for S&P 500 (4.86%), LSTM-STD for Nikkei 225 (4.58%), and CNN-STD for DAX (5.42%). These results satisfy both Kupiec and Christoffersen test requirements in most cases. The 1% VaR results show similarly strong performance, with exceedance rates clustering around the theoretical 1% level. LSTM-STD consistently performs well across multiple indices, achieving 1.01% for both S&P 500 and Nikkei 225. 7 Figure 3: VaR forecasts for S&P 500, Nikkei 225, and DAX indices. Blue lines show actual returns, green and red lines represent 5% and 1% VaR estimates respectively, with dots indicating VaR exceedances. The models effectively capture periods of high volatility, including the 2020 COVID-19 market stress. Source: Own study. 9 Figure 4: VaR forecasts for WIG, KOSPI, and BOVESPA indices. The models demonstrate consistent performance across different market conditions and geographical regions, with appropriate clustering of exceedances during crisis periods. Source: Own study. Figures 3 and 4 show the VaR forecasting performance across all six indices. The models successfully capture major market stress periods, with VaR exceedances clustered during times of high volatility such as the 2008 financial crisis, European debt crisis, and COVID-19 pandemic. Finally, table 4 shows detailed results, for LSTM model with S&P500 index, using all three probability distributions. Table 4: Probabilistic forecasts evaluation, S&P500, LSTM model Model/Metrics LPS CRPS PIT p-value VaR exc. VaR exc. (%) Kupiec 5% Kupiec 1% Christoff. 5% Chrisotff. 1% ES bootstrap 5% ES sample 5% ES bootstrap 1% ES sample 1% LSTM(N) LSTM(STD) LSTM(SSTD) 1,263296 0,5146218 2,412545e-07 121/49 4,86/1,97 0,7569064 1,756379e-05(R) 0,4313099 1,7719e-08(R) 1,266502e-05 1,520095e-07(R) 1,210462 0,5137579 0,005700126 133/25 5,34/1,005 0,4310731 0,9791164 0,01607378(R) 0,5192243 0,597149 0,6093057 0,7041345 0,7506336 Source: Own study. 1,193376 0,5094981 0,03094408 112/21 4,50/0,84 0,2481387 0,4229126 0,0264911(R) 0,2833833 0,5066612 0,4703062 0,7884135 0,8679756 The results show superior performance of the skewed Students specification, which achieves lowest LPS and CRPS values, indicating better distributional forecast accuracy. The PIT test shows improved calibration as distributional complexity increases, with p-values rising from near-zero for the normal distribution to 0.031 for the skewed Students t. VaR performance is competitive across all specifications, with exceedance rates close to theoretical levels and most configurations passing the Kupiec and Christoffersen tests. Expected Shortfall tests generally validate the adequacy of the Students and skewed Students specifications, while the normal distribution shows some rejections. 5.3. Comparison with GARCH Models The comparison includes various GARCH model variants: standard GARCH (G), asymmetric power GARCH (AP), exponential GARCH (E), and GJR-GARCH (GJR), each tested with Normal (N), Students (STD), and skewed Students (SSTD) distributions. For details, see Teräsvirta [20]. Table 5: VaR exceedance rates (%) for neural network vs. GARCH models. Expected exceedances: 5% and 1%. Index/VaR Best NN Model Exceedances (%) Best GARCH Exceedances (%) S&P 5% Nikkei 5% DAX 5% KOSPI 5% S&P 1% Nikkei 1% DAX 1% KOSPI 1% LSTM-N LSTM-STD CNN-STD LSTM-N LSTM-STD LSTM-STD CNN-STD LSTM-STD 4.86 4.58 5.42 5. 1.01 1.01 1.12 0.84 G(STD) AP(SSTD) E(SSTD) AP(SSTD) G(SSTD) AP(N) AP(N) AP(N) 5.11 4.91 6.03 6.15 0.97 0.92 0.84 0.80 Note: = GARCH, AP = Asymmetric Power GARCH, = Exponential GARCH, GJR = GJR-GARCH. Distributions: = Normal, STD = Students t, SSTD = Skewed Students t. Source: Own study. Table 5 compares the best-performing neural networks against classical GARCH specifications. For 5% VaR, the neural network models achieve exceedance rates very close to the theoretical 5% level. 10 Notable performances include LSTM-N for S&P 500 (4.86%), LSTM-STD for Nikkei 225 (4.58%), and CNN-STD for DAX (5.42%). These results satisfy both Kupiec and Christoffersen test requirements in most cases. The 1% VaR results show similarly strong performance, with exceedance rates clustering around the theoretical 1% level. LSTM-STD consistently performs well across multiple indices, achieving 1.01% for both S&P 500 and Nikkei 225. 6. Conclusion This study shows that deep neural networks are viable alternatives to traditional econometric models for financial return distribution forecasting. The evaluation across six global markets shows that neural network architectures, particularly LSTM models with skewed Students distributions, can provide accurate and well calibrated probabilistic forecasts. The LSTM architectures consistent outperformance of CNN models suggests that the sequential nature of financial time series benefits from its memory mechanisms. However, the CNN models remain close, offering computational advantages for high-frequency applications. The practical use for risk management applications is confirmed through VaR testing, with performance competitive to or better than classical GARCH models. The frameworks flexibility in capturing complex, non-linear patterns makes it valuable for modern portfolio management and regulatory compliance. Limitations of this approach include the computational complexity relative to traditional econometric models and the potential for overfitting in volatile market conditions. Future research directions include testing additional asset classes, evaluating advanced architectures such as Transformers and xLSTM, conducting sensitivity analysis, investigating quantile-based forecasting approaches, and exploring trading strategy development based on the forecasted distributions. Source code The complete code can be found and downloaded from Github repository: https://github.com/jmichankow/deep_learning_probability References [1] Chen, J., Lerch, S., Schienle, M., Serafin, T., and Weron, R. (2025). Probabilistic intraday electricity price forecasting using generative machine learning. arXiv preprint arXiv:2506.00044. [2] Chen, Y., Kang, Y., Chen, Y., and Wang, Z. (2020). Probabilistic forecasting with temporal convolutional neural network. arXiv:1906.04397 [cs, stat]. [3] Christoffersen, P. F. (1998). Evaluating interval forecasts. International Economic Review, 39(4), 841862. [4] Duerr, O., Sick, B., and Murina, E. (2020). Probabilistic deep learning: With python, keras and tensorflow probability. Simon and Schuster. [5] Fernandez, C. and Steel, M. F. J. (1998). On Bayesian modeling of fat tails and skewness. Journal of the American Statistical Association, 93(441):359371. [6] Gasthaus, J., Benidis, K., Wang, Y., Rangapuram, S. S., Salinas, D., Flunkert, V., and Januschowski, T. (2019). Probabilistic forecasting with spline quantile function rnns. In The 22nd international conference on artificial intelligence and statistics, pages 19011910. PMLR. [7] Gneiting, T. and Raftery, A. E. (2007). Strictly proper scoring rules, prediction, and estimation. Journal of the American Statistical Association, 102(477):359378. [8] Jordan, A., Krüger, F., and Lerch, S. (2018). Evaluating probabilistic forecasts with scoringrules. arXiv:1709.04743 [cs]. [9] Keilbar, G. and Wang, W. (2022). Modelling systemic risk using neural network quantile regression. Empirical Economics, 62(1):93118. [10] Kupiec, P. (1995). Techniques for verifying the accuracy of risk measurement models. SSRN Scholarly Paper ID 6697. Rochester, NY. [11] Marcjasz, G., Narajewski, M., Weron, R., and Ziel, F. (2023). Distributional neural networks for electricity price forecasting. Energy Economics, 125:106843. [12] McNeil, A. J. and Frey, R. (2000). Estimation of tail-related risk measures for heteroscedastic financial time series: An extreme value approach. Journal of Empirical Finance, 7(3), 271300. [13] Murphy, K. P. (2012). Machine learning: probabilistic perspective. MIT Press. [14] Murphy, K. P. (2022). Probabilistic machine learning: An introduction. MIT Press. [15] Murphy, K. P. (2023). Probabilistic machine learning: Advanced topics. The MIT Press. [16] Nowotarski, J. and Weron, R. (2018). Recent advances in electricity price forecastinga review of probabilistic forecasting. Renewable and Sustainable Energy Reviews, 81:15481568. [17] Opschoor, A., van Dijk, D., and van der Wel, M. (2017). Combining density forecasts using focused scoring rules. Journal of Applied Econometrics, 32(7):12981313. [18] Patton, A. J., Ziegel, J. F., and Chen, R. (2019). Dynamic semiparametric models for expected shortfall (and value-at-risk). Journal of Econometrics, 211(2):388413. [19] Salinas, D., Flunkert, V., Gasthaus, J., and Januschowski, T. (2020). Deepar: Probabilistic forecasting with autoregressive recurrent networks. International Journal of Forecasting, 36(3):11811191. [20] Teräsvirta, T. (2009). An introduction to univariate GARCH models. In T. G. Andersen, R. A. Davis, J.-P. Kreiss, and T. Mikosch (Eds.), Handbook of Financial Time Series (pp. 17-42). Springer. [21] Thiele, S., Giacomini, R., and Patton, A. J. (2020). Modeling the conditional distribution of financial returns with asymmetric tails. Journal of Applied Econometrics, 35(6):717742. [22] Toubeau, J.-F., Bottieau, J., Vallée, F., and De Grève, Z. (2019). Deep learning-based multivariate probabilistic forecasting for short-term scheduling in power markets. IEEE Transactions on Power Systems, 34(2):12031215. [23] Trottier, D.-A. and Ardia, D. (2016). Moments of standardized FernandezSteel skewed distributions: Applications to the estimation of GARCH-type models. Finance Research Letters, 18:311316."
        }
    ],
    "affiliations": [
        "TripleSun Krakow, Poland"
    ]
}