{
    "paper_title": "Motion Anything: Any to Motion Generation",
    "authors": [
        "Zeyu Zhang",
        "Yiran Wang",
        "Wei Mao",
        "Danning Li",
        "Rui Zhao",
        "Biao Wu",
        "Zirui Song",
        "Bohan Zhuang",
        "Ian Reid",
        "Richard Hartley"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Conditional motion generation has been extensively studied in computer vision, yet two critical challenges remain. First, while masked autoregressive methods have recently outperformed diffusion-based approaches, existing masking models lack a mechanism to prioritize dynamic frames and body parts based on given conditions. Second, existing methods for different conditioning modalities often fail to integrate multiple modalities effectively, limiting control and coherence in generated motion. To address these challenges, we propose Motion Anything, a multimodal motion generation framework that introduces an Attention-based Mask Modeling approach, enabling fine-grained spatial and temporal control over key frames and actions. Our model adaptively encodes multimodal conditions, including text and music, improving controllability. Additionally, we introduce Text-Music-Dance (TMD), a new motion dataset consisting of 2,153 pairs of text, music, and dance, making it twice the size of AIST++, thereby filling a critical gap in the community. Extensive experiments demonstrate that Motion Anything surpasses state-of-the-art methods across multiple benchmarks, achieving a 15% improvement in FID on HumanML3D and showing consistent performance gains on AIST++ and TMD. See our project website https://steve-zeyu-zhang.github.io/MotionAnything"
        },
        {
            "title": "Start",
            "content": "5 2 0 2 2 1 ] . [ 2 5 5 9 6 0 . 3 0 5 2 : r Motion Anything: Any to Motion Generation Zeyu Zhang1 Yiran Wang2 Wei Mao3 Danning Li4 Rui Zhao5 Biao Wu6 Zirui Song67 Bohan Zhuang8 Ian Reid7 Richard Hartley19 1ANU 2USYD 3Tencent 4McGill 5JD.com 6UTS 7MBZUAI 8ZJU 9Google https://steve-zeyu-zhang.github.io/MotionAnything Figure 1. Motion Anything is an any-to-motion method for generating high-quality, controllable human motion under multimodal conditions, including text queries, background music and mix of both. Video demos are included in the supplementary material."
        },
        {
            "title": "Abstract",
            "content": "Conditional motion generation has been extensively studied in computer vision, yet two critical challenges remain. First, while masked autoregressive methods have recently outperformed diffusion-based approaches, existing masking models lack mechanism to prioritize dynamic frames and body parts based on given conditions. Second, existing methods for different conditioning modalities often fail to integrate multiple modalities effectively, limiting control and coherence in generated motion. To address these challenges, we propose Motion Anything, multimodal motion generation framework that introduces an AttentionEqual contribution. based Mask Modeling approach, enabling fine-grained spatial and temporal control over key frames and actions. Our model adaptively encodes multimodal conditions, including text and music, improving controllability. Additionally, we introduce Text-Music-Dance (TMD), new motion dataset consisting of 2,153 pairs of text, music, and dance, making it twice the size of AIST++, thereby filling critical gap in the community. Extensive experiments demonstrate that Motion Anything surpasses state-of-the-art methods across multiple benchmarks, achieving 15% improvement in FID on HumanML3D and showing consistent performance gains on AIST++ and TMD. 1. Introduction Human motion generation [85] has been widely explored in recent years due to its broad applications in film production, video gaming, augmented and virtual reality (AR/VR), and embodied AI for human-robot interaction. Recent advancements in conditional motion generation, including text-tomotion [22, 45, 64] and music-to-dance [36, 51] models have shown promising potential in 3D motion generation. These developments mark significant progress in generating motion sequences directly from textual descriptions and background music. However, despite extensive research in motion generation, the field still faces two significant challenges. (1) Recently, masked autoregressive methods [21, 44] have shown promising trend, outperforming diffusionbased methods [10, 53, 70]. However, existing masking models have been underexplored in generating motion that prioritizes dynamic frames and body parts in motion sequences based on given conditions. (2) Although specialized and multitask methods [5, 17, 73, 83] exist for different conditioning modalities, they often overlook the importance of integrating multiple modalities to achieve more controllable generation, as shown in Table 1. For example, enhancing music-to-dance generation with precise text descriptions can improve control and coherence, whereas relying on only single modality as condition leads to underperformance. Our motivation is to address these challenges by presenting an innovative method that tackles them in nutshell. To overcome the first challenge, we designed conditional masking approach within an autoregressive generation paradigm across both spatial and temporal dimensions, enabling the model to focus on key frames and actions corresponding to the given condition, as shown in Figure 2. The conditional masking strategy also dynamically adjusts based on the modality of the condition, whether it is text or music. To tackle the second challenge, we design our architecture to handle multimodal conditions adaptively and simultaneously. From temporal perspective, our model aligns different input modalities to control motion generation in time-sensitive manner. Meanwhile, from spatial perspective, it maps action queries to specific body-part movements and aligns music genres with corresponding dance styles. Moreover, since multi-conditioning in motion generation is underexplored, there is no motion dataset with paired music and text available in the current community. Hence, we have curated new motion dataset with paired music and text as benchmark to help advance the communitys exploration of multimodal conditioning in motion generation. In general, our contributions can be summarized as follows: We present Motion Anything, an any-to-motion frameFigure 2. Masking strategy comparison. This figure demonstrates the key differences between the previous random masking strategy [21] (top) and our attention-based masking (bottom). Our masking strategy focuses on the more significant and dynamic parts of the motion (colored) corresponding to the condition. work that can seamlessly and adaptively encode multimodal conditions for more controllable motion generation. This fills the gap of multimodal conditioning in previous motion generation research and represents significant improvement. For generating more controllable motion, we design an Attention-based Mask Modeling approach across both temporal and spatial dimensions, focusing on key frames and key actions corresponding to the condition. We further customize the mask transformer to adaptively handle different modalities of conditions, enhancing motion generation by integrating multimodal conditioning. For exploring multi-conditioning motion generation, we introduce the new Text-Music-Dance (TMD) dataset, which includes 2,153 paired samples of text, music, and dance, making it twice as large as AIST++ [34]. We also conducted extensive experiments on standard benchmarks across multiple motion generation tasks. Our method achieved 15% improvement in FID on HumanML3D [19] and consistent improvement on AIST++ [34] and TMD datasets. 2. Related Works Text-to-Motion Generation. Recent advancements in human motion generation have skillfully combined diffusion and autoregressive models, achieving more realistic, versatile, and scalable motion synthesis. Foundational work like MDM [53] introduced transformer-based diffusion approach for lifelike, text-driven motion generation. Expanding on this, MotionDiffuse [72] added refined control and diversity mechanisms, while MLD [10] boosted efficiency by operating within latent space, reducing computational demands without sacrificing quality. Motion Mamba [78] Figure 3. Motion Anything architecture. The multimodal architecture consists of several key components: (a) temporal and (c) spatial attention-based masking, (b) motion generator, and (d) single block of motion generator. These components enable the model to learn key motions corresponding to the given conditions, and facilitate alignment between multi-modal conditions and motion features. Models Text-to-Motion Music-to-Dance Text and Music to Dance TM2D [17] UDE [83] UDE-2 [84] MoFusion [12] MCM [39] LMM [73] MotionCraft [5] MagicPose4D [67] STAR [8] TC4D [3] Motion Avatar [77] Motion Anything (Ours) Table 1. Methods comparison. Either single-task or multi-task models can handle only one condition at time, overlooking the importance of integrating multiple modalities for more controllable generation. Our Motion Anything introduces an innovative approach that encodes different modalities simultaneously and adaptively for more controllable generation. addressed the challenge of generating longer sequences, and ReMoDiffuse [70] further enriched motion variability by incorporating retrieval-augmented diffusion. Meanwhile, autoregressive models like MoMask [21] enhanced temporal coherence through generative masked modeling, selectively revealing segments of the motion sequence. BAMM [44] introduced bidirectional model to capture detailed motion with forward and backward dependencies. InfiniMotion [76] optimized transformer memory to support extended sequences, and KMM [75] prioritized essential frames to balance continuity and computational efficiency. MoGenTS [64] added spatial-temporal joint modeling for further structural consistency in generated motions. Music-to-Dance Generation. Recent work in musicdriven dance generation has leveraged autoregressive and diffusion-based models to achieve more synchronized, diverse, and controllable dance motions. TSMT [33] pioneered using transformer architectures to model complex dance motions. Subsequently, early methods like DanceNet [86] and Dance Revolution [24] customize autoregressive and sequence-to-sequence models to establish foundational mappings between music and movement. FACT [34] and Bailando [51] build upon this by incorporating 3D motion data and actor-critic memory models to capture richer choreography, and Bailando++ [52] enhances this framework further for refined generation quality. EDGE [54] introduces user-editable dance generation for greater customization. In recent work, Lodge [36] and Lodge++ [35] apply coarse-to-fine diffusion methods to extend sequence length and create vivid choreography patterns, while BeatIt [26] achieves beat-synchronized dance generation under multiple musical conditions. Lastly, the BADM [66] merges autoregressive and diffusion models, producing coherent, music-aligned dance sequences. Together, these works illustrate the fields progression toward adaptable, high-fidelity dance generation tightly integrated with musical features. Multi-Task Motion Generation. Human motion generation has evolved through multi-modal approaches, enabling contextually adaptive synthesis across diverse inputs like music, text, and visual cues. TM2D [17] introduced bimodal framework integrating music and text for 3D dance generation, using VQ-VAE to encode motion in shared latent space for flexible control. MotionCraft [5] builds on this by offering whole-body motion generation with adaptable multi-modal controls, from high-level semantics to specific joint details. MCM [39] further employs transformer-based model to process varied inputstext, audio, and videogenerating motion that reflects both style and context. LMM [73] extends these capabilities by integrating large-scale pre-trained models for complex human motion across modalities. Meanwhile, UDE [83] provides cohesive framework for motion synthesis, and UDE-2 [84] expands this to synchronize multi-part, multi-modal movements. MoFusion [12] complements these advances with diffusion-based denoising framework focused on robustness and quality in diverse motion styles. These works collectively guide the field toward adaptive, multi-modal, and context-aware human motion generation. 3. Methodology 3.1. Overview Motion Anything presents an innovative any-to-motion approach that generates controllable human motion by focusing on the dynamic and significant parts of human motion sequences and adaptively aligning with different condition modalities. As shown in Figure 3, Motion Anything can take different modalities either separately or simultaneously, enabling multimodal conditioning to enhance controllable motion generation instead of relying on single condition. The conditions are first encoded by text and audio encoders, then used to guide both masking and motion generation. We propose an attention-based masking approach that identifies the most significant parts of the motion corresponding to the conditions across both spatial and temporal dimensions by selecting high-attention scores as masking guidance. The guided mask tokens, along with condition embeddings, are then fed into masked transformer for guided mask restoration. We customize masked transformers into Temporal Adaptive Transformer and Spatial Aligning Transformer to adaptively align overall control and specific actions to the motion sequence. 3.2. Architecture Attention-based Masking. The core of attention-based masking involves guiding the condition modality to select key frames in the temporal dimension and key actions in the spatial dimension, allowing for the masking of these motions. As shown in the attention map in Figure 4, both temporal and spatial attention rely on either self-attention or cross-attention [56], depending on the condition modality. The condition serves as query Q, and motion serves as key and value , where the condition can be text, Figure 4. Attention map. The attention map provides direct visualization of our attention-based masking approach, which selectively masks regions in the motion sequence with high attention scores. audio, or combination of both. This process highlights specific regions in the attention map, indicating the key motions. We designed attention-based masking on both temporal and spatial dimensions to ensure the model focuses on learning key frames and joints in the motion sequence that correspond to the conditions, as shown in Figure 3 (a) and (c). This enables the model to learn more robust motion representations compared to traditional random masking [21, 64]. As shown in Algorithm 1, given motion sequence M, condition C, and masking ratio α, the model masks the top α% of attention scores, which represent the most important motions that are also most relevant to the corresponding condition. Algorithm 1 Attention-based Masking 1: Input: Motion M, Condition C, Masking Ratio α 2: Define: T: Text space, D: Audio space 3: Step 1: Define temporal Qtemp, Ktemp, Vtemp and spatial Qspatial, Kspatial, Vspatial 4: if then 5: Qtemp, Ktemp, Vtemp (C, M), (C, M), (C, M) 6: Qspatial, Kspatial, Vspatial C, M, 7: else if or then 8: Qtemp, Ktemp, Vtemp C, M, 9: Qspatial, Kspatial, Vspatial C, M, 10: end if 11: Step 2: Compute Attention Scores 12: Atemp = Attention(Qtemp, Ktemp, Vtemp) 13: Aspatial = Attention(Qspatial, Kspatial, Vspatial) 14: Step 3: Apply Masking 15: Sort Atemp and mask top α percent: masktemp = {i Atemp,i in top α%} 16: Sort Aspatial and mask top α percent: maskspatial = {i Aspatial,i in top α%} 17: Output: Masked motion sequence Mmasked Temporal Adaptive Transformer. The Temporal Adaptive Transformer (TAT) aligns the temporal tokens of the motion sequence with the temporal condition by dynamically adjusting its attention calculation according to the modality of the condition. This enables the TAT to align key frames of motion with keywords in text and beats in music. As shown in Algorithm 2 and Figure 3 (d), after attention-based masking, the key frames of the motion sequence are masked. The TAT then learns the motion representation by restoring the masked frames with guidance from the condition. If the condition consists only of text, it contains single token in the temporal dimension from CLIP, making self-attention in the temporal dimension more suitable. Otherwise, the motion sequence serves as Q, and the condition serves as KV , performing cross-attention to align the temporal information of the motion with music or the combination of music and text. This enables the Temporal Adaptive Transformer to become more adaptable and robust for different modalities of input conditions. Algorithm 2 Temporal Adaptive Transformer 1: Input: Motion M, Condition C, Mask Ratio α 2: Define: T: Text space, A: Audio space 3: Step 1: Apply Attention-based Masking to 4: Mmasked Attention-based Masking(M, C, α) 5: Step 2: Define Q, K, 6: if then 7: Q, K, (C, Mmasked), (C, Mmasked), (C, Mmasked) 8: else if or then 9: Q, K, Mmasked, C, 10: end if 11: Step 3: Compute Attention Scores 12: Mrestored = Attention(Q, K, ) 13: Output: Restored Motion Sequence Mrestored Spatial Aligning Transformer. In the Spatial Aligning Transformer (SAT), both the condition and motion embeddings are rearranged to expose the spatial dimension. As shown in Algorithm 3 and Figure 3 (d), during attentionbased masking, the key action in each frame, which refers to the key motion of specific body part in the spatial dimension, is masked. The SAT restores this feature with the guidance of the spatial condition. Aligning the spatial pose in each frame with the spatial condition is essential, especially in text-to-motion generation, where certain keywords describe specific body parts. In music-to-dance generation, the spectrum of each audio frame indicates the music genre [32, 55], which is crucial for generating the appropriate type of dance. Algorithm 3 Spatial Aligning Transformer 1: Input: Motion M, Condition C, Mask Ratio α 2: Step 1: Apply Attention-based Masking to 3: 4: Step 2: Define Q, K, 5: Q, K, 6: Step 3: Compute Attention Scores 7: restored = Attention(Q, K, ) 8: Output: Restored Motion Sequence masked Attention-based Masking(M, C, α) masked, C, restored 4. Experiments 4.1. Datasets and Evaluation Matrices TMD Dataset. Our Text-Music-Dance (TMD) dataset introduces pioneering benchmark with 2,153 pairs of text, music, and motion. We extract dance motions and corresponding text annotations from Motion-X [38], including AIST++ [34] and other datasets. For motion-text pairs without music, we generate corresponding music by implementing Stable Audio Open [14] with beat adjustment and evaluate the generated music through human expert assessments, ensuring inter-rater reliability. Public Benchmarks. To ensure fair comparison, we evaluate our method against both specialized and unified motion generation approaches on standard benchmarks including HumanML3D [19] and KIT-ML [46] for text-to-motion generation, and AIST++ [34] for music-to-dance generation. Evaluation Matrices. We adapt standard evaluation metrics to assess various aspects of our experiments. For text-to-motion generation, we implement FID and precision to quantify the realism and robustness of generated motions, MultiModal Distance to measure motiontext alignment, and the diversity metric to calculate variance in motion features. Additionally, we apply the multimodality (MModality) metric to evaluate diversity among motions sharing the same text description. For music-todance generation, we follow AIST++ [34] to evaluate generated dances from three perspectives: quality, diversity, and music-motion alignment. For quality, we calculate FID between the generated dance and motion sequence features (kinetic, FIDk, and geometric, FIDg) using the toolbox in [18]. For diversity, we compute the average feature distance as in AIST++ [34]. For alignment, we calculate the Beat Align Score (BAS) as the average temporal distance between music beats and their closest dance beats. 4.2. Model and Implementation Details Our model consists of 2 TAT and 2 SAT layers, with 12.65M parameters and 137.35 GFLOPs. The learning rate increases to 2 104 after 2000 iterations using linear warm-up schedule for all models. The mini-batch size is set to 512 for training the VQ-VAE tokenizer and 64 for training the masked transformers. All experiments were conducted on an Intel Xeon Platinum 8360Y CPU at 2.40GHz, equipped with single NVIDIA A100 40GB GPU and 32GB of RAM. 4.3. Comparative Study Text-to-Motion. We compared our method with other state-of-the-art approaches on both HumanML3D [19] and KIT-ML [46]. The results in Table 2 demonstrate that our method consistently outperforms specialized text-to-motion models and surpasses recent multi-task methods. Music-to-Dance. To highlight the music-to-dance capability of our method, we conducted evaluations on AIST++ [34]. The results in Table 3 indicate that our method surpasses previous state-of-the-art specialized and unified approaches, demonstrating superior motion quality, enhanced diversity, and better beat alignment in music-to-dance genDatasets Method Ground Truth TM2D [17] MotionCraft [5] ReMoDiffuse [70] MMM [45] DiverseMotion [41] BAD [22] BAMM [44] MCM [39] MoMask [21] LMM [73] MoGenTS [64] Motion Anything (Ours) Ground Truth ReMoDiffuse [70] MMM [45] DiverseMotion [41] BAD [22] BAMM [44] MoMask [21] LMM [73] MoGenTS [64] Motion Anything (Ours) Human ML3D [19] KITML [46] Top 1 0.511.003 0.319.000 0.501.003 0.510.005 0.504.003 0.515.003 0.517.002 0.525.002 0.502.002 0.521.002 0.525.002 0.529.003 0.546.003 0.424.005 0.427.014 0.404.005 0.416.005 0.417.006 0.438.009 0.433.007 0.430.015 0.445.006 0.449.007 Precision Top 2 0.703.003 - 0.697.003 0.698.006 0.696.003 0.706.002 0.713.003 0.720.003 0.692.004 0.713.002 0.719.002 0.719.002 0.735.002 0.649.006 0.641.004 0.621.005 0.637.008 0.631.006 0.661.009 0.656.005 0.653.017 0.671.006 0.678. Top 3 0.797.002 - 0.796.002 0.795.004 0.794.002 0.802.002 0.808.003 0.814.003 0.788.006 0.807.002 0.811.002 0.812.002 0.829.002 0.779.006 0.765.055 0.744.004 0.760.011 0.750.006 0.788.005 0.781.005 0.779.014 0.797.005 0.802.006 FID MultiModal Dist Diversity MultiModality 0.002.000 1.021.000 0.173.002 0.103.004 0.080.003 0.072.004 0.065.003 0.055.002 0.053.007 0.045.002 0.040.002 0.033.001 0.028. 0.031.004 0.155.006 0.316.028 0.468.098 0.221.012 0.183.013 0.204.011 0.137.023 0.143.004 0.131.003 2.974.008 4.098.000 3.025.008 2.974.016 2.998.007 2.941.007 2.901.008 2.919.008 3.037.003 2.958.008 2.943.012 2.867.006 2.859.010 2.788.012 2.814.012 2.977.019 2.892.041 2.941.025 2.723.026 2.779.022 2.791.018 2.711.024 2.705.024 9.503.065 9.513.000 9.543.098 9.018.075 9.411.058 9.683.102 9.694.068 9.717.089 9.585.082 - 9.814.076 9.570.077 9.521.083 11.08.097 10.80.105 10.91.101 10.87.101 11.00.100 11.01.094 - 11.24.103 10.92.090 10.94.098 - 4.139.000 - 1.795.043 1.164.041 1.869.089 1.194.044 1.687.051 0.810.023 1.241.040 2.683.054 - 2.705. - 1.239.028 1.232.039 2.062.079 1.170.047 1.609.065 1.131.043 1.885.127 - 1.374.069 Table 2. Quantitative comparison on HumanML3D [19] and KIT-ML [46]. The best and runner-up values are bold and underlined. The right arrow indicates that closer values to ground truth are better. Multimodal motion generation methods are highlighted in blue. Method Ground Truth TSMT [33] Dance Revolution [24] DanceNet [86] MoFusion [12] EDGE [54] Lodge [36] FACT [34] Bailando [51] TM2D [17] BADM [66] LMM [73] Bailando++ [52] UDE [83] MCM [39] Motion Anything (Ours) Motion Quality Motion Diversity FIDk FIDg Divk Divg BAS 4 demonstrate superior performance by our method when handling different modalities simultaneously. 17. 86.43 73.42 69.18 50.31 42.16 37.09 35.35 28.16 23.94 - 22.08 17.59 17.25 15.57 17.22 10.60 43.46 25.92 25.49 - 22.12 18.79 22.11 9.62 9.53 - 21.97 10.10 8.69 25.85 8.56 8. 6.85 3.52 2.86 9.09 3.96 5.58 5.94 7.83 7.69 8.29 9.85 8.64 7.78 6.50 7.45 3.32 4.87 2.85 - 4.61 4.85 6.18 6.34 4.53 6.76 6.72 6.50 5.81 5.74 0.2374 0.1607 0.1950 0.1430 0.2530 0.2334 0.2423 0.2209 0.2332 0.2127 0.2366 0.2249 0.2720 0.2310 0.2750 9. 6.79 0.2757 Motion Quality Motion Diversity Method FIDk FIDg Divk Divg BAS MMDist MModality Ground Truth TM2D [17] MotionCraft [5] 20.72 26.78 24.21 11. 12.04 26.39 7.42 6.25 7.02 6.94 4.41 5.79 0. 0.2001 0.2036 Motion Anything 21.46 11.44 7.04 6. 0.2094 5.07 6.13 5.82 5.34 - 2.232 2. 2.424 Table 4. Quantitative comparison on TMD. The best and runnerup values are bold and underlined. Table 3. Quantitative comparison on AIST++ [34]. The best and runner-up values are bold and underlined. Multimodal motion generation methods are highlighted in blue. eration. Text-and-Music-to-Dance. For paired text-and-music-todance (TM2D) generation, we evaluated open-source multimodal motion generation methods by directly combining their condition embeddings and compared them with our approach on the TMD dataset. The results in Table 4.4. Ablation Study Masking Strategy. To evaluate the effectiveness of our attention-based masking approach across both temporal and spatial dimensions, we conducted comprehensive experiments on HumanML3D [19], comparing it to other masking strategies such as random masking [21, 64], KMeans [40], GMM [49], confidence-based masking [45], and densitybased masking [75]. The results in Table 5 demonstrate that our attention-based masking outperforms these other strategies in human motion generation, yielding promising results for learning robust motion representations. Figure 5. Qualitative evaluation on text-to-motion generation. We qualitatively compared the visualizations generated by our method with those produced by BAD [22], BAMM [44], and MoMask [21]. Method Ground Truth Random Masking [21] KMeans [40] GMM [49] Confidence-based Masking [45] Density-based Masking [75] Attention-based Masking Top 1 0.511.003 0.522.004 0.528.003 0.531.002 0.524.007 0.538.005 0.546. Precision Top 2 0.703.003 0.714.003 0.709.004 0.721.004 0.731.001 0.733.002 0.735.002 Top 3 0.797.002 0.818.006 0.823.006 0.826.008 0.818.004 0.819.006 0.829.002 0.002.000 0.049.023 0.042.032 0.039.021 0.047.023 0.031.035 0.028.005 2.974.008 2.945.027 2.871.035 2.887.024 2.928.009 2.913.021 2.859.010 9.503.065 9.633.218 9.549.173 9.602.138 9.530.095 9.518.138 9.521. - 2.538.035 2.548.023 2.488.031 2.574.039 2.608.043 2.705.068 FID MM Dist Diversity MModality Method Table 5. Ablation study of the masking strategy on HumanML3D [19]. The best and runner-up values are bold and underlined. The right arrow indicates that closer values to ground truth are better. Masking Ratio. To demonstrate the robustness of our method across various hyperparameters and the impact of different masking ratios on overall performance, we conducted comprehensive ablation studies with different attention-based masking ratios on HumanML3D [19], as shown in Table 6. We conducted an ablation study on the masking ratio for temporal and spatial attention-based masking separately. The results show that our method is relatively robust across different masking ratios, with 30% identified as the superior setting in our paper. Cross-modal TAT for Text-to-Motion. To verify the necessity of self-attention in the Temporal Adaptive Transformer (TAT) for text-to-motion generation, we modified cross-modal attention layer in TAT to resemble the setup implemented for music-to-dance and text&music-to-dance generation. The results in Table 7 indicate that the crossmodal attention layer performs worse compared to the selfattention layer in TAT for text-to-motion on HumanML3D Ground Truth T:15% S:15% T:15% S:30% T:15% S:50% Top 1 0.511.003 0.523.005 0.529.002 0.530.002 0.535.007 T:30% S:15% T:30% S:30% 0.546.003 0.541.004 T:30% S:50% 0.525.005 0.525.007 0.524.006 T:50% S:15% T:50% S:30% T:50% S:50% Precision Top 2 0.703.003 0.716.002 0.718.005 0.715.007 0.728.001 0.735.002 0.726.003 0.720.003 0.723.004 0.712.003 Top 3 0.797.002 0.818.005 0.822.003 0.820.007 0.823.004 0.829.002 0.821.005 0.820.009 0.819.007 0.822.006 FID MM Dist Diversity MModality 0.002.000 0.047.034 0.044.046 0.045.035 0.036.027 0.028.005 0.033.035 0.043.028 0.040.042 0.048. 2.974.008 2.920.026 2.914.023 2.918.019 2.873.037 2.859.010 2.926.054 2.940.044 2.937.063 2.943.037 9.503.065 9.625.145 9.573.163 9.632.217 9.527.116 9.521.083 9.519.196 9.620.134 9.617.115 9.623.153 - 2.580.064 2.631.024 2.611.026 2.709.027 2.705.068 2.710.037 2.584.063 2.701.031 2.620.025 Table 6. Ablation study of masking ratio on HumanML3D [19]. The best and runner-up values are bold and underlined. The right arrow indicates that closer values to ground truth are better. Method Ground Truth Cross-modal Attention Motion Anything Precision Top 1 0.511.003 0.347.006 0.546.003 Top 2 0.703.003 0.587.007 0.735.002 Top 3 0.797.002 0.726.005 0.829.002 FID MM Dist Diversity MModality 0.002.000 0.583.024 0.028.005 2.974.008 3.356.022 2.859.010 9.503.065 9.032.153 9.521.083 - 2.153.056 2.705. Table 7. Ablation study of the TAT on HumanML3D [19]. The best values are bold. The right arrow indicates that closer values to ground truth are better. [19]. This underperformance can be attributed to the fact that the text embeddings from CLIP consist of only single token along the temporal dimension. Consequently, the temporal dimension does not align with the motion embeddings, making it unsuitable for effective cross-modal fusion with motion in the temporal context. Effectiveness of Multimodal Conditioning. To evaluate the effectiveness of multimodal conditioning, we examine whether paired text descriptions enable more controllable music-to-dance generation and whether our any-to-motion Figure 6. Qualitative evaluation on music-to-dance generation. We qualitatively compared the visualizations generated by our method with those produced by EDGE [54], Lodge [36], and Bailando [51]. method can seamlessly leverage both conditions. We conduct ablation studies on multimodal conditioning using our TMD dataset, comparing it with the single-condition setting using only music. The results in Table 8 demonstrate that introducing multimodal conditioning is more effective than using single modality, and our model can effectively and adaptively handle multimodal conditions. Motion Quality Motion Diversity FIDk FIDg Divk Divg BAS MMDist MModality Method Ground Truth 20.72 11. Motion Anything w/o text Motion Anything 25.07 21.46 14.23 11.44 7.42 6.95 7.04 6. 6.01 6.15 0.2105 0.2077 0.2094 5.07 6.24 5.34 - 2.398 2.424 Table 8. Single-modal vs. multimodal generation on TMD dataset. Number of Layers. To investigate the impact of our model by varying the number of layers in masked transformers, we conduct ablation studies on HumanML3D. Table 9 demonstrates the robustness of our model across different layer configurations. Method Ground Truth = 2 = 4 = 6 = Top 1 0.511.003 0.521.006 0.546.003 0.541.007 0.544.009 Precision Top 2 0.703.003 0.725.008 0.735.002 0.733.002 0.734.003 Top 3 0.797.002 0.819.005 0.829.002 0.826.010 0.826.007 FID MM Dist Diversity MModality 0.002.000 0.079.019 0.028.005 0.029.004 0.028.014 2.974.008 2.916.033 2.859.010 2.861.010 2.851.011 9.503.065 9.598.117 9.521.083 9.517.094 9.519.057 - 2.503.024 2.705.068 2.673.019 2.711.032 Table 9. Ablation study of number of layers on HumanML3D [19]. 4.5. Qualitative Evaluation Text-to-Motion Generation. To qualitatively evaluate our performance in text-to-motion generation, we compare the visualizations generated by our method with those produced by previous state-of-the-art methods specializing in text-tomotion generation, including BAD [22], BAMM [44], and MoMask [21]. The text prompts are customized based on the HumanML3D [19] test set. As shown in Figure 5 and video demos, our method generates motions with superior quality, greater diversity, and better alignment between text and motion compared to the previous state-of-the-art methods. Music-to-Dance Generation. To evaluate the quality of our music-to-dance generation, we compare the dances generated by our method against those from state-of-the-art approaches, including EDGE [54], Lodge [36], and Bailando [51]. Training on AIST++ [34], we ensure the evaluation reflects diverse musical styles. As shown in Figure 6 and demonstrated in the accompanying videos, our method produces dances with better visual quality and achieves superior alignment with the beat and genre of the music, surpassing previous state-of-the-art techniques. For more qualitative evaluations and videos, please refer to the supplementary materials. 5. Conclusion In conclusion, Motion Anything presents significant forward to motion generation by enabling adaptive and controllable multimodal conditioning. Our model introduces the attention-based masking within an autoregressive framework to focus on key frames and actions, addressing the challenge of prioritizing dynamic frames and body parts. Additionally, it bridges the gap in multimodal motion generation by aligning different input modalities both temporally and spatially, enhancing control and coherence. To further advance research in this area, we introduce the TextMusic-Dance (TMD) dataset, pioneering benchmark with paired music and text. Extensive experiments demonstrate that our method outperforms prior approaches, achieving substantial improvements across multiple benchmarks. By tackling these challenges, Motion Anything establishes new paradigm for motion generation, offering more versatile and precise framework for motion generation."
        },
        {
            "title": "References",
            "content": "[1] Unlock the power of 3d design with tripo: The aipowered 3d model generator, 2024. 2, 3 [2] Sherwin Bahmani, Ivan Skorokhodov, Victor Rong, Gordon Wetzstein, Leonidas Guibas, Peter Wonka, Sergey Tulyakov, Jeong Joon Park, Andrea Tagliasacchi, and David Lindell. 4d-fy: Text-to-4d generation using hybrid score distillation sampling. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 79968006, 2024. 2 [3] Sherwin Bahmani, Xian Liu, Wang Yifan, Ivan Skorokhodov, Victor Rong, Ziwei Liu, Xihui Liu, Jeong Joon Park, Sergey Tulyakov, Gordon Wetzstein, et al. Tc4d: Trajectory-conditioned text-to-4d generation. In European Conference on Computer Vision, pages 5372. Springer, 2025. 3 [4] Ilya Baran and Jovan Popovic. Automatic rigging and animation of 3d characters. ACM Transactions on graphics (TOG), 26(3):72es, 2007. [5] Yuxuan Bian, Ailing Zeng, Xuan Ju, Xian Liu, Zhaoyang Zhang, Wei Liu, and Qiang Xu. Adding multi-modal controls to whole-body human motion generation. arXiv preprint arXiv:2407.21136, 2024. 2, 3, 4, 6, 5 [6] Andreas Blattmann, Tim Dockhorn, Sumith Kulal, Daniel Mendelevitch, Maciej Kilian, Dominik Lorenz, Yam Levi, Zion English, Vikram Voleti, Adam Letts, et al. Stable video diffusion: Scaling latent video arXiv preprint diffusion models to large datasets. arXiv:2311.15127, 2023. 2 [7] Peter Borosan, Ming Jin, Doug DeCarlo, Yotam Gingold, and Andrew Nealen. Rigmesh: automatic rigging for part-based shape modeling and deformation. ACM Transactions on Graphics (TOG), 31(6):19, 2012. 2 [8] Zenghao Chai, Chen Tang, Yongkang Wong, and Mohan Kankanhalli. Star: Skeleton-aware text-based 4d avatar generation with in-network motion retargeting. arXiv preprint arXiv:2406.04629, 2024. 3 [9] Ling-Hao Chen, Wenxun Dai, Xuan Ju, Shunlin Lu, and Lei Zhang. Motionclr: Motion generation and training-free editing via understanding attention mechanisms. arXiv preprint arXiv:2410.18977, 2024. [10] Xin Chen, Biao Jiang, Wen Liu, Zilong Huang, Bin Fu, Tao Chen, and Gang Yu. Executing your comIn Promands via motion diffusion in latent space. ceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 1800018010, 2023. 2, 5 [11] Yushuo Chen, Zerong Zheng, Zhe Li, Chao Xu, and Yebin Liu. Meshavatar: Learning high-quality triangular human avatars from multi-view videos. arXiv preprint arXiv:2407.08414, 2024. 2 [12] Rishabh Dabral, Muhammad Hamza Mughal, Vladislav Golyanik, and Christian Theobalt. Mofusion: framework for denoising-diffusion-based In Proceedings of the IEEE/CVF motion synthesis. conference on computer vision and pattern recognition, pages 97609770, 2023. 3, 4, 6 [13] Wenxun Dai, Ling-Hao Chen, Jingbo Wang, Jinpeng Liu, Bo Dai, and Yansong Tang. Motionlcm: Realtime controllable motion generation via latent consistency model. arXiv preprint arXiv:2404.19759, 2024. 5 [14] Zach Evans, Julian Parker, CJ Carr, Zack Zukowski, Josiah Taylor, and Jordi Pons. Stable audio open. arXiv preprint arXiv:2407.14358, 2024. 5, 2 [15] Andrew Feng, Dan Casas, and Ari Shapiro. Avatar reshaping and automatic rigging using deformable In Proceedings of the 8th ACM SIGGRAPH model. Conference on Motion in Games, pages 5764, 2015. [16] Xuehao Gao, Yang Yang, Zhenyu Xie, Shaoyi Du, Zhongqian Sun, and Yang Wu. Guess: Gradually enriching synthesis for text-driven human motion generation. IEEE Transactions on Visualization and Computer Graphics, 2024. 5 [17] Kehong Gong, Dongze Lian, Heng Chang, Chuan Guo, Zihang Jiang, Xinxin Zuo, Michael Bi Mi, and Xinchao Wang. Tm2d: Bimodality driven 3d dance generation via music-text integration. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 99429952, 2023. 2, 3, 6, 4, 5 [18] Deepak Gopinath and Jungdam Won. Fairmotiontools to load, process and visualize motion capture data, 2020. 5 [19] Chuan Guo, Shihao Zou, Xinxin Zuo, Sen Wang, Wei Ji, Xingyu Li, and Li Cheng. Generating diverse and natural 3d human motions from text. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 51525161, 2022. 2, 5, 6, 7, 8, 1 [20] Chuan Guo, Xinxin Zuo, Sen Wang, and Li Cheng. Tm2t: Stochastic and tokenized modeling for the reciprocal generation of 3d human motions and texts. In European Conference on Computer Vision, pages 580597. Springer, 2022. 5 [21] Chuan Guo, Yuxuan Mu, Muhammad Gohar Javed, Sen Wang, and Li Cheng. Momask: Generative masked modeling of 3d human motions. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 19001910, 2024. 2, 3, 4, 6, 7, 8, [22] Rohollah Hosseyni, Ali Ahmad Rahmani, Jamal Seyedmohammadi, Sanaz Seyedin, and Arash Mohammadi. Bad: Bidirectional auto-regressive diffuarXiv preprint sion for text-to-motion generation. arXiv:2409.10847, 2024. 2, 6, 7, 8, 5 [23] Liangxiao Hu, Hongwen Zhang, Yuxiang Zhang, Boyao Zhou, Boning Liu, Shengping Zhang, and Liqiang Nie. Gaussianavatar: Towards realistic human avatar modeling from single video via animatable 3d gaussians. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 634644, 2024. 2 [24] Ruozi Huang, Huang Hu, Wei Wu, Kei Sawada, Mi Zhang, and Daxin Jiang. Dance revolution: Long-term dance generation with music via curriculum learning. In International Conference on Learning Representations, 2021. 3, 6 [25] Yiheng Huang, Hui Yang, Chuanchen Luo, Yuxi Wang, Shibiao Xu, Zhaoxiang Zhang, Man Zhang, and Junran Peng. Stablemofusion: Towards robust and efficient diffusion-based motion generation framework. arXiv preprint arXiv:2405.05691, 2024. 5 [26] Zikai Huang, Xuemiao Xu, Cheng Xu, Huaidong Zhang, Chenxi Zheng, Jing Qin, and Shengfeng He. Beat-it: Beat-synchronized multi-condition 3d dance generation. arXiv preprint arXiv:2407.07554, 2024. 3 [27] Biao Jiang, Xin Chen, Wen Liu, Jingyi Yu, Gang Yu, and Tao Chen. Motiongpt: Human motion as foreign language. Advances in Neural Information Processing Systems, 36:2006720079, 2023. 5 namic object generation from monocular video. arXiv preprint arXiv:2311.02848, 2023. [29] Peng Jin, Yang Wu, Yanbo Fan, Zhongqian Sun, Wei Yang, and Li Yuan. Act as you wish: Fine-grained control of motion diffusion model with hierarchical semantic graphs. Advances in Neural Information Processing Systems, 36, 2024. 5 [30] Hanyang Kong, Kehong Gong, Dongze Lian, Michael Bi Mi, and Xinchao Wang. Priority-centric human motion generation in discrete latent space. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 1480614816, 2023. 5 [31] Binh Huy Le and Zhigang Deng. Smooth skinning decomposition with rigid bones. ACM Transactions on Graphics (TOG), 31(6):110, 2012. 3 [32] Chang-Hsing Lee, Jau-Ling Shih, Kun-Ming Yu, and Hwai-San Lin. Automatic music genre classification based on modulation spectral analysis of spectral and cepstral features. IEEE Transactions on Multimedia, 11(4):670682, 2009. 5 [33] Jiaman Li, Yihang Yin, Hang Chu, Yi Zhou, Tingwu Wang, Sanja Fidler, and Hao Li. Learning to generate diverse dance motions with transformer. arXiv preprint arXiv:2008.08171, 2020. 3, 6 [34] Ruilong Li, Shan Yang, David Ross, and Angjoo Kanazawa. Ai choreographer: Music conditioned 3d dance generation with aist++. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 1340113412, 2021. 2, 3, 5, 6, [35] Ronghui Li, Hongwen Zhang, Yachao Zhang, Yuxiang Zhang, Youliang Zhang, Jie Guo, Yan Zhang, Xiu Li, and Yebin Liu. Lodge++: High-quality and long dance generation with vivid choreography patterns. arXiv preprint arXiv:2410.20389, 2024. 3 [36] Ronghui Li, YuXiang Zhang, Yachao Zhang, Hongwen Zhang, Jie Guo, Yan Zhang, Yebin Liu, and Xiu Li. Lodge: coarse to fine diffusion network for long dance generation guided by the characteristic dance In Proceedings of the IEEE/CVF Conprimitives. ference on Computer Vision and Pattern Recognition, pages 15241534, 2024. 2, 3, 6, 8 [37] Zhiqi Li, Yiming Chen, and Peidong Liu. Dreammesh4d: Video-to-4d generation with sparsecontrolled representation. gaussian-mesh arXiv preprint arXiv:2410.06756, 2024. 2 hybrid [38] Jing Lin, Ailing Zeng, Shunlin Lu, Yuanhao Cai, Ruimao Zhang, Haoqian Wang, and Lei Zhang. Motion-x: large-scale 3d expressive whole-body human motion dataset. Advances in Neural Information Processing Systems, 36, 2024. [28] Yanqin Jiang, Li Zhang, Jin Gao, Weimin Hu, and Yao Yao. Consistent4d: Consistent 360 {deg} dy- [39] Zeyu Ling, Bo Han, Yongkang Wongkan, Han Lin, Mohan Kankanhalli, and Weidong Geng. Mcm: Multi-condition motion synthesis framework. arXiv preprint arXiv:2404.12886, 2024. 3, 4, 6, 5 [40] Stuart Lloyd. Least squares quantization in pcm. IEEE transactions on information theory, 28(2):129137, 1982. 6, 7 [41] Yunhong Lou, Linchao Zhu, Yaxiong Wang, Xiaohan Wang, and Yi Yang. Diversemotion: Towards diverse human motion generation via discrete diffusion. arXiv preprint arXiv:2309.01372, 2023. 6, 5 [42] Natapon Pantuwong and Masanori Sugimoto. fully automatic rigging algorithm for 3d character animaIn SIGGRAPH Asia 2011 Posters, pages 11. tion. 2011. [43] Mathis Petrovich, Michael Black, and Gul Varol. Temos: Generating diverse human motions from texIn European Conference on Comtual descriptions. puter Vision, pages 480497. Springer, 2022. 5 [44] Ekkasit Pinyoanuntapong, Muhammad Usama Saleem, Pu Wang, Minwoo Lee, Srijan Das, and Chen Chen. Bamm: Bidirectional autoregressive motion model. arXiv preprint arXiv:2403.19435, 2024. 2, 3, 6, 7, 8, 5 [45] Ekkasit Pinyoanuntapong, Pu Wang, Minwoo Lee, and Chen Chen. Mmm: Generative masked motion model. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 15461555, 2024. 2, 6, 7, 5 [46] Matthias Plappert, Christian Mandery, and Tamim Asfour. The kit motion-language dataset. Big data, 4(4): 236252, 2016. 5, 6, 1 [47] Jiawei Ren, Liang Pan, Jiaxiang Tang, Chi Zhang, Ang Cao, Gang Zeng, and Ziwei Liu. DreamgausarXiv sian4d: Generative 4d gaussian splatting. preprint arXiv:2312.17142, 2023. [48] Jiawei Ren, Kevin Xie, Ashkan Mirzaei, Hanxue Liang, Xiaohui Zeng, Karsten Kreis, Ziwei Liu, Antonio Torralba, Sanja Fidler, Seung Wook Kim, et al. L4gm: Large 4d gaussian reconstruction model. arXiv preprint arXiv:2406.10324, 2024. 2 [49] Douglas Reynolds et al. Gaussian mixture models. Encyclopedia of biometrics, 741(659-663), 2009. 6, 7 [50] Yichun Shi, Peng Wang, Jianglong Ye, Mai Long, Kejie Li, and Xiao Yang. Mvdream: Multi-view diffusion for 3d generation. arXiv preprint arXiv:2308.16512, 2023. 2 [51] Li Siyao, Weijiang Yu, Tianpei Gu, Chunze Lin, Quan Wang, Chen Qian, Chen Change Loy, and Ziwei Liu. Bailando: 3d dance generation by actor-critic gpt In Proceedings of the with choreographic memory. IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 1105011059, 2022. 2, 3, 6, 8 [52] Li Siyao, Weijiang Yu, Tianpei Gu, Chunze Lin, Quan Wang, Chen Qian, Chen Change Loy, and Ziwei Liu. Bailando++: 3d dance gpt with choreographic memory. IEEE Transactions on Pattern Analysis and Machine Intelligence, 2023. 3, 6 [53] Guy Tevet, Sigal Raab, Brian Gordon, Yoni Shafir, Daniel Cohen-or, and Amit Haim Bermano. Human motion diffusion model. In The Eleventh International Conference on Learning Representations, 2022. 2, 5 [54] Jonathan Tseng, Rodrigo Castellon, and Karen Liu. Edge: Editable dance generation from music. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 448458, 2023. 3, 6, [55] George Tzanetakis and Perry Cook. Musical genre classification of audio signals. IEEE Transactions on speech and audio processing, 10(5):293302, 2002. 5 [56] Vaswani. Attention is all you need. Advances in Neural Information Processing Systems, 2017. 4 [57] Shaofei Wang, Bozidar Antic, Andreas Geiger, and Intrinsicavatar: Physically based inSiyu Tang. verse rendering of dynamic humans from monocular videos via explicit ray tracing. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 18771888, 2024. 2 [58] Wenjing Wang, Huan Yang, Zixi Tuo, Huiguo He, Junchen Zhu, Jianlong Fu, and Jiaying Liu. Swap attention in spatiotemporal diffusions for text-to-video generation. arXiv preprint arXiv:2305.10874, 2023. 2 [59] Yin Wang, Zhiying Leng, Frederick WB Li, ShunCheng Wu, and Xiaohui Liang. Fg-t2m: Fine-grained text-driven human motion generation via diffusion model. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 2203522044, 2023. 5 [60] Yuan Wang, Di Huang, Yaqi Zhang, Wanli Ouyang, Jile Jiao, Xuetao Feng, Yan Zhou, Pengfei Wan, Shixiang Tang, and Dan Xu. Motiongpt-2: general-purpose motion-language model for moarXiv preprint tion generation and understanding. arXiv:2410.21747, 2024. 5 [61] Qi Wu, Yubo Zhao, Yifan Wang, Yu-Wing Tai, and Chi-Keung Tang. Motionllm: Multimodal motionlanguage learning with large language models. arXiv preprint arXiv:2405.17013, 2024. 5 [62] Zijie Wu, Chaohui Yu, Yanqin Jiang, Chenjie Cao, Fan Wang, and Xiang Bai. Sc4d: Sparse-controlled video-to-4d generation and motion transfer. In European Conference on Computer Vision, pages 361379. Springer, 2025. 2 [63] Yuyang Yin, Dejia Xu, Zhangyang Wang, Yao Zhao, 4dgen: Grounded 4d content and Yunchao Wei. generation with spatial-temporal consistency. arXiv preprint arXiv:2312.17225, 2023. 2 [64] Weihao Yuan, Weichao Shen, Yisheng He, Yuan Dong, Xiaodong Gu, Zilong Dong, Liefeng Bo, and Qixing Huang. Mogents: Motion generation based on spatial-temporal joint modeling. arXiv preprint arXiv:2409.17686, 2024. 2, 3, 4, 6, 5 [65] Yifei Zeng, Yanqin Jiang, Siyu Zhu, Yuanxun Lu, Youtian Lin, Hao Zhu, Weiming Hu, Xun Cao, and Yao Yao. Stag4d: Spatial-temporal anchored generative 4d gaussians. In European Conference on Computer Vision, pages 163179. Springer, 2025. 2 [66] Canyu Zhang, Youbao Tang, Ning Zhang, Ruei-Sung Lin, Mei Han, Jing Xiao, and Song Wang. Bidirectional autoregessive diffusion model for dance generation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 687 696, 2024. 3, 6 [67] Hao Zhang, Di Chang, Fang Li, Mohammad Soleymani, and Narendra Ahuja. Magicpose4d: Crafting articulated models with appearance and motion control. arXiv preprint arXiv:2405.14017, 2024. 3, 2 [68] Haiyu Zhang, Xinyuan Chen, Yaohui Wang, Xihui Liu, Yunhong Wang, and Yu Qiao. 4diffusion: Multiview video diffusion model for 4d generation. arXiv preprint arXiv:2405.20674, 2024. 2 [69] Jianrong Zhang, Yangsong Zhang, Xiaodong Cun, Yong Zhang, Hongwei Zhao, Hongtao Lu, Xi Shen, and Ying Shan. Generating human motion from textual descriptions with discrete representations. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 1473014740, 2023. [70] Mingyuan Zhang, Xinying Guo, Liang Pan, Zhongang Cai, Fangzhou Hong, Huirong Li, Lei Yang, and Ziwei Liu. Remodiffuse: Retrieval-augmented motion diffuIn Proceedings of the IEEE/CVF Intersion model. national Conference on Computer Vision, pages 364 373, 2023. 2, 3, 6, 5 [71] Mingyuan Zhang, Huirong Li, Zhongang Cai, Jiawei Ren, Lei Yang, and Ziwei Liu. Finemogen: Finegrained spatio-temporal motion generation and editing. Advances in Neural Information Processing Systems, 36:1398113992, 2023. 5 [72] Mingyuan Zhang, Zhongang Cai, Liang Pan, Fangzhou Hong, Xinying Guo, Lei Yang, and Ziwei Liu. Motiondiffuse: Text-driven human motion generation with diffusion model. IEEE Transactions on Pattern Analysis and Machine Intelligence, 2024. 2, 5 [73] Mingyuan Zhang, Daisheng Jin, Chenyang Gu, Fangzhou Hong, Zhongang Cai, Jingfang Huang, Chongzhi Zhang, Xinying Guo, Lei Yang, Ying He, et al. Large motion model for unified multi-modal motion generation. arXiv preprint arXiv:2404.01284, 2024. 2, 3, 4, 6, 5 [74] Yaqi Zhang, Di Huang, Bin Liu, Shixiang Tang, Yan Lu, Lu Chen, Lei Bai, Qi Chu, Nenghai Yu, and Wanli Ouyang. Motiongpt: Finetuned llms are general-purpose motion generators. In Proceedings of the AAAI Conference on Artificial Intelligence, pages 73687376, 2024. [75] Zeyu Zhang, Hang Gao, Akide Liu, Qi Chen, Feng Chen, Yiran Wang, Danning Li, and Hao Tang. Kmm: Key frame mask mamba for extended motion generation. arXiv preprint arXiv:2411.06481, 2024. 3, 6, 7 [76] Zeyu Zhang, Akide Liu, Qi Chen, Feng Chen, Ian Reid, Richard Hartley, Bohan Zhuang, and Hao Tang. Infinimotion: Mamba boosts memory in transformer for arbitrary long motion generation. arXiv preprint arXiv:2407.10061, 2024. 3 [77] Zeyu Zhang, Yiran Wang, Biao Wu, Shuo Chen, Zhiyuan Zhang, Shiya Huang, Wenbo Zhang, Meng Fang, Ling Chen, and Yang Zhao. Motion avatar: Generate human and animal avatars with arbitrary motion. arXiv preprint arXiv:2405.11286, 2024. 3, 2 [78] Zeyu Zhang, Akide Liu, Ian Reid, Richard Hartley, Bohan Zhuang, and Hao Tang. Motion mamba: Efficient and long sequence motion generation. In European Conference on Computer Vision, pages 265282. Springer, 2025. 2, 5 [79] Yuyang Zhao, Zhiwen Yan, Enze Xie, Lanqing Hong, Zhenguo Li, and Gim Hee Lee. Animate124: Animating one image to 4d dynamic scene. arXiv preprint arXiv:2311.14603, 2023. 2 [80] Yufeng Zheng, Xueting Li, Koki Nagano, Sifei Liu, Otmar Hilliges, and Shalini De Mello. unified approach for text-and image-guided 4d scene generation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 7300 7309, 2024. 2 [81] Chongyang Zhong, Lei Hu, Zihao Zhang, and Shihong Xia. Attt2m: Text-driven human motion generation with multi-perspective attention mechanism. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 509519, 2023. 5 [82] Wenyang Zhou, Zhiyang Dou, Zeyu Cao, Zhouyingcheng Liao, Jingbo Wang, Wenjia Wang, Yuan Liu, Taku Komura, Wenping Wang, and Lingjie Liu. Emdm: Efficient motion diffusion model for fast, high-quality motion generation. arXiv preprint arXiv:2312.02256, 2, 2023. [83] Zixiang Zhou and Baoyuan Wang. Ude: unified driving engine for human motion generation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 56325641, 2023. 2, 3, 4, 6 [84] Zixiang Zhou, Yu Wan, and Baoyuan Wang. unified framework for multimodal, multi-part human motion synthesis. arXiv preprint arXiv:2311.16471, 2023. 3, 4 [85] Wentao Zhu, Xiaoxuan Ma, Dongwoo Ro, Hai Ci, Jinlu Zhang, Jiaxin Shi, Feng Gao, Qi Tian, and Yizhou Wang. Human motion generation: survey. IEEE Transactions on Pattern Analysis and Machine Intelligence, 46(4):24302449, 2023. 2 [86] Wenlin Zhuang, Congyi Wang, Jinxiang Chai, Yangang Wang, Ming Shao, and Siyu Xia. Music2dance: Dancenet for music-driven dance generation. ACM Transactions on Multimedia Computing, Communications, and Applications (TOMM), 18(2):121, 2022. 3, 6 [87] Qiran Zou, Shangyuan Yuan, Shian Du, Yu Wang, Chang Liu, Yi Xu, Jie Chen, and Xiangyang Ji. Parco: arXiv Part-coordinating text-to-motion synthesis. preprint arXiv:2403.18512, 2024. Motion Anything: Any to Motion Generation"
        },
        {
            "title": "Supplementary Material",
            "content": "1. Full Comparison for Text-to-Motion To comprehensively showcase our methods performance in text-to-motion generation, we present full comparison with previous T2M approaches in Table 1. The results demonstrate that our method consistently outperforms others, achieving state-of-the-art performance on both HumanML3D [19] and KIT-ML [46] datasets. 2. User Study This study provides comprehensive evaluation of our motion generation. We assessed the real-world applicability of four motion videos generated by Motion Anything and baseline models, as evaluated by 50 participants through Google Forms survey. Figure 1 displays the User Interface (UI) used in our user study, showcasing 34 videos (Video 1 to 3/4), each featuring distinct motion animations from the same model, and four videos for comparing different models (Video to D). Participants evaluate these animations based on aspects such as motion accuracy and overall user experience. They rate each aspect from 1 (low) to 3 (high) to assess how well the animations mirror realworld movements and their engagement level. In the comparison section, participants select the model with the best performance. This evaluation aims to determine the realism and engagement effectiveness of each motion. The evaluation consisted of three groups of motions: text-to-motion, music-to-dance, and text-and-music-to-dance. The results are as follows: Text-to-Motion: Our motion quality rating is 2.84. Additionally, 86% of participants believe that our method demonstrates high-quality motion generation with minimal jitter, sliding, and unrealistic movements. Our motion diversity rating is 2.68. Additionally, 72% of participants believe that our method generates complex and diverse motions. Our text-motion alignment rating is 2.74. Additionally, 76% of participants believe that our method generates motion that is well-aligned with their text condition. 88% of participants believe that our method outperforms other methods. Music-to-Dance: Our dance quality rating is 2.82. Additionally, 82% of participants believe that our method demonstrates high-quality dance generation with minimal jitter, sliding, and unrealistic movements. Our dance diversity rating is 2.88. Additionally, 88% of participants believe that our method generates comFigure 1. User study form. The User Interface (UI) used in our user study. plex and diverse dance. Our music-dance alignment rating is 2.66. Additionally, 70% of participants believe that our method generates dance that is well-aligned with the music genre and beats. 74% of participants believe that our method outperforms other methods. Text and Music to Dance: Our dance quality rating is 2.74. Additionally, 76% of participants believe that our method demonstrates high-quality dance generation with minimal jitter, sliding, and unrealistic movements. Our multimodal condition rating is 2.88. Additionally, 88% of participants believe that text enhances musics conditioning in dance generation. Our text&music-dance aligment rating is 2.76. There are 80% participants believe that our method generates dance that is well-aligned with the both music and text. 78% of participants believe that our method outperforms other methods. 3. Model Efficiency We evaluate the inference efficiency of motion generation compared to various methods. The inference cost is calculated as the average inference time over 100 samples on one NVIDIA GeForce RTX 2080 Ti device. Compared to baseline methods, Motion Anything achieves an outstanding balance between generation quality and efficiency, as shown in Figure 2. Figure 2. Comparisons on FID and AIT. All tests are conducted on the same NVIDIA GeForce RTX 2080 Ti. The closer the model is to the origin, the better. Figure 3. 4D Avatar Generation. This approach enables 4D avatar generation conditioned on multimodal inputs, achievable with just single text prompt. 4. Application: 4D Avatar Generation One of the most significant applications for conditional motion generation is 4D avatar generation. Previous methods [11, 23, 28, 37, 47, 48, 57, 62, 63, 65] for 4D avatar generation reconstruct dynamic avatars using 4D Gaussians to model both spatial and temporal dimensions from video data. Other approaches [2, 68, 79, 80] integrate video diffusion [6, 58] with geometry-aware diffusion models [50] to ensure spatial consistency and achieve visually appealing appearance. However, these methods face two significant challenges: (1) limited diversity and control over motion [28, 37, 62], and (2) inconsistencies in the mesh appearance at different time points [47]. Therefore, we propose comprehensive feedforward approach for 4D avatar generation with only single prompt, leveraging both Motion Anything and off-the-shelf tools, as illustrated in Figure 3. single prompt serves as the input, when music is not provided as condition, Stable Audio Open [14] generates background music as the audio condition. The text and audio conditions are then processed by the multimodal motion generator to create high-quality motion sequence. Simultaneously, 3D avatar generation employs Tripo AI 2.0 [1] to produce candidate meshes, as shown in Figure 4, and the Selective Rigging Mechanism (SRM) identifies the optimal rigged mesh. The motion sequence is then retargeted to the avatar meshes, resulting in complete 4D avatar. As shown in the Figure 3, automatic rigging is crucial for 4D generation, directly affecting the precision and realism of avatar movements [67, 77]. Although numerous optimization-based approaches [4, 7, 15, 42] have been proposed to achieve fully automated rigging, the outcomes are often unsatisfactory due to the diverse appearances of meshes. Hence, achieving high-quality automatic rigging has become an important challenge to address in skeletonbased avatar generation. To improve the underperformance of automatic rigging in generated avatars, we introduced straightforward yet effective Selective Rigging Mechanism that selects the best-rigged 3D avatar from multiple candidates, enhancing the realism of the avatars motion visualization. 4.1. Selective Rigging Mechanism To improve automatic rigging performance and reduce the need for human-in-the-loop adjustments, the Selective Rigging Mechanismn (SRM) presents two-stage selection process with constraints. This mechanism identifies the optimal rigging from set of candidate avatars, as shown in Figure 3. Stage 1: Centroid-Based Filtering. The purpose of this stage is to identify point clouds with centroids positioned within balanced and plausible bounding region for rigging. Each animated point cloud is defined by set of 3D coordinates = {p1, p2, . . . , pN }, where pi = (xi, yi, zi) R3, representing the characters surface. The centroid Gcloud of the point cloud, serving as an approximate center of mass, is calculated as Gcloud = (cid:32) 1 (cid:88) i=1 xi, 1 (cid:88) i=1 yi, 1 (cid:88) i=1 (cid:33) zi . The bounding box and stability filters ensure that Gcloud falls within spatial region aligned with the characters rigging needs. Specifically, the bounding box constraint requires 1 < XG < 1, 1 < YG < 1, 1 < ZG < 1, while the stability constraint approximates balance by enforcing XG 0, YG 0, and ZG > 0. These constraints are physically motivated: (1) Minimal lateral displacement, represented by XG 0 and YG 0, keeps the center of mass near the z-axis, avoiding lateral imbalances. (2) Ensuring ZG > 0 places the centroid above the ground plane, maintaining logical upright character orientation. Stage 2: Joint Weight Optimization. This stages goal is to select the point cloud configuration with the best joint weight distribution to support stable, smooth deformation during animation. Each vertex has joint weights wi1, wi2, . . . , win, where each wij denotes the influence of joint on vertex i. To achieve realistic deformation, these weights must sum to 1 across all joints for each vertex, as specified by [31]. The weight normalization condition is expressed as: (cid:88) j=1 wij = 1, {1, 2, . . . , } where is the total number of points in P. To evaluate and select the optimal point cloud from candidates, we define loss function based on the average Figure 4. 3D Avatars. This figure shows examples of 3D avatars generated by Tripo AI 2.0 [1]. These avatars will later serve as candidates for our Selective Rigging Mechanism. deviation from the ideal weight sum. For each point cloud, we calculate the average weight sum as = 1 N (cid:88) (cid:88) i=1 j=1 wij. Our loss function, defined as the absolute difference 1, quantifies how close each point clouds weight distribution is to the ideal configuration. Minimizing 1 allows us to select the point cloud whose joint weights best satisfy the normalization condition. Thus, the optimal point cloud Poptimal is chosen as: Poptimal = arg min Pk Sk 1 , where {1, 2, . . . , }, Pk is the k-th candidate point cloud, and Sk is the average weight sum for Pk. By selecting the configuration with the smallest 1, we ensure joint weight distribution close to ideal, supporting stable, natural rigging and deformation in animation. 0.138 0.094 0.105 0.117 Method MagicPose4D SRM (k = 1) SRM (k = 3) SRM (k = 5) 1 AIT(s) 1.93 1.78 1.36 1. To showcase our improvement and efficiency in rigging with SRM on TMD dataset, we create 200 avatar descriptions along with text randomly selected from TMD to generate corresponding avatars using TripoAI 2.0 [1]. We then evaluate our SRM with different candidate numbers based on the average weighted sum in terms of rigging quality, where value of closer to 1 indicates better performance. The results show in in Table 2. Video demos of the generated 4D avatars can also be found in the supplementary materials. Table 2. SRM evaluation. Figure 5. Qualitative evaluation on text-&-music-to-dance generation. We qualitatively compared the visualizations generated by our method with those produced by TM2D [17] and MotionCraft [5]. Algorithm 1 Selective Rigging Mechanism 1: Input: point clouds {P1, . . . , PM }, each with 3D vertices and joint weights {wi1, . . . , win} for each vertex 2: Stage 1: Centroid Filtering 3: for each Pk do 4: 5: Compute centroid ck = (Xk, Yk, Zk) if ck is outside 1 < Xk, Yk, Zk < 1 or not close to (0, 0, > 0) then Discard Pk 6: end if 7: 8: end for 9: Stage 2: Weight Optimization 10: for each remaining Pk do (cid:80)N Calculate Sk = 1 j=1 wij 11: Compute deviation = Sk 1 12: 13: end for 14: Select Poptimal = arg minPk 15: Output: Optimal point cloud Poptimal (cid:80)n i=1 5. Qualitative Evaluation In the main text, we have already demonstrated the qualitative evaluation of text-to-motion generation and music-todance generation. Here, we showcase some examples of the visualization of text-and-music-to-dance generation. 5.1. Text-&-Music-to-Dance To evaluate the quality of our text-and-music-to-dance generation, we compare the dances generated by our method against those from open-source state-of-the-art multi-task models, including TM2D [17] and MotionCraft [5]. Similar to Section 4.3 in the main text, despite these multitask models lacking the ability to simultaneously take two modalities of conditions, we combined their condition embeddings and compared them with our approach. Trained on our TMD dataset, we ensure that the evaluation reflects diverse musical styles. As shown in Figure 5 and demonstrated in the accompanying videos, our method produces dances with better visual quality and achieves superior alignment with both text description, beat, and genre of the music, surpassing previous state-of-the-art techniques. Datasets Method Precision FID MultiModal Dist Diversity MultiModality Ground Truth TEMOS [43] TM2T [20] T2M [19] TM2D [17] MotionGPT (Zhang et al.) [74] MotionDiffuse [72] MDM [53] MotionLLM [61] MLD [10] M2DM [30] MotionLCM [13] Motion Mamba [78] Fg-T2M [59] MotionGPT (Jiang et al.) [27] MotionGPT-2 [60] MotionCraft [5] FineMoGen [71] T2M-GPT [69] GraphMotion [29] EMDM [82] AttT2M [81] GUESS [16] ParCo [87] ReMoDiffuse [70] MotionCLR [9] StableMoFusion [25] MMM [45] DiverseMotion [41] BAD [22] BAMM [44] MCM [39] MoMask [21] LMM [73] MoGenTS [64] Motion Anything (Ours) Ground Truth TEMOS [43] TM2T [20] T2M [19] MotionDiffuse [72] MDM [53] MLD [10] M2DM [30] Motion Mamba [78] Fg-T2M [59] MotionGPT (Zhang et al.) [74] MotionGPT (Jiang et al.) [27] MotionGPT-2 [60] FineMoGen [71] T2M-GPT [69] GraphMotion [29] EMDM [82] AttT2M [81] GUESS [16] ParCo [87] ReMoDiffuse [70] StableMoFusion [25] MMM [45] DiverseMotion [41] BAD [22] BAMM [44] MoMask [21] LMM [73] MoGenTS [64] Motion Anything (Ours) Top 1 0.511.003 0.424.002 0.424.003 0.457.002 0.319.000 0.364.005 0.491.001 0.320.005 0.482.004 0.481.003 0.497.003 0.502.003 0.502.003 0.492.002 0.492.003 0.496.002 0.501.003 0.504.002 0.492.003 0.504.003 0.498.007 0.499.003 0.503.003 0.515.003 0.510.005 0.542.001 0.553.003 0.504.003 0.515.003 0.517.002 0.525.002 0.502.002 0.521.002 0.525.002 0.529.003 0.546.003 0.424.005 0.353.006 0.280.005 0.370.005 0.417.004 0.164.004 0.390.003 0.405.003 0.419.006 0.418.005 0.340.002 0.366.005 0.427.003 0.432.006 0.416.006 0.417.008 0.443.006 0.413.006 0.425.005 0.430.004 0.427.014 0.445.006 0.404.005 0.416.006 0.417.006 0.438.009 0.433.007 0.430.015 0.445.006 0.449. Top 2 0.703.003 0.612.002 0.618.003 0.639.003 - 0.533.003 0.681.001 0.498.004 0.672.003 0.673.003 0.682.002 0.698.002 0.693.002 0.683.003 0.681.003 0.691.003 0.697.003 0.690.002 0.679.002 0.699.002 0.684.006 0.690.002 0.688.002 0.706.003 0.698.006 0.733.002 0.748.002 0.696.003 0.706.002 0.713.003 0.720.003 0.692.004 0.713.002 0.719.002 0.719.002 0.735.002 0.649.006 0.561.007 0.463.006 0.569.007 0.621.004 0.291.004 0.609.003 0.629.005 0.645.005 0.626.004 0.570.003 0.558.004 0.627.002 0.649.005 0.627.006 0.635.006 0.660.006 0.632.006 0.632.007 0.649.007 0.641.004 0.660.005 0.621.005 0.637.008 0.631.006 0.661.009 0.656.005 0.653.017 0.671.006 0.678.004 Top 3 0.797.002 0.722.002 0.729.002 0.740.003 - 0.629.004 0.782.001 0.611.007 0.770.002 0.772.002 0.763.003 0.798.002 0.792.002 0.783.002 0.778.002 0.782.004 0.796.002 0.784.002 0.775.002 0.785.002 0.786.006 0.786.002 0.787.002 0.801.002 0.795.004 0.827.003 0.841.002 0.794.002 0.802.002 0.808.003 0.814.003 0.788.006 0.807.002 0.811.002 0.812.002 0.829.002 0.779.006 0.687.005 0.587.005 0.693.007 0.739.004 0.396.004 0.734.002 0.739.004 0.765.006 0.745.004 0.660.004 0.680.005 0.764.003 0.772.006 0.745.006 0.755.004 0.780.005 0.751.006 0.751.005 0.772.006 0.765.055 0.782.004 0.744.004 0.760.011 0.750.006 0.788.005 0.781.005 0.779.014 0.797.005 0.802.006 0.002.000 3.734.028 1.501.017 1.067.002 1.021.000 0.805.002 0.630.001 0.544.044 0.491.019 0.473.013 0.352.005 0.304.012 0.281.009 0.243.019 0.232.008 0.191.004 0.173.002 0.151.008 0.141.005 0.116.007 0.112.019 0.112.006 0.109.007 0.109.005 0.103.004 0.099.003 0.098.003 0.080.003 0.072.004 0.065.003 0.055.002 0.053.007 0.045.002 0.040.002 0.033.001 0.028.005 0.031.004 3.717.051 3.599.153 2.770.109 1.954.062 0.497.021 0.404.013 0.502.049 0.307.041 0.571.047 0.868.032 0.510.016 0.614.005 0.178.007 0.514.029 0.262.021 0.261.014 0.870.039 0.371.020 0.453.027 0.155.006 0.258.029 0.316.028 0.468.098 0.221.012 0.183.013 0.204.011 0.137.023 0.143.004 0.131. 2.974.008 3.703.008 3.467.011 3.340.008 4.098.000 3.914.013 3.113.001 5.566.027 3.138.010 3.196.010 3.134.010 3.012.007 3.060.058 3.109.007 3.096.008 3.080.013 3.025.008 2.998.008 3.121.009 3.070.008 3.110.027 3.038.007 3.006.007 2.927.008 2.974.016 2.981.011 - 2.998.007 2.941.007 2.901.008 2.919.008 3.037.003 2.958.008 2.943.012 2.867.006 2.859.010 2.788.012 3.417.019 4.591.026 3.401.008 2.958.005 9.190.022 3.204.010 3.012.015 3.021.025 3.114.015 3.721.018 3.527.021 3.164.013 2.869.014 3.007.023 3.085.031 2.874.015 3.039.021 2.421.022 2.820.028 2.814.012 - 2.977.019 2.892.041 2.941.025 2.723.026 2.779.022 2.791.018 2.711.024 2.705.024 9.503.065 8.973.071 8.589.076 9.188.002 9.513.000 9.972.026 9.410.049 9.559.086 9.838.244 9.724.082 9.926.073 9.607.066 9.871.084 9.278.072 9.528.071 9.860.026 9.543.098 9.263.094 9.722.082 9.692.067 9.551.078 9.700.090 9.826.104 9.576.088 9.018.075 - 9.748.092 9.411.058 9.683.102 9.694.068 9.717.089 9.585.082 - 9.814.076 9.570.077 9.521.083 11.08.097 10.84.100 9.473.117 10.91.119 11.10.143 10.85.109 10.80.082 11.38.079 11.02.098 10.93.083 9.972.026 10.35.084 11.26.026 10.85.115 10.92.108 11.21.106 10.96.093 10.96.123 10.93.110 10.95.094 10.80.105 10.94.077 10.91.101 10.87.101 11.00.100 11.01.094 - 11.24.103 10.92.090 10.94.098 - 0.368.018 2.424.093 2.090.083 4.139.000 2.473.041 1.553.042 2.799.072 - 2.413.079 3.587.072 2.259.092 2.294.058 1.614.049 2.008.084 2.137.022 - 2.696.079 1.831.048 2.766.096 1.641.078 2.452.051 2.430.100 1.382.060 1.795.043 2.145.043 1.774.051 1.164.041 1.869.089 1.194.044 1.687.051 0.810.023 1.241.040 2.683.054 - 2.705.068 - 0.532.034 3.292.081 1.482.065 0.753.013 1.907.214 2.192.079 3.273.045 1.678.064 1.019.029 2.296.022 2.328.117 2.357.022 1.877.093 1.570.039 3.568.132 1.343.089 2.281.047 2.732.084 1.245.022 1.239.028 1.362.062 1.232.039 2.062.079 1.170.047 1.609.065 1.131.043 1.885.127 - 1.374. Human ML3D [19] KITML [46] Table 1. Comprehensive comparison on HumanML3D [19] and KIT-ML [46]. The best and runner-up values are bold and underlined. The right arrow indicates that closer values to ground truth are better. Multimodal motion generation methods are highlighted in blue."
        }
    ],
    "affiliations": [
        "ANU",
        "Google",
        "JD.com",
        "MBZUAI",
        "McGill",
        "Tencent",
        "USYD",
        "UTS",
        "ZJU"
    ]
}