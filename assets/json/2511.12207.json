{
    "paper_title": "Mixture of States: Routing Token-Level Dynamics for Multimodal Generation",
    "authors": [
        "Haozhe Liu",
        "Ding Liu",
        "Mingchen Zhuge",
        "Zijian Zhou",
        "Tian Xie",
        "Sen He",
        "Yukang Yang",
        "Shuming Liu",
        "Yuren Cong",
        "Jiadong Guo",
        "Hongyu Xu",
        "Ke Xu",
        "Kam-Woh Ng",
        "Juan C. Pérez",
        "Juan-Manuel~Pérez-Rúa",
        "Tao Xiang",
        "Wei Liu",
        "Shikun Liu",
        "Jürgen Schmidhuber"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "We introduce MoS (Mixture of States), a novel fusion paradigm for multimodal diffusion models that merges modalities using flexible, state-based interactions. The core of MoS is a learnable, token-wise router that creates denoising timestep- and input-dependent interactions between modalities' hidden states, precisely aligning token-level features with the diffusion trajectory. This router sparsely selects the top-$k$ hidden states and is trained with an $ε$-greedy strategy, efficiently selecting contextual features with minimal learnable parameters and negligible computational overhead. We validate our design with text-to-image generation (MoS-Image) and editing (MoS-Editing), which achieve state-of-the-art results. With only 3B to 5B parameters, our models match or surpass counterparts up to $4\\times$ larger. These findings establish MoS as a flexible and compute-efficient paradigm for scaling multimodal diffusion models."
        },
        {
            "title": "Start",
            "content": "Mixture of States: Routing Token-Level Dynamics for Multimodal Generation Haozhe Liu1,2,, Ding Liu2, Mingchen Zhuge1,2, Shuming Liu1,2, Yuren Cong2, Jiadong Guo2, Hongyu Xu2, Ke Xu2, Kam-Woh Ng2, Juan C. Pérez2, Juan-Manuel Pérez-Rúa2, Tao Xiang2, Wei Liu2, Shikun Liu2, 1KAUST, 2Meta AI Joint First Authors, , Sen He2, Yukang Yang2, , Jürgen Schmidhuber1, , Zijian Zhou2,"
        },
        {
            "title": "Core Contributors",
            "content": ", Tian Xie2, We introduce MoS (Mixture of States), novel fusion paradigm for multimodal diffusion models that merges modalities using flexible, state-based interactions. The core of MoS is learnable, token-wise router that creates denoising timestepand input-dependent interactions between modalities hidden states, precisely aligning token-level features with the diffusion trajectory. This router sparsely selects the top-k hidden states and is trained with an ϵ-greedy strategy, efficiently selecting contextual features with minimal learnable parameters and negligible computational overhead. We validate our design with text-to-image generation (MoS-Image) and editing (MoS-Editing), which achieve state-of-the-art results. With only 3B to 5B parameters, our models match or surpass counterparts up to 4 larger. These findings establish MoS as flexible and compute-efficient paradigm for scaling multimodal diffusion models. Date: November 18, 2025 Correspondence: haozhe.liu@kaust.edu.sa; dingliu@meta.com (Project Lead) 5 2 0 2 5 1 ] . [ 1 7 0 2 2 1 . 1 1 5 2 : r MoS-Image MoS-Edit Figure 1 Generation examples by MoS-Image (left) and MoS-Edit (right). MoS introduces learnable, token-wise router that efficiently aggregates feature states across modalities. This allows for high-quality visual synthesis, producing photorealistic and stylized outputs from text and image inputs with precise control and quality."
        },
        {
            "title": "1 Introduction",
            "content": "Multimodal generation is fundamental application of modern AI, enabling models to synthesize high-quality visual content such as images and videos from conditional inputs (Rombach et al., 2022; Gao et al., 2025; 1 Baldridge et al., 2024; Polyak et al., 2024a; Wu et al., 2025a; Liu et al., 2025a). In this paper, we focus on text-toimage generation and instruction-based image editing tasks, domain where central challenge lies in effectively aligning textual and visual signals. This problem is non-trivial, as the modalities rely on different modeling objectives: text models are typically trained with contrastive learning (Radford et al., 2021), masked-token prediction (Devlin et al., 2019; Raffel et al., 2020), or next-token prediction (Team et al., 2024; Touvron et al., 2023; Brown et al., 2020), whereas visual models often adopt diffusion-based generation (Ho et al., 2020; Neal, 2001; Jarzynski, 1997) or flow matching (Lipman et al., 2023; Esser et al., 2024b). Consequently, alignment requires bridging not only heterogeneous representations but also distinct designs across modalities. Prior studies address this challenge through various hand-crafted designs. These dominant fusion techniques, as well as the method we propose, are built upon the transformer architecture (Vaswani et al., 2017; Dosovitskiy et al., 2020; Schmidhuber, 1992), which serves as powerful backbone for modeling both textual and visual representations. The primary strategies include: i) Cross-attention methods (Rombach et al., 2022; Vaswani et al., 2017; Chen et al., 2023; Xie et al., 2024) insert new attention blocks into the visual model, projecting text embeddings onto key-value vectors to enable cross-modal token interactions. ii) Self-attention methods (Esser et al., 2024a; Chen et al., 2025a; Qin et al., 2025) instead concatenate text and visual tokens into unified sequence, processed by shared attention layers. While this allows for deeper, bidirectional fusion than crossattention and often yield stronger performance, its computational cost is often prohibitive, scaling quadratically with the combined sequence length; iii) MoT (Mixture-of-Transformers), more recent method (Deng et al., 2025; Liao et al., 2025; Liang et al., 2025; Shi et al., 2024), establishing layer-wise cross-modal connections by sharing keyvalue vectors between corresponding text and visual blocks. This method facilitates finergrained interaction, but its rigid, layer-by-layer design imposes strong architectural constraint: the text and visual backbones must be symmetric, with one-to-one block correspondence. Through our ablations, we have identified three critical design principles for improving text-visual representation alignment that challenge the hand-crafted/fixed-interaction paradigms of prior work: Layer selection should be adaptive, not fixed. We find that using single fixed layer, typically the final-layer feature from the text branch, as commonly adopted in crossand self-attention methods, is suboptimal. Furthermore, the rigid one-to-one layer correspondence of MoT assumes that text and visual features align symmetrically, an assumption for which we find no experimental support. This suggests that diffusion models do not consume language features in strictly sequential or layer-aligned manner, making flexible selection mechanism essential. Conditional signals should be dynamic and timestep-dependent. We validate that the common design in modern text-to-image models, which encodes the text embedding once and keep it static, creates an \"information mismatch\" with the dynamic nature of the denoising process. We argue that the conditional guidance needs to adapt as the input noise level and denoising step change. Conditional signals should be token-specific. Our findings indicate that it is more effective to allow each token to source its representation adaptively from different layers, rather than using single, shared layer embedding to represent all tokens uniformly. This supports more granular, token-level view of context conditioning. To this end, we introduce Mixture of States (MoS), new framework that enables multimodal interaction to adapt dynamically to the input and denoising step. Unlike prior fixed-interaction designs, MoS grants the vision branch access to all textual hidden states across all layers and employs learnable, token-wise router to selectively aggregate features at each denoising step. As shown in Fig. 2, this sparse yet dynamic routing allows any visual token, at any denoising step and within any transformer block, to attend to tokens from any layer of the text encoder. This effectively bridges the gap between textual representations and visual diffusion dynamics. We systematically explore the design space of MoS, including its input formulation, architectural configuration, and training strategy. Building on this foundation, we present family of multimodal generation models that support two multimodal generation tasks: image generation (MoS-Image) and image editing (MoS-Edit). In all, our contributions are threefold: We propose Mixture of States, novel and flexible fusion mechanism for multimodal diffusion models. Its design, characterized by dynamic, sparse, and state-based interactions, is inherently adaptive and 2 (a) Cross-Attention (b) Self-Attention (c) MoT (d) MoS (Ours) Figure 2 MoS enables sparse and dynamic interactions across modalities and transformers. We illustrate MoS with textto-image generation. Previous approaches, such as (a) cross-attention and (b) self-attention, typically provide only the final text encoder blocks embedding as input to the visual branch, limiting the richness of cross-modal information. (c) MoT (Mixture-of-Transformers) attempts finer-grained interaction by passing outputs from all text blocks in rigid, layer-by-layer fashion. In contrast, our proposed (d) MoS (Mixture of States) employs learnable sparse interaction that dynamically links any text block to any visual block. The routing adapts to the current input, comprising the text prompt, visual latents, and denoising step embeddings, enabling flexible and efficient multimodal fusion. aligns with the iterative diffusion process. We introduce MoS-Image and MoS-Edit (3-5B parameters) that serve as blueprint for scaling the MoS design. We demonstrate its ability to effectively merge asymmetric text and visual backbones, overcoming key limitation of prior fusion methods. Extensive evaluations show that MoS-based models achieve state-of-the-art performance on image generation and editing tasks at similar parameter scale. Notably, our 5B parameter model matches or surpasses the performance of 20B-parameter (4 larger) model (Wu et al., 2025a), demonstrating exceptional computational efficiency."
        },
        {
            "title": "2 Related Work",
            "content": "Text-to-Image Network Architecture Diffusion models (Ho et al., 2020; Neal, 2001; Jarzynski, 1997; Peebles and Xie, 2023) have become dominant paradigm for text-to-image generation (Rombach et al., 2022; Dhariwal and Nichol, 2021; Ramesh et al., 2022; Chen et al., 2024b; Saharia et al., 2022; Dai et al., 2023; Betker et al., 2023) owing to their scalability and stable training. In multimodal diffusion models, prompt embeddings derived from frozen text encoder are incorporated through cross-attention (Chen et al., 2024b; Podell et al., 2023; Rombach et al., 2022), self-attention (Esser et al., 2024a; Chen et al., 2025a), or layer-wise attention (e.g., MoT) (Liang et al., 2025). fundamental challenge in this design is the \"static vs. dynamic\" mismatch. The diffusion process is inherently dynamic, operating over numerous timesteps with varying noise levels and visual features (Liu et al., 2025b; Kahatapitiya et al., 2025). However, the text encoder provides only single, static representation of the prompt. While selfand layer-wise attention allow this conditional information to evolve within the visual backbones blocks, the initial conditional signal provided to the model remains fixed. To address this limitation, our MoS framework introduces learnable router. The router jointly considers the prompt, denoising step, and noised image to dynamically select and aggregate conditional embeddings, enabling true inputand time-dependent conditioning. Unified Model Recent research (Team, 2024a; Wu et al., 2024a; Ge et al., 2024; Xie et al., 2025b) has increasingly sought to unify diverse tasks within single framework. While following this unified design philosophy, it diverges in its training methodology. Instead of the common approach of jointly training all tasks in single, complex stage (Deng et al., 2025; Xie et al., 2025c; Zhou et al., 2024; Chen et al., 2025c; Liao et al., 2025; Geng et al., 2025), we adopt multi-stage training strategy. This approach provides significant flexibility and efficiency. Specifically, by freezing our text branch, we can focus computational resources purely on optimizing 3 the multimodal generation components. This staged training also circumvents common challenges of joint training, such as throughput bottlenecks from mixed data batches and the difficulty of balancing diverse, and often competing, learning objectives across modalities. This strategy is consistent with other recent, successful large-scale models like BLIP-3o (Chen et al., 2025b), MetaQuery (Pan et al., 2025), Qwen-Image (Wu et al., 2025a) and LMFusion (Shi et al., 2024), which also employ multi-stage training. Dynamic Neural Networks MoS is also related to the principles of dynamic neural networks (Han et al., 2021; Schmidhuber, 1992; Jacobs et al., 1991; Hampshire and Waibel, 1989; Ivakhnenko and Lapa, 1966; Ivakhnenko, 2007), in which the computational graph or parameter usage is conditioned on the input. prominent example is the Mixture-of-Experts (MoE) (Hampshire and Waibel, 1989; Eigen et al., 2013; Shazeer et al., 2017; Lepikhin et al., 2020; Fedus et al., 2022; Liu et al., 2023; Jiang et al., 2024; Csordás et al., 2024), where tokens are adaptively processed by different \"expert\" sub-networks within each transformer block. Due to its sparsity, MoE efficiently scales model parameters while keeping computation tractable. Recent advances have explored other forms of dynamic computation, such as Mixture-of-Depths (MoD) (Raposo et al., 2024), which dynamically allocates compute across tokens and layers, and Mixture-of-Recursions (MoR) (Bae et al., 2025), which reuses layers recursively with variable-depth routing. These methods establish shared, powerful principle: computation should be sparse, adaptive, and conditional on the input. However, this principle has largely been applied to intra-model adaptivity, i.e., routing tokens within single large model. In contrast, MoS extends this principle to inter-model collaboration. Mixture of Transformers (MoT) MoS is conceptually related to the Mixture of Transformers (MoT) architecture (Liang et al., 2025). In MoT, each modality is processed by an independent transformer, while shared attention module in each block enables tokens to attend across modalities. This design has been widely adopted in multimodal research: LMFusion (Shi et al., 2024) and PGV3 (Liu et al., 2024a) use it to couple frozen LLMs as the text encoder with trainable diffusion transformer for strong text-to-image generation performance. More recently, Bagel (Deng et al., 2025) and Mogao (Liao et al., 2025) enable joint training of both transformers, unifying image understanding and generation. Despite these advances, critical limitation persists: MoT designs require identical hidden dimensions and strict one-to-one block correspondence across modalities to share global attention. This rigid, symmetric constraint is highly inflexible, as various modalities may follow distinct scaling laws and design principles. MoS is designed to solve this specific problem. We replace the rigid global attention mechanism with learnable, sparse router, which removes the identical-size constraint and enables adaptive, effective interactions between asymmetric transformers."
        },
        {
            "title": "3 Mixture of States: Unifying Asymmetric Transformers",
            "content": "G .1 The understanding tower"
        },
        {
            "title": "3.1 Design Overview\nMoS adopts a dual-tower architecture design for multimodal generation: an understanding tower\nand a\nprocesses multimodal context c (text for text-to-image tasks;\ngeneration tower\ntext+image for image editing tasks), producing contextual representations that guide the generation tower\nduring visual synthesis. Our design departs from prior work in two key ways. First, unlike MoT, which\nG\nlearns multi-modal interactions with the key-value vectors in the same layer, we use full layer-level hidden\nstates as the unit of interaction. This enables state transfer compatibility across heterogeneous transformers\nthrough conventional hand-crafted\nwith different model depths. Second, rather than connecting\nfeature fusing mechanisms like self- or cross-attention, we introduce a learnable router\nto dynamically\nmediate their interaction.",
            "content": "and U Following standard practice, the entire model is trained end-to-end with rectified flow matching: c,t,z0,z1 (cid:20) (cid:13) (cid:13) (zt, t, (t, c, zt, (c))) (cid:21) , (cid:13) (cid:13) 2 2 vt (1) 1We deliberately choose this more general terminology rather than the terms text or visual branch. MoS is designed to support both image generation and image editing tasks, where the latter requires both branches to process the reference image. For conceptual rigor, we thus avoid using terms such as text branch. 4 Figure 3 MoS Design Details. MoS introduces new paradigm for multimodal interaction within transformer architectures. Rather than depending on manually designed fusion strategies, MoS employs learned router to establish token-level sparse and dynamic connections between transformer blocks. For illustration, we use image generation as the running example and thus refer to the understanding-tower features as textual embeddings. z0 is the linear [0, 1]. The input zt where vt = dzt/dt = z1 . (0, I), given by zt = (1 interpolation between the image latent z0 t)z0 + tz1 (x). During training, the parameters of the The target latent z0 pre-trained understanding tower are learnable and the router and are initialized from scratch. This setup significantly reduces computational overhead and facilitates more tractable architectural exploration. denotes the target velocity at denoising step and the random noise z1 is obtained from the VAE encoder z0 = are frozen. Only the generation tower E"
        },
        {
            "title": "3.2 MoS Design Details",
            "content": "is the core component that governs collaboration between the two towers. As shown in Fig. 3, the MoS router It determines which hidden states from the understanding tower are transferred, how they are weighted, and to which layers of the generation tower they are delivered. We elaborate on this design along the following dimensions: Router Input Space The routers primary objective is to resolve the inherent representation mismatch between processes the context in single the understanding and generation towers. The understanding tower forward pass, producing fixed set of representations. The generation tower , however, is highly dynamic, evolves at each denoising timestep t. To address requiring step-specific guidance as its visual latent state zt this, we incorporate the denoising step as direct input to the router, enabling it to learn time-varying routing patterns. With additional input content, the complete router decision includes the timestep t, the noised image embedding zt , and the context embedding c. As the router is lightweight transformer, it expects all inputs to be token sequences sharing unified hidden size. The conditioning signal is processed through shared projection layer, followed by linear layer for dimensional alignment. The timestep is represented using sinusoidal embeddings (Ho et al., 2020) and projected into the same latent space. The patchify layer and is subsequently projected to the target dimensionality. latent zt"
        },
        {
            "title": "U\nshares",
            "content": "U W Rm Router Output Space For each token in the context prompt, the router generates corresponding logit n, where and denote the depths of the understanding and generation towers, matrix = [wij] respectively. Each entry wij represents the learned affinity weight for routing the hidden states from the i-th layer of the understanding tower to the j-th layer of the generation tower. Building on prior analyses showing token-wise representations evolve and serve different roles across layers (Raposo et al., 2024), we hypothesize that routing decisions should also be token-specific. Therefore, in MoS, each token predicts its own distinct routing matrix , rather than sharing single, uniform routing policy across the entire prompt. 5 Lightweight Router Design To ensure efficiency, we adopt lightweight router design. All input embeddings , and c) are individually tokenized, normalized, and concatenated into sequence. We then apply (t, zt two transformer blocks with bidirectional self-attention to capture in-context semantics. Finally, we apply projection layer on the context tokens to obtain the logit matrix. Sparsity and ϵ-Greedy Exploration In MoS, the router predicts the logit matrix to estimate the individual contributions of context tokens hidden states from different blocks to the generation task. We expect the logit matrix to be sparse, as simply averaging all hidden states would obscure layer-specific information. To address this, we implement sparse top-k routing strategy. Specifically, in each forward pass, the predicted [1, m]) are normalized using softmax. It then selects the top-k hidden states from the logits wij understanding tower with the highest normalized weights. These selected hidden states are then reweighted and aggregated by their corresponding wij to provide guidance to the j-th block of the generation tower. This selection process is performed independently for each generation block. (for To prevent the router from converging to sub-optimal local solution, we adopt an ϵ-greedy strategy during training. At pre-defined probability ϵ [0, 1], the router selects random layers to encourage exploration; and with probability 1 ϵ, it follows the top-k strategy described above."
        },
        {
            "title": "3.3 MoS for Image Generation",
            "content": "For the text-to-image generation task, the generation tower follows the foundations of the diffusion process. During training, given text-image pair (c, z0), we first extract all hidden states from the understanding tower conditioned our input context prompt c: i=1:m (c) = c {S [1, m] . } (2) diffusion timestep is then sampled, yielding the corresponding noisy latent zt the router predicts logit matrix the final normalized weight matrix . By applying softmax operation along each j-th column w:,j . Conditioned on c, t, and zt , , we obtain = [wi,j]: wi,j = exp(wi,j) i=1 exp(wi,j) . (3) For each j-th block in the generation tower top-k selection. The conditional input is then computed as weighted sum: , we identify the index set Ij = top-kϵ(w1:m,j) using ϵ-greedy = Sc Ij wij c . (4) is passed through linear projection layer, Proj( The aggregated hidden state Sc of the visual representation Sz at the j-th block. The projected features, Proj(Sc along the sequence dimension and jointly processed as in-context tokens within the block. During inference, this process is identical, but we set ϵ = 0 with deterministic top-k selection. The complete procedures for training and inference are summarized in Algorithms 1 and 2 in the Appendix. ), to match the dimensionality ), are then concatenated with Sz j"
        },
        {
            "title": "3.4 MoS for Image Editing",
            "content": "Fig. 4 illustrates the pipeline of MoS-Edit, following the same training strategy for MoS-Image but with different input space. For this task, the model is given both source reference image and textual instruction (e.g., wear doctoral cap). Figure 4 Image Editing Inference Pipeline. Both the understanding and generation towers take the reference image as input, with their interaction facilitated through the MoS module. 6 The understanding tower processes both the source image and the instruction to generate rich, combined contextual representation c. This representation is then dynamically routed through MoS to the generation tower. During inference, the generation tower receives both the Gaussian noise (for the target image) and the clean reference image, and iteratively refines the latent representation conditioned on the guidance from MoS. Finally, the refined latent is decoded back into the final edited target image."
        },
        {
            "title": "3.5 Training MoS at Scale\nTraining Strategy and Data Curation Following prior studies (Wu et al., 2025a; Esser et al., 2024a), we build\nour MoS-Image model upon the standard latent diffusion framework (Rombach et al., 2022), trained with a\nfour-stage progressive schedule:",
            "content": "Stage 1 (Low-Resolution): Leveraging 16 et al., 2025) with patch size of 2 stage consumes 1400 A100-days on resolution, caption length, aesthetic score). spatial compression rate from pre-trained VAE (Wan 2, we initiate training directly at resolution of 512 512. This (100M) samples filtered by standard image quality metrics (e.g., Stage 2 (High Resolution): The model is then scaled to 1024 1024 resolution, training for similar computational budget on the same data. These first two stages establish the models core capabilities on text-image alignment and high-fidelity image generation. Stage 3 (Aesthetic Tuning): We curate high-quality subset of (10M) samples, with stricter data filtering on caption quality, aesthetic evaluation, and text-image alignment. The model is continually trained on these data for an additional 100 A100 days, enhancing its aesthetic fidelity and instructionfollowing capabilities. Stage 4 (Super-Resolution Tuning): We curate high-quality dataset 2048 resolution images. We fine-tune the model on this dataset to support 2K-resolution text-to-image generation, resulting in final model with improved fidelity. This fine-tuning stage requires an additional 80 A100 GPU days. (1M) consisting of 2048 (1M) paired image-editing data. In total, our MoS-Image model (Stages 1-4) requires Following this, we extend the model to MoS-Edit, which requires an additional 50 A100 days of training on 3,000 A100 days, substantially less than the 6,250 A100 days reported for one of the earliest large-scale text-to-image models, Stable Diffusion v1.5 (Rombach et al., 2022), demonstrating the efficiency of our architecture. Model Configuration In this study, we develop two MoS variants: MoS-S and MoS-L. Both models adopt the Wan 2.2 VAE (Wan et al., 2025) for compressing image inputs. Our MoS-S employs the 8B PLM-8B (Cho et al., 2025) as the frozen understanding tower and 3B generation tower (trained from scratch); and our MoS-L employs the 14B InternVL-14B (Chen et al., 2024c) as the frozen understanding tower and 5B generation tower (trained from scratch). For both variants, the router is designed with lightweight 100M parameters, adding negligible overhead. For clarity, MoS-L/S refer to models with different parameter sizes, while MoS-Image/Edit denote variants targeting different tasks."
        },
        {
            "title": "4.1 Systematic Ablations on Router Designs",
            "content": "In this section, we systematically ablate the design choices of MoS. To balance computational cost, we conduct ablation studies with lightweight configuration: models are trained on randomly selected subset of pre-training data, and evaluation is performed on the MJHQ benchmark (Li et al., 2024a). We adopt FID (Heusel et al., 2017) and CLIP scores (Radford et al., 2021), GenEval (Ghosh et al., 2023) and DPG (Hu et al., 2024), following similar setup to Esser et al. (2024a). Unless otherwise specified, all studies use an asymmetric design with LLaMA3.2-3B as the understanding tower and 1B transformer for the generation tower. key exception is the ablation comparing MoS with MoT. As MoT requires symmetric towers with 7 identical depths, this specific study uses the smaller PLM-1B as the understanding tower, and instantiates an identical 1B architecture for the generation tower. Router should be conditioned on dynamic, timestep-dependent signals. We first validate our hypothesis that the routers conditional signal must be dynamic. We test three configurations: i) Prompt: The router only receives the static prompt embedding c; ii) Prompt + Latent: The router receives and the noised image latent zt ; iii) Prompt + Latent + Timestep: The router receives the full dynamic states: c, zt The results in Table 1 show clear trend. Relying on the static prompt alone yields the weakest performance. Adding the dynamic state from the generation tower (the noised latent zt and timestep t) progressively improves both FID and CLIP scores. The optimal performance is achieved with the one that has access to all three inputs. , and t. Table 1 FID and CLIP results on MJHQ with different routers conditions. Providing the router with the full dynamic state (prompt, noised latent, and timestep) yields the best performance. Prompt Latent Timestep FID CLIP 21.12 21.40 21.89 21.53 20.15 21.74 This confirms our design principle: the router must be aware of the diffusion processs dynamic state (noise level and timestep) to select the most effective features. This finding aligns with prior work (Liu et al., 2025b), showing that diffusion model behavior varies across denoising steps. We provide further validation in Appendix B.7, which visualizes how the routers guidance changes dynamically over different timesteps. Routers prediction should be token-specific. Next, we investigate whether the router should predict global logit matrix shared by all context tokens, or token-specific matrix that assigns individualized routing decisions to each token. As illustrated in Fig. 5, we compare two prediction formats for the router: Sample-wise Prediction: We append learnable [CLS] token to the input sequence. With full attention, this tokens final hidden state, which captures global context, is linearly projected to predict the logit matrix . In this setup, the entire prompt shares one routing solution. Token-specific Prediction: Instead of introducing new token, we use the hidden states of the prompt embeddings directly. shared linear projection maps each tokens representation to the logit matrix . Thus, each token in the prompt defines its own routing pattern. As shown in Table 2, token-wise prediction outperforms the sample-wise alternative. This observation supports our hypothesis on token-level dynamics and aligns with prior studies (Bae et al., 2025; Raposo et al., 2024) revealing that the behavior of tokens across LLMs varies significantly. We provide further validation in Appendix B.7, where visualizations show that the router naturally learns diverse, token-specific routing patterns without explicit regularization. (a) Sample-wise prediction. (b) Token-specific prediction. Figure 5 Output configurations for the router. (a) Predicts single logit for the entire prompt; (b) predicts logits for each token. Table 2 FID and CLIP results on MJHQ with different routers prediction format. The token-wise prediction consistently achieves superior results. Sample-wise Token-specific FID 21.66 CLIP 21.48 20.17 21.63 Layer selection should be adaptive, not fixed. To validate our hypothesis that features from different layers should be adaptively selected, we compare MoS against three fixed layer-selection baselines, under the same training recipe and data: Hand-crafted Routing: We first test baseline with predefined, static connections (i.e., by evenly skipping subset of text encoder layers). As shown in Table 3, MoS surpasses this rigid hand-crafted design on both FID and CLIP, highlighting the importance of adaptive selection. supplementary training-step analysis is in Appendix Fig. 11. Cross-Attention (Fixed-Layer): Next, we compare MoS with 5B-parameter cross-attention model that uses customized text encoders (Xu et al., 2023; Tay et al., 2022; Liu et al., 2024b). Such models typically use fixed, final-layer feature for conditioning. Table 4 shows that while our cross-attention baseline is strong (0.74 GenEval, 83.40 DPG), MoS trained with the same data and parameter budget significantly outperforms it (0.79 GenEval, 85.61 DPG). Mixture-of-Transformers (MoT): Finally, we compare MoS with MoT, which enforces rigid one-toone layer correspondence and has been shown to outperform crossand self-attention (Tang et al., 2025). Under fair comparison using identical parameters, data, and compute, Fig. 6 shows that MoS consistently outperforms MoT across all training stages. These quantitative results, supported by visualizations of the routers learned patterns (Appendix B.7), demonstrate the clear superiority of adaptive, state-based selection. This aligns with prior work (Clark et al., 2019; Rai et al., 2024; Raposo et al., 2024), which shows that token representations in LLMs play different functional roles across layers, implying that the fixed representation scheme might be inherently suboptimal. Table 3 FID and CLIP results on MJHQ comparing hand-crafted routing and MoS. MoS significantly outperforms the handcrafted routing baseline. Table 4 GenEval and DPG scores comparing the cross-attention baseline and MoS. MoS consistently achieves better results than the cross-attention baseline. Model FID CLIP Hand-Crafted 21.51 22.04 MoS 17.77 22.91 Model GenEval Cross-Attn MoS 0.74 0.79 DPG 83.40 85.61 Figure 6 Ablation results on GenEval and DPG scores comparing the MoT baseline with MoS. MoS consistently outperforms MoT across all training steps on both benchmarks. Router Efficiency The MoS router is designed to be lightweight transformer, adding negligible computational overhead. We report end-to-end latency for generating 1024 1024 image on single A100 GPU in Fig. 7. With 3B generation tower, the router contributes only 0.008s per iteration, cost that is effectively insignificant. This fractional overhead becomes even smaller when MoS is paired with larger generation towers. This lightweight design leads to improved overall latency performance. When benchmarked against two state-of-the-art image generation models: Qwen-Image (Wu et al., 2025a) and Bagel (Deng et al., 2025), MoS achieves lower end-to-end latency and thus higher throughput, demonstrating its efficiency. Additional Discussions We provide extensive additional analyses in the Appendix. We further dissect the routers core design, including an ablation on which features are most beneficial to route (Appendix B.1), an analysis of its architecture (Appendix B.2), and an empirical validation of the ϵ-greedy and top-k selection strategies (Appendix B.3). We also confirm the scalability and robustness of our framework: we verify that MoS adheres to standard scaling laws (Appendix B.4), demonstrate the specific advantages of our dualtower design for image editing (Appendix B.5), and show that MoS benefits from common inference-time optimizations (Appendix B.6)."
        },
        {
            "title": "4.2 Results on Text-to-Image Generation",
            "content": "We begin by evaluating MoS on standard text-to-image benchmarks. In Table 5, we report its performance on GenEval (Ghosh et al., 2023), DPG (Hu et al., 2024), WISE (Niu et al., 2025), and oneIG-EN (Chang et al., 2025). For comparison, we select 3 state-of-the-art methods representing different interaction types: SANA-1.5 (cross-attention) (Xie et al., 2025a), Flux.1[Dev] (self-attention) (Labs, 2024), and Bagel (MoT) (Deng et al., 2025). We also include Qwen-Image (Wu et al., 2025a) for reference, which establishes new state of the art for self-attention but uses more than four times the parameters of our model. The empirical results demonstrate that MoS-L, despite its smaller parameter count, consistently outperforms existing approaches across all benchmarks, highlighting the effectiveness of the MoS mechanism. 9 (a) Latency contribution of individual components. (b) Latency comparison with other methods. Figure 7 Latency analysis of MoS. We evaluate the latency of MoS-Image-S for generating 10242 images. The inference process includes three stages: (i) input encodingembedding the input text and initializing noise; (ii) generationiterative application of the generation tower and router; and (iii) decodingtransforming the latent code into the final image. We further compare MoS with existing methods on the same computational platform (a single A100 GPU). For reproducibility, baseline results are obtained using the default configurations in diffusers (Face, 2022; Ge, 2025). Table 5 Performance of Foundational Diffusion Models on Text-to-Image and Image Editing. The parameter size refers to the number of learnable parameters. This table summarizes state-of-the-art models across various multimodal interaction techniques. MoS achieves leading performance on most benchmarks and, in some cases, rivals product-level models with 20B or more parameters. The reported results are primarily drawn from Wu et al. (2025a) and Niu et al. (2025). SoTA Methods Interaction Type Model Parameters Qwen-Image(Wu et al., 2025a) Self-Attn SANA-1.5 (Xie et al., 2025a) FLUX.1[Dev] (Labs, 2024) Bagel (Deng et al., 2025) Cross-Attn Self-Attn MoT MoS-S MoS-L MoS MoS 20B 4.8B 12B 14B 3B 5B Image Generation Image Editing GenEval[0 0.87 1] DPG[ 100] 88.32 WISE[0 0.62 1] oneIG[0 0. 1] GEdit[0 7.56 10] ImgEdit[0 4. 5] 0.81 0.66 0.88 0.89 0.90 84.70 83.84 - 86.33 87.01 - 0.50 0.52 0.47 0.54 0.33 0.43 0. 0.50 0.52 - - 6.52 7.41 7.86 - - 3. 4.17 4.33 In Appendix, we report full GenEval results in Table 6, DPG-Bench results in Table 7, WISE results in Table 8, and OneIG performance in Table 9. When benchmarking MoS, no prompt rewriting strategies (e.g., self-CoT (Deng et al., 2025)) are applied; commercial models (OpenAI, 2025a; Team, 2025, 2024b; Google, 2025; Gao et al., 2025; OpenAI, 2025b) that may employ such techniques are shown in gray, as their exact generation pipelines and model sizes are unclear. Model size here refers only to learnable parameters, excluding text encoder or understanding tower parameters when frozen. Overall, MoS demonstrates strong performance, trailing only slightly behind Qwen-Image, which is substantially larger (20B vs 5B). Furthermore, Fig. 15 in the Appendix presents representative samples generated by MoS. As shown in Fig. 16 (see Appendix), we further compare our generated samples with recent advanced methods. The selected examples are intentionally challenging for standard text-to-image models, as each contains more than four entities in single scene, with fine-grained attributes such as location, color, and texture. To increase difficulty, we also include cases with dense visual text. The results show that our method aligns with input prompts more precisely and performs on par with, or in some cases surpasses, Qwen-Image, while significantly outperforming other baselines."
        },
        {
            "title": "4.3 Results on Instruction-based Image Editing",
            "content": "We evaluate the image editing performance of MoS on two benchmarks: ImgEdit (Ye et al., 2025) and GEdit (Liu et al., 2025d). All evaluations are conducted automatically using GPT-4o (Hurst et al., 2024). The benchmarks cover diverse set of dimensions, including object-level editing, scene-level editing, text 10 editing and hybrid challenging cases. As shown in Table 5, our 5B-parameter model achieves state-of-the-art performance on these two benchmarks. The detailed benchmarking results are provided in Table 10 for ImgEdit and Table 11 for GEdit in the Appendix. We further provide visual comparisons on image editing tasks in Fig. 17 (see Appendix), where MoS consistently produces results that precisely align with the given instructions and reference images, outperforming competing methods."
        },
        {
            "title": "4.4 Results on Image Understanding",
            "content": "Our model can be regarded as two-stage framework for unifying the understanding and generation tasks: the first stage addresses understanding tasks, while the second focuses on generation. As the understanding tower is frozen in our study, its image/video understanding performance remains identical to previously reported results and is therefore omitted. Importantly, even without additional training, the understanding capacity strengthens the generation tower in several dimensions. For instance, with self-CoT (Deng et al., 2025), the understanding model progressively generates reasoning-based caption, which then serves as contextual guidance for the generation tower to synthesize images. Following this approach, our performance on world knowledge reasoning improves on the WISE benchmarkfrom 0.54 to 0.65 for MoS-L and from 0.47 to 0.55 for MoS-S."
        },
        {
            "title": "5 Conclusion",
            "content": "We present new family of multimodal diffusion models based on Mixture of States (MoS) routing. By introducing learnable router that dynamically selects hidden states across modalities, tokens, and denoising steps, MoS overcomes the rigid synchronization of prior architectures and supports asymmetric understanding and generation towers. This efficiency-oriented design executes the larger understanding model only once while dedicating the generation module to fine-grained detail synthesis. Extensive evaluations demonstrate that MoS achieves state-of-the-art performance across multiple benchmarks, rivaling or surpassing models up to four times larger, yet at markedly reduced computational cost. These results position MoS as both practical and conceptual step toward scaling multimodal generative models, providing flexible, efficient, and unified foundation for future research and deployment."
        },
        {
            "title": "6 Limitation and Future Studies",
            "content": "One-Way to Dual-Way Setting. MoT has demonstrated strong scalability under early-fusion training. In contrast, while MoS shows promising results for multimodal generation, its effectiveness in early-fusion settings remains to be validated. principled extension is to endow the router with multiple projection layers to establish bidirectional transformer connections. We defer this exploration to future work due to computational and data constraints. In this paper, we primarily adopt SFT as the post-training strategy for our Human Preference Alignment. models. Recent studies have explored applying CoT to multimodal generation (Guo et al., 2025) or employing GRPO (Shao et al., 2024) to better align generated samples with human preferences (Liu et al., 2025c). Since our models behavior remains consistent with standard diffusion models, it can likewise benefit from such post-training techniques. We leave this direction for future work. Efficiency Improvement. Our model is relatively smaller than prior state-of-the-art models, making it naturally more efficient. Nonetheless, it could be further accelerated through techniques such as low-precision quantization (Xie et al., 2024), model distillation (Yin et al., 2024), or feature caching (Liu et al., 2025b; Kahatapitiya et al., 2025). We leave these directions for future exploration. Explainability. The MoS router predicts the relative importance of each potential connection, which offers basis for interpreting cross-modal interactions. While this property may provide insights into model explainability, such analysis lies beyond the scope of this work and is left for future investigation. 11 Visual Artifacts. The primary goal of this paper is to address the challenge of instruction-following in multimodal generation. Nevertheless, our model still faces issues similar to other DiT or unified models, such as producing artifacts when the generated objects are very small (see our visualizations)."
        },
        {
            "title": "Acknowledgements",
            "content": "The authors thank Yuhui Wang, Gordan (Guocheng) Qian, Jackson (Kuan-Chieh) Wang, Jinheng Xie, Enze Xie, Song Han, Chandan Akiti and Jinjie Mai for their valuable suggestions and contributions to the paper review. Haozhe Liu, Mingchen Zhuge, Shuming Liu and Jürgen Schmidhuber were supported by funding from the King Abdullah University of Science and Technology (KAUST) - Center of Excellence for Generative AI under award number 5940 and the SDAIA-KAUST Center of Excellence in Data Science and Artificial Intelligence."
        },
        {
            "title": "References",
            "content": "Zhipu AI and THUDM / CogView Team. Cogview4. https://github.com/THUDM/CogView4, 2025. Sangmin Bae, Yujin Kim, Reza Bayat, Sungnyun Kim, Jiyoun Ha, Tal Schuster, Adam Fisch, Hrayr Harutyunyan, Ziwei Ji, Aaron Courville, et al. Mixture-of-recursions: Learning dynamic recursive depths for adaptive token-level computation. arXiv preprint arXiv:2507.10524, 2025. Jason Baldridge, Jakob Bauer, Mukul Bhutani, Nicole Brichtova, Andrew Bunner, Lluis Castrejon, Kelvin Chan, Yichang Chen, Sander Dieleman, Yuqing Du, et al. Imagen 3. arXiv preprint arXiv:2408.07009, 2024. Stephen Batifol, Andreas Blattmann, Frederic Boesel, Saksham Consul, Cyril Diagne, Tim Dockhorn, Jack English, Zion English, Patrick Esser, Sumith Kulal, et al. Flux. 1 kontext: Flow matching for in-context image generation and editing in latent space. arXiv e-prints, pages arXiv2506, 2025. James Betker, Gabriel Goh, Li Jing, Tim Brooks, Jianfeng Wang, Linjie Li, Long Ouyang, Juntang Zhuang, Joyce Lee, Yufei Guo, et al. Improving image generation with better captions. Computer Science. https://cdn. openai. com/papers/dall-e-3. pdf, 2(3):8, 2023. Tim Brooks, Aleksander Holynski, and Alexei Efros. Instructpix2pix: Learning to follow image editing instructions. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR), pages 1839218402, 2023. Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. Language models are few-shot learners. Advances in neural information processing systems, 33:18771901, 2020. Qi Cai, Jingwen Chen, Yang Chen, Yehao Li, Fuchen Long, Yingwei Pan, Zhaofan Qiu, Yiheng Zhang, Fengbin Gao, Peihan Xu, et al. Hidream-i1: high-efficient image generative foundation model with sparse diffusion transformer. arXiv preprint arXiv:2505.22705, 2025. Jingjing Chang, Yixiao Fang, Peng Xing, Shuhan Wu, Wei Cheng, Rui Wang, Xianfang Zeng, Gang Yu, and Hai-Bao Chen. Oneig-bench: Omni-dimensional nuanced evaluation for image generation. arXiv preprint arxiv:2506.07977, 2025. Chen Chen, Rui Qian, Wenze Hu, Tsu-Jui Fu, Jialing Tong, Xinze Wang, Lezhi Li, Bowen Zhang, Alex Schwing, Wei Liu, et al. Dit-air: Revisiting the efficiency of diffusion model architecture design in text to image generation. arXiv preprint arXiv:2503.10618, 2025a. Jiuhai Chen, Zhiyang Xu, Xichen Pan, Yushi Hu, Can Qin, Tom Goldstein, Lifu Huang, Tianyi Zhou, Saining Xie, Silvio Savarese, et al. Blip3-o: family of fully open unified multimodal models-architecture, training and dataset. arXiv preprint arXiv:2505.09568, 2025b. Junsong Chen, Jincheng Yu, Chongjian Ge, Lewei Yao, Enze Xie, Yue Wu, Zhongdao Wang, James Kwok, Ping Luo, Huchuan Lu, et al. Pixart-alpha: Fast training of diffusion transformer for photorealistic text-to-image synthesis. arXiv preprint arXiv:2310.00426, 2023. Junsong Chen, Chongjian Ge, Enze Xie, Yue Wu, Lewei Yao, Xiaozhe Ren, Zhongdao Wang, Ping Luo, Huchuan Lu, and Zhenguo Li. Pixart-σ: Weak-to-strong training of diffusion transformer for 4k text-to-image generation. In Proceedings of the European Conference on Computer Vision (ECCV), pages 7491. Springer, 2024a. 12 Shoufa Chen, Mengmeng Xu, Jiawei Ren, Yuren Cong, Sen He, Yanping Xie, Animesh Sinha, Ping Luo, Tao Xiang, and Juan-Manuel Perez-Rua. Gentron: Diffusion transformers for image and video generation. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2024b. Xiaokang Chen, Zhiyu Wu, Xingchao Liu, Zizheng Pan, Wen Liu, Zhenda Xie, Xingkai Yu, and Chong Ruan. Janus-pro: Unified multimodal understanding and generation with data and model scaling. arXiv preprint arXiv:2501.17811, 2025c. Zhe Chen, Jiannan Wu, Wenhai Wang, Weijie Su, Guo Chen, Sen Xing, Muyan Zhong, Qinglong Zhang, Xizhou Zhu, Lewei Lu, et al. Internvl: Scaling up vision foundation models and aligning for generic visual-linguistic tasks. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR), pages 2418524198, 2024c. Jang Hyun Cho, Andrea Madotto, Effrosyni Mavroudi, Triantafyllos Afouras, Tushar Nagarajan, Muhammad Maaz, Yale Song, Tengyu Ma, Shuming Hu, Suyog Jain, et al. Perceptionlm: Open-access data and models for detailed visual understanding. arXiv preprint arXiv:2504.13180, 2025. Kevin Clark, Urvashi Khandelwal, Omer Levy, and Christopher Manning. What does bert look at? an analysis of berts attention. arXiv preprint arXiv:1906.04341, 2019. Róbert Csordás, Kazuki Irie, Jürgen Schmidhuber, Christopher Potts, and Christopher Manning. Moeut: Mixture-ofexperts universal transformers. Advances in Neural Information Processing Systems (NeurIPS), 37:2858928614, 2024. Xiaoliang Dai, Ji Hou, Chih-Yao Ma, Sam Tsai, Jialiang Wang, Rui Wang, Peizhao Zhang, Simon Vandenhende, Xiaofang Wang, Abhimanyu Dubey, et al. Emu: Enhancing image generation models using photogenic needles in haystack. arXiv preprint arXiv:2309.15807, 2023. Timothée Darcet, Maxime Oquab, Julien Mairal, and Piotr Bojanowski. Vision transformers need registers. arXiv preprint arXiv:2309.16588, 2023. Google DeepMind. Gemini 2.0. https://gemini.google.com/, 2025. Chaorui Deng, Deyao Zhu, Kunchang Li, Chenhui Gou, Feng Li, Zeyu Wang, Shu Zhong, Weihao Yu, Xiaonan Nie, Ziang Song, et al. Emerging properties in unified multimodal pretraining. arXiv preprint arXiv:2505.14683, 2025. Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. Bert: Pre-training of deep bidirectional transformers for language understanding. In Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (Long and Short Papers), 2019. Prafulla Dhariwal and Alexander Nichol. Diffusion models beat GANs on image synthesis. In Advances in Neural Information Processing Systems (NeurIPS), 2021. Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, et al. An image is worth 16x16 words: Transformers for image recognition at scale. In Proceedings of the International Conference on Learning Representations (ICLR), 2020. David Eigen, MarcAurelio Ranzato, and Ilya Sutskever. Learning factored representations in deep mixture of experts. arXiv preprint arXiv:1312.4314, 2013. Patrick Esser, Sumith Kulal, Andreas Blattmann, Rahim Entezari, Jonas Müller, Harry Saini, Yam Levi, Dominik Lorenz, Axel Sauer, Frederic Boesel, et al. Scaling rectified flow transformers for high-resolution image synthesis. In Proceedings of the International Conference on Machine Learning (ICML), 2024a. Patrick Esser, Sumith Kulal, Andreas Blattmann, Rahim Entezari, Jonas Müller, Harry Saini, Yam Levi, Dominik Lorenz, Axel Sauer, Frederic Boesel, et al. Scaling rectified flow transformers for high-resolution image synthesis. In Proceedings of the International Conference on Machine Learning (ICML), 2024b. Hugging Face. Diffusers: State-of-the-art diffusion models. https://github.com/huggingface/diffusers, 2022. William Fedus, Barret Zoph, and Noam Shazeer. Switch transformers: Scaling to trillion parameter models with simple and efficient sparsity. Journal of Machine Learning Research (JMLR), 23(120):139, 2022. Yu Gao, Lixue Gong, Qiushan Guo, Xiaoxia Hou, Zhichao Lai, Fanshi Li, Liang Li, Xiaochen Lian, Chao Liao, Liyang Liu, et al. Seedream 3.0 technical report. arXiv preprint arXiv:2504.11346, 2025. Jiaxin Ge. Diffusers-bagel: Bagel custom pipeline for diffusers (hugging face). https://huggingface.co/JiaxinGe/ Diffusers-BAGEL, 2025. arXiv preprint arXiv:2505.14683. Yuying Ge, Sijie Zhao, Jinguo Zhu, Yixiao Ge, Kun Yi, Lin Song, Chen Li, Xiaohan Ding, and Ying Shan. Seed-x: Multimodal models with unified multi-granularity comprehension and generation. arXiv preprint arXiv:2404.14396, 2024. 13 Zigang Geng, Yibing Wang, Yeyao Ma, Chen Li, Yongming Rao, Shuyang Gu, Zhao Zhong, Qinglin Lu, Han Hu, Xiaosong Zhang, et al. X-omni: Reinforcement learning makes discrete autoregressive image generative models great again. arXiv preprint arXiv:2507.22058, 2025. Dhruba Ghosh, Hannaneh Hajishirzi, and Ludwig Schmidt. Geneval: An object-focused framework for evaluating text-to-image alignment. Advances in Neural Information Processing Systems (NeurIPS), 36:5213252152, 2023. Google. Imagen. https://deepmind.google/models/imagen/, 2025. Ziyu Guo, Renrui Zhang, Chengzhuo Tong, Zhizheng Zhao, Rui Huang, Haoquan Zhang, Manyuan Zhang, Jiaming Liu, Shanghang Zhang, Peng Gao, et al. Can we generate images with cot? lets verify and reinforce image generation step by step. arXiv preprint arXiv:2501.13926, 2025. John Hampshire and Alex Waibel. Connectionist architectures for multi-speaker phoneme recognition. Advances in neural information processing systems, 2, 1989. Jiaming Han, Hao Chen, Yang Zhao, Hanyu Wang, Qi Zhao, Ziyan Yang, Hao He, Xiangyu Yue, and Lu Jiang. Vision as dialect: Unifying visual understanding and generation via text-aligned representations. arXiv preprint arXiv:2506.18898, 2025. Yizeng Han, Gao Huang, Shiji Song, Le Yang, Honghui Wang, and Yulin Wang. Dynamic neural networks: survey. IEEE Transactions on Pattern Analysis and Machine Intelligence (PAMI), 44(11):74367456, 2021. Alex Henry, Prudhvi Raj Dachapally, Shubham Pawar, and Yuxuan Chen. Query-key normalization for transformers. arXiv preprint arXiv:2010.04245, 2020. Martin Heusel, Hubert Ramsauer, Thomas Unterthiner, Bernhard Nessler, and Sepp Hochreiter. Gans trained by two time-scale update rule converge to local nash equilibrium. Advances in Neural Information Processing Systems (NeurIPS), 30, 2017. Jonathan Ho, Ajay Jain, and Pieter Abbeel. Denoising diffusion probabilistic models. In Advances in Neural Information Processing Systems (NeurIPS), 2020. Pin-Lun Hsu, Yun Dai, Vignesh Kothapalli, Qingquan Song, Shao Tang, Siyu Zhu, Steven Shimizu, Shivam Sahni, Haowen Ning, Yanning Chen, and Zhipeng Wang. Liger-kernel: Efficient triton kernels for LLM training. In Championing Open-source DEvelopment in ML Workshop @ ICML25, 2025. https://openreview.net/forum?id=36SjAIT42G. Xiwei Hu, Rui Wang, Yixiao Fang, Bin Fu, Pei Cheng, and Gang Yu. Ella: Equip diffusion models with llm for enhanced semantic alignment. arXiv preprint arXiv:2403.05135, 2024. Aaron Hurst, Adam Lerer, Adam Goucher, Adam Perelman, Aditya Ramesh, Aidan Clark, AJ Ostrow, Akila Welihinda, Alan Hayes, Alec Radford, et al. Gpt-4o system card. arXiv preprint arXiv:2410.21276, 2024. Aleksei Grigorevich Ivakhnenko and Valentin Grigorevich Lapa. Cybernetic predicting devices. Technical Report, 1966. Alexey Grigorevich Ivakhnenko. Polynomial theory of complex systems. IEEE transactions on Systems, Man, and Cybernetics, (4):364378, 2007. Robert Jacobs, Michael Jordan, Steven Nowlan, and Geoffrey Hinton. Adaptive mixtures of local experts. Neural computation, 3(1):7987, 1991. Christopher Jarzynski. Equilibrium free-energy differences from nonequilibrium measurements: master-equation approach. Physical Review E, 1997. Albert Jiang, Alexandre Sablayrolles, Antoine Roux, Arthur Mensch, Blanche Savary, Chris Bamford, Devendra Singh arXiv preprint Chaplot, Diego de las Casas, Emma Bou Hanna, Florian Bressand, et al. Mixtral of experts. arXiv:2401.04088, 2024. Kumara Kahatapitiya, Haozhe Liu, Sen He, Ding Liu, Menglin Jia, Chenyang Zhang, Michael Ryoo, and Tian Xie. Adaptive caching for faster video generation with diffusion transformers. Proceedings of the International Conference on Computer Vision (ICCV), 2025. Black Forest Labs. Flux. https://github.com/black-forest-labs/flux, 2024. Dmitry Lepikhin, HyoukJoong Lee, Yuanzhong Xu, Dehao Chen, Orhan Firat, Yanping Huang, Maxim Krikun, Noam Shazeer, and Zhifeng Chen. Gshard: Scaling giant models with conditional computation and automatic sharding. arXiv preprint arXiv:2006.16668, 2020. 14 Daiqing Li, Aleks Kamko, Ehsan Akhgari, Ali Sabet, Linmiao Xu, and Suhail Doshi. Playground v2.5: Three insights towards enhancing aesthetic quality in text-to-image generation, 2024a. Zhimin Li, Jianwei Zhang, Qin Lin, Jiangfeng Xiong, Yanxin Long, Xinchi Deng, Yingfang Zhang, Xingchao Liu, Minbin Huang, Zedong Xiao, et al. Hunyuan-dit: powerful multi-resolution diffusion transformer with fine-grained chinese understanding. arXiv preprint arXiv:2405.08748, 2024b. Weixin Liang, LILI YU, Liang Luo, Srini Iyer, Ning Dong, Chunting Zhou, Gargi Ghosh, Mike Lewis, Wen tau Yih, Luke Zettlemoyer, and Xi Victoria Lin. Mixture-of-transformers: sparse and scalable architecture for multi-modal foundation models. Transactions on Machine Laerning Research (TMLR), 2025. ISSN 2835-8856. https://openreview.net/ forum?id=Nu6N69i8SB. Chao Liao, Liyang Liu, Xun Wang, Zhengxiong Luo, Xinyu Zhang, Wenliang Zhao, Jie Wu, Liang Li, Zhi Tian, and Weilin Huang. Mogao: An omni foundation model for interleaved multi-modal generation. arXiv preprint arXiv:2505.05472, 2025. Bin Lin, Zongjian Li, Xinhua Cheng, Yuwei Niu, Yang Ye, Xianyi He, Shenghai Yuan, Wangbo Yu, Shaodong Wang, Yunyang Ge, et al. Uniworld: High-resolution semantic encoders for unified visual understanding and generation. arXiv preprint arXiv:2506.03147, 2025. Shanchuan Lin, Bingchen Liu, Jiashi Li, and Xiao Yang. Common diffusion noise schedules and sample steps are flawed. In IEEE Winter Conference on Applications of Computer Vision (WACV), pages 54045411, 2024. Yaron Lipman, Ricky TQ Chen, Heli Ben-Hamu, Maximilian Nickel, and Matt Le. Flow matching for generative modeling. In Proceedings of the International Conference on Learning Representations (ICLR), 2023. Bingchen Liu, Ehsan Akhgari, Alexander Visheratin, Aleks Kamko, Linmiao Xu, Shivam Shrirao, Chase Lambert, Joao Souza, Suhail Doshi, and Daiqing Li. Playground v3: Improving text-to-image alignment with deep-fusion large language models. arXiv preprint arXiv:2409.10695, 2024a. Haozhe Liu, Shikun Liu, Zijian Zhou, Mengmeng Xu, Yanping Xie, Xiao Han, Juan Camilo Perez, Ding Liu, Kumara Kahatapitiya, Menglin Jia, et al. Mardini: Masked auto-regressive diffusion for video generation at scale. Transactions on Machine Laerning Research (TMLR), 2025a. Haozhe Liu, Wentian Zhang, Jinheng Xie, Francesco Faccio, Mengmeng Xu, Tao Xiang, Mike Zheng Shou, Juan-Manuel Perez-Rua, and Jürgen Schmidhuber. Faster diffusion through temporal attention decomposition. Transactions on Machine Laerning Research (TMLR), 2025b. Jie Liu, Gongye Liu, Jiajun Liang, Yangguang Li, Jiaheng Liu, Xintao Wang, Pengfei Wan, Di Zhang, and Wanli Ouyang. Flow-grpo: Training flow matching models via online rl. arXiv preprint arXiv:2505.05470, 2025c. Shikun Liu, Linxi Fan, Edward Johns, Zhiding Yu, Chaowei Xiao, and Anima Anandkumar. Prismer: vision-language model with an ensemble of experts. arXiv preprint arXiv:2303.02506, 2023. Shiyu Liu, Yucheng Han, Peng Xing, Fukun Yin, Rui Wang, Wei Cheng, Jiaqi Liao, Yingming Wang, Honghao Fu, Chunrui Han, Guopeng Li, Yuang Peng, Quan Sun, Jingwei Wu, Yan Cai, Zheng Ge, Ranchen Ming, Lei Xia, Xianfang Zeng, Yibo Zhu, Binxing Jiao, Xiangyu Zhang, Gang Yu, and Daxin Jiang. Step1x-edit: practical framework for general image editing. arXiv preprint arXiv:2504.17761, 2025d. Zeyu Liu, Weicong Liang, Zhanhao Liang, Chong Luo, Ji Li, Gao Huang, and Yuhui Yuan. Glyph-byt5: customized text encoder for accurate visual text rendering. In European Conference on Computer Vision, pages 361377. Springer, 2024b. Radford Neal. Annealed importance sampling. Statistics and computing, 2001. Yuwei Niu, Munan Ning, Mengren Zheng, Weiyang Jin, Bin Lin, Peng Jin, Jiaqi Liao, Kunpeng Ning, Chaoran Feng, Bin Zhu, and Li Yuan. Wise: world knowledge-informed semantic evaluation for text-to-image generation. arXiv preprint arXiv:2503.07265, 2025. OpenAI. Dalle 3. https://openai.com/research/dall-e-3, 2025a. Accessed: 2025-09-16. OpenAI. gpt-image-1. https://openai.com/index/image-generation-api/, April 2025b. OpenAI blog post introducing the model. Accessed: 2025-09-15. Xichen Pan, Satya Narayan Shukla, Aashu Singh, Zhuokai Zhao, Shlok Kumar Mishra, Jialiang Wang, Zhiyang Xu, Jiuhai Chen, Kunpeng Li, Felix Juefei-Xu, et al. Transfer between modalities with metaqueries. arXiv preprint arXiv:2504.06256, 2025. 15 William Peebles and Saining Xie. Scalable diffusion models with transformers. In Proceedings of the International Conference on Computer Vision (ICCV), pages 41954205, 2023. Dustin Podell, Zion English, Kyle Lacey, Andreas Blattmann, Tim Dockhorn, Jonas Müller, Joe Penna, and Robin Rombach. Sdxl: Improving latent diffusion models for high-resolution image synthesis. arXiv preprint arXiv:2307.01952, 2023. Adam Polyak, Amit Zohar, Andrew Brown, Andros Tjandra, Animesh Sinha, Ann Lee, Apoorv Vyas, Bowen Shi, Chih-Yao Ma, Ching-Yao Chuang, et al. Movie gen: cast of media foundation models. arXiv preprint arXiv:2410.13720, 2024a. Adam Polyak, Amit Zohar, Andrew Brown, Andros Tjandra, Animesh Sinha, Ann Lee, Apoorv Vyas, Bowen Shi, Chih-Yao Ma, Ching-Yao Chuang, et al. Movie gen: cast of media foundation models. arXiv preprint arXiv:2410.13720, 2024b. Qi Qin, Le Zhuo, Yi Xin, Ruoyi Du, Zhen Li, Bin Fu, Yiting Lu, Jiakang Yuan, Xinyue Li, Dongyang Liu, et al. Lumina-image 2.0: unified and efficient image generative framework. arXiv preprint arXiv:2503.21758, 2025. Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, et al. Learning transferable visual models from natural language supervision. In Proceedings of the International Conference on Machine Learning (ICML), pages 87488763. PmLR, 2021. Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, Peter Liu, et al. Exploring the limits of transfer learning with unified text-to-text transformer. Journal of Machine Learning Research (JMLR), 2020. Daking Rai, Yilun Zhou, Shi Feng, Abulhair Saparov, and Ziyu Yao. practical review of mechanistic interpretability for transformer-based language models. arXiv preprint arXiv:2407.02646, 2024. Aditya Ramesh, Prafulla Dhariwal, Alex Nichol, Casey Chu, and Mark Chen. Hierarchical text-conditional image generation with clip latents. arXiv preprint arXiv:2204.06125, 2022. David Raposo, Sam Ritter, Blake Richards, Timothy Lillicrap, Peter Conway Humphreys, and Adam Santoro. Mixtureof-depths: Dynamically allocating compute in transformer-based language models. arXiv preprint arXiv:2404.02258, 2024. Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Björn Ommer. High-resolution image synthesis with latent diffusion models. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2022. Chitwan Saharia, William Chan, Saurabh Saxena, Lala Li, Jay Whang, Emily Denton, Kamyar Ghasemipour, Raphael Gontijo Lopes, Burcu Karagol Ayan, Tim Salimans, et al. Photorealistic text-to-image diffusion models with deep language understanding. In Advances in Neural Information Processing Systems (NeurIPS), 2022. Jürgen Schmidhuber. Learning to control fast-weight memories: An alternative to dynamic recurrent networks. Neural Computation, 4(1):131139, 1992. Zhihong Shao, Peiyi Wang, Qihao Zhu, Runxin Xu, Junxiao Song, Xiao Bi, Haowei Zhang, Mingchuan Zhang, YK Li, Yang Wu, et al. Deepseekmath: Pushing the limits of mathematical reasoning in open language models. arXiv preprint arXiv:2402.03300, 2024. Noam Shazeer, Azalia Mirhoseini, Krzysztof Maziarz, Andy Davis, Quoc Le, Geoffrey Hinton, and Jeff Dean. Outrageously large neural networks: The sparsely-gated mixture-of-experts layer. arXiv preprint arXiv:1701.06538, 2017. Weijia Shi, Xiaochuang Han, Chunting Zhou, Weixin Liang, Xi Victoria Lin, Luke Zettlemoyer, and Lili Yu. Lmfusion: Adapting pretrained language models for multimodal generation. arXiv preprint arXiv:2412.15188, 2024. Qiao Sun, Zhicheng Jiang, Hanhong Zhao, and Kaiming He. Is noise conditioning necessary for denoising generative models? In Proceedings of the International Conference on Machine Learning (ICML), 2025. Bingda Tang, Boyang Zheng, Sayak Paul, and Saining Xie. Exploring the deep fusion of large language models and diffusion transformers for text-to-image synthesis. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2025. Yi Tay, Mostafa Dehghani, Vinh Tran, Xavier Garcia, Jason Wei, Xuezhi Wang, Hyung Won Chung, Siamak Shakeri, Dara Bahri, Tal Schuster, et al. Ul2: Unifying language learning paradigms. arXiv preprint arXiv:2205.05131, 2022. Chameleon Team. Chameleon: Mixed-modal early-fusion foundation models. arXiv preprint arXiv:2405.09818, 2024a. Gemma Team, Morgane Riviere, Shreya Pathak, Pier Giuseppe Sessa, Cassidy Hardin, Surya Bhupatiraju, Léonard Hussenot, Thomas Mesnard, Bobak Shahriari, Alexandre Ramé, et al. Gemma 2: Improving open language models at practical size. arXiv preprint arXiv:2408.00118, 2024. Kuaishou Kolors Team. Kolors2.0. https://app.klingai.com/, 2025. Recraft Team. Recraft v3. https://www.recraft.ai/, 2024b. Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timothée Lacroix, Baptiste Rozière, Naman Goyal, Eric Hambro, Faisal Azhar, et al. Llama: Open and efficient foundation language models. arXiv preprint arXiv:2302.13971, 2023. Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan Gomez, Łukasz Kaiser, and Illia Polosukhin. Attention is all you need. Advances in Neural Information Processing Systems (NeurIPS), 2017. Team Wan, Ang Wang, Baole Ai, Bin Wen, Chaojie Mao, Chen-Wei Xie, Di Chen, Feiwu Yu, Haiming Zhao, Jianxiao Yang, et al. Wan: Open and advanced large-scale video generative models. arXiv preprint arXiv:2503.20314, 2025. Xinlong Wang, Xiaosong Zhang, Zhengxiong Luo, Quan Sun, Yufeng Cui, Jinsheng Wang, Fan Zhang, Yueze Wang, Zhen Li, Qiying Yu, et al. Emu3: Next-token prediction is all you need. arXiv preprint arXiv:2409.18869, 2024. Chenfei Wu, Jiahao Li, Jingren Zhou, Junyang Lin, Kaiyuan Gao, Kun Yan, Sheng-ming Yin, Shuai Bai, Xiao Xu, Yilei Chen, et al. Qwen-image technical report. arXiv preprint arXiv:2508.02324, 2025a. Chenyuan Wu, Pengfei Zheng, Ruiran Yan, Shitao Xiao, Xin Luo, Yueze Wang, Wanli Li, Xiyan Jiang, Yexin Liu, Junjie Zhou, et al. Omnigen2: Exploration to advanced multimodal generation. arXiv preprint arXiv:2506.18871, 2025b. Shengqiong Wu, Hao Fei, Leigang Qu, Wei Ji, and Tat-Seng Chua. Next-gpt: Any-to-any multimodal llm. In Proceedings of the International Conference on Machine Learning (ICML), 2024a. Yecheng Wu, Zhuoyang Zhang, Junyu Chen, Haotian Tang, Dacheng Li, Yunhao Fang, Ligeng Zhu, Enze Xie, Hongxu Yin, Li Yi, et al. Vila-u: unified foundation model integrating visual understanding and generation. arXiv preprint arXiv:2409.04429, 2024b. Shitao Xiao, Yueze Wang, Junjie Zhou, Huaying Yuan, Xingrun Xing, Ruiran Yan, Chaofan Li, Shuting Wang, Tiejun Huang, and Zheng Liu. Omnigen: Unified image generation. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR), pages 1329413304, 2025. Enze Xie, Junsong Chen, Junyu Chen, Han Cai, Haotian Tang, Yujun Lin, Zhekai Zhang, Muyang Li, Ligeng Zhu, Yao Lu, et al. Sana: Efficient high-resolution image synthesis with linear diffusion transformers. arXiv preprint arXiv:2410.10629, 2024. Enze Xie, Junsong Chen, Yuyang Zhao, Jincheng Yu, Ligeng Zhu, Chengyue Wu, Yujun Lin, Zhekai Zhang, Muyang Li, Junyu Chen, et al. Sana 1.5: Efficient scaling of training-time and inference-time compute in linear diffusion transformer. arXiv preprint arXiv:2501.18427, 2025a. Jinheng Xie, Weijia Mao, Zechen Bai, David Junhao Zhang, Weihao Wang, Kevin Qinghong Lin, Yuchao Gu, Zhijie Chen, Zhenheng Yang, and Mike Zheng Shou. Show-o: One single transformer to unify multimodal understanding and generation. Proceedings of the International Conference on Learning Representations (ICLR), 2025b. Jinheng Xie, Zhenheng Yang, and Mike Zheng Shou. Show-o2: Improved native unified multimodal models. arXiv preprint arXiv:2506.15564, 2025c. Hu Xu, Saining Xie, Xiaoqing Ellen Tan, Po-Yao Huang, Russell Howes, Vasu Sharma, Shang-Wen Li, Gargi Ghosh, Luke Zettlemoyer, and Christoph Feichtenhofer. Demystifying clip data. arXiv preprint arXiv:2309.16671, 2023. Yang Ye, Xianyi He, Zongjian Li, Bin Lin, Shenghai Yuan, Zhiyuan Yan, Bohan Hou, and Li Yuan. Imgedit: unified image editing dataset and benchmark. arXiv preprint arXiv:2505.20275, 2025. Tianwei Yin, Michaël Gharbi, Richard Zhang, Eli Shechtman, Fredo Durand, William Freeman, and Taesung Park. One-step diffusion with distribution matching distillation. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR), pages 66136623, 2024. Qifan Yu, Wei Chow, Zhongqi Yue, Kaihang Pan, Yang Wu, Xiaoyang Wan, Juncheng Li, Siliang Tang, Hanwang Zhang, and Yueting Zhuang. Anyedit: Mastering unified high-quality image editing for any idea. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR), pages 2612526135, 2025. Biao Zhang and Rico Sennrich. Root mean square layer normalization. Advances in Neural Information Processing Systems (NeurIPS), 32, 2019. Kai Zhang, Lingbo Mo, Wenhu Chen, Huan Sun, and Yu Su. Magicbrush: manually annotated dataset for instructionguided image editing. Advances in Neural Information Processing Systems (NeurIPS), 36:3142831449, 2023. 17 Zechuan Zhang, Ji Xie, Yu Lu, Zongxin Yang, and Yi Yang. In-context edit: Enabling instructional image editing with in-context generation in large scale diffusion transformer. arXiv preprint arXiv:2504.20690, 2025. Haozhe Zhao, Xiaojian Shawn Ma, Liang Chen, Shuzheng Si, Rujie Wu, Kaikai An, Peiyu Yu, Minjia Zhang, Qing Li, and Baobao Chang. Ultraedit: Instruction-based fine-grained image editing at scale. Advances in Neural Information Processing Systems (NeurIPS), 37:30583093, 2024. Chunting Zhou, Lili Yu, Arun Babu, Kushal Tirumala, Michihiro Yasunaga, Leonid Shamis, Jacob Kahn, Xuezhe Ma, Luke Zettlemoyer, and Omer Levy. Transfusion: Predict the next token and diffuse images with one multi-modal model. arXiv preprint arXiv:2408.11039, 2024."
        },
        {
            "title": "A Implementation Details",
            "content": "To support efficient and stable training, we introduce set of optimizations spanning both system-level infrastructure and model design: QK-Norm: To enhance training stability, we apply QK-Norm (Henry et al., 2020) in each transformer block. Specifically, before the attention operation, we normalize the query and key vectors using RMS-Norm (Zhang and Sennrich, 2019). Modality-specific Norm: We apply separate normalization layers for different modalities, which improves performance while maintaining training stability. Token Registers: Following Darcet et al. (2023), we introduce four auxiliary learnable tokens into the input sequence to enhance training stability, without assigning them explicit training objectives. Diffusion Step Sampling: To accelerate convergence during low-resolution (512 512) pretraining, we adopt logit-normal sampling, which is later replaced by mode sampling (scale = 0.8, shift = 3.0) to adapt to high-resolution (1024 2048) training. 1024 and 2048 Dropping timestep embeddings. Motivated by recent findings (Tang et al., 2025; Sun et al., 2025) and the empirical analysis, we confirm that timestep embeddings provide negligible benefit to the diffusion model while introducing an overhead of 20% parameters. For efficiency, we remove the timestep conditioning from the generation tower. FSDP: We adopt Fully Sharded Data Parallel (FSDP) as our primary distributed training framework and enable activation checkpointing in the high-resolution stages. Low-Precision Training: We employ module-specific mixed-precision strategy. The VAE compressor is maintained in float32 to ensure numerical stability, while the understanding tower is set entirely in bfloat16. For the generation tower and router, we use bfloat16 for all-gather operations and float32 for gradient reduce-scatter. Triton Kernel Optimization: To further improve training throughput, we employ custom Triton kernels, including an RMSNorm kernel and fused FFN kernel. Our implementation builds on the liger-kernel (Hsu et al., 2025). Bucket-wise Dynamic Resolution Training: To support dynamic-resolution training, we adopt resolutiondriven bucket dataloader. Data samples are assigned online to buckets based on their resolution and aspect ratio. Once bucket is filled, it is dispatched to the model for training. Data Reweighting: To achieve balanced performance across different dimensions, we adjust the mixing ratios of the training datasets. The optimal ratios are determined through grid search. Regarding the hyperparameters, we use AdamW with learning rate of 1 4, weight decay of 0.01, and betas set to (0.9, 0.95). The first 4k steps serve as warm-up phase, where the learning rate is linearly increased 5. The to the target value. cosine schedule is then applied to gradually decay the learning rate to 1.5 global batch size is dynamically set to 2048 or 1024, depending on available training resources. For each pre-training stage, we run 400k1200k steps based on visual inspection of convergence, while HQ fine-tuning is performed for 50k steps. We use top-k router (k = 2) with ϵ-greedy exploration (ϵ = 0.05). The training platform comprises both A100 and H100 GPUs; to standardize reporting, we compute the total training cost by counting 2 A100 days as equivalent to 1 H100 day. 10"
        },
        {
            "title": "B Additional Discussions on Router Designs",
            "content": "This ablation study aims to determine the MoS routers operation space, i.e., which features routed across transformers yield the optimal benefit. We begin with the MoT architecture (Liang et al., 2025). To enable representation transfer across transformers, MoT introduces global attention mechanism where keys, values, and queries are shared between towers. In contrast to MoT, competing approach fuses hidden states directly before sequence modeling. As shown in Fig. 8 (a)-(b), these two design philosophies yield four candidates: Global Attention (Head-Projection) 2: Apply projection layer on each head dimension for the key and value vectors from the understanding tower, then concatenate with the generation towers key and value representations, respectively. Global Attention (State-Projection): Apply the projection layer on the hidden dimension, split into multihead vectors, and fuse with the generation towers features. Global Hidden States (Independent-Projection): Concatenate hidden states in the generation tower, then apply separate key-value projection layers before attention. Global Hidden States (Shared Projection): Concatenate hidden states, then apply shared keyvalue projection layer. (a) Global Attention (b) Global Hidden States Figure 8 Illustration of the routers operation space. (a) In the global-attention scheme, keys and values from the understanding branch are projected to the generation branch for cross-modal interaction. (b) In the global hidden-state scheme, hidden representations from both branches are directly concatenated at the input of each transformer block. B.1 Router Operation Space As shown in Fig. 9 (a)-(b), the empirical results clearly indicate that using global hidden states with shared projection layer yields the optimal configuration. Note that the MoS router is excluded from this ablation. B.2 Router Architecture Design In the MoS router, token embeddings from different modalities are normalized using separate RMSNorm layers to align their representation scales. Our analysis shows that this design is key factor in improving performance. To verify its effectiveness, we compare two configurations: (a) shared RMSNorm applied 2Here, we avoid using the term layer-wise attention to describe this operation, to maintain symmetry with the case of global hidden states. Nonetheless, the mechanism is essentially equivalent to the layer-wise attention discussed in previous sections. 20 (a) FID comparison across feature types used for interaction. (b) CLIP comparison across feature types used for interaction. (c) FID performance with and without separate normalization in the router. (d) CLIP performance with and without separate normalization in the router. Figure 9 Ablation study results on FID and CLIP across the routers operation space and architectural design. (a)(b) indicate that using hidden states as the routers operation space outperforms using key/value features, while (c)(d) show that applying modality-specific normalization to the routers inputs further improves performance. (a) FID performance by using ϵ-greedy. (b) CLIP performance by using ϵ-greedy. (c) FID performance across different top-k settings. (d) CLIP performance across different top-k settings. Figure 10 Ablation study on the ϵ-greedy exploration strategy and sparsity settings. Results show that incorporating ϵ-greedy accelerates convergence, and that = 2 yields the best performance. to all modalities, and (b) separate RMSNorms for each modality. Fig. 9(cd) shows that configuration (b) consistently outperforms (a) across all evaluation metrics. B.3 ϵ-greedy Strategy and Sparsity Design To evaluate the impact of applying ϵ-greedy to the routers output, we conduct an ablation study. Here, ϵ = 0.05, meaning that with 5% probability the router randomly selects layer rather than following the predicted logits. This choice is informed by an empirically grid search. As shown in Fig. 10, the results indicate that incorporating ϵ-greedy notably accelerates convergence across training steps. Next, we study how many layers (k) should be consolidated to form the final guidance feature. As shown in Fig. 10, = 2 consistently outperforms other candidates. This is reasonable: when = 1, the model may become trapped in local view, as the router tends to overfit to single layer; conversely, larger values dilute representations, over-flatten hidden states, and ultimately degrade performance. B.4 Scalability of MoS Here, we validate the scalability of our model. Prior studies (Cai et al., 2025; Esser et al., 2024a; Polyak et al., 2024b; Wu et al., 2025a) have demonstrated the effectiveness of scaling the generation tower (diffusion model). Since our approach does not alter the fundamental formulation of the diffusion process, it should likewise benefit from enlarging the generation tower. In this section, however, we focus on complementary directionscaling the understanding tower. Unlike MoT, MoS provides more flexible framework that enables independent scaling of the understanding tower, thereby allowing the use of larger understanding 21 (a) FID performance by using different Und. Tower. (b) CLIP performance by using different Und. Tower. (c) FID performance across different interaction setting. (d) CLIP performance across different interaction setting. Figure 11 Ablation study results on FID and CLIP for understanding tower size and interaction types. Our ablations show that MoS interaction consistently outperforms hand-crafted design, while also benefiting from scaling up the understanding tower. (a) G-SC (Semantic Consistency) Performance with varying context. (b) G-PQ (Perceptual Quality) Performance with varying context. (c) G-O (Overall Score) Performance with varying context. Figure 12 Ablation study results on GEdit-Bench (Liu et al., 2025d). The best performance is obtained when the source images are provided to both the generation and editing towers. models. As shown in Fig. 11 (a)-(b), we find that enlarging the text encoder yields consistent and stable improvements as its size increases. Moreover, since the understanding tower does not need to be updated across all training stages and its embeddings can be provided in ProducerConsume manner, scaling the understanding tower emerges as cost-efficient solution based on our empirical analysis. B.5 MoS for Image Editing Our design feeds the reference image into both the understanding and generation towers. We hypothesize that this allows the model to leverage semantic information from the understanding tower and low-level visual features from the generation tower, thereby enabling more precise and consistent editing. To validate the effectiveness of our design, we compare three input configurations for image editing: (i) w/o generation-tower context, where the reference image input to the generation tower is removed; (ii) w/o understanding-tower context, where the reference image input to the understanding tower is removed; and (iii) w/ full context, where both towers receive the source images. As shown in Fig. 12, we conduct experiments on GEdit-Bench (Liu et al., 2025d), which evaluates model editing performance across three dimensions: semantic consistency (G-SC), perceptual quality (G-PQ), and overall score (G-O). All metrics are obtained from GPT-4obased automatic evaluations (Hurst et al., 2024), where higher values indicate better performance. The results demonstrate that incorporating reference images from both towers achieves the highest average score, consistent with our hypothesis. B.6 Ablation Study on Inference Strategy MoS incorporates denoising steps into its input, which may influence the underlying diffusion dynamics. We thereby empirically validate whether common inference enhancements can benefit MoS-based models. As shown in Fig. 13, we evaluate an intermediate checkpoint of MoS-S (trained for 400k steps at 512 512 22 (a) GenEval Score using different inference steps (b) GenEval Score using different CFG scales. (c) GenEval Score using or not using rescale strategy. (d) GenEval Score using different schedulers. Figure 13 Ablation study results on GenEval (Ghosh et al., 2023) with different inference strategies. MoS exhibits behavior similarly to other diffusion models. Incorporating commonly adopted enhancements into the inference process consistently improves performance. resolution) on GenEval (Ghosh et al., 2023). The results show that increasing inference steps consistently improves generation quality, and CFG guidance can be applied in the typical range (5.07.5), similar to the other models Podell et al. (2023). We further observe that adopting linearquadratic scheduler (Polyak et al., 2024a) and rescaling strategy (Lin et al., 2024) yields slight additional gains. B.7 MoS Router Visualization Analysis To analyze the routers behavior, we visualize its output patterns in Fig. 14 using the caption dog holding sign that says MoS in 2025 with MoS-S: i) The first row shows the denoising trajectory, where the model progressively refines the image from pure noise to the target output, guided by the input caption. ii) The second row visualizes the average contribution of each understanding layer. To obtain this, we compute the routers logit matrixmodeling the affinity between blocks in the understanding and generation towersand average the weights across all generation blocks and tokens. iii) The third row presents the routers output at fixed denoising step (t=1) for individual tokens, revealing that different tokens induce distinct routing patterns. The results indicate that: The routers predictions vary across denoising steps. In the early stages, features from layers of different depths are sparsely selected as the most influential. As the denoising process progresses, the weights of the middle layers gradually increase, leading to smoother importance distributions and reduced variation across steps. This trend is intuitive: at later stages, most semantic information has been established, and the model no longer requires highly specific features from individual layers. This observation is consistent with the findings reported in Liu et al. (2025b). The routers predictions also vary across tokens. As shown in Fig. 14, each token exhibits distinct connection pattern, reflecting the routers ability to adapt its routing strategy to token-specific semantics. This observation aligns with our ablation study, which demonstrates that token-wise prediction yields better performance than sample-wise prediction. Since the router is jointly optimized with the generation tower, it can be regarded as surrogate mechanism that approximates an optimal routing strategy. However, our analysis provides no evidence that the final-layer embedding serves as an effective solution. Similarly, we find no consistent pattern indicating strict layer-to-layer correspondence in MoT. These findings suggest that previous designs may not fully leverage the capacity of the understanding tower, thereby supporting our hypothesis and underlying motivation. 23 Figure 14 Visualization of the Router across Time Steps. The results show that different tokens induce distinct connection patterns, indicating that the router dynamically adjusts its layer-to-layer routing based on token-specific semantics. Table 6 Performance of Foundational Image Generation Models on the GenEval Benchmark (Ghosh et al., 2023). GenEval evaluates object-level prompt alignment in text-to-image models. Greyed rows denote models with unclear configuration references, which hinders fair comparison. MoS-L attains the strongest results across several dimensions and delivers the best overall performance. Model #Param Single Obj. Two Obj. Counting Colors Position Color Attri. Overall GPT Image 1 [High] (OpenAI, 2025b) Seedream 3.0 (Gao et al., 2025) Qwen-Image (Wu et al., 2025a) Emu3-Gen (Wang et al., 2024) SD3 Medium (Esser et al., 2024a) FLUX.1 [Dev] (Labs, 2024) SD3.5 Large (Esser et al., 2024a) Lumina-Image 2.0 (Qin et al., 2025) Show-O2 (Xie et al., 2025c) Janus-Pro (Chen et al., 2025c) SANA-1.5 (Xie et al., 2025a) HiDream-I1-Full (Cai et al., 2025) TAR (Han et al., 2025) Bagel (Deng et al., 2025) Mogao (Liao et al., 2025) MoS-S MoS-L - - 20B 8B 2B 12B 8.1B 2.6B 7B 7B 4.8B 17B 7B 14B 7B 3B 5B 0.99 0.99 0.99 0.98 0.98 0.98 0.98 - 1.00 0.99 0.99 1.00 0.99 0.98 1. 1.00 1.00 0.85 0.91 0.89 0.34 0.63 0.74 0.73 0.67 0.58 0.59 0.86 0.79 0.83 0.84 0.83 0.83 0. 0.92 0.93 0.88 0.81 0.67 0.79 0.83 - 0.92 0.90 0.84 0.91 0.85 0.95 0.93 0.89 0.91 0.75 0.47 0. 0.17 0.34 0.22 0.34 - 0.52 0.79 0.59 0.60 0.80 0.78 0.84 0.86 0.88 0.61 0.80 0.77 0.21 0.36 0.45 0.47 0.62 0.62 0.66 0.65 0.72 0.65 0.77 0.80 0.81 0. 0.84 0.84 0.87 0.54 0.62 0.66 0.71 0.73 0.76 0.80 0.81 0.83 0.84 0.88 0.89 0.89 0.90 0.92 0.96 0.92 0.71 0.74 0.81 0.89 0.87 0.87 0.89 0.93 0.98 0.92 0.95 0. 0.95 0.97 24 Table 7 Performance of Foundational Image Generation Models on the DPG Benchmark (Hu et al., 2024). DPG-Bench evaluates long-prompt alignment in text-to-image models. MoS-Image-L delivers state-of-the-art results across multiple dimensions, ranking just below Lumina. Model #Param DALL-E 3 (OpenAI, 2025a) GPT Image 1 [High] (OpenAI, 2025b) Seedream 3.0 (Gao et al., 2025) Qwen-Image (Wu et al., 2025a) SD v1.5 (Rombach et al., 2022) SDXL (Podell et al., 2023) Playground v2.5 (Li et al., 2024a) Hunyuan-DiT (Li et al., 2024b) PixArt-Σ (Chen et al., 2024a) BLIP-3o (Chen et al., 2025b) Emu3-Gen (Wang et al., 2024) FLUX.1 [Dev] (Labs, 2024) SD3 Medium (Esser et al., 2024a) Janus-Pro (Chen et al., 2025c) TAR (Han et al., 2025) Mogao (Liao et al., 2025) HiDream-I1-Full (Cai et al., 2025) Show-o2-7B Xie et al. (2025c) Lumina-Image 2.0 (Qin et al., 2025) MoS-Image-S MoS-Image-L - - - 20B 0.86B 6.6B 6.6B 1.5B 0.6B 8B 8B 12B 2B 7B 7B 7B 17B 7B 2.6B 3B 5B Global 90.97 88.89 94.31 91.32 Entity 89.61 88.94 92.65 91.56 Attribute 88.39 89.84 91.36 92. Relation 90.58 92.63 92.78 94.31 Other 89.83 90.96 88.24 92.73 Overall 83.50 85.15 88.27 88.32 74.63 83.27 83.06 84.59 86.89 - 85.21 74.35 87.90 86.90 83.98 82.37 76.44 89.00 - 89.29 91.74 74.23 82.43 82.59 80.59 82.89 - 86.68 90.00 91.01 88.90 88.62 90.03 90.22 91.78 91.97 92.17 90.59 75.39 80.91 81.20 88.01 88.94 - 86.84 88.96 88.83 89.40 88.05 88.26 89.48 89.96 90.20 92.09 91. 73.49 86.76 84.08 74.36 86.59 - 90.22 90.87 80.70 89.32 93.98 93.18 93.74 91.81 94.85 89.38 93.30 67.81 80.41 83.50 86.41 87.68 - 83.15 88.33 88.68 89.48 84.86 85.40 91.83 91.64 - 90.18 91.69 63.18 74.65 75.47 78.87 80.54 81.60 80.60 83.84 84.08 84.19 84.19 84.33 85.89 86.14 87. 86.33 87.01 Table 8 Performance on world knowledge reasoning with WISE (Niu et al., 2025). WISE evaluates complex semantic understanding and world knowledge in text-to-image generation. Model #Param Cultural GPT Image 1 [High] (OpenAI, 2025b) Qwen-Image (Wu et al., 2025a) VILA-U (Wu et al., 2024b) SDv1.5 (Rombach et al., 2022) Janus-Pro (Chen et al., 2025c) Emu3-Gen (Wang et al., 2024) SDXL (Podell et al., 2023) SD3.5 Large (Esser et al., 2024a) PixArt-Alpha (Chen et al., 2024a) Playground v2.5 (Li et al., 2024a) FLUX.1 [Dev] (Labs, 2024) BAGEL (Deng et al., 2025) UniWorld-V1 (Lin et al., 2025) MoS-Image-S MoS-Image-L - 20B 7B 0.86B 7B 8B 6.6B 8.1B 0.6B 6.6B 12B 14B 12B 3B 5B 0. 0.62 0.26 0.34 0.30 0.34 0.43 0.44 0.45 0.49 0.48 0.44 0.53 0.40 0.47 Chemistry Overall 0.74 0.40 0.23 0.21 0.26 0.27 0.27 0.31 0.34 0.33 0.35 0.39 0. 0.37 0.44 0.80 0.62 0.31 0.32 0.35 0.39 0.43 0.46 0.47 0.49 0.50 0.52 0.55 0.47 0. Time 0.71 0.63 Space 0.89 0.77 Biology 0.83 Physics 0. 0.57 0.75 0.37 0.32 0.49 0.48 0.47 0.58 0.48 0.55 0.62 0.68 0.73 0.65 0.74 0.35 0.28 0.36 0.41 0.44 0.44 0.49 0.43 0.42 0.44 0.45 0.43 0. 0.39 0.29 0.42 0.45 0.45 0.52 0.56 0.48 0.51 0.60 0.59 0.63 0.64 0.33 0.35 0.37 0.45 0.48 0.50 0.50 0.58 0.58 0.55 0.55 0.50 0.56 25 Table 9 Quantitative results on OneIG (Chang et al., 2025). The overall score is averaged across five dimensions. With only 5B parameters, our model matches Imagen4 and trails recent commercial models by small margin. Model # Param Imagen3 (Baldridge et al., 2024) Kolors 2.0 (Team, 2025) Recraft V3 (Team, 2024b) Imagen4 (Google, 2025) Seedream 3.0 (Gao et al., 2025) GPT Image 1 [High] (OpenAI, 2025b) Qwen-Image (Wu et al., 2025a) Janus-Pro (Chen et al., 2025c) BLIP3-o (Chen et al., 2025b) BAGEL (Deng et al., 2025) Show-o2 (Xie et al., 2025c) SDv1.5 (Rombach et al., 2022) SDXL (Podell et al., 2023) SANA-1.5(Xie et al., 2025a) Lumina-Image 2.0 (Qin et al., 2025) SD3.5 Large (Esser et al., 2024a) FLUX.1 [Dev] (Labs, 2024) CogView4 (AI and Team, 2025) OmniGen2 (Wu et al., 2025b) HiDream-I1-Full (Cai et al., 2025) MoS-Image-S MoS-Image-L - - - - - - 20B 7B 8B 14B 7B 0.86B 6.6B 4.8B 2.6B 8.1B 12B 6B 4B 17B 3B 5B Alignment 0.84 0.82 0.81 0.86 0.82 0.85 0.88 0.55 0.71 0.77 0.82 0.57 0.69 0.77 0.82 0.81 0.79 0.79 0.80 0. 0.82 0.85 Text 0.34 0.43 0.80 0.81 0.87 0.86 0.89 0.00 0.01 0.24 0.00 0.01 0.03 0.07 0.11 0.63 0.52 0.64 0.68 0.71 0.82 0.87 Reasoning 0.31 0.26 0.32 0.34 0. 0.35 0.31 0.14 0.22 0.17 0.23 0.21 0.24 0.22 0.27 0.29 0.25 0.25 0.27 0.32 0.26 0.26 Style 0.36 0.36 0.38 0.38 0.41 0.46 0.42 0.28 0.36 0.37 0.32 0.38 0.33 0.40 0.35 0.35 0.37 0.35 0.38 0. 0.38 0.41 Diversity 0.19 0.30 0.21 0.20 0.28 0.15 0.20 Overall 0.41 0.43 0.50 0.52 0.53 0.53 0.54 0.37 0.23 0.25 0.18 0.43 0.30 0.22 0.22 0.23 0.24 0.21 0.24 0. 0.20 0.19 0.27 0.31 0.36 0.31 0.32 0.32 0.33 0.35 0.46 0.43 0.45 0.48 0.48 0.50 0.52 Table 10 Performance of Foundational Image Editing Models on ImgEdit Benchmark (Ye et al., 2025). Greyed rows indicate models lacking clear configuration references, which prevents fair comparison. Model FLUX.1 Kontext [Pro](Batifol et al., 2025) GPT Image 1 [High] (OpenAI, 2025b) Qwen-Image (Wu et al., 2025a) #Param Add Adjust Extract Replace Remove Back. Style Hybrid Action Overall 3.57 3.66 4.14 4.26 4.57 4.57 4.93 4.81 4.38 4.00 4.20 4.27 3.68 3.96 3.82 4.15 4.33 4.16 4.25 4.61 4. 4.56 4.35 4.66 2.35 2.90 4.89 4.69 4.63 - - 20B 3.43 MagicBrush (Zhang et al., 2023) Instruct-Pix2Pix (Brooks et al., 2023) AnyEdit (Yu et al., 2025) UltraEdit (Zhao et al., 2024) OmniGen (Xiao et al., 2025) ICEdit (Zhang et al., 2025) Step1X-Edit (Liu et al., 2025d) BAGEL (Deng et al., 2025) UniWorld-V1 (Lin et al., 2025) OmniGen2 (Wu et al., 2025b) 0.86B 2.84 0.86B 2.45 0.86B 3.18 3.44 2.5B 3.47 3.8B 3.58 0.2B 3.88 12B 3.56 14B 3.82 12B 3.57 4B MoS-Image-S MoS-Image-L 3B 5B 4. 4.63 4.47 1.58 1.83 2.95 2.81 3.04 3.39 3.14 3.31 3.64 3.06 4.02 1.51 1.44 1.88 2.13 1.71 1.73 1.76 1.70 2.27 1.77 2.39 2. 1.97 2.01 2.47 2.96 2.94 3.15 3.40 3.30 3.47 3.74 4.80 4.85 1.58 1.50 2.23 1.45 2.43 2.93 2.41 2.62 3.24 3.20 4.60 4. 1.75 1.44 2.24 2.83 3.21 3.08 3.16 3.24 2.99 3.57 4.52 4.85 2.38 3.55 2.85 3.76 4.19 3.84 4.63 4.49 4.21 4.81 4.68 4. 1.62 1.20 1.56 1.91 2.24 2.04 2.64 2.38 2.96 2.52 3.80 4.16 1.22 1.46 2.65 2.98 3.38 3.68 2.52 4.17 2.74 4.68 4.31 4. 1.90 1.88 2.45 2.70 2.96 3.05 3.06 3.20 3.26 3.44 4.17 4.33 26 Table 11 Performance of Foundational Image Editing Models on GEdit Benchmark (Liu et al., 2025d). Greyed rows indicate models lacking clear configuration references, which prevents fair comparison. Model #param Gemini 2.0 (DeepMind, 2025) FLUX.1 Kontext [Pro](Batifol et al., 2025) GPT Image 1 [High] (OpenAI, 2025b) Qwen-Image (Wu et al., 2025a) Instruct-Pix2Pix (Brooks et al., 2023) AnyEdit (Yu et al., 2025) MagicBrush (Zhang et al., 2023) UniWorld-V1 (Lin et al., 2025) OmniGen (Xiao et al., 2025) OmniGen2 (Wu et al., 2025b) BAGEL (Deng et al., 2025) Step1X-Edit (Liu et al., 2025d) MoS-Image-S MoS-Image-L - - 20B 0.86B 0.86B 0.86B 12B 3.8B 4B 14B 12B 3B 5B G-Semantic Consistency 6.73 7.02 7.85 8.00 3.58 3.18 4.68 4.93 5.96 7.16 7.36 7.66 8.00 8. G-Perceptual Quality 6.61 7.60 7.62 7.86 5.49 5.82 5.66 7.43 5.89 6.77 6.83 7.35 7.34 7.64 G.-Overall 6.32 6.56 7.53 7.56 3.68 3.21 4.52 4.85 5.06 6.41 6.52 6. 7.41 7.86 Algorithm 1 Training procedure of MoS Require: Paired training data (z0, c); understanding tower m, n; top-kϵ selection function. 1: 1. Encode input. 2: Extract hidden states from the understanding tower: ; generation tower ; router ; number of layers 3: 2. Sample diffusion step. 4: Randomly sample timestep 5: 3. Predict routing weights. 6: Compute router logits: (c) = i {S [1, m] } Uniform(1, T) and obtain noisy latent zt . where Rm generation layer. 7: Normalize logits: and wij = (c, t, zt) R denotes the routing weight from the i-th understanding layer to the j-th ) where the softmax operation is applied to each column w1:m,j = softmax( 8: 4. Construct conditional signal for each generation block. 9: for = 1 to do 10: Select indices of top-k elements under ϵ-greedy rule: of . 11: Compute the conditional context: Ij = top-kϵ(w1:m,j) = Sc Ij wij . 12: Fuse with the generation tower features: Hj = Concat(cid:0)Proj(Sc ), Sz (cid:1) Perform Generation Tower Block-j on Hj 13: 14: end for 15: 5. Compute loss. 16: Apply the diffusion objective (e.g., ℓ2 to update Sz to Sz j+1 . loss) between predicted and ground-truth . 27 Algorithm 2 Inference procedure of MoS Require: Conditioning input c; understanding tower number of top connections k; number of layers m, n. Ensure: Generated sample ˆz0 ; generation tower ; router ; number of steps T; 1: 1. Encode conditioning signal. 2: Obtain hidden states from the understanding tower: 3: 2. Initialize latent. 4: Sample initial noise ˆz1 5: 3. Iterative denoising. 6: for = 1 down to 0 do 7: Compute router logits: (0, I). 8: Normalize logits: (c) = i {S [1, m] . } = (c, t, ˆzt) = softmax( ) 9: 10: 11: for = 1 to do Select top-k indices under ϵ-greedy rule: Compute the conditional context: Ij = top-kϵ(w1:m,j) = Sc Ij wij . 12: Fuse with the generation tower features: Hj = Concat(cid:0)Proj(Sc ), Sz (cid:1) 13: Perform Generation Tower Block-j on Hj to update Sz to Sz j+ . end for Predict ˆzt 14: 15: 16: end for 17: 4. Decode. 18: Obtain final output ˆz0 1 with ˆvt = (zt, , (c)) following the diffusion sampling rule. via the decoder of . 28 Figure 15 Visualization of MoS-L on text-to-image generation. The samples are produced under dynamic resolution setting, with the maximum side length capped at 2048 pixels. Figure 16 Visualization of MoS-L/S and baseline methods on text-to-image generation. All models are evaluated with their default parameters in Diffusers. We present results on challenging cases. These include scenarios such as arranging foods with distinct categories, colors, and patterns; posters combining natural objects with visual text; and purely textual prompts, such as generating menu. MoS-L demonstrates competitive performance in these demanding settings. Zoomed-in view for better clarity. 30 Figure 17 Visualization of MoS-L/S and baseline methods on instruction-based image editing. All models are evaluated using their default parameters in Diffusers. We showcase results on hybrid instructions and the cases involving visual text editing. Zoomed-in for better clarity."
        }
    ],
    "affiliations": [
        "KAUST",
        "Meta AI"
    ]
}