{
    "paper_title": "Light Forcing: Accelerating Autoregressive Video Diffusion via Sparse Attention",
    "authors": [
        "Chengtao Lv",
        "Yumeng Shi",
        "Yushi Huang",
        "Ruihao Gong",
        "Shen Ren",
        "Wenya Wang"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Advanced autoregressive (AR) video generation models have improved visual fidelity and interactivity, but the quadratic complexity of attention remains a primary bottleneck for efficient deployment. While existing sparse attention solutions have shown promise on bidirectional models, we identify that applying these solutions to AR models leads to considerable performance degradation for two reasons: isolated consideration of chunk generation and insufficient utilization of past informative context. Motivated by these observations, we propose \\textsc{Light Forcing}, the \\textit{first} sparse attention solution tailored for AR video generation models. It incorporates a \\textit{Chunk-Aware Growth} mechanism to quantitatively estimate the contribution of each chunk, which determines their sparsity allocation. This progressive sparsity increase strategy enables the current chunk to inherit prior knowledge in earlier chunks during generation. Additionally, we introduce a \\textit{Hierarchical Sparse Attention} to capture informative historical and local context in a coarse-to-fine manner. Such two-level mask selection strategy (\\ie, frame and block level) can adaptively handle diverse attention patterns. Extensive experiments demonstrate that our method outperforms existing sparse attention in quality (\\eg, 84.5 on VBench) and efficiency (\\eg, $1.2{\\sim}1.3\\times$ end-to-end speedup). Combined with FP8 quantization and LightVAE, \\textsc{Light Forcing} further achieves a $2.3\\times$ speedup and 19.7\\,FPS on an RTX~5090 GPU. Code will be released at \\href{https://github.com/chengtao-lv/LightForcing}{https://github.com/chengtao-lv/LightForcing}."
        },
        {
            "title": "Start",
            "content": "LIGHT FORCING: Accelerating Autoregressive Video Diffusion via Sparse Attention Chengtao Lv 1 2 Yumeng Shi 1 Yushi Huang 3 Ruihao Gong 4 5 Shen Ren 6 Wenya Wang 1 6 2 0 2 4 ] . [ 1 9 8 7 4 0 . 2 0 6 2 : r a"
        },
        {
            "title": "Abstract",
            "content": "Advanced autoregressive (AR) video generation models have improved visual fidelity and interactivity, but the quadratic complexity of attention remains primary bottleneck for efficient deployment. While existing sparse attention solutions have shown promise on bidirectional models, we identify that applying these solutions to AR models leads to considerable performance degradation for two reasons: isolated consideration of chunk generation and insufficient utilization of past informative context. Motivated by these observations, we propose LIGHT FORCING, the first sparse attention solution tailored for AR video generation models. It incorporates Chunk-Aware Growth mechanism to quantitatively estimate the contribution of each chunk, which determines their sparsity allocation. This progressive sparsity increase strategy enables the current chunk to inherit prior knowledge in earlier chunks during generation. Additionally, we introduce Hierarchical Sparse Attention to capture informative historical and local context in coarse-to-fine manner. Such two-level mask selection strategy (i.e., frame and block level) can adaptively handle diverse attention patterns. Extensive experiments demonstrate that our method outperforms existing sparse attention in quality (e.g., 84.5 on VBench) and efficiency (e.g., 1.21.3 end-to-end speedup). Combined with FP8 quantization and LightVAE, LIGHT FORCING further achieves 2.3 speedup and 19.7 FPS on an RTX 5090 GPU. Code will be released at https://github.com/chengtao-lv/LightForcing. 1Nanyang Technological University 2AUMOVIO-NTU Corporate Lab 3Hong Kong University of Science and 5Sensetime Research 4Beihang University Technology 6AUMOVIO Singapore Pte Ltd. to: Correspondence Wenya Wang <wangwy@ntu.edu.sg>, Ruihao Gong <gongruihao@sensetime.com>. Preprint. February 5, 2026. 1 Figure 1. Runtime comparison of attention versus other components across chunk indices for Self Forcing (Huang et al., 2025a) 1.3B on RTX 5090. When the chunk index reaches 14, attention accounts for approximately 75% of the total latency. 1. Introduction Recent notable advancements in video generation (Wan et al., 2025; Sun et al., 2024) have revolutionized artificial intelligence-generated content (AIGC). This progress can largely be attributed to the emergence of diffusion transformers (DiT) (Peebles & Xie, 2023), which leverage bidirectional attention to denoise all frames simultaneously. While video diffusion models (VDMs) can generate temporally consistent and long-duration videos, they struggle with temporal scalability, interactivity, and real-time deployment. In contrast, autoregressive (AR) video generation models naturally emerge as more promising alternative, better suited to tackle these constraints. Moreover, recent AR models (Yin et al., 2025; Cui et al., 2025) replace lossy vector quantization techniques (Van Den Oord et al., 2017) with chunk-by-chunk generation paradigm, yielding improved visual fidelity and interactivity. This also enables real-time applications in diverse downstream tasks, such as game simulation (Decart et al., 2024; Bruce et al., 2024; Parker-Holder et al., 2024) and robot learning (Yang et al., 2023; Li et al., 2025a). Similar to bidirectional VDMs, the quadratic computational complexity of spatiotemporal 3D full attention in AR models still remains major bottleneck for efficient deployment. As illustrated in Fig. 1, when generating 480p video using Self-Forcing (Huang et al., 2025a) 1.3B, the attention LIGHT FORCING: Accelerating Autoregressive Video Diffusion via Sparse Attention consumes nearly three times the runtime of all other components combined (i.e., linear layers, RoPE, etc.) at the last chunk. To mitigate the computational costs, one simple solution is to adopt various sparse attention methods introduced for bidirectional models (Zhang et al., 2025d; Xi et al., 2025; Yang et al., 2025b; Zhang et al., 2025b;a;c; Li et al., 2025c). These approaches mainly identify critical blocks utilizing either static (Zhang et al., 2025d; Li et al., 2025c) or dynamic (Wu et al., 2025; Zhang et al., 2025c;a) sparse patterns in advance, and thus compute attention scores only for small subset of tokens. However, directly applying these sparse attention solutions to autoregressive (AR) models leads to significantly degraded generation quality compared to dense attention. We conduct in-depth investigations and observe that this performance drop arises from two primary aspects: ➀ sparse attention exacerbates the accumulation errors in AR models (e.g., over-saturation in later chunks), while prior works largely ignore the heterogeneous contributions of different chunks to the global error accumulation. Our key insight is that, during denoising, the current chunk is essentially predicting the next noise level conditioned on past clean chunks. Therefore, later chunks are naturally prone to inheriting the quality of the past chunks. ➁ Another insight is the insufficient utilization of past key context. For each query block, the critical historical information varies significantly across model layers, attention heads, and denoising timesteps. However, existing methods (e.g., sliding window attention (Beltagy et al., 2020) or adding chunk sinks (Yang et al., 2025a; Liu et al., 2025b)), inevitably discard part of this information, thereby harming long-range consistency and the richness of motion in generated videos. Motivated by these findings, we propose LIGHT FORCING, an efficient variant specifically designed towards any autoregressive video generation models harnessing sparse attention. Specifically, ➀ we introduce Chunk-Aware Growth (CAG) mechanism to quantitatively estimate the contributions of each chunk. Unlike chunk-agnostic policies that treat chunk generation in isolation, we view the generation of the current chunk as further few-step denoising process conditioned on the previous clean chunk. From theoretical perspective, we formulate the final sparsity allocation for each chunk as determined by its global accumulation error, which depends on two components (i.e., the corresponding denoising steps and the score estimation). In other words, our method allocates lower sparsity priorities to earlier chunks, and progressively increases the sparsity in later chunks as they can inherit the structured knowledge stored in earlier chunks. ➁ We propose Hierarchical Sparse Attention (HSA), which preserves both global and local perception ability under fixed computational budget. Specifically, HSA adopts coarse-to-fine pipeline that selects sparse masks at both the frame and block levels for each query block, enabling flexible and versatile attention modeling. This two-level strategy efficiently captures informative historical context while maintaining fast execution, thereby achieving an effective trade-off between model performance and computational cost. We conduct extensive experiments to evaluate the effectiveness of our LIGHT FORCING. We compare our method with state-of-the-art sparse attention approaches on both the Self Forcing (Huang et al., 2025a) and LongLive (Yang et al., 2025a) models, and report results on the VBench (Huang et al., 2024b) benchmark. The results show that our method consistently outperforms existing approaches in both generation quality and latency, and even surpasses dense attention in several metrics. For example, on Self Forcing (Huang et al., 2025a), our method achieves total score of 84.5 while providing 1.3 end-to-end and 3.3 attention speedup. Furthermore, when combined with FP8 quantization and LightVAE (Contributors, 2025), LIGHT FORCING reaches 19.7 FPS, enabling real-time video generation on consumer-grade GPU (RTX 5090) for the first time. To summarize, our main contributions are threefold: To the best of our knowledge, LIGHT FORCING is the first sparse attention solution specifically designed for autoregressive video generation models. We present Chunk-Aware Growth (CAG). We allocate higher attention budgets to earlier chunks and progressively decay for later chunks, effectively reducing error propagation while preserving efficiency. We propose Hierarchical Sparse Attention (HSA), which captures global and local dependencies via coarse-to-fine frame and block selection. Extensive experiments demonstrate the superior performance (e.g., 84.5 on VBench) and real-time generation (e.g., 19.7 FPS) of LIGHT FORCING. 2. Related Work 2.1. Autoregressive Video Diffusion Compared with bidirectional video diffusion models (Wan et al., 2025; Yang et al., 2024; Brooks et al., 2024; Sun et al., 2024) that denoise all frames jointly, autoregressive video generation models (Zhang & Agrawala, 2025; Huang et al., 2025a; Gu et al., 2025; Kodaira et al., 2025; Henschel et al., 2025) generate the next token or frame sequentially, and are thus inherently more suitable for real-time streaming applications. Early approaches (Hu et al., 2024; Gao et al., 2024) adopted Teacher Forcing (TF), where training is conditioned on ground-truth tokens, but they suffer from reduced visual fidelity when generating long videos. Conversely, Diffusion Forcing (Chen et al., 2024) is trained with conditioning at arbitrary noise levels and has been adopted in models such as SkyReels-V2 (Chen et al., 2025a) and Magi-1 (Teng et al., 2 LIGHT FORCING: Accelerating Autoregressive Video Diffusion via Sparse Attention 2025). CausVid (Yin et al., 2025) employs block causal attention, distilling bidirectional teacher to few-step causal student via distribution matching distillation (Yin et al., 2024). More recently, Self Forcing (Huang et al., 2025a) introduced novel post-training paradigm that mitigates error accumulation arising from train-test misalignment. Subsequent works, including Rolling Forcing (Liu et al., 2025b), LongLive (Yang et al., 2025a), Self Forcing++ (Cui et al., 2025), and Reward Forcing (Lu et al., 2025), further address the limitation on the achievable generation length, object/scene dynamics or color drifts. Nevertheless, although autoregressive models with only few denoising steps (e.g., 4 steps) have substantially reduced latency, real-time generation on resource-constrained devices still remains challenging. 2.2. Sparse Attention large body of work (Wu et al., 2025; Xu et al., 2025; Xi et al., 2025; Yang et al., 2025b; Zhang et al., 2025b) has explored how to alleviate the runtime bottleneck caused by quadratic-complexity attention in bidirectional video diffusion models, covering low-bit attention (Zhang et al., 2024b;a) and linear attention (Xie et al., 2024; Chen et al., 2025b; Huang et al., 2025b). Another promising line of work focuses on sparse attention, where approaches can be roughly categorized by whether they follow static or dynamic patterns to identify critical tokens with block-wise granularity. Static schemes (Zhang et al., 2025d; Li et al., 2025c; Hassani et al., 2025) usually prescribe sparsity masks via handcrafted patterns, such as neighborhood (Hassani et al., 2025; Zhang et al., 2025d) or spatiotemporal structures (Xi et al., 2025). In contrast, dynamic solutions (Zhang et al., 2025b;a; Wu et al., 2025) additionally introduce an online identification stage. These methods either utilize 1D (Zhang et al., 2025b; Wu et al., 2025; Zhang et al., 2025a) or 3D (Zhang et al., 2025c; Wu et al., 2025) mean pooling to aggregate blocks, and estimate their importance subsequently. Clustering-based strategies (Yang et al., 2025b) instead group semantically similar tokens together. In addition, several emerging hybrid attention mechanisms have been applied to video generation, including mixtures across different attention types (e.g., combining linear attention and softmax attention (Zhang et al., 2025a)) and across different sparsity levels (e.g., gating of twin-level (Zhang et al., 2025c) or pyramid-level sparse representation (Li et al., 2025b; Zhou et al., 2025)). However, the exploration of sparse attention for autoregressive video generation remains largely uncharted. 3. Preliminaries Autoregressive video diffusion modeling. Autoregressive (AR) video diffusion models decompose video synthesis into inter-chunk autoregression and intra-chunk diffusion, combining the chain-rule factorization for temporal dependency modeling with the expressive denoising capability of diffusion models for high-fidelity frame generation. Specifically, given condition c, the joint distribution of an -frame video sequence x1:N is expressed as pθ(x1:N c) = (cid:89) i=1 pθ(xi x<i, c). (1) This formulation generates frames sequentially, where each conditional term pθ(xi x<i, c) is approximated by fewstep diffusion generator conditioned on KV cache (i.e., previous clean frames) (Huang et al., 2025a; Cui et al., 2025; Yang et al., 2025a). Specifically, the conditional term pθ(xi x<i, c) can be defined as fθ,t1 fθ,t2 (cid:0)xi (0, I) and each transition is fθ,tT tT given by (cid:1), where xi tT fθ,tj (cid:16) xi tj (cid:17) (cid:16) = Ψ Gθ(xi tj , tj, x<i, c), ϵtj1 , tj1 (cid:17) , (2) , tj, x<i, c) corresponds to the denoised eswhere Gθ(xi tj timate ˆx 0, i.e., prediction of the clean chunk from the current noisy state xi tj under the autoregressive context x<i and condition c. The operator Ψ() denotes the forward corruption (re-noising) mapping that injects Gaussian noise at lower noise level to produce the next state xi tj1 for subsequent denoising. Advanced few-step AR video diffusion models often adopt the probability flow ODE formulation to define the forward noising trajectory and inject Gaussian noise (Song et al., 2023), i.e., (1 σtj1) ˆx 0 + σtj1 ϵtj1 , where ϵtj1 (0, I) and σtj1 controls the noise level. Blockwise sparse attention. Practical sparse-attention systems enforce sparsity at the block (tile) granularity to better match modern accelerator hardware, enabling high utilization and efficient memory access patterns in GPU kernels such as FlashAttention (Dao, 2023). Concretely, given q, k, Rnd, we partition1 the sequence dimension into blocks and form = (cid:2)q1; . . . ; qnq (cid:3), = (cid:2)k1; . . . ; knk (cid:3), = (cid:2)v1; . . . ; vnk (cid:3), where qi Rbqd and kj, vj Rbkvd, with nq = n/bq and nk = n/bkv. We further define block mask {0, 1}nqnk , where Bij = 1 indicates that the (i, j) tile is active. Block-sparse attention can then be written as SparseAttn(q, k, v; B) = softmax (cid:18) qk dk (cid:19) (B) v, (3) where (B) {0, 1}nn expands to an element-wise mask that is constant within each (bq bkv) tile, and 1For simplicity, we assume and k/v have the same shape. 3 LIGHT FORCING: Accelerating Autoregressive Video Diffusion via Sparse Attention Figure 2. Comparison of different visual generation examples (i.e., 7 chunks for 21 latent frames), where blue, red, and green boxes denote attention sparsity rates of 0%, 80%, and 90%, respectively. denotes element-wise multiplication. Importantly, efficient implementations do not materialize (B). Instead, they compute only the tile products qik and the corresponding value aggregation for indices (i, j) with Bij = 1, skipping entire tiles when Bij = 0. Consequently, the computational and memory costs scale with the number of active blocks rather than n2, while retaining GPU-friendly dense computation within each tile. 4. LIGHT FORCING 4.1. Chunk-Aware Growth Mechanism Many acceleration techniques for bidirectional video diffusion models, including feature caching (Huang et al., 2024a; Liu et al., 2025a; Ma et al., 2024) and sparse attention (Li et al., 2025c; Zhang et al., 2025d), have observed pronounced sensitivity across different timesteps and layers. However, directly applying these chunk-agnostic policies of bidirectional models to few-step autoregressive video diffusion can be problematic: they ignore the heterogeneous contribution of different chunks to the global accumulation error that compounds over autoregressive rollout, and thus can easily trigger severe quality degradation or even collapse. To build intuition, we conduct several simple toy experiments that visually illustrate how generation behavior varies across chunks (as shown in Fig. 2). First, we apply moderately sparse attention ratio (e.g., 80%) either to the first chunk (Setting 2) or to the subsequent chunks 2-7 (Setting 3). Surprisingly, we observe an interesting phenomenon: Setting 2 incurs an irreversible loss of visual quality (even over-saturation in the later chunks) that cannot be recovered even if later chunks revert to dense attention. Later frames in Setting 2 exhibit severe oversaturation and exposure-bias artifacts. In contrast, Setting 3 achieves generation quality that is nearly lossless from Setting 1 even when only the first chunk is kept in dense attention. This further implies that once satisfactory priors are established in the first (or other early) chunk(s), subsequent chunks can readily inherit and propagate these priors with little difficulty. Intuitively, earlier chunks should adopt lower sparsity, while later chunks can tolerate higher sparsity. Therefore, natural question arises: Can we quantitatively allocate the sparsity budget across chunks? Sparsity-Induced Error. To solve the problem, we further explore generation performance under radical sparsity rate (i.e., 90%) in Setting 4, and observe that as time progresses, later chunks gradually become clearer compared to the initially Gaussian-noise-like appearance, suggesting that , tj, x<i, c) performs denoising toward the next Gθ(xi tj noise level compared with xi1. Moreover, we posit that Gθ(xi , tj, x<i, c) essentially continues denoising for tj additional steps starting from the noisy level of xi1 (verified in the Appendix). These analyses reveal that sparsity effects can be reflected by the noise level in the final generated chunk xi t1 (i = 1, . . . , ). To capture this, we measure the sparsity-induced error of chunk as the variation distance TV(, ) between the clean data distribution and the noised distribution of xi t1 , denoted qt, with the noise level t: TV(qt, p) C1 d2 log3 + C1 εscore log2 t, (4) where C1, log2 t, log3 are constants2 do not affect the asymptotic complexity. From this inequality, we can interpret the first term as the finite-step sampling error, which is 1/ t, while the second term captures the effect of score estimation error, reflecting the approximation error induced by imperfect model learning. Sparsity Allocation. Intuitively, we should lower the sparsity ratio for chunks with more errors to preserve generation quality. Leveraging this insight, we propose ChunkAware Growth (CAG) strategy that considers both the finitestep sampling error (Term 1) and the score estimation error (Term 2). For chunk i, the sparsity ratio si can be written as si = sbase αiβ (5) 2Logarithmic factors of the form logk arise only as technical amplification constants in the proof and do not affect the asymptotic complexity. 4 LIGHT FORCING: Accelerating Autoregressive Video Diffusion via Sparse Attention Figure 3. Overview of LIGHT FORCING. The left subfigure illustrates our Chunk-Aware Growth (Sec. 4.1) strategy for sparsity allocation across different chunks. The right subfigure demonstrates how Hierarchical Sparse Attention (Sec. 4.2) is utilized to efficiently retrieve long-range historical context. Note that chunk corresponds to group of frames processed in single generation (e.g., 3 frames in practice). For simplicity, we visualize each chunk as single frame in the overview. where αi denotes the noise level reached by the i-th chunk t. The hyperparameter sbase is preand scales as 1/ defined constant that reflects the score estimation error. To compute the modulated sparsity factor β, we have (1 starget) (cid:88) i=1 lq lk = (cid:88) i= (cid:0)1 sbase + αiβ) lq lk d, (6) where starget denotes the target sparsity ratio, and lq and lk denote the query and key sequence lengths for chunk i, respectively. By enforcing equal FLOPs on both sides of the equation, we can solve for β and thus obtain si. 4.2. Hierarchical Sparse Attention Another key challenge of autoregressive video generation models is that the number of historical frames grows linearly over time, making attention increasingly timeconsuming and slowing down later-chunk generation. Many approaches (Liu et al., 2025b; Lu et al., 2025; Yang et al., 2025a) mitigate this by adopting sliding-window attention (Beltagy et al., 2020) that truncates past frames to fixed context length. While this alleviates the latency growth, it can induce history forgetting, leading to poor long-range consistency and repetitive motions in subsequent chunks. We believe that treating nearby frames as keyframes is suboptimal, since the historical frames that the current query is interested in vary across different layers, heads, and timesteps. As illustrated in Fig. 4, attention between historical frames exhibits complex patterns such as diagonal, attention-sink structures, which makes it difficult for sliding window scheme to cover all informative context. Inspired by these findings, we propose the Hierarchical Sparse Attention (HSA), which follows coarse-to-fine paradigm for sparse attention on autoregressive video generation models. Specifically, each query block (as detailed in Sec. 3) first retrieves set of keyframes and then performs dynamic sparse attention over the selected frames. This dual-stage strategy not only bounds attention to fixed computational complexity but also mitigates long-video consistency degradation and history-forgetting issues. Formally, the process of generating chunk can be understood as computing attention between the query q(i) and the key-value pairs {k:i, v:i}. Here, q(i) R(f n)d and {k:i, v:i} R(if n)d, where and denote the number of frames per chunk and the number of tokens per frame, respectively. To avoid computing attention between all keyvalue pairs, our HSA mainly consists of three components, i.e., Token Compression, Mask Selection, and Blockwise Sparse Attention. Token Compression. We first compress q(i) at the blockwise granularity and k:i at the blockwise and framewise granularity, which can be written as blockwise: framewise: (cid:16) q(i) = ϕ ˆk<i = ϕ q(i), bq (cid:16)k<i, n/bkv (cid:17) , (cid:17) , k:i = ϕ(cid:0)k:i, bkv (cid:1) , (7) where ϕ(x, b) denotes mean pooling operator that aggregates sequential tokens with size b. After applying this operation, the final compressed representations have shapes q(i) R(f n/bq)d, k:i R(if n/bkv)d, and ˆk<i R((i1)f )d. It is worth noting that we only compress ˆk<i for past frames, since the key-value frames within current chunk are always selected. Mask Selection. Based on the compressed representations, we perform hierarchical mask selection in coarse-tofine manner. Specifically, for each query block in chunk i, we first retrieve small set of relevant historical frames using the framewise-compressed keys, and then select critical blocks within the retrieved frames using blockwisecompressed keys. Let q(i) Rr denote the r-th blockwise query summary in the current chunk, where {1, . . . , n/bq}. We 5 LIGHT FORCING: Accelerating Autoregressive Video Diffusion via Sparse Attention Figure 4. Visualization of attention logits between query blocks at chunk 7 (i.e., frame 18-20, 24 blocks per frame) and all past key frames (i.e., frame 0-17) on Self Forcing (Huang et al., 2025a). compute frame-level relevance scores between the query block and each past frame using the framewise-compressed keys ˆk<i as = (cid:10) q(i) p(i) , ˆk<i(cid:11) R(i1)f , (8) where p(i) denotes the vector of frame-level logits, whose entries are given by the inner products between the query block summary q(i) and each framewise-compressed key in ˆk<i. We then select the most relevant past frames: In summary, our HSA maintains fixed attention complexity (independent of the total number of historical frames) while alleviating long-range consistency degradation. Meanwhile, our dual-stage mask selection incurs only negligible overhead compared to conventional dynamic sparse attention, as it merely adds frame-retrieval step (approximately 2% increase in end-to-end runtime). CAG and HSA are complementary: CAG allocates sparsity ratio for each chunk at macro level, thereby determining how much historical information each block in the current chunk can leverage in HSA (discussed in the Appendix). Tr = TopKidx (cid:17) (cid:16) p(i) (i), (9) 5. Experiments where TopKidx returns indices of the most relevant topk past frames and (i) denotes the set of frames within the current chunk i. We always include all frames from the current chunk in the attention context, ensuring full visibility over intra-chunk temporal dependencies. Given the selected frame set Tr, we further perform finegrained blockwise selection within each frame. For frame τ Tr, let k(τ ) Rd denote the j-th blockwise key summary, where {1, . . . , n/bkv}. We compute blocklevel relevance logits as (τ, j) = q(i) o(i) , k(τ ) , (10) and select the top-k blocks within each selected frame: Jr = TopKidx (cid:16) {o(i) (τ, j)}n/bkv j= (cid:17) . (11) Blockwise Sparse Attention. Based on the selected frames and blocks, we construct block-level attention mask B(i) {0, 1}NqNkv . For the r-th query block, we have B(i) (τ, j) = 1(cid:2)τ Tr, Jr(τ )(cid:3). (12) Here, Nq and Nkv denote the number of query and key blocks, respectively. The final attention for the r-th query block is computed using blockwise sparse attention: (cid:32) Attn(i) = softmax q(i) (k:i) (cid:33) (B(i) ) v:i. (13) 6 5.1. Experimental Details Implementation. We build sparse attention on top of the currently open-sourced autoregressive video generation models, Self Forcing (Huang et al., 2025a) and LongLive (Yang et al., 2025a). Following them, we use chunk size of three latent frames. For Finetunable methods (i.e., VMoBA (Wu et al., 2025), SLA (Zhang et al., 2025a) and Light Forcing), we perform extra post-training for 2,000 iterations based on their pre-trained weights. For latency evaluation, we adopt the SpargeAttention kernel (Zhang et al., 2025b) for all methods (except Sparse VideoGen2 (Yang et al., 2025b) due to its variable block lengths, for which we use FlashInfer (Ye et al., 2025) as the inference backend), and report the measured latency on an RTX 5090 GPU. The hyperparameters starget and sbase are set to 0.9 and 0.98, respectively. Evaluation. We use VBench (Huang et al., 2024b) to evaluate generation quality on 5-second videos across 16 dimensions. These dimensions include Subject Consistency, Background Consistency, Aesthetic Quality, Imaging Quality, Object Class, Multiple Objects, Color, Spatial Relationship, Scene, Temporal Style, Overall Consistency, Human Action, Temporal Flickering, Motion Smoothness, Dynamic Degree, and Appearance Style. We report representative subset of these metrics in the main paper, and the complete results are reported in the appendix. Notably, we adopt the test prompts rewritten by Self Forcing (Huang et al., 2025a). For fair comparisons, we set the block size of all sparse attention methods to 64. We also adjust the resolution from 480832 LIGHT FORCING: Accelerating Autoregressive Video Diffusion via Sparse Attention Table 1. Performance comparison with state-of-the-art baselines on VBench (Huang et al., 2024b). Method Latency (s) Speedup Aesthetic Quality Imaging Quality Motion Smoothness Dynamic Degree Subject Consistency Background Consistency Quality Score Semantic Score Total Score Self-Forcing 1.3B (fps = 16) FlashAttention2 (Dao, 2023) 9.61 STA (Zhang et al., 2025d) Radial (Li et al., 2025c) SVG2 (Yang et al., 2025b) VMoBA (Wu et al., 2025) SLA (Zhang et al., 2025a) LIGHT FORCING 8.27 7.39 21.38 7.42 7.71 7.39 FlashAttention2 (Dao, 2023) 10.47 STA (Zhang et al., 2025d) Radial (Li et al., 2025c) SVG2 (Yang et al., 2025b) VMoBA (Wu et al., 2025) LIGHT FORCING 9.56 8.89 22.12 8.88 8.81 1.00 1.16 1.30 0.45 1.29 1.25 1.30 1.00 1.10 1.18 0.47 1.18 1.19 67.4 64.5 45.8 66.0 65.2 66.7 67.2 68. 65.6 55.1 66.7 59.9 67.2 70.0 71.7 66.1 68.2 69.9 69.8 71.0 98. 98.5 96.0 97.8 97.3 98.3 98.3 LongLive 1.3B (fps = 16) 69.3 71.2 72.0 67.0 68.2 70. 98.8 99.0 98.0 98.5 97.5 98.2 63.1 48.9 88.6 72.8 84.2 44.2 66. 39.2 22.8 25.0 44.4 50.6 59.4 95.3 96.3 90.2 93.6 92.8 95.6 96. 97.0 97.4 77.6 95.3 58.3 96.9 96.5 96.9 93.6 95.6 95.5 96.7 96. 97.2 97.8 88.9 96.1 80.9 96.7 84.8 84.0 78.7 83.9 84.5 83.4 85. 83.8 82.8 75.0 82.7 71.5 84.8 81.2 82.1 53.7 78.5 80.3 82.5 80. 80.7 81.6 66.6 78.7 70.7 80.2 84.1 83.6 73.7 82.8 83.6 83.2 84. 83.2 82.6 73.3 81.9 71.3 83.9 to 512 768, which avoids excessive padding overhead and potential non-equivalence introduced by certain methods (e.g., VMoBA (Wu et al., 2025)). Baselines. We compare Light Forcing with state-of-theart sparse attention methods for bidirectional video generation models, covering static mask selection approaches (STA (Zhang et al., 2025d), Sparse VideoGen2 (Yang et al., 2025b), and Radial Attention (Li et al., 2025c)) as well as dynamic mask selection approaches (VMoBA (Wu et al., 2025) and SLA (Zhang et al., 2025a)). To ensure fair comparison, we set the sparsity ratio of all static methods to around 80% (except for STA), and that of all dynamic methods to around 90%. Additional implementation details and hyperparameter settings for the specific method are provided in the Appendix. 5.2. Main Results Tab. 1 reports the evaluation results of our LIGHT FORCING and state-of-the-art baselines on two mainstream autoregressive video generation models, i.e., Self Forcing (Huang et al., 2025a) and LongLive (Yang et al., 2025a). We observe that LIGHT FORCING outperforms these methods by large margin on most metrics (e.g., Imaging Quality and Subject Consistency), while also achieving the highest speedups (1.3 on Self Forcing and 1.19 on LongLive). Notably, LIGHT FORCING yields higher Total Score than dense FlashAttention (Dao, 2023) baselines (84.5 vs. 84.1 for Self Forcing and 83.9 vs. 83.2 for LongLive), suggesting substantial redundancy in dense attention and indicating that properly designed sparse solutions can achieve lossless performance. Moreover, LIGHT FORCING demonstrates strong versatility and applicability, primarily in three aspects compared to prior methods. ➀ Sparsity ratio. Even with the smallest window size (i.e., (3, 3, 3)), STA (Zhang et al., 2025d) attains only 62.5% sparsity under such output resolution, and thus yields limited speedup. ➁ Extra overhead. While permutation-based methods such as SVG (Yang et al., 2025b) achieve relatively strong performance among training-free methods, they require repeated clustering initialization as the KV cache evolves, incurring particularly large extra overhead for few-step generators. ➂ Training difficulty. LongLive (Yang et al., 2025a) adopts LoRA (Hu et al., 2022) to finetune, making it challenging for some chunk-agnostic finetunable methods (e.g., VMoBA (Wu et al., 2025)) to converge. Even after fine-tuning, their performance still remains far from satisfactory. Qualitative comparisons are shown in Fig. 5. This further highlights that LIGHT FORCING preserves high-fidelity and consistent video examples, whereas other baselines exhibit pronounced degradation, including object duplication in multi-object scenes (e.g., STA (Zhang et al., 2025d) and VMoBA (Wu et al., 2025) producing two or more raccoons), anomalous objects (e.g., SLA (Zhang et al., 2025a) generating multiple handles of the guitar), and severe color shifts and artifacts (e.g., Radial (Li et al., 2025c)). These observations suggest that LIGHT FORCING better mitigates error accumulation and over-exposure effects, enabling high-quality long-duration video synthesis. Additional video examples on LongLive (Yang et al., 2025a) are in the Appendix. 5.3. Ablation Studies Ablation for Components. We evaluate the effect of our two components on Self Forcing (Huang et al., 2025a). We observe that directly applying 1D sparse attention (90% spar7 LIGHT FORCING: Accelerating Autoregressive Video Diffusion via Sparse Attention Figure 5. Qualitative comparisons of 5-second videos generated under the prompt cute raccoon playing guitar in boat on the ocean on Self Forcing (Huang et al., 2025a). We select frames at 0s, 2s, and 5s as representative snapshots of the video. Table 2. Ablation results for each component of LIGHT FORCING. +1D Sparse Attention means directly applying sparse attention (Zhang et al., 2025b) under the pretrained Self Forcing weights (Huang et al., 2025a). Method Subject Consistency Aesthetic Quality Imaging Quality Dynamic Degree Total Score FLASH ATTENTION +1D Sparse Attention +Finetune + CAG + CAG & HSA 95.3 86.9 94.9 96.1 96. 67.4 51.4 65.1 67.7 67.2 70.0 66.0 69.8 71.0 71. 63.1 52.8 46.4 37.5 66.7 84.1 73.0 82.8 83.2 84. sity) without fine-tuning results in severe quality collapse (e.g., degraded visual fidelity and dynamics). Although further fine-tuning partially recovers performance, it still falls short of dense attention (84.1 vs. 82.8 in Total Score). When combined with CAG, the model exhibits notable gains in Aesthetic Quality and Imaging Quality, but its dynamics deteriorate, suggesting that under aggressive sparsity the model relies more heavily on priors from preceding chunks at the expense of motion. In contrast, introducing HSA substantially improves dynamics and ultimately surpasses dense attention in Total Score. Table 3. Ablation study on the number of retrieved frames (topk) in HSA. We set topk = 6 in all experiments. Top-k Quality Score Semantic Score Total Score 6 9 12 85.4 85.2 85.1 80.9 80.8 80.9 84.5 84.4 84. Figure 6. Efficient deployment of Light Forcing. We measure its latency on RTX 5090 for 5-second video generation. 5.4. Efficient Deployment To better unlock the acceleration potential of autoregressive video generation models, we deploy LIGHT FORCING on the mainstream video generation inference framework LightX2V (Contributors, 2025) and profile the runtime latency of each component (see in Fig. 6). We replace the default Wan VAE (Wan et al., 2025) with efficient LightVAE (Contributors, 2025), and further deploy all linear layers in the model using low-bit FP8 precision (We quantize weights with per-channel granularity and activations with per-token granularity). Both choices are widely regarded as lossless acceleration techniques. In our latency evaluation, LIGHT FORCING achieves 3.29 speedup in attention time and 2.33 end-to-end speedup, while maintaining satisfactory generation quality. Remarkably, LIGHT FORCING 1.3B achieves 19.7 FPS, enabling real-time generation on consumer-level GPU for the first time. 6. Conclusion Sensitivity Analysis on HSA. We conduct hyperparameter study on the number of retrieved frames (topk) in the first-stage retrieval of HSA, reported in Tab. 3. Due to limited time and resources, we evaluate three settings (topk {6, 9, 12}). Our method remains highly robust, achieving similarly strong performance across these choices. This further suggests that, for each query block, attending to only small subset of past frames is sufficient to mitigate inconsistency issues. We proposed LIGHT FORCING, sparse attention framework tailored for autoregressive video diffusion. By introducing chunk-aware growth and hierarchical sparse attention, our method effectively mitigates error accumulation while preserving long-range context. Extensive experiments demonstrate consistent improvements in both efficiency and generation quality, enabling real-time video synthesis on consumer GPUs and establishing strong foundation for scalable AR video generation. 8 LIGHT FORCING: Accelerating Autoregressive Video Diffusion via Sparse Attention"
        },
        {
            "title": "Impact Statement",
            "content": "This paper presents work whose goal is to advance the field of Machine Learning. There are many potential societal consequences of our work, none of which we feel must be specifically highlighted here."
        },
        {
            "title": "References",
            "content": "Beltagy, LongI., Peters, M. E., and Cohan, A. former: The long-document transformer. arXiv preprint arXiv:2004.05150, 2020. Brooks, T., Peebles, B., Holmes, C., DePue, W., Guo, Y., Jing, L., Schnurr, D., Taylor, J., Luhman, T., Luhman, E., et al. Video generation models as world simulators. OpenAI Blog, 1(8):1, 2024. Bruce, J., Dennis, M. D., Edwards, A., Parker-Holder, J., Shi, Y., Hughes, E., Lai, M., Mavalankar, A., Steigerwald, R., Apps, C., et al. Genie: Generative interactive environments. In Forty-first International Conference on Machine Learning, 2024. Chen, B., Martı Monso, D., Du, Y., Simchowitz, M., Tedrake, R., and Sitzmann, V. Diffusion forcing: Nexttoken prediction meets full-sequence diffusion. Advances in Neural Information Processing Systems, 37:24081 24125, 2024. Chen, G., Lin, D., Yang, J., Lin, C., Zhu, J., Fan, M., Zhang, H., Chen, S., Chen, Z., Ma, C., et al. Skyreelsv2: Infinite-length film generative model. arXiv preprint arXiv:2504.13074, 2025a. Chen, J., Zhao, Y., Yu, J., Chu, R., Chen, J., Yang, S., Wang, X., Pan, Y., Zhou, D., Ling, H., et al. Sana-video: Efficient video generation with block linear diffusion transformer. arXiv preprint arXiv:2509.24695, 2025b. Contributors, L. Lightx2v: Light video generation inference https://github.com/ModelTC/ framework. lightx2v, 2025. Cui, J., Wu, J., Li, M., Yang, T., Li, X., Wang, R., Bai, A., Ban, Y., and Hsieh, C.-J. Self-forcing++: Towards minute-scale high-quality video generation. arXiv preprint arXiv:2510.02283, 2025. Dao, T. Flashattention-2: Faster attention with better parallelism and work partitioning. arXiv preprint arXiv:2307.08691, 2023. Decart, E., McIntyre, Q., Campbell, S., Chen, X., and Wachen, R. Oasis: universe in transformer. URL: https://oasis-model. github. io, 2024. Gao, K., Shi, J., Zhang, H., Wang, C., Xiao, J., and Chen, L. Ca2-vdm: Efficient autoregressive video diffusion model with causal generation and cache sharing. arXiv preprint arXiv:2411.16375, 2024. Gu, Y., Mao, W., and Shou, M. Z. Long-context autoregressive video modeling with next-frame prediction. arXiv preprint arXiv:2503.19325, 2025. Hassani, A., Zhou, F., Kane, A., Huang, J., Chen, C.-Y., Shi, M., Walton, S., Hoehnerbach, M., Thakkar, V., Isaev, M., et al. Generalized neighborhood attention: Multidimensional sparse attention at the speed of light. arXiv preprint arXiv:2504.16922, 2025. Henschel, R., Khachatryan, L., Poghosyan, H., Hayrapetyan, D., Tadevosyan, V., Wang, Z., Navasardyan, S., and Shi, H. Streamingt2v: Consistent, dynamic, and extendable long video generation from text. In Proceedings of the Computer Vision and Pattern Recognition Conference, pp. 25682577, 2025. Hu, E. J., Shen, Y., Wallis, P., Allen-Zhu, Z., Li, Y., Wang, S., Wang, L., Chen, W., et al. Lora: Low-rank adaptation of large language models. ICLR, 1(2):3, 2022. Hu, J., Hu, S., Song, Y., Huang, Y., Wang, M., Zhou, H., Liu, Z., Ma, W.-Y., and Sun, M. Acdit: Interpolating autoregressive conditional modeling and diffusion transformer. arXiv preprint arXiv:2412.07720, 2024. Huang, X., Li, Z., He, G., Zhou, M., and Shechtman, E. Self forcing: Bridging the train-test gap in autoregressive video diffusion. arXiv preprint arXiv:2506.08009, 2025a. Huang, Y., Wang, Z., Gong, R., Liu, J., Zhang, X., Guo, J., Liu, X., and Zhang, J. Harmonica: Harmonizing training and inference for better feature caching in diffusion transformer acceleration. arXiv preprint arXiv:2410.01723, 2024a. Huang, Y., Ge, X., Gong, R., Lv, C., and Zhang, J. Linvideo: post-training framework towards (n) attention in efficient video generation. arXiv preprint arXiv:2510.08318, 2025b. Huang, Z., He, Y., Yu, J., Zhang, F., Si, C., Jiang, Y., Zhang, Y., Wu, T., Jin, Q., Chanpaisit, N., et al. Vbench: Comprehensive benchmark suite for video generative models. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 2180721818, 2024b. Kodaira, A., Hou, T., Hou, J., Georgopoulos, M., JuefeiXu, F., Tomizuka, M., and Zhao, Y. Streamdit: Realtime streaming text-to-video generation. arXiv preprint arXiv:2507.03745, 2025. 9 LIGHT FORCING: Accelerating Autoregressive Video Diffusion via Sparse Attention Li, G., Wei, Y., Chen, Y., and Chi, Y. Towards faster nonasymptotic convergence for diffusion-based generative models. arXiv preprint arXiv:2306.09251, 2023. An open-source moe model with 52 billion activated parameters by tencent. arXiv preprint arXiv:2411.02265, 2024. Li, S., Gao, Y., Sadigh, D., and Song, S. Unified video action model. arXiv preprint arXiv:2503.00200, 2025a. Li, X., Gu, Y., Lin, X., Wang, W., and Zhuang, B. Psa: Pyramid sparse attention for efficient video understanding and generation. arXiv preprint arXiv:2512.04025, 2025b. Li, X., Li, M., Cai, T., Xi, H., Yang, S., Lin, Y., Zhang, L., Yang, S., Hu, J., Peng, K., et al. Radial attention: o(n log n) sparse attention with energy decay for long video generation. arXiv preprint arXiv:2506.19852, 2025c. Liu, F., Zhang, S., Wang, X., Wei, Y., Qiu, H., Zhao, Y., Zhang, Y., Ye, Q., and Wan, F. Timestep embedding tells: Its time to cache for video diffusion model. In Proceedings of the Computer Vision and Pattern Recognition Conference, pp. 73537363, 2025a. Liu, K., Hu, W., Xu, J., Shan, Y., and Lu, S. Rolling forcing: Autoregressive long video diffusion in real time. arXiv preprint arXiv:2509.25161, 2025b. Lu, Y., Zeng, Y., Li, H., Ouyang, H., Wang, Q., Cheng, K. L., Zhu, J., Cao, H., Zhang, Z., Zhu, X., et al. Reward forcing: Efficient streaming video generation with rewarded distribution matching distillation. arXiv preprint arXiv:2512.04678, 2025. Ma, X., Fang, G., Bi Mi, M., and Wang, X. Learningto-cache: Accelerating diffusion transformer via layer caching. Advances in Neural Information Processing Systems, 37:133282133304, 2024. Millon, E. Krea realtime 14b: Real-time video generation, 2025. URL https://github.com/krea-ai/ realtime-video. Parker-Holder, J., Ball, P., Bruce, J., Dasagi, V., Holsheimer, K., Kaplanis, C., Moufarek, A., Scully, G., Shar, J., Shi, J., et al. Genie 2: large-scale foundation world model. URL: https://deepmind. google/discover/blog/genie-2-alarge-scale-foundation-world-model, 2024. Peebles, W. and Xie, S. Scalable diffusion models with transformers. In Proceedings of the IEEE/CVF international conference on computer vision, pp. 41954205, 2023. Song, Y., Dhariwal, P., Chen, M., and Sutskever, I. Consistency models. 2023. Sun, X., Chen, Y., Huang, Y., Xie, R., Zhu, J., Zhang, K., Li, S., Yang, Z., Han, J., Shu, X., et al. Hunyuan-large: Teng, H., Jia, H., Sun, L., Li, L., Li, M., Tang, M., Han, S., Zhang, T., Zhang, W., Luo, W., et al. Magi-1: Autoregressive video generation at scale. arXiv preprint arXiv:2505.13211, 2025. Van Den Oord, A., Vinyals, O., et al. Neural discrete representation learning. Advances in neural information processing systems, 30, 2017. Wan, T., Wang, A., Ai, B., Wen, B., Mao, C., Xie, C.-W., Chen, D., Yu, F., Zhao, H., Yang, J., et al. Wan: Open and advanced large-scale video generative models. arXiv preprint arXiv:2503.20314, 2025. Wu, J., Hou, L., Yang, H., Tao, X., Tian, Y., Wan, P., Zhang, D., and Tong, Y. Vmoba: Mixture-of-block attention for video diffusion models. arXiv preprint arXiv:2506.23858, 2025. Xi, H., Yang, S., Zhao, Y., Xu, C., Li, M., Li, X., Lin, Y., Cai, H., Zhang, J., Li, D., et al. Sparse videogen: Accelerating video diffusion transformers with spatial-temporal sparsity. arXiv preprint arXiv:2502.01776, 2025. Xie, E., Chen, J., Chen, J., Cai, H., Tang, H., Lin, Y., Zhang, Z., Li, M., Zhu, L., Lu, Y., et al. Sana: Efficient highresolution image synthesis with linear diffusion transformers. arXiv preprint arXiv:2410.10629, 2024. Xu, R., Xiao, G., Huang, H., Guo, J., and Han, S. Xattention: Block sparse attention with antidiagonal scoring. arXiv preprint arXiv:2503.16428, 2025. Yang, M., Du, Y., Ghasemipour, K., Tompson, J., Schuurmans, D., and Abbeel, P. Learning interactive realworld simulators. arXiv preprint arXiv:2310.06114, 1(2): 6, 2023. Yang, S., Huang, W., Chu, R., Xiao, Y., Zhao, Y., Wang, X., Li, M., Xie, E., Chen, Y., Lu, Y., et al. Longlive: Realtime interactive long video generation. arXiv preprint arXiv:2509.22622, 2025a. Yang, S., Xi, H., Zhao, Y., Li, M., Zhang, J., Cai, H., Lin, Y., Li, X., Xu, C., Peng, K., et al. Sparse videogen2: Accelerate video generation with sparse attention via semanticaware permutation. arXiv preprint arXiv:2505.18875, 2025b. Yang, Z., Teng, J., Zheng, W., Ding, M., Huang, S., Xu, J., Yang, Y., Hong, W., Zhang, X., Feng, G., et al. Cogvideox: Text-to-video diffusion models with an expert transformer. arXiv preprint arXiv:2408.06072, 2024. 10 LIGHT FORCING: Accelerating Autoregressive Video Diffusion via Sparse Attention Ye, Z., Chen, L., Lai, R., Lin, W., Zhang, Y., Wang, S., Chen, T., Kasikci, B., Grover, V., Krishnamurthy, A., and Ceze, L. Flashinfer: Efficient and customizable attention engine for llm inference serving. arXiv preprint arXiv:2501.01005, 2025. URL https:// arxiv.org/abs/2501.01005. Yin, T., Gharbi, M., Park, T., Zhang, R., Shechtman, E., Durand, F., and Freeman, B. Improved distribution matching distillation for fast image synthesis. Advances in neural information processing systems, 37:4745547487, 2024. Yin, T., Zhang, Q., Zhang, R., Freeman, W. T., Durand, F., Shechtman, E., and Huang, X. From slow bidirectional to fast autoregressive video diffusion models. In Proceedings of the Computer Vision and Pattern Recognition Conference, pp. 2296322974, 2025. Zhang, J., Huang, H., Zhang, P., Wei, J., Zhu, J., and Chen, J. Sageattention2: Efficient attention with thorough outlier smoothing and per-thread int4 quantization. arXiv preprint arXiv:2411.10958, 2024a. Zhang, J., Wei, J., Huang, H., Zhang, P., Zhu, J., and Chen, J. Sageattention: Accurate 8-bit attention for plug-and-play inference acceleration. arXiv preprint arXiv:2410.02367, 2024b. Zhang, J., Wang, H., Jiang, K., Yang, S., Zheng, K., Xi, H., Wang, Z., Zhu, H., Zhao, M., Stoica, I., et al. Sla: Beyond sparsity in diffusion transformers via fine-tunable sparse-linear attention. arXiv preprint arXiv:2509.24006, 2025a. Zhang, J., Xiang, C., Huang, H., Wei, J., Xi, H., Zhu, J., and Chen, J. Spargeattn: Accurate sparse attention accelerating any model inference. arXiv preprint arXiv:2502.18137, 2025b. Zhang, L. and Agrawala, M. Packing input frame context in next-frame prediction models for video generation. arXiv preprint arXiv:2504.12626, 2025. Zhang, P., Chen, Y., Huang, H., Lin, W., Liu, Z., Stoica, I., Xing, E., and Zhang, H. Vsa: Faster video diffusion with trainable sparse attention. arXiv preprint arXiv:2505.13389, 2025c. Zhang, P., Chen, Y., Su, R., Ding, H., Stoica, I., Liu, Z., and Zhang, H. Fast video generation with sliding tile attention. arXiv preprint arXiv:2502.04507, 2025d. Zhou, Y., Xiao, Z., Wei, T., Yang, S., and Pan, X. Trainable log-linear sparse attention for efficient diffusion transformers. arXiv preprint arXiv:2512.16615, 2025. LIGHT FORCING: Accelerating Autoregressive Video Diffusion via Sparse Attention A. Implementation details of Baselines Since most existing sparse attention methods are originally designed for bidirectional video generation models, applying them to autoregressive video generation requires additional clarification and careful consideration. STA (Zhang et al., 2025d): STA partitions tokens into 3D tiles and applies sparse attention to neighboring tiles. In all experiments, we use window size of (3, 3, 3). The original paper keeps early timesteps in dense attention. Since autoregressive models are typically few-step generators (e.g., 4 steps), we do not adopt this setting and apply sparse attention to all steps. Radial Attention (Li et al., 2025c): Since the key-value (KV) sequence length varies over chunks in autoregressive video generation, the effective sparsity ratio of Radial Attention also changes accordingly. For 5 videos, we perform inference over 7 chunks (3 frames per chunk) with the following sparsity ratios: 67.7, 76.9, 80.6, 82.4, 83.5, 84.9, 86.5. SVG2 (Yang et al., 2025b): Since SVG2 relies on K-means clustering and the sequence length in autoregressive generators is shorter than that in bidirectional models, we adjust several hyperparameters to improve its runtime efficiency: num centroids=50, num centroids=100, kmeans iter init=20, top kmeans=0.9, min kc ratio=0.10, and kmeans iter step=2. Notably, whenever the KV length changes in AR models, we re-initialize K-means clustering accordingly. LIGHT FORCING: Once the sparsity ratio si for chunk is determined, the number of selected blocks in HSA is fixed accordingly, i.e., (1 si) n/bkv. We then perform block selection within the frames (i.e., 6 frames in our experiment) chosen by the frame-wise mask selection. Notably, we keep dense attention for the first chunk and apply Chunk-Aware Growth for sparsity allocation in subsequent chunks. This is because the first chunk has relatively short sequence length, so sparse attention yields limited speedup but can cause pronounced performance degradation. B. Theoretical proof of CAG Denoising-with-re-noising Markov kernel (chunk-wise). Fix an AR chunk index and conditioning (x<i, c), and let the inference schedule be tT > tT 1 > > t0 with corresponding noise levels {σtj }T j=0 (0, 1]. The transition operator Ψ induces the stochastic update xi tj1 (cid:16) = Ψ Gθ(xi tj , tj, x<i, c), ϵtj1 , tj1 (cid:17) = (1 σtj1 ) Gθ(xi tj , tj, x<i, c) + σtj1 ϵtj1 , (14) where ϵtj1 (0, I) are i.i.d. across steps. Hence, conditional on xi tj , the transition is Gaussian: xi tj1 xi tj N(cid:0)µθ,j(xi tj ), σ2 tj1 I(cid:1), µθ,j(y) := (1 σtj1 ) Gθ(y, tj, x<i, c). (15) Ideal reverse kernel and mean-map error. Let µ () be the ideal reverse mean map (defined by the exact score / optimal denoiser under the same schedule), and denote by p0( x<i, c) the true conditional data distribution of xi. Let q0( x<i, c) be the distribution of the generated output after transitions from xi (0, I). Assume the (average) conditional tT mean-map error 1 (cid:88) j=1 (cid:104)(cid:13) (cid:13)µθ,j(xi tj ) µ (xi tj )(cid:13) 2 (cid:13) 2 (cid:12) (cid:105) (cid:12) x<i, (cid:12) ε2 mean, which is implied by the score/denoiser estimation accuracy as in the assumptions of Theorem 3 in (Li et al., 2023). Stepwise KL control (Gaussian KL). For Gaussians with equal covariance, we have KL(N (µ, Σ) (µ, Σ)) = 1 2 (cid:13) (cid:13)Σ1/2(µ µ)(cid:13) 2 2. (cid:13) Using Σ = σ2 tj in equation 15 yields the per-step contribution (16) (17) (cid:104) (cid:16) KL (µ (xi tj ), σ2 tj1 I) (cid:13) (cid:13) (µθ,j(xi (cid:13) tj ), σ2 tj1 (cid:17) (cid:12) (cid:105) (cid:12) x<i, (cid:12) I) = 1 2σ2 tj1 (cid:104)(cid:13) (cid:13)µθ,j(xi tj ) µ (xi tj )(cid:13) 2 (cid:13) 2 (cid:12) (cid:105) (cid:12) x<i, (cid:12) . (18) 12 LIGHT FORCING: Accelerating Autoregressive Video Diffusion via Sparse Attention Telescoping KL and TV bound. Applying the KL chain rule along the Markov chain induced by equation 14, one obtains KL(cid:0)q0( x<i, c) p0( x<i, c)(cid:1) (cid:88) j=1 1 2σ2 tj (cid:104)(cid:13) (cid:13)µθ,j(xi tj ) µ (xi tj )(cid:13) 2 (cid:13) 2 (cid:12) (cid:105) (cid:12) x<i, (cid:12) + KL(qtT ptT ) . (19) By Pinskers inequality, TV(cid:0)q0( x<i, c), p0( x<i, c)(cid:1) (cid:114) 1 2 KL(q0( x<i, c) p0( x<i, c)). (20) Error vs. number of denoising steps . Under the same regularity and schedule conditions in (Li et al., 2023), the discretization/mixing term contributes O(d/T ) in KL (polylog factors), while the model (score) error contributes O(d ε2 score) in KL. Combining equation 20 with Eq. (31) of (Li et al., 2023) gives the conditional TV guarantee TV(cid:0)q0( x<i, c), p0( x<i, c)(cid:1) C1 log3 + C2 εscore log2 t, for universal constant > 0. In particular, when εscore = 0, the sampling error decays as O(d/ imperfect models it saturates at O( εscore) as . (21) ), whereas for C. Detailed VBench Results We report the full VBench (Huang et al., 2024b) results across all dimensions for each method in Tab. 4 and Tab. 5. Table 4. Detailed performance comparison with state-of-the-art baselines on VBench (Huang et al., 2024b) (quality part). Method Subject Consistency Background Consistency Temporal Flickering Motion Smoothness Aesthetic Quality Imaging Quality Dynamic Degree FlashAttention2 (Dao, 2023) STA (Zhang et al., 2025d) Radial (Li et al., 2025c) SVG2 (Yang et al., 2025b) SLA (Zhang et al., 2025a) VMoBA (Wu et al., 2025) OURS FlashAttention2 (Dao, 2023) STA (Zhang et al., 2025d) Radial (Li et al., 2025c) SVG2 (Yang et al., 2025b) VMoBA (Wu et al., 2025) OURS 95.3 96.3 90.2 93.6 95.6 92.8 96.2 97.0 97.4 77.6 95.3 58.3 96. Self-Forcing 1.3B (fps = 16) 96.5 96.9 93.6 95.6 96.7 95.5 96.5 99.1 99.2 95.6 98.2 99.2 98.0 99.2 LongLive 1.3B (fps = 16) 97.2 97.8 88.9 96.1 80.9 96.7 99.3 99.6 98.1 98.8 97.6 98. 98.3 98.5 96.0 97.8 98.3 97.3 98.3 98.8 99.0 98.0 98.5 97.5 98.2 67.4 64.5 45.8 66.0 66.7 65.2 67.2 68.7 65.6 55.1 66.7 59.9 67.2 70.0 71.7 66.1 68.2 69.8 69.9 71.0 69.3 71.2 72.0 67.0 68.2 70. 63.1 48.9 88.6 72.8 44.2 84.2 66.7 39.2 22.8 25.0 44.4 50.6 59.4 D. Limitations Despite the strong empirical results, our work has several limitations. First, we evaluate only 1.3B models. Scaling LIGHT FORCING to larger models (e.g., 14B realtime-video model (Millon, 2025)) is an important direction. Second, our sparse attention is built on FlashAttention 2 (Dao, 2023) and may require additional kernel adaptations for newer GPU architectures (e.g., Hopper). Finally, while sparsity is effective, combining it with other methods (e.g., step distillation or low-bit quantization) to achieve larger gains remains open. E. More Visualization Examples We provide more detailed qualitative comparisons in the supplementary material. In Fig. 7, we visualize results for all baselines under two prompts, person is clay pottery making and Turtle swimming in ocean. Most baselines exhibit 13 LIGHT FORCING: Accelerating Autoregressive Video Diffusion via Sparse Attention Table 5. Detailed performance comparison with state-of-the-art baselines on VBench (Huang et al., 2024b) (semantic part). Method Object Class Multiple Objects Human Action Color Spatial Relationship Scene Appearance Style Temporal Style Overall Consistency FlashAttention2 (Dao, 2023) 94.9 95.2 STA (Zhang et al., 2025d) 56.0 Radial (Li et al., 2025c) 93.5 SVG2 (Yang et al., 2025b) 96.4 SLA (Zhang et al., 2025a) 93.9 VMoBA (Wu et al., 2025) 94.3 OURS FlashAttention2 (Dao, 2023) 94.4 95.5 STA (Zhang et al., 2025d) 72.7 Radial (Li et al., 2025c) 92.3 SVG2 (Yang et al., 2025b) 78.2 VMoBA (Wu et al., 2025) 95.3 OURS Self-Forcing 1.3B (fps = 16) 88.4 86.1 31.8 73.5 87.5 81.1 88.9 87.8 86.0 59.9 78.9 64.8 89.6 96.4 95.2 80.4 96.4 96.8 96.8 96.0 88.6 91.7 86.5 87.8 91.8 90.5 88.2 83.1 91.1 39.0 76.9 89.3 81.0 81.4 LongLive 1.3B (fps = 16) 96.4 95.4 93.2 96.0 96.4 96.6 89.2 94.7 93.0 88.5 89.3 86.1 78.5 90.4 66.6 74.5 64.6 78.7 54.4 57.1 15.1 54.2 56.3 55.3 55.3 55.6 54.8 16.7 53.9 26.8 53.3 20.6 22.1 22.3 20.4 20.5 20.5 20. 20.6 21.7 22.4 20.4 21.5 20.1 24.6 23.0 15.9 24.5 24.2 24.3 24.6 24.3 22.2 19.4 24.4 23.2 24.4 26.9 25.5 18.1 27.0 26.8 26.8 26.9 26.7 25.1 22.4 27.0 26.0 26.7 noticeable degradation, including (1) loss of fine-grained details (e.g., distorted hands) and (2) anomalous generations (e.g., turtle with two heads). Figure 7. More qualitative examples on Self Forcing (Huang et al., 2025a)."
        }
    ],
    "affiliations": [
        "AUMOVIO Singapore Pte Ltd",
        "AUMOVIO-NTU Corporate Lab",
        "Beihang University",
        "Hong Kong University of Science and Technology",
        "Nanyang Technological University",
        "Sensetime Research"
    ]
}