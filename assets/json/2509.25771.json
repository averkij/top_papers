{
    "paper_title": "Free Lunch Alignment of Text-to-Image Diffusion Models without Preference Image Pairs",
    "authors": [
        "Jia Jun Cheng Xian",
        "Muchen Li",
        "Haotian Yang",
        "Xin Tao",
        "Pengfei Wan",
        "Leonid Sigal",
        "Renjie Liao"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Recent advances in diffusion-based text-to-image (T2I) models have led to remarkable success in generating high-quality images from textual prompts. However, ensuring accurate alignment between the text and the generated image remains a significant challenge for state-of-the-art diffusion models. To address this, existing studies employ reinforcement learning with human feedback (RLHF) to align T2I outputs with human preferences. These methods, however, either rely directly on paired image preference data or require a learned reward function, both of which depend heavily on costly, high-quality human annotations and thus face scalability limitations. In this work, we introduce Text Preference Optimization (TPO), a framework that enables \"free-lunch\" alignment of T2I models, achieving alignment without the need for paired image preference data. TPO works by training the model to prefer matched prompts over mismatched prompts, which are constructed by perturbing original captions using a large language model. Our framework is general and compatible with existing preference-based algorithms. We extend both DPO and KTO to our setting, resulting in TDPO and TKTO. Quantitative and qualitative evaluations across multiple benchmarks show that our methods consistently outperform their original counterparts, delivering better human preference scores and improved text-to-image alignment. Our Open-source code is available at https://github.com/DSL-Lab/T2I-Free-Lunch-Alignment."
        },
        {
            "title": "Start",
            "content": "FREE LUNCH ALIGNMENT OF TEXT-TO-IMAGE DIFFUSION MODELS WITHOUT PREFERENCE IMAGE PAIRS Jia Jun Cheng Xian1,2 Muchen Li1,2 Haotian Yang4 Xin Tao4 Pengfei Wan4 Leonid Sigal1,2,3,5 Renjie Liao1,2,3 1University of British Columbia 4Kling Team, Kuaishou Technology 5NSERC CRC Chair 2Vector Institute for AI 3Canada CIFAR AI Chair 5 2 0 2 0 3 ] . [ 1 1 7 7 5 2 . 9 0 5 2 : r Figure 1: Image generated by our aligned StableDiffusion 1.5 model. Notably, our model is trained on free lunch text preference data and does not require access to human preference data."
        },
        {
            "title": "ABSTRACT",
            "content": "Recent advances in diffusion-based text-to-image (T2I) models have led to remarkable success in generating high-quality images from textual prompts. However, ensuring accurate alignment between the text and the generated image remains significant challenge for state-of-the-art diffusion models. To address this, existing studies often employ reinforcement learning with human feedback (RLHF) to align T2I outputs with human preferences. These methods, however, either rely directly on paired image preference data or require learned reward function, both of which depend heavily on costly, high-quality human annotations and thus face scalability limitations. In this work, we introduce Text Preference Optimization (TPO), novel framework that enables free-lunc alignment of T2I models, achieving alignment without the need for paired image preference data. TPO works by training the model to prefer matched prompts over mismatched prompts, which are constructed by perturbing original captions using large language model (LLM). Our framework is general and compatible with existing preference-based algorithms. We extend both DPO and KTO to our setting, resulting in TDPO and TKTO. Quantitative and qualitative evaluations across multiple benchmarks show that our methods consistently outperform their original counterparts, yielding superior human preference scores and better text-to-image alignment. Our open-source code is available at https://github.com/DSL-Lab/T2I-Free-Lunch-Alignment."
        },
        {
            "title": "INTRODUCTION",
            "content": "Text-to-Image (T2I) models, driven by diffusion(Karras et al., 2022; Kingma et al., 2023; Rombach et al., 2022a; Song et al., 2020), rectified flow (Lipman et al., 2022; Liu et al., 2022), and next-token prediction methods (Wang et al., 2024), have seen significant advances in generating high-quality images from textual descriptions. This is achieved by pretraining on large-scale, high-quality image Equal contributions."
        },
        {
            "title": "Preprint",
            "content": "caption datasets. However, the trained models performance relies heavily on the quality of the pretraining datasets, and does not necessarily reflect human preferences for image quality and textual alignment. To mitigate this, recent studies have focused on aligning text-to-image (T2I) models with human preferences in the post-training phase. Motivated by the success of large language models (LLMs), line of work has introduced reinforcement learning from human feedback (RLHF) for T2I alignment (Black et al., 2024; Clark et al., 2024; Fan et al., 2023; Lee et al., 2023). These approaches involve first training reward model to predict human preferences and then optimizing the generative model via policy gradient methods. However, this pipeline remains both complex, due to the difficulty of constructing reliable and stable reward model, and resource-intensive, given the substantial amount of human-labeled data required for effective reward modeling. Moreover, the high-quality image datasets used during initial training offer limited utility at this stage, as alignment with human preferences necessitates separate and costly collection of preference-specific annotations. More recently, Direct Preference Optimization (DPO) (Rafailov et al., 2023) and KahnemanTversky Optimization (KTO) (Ethayarajh et al., 2024) have simplified this pipeline by formulating preference learning as single-stage, closed-form optimization problem. These methods have also been extended to align T2I models (Li et al., 2024; Wallace et al., 2024). significant drawback, however, is their heavy reliance on high-quality, preference-labeled image pairs, which are expensive to collect and have been shown to be vulnerable to noise and inconsistencies in human annotations (Yang et al., 2025). We aim to answer the following question: Can we improve text-to-image model alignment without annotating human preference image pairs? Motivated by contrastive methods used in training vision-language models, we propose to leverage the high-quality datasets originally used for T2I training by optimizing preference alignment over text pairs rather than image pairs, as in Diffusion-DPO or Diffusion-KTO. This approach is based on the observation that, given an image-caption dataset, generating mismatched prompts for given image is significantly easier than constructing image preference pairs for single prompt. To enable the construction of such text-preference pairs, we utilize LLMs to generate negative samplesprompts that closely resemble the original but are intentionally mismatched. For example, as shown in Fig. 2, our pipeline flips the word inside in the original prompt to outside, inducing spatial layout change. This strategy encourages the model to focus on subtle distinctions in prompt semantics, resulting in improved alignment performance. Our contributions are threefold: Preference-data-free alignment. We propose novel method for aligning text-to-image diffusion models without requiring human preference data, offering free lunch post-training solution while improving text-to-image alignment. Generalizable pipeline. Our approach is model-agnostic and can be seamlessly integrated into any RLHF-based method that utilizes preference pairs, making it broadly applicable across existing alignment frameworks. State-of-the-art results. By adapting DPO and KTO into our frameworkTDPO and TKTOwe achieve state-of-the-art performance both qualitatively and quantitatively, surpassing the baselines without using any human preference annotations."
        },
        {
            "title": "2 RELATED WORKS",
            "content": "Text-to-Image Models. Text-to-image diffusion models have emerged as one of the most powerful and widely adopted generative techniques for image synthesis (Balaji et al., 2022; Chang et al., 2023; Ho et al., 2020; Karras et al., 2022; Kingma et al., 2023; Rombach et al., 2022a; Shi et al., 2020; Song et al., 2020). These models are capable of generating high-quality images that closely match the semantics of given text prompt. Despite their impressive performance, achieving precise alignment between textual descriptions and visual outputs remains key challenge. Recent works have explored improvements through various directions, such as enhanced text encoding (Liu et al., 2024b; Ma et al., 2024; Wu et al., 2023a), improved text embedding interaction architectures (Esser et al., 2024; Liu et al., 2024a), better image captioning (Betker et al., 2023; Lei et al., 2023), and inference-time strategies (Jiang et al., 2024; Prabhudesai et al., 2024; Shen et al., 2024; Wallace et al., 2023). Orthogonal to these approaches, our work focuses on improving text-image alignment through novel preference optimization strategy, providing complementary direction to existing methods."
        },
        {
            "title": "Preprint",
            "content": "Figure 2: Overview of our Text Preference Optimization (TPO) alignment framework versus the standard Diffusion-DPO/KTO pipeline. (Top) We leverage LLMs to perform prompt editing under four principles (content, attribute, spatial, contextual), automatically generating mismatched prompts to form winning/losing text pairs. These prompt pairs are then used to align the diffusion model via our TDPO and TKTO variants in free lunch manner. (Bottom) In contrast, existing DiffusionDPO/KTO methods rely on costly human-annotated image preference pairs. LLM Alignment. In recent years, large language models (LLMs) have grown rapidly in scale and capability, demonstrating impressive generative performance across wide range of language tasks. This power, however, comes with the risk of harmful or undesirable behavior. To mitigate such risks, Reinforcement Learning from Human Feedback (RLHF) (Christiano et al., 2023; Ouyang et al., 2022) was introduced. In RLHF, human annotators rank model outputs to create preference dataset; reward model is then trained to predict these human preferences, and the LLM is fine-tuned to maximize the learned reward. More recently, alignment methods that avoid an explicit reward-modeling stage have emerged. Direct Preference Optimization (DPO) (Rafailov et al., 2023) represents preferences implicitly via BradleyTerry model, allowing the policy to be optimized in closed form. Several variants (Ethayarajh et al., 2024; Shao et al., 2024; Wu et al., 2024) and extensions of DPO have further simplified and improved LLM alignment. While these techniques were first developed for LLMs, our focus is on adapting these concepts to align text-to-image diffusion models. We build on the optimization strategies pioneered in language alignment and adapt them to the multimodal setting. Text-to-Image Preference Optimization. As with LLMs, text-to-image (T2I) diffusion models must also be aligned to respect user preferences and safety constraints. Building on the success of preference-based methods such as DPO, several studies have recently transplanted these objectives to the T2I setting and reported strong gains (Karthik et al., 2024; Li et al., 2024; Miao et al., 2024; Wallace et al., 2024; Yang et al., 2024). In particular, DiffusionDPO (Wallace et al., 2024) and DiffusionKTO (Li et al., 2024) extend the DPO and KTO objectives (Ethayarajh et al., 2024) to diffusion-based generators. DSPO (Zhu et al., 2025) aims to close the gap between preference alignment method used in LLM and T2I diffusion models by leveraging score matching. We introduce unified framework that is agnostic to the specific preference-alignment loss and can embed any diffusion T2I alignment objective. To illustrate its flexibility, we show how both DiffusionDPO and DiffusionKTO instantiate naturally within our formulation."
        },
        {
            "title": "3 METHOD",
            "content": "The effectiveness of preference alignment methods for diffusion models (Wallace et al., 2024; Li et al., 2024) depends heavily on access to high-quality image preference datasets. However, collecting such datasets at scale is challenging due to several key limitations. First, collecting human preferences is expensive, as it requires substantial manual effort for both annotation and validation. Moreover, when the underlying diffusion model changes (e.g., from Stable Diffusion 1.5 to 3.0), previously collected preference data may no longer be effective, requiring fresh round of data collection. Second, human preferences reflect mixture of factors, including image quality, alignment with the text prompt, and subjective aesthetic judgment, making them inherently noisy and inconsistent."
        },
        {
            "title": "Preprint",
            "content": "Figure 3: An example of how the four modification principles (content, attribute, spatial, contextual) are applied on given image-prompt pair. Our motivation stems from the observation that generating matched and mismatched text pairs is significantly easier than collecting image preference pairs. By leveraging LLMs to generate text preference pairs for each image, we can align text-to-image models with minimal human supervision, offering highly cost-effective alternative to manual preference annotation. This enables scalable and efficient alignment at virtually no additional labeling cost. More formally, given paired dataset {x, cw, cl} where is an high-quality image from human-curated dataset, cw and cl are the matched and mismatched captions, respectively, we aim to learn new model pθ(xc) that can achieve better on both text-to-image alignment and human preference alignment. Below, we first introduce our text preference pair construction pipeline, then formally derive our method. 3.1 TEXT PREFERENCE CONSTRUCTION WITH LLMS Given paired dataset {x, c}, where is an image and is its corresponding caption, we aim to construct new dataset with triplets x, cw, cl, where cw aligns better with the image than cl. We assume the original dataset is of high quality, meaning that the caption accurately describes the image. Based on this assumption, we use LLMs to generate negative captions cl while using the original caption from the dataset as cw = c. To achieve this, we prompt LLMs to modify the ground truth caption such that the perturbed caption describes an image that is visually distinct from the original. More specifically, we define the four core principles for LLMs to follow when editing the prompt: Content Modifications: Altering the presence or quantity of concrete objects in the scene. This includes adding or removing objects, replacing one object with another, or changing the number of instances (e.g., modifying three trees to five trees). Attribute and Descriptor Modifications: Changing visual or stylistic properties of objects, such as material, texture, or style. This also includes enriching descriptions with detailed qualifiers, while avoiding trivial edits like simple color changes. Spatial and Dynamic Modifications: Modifying the spatial arrangement or motion state of objects. Examples include adjusting object poses, changing spatial relations (foreground/background), or altering object alignment and composition. Contextual and Environmental Modifications: Editing elements related to the broader scene context, such as background, weather, lighting, or time of day. Changes may also involve shifting the physical or cultural setting (e.g., urban vs. historical). An example of how these four principles are applied is shown in Fig. 3. More examples and complete prompts for these principles are provided in Sec. A.4 and Sec. A.5. Preference Optimization over Input Conditions. In our setting, given an image x, we have matching text prompt cw and set of mismatched prompts {cl i=1. Unlike standard DPO and KTO, where preference optimization is performed over output images, our goal is to align the model with both matched and mismatched input conditions. In this section, we derive the text preference optimization objectives for DPO and KTO under our setting, which we refer to as Text Preference DPO (TDPO) and Text Preference KTO (TKTO), respectively. i}N"
        },
        {
            "title": "Preprint",
            "content": "TDPO. For Text Preference DPO, we have triplet {cw, cl, x} for each image. We have access to the preference relation cw cl x, where cw and cl {cl i=1 denote the matched and mismatched text prompts, respectively. Following Rafailov et al. (2023); Wallace et al. (2024), we use the Bradley-Terry (BT) (Bradley & Terry, 1952) model to model this preference as i}N pBT(cw cl x) = σ(r(cw, x) r(cl, x)), where σ is the sigmoid function and the reward model rθ is neural network parameterized by θ. Our training objective is to minimize the negative log-likelihood of this preference: (1) = Ecw,cl,x[log σ(r(cw, x) r(cl, x))] To simplify this optimization and avoid explicit reward modeling, DPO implicitly represents the reward function. Following the derivations in Rafailov et al. (2023), the reward function can be expressed by optimal θ and pref, and by taking Bayes rule, we get: θ(cx) pref(cx) θ(xc) pref(xc) + β log Z(x) = β θ(x) pref(x) r(c, x) = β log + β log Z(x) log log (3) (2) (cid:20) (cid:21) Here in the second equality of Eq. (3), we assume that the text condition is sampled from the dataset Dc and is independent of the model parameters θ. Consequently, we have pθ(c) = pref(c). However, pθ(x) = pref(x) since the distribution of the generated image is characterized by θ. We can then substitute Eq. (3) into Eq. (2) and get: (cid:18) pθ(xcw) pref(xcw) where optimizing Eq. (4) is equivalent to maximizing Eq. (1). TKTO. For Text Preference KTO, we have input {c, x, ω(c)} where ω(c) = 1 if = cw and ω(c) = 1 for {cl i=1. KTO seeks to maximize the following objective: pθ(xcl) pref(xcl) LTDPO(θ) = β log i}N cw ,cl,x β log log σ (cid:19)(cid:21) (4) (cid:20) max Here is the utility function, where r(c, x) and z0 are defined as: Ec,x[U (ω(c) (r(c, x) z0))] r(c, x) = β log pθ(xc) pref(xc) ; z0 = sg [βKL(pθ(xc)pref(xc))] (5) (6) where sg refers to the stop gradient operator. Following Kahneman-Tverskys prospect theory (Tversky & Kahneman, 1992) , we use centered sigmoid function as utility function. This gives us the training objective for TKTO: LTKTO(θ) = Ec,x (cid:20) σ(ω(c)β(log pθ(xc) pref(xc) (cid:21) z0)) (7)"
        },
        {
            "title": "3.2 TEXT PREFERENCE OPTIMIZATION FOR DIFFUSION MODEL",
            "content": "We now extend our TPO alignment algorithm to align diffusion-based T2I models. Diffusion Model. Diffusion models (Song et al., 2020; Ho et al., 2020; Kingma et al., 2023) are generative models that sample from learned distribution pθ(x0), trained to approximate the empirical data distribution q(x0). Training proceeds by learning to invert fixed forward process of (denoising) diffusion q(xtxt1). The forward process q(x1:T x0) is Markov chain with Gaussian transition probabilities governed by noise schedules {αt, σt}, as defined in Rombach et al. (2022a), that gradually add noise to data x0. The reverse (denoising) process is defined by pθ(x0:T ) = p(xT ) (cid:89) t=1 (cid:0)xt1 xt (cid:1), where pθ (cid:0)xt1 xt pθ (cid:16) (cid:1) = xt1; µθ(xt, t), σ2 (cid:17) . (8) neural network ϵθ(xt, t) is trained to predict the noise ϵ in xt by minimizing the simplified evidence lower bound associated to this model, LDM = Ex0q(x), tU [0,T ], ϵN (0,I),xtq(xtx0) (cid:104) w(t) (cid:13) (cid:13)ϵ ϵθ(xt, t)(cid:13) 2 (cid:13) 2 (cid:105) , (9) where w(t) is weighting function."
        },
        {
            "title": "Preprint",
            "content": "Diffusion TDPO. Existing work has adapted DPO into the field of diffusion model (Wallace et al., 2024; Yang et al., 2024). Diffusion-DPO (Wallace et al., 2024) frames the diffusion process as MDPs and defines the reward function as the reward of the whole chain, r(c, x0) = Epθ(x1:T x0,c)[R(c, x0:T )]. It then utilizes an evidence lower bound to adapt the training objective of DPO into the diffusion model setting. For the detail of this adaptation, please look at section 4 of Diffusion-DPO (Wallace et al., 2024). Following their work and Eq. (4), we can similarly derive the naive DiffusionTDPO objective, it can be rewritten as: LDiff-TDPO = x0,cw ,cl,t,xt [log σ(βw(t)(ϵθ(xt, cw, t) ϵw2 (ϵθ(xt, cl, t) ϵl2 2 ϵref(xt, cw, t) ϵw2 2 2 ϵref(xt, cl, t) ϵl2 2)))], (10) where ϵθ and ϵref are our training model and pretrained frozen model, is uniform distribution. Diffusion TKTO. With the success of previous work of adapting DPO into the diffusion model, another alignment optimization algorithm , KTO, has also been adapted to the this field. Based on the work of Diffusion-KTO (Li et al., 2024) and Eq. (11), we present the Diffusion TKTO as: (cid:1)z0 (cid:3)) (11) here w(c) = 1 if the prompt c0 is matched or mismatched. Consistent with Li et al. (2024), we set z0 = sg [βKL(pθ(xc)pref(xc))].In practice, we use biased but low-variance estimator for z0: LDiff-TKTO = Ex0,c,t,xtσ(w(c)β (cid:2)(ϵ ϵθ(xt, t, c)2 2 ϵ ϵref(xt, t, c)2 2 (cid:18) max 0, (cid:88) 1 β((cid:0)ϵ ϵθ(xt, t, c)2 2 ϵ ϵref(xt, t, c)2 (cid:1)) (cid:19)"
        },
        {
            "title": "4 EXPERIMENTS",
            "content": "4.1 EXPERIMENTS SETUP Datasets and Model. In this work, all experiments are conducted using Stable Diffusion v1.5 (SD v1.5) (Rombach et al., 2022b). We fine-tune SD v1.5 on the Human Preference Synthetic Dataset (HPSD) (Egan et al., 2024), which comprises one million high-quality imagecaption pairs. For evaluation, we follow previous work (Wallace et al., 2024; Li et al., 2024) and use the same HPDv2 test set (Wu et al., 2023b), Pick-a-Pic v2 test dataset (Kirstain et al., 2023) and Parti-Prompts dataset (Yu et al., 2022) for evaluation. In addition, we also include open-image-preferences-v1 (Berenstein et al., 2024) dataset for evaluation. This dataset contains prompts collect from everyday imagegeneration user requests from online platforms and have been filtered through toxicity-reduction pipeline, making it an especially robust evaluation suite. Details on each dataset are provided in the Sec. A.2. Training Setup and Baselines. Our training involves two stages, both of which fine-tune only the U-Net of SD v1.5 while freezing all other components. In the first stage, we fine-tune the pretrained SD v1.5 model on the HPSD dataset with pure SFT loss objective until we observe convergence. This stage is to close the gap between the pretrained model and the target dataset, such SFT stage is common in RLHF fine-tuning in LLM literature (Christiano et al., 2023; Ouyang et al., 2022). In Stage 2, we continue fine-tuning this SFT-adapted model under identical hyperparameters using our TDPO and TKTO methods alongside the Diffusion-DPO and Diffusion-KTO baselines. For Diffusion-KTO and Diffusion-DPO, we need to acquire preference image pairs. Specifically, with the original high-quality image being xw we need to construct xl. For details of baseline setups, please refer to Sec. 4.1 and Sec. B.5 In our experiments, we observed that direct fine-tuning with the objectives in Eq. (10) and Eq. (11) degraded sample quality after several steps. To address this, we introduce clipping mechanism inspired by Proximal Policy Optimization (Schulman et al., 2017) for more stable training. Concretely, we clamp the squared L2 norm of the negative-sample term in the loss, which bounds extreme negative signals and stabilizes training (see Sec. B.1 for details). Evaluation. In our evaluation, we generate images from each model using the same test prompts and assess alignment with five metrics: PickScore (Kirstain et al., 2023), CLIP alignment (Radford et al., 2021), HPSv2 (Wu et al., 2023b), and ImageReward (Xu et al., 2023). For each metric, we compute the win rate against the SD v1.5 baseline i.e., the fraction of prompts on which models score exceeds SD v1.5s, and report the average win rate across all five metrics. We also present the mean metric scores for each model in the Sec. B.2. All evaluations use identical sampling settings (fixed random seed, classifier-free guidance scale of 7.5, and 50 diffusion steps)."
        },
        {
            "title": "Preprint",
            "content": "Table 1: Comparison against Diffusion-DPO and Diffusion-KTO Baselines on different testsets. Win rates (%) for two sectors (each merges two datasets). Best in bold, second in underline. Our methods shaded in blue. IR = Image Reward. HPSV2 PARTYPROMPT Method PS CLIP HPS IR PS CLIP HPS IR SFT 76. 52.00 75.75 76.00 69.06 52.38 63. 73.81 77.00 Diff-DPO Ours-TDPO 83.25 Diff-KTO 80.25 Ours-TKTO 80.00 54.75 56.00 53.75 55.75 59.00 79. 76.00 81.00 70.00 82.25 76.75 80.75 71.06 74.63 73.50 73.62 54.06 57. 52.63 57.31 49.94 70.19 63.69 70.63 64.88 78.75 71.62 77.31 PICK-A-PIC OPENIMAGEPREF SFT 71.20 50.80 62.00 72. 80.60 53.60 77.00 77.60 Diff-DPO 72.80 Ours-TDPO 75.20 77.40 Diff-KTO Ours-TKTO 74. 54.20 56.00 55.20 58.60 52.40 69.20 65.60 70.80 68.60 82.60 74.60 80. 83.60 88.20 82.60 84.20 57.20 58.20 57.00 60.80 62.00 80.20 76.60 79. 73.20 86.00 80.80 82.40 4.2 RESULTS Baseline setup. In Fig. 4, we illustrate several different setups for aligning T2I models: (a) fine-tuning the model directly with desired promptimage pairs; (b) collecting human preference annotations and aligning with preference-based image pairs; (c) constructing synthetic preference pairs by altering the prompt and generating less-preferred images from the modified prompt; (d) our approach, which directly supervises the model with positive and negative prompt pairs. Figure 4: Finetune setup. Overall qualitative comparison in win-rate. Tab. 1 reports the average win rates against SD v1.5 across four datasets and five evaluation metrics. For both Diffusion-DPO and Diffusion-KTO, we adopt setup (c), where the same cl constructed by our method is used to generate xl with the supervised fintuned SD v1.5 model. We consider this fair comparison setup against our approach. Metrics are reported in win rate against sdv1.5. For instance, in Tab. 1, the value 83.25 (at the OursTDPO row and at the column of PS under the box of HPSv2) indicates that TDPO outperforms the SDv1.5 83.25% of the time on the HPSv2 dataset, as measured by PickScore. Overall, our methods consistently surpass all baselines, with the exception that Diffusion-KTO achieves slightly higher performance on few metrics. This demonstrates that our approach achieves stronger alignment under controlled and comparable evaluation setting. Additional ablations on alternative baselines shown in Fig. 4 are discussed and provided in Sec. B.5 and Tab. 4 of the appendix. Comparison with method trained on pick-a-pic preference data. Tab. 2 Here we compare against methods trained directly on Pick-a-Pic (Kirstain et al., 2023), where human preference annotations over images are available (setting from Fig. 4). In contrast, our method does not require annotated preference pairs for training; instead, we rely only on captions and the winning images provided by the dataset. We find that while our TDPO consistently outperforms Diffusion-DPO. Our TKTO variant falls short of Diffusion-KTO on HPS and IR scores. This is likely because our approach does not leverage the human-annotated preference information available in the dataset. Nevertheless, our method achieves comparable performance without using any human annotation. Qualitative Results. In Fig. 5, we show side-by-side comparison of images generated by our method and by the baselines. In the first row, we can see that only our methods, TDPO and TKTO, render both twilight and misty distinctly. Other baselines struggle to convey either concept. In the second row, all methods except vanilla SDv1.5 hint at something behind the biker, but only ours clearly shows monster in pursuit. The others look more like second rider tailing the first. In the third row, our methods and SFT successfully generate black wolf that is howling, and only our methods also embed the forest setting. The other methods either omit the trees altogether or render them too faintly. In the last row, we can see that the TKTO method precisely generates two"
        },
        {
            "title": "Preprint",
            "content": "Figure 5: Side-by-Side grid comparison of the image generation using our methods and the baselines. The leftmost column is the prompts used to generate the images. The important concept or element of the prompt that the model successfully or fails to capture is printed in orange color. Table 2: Comparison to previous methods trained on the pick-a-pick human preference dataset with Win rates (%) against SD-1.5 are reported. Our method only uses the winning image xw. Metrics with * are directly taken from Zhu et al. (2025). Method Supervision PICK-A-PIC PARTI-PROMPT PS CLIP HPS IR PS CLIP HPS IR SFT* (x, c) 70.20 61.20 84.20 76. 64.27 54.72 85.72 71.38 Diff.-DPO* Ours-TDPO Diff.-KTO* Ours-TKTO DSPO* (xw, xl, c) (x, cw, cl) (xw, xl, c) (x, cw, cl) (xw, xl, c) 71.60 70.60 71.40 74. 58.80 62.60 60.02 63.80 70.20 75.80 84.40 71.40 63.60 75.80 77.00 74. 61.18 67.12 64.80 65.25 55.45 57.06 54.34 59.63 66.48 69.88 86.16 68. 62.19 67.75 71.51 67.75 73.60 61.80 84.80 78. 65.32 54.86 87.50 71.75 red flowers against dark background with strong, directional light and deep shadows. The baselines produce the wrong number of red flowers and offer only weak chiaroscuro effects. Overall, these examples demonstrate that TKTO consistently captures each prompts detailed concepts and produces images that align more faithfully with what the user asked for. 4.3 ABLATIONS AND DISCUSSIONS Implicit preference over text prompts correlates with human preferences over images. How would learning preferences over text condition improve human preferences? Here we shed light on this matter in Fig. 6. To begin with, we define metric called implicit preference score, which shows how much model prefers x, cw over x, cl as follows: EtU ,xtq(xtx0)[ϵ ϵθ(xt, t, cl)2 2 ϵ ϵθ(xt, t, cw)2 2]. (12) The term, intuitively interpretable as the difference in diffusion loss between negative and positive pairs, quantifies how much more likely the model is to generate image given the matching prompt cw compared to the mismatched prompt cl. larger value indicates better alignment with the ground truth textual preference. The regression plots in Fig. 6 reveal clear positive correlation between human preference metrics and implicit preference score: models with higher preference"
        },
        {
            "title": "Preprint",
            "content": "Table 3: Ablation on prompt editing principles. Left: TDPO. Right: TKTO. Metrics are reported as win rates (%). TDPO TKTO Model TDPO w/o Attribute w/o Content w/o Contextual w/o Spatial PS CLIP HPS IR 74.6 74.3 75.2 73.6 76.4 57. 56.3 55.6 57.1 56.4 70.2 71.6 67.4 70.8 70.4 78.8 78.3 76.1 77.0 77.2 Model TKTO w/o Attribute w/o Content w/o Contextual w/o Spatial PS CLIP HPS IR 73. 72.4 74.9 69.2 74.4 57.3 70.6 77.3 57.4 56.1 56.1 57 70.1 69 67.6 72. 78.2 76.7 73.8 78 Figure 6: Regression plots between alignment metrics and implicit preference score. These plots shows positive correlation between alignment metrics and implicit preference score. Our method consistently achieves higher implicit preference score and higher human preference scores. score exhibit higher human preference metrics. This suggests an underlying connection between alignment with textual prompt pairs and alignment with human image preferences. Our methods, including their TDPO and TKTO variants, appear in the top-right region of the plots, indicating both the highest implicit preference score and the highest human preference scores among almost all evaluated baselines. The effectiveness of prompt editing principles. Despite the excellent performance improvement shown in Tab. 1, we are yet to answer the question: What is the benefit of the improvement brought by each editing principle? To this end, we conduct an ablation study of the effect of how each of the editing principles defined in Sec. 3.1 affect the performance of alignment. Specifically, in this ablation study, we fine-tune with four different settings, in each setting, one of the editing principles is dropped. Tab. 3 shows that dropping different modification principles leads to different performance among the metrics and methods. Interestingly, we observe that removing content-related modifications leads to significant drop in CLIP score. This is likely because content modifications most directly influence the models sensitivity to the semantic meaning of prompts. We also observe that dropping spatial-related modification leads to tradeoff, and even improvement for TKTO. The plausible cause is that spatial prompts inject high-variance, low-signal supervision: the viewpoint of the image is often underspecified, which bring ambiguity to spatial predicates like left and right, and enforcing spatial changes entangles cross-attention with object layout, which destabilizes the overall learning."
        },
        {
            "title": "5 CONCLUSION",
            "content": "We have presented novel free lunch alignment method that leverages LLM-generated textpreference pairs to fine-tune text-to-image diffusion models without requiring human preference annotations. Our instantiations, TDPO and TKTO, achieve consistent improvements over the baselines, while remaining model-agnostic and easily integrated into any RLHF-style pipeline. Future work include extending this framework by integrating other preference-optimization algorithms, applying it to other modalities such as text-to-video and text-to-3D generation, and exploring richer negative-sample generation techniques for greater diversity."
        },
        {
            "title": "6 AUTHOR CONTRIBUTIONS",
            "content": "Jia Jun Cheng Xian and Muchen Li co-led the project and contributed equally, with Anthony Cheng Xian focusing more on detailed algorithm design and experimental evaluations, while Muchen Li focused more on developing the overarching method and framework. Haotian Yang provided technical feedback. Xin Tao, Pengfei Wan, Leonid Sigal and Renjie Liao supervised the project, contributed to the research direction, and provided critical revisions. All authors discussed the results and contributed to the final manuscript."
        },
        {
            "title": "7 ACKNOWLEDGEMENT",
            "content": "This work was funded, in part, by the Vector Institute for AI, Canada CIFAR AI Chair, NSERC Canada Research Chair (CRC), NSERC Discovery Grants, and the Government of Canadas New Frontiers in Research Fund. Resources used in preparing this research were provided, in part, by the Province of Ontario, the Government of Canada through the Digital Research Alliance of Canada alliance.can.ca, and companies sponsoring the Vector Institute www.vectorinstitute. ai/#partners, and Advanced Research Computing at the University of British Columbia. Muchen Li is supported by the UBC Four Year Doctoral Fellowship. Jia Jun Cheng Xian is supported by The British Columbia Graduate Scholarships (BCGS)."
        },
        {
            "title": "REFERENCES",
            "content": "Yogesh Balaji, Seungjun Nah, Xun Huang, Arash Vahdat, Jiaming Song, Qinsheng Zhang, Karsten Kreis, Miika Aittala, Timo Aila, Samuli Laine, Bryan Catanzaro, Tero Karras, and Ming-Yu Liu. ediff-i: Text-to-image diffusion models with an ensemble of expert denoisers, 2022. David Berenstein, Ben Burtenshaw, Daniel Vila, Daniel van Strien, Sayak Paul, Ame Vi, and Linoy Tsaban. Open preference dataset for text-to-image generation, 2024. URL https://huggingface.co/blog/ image-preferences. James Betker, Gabriel Goh, Li Jing, Tim Brooks, Jianfeng Wang, Linjie Li, Long Ouyang, Juntang Zhuang, Joyce Lee, Yufei Guo, et al. Improving image generation with better captions. Computer Science. https://cdn. openai. com/papers/dall-e-3. pdf, 2(3):8, 2023. Kevin Black, Michael Janner, Yilun Du, Ilya Kostrikov, and Sergey Levine. Training diffusion models with reinforcement learning. In The Twelfth International Conference on Learning Representations, 2024. URL https://openreview.net/forum?id=YCWjhGrJFD. Ralph A. Bradley and Milton M. Terry. Rank analysis of incomplete block designs. i. the method of paired comparisons. Journal of the American Statistical Association, 48(262):495507, 1952. Huiwen Chang, Han Zhang, Jarred Barber, AJ Maschinot, Jose Lezama, Lu Jiang, Ming-Hsuan Yang, Kevin Murphy, William T. Freeman, Michael Rubinstein, Yuanzhen Li, and Dilip Krishnan. Muse: Text-to-image generation via masked generative transformers, 2023. URL https://arxiv.org/abs/2301.00704. Paul Christiano, Jan Leike, Tom B. Brown, Miljan Martic, Shane Legg, and Dario Amodei. Deep reinforcement learning from human preferences, 2023. URL https://arxiv.org/abs/1706.03741. Kevin Clark, Paul Vicol, Kevin Swersky, and David J. Fleet. Directly fine-tuning diffusion models on differentiable rewards. In The Twelfth International Conference on Learning Representations, 2024. URL https://openreview.net/forum?id=1vmSEVL19f. Ben Egan, Alex Redden, XWAVE, and SilentAntagonist. Dalle3 1 Million+ High QualURL https://huggingface.co/datasets/ProGamerGov/ ity Captions, May 2024. synthetic-dataset-1m-dalle3-high-quality-captions. Patrick Esser, Sumith Kulal, Andreas Blattmann, Rahim Entezari, Jonas Müller, Harry Saini, Yam Levi, Dominik Lorenz, Axel Sauer, Frederic Boesel, et al. Scaling rectified flow transformers for high-resolution image synthesis. In Forty-first International Conference on Machine Learning, 2024. Kawin Ethayarajh, Winnie Xu, Niklas Muennighoff, Dan Jurafsky, and Douwe Kiela. Kto: Model alignment as prospect theoretic optimization, 2024. URL https://arxiv.org/abs/2402.01306."
        },
        {
            "title": "Preprint",
            "content": "Ying Fan, Olivia Watkins, Yuqing Du, Hao Liu, Moonkyung Ryu, Craig Boutilier, Pieter Abbeel, Mohammad Ghavamzadeh, Kangwook Lee, and Kimin Lee. Reinforcement learning for fine-tuning text-to-image diffusion models. In Thirty-seventh Conference on Neural Information Processing Systems (NeurIPS) 2023. Neural Information Processing Systems Foundation, 2023. Jonathan Ho, Ajay Jain, and Pieter Abbeel. Denoising diffusion probabilistic models, 2020. URL https: //arxiv.org/abs/2006.11239. Dongzhi Jiang, Guanglu Song, Xiaoshi Wu, Renrui Zhang, Dazhong Shen, Zhuofan Zong, Yu Liu, and Hongsheng Li. Comat: Aligning text-to-image diffusion model with image-to-text concept matching. Advances in Neural Information Processing Systems, 37:7617776209, 2024. Tero Karras, Miika Aittala, Timo Aila, and Samuli Laine. Elucidating the design space of diffusion-based generative models, 2022. URL https://arxiv.org/abs/2206.00364. Shyamgopal Karthik, Huseyin Coskun, Zeynep Akata, Sergey Tulyakov, Jian Ren, and Anil Kag. Scalable ranked preference optimization for text-to-image generation, 2024. URL https://arxiv.org/abs/ 2410.18013. Diederik P. Kingma, Tim Salimans, Ben Poole, and Jonathan Ho. Variational diffusion models, 2023. URL https://arxiv.org/abs/2107.00630. Yuval Kirstain, Adam Polyak, Uriel Singer, Shahbuland Matiana, Joe Penna, and Omer Levy. Pick-a-pic: An open dataset of user preferences for text-to-image generation, 2023. URL https://arxiv.org/abs/ 2305.01569. Kimin Lee, Hao Liu, Moonkyung Ryu, Olivia Watkins, Yuqing Du, Craig Boutilier, Pieter Abbeel, Mohammad Ghavamzadeh, and Shixiang Shane Gu. Aligning text-to-image models using human feedback. arXiv preprint arXiv:2302.12192, 2023. Shiye Lei, Hao Chen, Sen Zhang, Bo Zhao, and Dacheng Tao. Image captions are natural prompts for text-toimage models. arXiv preprint arXiv:2307.08526, 2023. Shufan Li, Konstantinos Kallidromitis, Akash Gokul, Yusuke Kato, and Kazuki Kozuka. Aligning diffusion models by optimizing human utility. arXiv preprint arXiv:2404.04465, 2024. Yaron Lipman, Ricky T. Q. Chen, Heli Ben-Hamu, Maximilian Nickel, and Matt Le. Flow matching for generative modeling, 2022. Bingchen Liu, Ehsan Akhgari, Alexander Visheratin, Aleks Kamko, Linmiao Xu, Shivam Shrirao, Chase Lambert, Joao Souza, Suhail Doshi, and Daiqing Li. Playground v3: Improving text-to-image alignment with deep-fusion large language models. arXiv preprint arXiv:2409.10695, 2024a. Mushui Liu, Yuhang Ma, Xinfeng Zhang, Yang Zhen, Zeng Zhao, Zhipeng Hu, Bai Liu, and Changjie Fan. Llm4gen: Leveraging semantic representation of llms for text-to-image generation. arXiv preprint arXiv:2407.00737, 2024b. Xingchao Liu, Chengyue Gong, and Qiang Liu. Flow straight and fast: Learning to generate and transfer data with rectified flow, 2022. URL https://arxiv.org/abs/2209.03003. Bingqi Ma, Zhuofan Zong, Guanglu Song, Hongsheng Li, and Yu Liu. Exploring the role of large language In Thirty-eighth Conference on Neural Information models in prompt encoding for diffusion models. Processing Systems 2024. Neural Information Processing Systems Foundation, 2024. Yanting Miao, William Loh, Suraj Kothawade, Pascal Poupart, Abdullah Rashwan, and Yeqing Li. Subjectdriven text-to-image generation via preference-based reinforcement learning, 2024. URL https://arxiv. org/abs/2407.12164. OpenAI. DALLE 3 system card. https://openai.com/index/dall-e-3-system-card/, October 2023. Long Ouyang, Jeff Wu, Xu Jiang, Diogo Almeida, Carroll L. Wainwright, Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, John Schulman, Jacob Hilton, Fraser Kelton, Luke Miller, Maddie Simens, Amanda Askell, Peter Welinder, Paul Christiano, Jan Leike, and Ryan Lowe. Training language models to follow instructions with human feedback, 2022. URL https://arxiv.org/abs/ 2203.02155. Arka Pal, Deep Karkhanis, Samuel Dooley, Manley Roberts, Siddartha Naidu, and Colin White. Smaug: Fixing failure modes of preference optimisation with dpo-positive. arXiv preprint arXiv:2402.13228, 2024."
        },
        {
            "title": "Preprint",
            "content": "Mihir Prabhudesai, Anirudh Goyal, Deepak Pathak, and Katerina Fragkiadaki. Aligning text-to-image diffusion models with reward backpropagation, 2024. URL https://arxiv.org/abs/2310.03739. Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, Gretchen Krueger, and Ilya Sutskever. Learning transferable visual models from natural language supervision. In International Conference on Machine Learning, 2021. URL https://api.semanticscholar.org/CorpusID:231591445. Rafael Rafailov, Archit Sharma, Eric Mitchell, Christopher Manning, Stefano Ermon, and Chelsea Finn. Direct preference optimization: Your language model is secretly reward model. Advances in Neural Information Processing Systems, 36:5372853741, 2023. Yi Ren and Danica J. Sutherland. Learning dynamics of LLM finetuning. In The Thirteenth International Conference on Learning Representations, 2025. URL https://openreview.net/forum? id=tPNHOoZFl9. Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Björn Ommer. High-resolution image synthesis with latent diffusion models. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pp. 1068410695, 2022a. Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Björn Ommer. High-resolution image synthesis with latent diffusion models, 2022b. URL https://arxiv.org/abs/2112.10752. John Schulman, Filip Wolski, Prafulla Dhariwal, Alec Radford, and Oleg Klimov. Proximal policy optimization algorithms, 2017. URL https://arxiv.org/abs/1707.06347. Zhihong Shao, Peiyi Wang, Qihao Zhu, Runxin Xu, Junxiao Song, Xiao Bi, Haowei Zhang, Mingchuan Zhang, Y. K. Li, Y. Wu, and Daya Guo. Deepseekmath: Pushing the limits of mathematical reasoning in open language models, 2024. URL https://arxiv.org/abs/2402.03300. Guibao Shen, Luozhou Wang, Jiantao Lin, Wenhang Ge, Chaozhe Zhang, Xin Tao, Yuan Zhang, Pengfei Wan, Zhongyuan Wang, Guangyong Chen, et al. Sg-adapter: Enhancing text-to-image generation with scene graph guidance. arXiv preprint arXiv:2405.15321, 2024. Zhan Shi, Xu Zhou, Xipeng Qiu, and Xiaodan Zhu. Improving image captioning with better use of captions, 2020. URL https://arxiv.org/abs/2006.11807. Yang Song, Jascha Sohl-Dickstein, Diederik Kingma, Abhishek Kumar, Stefano Ermon, and Ben Poole. Score-based generative modeling through stochastic differential equations. arXiv preprint arXiv:2011.13456, 2020. Gemini Team, Rohan Anil, Sebastian Borgeaud, Jean-Baptiste Alayrac, Jiahui Yu, Radu Soricut, Johan Schalkwyk, Andrew Dai, Anja Hauth, Katie Millican, et al. Gemini: family of highly capable multimodal models. arXiv preprint arXiv:2312.11805, 2023. Amos Tversky and Daniel Kahneman. Advances in prospect theory: Cumulative representation of uncertainty. Journal of Risk and uncertainty, 5:297323, 1992. Bram Wallace, Akash Gokul, Stefano Ermon, and Nikhil Naik. End-to-end diffusion latent optimization improves classifier guidance, 2023. Bram Wallace, Meihua Dang, Rafael Rafailov, Linqi Zhou, Aaron Lou, Senthil Purushwalkam, Stefano Ermon, Caiming Xiong, Shafiq Joty, and Nikhil Naik. Diffusion model alignment using direct preference optimization. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 82288238, 2024. Xinlong Wang, Xiaosong Zhang, Zhengxiong Luo, Quan Sun, Yufeng Cui, Jinsheng Wang, Fan Zhang, Yueze Wang, Zhen Li, Qiying Yu, Yingli Zhao, Yulong Ao, Xuebin Min, Tao Li, Boya Wu, Bo Zhao, Bowen Zhang, Liangdong Wang, Guang Liu, Zheqi He, Xi Yang, Jingjing Liu, Yonghua Lin, Tiejun Huang, and Zhongyuan Wang. Emu3: Next-token prediction is all you need, 2024. URL https://arxiv.org/abs/2409. 18869. Weijia Wu, Zhuang Li, Yefei He, Mike Zheng Shou, Chunhua Shen, Lele Cheng, Yan Li, Tingting Gao, Di Zhang, and Zhongyuan Wang. Paragraph-to-image generation with information-enriched diffusion model. arXiv preprint arXiv:2311.14284, 2023a. Xiaoshi Wu, Yiming Hao, Keqiang Sun, Yixiong Chen, Feng Zhu, Rui Zhao, and Hongsheng Li. Human preference score v2: solid benchmark for evaluating human preferences of text-to-image synthesis, 2023b. URL https://arxiv.org/abs/2306.09341."
        },
        {
            "title": "Preprint",
            "content": "Yue Wu, Zhiqing Sun, Huizhuo Yuan, Kaixuan Ji, Yiming Yang, and Quanquan Gu. Self-play preference optimization for language model alignment, 2024. URL https://arxiv.org/abs/2405.00675. Jiazheng Xu, Xiao Liu, Yuchen Wu, Yuxuan Tong, Qinkai Li, Ming Ding, Jie Tang, and Yuxiao Dong. Imagereward: learning and evaluating human preferences for text-to-image generation. In Proceedings of the 37th International Conference on Neural Information Processing Systems, pp. 1590315935, 2023. Kai Yang, Jian Tao, Jiafei Lyu, Chunjiang Ge, Jiaxin Chen, Qimai Li, Weihan Shen, Xiaolong Zhu, and Xiu Li. Using human feedback to fine-tune diffusion models without any reward model, 2024. URL https://arxiv.org/abs/2311.13231. Yongjin Yang, Sihyeon Kim, Hojung Jung, Sangmin Bae, SangMook Kim, Se-Young Yun, and Kimin Lee. Automated filtering of human feedback data for aligning text-to-image diffusion models. In The Thirteenth International Conference on Learning Representations, 2025. URL https://openreview.net/forum? id=8jvVNPHtVJ. Jiahui Yu, Yuanzhong Xu, Jing Yu Koh, Thang Luong, Gunjan Baid, Zirui Wang, Vijay Vasudevan, Alexander Ku, Yinfei Yang, Burcu Karagol Ayan, Ben Hutchinson, Wei Han, Zarana Parekh, Xin Li, Han Zhang, Jason Baldridge, and Yonghui Wu. Scaling autoregressive models for content-rich text-to-image generation. Trans. Mach. Learn. Res., 2022, 2022. URL https://api.semanticscholar.org/CorpusID: 249926846. Huaisheng Zhu, Teng Xiao, and Vasant Honavar. DSPO: Direct score preference optimization for diffusion model alignment. In The Thirteenth International Conference on Learning Representations, 2025. URL https://openreview.net/forum?id=xyfb9HHvMe."
        },
        {
            "title": "A MORE EXPERIMENT DETAILS",
            "content": "A."
        },
        {
            "title": "IMPLICIT PREFERENCE SCORE",
            "content": "Recall that in Sec. 4.3 we defined the implicit preference score for triplet (x, cw, cl) as EtU ,xtq(xtx0)[ϵ ϵθ(xt, t, cl)2 2 ϵ ϵθ(xt, t, cw)2 2]. (13) In practice, we fix the diffusion timestep to = 0.5, sample xt three times for each triplet, and average the resulting diffusion losses to compute implicit preference score. All triplets are drawn from the HPSD evaluation set. The mismatched prompt cl is generated by applying single modification to the original prompt, as described in Sec. 4.3. We observe strong negative correlation between implicit preference score and human preference metrics. In other words, models that incur higher diffusion loss on the mismatched prompt cl relative to the matched prompt cw consistently receive higher human preference scores. This result aligns with our goal of improving textimage alignment and helps explain why our method achieves superior performance on human preference evaluations. A.2 DATASET Here, we list all the dataset we have used in this study, with short introduction and the usage in this sutdy. HPDv2 (Wu et al., 2023b) (Apache license 2.0) is large-scale (798k preference choices / 430k images), well-annotated dataset of human preference choices on images generated by text-toimage generative models. We have use the prompt of its test set for evaluation, the test set include 400 data. Pick-a-Pic v2 (MIT license) (Kirstain et al., 2023) is large and open dataset for human feedback in text-to-image generation. We use its test set for evaluation, which contain 500 data. Parti-prompts (Yu et al., 2022) (Apache license 2.0 license) is rich set of over 1600 prompts in English that we release as part of this work. We use the whole dataset for evaluation. open-image-preferences-v1 (Berenstein et al., 2024) (Apache license 2.0 license) is dataset contain over 7k human preference pairs on images generated by powerful text-to-image generative models. All the images pairs are generated by the same prompt, and preference binary label is provided by human annotators. We split the dataset into train and evaluation set, where the the last 500 data are split for evaluation while the rest form the training set. HPSD (Egan et al., 2024) (MIT license) dataset comprises of high-quality AI-generated images sourced from various websites and individuals, it contains over 780k image-prompt paris. For the first stage SFT, we use the whole dataset for training. For the second stage of fine-tuning, we use the first 100K. A.3 TRAINING DETAILS All experiments are run on two NVIDIA A100 GPUs using Stable Diffusion v1.5 (CreativeML Open RAIL-M license). Except for SFT fine-tuning, we use batch size of 16 and constant learning rate of 1 106. For SFT, we employ batch size of 256, learning rate of 1 105, and train for 17 500 steps until convergence on training loss has been observed. All models are optimized with AdamW (β1 = 0.9, β2 = 0.999, ϵ = 1 108) and constant learning rate. Our methods (TDPO, TKTO) and the baselines (Diffusion-DPO, Diffusion-KTO) share β = 5000. To select the best checkpoint, we sample 500 prompts from the HPSD evaluation set, generate corresponding images, and compute four evaluation metrics. We then choose the checkpoint with the highest score across these metrics for final evaluation. A.4 PROMPT MODIFICATION EXAMPLES In this subsection, we provide more examples of prompt modification such as the one in Figure 2, where given prompt is modified based on our modification principles in Fig. 7."
        },
        {
            "title": "Preprint",
            "content": "Figure 7: More Examples of prompt editing."
        },
        {
            "title": "Preprint",
            "content": "A.5 MODIFICATION PROMPT INSTRUCTION FOR LLM To generate the negative prompts cl for alignment training in our method, we have applied the Gemini 2.0 Flash model (Team et al., 2023) to modify the original prompts. Each modification is instructed to edit by using one or more editing strategies where each of these strategies following one of the modification principles described in Sec. 3. We show how we instruct the Gemini 2.0 Flash model to modify the original prompt to generate cl. The left side of Fig. 8 shows how we instruct Gemini model to modify given prompt. The right-hand side shows the choices of modification strategy, which correspond to the four modification principles. Figure 8: Left: the prompt template provide to the LLM to modify the given prompt from imageprompt pair. Right: the precise way of modification that the LLM should apply for the given prompt. A.6 T2I ALIGNMENTS STRONG CORRELATION WITH HUMAN PREFERENCE In Sec. 4.3, we demonstrated that the implicit preference score is strongly negatively correlated with all human preference score metrics. Intuitively, the implicit preference score is defined as the difference in diffusion loss between positive and negative imageprompt pairs. It quantifies how much more likely the model is to generate image given its matching prompt cw compared to the mismatched prompt cl. This loss therefore measures the models ability to capture textimage alignment: models with higher implicit preference score also produce images that receive higher human preference scores. Consequently, part of human preference can be attributed directly to better textimage alignment. To illustrate this, Figure 10 shows examples from the open-image-preferences-v1 dataset (Berenstein et al., 2024). Each pair compares two images generated from the same prompt: the left one contains the human-preferred image, and the right one the non-preferred image. Although all of these outputs are high-quality generations from state-of-the-art T2I models, annotators consistently favor the images that more faithfully reflect the prompt. For instance, in the first pair only the left image depicts the little girl Alice. In the second pair the left image shows two portals, whereas the right shows only one. In the third pair only the left image appears to float in the sky. In the fourth pair the two androids hold hands in the left image but merely stand together in the right."
        },
        {
            "title": "Preprint",
            "content": "Figure 9: Left: Alice in vibrant, dreamlike digital painting inside the Nemo Nautilus submarine. Right: An ethereal double portal with two paths illuminated by soft golden hour light, figures poised at the gateway in dynamic perspective, rendered in vibrant digital hues and sharp lines, with serene mood and rich textures Figure 10: Left: glowing white fantasy castle with towers and spires, floating in starlit sky. Right: Two androids hold hands, gazing into each others eyes, in the ruins of dystopian world, dramatic lighting. These qualitative examples, together with our quantitative ablation study of implicit preference score versus human preference scores, suggest that human judgments are closely tied to textimage alignment."
        },
        {
            "title": "B MORE EXPERIMENT RESULTS",
            "content": "B.1 CLIPPING NEGATIVE GRADIENT In our experiments, we observed that direct fine-tuning with objectives in the loss of TDPO and TKTO objectives would degrade the quality of the sample after several hundred steps. In our experiments, we found that directly optimizing towards the optimization objectives of TDPO and TKTO lead to unstable training. This is likely caused by the variance introduced by the negative gradient, phenomenon extensively analyzed in prior work (Pal et al., 2024; Ren & Sutherland, 2025). To mitigate this , we introduce clipping trick inspired by Proximal Policy Optimization (Schulman et al., 2017). Specifically, we clamp the squared L2 norm of the negative-sample term in the loss, which bounds extreme negative signals and stabilizes training. For example, in TDPO, we add clamp function to the L2 squared norm on the θ parameter term condition on cl: Ldiff-tdpo = (x0,cw ,cl)D,tU (0,T ),xtq(xtx0)[log σ( βw(t)(ϵθ(xt, cw, t) ϵw2 (clamp(ϵθ(xt, cl, t) ϵl2 ϵref(xt, cl, t) ϵl2 2)))], 2 ϵref(xt, cw, t) ϵw2 2 2, max = ϵref(xt, cl, t) ϵl 2 + λbound) (14) where λbound is hyper-parameter controls the strength of the maximum negative signal. For TKTO, this is applied similarly; the clamp function would be added when the condition prompt is not matched to the given image ( when ω(c) = 1 ). B.2 HUMAN PREFERENCE METRIC PERFORMANCE In addition to win rates, we also report the average scores of each model on each metric in radar chart Fig. 11 for each dataset. We can see that our methods cover almost all the baselines on all metric dimensions and all dataset, suggesting that our methods have higher average score for almost all the metric in all evaluation dataset. B.3 MORE QUALITATIVE COMPARISON In this subsection, we put more qualitative comparison images in Fig. 12 B.4 MORE ABLATION STUDY: MODIFICATION BUDGET For each experiment, we first choose an editing budget 1, 2, 3. Then, for each c, we perform edits: at each step we randomly select one of the four modification principles and prompt the language model to apply its corresponding editing strategy to the prompt. For both TDPO (Eq. (10)) and TKTO (Eq. (11)), we vary the editing budget {1, 2, 3}applying distinct prompt-editing strategies to generate negativesand denote variants as c1, c2, c3. Here we raise the question: what if we have multiple changes in the text prompt, that is, what is the effect of increasing the difference between the positive prompt xw and negative prompt xl. Our ablation study of the budget of prompt-editing strategies shows that our methods behave differently when we have different modification budgets. The plot in Fig. 13 reports the normalized metric scores for each of the PickScore, CLIP, HPS and ImageReward. The plot shows neither consistent improvement nor consistent degrading performance. Interestingly, certain metrics seem to benefit from having larger modification budgets; this is an interesting observation and we leave this for future work. B.5 DIFF-DPO AND DIFF-KTO BASELINES The images in our training dataset HPSD (Egan et al., 2024) are of exceptionally high quality. However, they cant be directly used by many preference alignment method. Methods such as Diffusion-DPO (Wallace et al., 2024) and Diffusion-KTO (Li et al., 2024) require human preference labels: they assume that each image pair is annotated with preferred example. In contrast, HPSD"
        },
        {
            "title": "Preprint",
            "content": "Figure 11: Radar chart of average scores of each model on each metric. The average scores are calculated by relax min-max normalization and shifting the lower bound by one standard deviation to keep all curves in view. It shows that our methods (TDPO and TKTO) outperform the baselines (Diffusion-DPO, Diffusion-KTO, SFT and SD v1.5) on all metrics by covering all the baselines. provides neither paired images nor preference annotations. To be able to compare our method with these baselines, we need to generate the second image in this dataset. We build our experiments according to (a), (c), (d) cases in Fig. 4. These cases summarize how we could finetune our model when human preference annotation or image pair is absent. (a) SFT: This is the straightforward baseline where we finetune our model on the HPSD dataset using the regular diffusion loss. We denote this baseline as SFT in Table 4 (c) Constructed Image Pair: This case cover how we are going to use Diffusion-DPO and DiffusionKTO in this setting. We conduct experiment under three different setting of how we generate the negative sample for the image pair. Diff-DPO-sd 35/Diff-KTO-sd35: refer to the case where we first construct negative prompt according to the four modification principles. Then we use the production level open-source T2I model, Stable Diffusion v3.5m to sample image according to that prompt. This sampled image generally has relatively high quality. Diff-DPOSFTsd15/Diff-KTO-SFTsd15: In this case, we also modified the prompt first and sample an image base on this modified prompt, except this time we sampled by feeding the Stage 1 SDv1.5 finetuned model checkpoint with the same HPSD prompts. This sampled image generally has lower quality than SD3.5, but should have closer distribution to the training set HPSD as the model is finetuned on it, which may align better to the default setup of Diffion-DPO and Diffuion-KTO. Diff-DPO-quality/Diff-KTO-quality: In this case, we dont construct modified prompt, we directly use the original prompt to construct our negative sample using the SFT sd15 model. This sample is negative because of the fact that they are less visually appealing than the original, high-"
        },
        {
            "title": "Preprint",
            "content": "Figure 12: Side-by-Side grid comparison of the image generation using our methods and the baselines. The left most column show the prompts used to generate the images. quality human-curated HPSD dataset, which were produced by stronger base model (DALLE 3) (OpenAI, 2023). This makes them naturally fall into the category of not preferred images. (d): Constructed Text Pair: In this setting, we dont generated the negative image sample, we only construct modified prompt, and using TDPO and TKTO objective function to finetune our model. We denote this method as Ours-TDPO/Ours-TKTO. We put all the result in Tab. 4. It shows that our method, TDPO and TKTO, consistently outperform most of these baselines, showing that our methods is more promising approach when an image pair or preference label is absent. THE USE OF LARGE LANGUAGE MODELS (LLMS) In this work, LLMs primarily serve as prompt-editing tool. Details are provided in Sec. A.5. We also used them to improve writing by refining the flow of paragraphs and correcting grammar errors. It serves as an additional tool for polishing paper writing, but is not the main writer of this work."
        },
        {
            "title": "D LIMITATION",
            "content": "Our preference-data-free alignment framework, while effective, has several limitations. First, its success hinges on the quality of prompt editing: our current strategies may overlook subtle semantic"
        },
        {
            "title": "Preprint",
            "content": "Figure 13: Normalized Metrics given different modification budgets in HPDv2 and PartiPrompts. distinctions or produce unnatural negative examples. Second, because we rely on budget-constrained off-the-shelf LLM, generated negatives can suffer from reduced fluency and faithfulness, which in turn can degrade alignment performance. Third, we fine-tune only the diffusion model while keeping the pre-trained text encoder fixed; this static encoder may limit the frameworks capacity to discriminate between closely related prompts. Finally, generating all negatives from single LLM restricts the diversity of hard negativesincorporating adversarial, retrieval-based, or multi-model sampling strategies could further improve robustness."
        },
        {
            "title": "E REPRODUCIBILITY STATEMENT",
            "content": "Detail setup of our method, model architecture, training and evaluation pipeline has been outlined in the work. We also have consistent training and evaluation setup for better reproduction. Moreover, the code for this work will be release and available soon."
        },
        {
            "title": "Preprint",
            "content": "Table 4: Comparison of Win Rates Across Datasets and Methods. The best results are highlighted in bold, and the second-best results are underlined. Baseline methods are shown in normal font, while our methods are highlighted in blue. It shows that our methods consistently outperform the baselines across all datasets for most metrics except for few entries."
        },
        {
            "title": "Method",
            "content": "PS"
        },
        {
            "title": "CLIP HPS",
            "content": "IR HPSv"
        },
        {
            "title": "PartyPrompt",
            "content": "Pick-A-Pic"
        },
        {
            "title": "OpenImagePref",
            "content": "SFT Diff-DPO-sd35 Diff-KTO-sd35 Diff-DPO-SFTsd15 Diff-KTO-SFTsd15 Diff-DPO-quality Diff-KTO-quality Ours-TDPO Ours-TKTO SFT Diff-DPO-sd35 Diff-KTO-sd35 Diff-DPO-SFTsd15 Diff-KTO-SFTsd15 Diff-DPO-quality Diff-KTO-quality Ours-TDPO Ours-TKTO SFT Diff-DPO-sd35 Diff-KTO-sd35 Diff-DPO-SFTsd15 Diff-KTO-SFTsd15 Diff-DPO-quality Diff-KTO-quality Ours-TDPO Ours-TKTO SFT Diff-DPO-sd35 Diff-KTO-sd35 Diff-DPO-SFTsd15 Diff-KTO-SFTsd15 Diff-DPO-quality Diff-KTO-quality Ours-TDPO Ours-TKTO 66.75 65.75 69.50 70.75 72.50 69.00 73.25 73.75 68.75 73.50 72.25 74.06 72.31 74.94 71.19 75.19 79.00 79. 67.40 66.20 68.80 70.00 73.00 69.80 75.20 72.6 74.20 58.80 58.80 62.40 69.80 66.60 68.00 68.20 70.60 68.40 52.00 49.00 52.25 54.75 53.75 56.00 52.75 56.00 55.75 52.38 48.69 50.94 54.06 52.63 52.25 52.19 57.25 57.31 50.80 52.2 54.00 54.20 55.20 53.80 56.20 56.00 58.20 53.60 46.60 54.60 57.20 57.00 56.40 57.40 58.20 60. 75.75 76.5 72.25 59.00 76.00 54.75 76.50 79.00 81.00 63.31 64.06 63.69 49.94 63.69 46.44 64.13 70.19 70.63 62.00 66.00 64.4 52.40 65.60 50.40 65.00 69.20 70.80 77.00 73.60 75.80 62.00 76.60 58.80 78.60 80.20 79.20 76.00 74.5 75.00 70.00 76.75 68.00 75.50 82.25 80.75 73.81 73.37 70.81 64.88 71.62 63.25 70.56 78.75 77. 72.80 76.80 74.60 68.60 74.60 68.20 74.60 82.60 80.20 77.60 77.40 77.00 73.20 80.80 73.80 78.60 86.00 82.40 76.25 79.5 77.25 77 80.25 75.75 80.75 83.25 80.00 69.06 71.94 68.06 71.06 73.50 69.88 73.31 74.63 73.62 71.20 72.40 72.20 72.80 77.40 72.20 77.20 75.20 74.40 80.60 81.40 77.80 83.60 82.60 83.60 82.60 88.20 84."
        }
    ],
    "affiliations": [
        "Canada CIFAR AI Chair",
        "Kling Team, Kuaishou Technology",
        "University of British Columbia",
        "Vector Institute for AI"
    ]
}