{
    "paper_title": "Don't Waste Mistakes: Leveraging Negative RL-Groups via Confidence Reweighting",
    "authors": [
        "Yunzhen Feng",
        "Parag Jain",
        "Anthony Hartshorn",
        "Yaqi Duan",
        "Julia Kempe"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Reinforcement learning with verifiable rewards (RLVR) has become a standard recipe for improving large language models (LLMs) on reasoning tasks, with Group Relative Policy Optimization (GRPO) widely used in practice. Yet GRPO wastes substantial compute on negative groups: groups in which no sampled response is correct yield zero advantage and thus no gradient. We ask whether negative groups can be leveraged without extra supervision. Starting from a maximum-likelihood (MLE) objective in reward modeling, we show that the MLE gradient is equivalent to a policy gradient for a modified value function. This value function adds a confidence-weighted penalty on incorrect responses, imposing larger penalties on more confident mistakes. We refer to this as \\textbf{L}ikelihood \\textbf{E}stimation with \\textbf{N}egative \\textbf{S}amples (\\textbf{LENS}). LENS modifies GRPO to assign non-zero, confidence-dependent rewards to incorrect generations, making negative groups informative and converting previously wasted samples into useful gradient updates. On the MATH benchmark with Llama-3.1-8B and Qwen-2.5-3B, the proposed variant consistently outperforms GRPO baseline, with significant gains on harder items. These results demonstrate a principled and practical way to \"rescue\" negative groups, improving efficiency and performance in RLVR."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 9 ] . [ 1 6 9 6 8 0 . 0 1 5 2 : r Dont Waste Mistakes: Leveraging Negative RL-Groups via Confidence Reweighting Yunzhen Feng2,, Parag Jain1, Anthony Hartshorn1, Yaqi Duan2,, Julia Kempe1,2, 1Meta Superintelligence Labs, 2New York University Joint advising Reinforcement learning with verifiable rewards (RLVR) has become standard recipe for improving large language models (LLMs) on reasoning tasks, with Group Relative Policy Optimization (GRPO) widely used in practice. Yet GRPO wastes substantial compute on negative groups: groups in which no sampled response is correct yield zero advantage and thus no gradient. We ask whether negative groups can be leveraged without extra supervision. Starting from maximum-likelihood (MLE) objective in reward modeling, we show that the MLE gradient is equivalent to policy gradient for modified value function. This value function adds confidence-weighted penalty on incorrect responses, imposing larger penalties on more confident mistakes. We refer to this as Likelihood Estimation with Negative Samples (LENS). LENS modifies GRPO to assign non-zero, confidence-dependent rewards to incorrect generations, making negative groups informative and converting previously wasted samples into useful gradient updates. On the MATH benchmark with Llama-3.1-8B and Qwen-2.5-3B, the proposed variant consistently outperforms GRPO baseline, with significant gains on harder items. These results demonstrate principled and practical way to rescue negative groups, improving efficiency and performance in RLVR. Date: October 13, 2025 Correspondence: Yunzhen Feng at yf2231@nyu.edu"
        },
        {
            "title": "1 Introduction",
            "content": "Large language models (LLMs) fine-tuned with reinforcement learning and verifiable rewards (RLVR) (Shao et al., 2024; Guo et al., 2025) have shown strong gains on complex reasoning tasks, with algorithms such as Group Relative Policy Optimization (GRPO) (Shao et al., 2024; Guo et al., 2025) emerging as practical defaults. persistent inefficiency, however, is how these methods handle negative groupsthe generation group in which no sampled response is correct. In GRPO and its variants, such groups contribute zero advantage and therefore no gradient signal. This is especially common at the start of training and on harder reasoning problems, where negative groups can constitute substantial fraction of compute, effectively wasting already-generated trajectories. We therefore ask: can we learn from negative groups without additional supervision in principled way? Our starting point is deliberately simple: to learn from negative groups, the natural approach is reward modeling that distinguishes correct from incorrect answers, optimized with maximum likelihood (MLE). From this likelihood perspective, the MLE gradient is equivalent to policy gradient on modified RLVR value function. The modified value adds confidence-weighted penalty for incorrect responses: the more confident the model is in wrong answer, the larger the penalty. Intuitively, it discourages overconfident failure modes, thereby encouraging exploration of lower-probability yet plausible alternatives. This equivalence lets us modify GRPO directly. It yields drop-in change in which incorrect generations receive non-zero, confidence-dependent rewards (i.e., lower rewards when confidence is higher). As result, negative groups now provide informative advantage estimates, converting previously wasted samples into useful gradient updates and promoting exploration on hard negatives. We term this algorithm LENS : Likelihood Estimation with Negative Samples. We evaluate LENS on mathematical reasoning using the MATH benchmark with Llama-3.1-8B-Instruct and 1 Figure 1 Overview of our approach. Standard approaches like GRPO assign uniform reward of 0 to all incorrect answers. This provides no learning signal, causing these samples to be discarded. Our method, LENS, is derived from reward modeling via Maximum Likelihood Estimation (MLE) and assigns non-zero, confidence-dependent rewards to incorrect responses. This creates clear learning signal where differences emerge from the samples, converting previously discarded information into useful gradient updates. Qwen-2.5-3B-Base. In both settings, our GRPO variant consistently outperforms the GRPO baseline across all Pass@k metrics. Stratifying by difficulty, we find that gains are concentrated on the Levels 4-5 subsets (hard items), consistent with repurposed negative groups driving increased exploration for hard questions. We train on two distinct math training datasets to demonstrate the generality of our method. We summarize our contributions as follows: We introduce likelihood framework, Likelihood Estimation with Negative Samples (LENS), that explicitly connects reward modeling and policy optimization. LENS yields principled value function whose additional term penalizes overconfident incorrect answers, formalizing how negative-group signals should be used and calibrated within the objective. We propose GRPO variant that assigns non-zero, confidence-dependent rewards to incorrect generations, It is plug-and-play with negligible thereby leveraging negative groups rather than wasting them. computational overhead. Empirical results support our algorithms effectiveness and show increased exploration, as reflected in Pass@k."
        },
        {
            "title": "2 Related Work",
            "content": "RLVR. Recent work has shown that reinforcement learning (RL) can effectively refine LLMs for reasoning. In RLVR, the LLM is treated as policy that generates chain-of-thought (CoT) reasoning process, and it receives deterministic reward based on whether the final answer can be algorithmically verified. Recent works (Shao et al., 2024; Guo et al., 2025; Team et al., 2025) show that RLVR can elicit emergent reasoning behaviors and dramatically boost math and coding performance compared to the base model. Underlying most of these RLVR methods is the Group Relative Policy Optimization (GRPO) algorithm (Shao et al., 2024). GRPO is an efficient variant of Proximal Policy Optimization (PPO) (Schulman et al., 2017) that drops the value network and instead computes advantages from grouped outputs. In this way, with group of all incorrect generations, the advantage is 0, and these groups do not contribute to the optimization. In this work, we try to make use of these negative groups. Learning from negatives. Recent work has emphasized that negative samples are not merely noise but useful 2 training signal in LLM reasoning. One direction explores asymmetric treatment of positives and negatives in REINFORCE-style training: Roux et al. (2025) introduce an asymmetric variant of importance sampling to speed up learning. Arnal et al. (2025) demonstrate that asymmetric REINFORCE, and in particular reducing the signal from negative samples, can be beneficial when data is off-policy. Lyu et al. (2025) propose to reweight positive and negative samples at the token level using learned reward model combined with 1 to incorrect log-likelihood. Zhu et al. (2025) demonstrate that training only on negatives, assigning reward and 0 to correct answers, can outperform baselines on Pass@k for large k. Another line of work argues that entirely wrong completions may still contain valuable sub-signals. Chen et al. (2025a) assign fractional rewards within all-negative groups, Yang et al. (2025) mine correct sub-steps from long chains of thought, and Li et al. (2024b) leverage negative rationales through dual-LoRA distillation framework. These methods demonstrate that even within incorrect trajectories, certain steps are worth reinforcing, particularly in long reasoning traces where correct and incorrect steps alternate. key drawback of these approaches is that evaluating intermediate reasoning steps is labor-intensive, and accurate automation remains underexplored. Several concurrent works also propose various approaches to address negative groups, which we summarize in Appendix A. Our contribution is to provide framework that stratifies reward signals within negative samples using only outcome rewards and probability, balancing computational efficiency with the benefits of learning from structured negatives."
        },
        {
            "title": "3 Preliminaries and Motivation",
            "content": "We start with background on policy optimization and the motivation for our method."
        },
        {
            "title": "3.1 Language Model Reasoning as Policy Optimization\nWe begin with a basic setting: given a question q\nanswer o\nwhich assigns 1 if the answer o is correct for the given question q, and 0 otherwise.\nThe ultimate goal of training the language model is to improve its accuracy rate. Formally, this corresponds\nto maximizing the expected reward:",
            "content": ", language model π is tasked with generating an , . To evaluate correctness, we assume the existence of reward function : { 0, 1 } maximizeπ J(π) : = E[r(q, o)] , where ξ, π( q) . (1) Here ξ denotes the distribution of questions. Equation (1) is the central criterion: it asks us to design policy π that maximizes the expected correctness of generated responses."
        },
        {
            "title": "3.2 Motivation: Negative Groups in RLVR\nIn practice, Group Relative Policy Optimization (GRPO) has become a default algorithm for optimizing LLM\nreasoning ability for the objective in Equation (1). Concretely, for each verifiable question q, we draw a group\nof G candidates\n, and form zero-mean,\nunit-variance group advantages",
            "content": "q), obtain scalar rewards ri : = r(q, oi) i=1 } πθold ( 0, 1 { oi { } rj { }j[G]) { With outcome-only rewards, the same advantage ri is assigned to all tokens in response oi. GRPO Ai,t = then maximizes clipped PPO-style surrogate with an explicit per-token KL regularizer to fixed reference πref : mean( rj std( }j[G]) ri = ri (cid:98) (2) . (cid:98) (cid:98) JGRPO(πθ) = Eq,{oi} 1 i=1 (cid:88) 1 oi oi t=1 (cid:104) (cid:88) min ρi,t Ai,t, clip(ρi,t, 1 (cid:0) (cid:98) ϵ, 1 + ϵ) Ai,t , (3) (cid:1) (cid:105) (cid:98) is the correction for off-policy samples. We omit the KL divergence term where ρi,t : = πθ(oi,tq,oi,<t) πθold (oi,tq,oi,<t) following the common practice as β = 0. GRPO is practical policy-gradient method for LLMs because it computes advantages from group-relative statistics rather than learned value function (critic). This makes it simple and robust for long-form reasoning, where sequences are long and rewards arrive only after complete solution. However, GRPO wastes substantial compute on negative groups. If an entire group is incorrect, i.e., all rewards } are zero, the advantages collapse to zero, yielding no contribution to the policy gradient. Figure 2 shows the fraction of all-negative groups during training with group size = 16: despite 16 generations per prompt, nearly 45% of groups are all-negative early in training, and about 35% remain even by the end. These groups consume substantial generation compute yet contribute no learning signal. ri {"
        },
        {
            "title": "4 A Likelihood-Based Framework for Rea-\nsoning",
            "content": "We now seek to find principled framework to use the negative groups. direct route is reward modeling: train model to discriminate correct from incorrect responses. We develop likelihood-based formulation of reward modeling and show how it connects to policy optimization. Figure 2 Negative group ratio during GRPO training of Llama-3.1-8B-Instruct with MATH and Numina 1.5. = 16."
        },
        {
            "title": "4.1 From Policy Learning to Reward Modeling",
            "content": "While our goal is to optimize the policy, the task becomes clearer when re-expressed through reward modeling. To illustrate this connection, we turn to simple multiple-choice example. Illustrative Example: Multiple-Choice Reasoning. Suppose single question comes with six possible answers: A, B, C, D, E, . Out of these, only and are correct. We can think of an unknown ground-truth probability function p(q, o) = Answer is correct for question . For math problems, this function is deterministic: each answer is either correct (p = 1) or incorrect (p = 0) and = r. More generally, however, could take fractional values in [0, 1] to reflect varying confidence or partial correctness. (cid:3) (cid:2) In this example, the desirable optimal policy π for Equation (1) is one that selects only from the correct options. For instance: 2 , π(C This π randomly chooses between the correct answers and B.1 This relationship can be expressed more generally as q) = π(B = π(F q) = 1 q) = 0. π(A q) = where D(q) is normalizing factor defined by p(q, o) = 1 D(q) π(o q), D(q) = p(q, o) (cid:40) oO (cid:88) (cid:41) 1 . (4) (5) 1Here we select an optimal policy that chooses uniformly at random among all correct answers. In more general settings we may have preferences over which correct answers to favor; for example, one might prefer shorter correct answers to longer ones. We extend the framework to incorporate preference function, as discussed in Appendix C. 4 Figure 3 An optimal policy π is derived from reward probabilities through normalization (see Equation (4)). This approach reframes the task of finding the best policy as more straightforward statistical problem: learning reward model from data. Intuitively, D(q) (0, 1] captures the difficulty of the question. If only one answer is correct, D(q) = 1, indicating hard question. If multiple answers are correct, D(q) becomes smaller, signaling an easier question. In practice, we do not have direct access to the full probability function p. Instead, we observe data samples of the form (q, o, r), where . Reward modeling then fits model pθ to these observations to approximate p. Through the relation in Equation (4), we can recover one optimal policy π. Therefore, policy learning reduces to the statistical task of estimating reward probabilities. Bernoulli p(q, o) (cid:1) (cid:0) Maximum Likelihood Estimation (MLE) as the Learning Principle. Formally, suppose we are given an i.i.d. dataset i=1. If we have an estimate of the difficulty D(qi) (as defined in Equation (5)), we can } reparameterize the probability model as (qi, oi, ri) = { pθ(q, o) = 1 D(q) πθ(o q), (6) where πθ belongs to parametric policy class. The straightforward way to solve pθ is through the maximum likelihood (equivalently, cross-entropy minimization) objective: minimizeθ 0(θ) = 1 ri i=1 (cid:110) (cid:88) log pθ(qi, oi) + (1 ri) log 1 (cid:0) pθ(qi, oi) . (cid:1)(cid:111) (cid:98) Plugging in the reparameterization yields the equivalent form: minimizeθ (θ) = 1 (cid:98) i=1 (cid:26) (cid:88) log πθ(oi ri qi) + (1 ri) log 1 (cid:16) πθ(oi qi) D(qi) . (cid:17)(cid:27) (7) (8) This formulation makes explicit the bridge between policy learning and reward modeling: by estimating p, we implicitly learn good policy πθ that maximizes accuracy."
        },
        {
            "title": "4.2 Calibrating Policy Gradient via MLE.",
            "content": "We now turn to the algorithmic perspective: how can the maximum likelihood objective (8) guide policy gradient methods? Our first step is to analyze the gradient of the MLE loss. This is summarized in Theorem 1. Theorem 1. The gradient of the log-likelihood (θ) with respect to the parameters θ is given by (θ) = θ 1 (cid:98) i=1 (cid:26) (cid:88) ri (1 (cid:98) ri) πθ(oi qi) πθ(oi θ log πθ(oi qi) . qi) (cid:27) (9) D(q) 5 Comparison with Policy Gradient. For reference, the standard policy gradient expression for maximizing the accuracy objective in Equation (1) is θ J(πθ) = θ log πθ(o q) . Classical algorithms such as REINFORCE, PPO, and GRPO are all built upon this form. In practice, the raw reward is often replaced by an advantage estimate to reduce variance. However, in GRPO, when all answers in batch are incorrect (i.e., = 0), the gradient contribution vanishes entirely (after centralization). This explains why negative groups are typically discarded in existing methods. (cid:2) (cid:3) MLE Perspective. Theorem 1 sheds new light on this issue. The first term of the gradient, θ log πθ(oi matches the standard policy gradient signal: positive samples (ri = 1) encourage the model to increase probability mass on correct answers. qi), ri But critically, the MLE gradient also contains an additional negative sample contribution: (1 ri) πθ(oi qi) πθ(oi D(qi) qi) θ log πθ(oi qi). Although typically smaller in scale, this term is non-negligible when only negative answers are observed, or when negative samples dominate the data. In other words, discarding negative groups overlooks legitimate part of the gradient revealed by the MLE formulation. Calibrated Policy Gradient. Motivated by this observation, we propose unified modification to REINFORCEtype algorithms for LLM reasoning. Specifically, we replace the raw reward = r(q, o) with calibrated reward that incorporates both positive and negative contributions: = r) ( πθ(o q) πθ(o D(q) . q) (10) (cid:101) When the generation is correct (r = 1), the calibrated reward is unchanged: = = 1. The adjustment applies only to incorrect samples. In negative groups, = 0 for every candidate, but the policy confidences also differ across candidates, reflecting their relative πθold (o (cid:101) confidence. This ensures that negative groups contribute informative gradients rather than being discarded, thereby yielding more statistically principled update rule. q) differ; consequently, the adjusted rewards (cid:101) We provide the proof and show that the estimator is consistent in Appendix B.1: if the model is correctly specified (i.e., π = πθ θΘ), then the true parameter vector θ is maximizer of the population } log-likelihood. { πθ"
        },
        {
            "title": "4.3 Confidence Weighted Value Function",
            "content": "After introducing the calibrated policy gradient, we can interpret it as solving modified policy optimization problem with redefined value function JMLE(πθ). The next theorem formalizes this perspective: in the on-policy setting, the MLE gradient coincides with the gradient of this specially constructed value function. The proof is deferred to Section B.2. Theorem 2. If we collect dataset (population) log-likelihood function according to qi qi), then the gradient of the (θ) is identical to the gradient of the following value function JMLE(πθ): ξ and oi πθ( maximizeθ JMLE(πθ) = J+(πθ) J(πθ) , where J+(πθ) : = Eqξ, oπθ(q) J(πθ) : = Eqξ, oπθ(q) r(q, o) , (cid:2) πθ(o (cid:104) (cid:0) q)/D(q) (cid:3) 1 (cid:1) (cid:8) r(q, o) . (cid:9)(cid:105) Here the weight function w( ) is defined as 1 w(z) : = log 1 1 1 for any 0 < 1. 6 (11) (12a) (12b) (13) This formulation provides insight into the behavior of the MLE optimizer. The objective JMLE(πθ) balances two components: J+(πθ): This is the standard policy gradient objective (REINFORCE), which maximizes the expected reward. It encourages the policy πθ to take actions (i.e., propose answers o) that are likely to be correct. r, is re-weighted by J(πθ): This term acts as penalty for incorrect answers. The cost of being incorrect, 1 , q)/D(q) which represents the policys own odds of its prediction being (cid:1) correct. The penalty is most severe when the policy is highly confident but wrong (as πθ ). Conversely, the penalty is negligible when the policy is uncertain and wrong (as πθ 0). It encourages diversity in the negative responses / exploration in the negative space. D, 0+, πθ(o (cid:0) The objective JMLE(πθ) creates powerful dynamic. It not only drives the policy to maximize rewards but, more critically, it uses the penalty term J(πθ) to enforce principled exploration. By penalizing misplaced confidence, the agent is forced to explore diverse responses rather than exploiting potentially flawed understanding. This balance between exploitation and exploration is essential for learning well-calibrated policy. Figure 4 Illustration of the weight function w(z)."
        },
        {
            "title": "5 Proposed Modification to GRPO",
            "content": "The likelihood framework naturally led to theoretically-grounded modification to GRPOs advantage function, J(πθ) objective to enhance exploration directly incorporating the insights from the JMLE(πθ) = J+(πθ) and policy calibration. The core of our proposal is to replace the original reward with our adjusted reward from Equation (10). The adjusted reward directly implements the gradient of our theoretical objective. The calibrated reward is then normalized and the obtained advantage is used in Equation (3). We do not modify (cid:101) the GRPO loss function."
        },
        {
            "title": "5.1 Implementation and Practical Considerations",
            "content": "We calibrate rewards using the ratio probability πθold πθold D(q)πθold and the difficulty factor D(q) are estimated and used. which requires careful handling, particularly in how the πθold Term. For LLMs with long generations, raw sequence probabilities are dominated by length: per-token probabilities tend to be of similar magnitude, so the sequence probability decays roughly as γo for some in directly makes the adjustment sparse: length-driven decay pushes γ most candidates terms to 0, while single dominant candidate gets much larger value. To mitigate this, we use the length-normalized (geometric-mean) probability (0, 1). Consequently, plugging πθold πθold (o q) : = πθold (o q)1/o. In Appendix we show that our likelihood framework naturally generalizes to incorporate preferences over corq) = ρ(q, B), rect generations (e.g., in the example in Section 4.1, we can make π(A rather than 0.5 and 0.5); empirically, the above substitution is equivalent to calibrated reward that encodes length preference for correct generations. q) = ρ(q, A) and π(B Estimating D(q). The true difficulty function D(q) (as defined in Equation (5)) is unknown and acts as key hyperparameter controlling learning dynamics. Smaller D(q) increases the penalty on confident but incorrect predictions, encouraging broader exploration to avoid overconfidence. This mechanism allows tuning between exploiting correct answers and exploring uncertain ones. 7 direct estimator follows from importance sampling: Dimp(q) = p(o 1 q) = Eoπθold r(q, o) 1 πθold (o q) 1 ri πθold (oi q) 1 . (14) (cid:27) (cid:26) (cid:88) oO (cid:20) In this formulation, we approximate the expectation with Monte Carlo average over group of samples (oi, ri) { For numerical stability, we should conservatively overestimate D(q) so that the denominator D(q) positive and well-conditioned. Concretely, over the candidates in the group we set drawn from πθold πθold i=1 (cid:88) i=1 (cid:27) (cid:26) is (cid:21) } . D(q) = max Dimp(q), max 1iG πθold (oi q) , (cid:16) 1, 1]. which keeps the calibrated rewards in [ Dimp(q) is undefined for negative groups as all ri are zero. In that case we fall back to (cid:17) D(q) = 2 max 1iG πθold (oi q). Handling Invariance. GRPOs group-wise normalization enjoys useful sign invariance: regardless of how many generations are correct, after normalization all incorrect generations have negative advantages and all correct generations have positive advantages. We aim to preserve this property under our calibration. Consider the 1 incorrect generations; the calibrated rewards might look like extreme mixed group with one correct and [1, 0, Calibrated Reward (per sample). In combination, our calibrated reward is 1]. To maintain sign invariance, we scale all negative calibrated rewards by 1/G. 1, . . . , ri : = ri ri) (1 1 πθold (oi q) πθold(oi D(q) , q) with D(q) = (cid:101) max Dimp(q), 2 2 (cid:16) maxj πθold (oj maxj πθold (oj q), q) , (mixed group), (cid:17) (negative group). Final Objective. In negative groups, the only signal comes from confidence differences rather than verifiable reward, so we treat it as weaker, auxiliary signal. For those groups we use de-meaning only in the normalization for simplicity, and we introduce the only hyperparameter, α, to down-weight their contribution: Jours = JGRPO[mixed groups] + α JGRPO[negative groups]."
        },
        {
            "title": "6 Experimental Results",
            "content": "We now empirically test the effectiveness of our algorithm. Set-up. We evaluate our method on mathematical reasoning. Since single RL run is affected by randomness, we conduct training with two different datasets to ensure robust results: 1) the MATH training split combined with the DAPO dataset (Yu et al., 2025a), and 2) the MATH training split combined with Numina 1.5 (Li et al., 2024a). For DAPO dataset, we perform two independent runs with different random seeds. All evaluations are on the MATH test set. We consider two models, Llama-3.1-8B-Instruct (Dubey et al., 2024) and Qwen-2.5-3B-Base (Yang et al., 2024) 2, and compare our method against the baseline GRPO. We report details and results on Numina in Appendix E. 2Following prior work, we apply RL to the Qwen base model (Liu et al., 2025b), which already follows instructions and produces outputs in the required format, whereas for Llama we use the instruction-tuned model (Arnal et al., 2025). This allows us to remove the format reward in RLVR. 8 (a) Llama-3.1-8B-Instruct (b) Qwen-2.5-3B-Base Figure 5 Comparison of our algorithm and GRPO baseline: performance on the full MATH test set and the Levels 45 (hard) subset. Top: Llama-3.1-8B-Instruct; bottom: Qwen-2.5-3B-Base. The accuracy is averaged across all 16 generations during evaluation and over two independent runs. Training set: MATH + DAPO. Our algorithm brings improvement for both models. Training protocol. To stress-test learning from negative groups, we use possibly large and sample 16 completions per question. Each gradient update uses global batch of 512 trajectories (32 questions 16 samples). We decode with temperature 1.0 and cap generations at 4,096 tokens. We do not add any KL regularization following common practices. The negative ratio α is set to 0.25 for all models. No format rewards are added to the scalar reward. Evaluation. At evaluation time, we use temperature 1.0 and top-p 1.0 to evaluate the model in the plain setup as training, and report Pass@k for . We present evaluation curves during training for both } the full MATH dataset, and the MATH Levels 4-5 subset to understand the performance on hard questions. We use Math-Verify (Kydlíček, 2025) as the verifier function for both training and evaluation. 1, 2, 4, 8, 16 { Results. We report training curves for Llama and Qwen in Figure 5. The curves are averaged across two independent runs. The full training results are in Appendix E. Across both models, LENS consistently attains higher accuracy than the GRPO baseline throughout training. On the hard split of MATH, LENS shows substantial additional gains, indicating that the method effectively converts negative groups, which often correspond to hard instances where no candidate is initially correct, into useful learning signals. As result, when the GRPO curve saturates, LENS continues to improve. These results indicate that our method learns effectively through exploration and explicitly leverages negative groups, yielding stronger performance on difficult problems. Moreover, training remains stable for >1,000 steps without ad hoc tricks or collapse. Training results using Numina set are included in Appendix E, where we observe consistent improvements with identical hyperparameters. We further report Pass@k in Table 1. Compared with the GRPO baseline, LENS achieves higher Pass@k for , with the improvement at Pass@16 also significant. These results indicate that our } algorithm consistently improves Pass@k for all k, rather than only Pass@1, and that its confidence-based design enables these exploration gains. Appendix D.2 presents ablations that separately evaluate the effect of 1, 2, 4, 8, 16 { 9 Table 1 Pass@k results on MATH with Llama-3.1-8B-Instruct and Qwen-2.5-3B-Base, averaged over two independent runs. Training set: MATH + DAPO. Model Algorithm Pass@1 Pass@2 Pass@4 Pass@8 Pass@16 Llama-3.1-8B-Instruct Qwen-2.5-3B-Base GRPO baseline LENS (Ours) GRPO baseline LENS (Ours) 54.09 56.63 67.06 68.59 61.09 63.05 72. 74.79 67.05 68.36 77.12 79.33 69.29 72.18 80. 72.70 75.34 82.67 82.62 84.44 adjusted rewards in mixed and negative groups, showing strong improvements from negative groups alone."
        },
        {
            "title": "7 Discussion",
            "content": "In this paper, we start from an observation. In GRPO, any generation group in which all samples are incorrect does not contribute to the optimization, even though these generations already consume substantial compute. We ask question: can we use this data in principled way? We develop theoretical framework that begins with reward modeling using both positive and negative data, connects it to policy optimization, and shows that the MLE objective corresponds to an adjusted value function. The adjustment adds confidence-weighted penalty for incorrect generations. This view yields calibrated reward that fits seamlessly into GRPO. Empirically, we demonstrate effectiveness on both Llama and Qwen models, with improvements across all Pass@k scores. Our empirical algorithm builds on the connection between reward modeling and policy optimization, and the framework can also incorporate preference, as shown in Appendix C. We study the simple case and leave further exploration of preference-aware variants for future work. To balance the impact of negative groups and mixed groups, we introduce single tunable hyperparameter. natural direction is to quantify the contributions of both sources in theory and design an objective that is free of hyperparameters. Our framework also covers nonbinary reward signals theoretically, and we defer systematic experimental study of this setting to future work."
        },
        {
            "title": "Acknowledgment",
            "content": "The authors would like to sincerely thank Dulhan Jayalath, Lovish Madaan, and Yuda Song for their technical guidance. An initial part of this work was completed while YF was an intern at Meta, and YF would like to thank Cheng Zhang for hosting. YF and JK acknowledge support from the Simons Foundation through the Collaborative Grant The Physics of Learning and Neural Computation and by the NSF through NRT Award 1922658. YD acknowledges support from NSF Grant DMS-2413812."
        },
        {
            "title": "References",
            "content": "Charles Arnal, Gaëtan Narozniak, Vivien Cabannes, Yunhao Tang, Julia Kempe, and Remi Munos. Asymmetric reinforce for off-policy reinforcement learning: Balancing positive and negative rewards, 2025. https://arxiv.org/ abs/2506.20520. Yuri Burda, Harrison Edwards, Amos Storkey, and Oleg Klimov. Exploration by random network distillation. arXiv preprint arXiv:1810.12894, 2018. Shicong Cen, Jincheng Mei, Katayoon Goshvadi, Hanjun Dai, Tong Yang, Sherry Yang, Dale Schuurmans, Yuejie Chi, and Bo Dai. Value-incentivized preference optimization: unified approach to online and offline rlhf. arXiv preprint arXiv:2405.19320, 2024. Peter Chen, Xiaopeng Li, Ziniu Li, Xi Chen, and Tianyi Lin. Spectral policy optimization: Coloring your incorrect reasoning in grpo. arXiv preprint arXiv:2505.11595, 2025a. Zhipeng Chen, Xiaobo Qin, Youbin Wu, Yue Ling, Qinghao Ye, Wayne Xin Zhao, and Guang Shi. Pass@ training for adaptively balancing exploration and exploitation of large reasoning models. arXiv preprint arXiv:2508.10751, 2025b. Daixuan Cheng, Shaohan Huang, Xuekai Zhu, Bo Dai, Wayne Xin Zhao, Zhenliang Zhang, and Furu Wei. Reasoning with exploration: An entropy perspective. arXiv preprint arXiv:2506.14758, 2025. Abhimanyu Dubey, Abhinav Jauhri, Abhinav Pandey, Abhishek Kadian, Ahmad Al-Dahle, Aiesha Letman, Akhil Mathur, Alan Schelten, Amy Yang, Angela Fan, et al. The llama 3 herd of models. arXiv e-prints, pages arXiv2407, 2024. Yunzhen Feng, Julia Kempe, Cheng Zhang, Parag Jain, and Anthony Hartshorn. What characterizes effective reasoning? revisiting length, review, and structure of cot. arXiv preprint arXiv:2509.19284, 2025a. Yunzhen Feng, Ariel Kwiatkowski, Kunhao Zheng, Julia Kempe, and Yaqi Duan. PILAF: Optimal human preference sampling for reward modeling. In Forty-second International Conference on Machine Learning, 2025b. Jingtong Gao, Ling Pan, Yejing Wang, Rui Zhong, Chi Lu, Qingpeng Cai, Peng Jiang, and Xiangyu Zhao. Navigate the unknown: Enhancing llm reasoning with intrinsic motivation guided exploration. arXiv preprint arXiv:2505.17621, 2025. Daya Guo, Dejian Yang, Haowei Zhang, Junxiao Song, Ruoyu Zhang, Runxin Xu, Qihao Zhu, Shirong Ma, Peiyi Wang, Xiao Bi, et al. Deepseek-r1: Incentivizing reasoning capability in llms via reinforcement learning. arXiv preprint arXiv:2501.12948, 2025. Hynek Kydlíček. Math-Verify: Math Verification Library, 2025. https://github.com/huggingface/math-verify. Thanh-Long Le, Myeongho Jeon, Kim Vu, Viet Lai, and Eunho Yang. No prompt left behind: Exploiting zero-variance prompts in llm reinforcement learning via entropy-guided advantage shaping. arXiv preprint arXiv:2509.21880, 2025. Jia Li, Edward Beeching, Lewis Tunstall, Ben Lipkin, Roman Soletskyi, Shengyi Huang, Kashif Rasul, Longhui Yu, Albert Jiang, Ziju Shen, et al. Numinamath: The largest public dataset in ai4maths with 860k pairs of competition math problems and solutions. Hugging Face repository, 13(9):9, 2024a. Pengyi Li, Matvey Skripkin, Alexander Zubrey, Andrey Kuznetsov, and Ivan Oseledets. Confidence is all you need: Few-shot rl fine-tuning of language models. arXiv preprint arXiv:2506.06395, 2025. Yiwei Li, Peiwen Yuan, Shaoxiong Feng, Boyuan Pan, Bin Sun, Xinglin Wang, Heda Wang, and Kan Li. Turning dust into gold: Distilling complex reasoning capabilities from llms by leveraging negative data. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 38, pages 1859118599, 2024b. 11 Wei Liu, Siya Qi, Xinyu Wang, Chen Qian, Yali Du, and Yulan He. Nover: Incentive training for language models via verifier-free reinforcement learning. arXiv preprint arXiv:2505.16022, 2025a. Zichen Liu, Changyu Chen, Wenjun Li, Penghui Qi, Tianyu Pang, Chao Du, Wee Sun Lee, and Min Lin. Understanding r1-zero-like training: critical perspective. arXiv preprint arXiv:2503.20783, 2025b. Chengqi Lyu, Songyang Gao, Yuzhe Gu, Wenwei Zhang, Jianfei Gao, Kuikun Liu, Ziyi Wang, Shuaibin Li, Qian Zhao, Haian Huang, et al. Exploring the limit of outcome reward for learning mathematical reasoning. arXiv preprint arXiv:2502.06781, 2025. Gongrui Nan, Siye Chen, Jing Huang, Mengyu Lu, Dexun Wang, Chunmei Xie, Weiqi Xiong, Xianzhou Zeng, Qixuan Zhou, Yadong Li, et al. Ngrpo: Negative-enhanced group relative policy optimization. arXiv preprint arXiv:2509.18851, 2025. Mihir Prabhudesai, Lili Chen, Alex Ippoliti, Katerina Fragkiadaki, Hao Liu, and Deepak Pathak. Maximizing confidence alone improves reasoning. arXiv preprint arXiv:2505.22660, 2025. Nicolas Le Roux, Marc G. Bellemare, Jonathan Lebensold, Arnaud Bergeron, Joshua Greaves, Alex Fréchette, Carolyne Pelletier, Eric Thibodeau-Laufer, Sándor Toth, and Sam Work. Tapered off-policy reinforce: Stable and efficient reinforcement learning for llms, 2025. https://arxiv.org/abs/2503.14286. John Schulman, Filip Wolski, Prafulla Dhariwal, Alec Radford, and Oleg Klimov. Proximal policy optimization algorithms. arXiv preprint arXiv:1707.06347, 2017. Zhihong Shao, Peiyi Wang, Qihao Zhu, Runxin Xu, Junxiao Song, Xiao Bi, Haowei Zhang, Mingchuan Zhang, YK Li, Yang Wu, et al. Deepseekmath: Pushing the limits of mathematical reasoning in open language models. arXiv preprint arXiv:2402.03300, 2024. Guangming Sheng, Chi Zhang, Zilingfeng Ye, Xibin Wu, Wang Zhang, Ru Zhang, Yanghua Peng, Haibin Lin, and Chuan Wu. Hybridflow: flexible and efficient rlhf framework. arXiv preprint arXiv: 2409.19256, 2024. Yunhao Tang, Kunhao Zheng, Gabriel Synnaeve, and Rémi Munos. Optimizing language models for inference time objectives using reinforcement learning. arXiv preprint arXiv:2503.19595, 2025. Kimi Team, Angang Du, Bofei Gao, Bowei Xing, Changjiu Jiang, Cheng Chen, Cheng Li, Chenjun Xiao, Chenzhuang Du, Chonghua Liao, et al. Kimi k1. 5: Scaling reinforcement learning with llms. arXiv preprint arXiv:2501.12599, 2025. Tengyang Xie, Dylan Foster, Akshay Krishnamurthy, Corby Rosset, Ahmed Awadallah, and Alexander Rakhlin. Exploratory preference optimization: Harnessing implicit q*-approximation for sample-efficient rlhf. arXiv preprint arXiv:2405.21046, 2024. Wei Xiong, Chenlu Ye, Baohao Liao, Hanze Dong, Xinxing Xu, Christof Monz, Jiang Bian, Nan Jiang, and Tong Zhang. Reinforce-ada: An adaptive sampling framework for reinforce-style llm training. arXiv preprint arXiv:2510.04996, 2025. Zhongwen Xu and Zihan Ding. Single-stream policy optimization. arXiv preprint arXiv:2509.13232, 2025. Qwen An Yang, Baosong Yang, Beichen Zhang, Binyuan Hui, Bo Zheng, Bowen Yu, Chengyuan Li, Dayiheng Liu, Fei Huang, Guanting Dong, Haoran Wei, Huan Lin, Jian Yang, Jianhong Tu, Jianwei Zhang, Jianxin Yang, Jiaxin Yang, Jingren Zhou, Junyang Lin, Kai Dang, Keming Lu, Keqin Bao, Kexin Yang, Le Yu, Mei Li, Mingfeng Xue, Pei Zhang, Qin Zhu, Rui Men, Runji Lin, Tianhao Li, Tingyu Xia, Xingzhang Ren, Xuancheng Ren, Yang Fan, Yang Su, Yi-Chao Zhang, Yunyang Wan, Yuqi Liu, Zeyu Cui, Zhenru Zhang, Zihan Qiu, Shanghaoran Quan, and Zekun Wang. Qwen2.5 technical report. ArXiv, abs/2412.15115, 2024. Zhaohui Yang, Yuxiao Ye, Shilei Jiang, Chen Hu, Linjing Li, Shihong Deng, and Daxin Jiang. Unearthing gems from stones: Policy optimization with negative sample augmentation for llm reasoning. arXiv preprint arXiv:2505.14403, 2025. Qiying Yu, Zheng Zhang, Ruofei Zhu, Yufeng Yuan, Xiaochen Zuo, Yu Yue, Weinan Dai, Tiantian Fan, Gaohong Liu, Lingjun Liu, et al. Dapo: An open-source llm reinforcement learning system at scale. arXiv preprint arXiv:2503.14476, 2025a. Tianyu Yu, Bo Ji, Shouli Wang, Shu Yao, Zefan Wang, Ganqu Cui, Lifan Yuan, Ning Ding, Yuan Yao, Zhiyuan Liu, et al. Rlpr: Extrapolating rlvr to general domains without verifiers. arXiv preprint arXiv:2506.18254, 2025b. 12 Shenao Zhang, Donghan Yu, Hiteshi Sharma, Han Zhong, Zhihan Liu, Ziyi Yang, Shuohang Wang, Hany Hassan, and Zhaoran Wang. Self-exploring language models: Active preference elicitation for online alignment. arXiv preprint arXiv:2405.19332, 2024. Xuandong Zhao, Zhewei Kang, Aosong Feng, Sergey Levine, and Dawn Song. Learning to reason without external rewards. arXiv preprint arXiv:2505.19590, 2025. Tianyu Zheng, Tianshun Xing, Qingshui Gu, Taoran Liang, Xingwei Qu, Xin Zhou, Yizhi Li, Zhoufutu Wen, Chenghua Lin, Wenhao Huang, et al. First return, entropy-eliciting explore. arXiv preprint arXiv:2507.07017, 2025. Xiangxin Zhou, Zichen Liu, Anya Sims, Haonan Wang, Tianyu Pang, Chongxuan Li, Liang Wang, Min Lin, and Chao Du. Reinforcing general reasoning without verifiers. arXiv preprint arXiv:2505.21493, 2025. Xinyu Zhu, Mengzhou Xia, Zhepei Wei, Wei-Lin Chen, Danqi Chen, and Yu Meng. The surprising effectiveness of negative reinforcement in llm reasoning. arXiv preprint arXiv:2506.01347, 2025."
        },
        {
            "title": "A Other Related Works",
            "content": "Exploration in RL. Enhancing exploration during RL training is an important part for all RL algorithms. In RLHF, Xie et al. (2024); Cen et al. (2024); Zhang et al. (2024) use the base model likelihood as an exploration bonus, nudging the policy toward outputs that are plausible yet seldom sampled. Closest in theoretical spirit to our view is Feng et al. (2025b), which studies the MLE objective of reward modeling to derive principled exploration method. In the reasoning setting, Gao et al. (2025) employ Random Network Distillation (Burda et al., 2018) to encourage novel solution traces. Other works (Cheng et al., 2025; Zheng et al., 2025) promote exploration through entropy based objectives. Finally, Chen et al. (2025b) optimize pass@k objective (Tang et al., 2025) to increase batch diversity during training. However, these approaches do not propose to differentiate rewards inside negative groups and focus mainly on mixed groups. Asymmetric treatment of positive and negative outputs. few recent work introduce asymmetric treatment of positive and negative generations during REINFORCE-style training. (Roux et al., 2025) introduces an asymmetric variant of importance sampling to speed up learning. Arnal et al. (2025) demonstrate that asymmetric REINFORCE, and in particular reducing the signal from negative generations, can be beneficial when data is off-policy. Using Confidence in RLVR. Confidence proxies have also been applied in RLVR, mainly proposed as surrogate for the rule-based verifier. Zhao et al. (2025) use the KL divergence between the per token generation probability and uniform distribution. Zhou et al. (2025); Yu et al. (2025b); Liu et al. (2025a) take the log prob of generating the reference answer conditioned on the existing CoT as the reward. Li et al. (2025) leverage confidence scores at test time for light tuning and report gains. Prabhudesai et al. (2025) similarly optimize the entropy of response tokens as the reward. In all of these studies, the rule-based reward is replaced with confidence-based proxy and light training is performed. Most works do not train beyond one hundred steps and focus only on Qwen models, which raises concerns about generalization and the risk of reward hacking without bag of tricks. In contrast, we do not aim to replace rule based rewards; instead, we propose to make use of negative groups in GRPO in principled way. We demonstrate effectiveness on both Llama and Qwen and show stable training for more than one thousand five hundred steps. Concurrent works addressing Negative Groups in GRPO Several concurrent works also propose various approaches to address negative groups. Xu and Ding (2025) leverage an on-the-fly baseline so that negative groups have non-zero baseline and the advantage is non-zero. Similarly, Nan et al. (2025) employ advantage calibration to adjust the baseline. Le et al. (2025) leverage entropy to create differentiation within negative groups. Xiong et al. (2025) propose addressing negative groups by adaptively allocating more generation samples to hard problems. Our work provides more theory-grounded motivation."
        },
        {
            "title": "B Proofs",
            "content": "B.1 Proof of Theorem 1 We now provide the proof of Theorem 1 and comment on the estimator consistency. Proof of Theorem 1. Let πθ term in the loss πθ(o (θ) with respect to θ is found using the chain rule: q) and D(q) for notational brevity. The gradient of each individual (cid:98) θ (cid:20) log πθ + (1 r) log 1 (cid:18) πθ = (cid:19)(cid:21) (cid:18) πθ 1 πθ (cid:19) θ πθ . By applying the identity for the gradient of logarithm, θ πθ = πθ θ log πθ, we can express the result as: which provides the final result. (1 r) (cid:18) πθ πθ (cid:19) θ log πθ , 14 Consistency of the Estimator. key property of this estimator is its consistency under ideal conditions. If θΘ), then the true parameter vector θ is maximizer of the the model is correctly specified (i.e., πθ population log-likelihood. This can be verified by observing that the gradient (θ) evaluates to zero at θ = θ. By taking the conditional expectation of the gradients inner term with respect to r, given and o, we find: { πθ } θ Erq,o (1 r) (cid:20) πθ (o q) πθ (o D(q) q) (cid:21) q) and the definition = π/D, this becomes: Using E[r q, o] = p(o = ( p) = ( p) 1 πθ πθ p = p = 0 . Since the conditional expectation of the term multiplying confirming that θ is stationary point. θ log πθ is zero, the full expectation is zero, B.2 Proof of Theorem 2 We will show that θ First, the target gradient from Theorem 1, with the sampling policy µ set to the model policy πθ, is: θ JMLE(πθ) is equivalent to (θ) when µ = πθ. (θ) θ µ=πθ = Eqξ, oπθ(q) (1 r) (cid:20)(cid:110) πθ πθ (cid:111) θ log πθ(o q) . (cid:21) (15) (cid:12) (cid:12) Next, we rigorously compute the gradient of J(πθ) = J+(πθ) standard: J(πθ). The gradient of the positive term is θ J+(πθ) = Eqξ, oπθ(q) θ log πθ . For the negative term, J(πθ) = Eoπθ w(πθ/D) (1 r) (cid:2) (cid:3) , we use the product rule and derive θ J(πθ) = Eq,oπθ Now we compute w(z) + w(z): (cid:2) (1 (cid:104) r) (cid:0) w(πθ/D) + (πθ/D) (cid:3) w(πθ/D) θ log πθ . w(z) + w(z) = log(1 z) 1 + z 1z + log(1 z2 (cid:19) This is exactly the term we needed. Substituting this result back into the gradient for J(πθ): (cid:18) (cid:18) (cid:19) (cid:1) = z) (cid:105) 1 = 1 1 (16) (17) . θ J(πθ) = Eq,,oπθ ( (cid:20) r) πθ πθ (cid:19) (cid:18) θ log πθ . (18) Finally, combining the gradients for the positive and negative parts of J(πθ): θ JMLE(πθ) = θ J+(πθ) θ J(πθ) = Eq,,oπθ (1 r) (cid:20)(cid:18) πθ (cid:19) θ log πθ . (19) (cid:21) This expression is identical to the MLE gradient in equation 15. The equivalence is proven. Preference-Aware Framework The framework introduced in Section 4.1 assumed that when multiple answers are correct, the optimal policy distributes probability mass uniformly across them. For example, if both and are correct answers to q) = 0.5. However, uniformity may not always reflect the true question q, we had π(A q) = π(B (cid:21) πθ q) = 0.9 and π(B reasoning process. In practice, we might prefer some answers over others. For instance, could be easier to infer, shorter in form, or more natural to express. In such cases, more realistic distribution might be π(A From the perspective of chain-of-thought reasoning, preferences can capture properties such as the length of the reasoning trajectory or the similarity of an answer to outputs from reference language model. To encode this flexibility, we introduce nonnegative preference function: q) = 0.1. which adjusts the weight assigned to each (q, o) pair. ρ(q, o) 0, Modified Framework. With the preference function, we adjust the relation between policy πθ and correctness probabilities. Specifically, we define where the difficulty factor D(q) is updated as pθ(q, o) = D(q) 1 ρ(q, o) πθ(o q), D(q) = (cid:40) oO (cid:88) p(q, o) ρ(q, o) . (cid:41) (20) (21) Intuitively, D(q) still measures how hard the question is, but it now accounts for the internal weighting across candidate answers. The maximum likelihood estimation (MLE) problem under this new framework becomes (22) (23) (cid:98) (cid:98) min θ (θ) = 1 i=1 (cid:26) (cid:88) log πθ(oi ri qi) + ( ri) log (cid:16) πθ(oi qi) ρ(qi, oi) D(qi) . (cid:17)(cid:27) The corresponding gradient of the log-likelihood is (θ) = θ 1 i=1 (cid:26) (cid:88) ri ri) (1 D(qi) πθ(oi ρ(qi, oi) qi) πθ(oi qi) (cid:27) θ log πθ(oi qi). Compared to the uniform case, the gradient now incorporates the additional signal encoded by ρ, ensuring that both positive and negative samples are scaled according to the chosen preference structure. Examples of Preference Functions. To illustrate the flexibility of this framework, we describe some concrete choices of ρ: Preference as the data collection distribution. Suppose we take ρ(q, o) = µ(o used to collect the dataset . Then the difficulty factor D(q) can be approximated by: q), where µ is the distribution D(q) 1 D(q) (cid:40) (cid:88)oOD(q) , r(q, o) (cid:41) where inverse of the empirical correctness rate. D(q) denotes the set of observed answers to question in . In words, D(q) can be estimated as the Preference as the policy itself. If we further set µ = πθ, then the negative calibration term simplifies to πθ(oi ρ(qi, oi) qi) πθ(oi qi) = 1 D(qi) . 1 D(qi) 16 In this case, the weight for negative samples is exactly the correction rate of the current policy πθ. Equivalently, in the ordinary policy gradient formulation, each question should be reweighted by its correction rate. Although this choice does not produce the confidence-based weighting we ultimately seek, it highlights that commonly used uniform weights (e.g., Arnal et al. (2025); Zhu et al. (2025)) emerge as special case of our framework. Preference as function of response length. Now, consider preference function that depends on the length of the candidate answer (Feng et al., 2025a): ρ(q, o) : = γo for fixed parameter γ (0, 1). Define the shorthand The negative-sample reward can then be expressed as πθ(o q) : = πθ(o 1 . q) rθ(o q) = D(q) πθ(o ρ(q, o) q) πθ(o q) = D(q) πθ(o γo q)o πθ(o . q)o For large (cid:101) , we have D(q) 1 1. If γ is chosen on the same scale as πθ, this weight simplifies to rθ(o q) = (cid:101) = (cid:26)(cid:16) 1 D(q) πθ(o 1 o γ q) (cid:17) πθ(o 1 1 (cid:27) q) 1 1 D(q) πθ(o πθ(o γ q) q) πθ(o γ q) (cid:26) 1 1 1 (cid:27) . Therefore, in practice, it is convenient to set negative-sample reward D(q) 1 γ πθ(o q) rθ(o q) : = 1 with γ > 0 properly tuned. (cid:101)"
        },
        {
            "title": "D Experiment Details",
            "content": "πθ(o q) πθ(o q) γ = 1 πθ(o πθ(o γ 1 q) 1 q) D.1 Hyperparameters We use learning rate 3e 6 for Qwen-2.5-3B-Base. 7 for Llama-3.1-8B-Instruct and learning rate 1e The base model requires larger learning rate while the instruct model has gone through the RLHF stages so smaller learning rate is better. Prior works (Zhu et al., 2025; Arnal et al., 2025) have used the same setup. The batch size is set to be 512, with 32 questions and 16 generations for each. We use clipping ratio of 0.2 for all the models to mitigate the impact of off-policy data. We set the maximum number of off-policy updates to 4; in VeRL (Sheng et al., 2024), this is implemented by using training batch size as 128 (432). We set temperature and top-p to 1.0 during both training and evaluation for both models. We combine the MATH training set of 7,500 samples with random subset of 17,500 samples from DAPO (Yu et al., 2025a). D.2 Ablation We also conduct an ablation to understand where the improvement comes from. In our algorithm, we modify the reward for all incorrect generations in both mixed and negative groups as in Equation 10. Compared with GRPO, we adjust rewards for incorrect generations within mixed groups, and negative groups now have nonzero advantages. To quantify the contribution of each component, we use the Llama model and consider two settings: (i) modify only the incorrect generations in mixed groups while keeping advantages for negative 17 Table 2 Ablation results of pass@k on MATH with Llama-3.1-8B-Instruct. Algorithm GRPO baseline LENS with only mixed groups LENS with only negative groups LENS (Ours) Pass@1 Pass@2 Pass@4 Pass@8 Pass@16 72.08 56.88 73.08 57.42 73.46 58.14 82.80 83.20 65.42 65. 78.34 78.80 66.48 66.08 58.64 73.98 79.79 79.46 83.40 83. groups at zero, and (ii) modify only the incorrect generations in negative groups while leaving mixed groups unchanged. This design isolates the effect of each part. We refer to these variants as LENS with only mixed groups and LENS with only negative groups. The training set is MATH and Numina 1.5. The pass@k results are reported in Table 2. The results show that both components help improve performance. Specifically, adjusting the reward in mixed groups encourages exploration in batches that already contain correct answer. This helps the model reinforce correct samples while rejecting incorrect generations. As result, LENS with only mixed groups yields its largest gains at pass@1. LENS with only negative groups also improves over GRPO and in several cases nearly matches the full LENS, suggesting that substantial portion of the improvement arises from the negative groups."
        },
        {
            "title": "E Additional Results",
            "content": "We report additional results of training with MATH + Numina 1.5 (Li et al., 2024a). These complementary results, omitted from the main paper for space, are summarized as follows. Figure 6 shows training curves for Llama and Qwen trained on MATH and Numina 1.5. Table 3 reports the Pass@ results. Due to computational constraints, we perform only one training run. On this training set, we significantly improve Pass@k for larger k, indicating greater diversity. Table 3 Pass@k results on MATH with Llama-3.1-8B-Instruct and Qwen-2.5-3B-Base. Model Algorithm Pass@1 Pass@2 Pass@4 Pass@8 Pass@16 Llama-3.1-8B-Instruct Qwen-2.5-3B-Base GRPO baseline LENS (Ours) GRPO baseline LENS (Ours) 56.88 58.64 65.88 68. 65.42 66.08 72.39 74.74 72.08 73.98 77.82 79. 78.34 79.46 82.05 83.54 82.80 83.40 85.13 86. 18 (a) Llama-3.1-8B-Instruct (b) Qwen-2.5-3B-Base Figure 6 Comparison of our algorithm and GRPO baseline: performance on the full MATH test set and the Levels 45 (hard) subset. Top: Llama-3.1-8B-Instruct; bottom: Qwen-2.5-3B-Base. The accuracy is averaged over all 16 generations during the evaluation. Our algorithm brings improvement for both models. The training set is MATH and Numina 1."
        }
    ],
    "affiliations": [
        "Meta Superintelligence Labs",
        "New York University"
    ]
}