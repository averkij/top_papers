{
    "paper_title": "TwinBrainVLA: Unleashing the Potential of Generalist VLMs for Embodied Tasks via Asymmetric Mixture-of-Transformers",
    "authors": [
        "Bin Yu",
        "Shijie Lian",
        "Xiaopeng Lin",
        "Yuliang Wei",
        "Zhaolong Shen",
        "Changti Wu",
        "Yuzhuo Miao",
        "Xinming Wang",
        "Bailing Wang",
        "Cong Huang",
        "Kai Chen"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Standard Vision-Language-Action (VLA) models typically fine-tune a monolithic Vision-Language Model (VLM) backbone explicitly for robotic control. However, this approach creates a critical tension between maintaining high-level general semantic understanding and learning low-level, fine-grained sensorimotor skills, often leading to \"catastrophic forgetting\" of the model's open-world capabilities. To resolve this conflict, we introduce TwinBrainVLA, a novel architecture that coordinates a generalist VLM retaining universal semantic understanding and a specialist VLM dedicated to embodied proprioception for joint robotic control. TwinBrainVLA synergizes a frozen \"Left Brain\", which retains robust general visual reasoning, with a trainable \"Right Brain\", specialized for embodied perception, via a novel Asymmetric Mixture-of-Transformers (AsyMoT) mechanism. This design allows the Right Brain to dynamically query semantic knowledge from the frozen Left Brain and fuse it with proprioceptive states, providing rich conditioning for a Flow-Matching Action Expert to generate precise continuous controls. Extensive experiments on SimplerEnv and RoboCasa benchmarks demonstrate that TwinBrainVLA achieves superior manipulation performance compared to state-of-the-art baselines while explicitly preserving the comprehensive visual understanding capabilities of the pre-trained VLM, offering a promising direction for building general-purpose robots that simultaneously achieve high-level semantic understanding and low-level physical dexterity."
        },
        {
            "title": "Start",
            "content": "6 2 0 2 0 2 ] . [ 1 3 3 1 4 1 . 1 0 6 2 : r 2026-01-21 Work in progress."
        },
        {
            "title": "TwinBrainVLA",
            "content": ": Unleashing the Potential of Generalist VLMs for Embodied Tasks via Asymmetric Mixture-of-Transformers Bin Yu1,2,* Shijie Lian2,4,* Xiaopeng Lin2,5,* Yuliang Wei1, Zhaolong Shen2,6 Changti Wu2,7 Yuzhuo Miao1,2 Xinming Wang2,8 Bailing Wang1 Cong Huang2,3 Kai Chen2,3,9, 1HIT 2ZGCA 3ZGCI 4HUST 5HKUST(GZ) 6BUAA 7ECNU 8CASIA 9DeepCybo (cid:97) https://github.com/ZGC-EmbodyAI/TwinBrainVLA"
        },
        {
            "title": "Abstract",
            "content": "Standard Vision-Language-Action (VLA) models typically ﬁne-tune monolithic VisionLanguage Model (VLM) backbone explicitly for robotic control. However, this approach creates critical tension between maintaining high-level general semantic understanding and learning low-level, ﬁne-grained sensorimotor skills, often leading to \"catastrophic forgetting\" of the models open-world capabilities. To resolve this conﬂict, we introduce TwinBrainVLA , novel architecture that coordinates generalist VLM retaining universal semantic understanding and specialist VLM dedicated to embodied proprioception for joint robotic control. TwinBrainVLA synergizes frozen \"Left Brain\", which retains robust general visual reasoning, with trainable \"Right Brain\", specialized for embodied perception, via novel Asymmetric Mixture-of-Transformers (AsyMoT) mechanism. This design allows the Right Brain to dynamically query semantic knowledge from the frozen Left Brain and fuse it with proprioceptive states, providing rich conditioning for FlowMatching Action Expert to generate precise continuous controls. Extensive experiments on SimplerEnv and RoboCasa benchmarks demonstrate that TwinBrainVLA achieves superior manipulation performance compared to state-of-the-art baselines while explicitly preserving the comprehensive visual understanding capabilities of the pre-trained VLM, offering promising direction for building general-purpose robots that simultaneously achieve highlevel semantic understanding and low-level physical dexterity."
        },
        {
            "title": "Introduction",
            "content": "The pursuit of embodied artiﬁcial intelligence has recently converged on the paradigm of VisionLanguage-Action (VLA) models (Kim et al., 2024; Intelligence et al., 2025; NVIDIA et al., 2025). By grafting robotic control heads onto pre-trained Vision-Language Models (VLMs), these systems inherit the rich semantic reasoning and open-world generalization capabilities of large-scale internet data. This uniﬁed approach promises robot that can not only recognize \"coffee mug\" but also understand the instruction \"place the mug on the coaster\" and execute the corresponding motor commands. However, fundamental tension lies at the heart of current VLA architectures: the misalignment between the optimization objectives of semantic pre-training and robotic ﬁne-tuning. Pre-trained VLMs are originally optimized for general semantic understanding and dialogue. In contrast, during VLA ﬁne-tuning, the backbone is repurposed to provide precise, spatially-grounded conditioning for the Action Expert to enable precise robotic control. Forcing single VLM backbone to satisfy these divergent objectives simultaneously inevitably precipitates \"catastrophic forgetting\"(Hancock et al., 2025a; Zhang et al., 2026; Zhou et al., 2025; Fang et al., 2025), where the model sacriﬁces its general linguistic capabilities to adapt to the specialized demands of robotic actuation. More critically, this catastrophic *Equal contribution Corresponding author Work done at Zhongguancun Academy (Beijing). 1 forgetting undermines the fundamental premise of the VLA paradigm, which aims to leverage the general world understanding acquired during VLM pre-training to enable generalized robotic control. However, current ﬁne-tuning methodologies paradoxically strip the VLM of this very capability. To resolve this conﬂict, we draw inspiration from the biological principle of hemispheric lateralization, where the human brain allocates distinct cognitive functions to specialized hemisphereslogic and language to the left, spatial attention and motor coordination to the right. We propose that VLA models should mimic this decoupling. By separating general semantic understanding from embodied action estimation, we cY-an achieve system that is both cognitively capable and physically skillful. In this paper, we introduce TwinBrainVLA , novel VLA framework that orchestrates two isomorphic VLM pathways via an asymmetric dual-stream joint training strategy. Structurally, our model consists of \"Left Brain\" and \"Right Brain\". The Left Brain is frozen, pre-trained VLM that retains robust open-world knowledge and instruction following capabilities. The Right Brain, initialized with the same architecture, is fully trainable and specialized for embodied perception. These two streams are seamlessly fused through Asymmetric Mixture-ofTransformers (AsyMoT) mechanism, which allows for joint attention over hidden states without sharing parameters. Crucially, the specialized representations from the Right Brain serve as the primary condition for our action expert utilizing ﬂow-matching for precise action generation. This design ensures that the action expert receives spatially rich, task-aligned embeddings from the Right Brain, while the Left Brain explicitly preserves the models general semantic understanding capabilities. Figure 1: Architectural comparison between Vanilla VLA and TwinBrainVLA. Our contributions are threefold: We propose TwinBrainVLA , the ﬁrst VLA architecture to explicitly decouple general semantic understanding and embodied perception via an asymmetric dual-stream design, resolving the training conﬂict inherent in single-backbone VLAs. We introduce the Asymmetric Mixture-of-Transformers (AsyMoT) mechanism for the information interaction of two isomorphic VLM pathways to build the VLA backbone, and employ an asymmetric parameter freezing strategy to achieve the joint training of the dual models. Extensive comparative experiments and evaluations on the SimplerEnv and RoboCasa benchmarks, demonstrates the effectiveness of the TwinBrainVLA architecture, the AsyMoT mechanism, and the proposed training strategy."
        },
        {
            "title": "2 Related Work",
            "content": "Multimodal understanding. The landscape of computer vision and natural language processing has been revolutionized by Vision Language Models (VLMs) (Liu et al., 2023; Bai et al., 2025b,a). By seamlessly integrating visual encoders with Large Language Models (LLMs) through sophisticated projection layers or adapter modules (Jiang et al., 2025a), these models exhibit emergent abilities in holistic scene understanding, visual question answering (VQA), and complex math reasoning. Given the inherent limitations of general-purpose VLMs in spatial perception (Xiao et al., 2026; Dang et al., 2025; Zhang et al., 2025b; Chen et al., 2025; Yang et al., 2025c), recent studies have increasingly focused on employing post-training strategies to enhance spatial intelligence (Yang et al., 2025a; Tan et al., 2025; Zhou et al., 2026; Zhu et al., 2025; Li et al., 2025) and construct specialized embodied foundation models tailored for embodied scenarios (Team et al., 2025a; Yang et al., 2025a; Hao et al., 2025; Lin et al., 2025c). 2 Vision-Language-Action (VLA) Models. Building upon the zero-shot generalization and rich semantic priors of VLMs, VLA models have emerged as scalable paradigm for embodied intelligence (Pertsch et al., 2025; Zheng et al., 2025a; Wen et al., 2025; Shukor et al., 2025; Zheng et al., 2025b; Lin et al., 2025b; Cai et al., 2026). By ﬁne-tuning pre-trained VLMs on large-scale robotic manipulation datasets, these models learn to map multimodal instructions and observations to low-level control signals (Jiang et al., 2025b; Fan et al., 2025; Zhai et al., 2025). To mitigate the degradation of general conversational skills during VLA training, recent approaches have integrated general visual-dialogue datasets (Mazzaglia et al., 2025; Yang et al., 2025b; Hancock et al., 2025b; Zhou et al., 2025) and reasoning mechanisms into the VLA framework (Zawalski et al., 2025; Yin et al., 2025; Lin et al., 2025a; Qu et al., 2025a; Ye et al., 2025; Huang et al., 2025; Guo et al., 2025; Lee et al., 2025; Huang et al., 2026)."
        },
        {
            "title": "3 Motivation: Catastrophic Forgetting in VLA Models",
            "content": "Standard Vision-Language-Action (VLA) models typically adopt transfer learning paradigm. Formally, let pre-trained Vision-Language Model (VLM) be parameterized by θpre, which has been aligned on massive internet-scale image-text pairs to maximize the likelihood of generating semantic text responses given an image and instruction x. To adapt this backbone for robotic manipulation, the model is ﬁne-tuned on robotic demonstration dataset Drobot = {(v, x, a)i}N i=1, where represents the robots action (e.g., end-effector pose or joint angles). The training objective shifts to minimizing the action prediction loss Laction: θ = arg min θ (v,x,a)Drobot log (av, x; θ), (1) initialized with θ = θpre. While this process effectively imparts manipulation skillsenabling the model to control servos and execute tasksit introduces critical side effect known as catastrophic forgetting. The distribution of Drobot differs fundamentally from the diverse visual-linguistic data seen during pre-training (Kachaev et al., 2025; Hancock et al., 2025b; Zhang et al., 2025a). As the parameters θ are updated to minimize the high-frequency spatial error in action space, the delicate semantic alignment established in θpre is often overwritten. Consequently, the resulting VLA model suffers from severe degradation in its original capabilities: it loses the ability to follow complex open-ended instructions, fails to describe visual scenes accurately, and exhibits weakened general semantic reasoning. This phenomenon turns the VLM backbone from \"generalist\" into \"specialist\", sacriﬁcing its linguistic brain to gain robotic body."
        },
        {
            "title": "4 Method: TwinBrainVLA",
            "content": "In this section, we present TwinBrainVLA , uniﬁed VLA framework designed to disentangle high-level semantic reasoning from ﬁne-grained sensorimotor control. As illustrated in Figure 2, our architecture operates on an asymmetric dual-stream paradigm to resolve the \"catastrophic forgetting\" dilemma. The framework comprises three core components: (1) frozen \"Left Brain\" that preserves open-world visual-linguistic knowledge; (2) trainable \"Right Brain\" that specializes in embodied motor control; and (3) Flow-Matching Action Expert that generates precise continuous actions. The two brains interact via novel Asymmetric Mixture-of-Transformers (AsyMoT) mechanism, enabling the specialist to leverage generalist semantics without corrupting them. We detail the Dual-VLM architecture and AsyMoT fusion strategy in Sec. 4.1, describe the ﬂow-matching policy formulation in Sec. 4.2, and ﬁnally present our joint-training objective in Sec. 4.3."
        },
        {
            "title": "4.1 Asymmetric Dual-VLM as Backbone",
            "content": "The backbone of TwinBrainVLA consists of two parallel Vision-Language Models (VLMs), denoted as the \"Left Brain\" (ML) and the \"Right Brain\" (MR). Both streams are initialized with identical pretrained weights (e.g., Qwen3-VL series (Bai et al., 2025a)) but serve distinct roles during training. Dual-Stream Inputs and State Encoding. The input to the system comprises visual observation I, natural language instruction T, and the robots proprioceptive state Rds (e.g., joint angles and 3 Figure 2: The framework of TwinBrainVLA . (a) Overall Architecture. The model features an Asymmetric Mixture-of-Transformers design composed of two distinct pathways: frozen \"Left Brain\" (Generalist) for semantic reasoning and trainable \"Right Brain\" (Specialist) for embodied motor control. The Right Brain fuses visual, textual, and proprioceptive state inputs to provide conditioning for the Action Expert, which utilizes Flow-Matching algorithm to denoise continuous robotic actions. (b) Asymmetric MoT Mechanism (AsyMoT). Through causal self-attention, the trainable Right Brain attends to the frozen Key-Value (KV) pairs of the Left Brain, enabling the transfer of general semantic knowledge to the robotic control policy without catastrophic forgetting. end-effector pose). The Left Brain functions as pure semantic generalist. It receives only the vision and language tokens: H0 = [V(I); (T)], (2) where and represent the vision encoder and text tokenizer, respectively. In contrast, the Right Brain is responsible for embodied control and must be grounded in the robots physical state. To achieve this, we introduce lightweight State Encoder ϕ, modeled as Multi-Layer Perceptron (MLP), which projects the low-level proprioceptive state into the VLMs embedding space. These state tokens are appended to the multimodal input sequence: H0 = [V(I); (T); ϕ(s)]. (3) This design ensures that the Right Brain explicitly attends to the robots current conﬁguration, critical requirement for closed-loop control. Asymmetric Mixture-of-Transformers (AsyMoT). The core innovation of our backbone lies in how these two streams interact. We propose the AsyMoT mechanism to fuse general semantic knowledge into the robotic control stream. Let Hl denote the hidden states of the Left and Right Brains at layer l, respectively. and Hl The Left Brain remains frozen during training. Its self-attention mechanism operates independently to preserve pre-trained general capabilities: Hl+1 = Attn(Ql L, L, L) + FFN(Hl L), (4) where Q, K, are derived from Hl using frozen projection weights. The Right Brain is trainable and employs an Asymmetric Joint Attention mechanism. It queries not only its own context but also the semantic features from the Left Brain. Speciﬁcally, at each attention head, the Right Brain computes its Query (QR), while the Key (K) and Value (V ) are constructed by concatenating the representations from both brains: Kjoint = [sg(Kl Vjoint = [sg(V Hl+1 = Softmax L) ; Kl R], L) ; R], ( Ql R(Kjoint)T dk ) Vjoint + FFN(Hl R), (5) (6) (7) 4 where [ ; ] denotes concatenation along the sequence length dimension, and sg() indicates the stopgradient operation. This asymmetric ﬂow ensures strict hierarchy: the Left Brain acts as stable \"semantic anchor\" providing high-level reasoning features, while the Right Brain dynamically fuses these semantics with ﬁne-grained proprioceptive cues to reason about spatial actions. The ﬁnal hidden states of the Right Brain, Hf inal , are then passed to the Action Expert. R"
        },
        {
            "title": "4.2 Flow-Matching Action Expert",
            "content": "To enable high-precision, continuous control, we move beyond the discrete tokenization paradigm (Kim et al., 2024) and adopt the state-of-the-art generative policy architecture utilized in recent foundation models like Isaac-GR00T (NVIDIA et al., 2025) and π0 (Intelligence et al., 2025). Speciﬁcally, our Action Expert employs the Diffusion Transformer (DiT) architecture (Peebles and Xie, 2023) and is trained via ﬂow matching. Architecture and Conditioning. Following the design in Isaac-GR00T, the Action Expert operates as conditional decoder that denoises action trajectories. It takes noisy action sequence at and the current timestep as input. While the policy architecture aligns with established baselines, key distinction in our framework lies in the source of the condition. We inject the spatially-rich representations from our trainable Right Brain, HR, into the DiT via cross-attention layers. This ensures that the proven control capabilities of the DiT are guided explicitly by the embodied perception features extracted by our asymmetric backbone. Flow-Matching Formulation. We utilize the ﬂow-matching objective to train the policy, as demonstrated effectively in prior work (NVIDIA et al., 2025). The process models conditional probability path ﬂowing from standard Gaussian prior a0 (0, I) to the ground-truth action distribution a1. The vector ﬁeld regresson loss is deﬁned as: LFM(ψ) = Et,a0,a1 [ vψ(at, t, HR) (a1 a0)2 ] , (8) where vψ is the DiT network, [0, 1], and the target vector ﬁeld is the straight line a1 a0. During inference, actions are synthesized by solving the Ordinary Differential Equation (ODE) using an Euler solver, translating the Right Brains understanding into smooth robotic movements."
        },
        {
            "title": "4.3 Training Strategy",
            "content": "Optimization Objective. Consistent with standard VLA ﬁne-tuning paradigms (NVIDIA et al., 2025), we train TwinBrainVLA using exclusively the robotic action objective. We do not incorporate auxiliary Next-Token Prediction (NTP) losses or mix in general vision-language datasets during this stage. The training objective is to minimize the Flow-Matching loss: Ltotal = LFM(θR, ψ, ϕ; Drobot), (9) where Drobot represents the robotic demonstration dataset, and {θR, ψ, ϕ} denote the trainable parameters of the Right Brain, Action Expert, and State Encoder, respectively. It is worth noting that in monolithic VLA architectures, relying solely on action loss typically leads to severe catastrophic forgetting of general semantic capabilities. However, our dual-stream design structurally immunizes the model against this degradation: the \"Right Brain\" is free to specialize entirely in control dynamics, while the frozen \"Left Brain\" implicitly safeguards the linguistic and semantic priors. Asymmetric Update Rule. We enforce strict parameter update policy. Let θL be the parameters of the Left Brain. During backpropagation, we set θL = 0. Gradients originating from the Action Expert ﬂow backwards through the DiT (ψ), permeate the Right Brain (θR) via the conditioning signals, and update the State Encoder (ϕ). Crucially, at the AsyMoT fusion layer, the gradient ﬂow is explicitly blocked (stop-gradient) at the Key-Value interface from the Left Brain. This ensures that the Left Brain acts as stable semantic anchor, providing consistent features KL, VL without its weights being perturbed by the high-variance gradients typical of robotic control tasks."
        },
        {
            "title": "5 Experiment",
            "content": "To comprehensively evaluate the efﬁcacy of TwinBrainVLA, we conduct extensive experiments on two simulation benchmarks: SimplerEnv (Li et al., 2024c) and RoboCasa (Nasiriany et al., 2024). Our training pipeline is built upon the starVLA framework (starVLA, 2025), distributed across 16 NVIDIA H100 GPUs. We strictly follow its default training protocols to ensure fair comparison. More extensive simulation benchmarks and real-world robotic experiments are in progress."
        },
        {
            "title": "5.1 Experiments on SimplerEnv",
            "content": "Implementation and Training Setup. To demonstrate the scalability and effectiveness of our architecture, we instantiate TwinBrainVLA with two state-of-the-art VLM backbones: Qwen2.5-VL-3B-Instruct and Qwen3-VL-4B-Instruct. Consistent with our asymmetric design, both the frozen Left Brain and the trainable Right Brain are initialized from these pre-trained checkpoints to ensure aligned feature spaces. For the training data, we utilize two large-scale subsets from the Open X-Embodiment (OXE) dataset: the Bridge-V2 and the Fractal dataset. We are actively working on scaling our training to the complete OXE dataset, and future experimental results will be reported in subsequent updates. Comprehensive implementation details are provided in Appendix A. Evaluation. The benchmark consists of four manipulation tasks: \"put spoon on towel\", \"put carrot on plate\", \"stack green block on yellow block\", \"put eggplant in the yellow basket\". For each task, we evaluate our VLA policy using the evaluation script provided by the SimplerEnv repository (Li et al., 2024c). To mitigate randomness, we run 480 independent trials and report the mean performance (Avg@480). Baselines. We benchmark TwinBrainVLA against comprehensive suite of state-of-the-art VLA policies, including RT-1-X (ONeill et al., 2024), Octo (Team et al., 2024), OpenVLA (Kim et al., 2024), RoboVLM (Li et al., 2024b), TraceVLA (Zheng et al., 2025c), SpatialVLA (Qu et al., 2025b), CogACT (Li et al., 2024a), VideoVLA (Shen et al., 2025), π0 (Black et al., 2024), and π0.5 (Intelligence et al., 2025). All baseline results are obtained from their original papers or ofﬁcial reproductions to ensure fair and reliable comparison. Table 1: Results of evaluating the VLA models with the WidowX robot in the SimplerEnv simulation environment. We highlight the best results in bold and the second-best results with underline. Method Put Spoon on Towel Put Carrot on Plate Stack Green Block on Yellow Block Put Eggplant in Yellow Basket Average RT-1-X (ONeill et al., 2024) Octo-Base (Team et al., 2024) Octo-Small (Team et al., 2024) OpenVLA (Kim et al., 2024) OpenVLA-OFT (Kim et al., 2025) RoboVLM (Li et al., 2024b) TraceVLA (Zheng et al., 2025c) SpatialVLA (Qu et al., 2025b) CogACT (Li et al., 2024a) VideoVLA (Shen et al., 2025) π0 (Black et al., 2024) π0.5 (Intelligence et al., 2025) QwenGR00T + Qwen3-VL-3B (starVLA, 2025) QwenGR00T + Qwen3-VL-8B (starVLA, 2025) QwenGR00T + Qwen2.5-VL-7B (starVLA, 2025) Isaac-GR00T-N1.6-Bridge (Team et al., 2025b) TwinBrainVLA + Qwen2.5-VL-3B-Instruct TwinBrainVLA + Qwen3-VL-4B-Instruct 0.0 15.8 41.7 4.2 12.5 50.0 12.5 20.8 71.7 75.0 29.1 49.3 87.5 70.8 59.2 64.5 83.3 73. 4.2 12.5 8.2 0.0 4.2 37.5 16.6 20.8 50.8 20.8 0.0 64.7 50.0 38.9 30.8 65.5 41.7 58.3 0.0 0.0 0.0 0.0 4.2 0.0 16.6 25.0 15.0 45.8 16.6 44.7 29.2 22.2 3.3 5.5 31.5 33.3 0.0 41.7 56.7 12.5 72.5 83.3 65.0 70.8 67.5 70.8 62.5 69.7 54.2 65.3 44.2 93.0 77.1 83. 1.1 17.5 26.7 4.2 23.4 42.7 27.7 34.4 51.3 53.1 27.1 57.1 55.2 49.3 34.4 57.1 58.4 62.0 Results. The quantitative results on the SimplerEnv benchmark are presented in Table 1. Notably, despite not undergoing large-scale pre-training for robotic action prediction, TwinBrainVLA achieves state-of-the-art performance among all listed methods. Our framework demonstrates strong generalizability across different VLM families, attaining competitive success rates of 58.4% with Qwen2.5-VL6 3B-Instruct and 62.0% with Qwen3-VL-4B-Instruct. The latter conﬁguration surpasses the strongest baseline, Isaac-GR00T-N1.6 (57.1%), by notable margin of +4.9%, validating the effectiveness of our asymmetric dual-brain architecture in bridging high-level semantic understanding and low-level robotic control."
        },
        {
            "title": "5.2 Experiments on RoboCasa",
            "content": "Implementation. We train TwinBrainVLA on the Humanoid Robot Tabletop Manipulation subset from the PhysicalAI-Robotics-GR00T-X-Embodiment-Sim dataset (NVIDIA et al., 2025). All other experimental settings, including model initialization, distributed training infrastructure, and hyperparameters, remain identical to those described in Sec. 5.1. Evaluation. We evaluate our policy on the RoboCasa GR1 Tabletop Benchmark, which comprises diverse suite of 24 tabletop manipulation tasks. These tasks involve complex interactions with articulated objects and varying geometries, such as PnPBottleToCabinetClose (placing bottle into cabinet and closing the door), PnPCanToDrawerClose, and various microwave/toaster interaction scenarios. To ensure statistical reliability while maintaining consistency with our previous experiments, we report the success rate averaged over multiple independent trials (Avg@50) for each of the 24 tasks. Table 2: Results of evaluating the VLA models with the GR1 robot in the RoboCasa Tabletop simulation environment. We highlight the best results in bold and the second-best results with underline. Task Isaac-GR00T N1.6 QwenGR00T + Qwen3VL QwenPI + Qwen3VL TwinBrainVLA + Qwen2.5VL TwinBrainVLA + Qwen3VL PnP Bottle To Cabinet Close PnP Can To Drawer Close PnP Cup To Drawer Close PnP Milk To Microwave Close PnP Potato To Microwave Close PnP Wine To Cabinet Close PnP Novel From Cuttingboard To Basket PnP Novel From Cuttingboard To Cardboardbox PnP Novel From Cuttingboard To Pan PnP Novel From Cuttingboard To Pot PnP Novel From Cuttingboard To Tieredbasket PnP Novel From Placemat To Basket PnP Novel From Placemat To Bowl PnP Novel From Placemat To Plate PnP Novel From Placemat To Tieredshelf PnP Novel From Plate To Bowl PnP Novel From Plate To Cardboardbox PnP Novel From Plate To Pan PnP Novel From Plate To Plate PnP Novel From Tray To Cardboardbox PnP Novel From Tray To Plate PnP Novel From Tray To Pot PnP Novel From Tray To Tieredbasket PnP Novel From Tray To Tieredshelf Average 51.5 13.0 8.5 14.0 41.5 16.5 58.0 46.5 68.5 65.0 46.5 58.5 57.5 63.0 28.5 57.0 43.5 51.0 78.7 51.5 71.0 64.5 57.0 31.5 47. 46.0 80.0 54.0 48.0 28.0 46.0 48.0 40.0 68.0 52.0 56.0 42.0 44.0 48.0 18.0 60.0 50.0 54.0 70.0 38.0 56.0 50.0 36.0 16.0 47.8 26.0 62.0 42.0 50.0 42.0 32.0 40.0 46.0 60.0 40.0 44.0 44.0 52.0 50.0 28.0 52.0 40.0 36.0 48.0 34.0 64.0 44.0 50.0 28.0 43.9 62.0 66.0 46.0 52.0 56.0 58.0 50.0 44.0 64.0 60.0 52.0 56.0 59.0 62.0 28.0 62.0 52.0 56.0 68.0 40.0 56.0 60.0 42.0 34.0 53. 74.0 72.0 52.0 60.0 36.0 46.0 62.0 46.0 70.0 66.0 52.0 30.0 54.0 64.0 38.0 60.0 58.0 56.0 66.0 46.0 72.0 56.0 46.0 28.0 54.6 Baselines. We compare TwinBrainVLA against several competitive baselines trained on the same dataset. Speciﬁcally, we include Isaac-GR00T-N1.6 (Team et al., 2025b), large-scale VLA model that demonstrates strong performance on humanoid manipulation tasks. Additionally, to directly assess the advantages of our dual-brain architecture, we benchmark against two representative models from the starVLA framework (starVLA, 2025): QwenGR00T and QwenPI, both instantiated with Qwen3-VL7 4B-Instruct backbones. These baselines follow similar training protocols and utilize the same dataset, enabling fair comparison and highlighting the speciﬁc contributions of our asymmetric dual-VLM design. Results. The quantitative results on the RoboCasa GR1 Tabletop Benchmark are presented in Table 2. TwinBrainVLA achieves the best performance across all 24 manipulation tasks, with our Qwen3-VL4B-Instruct variant attaining an average success rate of 54.6%, followed closely by the Qwen2.5-VL3B-Instruct variant at 53.5%. These results substantially outperform all baseline methods: our best model surpasses Isaac-GR00T-N1.6 (47.6%) by +7.0%, QwenGR00T (47.8%) by +6.8%, and QwenPI (43.9%) by +10.7%. These results reinforce our hypothesis that decoupling semantic understanding from embodied perception enables more effective learning of ﬁne-grained manipulation skills in complex tabletop scenarios."
        },
        {
            "title": "6 Conclusion",
            "content": "In this paper, we introduced TwinBrainVLA , novel VLA framework that addresses the fundamental tension between semantic generalization and embodied specialization in robotic learning. By drawing inspiration from hemispheric lateralization, we proposed an asymmetric dual-stream architecture that structurally decouples the visual cognitive process. Our AsyMoT mechanism effectively synergizes frozen \"Left Brain\" for robust open-world reasoning with trainable \"Right Brain\" for precise sensorimotor control, enabling the system to master complex manipulation skills without \"catastrophic forgetting\" of its general visual capabilities."
        },
        {
            "title": "Limitation and Future Work",
            "content": "While TwinBrainVLA provides powerful framework for decoupling semantic and embodied processing, several limitations remain that outline our future research directions. more decoupled model architecture. First, the current implementation of the Asymmetric Mixtureof-Transformers (AsyMoT) mechanism requires the Left and Right Brains to share an identical model architecture. This structural constraint ensures compatible hidden state dimensions for seamless joint attention but limits the ﬂexibility to pair heterogeneous modelsfor instance, synergizing massive, reasoningheavy VLM with lightweight, high-frequency control model. We are actively exploring generalized fusion mechanisms, such as learnable projection layers or cross-attention adapters, to support backbones of varying sizes and architectures. More diverse model checkpoints. Second, both streams in our current setup are initialized from standard general-purpose VLM checkpoints (e.g., Qwen-VL). However, the ﬁeld has arguably seen surge in specialized Embodied VLMs trained speciﬁcally for robotics. Integrating these specialized models into our dual-brain paradigmpotentially initializing the \"Right Brain\" with an embodied checkpoint while retaining generalist \"Left Brain\"presents promising avenue. We are currently investigating strategies to effectively adapt our framework to leverage these domain-speciﬁc post-trained weights. Training on larger-scale data. Third, regarding data scale, our current training utilizes subsets of the Open X-Embodiment dataset. We acknowledge that the capacity of our dual-stream architecture may not be fully realized with limited data. We are in the process of scaling our pre-training to the complete dataset to further enhance the models robustness and downstream performance. More benchmarks and real-robot experiment. Finally, future work will also focus on extending our evaluation to broader range of benchmarks, including real-robot scenarios, to comprehensively assess the versatility of TwinBrainVLA."
        },
        {
            "title": "References",
            "content": "Shuai Bai, Yuxuan Cai, Ruizhe Chen, Keqin Chen, Xionghui Chen, Zesen Cheng, Lianghao Deng, Wei Ding, Chang Gao, Chunjiang Ge, Wenbin Ge, Zhifang Guo, Qidong Huang, Jie Huang, Fei Huang, Binyuan Hui, Shutong Jiang, Zhaohai Li, Mingsheng Li, and 45 others. 2025a. Qwen3-vl technical report. 8 Shuai Bai, Keqin Chen, Xuejing Liu, Jialin Wang, Wenbin Ge, Sibo Song, Kai Dang, Peng Wang, Shijie Wang, Jun Tang, Humen Zhong, Yuanzhi Zhu, Mingkun Yang, Zhaohai Li, Jianqiang Wan, Pengfei Wang, Wei Ding, Zheren Fu, Yiheng Xu, and 8 others. 2025b. Qwen2.5-vl technical report. Kevin Black, Noah Brown, Danny Driess, Adnan Esmail, Michael Equi, Chelsea Finn, Niccolo Fusai, Lachy Groom, Karol Hausman, Brian Ichter, and 1 others. 2024. π0: vision-language-action ﬂow model for general robot control. arXiv preprint arXiv:2410.24164. Junhao Cai, Zetao Cai, Jiafei Cao, Yilun Chen, Zeyu He, Lei Jiang, Hang Li, Hengjie Li, Yang Li, Yufei Liu, Yanan Lu, Qi Lv, Haoxiang Ma, Jiangmiao Pang, Yu Qiao, Zherui Qiu, Yanqing Shen, Xu Shi, Yang Tian, Internvla-a1: Unifying understanding, generation and action for robotic manipulation. and 23 others. 2026. Preprint, arXiv:2601.02456. Shiqi Chen, Tongyao Zhu, Ruochen Zhou, Jinghan Zhang, Siyang Gao, Juan Carlos Niebles, Mor Geva, Junxian He, Jiajun Wu, and Manling Li. 2025. Why is spatial reasoning hard for VLMs? an attention mechanism perspective on focus areas. In International conference on machine learning (ICML). Ronghao Dang, Yuqian Yuan, Yunxuan Mao, Kehan Li, Jiangpin Liu, Zhikai Wang, Xin Li, Fan Wang, and Deli Zhao. 2025. Rynnec: Bringing mllms into embodied world. Preprint, arXiv:2508.14160. Shichao Fan, Kun Wu, Zhengping Che, Xinhua Wang, Di Wu, Fei Liao, Ning Liu, Yixue Zhang, Zhen Zhao, Zhiyuan Xu, Meng Li, Qingjie Liu, Shanghang Zhang, Min Wan, and Jian Tang. 2025. Xr-1: Towards versatile vision-language-action models via learning uniﬁed vision-motion representations. Preprint, arXiv:2511.02776. Zhen Fang, Zhuoyang Liu, Jiaming Liu, Hao Chen, Yu Zeng, Shiting Huang, Zehui Chen, Lin Chen, Shanghang Zhang, and Feng Zhao. 2025. Dualvla: Building generalizable embodied agent via partial decoupling of reasoning and action. Preprint, arXiv:2511.22134. Wenkai Guo, Guanxing Lu, Haoyuan Deng, Zhenyu Wu, Yansong Tang, and Ziwei Wang. 2025. Vla-reasoner: Empowering vision-language-action models with reasoning via online monte carlo tree search. Preprint, arXiv:2509.22643. Asher J. Hancock, Xindi Wu, Lihan Zha, Olga Russakovsky, and Anirudha Majumdar. 2025a. Actions as language: Fine-tuning vlms into vlas without catastrophic forgetting. Preprint, arXiv:2509.22195. Asher J. Hancock, Xindi Wu, Lihan Zha, Olga Russakovsky, and Anirudha Majumdar. 2025b. Actions as language: Fine-tuning vlms into vlas without catastrophic forgetting. Preprint, arXiv:2509.22195. Xiaoshuai Hao, Lei Zhou, Zhijian Huang, Zhiwen Hou, Yingbo Tang, Lingfeng Zhang, Guang Li, Zheng Lu, Shuhuai Ren, Xianhui Meng, Yuchen Zhang, Jing Wu, Jinghui Lu, Chenxu Dang, Jiayi Guan, Jianhua Wu, Zhiyi Hou, Hanbing Li, Shumeng Xia, and 25 others. 2025. Mimo-embodied: X-embodied foundation model technical report. Preprint, arXiv:2511.16518. Chi-Pin Huang, Yunze Man, Zhiding Yu, Min-Hung Chen, Jan Kautz, Yu-Chiang Frank Wang, and Fu-En Yang. 2026. Fast-thinkact: Efﬁcient vision-language-action reasoning via verbalizable latent planning. Preprint, arXiv:2601.09708. Chi-Pin Huang, Yueh-Hua Wu, Min-Hung Chen, Yu-Chiang Frank Wang, and Fu-En Yang. 2025. Thinkact: Vision-language-action reasoning via reinforced visual latent planning. Preprint, arXiv:2507.16815. Physical Intelligence, Kevin Black, Noah Brown, James Darpinian, Karan Dhabalia, Danny Driess, Adnan Esmail, Michael Equi, Chelsea Finn, Niccolo Fusai, Manuel Y. Galliker, Dibya Ghosh, Lachy Groom, Karol Hausman, Brian Ichter, Szymon Jakubczak, Tim Jones, Liyiming Ke, Devin LeBlanc, and 17 others. 2025. π0.5: visionlanguage-action model with open-world generalization. Preprint, arXiv:2504.16054. Jingjing Jiang, Chao Ma, Xurui Song, Hanwang Zhang, and Jun Luo. 2025a. Corvid: Improving multimodal In Proceedings of the IEEE/CVF International large language models towards chain-of-thought reasoning. Conference on Computer Vision (ICCV), pages 30343046. Tao Jiang, Tianyuan Yuan, Yicheng Liu, Chenhao Lu, Jianning Cui, Xiao Liu, Shuiqi Cheng, Jiyang Gao, Huazhe Xu, and Hang Zhao. 2025b. Galaxea open-world dataset and g0 dual-system vla model. Preprint, arXiv:2509.00576. Nikita Kachaev, Mikhail Kolosov, Daniil Zelezetsky, Alexey K. Kovalev, and Aleksandr I. Panov. 2025. Dont blind your vla: Aligning visual representations for ood generalization. Preprint, arXiv:2510.25616. Moo Jin Kim, Chelsea Finn, and Percy Liang. 2025. Fine-tuning vision-language-action models: Optimizing speed and success. Preprint, arXiv:2502.19645. 9 Moo Jin Kim, Karl Pertsch, Siddharth Karamcheti, Ted Xiao, Ashwin Balakrishna, Suraj Nair, Rafael Rafailov, Ethan Foster, Pannag Sanketi, Quan Vuong, Thomas Kollar, Benjamin Burchﬁel, Russ Tedrake, Dorsa Sadigh, Sergey Levine, Percy Liang, and Chelsea Finn. 2024. Openvla: An open-source vision-language-action model. In Annual Conference on Robot Learning (CoRL). Jason Lee, Jiafei Duan, Haoquan Fang, Yuquan Deng, Shuo Liu, Boyang Li, Bohan Fang, Jieyu Zhang, Yi Ru Wang, Sangho Lee, Winson Han, Wilbert Pumacay, Angelica Wu, Rose Hendrix, Karen Farley, Eli VanderBilt, Ali Farhadi, Dieter Fox, and Ranjay Krishna. 2025. Molmoact: Action reasoning models that can reason in space. Preprint, arXiv:2508.07917. Fuhao Li, Wenxuan Song, Han Zhao, Jingbo Wang, Pengxiang Ding, Donglin Wang, Long Zeng, and Haoang Li. 2025. Spatial forcing: Implicit spatial representation alignment for vision-language-action model. Preprint, arXiv:2510.12276. Qixiu Li, Yaobo Liang, Zeyu Wang, Lin Luo, Xi Chen, Mozheng Liao, Fangyun Wei, Yu Deng, Sicheng Xu, Yizhong Zhang, Xiaofan Wang, Bei Liu, Jianlong Fu, Jianmin Bao, Dong Chen, Yuanchun Shi, Jiaolong Yang, and Baining Guo. 2024a. Cogact: foundational vision-language-action model for synergizing cognition and action in robotic manipulation. Preprint, arXiv:2411.19650. Xinghang Li, Peiyan Li, Minghuan Liu, Dong Wang, Jirong Liu, Bingyi Kang, Xiao Ma, Tao Kong, Hanbo Zhang, and Huaping Liu. 2024b. Towards generalist robot policies: What matters in building vision-language-action models. Preprint, arXiv:2412.14058. Xuanlin Li, Kyle Hsu, Jiayuan Gu, Oier Mees, Karl Pertsch, Homer Rich Walke, Chuyuan Fu, Ishikaa Lunawat, Isabel Sieh, Sean Kirmani, Sergey Levine, Jiajun Wu, Chelsea Finn, Hao Su, Quan Vuong, and Ted Xiao. 2024c. In Annual Conference on Robot Learning Evaluating real-world robot manipulation policies in simulation. (CoRL). Fanqi Lin, Ruiqian Nai, Yingdong Hu, Jiacheng You, Junming Zhao, and Yang Gao. 2025a. Onetwovla: uniﬁed vision-language-action model with adaptive reasoning. Preprint, arXiv:2505.11917. Tao Lin, Yilei Zhong, Yuxin Du, Jingjing Zhang, Jiting Liu, Yinxinyu Chen, Encheng Gu, Ziyan Liu, Hongyi Cai, Yanwen Zou, Lixing Zou, Zhaoye Zhou, Gen Li, and Bo Zhao. 2025b. Evo-1: Lightweight vision-languageaction model with preserved semantic alignment. Preprint, arXiv:2511.04555. Xiaopeng Lin, Shijie Lian, Bin Yu, Ruoqi Yang, Changti Wu, Yuzhuo Miao, Yurun Jin, Yukun Shi, Cong Huang, Bojun Cheng, and Kai Chen. 2025c. Physbrain: Human egocentric data as bridge from vision language models to physical intelligence. Preprint, arXiv:2512.16793. Haotian Liu, Chunyuan Li, Qingyang Wu, and Yong Jae Lee. 2023. Visual instruction tuning. In Advances in neural information processing systems (NeurIPS). Ilya Loshchilov and Frank Hutter. 2019. Decoupled weight decay regularization. Preprint, arXiv:1711.05101. Pietro Mazzaglia, Cansu Sancaktar, Markus Peschl, and Daniel Dijkman. 2025. Hybrid training for visionlanguage-action models. Preprint, arXiv:2510.00600. Soroush Nasiriany, Abhiram Maddukuri, Lance Zhang, Adeet Parikh, Aaron Lo, Abhishek Joshi, Ajay Mandlekar, and Yuke Zhu. 2024. Robocasa: Large-scale simulation of everyday tasks for generalist robots. In Robotics: Science and Systems. NVIDIA, :, Johan Bjorck, Fernando Castañeda, Nikita Cherniadev, Xingye Da, Runyu Ding, Linxi \"Jim\" Fan, Yu Fang, Dieter Fox, Fengyuan Hu, Spencer Huang, Joel Jang, Zhenyu Jiang, Jan Kautz, Kaushil Kundalia, Lawrence Lao, Zhiqi Li, Zongyu Lin, and 24 others. 2025. Gr00t n1: An open foundation model for generalist humanoid robots. Preprint, arXiv:2503.14734. Abby ONeill, Abdul Rehman, Abhiram Maddukuri, Abhishek Gupta, Abhishek Padalkar, Abraham Lee, Acorn Pooley, Agrim Gupta, Ajay Mandlekar, Ajinkya Jain, and 1 others. 2024. Open x-embodiment: Robotic learning datasets and rt-x models: Open x-embodiment collaboration. In 2024 IEEE International Conference on Robotics and Automation (ICRA), pages 68926903. IEEE. William Peebles and Saining Xie. 2023. Scalable diffusion models with transformers. IEEE/CVF International Conference on Computer Vision (ICCV), pages 41954205. In Proceedings of the Karl Pertsch, Kyle Stachowicz, Brian Ichter, Danny Driess, Suraj Nair, Quan Vuong, Oier Mees, Chelsea Finn, and Sergey Levine. 2025. Fast: Efﬁcient action tokenization for vision-language-action models. Preprint, arXiv:2501.09747. 10 Delin Qu, Haoming Song, Qizhi Chen, Zhaoqing Chen, Xianqiang Gao, Xinyi Ye, Qi Lv, Modi Shi, Guanghui Ren, Cheng Ruan, Maoqing Yao, Haoran Yang, Jiacheng Bao, Bin Zhao, and Dong Wang. 2025a. Eo-1: Interleaved vision-text-action pretraining for general robot control. Preprint, arXiv:2508.21112. Delin Qu, Haoming Song, Qizhi Chen, Yuanqi Yao, Xinyi Ye, Yan Ding, Zhigang Wang, JiaYuan Gu, Bin Zhao, Dong Wang, and Xuelong Li. 2025b. Spatialvla: Exploring spatial representations for visual-language-action model. Preprint, arXiv:2501.15830. Yichao Shen, Fangyun Wei, Zhiying Du, Yaobo Liang, Yan Lu, Jiaolong Yang, Nanning Zheng, and Baining Guo. 2025. Videovla: Video generators can be generalizable robot manipulators. Preprint, arXiv:2512.06963. Mustafa Shukor, Dana Aubakirova, Francesco Capuano, Pepijn Kooijmans, Steven Palma, Adil Zouitine, Michel Aractingi, Caroline Pascal, Martino Russi, Andres Maraﬁoti, Simon Alibert, Matthieu Cord, Thomas Wolf, and Remi Cadene. 2025. Smolvla: vision-language-action model for affordable and efﬁcient robotics. Preprint, arXiv:2506.01844. starVLA. 2025. Starvla: lego-like codebase for vision-language-action model developing. GitHub repository. Huajie Tan, Yuheng Ji, Xiaoshuai Hao, Xiansheng Chen, Pengwei Wang, Zhongyuan Wang, and Shanghang Zhang. 2025. Reason-rft: Reinforcement ﬁne-tuning for visual reasoning of vision language models. Preprint, arXiv:2503.20752. BAAI RoboBrain Team, Mingyu Cao, Huajie Tan, Yuheng Ji, Xiansheng Chen, Minglan Lin, Zhiyu Li, Zhou Cao, Pengwei Wang, Enshen Zhou, Yi Han, Yingbo Tang, Xiangqi Xu, Wei Guo, Yaoxu Lyu, Yijie Xu, Jiayu Shi, Mengfei Du, Cheng Chi, and 34 others. 2025a. Robobrain 2.0 technical report. Preprint, arXiv:2507.02029. GEAR Team, Allison Azzolini, Johan Bjorck, Valts Blukis, Fernando Castañeda, Rahul Chand, and 1 others. 2025b. Gr00t n1.6: An improved open foundation model for generalist humanoid robots. https://research. nvidia.com/labs/gear/gr00t-n1_6/. Octo Model Team, Dibya Ghosh, Homer Walke, Karl Pertsch, Kevin Black, Oier Mees, Sudeep Dasari, Joey Hejna, Tobias Kreiman, Charles Xu, Jianlan Luo, You Liang Tan, Lawrence Yunliang Chen, Pannag Sanketi, Quan Vuong, Ted Xiao, Dorsa Sadigh, Chelsea Finn, and Sergey Levine. 2024. Octo: An open-source generalist robot policy. Preprint, arXiv:2405.12213. Junjie Wen, Yichen Zhu, Jinming Li, Minjie Zhu, Kun Wu, Zhiyuan Xu, Ning Liu, Ran Cheng, Chaomin Shen, Yaxin Peng, Feifei Feng, and Jian Tang. 2025. Tinyvla: Towards fast, data-efﬁcient vision-language-action models for robotic manipulation. Preprint, arXiv:2409.12514. Yuxi Xiao, Longfei Li, Shen Yan, Xinhang Liu, Sida Peng, Yunchao Wei, Xiaowei Zhou, and Bingyi Kang. 2026. Spatialtree: How spatial abilities branch out in mllms. Preprint, arXiv:2512.20617. Rui Yang, Ziyu Zhu, Yanwei Li, Jingjia Huang, Shen Yan, Siyuan Zhou, Zhe Liu, Xiangtai Li, Shuangye Li, Wenqian Wang, Yi Lin, and Hengshuang Zhao. 2025a. Visual spatial tuning. Preprint, arXiv:2511.05491. Shuai Yang, Hao Li, Yilun Chen, Bin Wang, Yang Tian, Tai Wang, Hanqing Wang, Feng Zhao, Yiyi Liao, and Jiangmiao Pang. 2025b. Instructvla: Vision-language-action instruction tuning from understanding to manipulation. Preprint, arXiv:2507.17520. Shusheng Yang, Jihan Yang, Pinzhi Huang, Ellis Brown, Zihao Yang, Yue Yu, Shengbang Tong, Zihan Zheng, Yifan Xu, Muhan Wang, Daohan Lu, Rob Fergus, Yann LeCun, Li Fei-Fei, and Saining Xie. 2025c. Cambrians: Towards spatial supersensing in video. Preprint, arXiv:2511.04670. Angen Ye, Zeyu Zhang, Boyuan Wang, Xiaofeng Wang, Dapeng Zhang, and Zheng Zhu. 2025. Vla-r1: Enhancing reasoning in vision-language-action models. Preprint, arXiv:2510.01623. Cheng Yin, Yankai Lin, Wang Xu, Sikyuen Tam, Xiangrui Zeng, Zhiyuan Liu, and Zhouping Yin. 2025. Deepthinkvla: Enhancing reasoning capability of vision-language-action models. Preprint, arXiv:2511.15669. Micha Zawalski, William Chen, Karl Pertsch, Oier Mees, Chelsea Finn, and Sergey Levine. 2025. Robotic control via embodied chain-of-thought reasoning. Preprint, arXiv:2407.08693. Andy Zhai, Brae Liu, Bruno Fang, Chalse Cai, Ellie Ma, Ethan Yin, Hao Wang, Hugo Zhou, James Wang, Lights Shi, Lucy Liang, Make Wang, Qian Wang, Roy Gan, Ryan Yu, Shalfun Li, Starrick Liu, Sylas Chen, Vincent Chen, and Zach Xu. 2025. Igniting vlms toward the embodied space. Preprint, arXiv:2509.11766. Chuheng Zhang, Rushuai Yang, Xiaoyu Chen, Kaixin Wang, Li Zhao, Yi Chen, and Jiang Bian. 2025a. How do vlas effectively inherit from vlms? Preprint, arXiv:2511.06619. 11 Jianke Zhang, Xiaoyu Chen, Qiuyue Wang, Mingsheng Li, Yanjiang Guo, Yucheng Hu, Jiajun Zhang, Shuai Bai, Junyang Lin, and Jianyu Chen. 2026. Vlm4vla: Revisiting vision-language-models in vision-language-action models. Preprint, arXiv:2601.03309. Ziang Zhang, Zehan Wang, Guanghao Zhang, Weilong Dai, Yan Xia, Ziang Yan, Minjie Hong, and Zhou Zhao. 2025b. Dsi-bench: benchmark for dynamic spatial intelligence. Preprint, arXiv:2510.18873. Jinliang Zheng, Jianxiong Li, Dongxiu Liu, Yinan Zheng, Zhihao Wang, Zhonghong Ou, Yu Liu, Jingjing Liu, Ya-Qin Zhang, and Xianyuan Zhan. 2025a. Universal actions for enhanced embodied foundation models. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pages 22508 22519. Jinliang Zheng, Jianxiong Li, Zhihao Wang, Dongxiu Liu, Xirui Kang, Yuchun Feng, Yinan Zheng, Jiayin Zou, Yilun Chen, Jia Zeng, Ya-Qin Zhang, Jiangmiao Pang, Jingjing Liu, Tai Wang, and Xianyuan Zhan. 2025b. X-vla: Soft-prompted transformer as scalable cross-embodiment vision-language-action model. Preprint, arXiv:2510.10274. Ruijie Zheng, Yongyuan Liang, Shuaiyi Huang, Jianfeng Gao, Hal Daumé III, Andrey Kolobov, Furong Huang, and Jianwei Yang. 2025c. Tracevla: Visual trace prompting enhances spatial-temporal awareness for generalist robotic policies. Preprint, arXiv:2412.10345. Enshen Zhou, Jingkun An, Cheng Chi, Yi Han, Shanyu Rong, Chi Zhang, Pengwei Wang, Zhongyuan Wang, Tiejun Huang, Lu Sheng, and Shanghang Zhang. 2026. Roborefer: Towards spatial referring with reasoning in vision-language models for robotics. Preprint, arXiv:2506.04308. Zhongyi Zhou, Yichen Zhu, Minjie Zhu, Junjie Wen, Ning Liu, Zhiyuan Xu, Weibin Meng, Ran Cheng, Yaxin Peng, Chaomin Shen, and Feifei Feng. 2025. Chatvla: Uniﬁed multimodal understanding and robot control with vision-language-action model. In Proceedings of the Empirical Methods in Natural Language Processing (EMNLP). Association for Computational Linguistics. Chenming Zhu, Tai Wang, Wenwei Zhang, Jiangmiao Pang, and Xihui Liu. 2025. Llava-3d: simple yet effective pathway to empowering lmms with 3d-awareness. Preprint, arXiv:2409.18125."
        },
        {
            "title": "A Training Hyperparameters",
            "content": "We instantiate the VLM backbones by initializing the weights from Qwen2.5-VL-3B-Instruct and Qwen3-VL-4B-Instruct. The model is ﬁne-tuned for 40k steps on cluster of 16 GPUs (batch size 16 per device). We employ the AdamW (Loshchilov and Hutter, 2019) optimizer initialized with learning rate of 1e-5 and cosine annealing schedule. System-level optimizations include DeepSpeed ZeRO2 (Loshchilov and Hutter, 2019), gradient clipping at norm of 1.0, and no gradient accumulation. Our training pipeline is built upon the starVLA (starVLA, 2025) framework."
        },
        {
            "title": "B VLM Prompt Template",
            "content": "This section presents the prompt templates used for VLM input during the VLA training process."
        },
        {
            "title": "System Prompt Template for Left Brain",
            "content": "You are helpful robot brain that can understand images and texts."
        },
        {
            "title": "System Prompt Template for Right Brain",
            "content": "You are helpful robot brain that can understand images, texts, and robot states. You will be provided with observation, an instruction, and the robot state. Take action to execute the instruction."
        },
        {
            "title": "User Prompt Template for Left Brain",
            "content": "<image> Instruction: {instruction}"
        },
        {
            "title": "User Prompt Template for Right Brain",
            "content": "<image> Instruction: {instruction} Robot Type: {robot_type} Predict the next action for the robot based on the observation, instruction and robot type."
        }
    ],
    "affiliations": [
        "BUAA",
        "CASIA",
        "DeepCybo",
        "ECNU",
        "HIT",
        "HKUST(GZ)",
        "HUST",
        "ZGCA",
        "ZGCI"
    ]
}