{
    "paper_title": "MemFly: On-the-Fly Memory Optimization via Information Bottleneck",
    "authors": [
        "Zhenyuan Zhang",
        "Xianzhang Jia",
        "Zhiqin Yang",
        "Zhenbo Song",
        "Wei Xue",
        "Sirui Han",
        "Yike Guo"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Long-term memory enables large language model agents to tackle complex tasks through historical interactions. However, existing frameworks encounter a fundamental dilemma between compressing redundant information efficiently and maintaining precise retrieval for downstream tasks. To bridge this gap, we propose MemFly, a framework grounded in information bottleneck principles that facilitates on-the-fly memory evolution for LLMs. Our approach minimizes compression entropy while maximizing relevance entropy via a gradient-free optimizer, constructing a stratified memory structure for efficient storage. To fully leverage MemFly, we develop a hybrid retrieval mechanism that seamlessly integrates semantic, symbolic, and topological pathways, incorporating iterative refinement to handle complex multi-hop queries. Comprehensive experiments demonstrate that MemFly substantially outperforms state-of-the-art baselines in memory coherence, response fidelity, and accuracy."
        },
        {
            "title": "Start",
            "content": "6 2 0 2 8 ] . [ 1 5 8 8 7 0 . 2 0 6 2 : r MemFly: On-the-Fly Memory Optimization via Information Bottleneck Zhenyuan Zhang 1 * Xianzhang Jia 1 * Zhiqin Yang 1 Zhenbo Song 2 Wei Xue 1 Sirui Han 1 Yike Guo 1 Abstract Long-term memory enables large language model agents to tackle complex tasks through historical interactions. However, existing frameworks encounter fundamental dilemma between compressing redundant information efficiently and maintaining precise retrieval for downstream tasks. To bridge this gap, we propose MEMFLY, framework grounded in information bottleneck principles that facilitates on-the-fly memory evolution for LLMs. Our approach minimizes compression entropy while maximizing relevance entropy via gradient-free optimizer, constructing stratified memory structure for efficient storage. To fully leverage MEMFLY, we develop hybrid retrieval mechanism that seamlessly integrates semantic, symbolic, and topological pathways, incorporating iterative refinement to handle complex multi-hop queries. Comprehensive experiments demonstrate that MEMFLY substantially outperforms state-of-the-art baselines in memory coherence, response fidelity, and accuracy. 1. Introduction The evolution of Large Language Models (LLMs) from stateless reasoning engines to persistent autonomous agents necessitates robust long-term memory systems capable of supporting complex, extended reasoning tasks (Xi et al., 2023; Wang et al., 2024; Ferrag et al., 2025). Such memory systems must address fundamental challenges: retaining entity states that evolve over time, resolving temporal dependencies across interaction sessions, and synthesizing evidence distributed across numerous conversational turns. However, existing frameworks encounter fundamental dilemma between compressing redundant information efficiently and maintaining precise retrieval for downstream tasks. Existing memory frameworks (Shinn et al., 2023; Sumers et al., 2024; Zhang et al., 2025; Fang et al., 2025; Zhai *Equal contribution 1The Hong Kong University of Science and Technology 2Nanjing University of Science and Technology. Preprint. February 10, 2026. 1 et al., 2025) generally fall into two paradigms, neither of which adequately resolves this tension. Retrieval-centric approaches (Lewis et al., 2021; Asai et al., 2023; Yan et al., 2024; Gao et al., 2024; Ram et al., 2023) preserve verbatim details but accumulate redundancy without consolidation, leading to monotonic entropy increase and elevated retrieval noise. Memory-augmented approaches (Packer et al., 2024; Zhong et al., 2023; Xu et al., 2025; Wang et al., 2025) employ LLM-driven summarization for compression but sacrifice fine-grained fidelity required for precise reasoning. Both paradigms lack unified, principled objective for determining what information to retain versus discard. This challenge is fundamentally an information-theoretic optimization problem that aligns with the Information Bottleneck (IB) principle (Slonim & Tishby, 1999): compress redundant observations while preserving sufficient fidelity for future tasks. To bridge this gap, we propose MEMFLY (Memory optimization on-the-Fly), framework grounded in information bottleneck principles that facilitates on-the-fly memory evolution for LLMs. Building upon the Agglomerative Information Bottleneck algorithm (Slonim & Tishby, 1999), MEMFLY addresses the compression-fidelity trade-off through two complementary mechanisms. To construct memory, we employ an LLM-driven gradient-free optimizer, which approximates Jensen-Shannon divergence through semantic assessment and actively merges redundant content to minimize representational complexity I(X; ) during memory ingestion. Simultaneously, we maintain stratified NoteKeyword-Topic hierarchy grounded in the double clustering principle (Slonim & Tishby, 2000), where Keywords serve as intermediate symbolic anchors stabilizing the semantic space between raw observations (Notes) and high-level semantic regions (Topics), thereby preserving task-relevant information I(M ; ). To leverage constructed memory, we design hybrid retrieval mechanism that seamlessly integrates semantic, symbolic, and topological pathways: macro-semantic navigation through Topics, micro-symbolic anchoring through Keywords, and topological expansion through associative links established during consolidation. For complex queries requiring multi-hop reasoning, we further introduce an iterative refinement protocol that progressively expands the evidence pool until sufficient information is gathered. The MemFly: On-the-Fly Memory Optimization via Information Bottleneck contributions of this work are summarized as follows: We formalize agentic memory as an Online Information Bottleneck problem, unifying the treatment of entropy accumulation and fidelity loss within single theoretical framework. We propose two mechanisms to optimize this objective: gradient-free optimizer that extends AIB to online settings through LLM-based semantic assessment, and Note-Keyword-Topic hierarchy grounded in double clustering that preserves evidence structure. We design tri-pathway retrieval with iterative refinement to exploit the optimized structure for complex reasoning tasks. Extensive evaluations on comprehensive benchmarks demonstrate that MEMFLY achieves substantial improvements, significantly outperforming state-of-theart baselines. 2. Related Work 2.1. Retrieval-Centric Systems Retrieval-augmented generation (RAG) (Lewis et al., 2021; Gao et al., 2024) has evolved from passive retrieve-thenread pipelines to active, iterative workflows. Recent advances introduce inference-time feedback loops for query refinement and hallucination filtering (Asai et al., 2023; Yan et al., 2024). Structural approaches further organize knowledge into graphs, enabling both local retrieval and global summarization (Edge et al., 2025; Wu et al., 2025). Beyond document-level retrieval, graph-based memory systems such as MemWalker (Chen et al., 2023) maintain structured knowledge representations through explicit traversal mechanisms. Despite these sophisticated capabilities, such methods fundamentally operate as inference-time optimizations that refine the read path for specific queries while treating the underlying memory structure as passive index. Consequently, these systems rely on query-centric embedding similarity to initiate retrieval, rendering them vulnerable to vector dilution in scenarios requiring multi-hop evidence synthesis. 2.2. Memory-Augmented Agents While retrieval-centric systems optimize retrieval, an orthogonal research direction addresses the construction path: how to structure and compress interaction history for effective long-term retention. Systems like MemGPT (Packer et al., 2024) and HiAgent (Hu et al., 2024) orchestrate context through tiered storage hierarchies, swapping information between active working memory and archival storage to emulate infinite retention. Parallel efforts seek to replicate biological memory processes. MemoryBank (Zhong et al., 2023) incorporates the Ebbinghaus forgetting curve to modulate information decay. A-MEM (Xu et al., 2025) and O-Mem (Wang et al., 2025) adopt associative strategies to foster autonomous knowledge evolution, such as, Zettelkasten-style linking or user-centric profiling. These approaches effectively mitigate the Goldfish Effect, the tendency of LLMs to prioritize recent context while losing track of earlier information (Hans et al., 2024), by structuring interaction history into discrete, retrievable memory units. While effective for managing token budgets, these approaches optimize for compression efficiency without principled mechanism for preserving task-relevant information. 3. The MEMFLY Framework We formulate the construction of agentic long-term memory as an Information Bottleneck (IB) optimization problem. In this framework, the memory system is not static repository but dynamic channel that compresses continuous input streams into compact, relevance-maximizing representation. Figure 1 illustrates the overall architecture of MEMFLY. 3.1. Problem Formulation Notation. Let X1:t = {x1, x2, . . . , xt} denote continuous stream of interaction data observed by the agent, where xt represents the input at time t. We define the agents memory state at time as random variable Mt taking values in structured state space. For computational realization, we instantiate Mt as dynamic graph Gt = (Vt, Et, Φt), where Vt is the set of memory nodes, Et Vt Vt represents topological connections, and Φt : Vt Rd Σ maps each node to its dense embedding and textual content, with Σ denoting the set of all strings over alphabet Σ. Remark 3.1 (Generality of the Framework). While we employ graph-based realization for its computational convenience in supporting discrete merge, link, and query operations, the underlying IB formulation is agnostic to the specific storage medium. Alternative instantiations, such as hierarchical databases or hybrid neuro-symbolic structures, are equally valid carriers for the abstract memory state Mt. The Optimization Objective. Following the Information Bottleneck principle (Slonim & Tishby, 1999; Tishby & Zaslavsky, 2015), our goal is to learn memory construction policy that maps the observed interaction history = {x1, . . . , xt} to the memory state Mt that maximizes task-relevant information while minimizing representational complexity. This is formalized as minimizing the Memory 2 MemFly: On-the-Fly Memory Optimization via Information Bottleneck Figure 1. Overview of the MEMFLY framework. Left: Memory construction processes incoming observations through semantic ingestion and gated structural update, where an LLM-based optimizer performs Merge, Link, or Append operations to minimize the IB objective. Center: The memory state is organized as stratified Note-Keyword-Topic hierarchy with associative edges following the double clustering principle. Right: Memory retrieval employs tri-pathway search via Topics, Keywords, and topological expansion, followed by iterative evidence refinement for complex queries. Information Bottleneck Lagrangian LIB: min π LIB(Mt) = I(X1:t; Mt) (cid:125) (cid:124) (cid:123)(cid:122) Compression , β I(Mt; ) (cid:125) (cid:123)(cid:122) Relevance (cid:124) (1) where π denotes the memory construction policy, β > 0 controls the compression-relevance trade-off, and represents future reasoning tasks. The Compression term I(X1:t; Mt) measures how much information from the raw input stream is retained in the memory state. Minimizing this term encourages the system to merge redundant information and discard irrelevant details. The Relevance term I(Mt; ) measures the mutual information between the memory state and future tasks . Maximizing this term ensures retention of critical evidence for downstream reasoning. key challenge in applying the Information Bottleneck principle to agentic memory is that future tasks are unknown at construction time. We define the relevance variable as the latent semantic structure governing future reasoning tasks. Since is not directly observable during memory construction, we approximate it through two proxy signals: (1) local coherence: the semantic consistency within and across memory units, captured by Keyword co-occurrence patterns; (2) global navigability: the accessibility of evidence chains, captured by the Topic hierarchy and associative links. These proxies reflect the observation that reasoning tasks typically require either entity-centric evidence retrieval or thematic evidence aggregation. Our ablation study (Sec. 4.3) empirically validates that optimizing these structural surrogates significantly improves downstream response fidelity and accuracy. Online Approximation via Greedy Agglomeration. Directly optimizing Eq. (1) over the entire history is computationally intractable due to the combinatorial explosion of possible memory configurations. Following the Agglomerative Information Bottleneck (AIB) algorithm (Slonim & Tishby, 1999), we adopt an online greedy strategy that makes locally optimal decisions at each time step. Specifically, we model the memory evolution as an online decision process where the state transition Mt+1 (Mt, xt) is governed by policy π. At each step, the policy seeks to minimize the incremental Lagrangian cost: = I(X1:t+1; Mt+1) I(X1:t; Mt) (cid:124) (cid:123)(cid:122) (cid:125) Icompress β (cid:0)I(Mt+1; ) I(Mt; )(cid:1) (cid:125) (cid:124) . (cid:123)(cid:122) Irelevance (2) In the original AIB algorithm, the merge decision between clusters zi and zj is determined by minimizing the informaMemFly: On-the-Fly Memory Optimization via Information Bottleneck tion loss quantified via the Jensen-Shannon divergence: δIY (zi, zj) = (cid:0)p(zi) + p(zj)(cid:1) DJS (cid:2)p(Y zi), p(Y zj)(cid:3). (3) LLM as JS-Divergence Approximator. Computing Eq. (3) exactly requires access to the conditional distributions p(Y zi) and p(Y zj), which are unavailable since future tasks are unknown. We address this through key observation: JS-divergence measures distributional similarity, which correlates with semantic similarity assessable by LLMs pre-trained on diverse tasks. Formally, we employ an LLM as gradient-free (Yang et al., 2024) policy π(Mt, xt) that approximates merge decisions through semantic assessment. Given two memory units nt and ni, the LLM evaluates their relationship and outputs scores sred(nt, ni) and scomp(nt, ni) defined in Sec. 3.3.2. We hypothesize that redundancy scores are inversely related to JS-divergence: sred(nt, ni) 1 DJS (cid:2)p(Y nt), p(Y ni)(cid:3), (4) where high redundancy indicates low JS-divergence, suggesting the units would provide similar information for downstream tasks. This design choice leverages the LLMs implicit knowledge of task-relevant distributional properties acquired during pre-training, and is empirically validated in our ablation study (Sec. 4.3). 3.2. Structural Prior To ensure the computational tractability of the online optimization, we impose structural prior on the memory state Mt. Direct manipulation of high-dimensional embedding spaces is ill-posed due to the curse of dimensionality, which manifests as sparsity and noise in similarity structures (Slonim & Tishby, 2000). To mitigate these topological degradations, we draw upon the design rationale of the Double Clustering framework established by Slonim and Tishby (Slonim & Tishby, 2000). Their informationtheoretic analysis demonstrated that for high-dimensional co-occurrence data, optimal compression is achieved not by clustering data points directly, but by first clustering the feature space to form robust intermediate representations. Specifically, the framework posits two-stage abstraction process: words are first aggregated into word clusters (Y ) based on their conditional distributions p(xy), yielding distributionally robust feature centroids. Subsequently, documents are clustered (X X) based on their distributions over these word clusters p(yx). This intermediate symbolic layer resolves the sparsity issue, allowing the system to achieve superior structural organization by projecting data onto denser, less noisy representation. Adhering to this principle, MEMFLY instantiates the memory state as stratified Note-Keyword-Topic hierarchy: Layer 1: Notes (Fidelity Layer). At the atomic level, we maintain the set of Notes, = {n1, . . . , nN }, serving as non-parametric memory units. Formally, each note is defined as tuple ni = (ri, ci, hi, Ki), where ri denotes the raw observational data (verbatim content) and ci represents the augmented contexta semantically denoised summary generated to enhance retrieval relevance. To facilitate hybrid access, these textual components are mapped into dual representational spaces: continuous dense embedding hi Rd encoding the context ci, and discrete set of symbolic keywords Ki serving as topological anchors. Analogous to the input variable in the Information Bottleneck framework, this layer is designed to preserve raw observational fidelity, mathematically approximating the condition I(N ; X) H(X). By explicitly maintaining non-parametric access to original inputs, we effectively mitigate the hallucination risks inherent in purely parametric or compression-heavy memory systems. Layer 2: Keywords (Anchoring Layer). To bridge continuous embedding spaces and discrete symbolic reasoning, we introduce Keywords = {k1, . . . , kK} as intermediate symbolic anchors. This layer serves an analogous role to word clusters ( ) in the double clustering framework. Unlike the original double clustering approach, which derives word clusters from co-occurrence statistics p(xy), MEMFLY extracts Keywords via LLM-based semantic parsing during the ingestion phase (Sec. 3.3.1). While this substitutes distributional clustering with neural semantic extraction, the functional role remains identical: Keywords provide lower-dimensional, distributionally robust feature space that stabilizes semantic proximity and mitigates vector dilution. The quality of this extraction depends on the LLMs capability, which we optimize through task-specific prompting strategies. Keywords resolve semantic sparsity by grounding proximity in shared symbolic substructures rather than potentially spurious vector correlations. Each Keyword kj maintains its own embedding ej Rd and tracks co-occurrence relationships with other Keywords extracted from the same Notes, forming the edge set ECO OCCUR. Layer 3: Topics (Navigation Layer). At the macro level, we aggregate keywords into topics = {C1, . . . , CT } based on their co-occurrence structure, analogous to document clusters ( X) in the double clustering framework. Topics serve as semantic centroids that partition the memory latent into navigable regions, enabling O(1) macro-semantic localization during retrieval. 4 MemFly: On-the-Fly Memory Optimization via Information Bottleneck 3.3. Memory Construction To tractably minimize the Memory IB Lagrangian (Eq. (1)), MEMFLY employs computation-on-construction mechanism. We model the memory update as an online agglomerative process, comprising three stages: ingestion, gated structural update, and topic evolution. 3.3.1. SEMANTIC INGESTION AND DENOISING. Raw input streams often contain elliptical references, syntactic noise, and implicit context. We project raw input xt into structured Note nt via an LLM-based transformation: nt = Fingest(xt) = (rt, ct, ht, Kt), (5) where rt preserves raw content, ct is the denoised context, ht = Embed(ct) Rd, and Kt is the extracted Keyword set. This transformation enhances signal-to-noise ratio, improving I(nt; ) relative to I(xt; ). 3.3.2. GATED STRUCTURAL UPDATE Before consolidation, we retrieve candidate neighborhood Ncand by querying existing memory through dual sparsedense indices, localizing the decision space to the most relevant subgraph. The LLM policy evaluates each candidate pair (nt, ni) with ni Ncand by generating two scalar scores through structured prompting: redundancy score sred(nt, ni) [0, 1] and complementarity score scomp(nt, ni) [0, 1]. Specifically, sred quantifies the semantic overlap between units, where unit value indicates identity in informational content. Conversely, scomp measures the strength of logical or topical connections between nodes that possess distinct, non-overlapping information. The prompting templates are provided in Appendix. Structural Operations. Based on these scores, the policy executes one of three operations: Mt+1 O(nt, ni) = MERGE(ni ni nt) LINK(ni nt) APPEND(nt) if sred(nt, ni) > τm if scomp(nt, ni) > τl otherwise (6) , where τm and τl are threshold hyperparameters. Merge Operation. When sred > τm, the content of nt is integrated into ni: = ri rt, = Fmerge(ci, ct), i, ni (r i, Embed(c i), Ki Kt), (7) (8) (9) 5 where Fmerge is an LLM-based function that synthesizes unified context preserving all distinct information from both units. This operation directly minimizes I(X1:t; Mt) by reducing Vt, analogous to the AIB merge step that selects pairs with minimal JS-divergence. Link Operation. When scomp > τl, directed edge is established: ERELATED ERELATED {(nt, ni)}. (10) Remark 3.2 (Information-Theoretic Interpretation of Link). While Link does not directly reduce I(X1:t; Mt) like Merge, it preserves conditional dependencies that support I(Mt; ). Formally, Link is triggered when: I(nt; ni) > 0 I(nt; ni) > 0, (11) indicating that nt provides additional task-relevant information beyond ni, and the two are logically related. By explicitly encoding this relationship in ERELATED, we preserve conditional structure necessary for multi-hop reasoning without increasing representational redundancy. Append Operation. When neither threshold is met, nt is appended as an autonomous unit, preserving distributional diversity for novel content. Remark 3.3 (Extension Beyond Classical AIB). The original AIB algorithm supports only merge operations on fixed cooccurrence matrices. MEMFLY extends this framework with Link and Append operations to handle streaming settings where information arrives incrementally and may exhibit complementary or novel content. This extension maintains the greedy optimization spirit while adapting to agentic memory requirements. 3.3.3. TOPIC EVOLUTION. Maintaining O(1) macro-navigability requires periodic restructuring of the Topic layer. We formalize this as constrained graph partitioning over the Keyword co-occurrence graph Gkw: max Q(T , Gkw) s.t. δmin Ci δmax, Ci , (12) where denotes the modularity function and δmin, δmax are cardinality bounds. We employ the Leiden algorithm (Traag et al., 2019) for efficiency. While modularity optimization differs from direct IB clustering, empirical studies demonstrate strong correlation between modularity-based and information-theoretic community structures (Fortunato, 2010). MemFly: On-the-Fly Memory Optimization via Information Bottleneck 3.4. Memory Retrieval 3.4.1. TRI-PATHWAY HYBRID RETRIEVAL. To exploit the optimized memory structure, MEMFLY employs tri-pathway hybrid retrieval strategy. Unlike conventional flat vector search, our approach decomposes queries into complementary semantic signals and executes parallel traversals over the memory graph. Evidence Fusion. The final evidence pool combines all pathways via Reciprocal Rank Fusion (RRF) (Cormack et al., 2009). RRF aggregates the reciprocal ranks of candidates across different retrieval pathways, prioritizing evidence that consistently appears at the top of multiple lists without requiring score normalization. The final pool is: Epool = TopKfinal (cid:0)scoreRRF Eexpand (cid:1), (20) The raw query is processed by an LLM-based semantic parser Fθ to disentangle retrieval intent: where scoreRRF denotes the fusion score calculated by RRF and Kfinal denotes the predefined budget for the final pool. (htopic, Hkeys) Fθ(q), (13) 3.4.2. ITERATIVE EVIDENCE REFINEMENT where htopic Rd encodes the topical description, and Hkeys = {hk1 , . . . , hkm } contains embeddings for core entities in the query. The intent signals drive three synergistic pathways: macrosemantic localization, micro-symbolic anchoring, and topological expansion. Pathway 1: Macro-Semantic Localization. This pathway addresses the navigation challenge in large-scale memory. Given htopic, we identify the top-Ktopic relevant Topic centroids: = TopKKtopic (cid:0)cos(htopic, µC) (cid:1), (14) where µC Rd is the centroid embedding of Topic C. Notes are retrieved by hierarchy traversal: Rtopic = {n Kn, , C}. (15) Pathway 2: Micro-Symbolic Anchoring. This pathway addresses the precision challenge for entity-centric queries. Query entities are matched against the keyword index: = (cid:91) TopKKkey (cid:0)cos(hk, ek) K(cid:1), (16) hkHkeys where ek Rd is the embedding of Keyword k. Notes are retrieved via keyword membership: Rkey = {n Kn = }. (17) Pathway 3: Topological Expansion. This pathway addresses connectivity for multi-hop reasoning by retrieving evidence that is logically related but vectorially distant. Starting from the anchor set: Eanc = Rtopic Rkey, (18) we expand along the ERELATED edges established during consolidation: Eexpand = {m Eanc, (n, m) ERELATED}. (19) 6 Complex reasoning tasks may require evidence not directly accessible from the initial query. We address this through an Iterative Evidence Refinement (IER) protocol that progressively expands the evidence pool. At each iteration i, the system evaluates whether the current evidence pool (i) sufficiently addresses the query. This evaluation is performed by an LLM that assesses information completeness. Formally, we define the sufficiency predicate: Suf(E (i), q) = (cid:40) 1, 0, if LLM(E (i), q) = true otherwise (21) If gaps are identified, refined sub-query q(i+1) is synthesized to target missing aspects, and retrieval is re-executed via the tri-pathway mechanism. The evidence pool is updated: (i+1) = (i) (cid:8)n R(q(i+1)) / (i)(cid:9), (22) where R(q) denotes the tri-pathway retrieval function. This process continues until Suf(E (i), q) = true or the maximum iteration count Imax is reached. 4. Experiments 4.1. Experimental Setup Dataset. We evaluate MEMFLY on the LoCoMo benchmark (Maharana et al., 2024), dataset specifically designed to assess the long-term information synthesis capabilities of LLM agents. LoCoMo contains long-horizon conversations with interleaved topics and evolving entity states, making it robust testbed for dynamic memory structures. To provide granular analysis of memory performance, we evaluate on five distinct reasoning categories: Multi-Hop, Temporal, Open Domain, Single Hop, and Adversarial. Evaluation Metrics. Following standard evaluation metrics (Xu et al., 2025), we employ two primary metrics: F1 Score to measure the token-level overlap and precision of the MemFly: On-the-Fly Memory Optimization via Information Bottleneck Table 1. Main results on LoCoMo benchmark using closed-source models (GPT series). We report F1 and BLEU-1 (%) scores across five categories. The best performance in each category is marked in bold, and the second best is underlined. Model Method i - 4 4 LOCOMO READAGENT MEMORYBANK MEMGPT A-MEM MEM-0 MEMFLY LOCOMO READAGENT MEMORYBANK MEMGPT A-MEM MEM-0 MEMFLY Multi Hop F1 25.02 9.15 5.00 26.65 27.02 34.72 32.11 28.00 14.61 6.49 30.36 32.86 35.13 35.89 BLEU 19.75 6.48 4.77 17.72 20.09 25.13 24.48 18.47 9.95 4.69 22.83 23.76 27.56 29.24 Temporal F1 18.41 12.60 9.68 25.52 45.85 45.93 46.61 9.09 4.16 2.47 17.29 39.41 52.38 39. BLEU 14.77 8.87 6.99 19.44 36.67 35.51 31.84 5.78 3.19 2.43 13.18 31.23 44.15 27.12 Category Open Domain BLEU F1 11.16 12.04 5.12 5.31 5.94 5.56 7.44 9.15 12.00 12.14 15.58 22.64 16.84 23.98 14.80 16.47 8.37 8.84 5.30 6.43 11.87 12.24 15.84 17.10 15.92 17.73 19.53 25.74 Single Hop F1 40.36 9.67 6.61 41.04 44.65 43.65 44.74 61.56 12.46 8.28 60.16 48.43 39.12 49.08 BLEU 29.05 7.66 5.16 34.34 37.06 37.42 38.17 54.19 10.29 7.10 53.35 42.97 35.43 43.05 Adversial F1 69.23 9.81 7.36 43.29 50.03 30.15 51.48 52.61 6.81 4.42 34.96 36.35 25.44 48.24 BLEU 68.75 9.02 6.48 42.73 49.47 27.44 51.96 51.13 6.13 3.67 34.25 35.53 24.19 48. Average F1 BLEU 39.74 9.89 6.99 35.45 41.97 38.70 43.76 44.12 9.98 6.13 41.02 40.53 36.59 44.39 33.47 7.87 5.73 30.16 36.16 32.07 37.27 38.70 8.07 5.15 36.23 35.36 32.25 38.70 Table 2. Main results on LoCoMo benchmark using open-source models (Qwen series). We report F1 and BLEU-1 (%) scores across five reasoning categories. The best performance in each category is marked in bold, and the second best is underlined. Model Method LOCOMO READAGENT MEMORYBANK MEMGPT A-MEM MEM-0 MEMFLY LOCOMO READAGENT MEMORYBANK MEMGPT A-MEM MEM-0 MEMFLY 8 - 3 Q 4 1 - 3 Q Multi Hop F1 25.09 13.17 21.25 22.13 24.30 23.04 28.24 33.37 13.16 25.97 24.12 21.36 20.98 30. BLEU 15.73 9.30 14.53 13.44 16.90 19.74 22.76 24.26 9.61 18.16 15.41 14.98 16.27 23.13 Temporal F1 32.82 34.91 30.20 31.47 34.50 29.65 38.39 31.49 18.12 25.37 25.48 23.06 31.5 29.25 BLEU 27.14 27.04 21.11 22.16 23.10 23.16 33.64 16.42 12.33 18.76 19.04 18.04 21.73 24.56 Category Open Domain BLEU F1 13.35 14.47 7.45 8.80 10.53 11.33 13.54 14.51 12.20 13.10 20.63 13.75 13.81 15.43 11.02 13.92 9.25 12.16 11.69 13.52 12.64 13.44 11.49 12.62 13.22 12.7 14.11 11.03 Single Hop F1 20.18 26.44 32.75 33.49 38.10 30.46 42.09 25.46 32.83 34.92 34.74 35.43 24.7 42.25 BLEU 18.39 24.83 26.33 34.12 33.30 25.62 36.57 24.82 28.35 30.6 32.41 30.92 19.14 35. Adversial F1 46.77 29.98 30.95 34.58 31.00 26.02 43.79 49.17 5.96 21.94 27.11 26.71 21.01 26.59 BLEU 40.81 28.34 30.13 31.44 30.10 22.48 43.14 35.00 4.2 17.56 24.32 25.78 19.84 25.02 Average F1 BLEU 28.62 25.87 29.27 30.88 32.76 27.80 38.62 32.42 20.63 28.16 28.99 28.37 23.86 33. 24.22 22.93 23.90 27.67 27.58 23.11 34.51 25.02 16.75 23.08 25.06 24.48 19.02 28.45 answer spans, and BLEU-1 (Papineni et al., 2002) to evaluate the lexical fidelity of the generated responses against ground truth. For ablation studies, we additionally report Recall, measuring the proportion of ground-truth evidence retrieved, and Hit Rate, indicating whether any relevant evidence appears in the candidates. Implementation Details. We implement MEMFLY using triple-layer graph architecture backed by Neo4j, integrating both vector indices and explicit topological relationships. For retrieval, we set Ktopic = 3 for Topic-based navigation, Kkey = 10 for Keyword anchoring, Kfinal = 20 for the final retrieval pool size, and perform 1-hop traversal along ERELATED edges for topological expansion. The iterative refinement protocol uses Imax = 3 iterations. For memory construction, we set the merge threshold τm = 0.7 and link threshold τl = 0.5 based on validation performance. Backbone Models and Baselines. We evaluate MEMFLY across four foundation models spanning closed-source (GPT (OpenAI, 2024)) and open-source (Qwen (Qwen, 2025)) families: GPT-4o-mini, GPT-4o, Qwen3-8B, and Qwen3-14B. The generation temperature is set to 0.7 for general reasoning and 0.5 for adversarial tasks. We compare MEMFLY against six representative methods: LOCOMO (Maharana et al., 2024), READAGENT (Lee et al., 2024), MEMORYBANK (Zhong et al., 2023), MEMGPT (Packer et al., 2024), A-MEM (Xu et al., 2025), and MEM0 (Chhikara et al., 2025). All baselines are implemented using their official system prompts and default configurations to ensure fair comparison. 4.2. Main Results Overall Performance. Tables 1 and 2 present performance comparisons on closed-source and open-source models, respectively. MEMFLY achieves the highest average 7 MemFly: On-the-Fly Memory Optimization via Information Bottleneck Table 3. Ablation study on LoCoMo (Qwen3-8B). We evaluate memory construction and retrieval components. Average F1, BLEU-1, Recall, and Hit Rate (%) are reported. The best performance in each category is marked in bold, and the second best is underlined. Phase Method BLEU Recall Hit Rate - MEMFLY - s n c e e w/o Update w/o Denoise w/o Link w/o Merge w/o Topic w/o Keyword w/o Neighbor w/o IER 38.62 27.97 36.07 33.57 34.79 36.79 32.69 34.26 32.94 36. 27.10 34.68 32.35 33.62 34.66 33.94 32.85 30.86 62.22 42.11 57.42 53.19 54.85 53.30 51.28 51.28 46.29 67. 48.20 62.55 56.18 59.42 58.91 54.26 54.35 51.26 F1 and BLEU-1 scores across all four backbone models. On closed-source models, it attains 43.76% and 44.39% F1 on GPT-4o-mini and GPT-4o respectively, outperforming the strongest baseline by 1.79 and 0.27 points. The advantage becomes more pronounced on open-source models: on Qwen3-8B, MEMFLY achieves 38.62% F1, surpassing the second-best A-MEM by 5.86 points. This larger margin on open-source models suggests that our structured memory organization effectively compensates for weaker in-context reasoning capabilities. The consistent improvements across heterogeneous architectures validate the generalization of our approach. Category-wise Analysis. Among the five reasoning categories, MEMFLY demonstrates the largest gains on Open Domain queries, achieving 25.74% F1 on GPT-4o compared to 17.73% for MEM-0. This improvement can be attributed to Topic-based navigation that localizes relevant memory regions before fine-grained retrieval. For Single Hop tasks requiring precise entity matching, MEMFLY achieves top performance on both Qwen models (42.09% and 42.25% F1), indicating effective Keyword-based anchoring. 4.3. Ablation Study Memory Construction Ablation. We examine the impact of IB-based memory consolidation by disabling core construction mechanisms. Removing the entire gated update (w/o Update) causes the most severe degradation, with average F1 dropping from 38.62% to 27.97% and Recall declining from 62.22% to 42.11%. As shown in Figure 2(a), this variant exhibits the largest performance gap across all five categories, with Adversarial and Temporal showing the most pronounced decline. This confirms that without active consolidation, noise accumulates and temporal dependencies become disrupted. Among individual operations, w/o Link shows larger impact than w/o Merge (33.57% vs 34.79% F1), and Figure 2(a) reveals that Link removal particularly affects Adversarial performance, indicating that associative 8 Figure 2. Category-wise F1 scores (%) for ablation variants on LoCoMo (Qwen3-8B). (a) Ablations on memory construction components. (b) Ablations on retrieval pathways and iterative refinement. edges are critical for filtering distractors. The w/o Denoise variant achieves the second-best performance (36.07% F1), maintaining relatively stable results across all categories as shown in the figure, suggesting that semantic preprocessing provides consistent but auxiliary improvements. Memory Retrieval Ablation. We systematically disable each retrieval pathway to assess individual contributions. The w/o Topic achieves the second-best performance among retrieval ablations (36.79% F1), and Figure 2(b) shows relatively uniform degradation across categories, indicating that macro-semantic navigation provides general retrieval guidance. In contrast, w/o Keyword (32.69% F1) and w/o IER (32.94% F1) exhibit more category-specific impacts. As illustrated in Figure 2(b), Keyword removal causes the most pronounced decline on Single-Hop queries, validating that symbolic anchoring is essential for precise entity matching. The w/o IER variant shows the largest degradation on Adversarial and Open Domain categories in the figure, demonstrating that iterative refinement is critical for queries requiring progressive evidence accumulation. The w/o Neighbor variant (34.26% F1) primarily impacts Adversarial, confirming that topological expansion via ERELATED edges helps distinguish relevant evidence from distractors. 5. Conclusion We presented MEMFLY, framework that formulates agentic long-term memory as an Information Bottleneck problem. Our approach employs an LLM-based gradient-free optimizer to consolidate redundant information while preserving task-relevant evidence through stratified NoteKeyword-Topic hierarchy. The tri-pathway retrieval mechanism with iterative refinement effectively exploits this structure for complex reasoning. Experiments on LoCoMo demonstrate consistent improvements over state-of-the-art baselines across diverse backbone models. Limitations. The current implementation prioritizes memory quality over construction speed, introducing moderate computational overhead. Extending evaluation to multimodal and domain-specific scenarios remains an avenue for future investigation. MemFly: On-the-Fly Memory Optimization via Information Bottleneck"
        },
        {
            "title": "Impact Statement",
            "content": "This paper presents work whose goal is to advance the field of Machine Learning. There are many potential societal consequences of our work, none which we feel must be specifically highlighted here."
        },
        {
            "title": "References",
            "content": "Asai, A., Wu, Z., Wang, Y., Sil, A., and Hajishirzi, H. Selfrag: Learning to retrieve, generate, and critique through self-reflection, 2023. Chen, H., Pasunuru, R., Weston, J., and Celikyilmaz, A. Walking down the memory maze: Beyond context limit through interactive reading. arXiv preprint arXiv:2310.05029, 2023. Chhikara, P., Khant, D., Aryan, S., Singh, T., and Yadav, D. Mem0: Building production-ready ai agents with scalable long-term memory, 2025. Cormack, G. V., Clarke, C. L. A., and Buettcher, S. Reciprocal rank fusion outperforms condorcet and individual rank learning methods. In Proceedings of the 32nd International ACM SIGIR Conference on Research and Development in Information Retrieval. Association for Computing Machinery, 2009. Edge, D., Trinh, H., Cheng, N., Bradley, J., Chao, A., Mody, A., Truitt, S., Metropolitansky, D., Ness, R. O., and Larson, J. From local to global: graph rag approach to query-focused summarization, 2025. Fang, J., Deng, X., Xu, H., Jiang, Z., Tang, Y., Xu, Z., Deng, S., Yao, Y., Wang, M., Qiao, S., Chen, H., and Zhang, N. Lightmem: Lightweight and efficient memoryaugmented generation, 2025. URL https://arxiv. org/abs/2510.18866. Ferrag, M. A., Tihanyi, N., and Debbah, M. From llm reasoning to autonomous ai agents: comprehensive review, 2025. URL https://arxiv.org/abs/ 2504.19678. Fortunato, S. Community detection in graphs. Physics Reports, 486(35):75174, February 2010. ISSN 0370-1573. doi: 10.1016/j.physrep.2009.11.002. URL http://dx.doi.org/10.1016/j.physrep. 2009.11.002. Gao, Y., Xiong, Y., Gao, X., Jia, K., Pan, J., Bi, Y., Dai, Y., Sun, J., Wang, M., and Wang, H. Retrieval-augmented generation for large language models: survey, 2024. URL https://arxiv.org/abs/2312.10997. Hans, A., Wen, Y., Jain, N., Kirchenbauer, J., Kazemi, H., Singhania, P., Singh, S., Somepalli, G., Geiping, J., Bhatele, A., and Goldstein, T. Be like goldfish, dont memorize! mitigating memorization in generative llms, 2024. URL https://arxiv.org/abs/ 2406.10209. Hu, M., Chen, T., Chen, Q., Mu, Y., Shao, W., and Luo, P. Hiagent: Hierarchical working memory management for solving long-horizon agent tasks with large language model, 2024. Lee, K.-H., Chen, X., Furuta, H., Canny, J., and Fischer, I. human-inspired reading agent with gist memory of very long contexts, 2024. Lewis, P., Perez, E., Piktus, A., Petroni, F., Karpukhin, V., Goyal, N., Kuttler, H., Lewis, M., Yih, W.-t., Rocktaschel, T., Riedel, S., and Kiela, D. Retrieval-augmented generation for knowledge-intensive nlp tasks, 2021. Maharana, A., Lee, D.-H., Tulyakov, S., Bansal, M., Barbieri, F., and Fang, Y. Evaluating very long-term conversational memory of llm agents, 2024. OpenAI. Gpt-4 technical report, 2024. URL https:// arxiv.org/abs/2303.08774. Packer, C., Wooders, S., Lin, K., Fang, V., Patil, S. G., Stoica, I., and Gonzalez, J. E. Memgpt: Towards llms as operating systems, 2024. Papineni, K., Roukos, S., Ward, T., and Zhu, W.-J. Bleu: method for automatic evaluation of machine translaIn Annual Meeting of the Association for Comtion. putational Linguistics, 2002. URL https://api. semanticscholar.org/CorpusID:11080756. Qwen. Qwen3 technical report, 2025. URL https:// arxiv.org/abs/2505.09388. Ram, O., Levine, Y., Dalmedigos, I., Muhlgay, D., Shashua, A., Leyton-Brown, K., and Shoham, Y. In-context retrieval-augmented language models, 2023. URL https://arxiv.org/abs/2302.00083. Shinn, N., Cassano, F., Berman, E., Gopinath, A., Narasimhan, K., and Yao, S. Reflexion: Language agents with verbal reinforcement learning, 2023. URL https://arxiv.org/abs/2303.11366. Slonim, N. and Tishby, N. Agglomerative information bottleneck. In Advances in Neural Information Processing Systems, volume 12. MIT Press, 1999. Slonim, N. and Tishby, N. Document clustering using word clusters via the information bottleneck method. In Proceedings of the 23rd Annual International ACM SIGIR Conference on Research and Development in Information Retrieval, SIGIR 00, pp. 208215, New York, NY, USA, 9 MemFly: On-the-Fly Memory Optimization via Information Bottleneck Agentevolver: Towards efficient self-evolving agent system, 2025. URL https://arxiv.org/abs/2511. 10395. Zhang, G., Fu, M., Wan, G., Yu, M., Wang, K., and Yan, S. G-memory: Tracing hierarchical memory for multiagent systems, 2025. URL https://arxiv.org/ abs/2506.07398. Zhong, W., Guo, L., Gao, Q., Ye, H., and Wang, Y. Memorybank: Enhancing large language models with long-term memory, 2023. 2000. Association for Computing Machinery. ISBN 9781-58113-226-7. doi: 10.1145/345508.345578. Sumers, T. R., Yao, S., Narasimhan, K., and Griffiths, T. L. Cognitive architectures for language agents, 2024. URL https://arxiv.org/abs/2309.02427. Tishby, N. and Zaslavsky, N. Deep learning and the information bottleneck principle, 2015. URL https: //arxiv.org/abs/1503.02406. Traag, V. A., Waltman, L., and van Eck, N. J. From louvain to leiden: guaranteeing well-connected communities. Scientific Reports, 9(1), March 2019. ISSN 2045-2322. doi: 10.1038/s41598-019-41695-z. URL http://dx.doi. org/10.1038/s41598-019-41695-z. Wang, L., Ma, C., Feng, X., Zhang, Z., Yang, H., Zhang, J., Chen, Z., Tang, J., Chen, X., Lin, Y., Zhao, W. X., Wei, Z., and Wen, J. survey on large language model based autonomous agents. Frontiers of Computer Science, 18(6), March 2024. ISSN 2095-2236. doi: 10.1007/ s11704-024-40231-1. URL http://dx.doi.org/ 10.1007/s11704-024-40231-1. Wang, P., Tian, M., Li, J., Liang, Y., Wang, Y., Chen, Q., Wang, T., Lu, Z., Ma, J., Jiang, Y. E., and Zhou, W. O-mem: Omni memory system for personalized, long horizon, self-evolving agents, 2025. Wu, X., Yang, C., Lin, X., Xu, C., Jiang, X., Sun, Y., Xiong, H., Li, J., and Guo, J. Think-on-graph 3.0: Efficient and adaptive llm reasoning on heterogeneous graphs via multi-agent dual-evolving context retrieval, 2025. Xi, Z., Chen, W., Guo, X., He, W., Ding, Y., Hong, B., Zhang, M., Wang, J., Jin, S., Zhou, E., Zheng, R., Fan, X., Wang, X., Xiong, L., Liu, Q., Zhou, Y., Wang, W., Jiang, C., Zou, Y., Liu, X., Yin, Z., Dou, S., Weng, R., Cheng, W., Zhang, Q., Qin, W., Zheng, Y., Qiu, X., Huan, X., and Gui, T. The rise and potential of large language model based agents: survey. ArXiv, abs/2309.07864, 2023. URL https://api.semanticscholar. org/CorpusID:261817592. Xu, W., Mei, K., Gao, H., Tan, J., Liang, Z., and Zhang, Y. A-mem: Agentic memory for llm agents, 2025. Yan, S.-Q., Gu, J.-C., Zhu, Y., and Ling, Z.-H. Corrective retrieval augmented generation, 2024. Yang, C., Wang, X., Lu, Y., Liu, H., Le, Q. V., Zhou, D., and Chen, X. Large language models as optimizers, 2024. URL https://arxiv.org/abs/2309.03409. Zhai, Y., Tao, S., Chen, C., Zou, A., Chen, Z., Fu, Q., Mai, S., Yu, L., Deng, J., Cao, Z., Liu, Z., Ding, B., and Zhou, J. 10 MemFly: On-the-Fly Memory Optimization via Information Bottleneck A. Prompting Templates This appendix provides the complete prompting templates used in MEMFLY. All prompts are designed to elicit structured JSON outputs for reliable parsing. A.1. Memory Construction Prompts A.1.1. SEMANTIC INGESTION PROMPT During the ingestion phase (Sec. 3.3.1), raw conversational input xt is transformed into structured Note nt = (rt, ct, ht, Kt). The following prompt instructs the LLM to extract keywords Kt and generate the denoised context ct:"
        },
        {
            "title": "Semantic Ingestion Prompt",
            "content": "You are an expert Knowledge Graph Extractor. Your task is to analyze the [TARGET TURN] to extract structured metadata. Input Text: {content} Guidelines: 1. Keywords (Entities): GOAL: Extract 3-5 specific Noun Phrases explicitly present in the text. FOCUS PRIORITIES: 1. Proper Nouns: People (e.g., Melanie), Locations, Organizations. 2. Concrete Objects: Physical items (e.g., painting, plate, contract). 3. Specific Topics: LGBTQ support group, deadline. CRITICAL STOP LIST: Ignore conversational meta-roles, abstract terms, and speaker names acting purely as subjects. ZERO-SHOT RULE: If the text is purely phatic (e.g., Wow, Thats cool), return an empty list. 2. Context (Factual Restatement): GOAL: Rewrite the text into self-contained factual statement. CONSTRAINT 1 (Strict Fidelity): Only use information present in the Input Text. CONSTRAINT 2 (Safe Resolution): Resolve I/my/we using the Speakers name if it appears in the text. For external pronouns where the antecedent is missing, keep the pronoun or use generic term. Do not guess. CONSTRAINT 3 (No Meta-Language): Start directly with the subject. Avoid The speaker says.... Output Format (JSON): {\"keywords\": [\"entity1\", \"entity2\"], \"context\": \"Melanie thinks the item is cool.\"} The extracted keywords are matched against the existing Keyword index to establish symbolic anchors, while the context is encoded via the embedding model to obtain ht = Embed(ct). This dual extraction enables both symbolic and semantic access pathways during retrieval. A.1.2. GATED STRUCTURAL UPDATE PROMPT During the gated structural update phase (Sec. 3.3.2), the LLM policy evaluates the relationship between new Note nt and each candidate Note ni Ncand. The following prompt generates the redundancy score sred and complementarity score scomp, and determines the appropriate structural operation (Merge, Link, or Append). Gated Structural Update Prompt Role: You are Knowledge Graph Updater. Your job is to evaluate the relationship between NEW NODE and existing CANDIDATE NODES. [NEW NODE] Content: {content} Context: {context} Keywords: {keywords} [CANDIDATE NODES] {candidates str} Instructions: Analyze each candidate and generate JSON response following these rules: 1. Analyze Relationship: Determine relation type: SUPPORTS, CONFLICTS, or RELATED TO. Assign connection strength (0.0 1.0), indicating the degree of semantic overlap or logical connection. 2. Determine Operation (Based on Strength): CASE A: Strength 0.8 (High Redundancy) MERGE Action: Integrate details from the New Node into the candidates context. Template: [Original Context]. Specifically, [New Node Info]... CASE B: Strength [0.5, 0.8) (Complementary) LINK Action: Establish associative edge; keep contexts separate. Template: [Original Context]. (Related: [New Node Keyword]) CASE C: Strength < 0.5 (Distinct) APPEND Action: Add New Node as autonomous unit; no modification. 11 MemFly: On-the-Fly Memory Optimization via Information Bottleneck CASE D: relation type is CONFLICTS LINK with Contrast Action: Note the conflicting information explicitly. Template: [Original Context]. However, [New Node] indicates that... 3. Output: Return strictly valid JSON matching the schema. Mapping to Paper Notation. The connection strength score directly corresponds to our redundancy score sred(nt, ni) when the relation type indicates semantic overlap, and to the complementarity score scomp(nt, ni) when the nodes contain distinct but logically related information. The threshold τm = 0.7 for Merge and τl = 0.5 for Link (Sec. 4) are applied to these scores to determine the final structural operation according to Eq. (6). When the Merge operation is triggered, the LLM generates unified context specified in Case A, preserving all distinct information from both units while eliminating redundancy. = Fmerge(ci, ct) following the template A.2. Memory Retrieval Prompts A.2.1. QUERY INTENT ANALYSIS PROMPT During the retrieval phase (Sec. 3.4.1), the raw query is processed by semantic parser Fθ to extract retrieval intent signals. The following prompt disentangles the query into topical description htopic and entity keywords Hkeys for driving the tri-pathway retrieval mechanism. Query Intent Analysis Prompt Analyze the user query to identify its Target Taxonomy Category, extract key entities, and detect time-related intent. Task 1: topic desc (Target Category) Predict the Taxonomy Category or Subject Heading this query falls under. Style: Strict Noun Phrase (like book chapter title or library category). Constraint: Keep it under 8 words. Do NOT describe the users intent (e.g., avoid how to..., techniques for...). Instead, name the topic itself. Task 2: Keywords Extract 3-5 core entities, technical terms, or specific concepts. CRITICAL: Convert terms to their canonical singular form (e.g., transformers Transformer). Exclude generic verbs or stop words. Query: {query} Output Format (JSON): {\"topic_desc\": \"Concise Noun Phrase\", \"keywords\": [\"keyword1\", \"keyword2\", \"keyword3\"]} Mapping to Paper Notation. The topic desc field is encoded via the embedding model to obtain htopic Rd, which drives Pathway 1 (Macro-Semantic Localization) through Topic matching (Eq. (14)). The keywords are similarly embedded to form Hkeys = {hk1, . . . , hkm}, enabling Pathway 2 (Micro-Symbolic Anchoring) via Keyword matching (Eq. (16)). A.2.2. ITERATIVE EVIDENCE REFINEMENT PROMPTS The Iterative Evidence Refinement protocol (Sec. 3.4.2) employs two complementary prompts: Sufficiency Evaluator that assesses whether the current evidence pool adequately addresses the query, and Sub-query Generator that synthesizes targeted follow-up queries when gaps are identified. Sufficiency Evaluation Prompt. At each iteration i, the following prompt evaluates whether the current evidence pool (i) satisfies the sufficiency predicate Suf(E (i), q) defined in Eq. (21). Sufficiency Evaluation Prompt You are reflector agent that evaluates whether the current context and answer is sufficient to answer question. Question: {question} Current Context and Current Answer: {context} Evaluate whether the provided context and answer contains enough information to answer the question comprehensively. MemFly: On-the-Fly Memory Optimization via Information Bottleneck If the context and answer is insufficient, identify what specific information is missing. Output Format (JSON): {\"sufficient\": true or false, \"missing_info\": \"description of missing information\", \"confidence\": 0.0 to 1.0} Sub-query Generation Prompt. When the sufficiency evaluation returns sufficient: generates refined sub-query q(i+1) targeting the identified information gaps. false, the following prompt Sub-query Generation Prompt You are Query Evolution Agent. Your goal is to decompose complex user question into specific, actionable sub-query to retrieve missing information from Knowledge Graph. Original Question: {query str} Current Known Information (Context): {context str} History of Reasoning Steps (Q&A): {prev reasoning} CRITICAL: The Reflector Agent has identified the following MISSING INFORMATION needed to answer the main question: {missing info} Task: Based on the Missing Information and History, formulate the NEXT single sub-question to retrieve this missing info. The sub-question must be specific. It should act as search query for the next hop. If we have enough information to answer the main question, return None. Sub-question: IER Protocol Flow. The two prompts work in tandem: the Sufficiency Evaluator determines whether to terminate (when sufficient: true or iteration count reaches Imax), while the Sub-query Generator drives evidence expansion by producing targeted queries that are re-executed through the tri-pathway retrieval mechanism (Eq. (22)). The confidence score from the Sufficiency Evaluator can optionally be used for early termination when confidence exceeds predefined threshold. B. Dataset Statistics Table 4 presents the sample distribution across the five reasoning categories. The categories are designed to test distinct memory capabilities: Multi-Hop requires synthesizing evidence across multiple memory units; Temporal tests reasoning about time-dependent information and event ordering; Open Domain evaluates retrieval of general knowledge from conversation history; Single Hop assesses precise entity matching and direct fact retrieval; and Adversarial challenges the system with distractors and misleading information. As shown in Table 4, the category distribution is imbalanced, with Single Hop comprising the largest proportion (42.3%) and Open Domain the smallest (4.8%). To account for this imbalance, the average scores reported in Table 1 and Table 2 are computed as weighted averages based on category sample sizes, ensuring that performance on larger categories contributes proportionally to the overall evaluation. Table 4. LoCoMo benchmark category distribution. Category Samples Proportion Multi-Hop Temporal Open Domain Single Hop Adversarial 282 321 96 841 446 Total 1,986 14.2% 16.2% 4.8% 42.3% 22.5% 100% 13 MemFly: On-the-Fly Memory Optimization via Information Bottleneck C. Hyperparameter Settings Table 5 summarizes all hyperparameters used in MEMFLY. These hyperparameters are organized by the two main phases of our framework: memory construction and memory retrieval. For memory construction, the merge threshold τm = 0.7 and link threshold τl = 0.5 control the gated structural update decisions (Eq. (6)). higher merge threshold ensures that only highly redundant information is consolidated, preserving fine-grained distinctions between memory units. The link threshold is set lower to capture complementary relationships that support multi-hop reasoning. For memory retrieval, we set Ktopic = 3 to balance navigation precision with coverage, allowing the system to explore multiple relevant Topic clusters. The keyword retrieval parameter Kkey = 10 provides sufficient anchor points for entitycentric queries. The final pool size Kf inal = 20 bounds the evidence passed to the generation stage, balancing context richness against computational cost. The maximum IER iterations Imax = 3 prevents excessive retrieval loops while allowing sufficient evidence expansion for complex queries. All hyperparameters were tuned on held-out validation set. We found the framework to be relatively robust to moderate variations in these values, with performance degrading gracefully when parameters deviate within 20% of the reported settings. Table 5. Hyperparameter settings. Phase Parameter Value Construction Merge threshold τm Link threshold τl Retrieval Generation Topic retrieval Ktopic Keyword retrieval Kkey Final pool size Kf inal Max IER iterations Imax Temperature (general) Temperature (adversarial) 0.7 0. 3 10 20 3 0.7 0."
        }
    ],
    "affiliations": [
        "Department of Computing, Imperial College London"
    ]
}