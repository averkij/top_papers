{
    "paper_title": "Shakti-VLMs: Scalable Vision-Language Models for Enterprise AI",
    "authors": [
        "Syed Abdul Gaffar Shakhadri",
        "Kruthika KR",
        "Kartik Basavaraj Angadi"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "We introduce Shakti VLM, a family of vision-language models in the capacity of 1B and 4B parameters designed to address data efficiency challenges in multimodal learning. While recent VLMs achieve strong performance through extensive training data, Shakti models leverage architectural innovations to attain competitive results with fewer tokens. Key advancements include QK-Normalization for attention stability, hybrid normalization techniques, and enhanced positional encoding. A three-stage training strategy further optimizes learning efficiency. Evaluations show that Shakti-Shakti-VLM-1B and Shakti-VLM-4B excel in document understanding, Visual Reasoning, OCR extraction, and general multimodal reasoning. Our results highlight that high performance can be achieved through model design and training strategy rather than sheer data volume, making Shakti an efficient solution for enterprise-scale multimodal tasks."
        },
        {
            "title": "Start",
            "content": "Shakti-VLMs: Scalable Vision-Language Models for Enterprise AI 5 2 0 2 4 2 ] . [ 1 2 9 0 7 1 . 2 0 5 2 : r Syed Abdul Gaffar Shakhadri Lead AI Developer SandLogic Technologies Pvt Ltd. syed.abdul@sandlogic.com Kruthika KR AI Researcher SandLogic Technologies Pvt Ltd kruthika.kr@sandlogic.com Kartik Basavaraj Angadi AI Developer SandLogic Technologies Pvt Ltd kartik.angadi@sandlogic.com"
        },
        {
            "title": "ABSTRACT",
            "content": "We introduce Shakti VLM, family of vision-language models in the capacity of 1B and 4B parameters designed to address data efficiency challenges in multimodal learning. While recent VLMs achieve strong performance through extensive training data, Shakti models leverage architectural innovations to attain competitive results with fewer tokens. Key advancements include QK-Normalization for attention stability, hybrid normalization techniques, and enhanced positional encoding. three-stage training strategy further optimizes learning efficiency. Evaluations show that Shakti-Shakti-VLM-1B and Shakti-VLM-4B excel in document understanding, Visual Reasoning, OCR extraction, and general multimodal reasoning. Our results highlight that high performance can be achieved through model design and training strategy rather than sheer data volume, making Shakti an efficient solution for enterprise-scale multimodal tasks. Keywords Shakti Vision Language Model QK Normalization Hybrid Layer Normalization Training Strategy"
        },
        {
            "title": "Introduction",
            "content": "Large Vision-Language Models (LVLMs) have emerged as transformative force in artificial intelligence, seamlessly integrating vision and language understanding to enhance multimodal perception and reasoning. By capitalizing on recent advancements in Vision Transformers (ViTs)[1] and Large Language Models (LLMs), these systems can interpret images, documents, and videos with remarkable textual comprehension. Yet, existing LVLMs often face challenges such as high computational costs, fine-grained visual perception limitations, extended context handling issues, and difficulties adapting to real-world data diversity. Recent advancements in vision-language models (VLMs) have significantly improved AI-driven multimodal applications, demonstrating strong performance in image-text understanding, object recognition, and reasoning. Notable models like Qwen2VL[2], Molmo[3], and SmolVLM[4] have showcased impressive capabilities but rely on extensive training data to achieve high accuracy across diverse tasks. This dependency presents scalability challenges, particularly for enterprise applications that require efficient and adaptable solutions. To address these limitations, we introduce Shakti-VLM, family of lightweight yet high-performing vision-language models (Shakti-VLM-1B and Shakti-VLM-4B), optimized for enterprise-scale and edge deployments. Building on insights from large-scale open-source efforts such as Qwen2.5-VL and InternVL, Shakti models focus on efficiency rather than size alone, ensuring robust multimodal capabilities while maintaining computational feasibility. Shakti models incorporate several architectural innovations that improve efficiency and generalization across multimodal tasks. Rather than merely increasing model size, Shakti-VLMs employ hybrid normalization strategy, leveraging QK-Normalization[5] for stable attention mechanisms and enhanced positional encoding, ensuring faster convergence and robust performance even under limited data scenarios. These design choices make Shakti models highly effective for document parsing, OCR extraction, and chart interpretation, making them ideal for real-world enterprise pipelines. Our training approach follows three-stage methodology to maximize efficiency. First, we pretrain the decoder on extended-context text-only data, enabling strong language understanding before multimodal alignment. Next, we align vision and language representations using frozen decoder, ensuring effective feature fusion without unnecessary computational overhead. Finally, we perform full model fine-tuning, incorporating instruction tuning, RLHF[6], and DPO[7], optimizing the model for real-world multimodal applications.This structured approach maximizes data efficiency, achieving strong multimodal alignment with significantly lower training requirements. Despite using significantly fewer training tokens than other VLMs (487 billion for Shakti-VLM-1B and 782 billion for Shakti-VLM-4B), both models demonstrate exceptional benchmark performance. Shakti-VLM-1B delivers balanced results across diverse multimodal tasks, particularly excelling in document and chart understanding, frequently outperforming larger models like SmolVLM-2.25B[4]. Meanwhile, Shakti-VLM-4B surpasses state-of-the-art models, including Qwen2VL-7B[2] and MiniCPM-V-2.6-8B[8], on complex multimodal reasoning benchmarks. Furthermore, Shakti models exhibit strong generalization across visual question answering (VQA), mathematical reasoning, and long-form textual comprehension tasks, frequently matching or surpassing models with significantly more parameters. By integrating scalable vision encoders, advanced attention mechanisms, and an optimized three-stage training process, Shakti-VLM models redefine efficiency in multimodal AI. Their strong performance across OCR, document understanding, and vision-language reasoning tasks establishes them as leading solutions in the evolving LVLM landscape, catering to real-world enterprise needs. Unlike conventional VLMs that demand vast computational resources, Shakti models are optimized for both enterprise-scale and edge deployments, ensuring favorable balance between accuracy, memory footprint, and inference speed. We evaluate Shakti models on broad range of multimodal benchmarks, including OCR tasks, document VQA, chart understanding, and general vision-language QA, demonstrating comparable or superior results against other models. Key Features of Shakti-VLM Models Adoption of QK-Normalization for improved stability and performance. Hybrid Normalization Strategy, combining Pre-LayerNorm in early layers with Post-LayerNorm using RMSNorm in later layers, ensuring an optimal balance between stability and efficiency. Optimized three-stage training methodology, allowing better performance across tasks with fewer training tokens. Scalability across different deployment scenarios, from enterprise-level document automation to edge computing applications requiring lightweight multimodal AI models."
        },
        {
            "title": "2 Related Work:",
            "content": "Recent years have seen rapid progress in vision-language models (VLMs), driven by breakthroughs in architecture, scaling laws, and multimodal alignment techniques. These models are becoming central to tasks that require seamless understanding of both visual and textual inputs, such as visual question answering, image captioning, document understanding, and OCR. This section highlights key developments in the field, with focus on pioneering VLM families and their contributions to model efficiency, document processing, and training innovations."
        },
        {
            "title": "2.1 Advancement in Vision Language Models",
            "content": "Recent advancements in vision-language models (VLMs) have significantly expanded the capabilities of multimodal AI systems. Several notable model families have emerged, each with distinct architectural approaches and scaling strategies. The Qwen-VL[9] [2] series (Qwen-VL and Qwen2-VL) represents significant milestones in open-source VLM development. The original Qwen-VL[9] built upon Qwen-LM with visual receptor and 3-stage training pipeline, demonstrating strong performance on visual grounding and OCR tasks. Its successor, Qwen2-VL, introduced the Naive Dynamic Resolution mechanism[2] for handling variable image resolutions and Multimodal Rotary Position Embedding (M-RoPE)[2] for effectively fusing positional information across modalities. Qwen2-VL explored scaling laws across model sizes at 2B, 8B, and 72B parameters, achieving performance competitive with proprietary models at the 72B scale. InternVL represents another significant branch of VLM research. The InternVL series scaled vision foundation models to 6B parameters and progressively aligned them with LLMs using web-scale image-text data. InternVL 1.5[10] improved upon this foundation with dynamic high-resolution processing supporting up to 4K resolution input and bilingual dataset enhancements for OCR and Chinese language tasks. The most recent iteration, InternVL 2.5[11], maintained the core architecture while focusing on training and testing strategy improvements, achieving high performance on multi-discipline reasoning tasks. Microsofts Phi-3[12] series has extended into the vision domain with Phi-3.5-Vision[12], relatively compact 4.2B parameter model derived from the Phi-3.5-mini language model. Despite its modest size, Phi-3.5-Vision demonstrates strong reasoning capabilities and handles both single and multi-image inputs effectively."
        },
        {
            "title": "2.2 Efficiency-Focused Approaches",
            "content": "A growing trend in VLM research focuses on developing efficient models that maintain high performance while reducing computational requirements. SmolVLM[4] represents this direction with its 2B parameter model designed for commercial use and local deployment. These models leverage open training pipelines and datasets like Cauldron and Docmatix, demonstrating that smaller models can still achieve practical utility. Similarly, Molmo[3] introduced family of VLMs built from scratch without distillation from proprietary models. Their approach combined careful modeling choices with high-quality original created PixMo dataset, including detailed image captions and innovative 2D pointing data. Despite focusing on open development principles, their models achieved competitive performance with larger models. Idefics3-8B[13] exemplifies efficient VLM development through straightforward training pipelines and exclusive use of open datasets. The creation of Docmatixa dataset 240 times larger than previously available document understanding resourcescontributed significantly to its document processing capabilities."
        },
        {
            "title": "2.3 Document Understanding and OCR Capabilities",
            "content": "Document understanding and OCR capabilities have become essential benchmarks for evaluating VLM performance. Several models have made notable progress in this domain. InternVL 1.5[10] incorporated high-quality datasets covering document images with bilingual annotations, significantly enhancing OCR-related task performance. Qwen-VL[9] implemented text-reading ability by aligning image-caption-box tuples, while Qwen2-VLs dynamic resolution approach improved document processing capabilities. The development of the Docmatix dataset by Idefics3[13] marks significant milestone in advancing document understanding, providing training resources at unprecedented scale. This development has raised the baseline for document processing capabilities in modern VLMs."
        },
        {
            "title": "2.4 Training Strategies and Data Efficiency",
            "content": "Training methodologies have diversified across VLM development. The Qwen-VL[9] series employed 3-stage training pipeline with multilingual multimodal cleaned corpus, while Molmo[3] emphasized dataset quality over quantity with their carefully curated PixMo datasets. InternVL explored continuous learning strategies for large-scale vision foundation models and high-quality bilingual dataset curation. While many approaches have focused on scaling both model size and training data volume as seen with Qwen2VLs 72B parameter model and InternVLs extensive data collection, our work with Shakti VLM contributes to this landscape by introducing architectural innovations specifically designed to improve data efficiency. Through adopting QK-Normalization[5] for attention stability, hybrid normalization techniques, and enhanced positional encoding, ShaktiVLM models achieve competitive performance despite using fewer training tokens than comparable models. This focus on efficiency through architectural design rather than sheer data volume positions Shakti-VLM as practical solution for enterprise-scale multimodal tasks."
        },
        {
            "title": "3 Architecture of Shakti-VLM",
            "content": "The Shakti-VLM-1B and Shakti-VLM-4B models are designed to provide multi-modal understanding through an efficient combination of vision encoding, projection layers, and textual decoding. Both models leverage dynamic patch sizes and hybrid normalization techniques to enhance stability and scalability, along with RoPE[14] with 2D positional bias and hybrid activation functions ensure improved visual feature extraction. The architecture is designed for tasks like OCR, visual reasoning, and contextual understanding, with decoders optimized for seamless integration of visual and textual modalities."
        },
        {
            "title": "3.1 Vision Encoder",
            "content": "The vision encoder of the Shakti-VLM-1B model is instantiated upon the Vision Encoder[1], comprising 36 layers, hidden dimensionality of 1536, and 16 attention heads, optimized for high-resolution visual processing across multiple tasks, including Optical Character Recognition (OCR), fine-grained visual understanding, and image summarization. The encoder incorporates dynamic patch size mechanism, adaptable within the range of 1414 at 224px to 3232 at 1024px resolutions, ensuring robust scalability across varying input image resolutions, which is imperative for precise text recognition and the extraction of intricate visual details. The Shakti-VLM-4B model extends this architecture with an expanded vibackbone, comprising 48 layers, hidden dimensionality of 1920, and 24 attention heads, thereby augmenting its capacity for advanced visual reasoning and scene interpretation. Its dynamic patch size mechanism (ranging from 1414 to 3232) ensures high-resolution adaptability. Our innovative approach to the Shakti-VLM model design incorporates several key optimizations to enhance stability, efficiency, and precision in multi-modal learning. To stabilize attention mechanisms, we utilized the QK-Norm[5], which applies RMS[15] normalization specifically to query and key vectors in attention layers. This rare optimization prevents gradient vanishing/explosion, ensuring robust attention score computations even in deeper architectures. We employ hybrid normalization strategy to further promote convergence stability. Pre-LayerNorm is applied to the initial layers, while RMSNorm[15] governs the remaining layers, facilitating smoother optimization. Specifically, in Shakti-VLM-1B, Pre-LayerNorm is utilized for the first 12 layers, with RMSNorm[15] applied to the next 24 layers. In contrast, Shakti-VLM-4B implements Pre-LayerNorm across the first 18 layers, followed by RMSNorm[15] for the subsequent 30 layers. For enhanced spatial encoding and improved object localization, we augment Rotary Position Embedding (RoPE)[14] with 2D absolute positional bias, strengthening scene comprehension. Additionally, our use of SiLU[16] and SwiGLU[17] activation functions ensures smoother gradient propagation, fostering fine-grained visual feature extraction essential for high-level visual understanding. The Shakti-VLM-1B model is trained on 487 billion tokens, equipping it with superior generalization capabilities across diverse visual-language tasks, ensuring high fidelity in contextual reasoning and information retrieval. In contrast, Shakti-VLM-4B is trained on dataset encompassing 782 billion tokens, fortifying its capacity for multi-modal alignment and high-precision reasoning in image summarization, contextual reasoning, and visual question answering."
        },
        {
            "title": "3.2 Projection Layer",
            "content": "A projection layer is employed in both models to transform visual features into visual tokens, ensuring their seamless integration with textual inputs. This transformation facilitates robust multi-modal representation learning, enhancing the models ability to align and process information across both modalities. The projected visual tokens are then concatenated with text embeddings and fed into the decoder."
        },
        {
            "title": "3.3 Decoder",
            "content": "The decoder component of the Visual Language Model facilitates the seamless integration of visual and textual modalities for comprehensive multi-modal understanding. The visual representations, processed through projection layer and encoded as visual tokens, are concatenated with textual inputs and subsequently fed into the decoder. The Shakti-VLM-1B employs the Shakti-500M model as its decoder, whereas the Shakti-VLM-4B integrates the Shakti-2.5B[18] model, both of which are optimized for multi-modal alignment and generative reasoning. Utilizing three-stage training pipeline, the decoders effectively synchronize visual and textual embeddings, facilitating superior performance across tasks such as OCR, image summarization, visual reasoning, and contextual understanding. This architectural design ensures high-precision multi-modal task execution with enhanced efficiency and accuracy."
        },
        {
            "title": "4 Training Details",
            "content": "The training framework for the Shakti-VLM-1B and Shakti-VLM-4B Visual Language Models (VLMs) is divided into three distinct stages: Pre-training Stage 1, Pre-training Stage 2, and Fine-tuning Stage 3. Each stage employs tailored training configurations to incrementally improve the models multi-modal comprehension and alignment capabilities. The training parameters shared between the models are summarized in Table 1, while Table 2 outlines the learning rate settings for each stage of training."
        },
        {
            "title": "4.1 Pre-training Stage 1",
            "content": "This stage focuses exclusively on training the decoder while the encoder remains frozen. The primary objective is to extend the decoders context length, thus enhancing its capacity for processing and understanding extended text sequences. For the Shakti-VLM-1B model, the Shakti 500M decoder is optimized to handle sequences of up to 16,384 tokens, whereas the Shakti 2.5B decoder in the Shakti-VLM-4B model is trained to accommodate sequences of up to 32,768 tokens. cosine learning rate scheduler is employed, with an initial learning rate of 3e-4 for the Shakti-VLM-1B model and 2e-4 for the 4B model. Gradient accumulation steps are set to 2 to ensure efficient learning over large sequence lengths, and rotary position embeddings with dynamic scaling are utilized to handle the extended context lengths. This stage is instrumental in establishing robust language modeling foundation, enhancing the models contextual retention and language comprehension over extended sequences. By isolating the training to the decoder, the models develop refined language generation capabilities before integrating multi-modal inputs in subsequent stages."
        },
        {
            "title": "4.2 Pre-training Stage 2",
            "content": "In this stage, the Multi-Layer Perceptron (MLP)[19] projector is initialized to bridge the visual and language representations. The primary focus is on training the vision encoder and projection layers to align visual and textual embeddings within the decoders input space. The decoder is kept frozen during this stage to prioritize cross-modal alignment. For the Shakti-VLM-1B model, the training configuration includes sequence length of 16,384 tokens, an image size of 448x448, and dynamic resizing to enhance robustness across varied visual inputs. The learning rate is set at 2e-5 with cosine learning rate schedule.In contrast to the Shakti-VLM-4B model, longer sequence length of 32,768 tokens is used, along with the same image size and dynamic resizing setup. The learning rate is set to 4e-5, also using cosine learning rate schedule. This stage is pivotal in ensuring effective alignment of visual and textual representations, providing robust foundation for fine-tuning the models on complex downstream tasks."
        },
        {
            "title": "4.3 Fine-tuning Stage 3",
            "content": "The final stage involves fine-tuning all three components: the encoder, projection layer, and decoder. The models are exposed to diverse set of image-text datasets to enhance their performance across range of vision-language tasks. Both the Shakti-VLM-1B and Shakti-VLM-4B models use learning rate of 4e-5 with cosine learning rate scheduler and weight decay of 0.01 to ensure regularization and mitigate overfitting. The image size is maintained at 448x448, with dynamic resizing enabled to ensure adaptability to varied visual contexts. This stage is crucial for aligning the visual and textual embeddings across multiple tasks, including Document Visual Question Answering (VQA), Visual Question Answering, and Multimodal Reasoning. Additionally, instruction tuning and in-context instruction tuning are employed to enhance the models responsiveness to diverse prompts and complex instructions. Reinforcement Learning from Human Feedback (RLHF)[6] is leveraged to refine vision-language outputs based on human preferences, further improving response quality and contextual coherence. Direct Preference Optimization (DPO)[7] is integrated to enhance model performance on specialized tasks, ensuring greater adaptability across various real-world applications. Collectively, this comprehensive training process ensures the models capacity to understand, reason, and accurately respond to complex multi-modal prompts with high precision and flexibility."
        },
        {
            "title": "4.4 Training Loss Analysis and Convergence",
            "content": "The training process of Shakti-VLM-1B and Shakti-VLM-4B was analyzed through loss graphs1 2, revealing their convergence behavior and stability. Shakti-VLM-1B began with high initial loss of approximately 10, which steadily decreased to around 1 over 35k steps, with noticeable stepwise drops likely due to scheduled learning rate adjustments, indicating more complex optimization trajectory. In contrast, Shakti-VLM-4B started with training loss of approximately 2.8, gradually reducing to around 1.8 after 20k steps, demonstrating smooth and stable convergence. The models were trained on 8A100 (40GB) GPUs, processing 487 billion tokens over 12 days for the 1B model and 782 billion tokens over 23 days for the 4B model. Both models achieved effective optimization, with Shakti-VLM-1B experiencing more dynamic loss variations compared to the steadier convergence of Shakti-VLM-4B."
        },
        {
            "title": "5 Dataset Details",
            "content": "The training of both the Shakti-VLM-1B and Shakti-VLM-4B Visual Language Models leverages diverse set of datasets spanning various tasks, with the objective of advancing the models multi-modal comprehension and performance. By Parameter LR Scheduler Type Max_seq_len Rope_theta Image Size Dynamic Size Shakti-VLM-1B Shakti-4B Cosine 16384 125000 448 True Cosine 32768 500000 448 True Table 1: This table presents the common training parameters for the Shakti-Shakti-VLM-1B and Shakti-4B models, focusing on key settings shared across both models during the training process. Training Stages Pre-Training Stage 1 Pre-Training Stage 2 Fine-Tuning Stage 3 Shakti-VLM-1B Shakti-4B 3e-4 2e-5 4e-5 2e-5 4e-5 4e-5 Table 2: This table outlines the learning rates employed for Shakti-VLM-1B and Shakti-4B models during different pre-training and fine-tuning stages, providing insights into the specific learning rates used to optimize each stage of the training process. exposing the models to broad spectrum of data modalities, both models acquire robust capabilities for visual-textual alignment, thus facilitating proficiency in complex vision-language tasks. Table 3 provides comprehensive summary of the datasets utilized throughout the three distinct training stages for both the Shakti-VLM-1B and Shakti-VLM-4B models, with particular emphasis on the supplementary datasets used exclusively for the Shakti-VLM-4B model to enhance its performance in visual reasoning, document analysis, and specialized vision-language tasks. Text-Only Data: In the initial stage, text-only datasets are employed to establish solid foundation for language comprehension and increase the context length of the decoder. The Dolma (Books subset)[20] is utilized to enhance the models general language modeling capabilities, while The Stack[21] is incorporated to improve code understanding. The FineWeb-Edu-dedup[22] dataset contributes to strengthening the models overall language understanding. This foundational stage enables the models to process and respond to text-based inputs with high degree of accuracy and coherence. Image and text captioning datasets play crucial role in aligning visual inputs with their corresponding text descriptions. LAION-400M[23] and LAION COCO[24] provide broad collection of image-caption pairs for general captioning tasks. COCOCaption[25] and TextCaptions[26] are employed to further improve caption generation. These datasets enable the models to produce highly accurate and contextually relevant captions for wide range of images, improving image-description alignment. Document analysis datasets enhance the models capabilities in processing and understanding structured and semistructured document images. PDFA[27] is utilized for document layout understanding, focusing on the structural relationships in document formatting. DocVQA[28] and Docmatrix[13] are used for document visual question answering, training the models to answer questions based on document content accurately. These datasets prepare the models for tasks involving complex document processing and information extraction. Visual question answering (VQA) datasets are designed to strengthen the models reasoning and answering capabilities for visual inputs. Datasets such as Visual-7W[29] and OCR-VQA[30] focus on answering questions derived from visual elements in both images and text-based documents. LLaVA-CoT-100k[31] and DataComp[32] provide additional training for context-based VQA tasks. This training equips the models to handle real-world visual and textual queries with precision and relevance. Instruction tuning and fine-tuning datasets are used to improve model adaptability for specialized and dynamic tasks. Leopard-instruct is employed for instruction tuning, allowing the models to interpret and follow diverse task instructions. MIMIC-IT[33] supports in-context instruction tuning to adapt to various prompts and queries. The cauldron and rlaif-v-formatted[34] are fine-tuning datasets that enhance the models performance on downstream vision-language Figure 1: Training Loss Curve for Shakti-VLM-1B:The graph shows the loss reduction from around 10 to 1 over 35k steps, with stepwise drops likely due to scheduled learning rate adjustments, reflecting more complex training trajectory. Figure 2: Training Loss Curve for Shakti-VLM-4B The graph illustrates the steady decline in training loss from approximately 2.8 to 1.8 over 20k steps, indicating stable convergence and effective optimization. tasks. RLAIF-V-Dataset[34] is used for reinforcement learning from human feedback (RLHF)[6], aligning the models outputs with human preferences. Specialized reasoning and multimodal tasks are supported by datasets such as ScienceQA[35], which focuses on science-based question answering with multi-modal inputs. This dataset improves the models ability to reason through complex visual and language information in logical and coherent manner. By leveraging this diverse set of datasets across different task categories, the Shakti-VLM-1B and Shakti-VLM-4B model develop comprehensive multi-modal alignment capabilities. This diverse training pipeline enables the models to excel at tasks such as image captioning, document analysis, visual reasoning, and visual question answering with high accuracy, adaptability, and contextual understanding across multiple domains."
        },
        {
            "title": "6 Evaluation and Results",
            "content": "We evaluated our Shakti Vision Language Models across diverse set of multimodal benchmarks, comparing their performance with contemporary models within similar parameter ranges. For the Shakti-VLM-1B model, comparisons were made against several popular VLM models in the 1B to 3B parameter range. For the Shakti-VLM-4B model, we compared leading models in the 4B to 8B parameter range. Our comprehensive benchmarking approach encompasses wide spectrum of multimodal understanding tasks, including document understanding, chart interpretation, mathematical reasoning, and general vision-language capabilities. The benchmark suite was carefully selected to evaluate multiple dimensions of model performance: OCR capabilities, visual reasoning, complex multimodal understanding, mathematical reasoning, and practical real-world applications. Pre-Training Stage 1 Stage 2 Used for both Shakti-VLM-1B and Shakti-VLM4B Dolma (Books subset) The Stack, FineWebEdu-dedup OBELICS PDFA LAION-400M"
        },
        {
            "title": "Used for only",
            "content": "Shakti-VLM-4B Dolma (Books subset) The Stack, FineWebEdu-dedup LAION COCO COYO DocVQA TextCaptions Visual-7W OCR-VQA DataComp COCOCaption Fine-Tuning Stage 3 PDFA Docmatrix Leopard-instruct MIMIC-IT ScienceQA RLAIF-V-Dataset LLaVA-CoT-100k the_cauldron rlaif-v_formatted Table 3: Datasets used across different training stages for both Shakti-VLM-1B and 4B models, highlighting additional datasets utilized exclusively for the 4B model to enhance multi-modal performance. The benchmark datasets represent diverse challenges in the multimodal domain, with varying degrees of complexity. This diversity allows us to thoroughly assess each models generalization capabilities across different task types, data distributions, and reasoning requirements. The benchmark results for Shakti-VLM-1B and Shakti-VLM-4B models are showcased in the table 4 and table 5 respectively along with the comparison models."
        },
        {
            "title": "6.1.1 Performance Highlights",
            "content": "Shakti-VLM-1B demonstrates exceptional performance across multiple benchmarks despite its compact size. The model achieves high performance in several key areas: MMMU (Multimodal Massive Multitask Understanding): Shakti-VLM-1B achieves 42.5% on the validation set, surpassing all comparison models of same parameter and compitative to the latest Qwen-2.5VL 3B[36] model. Document and Text Understanding: Strong performance on DocVQA , TextVQA, and OCRBench demonstrates the models robust text recognition and document understanding capabilities. Chart Understanding: Leading performance on ChartQA indicates superior ability to interpret and reason about visual data representations. General Multimodal Evaluation: Shakti-VLM-1B achieves the highest score on MME with 1910.62 points and MMStar with 50.13%, showcasing its balanced capabilities across diverse multimodal tasks. Mathematical Reasoning: Strongest performance on MathVista among models in its size class, demonstrating advanced visual mathematical reasoning capabilities and competitive to the latest model."
        },
        {
            "title": "6.1.2 Comparative Analysis",
            "content": "When compared to models of similar or larger sizes, Shakti-VLM-1B shows several notable strengths: Benchmarks Shakti-VLM-1B MolmoE-1B InternVL2-1B SmolVLM-2.25B MiniCPM-V-2.0-2.8B Qwen-2VL-2B InternVL2-2B Qwen-2.5VL-3B MMMUval DocVQAtest InfoVQAtest ChartQAtest TextVQAval OCRBench MMEsum MMStar MMMU Pro val VQA v2val Ai2d RealworldQA MathVista (testmini) MMT-Bench (test) MMVet HallusionBench MMBench (test) MathVision MathVerse Olympaid Bench BLINK MTVQA 42.5 87.96 56.8 79.56 80. 798 34.9 77.7 53.9 78 78. 684 36.7 81.7 50.9 72.9 70. 754 38.8 81.6 43.5 62.2 72. 701 38.2 71.9 49.1 70.1 74. 605 41.1 90.1 65.5 73.5 79. 794 36.3 86.9 58.9 76.2 73. 781 1910.62 1782.2 1794.4 1801.9 1808. 1872 1876.8 50.13 24.73 76.28 77. 64.82 46.2 57.4 44.9 40.07 42. 17.03 19 0.9 39.9 13.2 40. - 83.9 86.4 60.4 34 52. - - - - - - - - 39.4 - 69.5 64. 50.3 37.7 48.9 32.7 34 - 12.2 18.4 0.3 38.6 12.6 42. - 58.2 61.9 - 44.6 - - - - - - - - - 46.8 - 66.4 55. - 38.7 - - - - - - - - - - 71.2 - 62.9 - 54. - - - 19.7 21 - 44.4 20 49.8 - 67.6 - 57.3 - - - - - 15.8 25.3 0.4 43.8 10.9 53. 93.9 77.1 - 79.3 - - 55.9 31.6 - 81.5 - 62. - - - 77.6 21.2 - - - - Table 4: Benchmark Performance Comparison of Shakti-VLM-1B model against other VLM models in the parameter range of 1B to 3B parameters. Balanced Performance: While some comparison models excel in specific domains, Shakti-VLM-1B maintains high performance across broader spectrum of tasks, suggesting better generalization capabilities. Shakti-VLM-1B frequently outperforms models with significantly more parameters, such as SmolVLM2.25B[4] and MiniCPM-V-2.0-2.8B[8], highlighting the efficiency of its architecture and training methodology. Strong Document and Diagram Understanding: The model demonstrates particular strength in tasks requiring joint reasoning over text and visual elements, as evidenced by its leading performance on ChartQA and strong results on DocVQA and TextVQA. Mathematical Reasoning: Strong performance on MathVista demonstrates advanced visual mathematical reasoning capabilities, significantly outperforming MolmoE-1B[3] and MiniCPM-V-2.0-2.8B[8]."
        },
        {
            "title": "6.2.1 Performance Highlights",
            "content": "Shakti-VLM-4B demonstrates substantial improvements over its Shakti-VLM-1B counterpart and achieves excellent results across numerous benchmarks: Comprehensive Understanding: Exceptional performance on MMMU (59.78%), significantly outperforming all comparison models, indicating superior capabilities in complex multimodal reasoning tasks. Document Intelligence: The results on DocVQA, TextVQA and InfoVQA demonstrates the model capability in the document understanding."
        },
        {
            "title": "Benchmarks",
            "content": "Shakti-4B InternVL2-4B Phi-3-Vision-4B MiniCPM-V-2.6-8B Qwen2VL-7B Qwen2.5VL-7B"
        },
        {
            "title": "MMMU Pro val",
            "content": "VQA v2val Ai2d"
        },
        {
            "title": "RealworldQA",
            "content": "62.33 37.47 78.78 83.83 71.18 MathVista (testmini) 48.5 MMT-Bench (test) 66."
        },
        {
            "title": "HallusionBench",
            "content": "MMBench (test)"
        },
        {
            "title": "MTVQA",
            "content": "62.3 47.9 81.7 19.05 28.78 1. 50.11 16.02 59.78 92.92 77.3 85. 85.56 849 47.9 89.2 67.0 81. 74.4 788 46.1 - - 81. 70.9 639 2340.99 2064.1 1508.0 - - - 78.9 60.7 58.6 - 55.7 41.9 78.6 17.8 32 1. 46.1 15.3 - - - 76. 58.8 44.5 - 44.1 39 73. 17.4 24.1 - 58.3 - 49. 90.8 - - 80.1 852 2348. 57.5 - - - - 60. - 60 48.1 - 16.1 25. - 53 - 54.1 94.5 76. 83.0 84.3 845 2326.8 60.7 - - - 70.1 58.2 63.7 62. 50.6 83.0 16.3 31.9 - 53. 25.6 58.6 95.7 82.6 87.3 84. 864 - 63.9 41 - - - 68.2 63.6 67.1 52.9 82. 25.07 - - - - Table 5: Benchmark Performance Comparison of Shakti-VLM-4B model against other VLM models in the parameter range of 4B to 8B parameters. Visual Reasoning: The performance on ChartQA, MMStar , and MMVet showcases the models advanced visual reasoning abilities. Real-world Application: Highest scores on RealworldQA suggest superior practical applicability in everyday scenarios."
        },
        {
            "title": "6.2.2 Comparative Analysis",
            "content": "When compared to contemporary models in the 4B to 8B parameter range: Consistent Outperformance: Shakti-VLM-4B achieves high performance in most of the benchmarks, showcasing its consistency across variety of tasks. Efficiency vs. Larger Models: Despite having fewer parameters than Qwen2VL-7B[2] and MiniCPM-V-2.68B[8] and latest Qwen-2.5VL 7B[36], Shakti-VLM-4B achieves comparable and better performance across most benchmarks, highlighting its parameter efficiency. Balanced Capabilities: While some models demonstrate strength in specific domains, Shakti-VLM-4B maintains high performance across diverse task types, suggesting more balanced and generalizable capabilities. Mathematical and Visual Reasoning: Strong performance on complex reasoning tasks like MathVista, MMVet, and MMT-Bench demonstrates the models advanced reasoning capabilities. The comprehensive evaluation results presented above demonstrate that Shakti-VLM models achieve exceptional performance across diverse multimodal tasks, frequently outperforming contemporary models. These findings validate our architectural innovations and training methodology, positioning Shakti as highly competitive solution for real-world multimodal applications."
        },
        {
            "title": "6.3 Qualitative evaluation",
            "content": "We evaluated Shakti-VLM-1B, Shakti-VLM-4B, Qwen2VL-2B, and Qwen2.5VL-7B across multiple tasks3, 4, highlighting key performance distinctions. In descriptive tasks, Shakti-VLM-1B demonstrated greater contextual depth, particularly in historical and architectural analyses, whereas Qwen2VL-2B[2] prioritized concise factual reporting. For handwritten text extraction, both models exhibited high accuracy with minor spelling errors; however, Shakti-VLM-1B replicated source errors rather than correcting them. In multiple-choice question answering, both models accurately identified the correct responses, reflecting strong factual comprehension. Summarization tasks revealed notable differences, with Shakti-VLM-4B effectively capturing broader themes, while Qwen2.5VL-7B[36] introduced inaccuracies related to image captioning and datasets. In code generation, ShaktiVLM-4B provided both correct solutions and explanatory context, whereas Qwen2.5VL-7B omitted justifications. Visual reasoning assessments, such as identifying available parking slots, further demonstrated Shakti-4Bs superior interpretative accuracy, as Qwen2.5VL-7B misread indicators. Overall, Shakti models exhibited stronger contextual depth and reasoning capabilities, while Qwen models, though concise, occasionally introduced interpretative errors. These findings suggest that Shakti models are better suited for complex analytical tasks, whereas Qwen models favor brevity but may require refinement in reasoning-driven applications."
        },
        {
            "title": "7 Conclusion",
            "content": "Shakti VLM presents novel approach to vision-language modeling by emphasizing architectural efficiency and training optimization rather than sheer data volume. By incorporating QK-Normalization[5], hybrid normalization techniques, and enhanced positional encoding, Shakti-VLM-1B and Shakti-VLM-4B achieve competitive performance on various multimodal tasks such as document understanding, OCR extraction, and general reasoning. Despite using significantly fewer training tokens than comparable models, Shakti models outperform several state-of-the-art alternatives, demonstrating the effectiveness of our three-stage training strategy. These results highlight the potential of intelligent model design in advancing efficient and scalable multimodal AI solutions."
        },
        {
            "title": "8 Future Works",
            "content": "Future work on Shakti VLM presents several promising directions for further exploration. Scaling to larger models beyond 4B parameters could help assess whether the architectural innovations continue to yield efficiency improvements at greater scales. Enhancing data efficiency by exploring additional pretraining strategies, such as curriculum learning and contrastive learning, may reduce reliance on large-scale datasets. Fine-tuning for specialized domains, including medical imaging, legal document analysis, and financial reporting, can further expand its applicability. Optimizing real-time inference speed and efficiency is crucial for deployment in enterprise use cases. Additionally, expanding Shaktis VLM models capabilities to support multilingual and multimodal inputs, such as audio and video processing, will further enhance its versatility. Figure 3: Comparision of Shakti-1B and Qwen2VL-2B Results on different prompts. Figure 4: Comparision of Shakti-4B and Qwen2.5VL-7B Results on different prompts."
        },
        {
            "title": "References",
            "content": "[1] Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, Jakob Uszkoreit, and Neil Houlsby. An image is worth 16x16 words: Transformers for image recognition at scale, 2021. [2] Peng Wang, Shuai Bai, Sinan Tan, Shijie Wang, Zhihao Fan, Jinze Bai, Keqin Chen, Xuejing Liu, Jialin Wang, Wenbin Ge, Yang Fan, Kai Dang, Mengfei Du, Xuancheng Ren, Rui Men, Dayiheng Liu, Chang Zhou, Jingren Zhou, and Junyang Lin. Qwen2-vl: Enhancing vision-language models perception of the world at any resolution, 2024. [3] Matt Deitke, Christopher Clark, Sangho Lee, Rohun Tripathi, Yue Yang, Jae Sung Park, Mohammadreza Salehi, Niklas Muennighoff, Kyle Lo, Luca Soldaini, Jiasen Lu, Taira Anderson, Erin Bransom, Kiana Ehsani, Huong Ngo, YenSung Chen, Ajay Patel, Mark Yatskar, Chris Callison-Burch, Andrew Head, Rose Hendrix, Favyen Bastani, Eli VanderBilt, Nathan Lambert, Yvonne Chou, Arnavi Chheda, Jenna Sparks, Sam Skjonsberg, Michael Schmitz, Aaron Sarnat, Byron Bischoff, Pete Walsh, Chris Newell, Piper Wolters, Tanmay Gupta, Kuo-Hao Zeng, Jon Borchardt, Dirk Groeneveld, Crystal Nam, Sophie Lebrecht, Caitlin Wittlif, Carissa Schoenick, Oscar Michel, Ranjay Krishna, Luca Weihs, Noah A. Smith, Hannaneh Hajishirzi, Ross Girshick, Ali Farhadi, and Aniruddha Kembhavi. Molmo and pixmo: Open weights and open data for state-of-the-art vision-language models, 2024. [4] Hugging Face. Smolvlm:small yet mighty vision language model. https://huggingface.co/blog/smolvlm, 2024. Accessed: Feb. 22, 2025. [5] Alex Henry, Prudhvi Raj Dachapally, Shubham Pawar, and Yuxuan Chen. Query-key normalization for transformers, 2020. [6] Long Ouyang, Jeff Wu, Xu Jiang, Diogo Almeida, Carroll L. Wainwright, Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, John Schulman, Jacob Hilton, Fraser Kelton, Luke Miller, Maddie Simens, Amanda Askell, Peter Welinder, Paul Christiano, Jan Leike, and Ryan Lowe. Training language models to follow instructions with human feedback, 2022. [7] Rafael Rafailov, Archit Sharma, Eric Mitchell, Stefano Ermon, Christopher D. Manning, and Chelsea Finn. Direct preference optimization: Your language model is secretly reward model, 2024. [8] Yuan Yao, Tianyu Yu, Ao Zhang, Chongyi Wang, Junbo Cui, Hongji Zhu, Tianchi Cai, Haoyu Li, Weilin Zhao, Zhihui He, et al. Minicpm-v: gpt-4v level mllm on your phone. arXiv preprint arXiv:2408.01800, 2024. [9] Jinze Bai, Shuai Bai, Shusheng Yang, Shijie Wang, Sinan Tan, Peng Wang, Junyang Lin, Chang Zhou, and Jingren Zhou. Qwen-vl: versatile vision-language model for understanding, localization, text reading, and beyond, 2023. [10] Zhe Chen, Weiyun Wang, Hao Tian, Shenglong Ye, Zhangwei Gao, Erfei Cui, Wenwen Tong, Kongzhi Hu, Jiapeng Luo, Zheng Ma, Ji Ma, Jiaqi Wang, Xiaoyi Dong, Hang Yan, Hewei Guo, Conghui He, Botian Shi, Zhenjiang Jin, Chao Xu, Bin Wang, Xingjian Wei, Wei Li, Wenjian Zhang, Bo Zhang, Pinlong Cai, Licheng Wen, Xiangchao Yan, Min Dou, Lewei Lu, Xizhou Zhu, Tong Lu, Dahua Lin, Yu Qiao, Jifeng Dai, and Wenhai Wang. How far are we to gpt-4v? closing the gap to commercial multimodal models with open-source suites, 2024. [11] Zhe Chen, Weiyun Wang, Yue Cao, Yangzhou Liu, Zhangwei Gao, Erfei Cui, Jinguo Zhu, Shenglong Ye, Hao Tian, Zhaoyang Liu, Lixin Gu, Xuehui Wang, Qingyun Li, Yimin Ren, Zixuan Chen, Jiapeng Luo, Jiahao Wang, Tan Jiang, Bo Wang, Conghui He, Botian Shi, Xingcheng Zhang, Han Lv, Yi Wang, Wenqi Shao, Pei Chu, Zhongying Tu, Tong He, Zhiyong Wu, Huipeng Deng, Jiaye Ge, Kai Chen, Kaipeng Zhang, Limin Wang, Min Dou, Lewei Lu, Xizhou Zhu, Tong Lu, Dahua Lin, Yu Qiao, Jifeng Dai, and Wenhai Wang. Expanding performance boundaries of open-source multimodal models with model, data, and test-time scaling, 2025. [12] Marah Abdin, Jyoti Aneja, Hany Awadalla, Ahmed Awadallah, Ammar Ahmad Awan, Nguyen Bach, Amit Bahree, Arash Bakhtiari, Jianmin Bao, Harkirat Behl, Alon Benhaim, Misha Bilenko, Johan Bjorck, Sébastien Bubeck, Martin Cai, Qin Cai, Vishrav Chaudhary, Dong Chen, Dongdong Chen, Weizhu Chen, Yen-Chun Chen, Yi-Ling Chen, Hao Cheng, Parul Chopra, Xiyang Dai, Matthew Dixon, Ronen Eldan, Victor Fragoso, Jianfeng Gao, Mei Gao, Min Gao, Amit Garg, Allie Del Giorno, Abhishek Goswami, Suriya Gunasekar, Emman Haider, Junheng Hao, Russell J. Hewett, Wenxiang Hu, Jamie Huynh, Dan Iter, Sam Ade Jacobs, Mojan Javaheripi, Xin Jin, Nikos Karampatziakis, Piero Kauffmann, Mahoud Khademi, Dongwoo Kim, Young Jin Kim, Lev Kurilenko, James R. Lee, Yin Tat Lee, Yuanzhi Li, Yunsheng Li, Chen Liang, Lars Liden, Xihui Lin, Zeqi Lin, Ce Liu, Liyuan Liu, Mengchen Liu, Weishung Liu, Xiaodong Liu, Chong Luo, Piyush Madan, Ali Mahmoudzadeh, David Majercak, Matt Mazzola, Caio César Teodoro Mendes, Arindam Mitra, Hardik Modi, Anh Nguyen, Brandon Norick, Barun Patra, Daniel Perez-Becker, Thomas Portet, Reid Pryzant, Heyang Qin, Marko Radmilac, Liliang Ren, Gustavo de Rosa, Corby Rosset, Sambudha Roy, Olatunji Ruwase, Olli Saarikivi, Amin Saied, Adil Salim, Michael Santacroce, Shital Shah, Ning Shang, Hiteshi Sharma, Yelong Shen, Swadheen Shukla, Xia Song, Masahiro Tanaka, Andrea Tupini, Praneetha Vaddamanu, Chunyu Wang, Guanhua Wang, Lijuan Wang, Shuohang Wang, Xin Wang, Yu Wang, Rachel Ward, Wen Wen, Philipp Witte, Haiping Wu, Xiaoxia Wu, Michael Wyatt, Bin Xiao, Can Xu, Jiahang Xu, Weijian Xu, Jilong Xue, Sonali Yadav, Fan Yang, Jianwei Yang, Yifan Yang, Ziyi Yang, Donghan Yu, Lu Yuan, Chenruidong Zhang, Cyril Zhang, Jianwen Zhang, Li Lyna Zhang, Yi Zhang, Yue Zhang, Yunan Zhang, and Xiren Zhou. Phi-3 technical report: highly capable language model locally on your phone, 2024. [13] Hugo Laurençon, Andrés Marafioti, Victor Sanh, and Léo Tronchon. Building and better understanding visionlanguage models: insights and future directions, 2024. [14] Jianlin Su, Yu Lu, Shengfeng Pan, Ahmed Murtadha, Bo Wen, and Yunfeng Liu. Roformer: Enhanced transformer with rotary position embedding, 2023. [15] Biao Zhang and Rico Sennrich. Root mean square layer normalization, 2019. [16] Stefan Elfwing, Eiji Uchibe, and Kenji Doya. Sigmoid-weighted linear units for neural network function approximation in reinforcement learning, 2017. [17] Noam Shazeer. Glu variants improve transformer, 2020. [18] Syed Abdul Gaffar Shakhadri, Kruthika KR, and Rakshit Aralimatti. Shakti: 2.5 billion parameter small language model optimized for edge ai and low-resource environments, 2024. [19] Gurpreet Singh and Manoj Sachan. Multi-layer perceptron (mlp) neural network technique for offline handwritten gurmukhi character recognition. In 2014 IEEE International Conference on Computational Intelligence and Computing Research, pages 15, 2014. [20] Luca Soldaini, Rodney Kinney, Akshita Bhagia, Dustin Schwenk, David Atkinson, Russell Authur, Ben Bogin, Khyathi Chandu, Jennifer Dumas, Yanai Elazar, Valentin Hofmann, Ananya Harsh Jha, Sachin Kumar, Li Lucy, Xinxi Lyu, Nathan Lambert, Ian Magnusson, Jacob Morrison, Niklas Muennighoff, Aakanksha Naik, Crystal Nam, Matthew E. Peters, Abhilasha Ravichander, Kyle Richardson, Zejiang Shen, Emma Strubell, Nishant Subramani, Oyvind Tafjord, Pete Walsh, Luke Zettlemoyer, Noah A. Smith, Hannaneh Hajishirzi, Iz Beltagy, Dirk Groeneveld, Jesse Dodge, and Kyle Lo. Dolma: an open corpus of three trillion tokens for language model pretraining research, 2024. [21] Denis Kocetkov, Raymond Li, Loubna Ben Allal, Jia Li, Chenghao Mou, Carlos Muñoz Ferrandis, Yacine Jernite, Margaret Mitchell, Sean Hughes, Thomas Wolf, Dzmitry Bahdanau, Leandro von Werra, and Harm de Vries. The stack: 3 tb of permissively licensed source code. Preprint, 2022. [22] Guilherme Penedo, Hynek Kydlíˇcek, Loubna Ben allal, Anton Lozhkov, Margaret Mitchell, Colin Raffel, Leandro Von Werra, and Thomas Wolf. The fineweb datasets: Decanting the web for the finest text data at scale. In The Thirty-eight Conference on Neural Information Processing Systems Datasets and Benchmarks Track, 2024. [23] Christoph Schuhmann, Romain Beaumont, Richard Vencu, Cade Gordon, Ross Wightman, Mehdi Cherti, Theo Coombes, Aarush Katta, Clayton Mullis, Mitchell Wortsman, Patrick Schramowski, Srivatsa Kundurthy, Katherine Crowson, Ludwig Schmidt, Robert Kaczmarczyk, and Jenia Jitsev. Laion-5b: An open large-scale dataset for training next generation image-text models, 2022. [24] LAION. Laion-coco: 600m synthetic captions from laion-2b-en. https://laion.ai/blog/laion-coco/. [25] Xinlei Chen, Hao Fang, Tsung-Yi Lin, Ramakrishna Vedantam, Saurabh Gupta, Piotr Dollar, and C. Lawrence Zitnick. Microsoft coco captions: Data collection and evaluation server, 2015. [26] Oleksii Sidorov, Ronghang Hu, Marcus Rohrbach, and Amanpreet Singh. Textcaps: dataset for image captioningwith reading comprehension. 2020. [27] PixParse Team. pdfa-eng-wds dataset. https://huggingface.co/datasets/pixparse/pdfa-eng-wds, 2024. Accessed: 2025-02-22. [28] Minesh Mathew, Dimosthenis Karatzas, and CV Jawahar. Docvqa: dataset for vqa on document images. In Proceedings of the IEEE/CVF winter conference on applications of computer vision, pages 22002209, 2021. [29] Yuke Zhu, Oliver Groth, Michael Bernstein, and Li Fei-Fei. Visual7w: Grounded question answering in images, 2016. [30] Anand Mishra, Shashank Shekhar, Ajeet Kumar Singh, and Anirban Chakraborty. Ocr-vqa: Visual question answering by reading text in images. In 2019 International Conference on Document Analysis and Recognition (ICDAR), pages 947952, 2019. [31] Guowei Xu, Peng Jin, Hao Li, Yibing Song, Lichao Sun, and Li Yuan. Llava-cot: Let vision language models reason step-by-step, 2024. [32] Samir Yitzhak Gadre, Gabriel Ilharco, Alex Fang, Jonathan Hayase, Georgios Smyrnis, Thao Nguyen, Ryan Marten, Mitchell Wortsman, Dhruba Ghosh, Jieyu Zhang, Eyal Orgad, Rahim Entezari, Giannis Daras, Sarah Pratt, Vivek Ramanujan, Yonatan Bitton, Kalyani Marathe, Stephen Mussmann, Richard Vencu, Mehdi Cherti, Ranjay Krishna, Pang Wei Koh, Olga Saukh, Alexander Ratner, Shuran Song, Hannaneh Hajishirzi, Ali Farhadi, Romain Beaumont, Sewoong Oh, Alex Dimakis, Jenia Jitsev, Yair Carmon, Vaishaal Shankar, and Ludwig Schmidt. Datacomp: In search of the next generation of multimodal datasets, 2023. [33] Bo Li, Yuanhan Zhang, Liangyu Chen, Jinghao Wang, Fanyi Pu, Jingkang Yang, Chunyuan Li, and Ziwei Liu. Mimic-it: Multi-modal in-context instruction tuning, 2023. [34] Tianyu Yu, Haoye Zhang, Yuan Yao, Yunkai Dang, Da Chen, Xiaoman Lu, Ganqu Cui, Taiwen He, Zhiyuan Liu, Tat-Seng Chua, and Maosong Sun. Rlaif-v: Aligning mllms through open-source ai feedback for super gpt-4v trustworthiness. arXiv preprint arXiv:2405.17220, 2024. [35] Pan Lu, Swaroop Mishra, Tony Xia, Liang Qiu, Kai-Wei Chang, Song-Chun Zhu, Oyvind Tafjord, Peter Clark, and Ashwin Kalyan. Learn to explain: Multimodal reasoning via thought chains for science question answering. In The 36th Conference on Neural Information Processing Systems (NeurIPS), 2022. [36] Shuai Bai, Keqin Chen, Xuejing Liu, Jialin Wang, Wenbin Ge, Sibo Song, Kai Dang, Peng Wang, Shijie Wang, Jun Tang, Humen Zhong, Yuanzhi Zhu, Mingkun Yang, Zhaohai Li, Jianqiang Wan, Pengfei Wang, Wei Ding, Zheren Fu, Yiheng Xu, Jiabo Ye, Xi Zhang, Tianbao Xie, Zesen Cheng, Hang Zhang, Zhibo Yang, Haiyang Xu, and Junyang Lin. Qwen2.5-vl technical report, 2025."
        }
    ],
    "affiliations": [
        "SandLogic Technologies Pvt Ltd"
    ]
}