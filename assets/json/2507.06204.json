{
    "paper_title": "Differential Mamba",
    "authors": [
        "Nadav Schneider",
        "Itamar Zimerman",
        "Eliya Nachmani"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Sequence models like Transformers and RNNs often overallocate attention to irrelevant context, leading to noisy intermediate representations. This degrades LLM capabilities by promoting hallucinations, weakening long-range and retrieval abilities, and reducing robustness. Recent work has shown that differential design can mitigate this issue in Transformers, improving their effectiveness across various applications. In this paper, we explore whether these techniques, originally developed for Transformers, can be applied to Mamba, a recent architecture based on selective state-space layers that achieves Transformer-level performance with greater efficiency. We show that a naive adaptation of differential design to Mamba is insufficient and requires careful architectural modifications. To address this, we introduce a novel differential mechanism for Mamba, empirically validated on language modeling benchmarks, demonstrating improved retrieval capabilities and superior performance over vanilla Mamba. Finally, we conduct extensive ablation studies and empirical analyses to justify our design choices and provide evidence that our approach effectively mitigates the overallocation problem in Mamba-based models. Our code is publicly available."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 8 ] . [ 1 4 0 2 6 0 . 7 0 5 2 : r a"
        },
        {
            "title": "Differential Mamba",
            "content": "Nadav Schneider Ben-Gurion University, IAEC nadavsch@post.bgu.ac.il Itamar Zimerman Tel-Aviv University, IBM Research zimerman1@mail.tau.ac.il Eliya Nachmani School of Electrical and Computer Engineering, Ben Gurion University of the Negev, eliyanac@bgu.ac.il"
        },
        {
            "title": "Abstract",
            "content": "Sequence models like Transformers and RNNs often overallocate attention to irrelevant context, leading to noisy intermediate representations. This degrades LLM capabilities by promoting hallucinations, weakening long-range and retrieval abilities, and reducing robustness. Recent work has shown that differential design can mitigate this issue in Transformers, improving their effectiveness across various applications. In this paper, we explore whether these techniques, originally developed for Transformers, can be applied to Mamba, recent architecture based on selective state-space layers that achieves Transformer-level performance with greater efficiency. We show that naive adaptation of differential design to Mamba is insufficient and requires careful architectural modifications. To address this, we introduce novel differential mechanism for Mamba, empirically validated on language modeling benchmarks, demonstrating improved retrieval capabilities and superior performance over vanilla Mamba. Finally, we conduct extensive ablation studies and empirical analyses to justify our design choices and provide evidence that our approach effectively mitigates the overallocation problem in Mamba-based models. Our code is publicly available. https://github.com/NadavSc/Diff-Mamba"
        },
        {
            "title": "Introduction",
            "content": "Designing enhanced architectures for deep sequence modeling is pivotal task in the ML community, as sequence models drive breakthroughs such as ChatGPT [1] in NLP and Stable diffusion [32] in computer vision, which are fundamental to modern generative models. However, these models face two major challenges as: efficiency, constrained by the quadratic time complexity in sequence length of the Transformer architecture [36], and robustness, which is hindered by inconsistency, reliability issues, and hallucinations, leading to sub-optimal performance. Our work addresses these challenges by enhancing the robustness of recent efficient architectures with sub-quadratic complexity such as Mamba [16], making them more reliable and robust. To address this robustness issue, we aim to reduce the over-allocation of attention to irrelevant context across hidden layers, which often leads to noisy representations. Our approach is inspired by Ye et al. [43], who mitigated this problem in transformers through differential design. This architectural modification was the core of the Diff-Transformer, transformer variant that demonstrated improved empirical performance, including greater robustness, enhanced retrieval and long-context capabilities compared to the original model. Our focus is on improving the recently introduced Mamba architecture, which builds upon selective state-space layers (S6). This architecture is known for its efficiency, introducing sub-quadratic complexity in sequence length while also enabling auto-regressive decoding with complexity that remains independent of sequence length. Beyond these efficiency advantages, recent studies have demonstrated that Mamba-based architectures can match or even surpass the SoTA performance of transformers, even at relatively large scales. For example, Falcon-Mamba 7B [46], pure Mamba model, matches the performance of LLaMA3-8B on several language tasks. Additional notable examples of Mambas integration in LLMs include Jamba [23], Zamba [14], Hymba [11], and others [39, 37, 31]. More importantly, recent Mamba-based models have demonstrated remarkable performance as reasoning models [28, 40], underscoring their central role in the ongoing test-time scaling revolution, as exemplified by models such as OpenAIs O1 [20] and DeepSeeks R1 [19]. While the over-allocation problem is general issue not specific to any architecture, we hypothesize that Mamba-based LLMs have stronger tendency toward over-allocation compared to transformers. This is primarily due to two factors: (i) Mamba is softmax-free architecture, meaning it lacks the exponential scaling effect that helps suppress irrelevant attention weights, and (ii) as state-based model, Mamba operates locally and cannot directly process distant tokens without considering all intermediate tokens, resulting in the dispersion of important tokens among irrelevant ones. This leads to our central research question: Can differential design be leveraged to improve the robustness of Mamba models? where we hypothesize that improving robustness by mitigating the over-allocation problem can enhance the models general capabilities, improve retrieval and long-context processing, and potentially reduce issues related to consistency and hallucinations in LLMs. We provide positive answer to this question by showing that, although naive implementation of differential mechanisms does not improve Mamba architectures, more carefully designed mechanism does. Through systematic evaluation on language tasks, ablation studies, and empirical analysis, we conclude that our variant is favorable compared to the vanilla Mamba. Our main contributions are as follows: (i) We present Diff-Mamba, modification of the Mamba architecture inspired by Diff-Transformer, which mitigates the problem of over-allocating attention scores to irrelevant context and improves the general language modeling abilities of the model. (ii) Through series of ablation studies and empirical analysis using mechanistic interpretability tools, we justify our design choices and demonstrate that the intermediate representations obtained from our method are indeed less noisy. (iii) Finally, we show that Diff-Mamba demonstrates improved retrieval and long-context capabilities compared to Mamba. This is particularly important, as recurrent LLMs such as Mamba are primarily designed to address the inefficiencies caused by the quadratic complexity of transformers, which becomes especially critical in long-context regimes. Accordingly, Diff-Mamba achieves improvements over Mamba precisely in the domain where such architectures are most needed [6]."
        },
        {
            "title": "2 Background",
            "content": "Here we describe the scientific context and introduce the key terminology for discussing our method. 2.1 Differential Transformer Self-Attention Self-Attention is fundamental component of Transformer architectures [36], has significantly shaped recent advances in both NLP and computer vision. This mechanism enables dynamic focus allocation by capturing pairwise token dependencies, allowing the model to determine the relative importance of each token within sequence. Mathematically, it is expressed as: Attention(Q, K, ) = αV, α = softmax (cid:19) (cid:18) QK dk (1) In this formulation, Q,K, and represent the queries, keys, and values, respectively, while dk denotes the key dimension. Transformers extend this mechanism by employing parallel attention heads, enabling the model to capture broader spectrum of dependencies. Differential Attention To address the problem of over-allocation of attention to irrelevant tokens, Ye et al. [43], introduced Diff Transformer, mechanism that reduces attention noise through 2 differential denoising by splitting each attention head into two and subtracting one attention map from the other. This mechanism can be described as follows: Dif Attention(Q1, K1, Q2, K2, ) = (α1 λα2), αi = softmax (cid:33) (cid:32) QiKi dk (2) where λ is is learnable scalar. To better improve the training dynamics, λ is re-parameterized and Group Normalization [42] is applied at the end of each head (post-subtraction). 2.2 State-Space Layers State-space layers were first introduced in Gu et al. [17] and were later substantially improved by the S4 model [18]. Since then, they have demonstrated strong performance across variety of domains, including NLP [12, 26], audio generation [15], image modeling [3, 27], long-horizon video understanding [38], reinforcement learning [9, 25], and speech recognition [33]. These models implement linear recurrent update rules derived from time-invariant state-space formulations, which can be efficiently computed in parallel using convolutions and with sub-quadratic complexity. 2.3 Mamba and Selective State-Space Layers Mamba block processes signal RLD where is the hidden dimension and is the number of tokens. Its core mechanism is the S6 layer, and it forward path formalized by: = σ(Conv1D(Linear(U )), = σ(Linear(U )), = S6(X), ˆY = Linear(Y Z) (3) where is the input to the S6 layer, and to the S6 layers, X, Z, Y, ˆY RLD. The function σ represents SiLU activation, and represents element-wise multiplication with the gating branch. Each Mamba block is primarily parameterized by linear and convolutional layers, along with the internal components of the S6 layer described below. S6 The S6 layer is the most popular variant of SSMs, and it employ real, diagonal and selective SSM. Standard real and diagonal SSMs parameterized by diagonal transition matrix RN , input and output matrices B, RN 1 where is the state size, and timescale R. Each channel of such an SSM can be viewed as mapping from an input scalar sequence to an output scalar sequence via the following recurrent rule: ht = Aht1 + Bxt, yk = Cht, = fA(A, ), = fB(B, ) (4) where fA, fB are discretization functions, and the discrete system matrices are RN and RN 1. The recurrent rule in Eq. 4 can be computed efficiently in parallel on modern hardware accelerators using work-efficient parallel scans [35] or simple scalar convolution via FFTs [17]. Note that Eq. 4 is map from RL to RL, and to process channels, multiple independent instances are used. The S6 layer differs from standard SSMs by employing selective mechanism, where the system matrices are input-dependent. As result, the system becomes time-invariant, with the per-step system matrices determined by the entire set of channels and then applied to process each channel independently. The entire mechanism can be computed by: SB, SC RN D, RDN and R1D to define the time-variant matrices as follows: Bt = SBXt, Ct = SCXt, = softplus(SXt), At = exp(tA), Bt = tBt (5) and the time-variant recurrent rule by: ht = Atht1 + Btxt, yk = Ctht (6) We study the many-to-one setting, where the model processes an entire input sequence to produce single output. This regime is widely used in NLP, encompassing both auto-regressive next-token prediction and sequence classification tasks. 3 2.3.1 Mamba as Implicit Attention The connection between Mamba and linear attention layers is well-established in [2, 8, 34]. Specifically, it has been shown that the time-variant recurrent update rule of S6 for single channel (see Eq. 6) can be explicitly unrolled into the linear attention formulation = AX when is an implicit attention matrix defined by: C1 B1 C2 A2 B1 ..."
        },
        {
            "title": "0\nC2 ¯B2\n...",
            "content": "CLΠL k=2 Ak B1 CLΠL k=3 Ak B2 0 . . . CL BL 0 (7) This perspective is further extended by Zimerman et al. [45], who generalize the interpretation of S6 as implicit attention from Eq. 7 to encompass most components of the Mamba block, including activations, normalization layers, convolutional layers, and the gate branch, into unified implicit attention formulation. Data-Controlled Linear Operators The formulation of data-controlled linear operators was first introduced by Poli et al. [30], who demonstrated that self-attention can be viewed as an expressive form of such operators. This principle guided the authors in designing the Hyena layer. Subsequently, Ali et al. [2] showed that S6 layers could also be unified under an implicit variant of this formulation, which Zimerman et al. [45] further extended to additional architectures, including the entire Mamba block, RWKV [29], and Griffin [10]. This extended perspective inspired our approach, leading us to interpret differential design as method to implicitly parameterize less noisy data-controlled linear operators. This insight motivated our decision to apply differential design at the Mamba level."
        },
        {
            "title": "3 Method",
            "content": "We begin with the simplest implementation of incorporating the differential mechanism into the Mamba block. Our approach is inspired by the Differential Transformer [43], which applies subtraction at the attention-level rather than at the transformer level. This mechanism is built on top of self attention and it can be described as: [1, 2] : Qi = XW , Ki = XW , = XW (8) Q1K 1 where is the softmax function and λ is uniquely parameterized to ensure stability and improve training dynamics: Q2K 2 Diff Attn(x) = ) λS( S( (9) (cid:16) (cid:17) ) λ = exp(λq1 λk1) exp(λq2 λk2) λinit (10) To better adapt this technique to Mamba models, we reinterpret differential attention through the lens of data-controlled linear operator [30], leading to the following formulation: Diff Attn(x) = AV, = A1 λA2, A1 = S( Q1K 1 ), A2 = S( Q2K 2 ) (11) here, the matrix defines data-controlled linear operator. 3.1 Diff S6 Eq. 11 defines straightforward approach to implementing differential Mamba by subtracting values obtained from S6 layers instead of attention layers. This builds upon two key similarities between S6 and attention: (i) S6 in Mamba serves the same role as attention in Transformerscapturing interactions between tokens, and (ii) S6 layers have been shown to be an implicit form of causal linear attention. Thus, incorporating the differential mechanism into the Mamba block can be achieved by: 4 Figure 1: Comparative illustration of our variants Diff-Mamba and Diff-S6 versus the original Mamba architecture, where is elementwise multiplication, σ is the SILU activation, Linear and Conv1D are standard linear projection and 1-dimensional convolution layers, and stands for normalizations. Diff S6(X) = S61(X) λS62(X) (12) where λ is defined similarly to Eq. 10. Similar to Eq. 11, this formulation can be rewritten in the form of data-controlled linear operator: Diff S6(x) = (A1 λA2)X = AX (13) where A1 and A2 are the implicit attention matrices of S6 controlled by the system matrices Aij, Bij and Cij for any time-step [L] and model index 1, 2 defined as follows: Ci1 Bi1 Ci2 Ai2 Bi1 ... 0 Ci2 Bi2 ... CiLΠL k=2 Aik Bi1 CiLΠL k= Aik Bi2 0 0 . . . CiL BiL 0 (14) One crucial difference between Diff S6 and Diff Attention (see Eqs. 12 and 9) is that Diff Attention subtracts elements on the same scale, as softmax produces values in the range [0,1]. In contrast, S6 produces unnormalized and unbounded outputs. To address this discrepancy, we introduce an additional normalization step denoted by N: N-Diff S6(X) = N(S61(X) λS62(X)) (15) For simplicity, we define λ as: (16) where λ is learnable parameter λ RD used to parameterize λ as positive and more stable weight. λ = Sigmoid( (cid:88) λ) + λinit 3.2 Diff-Mamba However, as detailed in the results section, Diff S6 does not perform well and, in practice, falls short of standard Mamba layers. We suspect this arises from S6 being too simple and not functioning as general-purpose, expressive mixing alternative to attention layers. Consequently, it fails to leverage the full potential of differential techniques. To address this limitation, we draw inspiration from Zimerman et al. [45], who demonstrated that the entire Mamba block can function as an alternative mixing mechanism to attention by formulating it as data-controlled linear operator. def NormalizedDiffMamba(X, λ, Mamba): Length, Dim) # X.shape = (Batch, X_2 = X.repeat(1, 1, 2) = Mamba(X_2) Y_n = PostMambaNorm(Y) Minuend = Y_N[:, :, Dim // 2:] Subtrahend = Y_N[:, :, : Dim // 2] = Minuend - λ*Subtrahend O_Prog = Out_Proj(O) O_N = PostSubNorm(out_prog) return O_N * (1 - lambda_init) Figure 2: Parallel Implementation of the Normalized Differential Mamba on top of the Mamba layer. 5 In particular, the authors show that Mamba can be reformulated as implicit attention by: Mamba(X) = AX, = (cid:16) SILU(Linear(x)) (cid:17) (cid:16) ˆα diag Sig(Conv(x)) (cid:17) )M (17) where is matrix representing the convolution layer, and ˆα is the linear operator corresponding to S6, as formalized by Ali et al. [2]. This formulation characterizes Mamba as data-control linear operator with richer and more expressive implicit attention matrices. Building on this insight, we introduce Diff-Mamba, mechanism that extends the differential approach to the full Mamba block: Similar to Eqs. 11 and 13, this can be rewritten as data-controlled linear operator, defined by: Diff-Mamba(X) = Mamba1(X) λ Mamba2(X) (18) Diff-Mamba(X) = AX, = A1 λA2 (19) key distinction between Diff Attention and Diff-Mamba is that the latter applies subtraction across broader set of components, as illustrated in Figure 1. Similar to the normalized Diff S6 variant in Eq. 15, we add normalization term: N-Diff-Mamba(x) = N(Mamba1(x) λMamba2(x)) (20) Resulting in the normalized Diff-Mamba mechanism, which is our primary contribution. Finally, following Ye et al. [43], we multiply the output of all variants by 1 λinit. 3.3 Overall Architecture Following the Mamba architecture, our model avoids interleaving sequence mixers such as Mamba or self-attention with MLPs. Instead, it is composed entirely of Diff-Mamba blocks, augmented with standard deep learning components such as token embedding and unembedding layers, skipconnections, and normalization layers. Efficient Implementation Direct computation of the normalized Diff-Mamba (Eq. 20) through naive approach can nearly double the inference latency. To mitigate this, we parallelize the computation of both the minuend and subtrahend within single forward pass of the Mamba by partitioning the model into two independent paths. To preserve the computational complexity , parameter count, and memory footprint of the original Mamba architecture, we avoid the standard practice of channel expansion within the Mamba block, where traditionally the number of channels is doubled. Instead, we replicate the input representations across the channel dimension, effectively retaining the model size. visualization of this parallel implementation is provided in Figure 2."
        },
        {
            "title": "4 Experiments",
            "content": "Table 1: Final performance of Mamba and Diff-Mamba across model sizes. All models were trained for 40 epochs on each dataset. Trends shown in Figure 3. 6 6 Model Dataset # Layers # Params PPL () 167M 22.416 169M 22. Wikitext-103 Mamba Diff-Mamba Wikitext-103 Mamba Wikitext-103 Diff-Mamba Wikitext-103 In this section, we empirically evaluate the effectiveness of the Diff-Mamba architecture. We begin in Section 4.1 by demonstrating that Diff-Mamba outperforms the original Mamba architecture in small-scale language modeling tasks across multiple datasets. In Section 4.2, we justify our key design decisions through comprehensive series of ablation studies. Subsequently, in Section 4.3, we showcase the superior retrieval performance of Diff-Mamba relative to Mamba. Finally, in Section 4.4, we utilize tools from the domain of mechanistic interpretability, such as Tuned-lens [4], to examine the internal representations of Diff-Mamba in comparison to Mamba, empirically validating that our differential approach effectively reduces noise in intermediate representations. full description of our experimental setup is provided in Appendix A. Mamba Diff-Mamba Mamba Diff-Mamba Mamba Diff-Mamba Mamba Diff-Mamba 255M 20.413 259M 20.012 Enwik8 Enwik8 Enwik8 Enwik8 127M 129M 127M 129M 255M 259M 255M 259M Text8 Text8 Text8 Text8 2.321 2.314 2.422 2. 2.416 2.396 2.525 2.479 12 12 12 12 12 12 6 6 6 6 Figure 3: Comparison of test curves through the training for Mamba and Diff-Mamba. The top row shows results for 6-layer models, and the bottom row for 12-layer models. Columns correspond to datasets: Enwik8 (left), Text-8 (center), and WikiText-103 (right). 4.1 Language Modeling To evaluate the performance of Diff-Mamba relative to Mamba on general NLP tasks, we train both models from scratch using comparable model sizes and an identical training setup, including the same codebase [18], datasets, and hyperparameters. We focus on three widely used benchmarks: WikiText-103, Text8, and Enwik8, and experiment with models of varying depth. The final results are reported in Table 1. To provide more comprehensive view of the optimization process, we include test curves through the training in Figure 3. As shown in Table 1, Diff-Mamba outperforms Mamba across all evaluated environments, achieving consistently lower loss. In particular, for the 12 layer model, Diff-Mamba improves over Mamba by 0.4 perplexity on WikiText-103, 0.046 bits per byte (bpb) on Text8, and 0.041 bpb on Enwik8. For 6 layer model, Diff-Mamba improves over Mamba by 0.201 on WikiText-103, 0.02 on Text8, and 0.007 on Enwik8. Interestingly, we observe that as the number of layers increases, the differential design in Mamba becomes increasingly effective. possible explanation for this is that, in the lower layers, the dependencies captured by the implicit attention matrices are shorter and simpler. In these cases, Mamba can manage overallocation effectively without the need for differential mechanism. However, in the upper layers, the dependencies become more complex, spanning longer ranges [5] and exhibiting more diverse patterns. This amplifies the impact of overallocation, thereby making the benefits of the differential design more pronounced. Furthermore, the training curves in Figure 3 provide insight into the optimization properties of Diff-Mamba, showing that it consistently outperforms Mamba and achieves faster convergence. We hypothesize that this phenomenon arises from the fact that the differential design reduces the amount of noise, which appears to be critical for improving convergence [21, 44]. 4.2 Ablations Analysis To validate our design decisions regarding (i) applying the differential operation at the S6 versus Mamba layer, (ii) incorporating an additional normalization sub-layer before subtraction, and (iii) reparameterization λ RD to scalar, we conducted dedicated ablation experiments on the Text8 benchmark. Table 2 summarizes the results. All models share an identical parameter count and were trained with same hyper-parameters that optimized for the baseline model. It can Table 2: Ablation study comparing (i) the scope at which the differential mechanism is applied (Diff-S6 vs. DiffMamba), (ii) the effect of including normalization (\"w. Nrm\") versus excluding it (\"w.o Nrm\"), and (iii) the importance of reparameterization for λ (\"re. λ\"). All models were trained on full Text8 with an identical parameter count. Reported values are test perplexity (PPL) on epoch 10. Lower is Better. Model w.o Nrm w. Nrm # Params Mamba Diff-S6 Diff-S6 + re. λ Diff-Mamba Diff-Mamba + re. λ 2.577 2.520 2.529 2.508 2. 2.512 2.517 2.493 2.503 127M 128M 128M 128M 128M 7 (a) BABILong Finetuned (b) Zero-Shot Figure 4: Retrieval Abilities: Comparison of Diff-Mamba and Mamba models, each with 370M parameters, under both fine-tuned and zero-shot settings across five retrieval tasks from BABILong. The x-axis represents the context length, and the y-axis corresponds to the task index. Each cell displays the ratio in which one model outperforms the other. Green cells indicate wins by Diff-Mamba, while red cells indicate wins by Mamba. be seen that all three design choices are justified, specifically Diff-S6 outperforms Mamba without normalization, while Diff-Mamba outperforms Diff-S6. Incorporating additional normalization leads to improved results, yielding gains of 0.015 and 0.008 perplexity for Diff-Mamba and Diff-S6, respectively. Finally, λ reparameterization doesnt contribute to better performance demonstrated both in Diff-S6 and Diff-Mamba models. 4.3 Retrieval The Diff-Transformer exhibits significantly improved retrieval capabilities compared to the original Transformer architecture. Consequently, we conduct dedicated experiments in both zero-shot and fine-tuning settings across five retrieval tasks from the BABILong benchmark [22] to evaluate whether these enhanced retrieval abilities transfer to Diff-Mamba. BABILong comprises diverse array of reasoning tasks, including fact chaining, simple induction, and deduction, where relevant facts are embedded within lengthy natural language passages of varying context lengths. This setup provides rigorous benchmark for evaluating models ability to retrieve and reason over extended contexts. In this experiment, Diff-Mamba was developed by substituting the final 12 layers of the pretrained Mamba 370M model with Diff-Mamba layers. To ensure fair comparison, both Diff-Mamba and Mamba were fine-tuned on 2B tokens of the PG19 dataset and subsequently evaluated in zero-shot manner on five tasks from BABILong. Furthermore, both of them were fine-tuned on BABILong tasks with up to 1k tokens to facilitate more targeted comparison, ensuring that their learning processes were aligned with the same objectives. Results (Figure 4) are reported on the test set exclusively. Each cell displays the ratio in which one model outperforms the other. Green indicates wins by Diff-Mamba, while red indicates wins by Mamba. As evident from the results, Diff-Mamba consistently outperforms Mamba at longer context lengths, exhibiting slower performance degradation and larger ratio score as context length increases. Although its performance in the zero-shot setting is partially affected by suboptimal alignment during PG19 fine-tuning, Diff-Mamba maintains strong results in the zero-shot scenario, underscoring its capacity to attend effectively to relevant contextual information. Diff-Mamba wins consistently in the BABILong finetuned test, achieving ratio of up to 2.11, and wins the long context tasks in the zero-shot evaluation by up to 3.5. Overall, our findings demonstrate that Diff-Mamba achieves superior retrieval performance, particularly in long-context scenarios across both BABILong fine-tuned and zero-shot settings. 4.4 Noise Analysis in Intermediate Representations Our empirical analysis in Section 4.3 and 4.1, as well as the results of Diff Transformer suggest that the differential design can mitigate the overallocation problem and improve general performance. To further investigate the underlying causes of this phenomenon, we analyze the models internal representations using tools from the field of mechanistic interpretability. In particular, we leverage the tuned lens [4] - method designed to examine intermediate representations by training an affine probe to map activations at each layer to the models final prediction, thereby enabling layer-wise 8 Figure 5: Measuring Signal-to-Noise Ratio: Noise in intermediate representations is assessed using controlled setup integrating well-defined retrieval task with tools from the domain of model understanding and mechanistic interpretability. The y-axis represents the probability of predicting the desired needle token, where lower values indicate higher noise. The x-axis denotes various layers within the model where intermediate noise is measured. Models are fine-tuned BABILong from Subsec. 4.3. Results show the average needle probabilities in each layer on 2k examples in test BABILong questions of up to 1k tokens. interpretability and insight into the models internal computation. Building on this tool, we measure the signal-to-noise ratio in the hidden representations of Diff-Mamba compared to standard Mamba models. The Tuned-lens tool is used to project the representations from each layer into predictions for the next token on the retrieval task. By measuring the predicted probability of the needle token, we can estimate the signal-to-noise ratio at different layers. Notably, as can be seen in Figure 5, across the majority of layers Diff-Mamba exhibits higher signal-to-noise ratio compared to Mamba. This difference is especially pronounced in the early layers, where the predicted probability of the needle token in Diff-Mamba is several orders of magnitude higher. This analysis empirically demonstrates that Diff-Mamba produces less noisy representations, directly aligning with the key principles underlying the differential mechanism, which is designed to mitigate the overallocation problem. The linear projections for tuned-lens on both Diff-Mamba and Mamba were trained using the same configuration as in the original paper. 4.5 Early Results at Medium Scale To thoroughly evaluate Diff-Mamba, we trained both the Mamba and Diff-Mamba models from scratch, each with 370M parameters, on 50-billion-token subset of The Pile dataset [13]. After an ablation study (Figure 10 in the Appendix), the most effective Diff-Mamba variant for scalable performance was found to be repeated alternation of Mamba and Diff-Mamba layers throughout the network. This variant is designed to combine the strengths of both worlds: it performs better than both Mamba and Diff-Mamba, and benefits from the improved robustness, long-context, and retrieval capabilities of Diff-Mamba. Then, we evaluated the models via zero-shot tests on the LongCrawl64 dataset [7], long-sequence subset of RedPajama-v2 [41] designed particularly for research on long-context modeling. In Figure 6 (a), the Per-token loss was calculated over the dataset following [24]. In addition, in Figure 6 (b), we calculated PPL on the test subset of The-Pile. Diff-Mamba demonstrates impressive long context capabilities, maintaining loss per token of around 9.98 across different context lengths, while Mamba increases significantly as the context grows. Furthermore, Diff-Mamba outperforms Mamba by PPL scores of 0.131, and 1.445 on The Pile and PG19 test sets correspondingly (Figure 6). We consider the positive results at medium scale promising, suggesting that alternating between Mamba and Diff-Mamba layers yields more effective, scalable, and robust architecture. Model Pile PG19 Mamba Diff-Mamba 11.212 11.081 28.064 26. (b) PPL results on test set of The Pile and PG19. (a) The x-axis is the token index, and the y-axis is the corresponding per-token loss. Figure 6: Evaluation of Mamba and Diff-Mamba in medium scale after pre-training on The Pile dataset for 50B tokens. Diff-Mamba excels in both tests."
        },
        {
            "title": "5 Discussion: The Importance of Differential Design",
            "content": "A key question that arises from the empirical findings presented in this paper and in Diff-Transformer is why differential design is so effective and what the underlying causes of its power are. Our work sheds light on this question by first showing that the problem of over-allocating attention to irrelevant context is not phenomenon unique to transformers, but rather general challenge in architectural research. Moreover, while previous work has primarily motivated the differential design through analogies to noise-canceling headphones and differential amplifiers, and supported it with strong empirical performance, we take this step further. In Figure 5, we present quantitative analysis using tools from the field of model understanding, showing that the intermediate representations in Diff-Mamba models exhibit higher signal-to-noise ratio compared to their non-differential Mamba counterparts, providing empirical support for the motivation previously proposed in the literature. Yet, the underlying reasons behind the empirical success of differential design remain largely unexplored, and further investigation from both theoretical and empirical perspectives is required to advance progress in this important direction."
        },
        {
            "title": "6 Conclusions and Limitations",
            "content": "In this paper, we introduced Diff-Mamba, variant of the Mamba architecture that leverages differential design principles to mitigate the problem of attention score over-allocation to irrelevant tokens, thereby enhancing overall performance. Although the experimental results show promising improvements over language modeling and retrieval tasks, we did not provide rigorous theoretical framework explaining precisely why differential designs improve Transformer or Mamba-based LLMs. Developing such theoretical justification is left as future work. Additionally, our results are limited to small-to-medium scale experiments due to constraints imposed by our academic budget. Finally, it remains an open question whether differential design principles can be effective in other domains, beyond NLP tasks, for instance, to domains such as computer vision, graph modeling, or time-series analysis."
        },
        {
            "title": "References",
            "content": "[1] Josh Achiam, Steven Adler, Sandhini Agarwal, Lama Ahmad, Ilge Akkaya, Florencia Leoni Aleman, Diogo Almeida, Janko Altenschmidt, Sam Altman, Shyamal Anadkat, et al. Gpt-4 technical report. arXiv preprint arXiv:2303.08774, 2023. [2] Ameen Ali, Itamar Zimerman, and Lior Wolf. The hidden attention of mamba models. arXiv preprint arXiv:2403.01590, 2024. [3] Ethan Baron, Itamar Zimerman, and Lior Wolf. 2-d ssm: general spatial layer for visual transformers. arXiv preprint arXiv:2306.06635, 2023. [4] Nora Belrose, Zach Furman, Logan Smith, Danny Halawi, Igor Ostrovsky, Lev McKinney, Stella Biderman, and Jacob Steinhardt. Eliciting latent predictions from transformers with the tuned lens. arXiv preprint arXiv:2303.08112, 2023. [5] Assaf Ben-Kish, Itamar Zimerman, Shady Abu-Hussein, Nadav Cohen, Amir Globerson, Lior Wolf, and Raja Giryes. Decimamba: Exploring the length extrapolation potential of mamba. In The Thirteenth International Conference on Learning Representations, 2025. URL https://openreview.net/forum?id=iWSl5Zyjjw. [6] Assaf Ben-Kish, Itamar Zimerman, Jehanzeb Mirza, James Glass, Leonid Karlinsky, and Raja Giryes. Overflow prevention enhances long-context recurrent llms. arXiv preprint arXiv:2505.07793, 2025. [7] Jacob Buckman. Longcrawl64: Long-Context Natural-Language Dataset, 2024. [8] Tri Dao and Albert Gu. Transformers are ssms: Generalized models and efficient algorithms through structured state space duality. arXiv preprint arXiv:2405.21060, 2024. [9] Shmuel Bar David, Itamar Zimerman, Eliya Nachmani, and Lior Wolf. Decision s4: Efficient sequence-based rl via state spaces layers. In The Eleventh International Conference on Learning Representations, 2022. [10] Soham De, Samuel Smith, Anushan Fernando, Aleksandar Botev, George Cristian-Muraru, Albert Gu, Ruba Haroun, Leonard Berrada, Yutian Chen, Srivatsan Srinivasan, et al. Griffin: Mixing gated linear recurrences with local attention for efficient language models. arXiv preprint arXiv:2402.19427, 2024. [11] Xin Dong, Yonggan Fu, Shizhe Diao, Wonmin Byeon, Zijia Chen, Ameya Sunil Mahabaleshwarkar, Shih-Yang Liu, Matthijs Van Keirsbilck, Min-Hung Chen, Yoshi Suhara, et al. Hymba: hybrid-head architecture for small language models. arXiv preprint arXiv:2411.13676, 2024. [12] Daniel Fu, Tri Dao, Khaled Saab, Armin Thomas, Atri Rudra, and Christopher Ré. Hungry hungry hippos: Towards language modeling with state space models. arXiv preprint arXiv:2212.14052, 2022. [13] Leo Gao, Stella Biderman, Sid Black, Laurence Golding, Travis Hoppe, Charles Foster, Jason Phang, Horace He, Anish Thite, Noa Nabeshima, et al. The pile: An 800gb dataset of diverse text for language modeling. arXiv preprint arXiv:2101.00027, 2020. [14] Paolo Glorioso, Quentin Anthony, Yury Tokpanov, James Whittington, Jonathan Pilault, Adam Ibrahim, and Beren Millidge. Zamba: compact 7b ssm hybrid model. arXiv preprint arXiv:2405.16712, 2024. [15] Karan Goel, Albert Gu, Chris Donahue, and Christopher Ré. Its raw! audio generation with state-space models. In International Conference on Machine Learning, pages 76167633. PMLR, 2022. [16] Albert Gu and Tri Dao. Mamba: Linear-time sequence modeling with selective state spaces. In First Conference on Language Modeling, 2024. URL https://openreview.net/ forum?id=tEYskw1VY2. 11 [17] Albert Gu, Isys Johnson, Karan Goel, Khaled Saab, Tri Dao, Atri Rudra, and Christopher Ré. Combining recurrent, convolutional, and continuous-time models with linear state space layers. Advances in neural information processing systems, 34:572585, 2021. [18] Albert Gu, Karan Goel, and Christopher Ré. Efficiently modeling long sequences with structured state spaces. In The International Conference on Learning Representations (ICLR), 2022. [19] Daya Guo, Dejian Yang, Haowei Zhang, Junxiao Song, Ruoyu Zhang, Runxin Xu, Qihao Zhu, Shirong Ma, Peiyi Wang, Xiao Bi, et al. Deepseek-r1: Incentivizing reasoning capability in llms via reinforcement learning. arXiv preprint arXiv:2501.12948, 2025. [20] Aaron Jaech, Adam Kalai, Adam Lerer, Adam Richardson, Ahmed El-Kishky, Aiden Low, Alec Helyar, Aleksander Madry, Alex Beutel, Alex Carney, et al. Openai o1 system card. arXiv preprint arXiv:2412.16720, 2024. [21] Rie Johnson and Tong Zhang. Accelerating stochastic gradient descent using predictive variance reduction. In C.J. Burges, L. Bottou, M. Welling, Z. Ghahramani, and K.Q. Weinberger, editors, Advances in Neural Information Processing Systems, volume 26. Curran Associates, Inc., 2013. URL https://proceedings.neurips.cc/paper_files/paper/2013/file/ ac1dd209cbcc5e5d1c6e28598e8cbbe8-Paper.pdf. [22] Yuri Kuratov, Aydar Bulatov, Petr Anokhin, Ivan Rodkin, Dmitry Sorokin, Artyom Sorokin, and Mikhail Burtsev. Babilong: Testing the limits of llms with long context reasoning-in-a-haystack. In A. Globerson, L. Mackey, D. Belgrave, A. Fan, U. Paquet, J. Tomczak, and C. Zhang, editors, Advances in Neural Information Processing Systems, volume 37, pages 106519106554. Curran Associates, Inc., 2024. https://proceedings.neurips.cc/paper_files/paper/2024/ URL file/c0d62e70dbc659cc9bd44cbcf1cb652f-Paper-Datasets_and_ Benchmarks_Track.pdf. [23] Opher Lieber, Barak Lenz, Hofit Bata, Gal Cohen, Jhonathan Osin, Itay Dalmedigos, Erez Safahi, Shaked Meirom, Yonatan Belinkov, Shai Shalev-Shwartz, et al. Jamba: hybrid transformer-mamba language model. arXiv preprint arXiv:2403.19887, 2024. [24] Zhixuan Lin, Evgenii Nikishin, Xu He, and Aaron Courville. Forgetting transformer: Softmax attention with forget gate. In The Thirteenth International Conference on Learning Representations, 2025. URL https://openreview.net/forum?id=q2Lnyegkr8. [25] Chris Lu, Yannick Schroecker, Albert Gu, Emilio Parisotto, Jakob Foerster, Satinder Singh, and Feryal Behbahani. Structured state space models for in-context reinforcement learning. Advances in Neural Information Processing Systems, 36, 2024. [26] Harsh Mehta, Ankit Gupta, Ashok Cutkosky, and Behnam Neyshabur. Long range language modeling via gated state spaces. arXiv preprint arXiv:2206.13947, 2022. [27] Eric Nguyen, Karan Goel, Albert Gu, Gordon Downs, Preey Shah, Tri Dao, Stephen Baccus, and Christopher Ré. S4nd: Modeling images and videos as multidimensional signals with state spaces. Advances in neural information processing systems, 35:28462861, 2022. [28] Daniele Paliotta, Junxiong Wang, Matteo Pagliardini, Kevin Li, Aviv Bick, Zico Kolter, Albert Gu, François Fleuret, and Tri Dao. Thinking slow, fast: Scaling inference compute with distilled reasoners. arXiv preprint arXiv:2502.20339, 2025. [29] Bo Peng, Eric Alcaide, Quentin Anthony, Alon Albalak, Samuel Arcadinho, Huanqi Cao, Xin Cheng, Michael Chung, Matteo Grella, Kranthi Kiran GV, et al. Rwkv: Reinventing rnns for the transformer era. arXiv preprint arXiv:2305.13048, 2023. [30] Michael Poli, Stefano Massaroli, Eric Nguyen, Daniel Fu, Tri Dao, Stephen Baccus, Yoshua Bengio, Stefano Ermon, and Christopher Ré. Hyena hierarchy: Towards larger convolutional language models. arXiv preprint arXiv:2302.10866, 2023. [31] Liliang Ren, Yang Liu, Yadong Lu, Yelong Shen, Chen Liang, and Weizhu Chen. Samba: Simple hybrid state space models for efficient unlimited context language modeling. arXiv preprint arXiv:2406.07522, 2024. 12 [32] Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Björn Ommer. Highresolution image synthesis with latent diffusion models. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 1068410695, 2022. [33] George Saon, Ankit Gupta, and Xiaodong Cui. Diagonal state space augmented transformers for speech recognition. In ICASSP 2023-2023 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), pages 15. IEEE, 2023. [34] Jerome Sieber, Carmen Amo Alonso, Alexandre Didier, Melanie Zeilinger, and Antonio Orvieto. Understanding the differences in foundation models: Attention, state space models, and recurrent neural networks. Advances in Neural Information Processing Systems, 37:134534134566, 2025. [35] Jimmy TH Smith, Andrew Warrington, and Scott Linderman. Simplified state space layers for sequence modeling. arXiv preprint arXiv:2208.04933, 2022. [36] Vaswani. Attention is all you need. Advances in Neural Information Processing Systems, 2017. [37] Roger Waleffe, Wonmin Byeon, Duncan Riach, Brandon Norick, Vijay Korthikanti, Tri Dao, Albert Gu, Ali Hatamizadeh, Sudhakar Singh, Deepak Narayanan, et al. An empirical study of mamba-based language models. arXiv preprint arXiv:2406.07887, 2024. [38] Jue Wang, Wentao Zhu, Pichao Wang, Xiang Yu, Linda Liu, Mohamed Omar, and Raffay Hamid. Selective structured state-spaces for long-form video understanding. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 63876397, 2023. [39] Junxiong Wang, Tushaar Gangavarapu, Jing Nathan Yan, and Alexander Rush. Mambabyte: Token-free selective state space model. arXiv preprint arXiv:2401.13660, 2024. [40] Junxiong Wang, Wen-Ding Li, Daniele Paliotta, Daniel Ritter, Alexander Rush, and Tri Dao. M1: Towards scalable test-time compute with mamba reasoning models. arXiv preprint arXiv:2504.10449, 2025. [41] Maurice Weber, Daniel Y. Fu, Quentin Anthony, Yonatan Oren, Shane Adams, Anton Alexandrov, Xiaozhong Lyu, Huu Nguyen, Xiaozhe Yao, Virginia Adams, Ben Athiwaratkun, Rahul Chalamala, Kezhen Chen, Max Ryabinin, Tri Dao, Percy Liang, Christopher Ré, Irina Rish, and Ce Zhang. Redpajama: an open dataset for training large language models. NeurIPS Datasets and Benchmarks Track, 2024. [42] Yuxin Wu and Kaiming He. Group normalization, 2018. URL https://arxiv.org/abs/ 1803.08494. [43] Tianzhu Ye, Li Dong, Yuqing Xia, Yutao Sun, Yi Zhu, Gao Huang, and Furu Wei. Differential transformer. arXiv preprint arXiv:2410.05258, 2024. [44] Michael Zhang, James Lucas, Jimmy Ba, and Geoffrey Hinton. Lookahead optimizer: steps forward, 1 step back. Advances in neural information processing systems, 32, 2019. [45] Itamar Zimerman, Ameen Ali, and Lior Wolf. unified implicit attention formulation for gated-linear recurrent sequence models. arXiv e-prints, pages arXiv2405, 2024. [46] Jingwei Zuo, Maksim Velikanov, Dhia Eddine Rhaiem, Ilyas Chahed, Younes Belkada, Guillaume Kunsch, and Hakim Hacid. Falcon mamba: The first competitive attention-free 7b language model. arXiv preprint arXiv:2410.05355, 2024."
        },
        {
            "title": "A Experimental Setup",
            "content": "All experiments were conducted with Mamba-2 on an L40s GPU using PyTorch, on publicly available datasets. A.1 Language Modeling We train all models with maximum sequence length of 512 for 40 epochs using 3 seeds (0, 42, 77) with settings as in Table 3. Diff-Mamba uses 1024 channels and 64 heads, while Mamba uses 1024 channels and 128 heads. Table 3: Configuration of Mamba and Diff-Mamba across model sizes. All models were trained for 40 epochs on each dataset. Test trends are presented in Figure 3."
        },
        {
            "title": "Dataset",
            "content": "# Layers Dropout Batch Size lr Mamba Wikitext-103 Diff-Mamba Wikitext-103 Wikitext-103 Mamba Diff-Mamba Wikitext-103 Mamba Diff-Mamba Mamba Diff-Mamba Mamba Diff-Mamba Mamba Diff-Mamba Text8 Text8 Text8 Text8 Enwik8 Enwik8 Enwik8 Enwik8 6 12 12 6 6 12 12 6 6 12 12 0.4 0. 0.5 0.5 0.4 0.4 0.5 0.5 0.4 0.4 0.5 0.5 170 50 50 170 170 50 50 170 170 80 80 5e-4 5e5e-4 5e-4 5e-4 5e-4 5e-5 5e-5 5e-4 5e-4 1e-4 1e-4 A.2 Ablations Analysis To validate our architecture design, an ablation study has been done. To match the number of parameters, each architecture has different inner parameter division. Additional configuration and models architecture details are presented in Table 4 and Table 5. Table 4: Hyperparameters used for the ablation trainings. Params Layers lr Warmup Steps Steps Droput Batch Size Values 6 5e-4 1000 15,000 0.4 A.3 Retrieval For retrieval experiments, we first convert Mamba model to Diff-Mamba by finetune it over PG19, as described in Section A.3.1, followed by evaluation on BABILong. A.3.1 PG19 Fine-tuning On the retrieval task on BABILong we used Mamba with 370M params pretrained on The Pile for 300B tokens. Since we dont have sufficient resources to pretrain from scratch Diff-Mamba we 14 Table 5: Models architecture details used for the ablation study"
        },
        {
            "title": "Model",
            "content": "# Channels # Heads Expand Mamba Diff-S6 Diff-S6 + re. λ Diff-Mamba Diff-Mamba + re. λ 1024 864 864 1024 1024 128 108 108 64 64 2 2 2 1 aligned the last 12 layers of Mamba model into Diff-Mamba and finetuned on PG19 dataset for 2B tokens. Through the alignment, half the parameters were loaded to Mamba1(X) and the other half to Mamba2(X) in each Diff-Mamba layer. This method preserved some of the pretrained weights in those layers. As mentioned, Mamba model was finetuned as well for correct comparison. Both models were finetuned on PG19, lr 3e-4, max length of 2048, and batch size of 0.5M tokens. Training curves and full configuration are presented in Figure 7 and Table 6. Table 6: Hyperparameters used for the PG19 fine-tuning. Params Values Layers lr Max Length Steps Batch Size Warmup Steps Optimizer Weight Decay 48 3e-4 2048 4000 0.5M tokens 400 AdamW 0. Figure 7: Training curve of Mamba and Diff-Mamba during the fine-tuning of PG19. The high loss at the beginning of training Diff-Mamba is due to the alignment process, which is not perfect, and the adaptation process of the new architecture. The training is not sufficient and more steps should be done for full adaptation. Nonetheless, Diff-Mamba demonstrates impressive capabilities even in the current circumstances. For more clearer presentation, we show original scores for each model in Figure 8. (a) Zero-Shot Diff-Mamba (b) Zero-Shot Mamba Figure 8: Comparison of original scores of Diff-Mamba and Mamba models as Zero-Shot. Values are the percentage of correct examples while green indicates larger score than the other model and red the opposite. A.3.2 BABILong Fine-tuning The models were fine-tuned on 90% examples up to 1k tokens inside the dataset across all tasks, which is approximately equivalent to 17k examples. The other 10% remained to test. Following [22], we employ preprocessing step that shapes each sample as follows: \"<context>{input}</context>Question:{question} Answer:\" and the loss was calculated on the answer label only. Since the training tiny size, three seeds (0, 42, 77) have been tested and averaged through the results. Original scores for each model are presented in Figure 9. The configuration for the training is in Table 7. Table 7: Hyperparameters used for the BABILong fine-tuning"
        },
        {
            "title": "Values",
            "content": "48 3e-4 2048 500 6 50 AdamW 0.1 (a) Fine-tuned Diff-Mamba (b) Fine-tuned Mamba Figure 9: Comparison of original scores of Diff-Mamba and Mamba models after fine-tuning on BABILong. Values are the percentage of correct examples while green indicates larger score than the other model and red the opposite. A.4 Noise Analysis in Intermediate Representations To measure the signal-to-noise ratio, we employ Tuned-Lens [4]. Specifically, we train linear projections to learn the appropriate transformation over intermediate representations across layers. The lens for finetuned BABILong models have been trained on the validation set of The Pile with similar configuration to the original tuned lens paper. Evaluation has been done on the test set created in the retrieval task, which stands for 2k examples. For each example, the needle probability is gathered from each layer. 16 A.5 Ablations for Early Results at Medium Scale We pre-trained both the Mamba and Diff-Mamba architectures and conducted an ablation study on the Diff-Mamba model to evaluate the impact of architectural variations. The results indicate that interleaving Mamba and Diff-Mamba layers throughout the network yields improved loss performance during pre-training, compared to using fully Diff-Mamba architecture (Figure 10). The pre-training was performed on 8 GPUs over 50 billion tokens using the GPTNeoX tokenizer. The Mamba model contains 368 million parameters, while the Diff-Mamba-Alternate and Diff-MambaFull configurations comprise 375 million and 382 million parameters, respectively. All models were trained with 48 layers, learning rate of 1.5e-3, 10,000 warm-up steps, the AdamW optimizer with weight decay of 0.1, batch size of 1 million tokens per step, and maximum sequence length of 2048 tokens. Figure 10: Loss training curve of Diff-Mamba variants compared to Mamba through pre-training on The-Pile. Diff-Mamba-Alternate indicates alternating layers of Mamba and Diff-Mamba, while Diff-Mamba-Full indicates only Diff-Mamba layers model. The trends were smoothed for display adjustment."
        }
    ],
    "affiliations": [
        "Ben-Gurion University",
        "IBM Research",
        "School of Electrical and Computer Engineering, Ben Gurion University of the Negev",
        "Tel-Aviv University"
    ]
}