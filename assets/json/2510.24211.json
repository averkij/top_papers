{
    "paper_title": "MC-SJD : Maximal Coupling Speculative Jacobi Decoding for Autoregressive Visual Generation Acceleration",
    "authors": [
        "Junhyuk So",
        "Hyunho Kook",
        "Chaeyeon Jang",
        "Eunhyeok Park"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "While autoregressive (AR) modeling has recently emerged as a new paradigm in visual generation, its practical adoption is severely constrained by the slow inference speed of per-token generation, which often requires thousands of steps to produce a single sample. To address this challenge, we propose MC-SJD, a training-free, lossless parallel decoding framework designed to accelerate AR visual generation by extending the recently introduced Speculative Jacobi Decoding (SJD). Although SJD shows strong potential for accelerating AR generation, we demonstrate that token instability across iterations significantly reduces the acceptance rate, a limitation that primarily arises from the independent sampling process used during draft token generation. To overcome this, we introduce MC-SJD, an information-theoretic approach based on coupling, which substantially accelerates standard SJD by maximizing the probability of sampling identical draft tokens across consecutive iterations, all while preserving its lossless property. Remarkably, this method requires only a single-line modification to the existing algorithm, yet achieves substantial performance gains, delivering up to a ~4.2x acceleration in image generation and ~13.3x acceleration in video generation compared to standard AR decoding, without any degradation in output quality."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 8 2 ] . [ 1 1 1 2 4 2 . 0 1 5 2 : r a"
        },
        {
            "title": "Preprint",
            "content": "MC-SJD : MAXIMAL COUPLING SPECULATIVE JACOBI DECODING FOR AUTOREGRESSIVE VISUAL GENERATION ACCELERATION Junhyuk So1, Hyunho Kook1, Chaeyeon Jang1, Eunhyeok Park1,2 POSTECH, South Korea 1 Department of Computer Science and Engineering 2 Graduate School of Artificial Intelligence {junhyukso,kookhh0827,jcy2749,eh.park}@postech.ac.kr"
        },
        {
            "title": "ABSTRACT",
            "content": "While autoregressive (AR) modeling has recently emerged as new paradigm in visual generation, its practical adoption is severely constrained by the slow inference speed of per-token generation, which often requires thousands of steps to produce single sample. To address this challenge, we propose MC-SJD, training-free, lossless parallel decoding framework designed to accelerate AR visual generation by extending the recently introduced Speculative Jacobi Decoding (SJD). Although SJD shows strong potential for accelerating AR generation, we demonstrate that token instability across iterations significantly reduces the acceptance rate, limitation that primarily arises from the independent sampling process used during draft token generation. To overcome this, we introduce MC-SJD, an information-theoretic approach based on coupling, which substantially accelerates standard SJD by maximizing the probability of sampling identical draft tokens across consecutive iterations, all while preserving its lossless property. Remarkably, this method requires only single-line modification to the existing algorithm, yet achieves substantial performance gains, delivering up to 4.2 acceleration in image generation and 13.3 acceleration in video generation compared to standard AR decoding, without any degradation in output quality."
        },
        {
            "title": "INTRODUCTION",
            "content": "Recently, autoregressive (AR) modeling has emerged as cornerstone of modern generative AI (Brown et al., 2020; Achiam et al., 2023), achieving state-of-the-art performance not only in text generation (Touvron et al., 2023) but also across diverse modalities including images (Liu et al., 2024; Sun et al., 2024a), video (Agarwal et al., 2025), 3D meshes (Weng et al., 2025), audio (Du et al., 2024; Wang et al., 2023), and even robotics (Pertsch et al., 2025). Its key strength lies in the ability to unify training and inference across modalities within single framework, enabling flexible generation, editing, and translation. This cross-domain unification allows models to leverage rich knowledge from different sources, enhancing both understanding and creativity (Zhang et al., 2025). Figure 1: Comparison of recent SD methods for AR image generation. While recent works suffer from limited acceleration or sacrifice the quality, our MC-SJD achieves up to 4 speedup over standard AR without any quality degradation. However, the practical power of AR modeling is often constrained by the inherent cost of massive computation and exacerbated memory bottlenecks. Generating sequence of tokens requires AR forward passes, leading to significant latency. The problem becomes particularly severe for high-dimensional data such as images and video, where thousands of tokens are needed to represent"
        },
        {
            "title": "Preprint",
            "content": "a single high-resolution instance (Van Den Oord et al., 2017). This limitation acts as critical barrier to the real-world deployment of multimodal AR models at scale. Recently, speculative decoding (SD) ((Leviathan et al., 2023)) has been actively explored ((Sun et al., 2023; Yin et al., 2024)) to solve this problem, particularly for large language models (LLMs) in text generation. The core idea is to use smaller, computationally inexpensive draft model to propose multiple candidate tokens, which are then verified in parallel by the more powerful target model. More importantly, SD is lossless acceleration method, guaranteeing that its output distribution remains theoretically identical to standard AR sampling ((Leviathan et al., 2023)). However, despite its effectiveness, SD has notable drawbacks: the overhead of training separate draft model ((Cai et al., 2024)), and limited performance in vision generation tasks ((So et al., 2025; Jang et al., 2024)). To address these problems, the pioneering Speculative Jacobi Decoding (SJD) (Teng et al., 2024) was proposed, combining Jacobi iteration (Song et al., 2021) with the stochastic verification criterion of SD. Briefly, SJD uses the output distribution from its own previous verification step as the draft for the next. This Self-SD approach eliminates the need for separate, trained draft model, thereby resolving the idle-time bottleneck and demonstrating significant speedups, especially in image generation. However, while SJD shows promise, it delivers only about 2 speedup in image generation, relatively modest compared to state-of-the-art SD methods in text, which achieve over 4 acceleration ((Cai et al., 2024)). In this paper, we demonstrate that this problem can be solved with simple tweak to the SJD process, offering an incredibly high speedup while maintaining the lossless property of SD. Our key finding is that the performance of the SJD is significantly limited by instability in its draft token sampling, leaving substantial room for further improvement.To unlock the potential of SJD, we introduce simple yet highly effective, information-theory-inspired idea: Coupling (Lindvall, 2002). Specifically, we propose to couple the draft sampling process between consecutive Jacobi iterations, thereby increasing the probability of re-sampling the same tokens to promote stability. This method requires only single-line modification to the standard SJD, making it extremely simple to implement without any additional training. Despite its simplicity, we demonstrate that our method significantly enhances the acceptance rate of SJD, enabling remarkable lossless speedup of 3.8 for AR image generation and an 10 for video generation, compared to standard AR decoding."
        },
        {
            "title": "2 PRELIMINARIES",
            "content": "Notation. We denote by the token at the i-th position of sequence at Jacobi iteration (defined later). When clear from context, we omit the subscript/superscript or to refer to the entire sequence or to the collection of distributions, respectively. Similarly, we denote by pt i() the token distribution at position in iteration t. We assume all distributions are on the same support . 2.1 SPECULATIVE DECODING The main goal of Speculative Decoding (SD) (Leviathan et al., 2023) is to reduce the number of sequential calls of target model while ensuring that final outcome matches the AR sampling distribution, (cid:81) pi(x X<i). Specifically, we assume two models: target p(x), which we wish to accelerate, and draft q(x), which is faster than p(x) but less accurate. SD proceeds as follows: 1. Drafting: Sample draft tokens from the draft distributions, Xi:i+L1 qi:i+L1(). 2. Evaluate: In parallel, have the target model evaluate the token probabilities along the drafted prefixes, i.e., { pj(Xj X<j) }i+L j=i . 3. Verify: Run Algorithm 3 with (pi, qi, Xi) sequentially until rejection occurs (i.e., the procedure returns = 0); accept all previously verified tokens. 4. Repeat: If the generation is not yet complete, return to Drafting and repeat the process. Transformers natively support the parallel evaluation in step (2) via masked attention, ideally in O(1) sequential depth. Thus, if acceptance occurs in step (3), the procedure may emit multiple tokens in effectively O(1) sequential time, reducing the total NFEs compared with standard AR decoding."
        },
        {
            "title": "Preprint",
            "content": "Algorithm 1 Speculative Jacobi Decoding Require: AR Model pθ, draft Length L, Max 1: pi 2: Sequence Random() pt i, 3: while < do 4: 5: pt"
        },
        {
            "title": "X t",
            "content": "j(x) parallel for = to + : Drafting 6: 7: 8: 9: 10: pt+1 pθ( for = to + : 0:j1) MRS(pt+1 k, t+1 if = 0 : break; Verify j, j) , pt Initialize Initialize Algorithm 2 Pseudo Code for our MC-SJD Require: AR Model pθ, draft Length L, Max 1: pi 2: Sequence Random() pt i, 3: while < do 4: 5: , parallel for = to + : Drafting MRS(pt j, pt1 , t1 ) pt+1 pθ( for = to + : 0:j1) MRS(pt+1 k, t+1 if = 0 : break; Verify j, j) , pt 6: 7: 8: 9: 10: parallel for = to + : Evaluate parallel for = to + : Evaluate j, + 1 11: 12: end while 13: return j, + 11: 12: end while 13: return The Sampling of Algorithm 3 guarantees that even if the input is q(), the output returned by the algorithm satisfies p() (Chen et al., 2023). Because each Markov chain follows valid sampling from until the first rejection occurs, the theoretical correctness of speculative decoding is ensured. As shown in Alg 3, the acceptance probability per token, min{1, p(x)/q(x)}, is the key factor that determines the overall speedup. We formalize this in the following proposition: Proposition 1 Let be the draft distribution and q(x), then, final output from MRS(Alg.3) strictly follow the distribution of target model p(x). Moreover, the acceptance rate of this algorithm is defined as where DT denotes total variation 1 2 Exq(xi)min(1, p(x) q(x) p(v) q(v). (cid:80) ) = 1 DT (p, q) Proof: See appendix. Typically, standard SD methods employ cheaper AR model to produce draft tokens/distributions. While this strategy has shown promising results in the text-generation domain (Cai et al., 2024), it has several drawbacks: the need to train separate draft model, communication bottlenecks between the draft and target models, and limited speedups in non-text AR generation domains (So et al., 2025; Jang et al., 2024). These issues have hindered the adoption of SD techniques beyond text, limiting the potential of AR modeling across different modalities. Algorithm 3 MRS(p,q,x); Modified Rejection Sampling Input: Distribution , Q. Tokens Output: Accept signal k, Random variable 1: Sample [0, 1] 2: if min(1, (X) Q(X) ) 3: 4: return 0, norm(max(0, (x) Q(x))) return 1, Y=X 2.2 SPECULATIVE JACOBI DECODING Speculative Jacobi Decoding (SJD) (Teng et al., 2024) is pioneering, training-free algorithm to solve the aforementioned problems of standard speculative decoding. As depicted in Algorithm 1, SJD eliminates the need for separate draft model q(). Instead, it leverages the probability distribution from its own previous validation step as the draft for the next iteration. This Self-SD approach does not impact the theoretical accuracy guarantees of speculative decoding, because the"
        },
        {
            "title": "Preprint",
            "content": "verification mechanism (Alg. 3) ensures the output is always valid sample from the target models distribution, regardless of the input draft. This framework makes the process highly efficient as it removes the overhead of training separate model and eliminates the idle time where the target model would wait for draft tokens. Due to these properties, SJD first achieves 2x speedup in AR image generation domain, while retaining its lossless and training-free nature. Connection to the Fixed-Point Methods Another key difference between SJD and other SD frameworks is that it reuses information from rejected tokens in the next drafting. This allows SJD to be framed as fixed-point method (Coddington et al., 1956) that updates the entire sequence at once via the iteration t+1 (X t), which is known to converge to solution very fast (Hutzenthaler et al., 2021), under the assumption of continuity of and contraction on . SJD can be seen as practical variant of this, which relaxes the contraction property and is adapted for discrete token space, by using probabilistic convergence criterion from SD instead of numerical difference t1, which is ill-suited for the discrete case. In practice, while we want our sequence to be refined across iterations within the fixed-point framework, converging toward solution, we have found that the current SJD has very little effect in this regard."
        },
        {
            "title": "3 MOTIVATION AND ANALYSIS",
            "content": "Despite achieving 2x speedup in image AR, we find that performance potential of SJD is significantly limited by the variance introduced during its stochastic draft sampling process. To gain an intuitive understanding of this, we start with an analysis of the acceptance rate of SJD. At iteration in SJD, the target distribution is pt() and the draft distribution is pt1(). As noted in Proposition 1, the acceptance rate can be expressed in terms of the Total Variation, as follows : β(t) = 1 DT (cid:16) = 1 DT (cid:17) (x) (x), p(t1) p(t) (cid:12) (cid:17) (cid:16) (cid:12) (t1) (cid:12) <i pθ (cid:16) , pθ (cid:16) (cid:12) (cid:12) (t2) (cid:12) <i (cid:17)(cid:17) , (1) (2) where pθ denotes the autoregressive model and X<i denotes the prefixes {Xi1, Xi2, . . . }. As shown in Eq. 2, The acceptance rate β(t) is directly influenced by the context change between iterations 1 and 2. In other words, the acceptance rate for token is driven by changes in its prefixes, including both previously accepted tokens but also the other rejected tokens in the draft. This leads directly to the following observation: Observation 1 High context similarity between consecutive drafts tends to yield higher speedup. This can be easily validated by Eq. 2: the greater the similarity between the contexts (t1) and (t2) , the more similar their corresponding output distributions pθ(X<i) will be (under mild <i Lipschitzness assumption on the models logits from tokens). This results in lower TV distance and, consequently, higher acceptance rate β. <i We also empirically validate it in Fig. 2, plotting the 300 independent samples with their mean number of changed tokens between consecutive sequence drafts (Hamming distance) against the total number of function evaluations (NFE) required for SJD generation. As shown, there is strong correlation between these two metrics, indicating that context similarity plays crucial role for faster generation in SJD. However, despite this correlation, Fig. 2 shows that the average number of token difference is approximately 94% tokens - 60 of window size 64 -, indicating significantly large portion of tokens are changed in each iteration. We observe that this high degree of change not only critiFigure 2: Generation NFE v.s Mean Token Difference during SJD with window size = 64. As shown, sample that is generated with smaller NFE tends to have small mean token difference."
        },
        {
            "title": "Preprint",
            "content": "(a) SJD βt convergence (b) Ours βt convergence (c) βt statistics (Mean - Var) Figure 3: (a), (b) The trajectory of tokenwise acceptance rate βt during the jacobi iterations (a) Standard SJD shows most tokens have large variation during iteration and do not exhibit improvement behavior. (b) After applying our coupled sampler πM C. Now most of tokens has very small fluctuation, showing general upward trends. (c) Mean and variance of βt across all token index. While standard SJD does not show improvement, ours shows clear upward, refining behavior. cally limits SJDs single iteration acceptance rate but also poses more severe problem if we consider behavior on multiple consecutive iterations, the convergence of SJD. Observation 2 The per-token acceptance rate βt and does not show converging behavior. during the SJD process exhibits high variance Ideally, the acceptance rate for given token should increase over the SJD iterations. As the left-most context becomes filled with stable, accepted tokens, an improvement signal should propagate to the right, progressively enhancing the quality of the draft sequences. However, our empirical results reveal the opposite behavior. Fig. 3(a) plots the trajectory of βt for representative tokens, showing that the acceptance rate frequently fluctuates without any consistent upward trend. This instability is further confirmed in Fig. 3(c) (blue line), which aggregates the statistics across all tokens. After an initial jump, the mean acceptance rate not only remains low but also fails to improve, exhibiting random fluctuations with high variance throughout the process. 3.1 ANALYSIS We then investigate the root cause of this low context similarity. Since the context sequences (t) are realizations of random variables drawn from p(t)() at each iteration, natural way to quantify their similarity is by measuring the collision probability, defined as Pr[X (t) ]. As described in Algorithm 1, the drafting stage of SJD (Line 5) samples the draft token (t) independently from i(x). In this independent sampling scheme, the collision probability between (t) its distribution pt and (t1) can be analytically computed as follows: = (t1) Proposition 2 (SJD Collision Probability) Standard SJD has the following collision probability for token at iteration t: CSJD(p(t), p(t1)) = (x) p(t1) p(t) (x) (cid:88) xV where denotes the vocabulary. This value is bounded as follows: where H2(p) = log((cid:80) p(x)2) is the Renyi-2 entropy of p. CSJD(p, q) e1/2(H2(p)+H2(q)) Proof: See appendix. As shown, even when two distributions are similar, their collision probability is constrained by the (Renyi-2) entropy of the underlying distributions and vocabulary sizes. Unfortunately, unlike text AR models, visual AR models are known to generate very flat distributions (So et al., 2025). This is because of the inherent redundancy in visual tokens and the complexity of visual patterns makes large number of different tokens plausible continuations of sequence."
        },
        {
            "title": "Preprint",
            "content": "(a) SJD Pr[X = ] histogram (b) Ours Pr[X = ] histogram (c) DT v.s Pr[X = ] Figure 4: Visualization of Collision probabilities. (a) During standard SJD, CSJD are concentrated on extremely small values. (b) Our Coupler elevates this to much higher values, significantly enhancing the context similarity. (c) Standard SJD has low Pr[X = ] even when the corresponding TV distance is low. The green dot-line denotes the πGS lower bound πGS (1DT )/(1+DT ). We also visualize the empirical collision probability, CSJD, during the SJD process in Fig. 4. Fig. 4 (a) shows most of its values remain at an extremely low value. and (c) illustrates that this value is nearly zero regardless of whether the TV distance is small. Consequently, the standard SJD propagates different contextual information to subsequent tokens in each iteration, significantly destabilizing the iteration and causing the convergence to fluctuate unpredictably. We identify this discrepancy between the proximity in probability space and the realized token space as the key factor limiting the speedup in SJD."
        },
        {
            "title": "4 METHODS",
            "content": "Our main idea is that making Coupling ((Lindvall, 2002)) between the draft distributions from consecutive iterations can increase the collision probability without compromising the theoretical lossless correctness of the SJD. To formalize, we begin with mathematical definition of coupling : Definition 1 (Coupling) For two distributions () and Q() on the same support V, joint distribution π(, ) over is Coupling of and if its marginals satisfy: π(x, y) = (x) and (cid:88) yV π(x, y) = Q(y) (cid:88) xV The key insight lies in the marginalization property of coupling. If we sample pair of variables from joint distribution π(x, y), the marginal distribution of each individual variable remains identical to its original distribution (e.g., (x) and Q(y)). Therefore, using token sampled from coupling is provably valid replacement for independent sampling within the SJD framework. We formally stated this in the following theorem: Theorem 1 Let Π(t) pair (X (t) ) π(, ) for any π Π(t) , (t1) final output distribution still correctly matches the target models distribution. be the set of all possible couplings between p(t) and use (t) . If we sample as the draft token in Algorithm 1, the and p(t1) i Proof Sketch: See appendix. For any given coupling π, we can define its effectiveness using metric called the Coupling Cost, denoted C(π). This cost measures the probability of sampling identical variables from the joint distribution, which is the same metric we previously referred to as the collision probability: Definition 2 (Coupling Cost) Let πP,Q be coupling of distributions and as per Definition 1. The Coupling Cost is defined as: C(πP,Q) = Pr (X,Y )πP,Q [X = ] = E(X,Y )πP,Q 1{X = } From this perspective, the standard SJD process can be understood as using an independence coupling, where πSJD(x, y) = p(t) (y) and the cost of this particular coupling is C(πSJD) = (cid:80) p(t) (v), value we have already shown to be extremely low in AR image (x) p(t1) (v)p(t1) i"
        },
        {
            "title": "Preprint",
            "content": "generation. Finally, our main objective can be safely reframed as finding an alternative coupling, π, that maximizes this cost, thereby promoting context similarity without compromising the exactness guarantee of the framework. We next present alternative couplings that achieve this objective."
        },
        {
            "title": "4.1 MAXIMAL COUPLING",
            "content": "Consider the computation graph of and pt(x) during the SJD process : t2 pt1(X) t1 pt(X) (3) As shown, at the time of sampling step for (t), we have access to the full information of two probability distributions p(t) and p(t1). As is well-established in many literature on information theory and optimal transport (Villani et al., 2008; Bavarian et al., 2016), having complete information of both distributions allows us for the construction of maximal coupling, which has the cost of c(πp,q) = 1 DT (p, q) that any two distribution can maximally have. In Algorithm 2, we present the implementation of SJD with draft sampling with maximal coupling process. As shown, the only modification required is in the drafting phase (Line 4), where we now sample the draft token using coupled sampler instead of sampling it independently from pt i(). Interestingly, as shown, the implementation of this coupling is exactly identical with the modified rejection sampling, MRS() (Alg.3) which we used for speculative decoding verification process. This can be easily validated by the fact that MRS() returns from an input Q, ensuring that the marginals of the generated pair (Y, X) match and Q, which satisfies the definition of coupling. Moreover, as established in Proposition 1, the acceptance rate MRS() - probability of Pr[X = ] - is 1DT (p, q). This value is the theoretical upper bound of coupling cost, confirming that this procedure constitutes maximal coupling. We formally state this as follows: Theorem 2 Let the pair (X,Y) be generated by Algorithm 3. Then, their resulting joint distribution (X, ) πM C, is valid coupling of and Q. Moreover, its coupling cost, is the upper bound for the cost of any π Π with and Q. C(πM C) = 1 DT (P, Q) As illustrated in Fig. 4 (c), this upper bound, represented by the black dashed line, shows significant gap compared to the coupling cost of standard SJD (CSJD). Applying maximal coupling within SJD, elevates this low values to their upper bound (orange dots), thereby strongly promoting high context similarity and achieving greater speedup. We also show the distribution of C(πM C) in Fig. 4 (b). In Fig. 3 (b), (c), we show the trajectories and statistics of the βt, during iterations with MC-SJD. As shown, most tokens now exhibit minimal fluctuation with general upward trend, resulting in much higher overall acceptance rate compared to standard SJD, leading to lower NFEs. Algorithm 4 GS(P, Q, G); Gumbel Noise Sharing Input: Distributions P, over vocabulary V. shared Gumbel noise vector = (g1, . . . , gV) where gi Gumbel(0, 1). Output: coupled pair of random variables (X, ). 1: argmaxiV (log(Pi) + gi) 2: argmaxiV (log(Qi) + gi) 3: return (X, ) Sample from using Sample from using the same 4.2 GUMBEL COUPLING While maximal coupling is theoretically optimal , we also introduce simpler alternative, Gumbel Coupling, which is more computationally efficient but achieves comparable coupling cost with maximal coupling. We denote πGS in Algorithm 4 and SJD implementation with this coupling in Algorithm 5. As shown, this algorithm is based on the Gumbel-Max trick that relies on sharing the same random noise vector to couple two categorical sampling processes. We first establish its validity and provide lower bound for its cost:"
        },
        {
            "title": "Preprint",
            "content": "Table 1: Evaluation results of AR Image generation model, Lumina-mGPT, on MS-COCO dataset. Configuration NFE () Latency () Acceleration () Latency (A100) NFE FID () IS () CLIP-Score () Vanilla AR SJD (L=16) + Ours (πM C) + Ours (πGS) SJD (L=32) + Ours (πM C) + Ours (πGS) SJD (L=64) + Ours (πM C) + Ours (πGS) GSD (L=32,G=3) GSD (L=32,G=10) 2390 1058.6 814.5 819.4 1031.2 666.0 652. 1035.9 566.5 567.1 925.9 701.4 102.03s 41.49s 33.28s 33.57s 42.81s 29.86s 27.99s 43.54s 29.59s 26.89s 38.98s 29.13s 1.00 2.25 2.94 2.92 2.32 3.59 3.66 2.31 4.22 4.21x 2.58 3.40 1.00 2.46x 3.06x 3.04x 2.38x 3.42x 3.64x 2.34x 3.45x 3.79x 2.62x 3.50x 30. 30.77 30.73 30.78 30.78 30.79 30.75 30.81 30.83 30.90 31.50 33.21 32.81 32.78 33.56 32. 32.82 33.56 32.91 32.76 33.43 32.80 29.76 26.78 31.31 31.32 31.32 31.37 31.31 31.32 31. 31.31 31.37 31.37 31.33 31.25 Figure 5: Qualitative comparison between Ours v.s. AR on Lumina-mGPT. (zoom-in to view). Theorem 3 Let the pair (X,Y) be generated by Algorithm 4. Then, their resulting joint distribution (X, ) πGS, is valid coupling of and Q. Its worst-case coupling cost is lower-bounded by: C(πGS) (1 DT (P, Q))/(1 + DT (P, Q)) Proof sketch: The coupling validity of πGS can be easily shown based on the Gumbel-Max Trick (Gumbel, 1954), where this trick known to yield output that follows input categorical distribution. In this setting, this lower bound has been well-stuided in recent works (Bavarian et al., 2016) . Although it is not optimal in terms of coupling cost, as shown in Fig. 4 (c), this lower bound is almost tight to the optimal and is significantly greater than the independence coupling πSJD. Moreover, because this lower bound is applicable to any pair of distributions during an iteration (as long as we use the same gumbel noise), we can consider that this Gumbel Coupling promotes more longrange stabilization during the iteration, while πM greedily optimizes in each consecutive iteration. Finally, because this coupling is much computationally efficient, it typically has faster latency on real hardware. As shown in Table 1, we empirically found that this πGS has comparable NFE to πM in most cases and is even slightly lower in some cases, and typically has lower latency."
        },
        {
            "title": "5 EXPERIMENTAL RESULTS",
            "content": "In experiments section, we mainly focus on validating two aspects : (i) How much acceleration can we gain by applying our method atop SJD, (ii) Does our algorithm truly preserve generation quality, although we show it theoretically. Setup Similar to original SJD paper, we mainly evaluate with Lumina-mGPT (Liu et al., 2024) for AR image generation. We also evaluate our method with the more SOTA AR Image model, Janus-Pro (Chen et al., 2025), to validate our methods generalization. Moreover, beyond image generation, we also evaluate with an AR video generation model, cosmos-ar (Agarwal et al., 2025), which has longer generation sequence length and expected to have more redundancy. The more detailed settings are in the appendix. Metrics and Datasets To evaluate the quality, we measured FID (Heusel et al., 2017) , which denotes the distribution distance compared to reference and generated datasets, IS (Barratt & Sharma, 2018) and CLIP score (Radford et al., 2021) for fair comparison. To evaluate speed, we measure"
        },
        {
            "title": "Preprint",
            "content": "number of function evaluation (NFE), which indicates the number of sequential forward steps, and the real latency on 1x NVIDIA A100 device. We mainly use MS-COCO (val) (Lin et al., 2014) dataset for image generation evaluation and real-state-10k for video generation. More details are in appendix. Config NFE () FID () IS ()"
        },
        {
            "title": "A Vanilla AR",
            "content": "576 SJD (L=16) + Ours SJD (L=32) + Ours 319.93 189.99 318.01 154.42 37. 37.96 37.13 37.76 37.49 22.39 22.25 22.53 21.80 22.43 Table 2: Janus-Pro (7B) on MS-COCO 2017. Figure 6: CFG scale vs. NFE. All experiments use Lumina-mGPT 768768 (7B). Baselines: We benchmarked our method against three baselines: (A) standard autoregressive decoding (Vanilla AR), (B) Speculative Jacobi Decoding (SJD) (C) Grouped Speculative Decoding (GSD) (So et al., 2025), which is recently proposed lossy SD methods for image generation and (D) Ours. We implement (C) and (D) atop (B) for fair comparison. Configuration Vanilla AR SJD (L=16) + Ours SJD (L=32) + Ours SJD (L=64) NFE () Latency (s) () FVD () 7680 2272.8 1990. 1886.4 1293.7 157.25 54.12 48.93 48.43 32.36 156.9 157.1 159. 153.2 155.8 1802.3 835.9 48.19 22.38 163.6 155.8 + Ours + Ours 47.73 15.87 1789.9 577.8 SJD (L=128) Results Table 1 presents our main results for AR image generation on Lumina-mGPT. As shown, our method (D) accelerates the AR decoding (A) by up to 4x and SJD (B) by 1.8x without compromising its exactness guarantee, maintaining identical FID, IS and CLIP scores. Notably, while standard SJD fails to achieve meaningful speedup with an increased window size (L), our MC-SJD demonstrates consistent acceleration as the window size grows, strongly suggesting that our coupling helps to stabilize SJDs convergence. Finally, compared to (C) , lossy SD method GSD, while it also significantly reduces the NFE, it results in degradation of the FID and CLIP scores. Our method, in contrast, shows an even faster speedup than lossy GSD while maintaining quality exactness. In Table 2, we also report results on the SOTA AR image model, Janus-Pro (7B). As shown, our method consistently accelerates the standard SJD process by up to 2.1x, achieving final step compression of 3.7. Table 3: Video generation results on Cosmos1AR-4B 158.3 157. In Table 3, We depict the quantitative results on AR video generation model, cosmos-ar (4B). Remarkably, as shown, applying our method achieves 13 actual acceleration with no loss in performance. This large acceleration gain mainly stems from the strong temporal redundancy between consecutive frames in video AR generation, and we believe our results will unlock huge potential in this field, where research progress has recently been hindered by speed bottlenecks. 5.1 ABLATION STUDIES Effect on CFG AR vision models typically rely on CFG techniques to control prompt alignment and fidelity, using scale around 37. Specifically, the samples are generated from mixed logit: (1 + λ) λ u, where logit generated with prompt and generated with masked prompt. As shown in Fig. 6, as λ increases, speedup slightly decreased because the final logit becomes sharper. However, our method consistently outperforms SJD by large margin in practical range of scale λ. Qualitative Results: While we have quantitatively demonstrated the lossless property of our method, we also performed qualitative comparison experiment to visualize that our method does not degrade generation quality. As shown in Fig. 5, our method yields outputs that are visually indistinguishable from the AR model while achieving the 3 acceleration. We provide more visualizations in the Appendix."
        },
        {
            "title": "6 CONCLUSION",
            "content": "In this paper, we identify and resolve critical performance bottleneck in the recently proposed SelfSD framework for autoregressive image generation, Speculative Jacobi Decoding (SJD). Specifically, we find that the speed potential of SJD is severely limited by its context instabilities, arising from an independent draft sampling process. To solve this problem, we propose to use an information-theory-inspired approach, Coupling, to replace the draft sampling and stabilize the Jacobi iteration trajectory by increasing the probability of re-sampling the token, transferring distributional similarity to the realized discrete token space. As result, we show that this simple tweak can remarkably enhance the speedup of SJD, achieving 4 to 13 acceleration in visual AR generation, while maintaining its training-free and lossless properties."
        },
        {
            "title": "REFERENCES",
            "content": "Josh Achiam, Steven Adler, Sandhini Agarwal, Lama Ahmad, Ilge Akkaya, Florencia Leoni Aleman, Diogo Almeida, Janko Altenschmidt, Sam Altman, Shyamal Anadkat, et al. Gpt-4 technical report. arXiv preprint arXiv:2303.08774, 2023. Niket Agarwal, Arslan Ali, Maciej Bala, Yogesh Balaji, Erik Barker, Tiffany Cai, Prithvijit Chattopadhyay, Yongxin Chen, Yin Cui, Yifan Ding, et al. Cosmos world foundation model platform for physical ai. arXiv preprint arXiv:2501.03575, 2025. Gregor Bachmann, Sotiris Anagnostidis, Albert Pumarola, Markos Georgopoulos, Artsiom Sanakoyeu, Yuming Du, Edgar Schonfeld, Ali Thabet, and Jonas Kohler. Judge decoding: Faster speculative sampling requires going beyond model alignment. arXiv preprint arXiv:2501.19309, 2025. Jinze Bai, Shuai Bai, Yunfei Chu, Zeyu Cui, Kai Dang, Xiaodong Deng, Yang Fan, Wenbin Ge, Yu Han, Fei Huang, et al. Qwen technical report. arXiv preprint arXiv:2309.16609, 2023. Shane Barratt and Rishi Sharma. note on the inception score. arXiv preprint arXiv:1801.01973, 2018. Mohammad Bavarian, Badih Ghazi, Elad Haramaty, Pritish Kamath, Ronald Rivest, and Madhu Sudan. Optimality of correlated sampling strategies. arXiv preprint arXiv:1612.01041, 2016. Vasile Berinde. Approximating fixed points of weak contractions using the picard iteration. In Nonlinear Analysis Forum, volume 9, pp. 4354, 2004. Oscar Brown, Zhengjie Wang, Andrea Do, Nikhil Mathew, and Cheng Yu. Dynamic depth decoding: Faster speculative decoding for llms. arXiv preprint arXiv:2409.00142, 2024. Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. Language models are few-shot learners. Advances in neural information processing systems, 33:18771901, 2020. Tianle Cai, Yuhong Li, Zhengyang Geng, Hongwu Peng, Jason Lee, Deming Chen, and Tri Dao. Medusa: Simple llm inference acceleration framework with multiple decoding heads. arXiv preprint arXiv:2401.10774, 2024. Charlie Chen, Sebastian Borgeaud, Geoffrey Irving, Jean-Baptiste Lespiau, Laurent Sifre, and John Jumper. Accelerating large language model decoding with speculative sampling. arXiv preprint arXiv:2302.01318, 2023. Xiaokang Chen, Zhiyu Wu, Xingchao Liu, Zizheng Pan, Wen Liu, Zhenda Xie, Xingkai Yu, and Chong Ruan. Janus-pro: Unified multimodal understanding and generation with data and model scaling. arXiv preprint arXiv:2501.17811, 2025. Earl A. Coddington, Norman Levinson, and T. Teichmann. Theory of ordinary differential equations. Physics Today, 9(2):18, 1956."
        },
        {
            "title": "Preprint",
            "content": "Chaorui Deng, Deyao Zhu, Kunchang Li, Chenhui Gou, Feng Li, Zeyu Wang, Shu Zhong, Weihao Yu, Xiaonan Nie, Ziang Song, et al. Emerging properties in unified multimodal pretraining. arXiv preprint arXiv:2505.14683, 2025. Zhihao Du, Yuxuan Wang, Qian Chen, Xian Shi, Xiang Lv, Tianyu Zhao, Zhifu Gao, Yexin Yang, Changfeng Gao, Hui Wang, et al. Cosyvoice 2: Scalable streaming speech synthesis with large language models. arXiv preprint arXiv:2412.10117, 2024. Emil Julius Gumbel. Statistical theory of extreme values and some practical applications: series of lectures, volume 33. US Government Printing Office, 1954. Martin Heusel, Hubert Ramsauer, Thomas Unterthiner, Bernhard Nessler, and Sepp Hochreiter. Gans trained by two time-scale update rule converge to local nash equilibrium. Advances in neural information processing systems, 30, 2017. Aaron Hurst, Adam Lerer, Adam Goucher, Adam Perelman, Aditya Ramesh, Aidan Clark, AJ Ostrow, Akila Welihinda, Alan Hayes, Alec Radford, et al. Gpt-4o system card. arXiv preprint arXiv:2410.21276, 2024. Martin Hutzenthaler, Thomas Kruse, and Tuan Anh Nguyen. On the speed of convergence of picard iterations of backward stochastic differential equations. arXiv preprint arXiv:2107.01840, 2021. Doohyuk Jang, Sihwan Park, June Yong Yang, Yeonsung Jung, Jihun Yun, Souvik Kundu, SungYub Kim, and Eunho Yang. Lantern: Accelerating visual autoregressive models with relaxed speculative decoding. arXiv preprint arXiv:2410.03355, 2024. Yaniv Leviathan, Matan Kalman, and Yossi Matias. Fast inference from transformers via speculative decoding. pp. 1927419286, 2023. Tsung-Yi Lin, Michael Maire, Serge Belongie, James Hays, Pietro Perona, Deva Ramanan, Piotr In Computer Dollar, and Lawrence Zitnick. Microsoft coco: Common objects in context. visionECCV 2014: 13th European conference, zurich, Switzerland, September 6-12, 2014, proceedings, part 13, pp. 740755. Springer, 2014. Torgny Lindvall. Lectures on the coupling method. Courier Corporation, 2002. Dongyang Liu, Shitian Zhao, Le Zhuo, Weifeng Lin, Yu Qiao, Hongsheng Li, and Peng Gao. Lumina-mgpt: Illuminate flexible photorealistic text-to-image generation with multimodal generative pretraining. arXiv preprint arXiv:2408.02657, 2024. Adam Paszke, Sam Gross, Francisco Massa, Adam Lerer, James Bradbury, Gregory Chanan, Trevor Killeen, Zeming Lin, Natalia Gimelshein, Luca Antiga, et al. Pytorch: An imperative style, highperformance deep learning library. Advances in neural information processing systems, 32, 2019. Karl Pertsch, Kyle Stachowicz, Brian Ichter, Danny Driess, Suraj Nair, Quan Vuong, Oier Mees, Chelsea Finn, and Sergey Levine. Fast: Efficient action tokenization for vision-language-action models. arXiv preprint arXiv:2501.09747, 2025. Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, et al. Learning transferable visual models from natural language supervision. In International conference on machine learning, pp. 87488763. PmLR, 2021. Andy Shih, Suneel Belkhale, Stefano Ermon, Dorsa Sadigh, and Nima Anari. Parallel sampling of diffusion models. Advances in Neural Information Processing Systems, 36:42634276, 2023. Junhyuk So, Juncheol Shin, Hyunho Kook, and Eunhyeok Park. Grouped speculative decoding for autoregressive image generation. arXiv preprint arXiv:2508.07747, 2025. Yang Song, Chenlin Meng, Renjie Liao, and Stefano Ermon. Accelerating feedforward computation via parallel nonlinear equation solving. In International Conference on Machine Learning, pp. 97919800. PMLR, 2021."
        },
        {
            "title": "Preprint",
            "content": "Peize Sun, Yi Jiang, Shoufa Chen, Shilong Zhang, Bingyue Peng, Ping Luo, and Zehuan Yuan. Autoregressive model beats diffusion: Llama for scalable image generation. arXiv preprint arXiv:2406.06525, 2024a. Ziteng Sun, Ananda Theertha Suresh, Jae Hun Ro, Ahmad Beirami, Himanshu Jain, and Felix Yu. Spectr: Fast speculative decoding via optimal transport. Advances in Neural Information Processing Systems, 36:3022230242, 2023. Ziteng Sun, Uri Mendlovic, Yaniv Leviathan, Asaf Aharoni, Jae Hun Ro, Ahmad Beirami, and Ananda Theertha Suresh. Block verification accelerates speculative decoding. arXiv preprint arXiv:2403.10444, 2024b. Chameleon Team. Chameleon: Mixed-modal early-fusion foundation models. arXiv preprint arXiv:2405.09818, 2024. Yao Teng, Han Shi, Xian Liu, Xuefei Ning, Guohao Dai, Yu Wang, Zhenguo Li, and Xihui Liu. Accelerating auto-regressive text-to-image generation with training-free speculative jacobi decoding. arXiv preprint arXiv:2410.01699, 2024. Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timothee Lacroix, Baptiste Rozi`ere, Naman Goyal, Eric Hambro, Faisal Azhar, et al. Llama: Open and efficient foundation language models. arXiv preprint arXiv:2302.13971, 2023. Aaron Van Den Oord, Oriol Vinyals, et al. Neural discrete representation learning. Advances in neural information processing systems, 30, 2017. Cedric Villani et al. Optimal transport: old and new, volume 338. Springer, 2008. Chengyi Wang, Sanyuan Chen, Yu Wu, Ziqiang Zhang, Long Zhou, Shujie Liu, Zhuo Chen, Yanqing Liu, Huaming Wang, Jinyu Li, et al. Neural codec language models are zero-shot text to speech synthesizers. arXiv preprint arXiv:2301.02111, 2023. Haohan Weng, Zibo Zhao, Biwen Lei, Xianghui Yang, Jian Liu, Zeqiang Lai, Zhuo Chen, Yuhong Liu, Jie Jiang, Chunchao Guo, et al. Scaling mesh generation via compressive tokenization. In Proceedings of the Computer Vision and Pattern Recognition Conference, pp. 1109311103, 2025. Ming Yin, Minshuo Chen, Kaixuan Huang, and Mengdi Wang. theoretical perspective for speculative decoding algorithm. Advances in Neural Information Processing Systems, 37:128082 128117, 2024. Xinjie Zhang, Jintao Guo, Shanshan Zhao, Minghao Fu, Lunhao Duan, Jiakui Hu, Yong Xien Chng, Guo-Hua Wang, Qing-Guo Chen, Zhao Xu, et al. Unified multimodal understanding and generation models: Advances, challenges, and opportunities. arXiv preprint arXiv:2505.02567, 2025. Qingqing Zhao, Yao Lu, Moo Jin Kim, Zipeng Fu, Zhuoyang Zhang, Yecheng Wu, Zhaoshuo Li, Qianli Ma, Song Han, Chelsea Finn, et al. Cot-vla: Visual chain-of-thought reasoning for visionlanguage-action models. In Proceedings of the Computer Vision and Pattern Recognition Conference, pp. 17021713, 2025. Tinghui Zhou, Richard Tucker, John Flynn, Graham Fyffe, and Noah Snavely. Stereo magnification: Learning view synthesis using multiplane images. ACM Trans. Graph. (Proc. SIGGRAPH), 37, 2018. URL https://arxiv.org/abs/1805.09817."
        },
        {
            "title": "A PROOFS",
            "content": "A.1 PROOF OF PROPOSITION 1 Proof. We will first check that MRS() returns with input Q. Let the acceptance probability min(1, p(x)/q(x)) = α(x). Then, we can re-write the p.d.f of R.V , y(x) as follows y(x) = α(x) q(x) + (1 (cid:88) xV α(x) q(x))r(x) (4) where r(x) is residual distribution r(x) = norm(max(0, p(x) q(x)) = We can rewrite the left term as : (cid:80) max(0,p(x)q(x)) xV max(0,p(x)q(x)) . α(x) q(x) = min(1, p(x)/q(x)) q(x) = min(q(x), p(x)) (5) also with the right term : (1 (cid:88) xV α(x) q(x))r(x) = (1 = (1 = (1 (cid:88) xV (cid:88) xV (cid:88) xV min(q(x), p(x))) min(q(x), p(x))) (cid:80) (cid:80) (6) max(0, p(x) q(x)) xV max(0, p(x) q(x)) p(x) min(p(x), q(x)) xV p(x) min(p(x), q(x)) (7) min(q(x), p(x))) p(x) min(p(x), q(x)) 1 (cid:80) xV min(p(x), q(x)) (8) (9) = p(x) min(p(x), q(x)). So, adding two terms becomes p(x), the target distribution, as desired. Now, we will check the acceptance rate : Exq(x)min(1, p(x) q(x) ) = (cid:88) xV q(x) min(1, p(x) q(x) ) = (cid:88) xV min(p(x), q(x)) (10) = 1/2 (cid:88) xV p(x) + q(x) p(x) q(x) = 1 1 2 (cid:88) xV p(x) q(x) (11) which is 1 DT (p, q). A.2 PROOF OF PROPOSITION 2 To compute value of CSJD, let p(x) = p(t) (x) and q(x) = p(t1) CSJD(p, q) r[X (t) = (t1)] (cid:88) = r[X (t) = x, (t1) = x] (x) for simplicity. (12) (13) = = xV (cid:88) xV (cid:88) xV r[X (t) = x] r[X (t1) = x] (by independence) (14) p(x)q(x) (15) Now, we will derive its upper bound as follows :"
        },
        {
            "title": "Preprint",
            "content": "(CSJD(p, q))2 = (cid:32) (cid:88) (cid:32) (cid:88) (cid:33)2 p(x)q(x) (cid:33) (cid:32) (cid:88) (cid:33) q(x)2 p(x)2 (16) (by Cauchy-Schwarz) (17) Since Renyi-2 entropy is, by definition, H2(p) = log (cid:0)(cid:80) Hence, p(x)2(cid:1) = (cid:80) p(x)2 = eH2(p) , (CSJD(p, q))2 eH2(p) eH2(q) = e(H2(p)+H2(q)) 2 (H2(p)+H2(q)) = CSJD(p, q) (18) (19) So we can check that independence collision probability is exponentially restricted by their Renyi-2 entropy, regardless of how they are close to each other. A.3 PROOF OF THEOREM 1 Proof sketch. The theoretical correctness of our approach is based on the marginalization property of the couplings. The standard SJD framework requires that the draft token, which we denote (t) , be sampled from the draft distribution p(t) . If sample (t) follows its distribution, then correctness of SD framework is guaranteed by Proposition 1. According to Definition 1, when we sample pair (X (t) , the marginal distribution of the variable (t) component from the sampled pair is probabilistically identical to sampling token directly from p(t) . Since this modification preserves the required sampling distribution for the draft token at each step, the final output distribution of the algorithm is guaranteed to match that of the base model. ) π(, ) from any valid coupling π Π(t) , (t1) is precisely p(t) . Thus, using the (t) A.4 PROOF OF THEOREM We will formally check that MRS() satisfies the definition of Coupling. Let the joint distribution of this MRS() process (x, y) is (x, y) = q(x)(α(x)δx(y) + (1 α(x))r(y)) (20) where δx(y) is kronecker-delta symbol. Let α(x), r(x) is same as we defined on proof of proposition 1. Then for p(y), (cid:88) xV (x, y) = a(y) q(y) + (1 (cid:88) xV α(x)q(x))r(y) = p(y) (21) which directly came out from proof of proposition 1. Then next, for q(x), (cid:88) yV (x, y) = q(x)α(x) (cid:88) yV δx(y) + q(x)(1 α(x)) (cid:88) xV r(y) = q(x)α(x) + q(x)(1 α(x)) = q(x) (22) (23) So it satisfies the definition of Coupling. For the coupling cost optimality, it is well studied that any coupling can not have cost greater than 1 DT (P, Q) (Lindvall inequality) See (Lindvall, 2002; Bavarian et al., 2016)."
        },
        {
            "title": "B RELATED WORKS",
            "content": "Unified Multimodal Models. Recently, Unified Multimodal Models (Team, 2024; Deng et al., 2025; Hurst et al., 2024), which can process data from multiple modalities such as text, images, and audio for both input and output within single model, have gained significant attention. The advantage of this paradigm stems from the discovery that models trained on multiple data domains simultaneously exhibit superior performance across range of tasks compared to single-modality models. This includes enhanced understanding, generation (Chen et al., 2025), complex world reasoning (Hurst et al., 2024), instruction following, and iterative editing (Bai et al., 2023). Autoregressive Models in Vision Visual generation using an autoregressive (AR) (Team, 2024) approach is promising method for implementing Unified Multimodal Models. An AR vision model primarily consists of two key components: Vector Quantizer (Van Den Oord et al., 2017) and Transformer model (Brown et al., 2020). The vector quantizer divides an image into patches of specified size and maps each patch to discrete code from predefined codebook. This process effectively performs both downsampling and tokenization of the image. Subsequently, similar to autoregressive text generation, Transformer model is trained to predict these visual token IDs autoregressively. This paradigm enables the learning and inference of diverse data types under single, unified framework of AR modeling, naturally facilitating stable training, deployment, and capabilities such as in-context learning (Hurst et al., 2024), editing (Liu et al., 2024), and reasoning (Zhao et al., 2025). Speculative Decoding Speculative Decoding (SD) was first proposed by (Leviathan et al., 2023; Chen et al., 2023) to accelerate the inference speed of Large Language Models (LLMs) without compromising performance by generating multiple tokens at once. Later, (Sun et al., 2023) established connection between speculative sampling and optimal transport, proving that the tokenlevel acceptance scheme is theoretically optimal for individual tokens. More recently, (Sun et al., 2024b) showed that token-level acceptance is not globally optimal and that the block-wise acceptance approach is the theoretically optimal form of speculative decoding. As the theoretical optimality has been established, the recent research trend in SD has focused on designing better draft models (?Brown et al., 2024; Cai et al., 2024) or exploring methods that trade speed for slight degradation in quality (Bachmann et al., 2025; So et al., 2025). Parallel Decoding Parallel decoding, or fixed-point iteration (X), is widely used technique for rapidly finding the solution to specific system, from scientific computing for accelerating the solution of differential equations (Berinde, 2004) to, more recently, fast sampling of diffusion models (Shih et al., 2023). Building on this concept, (Song et al., 2021) first proposed using fixedpoint iteration to accelerate the sequential computation of neural networks. Based on the observation that this method guarantees the same result as sequential computation and always at least as faster than sequential when assuming fully parallelization model. Our method can be framed as novel methodology for accelerating the convergence speed of fixed-point (jacobi) iteration for sequential sampling that operates based on probabilistic process within discrete space."
        },
        {
            "title": "C EXPERIMENTAL DETAILS",
            "content": "C.1 IMAGE GENERATION Lumina mGPT: For Lumina-mGPT (Liu et al., 2024) , we use the standard 7B model and experiment with resolution of 768768. In all experiments, we follow the default settings of vanilla model, temperature τ = 1 and Top-K sampling with = 2000 and guidance scale of λ = 3.0. We used pytorch 2.3 (Paszke et al., 2019) for the main comparison. For quality evaluation, we generate 5000 images for each MS-COCO 2017 (val) (Lin et al., 2014) prompt and compute FID, IS, CLIP-Score with reference dataset. Janus Pro : For Janus-Pro (Chen et al., 2025), we use 7B model to generate images at resolution of 384 384. Following the setup of the vanilla Janus-Pro 7B model, 24 24 of image tokens are generated with downsampling size of 16. For sampling, we follws vanilla setting that guidance scale of 5.0 and temperature of 1.0. We also adopted Top-K logits processor with = 1000. For evaluation, we generate three images for each MS-COCO (val) prompt with different seeds (50003) and reported the mean values of the FID, IS, and CLIP score across the seeds."
        },
        {
            "title": "Preprint",
            "content": "C.2 VIDEO GENERATION Cosmos1-autoregressive. We evaluate our method on the Cosmos-1.0-Autoregressive-4B video AR model (Agarwal et al., 2025) using curated subset of 150 clips from the real-state-10k dataset (Zhou et al., 2018). For each clip, we provide 9-frame context to the model and autoregressively generate the next 24 frames, yielding 33-frame sequences in total (9 observed + 24 predicted). Unless otherwise noted, decoding uses nucleus (top-p) sampling with = 0.8 and temperature 1.0. We compare three decoders: (A) vanilla AR, (B) Speculative Jacobi Decoding (SJD), and (D) our MC-SJD on top of SJD. For SJD-based methods we sweep the parallel verification window {16, 32, 64, 128}. Speed is reported as (i) NFEthe number of sequential target-model evaluationsand (ii) end-to-end wall-clock Latency (seconds) measured on single RTX6000ADA. Quality is measured by FVD (lower is better), computed between the generated frames and the corresponding ground-truth future frames of each clip."
        },
        {
            "title": "D ALGORITHMS",
            "content": "We provide complete pseudo code of our SJD with Gumbel Coupling in Algorithm 5. Random(); Algorithm 5 Pseudo Code for our GS-SJD Require: AR Model pθ, draft Length L, Max Sequence pt 1: pi 2: for = 0 to 1: 3: Gj SampleGumbelNoise(V) 4: while < do 5: 6: parallel for = to + : , Gj) j, GS(pt j, pt1 X 7: 8: 9: 10: parallel for = to + : pt+1 pθ( for = to + : <j) k, t+1 MRS(pt+1 , pt j, j), if = 0 : break Initialize state Initialize shared Gumbel noise (Alg. 6) Drafting Evaluate Verify j, + 1 11: 12: end while 13: return Algorithm 6 SampleGumbelNoise(V) Input: Vocabulary size = V. Output: Gumbel noise vector of size . 1: [ ] 2: for = 1 do 3: 4: 5: 6: end for 7: return ui U(0, 1) gi log( log(ui)) Append gi to G"
        },
        {
            "title": "E MORE VISUALIZATION",
            "content": "Initialize an empty list Sample from standard uniform distribution Apply inverse transform sampling In this section, we provide further details about the visualization settings and discuss our findings based on both quantitative and qualitative results. For image generation, we employed prompts covering diverse categories such as humans, animals, landscapes, close-up shots, fantasy, and paintings. In particular, we included prompts designed to capture physical phenomena such as reflections and"
        },
        {
            "title": "Preprint",
            "content": "Figure 7: Qualitative comparison on Janus-Pro 7B waves. We also incorporated descriptors explicitly indicating high-quality imagery (e.g., 8K, sharp focus) to encourage the generation of fine-detailed, realistic images. As shown in Figs. 7, 8, 9, 10, we observed that our method produced images closely resembling those of the vanilla AR model while achieved more than 4 reduction in NFE in image generation and 13 in video generation. Moreover, our model was able to generate diverse categories of images, including physical phenomena like reflections and waves, under both the maximal coupling and the Gumbel coupling."
        },
        {
            "title": "Preprint",
            "content": "Figure 8: Qualitative comparison on Lumina-mGPT (1.0)"
        },
        {
            "title": "Preprint",
            "content": "Figure 9: Qualitative comparison on Lumina-mGPT 2."
        },
        {
            "title": "Preprint",
            "content": "Figure 10: Qualitative comparison on Video Generation ( Cosmos-1-ar )"
        },
        {
            "title": "F USE OF LLM",
            "content": "We used Large Language Model (LLM) for typo checking, grammar correction, and polishing of our paper draft."
        }
    ],
    "affiliations": [
        "POSTECH, South Korea"
    ]
}