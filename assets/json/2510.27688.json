{
    "paper_title": "Continuous Autoregressive Language Models",
    "authors": [
        "Chenze Shao",
        "Darren Li",
        "Fandong Meng",
        "Jie Zhou"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "The efficiency of large language models (LLMs) is fundamentally limited by their sequential, token-by-token generation process. We argue that overcoming this bottleneck requires a new design axis for LLM scaling: increasing the semantic bandwidth of each generative step. To this end, we introduce Continuous Autoregressive Language Models (CALM), a paradigm shift from discrete next-token prediction to continuous next-vector prediction. CALM uses a high-fidelity autoencoder to compress a chunk of K tokens into a single continuous vector, from which the original tokens can be reconstructed with over 99.9\\% accuracy. This allows us to model language as a sequence of continuous vectors instead of discrete tokens, which reduces the number of generative steps by a factor of K. The paradigm shift necessitates a new modeling toolkit; therefore, we develop a comprehensive likelihood-free framework that enables robust training, evaluation, and controllable sampling in the continuous domain. Experiments show that CALM significantly improves the performance-compute trade-off, achieving the performance of strong discrete baselines at a significantly lower computational cost. More importantly, these findings establish next-vector prediction as a powerful and scalable pathway towards ultra-efficient language models. Code: https://github.com/shaochenze/calm. Project: https://shaochenze.github.io/blog/2025/CALM."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 1 3 ] . [ 1 8 8 6 7 2 . 0 1 5 2 : r a"
        },
        {
            "title": "CONTINUOUS AUTOREGRESSIVE LANGUAGE MODELS",
            "content": "Chenze Shao1, Darren Li1,2, Fandong Meng1, Jie Zhou1 1WeChat AI, Tencent Inc 2Qiuzhen College, Tsinghua University"
        },
        {
            "title": "ABSTRACT",
            "content": "The efficiency of large language models (LLMs) is fundamentally limited by their sequential, token-by-token generation process. We argue that overcoming this bottleneck requires new design axis for LLM scaling: increasing the semantic bandwidth of each generative step. To this end, we introduce Continuous Autoregressive Language Models (CALM), paradigm shift from discrete next-token prediction to continuous next-vector prediction. CALM uses high-fidelity autoencoder to compress chunk of tokens into single continuous vector, from which the original tokens can be reconstructed with over 99.9% accuracy. This allows us to model language as sequence of continuous vectors instead of discrete tokens, which reduces the number of generative steps by factor of K. The paradigm shift necessitates new modeling toolkit; therefore, we develop comprehensive likelihood-free framework that enables robust training, evaluation, and controllable sampling in the continuous domain. Experiments show that CALM significantly improves the performance-compute trade-off, achieving the performance of strong discrete baselines at significantly lower computational cost. More importantly, these findings establish next-vector prediction as powerful and scalable pathway towards ultra-efficient language models. Code: https://github.com/shaochenze/calm Project: https://shaochenze.github.io/blog/2025/CALM"
        },
        {
            "title": "INTRODUCTION",
            "content": "Large Language Models (LLMs) have revolutionized the field of artificial intelligence, demonstrating unprecedented capabilities in understanding, generating, and reasoning with human language (Achiam et al., 2023; Google, 2025; DeepSeek-AI, 2025). However, this remarkable success is shadowed by critical challenge: their immense computational demands. The training and inference of state-of-the-art LLMs demand massive computational resources, leading to prohibitive expenses and significant environmental concerns (Strubell et al., 2019; Bender et al., 2021). At the heart of this inefficiency lies the foundational paradigm of these models: an autoregressive generation process that operates on sequence of discrete tokens. Because the computational cost scales with the length of the sequence, generating long-form text or processing extensive contexts remains fundamental bottleneck, limiting the scalability and accessibility of these powerful models. The now-ubiquitous use of discrete tokens in LLMs is the result of pivotal evolution from earlier modeling paradigms. Initially, models that operated at the character level struggled with the computational burden of extremely long sequences (Sutskever et al., 2011; Kim et al., 2016). The subsequent shift to modern subword tokenization (Sennrich et al., 2016) was driven by crucial insight: increasing the information density of each text unit reduces sequence length and dramatically boosts model efficiency. This historical success suggests clear path for unlocking the next order of magnitude in efficiency: continue to increase the semantic bandwidth of each predictive unit. We argue, however, that this path has reached fundamental limit, constrained by the very nature of discrete representation. With typical vocabularies in modern LLMs ranging from approximately 32,000 to 256,000 entries, each token carries surprisingly small amount of informationmerely 15 to 18 bits (e.g., log2(32768) = 15). To increase this capacityfor instance, to represent whole phrasethe vocabulary size would need to grow exponentially, making the final softmax computation over this vocabulary an untenable bottleneck. This reveals critical limitation: the information Corresponding author."
        },
        {
            "title": "Preprint",
            "content": "Autoencoder (K=3 tokens to 1 vector)"
        },
        {
            "title": "The",
            "content": "cat sat on the mat Vector Vector 2 Sequence Length = Sequence Length = T/K Conventional LM: Next-Token Prediction CALM: Next-Vector Prediction Figure 1: Comparison between conventional token-by-token generation and our proposed vector-byvector framework (CALM). By compressing tokens into single vector, we reduce the sequence length K-fold, fundamentally improving computational efficiency. density of discrete tokens is not scalable. Consequently, profound mismatch has emerged: while model capacity has scaled to unprecedented levels, the task itselfpredicting low-information discrete units one at timehas not evolved. We are now deploying models of immense representational power on task that fundamentally limits their throughput, forcing them to laboriously predict simple, low-information tokens one by one. In this work, we confront this limitation directly by introducing paradigm shift from discrete tokens to continuous-domain representation. Central to our approach is an autoencoder trained to compress chunk of tokens into single, dense continuous vector and, crucially, reconstruct the original tokens from this vector with high fidelity. Unlike the discrete paradigm, where increasing information density requires an exponential growth in vocabulary size, our continuous representation offers scalable path forward: the vectors information capacity can be gracefully expanded by simply increasing its dimensionality to accommodate larger K. This design directly reduces the number of autoregressive steps by factor of K. Ultimately, it allows us to reframe language modeling from task of next-token prediction on discrete token sequences to next-vector prediction on continuous vector sequences, as conceptually illustrated in Figure 1. However, shifting to the continuous domain introduces significant challenge: without finite vocabulary, model cannot compute an explicit probability distribution over all possible outcomes using standard softmax layer. To address this, we develop comprehensive, likelihood-free framework for our Continuous Autoregressive Language Models (CALM). Our primary contributions, which structure the remainder of this paper, are as follows: Powerful and Lightweight Autoencoder (Section 2): We first introduce an efficient autoencoder architecture designed to produce robust vector representations. We demonstrate that this model can be both compact and powerful, ensuring high-fidelity reconstruction of the original tokens, which is prerequisite for the downstream language modeling task. Likelihood-Free Language Modeling (Section 3): To perform generative modeling in the continuous vector space, we employ lightweight generative head that conditions on the last hidden state to generate the output vector. While the generative head can be any continuous generative model, options like Diffusion (Ho et al., 2020; Li et al., 2024) or Flow Matching (Lipman et al., 2023) rely on an iterative sampling process, re-introducing significant inference bottleneck. Our framework therefore specifically adopts the Energy Transformer (Shao et al., 2025b), recent architecture designed for efficient, single-step generation of continuous vectors, while empirically demonstrating superior generation quality. Likelihood-Free LM Evaluation (Section 4): The absence of explicit likelihoods makes traditional metrics like Perplexity inapplicable. We address this by proposing BrierLM, novel metric for language modeling based on the Brier score (Brier, 1950). We show that BrierLM is strictly proper, theoretically ensuring fair comparison of model capabilities. Crucially, BrierLM can be estimated unbiasedly by only drawing samples from the model, making it perfectly suited for CALM where likelihoods are intractable."
        },
        {
            "title": "Preprint",
            "content": "Likelihood-Free Temperature Sampling (Section 5): Controlled generation via temperature sampling is an indispensable feature of modern LLMs, yet it relies on the explicit manipulation of probability distribution. We introduce principled, likelihood-free sampling algorithm that can, in theory, draw samples from the exact temperature distribution, and we accompany it with highly efficient batch approximation. We empirically validate our CALM framework on standard language modeling benchmarks, which demonstrates superior performance-compute trade-off. For instance, CALM grouping K=4 tokens delivers performance comparable to strong discrete baselines, but at significantly lower computational cost. This findings highlight new design axis for language models: rather than solely scaling parameters and data for performance, one can now scale the information capacity of each step as powerful new lever for computational efficiency."
        },
        {
            "title": "2.1 HIGH-FIDELITY RECONSTRUCTION",
            "content": "The foundational component of our CALM framework is an autoencoder tasked with learning bijective mapping between chunk of discrete tokens and continuous vector. Formally, we seek an encoder fenc : Rl and decoder gdec : Rl K, where is the vocabulary, such that for given token sequence x1:K = (x1, . . . , xK), the reconstruction gdec(fenc(x1:K)) closely approximates x1:K. For simplicity and computational efficiency, we design our autoencoder to be context-free, meaning it processes each token chunk independently of its surrounding sequence. context-aware autoencoder that also conditions on previous vector representations is natural and promising next step, which we leave for future exploration. The encoder begins by mapping the input sequence x1:K to embeddings. Each embedding is independently processed by position-wise feed-forward network (FFN). The resulting hidden states are then flattened and compressed by linear layer: RKd Rd. This unified representation is passed through second FFN and linear projection to produce the l-dimensional latent vector z. The decoder architecture mirrors the encoder. It first transforms using linear layer and an FFN to obtain d-dimensional hidden state, which is then expanded by another linear layer to dimension Kd and reshaped into sequence of hidden states. Each of these states is passed through second FFN, followed by projection to vocabulary logits using the tied input embedding matrix. Finally, the tokens are reconstructed by applying an argmax operation over these logits. The autoencoder is trained to minimize the reconstruction error by optimizing the standard crossentropy loss across all token positions: Lae(x1:K) = (cid:88) i=1 log pdec(xiz = fenc(x1:K)). (1) We empirically validate this architecture and find it to be both highly effective and efficient. For instance, when grouping = 4 tokens, latent vector of just = 10 dimensions is sufficient to achieve high-fidelity reconstruction, with token-level accuracy of over 99.9%. Moreover, the autoencoder is exceptionally lightweight; with shallow architecture and modest hidden dimension of = 512, its computational overhead is nearly negligible compared to that of language model. 2.2 ROBUST VECTOR REPRESENTATION While the autoencoder described above achieves near-perfect reconstruction, we found that it is practically impossible to effectively train continuous language model based on the vector space it produces. The root cause of this challenge is that an autoencoder optimized solely for reconstruction learns an exceptionally brittle representation. Lacking any incentive to form smooth latent manifold, the encoder learns to pack information with maximum efficiency, creating highly irregular mapping. In such space, minor perturbation to latent vector zsuch as the small, inevitable errors made by generative model can cause the decoder to reconstruct completely unrelated token sequence. Therefore, for our CALM framework to be viable, the autoencoder must satisfy another critical objective: its vector representation should be robust."
        },
        {
            "title": "Preprint",
            "content": "Variational Regularization. To build robust latent space, our primary strategy is to smooth the latent manifold by moving from deterministic autoencoder to variational one (Kingma & Welling, 2014), aligning our approach with prominent generative models (Rombach et al., 2022; Liu et al., 2023) that operate within smooth and structured latent space. Instead of mapping an input chunk directly to vector z, the encoder now outputs the parameters of diagonal Gaussian distribution, µ and σ, from which the latent vector is sampled: (µ, σ2I). This change is accompanied by new objective term, KL divergence loss that penalizes the deviation of the encoded distribution from standard normal prior, (0, I). The total loss function is thus weighted sum of the reconstruction and regularization terms: Ltotal = Lae + β LKL, (2) where β is hyperparameter balancing the two objectives (we set β = 0.001), and LKL is the KL divergence, defined as: LKL(pE(zx1:K)N (0, I)) = 1 (cid:88) (1 + log σ2 σ2 µ2 ). i= (3) This variational objective discourages the encoder from relying on arbitrarily precise or largemagnitude values in z, thereby promoting smoother and more regularized latent manifold that is more amenable to generative modeling. Preventing Posterior Collapse. significant challenge in training VAEs is posterior collapse. This issue manifested in our model as tendency for some latent dimensions to fully collapse to the standard normal prior. While collapsing dimension drives its KL divergence to zero, it renders that dimension uninformative for reconstruction. More critically, these pure noise dimensions introduce chaotic signal that interferes with the training of the downstream language model, destabilizing the learning process. To mitigate this, we adopt the KL clipping strategy from Kingma et al. (2016), which modifies the objective by clipping each dimensions KL loss at constant floor: Lclip KL = (cid:88) i=1 max(λKL, LKL,i), (4) where LKL,i is the KL divergence for the i-th dimension and λKL is the threshold (we use λKL = 0.5). This technique ensures that every dimension is encouraged to actively participate in reconstruction, thus preventing collapse and fostering dense, structured representation. Dropout for Enhanced Robustness. Beyond structuring the latent space with variational methods, we further enhance its robustness by injecting noise during training using two complementary forms of dropout. First, we apply dropout with rate of = 0.15 to the latent vector before it is passed to the decoder. This forces the autoencoder to learn redundant representation, making it robust to minor prediction errors from the downstream generative model. Second, we apply dropout to input tokens by randomly masking fraction (p = 0.15) of tokens. Analogous to the Continuous Bag-ofWords (CBOW) method (Mikolov et al., 2013), this compels the autoencoder to infer masked tokens from their context, thereby enriching the latent vector with the chunks semantic context rather than just performing simple token-index compression. Crucially, these dropout techniques are employed exclusively during the autoencoders training phase to build robust latent representation; they are disabled during the subsequent training and inference of the continuous language model. The synthesis of these techniques produces powerful and robust autoencoder. For chunk of = 4 tokens, we now employ latent vector of = 128 dimensions, providing the necessary capacity to encode information redundantly. The encoder learns posterior distribution where the standard deviations, σi, converge to approximately 0.3. This means that sampling the latent vector effectively perturbs the predicted mean µ with substantial Gaussian noise σ 0.3I. Despite this significant latent perturbation, the decoder still maintains token-level accuracy exceeding 99.9%. This vector representation, which combines high fidelity with high robustness, lays solid foundation for the subsequent learning of Continuous Autoregressive Language Models (CALM)."
        },
        {
            "title": "3.1 NEXT-VECTOR PREDICTION",
            "content": "The autoencoder developed in Section 2 establishes robust and high-fidelity mapping between chunk of discrete tokens and single continuous vector, which allow us to reframe language modeling from task of next-token prediction on discrete token sequences to next-vector prediction on continuous vector sequences. Specifically, sequence of tokens, = (x1, . . . , xT ), is first grouped into = /K non-overlapping chunks. The encoder, fenc, then transforms the original sequence into new, more compact sequence of continuous vectors: = (z1, z2, . . . , zL), where zi = fenc(x(i1)K+1, . . . , xiK). Consequently, the autoregressive objective evolves to predicting the next vector in the sequence: (cid:89) p(Z) = p(ziz<i). (5) (6) i=1 While this autoregressive structure is preserved, the underlying mechanism for predicting the next element must be redesigned. Unlike standard language models, which rely on softmax layer to compute probability distribution over finite vocabulary, our model must predict vector within the infinite space Rl. The softmax function is not applicable over this uncountable set, rendering the explicit probability density p(ziz<i) intractable. This introduces two critical challenges: Training: The likelihood p(ziz<i) becomes intractable, precluding the use of maximum likelihood estimation (i.e., minimizing cross-entropy loss) for training. Evaluation: Standard evaluation metrics like Perplexity, which are derived directly from the models likelihood, can no longer be computed to measure model performance. We address both of these challenges in turn. For the training problem, we introduce our approach to likelihood-free language modeling in the remainder of this section. For the evaluation problem, we propose likelihood-free evaluation methodology in Section 4. 3.2 GENERATIVE HEAD Generative modeling of continuous data (Kingma & Welling, 2014; Goodfellow et al., 2014; Ho et al., 2020) is well-established field, foundational to domains such as image and audio synthesis where data is inherently continuous. promising recent paradigm (Tschannen et al., 2023; Li et al., 2024; Shao et al., 2025b) combines these approaches with autoregressive models: Transformer backbone predicts conditioning hidden state, which is used by subsequent generative model to produce the continuous output for each step. Our Continuous Autoregressive Language Models (CALM) adapts this paradigm, but with critical focus on computational efficiency that constrains the design of this generative component. We therefore conceptualize this component as lightweight generative head. Formally, the generative head is stochastic function that takes the Transformers hidden state, hi1 Rd, and draws sample zi Rl from the conditional distribution: hi1 = Transformer(z1:i1), zi p(hi1). (7) While the generative head can be any continuous generative model, prominent options like Diffusion (Ho et al., 2020; Li et al., 2024; Fan et al., 2025) or Flow Matching (Lipman et al., 2023; Ren et al., 2025a;b) are misaligned with our goal of efficiency. These models rely on an iterative sampling processrequiring dozens or even hundreds of network evaluations to produce single vectorwhich directly counteracts the speedup gained from reducing the number of autoregressive steps. The CALM architecture therefore demands generative head capable of high-quality, single-step generation, challenge we address next with an energy-based objective. 3.3 ENERGY TRANSFORMER 3.3.1 STRICTLY PROPER SCORING RULES To meet the demand for generative head capable of high-quality, single-step generation, we draw inspiration from Shao et al. (2024; 2025b), which frames the generative task as the optimization of"
        },
        {
            "title": "Preprint",
            "content": "strictly proper scoring rules (Gneiting & Raftery, 2007). Formally, scoring rule S(P, y) assigns numerical score to predictive distribution upon observing an outcome y, where higher scores are better. The quality of forecast against the true data-generating distribution is measured by its expected score, defined as S(P, Q) = EyQ[S(P, y)]. scoring rule is considered proper if the expected score is maximized when the predictive distribution matches the data distribution Q: S(P, Q) S(Q, Q) for all distributions P. (8) This property ensures that the scoring rule does not incentivize the model to predict biased or distorted distribution. Furthermore, scoring rule is strictly proper if equality holds only when = Q, meaning that the optimal score can only be achieved by reporting the true distribution. The use of strictly proper scoring rule as training objective is therefore powerful and principled approach for training our generative head, as maximizing the expected score is equivalent to driving the models predictive distribution to match the true distribution. This principle offers direct generalization of maximum likelihood estimation, where the negative log-likelihood is special case corresponding to the logarithmic score (Good, 1952). While the likelihood is intractable in the continuous domain, the theory of scoring rules provides rich family of alternatives. 3.3.2 ENERGY LOSS We build our training objective using the Energy Score (Szekely, 2003), strictly proper scoring rule that has proven effective across range of generative applications (Gritsenko et al., 2020; Vahidi et al., 2024; Pacchiardi et al., 2024; Shao et al., 2025b; Ma et al., 2025). The energy score is entirely likelihood-free; rather than evaluating probability densities, it measures the alignment between the prediction and the observation via sample distances. For predictive distribution and ground truth observation y, the energy score is defined as: S(P, y) = Ex,xP [x xα] 2 ExP [x yα], (9) where x, and are independent samples drawn from . The score is strictly proper for any α (0, 2). Typically, α is set to 1. The first term encourages diversity, penalizing the model for producing collapsed or overly confident predictions where all samples are identical. The second term encourages fidelity, driving the models predictions to be close to the ground truth observation. While the expectations in Equation 9 make the energy score intractable to compute exactly, we can construct an unbiased Monte Carlo estimator to serve as practical loss function, which we term the energy loss. To do this, we draw candidate samples, {zi,1, . . . , zi,N }, from the generative head at each step i. Furthermore, we leverage unique property of our setup: our autoencoder does not map token chunk to fixed point, but rather to conditional Gaussian posterior zi q(x(i1)K+1:iK). Relying on single sample zi as ground-truth can introduce high variance into the energy loss. To mitigate this and stabilize training, we draw target samples, {zi,1, . . . , zi,M }, from this posterior. Combining these sample sets, the final energy loss is formulated as: Lenergy = (cid:88) ( i=1 2 (cid:88) (cid:88) n=1 m= zi,m zi,n 1 (N 1) (cid:88) n=k zi,n zi,k). (10) In practice, we set = 8 and = 100. The number of model samples directly scales the training cost, as each sample requires an evaluation of the generative head; we therefore use small to maintain high training efficiency. The overhead of drawing target vectors from known Gaussian posterior is almost negligible, which allows us to use large to reduce the variance of loss. key advantage of this likelihood-free training objective is its flexibility: it only requires the ability to draw samples from the generative head, placing minimal constraints on its internal architecture and allowing for the simple and efficient designs we explore next. 3.3.3 MODEL ARCHITECTURE We now detail our model architecture. We use standard Transformer backbone, with modifications focused on the output-side generative head and the input-side adaptation. Energy-Based Generative Head. The inputs to the generative head are twofold: the hidden state hi1 from the Transformer backbone, which provides the conditional context, and random noise"
        },
        {
            "title": "Preprint",
            "content": "Random Noise ε0 Transformer Backbone Energy-Based Generative Head Input Compression MLP Token Embeddings εL Linear AE Decoder εl+ + SwiGLU Linear Linear εl Input Tokens Output Tokens Figure 2: The Architecture of the Continuous Autoregressive Language Model (CALM). Left: The main autoregressive loop where discrete tokens are compressed to condition Transformer, whose output hidden state guides an energy-based head to predict continuous vector z. The AE decoder then maps back to discrete tokens for the next step. Right: detailed view of the generative head, showing how it refines noise vector ε0 through series of residual MLP blocks. vector ε Rdnoise , which provides the necessary stochasticity for sampling. Each dimension of ε is drawn independently from uniform distribution U[0.5, 0.5]. Both the hidden state hi1 and the noise vector ε are projected by independent linear layers to match the heads internal dimension, which we set to match the Transformers hidden dimension d. The core of the generative head is stack of residual MLP blocks that progressively refine the initial noise representation ε0 = ε into the final output vector. As illustrated in Figure 2, each MLP block first fuses the current representation εl with the hidden state via two linear layers. This is followed by SwiGLU layer (Shazeer, 2020b) with an intermediate dimension of d. residual connection then adds the blocks input to its output. This process concludes with final linear layer that projects the representation to the target dimension l, producing the output vector zi. single MLP block contains approximately 6d2 parameters. We set the number of blocks to quarter of the number of Transformer layers; the entire generative head therefore accounts for only about 10% of the total model parameters, making its computational overhead minimal. Discrete Token Input. An intuitive approach for the models input would be to embed the predicted latent vectors zi1 from the previous step into the Transformers hidden dimension using linear projection. However, we empirically found that using these latent vectors as input for the Transformer leads to noticeable degradation in performance, as the model struggles to unpack the semantic information from such compact input representation. To circumvent this, we ground the models autoregressive process in the discrete token space. During training, the input for each step is formed by the tokens from the previous step. To maintain efficiency, we use lightweight input compression modulea two-layer MLPto map the embeddings into single input representation. The inference process unfolds as follows: 1. Input Processing: At step i, the previously generated chunk of tokens are embedded and compressed into single input representation and fed into the Transformer. 2. Continuous Prediction: The Transformer outputs the hidden state hi1, which our energybased generative head then uses to predict the next continuous vector, zi."
        },
        {
            "title": "Preprint",
            "content": "3. Discrete Feedback Loop: The predicted vector zi is immediately passed through the frozen decoder of our pre-trained autoencoder, gdec, to reconstruct the next discrete tokens. The complete architecture of CALM is illustrated in Figure 2."
        },
        {
            "title": "4.1 PRINCIPLES OF LM EVALUATION",
            "content": "The CALM framework operates as an implicit generative model, whose predictive probability distribution is defined by its sampling process. Consequently, standard LM evaluation metrics like Perplexity, which are defined in terms of explicit likelihoods, can no longer be employed to measure model performance. Furthermore, the energy loss used for training is itself unsuitable for evaluation, as its magnitude is subjective to the specific latent space shaped by the autoencoder. This necessitates the development of model-agnostic evaluation metric, one that can faithfully assess language modeling capabilities in principled, yet entirely likelihood-free, manner. The goal of evaluation metric is to quantify the divergence between the models predictive distribution, , and the true data distribution, Q. This principle is formalized by the property that the metric is uniquely optimized when the model accurately recovers the data distribution (P = Q). This ensures the evaluation is fair and cannot be hacked by model that systematically distorts its predictions. For instance, the conventional metric of Perplexity serves as prime example of this principle. It is grounded in the expected negative log-likelihood, which can be decomposed into the sum of the KL divergence and data entropy: (cid:21) (cid:20) EyQ[ log (y)] = EyQ log Q(y) (y) + EyQ[ log Q(y)] = DKL(QP ) (cid:125) (cid:123)(cid:122) (cid:124) Minimized at =Q . + H(Q) (cid:124) (cid:123)(cid:122) (cid:125) Constant (11) This property establishes Perplexity as theoretically sound measure of models capability to capture the true distribution, which is uniquely minimized when = Q. In contrast, naive metric like the raw likelihood of the observed outcome, (y), fails this principle. The expected score under this metric, EyQ[P (y)], is maximized by deterministic prediction that assigns probability of 1 to the single most frequent outcome, i.e., (arg maxy Q(y)) = 1. Such metric would therefore incorrectly favor an overconfident model that fails to capture the underlying data uncertainty. This highlights critical distinction: principled metric must balance rewarding accuracy with correctly representing the predictive uncertainty. The naive likelihood (y) only addresses the former, making it an inadequate measure of models predictive quality. 4.2 BRIERLM: BRIER FOR LANGUAGE MODELING For principled and likelihood-free evaluation, we turn to the Brier score (Brier, 1950), classic strictly proper scoring rule now widely used to assess the calibration of modern neural networks (Lakshminarayanan et al., 2017; Ovadia et al., 2019; Gruber & Buettner, 2022). For predictive distribution and ground-truth outcome y, the Brier score is defined as: Brier(P, y) = 2P (y) (x)2. (cid:88) (12) Unlike the raw likelihood (y), which solely measures accuracy, the Brier score incorporates an additional term, (cid:80) (x)2, to quantify predictive uncertainty. This structure balances two competing objectives, which ultimately rewards well-calibrated prediction. This property is revealed by the following decomposition of the expected Brier score: EyQ[Brier(P, y)] = (cid:88) (P (x) Q(x))2 + (cid:88) Q(x)2 . (13) (cid:124) (cid:123)(cid:122) Squared Error (minimized at =Q) (cid:125) (cid:124) (cid:123)(cid:122) Data Variance (constant) (cid:125) While the Brier score is theoretically sound, its direct computation remains intractable for CALM, as it requires knowledge of the full predictive distribution . We find, however, that an unbiased"
        },
        {
            "title": "Preprint",
            "content": "Monte Carlo estimator for the Brier score can be constructed in an entirely likelihood-free manner, using only samples drawn from the model. Specifically, the uncertainty term, (cid:80) (x)2, can be interpreted as the collision probability of two independent samples. Therefore, its unbiased estimator is simply the indicator function I{x1 = x2}, where x1, x2 . Similarly, the accuracy term (y) can be estimated by I{x = y} using single sample . Combining these, we construct practical, unbiased estimator for the Brier score using two samples drawn from the model: Brier(P, y) I{x1 = y} + I{x2 = y} I{x1 = x2}, x1, x2 P. (14) This estimator enables likelihood-free evaluation of CALMs predictive capabilities. straightforward approach is to assess next-token prediction performance in teacher-forcing setting. This would involve generating two latent vectors at each step, decoding them using the frozen autoencoders decoder, and computing the Brier score using only the first token of each resulting chunk. However, such an evaluation is insufficient as it ignores the generation quality of the remaining 1 tokens. To address this limitation, we further introduce Brier-n, metric that computes the Brier score over entire n-grams. In this formulation, the indicator functions of the estimator treat the n-gram as single, atomic outcome. Finally, following the convention of established n-gram-based metrics like BLEU (Papineni et al., 2002), we define our composite metric, BrierLM (Brier for Language Modeling), as the geometric mean of Brier-n scores for = 1 to 4, which we then scale by 100 to place it on more interpretable 0-100 range: BrierLM = 100 (cid:33)0.25 Brier-n . (cid:32) 4 (cid:89) n=1 (15) The utility of BrierLM extends beyond CALM, serving as universal evaluation protocol that is also applicable to conventional autoregressive models. For such models, the BrierLM estimator can be applied by simply drawing samples from the final softmax distribution, enabling direct and fair comparisons with our likelihood-free framework. To validate this, we evaluated both cross-entropy and BrierLM throughout the training of our baseline autoregressive models (detailed in Section 7.1). Figure 3 visualizes the joint distribution of the two metrics. Intriguingly, we find that BrierLM is highly consistent with cross-entropy loss, exhibiting nearly linear relationship with Pearson correlation coefficient of -0.966 and Spearmans rank correlation of -0.991. This strong monotonic alignment confirms that BrierLM is reliable measure of language modeling capability, establishing it as trustworthy likelihood-free alternative to Perplexity. Figure 3: Joint distribution of the cross-entropy loss and the BrierLM score across different models and training checkpoints. Furthermore, BrierLM offers particularly significant advantage for the growing class of implicit generative models, such as diffusion-based language models (Austin et al., 2021; Han et al., 2023; Lou et al., 2024; Arriola et al., 2025). These models have historically been challenging to evaluate, often relying on the complex and sometimes loose estimation of variational lower bounds (ELBOs) to approximate Perplexity. BrierLM circumvents this entire challenge, offering direct, unbiased method to faithfully assess their language modeling capabilities and enabling fair comparisons across different model families."
        },
        {
            "title": "Preprint",
            "content": "else x1 restart from stage 1 Integer part of 1/T Fractional part, 0 α < 1 1/T . α 1/T n. Stage 1: Integer Part (n) Draw i.i.d. samples x1, . . . , xn if x1 = = xn then Algorithm 1 Likelihood-free Temperature Sampling Input: base sampler for an implicit discrete distribution (x); target temperature (0, 1) Output: Sample accepted with probability PT (x) (x)1/T 1: procedure SAMPLEATTEMPERATURE(S, ) 2: 3: 4: 5: 6: 7: 8: 9: 10: 11: 12: 13: 14: 15: 16: 17: 18: 19: 20: 21: 22: 23: Stage 2: Fractional Part (α) 1 loop Draw if = then return Accept as Stage 2 is not needed Draw U(0, 1) if < α/i then if α = 0 then return Continue to next iteration Uniform distribution Find candidate restart from stage 1 Accept candidate Rejection Rejection + 1 else else"
        },
        {
            "title": "5 LIKELIHOOD-FREE TEMPERATURE SAMPLING",
            "content": "5.1 EXACT TEMPERATURE SAMPLING VIA REJECTION Controlled generation via temperature sampling is an indispensable feature of modern LLMs. Conventionally, this technique is implemented by rescaling pre-softmax logits, mechanism that requires explicit access to the models probability distribution. However, this approach is incompatible with our CALM framework, whose generative head is likelihood-free and provides only sampler. This presents critical challenge: performing temperature sampling with only black-box sampler. In this section, we address this challenge by developing an exact algorithm, grounded in the principles of rejection sampling, that provably achieves this goal. The intuition for our algorithm stems from the relationship between repeated sampling and probability exponentiation. In the context of CALM, sample corresponds to complete chunk of tokens produced at each step. Consider the simple case where the temperature = 1/n for an integer n, which makes the target distribution PT (x) (x)n. The probability of drawing the exact same sample in independent trials from the sampler is also (x)n. This motivates an elegant rejection sampling scheme: we draw samples and accept them if and only if all samples are identical. Otherwise, we reject the entire set and restart the process. The distribution of accepted samples is thus provably proportional to (x)n, providing foundation for our general algorithm. To generalize this approach for any arbitrary temperature (0, 1), we decompose the exponent 1/T into its integer part, = 1/T , and fractional part, α = 1/T n. This decomposition structures our algorithm as two-stage rejection sampling process. The first stage handles the integer component using the repetition-based scheme described above, producing candidate sample only if independent draws are identical. The second stage, which handles the fractional exponent α, requires more subtle approach. Here, we draw upon the theory of Bernoulli Factory (Keane & OBrien, 1994; Mendo, 2019) to construct an iterative procedure that simulates biased coin flip with success probability of (x)α. sample is accepted only if it passes both stages; failure at any point triggers restart of the entire process. The complete procedure is formally detailed in Algorithm 1. The following theorem guarantees its correctness."
        },
        {
            "title": "Preprint",
            "content": "Theorem 1. For an implicit discrete distribution (x) with sampler and temperature (0, 1), Algorithm 1 generates samples distributed as: PT (x) = (x)1/T ZT , ZT = (x)1/T . (cid:88) The proof is provided in Appendix A.1."
        },
        {
            "title": "5.2 EXPECTED SAMPLING COST",
            "content": "While Algorithm 1 provides an exact solution for likelihood-free temperature sampling, its practical viability hinges on its computational efficiency. central concern is the expected number of samples it requires, as each sampler call involves forward pass through the generative head and autoencoder. Although these forward passes can be executed in parallel during inference, prohibitively large number of samples would still create significant computational bottleneck. The following theorem provides closed-form expression for this expected number of sampler calls, with Corollary 2.1 offering more interpretable upper bound. The proof is provided in Appendix 2. Theorem 2. The expected number of calls to the base sampler S, denoted E[Ntotal], required to generate one sample using Algorithm 1 is: E[Ntotal] = + I(α > 0) (cid:80) ZT (x)1/T 1 where ZT = (cid:80) (x)1/T , = 1/T , α = 1/T n, and I() is the indicator function. Corollary 2.1. Let be the size of sample space. The expected number of sampler calls E[Ntotal] at temperature (0, 1) is bounded by: E[Ntotal] , 1 + ZT 1 + 21/T ZT if 0 < 0.5 , if 0.5 < < 1 where = 1/T and ZT = (cid:80) (x)1/T . These results highlight that the algorithms practicality is highly sensitive to the temperature . potential limitation first emerges for 1, as the cost can scale up to the size of sample space = VK. It is therefore advisable to avoid using temperatures in this high-temperature regime to prevent potential computational bottleneck. Conversely, at low temperatures, the integer part = 1/T becomes large. The algorithms success requires drawing identical samples, an event with vanishingly small probability for large that leads to an extremely high rejection rate. more sample-efficient approximate algorithm is therefore needed to enhance its practical utility. 5.3 BATCH APPROXIMATION The practical limitations of the exact algorithm become most pronounced in the low-temperature regime, where the requirement of drawing = 1/T identical samples leads to an extremely high rejection rate that results in poor sample utilization. To address this, we propose an efficient approximate algorithm tailored for low temperatures of the form = 1/n. The key insight is to shift from single, high-risk trial to combinatorial search within large batch of samples (N n). This shift allows single batch to constitute (cid:0)N (cid:1) distinct candidates, which dramatically improves sample utilization and increases the probability of finding successful match in single round. For example, to sample at = 0.5(n = 2), we might draw batch of = 10 samples, such as {A, C, A, D, B, E, A, F, B, G}. Here, sample appears three times, and sample appears twice. The algorithm then counts the number of successful n-tuple candidates within this batch. For sample A, there are (cid:0)3 (cid:1) = 1 successful candidate. Finally, the output is sampled from the set of valid candidates {A, B} according to their weighted probabilities, where (A) = 3/4 and (B) = 1/4. In the rare case that no sample (cid:1) = 3 successful candidates. For sample B, there is only (cid:0)2"
        },
        {
            "title": "Preprint",
            "content": "Algorithm 2 Approximate Temperature Sampling Input: base sampler S; Target temperature = 1/n; Batch size n. Output: sample approximating the distribution PT (x) (x)n. 1: procedure APPROXIMATETEMPSAMPLE(S, n, ) 2: 3: 4: 5: 6: 7: 8: 9: Draw batch of samples = {x1, . . . , xN } from sampler S. Compute counts cx for each unique sample B. for down to 1 do Initialize candidate set Xcand . Initialize weights list . for each unique sample with count cx do (cid:1) to . Add to Xcand. Add weight wx = (cid:0)cx if Xcand is not empty then break Sample xout from Xcand with probabilities proportional to weights in . return xout 10: 11: 12: 13: Weight is the number of combinations Found valid candidate set, exit fallback loop Start with the target and fallback if needed appears at least times, the candidate set would be empty. To ensure the algorithm always produces an output, we introduce fallback mechanism that iteratively reduce the matching requirement from to 1, 2, . . . , until non-empty candidate set is found. The detailed process is illustrated in Algorithm 2. For any finite batch size , the algorithm is biased. This bias arises because the output probability is determined by the ratio of weights calculated within single stochastic batch, and the expectation of ratio is generally not equal to the ratio of expectations. However, its key strength is that it is asymptotically unbiased: as the batch size approaches infinity, the output distribution converges to the true target distribution. We formalize this crucial property in the following theorem. Theorem 3. Let Palg(x; ) be the probability of sampling using Algorithm 2 with batch size of , and let PT (x) = (x)n/ZT be the true target distribution at temperature = 1/n, where ZT = (cid:80) (x)n. The algorithm is asymptotically unbiased: lim Palg(x; ) = PT (x). The proof is provided in Appendix A.3. This property of consistency establishes the algorithm as principled approximation, where the batch size serves as practical lever for the tradeoff between efficiency and accuracy. Because the algorithm relies solely on black-box sampling interface, its utility extends naturally beyond the CALM framework to the entire class of implicit language models. This positions it as universal toolkit for controlled decoding in discrete spaces."
        },
        {
            "title": "6 RELATED WORK",
            "content": "6.1 AUTOENCODER Latent Generative Modeling. prominent paradigm in generative modeling involves two-stage process: first learning compressed latent representation of the data, and then training generative model within that latent space. This approach often begins with Variational Autoencoder (Kingma & Welling, 2014), which learns mapping from high-dimensional data space into compact, continuous latent space. This principle enables modern architectures, such as latent diffusion models (Rombach et al., 2022; Liu et al., 2023), to efficiently generate high-dimensional data from continuous latent representation. An alternative path, the Vector Quantized VAE (VQ-VAE, van den Oord et al., 2017), learns discrete latent space by mapping inputs to finite, learned codebook. This approach has been foundational to the autoregressive generation of continuous data like images (Razavi et al., 2019; Esser et al., 2021; Ramesh et al., 2021; Sun et al., 2024a) and audio (Dhariwal et al., 2020; Zeghidour et al., 2021; Defossez et al., 2023). Our approach introduces distinct way by performing discrete-to-continuous mapping. Driven by the pursuit of efficiency, it significantly reduces the number of autoregressive steps required for language generation."
        },
        {
            "title": "Preprint",
            "content": "Text Compression. Compressing long text into compact vector representations is foundational concept in sequence modeling. For instance, Recurrent Neural Networks can be viewed as implicitly compressing the entire history of sequence into single hidden state vector (Elman, 1990; Hochreiter & Schmidhuber, 1997). In the era of LLMs, this concept has been revitalized, with focus on prompt compression to improve inference efficiency. For example, Mu et al. (2023) designed modified attention mechanism to distill prompt information into few memory tokens. Chevalier et al. (2023); Ge et al. (2024); Gao et al. (2024) further introduced explicit reconstruction objectives to promote high fidelity compression. Recently, Li et al. (2025); Kuratov et al. (2025); Mezentsev & Oseledets (2025) pushed the limits of compression to ratio up to 1568x, underscoring the inherent sparsity of discrete text representations. More recently, DeepSeek-OCR (Wei et al., 2025) demonstrated compressing text into continuous image tokens, showing promise for applications like long-context compression. The primary focus of these methods on prompt compression places greater emphasis on reconstruction fidelity than on the robustness of the resulting representation. Our work, by contrast, prioritizes the creation of robust and smooth latent manifold, which is critical prerequisite for stable downstream generative modeling. 6.2 LIKELIHOOD-FREE LANGUAGE MODELING Continuous Autoregressive Generation. Autoregressive generation over continuous vectors is an emerging research frontier, with notable successes in domains such as image (Tschannen et al., 2023; Li et al., 2024; Shao et al., 2025b; Fan et al., 2025; Team, 2025), video (Chen et al., 2024; Deng et al., 2025), and audio synthesis (Turetzky et al., 2024; Sun et al., 2024b; Ma et al., 2025). GIVT (Tschannen et al., 2023) pioneered this direction by fitting the distribution of the target vector with Gaussian Mixture Model. However, the expressive power of GIVT is confined to the pre-defined family of Gaussian mixtures, constraint that limits its ability to capture complex distributions. Li et al. (2024) overcomes this limitation by employing lightweight diffusion head to model the vector distribution. While being more expressive, this method comes at the cost of inference efficiency due to its iterative sampling process. More recently, Shao et al. (2025b) introduced general framework based on strictly proper scoring rules. The Energy Transformer was presented as concrete and powerful instance of this framework, capable of high-quality, single-step generation. Our work adopts the core Energy Transformer framework but introduce several key improvements to the generative head architecture, the energy loss, and the models input structure, to further enhance its performance and stability for the specific challenges of language modeling. Parallel Token Prediction. The goal of predicting multiple tokens in parallel to overcome the sequential bottleneck of autoregressive models is long-standing pursuit in sequence modeling. Early efforts in this area were pioneered by non-autoregressive machine translation (Gu et al., 2018; Gu & Kong, 2021; Shao et al., 2021; Shao & Feng, 2022; Huang et al., 2022; Gui et al., 2023), which aims to generate an entire target sentence in single step. While effective for highly constrained conditional tasks like translation, these methods often struggle with the inherent multi-modality of open-ended language generation. different line of work uses multi-token prediction to enrich training signals (Gloeckle et al., 2024; Shao et al., 2025a) or provide candidates for speculative decoding (Stern et al., 2018; Leviathan et al., 2023), while the underlying generation remains singletoken autoregressive. more direct approach involves hierarchical modeling, where global model predicts large semantic chunks, which are then decoded by local model (Lee et al., 2022; YU et al., 2023; Ho et al., 2024; team et al., 2024; Pagnoni et al., 2025; Neitemeier et al., 2025). For instance, MegaByte (YU et al., 2023) uses global Transformer to predict blocks of tokens, but still relies on local autoregressive model to generate tokens sequentially within each block. Conceptually closer to our work, Large Concept Models (team et al., 2024) also adopt hierarchical structure, where their global model autoregressively predicts continuous sentence embeddings. However, this approach faces several challenges that our CALM framework is designed to address: its SONAR autoencoder (Duquenne et al., 2023) is computationally heavy and fragile, and its reliance on diffusion-based generative process introduces iterative inference bottleneck. Finally, another paradigm for parallel generation is diffusion models for text, which iteratively refine sequence of tokens from noise, either at the full sentence (Austin et al., 2021; Li et al., 2022; Lou et al., 2024) or block level (Han et al., 2023; Arriola et al., 2025). These models, which currently operate in the challenging discrete token space, could potentially benefit from the robust continuous space our autoencoder provides."
        },
        {
            "title": "6.3 LIKELIHOOD-FREE LM EVALUATION",
            "content": "LM metrics. The evaluation of language models is split into two distinct paradigms, which reflects the separation between assessing the quality of generated output and the fidelity of the learned distribution. On one hand, likelihood-based metrics, such as Perplexity, offer principled way to evaluate the learned distribution, but they are limited to models where likelihoods are tractable. On the other hand, diverse family of sample-based metrics focuses on the generated output. Classic methods like BLEU (Papineni et al., 2002) and ROUGE (Lin, 2004) assess the quality of generated text by comparing it to reference outputs. More recent approaches such as MAUVE (Pillutla et al., 2021) or LLM-as-a-judge (Zheng et al., 2023) allows for reference-free evaluation, but they rely on heuristics or black-box models and lack the formal guarantees of scoring rules. Our proposed metric, BrierLM, is designed to bridge this gap by combining the advantages of both paradigms: it operates exclusively on model samples, yet as strictly proper scoring rule, it offers faithful assessment of the models predictive quality, akin to perplexity. Brier Score. The Brier Score was originally proposed by Brier (1950) for the evaluation of probabilistic weather forecasts. It is classic example of strictly proper scoring rules, theoretically guaranteeing that model is incentivized to report its true belief to achieve the optimal score (Gneiting & Raftery, 2007). Consequently, it has been widely adopted in classification tasks, primarily for evaluating the quality of probabilistic forecasts (Sanders, 1963; fer, 2009; Hui & Belkin, 2021) and assessing model calibration (Lakshminarayanan et al., 2017; Ovadia et al., 2019; Gruber & Buettner, 2022). The innovation of our work is twofold: first, we introduce method to unbiasedly estimate the Brier score in likelihood-free manner; second, we generalize its application from metric for simple classification tasks to one capable of assessing language modeling capabilities. 6.4 LIKELIHOOD-FREE TEMPERATURE SAMPLING Bernoulli Factory. The temperature sampling problem is conceptually related to the classic problem of the Bernoulli Factory (Keane & OBrien, 1994; Occil, 2020), which addresses the challenge of simulating new coin with success probability of (p) given only coin with an unknown success probability p. This mirrors our challenge of achieving target probability proportional to (x)1/T using only base sampler for the implicit distribution (x). key distinction is that the Bernoulli Factory problem assumes binary outcome, whereas we operate over large discrete sample space. Our two-stage algorithm elegantly bridges this gap. The first stage isolates single candidate and reduces the problem to binary one, and the second stage directly applies an existing Bernoulli Factory algorithm (Mendo, 2019) to construct an event with probability (x)α. Controlled Generation. While many generative models lack the explicit probabilistic controls for temperature sampling, they have developed alternative strategies to navigate the trade-off between sample quality and diversity. For instance, VAEs and normalizing flows (Kingma & Welling, 2014; Rezende & Mohamed, 2015) often achieve this by adjusting the variance of their prior latent distribution (Kingma & Dhariwal, 2018). In Generative Adversarial Networks (Goodfellow et al., 2014), the truncation trick restricts sampling to high-density region of the latent space (Brock et al., 2019). Similarly, diffusion models can control stochasticity by altering the noise variance during the reverse sampling process (Song et al., 2021). These techniques, however, are fundamentally heuristic, as it is generally intractable to characterize the shape of the modified output distribution, and they all require white-box access to model internals like the latent space. Our work, in contrast, proposes universal, black-box algorithm for temperature sampling from implicit models over discrete spaces, offering provably exact method for this broad class of models."
        },
        {
            "title": "7 EXPERIMENTS",
            "content": "7.1 SETTINGS Datasets. We train our models on the Pile uncopyrighted dataset (Gao et al., 2020). The raw text is processed with the Llama 3 tokenizer (Grattafiori et al., 2024), resulting in training set of 230B tokens. We evaluate model performance on the WikiText-103 benchmark (Merity et al., 2017). Model. Our models are built upon standard Transformer backbone. We adopt most of the architecture designs from the LLaMA family (Touvron et al., 2023), including RMSNorm (Zhang &"
        },
        {
            "title": "Preprint",
            "content": "Table 1: Performance and computational cost comparison between Transformer baselines and CALM (K=4). CALMs reported parameter counts and FLOPs include all overhead from the autoencoder (75M parameters, training cost, and encoding/decoding FLOPs). Attention FLOPs are calculated assuming context length of 2048."
        },
        {
            "title": "Model",
            "content": "#Params Train FLOPs (total, 1e20) Infer FLOPs (per token, 1e8)"
        },
        {
            "title": "BrierLM",
            "content": "Transformer-S Transformer-M Transformer-L CALM-M (K=4) CALM-L (K=4) CALM-XL (K=4) 281M 465M 849M 371M 735M 1.82B 6.6 11.9 22.5 3.7 7.7 19. 4.4 7.9 15.0 2.9 4.6 9.4 6.05 7.07 8.98 5.72 6.58 8.53 Sennrich, 2019), SwiGLU activation (Shazeer, 2020a), and rotary positional embeddings (Su et al., 2021). We experiment with four scales: (12 layers, hidden size=768, intermediate size=2048), (16 layers, hidden size=1024, intermediate size=2752), (16 layers, hidden size=1536, intermediate size=4096), and XL (16 layers, hidden size=2560, intermediate size=6880). Training Details. The training process for our CALM framework is two-staged. We first train suite of autoencoders on 15B token subset of the Pile to map token chunks of size {1, 2, 4, 8} into continuous vectors. These autoencoders use hidden size of 512, latent dimension of 32K, have approximately 75M parameters, and are trained for 30k steps with batch size of 512k tokens. Following this, the CALM models are trained on the remaining data for 250k steps with batch size of 2 million tokens. The context length is set to 2048 steps; for CALM, this corresponds to 2048K tokens. All models are optimized using the AdamW optimizer (Loshchilov & Hutter, 2019) with β1 = 0.9, β2 = 0.95, ϵ = 1e 8. We use learning rate of 3 104 with constant schedule and warmup of 2000 steps, weight decay of 0.1, and gradient clipping of 1.0. 7.2 MAIN RESULTS We present the primary results of our comparison between the standard Transformer baselines and our CALM framework (with fixed chunk size of K=4) in Table 1. The results demonstrate that CALM establishes new, more efficient performance-compute frontier for language modeling. By increasing the semantic bandwidth of each autoregressive step, CALM is allowed to be substantially larger in parameter count while demanding fewer FLOPs for both training and inference. For instance, our 371M parameter CALM-M model achieves BrierLM score comparable to the 281M Transformer-S baseline, yet requires 44% fewer training FLOPs and 34% fewer inference FLOPs. Furthermore, the results confirm that CALM benefits from scaling just as effectively as traditional Transformers, allowing performance to be consistently improved by increasing model size. In addition to scaling model size, our framework introduces the semantic bandwidth as new lever for navigating the performance-compute landscape. Figure 4 illustrates this by plotting the performance of CALM-L with varying against the standard Transformer scaling curve. Notably, CALM-L with = 1 operates at significant disadvantage, demanding more FLOPs for lower performance compared to its discrete counterpart. The gap arises because the model undertakes the more challenging continuous prediction task, which in turn highlights the significant room for future architectural and algorithmic optimizations. The advantages of CALM become apparent as we increase K. Moving from = 1 to = 2 nearly halves the cost with only marginal drop in performance, and at = 4, the CALM model surpasses the baseline performance-compute frontier. This finding validates our central hypothesis: scaling the semantic bandwidth of each generative step provides new and highly effective axis for optimizing the performance-compute trade-off in language models. Further increasing the chunk size to = 8 leads to larger performance drop, which is likely model capacity limitation. We hypothesize that larger models may be required to leverage the benefits of higher semantic bandwidths."
        },
        {
            "title": "Preprint",
            "content": "Figure 4: The effect of chunk size on the performance-compute trade-off. Figure 5: Training progress of CALM and traditional Transformer models. To further investigate the learning dynamics of our framework, we plot the training curves of CALMXL against the Transformer baselines in Figure 5. The baseline Transformer models exhibit rapid initial gains before their performance gradually begins to saturate. In contrast, CALM-XL displays more patient but ultimately steeper learning curve, initially trailing Transformer-M but progressively closing the performance gap with the Transformer-L model. We attribute this phenomenon to the different nature of the predictive task. While the baseline models learn the relatively simple task of predicting single, low-information discrete token, our CALM model must learn to model the complex, high-dimensional distribution of continuous vectors, which explains the slower initial progress. However, once this ability is established, the model can unlock the potential of its large parameter count, entering phase of more significant and sustained improvements. 7.3 EFFECT OF AUTOENCODER In this section, we study the effect of the autoencoders design choices on the final performance of the CALM framework. The autoencoder is critical component, as it defines the latent space in which the continuous language model operates. To isolate its effects, we hold the downstream language model fixed across all experiments in this section: an Energy Transformer with hidden size of 768, 12 hidden layers, 16 attention heads, an FFN intermediate size of 2048, and generative head with 3 MLP blocks. Each model configuration is trained for 50,000 steps. Unless otherwise specified, the autoencoder uses the default parameters as described in Section 7.1. We begin with comprehensive ablation study to validate the contribution of each proposed technique, followed by detailed analysis of the effect of several key hyperparameters. We first validate the design choices for the autoencoder, with results detailed in Table 2. While standard, reconstruction-only autoencoder provides reasonable baseline, naively incorporating variational objective leads to significant drop in performance. This degradation is traced to severe instance of posterior collapse, where we found that 71 of the 128 latent dimensions had collapsed to the standard normal prior. The introduction of the KL clipping strategy proves to be the crucial remedy, which effectively prevents dimensional collapse and leads to notable performance improvement. Furthermore, applying dropout regularization to both the input tokens and the latent vector yields considerable, orthogonal performance benefits, confirming that each technique contributes uniquely to shaping high-fidelity and robust latent space. KL weight. We next examine the models sensitivity to the KL divergence weight, β, which governs the trade-off between reconstruction fidelity and latent space regularization. We varied β across several orders of magnitude and present the results in Figure 6. Starting from baseline with no KL regularization (β = 0), we observe that introducing small amount of variational regularization significantly improves the final BrierLM score, which confirms our hypothesis: moderate regularization effectively smooths the latent manifold, making it more learnable for the Energy Transformer, while leaving reconstruction accuracy almost unaffected. However, this trend reverses as the regularization becomes overly aggressive. At β = 0.1, the BrierLM score drops sharply, decline directly linked to the autoencoders compromised reconstruction fidelity, which falls to 99%. Based on these findings, we selected β = 0.001 to train our autoencoder."
        },
        {
            "title": "Preprint",
            "content": "Table 2: Ablation study of the autoencoders regularization techniques. Performance is measured by BrierLM on the downstream language modeling task."
        },
        {
            "title": "DropToken DropLatent BrierLM",
            "content": "3.99 3.48 4.13 4.55 4.46 4.70 Figure 6: Effect of the KL divergence weight on the autoencoders reconstruction accuracy and the downstream BrierLM score. Figure 7: Effect of the latent demension on the autoencoders reconstruction accuracy and the downstream BrierLM score. Latent Demension. We next examine the influence of the latent dimension, l, which functions as the information bottleneck of the autoencoder. We evaluated latent dimensions of 32, 64, 128, and 256, and respectively scaled the dropout rate to 0.05, 0.1, 0.15, and 0.2. The results are illustrated in Figure 7. As seen, the reconstruction accuracy remains consistently high across all configurations, but the downstream performance varies, peaking at = 128. This suggests trade-off in selecting the optimal dimension. latent space that is too small, such as with = 32, forces the autoencoder to learn an overly compact and brittle representation. Conversely, large latent dimension may lead the autoencoder to encode noisy or irrelevant features from the input tokens. This forces the Energy Transformer to expend its finite capacity modeling this noise, making it more challenging to discern the underlying data manifold. dimension of = 128 thus appears to strike an optimal balance, providing sufficient capacity for robust representation while maintaining structured and learnable latent space for the downstream generative model. Scale. Finally, we examine the impact of scaling the autoencoder. We explored several axes of scaling: doubling the number of layers in both the encoder and decoder to 4, doubling the hidden dimension to 1024, and expanding the training dataset to 100B tokens. Interestingly, none of these scaling efforts resulted in significant improvement in the final BrierLM score. This finding suggests that the autoencoders task is inherently simple and does not benefit from the aggressive scaling. lightweight architecture, trained on relatively modest amount of data, is sufficient to learn the high-fidelity and robust representation required for our framework. This is desirable property, as it allows the autoencoder to be computationally negligible component of the overall system. 7.4 EFFECT OF MODEL ARCHITECTURE In this section, we conduct ablation studies on the CALM model architecture to investigate the impact of different design choices on model performance. Unless otherwise specified, all experiments are conducted using base configuration with hidden size of 768, 12 hidden layers, 16 attention heads, and an FFN intermediate size of 2048. The generative head consists of 3 MLP blocks, and all models are trained for 50,000 steps. Diffusion and Flow Matching. Since the generative head can be any continuous generative model, we also evaluate two prominent choices as alternatives: diffusion (Ho et al., 2020) and flow matching"
        },
        {
            "title": "Preprint",
            "content": "Figure 8: BrierLM scores during training for different generative heads. Figure 9: Effect of sampling steps on the generation quality of diffusion and flow matching. (Lipman et al., 2023). For these experiments, we adopt an architecture consistent with the diffusion head used in Li et al. (2024). To ensure fair comparison, we replicate the input hidden state = 8 times for both models, mirroring the multi-sample approach used for our energy loss and promoting stable learning. During inference, we use 100 iterative steps by default. For the Flow Matching model, we use midpoint sampler. Figure 8 compares the performance of the diffusion, flow matching, and energy-based generative heads. The results show that both flow matching and our energy-based head outperform the diffusion model, exhibiting noticeable performance gap. Between the two, flow matching exhibits faster initial convergence, whereas our energy-based head reaches higher performance ceiling. Figure 9 further compares the models performance across different numbers of inference iterations. For the flow matching model, we tested both the Euler and midpoint samplers. As shown, the diffusion model requires large number of iterations to generate valid results. In contrast, the flow matching model is significantly more efficient; the midpoint sampler, in particular, achieves decent quality in just 2 steps and reaches its near-optimal performance within 4 steps. Our energy-based generative head achieves the best of both worlds: it delivers superior performance while completely eliminating the need for iterative decoding, making it compelling choice for the CALM framework. Energy Loss. We now analyze the impact of the energy loss formulation on model performance. Our energy loss (Equation 10) involves two sampling hyperparameters: the number of model-generated samples, , and the number of target samples, . Larger values for and provide better estimation of the true energy score, but also increase the computational cost. Our default configuration is = 8 and = 100. Table 3 shows the results of varying and , which reveal clear tradeoff between performance and computational cost. As expected, increasing the number of samples consistently improves the BrierLM score, but this comes at nearly linear increase in training cost. Our default setting of = 8 and = 100 is thus justified as balanced configuration, leveraging moderately sized for robust gradient signal and large to stabilize training. Table 3: Effect of model samples and target samples on model performance and training cost. Varying (fixed = 100) Varying (fixed = 8) = 2 = 4 = 8 = 12 = 1 = 16 = 100 = 200 BrierLM Relative Cost 4.37 4.53 0.82 0.91 4.70 1.0 4.72 1.13 4.50 0.92 4.56 0.94 4.70 1.0 4.67 1.07 We also investigate the effect of the exponent α in the energy score (Equation 9), which is guaranteed to be strictly proper for any α (0, 2). As shown in Table 4, our empirical results align with this theoretical property. We observe that training fails for α < 1 (e.g., α = 0.75), phenomenon previously analyzed by Shao et al. (2025b) and attributed to gradient explosion issues. For values of α within the range of [1, 2), the model achieves decent performance, with the best empirical results obtained at our default setting of α = 1. The models BrierLM score drops to 0 at α = 2. This is expected, as the energy score is only proper but not strictly proper when α = 2. Consequently, the energy loss can no longer guide the model to uniquely match the true data distribution, leading to collapse in modeling capability."
        },
        {
            "title": "Preprint",
            "content": "Table 4: Effect of the exponent α in the energy score. α 0."
        },
        {
            "title": "BrierLM Fail",
            "content": "4.70 1.25 4.42 1.5 4.46 1. 4.30 2 0 Model Input. critical design choice in our CALM framework is the input representation fed into the Transformer backbone at each autoregressive step. We evaluate three distinct input schemes: (1) Discrete input, which first decodes the previously generated vector zi1 into discrete tokens, then passes them through an embedding layer and an input compression MLP to form the next step input; (2) Continuous input, more direct alternative where the vector zi1 is directly projected to the Transformers hidden dimension via single linear layer; (3) Combined input, which fuses the representations from the discrete and continuous methods through element-wise addition. Table 5: Effect of model input on language modeling performance. Performance is evaluated using Brier-n scores and the composite BrierLM. Higher scores are better. Model Input Brier-1 Brier-2 Brier-3 Brier-4 BrierLM Discrete Continuous Both 21.81 17.43 21.17 6.88 5.04 6.49 2.59 1.74 2.44 1.25 0.73 1.12 4.70 3.25 4. As summarized in Table 5, the results clearly favor the discrete input strategy. The combined input offers no advantage and slightly degrades performance, while the purely continuous input leads to substantial performance drop. This confirms our hypothesis: although the continuous vector theoretically contains all the information of its corresponding discrete tokens, its highly compact and brittle nature makes it challenging for the model to unpack the underlying semantic information. Grounding the autoregressive process in the discrete token space provides more structured and stable input signal, which is therefore critical for achieving optimal performance. 7.5 TEMPERATURE SAMPLING In this section, we conduct fine-grained analysis to characterize the practical behavior of our approximate temperature sampling algorithm (Algorithm 2), i.e., how the algorithm navigates the trade-off between predictive accuracy and generative diversity. To quantify them, we decompose the Brier score estimator, Brier(P, y) I{x1 = y} + I{x2 = y} I{x1 = x2}, into two metrics: Accuracy E[I{x = y}]: This metric measures the probability that single sample drawn from the model matches the ground truth. It directly reflects the models accuracy. Collision Rate E[I{x1 = x2}]: This metric measures the collision probability that two independent samples drawn from the model are identical. It serves as an inverse proxy for diversity, where higher collision rate indicates that the output distribution is less diverse. Consistent with our primary BrierLM metric, we report both accuracy and collision rate as the geometric mean of scores over n-grams from n=1 to 4. We first investigate how the two key hyperparameters of our algorithmtemperature and batch size influence these metrics. Specifically, we conduct two sets of experiments: for fixed temperature = 1/3, we vary the batch size {1, 10, 20, 50, 100, 200, 500, 1000}; for fixed batch size = 500, we vary the the temperature {1/2, 1/3, 1/4, 1/5, 1/6}. As shown in Figure 10, both increasing the batch size and decreasing the temperature sharpen the output distribution, achieving higher accuracy at the cost of reduced diversity (i.e., higher collision rate). key observation, however, is the dominant role of the batch size , which covers substantially greater range of this trade-off than the temperature . Intuitively, larger batch provides clearer statistical picture of the true distribution, making it easier to indentify high-probability candidates and confidently output them. In contrast, the effectiveness of temperature is capped"
        },
        {
            "title": "Preprint",
            "content": "Figure 10: The accuracy-diversity trade-off in CALM as function of batch size and temperature T. Figure 11: Comparison of the temperature sampling performance between CALM and the baseline Transformer. by the information available within the finite batch. Thus, while temperature serves its conventional purpose, these empirical results suggest that the batch size is more effective tool for navigating the accuracy-diversity frontier in our likelihood-free framework. Finally, we compare the behavior of our sampling algorithm against that of traditional Transformer. We ensure fair comparison by selecting model checkpoints with nearly identical BrierLM scores. For CALM, we fix the temperature at = 1/3 while varying the batch size {1, 10, 20, 50, 100, 200, 500, 1000}; for the Transformer baseline, we adjust its softmax temperature across {1, 0.9, . . . , 0.4}. The results, plotted in Figure 11, are compelling: the accuracy-diversity trajectory traced by tuning in CALM is nearly identical to the one produced by tuning in the traditional Transformer. This alignment shows that we can accurately replicate the generative behavior of traditional models across wide spectrum of temperatures. For instance, matching = 0.6 requires batch size of approximately = 100, while simulating lower temperature of = 0.5 necessitates larger batch of = 200. This suggests clear and predictable trade-off: the ability to simulate lowertemperature, higher-fidelity generation comes at the cost of an increased number of samples."
        },
        {
            "title": "8 CONCLUSION AND FUTURE WORK",
            "content": "In this work, we challenge the inefficient, token-by-token paradigm of LLMs by introducing Continuous Autoregressive Language Models (CALM), framework that shifts generation from discrete tokens to continuous vector space where single vector represents tokens. To support this approach, we develop comprehensive likelihood-free toolkit: robust and high-fidelity autoencoder, the energy loss for generative modeling, the BrierLM metric for LM evaluation, and new suite of algorithms for temperature sampling. Empirical results show that CALM achieves superior performance-compute trade-off, highlighting new scaling axis for language modeling: scaling the semantic bandwidth of each generative step to push the performance-compute frontier of LLMs. Despite these promising results, CALM still has significant room for future architectural and algorithmic optimizations, as indicated by the performance gap between CALM at K=1 and standard Transformer baseline. We identify several key areas with great potential for future research: Autoencoder. The autoencoder is the cornerstone of the CALM framework, which directly governs the quality of the latent space. One key limitation of the current autoencoder is its primary focus on reconstruction, with less emphasis on semantic structure. promising direction is to design an autoencoder that learns semantically grounded latent space, where proximity in the latent space corresponds to semantic similarity. We note that building such semantically rich latent spaces has been recent trend in the vision domain (Zheng et al., 2025). This could provide powerful inductive bias for the downstream generative model. Another direction is the development of more powerful architectures. Designs that"
        },
        {
            "title": "Preprint",
            "content": "are context-aware or autoregressive, for example, could offer superior robustness and more reliable reconstruction. Model. The core generative model also presents significant avenues for exploration. For instance, in terms of architecture, our current design employs Transformer backbone followed by lightweight generative head; while efficient, an alternative is to explore more integrated, end-to-end generative Transformer, which may yield stronger generative modeling capabilities. In terms of the training objective, while the energy loss provides robust foundation, investigating other strictly proper scoring rules or generative models is worthwhile, as they may offer different optimization dynamics and improved sample quality. Sampling. While our work introduces provably exact algorithm for likelihood-free temperature sampling, its reliance on rejection sampling can introduce significant inference overhead. promising direction for future work is to explore more lightweight, heuristic methods for navigating the diversity-fidelity trade-off at inference time. This could include techniques such as manipulating the scale of the input noise to the generative head, or finetuning the model with modified loss function to steer its generative behavior. Scaling. critical next step is to investigate the scaling properties of CALM. hypothesis to validate is that larger models possess the requisite capacity to support higher semantic bandwidths. further pursuit is to establish new family of scaling laws. While traditional laws (Kaplan et al., 2020) model performance as function of model and data size, our framework introduces the semantic bandwidth as third variable. Formulating unified scaling law would enable the principled selection of an optimal for any compute budget. Algorithmic Toolkit. The paradigm shift from discrete tokens to continuous domain necessitates re-evaluation of the standard LLM algorithmic toolkit. For instance, policy optimization methods in reinforcement learning typically update the model by increasing the log-probability of rewarded samples, quantity that CALM cannot directly compute. Similarly, knowledge distillation often requires minimizing the KL divergence between teacher and student distributions, which is intractable without access to the full probability mass function. How to reformulate these techniques to operate in sample-based regime is an important question for future research."
        },
        {
            "title": "REFERENCES",
            "content": "An experimental comparison of performance measures for classification. Pattern recognition letters, 30(1):2738, 2009. Josh Achiam, Steven Adler, Sandhini Agarwal, Lama Ahmad, Ilge Akkaya, Florencia Leoni Aleman, Diogo Almeida, Janko Altenschmidt, Sam Altman, Shyamal Anadkat, et al. Gpt-4 technical report. arXiv preprint arXiv:2303.08774, 2023. Marianne Arriola, Subham Sekhar Sahoo, Aaron Gokaslan, Zhihan Yang, Zhixuan Qi, Jiaqi Han, Justin Chiu, and Volodymyr Kuleshov. Block diffusion: Interpolating between autoregressive and diffusion language models. In The Thirteenth International Conference on Learning Representations, 2025. URL https://openreview.net/forum?id=tyEyYT267x. Jacob Austin, Daniel D. Johnson, Jonathan Ho, Daniel Tarlow, and Rianne van den Berg. Structured denoising diffusion models in discrete state-spaces. In M. Ranzato, A. Beygelzimer, Y. Dauphin, P.S. Liang, and J. Wortman Vaughan (eds.), Advances in Neural Information Processing Systems, volume 34, pp. 1798117993. Curran Associates, Inc., URL https://proceedings.neurips.cc/paper_files/paper/2021/ 2021. file/958c530554f78bcd8e97125b70e6973d-Paper.pdf. Emily M. Bender, Timnit Gebru, Angelina McMillan-Major, and Shmargaret Shmitchell. On the dangers of stochastic parrots: Can language models be too big? In Proceedings of the 2021 ACM Conference on Fairness, Accountability, and Transparency, FAccT 21, pp. 610623, New York, NY, USA, 2021. Association for Computing Machinery. ISBN 9781450383097. doi: 10.1145/ 3442188.3445922. URL https://doi.org/10.1145/3442188.3445922. Glenn Brier. Verification of forecasts expressed in terms of probability. Monthly weather review, 78(1):13, 1950."
        },
        {
            "title": "Preprint",
            "content": "Andrew Brock, Jeff Donahue, and Karen Simonyan. Large scale GAN training for high fidelity natural image synthesis. In International Conference on Learning Representations, 2019. URL https://openreview.net/forum?id=B1xsqj09Fm. Boyuan Chen, Diego Martı Monso, Yilun Du, Max Simchowitz, Russ Tedrake, and Vincent Sitzmann. Diffusion forcing: Next-token prediction meets full-sequence diffusion. In The Thirtyeighth Annual Conference on Neural Information Processing Systems, 2024. URL https: //openreview.net/forum?id=yDo1ynArjj. Alexis Chevalier, Alexander Wettig, Anirudh Ajith, and Danqi Chen. Adapting language models In Houda Bouamor, Juan Pino, and Kalika Bali (eds.), Proceedings of to compress contexts. the 2023 Conference on Empirical Methods in Natural Language Processing, pp. 38293846, Singapore, December 2023. Association for Computational Linguistics. doi: 10.18653/v1/2023. emnlp-main.232. URL https://aclanthology.org/2023.emnlp-main.232/. DeepSeek-AI. Deepseek-r1: Incentivizing reasoning capability in llms via reinforcement learning, 2025. URL https://arxiv.org/abs/2501.12948. Alexandre Defossez, Jade Copet, Gabriel Synnaeve, and Yossi Adi. High fidelity neural audio compression. Transactions on Machine Learning Research, 2023. ISSN 2835-8856. URL https://openreview.net/forum?id=ivCd8z8zR2. Featured Certification, Reproducibility Certification. Haoge Deng, Ting Pan, Haiwen Diao, Zhengxiong Luo, Yufeng Cui, Huchuan Lu, Shiguang Shan, Yonggang Qi, and Xinlong Wang. Autoregressive video generation without vector quantization. In The Thirteenth International Conference on Learning Representations, 2025. URL https: //openreview.net/forum?id=JE9tCwe3lp. Prafulla Dhariwal, Heewoo Jun, Christine Payne, Jong Wook Kim, Alec Radford, and Ilya Sutskever. Jukebox: generative model for music, 2020. URL https://arxiv.org/abs/2005. 00341. Paul-Ambroise Duquenne, Holger Schwenk, and Benoˆıt Sagot. Sonar: Sentence-level multimodal and language-agnostic representations, 2023. URL https://arxiv.org/abs/2308. 11466. Jeffrey Elman. Finding structure in time. Cognitive science, 14(2):179211, 1990. Patrick Esser, Robin Rombach, and Bjorn Ommer. Taming transformers for high-resolution image synthesis. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pp. 1287312883, June 2021. Lijie Fan, Tianhong Li, Siyang Qin, Yuanzhen Li, Chen Sun, Michael Rubinstein, Deqing Sun, Kaiming He, and Yonglong Tian. Fluid: Scaling autoregressive text-to-image generative models with continuous tokens. In The Thirteenth International Conference on Learning Representations, 2025. URL https://openreview.net/forum?id=jQP5o1VAVc. Jun Gao, Ziqiang Cao, and Wenjie Li. Selfcp: Compressing over-limit prompt via the frozen large language model itself. Inf. Process. Manage., 61(6), November 2024. ISSN 0306-4573. doi: 10.1016/j.ipm.2024.103873. URL https://doi.org/10.1016/j.ipm.2024.103873. Leo Gao, Stella Biderman, Sid Black, Laurence Golding, Travis Hoppe, Charles Foster, Jason Phang, Horace He, Anish Thite, Noa Nabeshima, et al. The pile: An 800gb dataset of diverse text for language modeling. arXiv preprint arXiv:2101.00027, 2020. Tao Ge, Hu Jing, Lei Wang, Xun Wang, Si-Qing Chen, and Furu Wei. In-context autoencoder In The Twelfth International Conferfor context compression in large language model. ence on Learning Representations, 2024. URL https://openreview.net/forum?id= uREj4ZuGJE. Fabian Gloeckle, Badr Youbi Idrissi, Baptiste Rozi`ere, David Lopez-Paz, and Gabriel Synnaeve. Better & faster large language models via multi-token prediction. In ICML, 2024. URL https: //openreview.net/forum?id=pEWAcejiU2."
        },
        {
            "title": "Preprint",
            "content": "Tilmann Gneiting and Adrian Raftery. Strictly proper scoring rules, prediction, and estimation. Journal of the American statistical Association, 102(477):359378, 2007. Irving John Good. Rational decisions. Journal of the Royal Statistical Society: Series (Methodological), 14(1):107114, 1952. Ian Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley, Sherjil Ozair, Aaron Courville, and Yoshua Bengio. Generative adversarial nets. Advances in neural information processing systems, 27, 2014. Gemini Team Google. Gemini: family of highly capable multimodal models, 2025. URL https: //arxiv.org/abs/2312.11805. Aaron Grattafiori, Abhimanyu Dubey, Abhinav Jauhri, Abhinav Pandey, Abhishek Kadian, Ahmad Al-Dahle, Aiesha Letman, Akhil Mathur, Alan Schelten, Alex Vaughan, et al. The llama 3 herd of models. arXiv preprint arXiv:2407.21783, 2024. spectral energy distance for parallel speech synthesis. Alexey Gritsenko, Tim Salimans, Rianne van den Berg, Jasper Snoek, and Nal KalchIn H. Larochelle, InInc., URL https://proceedings.neurips.cc/paper_files/paper/2020/ brenner. M. Ranzato, R. Hadsell, M.F. Balcan, and H. Lin (eds.), Advances in Neural formation Processing Systems, volume 33, pp. 1306213072. Curran Associates, 2020. file/9873eaad153c6c960616c89e54fe155a-Paper.pdf. Sebastian Gruber and Florian Buettner. for classification and beyond. scores D. Belgrave, K. Cho, and A. Oh (eds.), Advances in Neural ing Systems, volume 35, pp. 86188632. Curran Associates, https://proceedings.neurips.cc/paper_files/paper/2022/file/ 3915a87ddac8e8c2f23dbabbcee6eec9-Paper-Conference.pdf. Better uncertainty calibration via proper In S. Koyejo, S. Mohamed, A. Agarwal, Information ProcessURL Inc., 2022. Jiatao Gu and Xiang Kong. Fully non-autoregressive neural machine translation: Tricks of the In Chengqing Zong, Fei Xia, Wenjie Li, and Roberto Navigli (eds.), Findings of the trade. Association for Computational Linguistics: ACL-IJCNLP 2021, pp. 120133, Online, August 2021. Association for Computational Linguistics. doi: 10.18653/v1/2021.findings-acl.11. URL https://aclanthology.org/2021.findings-acl.11/. Jiatao Gu, James Bradbury, Caiming Xiong, Victor O. K. Li, and Richard Socher. NonIn 6th International Conference on Learning Repautoregressive neural machine translation. resentations, ICLR 2018, Vancouver, BC, Canada, April 30 - May 3, 2018, Conference Track Proceedings, 2018. URL https://openreview.net/forum?id=B1l8BtlCb. Shangtong Gui, Chenze Shao, Zhengrui Ma, Xishan Zhang, Yunji Chen, and Yang Feng. NonIn Thirty-seventh autoregressive machine translation with probabilistic context-free grammar. Conference on Neural Information Processing Systems, 2023. URL https://openreview. net/forum?id=LloZFVwWvj. Xiaochuang Han, Sachin Kumar, and Yulia Tsvetkov. SSD-LM: Semi-autoregressive simplex-based diffusion language model for text generation and modular control. In Anna Rogers, Jordan BoydGraber, and Naoaki Okazaki (eds.), Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pp. 1157511596, Toronto, Canada, July 2023. Association for Computational Linguistics. doi: 10.18653/v1/2023.acl-long.647. URL https://aclanthology.org/2023.acl-long.647/. Jonathan Ho, Ajay Jain, and Pieter Abbeel. Denoising diffusion probabilistic models. In H. Larochelle, M. Ranzato, R. Hadsell, M.F. Balcan, and H. Lin (eds.), Advances in Neural Information Processing Systems, volume 33, pp. 68406851. Curran Associates, Inc., URL https://proceedings.neurips.cc/paper_files/paper/2020/ 2020. file/4c5bcfec8584af0d967f1ab10179ca4b-Paper.pdf. Namgyu Ho, Sangmin Bae, Taehyeon Kim, Hyunjik Jo, Yireun Kim, Tal Schuster, Adam Fisch, James Thorne, and Se-Young Yun. Block transformer: Global-to-local language modeling for fast inference, 2024. URL https://arxiv.org/abs/2406.02657."
        },
        {
            "title": "Preprint",
            "content": "Sepp Hochreiter and Jurgen Schmidhuber. Long short-term memory. Neural Computation, 9(8): 17351780, 1997. doi: 10.1162/neco.1997.9.8.1735. Fei Huang, Hao Zhou, Yang Liu, Hang Li, and Minlie Huang. Directed acyclic transformer for non-autoregressive machine translation. In Proceedings of the 39th International Conference on Machine Learning, ICML 2022, 2022. Like Hui and Mikhail Belkin. Evaluation of neural architectures trained with square loss vs crossentropy in classification tasks. In International Conference on Learning Representations, 2021. URL https://openreview.net/forum?id=hsFN92eQEla. Jared Kaplan, Sam McCandlish, Tom Henighan, Tom B. Brown, Benjamin Chess, Rewon Child, Scott Gray, Alec Radford, Jeffrey Wu, and Dario Amodei. Scaling laws for neural language models, 2020. URL https://arxiv.org/abs/2001.08361. M. S. Keane and George L. OBrien. bernoulli factory. ACM Trans. Model. Comput. Simul., ISSN 1049-3301. doi: 10.1145/175007.175019. URL https: 4(2):213219, April 1994. //doi.org/10.1145/175007.175019. Yoon Kim, Yacine Jernite, David Sontag, and Alexander M. Rush. Character-aware neural language models. In Proceedings of the Thirtieth AAAI Conference on Artificial Intelligence, AAAI16, pp. 27412749. AAAI Press, 2016. Diederik P. Kingma and Max Welling. Auto-Encoding Variational Bayes. In 2nd International Conference on Learning Representations, ICLR 2014, Banff, AB, Canada, April 14-16, 2014, Conference Track Proceedings, 2014. Durk Kingma and Prafulla Dhariwal. Glow: Generative flow with invertible 1x1 conIn S. Bengio, H. Wallach, H. Larochelle, K. Grauman, N. Cesa-Bianchi, and volutions. R. Garnett (eds.), Advances in Neural Information Processing Systems, volume 31. Curran Associates, Inc., 2018. URL https://proceedings.neurips.cc/paper_files/ paper/2018/file/d139db6a236200b21cc7f752979132d0-Paper.pdf. Improved variational Durk Kingma, Tim Salimans, Rafal Jozefowicz, Xi Chen, Welling. M. Sugiyama, U. Luxburg, formation Processing Systems, volume 29. Curran Associates, https://proceedings.neurips.cc/paper_files/paper/2016/file/ ddeebdeefdb7e7e7a697e1c3e3d8ef54-Paper.pdf. Ilya Sutskever, and Max In D. Lee, I. Guyon, and R. Garnett (eds.), Advances in Neural InURL inference with inverse autoregressive flow. Inc., 2016. Yuri Kuratov, Mikhail Arkhipov, Aydar Bulatov, and Mikhail Burtsev. Cramming 1568 tokens into single vector and back again: Exploring the limits of embedding space capacity. In Wanxiang Che, Joyce Nabende, Ekaterina Shutova, and Mohammad Taher Pilehvar (eds.), Proceedings of the 63rd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pp. 1932319339, Vienna, Austria, July 2025. Association for Computational Linguistics. ISBN 979-8-89176-251-0. doi: 10.18653/v1/2025.acl-long.948. URL https://aclanthology. org/2025.acl-long.948/. Balaji Lakshminarayanan, Alexander Pritzel, and Charles Blundell. Simple and scalable predictive uncertainty estimation using deep ensembles. Advances in neural information processing systems, 30, 2017. Doyup Lee, Chiheon Kim, Saehoon Kim, Minsu Cho, and Wook-Shin Han. Autoregressive image generation using residual quantization. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pp. 1152311532, 2022. Yaniv Leviathan, Matan Kalman, and Yossi Matias. Fast inference from transformers via specIn Andreas Krause, Emma Brunskill, Kyunghyun Cho, Barbara Engelulative decoding. hardt, Sivan Sabato, and Jonathan Scarlett (eds.), Proceedings of the 40th International Conference on Machine Learning, volume 202 of Proceedings of Machine Learning Research, pp. 1927419286. PMLR, 2329 Jul 2023. URL https://proceedings.mlr.press/ v202/leviathan23a.html."
        },
        {
            "title": "Preprint",
            "content": "Tianhong Li, Yonglong Tian, He Li, Mingyang Deng, and Kaiming He. Autoregressive image generation without vector quantization. In The Thirty-eighth Annual Conference on Neural Information Processing Systems, 2024. URL https://openreview.net/forum?id=VNBIF0gmkb. Xiang Lisa Li, John Thickstun, Ishaan Gulrajani, Percy Liang, and Tatsunori Hashimoto. DiffusionLM improves controllable text generation. In Alice H. Oh, Alekh Agarwal, Danielle Belgrave, and Kyunghyun Cho (eds.), Advances in Neural Information Processing Systems, 2022. URL https://openreview.net/forum?id=3s9IrEsjLyk. Zongqian Li, Yixuan Su, and Nigel Collier. 500xCompressor: Generalized prompt compression for large language models. In Wanxiang Che, Joyce Nabende, Ekaterina Shutova, and Mohammad Taher Pilehvar (eds.), Proceedings of the 63rd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pp. 2508125091, Vienna, Austria, July 2025. Association for Computational Linguistics. ISBN 979-8-89176-251-0. doi: 10.18653/v1/2025. acl-long.1219. URL https://aclanthology.org/2025.acl-long.1219/. Chin-Yew Lin. ROUGE: package for automatic evaluation of summaries. In Text Summarization Branches Out, pp. 7481, Barcelona, Spain, July 2004. Association for Computational Linguistics. URL https://aclanthology.org/W04-1013/. Yaron Lipman, Ricky T. Q. Chen, Heli Ben-Hamu, Maximilian Nickel, and Matthew Le. Flow matching for generative modeling. In The Eleventh International Conference on Learning Representations, 2023. URL https://openreview.net/forum?id=PqvMRDCJT9t. Haohe Liu, Zehua Chen, Yi Yuan, Xinhao Mei, Xubo Liu, Danilo Mandic, Wenwu Wang, and Mark Plumbley. AudioLDM: Text-to-audio generation with latent diffusion models. In Andreas Krause, Emma Brunskill, Kyunghyun Cho, Barbara Engelhardt, Sivan Sabato, and Jonathan Scarlett (eds.), Proceedings of the 40th International Conference on Machine Learning, volume 202 of Proceedings of Machine Learning Research, pp. 2145021474. PMLR, 2329 Jul 2023. URL https://proceedings.mlr.press/v202/liu23f.html. Ilya Loshchilov and Frank Hutter. Decoupled weight decay regularization. In International Conference on Learning Representations, 2019. URL https://openreview.net/forum?id= Bkg6RiCqY7. Aaron Lou, Chenlin Meng, and Stefano Ermon. Discrete diffusion modeling by estimating the ratios of the data distribution. In Proceedings of the 41st International Conference on Machine Learning, ICML24. JMLR.org, 2024. Zhengrui Ma, Yang Feng, Chenze Shao, Fandong Meng, Jie Zhou, and Min Zhang. Efficient speech language modeling via energy distance in continuous latent space, 2025. URL https: //arxiv.org/abs/2505.13181. Luis Mendo. An asymptotically optimal bernoulli factory for certain functions that can be exStochastic Processes and their Applications, 129(11):43664384, pressed as power series. 2019. ISSN 0304-4149. doi: https://doi.org/10.1016/j.spa.2018.11.017. URL https://www. sciencedirect.com/science/article/pii/S0304414918306768. Stephen Merity, Caiming Xiong, James Bradbury, and Richard Socher. Pointer sentinel mixIn International Conference on Learning Representations, 2017. URL https: ture models. //openreview.net/forum?id=Byj72udxe. Gleb Mezentsev and Ivan Oseledets. Exploring the latent capacity of llms for one-step text generation, 2025. URL https://arxiv.org/abs/2505.21189. Tomas Mikolov, Kai Chen, Greg Corrado, and Jeffrey Dean. Efficient estimation of word representations in vector space, 2013. URL https://arxiv.org/abs/1301.3781. Jesse Mu, Xiang Lisa Li, and Noah Goodman. Learning to compress prompts with gist tokens. In Thirty-seventh Conference on Neural Information Processing Systems, 2023. URL https: //openreview.net/forum?id=2DtxPCL3T5."
        },
        {
            "title": "Preprint",
            "content": "Pit Neitemeier, Bjorn Deiseroth, Constantin Eichenberg, and Lukas Balles. Hierarchical autoregressive transformers: Combining byteand word-level processing for robust, adaptable language models. In The Thirteenth International Conference on Learning Representations, 2025. URL https://openreview.net/forum?id=tU074jg2vS. Peter Occil. Bernoulli factory algorithms, 2020. URL https://peteroupc.github.io/ bernoulli.html. Yaniv Ovadia, Emily Fertig, Jie Ren, Zachary Nado, D. Sculley, Sebastian Nowozin, Joshua Dillon, Balaji Lakshminarayanan, and Jasper Snoek. Can you trust your model's In H. Wallach, uncertainty? (eds.), AdH. Larochelle, A. Beygelzimer, F. d'Alche-Buc, E. Fox, and R. Garnett vances in Neural Information Processing Systems, volume 32. Curran Associates, Inc., URL https://proceedings.neurips.cc/paper_files/paper/2019/ 2019. file/8558cb408c1d76621371888657d2eb1d-Paper.pdf. evaluating predictive uncertainty under dataset shift. Lorenzo Pacchiardi, Rilwan A. Adewoyin, Peter Dueben, and Ritabrata Dutta. Probabilistic forecasting with generative networks via scoring rule minimization. Journal of Machine Learning Research, 25(45):164, 2024. URL http://jmlr.org/papers/v25/23-0038.html. Artidoro Pagnoni, Ramakanth Pasunuru, Pedro Rodriguez, John Nguyen, Benjamin Muller, Margaret Li, Chunting Zhou, Lili Yu, Jason Weston, Luke Zettlemoyer, Gargi Ghosh, Mike Lewis, Ari Holtzman, and Srini Iyer. Byte latent transformer: Patches scale better than tokens. In Wanxiang Che, Joyce Nabende, Ekaterina Shutova, and Mohammad Taher Pilehvar (eds.), Proceedings of the 63rd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pp. 92389258, Vienna, Austria, July 2025. Association for Computational Linguistics. ISBN 979-8-89176-251-0. doi: 10.18653/v1/2025.acl-long.453. URL https://aclanthology.org/2025.acl-long.453/. Kishore Papineni, Salim Roukos, Todd Ward, and Wei-Jing Zhu. Bleu: method for automatic evaluation of machine translation. In Proceedings of the 40th Annual Meeting of the Association for Computational Linguistics, July 2002. doi: 10.3115/1073083.1073135. URL https:// www.aclweb.org/anthology/P02-1040. Krishna Pillutla, Swabha Swayamdipta, Rowan Zellers, John Thickstun, Sean Welleck, Yejin Choi, and Zaid Harchaoui. MAUVE: Measuring the gap between neural text and human text using divergence frontiers. In A. Beygelzimer, Y. Dauphin, P. Liang, and J. Wortman Vaughan (eds.), Advances in Neural Information Processing Systems, 2021. URL https://openreview. net/forum?id=Tqx7nJp7PR. Aditya Ramesh, Mikhail Pavlov, Gabriel Goh, Scott Gray, Chelsea Voss, Alec Radford, Mark Chen, and Ilya Sutskever. Zero-shot text-to-image generation. In Marina Meila and Tong Zhang (eds.), Proceedings of the 38th International Conference on Machine Learning, volume 139 of Proceedings of Machine Learning Research, pp. 88218831. PMLR, 1824 Jul 2021. URL https://proceedings.mlr.press/v139/ramesh21a.html. Ali Razavi, Aaron van den Oord, and Oriol Vinyals. Generating diverse high-fidelity images In H. Wallach, H. Larochelle, A. Beygelzimer, F. d'Alche-Buc, E. Fox, and with vq-vae-2. R. Garnett (eds.), Advances in Neural Information Processing Systems, volume 32. Curran Associates, Inc., 2019. URL https://proceedings.neurips.cc/paper_files/ paper/2019/file/5f8e2fa1718d1bbcadf1cd9c7a54fb8c-Paper.pdf. Sucheng Ren, Qihang Yu, Ju He, Xiaohui Shen, Alan Yuille, and Liang-Chieh Chen. FlowAR: Scale-wise autoregressive image generation meets flow matching. In Forty-second International Conference on Machine Learning, 2025a. URL https://openreview.net/forum?id= JfLgvNe1tj. Sucheng Ren, Qihang Yu, Ju He, Xiaohui Shen, Alan Yuille, and Liang-Chieh Chen. Beyond nexttoken: Next-x prediction for autoregressive visual generation, 2025b. URL https://arxiv. org/abs/2502.20388."
        },
        {
            "title": "Preprint",
            "content": "Danilo Rezende and Shakir Mohamed. Variational inference with normalizing flows. In Francis Bach and David Blei (eds.), Proceedings of the 32nd International Conference on Machine Learning, volume 37 of Proceedings of Machine Learning Research, pp. 15301538, Lille, France, 0709 Jul 2015. PMLR. URL https://proceedings.mlr.press/v37/rezende15. html. Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Bjorn Ommer. Highresolution image synthesis with latent diffusion models. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pp. 1068410695, June 2022. Frederick Sanders. On subjective probability forecasting. Journal of Applied Meteorology and Climatology, 2(2):191 201, 1963. doi: 10.1175/1520-0450(1963)0020191:OSPF 2.0.CO;2. URL https://journals.ametsoc.org/view/journals/apme/2/2/ 1520-0450_1963_002_0191_ospf_2_0_co_2.xml. Rico Sennrich, Barry Haddow, and Alexandra Birch. Neural machine translation of rare words with subword units. In Katrin Erk and Noah A. Smith (eds.), Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pp. 17151725, Berlin, Germany, August 2016. Association for Computational Linguistics. doi: 10.18653/v1/P16-1162. URL https://aclanthology.org/P16-1162/. Chenze Shao and Yang Feng. Non-monotonic latent alignments for CTC-based non-autoregressive machine translation. In Alice H. Oh, Alekh Agarwal, Danielle Belgrave, and Kyunghyun Cho (eds.), Advances in Neural Information Processing Systems, 2022. URL https:// openreview.net/forum?id=Qvh0SAPrYzH. Chenze Shao, Yang Feng, Jinchao Zhang, Fandong Meng, and Jie Zhou. Sequence-level training for non-autoregressive neural machine translation. Computational Linguistics, 47(4):891925, December 2021. doi: 10.1162/coli 00421. URL https://aclanthology.org/2021. cl-4.29/. Chenze Shao, Fandong Meng, Yijin Liu, and Jie Zhou. Language generation with strictly proper scoring rules. In Forty-first International Conference on Machine Learning, 2024. URL https: //openreview.net/forum?id=LALSZ88Xpx. Chenze Shao, Fandong Meng, and Jie Zhou. Beyond next token prediction: Patch-level training for large language models. In The Thirteenth International Conference on Learning Representations, 2025a. URL https://openreview.net/forum?id=dDpB23VbVa. Chenze Shao, Fandong Meng, and Jie Zhou. Continuous visual autoregressive generation via score In Forty-second International Conference on Machine Learning, 2025b. URL maximization. https://openreview.net/forum?id=avGZE46gL6. Noam Shazeer. Glu variants improve transformer. arXiv preprint arXiv:2002.05202, 2020a. Noam Shazeer. Glu variants improve transformer, 2020b. URL https://arxiv.org/abs/ 2002.05202. Jiaming Song, Chenlin Meng, and Stefano Ermon. Denoising diffusion implicit models. In International Conference on Learning Representations, 2021. URL https://openreview.net/ forum?id=St1giarCHLP. Mitchell Stern, Noam Shazeer, and Jakob Uszkoreit. Blockwise parallel decoding for deep autoregressive models. Advances in Neural Information Processing Systems, 31, 2018. Emma Strubell, Ananya Ganesh, and Andrew McCallum. Energy and policy considerations for deep learning in NLP. In Anna Korhonen, David Traum, and Lluıs M`arquez (eds.), Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pp. 36453650, Florence, Italy, July 2019. Association for Computational Linguistics. doi: 10.18653/v1/P19-1355. URL https://aclanthology.org/P19-1355/. Jianlin Su, Yu Lu, Shengfeng Pan, Ahmed Murtadha, Bo Wen, and Yunfeng Liu. Roformer: Enhanced transformer with rotary position embedding. arXiv preprint arXiv:2104.09864, 2021."
        },
        {
            "title": "Preprint",
            "content": "Peize Sun, Yi Jiang, Shoufa Chen, Shilong Zhang, Bingyue Peng, Ping Luo, and Zehuan Yuan. Autoregressive model beats diffusion: Llama for scalable image generation, 2024a. URL https://arxiv.org/abs/2406.06525. Yutao Sun, Hangbo Bao, Wenhui Wang, Zhiliang Peng, Li Dong, Shaohan Huang, Jianyong Wang, and Furu Wei. Multimodal latent language modeling with next-token diffusion, 2024b. URL https://arxiv.org/abs/2412.08635. Ilya Sutskever, James Martens, and Geoffrey Hinton. Generating text with recurrent neural networks. In Lise Getoor and Tobias Scheffer (eds.), Proceedings of the 28th International Conference on Machine Learning (ICML-11), ICML 11, pp. 10171024, New York, NY, USA, June 2011. ACM. ISBN 978-1-4503-0619-5. Gabor Szekely. E-statistics: The energy of statistical samples. Bowling Green State University, Department of Mathematics and Statistics Technical Report, 3(05):118, 2003. LCM team, Loıc Barrault, Paul-Ambroise Duquenne, Maha Elbayad, Artyom Kozhevnikov, Belen Alastruey, Pierre Andrews, Mariano Coria, Guillaume Couairon, Marta R. Costa-juss`a, David Dale, Hady Elsahar, Kevin Heffernan, Joao Maria Janeiro, Tuan Tran, Christophe Ropers, Eduardo Sanchez, Robin San Roman, Alexandre Mourachko, Safiyyah Saleem, and Holger Schwenk. Large concept models: Language modeling in sentence representation space, 2024. URL https://arxiv.org/abs/2412.08821. NextStep Team. Nextstep-1: Toward autoregressive image generation with continuous tokens at scale, 2025. URL https://arxiv.org/abs/2508.10711. Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timothee Lacroix, Baptiste Rozi`ere, Naman Goyal, Eric Hambro, Faisal Azhar, et al. Llama: Open and efficient foundation language models. arXiv preprint arXiv:2302.13971, 2023. Michael Tschannen, Cian Eastwood, and Fabian Mentzer. Givt: Generative infinite-vocabulary transformers. arXiv:2312.02116, 2023. Arnon Turetzky, Nimrod Shabtay, Slava Shechtman, Hagai Aronowitz, David Haws, Ron Hoory, and Avihu Dekel. Continuous speech synthesis using per-token latent diffusion, 2024. URL https://arxiv.org/abs/2410.16048. Amirhossein Vahidi, Simon Schosser, Lisa Wimmer, Yawei Li, Bernd Bischl, Eyke Hullermeier, and Mina Rezaei. Probabilistic self-supervised representation learning via scoring rules minimization. In The Twelfth International Conference on Learning Representations, 2024. URL https: //openreview.net/forum?id=skcTCdJz0f. Aaron van den Oord, Oriol Vinyals, and koray kavukcuoglu. Neural discrete representation learning. In I. Guyon, U. Von Luxburg, S. Bengio, H. Wallach, R. Fergus, S. Vishwanathan, and R. Garnett (eds.), Advances in Neural Information Processing Systems, volume 30. Curran Associates, Inc., 2017. URL https://proceedings.neurips.cc/paper_files/ paper/2017/file/7a98af17e63a0ac09ce2e96d03992fbc-Paper.pdf. Haoran Wei, Yaofeng Sun, and Yukun Li. Deepseek-ocr: Contexts optical compression, 2025. URL https://arxiv.org/abs/2510.18234. LILI YU, Daniel Simig, Colin Flaherty, Armen Aghajanyan, Luke Zettlemoyer, and Mike Lewis. MEGABYTE: Predicting million-byte sequences with multiscale transformers. In Thirty-seventh Conference on Neural Information Processing Systems, 2023. URL https://openreview. net/forum?id=JTmO2V9Xpz. Neil Zeghidour, Alejandro Luebs, Ahmed Omran, Jan Skoglund, and Marco Tagliasacchi. Soundstream: An end-to-end neural audio codec, 2021. URL https://arxiv.org/abs/2107. 03312. Biao Zhang and Rico Sennrich. Root mean square layer normalization. Advances in Neural Information Processing Systems, 32, 2019."
        },
        {
            "title": "Preprint",
            "content": "Boyang Zheng, Nanye Ma, Shengbang Tong, and Saining Xie. Diffusion transformers with representation autoencoders, 2025. URL https://arxiv.org/abs/2510.11690. Lianmin Zheng, Wei-Lin Chiang, Ying Sheng, Siyuan Zhuang, Zhanghao Wu, Yonghao Zhuang, Zi Lin, Zhuohan Li, Dacheng Li, Eric P. Xing, Hao Zhang, Joseph E. Gonzalez, and Ion Stoica. Judging llm-as-a-judge with mt-bench and chatbot arena, 2023. URL https://arxiv.org/ abs/2306.05685."
        },
        {
            "title": "A PROOF",
            "content": "A.1 PROOF OF THEOREM 1 Theorem 1. For an implicit discrete distribution (x) with sampler and temperature (0, 1), Algorithm 1 generates samples distributed as: PT (x) = (x)1/T ZT , ZT = (x)1/T . (cid:88) Proof. Algorithm 1 implements rejection sampling scheme where sample is accepted with probability Paccept(x). The proof proceeds by showing that the acceptance probability Paccept(x) = (x)1/T , so the rejection sampling procedure yields the desired normalized sample distribution: PT (x) = Paccept(x) Paccept(x) (cid:80) = (x)1/T (x)1/T (cid:80) . (16) The inverse temperature is decomposed as 1/T = + α, where = 1/T is the integer part and α [0, 1) is the fractional part. The acceptance probability is the product of the success probabilities of the two corresponding stages. Stage 1 (Integer Part): For the algorithm to proceed to stage 2 with candidate sample x, it must first draw for consecutive times in stage 1. As each draw is independent with probability (x), the probability of passing stage 1 with candidate is (x)n. Stage 2 (Fractional Part): Let = (x). The probability of acceptance in stage 2 is the cumulative probability of being accepted at any given iteration 1: Pstage2 = (accept at = 1) + (pass = 1, accept at = 2) + . . . = + (1 p) (cid:16) 1 (cid:17) + (1 p)2 (cid:16) 1 (cid:17) (cid:16) 1 α (cid:17) α 2 + . . . (cid:19) α (17) α 1 (cid:18) = = (cid:88) k=0 (cid:88) k= (cid:89) (1 p)k 1 j=1 (cid:18)α 1 (cid:19) (p 1)k = (p 1 + 1)α1 = pα. The second to last step is due to the generalized binomial theorem. Total Probability: The total probability of accepting sample in single trial is the product of the probabilities from the two stages: Paccept(x) = (x)n (x)α = (x)1/T , (18) which completes the proof."
        },
        {
            "title": "Preprint",
            "content": "A.2 PROOF OF THEOREM 2 Theorem 2. The expected number of calls to the base sampler S, denoted E[Ntotal], required to generate one sample using Algorithm 1 is: E[Ntotal] = + I(α > 0) (cid:80) ZT (x)1/T 1 where ZT = (cid:80) (x)1/T , = 1/T , α = 1/T n, and I() is the indicator function. Proof. The algorithm conducts series of independent trials, each with identical probability of success, and continues until one trial is successful, thereby following geometric distribution. Therefore, the expected number of samples is the ratio of the expected number of samples per trial, E[Ntrial], to the probability of trials success, (success): E[Ntotal] = E[Ntrial] (success) . (19) Denominator (Success Probability): trial is successful if any sample is accepted. The total success probability is the sum of acceptance probabilities over all possible outcomes: (success) = (cid:88) Paccept(x) = (cid:88) (x)1/T = ZT . (20) Numerator (Expected Calls per Trial): Let Ntrial be the number of sampler calls in single trial. trial always involves N1 calls for stage 1 and may involve N2 calls for stage 2. The number of calls in Stage 1 is fixed at N1 = n. Thus, E[N1] = n. If α = 0, stage 2 is never performed, so E[N2] = 0. If α > 0, stage 2 is only executed if stage 1 succeeds with some candidate x. Let Ex be this event, so (Ex) = (x)n, and we have: E[N2] = (cid:88) (Ex) E[N2Ex]. (21) The conditional expectation E[N2Ex] is the expected number of draws in stage 2 given candidate x. Using the formula E[X] = (cid:80) k=1 (X k), where is the number of draws in stage 2: E[N2Ex] = = (cid:88) k=1 (cid:88) k= (stage 2 requires at least draws) (1 (x))k (cid:89) (cid:18) 1 j= (cid:19) . α (22) This is the same sum we evaluated in Equation 17, which equals (x)α1. Therefore, if α > 0: E[N2] = (cid:88) (x)n (x)α1 = (x)n+α1 = (cid:88) (cid:88) (x)1/T 1. (23) Combining the two cases, the total expected number of calls per trial is: E[Ntrial] = E[N1] + E[N2] = + I(α > 0) (x)1/T 1. (cid:88) Combining the numerator and denominator gives the final result: E[Ntotal] = + I(α > 0) (cid:80) ZT (x)1/T 1 . (24) (25)"
        },
        {
            "title": "Preprint",
            "content": "Corollary 2.1. Let be the size of sample space. The expected number of sampler calls E[Ntotal] at temperature (0, 1) is bounded by: E[Ntotal] , 1 + ZT 1 + 21/T ZT if 0 < 0.5 , if 0.5 < < 1 where = 1/T and ZT = (cid:80) (x)1/T . Proof. The proof is divided into two cases based on the temperature range. We start from the general formula for the expected cost from Theorem 2: E[Ntotal] = + I(α > 0) (cid:80) ZT (x)1/T 1 . (26) Case 1: Low-Temperature Regime (0 < 0.5) In this range, the exponent 1/T 1 1. Since (x) [0, 1], for any exponent β 1, we have (x)β (x). Thus, by summing over the entire sample space : (cid:88) (cid:88) (x)1/T 1 (x) = 1. (27) The numerator of the cost formula is therefore bounded by + 1: xX xX + I(α > 0) (cid:88) xX (x)1/T 1 + I(α > 0) 1 + 1. (28) This establishes the bound for the low-temperature regime. Case 2: High-Temperature Regime (0.5 < < 1) In this range,the exponent β = 1/T 1 is in the interval (0, 1). For such an exponent, the function (p) = pβ is concave. By Jensens inequality, the sum (cid:80) xX (x)β is maximized when (x) is uniform distribution over the sample space, i.e., (x) = 1/K for all . The bound is: (x)1/T 1 (cid:88) xX (cid:18) 1 (cid:88) xX (cid:19)1/T 1 (cid:19)1/T = (cid:18) 1 = 21/T . (29) Substituting this into the cost formula gives the bound for the high-temperature regime. This completes the proof."
        },
        {
            "title": "Preprint",
            "content": "A.3 PROOF OF THEOREM 3 Theorem 3. Let Palg(x; ) be the probability of sampling using Algorithm 2 with batch size of , and let PT (x) = (x)n/ZT be the true target distribution at temperature = 1/n, where ZT = (cid:80) (x)n. The algorithm is asymptotically unbiased: lim Palg(x; ) = PT (x). Proof. Let = {x1, . . . , xN } be batch of samples drawn i.i.d. from the base distribution (x). For any sample , let Cx be the random variable for the count of in B. The weight assigned to is Wx = (cid:0)Cx (cid:1). Let XN be the random variable for the probability of sampling from B: XN (x) = Wx zX Wz (cid:80) = (cid:0)Cx (cid:80) zX (cid:1) (cid:1) . (cid:0)Cz (30) The overall probability we seek to analyze is the expectation of this random variable: Palg(x; ) = E[XN ]. Our goal is to show that limN E[XN ] = PT (x). The proof proceeds in two main steps: first, we show that the random variable XN converges in probability to PT (x); second, we use the Bounded Convergence Theorem to show that this implies the convergence of its expectation. 1. Convergence in Probability. By the Weak Law of Large Numbers, the proportion of occurrences of any sample converges in probability to its true probability (x): Cx (x) as . (31) The weight Wx can be written as polynomial in Cx, Wx = 1 normalize this by n: n! Cx(Cx 1) . . . (Cx + 1). We Wx = 1 n! (cid:18) Cx (cid:19) (cid:18) Cx 1 (cid:19) . . . (cid:18) Cx + 1 (cid:19) . (32) Since Cx the Continuous Mapping Theorem, the entire expression converges in probability: 0 for any constant k, each term in the product converges to (x). By (x) and Now, we analyze the random variable XN by dividing its numerator and denominator by n: Wx 1 n! (x)n. XN = (cid:80) Wx/N zX (Wz/N n) . (33) (34) Applying the Continuous Mapping Theorem again for the ratio, we show that the random variable XN converges in probability to PT (x): XN 1 n! (x)n (cid:80) zX 1 n! (z)n = (x)n zX (z)n = PT (x). (cid:80) (35) 2. Convergence of Expectation. We have established that the random variable XN converges in probability to PT (x). Besides, we have that XN is inherently bounded: 0 XN = Wx zX Wz (cid:80) 1. (36) We can now invoke the Bounded Convergence Theorem, which states that if sequence of random variables XN converges in probability to X, and XN for all for some constant , then limN E[XN ] = E[limN XN ]. Applying this theorem to our case: lim Palg(x; ) = lim E[XN ] = (cid:104) lim XN (cid:105) = PT (x). (37) This completes the proof that the algorithm is asymptotically unbiased."
        }
    ],
    "affiliations": [
        "Qiuzhen College, Tsinghua University",
        "WeChat AI, Tencent Inc"
    ]
}