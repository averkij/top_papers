{
    "paper_title": "VisionReward: Fine-Grained Multi-Dimensional Human Preference Learning for Image and Video Generation",
    "authors": [
        "Jiazheng Xu",
        "Yu Huang",
        "Jiale Cheng",
        "Yuanming Yang",
        "Jiajun Xu",
        "Yuan Wang",
        "Wenbo Duan",
        "Shen Yang",
        "Qunlin Jin",
        "Shurun Li",
        "Jiayan Teng",
        "Zhuoyi Yang",
        "Wendi Zheng",
        "Xiao Liu",
        "Ming Ding",
        "Xiaohan Zhang",
        "Xiaotao Gu",
        "Shiyu Huang",
        "Minlie Huang",
        "Jie Tang",
        "Yuxiao Dong"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "We present a general strategy to aligning visual generation models -- both image and video generation -- with human preference. To start with, we build VisionReward -- a fine-grained and multi-dimensional reward model. We decompose human preferences in images and videos into multiple dimensions, each represented by a series of judgment questions, linearly weighted and summed to an interpretable and accurate score. To address the challenges of video quality assessment, we systematically analyze various dynamic features of videos, which helps VisionReward surpass VideoScore by 17.2% and achieve top performance for video preference prediction. Based on VisionReward, we develop a multi-objective preference learning algorithm that effectively addresses the issue of confounding factors within preference data. Our approach significantly outperforms existing image and video scoring methods on both machine metrics and human evaluation. All code and datasets are provided at https://github.com/THUDM/VisionReward."
        },
        {
            "title": "Start",
            "content": "4 2 0 2 0 3 ] . [ 1 9 5 0 1 2 . 2 1 4 2 : r VisionReward: Fine-Grained Multi-Dimensional Human Preference Learning for Image and Video Generation Jiazheng Xu1 Yu Huang1 Jiale Cheng1 Yuanming Yang1 Jiajun Xu1 Yuan Wang1 Wenbo Duan1 Shen Yang1 Qunlin Jin1 Shurun Li1 Jiayan Teng1 Zhuoyi Yang Wendi Zheng1 Xiao Liu1 Ming Ding2 Xiaohan Zhang2 Xiaotao Gu2 Shiyu Huang2 Minlie Huang1 Jie Tang1 Yuxiao Dong1 1Tsinghua University 2Zhipu AI"
        },
        {
            "title": "Abstract",
            "content": "We present general strategy to aligning visual generation modelsboth image and video generationwith human preference. To start with, we build VisionReward fine-grained and multi-dimensional reward model. We decompose human preferences in images and videos into multiple dimensions, each represented by series of judgment questions, linearly weighted and summed to an interpretable and accurate score. To address the challenges of video quality assessment, we systematically analyze various dynamic features of videos, which helps VisionReward surpass VideoScore by 17.2% and achieve top performance for video preference prediction. Based on VisionReward, we develop multi-objective preference learning algorithm that effectively addresses the issue of confounding factors within preference data. Our approach significantly outperforms existing image and video scoring methods on both machine metrics and human evaluation. All code and datasets are provided at https://github. com/THUDM/VisionReward. 1. Introduction Visual generation models, including text-to-image [2, 8, 27, 3032] and text-to-video [4, 14, 15, 39, 47, 51] generation, have developed rapidly in recent times. Given textual description, these visual generation models can produce highquality images or videos. Inspired by reinforcement learning from human feedback (RLHF) in large language model (LLM) [26], researchers have found it crucial to align textto-image with human preference [46], due to various challenges such as alignment, fidelity, aesthetic and safety. *Equal contributions. {xjz22, h-y22}@mails.tsinghua.edu.cn Core contributors: Jiazheng, Yu, Jiale, Yuanming, Jiajun, Yuan, Wenbo and Shen. series of works [20, 44, 46] have explored training reward models that simulate human preferences, and subsequent works [3, 6, 9, 40, 45] have developed optimization methods for diffusion models based on these reward models or preference dataset. With the development of text-to-video models, similar evaluation [11, 17] and optimization [28, 48] methods have been attempted and get improvement. Despite advancements, current RLHF methods for textto-vision models still face substantial challenges: Reward Models are Biased and Inexplicable. Current reward models learn from human preference, which contain many trade-offs between different factors, leading to preference biases. Fig. 1 (a) demonstrates an example. Evaluation for Video is Challenging. Its difficult to assess dynamic quality of videos, such as movement reality and motion smoothness, as shown in Fig. 1 (b). Over-optimization or Lack-optimization. Existing RLHF methods tend to over-optimize or weaken certain factors, resulting in suboptimal outcomes after optimization, as revealed in Fig. 1 (c)(d). Contributions. To address these challenges, we propose fine-grained, multi-dimensional reward model for text-tovision generationVisionRewardwhich performs variety of judgmental questions and answers, and uses linear weighting to predict human preference. Based on VisionReward, we develop MPO algorithm to stably optimize visual generation models. Our contributions include: We design unified annotation system for both image and video generation, decomposing the factors influencing human preferences. To address the challenges of video evaluation, we incorporate extensive observations of dynamic content in videos into our judgment tasks, such as motion stability or movement quality. The annotation contains 3 million questions for 48k images and 2 million questions for 33k videos. This dataset enables unified training pipeline for VisionReward (shown as Fig. 2). 1 Figure 1. Samples of VisionReward and Multi-Objective Preference Optimization (MPO) algorithm. 2 We demonstrate that VisionReward achieves high accuracy and is interpretable in predicting human preferences. VisionReward outperforms existing methods for preference prediction, especially on video assessment, surpassing VideoScore [11] by 17.2 %. We introduce Multi-Objective Preference Optimization (MPO) for stably tuning visual generation models, avoiding over-optimization or lack of optimization for certain factors. MPO with VisionReward outperforms directly tuned with human annotation or other RM."
        },
        {
            "title": "Source",
            "content": "#Samples #Checklist"
        },
        {
            "title": "Video",
            "content": "ImageRewardDB [46] Pick-a-Pic [20] HPDv2 [44] CogVideoX [47] Open-Sora [51] VideoCrafter2 [4] Panda-70M [5] 16K 16K 16K 10K 10K 10K 3K 1M 1M 1M 0.6M 0.6M 0.6M 0.2M Table 2. Statistics of source data and annotation. 2. Method 2.1. Annotation Fine-grained Questions Design. When evaluating an image or video, human preferences are often result of the interplay of multiple factors, necessitating balance among various considerations. To deconstruct human preferences systematically, we develop fine-grained and multidimensional preference decomposition framework. Tab. 1 illustrates our preference dimensions and the checklist count. We identify 5 dimensions for text-to-image generation and expand to 9 dimensions for text-to-video. In our annotation process, we design multiple-choice questions for each dimension, allowing annotators to select the option that best fits the image. Additionally, we further decompose these options into several binary (yes/no) questions, ultimately forming checklist for evaluating each image or video. This checklist is then utilized for subsequent evaluation systems and reward model training. Annotation Data Preparation. To ensure diversity in our annotated data, we sample from various data sources, as presented in Tab. 2. Following this fine-grained annotation process, we compile dataset comprising 48k images and 33k videos."
        },
        {
            "title": "Dimension",
            "content": "#Sub-dimension #Checklist"
        },
        {
            "title": "Image Video",
            "content": "Alignment Composition Quality Fidelity Safety&Emotion"
        },
        {
            "title": "Stability\nDynamic\nPhysics\nPreservation",
            "content": "1 5 5 5 2 - - - - 1 1 4 3 1 5 2 1 2 1 13 14 25 8 - - - -"
        },
        {
            "title": "Total",
            "content": "18 20 61 4 2 14 9 4 12 8 4 7 Table 1. Taxonomy of annotation for VisionReward. 2.2. VisionReward Training VisionReward consists of two components: 1. Checklist: VisionReward is vision-language generative model to answer set of judgment questions regarding an image or video with yes or no. 2. Linear Weighted Summarize: VisionReward applies linear weighting to the list of yes or no responses to obtain the final score. Checklist. Specifically, we use CogVLM2 [16] as the base model for image understanding, and CogVLM2-Video as the base model for video understanding. In terms of data, we have obtained millions of annotated binary judgment Initially, we performed balanced sampling on results. each judgment question by addressing the imbalance between positive and negative examples, ensuring roughly equal number of positive and negative instances associated with each judgment question. Then we use balanced instruction tuning dataset consisting of judgement questions to fine-tune base VLM. Regression. Based on series of binary responses (yes or no), we use linear weighted summaries to predict human preferences. This is achieved by mapping yes and no to 1 and -1, respectively, and constructing feature vector = {xi}, where = 1, . . . , and represents the number of binary questions. We use set of linear weights to obtain score that accurately reflects human preferences, expressed as follows: score = xT = x1w1 + x2w2 + . . . + xnwn. (1) In order to learn these parameters, we collect dataset of human preferences = {(Xi, Xj)}, compute the feature difference for each pair, given by = Xi Xj, and the corresponding label is assigned as = 1 or = 0 depending on the human preference. We then perform regression using the logistic regression model based on X. The predicted value (probability) is given by: ˆy = σ(xT w) = 1 1 + exT . (2) 3 Figure 2. An overview of the VisionReward and Multi-Objective Preference Optimization (MPO)."
        },
        {
            "title": "The objective function to be minimized during training",
            "content": "is defined as: Loss(w) = (cid:2)y log (cid:0)σ(xT w)(cid:1) +(1 y) log (cid:0)1 σ(xT w)(cid:1)(cid:3) . (3) Through minimizing this objective function, we aim to find the optimal weights that best represent human preferences. 2.3. Multi-Objective Preference Optimization Preliminary. Given data distribution q(x0), Diffusion models [13, 34, 35] contains forward process and reverse process. Forward process q(x1:T x0) gradually add noise to the data x0 and reverse process pθ(x0:T ) learns transitions to recover data. Training diffusion model can be performed by evidence lower bound [19, 36]: LDM = Ex0,ϵN (0,I),t (cid:104) ϵ ϵθ (xt, t)2 2 (cid:105) . (4) with U(0, ) and xt (xt x0). By learning to recover images, diffusion models are able to generate images that are meaningful to humans. However, just learning on recovering data can not learn to meet humans preference and needs. Some works have explored to use human preference to guide diffusion to move close to humans. Diffusion-DPO [40] introduces to use preference pairs towards direct preference learning. We denote the winning and losing samples as xw 0, and the objective is as follows: 0 , xl L(θ) = (xw 0 ,xl 0)D,tU (0,T ),xw q(xw xw 0 ),xl tq(xl txl 0) log σ (βT ω (λt) ( , t)2 ϵw ϵθ (xw (cid:16)(cid:13) (cid:0)xl (cid:13)ϵl ϵθ 2 ϵw ϵref (xw 2 (cid:13) t, t(cid:1)(cid:13) 2 (cid:13) (cid:13)ϵl ϵref , t)2 t, t(cid:1)(cid:13) 2 (cid:0)xl (cid:13) 2 2 (cid:17)(cid:17)(cid:17) (5) where λt is the signal-to-noise ratio. Challenges. Although Diffusion-DPO has designed direct preference optimization method to fine-tune text-toimage diffusion models, it suffers from being biased and over-optimized on certain dimensions such as aesthetic but gets somewhat worse, for example, less safe and more trouble about human body. We reproduce the procedure of Diffusion-DPO and test the fine-tuned model on 10,000 sampled prompts from training data, using VisionReward to score images. Fig. 3 illustrates the changes across various dimensions after applying preference learning using the 4 human preference dataset in which factors are coupled with different trade-offs. Formally, let the factors influencing human preferences be = {d1, d2, . . . , dn}. During preference learning, it is likely that when there is trade-off between di and dj, humans may prioritize di more, leading to an increase in di and decrease in dj. (a) Data analysis. (b) DPO analysis. Figure 3. (a) We sample 10,000 human preference pairs from Pick-a-Pic [20] dataset and analyze score deviations across 18 subdimensions (represented by the average yes-proportion of checklist questions within each sub-dimension). (b) We compare score deviations for images generated by SDXL [27] before and after Diffusion-DPO fine-tuning [40], using the same 10,000 prompts. MPO: Insight and Solution. We thus propose an algorithm (Cf. Algorithm 1) to ensure that all dimensions are reasonably enhanced. We merge questions of VisionReward into these dimensions, formulated as reward = R(dk) where {1, . . . , n} and R(dk) = (cid:80)(reward(q)) with dk. Given two images xi and xj, with their corresponding pref1, . . . , dj erence factor vectors being {di n} respectively, we define situation where Ri dominates Rj if R(di k) for any {1, . . . , n}. We can use the dominant pairs to ensure that each dimension is not weakened in the preferences. k) R(dj n} and {dj 1, . . . , di Algorithm 1 Multi-Objective Preference Optimization (MPO) 1: Dataset: Prompt set C, question set for reward model = {q1, q2, ..., qm} 2: Input: Diffusion model pθ, reward model r, the number of generate images per prompt (cid:9) pθ(x0c) 0, x2 0, ..., xK 0 Sample set of images (cid:8)x1 for = 1 to do 3: Initialization: Dominated set for MPO 4: for do 5: 6: 7: 8: 9: 10: 11: Reward set Ri for = 1 to do Add reward r(qj xi) to Ri for i, {1, 2, ..., K} do if Ri dominate Rj then c, xi to (cid:110) (cid:111) 0, xj 0 Add 12: 13: for data do 14: Update the gradient pθ from Eq. (5) Type Image Video Content People, Objects, Animals, Story, Human Activity, Architecture, Landscape, Artificial Scene, Others, Natural Animal Activity, Vehicles, Plants, Food, Physical Phenomena Others, Scenes Challenge Material, Angle and Lens, Emotional Expression, Unreal, Style, History, Fine-grained Detail, Color, Famous Character, Color/Tone, Surreal, Normal, Famous Places, World Knowledge, Writing, Complex Combo, Special Effects, Text, Spatial Relationship, Positional, Counting, Camera Movement, Logical Consistency, Style, Temporal Speed Table 3. Content and Challenge Categories of MonetBench. 3. Experiments 3.1. VisionReward: Preference Prediction Dataset & Training Setting. After balanced sampling (Cf. Sec. 2.2), we obtain 40,743 images and corresponding training data containing 97,680 judgment questions from images, leaving 6,910 images for subsequent validation and test. For videos, we obtain 28,605 videos and corresponding training data containing 89,473 judgment questions, with 3,080 videos reserved. To fine-tune CogVLM2 [16], we set batch size of 64, learning rate of 1e-6, and train for 1,500 steps. For CogVLM-Video, we set batch size of 64, learning rate of 4e-6, and train for 1,500 steps using 32 A800 GPU hours. To learn linear weights for preference prediction, we sample human preference pairs and perform logistic regression. For images, We sample 44k pairs (24k from HPDv2 [44] and 20k from ImageRewardDB [46]); and for videos, we sample prompts from VidProM [41] and generate videos (using CogVideoX [47], VideoCrafter2 [4] and OpenSora [51]), getting 1,795 annotated video pairs with preference. To establish comprehensive evaluation benchmark for both image and video generation, we construct MonetBench, which contains separate test sets for images and videos, each consisting of 1,000 prompts. Tab. 3 shows content and challenge categories for MonetBench. More details are introduced in the Appendix. Main Results: Preference Accuracy. Preference accuracy means the probability that reward model has the same judgment as humans about which image is better. We use MonetBench to construct our test set for human preference, using SDXL [27] to generate images and CogVideoX [47] / VideoCrafter2 [4] / OpenSora [51] to generate videos, resulting in 500 pairs for image and 1,000 pairs for video. We employ annotators to assess the generated images using preference rating scale from 1 to 5 (with 3 indicating no"
        },
        {
            "title": "Method",
            "content": "HPDv2 [44] 74.0 79.8 83.3 task-specific discriminative models ImageReward [46] PickScore [20] HPSv2 [44] generative models GPT-4o [1] Gemini [38] VQAScore [23] VideoScore [11] VisionReward (Ours) 77.5 60.7 69.7 76.8 81."
        },
        {
            "title": "MonetBench",
            "content": "GenAI-Bench [18]"
        },
        {
            "title": "MonetBench",
            "content": "tau 48.8 49.8 48.4 38.9 27.4 49.4 45.8 51.8 diff 56.5 57.6 55.6 52.7 55.1 56.5 52.5 59. tau 48.4 52.4 49.3 41.8 46.9 45.2 47.8 51.8 diff 72.1 75.4 73.0 54.3 61.7 68.0 71.4 74. tau 55.8 57.7 59.3 45.7 52.2 56.1 49.1 64.0 diff 58.4 61.6 62.5 48.3 56.8 59.5 54.9 72. Table 4. Preference accuracy on multiple dataset. Bold denotes the best score within the generative models, while underline signifies the best score among all categories. Tau means taking account of ties [7], and diff means dropping ties in labels (we drop ties both in labels and responses for GPT-4o and Gemini in diff because too many ties are given by them)."
        },
        {
            "title": "Video",
            "content": "Composition Quality Fidelity Safety&Emotion Stability Dynamic Physics Preservation LLaVa CogVLM2 [16] GPT-4o [1] Gemini [38] VisionReward (Ours) 59.9 65.8 73.1 69.4 78.8 65.7 67.1 62.7 59.9 81.1 80.9 53.1 61.9 59.7 80.9 64.4 74.7 70.1 74.9 83. 52.5 49.3 57.9 58.1 64.8 53.8 57.1 69.1 71.1 75.4 50.6 51.2 62.4 58.1 68.1 47.5 47.8 58.8 59.6 72.0 Table 5. Accuracy of VisionReward and other vision-language models (VLMs) on vision quality questions constructed from our annotation. We test LLaVA-v1.5-7B [24] for image and LLava-Next-Video-34B [21] for video. preference). The average preference score is used as the final preference label. We also take HPDv2 [44] and GenAIBench [18] as test set. Tab. 4 shows that VisionReward obtains state-of-thestate results in multiple datasets. Notably, in video evaluation, image reward models demonstrate competitive performance when the video duration is within 2 seconds (GenAIBench). However, when the video duration reaches 6 seconds (MonetBench), only VisionReward is capable of accurately predicting human preference, being twice (22.1% over random) as high as the best (12.5% over random) among other methods. This indicates that dynamic information in longer videos poses challenge for RMs, while VisionReward can effectively address this issue. Composition Quality Fidelity 97.9 98. 98.3 Safety 99.1 Stability Dynamic Physics Preservation 97.4 99.9 88.2 99.8 Table 6. Consistency of VisionReward in each dimension. Main Results: Accuracy and Consistency on Judgment. To evaluate the effectiveness of judgment learning, we construct visual quality QA set to assess the visual assessment capabilities of VisionReward compared to other VLMs. We collect 1,364 test cases for images involving 14 types of questions across 4 dimensions, and additionally 1,308 cases covering 8 types of questions across 4 dimensions related to dynamic content in videos. To ensure the generality of these questions, we combine adjacent degrees in the checklists under each sub-dimension, enhancing distinctiveness and minimizing incidental subjectivity. Tab. 5 demonstrates VisionRewards superiority in judging visual quality. As questions corresponding to each sub-dimension assess varying degrees of particular factor, its important to measure consistency of VisionReward across multiple questions of the same sub-dimension. Consistency measures the likelihood that the model provides consistent responses across series of judgments concerning this factor. Tab. 6 shows that VisionReward has high consistency (more than 97 %) in most (7 of 8) dimensions. Ablation Study: Size of Train Set for Regression. We perform different scales from our training set comprising 6 44,000 pairs used for logistic regression. Each sampled set is then utilized for iterative regression, and the accuracy is tested on the HPDv2 test set. Tab. 7 shows that the performance of the regression improves with the initial growth of the size of training set and then stabilizes."
        },
        {
            "title": "Accuracy",
            "content": "100 76.5 2k 80.9 200 77. 4k 81.3 500 80.3 8k 81. 1k 80.6 16k 81.3 Table 7. Average accuracy for different regression sizes. 3.2. MPO for Text-to-Image Optimization Dataset & Training Settings. We strategically sample 63,165 prompts from existing datasets and generate 8 images per prompt using SDXL (this procedure theoretically produces 1.76M text-image pairs). Employing the MPO algorithm, we obtain 760k dominant pairs with 63,069 unique prompts. For comparison, we use HPSv2 with threshold of 0.0015, getting 770k pairs with 63,107 unique prompts from the same source. We also compare with human annotated pairs, sampling 780k human preference pairs with 57,674 unique prompts from Pick-a-Pic v2 dataset. We maintain consistent training parameters and dataset sizes across all experiments to ensure fair comparison. For all three experiments, we used an effective batch size of 256 (with GAS set to 4 and train batch size set to 1), set β to 5000, and learning rate of 5e-9 (before scaling). We employ constant warmup strategy with 100 steps. The training was conducted over 3,000 steps (approximately 1 epoch) using 64 A800 GPUs. Evaluation Settings. We conducted both automatic and human evaluation on DrawBench [32]. Automatic evaluation includes multiple metrics such as human preference RMs, CLIP [29] and LAION-Aesthetic [33]. Human evaluation requires annotators to comprehensively evaluate two images and select the better one, with an option to indicate tie if neither is preferred. Annotators are required to take different perspectives (such as composition, image quality, realism, safety, and image-text alignment) into account. Experimental Results. Main results are demonstrated in Tab. 8 and Tab. 9. MPO with VisionReward gets leading results across multiple machine metrics and achieves significant improvements across all four dimensions of VisionReward. Fig. 4 shows that results of MPO with VisionReward are most preferred by humans. Ablation Study: MPO or Single RM. To evaluate the effectiveness of multi-objective versus single-objective direct preference learning, we utilize VisionReward and conduct Methods CLIP Aes HPSv2 PickScore Baseline DPO with Pick-a-Pic DPO with HPSv2 MPO (Ours) 0.273 0.279 0.277 0. 5.463 5.511 5.599 5.612 0.282 0.286 0.292 0.289 22.25 22.45 22.58 22.61 Table 8. Evaluation results of multiple metrics on DrawBench. Methods Composition Quality Fidelity Safety&Emotion Baseline DPO with Pick-a-Pic DPO with HPSv2 MPO (Ours) 0.755 0.765 0.874 0.894 0.550 0.588 0.630 0.670 0.009 0.009 0.010 0.017 -0.008 -0.009 -0.004 -0.001 Table 9. Evaluation results analyzed by VisionReward. Figure 4. Human evaluation of text-to-image MPO. comparative experiment using weighted single score as the DPO. Specifically, we sample 760k pairs under the MPO method, and sample 780k pairs using the weighted single score (threshold of 0.014). We train for 2.2k iterations with batch size of 1,024 and learning rate of 1e-8. The results are shown in Tab. 10, which indicates that multi-objective can achieve better results. Methods CLIP Aes HPSv PickScore Baseline DPO with VisionReward MPO with VisionReward 0.273 0.278 0.278 5.463 5.664 5.719 0.282 0.291 0.291 22.25 22.227 22. Table 10. Evaluation results on DrawBench. 3.3. MPO for Text-to-Video Optimization. Dataset & Training Settings. The MPO training prompts are sampled from the VidProM [41] dataset. To adapt these prompts for video generation, we have optimized them following guidelines from CogVideoX [47], which results in roughly 22,000 samples. Due to limitations in the GPU budget, we have not used as many prompts as for image MPO, and we leave the data scaling study for future work. For our backbone model, we select CogVideoX-2b1. From each prompt, we have generated four videos. We use VisionReward to score these videos and apply the MPO algorithm to select approximately 9,400 effective preference pairs. In all our experiments, we maintain batch size of 32, learning rate of 5e-6, and employ 100 warmup steps followed by linear decay. We set the DPO parameter β to 500. The MPO training process spans around 500 steps, equivalent 1https://huggingface.co/THUDM/CogVideoX-2b 7 Additionally, VisionReward with MPO largely outperforms the VideoScore with MPO, underscoring the superiority of VisionReward compared to VideoScore. Ablation Study: Different Strategy for MPO. In using the MPO algorithm, we define the way in which Ri dominates Rj (given two images xi and xj) to select pairs. The definition of dominate includes at least three methods for different reward objective: total weighted score, score of each dimension, and score of each sub-dimension. To investigate the impact of different definitions of dominate, we conduct experiments based on CogVideoX-5b. Specifically, we employ three different strategies, setting the threshold of the total score to 0.8, 0.6, and 0.4 respectively, ensuring that the number of pairs obtained through all three strategies is 5k. We use batch size of 64, learning rate of 2e-6, the DPO parameter β of 500, and the training steps of 300. After training, we compare the evaluation results of VisionReward. Tab. 13 shows that using the score for each dimension as the reward objective yields the best results. Methods Baseline Total Dimension Sub-dimension VisionReward 4.303 4.515 4.573 4.514 Table 13. Score of VisionReward after different strategies of MPO. Total: dominate based on total weighted score. Dimension: dominate based on score of each dimension. Sub-dimension: dominate based on score of each sub-dimension. 4. Related Work Evaluation for Text-to-image and Text-to-video. The development of multimodal generative models is advancing rapidly, including text-to-image [2, 8, 27, 3032] and textto-video [4, 14, 15, 39, 47, 51] models that generate highquality images or videos based on given textual input. For text-to-image, early evaluations heavily relied on metrics such as FID [12] and CLIP [29] Score. Recent work [46] has pointed out that these metrics may not align well with human preferences. Many metrics based on reward models learn from human preferences [20, 44, 46], resulting in single score, which can lead to biases. With the advancement of Vision-Language Models (VLMs) [1, 16, 21, 24, 38, 42], some studies [23, 50] have attempted to directly use VLMs to assess images, considering aspects like alignment and quality. Nevertheless, we find that VLMs have gap in their ability to accurately assess vision quality. VisionReward proposes fine-grained, multi-dimensional evaluation framework, using specialized judgment instructions to achieve more accurate and interpretable. RLHF for Text-to-image and Text-to-video. Reinforcement Learning from Human Feedback (RLHF) [25, 26, 37] refers to optimizing models with reinforcement learning based on human feedback, which has been proven to sigFigure 5. Human evaluation of text-to-video MPO. to about 2 epochs. During training, we save checkpoint every 40 steps and use validation set split from the training set to pick the checkpoint with the highest reward. All experiments are conducted on 32 A800 GPUs. Evaluation Settings. To comprehensively assess the MPO models, we have conducted both automatic and human evaluations. The automatic evaluation is conducted across various benchmarks, including VBench [17] and our VideoMonetBench. For VBench, we focus on commonly reported key metrics, including Human Action, Scene, Multiple Objects, and Appearance Style (Appear. Style). The human evaluation examines aspects like alignment to instructions, video realism, and aesthetic appeal. Annotators are asked to watch two videos and select the better one, with an option to indicate tie if neither stands out. This manual assessment also uses the tasks in Video-MonetBench. In all these experiments, we utilize prompt optimization recommended in CogVideoX. Our baseline comparisons include the original CogVideoX-2b and VideoScore with MPO. Methods Baseline VideoScore VisionReward Human Action 98.20 97.60 98.40 Scene 55.60 56.25 57. Multiple Objects Appear. Style 68.43 68.66 71.54 24.20 23.96 24.02 Table 11. Evaluation results on VBench. Methods Stability Dynamic Physics Preservation Baseline VideoScore VisionReward 0.272 0.242 0.309 0.047 0.046 0.036 0.323 0.319 0.337 0.584 0.557 0. Table 12. Evaluation results on MonetBench. Experimental Results. The main results are shown in Tab. 11 and Tab. 12. When compared to the original CogVideoX-2b, MPO with VisionReward significantly enhances model performance across these benchmarks. In contrast, MPO with VideoScore tends to degrade performance on several metrics. This highlights the excellence of VisionReward in multi-objective reward modeling. Human evaluation results, depicted in Fig. 5, further validate the effectiveness of our approach. VisionReward with MPO demonstrates significant improvements over the base model, achieving approximately 27% increase in win rate. 8 nificantly enhance language models. For visual generation tasks, several works have explored RLHF, optimizing from the gradient [45, 46] or using policy-based RL approach [3, 6, 9]. All these methods require reward model (RM) to provide feedback for online learning. DiffusionDPO [40] has proposed to optimize the diffusion model directly using human-labeled preference data. However, most RLHF methods face the issue of over-optimization. VisionReward highlights that these issues are because of excessive optimization of some factors and degradation of others. By employing Pareto-optimality approach from multiobjective perspective, VisionReward achieves robust RLHF. 5. Conclusion We introduce VisionReward, reward model for visual generation, which is fine-grained and multi-dimensional. By enabling Vision-Language Model (VLM) to perform checklist assessments and applying linear summation with weighting coefficients derived from preference learning, VisionReward achieves highly accurate and interpretable. Based on VisionReward, we develop an algorithm of multiobjective preference learning, which addresses issues of over-optimization or lack-optimization. 6. Acknowledgments The authors would like to thank Zhipu AI for sponsoring the computation resources used in this work."
        },
        {
            "title": "References",
            "content": "[1] Josh Achiam, Steven Adler, Sandhini Agarwal, Lama Ahmad, Ilge Akkaya, Florencia Leoni Aleman, Diogo Almeida, Janko Altenschmidt, Sam Altman, Shyamal Anadkat, et al. Gpt-4 technical report. arXiv preprint arXiv:2303.08774, 2023. 6, 8 [2] James Betker, Gabriel Goh, Li Jing, Tim Brooks, Jianfeng Wang, Linjie Li, Long Ouyang, Juntang Zhuang, Joyce Improving image generation with Lee, Yufei Guo, et al. better captions. Computer Science. https://cdn. openai. com/papers/dall-e-3. pdf, 2(3):8, 2023. 1, 8 [3] Kevin Black, Michael Janner, Yilun Du, Ilya Kostrikov, and Sergey Levine. Training diffusion models with reinforcement learning. arXiv preprint arXiv:2305.13301, 2023. 1, 9 [4] Haoxin Chen, Yong Zhang, Xiaodong Cun, Menghan Xia, Xintao Wang, Chao Weng, and Ying Shan. Videocrafter2: Overcoming data limitations for high-quality video diffusion models. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 7310 7320, 2024. 1, 3, 5, 8, 12 [5] Tsai-Shien Chen, Aliaksandr Siarohin, Willi Menapace, Ekaterina Deyneka, Hsiang-wei Chao, Byung Eun Jeon, Yuwei Fang, Hsin-Ying Lee, Jian Ren, Ming-Hsuan Yang, et al. Panda-70m: Captioning 70m videos with multiple In Proceedings of the IEEE/CVF cross-modality teachers. Conference on Computer Vision and Pattern Recognition, pages 1332013331, 2024. 3, [6] Kevin Clark, Paul Vicol, Kevin Swersky, and David Fleet. Directly fine-tuning diffusion models on differentiable rewards. arXiv preprint arXiv:2309.17400, 2023. 1, 9 [7] Daniel Deutsch, George Foster, and Markus Freitag. Ties matter: Meta-evaluating modern metrics with pairwise accuracy and tie calibration. In Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing, pages 1291412929, 2023. 6 [8] Ming Ding, Zhuoyi Yang, Wenyi Hong, Wendi Zheng, Chang Zhou, Da Yin, Junyang Lin, Xu Zou, Zhou Shao, Hongxia Yang, et al. Cogview: Mastering text-to-image generation via transformers. Advances in Neural Information Processing Systems, 34:1982219835, 2021. 1, 8 [9] Ying Fan, Olivia Watkins, Yuqing Du, Hao Liu, Moonkyung Ryu, Craig Boutilier, Pieter Abbeel, Mohammad Ghavamzadeh, Kangwook Lee, and Kimin Lee. Dpok: reinforcement learning for fine-tuning text-to-image diffusion models. In Proceedings of the 37th International Conference on Neural Information Processing Systems, pages 7985879885, 2023. 1, 9 [10] Team GLM, :, Aohan Zeng, Bin Xu, Bowen Wang, Chenhui Zhang, Da Yin, Dan Zhang, Diego Rojas, Guanyu Feng, Hanlin Zhao, Hanyu Lai, Hao Yu, Hongning Wang, Jiadai Sun, Jiajie Zhang, Jiale Cheng, Jiayi Gui, Jie Tang, Jing Zhang, Jingyu Sun, Juanzi Li, Lei Zhao, Lindong Wu, Lucen Zhong, Mingdao Liu, Minlie Huang, Peng Zhang, Qinkai Zheng, Rui Lu, Shuaiqi Duan, Shudan Zhang, Shulin Cao, Shuxun Yang, Weng Lam Tam, Wenyi Zhao, Xiao Liu, Xiao Xia, Xiaohan Zhang, Xiaotao Gu, Xin Lv, Xinghan Liu, Xinyi Liu, Xinyue Yang, Xixuan Song, Xunkai Zhang, Yifan An, Yifan Xu, Yilin Niu, Yuantao Yang, Yueyan Li, Yushi Bai, Yuxiao Dong, Zehan Qi, Zhaoyu Wang, Zhen Yang, Zhengxiao Du, Zhenyu Hou, and Zihan Wang. Chatglm: family of large language models from glm-130b to glm-4 all tools, 2024. 12, 21 [11] Xuan He, Dongfu Jiang, Ge Zhang, Max Ku, Achint Soni, Sherman Siu, Haonan Chen, Abhranil Chandra, Ziyan Jiang, Aaran Arulraj, et al. Videoscore: Building automatic metrics to simulate fine-grained human feedback for video generation. In Proceedings of the 2024 Conference on Empirical Methods in Natural Language Processing, pages 21052123, 2024. 1, 3, [12] Martin Heusel, Hubert Ramsauer, Thomas Unterthiner, Bernhard Nessler, and Sepp Hochreiter. Gans trained by two time-scale update rule converge to local nash equilibrium. Advances in neural information processing systems, 30, 2017. 8 [13] Jonathan Ho, Ajay Jain, and Pieter Abbeel. Denoising diffusion probabilistic models. Advances in neural information processing systems, 33:68406851, 2020. 4 [14] Jonathan Ho, William Chan, Chitwan Saharia, Jay Whang, Ruiqi Gao, Alexey Gritsenko, Diederik Kingma, Ben Imagen Poole, Mohammad Norouzi, David Fleet, et al. video: High definition video generation with diffusion models. arXiv preprint arXiv:2210.02303, 2022. 1, 8 9 [15] Wenyi Hong, Ming Ding, Wendi Zheng, Xinghan Liu, and Jie Tang. Cogvideo: Large-scale pretraining for text-to-video generation via transformers. arXiv preprint arXiv:2205.15868, 2022. 1, 8 [16] Wenyi Hong, Weihan Wang, Ming Ding, Wenmeng Yu, Qingsong Lv, Yan Wang, Yean Cheng, Shiyu Huang, Junhui Ji, Zhao Xue, et al. Cogvlm2: Visual language modarXiv preprint els for image and video understanding. arXiv:2408.16500, 2024. 3, 5, 6, [17] Ziqi Huang, Yinan He, Jiashuo Yu, Fan Zhang, Chenyang Si, Yuming Jiang, Yuanhan Zhang, Tianxing Wu, Qingyang Jin, Nattapol Chanpaisit, et al. Vbench: Comprehensive benchmark suite for video generative models. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 2180721818, 2024. 1, 8 [18] Dongfu Jiang, Max Ku, Tianle Li, Yuansheng Ni, Shizhuo Sun, Rongqi Fan, and Wenhu Chen. Genai arena: An open evaluation platform for generative models. arXiv preprint arXiv:2406.04485, 2024. 6 [19] Diederik Kingma, Tim Salimans, Ben Poole, and Jonathan Ho. Variational diffusion models. Advances in neural information processing systems, 34:2169621707, 2021. 4 [20] Yuval Kirstain, Adam Polyak, Uriel Singer, Shahbuland Matiana, Joe Penna, and Omer Levy. Pick-a-pic: An open dataset of user preferences for text-to-image generation. Advances in Neural Information Processing Systems, 36: 3665236663, 2023. 1, 3, 5, 6, 8, 12, 21 [21] Feng Li, Renrui Zhang, Hao Zhang, Yuanhan Zhang, Bo Li, Wei Li, Zejun Ma, and Chunyuan Li. Llava-next-interleave: Tackling multi-image, video, and 3d in large multimodal models. arXiv preprint arXiv:2407.07895, 2024. 6, 8 [22] Chin-Yew Lin. ROUGE: package for automatic evaluation of summaries. In Text Summarization Branches Out, pages 7481, Barcelona, Spain, 2004. Association for Computational Linguistics. 21 [23] Zhiqiu Lin, Deepak Pathak, Baiqi Li, Jiayao Li, Xide Xia, Graham Neubig, Pengchuan Zhang, and Deva Ramanan. Evaluating text-to-visual generation with image-to-text generation. In European Conference on Computer Vision, pages 366384. Springer, 2025. 6, 8, 13 [24] Haotian Liu, Chunyuan Li, Qingyang Wu, and Yong Jae Lee. Visual instruction tuning, 2023. 6, 8 [25] Reiichiro Nakano, Jacob Hilton, Suchir Balaji, Jeff Wu, Long Ouyang, Christina Kim, Christopher Hesse, Shantanu Jain, Vineet Kosaraju, William Saunders, et al. Webgpt: Browser-assisted question-answering with human feedback. arXiv preprint arXiv:2112.09332, 2021. 8 [26] Long Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida, Carroll Wainwright, Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, et al. Training language models to follow instructions with human feedback. Advances in Neural Information Processing Systems, 35: 2773027744, 2022. 1, 8 [27] Dustin Podell, Zion English, Kyle Lacey, Andreas Blattmann, Tim Dockhorn, Jonas Muller, Joe Penna, and Improving latent diffusion modRobin Rombach. Sdxl: arXiv preprint els for high-resolution image synthesis. arXiv:2307.01952, 2023. 1, 5, 8 [28] Mihir Prabhudesai, Russell Mendonca, Zheyang Qin, Katerina Fragkiadaki, and Deepak Pathak. Video diffusion alignment via reward gradients. arXiv preprint arXiv:2407.08737, 2024. 1 [29] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, et al. Learning transferable visual models from natural language supervision. In International conference on machine learning, pages 87488763. PMLR, 2021. 7, [30] Aditya Ramesh, Mikhail Pavlov, Gabriel Goh, Scott Gray, Chelsea Voss, Alec Radford, Mark Chen, and Ilya Sutskever. Zero-shot text-to-image generation. In International conference on machine learning, pages 88218831. Pmlr, 2021. 1, 8 [31] Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Bjorn Ommer. High-resolution image synthesis with latent diffusion models. In 2022 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pages 1067410685. IEEE Computer Society, 2022. [32] Chitwan Saharia, William Chan, Saurabh Saxena, Lala Li, Jay Whang, Emily Denton, Kamyar Ghasemipour, Raphael Gontijo Lopes, Burcu Karagol Ayan, Tim Salimans, et al. Photorealistic text-to-image diffusion models with deep language understanding. Advances in neural information processing systems, 35:3647936494, 2022. 1, 7, 8 [33] Christoph Schuhmann, Romain Beaumont, Richard Vencu, Cade Gordon, Ross Wightman, Mehdi Cherti, Theo Coombes, Aarush Katta, Clayton Mullis, Mitchell Wortsman, et al. Laion-5b: An open large-scale dataset for training next generation image-text models. In Thirty-sixth Conference on Neural Information Processing Systems Datasets and Benchmarks Track, 2022. 7 [34] Jascha Sohl-Dickstein, Eric Weiss, Niru Maheswaranathan, and Surya Ganguli. Deep unsupervised learning using In International confernonequilibrium thermodynamics. ence on machine learning, pages 22562265. PMLR, 2015. 4 [35] Yang Song, Jascha Sohl-Dickstein, Diederik Kingma, Abhishek Kumar, Stefano Ermon, and Ben Poole. Score-based generative modeling through stochastic differential equations. arXiv preprint arXiv:2011.13456, 2020. [36] Yang Song, Conor Durkan, Iain Murray, and Stefano Ermon. Maximum likelihood training of score-based diffusion models. Advances in neural information processing systems, 34: 14151428, 2021. 4 [37] Nisan Stiennon, Long Ouyang, Jeffrey Wu, Daniel Ziegler, Ryan Lowe, Chelsea Voss, Alec Radford, Dario Amodei, and Paul Christiano. Learning to summarize with human feedback. Advances in Neural Information Processing Systems, 33:30083021, 2020. 8 [38] Gemini Team, Petko Georgiev, Ving Ian Lei, Ryan Burnell, Libin Bai, Anmol Gulati, Garrett Tanzer, Damien Vincent, Zhufeng Pan, Shibo Wang, et al. Gemini 1.5: Unlocking multimodal understanding across millions of tokens of context. arXiv preprint arXiv:2403.05530, 2024. 6, 8 10 and Linda Ruth Petzold. Gpt-4v (ision) as generalarXiv preprint ist evaluator for vision-language tasks. arXiv:2311.01361, 2023. 8 [51] Zangwei Zheng, Xiangyu Peng, Tianji Yang, Chenhui Shen, Shenggui Li, Hongxin Liu, Yukun Zhou, Tianyi Li, and Yang You. Open-sora: Democratizing efficient video production for all, 2024. 1, 3, 5, 8, [39] Ruben Villegas, Mohammad Babaeizadeh, Pieter-Jan Kindermans, Hernan Moraldo, Han Zhang, Mohammad Taghi Saffar, Santiago Castro, Julius Kunze, and Dumitru Erhan. Phenaki: Variable length video generation from open domain textual descriptions. In International Conference on Learning Representations, 2022. 1, 8 [40] Bram Wallace, Meihua Dang, Rafael Rafailov, Linqi Zhou, Aaron Lou, Senthil Purushwalkam, Stefano Ermon, Caiming Xiong, Shafiq Joty, and Nikhil Naik. Diffusion model alignment using direct preference optimization. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 82288238, 2024. 1, 4, 5, 9, 23 [41] Wenhao Wang and Yi Yang. Vidprom: million-scale real prompt-gallery dataset for text-to-video diffusion models. arXiv preprint arXiv:2403.06098, 2024. 5, 7, 21 [42] Weihan Wang, Qingsong Lv, Wenmeng Yu, Wenyi Hong, Ji Qi, Yan Wang, Junhui Ji, Zhuoyi Yang, Lei Zhao, Xixuan Song, et al. Cogvlm: Visual expert for pretrained language models. arXiv preprint arXiv:2311.03079, 2023. 8 [43] Hao Wu, Jiayuan Mao, Yufeng Zhang, Yuning Jiang, Lei Li, Weiwei Sun, and Wei-Ying Ma. Unified visual-semantic embeddings: Bridging vision and language with structured In 2019 IEEE/CVF Conference meaning representations. on Computer Vision and Pattern Recognition (CVPR), pages 66026611, 2019. 21 [44] Xiaoshi Wu, Yiming Hao, Keqiang Sun, Yixiong Chen, Feng Zhu, Rui Zhao, and Hongsheng Li. Human preference score v2: solid benchmark for evaluating human preferences of text-to-image synthesis. arXiv preprint arXiv:2306.09341, 2023. 1, 3, 5, 6, 8, [45] Xiaoshi Wu, Yiming Hao, Manyuan Zhang, Keqiang Sun, Zhaoyang Huang, Guanglu Song, Yu Liu, and Hongsheng Li. Deep reward supervisions for tuning text-to-image diffusion models. arXiv preprint arXiv:2405.00760, 2024. 1, 9 [46] Jiazheng Xu, Xiao Liu, Yuchen Wu, Yuxuan Tong, Qinkai Li, Ming Ding, Jie Tang, and Yuxiao Dong. Imagereward: learning and evaluating human preferences for text-to-image In Proceedings of the 37th International Congeneration. ference on Neural Information Processing Systems, pages 1590315935, 2023. 1, 3, 5, 6, 8, 9, 12, 21 [47] Zhuoyi Yang, Jiayan Teng, Wendi Zheng, Ming Ding, Shiyu Huang, Jiazheng Xu, Yuanming Yang, Wenyi Hong, Xiaohan Zhang, Guanyu Feng, et al. Cogvideox: Text-to-video diffusion models with an expert transformer. arXiv preprint arXiv:2408.06072, 2024. 1, 3, 5, 7, 8, 12 [48] Hangjie Yuan, Shiwei Zhang, Xiang Wang, Yujie Wei, Tao Feng, Yining Pan, Yingya Zhang, Ziwei Liu, Samuel Alinstructing video difbanie, and Dong Ni. Instructvideo: In Proceedings of fusion models with human feedback. the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 64636474, 2024. 1 [49] Jiacheng Zhang, Jie Wu, Yuxi Ren, Xin Xia, Huafeng Kuang, Pan Xie, Jiashi Li, Xuefeng Xiao, Min Zheng, Lean Fu, and Guanbin Li. Unifl: Improve stable diffusion via unified feedback learning, 2024. 21 [50] Xinlu Zhang, Yujie Lu, Weizhi Wang, An Yan, Jun Yan, Lianke Qin, Heng Wang, Xifeng Yan, William Yang Wang, VisionReward: Fine-Grained Multi-Dimensional Human Preference Learning for Image and Video Generation"
        },
        {
            "title": "Appendix",
            "content": "1. Details of Annotation and Dataset 1.1. Details of Annotation Pipeline Details of Data Preparation. To ensure the diversity of our annotation, we collect our annotated data from various sources. For images, we select 3 popular datasets: ImageRewardDB [46], HPDv2 [44], and Pick-a-Pic [20]. We sample 16k images from each dataset for annotation (with the first two sampling 4k prompts each associated with 4 images, and Pick-a-Pic sampling 8k prompts each associated with 2 images), totaling 48k annotated images. For videos, we conduct random sampling from the VidProM dataset and use ChatGLM [10] for data cleaning (prompt Cf. Tab. 14), leading to 10k prompts. Then we use CogVideoX [47], VideoCrafter2 [4] and OpenSora [51] to generate 30k videos, sample from Panda-70M [5] to get 3k real videos, leading to 33k videos for annotation. Annotation Design. In the annotation process, we design comprehensive dimensional system and divide each dimension into several sub-dimensions. For each sub-dimension, we set options that vary gradually in degree. Annotators are required to label each sub-dimension for every image. Tab. 15 and Tab. 16 show the annotation taxonomy of images, while Tab. 17 and Tab. 18 show for videos. After the annotation is completed, we map the options of each subdimension into checklist questions, which are then used for model training. 1.2. Statistics of Annotation Result Fig. 6 is the statistical results of the labeled data for images and videos. When compiling the statistics, higher labels indicate better performance for the image or video sub-dimension, while label of 0 indicates neutrality. For video data, the original labels only had positive values and the neutral value was inconsistent. Hence, neutral value was determined for each sub-dimension, and the original labels were adjusted by subtracting this neutral value to make 0 represent neutrality. In sub-dimensions such as Background, Face, and Hand, there might be cases where these elements are not present in the image or video. In such instances, Not Contain is treated as separate category for statistical purposes. There are two main characteristics to note. For most sub-dimensions, the distribution of options roughly follows normal distribution, with the majority being ordinary, and the quantities of instances with ex-"
        },
        {
            "title": "SYSTEM Assume you are a model responsible for",
            "content": "refining and polishing English expressions. You will receive an English prompt that may contain abbreviations or non-standard expressions. Your task is to standardize the expressions, and your output must be in pure English without any non-English characters. If the prompt is fragmented or difficult to understand, discard it by outputting F. Your output must strictly follow the format: each sentence should be on single line, either as the rewritten prompt or standalone F. Here is the prompt you have received: [[PROMPT]] Soft rays of light through the many different types of trees inside forest, sunrise, misty, photorealistic, ground level, -neg &quot;no large bodies of water&quot; -ar 16:9 4K, -ar 16:9 The soft rays of light filter through the myriad types of trees within the forest at sunrise, creating misty, photorealistic scene from ground level. Exclude any large bodies of water. The aspect ratio should be 16:9 in 4K resolution. Aspect ratio: 16:9."
        },
        {
            "title": "OUTPUT",
            "content": "Table 14. Prompt template and example for prompt cleaning. treme characteristics, either very good or very bad, are reduced. To assist the model in learning the features of each sub-dimension, we can impose quantitative limit on the predominant options. Certain sub-dimensions, such as the presence of hands, require mask when predicting human preferences. This means that the sub-dimension should only be evaluated when the image indicates the presence of hands. We also annotate the sub-dimensions that require mask and record the relevant counts. 2. More Analysis on VisionReward VisionReward comprises two steps: visual judgment and linear regression. (a) Text-to-image (b) Text-to-video Figure 6. Annotation statistics of different sub-dimensions. Algorithm 2 Iterative Regression with Weight Masking Input: Dataset of human preferences = {(Xi, Xj , y)}, where each pair (Xi, Xj ) represents feature vectors of binary responses and {0, 1} is the human preference label 1: Initialization: Initialize linear weights = [w1, . . . , wn] 2: Initialize convergence criterion diff 3: while diff > ϵ do wold 4: for each (Xi, Xj , y) in do 5: 6: 7: 8: 9: 10: Mask negative weights (w > 0) diff wold 11: Output: Trained weights X Xi Xj ˆy σ(XT w) wLoss = (ˆy y)X αwLoss Visual Judgment Process. As the options for each subdimension are progressive, for any given option corresponding to judgment question in the checklist, samples with an option greater than or equal to the given one are considered positive examples for the judgment question, whereas samples with an option less than the given one are considered negative. To balance the number of positive and negative examples for each binary question, we screen out any excess positive and negative examples for each question, ensuring that the number of positive and negative examples used for training is balanced. For alignment, we use different methods on images and videos. For images, we use VQAScore [23] as an alignment judgment. For videos, we train five levels of judgment for VisionReward. Weight and Accuracy of Checklist. We curate separate test sets of images and videos from outside the training set to evaluate the accuracy of judgment questions. The test set comprises 1,209 images and 1,000 videos, respectively. We report the accuracy of judgment questions (Cf. Tab. 19 and Tab. 20 for text-to-image, Tab. 21 and Tab. 22 for textto-video). As reference, we specifically record the linear weights obtained from linear regression on human preference data as well as the Spearman rank correlation coefficient between human preference and the results of each judgment question. Weight Masking. In the linear regression step, we learn the correlation between human preferences and the results of visual judgment. In our design, if the result of the visual judgment is yes, human preference improves. We examine the correlation between human preference and each judgment result, and the numerical results indicate positive correlation. However, in linear regression, we observe that some coefficients corresponding to the judgment results were negative. This is because there are correlations among the judgment results themselves. To enhance the robustness of the regression outcomes, we employ an iterative masking algorithm during the regression."
        },
        {
            "title": "Dimension",
            "content": "Sub-dimension"
        },
        {
            "title": "Lighting Aesthetic",
            "content": "symmetrical ordinary asymmetrical coordinated ordinary uncoordinated prominent ordinary prominent very rich rich ordinary monotonous empty beautiful somewhat beautiful ordinary no background very clear clear ordinary blurry completely blurry bright ordinary dark beautiful colors ordinary colors ugly colors very distinct distinct ordinary no lighting very beautiful beautiful ordinary no lighting Is the image symmetrical? Does the image avoid asymmetry? Are the objects well-coordinated? Does the image avoid poorly coordinated objects? Is the main subject prominent? Does the image avoid an unclear main subject? Is the image very rich? Is the image rich? Is the image not monotonous? Is the image not empty? Is the background beautiful? Is the background somewhat beautiful? Is there background? Is the image very clear? Is the image clear? Does the image avoid being blurry? Does the image avoid being completely blurry? Are the colors bright? Are the colors not dark? Are the colors beautiful? Are the colors not ugly? Is the lighting and shadow very distinct? Is the lighting and shadow distinct? Is there lighting and shadow? Are the lighting and shadows very beautiful? Are the lighting and shadows beautiful? Is there lighting and shadow? Table 15. Annotation taxonomy and checklist details for text-to-image evaluation. (part 1) 14 Dimension Sub-dimension Option Checklist Fidelity Detail realism Fidelity Detail refinement Fidelity Body Fidelity Face Fidelity Hands Safety & Emotion Emotion Safety & Emotion Safety realistic neutral unrealistic very unrealistic greatly unrealistic very refined refined ordinary rough very rough indistinguishable fragmented no errors neutral some errors obvious errors serious errors no human figure very beautiful beautiful normal some errors serious errors no human face perfect mostly correct minor errors obvious errors serious errors no human hands very positive positive ordinary negative very negative safe neutral potentially harmful harmful very harmful Are the image details realistic? Do the image details avoid being unrealistic? Do the image details avoid being very unrealistic? Do the image details avoid being greatly unrealistic? Are the image details very exquisite? Are the image details exquisite? Do the image details avoid being coarse? Do the image details avoid being very coarse? Does the image avoid being hard to recognize? Does the image avoid being fragmented? Is the human body in the image completely correct? Does the human body in the image avoid errors? Does the human body in the image avoid obvious errors? Does the human body in the image avoid serious errors? Is there human body in the image? Is the human face very beautiful? Is the human face beautiful? Does the human face avoid errors? Does the human face avoid serious errors? Is there human face in the image? Are the human hands perfect? Are the human hands essentially correct? Do the human hands avoid obvious errors? Do the human hands avoid serious errors? Are there human hands in the image? Can the image evoke very positive emotional response? Can the image evoke positive emotional response? Does the image avoid evoking negative emotional response? Does the image avoid evoking very negative emotional response? Is the image completely safe? Is the image harmless? Does the image avoid obvious harmfulness? Does the image avoid serious harmfulness? Table 16. Annotation taxonomy and checklist details for text-to-image evaluation. (part 2) 15 Dimension Sub-dimension Option Checklist Alignment Alignment Composition Composition Quality Color Quality Lighting Accurate Quality Lighting Aes Quality Clear Fidelity Detail Refinement Fidelity Movement Reality Fidelity Letters Safety Safety meet 100% Does the video meet all the requirements stated in the text [[prompt]]? meet 80%-100% Does the video meet most of the requirements stated in the text [[prompt]]? Does the video meet some of the requirements stated in the text [[prompt]]? meet 60%-80% meet 40%-60% Does the video not completely fail to meet the requirements stated in the text [[prompt]]? meet 0-40% good normal bad very beautiful beautiful normal unattractive very unattractive good normal bad no lighting very good good normal bad Is the composition aesthetically pleasing? Does the composition have no obvious flaws? Are the colors exceptionally beautiful? Are the colors beautiful? Are the colors not unattractive? Are the colors not significantly unattractive? Is the lighting perfectly accurate? Does the lighting have no obvious errors? Is there any lighting present? Is the lighting exceptionally beautiful? Is the lighting beautiful? Is the lighting not unattractive? very clear clear normal blurry completely blurry Is it very clear? Is it clear? Is it not blurry? Is it not completely blurry? very refined refined normal rough very rough good normal bad good normal bad no letter 100% safe 80%-100% safe 60%-80% safe 40%-60% safe 0-40% safe Are the details very refined? Are the details refined? Are the details not rough? Are the details not significantly rough? Is the objects movement completely realistic? Does the objects movement have no obvious realism issues? Are all the letters correct? Do the letters have no obvious errors? Are there any letters present? Is the video content safe? Is the video content definitely free of harmful material? Does the video content contain no harmful material? Does the video content contain no extremely harmful material? Table 17. Annotation taxonomy and checklist details for text-to-video evaluation. (part 1) 16 Dimension Sub-dimension Stability Movement smoothness Stability Image quality stability Stability Focus Stability Camera movement Stability Camera stability Preservation Shape at beginning Preservation Shape throughout Dynamic Object Motion dynamic Dynamic Camera motion dynamic Option good normal bad very stable stable normal unstable very unstable good normal bad good normal bad stable normal unstable completely accurate no errors not chaotic flawed perfectly maintained no issues normal not chaotic flawed highly dynamic dynamic normal not static static highly dynamic dynamic not minimal not static static Checklist Is the smoothness of the objects movement good? Does the smoothness of the objects movement have no obvious issues? Is the image quality very stable? Is the image quality stable? Is the image quality not unstable? Is the image quality free of noticeable instability? Is the focus aesthetically pleasing? Does the focus have no obvious flaws? Is the camera movement aesthetically pleasing? Does the camera movement have no obvious flaws? Is the camera stable? Is the camera not unstable? Is the shape of the object at the beginning of the video completely accurate? Does the shape of the object at the beginning have no obvious errors? Is the shape of the object at the beginning not chaotic? Is the shape of the object perfectly maintained throughout the video? Does the shape of the object have no obvious issues throughout the video? Does the shape of the object generally have no major issues throughout the video? Is the shape of the object not chaotic throughout the video? Is the objects motion highly dynamic? Is the objects motion dynamic? Is the objects motion not minimal? Is the objects motion not static? Is the camera motion highly dynamic? Is the camera motion dynamic? Is the camera motion not minimal? Is the camera motion not static? Physics Physics law no obvious violations Does it have no obvious violations of the laws of physics? full compliance partial compliance Does it fully comply with the laws of physics? Does it partially comply with the laws of physics? physical world non-compliance Is the video content part of the physical world? Table 18. Annotation taxonomy and checklist details for text-to-video evaluation. (part 2) 17 ID"
        },
        {
            "title": "Checklist",
            "content": "1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 Is there human body in the image? Is there human face in the image? Are there human hands in the image? Is the image symmetrical? Does the image avoid asymmetry? Are the objects well-coordinated? Does the image avoid poorly coordinated objects? Is the main subject prominent? Does the image avoid an unclear main subject? Is the image very rich? Is the image rich? Is the image not monotonous? Is the image not empty? Is the background beautiful? Is the background somewhat beautiful? Is there background? Is the image very clear? Is the image clear? Does the image avoid being blurry? Does the image avoid being completely blurry? Are the colors bright? Are the colors not dark? Are the colors beautiful? Are the colors not ugly? Is the lighting and shadow very distinct? Is the lighting and shadow distinct? Is there lighting and shadow? Are the lighting and shadows very beautiful? Are the lighting and shadows beautiful? Can the image evoke very positive emotional response? Can the image evoke positive emotional response? Does the image avoid evoking negative emotional response? Does the image avoid evoking very negative emotional response? Are the image details very exquisite? Are the image details exquisite? Do the image details avoid being coarse? Do the image details avoid being very coarse? Does the image avoid being hard to recognize? Does the image avoid being fragmented? Are the image details realistic?"
        },
        {
            "title": "Acc",
            "content": "93.13 96.20 93.30 79.98 71.30 58.31 68.24 86.27 77.75 80.40 65.84 77.01 99.67 72.70 67.26 84.86 63.85 62.03 88.92 97.11 63.69 82.88 65.84 74.77 75.45 58.37 75.93 80.47 71.99 82.63 63.94 76.01 91.56 74.03 71.79 68.73 84.62 87.34 85.36 63.85 ρ"
        },
        {
            "title": "Weight",
            "content": "0.090 0.110 0.022 0.104 0.236 0.138 0.204 0.210 0.258 0.084 0.138 0.271 0.205 -0.019 0.021 0.079 0.111 0.170 0.284 0.282 0.098 0.141 0.115 0.232 -0.043 0.035 0.108 -0.055 -0.026 0.068 0.117 0.179 0.117 0.078 0.091 0.215 0.247 0.267 0.288 0.099 mask mask mask 0.069 0.102 0.000 0.000 0.131 0.070 0.056 0.044 0.211 0.583 0.000 0.000 mask 0.051 0.068 0.065 0.032 0.076 0.077 0.000 0.042 0.000 0.000 mask 0.000 0.000 0.051 0.000 0.000 0.000 0.010 0.000 0.000 0.000 0.017 0.115 0.000 Table 19. Accuracy, spearman correlation, and linear weights of VisionReward in text-to-image. (Part 1) 18 ID 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58"
        },
        {
            "title": "Checklist",
            "content": "Do the image details avoid being unrealistic? Do the image details avoid being very unrealistic? Do the image details avoid being greatly unrealistic? Is the human body in the image completely correct? Does the human body in the image avoid errors? Does the human body in the image avoid obvious errors? Does the human body in the image avoid serious errors? Is the human face very beautiful? Is the human face beautiful? Does the human face avoid errors? Does the human face avoid serious errors? Are the human hands perfect? Are the human hands essentially correct? Do the human hands avoid obvious errors? Do the human hands avoid serious errors? Is the image completely safe? Is the image harmless? Does the image avoid obvious harmfulness? Does the image avoid serious harmfulness? Does the image show [[prompt]]?"
        },
        {
            "title": "Acc",
            "content": "63.94 74.19 83.62 61.31 59.02 82.57 90.83 65.50 56.88 57.61 91.56 90.18 25.84 37.98 77.26 78.74 86.44 92.39 92.80 - ρ"
        },
        {
            "title": "Weight",
            "content": "0.140 0.156 0.177 0.063 0.129 0.135 0.121 -0.046 -0.006 0.113 0.132 -0.015 0.059 0.066 0.048 0.118 0.106 0.109 0.092 0.297 0.000 0.000 0.000 0.082 0.000 0.055 0.030 0.000 0.000 0.031 0.077 0.072 0.000 0.000 0.000 0.000 0.000 0.012 0.015 2.354 Table 20. Accuracy, spearman correlation, and linear weights of VisionReward in text-to-image. (Part 2) ID"
        },
        {
            "title": "Checklist",
            "content": "1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 Does the video meet all the requirements stated in the text [[prompt]]? Does the video meet most of the requirements stated in the text [[prompt]]? Does the video meet some of the requirements stated in the text [[prompt]]? Does the video not completely fail to meet the requirements stated in the text [[prompt]]? Is the composition aesthetically pleasing? Does the composition have no obvious flaws? Is the focus aesthetically pleasing? Does the focus have no obvious flaws? Is the camera movement aesthetically pleasing? Does the camera movement have no obvious flaws? Are the colors exceptionally beautiful? Are the colors beautiful? Are the colors not unattractive? Are the colors not significantly unattractive? Is the lighting perfectly accurate? Does the lighting have no obvious errors? Is there any lighting present? Is the lighting exceptionally beautiful? Is the lighting beautiful? Is the lighting not unattractive?"
        },
        {
            "title": "Acc",
            "content": "69.5 72.9 72.9 78.7 50.8 90.4 49.8 91.6 76.2 97.3 46.5 50.1 82.2 88.6 51.9 86.2 87.8 65.1 55.8 83.5 ρ"
        },
        {
            "title": "Weight",
            "content": "0.315 0.303 0.281 0.320 0.263 0.239 0.232 0.246 0.012 0.142 0.214 0.217 0.225 0.202 0.346 0.259 0.215 0.212 0.240 0.280 0.954 0.252 0.000 1.142 0.035 0.025 0.000 0.000 0.000 0.126 0.000 0.000 0.000 0.032 0.163 0.217 0.020 0.136 0.096 0.155 Table 21. Accuracy, spearman correlation, and linear weights of VisionReward in text-to-video. (Part 1) 19 ID"
        },
        {
            "title": "Checklist",
            "content": "21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 Is the shape of the object at the beginning of the video completely accurate? Does the shape of the object at the beginning have no obvious errors? Is the shape of the object at the beginning not chaotic? Is the shape of the object perfectly maintained throughout the video? Does the shape of the object have no obvious issues throughout the video? Does the shape of the object generally have no major issues throughout the video? Is the shape of the object not chaotic throughout the video? Is the objects motion highly dynamic? Is the objects motion dynamic? Is the objects motion not minimal? Is the objects motion not static? Is the camera motion highly dynamic? Is the camera motion dynamic? Is the camera motion not minimal? Is the camera motion not static? Is the smoothness of the objects movement very good? Does the smoothness of the objects movement have no obvious issues? Is the objects movement completely realistic? Does the objects movement have no obvious realism issues? Is it very clear? Is it clear? Is it not blurry? Is it not completely blurry? Is the image quality very stable? Is the image quality stable? Is the image quality not unstable? Is the image quality free of noticeable instability? Is the camera very stable? Is the camera not unstable? Are the details very refined? Are the details relatively refined? Are the details not rough? Are the details not significantly rough? Are all the letters correct? Do the letters have no obvious errors? Are there any letters present? Does it fully comply with the laws of physics? Does it partially comply with the laws of physics? Does it have no obvious violations of the laws of physics? Is the video content part of the physical world? Is the video content safe? Is the video content definitely free of harmful material? Does the video content contain no harmful material? Does the video content contain no extremely harmful material?"
        },
        {
            "title": "Acc",
            "content": "63.0 76.3 91.3 54.2 68.8 84.5 93.5 78.0 69.0 71.2 66.5 86.9 80.6 72.1 58.1 59.8 61.6 66.8 69.2 52.1 51.0 81.8 93.1 43.1 61.2 79.0 87.6 54.2 83.5 73.0 62.3 74.2 89.2 87.3 86.8 89.7 36.6 66.7 77.4 86.6 92.8 94.3 97.7 100.0 ρ"
        },
        {
            "title": "Weight",
            "content": "0.292 0.274 0.256 0.300 0.267 0.259 0.240 -0.079 -0.024 -0.009 -0.014 -0.054 -0.062 -0.061 -0.059 0.263 0.139 0.338 0.235 0.261 0.290 0.271 0.226 0.313 0.294 0.277 0.247 0.197 0.267 0.324 0.331 0.302 0.271 0.114 0.115 0.104 0.254 0.248 0.231 0.231 0.000 0.000 0.000 0.000 0.129 0.099 0.188 0.184 0.000 0.000 0.264 0.000 0.000 0.000 0.000 0.112 0.000 0.052 0.000 0.026 0.000 0.439 0.000 0.000 0.000 0.000 0.000 0.269 0.000 0.000 0.000 0.000 0.000 0.429 0.000 0.008 0.128 0.058 0.000 0.145 0.000 0.000 0.000 0.394 0.000 0.000 0.000 0.000 Table 22. Accuracy, spearman correlation, and linear weights of VisionReward in text-to-video. (Part 2) 20 3. More Details of MonetBench Image Video Type Ratio Count Type Ratio Count Image-MonetBench Construction. We first establish our dataset foundation by collecting 4,038 seed prompts through strategic sampling from established datasets (1,000 from ImageRewardDB [46], 1,000 from HPDv2 [46], and 2,038 from Pick-a-Pic [20]). Through systematic analysis of these prompts, we identify nine fundamental visual elements as content categories and twelve distinct aspects of generation complexity as challenge categories, maintaining the categorical distributions observed in the source datasets. Video-MonetBench Construction. For video prompt evaluation, we initially sample 20,000 prompts from the VproM [41] dataset, which are filtered to 13,342 valid entries after removing duplicates and invalid content. Our video classification system comprises seven content categories reflecting different video scenarios, and thirteen challenge categories capturing various technical and creative aspects of video generation. Prompt Generation and Filtering. To ensure benchmark quality and diversity, we employ ChatGLM [10] to generate 1,000 new prompts for each benchmark following the established category distributions. Each generated prompt undergoes three-stage filtering process: (1) Rouge-L [22] similarity checking for textual diversity, (2) semantic filtering with cosine similarity threshold of 0.9, and (3) proportional sampling to maintain the intended category distributions. The resulting benchmark achieves balanced coverage across all categories while maintaining high standards of prompt diversity and quality. This carefully crafted multidimensional design enables comprehensive evaluation of visual reward models across both fundamental content types and various generation challenges. For experimental efficiency, we provide condensed version by randomly sampling 500 prompts from each benchmark while preserving the categorical distribution. Unreal Style & Format Fine-grained Detail Color Famous Character History & Culture Normal Writing Complex Combo Famous Places Positional Counting 8 8 8 4 4 4 2 1 1 1 1 Style Special Effects 187 187 Material/Texture Emotional Expr. 186 93 Color/Tone 93 World Knowledge 93 46 World Knowledge Spatial Relat. 23 Camera Move. 23 Surreal 23 Logical Consist. 23 Temporal Speed 23 Text 13 8 7 7 5 5 4 4 4 3 2 1 1 465 292 249 261 192 183 192 136 153 108 116 66 46 Table 24. Challenge Categories for Image and Video prompt in the dataset across content and challenge dimensions. We then calculated the proportions of different classification labels for content and challenges. The content and challenge categories and their respective examples are summarized in Tables 23 and 24. Based on these proportions, we used ChatGLM to construct Benchmark prompts (all prompts were generated by ChatGLM, not directly sampled from the dataset). During the construction process, we specified the investigation direction and randomly sampled four seed prompts from the categorized prompts to generate new, higher-quality prompts with ChatGLM. This synthesis approach produced two benchmark datasets, containing 1,000 and 1,007 meticulously crafted prompts, respectively, preserving the statistical characteristics of the original data. The final datasets provide balanced and comprehensive coverage of content and challenge categories, as detailed in Tables 23 and 24. Table 25 lists the specific content and challenge categories with detailed descriptions and example prompts, providing clear understanding of the datasets composition. The structured methodology ensures the datasets diversity and alignment with real-world visual generation requirements, enabling nuanced benchmarking of visual models. 4. More Details and Results of MPO Image Video Type Ratio Count Type Ratio Count 4.1. Details of Prompt for MPO People Objects Animals Architecture Others Landscape Vehicles Plants Food 8 4 4 4 2 2 2 1 1 286 143 143 143 72 72 71 35 Story Human Activity Artificial Scene Natural Scenes Animal Activity Physical Phenomena Other 5 4 3 3 2 1 1 265 212 159 159 106 53 53 Table 23. Content Categories for Image and Video Details of Classification Proportions. After completing the design of the comprehensive two-dimensional classification framework, we utilized ChatGLM to categorize each Prompt Filtering. We employ the prompt filtering approach proposed by Zhang et al. [49] to curate our dataset. This strategy comprises two pivotal steps: Semantic-Based Filtering: Utilizing an existing scene graph parser [43], we evaluate the semantic richness of prompts by analyzing the number of subjective and objective relationships. Prompts with fewer than one meaningful relationship are filtered out to reduce noise in the dataset. Cosine Similarity-Based Selection: Following the semantic filtering process, we apply cosine similaritybased iterative selection mechanism. By maintaining 21 Categorie Description Example Prompt Human Activity Animal Activity Natural Scenes Artificial Scenes Content Descriptions about daily human activities, sports, performing arts, and professional skills. Descriptions about wild animals, domestic pets, and interactions between animals. family enjoying picnic in park, children playing soccer. group of dolphins jumping out of the water. Descriptions about weather changes, geological events, and astronomical phenomena. thunderstorm with lightning striking the ground. Descriptions about cityscapes, interiors of buildings, vehicles, and industrial production. bustling city street with traffic and pedestrians. Physical Phenomena Descriptions about physical occurrences like candle burning, ice melting, glass breaking, and explosions. glass shattering in slow motion. Story Other Style Color/Tone Camera Movement Special Effects Material/Texture Surreal Temporal Speed Spatial Relationships World Knowledge Logical Consistency Descriptions about coherent narratives based on story or fantasy rather than single scene or activity. Alice, young girl, falls down rabbit hole into wonderland full of fantastical creatures and adventures. Descriptions about various contents that do not fit into the other specified categories. Various clips of miscellaneous activities not fitting into other categories. Challenge Descriptions about artistic styles such as realistic, cyberpunk, and animated. futuristic city with neon lights and flying cars, portrayed in cyberpunk style. Descriptions about color schemes like warm tones, cool tones, monochrome, and high saturation. Descriptions about different camera movements, including fixed, panning, zooming, tracking, and aerial shots. serene landscape in warm, golden tones during sunset. drone shot capturing birds eye view of mountain range. Descriptions about special effects such as particle effects, lighting effects, and transitions. Fireworks exploding with sparkling particle effects. Descriptions about materials and textures like metal, wood, glass, and fabric. Close-up shot of rain droplets on glass window. Descriptions about dreamlike, fantastical, or non-realistic elements. dreamlike scene with floating islands in the sky. Descriptions about different speeds, including slow motion, normal speed, fast motion, and time reversal. Descriptions about the spatial arrangement of objects, their sizes, occlusions, and perspectives. Descriptions about physical famous landmarks, historical events, and renowned personalities. laws, Descriptions about ensuring logical relationships among events, timelines, and spatial layouts. Slow-motion capture of hummingbird in flight. house of cards being built, showing each layers spatial arrangement. documentary about the pyramids of Egypt. mystery story where clues are pieced together logically. Emotional Expression Descriptions about expressions of emotions such as joy, sorrow, fear, and surprise. close-up of person expressing joy after receiving good news. Text Descriptions about incorporating textual elements dynamically within the footage. An animated title sequence with dynamic text effects. Table 25. Video classification standards with example prompts. 22 maximum similarity threshold of 0.8 between any two prompts, we ensure dataset diversity and effectively eliminate redundant entries. Dataset Curation. We apply this filtering strategy to the combined training sets from the Pick-a-Pick and HPS datasets, which initially comprise 166,475 prompts. Following comprehensive filtering, we retain 63,165 highquality prompts for subsequent Multiobjective Preference Optimization (MPO) training. 4.2. More Results of MPO Training Curve. Fig. 7 shows the variation of the dimensional scores during the MPO process with respect to the number of training samples. The results demonstrate that the MPO method enables the model to avoid trade-offs during training, thereby achieving simultaneous improvements across various sub-dimensions. In contrast, the DPO [40] method fails to achieve this level of concurrent enhancement. Case Study. Fig. 8 shows MPO cases for text-to-image, while Fig. 9 and Fig. 10 show MPO cases for text-to-video. MPO fine-tuned model surpasses the original model in multiple aspects and also outperforms other scoring methods. 23 (a) Overall Score (b) Composition Score (c) Fidelity Score (d) Alignment Score (e) Quality Score (f) Safety & Emotion Score Figure 7. Variation of dimensional scores during the MPO process with respect to the number of training samples. 24 Figure 8. Qualitative result of MPO in text-to-image. 25 Figure 9. Qualitative result of MPO in text-to-video. 26 Figure 10. Qualitative result of MPO in text-to-video."
        }
    ],
    "affiliations": [
        "Tsinghua University",
        "Zhipu AI"
    ]
}