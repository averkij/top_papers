{
    "paper_title": "Condition Errors Refinement in Autoregressive Image Generation with Diffusion Loss",
    "authors": [
        "Yucheng Zhou",
        "Hao Li",
        "Jianbing Shen"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Recent studies have explored autoregressive models for image generation, with promising results, and have combined diffusion models with autoregressive frameworks to optimize image generation via diffusion losses. In this study, we present a theoretical analysis of diffusion and autoregressive models with diffusion loss, highlighting the latter's advantages. We present a theoretical comparison of conditional diffusion and autoregressive diffusion with diffusion loss, demonstrating that patch denoising optimization in autoregressive models effectively mitigates condition errors and leads to a stable condition distribution. Our analysis also reveals that autoregressive condition generation refines the condition, causing the condition error influence to decay exponentially. In addition, we introduce a novel condition refinement approach based on Optimal Transport (OT) theory to address ``condition inconsistency''. We theoretically demonstrate that formulating condition refinement as a Wasserstein Gradient Flow ensures convergence toward the ideal condition distribution, effectively mitigating condition inconsistency. Experiments demonstrate the superiority of our method over diffusion and autoregressive models with diffusion loss methods."
        },
        {
            "title": "Start",
            "content": "6 2 0 2 2 ] . e [ 1 2 2 0 7 0 . 2 0 6 2 : r Published as conference paper at ICLR 2026 CONDITION ERRORS REFINEMENT IN AUTOREGRESSIVE IMAGE GENERATION WITH DIFFUSION LOSS Yucheng Zhou, Hao Li, Jianbing Shen(cid:66) SKL-IOTSC, CIS, University of Macau yucheng.zhou@connect.um.edu.mo"
        },
        {
            "title": "ABSTRACT",
            "content": "Recent studies have explored autoregressive models for image generation, with promising results, and have combined diffusion models with autoregressive frameworks to optimize image generation via diffusion losses. In this study, we present theoretical analysis of diffusion and autoregressive models with diffusion loss, highlighting the latters advantages. We present theoretical comparison of conditional diffusion and autoregressive diffusion with diffusion loss, demonstrating that patch denoising optimization in autoregressive models effectively mitigates condition errors and leads to stable condition distribution. Our analysis also reveals that autoregressive condition generation refines the condition, causing the condition error influence to decay exponentially. In addition, we introduce novel condition refinement approach based on Optimal Transport (OT) theory to address condition inconsistency. We theoretically demonstrate that formulating condition refinement as Wasserstein Gradient Flow ensures convergence toward the ideal condition distribution, effectively mitigating condition inconsistency. Experiments demonstrate the superiority of our method over diffusion and autoregressive models with diffusion loss methods."
        },
        {
            "title": "INTRODUCTION",
            "content": "Diffusion models have demonstrated remarkable performance in image generation and have been widely adopted across various visual generative tasks (Wei et al., 2024; Rombach et al., 2022; Saharia et al., 2022). Recently, due to the impressive reasoning capabilities exhibited by large language models (LLMs), autoregressive modeling has garnered significant attention. Consequently, some studies are exploring autoregressive frameworks for image and video generation, aiming to integrate them with LLMs to build more powerful multimodal models (Sun et al., 2024b). Recent advancements in autoregressive image generation have shown performance comparable to diffusion models (Sun et al., 2024a; Tian et al., 2024; Zhou et al., 2025a). However, most autoregressive image generation methods rely on Vector Quantized Variational Autoencoders (VQ-VAEs (Rombach et al., 2022)) to encode visual content into discrete tokens for next-token prediction modeling. (Li et al., 2024a) indicate that VQ-based image generation is sensitive to gradient approximation strategies and suffers from quantization errors, and propose diffusion loss for autoregressive image generation, effectively pursuing autoregressive image generation without VQ. Nevertheless, comparative analysis between Conditional diffusion modeling and autoregressive modeling with diffusion loss remains underexplored. In this study, we investigate the differences between autoregressive modeling with diffusion loss and conditional diffusion modeling. Firstly, we delve into the theoretical underpinnings of patch denoising optimization in autoregressive models for condition error correction. We theoretically prove that, under standard assumptions of Markov property and Gaussian noise in diffusion modeling, the iterative patch denoising approach leads to stable condition distribution. Furthermore, our analysis reveals the crucial behavior of the conditional probability gradient, showing its attenuation as the condition stabilizes. Our theoretical exploration demonstrates that patch denoising in autoregressive Equal Contribution. (cid:66)Corresponding Author. 1 Published as conference paper at ICLR modeling effectively mitigates condition errors and consequently contributes to improved conditional generation quality in diffusion modeling. In addition, we theoretically demonstrate that the sequence of condition variables generated by an autoregressive process effectively refines the condition, leading to reduction in the gradient norm of the conditional probability distribution. Specifically, we demonstrate that the influence of the condition on the outcome, quantified by the gradient norm, decays exponentially towards stationary value as the autoregressive iteration progresses. Building upon these theoretical insights, we further analyze the issue of condition inconsistency in autoregressive condition generation, demonstrating how extraneous information accumulates and hinders optimal patch generation. To address this, we introduce novel condition refinement approach grounded in Optimal Transport (OT) theory. We theoretically prove that formulating condition refinement as Wasserstein Gradient Flow leads to convergence towards the ideal condition distribution, effectively mitigating condition inconsistency and ultimately enhancing the quality of patch generation within diffusion models. In the experiments, we compare our method against other diffusion and autoregressive models with diffusion loss on ImageNet (Li et al., 2024b). Results show the superiority of our method over these competitors. We also analyze the denoising process to demonstrate the effectiveness of our method in condition refinement. Our main contributions and findings are as follows: We theoretically prove that patch denoising optimization in autoregressive models mitigates condition errors and elucidates the attenuation behavior of the conditional probability gradient as the condition stabilizes. We theoretically establish the efficacy of autoregressive condition refinement, quantifying the exponential decay of the conditions influence on the outcome as autoregressive iteration progresses to stationary value. We propose condition refinement method based on Optimal Transport theory, and theoretically prove that formulating it as Wasserstein Gradient Flow ensures convergence towards the ideal condition distribution. Experiments demonstrate our methods superiority over other competitors. Extensive analysis shows the effectiveness of our method in condition refinement."
        },
        {
            "title": "2 PRELIMINARIES",
            "content": "Diffusion Modeling. Diffusion models are generative frameworks that consist of forward process. The forward (diffusion) process is Markov chain that transforms data x0 into Gaussian noise xT through sequence of Gaussian transitions: q(x1:T x0) = (cid:89) t=1 q(xtxt1), q(xtxt1) = (xt; (cid:112)1 βtxt1, βtI), (1) t=1 is predefined variance schedule with 0 < β1 < < βT < 1. The reverse (denoising) where βt process reconstructs x0 from xT via: pθ(x0:T ) = p(xT ) (cid:89) t= pθ(xt1xt), pθ(xt1xt) = (xt1; µθ(xt, t), Σθ(xt, t)), (2) where µθ and Σθ are predicted by neural network. Since the true posterior q(xt1xt) is intractable, it is approximated using q(xt1xt, x0) during training. The model is trained to maximize data likelihood, which is approximated via variational lower bound. This can be reformulated as score-matching problem, e.g., Ext pt(xt) (cid:2)xt log pt(xt) sθ(xt, t)2(cid:3) . (3) Autoregressive Modeling. Autoregressive (AR) models are generative frameworks that sequentially predict each element in data sequence by conditioning on all preceding elements. These models 2 Published as conference paper at ICLR 2026 assume that each data point xi depends only on the prior points x<i = {x1, x2, . . . , xi1}. The conditional and joint probabilities can be expressed as: p(x) = p(x1, x2, . . . , xn) = (cid:89) i=1 p(xix<i) (4) The generation process starts from x1 and proceeds sequentially to xn, with each step conditioned on all previously generated elements. Related work is in Appendix A."
        },
        {
            "title": "3.1 DIFFERENCE OF DIFFUSION MODELS",
            "content": "Diffusion models demonstrate exceptional capabilities in generating high-quality visual content. Recently, autoregressive modeling integrated with diffusion loss has shown significant potential in image generation. We will elucidate the differences between standard conditional diffusion modeling and autoregressive modeling with diffusion loss. Conditional Diffusion Modeling. is conditioned on single, static condition c. This can be formally expressed as: In traditional conditional diffusion models, the reverse process xt1 p(xt1xt, c), (5) where represents global condition that influences every step of the denoising trajectory. When dealing with images, we can extend this to an individual patch xi: (6) where represents the number of patches. Each patch is denoised based on the same shared condition c, irrespective of its position within the image. xi,t1 p(xi,t1xj,t, c), {1, . . . , n}, Autoregressive Modeling with Diffusion Loss. Autoregressive modeling with diffusion loss allows the condition to evolve autoregressively. Instead of fixed condition c, sequence of conditions {ci} depends on preceding conditions {c<i} including the initial condition c0, i.e., ct p(ctc<i, c0), (7) where c<i denotes all conditions up to time 1, i.e., {c0, c1, . . . , ci1}. For each patch generation xi, the reverse process is still denoising process but is guided by the dynamic condition ci, i.e., xi,t1 p(xi,t1xi,t, ci), (8) where ci represents the condition for i-th patch. After generating xi, it is passed as input to the autoregressive model along with the history of conditions {c<i+1}, enabling the prediction of the subsequent condition ci+1. 3.2 CONDITIONAL DENOISING MODEL ERROR DEFINITION Conditional Score Matching as an Upper Bound. Score matching is central to training diffusion models, and its loss is linked to the Wasserstein distance between generated and real data (Kwon et al., 2022). Conditional score matching refines this by incorporating conditioning. Understanding how conditional score matching relates to standard score matching is key to justifying its use. This section establishes that the standard score matching loss is upper-bounded by its conditional counterpart. This result supports the use of conditional score matching, suggesting it might lead to more controlled training process. Theorem 1 (Conditional Score Matching Upper Bound). The standard score matching loss is upper-bounded by the conditional score matching loss: Extpt(xt) xt log pt(xt) sθ(xt, t)2(cid:105) (9) (cid:104) Ecpc(c),xtpt(xtc) (cid:104) xt log pt(xtc) sθ(xt, t)2(cid:105) See Appendix for the proof, which uses the law of total probability and Jensens inequality. 3 Published as conference paper at ICLR 2026 The conditional score matching loss serves as an upper bound for the standard score matching loss. Consequently, minimizing the conditional score matching loss indirectly constrains the standard score matching loss from above. Error in Conditional Score Matching. To analyze the error in conditional score matching, we build upon the score matching error definition from (Li & Yan, 2024): ϵ2 score :="
        },
        {
            "title": "1\nT",
            "content": "T (cid:88) t=1 EXqt (cid:2)st(X) (X)2 2 (cid:3) . (10) To understand how conditioning affects the error structure, we need to decompose the conditional and unconditional score matching losses. Expanding these losses into their component terms allows us to identify and analyze the specific contributions of conditioning to the overall error. Lemma 1 (Expansion of Score Matching Loss). Expanding the square term in the score matching loss, we get: Extpt(xt) (cid:104) xt log pt(xt) sθ(xt, t)2(cid:105) (cid:104) = Extpt(xt) xt log pt(xt)2 + sθ(xt, t)2 2 sθ(xt, t), xt log pt(xt) (cid:105) . (11) Similarly, for conditional score matching: (cid:104) Ec,xtpc(c)pt(xtc) xt log pt(xtc) sθ(xt, t)2(cid:105) (cid:104) = Ec,xtpc(c)pt(xtc) xt log pt(xtc)2 + sθ(xt, t)2 2 sθ(xt, t), xt log pt(xtc) (cid:105) (12) This expansion separates the loss into terms related to the true score, the estimated score, and their interaction, facilitating more granular error analysis (Detailed Proof in Appendix D.1). Definition 1 (Conditional Error Term ϵc). To specifically measure the impact of conditioning on the true scores magnitude, we define the conditional error term ϵc as the change in the expected squared norm of the true score due to conditioning, relative to the unconditional case: ϵc = 1 (cid:88) t=1 Extpt(xt) (cid:104) Ecpt(cxt) (cid:104) xt log pt(xtc)2(cid:105) xt log pt(xt)2 (cid:105) (13) This term quantifies how much the expected squared norm of the true score changes when we move from unconditional to conditional score matching. positive ϵc would suggest that conditioning increases the magnitude of the true score, potentially indicating more complex or refined score function (Detailed Proof in Appendix D.2). Definition 2 (Simplified Conditional Error Term ϵc). For simpler metric focused purely on the magnitude of the conditional true score, we define the simplified conditional error term ϵc: ϵc = 1 (cid:88) t=1 Ecpc(c),xtpt(xtc) xt log pt(xtc)2(cid:105) (cid:104) (14) ϵc directly measures the expected squared norm of the conditional score. Analyzing ϵc, along with ϵc, will help understand the behavior of the true score in conditional settings (Detailed Proof in Appendix D.3). 3.3 CONDITIONAL CONTROL TERM ANALYSIS. We first investigate the uniqueness of the conditional control term under standard diffusion assumptions and Classifier-Free guidance. As described in Classifier-Free guidance (Ho & Salimans, 2022; Liu et al., 2023), the conditional reverse diffusion process for sampling is given by: p(xt1xt) = (xt1; µ(xt) + σ2 xt1 = µ(xt) + σ2 xt log p(cxt) + σtϵ, ϵ (0, 1) xt log p(cxt), σ2I) (15) 4 Published as conference paper at ICLR 2026 This shows that conditional control introduces an additional term σ2 xt log p(cxt) to the mean of the reverse process, compared to the unconditional diffusion sampling. To understand the impact of this conditional term, we define (ci) = xt log pt(xtci)2. We hypothesize that the difference between the expected value of (ci) and the expected value of the unconditional score norm isolates the contribution of this conditional control term. This is formalized in the following lemma: Lemma 2 (Uniqueness of Conditional Control Term). Under standard diffusion assumptions and Classifier-Free guidance, the difference between the expected squared norm of the conditional score function and the unconditional score function isolates the contribution of the conditional control term. Let (ci) := xt log pt(xtci)2. Then, E[f (ci)] Ext log pt(xt)2 = Eσ xt log p(cxt)2 (16) where the expectation is taken over xt pt(xt) and pc(c). This lemma indicates that (ci), through its expected difference with the unconditional score norm, precisely captures the impact of the conditional guidance term σ2 xt log p(cxt) in the diffusion denoising process. (Proof in Appendix E) 3.4 CONDITION REFINEMENT THROUGH PATCH DENOISING. Building upon the observation that incorporating conditions can amplify errors in diffusion models, we propose an optimization strategy focused on refining the condition using patch-related corrections. Specifically, we introduce mechanism where information from each newly generated patch is propagated to the condition of the subsequent patch through an iterative update process, ci+1 = (ci). This autoregressive approach aims to refine the condition during the denoising process iteratively. To formalize our approach, we first establish the foundational assumptions under which our model operates. Assumption 1 (Markov Property Assumption). The reverse diffusion process adheres to the Markov property, where each state xt1 is conditionally dependent only on the current state xt. I). Assumption 2 (Gaussian Distribution Assumption). The conditional probability distribution pt(xt1xt) in the reverse diffusion process is assumed to be Gaussian, expressed as: pt(xt1xt) = (xt1; µ(xt), σ2 Assumption 3 (Small Variance Assumption). As the number of time steps becomes sufficiently large, the variance σ2 of the conditional distribution pt(xt1xt) is assumed to be sufficiently small, approaching zero as increases. Furthermore, for simplicity, we approximate the variance at each step to be equal and denote it as σ2. With the above assumptions, we model the patch refinement process as an iterative update to the condition: ci+1 = (ci), (17) where ci+1 represents the condition at the (i + 1)-th iteration, corresponding to the refinement based on the i-th patch. is diffusion function that governs the transition from the current condition state ci to the next state ci+1, encapsulating the information propagation from the generated patch to the subsequent condition. The index denotes the iteration step, analogous to discrete time steps. The condition update process forms discrete-time Markov chain, as the future state ci+1 depends only on the present state ci: (ci+1ci, ci1, . . . , c0) = (ci+1ci). From the Gaussian expansion norm in Equation equation 15, we observe that the probability distribution of xt is influenced by xt itself. Our primary goal is to understand the trajectory of the conditional probability gradient as the condition ci iteratively refines through the diffusion reverse process. Therefore, we proceed to analyze how the conditional probability gradient evolves with the iterations of ci within standard normal conditional distribution setting. Proposition 1 (Condition Refinement via Patch Denoising). In the diffusion denoising process, autoregressively refining the condition through patch-related corrections using the iterative update ci+1 = (ci) leads to improved conditional generation quality.(Detailed Proof in Appendix F) 5 Published as conference paper at ICLR"
        },
        {
            "title": "3.5 AUTOREGRESSIVE MODELING CAN REFINE CONDITION",
            "content": "An autoregressive process is defined as follows: ci+1 = (cid:88) j=0 ajcj + εi+1, (18) Assumption 4 (Basic Assumptions for Autoregressive Process). We provide set of standard assumptions: 1. (cid:80) i=0 ai < and supiN ai < 1, i.e., the sequence {an} is convergent. i=1 are independent and identically distributed, following (0, σ2). 2. {εi} 3. pt(xtci) has continuous second-order derivatives with respect to xt. 4. 2 xt pt(xtci) for some constant > 0 uniformly holds, i.e., is bounded. 5. (X , ) is separable complete metric space. Lemma 3 (Markov Property (Meyn & Tweedie, 2012) (Bellet, 2006)). Under Assumption 4, by defining the state vector ci = (ci, ci1, . . . , cip+1), the sequence {ci}iN forms strong Markov chain. Specifically: 1. The transition probability kernel (ci+1 ci) on the augmented state space satisfies the Feller property. 2. There exists unique invariant probability measure π P(X p) such that πP = π, i.e. (cid:90) (Ac)π(dc) = π(A), B(X p) (19) 3. There exist constants > 0 and ρ (0, 1) such that for any initial distribution µ P(X p): µP πTV Cρnµ πTV, (20) where TV denotes the total variation norm, and denotes the n-step transition probability kernel. In particular, for any N0, we have: N0 L(cn) πTV Cρn (21) where L(cn) denotes the distribution of cn. Proof can be found in Appendix J. Lemma 4 (Regularity of Conditional Probability (Durrett, 1996)). Under Assumption 4, there exist constants δ, > 0 such that: 1. pt(xtci) δ for all (xt, ci) . 2. xtpt(xtc1) xtpt(xtc2) Lc1 c2 for all xt, c1, c2 . Proof can be found in Appendix I. Lemma 5 (Bounded Derivative Theorem). On fixed bounded closed interval [a, b], if the second derivative is bounded, then there exist constants M1, M2 > 0 such that: 1. supxt xt pt(xtc) < M1, i.e., the first derivative is bounded. 2. supxt pt(xtc) < M2, i.e., the original function is bounded. Proof can be found in Appendix M. Theorem 2 (Descent of Gradient Norm in Autoregressive Process). Under Assumptions 4 and Lemmas 3, 4, 5, there exist constants > 0 and β (0, 1) such that for any xt and N0: xt log pt(xtci) βi + (22) where is constant representing the stationary gradient norm (Proof in Appendix G). 6 Published as conference paper at ICLR"
        },
        {
            "title": "4 AUTOREGRESSIVE CONDITION OPTIMIZATION",
            "content": "Although autoregressive methods provide contextual information, they inevitably accumulate extraneous noise, leading to condition inconsistency. Why Optimal Transport? We employ Optimal Transport (OT) to rectify this distributional drift for three theoretical reasons: 1. Geometric Correction: Unlike overlap-based metrics (e.g., KL divergence), OT quantifies the geometric cost required to transform the noisy generated distribution back to the ideal one. 2. Least Action Principle: Formulating the refinement as Wasserstein Gradient Flow identifies the optimal path to eliminate inconsistency while preserving valid semantic information. 3. Convergence: The framework guarantees theoretical convergence to the stationary ideal distribution, effectively acting as mathematically grounded denoising step for the condition. The full algorithm is provided in Appendix L. 4.1 CONDITION INCONSISTENCY IN AUTOREGRESSIVE GENERATION The autoregressive condition generation process, as defined by Equation equation 7, sequentially constructs conditions, aiming to capture contextual dependencies. However, this sequential nature can lead to conditions that are not only influenced by relevant preceding patches but also by accumulated information that is extraneous to generating the current patch. This phenomenon, which we term condition inconsistency, arises because the autoregressive process, while capturing dependencies, does not inherently guarantee that each generated condition ci is optimally focused on information strictly necessary for the corresponding patch xi. Lemma 6 (Condition Information Inconsistency and Extraneous Information Accumulation). Let ci = Φθ(ci1) + Γθ(ϵi) be the autoregressively generated condition for patch xi, and (ci) be its projection onto the minimal sufficient information subspace (derived from x<i). The generated ci inherently contains an extraneous information component ηi = ci non-zero (i.e., E[ηi2 2] > 0), accumulating from (I πI . The actual conditional distribution p(xici) deviates from the ideal p(xic score xt log p(xtci) is perturbed from its optimal form under . = πI i Figure 1: The autoregressive model predicts an initial condition, which is processed by the OT Refinement module using sampled prior derived from Algorithm 1. The resulting refined condition then guides the Denoise MLP for latent generation. . This component ηi is generally )Φθ(ci1) and noise components outside ), and the conditional Proof. Let πI Rd denote the minimal sufficient information subspace for generating patch xi, and () be the orthogonal projection onto this subspace. The ideal condition satisfies: = πI (ci) where = span{fk(x<i)}K k=1 (23) for some basis functions {fk} encoding relevant dependencies from preceding patches x<i. The autoregressive condition generation follows Markov process: ci = Φθ(ci1) + Γθ(ϵi) (24) where Φθ : Rd Rd is the learned transition operator and Γθ modulates the noise injection. The extraneous information component ηi can be quantified through subspace decomposition: ηi = (I πI )ci = (cid:88) k=K+1 ci, vkvk (25) Published as conference paper at ICLR 2026 where {vk} forms an orthonormal basis for Rd with the first vectors spanning extraneous information ηi2 satisfies: (cid:104)(cid:13) (cid:13)(I πI + tr(ΓθΓ θ ) E[ηi2 2] = )Φθ(ci1)(cid:13) 2 (cid:13) (cid:105) . The ℓ2-norm of (26) The first term represents propagated extraneous information from previous conditions, while the second term quantifies newly introduced noise. For the denoising process Dt at timestep t, the conditional score function becomes perturbed: (27) xt log p(xtci) = xt log p(xtc ) (cid:125) (cid:124) + Jηici log p(xtci) (cid:123)(cid:122) (cid:125) Perturbation term (cid:123)(cid:122) Ideal score where Jηi is the Jacobian of the perturbation. The extraneous information induces an O(ηi2) deviation from the ideal denoising trajectory. The accumulated effect over patches yields total inconsistency: (cid:124) Etotal = (cid:88) i=1 [OTλ(p(xici), p(xic ))] (28) where OTλ denotes the Sinkhorn divergence with regularization parameter λ. This completes the proof of condition information inconsistency. 4.2 OPTIMAL TRANSPORT FOR CONDITION REFINEMENT VIA WASSERSTEIN GRADIENT FLOW Building upon the condition inconsistency analysis in Lemma 6, we present principled solution through optimal transport theory. Our approach establishes direct connections between the Wasserstein gradient flow framework and condition refinement in autoregressive generation. Proposition 2 (Optimal Transport as Wasserstein Gradient Flow). The condition refinement process can be formulated as Wasserstein gradient flow that minimizes: F(Pc) := 2 (29) where Pc denotes the ideal condition distribution and 1 represents the inverse process of information accumulation in Equation equation 24. The solution admits an implementable discrete-time scheme through JKO iterations (Jordan et al., 1998): 2 (Pc, Pc ) + λEcPc[c 1(x)2] (k+1) = arg min 2 2 (P, (k) ) + ηkF(P ) (30) Proof. Let P2(Rd) denote the space of probability measures with finite second moments. We consider the energy functional: F(P ) = 1 2 2 (P, Pc ) + λEcP [ϕ(c)] (31) where ϕ(c) = 1(x)2 encodes the inverse process regularization. The Wasserstein gradient flow tPt = W2F(Pt) can be discretized via the Jordan-Kinderlehrer-Otto (JKO) scheme: (k+1) = arg min (cid:110) 2 2 (P, (k)) + 2ηkF(P ) (cid:111) (32) Substituting our specific energy functional yields the update rule in Proposition 2. The first term maintains proximity to previous iterates while the second term drives the distribution toward both the ideal condition and inverse-process consistency. The optimal transport plan between (k) and (k+1) corresponds to the McCann interpolant: c(k+1) = c(k) ηk (cid:104) 2 (cid:105) 2 (, Pc )c(k) + λϕ(c(k)) Implementing this requires solving the regularized OT problem: inf γΓ(P (k),Pc ) E(c,c)[c c2] + ϵKL(γπ) (33) (34) where π is the independent coupling and ϵ controls entropy regularization. This leads to the Sinkhorn algorithm implementation described in Proposition 2. Published as conference paper at ICLR 2026 Theorem 3 (Convergence of Wasserstein Gradient Flow). Under the assumptions of Proposition 2, the Wasserstein gradient flow defined by the energy functional F(P ) converges to the ideal condition distribution Pc . Specifically, for any initial distribution (0) P2(Rd), the sequence of distributions {P (k) k=1 generated by the JKO scheme satisfies: } W2(P (k) , Pc ) ρkW2(P (0) , Pc ), (35) where ρ < 1 is the contraction rate determined by the regularization parameter λ and the step size ηk. Proof Sketch. The proof follows from the contractive properties of the Wasserstein gradient flow for convex energy functionals. By the JKO scheme and the regularization term λEcPc[c 1(x)2], the sequence {P (k) } forms Cauchy sequence in the Wasserstein space P2(Rd). The contraction rate ρ arises from the strong convexity of the energy functional F(P ) and Lipschitz continuity of the gradient flow. Remark 1. The inverse process regularization 1 directly counters the extraneous information accumulation characterized in Equation equation 26. By Theorem 3, the refinement ensures monotonic improvement in patch generation quality: E[OTλ(p(xic(k) ), p(xic ))] ρkE[OTλ(p(xic(0) ), p(xic ))] (36) where ρ < 1 quantifies the contraction rate of our OT-based refinement operator. This theorem ensures that the proposed Wasserstein gradient flow refinement process monotonically reduces the Wasserstein distance between the autoregressive condition distribution (k) and the ideal condition distribution Pc . The contraction rate ρ quantifies the refinement efficiency, with smaller values of ρ indicating faster convergence. c"
        },
        {
            "title": "5 EXPERIMENTS",
            "content": "5.1 EXPERIMENTAL SETTINGS Table 1: Comparison of different methods on various metrics on ImageNet 256256 conditional generation. Baseline (CDM) denotes baseline of conditional diffusion modeling. Method FID IS Pre. Rec. Our autoregressive model is directly based on GPT-XL, while the denoising module is implemented using the MAR-based denoising module. For the Variational Autoencoder (VAE) component, we use the KL-16 version of LDM (Rombach et al., 2022). Our experiments are conducted on the ImageNet dataset (Deng et al., 2009), with image resolutions set to 256 256. For evaluation, we adopt Frechet Inception Distance (FID) (Heusel et al., 2017), Inception Score (IS) (Salimans et al., 2016), as well as Precision and Recall metrics (Dhariwal & Nichol, 2021). During training, the noise schedule follows cosine shape and consists of 1000 steps. The learning rate is set to 1 105, with total of 400 epochs and batch size of 2048. The models are trained with 100-epoch linear learning rate warmup. We use an exponential moving average (EMA) in parameters with momentum of 0.9999. LDM-4 (Rombach et al., 2022) U-ViT-H/2-G (Bao et al., 2022) DiT-XL/2 (Peebles & Xie, 2023) DiffiT (Hatamizadeh et al., 2024) MDTv2-XL/2 (Gao et al., 2023) GIVT (Tschannen et al., 2024) MAR (Li et al., 2024a) De-MAR (Yao et al., 2025) RAR (Yu et al., 2025) Baseline (CDM) Baseline (AR) MAR (Li et al., 2024a) 247.7 263.9 278.2 276.5 314.7 - 303.7 305.8 306.9 259.6 282.6 303.7 0.87 0.81 0.83 0.80 0.79 0.84 0.81 0.83 0.80 0.81 0.80 0.81 3.60 2.29 2.27 1.73 1.58 3.35 1.55 1.47 1.50 3.26 2.02 1.55 0.48 0.62 0.57 0.62 0.65 0.53 0.62 0.62 0.62 0.58 0.59 0.62 Ours (AR) Ours (MAR) 317.6 324.2 0.82 0.81 0.60 0.63 1.52 1.31 5.2 PERFOMANCE COMPARISON Table 1 shows comparison of our method against state-of-the-art approaches on ImageNet 256 256 conditional generation. Our method achieves the best FID score of 1.52, outperforming MAR (Li et al., 2024a) (1.55), MDTv2-XL/2 (Gao et al., 2023) (1.58), and DiffiT (Hatamizadeh et al., 2024) (1.73). Based on MAR, it can further reach 1.31. In IS, our method also achieves the highest score. This demonstrates that our model produces samples with higher 9 Figure 2: Qualitative results on 256 256 ImageNet class-conditional generation. These images are generated by Ours. Published as conference paper at ICLR 2026 fidelity and better alignment with the real image distribution. For Precision and Recall, it attains 0.81 and 0.63, respectively, remaining competitive with other methods. Compared to the baseline, our model exhibits significant improvements across all evaluation metrics, highlighting the effectiveness of our approach. Qualitative results are shown in Figure 2."
        },
        {
            "title": "5.3 SCALABILITY ANALYSIS",
            "content": "To investigate the scalability and robustness of our proposed method, we conducted additional evaluations across varying model sizes and higher image resolutions. These experiments aim to verify whether the benefits of our Condition Refinement approach persist as the model capacity increases and the generation task becomes more challenging. Scalability across Model Sizes. We evaluated our method against the strong baseline MAR (Li et al., 2024a) on ImageNet 256 256 using three different model scales: 208M, 479M, and 943M parameters. As presented in Table 2, our method consistently outperforms MAR across all model sizes. Notably, the performance gap widens as the model size increases, suggesting that our autoregressive condition optimization effectively leverages larger capacities for superior generation quality. Table 2: Comparison of scalability across different model sizes on ImageNet 256 256. Our method consistently achieves lower FID and higher IS compared to MAR. Size Method FID IS 208M 479M 943M MAR (Li et al., 2024a) Ours MAR (Li et al., 2024a) Ours MAR (Li et al., 2024a) Ours 2.31 1.96 1.78 1.59 1.55 1.31 281.7 290.5 296.0 301.5 303.7 324. Table 3: Performance comparison on highresolution ImageNet 512 512. Method FID IS MAR (Li et al., 2024a) Ours 1.73 1. 279.9 302.3 High-Resolution Generation. To further assess the generalization capability of our method, we extended our evaluation to higher resolution setting on ImageNet 512 512 (using model size of approximately 481M parameters). Table 3 demonstrates the superiority of our approach in high-resolution synthesis. Our method achieves an FID of 1.58 compared to 1.73 for MAR, indicating that our OT-based condition refinement remains effective in mitigating inconsistencies even in higher-dimensional spaces. 5.4 CONDITION ERRORS ANALYSIS Figure 3 presents the denoising analysis, showing Signal-to-Noise Ratio (SNR) and Noise Intensity over time for both our method and baseline. The denoising process proceeds from right to left, with time steps decreasing as denoising progresses. Our method consistently achieves higher SNR, with widening gap in later stages. Similarly, the right panel shows the Noise Intensity, where both methods show reduction in noise as denoising progresses. Consistent with the SNR analysis, our method exhibits marginally lower Noise Intensity, especially in the earlier time steps, i.e., later stages of denoising. These results highlight the efficacy of our proposed OT-based refinement of conditional distributions and effectively mitigate the potential inconsistencies introduced by purely autoregressive methods. Figure 3: Analysis of Signal-to-Noise Ratio (SNR, Left) and Noise Intensity (Right) during the denoising process of our method and the baseline. All analyses are computed in the image space after VAE decoding."
        },
        {
            "title": "6 CONCLUSION",
            "content": "In this work, our analysis shows that patch denoising in autoregressive models mitigates condition errors, stabilizing condition distribution and enhancing generation quality. Autoregressive condition generation further refines conditions, exponentially reducing error influence. To address condition inconsistency, we introduce refinement method based on Optimal Transport and prove that casting it as Wasserstein Gradient Flow ensures convergence. Experimental results and analysis show the effectiveness of our method. 10 Published as conference paper at ICLR"
        },
        {
            "title": "REFERENCES",
            "content": "Fan Bao, Chongxuan Li, Yue Cao, and Jun Zhu. All are worth words: vit backbone for scorebased diffusion models. CoRR, abs/2209.12152, 2022. doi: 10.48550/ARXIV.2209.12152. URL https://doi.org/10.48550/arXiv.2209.12152. Luc Rey Bellet. Ergodic Properties of Markov Processes, pp. 139. Springer Berlin Heidelberg, ISBN 978-3-540-33966-3. doi: 10.1007/3-540-33966-3 1. URL Berlin, Heidelberg, 2006. https://doi.org/10.1007/3-540-33966-3_1. Riccardo Corvi, Davide Cozzolino, Giada Zingarini, Giovanni Poggi, Koki Nagano, and Luisa Verdoliva. On the detection of synthetic images generated by diffusion models. In IEEE International Conference on Acoustics, Speech and Signal Processing ICASSP 2023, Rhodes Island, Greece, June 4-10, 2023, pp. 15. IEEE, 2023. doi: 10.1109/ICASSP49357.2023.10095167. URL https://doi.org/10.1109/ICASSP49357.2023.10095167. Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li Fei-Fei. Imagenet: large-scale hierarchical image database. In 2009 IEEE Computer Society Conference on Computer Vision and Pattern Recognition (CVPR 2009), 20-25 June 2009, Miami, Florida, USA, pp. 248255. IEEE Computer Society, 2009. doi: 10.1109/CVPR.2009.5206848. URL https://doi.org/10. 1109/CVPR.2009.5206848. Prafulla Dhariwal and Alexander Quinn Nichol. Diffusion models beat gans on image synthesis. In MarcAurelio Ranzato, Alina Beygelzimer, Yann N. Dauphin, Percy Liang, and Jennifer Wortman Vaughan (eds.), Advances in Neural Information Processing Systems 34: Annual Conference on Neural Information Processing Systems 2021, NeurIPS 2021, December 6-14, 2021, virtual, pp. 87808794, 2021. Richard Durrett. Probability: theory and examples. Duxbury Press, Belmont, CA, second edition, 1996. ISBN 0-534-24318-5. Shanghua Gao, Pan Zhou, Ming-Ming Cheng, and Shuicheng Yan. Masked diffusion transformer is strong image synthesizer. In IEEE/CVF International Conference on Computer Vision, ICCV 2023, Paris, France, October 1-6, 2023, pp. 2310723116. IEEE, 2023. doi: 10.1109/ICCV51070.2023. 02117. URL https://doi.org/10.1109/ICCV51070.2023.02117. Ian J. Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley, Sherjil Ozair, Aaron C. Courville, and Yoshua Bengio. Generative adversarial networks. Commun. ACM, 63(11): 139144, 2020. doi: 10.1145/3422622. URL https://doi.org/10.1145/3422622. Ali Hatamizadeh, Jiaming Song, Guilin Liu, Jan Kautz, and Arash Vahdat. Diffit: Diffusion vision transformers for image generation. In Ales Leonardis, Elisa Ricci, Stefan Roth, Olga Russakovsky, Torsten Sattler, and Gul Varol (eds.), Computer Vision - ECCV 2024 - 18th European Conference, Milan, Italy, September 29-October 4, 2024, Proceedings, Part VIII, volume 15066 of Lecture Notes in Computer Science, pp. 3755. Springer, 2024. doi: 10.1007/978-3-031-73242-3 3. URL https://doi.org/10.1007/978-3-031-73242-3_3. Martin Heusel, Hubert Ramsauer, Thomas Unterthiner, Bernhard Nessler, and Sepp Hochreiter. Gans trained by two time-scale update rule converge to local nash equilibrium. In Isabelle Guyon, Ulrike von Luxburg, Samy Bengio, Hanna M. Wallach, Rob Fergus, S. V. N. Vishwanathan, and Roman Garnett (eds.), Advances in Neural Information Processing Systems 30: Annual Conference on Neural Information Processing Systems 2017, December 4-9, 2017, Long Beach, CA, USA, pp. 66266637, 2017. Jonathan Ho and Tim Salimans. Classifier-free diffusion guidance. CoRR, abs/2207.12598, 2022. doi: 10.48550/ARXIV.2207.12598. URL https://doi.org/10.48550/arXiv.2207. 12598. Jonathan Ho, Chitwan Saharia, William Chan, David J. Fleet, Mohammad Norouzi, and Tim Salimans. Cascaded diffusion models for high fidelity image generation. J. Mach. Learn. Res., 23:47:147:33, 2022. URL https://jmlr.org/papers/v23/21-0635.html. 11 Published as conference paper at ICLR 2026 Richard Jordan, David Kinderlehrer, and Felix Otto. The variational formulation of the fokker planck equation. SIAM Journal on Mathematical Analysis, 29(1):117, 1998. doi: 10.1137/ S0036141096303359. URL https://doi.org/10.1137/S0036141096303359. Diederik P. Kingma and Max Welling. Auto-encoding variational bayes. In Yoshua Bengio and Yann LeCun (eds.), 2nd International Conference on Learning Representations, ICLR 2014, Banff, AB, Canada, April 14-16, 2014, Conference Track Proceedings, 2014. URL http: //arxiv.org/abs/1312.6114. Dohyun Kwon, Ying Fan, and Kangwook Lee. Score-based generative modeling secretly minimizes the wasserstein distance. In Sanmi Koyejo, S. Mohamed, A. Agarwal, Danielle Belgrave, K. Cho, and A. Oh (eds.), Advances in Neural Information Processing Systems 35: Annual Conference on Neural Information Processing Systems 2022, NeurIPS 2022, New Orleans, LA, USA, November 28 - December 9, 2022, 2022. Gen Li and Yuling Yan. Adapting to unknown low-dimensional structures in score-based diffusion models. CoRR, abs/2405.14861, 2024. doi: 10.48550/ARXIV.2405.14861. URL https: //doi.org/10.48550/arXiv.2405.14861. Tianhong Li, Yonglong Tian, He Li, Mingyang Deng, and Kaiming He. Autoregressive image generation without vector quantization. CoRR, abs/2406.11838, 2024a. doi: 10.48550/ARXIV. 2406.11838. URL https://doi.org/10.48550/arXiv.2406.11838. Xiang Li, Kai Qiu, Hao Chen, Jason Kuen, Jiuxiang Gu, Bhiksha Raj, and Zhe Lin. Imagefolder: Autoregressive image generation with folded tokens. CoRR, abs/2410.01756, 2024b. doi: 10. 48550/ARXIV.2410.01756. URL https://doi.org/10.48550/arXiv.2410.01756. Dongyang Liu, Shitian Zhao, Le Zhuo, Weifeng Lin, Yu Qiao, Hongsheng Li, and Peng Gao. Luminamgpt: Illuminate flexible photorealistic text-to-image generation with multimodal generative pretraining. CoRR, abs/2408.02657, 2024. doi: 10.48550/ARXIV.2408.02657. URL https: //doi.org/10.48550/arXiv.2408.02657. Xihui Liu, Dong Huk Park, Samaneh Azadi, Gong Zhang, Arman Chopikyan, Yuxiao Hu, Humphrey Shi, Anna Rohrbach, and Trevor Darrell. More control for free! image synthesis with semantic diffusion guidance. In IEEE/CVF Winter Conference on Applications of Computer Vision, WACV 2023, Waikoloa, HI, USA, January 2-7, 2023, pp. 289299. IEEE, 2023. doi: 10.1109/WACV56688. 2023.00037. URL https://doi.org/10.1109/WACV56688.2023.00037. Sean Meyn and Richard Tweedie. Markov chains and stochastic stability. Springer Science & Business Media, 2012. Alexander Quinn Nichol, Prafulla Dhariwal, Aditya Ramesh, Pranav Shyam, Pamela Mishkin, Bob McGrew, Ilya Sutskever, and Mark Chen. GLIDE: towards photorealistic image generation and editing with text-guided diffusion models. In Kamalika Chaudhuri, Stefanie Jegelka, Le Song, Csaba Szepesvari, Gang Niu, and Sivan Sabato (eds.), International Conference on Machine Learning, ICML 2022, 17-23 July 2022, Baltimore, Maryland, USA, volume 162 of Proceedings of Machine Learning Research, pp. 1678416804. PMLR, 2022. URL https://proceedings. mlr.press/v162/nichol22a.html. Yun Pang, Jiawei Mao, Libo He, Hong Lin, and Zhenping Qiang. An improved face image restoration method based on denoising diffusion probabilistic models. IEEE Access, 12:35813596, 2024. doi: 10.1109/ACCESS.2024.3349423. URL https://doi.org/10.1109/ACCESS.2024. 3349423. William Peebles and Saining Xie. Scalable diffusion models with transformers. In IEEE/CVF International Conference on Computer Vision, ICCV 2023, Paris, France, October 1-6, 2023, pp. 41724182. IEEE, 2023. doi: 10.1109/ICCV51070.2023.00387. URL https://doi.org/10. 1109/ICCV51070.2023.00387. Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Bjorn Ommer. Highresolution image synthesis with latent diffusion models. In IEEE/CVF Conference on Computer Vision and Pattern Recognition, CVPR 2022, New Orleans, LA, USA, June 18-24, 2022, pp. 10674 10685. IEEE, 2022. doi: 10.1109/CVPR52688.2022.01042. URL https://doi.org/10. 1109/CVPR52688.2022.01042. 12 Published as conference paper at ICLR 2026 Chitwan Saharia, William Chan, Huiwen Chang, Chris A. Lee, Jonathan Ho, Tim Salimans, David J. Fleet, and Mohammad Norouzi. Palette: Image-to-image diffusion models. In Munkhtsetseg Nandigjav, Niloy J. Mitra, and Aaron Hertzmann (eds.), SIGGRAPH 22: Special Interest Group on Computer Graphics and Interactive Techniques Conference, Vancouver, BC, Canada, August 7 - 11, 2022, pp. 15:115:10. ACM, 2022. doi: 10.1145/3528233.3530757. URL https: //doi.org/10.1145/3528233.3530757. Tim Salimans, Ian J. Goodfellow, Wojciech Zaremba, Vicki Cheung, Alec Radford, and Xi Chen. Improved techniques for training gans. In Daniel D. Lee, Masashi Sugiyama, Ulrike von Luxburg, Isabelle Guyon, and Roman Garnett (eds.), Advances in Neural Information Processing Systems 29: Annual Conference on Neural Information Processing Systems 2016, December 5-10, 2016, Barcelona, Spain, pp. 22262234, 2016. Peize Sun, Yi Jiang, Shoufa Chen, Shilong Zhang, Bingyue Peng, Ping Luo, and Zehuan Yuan. Autoregressive model beats diffusion: Llama for scalable image generation. CoRR, abs/2406.06525, 2024a. doi: 10.48550/ARXIV.2406.06525. URL https://doi.org/10.48550/arXiv. 2406.06525. Weigao Sun, Jiaxi Hu, Yucheng Zhou, Jusen Du, Disen Lan, Kexin Wang, Tong Zhu, Xiaoye Qu, Yu Zhang, Xiaoyu Mo, et al. Speed always wins: survey on efficient architectures for large language models. arXiv preprint arXiv:2508.09834, 2025. Yutao Sun, Hangbo Bao, Wenhui Wang, Zhiliang Peng, Li Dong, Shaohan Huang, Jianyong Wang, and Furu Wei. Multimodal latent language modeling with next-token diffusion. CoRR, abs/2412.08635, 2024b. doi: 10.48550/ARXIV.2412.08635. URL https://doi.org/10. 48550/arXiv.2412.08635. Keyu Tian, Yi Jiang, Zehuan Yuan, Bingyue Peng, and Liwei Wang. Visual autoregressive modeling: Scalable image generation via next-scale prediction. CoRR, abs/2404.02905, 2024. doi: 10.48550/ ARXIV.2404.02905. URL https://doi.org/10.48550/arXiv.2404.02905. Michael Tschannen, Cian Eastwood, and Fabian Mentzer. GIVT: generative infinite-vocabulary transformers. In Ales Leonardis, Elisa Ricci, Stefan Roth, Olga Russakovsky, Torsten Sattler, and Gul Varol (eds.), Computer Vision - ECCV 2024 - 18th European Conference, Milan, Italy, September 29-October 4, 2024, Proceedings, Part LVII, volume 15115 of Lecture Notes in Computer Science, pp. 292309. Springer, 2024. doi: 10.1007/978-3-031-72998-0 17. URL https://doi.org/10.1007/978-3-031-72998-0_17. Pascal Vincent. connection between score matching and denoising autoencoders. Neural Comput., 23(7):16611674, 2011. doi: 10.1162/NECO 00142. URL https://doi.org/10.1162/ NECO_a_00142. Rahmatulloh Daffa Izzuddin Wahid, Novanto Yudistira, Candra Dewi, Irawati Nurmala Sari, Dyanningrum Pradhikta, and Fatmawati. Prompt conditioned batik pattern generation using lora IEEE Access, 13:24362448, 2025. weighted diffusion model with classifier-free guidance. doi: 10.1109/ACCESS.2024.3523494. URL https://doi.org/10.1109/ACCESS.2024. 3523494. Xinlong Wang, Xiaosong Zhang, Zhengxiong Luo, Quan Sun, Yufeng Cui, Jinsheng Wang, Fan Zhang, Yueze Wang, Zhen Li, Qiying Yu, Yingli Zhao, Yulong Ao, Xuebin Min, Tao Li, Boya Wu, Bo Zhao, Bowen Zhang, Liangdong Wang, Guang Liu, Zheqi He, Xi Yang, Jingjing Liu, Yonghua Lin, Tiejun Huang, and Zhongyuan Wang. Emu3: Next-token prediction is all you need. CoRR, abs/2409.18869, 2024. doi: 10.48550/ARXIV.2409.18869. URL https://doi.org/ 10.48550/arXiv.2409.18869. Xiao-Li Wei, Chunxia Zhang, Hongtao Wang, Chengli Tan, Deng Xiong, Baisong Jiang, Jiangshe Zhang, and Sang-Woon Kim. Seismic data interpolation via denoising diffusion implicit models with coherence-corrected resampling. IEEE Trans. Geosci. Remote. Sens., 62:117, 2024. doi: 10. 1109/TGRS.2024.3485573. URL https://doi.org/10.1109/TGRS.2024.3485573. Jian Yang, Dacheng Yin, Yizhou Zhou, Fengyun Rao, Wei Zhai, Yang Cao, and Zheng-Jun Zha. MMAR: towards lossless multi-modal auto-regressive probabilistic modeling. CoRR, 13 Published as conference paper at ICLR 2026 abs/2410.10798, 2024a. doi: 10.48550/ARXIV.2410.10798. URL https://doi.org/10. 48550/arXiv.2410.10798. Shuohang Yang, Jian Gao, Jiayi Zhang, and Chao Xu. Wrapped phase denoising using denoising diffusion probabilistic models. IEEE Geosci. Remote. Sens. Lett., 21:15, 2024b. doi: 10.1109/ LGRS.2024.3405000. URL https://doi.org/10.1109/LGRS.2024.3405000. Ting Yao, Yehao Li, Yingwei Pan, Zhaofan Qiu, and Tao Mei. Denoising token prediction in masked autoregressive models. In Proceedings of the IEEE/CVF international conference on computer vision, pp. 1802418033, 2025. Qihang Yu, Ju He, Xueqing Deng, Xiaohui Shen, and Liang-Chieh Chen. Randomized autoregressive visual generation. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pp. 1843118441, 2025. Lvmin Zhang, Anyi Rao, and Maneesh Agrawala. Adding conditional control to text-to-image diffusion models. In IEEE/CVF International Conference on Computer Vision, ICCV 2023, Paris, France, October 1-6, 2023, pp. 38133824. IEEE, 2023. doi: 10.1109/ICCV51070.2023.00355. URL https://doi.org/10.1109/ICCV51070.2023.00355. Yucheng Zhou, Jihai Zhang, Guanjie Chen, Jianbing Shen, and Yu Cheng. Less is more: Vision representation compression for efficient video generation with large language models, 2024. Yucheng Zhou, Jiahao Yuan, and Qianning Wang. Draw all your imagine: holistic benchmark and agent framework for complex instruction-based image generation. arXiv preprint arXiv:2505.24787, 2025a. Yucheng Zhou, Huan Zheng, Dubing Chen, Hongji Yang, Wencheng Han, and Jianbing Shen. From medical llms to versatile medical agents: comprehensive survey, 2025b. 14 Published as conference paper at ICLR"
        },
        {
            "title": "A RELATED WORK",
            "content": "A.1 DIFFUSION MODEL Diffusion models have emerged as powerful generative framework, surpassing GANs (Goodfellow et al., 2020) and VAEs (Kingma & Welling, 2014) in stability and sample quality. DDPMs (Yang et al., 2024b) introduced noise-based training and reconstruction paradigm, later linked theoretically to Score Matching and DAEs (Vincent, 2011). However, early diffusion models suffered from Improved DDPMs (Pang et al., 2024) refined slow sampling due to numerous iterative steps. noise scheduling, while DDIMs (Wei et al., 2024) accelerated generation through non-Markovian formulation. LDMs (Rombach et al., 2022) further optimized efficiency by applying diffusion in lower-dimensional latent space. Diffusion models also exhibit theoretical advantages over GANs, notably their implicit minimization of the Wasserstein distance (Kwon et al., 2022), leading to better convergence and robustness. Enhancing conditional control remains key research focus: Classifier-Free Diffusion Guidance (Wahid et al., 2025) enables flexible conditioning without external classifiers, and structure-aware adaptations (Li & Yan, 2024) improve efficiency for structured data. Various applications extend their utility: Palette (Saharia et al., 2022) enhances image restoration, GLIDE (Nichol et al., 2022) improves text-guided synthesis, CDMs (Ho et al., 2022) refine images progressively, and ControlNet (Zhang et al., 2023) integrates structural conditions for enhanced controllability. Detecting diffusion-generated images is increasingly challenging, with studies like Schaefer et al. (Corvi et al., 2023) highlighting the need for robust detection methods. A.2 AUTOREGRESSIVE IMAGE GENERATION Autoregressive models, despite their effectiveness, face computational constraints due to sequential generation (Zhou et al., 2024; Sun et al., 2025; Zhou et al., 2025b). Optimization efforts focus on efficiency and scalability: LlamaGen (Sun et al., 2024a) leverages large-scale training to surpass diffusion models in quality and efficiency, while VAR (Tian et al., 2024) reduces inference latency via next-scale prediction. Spatial alignment strategies like ImageFolder (Li et al., 2024b) improve autoregressive modeling, and Emu3 (Wang et al., 2024) unifies token prediction across modalities. Expanding autoregression to multimodal tasks requires bridging discrete and continuous data representations. Lumina-mGPT (Liu et al., 2024) employs decoder-only Transformer for high-quality text-to-image synthesis, while MMAR (Yang et al., 2024a) models continuous tokens to enhance understanding and generation. Traditional vector quantization in autoregressive models is being reconsidered: VQ-free autoregression (Li et al., 2024a) introduces diffusion-based per-token probabilities for efficiency, and LatentLM (Sun et al., 2024b) integrates next-token diffusion for multimodal synthesis across image, speech, and text."
        },
        {
            "title": "B LIMITATIONS",
            "content": "While our research provides novel theoretical insights and algorithmic advancements, it is important to acknowledge certain limitations. Specifically, our experimental evaluation has not been conducted on large-scale models due to the substantial computational resources required for such validation. Instead, our focus has been on rigorous theoretical analysis and the development of scalable algorithms. Despite the absence of experiments on very large models, the generality and applicability of our method have been theoretically established, and our experiments on general settings support the soundness of the proposed approach. We believe future work can extend these evaluations to more resource-intensive settings to further verify empirical performance at scale. PROOF OF THEOREM 1 (CONDITIONAL SCORE MATCHING UPPER BOUND) We begin by establishing two foundational lemmas required for the proof. Lemma 7 (Bayes Theorem for Conditional Scores). For any measurable sets and xt, the posterior distribution satisfies: pt(cxt) = pc(c)pt(xtc) pt(xt) = pc(c)pt(xtc) (cid:82) pc(c)pt(xtc)dc 15 Published as conference paper at ICLR 2026 where the second equality explicitly shows the marginalization over c. Lemma 8 (Jensens Inequality for Convex Functions). For any convex function : Rd and random variable with finite expectation: (E[Y]) [f (Y)] Equality holds if and only if is affine linear on the support of Y, or is constant almost surely. Step-by-Step Proof: Using these lemmas, we proceed with the main proof. Step 1: Marginal-Conditional Decomposition. Express the marginal distribution through conditioning variables: pt(xt) = (cid:90) pc(c)pt(xtc)dc (Law of total probability) Differentiate both sides under the integral sign (valid under Dominated Convergence Theorem conditions): xtpt(xt) = (cid:90) pc(c)xtpt(xtc)dc Step 2: Score Function Representation. Using Lemma 7, decompose the marginal score: xtpt(xt) pt(xt) xt log pt(xt) = = = (cid:82) pc(c)xtpt(xtc)dc pt(xt) (cid:90) pc(c)pt(xtc) pt(xt) (cid:123)(cid:122) pt(cxt) (cid:124) (cid:125) xt log pt(xtc)dc where the critical step (line 3) applies Lemma 7 to identify the posterior distribution. = Ecpt(cxt) [xt log pt(xtc)] Step 3: Jensens Inequality Application. Substitute into the unconditional loss: Ext log pt sθ2 = Ext Ext Here we specifically apply Lemma 8 with: (cid:13) (cid:13) 2 (cid:13)Ecxt[ log pt(c)] sθ (cid:13) Ecxt log pt(c) sθ2 (by Lemma 8) (y) = sθ2 (convex since 2 is convex) = xt log pt(xtc) Step 4: Law of Total Expectation. Convert the nested expectation to joint expectation: Ecxt[] = Ec,xt[] = Ecpc Extpt(c)[] Ext Thus, we obtain the final inequality: Ext log pt sθ2 Ec,xt log pt(c) sθ2 Tightness Analysis. By Lemma 8, equality holds iff xt log pt(xtc) is constant c-a.s., which requires pt(xtc) = pt(xt) for all in the support of pc. This corresponds to statistical independence xt c."
        },
        {
            "title": "D DETAILED DERIVATIONS FOR CONDITIONAL SCORE MATCHING ANALYSIS",
            "content": "D.1 PROOF OF LEMMA 1 Following the score matching framework from (Vincent, 2011), we begin by expanding the squared norm in both unconditional and conditional score matching objectives. 16 Published as conference paper at ICLR 2026 Unconditional Case: For the unconditional score matching loss: xt log pt(xt) sθ(xt, t)2(cid:105) (cid:104) (cid:104) (cid:105) (xt log pt(xt) sθ(xt, t)) (xt log pt(xt) sθ(xt, t)) Extpt(xt) = Extpt(xt) (cid:104) = Ext xt log pt(xt)2 (cid:124) (cid:123)(cid:122) (cid:125) True score norm + sθ(xt, t)2 (cid:124) (cid:123)(cid:122) (cid:125) Learned score norm 2 sθ(xt, t), xt log pt(xt) (cid:125) (cid:123)(cid:122) Alignment term (cid:124) (cid:105) This follows directly from the identity b2 = a2 + b2 2ab. Conditional Case: For conditional score matching, expanding the L2 norm of the training error, we have: Ec,xtpc(c)pt(xtc) xt log pt(xtc) sθ(xt, t)2(cid:105) (cid:104) (cid:104) = Ec,xtpc(c)pt(xtc) (cid:105) xt log pt(xtc)2 + sθ(xt, t)2 2 sθ(xt, t), xt log pt(xtc) = Extpt(xt)Ecpt(cxt) (cid:104) = Extpt(xt) (cid:104) Ecpt(cxt) (cid:105) xt log pt(xtc)2 + sθ(xt, t)2 2 sθ(xt, t), xt log pt(xtc) xt log pt(xtc)2(cid:105) (cid:104) (cid:105) + sθ(xt, t)2 2Ecpt(cxt) sθ(xt, t), xt log pt(xtc) D.2 DERIVATION OF ERROR DIFFERENCE ϵc (DEFINITION 1) Loss Difference Analysis: Subtracting the unconditional loss from the conditional loss (after expansion): Ec,xt[Loss] Ext[Loss] xt log pt(xtc)2(cid:105) (cid:104) = Ext Ecxt + Ext sθ2 Ext sθ2 (cid:123)(cid:122) (cid:125) =0 (cid:124) Ext xt log pt(xt)2 2 (cid:0)Ext,cxtsθ, log pt(xtc) Extsθ, log pt(xt)(cid:1) (cid:125) (cid:123)(cid:122) Vanishes by tower property (cid:124) The cross-terms cancel due to the tower property of expectation: Ext Ecxtsθ, log pt(xtc) = Extsθ, Ecxt log pt(xtc) where we used the identity log pt(xt) = Ecxt log pt(xtc) from (Li & Yan, 2024). = Extsθ, log pt(xt) Final Error Expression: Thus, the difference reduces to: (cid:88) (cid:104) Ext Ecxt log pt(xtc)2 log pt(xt)2(cid:105) ϵc = 1 t=1 This quantifies the excess score energy induced by conditioning, similar to variance decomposition in probability theory. D.3 PROPERTIES OF ϵc (DEFINITION 2) From Definition 2, we can relate ϵc to ϵc using the law of total variance: ϵc = ϵc + 1 (cid:88) t=1 Ext log pt(xt)2 This decomposition reveals that ϵc contains both the intrinsic score energy from the unconditional distribution and the additional energy ϵc from conditioning. Published as conference paper at ICLR"
        },
        {
            "title": "E CONDITIONAL CONTROL TERM UNIQUENESS PROOF",
            "content": "Proof. We prove Lemma 2 through three key steps: Step 1: Bayesian Score Decomposition Using Bayes rule pt(xtc) = p(cxt)pt(xt) p(c) , we derive: xt log pt(xtc) = xt log p(cxt) + xt log pt(xt) xt log p(c) (cid:125) (cid:124) (cid:123)(cid:122) =0 = xt log p(cxt) + xt log pt(xt) Step 2: Cross-Term Cancellation The squared norm decomposes as: xt log pt(xtc)2 = xt log p(cxt)2 + xt log pt(xt)2 + 2xt log p(cxt), xt log pt(xt) Taking expectation over pt(xt) and pc(c): E[xt log p(cxt), xt log pt(xt)] = Ext [Ec [xt log p(cxt), xt log pt(xt) xt]] Ec[xt log p(cxt)] , xt log pt(xt) (cid:125) (cid:123)(cid:122) =0 where the inner expectation vanishes because Ec[xt log p(cxt)] = xt = Ext (cid:124) = 0 Ec[p(cxt)] = xt1 = 0. Step 3: Variance Propagation Combining results from Steps 1-2: ϵc = Ext log pt(xtc)2 Ext log pt(xt)2 = Eσ2 xt log p(cxt)2 The scaling factor σ2 Liu et al., 2023), where the conditional mean adjustment contains an explicit σ2 completes the proof that ϵc isolates the conditional control terms contribution. emerges from the reverse process parameterization in (Ho & Salimans, 2022; multiplier. This"
        },
        {
            "title": "F PROOF OF CONDITION REFINEMENT VIA PATCH DENOISING",
            "content": "(PROPOSITION 1) To understand the long-term behavior of the condition refinement process, we invoke the Markov Chain Stationary Theorem. The conditional probability density function p(xc) is then given by: aBy invoking (Meyn & Tweedie, 2012), we establish that as ci iterates, its distribution converges to stationary distribution. (The details of the lemma we used can be found in Appendix K.) Consequently, our analysis shifts to understanding how the gradient of the conditional probability evolves when ci follows normal distribution. To compute the gradient of the conditional probability density function p(xc) with respect to x, the joint distribution of and follows multivariate normal distribution: (cid:19) (cid:19)(cid:19) (cid:19) (cid:18)X (cid:18)(cid:18)µx µc (cid:18)σxx σxc σcc σxc , . The conditional probability density function p(xc) is: p(xc) = (cid:113) 1 2π(σxx σ2 xc σcc ) (cid:32) exp (x (µx + σxc σcc 2(σxx σ2 xc σcc ) (c µc)))2 (cid:33) Taking the logarithm, we obtain the log-likelihood function: log p(xc) = (cid:19) ) σ2 xc σcc (cid:18) log 2π(σxx 1 2 (x (µx + σxc σcc 2(σxx σ2 xc σcc ) (c µc)))2 18 Published as conference paper at ICLR Differentiating with respect to x, while ignoring constant terms, yields: log p(xc) = (µx + σxc σcc σxx σ2 xc σcc (c µc)) . The squared norm of this gradient is given by: log p(xc)2 = (cid:32) (µx + σxc σcc σxx σ2 xc σcc (c µc)) (cid:33)2 . The conditional mean and the conditional variance primarily influence the gradient behavior. The conditional mean is: µx + (c µc), σxc σcc where µc represents the mean of c. As iterates and reaches its stationary distribution, µc converges to constant, which we denote as µstable . Consequently, the conditional mean stabilizes to fixed value. The conditional variance σxx σ2 is determined by the covariance structure of the joint distribution. xc σcc Since this variance does not depend on in its stationary distribution, it remains unchanged. As reaches its stationary distribution, the deviation of from the conditional mean gradually diminishes while the variance remains constant. It leads to gradient magnitude decay, indicating attenuation of the conditional probability gradient over iterations. c"
        },
        {
            "title": "G PROOF OF DESCENT OF GRADIENT NORM IN AUTOREGRESSIVE PROCESS",
            "content": "(THEOREM 2) Proof. = Remark 2 (State Space Representation). By introducing the (ci, ci1, . . . , cip+1), we can represent the original process as vector-valued first-order autoregressive process: state vector ci = a0 a1 0 1 1 0 ... ... 0 0 ci+1 = Aci + ei+1 . . . ap2 ap1 0 0 ... 0 0 ... 0 , ei+1 = εi+1 0 0 ... 0 In this representation, the conditional distribution of ci+1 depends only on ci. From Assumption 4 (1), the spectral radius of matrix is less than 1, which ensures the stability of the process. Let pt(xtπ) = (cid:82) pt(xtc)π(dc). The log-likelihood gradient can be decomposed as: xt log pt(xtci) = = xtpt(xtci) pt(xtci) (cid:18) xtpt(xtci) pt(xtci) xtpt(xtπ) pt(xtπ) (cid:19) + xtpt(xtπ) pt(xtπ) (cid:125) (cid:123)(cid:122) (cid:124) Stationary Term Using the triangle inequality for norms, we get: xt log pt(xtci) (cid:13) (cid:13) (cid:13) (cid:13) xtpt(xtci) pt(xtci) xtpt(xtπ) pt(xtπ) (cid:13) (cid:13) (cid:13) (cid:13) + (cid:13) (cid:13) (cid:13) (cid:13) (cid:13) (cid:13) (cid:13) (cid:13) xtpt(xtπ) (cid:13) (cid:13) (cid:13) (cid:13) pt(xtπ) (cid:13) (cid:13) (cid:13) (cid:13) (cid:125) (cid:123)(cid:122) (cid:124) (cid:13) (cid:13) Stationary Term 19 Published as conference paper at ICLR For the non-stationary term, we have: (cid:13) (cid:13) (cid:13) (cid:13) xtpt(xtci) pt(xtci) xtpt(xtπ) pt(xtπ) pt(xtπ)xtpt(xtci) pt(xtci)xt pt(xtπ) pt(xtci)pt(xtπ) (cid:13) (cid:13) (cid:13) (cid:13) (cid:13) (cid:13) (cid:13) (cid:13) 1 δ2 pt(xtπ)xtpt(xtci) pt(xtci)xtpt(xtπ) 1 δ2 pt(xtπ)[xtpt(xtci) xtpt(xtπ)] + xtpt(xtπ)[pt(xtπ) pt(xtci)] 1 δ pt(xtπ)[xtpt(xtci) xtpt(xtπ)] + xtpt(xtπ)[pt(xtπ) pt(xtci)] (cid:13) (cid:13) (cid:13) (cid:13) (cid:104) = = (cid:105) Using the Lipschitz property of both the conditional probability density function pt(xtc) and its gradient xtpt(xtc) with respect to (with Lipschitz constant L), and the upper bound M2 for pt(xtπ) from Lemma 4, the above inequality becomes: = 1 δ2 δ (cid:104) (cid:105) M2Lci cπ + xtpt(xtπ)Lci cπ (cid:104) (cid:105) ci cπ M2 + xtpt(xtπ) We consider cπ to be representative value from the stationary distribution π, for simplicity we can consider cπ = Eπ[c]. Applying the geometric ergodicity of the Markov chain (Lemma 9 in Appendix H), which gives us ci cπ (cid:112)Varπ(c)ρi, we arrive at: (cid:105)(cid:112) (cid:104) M2 + xtpt(xtπ) Varπ(c)ρi δ2 For the stationary term, from Lemma 5, we have: (cid:13) (cid:13) (cid:13) (cid:13) xtpt(xtπ) pt(xtπ) (cid:13) (cid:13) (cid:13) (cid:13) xtpt(π) δ M1 δ := where is constant. Combining the estimates for both terms, we have: xt log pt(xtci) (cid:16) δ2 M2 + xtpt(xtπ) (cid:17)(cid:112) Varπ(c)ρi + M1 δ where = δ2 (cid:16) M2 + xtpt(xtπ) (cid:17)(cid:112)Varπ(c) and β = ρ (0, 1). ρi + Thus, we obtain the desired exponential decay estimate for the gradient norm as the autoregressive process iterates. This estimate holds uniformly for all xt ."
        },
        {
            "title": "H GEOMETRIC ERGODICITY AND CONVERGENCE TO STATIONARY MEAN",
            "content": "Lemma 9 (Geometric Ergodicity and Convergence to Stationary Mean). Assume that the Markov chain {ct}t0 is geometrically ergodic with stationary distribution π, and let cπ = Eπ[c]. Then, there exist constants > 0 and 0 < ρ < 1 such that for all 0: where Varπ(c) = Eπ[c cπ2]. ci cπ Cρi(cid:112)Varπ(c) Explanation: Lemma 9 formalizes the geometric convergence of the Markov chain state ci towards the stationary mean cπ due to geometric ergodicity. This property ensures that the influence of the initial condition diminishes exponentially over time, allowing us to bound the distance ci cπ by geometrically decaying term proportional to (cid:112)Varπ(c). This justifies the transition in the eighth line of the derivation, replacing ci cπ with term that decays geometrically with i. 20 Published as conference paper at ICLR"
        },
        {
            "title": "I PROOF OF REGULARITY OF CONDITIONAL PROBABILITIES",
            "content": "1. Existence of lower bound: Since the conditional probability density is quadratically continuous and differentiable and is defined on tight set, there is minimum by the extreme value theorem. Since the probability density is always positive, the minimum must be greater than zero. 2. Using the boundedness of the second-order derivatives in our Assumption 2 or K > 0, 2 xt pt(xtci) Consider the difference of derivatives: xtpt(xtc1) xtpt(xtc2) By the median theorem in multivariable calculus, there exists some point between c1c2 such that: xtpt(xtc1) xtpt(xtc2) = 2 xt pt(xtc)(c1 c2) By taking the value of the paradigm of the above equation, we have: xtpt(xtc1) xtpt(xtc2) = 2 xt pt(xtc)(c1 c2) Using Multiplicative Inequality for Norms(a a b)and the boundedness of the second-order derivatives, we obtain: xtpt(xtc1) xtpt(xtc2) 2 xt pt(xtc) (c1 c2) (c1 c2) Thus, taking = M, we get the final inequality: xtpt(xtc1) xtpt(xtc2) L(c1 c2)"
        },
        {
            "title": "J PROOF OF THE MARKOV PROPERTY",
            "content": "This paper mainly uses the last geometric traversal of the theorem and therefore focuses on proving the geometric traversal of Markov chains. The proof is divided into three steps. 1. Drift Conditional Verification: Constructing Foster-Lyapunov Functions (x) = 1 + x2, for E[V (Xn+1)Xn = x] = 1 + E[anx + ϵn2] = 1 + an2x2 + σ2 = an2(1 + x2) + (1 an2 + σ2) λV (x) + whereλ = supn an2 < 1((cid:80) ai < ), = 1 + σ2. 2. Compactness:for > 0, the set {x : (x) R} is compact because it is equivalent to the closed ball{x : x2 1}. 3. Irregularity and continuity: Since the noise term ϵn obeys normal distribution, the transfer probability has positive density everywhere, which guarantees strong Feller and irregularity of the chain. According to Meyn-Tweedie theory, the above condition guarantees geometric ergodicity."
        },
        {
            "title": "K LEMMA OF MARKOV PROPERTY",
            "content": "Lemma 10 (Markov Chain Stationary Theorem). If random process has transition matrix and is ergodic (i.e., any two states are aperiodic and irreducible), then: 21 Published as conference paper at ICLR 2026 1. The limit of the n-step transition matrix exists and is given by: lim n = π(1) π(2) π(1) π(2) ... ... π(1) π(2) ... ... . . . ... π(j) π(j) ... π(j) ... . . . . . . 2. The stationary distribution π = [π(1), π(2), . . .] satisfies the equation: π(j) = (cid:88) πiPij 3. π is the unique non-negative solution to the stationary equation, with (cid:80) π(i) = 1."
        },
        {
            "title": "L AUTOREGRESSIVE CONDITION OPTIMIZATION ALGORITHM",
            "content": "The full algorithm integrates three key components: (1) autoregressive condition generation, (2) diffusion-based denoising, and (3) optimal transport refinement. The pseudocode below specifies the detailed computational workflow. Algorithm 1 Autoregressive Condition Optimization (ACO) with Denoising Integration Require: Initial condition c0 Φθ(ci1, x<i); Diffusion model {Dt}T t=1 with noise levels {βt}; Target latent distribution Pz , OT parameters λ, ϵ, Ksink; Learning rate schedule {ηk}, gradient clipping threshold τ , generated latent z(T ) Denoising trajectory: for = to 1 do z(k,t1) Dt(z(k,t), c(k)) {DDIM update} Ensure: Optimized condition 1: Initialize: c(0) c0, z(0) (0, I) 2: for = 0 to 1 do 3: 4: 5: 6: 7: 8: 9: Optimal transport computation: Sample reference latents {z 10: Compute pairwise cost matrix: Cmn = z(k,0) end for Inverse process alignment: ϕ(c(k)) = c(k) 1(z(k,0))2 + αzT 12 } Pz 11: n2 (cid:123)(cid:122) (cid:125) Latent matching +λ c(k) (cid:124) 1(z (cid:123)(cid:122) Condition consistency n)2 (cid:125) (cid:124) 12: 13: 14: 15: Entropy-regularized OT: Initialize u(0) 1, v(0) 1 for = 1 to Ksink do Pz Kϵ(u(l1),v(l1)) (k) Kϵ(u(l1),v(l)) u(l) v(l) end for γ(k) diag(u(Ksink)) Kϵ diag(v(Ksink)) 16: 17: 18: 19: Gradient computation & update: 20: cLOT γ(k) c(k) 21: cLreg ϕ c(k) 22: total 23: 24: end for 25: Return Clip(cLOT + cLreg, τ ) c(k+1) c(k) ηktotal c(K), zi z(K,0) 22 Published as conference paper at ICLR 2026 L."
        },
        {
            "title": "IMPLEMENTATION DETAILS",
            "content": "Target Distribution Estimation: Maintain an EMA of generated latents: = (1 ν)P (i1) (i) + ν"
        },
        {
            "title": "1\nB",
            "content": "B (cid:88) b=1 δ(z(i) ) with ν = 0.1 and buffer size = 2048. Adaptive Entropy Regularization: Schedule ϵ during Sinkhorn iterations: ϵ(k) = ϵmax (ϵmax ϵmin) Stochastic Optimization: Use Adam optimizer with: ηk = η0 min(1, (cid:112)kwarm/k) where kwarm = 100 controls learning rate warmup. L.2 CONVERGENCE ANALYSIS The algorithm maintains the convergence guarantee in Theorem 3 through: 1. Monotonic Improvement: For Lyapunov function Vk = W2(Pz(k) , Pz ) + λregE[ϕ(c(k))] we have Vk+1 Vk ηkVk2 + O(η2 k) 2. Error Propagation Bound: Approximation error from Sinkhorn iterations satisfies with ρ = ϵ γ(k) γF CρKsink ϵ+δ where δ is the minimum cost matrix entry. 3. Stability Condition: Gradient clipping ensures Lipschitz continuity: total 2 τ (1 + λregLT 1 ) where LT 1 is the Lipschitz constant of the inverse process."
        },
        {
            "title": "M BOUNDED THEOREM",
            "content": "Function pt(xtc) in the fixed interval [a, b] has second order derivatives 2 xt 2 xt pt(xtc) is bounded, so we have > 0, pt(xtc), and 2 xt pt(xtc) K, xt [a, b]. THE FIRST DERIVATIVE IS BOUNDED We use the Mean Value Theorem to show that first-order derivatives are bounded. According to the [a, b], there exists point ξ (xt, mean value theorem, if xt, pt(ξc)(x tc) xtpt(xtc) = 2 xt t), then: xt). xtpt(x pt(xtc) C, so we have: 2 xt xtpt(x t(xtc) = 2 xt This shows that xtpt(xtc) is Lipschitz continuous and the Lipschitz constant is C.So xtpt(xtc) is bounded. Next, we give specific boundedness estimates Taking xt = a, we have: xt) Cx pt(ξc)(x tc) xt. Since xt a, we get: xtpt(xtc) xtpt(ac) Cxt a. Therefore, there exists constant M1 = xt pt(ac) + C(b a) such that for all xt [a, b]: xtpt(xtc) xtpt(ac) + C(b a). xtpt(xtc) M1. Published as conference paper at ICLR"
        },
        {
            "title": "THE ORIGINAL FUNCTION IS BOUNDED",
            "content": "According to the Fundamental Theorem of Calculus, we have: pt(xtc) pt(ac) = (cid:90) xt xtpt(yc) dy. Since xtpt(xtc) M1 for all xt [a, b], we can estimate the above integral: pt(xtc) pt(ac) = (cid:12) (cid:12) (cid:12) (cid:12) (cid:90) xt xtpt(yc) dy (cid:12) (cid:12) (cid:12) (cid:12) (cid:90) xt xtpt(yc) dy M1xt a. Since xt a, we have: Therefore, the original function pt(xtc) is bounded and: pt(xtc) pt(ac) M1(b a). pt(xtc) pt(ac) + M1(b a). Thus, there exists the constant M2 = pt(ac) + M1(b a) such that for all xt [a, b]: pt(xtc) M2. Thus, in fixed interval, bounded second-order derivative is bounded by bounded first-order derivative, and the original function is bounded by the proof."
        },
        {
            "title": "N THE USE OF LARGE LANGUAGE MODELS",
            "content": "In this paper, ChatGPT was employed to assist in polishing the writing. The model was used as language aid to improve clarity, grammar, and readability of the text, while ensuring that the academic content and arguments remain entirely the work of the author."
        },
        {
            "title": "O TABLE OF NOTATIONS",
            "content": "To facilitate easier reading of the theoretical sections and provide quick reference for the mathematical symbols used throughout the paper, we summarize the key notations in Table 4. 24 Published as conference paper at ICLR 2026 Table 4: Summary of Notations Symbol Description Diffusion & Autoregressive Basics x0, xT x1:T q(xtxt1) pθ(xt1xt) sθ(xt, t) ci xi x<i Φθ, Γθ The original data (image) and the Gaussian noise at time . The sequence of latent variables in the forward diffusion process. The forward diffusion transition kernel. The reverse (denoising) process approximated by the network. The score function predicted by the neural network. global, static condition (in standard conditional diffusion). The autoregressively generated condition for the i-th patch. The i-th image patch. The set of patches preceding i, i.e., {x1, . . . , xi1}. The transition operator and noise modulation in the AR condition process. Error Analysis & Theory ϵc ϵc Conditional Error Term (Eq. 13). Measures the change in expected score squared norm due to conditioning. Simplified Conditional Error Term (Eq. 14). Directly measures the expected squared norm of the conditional score. The transition function representing patch-based condition refinement. (ci) σ2 xt log p(cxt) The conditional guidance term in the reverse process mean. M, β, Constants and decay rate describing the descent of the gradient norm (Theorem 2). Condition Refinement (Optimal Transport) I πI ηi W2(, ) F(P ) 1 (k) ρ The ideal condition for patch xi. The minimal sufficient information subspace for patch xi. Orthogonal projection onto the subspace . Extraneous Information Component (Eq. 25). The deviation from the ideal condition (ηi = ci The 2-Wasserstein distance between probability distributions. The free energy functional minimized by the Wasserstein Gradient Flow. The inverse process regularization operator. The probability distribution of the condition at refinement step k. The contraction rate of the Wasserstein Gradient Flow (Theorem 3). )."
        }
    ],
    "affiliations": [
        "SKL-IOTSC, CIS, University of Macau"
    ]
}