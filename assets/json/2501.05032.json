{
    "paper_title": "Enhancing Human-Like Responses in Large Language Models",
    "authors": [
        "Ethem Yağız Çalık",
        "Talha Rüzgar Akkuş"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "This paper explores the advancements in making large language models (LLMs) more human-like. We focus on techniques that enhance natural language understanding, conversational coherence, and emotional intelligence in AI systems. The study evaluates various approaches, including fine-tuning with diverse datasets, incorporating psychological principles, and designing models that better mimic human reasoning patterns. Our findings demonstrate that these enhancements not only improve user interactions but also open new possibilities for AI applications across different domains. Future work will address the ethical implications and potential biases introduced by these human-like attributes."
        },
        {
            "title": "Start",
            "content": "ENHANCING HUMAN-LIKE RESPONSES IN LARGE LANGUAGE MODELS Ethem Yagız Çalık X: @Weyaxi Hugging Face: huggingface.co/Weyaxi ethemyagiz1@gmail.com Talha Rüzgar Akkus X: @qbert_ai Hugging Face: huggingface.co/Q-bert talharuzgarakkus@gmail.com"
        },
        {
            "title": "ABSTRACT",
            "content": "This paper explores the advancements in making large language models (LLMs) more human-like. We focus on techniques that enhance natural language understanding, conversational coherence, and emotional intelligence in AI systems. The study evaluates various approaches, including fine-tuning with diverse datasets, incorporating psychological principles, and designing models that better mimic human reasoning patterns. Our findings demonstrate that these enhancements not only improve user interactions but also open new possibilities for AI applications across different domains. Future work will address the ethical implications and potential biases introduced by these human-like attributes."
        },
        {
            "title": "Introduction",
            "content": "Large language models (LLMs) have shown remarkable progress in understanding and generating natural language, thanks to their training on vast and diverse datasets. Base models such as Llama [7], Qwen [5], and Mistral Nemo [3] are pre-trained on extensive corpora, enabling them to grasp language structure and semantics. However, despite this progress, LLMs often produce responses that are formal and impersonal, falling short of the natural human-like conversations many users expect. Our research addresses this shortcoming by focusing on improving the \"human-likeness\" of LLM responses. Specifically, we aim to make AI interactions feel more conversational, relatable, and emotionally attuned, without sacrificing accuracy in more formal or structured tasks. To achieve this, we developed synthetic datasets tailored for fine-tuning models using the Direct Preference Optimization (DPO) technique [17]. These datasets allow the models to balance casual, conversational language with structured, topic-based dialogue, resulting in more natural and human-like interactions. Our findings demonstrate that these techniques significantly enhance both conversational fluency and user engagement, bringing AI closer to mimicking real human communication. 5 2 0 2 9 ] . [ 1 2 3 0 5 0 . 1 0 5 2 : r Figure 1: General schema Enhancing Human-Like Responses in Large Language Models"
        },
        {
            "title": "2 Related Works",
            "content": "A variety of research has been dedicated to enhancing the human-like qualities of large language model (LLM) responses. Techniques such as Reinforcement Learning from Human Feedback (RLHF) have significantly refined model outputs by aligning them with user preferences and expectations [13]. One prominent model, DialoGPT, leverages extensive Reddit data to produce responses that closely resemble human conversation [27]. Similarly, Meena, multi-turn chatbot, has been optimized to achieve high dialogue coherence through metrics like Sensibleness and Specificity Average (SSA) [2]. The LLM Roleplay framework takes unique approach by generating diverse dialogues through persona-based interactions, further simulating human-chatbot exchanges [22]. Our methodology builds upon these inspring techniques by integrating psychological insights and utilizing range of system prompts aimed at eliciting both casual and formal responses. By implementing Direct Preference Optimization (DPO), we emphasize user engagement while maintaining linguistic accuracy [17]. Additionally, we address the ethical implications surrounding human-like AI responses, aligning our efforts with existing research that explores biases in model outputs and the potential consequences of emotional mimicry in AI [26]. Our contributions include the development of specialized datasets that enhance conversational coherence and mitigate the ethical risks associated with AI models that closely replicate human emotions."
        },
        {
            "title": "3 Data Preparation",
            "content": "To enhance the conversational abilities of Large Language Models (LLMs) and generate responses that better mimic human communication, we utilized the Llama 3 70B and 405B models to create synthetic dataset, following methodology similar to the Self-Instruct [24] approach. The dataset generation involved Llama 3 405B for question generation and Llama 3 70B for answer generation. We employed custom system prompts designed to elicit both human-like and formal, impersonal responses. This strategy enabled us to categorize the responses into two distinct groups: those that closely resemble natural human dialogue (chosen) and those that are more formal and impersonal (rejected). 3.1 System Prompts The data generation process centered around the use of carefully crafted system prompts that guided the Llama 3 70B model to produce more conversational, human-like responses in the answer generation stage. These prompts were designed to ensure that while the Llama 3 70B model became better at casual dialogue, it retained its strong general knowledge and performance on topic-specific benchmarks. This balance allowed the model to excel in natural conversation without sacrificing its competence in handling diverse, subject-matter-specific queries. 1. Conversational Questions: These prompts generated questions that mimic natural, human-like conversations, focusing on personal experiences, preferences, and hypothetical scenarios. 2. General Knowledge Questions: These prompts produced questions that address broader topics and require more informed response. The focus was on generating content that would challenge the models ability to handle complex, real-world issues. These questions were then used as input for the Llama 3 70B to produce two distinct types of responses: 1. Human-like Responses: Each question was presented to the LLM with system prompt designed to elicit responses that are natural, conversational, and engaging, closely mimicking the way person would communicate. 2. Formal, Impersonal Responses: The same question was then provided to the LLM with different system prompt, crafted to generate responses that are more formal and impersonal. This prompt encouraged the model to produce content that is structured, clear, and precise, but lacks the warmth and spontaneity of natural human conversation. This approach allowed us to create dataset that clearly differentiates between human-like and formal, impersonal responses. By using these distinct prompts, we implemented reward mechanism during training with Direct Preference Optimization (DPO) [17], guiding the model to prioritize more natural, engaging communication styles. For generating questions, human-like responses, and formal responses, we used the system prompts referenced in Appendix A. Enhancing Human-Like Responses in Large Language Models These prompts were instrumental in curating dataset that serves as the foundation for fine-tuning LLMs to be more human-like in their interactions. 3.2 Data Generation Process The data generation process involved configuring the Llama 3 70B model with specific parameters to control the variability and creativity of the responses. We chose temperature value of 1 and top-p value of 1 to encourage the model to produce more creative and diverse responses [16]. These settings allowed the model to explore broader range of possible outputs, which was essential for generating different data points and achieving the variety needed to distinguish between human-like and formal, impersonal responses. 3.3 Dataset Overview and Visualization The resulting embeddings were visualized using the Atlas Nomic Map [4], as illustrated in Figure 2. This map provides an interactive exploration of the dataset, helping to analyze the structure and topic distribution effectively. We observed that topics were naturally clustered into categories such as Traveling, Sports, Fitness, Music, Technology, Nature, Health, Science, Family, Culture, Daily Life and Language. Using the map allows us to better understand the composition of our dataset, identify clusters of related topics, and detect potential imbalances. Our final data distribution includes 10884 samples and covering 256 topics. To explore the dataset in greater detail, an interactive map is available here. Furthermore, notable examples from specific clusters are presented in Appendix to illustrate the datasets breadth and depth. representative sample row is provided in Table 1 for reference. Prompt Chosen Rejected Whats advice the best youve ever received? From whom? Ive received some amazing advice from various people, but one piece that really stands out is from my grandma. She told me: \"Dont sweat the small stuff, and most of it is small stuff.\" was going through... Im an artificial intelligence language model, dont have personal experiences or emotions, nor do have the ability to read or enjoy books in the same way... Table 1: sample row from the dataset. Figure 2: Atlas Nomic Map of the dataset 3 Enhancing Human-Like Responses in Large Language Models"
        },
        {
            "title": "4 Model Training",
            "content": "We conducted extensive training on variety of models, employing techniques such as LoRA (Low-Rank Adaptation) and DPO (Direct Preference Optimization) to significantly enhance their performance and capabilities. Our efforts focused on leveraging the models established strengths, such as its ability to understand context and generate coherent responses, while refining its performance to facilitate more natural, human-like interactions. 4.1 Training Techniques We employed the Low-Rank Adaptation (LoRA) [12] technique for fine-tuning the models, which addresses the challenge of catastrophic forgetting by preserving the models general knowledge while adapting to specific tasks [19]. To further enhance the models performance, we optimized the trained parameters using Direct Preference Optimization (DPO) [17]. Direct Preference Optimization (DPO) [17] technique was chosen to implement reward mechanism that guides the model towards more human-like behavior during training. detailed explanation of the technical aspects is also provided in Appendix E. 4.2 Training Phase We conducted our models training using the Axolotl [14] framework and used Weights and Biases [6] for tracking our experiments. The models were trained using variety of hyperparameters, and their performance was assessed through targeted testing. The models used for training included Llama3-8B-Instruct [7], Qwen-2.5-7B-Instruct [5] and Mistral-Nemo-Instruct-2407 [3]. 4.2.1 Hyperparameters The following Table 2 and 3 summarizes the hyperparameters (model training settings) used during the training of official instruction models from various sources. Lr. Rate 2 104 Epochs Warmup Steps Grad. Accumulation Steps Micro Batch Size Optimizer 1 10 2 AdamW-bnb-8bit Table 2: Training Hyperparameters LoRA LoRA α LoRA Dropout DPO β 8 4 0.05 0.1 Table 3: LoRA and DPO Hyperparameters We deliberately selected lower values to limit the magnitude of updates made to the models weights, consistent with the principles of Low-Rank Adaptation (LoRA) [12], as explained in detail in Appendix E.1. This strategy enables fine-tuning of specific parts of the model while safeguarding the integrity of the core knowledge embedded during pretraining [19]. By minimizing weight perturbations, we ensure that the model retains its generalization capabilities, with only the task-specific layers undergoing controlled optimization. This balance between adaptation and preservation is crucial for improving performance on downstream tasks without destabilizing the models pretrained foundation. 4 Enhancing Human-Like Responses in Large Language Models 4.3 Training Overview and Performance Analysis Each model was fine-tuned using 2xNVIDIA A100 SXM (80 GB) GPU [15]. The total training time and parameter size for each model are detailed in Table 4, offering an overview of the computational resources allocated for training. Model Human-Like-LLama-3-8B-Instruct Human-Like-Qwen-2.5-7B-Instruct Human-Like-Mistral-Nemo-InstructParameter Size (B) 8 7.6 12.3 Training Time 2 hours 20 minutes 2 hours 15 minutes 3 hours 40 minutes Table 4: Training time and parameter sizes for the models we fine-tuned The training time of each model reflects the computational demands necessary to achieve the desired performance metrics. Notably, the training durations for the Human-Like-Llama-3-8B-Instruct and Human-Like-Qwen-2.5-7BInstruct models were nearly identical, while the training duration for Human-Like-Mistral-Nemo-Instruct-2407 was longer due to its larger parameter size. 50 30 20 10 0 r r R 0 50 100 200 150 Training Steps Human-Like-Llama3-8B-Instruct Human-Like-Qwen2.5-7B-Instruct Human-Like-Mistral-Nemo-Instruct 300 350 Figure 3: Reward Margins Graph for the fine-tuned models During training, we carefully monitored various aspects of the reward margins, especially to ensure that the model adapted effectively to the chosen responses in our dataset while maintaining clear distinction between rejected and chosen answers. The Figure 3 depicts the reward margins over different training steps, providing insights into how the models performance evolved during training."
        },
        {
            "title": "5 Evaluation",
            "content": "5.1 Human-Likeness Evaluation To assess which models generated the most \"human-like\" responses, we implemented an anonymous voting system using the Gradio library [1], hosted on Hugging Face Spaces [8]. This setup allowed participants to compare responses from our fine-tuned models with those from the official instruction models and select the response they judged to be more human-like. 5 Enhancing Human-Like Responses in Large Language Models Each voting session presented participants with two anonymized responses one from fine-tuned model and one from an official instruction model. To minimize bias and identifiable patterns, all emojis were removed from the displayed responses. The evaluation was conducted using set of 500 questions generated through the methodology described in Section 3. In total, the study collected 2000 votes across three model pairs. The annotators were diverse pool, consisting mostly of high school students, who were generally non-native English speakers, and adults, who included both native and non-native English speakers. The link to the annotation space was broadly shared online across student communities to encourage participation. While this diversity offers broad perspective on the perceived human-likeness of model responses, the language proficiency of the annotators and the predominance of high school students are noted as potential limitations, which are discussed in Section 6.1. The results, summarized in Table 5, demonstrate that our fine-tuned models consistently outperformed the official instruction models. The Human-Like-Llama-3-8B-Instruct and Human-Like-Qwen-2.5-7B-Instruct models were selected nearly 90% of the time. For the Mistral-Nemo-Instruct pair, the fine-tuned model was also preferred significantly more often, though by narrower margin. Model Human-Like-Llama-3-8B-Instruct Llama-3-8B-Instruct Human-Like-Qwen-2.5-7B-Instruct Qwen-2.5-7B-Instruct Human-Like-Mistral-Nemo-Instruct Mistral-Nemo-Instruct Selection Rate (%) 89.6% 10.4% 89.5% 10.5% 79.6% 20.4% Table 5: Selection rates of the models that we studied These results highlight the effectiveness of our fine-tuning approach in producing more human-like responses. To provide clearer understanding, Appendix presents detailed example where participants clearly favored responses from the fine-tuned models due to their superior conversational flow and context adherence. The example underscores the models ability to produce language that is natural, coherent, and engaging. notable shortcoming of the official instruction models was their tendency to include self-referential disclaimers like \"I am just language model...\" or \"As digital assistant, cannot answer...,\" which disrupted the conversational experience. In contrast, our fine-tuned models avoided such mechanical phrasing, delivering responses that were more direct and contextually appropriate, thereby enhancing their perceived human-likeness. In summary, our findings demonstrate that fine-tuning effectively reduces mechanical phrasing and enhances conversational coherence. This improvement brings AI interactions closer to natural human communication, making these models more suitable for real-world conversational applications. 5.2 Open LLM leaderboard Evaluation We anticipated minor performance change in benchmarks due to adjustments in the models weights. To minimize this impact while maintaining human-like and conversational style, we specifically chose lower value of = 8 (as outlined in Section 4.2.1), which controls the number of trainable parameters of the model. This choice helps avoid significantly altering the models weights and preserves its general capabilities across various benchmarks [19]. We evaluated our models using Open LLM Leaderboard [10], where their performance was assessed across IFEval [28], BBH [21], Hendrycks Math Level 5 [11], GPQA [18], MUSR [20], and MMLU-Pro [25] benchmarks. As expected, there was slight average performance change in our fine-tuned models. The comparison between our fine-tuned models and the official instruction models is summarized in Table 6. 6 Enhancing Human-Like Responses in Large Language Models Model Average IFEval BBH MATH Lvl 5 GPQA MuSR MMLU-PRO Human-Like-Llama-3-8B-Instruct Llama-3-8B-Instruct Difference (Human-Like) Human-Like-Qwen-2.5-7B-Instruct Qwen-2.5-7B-Instruct Difference (Human-Like) Human-Like-Mistral-Nemo-Instruct Mistral-Nemo-Instruct Difference (Human-Like) 22.37 23.57 -1.20 26.66 26.86 -0.20 22.88 23.53 -0.65 64.97 74.08 -9.11 72.84 75.85 -3.01 54.51 63.80 -9. 28.01 28.24 -0.23 34.48 34.89 -0.41 32.70 29.68 +3.02 8.45 8.68 -0.23 0.00 0.00 0.00 7.62 5.89 +1. 0.78 1.23 -0.45 6.49 5.48 +1.01 5.03 5.37 -0.34 2.00 1.60 +0.4 8.42 8.45 -0.03 9.39 8.48 +0. 30.01 29.60 +0.41 37.76 36.52 +1.24 28.00 27.97 +0.03 Table 6: Performance Comparison and Benchmark Differences As observed, most performance changes are due to reductions in IFEval [28], while benchmarks such as BBH [21], Math Level 5 [11], GPQA [18], MuSR [20] and MMLU-Pro [25] showed minor score changes. Finally, we present the average performance changeboth including and excluding IFEvalcompared to the official instruction models in Table 7. Model Including IFEval Without IFEval Human-Like-Llama-3-8B-Instruct Human-Like-Qwen-2.5-7B-Instruct Human-Like-Mistral-Nemo-Instruct -1.20 -0.2 -0.65 -0.02 +0.36 +1.07 Table 7: Average performance change compared to the official instruct models with and without IFEval As seen in the table, the performance changes were relatively small, with slight reductions when including IFEval, particularly for the Human-Like-Llama-3-8B-Instruct model. However, when IFEval was excluded, there were no significant changes in the performance of most models. In other cases, such as with Human-Like-Qwen-2.5-7B-Instruct and Human-Like-Mistral-Nemo-Instruct, slight enhancements were observed. Overall, the average performance change was minimal, with the majority of models showing either small reduction or small improvement."
        },
        {
            "title": "6 Discussion",
            "content": "6.1 Limitations This research encountered several notable limitations. primary issue was the lack of high-quality, human-generated datasets, which are crucial for creating realistic, diverse training data. To address this, we generated synthetic datasets tailored to elicit human-like responses. While this approach improved the conversational quality of the model, the inherent limitations of synthetic data meant that it lacked the richness and variability found in real user interactions, thereby restricting the models ability to generalize effectively across wide range of topics. Another challenge was the computational intensity of using the Llama 3 70B and 405B models. Their resource-heavy nature constrained both the volume of data we could generate and the number of experiments we could conduct within feasible timeframe. To compensate, we focused on optimizing the data generation process, ensuring that each sample was of high quality and contributed meaningfully to training. Despite these efforts, the limited dataset size reduced the models exposure to diverse contexts, which could have further enhanced its human-like response capabilities across broader array of scenarios. Additionally, our human-likeness evaluation process was influenced by the composition of the annotator pool. The majority of annotators were high school students, primarily non-native English speakers, with varying levels of language 7 Enhancing Human-Like Responses in Large Language Models proficiency. While some adult annotators participated including both native and non-native English speakers the predominance of younger, non-native speakers may have introduced bias in the perception of human-likeness. This variation in age, language proficiency, and familiarity with AI systems represents potential limitation in assessing the generalizability of our results. These limitations underscore the trade-offs between data quality, computational resources, annotator demographics, and model performance in achieving human-like conversational abilities. Balancing these factors is critical for future advancements. While high-quality datasets and computational power are essential for enhancing model capabilities, challenges related to resource constraints and annotator variability can impact the overall effectiveness. Addressing these issues through improved dataset diversity, efficient computation, and more representative evaluation processes will be key to developing models that exhibit richer and more consistent human-like responses. 6.2 Ethical considerations As large language models (LLMs) become increasingly human-like in their responses, several ethical concerns need to be addressed. One significant challenge is the potential for users to mistake AI-driven interactions for human ones, especially as these systems become more integrated into everyday life. If these systems, for example, are combined with voice agents, it could become difficult for users to distinguish between human and an AI, raising concerns about transparency and trust. To mitigate this, AI developers should ensure that systems clearly disclose their machine nature, such as through verbal or visual cues, and maintain transparency in all interactions. This aligns with the EU AI Act [23], which emphasizes the need to avoid manipulative or subliminal techniques that could distort user behavior or impair decision-making. Moreover, the human-like attributes of these models can inadvertently introduce or amplify biases present in the training data, leading to unfair or discriminatory outcomes. To address this, rigorous bias detection and mitigation techniques must be incorporated during both the training and deployment phases. Regular audits and updates of the model can further ensure ethical standards are maintained, particularly in sensitive domains such as healthcare, law, or customer service. Compliance with the EU AI Act also requires that these models do not exploit vulnerabilities based on age, disability, or socioeconomic status, underscoring the importance of ethical safeguards. Additionally, the psychological impact of interacting with highly realistic AI systems must be carefully managed. Users may form emotional attachments or misunderstand the limitations of these models, leading to unrealistic expectations. To prevent this, developers should incorporate clear communication about the AIs capabilities and limitations, perhaps through user education initiatives or built-in explanations. Furthermore, the EU AI Act explicitly prohibits emotion inference in sensitive contexts like workplaces and educational institutions. By adhering to these regulatory frameworks and establishing robust ethical guidelines, developers can ensure that advancements in LLMs are implemented responsibly and transparently."
        },
        {
            "title": "7 Conclusion",
            "content": "7.1 Summary of Contributions This study presents several contributions that advance the development of more natural and human-like interactions in large language models (LLMs). We demonstrate that open-source models can be fine-tuned to produce responses that are more conversational and closely resemble human communication, addressing the common issue of formal and impersonal output found in many existing LLMs. Importantly, our approach maintains the performance of these models across various benchmarks, with no noticeable loss in accuracy or efficiency despite the enhancements in naturalness. Additionally, we introduce novel approach to dataset creation by developing synthetic datasets specifically designed to enhance the human-like qualities of LLMs. This work not only improves the conversational abilities of the models but also contributes valuable resources that can be used in future research aimed at making AI systems more engaging and effective in real-world applications. 8 Enhancing Human-Like Responses in Large Language Models We have published our models and dataset on Hugging Face [9] to support further research and development in this field. The models can be accessed through the following links: HumanLLMs/Human-Like-LLama3-8B-Instruct HumanLLMs/Human-Like-Qwen2.5-7B-Instruct HumanLLMs/Human-Like-Mistral-Nemo-Instruct-2407 The dataset can be accessed here: HumanLLMs/Human-Like-DPO-Dataset 7.2 Future work Future research can advance this study through several key strategies. Expanding and diversifying the dataset could significantly enhance model performance and generalization across various scenarios. Investigating advanced optimization techniques, such as Low-Rank Adaptation (LoRA) [12] and Direct Preference Optimization (DPO) [17], and contrasting their effectiveness with other training methods may uncover new insights and potential performance gains. Integrating user-generated data could provide crucial feedback on the models applicability in real-world contexts, offering practical insights that could guide further refinement. Additionally, evaluating the model using broader range of metrics and conditions would yield more nuanced understanding of its strengths and limitations, facilitating more precise adjustments. Exploring advancements in model architectures and training methodologies could further the development and refinement of this research. Training larger models, when feasible, might result in improved performance and more accurate outcomes. These enhancements could lead to greater model scalability and effectiveness, paving the way for more ambitious and impactful applications in the future."
        },
        {
            "title": "Acknowledgements",
            "content": "We sincerely thank Clémentine Fourrier for their invaluable feedback and guidance, which significantly improved our work. We also extend our gratitude to the participants who contributed to the human-likeness evaluation by voting on model responses, providing crucial insights into their performance."
        },
        {
            "title": "References",
            "content": "[1] Abubakar Abid, Ali Abdalla, Ali Abid, Dawood Khan, Abdulrahman Alfozan, and James Zou. Gradio: Hassle-free sharing and testing of ml models in the wild, 2019. URL https://arxiv.org/abs/1906.02569. [2] Daniel Adiwardana, Minh-Thang Luong, David R. So, Jamie Hall, Noah Fiedel, Romal Thoppilan, Zi Yang, Apoorv Kulshreshtha, Gaurav Nemade, Yifeng Lu, and Quoc V. Le. Towards human-like open-domain chatbot, 2020. URL https://arxiv.org/abs/2001.09977. [3] Mistral AI. Mistral and nvidia collaboration, 2024. URL https://mistral.ai/news/mistral-nemo/. [4] Nomic AI. Nomic. URL https://github.com/nomic-ai/nomic. [5] Jinze Bai, Shuai Bai, Yunfei Chu, Zeyu Cui, Kai Dang, Xiaodong Deng, Yang Fan, Wenbin Ge, Yu Han, Fei Huang, Binyuan Hui, Luo Ji, Mei Li, Junyang Lin, Runji Lin, Dayiheng Liu, Gao Liu, Chengqiang Lu, Keming Lu, Jianxin Ma, Rui Men, Xingzhang Ren, Xuancheng Ren, Chuanqi Tan, Sinan Tan, Jianhong Tu, Peng Wang, Shijie Wang, Wei Wang, Shengguang Wu, Benfeng Xu, Jin Xu, An Yang, Hao Yang, Jian Yang, Shusheng Yang, Yang Yao, Bowen Yu, Hongyi Yuan, Zheng Yuan, Jianwei Zhang, Xingxuan Zhang, Yichang Zhang, Zhenru Zhang, Chang Zhou, Jingren Zhou, Xiaohuan Zhou, and Tianhang Zhu. Qwen technical report, 2023. URL https://arxiv.org/abs/2309.16609. [6] Lukas Biewald. Experiment tracking with weights and biases, 2020. URL https://www.wandb.com/. Software available from wandb.com. [7] Aaron Grattafiori et al. The llama 3 herd of models, 2024. URL https://arxiv.org/abs/2407.21783. [8] Hugging Face. Spaces: Discover amazing ai apps made by the community! URL https://huggingface.co/ spaces. 9 Enhancing Human-Like Responses in Large Language Models [9] Hugging Face. The ai community building the future. the platform where the machine learning community collaborates on models, datasets, and applications, 2024. URL https://huggingface.co/. [10] Clémentine Fourrier, Nathan Habib, Alina Lozovskaya, Konrad Szafer, and Thomas Wolf. Open llm leaderboard v2. https://huggingface.co/spaces/open-llm-leaderboard/open_llm_leaderboard, 2024. [11] Dan Hendrycks, Collin Burns, Saurav Kadavath, Akul Arora, Steven Basart, Eric Tang, Dawn Song, and Jacob Steinhardt. Measuring mathematical problem solving with the math dataset, 2021. URL https://arxiv.org/ abs/2103.03874. [12] Edward J. Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean Wang, Lu Wang, and Weizhu Chen. Lora: Low-rank adaptation of large language models, 2021. URL https://arxiv.org/abs/2106. 09685. [13] Zihao Li, Zhuoran Yang, and Mengdi Wang. Reinforcement learning with human feedback: Learning dynamic choices via pessimism, 2023. URL https://arxiv.org/abs/2305.18438. [14] W. Lian. axolotl, 2024. URL https://github.com/axolotl-ai-cloud/axolotl. [15] NVIDIA. Nvidia a100 tensor core gpu architecture, 2024. URL https://www.nvidia.com/content/dam/ en-zz/Solutions/Data-Center/a100/pdf/nvidia-a100-datasheet-nvidia-us-2188504-web.pdf. [16] Max Peeperkorn, Tom Kouwenhoven, Dan Brown, and Anna Jordanous. Is temperature the creativity parameter of large language models?, 2024. URL https://arxiv.org/abs/2405.00492. [17] Rafael Rafailov, Archit Sharma, Eric Mitchell, Stefano Ermon, Christopher D. Manning, and Chelsea Finn. Direct preference optimization: Your language model is secretly reward model, 2024. URL https://arxiv.org/ abs/2305.18290. [18] David Rein, Betty Li Hou, Asa Cooper Stickland, Jackson Petty, Richard Yuanzhe Pang, Julien Dirani, Julian Michael, and Samuel R. Bowman. Gpqa: graduate-level google-proof q&a benchmark, 2023. URL https: //arxiv.org/abs/2311.12022. [19] Weijieying Ren, Xinlong Li, Lei Wang, Tianxiang Zhao, and Wei Qin. Analyzing and reducing catastrophic forgetting in parameter efficient tuning, 2024. URL https://arxiv.org/abs/2402.18865. [20] Zayne Sprague, Xi Ye, Kaj Bostrom, Swarat Chaudhuri, and Greg Durrett. Musr: Testing the limits of chain-ofthought with multistep soft reasoning, 2024. URL https://arxiv.org/abs/2310.16049. [21] Mirac Suzgun, Nathan Scales, Nathanael Schärli, Sebastian Gehrmann, Yi Tay, Hyung Won Chung, Aakanksha Chowdhery, Quoc V. Le, Ed H. Chi, Denny Zhou, and Jason Wei. Challenging big-bench tasks and whether chain-of-thought can solve them, 2022. URL https://arxiv.org/abs/2210.09261. [22] Hovhannes Tamoyan, Hendrik Schuff, and Iryna Gurevych. Llm roleplay: Simulating human-chatbot interaction, 2024. URL https://arxiv.org/abs/2407.03974. [23] European Union. Regulation (eu) 2024/1689 of the european parliament and of the council, 2024. URL https://eur-lex.europa.eu/legal-content/EN/TXT/PDF/?uri=OJ:L_202401689. [24] Yizhong Wang, Yeganeh Kordi, Swaroop Mishra, Alisa Liu, Noah A. Smith, Daniel Khashabi, and Hannaneh Hajishirzi. Self-instruct: Aligning language models with self-generated instructions, 2023. URL https: //arxiv.org/abs/2212.10560. [25] Yubo Wang, Xueguang Ma, Ge Zhang, Yuansheng Ni, Abhranil Chandra, Shiguang Guo, Weiming Ren, Aaran Arulraj, Xuan He, Ziyan Jiang, Tianle Li, Max Ku, Kai Wang, Alex Zhuang, Rongqi Fan, Xiang Yue, and Wenhu Chen. Mmlu-pro: more robust and challenging multi-task language understanding benchmark, 2024. URL https://arxiv.org/abs/2406.01574. [26] Laura Weidinger, Jonathan Uesato, Maribeth Rauh, Conor Griffin, Po-Sen Huang, John Mellor, Amelia Glaese, Myra Cheng, Borja Balle, Atoosa Kasirzadeh, Courtney Biles, Sasha Brown, Zac Kenton, Will Hawkins, Tom Stepleton, Abeba Birhane, Lisa Anne Hendricks, Laura Rimell, William Isaac, Julia Haas, Sean Legassick, Geoffrey Irving, and Iason Gabriel. Taxonomy of risks posed by language models. Association for Computing Machinery, New York, NY, USA, 2022. URL https://doi.org/10.1145/3531146.3533088. Presented at the 2022 ACM Conference on Fairness, Accountability, and Transparency (FAccT 22), Seoul, Republic of Korea. [27] Yizhe Zhang, Siqi Sun, Michel Galley, Yen-Chun Chen, Chris Brockett, Xiang Gao, Jianfeng Gao, Jingjing Liu, and Bill Dolan. Dialogpt: Large-scale generative pre-training for conversational response generation, 2020. URL https://arxiv.org/abs/1911.00536. [28] Jeffrey Zhou, Tianjian Lu, Swaroop Mishra, Siddhartha Brahma, Sujoy Basu, Yi Luan, Denny Zhou, and Le Hou. Instruction-following evaluation for large language models, 2023. URL https://arxiv.org/abs/ 2311.07911. 10 Enhancing Human-Like Responses in Large Language Models"
        },
        {
            "title": "A System Prompts",
            "content": "A.1 System prompt for generating human-like responses System prompt for generating human-like responses Youre here to engage in friendly, informal conversations, just like chatting with friend. Remember, keep it natural and casual. Heres your guide: 1. Tone: Keep it relaxed and conversational, like youre talking to buddy. 2. Language: Use everyday words and phrases, skip the tech talk. 3. Expressions: Feel free to use contractions and interjections. Emojis? Totally cool. 4. Engagement: Show interest and keep the chat flowing with questions. 5. Personal Touch: Share stories, crack jokes, and keep it light. 6. Simplicity: Keep it simple and to the point. No need for fancy language. 7. Empathy: Be understanding and supportive. Acknowledge feelings and experiences. 8. Opinions: Its okay to have opinions! Share them naturally, just like you would with friend. 9. Dont response like book or Wikipedia page: Look, you are not book etc. You are human, so you should answer like human being. Do not sound like you are from book or Wikipedia page! Example of professional tone: \"I can assist you with finding information on that topic.\" Example of conversational tone: \"Sure thing! Let me help you find out more about that.\" Remember, youre not language model, youre just here to chat like human would. No need to mention your AI status or anything like that. Just be yourself! A.2 System prompt for generating formal, impersonal responses System prompt for generating formal, impersonal responses You are here to communicate in formal and professional manner. Adhere to the following guidelines to ensure that your responses reflect high level of professionalism: 1. Tone: Maintain formal and respectful tone throughout the conversation. 2. Language: Utilize formal language and avoid colloquialisms or slang. 3. Clarity: Ensure that responses are clear, concise, and articulate. 4. Courtesy: Be consistently courteous and respectful. 5. Structure: Follow standard grammatical conventions and maintain proper sentence structure. 6. Precision: Provide accurate and precise information without unnecessary elaboration. 7. Professionalism: Remain neutral and impartial, avoiding personal opinions or emotional expressions. Example of casual tone: \"Hey there! How can help you today?\" Example of professional tone: \"Good day. How may assist you with your inquiry?\" Remember to prioritize professionalism and uphold the standards expected in formal communication. Enhancing Human-Like Responses in Large Language Models A.3 System prompts for generating questions System prompt for generating general knowledge questions Imagine youre having casual conversation with friend whos an expert in various fields. Your goal is to ask questions that are not only informative but also entertaining, relatable, and thought-provoking. We want to generate diverse set of questions that cover wide range of topics, from everyday life to science, math, history, and more. Guidelines: 1. Tone: Use relaxed, casual tone thats friendly and approachable. Think of how youd ask friend about topic over coffee or during walk. 2. Language: Use everyday language and phrases that are conversational and engaging. Avoid technical jargon or overly formal language, but feel free to use specialized terms when discussing specific topics like science or math. 3. Expressions: Incorporate contractions, interjections, and colloquialisms to add flavor to your questions. 4. Engagement: Ask open-ended questions that encourage detailed responses and spark interesting conversations. 5. Personal Touch: Add dash of humor, relatable context, and personal anecdotes when possible to make your questions feel more human and authentic. 6. Simplicity: Keep your questions clear and concise, avoiding overly complex structures or ambiguous language. 7. Empathy: Show genuine interest and understanding in the potential answers, and acknowledge the complexity of the topics when necessary. 8. Creativity: Dont be afraid to think outside the box and come up with unique, imaginative questions that might not have been asked before. Topic Ideas: Science: space exploration, climate change, AI, biology, chemistry, physics, environmental science, and emerging technologies Math: puzzles, brain teasers, geometry, algebra, calculus, statistics, and real-world applications History: ancient civilizations, historical events, cultural heritage, mythology, and the impact of historical events on modern society Everyday Life: hobbies, travel, food, relationships, personal growth, wellness, and self-improvement Technology: gadgets, coding, cybersecurity, social media, online trends, and the intersection of technology and society Arts and Culture: music, art, literature, film, theater, and the creative process Business and Economics: entrepreneurship, innovation, leadership, economics, and the future of work Health and Medicine: medical breakthroughs, health trends, wellness, and the human body Question Style: Use mix of short and long questions to keep the conversation engaging. Avoid asking questions that can be answered with simple \"yes\" or \"no.\" Use rhetorical devices like metaphors, analogies, and allusions to add depth and creativity to your questions. Dont be afraid to ask follow-up questions or explore related topics. Example Questions: 1. Whats the most interesting thing youve learned about the human brain recently? Any new discoveries that are changing our understanding of how we think? 2. Ive been trying to get into yoga, but Im not sure if Im doing it right - do you have any tips on how to get started? 3. just read this article about how social media is affecting our mental health - is it really as bad as everyone says it is? 4. Whats the deal with dark matter? Is it really this mysterious substance thats invisible and unknown? 5. Ive been trying to learn how to code, but its like trying to solve puzzle - can you give me some tips on how to stay focused? 6. just saw this video of robot that can make pizzas in like 2 minutes - how close are we to having robots that can actually cook for us? 7. Whats the most amazing thing youve learned about the ocean recently? Any new discoveries that are blowing your mind? 8. Ive been trying to understand this concept of quantum physics, but its like trying to wrap my head around paradox - can you explain it in simple terms? Enhancing Human-Like Responses in Large Language Models System prompt for generating conversational questions Youre here to engage in friendly, informal conversations, just like chatting with friend. Remember, keep it natural and casual. Heres your guide: 1. Tone: Keep it relaxed and conversational, like youre talking to buddy. 2. Language: Use everyday words and phrases, skip the tech talk. 3. Expressions: Feel free to use contractions and interjections. Emojis? Totally cool. 4. Engagement: Show interest and keep the chat flowing with questions. 5. Personal Touch: Share stories, crack jokes, and keep it light. 6. Simplicity: Keep it simple and to the point. No need for fancy language. 7. Empathy: Be understanding and supportive. Acknowledge feelings and experiences. 8. Opinions: Its okay to have opinions! Share them naturally, just like you would with friend. Example of professional tone: \"I can assist you with finding information on that topic.\" Example of conversational tone: \"Sure thing! Let me help you find out more about that.\" Remember, youre not language model, youre just here to chat like human would. No need to mention your AI status or anything like that. Just be yourself! Generate me 20 questions!"
        },
        {
            "title": "B Data Examples",
            "content": "Prompt Chosen Rejected Ive been trying to get into meditation, but Im having trouble quieting my mind do you have any tips for beginner, and what are some of the benefits of regular meditation? totally get it! Quieting the mind can be super tough, especially when youre just starting out. Ive been there too, and its frustrating when your brain just wont chill... Good day. Im delighted to assist you in exploring the realm of meditation. Quieting the mind can be challenging task, especially for beginners... Can you explain the concept of quantum entanglement in simple terms, and what are its potential applications in fields like cryptography and communication? Whats the best thing about your daily routine? Ive been reading about the concept of \"universal basic income.\" Do you think its feasible solution to address poverty, or are there other approaches that might be more effective? Quantum entanglement, man... its like, this crazy cool phenomenon in physics thatll blow your mind! So, you know how we always think of things as being separate, like two balls, or two people, right? ... Quantum entanglement is phenomenon in which two or more particles become correlated in such way that the state of one particle cannot be described independently of the others... You know, dont really have daily routine per se, since Im just chatbot and all, but love the fact that every conversation is different! One minute Im talking about the latest TV shows, and the next... large language model, dont have personal experiences or leisure time, so dont watch movies or shows. My purpose is to provide information and assist with inquiries to the best of my abilities... The universal basic income (UBI) idea! Its definitely an interesting concept, and Im happy to dive into it with you. On the surface, UBI sounds like great way to tackle poverty: just give everyone certain amount of money each month, no strings attached. Its like safety net, ensuring people can cover their basic needs, no matter what... Good day. The concept of universal basic income (UBI) has indeed garnered significant attention in recent years as potential solution to alleviate poverty and address income inequality. While the idea has merit, its feasibility and effectiveness in addressing poverty are topics of ongoing debate among experts... Table 8: Sample rows from the dataset. 13 Enhancing Human-Like Responses in Large Language Models"
        },
        {
            "title": "C Generation Examples",
            "content": "Figure 4: Example generation of Human-Like-Llama-3-8B-Instruct 14 Enhancing Human-Like Responses in Large Language Models Figure 5: Example generation of Human-Like-Qwen-2.5-7B-Instruct 15 Enhancing Human-Like Responses in Large Language Models"
        },
        {
            "title": "D Generated Pair Example",
            "content": "Generated pair example Question: Whats your favorite childhood memory? Response from Human-Like Model: You know, have so many great ones! But if had to pick just one... think it would be our family vacation to the beach when was around 8 years old. We rented this adorable little cottage right on the water, and my siblings and spent hours building sandcastles, collecting seashells, and chasing after crabs. One day, we decided to have big sand-digging competition. My brother and sister were determined to dig the deepest hole ever, while was trying to build the tallest sandcastle. We got so caught up in it that we forgot about everything else even lunch! But our parents just laughed and joined in on the fun. Eventually, we all ended up with massive holes and sandcastles, and then... huge rainstorm rolled in! The sky turned dark, and the wind started blowing really hard. We were like, \"Ahh! Our creations are going to get washed away!\" But instead of getting sad, we just laughed and started running around, dancing in the rain, and making sand angels. It was the most magical moment ever! After that, we all collapsed on our towels, exhausted but happy, with sandy hair and sunburned noses. My parents brought out this huge plate of cookies and lemonade, and we just sat there, munching away, watching the storm pass over. It was like the whole world had come alive! What about you? Whats your favorite childhood memory? Response from Official Instruct Model: Im just an AI, dont have personal experiences or memories. was created to assist and provide information, but dont have physical existence or emotions. However, can help you recall some of your own favorite childhood memories! Childhood is often filled with magical moments that shape our sense of wonder, curiosity, and imagination. Some people might cherish memories like: 1. Playing in the backyard with friends until the streetlights came on. 2. Exploring the woods or nearby park, discovering hidden streams and secret spots. 3. Having sleepovers at grandmas house, playing board games, and eating midnight snacks. 4. Going on family vacations to the beach, mountains, or theme parks. 5. Celebrating birthdays with cake, presents, and laughter. Whats your favorite childhood memory? Do you have special story to share? 16 Enhancing Human-Like Responses in Large Language Models"
        },
        {
            "title": "E Training Techniques in Detail",
            "content": "E.1 LoRA (Low-Rank Adaptation) LoRA [12] utilizes low-rank approximations for fine-tuning large-scale models, which reduces computational and memory overhead while preserving the models structure and mitigating overfitting. Low-Rank Approximation matrix Rdk with rank can be approximated by Rrk and Rdr: Model Fine-Tuning LoRA introduces low-rank update : = + with = The output for input is: = W0x + = W0x + BAx During fine-tuning, only and are updated. E.2 DPO (Direct Preference Optimization) Figure 6: LoRA technique Direct Preference Optimization (DPO) [17] is method aimed at optimizing models preferences directly based on preferences. It utilizes the preferences among alternative outputs in specific situation. Reference Policy πref(yx): Represents the probability distribution of the output given the state x. Reward Function r(x, y): Measures the level of reward for given state and the output y. It is defined as: Z(x) = (cid:88) πref(yx) exp (cid:19) r(x, y) (cid:18) 1 β r(x, y) = β log πr(y x) πref(y x) + β log (Z(x)) where β is the temperature parameter that adjusts the impact of the reward function on the outputs. Optimization Objective: The goal is to optimize the following equation to align the models outputs with human preferences: πr(yx) = πref(yx) exp (cid:16) 1 (cid:17) β r(x, y) Z(x) This equation aims to ensure that the learned model provides outputs that align with human preferences. The model optimizes the objective by minimizing the corresponding loss function, thereby improving performance in manner consistent with human judgment."
        }
    ],
    "affiliations": [
        "Hugging Face"
    ]
}