{
    "paper_title": "LLM Pretraining with Continuous Concepts",
    "authors": [
        "Jihoon Tack",
        "Jack Lanchantin",
        "Jane Yu",
        "Andrew Cohen",
        "Ilia Kulikov",
        "Janice Lan",
        "Shibo Hao",
        "Yuandong Tian",
        "Jason Weston",
        "Xian Li"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Next token prediction has been the standard training objective used in large language model pretraining. Representations are learned as a result of optimizing for token-level perplexity. We propose Continuous Concept Mixing (CoCoMix), a novel pretraining framework that combines discrete next token prediction with continuous concepts. Specifically, CoCoMix predicts continuous concepts learned from a pretrained sparse autoencoder and mixes them into the model's hidden state by interleaving with token hidden representations. Through experiments on multiple benchmarks, including language modeling and downstream reasoning tasks, we show that CoCoMix is more sample efficient and consistently outperforms standard next token prediction, knowledge distillation and inserting pause tokens. We find that combining both concept learning and interleaving in an end-to-end framework is critical to performance gains. Furthermore, CoCoMix enhances interpretability and steerability by allowing direct inspection and modification of the predicted concept, offering a transparent way to guide the model's internal reasoning process."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 2 1 ] . [ 1 4 2 5 8 0 . 2 0 5 2 : r LLM Pretraining with Continuous Concepts Jihoon Tack1,2,, Jack Lanchantin1, Jane Yu1, Andrew Cohen1, Ilia Kulikov1, Janice Lan1, Shibo Hao1,3,, Yuandong Tian1, Jason Weston1, Xian Li1 1FAIR at Meta, 2KAIST, 3UC San Diego Work done during an internship at Meta Next token prediction has been the standard training objective used in large language model pretraining. Representations are learned as result of optimizing for token-level perplexity. We propose Continuous Concept Mixing (CoCoMix), novel pretraining framework that combines discrete next token prediction with continuous concepts. Specifically, CoCoMix predicts continuous concepts learned from pretrained sparse autoencoder and mixes them into the models hidden state by interleaving with token hidden representations. Through experiments on multiple benchmarks, including language modeling and downstream reasoning tasks, we show that CoCoMix is more sample efficient and consistently outperforms standard next token prediction, knowledge distillation and inserting pause tokens. We find that combining both concept learning and interleaving in an end-to-end framework is critical to performance gains. Furthermore, CoCoMix enhances interpretability and steerability by allowing direct inspection and modification of the predicted concept, offering transparent way to guide the models internal reasoning process. Date: February 13, 2025 Correspondence: Jihoon Tack at jihoontack@kaist.ac.kr, Xian Li at xianl@meta.com Code: https://github.com/facebookresearch/RAM/tree/main/projects/cocomix"
        },
        {
            "title": "1 Introduction",
            "content": "Recent progress in large language models (LLMs) has revolutionized natural language processing (Brown et al., 2020; Dubey et al., 2024) and thus became core technology in various real-world applications, such as coding assistants (Roziere et al., 2023), search engines (Xuan-Quy et al., 2023), and personal AI assistants (Gao et al., 2023). Central to these breakthroughs is the simple paradigm of next token prediction, which leverages massive amounts of unlabeled text to uncover rich linguistic patterns (Radford et al., 2018, 2019). However, natural language tokens are often superficial (e.g., function words like the or a), necessitating substantial training for models to acquire high-level reasoning and conceptual understanding while also hindering their ability to tackle long-horizon tasks such as planning (LeCun, 2022; Bachmann and Nagarajan, 2024). To tackle this issue, recent studies have investigated methods that go beyond token-level signals by leveraging richer information to train models. For instance, some approaches target more expressive prediction objectives, such as predicting multiple tokens at once to better capture semantic relationships (Gloeckle et al., 2024; DeepSeek-AI, 2024), while others augment the input with rich signals, e.g., self-generated thought tokens (Zelikman et al., 2024), or fixed pause tokens (Goyal et al., 2024) prior to next token prediction. Moreover, emerging evidence suggests that LLMs inherently encode high-level concepts and reasoning processes in their latent representations (Deng et al., 2023; Yang et al., 2024), indicating replacing discrete language tokens with continuous latent representations has promise in improving reasoning efficiency (Hao et al., 2024). While token-level modeling remains important for coherent text generation, the key challenge is to enrich or supplement these natural language tokens so that LLMs can learn more abstract reasoning abilities and long-range dependencies. This raises key question: can we augment the next token prediction objective to explicitly model concepts in latent representation space, thereby bridging semantic abstraction and fine-grained token-level guidance? To this end, we draw inspiration from recent findings that Sparse Autoencoders (SAEs) can effectively isolate meaningful latent features in LLMs by capturing the high-level semantic concepts (Cunningham et al., 2023; 1 Figure 1 Overview of CoCoMix. We use an SAE to extract concepts from pretrained models hidden state hcon and then select important concepts based on the attribution score (i.e., measuring the influence on the output). These selected concepts are used as labels for concept prediction by minimizing the cross-entropy loss CE(, ). The predicted concepts are then compressed into compact vector, forming continuous concept c, which is mixed into the models hidden state by interleaving with token hidden representations. We demonstrate that CoCoMix is more sample efficient and outperforms standard next-token prediction and knowledge distillation baselines. Bricken et al., 2023). As SAEs are trained to reconstruct the models hidden state with sparsity constraints, it encourages focusing on compact set of concept dimensions (Templeton et al., 2024). This makes it possible to highlight the pretrained models conceptsthe core semantic directions that underpin model predictionwhile avoiding unnecessary features. We propose Continuous Concept Mixing (CoCoMix), novel and effective language modeling framework using continuous concepts. Specifically, we extract semantic concepts using pretrained SAE and select the most influential ones based on attribution scores, which quantify each concepts influence on the models output. The model is then trained to predict these selected concepts from its hidden state using cross-entropy loss. Once multiple concepts are predicted, we compress them into single continuous concept and mix (or insert) into the hidden states by interleaving with token embeddings, thereby directly contributing to the next token prediction. An additional benefit is that one can probe or analyze the predicted concepts, enabling controllable generation and improving model interpretability. An overview of CoCoMix is shown in Figure 1. We demonstrate the efficacy of CoCoMix through extensive evaluations across multiple language modeling benchmarks and pretraining model sizes, ranging from million-scale to billion-scale parameter sizes. For instance, when applied to 1.38B-sized model, CoCoMix achieves comparable performance with the next token prediction with 21.5% fewer training tokens. Moreover, CoCoMix demonstrates substantial improvements in weak-to-strong supervision scenarios, where concepts extracted from small model can even be used as ground truth labels to supervise the training of larger model. Finally, we show that the insertion of compressed concept vectors enables us to steer and control the model by probing the predicted concept during generation."
        },
        {
            "title": "2 CoCoMix: Continuous Concept Mixing",
            "content": "In this section, we present Continuous Concept Mixing (CoCoMix), framework that extends next-token prediction with continuous concepts. We first review our problem setup of interest and the concept extraction process using sparse autoencoder (SAE) in Section 2.1. Then, we describe the concept selection framework and explain how the model predicts and interleaves these concepts into its hidden state to improve language modeling in Section 2.2. 2 , with training corpus Mcon = fcon hcon which predicts next token probability"
        },
        {
            "title": "2.1 Prelim: Problem Setup and Sparse Autoencoder\n∗,\nProblem setup. We consider a language-modeling task over a vocabulary\nV\nis drawn i.i.d. from the data distribution. We require a pretrained\nwhere each sequence x = (x1, x2, . . .)\nlanguage model for concept extraction\nx<t).\nHere, we extract the continuous concepts c from the hidden state hcon, where hcon outputs the hidden state at\na chosen layer of depth Lcon, and fcon maps that representation to the next token distribution. We aim to\ntrain\nx<t, c<t), but by augmenting\nwith continuous concepts c.\nSparse Autoencoder. We first propose to extract high-level concepts from a pretrained model’s hidden state. We\nuse an SAE, which maps the input to a high-dimensional sparse activation, with the objective of reconstructing\nthe original input (Lee et al., 2006). When applied to an LLM, the SAE decomposes the hidden state into\nmultiple dimensions, each of which can be viewed as a distinct concept capturing semantically meaningful\nfeatures (Yun et al., 2021; Bricken et al., 2023). We consider the TopK SAE (Makhzani and Frey, 2014),\nwhich uses a TopK activation to enforce sparsity.",
            "content": "to model the same autoregressive factorization (x) = (cid:81)x Mcon(xt t=1 (xt Formally, for given input sequence and corresponding pretrained models hidden state at position t, denoted as hcon RC and linear decoder Rdcon , SAE consists of linear encoder : Rdcon : RC Rdcon , where is the dimension of the concept space. The reconstruction process of SAE is as: = hcon(x)t = E(cid:0)hcon cpre RC is the pre-activation concept vector, TopK( (cid:1), ct = TopK(cid:0)cpre t is the reconstruction. The SAE is trained by minimizing the reconstruction loss: where cpre ) zeros out all but the largest KSAE entries, and . By (cid:98)hcon enforcing TopK sparsity, the SAE isolates the most critical dimensions in that explain the pretrained models features. Here, each activated element of ct is interpreted as concept. 2 (cid:98)hcon hcon (cid:1), (cid:98)hcon = D(cid:0)ct (cid:1),"
        },
        {
            "title": "2.2 Continuous Concept Mixing",
            "content": "Now, we describe the core training pipeline of CoCoMix, which consists of concept selection framework (refer to the left figure in Figure 1) and two training steps to learn and utilize continuous concepts (refer to the right figure in Figure 1). First, we select important concepts using the attribution score, which measures the influence of each concept on the output. Then, we propose predicting the selected concepts from the models hidden state using cross-entropy loss, allowing the model to implicitly learn which concepts should be encoded as hidden representation. Finally, we utilize the predicted concepts to create continuous concept, which is interleaved in the hidden states, enabling the model to explicitly learn to use both the continuous concepts as well as the token hidden states. Intuitively, the model selectively learns which concepts are useful for the next token prediction and how it should mix them with the token representations. Target concept selection using attribution. While the extracted concept ct represents the core features of the current input context, it may not directly indicate which concepts matter most for predicting the ground truth next token xt+1 (Templeton et al., 2024). To this end, we propose utilizing attribution, method to measure the causal influence of each concept on the output (Baehrens et al., 2010; Sundararajan et al., 2017). Specifically, the attribution score measures the influence based on the local linear approximation of the effect of changing the concept value. In this paper, we use simple attribution score that measures the influence by multiplying the loss gradient with given input (Simonyan and Zisserman, 2014; Shrikumar et al., 2016). Concretely, for given input and corresponding concept ct, we define the attribution score at at = cpre ct log fcon (cid:0)xt+ D(ct), h<t (cid:1), RC as: (1) denotes element-wise multiplication. Note that we multiplied the pre-activation cpre where (instead of ct) to capture non-activated concepts made by the TopK sparse activation. Based on the computed attribution score, we select salient concepts and incorporate them into language modeling. Predicting the selected concepts. For given attribution score at, we first select the indices of the concept that have high score and use these as discrete labels for concept prediction. Let be the set of i1, . . . , iKattr } Rd be the models hidden state of the indices corresponding to the top Kattr values of at, and ht = h(x)t = { 3 given input at the same token position. Then, we learn to predict these labels using linear prediction head ) is as follows: that outputs logit zt = (ht) RC by minimizing the following cross-entropy loss CE( , Lconcept(at) = 1 Kattr (cid:88) iI CE (cid:0)zt, i(cid:1). (2) Mixing continuous concepts with token embeddings. To encourage the model to internalize the concept more effectively, we propose mixing (i.e., interleaving) the predicted concept with the existing token hidden representations. As our model predicts multiple concepts at once, we propose to compress them into compact vector through learnable mapping, which we refer to as continuous concept. This compressed vector is interleaved with token vectors. Formally, for given concept prediction logit zt, we sparsify the logit using TopK activation to predict the concepts, then compress them into continuous concept vector ˆct Rd: ˆct = TopK(zt) + b, (3) RdC and where Rd project the TopK-sparse vector to d-dimensional embedding. We then append ˆct to the models hidden sequence ˆct as (h1, ˆc1, . . . , ht, ˆct), which is fed into the remaining transformer blocks . Note that this design not only improves the models performance but also offers interpretability and controllability through analyzing and steering the predicted concept zt, which can be probed or tuned during the models generation process. Furthermore, by analyzing the weights of the compression layer W, one can identify which concept is useful for the next token prediction. This approach shares similarities with intervention techniques in the mechanistic interpretability literature, which modify the hidden state by adding concept vector to the original hidden state (Zou et al., 2023; Wu et al., 2024). However, unlike intervention methods that directly manipulate the hidden state, our approach treats the predicted concept as separate unit of information by interleaving it in the hidden state. This design allows the model to process the concept independently, leveraging the pretrained models internal reasoning reflected in the prediction. Training objective. The final training objective for CoCoMix combines the standard next token prediction loss and the concept prediction term as follows: 1 (cid:88) t=1 log (cid:0)xt+1 ht, ˆct (cid:1) + λ Lconcept(at), (4) where λ is tunable coefficient."
        },
        {
            "title": "3 Experiments",
            "content": "We provide an empirical evaluation of CoCoMix by investigating the following questions: Can CoCoMix improve the performance of next token prediction in LLM pretraining? (Figure 2 and Figure 3) Does CoCoMix show improvement in weak-to-strong supervision setup compared to other knowledge distillation methods? (Table 1 and Figure 4) Does CoCoMix introduce model interpretability and steerability? (Figure 5) How does each proposed component of CoCoMix contribute to the performance? (Figure 6) Before answering each question, we outline the experimental protocol (more details in Appendix A). Training setup. We use pretrained open-source SAE that is trained on the 124M-sized GPT-2 (Gao et al., 2024). We consider training CoCoMix with three different numbers of active parameters, including, 68M, 386M, and 1.38B, with context length of 1024. For the analysis and ablation study, we mainly conducted experiments on the 68M model. Note that this activated parameter count includes those in the new layer 4 (a) Validation perplexity (b) Average downstream task perplexity (c) Average downstream task accuracy Figure 2 CoCoMix vs. NTP performance at different training checkpoints. Each model contains total of 1.38B parameters. Each model is trained on the OpenWebText dataset. For CoCoMix, the concepts are extracted from 124M-sized model (10 smaller than the base model). The plots show improvements in: (a) validation perplexity, (b) average perplexity on LAMBADA, WikiText-103, and (c) average accuracy on HellaSwag, PIQA, SIQA, Arc-Easy, and WinoGrande. (a) OpenWebText, PPL () (b) LAMBADA, PPL () (c) WikiText-103, PPL () (d) HellaSwag, Acc-n () (e) PIQA, Acc () (f) SIQA, Acc () (g) Arc-Easy, Acc () (h) WinoGrande, Acc () Figure 3 CoCoMix vs. NTP performance at different model sizes. We consider various model sizes, including 69M, 386M, and 1.38B parameters and train on 200B OpenWebText tokens. We evaluate the models on OpenWebText validation perplexity and downstream datasets LAMBADA, WikiText-103, HellaSwag, PIQA, SIQA, Arc-Easy, and WinoGrande. (i.e., the concept predictor), whereas for other baselines, we match the same number of active parameters by increasing the hidden state dimension size d. CoCoMix utilizes fewer FLOPs than Pause token (as discussed in Section 3.3) but more FLOPs than NTP, due to the interleaving of continuous concepts. We use the OpenWebText dataset (Radford et al., 2019) as the pretraining corpus to use the same distribution used to train Mcon. All experiments are conducted with 20B training tokens, except for the main experiment in Section 3.1, which uses 200B tokens. Baselines. We consider the standard pretraining next token prediction (NTP) procedure, and the commonly used distillation method, knowledge distillation (KD; Hinton et al., 2015), which is widely used in pretraining (Gu et al., 2024; Sanh et al., 2019; Team, 2024). We excluded KD baselines that require multiple models (i.e., more than single teacher model) to be trained (Gu et al., 2024). For KD, we minimize the KL divergence between teacher and student output while balancing the KL term with the NTP loss. Note that generating synthetic datasets for distillation (Kim and Rush, 2016; Yu et al., 2024) is excluded due to inefficiency, as it requires generating more than 20B tokens. Evaluation setup. For evaluation, we consider the validation perplexity of the pretraining dataset and 7 downstream tasks to benchmark commonsense reasoning and reading comprehension. This includes LAMBADA (Paperno et al., 2016), WikiText-103 (Merity et al., 2017), HellaSwag (Zellers et al., 2019), PIQA 5 (a) Weak-to-strong: Avg. Perplexity (b) Weak-to-strong: Avg. Accuracy (c) Distribution shift (OpenWebMath) Figure 4 CoCoMix vs. Knowledge Distillation (KD). For our weak-to-strong supervision setup, we train 386M model where the teacher of KD (or concept extraction for CoCoMix) is 124M-sized model: we report (a) average perplexity over OpenWebText, LAMABADA, and WikiText and (b) average accuracy over HellaSwag, PIQA, SIQA, Arc-Easy, and WiniGrande dataset. For (c) distribution shift setup, we train all methods on OpenWebMath, math specific pretraining corpus. Method NTP KD CoCoMix NTP KD CoCoMix NTP KD CoCoMix Total Params OWT LAMB Wiki HellaS PIQA SIQA Arc-E WinoG Avg Avg PPL () PPL () PPL () Acc-n () Acc () Acc () Acc () Acc () PPL () Acc () 69M 69M 69M 386M 386M 386M 1.38B 1.38B 1.38B 25.3 25.2 24. 16.3 16.4 15.9 14.3 14.2 13.9 107.6 099.3 099.1 26.3 24.6 19.3 16.6 16.6 15.4 52.3 51.0 50. 29.9 29.1 29.1 25.0 24.9 24.9 27.4 27.4 27.6 33.6 33.6 34.7 38.1 37.4 38.4 59.4 59.8 59. 64.1 63.6 63.6 66.1 66.7 66.9 36.6 36.2 37.2 38.4 38.0 39.2 38.9 39.0 39.5 39.7 39.8 39. 47.3 48.5 47.9 50.5 50.1 50.8 50.7 50.7 51.0 50.9 51.4 52.3 50.0 52.3 53.0 61.8 58.5 58. 24.2 23.4 21.4 18.6 18.5 18.1 42.7 42.8 42.9 46.8 47.0 47.5 48.7 49.1 49.7 Table 1 CoCoMix vs. Next Token Prediction (NTP) vs. Knowledge Distillation (KD). We report performance on the OpenWebText (OWT) training set, as well as downstream tasks, including LAMBADA (LAMB), WikiText-103 (Wiki), HellaSwag (HellaS), PIQA, Social Interaction QA (SIQA), Arc-Easy (Arc-E), and WinoGrande (WinoG). We train three different sizes of models where 124M model is used as teacher. All models are trained on 20B tokens sampled from the OpenWebText dataset. The bold indicates the best result. (Bisk et al., 2020), Social IQA (SIQA; Sap et al., 2019), ARC-easy (Clark et al., 2018), and WinoGrande (Sakaguchi et al., 2020) datasets. We also consider OpenWebMath (Paster et al., 2023) as pretraining dataset to demonstrate that concepts learned from pretrained LLM can still be applied to CoCoMix, even when the model was trained on different corpus."
        },
        {
            "title": "3.1 Main Results",
            "content": "In this section, we illustrate two core results: i) the comparison with NTP on relatively large-scale pretraining setup and ii) the comparison with KD baseline, especially on weak-to-strong supervision scenarios where concepts extracted from small model are used to guide larger model. Improving NTP with CoCoMix at scale. We first present the main result by applying CoCoMix to the NTP. Here, we consider training NTP and CoCoMix on 200B tokens. As shown in Figure 3, CoCoMix consistently and significantly improves the performance of overall downstream tasks on various model sizes. Our results also indicate that larger models (e.g., 386M and 1.38B) can benefit from using concepts extracted from smaller 124M model, showing effective weak-to-strong supervision. Moreover, as shown in Figure 2, CoCoMix consistently improves the performance over the NTP on billion-scale model. For instance, CoCoMix achieves similar performance to NTP while using 21.5% fewer tokens, demonstrating high sample efficiency. Finally, it is worth noting that the performance gain of using CoCoMix is increasing over the training steps, demonstrating strong generalization performance. 6 Figure 5 Qualitative demonstration of the concept steering effect. CoCoMix and GPT2 models are 350M and 124M parameter transformers, respectively, trained on the OpenWebText dataset. For CoCoMix, we manipulate the predicted concept logit z, while for GPT2, we adjust the SAE concept space by increasing the activation of specific concept index. This illustrates the impact of targeted concept steering on the respective model outputs. Comparison with KD baseline. We also compare CoCoMix with KD baseline across multiple scenarios, including (1) stronger teacher model teaching smaller student model; (2) weak-to-strong supervision, where weaker teacher teaches larger student model; and (3) distribution shift, where the student is trained on corpus different from the teachers pretraining distribution. As shown in Table 1, CoCoMix demonstrates improvements over KD in all considered model configurations. In particular, CoCoMix shows significant performance gain in the weak-to-strong supervision setup, e.g., improving average perplexity of 2.8 in 386M, while KD does not show great improvement. This arises because weaker teacher can introduce noisy or suboptimal knowledge, especially when the student surpasses the teacher in capability (Rawat et al., 2024). This trend is also observable in Figure 4, where models trained with KD fall behind standard training midway through training as the student outpaces the teacher (especially in the distribution shift scenario). In contrast, CoCoMix selectively utilizes useful concepts, resulting in consistent performance gain."
        },
        {
            "title": "3.2 Interpretability and Steerability of CoCoMix",
            "content": "Another core advantage of CoCoMix is its interpretability and model steering. Specifically, as the model is trained to predict concepts in its hidden state, we can analyze which concepts it focuses on based on the concept predictions. Furthermore, by amplifying the magnitude of the predicted concept zt, one can control the output generation of the model. Following Templeton et al. (2024), we multiply zt of desired concept element by constants ranging from -10 to 10. To verify whether this steerability works as intended, we steer the activation of the same concept in the pretrained models SAE latent space and confirm whether the output exhibits the corresponding concept. Here, we use 386M parameter model trained with CoCoMix, where the pretrained model is GPT-2. As shown in Figure 5, when the concept related to website address is amplified, both models start generating actual website addresses. This demonstrates that our model has successfully learned the GPT-2 aligned concepts. More examples of steering can be found in Appendix B.2."
        },
        {
            "title": "3.3 Analysis of CoCoMix’s effectiveness",
            "content": "In this section, we provide detailed analysis of CoCoMix to validate the effect of each proposed component. Unless otherwise specified, we use 69M model and train on 20B tokens sampled from the OpenWebText dataset across all methods throughout this section. Effectiveness of the attribution score. We first analyze whether the attribution score effectively extracts important concepts. To demonstrate this, we train CoCoMix using the activation value ct for concept extraction, i.e., Lconcept(ct) in Equation 2, instead of the attribution score at. Remark that the activation value also well reflects the importance of the concept (Bricken et al., 2023). As shown in Figure 6a, using attribution scores significantly improves performance, improving sample efficiency by 17.5% compared to activation value based selection. We believe it will be an interesting future direction to explore other selection criteria for improving CoCoMixs performance or removing undesirable concepts to reduce bias, e.g., selectively removing unsafe concepts for safe LLM pretraining. 7 (a) Effectiveness of the attribution score (b) Concept vs. direct hidden state (c) Compression layers weight analysis (d) Component analysis (e) Design choice for concept condition (f) Comparison with Pause Token Figure 6 Analysis of CoCoMix: (a) Effectiveness of the attribution score for selecting concepts. (b) Comparison between concept prediction and direct hidden state prediction (i.e., predicting the hidden state with continuous loss rather than discretizing the hidden state with SAE). (c) The sparsity in the compression weight. (d) Component analysis by analyzing the contribution of concept prediction and mixing. (e) Design choices for concept conditioning by comparing adding the concept vector to the original hidden state and mixing (interleaving the concept vector with token hidden representation). (f) Comparison between CoCoMix and the Pause token (i.e., adding learnable tokens). We use 69M transformer and train on 20B tokens from the OpenWebText dataset. Comparison with direct hidden state predictions. To evaluate the importance of predicting the concept extracted from SAE, we compare CoCoMix with direct hidden state prediction strategies (i.e., predict the full hidden state without projecting into the concept space). To have comparison under the same architecture as CoCoMix, we replace the concept prediction head with two-layer multilayer perceptron (MLP), denoted ), which predicts the pretrained LLMs hidden state hcon directly from the hidden state of the model g( h. The predicted representation, g(h), is then compressed into continuous embedding for insertion to have the same architecture as CoCoMix. Here, we use continuous loss including, ℓ1, ℓ2, and the cosine distance (e.g., for ℓ2) to predict the hidden state. As shown in Figure 6b, direct hidden state prediction leads to performance drop. We conjecture this to be due to SAEs ability to decompose the latent into semantically meaningful concepts while predicting all hidden states may include noisy components, emphasizing the effectiveness of our method. hcon g(h) 2 2 Compression layer weight analysis. Now, we analyze the weight of the compression layer in Equation 3 to show how CoCoMix utilize the predicted concept. To this end, we visualize the ℓ2 norm of each concepts RdC, where is the hidden dimension and is weights of 386M CoCoMix: for the weight matrix the number of concepts, the norm is defined as: . As shown in Figure 6c, we found W:,c that portion of concept weights are near zero, e.g., 5.8% has norm less than 1e-2. This indicates that CoCoMix learns to ignore these concepts when compressing them into continuous concept if it is not useful. We conjecture such process enabled CoCoMix for strong weak-to-strong supervision as it learned to ignore ineffective concepts extracted from weak model. 2 = d=1 2 d,c (cid:113)(cid:80)D Component Analysis. We analyze the contributions of each component of our method: (a) concept prediction Equation 2, and (b) concept insertion Equation 4. The results in Figure 6d demonstrate that both components are critical for performance improvement. Specifically, applying the concept prediction loss alone yields 8 modest reduction in perplexity. However, incorporating concept insertion alongside prediction enhances the effectiveness of the loss, resulting in further performance gains. This highlights the role of insertion in enabling the model to leverage the pretrained LLMs latent reasoning effectively. Notably, while concept insertion increases parameter count, it has limited impact on performance when used alone, emphasizing the critical role of concept prediction. Continuous concept mixing method. We explored two methods for introducing the continuous concept ˆct into the models hidden state ht, referred to as concept conditioning. The first method, insertion, insert the continuous concept in front of the token embedding, thus enabling the model to have mixed input of the token and concept. The other option is intervention, which directly modifies the hidden state by adding the ht + ˆct. As illustrated in Figure 6e, both methods enhance pretraining performance concept vector, i.e., ht (compared to NTP in Figure 6d), highlighting the importance of concept conditioning, where the insertion method performed better. By introducing distinct concept vector, the insertion method enables the model to explicitly recognize and effectively utilize concepts during generation, enhancing its overall performance. CoCoMix vs. pause tokens. Furthermore, we consider an additional baseline that jointly uses pause token (Goyal et al., 2024). Specifically, the pause token uses an additional learnable token that is inserted into the token embedding, enabling the LLM to think more before predicting the next token, which is similar to our continuous concept insertion. To this end, we insert the pause token for every input token on the same hidden state layer as CoCoMix to ensure comparable computation. Moreover, we also consider training the pause token with KD. As shown in Figure 6f, CoCoMix consistently outperforms the pause token, indicating our inserted (or interleaved) continuous concept indeed consists of useful information to improve the performance."
        },
        {
            "title": "4 Related Work",
            "content": "Beyond token-level guidance for language modeling. While next token prediction remains the standard paradigm for language modeling, recent approaches have begun to explore methods that provide guidance beyond language tokens. For instance, some methods explore better target, such as leveraging multi-token predictions to capture long context dependencies (Gloeckle et al., 2024; DeepSeek-AI, 2024) or predicting sequence embedding (Lee et al., 2024). Additionally, methods explore new types of inputs, e.g., using latents (Hao et al., 2024) or self-generated thought as inputs (Zelikman et al., 2024), which have shown improving reasoning capabilities. Only recently, concept-level modeling using local encoder-decoder architectures has also been explored to represent language at higher abstraction level (LCM et al., 2024). Other methods add extra tokens in the input space to increase computation at inference time Nye et al. (2021); Wei et al. (2022); Goyal et al. (2024); Lanchantin et al. (2024). In contrast to other works, we propose pretraining approach that integrates next token prediction with continuous concepts, connecting high level concepts with fine-grained token guidance. Sparse Autoencoders (SAEs). SAEs extend the autoencoder by enforcing sparsity constraints in the latent space (Lee et al., 2006). The features learned by SAEs are often interpretable and disentangled, making them useful across various domains, including language modeling (Bricken et al., 2023). Additionally, SAEs have gained attention in mechanistic interpretability due to their ability to capture coherent semantic concepts (Marks et al., 2024). This property has enabled practical advancements in identifying and manipulating semantic concepts and facilitating steering for controlled model outputs (Lieberum et al., 2024). Among SAE variants, TopK SAEs (Makhzani and Frey, 2014) enforce explicit sparsity using TopK activation function, demonstrating effectiveness even for large models (Gao et al., 2024). In this work, we leverage SAE and, to the best of our knowledge, are the first to apply it to LLM pretraining, achieving strong performance while enhancing the interpretability and controllability of the trained model. Knowledge distillation (KD). Our method can also be related to KD, i.e., transfers the expertise of teacher model to student model to enhance performance (Hinton et al., 2015; Zagoruyko and Komodakis, 2017), as CoCoMix extracts high-level semantic features from pretrained model which is used to train base model. Recently, KD for LLMs has garnered increasing attention, leveraging knowledge from teacher to improve the generative and encoding capabilities of student (Sanh et al., 2019; Ko et al., 2024). Especially, applying KD to LLM pretraining remains challenging due to the massive token scales (billions to trillions), forcing most current methods to resort to naive token-level probability matching (Team, 2024; Gu et al., 2024). 9 Additionally, while pretrained models contain vast amount of learned information and are thus beneficial to use, reusing knowledge from smaller teacher models remains challenging (Burns et al., 2023). In this work, we show CoCoMix can even leverage the concept extracted from small models to train large model showing weak-to-strong supervision."
        },
        {
            "title": "5 Conclusion",
            "content": "We propose Continuous Concept Mixing (CoCoMix), new LLM pretraining framework that augments next token prediction with continuous concepts. By leveraging concepts extracted from pretrained SAE as targets, our model predicts both the next token and the associated concept. Predicted concepts are then compressed into continuous concept vector, which is then mixed into the hidden state. This approach enhances interpretability and controllability by enabling direct probing of the distilled concepts. Experimental results show that CoCoMix consistently improves performance across benchmarks, particularly in challenging generalization scenarios such as weak-to-strong supervision. Future work could explore learning continuous concepts during pretraining without the need for distillation."
        },
        {
            "title": "References",
            "content": "Gregor Bachmann and Vaishnavh Nagarajan. The pitfalls of next-token prediction. In International Conference on Machine Learning, 2024. David Baehrens, Timon Schroeter, Stefan Harmeling, Motoaki Kawanabe, Katja Hansen, and Klaus-Robert Müller. How to explain individual classification decisions. The Journal of Machine Learning Research, 2010. Yonatan Bisk, Rowan Zellers, Jianfeng Gao, Yejin Choi, et al. Piqa: Reasoning about physical commonsense in natural language. In AAAI Conference on Artificial Intelligence, 2020. Trenton Bricken, Adly Templeton, Joshua Batson, Brian Chen, Adam Jermyn, Tom Conerly, Nick Turner, Cem Anil, Carson Denison, Amanda Askell, Robert Lasenby, Yifan Wu, Shauna Kravec, Nicholas Schiefer, Tim Maxwell, Nicholas Joseph, Zac Hatfield-Dodds, Alex Tamkin, Karina Nguyen, Brayden McLean, Josiah Burke, Tristan Hume, Shan Carter, Tom Henighan, and Christopher Olah. Towards monosemanticity: Decomposing language models with dictionary learning. Transformer Circuits Thread, 2023. https://transformer-circuits.pub/2023/monosemanticfeatures/index.html. Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. Language models are few-shot learners. In Advances in Neural Information Processing Systems, 2020. Collin Burns, Pavel Izmailov, Jan Hendrik Kirchner, Bowen Baker, Leo Gao, Leopold Aschenbrenner, Yining Chen, Adrien Ecoffet, Manas Joglekar, Jan Leike, et al. Weak-to-strong generalization: Eliciting strong capabilities with weak supervision. arXiv preprint arXiv:2312.09390, 2023. Peter Clark, Isaac Cowhey, Oren Etzioni, Tushar Khot, Ashish Sabharwal, Carissa Schoenick, and Oyvind Tafjord. Think you have solved question answering? try arc, the ai2 reasoning challenge. arXiv preprint arXiv:1803.05457, 2018. Hoagy Cunningham, Aidan Ewart, Logan Riggs, Robert Huben, and Lee Sharkey. Sparse autoencoders find highly interpretable features in language models. arXiv preprint arXiv:2309.08600, 2023. DeepSeek-AI. Deepseek-v3 technical report. arXiv preprint arXiv:2412.19437, 2024. Yuntian Deng, Kiran Prasad, Roland Fernandez, Paul Smolensky, Vishrav Chaudhary, and Stuart Shieber. Implicit chain of thought reasoning via knowledge distillation. arXiv preprint arXiv:2311.01460, 2023. Abhimanyu Dubey, Abhinav Jauhri, Abhinav Pandey, Abhishek Kadian, Ahmad Al-Dahle, Aiesha Letman, Akhil Mathur, Alan Schelten, Amy Yang, Angela Fan, et al. The llama 3 herd of models. arXiv preprint arXiv:2407.21783, 2024. Difei Gao, Lei Ji, Luowei Zhou, Kevin Qinghong Lin, Joya Chen, Zihan Fan, and Mike Zheng Shou. Assistgpt: general multi-modal assistant that can plan, execute, inspect, and learn. arXiv preprint arXiv:2306.08640, 2023. Leo Gao, Tom Dupré la Tour, Henk Tillman, Gabriel Goh, Rajan Troll, Alec Radford, Ilya Sutskever, Jan Leike, and Jeffrey Wu. Scaling and evaluating sparse autoencoders. arXiv preprint arXiv:2406.04093, 2024. Fabian Gloeckle, Badr Youbi Idrissi, Baptiste Rozière, David Lopez-Paz, and Gabriel Synnaeve. Better & faster large language models via multi-token prediction. In International Conference on Machine Learning, 2024. Sachin Goyal, Ziwei Ji, Ankit Singh Rawat, Aditya Krishna Menon, Sanjiv Kumar, and Vaishnavh Nagarajan. Think before you speak: Training language models with pause tokens. In International Conference on Learning Representations, 2024. Yuxian Gu, Hao Zhou, Fandong Meng, Jie Zhou, and Minlie Huang. Miniplm: Knowledge distillation for pre-training language models. arXiv preprint arXiv:2410.17215, 2024. Shibo Hao, Sainbayar Sukhbaatar, DiJia Su, Xian Li, Zhiting Hu, Jason Weston, and Yuandong Tian. Training large language models to reason in continuous latent space. arXiv preprint arXiv:2412.06769, 2024. Geoffrey Hinton, Oriol Vinyals, and Jeff Dean. Distilling the knowledge in neural network. arXiv preprint arXiv:1503.02531, 2015. Yoon Kim and Alexander Rush. Sequence-level knowledge distillation. In Annual Conference of the Association for Computational Linguistics, 2016. 11 Jongwoo Ko, Sungnyun Kim, Tianyi Chen, and Se-Young Yun. Distillm: Towards streamlined distillation for large language models. In International Conference on Machine Learning, 2024. Jack Lanchantin, Shubham Toshniwal, Jason Weston, Sainbayar Sukhbaatar, et al. Learning to reason and memorize with self-notes. Advances in Neural Information Processing Systems, 36, 2024. LCM, Loïc Barrault, Paul-Ambroise Duquenne, Maha Elbayad, Artyom Kozhevnikov, Belen Alastruey, Pierre Andrews, Mariano Coria, Guillaume Couairon, Marta Costa-jussà, et al. Large concept models: Language modeling in sentence representation space. arXiv preprint arXiv:2412.08821, 2024. Yann LeCun. path towards autonomous machine intelligence. Open Review, 2022. Honglak Lee, Alexis Battle, Rajat Raina, and Andrew Ng. Efficient sparse coding algorithms. In Advances in Neural Information Processing Systems, 2006. Hyunji Lee, Doyoung Kim, Jihoon Jun, Sejune Joo, Joel Jang, Kyoung-Woon On, and Minjoon Seo. Semiparametric token-sequence co-supervision. In Annual Conference of the Association for Computational Linguistics, 2024. Tom Lieberum, Senthooran Rajamanoharan, Arthur Conmy, Lewis Smith, Nicolas Sonnerat, Vikrant Varma, János Kramár, Anca Dragan, Rohin Shah, and Neel Nanda. Gemma scope: Open sparse autoencoders everywhere all at once on gemma 2. arXiv preprint arXiv:2408.05147, 2024. Loshchilov. Decoupled weight decay regularization. arXiv preprint arXiv:1711.05101, 2017. Alireza Makhzani and Brendan Frey. K-sparse autoencoders. In International Conference on Learning Representations, 2014. Samuel Marks, Can Rager, Eric Michaud, Yonatan Belinkov, David Bau, and Aaron Mueller. Sparse feature circuits: Discovering and editing interpretable causal graphs in language models. arXiv preprint arXiv:2403.19647, 2024. Stephen Merity, Caiming Xiong, James Bradbury, and Richard Socher. Pointer sentinel mixture models. In International Conference on Learning Representations, 2017. Maxwell Nye, Anders Johan Andreassen, Guy Gur-Ari, Henryk Michalewski, Jacob Austin, David Bieber, David Dohan, Aitor Lewkowycz, Maarten Bosma, David Luan, et al. Show your work: Scratchpads for intermediate computation with language models. arXiv preprint arXiv:2112.00114, 2021. Denis Paperno, Germán Kruszewski, Angeliki Lazaridou, Quan Ngoc Pham, Raffaella Bernardi, Sandro Pezzelle, Marco Baroni, Gemma Boleda, and Raquel Fernández. The lambada dataset: Word prediction requiring broad discourse context. In Annual Conference of the Association for Computational Linguistics, 2016. Keiran Paster, Marco Dos Santos, Zhangir Azerbayev, and Jimmy Ba. Openwebmath: An open dataset of high-quality mathematical web text. arXiv preprint arXiv:2310.06786, 2023. Alec Radford, Karthik Narasimhan, Tim Salimans, and Ilya Sutskever. Improving language understanding by generative pre-training. 2018. Alec Radford, Jeff Wu, Rewon Child, David Luan, Dario Amodei, and Ilya Sutskever. Language models are unsupervised multitask learners. 2019. Ankit Singh Rawat, Veeranjaneyulu Sadhanala, Afshin Rostamizadeh, Ayan Chakrabarti, Wittawat Jitkrittum, Vladimir Feinberg, Seungyeon Kim, Hrayr Harutyunyan, Nikunj Saunshi, Zachary Nado, et al. little help goes long way: Efficient llm training by leveraging small lms. arXiv preprint arXiv:2410.18779, 2024. Baptiste Roziere, Jonas Gehring, Fabian Gloeckle, Sten Sootla, Itai Gat, Xiaoqing Ellen Tan, Yossi Adi, Jingyu Liu, Romain Sauvestre, Tal Remez, et al. Code llama: Open foundation models for code. arXiv preprint arXiv:2308.12950, 2023. Keisuke Sakaguchi, Ronan Le Bras, Chandra Bhagavatula, and Yejin Choi. Winogrande: An adversarial winograd schema challenge at scale. In AAAI Conference on Artificial Intelligence, 2020. Victor Sanh, Lysandre Debut, Julien Chaumond, and Thomas Wolf. Distilbert, distilled version of bert: smaller, faster, cheaper and lighter. In Advances in Neural Information Processing Systems, 2019. Maarten Sap, Hannah Rashkin, Derek Chen, Ronan Le Bras, and Yejin Choi. Social IQa: Commonsense reasoning about social interactions. In Conference on Empirical Methods in Natural Language Processing, 2019. Avanti Shrikumar, Peyton Greenside, Anna Shcherbina, and Anshul Kundaje. Not just black box: Learning important features through propagating activation differences. In International Conference on Machine Learning, 2016. 12 Karen Simonyan and Andrew Zisserman. Very deep convolutional networks for large-scale image recognition. arXiv preprint arXiv:1409.1556, 2014. Mukund Sundararajan, Ankur Taly, and Qiqi Yan. Axiomatic attribution for deep networks. In International Conference on Machine Learning, 2017. Gemma Team. Gemma 2: Improving open language models at practical size. arXiv preprint arXiv:2408.00118, 2024. Adly Templeton, Tom Conerly, Jonathan Marcus, Jack Lindsey, Trenton Bricken, Brian Chen, Adam Pearce, Craig Citro, Emmanuel Ameisen, Andy Jones, Hoagy Cunningham, Nicholas Turner, Callum McDougall, Monte MacDiarmid, C. Daniel Freeman, Theodore R. Sumers, Edward Rees, Joshua Batson, Adam Jermyn, Shan Carter, Chris Olah, and Tom Henighan. Scaling monosemanticity: Extracting interpretable features from claude 3 sonnet. Transformer Circuits Thread, 2024. https://transformer-circuits.pub/2024/scaling-monosemanticity/index.html. Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Fei Xia, Ed Chi, Quoc Le, Denny Zhou, et al. Chain-of-thought prompting elicits reasoning in large language models. Advances in neural information processing systems, 35:2482424837, 2022. Zhengxuan Wu, Aryaman Arora, Zheng Wang, Atticus Geiger, Dan Jurafsky, Christopher Manning, and Christopher Potts. Reft: Representation finetuning for language models. In Advances in Neural Information Processing Systems, 2024. Dao Xuan-Quy, Le Ngoc-Bich, Phan Xuan-Dung, Ngo Bac-Bien, and Vo The-Duy. Evaluation of chatgpt and microsoft bing ai chat performances on physics exams of vietnamese national high school graduation examination. arXiv preprint arXiv:2306.04538, 2023. Sohee Yang, Elena Gribovskaya, Nora Kassner, Mor Geva, and Sebastian Riedel. Do large language models latently perform multi-hop reasoning? In Annual Conference of the Association for Computational Linguistics, 2024. Ping Yu, Jing Xu, Jason Weston, and Ilia Kulikov. Distilling system 2 into system 1. arXiv preprint arXiv:2407.06023, 2024. Zeyu Yun, Yubei Chen, Bruno Olshausen, and Yann LeCun. Transformer visualization via dictionary learning: contextualized embedding as linear superposition of transformer factors. arXiv preprint arXiv:2103.15949, 2021. Sergey Zagoruyko and Nikos Komodakis. Paying more attention to attention: Improving the performance of convolutional neural networks via attention transfer. In International Conference on Learning Representations, 2017. Eric Zelikman, Georges Harik, Yijia Shao, Varuna Jayasiri, Nick Haber, and Noah Goodman. Quiet-star: Language models can teach themselves to think before speaking. In Conference on Language Modeling, 2024. Rowan Zellers, Ari Holtzman, Yonatan Bisk, Ali Farhadi, and Yejin Choi. Hellaswag: Can machine really finish your sentence? In Annual Conference of the Association for Computational Linguistics, 2019. Andy Zou, Long Phan, Sarah Chen, James Campbell, Phillip Guo, Richard Ren, Alexander Pan, Xuwang Yin, Mantas Mazeika, Ann-Kathrin Dombrowski, et al. Representation engineering: top-down approach to ai transparency. arXiv preprint arXiv:2310.01405, 2023."
        },
        {
            "title": "A Experimental Details",
            "content": "Architecture details. Our method and baseline both utilize the GPT-2-based Transformer architecture and tokenizer (Radford et al., 2019), with context length of 1024. We consider three model sizes, defined by the number of activated parameters: 69M, 386M, and 1.38B. For CoCoMix, the hidden state dimensions are 512, 1024, and 2028, with 8, 24, and 24 layers, respectively. The number of attention heads is set to 8, 16, and 16 for the three configurations. Baseline models are configured to match the number of activated parameters by employing the same number of layers and attention heads, with hidden state dimensions of 624, 1072, and 2096, respectively. We leverage an open-source pretrained TopK Sparse Autoencoder (SAE) (Gao et al., 2024) for concept extraction, where Kconcept is set to 32 and the concept activation size is 32,768. Consequently, our models introduce an additional (C + Kconcept) activated parameters on top of the base Transformer parameters. The GPT-2 model (a teacher model for KD and concept extraction model for CoCoMix) has 12 layers, hidden dimension size of 768, and 124M parameters. The concept extraction layer Lcon is configured as the 6th middle layer of GPT-2 for all model sizes. For CoCoMix, the concept prediction is done at the 4th layer for the 69M model and the 6th layer for the larger 386M and 1.38B models. Training details. We mainly followed the training details outlined in (Brown et al., 2020). For the main results presented in Section 2, models were trained on 200B tokens over approximately 382K training steps, while other experiments were conducted on 20B tokens. We used OpenWebText as the training dataset to match the same training corpus as GPT-2. The learning rate schedule included warm-up phase over the first 1/300 of the total training steps, followed by cosine decay to 10% of the maximum learning rate at the end of training. The maximum learning rates were set to 6e-4, 3e-4, and 2e-4 for the 69M, 386M, and 1.38B models, respectively. The batch sizes were configured to 0.5M, 0.5M, and 1M tokens for the three model sizes. weight decay of 0.1 was applied across all configurations, and we utilized the AdamW optimizer (Loshchilov, 2017) with β1 = 0.9 and β2 = 0.95. For training CoCoMix, the concept prediction loss Equation 4 was scaled by hyperparameter λ = 0.1, and Kattri was set to 4. For the KD baseline, we employed the vanilla KD loss, where the output probabilities of the teacher model Mcon and the student were matched using the Kullback-Leibler (KL) divergence. Specifically, given an input x, the KD model x)), where λKD = 0.1 consistently demonstrated loss is defined as ( strong performance. Mcon( x) + λKD (xt+ KL( log x)"
        },
        {
            "title": "B Additional Results",
            "content": "B.1 Detailed Performance During Training In this section, we present the performance tracking during training on 200B tokens, including validation perplexity and the perplexity and accuracy of various downstream tasks, including LAMBADA, WikiText-103, HellaSwag, PIQA, SIQA, Arc-Easy, and WinoGrande datasets. As shown in Figure 7, Figure 8, and Figure 9, we compare CoCoMix with the next token prediction (NTP) across different active parameter sizes: 69M, 386M, and 1.38B. In most of the graphs, CoCoMix consistently demonstrates performance gains. Notably, our results show that CoCoMix achieves stable improvements in perplexity across all tasks. Furthermore, CoCoMix exhibits sample efficiency; for instance, in the 1.38B model, CoCoMix reaches the same OpenWebText validation perplexity as NTP while requiring approximately 43B fewer tokens (a 21.5% improvement in token efficiency). B.2 Additional Steerability Results To further analyze the steerability enabled by CoCoMix, we conducted experiments using both the same prompt as in the main figure and new prompt (in Figure 10). For consistency, we first applied steering on additional concepts identified during our analysis$ dollar and Phoneusing the same prompt as in Figure 5. These experiments confirmed that the model could effectively modulate its output based on these newly identified concepts, producing coherent and concept-aligned generations. Next, to verify whether the 14 (a) OpenWebText, PPL () (b) LAMBADA, PPL () (c) WikiText-103, PPL () (d) HellaSwag, Acc-n () (e) PIQA, Acc () (f) SIQA, Acc () (g) Arc-Easy, Acc () (h) WinoGrande, Acc () Figure 7 CoCoMix vs. NTP performance at different training checkpoints on 69M parameter model. Each model is trained on the 200B tokens sampled from the OpenWebText dataset. The plot shows the result of (a) OpenWebText, (b) LAMBADA, (c) WikiText-103, (d) HellaSwag, (e) PIQA, (f) SIQA, (g) Arc-Easy, and (h) WinoGrande datasets. We use the concepts extracted from 124M-sized model for training CoCoMix. identified concepts generalize to different contexts, we experimented with new prompt: Latent concept modeling is and steered the model using the previously identified concepts month/year and Phone. The results showed that the model successfully reproduced outputs aligned with these concepts, further supporting the robustness of our method. Additionally, we explored whether new concepts could be identified and steered using the same prompt. In this case, we identified two new concepts: politics/law and aggressive tone. Steering the model with these new concepts demonstrated that the outputs could be effectively controlled to exhibit characteristics aligned with the corresponding concepts. These findings further highlight the flexibility and interpretability of our approach. 15 (a) OpenWebText, PPL () (b) LAMBADA, PPL () (c) WikiText-103, PPL () (d) HellaSwag, Acc-n () (e) PIQA, Acc () (f) SIQA, Acc () (g) Arc-Easy, Acc () (h) WinoGrande, Acc () Figure 8 CoCoMix vs. NTP performance at different training checkpoints on 368M parameter model. Each model is trained on the 200B tokens sampled from the OpenWebText dataset. The plot shows the result of (a) OpenWebText, (b) LAMBADA, (c) WikiText-103, (d) HellaSwag, (e) PIQA, (f) SIQA, (g) Arc-Easy, and (h) WinoGrande datasets. We use the concepts extracted from 124M-sized model for training CoCoMix. (a) OpenWebText, PPL () (b) LAMBADA, PPL () (c) WikiText-103, PPL () (d) HellaSwag, Acc-n () (e) PIQA, Acc () (f) SIQA, Acc () (g) Arc-Easy, Acc () (h) WinoGrande, Acc () Figure 9 CoCoMix vs. NTP performance at different training checkpoints on 1.38B parameter model. Each model is trained on the 200B tokens sampled from the OpenWebText dataset. The plot shows the result of (a) OpenWebText, (b) LAMBADA, (c) WikiText-103, (d) HellaSwag, (e) PIQA, (f) SIQA, (g) Arc-Easy, and (h) WinoGrande datasets. We use the concepts extracted from 124M-sized model for training CoCoMix. 16 (a) Main figure prompt, $ dollar and Phone concept (b) New prompt, year/month and Phone concept (c) New prompt, Politic/law and aggressive tone concept Figure 10 More qualitative demonstration of the concept steering effect. CoCoMix and GPT2 models are 350M and 124M parameter transformers, respectively. For CoCoMix, we manipulate the predicted logit z, while for GPT2, we adjust the SAE concept space by increasing the activation of specific concept index"
        }
    ],
    "affiliations": [
        "FAIR at Meta",
        "KAIST",
        "UC San Diego"
    ]
}