{
    "paper_title": "Reasoning to Learn from Latent Thoughts",
    "authors": [
        "Yangjun Ruan",
        "Neil Band",
        "Chris J. Maddison",
        "Tatsunori Hashimoto"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Compute scaling for language model (LM) pretraining has outpaced the growth of human-written texts, leading to concerns that data will become the bottleneck to LM scaling. To continue scaling pretraining in this data-constrained regime, we propose that explicitly modeling and inferring the latent thoughts that underlie the text generation process can significantly improve pretraining data efficiency. Intuitively, our approach views web text as the compressed final outcome of a verbose human thought process and that the latent thoughts contain important contextual knowledge and reasoning steps that are critical to data-efficient learning. We empirically demonstrate the effectiveness of our approach through data-constrained continued pretraining for math. We first show that synthetic data approaches to inferring latent thoughts significantly improve data efficiency, outperforming training on the same amount of raw data (5.7\\% $\\rightarrow$ 25.4\\% on MATH). Furthermore, we demonstrate latent thought inference without a strong teacher, where an LM bootstraps its own performance by using an EM algorithm to iteratively improve the capability of the trained LM and the quality of thought-augmented pretraining data. We show that a 1B LM can bootstrap its performance across at least three iterations and significantly outperform baselines trained on raw data, with increasing gains from additional inference compute when performing the E-step. The gains from inference scaling and EM iterations suggest new opportunities for scaling data-constrained pretraining."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 4 2 ] . [ 1 6 6 8 8 1 . 3 0 5 2 : r a"
        },
        {
            "title": "Reasoning to Learn from Latent Thoughts",
            "content": "Yangjun Ruan1,2,3, Neil Band1, Chris J. Maddison2,3, Tatsunori Hashimoto1 1Stanford University 2University of Toronto 3Vector Institute {yjruan,cmaddis}@cs.toronto.edu, {nband,thashim}@stanford.edu"
        },
        {
            "title": "Abstract",
            "content": "Compute scaling for language model (LM) pretraining has outpaced the growth of human-written texts, leading to concerns that data will become the bottleneck to LM scaling. To continue scaling pretraining in this dataconstrained regime, we propose that explicitly modeling and inferring the latent thoughts that underlie the text generation process can significantly improve pretraining data efficiency. Intuitively, our approach views web text as the compressed final outcome of verbose human thought process and that the latent thoughts contain important contextual knowledge and reasoning steps that are critical to data-efficient learning. We empirically demonstrate the effectiveness of our approach through data-constrained continued pretraining for math. We first show that synthetic data approaches to inferring latent thoughts significantly improve data efficiency, outperforming training on the same amount of raw data (5.7% 25.4% on MATH). Furthermore, we demonstrate latent thought inference without strong teacher, where an LM bootstraps its own performance by using an EM algorithm to iteratively improve the capability of the trained LM and the quality of thought-augmented pretraining data. We show that 1B LM can bootstrap its performance across at least three iterations and significantly outperform baselines trained on raw data, with increasing gains from additional inference compute when performing the E-step. The gains from inference scaling and EM iterations suggest new opportunities for scaling data-constrained pretraining."
        },
        {
            "title": "Introduction",
            "content": "Human-written text is the culmination of an underlying thought processwhen we write, there is often an internal dialogue that clarifies or even determines the written word. However, modern language models (LMs) (Radford et al., 2019; Brown et al., 2020; OpenAI, 2023; Dubey et al., 2024) are pretrained directly on the final results of this process in highly compressed form (such as research papers). This may explain why LMs struggle with data efficiency and require large portion of the entire human-written web to learn (Kaplan et al., 2020; Hoffmann et al., 2022). Since the rate of growth in pretraining compute is far greater than that of the web itself (Villalobos et al., 2022; Muennighoff et al., 2024), we may soon enter data-constrained regime, motivating data efficiency approaches to extract more capabilities from limited web data. In contrast to LMs, humans learn very efficiently from the same compressed text, which suggests the possibility of significantly improving data-efficient pretraining. In this work, we focus on how we learn as one potential cause for this gap. For example, when we read research paper, we analyze specific claims, integrate them with prior knowledge, and attempt to decompress the authors original thought process. In other words, we use reasoning in service of learning, to infer the internal dialogue that undergirds the observed text. We refer to this procedureaugmenting the observed data with inferred, decompressed thoughts to enable more efficient learningas reasoning to learn. Equal advising Our code and data are available at https://github.com/ryoungj/BoLT 1 Figure 1: Reasoning to learn. (Left) Motivated by how humans apply deliberate thinking to learn from limited data, we train an LM to infer (or decompress) latent thoughts underlying the highly compressed observed data. These synthesized latent thoughts augment the raw observed data during pretraining, improving the LMs data efficiency. This procedure can be iteratively applied through an EM algorithm  (Fig. 5)  and form model self-improvement loop where increasingly capable LMs synthesize more effective latent thoughts, which in turn train more capable models. (Right) Our results demonstrate consistent improvement in model performance across bootstrap iterations. Inspired by this, we introduce an LM pretraining approach that implements this reasoningto-learn paradigm to improve data efficiency  (Fig. 1)  . Specifically, we approach language modeling from latent variable perspective, where the observed data depends on underlying latent thoughts Z. We train our LMs to learn from observed data augmented with the latents by modelling the joint distribution p(Z, X). The main challenge is synthesizing (and learning to synthesize) with latent thought generator q(Z X) (Fig. 2a). One key insight of our work is to observe that for natural language latent thought Z, the LM itself provides strong prior for producing latent thoughts (via its reasoning and theory-of-mind abilities (Wei et al., 2022b; Kojima et al., 2022)). This observation turns latent thought inference into synthetic data generation problem and has significant practical benefitsit allows us to leverage the strong capabilities of existing LMs, allows us to share weights between the LM and the latent thought generator, and simplifies training into small modification to the standard pretraining pipeline (Fig. 2b). We show that training model with latent thoughts enables it to produce higher-quality latent thoughts, allowing model to bootstrap its reasoning to learn ability with only small amount of initial supervision. We demonstrate this through simple ExpectationMaximization based approach which we refer to as Bootstrapping Latent Thoughts (BoLT) that enables an iterative improvement of the latent thought generator  (Fig. 5)  . Importantly, we show that BoLT can take advantage of additional inference compute to further improve data efficiency. In particular, the E-step in BoLT makes use of Monte-Carlo estimator that serves as non-parametric policy improvement operator, where the approximate posterior q(Z X) approaches the true posterior as the number of samples increases. We find in our experiments that BoLT is able to take advantage of additional samples (at least four) to improve its data efficiency and bootstrap its performance for at least three iterations, opening the possibility of new ways of scaling pretraining data efficiency. We validate the effectiveness of our approaches in improving model capabilities in dataconstrained setups. As testbed for data-efficient pretraining, our experiments continually pretrain 1.1B TinyLlama (Zhang et al., 2024) model on limited amount of data from FineMath (Lozhkov et al., 2024), reasoning-intensive web corpus. First, we demonstrate the potential of training LMs with data augmented with latent thoughts: when using GPT-4o-mini (Hurst et al., 2024) to synthesize latent thoughts, the 2 trained LM achieves 25.4% on MATH (Hendrycks et al., 2021b), significantly outperforming LMs trained on raw data (5.74%) or synthetic Chain-of-Thought (Wei et al., 2022b) style paraphrases (19.4%)  (Fig. 6)  . Furthermore, we show that our BoLT algorithm enables an LM to bootstrap its performance on limited data. In two distinct bootstrapping setups  (Fig. 7)  , we find LMs trained with self-generated latents improve across iterations on both likelihood metrics and MATH evaluations (Fig. 8 & Fig. 11). Crucially, these gains require no task-specific data and stem purely from improved latent thought quality across bootstrap iterations. Altogether, our results suggest that the powerful reasoning primitives of LMs may be leveraged to extract more capabilities from limited, task-agnostic data during pretraining."
        },
        {
            "title": "2 Related Work",
            "content": "Synthetic data for training LMs Recent work demonstrates the benefits of synthetic data in improving training efficiency, obtaining strong models at remarkably small data or model scales. Eldan & Li (2023) synthesize dataset of short stories with simple words and train very small LMs to generate coherent stories. Gunasekar et al. (2023) synthesize textbooks and exercises to train small LM with strong reasoning performance. Similar approaches have been adopted in follow-up works pretraining competitive small LMs (Li et al., 2023; Abdin et al., 2024a;b) or post-training LMs with synthetic supervised data to enhance their reasoning (Liu et al., 2023; Luo et al., 2023; Wei et al., 2023) or instruction following capabilities (Taori et al., 2023; Xu et al., 2023; Ge et al., 2024). These approaches carefully curate prompts and/or seed corpus to promote diversity in the generated corpus; failure to do so may lead to mode collapse (Martínez et al., 2023; Taori & Hashimoto, 2023; Alemohammad et al., 2023; Dohmatob et al., 2024; Kazdan et al., 2024). Another paradigm closer to our work is to augment raw pretraining data by paraphrasing (Maini et al., 2024) or describing relationships among extracted entities (Yang et al., 2025), which may avoid mode collapse due to the use of real data as seed corpus. Our work differs in two key aspects. First, we take latent variable perspective that augments (rather than replaces) each document chunk with latent thoughts Z. We show this is crucial to downstream performance, outperforming strong synthetic data generation baselines in Sec. 5.2. Additionally, our work studies bootstrapping of the data generator rather than the teacher-student distillation settings considered in most synthetic data approaches. Learning to reason using external supervision An increasingly popular area of work improves the reasoning capabilities of LMs using external supervision. Most paradigms rely on verifiable reward signal, enabling the use of reinforcement learning and/or self-play (Silver et al., 2016; 2017a; Trinh et al., 2024). This approach has been successfully applied to bootstrap reasoning capabilities in math and coding tasks with verifiable rewards (Zelikman et al., 2022; Huang et al., 2022; Singh et al., 2023; Jaech et al., 2024; Guo et al., 2025). An alternative approach uses supervised finetuning data to internalize verbalized chains-ofthought into continuous hidden states (Deng et al., 2024; Hao et al., 2024; Kong et al., 2025). Some recent works take latent variable inference perspective similar to ours, treating reasoning traces as latent variables and deriving variational inference (Hoffman et al., 2024; Hu et al., 2024a; Chen et al., 2024; Zhong et al., 2025) or Expectation-Maximization (Singh et al., 2023) approaches to optimization. Unlike our approach, these methods are domain-specific and rely on verifiable rewards or other external supervision. Learning to reason using pretraining data Closer to our setting, other works enhance LM reasoning capabilities using pretraining data and no explicit external rewards. Geiping et al. (2025) pretrain looped transformer (Dehghani et al., 2019; Giannou et al., 2023) on general web text and find that the continuous hidden states tend to converge with more applications of the backbone transformer, in thought-like process. Closest to our work among these is Zelikman et al. (2024) who propose domain-agnostic, post-training method that uses reinforcement learning to learn thought tokens to improve reasoning with pretraining data. While our work is also domain-agnostic and learns latent thoughts, our goal of leveraging reasoning to improve pretraining data efficiency is distinct and leads us to consider scalable, synthetic databased approaches rather than reinforcement learning ones. This leads to number of benefits, including simple training method and embarassingly parallel latent 3 (a) Learning by decompression (b) Training with next-token predictions Figure 2: Reasoning to learn with latent thought models (a) The latent thought model is trained to decompress plausible human thoughts underlying the observed data (i.e., q(Z X)) and to utilize the latent thoughts in learning more efficiently from the data (i.e., p(Z, X)), resembling deliberate human thought process. (b) The latent thought is modeled for each chunk of text in an autoregressive manner and in the same discrete text space. Given paired data {(Zn, Xn)}N n=1, we use standard next-token prediction to train single LM as both the p(Z, X) and q(ZX), by randomly placing Zn before or after Xn in the sequence. This strategy allows for minimal modifications to the standard LM pretraining pipeline. generation. Our work provides complementary perspective on how latent variable and synthetic data perspectives can enable both learning to reason and reasoning to learn. Lastly, Jiang et al. (2024) also adopt synthetic data approach, but focus on generating rationales for pretraining process reward model. Our work differs in our application of reasoning as primitive to enable data-efficient learning during pretraining, as well as our focus on enabling model to bootstrap its own latent thought synthesis abilities."
        },
        {
            "title": "3 Reasoning to Learn with Latent Thought Models",
            "content": "In this section, we introduce our reasoning to learn approach to pretraining LMs. Our key idea is to model the latent thoughts underlying the human data generation process, and train latent thought model both to reason about the latent thoughts underlying pretraining data and to learn from the data based on the synthesized latent thoughts."
        },
        {
            "title": "3.1 Latent Thought Models\nLearning by decompression Human-written data on the web is a compressed representation\nof an underlying thought process. For example, when Geoffrey Hinton wrote “GPT-4 is\nhumanity’s butterfly”1, he drew upon his knowledge of how GPT-4 was trained on a large\ncorpus of human-generated data and distilled human intelligence, and analogized this\nprocess to a butterfly’s metamorphosis. Understanding the context and reasoning steps\nbehind the observed data can facilitate deeper understanding of the text they generate,\nbut such reasoning traces are usually not explicitly presented in our pretraining data. Our\nhypothesis is twofold: augmenting pretraining data with underlying human thoughts can\nsignificantly improve its learning efficiency, and that this ability to infer latent thoughts can\nbe learned and improved after being trained with these thoughts during pretraining.",
            "content": "Formalizing latent thought models We formalize this from the perspective of latent variable models, as illustrated in Fig. 2a. We model the human thoughts underlying the observed data as the latent Z, and the generative process as joint distribution p(Z, X) = p(Z)p(X Z). Instead of training an LM to directly model the observed data p(X), we train it to both decompress the latent thoughts (i.e., approximate posterior q(Z X)) from the observed data and learn from the data using the synthesized latent thoughts (i.e., p(Z, X)); we call this LM latent thought model. The latent thought is modeled in the same discrete text space as the observed data (just as human thoughts can often be expressed in natural language). By augmenting the data with latent thoughts, 1The full quote is Caterpillars extract nutrients which are then converted into butterflies. People have extracted billions of nuggets of understanding and GPT-4 is humanitys butterfly. 4 You are provided with pair of web document prefix and suffix. Your task is to insert latent thoughts between them underlying the creation of the suffix conditioned on the prefix. The latent thoughts should include: the missing background knowledge and the reasoning traces underlying each claim (especially, stepby-step derivations or logical reasoning). [..omit..] (a) Prompt for GPT-4o-mini to synthesize latent thoughts. <StartOfLatent><Prior> Decentralization adjusts the data such that its mean is zero, which is crucial for PCA. If the data is not centered, the computed variances along the axes will be skewed, leading to misleading results in the identification of principal components. The principal component should ideally capture the largest variance, and uncentered data would cause the variance to reflect the mean of the data rather than the true distribution of the data points. [..omit..] <EndOfLatent> ## The first step, data decentralization The discussion of the new coordinate axis and the old coordinate axis is very troublesome. We directly decentralize the data (that is, the mean value of the data is at the far point). If the data is not decentralized, we cannot find the optimal dimensionality reduction. This step is necessary <StartOfLatent><Prior> To find the optimal axis for PCA, we utilize the covariance matrix of the decentralized data. The eigenvalues of this matrix indicate the amount of variance captured by each principal component, while the corresponding eigenvectors provide the direction of these components. The first principal component (PC1) is the eigenvector associated with the largest eigenvalue, representing the direction of maximum variance. The distance from the projected data points to the origin of the new axis (PC1) is calculated as the projection of the original data points onto this axis. The objective is to maximize the sum of squared distances ( d2 optimization problem can be framed as maximizing the Rayleigh quotient for the covariance matrix, leading to ), which mathematically quantifies the variance captured by the principal component. This the derivation of the eigenvalues and eigenvectors. <EndOfLatent> ## The second step is to find the new most marked axis How do we find the best most standard axis to achieve principal component analysis? That is, the greater the distance between the projected point and the origin of the coordinate axis, the better (this is the maximum variance) [..omit..] All we need to do is find the largest sum of squares (that's d2 ).[..omit..] (b) Example synthetic latent thoughts for document about PCAs mathematical derivation. Figure 3: We can use GPT-4o-mini to synthesize the latent thoughts to train latent thought models. The synthetic latent thoughts as shown in (b) typically contain the background knowledge and reasoning not explicitly stated in the raw data, presented in consistent and clean form. The prompt and example are simplified for clarity; see Prompt C.1.1 and Appx. D.2 for the full prompt and additional examples. the LMs learning process resembles the human process of reasoning to learn, where we deliberately think through the data to better absorb it. Importantly, we view latent thoughts as being encoded in natural language. This allows us to initialize q(Z X) using supervision from an existing model prompted to infer latent reasoning and background context, as well as to jointly model all conditional distributions for and using single, autoregressive language model. We describe the training and inference processes for latent thought models in the following."
        },
        {
            "title": "3.2 Training with Synthetic Latent Thoughts",
            "content": "We adopt an autoregressive generative model of latent thoughts that is compatible with standard language modeling. Given document X, we randomly chunk it into segments {Xn}N n=1 at the sentence boundaries and aim to infer the latent thought Zn underlying each chunk Xn conditioned on the previous context (see Fig. 2b, top). Synthesizing latent thoughts for training Human-generated data like internet text does not naturally come with underlying latent thoughts. Therefore, we need to synthesize the latent thought from some surrogate q(Z X) of the true posterior to augment the observed data for training the latent thought model. q(Z X) can either be instantiated as frontier 5 Problem: The positive difference between two consecutive perfect squares is 35. What is the greater of the two squares? <StartOfLatent><Prior> To solve the problem, we first need to find the two consecutive perfect squares. Let the two consecutive perfect squares be represented as (n2) and (n + 1)2. The difference between these squares is given by: (n + 1)2 n2 = 2n + 1. According to the problem, this difference equals 35: 2n + 1 = 35. Next, we isolate 2 = 17. by subtracting 1 from both sides: 2n = 34. Now, we divide both sides by 2 to solve for n: = 34 Thus, the two consecutive perfect squares are n2 = 172 = 289 and (n + 1)2 = 182 = 324. To find the greater of the two squares, we compare 289 and 324: 289 < 324. Therefore, the greater of the two squares is 324. <EndOfLatent> Final Answer: The final answer is 324. hope it is correct. Figure 4: Downstream transfer of latent thoughts. Since models have been trained on data augmented with latent thoughts at scale, they can be few-shot prompted to perform CoT reasoning in the latent space on downstream tasks (e.g., MATH). We provide the prompt in Appx. C.2 and additional examples in Appx. D.1. model, or the approximate posterior model q(Z X) itself for bootstrapping as in our EM algorithm. For example, we can prompt GPT-4o-mini (Hurst et al., 2024) to synthesize the latent thoughts by inferring missing reasoning steps or background knowledge (see Fig. 3 for the prompt and examples). Training latent thought models We develop simple method to train both joint model p(Z, X) and an approximate posterior model q(Z X) with only minor modifications to the standard LM training pipeline, as illustrated in Fig. 2b. In particular, since both and are presented in the same discrete text space, we train models with simple and standard next-token predictions. Given the synthetic latent thoughts paired with the observed data {(Zn, Xn)}N n=1, we can train both the joint and the posterior as the same model by formatting the data as conditional maximum likelihood estimation: we place Zn as the suffix after Xn in the sequence to train the approximate posterior q(Zn Xn), and place Zn as the prefix before Xn to train the joint p(Zn, Xn) (see the bottom of Fig. 2b). We format the data in these two modes with random coin flip and use two special tokens <Prior> and <Post> to differentiate them. All latents are wrapped within the special <StartofLatent> and <EndofLatent> tokens to differentiate them from the raw observed data (Fig. 3b). The formatted data can be directly fed into the standard LM pretraining pipeline to train both the joint and the posterior with next-token predictions."
        },
        {
            "title": "3.3 Prediction with Latent Thoughts",
            "content": "At prediction time, the standard language modeling task involves sampling continuation p(X X), which has straightforward analog with latent thought models. In our case, prediction involves sampling from the marginal distribution Ep(ZX)[p(X Z, X)]. Beyond this, however, we find that there are interesting choices when we perform few-shot, CoTstyle prompting during evaluation. Reasoning with CoT in the latent space Recall that during chain-of-thought prompting (Nye et al., 2021; Wei et al., 2022b), we augment the few-shot examples from dataset with an explicit reasoning chain. For standard LMs, these thoughts are simply part of the text prompt. However, in the case of latent thought models, we now have the more natural option of putting the CoT prompts into the latent space Z. Concretely, this takes the form of changing the formatting of the few-shot CoT examples: given question and its answer A, we wrap the CoT within the special tokens (<StartofLatent> and <EndofLatent>) and add the <Prior> prefix to indicate that this reasoning should occur in the latent space (see Appx. C.2 for few-shot examples). We find that latent thought models are highly effective latent CoT reasoners at inference time (see Fig. 4 for an example), achieving better downstream performance than when reasoning in the observed text space X. 6 Figure 5: Bootstrapping latent thoughts (BoLT) with iterative Expectation-Maximization. We bootstrap LMs with self-generated latents in an iterative EM algorithm. In the E-step, we use Monte Carlo sampling as policy improvement operator to obtain higher-quality latents. This boosts learning efficiency in the M-step, enabling the training of more capable LMs that synthesize better latent thoughts."
        },
        {
            "title": "4 Bootstrapping Latent Thought Models",
            "content": "The data efficiency of training latent thought models relies on the surrogate posterior q(Z X) used to synthesize the latent thoughts. This limits the potential of our approach in advancing frontier model capabilites, as the performance ceiling of the final model is bounded by the capability of q(Z X). To overcome this limitation, we introduce an Expectation-Maximization (EM) algorithm called Bootstrapping Latent Thoughts (BoLT), illustrated in Fig. 5. The key idea is to instantiate q(Z X) with non-parametric, enhanced version of the models approximate posterior q(X Z) via Monte Carlo sampling. This enables self-improvement loop, where more effective latents are synthesized at the E-step, leading to more capable models with improved learning efficiency at the M-step, which in turn improve latent quality at the next E-step."
        },
        {
            "title": "4.1 Expectation-Maximization with Monte Carlo Sampling\nAt each iteration t in EM, we have access to the current latent thought model Mt. This\nmodel can be instantiated as both the approximate posterior q(Z | X; Mt) and the joint\np(Z, X; Mt) through the same data formatting and special token usage used during training\n(Sec. 3.2). A naive instantiation of EM would be to alternate between sampling latents Z\nfrom the current posterior q(Z | X; Mt) in the E-step and training a model with the sampled\nlatents Z in the M-step. However, it is unclear that training a model on its self-generated\nlatents can improve it. We address this gap with Monte Carlo sampling, which induces a\nsurrogate posterior ˜q(Z | X; Mt) that is provably better than the current one to serve as a\n“policy improvement operator”. We detail the procedure of our EM algorithm below.",
            "content": "E-step: Synthesizing better latents with Monte Carlo sampling At the E-step, we sample latents {Z(k)}K k=1 from the current posterior q(Z X; Mt) and compute their importance weights w(k) = p(Z(k),X;Mt) q(Z(k) X;Mt) . Intuitively, w(k) upweights latent that is both simple (high p(Z(k); Mt)) and predictive of the data (high p(X Z(k); Mt)), and downweights latent that is too obvious (q(Z(k) X; Mt) in the denominator). Then, we resample one latent from the categorical distribution proportional to the importance weights, i.e., Cat(w(k)), and use Z(j) as the final synthesized latent. It is known that this procedure samples from posterior q(Z X; Mt) that achieves the importance-weighted evidence lower bound (ELBO), which is tighter than the naive ELBO (Burda et al., 2015; Cremer et al., 2017): (cid:20) (cid:21) log p(X; Mt) q(Z X;Mt) log (cid:34) p(Z, X; Mt) q(Z X; Mt) k=1 p(Z, X; Mt) q(Z X; Mt)"
        },
        {
            "title": "1\nK",
            "content": "log q(Z X;Mt) (cid:20) {Z(k)}K k= Zq(Z X;Mt) log 7 (cid:35) p(Z(k), X; Mt) q(Z(k) X; Mt) (cid:21) . (1) Algorithm 1: Bootstrapping Latent Thought Models (BoLT) Input: Base model Mb, warmstart data (X0, Z0), raw data corpora {Xt}T Parameters: EM iterations T, Monte Carlo samples K, context window size Output: Bootstrapped model MT t=1 /* Initialize with warmstart data M0 = TrainLM (Mb, (X0, Z0)) */ Train with next-token predictions as in Fig. 2b /* Iteratively train the latent thought model with EM, as illustrated in Fig. 5 for = 1, 2, , do /* E-step: synthesize latent thoughts with the current model for Xt do {Xn}N for = 1, 2, , do Cn = X[nL:n1] (cid:111)K (cid:110) n=1 = ChunkData(X) Randomly chunk the raw text into chunks Set context window (k) w q(Z Xn, Cn; Mt1) Sample latents from the model posterior k=1 (k) (k) ,Xn Cn;Mt1) = p(Z (k) Xn,Cn;Mt1) q(Z (j) , Cat Resample with importance weights Weight by the model likelihood (cid:18)(cid:110) (k) (cid:111)K (cid:19) Zn = k=1 */ */ end (Xt, Zt).append (cid:16) {Xn}N n=1 , {Zn}N n=1 (cid:17) Augment data with synthesized latents end /* M-step: train the model with bootstrapped thought data if retrain from scratch (cid:26) Mb Minit = Mt1 otherwise */ Mt = TrainLM (Minit, (Xt, Zt)) Train with next-token predictions as in Fig. 2b end return MT The bound is tighter with more samples and approximates the true posterior when . Crucially, this procedure offers potential way to improve the training efficiency of LMs by scaling up the inference compute to select better latent from more samples. M-step: Training the model with bootstrapped latents At the M-step, we use the latents synthesized by the current model Mt and paired them with raw data on the corpus Xt+1 to train the next iteration of model Mt+1, following the same training procedure described in Sec. 3.2. Note that the new posterior q(Z X; Mt+1) is trained to approximate the better posterior q(Z X; Mt) and the new joint p(Z, X; Mt+1) is trained by augmenting the data with bootstrapped latents from q(Z X; Mt)."
        },
        {
            "title": "4.2 Full algorithm",
            "content": "The full algorithm of our approach is described in Algorithm 1. Since the initial base LM has not been trained as latent thought model, we first train it on small corpus of warmstart data with synthetic latent thoughts (e.g., by GPT-4o-mini). The warmstart iterative EM loop resembles the supervised finetuning reinforcement learning pipeline (Ouyang et al., 2022) that is widely adopted in LM post-training, though our approach operates purely on task-agnostic pretraining data instead of supervised data. Our algorithm can be instantiated in different setups depending on the sources of data for bootstrapping: we may either have new corpus of data Xt at each iteration where we can initialize the training of Mt from Mt1, or we may have fixed corpus (with all Xt being the same) and retrain the model from scratch on better latents at each iteration; see Algorithm 1 for details. 8 Figure 6: Training on raw data augmented with synthetic latent thoughts significantly boosts LM data efficiency. We study data-constrained setup where TinyLlama-1.1B model is continually pretrained on 480M FineMath tokens with an 8B training token budget. GPT-4o-mini generates synthetic latent thoughts (as proxy for human ground-truth thoughts) for the corpus. This augmented data (Latent Thought) leads to substantial gains over both training on the same amount of 8B fresh raw tokens (Raw-Fresh) and paraphrased data with synthetic reasoning traces presented in the raw text (WRAP-CoT). The synthetic token budget for all synthetic data methods is fixed to 1.1B tokens. The advantages of our method are even more pronounced under prompt variations (see Fig. A.2) and when normalized by the effective raw tokens seen by each method (Fig. A.3)."
        },
        {
            "title": "5 Synthetic Latent Thoughts Improve LM Data Efficiency",
            "content": "In this section, we first demonstrate that the data efficiency of LMs can be significantly improved through joint training on observed data and latent thoughts. We show this by using capable LM (GPT-4o-mini) as the surrogate posterior q(X Z) to synthesize latent thoughts over fixed corpus. We use these synthesized latent thoughts to augment the corpus for training latent thought model and compare its performance against several baseline approaches."
        },
        {
            "title": "5.1 Experimental Setup",
            "content": "Continued pretraining on reasoning-intensive corpus In our setup, we conduct continued pretraining (CPT) of the TinyLlama-1.1B model (Zhang et al., 2024) on the reasoningintensive corpus FineMath-4+ (Lozhkov et al., 2024). While reasoning to learn is domainagnostic, we choose this setup to obtain meaningful downstream accuracy readouts under an academic budget. We choose TinyLlama-1.1B because it has not been specifically trained on mathematical reasoning data, enabling clean comparisons. To assess the data efficiency of various methods, we adopt data-constrained setup by fixing the total number of unique raw tokens for CPT to 480M. We fix total CPT compute budget of 8B tokens, with methods training on additional synthetic data and/or for multiple epochs on the raw tokens. Synthetic generation of latent thoughts We use GPT-4o-mini as the alternative q(Z X) and generate latent thoughts over the CPT corpus. We prompt the model with Prompt C.1.1 and temperature 0.7. We split each document into chunks of sentences, where each chunk contains random number of sentences following Poisson distribution with mean of 8, truncated between 1 and 20. We generate one latent for each chunk using the previous = 3 chunks as the prefix context. Overall, the token ratio between synthetic and raw tokens is 2.3:1 (1.1B synthetic tokens), and under the 8B token compute budget, we CPT on the thought-augmented data for 5 epochs. 9 Table 1: Downstream performance of different methods and ablation studies. For our method, it is crucial to embed the synthetic thoughts in latent space separated from the observed raw text X, and to utilize for CoT reasoning during downstream evaluation."
        },
        {
            "title": "Data",
            "content": "Raw-Repeat Raw-Fresh WRAP-Orig WRAP-CoT Latent Thought (Ours) - mixing latents in raw text space for training - using CoT in raw text space for eval MATH GSM8K MMLU-STEM 5.74 11.18 11.06 19.36 25.38 22.38 20.34 5.76 13.27 12.43 21. 33.59 20.17 22.97 27.31 30.63 31.40 34.51 35.87 33.33 31.78 Baselines We compared our approach with several natural baselines in the dataconstrained regime (Muennighoff et al., 2024; Yang et al., 2025) using combination of data repetition and synthetic data generation: Raw-Repeat: We train on the raw CPT corpus for 16 epochs, with early stopping to prevent overfitting. Raw-Fresh: As an anticipated upper bound, we CPT on 8B unique tokens from FineMath4+, without any repetition. WRAP-Orig: WRAP (Maini et al., 2024) rephrases the data in four diverse styles: easy (with simple language), hard (with complex language), Wikipedia (high-quality), and question-answer. We prompt GPT-4o-mini (see Prompt C.1.2) with temperature 0.7 to rephrase each document in these styles. The average token ratio between synthetic paraphrases and raw tokens is 0.48:1, so we generate 5 paraphrases per document to approximately match the total synthetic tokens of our approach. While Maini et al. (2024) found that mixing synthetic and raw data works best, we tuned the mixture coefficient and found that entirely synthetic works best (see Fig. A.1a). WRAP-CoT: To probe whether the gains of our approach arise from simply including synthetic reasoning traces in the training data, we develop WRAP variant that prompts GPT-4o-mini to rephrase documents while incorporating detailed reasoning steps (see Prompt C.1.3). This strong WRAP baseline allows us to assess whether it is advantageous to maintain thoughts in separate latent space to explain the corresponding text, rather than directly in the raw text space. The synthetic-raw token ratio is 0.7:1, so we generate 4 paraphrases per document for total of 1.3B synthetic tokens. As above, we tuned the synthetic-raw mixture coefficient and found entirely synthetic works best (see Fig. A.1b). Training We CPT using AdamW (Loshchilov & Hutter, 2019) with β1 = 0.9, β2 = 0.95, and weight decay of 0.01. We use cosine learning rate schedule with 1000 step warmup and peak learning rate of 1e-4 (tuned over {1e-5, 3e-5, 1e-4, 3e-4, 1e-3} in initial experiments). All models are trained with sequence length 2048 and batch size 96 on 4 H200 GPUs. Evaluation We evaluate the continually pretrained LMs on the popular reasoning benchmarks MATH (Hendrycks et al., 2021b), GSM8K (Cobbe et al., 2021), and MMLU-STEM (Hendrycks et al., 2021a). We use few-shot CoT prompting (Wei et al., 2022b) for all three benchmarks. To ensure robust evaluation, we evaluate with two distinct sets of few-shot CoT prompts (see Appx. C.2): (1) Standard prompts from previous works, i.e., Minerva CoT (Lewkowycz et al., 2022) for MATH, the default CoT from (Wei et al., 2022b) for GSM8K, and the FLAN CoT (Wei et al., 2021) for MMLU-STEM; (2) Synthetic CoT prompts, where the CoT traces are synthesized by GPT-4o-mini using Prompt C.1.1, using the question as the prefix and answer as the suffix. We report the results with synthetic CoT prompts by default, as they performed better uniformly across methods (see Fig. A.2). The few-shot CoT traces were placed in the latent space for our method, and in the raw text space for baselines. All models are evaluated with temperature 0.0. We conducted all evaluations using the 10 (a) Bootstrapping on fixed data (b) Continual bootstrapping Figure 7: Illustration of our two bootstrapping setups. (a) We study the scientific question of whether an LM can self-improve on fixed corpus by iteratively generating higherquality latent thoughts that in turn train more capable model. (b) We study more practical scenario in which an LM is continually pretrained on series of corpora, using the model at each iteration to generate latent thoughts for the subsequent corpus to train itself. LM Evaluation Harness (Gao et al., 2024), and the Math-Verify evaluator (HuggingFace, 2025) for scoring MATH final answers against the ground-truth."
        },
        {
            "title": "5.2 Results\nTraining with synthetic latent thoughts improves data efficiency over baselines Fig. 6\nand Table 1 show downstream performance during CPT for various methods. Training with\nsynthetic latent thoughts substantially outperforms all baselines, even outperforming train-\ning on an equivalent amount of unique raw tokens (“Raw-Fresh”). While rephrasing-based\nsynthetic data generation methods do improve over raw data baselines—particularly the\nvariant incorporating reasoning steps (“WRAP-CoT”)—they still considerably underper-\nform our approach. Moreover, our method is more robust to prompt variations compared\nto baselines that suffer considerable degradation with standard CoT prompts (see Fig. A.2).\nNotably, since our method trains jointly on synthetic latent thoughts and raw data (at a 2.3:1\ntoken ratio), it achieves better performance while seeing the raw data fewer times (3.3x) than\nthe baselines. We include a comparison normalized by the effective raw tokens seen by each\nmethod in Fig. A.3, where our method demonstrates even more significant gains.",
            "content": "Learning and utilizing latent thoughts in separate latent space is critical key distinction between our approach and baselines (such as WRAP-CoT) is that we model thought tokens in latent space separate from the observed text X. We hypothesized that separating the thoughts would improve performance by explicitly handling the language modeling task from the thought process (3.3). We ablate this design in Table 1. First, we demonstrate the importance of jointly modeling thought and document chunk pairs (Z, X). We test variant of our approach that does not separate the latent thoughts from the raw text and mixes them with randomly sampled (unpaired) text chunks at 1:1 ratio. This variant (second-to-last row) performed comparably to WRAP-CoT but significantly worse than our full method, demonstrating that our gains primarily stem from the latent model design rather than merely from the quality of synthetic thought data. Furthermore, we assessed the effectiveness of reasoning with CoT in the latent space during downstream evaluation. For our trained latent thought models, we instead provide all few-shot CoT prompts in the raw text space similar to the baselines and suppress the generation of all special latent tokens during evaluation. This evaluation-time intervention degrades performance (last row), demonstrating the importance of explicitly utilizing the latent thoughts during downstream evaluation."
        },
        {
            "title": "6 LMs Can Self-Improve by Bootstrapping Their Latent Thoughts",
            "content": "Now we have demonstrated that training jointly on raw texts and accompanying latent thoughts synthesized by powerful LM significantly improves data efficiency. In this section, 11 Figure 8: BoLT bootstraps the performance on fixed raw data. We instantiate the setup illustrated in Fig. 7a with bootstrapping on fixed corpus of 1.92B raw tokens and warmstarting with 240M raw tokens. Performance gains are consistently observed in both the smoother ELBO evaluation (left) and downstream MATH evaluations (right, up to the third iteration), see Fig. B.2 for detailed results and robust gains across prompt variations. The improvements on downstream tasks remain robust in fine-tuning evaluations and persist through the fourth iteration (see Fig. 9). Results are over 5 training runs for bootstrapping and 3 runs for baselines to reduce variability. we take step towards LMs that can self-improve on limited pretraining data, by investigating whether the BoLT algorithm enables monotonic improvement across iterations. We consider two experimental setups: one designed to scientifically understand BoLT, and another that better resembles practical use case: Bootstrapping on fixed data: we fix single training corpus and investigate whether BoLT can enable an LM to iteratively generate higher quality latents. This setup is outlined in Fig. 7a: We fix all training corpora Xt = , and retrain the model Mt from scratch in Algorithm 1. With limited amount of fixed data, the final model performance is purely determined by the quality of synthesized latent thoughts, enabling controlled study. Continual bootstrapping: An LM is continually trained on series of corpora, using the LM at each stage to generate latent thoughts to accompany the subsequent corpus. We study whether the increasingly capable model produces higher quality latents that lead to better trained models, compared to using latents from earlier, less capable versions of itself. This setup is outlined in Fig. 7b, using different training corpora Xt for each iteration and initializing the training of Mt from Mt1."
        },
        {
            "title": "6.1.1 Experimental Setup\nBootstrapping configuration We use TinyLlama-1.1B as the base model Mb and fix the\ntraining corpus as 1.92B raw tokens from FineMath-4+. For the warmstart data X0, we use\na separate corpus of 240M raw tokens with GPT-4o-mini synthetic latents as Z0 (∼ 550M\nsynthesized tokens), following Sec. 5.1. We train the warmstart model M0 for 1 epoch on\n(X0, Z0). At each bootstrap iteration t, we use the current trained model Mt−1 to instantiate\nthe posterior q(Z | X; Mt−1) at temperature 1.0, from which we sample latents for each\ntext chunk (with chunks split following Sec. 5.1). We sample K = 4 latents by default and\nselect one by sampling proportional to their importance weights (Sec. 4), which we found\nto provide decent performance gains while maintaining a reasonable cost for synthetic\ndata generation. We distribute the latent generation workload over a cluster of H100/200,",
            "content": "12 Figure 9: The performance gains of BoLT across multiple iterations and over raw data baselines remain robust in finetuning evaluations. For MATH, the gains persist through the fourth iteration, whereas they appear to plateau after the third with few-shot prompting (Fig. B.2). On GSM8K, BoLT demonstrates clear gains in the second iteration and appears to plateau in later iterations, possibly due to diminishing marginal benefits of further optimization on math-heavy corpus for solving simpler math word problems. Standard errors are over 3 training and 5 finetuning runs, totaling 15 seeds. A100/6000/5000/40, L40, and RTX3090 GPUs. The synthetic:raw token ratio is 2.4:1, and we train LMs on the augmented data for 2 epochs at each iteration, corresponding to 13B total training tokens. We follow the training configuration in Sec. 5.1, except we now use batch size 192 on 8 H200 GPUs for bootstrapping training runs. Evaluation Because of high run-to-run variability at our scale, we require high signal-tonoise ratio evaluation metrics that differentiate model quality across bootstrap iterations. We use both likelihood-based measures and downstream metrics: Likelihood-based measures: Likelihood (or perplexity) measures are standard metrics for evaluating pretrained LMs, providing smoother assessment than downstream metrics. We use holdout set of 1,000 documents from FineMath-4+ to measure test likelihoods. For latent thought models, we measure the multiple-sample ELBO (i.e., IWAE, Burda et al., 2015) as in Eq. (1) for tighter bound. In particular, we chunk each document as in training, measure the ELBO with 4 samples for each chunk, and compute the ELBO per token. While ELBO could be loose bound on the true negative log-likelihood (NLL), in our setting, we provide this as diagnostic metric that demonstrates that our model self-improves predictably with respect to our chosen training metric. We also measure the NLL directly on the raw data (which is out-of-distribution for latent thought models) for reference and observe similar results as the ELBO (see Fig. B.1). Downstream metrics: We evaluate performance on MATH and GSM8K at each bootstrap iteration with few-shot CoT prompting, following Sec. 5.1. We exclude MMLU-STEM as performance fell within the noise floor (<28%). Because few-shot prompting performance may be confounded by the LMs in-context learning ability (Dominguez-Olmedo et al., 2025), we also perform finetuning-based evaluations that measure the model performance after being finetuned on respective training splits. Specifically, we take the training set for MATH & GSM8K and finetune on this set. We synthetically generate CoT traces for the finetuning set using our prompt in Prompt C.1.1, by treating the question as the prefix and the answer as the suffix. We finetune using AdamW with standard hyperparameters β1 = 0.9, β2 = 0.95, weight decay 0, and cosine learning rate schedule with warmup over the first 5% of steps. We split out 10% of the train set as validation set and tune 13 Figure 10: More Monte Carlo samples lead to improved latent quality and better trained models. We vary the number of MC samples for latent generation at the first bootstrap iteration, and evaluate the final model performance trained with augmented data over 3 training runs. Results indicate that generating additional MC samples with more inference compute results in higher quality latents and contributes to more efficient model training. The gains remain robust on GSM8K and across prompt variations, see Fig. B.4. hyperparameters to obtain learning rate of 1e-4, batch size 64, and 5 epochs. We report test accuracy of the final model checkpoint with CoT, using standard sampling hyperparameters top_k= 50, top_p= 0.9, and temperature 0.6. Due to the small test set size of GSM8K, we follow Guo et al. (2025) and calculate the mean accuracy over 16 random samples for each test question to reduce variance. To reduce variability, we conduct 5 training runs for bootstrapping models and 3 runs for baselines with different seeds, and report the means and standard errors. For clear comparison across iterations, we plot the best performance achieved during training for each run. The exact performance curves are included in Fig. B.3, showing similar relative comparisons but with higher variance. For finetuning evaluations, standard error is over 3 upstream CPT 5 downstream finetuning = 15 total seeds."
        },
        {
            "title": "6.1.2 Results\nBoLT enables monotonic self-improvement across multiple iterations Fig. 8 shows the\nmodel performance after each bootstrap iteration. Our results demonstrate that BoLT consis-\ntently improves performance over multiple iterations in both ELBO and downstream MATH\naccuracy, forming a self-improvement loop where more capable models generate higher\nquality latents that further enhance model capabilities. Performance improvements remain\nclearly observable on the smoother likelihood-based metrics (ELBO and NLL (Fig. B.1))\nthrough the fourth iteration. Downstream MATH performance shows consistent improve-\nment through the third iteration, robust across prompt variations (Fig. B.2), with diminishing\nreturns by the fourth. This plateau may be due to the discontinuous nature of few-shot\nprompting evaluations (Wei et al., 2022a; Schaeffer et al., 2023) that can mask smaller im-\nprovements; after task-specific finetuning, gains persist through the fourth iteration (Fig. 9).\nWe speculate that the number of iterations before saturation could correlate with scale, as our\npreliminary smaller-scale experiments showed few-shot performance plateauing after the\nsecond iteration. Qualitatively, we observe instances where the latents at earlier iterations\nmake mistakes that are corrected in later ones: Example D.3.3 contains an example.",
            "content": "Training on self-generated latents outperforms training on raw data We compare the downstream MATH performance of BoLT-trained models with those trained on raw data  (Fig. 8)  . We include two baselines: FLOP-matched baseline trained for the same total training tokens as ours (with 3.4 more passes over the corpus than ours), and raw tokenmatched baseline trained on the same count of raw tokens as ours (2 epochs over the corpus). The results demonstrate that BoLT-trained models significantly outperform both baselines, demonstrating the effectiveness of reasoning to learn with self-generated 14 latents for improved data efficiency. These gains persist in finetuning evaluations on MATH & GSM8K  (Fig. 9)  . Examples of self-generated latents are provided in Appx. D.3. Synthesizing latents with more Monte Carlo samples improves data quality key component of our approach is the use of Monte Carlo sampling: we draw multiple samples and reweight them to select one, inducing an improved posterior and serving as policy improvement operator. Now we study the impact of the number of MC samples on the quality of synthesized latents. We synthesize latents with = 1, 2, 4, 8 at the first iteration and train models on data augmented with these latents; all other hyperparameters are fixed. Fig. 10 compares performance across MC samples. We find that the ELBO and downstream MATH accuracy improve monotonically in the number of MC samples, demonstrating potential avenue for scaling inference compute in improving pretraining efficiency. Potential side-effects of bootstrapping We note negative result where few-shot prompted GSM8K performance deteriorates over multiple BoLT iterations (while still outperforming the raw data baselines) in Fig. B.5. However, this degradation is not present after task-specific finetuning (Fig. 9, right). potential explanation is that CPT on the math-heavy FineMath corpus worsens natural language understanding and in turn the models ability to interpret GSM8K word problems (examples provided in Appx. D.4). It is possible this effect was observed to lesser extent in MATH evaluations, where performance plateaus in few-shot evaluations but not in finetuning. We tested this hypothesis by evaluating BoLT models on validation set from the general-domain DCLM corpus (Li et al., 2024) (Fig. B.5 right), where more optimization on FineMath tends to degrade DCLM NLL. Such potential side-effects warrant more extensive investigation into latent thought models beyond reasoning-heavy domains in future work."
        },
        {
            "title": "6.2 Continual Bootstrapping",
            "content": "Our previous experiment confirms that BoLT leads to iterative improvement of the latent thought model. However, re-training the latent thought model from scratch at each iteration could be wasteful, and more realistic setting would be to perform those experiments as part of continual learning, where the model updates its latent thought generator as it processes more and more data. We show that BoLT continues to work in this setting."
        },
        {
            "title": "6.2.1 Experimental Setup\nBootstrapping configuration We use TinyLlama-1.1B as the base model Mb and 240M\nraw tokens from the FineMath-4+ dataset as the warmstart data X0. We conduct T =\n4 bootstrap iterations, where each iteration t uses a distinct subset of the FineMath-4+\ndataset with logarithmically increasing size as Xt—960M, 960M, 1.92B, and 3.84B raw\ntokens respectively. Models are trained for 1 epoch on the warmstart data and 2 epochs on\nthe bootstrap data at each iteration. To mitigate forgetting and model degradation during\ncontinual training, we use the warmup-stable-decay (WSD, Hu et al., 2024b) schedule. In\nparticular, the learning rate is warmed up for 1000 steps only at the warmstart stage and\nthen maintained at a constant value until the decay phase. For each training stage (including\nboth warmstart and bootstrap iterations), we linearly decay the learning rate during the\nfinal 15% of training steps, where the final checkpoint is obtained as Mt for evaluation and\nsynthetic latent generation. When training Mt+1 at next iteration, we initialize both the\nmodel and the optimizer states from the pre-decay checkpoint, and continue training with\nthe same constant learning rate without re-warming up. The learning rate is set to 3e-5,\nwhich was tuned in our preliminary experiments to mitigate forgetting and achieve stable\ntransitions across iterations. All other configurations follow Sec. 6.1.1, such as the use of 4\nMC samples for synthetic latent generation.",
            "content": "Baseline comparison The key question to answer in this setup is whether iterative improvement of the latent thought model leads to performance benefits. To understand this, we compare to baseline where we stop iterative improvement at time t, fixing the latent thought generation model. We investigate this by comparing our bootstrapped models (that use the most recent and capable model Mt to synthesize latents for Mt+1 at each iteration) against the alternatives of fixing the latent generator at previous iteration Mt for training subsequent models Mt, > t. We conduct 3 training runs for each training 15 Figure 11: BoLT can bootstrap in continual learning settings. We instantiate the setup illustrated in Fig. 7b by training on series of corpora and compare two options: bootstrapping the latent model across all four iterations (blue), or fixing the latent generator at earlier iterations. Our bootstrapped models lead to steeper scaling trend in likelihoodbased metrics (left) and consistently outperform the fixed-latent-generator baselines on downstream MATH performance (right), with gains becoming more pronounced at later iterations. Results over 3 training runs are reported. Due to the log-sized scaling of the x-axis, the scaling trends here are comparable to traditional data-scaling law. See Fig. B.6 for additional results and Fig. B.7 for detailed evaluations over the training runs. setup to reduce the run-to-run variability and measure the models performance at each iteration following the same evaluation protocol described in Sec. 6.1.1."
        },
        {
            "title": "7.1 Broader Implications",
            "content": "Learning to reason and reasoning to learn Recent RL-based approaches such as o1 (Jaech et al., 2024) and R1 (Guo et al., 2025) learn to reason with LMs, using supervised data and verified rewards to unlock the reasoning capabilities of strong pretrained model. In contrast, our work demonstrates the potential of reasoning to learn, by training model to learn more from observed data by reasoning through it. We show promising results on one reasoning-heavy domain, but the likelihood-based supervision makes it applicable to more general domains, and we believe the same approach can enable more data-efficient learning of capabilities at scale. These two paradigms are complementary, and the base 16 models trained with our approach may serve as better starting point for task-specific RL by transferring their thoughts trained at scale. Using asynchronous synthetic data generation to improve synchronous training efficiency Our and recent works (Javaheripi et al., 2023; Maini et al., 2024; Yang et al., 2025) demonstrate that synthetic data generation can enable improved scaling trends for synchronous pretraining on centralized compute. Because synthetic data generation can be distributed across disparate resources (Silver et al., 2017b), this approach shifts portion of the overall synchronous pretraining compute to an asynchronous workload. This trend may inspire changes in infrastructure design for LM pretraining, enabling the effective use of distributed resources with low-bandwidth interconnects. Additionally, developing more scalable synthetic data generation techniques, such as increasing Monte Carlo samples in BoLT, could enable another axis for scaling the asynchronous compute used in pretraining."
        },
        {
            "title": "7.2 Limitations\nConstrained experimental setup under compute budget Due to our compute budget, our\nexperimental setup was constrained to a 1B parameter LM and continued pretraining on a\nfew billion tokens of mostly mathematical text. These choices were made to demonstrate a\nproof-of-concept; our use of a small model enables faster inference to synthesize billions of\nlatent tokens, while continued pretraining on reasoning-intensive data enables measurable\ndifferences among methods in downstream evaluations. We are hopeful that future work\nwill test reasoning to learn at larger scales and on general-domain pretraining data.",
            "content": "Limited exploration of design choices We focused on particular instantiation of reasoning to learn without extensive testing of design choices such as: the generative structure of latent thoughts (currently latents are modeled autoregressively for each text chunk), the initial warmstart data generated by different models or prompts, more efficient Monte Carlo sampling techniques such as Sequential Monte Carlo (Doucet et al., 2001), and various hyperparameters like the chunk size, etc. Side-effects of bootstrapping Bootstrapping LMs on their own synthetic data at scale may amplify specific bias and ultimately lead to unintended consequences in model behavior. We observed one possible example of such side effects: the few-shot CoT performance on GSM8K degraded with BoLT iterations, though an alternative explanation is that more optimization on mathematical tokens may degrade the language understanding necessary in GSM8K (see Fig. B.5). Future work should more extensively investigate the side effects of bootstrapping during pretraining."
        },
        {
            "title": "7.3 Future Directions\nBootstrapping on general-domain pretraining data We believe the most exciting appli-\ncation of our approach could be in enabling models to self-improve on general-domain\npretraining-scale data without task-specific supervision. This is in stark contrast to the\nrecent approaches which improve pretrained LMs with reinforcement learning on task-\nspecific labeled data (Jaech et al., 2024; Guo et al., 2025). By bootstrapping LMs with latent\nthoughts on pretraining-scale data, we believe it may be possible for models to acquire more\ngeneral-domain reasoning capabilities that are useful beyond specific domains.",
            "content": "Application to general data modalities Our approach is not limited to text data and may be applied to general data modalities. This is because every piece of human-generated data is result of an underlying latent generative process that is typically not observable. For example, when working with video data, one could train multimodal latent model to extract the creative intent or emotional pacing behind scene transitions, rather than focusing solely on pixel-level features. For non-textual data, these underlying latent structures may be even more obscure, and models trained on such data typically demonstrates much worse data efficiency than LMs (Brooks et al., 2024). Consequently, augmenting these data modalities with learned latents could possibly yield more pronounced gains in data efficiency than observed in the text domain. Hierarchical latent structures Our current instantiation models the latent generation process in an autoregressive manner, where each latent is generated for single chunk of text conditioned on the previous context. While we have demonstrated the effectiveness of this 17 approach on reasoning-intensive data, this local latent thought structure remains inherently myopic. It may not be sufficiently expressive to capture the hierarchical planning processes that humans employ when creating complex, long-form content such as research papers, novels, and large-scale codebases. Future work could explore more sophisticated latent structures that mirror human planning hierarchies, potentially incorporating both high-level planning and low-level reasoning."
        },
        {
            "title": "8 Conclusion",
            "content": "We have introduced reasoning to learna new approach to data-efficient LM pretraining by deliberately thinking through the observed data. We show that training LMs with synthetic latent thoughts significantly improves learning efficiency and downstream performance in data-constrained setups. Moreover, we instantiate an EM algorithmBootstrapping Latent Thoughts (BoLT)which enables model self-improvement, where more capable models synthesize higher-quality latents that in turn enable greater learning efficiency. We extensively validate BoLT with continued pretraining on math data, and show that BoLT demonstrates steeper scaling and consistent downstream gains across multiple bootstrap iterations. Our work demonstrates the promise of explicit inference of underlying latent thoughts in improving data efficiency on task-agnostic pretraining data. Acknowledgements We particularly thank Suhas Kotha and Yu Sun for many helpful discussions throughout the project. We also thank Christina Baek, Haonan Duan, Konwoo Kim, Ken Liu, Christopher Mohri, Anvith Thudi, Zitong Yang, Tianyi Zhang, and the Hashimoto Group for their helpful discussions or feedback on the paper draft. NB acknowledges funding from an NSF Graduate Research Fellowship and Quad Fellowship. TH was supported by HAI seed grant, DSO labs, gifts from Open Philanthropy, Amazon, Schmidt Sciences, the Tianqiao and Chrissy Chen Foundation and grant under the NSF CAREER IIS-2338866. CJM acknowledges the support of the Natural Sciences and Engineering Research Council of Canada (NSERC), RGPIN-2021-03445. Resources used in preparing this research were provided in part by the Province of Ontario, the Government of Canada through CIFAR, and companies sponsoring the Vector Institute."
        },
        {
            "title": "References",
            "content": "Marah Abdin, Jyoti Aneja, Hany Awadalla, Ahmed Awadallah, Ammar Ahmad Awan, Nguyen Bach, Amit Bahree, Arash Bakhtiari, Jianmin Bao, Harkirat Behl, et al. Phi-3 technical report: highly capable language model locally on your phone. arXiv preprint arXiv:2404.14219, 2024a. Marah Abdin, Jyoti Aneja, Harkirat Behl, Sébastien Bubeck, Ronen Eldan, Suriya Gunasekar, Michael Harrison, Russell Hewett, Mojan Javaheripi, Piero Kauffmann, et al. Phi-4 technical report. arXiv preprint arXiv:2412.08905, 2024b. Sina Alemohammad, Josue Casco-Rodriguez, Lorenzo Luzi, Ahmed Imtiaz Humayun, Hossein Babaei, Daniel LeJeune, Ali Siahkoohi, and Richard Baraniuk. Self-consuming generative models go mad. arXiv preprint arXiv:2307.01850, 2023. Tim Brooks, Bill Peebles, Connor Holmes, Will DePue, Yufei Guo, Li Jing, David Schnurr, Joe Taylor, Troy Luhman, Eric Luhman, Clarence Ng, Ricky Wang, and Aditya Ramesh. Video generation models as world simulators. 2024. URL https://openai.com/research/ video-generation-models-as-world-simulators. Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. Language models are few-shot learners. Advances in neural information processing systems, 33:18771901, 2020. Yuri Burda, Roger Grosse, and Ruslan Salakhutdinov. Importance weighted autoencoders. arXiv preprint arXiv:1509.00519, 2015. 18 Haolin Chen, Yihao Feng, Zuxin Liu, Weiran Yao, Akshara Prabhakar, Shelby Heinecke, Ricky Ho, Phil Mui, Silvio Savarese, Caiming Xiong, et al. Language models are hidden reasoners: Unlocking latent reasoning capabilities via self-rewarding. arXiv preprint arXiv:2411.04282, 2024. Karl Cobbe, Vineet Kosaraju, Mohammad Bavarian, Mark Chen, Heewoo Jun, Lukasz Kaiser, Matthias Plappert, Jerry Tworek, Jacob Hilton, Reiichiro Nakano, et al. Training verifiers to solve math word problems. arXiv preprint arXiv:2110.14168, 2021. Chris Cremer, Quaid Morris, and David Duvenaud. Reinterpreting importance-weighted autoencoders. arXiv preprint arXiv:1704.02916, 2017. Mostafa Dehghani, Stephan Gouws, Oriol Vinyals, Jakob Uszkoreit, and Lukasz Kaiser. Universal transformers. In International Conference on Learning Representations, 2019. URL https://openreview.net/forum?id=HyzdRiR9Y7. Yuntian Deng, Yejin Choi, and Stuart Shieber. From explicit cot to implicit cot: Learning to internalize cot step by step. arXiv preprint arXiv:2405.14838, 2024. Elvis Dohmatob, Yunzhen Feng, Pu Yang, Francois Charton, and Julia Kempe. tale of tails: Model collapse as change of scaling laws. In Forty-first International Conference on Machine Learning, 2024. URL https://openreview.net/forum?id=KVvku47shW. Ricardo Dominguez-Olmedo, Florian E. Dorner, and Moritz Hardt. Training on the test task confounds evaluation and emergence. In The Thirteenth International Conference on Learning Representations, 2025. Arnaud Doucet et al. Sequential Monte Carlo methods in practice, volume 1. Springer, 2001. Abhimanyu Dubey, Abhinav Jauhri, Abhinav Pandey, Abhishek Kadian, Ahmad Al-Dahle, Aiesha Letman, Akhil Mathur, Alan Schelten, Amy Yang, Angela Fan, et al. The llama 3 herd of models. arXiv preprint arXiv:2407.21783, 2024. Ronen Eldan and Yuanzhi Li. Tinystories: How small can language models be and still speak coherent english? arXiv preprint arXiv:2305.07759, 2023. Leo Gao, Jonathan Tow, Baber Abbasi, Stella Biderman, Sid Black, Anthony DiPofi, Charles Foster, Laurence Golding, Jeffrey Hsu, Alain Le Noach, Haonan Li, Kyle McDonell, Niklas Muennighoff, Chris Ociepa, Jason Phang, Laria Reynolds, Hailey Schoelkopf, Aviya Skowron, Lintang Sutawika, Eric Tang, Anish Thite, Ben Wang, Kevin Wang, and Andy Zou. framework for few-shot language model evaluation, 07 2024. URL https://zenodo.org/records/12608602. Tao Ge, Xin Chan, Xiaoyang Wang, Dian Yu, Haitao Mi, and Dong Yu. Scaling synthetic data creation with 1,000,000,000 personas. arXiv preprint arXiv:2406.20094, 2024. Jonas Geiping, Sean McLeish, Neel Jain, John Kirchenbauer, Siddharth Singh, Brian Bartoldson, Bhavya Kailkhura, Abhinav Bhatele, and Tom Goldstein. Scaling up test-time compute with latent reasoning: recurrent depth approach. arXiv preprint arXiv:2502.05171, 2025. Angeliki Giannou, Shashank Rajput, Jy-Yong Sohn, Kangwook Lee, Jason D. Lee, and Dimitris Papailiopoulos. Looped transformers as programmable computers. In Andreas Krause, Emma Brunskill, Kyunghyun Cho, Barbara Engelhardt, Sivan Sabato, and Jonathan Scarlett (eds.), Proceedings of the 40th International Conference on Machine Learning, volume 202 of Proceedings of Machine Learning Research, pp. 1139811442. PMLR, 2329 Jul 2023. URL https://proceedings.mlr.press/v202/giannou23a.html. Suriya Gunasekar, Yi Zhang, Jyoti Aneja, Caio César Teodoro Mendes, Allie Del Giorno, Sivakanth Gopi, Mojan Javaheripi, Piero Kauffmann, Gustavo de Rosa, Olli Saarikivi, et al. Textbooks are all you need. arXiv preprint arXiv:2306.11644, 2023. 19 Daya Guo, Dejian Yang, Haowei Zhang, Junxiao Song, Ruoyu Zhang, Runxin Xu, Qihao Zhu, Shirong Ma, Peiyi Wang, Xiao Bi, et al. Deepseek-r1: Incentivizing reasoning capability in llms via reinforcement learning. arXiv preprint arXiv:2501.12948, 2025. Shibo Hao, Sainbayar Sukhbaatar, DiJia Su, Xian Li, Zhiting Hu, Jason Weston, and Yuandong Tian. Training large language models to reason in continuous latent space. arXiv preprint arXiv:2412.06769, 2024. Dan Hendrycks, Collin Burns, Steven Basart, Andy Zou, Mantas Mazeika, Dawn Song, and Jacob Steinhardt. Measuring massive multitask language understanding. In International Conference on Learning Representations, 2021a. URL https://openreview.net/forum?id= d7KBjmI3GmQ. Dan Hendrycks, Collin Burns, Saurav Kadavath, Akul Arora, Steven Basart, Eric Tang, Dawn Song, and Jacob Steinhardt. Measuring mathematical problem solving with the MATH dataset. In Thirty-fifth Conference on Neural Information Processing Systems Datasets and Benchmarks Track (Round 2), 2021b. Matthew Douglas Hoffman, Du Phan, David Dohan, Sholto Douglas, Tuan Anh Le, Aaron Parisi, Pavel Sountsov, Charles Sutton, Sharad Vikram, and Rif Saurous. Training chain-of-thought via latent-variable inference. Advances in Neural Information Processing Systems, 36, 2024. Jordan Hoffmann, Sebastian Borgeaud, Arthur Mensch, Elena Buchatskaya, Trevor Cai, Eliza Rutherford, Diego de Las Casas, Lisa Anne Hendricks, Johannes Welbl, Aidan Clark, et al. Training compute-optimal large language models. arXiv preprint arXiv:2203.15556, 2022. Edward Hu, Moksh Jain, Eric Elmoznino, Younesse Kaddar, Guillaume Lajoie, Yoshua Bengio, and Nikolay Malkin. Amortizing intractable inference in large language models. In The Twelfth International Conference on Learning Representations, 2024a. Shengding Hu, Yuge Tu, Xu Han, Ganqu Cui, Chaoqun He, Weilin Zhao, Xiang Long, Zhi Zheng, Yewei Fang, Yuxiang Huang, et al. Minicpm: Unveiling the potential of small language models with scalable training strategies. In First Conference on Language Modeling, 2024b. Jiaxin Huang, Shixiang Shane Gu, Le Hou, Yuexin Wu, Xuezhi Wang, Hongkun Yu, and Jiawei Han. Large language models can self-improve, 2022. URL https://arxiv.org/ abs/2210.11610. HuggingFace. Math-verify: robust mathematical expression evaluation system, 2025. URL https://github.com/huggingface/Math-Verify. Accessed: 2025-02-23. Aaron Hurst, Adam Lerer, Adam Goucher, Adam Perelman, Aditya Ramesh, Aidan Clark, AJ Ostrow, Akila Welihinda, Alan Hayes, Alec Radford, et al. Gpt-4o system card. arXiv preprint arXiv:2410.21276, 2024. Aaron Jaech, Adam Kalai, Adam Lerer, Adam Richardson, Ahmed El-Kishky, Aiden Low, Alec Helyar, Aleksander Madry, Alex Beutel, Alex Carney, et al. Openai o1 system card. arXiv preprint arXiv:2412.16720, 2024. Mojan Javaheripi, Sébastien Bubeck, Marah Abdin, Jyoti Aneja, Sebastien Bubeck, Caio César Teodoro Mendes, Weizhu Chen, Allie Del Giorno, Ronen Eldan, Sivakanth Gopi, et al. Phi-2: The surprising power of small language models. Microsoft Research Blog, 1(3): 3, 2023. Dongwei Jiang, Guoxuan Wang, Yining Lu, Andrew Wang, Jingyu Zhang, Chuyu Liu, Benjamin Van Durme, and Daniel Khashabi. Rationalyst: Pre-training process-supervision for improving reasoning. arXiv preprint arXiv:2410.01044, 2024. Jared Kaplan, Sam McCandlish, Tom Henighan, Tom Brown, Benjamin Chess, Rewon Child, Scott Gray, Alec Radford, Jeffrey Wu, and Dario Amodei. Scaling laws for neural language models. arXiv preprint arXiv:2001.08361, 2020. 20 Joshua Kazdan, Rylan Schaeffer, Apratim Dey, Matthias Gerstgrasser, Rafael Rafailov, David Donoho, and Sanmi Koyejo. Collapse or thrive? perils and promises of synthetic data in self-generating world. arXiv preprint arXiv:2410.16713, 2024. Takeshi Kojima, Shixiang Shane Gu, Machel Reid, Yutaka Matsuo, and Yusuke Iwasawa. Large language models are zero-shot reasoners. Advances in neural information processing systems, 35:2219922213, 2022. Deqian Kong, Minglu Zhao, Dehong Xu, Bo Pang, Shu Wang, Edouardo Honig, Zhangzhang Si, Chuan Li, Jianwen Xie, Sirui Xie, et al. Scalable language models with posterior inference of latent thought vectors. arXiv preprint arXiv:2502.01567, 2025. Aitor Lewkowycz, Anders Andreassen, David Dohan, Ethan Dyer, Henryk Michalewski, Vinay Ramasesh, Ambrose Slone, Cem Anil, Imanol Schlag, Theo Gutman-Solo, et al. Solving quantitative reasoning problems with language models. Advances in Neural Information Processing Systems, 35:38433857, 2022. Jeffrey Li, Alex Fang, Georgios Smyrnis, Maor Ivgi, Matt Jordan, Samir Yitzhak Gadre, Hritik Bansal, Etash Guha, Sedrick Scott Keh, Kushal Arora, et al. Datacomp-lm: In search of the next generation of training sets for language models. Advances in Neural Information Processing Systems, 37:1420014282, 2024. Yuanzhi Li, Sébastien Bubeck, Ronen Eldan, Allie Del Giorno, Suriya Gunasekar, and Yin Tat Lee. Textbooks are all you need ii: phi-1.5 technical report. arXiv preprint arXiv:2309.05463, 2023. Bingbin Liu, Sebastien Bubeck, Ronen Eldan, Janardhan Kulkarni, Yuanzhi Li, Anh Nguyen, Rachel Ward, and Yi Zhang. Tinygsm: achieving> 80% on gsm8k with small language models. arXiv preprint arXiv:2312.09241, 2023. Ilya Loshchilov and Frank Hutter. Decoupled weight decay regularization. In International Conference on Learning Representations, 2019. URL https://openreview.net/forum?id= Bkg6RiCqY7. Anton Lozhkov, Loubna Ben Allal, Elie Bakouch, Leandro von Werra, and Thomas Wolf. Finemath: the finest collection of mathematical content, 2024. URL https://huggingface. co/datasets/HuggingFaceTB/finemath. Ziyang Luo, Can Xu, Pu Zhao, Qingfeng Sun, Xiubo Geng, Wenxiang Hu, Chongyang Tao, Jing Ma, Qingwei Lin, and Daxin Jiang. Wizardcoder: Empowering code large language models with evol-instruct. arXiv preprint arXiv:2306.08568, 2023. Pratyush Maini, Skyler Seto, He Bai, David Grangier, Yizhe Zhang, and Navdeep Jaitly. Rephrasing the web: recipe for compute & data-efficient language modeling. In Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics, 2024. Gonzalo Martínez, Lauren Watson, Pedro Reviriego, José Alberto Hernández, Marc Juarez, and Rik Sarkar. Towards understanding the interplay of generative artificial intelligence and the internet. In International Workshop on Epistemic Uncertainty in Artificial Intelligence, pp. 5973. Springer, 2023. Niklas Muennighoff, Alexander Rush, Boaz Barak, Teven Le Scao, Nouamane Tazi, Aleksandra Piktus, Sampo Pyysalo, Thomas Wolf, and Colin Raffel. Scaling data-constrained language models. Advances in Neural Information Processing Systems, 36, 2024. Maxwell Nye, Anders Johan Andreassen, Guy Gur-Ari, Henryk Michalewski, Jacob Austin, David Bieber, David Dohan, Aitor Lewkowycz, Maarten Bosma, David Luan, et al. Show your work: Scratchpads for intermediate computation with language models. 2021. OpenAI. Gpt-4 technical report, 2023. 21 Long Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida, Carroll Wainwright, Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, et al. Training language models to follow instructions with human feedback. Advances in neural information processing systems, 35:2773027744, 2022. Alec Radford, Jeff Wu, Rewon Child, David Luan, Dario Amodei, and Ilya Sutskever. Language models are unsupervised multitask learners. 2019. Rylan Schaeffer, Brando Miranda, and Sanmi Koyejo. Are emergent abilities of large language models mirage? In Thirty-seventh Conference on Neural Information Processing Systems, 2023. David Silver, Aja Huang, Chris J. Maddison, Arthur Guez, L. Sifre, George van den Driessche, Julian Schrittwieser, Ioannis Antonoglou, Vedavyas Panneershelvam, Marc Lanctot, Sander Dieleman, Dominik Grewe, John Nham, Nal Kalchbrenner, Ilya Sutskever, Timothy P. Lillicrap, Madeleine Leach, Koray Kavukcuoglu, Thore Graepel, and Demis Hassabis. Mastering the game of go with deep neural networks and tree search. Nature, 529:484489, 2016. David Silver, Thomas Hubert, Julian Schrittwieser, Ioannis Antonoglou, Matthew Lai, Arthur Guez, Marc Lanctot, L. Sifre, Dharshan Kumaran, Thore Graepel, Timothy P. Lillicrap, Karen Simonyan, and Demis Hassabis. Mastering chess and shogi by self-play with general reinforcement learning algorithm. ArXiv, abs/1712.01815, 2017a. David Silver, Julian Schrittwieser, Karen Simonyan, Ioannis Antonoglou, Aja Huang, Arthur Guez, Thomas Hubert, Lucas Baker, Matthew Lai, Adrian Bolton, et al. Mastering the game of Go without human knowledge. Nature, 550(7676):354359, 2017b. Avi Singh, John Co-Reyes, Rishabh Agarwal, Ankesh Anand, Piyush Patil, Xavier Garcia, Peter Liu, James Harrison, Jaehoon Lee, Kelvin Xu, et al. Beyond human data: Scaling self-training for problem-solving with language models. arXiv preprint arXiv:2312.06585, 2023. Rohan Taori and Tatsunori Hashimoto. Data feedback loops: Model-driven amplification of dataset biases. In International Conference on Machine Learning, pp. 3388333920. PMLR, 2023. Rohan Taori, Ishaan Gulrajani, Tianyi Zhang, Yann Dubois, Xuechen Li, Carlos Guestrin, Percy Liang, and Tatsunori B. Hashimoto. Stanford alpaca: An instruction-following llama model. https://github.com/tatsu-lab/stanford_alpaca, 2023. Trieu H. Trinh, Yuhuai Wu, Quoc V. Le, He He, and Thang Luong. Solving olympiad geometry without human demonstrations. Nature, 625:476 482, 2024. Pablo Villalobos, Anson Ho, Jaime Sevilla, Tamay Besiroglu, Lennart Heim, and Marius Hobbhahn. Will we run out of data? limits of llm scaling based on human-generated data. arXiv preprint arXiv:2211.04325, 2022. Jason Wei, Maarten Bosma, Vincent Zhao, Kelvin Guu, Adams Wei Yu, Brian Lester, Nan Du, Andrew Dai, and Quoc Le. Finetuned language models are zero-shot learners. arXiv preprint arXiv:2109.01652, 2021. Jason Wei, Yi Tay, Rishi Bommasani, Colin Raffel, Barret Zoph, Sebastian Borgeaud, Dani Yogatama, Maarten Bosma, Denny Zhou, Donald Metzler, Ed H. Chi, Tatsunori Hashimoto, Oriol Vinyals, Percy Liang, Jeff Dean, and William Fedus. Emergent abilities of large language models. Transactions on Machine Learning Research, 2022a. ISSN 2835-8856. Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Fei Xia, Ed Chi, Quoc Le, Denny Zhou, et al. Chain-of-thought prompting elicits reasoning in large language models. Advances in neural information processing systems, 2022b. Yuxiang Wei, Zhe Wang, Jiawei Liu, Yifeng Ding, and Lingming Zhang. Magicoder: Source code is all you need. arXiv preprint arXiv:2312.02120, 2023. 22 Can Xu, Qingfeng Sun, Kai Zheng, Xiubo Geng, Pu Zhao, Jiazhan Feng, Chongyang Tao, and Daxin Jiang. Wizardlm: Empowering large language models to follow complex instructions. arXiv preprint arXiv:2304.12244, 2023. Zitong Yang, Neil Band, Shuangping Li, Emmanuel Candes, and Tatsunori Hashimoto. Synthetic continued pretraining. In The Thirteenth International Conference on Learning Representations, 2025. URL https://openreview.net/forum?id=07yvxWDSla. Eric Zelikman, Yuhuai Wu, Jesse Mu, and Noah Goodman. STar: Bootstrapping reasoning with reasoning. In Advances in Neural Information Processing Systems, 2022. URL https: //openreview.net/forum?id=_3ELRdg2sgI. Eric Zelikman, Georges Raif Harik, Yijia Shao, Varuna Jayasiri, Nick Haber, and Noah Goodman. Quiet-STar: Language models can teach themselves to think before speaking. In First Conference on Language Modeling, 2024. URL https://openreview.net/forum?id= oRXPiSOGH9. Peiyuan Zhang, Guangtao Zeng, Tianduo Wang, and Wei Lu. Tinyllama: An open-source small language model. arXiv preprint arXiv:2401.02385, 2024. Han Zhong, Yutong Yin, Shenao Zhang, Xiaojun Xu, Yuanxin Liu, Yifei Zuo, Zhihan Liu, Boyi Liu, Sirui Zheng, Hongyi Guo, et al. Brite: Bootstrapping reinforced thinking process to enhance language model reasoning. arXiv preprint arXiv:2501.18858, 2025. 23 25 25 28 28 31 33 34 34 42 46 46 57 63 . . . . . . . . . . . ."
        },
        {
            "title": "A Synthetic Data Generation Experiments",
            "content": "A.1 Experimental Details . . . A.2 Additional Results . . . . . . . . . . . . . . . . ."
        },
        {
            "title": "B Bootstrapping Experiments",
            "content": "B.1 Bootstrapping on Fixed Data . . . . B.2 Continual Bootstrapping . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . ."
        },
        {
            "title": "C Prompts",
            "content": "C.1 Prompts for Synthetic Data Generation . . . C.2 Prompts for Few-Show CoT Evaluation . . Synthetic Few-Shot CoT Prompts . C.2.1 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . C.2. Standard Few-Shot CoT Prompts . . . . . . . . . . . . . . ."
        },
        {
            "title": "D Examples of Latent Thoughts",
            "content": "D.1 Examples of Downstream Transfer of Latent Thoughts . . . D.2 Examples of GPT-4o-mini Synthesized Latent Thoughts . . . . . . . . . . . . . . . . . . . . . . . . . D.3 Examples of TinyLlama-1B Self-Generated Latent Thoughts . . . D.4 Failure Examples of Bootstrapped Models on GSM8K . . . . . . ."
        },
        {
            "title": "A Synthetic Data Generation Experiments",
            "content": "A.1 Experimental Details Tuning the mixing ratio of WRAP baselines We have tuned the ratio of mixing raw data with paraphrased data for the WRAP baselines, similar to Maini et al. (2024). For each document during training, we randomly select either the raw data or the paraphrased data according to fixed mixing ratio drawn from {0.0, 0.25, 0.5}, where 0.0 means using only paraphrased data without mixing raw data. The results are shown in Fig. A.1. For both WRAP-Orig and WRAP-CoT, applying no mixing (ratio=0.0) leads to the best performance, which we used in our experiments. (a) WRAP-Orig (b) WRAP-CoT Figure A.1: Training purely on paraphrased data without mixing raw data generally leads to the best performance for WRAP baselines. A.2 Additional Results Evaluation results with different few-shot CoT prompts To facilitate robust evaluation of model downstream performance, we have tested each benchmark using two sets of few-shot CoT prompts. Besides the synthetic CoT prompts that we used by default, we also tested the standard CoT prompts from previous works specifically, the Minerva CoT (Lewkowycz et al., 2022) for MATH, the default CoT from (Wei et al., 2022b) for GSM8K, and the FLAN CoT (Wei et al., 2021) for MMLU-STEM. See Appx. C.2 for detailed prompts. The evaluation results are shown in Fig. A.2. Our method demonstrates robust performance across different prompt variations, maintaining consistent gains over baselines. In contrast, several baselines, most notably WRAP-CoT, exhibit substantial performance degradation when evaluated with standard prompts, leading to an even wider performance gap between our method and the baselines. Figure A.2: Evaluation results with different few-shot CoT prompts: synthetic CoT prompts (left) vs. standard CoT prompts used in previous works (right). Our method demonstrates robust performance across different prompt variations and consistent gains over baselines. The performance gap is more pronounced when using standard CoT prompts due to degradation of the baseline performance. 26 Comparison normalized by the effective raw tokens seen by each method Fig. 6 compares different methods with the same amount of training tokens. However, since different methods produce different amounts of training tokens per raw document, this means each method processes different number of raw documents during training. In particular, our method was trained on 3.3 times more raw tokens (due to the latent to raw token ratio of 2.3) than the raw data baselines. To provide complementary perspective, Fig. A.3 shows performance when methods are normalized by the effective number of raw tokens seen during training. For WRAP baselines, we computed the effective raw tokens based on the number of raw tokens that were paraphrased (i.e., training tokens divided by synthetic-toraw token ratio). We find that the gains of our method over baselines are more significant, highlighting the data efficiency of our method. Figure A.3: Comparison normalized by the effective raw tokens seen by each method. Our method achieves more significant gains over baselines, highlighting its data efficiency."
        },
        {
            "title": "B Bootstrapping Experiments",
            "content": "B.1 Bootstrapping on Fixed Data Log-likelihood evaluation In Fig. B.1, we measure the negative log-likelihood (NLL) of latent thought models at each bootstrapping iteration on holdout validation set. Note that this is not an ideal evaluation metric for latent thought models as they are not trained to directly optimize NLL and the evaluation does not utilize the latent thoughts. Nevertheless, we report it for reference and to provide relative comparison across bootstrap iterations as smooth evaluation metric. Our results show that latent thought models demonstrate lower NLL over multiple iterations with smooth and clear gains up to the fourth iteration, which is consistent with the ELBO results in Fig. 8. Figure B.1: Evaluation results of negative log-likelihood (NLL) on holdout validation set. Our latent thought models demonstrate lower NLL over multiple iterations with clear gains. Detailed evaluation results on MATH In Fig. B.2, we include detailed evaluation results on MATH on both our synthetic CoT prompt and the standard Minerva CoT prompt. We find that the gains of latent thought models across multiple bootstrap iterations are robust across prompt variations. The gains seem to plateau after the third iteration, which might partially be due to the discrete nature of downstream evaluations. The gains over the baseline of training on raw data is even more significant on the Minerva CoT prompt, indicating the effectiveness of training models with self-generated latent thoughts. Exact model performance curves In our main results  (Fig. 8)  , we show the performance curves of the best model during training at each bootstrap iteration. In Fig. B.3, we show the exact performance curves of all models during training, which demonstrates cosnistent relative comparison but with slightly larger variance. 28 Figure B.2: Detailed evaluation results on MATH. The gains of latent thought models across multiple bootstrap iterations are robust across prompt variations, and the gains over the baseline of training on raw data is more significant on the Minerva CoT prompt. Figure B.3: Exact model performance curves during training. We observe consistent relative comparison with Fig. 8 but with slightly larger variance. 29 Additional results on scaling MC samples In Fig. B.4, we include additional results of scaling MC samples, following the same setup as Fig. 10. The performance gains remain robust across different prompt variations and when evaluated GSM8K. Figure B.4: Additional results on scaling MC samples. The performance gains of increasing MC samples persist across prompt variations (left) and when evaluated on GSM8K (middle and right). Negative bootstrapping results In Fig. B.5, we include some negative bootstrapping results where the model performance on GSM8K with few-shot prompting deteriorates over multiple bootstrap iterations (but still outperforms the raw data baselines), which differs from our results on MATH  (Fig. 8)  or on fine-tuning evaluations (Fig. 9, right). potential explanation for the degradation could be that as the model optimizes better on the mathematical-heavy FineMath data, it becomes worse at natural language understanding. We tested this hypothesis on holdout validation set from the general-domain DCLM (Li et al., 2024) data, shown in Fig. B.5 (right). We find that models trained more on FineMath typically get worse on DCLM NLL (as evidenced by bootstrapped models performing worse than the warmstart model, and the train-FLOP-matched baseline performing worse than the raw-token-matched baseline). As result, the bootstrapped models worse performance on GSM8K might be attributed to their decreased natural language understanding capabilities due to increased optimization on the FineMath training data. We have also included some failure examples of bootstrapped models on GSM8K in Appx. D.4, where the bootstrapped models made mistake seemingly due to misinterpretation of the math word problems. Figure B.5: Negative bootstrapping results. The bootstrapped models performance on GSM8K deteriorates over multiple bootstrap iterations (left), which might be attributed to their decreased natural language understanding capabilities due to increased optimization on the FineMath training data, as evidenced by their worse NLL on DCLM (right). 30 B.2 Continual Bootstrapping Additional evaluation results In Fig. B.6, we include additional evaluation results of NLL on the holdout validation set and the MATH performance on the Minerva CoT prompt. We find that NLL evaluation demonstrates similar trend as the ELBO evaluation in Fig. 11 (left), where the bootstrapped models lead to steeper scaling trend than the fixedlatent-generator baselines. For the MATH performance on the Minerva CoT prompt, the bootstrapped models also demonstrate consistent gains, even though the performance gap at the forth iteration is bit less pronounced than using the synthetic CoT prompt (Fig. 11 right). Figure B.6: Additional evaluation results of NLL on the holdout validation set (left) and MATH performance on the Minerva CoT prompt (right). Similar to our main results  (Fig. 11)  , the bootstrapped models lead to steeper scaling trend in likelihood-based metrics and consistent improvement on downstream MATH performance. Detailed evaluation results during training runs In Fig. B.7, we include the evaluation results of each model during the training runs. We plot the best model performance during each training run to denoise the evaluation results and report the average over 3 runs, following the same practice in Fig. 8. From the plots, we can observe clear difference of scaling trend between the bootstrapped models and the fixed-latent-generator baselines, especially for the likelihood-based metrics (top row). Figure B.7: Detailed evaluation results during training runs. We report the best model performance during each training run to denoise the results and report the average over 3 runs. The dashed curves denote the learning rate decay phase of each training run, where the final models are used for evaluation and latent generation. Models right before the decay phase are used for continual training at the next iteration."
        },
        {
            "title": "C Prompts",
            "content": "C.1 Prompts for Synthetic Data Generation Prompt C.1.1: GPT-4o-mini to generate latent thoughts on warmstart data ## System You are an advanced AI system, highly knowledgeable and capable of deeply understanding and reasoning through any web document ## User You are provided with pair of web document prefix and suffix. Your task is to insert latent thoughts between them underlying the creation of the suffix conditioned on the prefix. The latent thoughts should include: the missing background knowledge and the reasoning traces underlying each claim (especially, stepby-step derivations or logical reasoning). ### Prefix {prefix} ### Suffix {suffix} ### Your turn Now provide the latent thoughts. Use concise, simple, and declarative language. Do not give any supporting remarks or references to the terms 'prefix' and 'suffix', as this output will go directly into computer program. Do not apply any markdown formatting or text embellishments. Optimize the content to ensure every word is informative, avoid vague language like 'xxx is essential'. Emphasize on the suffix without repeating the content in the prefix. Focus on implicit reasoning and background knowledge that is not explicitly stated in the suffix, and use concrete logical reasoning or mathematical derivations when applicable. Prompt C.1.2: GPT-4o-mini to rephrase the data with WRAP (Maini et al., 2024) prompts # Prompt For the following website, give me paraphrase of it using very small vocabulary and extremely simple sentences that toddler will understand. ## Website {text} # Prompt II For the following website, give me paraphrase of it using very terse and abstruse language that only an erudite scholar will understand. Replace simple words and phrases with rare and complex ones. ## Website {text} # Prompt III For the following website, give me diverse paraphrase of it in high quality English language, as in sentences on Wikipedia. ## Website {text} # Prompt IV Convert the following website into conversational format with multiple tags of 'Question:' followed by ' Answer:'. ## Website {text} 33 Prompt C.1.3: GPT-4o-mini to rephrase the data with explicit reasoning steps ## User You are provided with the following document. Give me diverse paraphrase in high quality English language that preserves all information in the original document. Also, insert detailed reasoning thoughts (especially, step-by-step mathematical derivations or logical reasoning). Do not output '## Paraphrased Document'. ## Document {text} C.2 Prompts for Few-Show CoT Evaluation Here we include the detailed prompts used in our few-shot CoT evaluation. In particular, we used distinct sets of few-shot CoT prompts (see Appx. C.2): (1) Synthetic CoT prompts (Appx. C.2.1) where the CoT traces are synthetically generated by GPT-4o-mini using Prompt C.1.1 given the question as prefix and answer as suffix; (2) Standard prompts from previous works (Appx. C.2.2) specifically, the Minerva CoT (Lewkowycz et al., 2022) for MATH, the default CoT from (Wei et al., 2022b) for GSM8K, and the FLAN CoT (Wei et al., 2021) for MMLU-STEM. We include the prompts used for our latent thought models for illustration for baselines we used the exact same prompts with special latent tokens (i.e., <StartofLatent>, <Prior>, and <EndofLatent>) removed. For each prompt, we include the few-shot CoT examples and randomly sampled test question. For MATH, we used the same few-shot questions as in the Minerva prompts. For GSM8K, we used randomly selected 8 problems from the training set as the few-shot examples in our synthetic CoT prompt, which aligns better with the actual test problem distribution than the easier problems in the default CoT prompt. For MMLU-STEM, we randomly selected 4 problems from 4 subjects in our synthetic CoT prompt, while FLAN CoT prompts use subject-specific few-shot examples for each subject (we include the one for high school statistics for an example). C.2.1 Synthetic Few-Shot CoT Prompts Prompt C.2.1: Synthetic Few-Shot CoT for MATH Problem: Find the domain of the expression x2 5x . <StartOfLatent><Prior> To find the domain of the expression, we need to ensure the numerator and denominator are both defined and the denominator is not zero. Start with the numerator: Thus, we require: - 2 0 2. 2. This square root is defined when the expression inside is non-negative. Next, consider the denominator: 5 x. This square root is defined when the expression inside is nonnegative as well, and it must also be strictly positive to avoid division by zero. Therefore, we need: 5 - > 0 < 5. Now, we combine the two inequalities. From the first inequality, we have 2, and from the second inequality, we have < 5. The combined conditions are: 2 < 5. In interval notation, this is expressed as [2, 5). Thus, the final answer is [2, 5). The final answer is [2, 5) 34 <EndOfLatent> Final Answer: The final answer is [2, 5). hope it is correct. Problem: If det = 2 and det = 12, then find det(AB). <StartOfLatent><Prior> To find the determinant of the product of two matrices, we use the property that states det(AB) = det(A) det(B). This property holds for any square matrices and of the same size. Given that det = 2 and det = 12, we can directly apply this property. We calculate: 1. Start with the known determinants: det = 2 and det = 12. 2. Multiply these two values together: 2 12. 3. Perform the multiplication: 2 12 = 24. Thus, based on these calculations, we conclude that det(AB) = 24. The final answer is 24 <EndOfLatent> Final Answer: The final answer is 24. hope it is correct. Problem: Terrell usually lifts two 20-pound weights 12 times. If he uses two 15-pound weights instead, how many times must Terrell lift them in order to lift the same total weight? <StartOfLatent><Prior> To determine how many times Terrell must lift the two 15-pound weights to equal the total weight lifted with the two 20-pound weights, we first calculate the total weight lifted with the 20-pound weights. Each 20-pound weight contributes 20 pounds, and since he lifts two weights, the total weight per lift is 20 pounds + 20 pounds = 40 pounds. He lifts this total 12 times, resulting in total weight of 40 pounds * 12 = 480 pounds. Next, we compute how many times he needs to lift the two 15-pound weights to match this total weight. Each 15-pound weight contributes 15 pounds, so the total weight per lift with the 15-pound weights is 15 pounds + 15 pounds = 30 pounds. To find the number of lifts required to reach the same total weight of 480 pounds, we set up the equation: 30 pounds * lifts = 480 pounds, where represents the number of lifts. Solving for x, we divide both sides by 30 pounds: = 480 pounds / 30 pounds = 16 lifts. Thus, Terrell must lift the two 15-pound weights 16 times to equal the total weight he lifts with the two 20-pound weights. The final answer is 16 <EndOfLatent> Final Answer: The final answer is 16. hope it is correct. Problem: If the system of equations 6x 4y = a, 6y 9x = b. has solution (x, y) where and are both nonzero, find b , assuming is nonzero. <StartOfLatent><Prior> To find the ratio the second is 6y 9x = b. , we start with the system of equations given. The first equation is 6x 4y = a, and We can express in terms of and from the first equation: = 6x - 4y. 35 Next, we rearrange the second equation to express b: = 6y - 9x. To find the ratio = 6x4y 6y9x . , we substitute the expressions we derived: Next, we need to simplify this expression. We can factor out common factor in the numerator and the denominator. First, observe that both and can be rewritten in way that may reveal their relationship : In the numerator, we can rearrange it as 6x 4y = 2(3x 2y). In the denominator, we rearrange 6y 9x = 3(2y 3x). Now, substituting these factorizations back into our ratio gives us: = 2(3x2y) 3(2y3x) . Next, we can simplify further. Notice that 2y 3x can be rewritten as (3x 2y): = 2(3x2y) 3((3x2y)) . This simplifies to: 3 = 2 = 2 . 3 Since we assumed is nonzero, this leads us to conclude that the system of equations has specified relationship between and when both and are nonzero. The final answer is 2 3 <EndOfLatent> Final Answer: The final answer is 2 . hope it is correct. Problem: Rationalize the denominator: . 1 7 <StartOfLatent><Prior> Prompt C.2.2: Synthetic Few-Shot CoT for GSM8K ##Problem Question: Ben works 8-hour shifts in furniture shop. It takes him 5 hours to build 1 rocking chair. How many chairs can he build in 10 days? Final Answer: <StartOfLatent><Prior> To find out how many rocking chairs Ben can build in 10 days, first determine the total number of hours he works in that period. Since he works 8-hour shifts, over 10 days, he works total of 8 hours/day multiplied by 10 days, which equals 80 hours. Next, calculate how many rocking chairs Ben can build in those 80 hours. Each rocking chair takes him 5 hours to complete. To find the total number of chairs, divide the total hours worked by the time taken to build one chair: 80 hours divided by 5 hours/chair equals 16 chairs. Thus, in 10 days, Ben can build 16 rocking chairs. Therefore, the answer is 16. <EndOfLatent> The final answer is 16. hope it is correct. ##Problem Question: family has three adults and children, both girls and boys. They went out for family trip and prepared 3 dozen boiled eggs for the trip. Every adult got 3 eggs, and the boys each received 1 more egg than each girl since the girls had some snacks. How many boys went on the trip if the number of girls was 7 and each girl received an egg? Final Answer: <StartOfLatent><Prior> To determine the number of boys on the trip, we first analyze the information given. We know there are 7 girls. Each girl received 1 egg. Therefore, the total number of eggs given to the girls is: 7 girls * 1 egg/girl = 7 eggs. 36 Next, we find out how many eggs were allocated to the adults. There are 3 adults, and each received 3 eggs , which results in: 3 adults * 3 eggs/adult = 9 eggs. Now we calculate the total number of eggs distributed so far: 7 eggs (to girls) + 9 eggs (to adults) = 16 eggs. Since the family prepared 3 dozen boiled eggs, we find the total number of eggs: 3 dozen = 3 * 12 = 36 eggs. Now we determine how many eggs remain for the boys: 36 total eggs - 16 eggs already distributed = 20 eggs available for the boys. According to the problem, each boy received 1 more egg than each girl, which means each boy received: 1 egg (for the girl) + 1 egg = 2 eggs. Let represent the number of boys. The total number of eggs given to the boys can be expressed as: boys * 2 eggs/boy = 2B eggs. Setting the available eggs for the boys equal to the eggs they received, we have: 2B = 20. To find B, we solve the equation: = 20 / 2 = 10. Thus, the number of boys who went on the trip is 10. Therefore, the answer is 10. <EndOfLatent> The final answer is 10. hope it is correct. ##Problem Question: deep-sea monster rises from the waters once every hundred years to feast on ship and sate its hunger. Over three hundred years, it has consumed 847 people. Ships have been built larger over time, so each new ship has twice as many people as the last ship. How many people were on the ship the monster ate in the first hundred years? Final Answer: <StartOfLatent><Prior> To find the number of people on the ship the monster ate in the first hundred years, we need to understand the pattern of ship sizes and the total consumption over three hundred years. The monster consumes ship every hundred years, meaning it consumes total of three ships over three hundred years. The problem states that ships have been built larger over time, specifically that each new ship has twice as many people as the last ship. Let's denote the number of people on the ship in the first hundred years as x. According to the problem, the second ship, which the monster eats in the second hundred years, has twice as many people, or 2x. The third ship has twice the number of people as the second ship, or 4x. Now we can express the total number of people consumed over the three hundred years as: (first ship) + 2x (second ship) + 4x (third ship) = 847 people. Combining these terms gives us: + 2x + 4x = 7x. Setting this equal to the total consumption: 7x = 847. To find x, we divide both sides by 7: = 847 / 7 = 121. Thus, the number of people on the ship the monster ate in the first hundred years is 121. Therefore, the answer is 121. 37 <EndOfLatent> The final answer is 121. hope it is correct. ##Problem Question: Nancy is filling an aquarium for her fish. She fills it halfway and goes to answer the door. While she's gone, her cat knocks the aquarium over and spills half the water in it. Then Nancy comes back and triples the amount of water in the aquarium. If the aquarium is 4 feet long, 6 feet wide, and 3 feet high, how many cubic feet of water are in the aquarium? Final Answer: <StartOfLatent><Prior> To find the total volume of water in the aquarium, calculate the aquarium's total volume first. The formula for the volume of rectangular prism is length multiplied by width multiplied by height. Here, the dimensions are 4 feet long, 6 feet wide, and 3 feet high. Calculate the total volume: Volume = length width height Volume = 4 feet 6 feet 3 feet = 72 cubic feet. Initially, Nancy fills the aquarium halfway. Therefore, the amount of water after she fills it is: Half of 72 cubic feet = 72 / 2 = 36 cubic feet. While Nancy is away, the cat spills half of this amount: Amount spilled = 36 cubic feet / 2 = 18 cubic feet. Thus, the remaining water in the aquarium after the spill is: 36 cubic feet - 18 cubic feet = 18 cubic feet. When Nancy returns, she triples the remaining amount of water. Calculate the new amount of water: New amount = 3 18 cubic feet = 54 cubic feet. The final amount of water in the aquarium in cubic feet is 54. Therefore, the answer is 54. <EndOfLatent> The final answer is 54. hope it is correct. ##Problem Question: There are 30 different nuts in bowl. If 5/6 of the nuts were eaten, how many nuts were left? Final Answer: <StartOfLatent><Prior> To determine how many nuts are left after 5/6 of them are eaten, first, calculate how many nuts were consumed. Start with the total number of nuts, which is 30. To find the number of nuts eaten, multiply 30 by 5/6. 30 * (5/6) = 25 nuts eaten. Next, subtract the number of nuts eaten from the total number of nuts to find how many are left. 30 - 25 = 5 nuts remaining. Thus, after consuming 5/6 of the nuts, 5 nuts are left in the bowl. Therefore, the answer is 5. <EndOfLatent> The final answer is 5. hope it is correct. ##Problem Question: Yuan is 7 years older than his brother David and twice his age. How old is David? Final Answer: <StartOfLatent><Prior> Let represent Yuan's age and represent David's age. From the information given, we can establish two equations based on the relationships described. First, the statement \"Yuan is 7 years older than his brother David\" can be mathematically expressed as: = + 7. Second, the statement \"Yuan is twice his age\" translates to: = 2D. Now, we can set the two equations equal to each other since both represent Yuan's age: + 7 = 2D. To isolate D, we need to rearrange the equation: 7 = 2D - D, 7 = D. Thus, David's age is determined to be 7 years old. Therefore, the answer is 7. <EndOfLatent> The final answer is 7. hope it is correct. ##Problem Question: Joey wants to buy the latest released pair of designer High Jump basketball sneakers. He plans to mow 3 neighbors' lawns for $8 lawn, sell 2 collectible figures to his friends for $9 each, and work an after-school job for 10 hours at $5 per hour. If his earnings just cover the price of the High Jump sneakers, how much do the shoes cost? Final Answer: <StartOfLatent><Prior> To find the total earnings Joey accumulates, we need to calculate each source of income step-by-step. First, calculate the income from mowing lawns. Joey mows 3 lawns at rate of $8 per lawn. Therefore, the total income from mowing is: 3 lawns * $8/lawn = $24. Next, calculate the income from selling collectible figures. Joey sells 2 figures at price of $9 each. Thus, the total income from selling figures is: 2 figures * $9/figure = $18. Now, calculate the income from his after-school job. Joey works for 10 hours at rate of $5 per hour. Therefore, his income from the job is: 10 hours * $5/hour = $50. Now, sum all these earnings to find the total amount Joey has: $24 (mowing) + $18 (selling figures) + $50 (job) = $92. Since Joey's total earnings of $92 exactly cover the price of the High Jump basketball sneakers, it can be concluded that the cost of the sneakers is $92. Therefore, the answer is 92. <EndOfLatent> The final answer is 92. hope it is correct. ##Problem Question: Before getting to work, Jess has few errands to run. Jess has to walk 11 blocks to the store and 6 blocks to the gallery, before walking the final 8 blocks to arrive at work. If Jess has already walked 5 blocks, how many more blocks must she walk before arriving at work? Final Answer: <StartOfLatent><Prior> To determine the total distance Jess needs to walk to work, first calculate the total number of blocks from the store and the gallery to work. The distance to the store is 11 blocks, and the distance to the gallery is 6 blocks. Thus, the total distance to work is the sum of these two distances plus the final stretch to work, which is 8 blocks. Total distance = 11 blocks (to store) + 6 blocks (to gallery) + 8 blocks (to work) = 25 blocks. Next, subtract the distance Jess has already walked, which is 5 blocks, from the total distance calculated . Remaining distance = Total distance - Distance already walked = 25 blocks - 5 blocks = 20 blocks. Thus, Jess must walk 20 more blocks to reach work. Therefore, the answer is 20. <EndOfLatent> The final answer is 20. hope it is correct. ##Problem Question: Marcell and Beatrice are having contest to see who can eat the most fruit roll-ups, so they unroll as many as they can find. Unfortunately, someone makes mistake and Beatrice's was two roll-ups wide and 24 rolls up long while Marcell's was 3 roll-ups wide and 14 roll-ups long. If they both ate their entire amount, how many did they eat on average? Final Answer: <StartOfLatent><Prior> Prompt C.2.3: Synthetic Few-Shot CoT for MMLU-STEM ##Problem Question: microwave oven is connected to an outlet, 120 V, and draws current of 2 amps. At what rate is energy being used by the microwave oven? (A) 10 (B) 30 (C) 60 (D) 240 Answer: <StartOfLatent><Prior> To determine the rate of energy usage, we apply the formula for electrical power, which is given by = I, where is power in watts, is voltage in volts, and is current in amps. In this scenario, the voltage is 120 and the current is 2 A. Calculating the power: = 120 2 = 240 W. This calculation shows that the microwave oven consumes energy at rate of 240 watts. Thus, the answer is (D) 240 W. <EndOfLatent> The answer is (D) 240W. ##Problem Question: The variable varies directly as the square of y, and varies directly as the cube of z. If equals 16 when equals 2, what is the value of when equals 1 2 (A) -1 (B) 16 (C) 1 256 (D) 1 16 Answer: <StartOfLatent><Prior> To solve the problem, we need to establish the relationships between the variables x, y, and based on the given conditions. ? Firstly, since varies directly as the square of y, we can express this relationship mathematically as: = k1 * y^2, where k1 is constant. Next, since varies directly as the cube of z, we can express this relationship as: = k2 * z^3, where k2 is another constant. Now we can substitute in the first equation with its expression in terms of z: = k1 * (k2 * z^3)^2 = k1 * k2^2 * z^6. This shows that varies directly as the sixth power of z, leading us to express this as: = * z^6, where = k1 * k2^2. Given that = -16 when = 2, we can substitute these values into the equation to find k: -16 = * (2^6) -16 = * 64 = -16 / 64 = -1/4. Now we have the relationship for in terms of z: = -1/4 * z^6. Next, we need to find the value of when = 1/2. We substitute = 1/2 into the equation: = -1/4 * (1/2)^6 = -1/4 * (1/64) 40 = -1/256. Therefore, the value of when = 1/2 is -1/256. Thus, the answer is (C) -frac{1}{256}. <EndOfLatent> ##Problem Question: Which expression is equivalent to 5 9? (A) (5 4) (6 5) (B) (5 5) + (5 4) (C) (5 5) + (5 9) (D) (5 9) (6 9) Answer: <StartOfLatent><Prior> To determine which expression is equivalent to 5 9, we need to evaluate each option step by step. Option (A): (5 4) (6 5) This expression simplifies to 20 30, which equals 600. This is not equal to 5 9 = 45. Option (B): (5 5) + (5 4) This expression simplifies to 25 + 20, which equals 45. This matches 5 9. Option (C): (5 5) + (5 9) This expression simplifies to 25 + 45, which equals 70. This is not equal to 5 9. Option (D): (5 9) (6 9) This expression simplifies to 45 54, which equals 2430. This is not equal to 5 9. Thus, the answer is (B) (5 5) + (5 4). <EndOfLatent> The answer is (B) (5 5) + (5 4). ##Problem Question: new smartwatch is manufactured in one part of factory, then secured for shipping in another, independent part of the factory. The weight of the smartwatch has mean of 62 grams and standard deviation of 1.0 grams. The weight of the packaging (box, user's guide, bubble wrap, etc.) has mean of 456 grams and standard deviation of 6 grams. Together, the distribution of the weight of the smartwatch and its packaging would have the following mean and standard deviation: (A) Mean 518 grams; standard deviation 7.0 grams (B) Mean 518 grams; standard deviation 3.5 grams (C) Mean 518 grams; standard deviation 6.1 grams (D) Mean 394 grams; standard deviation 6.1 grams Answer: <StartOfLatent><Prior> To find the combined mean and standard deviation of the smartwatch and packaging, we start by calculating the mean of the total weight. The mean weight of the smartwatch is 62 grams, and the mean weight of the packaging is 456 grams. The total mean weight is calculated by adding these two means: Mean total weight = Mean smartwatch + Mean packaging Mean total weight = 62 grams + 456 grams = 518 grams. Next, we need to calculate the standard deviation of the total weight. Since the weights of the smartwatch and packaging are independent, we can use the formula for the sum of two independent random variables: Standard deviation of total weight = sqrt((Standard deviation of smartwatch)^2 + (Standard deviation of packaging)^2). The standard deviation of the smartwatch is 1.0 grams, and the standard deviation of the packaging is 6 grams. Now, we apply the values to the formula: Standard deviation of total weight = sqrt((1.0 grams)^2 + (6 grams)^2) = sqrt(1 + 36) = sqrt(37) 6.08 grams. Rounding this value to one decimal place gives us approximately 6.1 grams. 41 Therefore, the combined mean weight is 518 grams, and the combined standard deviation is approximately 6.1 grams, which corresponds to option (C). Thus, the answer is (C) Mean 518 grams; standard deviation 6.1 grams. <EndOfLatent> The answer is (C) Mean 518 grams; standard deviation 6.1 grams. ##Problem Question: Mr. Bee's statistics class had standard deviation of 11.2 on standardized test, while Mr. Em 's class had standard deviation of 5.6 on the same test. Which of the following is the most reasonable conclusion concerning the two classes' performance on the test? (A) Mr. Bee's class is less heterogeneous than Mr. Em's. (B) Mr. Em's class is more homogeneous than Mr. Bee's. (C) Mr. Bee's class performed twice as well as Mr. Em's. (D) Mr. Em's class did not do as well as Mr. Bee's. Answer: <StartOfLatent><Prior> C.2.2 Standard Few-Shot CoT Prompts Prompt C.2.4: Minerva Few-Shot CoT for MATH (Lewkowycz et al., 2022) Problem: Find the domain of the expression x2 5x . <StartOfLatent><Prior> The expressions inside each square root must be non-negative. Therefore, 2 0, so 2, and 5 0, so 5. Also, the denominator cannot be equal to zero, so 5 > 0, which gives < 5. Therefore, the domain of the expression is [2, 5) . The final answer is [2, 5) <EndOfLatent> Final Answer: The final answer is [2, 5). hope it is correct. Problem: If det = 2 and det = 12, then find det(AB). <StartOfLatent><Prior> We have that det(AB) = (det A)(det B) = (2)(12) = 24 . The final answer is 24 <EndOfLatent> Final Answer: The final answer is 24. hope it is correct. Problem: Terrell usually lifts two 20-pound weights 12 times. If he uses two 15-pound weights instead, how many times must Terrell lift them in order to lift the same total weight? <StartOfLatent><Prior> If Terrell lifts two 20-pound weights 12 times, he lifts total of 2 12 20 = 480 pounds of weight. If he lifts two 15-pound weights instead for times, he will lift total of 2 15 = 30n pounds of weight. Equating this to 480 pounds, we can solve for n: 30n = 480 = 480/30 = 16 The final answer is 16 <EndOfLatent> Final Answer: The final answer is 16. hope it is correct. Problem: If the system of equations 6x 4y = a, 42 6y 9x = b. has solution (x, y) where and are both nonzero, find , assuming is nonzero. <StartOfLatent><Prior> If we multiply the first equation by 3 , we obtain 6y 9x = 3 2 a. Since we also know that 6y 9x = b, we have 3 2 = = 2 3 . The final answer is 2 3 <EndOfLatent> Final Answer: The final answer is 2 3 . hope it is correct. Problem: Rationalize the denominator: . 1 7 2 <StartOfLatent><Prior> Prompt C.2.5: Default Few-Shot CoT for GSM8K from (Wei et al., 2022b) ##Problem Q: Olivia has 23.Shebought ivebagels or3 each. How much money does she have left? A: <StartOfLatent><Prior> Olivia had 23 dollars. 5 bagels for 3 dollars each will be 5 3 = 15 dollars. So she has 23 - 15 dollars left. 23 - 15 is 8. <EndOfLatent> The answer is 8. ##Problem Q: Jason had 20 lollipops. He gave Denny some lollipops. Now Jason has 12 lollipops. How many lollipops did Jason give to Denny? A: <StartOfLatent><Prior> Jason started with 20 lollipops. Then he had 12 after giving some to Denny. So he gave Denny 20 - 12 = 8. <EndOfLatent> The answer is 8. ##Problem Q: There are 15 trees in the grove. Grove workers will plant trees in the grove today. After they are done , there will be 21 trees. How many trees did the grove workers plant today? A: <StartOfLatent><Prior> There are 15 trees originally. Then there were 21 trees after some more were planted. So there must have been 21 - 15 = 6. <EndOfLatent> The answer is 6. ##Problem Q: If there are 3 cars in the parking lot and 2 more cars arrive, how many cars are in the parking lot? 43 A: <StartOfLatent><Prior> There are originally 3 cars. 2 more cars arrive. 3 + 2 = 5. <EndOfLatent> The answer is 5. ##Problem Q: There were nine computers in the server room. Five more computers were installed each day, from monday to thursday. How many computers are now in the server room? A: <StartOfLatent><Prior> There were originally 9 computers. For each of 4 days, 5 more computers were added. So 5 * 4 = 20 computers were added. 9 + 20 is 29. <EndOfLatent> The answer is 29. ##Problem Q: Michael had 58 golf balls. On tuesday, he lost 23 golf balls. On wednesday, he lost 2 more. How many golf balls did he have at the end of wednesday? A: <StartOfLatent><Prior> Michael started with 58 golf balls. After losing 23 on tuesday, he had 58 - 23 = 35. After losing 2 more, he had 35 - 2 = 33 golf balls. <EndOfLatent> The answer is 33. ##Problem Q: Leah had 32 chocolates and her sister had 42. If they ate 35, how many pieces do they have left in total? A: <StartOfLatent><Prior> Originally, Leah had 32 chocolates. Her sister had 42. So in total they had 32 + 42 = 74. After eating 35, they had 74 - 35 = 39. <EndOfLatent> The answer is 39. ##Problem Q: Shawn has five toys. For Christmas, he got two toys each from his mom and dad. How many toys does he have now? A: <StartOfLatent><Prior> Shawn started with 5 toys. If he got 2 toys each from his mom and dad, then that is 4 more toys. 5 + 4 = 9. <EndOfLatent> The answer is 9. ##Problem Q: Marcell and Beatrice are having contest to see who can eat the most fruit roll-ups, so they unroll as many as they can find. Unfortunately, someone makes mistake and Beatrice's was two roll-ups wide and 24 rolls up long while Marcell's was 3 roll-ups wide and 14 roll-ups long. If they both ate their entire amount, how many did they eat on average? A: <StartOfLatent><Prior> Prompt C.2.6: FLAN Few-Shot CoT for MMLU-STEM from (Wei et al., 2021) The following are multiple choice questions (with answers) about high school statistics. ##Problem 44 Question: new smartwatch is manufactured in one part of factory, then secured for shipping in another, independent part of the factory. The weight of the smartwatch has mean of 62 grams and standard deviation of 1.0 grams. The weight of the packaging (box, user's guide, bubble wrap, etc.) has mean of 456 grams and standard deviation of 6 grams. Together, the distribution of the weight of the smartwatch and its packaging would have the following mean and standard deviation: (A) Mean 518 grams; standard deviation 7.0 grams (B) Mean 518 grams; standard deviation 3.5 grams (C) Mean 518 grams; standard deviation 6.1 grams (D) Mean 394 grams; standard deviation 6.1 grams Answer: <StartOfLatent><Prior> Let's think step by step. Since the weight of the watch and the weight of the packaging are independent random variables, the mean and variance of their sum is equal to the sum of their individual means and variances. So the mean is 62 + 456 = 518 grams, and the variances is 1.0^2 + 6.0^2 = 37, leading to standard deviation of 6.1 grams. The answer is (C). <EndOfLatent> The answer is (C). ##Problem Question: After frost warning was issued, the owner of large orange grove asked his workers to spray all his trees with water. The water was supposed to freeze and form protective covering of ice around the orange blossom. Nevertheless, the owner suspected that some trees suffered considerable damage due to the frost. To estimate the proportion of trees that suffered more than 50 percent damage due to the frost, he took random sample of 100 trees from his grove. What is the response variable in this experiment? (A) The proportion of trees that suffered more than 50 percent damage due to frost. (B) The number of trees affected by the frost. (C) The number of trees sampled from the grove. (D) For each sampled tree, whether it suffered more than 50 percent damage or at most 50 percent damage. Answer: <StartOfLatent><Prior> Let's think step by step. In this experiment, the response variable is what is measured. For each tree, what is measured is whether or not it suffered more than 50 percent damage due to the frost. The answer is (D). <EndOfLatent> The answer is (D). ##Problem Question: Suppose and are random variables with E(X) = 37, var(X) = 5, E(Y) = 62, and var(Y) = 12. What are the expected value and variance of the random variable + Y? (A) E(X + Y) = 99, var(X + Y) = 8.5 (B) E(X + Y) = 99, var(X + Y) = 13 (C) E(X + Y) = 99, var(X + Y) = 17 (D) There is insufficient information to answer this question. Answer: <StartOfLatent><Prior> Let's think step by step. While means of sums of random variables add (regardless of whether the variables are independent) in order to determine the variance of sum of random variables, we need to know not just their individual variances but the covariance of the two variables, which is not given in this problem. The answer is (D). <EndOfLatent> The answer is (D). ##Problem Question: Which of the following sets has the smallest standard deviation? Which has the largest? I: {1,2,3} II: {-10,10} III: {100} (A) I, II (B) II, III (C) III, (D) III, II Answer: <StartOfLatent><Prior> Let's think step by step. The variance of distribution is the expected squared deviation from its mean ( which is 2), so the variance is 2/3 . The variance of distribution II is 10^2 (because both elements are 10 away from the mean of zero). The variance of distribution III is 0, since it has single entry. So distribution III has the smallest standard deviation and distribution II has the largest. The answer is (D ). <EndOfLatent> The answer is (D). ##Problem Question: Mr. Bee's statistics class had standard deviation of 11.2 on standardized test, while Mr. Em 's class had standard deviation of 5.6 on the same test. Which of the following is the most reasonable conclusion concerning the two classes' performance on the test? (A) Mr. Bee's class is less heterogeneous than Mr. Em's. (B) Mr. Em's class is more homogeneous than Mr. Bee's. (C) Mr. Bee's class performed twice as well as Mr. Em's. (D) Mr. Em's class did not do as well as Mr. Bee's. Answer: <StartOfLatent><Prior>"
        },
        {
            "title": "D Examples of Latent Thoughts",
            "content": "D.1 Examples of Downstream Transfer of Latent Thoughts Example D.1.1: Solving algebra problems on MATH Problem: What is the distance between the two intersections of = x2 and + = 1? <StartOfLatent><Prior> To find the distance between the two intersections of the curves y=x^2 and x+y=1, we first need to determine the points of intersection. Setting the equations equal to each other gives us x^2 = + 1. Rearranging this leads to x^2 - - 1 = 0. We can solve this quadratic equation using the quadratic formula, = [1 sqrt(1 + 4)] / 2, which simplifies to = [1 sqrt(5)] / 2. This results in two intersection points: (1 + sqrt(5))/2 and (1 - sqrt(5))/2. Next, we need to find the distance between these two points. The distance formula between two points (x1, y1) and (x2, y2) is given by = sqrt((x2 - x1)^2 + (y2 - y1)^2). Substituting our points into this formula, we have = sqrt(((1 + sqrt(5))/2 - (1 - sqrt(5))/2)^2 + ((1 + sqrt(5))/2 - (1 - sqrt(5))/2)^2). Calculating the differences in the x-coordinates and y-coordinates separately, we find: 1. For the x-coordinates: (1 + sqrt(5))/2 - (1 - sqrt(5))/2 = (1 + sqrt(5) - 1 + sqrt(5))/2 = (2sqrt(5))/2 = sqrt(5). 2. For the y-coordinates: (1 + sqrt(5))/2 - (1 - sqrt(5))/2 = (1 + sqrt(5) - 1 + sqrt(5))/2 = (2sqrt(5))/2 = sqrt(5). Now, substituting these values back into the distance formula gives us = sqrt((sqrt(5))^2 + (sqrt(5))^2) = sqrt(5 + 5) = sqrt(10). Thus, the final answer is sqrt(10). <EndOfLatent> Final Answer: The final answer is 10. hope it is correct. Example D.1.2: Solving combinatorial problems on MATH Problem: Pat is to select six cookies from tray containing only chocolate chip, oatmeal, and peanut butter cookies. There are at least six of each of these three kinds of cookies on the tray. How many different assortments of six cookies can be selected? (Note that cookies of the same type are not distinguishable.) <StartOfLatent><Prior> To determine the number of different assortments of six cookies from tray containing chocolate chip, oatmeal, and peanut butter cookies, we need to consider the constraints imposed by the requirement of at least six of each type of cookie. First, we recognize that the total number of cookies is fixed at six. The problem can be approached using combinatorial methods, specifically the \"stars and bars\" theorem. This theorem helps us find the number of ways to distribute indistinguishable objects (the cookies) into distinguishable boxes (the types of cookies). Let: - = number of chocolate chip cookies - = number of oatmeal cookies - = number of peanut butter cookies The total number of cookies is given by the equation: + + = 6 The stars and bars theorem states that the number of ways to distribute indistinguishable objects into distinguishable boxes is given by the formula: (n + - 1) choose (k - 1) In our case, = 6 (the total number of cookies) and = 3 (the types of cookies: chocolate chip, oatmeal, and peanut butter). Applying the formula: (6 + 3 - 1) choose (3 - 1) = 8 choose 2 Calculating this gives: 8 choose 2 = 8! / (2!(8-2)!) = 8! / (2! * 6!) = (8 * 7) / (2 * 1) = 28 Thus, there are 28 different assortments of six cookies that can be selected from the tray, ensuring that at least six of each type are included. <EndOfLatent> Final Answer: The final answer is 28. hope it is correct. Example D.1.3: Solving number theory problems on MATH Problem: How many positive factors does 48 have? <StartOfLatent><Prior> To find the number of positive factors of 48, we start by determining its prime factorization. The prime factorization of 48 is 2^4 * 3^1. Next, we apply the formula for finding the number of positive factors from the prime factorization. For number expressed as p1^e1 * p2^e2 * ... * pk^ek, the total number of factors is given by (e1 + 1)(e2 + 1) ...(ek + 1). In the case of 48, we have: - For the prime factor 2, the exponent is 4. - For the prime factor 3, the exponent is 1. Applying the formula: (4 + 1)(1 + 1) = 5 * 2 = 10. Thus, 48 has 10 positive factors. The final answer is 10. <EndOfLatent> Final Answer: The final answer is 10. hope it is correct. Example D.1.4: Solving math word problems on GSM8K Question: Farmer Brown has 60 animals on his farm, all either chickens or cows. He has twice as many chickens as cows. How many legs do the animals have, all together? Final Answer: <StartOfLatent><Prior> To find the total number of legs, we need to establish the relationship between the number of chickens and cows. Let the number of cows be represented as C. According to the problem, the number of chickens is twice that of cows, so we can express the number of chickens as 2C. Next, we know that the total number of animals is 60. Therefore, we can set up the equation: + 2C = 60 This simplifies to: 3C = 60 To find C, we divide both sides by 3: = 20 Now that we have the number of cows, we can find the number of chickens: Number of chickens = 2C = 2 * 20 = 40 Now we can calculate the total number of legs. Each chicken has 2 legs, and each cow has 4 legs. The total number of legs can be calculated as follows: Total legs = Number of chickens * Legs per chicken + Number of cows * Legs per cow Substituting the known values: Total legs = 40 * 2 + 20 * 4 Calculating this gives: Total legs = 80 + 80 = 160 Thus, the total number of legs is 160. <EndOfLatent> The final answer is 160. D.2 Examples of GPT-4o-mini Synthesized Latent Thoughts Example D.2.1: Latent thoughts recover missing background knowledge and reasoning <StartOfLatent> Principal Component Analysis (PCA) is statistical technique used for dimensionality reduction, which is crucial in data analysis and machine learning. The objective of PCA is to identify the directions ( principal components) in which the variance of the data is maximized. This process involves transforming the original features into new set of uncorrelated features, ordered by the amount of variance they capture. To begin with, we consider dataset with multiple dimensions, where each dimension represents feature. The first step in PCA is to center the data by subtracting the mean of each feature from the dataset. This step is vital because it ensures that the new coordinate system is based around the origin, allowing for more accurate analysis of variance. Next, we compute the covariance matrix of the decentralized data. The covariance matrix captures the relationships between the different features, indicating how they vary together. The covariance values reveal the spread and the direction of the data in the feature space. Following the covariance matrix calculation, we need to determine the eigenvalues and eigenvectors of this matrix. The eigenvectors represent the directions of the principal components, while the eigenvalues indicate the magnitude of variance in those directions. The larger the eigenvalue, the more significant the corresponding eigenvector is in explaining the variance in the dataset. To select the principal components, we rank the eigenvectors based on their eigenvalues in descending order. The top eigenvectors, where is the number of dimensions we wish to retain, form new feature space that captures the most variance. Finally, we project the original, decentralized data onto this new feature space by multiplying the original data matrix by the matrix of the top eigenvectors. This projection results in reduced dataset that retains the essential characteristics of the original data while minimizing information loss. The effectiveness of PCA lies in its ability to simplify complex datasets, making subsequent analyses more manageable and interpretable. <EndOfLatent> # The basic idea of PCA Principal component analysis is to find out the most important aspects in the data and replace the original data with the most important aspects in the data. To put it bluntly, it is to reduce the data from n-dimensional to n'-dimensional, and hope that the data set of n'-dimensional features retains most of the information as much as possible. # PCA mathematical derivation (maximum variance method) 48 For example, if the data is two-dimensional plum, we project the original data (blue dots) onto the new most marked axis (yellow and blue crosshairs). The way to find this new coordinate axis is to find the point (red point) where the data is projected on the new coordinate axis and the distance from the origin of the new coordinate axis is the largest, which is the maximum variance method. <StartOfLatent> Decentralization adjusts the data such that its mean is zero, which is crucial for PCA. If the data is not centered, the computed variances along the axes will be skewed, leading to misleading results in the identification of principal components. The principal component should ideally capture the largest variance, and uncentered data would cause the variance to reflect the mean of the data rather than the true distribution of the data points. Without decentralization, the projection of the original data onto the new axis would not accurately reflect the inherent structure of the data, undermining the effectiveness of dimensionality reduction. Thus, the step of data decentralization is not merely preprocessing step; it fundamentally ensures that PCA captures the true variance of the data, allowing for an accurate reduction in dimensionality while retaining as much information as possible. <EndOfLatent> ## The first step, data decentralization The discussion of the new coordinate axis and the old coordinate axis is very troublesome. We directly decentralize the data (that is, the mean value of the data is at the far point). If the data is not decentralized, we cannot find the optimal dimensionality reduction. This step is necessary <StartOfLatent> To find the optimal axis for PCA, we utilize the covariance matrix of the decentralized data. The eigenvalues of this matrix indicate the amount of variance captured by each principal component, while the corresponding eigenvectors provide the direction of these components. The first principal component (PC1) is the eigenvector associated with the largest eigenvalue, representing the direction of maximum variance . The distance from the projected data points to the origin of the new axis (PC1) is calculated as the projection of the original data points onto this axis. The objective is to maximize the sum of squared distances ( d2 optimization problem can be framed as maximizing the Rayleigh quotient for the covariance matrix, leading to the derivation of the eigenvalues and eigenvectors. <EndOfLatent> ), which mathematically quantifies the variance captured by the principal component. This ## The second step is to find the new most marked axis How do we find the best most standard axis to achieve principal component analysis? That is, the greater the distance between the projected point and the origin of the coordinate axis, the better (this is the maximum variance) As shown in the figure, the red dotted line is the new coordinate axis, we call it PC1; the green dot is the initial data sample point; the green cross is the point projected on the new coordinate; d1, d2, d3... d6 are the projected ones The distance from the point to the far point. All we need to do is find the largest sum of squares (that's d2 ). Here comes the math! ! ! ! <StartOfLatent> To find the optimal new coordinate axis for dimensionality reduction, we must analyze the relationships among the data points. This involves calculating the correlation coefficient matrix, which quantifies how much each feature varies with others. Once we have the correlation matrix, we compute its eigenvalues and eigenvectors. The eigenvalues indicate the amount of variance captured by each corresponding eigenvector. The eigenvector with the highest eigenvalue gives us the direction of the axis that maximizes variance among the data points. In principal component analysis (PCA), we focus on the principal components (PCs) that represent the highest variance first. This ensures that we retain the most important information while reducing dimensions. We select percentage of these components, based on the cumulative variance they explain, to maintain balance between data fidelity and dimensionality reduction. 49 By projecting the original data onto these new axes (PC1, PC2, etc.), we can effectively reduce the number of features while preserving the essential structure and relationships within the data. The choice of how many components to keep is guided by examining the explained variance ratios. The goal is to achieve compact representation that still captures the underlying patterns in the dataset. <EndOfLatent> As can be seen from the figure above, when we are looking for the best newest axis, we are actually finding eigenvalues and eigenvectors by finding the data correlation coefficient matrix ## The third step is to choose few percent of the data you need When we perform PCA dimensionality reduction to find new coordinates, the number of coordinates is the same as the number of data features. But we project the data on the new coordinate axis in order to express the information of the entire data with as few features as possible. In the figure below, we can see that we have obtained two coordinate axes, PC1 and PC2, respectively. <StartOfLatent> To understand why the coordinate axes PC1 and PC2 are chosen, we first recognize that PCA aims to maximize the variance captured by the new axes. The eigenvalues obtained from the eigenvalue decomposition of the data correlation coefficient matrix indicate the variance explained by each principal component. The eigenvector corresponding to the largest eigenvalue defines the direction of PC1, capturing the most significant variance in the data. The second eigenvector, corresponding to the second largest eigenvalue, defines PC2 and is orthogonal to PC1, ensuring that these axes are independent and do not introduce redundancy in the representation of the data. The information captured by each principal component can be quantified by the proportion of the total variance they explain. In this case, PC1 accounts for 83% When reducing dimensions, it is logical to prioritize PC1 over PC2 due to its higher variance contribution . This prioritization allows for more efficient representation of the data with fewer dimensions, simplifying analysis while retaining essential information. In three dimensions, using only PC1 and PC2 to represent the data can effectively capture the underlying structure without the noise, as the dimensionality reduction is inherently aimed at compressing the data while minimizing loss. This approach demonstrates PCA's effectiveness in unsupervised learning, particularly for data compression and denoising, making it widely adopted technique in various practical applications. <EndOfLatent> The two coordinate axes are perpendicular to each other and do not interfere with each other. Among them, the information on the data on PC1 accounts for 83% When we look at three dimensions, we can see that if we use PC1 and PC2 to reduce the dimension As dimensionality reduction method for unsupervised learning, it only needs eigenvalue decomposition to compress and denoise data . Therefore, it is widely used in practical scenarios. <StartOfLatent> When performing PCA, we seek to maximize the variance captured by each principal component. Since variance is central to determining the principal components, squaring the differences ensures that components capturing larger variances are prioritized. This aligns with the goal of PCA, which is to identify directions in the data that account for the greatest amount of variability. The orthogonality of principal components guarantees that the information captured by each component is independent of the others. This independence is crucial for effective dimensionality reduction because it ensures that the chosen components do not overlap in the information they represent. The simplification of calculations through eigenvalue decomposition further supports this process, as it systematically identifies the components associated with the highest variance. In terms of interpretation, while PCA effectively reduces dimensionality and retains the most significant variance, it also introduces ambiguity regarding the meaning of the principal components. Each principal component is linear combination of the original features, making it challenging to directly interpret their significance in relation to the original data. The potential loss of small variance components is another critical consideration. Although these components may seem less important, they could contain subtle but meaningful differences in the dataset. Discarding them may lead to overlooking valuable insights during subsequent analysis. 50 Lastly, the effectiveness of PCA is contingent on the assumptions of data distribution. PCA assumes that the data is normally distributed; deviations from this assumption can lead to suboptimal results. Therefore, understanding the underlying data characteristics is essential for ensuring PCA's effectiveness in dimensionality reduction and information retention. <EndOfLatent> The main advantages of the PCA algorithm are: 1) Only the variance needs to be used to measure the amount of information, and it is not affected by factors other than the data set. 2) The principal components are orthogonal to each other, which can eliminate the mutual influence factors between the original data components. 3) The calculation method is simple, and the main operation is eigenvalue decomposition, which is easy to implement. The main disadvantages of the PCA algorithm are: 1) The meaning of each feature dimension of the principal component has certain degree of ambiguity, which is not as strong as the interpretation of the original sample features. 2) Non-principal components with small variance may also contain important information on sample differences, and discarding due to dimensionality reduction may have an impact on subsequent data processing. 3) When the data distribution is not normal distribution, the effect is not very good <StartOfLatent> To understand why variance is squared in PCA, we must first recognize that variance is measure of the spread of data points in dataset. Squaring the differences from the mean when calculating variance emphasizes larger deviations. This is important because larger deviations contribute more significantly to the overall variability. By squaring these differences, we ensure that both positive and negative deviations contribute positively to the variance, which allows us to assess the total spread of the data accurately. Without squaring, positive and negative distances could nullify each other, leading to misleading interpretations of data relationships. Squaring transforms all distances into non-negative values, ensuring that the overall variance remains measure of spread without cancellation effects. Using the correlation coefficient matrix instead of the covariance matrix is fundamental when attributes have different units. The correlation coefficient standardizes the data, allowing for comparison across variables that are measured on different scales. This standardization is achieved by dividing the covariance by the product of the standard deviations of the two variables involved. Consequently, it enables clear interpretation of the strength and direction of relationships without the influence of differing units. Dimensioning in PCA is necessary when attributes have different units. If attributes are in the same unit, covariance can directly reflect the degree of correlation. However, when units differ, covariance alone is insufficient as it does not convey the actual strength of the relationship. The correlation coefficient , derived by dividing the covariance by the standard deviations, effectively normalizes the data, ensuring that the relationships are comparable. This normalization is vital for accurately assessing how similar the attributes are in their variations. <EndOfLatent> # question 1. Why square it ? It is because if the distance is not squared, there will be positive and negative distances, which will cancel each other out. 2.**Why is the correlation coefficient matrix instead of the covariance matrix? ** You will know this when you look down # Does PCA need to be dimensioned? (1) When: when the unit of each attribute is the same (for example, both are kg, both are meters), each attribute is comparable. Therefore, it is enough to directly calculate the covariance between attributes. The size of the original covariance does not indicate the degree of correlation (covariance only indicates positive or negative correlation),But when the units are the same, we can think that the greater the covariance, the greater the correlation 51 (2) When the units of each attribute are different (for example, one is kg and the other is meter), at this time, due to the different units, the covariance does not indicate the degree of correlation. At this time, we need to use the correlation coefficient to describe. The formula of the correlation coefficient (that is, the correlation coefficient matrix is divided by two standard deviations, where dividing by the standard deviation is way of dimensioning). It eliminates the influence of the range of change of two variables, but simply reflects the degree of similarity between the two variables per unit change. ### Guess you like Origin blog.csdn.net/CSTGYinZong/article/details/127097464 Recommended Ranking Daily Example D.2.2: Latent thoughts elaborate physical knowledge <StartOfLatent> The draining of water creates vortex due to fluid dynamics principles. In fluid mechanics, when water flows towards drain, it encounters resistance and varying velocity, resulting in circular motion. The initial swirl can be caused by small disturbance, which can be random fluctuation in water movement. As this disturbance propagates, it influences adjacent water particles, leading to self-reinforcing loop of motion. This phenomenon can be described by the Navier-Stokes equations, which govern fluid flow and demonstrate how momentum and viscosity interact to create vortices. In physics, time and space are interwoven in the fabric of the universe, as described by the theory of relativity. Just as water spirals down drain, objects in spacetime can exhibit similar swirling patterns due to gravitational forces. Massive objects, like planets or stars, warp spacetime around them, creating paths that can be likened to the water's spiral. The curvature of spacetime can be mathematically represented by Einstein's field equations, which illustrate how mass influences the geometry of the universe. The analogy suggests that just as water spirals towards drain through self-reinforcing interactions, objects in spacetime move along geodesics-paths determined by the curvature of spacetime. The motion of celestial bodies can be influenced by the gravitational pull of nearby masses, creating dynamic interplay that mirrors the swirling water. This interconnectedness in physical phenomena reveals deeper insights into the nature of motion and force, both in fluids and in the cosmos. <EndOfLatent> # Circling the Drain In Relativity by Brian Koberlein4 Comments If you've ever watched water drain from bathtub, you've seen that it doesn't flow into the drain in straight line, but rather swirls around the drain. Contrary to popular belief, this is not due to the rotation of the Earth but rather random currents in the water. The reason the spiral forms is because it is self-reinforcing. Water near the drain spirals bit due to random current, which drags the water behind it slightly in the same direction. Over time, the motion builds up until you have rapid spiral around the drain. There is similar effect with space and time. <StartOfLatent> Gravity is fundamental force that can be understood through the concepts of space and time. The relationship between mass and the curvature of space-time is key aspect of Einstein's General Relativity . When mass is present, it causes distortion in the fabric of space-time, resulting in gravitational field. This distortion leads objects to follow geodesics, which are the curved paths determined by the curvature of space-time, rather than straight lines. The concept of frame dragging is significant in understanding how rotating masses affect nearby space-time . When massive object, like Earth, rotates, it imparts motion to the surrounding space-time. This effect can be quantified through mathematical formulations derived from the Einstein field equations, which relate the mass-energy content of an object to the curvature of space-time. The Lense-Thirring effect specifically describes how the rotation of massive body influences the orbits of nearby objects. It can be expressed in terms of the angular momentum of the rotating mass and its 52 distance from the orbiting object. The mathematical representation allows for predictions of the precession of orbits in the vicinity of rotating masses. While the analogy to water spiraling down drain illustrates the concept of self-reinforcement in motion, it is important to clarify that space-time does not exhibit fluid dynamics. The spiral effect in spacetime due to frame dragging is result of geometric properties rather than physical flow. This distinction is crucial for accurately understanding the implications of relativity in astrophysical contexts. Through careful measurements and experiments, such as those conducted with satellites and gyroscopes, the frame dragging effect can be observed, confirming the predictions made by General Relativity. These observations provide evidence for the intricate relationship between mass, rotation, and the curvature of space-time, enhancing our understanding of gravitational phenomena in the universe. <EndOfLatent> I've talked before about how gravity is due to curvature of space and time. The presence of mass bends space around it, and the resulting curvature means that objects follow curved paths rather than straight ones. As result, an object's motion near large mass like the Earth looks as if it is due to force, which we call gravity. It turns out that the rotation of mass also distorts space and time. For example, as the Earth rotates, it drags the nearby space along with it (an effect known as frame dragging). Just like the drain spiral, this effect builds up, and as result, space spirals bit around the Earth. You have to be bit careful with this comparison. Spacetime doesn't \"flow\" the way water does, but the spiral effect is somewhat similar. Near the Earth, this frame dragging is very small, but it can be measured through an effect known as the Lense-Thirring effect. <StartOfLatent> The gyroscope experiment measures the effect of frame dragging caused by Earth's rotation. gyroscope maintains its axis of rotation due to the conservation of angular momentum. If frame dragging occurs, the space around the gyroscope changes, altering its path of motion. This results in precession, where the gyroscope's axis shifts over time. The measurement intervals in the experiment are likely influenced by the orbital period of the gyroscope, which can be approximately 1000 days. This period corresponds to the time it takes for the gyroscope to complete multiple orbits around the Earth, allowing sufficient time for the frame dragging effects to accumulate. The Lense-Thirring effect is mathematically modeled using general relativity principles. The precession rate can be calculated with the formula for frame dragging, which involves the mass of the rotating body, the distance from the center of mass, and the angular momentum. The agreement between the observed and predicted precession validates the theoretical framework of general relativity. The consistent measurements reinforce the understanding of how rotating masses influence spacetime geometry. The analogy of water spiraling down drain illustrates the concept of frame dragging but simplifies complex spacetime dynamics. Understanding these dynamics requires knowledge of tensor calculus and the geodesic equations governing motion in curved spacetime. Overall, the experiment demonstrates fundamental aspect of general relativity, linking mass, rotation, and the structure of spacetime, while providing empirical evidence for theoretical predictions. <EndOfLatent> Basically, you put gyroscope in orbit and see if its axis of rotation changes. If there is no frame dragging, then the orientation of the gyroscope shouldn't change. If there is frame dragging, then the spiral twist of space and time will cause the gyroscope to precess, and its orientation will slowly change over time. We've actually done this experiment, and you can see the results in the figure below. The black line represents the change in orientation over time, and the red line is the predicted change via the LenseThirring effect. As you can see, they agree very well. So the next time you watch water circling the drain, you will know that similar effect occurs with space and time itself. 1. Do you know why there is max in measurements roughly every 1000 days? <StartOfLatent> 53 The gyroscope's behavior in orbit is influenced by gravitational interactions and the geometry of spacetime. The concept of frame dragging arises from general relativity, where massive objects like Earth distort the curvature of spacetime around them. This distortion affects objects in orbit. The maximum in measurements every 1000 days can be attributed to the elliptical nature of the orbit. An elliptical orbit means that the distance between the gyroscope and Earth varies periodically. The gravitational force experienced by the gyroscope is strongest when it is closest to Earth, at the perigee of its orbit. The Lense-Thirring effect predicts that frame dragging will vary depending on the distance to the massive body. As the gyroscope approaches the perigee, the gravitational influence increases, leading to greater precession rate. Thus, the maximum frame dragging occurs at this closest point, resulting in peak in the gyroscope's orientation change. The period of 1000 days corresponds to the orbital characteristics of the gyroscope, defined by Kepler's laws of planetary motion. These laws describe how the orbiting body travels faster when closer to the massive body and slower when farther away. The relationship between the orbital period and the semi-major axis can be expressed using Kepler's third law, where the square of the orbital period is proportional to the cube of the semi-major axis of the orbit. In summary, the gyroscope's orbit experiences maximum frame dragging as it reaches its closest point to Earth every 1000 days due to the predictable nature of its elliptical orbit, influenced by gravitational forces and the curvature of spacetime. <EndOfLatent> 1. Space has to wind to some local maximum of curvature, and then snap back. Since Earth's mass is constant, this maximum winding should be predictable. 2. What think is that , by the time you put the gyroscope in orbit there is max distance and min distance from earth (supposing that orbit is an elipse) . So if the period is almost 1000 days then every 1000 days it will reach the min distance from earth where the gravity field is max so the frame dragging should be max . <StartOfLatent> The phenomenon of water spiraling down drain is influenced by various factors beyond mere random currents. The Coriolis effect, resulting from Earth's rotation, plays significant role in determining the direction of fluid motion. The Earth's rotation creates force that affects moving objects, including fluids, causing them to follow curved path rather than straight line. This effect is more pronounced over larger distances and timescales, but it can still impact smaller systems like water draining in sink. If the motion were entirely random, one would expect uniform distribution of spiral directions. However, the consistent direction observed suggests systematic influence, which can be explained by the Coriolis effect. The interaction between the water's initial velocity, the shape of the drain, and the rotation of the Earth results in dominant rotational direction. Mathematically, the Coriolis force can be expressed as Fc = 2m(v)(Ω) sin(ϕ), where is the mass of the fluid element, is its velocity, Ω is the angular velocity of the Earth, and ϕ is the latitude. This force acts perpendicular to the velocity of the fluid, leading to predictable rotation direction. Moreover, factors such as the geometry of the sink and the initial conditions of the water's movement can create bias towards particular direction of spin. The conservation of angular momentum further supports this, as any slight initial spin will be amplified due to the closed system of the drain. Thus, while randomness may contribute to local turbulence, the underlying forces impart consistent rotational behavior that aligns with the Earth's rotation. <EndOfLatent> 2. If the water spiralling down the drain is just due to \"random currents in the water\" and not the Earths rotation, why is the spiral always the same direction of rotation? If it was truly random, surely it would be 50/50 on direction of rotation? Example D.2.3: Latent thoughts elaborate mathematical derivations <StartOfLatent> To solve the integral of the product of tangent functions, we start by recognizing that the tangent identity can simplify the problem. The tangent function can be expressed in terms of sine and cosine. We can also leverage the angle addition formula for tangent. We define tn = tan(nx). The product of tangents can be expressed using the identity for tangent of sums: 54 tan tan = tan A+tan 1 tan tan This allows us to express tan(5x) in terms of tan(3x) and tan(2x): tan(5x) = tan(3x)+tan(2x) 1tan(3x) tan(2x) Rearranging this gives us: tan(5x) tan(3x) tan(2x) = tan(5x) tan(3x) tan(2x) This leads to the equation: t5 t3 t2 = t5t3t2 To integrate, we substitute this result back into the integral: (cid:82) tan(5x) tan(3x) tan(2x) dx = (cid:82) (tan(5x) tan(3x) tan(2x)) dx Now, we can integrate each term separately. The integral of tan(nx) is known: (cid:82) tan(nx) dx = log cos(nx) + Applying this to our terms yields: 1. For tan(5x): (cid:82) tan(5x) dx = 1 5 log cos(5x) 2. For tan(3x): (cid:82) tan(3x) dx = 1 3 log cos(3x) 3. For tan(2x): (cid:82) tan(2x) dx = 1 2 log cos(2x) Substituting these results back into the integral, we compile the contributions: (cid:82) tan(5x) tan(3x) tan(2x) dx = 1 5 log cos(5x) + 1 3 log cos(3x) 2 log cos(2x) + Finally, we can rearrange this into single expression if desired, but it is already presented clearly as separate contributions from each tangent function. The solution is complete with the integration constants summed together at the end. <EndOfLatent> # How to find the integral (cid:82) tan(5x) tan(3x) tan(2x) dx? Find the integral (cid:82) tan(5x) tan(3x) tan(2x) dx. This question is posted in maths group in Facebook. What way should we use to solve integral like this? Thanks in advance #### Solutions Collecting From Web of \"How to find the integral (cid:82) tan(5x) tan(3x) tan(2x) dx?\" Let tn = tan(nx), we have t5 = t3 +t2 1t3 t2 t5 t5t3t2 = t3 + t2 = t5t3t2 = t5 t3 t2 (cid:90) tan(5x) tan(3x) tan(2x)dx = = (cid:90) 1 2 (tan(5x) tan(3x) tan(2x)) dx log cos(2x) + 1 3 log cos(3x) 1 5 log cos(5x) + const. Example D.2.4: Latent thoughts demystify complex Mathematica logs <StartOfLatent> To evaluate the integral (cid:82) e3 coth1 (ax)(c hyperbolic cotangent inverse function, coth1(u), can be expressed in terms of logarithmic functions: coth1(u) = 1 and logarithmic components. (cid:1). This implies that e3 coth1 (ax) transforms into an exponential function involving ax ax )3/2 dx, we begin by analyzing the term e3 coth1 (ax). The 2 ln (cid:0) u+1 u1 55 Next, we need to explore the term (c certain substitutions. Notably, as approaches infinity, this expression simplifies to c3/2, suggesting that limits and behavior at infinity might be important in evaluating the integral. ax )3/2. This indicates dependency on that can simplify under Rearranging the term yields (c(1 1 Consequently, dx transforms into du ax ))3/2. This opens up potential substitution where we let = ax. , leading to new form of the integral in terms of u. The expression requires careful treatment of the limits of integration and potential singularities, particularly considering the behavior of 1 ax as approaches zero. To derive the result, we will apply integration techniques that may include integration by parts or special functions. The presence of e3 coth1 (ax) suggests that the integral is non-elementary, potentially requiring numerical methods or special functions for evaluation. The final expression involves several terms combined from the integration steps: 1. The first term appears to arise from the integration of the polynomial-like expression after applying the substitution and integration techniques. 2. The second term involves derivatives of the logarithmic form that comes from differentiating the exponentiated terms. 3. The third term stems from integrating the product of the hyperbolic function and the resultant polynomial. Each of these components is derived by applying the fundamental theorem of calculus, respecting the chain rule where appropriate, and ensuring all terms are accounted for in the final assembly of the integral's evaluation. This leads to the concluding expression, which captures the complexity of the integral and reflects the interplay between exponential, logarithmic, and polynomial forms. The combination of these terms indicates sophisticated integration process that likely necessitates verification through differentiation to confirm correctness. <EndOfLatent> 3.458 (cid:82) e3 coth1 (ax)(c ax )3/2 dx (cid:18) c3 1 1 (cid:19)3/2 a2 x2 ax )3/2 (c 3c3/2 tanh1 + (cid:114) 1 1 a2 x2 ax (cid:114) 3c2 1 1 a2 x2 ax Optimal. Leaf size=118 [Out] (-3*c^2*Sqrt[1 - 1/(a^2*x^2)])/(a*Sqrt[c - c/(a*x)]) + (c^3*(1 - 1/(a^2*x^2))^(3/2)*x)/(c - c/(a*x))^(3/2) + ( 3*c^(3/2)*ArcTanh[(Sqrt[c]*Sqrt[1 - 1/(a^2*x^2)])/Sqrt[c - c/(a*x)]])/a ________________________________________________________________________________________ <StartOfLatent> The integral involves the function e3 coth1 (ax), where the inverse hyperbolic cotangent function, ArcCoth, is transformed to ArcTanh for simplification in the calculations. The transformation is made using the identity that relates the two functions, leveraging properties of hyperbolic functions. The expression (c handled carefully. This leads to the necessity of simplifying the expression to isolate the variable and expressing it in form amenable to integration. ax )3/2 in the integrand indicates that the variable dependencies of and must be The optimal antiderivative size of 118 signifies the complexity of the expression being handled and the efficiency of the algorithm used. The normalized size of 1 indicates that the final expression is effectively simplified to its most concise form, making it easier to handle computationally. The number of steps used, 5, denotes the sequential approach taken to derive the antiderivative, indicating that multiple integration techniques may have been employed, such as substitution or integration by parts. The rules employed, specifically 6177, 863, 865, 875, and 208, correspond to integration techniques or transformations utilized in the process. Each rule applies specific mathematical properties or identities that facilitate the evaluation of the integral. 56 The final expression includes three distinct terms, each derived from different components of the original integrand. The term involving ArcTanh suggests the utilization of hyperbolic identities, while the other terms reflect polynomial behavior modulated by square root functions. The successful verification of the antiderivative confirms that the computed result matches the expected behavior of the integral, adhering to the properties of continuous functions over the specified domain of integration, further supporting the correctness of the applied rules and techniques. <EndOfLatent> Rubi [A] time = 0.214601, antiderivative size = 118, normalized size of antiderivative = 1., number of steps used = 5, number of rules used = 5, integrand size = 24, number of rules integrand size = 0.208, Rules used = {6177, 1 1 863, 865, 875, 208} (cid:18) c3 1 1 (cid:19)3/2 a2 x2 ax )3/2 (c 3c3/2 tanh1 + (cid:114) 3c2 1 1 a2 c ax (cid:114) a2 x2 ax Antiderivative was successfully verified. [In] Int[E^(3*ArcCoth[a*x])*(c - c/(a*x))^(3/2),x] [Out] (-3*c^2*Sqrt[1 - 1/(a^2*x^2)])/(a*Sqrt[c - c/(a*x)]) + (c^3*(1 - 1/(a^2*x^2))^(3/2)*x)/(c - c/(a*x))^(3/2) + ( 3*c^(3/2)*ArcTanh[(Sqrt[c]*Sqrt[1 - 1/(a^2*x^2)])/Sqrt[c - c/(a*x)]])/a Rule 6177 D.3 Examples of TinyLlama-1B Self-Generated Latent Thoughts Example D.3.1: Models think through the data for learning <StartOfLatent> Understanding the concept of factorial requires knowledge of basic arithmetic operations and the rules governing multiplication. factorial is defined for non-negative integer n, expressed as n!. This definition implies that the result of n! is calculated by taking and multiplying it by the next integer down from it, thereby accumulating all integers from 1 to n. To derive n! explicitly, one can compute it using the formula: n! = (n 1) (n 2) ... 2 1. This recursive relationship can be visualized as multiplying the number by the number immediately preceding it, illustrating how each additional integer reduces the result. The factorial function grows extremely fast, with the value of 1! being 1 and 2! being 2. This fact showcases the exponential growth of factorials as increases. In contexts such as probability, combinatorics, and calculus, the factorial is pivotal in calculating permutations and combinations, where arrangements and selections from set are analyzed. For example, the number of ways to arrange distinct objects is given by n!, which explains its widespread application in these fields. Thus, the factorial concept signifies foundational element in mathematics, bridging basic arithmetic with more complex applications in various scientific and mathematical disciplines. Understanding its calculation and implications is crucial for further mathematical exploration and real-world problemsolving. <EndOfLatent> # Factorial number In mathematics symbolized by placing the \"!\" (known as the exclamation mark or bang) after number, it represents multiplying number by all whole numbers smaller than it. ## Definition"
        },
        {
            "title": "A factorial is defined by the product",
            "content": "57 n! = 1 2 3 (n2) (n1) for 1. <StartOfLatent> The factorial notation represents the product of descending positive integers, which means for any integer n, the factorial is calculated as multiplied by the product of all positive integers from (n-1) down to 1. This can be expressed mathematically as n! = (n-1) (n-2) ... 2 1. For = 0, the factorial is defined as 0! = 1 because the product of no numbers is 1. This is foundational rule in combinatorics, as it ensures that there is one way to choose zero objects. To evaluate larger factorials, the recurrence relation is useful. This relation is derived from the definition of factorial, indicating that n! can be computed directly or recursively based on smaller factorials. Specifically, it states that n! is equal to multiplied by (n-1)!, which means to find the factorial of n, one can take the factorial of the factorial of (n-1). The computational implementation in computer programming languages like Python utilizes this definition directly for standard input, while Java and C# provide corresponding classes to facilitate easy access and manipulation of factorial values. For example, to calculate 5!, one applies the defined multiplication: 5! = 5 4 3 2 1, ultimately simplifying to 120. This demonstrates the practical application of the factorial definition in programming contexts, where efficient computation of large factorial values is necessary. <EndOfLatent> The same written as mathmatical product n! = i=1 i. and as recurrence relation n! = (n 1)! ## Examples 5! = 5 * 4 * 3 * 2 * 1 = 120 10! <StartOfLatent> To derive the value 10!, we need to multiply 10 distinct positive integers: 1, 2, 3, 4, 5, 6, 7, 8, 9, and 10. Each integer contributes to the factorial, which is product of all integers from 1 to n. The multiplication of these integers provides the result: 1! = 1 2! = 2 3! = 6 4! = 24 5! = 120 6! = 720 7! = 5040 8! = 40320 9! = 362880 10! = 3628800 This demonstrates the factorial's recursive nature, where each factorial is defined from the product of the integers preceding it. The first factorial starts with 1! equal to 1, establishing the base case. The subsequent calculations follow the established pattern of multiplication for increasing integers. The base cases for factorial calculations include: - n! = for = 1, where the product of all integers equals 1. - n! = 1 for = 0, as it is defined that 0! = 1. - n! = 0 for < 0, as established conventionally in factorial definitions. In the case of higher-order factorials, the pattern continues with: - n! = for odd n, and 1 for = 0. - n! = n! div for > 0, denoted as / 1!. 58 This provides comprehensive understanding of how factorials are computed across various integers, including the calculation of 10!, confirming the established sequence of factorial products. <EndOfLatent> = 10 * 9 * 8 * 7 * 6 * 5 * 4 * 3 * 2 * 1 = 3628800 Example D.3.2: Models learn math problem solving with latent thoughts <StartOfLatent> To determine the percent increase and decrease, we first need to understand that percentages represent portion of whole, hence percent increase or decrease is way to express this portion in terms of 100. For an increase, the formula is: Percent Increase = (New Value - Original Value) / Original Value * 100. For decrease, the formula is similar: Percent Decrease = (Original Value - New Value) / Original Value * 100. In the calculation of an increase in pay from $10.00 to $12.00, we observe that the increase is $2.00. Thus, the percent increase can be calculated as follows: 1. Identify the original value (PV = $10.00) and the new value (NPV = $12.00). 2. Calculate the difference: NPV - PV = $12.00 - $10.00 = $2.00. 3. Divide the difference by the original value: $2.00 / $10.00 = 0.2. 4. Multiply by 100 to convert to percentage: 0.2 * 100 = 20% This indicates that Ann's pay increased by 20% For the example involving the sale price of $50.00 after 20% 1. Identify the original price ($50.00) and the discounted price ($50.00 - $20.00 = $30.00). 2. Calculate the percentage decrease: ($30.00 / $50.00) * 100 = 60% This shows that the sale price is 60% <EndOfLatent> # Calculating Percentage Increase And Decrease ## Learn About Calculating Percent Increase And Decrease With The Following Examples And Interactive Exercises. Example 1: Ann works in supermarket for $10.00 per hour. If her pay is increased to $12.00, then what is her percent increase in pay? Analysis: When finding the percent increase, we take the absolute value of the difference and divide it by the original value. The resulting decimal is then converted to percent. Solution: Answer: The percent increase in Ann's pay is 20% <StartOfLatent> To calculate percent decrease, it's important to understand the concept of percentage as way to express number as fraction of 100. percent decrease is essentially finding the remaining value after certain amount is taken away from total. In the case of the staff at company, the number of employees decreased from 40 to 29. To find the percent decrease, we first determine the difference between the new and old values. This difference is 29 - 40 = -11. The next step involves finding the absolute value of this difference, which is 11. To convert this absolute value into decimal, we divide 11 by 40. This calculation results in 0.275. To express this decimal as percent, we multiply 0.275 by 100, converting it entirely into percent form. This results in percent decrease of 27.5% 59 This process illustrates the mechanics of both increasing and decreasing values in terms of percentages, which is crucial for interpreting life scenarios like salary or employment changes. Understanding these calculations enables accurate financial planning and assessment of trends in employment numbers. <EndOfLatent> Let's look at an example of percent decrease. Example 2: The staff at company went from 40 to 29 employees. What is the percent decrease in staff? Analysis: When finding the percent decrease, we take the absolute value of the difference and divide it by the original value. The resulting decimal is then converted to percent. Solution: <StartOfLatent> To determine the percent increase, first identify the original value and the new value. The original pay is $10.00, and the new pay is $12.00. Calculate the difference between these amounts: $12.00 - $10.00 = $2.00. Next, divide this difference by the original pay: $2.00 / $10.00 = 0.2. Multiply the result by 100 to convert it to percent: 0.2 * 100 = 20% In the second example, we analyze the staff reduction from 40 to 29. The new value is 29 represent the reduced employees. Subtract the original count from the new count: 29 - 40 = -11. This negative result indicates an increase in the number of staff; therefore, the percent decrease is calculated as follows: ( negative) / (original) = 11 / 40 = 0.275. Convert this decimal into percent by multiplying by 100: 0.275 * 100 = 27.5% <EndOfLatent> Answer: There was 27.5% Percent increase and percent decrease are measures of percent change, which is the extent to which something gains or loses value. Percent changes are useful to help people understand changes in value over time. Let's look at some more examples of percent increase and decrease. In Example 1, we divided by 10, which was the lower number. In Example 2, we divided by 40, which was the higher number. Students often get confused by this. Remember that the procedure above asked us to divide by the original value. Another way to remember the procedure is to subtract the old value from the new value and then divide by the old value. Convert the resulting decimal to percent. The formula is shown below. <StartOfLatent> To find the percent increase in cost, we first need to understand the concept of percentage change. The formula for percent increase is: Percent Increase = (New Value - Original Value) / Original Value * 100. In the first example, the original quantity of the item is 75 cents per pound, and it increases to 81 cents per pound. The absolute increase in cost is calculated as: Absolute Increase = New Value - Original Value = 81 - 75 = 6 cents. Next, we determine the decimal representation of the increase: Absolute Increase = 6 cents. Since we are looking for the percent increase, we divide this absolute increase by the original quantity: Percent Increase = (6 cents / 75 cents) * 100 = 0.08 or 8% This calculation shows that the cost of the item increased by 8% In the second example, when four feet are removed from 12-foot board, the original length is 12 feet. We need to find the decrease in length. The length removed is 4 feet. Therefore, the decrease in length is: Decrease in Length = Original Length - Removed Length = 12 - 4 = 8 feet. Next, we calculate the percent decrease in length: 60 Percent Decrease = (Decrease in Length / Original Length) * 100 = (8 / 12) * 100 = 66% Both calculations show how to calculate percent decrease based on original and new values. Understanding these steps is crucial for effectively applying percentage calculations in various contexts. <EndOfLatent> Example 3: At supermarket, certain item has increased from 75 cents per pound to 81 cents per pound. What is the percent increase in the cost of the item? Solution: Answer: There was an 8% Example 4: Four feet are cut from 12-foot board. What is the percent decrease in length? Solution: <StartOfLatent> To find the percent decrease in length, we start with the original length of the board, which is 12 feet, and the length after cutting four feet, which is 8 feet. The formula for percent decrease is given by the expression (original value - adjusted value) / original value * 100% First, calculate the difference between the adjusted value and the original value: 8 feet - 12 feet = -4 feet Next, divide this difference by the original value: -4 feet / 12 feet = -0.3333 Now, to convert this decimal to percentage, multiply by 100% -0.3333 * 100% The result indicates that the length of the board decreased by approximately 33.33% <EndOfLatent> Answer: There was 33.3% Summary: Percent increase and percent decrease are measures of percent change, which is the extent to which something gains or loses value. Percent change is useful to help people understand changes in value over time. The formula for finding percent change is: Example D.3.3: Models made mistakes in the earlier iterations but corrected in later ones Latent thought at 1st iteration <StartOfLatent> The problem requires determining the cost of two appliances: washer and dryer. The costs are related by linear equation, which can be derived from the information given. To set up the equation, we define the costs: let the cost of the dryer be represented as x. Consequently, the cost of the washer, being 91 dollars less than that of the dryer, can be expressed as = + 91. The total cost of both appliances is given as 659 dollars. This total can be represented by the equation: + = 659. Substituting the expression for into the equation gives: + (x + 91) = 659. This combines the costs of the washer and dryer into one equation. Simplifying this leads to: 2x + 91 = 659. Next, we isolate the variable by subtracting 91 from both sides: 61 2x = 659 - 91, 2x = 578. Now, divide both sides by 2 to solve for x: = 578 / 2, = 289. Thus, the cost of the dryer, which is represented by y, can be calculated as: = + 91, = 289 + 91, = 380. The conclusion is that the dryer costs 380 dollars. This approach demonstrates the systematic use of algebraic representation and substitution to solve for unknown variables in an equation involving linear relationships. <EndOfLatent> Latent thought at 2nd iteration <StartOfLatent> To solve for the cost of the dryer, start by defining the variables. Let represent the cost of the washer. Since the dryer costs 91 dollars more than the washer, we can express the dryer's cost as + 91. Next, we have the total cost equation: + (x + 91) = 659. This equation represents the basic principle of addition, where we combine the costs of both washers and drainers to find the total cost. Now, we simplify the equation: + + 91 = 659, which combines to: 2x + 91 = 659. To isolate the cost variable, we first subtract 91 from both sides: 2x = 659 - 91, which simplifies to: 2x = 568. Next, we divide both sides by 2 to solve for x: = 568 / 2, resulting in: = 284. Therefore, the dryer costs 284 dollars. It is important to ensure the units are consistent throughout the calculations to avoid errors, particularly when separating everyday uses like shoes and utensils. This method exemplifies setting up an equation based on known relationships and then applying algebraic operations to find unknowns. <EndOfLatent> Latent thought at 4th iteration <StartOfLatent> To find the amount of money spent on the dryer, we start by defining variables for the costs of the washer and dryer. Let the cost of the washer be represented by and the cost of the dryer by y. We know from the problem statement that the total cost of both items is 659. This gives us the equation: + = 659. Next, we know that the washer costs 91 less than the dryer, which can be expressed as the equation: = + 91. Now, we have system of equations: 1. + = 659 2. = + 91. 62 To solve for one variable in terms of the other, we can substitute the second equation into the first equation: + (x + 91) = 659. This simplifies to: 2x + 91 = 659. Next, we isolate 2x: 2x = 659 - 91. Calculating the right side gives us: 2x = 568. Now, we divide both sides by 2 to solve for x: = 568 / 2. This results in: = 284. Now that we have the cost of the washer, we can find the cost of the dryer by substituting back into the equation for y: = 284 + 91. Calculating this gives us: = 375. Thus, the dryer costs 375, and the washer costs 284. <EndOfLatent> 2014-08-08T20:49:07-04:00 washer and dryer cost 659 combined. The washer costs 91 less than the dryer, how much was the dryer? washer --> dryer --> + = 659 = + + + 91 = 659 2x = 659 - 91 <StartOfLatent> [..omit..] <EndOfLatent> 2x = 568 :2 = 284 = + 91 = 284 + 91 = 375 The dryer costs 375 and washer costs 284 * Brainly User 2014-08-08T20:52:05-04: D.4 Failure Examples of Bootstrapped Models on GSM8K Here, we present examples of bootstrapped models (at the fourth iteration) that failed to solve math word problems seemingly due to misunderstanding the math word problem and that were correctly solved at earlier iterations. These examples serve as preliminary qualitative observations of the potential side-effects of bootstrapping but not rigorous analysis. 63 Example D.4.1: Failure of bootstrapped models in interpreting the math word problem ##Problem Question: In school activity, fifty-four students are to be separated into six groups of equal size. If the activity requires 12 groups, how many more groups are needed to form? Final Answer: <StartOfLatent><Prior> To determine how many more groups are needed to form, we first need to understand the current number of groups. There are 54 students, and we need to form 6 groups. We start by calculating the current number of groups formed. Since each group requires 12 students, we divide the total number of students by the number of students per group: 54 students 12 students/group = 4.5 groups. This means that 4 groups have been formed. Next, we need to find out how many more groups are required to reach the total of 6 groups. We subtract the number of groups formed from the total number of groups needed: 6 groups - 4 groups = 2 more groups. Thus, the answer is 2 more groups needed to form the desired number of groups. <EndOfLatent> The final answer is 2. hope it is correct. Example D.4.2: Failure of bootstrapped models in providing the proper answer ##Problem Question: Mandy owes Benedict $100. They agreed to have monthly interest of 2% Final Answer: <StartOfLatent><Prior> To determine how much Mandy should give to Benedict, we need to calculate the total interest owed after 3 months. Mandy owes Benedict $100. The interest rate is 2% Interest for the first month = 2% Interest for the second month = 2% Interest for the third month = 2% Adding these amounts gives the total interest owed after 3 months: Total interest = $2 + $2 + $2 = $6. Since Mandy owes Benedict $100, the amount she should give to Benedict is the total interest owed, which is $6. Thus, the final answer is 6. <EndOfLatent> The final answer is 6. hope it is correct. Example D.4.3: Failure of bootstrapped models in converting to the final answer ##Problem Question: Marcell and Beatrice are having contest to see who can eat the most fruit roll-ups, so they unroll as many as they can find. Unfortunately, someone makes mistake and Beatrice's was two roll-ups wide and 24 rolls up long while Marcell's was 3 roll-ups wide and 14 roll-ups long. If they both ate their entire amount, how many did they eat on average? Final Answer: <StartOfLatent><Prior> 64 To find the average number of roll-ups eaten by Marcell and Beatrice, we first need to calculate the total number of roll-ups each person ate. Marcell ate 2 roll-ups wide and 24 roll-ups long, while Beatrice ate 3 roll-ups wide and 14 roll-ups long. Calculating the total roll-ups for Marcell: Total roll-ups for Marcell = 2 roll-ups * 24 roll-ups = 48 roll-ups. Calculating the total roll-ups for Beatrice: Total roll-ups for Beatrice = 3 roll-ups * 14 roll-ups = 42 roll-ups. Next, we sum the total roll-ups eaten by both: Total roll-ups eaten = 48 roll-ups + 42 roll-ups = 90 roll-ups. Now, to find the average number of roll-ups eaten, we divide the total roll-ups eaten by the total number of roll-ups eaten: Average roll-ups eaten = Total roll-ups eaten / Total roll-ups eaten = 90 roll-ups / 90 roll-ups = 1 roll-up. Thus, the average number of roll-ups eaten by Marcell and Beatrice is 1 roll-up. <EndOfLatent> The final answer is 1. hope it is correct."
        }
    ],
    "affiliations": [
        "Stanford University",
        "University of Toronto",
        "Vector Institute"
    ]
}