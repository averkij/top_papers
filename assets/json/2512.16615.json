{
    "paper_title": "Trainable Log-linear Sparse Attention for Efficient Diffusion Transformers",
    "authors": [
        "Yifan Zhou",
        "Zeqi Xiao",
        "Tianyi Wei",
        "Shuai Yang",
        "Xingang Pan"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Diffusion Transformers (DiTs) set the state of the art in visual generation, yet their quadratic self-attention cost fundamentally limits scaling to long token sequences. Recent Top-K sparse attention approaches reduce the computation of DiTs by compressing tokens into block-wise representation and selecting a small set of relevant key blocks, but still suffer from (i) quadratic selection cost on compressed tokens and (ii) increasing K required to maintain model quality as sequences grow. We identify that their inefficiency is due to the single-level design, as a single coarse level is insufficient to represent the global structure. In this paper, we introduce Log-linear Sparse Attention (LLSA), a trainable sparse attention mechanism for extremely long token sequences that reduces both selection and attention costs from quadratic to log-linear complexity by utilizing a hierarchical structure. LLSA performs hierarchical Top-K selection, progressively adopting sparse Top-K selection with the indices found at the previous level, and introduces a Hierarchical KV Enrichment mechanism that preserves global context while using fewer tokens of different granularity during attention computation. To support efficient training, we develop a high-performance GPU implementation that uses only sparse indices for both the forward and backward passes, eliminating the need for dense attention masks. We evaluate LLSA on high-resolution pixel-space image generation without using patchification and VAE encoding. LLSA accelerates attention inference by 28.27x and DiT training by 6.09x on 256x256 pixel token sequences, while maintaining generation quality. The results demonstrate that LLSA offers a promising direction for training long-sequence DiTs efficiently. Code is available at: https://github.com/SingleZombie/LLSA"
        },
        {
            "title": "Start",
            "content": "Trainable Log-linear Sparse Attention for Efficient Diffusion Transformers Zeqi Xiao1 1S-Lab, Nanyang Technological University Yifan Zhou1 Tianyi Wei1 2 Wangxuan Institute of Computer Technology, Peking University Xingang Pan1 Shuai Yang {yifan006, zeqi001, tianyi.wei, xingang.pan}@ntu.edu.sg williamyang@pku.edu.cn 5 2 0 2 8 1 ] . [ 1 5 1 6 6 1 . 2 1 5 2 : r Figure 1. Comparison between general Top-K sparse attention and our Log-linear Sparse Attention (LLSA). In the example, we use token sequence of length = 8, block size = 2, Top-K parameter = 1. To reduce the complexity of the selection stage from O(N 2) to O(N ), we extend single-level selection to O(log ) levels. To achieve this, we compute the Top-K of the full sequence on the coarsest level and recursively compute the sparse Top-K on the remaining levels. To preserve the global context for attention, we enrich the key, value sets for each query with coarse tokens of length O(K log ) found in the selection stage."
        },
        {
            "title": "Abstract",
            "content": "Diffusion Transformers (DiTs) set the state of the art in visual generation, yet their quadratic self-attention cost fundamentally limits scaling to long token sequences. Recent Top-K sparse attention approaches reduce the computation of DiTs by compressing tokens into block-wise representation and selecting small set of relevant key blocks, but still suffer from (i) quadratic selection cost on compressed tokens and (ii) increasing required to maintain model quality as sequences grow. We identify that their inefficiency is due to the single-level design, as single coarse level is insufficient to represent the global structure. In this paper, we introduce Log-linear Sparse Attention (LLSA), trainable sparse attention mechanism for extremely long token sequences that reduces both selection and attention costs from quadratic to log-linear complexity by utilizing hierarchical structure. LLSA performs hierarchical TopK selection, progressively adopting sparse Top-K selection with the indices found at the previous level, and introduces Hierarchical KV Enrichment mechanism that preserves global context while using fewer tokens of different granularity during attention computation. To support efficient training, we develop high-performance GPU implementation that uses only sparse indices for both the forward and backward passes, eliminating the need for dense attention masks. We evaluate LLSA on high-resolution pixelspace image generation without using patchification and VAE encoding. LLSA accelerates attention inference by 28.27 and DiT training by 6.09 on 256 256 pixel token sequences, while maintaining generation quality. The results demonstrate that LLSA offers promising direction for training long-sequence DiTs efficiently. 1. Introduction Diffusion Transformers (DiTs) [25] have become the stateof-the-art backbone for visual generation tasks. As resolution and sequence length scale up, the dominant bottleneck lies in the quadratic complexity of full self-attention [30], where the computation cost grows as O(N 2) with token length . In practice, this prevents DiTs from scaling to high-resolution images or long video sequences. For example, FLUX [18] operates on 64 64 latent image (4096 tokens), while Wan 2.1 [31] uses 21 45 80 latent videos 1 (75,600 tokens). Scaling to longer sequences requires fundamentally more efficient attention mechanism. Sparse attention has recently emerged as promising alternative to full attention [34, 35, 39]. widely adopted variant is Top-K block sparse attention [22, 37, 39], which operates in three stages: (1) compress query and key tokens into coarse representations that summarize the block-wise information; (2) compute the coarse similarity scores between compressed tokens, and select the Top-K key blocks for each query block; (3) perform block sparse attention on the selected blocks. Although effective for moderate sequence lengths, this paradigm faces two major limitations when scaling further: (1) The selection stage still incurs quadratic cost on compressed tokens; (2) To maintain global context, prior methods set the sparsity as constant and use larger for longer sequences [33, 40, 41]. These limitations arise from the single-level design of existing Top-K sparse attention: single coarse-grained view is insufficient to represent global structure for long sequences. natural solution is to extend the single level into hierarchical structure, where global information can be represented using only O(log ) coarse tokens of progressively coarser granularity. Inspired by prior work showing that the dense attention matrix can be approximated by hierarchical coarse attention matrices [38, 43], we introduce Log-linear Sparse Attention (LLSA), trainable sparse attention mechanism that reduces attention complexity from quadratic to log-linear. LLSA builds upon Top-K sparse attention with two key innovations: (1) We compress query and key features across multiple logarithmic hierarchy levels and progressively perform Top-K selection from coarse to fine. This hierarchical design reduces the complexity of the selection stage from O(N 2) to O(N ). (2) Instead of using large for longer sequences, we propose Hierarchical KV Enrichment mechanism that incorporates coarse key/value representations selected at higher hierarchy levels into attention computing. This preserves global context and mitigates information loss from sparsification, allowing LLSA to operate with significantly smaller and lower cost. Efficient GPU implementation is essential for Top-K sparse attention. In standard FlashAttention [6], the sparse indices are represented as binary mask. Constructing and processing this mask leads to quadratic memory and computation overhead. To avoid this, we implement GPUefficient Top-K indexing algorithm that operates directly on sparse indices. In the forward pass, we gather only the selected key blocks. In the backward pass, we dynamically compute the reverse lookup of sparse indices via lightweight sparse index transpose kernel. This ensures end-to-end log-linear complexity during training. We evaluate LLSA on high-resolution pixel space image generation. Specifically, we train pixel DiTs without patchification and VAE encoding up to 256 256 (65,536) pixel tokens with one H200 GPU. LLSA significantly improves the training efficiency of full attention DiTs by 6.09 while maintaining generation quality. Compared to existing TopK sparse attention algorithms [40, 41], LLSA achieves higher generation quality and training throughput. Thanks to Hierarchical KV Enrichment, LLSA maintains global context even with significantly smaller K. In our experiments, LLSA with = 8 outperforms prior Top-K methods even when their is substantially larger (K = 20 or = 32), highlighting the practical efficiency of our design. We further integrate LLSA into PixelFlow [2] to show its capacity on ImageNet [8]. Moreover, our backward kernel achieves nearly constant throughput across different sequence lengths, confirming the linear complexity of our implementation. In summary, the contributions are listed as follows. We propose Log-linear Sparse Attention (LLSA), trainable attention mechanism that scales DiTs to long sequences with log-linear complexity and comparable quality. We develop high-performance GPU implementation for Top-K indexing that eliminates dense mask construction in both forward and backward passes. Extensive empirical results showing that LLSA consistently outperforms prior Top-K sparse attention methods, in both quality and efficiency, especially at small K, and scales effectively to high-resolution pixel DiTs and largescale datasets. 2. Related Works 2.1. Sparse Attention in DiTs Early attempts mainly adopt training-free strategies that prune irrelevant tokens using predefined sparsity patterns [1, 34, 36] or dynamic search rules [14, 35, 39]. More recently, inspired by trainable sparse attention in NLP [22, 37], several methods, such as VMoBA [33], VSA [41], and SLA [40], introduce learnable sparse attention for efficient DiT training. These approaches rely on single-level block selection mechanism that identifies important token blocks at one resolution. Our work extends this idea to multi-level hierarchical selection scheme, enabling substantially better scalability for long-sequence attention. 2.2. Log-linear Attention Log-linear attention approaches can be categorized by their strategies for determining the relevant keys for each query. Several methods use static rules based on absolute positions. H-Transformer [43] constructs fixed hierarchical decomposition of the attention matrix; Fast Multipole Attention [15] applies Fast Multipole Method to use differ2 ent resolutions according to the distance between the query and key tokens. Radial Attention [19] imposes static attention mask according to the pattern of the video data. Log-linear Attention [10] derives linear-attention variant that maintains logarithmic hidden states using Fenwick tree. Other methods rely on dynamic key selection. Reformer [17] efficiently clusters the queries and keys using locality-sensitive hashing. The most relevant prior work is Multi-resolution Attention [38], which performs hierarchical Top-K selection. However, its contribution is mainly theoretical and does not provide high-performance GPU implementation for extremely long token sequences. In contrast, our methods implementation is based on block sparse attention [6], and is validated on long-sequence DiT training. 2.3. Pixel-space DiTs Due to computational constraints, the original DiT [25] operates in latent space [26], relying on VAE encoders and patchification to reduce token length. Pixel-space DiTs therefore require either more efficient diffusion processes or architectural modifications involving aggressive downsampling. PixelFlow [2] splits the diffusion process into multiple stages of different resolutions and uses universal DiT for all stages. PixNerd [32] integrates coordinatebased MLP to decode pixel-level output of large-patch DiT. HDiT [4] introduces U-shaped DiT with progressive downsampling and upsampling. To our knowledge, no prior work has trained pure pixel-space DiT achieving state-of-the-art performance on high-resolution data without any input downsampling. In this paper, we verify our LLSA on lightweight pixel-space DiT trained on highresolution dataset, demonstrating that our attention mechanism matches the quality of full attention while significantly improving efficiency. 3. Background In this section, we introduce simplified implementation of Block Sparse FlashAttention [5, 6] without the scaling factor and safe softmax. We then analyze the computing complexity of Block Sparse FlashAttention with Top-K selection. 3.1. Block Sparse FlashAttention Given query, key, value features Q, K, RN d, where is the length of the token sequence and is the feature dimension, the attention output RN is: = softmax(QK), = PV. (1) To avoid storing the large intermediate matrix RN in high bandwidth memory (HBM), FlashAttention uses tiling to compute output block-by-block. Specifically, Q, K, V, are divided into blocks of block size such , e.g., = [Q1, ..., QT ], Qi RBd. For that = each query block Qi, we compute Oi as: ) RBB Pi,j = exp(QiKj (cid:88) Oi = Pi,jVj RBd j= (cid:88) li = rowsum(Pi,j) RB j=1 Oi = Oi li, (2) (3) (4) (5) where denotes element-wise division with li broadcast across columns. In implementation, we compute Pj in the j-th iteration and accumulate Oi and li across the loop. FlashAttention can be natively extended to sparse version if we know that the attention between the i-th block and the j-th block can be skipped: Given binary sparsity mask RT , the iteration that computes Pi,j is skipped if Mi,j = 0. The main challenge lies in determining M. common method for identifying the is the Top-K selection approach. 3.2. Complexity of Top-K Sparse Attention The Top-K selection consists of three steps. First, we compress the inputs in each block via mean pooling and obtain Q, RT d, where is the summarization of Qi, Kj. Then, we compute the full attention score = QK on the compressed representations. Finally, we sort the attention score for each query block and mark the Top-K key blocks as valid. i, With Top-K selection, the Top-K sparse attention can be divided into two stages: selection stage that computes similarities on compressed tokens and performs TopK to obtain M, and sparse attention stage that operates on the selected blocks. The selection stage computes pairwise scores among the coarse tokens and applies TopK selection on the scores, resulting in complexity of O(T 2d) + O(T 2K) = O(N 2B2(d + K)). In contrast, the sparse attention stage processes only blocks per query, with complexity of O(N KBd). The total cost is thus O(N 2B2(d + K)) + O(KN Bd), However, as we scale to longer sequences, the quadratic term in the selection stage, O(N 2), dominates the total running time. This means that even though the sparse attention computation grows only linearly with , the overall cost is still bottlenecked by the O(N 2) selection stage. As result, existing Top-K sparse attention methods fail to maintain efficiency when becomes large. 3 Algorithm 1 Log-linear Sparse Attention Input: Features Q, K, RN d, block size B, Top-K parameter K, number of coarse levels L, number of KV Enrichment levels Le. Output: Attention output O. 1: Q(0), K(0), V(0) = Q, K, 2: for = 1 to do 3: Hierarchical Compression Q(l), K(l), V(l) = poolB(Q(l1), K(l1), V(l1)) 4: end for 5: (L) = [1, ..., BL ] 6: for = to 1 do 7: {Q(l) Hierarchical Top-K Selection Bl+1 blocks Divide Q(l), K(l), V(l) into (l) = }, {V(l) }, {K(l) } for = 1 to do Gather K(l) S(l) = Q(l) (l1) = topk indices(S(l) from {K(l) K(l) , K) } with indices (l) 8: 9: 10: 11: end for 12: 13: end for 14: Divide Q(0), K(0), V(0) }, {V(0) } }, {K(0) {Q(0) into = blocks Attention 15: for = 1 to do 16: 17: 18: Kc {}, Vc {} for = 0 to Le do Hierarchical KV Enrichment } with inGather K(l) }, {V(l) from {K(l) , V(l) dices (l) W(l) = Bl Append K(l) KV Reweighting W(l), V(l) W(l) to Kc, Vc end for Oi = FlashAttention(Q(0) , Kc, Vc) 23: end for 24: return = {Oi} 4. Log-linear Sparse Attention Mechanism LLSA aims to reduce the complexity of Top-K sparse attention from O(N 2) to O(N log ). This requires both an efficient and effective algorithm, as well as high-performance GPU kernel implementation for sparse operations. We introduce the algorithm of LLSA in Sec. 4.1, the complexity analysis in Sec. 4.2, and the implementation of Top-K kernels in Sec. 4.3 4.1. Log-linear Sparse Attention As discussed in Sec. 3, general Top-K sparse attention can be divided into three steps: compression, Top-K selection, and sparse attention. To reduce the complexity of the attention, we extend the single-level compression into logarithmic hierarchy compression and apply sparse Top-K selection in coarse-to-fine manner. Since the receptive field of each query is reduced by the sparse selection, we propose 19: 20: 21: 22: Hierarchical KV Enrichment to preserve the global context. To ensure the coarse tokens have correct importance, we apply KV Reweighting to the coarse tokens during attention computation. The full algorithm is shown in Alg. 1. For simplicity, we assume the shapes of Q, K, are the same, the token length is divisible by the largest block size BL, and the parameter is the same for all coarse levels. Hierarchical Compression. Given attention inputs Q, K, RN d, block size B, number of levels = logB 1 (as we need to reserve enough tokens for tiling on the coarsest level), we compute hierarchical representation Q(l), K(l), V(l) RN/Bld. We set Q, K, as the finest tokens Q(0), K(0), V(0) and recursively downsample the features via mean pooling. As result, coarse token in l-th level is summarization of tokens in (l 1)-th level. Hierarchical Top-K Selection. In this step, we aim to obtain the hierarchical Top-K indices (l) for all levels. We introduce hyperparameter that denotes the number of activated K, tokens in each level. Similar to block sparse FlashAttention [6], we assume neighboring tokens share the same sparse pattern. Therefore, for level l, the shape of sparse indices (l) is N/Bl+1 K. In hierarchical Top-K selection, we first compute the full similarity matrix S(L) of coarsest tokens Q(L), K(L) by S(L) = Q(L)K(L) . We then compute the Top-K indices of S(L) for each query token. The indices indicate that one query block at the 1 level has KB key candidates. Starting from the second coarsest level, we gather the keys from the sparse indices and compute the similarity and sparse Top-K indices from KB candidates for the next level recursively. The total complexity of this stage is O(N ), and the proof will be given later. Hierarchical KV Enrichment. In the final step, we aim to compute the final attention output using the hierarchical Top-K indices obtained in the previous step. In addition to the finest sparse tokens K(0), V(0), we append the sparse tokens K(l) from all levels selected in the prei vious step to the final key, value sets for each query block. The coarse key, value tokens contain the information of the global context, filling up the information loss caused by the sparse selection. Since the activated tokens are obtained from sparse indices and the number of levels is O(log ), the number of activated tokens is O(N log ). , V(l) In practice, as shown in Alg. 1, we introduce hyperparameter Le to determine the number of KV enrichment levels. For example, when Le = 0, then we do not append the coarse token. When Le = 1, we append the coarse tokens from the second finest level. We set Le = by default. KV Reweighting. The design of the Hierarchical KV Enrichment mechanism introduced in the last section is not optimal: the coarse tokens contain more information, but their importance is equivalent to that of the finest tokens, which is not sufficient. To address this, we assume the finest to4 kens corresponding to one coarse token can be roughly recovered by nearest upsample. Therefore, the importance of each token should be proportional to its block size. e.g., coarse token obtained by averaging 16 tokens should have weight of 16. In practice, we add weighting term W(l) to K(l), V(l) such that W(l) = Bl. This design enhances the model quality without increasing the training overhead. 4.2. Complexity Analysis LLSA consists of two main components: hierarchical TopK selection and sparse attention with Hierarchical KV Enrichment. Selection Stage. At the hierarchy level l, let (l) = N/Bl For each level, we comdenote the sequence length. pute similarity scores only between the (l)/B fine query blocks and the KB candidate key blocks identified from the coarser level. The total cost across 1 levels (except the coarsest level) is therefore L1 (cid:88) l=0 (cid:18) Bl+1 KB (cid:19) (cid:32) = L1 (cid:88) (cid:33) Bl = O(N K), l=0 l=1 Bl < (6) because the geometric series (cid:80)L B1 converges to constant independent of and K. Thus, the selection stage runs in O(N K) instead of the O(N 2) cost of the previous Top-K sparse attention.1 Sparse Attention Stage. For each query, LLSA attends to (i) the fine-level Top-K keys and (ii) logarithmic number of enriched coarse KV tokens collected from all hierarchy levels. The number of enriched KV tokens is therefore O(K log ). Computing attention over these candidates gives complexity of O(N log ). Overall Complexity. Combining the two components, the total complexity of LLSA is O(N K) + O(N log ) = O(N log ). Since is kept constant regardless of , the overall complexity becomes O(N log ), which is asymptotically more efficient than the O(N 2) cost of full attention and the O(N 2) dominant term in previous Top-K sparse attention methods. 4.3. Efficient Kernel Implementation To ensure the algorithms align with theoretical computational complexity, we must implement high-performance, parallel program adapted to modern GPUs. For most querymajor operations (e.g., attention forward, query gradient backward), we only need to replace the full key, value traversal with sparse key, value gathering in FlashAttention [5] forward loop. However, the implementation of key, value backward, key-major operation, is not trivial since 1We only analyze the complexity of similarity matrix computation for simplicity. The complexity of Top-K depends on the algorithm used in implementation. If both and are constant, then the complexity of Top-K is still O(N ). 5 Algorithm 2 Sparse Top-K Indices Transposition Input: Indices RT K, Top-K parameter K. Output: Flat sparse query indices Iq RT K, cumulative key offsets RT +1. 1: 0 RN +1 2: Iq 0 RT 3: for = 1 to do 4: 5: 6: Count for = 1 to do Ii,j + 1 C end for + 1 Temporary offsets Indices transpose 7: 8: end for 9: cumsum(C) 10: CO 0 RN 11: for = 1 to do 12: for = 1 to do Ii,j (CO)I (CO)I + 1 CI + (CO)I (Iq)I 13: 14: 15: 16: end for 17: 18: end for 19: return Iq, we do not have the sparse query block indices for each key block. Existing Top-K sparse attention methods [40, 41] maintain sparse mask of shape for backward. However, this implementation increases the complexity of the algorithm from O(T ) to O(T 2). To avoid constructing the sparse mask and achieving optimal complexity, we implement an efficient sparse Top-K index transposition algorithm following the classic CSR-toCSC scan-based sparse matrix transpose algorithm [11] and its parallel variant [24]. The algorithm for one level is illustrated in Alg. 2. The hierarchical Top-K transposition can reuse the same program for each level. The overall idea of the algorithm is that we save all the query indices for each key of variable length into flat vector Iq RT K, and we use the cumulative key offsets RT +1 to obtain the start and end index for each key in Iq. To obtain Iq and C, we first count the number of relevant queries for each key into C, and the prefix-sum of is the cumulative key offsets C. Given C, we traverse the topK indices again and write the reverse mapping into Iq with the help of cumulative offset vector CO. Note that the update of both and CO requires atomic additions in parallel programming. Because the number of selected keys is small, the atomic additions are sparsely distributed across memory locations. Consequently, the probability of interprogram write conflicts is extremely low, and the overhead induced by atomic contention becomes negligible. Table 1. Ablation study results of Log-linear Sparse Attention (a) Attention Type Configuration Full Attention Top-K Attention (L = 1) + KV Enrichment (Le = 1) + KV Reweighting Top-K Attention (L = 2) + KV Enrichment (Le = 2) + KV Reweighting FID Throughput 24. 28.21 26.09 24.18 27.98 25.31 24.37 188.88 483.91 302.92 302.92 500.38 436.40 436.40 (b) Block Size (c) Top-K FID Throughput Config FID Throughput 16 64 16 64 8 2 32 8 28.21 31.33 25.88 26. 483.91 584.54 357.95 547.69 LLSA Baseline 8 8 16 24.37 28.21 27.23 25.88 436.40 483.91 436.40 357.95 pretrained weights. This design further reduces the training time. All detailed settings, including data resolution schedules and optimization parameters, are provided in the Appendix. Metrics. For the quality metric, we evaluate the FID [12] using 10,000 samples generated by 20 diffusion steps. For model efficiency, we report the training throughput measured as 103 pixel tokens per second on single H200 GPU. 5.2. Ablation Study Training Settings. In the ablation study, we use pixel DiT-S [25] with flow matching object [23] on 128 128 FFHQ [16] dataset. We incorporate RoPE [28] and QKNorm [7] to align the standard DiT with other advanced DiT architectures [2, 4]. In each experiment, we train the model for 20 epochs with the same training configurations. If not specified, we set = 8 and = 16 by default for all TopK sparse attention methods. For token length of , the maximum level should be logB 1. Attention Designs. We verify our attention designs in Table 1a. We test the effectiveness of KV Enrichment and KV Reweighting with coarse level = 1 and = 2. Solely with Top-K attention can hardly match the performance of full attention. Enabling KV Enrichment improves FID, and correctly setting the importance for coarse tokens further enhances the model quality, making it even better than the full attention baseline. However, the single-level attention with KV Enrichment still has an O((N/B)2) complexity in both the selection and attention stages. Extending to multi-level version increases the throughput significantly with minor quality drop. Lower Block Size is Better. We conduct two groups of experiments with Top-K attention (L = 1) in Table 1b to explore the influence of block size. In each group, we test = 16 and = 64 settings with the same relevant token numbers. larger block size increases the throughput by Figure 2. Illustration of index reordering. The default raster indices do not effectively cluster similar pixels during 1D pooling, while using index ordering guarantees that similar pixels receive neighboring 1D indices. The key-major data structure Iq, allow us to perform the key, value gradient backward with sparsedense matrix multiplication (SpMM) accumulation, i.e., we gather the sparse query indices (Iq)[Ci:Ci+1) for the i-th key and perform standard backward algorithm for key and value. 5. Experiments 5.1. Pixel DiT Implementation To validate that LLSA effectively reduces computational complexity, we train Pixel DiTs on the task of generating long pixel sequences without patchification or VAE encoding. To adapt LLSA to 2D visual data, we introduce an index-reordering scheme, and to accelerate the convergence of DiT on long token sequences, we apply modified noise scheduler with noise rescaling and enable lowresolution pretraining. Index Reordering. To extend LLSA beyond 1D sequence modalities, the only modification required is to reorder tokens so that spatially adjacent pixels become neighbors in the flattened 1D sequence. Following prior sparse-attention approaches [39, 42], we apply index reordering once at the beginning of the DiT. Specifically, pixels within each patch of size 2i are grouped as consecutive tokens in the flattened sequence, as illustrated in Fig. 2. This preserves local continuity in 2D layouts while maintaining compatibility with the 1D hierarchical Top-K routing in LLSA. Noise Rescaling. Diffusion models require stronger noise perturbations for higher-resolution images to maintain consistent signal-to-noise ratio (SNR) [3, 9, 13]. Inspired by the input-scaling strategy of [3], we accelerate pixel-space DiT training by instead scaling the noise. We introduce rescale factor into the flow matching scheduler [20, 21], xt = (1 t)x0 + tϵ, (7) where xt Rnn. We set = n/64 for images larger than 64 64, aligning their effective SNR with that of 64 64 images and thus stabilizing training at higher resolutions. Pretraining. Consistent with large text-to-image DiTs [9], we initialize high-resolution models from low-resolution 6 considerable amount because it reduces the overhead of the selection stage, but its quality is much worse than that of the small block size variant. Since the complexity of selection is reduced by our hierarchical design, unlike prior works that use larger block size [33, 3941], we use = 16 as the default setting. Small is Enough. We compare the final LLSA (L = Le = 2) to the baseline sparse attention (L = 1) of varying K. The baseline requires higher to match the quality of the full attention. In contrast, LLSA uses only = 8 and outperforms the = 32 baseline in both quality and efficiency. This indicates that the Hierarchical KV Enchirment ensures the LLSA to preserve global context with relatively small K. 5.3. Image Generation Benchmark Comparison with Baselines. We compare LLSA with two trainable Top-K sparse attention methods, VSA [41] and SLA [40], on FFHQ 128 128 and 256 256 image generation using DiT-S backbone. Both methods rely on single-level block selection scheme but process the coarse tokens differently: VSA performs full attention on the compressed (coarse) Q, K, tokens and adds the result to the sparse attention output, while SLA introduces an additional linear-attention branch to handle unselected tokens. To ensure fair comparison, we adjust the Top-K values so that all methods attend to approximately the same number of key blocks during sparse attention. Because LLSA incorporates multi-level coarse-to-fine KV enrichment, query with = 8 effectively attends to more blocks (e.g., for 128128 images, 8+8+1282/163 = 20 blocks). Thus, to provide VSA and SLA with equal capacity in the sparse attention stage, we set = 20 for the 128 128 image and = 32 for the 256256 image, for both methods. This increases the strength of the baselines, making our evaluation conservative: LLSA must match or exceed the performance of baselines that operate with larger K. Table 2 summarizes the results. Across both resolutions, LLSA achieves the best FID and the highest training throughput. This validates the effectiveness of hierarchical KV enrichment, which preserves the single-branch structure of standard attention. In contrast, the baselines rely on multiple attention branches (coarse attention or linear attention), which may distort the original attention formulation. ImageNet-256 Benchmark. We further evaluate LLSA on the ImageNet 256 256 dataset using PixelFlow [2], multi-stage pixel diffusion model that progressively upsamples noisy image from low-resolution token sequence to higher resolutions. On ImageNet-256, it employs 4 4 patchification and upsamples an 8 8 token image into 64 64 images via four-stage diffusion process. We replace the full attention with sparse attention methods only at the highest-resolution stage, as lower-resolution stages Table 2. Comparison of LLSA with other trainable Top-K sparse attention. We show the FID and training throughput for FFHQ128 (20 epochs) and FFHQ-256 (10 epochs). Training throughput is measured as 103 pixel tokens per second on single H200 GPU. 128 128 256 256 Method FID Throughput FID Throughput Full Attention 24.91 VSA SLA LLSA 26.91 25.73 24.37 188.88 421.02 365.48 436.40 38.77 40.69 39.98 39.29 61. 341.94 304.85 375.34 Table 3. PixelFlow ImageNet-256 benchmark on different sparse attention methods trained for 10 epochs. FID and Inception Score are computed on 10,000 samples with PixelFlows official script. Training throughput is measured as images per second on single H200 GPU. Attention FID Inception Score Throughput VSA SLA LLSA 23.59 22.58 20.41 64.07 65.31 73.21 32.30 29.81 34.16 ( 32 32) do not benefit from sparsification. All methods are trained for 10 epochs to ensure fair comparison. As shown in Table 3, LLSA again outperforms VSA and SLA in both FID and Inception Score, reinforcing its advantage on more challenging dataset and demonstrating its potential to accelerate large-scale diffusion models. Overall, our evaluation setup is conservative: we allocate baselines larger effective K, ensuring that LLSA improves performance not due to experimental bias but due to its more effective and computationally efficient attention formulation. More qualitative results are given in the supplementary material. 5.4. Efficiency Benchmark Attention Performance Figure 3 compares the inference and training speed of LLSA with VSA and SLA. LLSA is implemented in Triton [29], and VSA/SLA use their official Triton kernels. For fair comparison, we set = 8 for LLSA and increase for VSA and SLA such that all methods process the same number of KV blocks during sparse attention (Sec. 5.3). Across all settings, LLSA consistently outperforms VSA and SLA, often by substantial margin. The efficiency gains arise from three complementary factors: (1) LLSA uses more efficient sparse indices transpose algorithm rather than mask-based algorithm to compute the gradient of key and value. (2) Due to Hierarchical KV Enrichment, LLSA retrieves the same number of relevant tokens with smaller K, reducing the overhead of Top-K selection. (3) When the sequence becomes sufficiently long and = 1 is constrained by quadratic cost, switching to = 2 yields 7 Figure 3. Acceleration ratio of different attention methods compared to PyTorch Attention (FlashAttention2). We evaluate training and inference with block size {16, 64} across varying sequence lengths on an H200 GPU. mentation, we convert the sparse indices into dense binary mask and run standard block-sparse backward algorithm that skips the unmarked blocks. As shown in Fig. 4, our implementation maintains nearly constant throughput across all sequence lengths, confirming its linear complexity. In contrast, the baseline must convert sparse indices into dense block mask, an O(N 2) operation, and its block-sparse backward must scan this dense mask to skip unselected blocks. This results in steady degradation in throughput as the sequence length increases. Overall, the results demonstrate that LLSA achieves superior efficiency in attention computation, further validating its suitability for scaling transformers to extremely long sequences. 6. Conclusion We present Log-linear Sparse Attention, novel attention mechanism that reduces the computational complexity of self-attention from O(N 2) to O(N log ). LLSA extends the previous single-level search strategy to hierarchical search procedure and incorporates Hierarchical KV Enrichment to preserve global context and maintain model quality under long-sequence sparsification. The hierarchical design allows us to outperform previous sparse attention methods with smaller K. In addition, we develop high-performance GPU implementation based on block sparse attention and an efficient Top-K indices backward algorithm that eliminates the need for dense masks. Experiments on high-resolution pixel-space DiTs demonstrate that LLSA achieves better FID score and higher training throughput than prior sparse attention methods, indicating its potential for scaling diffusion transformers to substantially longer token sequences. Figure 4. The throughput of sparse key-value backward. Experiments are conducted on an H200 GPU using tokens with 64 heads and head dimension 64. We set = 8 and = 16 for sparse Top-K attention. clear improvement, reflecting the expected log-linear scaling. For block size of = 16, VSA and SLA exhibit significant kernel-level inefficiencies due to the quadratic runtime on both Top-K selection and mask-based backward algorithm. With = 64, their kernels become more competitive, yet LLSA still maintains clear advantage, especially at long sequence lengths where the selection stage becomes dominant. Efficient Backward Implementation We further compare our keyvalue backward kernel, which operates directly on sparse CSC indices obtained by the sparse-indices transpose operator, with mask-based baseline widely used in prior work [40]. Both methods use the same Top-K indices obtained from standard forward pass. Our implementation includes sparse indices transpose kernel and key-value backward kernel with CSC indices. For the baseline imple8 Table 4. Hyperparameters of Pixel DiT trained on FFHQ and ImageNet of various resolutions. Models with different attention implementations have identical configurations. FFHQ models are trained on one H200 GPU and ImagetNet models are trained on four H200 GPUs. Model FFHQ-32 FFHQ-128 FFHQFFHQ-512 ImageNet-128 ImageNet-256 Patch Size DiT Config Pretrained Model SNR Rescale Epochs Batch Size Learning Rate 1 1 DiT-S - 1 40 64 1 104 1 1 DiT-S FFHQ-32 2 20 16 1 10 1 1 DiT-S FFHQ-128 4 10 4 1 104 1 1 DiT-S FFHQ-256 8 2 1 1 104 4 4 PixelFlow-L - 1 40 32 1 104 4 4 PixelFlow-L ImageNet-128 1 10 8 1 104 A. Implementation Details A.1. Clarification of Kernel Implementation Both SLA [40] and VSA [41] use inefficient sparse backward implementations. SLA applies the standard maskbased sparse block sparse attention backward, skipping the unused queries for each key. VSA implements preprocessing kernel that extracts query indices for each key from the binary sparse mask, but it still requires O(N 2) complexity to construct the binary mask. In Table 2, to fairly evaluate the algorithms throughput rather than their GPU implementations, we reimplement their sparse backward kernels with our efficient sparse index transpose kernel. This conservative experimental setting significantly improves the baselines throughput, but results indicate that LLSA is still more efficient than SLA and VSA for an equal number of effective sparse tokens. In Fig. 3, we compare the kernel efficiency of LLSA with the official implementations of SLA and VSA. We modify their block size configurations to support = 16 inference. A.2. Training Configuration We provide the detailed training configurations on FFHQ [16] and ImageNet [8] in Table 4. B. Additional Experiment Results B.1. Additional Ablation Results Similar to the ablation studies presented in Sec. 5.2, all experiments in this section are conducted using DiT-S trained on 128 128 FFHQ unless otherwise specified. Table 5c and Table 5d report results after 10 epochs of training. More enrichment levels lead to better quality. We train three two-level LLSA DiT on FFHQ-128 with different KV enrichment layers Le, shown in Table 5a. More enrichment levels increase the effective token number and generation quality, while slightly reducing throughput. 512 512 pixel token sequence generation. To assess the scalability of LLSA on substantially longer token sequences, we train DiT-S on FFHQ-512 using different numbers of hierarchical levels (Table 5b). The single-level LLSA (L = 1) fails to converge within reasonable time budget due to its O(N 2) selection cost and coarse to9 Table 5. Ablation study results of Log-linear Sparse Attention (a) Enrichment Levels Configuration Le FID Throughput LLSA FFHQ-128 (L = 2) LLSA FFHQ-128 (L = 2) LLSA FFHQ-128 (L = 2) 0 1 2 27.98 25.49 24. 500.38 467.35 436.40 (b) Extension to 512 512 Resolution Configuration FID Throughput LLSA FFHQ-256 (L = 2) 39.29 LLSA FFHQ-512 (L = 1) LLSA FFHQ-512 (L = 2) LLSA FFHQ-512 (L = 3) - 39.26 40.77 375.34 44.90 292.66 323.29 (c) SNR Adjusting Methods (d) Index Reordering Configuration FID Configuration Baseline Timestep Shift Logit-normal Noise Rescale 32.66 30.32 30.22 29. Raster Order Index Reordering FID 31.19 29.46 1+( kens. Increasing to = 2 dramatically improves throughput. Further extending to = 3 yields additional speed gains. The scaling of per-token throughput, from 375.34 (at 256 256) to 323.29 (at 512 512), closely follows the O(N log ) complexity of LLSA. Noise rescaling is the most effective SNR-adjustment method. We evaluate three approaches for adjusting the SNR when training on higher-resolution images. (1) Timestep Shift: Following [9], we apply timestep shift to align the SNR of images with sequence length to those m/n tn of length n: tm = . Since our target SNR m/n1)tn corresponds to 64 64 images, we set (cid:112)m/n = 2 for the 128 128 experiments. (2) Logit-Normal Sampler: As proposed in [9], we replace uniform timestep sampling with logit-normal sampler using its default parameters (µ = 1, = 1.0). (3) Noise Rescaling: As described in Sec. 5.1, we rescale the noise by factor of = (cid:112)m/642 for images of resolution m, thereby matching their effective SNR to that of 64 64 images. Results in Table 5c show that noise rescaling yields the best generation quality among the tested methods. Index reordering improves model quality. To assess the impact of index reordering, we train an additional model using the default raster-scan ordering for 2D pixels. As shown in Table 5d, the model without index reordering can still converge but achieves worse FID, confirming the usefulness of the spatially coherent index reordering. Pretraining substantially reduces training cost. Figure 5 compares the FID curves of model trained from scratch and one initialized from lower-resolution pretrained checkpoint. The pretrained model converges rapidlywithin the first epochdemonstrating that Figure 5. The FID curves of different training strategies. Compared to training from scratch, starting from model pretrained on low-resolution data significantly reduces training cost. Figure 6. The FID and Inception Score curves of the first 4 epochs using VSA, SLA, and LLSA on PixelFlow ImageNet-256 benchmark. low-resolution pretraining significantly accelerates highresolution pixel-space DiT training. B.2. Training Curves We show the training FID [12] and Inception Score [27] curves on PixelFlow [2] ImageNet-256 benchmark in Fig. 6. The generation quality of LLSA is consistently better than that of baseline attention approaches throughout the training process. C. Qualitative Results We present qualitative results of LLSA in this section. In Fig. 7, we show the samples generated from LLSA DiTS trained on FFHQ-128, FFHQ-256, and FFHQ-512. For FFHQ-512, the model is only trained for 2 epochs. We believe that better quality can be obtained by longer training. In Fig. 8, we compare the ImageNet-256 samples generated by LLSA PixelFlow-L with the those produced by SLA and VSA variants. For reference, we also include samples generated by the official best-performing PixelFlow model."
        },
        {
            "title": "References",
            "content": "[1] Pengtao Chen, Xianfang Zeng, Maosen Zhao, Peng Ye, Mingzhu Shen, Wei Cheng, Gang Yu, and Tao Chen. Sparse-vdit: Unleashing the power of sparse attention to arXiv preprint accelerate video diffusion transformers. arXiv:2506.03065, 2025. 2 [2] Shoufa Chen, Chongjian Ge, Shilong Zhang, Peize Sun, and Ping Luo. Pixelflow: Pixel-space generative models with flow. arXiv preprint arXiv:2504.07963, 2025. 2, 3, 6, 7, 10 [3] Ting Chen. On the importance of noise scheduling for diffusion models. arXiv preprint arXiv:2301.10972, 2023. 6 [4] Katherine Crowson, Stefan Andreas Baumann, Alex Birch, Tanishq Mathew Abraham, Daniel Kaplan, and Enrico Shippole. Scalable high-resolution pixel-space image synIn Forty-first thesis with hourglass diffusion transformers. International Conference on Machine Learning, 2024. 3, 6 [5] Tri Dao. FlashAttention-2: Faster attention with better parallelism and work partitioning. In International Conference on Learning Representations (ICLR), 2024. 3, 5 [6] Tri Dao, Dan Fu, Stefano Ermon, Atri Rudra, and Christopher Re. Flashattention: Fast and memory-efficient exact attention with io-awareness. Advances in neural information processing systems, 35:1634416359, 2022. 2, 3, 4 [7] Mostafa Dehghani, Josip Djolonga, Basil Mustafa, Piotr Padlewski, Jonathan Heek, Justin Gilmer, Andreas Peter Steiner, Mathilde Caron, Robert Geirhos, Ibrahim Alabdulmohsin, et al. Scaling vision transformers to 22 billion parameters. In International conference on machine learning, pages 74807512. PMLR, 2023. 6 [8] Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li Fei-Fei. Imagenet: large-scale hierarchical image database. In 2009 IEEE conference on computer vision and pattern recognition, pages 248255. Ieee, 2009. 2, [9] Patrick Esser, Sumith Kulal, Andreas Blattmann, Rahim Entezari, Jonas Muller, Harry Saini, Yam Levi, Dominik Lorenz, Axel Sauer, Frederic Boesel, et al. Scaling rectified flow transformers for high-resolution image synthesis. In Forty-first International Conference on Machine Learning, 2024. 6, 9 [10] Han Guo, Songlin Yang, Tarushii Goel, Eric Xing, Tri Dao, and Yoon Kim. Log-linear attention. arXiv preprint arXiv:2506.04761, 2025. 3 [11] Fred Gustavson. Two fast algorithms for sparse matrices: Multiplication and permuted transposition. ACM Transactions on Mathematical Software (TOMS), 4(3):250269, 1978. 5 [12] Martin Heusel, Hubert Ramsauer, Thomas Unterthiner, Bernhard Nessler, and Sepp Hochreiter. Gans trained by two time-scale update rule converge to local nash equilibrium. Advances in neural information processing systems, 30, 2017. 6, 10 [13] Emiel Hoogeboom, Jonathan Heek, and Tim Salimans. simple diffusion: End-to-end diffusion for high resolution imIn International Conference on Machine Learning, ages. pages 1321313232. PMLR, 2023. 6 [14] Huiqiang Jiang, Yucheng Li, Chengruidong Zhang, Qianhui Wu, Xufang Luo, Surin Ahn, Zhenhua Han, Amir Abdi, 10 Dongsheng Li, Chin-Yew Lin, et al. Minference 1.0: Accelerating pre-filling for long-context llms via dynamic sparse attention. Advances in Neural Information Processing Systems, 37:5248152515, 2024. 2 [15] Yanming Kang, Giang Tran, and Hans De Sterck. Fast multipole attention: divide-and-conquer attention mechanism for long sequences. arXiv preprint arXiv:2310.11960, 2023. 2 [16] Tero Karras, Samuli Laine, and Timo Aila. style-based generator architecture for generative adversarial networks. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 44014410, 2019. 6, 9 [17] Nikita Kitaev, Łukasz Kaiser, and Anselm Levskaya. arXiv preprint transformer. Reformer: The efficient arXiv:2001.04451, 2020. 3 [18] Black Forest Labs. Flux. https://github.com/ black-forest-labs/flux, 2023. 1 [19] Xingyang Li, Muyang Li, Tianle Cai, Haocheng Xi, Shuo Yang, Yujun Lin, Lvmin Zhang, Songlin Yang, Jinbo Hu, Kelly Peng, et al. Radial attention: (nlog n) sparse attention with energy decay for long video generation. arXiv preprint arXiv:2506.19852, 2025. 3 [20] Yaron Lipman, Ricky TQ Chen, Heli Ben-Hamu, Maximilian Nickel, and Matt Le. Flow matching for generative modeling. arXiv preprint arXiv:2210.02747, 2022. 6 [21] Xingchao Liu, Chengyue Gong, and Qiang Liu. Flow straight and fast: Learning to generate and transfer data with rectified flow. arXiv preprint arXiv:2209.03003, 2022. 6 [22] Enzhe Lu, Zhejun Jiang, Jingyuan Liu, Yulun Du, Tao Jiang, Chao Hong, Shaowei Liu, Weiran He, Enming Yuan, Yuzhi Wang, et al. Moba: Mixture of block attention for longcontext llms. arXiv preprint arXiv:2502.13189, 2025. 2 [23] Nanye Ma, Mark Goldstein, Michael Albergo, Nicholas Boffi, Eric Vanden-Eijnden, and Saining Xie. Sit: Exploring flow and diffusion-based generative models with scalable interpolant transformers. arXiv preprint arXiv:2401.08740, 2024. [24] Duane Merrill and Michael Garland. Merge-based parallel sparse matrix-vector multiplication. In SC16: Proceedings of the International Conference for High Performance Computing, Networking, Storage and Analysis, pages 678689. IEEE, 2016. 5 [25] William Peebles and Saining Xie. Scalable diffusion models with transformers. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 41954205, 2023. 1, 3, 6 [26] Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Bjorn Ommer. High-resolution image In Proceedings of synthesis with latent diffusion models. the IEEE/CVF conference on computer vision and pattern recognition, pages 1068410695, 2022. 3 [27] Tim Salimans, Ian Goodfellow, Wojciech Zaremba, Vicki Cheung, Alec Radford, and Xi Chen. Improved techniques for training gans. Advances in neural information processing systems, 29, 2016. 10 rotary position embedding. Neurocomputing, 568:127063, 2024. 6 [29] Philippe Tillet, Hsiang-Tsung Kung, and David Cox. Triton: an intermediate language and compiler for tiled neural network computations. In Proceedings of the 3rd ACM SIGPLAN International Workshop on Machine Learning and Programming Languages, pages 1019, 2019. [30] Vaswani. Attention is all you need. Advances in Neural Information Processing Systems, 2017. 1 [31] Team Wan, Ang Wang, Baole Ai, Bin Wen, Chaojie Mao, Chen-Wei Xie, Di Chen, Feiwu Yu, Haiming Zhao, Jianxiao Yang, et al. Wan: Open and advanced large-scale video generative models. arXiv preprint arXiv:2503.20314, 2025. 1 [32] Shuai Wang, Ziteng Gao, Chenhui Zhu, Weilin Huang, and Limin Wang. Pixnerd: Pixel neural field diffusion. 2025. 3 [33] Jianzong Wu, Liang Hou, Haotian Yang, Xin Tao, Ye Tian, Pengfei Wan, Di Zhang, and Yunhai Tong. Vmoba: Mixtureof-block attention for video diffusion models. arXiv preprint arXiv:2506.23858, 2025. 2, 7 [34] Haocheng Xi, Shuo Yang, Yilong Zhao, Chenfeng Xu, Muyang Li, Xiuyu Li, Yujun Lin, Han Cai, Jintao Zhang, Dacheng Li, et al. Sparse videogen: Accelerating video diffusion transformers with spatial-temporal sparsity. arXiv preprint arXiv:2502.01776, 2025. 2 [35] Yifei Xia, Suhan Ling, Fangcheng Fu, Yujie Wang, Huixia Li, Xuefeng Xiao, and Bin Cui. Training-free and adaptive sparse attention for efficient long video generation. arXiv preprint arXiv:2502.21079, 2025. [36] Shuo Yang, Haocheng Xi, Yilong Zhao, Muyang Li, Jintao Zhang, Han Cai, Yujun Lin, Xiuyu Li, Chenfeng Xu, Kelly Peng, et al. Sparse videogen2: Accelerate video generation with sparse attention via semantic-aware permutation. arXiv preprint arXiv:2505.18875, 2025. 2 [37] Jingyang Yuan, Huazuo Gao, Damai Dai, Junyu Luo, Liang Zhao, Zhengyan Zhang, Zhenda Xie, Yuxing Wei, Lean Wang, Zhiping Xiao, et al. Native sparse attention: Hardware-aligned and natively trainable sparse attention. In Proceedings of the 63rd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 2307823097, 2025. 2 [38] Zhanpeng Zeng, Sourav Pal, Jeffery Kline, Glenn Fung, and Vikas Singh. Multi resolution analysis (mra) for approximate self-attention. In International conference on machine learning, pages 2595525972. PMLR, 2022. 2, 3 [39] Jintao Zhang, Chendong Xiang, Haofeng Huang, Haocheng Xi, Jun Zhu, Jianfei Chen, et al. Spargeattention: Accurate and training-free sparse attention accelerating any model inference. In Forty-second International Conference on Machine Learning. 2, 6, 7 [40] Jintao Zhang, Haoxu Wang, Kai Jiang, Shuo Yang, Kaiwen Zheng, Haocheng Xi, Ziteng Wang, Hongzhou Zhu, Min Zhao, Ion Stoica, Joseph E. Gonzalez, Jun Zhu, and Jianfei Chen. Sla: Beyond sparsity in diffusion transformers via fine-tunable sparse-linear attention. arXiv preprint arXiv:2509.24006, 2025. 2, 5, 7, 8, 9 [28] Jianlin Su, Murtadha Ahmed, Yu Lu, Shengfeng Pan, Wen Bo, and Yunfeng Liu. Roformer: Enhanced transformer with [41] Peiyuan Zhang, Haofeng Huang, Yongqi Chen, Will Lin, Zhengzhong Liu, Ion Stoica, Eric Xing, and Hao Zhang. 11 Faster video diffusion with trainable sparse attention. arXiv e-prints, pages arXiv2505, 2025. 2, 5, 7, 9 [42] Yuechen Zhang, Jinbo Xing, Bin Xia, Shaoteng Liu, Bohao Peng, Xin Tao, Pengfei Wan, Eric Lo, and Jiaya Jia. Training-free efficient video generation via dynamic token carving. arXiv preprint arXiv:2505.16864, 2025. 6 [43] Zhenhai Zhu and Radu Soricut. H-transformer-1d: Fast one-dimensional hierarchical attention for sequences. arXiv preprint arXiv:2107.11906, 2021. 2 Figure 7. The qualitative results of pixel space DiT-S using LLSA trained on FFHQ-128, FFHQ-256, and FFHQ-512. For FFHQ512, the model is only trained for two epochs. We believe that better quality can be obtained by longer training. Figure 8. The qualitative comparison of SLA, VSA, and LLSA trained on PixelFlow-L ImageNet-256. The reference images are generated by well-trained full-attention PixelFlow model from the official repository."
        }
    ],
    "affiliations": [
        "S-Lab, Nanyang Technological University",
        "Wangxuan Institute of Computer Technology, Peking University"
    ]
}