{
    "paper_title": "HI-TransPA: Hearing Impairments Translation Personal Assistant",
    "authors": [
        "Zhiming Ma",
        "Shiyu Gan",
        "Junhao Zhao",
        "Xianming Li",
        "Qingyun Pan",
        "Peidong Wang",
        "Mingjun Pan",
        "Yuhao Mo",
        "Jiajie Cheng",
        "Chengxin Chen",
        "Zhonglun Cao",
        "Chonghan Liu",
        "Shi Cheng"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Hearing-impaired individuals often face significant barriers in daily communication due to the inherent challenges of producing clear speech. To address this, we introduce the Omni-Model paradigm into assistive technology and present HI-TransPA, an instruction-driven audio-visual personal assistant. The model fuses indistinct speech with lip dynamics, enabling both translation and dialogue within a single multimodal framework. To address the distinctive pronunciation patterns of hearing-impaired speech and the limited adaptability of existing models, we develop a multimodal preprocessing and curation pipeline that detects facial landmarks, stabilizes the lip region, and quantitatively evaluates sample quality. These quality scores guide a curriculum learning strategy that first trains on clean, high-confidence samples and progressively incorporates harder cases to strengthen model robustness. Architecturally, we employs a novel unified 3D-Resampler to efficiently encode the lip dynamics, which is critical for accurate interpretation. Experiments on purpose-built HI-Dialogue dataset show that HI-TransPA achieves state-of-the-art performance in both literal accuracy and semantic fidelity. Our work establishes a foundation for applying Omni-Models to assistive communication technology, providing an end-to-end modeling framework and essential processing tools for future research."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 4 1 ] . [ 2 5 1 9 9 0 . 1 1 5 2 : r HI-TransPA: Hearing Impairments Translation Personal Assistant Zhiming Ma* SmartFlowAI Research Shanghai, China Shiyu Gan* Tongji University Shanghai, China Junhao Zhao* SmartFlowAI Research Shanghai, China Xianming Li CMIC Guangzhou, China Qingyun Pan BUPT Beijing, China Peidong Wang Northeastern University Shenyang, China Mingjun Pan Peking University Beijing, China Yuhao Mo CMIC Guangzhou, China Jiajie Cheng CMIC Guangzhou, China Chengxin Chen CMIC Guangzhou, China Zhonglun Cao CMIC Guangzhou, China Chonghan Liu Qiyuan Tech Beijing, China Shi Cheng SmartFlowAI Research Shanghai, China"
        },
        {
            "title": "Abstract",
            "content": "Hearing-impaired individuals often face significant barriers in daily communication due to the inherent challenges of producing clear speech. To address this, we introduce the Omni-Model paradigm into assistive technology and present HI-TransPA, an instruction-driven audio-visual personal assistant. The model fuses indistinct speech with lip dynamics, enabling both translation and dialogue within single multimodal framework. To address the distinctive pronunciation patterns of hearing-impaired speech and the limited adaptability of existing models, we develop multimodal preprocessing and curation pipeline that detects facial landmarks, stabilizes the lip region, and quantitatively evaluates sample quality. These quality scores guide curriculum learning strategy that first trains on clean, highconfidence samples and progressively incorporates harder cases to strengthen model robustness. Architecturally, we employs novel unified 3D-Resampler to efficiently encode the lip dynamics, which is critical for accurate interpretation. Experiments on purpose-built HI-Dialogue dataset show that HI-TransPA achieves state-of-the-art performance in both literal accuracy and semantic fidelity. Our *Equal contribution. Corresponding author. Email: mazhiming312@outlook.com & chengshi@shanghaitech.edu.cn work establishes foundation for applying Omni-Models to assistive communication technology, providing an end-toend modeling framework and essential processing tools for future research. 1. Introduction In recent years, the rapid evolution of Large Language Models (LLMs) has driven artificial intelligence into new era. Building upon this foundation, multimodal models have unified visual, auditory, and textual modalities, demonstrating remarkable capabilities in perception, reasoning, and generation [8, 19, 20, 22]. These advances have led to significant breakthroughs in areas such as machine translation, humancomputer interaction, and content creation. By integrating audio-visual information, machines can now not only see and hear but also comprehend complex crossmodal contexts, thereby establishing robust foundation for tackling real-world communication challenges. Despite the remarkable progress of artificial intelligence, accessibility and real-world applicability remain limited for certain groups. Individuals with disabilities constitute substantial share of the global population and still encounter barriers that prevent them from fully benefiting from AI technologies. Encouragingly, early research efforts have 1 present HI-TransPA, an instruction-driven audio-visual personal assistant. To the best of our knowledge, this is the first unified framework that integrates translation and chat functionalities within single model explicitly designed for assisting the hearing-impaired. To enable HI-TransPA to learn effectively from such inherently challenging data, we first design comprehensive data processing pipeline that purifies input signals and then evaluates and partitions the data based on quality. Building upon this partitioning, we then propose novel quality-aware curriculum learning strategy that leverages the structured data for robust training. Through these methods, HI-TransPA simultaneously processes both raw audio and high-frame-rate visual cues from lip dynamics, and acts as an intelligent conversational partner that understands context and infers user intent. Our main contributions are summarized as follows: 1. Unified audio-visual framework. We introduce HITransPA, an instruction-driven multimodal framework for assisting hearing-impaired individuals. By jointly modeling auditory signals and high-frame-rate lip dynamics, HI-TransPA performs accurate translation and natural dialogue within single end-to-end architecture. 2. Comprehensive multimodal data pipeline. We construct full pipeline covering preprocessing and qualityaware curation. It first extracts and aligns the lip region through two-stage method, then applies rejection sampling based on multimodal metrics to automatically split data into easy and hard subsets. 3. Quality-aware curriculum learning. We propose data-driven curriculum strategy that ranks multimodal samples by quality and schedules training from easy to hard, leading to stronger generalization across diverse hearing-impaired speech patterns. 2. Related Work 2.1. ASR and Multimodal Large Language Models The field of audio-centric AI has rapidly evolved from narrow, task-specific systems to comprehensive multimodal understanding. Early progress in Automatic Speech Recognition (ASR) was driven by large-scale weakly supervised training (e.g., Whisper [15]), data-efficient modeling (e.g., Canary [14]), and architectures optimized for fast inference (e.g., Paraformer [9]). major paradigm shift subsequently occurred with the integration of Large Language Models (LLMs), as demonstrated by FireRedASR-AED [21], which enhances ASR performance through greater contextual comprehension and reasoning. This evolution has given rise to Large Audio-Language Models (LALMs), which treat audio as rich semantic modality beyond transcription. Representative models such as Qwen2-Audio [6], MiDashengLM [7], and Step-Audio2[17] exhibit open-domain auditory understandFigure 1. HI-TransPA functions as (a) translator and (b) an intelligent assistant for individuals with hearing impairments. begun to bridge this gap by demonstrating how AI can promote greater inclusivity. For instance, vision-based systems such as electronic guide dogs [2] and OpenAIglasses for Navigation [1] have been developed to assist visually impaired users in enhancing their mobility and independence. These pioneering works underscore the potential of socially oriented, human-centric AI in promoting accessibility. Among various disability groups, individuals with hearing loss face particularly intricate and far-reaching challenges. According to the latest estimates from the World Health Organization, more than 1.5 billion people worldwide live with some degree of hearing loss, and over 430 million require rehabilitative support owing to moderate or higher levels of impairment [16]. The impact of hearing loss extends well beyond auditory perception itself. It interferes with natural language acquisition and speech development, which in turn leads to persistent difficulties in verbal communication. These limitations restrict educational and occupational participation and reduce access to information and social interaction. Consequently, hearing loss can result in social isolation, psychological distress, and deepened social inequality. Most existing assistive technologies focus on converting speech from hearing individuals into text, enabling deaf or hard-of-hearing users to access spoken information. However, these systems offer little support when hearingimpaired users attempt to express themselves. Conventional speech recognition models [3, 9] are typically trained on standard speech data and therefore struggle to interpret atypical or partially articulated utterances. This limitation hinders the ability of hearing-impaired individuals to engage in spoken interaction and underscores the need for adaptive multimodal models that can understand and process diverse and non-standard speech characteristics. To this end, we introduce the Omni-Model paradigm for assistive technologies targeting hearing-impaired users and 2 Figure 2. HI-TransPA System Architecture. ing, supporting instruction following and spoken dialogue across diverse contexts. Building upon these advances, Omni-Models have emerged to unify text, audio, and visual modalities within single foundation model. Both proprietary and open-source systems, including GPT-4o [10], and Qwen2.5-Omni [19], demonstrate impressive multimodal reasoning capabilities. However, their applications in assistive technology for individuals with hearing loss remain limited. Progress toward inclusive communication tools is further constrained by the scarcity of multimodal datasets, particularly those that containing indistinct speech aligned with lip dynamics. 2.2. AI for Hearing-Impaired Assistance Research on AI-based hearing support has traditionally focused on narrow subtasks, with predominant emphasis on sign language processing, while the development of tools facilitating speech expression has received comparatively little attention. Sign Language Recognition and Translation. Early studies in sign language understanding focused on isolated sign recognition through multimodal fusion of skeleton keypoints, RGB frames, and depth data [11]. For continuous sign translation, Camgoz et al. [4] introduced an end-to-end joint recognition and translation framework. More recently, contrastive learning has been used to align textual and visual sign embeddings [12], and incorporating non-manual cues such as lip dynamics has been shown to further enhance translation accuracy [18]. Lip Reading and Lip-to-Speech Synthesis. Complementary lines of work in visual speech processing have focused on decoding speech information directly from lip motions. Cross-modal knowledge distillation enables lipreading models to exploit representations from pre-trained speech recognizers [29]. In addition, recent lip-to-speech synthesis methods reconstruct intelligible speech directly from silent lip videos by leveraging self-supervised discrete speech units as intermediate representations [5]. Despite these promising advances, most approaches still regard individuals with hearing loss as passive recipients of translation output, focusing primarily on perception rather than expression. Enabling active speech articulation is essential to reducing communication barriers, promoting social inclusion, and enhancing self-perception. The emergence of multimodal Omni-Models offers promising path forward: unified frameworks capable of fine-grained multimodal understanding and context-driven generation. Motivated by this potential, our proposed HI-TransPA leverages 3 the Omni-Model paradigm to realize an intelligent audiovisual assistant capable of both accurate translation and interactive dialogue, supporting truly bidirectional communication for the hearing-impaired. 3. Method To address the two main challenges of noise and heterogeneity in raw data, and the inadequacy of existing OmniModels for modeling high-frame-rate lip dynamics, we develop HI-TransPA, an Omni-Model with three key components: (1) data preprocessing and curation pipeline, (2) high-speed vision architecture specialized for lip dynamics, and (3) multi-stage training objective based on curriculum learning. The overall framework is illustrated in Fig. 2. 3.1. Data Preprocessing and Curation Lip Region Extraction. To mitigate head pose variations, irrelevant facial movements, and background noise, we design two-stage lip region extraction pipeline. In the first stage, for each video frame It, pre-trained landmark detector Flm extracts = 468 facial keypoints with coordinates (xi, yi), from which we retain only the points corresponding to the lip region: Pt = {pi = (xi, yi) Slips}, (1) where {p1, ..., pN } = Flm(It) and Slips denotes the set of lip-related indices. These landmarks form temporal sequence representing lip motion across frames. In the second stage, we align and stabilize the lip video Vlips. For frames with valid landmarks Pt, we compute bounding boxes Bt and define uniform crop size as: = γ max( , H)2, (2) where γ = 1.2 is chosen as expansion factor and 2 rounds up to the nearest even number. Cropping is centered at each frames landmark centroid Ct, linearly interpolated when landmarks are missing. The resulting stabilized video Vlips normalizes head motion and emphasizes lip dynamics relevant to speech articulation. Data Curation via Rejection Sampling. Even after region extraction, hearing-impaired speech data can exhibit articulation ambiguity or acoustic distortion. To ensure data reliability, we introduce rejection sampling framework that scores each audio-visual pair and partitions the dataset into accepted and rejected subsets, as shown in Fig. 4. Figure 3. Overview of the lip region extraction pipeline. The left column shows input video frames at different timestamps. In the middle, face landmark detector identifies 468 facial keypoints (purple). The right column isolates and tracks lip landmarks across frames to produce stabilized lip crops used by the vision encoder. measures the textual consistency between Whisper-largev3s transcription ˆy and the ground truth y: SASR(x) = 1 dLev(ˆy, y) max(y, ˆy) , (3) where dLev denotes the Levenshtein distance. Signal clarity is quantified by the signal-to-noise ratio (SNR), defined as: SNR(x) = 10 log10 (cid:18) (cid:80) (cid:80) s2 t(st ˆst)2 (cid:19) , (4) where st represents the clean signal and ˆst its noisy counterpart. higher SNR indicates clearer speech with less background interference. The final composite audio score combines these two aspects: Saudio(x) = 0.5 SASR(x) + 0.5 Norm(SN R(x)), (5) where Norm() denotes min-max normalization. Video Quality. Video quality is characterized by the motion magnitude (x), defined as the mean pixel difference between consecutive frames. We normalize this metric into the range [0, 1]: Svideo(x) = min (cid:18) (x) Mmax (cid:19) , 1.0 , (6) Audio Quality. Audio quality is assessed using two complementary metrics. The ASR confidence score SASR(x) where Mmax is the 90th-percentile motion magnitude across the dataset. 4 3.3. Multi-Stage Alignment and Fine-Tuning With the vision subsystem redesigned, we perform threestage alignment and adaptation process to ensure multimodal synergy between vision, audio, and language. Stage 1: General Visual Alignment. This stage introduces the language model to the new visual features through two phases: (1) Image alignment: train only the 3D-Resampler on the Chinese-LLaVA-Vision dataset [13] while freezing the vision encoder and LLM; (2) Video alignment: urther train the Resampler on 30% of the LLaVAVideo-178K dataset [26] to capture temporal dynamics. Stage 2: Audio-Visual Co-adaptation. Using the Chinese-LiPS dataset [28], we jointly fine-tune the 3DResampler and the audio encoder so that both modalities produce complementary embeddings optimized for AudioVisual Speech Recognition (AVSR). translation and dialogue abilities. Stage 3: Conversational Fine-Tuning. We perform end-to-end instruction fine-tuning to jointly optimize the models mixed-instruction dataset is constructed with two modes: /translate, using paired audio-visual samples and their reference texts; and /chat, which shares the same inputs but pairs them with language-modelgenerated conversational replies. During training, both modes are combined under the curriculum learning strategy described in Section 3.4, where sample difficulty is determined by translation quality. The unified fine-tuning stage enables HITransPA to consolidate comprehension and transcription skills while adapting to real conversational scenarios. 3.4. Training Objective To enhance robustness and training stability when learning from multimodal data, we apply two-stage curriculum learning approach. Stage 1: Foundational Learning on Accepted Data. We start by fine-tuning the model on high-quality samples Daccept for three epochs using the cross-entropy loss: LStage 1 = ExDaccept[LCE(f (x), y)], (10) which aim to establish stable multimodal alignment between lip motion, audio, and textual content. Stage 2: Robustness Enhancement on Rejected Data. We then continue training for five epochs on the harder subset Dreject: LStage 2 = ExDreject[LCE(f (x), y)], (11) Figure 4. Overview of the rejection sampling pipeline. Each audio-visual sample is scored based on quality metrics from both modalities and then divided into accepted (Daccept) and rejected (Dreject) subsets for curriculum learning. Rejection Sampling. The final composite sample score is defined as: Scomp(x) = 0.6 Saudio(x) + 0.4 Svideo(x), (7) and samples are partitioned as: (cid:40) Daccept, Dreject, if Scomp(x) 0.55, otherwise. (8) Rejected samples are later reused as hard examples in the curriculum learning stage to enhance model robustness. 3.2. Model Architecture The proposed HI-TransPA builds upon the Qwen2.5Omni-3B framework, with re-engineered vision subsystem specifically optimized for high-frame-rate lip reading. The main architectural innovations are the integration of the SigLIP Vision Transformer and the Unified 3D-Resampler module from MiniCPM-V 4.5 [23]. Vision Encoder. The SigLIP encoder [24] provides finegrained visual representations suitable for lip articulation modeling. It processes the lip video Vlips RT HW and encodes it into sequence of patch tokens Zpatch. Spatiotemporal Resampler. To reduce computation for long video sequences, we apply the Unified 3D-Resampler after the vision encoder. It uses cross-attention with Nq = 64 learnable queries to compress the token sequence: Zfused = 3D-Resampler(Zpatch), (9) yielding Zfused RNqDllm , which preserves essential spatiotemporal cues while reducing token length. 5 which is designed to implicitly up-weights challenging examples, encouraging the model to learn robust representations that generalize to noisy, real-world conditions. Overall, this easy-to-hard learning schedule stabilizes convergence and yields model capable of consistent performance under diverse acoustic and visual quality levels. 3.5. Metrics Definition Character Error Rate (CER). For evaluation, we report the CER to measure literal transcription accuracy. Given ground-truth sequence and model output ˆy, CER is defined as: CER = dLev(ˆy, y) , (12) where dLev is the Levenshtein distance counting substitutions, deletions, and insertions. lower CER indicates higher transcription fidelity, while higher CER reflects poorer recognition performance. Embedding Similarity (EmbSim). To measure the alignment consistency between predicted and reference utterances in embedding space, we compute Embedding Similarity using cosine similarity. Given audio-visual feature embeddings epred and eref, EmbSim is defined as: EmbSim = epred eref epred2 eref2 . (13) Higher EmbSim values indicate closer semantic alignment between the predicted representation and the reference embedding, reflecting stronger cross-modal coherence. 4. Experiments 4.1. Experimental Setup Dataset. We collected and curated dedicated dataset, HI-Dialogue, to support the training and evaluation of audio-visual dialogue models for hearing-impaired individuals. Six volunteers with varying levels of hearing loss recorded audio-visual materials covering daily conversations, instructional texts, and emergency situations. After manual screening to remove samples with occluded lip regions or mismatched transcripts, we obtained 9,673 highquality samples. The dataset is split into 7,736 training and 1,937 test samples (80/20). Following the rejection sampling strategy (Sec. 3.1), the training data is further divided into an accepted set (Daccept, 4,733 samples) and rejected set (Dreject, 3,003 samples) used for curriculum learning. To enhance conversational capability, text responses are distilled for each training sample to support instruction tuning. 4.2. Evaluation Metrics We evaluate the models primarily on their comprehension accuracy. To quantify both literal and semantic correctness, we define Comprehensive Score (CS) as: CS = (1 α) (1 CER) + α EmbSim, (14) and α = 0.5 is adopted in our experiments. The Character Error Rate (CER) follows the definition in Eq. 12 of Sec. 3.4; it measures literal transcription accuracy, where lower value indicates better performance and higher value denotes more transcription errors. The Embedding Similarity (EmbSim), defined in Eq. 13, is computed using the cosine similarity between the predicted and reference embeddings, where higher values indicate stronger semantic consistency. We adopt Qwen3Embedding-0.6B [27] for embedding extraction. This balanced formulation in Eq. 14 provides holistic measure that reflects both surface-level accuracy (through CER) and deeper semantic fidelity (through EmbSim). We omit conversational fluency and empathy metrics to maintain comparability among models with different dialogue generation capabilities, focusing instead on the upstream comprehension of noisy audio-visual signalsa shared foundation across all systems evaluated. 4.3. Baselines We benchmark HI-TransPA against set of representative speech and multimodal systems, covering three major categories as summarized in Table 1. All baseline models are fine-tuned on the HI-Dialogue dataset for comparability. Together, they form comprehensive spectrum ranging from traditional audio-only recognizers to state-of-the-art multimodal Omni-Models. ASR Models. We select four widely used automatic speech recognition (ASR) systems representing strong audio-only foundations with different design philosophies. Whisperlarge-v3 [15] is trained on 680k hours of weakly supervised web data and serves as robust multilingual reference under zero-shot conditions. SenseVoice-small [3] is part of the FunAudioLLM framework, supporting fast multilingual ASR and audio-event recognition with low latency. Paraformer-large [9] employs non-autoregressive parallel Transformer to accelerate inference while maintaining competitive accuracy. FireRedASR-AED [21] represents an industrial-grade, Mandarin-optimized encoderdecoder architecture integrating large language modeling for superior character error rate (CER) performance. These models quantify the upper bound of purely acoustic transcription capability and provide foundation for measuring the gains from incorporating visual inputs. Large Audio Language Models (LALMs). To evaluate semantic reasoning on spoken content, we benchmark against large audiolanguage models that couple LLM 6 Table 1. Comparison of various models on the HI-Dialogue test set. Models Params Audio Video EmbSim CER CS Whisper-large-V3 SenseVoice-small Paraformer-large FireRedASR-AED Qwen2-Audio MiDashengLM InternLM-XComposer2.5-OmniLive Step-Audio 2 mini"
        },
        {
            "title": "ASR Models",
            "content": "1.6B 0.3B 0.2B 1.1B"
        },
        {
            "title": "LALMs",
            "content": "7B 7B 7B 7B Omni-Models Qwen2.5-Omni (3B) Qwen2.5-Omni (7B) HI-TransPA HI-TransPA (Curriculum Learning) 3B 7B 3B 3B 0.79 0.71 0.70 0.77 0.74 0.67 0.67 0. 0.73 0.75 0.77 0.84 0.32 0.35 0.38 0.38 0.44 0.53 0.54 0.34 0.44 0.42 0.37 0.27 0.74 0.68 0.66 0.70 0.65 0.57 0.57 0. 0.65 0.67 0.70 0.79 backbones with acoustic encoders, yet remain unimodal. Qwen2-Audio [6] enables direct comprehension of complex auditory scenes through instruction-following and conversational understanding. MiDashengLM [7], InternLMXComposer2.5-OmniLive [25], and Step-Audio 2 mini represent different scales of audio-centric systems, combining pretrained LLMs with efficient auditory front-ends for contextual reasoning. While these models excel in semantic comprehension, they cannot exploit visual cuessuch as lip dynamicsto disambiguate indistinct or impaired speech, which is the focus of our proposed system. Omni-Models. Finally, we compare with open-source multimodal Omni-Models that jointly process audio, visual, and textual information. Qwen2.5-Omni (3B and 7B) [19] integrates synchronized audiovisual encoders and shared language backbone to handle streaming multimodal tasks, representing the state of general-purpose cross-modal LLMs. Our HI-TransPA and its variant trained with curriculum learning follow the Omni-Model paradigm but are specifically optimized for accessibility. By aligning high-frame-rate lip features and audio inputs through instruction tuning, HI-TransPA demonstrates improvements in both embedding similarity and character error rate, outperforming generic Omni baselines by clear margin. 4.4. Main Results and Analysis The detailed comparison results are presented in Table 1. Baseline analysis. Audio-only models perform significantly worse on hearing-impaired speech: Whisper-largeV3 and Step-Audio 2 mini achieve CS scores of 0.74 and 0.73 but maintain high CERs (Eq. 12). Adding general viFigure 5. Interpretation of the Comprehensive Score (CS). Models are plotted in the (1 CER) vs. EmbSim space. Higher positions and rightward movement correspond to lower error rates and stronger semantic consistency, jointly yielding higher CS values. sion encoders, as in Qwen2.5-Omni, yields limited gains (CS = 0.67), suggesting that generic multimodal fusion fails to capture the fine-grained lip dynamics. Effectiveness of HI-TransPA. Our 3B HI-TransPA already surpasses the larger 7B Qwen2.5-Omni, achieving CS of 0.70. The specialized vision architecture, optimized for Incorporating high-frame-rate lip motion, proves crucial. curriculum learning further boosts performance, yielding CS of 0.79, an EmbSim of 0.84 (Eq. 13), and reduced 7 CER of 27%the best among all baselines. 4.5. Interpretation of the Comprehensive Score The comprehensive score (Eq. 14) combines literal correctness (1 CER) and semantic coherence (EmbSim), enabling us to compare models beyond recognition accuracy. From Table 1, three core observations emerge: 1. ASR models: Whisper-large-V3 shows moderate CER (32%) and EmbSim = 0.79, resulting in CS = 0.74. This indicates that despite high transcription accuracy, audioonly models lack multimodal semantic grounding. 2. Generic Omni-models: Qwen2.5-Omni (3B and 7B) achieve modest improvement. They benefit from visual cues but fail to capture fine-grained temporal lip dynamics, keeping both CER and EmbSim suboptimal. 3. HI-TransPA: Our model enhances both dimensions simultaneously, reducing the CER from 42% to 27% and increasing the EmbSim from 0.75 to 0.84, which leads to CS of 0.79. This indicates that HI-TransPA not only produces more accurate transcripts but also better preserves contextual meaning through specialized lipmotion modeling and curriculum-based training. Fig. 5 visualizes different models on two-dimensional plane defined by (1 CER) and EmbSim. Points closer to the upper-right corner indicate better balance between literal accuracy and semantic coherence. The distribution shows that the proposed Comprehensive Score (CS) effectively reflects each models overall comprehension ability by jointly capturing text-level correctness and meaning preservation, allowing direct comparison across architectures with different modalities and training paradigms. 4.6. Ablation Studies Role of the Visual Modality. To assess the contribution of visual input, we compare the full model with an audio-only variant. As shown in Table 2, removing the visual modality severely degrades performance (CS drops from 0.70 to 0.64; CER rises from 37% to 46%, see Eq. 12), confirming that lip motion provides indispensable cues for comprehension. Effect of Curriculum Learning. Finally, we analyze the impact of the proposed two-stage training strategy (Sec. 3.4). Compared with the base model trained without curriculum learning, the final version improves CS (Eq. 14) from 0.70 to 0.79 and reduces CER (Eq. 12) from 37% to 27%, confirming that progressive training enhances model robustness against noisy and difficult samples. 4.7. Qualitative Analysis To provide more intuitive demonstration of our models capabilities and to analyze the critical role of comprehension accuracy in generating meaningful dialogue, we present These two representative cases in Figure 6. cases showcase two modes of operation for HI-TransPA: Figure 6. Qualitative examples illustrating the importance of comprehension. The Good case (from our HI-TransPA) shows how accurate translation enables relevant and helpful chat response. The Bad case (from baseline) demonstrates how failure in comprehension leads to completely irrelevant dialogue, underscoring our Comprehension First evaluation principle. Translate mode, which aims for literal transcription that reveals the models true understanding of the input, and Chat mode, which aims to generate natural and helpful conversational response based on that understanding. In the Good case, our HI-TransPA model successfully processes complex sentence containing multiple entities. The output in Translate mode is near-perfect transcription, demonstrating its robust multimodal comprehension. 8 Table 2. Ablation on the effectiveness of the visual modality."
        },
        {
            "title": "Model",
            "content": "Audio Video EmbSim CER(%) CS Our(A) Our(O) 0.74 0. 46 37 0.64 0.70 Based on this precise understanding, the Chat mode is able to go beyond simple repetition, infer the broader context of an international grain shipment, and provide an empathetic and insightful response, truly functioning as an intelligent assistant. In contrast, the Bad case vividly illustrates what occurs when comprehension fails (this result is from poorly performing baseline). The Translate mode output reveals complete misinterpretation of the users intent, producing sentence semantically unrelated to the original input about transferring money at bank. This fundamental failure in comprehension directly explains why the Chat mode response is nonsensical in context. Although the chat response is grammatically coherent, it is logically derived from an incorrect premise, rendering it entirely useless and potentially confusing for the user. The comparison strongly validates our Comprehension First principle (Section 4.2), showing that accurate comprehension, reflected in literal translation, forms the basis of all higher-level dialogue abilities. As confirmed in Table 1, HITransPAs superior understanding establishes solid foundation for meaningful and supportive interaction. 5. Conclusion We presented HI-TransPA, an instruction-driven audiovisual Omni-Model that designed to assists people with hearing loss in natural dialogue and precise speech translation. With jointly modeling speech and high-frame-rate lip cues with redesigned visual encoder and quality-aware curriculum strategy for training, HI-TransPA achieves outstanding performance on multimodal real-world scenarios. Extensive experimental results on the HI-Dialogue dataset demonstrate that HI-TransPA consistently outperforms existing ASR methods, LALMs, and general Omni-Models, highlighting the potential of instruction-tuned multimodal intelligence for inclusive communication. [1] AI-FanGe. Openaiglasses for navigation: An open framework for blind navigation based on esp32, 2025. Accessed: 2025-11-13. 2 [2] Hongjun An, Jiangong Xiao, Haofei Zhao, Zhe Sun, and Xuelong Li. Interndog: An offline embodied intelligent guide dog system based on internlm2, 2024. GitHub repository. 2 [3] Keyu An, Qian Chen, Chong Deng, Zhihao Du, Changfeng Gao, Zhifu Gao, Yue Gu, Ting He, Hangrui Hu, Kai Hu, et al. Funaudiollm: Voice understanding and generation foundation models for natural interaction between humans and llms. arXiv preprint arXiv:2407.04051, 2024. 2, 6 [4] Necati Cihan Camgoz, Oscar Koller, Simon Hadfield, and Richard Bowden. Sign language transformers: Joint end-toend sign language recognition and translation. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 1002310033, 2020. 3 [5] Jeongsoo Choi, Minsu Kim, and Yong Man Ro. Intelligible lip-to-speech synthesis with speech units. arXiv preprint arXiv:2305.19603, 2023. [6] Yunfei Chu, Jin Xu, Qian Yang, Haojie Wei, Xipin Wei, Zhifang Guo, Yichong Leng, Yuanjun Lv, Jinzheng He, Junyang Lin, et al. Qwen2-audio technical report. arXiv preprint arXiv:2407.10759, 2024. 2, 7 [7] Heinrich Dinkel, Gang Li, Jizhong Liu, Jian Luan, Yadong Niu, Xingwei Sun, Tianzi Wang, Qiyang Xiao, Junbo Zhang, and Jiahao Zhou. Midashenglm: Efficient audio understanding with general audio captions. arXiv preprint arXiv:2508.03983, 2025. 2, 7 [8] Qingkai Fang, Shoutao Guo, Yan Zhou, Zhengrui Ma, Llama-omni: SeamShaolei Zhang, and Yang Feng. less speech interaction with large language models. arXiv preprint arXiv:2409.06666, 2024. 1 [9] Zhifu Gao, Shiliang Zhang, Ian McLoughlin, and Zhijie Yan. Paraformer: Fast and accurate parallel transformer for nonautoregressive end-to-end speech recognition. arXiv preprint arXiv:2206.08317, 2022. 2, 6 [10] Aaron Hurst, Adam Lerer, Adam Goucher, Adam Perelman, Aditya Ramesh, Aidan Clark, AJ Ostrow, Akila Welihinda, Alan Hayes, Alec Radford, et al. Gpt-4o system card. arXiv preprint arXiv:2410.21276, 2024. 3 [11] Songyao Jiang, Bin Sun, Lichen Wang, Yue Bai, KunSign language recognition via arXiv preprint peng Li, and Yun Fu. skeleton-aware multi-model ensemble. arXiv:2110.06161, 2021. 3 [12] Zifan Jiang, Gerard Sant, Amit Moryossef, Mathias Muller, Rico Sennrich, and Sarah Ebling. Signclip: Connecting text and sign language by contrastive learning. arXiv preprint arXiv:2407.01264, 2024. 3 [13] LinkSoul-AI. Chinese-LLaVA: An open-source bilingual chinese-english vision-language assistant, 2023. Apache-2.0 License. 5 [14] Krishna Puvvada, Piotr Zelasko, He Huang, Oleksii Hrinchuk, Nithin Rao Koluguri, Kunal Dhawan, Somshubra Majumdar, Elena Rastorgueva, Zhehuai Chen, Vitaly foundation models. arXiv preprint arXiv:2506.05176, 2025. 6 [28] Jinghua Zhao, Yuhang Jia, Shiyao Wang, Jiaming Zhou, Hui Wang, and Yong Qin. Chinese-lips: chinese audio-visual speech recognition dataset with lip-reading and presentation slides, 2025. 5 [29] Ya Zhao, Rui Xu, Xinchao Wang, Peng Hou, Haihong Tang, and Mingli Song. Hearing lips: Improving lip reading by distilling speech recognizers. In Proceedings of the AAAI Conference on Artificial Intelligence, pages 69176924, 2020. 3 Lavrukhin, et al. Less is more: Accurate speech recognition & translation without web-scale data. arXiv preprint arXiv:2406.19674, 2024. 2 [15] Alec Radford, Jong Wook Kim, Tao Xu, Greg Brockman, Christine McLeavey, and Ilya Sutskever. Robust speech In Internarecognition via large-scale weak supervision. tional conference on machine learning, pages 2849228518. PMLR, 2023. 2, 6 [16] World Health Organization. World report on hearing: Executive summary, 2021. Licence: CC BY-NC-SA 3.0 IGO. [17] Boyong Wu, Chao Yan, Chen Hu, Cheng Yi, Chengli Feng, Fei Tian, Feiyu Shen, Gang Yu, Haoyang Zhang, Jingbei arXiv preprint Li, et al. Step-audio 2 technical report. arXiv:2507.16632, 2025. 2 [18] Wenfang Wu, Tingting Yuan, Yupeng Li, Daling Wang, and Xiaoming Fu. Signclip: Leveraging mouthing cues for sign language translation by multimodal contrastive fusion. arXiv preprint arXiv:2509.10266, 2025. 3 [19] Jin Xu, Zhifang Guo, Jinzheng He, Hangrui Hu, Ting He, Shuai Bai, Keqin Chen, Jialin Wang, Yang Fan, Kai Dang, arXiv preprint et al. Qwen2. 5-omni technical report. arXiv:2503.20215, 2025. 1, 3, 7 [20] Jin Xu, Zhifang Guo, Hangrui Hu, Yunfei Chu, Xiong Wang, Jinzheng He, Yuxuan Wang, Xian Shi, Ting He, Xinfa Zhu, et al. Qwen3-omni technical report. arXiv preprint arXiv:2509.17765, 2025. 1 [21] Kai-Tuo Xu, Feng-Long Xie, Xu Tang, and Yao Hu. Fireredasr: Open-source industrial-grade mandarin speech recognition models from encoder-decoder to llm integration. arXiv preprint arXiv:2501.14350, 2025. 2, 6 [22] Hanrong Ye, Chao-Han Huck Yang, Arushi Goel, Wei Huang, Ligeng Zhu, Yuanhang Su, Sean Lin, An-Chieh Cheng, Zhen Wan, Jinchuan Tian, et al. Omnivinci: Enhancing architecture and data for omni-modal understanding llm. arXiv preprint arXiv:2510.15870, 2025. [23] Tianyu Yu, Zefan Wang, Chongyi Wang, Fuwei Huang, Wenshuo Ma, Zhihui He, Tianchi Cai, Weize Chen, Yuxiang Huang, Yuanqian Zhao, et al. Minicpm-v 4.5: Cooking efficient mllms via architecture, data, and training recipe. arXiv preprint arXiv:2509.18154, 2025. 5 [24] Xiaohua Zhai, Basil Mustafa, Alexander Kolesnikov, and Lucas Beyer. Sigmoid loss for language image pre-training. In Proceedings of the IEEE/CVF international conference on computer vision, pages 1197511986, 2023. 5 [25] Pan Zhang, Xiaoyi Dong, Yuhang Cao, Yuhang Zang, Rui Qian, Xilin Wei, Lin Chen, Yifei Li, Junbo Niu, Shuangrui Ding, et al. Internlm-xcomposer2. 5-omnilive: comprehensive multimodal system for long-term streaming video and audio interactions. arXiv preprint arXiv:2412.09596, 2024. 7 [26] Yuanhan Zhang, Jinming Wu, Wei Li, Bo Li, Zejun Ma, Ziwei Liu, and Chunyuan Li. Video instruction tuning with synthetic data, 2024. 5 [27] Yanzhao Zhang, Mingxin Li, Dingkun Long, Xin Zhang, Huan Lin, Baosong Yang, Pengjun Xie, An Yang, Dayiheng Liu, Junyang Lin, Fei Huang, and Jingren Zhou. Qwen3 embedding: Advancing text embedding and reranking through"
        }
    ],
    "affiliations": [
        "BUPT",
        "CMIC",
        "Northeastern University",
        "Peking University",
        "Qiyuan Tech",
        "SmartFlowAI Research",
        "Tongji University"
    ]
}