{
    "paper_title": "CompCap: Improving Multimodal Large Language Models with Composite Captions",
    "authors": [
        "Xiaohui Chen",
        "Satya Narayan Shukla",
        "Mahmoud Azab",
        "Aashu Singh",
        "Qifan Wang",
        "David Yang",
        "ShengYun Peng",
        "Hanchao Yu",
        "Shen Yan",
        "Xuewen Zhang",
        "Baosheng He"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "How well can Multimodal Large Language Models (MLLMs) understand composite images? Composite images (CIs) are synthetic visuals created by merging multiple visual elements, such as charts, posters, or screenshots, rather than being captured directly by a camera. While CIs are prevalent in real-world applications, recent MLLM developments have primarily focused on interpreting natural images (NIs). Our research reveals that current MLLMs face significant challenges in accurately understanding CIs, often struggling to extract information or perform complex reasoning based on these images. We find that existing training data for CIs are mostly formatted for question-answer tasks (e.g., in datasets like ChartQA and ScienceQA), while high-quality image-caption datasets, critical for robust vision-language alignment, are only available for NIs. To bridge this gap, we introduce Composite Captions (CompCap), a flexible framework that leverages Large Language Models (LLMs) and automation tools to synthesize CIs with accurate and detailed captions. Using CompCap, we curate CompCap-118K, a dataset containing 118K image-caption pairs across six CI types. We validate the effectiveness of CompCap-118K by supervised fine-tuning MLLMs of three sizes: xGen-MM-inst.-4B and LLaVA-NeXT-Vicuna-7B/13B. Empirical results show that CompCap-118K significantly enhances MLLMs' understanding of CIs, yielding average gains of 1.7%, 2.0%, and 2.9% across eleven benchmarks, respectively."
        },
        {
            "title": "Start",
            "content": "4 2 0 2 6 ] . [ 1 3 4 2 5 0 . 2 1 4 2 : r CompCap: Improving Multimodal Large Language Models with Composite Captions Xiaohui Chen1,2,, Satya Narayan Shukla1, Mahmoud Azab1, Aashu Singh1, Qifan Wang1, David Yang1, ShengYun Peng1,3,, Hanchao Yu1, Shen Yan1, Xuewen Zhang1, Baosheng He1 1Meta, 2Tufts University, 3Georgia Tech Work done during internship at Meta How well can Multimodal Large Language Models (MLLMs) understand composite images? Composite images (CIs) are synthetic visuals created by merging multiple visual elements, such as charts, posters, or screenshots, rather than being captured directly by camera. While CIs are prevalent in real-world applications, recent MLLM developments have primarily focused on interpreting natural images (NIs). Our research reveals that current MLLMs face significant challenges in accurately understanding CIs, often struggling to extract information or perform complex reasoning based on these images. We find that existing training data for CIs are mostly formatted for question-answer tasks (e.g., in datasets like ChartQA and ScienceQA), while high-quality image-caption datasets, critical for robust vision-language alignment, are only available for NIs. To bridge this gap, we introduce Composite Captions (CompCap), flexible framework that leverages Large Language Models (LLMs) and automation tools to synthesize CIs with accurate and detailed captions. Using CompCap, we curate CompCap-118K, dataset containing 118K image-caption pairs across six CI types. We validate the effectiveness of CompCap-118K by supervised fine-tuning MLLMs of three sizes: xGen-MM-inst.-4B and LLaVA-NeXT-Vicuna-7B/13B. Empirical results show that CompCap-118K significantly enhances MLLMs understanding of CIs, yielding average gains of 1.7%, 2.0%, and 2.9% across eleven benchmarks, respectively. Date: December 9,"
        },
        {
            "title": "1 Introduction",
            "content": "Recently, significant advancements have been made in Multimodal Large Language Models (MLLMs) (Alayrac et al., 2022; Li et al., 2023; McKinzie et al., 2024; Liu et al., 2023a). These models combine images with large language models (LLMs) (OpenAI, 2023b; Team et al., 2023; Dubey et al., 2024) to harness the powerful capabilities of LLMs, demonstrating exceptional powers in visual and language understanding and achieving remarkable conversational ability. However, despite these advances, notable limitation remains: MLLMs often struggle with comprehensive understanding of composite images (CIs), extracting only partially accurate information. composite image (CI) is visual creation that combines various elements, such as photos, graphics, text, or other media, into single cohesive image. It includes diverse types such as collages, posters, and charts. This raises an important question: Why do these limitations persist? Our hypothesis is that the observed shortcomings in MLLMs may stem from lack of CI-caption pairs in the training data. In essence, the training procedure for MLLMs generally involves two stages: first, pre-training (PT) on image-caption datasets to align the vision encoder with the LLM, and second, supervised fine-tuning (SFT) on instruction or visual question answering (VQA) datasets to enhance the MLLMs instruction-following abilities (Li et al., 2023; Liu et al., 2023a; McKinzie et al., 2024). Research has shown that using high-quality image captions enhances the alignment between vision and language modalities, thereby improving MLLMs image understanding (Chen et al., 2023; McKinzie et al., 2024). However, current training data primarily includes high-quality captions for natural images (NIs), while such captions for CIs are often missing. In this work, we find that MLLMs captioning abilities are strongly correlated with their VQA performance, suggesting that instruction data is insufficient for MLLMs to fully comprehend CIs. We introduce CompCap, framework that automatically synthesizes high-quality CI-caption pairs, to bridge the data shortage in training MLLMs. CompCap functions as flexible framework that utilizes various metadata to construct CIs along with their corresponding captions. This metadata could include range of sources such as pre-existing image-caption pairs, layout information, text, or tabular data. For example, one implementation of CompCap could 1 Figure 1 (a) CompCap implements image-caption synthesis pipelines for six composite image types. The composition of the curated CompCap-118K dataset are 42.3% Collage, 31.4% Image-Text, 18.7% Chart, 3.4% Table, 2.5% Diagram, and 1.7% Code. (b) Introducing CompCap-118K into the training data significantly improves MLLMs performance on benchmarks comprising of composite images. be leveraging metadata from individual image-caption pairs and layout specifications to create collage image with generated caption. The images can be arranged according to predefined layouts, and LLMs can then generate captions based on the individual image captions and their positional arrangement. Given the diverse nature and unique characteristics of CIs, in this work, we develop six data generation pipelines from CompCap to comprehensively cover broad spectrum of CI types. Each pipeline employs distinct types of raw data and various automated tools to facilitate data generation. In total, we produce 118K CI-caption pairs, dubbed CompCap-118K, significantly enhancing the diversity and volume of training data available for MLLMs. We highlight the composition of CompCap-118K in Figure 1a. We verify the effectiveness of our proposed method by fine-tuning xGen-MM (Xue et al., 2024) and LLaVA-NeXT (Liu et al., 2024) on their datasets combined with CompCap-118K. Experimental result shows that the integration of CompCap-118K leads to substantial performance improvements across eleven MLLM benchmarks (see Figure 1b). To sum up, our contributions are as follows: We demonstrate that current MLLMs exhibit limited proficiency in understanding CIs compared to NIs, which could be attributed to the lack of diverse and high-quality CI captions in their training data. We introduce CompCap, universal framework that generates CIs with detailed and precise captions. We curate CompCap-118K, dataset containing 118K synthetically generated CI-caption pairs. Our empirical study shows that CompCap-118K enhances MLLMs comprehension of CIs, leading to improved performance across various benchmarks."
        },
        {
            "title": "2 MLLMs Need Good CI Caption Data",
            "content": "In this section, we discuss the necessity of introducing high-quality CI captions for training MLLMs. We starts with giving the definition of composite image in 2.1. 2.2 illustrates the limitations of MLLMs in accurately understanding CI, which often leads to generating incorrect information during captioning. This observation motivates us to curate CI-caption dataset. 2 Figure 2 MLLMs demonstrate poorer understanding on CIs compared to NIs. (a) Example of assessing caption accuracy of MLLMs on CI with the help of LLMs. (b) MLLMs generally understand NIs much better than CIs. (c) Errors generated during captioning are consistent with errors generated in VQA, highlighting the necessity of caption data for better vision-language alignment"
        },
        {
            "title": "2.1 Composite Images",
            "content": "A CI is combination of various visual elements including photos, text, and graphics. They are usually designed to deliver rich information for many purposes. In this work, we consider the following six CI categories: Collage Image-Text Chart œ Diagram Code"
        },
        {
            "title": "Composite Images",
            "content": "While many CIs fall under the first four categories, we specifically include code and table images because they contain structured visual elements. Code images require MLLMs to understand precise syntax, indentation, and programmingspecific structures, which differentiates them from other OCR text images. Table images, on the other hand, often contain diverse content types within cells such as images, text, or numerical data, and require MLLMs to accurately extract values, interpret headers, and understand relationships across rows and columns. Additionally, more complex CIs, like posters or infographics, can be created by combining elements from these different CI categories."
        },
        {
            "title": "2.2 MLLMs Generate Erroneous Caption for CI",
            "content": "MLLMs show inferior performance when it comes to captioning CIs compared to NIs. To illustrate this, we design demonstrative experiment where MLLMs are tasked with generating detailed captions for both NIs and CIs (See Appendix B.1 for experiment setup). Based on these generated captions, we used an LLM to perform VQA conditioned on the caption and evaluated the performance quantitatively. We also instruct MLLMs to carry out the same VQA by directly referencing the images. Figure 2a shows an example of how the two pipelines work. We benchmark the caption and VQA accuracy on CIs and NIs using LLaVA-1.5-Vicuna-13B (Liu et al., 2023a), InstructBLIP (Dai et al., 2023), and LLaVA-NeXT-Vicuna-13B (Liu et al., 2023a). We made the following observations: MLLMs exhibit inferior CI understanding: Results from Figure 2b shows that all three MLLMs report much lower accuracy in CIs compared to NIs, regardless of the task (captioning or VQA). This indicates that MLLMs generally struggle more with understanding CIs. MLLMs make similar errors in captioning and VQA: We also observed that the captioning and VQA accuracy 3 are similar for each image type and model. This raises the question of whether the mistakes made by MLLMs in captions are consistent with those in VQA tasks. To verify this, we counted how often the answers from the two pipelines agreed with each other, regardless right or wrong. As shown in Figure 2c, the information derived from caption and VQA task tends to be consistent. The data used to fine-tune MLLMs generally includes high-quality caption data for NIs and instruction data for both NIs and CIs. However, high-quality caption data for CIs is often missing. Our analysis indicates that MLLMs face challenges in accurately extracting information from CIs when trained only on instruction data, which emphasizes instruction-following over detailed content. The inclusion of high-quality caption data has been shown to improve visual-language alignment of MLLMs (Chen et al., 2023; McKinzie et al., 2024). Additionally, scaling instruction data requires extra human effort to customize diverse and accurate QA pairs for different CI types, making it less economical than generating caption data. This motivates us to propose curation pipeline that generates high-quality CI-captions."
        },
        {
            "title": "3 CompCap",
            "content": "This section elaborates on the proposed method. 3.1 explains the characteristics that define high-quality CI caption. 3.2 illustrates the CompCap framework. Lastly, 3.3 provides detailed implementation of CompCap to generate collage image-caption pairs."
        },
        {
            "title": "3.1 What Makes a Good CI Caption",
            "content": "Unlike NIs, CIs contain diverse array of visual elements that convey complex information. Effective CI captions must be meticulously crafted to enhance MLLMs ability to accurately interpret CIs. Specifically, we focus on two principles: accuracy and detailedness. Accuracy: An accurate caption faithfully represents the content of the image without introducing any false or misleading information. This is crucial for ensuring MLLMs generate reliable responses. Detailedness: detailed caption provides specific insights into all visual elements and their relationships, going beyond basic description. This is essential because CIs often have multiple layers of meaning, combining text, graphics, and data. For instance, comprehensive caption for an infographic should not only describe the overall topic but also explain each section, data point, and visual cue. Including such details helps MLLMs align textual descriptions with all aspects of the visual content, thereby improving the models ability to fully understand and interpret CIs."
        },
        {
            "title": "3.2 The CompCap Framework",
            "content": "CompCap is general framework that synthesizes CI-caption pairs for training MLLMs. Figure 3 shows an illustration of CompCap. It leverages metadata to generate both CI and its corresponding caption. Below, we explain what constitutes the metadata and how it is utilized in the creation of the composite image and caption generation. Metadata: Metadata comprises both raw data and configuration details necessary for generating CI. The nature of the raw data varies depending on the CI class and could include collection of image-caption pairs, tabular data, or textual data such as code or math expressions. Raw data serves as the fundamental content that the CI will represent. Configuration, or customization, on the other hand, is generated through random process based on the raw data. It determines how this raw data is visually represented in the CI. For example, when generating chart, the raw data might contain tabular information, while the configuration specifies details like the chart type (such as bar or line charts), the data columns to visualize, titles, and other visual parameters such as color and style. 4 Figure 3 The CompCap Framework. The synthesis pipeline for different CI types implements CompCap differently. Figure 4 The Collage implementation. We sample raw data from image-caption datasets and randomly generate layout for the selected images. The images are then arranged into collage following this layout, while an LLM generates caption for the collage given both the layout details and captions of the individual images. Creation of Composite Image: The image pipeline reads the configuration and composes the raw data accordingly to produces the CI. For each CI class, the pipeline is implemented using rendering tools (Knsv, 2024; Carbon, 2024) or Python libraries (Inc., 2015; Bradski, 2000; Clark, 2015; Hunter, 2007). For instance, in the case of chart visualization, the configuration might provide visualization parameters like color, legend, and font size. These parameters are then applied using Matplotlib (Hunter, 2007) or Plotly (Inc., 2015) to create the final chart image. Caption Generation: We use LLMs to produce caption for the CIs. The key challenge here is designing prompts that effectively guide LLMs to generate accurate and detailed caption. base prompt is crafted for each CI type to provide specific instructions on which aspects of the image the LLMs should focus on when generating the caption. For instance, captions for collage should recognize the content of subimages, identify their layout, and understand the possible interplay among them. Captions for chart should extract and analyze the data, with less emphasis on aesthetics. To enhance the quality and diversity of the generated captions, we also include in-context examples when constructing prompts for some of the CI types."
        },
        {
            "title": "3.3 An Instantiation of CompCap for j Collage",
            "content": "This section outlines the CompCap framework applied to create collage. We highlight the implemented pipeline in Figure 4. Detailed implementations for all CI pipelines are provided in the Appendix (Collage: A.1; Image-Text: A.2; Chart: A.3; Diagram: A.4; Code: A.5; Table: A.6). Raw data: The pipeline begins with retrieving set of image-caption pairs from the database. To simulate diverse, real-world scenarios, we employ three retrieval methods: 1. Random retrieval: Sampling image-caption pairs uniformly from the datasets to create unrelated image. 2. Similarity-based retrieval: Sampling image-caption pairs with similar visual and textual features. We calculate the similarity between any two image-caption pairs by summing the cosine similarity of their image embeddings and that of their caption embeddings. We extract the image embeddings using Dino-v2 (Radford et al., 2021) and caption embedding using CLIP (Radford et al., 2021). 3. Entity-based retrieval: Retrieving image-caption pairs that depict the same entity (e.g., public figure or landmark). Beginning with entity randomly sampled from predefined entity list, we filter pairs to include those whose captions contain the chosen keyword, and sample from the filtered group. 5 Category Collage Image-Text Chart œ Diagram Code Table Metadata Image Simulator(s) Image-Caption & Layout OpenCV (Bradski, 2000) / PIL (Clark, 2015) Image-Caption & Text & Layout OpenCV / PIL / Augraphy (Groleau et al., 2023) (Geo) Tabular data Mermaid diagram code Code snippet Tabular data Plotly (Inc., 2015) Mermaid (Knsv, 2024) & Selenium Carbon (Carbon, 2024) & Selenium Matplotlib (Hunter, 2007) Caption Composition LGC Text + LGC / Text LGC LGC Code snippet + LGC Markdown table + LGC #Samples Avg. Char. 50K 37K 22K 3K 2K 4K 913 221 1468 2044 1106 Table 1 Statistics of CompCap-118K dataset. LGC stands for LLM-Generated Content, which encodes the detail breakdown and analysis of the information carried by CI. Images retrieved by similarity-based or entity-based methods are related, generating collage that resembles real-world data. On the other hand, random retrieval composes unrelated images to form collage. Such data are counterfactual and can help in model debiasing (Yu et al., 2024), thus mitigating hallucination caused by LLMs parametric knowledge (Bai et al., 2024). Layout: We consider layouts where at least one dimension (row or column) is aligned. Since the layout depends on the dimensions (width and height) of the sampled images, we implement two types of layout, each organically combined with image sampling: 1. Grid layout: We first generate grid layout that specifies the width/height ratios for each image, then samples images that meet these constraints. 2. Auto layout: We first sample images, and find layout that seamlessly composes them into collage. We discuss more details and shows some examples of the two employed layouts in Appendix A.1.1. Composing collage: We use image processing tools (OpenCV (Bradski, 2000) and PIL (Clark, 2015)) to assemble the retrieved images into collage based on the sampled layout. To enhance the diversity of the generated collage, we introduce three types of randomization: (1) the margin between images within the collage, (2) the padding around the border of the collage, and (3) the collage background using predefined background patterns. Prompt and caption design: To generate consistent and accurate captions from LLMs, the prompt is structured to include the following components: 1. Coordinate system: coordinate system that clarifies the spatial arrangement of images. 2. Layout and caption data: Metadata that describes the generated collage in bullet point style, where each bullet point contains the image location represented using the coordinate system and the caption of the image. 3. In-context example: An input-output example that helps LLMs understand the expected format and style for captions. The designed caption describes the collage by listing caption for each image in bullet-point format. When images are related, an inference highlighting their interplay is added. We use active in-context example selection to improve the accuracy and diversity of captions (Zhang et al., 2022). We further post-process the generated collage-caption pairs to improve the data quality, which includes: (1) filter out collages that contain duplicate images, and (2) reformat and filter captions from LLMs responses to ensure clarity and relevance."
        },
        {
            "title": "4 Training MLLMs with CompCap Data",
            "content": "We train MLLMs with CI-caption dataset to validate the effectiveness of the CompCap framework. We first describe the curated dataset in 4.1, then the training details in 4.2."
        },
        {
            "title": "4.1 The CompCap-118K Dataset",
            "content": "The CompCap-118K dataset, generated via CompCap, is synthetic collection of 117,879 image-caption pairs spanning six CI categories. Each category uses different types of metadata and simulation tools to generate the images, and the captions are created using various LLMs, depending on the complexity of the captioning task. Table 1 provides summary of the datasets statistics. 6 e E t c M ) n ( c A L i a e O r Q D o R W . A Model 4B MLLMs xGen-MM-inst.-4B (Xue et al., 2024) CompCap-4B 71.3 71.6 67.7 67.9 75.5 76. 64.0 64.7 78.2 81.0 32.6 35.0 51.6 52.7 54.8 57.4 55.2 58. 27.6 27.9 50.6 55.8 57.2 58.9 7B MLLMs LLaVA-NeXT-Vicuna-7B (Liu et al., 2024) CompCap-7B 71.2 70. 65.2 65.6 67.6 68.9 66.3 67.5 72.4 75.5 39.6 41.7 55.1 58. 63.5 68.9 76.5 77.6 39.2 40.8 70.4 73.7 62.5 64.5 13B MLLMs LLaVA-NeXT-Vicuna-13B (Liu et al., 2024) 65.6 CompCap-13B 68.5 (a) Comparison with MLLMs with same architectures and same amount of training data. 68.9 70.8 68.8 71.4 71.9 72.2 67.6 67.8 43.8 47. 75.3 79.3 79.9 81.1 68.5 73.9 57.7 61.4 42.4 45.0 77.1 83. Model 3B - 4B MLLMs PT/SFT #Data Avg. Phi-3-vision (Abdin et al., 2024) xGen-MM-inst.-4B (Xue et al., 2024) xGen-MM-inst.-4B (Xue et al., 2024) CompCap-4B 5B/>8.3M >25M/UNK. >25M/1M >25M/1M 7B - 8B MLLMs ShareGPT4V-7B (Chen et al., 2023) Qwen-VL-chat-7B (Wang et al., 2024) Cambrian-8B Tong et al. (2024) LLaVA-NeXT-Vicuna-7B Liu et al. (2024) CompCap-7B 1.2M/665K UNK./UNK. 1.2M/7M 558K/779K 558K/779K 66.9 60.2 57.2 58.9 43.8 54.5 65.9 62.5 64.5 13B MLLMs ShareGPT4V-13B (Chen et al., 2023) 1.2M/665K 44.8 >6.5B/20M 75.0 OmChat-v2.0-13B (Zhao et al., 2024) Cambrian-13B (Tong et al., 2024) 67.2 LLaVA-NeXT-Vicuna-13B (Liu et al., 2024) 65.6 CompCap-13B 68.5 (b) Comparison with SoTA MLLMs. 1.2M/7M 558K/779K 558K/779K Table 2 Evaluation on MLLM benchmarks. (a) indicates benchmarks with NIs, indicates CIs, and combined symbols represent benchmarks containing both, with symbol size reflecting the dominant type. For each model size, the two rows share the same MLLM architecture and PT dataset and are only different in SFT data mixture (We retrain the SFT stage for xGen-MM-inst.-4B for fair comparison). Better performance are marked in bold in each model size. (b) We report the number of samples used for training (PT+SFT) to demonstrate SoTA MLLMs. The greater sign > indicates lower bound of the number of samples, which is obtained by dividing the number of used tokens by the context window size 4K. We bold the best performance and underline the second best in each model size."
        },
        {
            "title": "4.2 The CompCap-4B/7B/13B MLLMs",
            "content": "We develop the CompCap series MLLMs using two recently introduced MLLM architectures: LLaVA-NeXT (Liu et al., 2024) and xGen-MM (Xue et al., 2024). For LLaVA-NeXT, we use the 2024-01 release (7B and 13B Vicuna versions), while for xGen-MM, we use version 1.5 (4B model). The MLLMs are trained in two stages: PT stage and SFT stage. We incorporate CompCap-118K dataset into the SFT stage. To ensure fair comparison, we uniformly downsample the original SFT dataset and add CompCap-118K such that the total number of training samples remained equivalent. Since the SFT dataset for xGen-MM is not released, we curate SFT dataset comprising 782K image-text pairs and 221K pure text samples, closely following the data recipe reported in Xue et al. (2024). We refer to the resulting MLLMs as CompCap-4B/7B/13B. We validate the effectiveness of the proposed framework through the experiments outlined in the following section."
        },
        {
            "title": "5.1 Evaluation Benchmarks",
            "content": "We evaluate the MLLMs across multiple benchmarks, particularly with the focus on their ability to comprehend CIs. We adopt NI-focused benchmarks like SEEDBench (Li et al., 2024), TextVQA (Singh et al., 2019), MMBench (Liu et al., 2025), MME (Yin et al., 2023), and LLaVABench (Liu et al., 2023b) to test conversational, reasoning, perception, and text recognition abilities. We also use CI-focused benchmarks, including ChartQA (Masry et al., 2022), DocVQA (Mathew et al., 2021), InfoVQA (Mathew et al., 2022), WebSRC (Chen et al., 2021), MathVista (Lu et al., 2023), and OCRBench (Liu et al., 2023c). Specifically, ChartQA, DocVQA, and InfoVQA measure the ability to interpret visually rich chart, document, or diagram images, while WebSRC focuses on web-based reading comprehension. MathVista and OCRBench contain both NIs and CIs, testing OCR abilities across various formats. For evaluation, we use VLMEvalKit (Duan et al., 2024) and LMMs-EVAL (Zhang et al., 2024)."
        },
        {
            "title": "5.2 Main Results",
            "content": "We present the quantitative results in Table 2, comparing MLLMs of three different sizes against similarly scaled models (3B-4B, 7B-8B, and 13B). From Table 2a, we can see that CompCap-4B/7B/13B consistently outperform the other MLLMs (xGen-MM-inst.-4B and LLaVA-NeXT-Vicuna-7B/13B) that share the same architectures and similar size of training data by 1.7%, 2.0%, and 2.9%, respectively. The performance gains are particularly noticeable on 7 Figure 6 Example of MLLMs on CI captioning: Informative content is highlighted in red if incorrect, in orange if correct but incomplete, and in blue if correct and complete. Component Baseline + Collage + Code + Table + œ Diagram + Chart + Image-Text (CompCap-118K) () 70.9 71.5 71.3 71.7 71.5 72.2 73.1 () 61.3 62.4 62.8 63.0 63.1 63.9 64.6 Avg. 65.6 66.4(+0.8) 66.6(+1.0) 67.0(+1.4) 67.4(+1.8) 68.0(+2.4) 68.5(+2.9) Table 3 Ablation study of each CI category on LLaVANeXT-Vicuna-13B. We report the average scores over NIdominated benchmarks () (SEEDBench, TextVQA, MMBench, MME, LLaVABench), CI-dominated benchmarks () (MathVista, OCRBench, ChartQA, DocVQA, InfoVQA, WebSRC), and all benchmarks. Baseline stands for the original SFT data recipe in LLaVA-NeXT. Figure 5 #Captions/#Instructions Ablation on ChartQA. We replace k: {0,20,40,60,80,100} percents of the instructions in ChartQA training set with captions and train the LLaVANeXT-Vicuna-13B. We evaluate on ChartQA, DocVQA and InfoVQA benchmarks. benchmarks containing CIs. For benchmarks containing NIs, CompCap-4B/7B/13B also achieve competitive performance. Interestingly, even though the CompCap-118K dataset does not include math-specific data, our models still show substantial improvements on the MathVista benchmark. This is likely due to the significant presence of CIs in mathematical tasks, where accurate CI understanding is crucial for mathematical reasoning. Additionally, we report comparison with SoTA MLLMs in Table 2b. Although there remains performance gap between the CompCap series and SoTA MLLMs within each size category, its important to note that CompCap series are trained on significantly smaller datasets (10x-100x fewer samples). However, when compared to MLLMs trained on datasets of similar scale (less than 10x difference), the CompCap series demonstrate competitive performance across the board. We also show qualitative example in Figure 6. Compared to LLaVA-NeXT-Vicuna-13B, CompCap-13B generates more informative caption that comprehensively summarizes the statistics presented in the image. This showcases how better visual-language alignment is achieved by introducing caption data for CIs. These results indicate that high-quality captions significantly enhance MLLMs understanding of CIs. Our CompCap framework and its curated dataset address critical gap in the dataset blueprint for training MLLMs."
        },
        {
            "title": "5.3 Ablations",
            "content": "CI component ablation: We incrementally include the curated image-caption pairs from each CI type into the CompCap dataset to investigate the effectiveness of each type. We train LLaVA-NeXT-Vicuna-13B on the dataset variants and assess how the inclusions affect MLLMs performance on NI-dominated benchmarks and CI-dominated benchmarks. Results are summarized in Table 3. With the introduction of each CI-caption component, the MLLM 8 consistently achieves better performance, suggesting the effectiveness of each CI caption design. Caption-Instruction ablation on ChartQA: We design controlled experiment utilizing the ChartQA dataset to illustrate how high-quality caption data can enhance performance more effectively than instruction data. We first use the advanced MLLM to generate detailed caption for each chart image in ChartQA training set, resulting in 18,317 image-caption pairs. Then we randomly select k% of the image-instruction pairs from ChartQA training set, and replace the instruction text with the generated captions, where k{0, 20, 40, 60, 80, 100}. Note that we only modify the ChartQA training set, and all other components in LLaVA-NeXT SFT dataset remain unchanged. Such setting ensures MLLMs are trained on the same amount of data. Figure 5 reports the ChartQA/DocVQA/InfoVQA performance of MLLMs trained with varing Caption-Instruction ratio. First, results on the ChartQA test set show that caption data is more effective in boosting performance. Second, MLLMs instruction-following abilities on ChartQA dramatically decrease when only caption data are used, indicating the necessity of including instruction data for training. Third, caption data significantly improve MLLMs performance in other domains, such as DocVQA and InfoVQA, suggesting that knowledge gained from captions is more transferable. These findings emphasize the importance of incorporating high-quality caption data for CI, further supporting the motivation for our research."
        },
        {
            "title": "6 Related Works",
            "content": "MLLMs (OpenAI, 2023a; Liu et al., 2024; Dai et al., 2023; McKinzie et al., 2024) are designed to enhance LLMs (OpenAI, 2023b; Touvron et al., 2023; Dubey et al., 2024; Yang et al., 2024; Bai et al., 2023) with multimodal understanding, particularly for visual information. These models typically connect pre-trained vision encoder (Radford et al., 2021; Zhai et al., 2023) to powerful LLM, using vision-language connector like MLPs (Liu et al., 2023b) or Q-former (Dai et al., 2023) to align the visual with text modalities. Recent advancement on MLLMs majorly focus on leveraging and curating extensive, diverse, and high-quality training datasets (Chen et al., 2023; Xue et al., 2024; Tong et al., 2024; McKinzie et al., 2024) to improve the MLLMs abilities. Particularly, Chen et al. (2023); McKinzie et al. (2024) highlight the importance of high-quality caption data. While image-caption pairs being fundamental for aligning visual and textual representations, there is lack of such dataset for CIs, which is an important gap our work aims to address. Multimodal synthetic datasets (Johnson et al., 2017; Kafle et al., 2018; Methani et al., 2020; Kim et al., 2022; Li et al., 2022; Chang et al., 2022; Lindström and Abraham, 2022; Liu et al., 2023b) have emerged as scalable and cost-effective solution for training MLLMs. These datasets are either generated by producing synthetic captions or instructions for real images using AI tools or by creating synthetic images paired with template-based instructional text. For instance, LLaVA (Liu et al., 2023b) prompts GPT to generate instruction data for COCO images (Lin et al., 2014), while BLIP (Li et al., 2022) employs CapFilt to generate more refined caption data. For complete image-text pair synthesis pipelines, DVQA (Kafle et al., 2018) and PlotQA (Methani et al., 2020) focus on synthetically generated charts, aiming to develop question-answering pairs that test the ability to interpret, retrieve data from, and reason about the information presented in these charts. Similarly, MapQA (Chang et al., 2022) emphasizes choropleth maps of the United States, where color variations depict data values across geographic regions, offering range of map styles and question types to evaluate map interpretation and information extraction. Donut (Kim et al., 2022) presents SynthDoG, which generates synthetic document image from given text, targeting the ability of document understanding. In contrast to previous methods that generate synthetic images, our proposed CompCap framework covers wider spectrum of image types (Collage, Image-Text, Chart, Diagram, Table, and Code). Moreover, CompCap targets at curating detailed caption for the generated images, which benefits more on the vision-language alignment rather than instruction-following ability."
        },
        {
            "title": "7 Conclusion\nIn this work, we propose CompCap, a versatile framework designed to generate high-quality, detailed captions for\ncomposite images (CIs) such as charts, diagrams, and tables. The resulting dataset, CompCap-118K, comprises 118K\ncaptions across six CI categories, significantly enhancing MLLMs’ capabilities in CI understanding. Experimental results\ndemonstrate that incorporating CompCap-118K notably improves MLLMs’ performance across eleven benchmarks,\nparticularly in CI-specific tasks, emphasizing the critical role of caption data in achieving robust vision-language\nalignment. Additionally, by expanding CompCap with more CI pipeline implementation and raw data sources, we can\nfurther scale and enhance the generated dataset.",
            "content": ""
        },
        {
            "title": "References",
            "content": "Marah Abdin, Sam Ade Jacobs, Ammar Ahmad Awan, Jyoti Aneja, Ahmed Awadallah, Hany Awadalla, Nguyen Bach, Amit Bahree, Arash Bakhtiari, Harkirat Behl, et al. Phi-3 technical report: highly capable language model locally on your phone. arXiv preprint arXiv:2404.14219, 2024. Joshua Ainslie, James Lee-Thorp, Michiel de Jong, Yury Zemlyanskiy, Federico Lebrón, and Sumit Sanghai. Gqa: Training generalized multi-query transformer models from multi-head checkpoints. arXiv preprint arXiv:2305.13245, 2023. Jean-Baptiste Alayrac, Jeff Donahue, Pauline Luc, Antoine Miech, Iain Barr, Yana Hasson, Karel Lenc, Arthur Mensch, Katherine Millican, Malcolm Reynolds, et al. Flamingo: visual language model for few-shot learning. Advances in neural information processing systems, 35:2371623736, 2022. Jinze Bai, Shuai Bai, Yunfei Chu, Zeyu Cui, Kai Dang, Xiaodong Deng, Yang Fan, Wenbin Ge, Yu Han, Fei Huang, et al. Qwen technical report. arXiv preprint arXiv:2309.16609, 2023. Zechen Bai, Pichao Wang, Tianjun Xiao, Tong He, Zongbo Han, Zheng Zhang, and Mike Zheng Shou. Hallucination of multimodal large language models: survey. arXiv preprint arXiv:2404.18930, 2024. G. Bradski. The OpenCV Library. Dr. Dobbs Journal of Software Tools, 2000. Jennifer Bryan. gapminder: Data from Gapminder, 2023. https://github.com/jennybc/gapminder. Carbon. Carbon: Create and share beautiful images of your source code. https://carbon.now.sh/, 2024. Accessed: 2024-10-18. Shuaichen Chang, David Palzer, Jialin Li, Eric Fosler-Lussier, and Ningchuan Xiao. Mapqa: dataset for question answering on choropleth maps. arXiv preprint arXiv:2211.08545, 2022. Lin Chen, Jisong Li, Xiaoyi Dong, Pan Zhang, Conghui He, Jiaqi Wang, Feng Zhao, and Dahua Lin. Sharegpt4v: Improving large multi-modal models with better captions. arXiv preprint arXiv:2311.12793, 2023. Xingyu Chen, Zihan Zhao, Lu Chen, Danyang Zhang, Jiabao Ji, Ao Luo, Yuxuan Xiong, and Kai Yu. Websrc: dataset for web-based structural reading comprehension. arXiv preprint arXiv:2101.09465, 2021. Zhe Chen, Weiyun Wang, Hao Tian, Shenglong Ye, Zhangwei Gao, Erfei Cui, Wenwen Tong, Kongzhi Hu, Jiapeng Luo, Zheng Ma, et al. How far are we to gpt-4v? closing the gap to commercial multimodal models with open-source suites. arXiv preprint arXiv:2404.16821, 2024. Alex Clark. Pillow (pil fork) documentation, 2015. https://buildmedia.readthedocs.org/media/pdf/pillow/ latest/pillow.pdf. Karl Cobbe, Vineet Kosaraju, Mohammad Bavarian, Mark Chen, Heewoo Jun, Lukasz Kaiser, Matthias Plappert, Jerry Tworek, Jacob Hilton, Reiichiro Nakano, et al. Training verifiers to solve math word problems. arXiv preprint arXiv:2110.14168, 2021. Wenliang Dai, Junnan Li, Dongxu Li, Anthony Meng Huat Tiong, Junqi Zhao, Weisheng Wang, Boyang Albert Li, Pascale Fung, and Steven C. H. Hoi. Instructblip: Towards general-purpose vision-language models with instruction tuning. ArXiv, abs/2305.06500, 2023. https://api.semanticscholar.org/CorpusID:258615266. Haodong Duan, Junming Yang, Yuxuan Qiao, Xinyu Fang, Lin Chen, Yuan Liu, Xiaoyi Dong, Yuhang Zang, Pan Zhang, Jiaqi Wang, et al. Vlmevalkit: An open-source toolkit for evaluating large multi-modality models. arXiv preprint arXiv:2407.11691, 2024. Abhimanyu Dubey, Abhinav Jauhri, Abhinav Pandey, Abhishek Kadian, Ahmad Al-Dahle, Aiesha Letman, Akhil Mathur, Alan Schelten, Amy Yang, Angela Fan, et al. The llama 3 herd of models. arXiv preprint arXiv:2407.21783, 2024. Eurostat. Eurostat: Statistical office of the european union, 2024. https://ec.europa.eu/eurostat. Accessed: 2024-11-03. Yash Goyal, Tejas Khot, Douglas Summers-Stay, Dhruv Batra, and Devi Parikh. Making the in VQA matter: Elevating the role of image understanding in Visual Question Answering. In Conference on Computer Vision and Pattern Recognition (CVPR), 2017. Alex Graves. Generating sequences with recurrent neural networks. arXiv preprint arXiv:1308.0850, 2013. Alexander Groleau, Kok Wei Chee, Stefan Larson, Samay Maini, and Jonathan Boarman. Augraphy: data augmentation library for document images. In Proceedings of the 17th International Conference on Document Analysis and Recognition (ICDAR), 2023. https://arxiv.org/pdf/2208.14558.pdf. J. D. Hunter. Matplotlib: 2d graphics environment. Computing in Science & Engineering, 9(3):9095, 2007. doi: 10.1109/MCSE. 2007.55. 10 Plotly Technologies Inc. Collaborative data science, 2015. https://plot.ly. Justin Johnson, Bharath Hariharan, Laurens Van Der Maaten, Li Fei-Fei, Lawrence Zitnick, and Ross Girshick. Clevr: diagnostic dataset for compositional language and elementary visual reasoning. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 29012910, 2017. Kushal Kafle, Brian Price, Scott Cohen, and Christopher Kanan. Dvqa: Understanding data visualizations via question answering. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 56485656, 2018. Samira Ebrahimi Kahou, Vincent Michalski, Adam Atkinson, Ákos Kádár, Adam Trischler, and Yoshua Bengio. Figureqa: An annotated figure dataset for visual reasoning. arXiv preprint arXiv:1710.07300, 2017. Aniruddha Kembhavi, Mike Salvato, Eric Kolve, Minjoon Seo, Hannaneh Hajishirzi, and Ali Farhadi. diagram is worth dozen images. In Computer VisionECCV 2016: 14th European Conference, Amsterdam, The Netherlands, October 1114, 2016, Proceedings, Part IV 14, pages 235251. Springer, 2016. Geewook Kim, Teakgyu Hong, Moonbin Yim, JeongYeon Nam, Jinyoung Park, Jinyeong Yim, Wonseok Hwang, Sangdoo Yun, Dongyoon Han, and Seunghyun Park. Ocr-free document understanding transformer. In European Conference on Computer Vision, pages 498517. Springer, 2022. Knsv. Mermaid: Generation of diagrams and flowcharts from text. https://mermaid-js.github.io/mermaid/, 2024. Accessed: 2024-10-18. Ranjay Krishna, Yuke Zhu, Oliver Groth, Justin Johnson, Kenji Hata, Joshua Kravitz, Stephanie Chen, Yannis Kalantidis, Li-Jia Li, David Shamma, et al. Visual genome: Connecting language and vision using crowdsourced dense image annotations. International journal of computer vision, 123:3273, 2017. Bohao Li, Yuying Ge, Yixiao Ge, Guangzhi Wang, Rui Wang, Ruimao Zhang, and Ying Shan. Seed-bench: Benchmarking multimodal large language models. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 1329913308, 2024. Junnan Li, Dongxu Li, Caiming Xiong, and Steven Hoi. Blip: Bootstrapping language-image pre-training for unified vision-language understanding and generation. In International conference on machine learning, pages 1288812900. PMLR, 2022. Junnan Li, Dongxu Li, Silvio Savarese, and Steven C. H. Hoi. Blip-2: Bootstrapping language-image pre-training with In International Conference on Machine Learning, 2023. https: frozen image encoders and large language models. //api.semanticscholar.org/CorpusID:256390509. Ji Lin, Hongxu Yin, Wei Ping, Pavlo Molchanov, Mohammad Shoeybi, and Song Han. Vila: On pre-training for visual language models. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 2668926699, 2024. Tsung-Yi Lin, Michael Maire, Serge Belongie, James Hays, Pietro Perona, Deva Ramanan, Piotr Dollár, and Lawrence Zitnick. Microsoft coco: Common objects in context. In Computer VisionECCV 2014: 13th European Conference, Zurich, Switzerland, September 6-12, 2014, Proceedings, Part 13, pages 740755. Springer, 2014. Adam Dahlgren Lindström and Savitha Sam Abraham. Clevr-math: dataset for compositional language, visual and mathematical reasoning. arXiv preprint arXiv:2208.05358, 2022. Haotian Liu, Chunyuan Li, Yuheng Li, and Yong Jae Lee. Improved baselines with visual instruction tuning. ArXiv, abs/2310.03744, 2023a. https://api.semanticscholar.org/CorpusID:263672058. Haotian Liu, Chunyuan Li, Qingyang Wu, and Yong Jae Lee. Visual instruction tuning. ArXiv, abs/2304.08485, 2023b. https: //api.semanticscholar.org/CorpusID:258179774. Haotian Liu, Chunyuan Li, Yuheng Li, Bo Li, Yuanhan Zhang, Sheng Shen, and Yong Jae Lee. Llava-next: Improved reasoning, ocr, and world knowledge, January 2024. https://llava-vl.github.io/blog/2024-01-30-llava-next/. Yuan Liu, Haodong Duan, Yuanhan Zhang, Bo Li, Songyang Zhang, Wangbo Zhao, Yike Yuan, Jiaqi Wang, Conghui He, Ziwei Liu, et al. Mmbench: Is your multi-modal model an all-around player? In European Conference on Computer Vision, pages 216233. Springer, 2025. Yuliang Liu, Zhang Li, Biao Yang, Chunyuan Li, Xucheng Yin, Cheng-lin Liu, Lianwen Jin, and Xiang Bai. On the hidden mystery of ocr in large multimodal models. arXiv preprint arXiv:2305.07895, 2023c. Pan Lu, Swaroop Mishra, Tanglin Xia, Liang Qiu, Kai-Wei Chang, Song-Chun Zhu, Oyvind Tafjord, Peter Clark, and Ashwin Kalyan. Learn to explain: Multimodal reasoning via thought chains for science question answering. Advances in Neural Information Processing Systems, 35:25072521, 2022. 11 Pan Lu, Hritik Bansal, Tony Xia, Jiacheng Liu, Chunyuan Li, Hannaneh Hajishirzi, Hao Cheng, Kai-Wei Chang, Michel Galley, and Jianfeng Gao. Mathvista: Evaluating mathematical reasoning of foundation models in visual contexts. arXiv preprint arXiv:2310.02255, 2023. Ahmed Masry, Do Xuan Long, Jia Qing Tan, Shafiq Joty, and Enamul Hoque. Chartqa: benchmark for question answering about charts with visual and logical reasoning. arXiv preprint arXiv:2203.10244, 2022. Ahmed Masry, Parsa Kavehzadeh, Xuan Long Do, Enamul Hoque, and Shafiq Joty. Unichart: universal vision-language pretrained model for chart comprehension and reasoning. arXiv preprint arXiv:2305.14761, 2023. Minesh Mathew, Dimosthenis Karatzas, and CV Jawahar. Docvqa: dataset for vqa on document images. In Proceedings of the IEEE/CVF winter conference on applications of computer vision, pages 22002209, 2021. Minesh Mathew, Viraj Bagal, Rubèn Tito, Dimosthenis Karatzas, Ernest Valveny, and CV Jawahar. Infographicvqa. In Proceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision, pages 16971706, 2022. Brandon McKinzie, Zhe Gan, Jean-Philippe Fauconnier, Sam Dodge, Bowen Zhang, Philipp Dufter, Dhruti Shah, Xianzhi Du, Futang Peng, Floris Weers, et al. Mm1: Methods, analysis & insights from multimodal llm pre-training. arXiv preprint arXiv:2403.09611, 2024. Nitesh Methani, Pritha Ganguly, Mitesh Khapra, and Pratyush Kumar. Plotqa: Reasoning over scientific plots. In Proceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision, pages 15271536, 2020. Anand Mishra, Shashank Shekhar, Ajeet Kumar Singh, and Anirban Chakraborty. Ocr-vqa: Visual question answering by reading text in images. In 2019 international conference on document analysis and recognition (ICDAR), pages 947952. IEEE, 2019. Subhabrata Mukherjee, Arindam Mitra, Ganesh Jawahar, Sahaj Agarwal, Hamid Palangi, and Ahmed Awadallah. Orca: Progressive learning from complex explanation traces of gpt-4. arXiv preprint arXiv:2306.02707, 2023. GPT OpenAI. 4v (ision) system card. preprint, 2023a. OpenAI. Gpt-4 technical report. arxiv 2303.08774. View in Article, 2(5), 2023b. Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, et al. Learning transferable visual models from natural language supervision. In International conference on machine learning, pages 87488763. PMLR, 2021. Amanpreet Singh, Vivek Natarajan, Meet Shah, Yu Jiang, Xinlei Chen, Dhruv Batra, Devi Parikh, and Marcus Rohrbach. Towards vqa models that can read. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 83178326, 2019. Gemini Team, Rohan Anil, Sebastian Borgeaud, Yonghui Wu, Jean-Baptiste Alayrac, Jiahui Yu, Radu Soricut, Johan Schalkwyk, Andrew Dai, Anja Hauth, et al. Gemini: family of highly capable multimodal models. arXiv preprint arXiv:2312.11805, 2023. Shengbang Tong, Ellis Brown, Penghao Wu, Sanghyun Woo, Manoj Middepogu, Sai Charitha Akula, Jihan Yang, Shusheng Yang, Adithya Iyer, Xichen Pan, et al. Cambrian-1: fully open, vision-centric exploration of multimodal llms. arXiv preprint arXiv:2406.16860, 2024. Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timothée Lacroix, Baptiste Rozière, Naman Goyal, Eric Hambro, Faisal Azhar, et al. Llama: Open and efficient foundation language models. arXiv preprint arXiv:2302.13971, 2023. Peng Wang, Shuai Bai, Sinan Tan, Shijie Wang, Zhihao Fan, Jinze Bai, Keqin Chen, Xuejing Liu, Jialin Wang, Wenbin Ge, et al. Qwen2-vl: Enhancing vision-language models perception of the world at any resolution. arXiv preprint arXiv:2409.12191, 2024. Le Xue, Manli Shu, Anas Awadalla, Jun Wang, An Yan, Senthil Purushwalkam, Honglu Zhou, Viraj Prabhu, Yutong Dai, Michael Ryoo, et al. xgen-mm (blip-3): family of open large multimodal models. arXiv preprint arXiv:2408.08872, 2024. An Yan, Zhengyuan Yang, Junda Wu, Wanrong Zhu, Jianwei Yang, Linjie Li, Kevin Lin, Jianfeng Wang, Julian McAuley, Jianfeng Gao, et al. List items one by one: new data source and learning paradigm for multimodal llms. arXiv preprint arXiv:2404.16375, 2024. An Yang, Baosong Yang, Binyuan Hui, Bo Zheng, Bowen Yu, Chang Zhou, Chengpeng Li, Chengyuan Li, Dayiheng Liu, Fei Huang, et al. Qwen2 technical report. arXiv preprint arXiv:2407.10671, 2024. Shukang Yin, Chaoyou Fu, Sirui Zhao, Ke Li, Xing Sun, Tong Xu, and Enhong Chen. survey on multimodal large language models. arXiv preprint arXiv:2306.13549, 2023. Qifan Yu, Juncheng Li, Longhui Wei, Liang Pang, Wentao Ye, Bosheng Qin, Siliang Tang, Qi Tian, and Yueting Zhuang. Hallucidoctor: Mitigating hallucinatory toxicity in visual instruction data. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 1294412953, 2024. Weihao Yu, Zhengyuan Yang, Linjie Li, Jianfeng Wang, Kevin Lin, Zicheng Liu, Xinchao Wang, and Lijuan Wang. Mm-vet: Evaluating large multimodal models for integrated capabilities. arXiv preprint arXiv:2308.02490, 2023. Xiaohua Zhai, Basil Mustafa, Alexander Kolesnikov, and Lucas Beyer. Sigmoid loss for language image pre-training. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 1197511986, 2023. Kaichen Zhang, Bo Li, Peiyuan Zhang, Fanyi Pu, Joshua Adrian Cahyono, Kairui Hu, Shuai Liu, Yuanhan Zhang, Jingkang Yang, Chunyuan Li, et al. Lmms-eval: Reality check on the evaluation of large multimodal models. arXiv preprint arXiv:2407.12772, 2024. Yiming Zhang, Shi Feng, and Chenhao Tan. Active example selection for in-context learning. arXiv preprint arXiv:2211.04486, 2022. Tiancheng Zhao, Qianqian Zhang, Kyusong Lee, Peng Liu, Lu Zhang, Chunxin Fang, Jiajia Liao, Kelei Jiang, Yibo Ma, and Ruochen Xu. Omchat: recipe to train multimodal language models with strong long context and video understanding. arXiv preprint arXiv:2407.04923, 2024. Chunting Zhou, Pengfei Liu, Puxin Xu, Srinivasan Iyer, Jiao Sun, Yuning Mao, Xuezhe Ma, Avia Efrat, Ping Yu, Lili Yu, et al. Lima: Less is more for alignment. Advances in Neural Information Processing Systems, 36, 2024."
        },
        {
            "title": "A CI Implementations of CompCap",
            "content": "This section elaborates the implementation of each CI type. We show the detailed pipeline implementation of collage in A.1; image-text in A.2; chart in A.3; diagram in A.4; Code in A.5; and table in A.6. A.1 Collage Implementation We summarize the workflow in Figure 4. In this section, we first show how the image-captions used for composing collage are retrieved, then we elaborate on the design of collage caption and LLM prompting. A.1.1 Data Sources, Layout, and Retrieval Engines We retrieve image-caption pairs from existing image-caption datasets. And we maintain curated entity list of public figures, artworks, landmarks, and brands sourced from web data. Pre-processing: We first process the datasets for better retrieval quality and efficiency: 1. Construct entity-sample lookup table. For each image, we identify entities in the original caption that match entries in our maintained entity list and create an entity-image lookup table specifically for entity-based retrieval. 2. Pre-compute embeddings. For each sample in both datasets, we pre-compute Dino-v2 image embeddings and CLIP caption embeddings. Layout: We define two collage layouts: grid collage and auto collage. In the grid collage layout, images are arranged in an grid, where n, {1, 2, 3, 4}. To increase layout diversity, cells within the grid can merge to form larger cells. Since rows and columns in the grid layout are aligned, the layout will specify the width/height ratio for each image within cell, posing constraints for the retrieval process. To further enhance diversity, we introduce the auto collage layout, where only rows or columns are aligned. This enable composing images of arbitrary width/height ratio into collage image. We demonstrate some examples of the layout in Figure 7. Similarity-based retrieval: We start by uniformly sampling an image-caption pair as the anchor data xα = (Iα, Tα) and then retrieve the top 20 most similar imagecaption pairs from the database D. Let dino α represent the Dino-v2 image embedding and CLIP text embedding for the anchor data, while dino and clip are the embeddings of an data = (I, ) D. The similarity score between xα and is computed as follow: and clip α sim(xα, x) = cos(I dino α , dino) + cos(T clip α , clip). From the top 20 candidates, we randomly select samples to construct the collage. Where width/height ratios are specified for candidate images, filter is applied to the database prior to calculating similarity. Entity-based retrieval: To optimize retrieval, we narrow down the entity list to include only entities that appear more than twice in the dataset. We randomly sample keyword from this list and apply rule-based matching in the database to select related data. Since such data are sparse, we only use the auto layout to compile collages to avoid the width/height restrictions. In post-processing, we further de-duplicate collages to ensure variety. Figure 7 Layout examples of grid collage and auto collage. In both similarity-based and entity-based retrieval, there may be cases where retrieved images are only loosely related to the anchor image. For instance, when retrieving images based on an anchor image of cricket game, some results might instead depict baseball due to their visual similarities. However, as long as the corresponding caption accurately 14 describes the image, the final generated caption for the collage will maintain accuracy. This introduces counterfactual samples into the CompCap-118K dataset, contributing to model debiasing by providing varied contexts. Figure 8 Prompt design and response example for grid layout. 15 Figure 9 Prompt design and response example for auto layout. 16 Figure 10 The Image-Text implementation. A.1.2 Caption and Prompt Design We design the caption such that it provides detailed walk-through on the images in the collage. Particularly, it either goes over the images by rows or by columns. For each row or column, we specify the demonstration order to be from left to right or from top to bottom, which comes first depends on the generated layout. For instance, for grid collage that comes with multi-column cell, the caption is designed to go over images by row. And for auto collage whose columns are aligned, the caption is designed to go over images by column. In order for LLMs to generate desired caption, different layout uses prompt that are slightly different in terms of the coordinate system notation and the in-context examples. We demonstrate the designed prompts and the example outputs from LLMs in Figures 8 and 9. We use Llama-3.1-70B (Dubey et al., 2024) for all caption generation. Caption post-processing: We find that some of the responses from LLMs do not completely follow the given instructions. For example, the response may start with #Recaption:or Here is the generated caption: before the actual image caption, or contain follow-up question such as Let me know if you have further instruction after the caption. To address this, we perform manual check on batch of generated responses and summarize all possible patterns, and implement post-processing pipeline to replace and delete undesired text automatically. A.2 Image-Text Implementation The curation pipeline for the Image-Text CI class is illustrated in Figure 10. This class is designed to assess MLLMs capabilities in extracting text from images and understanding the relationship between text and visuals. We divide this into two subcategories: image-and-text and pure-text. The image-and-text category tests the models ability to infer the relationship between text and the image, expecting MLLMs to interpret how text interacts with visual elements. The pure-text category focuses solely on text extraction. We present text in various styles against different backgrounds to strengthen the MLLMs robustness in text recognition. A.2.1 Image-and-text Pipeline Similar to collage, we sample image-caption pair as the background. We only consider random retrieval as we only retrieve one image at time. We then prompt an LLM to generate relevant text content to the image based on the caption. Note that we specifically instruct the LLM not to rephrase the caption. The prompt used for text content generation is shown in Figure 11. We first ask Llama-3.1-70B to infer the topic related to the image, then generate sentence within the topic. To enhance visual diversity, we control two primary configurations: Box layout: We position the text within bounding box, arranging the box alongside or overlaying the image. The boxs size, color, and opacity are randomized to increase variety. Text style: We customize the texts appearance by adjusting its size, color, font, and spacing, ensuring it contrasts well with the background for clear visibility. 17 Figure 11 Prompt for generating related text from given image. Augmentation Applied Effect Ink Phase InkBleed BleedThrough Paper Phase Applies random noise along text edges them to mimic fuzzy, bleeding ink when blurred. Combines ink bleed with Gaussian blur to recreate an effect where ink seeps through the paper. PaperFactory Tessellation NoiseTexturize BrightnessTexturize Replaces the background with randomly chosen texture, resized or tiled to cover the entire image. Applies repeating geometric pattern that interlocks seamlessly, giving structured, mosaic-like texture. Adds random noise pattern in varying scales to create realistic paper texture. Introduces random brightness variations to mimic subtle textural differences in paper. Combined Phase DirtyDrum DirtyRoller SubtleNoise BadPhotoCopy ShadowCast ReflectedLight Simulates dirty drum effect by adding vertical and horizontal noise lines across the image. Recreates the streaking effect of old or dirty document rollers in scanners. Adds slight, uneven noise to replicate minor lighting inconsistencies seen in scans of solid colors. Adds grainy, noisy overlay to mimic the quality of worn-out photocopier. Casts shadows on the paper to simulate natural shadows from scanning or photocopying. Draws bright ellipses on the paper to recreate the effect of light reflecting on the surface. Table 4 Augraphy augmentations for pure text. Effects are applied on the image sequentially according to the row order. A.2.2 Pure Text Pipeline This pipeline synthesizes both digital and handwritten text images. The text corpus is sourced from Wikipedia, with digital text generated as in the image-and-text pipeline. Additional details for handwritten text and background generation are as follows: Handwritten text: We generate handwritten text images in SVG format following Graves (2013), offering 12 distinct writing styles. Similar to digital text, color, stroke width, line spacing, and alignment are customized to increase diversity. Background: We choose two types of background image: 1. Natural image. We sample images from COCO dataset and apply blurring effect to highlight the foreground text. 2. Synthetic paper image. We use Augraphy (Groleau et al., 2023) to render realistic document effect, which sequentially modifies the ink style and the background paper style to create an authentic appearance. We summarize the used pipelines in Table 4. 18 A.3 Chart Implementation For chart visualizations, we consider bar charts, line plots, pie charts, and choropleth maps. In this section, we first explain how the bar, line and pie charts and their captions are generated generated as they share similar data sources. Then we illustrate the map visualization and caption design. Finally, we provide comparison of our curate dataset against previous synthetic chart-text datasets. For all chart visualizations, we use Plotly (Inc., 2015). And for all caption generations, we use Llama-3.1-405B. A.3.1 Bar, Line, and Pie Charts Data source: The tabular data for visualization come from DVQA (Kafle et al., 2018) and UniChart (Masry et al., 2023). DVQA provides canonical tabular data suitable for bar and pie chart visualizations, while UniChart contains time-series tabular data for line charts. Bar chart: The bar chart generation pipeline supports three bar types: single, grouped, and stacked bar charts. Single bar charts visualize one row of data, whereas grouped and stacked bar charts incorporate multiple rows. To enhance variety, each bar type includes the following customizations: 1. Bar pattern. Adjustments include bar texture, color, border, width, spacing, and the presence or absence of text on the bars. 2. Orientation. Bars can be arranged horizontally or vertically. 3. Axes. Customizations cover the range and tick intervals on both x-axis and y-axis. 4. Annotations and layouts. Variations include font styles, colors, and layout adjustments for titles, axis labels, and legends. Line chart: For line charts, we use both single-row and multiple-row time-series data, where each line corresponds to one line in chart. Many customizations from bar charts apply here, including axes, annotations, and layouts. The primary distinction for line charts is in line pattern customization, such as line style, color, and marker pattern. Pie chart: Pie charts use single-row data for visualization. Customizations include pie appearance adjustments, such as color, size, and display text placement. Text can be displayed inside or outside the pie; when segments are too small for text, pointers are used to indicate the designated region. Other customizations align with those used in bar and line charts. Prompt and caption design: For generating captions, the input to LLMs includes both data details and visualization parameters. Specifically, it incorporates axes details (type, range, and label) and additional elements like background patterns, titles, and style specifications. We instruct LLMs to create captions that summarize the charts data, identify trends, and compare groups. Figures 12, 13, 14 illustrate the designed prompts and sample outputs. A.3.2 Choropleth Maps Choropleth maps are created for four regions: European countries, global countries, the United States, and Chinese provinces. Data source: For European and global countries, data is visualized at the country level, with each country assigned data value. Data is sourced from Eurostat (Eurostat, 2024) and Gapminder (Bryan, 2023), or generated randomly. For Chinese provinces, we use randomly generated data, while for the United States, randomly generated state data from MapQA (Chang et al., 2022) is used. Visualization: Depending on the data type, choropleth maps can represent values using either color bar for numerical data or discrete color legend for categorical data. Each region is colored based on its value in the legend or color bar. Various visualization customizations include: 1. Color pattern. Varying color schemes for regions, titles, and legends. 2. Projection. Different projection methods for map rendering. 3. Value dropout. Randomly omitting values for certain regions and marking these with distinct color. 4. Layouts. Randomized layout of titles, map entries, and legends. 5. Region annotation. Optional display of country/province names or acronyms within the map."
        },
        {
            "title": "MapQA",
            "content": "CompCap-Chart"
        },
        {
            "title": "Image type\nText type\nText generation",
            "content": "Scatter/Bar/Line/Pie Yes/No QA Template-based"
        },
        {
            "title": "Bar",
            "content": "Bar/Line/Pie/Map Open-ended QA Open-ended QA Open-ended QA Detailed Caption LLM-generated Template-based Scatter/Bar/Line Template-based Template-based"
        },
        {
            "title": "Map",
            "content": "Table 5 Comparison of existing synthetic chart datasets. Prompt and caption design: We focus captions on regions that are clearly visible on the map to ensure clarity. Along with listing data values for key regions, we analyze the overall data distribution, such as trends by cardinal direction or differences between coastal and inland areas. We also include additional details like title and legend interpretation. Figure 15 shows the prompt used to generate captions. When preparing the data table in the prompt, we include each regions data value, color name, and some geographic details: 1. Location. Compass direction (e.g., north, southeast). 2. Type. Whether the region is coastal or inland. 3. Area. The size of the region. A.3.3 Post-processing We filter and modify the generated responses from LLMs such that they mostly resembles caption of an image. We observe that the generated response describes the details without first identifying the type of the chart since it is provided in the context of the prompt. However, such information is not granted in real conversation. We rephrase the first sentence such that it always start with identifying the chart type presented in the image before actual captioning. For instance, The line chart titled xxxx visually represents... is rephrased into The image shows line chart titled xxxx, which visually represents.... We implement it by rule-based matching and replacement. Apart from first sentence rephrasing, we also reuse the processing strategies stated in A.1.2 to enhance caption quality. A.3.4 Comparison with Existing Synthetic Datasets Table 5 compares existing synthetic chart-QA datasets (FigureQA (Kahou et al., 2017), DVQA (Kafle et al., 2018), PlotQA (Methani et al., 2020), and MapQA (Chang et al., 2022)) with our chart-caption dataset. Unlike previous methods that generate templated question-answer pairs for charts, our pipeline emphasizes detailed captions. Previous methods revoke system to learn three abilities: structure understanding, data retrieval, and reasoning, often through carefully designed templates targeting single ability. By providing precise instructions to the LLM, we enable it to generate captions that naturally integrate all three abilities. This approach not only eliminates the need for rigid templates but also encourage diversity in the generated captions. 20 Figure 12 Prompt design and response example for bar chart. Figure 13 Prompt design and response example for line plot. 22 Figure 14 Prompt design and response example for pie chart. 23 Figure 15 Prompt design and response example for choropleth map. A.4 œ Diagram Implementation We employ Mermaid (Knsv, 2024), JavaScript-based diagramming tool, to convert markdown text into diagram images. This tool allows us to transform text into visual representations seamlessly. Additionally, we prompt LLMs to analyze the markdown text, generating captions that not only describe each element in the diagram but also clarify relationships and provide potential insights. Figure 16 demonstrates the implementation of the pipeline. A.4.1 Diagram Visualization This section elaborates on the data source of the Mermaid codes, how they are rendered into diagram images, and customizations of the diagram style. Data source: We acquire Mermaid code through two primary methods: 1. GitHub Crawling. We collect text files containing mermaid as keyword from licensed repositories on GitHub. 2. LLM-Generated Code. We prompt LLMs to generate Mermaid code for specific diagram types such as class diagrams, ER diagrams, and flowcharts. We filter the collected codes by running rendering test, yielding approximately 3K valid diagram codes: 2K from GitHub and 1K generated by LLMs. Rendering: Mermaids advantage lies in its automatic optimization of spatial arrangements, ensuring diagrams display well in HTML. By simply declaring the required packages and placing the Mermaid code within the HTML body, the browser renders the diagram seamlessly. In our process, we generate an HTML file for each Mermaid code, open it in Chrome, and capture screenshot of the rendered diagram. We use Selenium to automate this process of browsing and saving images. Diagram style customizations: Mermaid also offers styling parameters to customize the theme and visual appearance of rendered diagrams. These parameters can be included in the HTML header and thus separated from the main diagram content. We prompt LLMs to generate 53 styling specifications, creating candidate set. For each HTML file, we randomly select one styling option from this set to increase visual diversity. In cases where styling option is incompatible with specific diagram type, the default styling is automatically applied. We retain all successfully rendered HTML files. A.4.2 Prompt and Caption Design Understanding diagrams is more challenging because they contain numerous objects and emphasize the relationships among them. Specifically, object relationships in diagrams are often more complex compared to other CI types, as they frequently use arrows or nesting to indicate directions or hierarchies. Therefore, our designed captions focus on extracting elements and relationships, placing less emphasis on the diagrams appearance details. To generate captions that provide detailed breakdown of the diagrams, we prompt Llama-3.1-405B to read the diagram code and translate it into natural language. To ensure the generated captions are as invariant as possible to the diagrams appearance, we include only the Mermaid code in the prompt, excluding any styling-related codes. We find that minimal instruction is sufficient for the LLM to accurately analyze the code. In post-processing the generated captions, we first modify the opening sentence to include an identification of the diagram type, similar to our approach with chart captions. Some Mermaid code retrieved from GitHub contains style arguments like hex color codes or stroke widths for text boxes. Since the LLM interprets code, these styling details sometimes appear in their responses. For example, box labeled Customer might be described in the caption as Customer (#a1320f, stroke width 2). This pattern also occurs when the diagram code assigns shorter variable name (e.g., A) to an object like Customer A. To enhance caption quality, we refine the LLM-generated responses by removing parentheses that contain styling arguments or variable names. 25 Figure 16 The Diagram implementation. 26 Figure 17 Caption design for code screenshot. A.5 Code Implementation We use Carbon (Carbon, 2024) to create code screenshots with customized syntax theme and font style. Specifically, it provides 29 themes and 14 font families, as well as other style parameters such as font color, presence or absence of line numbers, line space, window size, etc. We randomize those options to enhance the diversity of the generated screenshots. Next, we demonstrate what code data are used for screenshot rendering and the caption design. Data source: While numerous code generation datasets provide variety of sources for code snippets, we find these datasets often contain overly complex, lengthy examples with extensive comments. This complexity results in code snippets that are too long to fit within suitably sized screenshot. Additionally, the detailed comments provide explicit explanations of functionality, whereas we aim for MLLMs to learn inference directly from the code itself, without relying on predefined explanations. Therefore, we seek to use LLMs to generate simpler code snippets in different languages. We consider 9 programming languages: C, C++, Ruby, R, Python, Java, JavaScript, CSS, and SQL. For each language, we ask Llama-3.1-70B to generate 200 functions/topics that can be implemented by the selected language. We then prompt the LLM to generate the code given the topic and the language. We focus on relatively simple functions such as mathematical implementations, textbook algorithms and use case of data structures. Caption design: Our goal is for MLLMs to first extract the code text from the screenshot, interpret it, and then provide an explanation of its functionality The code explanation can be obtained by LLMs. Building on the previous code generation step, we further have the Llama-3.1-70B to generate the corresponding explanation for its generated code snippet. The code snippet and its corresponding explanation are then concatenated together to compose the caption. Figure 17 illustrates an example code screenshot and its composed caption. Figure 18 Caption design for table image. A.6 Table Implementation We use Matplotlib (Hunter, 2007) to generate table images from tabular data. Similar to our approach for chart visualization, the tabular data for these table images is sourced from the UniChart dataset. Below, we outline the different types of customizations applied to the table images, followed by discussion of the caption design. Visualization: The following table customizations were applied: 1. Table size: Varying the tables width and height. 2. Cell style: Adjusting the width, height, and color of individual rows or columns. 3. Border style: Modifying the style, thickness, and color of table borders. 4. Font style: Varying font color, type, and size. 5. Alignment: Applying different alignments (left, center, right) to individual columns. We ensure that the data remains clearly visible despite these customizations. Specifically, we maintain strong contrast between foreground and background colors, and adjust font and table sizes appropriately for readability. Caption design. The caption design follows similar principles to those used in code image captions, focusing on extracting the table from the image and analyzing the presented data. To obtain the analysis text, we convert the table into markdown format and prompt the Llama-3.1-70B to generate response that provides summary and detailed breakdowns. The final caption for the table image is composed by combining the markdown table with the analysis text. Figure 18 shows an example table image and its composed caption."
        },
        {
            "title": "B Experiment Details",
            "content": "B.1 Demonstrative Experiment Collecting VQA data for NIs and CIs: We randomly select 1K samples from the VQAv2 (Goyal et al., 2017) validation set to create VQA pairs for NI. However, to our knowledge, no existing benchmark comprehensively covers the wide variety of CI types. To address this, we curate toy benchmark comprising 1K VQA pairs for CIs by sampling from datasets ChartQA (Masry et al., 2022), DocVQA (Mathew et al., 2021), InfoVQA (Mathew et al., 2022), MapQA (Chang et al., 2022), MME (Yin et al., 2023), OCRBench (Liu et al., 2023c), MMVet (Yu et al., 2023), and MMBench (Liu et al., 2025). This curated benchmark includes various CI types collages, charts, tables, code, documents, diagrams, infographics, etc, to offer broad evaluation of MLLMs CI comprehension abilities. The composition of this curated benchmark is summarized in Table"
        },
        {
            "title": "Covered CI Types",
            "content": "#Samples ChartQA (Masry et al., 2022) DocVQA (Mathew et al., 2021) InfoVQA (Mathew et al., 2022) MapQA (Chang et al., 2022) MME (Yin et al., 2023) OCRBench (Liu et al., 2023c) MMVet (Yu et al., 2023) MMBench (Liu et al., 2025) Chart Document Infographic Chart (Map) Collage/Code Document/Infographic Collage/Chart/Diagram/Table Collage/Chart/Diagram/Table 200 100 200 50 60 100 40 250 Table 6 Dataset sources for the curated CI benchmark. Inference: We evaluate the MLLMs caption and VQA accuracy using the provided QA pairs. In the captionconditioned QA pipeline, the captions generated by MLLMs may sometimes lack sufficient context for an LLM to answer the visual question accurately. We instruct the LLM to respond with dont know (IDK) when it identifies insufficient information in the caption. We count the IDK percentages for NIs and CIs. Table 7 shows higher IDK rate for CIs than for NIs, indicating that CI captions tend to capture less information or information of lesser relevance. CI NI"
        },
        {
            "title": "IDK percentage",
            "content": "58% 12% Table 7 IDK percentage: Percentage of an LLM answering IDK given the visual question and caption context. Figure 19 Prompt for evaluating of MLLMs predictions. Evaluation: We only evaluate MLLMs caption accuracy on VQA pairs that are answerable by LLMs. We discard original benchmark evaluation guidelines, as each has its own rubric, and instead adopt MMVets evaluation approach by prompting GPT-4 to score the predicted answers. We demonstrate the prompt used for predicting answer correctness in Figure 19. Figure 2b reports the average prediction score as measures of caption accuracy and VQA accuracy for both CI and NI. For agreement percentage in Figure 2c, we evaluated agreement by checking the correctness scores for each VQA pair; we considered VQA and caption predictions to be in agreement if the absolute difference between their scores was less than 0.2. B.2 MLLMs Training Pre-training: We do not pre-train the MLLMs in our experiment. Instead, we directly use the public pre-trained MLLMs checkpoint to initialize the weights and focus on the SFT stage. For xGen-MM, we use the v1.5 checkpoint to initialize the model weights. SFT data recipe for xGen-MM-inst: Since the dataset used to instruction fine-tune xGen-MM is not released, we curate similar SFT dataset according to the data mixture mentioned in their paper (Xue et al., 2024). Specifically, we include 781K image-text instruction data from various domains (Liu et al., 2023a; Masry et al., 2022; Kafle et al., 2018; Mathew et al., 2021; Lin et al., 2014; Yan et al., 2024; Lindström and Abraham, 2022; Ainslie et al., 2023; Mishra 29 et al., 2019; Krishna et al., 2017; Chen et al., 2023; Singh et al., 2019; Lu et al., 2022; Kembhavi et al., 2016), and 211K pure text instruction following data (Mukherjee et al., 2023; Cobbe et al., 2021; Zhou et al., 2024). We retrain xGen-MM-inst-4B using our own curated dataset for fair comparison. CompCap-4B CompCap-7B CompCap-13B #vision tokens vision encoder image aspect ratio batch size lr lr schedule weight decay optimizer #epochs"
        },
        {
            "title": "2880\nCLIP\nanyres\n128\n2e-5\ncosine\n0\nAdamW\n1",
            "content": "Table 8 Hyperparameters for training CompCap series Hyperparameters: We show the training hyperparameters for the CompCap series in Table 8. The reproduction of xGen-MM-inst. follows the same hyperparameters as in CompCap-4B. We use 8 Nvidia A100 GPUs to train the 4B MLLMs and 32 Nvidia A100 GPUs for the 7B and 13B MLLMs. B.3 ChartQA Image Captioning We employ an superior MLLM to generate captions for chart images in the ChartQA training set. This process produces total of 18,317 chart-caption pairs. As there are multiple instruction data corresponding to one chart image, we replace data in an image-level in the caption-instruction ablation study. Figure 20 shows some examples of chart caption. Figure 20 Examples of ChartQA captions e E PT/SFT #Data t c M ) n ( M e a t h c R Q h Q D o UNK./UNK. UNK./UNK. UNK./UNK. 77.1 77.9 77.6 - 85.5 84.4 81.8 - - 83.2 88.7 86.3 102.0 74.9 96.3 63.8 70.5 65.6 73.6 85.5 84.2 85.7 88.3 88.4 92.8 96.5 94. - 84.5 82.0 Model SoTA MLLMs GPT-4o (OpenAI, 2023a) Qwen-VL-Max (Wang et al., 2024) InternVL-76B (Chen et al., 2024) 3B - 4B MLLMs MM1-3B (McKinzie et al., 2024) VILA-1.5-3B (Lin et al., 2024) Phi-3-vision (Abdin et al., 2024) xGen-MM-inst.-4B (Xue et al., 2024) xGen-MM-inst.-4B (Xue et al., 2024) CompCap-4B 7B - 8B MLLMs 2.9B/1.45M 68.8 32.8M/5.9M 68.0 5B/>8.3M 70.9 >25M/UNK. 71.8 >25M/1M 71.3 >25M/1M 71.6 VILA-1.5-8B ShareGPT4V-7B (Chen et al., 2023) Qwen-VL-chat-7B (Wang et al., 2024) Cambrian-8B Tong et al. (2024) LLaVA-NeXT-Vicuna-7B Liu et al. (2024) CompCap-7B 32.8M/5.9M 65.0 69.3 1.2M/665K 64.8 UNK./UNK. 1.2M/7M 73.3 71.2 558K/779K 70.5 558K/779K 13B MLLMs VILA-1.5-13B (Lin et al., 2024) ShareGPT4V-13B (Chen et al., 2023) OmChat-v2.0-13B (Zhao et al., 2024) Cambrian-13B (Tong et al., 2024) LLaVA-NeXT-Vicuna-13B (Liu et al., 2024) CompCap-13B 32.8M/5.9M 72.7 1.2M/665K 70.6 >6.5B/20M 75.2 73.2 71.9 72.2 1.2M/7M 558K/779K 558K/779K 71.9 55.6 63.6 72.0 67.7 67.9 60.2 58.3 60.7 72.6 65.2 65.6 61.2 52.7 79.8 72.8 67.6 67.8 67.8 62.4 74.2 76.0 75.5 76. 68.6 68.8 60.6 75.9 67.6 68.9 74.3 69.0 82.1 75.7 68.9 70.8 62.9 58.2 55.2 64.1 64.0 64.7 60.7 68.4 66.4 64.4 66.3 67.5 61.4 66.2 76.1 66.8 68.8 71.4 72.1 65.5 82.2 75.7 78.2 81. 71.7 66.9 67.7 71.0 72.4 75.5 73.4 69.1 66.1 76.1 77.1 83.4 32.0 30.6 45.1 39.5 32.6 35.0 37.3 26.5 34.9 47.0 39.6 41.7 42.5 29.3 57.1 47.4 42.4 45.0 - 43.7 63.7 54.8 51.6 52. 43.8 37.1 48.8 61.4 55.1 58.5 46.0 39.8 72.8 61.0 57.7 61.4 - 52.9 81.8 59.5 54.8 57.4 50.9 21.3 49.8 72.6 63.5 68.9 74.6 24.6 79.9 73.8 68.5 73.9 - - 84.3 61.1 55.2 58. - 14.4 62.6 77.8 76.5 77.6 - 14.5 88.7 76.8 79.9 81.1 - - 50.0 31.3 27.6 27.9 - 14.7 29.7 40.1 39.2 40.8 - 17.2 58.8 44.6 43.8 47.0 b - - - - - 65.2 55.8 50.6 55.8 - 36.4 53.6 68.9 70.4 73.7 - 39.4 88.2 70.7 75.3 79.3 . - - - - - 66.9 60.2 57.2 58.9 - 43.8 54.5 65.9 62.5 64.5 - 44.8 75.0 67.2 65.6 68.5 Table 9 Full comparison on MLLM benchmarks. e E t T e ) n ( c A L i a c R Q h c Q n R W . Component Baseline 71.9 67. 68.9 68.8 77.1 42.4 57.7 68. 79.9 43.8 75.3 65.6 + Collage + Code + Table + œ Diagram + Chart + Image-Text 72.1 72.3 72.3 72.2 72.4 72. 67.3 67.8 67.8 67.6 67.5 67.8 69.9 70.2 70.3 70.7 70.5 70.8 69.1 69.3 69.4 69.9 70.0 71.4 78.4 76.4 78.7 80.4 84.0 83.4 43.2 43.6 43.6 43.6 46.8 45.0 58.8 58.7 59.1 58.3 58.6 61. 70.9 71.1 72.0 72.9 73.1 73.9 80.4 80.6 80.5 81.2 81.1 81.1 45.3 46.1 46.1 46.9 46.1 47.0 75.5 76.1 76.6 77.6 77.6 79.3 66.4 66.6 67.0 67.4 68.0 68.5 Table 10 Full benchmark result of the ablation study on each CI category."
        },
        {
            "title": "C Additional Results",
            "content": "C.1 Extended Comparison We provide the full comparison result against SoTA MLLMs in Table 9. We additionally include the results from MLLMs such as GPT-4o (OpenAI, 2023a), Qwen-VL-Max (Wang et al., 2024), and InternVL-76B (Chen et al., 2024). C.2 Full result on the ablation study of CI category Table 10 provides the scores of all benchmark for the CI component ablation study. 31 e E Training schedule CompCap-7B t c M ) n ( M e a t h c R Q h Q D o R W . 70. 65.4 67.0 65.5 73.2 40.4 54. 67.0 78.0 39.2 72.1 63.0 (truncated) (cosine) (linear) (uniform) CompCap-13B (truncated) (cosine) (linear) (uniform) 70.8 65.3 67.9 65. 71.1 40.2 55.2 67.4 77.3 40. 73.0 63.2 70.6 65.8 67.6 67. 73.9 40.8 54.7 67.4 77.1 39. 71.8 63.4 70.5 65.6 68.9 67. 75.5 41.7 58.5 68.9 77.6 40. 73.7 64.5 72.2 67.8 70.2 69. 78.7 44.6 58.9 73.2 82.0 45. 76.6 67.2 72.1 67.4 69.9 70. 76.6 44.8 60.4 73.1 80.7 45. 78.2 67.2 72.5 67.5 71.2 69. 79.6 45.3 58.6 72.3 80.8 45. 78.3 67.3 72.2 67.8 70.8 71. 83.4 45.0 61.4 73.9 81.1 47. 79.3 68.5 C.3 Ablation on training data sampler Table 11 Ablation study on train data samplers. During the SFT stage, MLLMs are typically trained to follow instructions and enhance conversational capabilities. Unlike the usual instruction data, which is often in QA format, CompCap-118K primarily emphasizes caption data, which aims at facilitating vision-language alignment. In this experiment, we explore various strategies for combining our caption data with instructional data during training. Specifically, we set proportion of caption data from CompCap-118K in each training batch, and adjust this ratio at every training step. We hypothesize that increasing caption data early in training will strengthen alignment, while focusing more on instructional data in later stages will maintain the models instruction-following proficiency. To explore this, we experiment with four training data samplers: 1. Truncated. This sampler first samples from CompCap-118K until all data is used, then shifts to the original downsampled SFT dataset. 2. Cosine. For the t-th training step, this sampler returns batch where αcosine(t/T ) percent is drawn from CompCap-118K, with representing the total training steps and α adjusted so all CompCap-118K data is covered by trainings end. 3. Linear. Similar to the Cosine sampler, but the schedule changes linearly to 1 t/T 4. Uniform. This sampler uniformly mixes data from CompCap-118K and the original downsampled SFT dataset in each batch. We evaluate the impact of these samplers using CompCap-7B/13B. We train them on the SFT dataset with each sampler and present the results in Table 11. We find that the uniform sampler consistently outperforms others for both 7B and 13B MLLMs. Unless specified otherwise, all models in our experiments use the uniform sampler for training. C.4 Diversity Analysis We demonstrate the diversity of the caption for each CI category in Figure 21. The inner circle of the plot displays the root verbs of the captions, while the outer circle represents the corresponding direct nouns. We display the top 20 root verbs for each CI class, along with the top 5 nouns associated with each root verb. C.5 Examples of Synthesized CI-caption Pairs We show examples of the curated image-caption pairs for each CI type in Figures 22, 23, 24, 25, 26, 27."
        },
        {
            "title": "Collage",
            "content": "Image-Text"
        },
        {
            "title": "Table",
            "content": "Figure 21 Diversity analysis of captions for different CI types. 34 Figure 22 Image and caption examples of collage in CompCap-118K 35 Figure 23 Image and caption examples of image-text in CompCap-118K Figure 24 Image and caption examples of chart in CompCap-118K 37 Figure 25 Image and caption examples of diagram in CompCap-118K 38 Figure 26 Image and caption examples of table in CompCap-118K Figure 27 Image and caption examples of code in CompCap-118K"
        },
        {
            "title": "D Qualitative Examples",
            "content": "In this section, we show more qualitative captioning results (Figures 28, 29, 30, 31) of CompCap-13B, in comparison with LLaVA-NeXT-Vicuna-13B. Figure 28 More examples of MLLMs on CI captioning (part 1). Figure 29 More examples of MLLMs on CI captioning (part 2). 41 Figure 30 More examples of MLLMs on CI captioning (part 3). Figure 31 More examples of MLLMs on CI captioning (part 4)."
        }
    ],
    "affiliations": [
        "Georgia Tech",
        "Meta",
        "Tufts University"
    ]
}