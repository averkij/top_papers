{
    "paper_title": "ThinkRL-Edit: Thinking in Reinforcement Learning for Reasoning-Centric Image Editing",
    "authors": [
        "Hengjia Li",
        "Liming Jiang",
        "Qing Yan",
        "Yizhi Song",
        "Hao Kang",
        "Zichuan Liu",
        "Xin Lu",
        "Boxi Wu",
        "Deng Cai"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Instruction-driven image editing with unified multimodal generative models has advanced rapidly, yet their underlying visual reasoning remains limited, leading to suboptimal performance on reasoning-centric edits. Reinforcement learning (RL) has been investigated for improving the quality of image editing, but it faces three key challenges: (1) limited reasoning exploration confined to denoising stochasticity, (2) biased reward fusion, and (3) unstable VLM-based instruction rewards. In this work, we propose ThinkRL-Edit, a reasoning-centric RL framework that decouples visual reasoning from image synthesis and expands reasoning exploration beyond denoising. To the end, we introduce Chain-of-Thought (CoT)-based reasoning sampling with planning and reflection stages prior to generation in online sampling, compelling the model to explore multiple semantic hypotheses and validate their plausibility before committing to a visual outcome. To avoid the failures of weighted aggregation, we propose an unbiased chain preference grouping strategy across multiple reward dimensions. Moreover, we replace interval-based VLM scores with a binary checklist, yielding more precise, lower-variance, and interpretable rewards for complex reasoning. Experiments show our method significantly outperforms prior work on reasoning-centric image editing, producing instruction-faithful, visually coherent, and semantically grounded edits."
        },
        {
            "title": "Start",
            "content": "6 2 0 2 6 ] . [ 1 7 6 4 3 0 . 1 0 6 2 : r , ThinkRL-Edit: Thinking in Reinforcement Learning for Reasoning-Centric Image Editing Hengjia Li1,2,, Liming Jiang2,, Qing Yan2, Yizhi Song2, Hao Kang2, Zichuan Liu2, Xin Lu2, Boxi Wu1,, Deng Cai1 1Zhejiang University, 2Intelligent Creation, ByteDance Work done during internship at ByteDance, Project Lead, Corresponding author"
        },
        {
            "title": "Abstract",
            "content": "Instruction-driven image editing with unified multimodal generative models has advanced rapidly, yet their underlying visual reasoning remains limited, leading to suboptimal performance on reasoning-centric edits. Reinforcement learning (RL) has been investigated for improving the quality of image editing, but it faces three key challenges: (1) limited reasoning exploration confined to denoising stochasticity, (2) biased reward fusion, and (3) unstable VLM-based instruction rewards. In this work, we propose ThinkRL-Edit, reasoning-centric RL framework that decouples visual reasoning from image synthesis and expands reasoning exploration beyond denoising. To the end, we introduce Chain-of-Thought (CoT)based reasoning sampling with planning and reflection stages prior to generation in online sampling, compelling the model to explore multiple semantic hypotheses and validate their plausibility before committing to visual outcome. To avoid the failures of weighted aggregation, we propose an unbiased chain preference grouping strategy across multiple reward dimensions. Moreover, we replace interval-based VLM scores with binary checklist, yielding more precise, lower-variance, and interpretable rewards for complex reasoning. Experiments show our method significantly outperforms prior work on reasoning-centric image editing, producing instruction-faithful, visually coherent, and semantically grounded edits. Date: January 8,"
        },
        {
            "title": "Introduction",
            "content": "Recent progress in unified multi-modal generative models [18, 19, 30, 39, 40, 42] has significantly advanced instruction-driven image editing. However, despite impressive visual fidelity, the reasoning capability behind such edits remains largely underexplored. In particular, reasoning-centric editing requires models to thoroughly understand both the reference image and the given instruction before synthesis, rather than merely producing visually plausible content as illustrated in figure 1. Prior efforts have explored reinforcement learning (RL) [21, 29, 34, 36, 47] to substantially improve the editing quality. However, they exhibit clear challenges when applied to reasoning-centric image editing, which requires not only high-fidelity synthesis but also strong visual reasoning prior to generation. Three major challenges arise: Limited reasoning exploration. Existing RL approaches typically restrict exploration to stochasticity within the denoising process while the reasoning processes underpinning the edits remain under-explored 1 Figure 1 Comparisons on reasoning-centric image editing. Although unified multimodal generative models such as Qwen-Edit [38] have substantially improved editing quality, their underlying reasoning remains underexplored, especially for reasoning-centric editing. In contrast, our method delivers accurate edits with deep reasoning, achieving strong consistency and high perceptual quality across diverse reasoning-driven editing scenarios. as shown in figure 2. For example, FlowGRPO [21] expands the search space by converting ODE-based denoising into SDE-based sampling, yet it neglects exploration across diverse visual reasoning trajectories. Thus, these methods are better suited for text rendering and aesthetic enhancement, but fundamentally insufficient for reasoning-driven editing, where reasoning must precede generation. Biased reward aggregation. Editing requires balancing instruction fidelity, visual consistency, and generation quality. Previous methods [21, 47] typically combine these rewards using simple weighted sums. This naive aggregation is highly vulnerable to edge cases. For example, an unchanged image may obtain very high consistency score, while an instruction-accurate edit might be unfairly penalized for larger semantic changes. Unstable instruction rewards. Prior works often rely on vision-language models (VLMs) [2, 4] to assign discrete instruction-following scores (e.g., 15). However, such reward signals are high-variance and inconsistent, especially for complex reasoning tasks, where repeated evaluations frequently produce differing results. In this work, we address these challenges by introducing reasoning-centric RL framework for instruction-based image editing that decouples reasoninggeneration during exploration. Specifically, to expand the exploration space beyond denoising stochasticity and enable optimization over diverse reasoning trajectories, we explicitly separate and optimize visual reasoning prior to image generation. Furthermore, we introduce chain-of-thought (CoT) [37] sampling, incorporating planning and reflection stages prior to image generation. This design 2 Figure 2 Comparison with prior methods. Prior RL methods for visual generation [21, 47] focus on exploration within the stochastic space of generation, improving synthesis quality but offering limited reasoning capability. To address this issue, we decouple and optimize the understanding and generation modules to preserve high-fidelity synthesis while enabling exploration of optimal trajectories in the reasoning space. Besides, we introduce CoT-based sampling and optimization to further expand stochastic exploration over reasoning pathways. compels the model to explore multiple semantic hypotheses and evaluate their plausibility before committing to visual outcome. It helps the model establish correct semantic interpretations not just by seeking better denoising, but by first discovering and refining meaningful visual reasoning paths. Besides, to avoid the limitations of naive weighted reward fusion, which often collapses towards trivial solutions or overfits individual objectives, we introduce an unbiased chain preference optimization strategy that holistically ranks reasoning chains across all reward dimensions. Instead of collapsing rewards into one scalar, we jointly sort sampled chains per group and update gradients only from chains that form consistent total order. This captures unified preferences across objectives (e.g., instruction faithfulness, visual coherence, perceptual quality) and prevents trivial solutions or overfitting to single objectives. Furthermore, to provide more precise and stable reasoning rewards from vision-language models, we replace interval-based scoring with checklist evaluation. For each editing instruction, we derive binary questions from the reference image and prompt, have the VLM answer yes/no, and use the count of yes as the alignment score. Experiments show this fine-grained reasoning reward yields more accurate, lower-variance, and more interpretable rewards, especially for complex reasoning where scalar scores fluctuate or miss nuanced compliance. Extensive experiments demonstrate that our approach significantly outperforms prior methods on reasoningcentric image editing tasks, producing edits that are not only instruction-faithful but also visually coherent and semantically grounded. In summary, our contributions are as follows. We propose to decouple visual reasoning from image synthesis and further introduce CoT-based reasoning sampling to explore diverse trajectories before generation. We introduce unbiased ranking-based grouping strategy that orders sampled reasoning chains across multiple reward dimensions, avoiding weighted-fusion collapse. We replace interval-based VLM scoring with binary checklist from the reference image and instruction, yielding more precise, lower-variance, and interpretable rewards for complex reasoning. 3 Figure 3 Overview of our method. During sampling, we perform Chain-of-Thought reasoning with explicit planning and reflection to enlarge stochasticity in the reasoning space. For rewards, fine-grained, sample-specific checklist guides the VLM to produce accurate and stable reasoning scores. In grouping, we construct an unbiased preference chain across candidates to select training samples and compute advantages A. Finally, policy updates apply unified editing reward while decoupling updates to the reasoning, understanding, and generation modules, enhancing reasoning capability without sacrificing synthesis quality. We conduct comprehensive experiments across multiple benchmarks, demonstrating that our method substantially outperforms prior works on reasoning-centric image editing."
        },
        {
            "title": "2.1 Reasoning-Centric Image Editing",
            "content": "Reasoning-centric image editing models aim to bridge high-level semantic understanding and reasoning of textual instructions with precise visual manipulation. Traditional approaches achieve accurate editing by modifying the diffusion trajectory without additional training, such as partial denoising from intermediate SDE steps [24], cross-attention control [11, 31], mask-guided blending [1, 33, 35], CLIPor diffusion-guided manipulation [1417], and latent inversion for fidelity preservation [13, 32]. Despite their strong controllability, these methods lack the capacity to handle complex, reasoning-intensive semantic edits. Recent unified multimodal models advance in complementary direction by employing single framework for both image understanding and editing [18, 19, 30, 39, 40, 42]. For example, Bagel [7] introduces think mode that first generates reasoning text to enhance instruction fidelity and semantic consistency during editing. However, despite these advances, current models still struggle with tasks that require deeper logical reasoning and multi-step inference during visual editing."
        },
        {
            "title": "2.2 Reinforcement Learning for Visual Generation",
            "content": "Reinforcement Learning from Human Feedback (RLHF) [27] has emerged as the dominant paradigm for aligning large language models (LLMs) to be more helpful [12, 29], honest [10], and harmless [48]. Inspired by its success in language alignment, recent studies have extended RL-based frameworks to text-to-image (T2I) generation [3], typically by training reward model (RM) on human preference data [45] or prompt-image 4 alignment scores [44]. Building on this foundation, advanced algorithms such as Group Relative Policy Optimization (GRPO) [21, 29, 34, 36, 47] have shown strong potential in aligning both diffusion and flowmatching models. For example, FlowGRPO reformulates the deterministic ODE process of flow matching into stochastic differential equation, effectively expanding the exploration space of denoising trajectories. However, these methods largely overlook the semantic reasoning search space, and their reward models remain limited in evaluating reasoning-intensive editing tasks, resulting in suboptimal performance when complex logical inference is required during visual editing. Figure 4 Comparisons between ThinkRL-Edit and the leading baselines. We conduct the comparison across diverse reasoning-centric editing tasks. As observed, our method achieves precise instruction following with strong consistency and high quality, which significantly surpasses previous methods. Blue text denotes the instruction, and green text indicates the desired editing outcome."
        },
        {
            "title": "2.3 Chain of Thought for Visual Generation",
            "content": "Chain-of-Thought (CoT) [37] reasoning improves the ability of Large Language Models (LLMs) to solve complex problems by emulating human step-by-step thinking. Instead of directly outputting final answers, CoT encourages models to generate explicit intermediate reasoning steps, thereby enhancing their interpretability and logical consistency. Building on its effectiveness [5, 8, 23], recent studies [25, 28, 49, 51] have sought 5 5: 6: 7: 8: 9: 10: 11: 12: 13: 14: 15: 16: 17: 18: Algorithm 1 ThinkRL-Edit Training Algorithm Require: Initial policy model πθ = (πUnd , πGen θ θ k=1 dataset = {(P, C)}; timestep selection ratio τ ; total sampling steps ); reward models {Rk}K ; instruction-based image editing Ensure: Optimized policy model πθ 1: for training iteration = 1 to do Update old policy: πθold πθ 2: for each reference image and prompt (p, c) Db do 3: 4: Generate reasoning prompt based on (p, c) using πUnd θold Generate samples: {oi}G i=1 Generate reflected prompt {c Generate reflected samples: {oi}2G for each sample 1..2G do with (p, c) using πθold i}G based on (oi, p, c) using πUnd θold with (p, c) using πθold i=G+1 i=1 Calculate multiple rewards {rk }K k=1 Db is the sampled batch CoT-based Reasoning Path Sampling end for Filter samples by unbiased grouping to get {{rk for each filtered sample 1..N do Calculate advantage Ai Kµ (cid:80)K k=1 rk Kσ }K k=1}N i=1 Unbiased Chain Preference Grouping is the length of current preference chain via gradient ascent: θ θ + ηθJUnd Decoupled Und-Gen Optimization end for Update πUnd for τ do Update πGen θ θ end for via gradient ascent: θ θ + ηθJGen end for 19: 20: end for to extend CoT into the multi-modal domain. These efforts aim to endow Multi-modal Large Language Models (MLLMs) with structured reasoning abilities for handling complex vision-language tasks, ranging from challenging visual question answering [43] to reasoning-driven image editing [9] and embodied planning [26]."
        },
        {
            "title": "3.1 Preliminary",
            "content": "GRPO [29] introduces group-relative advantage to stabilize policy updates. When applied to flow matching models [20], for group of generated images {xi , the advantage of the i-th image is 0}G i=1 0, c) mean({R(xj 0, c)}G The policy is updated by maximizing the regularized objective std({R(xj R(xi = Ai 0, c)}G j=1) j=1) . where (rGen, A, θ, ϵ, β) = with ri t(θ) = pθ(xi pθold (xi t1xi t1xi t,c) t,c) JGen(θ) = Ec,{xi} (cid:104) (rGen, A, θ, ϵ, β) (cid:105) , min (cid:0)ri t(θ)Ai t, clip(ri t(θ), 1 ϵ, 1 + ϵ)Ai t"
        },
        {
            "title": "1\nG",
            "content": "G (cid:88) i="
        },
        {
            "title": "1\nT",
            "content": "T 1 (cid:88) t=0 . (1) (2) (cid:1) βDKL(πθπref), (3) To satisfy GRPOs stochastic exploration requirements for flow matching models, FlowGRPO[22] convert the deterministic Flow-ODE dxt = vtdt to an equivalent SDE: dxt = (cid:0)vθ(xt, t) + σ2 2t (xt + (1 t)vθ(xt, t))(cid:1)dt + σtdwt, (4) 6 Table 1 Quantitative comparisons on KRIS-Bench. We report the composite score for each category and the average Instruction Following (IF), Visual Consistency (VC), Visual Quality (VQ)."
        },
        {
            "title": "Method",
            "content": "Attribute Percep. Spatial Percep. Social Science Natural Science Logical Reasoning Instruction Decompos. Factual Know. Conceptual Know. Procedural Know."
        },
        {
            "title": "Overall\nScore",
            "content": "Avera. IF Avera. VC Avera. VQ OmniGen2 65.41 53.36 50. 45.30 32.19 56.36 63.57 Flux-Kontext 70.78 69.20 51. 52.05 45.82 73.67 70.38 Bagel 61. 62.08 50.21 46.26 30.21 48.44 61.55 Bagel-Think 60.39 61.19 49.06 47.44 29.44 48.36 60.61 UniCoT 67.94 73.72 59.45 53.19 40.97 54.67 69.38 Qwen-Edit 72.57 79.92 61.45 56.38 48."
        },
        {
            "title": "Ours",
            "content": "81.02 81.45 75.67 71.25 49.07 78.44 79.71 74. 81.13 46.55 51.86 47.21 47.83 54. 57.60 72.31 38.83 53.55 35.23 34. 44.78 56.68 49.24 39.40 66.72 93. 57.35 46.61 77.09 94.08 48.69 51. 52.49 86.98 48.71 55.68 70.00 96. 56.76 57.24 59.52 92.60 62.77 56. 76.37 95.86 57.44 71.65 71.16 77.52 97.12 where dwt denotes Wiener process increments and σt controls the stochasticity."
        },
        {
            "title": "3.2 CoT-based Reasoning Sampling",
            "content": "FlowGRPO improves generation quality by searching for optimal trajectories in the extended denoising space. However, its performance remains limited on reasoning-oriented generation tasks due to the lack of exploration in the semantic reasoning space. To address this limitation, we propose to separately optimize the semantic reasoning path and introduce stochasticity within the reasoning space. Specifically, we incorporate Chain-of-Thought (CoT), instruction reasoning, and editing reflection into the sampling phase. As illustrated in figure 3 and algorithm 1, during GRPO sampling, the model first employs its understanding module πUnd to perform reasoning and atomic decomposition of the instruction based on the reference image. The reasoning-enhanced instruction is then used for sampling. Afterwards, the generated editing result undergoes single reflection process, where the understanding module provides feedback that is concatenated with the previous reasoning instruction and fed back into the next sampling stage. Consistently with training time, we enable planning and single reflection at inference time."
        },
        {
            "title": "3.3 Fine-Grained Reasoning Reward",
            "content": "To provide more precise and stable reasoning rewards from vision-language models (VLMs) [2], we replace conventional interval-based scoring [21, 47] with fine-grained checklist-based evaluation. Specifically, for each editing instruction, we construct set of binary questions derived from both the reference image and the instruction using Gemini [6]. Unlike previous methods that query VLMs [21, 47] with unified system prompt, our checklist is individually constructed for each referenceinstruction pair, enabling fine-grained and context-aware assessment. The VLM is then guided to answer each question with yes or no, and the proportion of positive responses is averaged across all dimensions to obtain the final reasoning score. Empirical results demonstrate that this checklist formulation produces more accurate, lower-variance, and interpretable reward signals, particularly for complex reasoning tasks where conventional scalar scores often fluctuate or fail to capture subtle instruction compliance."
        },
        {
            "title": "3.4 Unbiased Chain Preference Grouping",
            "content": "In addition to the instruction score, we further evaluate consistency and image quality, as both are crucial for editing tasks. To mitigate the limitations of naïve weighted reward fusion, which often collapses toward trivial solutions or overfits specific objectives, we introduce an unbiased chain preference grouping strategy that holistically ranks preference chains. Instead of aggregating heterogeneous rewards {rk into single scalar, we jointly sort all rewarded samples across multiple dimensions to construct total order of candidates, where only chains that maintain consistent global ranking contribute to gradient updates. This design enables the policy to capture unified preference structure across diverse objectives, e.g., instruction faithfulness, visual coherence, and perceptual quality. Finally, we average and normalize the scores across all dimensions within the full ordered chain {{rk to obtain the final grouped advantage A. }K k=1 }K k=1}N i= 7 Table 2 Quantitative comparisons on RISE-Bench."
        },
        {
            "title": "Method",
            "content": "Temporal Causal Spatial Logical Overall"
        },
        {
            "title": "Overall\nQuality",
            "content": "Flux-Kontext OmniGen2 Bagel Bagel-Think UniCoT Qwen-Edit Ours 2.3 1.2 2.4 5.9 8.2 4.7 18.8 5.5 1.0 5.6 17.8 18.9 10.0 37.5 13.0 0.0 14.0 21.0 20.0 17.0 25.0 1.2 1.2 1.2 1.2 1.2 2.4 37.5 5.8 0.8 6.1 11.9 12.5 8.9 29. 26.0 22.0 36.5 45.9 48.3 37.2 61.7 71.6 32.6 53.5 73.8 76.2 66.4 81.64 85.2 55.3 73.0 80.1 83.8 86.9 62."
        },
        {
            "title": "3.5 Decoupled Und-Gen Optimization",
            "content": "Unlike FlowGRPO, which optimizes only the generation trajectory, we jointly optimize both the reasoning and understanding components. As illustrated in figure 3, during the policy update stage, beyond the generation part, we first compute the conditional probabilities for both the reasoning and understanding modules. ri Und = (yi x) pUnd θ pUnd old (yi x) (cid:32) (cid:88) = exp log pUnd θ (yi x, yi <t) t=1 (cid:88) t=1 log pUnd old (yi x, yi <t) (cid:33) (5) is the t-th token for the i-th sampled response sequence, where denotes the input image and prompt, yi log pθ(yi x) represents the probability of generating yi by the understanding module. The reasoning and understanding module are then updated by maximizing the objectives respectively JUnd(θ) = Ex (cid:104) (rUnd, A, θ, ϵ, β) (cid:105) , After that, we compute the probability of generating xi t1 from xi by the generation module ri Gen,t(θ) = pθ(xi pθold(xi t1xi t1xi t, c) t, c) (6) (7) where xi is the latent for timestep of the i-th sample. Then we update the generation module by maximizing JGen(θ) = c,{xi t} (cid:104) (rGen, A, θ, ϵ, β) (cid:105) . (8)"
        },
        {
            "title": "4.1 Experiment Setup\nTraining We adopt Qwen-Edit [39] as our base model. Training is conducted with a group size of 128 and\na batch size of 4. The rewards of reasoning, consistency, and quality are computed using Qwen3-VL [46].\nTo optimize GPU memory utilization, we employ Fully Sharded Data Parallelism (FSDP) for the trainable\nmodules along with gradient checkpointing.",
            "content": "Evaluation For quantitative evaluation, we employ two comprehensive benchmarks: KRIS [41] and RISE [50] which assesses reasoning-centric image editing through diverse natural language instructions. Specifically, RISE focuses on reasoning-informed editing across temporal, causal, spatial, and logical dimensions, while KRIS serves as diagnostic benchmark categorizing editing tasks into factual, conceptual, and procedural knowledge types. 8 Table 3 Results for user study. Method Instruction Following (%) Visual Consistency (%) Visual Quality (%) Bagel Bagel-Think UniCoT Qwen-Edit Ours 0.43 4.68 8.72 1.97 79.36 3.19 3.83 8.09 6.81 76.60 4.89 2.55 9.36 8.09 75.11 Table 4 Ablation study for CoT-based und-gen optimization. Gen. Und. Plan. Reflect."
        },
        {
            "title": "Average\nVQ",
            "content": "59.68 60.79 66.82 69.29 71.16 75.60 74.67 78.58 77.81 77. 95.34 96.58 96.15 96.59 97."
        },
        {
            "title": "4.2 Qualitative Analysis",
            "content": "figure 4 showcases results on diverse, challenging instructions. As shown, prior methods exhibit poor instruction following, revealing limited reasoning capability. In contrast, our approach maintains strong fidelity to reasoning-centric context while making precise visual editings. It achieves high instruction following, substantial image consistency, and plausible visual transitions, highlighting both the effectiveness and interpretability of our RL strategy."
        },
        {
            "title": "4.3 Quantitative Analysis\nResults on KRIS-Bench. As shown in table 1, our method improves performance across all metrics, with the\nlargest gains on instruction following. Building on Qwen-Edit, we raise the instruction-following score from\n56.54 to 71.16 (+14.62), achieving state-of-the-art results among open-source models. Beyond the overall\nimprovement, we observe pronounced gains in Attribute Perception, Social Science, Natural Science, and\nConceptual Knowledge, indicating substantially enhanced reasoning capabilities in previously underperforming\ndimensions.",
            "content": "Results on RISE-Bench. On the out-of-domain RISE-Bench, our method exhibits strong generalization as shown in table 2. It improves Qwen-Edits overall score from 8.9 to 29.7 (+20.8) and boosts the reasoning score from 37.2 to 61.7 (+24.5). These results indicate that our method effectively preserves and enhances reasoning ability under distribution shift. Results of the User Study. For comprehensive evaluation, we conducted human preference study comparing our method with baselines along three dimensions: instruction following, visual consistency, and visual quality. We conduct it with 20 participants, each presented with 24 comparison groups. In each group, participants are asked to select the best result along all evaluation dimensions. As shown in table 3, users consistently preferred our method across all criteria, indicating that it produces outputs more aligned with human preferences."
        },
        {
            "title": "4.4 Ablation Study\nCoT-based Und-Gen Optimization. To assess the effectiveness of our cot-based understanding–generation\noptimization, we conduct a comprehensive ablation study in table 4. During training, we incrementally\nadd each module. At inference, we consistently enable planning and a single reflection. Results show that\nintroducing the understanding module yields a large gain in instruction following, and adding planning\nand reflection provides further improvements, indicating that our approach effectively enhances the model’s\nreasoning capability.",
            "content": "Fine-Grained Reasoning Reward. In table 5, we compare two scoring schemes: (i) traditional 15 rating 9 Table 5 Ablation study for checklist-based reasoning reward and unbiased multi-rewards grouping. Checklist UCPG"
        },
        {
            "title": "Average\nVQ",
            "content": "64.28 68.04 71.16 77.13 78.81 77.52 96.58 96.51 97.12 from the VLM, and (ii) checklist-guided procedure that elicits reasoning-based rewards. Comparing Row 1 and Row 2, the fine-grained checklist yields higher instruction-following score, indicating that it helps the VLM provide more accurate judgments and, in turn, enables more precise learning of reasoning abilities. Unbiased Chain Preference Grouping (UCPG). In table 5, we compare simple weighted average with our UCPG strategy. As observed, weighted averaging (Row 2) modestly improves reasoning, but the consistently high consistency scores introduce bias that overfits to the results with more consistency and less instruction following. With UCPG (Row 3), the instruction following score improves further, indicating that UCPG effectively mitigates the bias induced by high consistency."
        },
        {
            "title": "5 Conclusion",
            "content": "In this work, we revisited instruction-driven image editing from reasoning-centric perspective. Unlike previous reinforcement learning approaches that primarily optimize the generative process, our method explicitly separates visual reasoning from synthesis, enabling models to explore diverse reasoning trajectories before producing final edits. By integrating chain-of-thought sampling, unbiased chain preference grouping, and checklist-based reward design, our framework achieves stable, interpretable, and semantically grounded policy updates. Extensive experiments verify that this reasoninggeneration decoupling not only enhances instruction faithfulness but also preserves visual coherence and image quality. We believe this study highlights the importance of reasoning as first-class objective in visual editing, paving the way toward multi-modal generative models capable of deliberate and explainable visual reasoning."
        },
        {
            "title": "6 Limitations and Future Work",
            "content": "Our method expresses the reasoning process through chain-of-thoughts (CoT) with explicit planning and reflection. While this design improves semantic interpretability, it introduces redundant linguistic descriptions and nearly doubles the editing time overhead. Future research can explore latent CoT representations that encode multi-modal reasoning directly in the latent space, thereby integrating visual and textual cues more holistically and eliminating the need for additional editing iteration. We believe such latent reasoning frameworks will further bridge the gap between visual understanding and generation, leading to more efficient and visually grounded reasoning processes in unified multi-modal models."
        },
        {
            "title": "References",
            "content": "[1] Omri Avrahami, Dani Lischinski, and Ohad Fried. Blended diffusion for text-driven editing of natural images. In CVPR, pages 1820818218, 2022. [2] Shuai Bai, Keqin Chen, Xuejing Liu, Jialin Wang, Wenbin Ge, Sibo Song, Kai Dang, Peng Wang, Shijie Wang, Jun Tang, Humen Zhong, Yuanzhi Zhu, Mingkun Yang, Zhaohai Li, Jianqiang Wan, Pengfei Wang, Wei Ding, Zheren Fu, Yiheng Xu, Jiabo Ye, Xi Zhang, Tianbao Xie, Zesen Cheng, Hang Zhang, Zhibo Yang, Haiyang Xu, and Junyang Lin. Qwen2.5-vl technical report. arXiv preprint arXiv:2502.13923, 2025. [3] Kevin Black, Michael Janner, Yilun Du, Ilya Kostrikov, and Sergey Levine. Training diffusion models with reinforcement learning. arXiv preprint arXiv:2305.13301, 2023. [4] Zhe Chen, Jiannan Wu, Wenhai Wang, Weijie Su, Guo Chen, Sen Xing, Muyan Zhong, Qinglong Zhang, Xizhou Zhu, Lewei Lu, et al. Internvl: Scaling up vision foundation models and aligning for generic visual-linguistic tasks. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 2418524198, 2024. [5] Antonia Creswell, Murray Shanahan, and Irina Higgins. Selection-inference: Exploiting large language models for interpretable logical reasoning. In The Eleventh International Conference on Learning Representations. [6] Google DeepMind. Gemini 2.5 pro. https://deepmind.google/technologies/gemini/, 2025. [7] Chaorui Deng, Deyao Zhu, Kunchang Li, Chenhui Gou, Feng Li, Zeyu Wang, Shu Zhong, Weihao Yu, Xiaonan Nie, Ziang Song, Guang Shi, and Haoqi Fan. Emerging properties in unified multimodal pretraining. arXiv preprint arXiv:2505.14683, 2025. [8] Yanlin Feng, Xinyue Chen, Bill Yuchen Lin, Peifeng Wang, Jun Yan, and Xiang Ren. Scalable multi-hop relational reasoning for knowledge-aware question answering. In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 12951309, 2020. [9] Xingyu Fu, Minqian Liu, Zhengyuan Yang, John Corring, Yijuan Lu, Jianwei Yang, Dan Roth, Dinei Florencio, and Cha Zhang. Refocus: Visual editing as chain of thought for structured image understanding. arXiv preprint arXiv:2501.05452, 2025. [10] Chujie Gao, Siyuan Wu, Yue Huang, Dongping Chen, Qihui Zhang, Zhengyan Fu, Yao Wan, Lichao Sun, and Xiangliang Zhang. Honestllm: Toward an honest and helpful large language model. arXiv preprint arXiv:2406.00380, 2024. [11] Amir Hertz, Ron Mokady, Jay Tenenbaum, Kfir Aberman, Yael Pritch, and Daniel Cohen-Or. Prompt-to-prompt image editing with cross attention control. arXiv preprint arXiv:2208.01626, 2022. [12] Edward Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean Wang, Lu Wang, Weizhu Chen, et al. Lora: Low-rank adaptation of large language models. ICLR, 1(2):3, 2022. [13] Bahjat Kawar, Shiran Zada, Oran Lang, Omer Tov, Huiwen Chang, Tali Dekel, Inbar Mosseri, and Michal Irani. Imagic: Text-based real image editing with diffusion models. In CVPR, pages 60076017, 2023. [14] Gwanghyun Kim, Taesung Kwon, and Jong Chul Ye. Diffusionclip: Text-guided diffusion models for robust image manipulation. In CVPR, pages 24262435, 2022. [15] Hengjia Li, Yang Liu, Linxuan Xia, Yuqi Lin, Tu Zheng, Zheng Yang, Wenxiao Wang, Xiaohui Zhong, Xiaobo Ren, and Xiaofei He. Few-shot hybrid domain adaptation of image generators. arXiv preprint arXiv:2310.19378, 2023. [16] Hengjia Li, Yang Liu, Yuqi Lin, Zhanwei Zhang, Yibo Zhao, Tu Zheng, Zheng Yang, Yuchun Jiang, Boxi Wu, Deng Cai, et al. Unihda: Towards universal hybrid domain adaptation of image generators. CoRR, 2024. [17] Hengjia Li, Yang Liu, Yibo Zhao, Haoran Cheng, Yang Yang, Linxuan Xia, Zekai Luo, Qibo Qiu, Boxi Wu, Tu Zheng, et al. Gca-3d: Towards generalized and consistent domain adaptation of 3d generators. arXiv preprint arXiv:2412.15491, 2024. [18] Zongjian Li, Zheyuan Liu, Qihui Zhang, Bin Lin, Shenghai Yuan, Zhiyuan Yan, Yang Ye, Wangbo Yu, Yuwei Niu, and Li Yuan. Uniworld-v2: Reinforce image editing with diffusion negative-aware finetuning and mllm implicit feedback. arXiv preprint arXiv:2510.16888, 2025. 11 [19] Bin Lin, Zongjian Li, Xinhua Cheng, Yuwei Niu, Yang Ye, Xianyi He, Shenghai Yuan, Wangbo Yu, Shaodong Wang, Yunyang Ge, et al. Uniworld: High-resolution semantic encoders for unified visual understanding and generation. arXiv preprint arXiv:2506.03147, 2025. [20] Yaron Lipman, Ricky TQ Chen, Heli Ben-Hamu, Maximilian Nickel, and Matt Le. Flow matching for generative modeling. arXiv preprint arXiv:2210.02747, 2022. [21] Jie Liu, Gongye Liu, Jiajun Liang, Yangguang Li, Jiaheng Liu, Xintao Wang, Pengfei Wan, Di Zhang, and Wanli Ouyang. Flow-grpo: Training flow matching models via online rl. arXiv preprint arXiv:2505.05470, 2025. [22] Xingchao Liu, Chengyue Gong, et al. Flow straight and fast: Learning to generate and transfer data with rectified flow. In The Eleventh International Conference on Learning Representations. [23] Pan Lu, Liang Qiu, Kai-Wei Chang, Ying Nian Wu, Song-Chun Zhu, Tanmay Rajpurohit, Peter Clark, and Ashwin Kalyan. Dynamic prompt learning via policy gradient for semi-structured mathematical reasoning. In The Eleventh International Conference on Learning Representations. [24] Chenlin Meng, Yutong He, Yang Song, Jiaming Song, Jiajun Wu, Jun-Yan Zhu, and Stefano Ermon. Sdedit: Guided image synthesis and editing with stochastic differential equations. arXiv preprint arXiv:2108.01073, 2021. [25] Chancharik Mitra, Brandon Huang, Trevor Darrell, and Roei Herzig. Compositional chain-of-thought prompting for large multimodal models. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 1442014431, 2024. [26] Yao Mu, Qinglong Zhang, Mengkang Hu, Wenhai Wang, Mingyu Ding, Jun Jin, Bin Wang, Jifeng Dai, Yu Qiao, and Ping Luo. Embodiedgpt: Vision-language pre-training via embodied chain of thought. Advances in Neural Information Processing Systems, 36:2508125094, 2023. [27] Long Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida, Carroll Wainwright, Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, et al. Training language models to follow instructions with human feedback. Advances in neural information processing systems, 35:2773027744, 2022. [28] Luozheng Qin, Jia Gong, Yuqing Sun, Tianjiao Li, Mengping Yang, Xiaomeng Yang, Chao Qu, Zhiyu Tan, and Hao Li. Uni-cot: Towards unified chain-of-thought reasoning across text and vision. arXiv preprint arXiv:2508.05606, 2025. [29] Zhihong Shao, Peiyi Wang, Qihao Zhu, Runxin Xu, Junxiao Song, Xiao Bi, Haowei Zhang, Mingchuan Zhang, YK Li, Yang Wu, et al. Deepseekmath: Pushing the limits of mathematical reasoning in open language models. arXiv preprint arXiv:2402.03300, 2024. [30] Xinlong Wang, Xiaosong Zhang, Zhengxiong Luo, Quan Sun, Yufeng Cui, Jinsheng Wang, Fan Zhang, Yueze Wang, Zhen Li, Qiying Yu, et al. Emu3: Next-token prediction is all you need. arXiv preprint arXiv:2409.18869, 2024. [31] Yibin Wang, Changhai Zhou, and Honghui Xu. Enhancing object coherence in layout-to-image synthesis. arXiv preprint arXiv:2311.10522, 2023. [32] Yibin Wang, Weizhong Zhang, and Cheng Jin. Magicface: Training-free universal-style human image customized synthesis. arXiv preprint arXiv:2408.07433, 2024. [33] Yibin Wang, Weizhong Zhang, Jianwei Zheng, and Cheng Jin. Primecomposer: Faster progressively combined diffusion for image composition with attention steering. In ACM MM, pages 1082410832, 2024. [34] Yibin Wang, Zhimin Li, Yuhang Zang, Yujie Zhou, Jiazi Bu, Chunyu Wang, Qinglin Lu, Cheng Jin, and Jiaqi Wang. Pref-grpo: Pairwise preference reward-based grpo for stable text-to-image reinforcement learning. arXiv preprint arXiv:2508.20751, 2025. [35] Yibin Wang, Weizhong Zhang, Honghui Xu, and Cheng Jin. Dreamtext: High fidelity scene text synthesis. In CVPR, pages 2855528563, 2025. [36] Hongyang Wei, Baixin Xu, Hongbo Liu, Cyrus Wu, Jie Liu, Yi Peng, Peiyu Wang, Zexiang Liu, Jingwen He, Yidan Xietian, et al. Skywork unipic 2.0: Building kontext model with online rl for unified multimodal model. arXiv preprint arXiv:2509.04548, 2025. 12 [37] Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Fei Xia, Ed Chi, Quoc Le, Denny Zhou, et al. Chain-of-thought prompting elicits reasoning in large language models. Advances in neural information processing systems, 35:2482424837, 2022. [38] Chenfei Wu, Jiahao Li, Jingren Zhou, Junyang Lin, Kaiyuan Gao, Kun Yan, Sheng-ming Yin, Shuai Bai, Xiao Xu, Yilei Chen, et al. Qwen-image technical report. arXiv preprint arXiv:2508.02324, 2025. [39] Chenfei Wu, Jiahao Li, Jingren Zhou, Junyang Lin, Kaiyuan Gao, Kun Yan, Sheng-ming Yin, Shuai Bai, Xiao Xu, Yilei Chen, et al. Qwen-image technical report. arXiv preprint arXiv:2508.02324, 2025. [40] Chenyuan Wu, Pengfei Zheng, Ruiran Yan, Shitao Xiao, Xin Luo, Yueze Wang, Wanli Li, Xiyan Jiang, Yexin Liu, Junjie Zhou, et al. Omnigen2: Exploration to advanced multimodal generation. arXiv preprint arXiv:2506.18871, 2025. [41] Yongliang Wu, Zonghui Li, Xinting Hu, Xinyu Ye, Xianfang Zeng, Gang Yu, Wenbo Zhu, Bernt Schiele, MingHsuan Yang, and Xu Yang. Kris-bench: Benchmarking next-level intelligent image editing models. arXiv preprint arXiv:2505.16707, 2025. [42] Yi Xin, Qi Qin, Siqi Luo, Kaiwen Zhu, Juncheng Yan, Yan Tai, Jiayi Lei, Yuewen Cao, Keqi Wang, Yibin Wang, et al. Lumina-dimoo: An omni diffusion large language model for multi-modal generation and understanding. arXiv preprint arXiv:2510.06308, 2025. [43] Guowei Xu, Peng Jin, Ziang Wu, Hao Li, Yibing Song, Lichao Sun, and Li Yuan. Llava-cot: Let vision language models reason step-by-step. arXiv preprint arXiv:2411.10440, 2024. [44] Jiazheng Xu, Xiao Liu, Yuchen Wu, Yuxuan Tong, Qinkai Li, Ming Ding, Jie Tang, and Yuxiao Dong. Imagereward: Learning and evaluating human preferences for text-to-image generation. NeurIPS, 36:1590315935, 2023. [45] Jiazheng Xu, Yu Huang, Jiale Cheng, Yuanming Yang, Jiajun Xu, Yuan Wang, Wenbo Duan, Shen Yang, Qunlin Jin, Shurun Li, et al. Visionreward: Fine-grained multi-dimensional human preference learning for image and video generation. arXiv preprint arXiv:2412.21059, 2024. [46] Jin Xu, Zhifang Guo, Hangrui Hu, Yunfei Chu, Xiong Wang, Jinzheng He, Yuxuan Wang, Xian Shi, Ting He, Xinfa Zhu, et al. Qwen3-omni technical report. arXiv preprint arXiv:2509.17765, 2025. [47] Zeyue Xue, Jie Wu, Yu Gao, Fangyuan Kong, Lingting Zhu, Mengzhao Chen, Zhiheng Liu, Wei Liu, Qiushan Guo, Weilin Huang, et al. Dancegrpo: Unleashing grpo on visual generation. arXiv preprint arXiv:2505.07818, 2025. [48] Shuo Yang, Qihui Zhang, Yuyang Liu, Yue Huang, Xiaojun Jia, Kunpeng Ning, Jiayu Yao, Jigang Wang, Hailiang Dai, Yibing Song, et al. Asft: Anchoring safety during llm fine-tuning within narrow safety basin. arXiv preprint arXiv:2506.08473, 2025. [49] Zhuosheng Zhang, Aston Zhang, Mu Li, George Karypis, Alex Smola, et al. Multimodal chain-of-thought reasoning in language models. Transactions on Machine Learning Research. [50] Xiangyu Zhao, Peiyuan Zhang, Kexian Tang, Xiaorong Zhu, Hao Li, Wenhao Chai, Zicheng Zhang, Renqiu Xia, Guangtao Zhai, Junchi Yan, et al. Envisioning beyond the pixels: Benchmarking reasoning-informed visual editing. arXiv preprint arXiv:2504.02826, 2025. [51] Ge Zheng, Bin Yang, Jiajin Tang, Hong-Yu Zhou, and Sibei Yang. Ddcot: Duty-distinct chain-of-thought prompting for multimodal reasoning in language models. Advances in Neural Information Processing Systems, 36:51685191, 2023."
        }
    ],
    "affiliations": [
        "Intelligent Creation, ByteDance",
        "Zhejiang University"
    ]
}