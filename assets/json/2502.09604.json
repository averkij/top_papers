{
    "paper_title": "SelfCite: Self-Supervised Alignment for Context Attribution in Large Language Models",
    "authors": [
        "Yung-Sung Chuang",
        "Benjamin Cohen-Wang",
        "Shannon Zejiang Shen",
        "Zhaofeng Wu",
        "Hu Xu",
        "Xi Victoria Lin",
        "James Glass",
        "Shang-Wen Li",
        "Wen-tau Yih"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "We introduce SelfCite, a novel self-supervised approach that aligns LLMs to generate high-quality, fine-grained, sentence-level citations for the statements in their generated responses. Instead of only relying on costly and labor-intensive annotations, SelfCite leverages a reward signal provided by the LLM itself through context ablation: If a citation is necessary, removing the cited text from the context should prevent the same response; if sufficient, retaining the cited text alone should preserve the same response. This reward can guide the inference-time best-of-N sampling strategy to improve citation quality significantly, as well as be used in preference optimization to directly fine-tune the models for generating better citations. The effectiveness of SelfCite is demonstrated by increasing citation F1 up to 5.3 points on the LongBench-Cite benchmark across five long-form question answering tasks."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 3 1 ] . [ 1 4 0 6 9 0 . 2 0 5 2 : r SelfCite: Self-Supervised Alignment for Context Attribution in Large Language Models Yung-Sung Chuang2, Benjamin Cohen-Wang2, Shannon Zejiang Shen2, Zhaofeng Wu2, Hu Xu1, Xi Victoria Lin1, James Glass2, Shang-Wen Li1, Wen-tau Yih1 1Meta FAIR, 2Massachusetts Institute of Technology We introduce SelfCite, novel self-supervised approach that aligns LLMs to generate high-quality, fine-grained, sentence-level citations for the statements in their generated responses. Instead of only relying on costly and labor-intensive annotations, SelfCite leverages reward signal provided by the LLM itself through context ablation: If citation is necessary, removing the cited text from the context should prevent the same response; if sufficient, retaining the cited text alone should preserve the same response. This reward can guide the inference-time best-of-N sampling strategy to improve citation quality significantly, as well as be used in preference optimization to directly fine-tune the models for generating better citations. The effectiveness of SelfCite is demonstrated by increasing citation F1 up to 5.3 points on the LongBench-Cite benchmark across five long-form question answering tasks. Date: February 14, 2025 Correspondence: Yung-Sung Chuang yungsung@mit.edu, Shang-Wen Li shangwel@meta.com, Scott Yih scottyih@meta.com"
        },
        {
            "title": "1 Introduction",
            "content": "Assistants built using large language models (LLMs) have become ubiquitous in helping users gather information and acquire knowledge (OpenAI, 2022, 2023). For instance, when asked about recent news, an assistant can read through dozens of relevant articlespotentially more than user could comb through themselvesand use these articles as context to provide clear, specific answer to the users query. While this ability can greatly accelerate information gathering, LLMs often produce hallucinationscontent that sounds plausible but is actually fabricated (Ji et al., 2023). Even when provided with accurate context, models may misinterpret the data or include details that are not supported by the context (Shi et al., 2024; Chuang et al., 2024). Although completely eliminating hallucinations remains difficult, existing approaches have sought to enhance the reliability of LLMs by providing context attributionscommonly referred to as citationswhich are fine-grained references to relevant evidences from the context, alongside generated responses for user verification (Menick et al., 2022; Slobodkin et al., 2024; Zhang et al., 2024). While they have shown promise in generating citations, an outstanding challenge is their reliance on annotated data either from human (Menick et al., 2022; Slobodkin et al., 2024) or costly proprietary APIs (Zhang et al., 2024; Huang et al., 2024) to train models to generate citations. Collecting annotations can be time-consuming and costly, especially with long-context documents. To address this challenge, we introduce SelfCite, novel alignment approach designed to autonomously enhance the quality of citations generated by LLMs without the need for any annotations in the alignment process. Drawing inspiration from model interpretability techniques (Lei et al., 2016; Cohen-Wang et al., 2024), SelfCite leverages the inherent capabilities of LLMs to provide feedback through context ablationa process to evaluate the necessity and sufficiency of citation. If removing the cited text prevents the LLM from assigning high probability to the same response, we can infer that it is necessary for the LLM. Conversely, if the response remains highly probable despite removing all context other than the cited text, this indicates that the citation is sufficient for the LLM to make the claim. This self-evaluation mechanism enables SelfCite to calculate reward signal without relying on the annotation processes. Building on this intuition, we design reward that can be cheaply computed by the LLM itself, composed by 1 Figure 1 The SelfCite framework calculates rewards based on two metrics: necessity score (probability drop) and sufficiency score (probability hold). First, the full context is used to generate response. Then, the framework evaluates the probability of generating the same response after (1) removing the cited sentences from the context and (2) using only the cited sentences in the context. The probability drop and hold are computed from these probability differences, and their sum is used as the final reward. probability drop and probability hold in context ablation. By integrating this reward function into best-of-N sampling strategy, SelfCite achieves substantial improvements in citation quality. Furthermore, we employ this reward for preference optimization using SimPO (Meng et al., 2024), which not only maintains these improvements but also eliminates the need for best-of-N sampling. We outperform the previous state of the art on the LongBench-Cite benchmark (Zhang et al., 2024) by up to 5.3 points in F1 scores, and showing promising direction to bootstrap the citation quality from LLMs via self-rewarding."
        },
        {
            "title": "2 Method",
            "content": "In this section, we describe the SelfCite framework. We begin by introducing the task of generating responses with context attributions (2.1), referred to as citations for brevity. We then design reward for providing feedback on citation quality without human annotations (2.2) as illustrated in Fig. 1. Finally, we discuss two approaches for utilizing this reward to improve citation quality: best-of-N sampling (2.3) and preference optimization (2.4)."
        },
        {
            "title": "2.1 Problem Formulation",
            "content": "We first formalize the task of generating responses with context attributions and the metrics to self-evaluate context attributions within the SelfCite framework, inspired by previous papers (Zhang et al., 2024; CohenWang et al., 2024) but adapted to our proposed self-supervised reward. Setup. Consider employing an autoregressive language model (LM) to generate response to specific query given context of relevant information. Specifically, given an LM pLM, let pLM(ti t1, . . . , ti1) denote its output distribution over the next token ti based on sequence of preceding tokens t1, . . . , ti1. Next, let represent the context of relevant information. This context is partitioned into sentences: c1, c2, . . . , cC. Each sentence cj is prepended with unique identifier (e.g. sentence index j) as way for the model to reference the sentence when generating citations. The context is followed by query Q, question or instruction for the model. response is then sampled from the LM pLM. In SelfCite, following prior work on generating responses Generating Responses with Context Attributions. with context attributions (Zhang et al., 2024), each statement ri in the response is followed by citation sequence ei consisting of the identifiers of sentences from the context C. Thus, the entire response sequence is {r1, e1, r2, e2, . . . , rS, eS}, where is the total number of generated statements. The citation ei is intended to reference sentences that support the generation of ri. Formally, for each response statement ri, the model {1, 2, . . . , C} corresponds to specific outputs citation sequence ei = {e1 }, where each ej , . . . , em , 2 sentence number in the context C, and sentences are cited. Note that this citation sequence may be empty. The entire response consisting of statements ri followed by citations ei is sampled from the LM pLM as follows: ri pLM ei pLM (cid:0) c1, . . . , cC, Q, r1, e1, . . . , ri1, ei1 (cid:1) , (cid:0) c1, . . . , cC, Q, r1, e1, . . . , ri1, ei1, ri (cid:1) . The objective of optimizing the LM is to ensure that the citation sequence ei accurately reflects the evidence from the context that supports the generation of ri. In the SFT setting (Zhang et al., 2024), the probability of ground truth annotated responses and citations {ˆr1, ˆe1, ..., ˆrS, ˆeS} will be maximized, given the input and Q, but it is not trivial to do further alignment with feedback after the SFT data is used up. To achieve this, we introduce SelfCite that can evaluate the quality of these citations based on context ablation as reward for further preference optimization."
        },
        {
            "title": "2.2 Self-Supervised Reward via Context Ablation\nWe measure the quality of a citation sequence ei by the changes in the LM’s probability of generating ri\nwhen the cited sentences are either removed from or isolated within the context. To simplify the notation, let\nall the cited context sentences be Ei = {ce1\n}. We define two key metrics: necessity score and\nsufficiency score, and finally combine them into the final reward, as shown in Fig. 1.",
            "content": ", . . . , cem , ce2 Necessity Score: Probability Drop. This metric quantifies the decrease in the probability of generating ri when the cited sentences in Ei are all removed from the context (denoted as set minus operator). Formally, it is defined as: Prob-Drop(ei) = log pLM(ri C) log pLM (ri Ei) . To keep the equation concise, we ignore and {r1, e1, ..., ri1, ei1} in the equation, but they are staying in the context history when computing the probabilities. larger probability drop indicates that the removal of Ei significantly diminishes the likelihood of generating ri, thereby validating the necessity of the cited evidence. Sufficiency Score: Probability Hold. Conversely, this metric measures if the probability of generating ri is still kept large when only the cited sentences are kept in the context, effectively testing the sufficiency of the citation to support the response statement. Formally: Prob-Hold(ei) = log pLM (ri Ei) log pLM(ri C). more positive value of probability hold indicates that the cited sentences alone are sufficient to support the generation of ri, while removing all the other irrelevant context. Please note that the values of probability drop or hold can be either positive or negative. For example, if the citation is not relevant to ri or even distracting, it is possible for p(ri Ei) to be lower than p(ri C). Final Reward. To comprehensively evaluate the necessity and sufficiency of the generated citations, we add the two metrics together, where the opposing terms cancel out: Reward(ei) = log pLM (riEi) log pLM (riC Ei) . (1) The combined reward measures if the citations are both necessary and sufficient for generating the response ri."
        },
        {
            "title": "2.3 Best-of-N Sampling\nTo leverage the self-supervised reward computed via context ablation, we employ a best-of-N sampling strategy,\nwhich is a common way to test the effectiveness of a reward design (Gao et al., 2023a; Lightman et al., 2024)\nas a performance oracle without any confounders from training. After generating the full response, we locate\nthe position where the citation tags <cite>...</cite> are generated. Within the citation tags, we sample N\ncandidate citation sequences and select the citation set that maximizes the combined reward metric, Eq. (1).",
            "content": "3 Algorithm 1 SelfCite Best-of-N Sampling for Citations Require: LM pLM, context C, query Q, response R, number of candidates for ri do for = 1, . . . , do e(k) pLM( ri, C, Q) Reward(e(k) ) = log pLM (cid:0)ri E(n) (cid:1) log pLM (cid:0)ri E(n) (cid:1) end for = arg maxk reward(cid:0)e(k) end for return = {r1, 1, . . . , rS, (cid:1) S} The corresponding procedure is shown in Algorithm 1. After obtaining the selected citations {e S}, we for the response statement ri, while replace the original citation sequence ei with the optimal citation set keeping the response statements {r1, . . . , rS} unchanged. This process is repeated for each statement in the response to obtain the final, citation-improved output = {r1, S}. To prevent the model from citating too many sentences, we exclude the BoN candidates that cites more than 384 tokens in total, unless they are all from single sentence. 1, . . . , rS, 1, . . . , e"
        },
        {
            "title": "2.4 Preference Optimization",
            "content": "Best-of-N sampling is straightforward way to obtain better citations, but at the additional inference time of generating candidates and reranking. Thus, we try to internalize the ability of generating better citations back to the LM itself. Given documents and queries, we can prompt the LM to generate the responses along with the citations = {r1, e1, ..., rS, eS}. By further applying best-of-N sampling, we can obtain new responses of the same statements but with better citations = {r1, S}. Such preference data can be used in direct 1, ..., rS, preference optimization (DPO) (Rafailov et al., 2024) to align the model based on the preference between the original outputs and improved outputs. DPO typically requires more memory usage than SFT due to the need of reference model. Also, optimizing with preference data pairs inherently makes the per-GPU batch size to be at least 2, limiting the maximum context length that can be used. To address these challenges, we use SimPO (Meng et al., 2024), variant of DPO that does not require reference model, freeing up memory for long-context fine-tuning. We further apply Liger-Kernel (Hsu et al., 2024), collection of efficient Triton kernels, to optimize memory usage and scale the context length to 25.6K, as detailed in Appendix A. Through this self-supervised alignment process, which does not require ground-truth answers or human annotations, the model learns to generate more accurate and contextually grounded citations on its own."
        },
        {
            "title": "3 Experiments",
            "content": "We evaluate the effectiveness of SelfCite by applying the best-of-N sampling and preference optimization methods to existing models that generate responses with citations."
        },
        {
            "title": "3.1 Model Details",
            "content": "We use the Llama-3.1-8B model (Dubey et al., 2024) fine-tuned on LongCite-45K SFT data, namely the LongCite-8B model (Zhang et al., 2024) as the start point for both best-of-N sampling and preference optimization. We adopt the same text segmentation strategy from Zhang et al. (2024): each document is split into individual sentences using NLTK (Bird, 2006) and Chinese punctuations, and each sentence is prepended with unique identifier in <C{i}> format. These identifiers serve as the citation indices, enabling the model to cite relevant context right after the statements with the format of <statement> {content ...} <cite>[i1 i2][i3 i4]...</cite></statement>. This format allows the model to cite single sentence (e.g. i1 = i2) or span (e.g. i1 < i2) efficiently within several tokens. The responses are generated via top-p sampling (Holtzman et al., 2020) with p=0.7 and temperature=0.95. We set p=0.9 and temperature=1.2 4 when doing best-of-N sampling for the citation strings to increase the diversity. We set N=10 in all the experiments considering the limited diversity in citations."
        },
        {
            "title": "3.2 Preference Optimization\nLongCite-45K. Best-of-N sampling (Section 2.3) requires no training, so no training data is used. For\npreference optimization with SimPO (Section 2.4), we use 2K document–question pairs from LongCite-\n45K (Zhang et al., 2024) as the training set but we do not use its ground-truth responses with high-quality\ncitations for SFT. Instead, we generate model responses from the documents and queries, then apply best-of-N\nto refine citations. We label the original responses as rejected and replace their citations with BoN-refined\nones to create the chosen responses, forming preference pairs to build the dataset for SimPO.",
            "content": "Data Construction and Length Balancing Since best-of-N responses tend to have slightly longer citations, directly fine-tuning on them can lead the model to adopt shortcutgenerating longer citations instead of improving citation quality. To prevent this, we introduce length balancing: if an original response has shorter citation length than the best-of-N response, we insert random citations from nearby sentences. This encourages the model to focus on where to cite rather than simply citing more. Details are provided in Appendix C, with an ablation study in Section 4.2."
        },
        {
            "title": "3.3 Evaluation\nBenchmark. We evaluate our approach on LongBench-Cite (Zhang et al., 2024), a comprehensive benchmark\nspecifically designed for long-context QA with citations (LQAC). Given a long context C and a query Q, the\nmodel must produce a multi-statement answer with each statement cites relevant supporting sentences in C.\nUnlike chunk-level citation schemes (Gao et al., 2023b) which cites short paragraphs, LongBench-Cite adopts\nsentence-level citations to ensure semantic integrity and finer-grained evidence tracking. LongBench-Cite\nassesses two main aspects:",
            "content": "Citation Quality: Whether each statement is fully supported by relevant and only relevant sentences. GPT-4o measures citation recall (extent to which statement is fully or partially supported by the cited text) and citation precision (whether each cited text truly supports the statement). These are combined into citation F1 score. Additionally, we track average citation length (tokens per citation) to promote fine-grained citations over unnecessarily long passages. Correctness: How accurately and comprehensively the response answers the query disregarding the citations. This is scored by GPT-4o in zero-/few-shot fashion based on the query and reference answers. The benchmark contains five datasets, including single-doc QA MultiFieldQA-en/zh (Bai et al., 2023), multi-doc QA HotpotQA (Yang et al., 2018) and DuReader (He et al., 2018), one summarization dataset GovReport (Huang et al., 2021), and LongBench-Chat (Bai et al., 2024) which covers diverse real-world queries with long contexts such as document QA, summarization, and coding. Baselines. SelfCite is compared with these baselines. Prompting: Zhang et al. (2024) propose the baseline of prompting LLMs with an one-shot example. This can be applied to proprietary models including GPT-4o (OpenAI, 2023), Claude-3-sonnet (Anthropic, 2024), and GLM-4 (GLM et al., 2024), as well as open-source models including GLM-4-9B-chat (GLM et al., 2024), Llama-3.1-{8,70}B-Instruct (Dubey et al., 2024), and Mistral-Large-Instruct (Mistral, 2024). Contributive context attribution: Contributive context attribution seeks to directly identify the parts of the context that cause the model to generate particular statement. We consider ContextCite (Cohen-Wang et al., 2024), contributive context attribution method that performs several random context ablations to model the effect of ablating different parts of the context on generated statement. 1After deduplicating repeated citation candidates, on average there are only 4.8 candidates left per statement in the BoN experiment on LongBench-Cite, with standard deviation of 3.2. 5 Table 1 Citation recall (R), citation precision (P), citation F1 (F1), and citation length evaluated on LongBench-Cite benchmark. The best of our results are bolded. The best of previous state of the art are underlined. indicates the results taken from Zhang et al. (2024). Our repro. means our reproduced results. Model Proprietary models GPT-4o Claude-3-sonnet GLM-4 Open-source models GLM-4-9B-chat Llama-3.1-8B-Instruct Llama-3.1-70B-Instruct Mistral-Large-Instruct Longbench-Chat MultifieldQA F1 F1 R HotpotQA F1 Dureader F1 GovReport F Avg. Citation Length F1 46.7 52.0 47.6 25.9 14.1 25.8 19.8 53.5 67.8 53.9 20.5 19.5 32.0 23.9 46.7 55.1 47. 16.7 12.4 23.2 19.0 79.0 64.7 72.3 51.1 29.8 53.2 71.8 87.9 85.8 80.1 60.6 44.3 65.2 80.7 80.6 71.3 73. 52.0 31.6 53.9 73.8 55.7 46.4 47.0 22.9 20.2 29.6 34.5 62.3 65.8 50.1 28.8 30.9 37.3 40.9 53.4 49.9 44. 20.1 20.9 28.6 32.1 65.6 67.7 73.4 45.4 22.0 38.2 58.3 74.2 89.2 82.3 48.3 25.1 46.0 67.0 67.4 75.5 75. 40.9 17.0 35.4 60.1 73.4 77.4 82.8 90.4 93.9 93.4 79.8 84.1 87.1 5.7 16.2 53.4 67.9 8.2 25.3 77.5 79. 6.3 16.8 60.7 72.5 65.6 67.2 65.4 27.2 19.7 40.4 51.5 220 132 169 96 100 174 132 Contributive context attribution (with Llama-3.1-8B-Instruct) ContextCite (32 calls) ContextCite (256 calls) 56.7 63.5 76.8 83.1 58.0 64.7 76.1 78.8 87.2 89. 78.9 81.8 40.5 46.5 54.7 60.8 43.9 49.2 58.0 61.7 82.4 89. 65.0 70.1 67.1 69.1 88.8 93.5 75.6 78.8 64.3 68.9 92.7 100. Fine-tuned models LongCite-9B LongCite-8B Ours: SelfCite 57.6 62.0 78.1 79.7 63.6 67. 67.3 74.7 91.0 93.0 74.8 80.8 61.8 59.2 78.8 72.1 64.8 60. 67.6 68.3 89.2 85.6 74.4 73.1 63.4 74.0 76.5 86.6 68.2 78. 69.2 72.0 91 85 LongCite-8B (Our repro.) + BoN + SimPO + SimPO then BoN 67.0 68.4 68.1 73.3 78.1 81.3 79.5 79.4 66.6 71.2 69.1 72. 74.8 76.1 75.5 76.7 Llama-3.1-8B-Instruct (fully self-supervised setting) + SFT on ContextCite + BoN + SimPO + SimPO then BoN 52.3 54.8 63.3 66.0 70.6 67.6 74.3 82.4 56.5 58.1 64.6 71. 79.1 80.4 80.2 81.5 90.7 92.8 92.6 93.2 90.5 90.5 88.9 90.7 79.9 81.2 81.0 82.2 82.0 83.0 82.4 83.2 77.9 60.8 81.0 67.2 69.4 82.3 69.4 83. 64.1 68.8 71.5 71.1 73.7 87.2 67.1 76.9 90.9 70.6 72.7 78.9 91.6 74.2 92.2 80.3 81.6 87.6 86.4 86.7 54.5 58.3 59.7 61.3 72.3 70.0 76.9 70.0 56.3 57.5 61.0 59. 54.9 57.6 59.0 62.1 79.0 79.0 80.9 81.4 61.6 63.1 65.4 67.4 63.7 67.2 68.5 68.8 89.3 92.4 92.9 92.7 84.9 84.8 86.6 86. 84.5 89.3 89.1 89.2 72.3 74.6 76.1 76.1 73.8 77.5 77.9 79.1 65.7 67.3 69.9 71.5 83.5 93.4 105.7 94.7 83.0 80.4 90.2 87. We apply ContextCite with 32 and 256 times of random context ablations, with the details described in Appendix B. Fine-tuned models: LongCite-8B and 9B released by Zhang et al. (2024), trained on LongCite-45K, fine-tuned from Llama-3.1-8B (Dubey et al., 2024) and GLM-4-9B (GLM et al., 2024), respectively."
        },
        {
            "title": "3.4 Main Results\nCitation Quality. Table 1 presents our main results. Our best-of-N sampling (BoN) consistently improves\nboth citation recall and citation precision across tasks, increasing the overall F1 score from 73.8 to 77.5. Using\nSimPO to internalize BoN’s gains—eliminating the need for BoN sampling—achieves a similar improvement,\nwith an F1 of 77.9. Applying BoN again to the SimPO fine-tuned model further boosts F1 by 5.3 points to\n79.1, the highest across the datasets, suggesting room for further gains. Our results surpass LongCite-8B/9B\nat similar citation lengths and outperform proprietary model prompting while producing shorter citations.\nCompared to ContextCite, which also relies on context ablation, our approach is significantly better. A key\nreason is that ContextCite estimates sentence importance from scratch using linear regression, while we rerank\nexisting LLM-generated citation candidates, leading to more efficient and accurate citation quality estimation.\nAdditionally, we evaluate the latest released Claude Citations API, as shown in Appendix D that SelfCite\nachieves strong results very close to this commercial-level API that specialized for providing citations.",
            "content": "In our main experiment, we start from the Llama-3.1-8B model fine-tuned Fully Self-Supervised Setting. on the LongCite-45K SFT data, which effectively kick-starts its ability to generate structured citations for best-of-N sampling. The subsequent SimPO alignment stage is entirely self-supervised. We are also curious if it is possible to start from fully self-supervised SFT model and then apply our self-supervised alignment after that. To begin with, we automatically generate 11K citation SFT data using ContextCite (see Appendix for details) to replace the LongCite-45K annotations in the training data, as shown in the results at the bottom of Table 1. We can see that SFT on ContextCite can achieve decent initial results (65.7 F1) but still far from LongCite-8B (73.8 F1). BoN helps improving F1 to 67.3. After SimPO training, it achieves 69.9 F1, 6 Table 2 Answer correctness when responding with or without citations. indicates results taken from Zhang et al. (2024). The header contains abbreviations for the same five datasets in Table 1."
        },
        {
            "title": "Model",
            "content": "Long. Multi. Hot. Dur. Gov. Avg"
        },
        {
            "title": "Answering without citations",
            "content": "LongSFT-8B LongSFT-9B Llama-3.1-8B-Instruct"
        },
        {
            "title": "Answering with citations",
            "content": "LongCite-8B (Our repro.) + SimPO Llama-3.1-8B-Instruct + SFT on ContextCite + SimPO 68.6 64.6 61.6 67.6 67.4 67.4 58.8 56. 83.6 83.3 73.3 86.7 86.7 87.9 83.4 80.9 69.0 67.5 64.5 69.3 67.5 73.5 65.8 65. 62.3 66.3 39.4 64.0 66.0 67.8 57.8 59.5 54.4 46.4 62.1 60.4 61.3 62.1 57.5 60. 67.6 65.6 60.2 69.6 69.8 71.7 64.6 64.7 and additionally applying BoN can boost its F1 by 5.8 to 71.5, significantly closing the gap to LongCite-8B, showing our alignment method not only improve the supervised models, but also enhance the models purely trained from self-supervision. Answer Correctness. For best-of-N sampling, only the citation parts are modified, so the responses it generates to answer the questions are the same as those of the original LongCite-8B model, maintaining the same correctness. For the SimPO finetuned models, we test their answer correctness by the evaluation in Zhang et al. (2024), which contains two settings: answering with/without citations. If answering with citations, the citation parts will be removed when evaluating the answer correctness. The results in Table 2 show that the SimPO finetuning does not change the correctness of the LongCite-8B model much. The correctness is similar to LongSFT-8B/9B (Zhang et al., 2024), which are ablation baselines finetuned on LongCite-45k QA pairs but without the citation parts. When finetuning Llama-3.1-8B-Instruct with ContextCite SFT data, the answer correctness slightly degrades. It is probably caused by the fact that we do not mix any instruction following dataset in the SFT stage as Zhang et al. (2024) does. However, the further SimPO step does not change the answer correctness significantly, while lead to better citations for user verification. Since the SFT stage is not our main method here, we leave the better data mixture strategy in SFT stage in the future work."
        },
        {
            "title": "4.1 Ablation Study on Rewards",
            "content": "To better understand our final reward design, we explore various reward strategies in the BoN sampling process. Here, all BoN candidates are pre-generated and fixed, the reward is the only factor affecting results. Table 3 presents our ablation results on HotpotQA, while citation lengths are computed across all LongBench-Cite datasets for direct comparison with Table 1. We evaluate four alternative reward designs. BoN by LM log prob re-ranks candidates simply by the probability of the citation string, <cite>[i1 i2][i3 i4]...</cite>, which is similar to the effects of beam search. We observe that this strategy slightly boosts recall while reducing precision, resulting in minor reduction in F1. BoN by max citation length always selects the candidates with the longest citations, i.e. citing the greatest number of sentences. Although it improves recall, it significantly reduces precision from 77.9 to 73.6 and inflates the citation length from 83.5 to 139.8. By contrast, both BoN by Prob-Drop and BoN by Prob-Hold improve recall without sacrificing precision. Finally, by combining both Prob-Drop and Prob-Hold into our final SelfCite reward, we achieve the best outcome, increasing both recall and precision and 4-point improvement in F1. We also explored different token-length limits for citations in the bottom of Table 3, as discussed in Section 2.3. By default, we exclude candidates citing more than 384 tokens, unless the citation contains only single sentence. Lowering the cap to 256 tokens slightly hurts F1, while raising it to 512 tokens has negligible impact. Completely removing length limits inflates citation length to 121.9 tokens and yields worse precision (79.3) 7 Table 3 Ablation study on HotpotQA citation recall, precision, and F1 (R, P, F1) and citation length for BoN decoding methods."
        },
        {
            "title": "HotpotQA\nP",
            "content": "R F1 LongCite-8B (Our repro.) 60.8 77.9 64. + BoN by LM log prob + BoN by max citation length + BoN by Prob-Drop + BoN by Prob-Hold + BoN by SelfCite w/ lower length limit (256) w/ higher length limit (512) w/o length limit () 62.7 66.5 65.6 66.2 67.2 65.8 67.0 67. 75.5 73.6 78.1 78.1 81.0 78.8 82.2 79.3 63.4 65.1 66.6 67.0 68.8 66.4 68.5 68."
        },
        {
            "title": "Citation\nLength",
            "content": "83.5 74.6 139.8 92.9 93.4 93.4 84.5 99.2 121.9 Table 4 Ablation study on HotpotQA citation recall, precision, and F1 (R, P, F1) and citation length for finetuned models. Fine-tuning Methods"
        },
        {
            "title": "HotpotQA\nP",
            "content": "R F1 LongCite-8B (Our repro.) 60.8 77.9 64. + SimPO + SimPO + BoN 69.4 72.0 82.3 82.7 71.5 72.9 + SimPO w/ or w/o length balancing"
        },
        {
            "title": "Citation\nLength",
            "content": "83.5 105.7 126.9 w/ length balancing w/o length balancing 69.4 64.4 82.3 62.9 71.5 60. 105.7 152.9 + SimPO w/ varying data sizes 1K examples 2K examples 4K examples 8K examples 62.5 69.4 68.5 63.6 78.9 82.3 80.4 75.3 65.7 71.5 70.3 64. + SFT on BoN responses 68.8 77.3 68.4 + SimPO by denoising perturbed citations 90.1 105.7 134.1 195. 98."
        },
        {
            "title": "On original responses\nOn BoN responses",
            "content": "40.5 42.6 50.5 50.7 41.6 42.3 88.8 79.7 but slightly improved recall (67.9). We also notice that the 256 length limit still outperforms the LongCite-8B baseline (66.4 vs 64.1) while having almost equally long citation length (84.5 vs 83.5), showing that the improvement of SelfCite correlates less with the citation length. Overall, using 384-token limit achieves good balance for short citation lengths and strong performance."
        },
        {
            "title": "4.2 Citation Length Balance",
            "content": "As noted in Section 3.2, BoN selects slightly longer citations, making it easy for model trained directly on BoN-preferred data to adopt the shortcut of generating longer citations without improving quality. To counter this, we apply length balancing, injecting random citations into examples where length bias exists to equalize the number of cited sentences. Table 4 (see w/ vs. w/o length balancing) highlights its critical role in length balancing. Without length balancing, the model overextends citations (average length 152.9), leading to lower precision (62.9) and F1 (60.5). In contrast, enabling length balancing maintains high precision (82.3) and recall (69.4), achieving better F1 of 71.5 while keeping citation length reasonable (105.7). These results confirm that length balancing prevents shortcut learning, ensuring the model truly learns to cite accurately."
        },
        {
            "title": "4.3 Training Size of SimPO",
            "content": "In prior study (Zhou et al., 2023), 1K examples are sufficient to align user preferences effectively. Table 4 presents SimPO results with 1K to 8K examples. 1K examples already bring moderate improvement, raising F1 from 64.1 to 65.7, with gains in precision and recall. Using 2K examples further boosts F1 to 71.5, while 8 Figure 2 Iteratively applying SimPO for three iterations. 4K leads to saturated improvement. However, at 8K examples, performance declines, and citation length rises to 195.2. We attribute this to SimPOs off-policy nature, especially because it lacks reference model to constrain the output distributions to be similar to the collected data. As training steps grow, the model may drift from the collected data, potential overfitting to the biases in preference data. Thus, further fine-tuning may degrade citation quality. To address this, we show initial results from iterative SimPO in Section 4.6."
        },
        {
            "title": "4.4 SimPO vs. SFT on Best-of-N responses",
            "content": "We also show the effect of applying standard supervised fine-tuning (SFT) on the responses selected by best-of-N sampling, which is simplified alternative of preference optimization. As the result shown in the last row in Table 4, SFT also improves the F1 score from 64.1 to 68.4, but it still falls behind 71.5 of SimPO. This result confirms that it is necessary to train the model via SimPO with preference data, which enables the model to distinguish between bad and good citations, and thus improve the citation quality."
        },
        {
            "title": "4.7 Qualitative Study",
            "content": "Finally, we examine an example that requires citing multiple context sentences to support complex response. As shown in Table 5, the response integrates information from sentences 302, 303, and 306. Direct sampling (2) 9 Table 5 An example of differences in the citation from baseline vs BoN. Related information are highlighted in the context/response. Sent. ID 302 () 303 () 304 () 305 () 306 () Context Sentences (only showing paragraph due to limited space) In general, consumer advocates believe that any comprehensive federal privacy policy should complement, and not supplant, sector-specific privacy legislation or state-level legislation. Finding global consensus on how to balance open data flows and privacy protection may be key to maintaining trust in the digital environment and advancing international trade. One study found that over 120 countries have laws related to personal data protection. Divergent national privacy approaches raise the costs of doing business and make it harder for governments to collaborate and share data, whether for scientific research, defense, or law enforcement. system for global interoperability in least trade-restrictive and nondiscriminatory way between different national systems could help minimize costs and allow entities in different jurisdictions with varying online privacy regimes to share data via cross-border data flows."
        },
        {
            "title": "Query",
            "content": "Please write one-page summary of the above government report. Response (only single statement due space) to [...] The report concludes by noting that finding global consensus on how to balance open data flows and privacy protection may be key to maintaining trust in the digital environment and advancing international trade. The report suggests that Congress may consider comprehensive privacy legislation and examine the potential challenges and implications of building system of interoperability between different national privacy regimes. [...]"
        },
        {
            "title": "BoN Candidates",
            "content": "Citation Strings (green: correct; red: wrong)"
        },
        {
            "title": "SelfCite Reward",
            "content": "(1) Best candidate (2) Direct sampling (3) Other candidate (4) Other candidate [302-303][306-306] [303-303][305-306] [303-304][308-308][310-311] [303-303][309-309][311-311] (302) (302, 306) (302, 306) 0.578 0.547 0.461 0.375 omits sentence 302 while incorrectly including 305. In contrast, the best-of-N candidate (1) correctly includes 302 and excludes 305, achieving slightly higher reward (0.578 vs. 0.547), demonstrating the effectiveness of our reward design. We also present candidates (3) and (4), which cite more irrelevant sentences and miss key citations, leading to even lower rewards. Additional qualitative examples are provided in Appendix E."
        },
        {
            "title": "5 Related Work",
            "content": "Citations for Language Models. Recent work has explored various approaches to teaching language models to generate citations, including fine-tuning with direct human feedback or annotations (Nakano et al., 2021; Menick et al., 2022; Slobodkin et al., 2024), annotation/feedback from external models (Huang et al., 2024), and prompting-based methods (Gao et al., 2022, 2023b) to explicitly incorporate relevant retrieved documents. To avoid the human annotation processes, Zhang et al. (2024) introduced CoF (Coarse to Fine), an automated multi-stage pipeline that simulates human annotations. This approach leverages proprietary LLMs for chunk-level retrieval and sentence-level citation extraction, achieving high citation quality through supervised fine-tuning. However, it depends on two powerful proprietary APIsGLM-4 for the LLM and Zhipu Embedding-v2 for retrieval2with carefully designed prompting, effectively distilling the capabilities of these powerful proprietary APIs into much smaller models in 8B/9B. In contrast, our SelfCite aims at completely eliminating the reliance on annotations for citation, either from human or proprietary APIs. Instead, our method enables small 8B model to assess citation quality itself using self-supervised reward signal from context ablation, effectively self-improving without external supervision. Contributive Context Attribution. Besides being self-supervised, SelfCite also adopts the view that citations should reference the sources from the context that model actually uses when generating statementknown as contributive attribution (Worledge et al., 2023)rather than any sources that merely support the claim. Our reward signal naturally aligns with this attribution framework, as context ablation identifies the sources that 2https://open.bigmodel.cn/pricing 10 cause the model to produce statement. Existing contributive attribution methods for LLMs typically require extensive context ablations or other computationally expensive techniques, such as gradient-based analysis during inference (Cohen-Wang et al., 2024; Qi et al., 2024; Phukan et al., 2024). In contrast, SelfCite simply generate the citation tags, and refine citation candidates by preference optimization with reward signals from context ablations, effectively teaching the model to perform contributive context attribution itself. Selfor Weakly-Supervised Alignment. Another relevant area is selfor weakly-supervised approaches for aligning LLMs without human supervision (Kim et al., 2023; Yuan et al., 2024), reducing the need for explicit human feedback (Ouyang et al., 2022), or curating high-quality data for supervised fine-tuning (Zhou et al., 2023). SelfCite shares the same spirit by computing simple probability differences under context ablation as rewards, eliminating the need for additional annotation process."
        },
        {
            "title": "6 Conclusion and Limitations",
            "content": "In this work, we introduced SelfCite, self-supervised framework that aligns LLMs to generate more accurate, fine-grained citations by directly leveraging their own probabilities for necessity and sufficiency rewards through context ablation. With best-of-N sampling and preference optimization, SelfCite significantly improves citation correctness on the LongBench-Cite benchmark without requiring human annotation, offering promising self-improving direction towards verifiable and trustworthy LLMs. SelfCite also has limitations: 1) While achieving good results with SimPO, integrating other alignment algorithms remains unexplored. 2) While we focus on self-supervision in preference optimization, our attempt to make the SFT stage self-supervised via ContextCite is still preliminary. Better unsupervised ways to kick-start LLMs ability in generating structured citations can be further explored."
        },
        {
            "title": "Acknowledgements",
            "content": "We thank Jiajie Zhang and Yushi Bai for their help in providing implementation details of LongCite. We also thank Andrei Barbu, Linlu Qiu, Nour Jedidi, Weijia Shi, and Tianyu Gao for their valuable discussions. At MIT, Yung-Sung was sponsored by the United States Air Force Research Laboratory and the United States Air Force Artificial Intelligence Accelerator and was accomplished under Cooperative Agreement Number FA8750-19-2-1000. The views and conclusions contained in this document are those of the authors and should not be interpreted as representing the official policies, either expressed or implied, of the United States Air Force or the U.S. Government. The U.S. Government is authorized to reproduce and distribute reprints for Government purposes notwithstanding any copyright notation herein."
        },
        {
            "title": "References",
            "content": "Anthropic. Anthropic: Introducing claude 3.5 sonnet, 2024. https://www.anthropic.com/news/claude-3-5-sonnet. Yuntao Bai, Andy Jones, Kamal Ndousse, Amanda Askell, Anna Chen, Nova DasSarma, Dawn Drain, Stanislav Fort, Deep Ganguli, Tom Henighan, et al. Training helpful and harmless assistant with reinforcement learning from human feedback. arXiv preprint arXiv:2204.05862, 2022. Yushi Bai, Xin Lv, Jiajie Zhang, Hongchang Lyu, Jiankai Tang, Zhidian Huang, Zhengxiao Du, Xiao Liu, Aohan Zeng, Lei Hou, et al. Longbench: bilingual, multitask benchmark for long context understanding. arXiv preprint arXiv:2308.14508, 2023. Yushi Bai, Xin Lv, Jiajie Zhang, Yuze He, Ji Qi, Lei Hou, Jie Tang, Yuxiao Dong, and Juanzi Li. Longalign: recipe for long context alignment of large language models. arXiv preprint arXiv:2401.18058, 2024. Steven Bird. Nltk: the natural language toolkit. In Proceedings of the COLING/ACL 2006 Interactive Presentation Sessions, pages 6972, 2006. Yung-Sung Chuang, Linlu Qiu, Cheng-Yu Hsieh, Ranjay Krishna, Yoon Kim, and James Glass. Lookback lens: Detecting and mitigating contextual hallucinations in large language models using only attention maps. In Proceedings of the 2024 Conference on Empirical Methods in Natural Language Processing, pages 14191436, 2024. 11 Benjamin Cohen-Wang, Harshay Shah, Kristian Georgiev, and Aleksander Madry. Contextcite: Attributing model generation to context. In The Thirty-eighth Annual Conference on Neural Information Processing Systems, 2024. Abhimanyu Dubey, Abhinav Jauhri, Abhinav Pandey, Abhishek Kadian, Ahmad Al-Dahle, Aiesha Letman, Akhil Mathur, Alan Schelten, Amy Yang, Angela Fan, et al. The llama 3 herd of models. arXiv preprint arXiv:2407.21783, 2024. Leo Gao, John Schulman, and Jacob Hilton. Scaling laws for reward model overoptimization."
        },
        {
            "title": "In International",
            "content": "Conference on Machine Learning, pages 1083510866. PMLR, 2023a. Luyu Gao, Zhuyun Dai, Panupong Pasupat, Anthony Chen, Arun Tejasvi Chaganty, Yicheng Fan, Vincent Zhao, Ni Lao, Hongrae Lee, Da-Cheng Juan, et al. Rarr: Researching and revising what language models say, using language models. arXiv preprint arXiv:2210.08726, 2022. Tianyu Gao, Howard Yen, Jiatong Yu, and Danqi Chen. Enabling large language models to generate text with citations. In Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing, pages 64656488, 2023b. Team GLM, Aohan Zeng, Bin Xu, Bowen Wang, Chenhui Zhang, Da Yin, Diego Rojas, Guanyu Feng, Hanlin Zhao, Hanyu Lai, Hao Yu, Hongning Wang, Jiadai Sun, Jiajie Zhang, Jiale Cheng, Jiayi Gui, Jie Tang, Jing Zhang, Juanzi Li, Lei Zhao, Lindong Wu, Lucen Zhong, Mingdao Liu, Minlie Huang, Peng Zhang, Qinkai Zheng, Rui Lu, Shuaiqi Duan, Shudan Zhang, Shulin Cao, Shuxun Yang, Weng Lam Tam, Wenyi Zhao, Xiao Liu, Xiao Xia, Xiaohan Zhang, Xiaotao Gu, Xin Lv, Xinghan Liu, Xinyi Liu, Xinyue Yang, Xixuan Song, Xunkai Zhang, Yifan An, Yifan Xu, Yilin Niu, Yuantao Yang, Yueyan Li, Yushi Bai, Yuxiao Dong, Zehan Qi, Zhaoyu Wang, Zhen Yang, Zhengxiao Du, Zhenyu Hou, and Zihan Wang. Chatglm: family of large language models from glm-130b to glm-4 all tools, 2024. Wei He, Kai Liu, Jing Liu, Yajuan Lyu, Shiqi Zhao, Xinyan Xiao, Yuan Liu, Yizhong Wang, Hua Wu, Qiaoqiao She, et al. Dureader: chinese machine reading comprehension dataset from real-world applications. In Proceedings of the Workshop on Machine Reading for Question Answering, pages 3746, 2018. Ari Holtzman, Jan Buys, Li Du, Maxwell Forbes, and Yejin Choi. The curious case of neural text degeneration. In International Conference on Learning Representations, 2020. https://openreview.net/forum?id=rygGQyrFvH. Pin-Lun Hsu, Yun Dai, Vignesh Kothapalli, Qingquan Song, Shao Tang, Siyu Zhu, Steven Shimizu, Shivam Sahni, Haowen Ning, and Yanning Chen. Liger kernel: Efficient triton kernels for llm training. arXiv preprint arXiv:2410.10989, 2024. https://arxiv.org/abs/2410.10989. Chengyu Huang, Zeqiu Wu, Yushi Hu, and Wenya Wang. Training language models to generate text with citations via fine-grained rewards. arXiv preprint arXiv:2402.04315, 2024. Luyang Huang, Shuyang Cao, Nikolaus Parulian, Heng Ji, and Lu Wang. Efficient attentions for long document summarization. In Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 14191436, 2021. Ziwei Ji, Nayeon Lee, Rita Frieske, Tiezheng Yu, Dan Su, Yan Xu, Etsuko Ishii, Ye Jin Bang, Andrea Madotto, and Pascale Fung. Survey of hallucination in natural language generation. ACM Computing Surveys, 55(12):138, 2023. Sungdong Kim, Sanghwan Bae, Jamin Shin, Soyoung Kang, Donghyun Kwak, Kang Min Yoo, and Minjoon Seo. Aligning large language models through synthetic feedback. In The 2023 Conference on Empirical Methods in Natural Language Processing, 2023. Tao Lei, Regina Barzilay, and Tommi Jaakkola. Rationalizing neural predictions. In Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing, pages 107117, 2016. Hunter Lightman, Vineet Kosaraju, Yuri Burda, Harrison Edwards, Bowen Baker, Teddy Lee, Jan Leike, John Schulman, Ilya Sutskever, and Karl Cobbe. Lets verify step by step. In The Twelfth International Conference on Learning Representations, 2024. https://openreview.net/forum?id=v8L0pN6EOi. Yu Meng, Mengzhou Xia, and Danqi Chen. SimPO: Simple preference optimization with reference-free reward. In The Thirty-eighth Annual Conference on Neural Information Processing Systems, 2024. https://openreview.net/ forum?id=3Tzcot1LKb. Jacob Menick, Maja Trebacz, Vladimir Mikulik, John Aslanides, Francis Song, Martin Chadwick, Mia Glaese, Susannah Young, Lucy Campbell-Gillingham, Geoffrey Irving, et al. Teaching language models to support answers with verified quotes. arXiv preprint arXiv:2203.11147, 2022. Mistral. Mistral large, 2024. https://mistral.ai/news/mistral-large/. 12 Reiichiro Nakano, Jacob Hilton, Suchir Balaji, Jeff Wu, Long Ouyang, Christina Kim, Christopher Hesse, Shantanu Jain, Vineet Kosaraju, William Saunders, et al. Webgpt: Browser-assisted question-answering with human feedback. arXiv preprint arXiv:2112.09332, 2021. OpenAI. Introducing chatgpt, November 2022. https://openai.com/blog/chatgpt. OpenAI. Gpt-4 technical report, 2023. https://cdn.openai.com/papers/gpt-4.pdf. Long Ouyang, Jeff Wu, Xu Jiang, Diogo Almeida, Carroll L. Wainwright, Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, John Schulman, Jacob Hilton, Fraser Kelton, Luke Miller, Maddie Simens, Amanda Askell, Peter Welinder, Paul Christiano, Jan Leike, and Ryan Lowe. Training language models to follow instructions with human feedback. arXiv preprint 2203.02155, 2022. Richard Yuanzhe Pang, Weizhe Yuan, Kyunghyun Cho, He He, Sainbayar Sukhbaatar, and Jason Weston. Iterative reasoning preference optimization. arXiv preprint arXiv:2404.19733, 2024. Anirudh Phukan, Shwetha Somasundaram, Apoorv Saxena, Koustava Goswami, and Balaji Vasan Srinivasan. Peering into the mind of language models: An approach for attribution in contextual question answering. arXiv preprint arXiv:2405.17980, 2024. Jirui Qi, Gabriele Sarti, Raquel Fernández, and Arianna Bisazza. Model internals-based answer attribution for trustworthy retrieval-augmented generation. arXiv preprint arXiv:2406.13663, 2024. Rafael Rafailov, Archit Sharma, Eric Mitchell, Christopher Manning, Stefano Ermon, and Chelsea Finn. Direct preference optimization: Your language model is secretly reward model. Advances in Neural Information Processing Systems, 36, 2024. John Schulman, Filip Wolski, Prafulla Dhariwal, Alec Radford, and Oleg Klimov. Proximal policy optimization algorithms. In arXiv preprint arXiv:1707.06347, 2017. Weijia Shi, Xiaochuang Han, Mike Lewis, Yulia Tsvetkov, Luke Zettlemoyer, and Wen-tau Yih. Trusting your evidence: Hallucinate less with context-aware decoding. In Proceedings of the 2024 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (Volume 2: Short Papers), pages 783791, 2024. Aviv Slobodkin, Eran Hirsch, Arie Cattan, Tal Schuster, and Ido Dagan. Attribute first, then generate: Locallyattributable grounded text generation. arXiv preprint arXiv:2403.17104, 2024. Thomas Wolf, Lysandre Debut, Victor Sanh, Julien Chaumond, Clement Delangue, Anthony Moi, Pierric Cistac, Tim Rault, Rémi Louf, Morgan Funtowicz, Joe Davison, Sam Shleifer, Patrick von Platen, Clara Ma, Yacine Jernite, Julien Plu, Canwen Xu, Teven Le Scao, Sylvain Gugger, Mariama Drame, Quentin Lhoest, and Alexander M. Rush. Transformers: State-of-the-art natural language processing. In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing: System Demonstrations, pages 3845, Online, October 2020. Association for Computational Linguistics. https://www.aclweb.org/anthology/2020.emnlp-demos.6. Theodora Worledge, Judy Hanwen Shen, Nicole Meister, Caleb Winston, and Carlos Guestrin. Unifying corroborative and contributive attributions in large language models. arXiv preprint arXiv:2311.12233, 2023. Zhilin Yang, Peng Qi, Saizheng Zhang, Yoshua Bengio, William Cohen, Ruslan Salakhutdinov, and Christopher Manning. Hotpotqa: dataset for diverse, explainable multi-hop question answering. arXiv preprint arXiv:1809.09600, 2018. Michihiro Yasunaga, Leonid Shamis, Chunting Zhou, Andrew Cohen, Jason Weston, Luke Zettlemoyer, and Marjan Ghazvininejad. Alma: Alignment with minimal annotation. arXiv preprint arXiv:2412.04305, 2024. Weizhe Yuan, Richard Yuanzhe Pang, Kyunghyun Cho, Xian Li, Sainbayar Sukhbaatar, Jing Xu, and Jason In Forty-first International Conference on Machine Learning, 2024. Weston. Self-rewarding language models. https://openreview.net/forum?id=0NphYCmgua. Jiajie Zhang, Yushi Bai, Xin Lv, Wanjun Gu, Danqing Liu, Minhao Zou, Shulin Cao, Lei Hou, Yuxiao Dong, Ling Feng, et al. Longcite: Enabling llms to generate fine-grained citations in long-context qa. arXiv preprint arXiv:2409.02897, 2024. Chunting Zhou, Pengfei Liu, Puxin Xu, Srinivasan Iyer, Jiao Sun, Yuning Mao, Xuezhe Ma, Avia Efrat, Ping Yu, LILI YU, Susan Zhang, Gargi Ghosh, Mike Lewis, Luke Zettlemoyer, and Omer Levy. LIMA: Less is more for alignment. In A. Oh, T. Naumann, A. Globerson, K. Saenko, M. Hardt, and S. Levine, editors, Advances in Neural Information Processing Systems, volume 36, pages 5500655021, 2023."
        },
        {
            "title": "A Implementation Details",
            "content": "For SimPO fine-tuning, we randomly sample 2K document and question pairs from the LongCite-45k data, without using any ground-truth responses and citations. We generate the our own best-of-N responses with our Algorithm 1 to obtain the preference data, and train for one epoch. We sample another 100 examples as development set to pick the best learning rate from {1e-7, 3e-7, 5e-7, 7e-7}. We keep other hyperparameters the same as the original SimPO (Meng et al., 2024). We follow the same prompt format used in Zhang et al. (2024)3 to keep the comparison fair. For the iterative SimPO experiment, in each iteration, we sampled new, non-overlapping subset of 2K examples to ensure no data repetition across iterations. For self-supervised SFT, we generate 11K citation data unsupervisedly from ContextCite outputs as described in Appendix B, trained with larger learning rate 7e-6. We use the SimPO source code 4 built from Huggingface Transformers (Wolf et al., 2020) for the finetuning experiments, as well as Liger-Kernel (Hsu et al., 2024)5 to enable memory efficient training for long-context examples in LongCite-45K without tensor parallelization. We run all the finetuning experiments on with 8A100 GPUs of 80 GB memory on single node. The batch size is set to 1 per GPU due to the long context examples. We set our max context length to 25600 to prevent OOM. For the data examples longer than 25600, we perform truncation, starting from removing the sentences that are the most far away from the sentences cited by the ground truth annotation, so as to keep the impact of truncation to be minimum. When evaluating the citation length, as well as calculating the token length limit of 384 for excluding long BoN candidates, we follow Zhang et al. (2024) to use GLM4-9Bs tokenizer to count tokens. In Section 4.5, the denoising citation examples are done by randomly shifting existing citation spans by 3-10 positions in sentence indices."
        },
        {
            "title": "B Obtaining Citations from ContextCite",
            "content": "In this section, we first describe how the ContextCite method (Cohen-Wang et al., 2024) estimates continuous attribution scores for each sentence in the context. We then explain simple heuristic for extracting citations (i.e., selecting subset of context sources) from these scores. B.1 ContextCite Given language model pLM, context C, query and generated response R, ContextCite aims to quantify how each source in the context = {c1, c2, . . . , cC} contributes to the generated response (in our case, the sources are sentences). To do so, ContextCite performs several random context ablations. We begin by introducing some notation to describe these ablations. Let {0, 1}C be an ablation vector whose i-th entry toggles whether source ci is included (vi = 1) or excluded (vi = 0). We write Ablate(C, v) to denote modified version of the original context in which sources for which vi = 0 are omitted. ContextCite seeks to understand how the probability of generating the original generated response, changes as function of the ablation vector v. (v) := pLM(R Ablate(C, v), Q), Attribution via Surrogate Modeling. Directly measuring (v) for all 2C ablation vectors is infeasible for large C. Hence, ContextCite seeks to identify surrogate model ˆf (v) that is easy to understand and approximates 3https://github.com/THUDM/LongCite 4https://github.com/princeton-nlp/SimPO 5https://github.com/linkedin/Liger-Kernel 14 (v) well. To simplify this surrogate modeling task, ContextCite applies logit transform to , which maps values in (0, 1) to (, )): g(v) := σ1(f (v)) = log (cid:16) (v) 1 (v) (cid:17) . ContextCite then approximates g(v) using sparse linear function, ˆg(v) = ˆwv + ˆb. Notice that resulting weights ˆw RC encode the importance of each source ci to the probability of generating the original response; they can be interpreted directly as attribution scores (higher scores suggest greater importance). Finding Surrogate Model via Lasso. To learn the parameters ˆw and ˆb of the surrogate model, ContextCite randomly samples small number of ablation vectors and measures the corresponding probabilities of generating the original response. It then uses this training dataset to fit sparse linear model with Lasso. Concretely, it learns surrogate model with the following three steps: 1. Sample ablation vectors {vi}n 2. For each sample vi, compute g(vi) = σ1(f (vi)) by running the LM with only the sources specified by uniformly at random from {0, 1}C. i=1 vi and measuring the (sigmoid) probability of R. 3. Solve Lasso regression problem to find ˆw and ˆb: ˆw, ˆb = arg min w, 1 (cid:88) i=1 (cid:0)g(vi) wvi b(cid:1)2 + λw1, where λ controls sparsity (larger λ drives more coefficients to zero). In Cohen-Wang et al. (2024), typical choices of range from 32 to 256, balancing computation time (requires LM forward passes) and accuracy. If there are multiple statements {r1, r2, ..., rR} in R, the same method can also be applied by focusing only on subset of tokens in R. B.2 Heuristic Citation Extraction In our setting, we would like discrete list of cited sentences for each generated statement, rather than score for every sentence. We will now describe how to convert the attribution scores ˆw into discrete subset of citations. Let be threshold, be cumulative probability mass cutoff, and be maximum citation limit. Thresholding and Merging. 1. Filtering: Include only those sources ci whose attribution score ˆwi t. 2. Merging Adjacent Sources: If multiple consecutive sources in the original text each exceed t, merge them into single span Sj. We assign this merged span the maximum score among its constituents: ˆw(Sj) = max ci Sj ˆwi. Here, adjacency is defined by the original ordering in C. For instance, if c2 and c3 both pass the threshold and appear consecutively, we merge them into single span Sj. Softmax Normalization. Let {Sj} be the set of spans (or single sources) that survived the threshold. We normalize their scores into probability distribution: so that (cid:80) ˆw(Sj) = 1. ˆw(Sj) = exp(cid:0) ˆw(Sj)(cid:1) exp(cid:0) ˆw(Si)(cid:1) , (cid:80) 15 Top-p Selection. To avoid including too many low-value sources, we adopt greedy approach: Add spans in order of descending ˆw(Sj), stopping once (cid:88) ˆw(Sj) p. Sj Top-k Filtering. Finally, if > k, we take only the highest-scoring spans. We set = 1.5, = 0.7, = 4 in the experiment. When generating supervised fine-tuning (SFT) data, we discard any example for which more than 30% of its statements have no any citations that can survive threshold t. This ensures the dataset emphasizes cases where the LMs response can be tied to explicit context sources. We take the LongCite-45K document and question pairs to generate the responses by Llama-3.1-8B-Instruct itself, and then obtain citations with ContextCite (256 calls), transformed into the statement/citation format of LongCite-45K. Finally, we collect 11K examples used for SFT."
        },
        {
            "title": "C Length Balancing",
            "content": "To prevent the model from simply generating longer citations rather than focusing on citation correctness, we apply length balancing procedure to align the total citation length in our two training responses: chosen prediction and reject prediction. First, we find the citation string (e.g., [435-437]) enclosed in <cite>...</cite> tags for each statement. We then measure each strings total citation coverage, which means the total number of cited sentences in these intervals. If reject prediction has total coverage lower than the corresponding chosen prediction, we insert additional citations around nearby sentence indices to match the chosen coverage. Conversely, if the reject coverage is larger, we randomly remove some of its intervals. We ensure new or inserted citations do not overlap existing intervals and keep them within small window of 510 sentences away from the original citations to maintain realism. Finally, the reject and chosen will have matched coverage. This approach discourages the model from trivially learning to cite more sentences, instead prompting it to learn where and how to cite evidence more accurately. Our ablation in Section 4.2 shows that this length balancing technique significantly improves final citation quality."
        },
        {
            "title": "D Comparison with Claude Citations API",
            "content": "On January 23rd, 2025, Claude announced an API specialized for providing citations along with responses: Claude Citations 6. We also try to evaluate this API on the LongBench-Cite benchmark. Since the implementation details and resource requirements (e.g., training data) of Claude Citations are not publicly available yet, and it relies on significantly larger and more powerful LLM, Claude-3.5-Sonnet, which potentially has over 100 billions of parameters, we consider it as topline of the benchmark rather than baseline. When evaluating it on Chinese examples from LongBench-Cite, we found that the API does not split Chinese text properly. As result, it cites large passages when processing Chinese examples, leading to an average citation length of approximately 800 tokens per citation. To address this issue, we pre-segment the text ourselves using exactly the same method as our approach following LongCite (Zhang et al., 2024), which uses NLTK and Chinese punctuation segmentation. We then run the Claude Citations API, as it supports both non-segmented and pre-segmented document inputs. The evaluation was conducted using the latest version of claude-3-5-sonnet-20241022. As shown in Table 6, Claude Citations achieves an overall F1 score of 81.3, which is higher than all other models we have tested. However, the performance of Claude Citations is not consistent over all datasets. For example, it is worse than SelfCite on LongBench-Chat and GovReport. The main improvement of Claude is from the DuReader dataset, while the results on other datasets are comparable to the results of SelfCite. Given the fact that SelfCite leverages much smaller 8B model compared to the Claude-3.5-Sonnet model, the result of SelfCite is very impressive, demonstrating its potential to serve as strong alternative to proprietary solutions. 6https://www.anthropic.com/news/introducing-citations-api 16 Table 6 Citation recall (R), citation precision (P), citation F1 (F1), and citation length evaluated on LongBench-Cite benchmark. The best results are bolded. indicates the results taken from Zhang et al. (2024). Model Proprietary models GPT-4o Claude-3-sonnet GLM-4 Ours: SelfCite LongCite-8B (Our repro.) + BoN + SimPO + SimPO then BoN 67.0 68.4 68.1 73. Topline Longbench-Chat MultifieldQA F1 F1 P HotpotQA F1 Dureader F1 GovReport F1 Avg. Citation Length F1 46.7 52.0 47. 53.5 67.8 53.9 78.1 81.3 79.5 79.4 46.7 55.1 47.1 79.0 64.7 72.3 87.9 85.8 80.1 66.6 71.2 69.1 72. 74.8 76.1 75.5 76.7 90.7 92.8 92.6 93.2 80.6 71.3 73.6 79.9 81.2 81.0 82.2 55.7 46.4 47.0 60.8 67.2 69.4 69. 62.3 65.8 50.1 77.9 81.0 82.3 83.0 53.4 49.9 44.4 64.1 68.8 71.5 71.1 65.6 67.7 73.4 67.1 70.6 72.7 74. 74.2 89.2 82.3 87.2 90.9 91.6 92.2 67.4 75.5 75.0 73.7 76.9 78.9 80.3 73.4 77.4 82.8 81.6 87.6 86.4 86. 90.4 93.9 93.4 89.3 92.4 92.9 92.7 79.8 84.1 87.1 84.5 89.3 89.1 89.2 65.6 67.2 65.4 73.8 77.5 77.9 79. 220 132 169 83.5 93.4 105.7 94.7 Claude Citations 61.2 81.7 67. 76.8 98.4 84.9 61.9 94.1 72.9 88.5 99.7 93.2 79.4 99. 87.7 81.3 88.8 Table 7 An example of differences in the citation from baseline vs BoN. Related information are highlighted in the context/response. Sent. ID 23 () 42 () 45 () 46 () 47-50 ()"
        },
        {
            "title": "Query",
            "content": "Context Sentences (only showing cited sentences due to space) We explored using simple and cost-effective procedure to instruction finetune our continually pretrained long models without any human-annotated data. Collecting human demonstration and preference labels for LLM alignment is cumbersome and expensive process (Ouyang et al., 2022; Touvron et al., 2023). In this work, we found that simple and cheap approach which leverages pre-built large and diverse short-prompt dataset works surprisingly well on long-context benchmarks. Specifically, we take the RLHF dataset used in LLAMA 2 CHAT and augment it with synthetic self-instruct (Wang et al., 2022) long data generated by LLAMA 2 CHAT itself, in the hope that the model can learn diverse set of skills through the large amount of RLHF data and transfer that knowledge to long-context scenarios via self-instruct data. The data generation process focuses on QA-format tasks: starting from long document in our pretraining corpus, we select random chunk and prompt LLAMA 2 CHAT to write question-answer pairs based on information in the text chunk. [...] What aspects of the LLAMA Long model proposed above have changed relative to the LLAMA-based model? What improvements have been made? Response (single statement due to space) [...] 3. Instruction Tuning: The paper proposes simple and cost-effective procedure to instruction finetune the continually pretrained long models without any human-annotated data. This involves using pre-built large and diverse short-prompt dataset and augmenting it with synthetic self-instruct long data generated by LLAMA CHAT itself. [...] Citation Strings (green: correct; red: wrong)"
        },
        {
            "title": "Baseline\nSelfCite BoN",
            "content": "[42-42][45-50] [23-23][45-45][46-46]"
        },
        {
            "title": "E More Qualitative Examples",
            "content": "We further show more qualitative examples in Table 7,8, and 9, to represent the cases where SelfCite is better as well as where the LongCite-8B direct sampling baseline is better. In Table 7, SelfCite BoN avoid the cited irrelevant sentence (42, 47-50) by the baseline, while further including correct citation (23) that are not found by the baseline. In Table 8, both SelfCite BoN and the baseline cites too many irrelevant sentences (391-393) but SelfCite BoNs citation is slightly better. In Table 9, SelfCite BoN wrongly includes 30 and misses 70, but the baseline is slightly better and only wrongly includes 71. 17 Table 8 An example of differences in the citation from baseline vs BoN. Related information are highlighted in the context/response. Sent. ID 299 () 300 () 301 () 302 () 383 () 384 () 385 () 390 () 391 () 392 () 393 ()"
        },
        {
            "title": "Query",
            "content": "Context Sentences (only showing cited sentences due to space) Ribosomes link amino acids together in the order specified by the codons of messenger RNA (mRNA) molecules to form polypeptide chains. Ribosomes consist of two major components: the small and large ribosomal subunits. Each subunit consists of one or more ribosomal RNA (rRNA) molecules and many ribosomal proteins (RPs or r-proteins). The ribosomes and associated molecules are also known as the translational apparatus. Ribosomes are minute particles consisting of RNA and associated proteins that function to synthesize proteins. Proteins are needed for many cellular functions such as repairing damage or directing chemical processes. Ribosomes can be found floating within the cytoplasm or attached to the endoplasmic reticulum. Each subunit is made of one or more rRNAs and many r-proteins. The small subunit (30S in bacteria and archaea, 40S in eukaryotes) has the decoding function, whereas the large subunit (50S in bacteria and archaea, 60S in eukaryotes) catalyzes the formation of peptide bonds, referred to as the peptidyl-transferase activity. The bacterial (and archaeal) small subunit contains the 16S rRNA and 21 r-proteins (Escherichia coli), whereas the eukaryotic small subunit contains the 18S rRNA and 32 r-proteins (Saccharomyces cerevisiae; although the numbers vary between species). The bacterial large subunit contains the 5S and 23S rRNAs and 34 r-proteins (E. coli), with the eukaryotic large subunit containing the 5S, 5.8S, and 25S / 28S rRNAs and 46 r-proteins (S. cerevisiae; again, the exact numbers vary between species). The large subunit and small subunit that use two types of RNA are major components that make up what? Response (single statement due to space) [...] The ribosome consists of two major components: the small and large ribosomal subunits. Each subunit consists of one or more ribosomal RNA (rRNA) molecules and many ribosomal proteins (RPs or r-proteins). The ribosomes and associated molecules are also known as the translational apparatus. [...] Citation Strings (green: correct; red: wrong)"
        },
        {
            "title": "Baseline\nSelfCite BoN",
            "content": "[299-302][383-385][390-393] [300-302][390-393] 18 Table 9 An example of differences in the citation from baseline vs BoN. Related information are highlighted in the context/response. Sent. ID 28 () 29 () 30 () 61 () 62 () 70 () 71 ()"
        },
        {
            "title": "Query",
            "content": "Context Sentences (only showing cited sentences due to space) The Aegis BMD system exists in several variants. Listed in order of increasing capability, these include (but are not necessarily limited to) 3.6.X variant, the 4.0.3 variant, the 4.1 variant (also known as the Aegis Baseline [BL] 5.4 variant), the 5.0 CU (Capability Upgrade) variant (also known as the BL 9.1 variant), the 5.1 variant (also known as the BL 9.2 variant), and the 6.X variant (also known as the BL 10 variant). Figure 1 summarizes the capabilities of some of these variants (using their designations as of 2016) and correlates them with the phases of the European Phased Adaptive Approach (or EPAA; see discussion below) for European BMD operations. The BMD interceptor missiles used by Aegis ships are the Standard Missile-3 (SM-3), the SM-2 Block IV, and the SM-6. The SM-3 is designed to intercept ballistic missiles above the atmosphere (i.e., exo-atmospheric intercept), in the midcourse phase of an enemy ballistic missiles flight. The SM-2 Block IV is designed to intercept ballistic missiles inside the atmosphere (i.e., endoatmospheric intercept), during the terminal phase of an enemy ballistic missiles flight. It is equipped with blast fragmentation warhead. Please write one-page summary of the above government report. Response (single statement due to space) [...] The Aegis BMD system exists in several variants with increasing capability, including the 3.6.X, 4.0.3, 4.1, 5.0 CU, 5.1, and 6.X variants. The system uses the Standard Missile-3 (SM-3), SM-2 Block IV, and SM-6 interceptor missiles. The SM-3 is designed to intercept ballistic missiles above the atmosphere in the midcourse phase, while the SM-2 Block IV and SM-6 are designed to intercept ballistic missiles inside the atmosphere in the terminal phase. [...] Citation Strings (green: correct; red: wrong)"
        },
        {
            "title": "Baseline\nSelfCite BoN",
            "content": "[28-29][61-62][70-71] [28-30][61-62] (missing: 70)"
        }
    ],
    "affiliations": [
        "Massachusetts Institute of Technology",
        "Meta FAIR"
    ]
}