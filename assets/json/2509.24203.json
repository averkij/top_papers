{
    "paper_title": "Group-Relative REINFORCE Is Secretly an Off-Policy Algorithm: Demystifying Some Myths About GRPO and Its Friends",
    "authors": [
        "Chaorui Yao",
        "Yanxi Chen",
        "Yuchang Sun",
        "Yushuo Chen",
        "Wenhao Zhang",
        "Xuchen Pan",
        "Yaliang Li",
        "Bolin Ding"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Off-policy reinforcement learning (RL) for large language models (LLMs) is attracting growing interest, driven by practical constraints in real-world applications, the complexity of LLM-RL infrastructure, and the need for further innovations of RL methodologies. While classic REINFORCE and its modern variants like Group Relative Policy Optimization (GRPO) are typically regarded as on-policy algorithms with limited tolerance of off-policyness, we present in this work a first-principles derivation for group-relative REINFORCE without assuming a specific training data distribution, showing that it admits a native off-policy interpretation. This perspective yields two general principles for adapting REINFORCE to off-policy settings: regularizing policy updates, and actively shaping the data distribution. Our analysis demystifies some myths about the roles of importance sampling and clipping in GRPO, unifies and reinterprets two recent algorithms -- Online Policy Mirror Descent (OPMD) and Asymmetric REINFORCE (AsymRE) -- as regularized forms of the REINFORCE loss, and offers theoretical justification for seemingly heuristic data-weighting strategies. Our findings lead to actionable insights that are validated with extensive empirical studies, and open up new opportunities for principled algorithm design in off-policy RL for LLMs. Source code for this work is available at https://github.com/modelscope/Trinity-RFT/tree/main/examples/rec_gsm8k."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 9 2 ] . [ 1 3 0 2 4 2 . 9 0 5 2 : r Group-Relative REINFORCE Is Secretly an Off-Policy Algorithm: Demystifying Some Myths About GRPO and Its Friends Chaorui Yao, Yanxi Chen, Yuchang Sun, Yushuo Chen, Wenhao Zhang, Xuchen Pan, Yaliang Li, Bolin Ding Abstract Off-policy reinforcement learning (RL) for large language models (LLMs) is attracting growing interest, driven by practical constraints in real-world applications, the complexity of LLM-RL infrastructure, and the need for further innovations of RL methodologies. While classic REINFORCE and its modern variants like Group Relative Policy Optimization (GRPO) are typically regarded as on-policy algorithms with limited tolerance of off-policyness, we present in this work first-principles derivation for group-relative REINFORCE without assuming specific training data distribution, showing that it admits native off-policy interpretation. This perspective yields two general principles for adapting REINFORCE to off-policy settings: regularizing policy updates, and actively shaping the data distribution. Our analysis demystifies some myths about the roles of importance sampling and clipping in GRPO, unifies and reinterprets two recent algorithms Online Policy Mirror Descent (OPMD) and Asymmetric REINFORCE (AsymRE) as regularized forms of the REINFORCE loss, and offers theoretical justification for seemingly heuristic data-weighting strategies. Our findings lead to actionable insights that are validated with extensive empirical studies, and open up new opportunities for principled algorithm design in off-policy RL for LLMs. Source code for this work is available at https://github.com/modelscope/Trinity-RFT/tree/main/examples/rec_gsm8k."
        },
        {
            "title": "1 Introduction",
            "content": "The past few years have witnessed rapid progress in reinforcement learning (RL) for large language models (LLMs). This began with reinforcement learning from human feedback (RLHF) [Bai et al., 2022, Ouyang et al., 2022] that aligns pre-trained LLMs with human preferences, followed by reasoning-oriented RL that enables LLMs to produce long chains of thought [OpenAI, 2024, DeepSeek-AI, 2025, Kimi-Team, 2025b, Zhang et al., 2025b]. More recently, agentic RL [Kimi-Team, 2025a, Gao et al., 2025, Zhang et al., 2025a] aims to train LLMs for agentic capabilities such as tool use, long-horizon planning, and multi-step task execution in dynamic environments. Alongside these developments, off-policy RL has been attracting growing interest. In the era of experience [Silver and Sutton, 2025], LLM-powered agents need to be continually updated through interaction with the environment. Practical constraints in real-world deployment and the complexity of LLM-RL infrastructure often render on-policy training impractical [Noukhovitch et al., 2025]: rollout generation and model training can proceed at mismatched speeds, data might be collected from different policies, reward feedback might be irregular or delayed, and the environment may be too costly or unstable to query for fresh trajectories. Moreover, in pursuit of higher sample efficiency and model performance, it is desirable to go beyond the standard paradigm of independent rollout sampling, e.g., via replaying past experiences [Schaul et al., 2016, Rolnick et al., 2019, An et al., 2025], synthesizing higher-quality experiences based on auxiliary information [Da et al., 2025, Liang et al., 2025, Guo et al., 2025], or incorporating expert demonstrations into online RL [Yan et al., 2025, Zhang et al., 2025c] all of which incur off-policyness. However, the prominent algorithms in LLM-RL Proximal Policy Optimization (PPO) [Schulman et al., 2017] and Group Relative Policy Optimization (GRPO) [Shao et al., 2024] are essentially on-policy Equal contribution. Contact: chaorui@ucla.edu, chenyanxi.cyx@alibaba-inc.com UCLA. Work done during an internship at Alibaba Group. Alibaba Group. 1 methods: as modern variants of REINFORCE [Williams, 1992], their fundamental rationale is to produce unbiased estimates of the policy gradient, which requires fresh data sampled from the current policy. PPO and GRPO can handle limited degree of off-policyness via importance sampling, but require that the current policy remains sufficiently close to the behavior policy. Truly off-policy LLM-RL often demands ad-hoc analysis and algorithm design; worse still, as existing RL infrastructure [Sheng et al., 2024, Hu et al., 2024, von Werra et al., 2020, Wang et al., 2025, Pan et al., 2025, Fu et al., 2025a] is typically optimized for REINFORCE-style algorithms, their support for specialized off-policy RL algorithms could be limited. All these have motivated our investigation into principled and infrastructure-friendly algorithm design for off-policy RL. y1, . . . , yK} { Core finding: native off-policy interpretation for group-relative REINFORCE. Consider one-step RL setting and group-relative variant of REINFORCE that, like in GRPO, assumes access to for the same prompt and use the group mean reward as the baseline in multiple responses , y2 advantage calculation. Each response is sequence of tokens yi = (y1 , . . . ), and receives response-level x) denote an autoregressive policy parameterized by θ. The update rule for reward ri = r(x, yi). Let πθ( each iteration of group-relative REINFORCE is θ = θ + ηg, where η is the learning rate, and is the sum of updates from multiple prompts and their corresponding responses. For specific prompt x, the update would be1 g(cid:0)θ; x, yi, ri}1 { (cid:1) = = 1 1 (cid:88) 1 (cid:88) (ri (cid:88) 1 1 yi r) θ log πθ(yi x) (response-wise) (1a) r) θ log πθ(yt (ri x, y<t ) (token-wise). (1b) x, y<t ), where y<t denotes the first Here, the response-wise and token-wise formulas are linked by the elementary decomposition log πθ(yi (cid:80) log πθ(yt major finding of this work is that group-relative REINFORCE admits native off-policy interpretation. We establish this in Section 2 via novel, first-principles derivation that makes no explicit assumption , in contrast to the standard policy gradient theory. yi} about the sampling distribution of the responses Our derivation provides new perspective for understanding how REINFORCE makes its way towards the optimal policy by constructing series of surrogate objectives and taking gradient steps for the corresponding surrogate losses. Such analysis can be extended to multi-step RL settings as well, with details deferred to Appendix A. 1 tokens of yi. x) = { Implications: principles and concrete methods for augmenting REINFORCE. While the proposed off-policy interpretation does not imply that vanilla REINFORCE should converge to the optimal policy when given arbitrary training data (which is too good to be true), our analysis in Section 3 identifies two general principles for augmenting REINFORCE in off-policy settings: (1) regularize the policy update step to stabilize learning, and (2) actively shape the training data distribution to steer the policy update direction. As we will see in Section 4, this unified framework demystifies common myths about the rationales behind many recent RL algorithms: (1) It reveals that in GRPO, clipping (as form of regularization) plays much more essential role than importance sampling, and it is often viable to enlarge the clipping range far beyond conventional choices for accelerated convergence without sacrificing stability. (2) Two recent algorithms Kimis Online Policy Mirror Descent (OPMD) [Kimi-Team, 2025b] and Metas Asymmetric REINFORCE (AsymRE) [Arnal et al., 2025] can be reinterpreted as adding regularization loss to the standard REINFORCE loss, which differs substantially from the rationales explained in their original papers. (3) Our framework justifies heuristic data-weighting strategies like discarding certain low-reward samples or 1For notational simplicity and consistency, we use the same normalization factor 1/K for both response-wise and token-wise formulas in Eq. (1a) and (1b). For practical implementation, the gradient is calculated with samples from mini-batch, and typically normalized by the total number of response tokens. This mismatch does not affect our theoretical studies in this work. Interestingly, our analysis of REINFORCE in this work provides certain justifications for calculating the token-mean loss within mini-batch, instead of first taking the token-mean loss within each sequence and then taking the average across sequences [Shao et al., 2024]; our perspective is complementary to the rationales explained in prior works like DAPO [Yu et al., 2025], although deeper understanding of this aspect is beyond our current focus. 2 up-weighting high-reward ones, even though they violate assumptions in policy gradient theory and often require ad-hoc analysis in prior works. Extensive empirical studies in Section 4 and Appendix validate these insights and demonstrate the efficacy and/or limitations of various algorithms under investigation. By revealing the off-policy nature of group-relative REINFORCE, our work opens up new opportunities for principled, infrastructure-friendly algorithm design in off-policy LLM-RL with solid theoretical foundation."
        },
        {
            "title": "2 Two interpretations for REINFORCE",
            "content": "Consider the standard reward-maximization objective in reinforcement learning: max θ J(θ) := Ex J(θ; x), where J(θ; x) := x) r(x, y), πθ ( (2) where is distribution over the prompts x. We first recall the standard on-policy interpretation of REINFORCE in Section 2.1, and then present our proposed off-policy interpretation in Section 2.2."
        },
        {
            "title": "2.1 Recap: on-policy interpretation via policy gradient theory",
            "content": "In the classical on-policy view, REINFORCE updates policy parameters θ using samples that are drawn directly from πθ. The policy gradient theorem [Sutton et al., 1998] tells us that θJ(θ; x) = θ πθ ( x) r(x, y) = πθ ( (cid:104)(cid:0)r(x, y) x) b(x)(cid:1) θ log πθ(y (cid:105) , x) where b(x) is baseline for reducing variance when are drawn from different behavior policy πb instead, the gradient can be rewritten as θJ(θ; x) is estimated with finite samples. If samples θJ(θ; x) = πb( (cid:20) (cid:0)r(x, y) x) b(x)(cid:1) πθ(y πb(y x) x) θ log πθ(y (cid:21) . x) While the raw importance-sampling weight πθ(y x) facilitates unbiased policy gradient estimate, it x)/πb(y may be unstable when πθ and πb diverge. Modern variants of REINFORCE address this by modifying the probability ratios (e.g., via clipping or normalization), which achieves better bias-variance trade-off in the policy gradient estimate and leads to stable learning process. In the LLM context, we have x, y<t), but the response-wise probability x) can blow up or shrink exponentially with the sequence length. Practical implementaθ log πθ(yt θ log πθ(y ratio πθ(y tions typically adopt token-wise probability ratio instead: x)/πb(y x) = (cid:80) (cid:101)g(θ; x) = πb( (cid:20) (cid:0)r(x, y) x) b(x)(cid:1) (cid:88) 1 πθ(yt πb(yt x, y<t) x, y<t) θ log πθ(yt (cid:21) x, y<t) . θJ(θ; x), classical RL theory still offers policy improveAlthough this becomes biased approximation of ment guarantees if πθ is sufficiently close to πb [Kakade and Langford, 2002, Fragkiadaki, 2018, Schulman et al., 2015, 2017, Achiam et al., 2017]."
        },
        {
            "title": "2.2 A new interpretation: REINFORCE is inherently off-policy",
            "content": "We now provide an alternative off-policy interpretation for group-relative REINFORCE. Let us think of policy optimization as an iterative process θ1, θ2, . . . , and focus on the t-th iteration that updates the policy model parameters from θt to θt+1. Our derivation consists of three steps: (1) define KL-regularized surrogate objective, and show that its optimal solution must satisfy certain consistency conditions; (2) define surrogate loss (with finite samples) that enforces such consistency conditions; and (3) take one gradient step of the surrogate loss, which turns out to be equivalently the group-relative REINFORCE method. 3 Step 1: surrogate objective and consistency condition. Consider the following KL-regularized surrogate objective that incentivizes the policy to make stable improvement over πθt: (cid:104) max θ J(θ; πθt) := Ex y πθ ( x)[r(x, y)] τ DKL (cid:0)πθ( x) πθt( x)(cid:1)(cid:105) , (3) where τ is regularization coefficient. It is well-known fact that the optimal policy π for this surrogate objective satisfies the following [Nachum et al., 2017, Korbak et al., 2022, Rafailov et al., 2023, Richemond et al., 2024, Kimi-Team, 2025b]: for any prompt and response y, x) = π(y πθt(y x)er(x,y)/τ Z(x, πθt) , where Z(x, πθt) := (cid:90) πθt(y x)er(x,y)/τ dy. (4) Note that Eq. (4) is equivalent to the following: for any pair of responses y1 and y2, π(y1 π(y2 x) x) = πθt(y1 πθt(y2 x) x) exp (cid:18) r(x, y1) r(x, y2) (cid:19) . τ Taking logarithm of both sides, we have this pairwise consistency condition: τ r1 x)(cid:1) = r2 Step 2: surrogate loss with finite samples. Given prompt and responses y1, . . . , yK, we define the following mean-squared surrogate loss that enforces the consistency condition: (cid:0) log π(y1 (cid:0) log π(y2 log πθt(y1 log πθt(y2 x)(cid:1). (5) x) x) τ (cid:98)L(θ; x, πθt) := 1 2 (cid:88) i<j 1 aj)2 (ai (1 + τ )2 , where ai := ri (cid:16) τ log πθ(yi x) log πθt(yi x) (cid:17) . (6) aj by 1 + τ to account for the loss scale. In theory, if this surrogate loss is defined Here, we normalize ai by infinite samples with sufficient coverage of the action space, then its minimizer is the same as the optimal policy for the surrogate objective in Eq. (3). Step 3: one gradient step of the surrogate loss. Let us conduct further analysis for (ai trick here is that, if we take only one gradient step of this loss at θ = θt, then the values of log πθ(yi log πθt(yi x) are simply zero. As result, x) aj)2. The x) x)(cid:12) θ log πθ(yi 2τ (1 + τ )2 (ri (cid:12)θt (cid:16) θ log πθ(yj x)(cid:12) rj) θ log πθ(yi (cid:12)θt θ log πθ(yj (cid:17) x)(cid:12) (cid:12)θt (cid:17) x)(cid:12) (cid:12)θt (cid:1) ri θ log πθ(yj (cid:19) x)(cid:12) (cid:12)θt x) and log πθ(yj aj)2(cid:12) = θ(ai (cid:12)θt aj)2 (ai (cid:88) (1 + τ )2 2τ (ri (cid:12) (cid:12) = (cid:12)θt log πθt(yj (cid:16) rj) θ (cid:88) i<j 2τ (1 + τ )2 (cid:18) (cid:0)ri (cid:1) rj 1 i<j (cid:88) = i<j = 2τ (1 + τ )2 = 2τ (1 + τ ) x)(cid:12) (cid:12)θt + (cid:0)rj x)(cid:12) (cid:12)θt θ log πθ(yi (cid:1) θ log πθ(yi (cid:0)ri r(cid:1) θ log πθ(yi rj (cid:88) (cid:88) 1 (cid:88) 1 1 (cid:0)ri x)(cid:12) (cid:12)θt , where := 1 (cid:88) rj. 1 Putting these back to the surrogate loss defined in Eq. (6), we end up with this policy update step: g(cid:0)θ; x, yi, ri}1 { (cid:1) = 2τ (1 + τ )2 1 (cid:88) (cid:0)ri r(cid:1) θ log πθ(yi x). (7) Thats it! We have just derived the group-relative REINFORCE method, but without any on-policy assumption about the distribution of training data . The regularization coefficient τ > 0 controls yi, ri}1 the update step size; larger τ effectively corresponds to smaller learning rate. K} x, { { 4 Figure 1: visualization of our off-policy interpretation for group-relative REINFORCE. Here (cid:98)L(θ; πθt) = (cid:98)D[(cid:98)L(θ; x, πθt)], where (cid:98)D is the sampling distribution for prompts, and (cid:98)L(θ; x, πθt) is the surrogate loss defined in Eq. (6) for specific prompt x. Summary and remarks. Figure 1 visualizes the proposed interpretation of what REINFORCE is actually θ stands for the ideal optimization trajectory from θt doing. The curve going through θt to the optimal policy model parametrized by θ, if the algorithm solves each intermediate surrogate objective J(θ; πθt) / surrogate loss (cid:98)L(θ; πθt) exactly at each iteration t. In comparison, REINFORCE is effectively taking single gradient step of the surrogate loss and immediately moving on to the next iteration θt+1 with new surrogate objective. (cid:101)θt+1 θt+1 Two remarks are in place. (1) Our derivation of group-relative REINFORCE can be generalized to multi-step RL settings, by replacing response in the previous analysis with full trajectory consisting of multiple turns of agent-environment interaction. For example, regarding the surrogate objective in Eq. (3), we need to replace the response-level reward and KL divergence with their trajectory-level counterparts. Interested readers might refer to Appendix for the full analysis. (2) The above analysis suggests that we might interpret group-relative REINFORCE from pointwise or pairwise perspective. While the policy update in Eq. (7) is stated in pointwise manner, we have also seen that, at each iteration, REINFORCE is implicitly enforcing the pairwise consistency condition in Eq. (5) among multiple responses. This allows us the flexibility to choose whichever perspective that offers more intuition for our analysis later in this work."
        },
        {
            "title": "3 Pitfalls and augmentations",
            "content": "Although we have provided native off-policy interpretation for REINFORCE, it certainly does not guarantee convergence to the optimal policy when given arbitrary training data. This section identifies pitfalls that could undermine vanilla REINFORCE, which motivate two principles for augmentations in off-policy settings. θt θt θt, θ (cid:101)θt+1 > 0 do not imply θt should align well with θ θt θt+1 In Figure 1, we might expect that ideally, (1) (cid:101)θt+1 θt aligns with the direction of (cid:101)θt+1 θt+1 θt aligns with Pitfalls of vanilla REINFORCE. the direction of θ θt. One pitfall, however, is that θt; and (2) θt+1 θt. even if both conditions hold, they do not necessarily imply that θt+1 θt, (cid:101)θt+1 > 0. That is, > 0 and θt. Recall from Eq. (7) that, from θt might not align well with (cid:101)θt+1 Moreover, it is possible that θt+1 θt to θt+1, we take one gradient step for surrogate loss that enforces the pairwise consistency condition among finite number of samples. Given the enormous action space of an LLM, some implicit assumptions about the training data (e.g., balancedness and coverage) would be needed to ensure that the gradient aligns well with the direction towards the optimum of the surrogate objective, namely (cid:101)θt+1 In fact, without mechanism that ensures boundedness of policy update under sub-optimal data distribution, vanilla REINFORCE could eventually converge to sub-optimal policy. Let us show this with minimal example in didactic 3-arm bandit setting. Suppose that there are three actions aj}1 3 with rewards 3 is sampled from some } { behavior policy πb. Denote by µr := (cid:80) r(yi)/K the average reward of training samples. We consider the softmax parameterization, i.e., πθ(aj) = eθj / (cid:80) ℓ eθℓ R3 is πθ, where ej for policy parameterized by θ one-hot vector with value 1 at entry j. Now we examine the policy update direction of REINFORCE, as { 3 πb(aj)r(aj) the expected reward under πb, and := (cid:80) R3. standard fact is that θ log πθ(aj) = ej . Consider training samples K, where yi { aj} yi}1 θt, θ r(aj) θt. { 1 i 5 : = 1 (cid:88) 1 = (cid:88) 1 3 (r(yi) r) θ log πθ(yi) πb(aj)(r(aj) µr) θ log πθ(aj) πb(aj)(r(aj) µr)(ej πb(aj)(r(aj) µr)ej. (cid:88) 3 πθ) = (cid:88) 3 For example, if = [r(aj)]1 0.58, 0.22, 0.42], and finally g2 = πb(a2)(r(a2) that the policy will converge to the sub-optimal action a2. µr = [ 3 = [0, 0.8, 1] and πb = [0.3, 0.6, 0.1], then basic calculation says µr = 0.58, µr) = g3, which implies µr) > πb(a3)(r(a3) Two principles for augmenting REINFORCE. The identified pitfalls of vanilla REINFORCE suggest two general principles for augmenting REINFORCE in off-policy scenarios: One is to regularize the policy update step, ensuring that the optimization trajectory remains bounded and reasonably stable when given training data from sub-optimal distribution; The other is to steer the policy update direction, by actively weighting the training samples rather than naively using them as is. These two principles are not mutually exclusive, and might be integrated within single algorithm. We will see in the next section that many RL algorithms can be viewed as instantiations of them."
        },
        {
            "title": "4 Rethinking the rationales behind recent RL algorithms",
            "content": "This section revisits various RL algorithms through unified lens the native off-policy interpretation of group-relative REINFORCE and its augmentations and demystifies some common myths about their working mechanisms. Our main findings are summarized as follows: ID Finding F1 GRPOs effectiveness in off-policy settings stems from clipping as regularization rather than importance sampling. wider clipping range than usual often accelerates training without harming stability. F2 Kimis OPMD and Metas AsymRE can be interpreted as REINFORCE loss + regularization loss, perspective that is complementary to the rationales in their original papers. F3 Data-oriented heuristics such as dropping excess negatives or upweighting high-reward rollouts fit naturally into our off-policy view and show strong empirical performance. Analysis & Experiments Section 4.1, Figures 3, 4, 6, 9, 10 Section 4.2, Figure 11 Section 4.3, Figures 5, 6, 7 Experimental setup. We primarily consider two off-policy settings in our experiments, specified by the sync interval and sync offset parameters in the Trinity-RFT framework [Pan et al., 2025]. sync interval specifies the number of generated rollout batches (each corresponding to one gradient step) between two model synchronization operations, while sync offset specifies the relative lag between the generation and consumption of each batch. These parameters can be deliberately set to large values in practice, for improving training efficiency via pipeline parallelism and reduced frequency of model synchronization. In addition, sync offset > 1 serves to simulate realistic scenarios where environmental feedback could be delayed. We also consider stress-test setting that only allows access to offline data generated by the initial policy model. See Figure 2 for an illustration of these off-policy settings, and Appendix B.2 for further details. We conduct experiments on math reasoning tasks like GSM8k [Cobbe et al., 2021], MATH [Hendrycks et al., 2021] and Guru (math subset) [Cheng et al., 2025], as well as tool-use tasks like ToolACE [Liu et al., 2025a]. We consider models of different families and scales, including Qwen2.5-1.5B-Instruct, Qwen2.57B-Instruct [Qwen-Team, 2025], Llama-3.1-8B-Instruct, and Llama-3.2-3B-Instruct [Dubey et al., 2024]. Additional experiment details can be found in Appendix B. Figure 2: visualization of the rollout-training scheduling in sync interval = 4 (left) or sync offset = 4 (right) modes. Each block denotes one batch of samples for one gradient step, and the number in it denotes the corresponding batch id. Training blocks are color-coded by data freshness, with lighter color indicating increasing off-policyness."
        },
        {
            "title": "4.1 Demystifying myths about GRPO",
            "content": "r)/σr, where and σr Recall that in GRPO, the advantage for each response yi is defined as Ai = (ri denote the within-group mean and standard deviation of the rewards respectively. We consider the practical implementation of GRPO with token-wise importance-sampling (IS) weighting and clipping, whose loss function for specific prompt and responses ri}1 is2 { yi} { (cid:98)L = 1 (cid:88) (cid:88) 1 yi min (cid:26) πθ(yt πold(yt x, y<t ) x, y<t ) Ai, clip (cid:16) πθ(yt πold(yt x, y<t ) x, y<t ) , ϵlow, 1 + ϵhigh (cid:17) (cid:27) Ai , where πold denotes the older policy version that generated this group of rollout data. The gradient of this loss can be written as [Schulman et al., 2017] g(cid:0)θ; x, yi, ri}1 { (cid:1) = 1 (cid:88) (cid:88) 1 yi θ log πθ(yt x, y<t ) Ai πθ(yt πold(yt x, y<t ) x, y<t ) , where i denotes one-side clipping mask: (cid:18) = 1 Ai > 0, πθ(yt πold(yt x, y<t ) x, y<t ) 1 + ϵhigh (cid:19) (cid:18) + 1 Ai < 0, πθ(yt πold(yt x, y<t ) x, y<t ) 1 (cid:19) ϵlow . (8) Ablation study with the REC series. To isolate the roles of importance sampling and clipping, we consider series of REINFORCE-with-Clipping (REC) algorithms. Due to space limitation, we defer our studies of more clipping mechanisms to Appendix B.3, and focus on REC with one-side clipping in this section. More specifically, REC-OneSide-IS removes advantage normalization in GRPO (to reduce variability), and REC-OneSide-NoIS further removes IS weighting: REC-OneSide-IS: = REC-OneSide-NoIS: = 1 1 (cid:88) (cid:88) 1 (cid:88) 1 yi (cid:88) 1 1 yi θ log πθ(yt x, y<t ) (ri r) πθ(yt πold(yt x, y<t ) x, y<t ) , θ log πθ(yt x, y<t ) (ri r) . 2In our experiments with GRPO, we neglect KL regularization with respect to an extra reference model, or entropy regularization that encourages output diversity. Recent works [Yu et al., 2025, Liu et al., 2025b] have shown that these practical techniques are often unnecessary. 7 Figure 3: Empirical results for REC algorithms on GSM8k with Qwen2.5-1.5B-Instruct. Training reward curves are smoothed with running-average window of size 3. Numbers in the legend denote clipping parameters ϵlow, ϵhigh. Experiments. Figure 3 presents GSM8k results with Qwen2.5-1.5B-Instruct in various off-policy settings. REC-OneSide-IS/NoIS and GRPO (with the same ϵlow = ϵhigh = 0.2) have nearly identical performance, indicating that importance sampling is non-essential, whereas the collapse of REINFORCE highlights the critical role of clipping. Radically enlarging (ϵlow, ϵhigh) to (0.6, 2.0) accelerates REC-OneSide-NoIS without compromising stability in both sync interval = 20 and sync offset = 10 settings. Similar patterns also appear in Figure 4 (ToolAce with Llama-3.2-3B-Instruct) and other results in Appendix B. As for the stress-test (offline) setting, Figure 3 reveals an intrinsic trade-off between the speed and stability of policy improvement, motivating future work toward better algorithms that achieve both."
        },
        {
            "title": "4.2 Understanding Kimi’s OPMD and Meta’s AsymRE",
            "content": "Besides clipping, another natural method is to add regularization loss R( ) to vanilla REINFORCE: (cid:98)L(cid:0)θ; x, yi, ri}1 { (cid:1) = 1 (cid:88) [K] (ri r) log πθ(yi x) + τ R(cid:0)θ; x, yi, ri}1 { (cid:1), and take = this unified formula, with empirical validation of their efficacy deferred to Appendix B.5. θ (cid:98)L. We show below that Kimis OPMD and Metas AsymRE are indeed special cases of Kimis OPMD. Kimi-Team [2025b] derives an OPMD variant by taking logarithm of both sides of Eq. (4), which leads to consistency condition and further motivates the following surrogate loss: (cid:101)L = 1 (cid:18) (cid:88) ri τ log Z(x, πθt) (cid:16) τ log πθ(yi x) log πθt(yi x) (cid:17)(cid:19)2 . With responses generated by πold = πθt, the term τ log Z(x, πθt) can be approximated by finite-sample estimate τ log((cid:80) ri/K if τ is large. With these approximations, the gradient of (cid:101)L becomes equivalent to that of the following loss (which eri/τ /K), which can be further approximated by the mean reward = (cid:80) Figure 4: Empirical results for REC on ToolACE with Llama-3.2-3B-Instruct. Training reward curves are smoothed with running-average window of size 3. Details about REC-TwoSide and REC-Ring are provided in Appendix B.3. is the final version of Kimis OPMD): (cid:98)L = 1 (cid:88) 1 (ri r) log πθ(yi x) + τ 2K (cid:88) (cid:16) 1 log πθ(yi x) log πold(yi (cid:17)2 x) . In comparison, our analysis in Sections 2 and 3 suggests that this is in itself principled loss function for off-policy RL, adding mean-squared regularization loss to the vanilla REINFORCE loss. Metas AsymRE. AsymRE [Arnal et al., 2025] modifies REINFORCE by tuning down the baseline (from to τ ) in advantage calculation, which was motivated by the intuition of prioritizing learning from positive samples and justified by multi-arm bandit analysis in the original paper. We offer an alternative interpretation for AsymRE by rewriting its loss function: (cid:98)L = 1 (cid:88) (cid:16) (cid:17) (r ri τ ) log πθ(yi x) = 1 (cid:88) (ri r) log πθ(yi x) τ (cid:88) log πθ(yi x). Note that the first term on the right-hand side is the REINFORCE loss, and the second term serves as regularization, enforcing imitation of responses from an older version of the policy model. For the latter, we may also add term that is independent of θ to it and take the limit : 1 1 (cid:88) log πθ(yi (cid:104) πold( x) x) + 1 (cid:88) log πold(y πθ(y 1 x) x) log πold(yi (cid:16) = DKL πold( (cid:105) (cid:88) (cid:17) , x) = 1 πθ( x) x) log πold(yi πθ(yi x) x) which turns out to be finite-sample approximation of KL regularization."
        },
        {
            "title": "4.3 Understanding data-weighting methods",
            "content": "We now shift our attention to the second principle for augmenting REINFORCE, i.e., actively shaping the training data distribution. Pairwise weighting. Recall from Section 2 that we define the surrogate loss in Eq. (6) as an unweighted sum of pairwise mean-squared losses. However, if we have certain knowledge about which pairs are more informative for RL training, we may assign higher weights to them. This motivates generalizing (cid:80) i<j(ai 9 aj)2 to (cid:80) the steps in Section 2, we end up with i<j wi,j(ai aj)2, where wi,j} { are non-negative weights. Assuming that wi,j = wj,i and following g(cid:0)θ; x, yi, ri}1 { (cid:1) = 1 (cid:88) (cid:16) (cid:88) 1 1 In the special case where wi,j = wiwj, this becomes (cid:17)(cid:18) wi,j ri (cid:80) wi,jrj wi,j (cid:80) (cid:19) θ log πθ(yi x). (cid:16) (cid:88) = wj (cid:17) 1 (cid:88) 1 wi (cid:0)ri (cid:1) rw θ log πθ(yi x), where rw := (cid:80) wjrj wj (cid:80) . (9) Based on this, we investigate two REINFORCE-with-data-weighting (RED) methods. RED-Drop: sample dropping. The idea is to use filtered subset [K] of responses for training; for example, the Kimi-Researcher technical blog [Kimi-Team, 2025a] proposes to discard some negative samples strategically, as negative gradients increase the risk of entropy collapse. This is indeed special case of Eq. (9), by setting wi = K/ and 0 otherwise: for g(cid:0)θ; x, yi, ri}1 { (cid:1) = 1 (cid:88) S (ri ) θ log πθ(yi x), where = 1 (cid:88) ri. i (10) While this is no longer an unbiased estimate of policy gradient even if all responses are sampled from the current policy, it is still well justified by our off-policy interpretation of REINFORCE. RED-Weight: pointwise loss weighting. Another approach for prioritizing high-reward responses is to directly up-weight their gradient terms in Eq. (1a). To better understand the working mechanism of this seemingly heuristic method, we rewrite its policy update: (cid:88) = = 1 (cid:88) 1 wi(ri r) θ log πθ(yi x) = (cid:88) 1 rw + rw r) θ log πθ(yi x) wi(ri rw) θ log πθ(yi x) + (rw r) wi θ log πθ(yi x). wi(ri (cid:88) 1 This is the pairwise-weighted REINFORCE gradient in Eq. (9), plus regularization term (weighted by > 0) that resembles the one in AsymRE but prioritizes imitating higher-reward responses, echoing rw the finding from offline RL literature [Hong et al., 2023a,b] that regularizing against high-reward trajectories can be more effective than conservatively imitating all trajectories in the dataset. Implementation details. Below are the concrete instantiations adopted in our empirical studies: RED-Drop: When the number of negative samples in group exceeds the number of positive ones, we randomly drop the excess negatives so that positives and negatives are balanced. After this subsampling step, we recompute the advantages using the remaining samples, which are then fed into the loss. RED-Weight: Each sample is weighted by wi = exp(Ai/τ ), where Ai denotes its advantage estimate and τ > 0 is temperature parameter controlling the sharpness of weighting. This scheme amplifies high-advantage samples while down-weighting low-advantage ones. We fix τ = 1 for all experiments. Experiments. Figure 5 presents GSM8k results with Qwen2.5-1.5B-Instruct, which confirm the efficacy of RED-Drop and RED-Weight in on/off-policy settings, comparable to REC-OneSide-NoIS with enlarged (ϵlow, ϵhigh). Figure 6 reports larger-scale experiments on Guru-Math with Qwen2.5-7B-Instruct, where RED-Weight achieves higher rewards than GRPO, with similar KL distance to the initial policy. Figure 7 further validates the efficacy of RED-Weight on MATH with Llama-3.1-8B-Instruct; compared to GRPO and REC-OneSide-NoIS, RED-Weight achieves higher rewards with lower KL divergence, while maintaining more stable entropy and response lengths. Figure 5: Empirical performance of RED-Drop and RED-Weight on GSM8k with Qwen2.5-1.5B-Instruct, in both on-policy and off-policy settings. Training reward curves are smoothed with running-average window of size 3. Figure 6: Empirical results on Guru-Math with Qwen2.5-7B-Instruct. Training reward curves are smoothed with running-average window of size 3. 11 Figure 7: Comparison of RED-Weight, RECOneSide-NoIS, and GRPO on MATH with Llama3.1-8B-Instruct. Reported metrics for training include reward, KL distance to the initial model, entropy, and response length. We also report evaluation accuracy on the MATH500 subset."
        },
        {
            "title": "5 Related works",
            "content": "Off-policy RL for LLMs has been studied from various perspectives. Importance sampling has long been considered one foundational mechanism for off-policy RL; besides PPO and GRPO, recent extensions include GSPO [Zheng et al., 2025] and GMPO [Zhao et al., 2025] that work with sequence-wise probability ratios, CISPO [Chen et al., 2025] that clips probability ratios rather than token updates, decoupled PPO [Fu et al., 2025a] that adapts PPO to asynchronous RL, among others. AsymRE [Arnal et al., 2025] offers an alternative baseline-shift approach (with ad-hoc analysis for discrete bandit settings), while OPMD [Kimi-Team, 2025b] partly overlaps with our analysis up to Eq. (4) before diverging, as discussed earlier in Section 4.2. Contrastive Policy Gradient [Flet-Berliac et al., 2024] overlaps with our analysis up to Eq. (6), but it requires paired responses within the same micro-batch (in order to optimize the pairwise surrogate loss), rendering it less infra-friendly than REINFORCE variants. Other perspectives include learning dynamics of DPO and SFT [Ren and Sutherland, 2025], training offline loss functions with negative gradients on on-policy data [Tajwar et al., 2024], or improving generalization of SFT via probability-aware rescaling [Wu et al., 2025]. Another line of research integrates expert data into online RL [Yan et al., 2025, Zhang et al., 2025c, Fu et al., 2025b]. Our work contributes complementary perspectives to this growing toolkit for off-policy LLM-RL."
        },
        {
            "title": "6 Limitations and future work",
            "content": "While our work offers new off-policy interpretation for group-relative REINFORCE and shows its broad implications for LLM-RL, several limitations remain. (1) Our current analysis covers single/multi-step RL with response/trajectory-level rewards, and assumes access to multiple rollouts per query. Future work may expand its scope and applicability, e.g., generalizing to settings with step-level rewards or only one rollout per query. (2) Our analysis lacks formal guarantees for policy improvement or convergence. Future work may identify distributional assumptions that yield provable guarantees for REINFORCE variants in off-policy settings. (3) Our experiments focus on settings where training data is generated by older policy versions. Extensions to broader off-policy settings (e.g., advanced experience synthesis or incorporation of expert data) may reveal new insights. Addressing these limitations will further solidify the theoretical foundation and advance principled algorithm design for off-policy LLM-RL."
        },
        {
            "title": "References",
            "content": "Joshua Achiam, David Held, Aviv Tamar, and Pieter Abbeel. Constrained policy optimization. In Proceedings of the 34th International Conference on Machine Learning, volume 70 of Proceedings of Machine Learning Research, pages 2231. PMLR, 2017. Chenxin An, Zhihui Xie, Xiaonan Li, Lei Li, Jun Zhang, Shansan Gong, Ming Zhong, Jingjing Xu, Xipeng Qiu, Mingxuan Wang, and Lingpeng Kong. POLARIS: post-training recipe for scaling reinforcement learning on advanced reasoning models, 2025. URL https://hkunlp.github.io/blog/2025/Polaris. Charles Arnal, Ga ˇTtan Narozniak, Vivien Cabannes, Yunhao Tang, Julia Kempe, and Remi Munos. Asymmetric reinforce for off-policy reinforcement learning: Balancing positive and negative rewards. arXiv Preprint arXiv:2506.20520, 2025. Yuntao Bai, Andy Jones, Kamal Ndousse, Amanda Askell, Anna Chen, Nova DasSarma, Dawn Drain, Stanislav Fort, Deep Ganguli, Tom Henighan, et al. Training helpful and harmless assistant with reinforcement learning from human feedback. arXiv preprint arXiv:2204.05862, 2022. Aili Chen, Aonian Li, Bangwei Gong, Binyang Jiang, Bo Fei, Bo Yang, Boji Shan, Changqing Yu, Chao Wang, Cheng Zhu, et al. MiniMax-M1: Scaling test-time compute efficiently with lightning attention. arXiv preprint arXiv:2506.13585, 2025. Zhoujun Cheng, Shibo Hao, Tianyang Liu, Fan Zhou, Yutao Xie, Feng Yao, Yuexin Bian, Yonghao Zhuang, Nilabjo Dey, Yuheng Zha, Yi Gu, Kun Zhou, Yuqi Wang, Yuan Li, Richard Fan, Jianshu She, Chengqian Gao, Abulhair Saparov, Haonan Li, Taylor W. Killian, Mikhail Yurochkin, Zhengzhong Liu, Eric P. Xing, and Zhiting Hu. Revisiting reinforcement learning for LLM reasoning from cross-domain perspective. arXiv preprint arXiv:2506.14965, 2025. Karl Cobbe, Vineet Kosaraju, Mohammad Bavarian, Mark Chen, Heewoo Jun, Lukasz Kaiser, Matthias Plappert, Jerry Tworek, Jacob Hilton, Reiichiro Nakano, Christopher Hesse, and John Schulman. Training verifiers to solve math word problems. arXiv, 2021. Jeff Da, Clinton Wang, Xiang Deng, Yuntao Ma, Nikhil Barhate, and Sean Hendryx. Agent-RLVR: Training software engineering agents via guidance and environment rewards. arXiv, 2025. DeepSeek-AI. DeepSeek-R1: Incentivizing reasoning capability in LLMs via reinforcement learning. arXiv, 2025. Abhimanyu Dubey, Abhinav Jauhri, Abhinav Pandey, Abhishek Kadian, Ahmad Al-Dahle, Aiesha Letman, Akhil Mathur, Alan Schelten, Amy Yang, Angela Fan, et al. The Llama 3 herd of models. arXiv, 2024. Yannis Flet-Berliac, Nathan Grinsztajn, Florian Strub, Eugene Choi, Bill Wu, Chris Cremer, Arash Ahmadian, Yash Chandak, Mohammad Gheshlaghi Azar, Olivier Pietquin, and Matthieu Geist. Contrastive policy gradient: Aligning LLMs on sequence-level scores in supervised-friendly fashion. In Proceedings of the 2024 Conference on Empirical Methods in Natural Language Processing, pages 2135321370, 2024. Katerina Fragkiadaki. Natural policy gradients, TRPO, PPO. https://www.andrew.cmu.edu/course/ 10-703/slides/Lecture_NaturalPolicyGradientsTRPOPPO.pdf, 2018. Wei Fu, Jiaxuan Gao, Xujie Shen, Chen Zhu, Zhiyu Mei, Chuyi He, Shusheng Xu, Guo Wei, Jun Mei, Jiashu Wang, Tongkai Yang, Binhang Yuan, and Yi Wu. AReaL: large-scale asynchronous reinforcement learning system for language reasoning. arXiv, 2025a. Yuqian Fu, Tinghong Chen, Jiajun Chai, Xihuai Wang, Songjun Tu, Guojun Yin, Wei Lin, Qichao Zhang, Yuanheng Zhu, and Dongbin Zhao. SRFT: single-stage method with supervised and reinforcement fine-tuning for reasoning. arXiv, 2025b. Jiaxuan Gao, Wei Fu, Minyang Xie, Shusheng Xu, Chuyi He, Zhiyu Mei, Banghua Zhu, and Yi Wu. Beyond ten turns: Unlocking long-horizon agentic search with large-scale asynchronous rl. arXiv, 2025. 13 Yongxin Guo, Wenbo Deng, Zhenglin Cheng, and Xiaoying Tang. G2RPO-A: Guided group relative policy optimization with adaptive guidance. arXiv, 2025. Dan Hendrycks, Collin Burns, Saurav Kadavath, Akul Arora, Steven Basart, Eric Tang, Dawn Song, and Jacob Steinhardt. Measuring mathematical problem solving with the MATH dataset. NeurIPS, 2021. Zhang-Wei Hong, Pulkit Agrawal, Remi Tachet des Combes, and Romain Laroche. Harnessing mixed offline reinforcement learning datasets via trajectory weighting. In The Eleventh International Conference on Learning Representations, 2023a. Zhang-Wei Hong, Aviral Kumar, Sathwik Karnik, Abhishek Bhandwaldar, Akash Srivastava, Joni Pajarinen, Romain Laroche, Abhishek Gupta, and Pulkit Agrawal. Beyond uniform sampling: Offline reinforcement learning with imbalanced datasets. In Thirty-seventh Conference on Neural Information Processing Systems, 2023b. Jian Hu, Xibin Wu, Zilin Zhu, Xianyu, Weixun Wang, Dehao Zhang, and Yu Cao. OpenRLHF: An easy-touse, scalable and high-performance RLHF framework. arXiv preprint arXiv:2405.11143, 2024. Sham M. Kakade and John Langford. Approximately optimal approximate reinforcement learning. In International Conference on Machine Learning, 2002. Kimi-Team. Kimi-Researcher. https://moonshotai.github.io/Kimi-Researcher, 2025a. Kimi-Team. Kimi k1.5: Scaling reinforcement learning with LLMs. arXiv preprint arXiv:2501.12599, 2025b. Tomasz Korbak, Ethan Perez, and Christopher L. Buckley. RL with KL penalties is better viewed as bayesian inference. In Conference on Empirical Methods in Natural Language Processing, 2022. Xiao Liang, Zhong-Zhi Li, Yeyun Gong, Yang Wang, Hengyuan Zhang, Yelong Shen, Ying Nian Wu, and Weizhu Chen. SwS: Self-aware weakness-driven problem synthesis in reinforcement learning for LLM reasoning. arXiv Preprint arXiv:2506.08989, 2025. Weiwen Liu, Xu Huang, Xingshan Zeng, xinlong hao, Shuai Yu, Dexun Li, Shuai Wang, Weinan Gan, Zhengying Liu, Yuanqing Yu, Zezhong WANG, Yuxian Wang, Wu Ning, Yutai Hou, Bin Wang, Chuhan Wu, Wang Xinzhi, Yong Liu, Yasheng Wang, Duyu Tang, Dandan Tu, Lifeng Shang, Xin Jiang, Ruiming Tang, Defu Lian, Qun Liu, and Enhong Chen. ToolACE: Winning the points of LLM function calling. In The Thirteenth International Conference on Learning Representations, 2025a. Zihe Liu, Jiashun Liu, Yancheng He, Weixun Wang, Jiaheng Liu, Ling Pan, Xinyu Hu, Shaopan Xiong, Ju Huang, Jian Hu, Shengyi Huang, Siran Yang, Jiamang Wang, Wenbo Su, and Bo Zheng. Part I: Tricks or traps? deep dive into RL for LLM reasoning. arXiv preprint arXiv:2508.08221, 2025b. Ofir Nachum, Mohammad Norouzi, Kelvin Xu, and Dale Schuurmans. Bridging the gap between value and policy based reinforcement learning. In NIPS, 2017. Michael Noukhovitch, Shengyi Huang, Sophie Xhonneux, Arian Hosseini, Rishabh Agarwal, and Aaron In The Courville. Asynchronous RLHF: Faster and more efficient off-policy RL for language models. Thirteenth International Conference on Learning Representations, 2025. OpenAI. OpenAI o1 system card. arXiv Preprint arXiv:2412.16720, 2024. Long Ouyang, Pamela Mishkin, Jeff Wu, Mar, Jacob Hilton, Amanda Askell, and Paul Christiano. Training language models to follow instructions with human feedback. arXiv, 2022. Xuchen Pan, Yanxi Chen, Yushuo Chen, Yuchang Sun, Daoyuan Chen, Wenhao Zhang, Yuexiang Xie, Yilun Huang, Yilei Zhang, Dawei Gao, Weijie Shi, Yaliang Li, Bolin Ding, and Jingren Zhou. Trinity-RFT: general-purpose and unified framework for reinforcement fine-tuning of large language models. arXiv Preprint arXiv:2505.17826, 2025. Qwen-Team. Qwen2.5 technical report, 2025. URL https://arxiv.org/abs/2412.15115. 14 Rafael Rafailov, Archit Sharma, Eric Mitchell, Christopher Manning, Stefano Ermon, and Chelsea Finn. Direct preference optimization: Your language model is secretly reward model. In Thirty-seventh Conference on Neural Information Processing Systems, 2023. Yi Ren and Danica J. Sutherland. Learning dynamics of LLM finetuning. In The Thirteenth International Conference on Learning Representations, 2025. Pierre Harvey Richemond, Yunhao Tang, Daniel Guo, Daniele Calandriello, Mohammad Gheshlaghi Azar, Rafael Rafailov, Bernardo Avila Pires, Eugene Tarassov, Lucas Spangher, Will Ellsworth, Aliaksei Severyn, Jonathan Mallinson, Lior Shani, Gil Shamir, Rishabh Joshi, Tianqi Liu, Remi Munos, and Bilal Piot. Offline regularised reinforcement learning for large language models alignment. arXiv Preprint arXiv:2405.19107, 2024. David Rolnick, Arun Ahuja, Jonathan Schwarz, Timothy Lillicrap, and Gregory Wayne. Experience replay for continual learning. In Advances in Neural Information Processing Systems, volume 32, 2019. Tom Schaul, John Quan, Ioannis Antonoglou, and David Silver. Prioritized experience replay. arXiv Preprint arXiv:1511.05952, 2016. John Schulman, Sergey Levine, Pieter Abbeel, Michael Jordan, and Philipp Moritz. Trust region policy optimization. In International conference on machine learning, pages 18891897. PMLR, 2015. John Schulman, Filip Wolski, Prafulla Dhariwal, Alec Radford, and Oleg Klimov. Proximal policy optimization algorithms. arXiv preprint arXiv:1707.06347, 2017. Zhihong Shao, Peiyi Wang, Qihao Zhu, Junxiao Song Runxin Xu, Mingchuan Zhang, Y.K. Li, Y. Wu, and Daya Guo. DeepSeekMath: Pushing the limits of mathematical reasoning in open language models. arXiv, 2024. Guangming Sheng, Chi Zhang, Zilingfeng Ye, Xibin Wu, Wang Zhang, Ru Zhang, Yanghua Peng, Haibin Lin, and Chuan Wu. HybridFlow: flexible and efficient RLHF framework. arXiv, 2024. David Silver and Richard S. Sutton. Welcome to the era of experience. https://storage.googleapis. com/deepmind-media/Era-of-Experience%20/The%20Era%20of%20Experience%20Paper.pdf, 2025. Richard Sutton, Andrew Barto, et al. Reinforcement learning: An introduction. MIT press Cambridge, 1998. Fahim Tajwar, Anikait Singh, Archit Sharma, Rafael Rafailov, Jeff Schneider, Tengyang Xie, Stefano Ermon, Chelsea Finn, and Aviral Kumar. Preference fine-tuning of LLMs should leverage suboptimal, on-policy data. In Forty-first International Conference on Machine Learning, 2024. Sharan Vaswani, Olivier Bachem, Simone Totaro, Robert Muller, Shivam Garg, Matthieu Geist, Marlos C. Machado, Pablo Samuel Castro, and Nicolas Le Roux. functional mirror ascent view of policy gradient methods with function approximation. In Proceedings of the 25th International Conference on Artificial Intelligence and Statistics (AISTATS), volume 151, Valencia, Spain, 2022. PMLR. Leandro von Werra, Younes Belkada, Lewis Tunstall, Edward Beeching, Tristan Thrush, Nathan Lambert, Shengyi Huang, Kashif Rasul, and Quentin Gallouedec. TRL: Transformer reinforcement learning. https: //github.com/huggingface/trl, 2020. Weixun Wang, Shaopan Xiong, Gengru Chen, Wei Gao, Sheng Guo, Yancheng He, Ju Huang, Jiaheng Liu, Zhendong Li, Xiaoyang Li, et al. Reinforcement learning optimization for large-scale learning: An efficient and user-friendly scaling library. arXiv preprint arXiv:2506.06122, 2025. Ronald Williams. Simple statistical gradient-following algorithms for connectionist reinforcement learning. Machine learning, 8(3):229256, 1992. Yongliang Wu, Yizhou Zhou, Zhou Ziheng, Yingzhe Peng, Xinyu Ye, Xinting Hu, Wenbo Zhu, Lu Qi, MingHsuan Yang, and Xu Yang. On the generalization of SFT: reinforcement learning perspective with reward rectification. arXiv preprint arXiv:2508.05629, 2025. Jianhao Yan, Yafu Li, Zican Hu, Zhi Wang, Ganqu Cui, Xiaoye Qu, Yu Cheng, and Yue Zhang. Learning to reason under off-policy guidance. arXiv Preprint arXiv:2504.14945, 2025. Feng Yao, Liyuan Liu an Dinghuai Zhang, Chengyu Dong, Jingbo Shang, and Jianfeng Gao. Your efficient RL framework secretly brings you off-policy RL training. https://fengyao.notion.site/off-policy-rl, 2025. Qiying Yu, Zheng Zhang, Ruofei Zhu, Yufeng Yuan, Xiaochen Zuo, Yu Yue, Weinan Dai, Tiantian Fan, Gaohong Liu, Lingjun Liu, Xin Liu, Haibin Lin, Zhiqi Lin, Bole Ma, Guangming Sheng, Yuxuan Tong, Chi Zhang, Mofan Zhang, Wang Zhang, Hang Zhu, Jinhua Zhu, Jiaze Chen, Jiangjie Chen, Chengyi Wang, Hongli Yu, Yuxuan Song, Xiangpeng Wei, Hao Zhou, Jingjing Liu, Wei-Ying Ma, Ya-Qin Zhang, Lin Yan, Mu Qiao, Yonghui Wu, and Mingxuan Wang. DAPO: An open-source LLM reinforcement learning system at scale. arXiv preprint arXiv:2503.14476, 2025. Guibin Zhang, Hejia Geng, Xiaohang Yu, Zhenfei Yin, Zaibin Zhang, Zelin Tan, Heng Zhou, Zhongzhi Li, Xiangyuan Xue, Yijiang Li, Yifan Zhou, Yang Chen, Chen Zhang, Yutao Fan, Zihu Wang, Songtao Huang, Yue Liao, Hongru Wang, Mengyue Yang, Heng Ji, Michael Littman, Jun Wang, Shuicheng Yan, Philip Torr, and Lei Bai. The landscape of agentic reinforcement learning for LLMs: survey, 2025a. Kaiyan Zhang, Yuxin Zuo, Bingxiang He, Youbang Sun, Runze Liu, Che Jiang, Yuchen Fan, Kai Tian, Guoli Jia, Pengfei Li, et al. survey of reinforcement learning for large reasoning models. arXiv preprint arXiv:2509.08827, 2025b. Wenhao Zhang, Yuexiang Xie, Yuchang Sun, Yanxi Chen, Guoyin Wang, Yaliang Li, Bolin Ding, and Jingren Zhou. On-policy RL meets off-policy experts: Harmonizing supervised fine-tuning and reinforcement learning via dynamic weighting. arXiv preprint arXiv:2508.11408, 2025c. Yuzhong Zhao, Yue Liu, Junpeng Liu, Jingye Chen, Xun Wu, Yaru Hao, Tengchao Lv, Shaohan Huang, Lei Cui, Qixiang Ye, Fang Wan, and Furu Wei. Geometric-mean policy optimization. arXiv preprint arXiv:2507.20673, 2025. Chujie Zheng, Shixuan Liu, Mingze Li, Xiong-Hui Chen, Bowen Yu, Chang Gao, Kai Dang, Yuqiong Liu, Rui Men, An Yang, Jingren Zhou, and Junyang Lin. Group sequence policy optimization. arXiv preprint arXiv:2507.18071, 2025. 16 Extending Section 2.2 to multi-step RL This section extends the off-policy interpretation proposed in Section 2.2 to multi-step RL settings. Let us start by introducing some notations. In multi-step RL, the initial prompt is also regarded as the initial state s1 = x. rollout trajectory consisting of multiple turns of agent-environment interaction is denoted by = (s1, a1, s2, a2, . . . ) = (sℓ, aℓ)1 , ℓ T where sℓ is the state and aℓ is the action, i.e., an LLM response (akin to in Section 2.2). Let cℓ denote the cℓ) for some policy π. Throughout this section, we consider trajectorycontext up to step ℓ, so that aℓ level rewards r(x, x) denote the trajectory distribution induced by policy πθ at initial state ). Let ρθ( s1 = x. π( The following analysis focuses on the t-th iteration, updating the policy model from θt to θt+1. Step 1: surrogate objective and consistency condition. For the t-th iteration of policy optimization, consider the following KL-regularized objective: max θ J(θ; πθt) := Ex (cid:20) D ρθ ( x)[r(x, )] τ DKL (cid:0)ρθ( x) ρθt ( (cid:21) . x)(cid:1) (11) The optimal policy π and the induced trajectory distribution ρ satisfies the following: for any trajectory , ρθt( (cid:90) x) = ρ( Z(x, ρθt) := )/τ x)er(x, Z(x, ρθt) , where ρθt( x)er(x, )/τ = ρθt ( x)[er(x, )/τ ]. (12) (13) This is equivalent to the following: for any pair of trajectories T1 and T2, (cid:1) /τ . 2) (cid:0) r(x, 1) r(x, ρ( ρ( T1 T2 x) x) = ρθt( πθt( x) x) T1 T2 Taking logarithm of both sides and doing some rearrangement, we have equivalently r(x, T1) τ (cid:0) log ρ( x) T1 , we have Note that for trajectory log ρθt( T1 x)(cid:1) = r(x, T2) τ (cid:0) log ρ( T2 x) log ρθt( x)(cid:1). T2 (14) log ρ( x) log ρθt( x) = (cid:88) ℓ log π(aℓ cℓ) (cid:88) ℓ log πθt(aℓ cℓ) since the state-transition probability terms in log ρ( x) and log ρθt( x) cancel out. Step 2: surrogate loss with finite samples. Given trajectories from the same initial state s1 = x, we define the following mean-squared surrogate loss that enforces the consistency condition: (cid:98)L(θ; x, πθt) := 1 1 (cid:88) i<j where ai := r(x, Ti) (cid:16) (cid:88) τ aj)2 (ai (1 + τ )2 , log πθ(aℓ cℓ ) log πθt(aℓ (cid:17) cℓ ) . (cid:88) ℓ ℓ (15) (16) With infinite samples and sufficient coverage of the action space, the optimum of this surrogate loss would be the same as the optimal policy for the surrogate objective in Eq. (11). 17 Step 3: one gradient step of the surrogate loss. By the same trick as in Section 2.2, we have θ(ai aj)2(cid:12) (cid:12)θt = 2τ (cid:16) r(x, Ti) r(x, Tj) (cid:17)(cid:16) θ (cid:88) ℓ log πθ(aℓ )(cid:12) cℓ (cid:12)θt θ log πθ(aℓ j)(cid:12) cℓ (cid:12)θt (cid:17) , (cid:88) ℓ and θ (cid:88) 1 i<j aj)2 (ai (1 + τ ) (cid:12) (cid:12) (cid:12)θt = 2τ (1 + τ )2 (cid:88) (cid:0)r(x, Ti) r(x)(cid:1) (cid:88) ℓ θ log πθ(aℓ (cid:12) (cid:12) cℓ ) (cid:12)θt , where r(x) := (cid:80) In sum, the gradient of the surrogate loss in Eq. (16) becomes: 1 r(x, Tj)/K denotes the group mean reward in the last line. θ (cid:98)L(θ; x, πθt)(cid:12) (cid:12)θt = 2τ (1 + τ )2 1 (cid:88) (cid:0)r(x, 1 r(x)(cid:1) Ti) (cid:88) ℓ θ log πθ(aℓ cℓ ) (cid:12) (cid:12) (cid:12)θt . This motivates the following policy update step: g(cid:0)θ; x, {Ti, ri}1 (cid:1) = 2τ (1 + τ )"
        },
        {
            "title": "1\nK",
            "content": "(cid:88) (cid:0)r(x, 1 Ti) r(x)(cid:1) θ (cid:88) 1 ℓ log πθ(aℓ cℓ ), (17) which concludes our derivation of group-relative REINFORCE in multi-step RL settings."
        },
        {
            "title": "B Implementation details and additional experiments",
            "content": "We implement all algorithms with the Trinity-RFT framework [Pan et al., 2025], and run experiments on NVIDIA L20, H20, and A800 GPUs. See Tables 1 and 2 for detailed configurations of our experiments. B.1 Dataset details We provide additional descriptions of the datasets used in our experiments: GSM8k [Cobbe et al., 2021] is widely used benchmark with 8.5k grade-school math word problems, designed to test arithmetic reasoning and step-by-step problem solving. MATH [Hendrycks et al., 2021] covers algebra, geometry, probability, and number theory, containing 12.5k examples in total (7.5k for training and 5k for testing); it demands advanced symbolic reasoning beyond GSM8k. Guru [Cheng et al., 2025] is multi-domain reasoning dataset with 91.9k examples spanning math, code, science, logic, simulation, and tabular tasks; we use its math subset (around 54k samples), which introduces diverse problem formats for evaluating transfer of reasoning strategies. ToolACE [Liu et al., 2025a] is multilingual benchmark with around 11k synthetic samples designed to evaluate LLMs ability to solve tasks by selecting and invoking external tools via strict JSON-formatted function calls; we use 5k single-turn subset in our experiments. B.2 Understanding the synchronization parameters We parameterize rollout-training scheduling by two configuration parameters in Trinity-RFT: the synchronization interval (sync interval) and synchronization offset (sync offset). Their meanings are visualized in Figure 2 and explained in the following. The parameter sync interval specifies the number of generated rollout batches (which equals the number of gradient steps for training the policy model) between two consecutive executions of model weight synchronization. When sync interval = 1, the rollout and policy models synchronize after each gradient Table 1: Default hyperparameters. Deviations from defaults are noted in figure captions. GSM8K (Qwen2.5-1.5B) ToolACE (Llama-3.2-3B) Guru (Qwen2.5-7B) MATH (Llama-3.1-8B) Learning rate Batch size Weight decay Warmup steps Eval temperature Eval top-p Figures 6 1 10 96 8 0.01 0 1.0 1.0 3, 5, 9, 10, 11 1 10 96 8 0.01 0 N/A N/A 4 1 6 10 64 16 0.1 80 N/A N/A 5 7 10 64 16 0.1 40 0.6 1.0 7 Table 2: Other shared hyperparameters across all experiments. Parameter Optimizer (β1, β2) Gradient clipping Warmup style Weight-decay increment style Auxiliary LR decay style Training inference temperature Training inference top-p Value AdamW (0.9, 0.999) 1.0 constant constant exponential 1.0 1.0 step with one batch of samples, yielding strictly on-policy process (if we ignore the issue of precision mismatch between rollout and training engines [Yao et al., 2025]). When sync interval > 1, sync interval rollout batches are generated with stale model weights before synchronization, which accelerates the overall RL process through pipeline parallelism but incurs off-policyness. The parameter sync offset specifies the lag between the generation and consumption of each batch. More specifically, sync offset batches are generated and saved to the buffer before training is launched, which is also useful for reducing pipeline bubbles and improving hardware utilization [Noukhovitch et al., 2025]. In some of our experiments, we deliberately set sync offset to large value, in order to simulate scenario where reward signals from the environment are lagged. In general, with (sync interval, sync offset) = (m, n), the off-policyness of consumed batch with zero-index id corresponds to its temporal distance from the most recent synchronized policy is (l mod m)+n. For example, (4, 0) yields off-policyness 0, 1, 2, 3 within each interval, while (1, 4) yields constant offpolicyness of 4. B.3 REC with different clipping mechanisms In addition to one-side clipping investigated in Section 4, here we compare additional clipping mechanisms for the REC series, to understand how the geometry of clipping asymmetric vs. symmetric bounds and the presence of zero-gradient band affects the learning process. REC-TwoSide-IS/NoIS. We replace the mask mask3: in REC-OneSide-IS/NoIS in Eq. (8) with two-side πθ(yt πold(yt 3It turns out that REC-TwoSide-NoIS resembles the sPPO algorithm proposed by Vaswani et al. [2022], though derived x, y<t ) x, y<t ) (cid:16) = 1 1 + ϵhigh (18) (cid:102)M ϵlow (cid:17) 1 . with different rationales. Figure 8: visualization of activated gradient for various REC algorithms. Here, represents the advantage of specific token, and an arrow pointing to the right and annotated with > 0 means there is activated gradient that incentivizes increasing πθ when the token advantage is positive and the probability ratio πθ/πold lies in the corresponding interval. Two-side clipping imposes weaker regularization than one-side clipping does with the same clipping parameter (ϵlow, ϵhigh). This can potentially improve training efficiency, but might also be risky when the probability ratio πθ/πold goes far off. To compensate for this, we design REC-Ring. REC-Ring. margins ϵlow In addition to the inner band (1 ϵlow and ϵhigh ϵhigh. The REC-Ring mask is: ϵlow, 1 + ϵhigh) as in Eq. (18), we further specify outer safety (cid:16) = 1 (cid:99)M 1 (cid:17) 1 + ϵhigh ϵlow (cid:16) + 1 Ai > 0 and (cid:16) + 1 Ai < 0 and x, y<t ) x, y<t ) πθ(yt πold(yt πθ(yt πold(yt πθ(yt πold(yt x, y<t ) x, y<t ) x, y<t ) x, y<t ) (cid:17) ϵlow 1 1 + ϵhigh (cid:17) . (19) (20) (21) comparison of the clipping mechanisms are visualized in Figure 8. Note that REC-OneSide and REC-TwoSide can be regarded as special cases of REC-Ring. Experiments. We compare the following algorithms: REINFORCE, GRPO, REC-TwoSide-IS, RECTwoSide-NoIS, and REC-Ring-NoIS. Clipping parameters are set to (ϵlow, ϵhigh) = (0.2, 0.2), and for REC-Ring we additionally set (ϵlow, ϵhigh) = (0.6, 2.0). Figure 9 presents the empirical results. We observe that for REC-TwoSide, importance sampling is non-essential in all three settings, akin to the case of REC-OneSide. In addition, REC-TwoSide methods demonstrate fast policy improvement at the beginning but tend to collapse later on, whereas REC-Ring achieves better balance of convergence speed and stability. B.4 Ablation: the impact of learning rates Recall that in Section 4.1, we have demonstrated empirically the advantages of enlarging the clipping parameters ϵlow, ϵhigh for REC-OneSide-NoIS. One might wonder if the relatively weak performance of GRPO or REC-OneSide with conventional ϵlow = ϵhigh = 0.2 is genuinely rooted in the clipping mechanism itself, or simply due to the choice of small learning rate. 6 10 To answer this, we enhance the experiment of Figure 3 by sweeping learning rates over 6, 10 . The results are illustrated in Figure 10, which confirm that simply increasing the learning rate cannot bridge the performance gap between GRPO with ϵlow = ϵhigh = 0.2 and REC-OneSide-NoIS with ϵlow = 0.6, ϵhigh = 2.0. This shows that relaxing the clipping range acts as genuine improvement of regularization, rather than merely mimicking larger learning rate. 10 } { 1 5, 2 20 Figure 9: Comparison of REC variants on GSM8K with Qwen2.5-1.5B-Instruct under different off-policy settings. Evaluation accuracy, training reward, KL divergence (with respect to the initial model) and clipping fraction are reported. Training reward curves are smoothed with running-average window of size 3. 21 Figure 10: Comparison of GRPO and REC-OneSide-NoIS on GSM8K with Qwen2.5-1.5B-Instruct. Evaluation accuracy (left) and training reward (right) are reported for varying learning rates. Figure 11: Empirical results for OPMD and AsymRE (cf. Section 4.2) on GSM8K with Qwen2.5-1.5BInstruct under various off-policy settings. The regularization coefficient for OPMD and the baseline shift for AsymRE are both 0.1. Training reward curves are smoothed with running-average window of size 3. B.5 Experiments for OPMD and AsymRE Figure 11 presents empirical results for OPMD and AsymRE in various off-policy settings. It is worth noting that, while the analysis and experiments in their original papers [Kimi-Team, 2025b, Arnal et al., 2025] focus on setting that is effectively the same as our sync interval > 1 setting, our analysis and experiments have also validated their efficacy in sync offset > 1 scenarios. Summary: unified view of various algorithms For convenient reference, Table 3 summarizes the algorithms investigated in Section 4. 2 ) ) ( π ti ) ) < , < , ti ) ) < , t < , ti ( θ π ti ( π ti ( θ π ti ( π ti ) ) ) τ / ( = ) ( θ π ( ) ( θ π l ) ( θ π θ ) (cid:80) (cid:80) τK 2 + ) ) (cid:17) , j , j (cid:80) (cid:80) r ( ( τ ) < , ) < , ) < , ti ( θ π ti ( θ π l ti ( θ π i ( θ π ) ( θ π ) θ θ t θ (cid:80) (cid:80) 1K (cid:80) (cid:80) 1K r ( ( (cid:80) 1K (cid:80) 1K (cid:80) (cid:80) 1K , ) ( θ π i ( θ π l θ ) θ ) ( (cid:80) 1 r ( (cid:80) = (cid:16) (cid:17) , w (cid:80) (cid:16) (cid:80) 1K = = = = (cid:98)L = (cid:98)L = = g -"
        },
        {
            "title": "W\nD\nE\nR",
            "content": "E F R h w - w P D - d i R y D s n a a e A o - e E - - e E - G p c e a e 23 . 4 t n e i v m r a y m : 3 a o / e r t g o t g A"
        }
    ],
    "affiliations": [
        "Alibaba Group",
        "UCLA"
    ]
}