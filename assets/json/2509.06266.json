{
    "paper_title": "Spatial Reasoning with Vision-Language Models in Ego-Centric Multi-View Scenes",
    "authors": [
        "Mohsen Gholami",
        "Ahmad Rezaei",
        "Zhou Weimin",
        "Yong Zhang",
        "Mohammad Akbari"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Understanding 3D spatial relationships remains a major limitation of current Vision-Language Models (VLMs). Prior work has addressed this issue by creating spatial question-answering (QA) datasets based on single images or indoor videos. However, real-world embodied AI agents such as robots and self-driving cars typically rely on ego-centric, multi-view observations. To this end, we introduce Ego3D-Bench, a new benchmark designed to evaluate the spatial reasoning abilities of VLMs using ego-centric, multi-view outdoor data. Ego3D-Bench comprises over 8,600 QA pairs, created with significant involvement from human annotators to ensure quality and diversity. We benchmark 16 SOTA VLMs, including GPT-4o, Gemini1.5-Pro, InternVL3, and Qwen2.5-VL. Our results reveal a notable performance gap between human level scores and VLM performance, highlighting that current VLMs still fall short of human level spatial understanding. To bridge this gap, we propose Ego3D-VLM, a post-training framework that enhances 3D spatial reasoning of VLMs. Ego3D-VLM generates cognitive map based on estimated global 3D coordinates, resulting in 12% average improvement on multi-choice QA and 56% average improvement on absolute distance estimation. Ego3D-VLM is modular and can be integrated with any existing VLM. Together, Ego3D-Bench and Ego3D-VLM offer valuable tools for advancing toward human level spatial understanding in real-world, multi-view environments."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 8 ] . [ 1 6 6 2 6 0 . 9 0 5 2 : r Spatial Reasoning with Vision-Language Models in Ego-Centric Multi-View Scenes Mohsen Gholami1, Ahmad Rezaei1, Zhou Weimin2, Yong Zhang1, Mohammad Akbari1 1 Huawei Technologies Canada, 2 Huawei Cloud Code Project Page Ego3D-Bench Figure 1: Ego3D-Bench is the first 3D spatial benchmark for VLMs using ego-centric multi-view images. It spans egoand object-centric perspectives across 5 categories. significant gap exists between human and VLM performance; our method, Ego3D-VLM, consistently narrows this gap."
        },
        {
            "title": "Abstract",
            "content": "Understanding 3D spatial relationships remains major limitation of current Vision-Language Models (VLMs). Prior work has addressed this issue by creating spatial question-answering (QA) datasets based on single images or indoor videos. However, real-world embodied AI agentssuch as robots and self-driving carstypically rely on ego-centric, multi-view observations. To this end, we introduce Ego3D-Bench, new benchmark designed to evaluate the spatial reasoning abilities of VLMs using ego-centric, multi-view outdoor data. Ego3D-Bench comprises over 8,600 QA pairs, created with significant involvement from human annotators to ensure quality and diversity. We benchmark 16 SOTA VLMs, including GPT-4o, Gemini1.5-Pro, InternVL3, and Qwen2.5-VL. Our results reveal notable performance gap between human level scores and VLM performance, highlighting that current VLMs still fall short of human level spatial understanding. To bridge this gap, we propose Ego3D-VLM, post-training framework that enhances 3D spatial reasoning of VLMs. Ego3D-VLM generates cognitive map based on estimated global 3D coordinates, resulting in 12% average improvement on multi-choice QA and 56% average improvement on absolute distance estimation. Ego3D-VLM is modular and can be integrated with any existing VLM. Together, Ego3D-Bench and Ego3D-VLM offer valuable tools for advancing toward human level spatial understanding in real-world, multi-view environments. Preprint. Under review."
        },
        {
            "title": "Introduction",
            "content": "1 3D spatial understanding is critical capability for embodied AI agents operating in the real world [3, 36, 32]. This includes perceiving the location of surrounding objects, estimating their distances, and reasoning about their motion. VLMs have recently emerged as powerful tools to integrate visual perception and language reasoning, making them promising components for building intelligent embodied AI systems [20, 16]. Thus, enhancing and evaluating the spatial understanding abilities of VLMs has become an increasingly important research direction [30, 5, 4, 7, 17, 31, 34, 33, 28, 25]. Recent benchmarks and datasets on spatial understanding primarily focus on spatial reasoning from single images [5, 4, 17] or videos captured in static, indoor environments [37, 7]. In these cases, spatial understanding is framed around passive observations, where single camera moves in room to create video, and the model is expected to infer spatial relationships in relatively static scene. This setup differs fundamentally from the perceptual experience of real-world embodied agents, such as autonomous vehicles or mobile robots. These agents rely on ego-centric, multi-view observations [20], typically provided by multiple cameras simultaneously capturing the front, sides, and rear views of their surroundings. These views are not interchangeable or purely visual; they carry explicit spatial semantics tied to the agents frame of reference. For example, left and right refer to fixed directions relative to the agents body, and must be interpreted consistently over time as the agent moves through dynamic environments. This distinction is crucial: while prior video-based datasets may offer multiple viewpoints, they do not reflect the structured, directional, and temporally evolving nature of ego-centric multi-view inputs. Moreover, existing benchmarks do not evaluate the ability of VLMs to reason across these spatially grounded perspectives in dynamic, real-world scenes. This gap motivates the need for new benchmarks that better align with the spatial reasoning demands of embodied agents. To this end, we introduce Ego3D-Bench, benchmark of 8,600 question-answer (QA) pairs carefully curated from the validation set of three public datasets: NuScenes [1], Waymo Open Dataset [29], and Argoverse 1 [2]. Human annotators played central role in both the dataset construction and the rigorous quality review process to ensure the reliability of the benchmark. We focused specifically on ego-centric multi-view tasks, rather than building general-purpose benchmark, to complement existing monocular spatial benchmarks (e.g., VSI-Bench [37]). As such, we excluded questions that can be answered by examining each view independently across multiple images (e.g., counting objects in each image) or by relying on general knowledge of LLMs (e.g., estimating the size of well-known objects). To our knowledge, Ego3D-Bench is the first benchmark to evaluate spatial reasoning of VLMs given ego-centric multi-view inputs. We evaluated 16 SOTA VLMs, including generalist and 3D spatial models on Ego3D-Bench, revealing significant gap between human performance and current VLMs. We hypothesize that key limitation lies in the inability of VLMs to construct coherent world model from multi-view images. In contrast, humans naturally integrate visual information from their left, right, and front views into unified spatial representation, enabling real-time reasoning and navigation. Prior work has attempted to bridge this gap by first generating 3D point-cloud [30, 14, 13, 39, 12] or rendering bird-eye-view (BEV) image of the scene [23]. Although point-clouds and BEV images offer rich spatial information, they are challenging to reconstruct in dynamic environments, struggle with sparse multi-view inputs, and significantly increase inference timeoften by factor of ten [42]. To this end, we propose, Ego3D-VLM, post-training method that improves 3D spatial understanding of VLMs. The main idea of Ego3D-VLM is to create textual cognitive map of the surrounding. The textual cognitive map defines coordinate system center on the ego and locates important object (i.e., those referred to in the input prompt) in 3D coordinate space. Unlike point-clouds and BEV image methods [30, 23], our cognitive map only focuses on referred objects, making the number of input tokens significantly smaller and enabling efficient reasoning. Given multi-view images as input, we first use referring expression comprehension (REC) models to find the 2D location of referred expressions in pixel space. We also use metric depth estimator to estimate the depth values. We then convert the 2D points to 3D points in camera coordinate space and transform 3D points from all views to the global coordinate space (i.e., front camera coordinate space). We define cognitive map generator function that returns textual cognitive map given 3D coordinates. The textual cognitive map defines coordinate space for the VLM and organizes detected objects (expressions) based on the view-point. The textual cognitive map consistently improves SOTA VLMs on spatial reasoning. In summary, our work has three main contributions: 2 We introduce Ego3D-Bench, an ego-centric multi-view benchmark for evaluating the 3D spatial understanding of VLMs. We introduce Ego3D-VLM, plug-and-play post-training framework to enhance 3D spatial understanding of VLMs, especially in ego-centric multi-view scenarios. Through extensive experiments, we demonstrate that Ego3D-VLM significantly improves SOTA generalist as well as 3D spatial VLMs in 3D reasoning."
        },
        {
            "title": "2 Related Work",
            "content": "3D Spatial Benchmarks for VLMs. VSI-Bench [37], CA-VQA [7], Q-Spatial-Bench [17], SpatialRGPT-Bench [5], and All-Angle Bench [38] are five recent spatial benchmarks for VLMs. Q-Spatial-Bench and SpatialRGPT-Bench focus on single-view images. VSI-Bench and CA-VQA focus on video data captured from indoor static scenes. As noted, this setup differs fundamentally from the perceptual experience of real-world embodied agents, i.e., ego-centric multi-view images. All-Angle Bench is the first multi-view benchmark for VLMs. However, in their setup multiple cameras look at scene from different directioni.e., setup for surveillance cameras and motion capture systems. Ego3D-Bench is the first ego-centric multi-view benchmark for VLMs created from dynamic outdoor scenes. 3D Spatial VLMs. These modelsalso referred to as 3D-LLMs or 3D-MLLMsaim to perform tasks such as 3D grounding, spatial reasoning, depth estimation, and distance measurement. We categorize existing approaches into two main groups: (1) models that take point-clouds as input or reconstruct them from multi-view images, and (2) models that operate directly on image data. The first group includes 3D-LLM [13], 3D-CLR [12], LEO [14], SpatialLM [30], and ChatScene [39]. While point-cloud representations offer rich spatial information, they are difficult to reconstruct in dynamic scenes, often struggle with sparse multi-view data, and significantly increase inference time (often by over 10). The second group includes models such as LLaVA-3D [42], Video-3D LLM [41], GPT4Scene [23], MM-Spatial [7], SpatialVLM [4], SpatialRGPT [5], and SpatialPIN [19]. Our work falls in this image-based category. LLaVA-3D and Video-3D LLM use depth maps and camera poses for 3D positional encoding. GPT4Scene leverages BEV to address 3D queries. However, these models are primarily trained on indoor, static scenes and are limited in handling quantitative spatial relationships such as object distances. SpatialVLM addresses this limitation by introducing synthetic data generation pipeline, enabling more robust spatial reasoning [26]. SpatialRGPT enhances input representation by incorporating region proposals alongside the original image. MM-Spatial proposes VLM that supports Chain-of-Thought spatial reasoning involving 2D grounding and depth estimation, and can also leverage depth input via tool-use. Different from SpatialRGPT, SpatialVLM, and MM-Spatial, our proposed Ego3D-VLM is post-training method, can be applied to any existing VLM, and enhances spatial understanding of VLMs and outperforms prior works on ego-centric multi-view reasoning."
        },
        {
            "title": "3 Ego3D-Bench",
            "content": "Ego3D-Bench is designed to quantitatively evaluate the 3D spatial understanding of VLMs from multiview outdoor images. major distinction between Ego3D-Bench and previous works is its focus on ego-centric multi-view dataan approach particularly relevant for applications in autonomous driving and robotics. Ego3D-Bench contains over 8,600 QA pairs in five distinct categories. This benchmark is constructed from the validation sets of three prominent outdoor multi-view datasets including nuScenes [1], Waymo Open Dataset [29], and Argoverse 1 [2], featuring 6, 7, and 5 distinct camera views, respectively (Figure 2). These datasets cover diverse outdoor environments, including urban areas, highways, and rural regions. Ego3D-Bench leverages the multi-view nature of these datasets to formulate questions that require fine-grained visual comprehension across different viewpoints as well as reasoning over 3D spatial relationships. In this section, we describe the detailed process of constructing the questions of Ego3D-Bench (Figure 2). 3.1 Benchmark Construction Creation of Source Files: Outdoor datasets such as the ones used in this work, unlike their indoor counterparts, often feature multiple instances of the same object within each scene. While indoor 3 Figure 2: Overview of our benchmark creation pipeline. Human annotators played key role throughout the process. On the right, we show the distribution of the samples in our Ego3D-Bench. Ego.: Ego-centric, Obj.: Object-centric. environments typically contain unique itemssuch as single oven or television per roomoutdoor scenes commonly include numerous similar objects, like multiple cars or pedestrians. This makes it challenging to uniquely reference target object in the scene. Thus, annotators begin by carefully reviewing each scene to identify unique objects (called \"Filtering\" in Figure 2). They then compose concise captions describing these objects (called \"Captioning\" in Figure 2). The descriptions are designed to be short, yet discriminative such that each object can be uniquely identified. Furthermore, the ground-truth 3D annotations (bounding boxes) are collected from the source datasets. Object captions, 3D annotations, and the camera view from which the object is visible are used to construct source file for QA creation. Creation of QAs: Each question category follows predefined template with placeholders for object names, such as How far is <obj1> in <view1> from <obj2> in <view2>? (see Appendix for all question templates). Each question is constructed by placing the generated object annotation and camera views from the source file in the question template. To generate the answers, we use rule-based functions that leverage ground-truth 3D annotations. For challenging categories, such as motion reasoning, final human review is conducted to ensure the accuracy of the QA pairs. 3.2 Benchmark Details Figure 3 illustrates different question categories in Ego3D-Bench. We emphasize on questions that require understanding the 3D world space by analyzing and consolidating information from multiple views. Thus, we exclude questions that can be answered using single-view or by analyzing each view independently (e.g., counting the number of objects), as they do not contribute to multi-view spatial reasoning. We define question from the ego-perspective and from the perspective of objects in the scene. To clearly indicate the perspective of each question, we categorize them into ego-centric or object-centric. In the following, we describe the five tasks (each composed of ego-centric and object-centric types) in our benchmark. (1) Absolute Distance Measurement. This category asks the VLM to estimate the absolute metric distance between the ego and another object in the scene or between two different objects from different camera views. As shown in Figure 3, this category is designed in two forms of multi-choice QA and absolute meter. (2) Relative Distance Measurement. In this task, the VLM is asked to determine which of two objects is closereither to the ego or to specific object, designed in the form of two-choice QA. (3) Localization. This category is only object-centric and assesses the VLMs ability to localize objects within scene. Specifically, the model is asked to infer the location of object-1 from the perspective of object-2 in the form of multi-choice QA. (4) Motion Reasoning. This category defines coordinate system using cardinal directions and asks the VLM that if the ego or object-1 moves in direction, whether or not it gets closer to or farther away from object-2. Answering this yes/no question requires mapping the spatial relationship between the objects and how the distances would change if one object moves in specific direction. (5) Travel Time. Given specific motion speed, this category asks (via multi-choice QA) to estimate the required time to move from the location of either the ego or object-1 to the location of object-2. 4 Figure 3: Egoand object-centric samples from each category in Ego3D-Bench. Evaluation Metrics. We design most of the questions as multi-choice QA and use accuracy as the evaluation metric. We also have two absolute distance estimation for which we use root mean squared error (RMSE) in meters as the evaluation metric."
        },
        {
            "title": "4 Post-Training 3D Spatial Understanding: Ego3D-VLM",
            "content": "Figure 4 shows an overview of our proposed framework, Ego3D-VLM. Given set of multi-view images and natural language prompt, we use REC model to detect all objects mentioned in the prompt. For each camera view {1, 2, . . . , }, the REC model returns set of detected objects: O(v) = (cid:110)(cid:16) b(v) , c(v) (cid:17)(cid:111)N (v) i=1 , (1) R2 to create list of 2D centers of the objects in the pixel space. R4 denotes the 2D bounding box coordinates for the i-th object in view v, and c(v) where b(v) is the corresponding referring expression match. We compute the 2D pixel-space center of each bounding box as u(v) Camera Coordinate Transformation. To obtain 3D spatial information, we use metric depth estimator to predict dense depth map D(v) RHW for each view v. For each detected object center u(v) = (xi, yi), we extract the corresponding depth value d(v) = D(v)(xi, yi). We then project each center point into the 3D camera coordinate system using the camera intrinsics (v) R33: cam,i = d(v) p(v) (cid:16) (cid:35) (v)(cid:17)1 (cid:34)xi yi 1 R3. (2) Next, we transform the 3D point from the local to the global coordinate system, i.e., defined as the front camera view point coordinate system. This replicates the human perception mechanism given multi-view cameras by using the front view as the reference building the 3D world based on that. Using the rotation matrix R(v) R33 and translation vector (v) R33, we have: p(v) global,i = (cid:20)R(v) (v) 1 0 (cid:21) (cid:21) (cid:20) p(v) cam,i R4. (3) This process effectively simulates human spatial perception by leveraging multi-view images to construct unified 3D representation of the scene. Relational Scaling. Humans estimate object sizes using known referencese.g., knowing person is 1.7 tall helps infer the size of nearby objects. Inspired by this, we scale 3D points p(v) global,i using familiar object categories (e.g., sedans, humans, bikes) identified in few representative frames across all camera views. We compute the average observed height hest and scale all 3D points by = hcs/hest, where hcs is the canonical common sense height (e.g., 1.7 for humans). This yields scaled,i = p(v) p(v) Creating Cognitive Map. We define cognitive map generator function, Fcog, which takes as input the set of 3D global coordinates and corresponding referring expressions for all detected objects global,i, producing physically plausible scales without ground-truth depth. Figure 4: An overview of Ego3D-VLM, our post-training 3D spatial understanding framework. across all camera views. Specifically, for each object detected from view v, we denote its global 3D position as p(v) . The function outputs textual cognitive map C, defined as: global,i and its matched referring expression as c(v) = Fcog (cid:18)(cid:110)(cid:16) global,i, c(v) p(v) (cid:19) . (cid:17)(cid:111) i,v (4) Fcog constructs an ego-centric world model centered on the agent. It integrates multi-view detections and links each referred object to its spatial position and originating viewpoint. The resulting cognitive map captures both linguistic references and spatial semantics in compact, human-interpretable formenabling grounded reasoning and situational awareness. Figure 5 illustrates sample prompt, the corresponding multi-view images, and the generated cognitive maps. Given VLM as that answers queries using visual and textual contexts, it takes as input the cognitive map C, set of multi-view images = {I (v)}v, and natural language query q, and returns an answer a: = V(C, I, q). (5) The cognitive map provides structured spatial grounding, while supplies rich visual cuessuch as appearance, color, and fine-grained contextnot captured in C. Together, they guide the VLM in interpreting and answering the query."
        },
        {
            "title": "5 Evaluation on Ego3D-Bench",
            "content": "In this section, we present comprehensive evaluation of VLMs on Ego3D-Bench. We organize our experiments into four parts (1) benchmarking generalist VLMs (i.e., models trained for general vision-language tasks), (2) benchmarking 3D-VLMs (i.e., models trained specifically for 3D spatial understanding), (3) benchmarking VLM+Depth+REC (i.e., generalist VLMs augmented with depth and REC tools), and (4) ablation studies. We use fixed R1-style prompt [11] in the evaluations of all models (details in the Appendix). We use Grounding-Dino-Base [27] as the REC model and Depth-Anything-V2-Metric-Large [6] as the metric depth estimator in all experiments. Chance/Human Performance Levels. We provide frequency-based random selection as the chance level baseline for the multi-choice questions. Furthermore, we conduct human evaluation on multichoice questions, where 10% of the questions from each category are randomly sampled and evaluated by each human annotator. Table 1 presents the results of this analysis. While humans can accurately answer the questions that require reasoning about relative location of the objects in space, their performance degrades in questions that require estimation of the exact distance between objects. This highlights the challenging nature of accurate distance estimation. 5.1 Benchmarking Generalist VLMs From the closed-source models, we use GPT-4o [22] and Gemini-2-Flash [8]. From the open-source models, we select three competitive families of open-source models: InternVL3 [9], Qwen2.5-VL [24], and Ovis2 [18]. Qualitative examples, inference time analysis, and numerical results on more open-source models are given in the Appendix. 6 Table 1: Comparison results of generalist VLMs and our method (Ego3D-VLM) on Ego3D-Bench. Accuracy (%) RMSE Model Human Level Chance Level Closed-source Models Ego Obj. Travel Ego Obj. Ego Obj. Dist. Dist. Loc. Mot. Mot. Time Rel. Rel. Avg. Dist. Dist. Avg. 100 95.3 78.2 57.1 100 25.8 25.5 24.3 49.7 50.6 85.0 94.1 85.3 49.9 50.1 37. Ego Obj. 72.7 24.0 - - - - - - 51.4 38.4 47.7 86.9 71.1 GPT-4o Ego3D-VLM GPT-4o 76.3 58.9 57.3 87.3 89.7 Gemini-1.5-Pro 51.3 35.8 49.4 88.4 70.6 Ego3D-VLM Gemini-1.5-Pro 65.0 62.0 67.2 93.9 91. Qwen2.5 Family Qwen2.5-3B Ego3D-VLM Qwen2.5-3B Qwen2.5-7B Ego3D-VLM Qwen2.5-7B Qwen2.5-32B Ego3D-VLM Qwen2.5-32B Qwen2.5-72B Ego3D-VLM Qwen2.5-72B 21.5 29.4 28.8 50.3 41.9 35.0 30.0 29.8 52.4 56.4 32.7 31.5 30.5 45.9 44.0 59.4 54.5 33.1 62.3 58.2 45.4 40.7 49.6 75.6 74.1 63.7 62.6 54.5 87.7 86.2 42.4 38.6 54.8 86.8 68.9 62.1 61.4 58.2 94.0 84.5 InternVL3 Family 25.8 28.7 29.8 54.1 54.8 InternVL3-8B Ego3D-VLM InternVL3-8B 65.4 56.1 37.0 73.0 71.4 InternVL3-14B 46.0 35.6 35.9 63.2 65.9 Ego3D-VLM InternVL3-14B 70.3 60.9 50.5 79.8 83.1 InternVL3-38B 35.4 31.0 39.4 66.6 64.9 Ego3D-VLM InternVL3-38B 55.1 64.5 53.4 87.2 88.9 InternVL3-78B 54.6 48.4 50.3 77.7 70.0 Ego3D-VLM InternVL3-78B 68.3 62.7 62.9 91.6 89.2 Ovis2 Family Ovis2-4B Ego3D-VLM Ovis2-4B Ovis2-8B Ego3D-VLM Ovis2-8B Ovis2-16B Ego3D-VLM Ovis2-16B 29.8 28.9 18.4 47.5 48.1 62.1 46.9 20.1 49.1 51.9 25.6 28.6 31.1 45.3 51.4 64.9 54.9 33.5 57.2 57.1 41.7 36.5 41.1 50.5 52.5 63.4 58.1 43.7 53.9 73.4 35.8 59.9 37.7 58. 30.9 29.6 34.5 49.4 40.1 40.8 38.5 56.0 36.1 49.0 41.6 50.2 38.0 51.9 44.8 55.1 36.9 33.3 31.4 46.1 27.9 51.1 65.9 84.2 56.7 70.6 85.3 73.2 55.5 71.4 57.5 66.2 80.6 73.1 29.5 19.2 8.5 7.4 6.3 8.4 10.7 28.6 19.6 7.2 7.8 6.6 54.1 56.1 39.1 60.1 62.0 44.4 43.2 66.5 41.1 50.5 66.9 54.3 54.0 79.0 57.3 62.6 72.0 65.5 53.3 80.5 58.0 63.1 76.6 69. 49.9 65.2 43.1 63.5 66.0 60.1 55.5 70.1 51.7 63.0 70.7 66.1 61.0 77.3 51.7 66.6 76.5 68.0 57.0 76.6 59.9 66.3 78.2 71.8 30.5 33.7 32.1 12.7 12.5 12.6 25.1 35.5 30.3 10.9 9.5 8.1 21.2 10.4 15.8 11.6 15.9 13.7 10.3 22.2 16.2 7.5 8.3 6.8 15.2 39.1 27.2 8.0 6.8 9.0 10.6 24.5 17.6 7.7 8.8 6.6 42.2 25.4 8.6 8.5 8.2 8.7 12.0 15.5 13.8 7.4 8.1 6.8 54.2 70.3 41.8 62.0 60.4 48.2 50.7 68.2 41.5 65.1 71.0 56.2 50.9 74.9 47.0 61.0 82.9 60.9 17.1 29.5 23.3 10.4 8.5 6.5 11.5 30.2 20.8 7.8 6.0 9.5 10.8 16.6 13.7 7.4 8.3 6.6 Figure 5: Example question with associated textual and visual cognitive maps. As given in Table 1, the performance of the VLMs varies considerably across different model parameter sizes. Smaller models (e.g., 3B and 8B) operate near chance level, indicating limited capacity for multi-view 3D reasoning. In contrast, larger models demonstrate substantial improvements over the chance level, yet still exhibit noticeable gap when compared to human level performance. Furthermore, our proposed Ego3D-VLM provides significant improvement across all model sizes and tasks (average 56% relative improvement in RMSE and 12% in Accuracy), underscoring the importance of providing structured spatial representations to the model in 3D understanding tasks. Performance Analysis. Figure 6 illustrates the average performance of the four leading models (GPT-4o, Gemini-1.5-Pro, InternVL3-78B, and Qwen2.5-VL-72B) with and without the integration of Ego3D-VLM across all multiple-choice questions. Travel time, localization, and object-centric absolute distance are the most challenging categories for VLMs with 40%-45% average accuracy with the largest models of distinct families. We argue that these categories require intricate spatial reasoning 7 and the ability to construct an accurate mental map based on the relative positioning of objects. While humans can effortlessly build such mental maps and achieve perfect accuracy, current VLMs struggle to replicate this level of spatial understanding, highlighting key area for further development. In the absolute distance measurement task, the incorporation of 3D location data through the cognitive map substantially narrows the gap between model performance and human level accuracy. Notably, for object-centric absolute distance estimation, VLMs augmented with Ego3D-VLM even surpass human performance. This outcome is anticipated, as human approximation of 3D distances in object-centric scenarios is prone to substantial error without explicit 3D spatial information. Conversely, in the localization task, models continue to fall short of human proficiency, even with cognitive map support. Blind VLM Performance. This baseline evaluates how much spatial reasoning can be achieved by VLMs using only textual input, without any visual information, relying solely on their world knowledge [21]. We report the average performance of GPT-4o and Gemini-1.5-Pro models. The blind VLMs perform 5% worse than vision-enabled VLMs (53.8% vs. 58.8%) and 16.4% better than chance level. These results are consistent with findings from prior singleview and video-based spatial reasoning benchmarks [5, 37]. Figure 6: Average performance of leading VLMs (GPT-4o, Gemini-1.5Pro, InternVL3-78B, and Qwen2.5VL-72B) with/without Ego3D-VLM vs. chance and human levels on each category of Ego3D-Bench. Table 2: Performance of cognitive map fed to LLM. Blind Ego3D-VLM Performance. In this section, we perform an experiment by feeding the cognitive map to text only LLM. Since InternVL3-8B/14B use Qwen2.5-7B/14B as their backbone LLM, we use the same model as the text only LLM for fair comparison. Table 2 shows that Ego3D-VLM with VLM outperforms Ego3D-VLM with LLM since VLMs can ignore false positives in the cognitive map and remain robust when false negatives occur, whereas LLMs suffer performance drops in those cases. Only Mult. Choice Abs. Ans. (RMSE) LLM (Acc. ) 27.2 11.4 8.0 17.6 10.8 8.5 InternVL3-8B Ego3D-VLMInternVL3-8B Ego3D-VLMInternVL3-8B InternVL3-14B Ego3D-VLMInternVL3-14B Ego3D-VLMInternVL3-14B 43.1 53.1 60.1 51.7 55.6 68.0 5.2 Benchmarking 3D-VLMs 3D-VLM models have been trained on datasets designed for 3D spatial understanding, such as absolute distance estimation and relative location inference. We benchmark SpatialRGPT and two checkpoints trained with the Spatial-VLM framework [4]: SpaceThinker-Qwen2.5-3B [26] and SpaceQwen2.5-3B. The SpatialRGPT [5] model assumes that specific regions of the image are annotated with bounding boxes and is trained to answer 3D questions based on these regions. To evaluate SpatialRGPT on our Ego3D-Bench, we reformat the input: object names in the questions are replaced with placeholder labels (e.g., region-i), and list of corresponding region captions is passed to the REC model to generate bounding boxes. These estimated bounding boxes were then overlaid on the images before being fed to SpatialRGPT. Table 3 presents the performance of the above-mentioned 3D-VLMs on our Ego3D-Bench. SpaceThinker-Qwen2.5-3B achieves the highest overall performance, surpassing both SpatialRGPT with 8B parameters and other generalist VLMs of similar scale. This outcome highlights the critical role of dedicated 3D spatial pretraining and end-to-end architecture in advancing VLM capabilities for spatial reasoning tasks. Moreover, augmenting SpaceThinker with Ego3D-VLM leads to an average improvement of 3% on multiple-choice questions and reduction of more than 4 meters in absolute distance RMSE, emphasizing the effectiveness of our proposed solution. 5.3 Benchmarking VLM+Depth+REC. This category enhances the base VLM with metric depth estimator and REC model. We pass each query to the REC model [27] to extract bounding boxes of the referred objects, and use metric depth 8 Table 3: Comparison results of 3D-VLMs and our method (Ego3D-VLM) on Ego3D-Bench. Accuracy (%) RMSE Ego Obj. Dist. Dist. Loc. Mot. Mot. Time Rel. Rel. Avg. Dist. Dist. Avg. Model 18.1 21.4 16.1 35.6 35.2 SpaceQwen2.5-VL-3B 45.7 35.5 39.6 43.6 45.8 SRGPT-VILA1.5-8B SThinker-Qwen2.5-3B 38.9 39.0 21.9 57.5 53.4 Ego3D-VLMSThinker-Qwen2.5-3B 50.6 44.2 26.4 53.4 57.0 10.6 15.7 13.2 11.5 15.1 13.3 15.2 16.9 16.0 11.1 12.2 11.6 31.2 32.0 27.5 50.8 71.7 44.7 52.8 71.6 45.2 54.7 59.8 48.6 Ego Obj. Travel Ego Obj. 30.2 24.9 27.1 Ego Obj. 42.6 Table 4: Ego3D-VLM vs. generalist VLMs empowered with REC and depth tools on Ego3D-Bench. Accuracy (%) RMSE Ego Obj. Travel Ego Obj. Ego Obj. Dist. Dist. Loc. Mot. Mot. Time Rel. Rel. Avg. Dist. Dist. Avg. Model 25.8 28.7 29.8 54.1 54.8 InternVL3-8B InternVL3-8B+Depth+REC 60.9 46.9 35.2 58.8 49.0 Ego3D-VLM InternVL3-8B 65.3 56.1 36.9 69.0 74.0 32.7 31.5 30.5 45.9 44.0 Qwen2.5-7B 53.6 37.6 33.2 54.8 50.4 Qwen2.5-7B+Depth+REC Ego3D-VLM Qwen2.5-7B 59.4 54.5 33.1 62.3 58.2 15.2 39.1 27.2 19.6 13.1 6.6 8.0 6.8 9.0 25.1 35.5 30.3 16.0 11.8 7.5 10.9 9.5 8.1 65.2 49.9 43.1 69.1 60.7 51.6 67.4 63.0 60.1 43.2 66.5 41.1 65.3 63.8 49.4 50.5 66.9 54.3 36.1 31.9 47.5 34.5 36.1 49.4 Ego Obj. estimator [6] to estimate depth values. We then construct list that pairs each matched referring expression with its corresponding bounding box and estimated depth (i.e., distance from the ego). An example entry in this list is: [Front-View: Detected pedestrian with red hat at bbox z, Back-View:...] (examples in the Appendix). As shown in [x1, y1, x2, y2], depth: Table 4, equipping the VLM with REC and depth estimator models improves its baseline performance, highlighting the base models limitations in accurately identifying objects and estimating their depths. However, even with these enhancements, it falls short compared to our proposed Ego3D-VLM framework. This underscores the importance of integrating depth information into unified map representation, enabling the VLM to reason more effectively about the 3D relationships between objects across different views. 5.4 Ego3D-VLM on Further Benchmarks Our proposed Ego3D-VLM is designed for ego-centric multi-view scenarios, which are particularly relevant for AI agent applications. In this section, however, we evaluate the performance of Ego3D-VLM in alternative multi-view settings, specifically those used in All-Angle Bench [38] and VSI-Bench [37]. Figure 7 illustrates the configurations of these benchmarks in comparison to Ego3D-Bench. All-Angle Bench features stationary multi-view cameras observing the same scene from different anglesa setup commonly used in motion capture and surveillance systems. VSI-Bench, on the other hand, involves single moving camera within static indoor environment, typically used for static scene reconstruction and spatial understanding. In contrast, our Ego3D-Bench assumes multiple cameras mounted on moving ego agent, capturing the surrounding environmenta configuration suited for AI agent applications such as robotics and autonomous driving. Table 5 presents the performance of Ego3D-VLM compared to openand closed-source baseline VLMs on All-Angle Bench and VSI-Bench. Despite the differences in data settings compared to our primary benchmark, Ego3D-VLM still outperforms the respective baselines, demonstrating its adaptability across diverse multi-view scenarios. All-Angle-Bench VSI-Bench Table 5: Comparison results on All-Angle Bench and VSI-Bench (accuracy). Model GPT-4o Gemini-1.5-Pro Gemini-1.5-Flash InternVL3-8B Ego3D-VLMInternVL3-8B InternVL3-14B Ego3D-VLMInternVL3-14B InternVL3-38B Ego3D-VLMInternVL3-38B 34.0 45.4 42.1 38.1 39.6 38.2 40.0 41.1 42. 47.8 47.4 46.6 47.9 49.5 50.3 52.1 54.4 55.7 Figure 7: Settings of multi-view/video spatial understanding benchmarks for VLMs. 9 5.5 Ablation Studies (Acc. ) 43.1 56.0 56.3 58.4 60.1 61.8 79.4 v0 InternVL3-8B v1 + CogMap (est. R, T, K) v2 + v3 + R, v4 + Scaling v5 + List of objects v6 GT CogMap Mult. Choice Abs. Ans. (RMSE) 27.2 10.8 10.1 10.4 8.0 6.5 1.3 Table 6: Ablation on the main components of Ego3D-VLM. v4 includes all of the main components. v5 and v6 are shown for further analysis. Components of Ego3D-VLM. Table 6 presents an ablation study on the core components of Ego3DVLM. Starting from the baseline, v0, we incrementally add each component to investigate its contribution. In version v1, we incorporate cognitive map with estimated rotation (R), translation (T ), and intrinsic parameters (K). Specifically, all cameras are positioned at the coordinate center with = [0, 0, 0]. The front camera uses an identity rotation matrix, while all the other camerasfrontright, right, back-right, back, etc. are incrementally rotated 45 around the Y-axis. The focal length is estimated from the images approximate field of view. In v2, we use the actual and in v3 we further use actual R, and relative to the front camera, i.e., often available as fixed parameter for an embodied AI agent. v3 and v1 obtain comparable RMSE, while v3 is only 2.4% higher in multi-choice QAs. Thus, even with estimated camera parameters our cognitive map can significantly boost the baselines performance. v4 adds relational scaling to cognitive map which decreases the RMSE by 2.5 meters. v4 is indeed Ego3D-VLM with all components. We provide two more ablations to evaluate the upper-bound of Ego3D-VLM. v5 assumes that the list of objects (only their names) in the input query are provided which enhances the REC results. v6 is the ground-truth cognitive map with ground-truth 3D locations.The difference between v6 and human level is only 5% showing the upper bound of Ego3D-VLM. Disentanglement of Perception from Reasoning. In order to disentangle perception from reasoning, we have added the ground-truth 2D bounding boxes (BBox) for all objects, to the benchmark. This allows repeating our experiments with bounding boxes overlaid, effectively isolating the reasoning and perception components. As expected, adding GT BBox as visual prompts to the images will help the VLM to handle the perception procedure easier, therefore improving the results. The results in Table 7 show that the major limitation of baselines is not 2D perception, but 3D perception and reasoning. GT Mult. Choice Abs. Ans. (RMSE) 27.2 21.0 8.0 6.8 17.6 16.2 7.7 7.5 InternVL3-8B InternVL3-8B Ego3D-VLMInternVL3-8B Ego3D-VLMInternVL3-8B InternVL3-14B InternVL3-14B Ego3D-VLMInternVL3-14B Ego3D-VLMInternVL3-14B Table 7: Experiments on disentanglement of perception from reasoning (Acc. ) 43.1 50.2 60.1 62.2 51.7 51.8 66.5 66.8 BBOX Format of Cognitive Map. We explore three different formats for our generated cognitive map: visual, JSON, and textual. Examples of the visual and textual maps are shown in Figure 5. The average results for each format are presented in Table 8. Among the three, the textual and JSON formats achieve the best overall performance."
        },
        {
            "title": "6 Conclusion and Future Work",
            "content": "Table 8: Ablation on the cognitive map format. Visual Cog-Map JSON Cog-Map Textual Cog-Map Mult. Choice Abs. Ans. (RMSE) 14.4 8.4 8.0 (Acc. ) 50.9 60.0 60.1 In this work, we proposed Ego3D-Bench, benchmark for spatial understanding of VLMs on egocentric multi-view images. Overall, the benchmark shows significant gap between human scores and VLMs. To address this limitation, we provided post-training solution named Ego3D-VLM to enhance the performance of VLMs. Future work should explore fine-tuning VLMs with ego-centric multi-view QAs and incorporating 3D projection modules proposed in Ego3D-VLM in the course of fine-tuning. The limitations of this work will be provided in the Appendix."
        },
        {
            "title": "References",
            "content": "[1] Holger Caesar, Varun Bankiti, Alex Lang, Sourabh Vora, Venice Erin Liong, Qiang Xu, Anush Krishnan, Yu Pan, Giancarlo Baldan, and Oscar Beijbom. nuscenes: multimodal dataset for autonomous driving. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 11621 11631, 2020. 2, 3 [2] Ming-Fang Chang, John Lambert, Patsorn Sangkloy, Jagjeet Singh, Slawomir Bak, Andrew Hartnett, De Wang, Peter Carr, Simon Lucey, Deva Ramanan, and James Hays. Argoverse: 3d tracking and forecasting with rich maps. In Conference on Computer Vision and Pattern Recognition (CVPR), 2019. 2, 3 [3] Devendra Singh Chaplot, Murtaza Dalal, Saurabh Gupta, Jitendra Malik, and Russ Salakhutdinov. Seal: Self-supervised embodied active learning using exploration and 3d consistency. Proceedings of the Conference on Neural Information Processing Systems (NeurIPS), 34:1308613098, 2021. 2 [4] Boyuan Chen, Zhuo Xu, Sean Kirmani, Brain Ichter, Dorsa Sadigh, Leonidas Guibas, and Fei Xia. Spatialvlm: Endowing vision-language models with spatial reasoning capabilities. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pages 1445514465, 2024. 2, 3, 8 [5] An-Chieh Cheng, Hongxu Yin, Yang Fu, Qiushan Guo, Ruihan Yang, Jan Kautz, Xiaolong Wang, and Sifei Liu. Spatialrgpt: Grounded spatial reasoning in vision-language models. In Proceedings of the Conference on Neural Information Processing Systems (NeurIPS), 2024. 2, 3, 8 [6] Hugging Face Contributors. Depth anything v2 - metric outdoor large. https://huggingface.co/ depth-anything/Depth-Anything-V2-Metric-Outdoor-Large-hf, 2024. Accessed: 2025-05-16. 6, [7] Erik Daxberger, Nina Wenzel, David Griffiths, Haiming Gang, Justin Lazarow, Gefen Kohavi, Kai Kang, Marcin Eichner, Yinfei Yang, Afshin Dehghan, and Peter Grasch. Mm-spatial: Exploring 3d spatial understanding in multimodal llms, 2025. 2, 3 [8] Google DeepMind. Gemini: family of highly capable multimodal models. Google DeepMind Technical Report, 2023. 6, 14 [9] Jinguo Zhu et al. Internvl3: Exploring advanced training and test-time recipes for open-source multimodal models, 2025. 6 [10] Marah Abdin et al. Phi-3 technical report: highly capable language model locally on your phone, 2024. 14 [11] Daya Guo, Dejian Yang, Haowei Zhang, Junxiao Song, Ruoyu Zhang, Runxin Xu, Qihao Zhu, Shirong Ma, Peiyi Wang, Xiao Bi, et al. Deepseek-r1: Incentivizing reasoning capability in llms via reinforcement learning. arXiv preprint arXiv:2501.12948, 2025. 6 [12] Yining Hong, Chunru Lin, Yilun Du, Zhenfang Chen, Joshua B. Tenenbaum, and Chuang Gan. 3d concept learning and reasoning from multi-view images, 2023. 2, 3 [13] Yining Hong, Haoyu Zhen, Peihao Chen, Shuhong Zheng, Yilun Du, Zhenfang Chen, and Chuang Gan. 3d-llm: Injecting the 3d world into large language models. Proceedings of the Conference on Neural Information Processing Systems (NeurIPS), 2023. 2, 3 [14] Jiangyong Huang, Silong Yong, Xiaojian Ma, Xiongkun Linghu, Puhao Li, Yan Wang, Qing Li, Song-Chun Zhu, Baoxiong Jia, and Siyuan Huang. An embodied generalist agent in 3d world. In Proceedings of the International Conference on Machine Learning (ICML), 2024. 2, [15] Bo Li, Yuanhan Zhang, Dong Guo, Renrui Zhang, Feng Li, Hao Zhang, Kaichen Zhang, Yanwei Li, Ziwei Liu, and Chunyuan Li. Llava-onevision: Easy visual task transfer. arXiv preprint arXiv:2408.03326, 2024. 14 [16] Feng Li, Renrui Zhang, Hao Zhang, Yuanhan Zhang, Bo Li, Wei Li, Zejun Ma, and Chunyuan Li. Llava-next-interleave: Tackling multi-image, video, and 3d in large multimodal models, 2024. 2 [17] Yuan-Hong Liao, Rafid Mahmood, Sanja Fidler, and David Acuna. Reasoning paths with reference objects elicit quantitative spatial reasoning in large vision-language models, 2024. 2, 3 [18] Shiyin Lu, Yang Li, Qing-Guo Chen, Zhao Xu, Weihua Luo, Kaifu Zhang, and Han-Jia Ye. Ovis: Structural embedding alignment for multimodal large language model. arXiv:2405.20797, 2024. 6 11 [19] Chenyang Ma, Kai Lu, Ta-Ying Cheng, Niki Trigoni, and Andrew Markham. Spatialpin: Enhancing spatial reasoning capabilities of vision-language models through prompting and interacting 3d priors. In Proceedings of the Conference on Neural Information Processing Systems (NeurIPS), 2024. 3 [20] Yueen Ma, Zixing Song, Yuzheng Zhuang, Jianye Hao, and Irwin King. survey on vision-language-action models for embodied ai. arXiv preprint arXiv:2405.14093, 2024. 2 [21] Arjun Majumdar, Anurag Ajay, Xiaohan Zhang, Pranav Putta, Sriram Yenamandra, Mikael Henaff, Sneha Silwal, Paul Mcvay, Oleksandr Maksymets, Sergio Arnaud, Karmesh Yadav, Qiyang Li, Ben Newman, Mohit Sharma, Vincent Berges, Shiqi Zhang, Pulkit Agrawal, Yonatan Bisk, Dhruv Batra, Mrinal Kalakrishnan, Franziska Meier, Chris Paxton, Alexander Sax, and Aravind Rajeswaran. Openeqa: Embodied question answering in the era of foundation models. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pages 1648816498, 2024. [22] OpenAI. Gpt-4o. https://openai.com/index/gpt-4o, 2024. Accessed: 2025-04-29. 6 [23] Zhangyang Qi, Zhixiong Zhang, Ye Fang, Jiaqi Wang, and Hengshuang Zhao. Gpt4scene: Understand 3d scenes from videos with vision-language models. arXiv preprint arXiv:2501.01428, 2024. 2, 3 [24] Qwen, :, and An Yang et al. Qwen2.5 technical report, 2025. 6 [25] Arijit Ray, Jiafei Duan, Ellis Brown, Reuben Tan, Dina Bashkirova, Rose Hendrix, Kiana Ehsani, Aniruddha Kembhavi, Bryan A. Plummer, Ranjay Krishna, Kuo-Hao Zeng, and Kate Saenko. Sat: Dynamic spatial aptitude training for multimodal language models, 2025. 2 [26] remyxai. Vqasynth, 2024. GitHub repository. 3, [27] IDEA Research. Grounding dino base - model on hugging face, 2023. Accessed: 2025-05-15. 6, 8 [28] Chan Hee Song, Valts Blukis, Jonathan Tremblay, Stephen Tyree, Yu Su, and Stan Birchfield. RoboSpatial: Teaching spatial understanding to 2D and 3D vision-language models for robotics. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), 2025. Oral Presentation. 2 [29] Pei Sun, Henrik Kretzschmar, Xerxes Dotiwalla, Aurelien Chouard, Vijaysai Patnaik, Paul Tsui, James Guo, Yin Zhou, Yuning Chai, Benjamin Caine, et al. Scalability in perception for autonomous driving: Waymo open dataset. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 24462454, 2020. 2, 3 [30] ManyCore Research Team. Spatiallm: Large language model for spatial understanding. https://github. com/manycore-research/SpatialLM, 2025. 2, 3 [31] Jiayu Wang, Yifei Ming, Zhenmei Shi, Vibhav Vineet, Xin Wang, Sharon Li, and Neel Joshi. Is picture worth thousand words? delving into spatial reasoning for vision language models. Proceedings of the Conference on Neural Information Processing Systems (NeurIPS), 37:7539275421, 2024. [32] Tai Wang, Xiaohan Mao, Chenming Zhu, Runsen Xu, Ruiyuan Lyu, Peisen Li, Xiao Chen, Wenwei Zhang, Kai Chen, Tianfan Xue, et al. Embodiedscan: holistic multi-modal 3d perception suite towards embodied ai. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 1975719767, 2024. 2 [33] Changli Wu, Qi Chen, Jiayi Ji, Haowei Wang, Yiwei Ma, You Huang, Gen Luo, Hao Fei, Xiaoshuai Sun, and Rongrong Ji. Rg-san: Rule-guided spatial awareness network for end-to-end 3d referring expression segmentation, 2024. 2 [34] Wenshan Wu, Shaoguang Mao, Yadong Zhang, Yan Xia, Li Dong, Lei Cui, and Furu Wei. Minds eye of llms: Visualization-of-thought elicits spatial reasoning in large language models. In Proceedings of the Conference on Neural Information Processing Systems (NeurIPS), 2024. 2 [35] Zhiyu Wu, Xiaokang Chen, Zizheng Pan, Xingchao Liu, Wen Liu, Damai Dai, Huazuo Gao, Yiyang Ma, Chengyue Wu, Bingxuan Wang, Zhenda Xie, Yu Wu, Kai Hu, Jiawei Wang, Yaofeng Sun, Yukun Li, Yishi Piao, Kang Guan, Aixin Liu, Xin Xie, Yuxiang You, Kai Dong, Xingkai Yu, Haowei Zhang, Liang Zhao, Yisong Wang, and Chong Ruan. Deepseek-vl2: Mixture-of-experts vision-language models for advanced multimodal understanding, 2024. 14 [36] Fei Xia, Amir Zamir, Zhiyang He, Alexander Sax, Jitendra Malik, and Silvio Savarese. Gibson env: Real-world perception for embodied agents. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 90689079, 2018. 2 [37] Jihan Yang, Shusheng Yang, Anjali Gupta, Rilyn Han, Li Fei-Fei, and Saining Xie. Thinking in Space: How Multimodal Large Language Models See, Remember and Recall Spaces. arXiv preprint arXiv:2412.14171, 2024. 2, 3, 8, 9 [38] Chun-Hsiao Yeh, Chenyu Wang, Shengbang Tong, Ta-Ying Cheng, Rouyu Wang, Tianzhe Chu, Yuexiang Zhai, Yubei Chen, Shenghua Gao, and Yi Ma. Seeing from another perspective: Evaluating multi-view understanding in mllms. arXiv preprint arXiv:2504.15280, 2025. 3, 9 [39] Jiawei Zhang, Chejian Xu, and Bo Li. Chatscene: Knowledge-enabled safety-critical scenario generation for autonomous vehicles. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 1545915469, 2024. 2, 3 [40] Yuanhan Zhang, Bo Li, haotian Liu, Yong jae Lee, Liangke Gui, Di Fu, Jiashi Feng, Ziwei Liu, and Chunyuan Li. Llava-next: strong zero-shot video understanding model, 2024. 14 [41] Duo Zheng, Shijia Huang, and Liwei Wang. Video-3d llm: Learning position-aware video representation for 3d scene understanding, 2024. 3 [42] Chenming Zhu, Tai Wang, Wenwei Zhang, Jiangmiao Pang, and Xihui Liu. Llava-3d: simple yet effective pathway to empowering lmms with 3d-awareness. arXiv preprint arXiv:2409.18125, 2024. 2,"
        },
        {
            "title": "A Appendix",
            "content": "In this appendix, we present additional results on various source dataset splits of Ego3D-Bench, along with extended quantitative and qualitative analysis. We also report the performance of Ego3D-VLM on two public benchmarks, provide an inference time analysis, and discuss the limitations of our approach. A.1 Ego3D-Bench: Further Details In this section, we provide the templates used to create questions of the benchmark. Figure 8 shows templates used to create each category of Ego3D-Bench. We replace <obj1>, <obj2>, and <obj3> in the templates with object descriptions. <view1>, <view2>, and <view3> are replaced with the camera views from which the object is visible (e.g., Front-Right, Left, etc.). The placeholder <direction> in motion reasoning tasks is substituted with one of the four cardinal directions\"north\", \"east\", \"west\", or \"south\"and this process is repeated until all directions have been used. The placeholder <Y> is for motion reasoning tasks and is replaced with number between 2 to 5 meters. <X1>, <X2>, <X3>, and <X4> serve as placeholders for multiple-choice options in absolute distance estimation tasks. One of these options is replaced with the ground-truth distance, while the remaining are filled with randomly generated values, ensuring that the distance between any two options is at least 8 meters. A.2 Results of different Source Datasets Table 9 presents the results for different source dataset splits used to create Ego3D-Bench. Despite the varying number of camera viewpoints across the three datasets, the performance deviations are minimal. This consistency underscores the reliability of Ego3D-Bench as benchmark, indicating that model performance is not heavily influenced by the specific choice of source dataa desirable property for robust and fair benchmark design. A.3 Benchmarking Generalist VLMs: Further Results In the main paper, we reported results for InternVL3, Qwen2.5-VL, and Ovis-2 as representative opensource models, as they are, to the best of our knowledge, the current SOTA among open-source VLMs. Table 10 extends this comparison by including additional generalist VLMs on Ego3D-Bench, such as Gemini-1.5-Flash [8], Gemini-2-Flash [8], Gemini-2.5-Flash [8], Phi-3.5 [10], LLaVA-One-Vision7B [15], LLaVA-Next-Video-7B [40], and DeepSeek-VL2 [35]. We observe that LLaVA-One-Vision, Phi-3.5, and DeepSeek-VL2 underperform compared to InternVL3 and Qwen2.5-VL models of similar size. For instance, InternVL3-8B achieves an accuracy of 43%, while LLaVA-One-Vision-7B achieves 38.7%. Likewise, Phi-3.5 (3.8B) attains an accuracy of 40.3%, compared to 41% from Ovis-2 (4B). A.4 Inference Time Analysis Table 11 shows the inference time analysis of Ego3D-VLM compared to baseline models. We report End-to-End Latency (E2E Lat.) in seconds and Peak Memory in GB. For the E2E Lat., we measure the average end-to-end inference of models on 50 samples of Ego3D-Bench and for the peak memory we report the peak memory usage during inference on the same samples. Experiments are performed on NVIDIA H20 GPUs using flash-attention-2. The memory and latency overhead of Ego3D-VLM over InternVL3-78B is 0.6% and 31% , respectively. The main reason for the latency overhead is that the model reasons more when cognitive map is provided in the Ego3D-VLM. A.5 Robustness of External Tools To isolate tool performance, we included an ablation with ground-truth cognitive maps  (Table 6)  , which improve the results from 60.1% to 79.4% accuracy. This confirms that, there is still gap between the accuracy of such imperfect tools and ground-truth REC/depth. However, compared to the baseline, our solution still achieves more robust results even in challenging conditions such as low brightness, motion blur, and occlusion. To support this claim, we simulated these conditions and re-ran more specific experiments shown in the Table 12. 14 Figure 8: Templates used to create questions of Ego3D-Bench. 15 Table 9: Results on samples of Ego3D-Bench generated from each of the source datasets. NuScenes Waymo Acc RMSE Acc RMSE Acc RMSE Argoverse Closed-source Models 60.9 GPT-4o 73.5 Ego3D-VLM GPT-4o Gemini-1.5-Pro 59.3 Ego3D-VLM Gemini-1.5-Pro 77.1 Qwen2.5 Family Qwen2.5-3B Ego3D-VLM Qwen2.5-3B Qwen2.5-7B Ego3D-VLM Qwen2.5-7B Qwen2.5-32B Ego3D-VLM Qwen2.5-32B Qwen2.5-72B Ego3D-VLM Qwen2.5-72B InternVL3 Family InternVL3-8B Ego3D-VLM InternVL3-8B InternVL3-14B Ego3D-VLM InternVL3-14B InternVL3-38B Ego3D-VLM InternVL3-38B InternVL3-78B Ego3D-VLM InternVL3-78B Ovis2 Family Ovis2-4B Ego3D-VLM Ovis2-4B Ovis2-8B Ego3D-VLM Ovis2-8B Ovis2-16B Ego3D-VLM Ovis2-16B 39.7 44.7 40.9 56.4 56.6 64.3 58.2 74.4 42.3 62.0 51.7 69.0 51.2 71.7 60.0 76.5 41.2 47.3 41.5 55.6 48.6 62.6 21.2 8.5 26.4 8.1 33.4 13.0 36.0 11.3 17.5 17.1 21.2 8.5 28.3 10.5 20.4 8.8 31.4 8.7 16.7 8. 29.2 9.8 21.4 9.4 16.2 8.5 60.0 73.1 55.6 68.9 38.6 40.3 40.6 51.9 56.0 64.8 57.7 64.4 44.0 57.1 50.2 63.7 50.5 64.3 59.7 67.0 43.0 47.5 41.9 55.1 45.5 57.9 19.0 7.5 12.7 6. 31.5 12.1 24.7 8.3 11.0 10.6 11.1 6.4 21.7 6.5 15.8 7.5 19.4 8.2 10.7 6.5 18.1 7.1 20.7 6.8 11.9 7.15 59.4 72.7 57.5 73.0 38.9 48.1 41.6 54.4 59.2 64.0 58.2 69.5 42.9 61.3 53.3 67.0 53.4 67.0 60.1 71. 40.9 49.7 41.0 57.9 46.8 62.2 17.1 6.2 19.5 7.1 31.2 12.6 30.1 8.7 18.7 12.1 16.2 7.6 31.5 6.7 16.6 6.3 25.4 6.3 13.7 7.5 22.5 8.3 20.2 7.0 12.9 6.5 Table 10: Results of further generalist VLMs on Ego3D-Bench. Accuracy (%) RMSE Model Gemini Gemini-1.5 Flash Gemini-2-Flash Gemini-2.5-Flash Phi Phi3. Ego Obj. Dist. Dist. Loc. Mot. Mot. Time Rel. Rel. Avg. Dist. Dist. Avg. Ego Obj. Travel Ego Obj. Ego Obj. 40.4 26.9 50.5 70.1 61.8 43.3 38.6 54.1 58.0 39.3 41.3 25.6 65.1 93.4 84.8 28.0 29.1 20.1 53.6 75.6 50.9 47.1 68.5 47.2 63.5 68.5 57. 10.8 31.0 20.9 10.9 29.4 20.1 11.4 19.6 15.5 28.3 30.8 24.2 59.6 55.9 21.3 45.3 56.7 40.3 22.8 49.4 36.1 LLaVA Family LLaVA-Next-Video-7B 30.2 26.6 42.2 60.8 58.0 23.5 22.1 58.4 56.3 52.5 LLaVA-OV-7B DeepSeek Family DeepSeek-VL2-tiny DeepSeek-VL2-Small DeepSeek-VL2 34.7 29.0 19.9 60.0 59.3 18.1 19.2 28.6 50.3 55.6 22.3 25.3 32.4 60.0 59.1 48.3 17.9 55.1 73.5 49.3 59.0 20.0 38.7 19.5 22.8 21.2 17.8 19.3 18. 46.6 37.4 21.4 57.8 65.0 46.5 50.5 59.3 39.9 56.7 62.0 42.4 14.0 18.7 16.3 12.7 14.7 13.7 20.3 17.4 18.8 Although G-DINO and Depth-Anything used in Ego3D-VLM are among the best tools for REC and depth estimation, our solution is orthogonal to any such tools, and has no limitation in this regard. A.6 Qualitative Results Figure 9-19 demonstrate example responses of InternVL3-78B and Ego3D-VLMInternVL3-78B on all categories of Ego3D-Bench. As seen, Ego3D-Bench enhances the spatial reasoning ability of the baseline by providing the textual cognitive map. 16 Table 11: Inference time and memory usage of Ego3D-VLM compared to the baselines. InternVL3-8B Ego3D-VLMInternVL3-8B InternVL3-14B Ego3D-VLMInternVL3-14B E2E Lat. Memory (sec) 5.2 8.6 15.5 16.4 (GB) 18.1 26.5 33.1 40. InternVL3-38B Ego3D-VLMInternVL3-38B InternVL3-78B Ego3D-VLMInternVL3-78B E2E Lat. Memory (sec) 16.9 19.1 35.0 46.9 (GB) 80.0 84.6 161.7 162.4 Table 12: Robustness of external tools in scenarios involving occlusion, motion blur, and low light. InternVL3-8B Ego3D-VLM InternVL3-8B InternVL3-8B [60% Darkness] Ego3D-VLM InternVL3-8B [60% Darkness] InternVL3-8B [Motion Blur, 151 kernel] Ego3D-VLM InternVL3-8B [motion blur, 151 kernel] InternVL3-8B [30% Occlusion] Ego3D-VLM InternVL3-8B [30% Occlusion] Mult. Choice Abs. Ans. (RMSE) 27.2 8.0 29.8 10.6 28.5 9.9 28.9 10.7 (Acc. ) 43.1 60.1 41.1 59.6 42.5 57.9 42.0 58.7 A.7 Benchmarking the Thinking of Ego3D-Bench as an Open-QA All our question-answer pairs include \"thinking\" step in open-ended format. In this section, we generate \"GT thinking\" for further open-ended type evaluation. To this end, we used the GT cognitive maps along with GPT-4o to generate the reasoning for two categories that require numerical answers: Ego-Centric / Object-Centric Absolute Distance (in meters). Table 13 shows that Ego3D-VLM thinking is better than the baselines. Table 13: Benchmarking the thinking of VLMs on Ego3D-Bench as an open-ended category. The GT thinking is generated using GT cognitive maps along with GPT-4o. InternVL3-8B Ego3D-VLM InternVL3-8B InternVL3-14B Ego3D-VLM InternVL3-14B InternVL3-38B Ego3D-VLM InternVL3-38B InternVL3-78B Ego3D-VLM InternVL3-78B Thinking of Obj. Abs. Dist. Thinking of Ego Abs. Dist. GPT-4o Score (0-10) 1.9 4.7 2.3 5.7 2.1 7.1 2.1 7.0 GPT-4o Score (0-10) 2.0 5.2 2.4 5.0 2.5 3.7 1.7 6.8 A.8 Qualitative Results Figure 9-19 demonstrate example responses of InternVL3-78B and Ego3D-VLMInternVL3-78B on all categories of Ego3D-Bench. As seen, Ego3D-Bench enhances the spatial reasoning ability of the baseline by providing the textual cognitive map. A.9 Limitations and Future Work This work has several limitations that need further investigation. The proposed Ego3D-VLM relies on the reasoning capabilities of the underlying vision-language models (VLMs). Consequently, for models with limited reasoning ability (e.g., VILA1.5-8B), we observe little to no improvement when combined with Ego3D-VLM. Additionally, the REC models used in our pipeline provide 2D bounding box locations for all expressions in the prompt. This results in redundant information within the cognitive maps, which 17 Figure 9: Example responses of the baseline and Ego3D-VLM on ego-centric absolute distance task. may confuse or mislead the VLMs. Our ablation studies demonstrated that when the list of objects mentioned in the prompt is known in advance, the RMSE improves by approximately 1.5 meters. Another limitation lies in the accuracy of metric depth estimation in outdoor environments. To mitigate this, we proposed relational scaling approach based on common-sense object sizes. However, this method is inherently approximate and not fully reliable. Future work should aim to address these issues by enhancing the depth understanding of VLMs or by improving metric depth estimation in outdoor settings. Better integration of spatial reasoning and scene geometry into VLMs could further improve performance in complex 3D environments. 18 Figure 10: Example responses of the baseline and Ego3D-VLM on object-centric absolute distance task. 19 Figure 11: Example responses of the baseline and Ego3D-VLM on multi-choice ego-centric absolute distance task. 20 Figure 12: Example responses of the baseline and Ego3D-VLM on multi-choice object-centric absolute distance task. 21 Figure 13: Example responses of the baseline and Ego3D-VLM on object-centric motion reasoning task. 22 Figure 14: Example responses of the baseline and Ego3D-VLM on ego-centric motion reasoning task. 23 Figure 15: Example responses of the baseline and Ego3D-VLM on travel time (ego-centric) task. 24 Figure 16: Example responses of the baseline and Ego3D-VLM on travel time (object-centric) task. 25 Figure 17: Example responses of the baseline and Ego3D-VLM on object-centric relative distance task. 26 Figure 18: Example responses of the baseline and Ego3D-VLM on ego-centric relative distance task. 27 Figure 19: Example responses of the baseline and Ego3D-VLM on localization task."
        }
    ],
    "affiliations": [
        "Huawei Cloud",
        "Huawei Technologies Canada"
    ]
}