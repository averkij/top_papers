{
    "paper_title": "Multi-Agent Collaborative Data Selection for Efficient LLM Pretraining",
    "authors": [
        "Tianyi Bai",
        "Ling Yang",
        "Zhen Hao Wong",
        "Jiahui Peng",
        "Xinlin Zhuang",
        "Chi Zhang",
        "Lijun Wu",
        "Jiantao Qiu",
        "Wentao Zhang",
        "Binhang Yuan",
        "Conghui He"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Efficient data selection is crucial to accelerate the pretraining of large language models (LLMs). While various methods have been proposed to enhance data efficiency, limited research has addressed the inherent conflicts between these approaches to achieve optimal data selection for LLM pretraining. To tackle this problem, we propose a novel multi-agent collaborative data selection mechanism. In this framework, each data selection method serves as an independent agent, and an agent console is designed to dynamically integrate the information from all agents throughout the LLM training process. We conduct extensive empirical studies to evaluate our multi-agent framework. The experimental results demonstrate that our approach significantly improves data efficiency, accelerates convergence in LLM training, and achieves an average performance gain up to 10.5% across multiple language model benchmarks compared to the state-of-the-art methods."
        },
        {
            "title": "Start",
            "content": "MULTI-AGENT COLLABORATIVE DATA SELECTION FOR EFFICIENT LLM PRETRAINING Tianyi Bai1,2 Ling Yang3 Zhenhao Huang3 Chi Zhang2 Lijun Wu2 1Hong Kong University of Science and Technology Jiantao Qiu2 Wentao Zhang3 Binhang Yuan1 Conghui He2 2Shanghai AI Laboratory 3 Peking University Jiahui Peng2 Xinling Zhuang2 4 2 0 2 4 ] . [ 2 2 0 1 8 0 . 0 1 4 2 : r a"
        },
        {
            "title": "ABSTRACT",
            "content": "Efficient data selection is crucial to accelerate the pretraining of large language models (LLMs). While various methods have been proposed to enhance data efficiency, limited research has addressed the inherent conflicts between these approaches to achieve optimal data selection for LLM pretraining. To tackle this problem, we propose novel multi-agent collaborative data selection mechanism. In this framework, each data selection method serves as an independent agent, and an agent console is designed to dynamically integrate the information from all agents throughout the training process. We conduct extensive empirical studies to evaluate our multi-agent framework. The experimental results demonstrate that our approach significantly improves data efficiency, accelerates convergence in LLM training, and achieves an average performance gain up to 10.5% across multiple language model benchmarks compared to the state-of-the-art methods. Code and checkpoints are publicly released at github.com/beccabai/multi-agent-data-selection."
        },
        {
            "title": "INTRODUCTION",
            "content": "Efficient data selection is crucial for the pretraining of large language models (LLMs), as the quality of training data significantly impacts the statistical efficiency of the training procedure and the model performance (Brown, 2020; Du et al., 2022; Chowdhery et al., 2023). Recently, we have witnessed numerous approaches, such as filtering high-quality data (Xie et al., 2023b; Wettig et al., 2024), mixing data from multiple domains (Xie et al., 2023a; Liu et al., 2024), and selecting data that optimally boosts downstream task performance dynamically (Engstrom et al., 2024; Yu et al., 2024), which aim to improve data efficiency by prioritizing more informative training samples. However, these methods often operate independently or in isolated settings, limiting their potential when integrated into collaborative framework. In this work, we want to explore how to effectively, flexibly, and robustly combine these advanced data selection techniques through the dynamic pretraining process, addressing the challenges of optimizing data efficiency for LLM pretraining at scale. Nowadays, various heuristic methods have been proposed to provide measurements for the data samples used during LLM pre-training, aiming to optimize data efficiency by selecting or weighting the most informative training examples. However, we observe that integrating multiple data selection and mixing strategies presents significant challenges due to their inherent conflicts. For example, high-quality data identified by scoring functions may not align with data that strongly impact model performance as measured by influence functions (Engstrom et al., 2024); similar conflicts also exists between other methods further details are enumerated in 2. These observations actually motivate us to launch systematic discussion about how to effectively integrate these methods during the dynamic pretraining process that provides superior data efficiency for LLM pretraining. On the other hand, effectively integrating these data selection methods into single framework is much harder to implement than to ask for. In fact, one may have to explore an exponential space to find the optimal combination for different data sampling schemas. Such heavy burden will be Work done during an internship at Shanghai AI Laboratory. Contact: tbaiag@cse.ust.hk Corresponding biyuan@ust.hk, jiantao}@pjlab.org.cn. wentao.zhang@pku.edu.cn, authors. {heconghui, qiu1 Figure 1: Statistics of the SlimPajama dataset. This figure shows the distribution of overall 627B tokens data across four dimensions: quality, domain, topic diversity, and impact on the pretrained model at the 1500th step. Each bar represents subset defined by specific quality interval and domain. This visualization reveals the conflict among diversity, quality, and model influence. further amplified when we consider the dynamic adjustment during the training process introduced by state-of-the-art online data selection approaches. In fact, different from the offline methods that leverage fixed classifier-based scoring functions (Brown, 2020; Gao et al., 2020; Du et al., 2022; Chowdhery et al., 2023; Sachdeva et al., 2024; Wettig et al., 2024), domain weights (Brown, 2020; Team, 2024; Rae et al., 2021; Xie et al., 2023a; Liu et al., 2024), or down-sampled topics (Team, 2024; Chen et al., 2024), online methods use techniques like influence functions (Engstrom et al., 2024; Yu et al., 2024) to assess the models sensitivity to individual data points during the LLM pretraining process to enable dynamic selection of high-impact data at any optimization step. Such an online paradigm is computationally intensive by itself, which presents the essential challenge for an effective dynamic integration in our problem setting that demands computationally efficient solution to preserve or even amplify the advantage of each data selection heuristic. To address these challenges, we conduct case study to identify the inherent conflicts for existing data selection methods and provide multi-agent collaborative data selection framework to resolve this issue. Concretely, we make the following contributions: Contribution 1: We conduct case study in 2 over the SlimPajama dataset to present some interesting discoveries about the relationship among four popular data selection measurements used in LLM pretraining, i.e., data quality, topic diversity, data impact, and data domain, where the statistic suggests some interesting inherent conflicts among the existing methods. Contribution 2: We propose novel multi-agent collaborative data selection mechanism in 3. In this framework, each data selection method operates as an agent capable of providing scores to prioritize the training data samples. We also design an agent console to effectively integrate the scores from all agents, producing optimized data selection results. Furthermore, we implement dynamic collaboration mechanism, where the contribution of each agent can be adjusted dynamically throughout the LLM training process. This approach enables more flexible and adaptive data selection, improving overall data efficiency during LLM pretraining. Contribution 3: To evaluate our multi-agent collaborative data selection method, we conducted extensive experiments to show that: (1) In end-to-end experiments, our approach significantly improves data efficiency, leading to faster convergence in LLM training and achieving an average improvement up to 10.5% across various language model benchmarks compared to baseline methods (4.1); and (2) Ablation studies confirm that the design and implementation of key components in our multi-agent framework are crucial for attaining this advanced performance (4.2). These findings highlight the effectiveness of our method to optimize data efficiency for LLM training."
        },
        {
            "title": "2 CASE STUDY - INHERENT CONFLICTS IN DATA SELECTION",
            "content": "In this section, we present several observations derived from the SlimPajama datasets (Soboleva et al., 2023), which reveal some inherent conflicts for different data selection measurements. To 2 conduct this case study, we first label all data from the SlimPajama datasets using the quality scorer FineWeb-Edu (Lozhkov et al., 2024). We then divide the data into subsets based on domain and quality ranges. From each subset, we uniformly sample data to assess topic diversity, i.e., the topic classification of the sampled data according to our methods. We analyze this diversity by examining the topic distribution within each subset. Additionally, we compute the normalized influence of the data on pretrained 1.3B model at the 1500th step using influence functions to evaluate the datas impact on the model (Engstrom et al., 2024). Figure 1 illustrates the results, which presents bar chart representing four dimensions: quality, domain, topic diversity, and influence on the pretrained model. The x-axis shows data quality, with higher intervals reflecting better scores from the FineWeb-Edu quality scorer. The y-axis indicates the datasets domain, while the z-axis shows topic diversity within each subset, with taller bars indicating more diversity. The color gradient represents influence on the model, with darker shades showing greater impact. From this analysis, we highlight the following interesting observations: High-quality data identified by the quality scorer may not have significant impact on model performance. For example, ArXiv documents rated between 4 and 5 by the scorer are considered high-quality. However, at the 1500th training step, they exert minimal impact on the model according to the influence functions, revealing discrepancy between data quality and model impact. This observation is consistent with the previous discussion in Engstrom et al. (2024). High-quality data may exhibit low topic diversity. Documents in the Book domain with quality score of 4 to 5 are classified as high-quality by the scorer. Nevertheless, 85% of these documents belong to the same topic, indicating lack of diversity. Data with high topic diversity may not strongly influence model performance. Documents from the C4 domain display considerable topic diversity. However, at the 1500th training step, they have limited impact on the model as measured by the influence functions, suggesting conflict between diversity and model influence. Data with high topic diversity can be low quality. Wikipedia documents show substantial topic diversity, which benefits the topic classifier. However, some of these documents are rated as lowquality by the quality classifier, revealing trade-off between diversity and quality. We believe this inherent conflict illustrates that naive ensemble of these mechanisms may lead to poor performance in terms of data efficiency for LLM pretraining, which motivates the design and implementation of our multi-agent collaborative framework in 3."
        },
        {
            "title": "3 MULTI-AGENT COLLABORATIVE DATA SELECTION",
            "content": "In this section, we present the formalization of the data selection problem in 3.1, outline the overall framework of our methods in 3.2, and detail the agent initialization and update in 3.3, along with the collaborative mechanism in 3.4."
        },
        {
            "title": "3.1 PROBLEM FORMULATION",
            "content": "We follow the definition of the data selection problem in Engstrom et al. (2024) and Yu et al. (2024) with slight modification. The objective for data selection is to choose subset of size from the entire pretraining dataset in such way that the trained models loss on downstream tasks is minimized. Let represent an optimization algorithm that maps training dataset to trained model. The optimal subset of the pretraining dataset can be expressed as: := arg min L(Dk M, Deval), DkD,Dk=k where L(Dk M, Deval) := ExDeval [ℓ(x; O(M, Dk))] , (1) where Deval denotes the downstream tasks and ℓ() denotes the loss function on the data sample x. Minimizing this objective directly is computationally challenging. Given that the real downstream tasks are unknown during model training, prior works have approximately optimized this problem by minimizing the loss on selected reference tasks Dref (e.g. LAMBADA Paperno et al. (2016), SQuAD Rajpurkar (2016) and Jeopardy Tunguz (2019) in Engstrom et al. (2024)). Specifically, they train proxy models to compute one-step training loss (Yu et al., 2024) or influence functions (Engstrom et al., 2024) on the reference tasks to approximate the true loss. However, this approach heavily depends on the selection of the reference tasks, while the chosen reference tasks may not be fully representative of all potential downstream tasks. 3 Figure 2: Illustration of multi-agent collaborative framework. Multi-agent collaborative framework for pretraining data selection that integrates multiple perspectives by combining offline priors and online model-derived preferences. To avoid this obstacle, we do not directly minimize the loss on the reference tasks. Instead, we view this loss as reward signal that guides the update of predefined data selection methods. Concretely, we define reward function R(Dk M, Dref), where the reward is based on the performance gain of current model trained on the subset Dk and evaluated on the reference tasks Dref. Then our optimization goal becomes maximizing this reward over time, as: [R(Dk M, Dref)] , = arg max DkD,Dk=k where R(Dk M, Dref) := ExDref [ℓ(x; O(M, Dk))] . (2)"
        },
        {
            "title": "3.2 MULTI-AGENT COLLABORATIVE DATA SELECTION FRAMEWORK",
            "content": "In order to solve the optimization problem in Equation 2, we develop framework illustrated in Figure 2. This framework consists of two primary stages: the offline labeling stage and the online update stage. Before the training process, some initial information (i.e., the initialized measurements in some heuristic) is computed for the entire pretraining corpus, and this information is stored separately in each agents memory (formally defined below). During the training process, the current model (i.e., LLM to be trained) is used to update the agents memory and their collaboration mechanism based on rewards computed on the current model. An agent console is responsible for aggregating the opinions of each agent and making the final data selection decision. Formally, we define the agent in Definition 1 and the agent console in Definition 2. Definition 1 (Agent). An agent is data selection method defined by specific attribute (e.g., quality, domain, or topic) with memory HA that stores labels for each data point and their associated scores. During training, the agent samples data DA, calls the current model to compute the reward R(xi) for each sample xi DA, and updates its internal weights wA that determine the score for each data point based on the rewards calculated by this particular agent. The agent then assigns score SA to each data point based on its memory and the updated weights. One agents objective is to maximize its reward R(DA M, Dref) by updating this agents internal weights and increasing the score of higher-reward data points. Definition 2 (Agent Console). The agent console is coordination mechanism that consolidates scores SA(xi) from multiple agents {A1, . . . , An} to calculate final score S(xi) for each data point xi. The console adjusts the collaborative weights θA for each agent based on their respective aggregate rewards RA, balancing their contributions during training. In cases where there are 4 Figure 3: Illustration of training process for topic classifier. This diagram shows the process of training BERT-based topic classifier using CommonCrawl data. 1.44 billion documents are clustered to generate topics. GPT-4o handles topic summarization and annotation, while BERT model is trained to classify 13 topics, with humans doing final proofreading. conflicts in the decisions made by agents, the console resolves these by adjusting the weights θA to prioritize the agents that have greater positive impact on the models performance, ensuring an effective data selection process. Now the reward signal is actually came from multiple agents, the optimization goal in Equation 2 becomes maximizing the expectation of collaborative agents E{A1,...,An} [R(Dk M, Dref)]. In our current implementation, we include three agents, which are topic agents, quality agents and domain agents. They are aiming to maximize the rewards from topic, quality and domain perspective respectively. In the following sections, we will detail how we initialize and update single agent (3.3), and how we update the agent console for multi-agent collaboration (3.4)."
        },
        {
            "title": "3.3 SINGLE AGENT INITIALIZATION AND UPDATE",
            "content": "Agent initialization. As defined in Definition 1, for particular agent, we have to maintain its memory HA throughout the training process. Before training process begin, we label the whole training dataset offline and store the labled information to the memory of corresponding agents. Specifically, for each data point xi D, i.e., single document in our settings, we first get the quality, topic and domain label using scorer and classifier. For quality agent, we adopt the FineWeb-Edu quality scorer (Lozhkov et al., 2024), which is finetuned as BERT-like regression model Merrick et al. (2024) using Llama3-70B-Instruct annotated 500k examples. This will give out successive quality score Quality(xi) R[0,5] with higher score represent higher quality. We then map this score into five quality intervals {Ij}5 (cid:26)[j 1, j), [4, 5], if Quality(xi) [j 1, j) if Quality(xi) [4, 5] Quality(xi) (cid:55) Ij = (j = 1, 2, 3, 4) j=1, as (j = 5) (3) We store the quality interval corresponding to each data point in the quality agents memory. For the domain agent, we use the documents meta-information, label the data with domain information and save this into the domain agents memory, where the domain Domain(xi) belongs to one of ArXiv, Book, Wikipedia, CommonCrawl, GitHub, StackExchange, C4. For the topic agent, due to the absence of suitable pretrained model for topic classification and labeling, we designed classification schema using 1.44 billion documents collected by the Common Crawl project (Project, 2007) and fine-tuned BERT-like regression model on 500k GPT-4o annotated samples, the overall pipeline is depicted in Figure 3. Further details on the topic classification approach and BERT model training are provided in A.1. Using this topic classifier, we categorize each document into one of 13 topics: Activity, Education, Entertainment, Finance, Health, Business and Industrial, Infrastructure, Literature and Art, Nature, Others, Law and Government, Networking, Technology, and store the topic information in the topic agents memory. We initialize the weight of the topic agent and domain agent following the RegMix Liu et al. (2024) framework. Unlike the original RegMix, which only considers mixing data based on domain labels, we examine data mixing weights based on domain as well as the topic labels. We initialize our quality agent similar to the data selection decision of QuRating Wettig et al. (2024) and FineWebEdu Lozhkov et al. (2024). Further details can be found in A.2.3. The initial weights for each agent are stored in their respective memory. During the training phase, we leverage the current model to adjust the weight of each agent. As depicted in Figure 2, at the data selection stage, each agent performs several actions to update its memory and inform decision-making. Take domain agent as an example, it takes three-step action during data selection stage: (1) Sample pretraining data points from the pretraining data pool, distributing them uniformly across each domain; (2) Call the current model to assess the reward of each data point and gather feedback; (3) Update the memory of domain weights based on gathered feedback and adjusts the score for each data point by incorporating prior knowledge from the offline labeling process. This process is similarly followed by the quality agent and the topic agent. Agent update. After sampling data uniformly from agent search space, each agent updates its internal weights using local information based on the sampled data. For example, for domain agent, it calculates the average influence of each domain. For agent {AQuality, ADomain, ATopic}, it updates its internal weights by calculating the overall rewards sampled from each interval as: (cid:88) R(xi M, Dref), (4) A = 1 Dj xiDj where Dj represents the sample set of the j-th subcategory under agent A, e.g. Wikipedia for domain agent. And xi is sample within this sample set. Then, the policy gradient method is used to update the weight for each subcategory wj A: wj wj (5) where ηA is the learning rate for agent A. The overall updated weight of agent is an vector in nA dimension, wA = [w1 ], where nA is the number of total subcategory within the space of agent A. Utilizing the prior memory stored by each agent, it can give out final score for each data point as SA(xi) = wj A, where is the subcategory that xi belongs to. A, ..., wnA + ηA A, Calculating the rewards. For each sampled data point, we approximate rewards using influence functions Engstrom et al. (2024); Yu et al. (2024). The influence function value is computed to measure the impact of each sample on the models performance. The formula for the influence function is: R(xi M, Dref) = ˆExDref [ℓ(x; O(M, Dk))] , (6) = ML(Dref M)H 1 ML(xi M), where HM = 1 lating influence functions for pretraining data point can be found in A.4. MLM(xi M) is the Hessian and its positive definite. Details of calcuj=1 (cid:80)n"
        },
        {
            "title": "3.4 MULTI-AGENT COLLABORATION",
            "content": "Ultimately, the agent console defined in Definition 2 aggregates all agents feedback to compute final score for each data point, determining the final data selection decision. Multi-agent collaboration. In the context of multi-agent collaboration, the weighted score for each agent must be calculated to evaluate their respective contributions effectively. This calculation takes into account various factors specific to each agent. For every data sample xi, the overall score S(xi) is determined by the following formula: S(xi) = (θQuality SQuality(xi) + θDomain SDomain(xi) + θTopic STopic(xi)), (7) where SQuality(xi), SDomain(xi), and STopic(xi) are the scores calculated by the quality, domain, and topic agents for the sample xi, respectively. And θA {θQuality, θDomain, θTopic} is the collaborative weight for each agent, which is updated during training process. 6 Algorithm 1 Multi-agent collaborative data selection for LLM pretraining Require: Training data D, reference task Dref, main model M, optimizer O, total training steps , selected size k, update step , Memory for each agent HA Sample data points uniformly from agent search space Compute rewards RM(xi; Dref) for each sampled data point xi Update agent weight wA wA + ηA RA for each agent do if mod = 0 then 1: Initialize model parameters for main model 2: Initialize Dk as size-k randomly sampled subset from 3: for = 1 to do 4: 5: 6: 7: 8: 9: 10: 11: 12: 13: 14: 15: 16: 17: end for end if Sample batch of data from Dk Update Main Model O(M, B) end for Compute agent score RA and average score according to Eq. 8 Update collaborative weight θA θA + ηA (RA R). Calculate coordinator score S(xi) for xi according to Eq. 7 Select dataset for next training stage Dk Top-k(S(xi)) for xi Collaborative weight update. To dynamically adjust the importance of each agent during various training phases, we modify the agents collaborative weight based on its overall rewards. We compute the reward of each agent and the average reward across all agents: RA = 1 (cid:88) j=1 wj j A, = (cid:88) RA, 1 (8) This information is then used to update each agents collaborative weight, which is stored in the agent consoles memory for future decision-making: θA θA + ηA (RA R). (9) By continuously refining these weights, the collaboration strategy adapts to optimize overall performance and appropriately adjust the role of each agent throughout different stages of training. The complete training pipeline is outlined in Algorithm 1."
        },
        {
            "title": "4 EXPERIMENTS",
            "content": "We conduct series of experiments to evaluate the effectiveness of our multi-agent collaborative data selection method. Comprehensively, we find that: (1) In the end-to-end experiments, our approach introduces significant improvement in terms of data efficiency leading to faster convergence for LLM training, and achieves up to 10.5% improvements on average across various language model benchmarks when compared with other baseline approaches (4.1); (2) We also verify that the design and implementation of the core components in our multi-agent framework design are necessary to reach this advanced performance through set of carefully designed ablation studies (4.2)."
        },
        {
            "title": "4.1 END-TO-END EXPERIMENTS",
            "content": "We evaluate our multi-agent framework against wide category of state-of-the-art approaches to compare the data efficiency for LLM pretraining. We train 1.3 billion parameter LLAMA-2 architecture model with 30 billion selected tokens. Experimental setup. We first enumerate the experimental setup as below: Pretraining datasets. We utilize the popular SlimPajama (Soboleva et al., 2023) dataset including 627 billion tokens, which is derived from the RedPajama (Computer, 2023) dataset. The SlimPajama (Soboleva et al., 2023) provide the meta-data about the domain information for each sample. Before the training process, we annotate the entire dataset using the FineWeb-Edu quality scorer (Penedo et al., 2024) along with our custom-trained BERT-based topic classifier. The training details for the topic classifier is provided in Appendix A.1. 7 Table 1: Our approach improves model performance across multiple tasks. To ensure all demonstrations fit within the 1024-token context window, we present comprehensive results table for 0-shot, 3-shot, and 5-shot scenarios in Appendix A.3. The selected tasks in this table include: Problem Solving: ARC-Easy (3), ARC-Challenge (3), MathQA (3), MMLU (3); Commonsense Reasoning: OpenBookQA (3), SocialIQA (3), Winogrande (0), CommonsenseQA (5); and Reading Comprehension: BoolQ (0), Race (5). For the QuRating and DSIR methods, we use their best-performing variants: QuRating-Edu and DSIR-Wiki, respectively."
        },
        {
            "title": "Selection Method",
            "content": "Random sampling - 30B tokens Random sampling - 60B tokens Perplexity PPL (Ankner et al., 2024) Classifier-based data selection QuRating (Wettig et al., 2024) FineWeb-Edu (Penedo et al., 2024) DSIR (Xie et al., 2023b) Domain mixing methods DOGE Fan et al. (2024) DoReMi (Xie et al., 2023a) DMLaw (Ye et al., 2024) RegMix (Liu et al., 2024) Influence MATES (Yu et al., 2024) Multi-agent collaboration (ours) Problem Solving (4 tasks) 31.1 33.62.5 29.91.2 34.13.0 32.61.5 30.90.2 30.90.2 30.40.7 30.20.9 30.70.4 30.90. 36.75.6 Commonsense Reasoning (4 tasks) Reading Comprehension (2 tasks)"
        },
        {
            "title": "Average",
            "content": "(10 tasks) 32.9 33.70.8 30.52.4 34.11.2 33.00.1 32.00.8 32.20.7 32.60.3 32.10.9 32.50.4 34.01. 34.81.9 43.1 46.13.0 42.40.7 41.41.7 45.32.2 41.51.6 45.12.0 44.81.7 45.12.0 44.61.5 46.53. 45.92.8 34.2 36.11.9 32.71.5 35.61.4 35.31.1 33.50.7 34.30.1 34.10.1 33.90.3 34.20.0 35.31. 37.83.6 Training details. We adopt the model architecture from LLAMA-2 (Touvron et al., 2023b) at the scale of 1.3 billion parameters (see the detailed configuration in Appendix A.2-Table 3). Following the principles of the scaling law (Hoffmann et al., 2022) and the DCLM framework (Li et al., 2024), we decide to use total of 30 billion tokens. All training tokens are sampled from the 670 billion-token SlimPajama (Soboleva et al., 2023) dataset using various sampling strategies. Further details regarding the training process can be found in A.2. Evaluation benchmarks. To evaluate the pre-trained models thoroughly, we conduct extensive assessments across various downstream tasks, categorized into three areas: (1) problem solving: MMLU (Hendrycks et al., 2021), ARC-Easy/Challenge (Clark et al., 2018), and MathQA Welbl et al. (2017); (2) commonsense reasoning: SIQA (Sap et al., 2019), WinoGrande (Sakaguchi et al., 2020), OpenbookQA (Mihaylov et al., 2018), and CommonsenseQA (Talmor et al., 2019); (3) reading comprehension: RACE (Lai et al., 2017) and BoolQ (Clark et al., 2019). Evaluations are conducted using the lm-evaluation-harness framework (Gao et al., 2023) in an in-context learning setting, and average accuracy is reported for easy comparison. Baselines. We select wide range of baselines to conduct extensive the data efficiency comparison, where these methods can be classified to five main categories: (1) random sampling, we test this policy with both the standard data volume of 30B tokens and supplemented version with 60B tokens; (2) perplexity-based data selection Ankner et al. (2024); (3) classifier-based data selection, where we select the following methods: QuRating Wettig et al. (2024), FineWeb-Edu Penedo et al. (2024), DSIR-Book Xie et al. (2023b) and DSIR-Wiki Xie et al. (2023b); (4) domain mixingbased methods, where we select the following methods: DOGE Fan et al. (2024), DoReMi Xie et al. (2023a), DMLaw Ye et al. (2024) and RegMix Liu et al. (2024); and (5) influence function based methods for online data selection, i.e., MATES Yu et al. (2024). Implementation details of these baselines can be found in A.2.2. Results. We present the results of three types of downstream tasks in Table 1, with the complete 0-shot  (Table 5)  , 3-shot  (Table 6)  , and 5-shot  (Table 7)  results for all tasks enumerated in A.3. We highlight that our methods show substantial improvement in the average performance across all downstream tasks when compared with all the baselines. Concretely, we observe that when compared with the random sampling based approach, our method not only significantly outperforms the standard 30 billion token setup but also surpasses the model trained on 60 billion tokens with performance gain of 4.7%. Similarly, we also show an improvement of 15.6% compared with perplexity-based data selection Ankner et al. (2024), an improvement of up to 6.2% compared with classifier-based data selection, an improvement of up to 10.2% compared with domain mixingbased methods, and an improvement of 7.1% compared with influence function based approach, i.e., MATES Yu et al. (2024). Discussion. We highlight that our proposed multi-agent collaborative data selection mechanism introduces statistical efficiency in terms of LLM training convergence and also provides some computational efficiency in terms of data processing overheads. In terms of statistical efficiency, our method consistently outperforms others at every benchmarked training step, as shown in Figure 4. While MATES Yu et al. (2024) performs comparably to our methods during the early training phase (steps 1500 to 3000), its performance drops in later stages. This aligns with its original paper, which notes that relying solely on influence functions for specific reference tasks (e.g., LAMBADA (Paperno et al., 2016)) can degrade performance in mid-to-late pretraining. Despite this, MATES still outperforms other methods without dynamic adjustments shown in Figure 4. In contrast, our multi-agent collaborative data selection mechanism can dynamically adjust the corresponding weights from different agents and select data based on the most up-to-date model preferences, effectively mitigating biases and surpassing other domain-mixing and data-selection techniques. In terms of computational efficiency, we also achieve higher computational efficiency than previous methods. For example, QuRating Wettig et al. (2024) requires around 2000 NVIDIA A800 GPU hours to label the entire SlimPajama dataset, while our offline labeling takes just 180 GPU hours. MATES Yu et al. (2024), which recalculates influence scores and trains BERT model for each labeling cycle, incurs 360 GPU hours for four-stage update. Additionally, MATES labels are only usable in the next training stage, making it time-consuming and difficult to scale. In contrast, our method can improve the computational efficiency from two aspects: (1) we find that group of light-weight agents collaboratively enables superior data selection, which is more computational efficiency than any method that requires heavy data processing or label procedure; (2) the collaborative, dynamic learning procedure introduced in our multi-agent framework is computational efficient; by using sampled holdout set and CPU-based calculations for updating agent parameters, our computational overhead is ignorable compared with heavy LLM training computation. Figure 4: Downstream three-shot performance of the 1.3B model in relation to pretraining steps, using 7500 steps for 30B tokens. Our methods outperform baselines from all categories."
        },
        {
            "title": "4.2 ABLATION STUDY",
            "content": "We introduce set of carefully designed ablation studies to justify the design and implementation of our multi-agent collaborative data selection framework. Concretely, (1) we test the combination of different agents to show the advance introduced by collaboration, and (2) we verify the necessity of the dynamic adjustment of the agents weight for data selection. Experiment setups. The dataset used in our experiments is identical to that in the end-to-end experiment. We perform our ablation study using 373M model. The complete model architecture is detailed in Appendix A.2-Table 3. We train all these 373M models using 8 A800 GPUs for 5000 steps. Our evaluation tasks are similar to those in the end-to-end results, but we specifically report the in-context learning performance for 3-shot evaluation. Results and discussion. The results of the ablation study are shown in Table 2. We want to highlight the result from two aspects: First, the ablation study underscores the importance of each agent in achieving optimal performance across the training tasks. When the quality, domain, and topic agents are used together, the model performs best, highlighting the benefits of their combined use, as shown in Table 2. In terms of evaluating the performance of each agent, we find that the quality agent outperforms other single-agent configurations, excelling in problem solving tasks. However, its performance drops by 7.6% on tasks requiring domain knowledge or contextual understanding. Here, the domain and topic agents play crucial role, as they excel in these areas. Despite this, 9 Table 2: This ablation study examines the performance of various combinations of agent collaboration and update mechanisms. Accuracy is reported for all tasks, with the highest value in each column shown in bold."
        },
        {
            "title": "Commonsense Reasoning",
            "content": "Reading Compreh."
        },
        {
            "title": "Selection Method",
            "content": "ARC-E ARC-C MathQA MMLU O.B.QA"
        },
        {
            "title": "SIQA",
            "content": "W.G. C.S.QA"
        },
        {
            "title": "Average",
            "content": "Quality&Domain&Topic Agent without collaboration update Domain&Quality Agent Topic&Quality Agent Domain&Topic Agent"
        },
        {
            "title": "No Agent",
            "content": "57.9 47.9 55.1 56.2 44.6 53.0 44.1 42.7 42.5 24.7 20.4 18.6 24.4 18. 24.7 19.1 19.2 20.0 21.9 21.0 21.7 21.8 21.7 21.8 20.8 21.0 21. 25.4 25.1 24.4 25.2 25.7 25.5 25.6 27.0 23.8 20.2 17.2 17.4 19.4 16. 18.0 16.6 17.4 14.6 37.9 37.3 37.1 36.3 36.6 36.3 36.8 37.1 35. 52.6 51.3 51.2 49.0 51.9 49.5 52.0 50.7 50.1 20.4 20.0 19.8 19.7 19. 18.1 19.7 19.7 18.8 59.6 56.5 61.7 56.1 61.6 57.0 56.7 54.6 56. 29.4 28.3 28.2 28.5 27.8 28.0 28.2 28.5 27.9 35.0 32.5 33.5 33.6 32. 32.9 32.0 31.8 31.1 neither performs well on problem solving tasks, except for the topic agent, which significantly imIn terms of proves MMLU performance, indicating that topic diversity may benefit such tasks. evaluating the combination of the agents, we find that removing any agent noticeably reduces overall accuracy, though the impact varies. Excluding the quality agent leads to the largest drop, over 7.4%, significantly affecting performance in problem solving tasks, and commonsense reasoning tasks like OpenbookQA. This highlights the quality agents vital role in reasoning and problemsolving. Similarly, excluding the topic agent causes performance drop in ARC-Challenge and 4% reduction in MMLU, emphasizing its importance in tasks covering diverse subjects; removing the domain agent results in 5% performance drop in commonsense reasoning tasks, underscoring its key contribution to these areas. Second, the ablation study verifies the design and implementation of the collaborative dynamic adjustment of the agents weights (introduced 3.4) for efficient data selection. Concretely, in this variant, all agents were initialized with equal weights, which remained fixed throughout training without adjusting for individual agent performance. Surprisingly, this fixed-weight approach (equal to random sampling) resulted in significant 7.1% drop in overall performance compared to the dynamic weighting used in the collaborative update framework, as shown in Table 2. We believe this result from the ablation study is strong indicator that the dynamic adjustment of the celebration mechanism is essential for efficient data selection."
        },
        {
            "title": "5 RELATED WORK",
            "content": "Data selection in LLM pretraining. Selecting high-quality pretraining data from large corpora is crucial for effective LLM training. Recent approaches leverage various methodologies for efficient data selection. Concretely, classifiers (Brown, 2020; Chowdhery et al., 2023; Du et al., 2022; Xie et al., 2023b) and language modeling perplexity (Wenzek et al., 2020; Thrush et al., 2024) have been applied to identify data resembling high-quality samples; recently, more advanced quality scores based on classifier have shown the effectiveness in data selection, e.g., QuRating (Wettig et al., 2024), FineWeb-Edu (Lozhkov et al., 2024), etc. Data mixture is another effective way to improve data diversity, at both token level (Touvron et al., 2023a; Gao et al., 2020; Soboleva et al., 2023) and sample level, e.g., DoReMi (Xie et al., 2023a), DOGE (Fan et al., 2024), DMLaw (Ye et al., 2024), and RegMix (Liu et al., 2024); very recently, topic distributions has also been considered as an effective data mixing method, e.g., the downsampling overrepresented topics in Llama 3.1 (Team, 2024). Influence functions have been studied to understand for data efficiency (Koh & Liang, 2017), and some recent attempts based on efficient approximation have been proposed to improve data efficiency in LLM pretraining (Schioppa et al., 2022; Grosse et al., 2023; Isonuma & Titov, 2024); for example, MATES (Yu et al., 2024) uses staged BERT model to assess data influence, QUAD (Zhang et al., 2024) leverage cluster information to reduce the computational cost of calculating individual data influence. Multi-agent collaborative frameworks. Multi-agent collaborative frameworks (Russell & Norvig, 2016; Wooldridge, 2009) facilitate cooperative problem solving among autonomous agents and have been widely applied to solve various problems, e.g., neural optimizer search (Bello et al., 2017), collebrative LLM programming (Hong et al.). In these systems, agents may have conflicting goals and independently take actions based on their objectives; reward mechanism evaluates these ac10 tions, providing feedback that allows each agent to adjust and refine its strategy over time; central console coordinates the agents by synthesizing their feedback and guiding the overall system towards more optimal decisions (Russell & Norvig, 2016; Wooldridge, 2009). Collaboration among agents is dynamic, as they adapt their ability to work together improves (Olfati-Saber, 2006)."
        },
        {
            "title": "6 CONCLUSIONS",
            "content": "In this paper, we addressed the critical challenge of efficient data selection for LLM pretraining by proposing novel multi-agent collaborative data selection mechanism. Our framework allows multiple data selection methods to operate as independent agents, with an agent console designed to dynamically integrate their outputs throughout the LLM training process. Through extensive empirical studies, we demonstrated that our approach significantly improves data efficiency, leading to faster convergence during LLM pretraining and achieving an average performance improvement up to 10.5% across various language model benchmarks compared to state-of-the-art methods. Such promising results highlight the effectiveness of combining multiple data selection strategies dynamically, offering solution to resolve the potential inherent conflicts from data selection methods to provide superior data efficiency for LLM pretraining."
        },
        {
            "title": "REFERENCES",
            "content": "Zachary Ankner, Cody Blakeney, Kartik Sreenivasan, Max Marion, Matthew Leavitt, and Mansheej Paul. Perplexed by perplexity: Perplexity-based pruning with small reference models. In ICLR 2024 Workshop on Mathematical and Empirical Understanding of Foundation Models, 2024. URL https://openreview.net/forum?id=0r0Bg1NY1X. Irwan Bello, Barret Zoph, Vijay Vasudevan, and Quoc Le. Neural optimizer search with reinIn International Conference on Machine Learning, pp. 459468. PMLR, forcement learning. 2017. Andrei Broder. On the resemblance and containment of documents. In Proceedings. Compression and Complexity of SEQUENCES 1997 (Cat. No. 97TB100171), pp. 2129. IEEE, 1997. Tom Brown. Language models are few-shot learners. arXiv preprint arXiv:2005.14165, 2020. Jie Chen, Zhipeng Chen, Jiapeng Wang, Kun Zhou, Yutao Zhu, Jinhao Jiang, Yingqian Min, Wayne Xin Zhao, Zhicheng Dou, Jiaxin Mao, et al. Towards effective and efficient continual pre-training of large language models. arXiv preprint arXiv:2407.18743, 2024. Aakanksha Chowdhery, Sharan Narang, Jacob Devlin, Maarten Bosma, Gaurav Mishra, Adam Roberts, Paul Barham, Hyung Won Chung, Charles Sutton, Sebastian Gehrmann, et al. Palm: Scaling language modeling with pathways. Journal of Machine Learning Research, 24(240): 1113, 2023. Christopher Clark, Kenton Lee, Ming-Wei Chang, Tom Kwiatkowski, Michael Collins, and Kristina Toutanova. Boolq: Exploring the surprising difficulty of natural yes/no questions. In Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers), pp. 29242936, 2019. Peter Clark, Isaac Cowhey, Oren Etzioni, Tushar Khot, Ashish Sabharwal, Carissa Schoenick, and Oyvind Tafjord. Think you have solved question answering? try arc, the ai2 reasoning challenge. arXiv preprint arXiv:1803.05457, 2018. Together Computer. Redpajama: an open dataset for training large language models, 2023. URL https://github.com/togethercomputer/RedPajama-Data. Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. BERT: pre-training of deep bidirectional transformers for language understanding. CoRR, abs/1810.04805, 2018. URL http://arxiv.org/abs/1810.04805. 11 Nan Du, Yanping Huang, Andrew Dai, Simon Tong, Dmitry Lepikhin, Yuanzhong Xu, Maxim Krikun, Yanqi Zhou, Adams Wei Yu, Orhan Firat, et al. Glam: Efficient scaling of language models with mixture-of-experts. In International Conference on Machine Learning, pp. 5547 5569. PMLR, 2022. Logan Engstrom, Axel Feldmann, and Aleksander Madry. Dsdm: Model-aware dataset selection with datamodels. arXiv preprint arXiv:2401.12926, 2024. Simin Fan, Matteo Pagliardini, and Martin Jaggi. DOGE: Domain reweighting with generalization estimation. In Forty-first International Conference on Machine Learning, 2024. URL https: //openreview.net/forum?id=7rfZ6bMZq4. Leo Gao, Stella Biderman, Sid Black, Laurence Golding, Travis Hoppe, Charles Foster, Jason Phang, Horace He, Anish Thite, Noa Nabeshima, et al. The pile: An 800gb dataset of diverse text for language modeling. arXiv preprint arXiv:2101.00027, 2020. Leo Gao, Jonathan Tow, Baber Abbasi, Stella Biderman, Sid Black, Anthony DiPofi, Charles Foster, Laurence Golding, Jeffrey Hsu, Alain Le Noach, Haonan Li, Kyle McDonell, Niklas Muennighoff, Chris Ociepa, Jason Phang, Laria Reynolds, Hailey Schoelkopf, Aviya Skowron, Lintang Sutawika, Eric Tang, Anish Thite, Ben Wang, Kevin Wang, and Andy Zou. framework for few-shot language model evaluation, 12 2023. URL https://zenodo.org/records/ 10256836. Roger Grosse, Juhan Bae, Cem Anil, Nelson Elhage, Alex Tamkin, Amirhossein Tajdini, Benoit Steiner, Dustin Li, Esin Durmus, Ethan Perez, et al. Studying large language model generalization with influence functions. arXiv preprint arXiv:2308.03296, 2023. Dan Hendrycks, Collin Burns, Steven Basart, Andy Zou, Mantas Mazeika, Dawn Song, and Jacob Steinhardt. Measuring massive multitask language understanding. In International Conference on Learning Representations, 2021. URL https://openreview.net/forum?id= d7KBjmI3GmQ. Jordan Hoffmann, Sebastian Borgeaud, Arthur Mensch, Elena Buchatskaya, Trevor Cai, Eliza Rutherford, Diego de Las Casas, Lisa Anne Hendricks, Johannes Welbl, Aidan Clark, et al. Training compute-optimal large language models. In Proceedings of the 36th International Conference on Neural Information Processing Systems, pp. 3001630030, 2022. Sirui Hong, Mingchen Zhuge, Jonathan Chen, Xiawu Zheng, Yuheng Cheng, Jinlin Wang, Ceyao Zhang, Zili Wang, Steven Ka Shing Yau, Zijuan Lin, et al. Metagpt: Meta programming for In The Twelfth International Conference on Learning multi-agent collaborative framework. Representations. Masaru Isonuma and Ivan Titov. Unlearning traces the influential training data of language models. In Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pp. 63126325, 2024. Carolyn Kim, Ashish Sabharwal, and Stefano Ermon. Exact sampling with integer linear programs In Proceedings of the AAAI Conference on Artificial Intelligence, and random perturbations. volume 30, 2016. Pang Wei Koh and Percy Liang. Understanding black-box predictions via influence functions. In International conference on machine learning, pp. 18851894. PMLR, 2017. Guokun Lai, Qizhe Xie, Hanxiao Liu, Yiming Yang, and Eduard Hovy. RACE: Large-scale In Proceedings of the 2017 Conference ReAding comprehension dataset from examinations. on Empirical Methods in Natural Language Processing, pp. 785794, Copenhagen, Denmark, September 2017. Association for Computational Linguistics. doi: 10.18653/v1/D17-1082. URL https://aclanthology.org/D17-1082. Jeffrey Li, Alex Fang, Georgios Smyrnis, Maor Ivgi, Matt Jordan, Samir Gadre, Hritik Bansal, Etash Guha, Sedrick Keh, Kushal Arora, et al. Datacomp-lm: In search of the next generation of training sets for language models. arXiv preprint arXiv:2406.11794, 2024. 12 Qian Liu, Xiaosen Zheng, Niklas Muennighoff, Guangtao Zeng, Longxu Dou, Tianyu Pang, Jing Jiang, and Min Lin. Regmix: Data mixture as regression for language model pre-training. arXiv preprint arXiv:2407.01492, 2024. Anton Lozhkov, Loubna Ben Allal, Leandro von Werra, and Thomas Wolf. Fineweb-edu, May 2024. URL https://huggingface.co/datasets/HuggingFaceFW/fineweb-edu. Luke Merrick, Danmei Xu, Gaurav Nuti, and Daniel Campos. Arctic-embed: Scalable, efficient, and accurate text embedding models. arXiv preprint arXiv:2405.05374, 2024. Todor Mihaylov, Peter Clark, Tushar Khot, and Ashish Sabharwal. Can suit of armor conduct electricity? new dataset for open book question answering. In Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, pp. 23812391, Brussels, Belgium, 2018. Association for Computational Linguistics. doi: 10.18653/v1/D18-1260. URL https://aclanthology.org/D18-1260. Reza Olfati-Saber. Flocking for multi-agent dynamic systems: Algorithms and theory. IEEE Transactions on automatic control, 51(3):401420, 2006. OpenAI. Hello gpt-4o. https://openai.com/index/hello-gpt-4o/, 2024. Denis Paperno, German David Kruszewski Martel, Angeliki Lazaridou, Ngoc Pham Quan, Raffaella Bernardi, Sandro Pezzelle, Marco Baroni, Gemma Boleda Torrent, Fernandez Raquel, et al. The lambada dataset: Word prediction requiring broad discourse context. In The 54th Annual Meeting of the Association for Computational Linguistics Proceedings of the Conference: Vol. 1 Long Papers, volume 3, pp. 15251534. ACL, 2016. Sung Min Park, Kristian Georgiev, Andrew Ilyas, Guillaume Leclerc, and Aleksander Madry. Trak: Attributing model behavior at scale. arXiv preprint arXiv:2303.14186, 2023. Guilherme Penedo, Hynek Kydlıˇcek, Anton Lozhkov, Margaret Mitchell, Colin Raffel, Leandro Von Werra, Thomas Wolf, et al. The fineweb datasets: Decanting the web for the finest text data at scale. arXiv preprint arXiv:2406.17557, 2024. Common Crawl Project. Common crawl: Open repository of web crawl data, 2007. URL https: //commoncrawl.org/. Jack Rae, Sebastian Borgeaud, Trevor Cai, Katie Millican, Jordan Hoffmann, Francis Song, John Aslanides, Sarah Henderson, Roman Ring, Susannah Young, et al. Scaling language models: Methods, analysis & insights from training gopher. arXiv preprint arXiv:2112.11446, 2021. Rajpurkar. Squad: 100,000+ questions for machine comprehension of text. arXiv preprint arXiv:1606.05250, 2016. Stuart Russell and Peter Norvig. Artificial intelligence: modern approach. Pearson, 2016. Noveen Sachdeva, Benjamin Coleman, Wang-Cheng Kang, Jianmo Ni, Lichan Hong, Ed Chi, James Caverlee, Julian McAuley, and Derek Zhiyuan Cheng. How to train data-efficient llms. arXiv preprint arXiv:2402.09668, 2024. Keisuke Sakaguchi, Ronan Le Bras, Chandra Bhagavatula, and Yejin Choi. Winogrande: An adversarial winograd schema challenge at scale. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 34, pp. 87328740, 2020. Maarten Sap, Hannah Rashkin, Derek Chen, Ronan Le Bras, and Yejin Choi. Social IQa: Commonsense reasoning about social interactions. In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP), pp. 44634473, Hong Kong, China, November 2019. Association for Computational Linguistics. doi: 10.18653/v1/D19-1454. URL https://aclanthology.org/D19-1454. Andrea Schioppa, Polina Zablotskaia, David Vilar, and Artem Sokolov. Scaling up influence functions. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 36, pp. 8179 8186, 2022. 13 Daria Soboleva, and Nolan Dey. Faisal Al-Khateeb, Robert Myers, tness, plicated slimpajama-a-627b-token-cleaned-and-deduplicated-version-of-redpajama, 2023. URL https://huggingface.co/datasets/cerebras/SlimPajama-627B. Joel Hesdeduand https://www.cerebras.net/blog/ Jacob Steeves, cleaned 627B token SlimPajama: RedPajama. version of Alon Talmor, Jonathan Herzig, Nicholas Lourie, and Jonathan Berant. CommonsenseQA: question answering challenge targeting commonsense knowledge. In Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers), pp. 41494158, Minneapolis, Minnesota, June 2019. Association for Computational Linguistics. doi: 10.18653/v1/N19-1421. URL https://aclanthology.org/N19-1421. Llama Team. The Llama 3 Herd of Models, 2024. URL https://ai.meta.com/research/ publications/the-llama-3-herd-of-models/. Tristan Thrush, Christopher Potts, and Tatsunori Hashimoto. Improving pretraining data using perplexity correlations. arXiv preprint arXiv:2409.05816, 2024. Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timothee Lacroix, Baptiste Rozi`ere, Naman Goyal, Eric Hambro, Faisal Azhar, et al. Llama: Open and efficient foundation language models. arXiv preprint arXiv:2302.13971, 2023a. Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, et al. Llama 2: Open foundation and fine-tuned chat models. arXiv preprint arXiv:2307.09288, 2023b. Bojan Tunguz. Jeopardy! questions, 2019. URL https://www.kaggle.com/datasets/ tunguz/200000-jeopardy-questions. Johannes Welbl, Nelson F. Liu, and Matt Gardner. Crowdsourcing multiple choice science questions. In Proceedings of the 3rd Workshop on Noisy User-generated Text, pp. 94106, Copenhagen, Denmark, September 2017. Association for Computational Linguistics. doi: 10.18653/v1/W17-4413. URL https://aclanthology.org/W17-4413. Guillaume Wenzek, Marie-Anne Lachaux, Alexis Conneau, Vishrav Chaudhary, Francisco Guzman, Armand Joulin, and Edouard Grave. Ccnet: Extracting high quality monolingual datasets from web crawl data. In Proceedings of the Twelfth Language Resources and Evaluation Conference, pp. 40034012, 2020. Alexander Wettig, Aatmik Gupta, Saumya Malik, and Danqi Chen. Qurating: Selecting high-quality data for training language models. In Forty-first International Conference on Machine Learning, 2024. URL https://openreview.net/forum?id=GLGYYqPwjy. Michael Wooldridge. An introduction to multiagent systems. John wiley & sons, 2009. Mengzhou Xia, Sadhika Malladi, Suchin Gururangan, Sanjeev Arora, and Danqi Chen. Less: Selecting influential data for targeted instruction tuning. In Forty-first International Conference on Machine Learning. Sang Michael Xie, Hieu Pham, Xuanyi Dong, Nan Du, Hanxiao Liu, Yifeng Lu, Doremi: OptimizPercy Liang, Quoc Le, Tengyu Ma, and Adams Wei Yu. In Advances in Neural Ining data mixtures speeds up language model pretraining. formation Processing Systems, volume 36, pp. 6979869818. Curran Associates, Inc., 2023a. URL https://proceedings.neurips.cc/paper_files/paper/2023/ file/dcba6be91359358c2355cd920da3fcbd-Paper-Conference.pdf. Sang Michael Xie, Shibani Santurkar, Tengyu Ma, language models via importance resampling. Data seIn A. Oh, T. Naumann, lection for InA. Globerson, K. Saenko, M. Hardt, and S. Levine (eds.), Advances in Neural formation Processing Systems, volume 36, pp. 3420134227. Curran Associates, Inc., 2023b. URL https://proceedings.neurips.cc/paper_files/paper/2023/ file/6b9aa8f418bde2840d5f4ab7a02f663b-Paper-Conference.pdf. and Percy Liang. 14 Jiasheng Ye, Peiju Liu, Tianxiang Sun, Yunhua Zhou, Jun Zhan, and Xipeng Qiu. Data mixing laws: Optimizing data mixtures by predicting language modeling performance. arXiv preprint arXiv:2403.16952, 2024. Zichun Yu, Spandan Das, and Chenyan Xiong. Mates: Model-aware data selection for efficient pretraining with data influence models. arXiv preprint arXiv:2406.06046, 2024. Chi Zhang, Huaping Zhong, Kuan Zhang, Chengliang Chai, Rui Wang, Xinlin Zhuang, Tianyi Bai, Jiantao Qiu, Lei Cao, Ye Yuan, Guoren Wang, and Conghui He. Harnessing diversity for important data selection in pretraining large language models, 2024. URL https: //arxiv.org/abs/2409.16986."
        },
        {
            "title": "A APPENDIX",
            "content": "A.1 DETAILS OF TRAINING TOPIC CLASSIFIER As shown in Figure 3, we first cluster 1.4 billion documents obtained from Common Crawl (Project, 2007) into 10,000 clusters using KNN. And we use GPT-4o (OpenAI, 2024) to generate summary for the content in each cluster. Additionally, we implement two parallel steps: unsupervised and supervised. In the unsupervised step, we perform secondary clustering of the 10,000 clusters into 100 clusters, from which we extract 20 summaries for each cluster. We utilize GPT-4o to extract category labels, refining these into coherent hierarchical labeling system for the classification of 42 distinct topics. In the supervised data processing step, leveraging Gopher cleaning ruls (Rae et al., 2021) and MinHash (Broder, 1997) deduplication, we clean the whole datasets and cluster the datasets into 10,000 clusters. We then extract 50 equidistant samples from each cluster. This process yields approximately 500,000 data points, which we categorize into the aforementioned 42 topics by calling GPT4o (OpenAI, 2024) using the prompt shown below: Figure 5: We illustrate the prompt construction process for GPT-4 to reorganize the topic of 500k data points. Since GPT-4o is not specialized for classification tasks, we obtain actual topic data with slightly more than 42 topics, as shown in Figure 6. We then manually summarize the topics provided by GPT-4 into 13 categories, ensuring that the subtopics within each category shared similarities. The detailed category distributions appear in Figure 6, along with specific clustering information. Ultimately, we employ the annotated data to fine-tune BERT-like regression model (Devlin et al., 2018). Following model classification, we conduct human proofreading to ensure accuracy, and we present the final results below. Figure 6: We illustrate the distribution of manually annotated and clustered data, which includes 13 topics: Infrastructure, Law and Government, Networking, Activity, Business and Industry, Nature, Literature and Art, Education, Finance, Technology, Entertainment, Health, and Others. 17 A.2 DETAILS OF PRETRAINING A.2.1 DETAILS OF PRETRAINING MODELS ARCHITECTURE The specific architecture of pretraining model is shown in Table 3. Each model was trained on 32x NVIDIA A800, employing global batch size of 4 220 tokens and completing 7,500 steps in about 14 hours. The average token processing rate per GPU was about 20,000 tokens per second. The learning rate was set to 5 105, and the Adam optimizer was employed with hyperparameters (β1 = 0.9, β2 = 0.95, ϵ = 108). Table 3: Architecture of pre-trained decoder-only models."
        },
        {
            "title": "Hyperparameter",
            "content": "1.3B Model Value 370M Model Value"
        },
        {
            "title": "Vocabulary Size\nMLP Ratio\nHidden Dimension Size\nNumber of Layers\nNumber of Attention Heads\nNumber of KV Attention Heads\nRoPE Base\nMaximum Context Window Length\nNumber of Parameters",
            "content": "32,000 8/3 2048 24 16 16 10,000 1024 1,345,423,360 (1.3B) 32,000 8/3 1024 24 8 8 10,000 1024 373,867,520 (370M) A.2.2 DETAILS OF BASELINE METHOD IMPLEMENTATION Regarding the classifier methods, QuRating (Wettig et al., 2024) and DSIR (Xie et al., 2023b), we implement QuRating by downloading the open-source checkpoint from Hugging Face. Notably, the released model has context length of 4096, whereas ours is 1024. However, this discrepancy does not impact our testing tasks, as our maximum of 5-shot examples remains within the 1024 limit. Despite this, we have totally similar model configuration as well as the total number of training tokens with all the checkpoints we downloaded. Similarly, the replication of PPL is based on the publicly available checkpoint from the original paper. For FineWeb-Edu (Lozhkov et al., 2024), we download the quality scorer to label all the training data from SlimPajama datasets, and adopt the methodology described in the corresponding publication and train all the model from scratch. Domain mixing refers to the technique of combining data from different sources or domains to enhance the diversity and robustness of models training dataset. In our implementation, we apply various mixing methods: DoReMi (Xie et al., 2023a), DOGE (Fan et al., 2024), DMLaw (Ye et al., 2024), and RegMix (Liu et al., 2024). Each method contributes distinct proportions of data from specific domains, as reflected in the domain weights presented in Table 4. Notably, the weights indicate the percentage of contributions from each domain. Table 4: Exact domain weights (%) on SlimPajama obtained by data mixing methods. Abbreviations: C.C. = CommonCrawl, Wiki = Wikipedia, StackEx. = StackExchange"
        },
        {
            "title": "Mixing Method",
            "content": "C.C. C"
        },
        {
            "title": "GitHub Books ArXiv Wiki",
            "content": "StackEx."
        },
        {
            "title": "SlimPajama\nDoReMi\nDOGE\nDMLaw\nRegMix",
            "content": "52.20 38.11 21.35 12.50 17.37 26.70 11.41 26.93 25.00 51.03 5.20 6.54 7.03 14.06 0.23 18 4.20 8.19 4.50 9.38 0.23 4.60 4.24 8.80 25.00 0. 3.80 23.07 14.82 1.56 29.77 3.30 8.47 16.58 12.50 1.27 For the reproduction of MATES (Yu et al., 2024), we start by utilizing Random-Slimpajama at the 1500th training step as our primary pretraining model and fine-tune the BERT-base from the original thesis as our data influence model. During the training of the data influence model, we uniformly sample 1/13 of the data as hold-out data from each area of our dataset and employ LAMBADA (Paperno et al., 2016) as reference task, following the MATES methodology. Ultimately, we use the trained BERT-base data influence model to predict the entire training dataset, selecting the top 1/20 as our pretraining data. This selection process is executed using the Gumbel-Top-k algorithm (Kim et al., 2016), consistent with MATES. We leverage four-step updates similar to the original paper, and conduct the above implementation at 1500th, 3000th, 4500th and 6000th model training steps using the current models. A.2.3 DETAILS AGENT WEIGHT INITIALIZATION We employ an agent weight initialization technique within the RegMix (Liu et al., 2024) framework, which is crucial for the effective training of proxy models. Our dataset is organized into three distinct categories: domain, quality, and topic. For each category, we initialize the data weights based on the original proportions across 512 configurations and subsequently train TinyLlama-1M with 1 billion tokens as proxy model for each configuration. We evaluate this model on previously unseen data mixtures, specifically using validation set loss, following RegMix, for assessment. We then fit regression model based on the performance results of the 512 proxy models to predict the optimal data mixture for training large-scale LLMs. The results of the LightGBM regression analysis and Spearman correlation of the loss prediction performance are presented in 7. (a) Topic (b) Quality Interval (c) Domain Figure 7: We present the results of the LightGBM regression analysis and Spearman correlation regarding the loss prediction performance and the weights of each candidate data-(a) Topic, (b) Quality Interval and (c) Domain after mixture across all categories. 19 Upon training the regression model, we systematically investigate the entire spectrum of potential data mixtures by utilizing the trained model to predict the target values for each candidate mixture. This process allows us to identify the input that produces the optimal target value. Following the simulation and identification of the most effective data mixture, we then generalize this top-ranked configuration for large-scale model training, incorporating significantly larger volume of tokens. A.3 FULL EXPERIMENTAL RESULTS Table 5: Table Showing Various Selection Methods and Their Scores with Changes. We report accuracy for all tasks, and bold the best result in each column. Abbreviations: O.B.QA = OpenbookQA W.G. = WinoGrande, C.S.QA = CommonSenseQA, Compreh. = Comprehensions Selection Method Random sample Uniform-30B Uniform-60B Perplexity-based data selection PPL Classifier-based data selection QuRating-Facts QuRating-Req QuRating-Writing QuRating-Edu FineWeb-Edu DSIR-Book DSIR-Wiki Domain mixing methods DOGE DoReMi DMLaw RegMix Influence functions MATES Multi-Agent Collaboration (Ours) Problem Solving Commonsense Reasoning Reading Compreh. ARC-E ARC-C MathQA MMLU O.B.QA SIQA W.G. C.S.QA BoolQ RACE Average 54.3 55.2 23.4 24.6 22.3 22.5 23.9 23.4 18.6 21.0 39.8 39. 52.8 51.9 19.2 19.5 55.4 59.8 30.0 33.1 34.0 35.1 49. 20.1 22.4 23.6 16.2 36.0 48. 18.8 61.4 29.3 32.5 56.1 54.9 53.6 57.0 53.8 45.4 50.6 49.4 50.1 49.6 50. 50.0 61.1 23.3 24.4 23.2 24.4 23.4 20.8 21.1 21.8 20.2 21.9 22.3 21.4 28. 22.4 23.2 23.4 22.0 21.8 22.0 21.6 22.5 22.5 23.2 22.1 22.7 22.6 24.8 25.2 23.2 25.0 23.9 23.0 23.0 23.0 23.7 23.6 22. 25.3 26.0 21.6 21.4 21.0 20.4 19.8 18.8 19.2 18.0 17.8 17.8 18.8 19.0 24. 39.2 38.1 38.1 40.3 39.2 39.9 36.6 38.0 38.7 38.6 38.0 39.8 38.2 54.1 54.5 52.8 53.7 51.7 54.6 53.0 52.7 52.8 51.8 52. 53.6 54.2 19.9 20.6 19.7 20.2 20.8 19.7 19.8 19.9 19.7 20.1 19.9 21.3 19. 61.5 61.6 59.4 60.1 59.7 58.3 60.5 60.0 58.6 60.4 58.9 59.9 61.0 31.6 31.3 31.6 32.2 32.0 30.8 29.2 30.0 30.8 29.0 31. 32.1 29.8 35.5 35.5 34.6 35.5 34.6 33.3 33.5 33.5 33.5 33.6 33.7 34.5 36. Table 6: Table showing various selection methods and their three-shots performance. We report accuracy for all tasks, and bold the best result in each column. Abbreviations: O.B.QA = OpenbookQA W.G. = WinoGrande, C.S.QA = CommonSenseQA, Compreh. = Comprehensions Selection Method Random sample Uniform-30B Uniform-60B Perplexity-based data selection PPL Classifier-based data selection QuRating-Facts QuRating-Req QuRating-Writing QuRating-Edu FineWeb-Edu DSIR-Book DSIR-Wiki Domain mixing methods DOGE DoReMi DMLaw RegMix Influence functions MATES Multi-Agent Collaboration (Ours) Problem Solving Commonsense Reasoning Reading Compreh. ARC-E ARC-C MathQA MMLU O.B.QA SIQA W.G. C.S.QA BoolQ RACE Average 54.6 58.8 23.0 25.5 22.1 23.0 24.9 27.2 18.8 20.0 40.3 41. 52.9 53.6 21.5 19.6 53.0 56.9 29.8 32.7 34.1 35.9 50. 21.3 22.7 25.2 15.6 37.7 48. 20.1 61.5 22.3 32.6 59.5 59.3 56.9 60.8 56.2 48.7 53.2 52.4 53.2 51.5 53. 52.6 65.8 25.7 25.9 25.7 26.5 25.7 21.0 22.4 21.9 21.4 21.4 22.1 21.8 31. 22.6 22.7 23.1 22.5 22.3 22.6 22.6 22.4 22.2 22.4 22.2 22.6 23.0 25.9 26.1 26.0 26.7 26.2 25.6 25.3 27.0 24.7 25.2 25. 26.7 26.6 19.8 19.6 20.4 20.2 20.6 18.6 17.6 17.4 18.2 18.2 19.0 20.4 24. 40.2 39.7 41.1 41.4 40.1 42.5 37.1 39.9 38.4 39.0 39.1 40.9 39.9 54.6 53.7 53.6 54.6 50.5 53.7 52.7 52.0 50.9 50.7 53. 53.7 54.1 19.2 20.5 20.2 20.6 19.7 19.5 21.4 18.2 20.6 19.4 18.4 19.7 20. 60.8 58.5 51.4 55.5 56.6 57.9 61.6 57.8 59.7 52.6 60.7 57.6 60.4 24.8 22.7 22.6 22.7 31.4 22.9 24.2 29.8 31.1 29.8 30. 31.8 30.5 35.3 34.9 34.1 35.1 34.9 33.3 33.8 33.9 34.0 33.0 34.4 34.8 37. We show the full results of all tasks in Table 5, Table 6 and Table 7. In analyzing the full experiment results, it is evident that our model consistently outperforms other methods across various tasks. 20 Table 7: Table showing various selection methods and their five-shots performance. We report accuracy for all tasks, and bold the best result in each column. Abbreviations: O.B.QA = OpenbookQA W.G. = WinoGrande, C.S.QA = CommonSenseQA, Compreh. = Comprehensions Selection Method Random sample Uniform-30B Uniform-60B Perplexity-based data selection PPL Classifier-based data selection QuRating-Facts QuRating-Req QuRating-Writing QuRating-Edu FineWeb-Edu DSIR-Book DSIR-Wiki Domain mixing methods DOGE DoReMi DMLaw RegMix Influence functions MATES Multi-Agent Collaboration (Ours) Problem Solving Commonsense Reasoning Reading Compreh. ARC-E ARC-C MathQA MMLU O.B.QA SIQA W.G. C.S.QA BoolQ RACE Average 54.5 59.1 21.9 26.0 22.4 22.4 25.6 26. 19.2 21.6 39.7 42.1 54.2 54.3 19.7 21.0 53.2 55.7 30.8 32. 34.1 36.2 49.2 21.2 22.5 24.9 14. 36.7 49.8 20.6 60.6 23.3 32. 60.5 59.9 57.3 60.8 56.6 49.7 53.6 53.0 52.7 52.4 53.5 53.6 64.9 25.4 26.4 25.3 26.5 24.9 21.1 22.3 21.8 22.2 21.4 24. 21.6 31.1 23.4 22.8 22.6 22.5 22.6 22.1 23.0 22.0 22.4 23.0 21.2 22.6 22. 26.4 25.6 25.0 26.5 25.8 25.6 25.3 26.3 25.5 25.7 25.0 26.1 26.3 20.2 21.8 21.4 20.2 19.8 19.8 17.6 17.2 16.2 17.2 19. 20.4 23.6 40.3 40.1 41.6 41.4 39.4 41.7 36.7 40.1 39.3 39.2 41.0 41.7 39. 51.9 53.7 53.5 54.6 51.2 54.1 52.2 51.7 51.9 50.6 53.2 53.1 53.1 19.0 19.6 19.6 21.1 19.7 18.3 20.4 18.8 20.9 19.2 19. 20.4 20.4 58.0 56.9 49.5 55.5 55.9 55.6 60.2 58.5 60.0 51.4 61.3 60.1 60. 23.3 22.3 22.1 22.7 30.9 22.9 22.6 30.1 31.0 29.9 30.2 32.0 30.7 34.8 34.9 33.8 35.2 34.7 33.1 33.4 33.9 34.2 33.0 34. 35.2 37.2 Overall, for the zero-shot scenario, the classifier method outperforms the influence function in terms of average performance, while domain mixing yields the poorest results. Our method achieves an impressive average accuracy of 36.5, significantly surpassing the next best classifier, QuRatings series, which scores 35.5. This underscores the robustness of our approach, particularly in challenging problem-solving domains such as ARC-C, ARC-E, and MMLU, where we exceed competing models by considerable margins. Our model demonstrates superior performance in the three-shot scenario, achieving an impressive average accuracy of 37.7, thereby maintaining its lead. Notably, we excel in the ARC-E and ARC-C benchmarks, attaining scores of 65.8 and 31.5, respectively, which highlights our models effective utilization of few-shot learning. In contrast, the leading alternative methods underperform, particularly in more complex tasks such as MMLU and BoolQ. In the five-shots evaluation, our model continues to demonstrate competitive performance, with scores reflecting consistent trend of superiority across various domains, while other non-leading methods also maintain high levels. These results underscore our models robust capacity to generalize across diverse question-answering tasks, affirming its advantages over conventional classifiers and highlighting its potential for practical applications in real-world scenarios. A."
        },
        {
            "title": "IMPLEMENTATION DETAILS OF OUR METHODS",
            "content": "To further refine the models performance, we calculate rewards for each sampled data point by approximating the rewards using influence functions, as shown in Equation 6. Following Engstrom et al. (2024), we choose LAMBADA Paperno et al. (2016), SQuAD Rajpurkar (2016) and Jeopardy Tunguz (2019) as reference tasks. We followed methods provided in Engstrom et al. (2024), Xia et al. and Park et al. (2023) to calculate the Hessian and the gradients in the influence functions. In our implementation, we project gradients into an 8,192-dimensional space for both the validation and training datasets. To optimize the gradient computation process, we divide each data category into eight slices, thereby enabling parallel computation across eight GPUs. Each slice contains 1,250 data points. After calculating gradients for each slice, the results are concatenated in their original sequence to ensure data integrity. This slicing strategy not only accelerates the processing by utilizing GPU parallelism but also maintains consistency in gradient calculation. Additionally, for the validation datasets, we uniformly sample 500 data points to ensure balanced evaluation procedure. All prompts across the datasets are carefully aligned to maintain task coherence, crucial factor in multi-task learning scenarios. Furthermore, we implement sliding window of 1,024 tokens with 21 256-token overlap to ensure consistent tokenization across the entire dataset. This sliding window technique efficiently extracts maximum of 1,024 tokens from each data point, ensuring uniform encoding across different datasets and tasks, thus improving the overall consistency and reliability of the data processing pipeline. A.5 DETAILS OF INFLUENCE CHANGES DURING DIFFERENT PRETRAINING STAGES We present the details of influence change during the pretraining process for domain (Figure 8), quality intervals (Figure 9) and topic (Figure 10). Figure 8: We present the normalized influence for each domain across various training steps. Figure 9: We present the normalized influence for each quality interval across various training steps. 22 Figure 10: We present the normalized influence for each topic across various training steps."
        },
        {
            "title": "B DATA DISTRIBUTION ANALYSIS OF THE SLIMPAJAMA DATASET",
            "content": "We finally present the data distribution analysis of the SlimPajama dataset from three dimensions: topic, domain and quality intervals, as Figure 11 to Figure 13 shows. Figure 11: The illustration of the joint distribution of topics and domains. 23 Figure 12: The illustration of the joint distribution of quality intervals and domains. Figure 13: The illustration of the joint distribution of quality intervals and topics."
        }
    ],
    "affiliations": [
        "Hong Kong University of Science and Technology",
        "Peking University",
        "Shanghai AI Laboratory"
    ]
}