{
    "paper_title": "Sparse-vDiT: Unleashing the Power of Sparse Attention to Accelerate Video Diffusion Transformers",
    "authors": [
        "Pengtao Chen",
        "Xianfang Zeng",
        "Maosen Zhao",
        "Peng Ye",
        "Mingzhu Shen",
        "Wei Cheng",
        "Gang Yu",
        "Tao Chen"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "While Diffusion Transformers (DiTs) have achieved breakthroughs in video generation, this long sequence generation task remains constrained by the quadratic complexity of attention mechanisms, resulting in significant inference latency. Through detailed analysis of attention maps in Video Diffusion Transformer (vDiT), we identify three recurring sparsity patterns: diagonal, multi-diagonal, and vertical-stripe structures. And even 3-6\\% attention heads can be skipped. Crucially, these patterns exhibit strong layer-depth and head-position correlations but show limited dependence on the input content. Leveraging these findings, we propose Sparse-vDiT, a sparsity acceleration framework for vDiT comprising: 1) Pattern-optimized sparse kernels that replace dense attention with computationally efficient implementations for each identified sparsity pattern. 2) An offline sparse diffusion search algorithm that selects the optimal sparse computation strategy per layer and head via hardware-aware cost modeling. After determining the optimal configuration, we fuse heads within the same layer that share the same attention strategy, enhancing inference efficiency. Integrated into state-of-the-art vDiT models (CogVideoX1.5, HunyuanVideo, and Wan2.1), Sparse-vDiT achieves 2.09$\\times$, 2.38$\\times$, and 1.67$\\times$ theoretical FLOP reduction, and actual inference speedups of 1.76$\\times$, 1.85$\\times$, and 1.58$\\times$, respectively, while maintaining high visual fidelity, with PSNR values reaching 24.13, 27.09, and 22.59. Our work demonstrates that latent structural sparsity in vDiTs can be systematically exploited for long video synthesis."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 3 ] . [ 1 5 6 0 3 0 . 6 0 5 2 : r Sparse-vDiT: Unleashing the Power of Sparse Attention to Accelerate Video Diffusion Transformers Pengtao Chen1 Xianfang Zeng2 Maosen Zhao1 Peng Ye3 Mingzhu Shen4 Wei Cheng2 Gang Yu2 Tao Chen1 1 Fudan University 2 StepFun 3 The Chinese University of Hong Kong 4 Imperial College London Code: https://github.com/Peyton-Chen/Sparse-vDiT"
        },
        {
            "title": "Abstract",
            "content": "While Diffusion Transformers (DiTs) have achieved breakthroughs in video generation, this long sequence generation task remains constrained by the quadratic complexity of attention mechanisms, resulting in significant inference latency. Through detailed analysis of attention maps in Video Diffusion Transformer (vDiT), we identify three recurring sparsity patterns: diagonal, multi-diagonal, and vertical-stripe structures. And even 3-6% attention heads can be skipped. Crucially, these patterns exhibit strong layer-depth and head-position correlations but show limited dependence on the input content. Leveraging these findings, we propose Sparse-vDiT, sparsity acceleration framework for vDiT comprising: 1) Pattern-optimized sparse kernels that replace dense attention with computationally efficient implementations for each identified sparsity pattern. 2) An offline sparse diffusion search algorithm that selects the optimal sparse computation strategy per layer and head via hardware-aware cost modeling. After determining the optimal configuration, we fuse heads within the same layer that share the same attention strategy, enhancing inference efficiency. Integrated into state-of-the-art vDiT models (CogVideoX1.5, HunyuanVideo, and Wan2.1), Sparse-vDiT achieves 2.09, 2.38, and 1.67 theoretical FLOP reduction, and actual inference speedups of 1.76, 1.85, and 1.58, respectively, while maintaining high visual fidelity, with PSNR values reaching 24.13, 27.09, and 22.59. Our work demonstrates that latent structural sparsity in vDiTs can be systematically exploited for long video synthesis."
        },
        {
            "title": "Introduction",
            "content": "In recent years, diffusion models have achieved significant advances in image generation [27], prompting growing interest in extending them to video synthesis. Early approaches, such as SVD [2] and Dynamicrafter [42], employed 2D+1D framework that provided computational efficiency but lacked real-time interaction between spatial and temporal features, resulting in limited spatiotemporal consistency. Recent progress in 3D full-attention Video Diffusion Transformers (vDiT) [26] has effectively addressed these limitations. Built on this foundation, models such as OpenSora [18], CogVideoX [43], HunyuanVideo [16], and Wan2.1 [34] demonstrate strong spatiotemporal coherence and high video quality. These methods have been widely applied in fields including animation generation [11, 12], video editing [46, 35], and world modeling [25, 10]. Although 3D full-attention vDiT models demonstrate strong video generation performance and are widely adopted, they suffer from high computational costs and large inference latency. For instance, generating 5-second 720p video at 24 fps using the HunyuanVideo model on single NVIDIA A800 GPU takes approximately fifty minutes. This inefficiency primarily results from the joint spatiotemporal tokenization process, which generates up to 120k tokens in this setting. Given Corresponding author. Project leader. Work was done when interned at StepFun. Preprint. Under review. Figure 1: The architecture of vDiT and inference latency analysis of its two variants, CogVideoX1.5 and HunyuanVideo, across different components. The latency of the attention module dominates under long sequence settings, and its proportion increases as the sequence length grows. that attention complexity scales quadratically with sequence length [33], this leads to substantial computational burden. As shown in Figure 1, in the classical model CogVideoX1.5, which is based on the vDiT architecture, attention accounts for 77% of the total latency at 86k tokens. Specifically, for HunyuanVideo with 120k tokens, attention accounts for 81% of the total inference latency, and this proportion increases with longer sequence length. Thus, 3D full attention is the primary bottleneck in inference efficiency for vDiT-based video generation. Fortunately, the 3D full attention mechanism exhibits significant redundancy despite its considerable computational cost. First, we observe that some attention heads in vDiT are redundant, as skipping their computations has minimal effect on the final output. Second, redundancy is also present in the computation of the attention map, namely in the QKT product. We find that vDiT attention maps commonly follow four distinct patterns: full attention, diagonal sparsity, multi-diagonal sparsity, and vertical-strip sparsity. The latter three patterns suggest that computing the full attention map is often unnecessary. Further experiments reveal that these sparse patterns remain stable across different input texts and are primarily determined by the position of attention within the vDiT architecture. This fixed redundancy provides strong basis for optimization. Building on these findings, we propose Sparse-vDiT, sparse method designed to accelerate vDiT for video generation. To reduce redundancy among attention heads, we introduce head skipping strategy. We observe that vDiTs attention maps commonly follow three sparse patterns: diagonal, multi-diagonal, and vertical-stripes. To enable actual speedup, we design predefined kernels tailored to each pattern. Since these sparsity patterns are relatively fixed and input-invariant, we develop an offline sparse diffusion search algorithm that identifies the optimal attention pattern for each head using only small number of samples. After the search, the computation pattern of each head is fixed. We then group heads with the same sparsity pattern within each layer and fuse them to further accelerate inference by leveraging their fixed structure. We conducted experiments on three widely used vDiT-based models: CogVideoX1.5, HunyuanVideo, and Wan2.1. On CogVideoX1.5, Sparse-vDiT achieved 2.09 reduction in theoretical FLOPs and 1.76 end-to-end speedup in real, while keeping the LPIPS score low at 0.14, and even outperforming the baseline in the ImageQual metric. On HunyuanVideo, Sparse-vDiT achieved 2.38 reduction in theoretical FLOPs and 1.85 speedup, with generation quality reaching SSIM = 0.87 and PSNR = 27.03. On Wan2.1, Sparse-vDiT achieved 1.67 reduction in theoretical FLOPs and 1.58 speedup, with generation quality reaching SSIM = 0.80 and PSNR = 22.59. These results indicate that Sparse-vDiT effectively balances computational efficiency and generation quality. The contributions of our paper are as follows: We find that attention heads in vDiT are partly redundant. Meanwhile, many heads often exhibit recurring sparse attention patterns, including diagonal sparsity, multi-diagonal sparsity, and verticalstripe sparsity. These patterns are consistent across different inputs but are closely related to the attention position within the vDiT architecture. Building on these insights, we propose Sparse-vDiT, which accelerates vDiT by skipping redundant heads and applying pattern-aligned sparse attention kernels. It introduces an offline sparse diffusion search that selects the optimal sparse mode for each head using small number of samples, followed by intra-layer fusion of heads with identical attention patterns to enhance inference efficiency. Sparse-vDiT achieves 2.09, 2.38 and 1.67 theoretical FLOPs reduction on CogVideoX1.5, HunyuanVideo and Wan2.1, respectively. It also delivers 1.76, 1.85, and 1.58 end-to-end video generation speedups while maintaining comparable generation quality, with PSNR scores of 24.13, 27.09, and 22.59. Sparse-vDiT consistently outperforms existing state-of-the-art (SOTA) methods, such as SVG and MInference."
        },
        {
            "title": "2 Related Work",
            "content": "Efficient Diffusion Model. Diffusion models are inherently slow because of their iterative denoising process, leading to growing interest in accelerating inference. Existing approaches include pruning methods [8, 3] that reduce model parameters, quantization techniques [37, 28, 49] that decrease parameter bit-width and computational overhead, and caching strategies [24, 4, 29] that trade memory for computation speed. However, most of these methods are primarily designed for image generation, with relatively few acceleration methods specifically tailored for video diffusion models. For video diffusion, techniques like PAB [50], TeaCache [20], FasterCache [23], and AdaCache [15] reuse features by exploiting the similarity between adjacent denoising steps. Other methods reduce the number of timesteps using distillation [19, 45] or compress latent spaces using high-ratio VAEs [30]. In contrast, our approach accelerates inference by exploiting the sparsity in vDiTs attention. Efficient Attention Mechanism. The attention [33] is central to transformers but suffers from quadratic complexity in sequence length, limiting efficiency in long sequences. To address this, various sparse attention methods have been proposed. In traditional vision, Swin Transformer [22], NAT [9], and Sparse Transformers [5] restrict attention to local windows. Similarly, Longformer [1] applies windowed attention in NLP. Large language models [32] have identified attention sink phenomena [40, 39], introducing streaming attention that combines sink masking with windowing. Later works, such as MInference [14] and FlexPrefill [17], explore diverse static and dynamic sparse patterns. In diffusion models, DiTFastAttn [44, 47] noted strong local neighbor attention in DiTs, enabling acceleration via windowed attention and cached contexts. CLEAR [21], DiG [51], and SANA [41] further exploit the sparsity of the attention mechanism to achieve linearized computation. For video diffusion, Efficient-vDiT [6] observed that each frame in the attention primarily attends to fixed set of other frames. This observation introduces tile-based attention to linearized computation. SVG [38] identified spatiotemporal sparsity in video attention and optimized attention computation through data reordering and an online scheme. However, this paper thoroughly reveals multiple patterns and invariances of redundancy in vDiT attention. Based on these findings, we propose an offline sparse acceleration framework that integrates head skipping with three attention sparsity patterns. Considering the fixed nature of offline optimization, fusion optimization is performed on fixed attention pattern at each attention layer."
        },
        {
            "title": "3 Preliminary",
            "content": "Full Attention. The multi-head attention mechanism [33] constitutes fundamental building block in vDiT. Let the input hidden features be denoted as RBN D, where is the batch size, the number of tokens, and the original feature dimension. Through learnable linear projections, is transformed into three tensors: query (Q), key (K) and value (V ). Each of these tensors has dimensions RBHN d, where denotes the number of attention heads, and = D/H represents the reduced feature dimension per head. The attention outputs refined features RBN preserving the original dimension of I. The attention transformation process is defined as follows: for each head {1, ..., H}, Attention(Qh, Kh, Vh) = sof tmax(QhKT / d)Vh RBN d, (1) where Qh, Kh, Vh are slice operations on the head dimension. Then, merging along the head dimension yields the final output of the attention. For the full attention mechanism, the entire process described above is executed. Sparse Attention. In Eq 1, sof tmax(QhKT d) is known as the attention map, where each value represents how much one token attends to another at the corresponding position. Since its computational complexity is O(N 2), generating the attention map takes up most of the computation in the attention mechanism. However, in practice, token usually attends to only small number of / 3 Figure 2: Visualization of the vDiT attention map showing four interaction regions. The dominant V-V region has diagonal blocks for self-frame and off-diagonal blocks for cross-frame interactions. other tokens, rather than maintaining global attention. This results in most values in the attention map being close to zero, showing strong sparsity. In most cases, it is sufficient to compute only the dense regions of the attention map to obtain sufficiently accurate result. If the sparsity pattern of the attention map is structured, computations involving sparse regions can be omitted at the hardware level using Triton [31] or CUDA, enabling practical acceleration."
        },
        {
            "title": "4 Method",
            "content": "4.1 Attention Mechanism in vDiT In the following, we present the attention mechanism employed in vDiT. We first describe the distinctive layout of attention maps tailored for video generation. Next, we demonstrate that the attention mechanism exhibits substantial redundancy. Finally, we show that this redundancy is largely intrinsic to the model architecture and remains relatively insensitive to variations in the input. 4.1.1 Attention Map in vDiT. Table 1: Quantitative impact of skipping different ratios of attention heads on the final generation. CogVideoX1.5 PSNR SSIM LPIPS skipping 1% skipping 3% skipping 6% skipping 10% 36.62 33.31 30.02 26.87 0.96 0.95 0.92 0.85 0.01 0.02 0.04 0.09 HunyuanVideo PSNR SSIM LPIPS skipping 1% skipping 3% skipping 6% skipping 10% 31.84 28.94 24.21 17.98 0.95 0.91 0.81 0.72 0.02 0.06 0.12 0. Current mainstream vDiT models, such as CogVideoX and HunyuanVideo, mainly adopt the MMDiT paradigm [7]. In this design, the token sequence is formed by concatenating text tokens and video tokens, and the corresponding attention map is shown on the left side of Figure 2. The attention map is divided into four parts based on token type and position: TT, TV, VT, and VV, where denotes text tokens and denotes video tokens. Text tokens make up only small portion of the sequence, while video tokens account for over 99%. In the VV region (the middle part of Figure 2), video tokens are arranged in the temporal order of frames. As result, the diagonal blocks correspond to self-frame interactions among image (frame) tokens. In contrast, the off-diagonal blocks correspond to cross-frame interactions, as illustrated on the right part of Figure 2. 4.1.2 Analyzing Attention Redundancy in vDiT We find that attention in vDiT contains considerable redundancy. Some attention heads are nonessential, and skipping them results in minimal performance loss. Moreover, the attention maps exhibit patterns of structured sparsity, which can be exploited to enable efficient sparse computation. Head Skipping. Not all attention heads in vDiT contribute equally to performance. Based on minimum mean squared error (MSE) criterion, we evaluate head skipping on CogVideoX1.5 and HunyuanVideo. As shown in Table 1, in CogVideoX1.5, skipping 6% of the attention heads preserves generation quality comparable to the original model. In HunyuanVideo, skipping 3% of the heads similarly causes little degradation in video quality. These results indicate that certain attention heads in vDiT are redundant, suggesting that head skipping may be practical means to improve efficiency. 4 Figure 3: Visualization of the four recurring attention patterns in vDiT. However, relying solely on skipping is insufficient to achieve high efficiency. As coarse-grained method, it results in performance degradation beyond certain threshold. As shown in Table 1, both models exhibit noticeable degradation when the skip ratio reaches 10%. Therefore, more fine-grained strategy is required to achieve greater speedup. Given that the sparsity of attention maps can improve the efficiency of transformer models, we conduct an in-depth analysis of the attention map in vDiT. Taking CogVideoX as an example, we visualize its attention maps in Figure 3 and identify four recurring patterns: Full Attention Pattern. The attention values are evenly distributed, indicating global interactions among tokens. Applying sparse computation to such dense patterns often degrades performance, making efficiency optimization difficult. Diagonal Pattern. Large values appear along the main diagonal, representing interactions among neighboring tokens within the same frame (as shown in Figure 2). This pattern reflects the models ability to capture self-frame structure. Since most off-diagonal values are close to zero, the full attention can be well approximated by computing only the diagonal elements of the attention map. This structured sparsity allows for efficient acceleration using window attention [1]. Multi-Diagonal Pattern. Large values are distributed along multiple evenly spaced diagonals. These diagonals align with the diagonal blocks in the I-I region of Figure 2, indicating strong attention between tokens at nearby spatial positions across different frames. Therefore, this pattern is associated with vDiTs ability to model cross-frame consistency. By rearranging tokens [38], this pattern can be transformed into diagonal structure suitable for optimization with window attention. Vertical-Stripe Pattern. In the attention map, large values form vertical stripe pattern, suggesting the presence of global tokens that strongly attend to all others in vDiT. This structured sparsity also enables efficient computation by sparse kernel. Figure 4: t-SNE visualization of attention patterns along the head dimension on VBench subset, with different layers indicated by distinct colors. Patterns from different prompts exhibit clustering. 4.1.3 Invariant Property of Attention Patterns We revealed the presence of diverse attention patterns in vDiT above. We further observe that these patterns are strongly correlated with the depth of the attention layers, while being largely independent of the input text. To verify this, we randomly sampled 50 diverse prompts from VBench as subset 5 Figure 5: Overview of the Sparse-vDiT. We first predefine five types of attention mode M0:4. Then, using an offline sparse diffusion search algorithm, we select the best attention mode for each layer and head in vDiT. After the search, for heads set to skip attention, we set their outputs to zero. For the three sparse attention patterns, we create specialized sparse attention kernels to speed up computation. Finally, heads within the same layer that use the same attention mode are fused to improve efficiency. and used them to generate videos. For each layer and each attention head in vDiT, we saved the corresponding attention maps. Since we only needed to determine the pattern types, we stored the maps as memory-efficient image files. We then used ResNet50 to extract high-dimensional features from the images and applied t-SNE to project them into 2D space along the head dimension. The results are shown in Figure 4, where different colors represent different layers. We observed that, regardless of the head, the attention patterns from different layers form distinct clusters, while those from different prompts tend to cluster together. This confirms that the attention patterns exhibit strong correlations with attention position in vDiT but are minimally affected by the input content. 4.2 Sparse-vDiT: Sparse Acceleration Framework for vDiT In the previous part, we identified two types of redundancy in the attention mechanism of vDiT: redundancy within the attention heads and redundancy in the attention map computation. We also found that this redundancy is intrinsic to vDiT and only weakly dependent on the input text. Based on these findings, we introduce Sparse-vDiT, sparse acceleration method designed for vDiT. This method determines the most effective sparse strategy for each head in each layer through offline search, resulting in acceleration. The overall structure of Sparse-vDiT is illustrated in Figure 5. Sparse Computation Pre-definition. To reduce redundancy in the attention head, we apply the skip strategy M1, which bypasses the entire process in Eq 1. To maintain consistent output dimensions, the attention output is set to zero. The sparsity of the skip strategy is defined as S1 = 1, while the sparsity of full attention M0 is S0 = 0. Regarding the three sparse forms Mi(i = 2, 3, 4) shown in Figure 3, we have designed specific sparse kernels to reduce the computation of sof tmax(QKT / d). The sparsity of these kernels is determined by the ratio of actual computation blocks to the total number of blocks, denoted as Si(i = 2, 3, 4), as shown in Figure 5. In the Sparse-vDiT framework, the sparsity of these kernels is predefined and treated as fixed constant. Offline Sparse Diffusion Search. In vDiT, different heads at various layers exhibit distinct attention patterns. Given the set = {Mi(i = 0, ..., 4)} of attention computation modes, the challenge lies in selecting the most appropriate mode for each head. In Sparse-vDiT, we propose an offline sparse diffusion search method to address this. As shown in Figure 5, for each layer in every step of vDiT, we pass the inputs through M0 to M4, obtaining the corresponding hidden state results O0 to O4. We then compute the MSE distances between O1 to O4 and O0, that is, SE(Oi O0), = 1, ..., 4, which represent the loss introduced by the sparse attention computation. Our final loss is Li = SE(Oi O0) + λ (1 Si), (2) 6 where the sparsity penalty is added and λ balances quality and computational cost. If all losses in = {Li(i = 1, ..., 4)} exceed the desired threshold ϵ, the head retains full attention. Otherwise, the sparse mode with the smallest loss replaces full attention. The specific formulation is as follows: Attention(Q, K, , ) = M0(Q, K, ) Margmini{Li}(Q, K, ) , if (cid:84) (Li > ϵ) i=1,...,4 , otherwise (3) where ϵ controls the overall sparsity ratio during inference. As discussed in the previous part, vDiTs sparse attention pattern is inherent after pretraining and largely independent of input types. Thus, the search in Sparse-vDiT is offline and requires only small number of input samples. Once the search is completed, the sparse modes for the entire inference process are fixed. This fixity allows heads with the same sparse mode within layer to be fused, further accelerating the inference."
        },
        {
            "title": "5 Experiment",
            "content": "5.1 Experimental Settings Pretrained Model. To evaluate the effectiveness of Sparse-vDiT, we conducted text-to-video generation experiments using three leading open-source pretrained vDiT models: CogVideoX1.5 [43], HunyuanVideo [16], and Wan2.1 [34]. CogVideoX1.5 generates 81 frames at resolution of 1360768, while HunyuanVideo generates 129 frames at 1280720. In the latent space encoded by the 3D-VAE, the vDiT in CogVideoX1.5 processes 45,106 tokens, including 226 text tokens and 11 video frames with 4,080 tokens each. The vDiT in HunyuanVideo processes 119,056 tokens, including 256 text tokens and 33 frames with 3,600 tokens each. And the vDiT in Wan2.1 processes 75,600 tokens, including 21 frames with 3,600 tokens each. Dataset & Evaluation Metrics. We adopted comprehensive evaluation framework covering both video generation quality and efficiency. For quality evaluation, we used three types of metrics. The first category measures reconstruction fidelity after inference acceleration, including Peak Signal-toNoise Ratio (PSNR) [50], Structural Similarity Index Measure (SSIM) [36], and Learned Perceptual Image Patch Similarity (LPIPS) [48]. The remaining two categories assess frame-level visual quality and temporal consistency, using the Imaging Quality (ImageQual) and Subject Consistency (SubConsist) metrics from the VBench [13]. For efficiency evaluation, we considered theoretical FLOPS, actual inference latency, and the speedup relative to the pretrain model. Regarding evaluation datasets, we followed the original protocol of CogVideoX [43], using prompts from the GPT-enhanced version of VBench. For HunyuanVideo, we used prompts from the Penguin Video Benchmark [16]. Baseline. We compared several existing acceleration methods for vDiT, including both classical approaches and state-of-the-art techniques. These methods include MInference [14], classical sparse acceleration technique migrated from large language models. WinAttn [1], which applies sparse acceleration along both temporal and spatial dimensions of video. SVG [38], the current stateof-the-art method for sparse accelerating vDiTs, and PAB [50], caching-based method designed specifically for video diffusion models. Implementation Details. The baselines MInference, PAB, and SVG are implemented using their official code and configurations. Since PAB only provides code for CogVideo, we do not include it in the evaluation on HunyuanVideo. The window sizes for WinAttn-Spatial and WinAttn-Temporal follow the settings used in SVG. In SVG, full attention is applied during the first 10 steps, and we follow the same setup for all baselines. However, this constraint is not required for Sparse-vDiT on CogVideoX1.5. SVG also applies full attention to the first two layers of vDiT, and we adopt the same configuration for our baselines, although it is unnecessary for Sparse-vDiT. Both CogVideoX1.5 and HunyuanVideo inference results were obtained on single NVIDIA A800 GPU, while Wan2.1 was obtained on single NVIDIA H800 with batch size of 1. 5.2 Experimental Results Analysis The qualitative and quantitative results are shown in Figure 6 and Table 2, respectively. Both consistently demonstrate that Sparse-vDiT effectively accelerates video diffusion models without compromising the quality of generation. This can be explained as follows. 7 Table 2: Comparison of video generation quality and efficiency between Sparse-vDiT and the baseline. XBench refers to VBench for CogVideoX1.5 and Wan2.1 evaluation and Penguin Video Bench for HunyuanVideo. CogVideoX1.5 & HunyuanVideo on single A800, Wan2.1 on H800, batch size 1. Method CogVideoX1.5 [43] MInference [14] WinAttn (Spatial) [1] WinAttn (Temporal) [1] PAB [50] SVG [38] Sparse-vDiT (Ours) HunyuanVideo [16] MInference [14] WinAttn (Spatial) [1] WinAttn (Temporal) [1] SVG [38] Sparse-vDiT (Ours) Wan2.1 [34] MInference [14] WinAttn (Spatial) [1] WinAttn (Temporal) [1] SVG [38] Sparse-vDiT (Ours) Against Original XBench Score SSIM () PSNR () LPIPS () ImageQual () SubConsist () PFLOPS () Latency () Speedup () - 0.61 0.64 0.69 0.72 0.75 0.82 - 0.64 0.56 0.80 0.86 0.87 - 0.62 0.68 0.73 0.78 0.80 - 14.63 19.07 19.64 20.93 21.92 24. - 19.23 17.81 23.76 26.83 27.09 - 15.49 19.14 20.29 21.96 22.59 - 0.37 0.32 0.28 0.23 0.22 0.14 - 0.43 0.56 0.22 0.14 0.12 - 0.36 0.25 0.21 0.18 0.16 63.28% 56.04% 64.84% 63.69% 59.03% 63.11% 63.45% 67.28% 60.53% 63.55% 67.32% 67.06% 67.13% 67.61% 63.29% 67.27% 67.40% 67.18% 67.35% 92.96% 87.12% 90.92% 92.66% 92.38% 92.49% 92.66% 96.79% 88.96% 90.26% 96.38% 96.54% 96.69% 91.95% 89.32% 91.34% 91.47% 91.27% 91.39% 147.87 84.89 72.34 72.34 105.88 74.57 70. 612.37 293.87 258.84 258.84 259.79 257.09 660.49 469.79 401.21 401.21 403.50 397.39 901s 634s 531s 537s 630s 550s 511s 3166s 2042s 1755s 1764s 1802s 1715s 1935s 1453s 1265s 1280s 1298s 1228s 1.00 1.42 1.69 1.67 1.43 1.64 1.76 1.00 1.55 1.80 1.79 1.75 1.85 1.00 1.33 1.53 1.51 1.49 1.58 Reconstruction Fidelity. On both CogVideoX1.5 and HunyuanVideo, Sparse-vDiT achieves the best performance across all fidelity metrics. For CogVideoX1.5, Sparse-vDiT yields an SSIM of 0.82, significantly higher than the closest baseline, SVG (0.75), and substantially higher than earlier sparse methods, such as MInference (0.61) and PAB (0.72). Similarly, the PSNR for Sparse-vDiT is 24.13 dB, surpassing all baselines, with the suboptimal result from SVG at 21.92 dB. Most notably, Sparse-vDiT achieves substantially lower LPIPS score (0.14), indicating greater perceptual similarity to the original outputs. The trends hold consistently on HunyuanVideo, where Sparse-vDiT again records the highest SSIM (0.87) and PSNR (27.09), along with the lowest LPIPS (0.12). The margins are particularly significant compared to early techniques such as WinAttn (Temporal), which, while effective (SSIM: 0.76, LPIPS: 0.22), still underperforms relative to Sparse-vDiT. These results confirm the strong preservation of spatial and perceptual detail after applying our acceleration scheme. Visual Quality. The ImageQual score from the VBench benchmark quantifies the frame-level visual quality as judged by pretrained evaluation models. Sparse-vDiT performs on par with or better than most baselines, achieving 63.45% on CogVideoX1.5 and 67.13% on HunyuanVideo. Although WinAttn (Spatial) slightly surpasses Sparse-vDiT in ImageQual on CogVideoX1.5 (64.84%), it comes with lower fidelity scores and higher LPIPS, suggesting potential overfitting to local texture patterns at the cost of content preservation. On HunyuanVideo, Sparse-vDiT delivers ImageQual scores highly comparable to the best-performing methods, including SVG (67.06%) and WinAttn (Temporal) (67.32%). These results indicate that Sparse-vDiT maintains competitive frame-level realism while significantly outperforming others in reconstructive metrics, highlighting its balanced and robust generation performance. Temporal Consistency. Temporal coherence is critical in video generation, and the SubConsist metric evaluates the consistency of subjects and motion across frames. Sparse-vDiT delivers stateof-the-art temporal stability in both benchmarks. On CogVideoX1.5, its SubConsist score reaches 92.66%, on par with the strongest existing methods, including WinAttn (Temporal) and PAB. On HunyuanVideo, Sparse-vDiT attains 96.69%, closely matching the best score of 96.79% from the original unaccelerated model. This observation is particularly important because many acceleration methods compromise temporal stability in favor of spatial quality. The ability of Sparse-vDiT to achieve high consistency while also delivering best-in-class fidelity underscores the effectiveness of its sparse acceleration strategy. By preserving computation in more temporally sensitive heads, Sparse-vDiT minimizes temporal artifacts common in other sparsity approaches. Visualization. Figure 6 shows visual comparison between the video results generated by SparsevDiT and those from the top three baseline methods. We observe that MInference produces blurry results, while PAB shows over-smoothing, as indicated by the yellow box in the first row. Both SVG and PAB lose some fine details, as shown in the white box in the second row. For object contours, SVG exhibits slight misalignment, as indicated by the red box in the third row. In contrast, our method remains closely aligned with the pretrained model in all these aspects. 8 Figure 6: Visual comparison between the proposed Sparse-vDiT and the baseline method. The green box indicates the ground truth. Yellow boxes highlight differences in blurriness and smoothness. White boxes highlight differences in fine details, while red boxes emphasize contour comparisons. λ Table 3: Ablation study on the effects of hyperparameters λ and ϵ in Sparse-vDiT. Speedup Hyperparameter 1.74 1.73 1.76 1.73 1.68 1.76 1.81 1.87 1.91 PSNR LPIPS 0.1501 24.0864 0.1503 24.0558 0.1477 24.1311 0.1479 24.0946 0.1219 25.4929 0.1477 24.1311 0.1785 22.7048 0.1947 22.0171 0.2231 20.8411 SubConsist 92.61% 92.62% 92.66% 92.58% 92.60% 92.66% 92.66% 92.45% 92.49% ImageQual 63.37% 63.35% 63.45% 63.37% 63.26% 63.45% 63.34% 63.27% 63.30% SSIM 0.8182 0.8180 0.8212 0.8203 0.8512 0.8212 0.7883 0.7716 0.7399 0 0.1 0.5 1 0.5 1 3 5 10 ϵ Computational Efficiency. One of the primary objectives of Sparse-vDiT is to achieve significant inference acceleration without compromising output quality. On CogVideoX1.5, it reduces computational cost from 147.87 to 70.69 PFLOPS (52.2% reduction), and on HunyuanVideo, from 612.37 to 257.09 PFLOPS (57.9%). These are the lowest among all compared methods, demonstrating the effectiveness of our sparsity strategy. In real-world latency, Sparse-vDiT consistently outperforms all baselines, reducing inference time from 901 seconds to 511 seconds on CogVideoX1.5 and from 3166 seconds to 1715 seconds on HunyuanVideo. These improvements are critical for time-sensitive applications. In terms of speedup, Sparse-vDiT achieves the highest ratios: 1.76 on CogVideoX1.5 and 1.85 on HunyuanVideo, surpassing all baseline methods. These results highlight the practical advantages of our sparsity. Overall, Sparse-vDiT achieves an optimal trade-off between generation quality and efficiency, setting new state-of-the-art for accelerated vDiT. These results confirm that Sparse-vDiT is not only theoretically elegant solution but also highly practical one, enabling scalable deployment of vDiT in latency-sensitive applications. 5.3 Ablation There are two hyperparameters in Sparse-vDiT, λ and ϵ. The parameter λ controls the trade-off between efficiency loss and quality loss, while ϵ regulates the overall sparsity of the vDiT. This section analyzes their impact through experiments on CogVideoX1.5. Quality-Efficiency trade-off. With ϵ fixed at its optimal value of 1, we vary λ across 0, 0.1, 0.5, and 1. Results are reported in Table 3. Comparisons across metrics show that both λ = 0.5 and λ = 1 yield strong generation quality. However, λ=1 is less efficient. Thus, λ = 0.5 offers better trade-off between generation quality and efficiency, and is used as the default configuration in Table 2. Performance under different levels of sparsity. Fixing λ at 0.5, we evaluate ϵ values of 0.5, 1, 3, 5, and 10. Table 3 illustrates that increasing ϵ leads to greater sparsity, resulting in higher acceleration. For instance, ϵ = 10 achieves speedup of 1.91. However, higher sparsity can impair the quality of generation, as reflected in performance metrics. Notably, at ϵ = 5, Sparse-vDiT achieves 1.87 speedup while still outperforming the SVG baseline (1.64 speedup). In practice, ϵ can be adjusted to achieve the desired balance between quality and efficiency."
        },
        {
            "title": "6 Conclusion and Limitation",
            "content": "We propose Sparse-vDiT, an efficient inference method for vDiT based on structured sparsity. It combines predefined sparsity patterns with an offline diffusion-guided search to assign the most suitable configuration to each attention head. Experiments on CogVideo and HunyuanVideo demonstrate theoretical speedups of 2.09 and 2.38, and actual speedups of 1.76 and 1.85, respectively. Despite the acceleration, video quality remains comparable to that of the original models, with PSNR values of 24.13 and 27.09. These results highlight Sparse-vDiTs ability to balance efficiency and generation quality, establishing new state-of-the-art for sparsity-based vDiT acceleration. Limitation: In our framework, the sparse kernel for attention is predefined. However, in practice, the predefined sparsity level may not fully align with the actual sparsity of the attention maps, potentially leading to underor over-sparsification. We believe that enabling adaptive sparsity adjustment based on the characteristics of the attention maps, or establishing more principled approach to sparsity design, could further enhance both sparsification effectiveness and generative performance."
        },
        {
            "title": "References",
            "content": "[1] Iz Beltagy, Matthew Peters, and Arman Cohan. Longformer: The long-document transformer. arXiv preprint arXiv:2004.05150, 2020. [2] Andreas Blattmann, Tim Dockhorn, Sumith Kulal, Daniel Mendelevitch, Maciej Kilian, Dominik Lorenz, Yam Levi, Zion English, Vikram Voleti, Adam Letts, et al. Stable video diffusion: Scaling latent video diffusion models to large datasets. arXiv preprint arXiv:2311.15127, 2023. [3] Thibault Castells, Hyoung-Kyu Song, Bo-Kyeong Kim, and Shinkook Choi. Ld-pruner: Efficient pruning of latent diffusion models using task-agnostic insights. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 821830, 2024. [4] Pengtao Chen, Mingzhu Shen, Peng Ye, Jianjian Cao, Chongjun Tu, Christos-Savvas Bouganis, Yiren Zhao, and Tao Chen. Delta-dit: training-free acceleration method tailored for diffusion transformers. arXiv preprint arXiv:2406.01125, 2024. [5] Rewon Child, Scott Gray, Alec Radford, and Ilya Sutskever. Generating long sequences with sparse transformers. arXiv preprint arXiv:1904.10509, 2019. [6] Hangliang Ding, Dacheng Li, Runlong Su, Peiyuan Zhang, Zhijie Deng, Ion Stoica, and Hao Zhang. Efficient-vdit: Efficient video diffusion transformers with attention tile. arXiv preprint arXiv:2502.06155, 2025. [7] Patrick Esser, Sumith Kulal, Andreas Blattmann, Rahim Entezari, Jonas Müller, Harry Saini, Yam Levi, Dominik Lorenz, Axel Sauer, Frederic Boesel, et al. Scaling rectified flow transformers for high-resolution image synthesis, 2024. URL https://arxiv. org/abs/2403.03206, 2. [8] Gongfan Fang, Xinyin Ma, and Xinchao Wang. Structural pruning for diffusion models. In Proceedings of the 37th International Conference on Neural Information Processing Systems, NIPS 23, Red Hook, NY, USA, 2023. Curran Associates Inc. [9] Ali Hassani, Steven Walton, Jiachen Li, Shen Li, and Humphrey Shi. Neighborhood attention transformer. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 61856194, 2023. [10] Haoran He, Yang Zhang, Liang Lin, Zhongwen Xu, and Ling Pan. Pre-trained video generative models as world simulators. arXiv preprint arXiv:2502.07825, 2025. [11] Yingqing He, Menghan Xia, Haoxin Chen, Xiaodong Cun, Yuan Gong, Jinbo Xing, Yong Zhang, Xintao Wang, Chao Weng, Ying Shan, et al. Animate-a-story: Storytelling with retrieval-augmented video generation. arXiv preprint arXiv:2307.06940, 2023. [12] Li Hu. Animate anyone: Consistent and controllable image-to-video synthesis for character animation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 81538163, 2024. 10 [13] Ziqi Huang, Yinan He, Jiashuo Yu, Fan Zhang, Chenyang Si, Yuming Jiang, Yuanhan Zhang, Tianxing Wu, Qingyang Jin, Nattapol Chanpaisit, et al. Vbench: Comprehensive benchmark suite for video generative models. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 2180721818, 2024. [14] Huiqiang Jiang, Yucheng Li, Chengruidong Zhang, Qianhui Wu, Xufang Luo, Surin Ahn, Zhenhua Han, Amir Abdi, Dongsheng Li, Chin-Yew Lin, et al. Minference 1.0: Accelerating pre-filling for long-context llms via dynamic sparse attention. Advances in Neural Information Processing Systems, 37:5248152515, 2024. [15] Kumara Kahatapitiya, Haozhe Liu, Sen He, Ding Liu, Menglin Jia, Chenyang Zhang, Michael Ryoo, and Tian Xie. Adaptive caching for faster video generation with diffusion transformers. arXiv preprint arXiv:2411.02397, 2024. [16] Weijie Kong, Qi Tian, Zijian Zhang, Rox Min, Zuozhuo Dai, Jin Zhou, Jiangfeng Xiong, Xin Li, Bo Wu, Jianwei Zhang, et al. Hunyuanvideo: systematic framework for large video generative models. arXiv preprint arXiv:2412.03603, 2024. [17] Xunhao Lai, Jianqiao Lu, Yao Luo, Yiyuan Ma, and Xun Zhou. Flexprefill: contextaware sparse attention mechanism for efficient long-sequence inference. arXiv preprint arXiv:2502.20766, 2025. [18] Bin Lin, Yunyang Ge, Xinhua Cheng, Zongjian Li, Bin Zhu, Shaodong Wang, Xianyi He, Yang Ye, Shenghai Yuan, Liuhan Chen, et al. Open-sora plan: Open-source large video generation model. arXiv preprint arXiv:2412.00131, 2024. [19] Shanchuan Lin, Xin Xia, Yuxi Ren, Ceyuan Yang, Xuefeng Xiao, and Lu Jiang. Diffusion adversarial post-training for one-step video generation. arXiv preprint arXiv:2501.08316, 2025. [20] Feng Liu, Shiwei Zhang, Xiaofeng Wang, Yujie Wei, Haonan Qiu, Yuzhong Zhao, Yingya Zhang, Qixiang Ye, and Fang Wan. Timestep embedding tells: Its time to cache for video diffusion model. arXiv preprint arXiv:2411.19108, 2024. [21] Songhua Liu, Zhenxiong Tan, and Xinchao Wang. Clear: Conv-like linearization revs pre-trained diffusion transformers up. arXiv preprint arXiv:2412.16112, 2024. [22] Ze Liu, Yutong Lin, Yue Cao, Han Hu, Yixuan Wei, Zheng Zhang, Stephen Lin, and Baining Guo. Swin transformer: Hierarchical vision transformer using shifted windows. In Proceedings of the IEEE/CVF international conference on computer vision, pages 1001210022, 2021. [23] Zhengyao Lv, Chenyang Si, Junhao Song, Zhenyu Yang, Yu Qiao, Ziwei Liu, and Kwan-Yee Wong. Fastercache: Training-free video diffusion model acceleration with high quality. arXiv preprint arXiv:2410.19355, 2024. [24] Xinyin Ma, Gongfan Fang, and Xinchao Wang. Deepcache: Accelerating diffusion models for free. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 1576215772, 2024. [25] Fanqing Meng, Jiaqi Liao, Xinyu Tan, Wenqi Shao, Quanfeng Lu, Kaipeng Zhang, Yu Cheng, Dianqi Li, Yu Qiao, and Ping Luo. Towards world simulator: Crafting physical commonsensebased benchmark for video generation. arXiv preprint arXiv:2410.05363, 2024. [26] William Peebles and Saining Xie. Scalable diffusion models with transformers. In Proceedings of the IEEE/CVF international conference on computer vision, pages 41954205, 2023. [27] Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Björn Ommer. Highresolution image synthesis with latent diffusion models. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 1068410695, 2022. [28] Yuzhang Shang, Zhihang Yuan, Bin Xie, Bingzhe Wu, and Yan Yan. Post-training quantization on diffusion models. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 19721981, 2023. 11 [29] Mingzhu Shen, Pengtao Chen, Peng Ye, Guoxuan Xia, Tao Chen, Christos-Savvas Bouganis, and Yiren Zhao. MD-dit: Step-aware mixture-of-depths for efficient diffusion transformers. In Adaptive Foundation Models: Evolving AI for Personalized and Efficient Learning, 2024. [30] Rui Tian, Qi Dai, Jianmin Bao, Kai Qiu, Yifan Yang, Chong Luo, Zuxuan Wu, and YuGang Jiang. Reducio! generating 1024times1024 video within 16 seconds using extremely compressed motion latents. arXiv preprint arXiv:2411.13552, 2024. [31] Philippe Tillet, Hsiang-Tsung Kung, and David Cox. Triton: an intermediate language and compiler for tiled neural network computations. In Proceedings of the 3rd ACM SIGPLAN International Workshop on Machine Learning and Programming Languages, pages 1019, 2019. [32] Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timothée Lacroix, Baptiste Rozière, Naman Goyal, Eric Hambro, Faisal Azhar, et al. Llama: Open and efficient foundation language models. arXiv preprint arXiv:2302.13971, 2023. [33] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan Gomez, Łukasz Kaiser, and Illia Polosukhin. Attention is all you need. Advances in neural information processing systems, 30, 2017. [34] Ang Wang, Baole Ai, Bin Wen, Chaojie Mao, Chen-Wei Xie, Di Chen, Feiwu Yu, Haiming Zhao, Jianxiao Yang, Jianyuan Zeng, et al. Wan: Open and advanced large-scale video generative models. arXiv preprint arXiv:2503.20314, 2025. [35] Jiangshan Wang, Junfu Pu, Zhongang Qi, Jiayi Guo, Yue Ma, Nisha Huang, Yuxin Chen, Xiu Li, and Ying Shan. Taming rectified flow for inversion and editing. arXiv preprint arXiv:2411.04746, 2024. [36] Zhou Wang and Alan Bovik. universal image quality index. IEEE signal processing letters, 9(3):8184, 2002. [37] Junyi Wu, Haoxuan Wang, Yuzhang Shang, Mubarak Shah, and Yan Yan. Ptq4dit: Post-training quantization for diffusion transformers. arXiv preprint arXiv:2405.16005, 2024. [38] Haocheng Xi, Shuo Yang, Yilong Zhao, Chenfeng Xu, Muyang Li, Xiuyu Li, Yujun Lin, Han Cai, Jintao Zhang, Dacheng Li, et al. Sparse videogen: Accelerating video diffusion transformers with spatial-temporal sparsity. arXiv preprint arXiv:2502.01776, 2025. [39] Guangxuan Xiao, Jiaming Tang, Jingwei Zuo, Junxian Guo, Shang Yang, Haotian Tang, Yao Fu, and Song Han. Duoattention: Efficient long-context llm inference with retrieval and streaming heads. arXiv preprint arXiv:2410.10819, 2024. [40] Guangxuan Xiao, Yuandong Tian, Beidi Chen, Song Han, and Mike Lewis. Efficient streaming language models with attention sinks. arXiv preprint arXiv:2309.17453, 2023. [41] Enze Xie, Junsong Chen, Junyu Chen, Han Cai, Haotian Tang, Yujun Lin, Zhekai Zhang, Muyang Li, Ligeng Zhu, Yao Lu, et al. Sana: Efficient high-resolution image synthesis with linear diffusion transformers. arXiv preprint arXiv:2410.10629, 2024. [42] Jinbo Xing, Menghan Xia, Yong Zhang, Haoxin Chen, Wangbo Yu, Hanyuan Liu, Gongye Liu, Xintao Wang, Ying Shan, and Tien-Tsin Wong. Dynamicrafter: Animating open-domain images with video diffusion priors. In European Conference on Computer Vision, pages 399417. Springer, 2024. [43] Zhuoyi Yang, Jiayan Teng, Wendi Zheng, Ming Ding, Shiyu Huang, Jiazheng Xu, Yuanming Yang, Wenyi Hong, Xiaohan Zhang, Guanyu Feng, et al. Cogvideox: Text-to-video diffusion models with an expert transformer. arXiv preprint arXiv:2408.06072, 2024. [44] Zhihang Yuan, Hanling Zhang, Lu Pu, Xuefei Ning, Linfeng Zhang, Tianchen Zhao, Shengen Yan, Guohao Dai, and Yu Wang. Ditfastattn: Attention compression for diffusion transformer models. Advances in Neural Information Processing Systems, 37:11961219, 2024. [45] Yuanhao Zhai, Kevin Lin, Zhengyuan Yang, Linjie Li, Jianfeng Wang, Chung-Ching Lin, David Doermann, Junsong Yuan, and Lijuan Wang. Motion consistency model: Accelerating video diffusion with disentangled motion-appearance distillation. Advances in Neural Information Processing Systems, 37:111000111021, 2024. [46] Chi Zhang, Chengjian Feng, Feng Yan, Qiming Zhang, Mingjin Zhang, Yujie Zhong, Jing Zhang, and Lin Ma. Instructvedit: holistic approach for instructional video editing. arXiv preprint arXiv:2503.17641, 2025. [47] Hanling Zhang, Rundong Su, Zhihang Yuan, Pengtao Chen, Mingzhu Shen Yibo Fan, Shengen Yan, Guohao Dai, and Yu Wang. Ditfastattnv2: Head-wise attention compression for multimodality diffusion transformers. arXiv preprint arXiv:2503.22796, 2025. [48] Richard Zhang, Phillip Isola, Alexei Efros, Eli Shechtman, and Oliver Wang. The unreasonable effectiveness of deep features as perceptual metric. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 586595, 2018. [49] Maosen Zhao, Pengtao Chen, Chong Yu, Yan Wen, Xudong Tan, and Tao Chen. Pioneering 4-bit fp quantization for diffusion models: Mixup-sign quantization and timestep-aware fine-tuning, 2025. [50] Xuanlei Zhao, Xiaolong Jin, Kai Wang, and Yang You. Real-time video generation with pyramid attention broadcast. arXiv preprint arXiv:2408.12588, 2024. [51] Lianghui Zhu, Zilong Huang, Bencheng Liao, Jun Hao Liew, Hanshu Yan, Jiashi Feng, and Xinggang Wang. Dig: Scalable and efficient diffusion models with gated linear attention. arXiv preprint arXiv:2405.18428, 2024. 13 Appendix for SPARSE-VDIT"
        },
        {
            "title": "A Algorithm Implementation",
            "content": "Figure 5 presents the overall process of the offline sparse diffusion search algorithm, with implementation details provided in the accompanying pseudocode. By optimizing across layers and heads, the algorithm selects attention patterns for each head in vDiT. These optimized patterns are subsequently used to accelerate inference. Algorithm 1 Offline Sparse Diffusion Search Input: Pretraine vDiT model (N layers and heads), hyperparameter λ and ϵ, predefined attention pattern Mi and sparsity Si, timestep Output: Attention pattern config for in0, ..., 4 do Predefined Attention Kernel. Compile sparse attention Mi accoding to Si end Offline Sparse Search. =[] xT (0, I) for inT, ..., 1 do xp = reprocess (xt) vDiT Layers. for in1, ..., do Q, K, = Linear, RoP and orm (xp ) Attention Part (Our Optimization Object). loss = [] xgt = M0(Q, K, ) = zeros_like(xgt xo ) for in1, ..., 4 do xi = Mi(Q, K, ) loss.append(M SE(xi t, xgt ) + λSi) end Per-Head Optimization. for in1, ..., do if (lossh > ϵ).sum 4 then t,h = xgt xo f.append(0) t,h end else = argmin(lossh) t,h = xi xo t,h f.append(i) end end FFN Part. xp = P (xo ) end Denoising. xt1 = Solver(xt, xp ) end returnf 14 Table 4: Comparison of video generation quality and efficiency between Sparse-vDiT and the baseline. All reported efficiency metrics are measured on single NVIDIA H800 GPU with batch size of 1. Method SSIM () PSNR () LPIPS () ImageQual () SubConsist () Latency () Speedup () Against Original VBench Score Wan2.1 SVG Sparse-vDiT (Ours) Sparse-vDiT + FP8 (Ours) - 0.78 0.80 0.79 - 21.96 22.59 22.39 - 0.18 0.16 0. 67.61% 67.18% 67.35% 67.22% 91.95% 91.27% 91.39% 91.29% 1935s 1298s 1228s 1089s 1.00 1.49 1.58 1."
        },
        {
            "title": "B Performance on more pretrain models",
            "content": "Recent models with Self-Attn and Cross-Attn structure, such as wan2.1, have also demonstrated strong performance. To further assess Sparse-vDiT, we evaluate it under this architecture as well. As shown in Table 4, our Sparse-vDiT framework introduces sparse video Diffusion Transformer that achieves 1.78 speedup (1089ms vs. 1298ms) over SVG while maintaining superior perceptual quality (SSIM: 0.80, LPIPS: 0.16), leveraging structured sparsity and FP8 quantization to reduce latency by 17% with negligible quality degradation (<0.5% drop on VBench), outperforming Wan2.1 and SVG across all metrics (+0.0204 SSIM) and demonstrating hardware-efficient scalability on H800 GPUs with near-linear acceleration, bridging the gap between theoretical sparsity and practical deployment in diffusion-based video generation."
        },
        {
            "title": "C More visual results",
            "content": "Due to space constraints, the main manuscript only compares visualization results for limited set of baseline methods. Here, we present additional visualizations. Figures 8, Figure 7, and Figure 9 compare our method against all baselines. The WinAttn method exhibits significant contour shifts, while SVG shows smaller deviations. PAB and MInference suffer from frame smoothing and blurring. In contrast, our method preserves the overall contour consistent with the pretrained model and achieves the highest acceleration ratio to date, effectively balancing generation speed and quality. Beyond individual frame quality, frame-to-frame consistency is visualized in Figure 10 and Figure 11. Sparse-vDiT closely matches the pretrained models temporal consistency, indicating strong frame coherence. The visualization results start on the next page. 15 Figure 7: More visual comparison between the proposed Sparse-vDiT and the baseline method. Our method maximizes computational speedup while maintaining high fidelity to the pretrain model. 16 Figure 8: More visual comparison between the proposed Sparse-vDiT and the baseline method. Our method maximizes computational speedup while maintaining high fidelity to the pretrain model. Figure 9: More visual comparison between the proposed Sparse-vDiT and the baseline method. Our method maximizes computational speedup while maintaining high fidelity to the pretrain model. 18 Figure 10: More visual comparison between the proposed Sparse-vDiT and the pretrain model. Beyond demonstrating superior performance in frame generation quality, our method exhibits robust capabilities in maintaining inter-frame consistency. 19 Figure 11: More visual comparison between the proposed Sparse-vDiT and the pretrain model. Beyond demonstrating superior performance in frame generation quality, our method exhibits robust capabilities in maintaining inter-frame consistency."
        }
    ],
    "affiliations": [
        "Fudan University",
        "Imperial College London",
        "StepFun",
        "The Chinese University of Hong Kong"
    ]
}