{
    "paper_title": "XR: Cross-Modal Agents for Composed Image Retrieval",
    "authors": [
        "Zhongyu Yang",
        "Wei Pang",
        "Yingfang Yuan"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Retrieval is being redefined by agentic AI, demanding multimodal reasoning beyond conventional similarity-based paradigms. Composed Image Retrieval (CIR) exemplifies this shift as each query combines a reference image with textual modifications, requiring compositional understanding across modalities. While embedding-based CIR methods have achieved progress, they remain narrow in perspective, capturing limited cross-modal cues and lacking semantic reasoning. To address these limitations, we introduce XR, a training-free multi-agent framework that reframes retrieval as a progressively coordinated reasoning process. It orchestrates three specialized types of agents: imagination agents synthesize target representations through cross-modal generation, similarity agents perform coarse filtering via hybrid matching, and question agents verify factual consistency through targeted reasoning for fine filtering. Through progressive multi-agent coordination, XR iteratively refines retrieval to meet both semantic and visual query constraints, achieving up to a 38% gain over strong training-free and training-based baselines on FashionIQ, CIRR, and CIRCO, while ablations show each agent is essential. Code is available: https://01yzzyu.github.io/xr.github.io/."
        },
        {
            "title": "Start",
            "content": "XR: Cross-Modal Agents for Composed Image Retrieval Zhongyu Yang BCML, Heriot-Watt University Edinburgh, UK zy4028@hw.ac.uk Wei Pang BCML, Heriot-Watt University Edinburgh, UK w.pang@hw.ac.uk Yingfang Yuan BCML, Heriot-Watt University Edinburgh, UK y.yuan@hw.ac.uk 6 2 0 2 J 0 2 ] . [ 1 5 4 2 4 1 . 1 0 6 2 : r Figure 1: The workflows of existing CIR methods and ours: (a) Joint embeddingbased methods encode multimodal query into shared space, but they often struggle to capture complex text-specified edits. (b) Caption-to-Image methods first generate target caption from the multimodal query prior to retrieval, but they often fail to preserve fine-grained details. (c) Caption-to-Caption methods build upon Caption-to-Image but restrict comparison to the text space, thereby discarding visual cues. (d) XR (ours) introduces an agentic AI framework with cross-modal agents and progressive retrieval process consisting of an imagination stage followed by coarse-to-fine filtering, enabling robust reasoning that better aligns results with user intent."
        },
        {
            "title": "Abstract",
            "content": "Retrieval is being redefined by agentic AI, demanding multimodal reasoning beyond conventional similarity-based paradigms. Composed Image Retrieval (CIR) exemplifies this shift as each query combines reference image with textual modifications, requiring compositional understanding across modalities. While embeddingbased CIR methods have achieved progress, they remain narrow in perspective, capturing limited cross-modal cues and lacking semantic reasoning. To address these limitations, we introduce XR, training-free multi-agent framework that reframes retrieval as progressively coordinated reasoning process. It orchestrates three specialized types of agents: imagination agents synthesize target representations through cross-modal generation, similarity agents perform coarse filtering via hybrid matching, and question agents verify factual consistency through targeted reasoning for fine filtering. Through progressive multi-agent coordination, XR iteratively refines retrieval to meet both semantic and visual query constraints, achieving up to 38% gain over strong trainingfree and training-based baselines on FashionIQ, CIRR, and CIRCO, while ablations show each agent is essential. Code is available: https://01yzzyu.github.io/xr.github.io/. CCS Concepts Information systems Information retrieval; Retrieval models and ranking; Users and interactive retrieval. Corresponding author"
        },
        {
            "title": "Keywords",
            "content": "This work is licensed under Creative Commons Attribution 4.0 International License. WWW 26, Dubai, United Arab Emirates 2026 Copyright held by the owner/author(s). ACM ISBN 979-8-4007-2307-0/2026/04 https://doi.org/10.1145/3774904.3792276 Compose Image Retrieval, Agents, Cross-modality ACM Reference Format: Zhongyu Yang, Wei Pang, and Yingfang Yuan. 2026. XR: Cross-Modal Agents for Composed Image Retrieval. In Proceedings of the ACM Web Conference 2026 (WWW 26), April 1317, 2026, Dubai, United Arab Emirates. , 12 pages. https://doi.org/10.1145/3774904.3792276 WWW 26, April 1317, 2026, Dubai, United Arab Emirates Yang et al."
        },
        {
            "title": "1 Introduction",
            "content": "Composed Image Retrieval (CIR) [4, 7, 17, 30, 42] is retrieval paradigm where query is explicitly composed by the user through reference image and modification text. CIR queries embody specific intent through the controlled composition of image and text. This not only establishes CIR as new direction in web information access, where users refine searches by combining images and text, but also links it to broader developments in retrieval-augmented agentic AI. The demand for such interaction is evident in applications such as e-commerce [6, 56] and search engines [46], where navigating massive image repositories requires fine-grained multimodal control. Compared with conventional retrieval [11, 14, 29, 43], CIR is particularly challenging because it requires cross-modal reasoning to integrate heterogeneous signals rather than relying on single unimodal cue. By pairing reference image with textual modifications, CIR moves retrieval beyond simple content matching toward retrieving images that preserve reference semantics while faithfully applying the edits. As illustrated in Figure 1, existing approaches can be broadly grouped into three categories: (a) Joint embedding-based methods project the multimodal query into vector space and formulate CIR as similarity-based matching; (b) Caption-to-Image methods first generate target caption based on the multimodal query, then embed it and compare it with candidate image embeddings in terms of similarity; and (c) Caption-to-Caption methods that directly compare candidate captions with the target caption. Despite notable progress, these approaches exhibit persistent limitations. First, joint embeddings struggle to capture fine-grained, edit-specific correspondences due to imperfect cross-modal alignment. Second, single similarity-based matching approach may fail to capture both textual and visual evidence. Third, the absence of cross-modal verification-based refinement undermines reliability, as each modality provides important information. These challenges highlight our motivation that effective CIR must fully exploit cross-modal interactions. To address these challenges, we propose XR, training-free multi-agent framework that explicitly orchestrates cross-modal reasoning, providing robust retrieval under heterogeneous signals. XR consists of three sequential modules: imagination, coarse filtering, and fine filtering. In imagination, agents construct target proxy by generating captions from two cross-modal pairings, namely modification text with the reference image caption and modification text with the reference image, which helps reduce modality gaps and anchor the target semantics. In coarse filtering, similarity-based agents evaluate candidates by producing multi-perspective scores using visual and textual cues, each conditioned on cross-modal captions. Reciprocal Rank Fusion (RRF) then aggregates these scores to form an initial ranked subset that addresses the limitations of single-criterion matching. In fine filtering, question-based agents re-evaluate this subset through cross-modal factual verification by testing candidate images and captions with predicate-style queries, which mimic how humans validate retrieval consistency. Finally, verification scores are integrated with similarity scores through re-ranking to produce the final retrieval set, benefiting from both similarity-based matching and factual verification. The similarity-based and question-based agents play complementary roles, where the former enables efficient high-level retrieval for broad coverage, while the latter enforces factual verification to refine results for accuracy. This design preserves diverse sources of evidence that single-score pipelines would otherwise overlook. Moreover, the cross-modality employed in both agents within XR enhances reliability by providing multi-perspective evidence. This is achieved through combination of implicit coupling and explicit decoupling of modalities, enabling effective integration while maintaining per-modality interpretability. The proposed framework is tailored for edit-sensitive compositionality, capturing fine-grained modifications beyond the capability of unimodal systems. We evaluate XR on three CIR benchmarks, CIRR, CIRCO, and FashionIQ, covering diverse retrieval scenarios from controlled reference-based queries to open-domain compositional settings. Across all datasets, XR consistently improves edit-sensitive retrieval accuracy over strong training-free and training-based baselines, demonstrating both its effectiveness and generality. These results suggest practical value for web systems and applications, including personalized e-commerce search and multimodal recommendation. In summary, our contributions are as follows: We propose XR, training-free framework that orchestrates multiple cross-modal agents for CIR. We demonstrate the necessity of explicit cross-modality by showing its advantage over unimodal and single-score pipelines, which fail on edit-sensitive reasoning. Extensive experiments on CIRR, CIRCO, and FashionIQ show consistent gains over strong baselines, with ablations substantiating each modules role, positioning XR as general paradigm for multimodal retrieval."
        },
        {
            "title": "2 Related Works\nMultimodal Agent Systems: The rapid progress in MLLMs [15, 16,\n33, 44] has enabled agentic frameworks with emerging abilities in\nautonomous planning, tool use, and decision-making. Such frame-\nworks show strong potential for decomposing complex reasoning\nand coordinating across modalities [37, 49, 50, 53, 55], though co-\nordination remains fragile in practice. By iterative reflection and\ncollaborative strategies, they mitigate hallucination and enhance in-\nterpretability, outperforming single-pass inference albeit at higher\ncost [28, 32, 34, 52]. Yet most multimodal agents operate under\na closed-world assumption, relying solely on internal inference\nand often hallucinating unsupported content [10, 20], reflecting a\nlack of external grounding. In contrast, retrieval has long served\nas grounding in NLP pipelines, reducing uncertainty and improv-\ning adaptability [2, 26, 38]. Recent studies on retrieval-augmented\nagents, such as Storm [21, 36] and WikiAutoGen [48] highlight the\npromise of retrieval-augmented agents, yet remain limited: Storm\nis text-centric, while WikiAutoGen extends to multimodality but\nin a narrow scope. Overall, these findings underscore retrieval as a\nkey enabler of reasoning, yet a systematic integration into general\nmultimodal agents remains missing.\nComposed Image Retrieval: CIR provides a natural testbed for\nretrieval-enhanced reasoning, where the task is to locate a target im-\nage given a reference image and textual modifications [39, 54]. Most\nexisting methods fuse features into a joint embedding and rank",
            "content": "XR: Cross-Modal Agents for Composed Image Retrieval WWW 26, April 1317, 2026, Dubai, United Arab Emirates candidates by similarity, achieving coarse alignment but blurring fine-grained changes [7, 23, 42]. Training-based models enhance representations but demand costly supervision and frequent retraining [3, 5, 18, 47]. Training-free approaches avoid task-specific supervision and generalize across domains. However, they rely on static fusion and one-shot pipelines, with little flexibility to refine uncertain retrieval results (e.g., candidate images or captions) [8, 24, 25]. Reasoning-style retrieval has been explored [13, 40, 41], but existing methods remain fixed templates rather than adaptive workflows. In practice, models still fail on fine-grained edits, for example, misinterpreting color changes in FashionIQ or mismatching object replacements in CIRR, underscoring the persistent limits of static similarity matching. This indicates that static similarity is not only brittle to edits but also fundamentally unable to capture compositional semantics. In short, multimodal agents excel at reasoning but underexploit retrieval, while CIR methods leverage retrieval but lack reasoning, leaving the two largely disconnected. XR bridges this gap by embedding retrieval within an agentic workflow: (1) imagination agents approximate the target image, preserving fine-grained details often missed by embeddings; (2) similarity-based agents score candidates across modalities, reducing the rigidity of one-shot pipelines; (3) question-based agents enforce factual checks, ensuring textual modifications are faithfully satisfied. Unified in training-free system, these components elevate retrieval into dynamic reasoning, addressing the above limitations and yielding results that are both faithful to user intent and verifiable across modalities."
        },
        {
            "title": "3 Method\n3.1 Preliminaries\nGiven a multimodal query consisting of a reference image ğ¼ğ‘Ÿ and\na modification text ğ‘‡ğ‘š, CIR assumes the existence of an ideal tar-\nget image ğ¼ğ‘– that represents the desired outcome by preserving\nthe visual characteristics of ğ¼ğ‘Ÿ while incorporating the modifica-\ntions specified by ğ‘‡ğ‘š. The CIR task then aims to retrieve a subset\nof images Iâˆ— âŠ† I, where I = {ğ¼1, ğ¼2, . . . , ğ¼ğ‘ } denotes the candi-\ndate image set containing ğ‘ images. Each ğ¼ âˆˆ Iâˆ— is expected to\napproximate the ideal target image ğ¼ğ‘– .",
            "content": "To obtain I, CIR typically proceeds in two stages. In the scoring stage, each candidate image ğ¼ is evaluated against the query (ğ¼ğ‘Ÿ ,ğ‘‡ğ‘š), which implicitly defines the ideal target image ğ¼ğ‘– . matching score is then assigned, ğ‘† (ğ¼ ) = ğ‘ (ğ¼ ğ¼ğ‘Ÿ ,ğ‘‡ğ‘š), which represents the conditional probability that ğ¼ belongs to the target set given the query. This score can also be viewed as similarity measure between ğ¼ and the ideal target ğ¼ğ‘– . In the subsequent ranking stage, candidates are ordered by their scores, and the top-ğ‘˜ images are selected to form I, where = ğ‘˜."
        },
        {
            "title": "3.2 XR Framework\nTo address CIR, we propose XR, a training-free multi-agent frame-\nwork that emphasizes the role of cross-modality in improving re-\ntrieval accuracy. Unlike existing approaches, our framework is\ncomposed of three cross-modal modules: imagination, coarse fil-\ntering, and fine filtering. In the coarse filtering stage, similarity-\nbased scoring agents identify an initial subset of candidates that\napproximate the ideal target image through similarity evaluation.",
            "content": "In the fine filtering stage, question-based scoring agents perform factual verification to further filter and refine this subset, producing the final ordered set I. These two components complement each other, working together to progressively narrow down candidates and improve ranking. Throughout the process, cross-modal mechanisms offer multi-perspective support that enhances robustness and reliability in retrieval while reducing the risks associated with multimodal misalignment. ğ‘¡ and the vision imagination agent Ağ‘– The workflow of XR is outlined in Algorithm 1 and depicted in Figure 1. The caption agent Ağ‘ generates set of candidate captions by iteratively producing caption for each ğ¼ (Line 1). It also generates the caption ğ¶ğ‘Ÿ for the reference image ğ¼ğ‘Ÿ (Line 2). To construct or imagine the ideal target image ğ¼ğ‘– , the text imagination agent Ağ‘– ğ‘£ jointly form the cross-modal imagination module. These two agents generate captions ğ¶ğ‘¡ and ğ¶ğ‘£, respectively, to describe ğ¼ğ‘– from different modalities. In addition, Ağ‘– ğ‘¡ produces ğ‘€ğ‘¡ , which specifies the manipulations required to transform ğ¶ğ‘Ÿ with ğ‘‡ğ‘š in order to approximate ğ¼ğ‘– , thereby unifying the information in the text modality. Meanwhile, ğ‘€ğ‘£ denotes set indicating the presence or absence of visual attributes. In coarse filtering, to evaluate each candidate image ğ¼ , XR employs text similarity-based scoring agent Ağ‘  ğ‘¡ (Line 6) and vision similarity-based scoring agent Ağ‘  ğ‘£ (Line 7). These agents assess the ğ‘-th candidate from different modalities, using ğ¶ğ‘ and ğ¼ğ‘ respectively, conditioned on ğ¶ğ‘¡ and ğ¶ğ‘£. Each scoring agent produces two scores, denoted ğ‘ ğ‘¡ and ğ‘  ğ‘£, through process termed cross-modal multiperspective scoring, which operates within the embedding space based on similarity. The text-based score ğ‘ ğ‘¡ and vision-based score ğ‘  ğ‘£ are then aggregated across agents in Lines 8 and 9. In Line 10, the score vectors ğ‘†ğ‘¡ = {ğ‘ ğ‘¡ ğ‘ } 1 are processed separately using Reciprocal Rank Fusion function. The resulting rank scores are combined, after which candidates are ranked and filtered to yield the top-ğ‘˜ results, denoted by Iğ‘˜ ğ‘ } and ğ‘† ğ‘£ = {ğ‘  ğ‘£ 1 , . . . , ğ‘  ğ‘£ , . . . , ğ‘ ğ‘¡ . To enhance the ranking accuracy of the selected top-ğ‘˜ candidates and further refine this subset, XR incorporates cross-modal question-based scoring agents in the fine filtering stage. The question agent Ağ‘ (Line 11) formulates set of questions ğ‘„ with corresponding answers ğ´ based on the information ğ‘€ğ‘¡ , ğ‘€ğ‘£, and ğ‘‡ğ‘š, focusing on the essential attributes that the ideal target image ğ¼ğ‘– should contain. Unlike the similarity-based scoring agents, Ağ‘ emphasizes the critical differences between candidate image ğ¼ ğ‘ and the ideal target ğ¼ğ‘– . Two question-based scoring agents, ğ‘¡ and ğ‘ ğ‘£ , then use ğ‘„ and ğ´ to re-evaluate candidates in Iğ‘˜ from the text and vision modalities, applying ğ¶ğ‘ and ğ¼ğ‘ respectively (Lines 1314). Finally, the scores from the question-based scoring agents are combined with the aggregated similarity-based scores through cross-modal re-ranking function (Line 15). The candidates are then re-ordered based on the resulting scores to form the fine-filtered subset, which constitutes the final output of size ğ‘˜, where ğ‘˜ < ğ‘˜ . The following paragraphs describe in detail the roles of imagination, coarse filtering, and fine filtering."
        },
        {
            "title": "3.3 Imagination\nğ‘¡ and Ağ‘–\nThe imagination agents Ağ‘–\nğ‘£ play a central role in XR. In\nCIR, accurate retrieval requires a clear representation of the ideal\ntarget image ğ¼ğ‘– that satisfies the multimodal query, since defining",
            "content": "WWW 26, April 1317, 2026, Dubai, United Arab Emirates Yang et al. Figure 2: Framework of XR. The multi-agent system integrates textual and visual imagination with cross-modal similarity and question-based scoring, followed by re-ranking. This multi-stage reasoning process exploits complementary cues from both modalities, effectively handling fine-grained modifications that single-modality approaches often miss. Algorithm 1 XR Input: cadidate image set, ğ¼ğ‘Ÿ reference image, ğ‘‡ğ‘š modification text Output: target image set # initialization 1: = Ağ‘ (I) 2: ğ¶ğ‘Ÿ = Ağ‘ (ğ¼ğ‘Ÿ ) # imagination 3: ğ‘€ğ‘¡, ğ¶ğ‘¡ = Ağ‘– 4: ğ‘€ğ‘£, ğ¶ğ‘£ = Ağ‘– ğ‘¡ (ğ‘‡ğ‘š, ğ¶ğ‘Ÿ ) ğ‘£ (ğ‘‡ğ‘š, ğ¼ğ‘Ÿ ) 7: # coarse filtering 5: for ğ‘ = 1 to ğ‘ do ğ‘¡ = Ağ‘  ğ‘¡ , ğ‘  ğ‘£ ğ‘ ğ‘¡ ğ‘¡ (ğ¶ğ‘¡, ğ¶ğ‘£, ğ¶ğ‘) 6: ğ‘ ğ‘¡ ğ‘£, ğ‘  ğ‘£ ğ‘£ = Ağ‘  ğ‘£ (ğ¶ğ‘¡, ğ¶ğ‘£, ğ¼ğ‘) ğ‘ ğ‘¡ ğ‘¡ + ğ‘ ğ‘¡ ğ‘ = ğ‘ ğ‘¡ ğ‘£ ğ‘  ğ‘£ ğ‘ = ğ‘  ğ‘£ ğ‘£ + ğ‘  ğ‘£ ğ‘¡ = ranking(ğ‘†ğ‘¡ , ğ‘† ğ‘£) 9: 10: Iğ‘˜ 8: # fine filtering 11: ğ‘„, ğ´ = Ağ‘ (ğ‘€ğ‘¡, ğ‘€ğ‘£,ğ‘‡ğ‘š) 12: for ğ‘ = 1 to ğ‘˜ do ğ‘ ğ‘¡ ğ‘ = 13: ğ‘  ğ‘£ ğ‘ = 14: 15: ğ‘†ğ‘˜ (ğ‘†ğ‘¡ re-ranking ğ‘ ğ‘¡ (ğ¶ğ‘, ğ‘„, ğ´) ğ‘ ğ‘£ (ğ¼ğ‘, ğ‘„, ğ´) ğ‘ + ğ‘† ğ‘£ 16: = re-ranking(ğ‘†ğ‘˜ ) 17: return // candidate image caption set // text imagination agent // vision imagination agent // ğ¼ğ‘ I, ğ¶ğ‘ // text similarity-based scoring agent // vision similarity-based scoring agent // reciprocal rank fusion function // question agent // text question-based scoring agent // vision question-based scoring agent // cross-modal // = ğ‘˜ ğ‘) norm(ğœ†ğ‘†ğ‘¡ + (1 ğœ†)ğ‘† ğ‘£) ğ¼ğ‘– provides prior knowledge and evidence to guide the retrieval of similar candidates. This prior knowledge is critical, as incorrect priors can trigger cascade of errors. To address this, the agents Ağ‘– ğ‘¡ and Ağ‘– ğ‘£ are designed to imagine and approximate the ideal target ğ‘¡ take different inputs: Ağ‘– image by generating cross-modal captions that capture complementary aspects of ğ¼ğ‘– . The cross-modality arises from the fact that Ağ‘– ğ‘£ and Ağ‘– ğ‘£ uses the reference image ğ¼ğ‘Ÿ and Ağ‘– ğ‘¡ uses the reference caption ğ¶ğ‘Ÿ , and both are conditioned on the modification text ğ‘‡ğ‘š. Their outputs are the vision-based caption ğ¶ğ‘£ and the text-based caption ğ¶ğ‘¡ , respectively. The design of cross-modal imagination is motivated by the observation that information from different modalities provides complementary strengths and weaknesses when estimating ğ¼ğ‘– . Specifically, the pair (ğ‘‡ğ‘š, ğ¶ğ‘Ÿ ) is more straightforward, which facilitates the extraction of key information for depicting ğ¼ğ‘– . In contrast, the pair (ğ‘‡ğ‘š, ğ¼ğ‘Ÿ ) combines textual and visual inputs, where the image contributes fine-grained details that complement the textual description. We argue that combining both perspectives enables the model to adapt flexibly to diverse situations encountered in real-world retrieval tasks. As shown in Figure 2, in cross-modal imagination, ğ¶ğ‘£ and ğ¶ğ‘¡ generate captions that are similar but not identical. The former captures fine-grained visual details, such as outdoors and multiple medium dogs, which are grounded in the image. The latter, by contrast, emphasizes semantic transformation, for example transformed from several smaller dogs, reflecting more abstract and text-driven perspective rather than visually specific cues. Additionally, Ağ‘– ğ‘¡ is designed to output ğ‘€ğ‘¡ , which represents the modifications between ğ‘‡ğ‘š and ğ¶ğ‘Ÿ . While ğ‘‡ğ‘š is provided by the user or predefined to describe changes in the text modality that are applied to the visual modality, ğ‘€ğ‘¡ is derived entirely from the text modality and provides more specific and explicit modifications. At the same time, Ağ‘– ğ‘£ produces ğ‘€ğ‘£, which denotes set indicating the presence or absence of visual attributes. The example of ğ‘€ğ‘£ and ğ‘€ğ‘¡ can be found in Figure 2. The outputs ğ‘€ğ‘¡ and ğ‘€ğ‘£ will later be used by the question agent, which will be discussed in detail in subsequent section. XR: Cross-Modal Agents for Composed Image Retrieval WWW 26, April 1317, 2026, Dubai, United Arab Emirates Table 1: Performance comparison on CIRCO and CIRR test set. The best results are in bold, and the second best are underlined. Backbone Method Venue Training-free 2 3 / - - C 4 1 / - - C ECCV 2022 ICCV 2023 ICCV 2023 TPAMI 2025 TPAMI 2025 ICLR 2024 SIGIR 2024 PALAVRA [9] SEARLE [4] SEARLE-OTI [4] iSEARLE [1] iSEARLE-OTI [1] CIReVL [22] LDRE [51] ImageScope [31] WWW 2025 XR(Ours) Pic2Word [35] SEARLE [4] SEARLE-OTI [4] iSEARLE [1] iSEARLE-OTI [1] LinCIR [12] FTI4CIR [27] CIReVL [22] LDRE [51] ImageScope [31] WWW 2025 XR(Ours) Proposed CVPR 2023 ICCV 2023 ICCV 2023 TPAMI 2025 TPAMI 2025 CVPR 2024 SIGIR 2024 ICLR 2024 SIGIR 2024 Proposed CIRCO mAP@5 mAP@10 mAP@25 mAP@50 6.80 11.84 9.60 13.26 13.01 17.82 21.11 23.83 30.95 11.29 15.12 13.67 16.25 15.34 15.85 19.05 21.80 27.50 27.98 36.50 5.32 9.94 7.38 11.24 10.94 15.42 18.32 22.19 28.33 9.51 12.73 11.03 13.61 12.67 13.58 16.32 19.01 24.03 25.82 32.88 6.33 11.13 8.99 12.51 12.27 17.00 20.21 23.03 30.28 10.64 14.33 12.72 15.36 14.46 15.00 18.06 20.89 26.44 27.07 35.46 4.61 9.35 7.14 10.58 10.31 14.94 17.96 22.36 27.51 8.72 11.68 10.18 12.50 11.31 12.59 15.05 18.57 23.35 25.39 31. CIRR R@1 16.62 24.00 24.27 25.23 26.19 23.94 25.69 34.36 43.06 23.90 24.24 24.87 25.28 25.40 25.04 25.90 24.55 26.53 34.99 43.13 R@5 43.49 53.42 53.25 55.69 55.18 52.51 55.13 60.58 73.86 51.70 52.48 52.32 54.00 54.05 53.25 55.61 52.31 55.57 61.35 73.59 R@10 R@50 83.95 58.51 89.78 66.82 88.84 66.10 90.82 68.05 90.65 68.05 86.95 66.00 89.90 69.04 88.41 71.40 94.36 83.15 87.80 65.30 88.84 66.29 88.58 66.29 88.80 66.72 88.92 67.47 - 66.68 89.66 67.66 86.34 64.92 88.50 67.54 88.84 71.49 94.05 83.09 CIRRğ‘ ğ‘¢ğ‘ğ‘ ğ‘’ğ‘¡ R@2 65.30 76.60 75.81 - - 80.05 80.65 87.93 90.27 - 75.01 74.31 - - 77.37 75.88 79.88 80.31 88.24 90.68 R@1 41.61 54.89 54.10 - - 60.17 60.53 74.63 77.54 - 53.76 53.80 - - 57.11 55.21 59.54 60.43 74.94 77. R@3 80.95 88.19 87.33 - - 90.19 90.70 93.83 95.21 - 88.19 86.94 - - 88.89 87.98 89.69 89.90 94.02 95."
        },
        {
            "title": "3.4 Coarse Filtering\nIn Lines 5â€“9, each candidate image ğ¼ âˆˆ I is evaluated by the text\nsimilarity-based scoring agent Ağ‘ \nğ‘¡ and the vision similarity-based\nscoring agent Ağ‘ \nğ‘£. To improve robustness, the scoring process inte-\ngrates three levels of cross-modality, providing multiperspective\ninformation across different stages and producing more reliable\nresults through hybrid cross-modal similarity.",
            "content": "ğ‘¡ and Ağ‘– ğ‘¡ and Ağ‘  First, the scoring of both agents relies on ğ¶ğ‘¡ and ğ¶ğ‘£ produced by Ağ‘– ğ‘£, which together approximate the ideal target image ğ¼ğ‘– . Although both ğ¶ğ‘¡ and ğ¶ğ‘£ are textual representations, they are derived from different modalities: the text modality and the vision modality, respectively. Second, each candidate image indexed by ğ‘ {1, . . . , ğ‘ } is also evaluated with respect to its caption ğ¶ğ‘ and its visual content ğ¼ğ‘ by Ağ‘  ğ‘£, respectively. Third, each scoring agent produces two cross-modal scores. In Line 6, the scores ğ‘ ğ‘¡ ğ‘¡ ğ‘¡ are generated by the text similarity-based scoring agent Ağ‘  and ğ‘  ğ‘£ ğ‘¡ . Specifically, ğ‘ ğ‘¡ ğ‘¡ measures the similarity between ğ¶ğ‘¡ and ğ¶ğ‘, while ğ‘  ğ‘£ ğ‘¡ measures the similarity between ğ¶ğ‘£ and ğ¶ğ‘. Since ğ¶ğ‘¡ originates from the text modality and ğ¶ğ‘£ implicitly reflects visual content, these two scores capture cross-modal signals that combine implicitly coupled and explicitly decoupled multimodality. Here, explicitly decoupled multimodality refers to processing that occurs entirely within the text modality, whereas implicit coupling indicates that visual information is embedded within the textual representation. The same procedure is applied by the vision similarity-based scoring agent Ağ‘  ğ‘£ measure the similarity between ğ¼ğ‘ and ğ¶ğ‘¡ , and between ğ¼ğ‘ and ğ¶ğ‘£, respectively. Here, the pair (ğ¼ğ‘, ğ¶ğ‘£) belongs to the visual modality, whereas the pair (ğ¼ğ‘, ğ¶ğ‘¡ ) combines the vision and text modalities. These procedures are collectively referred to as cross-modal scoring. Each score is generated by its corresponding agent, which encodes the inputs using an MLLM and computes the cosine similarity between the paired representations. In Lines 8 and 9, for each candidate ğ¼ , the scores from the text and vision modalities across agents are summed to obtain ğ‘ ğ‘¡ and ğ‘  ğ‘£, respectively. Rather than aggregating the outputs of single agent, the scores are aligned within each modality to ensure consistency ğ‘£. In Line 7, the scores ğ‘ ğ‘¡ ğ‘£ and ğ‘  ğ‘£ in cross-modal scoring. In Line 10, reciprocal rank fusion function is applied to transform the similarity score vectors ğ‘†ğ‘¡ = (ğ‘ ğ‘¡ ğ‘ ) 1 and ğ‘† ğ‘£ = (ğ‘  ğ‘£ ğ‘ ) into rank values, which are then summed 1 across the text and vision modalities. The ranking function is defined as: , . . . , ğ‘  ğ‘£ , . . . , ğ‘ ğ‘¡ RRF (ğ‘) = 1 ğ‘§ + rank (cid:0)ğ‘ ğ‘¡ ğ‘ (cid:1) + 1 ğ‘§ + rank (ğ‘  ğ‘£ ğ‘) , (1) ğ‘) and rank(ğ‘  ğ‘£ ğ‘ and the vision-based score ğ‘  ğ‘£ where rank(ğ‘ ğ‘¡ ğ‘) denote the rank positions of the textbased score ğ‘ ğ‘¡ ğ‘ among all candidates, respectively, and ğ‘§ is smoothing constant. This formulation ensures that higher-ranked candidates in either modality contribute more to the final score, while still incorporating signals from both modalities. Finally, the top-ğ‘˜ candidates Iğ‘˜ are selected and passed to the next stage for fine filtering, balancing retrieval accuracy with computational cost. If ğ‘˜ = ğ‘ , then all candidates are selected and proceed to the next step. Notably, the process of producing and aggregating multiple scores is also inspired by human cognitive mechanisms. When humans search for target images, cross-modal information is interwoven in the mind, collectively forming unified body of evidence that supports decision-making."
        },
        {
            "title": "3.5 Fine Filtering",
            "content": "The previously discussed efforts for CIR primarily focus on approximating ğ¼ğ‘– through similarity-based scoring. However, in real-world scenarios, ensuring retrieval accuracy often requires factual verification with semanic reasoning. To address this, we introduce the question agent Ağ‘ (Line 11). This agent operates on three types of instructions: ğ‘€ğ‘¡ , which represents modifications derived from Ağ‘– ğ‘¡ ; ğ‘€ğ‘£, which represents visual attribute indicators; and ğ‘‡ğ‘š, which encodes atomic user-specified instructions. The agent Ağ‘ generates set of verification questions ğ‘„ with corresponding answers ğ´ by transforming each atomic instruction in ğ‘‡ğ‘š into declarative statement, while using ğ‘€ğ‘¡ as supporting context and ğ‘€ğ‘£ as factual grounding. When ğ‘‡ğ‘š alone WWW 26, April 1317, 2026, Dubai, United Arab Emirates Yang et al. Table 2: Performance comparison on FashionIQ validation set. The best results are in bold, and the second best are underlined. Backbone Method Venue Training-free 2 3 / - - C 4 1 / - - C ECCV 2022 ICCV 2023 ICCV 2023 TPAMI 2025 TPAMI 2025 ICLR 2024 SIGIR 2024 PALAVRA [9] SEARLE [4] SEARLE-OTI [4] iSEARLE [1] iSEARLE-OTI [1] CIReVL [22] LDRE [51] ImageScope [31] WWW 2025 XR(Ours) Pic2Word [35] SEARLE [4] SEARLE-OTI [4] iSEARLE [1] iSEARLE-OTI [1] LinCIR [12] FTI4CIR [27] CIReVL [22] LDRE [51] ImageScope [31] WWW 2025 XR(Ours) Proposed CVPR 2023 ICCV 2023 ICCV 2023 TPAMI 2025 TPAMI 2025 CVPR 2024 SIGIR 2024 ICLR 2024 SIGIR 2024 Proposed ğ‘ ğ‘¡ and is insufficient to define clear question, ğ‘€ğ‘¡ provides additional context. All questions are formulated in True/False format, with examples provided in Figure 2. In Lines 13 and 14, ğ‘ ğ‘£ are two question-based scoring agents that use ğ‘„ to evaluate each candidate image ğ‘ {1, . . . , ğ‘˜ } based on different modality information, namely ğ¶ğ‘ and ğ¼ğ‘. We consider fact to be true if the candidate is able to pass the verification check under both modalities. If the answer to question is correct, the agent assigns score of +1; otherwise, the score is 0. The results are denoted as ğ‘ ğ‘¡ ğ‘. We consider this design to be effective because it provides discrete, verifiable signals that emphasize factual consistency across modalities. ğ‘ and ğ‘  ğ‘£ ğ‘ = ğ‘† ğ‘£ ğ‘, and ğ‘†ğ‘¡ In Lines 15 and 16, we define the cross-modal re-ranking procedure for the top-ğ‘˜ previously selected candidates. For notational simplicity, we omit the index ğ‘˜ here. First, the question-based scores for each candidate are summed as ğ‘†ğ‘¡ ğ‘ ğ‘†ğ‘¡ ğ‘, ğ‘  ğ‘£ ğ‘ ğ‘† ğ‘£ ğ‘ = ğ‘˜ . This sum is then multiplied by normalized weighted combination of the similarity-based scores ğ‘†ğ‘¡ and ğ‘† ğ‘£ from Lines 8 and 9, with weights ğœ† and 1 ğœ†, respectively. It is worth noting that only the similarity-based scores ğ‘†ğ‘¡ and ğ‘† ğ‘£ of the top-ğ‘˜ candidates are used at this stage. The result, denoted ğ‘†ğ‘˜ , represents the refined scores used for re-ranking in Line 16. Finally, the re-ranked set is returned as the final output. ğ‘, where ğ‘ ğ‘¡ ğ‘ + ğ‘† ğ‘£ This design is motivated by the complementary strengths of the two scoring mechanisms. Similarity-based scores (ğ‘†ğ‘¡ and ğ‘† ğ‘£) capture soft alignment between the candidate images and the multimodal query, but they may overlook fine-grained factual details. In contrast, question-based scores (ğ‘†ğ‘¡ ğ‘) enforce explicit verification of atomic modifications and provide binary, interpretable signals. By combining the two, the re-ranking step integrates the broad cross-modal coverage of similarity-based scoring with the precision of question-based verification, thereby improving robustness ğ‘ and ğ‘† ğ‘£ Shirt Dress Toptee Avg. R@10 R@50 R@10 R@50 R@10 R@50 R@10 R@50 37.25 21.49 42.53 24.44 42.34 25.37 44.80 25.81 44.79 27.09 49.35 28.36 45.63 27.38 38.03 24.29 57.10 36.06 43.73 26.20 46.23 26.89 47.91 30.37 48.96 28.75 49.54 31.80 46.48 29.10 50.88 31.35 48.57 29.49 50.54 31.04 41.22 27.82 57.30 38.91 19.76 22.89 22.45 24.40 25.06 28.29 24.81 22.42 36.66 24.70 25.56 27.61 27.52 29.24 26.28 29.42 28.55 28.51 25.54 37.18 38.76 46.46 45.79 48.70 48.75 53.85 48.78 41.41 64.56 47.40 49.97 51.76 52.68 53.29 50.18 54.21 53.65 53.64 44.42 62. 37.05 41.61 41.32 43.52 43.42 47.84 46.27 37.49 54.66 43.60 45.58 47.49 47.84 50.20 46.81 50.59 47.40 51.22 41.76 56.82 17.25 18.54 17.85 20.92 21.27 25.29 19.97 18.00 30.94 20.00 20.48 21.57 22.51 24.19 20.92 24.49 24.79 22.93 20.18 28.71 35.94 39.51 39.91 42.19 42.19 46.36 41.84 35.20 52.06 40.20 43.13 44.47 46.36 45.12 42.44 47.84 44.76 46.76 37.48 52.50 20.55 25.70 24.12 26.47 26.82 31.21 27.07 24.99 42.99 27.90 29.32 30.90 31.31 31.72 28.81 32.43 31.36 31.57 28.61 43.91 and ensuring that the final retrieved ordered set more faithfully reflects the intended modifications through semantic reasoning. In fact, the use of XR is flexible. So far, we have discussed how similarity-based scoring and question-based scoring collaborate through ranking (selection) and re-ranking (re-selection). Moreover, when ğ‘˜ = ğ‘˜, the two scoring processes act as rankingselection and re-ranking. When ğ‘˜ = ğ‘ , the two processes operate jointly, performing ranking and selection directly. It is important to note that these three configurations correspond to increasing computational cost. In summary, we propose XR, training-free cross-modal multiagent framework for CIR. The framework highlights the benefits of agentic AI, including minimal human intervention and autonomous collaboration among multiple agents that mimic human cognitive processes. By enabling retrieval that is both robust and adaptive across modalities, XR points toward promising directions for largescale information access scenarios that are becoming increasingly central in web-driven environments."
        },
        {
            "title": "4 Experiments\n4.1 Experiment Setup\nBenchmark. We evaluate XR on three representative CIR bench-\nmarks (Table 4): CIRR [30], the first natural-image CIR dataset\nwith subset retrieval for fine-grained candidate discrimination;\nCIRCO [4], a large-scale benchmark with multiple ground truths\nto reduce false negatives; and FashionIQ [45], a fashion-domain\ndataset with three categories (dress, shirt, toptee).\nMetrics. Following the original protocols, we use Recall@k (R@k)\nfor CIRR and FashionIQ to capture retrieval accuracy, and mean\naverage precision (mAP@k) for CIRCO to account for multiple\nvalid ground truths.",
            "content": "XR: Cross-Modal Agents for Composed Image Retrieval WWW 26, April 1317, 2026, Dubai, United Arab Emirates Table 3: Ablation studies on CLIP-ViT-B/32 with InternVL3-8B. Similarity-based Question-based FashionIQ CIRCO CIRR CIRRğ‘ ğ‘¢ğ‘ğ‘ ğ‘’ğ‘¡ Textual Visual Textual Visual R@10 R@50 mAP@5 mAP@10 mAP@25 R@1 R@5 R@ R@1 R@2 R@3 14.78 19.36 32.48 32.84 23.93 36.01 34.78 36.62 36.66 29.60 37.65 54.55 55.37 41.72 56.57 55.63 56.84 57.10 2.65 11.98 15.18 16.73 17.22 24.12 24.87 26.21 27. 3.25 13.40 16.02 17.69 17.7 24.84 25.51 27.01 28.33 4.14 14.11 17.54 19.29 19.05 26.53 27.34 28.87 30.28 11.71 18.12 27.02 27.33 30.53 40.24 36.60 41.34 43.06 35.06 51.16 61.04 63.57 58.96 71.57 65.69 73.06 73.86 48.94 65.11 74.05 76.36 69.78 81.75 76.72 82.43 83.15 32.77 59.76 64.53 66.39 68.12 75.42 72.72 76.45 77. 56.89 79.88 83.18 84.00 83.88 89.68 86.41 89.95 90.27 74.96 90.00 91.93 92.89 91.64 93.77 93.64 95.13 95.21 Table 4: Benchmark details. Dataset Split Type # Queries # Images CIRR [30] CIRCO [4] FashionIQ-Shirt [45] FashionIQ-Dress [45] FashionIQ-Toptee [45] Test Test Val. Val. Val. CIR CIR CIR CIR CIR 4,148 800 2,038 2,017 1,961 2,316 123,403 6,346 3,817 5, Baselines. We compare XR against nine representative CIR baselines. Since XR is training-free, we primarily focus on zero-shot methods for fair comparison, while also reporting strong trainingbased models for completeness: Training-based: PALAVRA [9], Pic2Word [35], SEARLE [4], iSEARLE [1], LinCIR [12], and FTI4CIR [27]. Training-free: CIReVL [22], LDRE [51], and ImageScope [31]. Implementation Details. We use CLIP-ViT-L/14 and CLIP-ViTB/32 [19] as dual-encoder backbones for similarity agents, and InternVL3-8B [57] for imagination and question-based verification. We set the temperature = 0 and top-ğ‘ = 1 for deterministic outputs. The fusion weight is ğœ† = 0.15 to balance textimage similarity. We set ğ‘˜ = 100 candidates for fine filtering to allow question-based scoring to take effect. If ğ‘˜ were set equal to ğ‘˜, as in Recall@k, the benefit of re-ranking would be masked since Recall@k is orderinsensitive. In contrast, fine filtering both re-ranks and prunes candidates, making ğ‘˜ = 100 balanced choice. All experiments are conducted on single NVIDIA H800-80G GPU with FP16 precision."
        },
        {
            "title": "4.2 Main Result.\nTable 1 and Table 2 illustrate that XR consistently outperforms both\ntraining-free and training-based CIR methods. On FashionIQ, XR\nachieves consistent gains across all three categories. With CLIP-\nViT-B/32, it reaches 36.66% R@10 and 57.10% R@50 on average,\nsurpassing CIReVL by over 8 points in R@10. These gains hold\nacross shirts, dresses, and tops, indicating that the method gener-\nalizes across diverse attribute-level edits rather than overfitting\nto a single category. On CIRCO, which introduces large distrac-\ntor sets and multiple ground truths, XR attains 30.95% mAP@50,\nover 7 points higher than the best baseline. This shows that multi-\nagent reasoning maintains robustness in noisy, large-scale retrieval\nwhere static similarity models often collapse. On CIRR, XR achieves\n83.15% R@10 and 95.21% R@3 in the fine-grained subset retrieval",
            "content": "task (CIRRsubset ), surpassing training-free and training-based baselines. These results show that imagination and verification act as complementary safeguards against error propagation, ensuring faithful alignment to user intent in fine-grained scenarios. Overall, XR demonstrates consistent advantages across domain-specific (FashionIQ), distractor-heavy (CIRCO), and fine-grained (CIRR) benchmarks, pointing to strong cross-benchmark generalization."
        },
        {
            "title": "4.4 Discussion.\nRRF ğ‘§ vs. summation. Figure 3(a) compares reciprocal rank fusion\n(RRF) with varying ğ‘§ against direct score summation. Direct score\nsummation performs worst, showing that naive averaging fails\nto normalize heterogeneous modalities. RRF instead focuses on\nranks, which makes aggregation more robust to noisy candidates.\nAt ğ‘§ = 60, it strikes the best balance, raising CIRCO mAP@25 to\n30.28% and CIRR R@10 to 83.15%.\nGenerality of MLLMs. Figure 3(b) compares different multimodal\nbackbones. Medium-scale models such as InternVL3-8B and Qwen2.5VL-\n7B achieve the best trade-off, combining strong grounding ability\nwith efficiency (e.g., 57.10% R@50 on FashionIQ and 95.21% R@3\non CIRRğ‘ ğ‘¢ğ‘ğ‘ ğ‘’ğ‘¡ ). Smaller backbones lack grounding ability, while ex-\ntremely large ones bring only marginal gains at a much higher cost.",
            "content": "WWW 26, April 1317, 2026, Dubai, United Arab Emirates Yang et al. Figure 3: Parameter analysis of XR. (a) Effect of RRF with different ğ‘§ values. (b) Comparison across multimodal backbones. (c) Impact of the number of verification questions. (d) Influence of candidate pool size ğ‘˜ . Figure 4: Effect of ğœ† on textimage fusion: best at ğœ†=0.15; extremes degrade by losing cross-modal cues. This suggests that XR benefits most from medium-scale MLLMs, which balance expressiveness and efficiency and indicate that the framework scales smoothly across backbones. Number of questions. Figure 3(c) shows that single verification question is insufficient, while three yield consistent gains across benchmarks (e.g., CIRR R@10 = 82.84%). Using more than three slightly reduces performance, as redundant checks add overhead without new information. These results indicate that small, diverse set of questions suffices for robust factual alignment, consistent with the principle that each agent contributes distinct value. Top-ğ‘˜ analysis. Figure 3(d) illustrates the impact of the coarse filtering pool size. Small ğ‘˜ limits recall, while larger pools improve coverage by ensuring candidate diversity. Gains plateau beyond ğ‘˜ = 100, with only marginal improvements up to ğ‘˜ = 500. This indicates that moderately large pools provide the best efficiencyeffectiveness balance. Effect of ğœ†. Figure 4 examines the balance between textual and visual similarity signals. Relying solely on either modality (ğœ† = 0 or ğœ† = 1) significantly degrades performance, as it ignores complementary cues. The best results occur at ğœ† = 0.15, suggesting that cross-modal fusion is most effective when neither modality dominates, validating the principle of balanced agent collaboration. Latency analysis. Figure 5 shows average and total latency under different candidate pool sizes (ğ‘˜ ). As expected, larger ğ‘˜ increases cost as more candidates enter fine filtering. Per-query latency grows nearly linearly, with CIRCO highest due to its many distractors. Total latency is dominated by CIRR because of its dataset scale, while (a) Average latency per query. (b) Total latency. Figure 5: Latency of XR under different top-ğ‘˜ : larger pools increase cost nearly linearly, but ğ‘˜ 100 balances coverage and overhead. FashionIQ remains relatively lightweight. Overall, moderately large ğ‘˜ (around 100) offers the best trade-off, balancing diversity for robust retrieval against computational overhead. These results highlight that the strength of XR comes from orchestrating multiple agents rather than relying on any single component. By uniting semantic alignment with factual verification, cross-modal reasoning refines retrieval and overcomes the inherent limits of unimodal pipelines. More broadly, this shows that cross-modal multi-agent reasoning is not only effective for CIR but establishes general paradigm for multimodal retrieval and reasoning."
        },
        {
            "title": "5 Conclusion\nWe presented XR, a training-free cross-modal multi-agent frame-\nwork for composed image retrieval. Unlike unimodal pipelines, XR\nintegrates imagination, coarse filtering, and fine filtering through\nsimilarity- and question-based agents, progressively refining results\nvia semantic alignment and factual verification. Experiments on\nFashionIQ, CIRCO, and CIRR show consistent improvements over\nboth training-free and training-based baselines, particularly in fine-\ngrained and distractor-rich scenarios. Ablation analyses confirm\nthat while each agent contributes independently, their coordination\nyields more stable and accurate retrieval. Overall, our findings un-\nderscore that cross-modal reasoning is not only advantageous but\noften essential for aligning retrieval with user intent. Looking ahead,\nwe envision XR as a foundation for retrieval-augmented reasoning,\nwhere agentic systems actively interpret, verify, and adapt across\nmodalities to achieve reliable and human-aligned intelligence.",
            "content": "XR: Cross-Modal Agents for Composed Image Retrieval WWW 26, April 1317, 2026, Dubai, United Arab Emirates References [1] Lorenzo Agnolucci, Alberto Baldrati, Marco Bertini, and A. Bimbo. 2024. iSEARLE: Improving Textual Inversion for Zero-Shot Composed Image Retrieval. IEEE Transactions on Pattern Analysis and Machine Intelligence 47 (2024), 1080110817. https://api.semanticscholar.org/CorpusID:269604752 [2] Lakshya Agrawal, Shangyin Tan, Dilara Soylu, Noah Ziems, Rishi Khare, Krista Opsahl-Ong, Arnav Singhvi, Herumb Shandilya, Michael Ryan, Meng Jiang, Christopher Potts, Koushik Sen, Alexandros G. Dimakis, Ion Stoica, Dan Klein, Matei A. Zaharia, and O. Khattab. 2025. GEPA: Reflective Prompt Evolution Can Outperform Reinforcement Learning. ArXiv abs/2507.19457 (2025). https: //api.semanticscholar.org/CorpusID:280046245 [3] Yang bai, Xinxing Xu, Yong Liu, Salman Khan, Fahad Khan, Wangmeng Zuo, Rick Siow Mong Goh, and Chun-Mei Feng. 2024. Sentence-level Prompts Benefit Composed Image Retrieval. In The Twelfth International Conference on Learning Representations. https://openreview.net/forum?id=m3ch3kJL7q [4] Alberto Baldrati, Lorenzo Agnolucci, Marco Bertini, and A. Bimbo. 2023. ZeroShot Composed Image Retrieval with Textual Inversion. 2023 IEEE/CVF International Conference on Computer Vision (ICCV) (2023), 1529215301. https: //api.semanticscholar.org/CorpusID: [5] Tong Bao, Che Liu, Derong Xu, Zhi Zheng, and Tong Xu. 2025. MLLM-I2W: Harnessing Multimodal Large Language Model for Zero-Shot Composed Image Retrieval. In International Conference on Computational Linguistics. https://api. semanticscholar.org/CorpusID:275821154 [6] Ben Chen, Linbo Jin, Xinxin Wang, Dehong Gao, Wen Jiang, and Wei Ning. 2023. Unified Vision-Language Representation Modeling for E-Commerce Same-style Products Retrieval. Companion Proceedings of the ACM Web Conference 2023 (2023). https://api.semanticscholar.org/CorpusID:256808380 [7] Yanbei Chen, Shaogang Gong, and Loris Bazzani. 2020. Image Search With Text Feedback by Visiolinguistic Attention Learning. 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR) (2020), 29983008. https: //api.semanticscholar.org/CorpusID:219401805 [8] Zhangtao Cheng, Yuhao Ma, Jian Lang, Kunpeng Zhang, Ting Zhong, Yong Wang, and Fan Zhou. 2025. Generative Thinking, Corrective Action: User-Friendly Composed Image Retrieval via Automatic Multi-Agent Collaboration. Proceedings of the 31st ACM SIGKDD Conference on Knowledge Discovery and Data Mining V.2 (2025). https://api.semanticscholar.org/CorpusID:280448098 [9] Niv Cohen, Rinon Gal, Eli A. Meirom, Gal Chechik, and Yuval Atzmon. 2022. \"This is my unicorn, Fluffy\": Personalizing frozen vision-language representations. ArXiv abs/2204.01694 (2022). https://api.semanticscholar.org/CorpusID: [10] Zane Durante, Qiuyuan Huang, Naoki Wake, Ran Gong, Jae Sung Park, Bidipta Sarkar, Rohan Taori, Yusuke Noda, Demetri Terzopoulos, Yejin Choi, Katsushi Ikeuchi, Hoi Vo, Fei-Fei Li, and Jianfeng Gao. 2024. Agent AI: Surveying the Horizons of Multimodal Interaction. ArXiv abs/2401.03568 (2024). https://api. semanticscholar.org/CorpusID:266844635 [11] Darren Edge, Ha Trinh, Newman Cheng, Joshua Bradley, Alex Chao, Apurva N. Mody, Steven Truitt, and Jonathan Larson. 2024. From Local to Global: Graph RAG Approach to Query-Focused Summarization. ArXiv abs/2404.16130 (2024). https://api.semanticscholar.org/CorpusID:269363075 [12] Geonmo Gu, Sanghyuk Chun, Wonjae Kim, Yoohoon Kang, and Sangdoo Yun. 2023. Language-only Efficient Training of Zero-shot Composed Image Retrieval. 2024 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR) (2023), 1322513234. https://api.semanticscholar.org/CorpusID:265609308 [13] Jiawei Gu, Ziting Xian, Yuanzhen Xie, Ye Liu, Enjie Liu, Ruichao Zhong, Mochi Gao, Yunzhi Tan, Bo Hu, and Zang Li. 2025. Toward Structured Knowledge Reasoning: Contrastive Retrieval-Augmented Generation on Experience. In Annual Meeting of the Association for Computational Linguistics. https: //api.semanticscholar.org/CorpusID:279075943 [14] Venkat N. Gudivada and Vijay V. Raghavan. 1995. Content-Based Image Retrieval Systems - Guest Editors Introduction. Computer 28 (1995), 1822. https://api. semanticscholar.org/CorpusID:206402728 [15] Daya Guo, Dejian Yang, Haowei Zhang, Junxiao Song, Ruoyu Zhang, Runxin Xu, Qihao Zhu, Shirong Ma, Peiyi Wang, Xiao Bi, et al. 2025. Deepseek-r1: Incentivizing reasoning capability in llms via reinforcement learning. arXiv preprint arXiv:2501.12948 (2025). [16] Demis Hassabis. 2023. Introducing Gemini: our largest and most capable AI model. Google Blog (2023). Accessed 2025-10-03. [17] M. Hosseinzadeh and Yang Wang. 2020. Composed Query Image Retrieval Using Locally Bounded Features. 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR) (2020), 35933602. https://api.semanticscholar.org/ CorpusID:219963281 [18] Chuong Huynh, Jinyu Yang, Ashish Tawari, Mubarak Shah, Son Tran, Raffay Hamid, Trishul M. Chilimbi, and Abhinav Shrivastava. 2025. CoLLM: Large Language Model for Composed Image Retrieval. 2025 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR) (2025), 39944004. https: //api.semanticscholar.org/CorpusID:277314021 [19] Chao Jia, Yinfei Yang, Ye Xia, Yi-Ting Chen, Zarana Parekh, Hieu Pham, Quoc V. Le, Yun-Hsuan Sung, Zhen Li, and Tom Duerig. 2021. Scaling Up Visual and Vision-Language Representation Learning With Noisy Text Supervision. In International Conference on Machine Learning. https://api.semanticscholar.org/ CorpusID:231879586 [20] Bowen Jiang, Yangxinyu Xie, Xiaomeng Wang, Weijie J. Su, Camillo Jose Taylor, and Tanwi Mallick. 2024. Multi-Modal and Multi-Agent Systems Meet Rationality: Survey. ArXiv abs/2406.00252 (2024). https://api.semanticscholar.org/CorpusID: 278935961 [21] Yucheng Jiang, Yijia Shao, Dekun Ma, Sina J. Semnani, and Monica S. Lam. 2024. Into the Unknown Unknowns: Engaged Human Learning through Participation in Language Model Agent Conversations. In Conference on Empirical Methods in Natural Language Processing. https://api.semanticscholar.org/CorpusID:271963301 [22] Shyamgopal Karthik, Karsten Roth, Massimiliano Mancini, and Zeynep Akata. 2024. Vision-by-Language for Training-Free Compositional Image Retrieval. In International Conference on Learning Representations. [23] Wei Li, Hehe Fan, Yongkang Wong, Yi Yang, and Mohan S. Kankanhalli. 2024. Improving Context Understanding in Multimodal Large Language Models via Multimodal Composition Learning. In International Conference on Machine Learning. https://api.semanticscholar.org/CorpusID:272330468 [24] You Li, Fan Ma, and Yi Yang. 2024. Imagine and Seek: Improving Composed Image Retrieval with an Imagined Proxy. 2025 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR) (2024), 39843993. https://api.semanticscholar. org/CorpusID:274281272 [25] Zhe Li, Lei Zhang, Kun Zhang, Weidong Chen, Yongdong Zhang, and Zhendong Mao. 2025. Rethinking Pseudo Word Learning in Zero-Shot Composed Image Retrieval: From an Object-Aware Perspective. Proceedings of the 48th International ACM SIGIR Conference on Research and Development in Information Retrieval (2025). https://api.semanticscholar.org/CorpusID:280069892 [26] Jintao Liang, Gang Su, Huifeng Lin, You Wu, Rui Zhao, and Ziyue Li. 2025. Reasoning RAG via System 1 or System 2: Survey on Reasoning Agentic RetrievalAugmented Generation for Industry Challenges. arXiv:2506.10408 [cs.AI] https://arxiv.org/abs/2506.10408 [27] Haoqiang Lin, Haokun Wen, Xuemeng Song, Meng Liu, Yupeng Hu, and Fine-grained Textual Inversion Network for Zero-Shot Liqiang Nie. 2024. Composed Image Retrieval. Proceedings of the 47th International ACM SIGIR Conference on Research and Development in Information Retrieval (2024). https://api.semanticscholar.org/CorpusID:271114410 [28] Liping Liu, Chunhong Zhang, Likang Wu, Chuang Zhao, Zheng Hu, Ming He, and Jianpin Fan. 2025. Instruct-of-Reflection: Enhancing Large Language Models Iterative Reflection Capabilities via Dynamic-Meta Instruction. ArXiv abs/2503.00902 (2025). https://api.semanticscholar.org/CorpusID: [29] Ying Liu, Dengsheng Zhang, Guojun Lu, and Wei-Ying Ma. 2007. survey of content-based image retrieval with high-level semantics. Pattern Recognit. 40 (2007), 262282. https://api.semanticscholar.org/CorpusID:9160719 [30] Zheyuan Liu, Cristian Rodriguez-Opazo, Damien Teney, and Stephen Gould. 2021. Image Retrieval on Real-life Images with Pre-trained Vision-and-Language Models. 2021 IEEE/CVF International Conference on Computer Vision (ICCV) (2021), 21052114. https://api.semanticscholar.org/CorpusID:236956879 [31] Pengfei Luo, Jingbo Zhou, Tong Xu, Yuan Xia, Linli Xu, and Enhong Chen. 2025. ImageScope: Unifying Language-Guided Image Retrieval via Large Multimodal Model Collective Reasoning. Proceedings of the ACM on Web Conference 2025 (2025). https://api.semanticscholar.org/CorpusID:276961009 [32] Aman Madaan, Niket Tandon, Prakhar Gupta, Skyler Hallinan, Luyu Gao, Sarah Wiegreffe, Uri Alon, Nouha Dziri, Shrimai Prabhumoye, Yiming Yang, Sean Welleck, Bodhisattwa Prasad Majumder, Shashank Gupta, Amir Yazdanbakhsh, and Peter Clark. 2023. Self-Refine: Iterative Refinement with Self-Feedback. ArXiv abs/2303.17651 (2023). https://api.semanticscholar.org/CorpusID:257900871 [33] OpenAI. 2025. Introducing GPT-5. Accessed 2025-10-03. [34] Matthew Renze and Erhan Guven. 2024. The benefits of concise chain of thought on problem-solving in large language models. In 2024 2nd International Conference on Foundation and Large Language Models (FLLM). IEEE, 476483. [35] Kuniaki Saito, Kihyuk Sohn, Xiang Zhang, Chun-Liang Li, Chen-Yu Lee, Kate Saenko, and Tomas Pfister. 2023. Pic2word: Mapping pictures to words for zeroshot composed image retrieval. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. 1930519314. [36] Yijia Shao, Yucheng Jiang, Theodore Kanell, Peter Xu, Omar Khattab, and Monica Lam. 2024. Assisting in writing wikipedia-like articles from scratch with large language models. In Proceedings of the 2024 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (Volume 1: Long Papers). 62526278. [37] Noah Shinn, Federico Cassano, Ashwin Gopinath, Karthik Narasimhan, and Shunyu Yao. 2023. Reflexion: Language agents with verbal reinforcement learning. Advances in Neural Information Processing Systems 36 (2023), 86348652. [38] Aditi Singh, Abul Ehtesham, Saket Kumar, and Tala Talaei Khoei. 2025. Agentic retrieval-augmented generation: survey on agentic rag. arXiv preprint arXiv:2501.09136 (2025). WWW 26, April 1317, 2026, Dubai, United Arab Emirates Yang et al. [39] Xuemeng Song, Haoqiang Lin, Haokun Wen, Bohan Hou, Mingzhu Xu, and Liqiang Nie. 2025. comprehensive survey on composed image retrieval. ACM Transactions on Information Systems 44, 1 (2025), 154. [40] Yuanmin Tang, Jue Zhang, Xiaoting Qin, Jing Yu, Gaopeng Gou, Gang Xiong, Qingwei Lin, Saravan Rajmohan, Dongmei Zhang, and Qi Wu. 2025. Reasonbefore-retrieve: One-stage reflective chain-of-thoughts for training-free zeroshot composed image retrieval. In Proceedings of the Computer Vision and Pattern Recognition Conference. 1440014410. [41] Rong-Cheng Tu, Wenhao Sun, Hanzhe You, Yingjie Wang, Jiaxing Huang, Li Shen, and Dacheng Tao. 2025. Multimodal Reasoning Agent for Zero-Shot Composed Image Retrieval. arXiv preprint arXiv:2505.19952 (2025). [42] Nam Vo, Lu Jiang, Chen Sun, Kevin Murphy, Li-Jia Li, Li Fei-Fei, and James Hays. 2019. Composing text and image for image retrieval-an empirical odyssey. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition. 64396448. [43] Ji Wan, Dayong Wang, Steven Chu Hong Hoi, Pengcheng Wu, Jianke Zhu, Yongdong Zhang, and Jintao Li. 2014. Deep learning for content-based image retrieval: comprehensive study. In Proceedings of the 22nd ACM international conference on Multimedia. 157166. [44] Peng Wang, Shuai Bai, Sinan Tan, Shijie Wang, Zhihao Fan, Jinze Bai, Keqin Chen, Xuejing Liu, Jialin Wang, Wenbin Ge, et al. 2024. Qwen2-vl: Enhancing vision-language models perception of the world at any resolution. arXiv preprint arXiv:2409.12191 (2024). [45] Hui Wu, Yupeng Gao, Xiaoxiao Guo, Ziad Al-Halah, Steven Rennie, Kristen Grauman, and Rogerio Feris. 2021. Fashion iq: new dataset towards retrieving images by natural language feedback. In Proceedings of the IEEE/CVF Conference on computer vision and pattern recognition. 1130711317. [46] Xiaohui Xie, Yiqun Liu, Maarten De Rijke, Jiyin He, Min Zhang, and Shaoping Ma. 2018. Why people search for images using web search engines. In Proceedings of the eleventh ACM international conference on web search and data mining. 655663. [47] Eric Xing, Pranavi Kolouju, Robert Pless, Abby Stylianou, and Nathan Jacobs. 2025. ConText-CIR: Learning from Concepts in Text for Composed Image Retrieval. In Proceedings of the Computer Vision and Pattern Recognition Conference. 19638 19648. [48] Zhongyu Yang, Jun Chen, Dannong Xu, Junjie Fei, Xiaoqian Shen, Liangbing Zhao, Chun-Mei Feng, and Mohamed Elhoseiny. 2025. WikiAutoGen: Towards Multi-Modal Wikipedia-Style Article Generation. arXiv preprint arXiv:2503.19065 (2025). [49] Zhongyu Yang, Junhao Song, Siyang Song, Wei Pang, and Yingfang Yuan. 2025. MERMAID: Multi-perspective Self-reflective Agents with Generative Augmentation for Emotion Recognition. In Proceedings of the 2025 Conference on Empirical Methods in Natural Language Processing, Christos Christodoulopoulos, Tanmoy Chakraborty, Carolyn Rose, and Violet Peng (Eds.). Association for Computational Linguistics, Suzhou, China, 2463924655. doi:10.18653/v1/2025.emnlp-main.1252 [50] Zuhao Yang, Sudong Wang, Kaichen Zhang, Keming Wu, Sicong Leng, Yifan Zhang, Bo Li, Chengwei Qin, Shijian Lu, Xingxuan Li, and Lidong Bing. 2025. LongVT: Incentivizing \"Thinking with Long Videos\" via Native Tool Calling. arXiv:2511.20785 [cs.CV] https://arxiv.org/abs/2511.20785 [51] Zhenyu Yang, Dizhan Xue, Shengsheng Qian, Weiming Dong, and Changsheng Xu. 2024. Ldre: Llm-based divergent reasoning and ensemble for zero-shot composed image retrieval. In Proceedings of the 47th International ACM SIGIR conference on research and development in information retrieval. 8090. [52] Zhongyu Yang, Yingfang Yuan, Xuanming Jiang, Baoyi An, and Wei Pang. 2025. InEx: Hallucination Mitigation via Introspection and Cross-Modal Multi-Agent Collaboration. arXiv:2512.02981 [cs.CV] https://arxiv.org/abs/2512.02981 [53] Shunyu Yao, Jeffrey Zhao, Dian Yu, Nan Du, Izhak Shafran, Karthik Narasimhan, and Yuan Cao. 2022. React: Synergizing reasoning and acting in language models. In The eleventh international conference on learning representations. [54] Kun Zhang, Jingyu Li, Zhe Li, Jingjing Zhang, Fan Li, Yandong Liu, Rui Yan, Zihang Jiang, Nan Chen, Lei Zhang, et al. 2025. Composed multi-modal retrieval: survey of approaches and applications. arXiv preprint arXiv:2503.01334 (2025). [55] Kaichen Zhang, Keming Wu, Zuhao Yang, Bo Li, Kairui Hu, Bin Wang, Ziwei Liu, Xingxuan Li, and Lidong Bing. 2025. OpenMMReasoner: Pushing the Frontiers for Multimodal Reasoning with an Open and General Recipe. arXiv:2511.16334 [cs.AI] https://arxiv.org/abs/2511. [56] Xiaoyang Zheng, Zilong Wang, Sen Li, Ke Xu, Tao Zhuang, Qingwen Liu, and Xiaoyi Zeng. 2023. Make: Vision-language pre-training based product retrieval in taobao search. In Companion Proceedings of the ACM Web Conference 2023. 356360. [57] Jinguo Zhu, Weiyun Wang, Zhe Chen, Zhaoyang Liu, Shenglong Ye, Lixin Gu, Hao Tian, Yuchen Duan, Weijie Su, Jie Shao, et al. 2025. Internvl3: Exploring advanced training and test-time recipes for open-source multimodal models. arXiv preprint arXiv:2504.10479 (2025)."
        },
        {
            "title": "A Detailed Experiment Results",
            "content": "In this section, we provide the complete parameter analysis and ablation study results on the FashionIQ dataset. While the main paper reports the representative metrics (R@10 and R@50), here we include the full set of scores across different categories (shirts, dresses, and tops) and wider range of evaluation metrics. These results further demonstrate the consistent advantages of our crossmodal framework over single-modality baselines."
        },
        {
            "title": "B Statistical Significance Study",
            "content": "We conduct 10 independent runs with different random seeds and compare our method against baseline models under identical settings. To assess statistical significance, we perform paired one-sided ğ‘¡-tests and Wilcoxon signed-rank tests. The null hypothesis (H0) states that XR performs equally or worse than the baseline, while the alternative hypothesis (H1) states that our method performs better. As shown in Table B.1, XR achieves the highest mean score (57.16 0.07), compared to CIReVL (49.06 0.23), ImageScope (37.99 0.25), and the Raw baseline (29.62 0.11). All comparisons yield ğ‘-values well below the threshold ğ›¼ = 0.05 (e.g., ğ‘¡-test: 3.94 1017 against CIReVL), thereby allowing us to confidently reject H0 in favor of H1. These results confirm that XR consistently and significantly outperforms all baselines."
        },
        {
            "title": "C Experimental Code",
            "content": ". To promote transparency and ensure the reproducibility of our work, we will release all experimental code, datasets, and detailed tutorials necessary for replicating our experiments. Our goal is to make it straightforward for researchers and practitioners to reproduce our results, regardless of their technical background. Additionally, by providing comprehensive documentation and clear guidelines, we aim to facilitate the extension of our method to other models and architectures, enabling the broader research community to explore its potential applications and improvements. We believe that open and reproducible research is essential for advancing the field and fostering collaboration. Limitations and Future Work While XR achieves strong results on CIR benchmarks, several limitations remain. The framework is currently tailored to imagetext composition and has not yet been explored in settings involving richer modalities or temporal data. Its reliance on captions and verification questions generated by large models can also introduce subtle biases, which may affect consistency. Moreover, scaling to very large candidate pools requires further optimization of efficiency. Looking ahead, we envision cross-modal reasoning as the key avenue for progress. Extending XR beyond images and text to modalities such as video, audio, or interactive queries would open new opportunities for retrieval systems. Developing more lightweight and adaptive agents, together with diverse verification signals, could further enhance both robustness and scalability. These directions highlight the potential of cross-modal multi-agent reasoning as general paradigm for future multimodal search. XR: Cross-Modal Agents for Composed Image Retrieval WWW 26, April 1317, 2026, Dubai, United Arab Emirates Table A.1: Ablation studies on CLIP-ViT-B/32 with InternVL3-8B on FashionIQ. Similarity-based Question-based"
        },
        {
            "title": "Toptee",
            "content": "Avg. Textual Visual Textual Visual R@10 R@50 R@10 R@50 R@10 R@50 R@10 R@50 29.60 26.25 37.65 35.62 54.55 53.19 41.72 39.63 56.57 54.41 55.37 53.54 55.63 53.48 56.84 54.17 57.10 54.66 31.67 43.35 61.30 47.93 63.64 61.86 63.34 64.41 64.56 14.78 19.36 32.48 23.93 36.01 32.84 34.78 36.62 36.66 17.08 22.39 38.40 28.06 42.51 37.68 40.69 42.65 42.99 30.88 33.96 49.18 37.60 51.66 50.69 50.07 51.96 52. 13.83 16.21 26.13 19.84 29.90 27.37 28.90 31.04 30.94 13.44 19.48 32.92 23.87 35.62 33.45 34.74 36.16 36.06 Table A.2: Number of questions studies on CLIP-ViT-B/32 with InternVL3-8B on FashionIQ. Table A.6: Latency analysis (in seconds) across categories under different Top-ğ‘˜ . Question Num Shirt Dress Toptee Avg. R@10 R@50 R@10 R@50 R@10 R@50 R@10 R@50 32.14 34.79 35.82 35.87 36.11 51.67 54.51 54.66 54.76 54. 26.38 30.00 31.18 31.18 30.94 49.33 51.41 52.01 51.96 52.01 38.60 42.84 42.73 43.04 42.94 60.53 63.59 64.10 64.30 64.41 32.37 35.88 36.58 36.70 36.67 53.83 56.50 56.92 57.01 57. 1 2 3 4 5 Table A.3: Generality of MLLMs studies on CLIP-ViT-B/32 with InternVL3-8B on FashionIQ. MLLMs Shirt Dress Toptee Avg. R@10 R@50 R@10 R@50 R@10 R@50 R@10 R@50 InternVL3-1B InternVL3-8B InternVL3-14B MiniCPM-V-4.5-8B Qwen2.5VL-7B 18.66 36.06 38.86 33.52 31.79 29.99 54.66 56.82 54.40 52.89 17.90 30.94 33.47 30.51 31. 34.26 52.06 54.88 50.33 50.22 27.72 42.99 45.18 41.84 39.86 41.72 64.56 65.02 61.12 61.73 21.26 36.66 39.17 35.29 34.33 35.32 57.10 58.91 55.28 54.95 Table A.4: RRF ğ‘§ vs. summation studies on CLIP-ViT-B/32 with InternVL3-8B on FashionIQ. Method Shirt Dress Toptee Avg. R@10 R@50 R@10 R@50 R@10 R@50 R@10 R@ RRF=10 RRF=60 RRF=100 Sum 35.97 36.06 36.31 35.91 55.10 54.66 55.05 54.35 30.94 30.94 30.99 30.99 52.06 52.06 52.06 52.06 42.78 42.99 43.09 42. 63.90 64.56 64.00 64.00 36.56 36.66 36.80 36.63 57.02 57.10 57.03 56.80 Table A.5: Top-ğ‘˜ analysis on CLIP-ViT-B/32 with InternVL38B on FashionIQ. Top-ğ‘˜ Shirt Dress Toptee Avg. R@10 R@50 R@10 R@50 R@10 R@50 R@10 R@50 10 50 100 200 500 33.37 35.77 36.06 36.26 36. 53.73 53.29 54.66 55.64 56.87 26.87 29.77 30.94 31.68 31.63 48.98 51.30 52.06 53.54 54.88 39.16 42.38 42.99 43.65 43.86 62.26 61.96 64.56 65.32 65.63 33.13 35.97 36.66 37.20 37. 54.99 55.52 57.10 58.17 59.13 Top-ğ‘˜ CIRR CIRCO FashionIQğ‘†â„ğ‘–ğ‘Ÿğ‘¡ FashionIQğ·ğ‘Ÿğ‘’ğ‘ ğ‘  FashionIQğ‘‡ ğ‘œğ‘ğ‘¡ğ‘’ğ‘’ Avg. 10 50 100 200 500 945 1808 2464 5970 881 1203 1645 2511 5132 831 1467 2298 4037 9262 829 1611 2643 4781 11476 847 1585 2507 4483 10509 867 1535 2311 4356 12177 Table A.7: Average latency per query (in seconds) across categories under different Top-ğ‘˜ . Top-ğ‘˜ CIRR CIRCO FashionIQğ‘†â„ğ‘–ğ‘Ÿğ‘¡ FashionIQğ·ğ‘Ÿğ‘’ğ‘ ğ‘  FashionIQğ‘‡ ğ‘œğ‘ğ‘¡ğ‘’ğ‘’ Avg. 10 50 100 200 500 0.228 0.436 0.594 1.439 5.908 1.101 1.504 2.056 3.139 6. 0.408 0.720 1.128 1.981 4.545 0.411 0.799 1.310 2.370 5.690 0.432 0.608 1.278 2.286 5.359 0.516 0.653 1.273 2.243 5.983 Table B.1: Statistical comparison on FashionIQ benchmark with average Recall@50. StdDev denotes standard deviation. Paired one-sided ğ‘¡-test and Wilcoxon signed-rank test ğ‘values are reported (ğ›¼ = 5%). Method Mean (%) StdDev ğ‘¡-test ğ‘ Wilcoxon ğ‘ Raw CIReVL ImageScope XR (Ours) 29.62 49.06 37.99 57. 0.11 0.23 0.25 0.07 3.39 1026 3.94 1017 6.82 1021 4.88 104 Ethical Considerations Reliability and Transparency. XR enhances retrieval reliability by coordinating imagination, similarity, and verification, reducing semantic drift and promoting more trustworthy multimodal systems. Its modular design decomposes decisions into interpretable stages, enabling auditing and analysis of system behavior. Responsible Data Use. All experiments are conducted on publicly available datasets with proper licenses, ensuring compliance with ethical data standards."
        },
        {
            "title": "F Case Studies",
            "content": "Beyond aggregate metrics, we present case studies on CIRR, FashionIQ, and CIRCO to illustrate how XR behaves on concrete queries. These examples highlight complementary aspects of the framework: WWW 26, April 1317, 2026, Dubai, United Arab Emirates Yang et al. Figure F.1: Case study on CIRR. XR correctly grounds complex scene edits (e.g., bus orientation, reflective jackets) through factual verification. Target image is marked with the green box. Figure F.2: Case study on FashionIQ. XR captures subtle attribute edits (e.g., tone, lettering) and validates them via text-based questioning. Target image is marked with the green box. Figure F.3: Case study on CIRCO. XR remains robust under distractor-heavy settings by verifying entity-level edits (e.g., food type, clothing). Target image is marked with the green box. on CIRR, it grounds complex scene edits through factual verification; on FashionIQ, it captures subtle attribute modifications such as color or lettering; and on CIRCO, it remains robust under distractorheavy settings where static similarity often fails. Together, these cases not only showcase the strengths of multi-agent reasoning but also reveal remaining challenges, offering qualitative evidence that complements our quantitative findings."
        }
    ],
    "affiliations": [
        "BCML, Heriot-Watt University Edinburgh, UK"
    ]
}