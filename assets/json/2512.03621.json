{
    "paper_title": "ReCamDriving: LiDAR-Free Camera-Controlled Novel Trajectory Video Generation",
    "authors": [
        "Yaokun Li",
        "Shuaixian Wang",
        "Mantang Guo",
        "Jiehui Huang",
        "Taojun Ding",
        "Mu Hu",
        "Kaixuan Wang",
        "Shaojie Shen",
        "Guang Tan"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "We propose ReCamDriving, a purely vision-based, camera-controlled novel-trajectory video generation framework. While repair-based methods fail to restore complex artifacts and LiDAR-based approaches rely on sparse and incomplete cues, ReCamDriving leverages dense and scene-complete 3DGS renderings for explicit geometric guidance, achieving precise camera-controllable generation. To mitigate overfitting to restoration behaviors when conditioned on 3DGS renderings, ReCamDriving adopts a two-stage training paradigm: the first stage uses camera poses for coarse control, while the second stage incorporates 3DGS renderings for fine-grained viewpoint and geometric guidance. Furthermore, we present a 3DGS-based cross-trajectory data curation strategy to eliminate the train-test gap in camera transformation patterns, enabling scalable multi-trajectory supervision from monocular videos. Based on this strategy, we construct the ParaDrive dataset, containing over 110K parallel-trajectory video pairs. Extensive experiments demonstrate that ReCamDriving achieves state-of-the-art camera controllability and structural consistency."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 3 ] . [ 1 1 2 6 3 0 . 2 1 5 2 : r ReCamDriving: LiDAR-Free Camera-Controlled Novel Trajectory Video Generation Yaokun Li1 Shuaixian Wang1,3 Mantang Guo2 Jiehui Huang4 Taojun Ding2 Mu Hu4 Kaixuan Wang2 2ZYT 1Sun Yat-sen University Shaojie Shen4 Guang Tan1* 3Shenzhen Polytechnic University 4The Hong Kong University of Science and Technology Figure 1. Comparison of novel-trajectory generation. Repair-based methods (e.g., Difix3D+ [53]) suffer from severe artifacts under novel viewpoints, while LiDAR-based camera-controlled methods (e.g., StreetCrafter [57]) show geometric inconsistencies in occluded or distant regions due to incomplete cues. In contrast, ReCamDriving employs coarse-to-fine two-stage training strategy that leverages dense scenestructure information from novel-trajectory 3DGS renderings for precise camera control and structurally consistent generation."
        },
        {
            "title": "Abstract",
            "content": "We propose ReCamDriving, purely vision-based, cameracontrolled novel-trajectory video generation framework. While repair-based methods fail to restore complex artifacts and LiDAR-based approaches rely on sparse and incomplete cues, ReCamDriving leverages dense and scenecomplete 3DGS renderings for explicit geometric guidance, achieving precise camera-controllable generation. To mitigate overfitting to restoration behaviors when conditioned on 3DGS renderings, ReCamDriving adopts twostage training paradigm: the first stage uses camera poses 1 for coarse control, while the second stage incorporates 3DGS renderings for fine-grained viewpoint and geometric guidance. Furthermore, we present 3DGS-based crosstrajectory data curation strategy to eliminate the traintest gap in camera transformation patterns, enabling scalable multi-trajectory supervision from monocular videos. Based on this strategy, we construct the ParaDrive dataset, containing over 110K parallel-trajectory video pairs. Extensive experiments demonstrate that ReCamDriving achieves state-of-the-art camera controllability and structural consistency. Project page: https://recamdriving.github.io/. 1. Introduction High-quality multi-trajectory video data is essential for autonomous driving, as it provides diverse viewpoints for tasks such as 3D reconstruction [56, 65] and world-model training [8, 64]. However, collecting real-world multitrajectory data is extremely costly, as it either requires multiple synchronized vehicles to capture videos from different viewpoints or involves repeated traversals with inevitable spatio-temporal inconsistencies. These challenges make high-fidelity novel-trajectory video synthesis from single recorded trajectory an attractive and scalable alternative. common approach for novel-trajectory generation is the reconstruction-then-repair pipeline [27, 33, 53, 59]. It first reconstructs the scene using Neural Radiance Fields (NeRF) [32] or 3D Gaussian Splatting (3DGS) [20], renders videos from novel trajectories, and then trains diffusionbased repair models to restore artifacts in the rendered results. Although effective at artifact removal, this pipeline often fails under complex rendering degradations (Fig. 1a). The core limitation is that repair models learn local degraded-to-clean mapping based on training-time degradation patterns, whereas highly varying 3D rendering artifacts across scenes and viewpoints often fall out of distribution, leading to failed restoration and 3D inconsistency. Another paradigm is camera-controlled video generation [2, 63], which regenerates novel-trajectory videos conditioned on camera pose sequences and generally produces visually coherent results. However, conditioning simply on camera poses often leads to imprecise viewpoint control. To achieve more accurate camera guidance, FreeVS [46] and StreetCrafter [57] incorporate novel-trajectory LiDAR projections to provide explicit geometric cues, but LiDAR projections are sparse and incomplete in background or occluded regions, often resulting in 3D-inconsistent results (Fig. 1b). Moreover, training such camera-transformation models requires ground-truth novel-trajectory supervision, which is unavailable in autonomous driving datasets. As compromise, prior works [46, 57] construct pseudo training pairs from different segments of single recorded trajectory (Fig. 2a), which only captures longitudinal motion patterns and leads to degraded performance when inferring lateral motion trajectories. To address these challenges, we propose ReCamDriving, pure vision-based novel-trajectory video generation framework for autonomous driving. Our approach performs camera-controlled video generation and, crucially, replaces costly LiDAR projections with novel-trajectory 3DGS renderings as camera-control signals. The key insight is that, although 3DGS renderings have lower geometric fidelity than LiDAR projections, they preserve more complete and structurally informative scene cues across all regions, enabling more precise camera control and structurally consistent generation. Specifically, to avoid the model collapsing into trivial artifact-repair solution when introducing 3DGS renderings, we adopt two-stage training strategy for progressive learning of viewpoint transformation. We first train the model with camera poses to establish basic camera-controlled generation capability. Then, we freeze the base network and introduce additional modules that incorporate 3DGS renderings for fine-grained viewpoint and structural guidance. Without this scheme, the model tends to overfit to artifact restoration rather than learning true camera-controlled generation, exhibiting limitations similar to repair-based methods. Furthermore, to construct lateral trajectory supervision and bridge the mismatch between training and inference camera-transformation patterns, we introduce 3DGSbased cross-trajectory data curation strategy  (Fig. 2)  . During training, 3DGS renderings of laterally offset trajectories are used as source views, while the recorded trajectory provides supervision. At inference, the recorded trajectory serves as the source to generate laterally novel trajectories, ensuring consistency between training and inference camera-transformation patterns. This strategy enables multi-trajectory supervision from monocular videos, allowing us to construct the ParaDrive dataset containing about 1.6K 3DGS scenes from Waymo Open Dataset (WOD) [40] and NuScenes [7], containing over 110K parallel-trajectory pairs of 3DGS renderings and recorded videos. By relying solely on monocular data, this approach enables scalable multi-trajectory dataset construction and has the potential to extend camera-controlled models toward web-scale videos. Extensive experiments validate the effectiveness of our method. Our main contributions are: We propose ReCamDriving, novel vision-based framework for novel-trajectory video generation that leverages 3DGS renderings to achieve precise camera control and structurally consistent generation. We introduce novel 3DGS-based cross-trajectory data curation strategy for scalable multi-trajectory supervision, and construct the ParaDrive dataset with over 110K parallel-trajectory pairs. Extensive experiments demonstrate that ReCamDriving achieves the state-of-the-art performance in both camera controllability and 3D consistency. 2. Related Work 2.1. Diffusion Priors for Repairing 3D Renderings. Recent advances in NeRF [32] and 3DGS [20] have greatly advanced 3D reconstruction [3, 4, 19, 23, 31, 4951, 62]. However, under limited viewpoints, these methods still produce noticeable artifacts during novel-view synthesis, especially in extrapolated regions [22, 43, 66]. To mitigate this issue, recent works explore learning diffusion priors to repair degraded 3D renderings. 3DGS-Enhancer [27] 2 fine-tunes video diffusion model for rendering restoration, Difix3D+ [53] enables real-time neural enhancement, and GSFixer [59] conditions video restoration on both semantic and geometric cues. Freesim [33] introduces framework for novel-trajectory restoration through progressive reconstruction strategy. While these methods can improve the visual fidelity of 3D renderings, they essentially address restoration problem, focusing on local artifact correction rather than consistent scene-level geometry modeling. Consequently, their performance heavily depends on the training data distribution, and since rendering artifacts vary significantly across scenes and viewpoints, they often fail when facing out-of-distribution degradations. 2.2. Camera-Controlled Video Generation. Camera-controlled generation has attracted increasing attention for synthesizing immersive scenes and novel viewpoints [2, 15, 30, 37, 47, 54, 55, 63]. The key challenge lies in how to represent camera motion and inject it into generative models for precise viewpoint control. Early works [1, 2, 12, 52, 58] condition diffusion models on camera extrinsics or Plucker embeddings, enabling controllable generation but often resulting in inconsistent targetview geometry. From representation-learning perspective [5, 35, 41, 42], such pose-conditioned models primarily capture statistical correlations between camera motion and appearance variation rather than geometric causality, leading to spatial misalignment and geometric instability. To address these issues, recent approaches [60, 61] incorporate 3D point-map priors [16, 21, 48] to inject explicit geometric structure, yet their performance remains limited by the quality of reconstructed point maps in large-scale driving scenes. Alternatively, FreeVS [46] and StreetCrafter [57] leverage LiDAR point clouds for camera control, yet LiDAR data are costly and spatially incomplete. In contrast, our approach adopts purely visual formulation for novel-trajectory generation, replacing LiDAR with 3DGS renderings as structural camera conditions to achieve fine-grained, geometrically consistent control. 3. Cross-trajectory Data Curation and"
        },
        {
            "title": "ParaDriving Dataset",
            "content": "Training camera-controlled video regeneration model requires ground-truth videos from novel trajectories for supervision. While general datasets [25, 36] provide such supervision, autonomous driving datasets typically contain only single-pass trajectories. Prior works [46, 57] mitigate this limitation by splitting single trajectory into source and target segments (Fig. 2a), but this setting models only longitudinal motion, resulting in degraded performance for lateralview generation during inference. To address this challenge, we propose 3DGS-based cross-trajectory data curation strategy that enables superFigure 2. (ab) Comparison of training and inference cameratransformation patterns. (c) Our training and inference data strategy. (Trans.: Transformation; Traj.: Trajectory) vised learning of lateral camera transformations. Specifically, given an original trajectory video, we first reconstruct 3DGS representation of the scene and laterally shift the camera trajectory by fixed offset (e.g., +3 m) to render novel trajectory. Since the rendered novel-trajectory video contains artifacts and cannot serve as ground truth, we instead use it as the source input during training, while the clean recorded-trajectory video provides the groundtruth supervision (Fig. 2c). At inference, we use the clean recorded video as the source input to generate laterally shifted novel-trajectory views . Although the network is trained with rendered videos as source inputs but tested with clean recorded videos, experiments show that this traininference source mismatch does not degrade performance. On the contrary, using clean source videos during inference further improves visual quality. Please refer to Sec. 5.3 for more details."
        },
        {
            "title": "To construct",
            "content": "the training pairs, we reconstruct approximately 1.6K 3DGS scenes from WOD [40] and NuScenes [7] using the DriveStudio framework [9]. During 3DGS training, each scene saves intermediate models at 100, 500, and 1,000 iterations, producing underfitted recorded-trajectory renderings that serve as training-time camera conditions with varying artifact levels, in order to align them with the artifact-prone novel-trajectory renderings used during inference. The 30K-iteration models are used to render eight laterally shifted trajectories (1 m, 2 m, 3 m, and 4 m), which serve as source videos during training. Each trajectory contains three 121-frame 3 In total, the 3DGS trainclips (front, middle, and rear). ing process consumed 8,240 L20 GPU hours and produced over 110K dual-trajectory video pairs. We name this dataset ParaDrive, which will be released to facilitate research on camera-controlled video generation. 4. ReCamDriving Given source trajectory video Vs, our goal is to synthesize novel trajectory video Vt with lateral offsets. To this end, we adopt two-stage training strategy that progressively guides the model to perform viewpoint transformation, as illustrated in Fig. 3. In Stage 1, we aim for the network to understand and learn the physical process of viewpoint transformation by using the relative camera pose = Tts SE(3) between trajectories, enabling it to warp information from the source video to the target trajectory. In Stage 2, we introduce novel-trajectory 3DGS renderings as fine-grained guidance, freeze the first-stage parameters, and add two additional attention modules to incorporate 3DGS renderings for accurate viewpoint and structural generation. 4.1. Preliminary: Latent Diffusion Models with"
        },
        {
            "title": "Flow Matching",
            "content": "Latent Diffusion Models (LDMs) [38] perform diffusion in compact latent space learned by pretrained autoencoder, achieving high-fidelity generation with reduced computational cost. This balance between efficiency and quality has made LDMs the foundation of many state-of-theart image and video generators [6, 38, 45]. Given video RF 3HW , 3D VAE encoder projects it into latents Rf chw with compressed spatial and temporal dimensions. Diffusion and generation are then performed in this latent space, and the decoded outputs reconstruct the final video frames. Traditional diffusion models [14, 18, 39] formulate generation as stochastic differential equation (SDE), reversing predefined noise process but suffering from training instability and slow inference. Flow Matching [10, 26] replaces this stochastic formulation with deterministic ordinary differential equation (ODE) that learns time-dependent velocity field to transport samples from noise to data, yielding more stable training and faster sampling. The forward process is typically defined as straight path between noise x0 and data x1: xt = tx1 + (1 t)x0, [0, 1]. (1) Differentiating with respect to gives the corresponding velocity field: matching can be formulated as: LFM = Ex0, x1, ccam, tU (0,1) (cid:13) (cid:13)ϵθ(xt; ccam, t) vt (cid:13) 2 (cid:13) , (3) where ϵθ denotes the neural network predicting the velocity field. 4.2. Training Stage 1: Relative Pose-Guided Camera Transformation In this stage, the relative camera pose is used as condition to guide the model in re-generating the sourcetrajectory video. Instead of using absolute poses, relative poses are easier to specify during inference and are more robust to calibration errors. ReCamMaster [2] adopts similar idea but concatenates the relative pose with both the source latent xs and the noisy latent xt, which can introduce ambiguity regarding which latent corresponds to the source or the target view. To resolve this, we separately encode the relative pose and the identity pose TI = I4 SE(3) (representing zero motion) using camera encoder cam, producing cr = Ecam(T ) Rf and cI = Ecam(TI ) Rf d, where denotes the feature dimension. We further introduce learnable frame embedding Ef Rf that provides frame-wise correspondence cues, enhancing temporal alignment in self-attention operations. After encoding the source and noisy videos using the 3D VAE encoder, we obtain latent representations xs, xt Rf chw. patchify operation unfolds each spatial feature map into sequence of local tokens, resulting in xs, xt Rf ld, where = w. The camera and frame embeddings are then added to each latent via broadcasting along the l-dimension: xi = Cat(xt + cr + Ef , xs + cI + Ef ), (4) where Cat denotes concatenation along the frame dimension . The resulting sequence xi is processed by layers of Diffusion Transformer (DiT) [34] blocks to predict the velocity field. Each DiT block in this stage (see Fig. 3) consists of self-attention layer, feed-forward network (FFN), and normalization layers, with only the self-attention parameters unfrozen for adaptation. We adopt the flow-matching framework to schedule noise levels and optimize the diffusion process. We adopt the flow-matching framework to schedule noise levels and optimize the diffusion process. The training objective follows Eq. 3, where ccam denotes the relative camera-pose embedding and the target data x1 corresponds to the clean recorded-trajectory videos. 4.3. Training Stage 2: Fine-grained Camera Convt = = x1 x0. (2) trol via 3DGS Renderings dxt dt In the context of camera-controlled video generation, given camera condition ccam, the training objective of flow In this stage, to improve camera control accuracy and structural guidance, besides the relative camera pose, we utilize 4 Figure 3. Overview of our framework. We adopt two-stage training scheme for precise and structurally consistent novel-trajectory video generation. In Stage 1, ReCamDriving trains DiT blocks conditioned on the source trajectory video and relative camera (cam.) pose. When switching to Stage 2, the original DiT parameters are frozen, and additional attention modules are introduced to integrate 3DGS renderings for fine-grained view control and structural guidance. Shared modules between stages are connected with blue dashed lines. DriveStudio [9] to reconstruct 3DGS representations and render novel-trajectory 3DGS renderings as additional conditions. The renderings Vgs are encoded by the same 3D VAE encoder and patchified to obtain xgs Rf ld. We then augment xgs with the relative camera-pose embedding and frame embedding: xgs = xgs + cr + Ef . (5) To integrate 3DGS features effectively, each DiT block introduces two additional components: Rendering Attention and Cross Attention, while the self-attention parameters from stage 1 are kept frozen. The Rendering Attention serves as an auxiliary self-attention layer that refines spatiotemporal representations within the 3DGS rendering latent space. It shares the same structure as the self-attention used in stage 1 but is named differently to avoid confusion. The Cross Attention then fuses the rendering latent features xgs with the diffusion latent xi = SelfAttn(xi) to enhance structural and geometric consistency between the generated and target trajectories. The self-attention modules from stage 1 remain frozen to preserve their cameracontrolled generation capability, ensuring that xi already encodes coarse camera transformations when interacting with 3DGS rendering features. This design encourages the model to leverage the geometric cues embedded in 3DGS renderings for fine-grained view refinement. Without this constraint, the model tends to overfit to artifact-repair behaviors on 3DGS renderings rather than assisting the firststage module in camera relocalization, leading to failure cases similar to repair-based models during inference. Training in this stage also follows Eq. 3, but with the condition ccam extended to include both the relative pose embedding and the 3DGS rendering latent. Finally, during inference, the 3D VAE decoder reconstructs the noveltrajectory video Vt from the stage 2 latent representation, achieving precise camera control and structural consistency. 5. Experiments 5.1. Experimental setup Implementation. We train ReCamDriving on the proposed ParaDrive dataset in two stages. Each stage is conducted on 64 NVIDIA A100 GPUs for 6,000 steps, with batch size of 1 and learning rate of 1e-4. The total training takes approximately 3.5 days. The training resolution is set to 480 832, and each video consists of 121 frames. We initialize our model from the Wan2.1 text-to-video foundation model [45], with the text prompt set to an empty string by default. Specifically, the 3D VAE, rendering attention, cross attention, and FFN modules are initialized from Wan2.1, while the camera encoder and self-attention layers are initialized from ReCamMaster [2]. Evaluation Dataset and Metrics. We select 20 scenes each from WOD [40] and NuScenes [7] for validation. We (1) Visual evaluate our method from three perspectives: Quality: we use Imaging Quality (IQ) [17] to assess fidelity and the average CLIP similarity of adjacent frames (CLIPF) to measure temporal consistency. (2) Camera Accuracy: 5 Table 1. Quantitative comparison results on WOD [40], where bold indicates the best performance, and underline denotes the second best. Lateral Offset 1m Lateral Offset 2m Method Visual Quality Cam. Accuracy View Consistency Visual Quality Cam. Accuracy View Consistency IQ CLIP-F RErr. TErr. FID FVD CLIP-V IQ CLIP-F RErr. TErr. FID FVD CLIP-V DriveStudio Difix3D+ FreeVS 52.13 64.24 62.74 StreetCrafter 63.57 98. 98.92 95.74 97.31 - 1.36 1. 1.52 - 2.42 2.88 2.53 83. 56.35 63.06 28.18 25.37 27.80 37. 20.51 94.78 95.32 88.99 96.01 47. 63.11 60.16 63.78 98.49 98.41 92. 97.17 - 1.64 2.12 1.79 - 104.24 39.79 2.66 2.93 2.77 57. 67.87 46.78 31.88 43.59 22.81 94. 92.85 88.41 94.74 Ours 65.18 99. 1.32 2.37 13.76 13.27 97.96 65. 99.03 1.45 2.43 25.01 14.08 97. Method Lateral Offset 3m Lateral Offset 4m DriveStudio Difix3D+ FreeVS 43.83 60.88 57."
        },
        {
            "title": "StreetCrafter",
            "content": "59.34 98.27 97.38 92.01 96.89 - 2.01 3.17 1.91 - 116.12 63. 2.97 3.78 3.13 66.39 84.87 50. 45.23 55.76 30.26 90.37 91.76 86. 92.13 41.47 58.81 56.15 59.89 98. 97.14 91.26 96.63 - 2.68 3. 2.87 - 144.05 72.50 3.12 3. 3.03 78.08 65.37 107.04 58.39 68. 36.67 88.76 90.12 85.17 91.17 Ours 62.68 98.79 1.63 2.65 28.38 22. 96.50 61.32 98.45 1.57 2.73 32. 26.76 94.91 Table 2. Average quantitative results of novel-trajectory generation on NuScenes [7]. Method Visual Quality View Consistency IQ CLIP-F FID FVD CLIP-V DriveStudio Difix3D+ 45.39 61.76 97.78 98.16 103. 65.14 52.93 41.37 89.19 90.48 Ours 62.38 98.75 25.68 18.98 96.14 Figure 4. Qualitative comparison results on NuScenes [7]. 5.2. Comparison Results Following CamCloneMaster [29], we adopt the state-ofthe-art (SOTA) camera estimation method MegaSaM [24] to recover camera parameters from the generated videos, and evaluate accuracy using Rotation Error (RErr.) and Translation Error (TErr.). (3) View Consistency: We compute the Frechet Video Distance (FVD) [44] and Frechet Image Distance (FID) [13] between the source and generated video distributions, and further compute the framewise CLIP similarity (CLIP-V) to assess cross-view semantic consistency. Baselines. We compare ReCamDriving against state-ofthe-art novel trajectory synthesis methods [9, 46, 53, 57]. DriveStudio [9] reconstructs 3DGS using dynamic neural scene graph for novel-trajectory rendering. Difix3D+ [53] follows the reconstructionrepair paradigm, using singlestep diffusion model to restore artifacted 3DGS renderings. In addition, FreeVS [46] and StreetCrafter [57] adopt camera-controlled video generation approach, where the target trajectory is conditioned on colored LiDAR point projections to synthesize novel-trajectory videos. Qualitative Results. FreeVS [46] and StreetCrafter [57] provide pretrained weights only on WOD. For fair comparison, we train version of our model on the WOD subset of ParaDrive, with qualitative results shown in Fig. 5. We further compare our full model trained on the entire ParaDrive with Difix3D+ [53] dataset and DriveStudio [9] on NuScenes [7]  (Fig. 4)  . ReCamDriving consistently outperforms all baselines. Difix3D+ often produces view-inconsistent corrections on fine structures near the ego vehicle (e.g., lane markings, road text), whereas ReCamDriving maintains structural continuity. FreeVS and StreetCrafter exhibit severe inconsistencies under large viewpoint shifts due to sparse LiDAR conditioning. As shown in Fig. 5, StreetCrafter fails to reconstruct complete vehicle geometry in the first scene, while both baselines yield blurred or 3D-inconsistent distant backgrounds caused by occluded or sparsely sampled LiDAR regions. In contrast, ReCamDriving leverages dense structural cues from 3DGS renderings for robust view and geometric guidance, enabling more reliable generation of Figure 5. Qualitative comparison results on WOD [40]. Our method and Difix3D+ [53] use novel-trajectory renderings from DriveStudio [9] for camera control and restoration, respectively. Note that the officially released FreeVS [46] model is trained and tested on cropped resolution that excludes sky regions to reduce computation and avoid LiDAR-sparse areas. Table 3. Quantitative ablation of camera conditions on WOD [40]. Camera Condition IQ FID FVD RErr. TErr. Pose Pose + LiDAR Pose + LiDAR + GS 60.13 61.32 63.42 34.86 31.23 24.75 32.31 27.78 19.27 Pose + GS (Ours) 63.63 24.88 19.18 3.01 1.53 1.41 1.49 4.23 2.69 2. 2.55 ble 2, respectively. Since DriveStudio renders results using ground-truth poses, we do not evaluate its camera accuracy. As shown, ReCamDriving consistently outperforms all baselines across all metrics. Notably, as the lateral offset increases, the view consistency of FreeVS [46], StreetCrafter [57], and Difix3D+ [53] degrades sharply, whereas ReCamDriving remains stable, demonstrating superior robustness under large viewpoint shifts. 5.3. Ablation Studies Effectiveness of 3DGS Rendering Condition. In Stage 2, 3DGS renderings of the target trajectory are used as conditions for fine-grained camera control. We ablate it with three variants: (1) without Stage 2 (Pose only); (2) Stage 2 with LiDAR projection (Pose + LiDAR); (3) Stage 2 with both LiDAR projection and 3DGS renderings (Pose + LiDAR + GS). LiDAR projections are generated following StreetCrafter [57] and injected into the Rendering Attention branch. Qualitative and quantitative results are presented in Fig. 6 and Table 3. As shown, using only camera poses often results in inaccurate camera control, while incorporating LiDAR improves pose accuracy but still causes geometric Figure 6. Qualitative ablation of camera conditions on WOD [40]. Figure 7. Qualitative ablation of training paradigms on WOD [40]. distant content and occluded objects. Quantitative Results. Quantitative comparisons on WOD and NuScenes are reported in Table 1 and Ta7 Figure 8. Qualitative ablation on our two-stage training strategy. Table 6. Ablation of our data curation strategy on WOD [40]. Table 4. Qualitative ablation of training paradigms on WOD [40]. Camera Transformation RErr. TErr. FID FVD Figure 9. Ablation on different source inputs at inference. Method IQ CLIP-F FID FVD CLIP-V Repair Baseline 62. Ours 63.63 98.18 98.90 40.87 24. 31.44 19.18 91.32 96.64 Table 5. Quantitative ablation on our two-stage training strategy on WOD [40] and NuScenes [7]. Training strategy IQ FID FVD CLIP-V One-stage Two-stage (Ours) 59.97 63.42 32.64 25.13 25.16 18. 94.78 96.32 inconsistencies under large offsets due to LiDAR sparsity. Our 3DGS-conditioned model effectively mitigates these issues. Moreover, although combining LiDAR and 3DGS yields slightly better scores, the gain is marginal compared to the higher LiDAR cost, so we adopt 3DGS-only conditioning as our final design. Repair Model vs Regeneration Model. We further compare the adopted video regeneration paradigm with repair-based baseline. To train the repair baseline, we construct pairs on the recorded trajectory using blurred 3DGS renderings as inputs and clean videos as targets, with Rendering Attention removed. Both methods are evaluated on WOD [40], and qualitative and quantitative results are presented in Fig. 7 and Table 4. As shown, the repair-based model often fails to correct artifacts, as these degradations are not well covered by its training distribution. Ablation on the Training Strategies. We employ two-stage strategy to ensure that 3DGS renderings enhance camera control rather than artifact repair. To evaluate its effectiveness, we compare it with single-stage scheme where all modules are trained jointly without freezing on the full ParaDrive dataset. Quantitative and qualitative results are shown in Table 5 and Fig. 8. The single-stage model tends to produce artifacts and exhibits degraded visual quality similar to repair-based methods, whereas the two-stage strategy yields clearer and more 3D-consistent results, conLongitudinal Lateral (Ours) 1.97 1.49 3.02 2.55 34. 28.54 24.88 19.18 firming its effectiveness. Ablation on Cross-Trajectory Data Curation. We propose cross-trajectory data curation strategy to align camera-transformation modes between training and inference. To evaluate its effectiveness, we train baseline using longitudinal transformations following FreeVS [46], where the first and last 121 frames of each recorded trajectory are used as source and supervision, respectively. Table 6 presents the comparison results, showing that our crosstrajectory strategy significantly improves both camera accuracy and view consistency for novel-trajectory prediction, confirming its effectiveness. Impact of TrainTest Source-Video Mismatch. During training, we use blurred 3DGS renderings as source videos, while at test time the source is the clean recorded trajectory. To examine the impact of this mismatch, we compare inference using either clean recorded videos or their degraded 3DGS-rendered counterparts as sources. As shown in Fig. 9, degraded inputs still yield reasonable results, but clean sources produce noticeably sharper backgrounds, indicating that clean source videos at test time do not cause degradation but instead improve performance. 6. Conclusion and Limitations In this work, we introduced ReCamDriving, pure visionbased framework for controllable novel-trajectory video generation. By leveraging 3DGS renderings as structural camera conditions and adopting two-stage training paradigm, our model achieves precise camera control and consistent geometry. We also construct the large-scale ParaDrive dataset using the proposed cross-trajectory data curation strategy, which enables scalable multi-trajectory supervision from monocular videos. Extensive experiments show that ReCamDriving achieves higher camera accuracy and visual fidelity than existing baselines. Nevertheless, our 8 method remains limited in handling small distant objects (e.g., faraway pedestrians), where structural cues become unreliable. Incorporating stronger structural priors for such regions is promising direction for future work."
        },
        {
            "title": "References",
            "content": "[1] Sherwin Bahmani, Ivan Skorokhodov, Guocheng Qian, Aliaksandr Siarohin, Willi Menapace, Andrea Tagliasacchi, David Lindell, and Sergey Tulyakov. Ac3d: Analyzing and improving 3d camera control in video diffusion transformers. In Proceedings of the Computer Vision and Pattern Recognition Conference, pages 2287522889, 2025. 3 [2] Jianhong Bai, Menghan Xia, Xiao Fu, Xintao Wang, Lianrui Mu, Jinwen Cao, Zuozhu Liu, Haoji Hu, Xiang Bai, Pengfei Wan, et al. Recammaster: Camera-controlled generative rendering from single video. arXiv preprint arXiv:2503.11647, 2025. 2, 3, 4, 5 [3] Jonathan Barron, Ben Mildenhall, Matthew Tancik, Peter Hedman, Ricardo Martin-Brualla, and Pratul Srinivasan. Mip-nerf: multiscale representation for anti-aliasing neural radiance fields. In Proceedings of the IEEE/CVF international conference on computer vision, pages 58555864, 2021. 2 [4] Jonathan Barron, Ben Mildenhall, Dor Verbin, Pratul Srinivasan, and Peter Hedman. Mip-nerf 360: Unbounded In Proceedings of anti-aliased neural radiance fields. the IEEE/CVF conference on computer vision and pattern recognition, pages 54705479, 2022. 2 [5] Yoshua Bengio, Aaron Courville, and Pascal Vincent. Representation learning: review and new perspectives. IEEE transactions on pattern analysis and machine intelligence, 35(8):17981828, 2013. 3 [6] Andreas Blattmann, Tim Dockhorn, Sumith Kulal, Daniel Mendelevitch, Maciej Kilian, Dominik Lorenz, Yam Levi, Zion English, Vikram Voleti, Adam Letts, et al. Stable video diffusion: Scaling latent video diffusion models to large datasets. arXiv preprint arXiv:2311.15127, 2023. 4, 12 [7] Holger Caesar, Varun Bankiti, Alex Lang, Sourabh Vora, Venice Erin Liong, Qiang Xu, Anush Krishnan, Yu Pan, Giancarlo Baldan, and Oscar Beijbom. nuscenes: multiIn Proceedings of modal dataset for autonomous driving. the IEEE/CVF conference on computer vision and pattern recognition, pages 1162111631, 2020. 2, 3, 5, 6, 8, 12 [8] Yuntao Chen, Yuqi Wang, and Zhaoxiang Zhang. Drivinggpt: Unifying driving world modeling and planning with multi-modal autoregressive transformers. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 2689026900, 2025. 2 [9] Ziyu Chen, Jiawei Yang, Jiahui Huang, Riccardo de Lutio, Janick Martinez Esturo, Boris Ivanovic, Or Litany, Zan Gojcic, Sanja Fidler, Marco Pavone, Li Song, and Yue Wang. In The ThirOmnire: Omni urban scene reconstruction. teenth International Conference on Learning Representations, 2025. 3, 5, 6, Lorenz, Axel Sauer, Frederic Boesel, et al. Scaling rectified flow transformers for high-resolution image synthesis. In Forty-first international conference on machine learning, 2024. 4 [11] Shenyuan Gao, Jiazhi Yang, Li Chen, Kashyap Chitta, Yihang Qiu, Andreas Geiger, Jun Zhang, and Hongyang Li. Vista: generalizable driving world model with high fidelity and versatile controllability. Advances in Neural Information Processing Systems, 37:9156091596, 2024. 12 [12] Hao He, Yinghao Xu, Yuwei Guo, Gordon Wetzstein, Bo Dai, Hongsheng Li, and Ceyuan Yang. Cameractrl: Enabling camera control for text-to-video generation. arXiv preprint arXiv:2404.02101, 2024. 3 [13] Martin Heusel, Hubert Ramsauer, Thomas Unterthiner, Bernhard Nessler, and Sepp Hochreiter. Gans trained by two time-scale update rule converge to local nash equilibrium. Advances in neural information processing systems, 30, 2017. 6 [14] Jonathan Ho, Ajay Jain, and Pieter Abbeel. Denoising diffusion probabilistic models. Advances in neural information processing systems, 33:68406851, 2020. 4 [15] Teng Hu, Jiangning Zhang, Ran Yi, Yating Wang, Hongrui Huang, Jieyu Weng, Yabiao Wang, and Lizhuang Ma. Motionmaster: Training-free camera motion transfer for video generation. arXiv preprint arXiv:2404.15789, 2024. 3 [16] Wenbo Hu, Xiangjun Gao, Xiaoyu Li, Sijie Zhao, Xiaodong Cun, Yong Zhang, Long Quan, and Ying Shan. Depthcrafter: Generating consistent long depth sequences for open-world videos. In Proceedings of the Computer Vision and Pattern Recognition Conference, pages 20052015, 2025. [17] Ziqi Huang, Yinan He, Jiashuo Yu, Fan Zhang, Chenyang Si, Yuming Jiang, Yuanhan Zhang, Tianxing Wu, Qingyang Jin, Nattapol Chanpaisit, et al. Vbench: Comprehensive benchmark suite for video generative models. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 2180721818, 2024. 5 [18] Aapo Hyvarinen and Peter Dayan. Estimation of nonnormalized statistical models by score matching. Journal of Machine Learning Research, 6(4), 2005. 4 [19] Yifan Jiang, Peter Hedman, Ben Mildenhall, Dejia Xu, Jonathan Barron, Zhangyang Wang, and Tianfan Xue. Alignerf: High-fidelity neural radiance fields via alignmentaware training. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 4655, 2023. 2 [20] Bernhard Kerbl, Georgios Kopanas, Thomas Leimkuhler, and George Drettakis. 3d gaussian splatting for real-time radiance field rendering. ACM Trans. Graph., 42(4):1391, 2023. 2 [21] Vincent Leroy, Yohann Cabon, and Jerˆome Revaud. Grounding image matching in 3d with mast3r. In European Conference on Computer Vision, pages 7191. Springer, 2024. [22] Yaokun Li, Chao Gou, and Guang Tan. Taming uncertainty in sparse-view generalizable nerf via indirect diffusion guidance. arXiv preprint arXiv:2402.01217, 3, 2024. 2 [10] Patrick Esser, Sumith Kulal, Andreas Blattmann, Rahim Entezari, Jonas Muller, Harry Saini, Yam Levi, Dominik [23] Yaokun Li, Shuaixian Wang, and Guang Tan. Id-nerf: Indirect diffusion-guided neural radiance fields for generaliz9 able view synthesis. Expert Systems with Applications, 266: 126068, 2025. of the IEEE/CVF conference on computer vision and pattern recognition, pages 69646974, 2021. 3 [24] Zhengqi Li, Richard Tucker, Forrester Cole, Qianqian Wang, Linyi Jin, Vickie Ye, Angjoo Kanazawa, Aleksander Holynski, and Noah Snavely. Megasam: Accurate, fast and roIn bust structure and motion from casual dynamic videos. Proceedings of the Computer Vision and Pattern Recognition Conference, pages 1048610496, 2025. 6 [25] Lu Ling, Yichen Sheng, Zhi Tu, Wentian Zhao, Cheng Xin, Kun Wan, Lantao Yu, Qianyu Guo, Zixun Yu, Yawen Lu, et al. Dl3dv-10k: large-scale scene dataset for deep learning-based 3d vision. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 2216022169, 2024. 3 [26] Yaron Lipman, Ricky TQ Chen, Heli Ben-Hamu, Maximilian Nickel, and Matt Le. Flow matching for generative modeling. arXiv preprint arXiv:2210.02747, 2022. 4 [27] Xi Liu, Chaoyi Zhou, and Siyu Huang. 3dgs-enhancer: Enhancing unbounded 3d gaussian splatting with viewconsistent 2d diffusion priors. Advances in Neural Information Processing Systems, 37:133305133327, 2024. 2 [28] Matthew Loper, Naureen Mahmood, Javier Romero, Gerard Pons-Moll, and Michael Black. Smpl: skinned multiperson linear model. In Seminal Graphics Papers: Pushing the Boundaries, Volume 2, pages 851866. 2023. 12 [29] Yawen Luo, Jianhong Bai, Xiaoyu Shi, Menghan Xia, Xintao Wang, Pengfei Wan, Di Zhang, Kun Gai, and Tianfan Xue. Camclonemaster: Enabling reference-based camera control arXiv preprint arXiv:2506.03140, for video generation. 2025. [30] Yue Ma, Kunyu Feng, Zhongyuan Hu, Xinyu Wang, Yucheng Wang, Mingzhe Zheng, Xuanhua He, Chenyang Zhu, Hongyu Liu, Yingqing He, et al. Controllable video arXiv preprint arXiv:2507.16869, generation: survey. 2025. 3 [31] Ricardo Martin-Brualla, Noha Radwan, Mehdi SM Sajjadi, Jonathan Barron, Alexey Dosovitskiy, and Daniel Duckworth. Nerf in the wild: Neural radiance fields for unconstrained photo collections. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 72107219, 2021. 2 [32] Ben Mildenhall, Pratul Srinivasan, Matthew Tancik, Jonathan Barron, Ravi Ramamoorthi, and Ren Ng. Nerf: Representing scenes as neural radiance fields for view synthesis. Communications of the ACM, 65(1):99106, 2021. 2 [33] Jeffrey Miller and Ellis Horowitz. Freesim-a free real-time In 2007 IEEE Intelligent Transfreeway traffic simulator. portation Systems Conference, pages 1823. IEEE, 2007. 2, 3 [34] William Peebles and Saining Xie. Scalable diffusion models with transformers. In Proceedings of the IEEE/CVF international conference on computer vision, pages 41954205, 2023. 4 [35] Rui Qian, Tianjian Meng, Boqing Gong, Ming-Hsuan Yang, Huisheng Wang, Serge Belongie, and Yin Cui. Spatiotemporal contrastive video representation learning. In Proceedings [36] Jeremy Reizenstein, Roman Shapovalov, Philipp Henzler, Luca Sbordone, Patrick Labatut, and David Novotny. Common objects in 3d: Large-scale learning and evaluation In Proceedings of of real-life 3d category reconstruction. the IEEE/CVF international conference on computer vision, pages 1090110911, 2021. 3 [37] Xuanchi Ren, Tianchang Shen, Jiahui Huang, Huan Ling, Yifan Lu, Merlin Nimier-David, Thomas Muller, Alexander Keller, Sanja Fidler, and Jun Gao. Gen3c: 3d-informed world-consistent video generation with precise camera conIn Proceedings of the Computer Vision and Pattern trol. Recognition Conference, pages 61216132, 2025. 3 [38] Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Bjorn Ommer. High-resolution image In Proceedings of synthesis with latent diffusion models. the IEEE/CVF conference on computer vision and pattern recognition, pages 1068410695, 2022. 4 [39] Yang Song, Jascha Sohl-Dickstein, Diederik Kingma, Abhishek Kumar, Stefano Ermon, and Ben Poole. Score-based generative modeling through stochastic differential equations. arXiv preprint arXiv:2011.13456, 2020. 4 [40] Pei Sun, Henrik Kretzschmar, Xerxes Dotiwalla, Aurelien Chouard, Vijaysai Patnaik, Paul Tsui, James Guo, Yin Zhou, Yuning Chai, Benjamin Caine, et al. Scalability in perception for autonomous driving: Waymo open dataset. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 24462454, 2020. 2, 3, 5, 6, 7, 8, 12 [41] Zhenchao Tang, Hualin Yang, and Calvin Yu-Chian Chen. Weakly supervised posture mining for fine-grained classification. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 2373523744, 2023. [42] Zhenchao Tang, Guanxing Chen, Shouzhi Chen, Jianhua Yao, Linlin You, and Calvin Yu-Chian Chen. Modal-nexus auto-encoder for multi-modality cellular data integration and imputation. Nature Communications, 15(1):9021, 2024. 3 [43] Prune Truong, Marie-Julie Rakotosaona, Fabian Manhardt, and Federico Tombari. Sparf: Neural radiance fields from In Proceedings of the IEEE/CVF sparse and noisy poses. Conference on Computer Vision and Pattern Recognition, pages 41904200, 2023. 2 [44] Thomas Unterthiner, Sjoerd Van Steenkiste, Karol Kurach, Raphael Marinier, Marcin Michalski, and Sylvain Gelly. Towards accurate generative models of video: new metric & challenges. arXiv preprint arXiv:1812.01717, 2018. 6 [45] Team Wan, Ang Wang, Baole Ai, Bin Wen, Chaojie Mao, Chen-Wei Xie, Di Chen, Feiwu Yu, Haiming Zhao, Jianxiao Yang, et al. Wan: Open and advanced large-scale video generative models. arXiv preprint arXiv:2503.20314, 2025. 4, 5, 12 [46] Qitai Wang, Lue Fan, Yuqi Wang, Yuntao Chen, and Zhaoxiang Zhang. Freevs: Generative view synthesis on free driving trajectory. arXiv preprint arXiv:2410.18079, 2024. 2, 3, 6, 7, 8 10 [59] Xingyilang Yin, Qi Zhang, Jiahao Chang, Ying Feng, Qingnan Fan, Xi Yang, Chi-Man Pun, Huaqi Zhang, and Xiaodong Cun. Gsfixer: Improving 3d gaussian splatting with reference-guided video diffusion priors. arXiv preprint arXiv:2508.09667, 2025. 2, [60] Mark YU, Wenbo Hu, Jinbo Xing, and Ying Shan. Trajectorycrafter: Redirecting camera trajectory for monocarXiv preprint ular videos via diffusion models. arXiv:2503.05638, 2025. 3 [61] Wangbo Yu, Jinbo Xing, Li Yuan, Wenbo Hu, Xiaoyu Li, Zhipeng Huang, Xiangjun Gao, Tien-Tsin Wong, Ying Shan, and Yonghong Tian. Viewcrafter: Taming video diffusion models for high-fidelity novel view synthesis. arXiv preprint arXiv:2409.02048, 2024. 3 [62] Zehao Yu, Anpei Chen, Binbin Huang, Torsten Sattler, and Andreas Geiger. Mip-splatting: Alias-free 3d gaussian splatting. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 1944719456, 2024. 2 [63] David Junhao Zhang, Roni Paiss, Shiran Zada, Nikhil Karnad, David Jacobs, Yael Pritch, Inbar Mosseri, Mike Zheng Shou, Neal Wadhwa, and Nataniel Ruiz. Recapture: Generative video camera controls for user-provided videos using masked video fine-tuning. In Proceedings of the Computer Vision and Pattern Recognition Conference, pages 2050 2062, 2025. 2, 3 [64] Guosheng Zhao, Chaojun Ni, Xiaofeng Wang, Zheng Zhu, Xueyang Zhang, Yida Wang, Guan Huang, Xinze Chen, Boyuan Wang, Youyi Zhang, et al. Drivedreamer4d: World models are effective data machines for 4d driving scene representation. In Proceedings of the Computer Vision and Pattern Recognition Conference, pages 1201512026, 2025. 2 [65] Jingqiu Zhou, Lue Fan, Linjiang Huang, Xiaoyu Shi, Si Liu, Zhaoxiang Zhang, and Hongsheng Li. Flexdrive: Toward trajectory flexibility in driving scene gaussian splatting reconstruction and rendering. In Proceedings of the Computer Vision and Pattern Recognition Conference, pages 1549 1558, 2025. 2 [66] Zixin Zou, Weihao Cheng, Yan-Pei Cao, Shi-Sheng Huang, Ying Shan, and Song-Hai Zhang. Sparse3d: Distilling multiview-consistent diffusion for object reconstruction from sparse views. In Proceedings of the AAAI conference on artificial intelligence, pages 79007908, 2024. [47] Qinghe Wang, Yawen Luo, Xiaoyu Shi, Xu Jia, Huchuan Lu, Tianfan Xue, Xintao Wang, Pengfei Wan, Di Zhang, and Kun Gai. Cinemaster: 3d-aware and controllable framework for cinematic text-to-video generation. In Proceedings of the Special Interest Group on Computer Graphics and Interactive Techniques Conference Conference Papers, pages 110, 2025. 3 [48] Shuzhe Wang, Vincent Leroy, Yohann Cabon, Boris Chidlovskii, and Jerome Revaud. Dust3r: Geometric 3d vision made easy. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 20697 20709, 2024. 3 [49] Shuaixian Wang, Haoran Xu, Yaokun Li, Jiwei Chen, and Guang Tan. Ie-nerf: Inpainting enhanced neural radiance fields in the wild. arXiv preprint arXiv:2407.10695, 2024. 2 [50] Shuaixian Wang, Yaokun Li, Chenhui Guo, and Guang Tan. Learning hierarchical uncertainty from hybrid representations for neural active reconstruction. Pattern Recognition, page 112493, 2025. [51] Yuehao Wang, Chaoyi Wang, Bingchen Gong, and Tianfan Xue. Bilateral guided radiance field processing. ACM Transactions on Graphics (TOG), 43(4):113, 2024. 2 [52] Zhouxia Wang, Ziyang Yuan, Xintao Wang, Yaowei Li, Tianshui Chen, Menghan Xia, Ping Luo, and Ying Shan. Motionctrl: unified and flexible motion controller for video generation. In ACM SIGGRAPH 2024 Conference Papers, pages 111, 2024. 3 [53] Jay Zhangjie Wu, Yuxuan Zhang, Haithem Turki, Xuanchi Ren, Jun Gao, Mike Zheng Shou, Sanja Fidler, Zan Gojcic, and Huan Ling. Difix3d+: Improving 3d reconstrucIn Proceedings of tions with single-step diffusion models. the Computer Vision and Pattern Recognition Conference, pages 2602426035, 2025. 1, 2, 3, 6, [54] FU Xiao, Xian Liu, Xintao Wang, Sida Peng, Menghan Xia, Xiaoyu Shi, Ziyang Yuan, Pengfei Wan, Di Zhang, and Dahua Lin. 3dtrajmaster: Mastering 3d trajectory for multientity motion in video generation. In The Thirteenth International Conference on Learning Representations, 2024. 3 [55] Zeqi Xiao, Wenqi Ouyang, Yifan Zhou, Shuai Yang, Lei Yang, Jianlou Si, and Xingang Pan. Trajectory attention for fine-grained video motion control. arXiv preprint arXiv:2411.19324, 2024. 3 [56] Yunzhi Yan, Haotong Lin, Chenxu Zhou, Weijie Wang, Haiyang Sun, Kun Zhan, Xianpeng Lang, Xiaowei Zhou, and Sida Peng. Street gaussians: Modeling dynamic urban scenes with gaussian splatting. In European Conference on Computer Vision, pages 156173. Springer, 2024. 2 [57] Yunzhi Yan, Zhen Xu, Haotong Lin, Haian Jin, Haoyu Guo, Yida Wang, Kun Zhan, Xianpeng Lang, Hujun Bao, Xiaowei Zhou, et al. Streetcrafter: Street view synthesis with controllable video diffusion models. In Proceedings of the Computer Vision and Pattern Recognition Conference, pages 822832, 2025. 1, 2, 3, 6, 7 [58] Shiyuan Yang, Liang Hou, Haibin Huang, Chongyang Ma, Pengfei Wan, Di Zhang, Xiaodong Chen, and Jing Liao. Direct-a-video: Customized video generation with userdirected camera movement and object motion. In ACM SIGGRAPH 2024 Conference Papers, pages 112, 2024. 3 A. More Details of Data Construction C. More Comparisons We train 3D Gaussian Splatting (3DGS) on approximately 1.6K scenes from the official Waymo Open Dataset (WOD) [40] v1.4.3-train and NuScenes [7] v1.0-train. During training, we employ DriveStudio as the training framework to reconstruct scenes over their full temporal duration. For simplicity, we do not incorporate the SMPL model [28] for additional human body pose modeling, and we adopt default settings for hyperparameters such as the learning rate. For each 3DGS scene, we save 3DGS models at four training stages: after 100, 500, 1,000, and 30,000 iterations. The first three iterations are used to render underfitted videos along the original trajectory, serving as camera structural conditions during training. The final iteration (at 30,000 iterations) is used to render videos along offset trajectories, which act as source inputs during training. Visualizations of some training pairs are shown in Fig. 10. StreetCrafter is trained on Vista [11], while FreeVS is built upon Stable Video Diffusion (SVD) [6]. The frame lengths of their generated videos are summarized in Table 8. We use the officially released rollout inference code to generate full-length sequences on WOD. However, because both models generate only short temporal windows, the rollout procedure often introduces background inconsistencies and noticeable temporal jitter. For clearer assessment, please refer to the video results on our project page. D. More Qualitative Results More qualitative results are shown in Fig. 11 and Fig. 12, where our method maintains geometric consistency and visual fidelity while accurately generating novel trajectories. For clearer and more intuitive comparisons, we recommend viewing the video results on our project page."
        },
        {
            "title": "NuScenes",
            "content": "022 049 089 350 430 025 053 096 382 534 031 084 210 403 570 034 086 323 410 640 Table 7. Selected 20 evaluation sequences from NuScenes. Each row contains 4 sequence indices. B. Evaluation Dataset We evaluate our method on 20 scenes selected respectively from WOD and NuScenes, whose sequence names are listed in Table 9 and Table 7, respectively. For each scene, we use the recorded trajectory video as the source trajectory input and the novel-trajectory 3DGS renderings as the cameracontrol conditions. Specifically, following the same training setup as in DriveStudio, we train 3DGS representation for each scene for 30,000 iterations, and then render eight laterally offset novel trajectories (1 m, 2 m, 3 m, and 4 m) to serve as the structural camera conditions."
        },
        {
            "title": "FreeVS\nStreetCrafter\nOurs",
            "content": "SVD [6] Vista [11] Wan2.1 [45] 8 25 121 Table 8. Frame length comparison of different methods."
        },
        {
            "title": "File Names",
            "content": "segment-10444454289801298640 4360 000 4380 000 with camera labels.tfrecord segment-10498013744573185290 1240 000 1260 000 with camera labels.tfrecord segment-10588771936253546636 2300 000 2320 000 with camera labels.tfrecord segment-10625026498155904401 200 000 220 000 with camera labels.tfrecord segment-10963653239323173269 1924 000 1944 000 with camera labels.tfrecord segment-11017034898130016754 697 830 717 830 with camera labels.tfrecord segment-11846396154240966170 3540 000 3560 000 with camera labels.tfrecord segment-1191788760630624072 3880 000 3900 000 with camera labels.tfrecord segment-11928449532664718059 1200 000 1220 000 with camera labels.tfrecord"
        },
        {
            "title": "WOD",
            "content": "segment-12161824480686739258 1813 380 1833 380 with camera labels.tfrecord segment-16801666784196221098 2480 000 2500 000 with camera labels.tfrecord segment-18111897798871103675 320 000 340 000 with camera labels.tfrecord segment-6242822583398487496 73 000 93 000 with camera labels.tfrecord segment-1921439581405198744 1354 000 1374 000 with camera labels.tfrecord segment-2323851946122476774 7240 000 7260 000 with camera labels.tfrecord segment-1999080374382764042 7094 100 7114 100 with camera labels.tfrecord segment-4898453812993984151 199 000 219 000 with camera labels.tfrecord segment-4266984864799709257 720 000 740 000 with camera labels.tfrecord segment-175830748773502782 1580 000 1600 000 with camera labels.tfrecord segment-14561791273891593514 2558 030 2578 030 with camera labels.tfrecord Table 9. Selected 20 evaluation sequences with official file names from the WOD dataset. Figure 10. Visualization of training pairs. During training, we use the 3DGS renderings of novel trajectories at 30,000 iterations as the source inputs, the 3DGS renderings of the original trajectory at 100, 500, or 1,000 iterations as the camera conditions, and the clean recorded videos of the original trajectory as the supervision. 13 Figure 11. Qualitative results of our method on multiple novel-trajectory generations. Figure 12. Qualitative novel-trajectory results and 3DGS rendering conditions, with the two scenes shifted right by 3 and 4 m."
        }
    ],
    "affiliations": [
        "Shenzhen Polytechnic University",
        "Sun Yat-sen University",
        "The Hong Kong University of Science and Technology",
        "ZYT"
    ]
}