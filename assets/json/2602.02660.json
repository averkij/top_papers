{
    "paper_title": "MARS: Modular Agent with Reflective Search for Automated AI Research",
    "authors": [
        "Jiefeng Chen",
        "Bhavana Dalvi Mishra",
        "Jaehyun Nam",
        "Rui Meng",
        "Tomas Pfister",
        "Jinsung Yoon"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Automating AI research differs from general software engineering due to computationally expensive evaluation (e.g., model training) and opaque performance attribution. Current LLM-based agents struggle here, often generating monolithic scripts that ignore execution costs and causal factors. We introduce MARS (Modular Agent with Reflective Search), a framework optimized for autonomous AI research. MARS relies on three pillars: (1) Budget-Aware Planning via cost-constrained Monte Carlo Tree Search (MCTS) to explicitly balance performance with execution expense; (2) Modular Construction, employing a \"Design-Decompose-Implement\" pipeline to manage complex research repositories; and (3) Comparative Reflective Memory, which addresses credit assignment by analyzing solution differences to distill high-signal insights. MARS achieves state-of-the-art performance among open-source frameworks on MLE-Bench under comparable settings, maintaining competitiveness with the global leaderboard's top methods. Furthermore, the system exhibits qualitative \"Aha!\" moments, where 63% of all utilized lessons originate from cross-branch transfer, demonstrating that the agent effectively generalizes insights across search paths."
        },
        {
            "title": "Start",
            "content": "2026-2-4 MARS: Modular Agent with Reflective Search for Automated AI Research Jiefeng Chen1, Bhavana Dalvi Mishra1, Jaehyun Nam1, Rui Meng1, Tomas Pfister1 and Jinsung Yoon1 1Google Cloud AI Research Automating AI research differs from general software engineering due to computationally expensive evaluation (e.g., model training) and opaque performance attribution. Current LLM-based agents struggle here, often generating monolithic scripts that ignore execution costs and causal factors. We introduce MARS (Modular Agent with Reflective Search), framework optimized for autonomous AI research. MARS relies on three pillars: (1) Budget-Aware Planning via cost-constrained Monte Carlo Tree Search (MCTS) to explicitly balance performance with execution expense; (2) Modular Construction, employing Design-Decompose-Implement pipeline to manage complex research repositories; and (3) Comparative Reflective Memory, which addresses credit assignment by analyzing solution differences to distill high-signal insights. MARS achieves state-of-the-art performance among open-source frameworks on MLE-Bench under comparable settings, maintaining competitiveness with the global leaderboards top methods. Furthermore, the system exhibits qualitative Aha! moments, where 63% of all utilized lessons originate from cross-branch transfer, demonstrating that the agent effectively generalizes insights across search paths. 1. Introduction The integration of Large Language Models (LLMs) into software engineering has fundamentally transformed code generation, evolving from simple auto-completion to autonomous agents capable of resolving GitHub issues Jimenez et al. (2023); Yang et al. (2024) and generating functional scripts (Jiang et al., 2025; Li et al., 2022; Wang et al., 2024). However, while current agents excel at general software maintenance tasks such as patching bugs or writing unit tests they face significant hurdles when applied to the domain of Automating AI Research (Chan et al., 2024; Starace et al., 2025; Tian et al., 2024; Wijk et al., 2024; Yamada et al., 2025). Unlike standard software development, where correctness is often binary and verification is computationally cheap, AI research is probabilistic, resource-intensive endeavor. It requires not only coding intelligence but also the strategic foresight to navigate landscape defined by computationally expensive evaluations, opaque performance attribution, and high architectural complexity. Existing agentic frameworks, designed primarily for monolithic code generation, struggle to adapt to these constraints. First, they typically view problem-solving as purely code-based challenge (Huang et al., 2023; Jiang et al., 2025; Toledo et al., 2025), ignoring the economic reality of research: model training and data processing consume vast computational resources. An agent that improves model accuracy by 0.1% but increases training time from one hour to ten hours is often practically useless, yet standard search algorithms would prioritize it. Second, the monolithic and unstructured scripts often produced by previous LLM agents are fragile and ill-suited for the modular complexity required in research repositories, where data loading, model architecture, and training loops must interact seamlessly. Finally, research progress is iterative and opaque; when new experiment yields better results, it is difficult to isolate the causal factor. Standard memory-based agents (Ouyang et al., 2025; Packer et al., 2023; Shinn et al., 2023; Xu et al., 2025) lack the mechanism to solve this credit assignment problem, often failing to learn effectively from past trials. 6 2 0 2 2 ] . [ 1 0 6 6 2 0 . 2 0 6 2 : r MARS: Modular Agent with Reflective Search for Automated AI Research Figure 1 The Aha! moment of MARS on the challenging iMet-2020-FGVC7 task. The visualization tracks validation performance gains triggered by specific strategic lessons. While existing methods fail to reach medal-level performance, MARS progressively refines its strategy evolving from lightweight residual network to model ensemble techniques to ultimately achieve silver medal. To bridge this gap, we introduce MARS (Modular Agent with Reflective Search), framework explicitly optimized for the distinct constraints of autonomous scientific discovery. MARS reformulates the research process as search for an optimal software repository, governed by three core pillars. To address the high cost of evaluation, we employ Budget-Aware Planning via cost-constrained Monte Carlo Tree Search (MCTS). Unlike general search algorithms, our method explicitly balances performance maximization with execution expense, prioritizing efficient solutions such as favoring 1-hour training run over 4-hour run if performance is comparable to optimize the discovery rate within fixed budget. To manage architectural complexity, we replace fragile scripting with Modular Design-Decompose-Implement pipeline. This structure employs specialized agents to architect solutions into independent, testable modules. Finally, to resolve the credit assignment problem, we introduce Comparative Reflective Memory. By analyzing the differences between the current solution and the best-known solution, the agent distills high-signal, causal insights, isolating the specific factors driving performance shifts in way that standard memory mechanisms cannot. As illustrated in Figure 1, these pillars allow MARS to experience Aha! moments during long-horizon exploration, successfully navigating complex optimization landscapes where baselines fail. Our contributions are summarized as follows: We introduce MARS, framework designed for automated AI research, featuring novel combination of Budget-Aware MCTS, modular implementation pipeline, and Comparative Reflective Memory. We perform extensive evaluation on the MLE-Bench benchmark, where MARS achieves stateof-the-art performance among open-source frameworks under comparable settings. Ablation studies further validate the necessity of each proposed mechanism. We provide qualitative analyses of how MARS drives long-horizon exploration. To facilitate future research, we release prompts in Appendix F, and MARS generated code, trajectories in https://github.com/jfc43/MARS. 2. Related Work Automated AI Research & Engineering. Recent advancements in LLMs have enabled autonomous agents to tackle complex, long-horizon AI research problems, including Machine Learning Engineering (MLE) (Chan et al., 2024), Research Engineering (Wijk et al., 2024), and Automated Research Replication (Starace et al., 2025). While numerous agentic frameworks have been proposed to 2 MARS: Modular Agent with Reflective Search for Automated AI Research System Modular? Budget-Aware Search? Memory Mechanism AIDE (Jiang et al., 2025) MLE-STAR (Nam et al., 2025) AIRA (Toledo et al., 2025) R&D-Agent (Yang et al., 2025) ML-Master 2.0 (Zhu et al., 2026) MARS (Ours) All previous designs, scores, and notes Some previous plans, code and results Scoped Memory: some previous designs, scores, and notes Collaborative Memory: previous solutions, results, and insights Hierarchical Cognitive Caching: scripts, facts, strategies Comparative Reflective Memory: solution & debug lessons Table 1 Comparison of MLE agents in terms of: 1) Do they generate modular code? 2) Do the agents take into account runtime/budget during search? 3) What types of memory mechanisms do they use to enhance performance on given task? (: yes, : no). address these challenges (Jiang et al., 2025; Li et al., 2025; Liu et al., 2025; Nam et al., 2025; Team et al., 2025; Toledo et al., 2025; Yang et al., 2025; Zhu et al., 2026), existing systems predominantly operate under monolithic paradigm, generating expansive, single-file scripts. This approach typically results in fragile codebases that lack the modularity essential for rigorous engineering. MARS departs from this by enforcing repository-level paradigm that systematically decomposes tasks into distinct, testable, and maintainable modules, mirroring professional software architecture. Search Algorithms in Code Generation. Solving long-horizon AI research problems, where code execution is resource-intensive, necessitates effective search strategies for code optimization. While various algorithms have been adapted for these systems including greedy search (Jiang et al., 2025), Monte Carlo Tree Search (MCTS) (Kocsis and SzepesvÃ¡ri, 2006; Liu et al., 2025), and Evolutionary search (Team et al., 2025) they typically optimize solely for task performance, neglecting computational cost. In contrast, we introduce Budget-aware MCTS. By employing an efficiency-guided reward function, our framework balances the exploitation of high-performing strategies with the exploration of novel ideas, penalizing computationally expensive solutions to ensure both performance and efficiency. Reflective Learning and Memory. Enabling agents to improve iteratively through environmental interaction is rapidly evolving research area. Approaches such as Reflexion (Shinn et al., 2023) enable self-correction via verbal reinforcement derived from prior mistakes, Zhu et al. (2026) propose Hierarchical Cognitive Caching to distill execution traces into stable knowledge, while Jansen et al. (2025) cache useful codeblocks for future reuse. MARS advances this by introducing Lesson Learning. Distinct from prior methods that primarily summarize execution logs and focus on debugging errors, our approach explicitly analyzes the causal link between code changes and performance variations. This comparative analysis isolates effective algorithmic changes from confounding factors, distilling high-value insights into lesson pool to guide future exploration. Table 1 summarizes the key differences between MARS and existing MLE agent frameworks. 3. Problem We first formalize the general problem of Long-Horizon Agentic Problem Solving, where an autonomous agent is tasked with constructing complex artifacts (e.g., software system) to satisfy set of requirements within constrained budget. Let denote problem instance defined by the tuple = (I, E, O), where: (1) represents the Instruction or requirements provided in natural language. (2) denotes the Environment with which the agent interacts to validate its solutions. This can be compiler, simulator, or dataset depending on the task scenario. (3) is the Objective function that quantifies the quality of the solution. The goal is to find solution ğ‘  that maximizes by interacting with E, subject to cost constraint 3 MARS: Modular Agent with Reflective Search for Automated AI Research Figure 2 Overview of the MARS Framework. MARS reformulates long-horizon coding as search for an optimal software repository. (1) Task Preparation: The agent grounds the abstract problem (Instruction, Environment, Objective) tuple by exploratory analysis of the given dataset and metadata. (2) The MARS Loop: The agent iteratively evolves solutions through three synergistic modules: (A) Resource-Aware Planning: Budget-Aware MCTS strategically navigates the search space by selecting actions from {Draft new architecture, Debug runtime errors, Improve valid solution}. It optimizes an efficiency-guided reward that explicitly balances performance maximization with the penalty of high execution costs. (B) Modular Decomposition: To replace fragile monolithic scripting, the system employs Design-Decompose-Implement pipeline. Specialized {Idea, Modular, Coding} agents architect the solution into independent, testable modules. This structure enables precise Diff-Based Refinement, allowing the agent to update specific logic blocks without regenerating the entire codebase. (C) Reflective Memory: This module distills raw execution logs into structured Debugging and Solution Lessons to proactively prevent error repetition and accelerate convergence in later iterations. ğµ (e.g., time budget or monetary cost): ğ‘  = arg max ğ‘  (ğ‘ , E), ğ‘ .ğ‘¡. Cost(ğ‘ ) ğµ (1) where the search space for ğ‘  is often vast and unstructured (e.g., the space of all possible Python programs). MLE Task Scenario. Machine Learning Engineering (MLE) is representative and challenging instantiation of this general problem class. MLE requires the agent to engineer full pipeline that processes data, trains models, and validates results. In this scenario, consists of the provided datasets while is the performance metric (e.g. accuracy) on the held-out test set. Refer to Appendix for details. MARS: Modular Agent with Reflective Search for Automated AI Research 4. Method 4.1. Overall Framework We propose MARS, general agent scaffolding framework designed to enable autonomous agents to solve long-horizon AI Research problems, as illustrated in Figure 2. Formally, we define the problem as tuple = (I, E, O), where the agent must follow the instruction within an environment to maximize an objective under cost budget ğµ. To address the core challenges of exploration complexity, context management, and solution robustness in this setting, our framework integrates three key capabilities: Modular Construction Strategy: Instead of generating monolithic scripts, we enforce structured, repository-level software architecture. This paradigm allows for handing complex logic with greater accuracy, efficient code reuse, and improving testability. Reflective Memory: To overcome context window limitations, we introduce Lesson Learning mechanism that distills high-value insights from past interactions (both successes and failures) into compact, retrievable knowledge base. Resource-Aware Planning: We employ budget-aware Monte Carlo Tree Search (MCTS) algorithm to systematically explore the solution space. This allows the system to balance the exploitation of promising candidates with the exploration of novel ideas, preventing local optima, and penalize solutions that are costly. 4.2. Modular Decomposition primary contribution of this work is the strategic shift from generating monolithic scripts to Modular Implementation paradigm. This paradigm addresses several inherent limitations of LLMbased coding. First, it bypasses token output limits by distributing code across multiple files. Second, it enhances precision; by focusing on smaller logical units, the agent encounters less context noise and can handle complex logic with greater accuracy. Third, it enables efficiency via caching, as validated modules can be reused without regeneration. Finally, it significantly improves testability, as debugging is localized to specific files rather than requiring full-script diagnosis. We define node solution ğ‘ ğ‘› as tuple comprising set of ğ‘™ independent modules and one orchestration script: ğ‘ ğ‘› = {M ğ‘—}ğ‘™ ğ‘—=1 , ğœ‹ğ‘šğ‘ğ‘–ğ‘› (2) Each module ğ‘— encapsulates specific sub-task (e.g., data preprocessing, configuration), while the main script ğœ‹main orchestrates the end-to-end pipeline. To instantiate this structure, we employ three-stage Design-Decompose-Implement workflow: Idea Generation: An Idea Generation Agent articulates comprehensive natural language plan covering various aspects of the solution. Module Decomposition: Modular Agent parses the plan and decomposes the solution into logical, independent functional modules. Component Implementation and Debugging: Coding Agent sequentially implements each module ğ‘—, employing validation script to debug and verify functionality. Once validated, the agent orchestrates the modules via the main script ğœ‹main. To prevent wasteful full-repository regeneration, we adopt Diff-Based Editing mechanism. Code MARS: Modular Agent with Reflective Search for Automated AI Research modifications are structured in standardized diff format, specifying the target file, the block to replace, and the new code. This enables atomic, multi-file updates in single inference step. 4.3. Lesson Learning Solving complex tasks requires long-horizon exploration, generating extensive interaction trajectories that often exceed context window constraints. To address this, we propose Lesson Learning, generic Reflective Memory mechanism to distill high-value insights from the exploration process into compact lesson pool. We categorize these insights into two types: Solution Improvement Lessons. We employ two-stage process to synthesize lessons from valid solutions. First, an Empirical Analysis Agent reviews execution logs to extract objective findings (e.g., loss trends, metric validity). Subsequently, Lesson Distillation Agent compares the new solution against the current best, distilling structured lesson containing: (1) Algorithmic changes, (2) Impact analysis, and (3) generalized rule for future iterations. Debugging Lessons. For failed executions, dedicated agent analyzes the buggy code, error logs, and the applied fix. It outputs lesson confirming the fixs efficacy, explaining the failure logic, and providing guidelines to preemptively identify similar errors. Lesson Management. To maintain high-signal lesson pool, Review Agent evaluates new lessons against the existing pool through LLM-based reasoning, filtering out redundant insights to ensure the retrieved context remains diverse and relevant. Lesson Utilization. When executing solution improvement or debugging actions, the agent utilizes relevant knowledge from the corresponding lesson categories. We retain the ğ¾ğ‘š most recent lessons in the agents memory to manage context. To ensure interpretability, the agent is instructed to explicitly cite specific lessons whenever they are applied. 4.4. Budget-Aware MCTS We adopt the Monte Carlo Tree Search (MCTS) framework to explore the solution space, which iterates through four phases: Selection, Expansion, Simulation, and Backpropagation. In this section, we detail our domain-specific modifications: (1) specialized expansion operators, (2) coherent node selection strategy, and (3) an Efficiency-Guided Reward Function that balances performance with cost. Appendix provides review of standard MCTS principles. 4.4.1. Actions and Expansion We define three distinct operators to transform parent state ğ‘ ğ‘ğ‘ğ‘Ÿğ‘’ğ‘›ğ‘¡ into child solution ğ‘ ğ‘›ğ‘’ğ‘¤: Drafting (Root Expansion): Generates completely new solution ğ‘ ğ‘›ğ‘’ğ‘¤ from scratch. Improvement: Applied to valid, executable nodes. The agent modifies the modules and the main script from ğ‘ ğ‘ğ‘ğ‘Ÿğ‘’ğ‘›ğ‘¡ to maximize the objective O. Debugging: Applied to nodes where execution failed. The agent inherits the solution structure from ğ‘ ğ‘ğ‘ğ‘Ÿğ‘’ğ‘›ğ‘¡ but modifies specific modules or the orchestration script to resolve runtime errors. 6 MARS: Modular Agent with Reflective Search for Automated AI Research Buggy children enter an automatic debugging loop with up to ğ‘ğ‘‘ debugging actions to fix the errors. 4.4.2. Node Selection We employ the Upper Confidence Bound for Trees (UCT) algorithm to navigate the solution space, balancing the exploitation of high-performing solutions with the exploration of new solutions. The selection phase begins at the root node. In each step, we select the child node that maximizes the UCT value. This traversal continues recursively until we identify candidate node, defined as node that is not yet fully expanded. The root node is set fully expanded unless any of the follow condition occurs: (1) It does not have any children; (2) the best solution has not been improved after implementing ğ‘›ğ‘  valid nodes. If the traversal reaches leaf node that is already fully expanded (it implies that no further debugging or improvement is permitted for that branch), then the root node is re-activated to allow for new drafts. The buggy nodes are always set fully expanded. The valid nodes are set fully expanded if they have ğ‘ğ‘– children (attempts to improve). 4.4.3. Efficiency-Guided Reward Function To guide the search efficiently, we design reward function ğ‘…(ğ‘£) that rewards performance gains and penalizes long execution time. Let ğ‘€ (ğ‘£) denote the performance metric of node ğ‘£, and let ğ‘¡(ğ‘£) and ğ¿(ğ‘£) represent its execution time and time limit, respectively. We first normalize the performance metric relative to the history of explored nodes V. Let ğ‘€ğ‘šğ‘ğ‘¥ = maxğ‘£ ğ‘€ (ğ‘£) and ğ‘€ğ‘šğ‘–ğ‘› = minğ‘£ ğ‘€ (ğ‘£). We define the global normalized score ğº(ğ‘£) as: (cid:40) ğº(ğ‘£) := 0.5 if ğ‘€ğ‘šğ‘ğ‘¥ = ğ‘€ğ‘šğ‘–ğ‘›, ğ‘€ (ğ‘£) ğ‘€ğ‘šğ‘–ğ‘› otherwise ğ‘€ğ‘šğ‘ğ‘¥ ğ‘€ğ‘šğ‘–ğ‘› (3) To incorporate budget constraints, we modulate this score by execution latency, defining efficiencyguided reward as: ğ‘…(ğ‘£) := ğº(ğ‘£) [ğ‘¡(ğ‘£)/ğ¿(ğ‘£)]ğ‘¤ (4) Where ğ‘¤ is penalty weight hyperparameter. similar function has been proposed in Tan et al. (2019). 4.5. Task Specific Components While MARS is general framework, its application requires task-specific components. For Machine Learning Engineering (MLE) tasks, we integrate the following: Task preparation. We employ multi-agent system to extract task metadata, formalizing the optimization objective and preparing training, validation, and test datasets. 7 MARS: Modular Agent with Reflective Search for Automated AI Research Data analysis. We employ an agent to perform Exploratory Data Analysis (EDA) to generate report that guides downstream feature engineering. Curriculum-Based Exploration. We implement curriculum-based idea generation strategy that progressively explores simple baselines to complex methods. Refer to the Appendix for the details. 5. Experiment 5.1. Setup Datasets. We evaluate our agent on MLE-Bench (Chan et al., 2024), which consists of 75 challenging competitions from Kaggle, forming diverse collection of tasks covering natural language processing, computer vision, and tabular data analysis. Environments. We adhere to the standard MLE-Bench protocol, where agents are allocated strict 24-hour wall-clock time budget per competition. This budget encompasses the entire pipeline, including dataset preparation, feature engineering, model training, and inference. The experiment for each agent on each competition is conducted on standard node equipped with one NVIDIA A100 GPU (40GB), 12 vCPUs, 220 GB of RAM, and 1 TB of SSD storage. This setup simulates realistic, resource-constrained machine learning engineering environment. Baselines. We compare our method to the agents in the MLE-Bench leaderboard 1 and two state-of-the-art open-source agents: AIDE (Jiang et al., 2025) and AIRA (Toledo et al., 2025). For open-source baselines, we ensure strictly fair comparison by running them under identical environment configurations and using the same underlying LLMs. Metrics. Following the standard MLE-Bench evaluation protocol, we report the mean and standard error of the mean (SEM) across three independent runs. Our evaluation focuses on three primary metrics: Above Median Rate (percentage of runs outperforming the median participant), Any Medal Rate (percentage achieving at least Bronze medal), and Gold Medal Rate (percentage securing Gold medal). Hyper-parameters for MARS. We set the maximum number of lessons in the agents memory to ğ¾ğ‘š = 30 to maintain relevant context without context window overflow. We allow up to ğ‘ğ‘‘ = 10 debugging actions per failure to resolve runtime errors effectively. The branching factor for valid nodes is set to ğ‘ğ‘– = 2, balancing exploration breadth with depth. We set ğ‘¤ = 0.07 in the reward function (4) following Tan et al. (2019) to penalize excessive execution time (refer to Appendix E.2 for sensitivity analysis of ğ‘¤). 5.2. Main Results We compare MARS against state-of-the-art baselines in Table 2. In the controlled evaluation, MARS establishes new state-of-the-art among open-source frameworks, significantly outperforming AIDE and AIRA-dojo under identical constraints. When compared to the official leaderboard, our method remains highly competitive despite using significantly fewer resources (see Appendix for setup disparities). Notably, the standard MARS achieves the highest Gold Medal rate (31.1%) among all reported agents. To assess scalability, we evaluate MARS+, variant configured to execute two concurrent search trees with increased compute (2A100 GPUs and 24 vCPUs). This scaled approach achieves the highest Above Median rate (73.3%) and Any Medal rate (59.6%), surpassing 1https://github.com/openai/mle-bench/tree/main MARS: Modular Agent with Reflective Search for Automated AI Research Table 2 Performance comparison on MLE-Bench. Results are reported as mean SEM across three independent runs. All values are in percentages (%). Bold and underlined values denote the best and second-best performance, respectively. Refer to Appendix for detailed comparison of evaluation setups. Agent Model Valid Submission Above Median Bronze Silver Gold Any Medal Official MLE-Bench Leaderboard Results ML-Master (Liu et al., 2025) Deepseek-R1 93.3 1.3 44.9 1.2 4.4 0.9 7.6 0.4 17.3 0.8 29.3 0. R&D-Agent (Yang et al., 2025) GPT-5 53.3 0.0 40.4 0.9 6.7 1.5 12.0 0.8 16.4 0.9 35.1 0. InternAgent (Team et al., 2025) Deepseek-R1 96.4 0.4 48.4 1.2 7.1 1.6 10.7 0.8 18.7 0.8 36.4 1.2 Famou-Agent (Li et al., 2025) Gemini-2.5-Pro 96.9 1.2 51.6 1.2 8.4 0.4 12.4 1.9 22.7 0.8 43.6 0.9 Leeroo (Nadaf et al., 2025) Gemini-3-Pro-Preview 50.7 1.3 50.7 1.3 14.2 1.2 15.1 0.9 21.3 2.0 50.7 1.3 ML-Master 2.0 (Zhu et al., 2026) Deepseek-V3.2-Speciale 95.6 1.2 63.1 1.2 11.1 0.4 25.8 2.5 19.6 0.9 56.4 2. AIDE (Jiang et al., 2025) AIRA-dojo (Toledo et al., 2025) MARS (ours) MARS+ (ours) Controlled Evaluation in Our Environment Gemini-2.5-Pro Gemini-3-Pro-Preview Gemini-2.5-Pro Gemini-3-Pro-Preview Gemini-2.5-Pro Gemini-3-Pro-Preview 84.4 0.4 82.7 0.8 83.6 2.4 98.2 1.2 94.2 0.4 98.7 0.0 40.0 0.8 48.0 0. 38.7 0.8 55.6 1.2 5.8 0.9 4.9 0.4 2.7 0.8 5.8 1.9 4.9 1.2 12.4 0.9 23.1 0.4 11.1 1.2 16.4 1.8 32.4 2.5 6.7 2.3 8.0 0.8 15.1 1.2 24.4 1.2 24.0 1.5 37.8 2. 52.4 0.9 11.6 1.9 12.4 0.9 19.1 0.4 43.1 1.6 15.6 1.2 31.1 0.4 56.0 1.5 65.8 1.6 9.3 0.0 Gemini-3-Pro-Preview 98.7 0.0 73.3 0.8 10.2 0.9 18.7 1.5 30.7 1.3 59.6 1.2 resource-intensive competitors like ML-Master 2.0. Finally, Table 3 decomposes performance by task complexity, demonstrating that MARS consistently outperforms baselines across the Lite, Medium, and High splits. 5.3. Ablation Study We conduct ablation studies for MARS on the MLE-Bench Lite containing 22 competitions. Figure 3 illustrates the performance of MARS versus variants lacking the Modular Decomposition or Lesson Learning component. The results demonstrate that both techniques significantly contribute to the agents overall success. Figure 4 compares different tree search algorithms for MARS. Greedy Search selects the node with the best validation metric for expansion at each step, while Vanilla MCTS is Table 3 Controlled evaluation in our environment across different splits of MLE-Bench. Results are reported as mean SEM across three independent runs. The best performance is highlighted in bold, and the second-best is underlined. The complete results including leaderboard results and other metrics are in Appendix E.1."
        },
        {
            "title": "Agent Model",
            "content": "Any Medal Lite (%) Medium (%) High (%)"
        },
        {
            "title": "AIDE",
            "content": "36.4 4.5 Gemini-2.5-Pro Gemini-3-Pro-Prev 53.0 6.1 40.9 2.6 AIRA Gemini-2.5-Pro -dojo Gemini-3-Pro-Prev 56.1 1.5 68.2 2.6 MARS Gemini-2.5-Pro (ours) Gemini-3-Pro-Prev 74.2 1.5 18.4 2.6 26.3 3.0 16.7 3.5 29.8 3.8 33.3 1.8 52.6 3. 15.6 2.2 17.8 2.2 20.0 0.0 31.1 4.4 31.1 2.2 37.8 2.2 9 MARS: Modular Agent with Reflective Search for Automated AI Research Figure 3 Impact of Modular Decomposition and Lesson Learning. Figure 4 Comparison of tree search strategies for MARS. variant of Budget-aware MCTS where ğ‘¤ = 0 in Eq (4). The results indicate that the proposed BudgetAware MCTS consistently yields superior performance over time compared to others, effectively balancing exploration with resource constraints. 6. Discussions How does Modular Decomposition impact solution complexity? We investigate whether Modular Decomposition facilitates the construction of complex solutions for each task. Table 4 compares the repository statistics of MARS with and without modular decomposition for the best solution. The results show that the modular approach encourages the generation of more extensive and structured codebases (measured by lines of code and number of files in the best solution). To illustrate this structural adaptability, Table 5 enumerates the specific modules synthesized for five representative competitions. The diversity of these modules tailored to specific sub-tasks such as preprocessing and model architecture demonstrates the agents ability to decompose intricate problems into logical components. This capacity to architect organized, repository-level solutions closely mirrors 10 MARS: Modular Agent with Reflective Search for Automated AI Research Table 4 Comparison of repository statistics between MARS and the variant without Modular Decomposition on MLE-Bench Lite."
        },
        {
            "title": "Lines of Code\nNumber of Files",
            "content": "474.8 13.5 1.0 0.0 1103.9 35.9 6.7 0.1 professional software engineering workflows. Table 5 Modules generated by MARS on challenging competitions. Competition Modules aptos2019-blindness-detection jigsaw-toxic-comment-classification-challenge us-patent-phrase-to-phrase-matching h-and-m-personalized-fashion-recommendations multi-modal-gesture-recognition dataset.py, engine.py, model.py, utils.py data_processing.py, model_definitions.py, training_engine.py, utils.py config.py, cpc_utils.py, dataset.py, engine.py, loss.py, model.py, utils.py config.py, data_factory.py, embedder.py, features.py, ranker.py, retrieval.py config.py, data_loader.py, inference.py, losses.py, model.py, trainer.py, utils.py Figure 5 Reward modulation: Budget-aware MCTS assigns higher rewards to faster candidates when performance is comparable. Does Budget-aware MCTS improve exploration? We examine whether Budget-aware MCTS discovers high-quality solutions more frequently than the Vanilla MCTS. We define the effective solution rate as the proportion of explored solutions that improve upon the current best validation metric per task. Empirically, Budget-aware MCTS achieves an effective solution rate of 19.5% 1.5%, notably higher than the 16.1% 1.3% observed with Vanilla MCTS. This suggests that the latency penalty acts as heuristic to prune inefficient trajectories. As illustrated in Figure 5, when the agent encounters solutions with comparable accuracy but differing costs, our efficiency-guided reward favors the faster candidate. This bias directs computational resources toward efficient nodes, accelerating the discovery of optimal solutions within the time limit. How lessons guide the evolution process? We examine the role of Lesson Learning in guiding the agents solution exploration. Figure 1 illustrates an example where the agent formulates lessons from early failures or partial successes and applies them to refine subsequent solutions. To quantify MARS: Modular Agent with Reflective Search for Automated AI Research this behavior, we introduce two metrics: the lesson-utilization rate (the proportion of solutions that incorporate existing lessons) and the lesson-transfer rate (the proportion of utilized solution lessons originating from different tree branch). MARS achieves lesson-utilization rate of 65.8% 1.1% and lesson-transfer rate of 63.0% 1.8% on MLE-Bench. These results demonstrate that the agent actively leverages learned knowledge and cross-branch transfer to steer the search toward high-quality strategies. Figure 6 Distribution of maximum code similarity scores for medal-winning submissions from AIRA-dojo and MARS, compared against top public Kaggle notebooks. Does MARS follow the MLE-Bench rules? To verify compliance, we employ the official MLE-Bench log analysis tool, which utilizes gpt-4.1-mini to audit the logs and code outputs of all medal-winning submissions. The evaluation confirms that MARS strictly adheres to the protocol, registering 0% violation rate across all monitored dimensions, including Tried to access unauthorized resources, Tried to call external LLM API service, and Manually-written submission. Furthermore, we assess code originality using the provided plagiarism detection tool based on Dolos (Maertens et al., 2024). We calculate the maximum similarity score between our agents code concatenated into single file for multi-module repositories and the top public notebooks for each competition. As shown in Figure 6, the similarity distribution of MARS mirrors that of the baseline AIRA-dojo. Crucially, no submission exceeds 60% similarity threshold, demonstrating that MARS generates distinct, original solutions rather than reproducing existing public code. Cost Analysis. As detailed in Appendix E.3, MARS incurs higher cost per task due to the maintenance of comprehensive memory context ($39.0 for AIRA-dojo vs $60.5 for MARS). However, this investment yields substantial returns: the Any Medal Rate nearly doubles from 24.4% for AIRA-dojo to 43.1% for MARS, justifying the expense through superior efficacy. 7. Conclusion In this work, we addressed the limitations of current autonomous agents in Long-Horizon AI Research by introducing MARS. Unlike traditional code generation approaches, our framework treats research as rigorous, repository-level engineering challenge. By integrating Resource-Aware Planning via Budget-Aware MCTS, Modular Construction, and Reflective Memory, MARS effectively resolves the credit assignment problem while balancing exploration with computational efficiency. Our extensive 12 MARS: Modular Agent with Reflective Search for Automated AI Research evaluation on MLE-Bench demonstrates that this structured approach mimicking the strategic foresight of human engineers enables state-of-the-art performance in complex Machine Learning Engineering tasks. Future work will focus on extending MARS to broader scientific discovery domains and optimizing the frameworks economic viability through advanced context caching and early stopping mechanisms."
        },
        {
            "title": "Impact Statement",
            "content": "MARS contributes to the advancement of autonomous AI agents. While our work aims to enhance the reliability and efficiency of automated software engineering, we acknowledge potential broader impacts. The deployment of LLM-based agents involves risks related to the generation of incorrect or hallucinatory code; we mitigate this through iterative self-correction with code execution feedback. We do not foresee immediate negative societal consequences beyond those generally associated with the advancement of generative AI."
        },
        {
            "title": "References",
            "content": "J. S. Chan, N. Chowdhury, O. Jaffe, J. Aung, D. Sherburn, E. Mays, G. Starace, K. Liu, L. Maksin, T. Patwardhan, et al. Mle-bench: Evaluating machine learning agents on machine learning engineering. arXiv preprint arXiv:2410.07095, 2024. Q. Huang, J. Vora, P. Liang, and J. Leskovec. Mlagentbench: Evaluating language agents on machine learning experimentation. arXiv preprint arXiv:2310.03302, 2023. P. Jansen, O. Tafjord, M. Radensky, P. Siangliulue, T. Hope, B. Dalvi Mishra, B. P. Majumder, D. S. Weld, and P. Clark. CodeScientist: End-to-end semi-automated scientific discovery with code-based experimentation. In W. Che, J. Nabende, E. Shutova, and M. T. Pilehvar, editors, Findings of the Association for Computational Linguistics: ACL 2025, Vienna, Austria, July 2025. Association for Computational Linguistics. Z. Jiang, D. Schmidt, D. Srikanth, D. Xu, I. Kaplan, D. Jacenko, and Y. Wu. Aide: Ai-driven exploration in the space of code. arXiv preprint arXiv:2502.13138, 2025. C. E. Jimenez, J. Yang, A. Wettig, S. Yao, K. Pei, O. Press, and K. Narasimhan. Swe-bench: Can language models resolve real-world github issues? arXiv preprint arXiv:2310.06770, 2023. L. Kocsis and C. SzepesvÃ¡ri. Bandit based monte-carlo planning. In European conference on machine learning, pages 282293. Springer, 2006. A. Li, C. Wu, Z. Ge, Y. H. Chong, Z. Hou, L. Cao, C. Ju, J. Wu, H. Li, H. Zhang, et al. The fm agent. arXiv preprint arXiv:2510.26144, 2025. Y. Li, D. Choi, J. Chung, N. Kushman, J. Schrittwieser, R. Leblond, T. Eccles, J. Keeling, F. Gimeno, A. Dal Lago, et al. Competition-level code generation with alphacode. Science, 378(6624):1092 1097, 2022. Z. Liu, Y. Cai, X. Zhu, Y. Zheng, R. Chen, Y. Wen, Y. Wang, S. Chen, et al. Ml-master: Towards ai-for-ai via integration of exploration and reasoning. arXiv preprint arXiv:2506.16499, 2025. R. Maertens, M. Van Neyghem, M. Geldhof, C. Van Petegem, N. Strijbol, P. Dawyndt, and B. Mesuere. Discovering and exploring cases of educational source code plagiarism with dolos. SoftwareX, 26: 101755, 2024. 13 MARS: Modular Agent with Reflective Search for Automated AI Research A. Nadaf, A. Mohammadshahi, M. Yazdani, and Leeroo Coding Agent. Kapso: knowledge-grounded framework for autonomous program synthesis and optimization, 2025. URL https://github. com/leeroo-ai/kapso. J. Nam, J. Yoon, J. Chen, J. Shin, S. Ã–. ArÄ±k, and T. Pfister. Mle-star: Machine learning engineering agent via search and targeted refinement. arXiv preprint arXiv:2506.15692, 2025. S. Ouyang, J. Yan, I. Hsu, Y. Chen, K. Jiang, Z. Wang, R. Han, L. T. Le, S. Daruki, X. Tang, et al. Reasoningbank: Scaling agent self-evolving with reasoning memory. arXiv preprint arXiv:2509.25140, 2025. C. Packer, V. Fang, S. Patil, K. Lin, S. Wooders, and J. Gonzalez. Memgpt: Towards llms as operating systems. 2023. N. Shinn, F. Cassano, A. Gopinath, K. Narasimhan, and S. Yao. Reflexion: Language agents with verbal reinforcement learning. Advances in Neural Information Processing Systems, 36:86348652, 2023. G. Starace, O. Jaffe, D. Sherburn, J. Aung, J. S. Chan, L. Maksin, R. Dias, E. Mays, B. Kinsella, W. Thompson, et al. Paperbench: Evaluating ais ability to replicate ai research. arXiv preprint arXiv:2504.01848, 2025. M. Tan, B. Chen, R. Pang, V. Vasudevan, M. Sandler, A. Howard, and Q. V. Le. Mnasnet: Platform-aware neural architecture search for mobile. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 28202828, 2019. N. Team, B. Zhang, S. Feng, X. Yan, J. Yuan, Z. Yu, X. He, S. Huang, S. Hou, Z. Nie, et al. Novelseek: When agent becomes the scientistbuilding closed-loop system from hypothesis to verification. arXiv preprint arXiv:2505.16938, 2025. M. Tian, L. Gao, S. Zhang, X. Chen, C. Fan, X. Guo, R. Haas, P. Ji, K. Krongchon, Y. Li, et al. Scicode: research coding benchmark curated by scientists. Advances in Neural Information Processing Systems, 37:3062430650, 2024. E. Toledo, K. Hambardzumyan, M. Josifoski, R. Hazra, N. Baldwin, A. Audran-Reiss, M. Kuchnik, D. Magka, M. Jiang, A. M. Lupidi, et al. Ai research agents for machine learning: Search, exploration, and generalization in mle-bench. arXiv preprint arXiv:2507.02554, 2025. X. Wang, B. Li, Y. Song, F. F. Xu, X. Tang, M. Zhuge, J. Pan, Y. Song, B. Li, J. Singh, et al. Openhands: An open platform for ai software developers as generalist agents. arXiv preprint arXiv:2407.16741, 2024. H. Wijk, T. Lin, J. Becker, S. Jawhar, N. Parikh, T. Broadley, L. Chan, M. Chen, J. Clymer, J. Dhyani, et al. Re-bench: Evaluating frontier ai r&d capabilities of language model agents against human experts. arXiv preprint arXiv:2411.15114, 2024. W. Xu, Z. Liang, K. Mei, H. Gao, J. Tan, and Y. Zhang. A-mem: Agentic memory for llm agents. arXiv preprint arXiv:2502.12110, 2025. Y. Yamada, R. T. Lange, C. Lu, S. Hu, C. Lu, J. Foerster, J. Clune, and D. Ha. The ai scientistarXiv preprint v2: Workshop-level automated scientific discovery via agentic tree search. arXiv:2504.08066, 2025. J. Yang, C. E. Jimenez, A. Wettig, K. Lieret, S. Yao, K. Narasimhan, and O. Press. SWE-agent: Agentcomputer interfaces enable automated software engineering. Advances in Neural Information Processing Systems, 37:5052850652, 2024. 14 MARS: Modular Agent with Reflective Search for Automated AI Research X. Yang, X. Yang, S. Fang, Y. Zhang, J. Wang, B. Xian, Q. Li, J. Li, M. Xu, Y. Li, H. Pan, Y. Zhang, W. Liu, Y. Shen, W. Chen, and J. Bian. R&d-agent: An llm-agent framework towards autonomous data science, 2025. URL https://arxiv.org/abs/2505.14738. X. Zhu, Y. Cai, Z. Liu, B. Zheng, C. Wang, R. Ye, J. Chen, H. Wang, W.-C. Wang, Y. Zhang, et al. Toward ultra-long-horizon agentic science: Cognitive accumulation for machine learning engineering. arXiv preprint arXiv:2601.10402, 2026. MARS: Modular Agent with Reflective Search for Automated AI Research"
        },
        {
            "title": "Appendix",
            "content": "This Appendix is organized as follows: Appendix describes the MLE task scenario, while Appendix provides background on the standard MCTS algorithm. We detail the instantiation of MARS for MLE tasks in Appendix and contrast our evaluation setup with other agents in Appendix D. Finally, we provide additional experimental results (Appendix E), comprehensive agent prompts (Appendix F), and representative code examples generated by our system (Appendix G). A. MLE Task Scenario Machine Learning Engineering (MLE) is representative and challenging instantiation of this general problem class. MLE requires the agent not just to write snippet of code, but to engineer full pipeline that processes data, trains models, and validates results. We map the general problem = (I, E, O) to an MLE task ğ‘„ = (ğ¼, ğ·, ğ‘€), where: ğ¼ corresponds to the natural language task description (I). ğ· represents the datasets (ğ· = {ğ·ğ‘‘ğ‘’ğ‘£, ğ·ğ‘¡ğ‘’ğ‘ ğ‘¡}) which form the data environment (E). ğ‘€ is the evaluation metric (e.g., Accuracy, F1-score) defining the objective (O). Without loss of generality, we treat the optimization of ğ‘€ as maximization problem. If pre-defined validation set is not provided in the development set ğ·ğ‘‘ğ‘’ğ‘£, the agent must partition ğ·ğ‘‘ğ‘’ğ‘£ to create validation set ğ·ğ‘£ğ‘ğ‘™ for internal evaluation, as the test set ğ·ğ‘¡ğ‘’ğ‘ ğ‘¡ is strictly hidden. We aim to build an MLE agent that explores space of possible solutions and outputs final executable solution ğ‘ . We define the solution ğ‘  as structured software repository comprising the distinct code modules, dependencies, and entry points required to orchestrate the end-to-end pipeline. The performance of solution ğ‘  is quantified by the metric function ğ‘“ (ğ‘ , ğ·, ğ‘€) â„. While the ultimate goal is to maximize performance on the unseen test set ğ·ğ‘¡ğ‘’ğ‘ ğ‘¡, the agent must rely on proxy objective using the validation set ğ·ğ‘£ğ‘ğ‘™. The optimization objective becomes: ğ‘  = arg max ğ‘  SA ğ‘“ (ğ‘ , ğ·ğ‘¡ğ‘’ğ‘ ğ‘¡, ğ‘€), ğ‘ .ğ‘¡. ğ¶(ğ‘ ) ğ‘‡ (5) where ğ‘‡ is the wall-clock time budget, SA is the set of candidate solutions generated by agent given task ğ‘„, and ğ¶(ğ‘ ) denotes the total wall-clock time consumed by the agent to search for the solution ğ‘ . Since ğ·ğ‘¡ğ‘’ğ‘ ğ‘¡ is unobservable, the agent optimizes via ğ‘“ (ğ‘ , ğ·ğ‘£ğ‘ğ‘™, ğ‘€). B. Monte Carlo Tree Search (MCTS) Monte Carlo Tree Search (MCTS) is heuristic search algorithm for decision processes, most notably employed in game play. The algorithm builds search tree where each node ğ‘£ represents state ğ‘ , and each edge represents an action ğ‘ leading to new state. The value of state is estimated by simulating outcomes from that state. As shown in Algorithm 1, each MCTS iteration consists of four distinct phases: 1. Selection: Starting from the root node ğ‘£0, the algorithm recursively traverses down the tree by selecting child nodes according to selection policy, typically aiming to balance exploration 16 MARS: Modular Agent with Reflective Search for Automated AI Research Algorithm 1 Monte Carlo Tree Search (MCTS) 1: Input: Task P, Time Budget ğ‘‡. 2: Output: Best Solution Node ğ‘£ 3: Initialize root node ğ‘£0 with empty solution 4: ğ‘£ ğ‘£0 5: while Time used < ğ‘‡ do 6: ğ‘£ğ‘™ SELECT(ğ‘£0) {Tree Traversal using UCT} ğ‘£ğ‘›ğ‘’ğ‘¤ EXPAND(ğ‘£ğ‘™) {Apply Drafting/Improvement/Debugging} ğ‘… SIMULATE(ğ‘£ğ‘›ğ‘’ğ‘¤) {Execute and Evaluate Solution} BACKPROPAGATE(ğ‘£ğ‘›ğ‘’ğ‘¤, ğ‘…) {Update ğ‘„ and ğ‘ values} if ğ‘‰ ğ‘ğ‘™ğ‘€ğ‘’ğ‘¡ğ‘Ÿğ‘–ğ‘(ğ‘£ğ‘›ğ‘’ğ‘¤) > ğ‘‰ ğ‘ğ‘™ğ‘€ğ‘’ğ‘¡ğ‘Ÿğ‘–ğ‘(ğ‘£) then ğ‘£ ğ‘£ğ‘›ğ‘’ğ‘¤ 7: 8: 9: 10: 11: end if 12: 13: end while 14: return ğ‘£ and exploitation. common strategy is the Upper Confidence Bound for Trees (UCT) Kocsis and SzepesvÃ¡ri (2006): ğ‘ = arg max ğ‘ (ğ‘ ) (cid:169) (cid:173) (cid:171) ğ‘„(ğ‘ , ğ‘) + ğ‘ğ‘¢ğ‘ğ‘¡ ln ğ‘ (ğ‘ ) ğ‘ (ğ‘ , ğ‘) (cid:170) (cid:174) (cid:172) (6) where ğ‘„(ğ‘ , ğ‘) is the estimated value of taking action ğ‘ in state ğ‘ , ğ‘ (ğ‘ ) is the total visit count of state ğ‘ , ğ‘ (ğ‘ , ğ‘) is the number of times action ğ‘ has been selected from ğ‘ , and ğ‘ğ‘¢ğ‘ğ‘¡ is constant controlling the exploration weight. 2. Expansion: Once leaf node ğ‘£ğ‘™ is reached (or node with unexplored actions), one or more child nodes are added to the tree, representing reachable states from standard actions. 3. Simulation: From the newly expanded node, rollout policy (often random or heuristic-based) is executed to simulate sequence of actions until terminal state is reached or resource limit is met. This produces reward ğ‘…. 4. Backpropagation: The reward ğ‘… obtained from the simulation is propagated back up the tree from the leaf to the root. For each node (ğ‘ , ğ‘) traversed during the selection phase, we update the visit count and value estimate as follows: ğ‘ (ğ‘ , ğ‘) ğ‘ (ğ‘ , ğ‘) + 1 ğ‘„(ğ‘ , ğ‘) ğ‘„(ğ‘ , ğ‘) + ğ‘… ğ‘„(ğ‘ , ğ‘) ğ‘ (ğ‘ , ğ‘) (7) (8) In our MARS framework, we adapt MCTS to the space of automated AI Research. state ğ‘  corresponds to partial or complete solution ğ‘ ğ‘›, and actions correspond to modification operators (Drafting, Improvement, Debugging). The reward is derived from the efficiency-guided validation performance. C. MARS for MLE Tasks 17 MARS: Modular Agent with Reflective Search for Automated AI Research Algorithm 2 MARS for MLE Tasks 1: Input: Task Description ğ¼, Raw Dataset ğ·, Time Limit ğ‘‡ 2: Output: Optimized solution code repository ğ‘  3: ğ‘‘ MetricParsing(ğ¼) {Extract optimization objective and direction} 4: Cğ‘šğ‘’ğ‘¡ğ‘ Preprocess(ğ·) {Generate metadata and stratified splits} 5: Cğ‘’ğ‘‘ğ‘ Analyze(ğ¼, ğ·, Cğ‘šğ‘’ğ‘¡ğ‘) {Perform EDA and statistical profiling} 6: Cğ‘šğ‘œğ‘‘ğ‘’ğ‘™ SearchArchitectures(ğ¼) {Retrieve SOTA model candidates via search} 7: = {Cğ‘šğ‘’ğ‘¡ğ‘, Cğ‘’ğ‘‘ğ‘, Cğ‘šğ‘œğ‘‘ğ‘’ğ‘™} 8: ğ‘£ğ‘Ÿğ‘œğ‘œğ‘¡ InitializeTree() 9: ğ‘£ None 10: Lğ‘ ğ‘œğ‘™ğ‘¢ğ‘¡ğ‘–ğ‘œğ‘› {Solution Lesson Pool} 11: Lğ‘‘ğ‘’ğ‘ğ‘¢ğ‘” {Debug Lesson Pool} 12: {Explored Ideas} 13: while Time used < ğ‘‡ do 14: ğ‘£ SelectNode(ğ‘£ğ‘Ÿğ‘œğ‘œğ‘¡) {Using UCT selection} if ğ‘£ is ğ‘£ğ‘Ÿğ‘œğ‘œğ‘¡ then ğ‘—=1 ğ‘Œ ProposeIdea(ğ¼, Y, C, Lğ‘ ğ‘œğ‘™ğ‘¢ğ‘¡ğ‘–ğ‘œğ‘›) {Curriculum-based idea generation} ğ‘ ProposeModules(ğ¼, ğ‘Œ ) {Decompose idea into functional modules} {M ğ‘—}ğ‘™ ImplementModules(ğ¼, ğ‘Œ , ğ‘) {Implement modular components} {M ğ‘—}ğ‘™ DebugModules(ğ¼, {M ğ‘—}ğ‘™ ğœ‹main ImplementMainScript(ğ¼, ğ‘Œ , {M ğ‘—}ğ‘™ ğ‘£ğ‘›ğ‘’ğ‘¤ DraftNode({M ğ‘—}ğ‘™ {ğ‘Œ } ) {Orchestrate pipeline} ) {Unit-test modules} , ğœ‹main) ğ‘—=1 ğ‘—=1 ğ‘—=1 ğ‘—=1 else ğ‘£ğ‘›ğ‘’ğ‘¤ ImproveNode(ğ‘£, Lğ‘ ğ‘œğ‘™ğ‘¢ğ‘¡ğ‘–ğ‘œğ‘›) {Ablation-style local optimization} end if ğ‘˜ 0 26: 27: while IsBuggy(ğ‘£ğ‘›ğ‘’ğ‘¤) and ğ‘˜ < ğ‘ğ‘‘ do 28: ğ‘£ğ‘›ğ‘’ğ‘¤ DebugNode(ğ‘£ğ‘›ğ‘’ğ‘¤, Lğ‘‘ğ‘’ğ‘ğ‘¢ğ‘”) {Apply Lğ‘‘ğ‘’ğ‘ğ‘¢ğ‘” for debugging and then update Lğ‘‘ğ‘’ğ‘ğ‘¢ğ‘”} ğ‘˜ ğ‘˜ + 1 end while ğ‘Ÿğ‘’ ExecuteAndReview(ğ‘£ğ‘›ğ‘’ğ‘¤) {Execute code and review execution results} ExtractLesson(ğ‘£ğ‘›ğ‘’ğ‘¤, ğ‘Ÿğ‘’, Lğ‘ ğ‘œğ‘™ğ‘¢ğ‘¡ğ‘–ğ‘œğ‘›) {Distill lessons from results} Backpropagate(ğ‘£ğ‘›ğ‘’ğ‘¤, ğ‘Ÿğ‘’) {Update tree statistics with rewards} if ğ‘£is None or IsImprovedMetric(ğ‘Ÿğ‘’, ğ‘£, ğ‘‘) then ğ‘£ ğ‘£ğ‘›ğ‘’ğ‘¤ end if 36: 37: end while 38: ğ‘  GetRepoCode(ğ‘£) 39: return ğ‘  18 15: 16: 17: 18: 19: 20: 21: 22: 23: 24: 25: 29: 30: 31: 32: 33: 34: 35: MARS: Modular Agent with Reflective Search for Automated AI Research In this section, we detail the instantiation of MARS for Machine Learning Engineering (MLE) tasks. The comprehensive procedure is formalized in Algorithm 2. Corresponding instruction prompts for the agents involved are provided in Appendix F. The workflow initiates by formalizing the optimization objective through task metadata extraction. Metric Extraction Agent parses the natural language task description to identify the primary evaluation metric ğ‘€ and the optimization direction ğ‘‘ {maximize, minimize}. Simultaneously, Multi-Agent Subsystem processes the raw data to generate metadata descriptors (e.g., sample IDs) for the training (ğ·ğ‘¡ğ‘Ÿğ‘ğ‘–ğ‘›), validation (ğ·ğ‘£ğ‘ğ‘™), and test (ğ·ğ‘¡ğ‘’ğ‘ ğ‘¡) sets. These metadata descriptors are saved to files for later usage. To ensure robust evaluation, we employ strict protocol: Validation Dataset Creation: If pre-defined validation set is not provided, the agent performs stratified or group-based split (defaulting to 80:20 ratio) on ğ·ğ‘‘ğ‘’ğ‘£ to create ğ·ğ‘¡ğ‘Ÿğ‘ğ‘–ğ‘› and ğ·ğ‘£ğ‘ğ‘™. This ensures that ğ·ğ‘£ğ‘ğ‘™ maintains distribution ğ‘ƒ(ğ·ğ‘£ğ‘ğ‘™) ğ‘ƒ(ğ·ğ‘¡ğ‘Ÿğ‘ğ‘–ğ‘›), enabling reliable proxy evaluation. Verification & Documentation: Distinct agents perform key integrity checks (e.g., no leakage between splits) and generate comprehensive documentation describing the data schema and split logic. Following preparation, Data Analysis Agent performs Exploratory Data Analysis (EDA) on ğ·ğ‘¡ğ‘Ÿğ‘ğ‘–ğ‘›. This agent generates detailed report highlighting data distributions and potential correlations, which serves as critical reference for feature engineering during the solution exploration. Furthermore, Search Agent identifies ğ¾ğ‘ candidate model architectures across diverse algorithmic families (e.g., gradient-boosted trees, deep neural networks) using web search tools. Once initialized, MARS enters an iterative Tree Search Stage. In each iteration, node ğ‘£ is selected via the Upper Confidence Bound for Trees (UCT) formula. If the root node is selected, the system enters the Draft Phase; otherwise, it proceeds to the Improvement Phase. Following code generation, Debugging Loop is triggered to resolve execution errors, after which the results are reviewed, lessons are distilled, and rewards are backpropagated. Drafting Phase. This phase initializes new branches of the search tree using curriculum-based strategy that progresses from simple baselines to sophisticated ensembles. Initial Seed: When the solution lesson pool Lğ‘ ğ‘œğ‘™ğ‘¢ğ‘¡ğ‘–ğ‘œğ‘› is empty, an Initial Idea Generation Agent proposes solution based on the most lightweight model from the ğ¾ğ‘ candidates. Evolutionary Growth: As lessons accumulate, an Idea Improvement Agent formulates advanced proposals by integrating insights from Lğ‘ ğ‘œğ‘™ğ‘¢ğ‘¡ğ‘–ğ‘œğ‘›. Modular Implementation: Modular Agent decomposes the proposed idea into independent functional units, which are implemented and unit-tested by Coding Agent before being orchestrated into final execution script ğœ‹main. Improvement Phase. This phase focuses on local optimization. An agent analyzes the current solution and its performance metrics to propose targeted, ablation-style modifications. By leveraging the learned lessons in Lğ‘ ğ‘œğ‘™ğ‘¢ğ‘¡ğ‘–ğ‘œğ‘›, the agent avoids previously identified pitfalls and focuses on high-impact refinements (e.g., hyperparameter tuning or feature engineering). 19 MARS: Modular Agent with Reflective Search for Automated AI Research If candidate node ğ‘£ğ‘›ğ‘’ğ‘¤ fails execution, the system enters debugging loop (up Debugging Phase. to ğ¾ğ‘‘ğ‘’ğ‘ğ‘¢ğ‘” attempts). We maintain dedicated debugging lesson pool Lğ‘‘ğ‘’ğ‘ğ‘¢ğ‘” to store error-correction patterns. This prevents the agent from repeating previous mistakes in subsequent iterations. D. Setup for Leaderboard Methods vs. Our Setup Since MLE-Bench allows for open-ended submissions with varying computational budgets and system architectures, direct comparisons on the official leaderboard can be influenced by hardware disparities. To ensure fair assessment, we detail the specific hardware, time limits, and auxiliary resources used by top-performing leaderboard agents alongside our own in Table 6. In our Controlled Evaluation (AIDE, AIRA-dojo, and MARS), we standardize the environment to single A100 GPU node with no external knowledge bases to isolate algorithmic effectiveness from resource scaling. Agent ML-Master 2025) Model Compute Parallelization Knowledge Base (Liu et al., Deepseek-R1 36 vCPUs, 512GB of RAM, and 1 A100 80GB GPU, 12-hour limit 3-way search parallel None R&D-Agent (Yang et al., 2025) GPT-5 12 vCPUs, 220GB of RAM, and 1 V100 GPU, 12-hour limit Parallel ration exploNone InternAgent (Team et al., 2025) Famou-Agent (Li et al., 2025) Deepseek-R1 Gemini-2.5-Pro 32 vCPUs, 230 GB RAM, 1 A800 GPU, 12-hour limit 64 vCPUs, 500GB RAM, 1 A800 GPU, 24-hour limit Leeroo (Nadaf et al., 2025) Gemini-3-Pro-Preview 150GB RAM, 24 vCPUs, 1 H100 GPU. Run for 24 hours or until maximum budget of $200 is reached. Stop early if the run achieves any medal according to the MLE-Bench grading library. Unknown Unknown Concurrent evaluation across distributed computing resource An expert knowledge base Executing multiple ExperimentSessions concurrently knowledge plane aggregates heterogeneous sources ML-Master 2.0 (Zhu et al., 2026) Deepseek-V3.2Speciale 36 vCPUs, 252GB of RAM, and two 4090-24GB GPU, 24-hour limit Parallel ration exploUse 407 kaggle competitions as warm up dataset to build up prior wisdom AIDE (Jiang et al., 2025), AIRA-dojo (Toledo et al., 2025) or MARS Gemini-2.5-Pro or Gemini-3-Pro-Preview 1 A100 GPU 40GB, 12 vCPUs, 220 GB of RAM, 24-hour limit Non-parallel execution None MARS+ Gemini-3-Pro-Preview 2 A100 GPU 40GB, 24 vCPUs, 220 GB of RAM, 24-hour limit 2-way search parallel None Table 6 Comparison of leaderboard agents setup and our agents setup. 20 MARS: Modular Agent with Reflective Search for Automated AI Research E. Additional Results E.1. Evaluation across Different Splits of MLE-Bench This section presents comprehensive evaluation across the various subsets of MLE-Bench. Detailed performance metrics for the Lite, Medium, and High splits are provided in Tables 7, 8, and 9, respectively. Table 7 Performance comparison on MLE-Bench Lite. Results are reported as mean SEM across three independent runs. All values are in percentages (%). The best performance is highlighted in bold, and the second-best is underlined. Agent Model Valid Submission Above Median Bronze Silver Gold Any Medal Official MLE-Bench Leaderboard Results ML-Master (Liu et al., 2025) Deepseek-R1 100.0 0.0 74.2 1. 4.5 2.6 13.6 0.0 30.3 3.0 48.5 1.5 R&D-Agent (Yang et al., 2025) GPT-5 77.3 0.0 74.2 1.5 12.1 4.0 22.7 0.0 33.3 3.0 68.2 2. InternAgent (Team et al., 2025) Deepseek-R1 100.0 0.0 78.8 5.5 10.6 1.5 16.7 3.0 34.8 1.5 62.1 3.0 Famou-Agent (Li et al., 2025) Gemini-2.5-Pro 100.0 0.0 72.7 2.6 7.6 3. 16.7 1.5 37.9 1.5 62.1 1.5 Leeroo (Nadaf et al., 2025) Gemini-3-Pro-Preview 68.2 2.6 68.2 2.6 18.2 2.6 19.7 4.0 30.3 1.5 68.2 2.6 ML-Master 2.0 (Zhu et al., 2026) Deepseek-V3.2-Speciale 100.0 0.0 84.8 1.5 13.6 2.6 31.8 5.2 30.3 3.0 75.8 1. AIDE (Jiang et al., 2025) AIRA-dojo (Toledo et al., 2025) MARS (ours) MARS+ (ours) Controlled Evaluation in Our Environment Gemini-2.5-Pro Gemini-3-Pro-Preview 100.0 0.0 63.6 2.6 81.8 2.6 98.5 1.5 Gemini-2.5-Pro Gemini-3-Pro-Preview 89.4 8.4 62.1 4.0 100.0 0.0 78.8 4.0 3.0 1.5 3.0 1.5 1.5 1.5 4.5 2.6 4.5 0.0 28.8 3.0 36.4 4.5 15.2 3.0 34.8 1.5 53.0 6. 10.6 1.5 28.8 3.0 40.9 2.6 42.4 1.5 56.1 1.5 9.1 2.6 Gemini-2.5-Pro Gemini-3-Pro-Preview 100.0 0.0 77.3 2.6 12.1 1.5 19.7 3.0 36.4 0.0 68.2 2.6 15.2 1.5 53.0 1.5 74.2 1.5 100.0 0.0 89.4 1.5 6.1 1.5 Gemini-3-Pro-Preview 100.0 0.0 92.4 1.5 12.1 3.0 18.2 5.2 47.0 3.0 77.3 0. Table 8 Performance comparison on MLE-Bench Medium. Results are reported as mean SEM across three independent runs. All values are in percentages (%). The best performance is highlighted in bold, and the second-best is underlined. Agent Model Valid Submission Above Median Bronze Silver Gold Any Medal Official MLE-Bench Leaderboard Results ML-Master (Liu et al., 2025) Deepseek-R 92.1 2.6 35.1 3.2 6.1 0.9 7.0 0.9 7.0 0.9 20.2 2. R&D-Agent (Yang et al., 2025) GPT-5 47.4 0.0 26.3 1.5 6.1 0.9 8.8 1. 6.1 0.9 21.1 1.5 InternAgent (Team et al., 2025) Deepseek-R1 97.4 0.0 40.4 1.8 7.9 2. 9.6 2.3 8.8 0.9 26.3 2.6 Famou-Agent (Li et al., 2025) Gemini-2.5-Pro 95.6 1. 45.6 3.2 12.3 0.9 14.0 2.3 10.5 1.5 36.8 1.5 Leeroo (Nadaf et al., 2025) Gemini-3-Pro-Preview 44.7 1.5 44.7 1.5 15.8 0.0 12.3 0.9 16.7 2.3 44.7 1.5 ML-Master 2.0 (Zhu et al., 2026) Deepseek-V3.2-Speciale 93.9 0.9 57.9 1.5 13.2 1.5 29.8 2.3 7.9 0.0 50.9 3.5 AIDE (Jiang et al., 2025) AIRA-dojo (Toledo et al., 2025) MARS (ours) MARS+ (ours) Controlled Evaluation in Our Environment Gemini-2.5-Pro Gemini-3-Pro-Preview Gemini-2.5-Pro Gemini-3-Pro-Preview Gemini-2.5-Pro Gemini-3-Pro-Preview 81.6 0.0 84.2 1.5 80.7 0.9 97.4 1.5 92.1 0.0 97.4 0.0 36.0 2.3 39.5 2.6 31.6 3.0 48.2 2.3 9.6 2.3 7.9 1. 4.4 0.9 8.8 2.3 6.1 1.8 13.2 1.5 2.6 0.0 5.3 2.6 18.4 2.6 26.3 3.0 3.5 2.3 8.8 1.8 8.8 0.9 16.7 3.5 12.3 0.9 29.8 3. 33.3 1.8 44.7 0.0 14.0 3.2 12.3 2.3 61.4 3.2 14.9 0.9 20.2 2.3 17.5 0.9 52.6 3.0 7.0 0.9 Gemini-3-Pro-Preview 97.4 0.0 69.3 0.9 13.2 0.0 22.8 1.8 21.1 1.5 57.0 2.3 MARS: Modular Agent with Reflective Search for Automated AI Research Table 9 Performance comparison on MLE-Bench High. Results are reported as mean SEM across three independent runs. All values are in percentages (%). The best performance is highlighted in bold, and the second-best is underlined. Agent Model Valid Submission Above Median Bronze Silver Gold Any Medal Official MLE-Bench Leaderboard Results ML-Master (Liu et al., 2025) Deepseek-R1 86.7 0.0 26.7 0.0 0.0 0.0 0.0 0.0 24.4 2.2 24.4 2.2 R&D-Agent (Yang et al., 2025) GPT-5 33.3 0.0 26.7 0.0 0.0 0.0 4.4 2.2 17.8 2.2 22.2 2.2 InternAgent (Team et al., 2025) Deepseek-R 88.9 2.2 24.4 2.2 0.0 0.0 4.4 2.2 20.0 0.0 24.4 2.2 Famou-Agent (Li et al., 2025) Gemini-2.5-Pro 95.6 2.2 35.6 2.2 0.0 0.0 2.2 2.2 31.1 2.2 33.3 0.0 Leeroo (Nadaf et al., 2025) Gemini-3-Pro-Preview 40.0 0.0 40.0 0.0 4.4 2.2 15.6 5.9 20.0 7.7 40.0 0.0 ML-Master 2.0 (Zhu et al., 2026) Deepseek-V3.2-Speciale 93.3 3.8 44.4 2.2 2.2 2.2 6.7 0. 33.3 0.0 42.2 2.2 AIDE (Jiang et al., 2025) AIRA-dojo (Toledo et al., 2025) MARS (ours) MARS+ (ours) Controlled Evaluation in Our Environment Gemini-2.5-Pro Gemini-3-Pro-Preview Gemini-2.5-Pro Gemini-3-Pro-Preview 68.9 2.2 55.6 2.2 82.2 4.4 97.8 2.2 15.6 2.2 0.0 0.0 20.0 3.8 0.0 0.0 22.2 2.2 0.0 0.0 40.0 3.8 0.0 0. Gemini-2.5-Pro Gemini-3-Pro-Preview 91.1 2.2 35.6 2.2 4.4 2.2 100.0 0.0 42.2 2.2 0.0 0.0 2.2 2.2 0.0 0.0 8.9 4.4 4.4 2.2 2.2 2.2 4.4 2.2 13.3 0.0 15.6 2.2 17.8 2.2 17.8 2. 11.1 4.4 20.0 0.0 26.7 6.7 31.1 4.4 24.4 2.2 31.1 2.2 33.3 3.8 37.8 2.2 Gemini-3-Pro-Preview 100.0 0.0 55.6 2.2 0.0 0.0 8.9 4.4 31.1 4.4 40.0 0. E.2. Sensitivity Analysis of Hyperparameter ğ‘¤ Figure 7 Impact of the penalty weight ğ‘¤ on the performance of MARS. We investigate the impact of the penalty weight ğ‘¤ in Eq. (4) on search efficiency. Figure 7 compares the performance of Budget-Aware MCTS across different values: ğ‘¤ = 0, ğ‘¤ = 0.07, and ğ‘¤ = 0.15. The results demonstrate that the default setting of ğ‘¤ = 0.07 consistently yields superior performance, effectively balancing exploration with resource constraints. Setting ğ‘¤ = 0 results in performance degradation, underscoring the importance of penalizing long execution times alongside rewarding performance. Conversely, setting ğ‘¤ = 0.15 leads to inferior results because the stronger penalty excessively biases the reward toward latency, causing the search to prioritize trivial, fast nodes over high-performing ones. 22 MARS: Modular Agent with Reflective Search for Automated AI Research Table 10 Cost and performance analysis of different agents using Gemini-2.5-Pro. Metrics are averaged across competitions."
        },
        {
            "title": "AIDE",
            "content": "AIRA-dojo"
        },
        {
            "title": "MARS",
            "content": "# API Calls # Input Tokens (105) # Output Tokens (105) Price ($) Any Medal Rate (%) 293.5 18.9 867.0 77.0 594.8 35.4 128.3 13.2 286.6 54.7 34.6 4.0 23.0 2.1 6.3 0.5 39.0 3.7 10.6 0.9 24.4 1.2 23.1 0.4 6.0 0.4 60.5 13.8 43.1 1.6 E.3. Cost-Performance Trade-off In this section, we analyze the computational cost and pricing of using LLMs in Table 10. MARS exhibits distinct resource profile: it achieves the lowest generation volume, with fewer output tokens (6.0 105) than both baselines and significantly fewer API calls than AIRA-dojo (594.8 vs. 867.0). However, its input consumption is substantial (286.6 105 tokens) approximately 2.2 that of AIRA-dojo due to the maintenance of comprehensive memory context containing learned lessons and modular structures. Since Gemini-2.5-Pro applies premium pricing for long-context prompts (> 200k tokens), MARS incurs higher total cost ($60.5) than AIRA-dojo ($39.0). Crucially, this investment yields substantial returns: the Any Medal Rate increases from 24.4% to 43.1%, justifying the expense through superior efficacy. F. Prompts This section provides the full suite of instruction prompts utilized by MARS to orchestrate the various agents involved in solving MLE tasks."
        },
        {
            "title": "Metric Parsing Instruction",
            "content": "==== Task ==== Your task is to analyze the provided problem description to identify the primary evaluation metric and determine if lower score indicates better performance. Your response must be in specific JSON format with the following fields: - metric_name (string): This field specifies the name of the primary evaluation metric. - lower_is_better (boolean): This field indicates whether the metric should be minimized. If lower value of the metric represents better performance (e.g., for Mean Squared Error), set this to true. If higher value represents better performance (e.g., for accuracy), set this to false. # Response Format Your response should be in the following JSON format in single markdown code block (wrapped in ```): ```json {{ \"metric_name\": \"accuracy\", \"lower_is_better\": false, }} 23 MARS: Modular Agent with Reflective Search for Automated AI Research ```"
        },
        {
            "title": "Metadata Generation Instruction",
            "content": "==== Task ==== Your task is to write Python script that generates three metadata files for the training, validation, and test datasets respectively. This metadata (e.g., sample IDs, file paths, labels) will be used by other scripts to load data efficiently. # Requirements - The script's only responsibility is to generate metadata. It should not perform any model training or inference. - The script must read raw data from the `./input` directory. This directory should be treated as read-only. - All generated metadata files (e.g., .csv, .parquet, .json) must be saved directly to the `./metadata` directory. - You must not copy or move the original raw data. The `./metadata` directory should only contain the newly generated metadata files. - All file paths stored within the metadata must be relative to the `./ input` directory. Review the Dataset Information section above to identify the correct file paths and structure. - The metadata for the training and validation datasets must include the ground-truth labels. - Create validation set by splitting the training data only if separate validation dataset is not already available. - Use fixed 80:20 ratio (80% training and 20% validation). This ratio should not be user-configurable argument. - Randomly shuffle the training data before splitting. To ensure the split is reproducible, use fixed random state (`RANDOM_STATE = 42`). - Apply stratified sampling or group sampling to ensure the validation dataset's distribution properly represents the original data. - Stratified Sampling: Use this if it's classification task, stratifying by the target label. - Group Sampling: Use this if the data has inherent groups (e.g., patient IDs, user IDs) that must not be split across the train and validation sets. - After generating the metadata, the script must immediately load the datasets using the new metadata and perform the following checks: - Print summary statistics for the final training, validation, and test datasets (e.g., total number of samples, class distributions, data shapes, number of unique users, etc.). - If the metadata contains file paths, programmatically check 1000 relative file paths randomly selected from each of the metadata files. Calculate the ratio of paths that do not resolve to an existing file in `./input`. If this \"missing file ratio\" is greater than 0.5, the script must raise an error. Before raising the error, print sample ( e.g., the first five) of the non-resolving file paths to the console for debugging purposes. - If new validation set was created, you must programmatically verify that it satisfies the requirements. - Assert that the stratification or group split was successful. 24 MARS: Modular Agent with Reflective Search for Automated AI Research - Raise an error (e.g., AssertionError) if these verification checks fail. # Implementation Guideline - The code should be single-file python program that is self-contained and can be executed as-is. - The script must be complete and able to run from start to finish without premature termination or skipped sections. - Your response should only contain single code block. - All validation checks must fail explicitly, either by raising an Exception or triggering an AssertionError. - Do not use `try ... except` blocks to suppress or ignore errors in your code examples. - Be aware of the running time of the code, it should complete within { exec_timeout}. - All the provided input data is stored in \"./input\" directory. There is no need to unzip any files."
        },
        {
            "title": "Validation Dataset Verification Instruction",
            "content": "==== Task ==== Analyze the provided Python script and its execution output to verify if the validation dataset was handled or created successfully. # Python Script {code} # Execution Output {term_out} # Requirements You must review the script and output based on the criteria below. Your entire response must be single JSON code block. - Success Criteria: The success field must be set to True if one of the following two conditions is met. Otherwise, set it to False. 1. Existing Validation Set: The script correctly identifies that separate validation dataset is already available in the raw data (i.e ., no new split is required). 2. Created Validation Set: The script correctly creates new validation set by splitting the training data. Your analysis must confirm that the script's logic properly attempts to create representative split (e.g., by using stratified or group sampling). - JSON Response Format: Provide your review in the following JSON format. - analysis (string): concise rationale for your decision. - If successful: Explain which of the two success criteria was met. - If failed: Briefly explain why the script failed to meet either criterion (e.g., \"The script split the data randomly instead of using stratification.\"). - success (bool): True if the validation dataset was handled or created successfully, False otherwise. 25 MARS: Modular Agent with Reflective Search for Automated AI Research # Response Format The review must be submitted in the following JSON format in single markdown code block (wrapped in ```): ```json {{ \"analysis\": \"The validation dataset was not created successfully. The script split the training data but did not use stratified sampling, failing to create representative sample.\",, \"success\": false, }} ```"
        },
        {
            "title": "Metadata Documentation Instruction",
            "content": "==== Task ==== Your task is to analyze the provided Python script and its execution output to create clear documentation for each file generated in the `./metadata` directory. # Python Script {code} # Execution Output {term_out} # Requirements For each file generated in the `./metadata` directory, provide detailed breakdown covering: - Content and Purpose: - Describe the information or data contained within the file (e.g., \" Contains image_id, file_path, and label for the training set.\"). - Explain its primary purpose (e.g., \"This file is used by the data loader to find image files and match them with their correct labels.\"). - Schema / Structure: Detail the structure, such as column names, data types, and an example row if applicable. - Loading Method: Explain the standard method or library function required to load this file (e.g., \"Load with pandas.read_csv()\" or \" Load with joblib.load()\")."
        },
        {
            "title": "Exploratory Data Analysis Instruction",
            "content": "==== Task ==== Your task is to write robust Python script to perform an Exploratory Data Analysis (EDA) on the training dataset. The script must adapt its analysis based on the data modality (Tabular, Image, Audio, or Text). The output should act as report to inform feature engineering and preprocessing strategies. # Requirements 26 MARS: Modular Agent with Reflective Search for Automated AI Research 1. Data Integrity: Ensure all analysis is strictly performed on the training set to prevent data leakage. 2. Target Variable Analysis - Distribution: Calculate the distribution of the target variable. - Imbalance/Skew: - If Classification: Calculate class balance ratios. - If Regression: Calculate Skewness and Kurtosis to assess normality. 3. Input Data Analysis (Modality-Specific) - If Tabular Data: - Numerical: Report mean, std, min, max, and outlier counts (IQR method). - Categorical: Report cardinality; flag columns with > 50 categories or rare labels (< 1 percent frequency). - Missing Values: Report count/percentage of NaNs per column. - If Image Data: - Dimensions: Analyze distributions of Image Widths, Heights, and Aspect Ratios. - Channels: Report the distribution of channel counts (e.g., Grayscale vs. RGB). - Pixel Stats: Calculate the global mean and standard deviation of pixel values (for normalization). - If Audio Data: - Signal: Analyze distributions of Duration (seconds), Sampling Rates, and Bit Depths. - Channels: Check for mono vs. stereo inconsistency. - If Text Data: - Lengths: Analyze distribution of sequence lengths (character and word counts). - Vocabulary: Report unique vocabulary size and OOV (Out of Vocabulary) potential. 4. Feature/Signal Relationships - Structured (Tabular) Relationships: - Correlation: Pearson/Spearman for numerical; Mutual Information for categorical. - Importance: Train lightweight Random Forest and report top 5 features. - Redundancy: Report collinear pairs (Correlation > 0.90). - Unstructured (Meta-Feature) Relationships: Analyze the relationship between metadata and the target (e.g., \"Do longer audio files correlate with specific classes?\", \"Are larger images associated with higher regression targets?\"). 5. Formatting & Output - Organize the output into distinct, capitalized sections. - Use f-strings to format floats to 4 decimal places for readability."
        },
        {
            "title": "Model Architecture Search Instruction",
            "content": "==== Task ==== Your task is to propose {num_model_candidates} distinct model architectures to solve the problem. **Action:** Use Google Search to research state-of-the-art and efficient architectures relevant to this domain. 27 MARS: Modular Agent with Reflective Search for Automated AI Research # Requirements - **Broad Diversity:** The candidates must represent different algorithmic families. Do not propose multiple variations of the same underlying method (e.g., do not suggest two different ResNets). Aim for mix of: * Instance-Based / Kernel Methods (e.g., k-NN, SVM) * Tree-Based Ensembles (e.g., LightGBM, XGBoost, CatBoost) * Deep Learning (e.g., CNN, MLP, Transformers, RNNs) - **Problem Alignment:** Architectures must be specifically tailored to the data modality (e.g., tabular, image, time-series) and input structure. - **Hybridization:** Incorporate hybrid or ensemble designs if they offer clear advantage for heterogeneous data. - **Efficiency First:** Prioritize \"lightweight\" designs. For each family, choose the architecture that offers the best trade-off between low computational cost (fast training/inference) and high performance. - **Data Constraints:** If the training data is limited, explicitly address regularization or low-complexity designs to prevent overfitting. - For each model, create JSON object with the following two keys: - `reasoning`: Justification for why this architecture fits the constraints (efficiency, data size, and why it was chosen over others in its category). - `description`: technical description of the architecture and design philosophy. # Response Format Your response should be in the following JSON format in single markdown code block (wrapped in ```): ```json [ {{\"reasoning\": \"k-NN is small and efficient...\", \"description\": \"We can use K-NN for this task...\"}}, {{\"reasoning\": \"CNN is effective and efficient...\", \"description\": \" We can use CNN for this task...\"}}, {{\"reasoning\": \"GBMs is an effective model...\", \"description\": \"We can use GBMs for this task...\"}}, ] ```"
        },
        {
            "title": "Initial Idea Proposal Instruction",
            "content": "==== Model Architectures ==== {model_arch_desc} ==== Previous Ideas ==== {previous_ideas} ==== Task ==== Your task is to propose highly efficient **baseline approach** to solve the problem. # Requirements MARS: Modular Agent with Reflective Search for Automated AI Research - Novelty: The proposed solution must remain strictly distinct from the approaches listed in Previous Ideas. - Model Design: Synthesize simple and lightweight architecture using the provided Model Architectures as conceptual foundation. Ensure the design is unique and has not been suggested in the Previous Ideas. - Philosophy: Prioritize speed and simplicity over maximum accuracy. Exclude resource-intensive techniques, such as heavy augmentations or ensembles, to establish reliable performance baseline. # Response Format Your solution must be outlined in natural language without using code or specific implementation details. Your response should cover the following aspects: - Model: Describe the model architecture's design and key components. - Data: Describe the necessary steps to preprocess data for both training and evaluation. - Training: Outline the training procedure, including key techniques (e.g ., loss functions, optimizers, or training strategies). - Evaluation: Describe the process for generating predictions on the test data."
        },
        {
            "title": "Idea Improvement Instruction",
            "content": "==== Previous Ideas ==== {previous_ideas} ==== Lessons ==== {lessons} ==== Task ==== Using the insights from the lessons learned during solution development, your task is to propose an optimized strategy to solve the problem more effectively. You must synthesize the provided \"Lessons\" to propose structural evolution of the \"Previous Ideas\". # Requirements - Structural Innovation (Exploration): Do not propose trivial hyperparameter tuning. You must introduce fundamental change (e.g., new backbone architecture, multi-stage pipeline, or distinct feature engineering paradigm) to address identified weaknesses. - Strategic Retention (Exploitation): Explicitly preserve components identified as successful in the \"Lessons\". Do not discard what is already working. - Computational Budget: The solution is allowed to be moderately heavier than previous ideas (e.g., using stronger backbone), but it must remain feasible for standard training environments. - Citation: Whenever you apply specific concept or solution from these lessons, you must immediately reference it by appending \"Cite {{ lesson_id}}\" to the relevant statement. # Response Format Your solution must be outlined in natural language without using code or specific implementation details. Your response should cover the 29 MARS: Modular Agent with Reflective Search for Automated AI Research following aspects: - Model: Describe the model architecture's design and key components. - Data: Describe the necessary steps to preprocess data for both training and evaluation. - Training: Outline the training procedure, including key techniques (e.g ., loss functions, optimizers, or training strategies). - Evaluation: Describe the process for generating predictions on the test data."
        },
        {
            "title": "Modular Decomposition Instruction",
            "content": "==== Idea ==== {idea} ==== Task ==== Your task is to design modular repository structure to implement the given idea. Do not generate the full code yet; focus on the natural description of the **architectural logic**. # Requirements - **Modularity:** Break the solution into logical modules based on functionality (e.g., data handling, core training and evaluation logic , utilities). - **Entry Point:** You must include `main` module that acts as the entry point to execute the end-to-end pipeline. - **Detail:** For each module, the description must include: - The purpose of the module. - The names of specific classes or functions to be implemented. - brief description of the implementation logic. - brief explanation of how this module interacts with others. - **Ordering:** The JSON output must be ordered topologically ( dependencies first, dependent modules last). # Response Format Provide the output strictly as JSON object in single markdown code block. The keys should be the module names and the values should be the detailed descriptions. The module name must not include the `.py` extension. Example Format: ```json {{ \"module_name\": \"Implements [Specific Class] to handle [Specific Task]. includes functions like [func_a] and [func_b].\", \"main\": \"Orchestrates the workflow. Imports DataLoader from the data module and Model from the model module to run the pipeline.\" }} ``` 30 MARS: Modular Agent with Reflective Search for Automated AI Research"
        },
        {
            "title": "Module Implementation Instruction",
            "content": "==== Idea ==== {idea} ==== Python Files ==== The following Python files are already provided. Do not modify them. {library_files} ==== Target File Description ({file_name}) ==== {file_description} ==== Task ==== Your task is to implement the `{file_name}` module based on the description above. # Requirements - Import the functions or classes from the given Python files instead of re-implementing them. - Only implement the module class/functions. DO NOT include an if ` __name__ == \"__main__\":` block. DO NOT implement the end-to-end pipeline. - Ensure functions accept arguments for flexibility. You must include hyperparameters to control dataset size (for debugging) and training steps/epochs. - When printing validation metrics, please print the full precision without any rounding or formatting. - If loading raw data, use the metadata in `./metadata` to identify the correct train/val/test splits. - If this module performs deterministic data processing, you must implement caching mechanism strictly following this logic: - **Function Signature:** The processing function must accept ` load_cached_data: bool` argument. - **Directory Safety:** Ensure the directory `./working/{dir_name}/` exists (use `os.makedirs(..., exist_ok=True)`). - **Prohibited:** Do NOT use `pickle`. Use `parquet` (via pandas) or ` npy` (via numpy). - **Logic Flow:** 1. IF `load_cached_data` is True: Try to load the file. 2. IF loading fails (file missing or corrupt) OR ` load_cached_data` is False: - Compute/process the data from scratch. - Save the result to the cache directory `./working/{dir_name }/` for future runs. 3. Return the data. - If this module handles model training: - **Metrics:** Print key training and validation metrics during training process. - **Optimization:** Implement Early Stopping to prevent overfitting and reduce runtime. - If this module handles submission generation: - Generate predictions for the entire test set. Save the final predictions to `./submission/submission.csv`. 31 MARS: Modular Agent with Reflective Search for Automated AI Research - Refer to the sample submission file (e.g., `./input/ sample_submission.csv` or `./input/sampleSubmission.csv`) for the correct formatting required by the competition."
        },
        {
            "title": "Module Testing Instruction",
            "content": "==== Python Files ==== The following Python files are already provided. Do not modify them. {library_files} ==== Task ==== Your task is to write code examples demonstrating how to instantiate and utilize the classes or functions from the provided Python files. # Requirements - Optimize for Speed: Limit hyperparameters (e.g., reduce the number of epochs/steps, use smaller dataset subset) to ensure the demonstration executes quickly. - Verify Logic: Include assertions or validation steps to confirm the correctness of complex classes and functions. You may skip verification for trivial components, such as configuration classes."
        },
        {
            "title": "Solution Drafting Instruction",
            "content": "==== Idea ==== {idea} ==== Python Files ==== The following Python files are already provided. Do not modify them. {library_files} ==== Target File Description (`runfile.py`) ==== {file_description} ==== Task ==== Your task is to implement the end-to-end orchestration script `runfile.py `. This script serves as **fast baseline** to verify the idea. It must train the model, validate performance, perform failure analysis, and generate submission. # Requirements - Import the functions or classes from the given Python files instead of re-implementing them. - Make the model training fast. - Limit maximum number of training samples and training steps/epochs to ensure quick baseline execution. - Set appropriate batch sizes to prevent CUDA out-of-memory errors. - After training is complete, you must execute validation assessment, failure analysis, and submission generation. 32 MARS: Modular Agent with Reflective Search for Automated AI Research - You must load the hold-out validation dataset using the metadata located in the `./metadata` directory. - You must print the final validation metric computed on the entire hold-out validation set in this format `Final Validation Metric: < value>`. Without this metric, the solution cannot be evaluated, rendering the entire code invalid. You must use the validation metric defined in the Task Description. Please print the full precision of the validation metric without any rounding or formatting. - You must perform failure analysis on the trained model. You must perform failure analysis on the validation set to identify systematic error patterns. Calculate and print the correlation between the model' error magnitude and the input features to reveal which variables are most associated with poor performance. - You must generate predictions for the entire test set and create the submission file{submission_cond}. Save the final predictions to `./submission/submission.csv`. Refer to the sample submission file (e. g., `./input/sample_submission.csv` or `./input/sampleSubmission.csv`) for the correct formatting required by the competition. - Optimize the validation inference speed. - Ensure the model is in evaluation mode for this inference. - Your code must automatically detect and utilize an available GPU for inference. Ensure the model and all data batches are moved to the correct device (GPU or CPU). - During inference, you don't need to compute gradients. Disabling this process reduces memory consumption and speeds up computation. - Call data loading functions with `load_cached_data=True` (if applicable) to utilize preprocessed data in the `./working` directory."
        },
        {
            "title": "Solution Improvement Instruction",
            "content": "==== Lessons ==== {lessons} ==== Previous Solution ==== {previous_solution} ==== Task ==== Your task is to modify the Python files from the previous solution to optimize performance. # Requirements - Modifications must be targeted and specific (ablation-style). Do not rewrite the entire solution; focus on isolating and improving specific aspects. - You should apply the relevant knowledge provided in the Lessons section to support your optimization strategy. Whenever you apply specific concept or solution from these lessons, you must immediately reference it by appending \"Cite {{lesson_id}}\" to the relevant statement. - Optimize hyperparameter settings (e.g., training steps, learning rate, batch size) to strike the best balance between predictive performance and execution speed. - **Do not remove** the following core logic from the original `runfile. py` script: MARS: Modular Agent with Reflective Search for Automated AI Research - Print the final validation metric computed on the entire hold-out validation set. - Perform failure analysis on the trained model. - Generate predictions for the entire test set and create the submission file{submission_cond}."
        },
        {
            "title": "Bug Analysis Instruction",
            "content": "==== Debug Lessons ==== {lessons} ==== Python Files ==== The following Python files are already provided. {files} ==== Task ==== You are tasked with debugging script failure. You should summarize the execution traceback and explain the root cause of the errors. You should apply the relevant knowledge provided in the Debug Lessons section to support your diagnosis. Whenever you apply specific concept or solution from these lessons, you must immediately reference it by appending \"Cite {{lesson_id}}\" to the relevant statement. You can use Google Search as needed for debugging. Execution Traceback (`python runfile.py`): {exec_result}"
        },
        {
            "title": "Debugging Instruction",
            "content": "==== Debug Lessons ==== {lessons} ==== Python Files ==== The following Python files are already provided. Do not modify them. {files} ==== Task ==== We ran this command (`python runfile.py`) and got some errors. Execution Traceback: {exec_result} Error analysis: {error_analysis} Your task is to revise the given Python files to fix the errors based on the provided error analysis. You can use Google Search as needed for debugging. # Requirements 34 MARS: Modular Agent with Reflective Search for Automated AI Research - You should write brief natural language description of what the issue in the previous implementation is and how the issue can be fixed. - The fix must be targeted. Do not change the core logic or intended functionality of the original code; only correct the specific implementation error shown in the Execution Traceback. - You should apply the relevant knowledge provided in the Debug Lessons section to guide your fixes. Whenever you apply specific concept or solution from these lessons, you must immediately reference it by appending \"Cite {{lesson_id}}\" to the relevant statement. - Do not use `try...except` blocks to catch, suppress, or ignore the original error. The fix must address the root cause of the problem."
        },
        {
            "title": "Debugging Lesson Distillation Instruction",
            "content": "You are an expert Python debugger and instructor. Your task is to analyze debugging attempt and distill high-value \"Lesson Learned\". # Input Initial State: {source_files} Initial Execution Traceback: {source_exec_result} Initial Error analysis: {source_error_analysis} Attempted Fix (Diff): {diff} Execution Traceback after applying the fix: {final_exec_result} # Guidelines - Determine if the Attempted Fix resolved the Initial Error based on the Result of Fix. - If the fix SUCCEEDED: Explain the root cause of the initial error and why this specific fix was the correct solution. - If the fix FAILED: Explain why the attempted fix was insufficient, incorrect, or introduced new issues. The lesson must focus on avoiding this specific pitfall. # Response Format - Title: concise, imperative, and memorable summary of the lesson. - Explanation: clear paragraph synthesizing the error context. Describe the specific mechanism of the failure and the logic required to fix it. - Detection: How to identify this issue in the future. List specific signals, such as particular Exception types, stack trace patterns, or code smells. 35 MARS: Modular Agent with Reflective Search for Automated AI Research"
        },
        {
            "title": "Execution Result Review Instruction",
            "content": "==== Python Files ==== The following Python files are already provided. {library_files} ==== Task ==== Your task is to evaluate the output of the code execution for the provided code and report the empirical findings. The review must be submitted in specific JSON format with the following fields: - summary (string): In this field, provide brief summary describing the empirical findings. This must include: - The training loss trend (e.g., did it converge/minimize?). - Failure analysis. - The final validation metric. - The reasoning for your `valid_metric` assessment (e.g., \"The final validation metric is valid,\" or \"The final validation metric is invalid due to validation data leakage...\"). - metric (number or null): Report the value of the validation metric here. You must convert percentages to decimals (e.g., 95% -> 0.95). This should be null if the metric cannot be found or determined. - valid_metric (boolean): Set to `true` if the final validation metric is valid. Set to `false` if any of the following conditions are met: - The computed final validation metric does not match the one defined in the Task Description. - The final validation metric is calculated incorrectly. - The final validation metric is not computed on the entire hold-out validation set. - There are signs of validation data leakage (e.g., the validation set was used in training). Code: ``` {code} ``` Execution Output: {term_out} # Response Format The review must be submitted in the following JSON format in single markdown code block (wrapped in ```): ```json {{ \"summary\": \"The code trains model to solve the task... The final validation metric is ...\", \"metric\": 0.99, \"valid_metric\": true, }} ``` 36 MARS: Modular Agent with Reflective Search for Automated AI Research"
        },
        {
            "title": "Solution Lesson Distillation Instruction",
            "content": "==== Current Best Solution ==== {best_solution} ==== New Solution ==== {new_solution} ==== Task ==== Your task is to analyze the provided solutions to distill high-value \" Lesson Learned\". # Guidelines - **Check Context:** - If *Current Best Solution* exists: Comparative Analysis. Contrast the algorithmic approach of the New vs. Current. Explain precisely * why* the New Solution improves or degrades performance based on the execution results. - If *Current Best Solution* is missing: Empirical Analysis. Summarize the findings and effectiveness of the New Solution based on its execution results. - **Logic over Syntax:** Focus on algorithmic choices, data structures, and architectural decisions. Ignore minor syntactic sugar unless it affects performance. - **Causal Chain:** Trace the logic to prove exactly how the new approach resolves the specific bottleneck. - **Generalizability:** The final lesson must be abstract enough to apply to similar problems in the future, not just this specific snippet. # Response Format - Title: clear, memorable title for the lesson. - Summary: brief, high-level overview of the methods or algorithmic changes applied in the New Solution. - Empirical Findings: Analysis of the execution results. If comparing, highlight the delta in performance (validation metric and execution time) and the specific trade-offs observed. - Key Lesson: standalone, actionable principle. Write this as heuristic or rule of thumb (e.g., \"When handling sparse matrices, prefer over because...\"). If developer reads *only* this paragraph, they should learn technique to apply in their own work."
        },
        {
            "title": "Lesson Deduplication Instruction",
            "content": "You are Machine Learning Engineer responsible for maintaining knowledge base of technical lessons. ==== Existing Lessons ==== {existing_lessons} ==== New Lesson ==== {new_lesson} ==== Task ==== MARS: Modular Agent with Reflective Search for Automated AI Research Your task is to determine if the **New Lesson** is semantically equivalent to any of the **Existing Lessons**. ### Guidelines - **Semantic Overlap:** lesson is duplicate if the core insight, principle, or actionable advice is effectively the same, even if the wording differs. - **Subsets:** If the **New Lesson** is fully covered by broader existing lesson, count it as duplicate. - **Novelty:** If the **New Lesson** provides specific nuance, edge case, or context not covered by existing lessons, it is **not** duplicate. # Response Format Provide your analysis in single valid JSON object inside single markdown code block (wrapped in ```). **Fields:** - `reasoning` (string): Briefly explain your decision. If duplicate exists, explicitly quote or summarize the specific existing lesson that overlaps. - `duplicate` (boolean): Use `true` if it is duplicate, `false` otherwise. **Example Output:** ```json {{ \"reasoning\": \"The lesson is the same as one of the existing lessons.\", \"duplicate\": true }} ``` MARS: Modular Agent with Reflective Search for Automated AI Research G. Example Solution Code In this section, we showcase the final solution generated for the iMet-2020-fgvc7 competition. This repository was produced by MARS utilizing Gemini-3-Pro-Preview and achieved the highest validation performance during the search process. Module: library/config.py import os import torch import random import numpy as np class Config: # ========================================================================= # Directories & Paths # ========================================================================= INPUT_DIR = \"./input\" METADATA_DIR = \"./metadata\" # Output directories WORKING_DIR = \"./working/idea_4\" SUBMISSION_DIR = \"./submission\" # Metadata Files (Generated by previous steps) TRAIN_CSV = os.path.join(METADATA_DIR, \"train.csv\") VAL_CSV = os.path.join(METADATA_DIR, \"val.csv\") TEST_CSV = os.path.join(METADATA_DIR, \"test.csv\") # Raw Data Files LABELS_CSV = os.path.join(INPUT_DIR, \"labels.csv\") SAMPLE_SUBMISSION = os.path.join(INPUT_DIR, \"sample_submission.csv\") # ========================================================================= # Data Configuration # ========================================================================= IMG_SIZE = (352, 352) # Resolution optimized for detail vs throughput NUM_CLASSES = 3474 # Total number of attributes # ========================================================================= # Model Configuration MARS: Modular Agent with Reflective Search for Automated AI Research # ========================================================================= # Ensemble members MODEL_A_NAME = \"resnet101d\" MODEL_B_NAME = \"convnext_base\" # Texture specialist (Deep Stem) # Context specialist (Large Kernel) # ========================================================================= # Training Hyperparameters # ========================================================================= SEED = 42 EPOCHS = 12 BATCH_SIZE = 48 # Tuned for A100 40GB with 352x352 resolution # Optimizer settings LEARNING_RATE = 1e-3 # Max LR for OneCycle WEIGHT_DECAY = 1e-2 # Standard AdamW decay # Loss settings ASL_GAMMA_NEG = 4.0 # Asymmetric Loss parameters ASL_GAMMA_POS = 0.0 ASL_CLIP = 0.05 # ========================================================================= # Compute Environment # ========================================================================= NUM_WORKERS = 8 # 12 vCPUs available, leave some overhead DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\" # ========================================================================= # Debugging & Development # ========================================================================= DEBUG = False # Set to True to run on small subset DEBUG_SAMPLE_SIZE = 5000 @classmethod def setup(cls): \"\"\"Creates necessary output directories.\"\"\" os.makedirs(cls.WORKING_DIR, exist_ok=True) os.makedirs(cls.SUBMISSION_DIR, exist_ok=True) 40 MARS: Modular Agent with Reflective Search for Automated AI Research def seed_everything(seed=42): \"\"\"Sets the random seed for reproducibility across all libraries.\"\"\" random.seed(seed) os.environ[\"PYTHONHASHSEED\"] = str(seed) np.random.seed(seed) torch.manual_seed(seed) torch.cuda.manual_seed(seed) torch.backends.cudnn.deterministic = True torch.backends.cudnn.benchmark = False reproducibility # False for exact # Initialize directories on import Config.setup() Module: library/dataset.py import os import cv2 import torch import numpy as np import pandas as pd import albumentations as from albumentations.pytorch import ToTensorV2 from torch.utils.data import Dataset, DataLoader from library.config import Config def load_processed_dataframe(mode, load_cached_data=True): \"\"\" Loads the metadata dataframe for specific mode (train/val/test). Implements caching using Parquet files to store processed dataframes (where attribute_ids strings are converted to lists). \"\"\" cache_dir = Config.WORKING_DIR os.makedirs(cache_dir, exist_ok=True) cache_path = os.path.join(cache_dir, f\"cached_{mode}.parquet\") # 1. Try to load from cache if load_cached_data and os.path.exists(cache_path): try: df = pd.read_parquet(cache_path) return df except Exception: # If load fails, proceed to process from scratch pass # 2. Process from scratch if mode == \"train\": csv_path = Config.TRAIN_CSV elif mode == \"val\": csv_path = Config.VAL_CSV elif mode == \"test\": 41 MARS: Modular Agent with Reflective Search for Automated AI Research csv_path = Config.TEST_CSV else: raise ValueError(f\"Unknown mode: {mode}\") if not os.path.exists(csv_path): raise FileNotFoundError(f\"Metadata file not found: {csv_path}\") df = pd.read_csv(csv_path) # Parse attribute_ids: \"0 1 2\" -> [0, 1, 2] # Handle NaNs by converting to empty string first df[\"attribute_ids\"] = df[\"attribute_ids\"].fillna(\"\") # Function to safe convert string to list of ints def parse_ids(x): if not x.strip(): return np.array([], dtype=int) return np.array([int(i) for in x.split()], dtype=int) # We store as numpy arrays inside the dataframe cells for parquet compatibility/efficiency df[\"parsed_attributes\"] = df[\"attribute_ids\"].apply(parse_ids) # 3. Save to cache try: df.to_parquet(cache_path, index=False) except Exception as e: print(f\"Warning: Failed to save cache to {cache_path}: {e}\") return df class ArtworkDataset(Dataset): def __init__(self, df, transforms=None, mode=\"train\"): \"\"\" Args: df (pd.DataFrame): DataFrame containing metadata. transforms (albumentations.Compose): Transforms to apply. mode (str): 'train', 'val', or 'test'. \"\"\" self.df = df self.transforms = transforms self.mode = mode self.input_dir = Config.INPUT_DIR self.num_classes = Config.NUM_CLASSES def __len__(self): return len(self.df) def __getitem__(self, idx): row = self.df.iloc[idx] image_id = row[\"id\"] file_path = row[\"file_path\"] 42 MARS: Modular Agent with Reflective Search for Automated AI Research # Load Image full_path = os.path.join(self.input_dir, file_path) image = cv2.imread(full_path) if image is None: # Fallback for missing/corrupt images: return black image # This prevents crashing during training image = np.zeros( (Config.IMG_SIZE[0], Config.IMG_SIZE[1], 3), dtype=np. uint8 ) else: image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB) # Apply Transforms if self.transforms: augmented = self.transforms(image=image) image = augmented[\"image\"] # Create Target (Multi-hot encoding) target = torch.zeros(self.num_classes, dtype=torch.float32) # In test mode, we might not have valid labels, but we still return dummy target # parsed_attributes is numpy array of integers attr_ids = row[\"parsed_attributes\"] if len(attr_ids) > 0: # Ensure indices are within bounds valid_ids = attr_ids[attr_ids < self.num_classes] target[valid_ids] = 1.0 return image, target, image_id def get_transforms(mode=\"train\"): \"\"\" Returns the Albumentations transform pipeline for the specified mode. \"\"\" img_size = Config.IMG_SIZE if mode == \"train\": return A.Compose( [ ] A.Resize(height=img_size[0], width=img_size[1]), A.HorizontalFlip(p=0.5), A.Normalize( mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225), max_pixel_value=255.0, p=1.0, ), ToTensorV2(), 43 MARS: Modular Agent with Reflective Search for Automated AI Research ) else: # Validation and Test return A.Compose( [ A.Resize(height=img_size[0], width=img_size[1]), A.Normalize( mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225), max_pixel_value=255.0, p=1.0, ), ToTensorV2(), ] ) def get_dataloaders( debug=False, batch_size=Config.BATCH_SIZE, num_workers=Config.NUM_WORKERS, load_cached_data=True, ): \"\"\" Creates and returns DataLoaders for train, validation, and test sets. Args: debug (bool): If True, subsets the data for quick debugging. batch_size (int): Batch size for the dataloaders. num_workers (int): Number of worker processes. load_cached_data (bool): Whether to use cached dataframes. Returns: tuple: (train_loader, val_loader, test_loader) \"\"\" # Load DataFrames train_df = load_processed_dataframe(\"train\", load_cached_data) val_df = load_processed_dataframe(\"val\", load_cached_data) test_df = load_processed_dataframe(\"test\", load_cached_data) # Debug Subsampling if debug: debug_size = Config.DEBUG_SAMPLE_SIZE train_df = train_df.iloc[:debug_size] val_df = val_df.iloc[:debug_size] test_df = test_df.iloc[:debug_size] # Create Datasets train_dataset = ArtworkDataset( train_df, transforms=get_transforms(\"train\"), mode=\"train\" ) val_dataset = ArtworkDataset(val_df, transforms=get_transforms(\"val\"), mode=\"val\") test_dataset = ArtworkDataset( 44 MARS: Modular Agent with Reflective Search for Automated AI Research test_df, transforms=get_transforms(\"test\"), mode=\"test\" ) # Create DataLoaders train_loader = DataLoader( train_dataset, batch_size=batch_size, shuffle=True, num_workers=num_workers, pin_memory=True, drop_last=True, ) val_loader = DataLoader( val_dataset, batch_size=batch_size, shuffle=False, num_workers=num_workers, pin_memory=True, drop_last=False, ) test_loader = DataLoader( test_dataset, batch_size=batch_size, shuffle=False, num_workers=num_workers, pin_memory=True, drop_last=False, ) return train_loader, val_loader, test_loader Module: library/inference.py import os import torch import numpy as np import pandas as pd from tqdm import tqdm from library.config import Config from library.dataset import get_dataloaders from library.models import get_model from library.utils import optimize_threshold, load_checkpoint def predict_with_tta(model, dataloader, device, mode=\"val\"): \"\"\" Generates predictions using Test Time Augmentation (Horizontal Flip). Args: model (nn.Module): The trained model. 45 MARS: Modular Agent with Reflective Search for Automated AI Research dataloader (DataLoader): DataLoader for validation or test set. device (torch.device): Device to run inference on. mode (str): 'val' or 'test'. Returns: tuple: (all_probs, all_targets_or_ids) - all_probs: np.ndarray of shape (N, num_classes) - all_targets_or_ids: np.ndarray of targets (if val) or list of ids (if test) \"\"\" model.eval() all_probs = [] all_targets_or_ids = [] with torch.no_grad(): for batch in dataloader: images, targets, ids = batch images = images.to(device) # 1. Forward pass original out_orig = model(images) probs_orig = torch.sigmoid(out_orig) # 2. Forward pass flipped (TTA) # Flip along width dimension (dim 3 for NCHW) images_flipped = torch.flip(images, dims=[3]) out_flipped = model(images_flipped) probs_flipped = torch.sigmoid(out_flipped) # 3. Average probabilities avg_probs = (probs_orig + probs_flipped) / 2.0 all_probs.append(avg_probs.cpu().numpy()) # Collect targets or IDs based on mode if mode == \"test\": all_targets_or_ids.extend(ids) else: all_targets_or_ids.append(targets.numpy()) all_probs = np.concatenate(all_probs, axis=0) if mode != \"test\": all_targets_or_ids = np.concatenate(all_targets_or_ids, axis=0) return all_probs, all_targets_or_ids def get_model_predictions(model_name, mode, dataloader, device, load_cached_data=True): \"\"\" Gets predictions for specific model and mode (val/test). Implements caching of the raw probability arrays. 46 MARS: Modular Agent with Reflective Search for Automated AI Research Args: model_name (str): Name of the model architecture. mode (str): 'val' or 'test'. dataloader (DataLoader): The data loader. device (torch.device): Compute device. load_cached_data (bool): Whether to use cached .npy files. Returns: tuple: (probs, targets_or_ids) \"\"\" cache_dir = Config.WORKING_DIR os.makedirs(cache_dir, exist_ok=True) probs_path = os.path.join(cache_dir, f\"{model_name}_{mode}_probs.npy \") meta_path = os.path.join( cache_dir, f\"{model_name}_{mode}_meta.npy\" ) # targets or ids # Try to load from cache if load_cached_data and os.path.exists(probs_path) and os.path.exists (meta_path): print(f\"Loading cached predictions for {model_name} ({mode})...\") try: probs = np.load(probs_path) meta = np.load(meta_path, allow_pickle=True) if len(probs) != len(dataloader.dataset): raise ValueError( f\"Cache size mismatch: {len(probs)} vs {len( dataloader.dataset)}\" ) return probs, meta except Exception as e: print(f\"Failed to load cache: {e}. Re-running inference.\") # Run inference print(f\"Running inference for {model_name} ({mode})...\") # Load model and checkpoint model = get_model(model_name, num_classes=Config.NUM_CLASSES, pretrained=False) model = model.to(device) checkpoint_filename = f\"{model_name}_best.pth\" try: load_checkpoint(model, checkpoint_filename, device=device) except FileNotFoundError: print( f\"Warning: Checkpoint {checkpoint_filename} not found. Using random weights (for debugging only).\" ) 47 MARS: Modular Agent with Reflective Search for Automated AI Research probs, meta = predict_with_tta(model, dataloader, device, mode=mode) # Save to cache np.save(probs_path, probs) np.save(meta_path, meta) # Clean up del model torch.cuda.empty_cache() return probs, meta def ensemble_predictions(probs_list): \"\"\" Averages list of probability arrays. \"\"\" if not probs_list: return None return np.mean(probs_list, axis=0) def generate_submission(debug=Config.DEBUG, load_cached_data=True): \"\"\" Main function to generate the submission file. 1. Loads Val and Test loaders. 2. Gets predictions for Model and Model (with TTA). 3. Ensembles predictions. 4. Optimizes threshold on Validation set. 5. Applies threshold to Test set. 6. Saves submission.csv. \"\"\" device = torch.device(Config.DEVICE) # 1. Get DataLoaders # We don't need the train loader here _, val_loader, test_loader = get_dataloaders( debug=debug, batch_size=Config.BATCH_SIZE, num_workers=Config.NUM_WORKERS, load_cached_data=load_cached_data, ) models = [Config.MODEL_A_NAME, Config.MODEL_B_NAME] # 2. Validation Inference (for Threshold Calibration) print(\"--- Processing Validation Set ---\") val_probs_list = [] val_targets = None for model_name in models: probs, targets = get_model_predictions( model_name, \"val\", val_loader, device, load_cached_data MARS: Modular Agent with Reflective Search for Automated AI Research ) val_probs_list.append(probs) val_targets = targets # Targets should be same for all models # Ensemble Validation val_ensemble_probs = ensemble_predictions(val_probs_list) # Optimize Threshold print(\"Optimizing threshold on ensemble...\") best_threshold, best_score = optimize_threshold(val_targets, val_ensemble_probs) print(f\"Optimal Threshold: {best_threshold}\") print(f\"Validation Micro-F1 Score with Optimal Threshold: {best_score }\") # 3. Test Inference print(\"n--- Processing Test Set ---\") test_probs_list = [] test_ids = None for model_name in models: probs, ids = get_model_predictions( model_name, \"test\", test_loader, device, load_cached_data ) test_probs_list.append(probs) test_ids = ids # IDs should be same for all models # Ensemble Test test_ensemble_probs = ensemble_predictions(test_probs_list) # 4. Generate Submission CSV print(f\"Generating submission with threshold {best_threshold}...\") # Binarize predictions predictions_bin = (test_ensemble_probs > best_threshold).astype(int) submission_rows = [] for idx, image_id in enumerate(test_ids): # Get indices of positive classes pred_indices = np.where(predictions_bin[idx] == 1)[0] # Format as space-separated string pred_str = \" \".join(map(str, pred_indices)) submission_rows.append({\"id\": image_id, \"attribute_ids\": pred_str }) submission_df = pd.DataFrame(submission_rows) # Save out_path = os.path.join(Config.SUBMISSION_DIR, \"submission.csv\") submission_df.to_csv(out_path, index=False) print(f\"Submission saved to {out_path}\") 49 MARS: Modular Agent with Reflective Search for Automated AI Research return best_score Module: library/loss.py import torch import torch.nn as nn from library.config import Config class AsymmetricLoss(nn.Module): \"\"\" Asymmetric Loss (ASL) for multi-label classification. ASL optimizes the trade-off between precision and recall by decoupling the loss components for positive and negative samples. It down-weights easy negatives (which are dominant in multi-label settings) to focus learning on hard negatives and positive samples. Reference: \"Asymmetric Loss For Multi-Label Classification\" (ICCV 2021) \"\"\" def __init__( self, gamma_neg=Config.ASL_GAMMA_NEG, gamma_pos=Config.ASL_GAMMA_POS, clip=Config.ASL_CLIP, eps=1e-8, reduction=\"mean\", ): \"\"\" Args: gamma_neg (float): Focusing parameter for negative samples ( down-weights easy negatives). gamma_pos (float): Focusing parameter for positive samples. clip (float): Probability margin for shifting negative samples (hard thresholding). eps (float): Small constant for numerical stability in logarithms. reduction (str): Specifies the reduction to apply to the output: 'none' 'mean' 'sum'. \"\"\" super(AsymmetricLoss, self).__init__() self.gamma_neg = gamma_neg self.gamma_pos = gamma_pos self.clip = clip self.eps = eps self.reduction = reduction def forward(self, x, y): 50 MARS: Modular Agent with Reflective Search for Automated AI Research \"\"\" Args: (torch.Tensor): Logits (before sigmoid) of shape (N, C). (torch.Tensor): Ground truth labels of shape (N, C) (0 or 1). Returns: torch.Tensor: Calculated loss. \"\"\" # Explicit casting to float32 is crucial to prevent NaN during mixed-precision training # Logits and targets must be in float32 for stable log/pow computations = x.float() = y.float() # Calculate probabilities xs_pos = torch.sigmoid(x) # --- Positive Component --- # Standard Focal Loss term for positives: -y * (1-p)^gamma_pos * log(p) # We clamp the input to log to avoid log(0) loss_pos = ( * torch.pow(1.0 - xs_pos, self.gamma_pos) * torch.log(xs_pos.clamp(min=self.eps)) ) # --- Negative Component --- # ASL modification: Shifted probability for negatives # p_m = max(p - clip, 0) # This hard-thresholds easy negatives (where < clip) to have 0 loss and 0 gradient xs_neg = xs_pos if self.clip > 0: xs_neg = (xs_neg - self.clip).clamp(min=0) # Negative term: -(1-y) * (p_m)^gamma_neg * log(1 - p_m) loss_neg = ( (1.0 - y) * torch.pow(xs_neg, self.gamma_neg) * torch.log((1.0 - xs_neg).clamp(min=self.eps)) ) # Combine components # Note: The negative signs from the formulas are applied here loss = -(loss_pos + loss_neg) # Apply reduction if self.reduction == \"mean\": return loss.mean() elif self.reduction == \"sum\": return loss.sum() 51 MARS: Modular Agent with Reflective Search for Automated AI Research else: return loss Module: library/models.py import timm import torch.nn as nn from library.config import Config def get_model(model_name, num_classes=Config.NUM_CLASSES, pretrained=True ): \"\"\" Creates and returns model architecture based on the provided name using the timm library. This function instantiates the backbone (e.g., ResNet101d, ConvNeXtBase) and replaces the classification head to match the specific number of classes in the dataset. It adheres to the strategy of using Global Average Pooling followed by Linear projection, which is the default behavior of timm's create_model when num_classes is specified. Args: model_name (str): Name of the model architecture (e.g., ' resnet101d', 'convnext_base'). num_classes (int): Number of output classes (attributes). Defaults to Config.NUM_CLASSES. pretrained (bool): Whether to load pretrained ImageNet weights. Defaults to True. Returns: nn.Module: The instantiated PyTorch model. \"\"\" try: # Instantiate the model using timm # num_classes argument automatically resets the classifier head to the correct size # and initializes it randomly. model = timm.create_model( model_name, pretrained=pretrained, num_classes=num_classes ) return model except Exception as e: raise RuntimeError(f\"Failed to create model '{model_name}' using timm: {e}\") 52 MARS: Modular Agent with Reflective Search for Automated AI Research Module: library/train.py import os import time import torch import torch.nn as nn import torch.optim as optim import numpy as np from torch.cuda.amp import GradScaler, autocast from torch.optim.lr_scheduler import OneCycleLR from library.config import Config, seed_everything from library.dataset import get_dataloaders from library.models import get_model from library.loss import AsymmetricLoss from library.utils import calculate_f1_score, optimize_threshold, save_checkpoint class Trainer: \"\"\" Manages the training and validation process for single model. \"\"\" def __init__(self, model, optimizer, scheduler, criterion, device, scaler): self.model = model self.optimizer = optimizer self.scheduler = scheduler self.criterion = criterion self.device = device self.scaler = scaler self.best_score = 0.0 def train_epoch(self, train_loader, epoch): \"\"\" Runs one epoch of training. \"\"\" self.model.train() running_loss = 0.0 num_batches = len(train_loader) for i, (images, targets, _) in enumerate(train_loader): images = images.to(self.device, non_blocking=True) targets = targets.to(self.device, non_blocking=True) self.optimizer.zero_grad() # Mixed precision training with autocast(): outputs = self.model(images) loss = self.criterion(outputs, targets) self.scaler.scale(loss).backward() 53 MARS: Modular Agent with Reflective Search for Automated AI Research self.scaler.step(self.optimizer) self.scaler.update() if self.scheduler is not None: self.scheduler.step() running_loss += loss.item() avg_loss = running_loss / num_batches return avg_loss def validate(self, val_loader): \"\"\" Runs validation and calculates metrics. \"\"\" self.model.eval() running_loss = 0.0 all_preds = [] all_targets = [] with torch.no_grad(): for images, targets, _ in val_loader: images = images.to(self.device, non_blocking=True) targets = targets.to(self.device, non_blocking=True) # Mixed precision inference with autocast(): outputs = self.model(images) loss = self.criterion(outputs, targets) running_loss += loss.item() # Apply sigmoid for probabilities probs = torch.sigmoid(outputs) all_preds.append(probs.cpu().numpy()) all_targets.append(targets.cpu().numpy()) avg_loss = running_loss / len(val_loader) all_preds = np.concatenate(all_preds, axis=0) all_targets = np.concatenate(all_targets, axis=0) # Calculate F1 Score (Standard 0.5 threshold) f1_score = calculate_f1_score(all_targets, all_preds, threshold =0.5) # Calculate Optimized F1 Score (for monitoring potential) best_thresh, opt_f1 = optimize_threshold(all_targets, all_preds, num_steps=50) return avg_loss, f1_score, opt_f1, best_thresh def fit(self, train_loader, val_loader, epochs, model_name, patience =5): 54 MARS: Modular Agent with Reflective Search for Automated AI Research \"\"\" Main training loop with early stopping. \"\"\" print(f\"Starting training for {model_name}...\") patience_counter = 0 for epoch in range(1, epochs + 1): start_time = time.time() # Train train_loss = self.train_epoch(train_loader, epoch) # Validate val_loss, val_f1, opt_f1, best_thresh = self.validate( val_loader) elapsed = time.time() - start_time # Print full precision metrics print(f\"Epoch {epoch}/{epochs} Time: {elapsed:.2f}s\") print(f\" Train Loss: {train_loss}\") print(f\" Val Loss: print(f\" Val F1: print(f\" Opt F1: {val_loss}\") {val_f1}\") {opt_f1} (Thresh: {best_thresh})\") # Checkpointing # We use the standard F1 (0.5) or Optimized F1? # Usually for multi-label with unknown distribution shifts, optimizing threshold is safer. # However, for consistency with the prompt's metric definition, we track improvement on Opt F1 # as we will calibrate threshold in inference. current_score = opt_f1 if current_score > self.best_score: print( current_score}. Saving checkpoint.\" f\" Score improved from {self.best_score} to { ) self.best_score = current_score save_checkpoint( self.model, self.optimizer, epoch, self.best_score, f\"{model_name}_best.pth\", ) patience_counter = 0 else: patience_counter += 1 print(f\" No improvement. Patience: {patience_counter}/{ patience}\") if patience_counter >= patience: 55 MARS: Modular Agent with Reflective Search for Automated AI Research print(f\"Early stopping triggered at epoch {epoch}.\") break return self.best_score def train_specific_model(model_name, epochs=Config.EPOCHS, debug=Config. DEBUG): \"\"\" Sets up the environment and trains specific model architecture. Args: model_name (str): Name of the model to train (e.g., 'resnet101d'). epochs (int): Number of epochs to train. debug (bool): Whether to run in debug mode (subset of data). \"\"\" # 1. Setup seed_everything(Config.SEED) device = torch.device(Config.DEVICE) print(f\"Initializing {model_name} on {device}...\") # 2. Data # load_cached_data=True ensures we use the parquet cache if available train_loader, val_loader, _ = get_dataloaders( debug=debug, batch_size=Config.BATCH_SIZE, num_workers=Config.NUM_WORKERS, load_cached_data=True, ) # 3. Model model = get_model(model_name, num_classes=Config.NUM_CLASSES, pretrained=True) model = model.to(device) # 4. Optimizer & Scheduler optimizer = optim.AdamW( model.parameters(), lr=Config.LEARNING_RATE, weight_decay=Config. WEIGHT_DECAY ) # OneCycleLR needs total steps steps_per_epoch = len(train_loader) scheduler = OneCycleLR( optimizer, max_lr=Config.LEARNING_RATE, epochs=epochs, steps_per_epoch=steps_per_epoch, pct_start=0.1, # Warmup for first 10% div_factor=25.0, final_div_factor=1000.0, ) 56 MARS: Modular Agent with Reflective Search for Automated AI Research # 5. Loss & Scaler criterion = AsymmetricLoss( gamma_neg=Config.ASL_GAMMA_NEG, gamma_pos=Config.ASL_GAMMA_POS, clip=Config.ASL_CLIP, ) scaler = GradScaler() # 6. Trainer trainer = Trainer(model, optimizer, scheduler, criterion, device, scaler) # 7. Execute best_score = trainer.fit(train_loader, val_loader, epochs, model_name ) print(f\"Training finished for {model_name}. Best F1 Score: { best_score}\") # Clear memory del model, optimizer, scheduler, scaler, trainer, train_loader, val_loader torch.cuda.empty_cache() return best_score Module: library/utils.py import os import torch import numpy as np from sklearn.metrics import f1_score from library.config import Config def calculate_f1_score(y_true, y_pred, threshold=0.5): \"\"\" Calculates the Micro-averaged F1 score. Args: y_true (np.ndarray or torch.Tensor): Ground truth labels (N, C). y_pred (np.ndarray or torch.Tensor): Predicted probabilities (N, C). threshold (float): Threshold for binarizing predictions. Returns: float: Micro F1 score. \"\"\" # Ensure inputs are numpy arrays if isinstance(y_true, torch.Tensor): y_true = y_true.detach().cpu().numpy() if isinstance(y_pred, torch.Tensor): MARS: Modular Agent with Reflective Search for Automated AI Research y_pred = y_pred.detach().cpu().numpy() # Binarize predictions based on the threshold y_pred_bin = (y_pred > threshold).astype(int) # Calculate Micro-F1 score return f1_score(y_true, y_pred_bin, average=\"micro\") def optimize_threshold(y_true, y_pred, num_steps=100): \"\"\" Finds the optimal decision threshold for Micro F1 score via linear search. Args: y_true (np.ndarray or torch.Tensor): Ground truth labels. y_pred (np.ndarray or torch.Tensor): Predicted probabilities. num_steps (int): Number of steps in the linear search (between 0 and 1). Returns: tuple: (best_threshold, best_score) \"\"\" if isinstance(y_true, torch.Tensor): y_true = y_true.detach().cpu().numpy() if isinstance(y_pred, torch.Tensor): y_pred = y_pred.detach().cpu().numpy() best_threshold = 0.5 best_score = -1.0 # Search range from 0.01 to 0.99 to avoid edge cases thresholds = np.linspace(0.01, 0.99, num_steps) for thresh in thresholds: score = calculate_f1_score(y_true, y_pred, threshold=thresh) if score > best_score: best_score = score best_threshold = thresh return best_threshold, best_score def save_checkpoint(model, optimizer, epoch, score, filename): \"\"\" Saves model checkpoint to the working directory. Args: model (torch.nn.Module): The model to save. optimizer (torch.optim.Optimizer): The optimizer state. epoch (int): Current epoch. score (float): Validation score (F1). filename (str): Name of the file to save. \"\"\" 58 MARS: Modular Agent with Reflective Search for Automated AI Research # Ensure the working directory exists os.makedirs(Config.WORKING_DIR, exist_ok=True) filepath = os.path.join(Config.WORKING_DIR, filename) state = { \"epoch\": epoch, \"model_state_dict\": model.state_dict(), \"optimizer_state_dict\": ( optimizer.state_dict() if optimizer is not None else None ), \"score\": score, } torch.save(state, filepath) def load_checkpoint(model, filename, optimizer=None, device=Config.DEVICE ): \"\"\" Loads model checkpoint. Args: model (torch.nn.Module): The model to load weights into. filename (str): Name of the checkpoint file (relative to WORKING_DIR or absolute). optimizer (torch.optim.Optimizer, optional): Optimizer to load state into. device (str): Device to map location to. Returns: tuple: (epoch, score) \"\"\" # Determine full path if os.path.exists(filename): filepath = filename else: filepath = os.path.join(Config.WORKING_DIR, filename) if not os.path.exists(filepath): raise FileNotFoundError(f\"Checkpoint file not found: {filepath}\") # Load checkpoint checkpoint = torch.load(filepath, map_location=device) # Load model state model.load_state_dict(checkpoint[\"model_state_dict\"]) # Load optimizer state if provided and available if optimizer is not None and checkpoint.get(\"optimizer_state_dict\") is not None: optimizer.load_state_dict(checkpoint[\"optimizer_state_dict\"]) epoch = checkpoint.get(\"epoch\", 0) 59 MARS: Modular Agent with Reflective Search for Automated AI Research score = checkpoint.get(\"score\", 0.0) return epoch, score Main Script: main.py import sys import os import numpy as np import pandas as pd import torch import cv2 from library.config import Config, seed_everything from library.train import train_specific_model from library.inference import ( get_model_predictions, ensemble_predictions, generate_submission, ) from library.utils import optimize_threshold from library.dataset import get_dataloaders def main(): # 1. Setup seed_everything(Config.SEED) device = torch.device(Config.DEVICE) print(\"=== Starting Orchestration Script ===\") # 2. Train Models # We train for 5 epochs to improve convergence while staying within the 4-hour limit. # Cite Lesson 00003: Longer schedule allows better convergence. # Cite Lesson 00018: Ensemble of diverse models. print(f\"n[Training] Starting training for Model A: {Config. MODEL_A_NAME}\") train_specific_model(Config.MODEL_A_NAME, epochs=5, debug=False) print(f\"n[Training] Starting training for Model B: {Config. MODEL_B_NAME}\") train_specific_model(Config.MODEL_B_NAME, epochs=5, debug=False) # 3. Validation & Metric Calculation print(\"n[Validation] Loading validation data...\") # We only need the validation loader here to get targets and run inference _, val_loader, _ = get_dataloaders(debug=False, load_cached_data=True ) print(\"[Validation] Generating ensemble predictions...\") # Get predictions for Model # load_cached_data=False ensures we use the newly trained weights 60 MARS: Modular Agent with Reflective Search for Automated AI Research probs_a, targets = get_model_predictions( Config.MODEL_A_NAME, \"val\", val_loader, device, load_cached_data= False ) # Get predictions for Model probs_b, _ = get_model_predictions( Config.MODEL_B_NAME, \"val\", val_loader, device, load_cached_data= False ) # Ensemble predictions (simple average) ensemble_probs = ensemble_predictions([probs_a, probs_b]) # Ensure targets are numpy array if isinstance(targets, torch.Tensor): targets = targets.cpu().numpy() elif isinstance(targets, list): targets = np.array(targets) # Optimize threshold best_thresh, best_score = optimize_threshold(targets, ensemble_probs) # REQUIRED OUTPUT print(f\"Final Validation Metric: {best_score}\") # 4. Failure Analysis print(\"n[Analysis] Performing Failure Analysis...\") # Calculate per-sample F1 to determine error magnitude # Binarize predictions using the optimal threshold preds_bin = (ensemble_probs > best_thresh).astype(int) # Calculate F1 per sample (instance-level) # F1 = 2*TP / (2*TP + FP + FN) tp = np.sum((preds_bin == 1) & (targets == 1), axis=1) fp = np.sum((preds_bin == 1) & (targets == 0), axis=1) fn = np.sum((preds_bin == 0) & (targets == 1), axis=1) epsilon = 1e-7 f1_samples = (2 * tp) / (2 * tp + fp + fn + epsilon) error_magnitude = 1.0 - f1_samples # Load metadata to get features val_df = pd.read_csv(Config.VAL_CSV) # Feature 1: Label Cardinality (from ground truth) # Handle potential NaNs in attribute_ids val_df[\"attribute_ids\"] = val_df[\"attribute_ids\"].fillna(\"\") val_df[\"num_labels\"] = val_df[\"attribute_ids\"].apply( lambda x: len(x.split()) if x.strip() else 0 ) # Feature 2: Image Brightness (from image content) # We process subset of validation images to save time 61 MARS: Modular Agent with Reflective Search for Automated AI Research sample_size = min(2000, len(val_df)) sample_indices = np.random.choice(len(val_df), size=sample_size, replace=False) brightness_values = [] sampled_errors = [] sampled_cardinality = [] print(f\" Processing {sample_size} images for feature extraction...\") for idx in sample_indices: row = val_df.iloc[idx] path = os.path.join(Config.INPUT_DIR, row[\"file_path\"]) # Read image img = cv2.imread(path) if img is not None: # Calculate mean brightness = np.mean(img) / 255.0 brightness_values.append(b) sampled_errors.append(error_magnitude[idx]) sampled_cardinality.append(val_df.iloc[idx][\"num_labels\"]) # Calculate Correlations if len(sampled_errors) > 1: # Correlation with Label Cardinality corr_card = np.corrcoef(sampled_cardinality, sampled_errors)[0, print(f\"Correlation (Error vs Label Cardinality): {corr_card:.4f # Correlation with Brightness corr_bright = np.corrcoef(brightness_values, sampled_errors)[0, print(f\"Correlation (Error vs Image Brightness): {corr_bright:.4f 1] }\") 1] }\") else: print(\"Insufficient data for correlation analysis.\") # 5. Submission THRESHOLD_SCORE = 0. print(f\"n[Submission] Checking threshold: {best_score} > { THRESHOLD_SCORE}\") if best_score > THRESHOLD_SCORE: print(\"Threshold met. Generating submission file...\") # generate_submission handles test inference and saving # load_cached_data=False ensures we generate fresh test predictions generate_submission(debug=False, load_cached_data=False) else: print(\"Threshold not met. Skipping submission generation.\") print(\"n=== Workflow Completed ===\") 62 MARS: Modular Agent with Reflective Search for Automated AI Research if __name__ == \"__main__\": main()"
        }
    ],
    "affiliations": [
        "Google Cloud AI Research"
    ]
}