{
    "paper_title": "Optimizing Chain-of-Thought Reasoners via Gradient Variance Minimization in Rejection Sampling and RL",
    "authors": [
        "Jiarui Yao",
        "Yifan Hao",
        "Hanning Zhang",
        "Hanze Dong",
        "Wei Xiong",
        "Nan Jiang",
        "Tong Zhang"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Chain-of-thought (CoT) reasoning in large language models (LLMs) can be formalized as a latent variable problem, where the model needs to generate intermediate reasoning steps. While prior approaches such as iterative reward-ranked fine-tuning (RAFT) have relied on such formulations, they typically apply uniform inference budgets across prompts, which fails to account for variability in difficulty and convergence behavior. This work identifies the main bottleneck in CoT training as inefficient stochastic gradient estimation due to static sampling strategies. We propose GVM-RAFT, a prompt-specific Dynamic Sample Allocation Strategy designed to minimize stochastic gradient variance under a computational budget constraint. The method dynamically allocates computational resources by monitoring prompt acceptance rates and stochastic gradient norms, ensuring that the resulting gradient variance is minimized. Our theoretical analysis shows that the proposed dynamic sampling strategy leads to accelerated convergence guarantees under suitable conditions. Experiments on mathematical reasoning show that GVM-RAFT achieves a 2-4x speedup and considerable accuracy improvements over vanilla RAFT. The proposed dynamic sampling strategy is general and can be incorporated into other reinforcement learning algorithms, such as GRPO, leading to similar improvements in convergence and test accuracy. Our code is available at https://github.com/RLHFlow/GVM."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 5 ] . [ 1 1 9 3 2 0 . 5 0 5 2 : r Optimizing Chain-of-Thought Reasoners via Gradient Variance Minimization in Rejection Sampling and RL Jiarui Yao1 Yifan Hao1 Hanning Zhang1 Hanze Dong2 Wei Xiong1 Nan Jiang1 Tong Zhang1 1University of Illinois Urbana-Champaign 2Salesforce AI Research"
        },
        {
            "title": "Abstract",
            "content": "Chain-of-thought (CoT) reasoning in large language models (LLMs) can be formalized as latent variable problem, where the model needs to generate intermediate reasoning steps. While prior approaches such as iterative reward-ranked fine-tuning (RAFT) have relied on such formulations, they typically apply uniform inference budgets across prompts, which fails to account for variability in difficulty and convergence behavior. This work identifies the main bottleneck in CoT training as inefficient stochastic gradient estimation due to static sampling strategies. We propose GVM-RAFT, prompt-specific Dynamic Sample Allocation Strategy designed to minimize stochastic gradient variance under computational budget constraint. The method dynamically allocates computational resources by monitoring prompt acceptance rates and stochastic gradient norms, ensuring that the resulting gradient variance is minimized. Our theoretical analysis shows that the proposed dynamic sampling strategy leads to accelerated convergence guarantees under suitable conditions. Experiments on mathematical reasoning show that GVM-RAFT achieves 2-4 speedup and considerable accuracy improvements over vanilla RAFT. The proposed dynamic sampling strategy is general and can be incorporated into other reinforcement learning algorithms, such as GRPO, leading to similar improvements in convergence and test accuracy. Our code is available at https://github.com/RLHFlow/GVM."
        },
        {
            "title": "Introduction",
            "content": "We consider mathematical reasoning with large language models (LLMs): given prompt , and aims to produce correct final answer Z. prevalent approach in this area is the chain-ofthought (CoT) reasoning (Wei et al., 2022), in which the model generates step-by-step rationale before outputting the final answer z. In practice, we are typically given pre-trained and instruction fine-tuned LLM parameterized by θ0 and training samples {(xi, zi)} of prompt-answer pairs. Additionally, we assume access to verifier r(x, z) {0, 1} that indicates whether predicted answer is correct or not. This is standard in practice and is particularly popular by the recent DeepSeek-R1 project (DeepSeek-AI et al., 2025), which suggests using only symbolic verifier rather than training neural reward model. The goal is to improve model performance by training it to generate high-quality CoT rationales that improve the final answer accuracy on unseen prompts. We formalize CoT reasoning as latent variable problem, treating the rationale as hidden. From this perspective, we propose new algorithmic framework based on the expectation-maximization (EM) The first two authors contributed equally with random author order, detailed contributions deferred to Appendix A. Emails: {jiarui14, yifanh12, hanning5, wx13, nanjiang, tozhang}@illinois.edu, hanze.dong@salesforce.com. Preprint. Under review. algorithm, which we formalize in Section 2. Prior works such as Sordoni et al. (2023); Singh et al. (2023) have shown that this EM framework can be implemented as variant of iterative reward-ranked fine-tuning (RAFT) (Dong et al., 2023; Touvron et al., 2023), also known as rejection sampling fine-tuning in the literature. Specifically, RAFT iteratively alternates between the expectation and maximization steps: 1. E-step: Prompt LLMs to generate responses per prompt, and keep responses with the highest reward only (with the correct final answers). This process can be thought to be approximating the posterior distribution of the latent variable and the evidence lower bound (ELBO). See Section 2 for formal presentation. 2. M-step: Fine-tune the LLMs on the selected responses from the E-step. The fine-tuned model is used for the next-iteration E-step. Figure 1: The demonstration of the whole pipeline for GVM. Starting from π0, which supervised fine-tuning (SFT) model, GVM will iteratively conduct the estimation and budget assignment process for times according to the description in Algorithm 1. Each iteration could be decomposed into two stages, and the final policy model will be returned after those iterations. Although RAFT and its variants have been widely applied to tasks in the post-training stage of LLMs, they are primarily motivated by the idea of imitating the best-of-n sampling rather than variance reduction. As result, theoretical understanding is often lacking. For instance, these RAFT-style approaches typically adopt uniform sampling strategy that treats all prompts equally, ignoring differences in sample efficiency or convergence behavior In this work, we revisit CoT reasoning under the EM framework and provide deeper theoretical analysis. We identify the key bottleneck as the Monte Carlo estimation of the ELBO gradient during the E-step, which arises due to the intractability of going over all possible CoT rationales. Previous works mainly use the best-of-n sampling to allocate the inference budget uniformly (Sordoni et al., 2023; Singh et al., 2023), which overlook the differences in the convergence rate under different prompts. To address this, we propose dynamic sample budget allocation strategy that adaptively assigns computational resources across prompts based on theoretical insights. This leads to more efficient Monte Carlo estimation of the ELBO gradient. Our resulting algorithm, refined RAFT variant with dynamic inference budget scheduling through Gradient Variance Minimization (GVM-RAFT), achieves substantial performance improvements and even sometimes outperforms state-of-the-art deep RL methods such as GRPO (Shao et al., 2024) in our Qwen-based case studies. In particular, the sample budget allocation schedule itself can be of independent interests and we also extend our techniques to the RL algorithm, where it also brings notable improvements. We summarize our contributions as follows: 1. We revisit the EM framework and RAFT in the context of CoT reasoning, and identify that major limitation of current approaches lies in inefficient stochastic gradient estimation 2 caused by uniform and static sampling strategies (i.e., best-of-n sampling), which fail to account for prompt-specific difficulty and convergence behavior. 2. Motivated by the goal of minimizing the variance of stochastic gradient, we propose dynamic sampling strategy that adaptively allocates computational resources based on prompt hardness and gradient norms. Our approach provides both intuitive theoretical insight and rigorous convergence guarantees, establishing principled framework for efficient onpolicy sampling under computational budget constraints. 3. We apply our method to both RAFT++ and GRPO algorithms with real-world experiments on mathematical reasoning tasks. Our results demonstrate that the proposed approach achieves 2-4 speedup in convergence rate and also considerably improve the final test accuracy. 1.1 Related Work Iterative rejection sampling fine-tuning. line of works studies the RAFT-type algorithms (iterative rejection sampling fine-tuning) (Dong et al., 2023; Touvron et al., 2023; Gulcehre et al., 2023; Yuan et al., 2023). These algorithms iteratively generate responses per prompt, discard those with incorrect answers, and fine-tune the LLM on the remaining accepted samples. Another closely related method is STaR (Zelikman et al., 2022), which also involves iterative training on self-generated CoT responses. In comparison, STaR uses greedy decoding rather than sampling, which deviates from the EM principle and suffers from severe bias across prompts of different difficulty levels (see Section 2 for details). STaR also proposes to provide the answer in the prompt to generate CoT responses for harder problems. However, our preliminary experiments suggest that LLMs often generate wrong reasoning path and output the correct answer regardless of the context. Additionally, STaR retrains from the original pre-trained model in each iteration, rather than bootstrapping from the updated model. These algorithms have been widely applied in the post-training stage of LLMs since then (Touvron et al., 2023; Xiong et al., 2025b; AI@Meta, 2024; Yang et al., 2024a). We also mention in passing that this algorithm framework can further date back to the expert iteration in the RL literature Anthony et al. (2017). Lastly, Tong et al. (2024) study inference budget allocation in the context of synthetic data construction. Their approach uses fixed model to repeatedly sample responses until correct CoT rationales are obtained per prompt (m is hyper-parameter). While this setup also studies budget-aware sampling, they only study the offline setting and their method lacks dynamic scheduling mechanism so cannot be applied to more general iterative online algorithms. Chain-of-thought reasoning under EM framework. Our work is also closely related with the line of work studying EM framework in CoT reasoning. Singh et al. (2023) connects RAFT with the EM algorithm under the CoT reasoning framework. We will cover such connection in Section 2 for completeness and also discuss the limitation of the vanilla RAFT algorithm, and motivate our algorithm designs. Another related work is Sordoni et al. (2023), which proposes gradient-free EM-based approach for prompt selection, emphasizing different direction. Our work is also closely related to Hoffman et al. (2023), which proposes TRICE that optimizes an ELBO-inspired objective using MCMC to approximate the posterior over the latent CoT rationales. TRICE maintains table of promptCoT pairs and, in each iteration, samples new CoT per prompt, and replace the old CoT record yold with if leads to correct final answer. They also propose to add mean-zero baseline to reduce the variance of the gradient estimator (Owen and Zhou, 2000). In comparison, we design dynamic inference budget allocation scheduling for more effective estimation of the ELBO gradient by rejection sampling so the algorithm framework and techniques diverge significantly. Moreover, we implement our algorithm in highly online and on-policy way (see Section 3.3 for details) to achieve the best performance. The lazy update of TRICE in maintaining the table of prompt-CoT pairs may not fit in this scenario. Beyond algorithmic differences, we also provide theoretical analysis under standard conditions such as smoothness, which is missing in all these prior works. Hu et al. (2024) offers more detailed statistical perspective on CoT reasoning as latent variable modeling. However, they mainly focus on in-context learning rather than iterative fine-tuning. Lastly, recent work Zhong et al. (2025) proposes to use neural network to approximate the posterior distribution of latent CoT rationales and train the network by PPO. They then use the network to generate responses for the SFT or DPO algorithms, whose techniques are completely different from ours. They also provide theoretical convergence analysis of EM algorithm under the KL-regularized Markov Decision Process 3 (MDP) framework using the techniques from RL theory literature (Zhong et al., 2024). This type of analysis typically ignores the optimization complexity and requires to find the exact minimizer of E-step and M-step at each iteration, thus differing from our techniques and results. RL algorithms for LLM post training. Reinforcement learning has played central role in the post-training of LLMs. The most prominent example is reinforcement learning from human feedback (RLHF), which brings the first generation of Chat-GPT3.5 (Ouyang et al., 2022; Bai et al., 2022). These methods typically optimize learned Bradley-Terry reward model (Bradley and Terry, 1952) using the Proximal Policy Optimization (PPO) algorithm (Schulman et al., 2017). To simplify the RLHF, long line of works has proposed simplified contrastive learning objectives based on pairwise comparisons, such as Slic-HF (Zhao et al., 2023), DPO (Rafailov et al., 2023), IPO (Azar et al., 2023), GPO (Tang et al., 2024), and their online variants (Xiong et al., 2023; Dong et al., 2024). After the release of OpenAIs O1 (Jaech et al., 2024) and DeepSeeks R1 (DeepSeek-AI et al., 2025), RL methods based on verifier scores, which evaluate the correctness of the final answer, have attracted significant attention in building reasoning models (Yu et al., 2025; Shi et al., 2025). This project also follows the recent convention and falls into this framework. In particular, to avoid the critic in PPO, both industry and academic researchers have revisited REINFORCE-style algorithms (Williams and Peng, 1991), including ReMax (Li et al., 2023), RLOO (Ahmadian et al., 2024; Kool et al., 2019), GRPO (Shao et al., 2024), Reinforce++ (Hu, 2025), and Reinforce-rej (Xiong et al., 2025a). We also mention in passing that the RAFT-type algorithms can also be viewed as special variant of Reinforce-style algorithms, where reward is either 1 or 0. Throughout the rest of this paper, we primarily present our methods with RAFT, since it aligns most naturally with the EM objective. However, the presented dynamic inference budget scheduling can be of independent interests beyond the RAFT algorithm. We will show that integrating it into other RL-style methods, such as PPO, REINFORCE, or GRPO, can also bring notable improvements."
        },
        {
            "title": "2 Problem Formulation and EM Framework",
            "content": "In this section, we formally define the problem, review existing approaches, and motivate our algorithm designs. We consider the chain-of-thought (CoT) reasoning process as: z, , Y, Z, where is prompt, is the intermediate CoT and is the final predicted answer. We fit the data [x, y, z] within the following distribution function class: Ξ := {P(y, zx, θ) = P(yx, θ) P(zy, θ) θ Rp} . (1) Our target is to learn good model (θ) Ξ, which can minimize the negative log-likelihood of predicting the correct answer: L(θ) = Exd0 ln P(zx, θ), (2) where d0 is prompt distribution and P(θ) denotes the distribution induced by the model with parameters θ. While many math datasets include reference CoT rationales, we do not include these reference rationales here. This is because recent practice typically does not fine-tune LLMs on these CoT rationales after the pre-training/SFT stages (DeepSeek-AI et al., 2025; Team et al., 2025). The process from to can be complex, making it difficult to directly estimate the distribution (z x) from the observed data [x, z]. However, by introducing latent intermediate variable y, the conditional distributions (y x) and (z y) are often easier to estimate, thereby simplifying the problem: L(θ) = Exd0 ln (cid:88) yY P(yx, θ)P(zx, y, θ). Introducing the intermediate CoT naturally motivates the use of the expectation-maximization (EM) algorithm, which iteratively approximates the posterior over latent variables. Derivation of the EM algorithm. We consider training set = {(xi, zi)}m i=1 drawn from d0 with zi being the labeled ground-truth answer to illustrate the idea. Following the standard derivation 4 of EM algorithm, we can bound the L(θ) as follows: (cid:88) i=1 (cid:88) L(θ) = ln P(zixi, θ) = ln (cid:88) i=1 (cid:88) Qi(y) ln (cid:18) P(y, zixi, θ) Qi(y) i=1 yY (cid:88) Qi(y) yY (cid:19) P(y, zixi, θ) Qi(y) (3) (cid:88) = EyQi(y) ln P(y, zixi, θ) + (cid:88) H(Qi(y)) i=1 i= (cid:124) (cid:125) (cid:123)(cid:122) JQ(θ) where Qi() is probability distribution over so that Qi(y) 0 and (cid:80) yY Qi(y) = 1. The last inequality is from the convexity of ln() and Jensens inequality (Lemma 2) and H(p) := Etp(t) ln p(t) is the entropy. In the E-step, our goal is to find Qi(y) to get sharper upper bound for L(θ). In particular, the equality is reached with the posterior distribution of y: Qi(y) = P(yxi, zi, θ) = P(y, zixi, θ) P(zixi, θ) = P(yxi, θ) P(ziy, θ) (cid:80) yY P(yxi, θ)P(ziy, θ) := P(yxi, θ) P(ziy, θ) Z(xi, zi, θ) , (4) where Z(xi, zi, θ) denotes the normalization constant and the second equality uses Equation (1). Here JQ(θ) (cid:80)m i=1 H(Qi(y)) is referred to as the evidence lower bound (ELBO) in the literature (Bishop and Nasrabadi, 2006; Kingma et al., 2013), and P(yxi, zi, θ) is the induced posterior distribution, which is not the equivalent to the autoregressive distribution of CoT when we present the LLMs with prompts and answers. In the M-step, to minimize L(θ), we can fix Qi(y) as in Equation (4) and indirectly minimize JQ(θ). Note that we do not include the entropy loss because it is constant in the M-step since we will fix Qi(y). To summarize, the EM algorithm will alternate between the following two steps: at iteration t, 1. E-step: Update the posterior distribution of the latent CoT Qt according to Equation (4) and obtain the JQt defined in Equation (3). 2. M-step: The goal of the M-step is to update LLMs (θt) to minimize JQt: 1 (cid:88) i=1 EyQt i(y) ln P(y, zixi, θ). The updated model (θt+1) is used for the next-iteration E-step. To apply the EM algorithm, we need to sample Qt i(y) to approximate the objective. central challenge is that computing Qt i(y) and JQt(θ) requires summing over all possible latent CoT reasoning because we need to get the normalization constant in Equation (4). This process is usually computationally intractable. Therefore, the objective JQt(θ) must be approximated via sampling. To approximate the posterior Qi(y), one standard approach is rejection sampling (Neal, 2003). We remark that we refer rejection sampling to the one in statistics, which is used to approximate target distribution Qi(y) by proposal distribution P(yx, θ), which we can sample from. In the literature of RLHF or LLM, rejection sampling is often referred to as the best-of-n sampling (Bai et al., 2022; Ouyang et al., 2022). Specifically, to approximate Qi(y) by P(yxi, θ), the rejection sampling proceeds as follows: 1. Sample P(yxi, θ); 2. Draw Uniform([0, 1]); 3. Accept if Qi(y) P(yxi,θ)M , where > 0 satisfies Qi(y) P(yxi, θ) for all Y. We consider the negative log-likelihood here so it becomes an upper bound 5 We notice that valid choice of is 1/Z(xi, zi, θ), leading to an accept probability of P(ziy, θ). In mathematical reasoning, given the CoT rationale, the final predicted answer is typically of low randomness. Then, the rejection sampling in statistics aligns well with the best-of-n sampling, where we only keep all the responses with the correct answer. This eventually leads to the RAFTtype algorithms. We remark that this connection between the EM framework and the RAFT-type algorithms has been previously observed in Singh et al. (2023); Sordoni et al. (2023). However, we argue that the current best-of-n sampling implementation is overly coarse and insufficiently faithful to the true E-step. Specifically, if prompt xi is very difficult for current (θt), the density of P(yxi, θ) will concentrate on the wrong CoT rationales so that P(ziy, θ) is close to zero. In this case, the accept probability will be very low and we need many samples before we can accept one valid y. In contrast, if our current model (θt) can already output correct CoT rationale in most of the time, we will accept most of the generated responses. Eventually, with fixed inference budget, this uniform allocation (n responses per prompt) tends to bias the accepted samples toward easier prompts with higher acceptance rates. Algorithm 1 Meta Algorithm: GVM-EM 1: Input: Initial parameters θ0, training samples = {(xi, zi)}n i=1, number of epochs , initial posterior Q0 = P( θ0). 2: for = 0, . . . , do 3: 4: i=1. Update the posterior distribution over latent CoT rationales E-step (Expectation): Sample set of samples Bt = {xi, zi}m Qt() using Equation (4). For each prompt xi, compute the required number of samples nt or (2) Practical Algorithm 2. Perform rejection sampling to obtain accepted responses y. Collect corresponding (xi, zi, y) into Dt , such that Qt M-step (Maximization): Update model parameters via gradient descent using: according to (1) Theoretical Proposition 1 i(). 5: 6: 7: 8: θM (θt) = 1 (cid:88) i=1 1 Dt (cid:88) yj Dt θ log P(yj, zi xi, θ). 9: end for 10: Output: Final model (θT ). 2.1 Notation The true marginal negative log-likelihood is denoted as L(θ). The negative ELBO at iteration is denoted as Lt, which is equal to Lt(θ) = JQt(θ) + (cid:88) i=1 H(Qi(y)). Our derivation in the next section will focus on JQt(θ) since entropy term is considered to be constant in M-step when fixing Qt. We also present notation table in Table 3 to improve readability."
        },
        {
            "title": "3 Gradient Variance Minimization by Dynamic Sample Allocation",
            "content": "To address the limitations of best-of-n sampling, we propose dynamic inference budget allocation strategy that adapts to the acceptance rates of rejection sampling for each prompt xi. The overall meta-algorithm is presented in Algorithm 1, and in what follows, we describe the budget allocation mechanism in detail. 6 3.1 Dynamic Inference Budget Scheduling to Minimize Gradient Variance Unbiased gradient estimation. We begin by formulating the true gradient at iteration under the EM objective JQt: JQt(θ) = (cid:88) (cid:88) i=1 yY Qt i(y) ln P(y, zixi, θ) = (cid:88) i=1 EyQt ln P(y, zixi, θ), (5) i(y) = P(yxi, zi, θt1) is the posterior distribution of y. However, this distribution is where Qt intractable to compute exactly. Therefore, we approximate Qt i(y) via rejection sampling by drawing times from current LLMs P(yxi, θt1). This leads to the following unbiased estimator for nt Equation (5). Lemma 1 (Unbiased Gradient Estimator). In the iteration t, denoting Dt samples on related to (xi, zi), we have the following unbiased gradient estimator for JQt: as the set of accepted (cid:88) i=1 1 ipt nt (cid:88) yj Dt ln P(yj, zixi, θt1), (6) where pt = EyP(xi,θ)P (ziy, θ) is the average accept rate of rejection sampling. Proof. Taking the expectation of Equation (6), we obtain: (cid:88) i= 1 ipt nt (cid:88) yj Dt ln P(yj, zixi, θt1) = = = (cid:88) i=1 (cid:88) i=1 (cid:88) i=1 1 ipt nt 1 ipt nt nt i(cid:88) i=1 nt i(cid:88) j=1 Eyj P(xi,θt1) [1(accept yj) ln P(yj, zixi, θt1)] pt Eyj Qt ln P(yj, zixi, θt1) EyQt ln P(y, zixi, θt1), which is the same as Equation (5). Variance-aware sampling allocation. While the estimator above is unbiased, its variance can vary significantly across prompts. Prompts with low acceptance rates introduce high variance due to the small number of accepted samples. Reducing variance is crucial for efficient training with stochastic gradient, as also emphasized in prior works in statistics and optimization, including Roux et al. (2012); Johnson and Zhang (2013); Defazio et al. (2014); Chen et al. (2018). To design more efficient sampling strategy, we analyze the variance of the gradient estimator and optimize the allocation. (cid:88) i= (ln P(yj, zixi, θ)) 1 ipt nt (cid:88) yj Dt = = = (cid:88) i= (cid:88) i=1 (cid:88) i=1 (cid:88) i=1 1 ipt (nt i)2 1 ipt (nt i)2 1 ipt (nt i)2 nt i(cid:88) j=1 nt i(cid:88) j=1 nt i(cid:88) j=1 1 ipt nt EyQt (cid:124) Vyj P(xi,θt1) (1(accept yj) (ln P(yj, zixi, θ))) Eyj P(xi,θt1) 1(accept yj) (ln P(yj, zixi, θ))2 pt Eyj Qt (ln P(yj, zixi, θ))2 (ln P(y, zixi, θ))2 (cid:125) (cid:123)(cid:122) G2 . Given fixed total sampling budget , we seek to allocate {nt i} to minimize this upper bound: min (cid:41) (cid:40) (cid:88) i=1 G2 int pt , s.t. (cid:88) i=1 nt = N. In practice, some prompts are totally beyond the ability of the current LLMs or cannot be evaluated by the verifier (e.g., due to some label error). This leads to extremely low acceptance rates and unstable gradient estimations. To mitigate this, we introduce regularization term that penalizes sampling on such prompts. The revised objective becomes: min (cid:40) (cid:88) i=1 1 1 + α/(pt i)β G2 int pt (cid:41) , s.t. (cid:88) i= nt = N, (7) where α > 0, β 2 are hyperparameters that control the regularization strength. For example, as the accept rate pt 0 and prevents excessive sampling on uninformative prompts. Recent studies such as Xiong et al. (2025a) have also demonstrated the critical role it plays for stable training of online RL algorithms. i)β)1 0, which leads to the fact that sample size nt 0, the weight (1 + α/(pt Solving the regularized optimization problem in Equation (7) yields the following closed-form solution to the optimal sampling allocation: Proposition 1. The optimal number of samples allocated to each prompt is: nt = (cid:113) Gi/ (cid:80)n l=1 Gl/ + α pt (pt i)β1 (cid:113) + α pt (pt )β1 (cid:113) Gi + α pt (pt i)β1 , = 1, . . . , m. The proof is deferred to Appendix D. Remark 1. Accepted sample size has lower bound as: 2(α(β 1))1/(2β) (cid:88) i=1 Gi l=1 Gl (cid:80)m (cid:113) pt + α pt (pt i)β1 . With Remark 1, our proposed budget scheduling method is not only efficient but also guarantees sufficient number of accepted samples during the training process, even in the presence of informative prompts. Algorithm 2 shows practical implementation of the GVM algorithm. 8 Algorithm 2 GVM: Practical Implementation 1: Input: Model parameter θ, training samples {xi, zi}n sample size , penalty parameter {α, β}. i=1, total inference budget , parameter estimation 2: for = 1, . . . , do 3: 4: For each prompt xi, sample times to get {xi, yj Estimate the accept rate pt , zj }N j=1. and Lipschitz bounds Gi on each prompt xi as pi = Gi = (cid:80)N j=1 1(zj = zi) , (cid:88) 1jN ,zj =zi 1 pi θ ln P(yj , zixi, θ)2. 5: Calculate sample size {ni} as: (cid:113) Gi/ ni = 6: end for 7: Output: {ni}m i=1. 3.2 Theoretical Result pi + α (cid:113) (pi)β1 pl + α (pl)β1 . (cid:80)n l=1 Gl/ In this section, we present the theoretical guarantee of loss convergence. Without loss of generalization, we assume that each E-step is followed by M-steps. For the t-th E-step, let br denote the batch size for prompt xi at the r-th M-step, where kt < kt, and the corresponding sample batch is denoted by Br . The upper bound loss function we construct is denoted by Lt(θ): Lt(θ) := Exd0 EyQt i(y) ln P(y, zx, θ). In further analysis, we take the following notations for simplifying the statement: 1(k, ) := (cid:88) k1 (cid:88) t=1 r=0 EθLt(θktk+r)2 > 0, 2(k, ) := (cid:88) k1 (cid:88) t=1 r=0 Lt(θktk+r)2 > 0, Ω(k, ) := (cid:88) k1 (cid:88) t=1 r=0 where EV (gktk+r) > 0, (gktk+r) = 1 (cid:88) i=1 1 bktk+r (cid:88) yj Bktk+r θ (ln P(yj, zixi, θktk+r)) . Under mild smoothness conditions, we can derive the following result. Theorem 1 (Decreasing rate with smoothness condition.). Suppose ln P(y, zx, θ) is 1/γ-smooth with respect to θ. If 0 < η γ, then the proposed algorithm satisfies that [L(θkT ) L(θ)] [L(θ0) L(θ)] η 2 1(k, ) + η2 2γ Ω(k, ). In Theorem 1, with sufficiently large enough sample size, Ω(k, ) will be small enough, which ensures that the right-hand side of the inequality is negative. This guarantees that the loss function decreases at each iteration. Furthermore, if the loss function exhibits convexity, we can derive the following result: The expectation is taken over all sources of randomness, including the sampled data {Dt } and the selected batches {Br }. 9 Theorem 2 (Decreasing rate with smooth and convex condition.). Suppose ln P(y, zx, θ) is 1/γ-smooth and convex with respect to θ. If 0 < η γ/2, then the proposed algorithm satisfies that [L(θkT ) L(θ)] [L(θ0) L(θ)] η 2k 2(k, ) η 4k 1(k, ) + η 4k Ω(k, ). In Theorem 2, the right-hand side includes an additional negative term, 2(k, ), which indicates faster rate of decrease in the loss function. Specifically, during each E-step, the gradients across M-steps vary only slightly, this implies that: k1 (cid:88) r=0 Lt(θktk+r)2 k2ELt(θktk)2 2 k1 (cid:88) r= ELt(θktk+r)2 = 2(k, ) k1(k, ), which further induces that 2(k, ) + 1(k, ) (cid:16) η 2 + (cid:17) η 4k 1(k, ) > η 2 1(k, ), η 2k η 4k η 4k η2 2γ Ω(k, ) < Ω(k, ), as well as η 2k 2(k, ) η 4k 1(k, ) + η 4k Ω(k, ) < η 1(k, ) + η2 2γ Ω(k, ). All of the proofs are deferred to Appendix D. Guaranteed Decrease in Our Proposed Method. From the theorems above, we know that with sufficiently large sample size, Ω(k, ) becomes small, and the upper bound for [L(θkT ) L(θ)] [L(θ0) L(θ)] is strictly negative. This implies that our proposed method consistently decreases the objective loss, thereby ensuring an effective training process. Reduction of the variance term Ω(k, ). According to the upper bounds derived above, smaller Ω(k, ) leads to more efficient training. On one hand, increasing the sample size reliably reduces this term. On the other hand, under finite budget in practice, an effective budget scheduling strategy can also reduce Ω(k, ) and thereby improve training efficiency. Our algorithm achieves this via the scheduling scheme defined in Algorithm 2. Trade-off in Budget Scheduling Update Frequency. In our algorithm, the budget scheduling is updated every optimization steps. Updating more frequently (i.e., using smaller k) improves training efficiency but increases computational overhead. Conversely, updating less frequently reduces computation, but as optimization proceeds, the loss gradient norm θLt2 tends to become small, which can slow down training. Therefore, choosing an appropriate value of requires balancing this trade-off between efficiency and computational cost. 3.3 Practical Implementation Practical implementation of GVM. In this subsection, we describe how to implement the inference budget allocation strategy in practice, as summarized in Proposition 1, and the implementation is referred to GVM - Gradient Variance Minimization. Although the optimal sample sizes are of closed form, the expression involves the average accept rate pt and the gradient norm Gi,t, both of which depend on the posterior Qt i() and are therefore not available directly. Specifically, suppose that we are given the training set {xi, zi}n pi = EyP(xi,θ)P(ziy), Gi = EyQt i=1 and current LLM (θ). We can write: i() ln P(y, zixi, θ). We also use sampling to estimate these quantities. We can first generate samples per prompt to get {xi, yj j=1. Then, we can compute the following empirical estimators: (cid:80)N }N , zj pi = = zi) j=1 1(zj , Gi = (cid:88) 1 pi 1jN ,zj =zi θ ln P(yj , zixi, θ)2. Then, we simply plug these empirical estimators into Proposition 1 to get the sample sizes. The entire procedure is summarized in Algorithm 2. 10 Figure 2: The assigned sample budget in GVM-RAFT++ with = 32, = 32n for three iterations. Visualization We present the relationship between the final sample budget and the accept rates of different prompts for GVM-RAFT++ in Figure 2. From the trend, it could be inferred that the sample budget allocated to prompts increases smoothly as the accept rates decrease. However, other affecting factors together determine the trend of the sample budget besides the hardness, where the Lipschitz coefficients measuring the contribution of the samples by the gradients play role. If we do not take the impact of Lipschitz coefficients {Gi} into account, then we could have the allocated sample budget to prompts with different levels of hardness measured by accept rates as shown in Figure 6. GVM-RAFT++. We implement GVM and the meta EM Algorithm 1 in highly online fashion, building on the RAFT++ framework. In each iteration t, we draw set of prompts {xi, zi} d0 and use the current model θold to collect Dt = iDt as the replay buffer where the inference budget allocation is determined via Algorithm 2. RAFT++ then uses these samples to compute stochastic gradient estimator of the objective JQt. To accelerate training, we perform multiple gradient steps per iteration in mini-batch way. This causes the model distribution to shift away from the distribution used to generate the data. To address this mismatch, RAFT++ incorporates importance sampling and clipping strategies from PPO (Schulman et al., 2017) into the original RAFT, arriving at the following loss function on the prompt-response pair (x, a): LRAFT++(θ) = 1 a (cid:88) (cid:104) (cid:16) min t=1 st(θ), clip(st(θ), 1ϵ, 1+ϵ) (cid:17)(cid:105) I(cid:0)r(x, a) = argmax r(x, ai)(cid:1), (8) where st(θ) = πθ(atx,a1:t1) train on accepted responses: those approximating the posterior via rejection sampling. πθold (atx,a1:t1) and at is the t-th token of a. Here, the indicator ensures that we only Extension to the RL algorithms. While we focus primarily on RAFT-like algorithms, the proposed GVM strategy can be readily adapted to other RL-style fine-tuning algorithms. We focus on the GRPO, which receives significant attention recently due to its successful application to training DeepSeek-R1. Specifically, for each prompt x, GRPO will sample > 1 responses and compute the following advantage for the t-th token of the i-th response: At(x, ai) = ri mean(r1, rm) std(r1, , rm) , where ri denotes the final reward of the i-th response. This leads to the following loss function for GRPO, JGRPO(θ) = Ex,{yi}m i=1πθold (yx) (cid:34) 1 (cid:88) i=1 1 yi yi (cid:88) (cid:26) t=1 min (cid:104) πθ(yi,tx, yi,<t) πθold(yi,tx, yi,<t) ˆAi,t, clip(cid:0) πθ(yi,tx, yi,<t) πθold (yi,tx, yi,<t) , 1 ϵ, 1 + ϵ(cid:1) ˆAi,t (cid:105) βDKL[πθπref] (cid:27)(cid:35) ."
        },
        {
            "title": "4 Experiments and Results",
            "content": "In this section, we validate the effectiveness of the proposed algorithm by real-world experiments. We will implement our approach with both RAFT++ and GRPO. 4.1 Experiments Setup Datasets and Models To validate the effectiveness of the proposed methods, we conduct experiments with Qwen2.5-Math-1.5B and Qwen2.5-Math-7B (Yang et al., 2024b). We focus on the mathematical reasoning task and use Math-Verify as the verifier. Hyperparameters We utilize verl (Sheng et al., 2024) as the training framework, and implement the RAFT++ as Xiong et al. (2025a) show that the additional importance sampling and clipping improve over the vanilla RAFT (Dong et al., 2023). For each iteration, we use prompt batch size of 1024, and use mini-batch size 256 for gradient update. The max prompt length is set to be 1024, and the models are allowed to generate at most 3072 tokens so that they do not exceed the context window of 4096 tokens. There is no warmup stage and the learning rate is chosen to be constant 1e 6. For the training dataset, we use the Numina-Math (LI et al., 2024). For simplicity of implementation, we update the posterior distribution (i.e., the GVM in Algorithm 2) every 10 steps. In other words, we will compute the inference buget allocation for the next 10 M-steps and take 10 M-steps before entering another E-step. The 10 M-steps and the 1 E-step are together referred to as one iteration of GVM-RAFT++. We also experiment with the clip higher (Yu et al., 2025) to use an asymmetric clip threshold design. Specifically, we clip the importance sampling ratio πθ/πθold to the range (1 εlow, 1 + εhigh), where εlow = 0.2, and εhigh = 0.28, 0.4 then 0.35 for RAFT++ and GVM-RAFT++ respectively. We omit this technique in GRPO training as we observe it leads to worse performance in our experiments. For detailed experiments setup, please refer to Appendix B. 4.2 Main Results In this section, we summarize the results from integrating GVM into both RAFT++ and GRPO algorithms as sample budget rebalancing strategy. The performance is measured by Average @ 8, which means we randomly sample 8 instances from the model with non-zero temperature, and take the average accuracy as the final result. For Qwen2.5-Math-1.5B, we use temperature of 1.0 in evaluation, while for Qwen2.5-Math-7B, we use temperature of 0.7 as the entropy loss is higher after integrating the clip higher trick, which leads to more diverse outputs. Table 1: Performance of different algorithms across five benchmarks including Math500 (Hendrycks et al., 2021), Minerva Math (Lewkowycz et al., 2022), Olympiad Bench (He et al., 2024), AIME24, and AMC23. From the results, we could observe that after reweighting the sample size of prompts, GVM-RAFT++ and GVM-GRPO could indeed outperform both RAFT++ without sample size reweighting and GRPO. Method Math500 Minerva Math Olympiad Bench AIME24 AMC23 5 Average Qwen2.5 Math-1.5B Qwen2.5 Math-7B Base GRPO RAFT++ GVM-GRPO GVM-RAFT++ Base GRPO RAFT++ GVM-GRPO GVM-RAFT++ 56.35 70.78 69.02 73.92 72.90 42.00 81.20 81.68 81.55 81.00 17.00 29.00 27.71 29.96 29. 12.82 36.03 35.85 36.26 36.67 25.22 33.56 31.74 36.26 36.20 19.20 44.15 43.83 43.56 43.48 3.33 10.00 9.58 12.92 9.17 12.92 20.83 20.83 22.92 22.92 37.81 47.19 44.06 49.06 51. 30.00 63.12 63.12 65.00 61.56 27.94 38.11 36.42 40.42 39.64 23.39 49.07 49.06 49.86 49.13 GVM Improves Efficiency with Comparable Performance From Table 1, we could conclude that GVM could indeed improve the final performance of both RAFT++ and GRPO when applied on https://github.com/huggingface/Math-Verify Figure 3: The average accuracy of RAFT++ and GVM-RAFT++ with sample sizes 8 and 32, respectively, on Math500, Minerva Math, and Olympiad Bench, with base model Qwen2.5-Math1.5B. Qwen2.5-Math-1.5B. For Qwen2.5-Math-7B, the final performance is comparable to baselines, under faster convergence rate. This verifies the effectiveness of sample budget rebalancing using both the accept rates (standing for the hardness) and the Lipschitz coefficients (standing for the gradients) of different prompts. In Figure 3, we display the step-wise performance of both RAFT++ and GVM-RAFT++ based on Qwen2.5-Math-1.5B with sample sizes per prompt of 8 and 32, respectively. From the results, we could infer that GVM firstly could enhance the convergence rate evidently, with about 2 speedup for = 8, = 8n and 4 speed up for = 32, = 32n measured in update steps compared to its vanilla versions. Besides, upon convergence, GVM-RAFT++ could achieve around 1.25% and 5% performance improvement for both configurations, respectively. In addition, we observed that for Qwen2.5-Math-1.5B, when = 8, the training rewards of GVMRAFT++ are higher than RAFT++ itself, and when = 16, the training rewards of GVM-RAFT++ are approximately the same as RAFT++, while when = 32, the training rewards are lower than that of RAFT++. This implies that using larger for GVM will bias the sample budget to harder prompts, which may be due to the fact that with larger , harder problems are more likely to be solved at least once compared to when is smaller. GVM Could Be Generalized to RL Algorithms Though our derivation and theoretical proofs are based on EM algorithm, the sampling strategy itself could be disentangled from the pipeline and utilized solely. Take the GRPO algorithm proposed in Shao et al. (2024) as an example. Figure 5 demonstrates that with the same sample budget rebalancing strategy as in RAFT++, GVM-GRPO could achieve similar performance convergence to GVM-RAFT++. This further verifies the usefulness of GVM as single sampling strategy compared to being deployed in the EM pipeline. Zhong et al. (2025) makes extra assumptions (please refer to Example 3.5) on the reward structure, which enables their frameworks natural generalization to RL from theoretical perspective. The similar ideas shed light on the generalization of our method to RL algorithms, and we verified the empirical effectiveness of GVM in our experiments, indicating the success of budget reweighting through gradient variance minimization. 4.3 Ablation Study In the section of the ablation study, we focus on several aspects that might affect the performance of GVM, including the sample sizes and , together with different policy update algorithms, etc. 13 GVM Balances the Sampling Budget in Stable Way Figure 4 displays the performance trend concerning the training steps, with each training step corresponding to exactly 1024 4 samples, which means that the definition of step here may not be mapped to the actual training step. Take the case = 16, = 16n for example, under this setting, each iteration is optimized over 1024 prompts, with total sampling budget 1024 16 = 4 (1024 4), therefore corresponding to 4 steps in the figure. Though increasing the number of samples in the first stage, , could make more prompts be accepted at least once, thus leading to more useful prompts during the training procedure, the convergence rate does not change evidently. This means GVM could possibly estimate the difficulties of the prompts with relatively small sampling budget to sufficient threshold, and we could choose small and in realistic applications. Figure 4: The average accuracy of GVM-RAFT++ with different sample sizes for both stage one and stage two on Math500, Minerva Math and Olympiad Bench. According to the trend, though the number of sample sizes in both stage one and stage two increases, the convergence rate stays almost the same. Figure 5: The average accuracy of RAFT++, GVM-RAFT++ and GRPO, GVM-GRPO with sample sizes = 8, = 8n respectively on Math500, Minerva Math and Olympiad Bench. Applying the GVM sample strategy to RL algorithms like GRPO achieves similar results to GVM-RAFT++ compared to vanilla GRPO. Here for vanilla RAFT++ and GRPO, the rollout number per prompt is set to 8 as well."
        },
        {
            "title": "5 Conclusion and Discussion",
            "content": "In summary, we propose novel dynamic sampling and updating strategy - GVM, which could assign prompt-specific sampling budget in fine-grained manner. The algorithm consists of two stages: pre-sampling phase that estimates the difficulty of each instance and computes promptspecific Lipschitz coefficients, followed by an update phase that performs parameter optimization. Our experiments have demonstrated the effectiveness of GVM, achieving faster convergence and even superior final performance under suitable settings compared to other baselines. GVM could improve the convergence for both rejection sampling backed pipelines and then be generalized to RL algorithms like GRPO, which demonstrates the significant potential for adaptively reweighting the sampling and update budget. Finally, we also provide rigorous theoretical analysis and establish performance guarantees for this class of two-stage algorithms. The experiments are conducted with Qwen series base models, while the effectiveness of GVM still awaits broader verification on other base models. Besides, we believe GVM could generalize to other RL algorithms like PPO and Reinforce, while more experiments need to be performed to support the hypothesis. These could serve as the directions for possible further explorations."
        },
        {
            "title": "References",
            "content": "Ahmadian, A., Cremer, C., Gallé, M., Fadaee, M., Kreutzer, J., Pietquin, O., Üstün, A., and Hooker, S. (2024). Back to basics: Revisiting reinforce style optimization for learning from human feedback in llms. arXiv preprint arXiv:2402.14740. AI@Meta (2024). Llama 3 model card. Anthony, T., Tian, Z., and Barber, D. (2017). Thinking fast and slow with deep learning and tree search. Advances in neural information processing systems, 30. Azar, M. G., Rowland, M., Piot, B., Guo, D., Calandriello, D., Valko, M., and Munos, R. (2023). general theoretical paradigm to understand learning from human preferences. arXiv preprint arXiv:2310.12036. Bai, Y., Jones, A., Ndousse, K., Askell, A., Chen, A., DasSarma, N., Drain, D., Fort, S., Ganguli, D., Henighan, T., et al. (2022). Training helpful and harmless assistant with reinforcement learning from human feedback. arXiv preprint arXiv:2204.05862. Bishop, C. M. and Nasrabadi, N. M. (2006). Pattern recognition and machine learning, volume 4. Springer. Bradley, R. A. and Terry, M. E. (1952). Rank analysis of incomplete block designs: I. the method of paired comparisons. Biometrika, 39(3/4):324345. Chen, J., Zhu, J., Teh, Y. W., and Zhang, T. (2018). Stochastic expectation maximization with variance reduction. Advances in Neural Information Processing Systems, 31. DeepSeek-AI, Guo, D., Yang, D., Zhang, H., Song, J., Zhang, R., Xu, R., Zhu, Q., Ma, S., Wang, P., Bi, X., Zhang, X., Yu, X., Wu, Y., Wu, Z. F., Gou, Z., Shao, Z., Li, Z., Gao, Z., Liu, A., Xue, B., Wang, B., Wu, B., Feng, B., Lu, C., Zhao, C., Deng, C., Zhang, C., Ruan, C., Dai, D., Chen, D., Ji, D., Li, E., Lin, F., Dai, F., Luo, F., Hao, G., Chen, G., Li, G., Zhang, H., Bao, H., Xu, H., Wang, H., Ding, H., Xin, H., Gao, H., Qu, H., Li, H., Guo, J., Li, J., Wang, J., Chen, J., Yuan, J., Qiu, J., Li, J., Cai, J. L., Ni, J., Liang, J., Chen, J., Dong, K., Hu, K., Gao, K., Guan, K., Huang, K., Yu, K., Wang, L., Zhang, L., Zhao, L., Wang, L., Zhang, L., Xu, L., Xia, L., Zhang, M., Zhang, M., Tang, M., Li, M., Wang, M., Li, M., Tian, N., Huang, P., Zhang, P., Wang, Q., Chen, Q., Du, Q., Ge, R., Zhang, R., Pan, R., Wang, R., Chen, R. J., Jin, R. L., Chen, R., Lu, S., Zhou, S., Chen, S., Ye, S., Wang, S., Yu, S., Zhou, S., Pan, S., Li, S. S., Zhou, S., Wu, S., Ye, S., Yun, T., Pei, T., Sun, T., Wang, T., Zeng, W., Zhao, W., Liu, W., Liang, W., Gao, W., Yu, W., Zhang, W., Xiao, W. L., An, W., Liu, X., Wang, X., Chen, X., Nie, X., Cheng, X., Liu, X., Xie, X., Liu, X., Yang, X., Li, X., Su, X., Lin, X., Li, X. Q., Jin, X., Shen, X., Chen, X., Sun, X., Wang, X., Song, X., Zhou, X., Wang, X., Shan, X., Li, Y. K., Wang, Y. Q., Wei, Y. X., Zhang, Y., Xu, Y., Li, Y., Zhao, Y., Sun, Y., Wang, Y., Yu, Y., Zhang, Y., Shi, Y., Xiong, Y., He, Y., Piao, Y., Wang, Y., Tan, Y., Ma, Y., Liu, Y., Guo, Y., Ou, Y., Wang, Y., Gong, Y., Zou, Y., He, Y., Xiong, Y., Luo, Y., You, Y., Liu, Y., Zhou, Y., 15 Zhu, Y. X., Xu, Y., Huang, Y., Li, Y., Zheng, Y., Zhu, Y., Ma, Y., Tang, Y., Zha, Y., Yan, Y., Ren, Z. Z., Ren, Z., Sha, Z., Fu, Z., Xu, Z., Xie, Z., Zhang, Z., Hao, Z., Ma, Z., Yan, Z., Wu, Z., Gu, Z., Zhu, Z., Liu, Z., Li, Z., Xie, Z., Song, Z., Pan, Z., Huang, Z., Xu, Z., Zhang, Z., and Zhang, Z. (2025). Deepseek-r1: Incentivizing reasoning capability in llms via reinforcement learning. Defazio, A., Bach, F., and Lacoste-Julien, S. (2014). Saga: fast incremental gradient method with support for non-strongly convex composite objectives. Advances in neural information processing systems, 27. Dong, H., Xiong, W., Goyal, D., Zhang, Y., Chow, W., Pan, R., Diao, S., Zhang, J., SHUM, K., and Zhang, T. (2023). RAFT: Reward ranked finetuning for generative foundation model alignment. Transactions on Machine Learning Research. Dong, H., Xiong, W., Pang, B., Wang, H., Zhao, H., Zhou, Y., Jiang, N., Sahoo, D., Xiong, C., and Zhang, T. (2024). Rlhf workflow: From reward modeling to online rlhf. arXiv preprint arXiv:2405.07863. Gulcehre, C., Paine, T. L., Srinivasan, S., Konyushkova, K., Weerts, L., Sharma, A., Siddhant, A., Ahern, A., Wang, M., Gu, C., et al. (2023). Reinforced self-training (rest) for language modeling. arXiv preprint arXiv:2308.08998. He, C., Luo, R., Bai, Y., Hu, S., Thai, Z. L., Shen, J., Hu, J., Han, X., Huang, Y., Zhang, Y., et al. (2024). Olympiadbench: challenging benchmark for promoting agi with olympiad-level bilingual multimodal scientific problems. arXiv preprint arXiv:2402.14008. Hendrycks, D., Burns, C., Kadavath, S., Arora, A., Basart, S., Tang, E., Song, D., and Steinhardt, J. (2021). Measuring mathematical problem solving with the math dataset. arXiv preprint arXiv:2103.03874. Hoffman, M. D., Phan, D., Dohan, D., Douglas, S., Le, T. A., Parisi, A., Sountsov, P., Sutton, C., Vikram, S., and Saurous, R. A. (2023). Training chain-of-thought via latent-variable inference. In NeurIPS. Hu, J. (2025). Reinforce++: simple and efficient approach for aligning large language models. arXiv preprint arXiv:2501.03262. Hu, X., Zhang, F., Chen, S., and Yang, Z. (2024). Unveiling the statistical foundations of chain-ofthought prompting methods. arXiv preprint arXiv:2408.14511. Jaech, A., Kalai, A., Lerer, A., Richardson, A., El-Kishky, A., Low, A., Helyar, A., Madry, A., Beutel, A., Carney, A., et al. (2024). Openai o1 system card. arXiv preprint arXiv:2412.16720. Johnson, R. and Zhang, T. (2013). Accelerating stochastic gradient descent using predictive variance reduction. Advances in neural information processing systems, 26. Kingma, D. P., Welling, M., et al. (2013). Auto-encoding variational bayes. Kool, W., van Hoof, H., and Welling, M. (2019). Buy 4 reinforce samples, get baseline for free! Lewkowycz, A., Andreassen, A., Dohan, D., Dyer, E., Michalewski, H., Ramasesh, V., Slone, A., Anil, C., Schlag, I., Gutman-Solo, T., et al. (2022). Solving quantitative reasoning problems with language models. Advances in Neural Information Processing Systems, 35:38433857. LI, J., Beeching, E., Tunstall, L., Lipkin, B., Soletskyi, R., Huang, S. C., Rasul, K., Yu, L., Jiang, A., Shen, Z., Qin, Z., Dong, B., Zhou, L., Fleureau, Y., Lample, G., and Polu, S. (2024). Numinamath. https://github.com/project-numina/aimo-progress-prize/blob/main/ report/numina_dataset.pdf. Li, Z., Xu, T., Zhang, Y., Yu, Y., Sun, R., and Luo, Z.-Q. (2023). Remax: simple, effective, and efficient reinforcement learning method for aligning large language models. arXiv e-prints, pages arXiv2310. Neal, R. M. (2003). Slice sampling. The annals of statistics, 31(3):705767. 16 Ouyang, L., Wu, J., Jiang, X., Almeida, D., Wainwright, C., Mishkin, P., Zhang, C., Agarwal, S., Slama, K., Ray, A., et al. (2022). Training language models to follow instructions with human feedback. Advances in Neural Information Processing Systems, 35:2773027744. Owen, A. and Zhou, Y. (2000). Safe and effective importance sampling. Journal of the American Statistical Association, 95(449):135143. Rafailov, R., Sharma, A., Mitchell, E., Ermon, S., Manning, C. D., and Finn, C. (2023). Direct preference optimization: Your language model is secretly reward model. arXiv preprint arXiv:2305.18290. Roux, N., Schmidt, M., and Bach, F. (2012). stochastic gradient method with an exponential convergence _rate for finite training sets. Advances in neural information processing systems, 25. Schulman, J., Wolski, F., Dhariwal, P., Radford, A., and Klimov, O. (2017). Proximal policy optimization algorithms. arXiv preprint arXiv:1707.06347. Shao, Z., Wang, P., Zhu, Q., Xu, R., Song, J., Zhang, M., Li, Y., Wu, Y., and Guo, D. (2024). Deepseekmath: Pushing the limits of mathematical reasoning in open language models. arXiv preprint arXiv:2402.03300. Sheng, G., Zhang, C., Ye, Z., Wu, X., Zhang, W., Zhang, R., Peng, Y., Lin, H., and Wu, C. (2024). Hybridflow: flexible and efficient rlhf framework. arXiv preprint arXiv: 2409.19256. Shi, T., Wu, Y., Song, L., Zhou, T., and Zhao, J. (2025). Efficient reinforcement finetuning via adaptive curriculum learning. arXiv preprint arXiv:2504.05520. Singh, A., Co-Reyes, J. D., Agarwal, R., Anand, A., Patil, P., Liu, P. J., Harrison, J., Lee, J., Xu, K., Parisi, A., et al. (2023). Beyond human data: Scaling self-training for problem-solving with language models. arXiv preprint arXiv:2312.06585. Sordoni, A., Yuan, E., Côté, M.-A., Pereira, M., Trischler, A., Xiao, Z., Hosseini, A., Niedtner, F., and Le Roux, N. (2023). Joint prompt optimization of stacked llms using variational inference. Advances in Neural Information Processing Systems, 36:5812858151. Tang, Y., Guo, Z. D., Zheng, Z., Calandriello, D., Munos, R., Rowland, M., Richemond, P. H., Valko, M., Pires, B. Á., and Piot, B. (2024). Generalized preference optimization: unified approach to offline alignment. arXiv preprint arXiv:2402.05749. Team, K., Du, A., Gao, B., Xing, B., Jiang, C., Chen, C., Li, C., Xiao, C., Du, C., Liao, C., et al. (2025). Kimi k1. 5: Scaling reinforcement learning with llms. arXiv preprint arXiv:2501.12599. Tong, Y., Zhang, X., Wang, R., Wu, R., and He, J. (2024). Dart-math: Difficulty-aware rejection tuning for mathematical problem-solving. arXiv preprint arXiv:2407.13690. Touvron, H., Martin, L., Stone, K., Albert, P., Almahairi, A., Babaei, Y., Bashlykov, N., Batra, S., Bhargava, P., Bhosale, S., et al. (2023). Llama 2: Open foundation and fine-tuned chat models. arXiv preprint arXiv:2307.09288. Wei, J., Wang, X., Schuurmans, D., Bosma, M., Xia, F., Chi, E., Le, Q. V., Zhou, D., et al. (2022). Chain-of-thought prompting elicits reasoning in large language models. Advances in neural information processing systems, 35:2482424837. Williams, R. J. and Peng, J. (1991). Function optimization using connectionist reinforcement learning algorithms. Connection Science, 3(3):241268. Xiong, W., Dong, H., Ye, C., Wang, Z., Zhong, H., Ji, H., Jiang, N., and Zhang, T. (2023). Iterative preference learning from human feedback: Bridging theory and practice for rlhf under kl-constraint. Xiong, W., Yao, J., Xu, Y., Pang, B., Wang, L., Sahoo, D., Li, J., Jiang, N., Zhang, T., Xiong, C., et al. (2025a). minimalist approach to llm reasoning: from rejection sampling to reinforce. arXiv preprint arXiv:2504.11343. Xiong, W., Zhang, H., Ye, C., Chen, L., Jiang, N., and Zhang, T. (2025b). Self-rewarding correction for mathematical reasoning. arXiv preprint arXiv:2502.19613. 17 Yang, A., Zhang, B., Hui, B., Gao, B., Yu, B., Li, C., Liu, D., Tu, J., Zhou, J., Lin, J., et al. (2024a). Qwen2. 5-math technical report: Toward mathematical expert model via self-improvement. arXiv preprint arXiv:2409.12122. Yang, A., Zhang, B., Hui, B., Gao, B., Yu, B., Li, C., Liu, D., Tu, J., Zhou, J., Lin, J., Lu, K., Xue, M., Lin, R., Liu, T., Ren, X., and Zhang, Z. (2024b). Qwen2.5-math technical report: Toward mathematical expert model via self-improvement. arXiv preprint arXiv:2409.12122. Yu, Q., Zhang, Z., Zhu, R., Yuan, Y., Zuo, X., Yue, Y., Fan, T., Liu, G., Liu, L., Liu, X., et al. (2025). Dapo: An open-source llm reinforcement learning system at scale. arXiv preprint arXiv:2503.14476. Yuan, Z., Yuan, H., Li, C., Dong, G., Tan, C., and Zhou, C. (2023). Scaling relationship on learning mathematical reasoning with large language models. arXiv preprint arXiv:2308.01825. Zelikman, E., Wu, Y., Mu, J., and Goodman, N. (2022). Star: Bootstrapping reasoning with reasoning. Advances in Neural Information Processing Systems, 35:1547615488. Zhang, T. (2023). Mathematical analysis of machine learning algorithms. Cambridge University Press. Zhao, P. and Zhang, T. (2014). Accelerating minibatch stochastic gradient descent using stratified sampling. arXiv preprint arXiv:1405.3080. Zhao, Y., Joshi, R., Liu, T., Khalman, M., Saleh, M., and Liu, P. J. (2023). Slic-hf: Sequence likelihood calibration with human feedback. arXiv preprint arXiv:2305.10425. Zhong, H., Feng, G., Xiong, W., Zhao, L., He, D., Bian, J., and Wang, L. (2024). Dpo meets ppo: Reinforced token optimization for rlhf. arXiv preprint arXiv:2404.18922. Zhong, H., Yin, Y., Zhang, S., Xu, X., Liu, Y., Zuo, Y., Liu, Z., Liu, B., Zheng, S., Guo, H., et al. (2025). Brite: Bootstrapping reinforced thinking process to enhance language model reasoning. arXiv preprint arXiv:2501.18858."
        },
        {
            "title": "A Author Contributions",
            "content": "This work stems from all authors valuable contributions and close collaborations. JY implements the workflow based on the algorithm pipeline; conducts preliminary experiments and verifies the effectiveness of GVM in the early stage; scales up the experiments; mainly writes the experiments-relevant sections of this paper. YH initializes the project; contributes to the algorithm proposal and the clarification of the pipeline procedure; derives the theoretical analysis for GVM and establishes corresponding bounds for the algorithm; mainly writes the sections on methodology and theoretical analysis. HZ devotes to the conduct of experiments; contributes to the ablation studies of the tradeoff between entropy loss and model performance; reviews related works on the techniques to mitigate fast entropy loss in the early stage of RL training and provides insightful suggestions on how to balance the tradeoff. HD implements baselines and provides practical implementation suggestions; conducts series of experiments, including baselines comparisons, GVM iterations in both RAFT++ and RL (GRPO) algorithmic settings; provides discussions and suggestions about experiment design; contributes to writing. WX provides insightful advice through the project timeline on theoretical analysis, experiment design, and paper writing; contributes to the initial codebase and baseline training; mainly writes the introduction, related works, and problem formulation sections; and revises the draft version of this paper. NJ and TZ support the work and provide computational resources, guidance, and suggestions for theoretical analysis, experiment design and paper writing."
        },
        {
            "title": "B Experiment Details",
            "content": "To understand how to choose the hyperparameters α and β, we first simplify the question and omit the impacts of gradients here. Then the sample weight of function (x) = with different 1 x+ α xβ1 choices of α and β is visualized as the figure below. From Figure 6, we could see that in order to assign more sample weights on problems with lower accept rates, we should choose both α and β as low as possible within reasonable region. Therefore, in our experiments, we fix α = 0.001 and β = 2.0. For all the hyperparameters, please refer to Table 2. Table 2: Full hyperparameters. Parameter α β batch size mini batch size max prompt length max response length learning rate KL loss coefficient Value 1e 3 2 1024 256 1024 3072 1e 6 0.001 As for the choices of α and β is GVM stage two sample budget assignment, we visualize the effects of both parameters in Figure 6. From the figure, we could see that when β is fixed, the smaller α is, the more sample budget will be allocated to harder prompts. Similarly, when α is fixed, the smaller β is, the more sample budget will be allocated to harder prompts. Figure 6: Visualization of different choices of α and β for (x) = 1/ + αx1β. To analyze the impacts of which layers are used in the gradients calculation and the reduction methods of gradients, we conduct series of experiments using Qwen2-math-7B-Instruct (Yang et al., 2024a). For the layers from which we retrieve the gradients, we choose the top one, lm_head, and the bottom one, embed_tokens, as the activated layers for loss backpropagation and gradients calculation. For gradients reduction, we compare both summation and average by sequence length. From the results shown in Figure 7, we could see that different kinds of layers for gradient calculation do not make huge difference on the final sample sizes, while the summation of gradients biases the sample sizes to 19 problems with lower accept rates, which we speculate the correct answers for problems with lower accept rates could steer the model more than easier problems. Therefore, in our experiments, we choose summation as the method for gradients reduction. According to the relationship between the sample sizes and accept rates. The relationship between gradients and accept rates, it could be inferred that in high level, the gradients are larger for prompts with lower accept rates if we use the average reduction. At the same time, it is not always the case, as prompts with higher accept rates may also have large gradients. In the second stage of GVM, to make the total sample budget exactly , we sort the prompts according to their accept rates in descending order, and add more samples to the prompts with lower accept rates. In our experiments, for GVM, each iteration consists of nine steps of update as the amount of training data per iteration is slightly less than 10 1024 = 10240, which is the amount of data needed for ten steps of update with train batch size 1024. For the implementation of GVM-GRPO, we divide the sample budget calculated in the second stage by fixed size (for example, 4 in our experiments), and copy the prompts multiple times by ni/4. Then we set the rollout number to 4 during the training procedure. For compute resources, we mainly conduct the experiments on NVIDIA RTX A6000 and H100 GPUs, and each iteration in GVM typically takes 90 minutes with sample sizes = 8, = 8n on 4H100 GPU server."
        },
        {
            "title": "C Notation Table",
            "content": "All of the notations are listed in Table 3. Table 3: The table of notations used in this paper. Description Prompt, CoT rationale, and predicted answer the space of prompt, CoT rationale, and predicted answer the number of training samples (xi, zi) the posterior distribution P(yxi, zi, θ) on the t-th E-step the collected sample set for (xi, zi, y) on the t-th E-step the size of scheduling budget for prompt xi on the t-th E-step the average accept rate for prompt xi on the t-th E-step the whole size of sampling budget tuning parameters in budget scheduling the sample batch corresponding to xi on the r-th M-step the batch size of Br required number of samples for prompt xi on the t-th E-step the number of M-step after each E-step the model parameter on the r-th M-step within the t-th E-step learning rate upper bound loss function on the t-th E-step Notation Qt x, y, , Y, i(y) Dt nt pt (α, β) Br bi nt θktk+r η Lt(θ)"
        },
        {
            "title": "D More Theoretical Results and Missing Proofs",
            "content": "If we consider further strongly convex loss function, there will be: Theorem 3 (Decreasing rate with smooth and strongly convex condition.). Suppose ln P(y, zx, θ) is 1/γ-smooth and H-strongly convex with respect to θ. If 0 < η min{γ/2, 1/(4kH)}, then the proposed algorithm satisfies that [L(θkT ) L(θ)] [L(θ0) L(θ)] Hη2 2k 3(k, ) η 4k 1(k, ) + η 8k Ω(k, ), where 3(k, ) := (cid:88) (cid:88) r1 (cid:88) t=1 r=1 m=0 Lt(θktk+m)2 > 0. 20 Proof of Proposition 1. Based on the objective function in Equation (7), we can define the Lagrange objective function as G({nt i}, λ) := (cid:88) i= 1 1 + α/(pt i)β G2 int pt (cid:88) λ( i= nt ). Taking derivative on the variables, we could obtain that nt λ"
        },
        {
            "title": "To achieve the quations as",
            "content": "we can obtain the result as G2 i)2 λnt i, i(nt pt [m], i)β = = 2 1 + α/(pt (cid:88) nt i. i=1 nt = 0, λ = 0, (cid:113) Gi/ (cid:80)n l=1 Gl/ + α pt (pt i)β1 (cid:113) + α pt (pt )β1 (cid:113) Gi + α pt (pt i)β1 , [m], nt = which finishes the proof. Proof for Theorem 1. To simplify the analysis, we denote gktk+r = 1 (cid:88) i=1 1 bktk+r (cid:88) yj Bktk+r θ (ln P(yj, zixi, θktk+r)) . With the fact that ln P(y, zx, θt) are 1/γ-smooth, we can obtain [Lt(θktk+r+1) Lt(θktk+r)] ηEθLt(θktk+r)2 + η2 2γ Egktk+r2, for any = 0, 1, . . . , 1 on iter-t. Summing the above inequality over all r, and with the fact that EL(θkt) ELt(θkt) ELt(θktk) = EL(θktk), we have [L(θkt) L(θktk)] η = η k1 (cid:88) r= k1 (cid:88) EθLt(θktk+r)2 + EθLt(θktk+r)2 + k1 (cid:88) r=0 k1 (cid:88) r=0 k1 (cid:88) t=0 η 2 EθLt(θktk+r)2 + r=0 k1 (cid:88) r=0 η2 2γ η2 2γ Egktk+r (cid:0)EV (gktk+r) + EθLt(θktk+r)2(cid:1) η2 2γ EV (gktk+r) , where the last inequality is due to η/γ 1. Further summing over 1, . . . , , we can obtain that [L(θkT ) L(θ0)] η (cid:88) k1 (cid:88) t=1 r=0 EθLt(θktk+r)2 + (cid:88) k1 (cid:88) t=1 r=0 η2 2γ EV (gktk+r) . Proof for Theorem 2. The proof techniques are mainly from Zhao and Zhang (2014). To be specific, for each iteration t, we denote that (cid:42) δktk+r = 1 (cid:88) i=1 1 bktk+r (cid:88) yj Bktk+r θ (ln P(yj, zixi, θktk+r)) , θktk+r θktk (cid:43) + 1 (cid:88) i=1 1 bktk+r (cid:88) yj Bktk+r which implies that (ln P(yj, zixi, θktk+r) ln P(yj, zixi, θktk)) , Eδktk+r = ELt(θktk+r), θktk+r θktk ELt(θktk+r) + ELt(θktk) 0. Based on these notations, we can obtain that θktk+r θktk2 θktk+r+1 θktk2 = 2ηgktk+r, θktk+r θktk ηgktk+r = 2ηδktk+r 2η (cid:88) i=1 1 bktk+r (cid:88) yj Bktk+r (ln P(yj, zixi, θktk+r) ln P(yj, zixi, θktk)) ηgktk+r2, taking expectation of both sides, we have Eθktk+rθktk2Eθktk+r+1θktk2 2η (ELt(θktk+r) ELt(θktk))η2Egktk+r2. Summing over = 0, 1, . . . , 1, we have k1 (cid:88) r= (Lt(θktk+r) Lt(θktk)) 1 2η Eθkt θktk2 + η 2 k1 (cid:88) r= Egktk+r2. Also, considering the fact obtained in previous analysis as: k1 (cid:88) r=0 [Lt(θktk+r+1) Lt(θktk+r)] η k1 (cid:88) r=0 EθLt(θktk+r)2 + k1 (cid:88) r=0 η2 2γ Egktk+r2, we can combine the two inequalities above, and obtain that [L(θkt) L(θktk)] [Lt(θkt) Lt(θktk)] 1 k1 (cid:88) r=0 [Lt(θktk+r+1) Lt(θktk)] 1 2ηk Eθkt θktk2 η k1 (cid:88) r= EθLt(θktk+r)2 + (cid:18) η 2 1 + η2 2γ (cid:19) k1 (cid:88) r=0 Egktk+r2 = η 2k η 2k E k1 (cid:88) r=0 k1 (cid:88) r= Lt(θktk+r)2 (cid:18) η 2 1 η2 2γ (cid:19) k1 (cid:88) r=0 EθLt(θktk+r)2 + η2 2γk k1 (cid:88) r=0 EV (gktk+r) Lt(θktk+r)2 η 4k k1 (cid:88) r=0 EθLt(θktk+r)2 + η 4k k1 (cid:88) r=0 EV (gktk+r)2 , where the last inequality is from η/γ 1/2. Summing over = 1, . . . , , we have [L(θkT ) L(θ0)] + (cid:88) t=1 (cid:88) t=1 η 2k η 4k k1 (cid:88) r=0 Lt(θktk+r)2 (cid:88) t=1 η 4k k1 (cid:88) r=0 EθLt(θktk+r)2 k1 (cid:88) r=0 EV (gktk+r)2 . 22 Proof for Theorem 3. For each iteration t, we denote that (cid:42) δktk+r = 1 (cid:88) i=1 1 bktk+r (cid:88) yj Bktk+r θ (ln P(yj, zixi, θktk+r)) , θt θktk (cid:43) (cid:88) i=1 1 bktk+r (cid:88) yj Bktk+r θt θktk2 2, + 1 H 2 (ln P(yj, zixi, θktk+r) ln P(yj, zixi, θktk)) which implies that Eδktk+r = Lt(θktk+r), θktk+rθktkLt(θktk+r)+Lt(θktk) Based on these notations, we can obtain that 2 θktk+rθktk 2 0. θktk+r θktk2 θktk+r+1 θktk2 = 2ηsgktk+r, θktk+r θktk ηsgktk+r2 = 2ηδktk+r 2η ηgktk+r2, (cid:88) i=1 1 bktk+r (cid:88) yj Bktk+r (ln P(yj, zixi, θktk+r) ln P(yj, zixi, θktk)) + 2 θktk+r θktk2 taking expectation of both sides, we have Eθktk+r θktk2 Eθktk+r+1 θktk2 2η (cid:18) ELt(θktk+r) Lt(θktk) + θktk+r θktk2 (cid:19) η2Egktk+r2. Summing over = 0, 1, . . . , 1, we have k1 (cid:88) (Lt(θktk+r) Lt(θktk)) 1 2η r=0 Also, considering the fact obtained in previous analysis as: Eθktkθktk2 2 k1 (cid:88) r=0 Eθktk+rθktk2+ η 2 k1 (cid:88) r=0 Egktk+r2. k1 (cid:88) r=0 [Lt(θktk+r+1) Lt(θktk+r)] η k1 (cid:88) r=0 EθLt(θktk+r)2 + k1 (cid:88) r=0 η2 2γ Egktk+r2, we can combine the two inequalities above, and obtain that [L(θkt) L(θktk)] [Lt(θkt) Lt(θktk)] 1 k1 (cid:88) r=0 [Lt(θktk+r+1) Lt(θktk)] 1 2ηk Eθkt θktk2 2k k1 (cid:88) r= + (cid:18) η 2 1 + η2 2γ (cid:19) k1 (cid:88) r=0 Egktk+r2 Eθktk+r θktk2 η k1 (cid:88) r= EθLt(θktk+r)2 = η 2k k1 (cid:88) r= Lt(θktk+r)2 (cid:18) η 2 1 η2 2γ Hη2 2k (cid:88) r1 (cid:88) r=1 m= Lt(θktk+m)2 Hη2 2k r=0 (cid:88) r1 (cid:88) r= m=0 EV(gktk+m) (cid:19) k1 (cid:88) EθLt(θktk+r)2 + η2 2γk k1 (cid:88) r=0 EV (gktk+r) η 2k k1 (cid:88) r=0 Lt(θktk+r)2 Hη2 2k (cid:88) r1 (cid:88) r=1 m=0 Lt(θktk+m)2 η 4k k1 (cid:88) r= EθLt(θktk+r)2 + η 8k k1 (cid:88) r=0 EV (gktk+r) , 23 where the last inequality is from η/γ 1/2. Summing over = 1, . . . , , we have [L(θkT ) L(θ)] [L(θ0) L(θ)] (cid:88) t=1 Hη2 2k (cid:88) r1 (cid:88) r= m=0 Lt(θktk+m)2 (cid:88) t=1 η 4k k1 (cid:88) r=0 EθLt(θktk+r)2 + (cid:88) t=1 η 8k k1 (cid:88) r=0 EV (gktk+r) ."
        },
        {
            "title": "E Technical Lemmas",
            "content": "Lemma 2 (Jensens Inequality). Suppose that ϕ(w) is convex function on Ω. Consider w1, , wm Ω, and non-negative numbers α1, , αm so that (cid:80)m i=1 αi = 1. Then, (cid:88) ϕ( i=1 αiwi) (cid:88) i=1 αiϕ(wi). More generally, let be probability measure on Ω, then ϕ(Ewpw) Ewpϕ(w). In particular, since is convex (by triangle inequality of the norm), we know that Ez Ez. Proof. See Proposition A.9 of Zhang (2023) for proof."
        },
        {
            "title": "F More about Rejection Sampling",
            "content": "A general algorithm pipeline for rejection sampling is displayed in Algorithm 3, while in practice, we directly use the outcome-based reward signal to decide whether prompt will be chosen or not. Algorithm 3 Rejection sampling 1: Input: probability with current model {P(yxi, θt1), P(ziy, θt1)}, auxiliary probability q(yxi, zi), i, accept rate pt i, empty set sample size nt 2: for = 1, . . . , nt 3: 4: 5: do Obtain sample point yj from the distribution q(yxi, zi). Sample random point uj (0, 1). Calculate Qi(yj) as: Qi(yj) = (cid:80) P(yjxi, θt1)P(ziyj, θt1) P(yxi, θt1)P(ziy, θt1)/ (cid:80) (cid:80) y P(yxi, θt1)P(zy, θt1) Qi(yj ) q(yj xi,zi) then Enlarge = {yj}. if uj pt 6: 7: end if 8: 9: end for 10: Output: set D. 24 Figure 7: Comparison among different choices of the layers for gradient calculation and the methods of gradients reduction on Qwen2.5-Math-7B."
        },
        {
            "title": "G Case Study",
            "content": "Here we display several instances from several benchmarks we used respectively. In these cases, the model trained by vanilla RAFT++ did not generate correct answer within fixed budget of attempts, while models trained by GVM-RAFT++ or GVM-GRPO managed to find the correct answers. Math500 Example Problem The expression 2 3 4 5 + 1 is equal to 121, since multiplication is carried out before addition. However, we can obtain values other than 121 for this expression if we are allowed to change it by inserting parentheses. For example, we can obtain 144 by writing (2 (3 4)) (5 + 1) = 144. In total, how many values can be obtained from the expression 2 3 4 5 + 1 by inserting parentheses? (Note that rearranging terms is not allowed, only inserting parentheses). Answer 4 RAFT++ Output Given the expression 2 3 4 5 + 1, we can get different results by inserting parentheses in various ways. Lets denote the expression with parentheses as E. The expression 2 3 4 5 + 1 without parentheses is evaluated as: 2 3 4 5 + 1 = 120 + 1 = 121. Next, lets consider the possible ways to insert parentheses: 1. (2 3) 4 (5 + 1) 2. 2 (3 4) (5 + 1) 3. (2 3 4) 5 + 1 4. 2 (3 (4 5) + 1) 5. (2 3 4 5) + 1 However, not all of the above are valid expressions because we need to evaluate the expression step by step. For simplicity, lets evaluate each possible corrected expression: 1. (2 3) 4 (5 + 1) = 6 4 6 = 144 2. 2 (3 4) (5 + 1) = 2 12 6 = 144 3. (2 3 4) 5 + 1 = 24 5 + 1 = 120 + 1 = 121 4. 2 (3 (4 5) + 1) = 2 (3 20 + 1) = 2 (60 + 1) = 2 61 = 122 5. (2 3 4 5) + 1 = 120 + 1 = 121 From the above evaluations, we see that we get the values 144, 144, 121, 122, and 121, which means the distinct values are 121, 122, and 144. So the total number of distinct values we can obtain is 3. The answer is: 3 GVM-RAFT++ Output Given the expression 2 3 4 5 + 1, we need to determine how many distinct values can be obtained by inserting parentheses in different ways. First, lets note that without parentheses, the expression evaluates to 2345+1 = 120+1 = 121. When we insert parentheses, the expression can take different forms based on where the parentheses are placed. Lets evaluate all possible ways of inserting parentheses: 1. (2 3 4) (5 + 1) = (24) 6 = 144 2. 2 (3 4 5) + 1 = 2 60 + 1 = 120 + 1 = 121 3. 2 3 (4 5 + 1) = 2 3 21 = 126 4. (2 3) (4 5 + 1) = (6) 21 = 126 5. 2 (3 4 5 + 1) = 2 61 = 122 By evaluating these different forms, we see that the possible values are 121, 122, 126, and 144. So the distinct values that can be obtained are 121, 122, 126, and 144. Therefore, the total number of distinct values is 4. The final answer is 4 ."
        },
        {
            "title": "Minerva Math Example",
            "content": "Problem Preamble: spectrum is taken of single star (i.e., one not in binary). Among the observed spectral lines is one from oxygen whose rest wavelength is 5007. The Doppler shifted oxygen line from this star is observed to be at wavelength of 5012. The star is also observed to have proper motion, µ, of 1 arc second per year (which corresponds to 1.5 1013 radians per second of time). It is located at distance of 60pc from the Earth. Take the speed of light to be 3 108 meters per second. What is the component of the stars velocity parallel to its vector to the Earth (in kilometers per second)? Answer To find this longitudinal velocity component, we use the Doppler shift, finding Vr = λ 5000 = 300 km/s. λ = 5 RAFT++ Answer To solve this problem, we need to follow these steps: 1. Calculate the radial velocity of the star using the Doppler effect. 2. Use the radial velocity and the proper motion to find the component of the stars velocity parallel to its vector to the Earth. Step 1: Calculate the radial velocity The radial velocity, v, of the star can be found using the formula for the Doppler shift of photon: = (cid:18) λobs λrest λrest (cid:19) where λobs is the observed wavelength and λrest is the rest wavelength. Given: - λrest = 5007 Å - λobs = 5012 Å - = 3 108 m/s We first need to convert the wavelengths from to meters: So: Now we can calculate v: 1 Å = 1010 λrest = 5007 1010 λobs = 5012 1010 v = 3 108 (cid:18) 5012 1010 5007 1010 5007 1010 (cid:19) = 3 108 (cid:18) 5 1010 (cid:19) 5007 1010 = 3 108 (cid:18) 5 (cid:19) 5007 15 108 5007 29.96 km/s Step 2: Calculate the parallel velocity component The proper motion µ is given as 1.5 1013 rad/s, and the distance to the star is 60 pc. We convert 1 pc = 3.086 1016 m: 29956 m/s = distance = 60 3.086 1016 = 1.8516 1018 The radial velocity is the component of the stars velocity parallel to its vector to the Earth plus the effect of the proper motion: = + µ where is the component of the stars velocity parallel to its vector to the Earth, µ is the proper motion, and is the distance. Since we calculated 29.96 km/s = 29960 m/s, we can solve for v: = µ 27 = 29960 (1.5 1013 1.8516 1018) = 29960 2777.4 27182.6 m/s 27.18 km/s So the component of the stars velocity parallel to its vector to the Earth is approximately 27.18 km/s. The final answer is: 27.18 GVM-GRPO Answer To solve this problem, we need to determine the component of the stars velocity parallel to its vector to the Earth. We can break this into two parts: 1. **Rest-wavelength shift using the Doppler effect:** The Doppler effect tells us that the observed wavelength λobs of spectral line with rest wavelength λrest and velocity relative to the observer is given by: λobs = λrest 1 + (cid:16) (cid:17) Here, λobs = 5012 Å, λrest = 5007 Å, and = 3 108 m/s. We can rearrange the equation to solve for v: λobs λrest 5012 5007 5012 = 1 + = 1 + 1 = c 5012 5007 5007 5 5007 5 5007 Converting the velocity from m/s to km/s: = = 3 108 m/s = = 5 3 108 1 1000 km/s = 5 3 105 5007 km/s 299.5 km/s 2. **Proper motion effect:** The proper motion µ of the star is given as 1 arc second per year. This can be converted to an angular velocity ω using the formula: 1 year 3.154 107 seconds 3.154 107 arc seconds per second 1 arc second 1 year ω = = The proper motion µ is in arc seconds per year, so we need to convert it to radians per second: µ =1 arc second/year 1 radian 206265 arc seconds 1 year 3.154 107 seconds = 1 206265 3.154 107 radians/second 1.5 1013 radians/second The component of the stars velocity parallel to its vector to the Earth (v) is given by: = µ where is the distance to the star in meters (60 pc = 603.0861016 = 1.85161018 m): = 1.5 1013 1.8516 1018 m/s = 277.74 km/s Since the rest-wavelength shift calculation gives us velocity of approximately 299.5 km/s, and the proper motion effect gives us component of velocity of approximately 277.74 km/s, we can conclude that the component of the stars velocity parallel to its vector to the Earth is dominated by the rest-wavelength shift effect. Therefore, the final answer is: 300 km/s"
        },
        {
            "title": "Olympiad Bench Example",
            "content": "Problem cubic sequence is sequence of integers given by an = n3 + bn2 + cn + d, where b, and are integer constants and ranges over all integers, including negative integers. Determine the possible values of a2015 a2016 for cubic sequence satisfying the condition in part (a). Answer 0 RAFT++ Output First, lets understand the cubic sequence an = n3 + bn2 + cn + d. We need to find the possible values of a2015 a2016. Given that an is cubic sequence, it can be expressed in the form an = n3 + bn2 + cn + d. Therefore, a2015 = 20153 + 20152 + 2015 + a2016 = 20163 + 20162 + 2016 + The product of these two terms can be expressed as: a2015 a2016 = (20153 + 20152 + 2015 + d) (20163 + 20162 + 2016 + d) Since the sequence terms are given by polynomial, and the polynomials coefficients can take any integer value, the product a2015 a2016 could theoretically take any integer value depending on the integers b, c, and d. Hence, the possible values of a2015 a2016 span all integers Z. Thus, the set of possible values of a2015 a2016 is all integers Z. The final answer is: GVM-GRPO Output The general form of cubic sequence is given by an = n3 + bn2 + cn + d. We need to find the possible values of a2015 a2016. First, lets write down the expressions for a2015 and a2016: a2015 = 20153 + 20152 + 2015 + a2016 = 20163 + 20162 + 2016 + To find a2015 a2016, lets look at the difference between consecutive terms of the sequence: an+1 an = [(n + 1)3 + b(n + 1)2 + c(n + 1) + d] [n3 + bn2 + cn + d] = [(n + 1)3 n3] + [b(n + 1)2 bn2] + [c(n + 1) cn] = [(n + 1 n)((n + 1)2 + (n + 1)n + n2)] + [b((n + 1)2 n2)] + [c(n + 1 n)] = [1 (n2 + 2n + 1 + n2 + + n2)] + [b(n2 + 2n + 1 n2)] + [c] = 3n2 + 3n + 1 + b(2n + 1) + = 3n2 + 3n + 1 + 2bn + + = 3n2 + (3 + 2b)n + (1 + + c) Since b, are constants, the difference an+1 an is quadratic function of n. This means an itself must be cubic function, which confirms the form an = n3 + bn2 + cn + d. Now, lets look at the product a2015 a2016: a2015 = 20153 + 20152 + 2015 + a2016 = 20163 + 20162 + 2016 + The product a2015 a2016 can be simplified by noting that: a2015 a2016 = (20153 + 20152 + 2015 + d) (20163 + 20162 + 2016 + d) 29 This can be simplified further by noting that: a2016 = a2015 + (20163 20153) + b(20162 20152) + c(2016 2015) = a2015+(20162015)(20162+20162015+20152+2016+2015)+b(20162015)(2016+2015)+c = a2015 + (20162 + 2016 2015 + 20152 + 2016 + 2015) + b(2016 + 2015) + Since the terms are cubic, the exact values of a2015 and a2016 depend on the specific values of b, c, and d, but the general form shows that a2015 and a2016 are very close together. Specifically, since 20163 is very large compared to 20153, the terms involving b, c, and may cancel out in the product. Thus, the product a2015 a2016 is essentially the square of a2015 plus some constant terms. This suggests that the only possible values for a2015 a2016 are those that can be expressed as the square of an integer. Therefore, the possible values of a2015 a2016 are the squares of the product of integers. The final answer is: 0 This is because the only integers a2015 and a2016 can satisfy are 0, given the cubic nature of the sequence and the sequence behavior around = 2015 and = 2016. AIME24 Example Problem list of positive integers has the following properties: The sum of the items in the list is 30. The unique mode of the list is 9. The median of the list is positive integer that does not appear in the list itself. Find the sum of the squares of all the items in the list. Answer 236 RAFT++ Output Given the conditions of the problem, we can determine the structure of the list step by step. Lets denote the list of positive integers as a1, a2, . . . , an. 1. The sum of the items in the list is 30. 2. The unique mode of the list is 9. Since 9 is the unique mode, it must be the most frequently occurring number in the list. 3. The median of the list is positive integer that does not appear in the list itself. Lets start by considering the minimum possible values for n. Given that the mode is 9 and it must appear more frequently than any other number, lets consider small values for n. If = 5, the list would have to include 9 three or more times to be the mode. But then the median would be 9, which contradicts the condition that the median does not appear in the list. So 6. If = 6, the list could potentially have 9 twice and the other numbers distributed such that the median is the third number in the sorted list and does not appear in the list. If the median is 6, the list [6, 9, 9, a, b, c] would satisfy the conditions, where a, b, 6 and a, b, = 9. The sum of the numbers should be 30: 6 + 9 + 9 + + + = 30 = + + = Since a, b, 6 and they are distinct integers, the only possible values for a, b, are 1, 2, 3. Thus, the list is [6, 9, 9, 1, 2, 3] or any permutation that maintains the same properties. The sum of the squares of the numbers is: 62 + 92 + 92 + 12 + 22 + 32 = 36 + 81 + 81 + 1 + 4 + 9 = 212 We can now verify that this is correct by checking the conditions. The sum is 30, the mode is 9, and the median is 6, which does not appear in the list. The final answer is: 212 30 GVM-GRPO Output To solve this problem, we need to carefully consider the properties of the list of positive integers: 1. The sum of the items in the list is 30. 2. The unique mode of the list is 9. This means that 9 appears more frequently than any other number in the list. 3. The median of the list is positive integer that does not appear in the list itself. Lets denote the list of integers as [a1, a2, . . . , an] where a1 a2 . . . an. Since 9 is the unique mode, it must appear at least twice, and more frequently than any other number. The median is the middle value when the list is sorted. Since the median is not in the list, must be odd. Lets assume = 2k + 1. The median is then ak+1. To find the sum of the squares of all the items in the list, we need to determine the possible distribution of the numbers. Since 9 is the unique mode, it must appear at least twice, and more frequently than any other number. The list must include other numbers, but 9 must be the most frequent number. The sum of the numbers is 30, and the median must be positive integer that does not appear in the list. Lets start by assuming the simplest case where 9 appears exactly twice. This means a1 = a2 = 9. The list then looks like [9, 9, a3, a4, . . . , a2k+1]. The sum of these numbers is 30, so 9 + 9 + a3 + a4 + . . . + a2k+1 = 30. This simplifies to 18 + a3 + a4 + . . . + a2k+1 = 30, or a3 + a4 + . . . + a2k+1 = 12. The median ak+1 must be positive integer that does not appear in the list. If = 1, then = 3, and the list is [9, 9, a3]. The median is a2 = 9, which is not valid solution since 9 must appear more frequently than any other number. If = 2, then = 5, and the list is [9, 9, a3, a4, a5]. The median is a3. Since 9 appears twice, a3 must be different from 9. The sum of the other three numbers is 12. The possible combinations are (1, 1, 10), (1, 2, 9), (1, 3, 8), (1, 4, 7), (1, 5, 6), (2, 2, 8), (2, 3, 7), (2, 4, 6), (3, 3, 6), (3, 4, 5). However, since 9 must be the mode, a3 cannot be 9. Also, a3 must be positive integer that does not appear in the list. The only valid combination is (1, 2, 9), but 9 is not valid median. The next valid combination is (1, 3, 8), where the median a3 = 3. This means the list is [9, 9, 3, a4, a5], and the remaining sum is 12 3 = 9. The only way to split 9 into two positive integers that are not 9 or 3 is 1 and 8. So the list is [9, 9, 3, 1, 8]. The sum of the squares of these numbers is 92 + 92 + 32 + 12 + 82 = 81 + 81 + 9 + 1 + 64 = 236. Therefore, the sum of the squares of all the items in the list is 236 . AMC23 Example Problem Rows 1, 2, 3, 4, and 5 of triangular array of integers are shown below. 1 1 1 1 3 1 1 5 5 1 1 7 11 7 1 Each row after the first row is formed by placing 1 at each end of the row, and each interior entry is 1 greater than the sum of the two numbers diagonally above it in the previous row. What is the units digits of the sum of the 2023 numbers in the 2023rd row? Answer 5 RAFT++ Output We need to determine the units digit of the sum of the 2023rd row in the triangular array. Observing the Behavior of Units Digits 1. **First Row:** 1 (units digit is 1) 2. **Second Row:** 1 1 (units digit is 1 + 1 = 2) 3. **Third Row:** 1 3 1 (units digit is 3) 4. **Fourth Row:** 1 5 5 1 (units digit is 1 + 5 = 6) 5. **Fifth Row:** 1 7 11 7 1 (units digit is 7 + 11 = 18, units digit is 8) 6. **Sixth Row:** 1 (7+19) 1 (units digit is 9 + 9 = 18, units digit is 6) If we continue observing this pattern, we notice that the units digits of the sums of the diagonals in each row follow specific pattern which repeats every 2 rows: - First row: 1 - Second row: 2 - Third row: 1 - Fourth row: 6 - Fifth row: 8 - Sixth row: 6 31 We notice that the units digits of the sums for rows 3, 5, 7, 9, . . . are 1, 8, 1, 8, ... and for rows 2, 4, 6, 8, . . . are 2, 6, 6, 6, ... Since the 2023rd row is an odd row, its units digit of the sum will be the same as that of the 3rd, 5th, etc. rows, which is 1. Therefore, the units digit of the sum of the 2023rd row is 1 . 3 1 7 3 7 11 1 4. The fourth 1 5. The fifth row is: 1 1 (units digits: 1, 1) - Row 3: 1 1 (units digits: 1, 1) - Row 3: 1 1 (units digits: 1, 1 (units digits: 5 5 1 (units digits: 1, 5, 5, 1) - Row 5: 1 5 5 1 (units digits: 1, 5, 5, 1) - Row 5: 1 1 3. The third row is: 1 11 GVM-RAFT++ Output To solve this problem, we need to analyze the triangular array and find pattern. Lets start by examining the structure of the array. 1. The first row is: 1 2. The second row is: 1 7 row is: 1 5 The pattern for each interior entry is that it is 1 greater than the sum of the two numbers diagonally above it in the previous row. To find the units digit of the sum of the numbers in the 2023rd row, we can focus on the units digits of the numbers in each row. Notice that each row starts and ends with 1. The units digits of the interior numbers follow specific pattern. Lets calculate the units digits of the first few rows to identify pattern: - Row 1: 1 (units digit: 1) - Row 2: 1 3, 1) - Row 4: 1 1, 7, 1, 7, 1) We can see that the units digits repeat pattern. Lets look at the units digits of the interior numbers more closely: - Row 1: 1 (units digit: 1) - Row 2: 1 3, 1) - Row 4: 1 1, 7, 1, 7, 1) It seems that the units digits of the interior numbers in each row follow pattern where the units digit of each interior number is determined by the sum of the two units digits diagonally above it, incremented by 1. For example, in row 5, the units digit of the middle number 11 is 1 because 5 + 5 + 1 = 11, and the units digit of 1 is 1. Given this pattern, the units digits of the sums of the numbers in each row will also follow pattern. Specifically, the units digits of the sums of the numbers in the rows will be 2, 4, 6, 8, 0, and then repeat (since the sum of the units digits in each row is always 2 more than the sum of the previous rows units digits). The units digits of the sums of the numbers in the rows are as follows: - Row 1: 1 (units digit of sum is 1) - Row 2: 1 + 1 = 2 (units digit of sum is 2) - Row 3: 1 + 3 + 1 = 5 (units digit of sum is 5) - Row 4: 1 + 5 + 5 + 1 = 12 (units digit of sum is 2) - Row 5: 1 + 7 + 11 + 7 + 1 = 27 (units digit of sum is 7) We can see that the units digits of the sums repeat every 4 rows: 1, 2, 5, 2. Therefore, the units digit of the sum of the numbers in the 2023rd row will be the same as the units digit of the sum of the numbers in the 3rd row (since 2023 mod 4 = 3). So, the units digit of the sum of the numbers in the 2023rd row is 5. Lets write the final answer: 1 (units digits: 1, 1 (units digits: 7 3 7"
        }
    ],
    "affiliations": [
        "Salesforce AI Research",
        "University of Illinois Urbana-Champaign"
    ]
}