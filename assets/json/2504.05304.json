{
    "paper_title": "Gaussian Mixture Flow Matching Models",
    "authors": [
        "Hansheng Chen",
        "Kai Zhang",
        "Hao Tan",
        "Zexiang Xu",
        "Fujun Luan",
        "Leonidas Guibas",
        "Gordon Wetzstein",
        "Sai Bi"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Diffusion models approximate the denoising distribution as a Gaussian and predict its mean, whereas flow matching models reparameterize the Gaussian mean as flow velocity. However, they underperform in few-step sampling due to discretization error and tend to produce over-saturated colors under classifier-free guidance (CFG). To address these limitations, we propose a novel Gaussian mixture flow matching (GMFlow) model: instead of predicting the mean, GMFlow predicts dynamic Gaussian mixture (GM) parameters to capture a multi-modal flow velocity distribution, which can be learned with a KL divergence loss. We demonstrate that GMFlow generalizes previous diffusion and flow matching models where a single Gaussian is learned with an $L_2$ denoising loss. For inference, we derive GM-SDE/ODE solvers that leverage analytic denoising distributions and velocity fields for precise few-step sampling. Furthermore, we introduce a novel probabilistic guidance scheme that mitigates the over-saturation issues of CFG and improves image generation quality. Extensive experiments demonstrate that GMFlow consistently outperforms flow matching baselines in generation quality, achieving a Precision of 0.942 with only 6 sampling steps on ImageNet 256$\\times$256."
        },
        {
            "title": "Start",
            "content": "Hansheng Chen 1 Kai Zhang 2 Hao Tan 2 Zexiang Xu 3 Fujun Luan 2 Leonidas Guibas 1 Gordon Wetzstein 1 Sai Bi 2 https://github.com/Lakonik/GMFlow 5 2 0 2 7 ] . [ 1 4 0 3 5 0 . 4 0 5 2 : r a"
        },
        {
            "title": "Abstract",
            "content": "Diffusion models approximate the denoising distribution as Gaussian and predict its mean, whereas flow matching models reparameterize the Gaussian mean as flow velocity. However, they underperform in few-step sampling due to discretization error and tend to produce oversaturated colors under classifier-free guidance (CFG). To address these limitations, we propose novel Gaussian mixture flow matching (GMFlow) model: instead of predicting the mean, GMFlow predicts dynamic Gaussian mixture (GM) parameters to capture multi-modal flow velocity distribution, which can be learned with KL divergence loss. We demonstrate that GMFlow generalizes previous diffusion and flow matching models where single Gaussian is learned with an L2 denoising loss. For inference, we derive GM-SDE/ODE solvers that leverage analytic denoising distributions and velocity fields for precise few-step sampling. Furthermore, we introduce novel probabilistic guidance scheme that mitigates the over-saturation issues of CFG and improves image generation quality. Extensive experiments demonstrate that GMFlow consistently outperforms flow matching baselines in generation quality, achieving Precision of 0.942 with only 6 sampling steps on ImageNet 256256. 1. Introduction Diffusion probabilistic models (Sohl-Dickstein et al., 2015; Ho et al., 2020), score-based models (Song & Ermon, 2019; Song et al., 2021b), and flow matching models (Lipman et al., 2023; Liu et al., 2022) form family of generative models that share an underlying theoretical framework and 1Stanford University, CA 94305, USA 2Adobe Research, CA 95110, USA 3Hillbot. Correspondence to: Hansheng Chen <hanshengchen@stanford.edu.>. Preliminary work. Figure 1. Comparison between vanilla diffusion (with flow velocity parameterization) and GMFlow on 2D checkerboard distribution. (a) The vanilla diffusion model predicts mean velocity, modeling the denoising distribution qθ(x0xt) as single Gaussian, and then samples from Gaussian transition distribution qθ(xttxt). (b) GMFlow predicts GM for velocity and yields multi-modal GM denoising distribution, from which the transition distribution can be analytically derived for precise next-step sampling, allowing more accurate few-step sampling (4 steps in this case). have made significant advances in image and video generation (Yang et al., 2023; Po et al., 2024; Rombach et al., 2022; Saharia et al., 2022b; Podell et al., 2024; Chen et al., 2024; Esser et al., 2024; Blattmann et al., 2023; Hong et al., 2023; HaCohen et al., 2024; Kong et al., 2025). Standard diffusion models approximate the denoising distribution as Gaussian Mixture Flow Matching Models Gaussian and train neural networks to predict its mean, and optionally its variance (Nichol & Dhariwal, 2021; Bao et al., 2022b;a). Flow matching models reparameterize the Gaussian mean as flow velocity, formulating an ordinary differential equation (ODE) that maps noise to data. These formulations remain dominant in image generation as per user studies (Artificial Analysis, 2025; Jiang et al., 2024). However, vanilla diffusion and flow models require tens of sampling steps for high-quality generation, since Gaussian approximations hold only for small step sizes, and numeric ODE integration introduces discretization errors. In addition, high quality generation requires higher classifier-free guidance (CFG) scale (Ho & Salimans, 2021), yet stronger CFG often leads to over-saturated colors (Saharia et al., 2022a; Kynkaanniemi et al., 2024) due to out-of-distribution (OOD) extrapolation (Bradley & Nakkiran, 2024), thus limiting overall image quality even with hundreds of steps. To address these limitations, we deviate from previous single-Gaussian assumption and introduce Gaussian mixture flow matching (GMFlow). Unlike vanilla flow models, which predict the mean of flow velocity u, GMFlow predicts the parameters of Gaussian mixture (GM) distribution, representing the probability density function (PDF) of u. This provides two key benefits: (a) our GM formulation captures more intricate denoising distributions, enabling more accurate transition estimates at larger step sizes and thus requiring fewer steps for high-quality generation  (Fig. 1)  ; (b) CFG can be reformulated by reweighting the GM probabilities rather than extrapolation, thus bounding the samples within the conditional distribution and avoiding over-saturation, thereby improving overall image quality. We train GMFlow by minimizing the KL divergence between the predicted velocity distribution and the ground truth distribution, which we show is generalization of previous diffusion and flow matching models ( 3.1). For inference, we introduce novel SDE and ODE solvers that analytically derive the reverse transition distribution and flow velocity field from the predicted GM, enabling fast and precise few-step sampling ( 3.3). Meanwhile, we develop probabilistic guidance approach for conditional generation, which reweights the GM PDF using Gaussian mask to enhance condition alignment ( 3.2). For evaluation, we compare GMFlow against vanilla flow matching baselines on both 2D toy dataset and ImageNet (Deng et al., 2009). Extensive experiments reveal that GMFlow consistently outperforms baselines equipped with advanced solvers (Lu et al., 2022; 2023; Zhao et al., 2023; Karras et al., 2022). Notably, on ImageNet 256256, GMFlow excels in both Precision and FID metrics with fewer than 8 sampling steps; with 32 sampling steps, GMFlow achieves state-of-the-art Precision of 0.950  (Fig. 6)  . The main contributions of this paper are as follows: We propose GMFlow, generalized formulation of diffusion models based on GM denoising distributions. We introduce GM-based sampling framework consisting of novel SDE/ODE solvers and probabilistic guidance. We empirically validate that GMFlow outperforms flow matching baselines in both fewand many-step settings. 2. Diffusion and Flow Matching Models In this section, we provide background on diffusion and flow matching models as the basis for GMFlow. Note that we introduce flow matching as special diffusion parameterization since they largely overlap in practice (Albergo & Vanden-Eijnden, 2023; Gao et al., 2024). Forward diffusion process. Let RD be data point sampled from distribution with its PDF denoted by p(x). typical diffusion model defines time-dependent interpolation between the data point and random Gaussian noise ϵ (0, I), yielding the noisy data xt = αtx + σtϵ, where [0, ] denotes the diffusion time, and αt, σt are the predefined time-dependent monotonic coefficients (noise schedule) that satisfy the boundary condition x0 = x, xT ϵ. Apparently, the marginal PDF of the noisy data p(xt) can be written as the data distribution p(x) convolved by Gaussian kernel p(xtx0) = (xt; αtx0, σ2 I), i.e., p(xt) = (cid:82) RD p(xtx0)p(x0) dx0. Alternatively, p(xt) can be expressed as previous PDF p(xtt) convolved by transition Gaussian kernel p(xtxtt), given by: p(xtxtt) (cid:18) = xt; αt αtt (cid:18) (cid:122) σ2 xtt, βt,t (cid:125)(cid:124) α2 α tt (cid:123) σ2 tt (cid:19) (cid:19) , (1) where the transition variance is denoted by βt,t for brevity. series of convolutions from 0 to constructs diffusion process over time. With infinitesimal step size t, this process can be continuously modeled by the forward-time SDE (Song et al., 2021b). Reverse denoising process. Diffusion models generate samples by first sampling the noise xT ϵ and then reversing the diffusion process to obtain x0. This can be achieved by recursively sampling from the reverse transition distribution p(xttxt) = p(xtt)p(xtxtt) . DDPM (Ho et al., p(xt) 2020) approximate p(xttxt) with Gaussian and reparameterize its mean in terms of residual noise, which is then learned by neural network. Its sampling process is equivalent to first-order solver of reverse-time SDE. Song et al. (2021b) reveal that the time evolution of the marginal distribution p(xt) described by the SDE is the same as that described by flow ODE, which maps the noise xT to data x0 deterministically. In particular, flow matching models 2 Gaussian Mixture Flow Matching Models train neural network to directly predict the ODE velocity field dxt dt . They typically adopt linear noise schedule, which defines := 1, αt := 1 t, σt := t, thus yielding dt = Ex0p(x0xt)[u], simplified velocity formulation dxt with the random flow velocity defined as: := xt x0 σt . (2) This reveals that the velocity field dxt dt represents the mean of the random velocity over the denoising distribution p(x0xt) = p(x0)p(xtx0) . p(xt) Flow matching loss. Flow models stochastically regress dxt dt using randomly paired samples of x0 and xt. Let µθ(xt) denote flow velocity neural network with learnable parameters θ, the L2 flow matching loss is given by: = Et,x0,xt µθ(xt)2 (cid:21) . (cid:20) 1 (3) In practice, additional condition signals (e.g., class label or text prompt) can be fed to the network µθ(xt, c), making it conditioned diffusion model that learns p(x0c). Limitations. Sampling errors in flow models can arise from two sources: (a) discretization errors in SDE/ODE solvers, and (b) inaccurate flow prediction µθ(xt, c) due to underfitting (Karras et al., 2024). While discretization errors can be reduced by reducing step size, it increases the number of function evaluations (NFE), causing significant computational overhead. To mitigate prediction inaccuracies, CFG (Ho & Salimans, 2021) performs an extrapolation of conditional and unconditional predictions, given by wµθ(xt, c) + (1 w)µθ(xt), where [1, +) is the guidance scale. Such method improves image quality and condition alignment at the expense of diversity. However, higher may lead to OOD samples and cause image oversaturation, which is often mitigated with heuristics such as thresholding (Saharia et al., 2022a; Sadat et al., 2024). 3. Gaussian Mixture Flow Matching Models In this section, we introduce our GMFlow models, covering its parameterization and loss function ( 3.1), probabilistic guidance mechanism ( 3.2), GM-SDE/ODE solvers ( 3.3), and other practical designs for image generation ( 3.4). Table 1 summarizes the key differences between GMFlow and vanilla flow models. Algorithms 1 and 2 present the outlines of training and sampling schemes, respectively. 3.1. Parameterization and Loss Function Different from vanilla flow models that regress mean velocity, we model the velocity distribution p(uxt) as Gaussian 3 Table 1. Comparison between vanilla flow models and GMFlow. Vanilla diffusion flow GMFlow Transition assumption is small p(xttxt) is approximately Gaussian Derive p(xttxt) from the predicted GM Network output Training loss Mean of flow velocity = (cid:80)K k=1 AkN (u; µk, s2I) µθ(xt) 2 µθ(xt)2(cid:3) Et,x0,xt[ log qθ(uxt)] (cid:2) 1 GM params in qθ(uxt) Et,x0,xt Sampling methods 1st ord: Euler, DDPM. . . 2nd ord: DPM++, UniPC. . . 1st ord: GM-SDE/ODE 2nd ord: GM-SDE/ODE 2 Guidance Mean extrapolation wµθ(xt, c) + (1 w)µθ(xt) GM reweighting w(u) qθ(uxt, c) mixture (GM): qθ(uxt) = (cid:88) k=1 AkN (u; µk, Σk), (4) where {Ak, µk, Σk} are dynamic GM parameters predicted by network with parameters θ, and is hyperparameter specifying the number of mixture components. To enforce (cid:80) Ak = 1 with Ak 0, we employ softmax activation Ak = exp ak , where {ak R} are pre-activation logits. (cid:80) exp ak We train GMFlow by matching the predicted distribution qθ(uxt) with the ground truth velocity distribution p(uxt). Specifically, we minimize the KullbackLeibler (KL) divergence (Kullback & Leibler, 1951) Eup(uxt)[log p(uxt)log qθ(uxt)], where log p(uxt) does not affect backpropagation and can be omitted. During training, we sample p(uxt) by drawing pair of x0, xt, and then calculate the velocity using Eq. (2). The resulting loss function is therefore reformulated as: = Et,x0,xt[ log qθ(uxt)]. In Eq. (6), we present an expanded form of this loss function. (5) Why choosing Gaussian mixture? While there are large family of parameterized distributions, we choose Gaussian mixture for its following desired properties: Mean alignment. The mean of the ground truth distribution p(uxt) is crucial since it decides the velocity term in the flow ODE and the drift term in the reverse-time SDE ( C.1). When the loss in Eq. (5) is minimized (assuming sufficient network capacity), the GM distribution qθ(uxt) guarantees that its mean aligns with that of p(uxt). Theorem 3.1. Given any distribution p(u) and any symmetric positive definite matrices {Σk}, if {a k} are the optimal GM parameters w.r.t. with the objective min{ak,µk} Eup(u)[ log (cid:80) AkN (u; µk, Σk)], then the GM mean satisfies (cid:80) kµ We provide the proof of the theorem in C.2. Its worth pointing out that not all distributions satisfy this property = Eup(u)[u]. k, µ Gaussian Mixture Flow Matching Models (e.g. the mean of Laplace distribution aligns to the median instead of the mean). Analytic calculation. GM enables necessary calculations to be approached analytically (e.g., for deriving the mean and the transition distribution), as detailed in D. Expressiveness. GMs can approximate intricate multimodal distributions. With sufficient number of components, GMs are theoretically universal approximators (Huix et al., 2024). Simplified covariances. The expansion of Eq. (5) involves the inverse covariance matrix Σ1 , which can lead to training instability. To mitigate this, we simplify each covariance to scaled identity matrix, i.e., Σk = s2I, where R+ is the predicted standard deviation shared by all mixture components. Its worth pointing out that this simplification does not limit the GMs expressiveness, which is mainly dominated by K. Moreover, Theorem 3.1 implies that the structure of the covariance matrix is irrelevant to the accuracy of the mean velocity, mitigating the need for intricate covariances. Under this simplification, the expanded GM KL loss is reduced to: (cid:34) (cid:32) = Et,x0,xt log (cid:88) exp k=1 1 2s2 µk2 log + log Ak (cid:33)(cid:35) , (6) which can be interpreted as hybrid of regression loss to the centroids and classification loss to the components. Special cases of GMFlow. GMFlow generalizes several formulations of previous diffusion and flow models. In special case where = 1, = 1, Eq. (6) is identical to the L2 loss in Eq. (3). Therefore, GMFlow is generalization of vanilla diffusion and flow models. In another case where {µk} are velocities towards predefined tokens and 0, then {Ak} represent token probabilities, making Eq. (6) analogous to categorical diffusion objectives (Gu et al., 2022; Dieleman et al., 2022; Campbell et al., 2022). u-to-x0 reparameterization. While the neural network directly outputs the distribution, we can flexibly reparameterize it into an x0 distribution by substituting = xtx0 into Eq. (4), yielding qθ(x0xt) = (cid:80) xI), with the new parameters µxk = xt σtµk and sx = σts. The velocity KL loss to an x0 likelihood loss = is thus equivalent Et,x0,xt[ log qθ(x0xt)]. AkN (x0; µxk, σt 3.2. Probabilistic Guidance via GM Reweighting Vanilla CFG suffers from over-saturation due to unbounded extrapolation, which overshoots samples beyond the valid data distribution. In contrast, GMFlow can provide well4 defined conditional distribution qθ(uxt, c). This allows us to formulate probabilistic guidance, principled approach that reweights the predicted distribution while preserving its intrinsic bounds and structure. To reweight the GM PDF qθ(uxt, c) analytically, we multiply it by Gaussian mask w(u): qw(uxt, c) := w(u) qθ(uxt, c), (7) where is normalization factor. The reweighted qw(uxt, c) remains GM with analytically derived parameters (see D.2 for derivation), and is treated as the model output for sampling. Then, our goal is to design w(u) so that it enhances condition alignment without inducing OOD samples. To this end, we approximate the conditional and unconditional GM predictions qθ(uxt, c) and qθ(uxt) as isotropic Gaussian surrogates (u; µc, s2 uI) by matching the mean and total variance of the GM (see A.1 for details). Using these approximations, we define the unnormalized Gaussian mask as: I) and (u; µu, s2 w(u) := (cid:0)u; µc + wscµn, (cid:0)1 w2(cid:1)s2 (u; µc, s2 I) I(cid:1) , (8) where [0, 1) is the probabilistic guidance scale, and µn := is the normalized mean difference. µcµu µcµu/ Intuitively, the numerator in Eq. (8) shifts the conditional mean by the bias wscµn to enhance conditioning, while reducing the conditional variance according to biasvariance decomposition. Notably, for any w, sample from the numerator Gaussian satisfies Eu c. This ensures that the original bounds of the conditional distribution are preserved. (cid:2)u µc2/D(cid:3) s2 Meanwhile, the denominator in Eq. (8) cancels out the original mean and variance of the conditional GM, so that the reweighted GM qw(uxt, c) approximately inherits the adjusted mean and variance of the numerator. Since the masking operation retains the original GM components, the fine-grained structure of the conditional GM is preserved. With preserved bounds and structure, probabilistic guidance reduces the risk of OOD samples compared to vanilla CFG, effectively preventing over-saturation and enhancing overall image quality. 3.3. GM-SDE and GM-ODE Solvers In this subsection, we show that GMFlow enables unique SDE and ODE solvers that greatly reduce discretization errors by analytically deriving the reverse transition distribution and flow velocity field. GM-SDE solver. Given the predicted x0-based GM Gaussian Mixture Flow Matching Models qθ(x0xt) = (cid:80) xI), the reverse transition distribution qθ(xttxt) can be analytically derived (see C.3 for derivation): AkN (x0; µxk, s2 qθ(xttxt) (cid:88) = AkN (cid:0)xtt; c1xt + c2µxk, (cid:0)c3 + c2 2s2 (cid:1)I(cid:1), (9) k= σ2 αt αtt , c2 = βt,t σ2 tt σ2 αtt, σ2 tt. with the coefficients c1 = c3 = βt,t By recursively sampling from σ2 qθ(xttxt), we obtain GM approximation to the reverse-time SDE solution. Alternatively, we can implement the solver by first sampling ˆx0 qθ(x0xt) and then sampling p(xttxt, ˆx0) = (xtt; c1xt + c2 ˆx0, c3I) (Song et al., 2021a) (see C.3 for details). Theoretically, if qθ(x0xt) is accurate, the GM-SDE solution incurs no error even in single step. In practice, smaller step size increases reliance on the mean (see C.4 for details) over the shape of the distribution, which can be more accurate as per Theorem 3.1. GM-ODE solver. While GMFlow supports standard ODE integration by converting its GM prediction into the current mean velocity, it also enables unique sampling scheme with reduced discretization errors by analytically deriving the flow velocity field for any time τ < t, thereby facilitating sub-step integration without additional neural network evaluations. Specifically, given the x0-based GM qθ(x0xt) at xt, we show in C.5 that the denoising distribution at xτ can be derived as: ˆq(x0xτ ) = p(xτ x0) p(xtx0) qθ(x0xt), (10) where is normalization factor, and ˆq(x0xτ ) is also GM with analytically derived parameters. This allows us to instantly estimate GMFlows next-step prediction at xτ from its current prediction at xt, without re-evaluating the neural network. The velocity can therefore be derived by reparameterizing ˆq(x0xτ ) in terms of and computing its mean. This enables the GM-ODE solver, which integrates curved trajectory along the analytic velocity field via multiple Euler sub-steps between and (Algorithm 2 line 1822). Theoretically, if qθ(x0xt) is accurate, the GMODE solution also incurs no error. In practice, the velocity field is only locally accurate as τ and xτ xt, thus multiple network evaluations are still required. Second-order multistep GM solvers. Vanilla diffusion models often use AdamsBashforth-like second-order multistep solvers (Lu et al., 2023), which extrapolate the mean x0 predictions of the last two steps to estimate the next midpoint, and then apply an Euler update. Analogously, we extend this approach to GMs. Given GM predictions at times and + t, we first convert the latter to time via 5 Eq. (10), yielding ˆq(x0xt). In an ideal scenario where both GMs are accurate, ˆq(x0xt) perfectly matches qθ(x0xt); otherwise, their discrepancy is extrapolated following the AdamsBashforth scheme. To perform GM extrapolation, we adopt GM reweighting scheme similar to that in 3.2. More details are provided in A.2. 3.4. Practical Designs Pixel-wise factorization. For high-dimensional data such as images, Gaussian mixture models often suffer from mode collapse. To address this, we treat each pixel (in the latent grid for latent diffusion (Rombach et al., 2022)) as an independent low-dimensional GM, making the training loss the sum of per-pixel GM KL terms. Spectral sampling. Due to the factorization, image generation under GM-SDE solvers performs the sampling step ˆx0 qθ(x0xt) independently for each pixel, neglecting spatial correlations. To address this, we adopt spectral sampling technique that imposes correlations through the frequency domain. Specifically, we generate spatially correlated Gaussian random field from learnable power spectrum. The per-pixel Gaussian samples are then mapped to GM samples via KnotheRosenblatt transport (Knothe, 1957; Rosenblatt, 1952). More details are provided in A.3. Transition loss. Empirically, we observe that the GM KL loss may still induce minor gradient spikes when becomes small. To stabilize training, we adopt modified loss based on the transition distributions (Eq. (9)), formulated as Ltrans = Et,xtt,xt[ log qθ(xttxt)], where we define := λt with the transition ratio hyperparameter λ (0, 1]. When λ = 1, the loss is equivalent to the original; when λ < 1, the transition distribution has lower bound of variance c3, which improves training stability. The modified training scheme is described in Algorithm 1. 4. Experiments To evaluate the effectiveness of GMFlow, we compare it against vanilla flow matching baselines on two datasets: (a) simple 2D checkerboard distribution, which facilitates visualization of sample histograms and analysis of underlying mechanisms; (b) the class-conditioned ImageNet dataset (Deng et al., 2009), challenging benchmark that demonstrates the practical advantages of GMFlow for large-scale image generation. 4.1. Sampling 2D Checkerboard Distribution In this subsection, we compare GMFlow with the vanilla flow model on simple 2D checkerboard distribution, following the experimental settings of Lipman et al. (2023). All configurations adopt the same 5-layer MLP architecture for denoising 2D coordinates, differing only in their output Gaussian Mixture Flow Matching Models Figure 2. Comparison among vanilla flow models with different solvers and GMFlow. For both SDE and ODE, our method achieves higher quality in few-step sampling. channels. For GMFlow, we train multiple models with different numbers of GM components (with λ = 0.9); for GM-ODE sampling, we use = 128/NFE sub-steps. Comparison against flow model baseline. In Fig. 2, we compare the 2D sample histograms of GMFlow using second-order GM solvers (GM-SDE/ODE 2) against the vanilla flow matching baseline using established SDE and ODE solvers (Lu et al., 2023; Zhao et al., 2023; Ho et al., 2020; Song et al., 2021a). Notably, vanilla flow models require approximately 8 steps to achieve reasonable histogram and 1632 steps for high-quality sampling. In contrast, GMFlow (K = 64) can approximate the checkerboard in single step and achieve high-quality sampling in 4 steps. Moreover, for vanilla flow models, samples generated by first-order solvers (DDPM, Euler) tend to concentrate toward the center, whereas those from second-order solvers (DPM++, UniPC) concentrate near the outer edges. In contrast, GMFlow samples are highly uniform. This validates GMFlows advantage in few-step sampling and multi-modal distribution modeling. Impact of the GM component count K. Fig. 3 (a) demonstrates that increasing significantly improves few-step sampling results, leading to sharper histograms. Notably, GM-SDE sampling with = 1 is theoretically equivalent to DDPM with learned variance (Nichol & Dhariwal, 2021), which produces blurrier histogram. This further highlights Figure 3. (a) Comparison of firstand second-order GM-SDE and GM-ODE solvers with varying NFE and GM components K. Increasing improves few-step sampling results. Second-order solvers produce sharper histograms with fewer outliers than firstorder solvers. (b) Ablation study on the ˆq(x0xt) conversion in second-order solvers. Removing this conversion causes samples to overshoot and concentrate at the edges. Figure 4. Architecture of GMFlow-DiT. The original DiT (Peebles & Xie, 2023) is shown in blue, and the modified output layers are shown in purple. the expressiveness of GMFlow. Second-order GM solvers. Fig. 3 (a) also compares our firstand second-order GM solvers across various NFEs and GM component counts. Comparing the left (first-order) and right (second-order) columns, we observe that secondorder solvers produce sharper histograms and better suppress outliers, particularly when NFE and are small. With = 8, the GM probabilities are sufficiently accu6 Gaussian Mixture Flow Matching Models Figure 5. Qualitative comparisons (at best Precision) among GMFlow (GM-ODE 2) and vanilla flow model baselines (UniPC and Euler). GMFlow produces consistent results across various NFEs, whereas baselines struggle in few-step sampling, exhibiting distorted structures. rate that the second-order solvers do not make difference, aligning with our theoretical analysis. Ablation studies. To validate the importance of the ˆq(x0xt) conversion in second-order GM solvers, we conduct an ablation study by removing the conversion and directly extrapolating the x0 distributions of the last two steps, similar to DPM++ (Lu et al., 2023). As shown in Fig. 3 (b), removing this conversion introduces an overshooting bias, similar to DPM++ 2M SDE (NFE = 8), underscoring its importance in maintaining sample uniformity. 4.2. ImageNet Generation For image generation evaluation, we benchmark GMFlow against vanilla flow baselines on class-conditioned ImageNet 256256. We train Diffusion Transformer (DiTXL/2) (Peebles & Xie, 2023; Vaswani et al., 2017) using the flow matching objective (Eq. (3)) as the baseline, and then adapt it into GMFlow-DiT by expanding its output channels and training it with the transition loss (λ = 0.5). As illustrated in Fig. 4, GMFlow-DiT produces weight maps and mean maps as the pixel-wise parameters {Ak, µk}. Meanwhile, tiny two-layer MLP separately predicts using only the time and condition as inputs. Empirically, we find this design to be more stable than predicting using the main DiT. The adaptation results in only 0.2% increase in network parameters for = 8. During inference, we apply the orthogonal projection technique by Sadat et al. (2024) to both the baseline and GMFlow since it universally improves Precision. Additional training and inference details are provided in A.4. Evaluation protocol. For quantitative evaluation, we adopt the standard metrics used in ADM (Dhariwal & Nichol, 2021), including Frechet Inception Distance (FID) (Heusel et al., 2017), Inception Score (IS), and PrecisionRecall (Kynkaanniemi et al., 2019). These metrics are highly senFigure 6. (a) Comparison of the best Precision and best FID among GMFlow and vanilla flow model baselines using different solvers across varying NFEs on ImageNet. For best FID, GMFlow significantly outperforms the baselines in few-step sampling; for best Precision, GMFlow consistently excels across different NFEs. (b) Precision-Recall curves of different methods at NFE = 8. Points corresponding to the best FID and best Precision are marked on the curves. GMFlow achieves superior Precision and Recall. sitive to the classifier-free guidance (CFG) scale: while smaller CFG scales (w 1.4) often yield the best FID by balancing diversity and quality, human users generally favor 7 Gaussian Mixture Flow Matching Models higher CFG scales (w > 3.0) for the best perceptual quality, which also leads to the best Precision. The best Recall and IS values typically occur outside these CFG ranges and are less representative of typical usage. Therefore, for fair and complete evaluation, we sweep over the CFG scale or the probabilistic guidance scale for each model, and report the best FID and best Precision. We also present PrecisionRecall curves, illustrating the qualitydiversity trade-off more comprehensively. Additionally, following Sadat et al. (2024), we report the Saturation metric at the best Precision setting to assess over-saturation. Comparison against flow model baselines. For baselines, we test the vanilla flow model using various first-order solvers (DDPM (Ho et al., 2020), Euler (DDIM) (Song et al., 2021a)) and advanced second-order solvers (DPM++ (Lu et al., 2023), DEIS (Zhang & Chen, 2023), UniPC (Zhao et al., 2023), SA-Solver (Xue et al., 2023)). Details on adapting these solvers for flow matching are presented in A.5. In Fig. 6 (a), we compare these baselines with our GMFlow (K = 8) model equipped with second-order GM solvers (GM-SDE/ODE 2). GMFlow consistently achieves superior Precision in both SDE and ODE sampling across various NFEs. Notably, GM-ODE 2 reaches Precision of 0.942 in just 6 steps, while GM-SDE 2 attains state-of-the-art Precision of 0.950 in 32 steps. For FID, GMFlow outperforms baselines in few-step settings (NFE 8) and remains competitive in many-step settings (NFE 32). Fig.6 (b) further illustrates that the Precision-Recall curves of firstand second-order GM solvers consistently outperform those of their baseline counterparts. Qualitative comparisons are presented in Fig. 5. Saturation assessment. Table 2 compares the Saturation metrics of different methods at their best Precision. GMFlow (K = 8) effectively reduces over-saturation, achieving Saturation levels closest to real data. Visual comparisons in Fig. 5 further support this finding. Additionally, ablating the orthogonal projection technique results in the same trend: GMFlow consistently achieves the best Saturation, while baseline methods perform even worse without orthogonal projection. Impact of the GM component count K. In Fig. 7, we analyze the impact of the number of GM components on the metrics. Few-step sampling (NFE = 8) benefits the most from increasing GM components, particularly with the GMSDE 2 solver. The metrics generally saturate at = 8. Beyond this point, the GM-SDE 2 Precisions decline because spectral sampling introduces larger numerical errors when more Gaussian components are used. Additionally, Table 3 presents the time-averaged negative log-likelihood (NLL) values of data under the predicted distribution qθ(x0xt, c), showing that increasing the number of Gaussians significantly reduces NLL, suggesting its potential benefits for Table 2. ImageNet evaluation results at best Precision (NFE = 32). The reported Saturation values (Sadat et al., 2024) are relative to the real data statistics (Saturation=0.318). Method DDPM small DPM++ 2M SDE GM-SDE GM-SDE 2 Orthogonal projection Euler UniPC GM-ODE GM-ODE 2 DDPM small DPM++ 2M SDE GM-SDE Euler UniPC GM-ODE Guidance Precision Saturation = 3.3 = 2.9 = 0.47 = 0.39 = 3.3 = 3.3 = 0.47 = 0. = 3.3 = 2.9 = 0.27 = 3.3 = 3.3 = 0.27 0.944 0.938 0.950 0.950 0.934 0.931 0.947 0.947 0.939 0.933 0.943 0.931 0.929 0. +0.032 +0.032 0.019 0.019 +0.052 +0.048 0.024 0.024 +0.056 +0.054 +0.023 +0.064 +0.060 +0.016 Figure 7. Best FIDs and best Precisions of GMFlow models with varying numbers of GM components K. Table 3. Validation NLL of ImageNet training images (latents) under different GMFlow configurations. Method = 1 = 4 = 8 = 16 = 16 +spectral NLL (bits/dim) 0.346 0.263 0.242 0. 0.173 posterior sampling applications. Ablation studies. Table 4 presents ablation study results evaluating the impact of various design choices in our method. For GM-SDE, removing spectrum sampling (A2) leads to noticeable degradation in FID. Replacing our probabilistic guidance with vanilla CFG (A3)i.e., naively shifting the mean of the predicted GMresults in severe quality degradation (lower Precision) and over-saturation, which is also evident in Fig. 8. Replacing GM-SDE 2 with the second-order DPM++ SDE solver (A4) worsens both FID and Precision. Reducing the transition loss to the original KL loss (A5) degrades FID. Finally, for GM-ODE 2, directly applying ODE integration without taking sub-steps (B1) leads to significantly worse FID. Inference time. GMFlow only alters the output layer and uses solvers based on simple arithmetic operations. As result, it adds only 0.005 sec of overhead per step (batch size 125, A100 GPU) compared to its flow-matching counterpart, 8 Gaussian Mixture Flow Matching Models which is minimal compared to the total inference time of 0.39 sec per stepmost of which is spent on DiT. Table 4. Ablation studies on GMFlow (K = 8, NFE = 8) using ImageNet evaluation. 5. Related Work Prior works (Nichol & Dhariwal, 2021; Bao et al., 2022b;a) extend standard diffusion models by learning the variance of denoising distributions, which is effectively special case of GMFlow with = 1 and learnable s. GMS (Guo et al., 2023) further extends this approach to third-order moments and fits bimodal GM to the moments during inference. In B.3, we present comparison between GMFlow (K = 2) and GMS. Conversely, Xiao et al. (2022) employ generative adversarial network (Goodfellow et al., 2014) to capture more expressive distribution, though adversarial diffusion typically relies on additional objectives for better quality and stability (Jolicoeur-Martineau et al., 2021; Kim et al., 2024). However, none of these methods address deterministic ODE sampling. growing line of work trains networks to directly predict ODE integrals or denoised samples, enabling extremely fewstep sampling (Salimans & Ho, 2022; Song et al., 2023; Yin et al., 2024b; Sauer et al., 2024). These approaches fall under the category of distillation methods, whose quality typically remains bounded by the many-step performance of standard diffusion models, unless supplemented by additional adversarial training (Kim et al., 2024; Yin et al., 2024a). Efforts to refine CFG beyond thresholding (Saharia et al., 2022a) and orthogonal projection (Sadat et al., 2024) include disabling CFG in early steps (Kynkaanniemi et al., 2024), which compromises Precision, and adding Langevin corrections (Bradley & Nakkiran, 2024), which reduces efficiency. Beyond diffusion models, Gaussian mixtures have also been employed in other generative models. DeLiGAN (Gurumurthy et al., 2017) and GM-GAN (Ben-Yosef & Weinshall, 2018) enhance the latent expressiveness of GANs through the introduction of mixture priors. GIVT (Tschannen et al., 2024) models the output of an autoregressive Transformer as Gaussian mixture distribution, enabling continuous data sampling and outperforming its quantization-based counterpart. 6. Conclusion In this work, we introduced GMFlow, generalization of diffusion and flow models that represents flow velocity as Gaussian mixture, offering greater expressiveness for capturing complex multi-modal structures. We derived principled SDE/ODE solvers unique to this approach and proposed novel probabilistic guidance technique to eliminate ID Method Best FID Best Precision Saturation @Best Prec. A0 Full model (GM-SDE 2) A1 A0 GM-SDE A2 A1 w/o spec. sampling A3 A2 Vanilla CFG A4 A0 DPM++ 2M SDE A5 A0 λ = 1.0 0.939 3.43 6.96 (+3.53) 0.938 (0.001) 8.98 (+2.02) 0.940 (+0.002) 9.02 (+0.04) 0.917 (0.023) 4.59 (+1.16) 0.912 (0.027) 4.49 (+1.06) 0.941 (+0.002) B0 Full model (GM-ODE 2) 2.77 B1 B0 w/o sub-steps 7.47 (+4.70) 0.947 (+0.001) 0.946 0.003 +0.009 0.022 +0.049 0.062 0.019 0.028 0.031 Figure 8. Ablation study on probabilistic guidance, comparing samples from Table 4 A2 and A3. over-saturation. Extensive experiments demonstrated that GMFlow significantly improves few-step generation while enhancing overall sample quality. This framework lays the foundation for potential future research in both theoretical and practical directions, including applications such as posterior sampling with GMFlow priors."
        },
        {
            "title": "Limitations",
            "content": "To apply Gaussian mixture to high-dimensional data, we adopted pixel-wise factorization for image generation, which may not fully exploit the potential of GMFlow, leaving rooms for further development."
        },
        {
            "title": "Acknowledgments",
            "content": "We thank Liwen Wu, Lvmin Zhang, and Guandao Yang for discussions and feedback. Part of this work was done while Hansheng was an intern at Adobe Research. This project was partially supported by Qualcomm Innovation Fellowship, ARL grant W911NF-21-2-0104, and Vannevar Bush Faculty Fellowship."
        },
        {
            "title": "Impact Statement",
            "content": "This paper presents work whose goal is to advance the field of generation models. There are many potential societal consequences of our work, none which we feel must be specifically highlighted here. 9 Gaussian Mixture Flow Matching Models"
        },
        {
            "title": "References",
            "content": "Albergo, M. S. and Vanden-Eijnden, E. Building normalizing flows with stochastic interpolants. In ICLR, 2023. URL https://arxiv.org/abs/2209.15571. Artificial Analysis. Artificial analysis text to image model leaderboard, 2025. URL https://huggingface. co/spaces/ArtificialAnalysis/ Text-to-Image-Leaderboard. 2025-01-25. Accessed: Bao, F., Li, C., Sun, J., Zhu, J., and Zhang, B. Estimating the optimal covariance with imperfect mean in diffusion probabilistic models. In ICML, 2022a. Bao, F., Li, C., Zhu, J., and Zhang, B. Analytic-DPM: an analytic estimate of the optimal reverse variance in diffusion probabilistic models. In ICLR, 2022b. URL https: //openreview.net/forum?id=0xiJLKH-ufZ. Ben-Yosef, M. and Weinshall, D. Gaussian mixture generative adversarial networks for diverse datasets, and the unsupervised clustering of images, 2018. URL https://arxiv.org/abs/1808.10356. Blattmann, A., Dockhorn, T., Kulal, S., Mendelevitch, D., Kilian, M., Lorenz, D., Levi, Y., English, Z., Voleti, V., Letts, A., Jampani, V., and Rombach, R. Stable video diffusion: Scaling latent video diffusion models to large datasets, 2023. URL https://arxiv.org/abs/ 2311.15127. Bradley, A. and Nakkiran, P. Classifier-free guidance is predictor-corrector, 2024. URL https://arxiv. org/abs/2408.09000. Campbell, A., Benton, J., Bortoli, V. D., Rainforth, T., Deligiannidis, G., and Doucet, A. continuous time framework for discrete denoising models. In NeurIPS, 2022. Chen, J., YU, J., GE, C., Yao, L., Xie, E., Wang, Z., Kwok, J., Luo, P., Lu, H., and Li, Z. Pixart-$alpha$: Fast training of diffusion transformer for photorealistic textto-image synthesis. In ICLR, 2024. URL https:// openreview.net/forum?id=eAKmQPe3m1. Deng, J., Dong, W., Socher, R., Li, L.-J., Li, K., and Fei-Fei, L. Imagenet: large-scale hierarchical image database. In CVPR, pp. 248255, 2009. doi: 10.1109/CVPR.2009. 5206848. Dettmers, T., Lewis, M., Shleifer, S., and Zettlemoyer, L. 8-bit optimizers via block-wise quantization. In ICLR, 2022. Dhariwal, P. and Nichol, A. Q. Diffusion models beat GANs on image synthesis. In Beygelzimer, A., Dauphin, Y., Liang, P., and Vaughan, J. W. (eds.), NeurIPS, 2021. URL https://openreview.net/forum? id=AAWuCvzaVt. Dieleman, S., Sartran, L., Roshannai, A., Savinov, N., Ganin, Y., Richemond, P. H., Doucet, A., Strudel, R., Dyer, C., Durkan, C., Hawthorne, C., Leblond, R., Grathwohl, W., and Adler, J. Continuous diffusion for categorical data, 2022. URL https://arxiv.org/abs/ 2211.15089. Esser, P., Kulal, S., Blattmann, A., Entezari, R., Muller, J., Saini, H., Levi, Y., Lorenz, D., Sauer, A., Boesel, F., Podell, D., Dockhorn, T., English, Z., Lacey, K., Goodwin, A., Marek, Y., and Rombach, R. Scaling rectified flow transformers for high-resolution image synthesis. In ICML, 2024. Gao, R., Hoogeboom, E., Heek, J., Bortoli, V. D., Murphy, K. P., and Salimans, T. Diffusion meets flow matching: Two sides of the same coin, 2024. URL https:// diffusionflow.github.io/. Goodfellow, I., Pouget-Abadie, J., Mirza, M., Xu, B., Warde-Farley, D., Ozair, S., Courville, A., and Bengio, Y. Generative adversarial nets. In Ghahramani, Z., Welling, M., Cortes, C., Lawrence, N., and Weinberger, K. (eds.), NeurIPS, volume 27. Curran Associates, Inc., 2014. URL https://proceedings.neurips. cc/paper_files/paper/2014/file/ 5ca3e9b122f61f8f06494c97b1afccf3-Paper. pdf. Gu, S., Chen, D., Bao, J., Wen, F., Zhang, B., Chen, D., Yuan, L., and Guo, B. Vector quantized diffusion model for text-to-image synthesis. In CVPR, 2022. Guo, H. A., Lu, C., Bao, F., Pang, T., YAN, S., Du, C., and Li, C. Gaussian mixture solvers for diffusion models. In NeurIPS, 2023. URL https://openreview.net/ forum?id=0NuseeBuB4. Gurumurthy, S., Sarvadevabhatla, R. K., and Babu, R. V. Deligan : Generative adversarial networks for diverse and limited data. In CVPR, 2017. HaCohen, Y., Chiprut, N., Brazowski, B., Shalem, D., Moshe, D., Richardson, E., Levin, E., Shiran, G., Zabari, N., Gordon, O., Panet, P., Weissbuch, S., Kulikov, V., Bitterman, Y., Melumian, Z., and Bibi, O. Ltx-video: Realtime video latent diffusion, 2024. URL https: //arxiv.org/abs/2501.00103. Heusel, M., Ramsauer, H., Unterthiner, T., Nessler, B., and Hochreiter, S. Gans trained by two time-scale update rule converge to local nash equilibrium. In NeurIPS, 2017. Gaussian Mixture Flow Matching Models Ho, J. and Salimans, T. Classifier-free diffusion guidance. Krizhevsky, A., Hinton, G., et al. Learning multiple layers In NeurIPS Workshop, 2021. of features from tiny images, 2009. Ho, J., Jain, A., and Abbeel, P. Denoising diffusion probabilistic models. In NeurIPS, 2020. Hong, W., Ding, M., Zheng, W., Liu, X., and Tang, J. Cogvideo: Large-scale pretraining for text-to-video generation via transformers. In ICLR, 2023. URL https: //openreview.net/forum?id=rB6TpjAuSRy. Huix, T., Korba, A., Durmus, A., and Moulines, E. Theoretical guarantees for variational inference with fixedvariance mixture of gaussians. In ICML, 2024. Jiang, D., Ku, M., Li, T., Ni, Y., Sun, S., Fan, R., and Chen, W. GenAI arena: An open evaluation platform for generative models. In NeurIPS Datasets and Benchmarks Track, 2024. URL https://openreview.net/forum? id=0Gmi8TkUC7. Jolicoeur-Martineau, A., Piche-Taillefer, R., Mitliagkas, I., and des Combes, R. T. Adversarial score matching and improved sampling for image generation. In ICLR, 2021. URL https://openreview.net/forum? id=eLfqMl3z3lq. Karras, T., Aittala, M., Aila, T., and Laine, S. Elucidating the design space of diffusion-based generative models. In NeurIPS, 2022. Karras, T., Aittala, M., Kynkaanniemi, T., Lehtinen, J., Aila, T., and Laine, S. Guiding diffusion model with bad version of itself. In NeurIPS, 2024. URL https: //openreview.net/forum?id=bg6fVPVs3s. Kim, D., Lai, C.-H., Liao, W.-H., Murata, N., Takida, Y., Uesaka, T., He, Y., Mitsufuji, Y., and Ermon, S. Consistency trajectory models: Learning probability flow ODE trajectory of diffusion. In ICLR, 2024. URL https: //openreview.net/forum?id=ymjI8feDTD. Knothe, H. Contributions to the theory of convex bodies. Michigan Mathematical Journal, 4(1):3952, 1957. Kong, W., Tian, Q., Zhang, Z., Min, R., Dai, Z., Zhou, J., Xiong, J., Li, X., Wu, B., Zhang, J., Wu, K., Lin, Q., Yuan, J., Long, Y., Wang, A., Wang, A., Li, C., Huang, D., Yang, F., Tan, H., Wang, H., Song, J., Bai, J., Wu, J., Xue, J., Wang, J., Wang, K., Liu, M., Li, P., Li, S., Wang, W., Yu, W., Deng, X., Li, Y., Chen, Y., Cui, Y., Peng, Y., Yu, Z., He, Z., Xu, Z., Zhou, Z., Xu, Z., Tao, Y., Lu, Q., Liu, S., Zhou, D., Wang, H., Yang, Y., Wang, D., Liu, Y., Jiang, J., and Zhong, C. Hunyuanvideo: systematic framework for large video generative models, 2025. URL https://arxiv.org/abs/2412.03603. Kullback, S. and Leibler, R. A. On information and sufficiency. The Annals of Mathematical Statistics, 22(1): 7986, 1951. Kynkaanniemi, T., Karras, T., Laine, S., Lehtinen, J., and Aila, T. Improved precision and recall metric for assessing generative models. In NeurIPS, 2019. Kynkaanniemi, T., Aittala, M., Karras, T., Laine, S., Aila, T., and Lehtinen, J. Applying guidance in limited interval improves sample and distribution quality in diffusion models. In NeurIPS, 2024. Lipman, Y., Chen, R. T. Q., Ben-Hamu, H., Nickel, M., and Le, M. Flow matching for generative modeling. In ICLR, 2023. URL https://openreview.net/forum? id=PqvMRDCJT9t. Liu, X., Gong, C., and Liu, Q. Flow straight and fast: Learning to generate and transfer data with rectified flow. arXiv preprint arXiv:2209.03003, 2022. Loshchilov, I. and Hutter, F. Decoupled weight decay regularization. In ICLR, 2019. Lu, C., Zhou, Y., Bao, F., Chen, J., Li, C., and Zhu, J. DPM-solver: fast ODE solver for diffusion probabilistic model sampling in around 10 steps. In Oh, A. H., Agarwal, A., Belgrave, D., and Cho, K. (eds.), NeurIPS, 2022. URL https://openreview.net/forum? id=2uAaGwlP_V. Lu, C., Zhou, Y., Bao, F., Chen, J., Li, C., and Zhu, J. Dpmsolver++: Fast solver for guided sampling of diffusion probabilistic models, 2023. URL https://arxiv. org/abs/2211.01095. Nichol, A. and Dhariwal, P. Improved denoising diffusion probabilistic models. In ICML, 2021. Peebles, W. and Xie, S. Scalable diffusion models with transformers. In ICCV, 2023. Po, R., Yifan, W., Golyanik, V., Aberman, K., Barron, J. T., Bermano, A. H., Chan, E. R., Dekel, T., Holynski, A., Kanazawa, A., Liu, C. K., Liu, L., Mildenhall, B., Nießner, M., Ommer, B., Theobalt, C., Wonka, P., and Wetzstein, G. State of the art on diffusion models for visual computing. In Eurographics STAR, 2024. Podell, D., English, Z., Lacey, K., Blattmann, A., Dockhorn, T., Muller, J., Penna, J., and Rombach, R. SDXL: Improving latent diffusion models for high-resolution image synthesis. In ICLR, 2024. URL https://openreview. net/forum?id=di52zR8xgf. 11 Gaussian Mixture Flow Matching Models Rezende, D. and Mohamed, S. Variational inference with normalizing flows. In Bach, F. and Blei, D. (eds.), ICML, volume 37 of Proceedings of Machine Learning Research, pp. 15301538, Lille, France, 0709 Jul 2015. PMLR. URL https://proceedings.mlr.press/v37/ rezende15.html. Rombach, R., Blattmann, A., Lorenz, D., Esser, P., and Ommer, B. High-resolution image synthesis with latent diffusion models. In CVPR, 2022. Ronneberger, O., Fischer, P., and Brox, T. U-net: Convolutional networks for biomedical image segmentation. In Medical Image Computing and Computer-Assisted Intervention (MICCAI), pp. 234241, 2015. Rosenblatt, M. Remarks on multivariate transformation. The Annals of Mathematical Statistics, 23(3):470472, 1952. Song, Y., Dhariwal, P., Chen, M., and Sutskever, I. Consistency models. In ICML, 2023. Tschannen, M., Eastwood, C., and Mentzer, F. Givt: Generative infinite-vocabulary transformers. In ECCV, pp. 292309, 2024. Vaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A. N., Kaiser, L. u., and Polosukhin, I. Attention is all you need. In Guyon, I., Luxburg, U. V., Bengio, S., Wallach, H., Fergus, R., Vishwanathan, S., and Garnett, R. (eds.), NeurIPS, volume 30. Curran Associates, Inc., 2017. von Platen, P., Patil, S., Lozhkov, A., Cuenca, P., Lambert, N., Rasul, K., Davaadorj, M., Nair, D., Paul, S., Berman, W., Xu, Y., Liu, S., and Wolf, T. Diffusers: State-of-the-art diffusion models. https://github. com/huggingface/diffusers, 2022. Sadat, S., Hilliges, O., and Weber, R. M. Eliminating oversaturation and artifacts of high guidance scales in diffusion models, 2024. URL https://arxiv.org/ abs/2410.02416. Xiao, Z., Kreis, K., and Vahdat, A. Tackling the generative learning trilemma with denoising diffusion GANs. In ICLR, 2022. URL https://openreview.net/ forum?id=JprM0p-q0Co. Saharia, C., Chan, W., Saxena, S., Li, L., Whang, J., Denton, E. L., Ghasemipour, K., Gontijo Lopes, R., Karagol Ayan, B., Salimans, T., Ho, J., Fleet, D. J., and Norouzi, M. Photorealistic text-to-image diffusion models with deep language understanding. In Koyejo, S., Mohamed, S., Agarwal, A., Belgrave, D., Cho, K., and Oh, A. (eds.), NeurIPS, volume 35, 2022a. Saharia, C., Chan, W., Saxena, S., Li, L., Whang, J., Denton, E. L., Ghasemipour, K., Gontijo Lopes, R., Karagol Ayan, B., Salimans, T., et al. Photorealistic text-to-image difIn fusion models with deep language understanding. NeurIPS, 2022b. Salimans, T. and Ho, J. Progressive distillation for fast sampling of diffusion models. In ICLR, 2022. Sauer, A., Lorenz, D., Blattmann, A., and Rombach, R. Adversarial diffusion distillation. In ECCV, pp. 87103, 2024. Sohl-Dickstein, J., Weiss, E., Maheswaranathan, N., and Ganguli, S. Deep unsupervised learning using nonequilibrium thermodynamics. In ICML, pp. 22562265, 2015. Song, J., Meng, C., and Ermon, S. Denoising diffusion implicit models. In ICLR, 2021a. Song, Y. and Ermon, S. Generative modeling by estimating gradients of the data distribution. In NeurIPS, 2019. Song, Y., Sohl-Dickstein, J., Kingma, D. P., Kumar, A., Ermon, S., and Poole, B. Score-based generative modeling through stochastic differential equations. In ICLR, 2021b. 12 Xue, S., Yi, M., Luo, W., Zhang, S., Sun, J., Li, Z., and Ma, Z.-M. SA-solver: Stochastic adams solver In NeurIPS, for fast sampling of diffusion models. 2023. URL https://openreview.net/forum? id=f6a9XVFYIo. Yang, L., Zhang, Z., Song, Y., Hong, S., Xu, R., Zhao, Y., Zhang, W., Cui, B., and Yang, M.-H. Diffusion models: comprehensive survey of methods and applications. ACM Computing Surveys, 56(4), November 2023. ISSN 0360-0300. doi: 10.1145/3626235. URL https:// doi.org/10.1145/3626235. Yin, T., Gharbi, M., Park, T., Zhang, R., Shechtman, E., Durand, F., and Freeman, W. T. Improved distribution matching distillation for fast image synthesis. In NeurIPS, 2024a. URL https://openreview.net/forum? id=tQukGCDaNT. Yin, T., Gharbi, M., Zhang, R., Shechtman, E., Durand, F., Freeman, W. T., and Park, T. One-step diffusion with distribution matching distillation. In CVPR, 2024b. Zhang, Q. and Chen, Y. Fast sampling of diffusion models with exponential integrator. In ICLR, 2023. URL https: //openreview.net/forum?id=Loek7hfb46P. Zhao, W., Bai, L., Rao, Y., Zhou, J., and Lu, J. Unipc: unified predictor-corrector framework for fast sampling of diffusion models. In NeurIPS, 2023. Gaussian Mixture Flow Matching Models A. Additional Technical Details A.1. Details on Gaussian Surrogates For GM distribution of the form (cid:80)K k=1 AkN (µk, s2I), we approximate it with an isotropic Gaussian surrogate (µ, s2I) by matching the first two moments. The surrogate mean is computed as: µ = (cid:88) k=1 Akµk. (11) To determine the surrogate variance, we equate the trace of the GMs covariance matrix (i.e., the GMs total variance) with the trace of the surrogate covariance, which yields: s2 ="
        },
        {
            "title": "1\nD",
            "content": "K (cid:88) k=1 Akµk µ2 + s2. (12) A.2. Details on Second-Order GM Solvers For GM extrapolation, we follow the method in 3.2 and fit the isotropic Gaussian surrogates (x0; µ+, s2 +I) : qθ(x0xt), (x0; µ, s2 I) : ˆq(x0xt). We then define Gaussian mask: ρ(x0) = (cid:16) x0; µ+ + µ, (cid:16) + µ2 s2 +I) (cid:17) (cid:17) , (13) (x0; µ+, s2 where µ = µ+µ , so that µ+ + µ represents the extrapolated mean at the next midpoint. The final extrapolated GM is obtained via the reweighting formulation: 2 qext(x0xt) = ρ(x0) qθ(x0xt). (14) We then feed qext(x0xt) to the first-order GM-SDE or ODE solver as substitution for qθ(x0xt). Empirically, we observe that second-order GM solvers using the above formulation slightly underperform with higher guidance scales, which is common issue with multistep solvers (Lu et al., 2023; Zhao et al., 2023). To address this, we rescale the mean difference µ by an empirical max(0, 1 ( w2+ca)s2 ) (with the hyperparameter factor ca = 0.005), so that high practically disables multistep extrapolation. c (cid:113) A.3. Details on Spectral Sampling Due to pixel-wise factorization, image generation under GM-SDE solvers performs the sampling step ˆx0 qθ(x0xt) independently for each pixel, neglecting spatial correlations. Spectral sampling addresses this by establishing an invertible mapping between frequency-space Algorithm 1: Complete GMFlow training scheme. Input: Data distribution p(x0, c), transition ratio λ Output: Network params θ 1 Initialize network params θ 2 for sample {x0, c} p(x0, c) do if use logit-normal then 3 5 6 7 8 9 11 12 13 14 Sample LogitNormal (0, 1) else Sample (0, 1) λt Sample xtt p(xttx0) Sample xt p(xtxtt) Predict GM params in qθ(xttxt, c) Ltrans log qθ(xttxt, c) Predict magnitude spectrum sF Lspec log (cid:0)vec(zr); 0, diag(sF)2(cid:1) Backpropagate and update θ // Eq. (9) // Eq. (20) Algorithm 2: Complete GMFlow sampling scheme. Input: Steps NFE , sub-steps n, guidance scale w, condition c, network params θ Output: x0 1 1, = 1 2 while > 0 do 3 NFE , x1 (0, I), Cache {} Predict GM params in qθ(x0xt, c) if > 0 then Predict GM params in qθ(x0xt) Compute qw(x0xt, c) qθ(x0xt, c) if use 2nd-order then param qw(x0xt, c) if Cache = {} then // Eq. (7) Compute ˆq(x0xt, c) from Cache // Eq. (10) Cache {xt, qθ(x0xt, c)} Compute qext(x0xt, c) qθ(x0xt, c) param qext(x0xt, c) // Eq. (14) else Cache {xt, qθ(x0xt, c)} if use GM-SDE then if use spectral sampling then Predict magnitude spectrum sF Sample ˆx0 qS θ(x0xt, c) else Sample ˆx0 qθ(x0xt, c) // Eq. (19) Sample xtt (xtt; c1xt + c2 ˆx0, c3I) else if use GM-ODE then τ t, = t/n while τ > do Compute ˆq(x0xτ , c) xτ = xτ hEx0ˆq(x0xτ ,c)[ xτ x0 τ τ τ // Eq. (10) ] t 4 5 6 7 9 10 11 12 13 15 16 17 18 19 21 22 23 24 25 27 28 29 distribution (power spectrum) and the pixel-space GM distribution. During training, the power spectrum is optimized via 13 Gaussian Mixture Flow Matching Models Figure 9. The optional spectral sampling pipeline used during inference. The spectrum MLP takes the statistics of pixel-wise GMs as input, and predicts the power spectrum s2 . Alternatively, if probabilistic guidance or second-order GM solvers are employed, one can re-use the mean and variance from the numerator in the Gaussian mask to compute the input statistics, which serves as good approximation. likelihood loss, while during inference, frequency-space samples are transformed back into pixel space to introduce spatial correlation. We establish an invertible mapping using two building blocks: the fast Fourier transform (FFT) and Knothe Rosenblatt (KR) transport (Knothe, 1957; Rosenblatt, 1952). The FFT bridges frequency-space Gaussian distribution and pixel-space Gaussian distribution, while KR transport bridges each per-pixel Gaussian to its corresponding Gaussian mixture (GM) distribution. During training, given real image x0 RCHW and the factorized denoising distribution qθ(x0xt) = (cid:81)H,W k=1 Ak,ijN (x0,ijµk,ij, s2I), KR transport i=1,j=1 (cid:80)K defines an invertible mapping for each pixel: Tij : x0,ij (cid:55) ζij, (15) such that each ζij can be viewed as standard Gaussian sample. We then assemble all ζij into tensor ζ RCHW and apply forward 2D FFT with orthogonal normalization, yielding the complex frequency representation = FFT (ζ) CCHW . Since is Hermitian symmetric, we can derive real-valued representation zr while preserving invertibility: zr = Re[z] + Im[z]. (16) Finally, we impose zero-mean Gaussian prior on zr, given by (cid:0)vec(zr); 0, diag(sF)2(cid:1), where sF RD + represents the magnitude spectrum. To dynamically model sF RD + , we use tiny two-layer MLP, which takes the mean of perpixel GM variances and the variance of per-pixel GM means as input, and outputs the power spectrum s2 with softmax activation, where ()2 stands for element-wise square. With this invertible mapping and the spectral Gaussian prior, we can derive the models spectrum-enhanced denoising PDF using the change of variables technique in similar way to normalizing flow models (Rezende & Mohamed, 2015), (cid:19)(cid:12) (cid:12) (cid:12) (cid:12) which can be expressed as: (cid:12) (cid:12) (cid:12) (cid:12) (cid:18) vec(zr) x0 det qS θ(x0xt) = (cid:0)vec(zr); 0, diag(sF)2(cid:1), (17) where the absolute determinant can be easily derived using the properties of FFT and KR transport: (cid:12) (cid:19)(cid:12) (cid:12) (cid:12) (cid:12) (cid:12) (cid:12) (cid:12) (cid:18) vec(zr) x0 (cid:18) vec(ζ) x0 (cid:19)(cid:12) (cid:12) (cid:12) (cid:12) det det det (cid:12) (cid:12) (cid:12) (cid:12) (cid:12) (cid:12) (cid:12) (cid:12) = (cid:19)(cid:12) (cid:12) (cid:12) (cid:12) (cid:18) vec(zr) vec(ζ) qθ(x0xt) (vec(ζ); 0, I) qθ(x0xt) (vec(zr); 0, I) . = 1 = (18) Substituting Eq. (18) into Eq. (17), we obtain: qS θ(x0xt) = qθ(x0xt) (cid:0)vec(zr); 0, diag(sF)2(cid:1) (vec(zr); 0, I) . (19) With the derived PDF, the entire model can be trained by minimizing the negative log-likelihood (NLL) of data samples under the PDF, which inherently includes the GM KL loss (Eq. (5) and (6)) as one of its terms. Therefore, the total loss consists of the GM KL loss (replaced with the transition loss in practice) and an additional spectral loss, defined as: (cid:34) Lspec = Et,x0,xt log (cid:0)vec(zr); 0, diag(sF)2(cid:1) (vec(zr); 0, I) (cid:35) = Et,x0,xt (cid:20) 1 2 (cid:0)diag(sF)1 I(cid:1)vec(zr)(cid:13) (cid:13) 2 (cid:13) (cid:13) + log det(diag(sF)) (cid:21) . (20) In practice, we stop the gradient flow through KR transport to prevent spectral learning from influencing the main GMFlow model. During inference, we employ the spectral sampling pipeline illustrated in Fig. 9. Gaussian Mixture Flow Matching Models A.4. ImageNet Experiment Details. We train both the baseline and GMFlow-DiT on ImageNet 256256 with batch size of 4096 images across 16 A100 GPUs, using total training schedule of 200K iterations. We adopt the 8-bit AdamW (Dettmers et al., 2022; Loshchilov & Hutter, 2019) optimizer with fixed learning rate of 0.0002. Following Stable Diffusion 3 (Esser et al., 2024), both models sample from logit-normal distribution during training (Algorithm 1), which accelerates convergence. While we set the transition ratio to λ = 0.5 in the main experiments, the results in Fig. 7 and Table 3 are based on an earlier design iteration that uses randomly sampled transition ratios λ LogitNormal (0, 1). This setting slightly increases Precision and decreases Recall, though the overall differences remain minor. We densely evaluate the models across different guidance scales to identify the optimal FID and Precision. For vanilla CFG, we use guidance scales from the set {1.2, 1.3, 1.4, 1.5, 1.6, 1.7, 1.8, 1.9, 2.1, 2.3, 2.6, 2.9, 3.3, 3.7, 4.3, 4.9, 5.7, 6.5}; for probabilistic guidance, we use probabilistic guidance scales from the set {0.02, 0.03, 0.04, 0.05, 0.06, 0.07, 0.08, 0.09, 0.11, 0.13, 0.16, 0.19, 0.23, 0.27, 0.33, 0.39, 0.47, 0.55, 0.65, 0.75}. In practice (Algorithm 2), we implement probabilistic guidance on x0-based GMs, which is equivalent to guidance on u. For inference with GM-ODE solvers, we generally set the number of sub-steps to = max(cid:0) 128 NFE , 2(cid:1), which performs well when NFE 8. For the exception when NFE = 4 or 6, we observe that reducing to 8 yields better performance. In Table 3, the time-averaged NLL values are computed on 50K samples from the training dataset using the following equation: Figure 10. Qualitative comparison (at best Precision) from the ablation study on GM-ODE sub-steps. Without sub-steps (forcing = 1), few-step sampling tends to produce over-smoothed textures and poor detail. For DPM++ (Lu et al., 2023), DEIS (Zhang & Chen, 2023), UniPC (Zhao et al., 2023), and SA-Solver (Xue et al., 2023), we use their Diffusers implementations (von Platen et al., 2022) and rescale their noise schedules to match the flow matching noise schedule. This approach is similar to how the EDM Euler solver (Karras et al., 2022) rescales variance-preserving diffusion model into variance-exploding one at test time. NLL = 1 Et,x0,xt[ log2 qθ(x0xt)], (21) B. Additional Experiment Results where (0, 1) and = . This is equivalent to the original training loss (λ = 1) with uniform time sampling scheme. When spectral prior is enabled, we replace qθ(x0xt) with qS θ(x0xt) (Eq. (19)). A.5. Adapting Diffusion Solvers for Flow Matching For DDPM solvers (Ho et al., 2020), we implement them as special cases of GM-SDE solver with = 1. The original DDPM solvers include large variance variant (β) and small variance variant ( β). These are equivalent to setting = and = 0, respectively. 1 α2 +σ2 For DDIM solvers (Song et al., 2021a), its stochastic variant (η = 1) is equivalent to DDPM with small variance, whereas its deterministic variant (η = 0) is equivalent to Euler solver in flow matching models. 15 B.1. Ablation Study on GM-ODE Sub-Steps In addition to the results in Table 4 (B0 and B1), Fig. 10 presents additional qualitative results from the ablation study on GM-ODE sub-steps, using = 8 and the GM-ODE 2 solver. As shown in the figure, sub-steps are essential for producing detailed textures when NFE < 8. B.2. Additional Qualitative Comparison Fig. 11 presents comparison among uncurated random samples (conditioned on random class labels) from GMFlow and vanilla flow matching baselines, showcasing their respective best results under the many-step setting. Overall, the images generated by GMFlow exhibit more natural color saturation and, in some cases, improved structural coherence. These observations are consistent with the quantitative results shown in Fig. 6 and Table 2. Gaussian Mixture Flow Matching Models Table 5. FIDs on CIFAR-10 unconditional image generation. Competitor results are sourced from Guo et al. (2023). NFE 10 25 50 100 DDPM large DDPM small SN-DDPM GMS GM-SDE 2 205.31 34.76 16.33 13.80 9. 84.71 16.18 6.05 5.48 4.16 37.35 11.11 4.19 4.00 3.79 14.81 8.38 3.83 3.46 3.76 B.3. Comparison with Moment Matching Methods Table 5 presents quantitative comparison among GMFlow (K = 2), GMS (Guo et al., 2023), SN-DDPM (Bao et al., 2022a), and DDPM (Ho et al., 2020) for CIFAR-10 (Krizhevsky et al., 2009) unconditional image generation using SDE sampling. The GMFlow model is trained from scratch using the same U-Net architecture (Ronneberger et al., 2015; Ho et al., 2020) as its competitors, but with modified output layers as in GM-DiT. The results demonstrate that GMFlow significantly outperforms its competitors in few-step sampling. C. Additional Theoretical Analysis C.1. Details on the SDE and ODE Background For notation references, this subsection briefly recaps the SDE and ODE formulation of diffusion models by Song et al. (2021b). The forward-time SDE of the diffusion process can be derived by taking an infinitesimal step size when applying the transition Gaussian kernel in Eq. (1), which yields: dx = ftxt dt + gt dwt (22) (cid:12) where ft = 1 (cid:12)t=0, and wt is the αt standard Wiener process. The corresponding reverse-time SDE is: dαt dt , gt = (cid:113) dβt,t dt dxt = (cid:0)ftxt g2 st(xt)(cid:1) dt + gt wt, (23) (cid:105) (cid:104) αtx0xt σ2 := xt log p(xt) = with the score function st(xt) Ex0p(x0xt) and the reverse-time standard Wiener process w. Using the FokkerPlanck equation, we can prove that the time evolution of the PDF p(xt) described by the SDEs is equivalent to that described by an ODE: positive definite matrices. We aim to show that if {a k, µ k} = arg min {ak,µk} (cid:34) Eup(u) log (cid:88) k=1 AkN (u; µk, Σk) (cid:35) , (25) , then the mean with ak R, µk RD, Ak = exp ak alignment property holds: (cid:80)K k=1 exp ak (cid:88) k=1 kµ = Eup(u)[u], with = exp k k=1 exp (cid:80)K . For brevity, we define kN (u; µ qk(u) := (cid:88) q(u) := qk(u). k, Σk), (26) (27) (28) k= Since {a k, µ mality conditions imply k} minimizes the objective, the first-order opti- = Eup(u) = Eup(u) (cid:21) Eup(u)[ log q(u)] (cid:20) log q(u) qk(u) q(u) (cid:20) qk(u) q(u) Eup(u) (cid:20) (cid:21) (cid:21) = = 0, (29) and Eup(u)[ log q(u)] µ (cid:20) log q(u) µ = Eup(u) (cid:21) = Eup(u) (cid:20) qk(u) q(u) = Σ 1 2 (cid:18) µ Eup(u) Σ 1 2 (µ (cid:20) qk(u) q(u) (cid:21) u) (cid:21) Eup(u) (cid:21)(cid:19) (cid:20) qk(u) q(u) (cid:18) dxt = ftxt (cid:19) g2 st(xt) 1 dt, (24) which maps the noise xt to deterministic data point x0. = 0. From the above, it follows that C.2. Proof of Theorem 3. Proof. Let p(u) be the PDF of an arbitrary distribution on RD, and {Σk}K k=1 be set of arbitrary symmetric = Eup(u) (cid:21) = Eup(u) µ Eup(u) (cid:20) qk(u) q(u) 16 (30) (31) (32) (cid:20) qk(u) q(u) (cid:20) qk(u) q(u) (cid:21) , (cid:21) . Gaussian Mixture Flow Matching Models Substituting Eq. (31) into Eq. (32), we have kµ = Eup(u) (cid:20) qk(u) q(u) (cid:21) . (33) Summing over all k, we conclude that (cid:88) k=1 kµ = Eup(u) (cid:34) (cid:80)K k=1 qk(u) q(u) (cid:35) = Eup(u)[u]. (34) C.3. Derivation of the Reverse Transition Distribution Given the ground truth data distribution p(x0) and the forward diffusion Gaussian p(xtx0) = (xt; αtx0, σ2 I), the ground truth denoising distribution p(x0xt) can be derived using Bayes theorem: p(x0xt) = p(x0)p(xtx0) p(xt) . (35) which is convolution between p(xttxt, x0) and p(x0xt). Sampling from p(xttxt) can therefore be simulated by first sampling x0 p(x0xt) and then sampling xtt p(xttxt, x0). In general, given any empirical denoising distribution qθ(x0xt), we can perform stochastic denoising sampling by substituting p(x0xt) qθ(x0xt) into the above sampling process. Specifically for GMFlow, qθ(x0xt) is GM PDF. D.3 shows that the convolution of GM and Gaussian is also GM with analytically derived parameters. Therefore, GMFlow models the reverse transition distribution as GM qθ(xttxt), given by Eq. (9). C.4. Additional Analysis of the Reverse Transition GM By computing the first-order Taylor approximation of Eq. (9) w.r.t. t, we can re-write the GM reverse transition distribution as: Conversely, let us assume that we know the ground truth denoising distribution p(x0xt). By rearranging Eq. (35), we can derive the data distribution as: = qθ(xttxt) (cid:88) AkN (cid:0)xtt; xt (cid:0)ftxt g2 µsk (cid:1)t, g2 tI(cid:1) p(x0) = p(xt)p(x0xt) p(xtx0) . (36) With the data distribution, we can apply the forward diffusion process and derive the noisy data distribution at t: p(xtt) = = (cid:90) RD (cid:90) RD p(xttx0)p(x0) dx0 p(xttx0) p(xt)p(x0xt) p(xtx0) dx0. (37) Finally, the reverse transition distribution p(xttxt) can be derived using Bayes theorem: p(xttxt) p(xtt)p(xtxtt) p(xt) (cid:90) RD p(xtxtt)p(xttx0) p(xtx0) p(x0xt) dx0, (38) = = where p(xtxtt) is the forward transition Gaussian defined in Eq. (1). The term p(xtxtt)p(xttx0) can be fused into one Gaussian PDF using the conflation operation described in D.1, which yields: p(xtx0) p(xttxt, x0) = (xtt; c1xt + c2x0, c3I), (39) with the coefficients c1, c2, c3 defined in 3.3. Therefore, Eq. (38) can be simplified into: p(xttxt) = (cid:90) RD p(xttxt, x0)p(x0xt) dx0, (40) 17 k=1 + O(t) (cid:32) (cid:32) = xtt; xt ftxt g2 + O(t), (cid:33) (cid:33) Akµsk t, g2 tI (cid:88) k=1 (41) (xt + αtµk). The term (cid:80)K where µsk := 1 k=1 Akµsk σt represents the models prediction of the score function st(xt), and is linear transformation of the predicted mean velocity (cid:80)K k=1 Akµk. Recursively sampling xtt using the first-order approximation in Eq. (41) is equivalent to solving the reverse-time SDE in Eq. (23) (with predicted score) using the EulerMaruyama method, simple firstorder SDE solver. Eq. (41) is consistent with the standard diffusion model assumption that the reverse transition distribution is approximately Gaussian for small t. Even with GM parameterization, SDE sampling is primarily influenced by the mean prediction when is sufficiently small. This underscores the significance of mean alignment not only for ODE solving but also for SDE solving. C.5. Derivation of ˆq(x0xτ ) Let us assume that we know the ground truth denoising distribution at xt, i.e., p(x0xt). Eq. (36) shows us how to derive the data distribution p(x0) from p(x0xt). From p(x0), the denoising distribution at xτ can be derived using Bayes theorem: D.3. Convolution of Gaussian and GM Gaussian Mixture Flow Matching Models Let p(x1x2) = (x1; µ + cx2, Σ) be conditional Gaussian PDF, where is linear coefficient, and p(x2) = (cid:80)K k=1 AkN (x2; µk, Σk) be GM PDF, where exp ak with logit ak. Their convolution yields Ak = k=1 exp ak the marginal PDF of x1: (cid:80)K p(x1) = (cid:90) RD p(x1x2)p(x2) dx2. (51) Since the GM PDF is sum of Gaussians, the convolution of Gaussian and GM expands to sum of convolution of Gaussians, which simplifies to sum of Gaussians. Therefore p(x1) can also be expressed as GM (cid:80)K k), with the new parameters: k=1 AkN (x; µ k, Σ Σ µ = Σ + c2Σk, = µ + cµk. (52) (53) p(x0xτ ) = p(x0)p(xτ x0) p(xτ ) = = p(xt)p(x0xt)p(xτ x0) p(xtx0)p(xτ ) p(xτ x0) p(xtx0) p(x0xt), (42) where = p(xτ ) p(xt) is normalization factor. Substituting p(x0xt) qθ(x0xt) into the above equation yields the denoising distribution conversion rule in Eq. (10). The conversion involves conflations of Gaussians and GM, which can be approached analytically as shown in D. D. Gaussian Mixture Math References D.1. Conflation of Two Gaussians Let p1(x) = (x; µ1, Σ1), p2(x) = (x; µ2, Σ2) be two multivariate Gaussian PDFs, their powered conflation is defined as: p(x) = 1 (x)pγ2 pγ1 2 (x) , (43) where γ1, γ2 R, assuming γ1Σ1 1 + γ2Σ1 is positive 2 definite, and = (cid:82) 1 (x)pγ2 2 (x) dx is normalization factor. Its easy to prove that, p(x) can also be expressed as Gaussian (x; µ, Σ), with the new parameters: RD pγ Σ = (cid:0)γ1Σ1 µ = Σ(γ1Σ1 1 + γ2Σ1 , 2 1 µ1 + γ2Σ1 (cid:1)1 2 µ2). (44) (45) D.2. Conflation of Gaussian and GM Let p1(x) = (x; µ, Σ) be Gaussian PDF, and p2(x) = (cid:80)K k=1 AkN (x; µk, Σk) be GM PDF, where Ak = exp ak k=1 exp ak with logit ak. Their conflation is defined as: (cid:80)K p(x) = p1(x)p2(x) , (46) where = (cid:82) RD p1(x)p2(x) dx is normalization factor. Since the GM PDF is sum of Gaussians, the conflation of Gaussian and GM expands to sum of conflations of Gaussians, which simplifies to sum of Gaussians. Therefore, p(x) can also be expressed as GM (cid:80)K k), with the new parameters: k=1 kN (x; µ Σ µ k, Σ = (Σ + Σk)1, = Σ = µk), k(Σ1µ + Σ1 exp k=1 exp k is given by: (cid:80)K , where the new logit (47) (48) (49) = ak 1 2 (µ µk)T(Σ + Σk)(µ µk). (50) 18 Gaussian Mixture Flow Matching Models E. Notation Notation Description Table 6. summary of frequently used notations. Data dimension. Diffusion time. Flow models define := 1. Noise schedule coefficient. Flow models define αt := 1 t. Noise schedule coefficient. Flow models define σt := t. Standard Gaussian noise. Data. Noisy data. PDF of data (ground truth). PDF of noisy data (ground truth). PDF of forward diffusion distribution. x0, βt,tI) PDF of forward transition distribution. σ2 Variance of forward transition distribution. [0, ] RD := αtx + σtϵ αtt I) = (xt; αtx0, σ2 = (xt; αt α2 = σ2 = p(x0)p(xtx0) p(xt) = p(xtt)p(xtxtt) p(xt) tt tt α2 αt σt ϵ x, x0 xt p(x), p(x0) p(xt) p(xtx0) p(xtxtt) βt,t p(x0xt) p(xttxt) θ qθ(x0xt) qθ(xttxt) Ex0p(x0xt)[u] p(uxt) qθ(uxt) µθ(xt) Ak µk Σk ak µxk sx w ˆx NFE τ ˆq(x0xτ ) ˆq(x0xt) λ := xtx σt := exp ak (cid:80)K k=1 exp ak . := xt σtµk := σts [1, +) [0, 1) (0, 1] PDF of reverse denoising distribution (ground truth). PDF of reverse transition distribution (ground truth). Neural network parameters. PDF of reverse denoising distribution, predicted by network. PDF of reverse transition distribution, predicted by network. Random flow velocity. Mean flow velocity at xt (ground truth). PDF of velocity distribution at xt (ground truth), derived from p(x0xt). PDF of velocity distribution at xt, predicted by network. Mean flow velocity at xt, predicted by network. Number of Gaussian components. Index of Gaussian components. Mixture weight of the k-th Gaussian component. Mean of the k-th Gaussian component. Covariance of the k-th Gaussian component. GMFlow defines Σk := s2I. Pre-activation logit. Shared standard deviation. Mean of the k-th Gaussian component after u-to-x0 conversion. Shared standard deviation after u-to-x0 conversion. Condition. CFG scale. Probabilistic guidance scale. Intermediate sample of denoised data. Number of function (network) evaluations, a.k.a. sampling steps. Number of sub-steps in GM-ODE solvers. Sub-step diffusion time in GM-ODE solvers. PDF of reverse denoising distribution, derived from qθ(x0xτ ) (Eq. (10)). PDF of reverse denoising distribution, derived from qθ(x0xt+t). Transition ratio. 19 Gaussian Mixture Flow Matching Models Figure 11. Uncurated samples (at best Precision, NFE = 32) from GMFlow and vanilla flow matching baselines using different solvers. The images generated by GMFlow exhibit more natural color saturation and, in some cases, improved structural coherence."
        }
    ],
    "affiliations": [
        "Adobe Research, CA 95110, USA",
        "Hillbot",
        "Stanford University, CA 94305, USA"
    ]
}