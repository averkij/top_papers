{
    "paper_title": "Dynadiff: Single-stage Decoding of Images from Continuously Evolving fMRI",
    "authors": [
        "Marlène Careil",
        "Yohann Benchetrit",
        "Jean-Rémi King"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Brain-to-image decoding has been recently propelled by the progress in generative AI models and the availability of large ultra-high field functional Magnetic Resonance Imaging (fMRI). However, current approaches depend on complicated multi-stage pipelines and preprocessing steps that typically collapse the temporal dimension of brain recordings, thereby limiting time-resolved brain decoders. Here, we introduce Dynadiff (Dynamic Neural Activity Diffusion for Image Reconstruction), a new single-stage diffusion model designed for reconstructing images from dynamically evolving fMRI recordings. Our approach offers three main contributions. First, Dynadiff simplifies training as compared to existing approaches. Second, our model outperforms state-of-the-art models on time-resolved fMRI signals, especially on high-level semantic image reconstruction metrics, while remaining competitive on preprocessed fMRI data that collapse time. Third, this approach allows a precise characterization of the evolution of image representations in brain activity. Overall, this work lays the foundation for time-resolved brain-to-image decoding."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 0 2 ] . [ 1 6 5 5 4 1 . 5 0 5 2 : r Dynadiff: Single-stage Decoding of Images from Continuously Evolving fMRI Marlène Careil1,, Yohann Benchetrit1,, Jean-Rémi King1 1FAIR at Meta Joint first author Brain-to-image decoding has been recently propelled by the progress in generative AI models and the availability of large ultra-high field functional Magnetic Resonance Imaging (fMRI). However, current approaches depend on complicated multi-stage pipelines and preprocessing steps that typically collapse the temporal dimension of brain recordings, thereby limiting time-resolved brain decoders. Here, we introduce Dynadiff (Dynamic Neural Activity Diffusion for Image Reconstruction), new singlestage diffusion model designed for reconstructing images from dynamically evolving fMRI recordings. Our approach offers three main contributions. First, Dynadiff simplifies training as compared to existing approaches. Second, our model outperforms state-of-the-art models on time-resolved fMRI signals, especially on high-level semantic image reconstruction metrics, while remaining competitive on preprocessed fMRI data that collapse time. Third, this approach allows precise characterization of the evolution of image representations in brain activity. Overall, this work lays the foundation for time-resolved brain-to-image decoding. Date: May 21, 2025 Correspondence: marlenec@meta.com, ybenchetrit@meta.com, jeanremi@meta.com Code: https://github.com/facebookresearch/dynadiff"
        },
        {
            "title": "1 Introduction",
            "content": "Reconstructing images from fMRI. The reconstruction of visual perception from brain activity, first started in the early 2000s (Haxby et al., 2001; Carlson et al., 2003; Kamitani and Tong, 2005; Miyawaki et al., 2008), has substantially improved within the past two years (Takagi and Nishimoto, 2023; Ozcelik and VanRullen, 2023; Scotti et al., 2024; Wang et al., 2024b; Benchetrit et al., 2024; Le et al., 2025; Chen et al., 2023b). This recent progress stems from two key factors: the availability of large-scale neuroimaging data in response to natural images (Allen et al., 2022; Hebart et al., 2019), and the emergence of powerful image-generation models (Rombach et al., 2022; Xu et al., 2023; Podell et al., 2023). One dataset has catalyzed brain-to-image decoding: the Natural Scenes Dataset (NSD) (Allen et al., 2022) is the largest dataset of brain responses to natural images, and consists of subjects watching 70K images over 40 sessions, while their brain activity is recorded with ultra-high field (7T) functional Magnetic Resonance Imaging (fMRI). Typically, brain-to-image decoding consists of multiple steps. First, the images successively seen by the participants are embedded into pretrained computer vision models. Second, deep neural network is trained to transform the brain responses into these image representations. Third, the predicted representations are used to condition pretrained image-generation model. Finally, several studies generate multiple images, and use an ad-hoc scoring to select the best image (Kneeland et al., 2023; Scotti et al., 2023; Wang et al., 2024b). This approach, first based on principal components analyses (Cowen et al., 2014), auto-encoders (Han et al., 2019) and generative adversarial networks (Seeliger et al., 2018; Shen et al., 2019; Gu et al., 2022; Ozcelik et al., 2022) now primarily relies on diffusion models (Takagi and Nishimoto, 2023; Ozcelik and VanRullen, 2023; Scotti et al., 2024; Wang et al., 2024b; Benchetrit et al., 2024; Le et al., 2025). 1 Figure 1 Schematic birds-eye view of four seminal fMRI-to-image architectures: Brain-Diffusers (Ozcelik and VanRullen, 2023), MindEye1 (Scotti et al., 2023), WAVE (Wang et al., 2024b), MindEye2 (Scotti et al., 2024). They all consist of multiple independent training modules, and cant be trained in single stage. Except WAVE, they use preprocessing of fMRI data which collapses time. We illustrate the simplicity and time-resolved capability of our approach, Dynadiff, trained in single stage on timeseries of fMRI activity, in comparison to these pipelines. Challenge 1: time-collapsing fMRI preprocessing. The best brain-to-image reconstructions to date have been achieved on the Natural Scenes Dataset by decoding time-collapsed fMRI beta values. These are derived by fitting generalized linear model (GLM) across time to isolate the brains response to each image, and entail multiple challenges. First, the type of beta values used in prior state-of-the-art studies completely discards the time dimension of fMRI data. Second, most studies average these beta values across multiple presentations of the same image. As result, this preprocessing of fMRI data severely restricts the ability to perform time-resolved image decoding. Challenge 2: complex multi-stage decoding pipelines. The complexity of decoding pipelines has substantially increased in the past two years. State-of-the-art models now consist of up to four independent stages including pretrained fMRI encoders (Chen et al., 2023b; Wang et al., 2024b; Huo et al., 2024), contrastive learning (Chen et al., 2023b; Wang et al., 2024b; Scotti et al., 2024; Xia et al., 2024a), diffusion priors (Scotti et al., 2024, 2023; Wang et al., 2024b), automatic image captioning (Ferrante et al., 2023; Scotti et al., 2024), control nets (Ferrante et al., 2023; Huo et al., 2024) and post-candidate selection (Kneeland et al., 2023; Scotti et al., 2023). Many of these steps optimize, either separately or jointly, variety of losses, supported by advanced data augmentation techniques (Scotti et al., 2024; Wang et al., 2024b). Figure 1 illustrates four seminal pipelines to highlight both the high complexity of modern brain-to-image decoders, and the increase of this complexity over the years (Ozcelik and VanRullen, 2023; Scotti et al., 2023, 2024; Wang et al., 2024b). For example, the state-of-the-art pipeline MindEye2 (Scotti et al., 2024) requires pretraining custom image generation model (SDXL-UnCLIP), captioning its outputs and refining reconstructions using SDXL. Even the lower-performing but arguably simple Brain-Diffuser (Ozcelik and VanRullen, 2023), which only uses ridge regression models, also requires two independent training / inference stages, for lowand high-level image reconstruction respectively. Overall, the simplification of the many manual feature engineering steps, often promised by deep learning approaches, seems here to fall short. Our main contributions. We introduce Dynadiff, pipeline to reconstruct images from dynamically evolving fMRI signals. First, it uses only single-stage of training and inference, contrasting with the complexity of previous approaches. Second, it outperforms state-of-the-art models on fMRI time-series from the Natural Scenes Dataset. Third, the dynamic nature of our approach enables an accurate description of how image representations change in brain activity over time."
        },
        {
            "title": "2.1 Problem statement",
            "content": "Our goal is to reconstruct images from continuously evolving BOLD fMRI signals recorded while participants watched natural images. Let (s, t, d) denote the time-window of seconds starting seconds after the TR (see Section 2.3), this onset of an image stimulus s. Since fMRI volumes are acquired at frequency = 1 time window corresponds to time-series IRCT of volumes of fMRI voxels each (where typically varies across participants due to anatomical differences). Given fixed and d, we aim to reconstruct given X. To tackle this task, we propose Dynadiff, which directly fine-tunes pretrained image-generation diffusion model with the fMRI signals, as illustrated in Figure 1. Specifically, we design brain module that projects to conditioning embedding of the diffusion model. This brain module is jointly trained with the diffusion model to learn to reconstruct realistic and consistent images. We give more details about this brain module, explain how we adapt the pretrained diffusion model and the joint training in Section 2.2."
        },
        {
            "title": "2.2 Dynadiff\nBrain Module. The brain module projects fMRI data X to the conditioning space of the image-generation\nmodel and is shown in Figure 2. It consists of a subject-specific linear layer S : IRC → IR1552 (Défossez et al.,\n2022) that projects each fMRI volume (of C voxels) to 1,552 channels, while keeping the same number T of\nfMRI time samples. This layer outputs a vector Z ∈ IR1552×T .",
            "content": "Then, is passed to timestep-specific linear layer, that applies distinct set of weights to each time sample. This is followed by layer normalization, GELU activation and dropout (p = 0.5). Next, linear temporal aggregation layer merges the temporal dimension. Finally, an additional linear layer outputs fMRI embeddings with the same shape as the conditional embedding of the image-generation model: 257 patches and 768 channels. Our brain module has around 400M parameters. Brain-conditioned diffusion model. For simplicity, we use the same pretrained latent diffusion model as in Ozcelik and VanRullen (2023); Scotti et al. (2023). This conditional image-generation model is based on U-Net architecture (Ronneberger et al., 2015) and was trained to synthesize images conditioned on texts and images. These two prompts are first projected to token embeddings using the text and image encoders from CLIP (Radford et al., 2021). Then, these embeddings are processed through cross-attention layers, which are present at different feature-map scales of the U-Net. To condition the diffusion model on fMRI data, we replace image embeddings with the output from the brain module, and provide null text embeddings. This approach enables us to leverage the pretrained generative models ability to synthesize high-quality images. Figure 2 The architecture of our brain module, corresponding to the only MLP block of our approach (Figure 1). Single-stage training. We jointly train the brain module and the brain-conditioned diffusion model to reconstruct seen images from fMRI data. The brain module and LoRA adapters (Hu et al., 2021) for the diffusion models cross-attention layers are trained from scratch while the generation model weights are left untouched ( 25M parameters). We use the standard diffusion loss to optimize the model weights. Additionally, we use (1) bicubic sampling (Mou et al., 2024), which involves more frequent sampling of early timesteps during training and (2) an offset noise. Finally, to enable classifier-free guidance at inference time, we remove the brain-conditioning in 10% of training iterations, and replace it with constant learned embedding instead. Further details on training optimization can be found in Appendix B. Inference. We reconstruct an image from time-series of fMRI volumes as follows. First, we apply We empirically observed that replacing text embeddings didnt boost performance. https://www.crosslabs.org/blog/diffusion-with-offset-noise 3 the brain module on to obtain fMRI embeddings Z. Then, we sample an initial random gaussian noise ϵ, and provide both and ϵ to the diffusion models U-Net to start the denoising process; we use DDIM scheduler with 20 denoising steps and classifier-free-guidance scale of 3. This process yields denoised latent embedding, which is then passed to the diffusion models autoencoder to produce reconstruction of I. To ensure reproducibility, we make our models inference code publicly available. ."
        },
        {
            "title": "2.3 Experimental setting\nDataset. We here use the Natural Scenes Dataset (Allen et al., 2022). Eight healthy volunteers participated\nin this dataset, six females and two males, with a mean age range of 19 to 32 years. Each volunteer underwent\n30 to 40 fMRI sessions, each lasting approximately one hour. Consistent with prior research (Ozcelik and\nVanRullen, 2023; Scotti et al., 2023, 2024), we focus on the data from subjects who completed all 40 recording\nsessions, i.e., subjects 1, 2, 5, and 7. Each participant viewed 10,000 unique images from the MS-COCO\ndataset (Lin et al., 2014), and each unique image was presented three times over the 40 sessions. Of these\nimages, 9,000 are used for training, while a shared set of 1,000 images, viewed by all subjects, is reserved for\ntesting. Each image is displayed for 3 s, followed by a 1 s blank interval before the next image presentation.\nTo maintain the time-resolved compatibility of our approach, we don’t average the repetitions to the same\nimages, neither during training nor testing. This results in a training dataset of 9000 x 3 = 27,000 trials and\na test dataset of 1000 x 3 = 3,000 trials for each subject. We evaluate metrics on the set of 1,000 unique test\nimages, by randomly selecting one test presentation out of three, for each unique test image.",
            "content": "fMRI preprocessing. We use the standard-resolution timeseries of BOLD fMRI volumes provided by the NSD authors (TR=1.3 s, 1.8 mm isotropic resolution) (Allen et al., 2022). As described in Allen et al. (2022), these data are computed from raw functional timeseries by applying (i) temporal upsampling, which corrects slice-time differences, and (ii) spatial resampling, which corrects for head motion, EPI distortion, gradient nonlinearities, and scan session alignment. Compared to the computation of averaged beta values used in previous studies (Scotti et al., 2023, 2024; Huo et al., 2024), this preprocessing does not collapse the time domain. It can be reproduced from scripts from the NSD GitHub repository , and deliberately leaves out high-pass filtering, nuisance regression, to avoid unnecessary underlying assumptions. Following previous works (Ozcelik and VanRullen, 2023; Scotti et al., 2023, 2024), we restrict fMRI volumes to the nsdgeneral subset (Allen et al., 2022), Region of Interest manually-outlined on fsaverage located in the posterior cortex (Fischl et al., 1999). Then, we remove low-frequency noise in the fMRI signal using an additional detrending step: we fit cosine-drift linear model to each voxel in the time series, and subtract it from the raw signal. Finally, each voxel time-series is z-score-normalized. This preprocessing is used for training and evaluating Dynadiff and the other baselines reported for fMRI BOLD time-series. Please refer to Appendix for an ablation on fMRI preprocessing. Reconstructing images over time. As explained in Section 2.1, our models are trained on fixed fMRI time windows (s, t, d). To evaluate how well these models generalize across time, we also infer reconstructions from time windows shifted with regard to the image onset. More precisely, at test time, instead of conditioning our model on the usual training time-window, we evaluate it on shifted window (s, + δ, d) in which δ can take positive or negative values. Please note that even if the window starts before the image onset (i.e., + δ is negative), the fMRI timeseries may still contain relevant information about the image s, depending on the duration of the window. split for time-resolved decoding. NSD interleaves train and test image presentations. For example, an image from the train set can be presented immediately after an image from the test set. Consequently, evaluating decoding performance over succession of images requires re-definition of the train/test splits. For Figures 3 and 5, where we directly report the generation of successive images, we thus create new time-resolved train/test split that ensures that successive trials belong to the same split. For this, we used, for each subject separately, 45 fMRI recording runs for the test set. We then use the 435 remaining runs for training. This split yields approximately 27,000 training trials and 3,000 testing trials (making it aligned with the sizes of the original NSD split). Note that we use this split exclusively for Figures 3 and 5, as it is the only analysis https://github.com/facebookresearch/dynadiff https://github.com/cvnlab/nsddatapaper 4 Table 1 Comparison to baselines on the Natural Scenes Dataset. We average results across subjects 1, 2, 5 and 7 (as done in the corresponding studies). Notably we evaluate all the methods in single trial, without averaging same-image repetitions and provide SEM."
        },
        {
            "title": "Baselines",
            "content": "Low-level Semantic and High-level SSIM PixCorr AlexNet(2) AlexNet(5) CLIP-12 Incep Eff SwAV mIoU DreamSim Wave Brain-Diffusers MindEye1 MindEye2 Dynadiff 0.15 0. 0.21 0. 0.31 0.01 0.36 0.00 0.34 0.00 0.07 0. 0.21 0.01 0.27 0.01 0.24 0.01 0.21 0.01 68.99 0.71 89.41 1.16 91.45 2.21 94.15 0.99 95.82 0.82 77.44 0.81 92.68 0.88 95.45 0.62 97.34 0.50 98.20 0.41 76.76 0.41 85.36 1.05 91.11 0.61 90.38 0.80 93.53 0. 73.24 0.67 84.06 1.07 88.78 0.92 89.47 0.89 91.30 0.74 0.85 0. 0.80 0.01 0.73 0.00 0.71 0.01 0.68 0.01 0.53 0. 0.48 0.01 0.40 0.00 0.38 0.01 0.36 0.01 2.24 0.14 7.73 0.28 7.55 0.35 8.15 0.45 8.50 0.41 68.67 0.33 60.39 0.86 57.68 0.67 56.28 0.95 52.52 0.97 that shows results for successive images. All the other evaluations are conducted with the standard splits defined by NSD. Evaluation metrics. Following previous works e.g. Ozcelik and VanRullen (2023); Scotti et al. (2024); Wang et al. (2024b), we assess low-level image similarity using PixCorr (pixel-wise correlation), SSIM (Structural Similarity Index Metric) and Alexnet(2/5), and high-level resemblance include CLIP, Inception, Efficient-Net, and SwAV. We complement our analysis with two additional metrics, which are computed trial-wise, i.e., : for each unique test image I, we evaluate the metric on the pair consisting of and its reconstruction from randomly sampled repetition of I. Then, the results are averaged over the 1,000 unique test images. First, we use the DreamSim metric (Fu et al., 2023), which leverages mixture of pretrained backbones trained on human similarity-judgments dataset. Second, we use mIoU over semantic-segmentation masks for semantic consistency and interpretability. It is computed by passing each image and its reconstruction through semantic segmentation network and comparing their predicted semantic maps. We use ViT-Adapter (Chen et al., 2023a) as segmentation model. All metrics are computed after resizing images to 224224 pixels. Baselines. We compare our method to the seminal work of Brain-Diffuser (Ozcelik and VanRullen, 2023) as well as three state-of-the-art approaches: (i) MindEye (Scotti et al., 2023) and MindEye 2 (Scotti et al., 2024), originally designed for time-collapsed fMRI beta-values, and (ii) WAVE, which uses timeseries of BOLD fMRI signal as input. When applicable, we provide the details of how we adapted these methods to time series of NSD in Appendix A."
        },
        {
            "title": "3.1 Diffusion-based image reconstruction\nComparison to Baselines. Table 1 reports quantitative metrics using the four subjects of NSD, for our model\nand competing approaches, evaluated in single-trial (using individual and non-pooled fMRI timeseries). We\nalso compute Standard Error of the Mean (SEM) computed across the four subjects. Brain-Diffuser, MindEye,\nMindEye2 and our model were trained with a time-window of 6 · TR (8 s), starting 3 seconds after image onset.\nTo follow the original approach of Wang et al. (2024b), WAVE was trained with windows starting one TR\n(1.3 s) after image onset and lasting 5 · TR (6.5 s). Our approach outperforms other methods including the\nstate-of-the-art MindEye2. Indeed, our model improves by respectively 1.67 and 0.86 points on Alexnet(2) and\nAlexNet(5) compared to MindEye2, showing that it better preserves low-level contents (e.g. color, texture).\nWe also achieve a 3.76-point improvement over MindEye2 on DreamSim and a 3.25-point increase on CLIP-12.\nThis emphasizes our model’s ability to correctly decode object semantics and positions. Additionally, the\nperformance for each subject is reported in Table 6. To complete our analysis, we report the evaluation of\nthese models on i) fMRI test-trial windows averaged across same-image repetitions (in Table 7), and on ii)\nthe “average beta-values” commonly used in previous studies on NSD (in Table 4). The latter is obtained by\ntreating a beta-value as a fMRI time-series with a single timestep. Please refer to our appendix in appendix E\nfor our analysis on cross-subject decoding.",
            "content": "Qualitative comparison. Figure 3 displays comparison of reconstructions from Subjects 1 and 2 with our approach and other methods, using trials of four different images. Our approach shows superior alignment between the seen and reconstructed stimuli. For instance, in the first row, the positioning and size of the zebras in the reconstructed image more closely resembles their arrangement in the stimulus. Also, in the"
        },
        {
            "title": "Stimulus",
            "content": "WAVE MindEye1 MindEye2 Dynadiff 1 j 2 j t b 2 j 1 j t b 1 j 2 j Figure 3 Qualitative comparisons of Wave, MindEye1, MindEye2 and our model on the NSD dataset. We display the image stimuli on the left column and the next columns show WAVE, MindEye1, MindEye2 and our model successively. second row, our model accurately places cat at the doorway, demonstrating improved scene compositionality. Additional qualitative results for the four subjects are provided in Figures 7 and 8 of Appendix F. In Figure 4 and Figure 5, we evaluate our model in the time-resolved Time-resolved decoding of images. setting by decoding images through time. We use the new time-resolved train/test split defined in Section 2.3, which ensures that only image stimuli from the test set are input to the decoder at inference. We consider two evaluation settings. In the first setting called General, we train model Mgen on fMRI time windows (s, t, d) with fixed = 3 and = 8 (6 TRs). At test time, we evaluate Mgen on shifted (test) windows (s, + δ, d), assessing its abilities to generalize to new timesteps. Figure 4 shows seven columns of reconstructed stimuli respectively obtained for δ = TR, with {3, 2, . . . , 3}. Additionally, Figure 5 examines 16 different shifted windows corresponding to {6, 5, . . . , 9}. Please note that the x-axis values correspond to the upper end of the shifted time windows (meaning = + δ + d). In the second setting named Specialized, we train for each δ model Mt+δ on time windows (s, + δ, d) with fixed = 8 s. For the General model Mgen , the two extreme values δ = 3 TR and δ = 3 TR correspond precisely to the time windows (, t, d) of the previous and next stimuli presentations respectively, and we observe that the model effectively tends to decode the previous and next images. Furthermore, we notice that Mgen generalizes well to time windows unseen at training: it is able to reconstruct quite well stimuli presented at timestep from the shifted window (s, t, d) for values of that are close enough to t. However, the best performance across all timesteps is clearly achieved by using at the Specialized model Mt, which was trained to decode at the stimulus relative onset t. This phenomenon is also illustrated in Figure 5 in which we compute the temporal evolution of SSIM, AlexNet(2), CLIP and mIoU with the specialized models Mt 6 and the general model Mgen. We observe that we start to decode the image stimulus 3 after it was shown to the participant, which is coherent with the Hemodynamic Response Functions (HRF) profile. We can still decode the stimulus reasonably well by taking time window starting 10 after presentation using the specialized models. Previous Stimuli Current Next δ = 3T Predicted Stimuli δ = 0 δ = 3T r G i c S r G i c l n e a p r G i c S Figure 4 Real-time decoding of images using our specialized or general models Dynadiff. The General model Mgen is trained on time windows (s, t, d) (with = 3 and duration = 8 s) and we evaluate its generalization capabilities by reconstructing images from shifted windows (s, + δ, d). In the Specialized setting , we train separate model for each shift + δ on windows (s, + δ, d). This means that each column corresponds to different model. Since participants see stimulus every 4 seconds, δ = 3 TR and δ = 3 TR correspond to the windows of the previous and next image presentations respectively. As expected, Mgen can decode these images quite well."
        },
        {
            "title": "3.2 Ablations\nAblation on time window duration. We perform an ablation study on the duration d of the time window\nW (s, t, d). Specifically, we fix t = 3 s and compare durations d ∈ {1 · TR, . . . , 6 · TR}. We train one model for\neach of these six time windows. Then, we conduct a quantitative evaluation by computing low-level metrics\n(AlexNet2 and AlexNet5) and high-level metrics (CLIP and Inception). Figure 6 shows the evolution of these\nscores as a function of d. We observe that almost optimal performance is obtained with a window duration of\n3 · TR (3.9 s) while performance can be slightly enhanced by extending the duration to 6 · TR (7.8 s).",
            "content": "In Table 2, we analyze the effect of specific components of our brain module. Ablation on brain module design. First, we investigate the role of the time-specific layers. The first row of Table 2 shows that replacing them with single linear layer shared across all fMRI time samples reduces performance by 2.95 CLIP-12 points and 1.33 AlexNet(2) points. This implies that these layers are important for allowing the model to leverage the information encoded in the fMRI brain volumes independently. Second, we examine how the position of our time-aggregation layer affects performance. The second row of Table 2 shows that relocating the component 7 Figure 5 Each point is obtained by reconstructing images from different fMRI time window (s, + δ, d). We fix = 3 and duration = 8 and vary δ as explained in Section 3. The x-axis represents the end time of time window, i.e., + δ + Orange curve is obtained with specialized models trained specifically for each time window (s, + δ, d) while the blue curve displays the performance results of general model trained at (s, t, d) and evaluated at shifted test windows. We provide standard error of the mean on the four NSD subjects. The shaded gray area indicates the 3 sec. interval during which images were presented to the participants. Figure 6 Evolution of AlexNet(2/5), CLIP and Inception metrics when varying the time window duration used to train Dynadiff. More precisely, we use time windows (s, t, d) that start = 3 after the stimulus onset and vary their duration {1 TR, . . . , 6 TR}, with TR =1.3 s. from the output layer to the input layer of the brain module decreases performance. This decline might occur because our model is more effective at capturing dynamics of fMRI data when it undergoes additional processing. Ablation on diffusion model finetuning. In Table 3, we explore which layers of the latent diffusion model are the most useful to finetune, by computing SSIM, AlexNet(2), CLIP-12 and mIoU metrics. First, we consider finetuning all the diffusion models weights (1.1B parameters). This leads to overfitting quickly and poor performance. Second, we experimented with finetuning (i) all the linear layers of the diffusion model (totalizing 500M parameters), or (ii) all cross-attention linear layers (100M). Both options led to suboptimal results. Finally, keeping the entire diffusion model frozen (i.e., training only the brain module with the diffusion loss) also performed worse comparing to adding LoRA adapters to the diffusion models cross-attention layers. Table 2 Ablation on brain module design. We report SEM computed across the four subjects."
        },
        {
            "title": "Timestep layer",
            "content": "Temporal agg. layer Low-level High-level SSIM AlexNet(2) CLIPmIoU"
        },
        {
            "title": "OUT\nIN\nOUT",
            "content": "0.29 0.00 0.28 0.00 0.34 0.00 94.87 0.88 94.31 0.92 94.67 0.91 90.44 0.51 91.69 0.48 97.45 0.53 6.95 0.92 7.53 0.86 8.50 0."
        },
        {
            "title": "4 Discussion",
            "content": "Contributions and related works. The study offers three major contributions. First, we present considerably simplified decoding pipeline. Contrasting with recent proposals (Ozcelik and VanRullen, 2023; Scotti et al., 2023, 2024; Chen et al., 2023b; Wang et al., 2024b; Huo et al., 2024), Dynadiff neither depends on (1) pretrained fMRI encoder (2) an alignment stage between fMRI and pretrained embeddings (3) the post8 Table 3 Ablation on diffusion model training. We report SEM computed across the four subjects."
        },
        {
            "title": "Trainable Layers",
            "content": "Params Low-level High-level SSIM AlexNet(2) CLIPmIoU All Linear Cross-Attn LoRA on Cross-Attn 1.1B 500M 100M 0 25M 0.29 0.01 0.36 0.00 0.33 0.00 0.32 0.00 0.34 0.00 90.42 0.86 95.74 0.84 95.63 0.89 92.78 0.92 94.67 0.91 88.46 0.52 90.27 0.56 91.80 0.48 92.77 0.45 97.45 0. 7.56 0.93 8.29 0.98 8.08 0.90 7.12 0.99 8.50 0.97 selection and refining of image generation (4) independent lowand high-level reconstructions. Instead, Dynadiff is trained in single stage, with single diffusion loss. Second, Dynadiff obtains state-of-the-art performance on continuously-evolving fMRI BOLD signals. In particular, the fMRI preprocessing used in this study contrasts with current decoders, especially those trained on the NSD dataset (Ozcelik and VanRullen, 2023; Scotti et al., 2023, 2024), which use beta values obtained from GLM preprocessing stage that removes the time dimension of fMRI recordings. Notably, even though some previous studies employ fMRI preprocessing which maintain temporal dynamics (Wang et al., 2024b), their reconstruction quality still does not match that achieved with betas. Leveraging the temporal dimension of fMRI recordings naturally suggests progressing from decoding static images to decoding videos. Several studies have now started to develop pipelines to decode videos from 3T fMRI (Chen et al., 2023b; Nishimoto et al., 2011; Fosco et al., 2024). However, these approaches often rely on time-collapsed beta-values, involve multiple training stages and are typically based on much smaller amount of data. Also, since NSD facilitated the most impressive image reconstructions to date, our approach focuses on temporally-resolved decoding of static images, to establish robust baseline before advancing to the more complex video objectives. Similarly, the decoding of time-resolved brain activity is now generalized to variety of tasks such as speech perception (Défossez et al., 2022; Tang et al., 2023; dAscoli et al., 2024), continuous visual perception and behavior (Schneider et al., 2023). major goal for future research will be to unify these efforts into general architecture for decoding the representation of the brain. Third, beyond its decoding performance, the present approach enables temporal analysis of image representations in brain activity. More precisely, it reveals an unexpected phenomenon. The decoder trained at given time sample with respect to image onsets, can decode the image for relatively short amount of time. Yet, outside of this generalization window, it is still possible to decode the image, but thanks to decoder trained specifically around this time point. This result is clearest in Figure 3, where it is possible to decode the current image with specialized decoders, at moments where the generalized decoder reconstruct either the preceding or the next image. This result suggests that the neural patterns that represent images in the fMRI continuously change over time, and allow the simultaneous decoding of successive images. This dynamic coding, typically observed with electrophysiology or M/EEG (King and Dehaene, 2014), may thus apply to fMRI, in spite of its notoriously low temporal resolution. If confirmed, this result would indicate that dynamic coding may be general process to represent the succession of images, while avoiding their mutual interference. Limits. The present approach remains limited in three ways. First, while NSD is the largest fMRI dataset of individual responses to images (Allen et al., 2022), it was highlighted that the image distribution of images presented to participants tends to follow stereotypical clusters (Shirakawa et al., 2024). It will thus be important to validate the present approach to potentially less biased datasets. Second, Dynadiff is trained on preprocessed fMRI data, step used to remove movement and cardiac artifacts, align MRI segments, select the relevant voxels. This preprocessing step would likely improve if it was replaced with foundational model of brain activity, similarly to Wang et al. (2024b); Dadi et al. (2020); Thomas et al. (2022). Third, Dynadiff currently requires many data per participants. It is not adapted to generalize decoding to participants that are not in the training set. Whether it is possible to reliably reconstruct images from any brain remains an open challenge."
        },
        {
            "title": "5 Impact Statement\nThe potential for decoding brain activity to aid various patients with brain lesions is promising (Metzger\net al., 2023; Moses et al., 2021; Défossez et al., 2022; Liu et al., 2023; Willett et al., 2023). However, the rapid\nprogress in this field brings several ethical concerns, particularly the need to protect mental privacy. Several\nempirical studies are pertinent to this matter. High decoding performance with non-invasive recordings is\nprimarily achieved in perceptual tasks. In contrast, accuracy significantly drops when individuals are asked to\nimagine scenarios (Horikawa and Kamitani, 2017; Tang et al., 2023). Additionally, decoding performance is\nimpaired when participants perform disruptive tasks, like counting backward (Tang et al., 2023). This implies\nthat obtaining subjects’ consent is not just an ethical necessity but also a technical one for brain decoding.\nTo address these issues thoroughly, we advocate for open and peer-reviewed research standards.",
            "content": "Finally, image generation models that synthesize human faces pose risks of misuse, particularly if they replicate faces from the training data. To address this, our model release will automatically blur any reconstructed human faces. Acknowledgement. Collection of the NSD dataset was supported by NSF IIS-1822683 and NSF IIS-1822929."
        },
        {
            "title": "References",
            "content": "Emily Allen, Ghislain St-Yves, Yihan Wu, Jesse Breedlove, Jacob Prince, Logan Dowdle, Matthias Nau, Brad Caron, Franco Pestilli, Ian Charest, et al. massive 7T fMRI dataset to bridge cognitive neuroscience and artificial intelligence. Nature neuroscience, 25(1):116126, 2022. Yohann Benchetrit, Hubert Banville, and Jean-Rémi King. Brain decoding: toward real-time reconstruction of visual perception. In ICLR 2024, 2024. Thomas Carlson, Paul Schrater, and Sheng He. Patterns of activity in the categorical representations of objects. Journal of cognitive neuroscience, 15(5):704717, 2003. Zhe Chen, Yuchen Duan, Wenhai Wang, Junjun He, Tong Lu, Jifeng Dai, and Yu Qiao. Vision transformer adapter for dense predictions. 2023a. Zijiao Chen, Jiaxin Qing, Tiange Xiang, Wan Lin Yue, and Juan Helen Zhou. Seeing beyond the brain: Conditional diffusion model with sparse masked modeling for vision decoding. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 2271022720, 2023b. Alan Cowen, Marvin Chun, and Brice Kuhl. Neural portraits of perception: reconstructing face images from evoked brain activity. Neuroimage, 94:1222, 2014. Kamalaker Dadi, Gaël Varoquaux, Antonia Machlouzarides-Shalit, Krzysztof Gorgolewski, Demian Wassermann, Bertrand Thirion, and Arthur Mensch. Fine-grain atlases of functional modes for fmri analysis. NeuroImage, 221: 117126, 2020. Stéphane dAscoli, Corentin Bel, Jérémy Rapin, Hubert Banville, Yohann Benchetrit, Christophe Pallier, and Jean-Rémi King. Decoding individual words from non-invasive brain recordings across 723 participants, 2024. https://arxiv.org/abs/2412.17829. Alexandre Défossez, Charlotte Caucheteux, Jérémy Rapin, Ori Kabeli, and Jean-Rémi King. Decoding speech from non-invasive brain recordings. arXiv preprint arXiv:2208.12266, 2022. Matteo Ferrante, Furkan Ozcelik, Tommaso Boccato, Rufin VanRullen, and Nicola Toschi. Brain captioning: Decoding human brain activity into images and text, 2023. https://arxiv.org/abs/2305.11560. Bruce Fischl, Martin I. Sereno, Roger B.H. Tootell, and Anders M. Dale. High-resolution intersubject averaging and coordinate system for the cortical surface. Human Brain Mapping, 8(4):272284, 1999. Camilo Fosco, Benjamin Lahner, Bowen Pan, Alex Andonian, Emilie Josephs, Alex Lascelles, and Aude Oliva. Brain netflix: Scaling data to reconstruct videos from brain signals. In European Conference on Computer Vision, pages 457474. Springer, 2024. Stephanie Fu, Netanel Tamir, Shobhita Sundaram, Lucy Chai, Richard Zhang, Tali Dekel, and Phillip Isola. Dreamsim: Learning new dimensions of human visual similarity using synthetic data, 2023. https://arxiv.org/abs/2306.09344. Zijin Gu, Keith Jamison, Amy Kuceyeski, and Mert Sabuncu. Decoding natural image stimuli from fmri data with surface-based convolutional network. arXiv preprint arXiv:2212.02409, 2022. Kuan Han, Haiguang Wen, Junxing Shi, Kun-Han Lu, Yizhen Zhang, Di Fu, and Zhongming Liu. Variational autoencoder: An unsupervised model for encoding and decoding fmri activity in visual cortex. NeuroImage, 198: 125136, 2019. James Haxby, Ida Gobbini, Maura Furey, Alumit Ishai, Jennifer Schouten, and Pietro Pietrini. Distributed and overlapping representations of faces and objects in ventral temporal cortex. Science, 293(5539):24252430, 2001. Martin Hebart, Adam Dickter, Alexis Kidder, Wan Kwok, Anna Corriveau, Caitlin Van Wicklin, and Chris Baker. Things: database of 1,854 object concepts and more than 26,000 naturalistic object images. PloS one, 14 (10):e0223792, 2019. Tomoyasu Horikawa and Yukiyasu Kamitani. Generic decoding of seen and imagined objects using hierarchical visual features. Nature communications, 8(1):15037, 2017. Edward Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean Wang, Lu Wang, and Weizhu Chen. Lora: Low-rank adaptation of large language models. arXiv preprint arXiv:2106.09685, 2021. Jingyang Huo, Yikai Wang, Yun Wang, Xuelin Qian, Chong Li, Yanwei Fu, and Jianfeng Feng. Neuropictor: Refining fmri-to-image reconstruction via multi-individual pretraining and multi-level modulation. In European Conference on Computer Vision, pages 5673. Springer, 2024. Yukiyasu Kamitani and Frank Tong. Decoding the visual and subjective contents of the human brain. Nature neuroscience, 8(5):679685, 2005. Jean-Rémi King and Stanislas Dehaene. Characterizing the dynamics of mental representations: the temporal generalization method. Trends in cognitive sciences, 18(4):203210, 2014. Reese Kneeland, Jordyn Ojeda, Ghislain St-Yves, and Thomas Naselaris. Reconstructing seen images from human brain activity via guided stochastic search, 2023. https://arxiv.org/abs/2305.00556. Lynn Le, Thirza Dado, Katja Seeliger, Paolo Papale, Antonio Lozano, Pieter Roelfsema, Yağmur Güçlütürk, Marcel van Gerven, and Umut Güçlü. Inverse receptive field attention for naturalistic image reconstruction from the brain. arXiv preprint arXiv:2501.03051, 2025. Tsung-Yi Lin, Michael Maire, Serge J. Belongie, Lubomir D. Bourdev, Ross B. Girshick, James Hays, Pietro Perona, Deva Ramanan, Piotr Dolla r, and C. Lawrence Zitnick. Microsoft COCO: common objects in context. CoRR, abs/1405.0312, 2014. http://arxiv.org/abs/1405.0312. Yan Liu, Zehao Zhao, Minpeng Xu, Haiqing Yu, Yanming Zhu, Jie Zhang, Linghao Bu, Xiaoluo Zhang, Junfeng Lu, Yuanning Li, et al. Decoding and synthesizing tonal language speech from brain activity. Science Advances, 9(23): eadh0478, 2023. Sean Metzger, Kaylo Littlejohn, Alexander Silva, David Moses, Margaret Seaton, Ran Wang, Maximilian Dougherty, Jessie Liu, Peter Wu, Michael Berger, et al. high-performance neuroprosthesis for speech decoding and avatar control. Nature, pages 110, 2023. Yoichi Miyawaki, Hajime Uchida, Okito Yamashita, Masa-aki Sato, Yusuke Morito, Hiroki Tanabe, Norihiro Sadato, and Yukiyasu Kamitani. Visual image reconstruction from human brain activity using combination of multiscale local image decoders. Neuron, 60(5):915929, 2008. David Moses, Sean Metzger, Jessie Liu, Gopala Anumanchipalli, Joseph Makin, Pengfei Sun, Josh Chartier, Maximilian Dougherty, Patricia Liu, Gary Abrams, et al. Neuroprosthesis for decoding speech in paralyzed person with anarthria. New England Journal of Medicine, 385(3):217227, 2021. Chong Mou, Xintao Wang, Liangbin Xie, Yanze Wu, Jian Zhang, Zhongang Qi, and Ying Shan. T2i-adapter: Learning In Proceedings of the AAAI adapters to dig out more controllable ability for text-to-image diffusion models. Conference on Artificial Intelligence, volume 38, pages 42964304, 2024. Shinji Nishimoto, An Vu, Thomas Naselaris, Yuval Benjamini, Bin Yu, and Jack Gallant. Reconstructing visual experiences from brain activity evoked by natural movies. Current biology, 21(19):16411646, 2011. Furkan Ozcelik and Rufin VanRullen. Natural scene reconstruction from fMRI signals using generative latent diffusion. Scientific Reports, 13(1):15666, 2023. Furkan Ozcelik, Bhavin Choksi, Milad Mozafari, Leila Reddy, and Rufin VanRullen. Reconstruction of perceived images from fmri patterns and semantic brain exploration using instance-conditioned gans, 2022. https://arxiv.org/ abs/2202.12692. Dustin Podell, Zion English, Kyle Lacey, Andreas Blattmann, Tim Dockhorn, Jonas Müller, Joe Penna, and Robin Rombach. Sdxl: Improving latent diffusion models for high-resolution image synthesis, 2023. https://arxiv.org/ abs/2307.01952. Ruijie Quan, Wenguan Wang, Zhibo Tian, Fan Ma, and Yi Yang. Psychometry: An omnifit model for image reconstruction from human brain activity. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 233243, 2024. Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, Gretchen Krueger, and Ilya Sutskever. Learning transferable visual models from natural language supervision, 2021. Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Björn Ommer. High-resolution image synthesis with latent diffusion models, 2022. https://arxiv.org/abs/2112.10752. 12 Olaf Ronneberger, Philipp Fischer, and Thomas Brox. U-net: Convolutional networks for biomedical image segmentation. In Medical image computing and computer-assisted interventionMICCAI 2015: 18th international conference, Munich, Germany, October 5-9, 2015, proceedings, part III 18, pages 234241. Springer, 2015. Steffen Schneider, Jin Hwa Lee, and Mackenzie Weygandt Mathis. Learnable latent embeddings for joint behavioural and neural analysis. Nature, 617(7960):360368, 2023. Paul Scotti, Atmadeep Banerjee, Jimmie Goode, Stepan Shabalin, Alex Nguyen, Ethan Cohen, Aidan Dempster, fMRI-to-image with Nathalie Verlinde, Elad Yundler, David Weisberg, et al. Reconstructing the minds eye: contrastive learning and diffusion priors. arXiv preprint arXiv:2305.18274, 2023. Paul Scotti, Mihir Tripathy, Cesar Kadir Torrico Villanueva, Reese Kneeland, Tong Chen, Ashutosh Narang, Charan Santhirasegaran, Jonathan Xu, Thomas Naselaris, Kenneth Norman, et al. MindEye2: Shared-subject models enable fMRI-to-image with 1 hour of data. arXiv preprint arXiv:2403.11207, 2024. Katja Seeliger, Umut Güçlü, Luca Ambrogioni, Yagmur Güçlütürk, and Marcel AJ van Gerven. Generative adversarial networks for reconstructing natural images from brain activity. NeuroImage, 181:775785, 2018. Guohua Shen, Tomoyasu Horikawa, Kei Majima, and Yukiyasu Kamitani. Deep image reconstruction from human brain activity. PLoS computational biology, 15(1):e1006633, 2019. Ken Shirakawa, Yoshihiro Nagano, Misato Tanaka, Shuntaro C. Aoki, Kei Majima, Yusuke Muraki, and Yukiyasu Kamitani. Spurious reconstruction from brain activity, 2024. https://arxiv.org/abs/2405.10078. Yu Takagi and Shinji Nishimoto. High-resolution image reconstruction with latent diffusion models from human brain activity. bioRxiv, 2023. doi: 10.1101/2022.11.18.517004. https://www.biorxiv.org/content/early/2023/03/11/2022. 11.18.517004. Jerry Tang, Amanda LeBel, Shailee Jain, and Alexander Huth. Semantic reconstruction of continuous language from non-invasive brain recordings. Nature Neuroscience, 26(5):858866, 2023. Armin Thomas, Christopher Ré, and Russell Poldrack. Self-supervised learning of brain dynamics from broad neuroimaging data. Advances in neural information processing systems, 35:2125521269, 2022. Shizun Wang, Songhua Liu, Zhenxiong Tan, and Xinchao Wang. Mindbridge: cross-subject brain decoding framework. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 1133311342, 2024a. Yanchen Wang, Adam Turnbull, Tiange Xiang, Yunlong Xu, Sa Zhou, Adnan Masoud, Shekoofeh Azizi, Feng Vankee Lin, and Ehsan Adeli. Decoding visual experience and mapping semantics through whole-brain analysis using fmri foundation models, 2024b. https://arxiv.org/abs/2411.07121. Francis Willett, Erin Kunz, Chaofei Fan, Donald Avansino, Guy Wilson, Eun Young Choi, Foram Kamdar, Matthew Glasser, Leigh Hochberg, Shaul Druckmann, et al. high-performance speech neuroprosthesis. Nature, pages 16, 2023. Weihao Xia, Raoul De Charette, Cengiz Oztireli, and Jing-Hao Xue. Dream: Visual decoding from reversing human visual system. In Proceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision, pages 82268235, 2024a. Weihao Xia, Raoul de Charette, Cengiz Oztireli, and Jing-Hao Xue. Umbrae: Unified multimodal brain decoding. In European Conference on Computer Vision, pages 242259. Springer, 2024b. Xingqian Xu, Zhangyang Wang, Gong Zhang, Kai Wang, and Humphrey Shi. Versatile diffusion: Text, images and variations all in one diffusion model. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 77547765, 2023."
        },
        {
            "title": "A Baselines",
            "content": "MindEye The MindEye (Scotti et al., 2023) and MindEye 2 Scotti et al. (2024) approaches reconstruct the stimuli from NSD using volumes of time-collapsed fMRI averaged beta-values restricted to the Region Of Interest nsdgeneral. We keep the exact same stages, hyperparameters and architectures as in Scotti et al. (2023, 2024), with one exception. Since we train and evaluate on BOLD timeseries of NSD (also restricted to nsdgeneral), we have to slightly adapt the architectures to account for the additional time dimension: given an input BOLD fMRI time-series with time samples of voxels each, we flatten this window into vector of size pass it to the first linear layer of MindEye1 and MindEye2, whose size is increased from to . The rest of the architecture is left untouched. WAVE The approach of WAVE Wang et al. (2024b) reconstructs stimuli from timeseries IR10245 of 5 consecutive TRs of BOLD fMRI volumes, picked at one TR after stimulus onset, and registered to the DiFuMo-1024 atlas Dadi et al. (2020). In first contrastive stage, fMRI representations are extracted from this atlas using an off-the-shelf fMRI encoder pretrained in self-supervised fashion Thomas et al. (2022). This encoder is fine-tuned, together with prompt learning model, with modality-wise contrastive loss. In second and independent decoding stage, the same fMRI encoder is tuned again with diffusion prior module (trained from scratch) to obtain representations that condition an image-generation model Xu et al. (2023). Then, the latter is used to infer two candidate reconstructions for X, among which top one is selected for output (via clip-scoring against the CLIP-aligned fMRI representation). We evaluate the WAVE method on the NSD dataset as follows: (i) timeseries of BOLD volumes are mapped to the DiFuMo-1024 space using the default parameters of Dadi et al. (2020), and windows of 5 TRs of fMRI starting at 1 TR after stimulus onset are extracted, (ii) We apply successively the two training stages contrastive and decode, and the inference step reconstruct of the WAVE pipeline as provided by Wang et al. (2024b), with default parameters, with one exception: To accommodate the fact that NSD contains twice as many unique images per subject as the dataset used in Wang et al. (2024b), we have doubled the number of training steps for the contrastive and decode stages. This scaling factor was chosen following the approach of Wang et al. (2024b) on scaling training from 1 to 4 subjects simultaneously. We report metrics for the reconstructions obtained using the code made available by Wang et al. (2024b). Figure 3 shows some of the NSD reconstructions obtained using this approach."
        },
        {
            "title": "B Training hyperparameters",
            "content": "We train the brain module described in Section 2.1 and LoRA adapters of the pretrained latent diffusion model Xu et al. (2023) with AdamW optimizer and maximum learning rate of 103, weight decay of 0.01 and values of betas parameters of (0.9, 0.999). We append LoRA adapters to the diffusion model at all cross-attention layers, each with rank = 4 and alpha = 4. Besides, we apply linear learning rate warmup during the first 1k training steps and then use cosine decay schedule. Our model is trained for around 60k training steps using total batch size of 320 on 8 A100 gpus. This typically results in training time of 2.5 days. To optimize training efficiency, we use DeepSpeed ZeRO stage 2 Offload which offloads optimizer states and gradients to CPU and we train in float16 precision."
        },
        {
            "title": "C Additional quantitative metrics",
            "content": "In Table 6 we report single-trial quantitative metrics for all the four subjects, i.e., without averaging the fMRI responses of the three image repetitions. We also report in the last five rows the average metrics across the four subjects. https://github.com/ppwangyc/wave 14 Table 4 Comparison of Dynadiff and baselines on averaged betas values from NSD (1.8 mm resolution, restricted to nsdgeneral). Baseline results are reported directly from their respective introductory papers. As commonly done in prior studies, we report the average performance across all four subjects (1, 2, 5, and 7)."
        },
        {
            "title": "Baseline",
            "content": "SSIM PixCorr AlexNet(2) AlexNet(5) CLIP-12 Incep Low-level Semantic and High-level DREAM (Xia et al., 2024a) UMBRAE (Xia et al., 2024b) MindBridge (Wang et al., 2024a) MindEye1 (Scotti et al., 2023) Psychometry (Quan et al., 2024) NeuroPictor (Huo et al., 2024) MindEye2 (Scotti et al., 2024) Dynadiff 0.33 0.33 0.26 0.32 0.34 0.38 0.43 0.37 0.27 0.27 0.15 0.31 0.30 0.23 0.32 0.21 93.90 93.90 86.90 94.67 96.40 96.55 96.10 95.72 96.70 96.70 95.30 97.80 98.60 98.38 98.61 98. 94.10 94.10 94.30 94.05 96.80 93.35 92.97 94.09 93.40 93.40 92.20 93.75 95.80 94.50 95.41 95.03 Eff 0.64 0.64 0.71 0.65 0.63 0.64 0.62 0.61 SwAV 0.42 0.37 0.41 0.37 0.35 0.35 0.34 0. Table 5 Preprocessing ablation"
        },
        {
            "title": "Preprocessing type",
            "content": "SSIM PixCorr Low-level AlexNet(2) AlexNet(5) CLIP-12 Incep Semantic and High-level Eff SwAV mIoU DreamSim Dynadiff w/ fMRIprep Dynadiff w/ NSD prep. 0.30 0.30 0.19 0.21 92.62 94.77 97.08 97.34 91.84 93.54 90.67 91. 0.71 0.69 0.38 0.36 6.45 8.39 54.29 52.80 To complete our experimental study, we trained Dynadiff on the beta values used in all previous image-decoding works on NSD. These values lack time dimension thus we treat them as time series with single time sample. Then, we adjust our brain modules architecture to take such data as input, by removing the timestep and temporal-aggregation layers (see Figure 2). We report our results in Table 4, where performances of other methods are sourced from their respective papers. While not specifically designed for beta values, we observe that our model surpasses DREAM (Xia et al., 2024a), UMBRAE (Xia et al., 2024b), MindBridge (Wang et al., 2024a) and MindEye1 (Scotti et al., 2023). Furthermore, it performs on par with or only slightly below Psychometry, Neuropictor (Huo et al., 2024) and MindEye2 (Scotti et al., 2024), which all require more than one stage of training (unlike Dynadiff)."
        },
        {
            "title": "D Additional ablation",
            "content": "Preprocessing types. To measure the impact of data preprocessing on our models performance, we provide an additional fMRI preprocessing ablation in Table 5, where we use fMRIPrep instead of NSD authors custom preprocessing. More precisely, we use fMRIPrep 23.2.0 with default settings to convert raw fMRI NSD data into MNI152NLin2009aAsym space. Then, brain volumes are mapped onto fsaverage5. This process yields time series of brain volumes for each recording run. Next, we eliminate low-frequency noise through detrending step: cosine-drift linear model is fitted to each voxel and subtracted from the raw signal. Each time series is then z-scored. Finally, the data is segmented into epochs starting 3s after stimulus onset and lasting for total of 8s. As observed in Table 5, the NSD authors custom preprocessing enables superior image-decoding performance compared to using fMRIprep and projecting to fsaverage5. Cross-subject decoding with Dynadiff We evaluate the cross-subject decoding capabilities of Dynadiff in two ways. First, we train Dynadiff jointly on the four subjects 1,2,5,7 of NSD. The goal is to determine whether multi-subject model can perform competitively with single-subject models, while sharing most of the brain modules parameters across subjects. Indeed, instead of having four different independent brain modules, multi-subject model only has subject-layer and timestep-layer per subject while the remaining layers are shared, thereby improving computational efficiency. The results are displayed in table 8 and show that our multi-subject model performs on par with single-subject models. Second, we investigate our models generalization to unseen participants. For this, we pretrained Dynadiff on three of the four NSD subjects (2,5,7 and finetuned it on subject 1, varying the amount of data used for finetuning. To finetune the model on new subject, we train subjectand timestep-layers from scratch, Table 6 Comparison of Dynadiff and baselines for single-trial BOLD fMRI time series from NSD. Subjects Models Low-level Semantic and High-level Subject Subject 2 Subject 5 Subject 7 Average Dynadiff WAVE MindEye1 MindEye2 Dynadiff Wave MindEye1 MindEye Dynadiff Wave MindEye1 MindEye2 Dynadiff Wave MindEye1 MindEye2 Dynadiff Wave MindEye1 MindEye2 SSIM PixCorr AlexNet(2) AlexNet(5) CLIP-12 Incep 0.35 0.16 0.32 0.37 0.35 0.15 0.31 0.36 0.34 0.15 0.31 0. 0.34 0.15 0.30 0.35 0.25 0.08 0.32 0.27 0.22 0.07 0.29 0.25 0.19 0.08 0.25 0.22 0.19 0.06 0.24 0.21 97.30 70.37 94.18 96. 97.42 69.71 92.89 96.03 95.12 69.28 90.25 93.16 93.44 66.62 88.51 91.36 98.84 77.46 96.57 98.03 98.87 77.84 96.28 98.07 98.23 79.50 95.54 97. 96.84 74.95 93.39 95.61 93.02 76.65 91.49 90.46 93.43 77.13 90.72 90.74 95.67 77.75 92.79 92.42 92.00 75.50 89.42 87.91 91.00 72.59 89.33 88. 91.63 74.95 89.01 91.15 93.36 74.01 90.92 91.11 89.21 71.42 85.84 86.95 Eff 0.69 0.85 0.72 0.72 0.67 0.84 0.72 0. 0.65 0.84 0.72 0.69 0.71 0.86 0.75 0.75 SwAV mIoU DreamSim 0.36 0.55 0.40 0. 0.36 0.53 0.40 0.38 0.34 0.52 0.40 0.37 0.38 0.54 0.42 0.41 8.44 2.18 7.94 8.36 8.97 2.46 7.93 8.95 9.39 2.51 8.00 8. 7.22 1.80 6.31 6.65 52.58 69.40 57.34 56.05 51.94 68.03 56.98 55.30 50.08 67.98 56.47 54.35 55.47 69.26 59.95 59.42 0.34 0.00 0.15 0.00 0.31 0.01 0.36 0. 0.21 0.01 0.07 0.00 0.27 0.01 0.24 0.01 95.82 0.82 68.99 0.82 91.45 2.21 94.15 0.99 98.20 0.41 77.44 0.94 95.45 0.62 97.34 0.50 93.53 0.67 76.76 0.48 91.11 0.61 90.38 0.80 91.30 0.74 73.24 0.78 88.78 0.92 89.47 0.89 0.68 0.01 0.85 0.00 0.73 0.00 0.71 0. 0.36 0.01 0.53 0.01 0.40 0.00 0.38 0.01 8.50 0.41 2.24 0.16 7.55 0.35 8.15 0.45 52.52 0.97 68.67 0.38 57.68 0.67 56.28 0.95 while other brain-module weights are finetuned. Figure 9 shows how performance increases with the number training sessions for: i) single-subject model trained from scratch for subject 1 (Not pretrained setting), ii) the multi-subject model trained on 2,5 and 7 and fine-tuned on subject 1s data (Pretrained setting). Notably, we see performance gain when finetuning the pretrained model on limited number of sessions, confirming that multi-subject pretraining reduces per-subject data requirements. These experiments are in line with several previous works that tackled multi-subject training (Wang et al., 2024a; Xia et al., 2024b; Quan et al., 2024; Huo et al., 2024). These methods proposed different types of architectures and training recipes to account for the variability of brain activity between subjects. For example, MindBridge (Wang et al., 2024a) learns subject-invariant embeddings through cycle consistency loss. UMBRAE (Xia et al., 2024b) uses transformer with subject-specific tokens. Psychometry (Quan et al., 2024) further advances this with an Omni Mixture-of-Experts architecture. Neuropictor (Huo et al., 2024) trains unified latent fMRI encoder across subjects, and follows with multi-subject pretraining. Unlike Dynadiff, these methods need to additionally finetune their pretrained multi-subject model on given subject data to reach optimal performance for this subject. Finally, MindEye2 (Scotti et al., 2024) shows good transfer capability to unseen subjects learning from limited number of sessions and using averaged beta values. We demonstrate that the same phenomenon occurs with single-trial BOLD fMRI data in figure 9."
        },
        {
            "title": "F Visualizations",
            "content": "In Figures 7 and 8, we show additional reconstructions of Dynadiff for each subject. Figure 7 shows some of the best image reconstructions obtained, while Figure 8 displays failure cases. We observe that image stimuli with uncommon content in the training data tend to be reconstructed with reduced accuracy. For instance, the stimulus in the fourth row (a picture with scissors and pens) is rather unusual in the NSD dataset and is poorly reconstructed by Dynadiff. Besides, complex images containing many objects are challenging for Dynadiff, as shown by the last row of Figure 8. 16 Table 7 Comparison of Dynadiff and baselines for BOLD fMRI time series from NSD, averaged across same-image repetitions. Subject Model Low-level Semantic and High-level Subject 1 Subject 2 Subject 5 Subject 7 Average SSIM PixCorr AlexNet(2) AlexNet(5) CLIP-12 Incep Dynadiff Wave MindEye1 MindEye2 Dynadiff Wave MindEye1 MindEye2 Dynadiff Wave MindEye1 MindEye2 Dynadiff Wave MindEye1 MindEye2 0.36 0.20 0.31 0.37 0.36 0.21 0.31 0. 0.35 0.22 0.30 0.37 0.35 0.20 0.30 0.36 0.28 0.09 0.37 0.29 0.25 0.09 0.33 0.27 0.23 0.09 0.29 0.25 0.22 0.07 0.28 0. 97.99 75.79 95.93 97.44 97.78 76.90 94.66 97.16 96.55 76.94 92.68 95.81 95.64 74.23 92.13 95.44 99.03 86.19 97.45 98.92 98.96 86.44 97.30 98. 98.88 86.34 96.52 98.68 98.25 82.21 96.12 98.01 95.74 84.44 93.69 92.91 95.23 84.78 93.61 92.57 96.71 86.06 94.13 94.10 95.11 81.50 93.12 92. 93.97 81.91 92.23 92.83 94.50 83.71 90.76 92.46 95.16 83.22 91.84 93.06 93.34 78.87 90.55 91.15 Eff 0.63 0.79 0.69 0. 0.63 0.78 0.69 0.68 0.60 0.78 0.68 0.65 0.64 0.80 0.70 0.69 SwAV mIoU DreamSim 0.32 0.50 0.37 0.35 0.33 0.50 0.37 0.36 0.31 0.48 0.37 0.34 0.34 0.51 0.39 0.36 10.85 3.84 8.98 10.32 10.96 3.78 8.52 10. 10.97 3.93 8.92 9.73 9.49 3.28 7.67 9.80 47.92 64.05 54.04 52.59 48.28 63.19 54.31 53.08 46.03 62.26 53.53 51.42 49.59 64.83 55.42 53. Our model Wave MindEye1 MindEye2 0.35 0.00 0.21 0.00 0.31 0.00 0.37 0.00 0.25 0.01 0.08 0.00 0.32 0.02 0.26 0.01 96.99 0.48 75.97 0.64 93.85 0.76 96.46 0.43 98.78 0.16 85.29 1.03 96.85 0.28 98.60 0.17 95.70 0.32 84.2 0.96 93.64 0.18 92.92 0. 94.24 0.34 81.93 1.09 91.34 0.35 92.37 0.37 0.63 0.01 0.78 0.00 0.69 0.00 0.67 0.00 0.33 0.01 0.5 0.01 0.38 0.00 0.35 0.00 10.57 0.31 3.71 0.15 8.52 0.25 10.01 0.12 47.95 0.64 63.58 0.55 54.33 0.35 52.73 0.44 Table 8 Performance of Dynadiff when trained on multiple subjects."
        },
        {
            "title": "Multisubject model",
            "content": "Subject 1 Subject 2 Subject 5 Subject 7 Average SSIM 0.34 0.35 0.35 0.33 0.34 0.01 Low-level PixCorr AlexNet(2) AlexNet(5) CLIP-12 Incep Semantic and High-level SwAV Eff mIoU DreamSim 0.17 0.19 0.17 0.13 0.17 0.01 96.29 96.24 94.72 92.61 94.96 0.87 98.57 98.68 98.64 96.42 98.08 0.55 92.66 93.46 95.49 91.31 93.23 0.87 90.73 92.06 92.79 89.15 91.18 0. 0.70 0.69 0.66 0.73 0.69 0.01 0.37 0.37 0.36 0.38 0.37 0.0 8.10 8.45 51.02 7.14 8.17 0.39 52.87 52.34 55.84 53.02 1."
        },
        {
            "title": "Stimuli",
            "content": "Subject 1 Subject 2 Subject 5 Subject 7 Figure 7 Examples of images generated with Dynadiff, choosing among the best reconstructions."
        },
        {
            "title": "Stimuli",
            "content": "Subject 1 Subject 2 Subject 5 Subject 7 Figure 8 Examples of failure cases from Dynadiff. Figure 9 Cross-subject performance with varying amounts of data per subject."
        }
    ],
    "affiliations": [
        "FAIR at Meta"
    ]
}