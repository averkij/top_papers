{
    "paper_title": "StrandDesigner: Towards Practical Strand Generation with Sketch Guidance",
    "authors": [
        "Na Zhang",
        "Moran Li",
        "Chengming Xu",
        "Han Feng",
        "Xiaobin Hu",
        "Jiangning Zhang",
        "Weijian Cao",
        "Chengjie Wang",
        "Yanwei Fu"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Realistic hair strand generation is crucial for applications like computer graphics and virtual reality. While diffusion models can generate hairstyles from text or images, these inputs lack precision and user-friendliness. Instead, we propose the first sketch-based strand generation model, which offers finer control while remaining user-friendly. Our framework tackles key challenges, such as modeling complex strand interactions and diverse sketch patterns, through two main innovations: a learnable strand upsampling strategy that encodes 3D strands into multi-scale latent spaces, and a multi-scale adaptive conditioning mechanism using a transformer with diffusion heads to ensure consistency across granularity levels. Experiments on several benchmark datasets show our method outperforms existing approaches in realism and precision. Qualitative results further confirm its effectiveness. Code will be released at [GitHub](https://github.com/fighting-Zhang/StrandDesigner)."
        },
        {
            "title": "Start",
            "content": "StrandDesigner: Towards Practical Strand Generation with Sketch Guidance Moran Li Tencent YouTu Lab Shanghai, China moranli@tencent.com Na Zhang Fudan University Shanghai, China nazhang23@m.fudan.edu.cn Chengming Xu Tencent YouTu Lab Shanghai, China chengmingxu@tencent.com 5 2 0 2 3 ] . [ 1 0 5 6 1 0 . 8 0 5 2 : r Han Feng, Xiaobin Hu, Jiangning Zhang, Weijian Cao, Chengjie Wang Tencent YouTu Lab Shanghai, China Yanwei Fu Fudan University Shanghai, China yanweifu@fudan.edu.cn ABSTRACT Realistic hair strand generation is crucial for applications like computer graphics and virtual reality. While diffusion models can generate hairstyles from text or images, these inputs lack precision and user-friendliness. Instead, we propose the first sketch-based strand generation model, which offers finer control while remaining user-friendly. Our framework tackles key challenges, such as modeling complex strand interactions and diverse sketch patterns, through two main innovations: learnable strand upsampling strategy that encodes 3D strands into multi-scale latent spaces, and multi-scale adaptive conditioning mechanism using transformer with diffusion heads to ensure consistency across granularity levels. Experiments on several benchmark datasets show our method outperforms existing approaches in realism and precision. Qualitative results further confirm its effectiveness. Code will be released at GitHub. CCS CONCEPTS Computing methodologies Computer vision tasks. KEYWORDS Strand generation, learning to upsample"
        },
        {
            "title": "1 INTRODUCTION\nGenerating realistic hair strands is a crucial challenge in computer\ngraphics, virtual reality, and digital content creation. High-quality\nstrand generation can substantially elevate the visual realism of\ndigital avatars, video game characters, and virtual humans. Recent\nadvancements in generative models, especially diffusion models,\nhave enabled the creation of detailed hairstyles controlled by text\nprompts. For example, HAAR [23] proposed leveraging latent diffu-\nsion model to generate guiding strand latent features conditioned\non prompts, which are upscaled to full strands using composite\nstrategies such as nearest neighbor and bilinear interpolation.",
            "content": "Both authors contributed equally to this research. During internship at Tencent YouTu Lab. Corresponding author. Prof. Yanwei Fu is with School of Data Science, Fudan University, Shanghai Innovation Institute, Institute of Trustworthy Embodied AI, Fudan University. Figure 1: Text prompts often fail to describe hairstyles precisely, and finding exact reference images is challenging. By contrast, sketches are generally clearer and more flexible. While these methods show promising potential in using visual generative models for strand generation, we argue that text or raw image prompts are not ideal for conditional strand generation. Instead, sketch images with binarized strokes are more suitable. The key differences are illustrated in Fig. 1: (1) Text-based conditioning often leads to one-to-many mapping, making precise control difficult. For example, curly long hair can correspond to many variations in curl patterns and lengths, lacking specificity for detailed strand generation. Furthermore, text descriptions are often ambiguous and may not capture the intricate details required for high-quality strand generation. (2) Image-based conditioning provides more detail but faces challenges about user-friendliness. Photographing real hairstyles is time-consuming, and static images may not fully capture hair dynamics across angles and lighting. Building diverse dataset is also resource-intensive. (3) Sketch images offer compelling alternative for strand generation. Sketches allow users to draw desired hairstyles with precision, capturing shape and flow more effectively than text or images. This control is valuable for customization. Moreover, sketches are inherently flexible and can be easily modified to reflect changes in hairstyle preferences or design requirements. Based on such clues, this paper focuses on proposing the first sketch-based strand generation model. Despite these advantages, the inherent challenges posed by sketch images make it difficult to directly adapt previous methods to the new prompt format, including: (1) Complex modeling of neighboring strands. Sketches contain fine-grained structural information that requires sophisticated modeling techniques. Fixed upsampling strategies used by methods like HAAR, such as nearest-neighbor or bilinear interpolation, are insufficient for capturing these nuanced details, as also noted by prior works such as GroomGen [34]. These traditional methods fail to account for intricate relationships between neighboring strands, leading to suboptimal results. The complexity of hair strand interactions requires advanced modeling approaches that can accurately represent the dependencies and variations in strand configurations. (2) Varied sketch patterns for different strand densities. Intuitively, users expertise can produce sketches with divergent patterns. Professional designers may create detailed full-hairstyle sketches, while amateurs provide only sparse guiding strands. This diversity makes single-conditioning approaches ineffective, necessitating adaptive mechanisms to handle different sketch densities while maintaining realistic strand generation. To address these challenges, we propose novel framework incorporating learnable strand upsampling strategy and multi-scale adaptive conditioning based on an autoregressive model. Our framework consists of two key components. First, we employ the idea of next-scale prediction to build learning-to-upsample strand generation strategy. This involves encoding the 3D strands into multiple latent spaces corresponding to different guiding scales, with the smallest scale representing basic guiding strands and other scales representing residual latents involved in gradually adding more guiding strands until the full strands are built. Next, the conditional distribution of each scales latent embeddings given previous scales is modeled by transformer model to which diffusion heads are attached, following MAR [15]. This architecture effectively utilizes continuous latent embeddings and avoids performing discrete tokenization, which is empirically proven ineffective. Second, we introduce multi-scale adaptive conditioning mechanism to endow our model with the ability to handle diverse sketch patterns. This mechanism involves leveraging learnable visual tokens for sketches at each scale. For each scale, these visual tokens are fed to the pretrained DINOv2 [17] to adapt the extracted features. By aligning the adapted features of sketch images with various scales with the original DINOv2 features of sketches belonging to the specific scale, the visual tokens learn to maintain consistency with the input sketch across various granularity levels. Based on the adapted sketch embeddings, we build dual-level conditioning mechanism: local patch tokens guide fine-grained details via attention layers, while the global tokens enforce global shape consistency through direct summation. To validate the effectiveness of our proposed method, we conduct comprehensive experiments and comparisons on the USCHairSalon [12] and CT2Hair [20] datasets. Our method significantly outperforms other competitors across multiple metrics (e.g., Point Cloud IoU, Chamfer Distance, Hausdorff Distance, CLIP Score [9], and LPIPS [31]). The results demonstrate that our framework generates strands that accurately reflect input sketches and exhibit realistic interactions and patterns. Overall, our contributions are summarized as follows: We propose the first sketch-conditioned strand generation framework, solving both the challenges of text ambiguity and difficulty in collecting photographic data. Zhang et al. We propose the multi-scale learnable upsampling strategy as an alternative to the previously used fixed upsampling methods. In order to address the intrinsic variety of sketch images, we propose the adaptive multi-scale conditioning mechanism via adaptation of pretrained DINOv2."
        },
        {
            "title": "2 RELATED WORK\n2.1 3D Hair Representation\nEarly parametric methods for 3D hair representation explored var-\nious approaches. Yang et al. [28] and Wang et al. [25] developed\ngeneralized cylinders and hierarchical cluster models, while Deep-\nMVSHair [13] leveraged continuous direction fields for multi-view\nreconstruction. Bhokare et al. [1] later demonstrated real-time ren-\ndering with hair meshes. While these methods provided intuitive\nstyling control, they struggled with complex geometric details. Re-\ncent volumetric and implicit representations have shown promising\nresults, with MonoHair [27] and TECA [29] employing Neural Radi-\nance Fields (NeRFs) for coarse hair geometry and UniHair [33] lever-\naging Gaussian Splatting for single-view reconstruction. However,\nthese approaches only model the visible surface without internal\nstructure, making them incompatible with downstream applications.\nAs a practical solution for high-fidelity hair modeling, strand-based\nrepresentations have become prevalent in both research [20] and\nindustry [3, 6]. Neural Haircut [22] and HAAR [23] advanced this\ndirection by mapping each strand on the scalp surface to a hair\nmap through UV-space parameterization, using a strand-VAE to\ncompress each 3D curve into a 64-D vector. Our approach further\ndecomposes the strand map into multiple latent spaces, enabling\na coarse-to-fine generation process that progressively increases\ndetail from guide strands to full resolution. This design naturally\naligns with professional hair modeling workflows.",
            "content": "2.2 3D Hair Generation 3D hair generation has garnered growing research interest. Early attempts employed example-based methods [18] and volumetric VAEs [19], yet suffered from limited diversity and over-smoothing. For unconditional generation, GroomGen [34] introduced hierarchical VAE framework to model detailed strand geometry. More recently, Perm [8] used PCA-based parameterization for frequency decomposition, while Curly-Cue [26] proposed algorithms to model high-frequency helical structures in tightly coiled hair. Recent works have investigated text-guided generation, with TECA [29] using NeRF-based representations and HAAR [23] adopting UV-space latent diffusion. However, text descriptions often fail to effectively convey users detailed hairstyle requirements. In contrast, images and sketches offer more direct and precise control over hair details. HairStep [32] leveraged intermediate representations extracted from images for single-view reconstruction, yet struggled with occluded regions. DeepSketchHair [21] employed GANs to generate 3D orientation fields from multi-view sketches, but exhibited limited generation quality and practical utility. Our method enables intuitive control via sketch inputs while maintaining consistency in strand-based hairstyle generation, seamlessly integrating with physics-based rendering and simulation. StrandDesigner: Towards Practical Strand Generation with Sketch Guidance"
        },
        {
            "title": "3.1 Learnable Strand Upsampling Strategy\nThe core of our framework lies in the learnable strand upsampling\nstrategy, which addresses the limitations of traditional fixed up-\nsampling. This strategy involves encoding 3D strands into multiple\nlatent spaces, each representing a guiding scale with distinct detail\nlevels. The smallest guiding scale captures basic guide strands, while\nsubsequent scales progressively add more strands, culminating in a\ncomplete hairstyle.",
            "content": "3.1.1 Multi-scale strand encoding. To encode and decompose 3D strand data into several latent spaces denoting different scales, we propose utilizing hierarchical structure for the autoencoder. Specifically, for strands 𝑆 R𝑁 𝑃 3, where 𝑁 denotes the number of strands, 𝑃 denotes the number of 3D points in each strand, we first follow HAAR [23] to employ pre-trained strand VAE 𝜀 to encode 𝑆 into the strand-level latent space to yield ˆ𝑆 R𝑁 64. This is then converted to hair map 𝐻 R12812864 via fixed mapping for each scalp position. 𝐻 is further decomposed into map set {𝐻𝑘 }𝐾 𝑘=1 corresponding to 𝐾 different guiding scales, via max pooling with kernel size of 2𝐾 𝑘 applied to 𝐻 for the 𝑘-th scale. Intuitively, 𝐻𝑘 with the smaller guiding scale (i.e., smaller 𝑘) represents the hairstyle more abstractly with fewer strands, generally describing the coarse style of the current sample. As 𝑘 increases, the spatial dimension of 𝐻𝑘 grows, i.e., more strands depicting the details are added. While directly utilizing {𝐻𝑘 } for strands generation is straightforward, we find this set suffers from two primary information redundancies: (1) Obviously, the guiding strands from ascendant scales are inherently contained in subsequent scales. (2) Guiding strands can also provide contextual information for neighboring strands. To address these issues, we further process {𝐻𝑘 } into new set { ˆ𝐻𝑘 } to reduce the redundancy as follows: ˆ𝐻𝑘 = (cid:40)𝐻1, 𝑘 = 1, 𝐻𝑘 tile(𝐻𝑘 1), 𝑘 {2, , 𝐾 } (1) where tile denotes spatially tiling each pixel into 2 2 grid. Thus, latent maps corresponding to each scale, except for the smallest one, solely contain residual information relevant to the specific scale. To further facilitate the generation process, we train multi-scale latent VAE set {𝜀𝐿 𝑘=1 respectively for each scale, yielding in the latent embedding set { ˆℎ𝑘 }𝐾 𝑘 }𝐾 𝑘=1. 3.1.2 Generation by Learning to Upsample . To effectively learn the coarse-to-fine procedure embedded in the sequence of latent embeddings { ˆℎ𝑘 }, we adopt simple yet effective approach inspired by the next-scale prediction methodology proposed in VAR [24]. This involves constructing scale-wise autoregressive generation model to capture the hierarchical nature of the data. For each guiding scale 𝑘, we model the conditional distribution of the latent embeddings ˆℎ𝑘 given embeddings from all preceding scales, denoted as 𝑝 ( ˆℎ𝑘 ˆℎ1, , ˆℎ𝑘 1). To achieve this, we employ transformer architecture augmented with diffusion heads, inspired by MAR [15]. The generation model is structured as an encoder-decoder framework, where both components utilize transformer backbones. During the training phase, unmasked strand tokens ˆℎ𝑢𝑛𝑚𝑎𝑠𝑘 are randomly selected, whereas during inference, the unmasked tokens represent those generated in the previous iterations. These tokens are then processed by the mask encoder E𝐺 , which outputs the encoded unmasked tokens that are merged with the mask tokens. This combined embedding sequence is subsequently used by the mask decoder D𝐺 to predict latent conditioning embedding sequence 𝑧𝑘 . Importantly, latent embeddings from all preceding scales { ˆℎ𝑖 }1<=𝑖<𝑘 are incorporated into the input of both the mask encoder E𝐺 and the mask decoder D𝐺 , ensuring that the model has 𝑘 Zhang et al. Figure 2: Overview of our proposed framework employing learnable multi-scale upsampling strategy via next-scale prediction and an adaptive conditioning mechanism with learnable visual tokens for sketch-based strand generation. access to necessary context. The final output is obtained by denoising random Gaussian noise via MLP denoiser D𝑀𝐿𝑃 with latent conditioning embedding 𝑧𝑘 , following the DDPM [10] framework. We optimize with diffusion denoising loss, formulated as: L𝑑𝑖 𝑓 𝑓 = 𝜖𝑡 D𝑀𝐿𝑃 ( ˆℎ𝑘, 𝑧𝑘, 𝑡)2 (2) upsampling methods such as nearest neighbor or bilinear interpolation. By learning to predict the next scale, our model generates strands with realistic interactions and patterns, capability crucial for high-quality strand generation, as it ensures that the generated strands are not only visually plausible but also exhibit the complex dependencies observed in real-world data. where 𝑡 denotes randomly chosen diffusion timestep, and 𝜖𝑡 denotes the corresponding random noise. Compared to straightforward autoregressive model, our approach offers significant advantages by effectively utilizing continuous latent embeddings. This allows us to circumvent the common problems associated with discrete tokenization, such as information loss and reduced fidelity, which can adversely affect the quality of the generated output. Furthermore, each scale within our framework yields decodable intermediate representation, corresponding to tangible hair geometry. This unique property allows users to inspect early-stage generation results and iteratively refine input sketches, in contrast to VAR models, which require generating the complete sequence before assessment or adjustment. Moreover, unlike the diffusion model in HAAR, our structure provides flexible control based on the embeddings from preceding scales, enabling the model to adaptively capture the intricate relationships between neighboring strands, thereby addressing the limitations of fixed"
        },
        {
            "title": "3.2 Multi-scale Adaptive Conditioning\nTo effectively condition the generation process on sketch image I𝑆 ,\na straightforward approach might involve using a fixed pretrained\nbackbone, such as DINOv2 [17], to extract features ˆ𝑐 from I𝑆 , similar\nto existing text-based or image-based methods. These extracted\nfeatures could then be integrated into the generation model through\nan attention mechanism. However, as elaborated in Sec. 1, while\nsketch images are inherently more suitable as conditioning inputs\ncompared to text or raw images, they present unique challenges\ndue to their inherent variability. This variability arises from the\ndiffering skill levels, artistic styles, and interpretations of individual\nusers, leading to a wide array of sketch patterns. Such diversity can\ncomplicate the conditioning process, as it introduces inconsistencies\nthat a fixed feature extraction method may not adequately address.\nMoreover, within our multi-scale framework, utilizing sketches that\ndo not align with the target scale can result in either missing critical",
            "content": "StrandDesigner: Towards Practical Strand Generation with Sketch Guidance details or introducing redundant information, both of which can adversely affect the models performance. We tackle these challenges by proposing novel multi-scale adaptive conditioning mechanism that is crucial for effectively managing diverse sketch inputs. During training, when generating latent maps for the 𝑘-th guiding scale, we adapt DINOv2 by injecting learnable scale-specific tokens 𝜃𝑙 𝑘 into the last third of its transformer layers. This adaptation enables the model to dynamically adjust to the specific characteristics of each scale. Specifically, for the 𝑙-th transformer layer, we append set of learnable scalespecific tokens 𝜃𝑙 to the layers original input 𝑐𝑙 1, which denotes the latent embedding of I𝑆 . The output of each self-attention layer is then modified as follows: {𝑄, 𝐾, 𝑉 } = 𝑊𝑄,𝐾,𝑉 ( 𝑐𝑙 1; 𝜃𝑙 (cid:105) (cid:104) ) 𝑐𝑙 = Attn(𝑄 , 𝐾 , 𝑉 ) (3) (4) To incorporate scale-related knowledge into {𝜃𝑙 𝑘 }, we introduce an optimization objective that aligns sketches of varying granularity with the target guiding scale. Specifically, for each strand data 𝑆, initially, we obtain its corresponding sketch images {I𝑆𝑘 } for each guiding scale, as detailed in Sec. 3.1.1. Subsequently, we derive the adapted embedding 𝑐𝑖 , with 𝑖 randomly sampled from {1, , 𝐾 }, and compare it to the original DINO embedding ˆ𝑐𝑘 . The objective is defined as: L𝑎𝑙𝑖𝑔𝑛 = 𝜎 ( ˆ𝑐𝑘 ) log 𝜎 ( 𝑐𝑖 ) (5) where 𝜎 denotes the normalization following the DINO loss [17]. By optimization with this objective, our multi-scale conditioning mechanism ensures the model can effectively leverage the diverse patterns inherent in sketches, irrespective of their complexity or stylistic variations. This adaptability is crucial for preserving the integrity of generated output, as it enables the model to accommodate the wide range of sketch inputs it may encounter. Consequently, during inference, our framework maintains consistency and robustness when conditioned on sketches with various levels of granularity, ensuring that the generated strands are both global-shape accurate and visually realistic. Furthermore, since sketches can convey both coarse and fine details of hairstyle, we propose dual-level conditioning mechanism to enhance sketch embeddings. This mechanism captures comprehensive sketch information by integrating local and global cues. Specifically, local patch tokens derived from the adapted DINOv2 are concatenated with strand tokens and interact through attention layers in the generation model, facilitating detail refinement. This interaction allows the model to focus on the intricate features of the sketch, ensuring that fine-grained details are accurately represented. Meanwhile, the class token, which encapsulates global information, is directly added to all strand tokens to guide the global shape. This ensures that the model retains coherent understanding of the hairstyles general geometry. This dual-level approach ensures that the model captures both the fine-grained details and the overarching structure of the hairstyle, resulting in more accurate and realistic strand generation. Through the integration of both local and global information, our approach provides comprehensive framework for strand generation, resilient to the inherent variability in sketch inputs."
        },
        {
            "title": "4.2 Quantitative Comparison\nDue to CFG, our method supports both unconditional and condi-\ntional hairstyle generation. Quantitative evaluations against state-\nof-the-art methods are conducted.\nCompetitors. We compare our sketch-based strand generation\nmodel with two recent approaches: (1) HAAR[23], a text-guided\nstrand generation model trained on LLaVA [16] VQA answers. For\nfairness, we align with HAAR’s conditioning by rendering front",
            "content": "Zhang et al. Figure 3: Qualitative comparison across HAAR [23], Sketch+HAAR, HairStep [32], and our method. HAAR often ignores input conditions and defaults to short/medium styles. Sketch+HAAR improves accuracy via visual conditioning but still lacks detail fidelity. HairStep struggles with consistent geometry and produces artifacts such as incorrect back strands. In contrast, our method achieves high fidelity and precise control over diverse inputs. Table 1: Comparison of Unconditional Generation Table 2: Comparison of Conditional Generation Method MMD-CD COV-CD(%) 1-NNA(%) 50% Method PC-IoU(%) CD(%) Hausdorff CLIP LPIPS HAAR [23] Ours 0.0147 0.0090 30.31 35. 91.93 88.95 and side views for test hairstyles, processing with LLaVA, and using identical prompts and BLIP [14] features. To further evaluate the effect of sketch conditioning, we also re-implement variant named Sketch+HAAR by replacing BLIP features with sketchbased DINOv2 features. (2) HairStep[32], single-view reconstruction method. We use SD3 [4] to translate sketches into photorealistic images and retain only successful reconstructions. Unconditional Generation. Distribution quality is compared against HAAR [23] using Chamfer Distance-based metrics: Minimum Matching Distance (MMD-CD) for sample quality, Coverage (COV-CD) for diversity, and 1-Nearest-Neighbor Accuracy (1-NNA) for overall distribution fit. As presented in Tab. 1, our method demonstrates an improved capability in capturing the target hairstyle distribution, with lower MMD-CD (higher fidelity, closer to the references), and higher COV-CD (greater diversity) than HAAR. Conditional Generation. Conditional generation is evaluated via consistency with ground truth geometry and input conditions. Geometric fidelity relative to the ground truth is measured by Point HAAR [23] Sketch+HAAR HairStep [32] Ours 53.83 60.85 58.87 64.54 2.21 1.06 1.86 0.80 0.1392 0.1093 0.1514 0. 0.9197 0.2417 0.9411 0.1804 0.9433 0.1968 0.9507 0.1483 Cloud IoU (PC-IoU), Chamfer Distance (CD), and Hausdorff Distance. Semantic alignment between front-view rendered outputs and input sketches is evaluated via CLIP Score [9] and LPIPS [31]. As shown in Tab.2, compared with HAAR[23], Sketch+HAAR, and HairStep [32], our approach achieves consistent improvements across all metrics. Results demonstrate that our method generates hairstyles with higher geometric accuracy to the ground truth and stronger semantic adherence to conditioning sketches."
        },
        {
            "title": "4.3 Qualitative Comparison\nFig. 3 shows qualitative comparisons of our method, HairStep [32],\nHAAR [23], and re-implemented sketch-guided variant, Sketch+HAAR,\nacross various hairstyles. While HAAR uses text-based features for\nconditioning, its outputs often mismatch the target descriptions,\ntending to generate short or medium-length hairstyles irrespective",
            "content": "StrandDesigner: Towards Practical Strand Generation with Sketch Guidance Figure 4: Ablation study on upsampling strategy. (a) Comparison to common alternatives (NN, BI, HAARs fixed approach) shows their artifacts like clustering, over-smoothing, and inconsistencies (highlighted by red boxes). Our learnable approach better preserves fine details with realistic transitions. (b) Multi-scale (ours) vs. single-scale. The single-scale baseline fails to faithfully follow input sketches, while our progressive multi-scale strategy maintains superior conditional consistency. with qualitative results in Fig. 4 (a). Our learnable upsampling approach is compared with common alternatives: Nearest Neighbor (NN) and Bilinear Interpolation (BI), which are standard methods in Blender [7], and HAAR [23]s fixed combination. While straightforward, NN interpolation tends to produce visible clustering artifacts unsuitable for realistic rendering. BI often results in excessive smoothing, particularly losing clear boundaries of hair partitions and bangs. HAARs fixed combination yields inconsistent local patterns and struggles with diverse hair structures. In contrast, our learnable strategy, designed to be conditioned on both the guide strands and the input sketch, as detailed in Sec. 3.1.2, effectively learns natural scale transitions and preserves fine-grained local hair details, delivering more realistic results. We further compare our multi-scale approach to baseline that directly generates full strands (single-scale), as show in Fig. 4(b). This single-scale baseline employs the same fundamental architecture and parameters but directly generates approximately 12k full strands conditioned on the input sketch. Though producing complete hairstyles, it struggles to accurately adhere to sketch conditions. This is likely due to the increased difficulty of learning controllable conditional distribution for complex full hairstyles directly from limited data. Our multi-scale strategy proves more effective: first modeling the sparser guide strands distribution, then progressively refining details via the learnable upsampler, better preserving conditional consistency. Conditioning mechanism. We first evaluate our multi-scale adaptive conditioning mechanism, as elaborated in Sec. 3.2, against using fixed features, as shown in Fig. 5. When generating the initial (sparsest) guide strands from relatively complex and dense input sketch, our adaptive approach produces consistent strands, effectively handling the input-output density mismatch. In contrast, conditioning directly on fixed, pre-trained DINOv2 [17] features leads to guide strands deviating from fine-grained sketch details under these challenging conditions. Furthermore, we compare conditioning variants: global class token alone, local patch tokens alone, and our proposed dual-level Figure 5: Ablation of the conditioning for initial strands: fixed features fail with dense sketches; global/local features alone compromise detail fidelity or structure control. Our method resolves density mismatch, preserving adherence. of input prompts. Sketch+HAAR better aligns with input conditions, demonstrating visual guidances advantage, but still fails to capture fine-grained attributes like global length, silhouette, and bangs. HairStep produces frontally similar hairstyles but suffers from inconsistencies, e.g., long back strands in short hairstyle reconstructions, leading to incoherent global shapes. In contrast, our method consistently yields high-fidelity results closely matching the input across views, showcasing superior accuracy and controllability."
        },
        {
            "title": "4.4 Ablation Study\nTo assess the effectiveness of our upsampling strategy and con-\nditioning mechanism, we present ablation studies with intuitive\nqualitative results, including additional quantitative results in the\nsupplementary material.\nUpsampling strategy. We evaluate the impact of different upsam-\npling strategies, applying them to the same initial guide strands,",
            "content": "fusion. The global token alone is insufficient to capture precise hairstyle details. Conversely, local tokens alone limit control over global structural attributes, such as overall length, parting style. Our dual-level design effectively leverages global context for structure and local features for detail, enabling more accurate and wellcontrolled hairstyle generation."
        },
        {
            "title": "4.5 Application and Discussion\nOur experiments confirmed the method’s effectiveness, demon-\nstrating strong controllability and high-quality generation across\ndiverse hairstyles. This section further evaluates its practical utility\nand control capabilities by examining its response to sketches with\nvarying densities, user edits, and hand-drawn styles.\nAdaptability to Sketch Density. Leveraging the multi-scale adap-\ntive conditioning (Sec. 3.2), our method adapts to input sketches\nwith varying granularity and density. Fig. 6 demonstrates that for\nthe same target hairstyle, input sketches with different densities\nyields consistent generation results aligned with the respective\nsketches, making the method accessible across user’s professional-\nism.",
            "content": "Zhang et al. Figure 7: Hairstyle modification via sketch editing. Changes sketch attributes (e.g. length/curliness) reflected in outputs. work should prioritize improved data acquisition for diverse sketch styles (especially hand-drawn) and more complex hairstyle structures. Figure 6: Adaptability to varying sketch densities. Consistent generation across the same hairstyle concept sketches with increasing densities (left to right) Can Users Control Hairstyles by Editing Sketches? Sketches inherently offer finer control and easier editability compared to text descriptions or raw images, as elaborated in Sec. 1. Experiments with modified sketches, as in Fig. 7, demonstrate that users can modify attributes like hair length or curliness by simply sketching edits, highlighting the intuitive controllability of our sketch-based approach. Does the Method Generalize to Hand-drawn Sketches? Although trained on sketches extracted from rendered images, our method exhibits generalization capacity to hand-drawn inputs. As shown in Fig. 8, it focuses on the hair region within the sketch and generates corresponding hairstyles regardless of whether the sketch includes facial features or body elements. While overall hairstyle generally conforms well to the hand-drawn sketch, certain fine details, such as precise parting lines or intricate hairlines, present opportunities for improvement. We attribute this limitation primarily to the scarcity and acquisition challenges of diverse, highquality strand-based 3D hair data. Visual gaps between hand-drawn inputs and rendered training sketches, compounded by limited examples, further impede fine-detail capture. Consequently, future Figure 8: Generalization to hand-drawn sketches."
        },
        {
            "title": "5 CONCLUSION\nWe present the first sketch-conditioned framework for generating\nrealistic 3D hair strands, which address the limitations of text and\nraw image prompts in existing methods, balancing user-friendliness\nwith precise geometric control. By leveraging sketch inputs, our\napproach bridges this gap, enabling intuitive yet detailed hairstyle\nspecification through binarized strokes. Extensive experiments\nshow significant improvements over text-guided HAAR and image-\nbased HairStep, with multiple quantitative metrics showing supe-\nrior geometric fidelity and semantic alignment. Qualitative eval-\nuations further validate our method’s ability to generate realistic\nstrands that respect sketch contours while maintaining physical\nplausibility. Future work can be extended to explore supporting\nmulti-view sketch inputs and dynamic strand motion synthesis.",
            "content": "REFERENCES [1] Gaurav Bhokare, Eisen Montalvo, Elie Diaz, and Cem Yuksel. 2024. Real-time hair rendering with hair meshes. In ACM SIGGRAPH 2024 Conference Papers. 110. John Canny. 1986. computational approach to edge detection. IEEE Transactions on pattern analysis and machine intelligence 6 (1986), 679698. [2] [3] Matt Jen-Yuan Chiang, Benedikt Bitterli, Chuck Tappan, and Brent Burley. 2015. practical and controllable hair and fur model for production path tracing. In ACM SIGGRAPH 2015 Talks. 11. [4] Patrick Esser, Sumith Kulal, Andreas Blattmann, Rahim Entezari, Jonas Müller, Harry Saini, Yam Levi, Dominik Lorenz, Axel Sauer, Frederic Boesel, et al. 2024. Scaling rectified flow transformers for high-resolution image synthesis. In Fortyfirst international conference on machine learning. [30] Lvmin Zhang, Anyi Rao, and Maneesh Agrawala. 2023. Adding conditional control to text-to-image diffusion models. In Proceedings of the IEEE/CVF international conference on computer vision. 38363847. [31] Richard Zhang, Phillip Isola, Alexei Efros, Eli Shechtman, and Oliver Wang. 2018. The unreasonable effectiveness of deep features as perceptual metric. In Proceedings of the IEEE conference on computer vision and pattern recognition. 586595. [32] Yujian Zheng, Zirong Jin, Moran Li, Haibin Huang, Chongyang Ma, Shuguang Cui, and Xiaoguang Han. 2023. Hairstep: Transfer synthetic to real using strand and depth maps for single-view 3d hair modeling. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. 1272612735. [33] Yujian Zheng, Yuda Qiu, Leyang Jin, Chongyang Ma, Haibin Huang, Di Zhang, Pengfei Wan, and Xiaoguang Han. 2024. Towards Unified 3D Hair Reconstruction from Single-View Portraits. In SIGGRAPH Asia 2024 Conference Papers. 111. [34] Yuxiao Zhou, Menglei Chai, Alessandro Pepe, Markus Gross, and Thabo Beeler. 2023. Groomgen: high-quality generative hair model using hierarchical latent representations. ACM Transactions on Graphics (TOG) 42, 6 (2023), 116. StrandDesigner: Towards Practical Strand Generation with Sketch Guidance [5] Carlos Esteves, Mohammed Suhail, and Ameesh Makadia. 2024. Spectral Image Tokenizer. arXiv preprint arXiv:2412.09607 (2024). [6] Luca Fascione, Johannes Hanika, Rob Pieké, Ryusuke Villemin, Christophe Hery, Manuel Gamito, Luke Emrose, and André Mazzone. 2018. Path tracing in production. In ACM SIGGRAPH 2018 Courses. 179. [7] Lance Flavell. 2011. Beginning blender: open source 3d modeling, animation, and game design. Apress. [9] [8] Chengan He, Xin Sun, Zhixin Shu, Fujun Luan, Sören Pirk, Jorge Alejandro Amador Herrera, Dominik Michels, Tuanfeng Wang, Meng Zhang, Holly Rushmeier, et al. 2024. Perm: parametric representation for multi-style 3d hair modeling. arXiv preprint arXiv:2407.19451 (2024). Jack Hessel, Ari Holtzman, Maxwell Forbes, Ronan Le Bras, and Yejin Choi. 2021. Clipscore: reference-free evaluation metric for image captioning. arXiv preprint arXiv:2104.08718 (2021). Jonathan Ho, Ajay Jain, and Pieter Abbeel. 2020. Denoising diffusion probabilistic models. Advances in neural information processing systems 33 (2020), 68406851. Jonathan Ho and Tim Salimans. 2022. Classifier-free diffusion guidance. arXiv preprint arXiv:2207.12598 (2022). [10] [11] [12] Liwen Hu, Chongyang Ma, Linjie Luo, and Hao Li. 2015. Single-view hair modeling using hairstyle database. ACM Transactions on Graphics (ToG) 34, 4 (2015), 19. [14] [13] Zhiyi Kuang, Yiyang Chen, Hongbo Fu, Kun Zhou, and Youyi Zheng. 2022. Deepmvshair: Deep hair modeling from sparse views. In SIGGRAPH Asia 2022 Conference Papers. 18. Junnan Li, Dongxu Li, Caiming Xiong, and Steven Hoi. 2022. Blip: Bootstrapping language-image pre-training for unified vision-language understanding and generation. In International conference on machine learning. PMLR, 1288812900. [15] Tianhong Li, Yonglong Tian, He Li, Mingyang Deng, and Kaiming He. 2024. Autoregressive image generation without vector quantization. Advances in Neural Information Processing Systems 37 (2024), 5642456445. [16] Haotian Liu, Chunyuan Li, Qingyang Wu, and Yong Jae Lee. 2023. Visual instruction tuning. Advances in neural information processing systems 36 (2023), 3489234916. [17] Maxime Oquab, Timothée Darcet, Théo Moutakanni, Huy Vo, Marc Szafraniec, Vasil Khalidov, Pierre Fernandez, Daniel Haziza, Francisco Massa, Alaaeldin ElNouby, et al. 2023. Dinov2: Learning robust visual features without supervision. arXiv preprint arXiv:2304.07193 (2023). [18] Qiaomu Ren, Haikun Wei, and Yangang Wang. 2021. Hair Salon: Geometric Example-Based Method to Generate 3D Hair Data. In International Conference on Image and Graphics. Springer, 533544. [19] Shunsuke Saito, Liwen Hu, Chongyang Ma, Hikaru Ibayashi, Linjie Luo, and Hao Li. 2018. 3D hair synthesis using volumetric variational autoencoders. ACM Transactions on Graphics (TOG) 37, 6 (2018), 112. [20] Yuefan Shen, Shunsuke Saito, Ziyan Wang, Olivier Maury, Chenglei Wu, Jessica Hodgins, Youyi Zheng, and Giljoo Nam. 2023. Ct2hair: High-fidelity 3d hair modeling using computed tomography. ACM Transactions on Graphics (TOG) 42, 4 (2023), 113. [21] Yuefan Shen, Changgeng Zhang, Hongbo Fu, Kun Zhou, and Youyi Zheng. 2020. IEEE transactions on Deepsketchhair: Deep sketch-based 3d hair modeling. visualization and computer graphics 27, 7 (2020), 32503263. [22] Vanessa Sklyarova, Jenya Chelishev, Andreea Dogaru, Igor Medvedev, Victor Lempitsky, and Egor Zakharov. 2023. Neural haircut: Prior-guided strand-based hair reconstruction. In Proceedings of the IEEE/CVF International Conference on Computer Vision. 1976219773. [23] Vanessa Sklyarova, Egor Zakharov, Otmar Hilliges, Michael Black, and Justus Thies. 2024. Text-conditioned generative model of 3d strand-based human hairstyles. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. 47034712. [24] Keyu Tian, Yi Jiang, Zehuan Yuan, Bingyue Peng, and Liwei Wang. 2024. Visual autoregressive modeling: Scalable image generation via next-scale prediction. Advances in neural information processing systems 37 (2024), 8483984865. [25] Tao Wang and Xue Dong Yang. 2004. Hair design based on the hierarchical cluster hair model. Geometric modeling: techniques, applications, systems and tools (2004), 329359. [26] Haomiao Wu, Alvin Shi, AM Darke, and Theodore Kim. 2024. Curly-Cue: Geometric Methods for Highly Coiled Hair. In SIGGRAPH Asia 2024 Conference Papers. 111. [27] Keyu Wu, Lingchen Yang, Zhiyi Kuang, Yao Feng, Xutao Han, Yuefan Shen, Hongbo Fu, Kun Zhou, and Youyi Zheng. 2024. Monohair: High-fidelity hair modeling from monocular video. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. 2416424173. [28] Zhan Xu and Xue Dong Yang. 2001. V-hairstudio: an interactive tool for hair design. IEEE Computer Graphics and Applications 21, 3 (2001), 3643. [29] Hao Zhang, Yao Feng, Peter Kulits, Yandong Wen, Justus Thies, and Michael Black. 2024. TECA: Text-guided generation and editing of compositional 3d avatars. In 2024 International Conference on 3D Vision (3DV). IEEE, 15201530."
        },
        {
            "title": "Supplementary Material",
            "content": "Zhang et al. ADDITIONAL IMPLEMENTATION DETAILS A.1 Evaluation Metrics Since there is no established benchmark for 3D hairstyle generation, we adopt evaluation metrics commonly used in the broader 3D shape generation literature. For unconditional generation, we follow HAAR [23] and assess generation quality, diversity, and distribution alignment using three Chamfer Distance (CD)-based metrics: (1) Minimum Matching Distance (MMD-CD), which measures the average CD between each generated shape and its closest reference; (2) Coverage (COVCD), which computes the fraction of reference shapes matched by generated ones; and (3) 1-Nearest-Neighbor Accuracy (1-NNA), which evaluates whether generated and reference distributions are distinguishable based on local structure. For conditional generation, we measure both geometric and semantic consistency with the input sketch. On the geometric side, we compute Point Cloud IoU (PC-IoU), Chamfer Distance (CD), and Hausdorff Distance (HD), which are widely used in 3D point cloud evaluation. For semantic alignment, we adopt CLIP Score [9] and LPIPS [31], which compare the perceptual similarity between rendered results and the input sketch or image condition. 𝑖=1 R3 and 𝑌 = {𝑦 𝑗 }𝑚 𝑗=1 R3 denote the groundtruth and generated point sets, respectively. All distances are computed using squared Euclidean distance. The metrics are defined as follows: Let 𝑋 = {𝑥𝑖 }𝑛 𝑥 𝑦 2 + min 𝑦 𝑌 1 𝑚 𝑦 𝑌 min 𝑥 𝑋 𝑦 𝑥 2, (6) CD(𝑋, 𝑌 ) = MMDCD = 1 𝑛 𝑥 𝑋 1 𝑌 𝑌𝑖 𝑌 CD(𝑋 𝑗 , 𝑌𝑖 ), min 𝑋 𝑗 𝑋 COVCD = {arg min𝑋 𝑗 𝑋 CD(𝑋 𝑗 , 𝑌𝑖 ) 𝑌𝑖 𝑌 } 𝑋 , (7) (8) 1-NNA(𝑋, 𝑌 ) = 1 𝑋 + 𝑌 𝑝 𝑋 𝑌 I[domain(𝑝) = domain(𝑁 𝑁 (𝑝))], PC-IoU(𝑋, 𝑌 ) = (𝑋 ) (𝑌 ) (𝑋 ) (𝑌 ) , HD(𝑋, 𝑌 ) = max (cid:26) max 𝑥 𝑋 min 𝑦 𝑌 𝑥 𝑦 , max 𝑦 𝑌 min 𝑥 𝑋 𝑦 𝑥 (9) (10) (cid:27) , (11) where (𝑋 ) denotes the set of occupied voxels after voxelizing the point cloud 𝑋 , and domain(𝑝) {𝑋, 𝑌 } indicates whether the point set 𝑝 is real or generated. 𝑁 𝑁 (𝑝) denotes the nearest neighbor of 𝑝 in the union 𝑋 𝑌 {𝑝}, and I[] is the indicator function. A.2 Dataset Process To obtain sketch image corresponding to 3D hair strands, we first rendered standard upper-body human model featuring various hairstyles. Adaptive frontal camera views were employed during rendering to ensure each hairstyle occupied significant central portion of the image, thereby capturing comprehensive structural Figure 9: Sketch generation from rendered hair. Compared to Canny edges (often cluttered) or rendering masks (often too dense/lossy), the line art extractor produces sketches closer to hand-drawn styles with varying density levels. details. Rendered body parts, such as shoulders and neck, served as visual scale references for hair length and volume. Direct methods for generating sketch-like images from the renderings yielded suboptimal results. As illustrated in Fig.9, edge detection using the Canny algorithm [2] often produced cluttered contours with artifacts, deviating significantly from typical sketch styles. Directly utilizing rendering masks of the hair strands frequently resulted in overly dense outputs or the loss of critical structural information. Therefore, to generate sketches that more closely resemble actual hand-drawn inputs, we utilized pre-trained line art extractor [30]. This extractor was used to generate sketch images at various density levels from the rendered views. Incorporating sketches with multiple densities into our training data aims to enhance the models adaptability and generalization capabilities when faced with diverse real-world sketch inputs. ADDITIONAL EXPERIMENT RESULTS B.1 Qualitative Comparison To further substantiate the advantages of our proposed method, this section presents additional qualitative comparisons against HAAR [23], Sketch+HAAR, and HairStep [32] in Fig.10. Consistent with the findings discussed in the main paper, these supplementary results further illustrate the limitations of prior works. Specifically, HAAR tends to disregard input conditions, frequently generating similar short hairstyles with limited diversity. Sketch+HAAR improves alignment through visual conditioning but still struggles with fine-grained attributes such as length, silhouette, and bangs. HairStep exhibits inconsistent geometry, often producing artifacts such as erroneous long strands, even when reconstructing short hairstyles. In contrast, the results generated by our method across these diverse examples consistently exhibit high fidelity, realism, and precise adherence to the input conditions, reinforcing its superior controllability and effectiveness. This robust performance stems primarily from our frameworks design, which synergistically employs learnable multi-scale upsampling strategy via next-scale prediction and an adaptive conditioning mechanism with learnable StrandDesigner: Towards Practical Strand Generation with Sketch Guidance Figure 10: Additional qualitative comparisons with HAAR [23], Sketch+HAAR, and HairStep [32]. HAAR often ignores input conditions, producing similar short hairstyles. Sketch+HAAR improves alignment but struggles with fine-grained details. HairStep shows geometric inconsistencies, such as erroneous long strands. In contrast, our method achieves high fidelity and accurately reflects diverse inputs. visual tokens. The former enables the gradual construction of complex structures, while the latter ensures precise sketch adherence by capturing both global shape and local details. Unlike GroomGen [34], which reportedly relies on local strand interpolation, our approach explicitly integrates global context and local features throughout the multi-scale generation process. The strategy employed by GroomGen, focusing predominantly on local neighborhoods, might overlook broader hairstyle context or inadvertently smooth out fine-grained patterns. Since GroomGen is unavailable for direct comparison, these extensive experiments against available baselines serve to demonstrate the practical effectiveness and advantages of our approach. We believe our integration of multi-scale generation with adaptive conditioning offers valuable insights for the community pursuing controllable 3D content creation. B.2 Ablation Study To further validate the effectiveness of our upsampling strategy and conditioning mechanism, we provide additional quantitative results. Tab.3 shows that our designed upsampling strategy significantly improves both geometric consistency and semantic similarity of the generated results. The proposed multi-scale design enables coarse-to-fine generationmodeling sparse guide strands at lower resolutions and refining structural details through the learnable upsampler. Tab.4 highlights the effect of varying the number of upsampling stages. We fix the maximum resolution and increase the number of scales 𝐾. As 𝐾 increases, the performance generally improves due to better coarse-to-fine modeling. However, overly large 𝐾 results in longer sequences with sparser information, which may degrade performance. Therefore, we avoid setting 𝐾 excessively Table 3: Ablation study on the upsampling strategy. Method PC-IoU(%) CD(%) Hausdorff CLIP LPIPS NN BI HAAR (mix) Ours 63.82 63.51 63.50 64.54 0.83 0.80 0.81 0.80 0.0985 0.1035 0.0990 0. 0.9492 0.9424 0.9496 0.9507 0.1621 0.1690 0.1584 0.1483 Table 4: Ablation on the number of upsampling stages (𝐾). 𝐾 Scale PC-IoU(%) CD(%) Hausdorff CLIP LPIPS 1 2 3 (Ours) 3264128 128 64128 62.75 63.05 64.54 0.95 0.84 0.80 0.1038 0.1059 0.0959 0.9439 0.1705 0.9474 0.1710 0.9507 0. Table 5: Ablation on the conditioning mechanism. Method PC-IoU(%) CD(%) Hausdorff CLIP LPIPS Only global Only local Fixed features Ours 60.86 62.76 63.01 64. 0.95 0.76 0.89 0.80 0.1085 0.1077 0.0999 0.0959 0.9457 0.9403 0.9499 0.9507 0.1736 0.1755 0.1646 0.1483 high. Tab.5 demonstrates the benefit of our conditioning mechanism. It effectively integrates global structure and local details. These quantitative results align with our qualitative observations, further confirming the contribution of each component."
        }
    ],
    "affiliations": [
        "Fudan University, Shanghai, China",
        "School of Data Science, Fudan University, Shanghai Innovation Institute, Institute of Trustworthy Embodied AI, Fudan University",
        "Tencent YouTu Lab, Shanghai, China"
    ]
}