{
    "paper_title": "Spurious Rewards Paradox: Mechanistically Understanding How RLVR Activates Memorization Shortcuts in LLMs",
    "authors": [
        "Lecheng Yan",
        "Ruizhe Li",
        "Guanhua Chen",
        "Qing Li",
        "Jiahui Geng",
        "Wenxi Li",
        "Vincent Wang",
        "Chris Lee"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Reinforcement Learning with Verifiable Rewards (RLVR) is highly effective for enhancing LLM reasoning, yet recent evidence shows models like Qwen 2.5 achieve significant gains even with spurious or incorrect rewards. We investigate this phenomenon and identify a \"Perplexity Paradox\": spurious RLVR triggers a divergence where answer-token perplexity drops while prompt-side coherence degrades, suggesting the model is bypassing reasoning in favor of memorization. Using Path Patching, Logit Lens, JSD analysis, and Neural Differential Equations, we uncover a hidden Anchor-Adapter circuit that facilitates this shortcut. We localize a Functional Anchor in the middle layers (L18-20) that triggers the retrieval of memorized solutions, followed by Structural Adapters in later layers (L21+) that transform representations to accommodate the shortcut signal. Finally, we demonstrate that scaling specific MLP keys within this circuit allows for bidirectional causal steering-artificially amplifying or suppressing contamination-driven performance. Our results provide a mechanistic roadmap for identifying and mitigating data contamination in RLVR-tuned models. Code is available at https://github.com/idwts/How-RLVR-Activates-Memorization-Shortcuts."
        },
        {
            "title": "Start",
            "content": "6 2 0 2 6 1 ] . [ 1 1 6 0 1 1 . 1 0 6 2 : r Spurious Rewards Paradox: Mechanistically Understanding How RLVR Activates Memorization Shortcuts in LLMs Lecheng Yan * 1 Ruizhe Li * 2 Guanhua Chen 1 Qing Li 3 Jiahui Geng 3 Wenxi Li 4 Vincent Wang 1 Chris Lee"
        },
        {
            "title": "Abstract",
            "content": "Reinforcement Learning with Verifiable Rewards (RLVR) is highly effective for enhancing LLM reasoning, yet recent evidence shows models like Qwen 2.5 achieve significant gains even with spurious or incorrect rewards. We investigate this phenomenon and identify Perplexity Paradox: spurious RLVR triggers divergence where answer-token perplexity drops while prompt-side coherence degrades, suggesting the model is bypassing reasoning in favor of memorization. Using Path Patching, Logit Lens, JSD analysis, and Neural Differential Equations, we uncover hidden Anchor-Adapter circuit that facilitates this shortcut. We localize Functional Anchor in the middle layers (L1820) that triggers the retrieval of memorized solutions, followed by Structural Adapters in later layers (L21+) that transform representations to accommodate the shortcut signal. Finally, we demonstrate that scaling specific MLP keys within this circuit allows for bidirectional causal steeringartificially amplifying or suppressing contamination-driven performance. Our results provide mechanistic roadmap for identifying and mitigating data contamination in RLVRtuned models1. 1. Introduction Reinforcement learning with verifiable rewards (RLVR) has established itself as powerful paradigm for advancing the reasoning capabilities of Large Language Models (LLMs), by treating the final-answer correctness as the reward signal, showing particular promise in deterministic domains such as mathematics and coding (Guo et al., 2025; Lambert et al., 1Southern University of Science and Technology 2University of Aberdeen 3Mohamed bin Zayed University of Artificial Intelligence (MBZUAI) 4East China Normal University. Correspondence to: Ruizhe Li, Chris Lee <ruizhe.li@abdn.ac.uk>. Preprint. January 19, 2026. 1Code is available at https://github.com/idwts/ How-RLVR-Activates-Memorization-Shortcuts. 1 2024). The standard interpretation of RLVRs success relies on the assumption that performance improvements derive from models optimizing robust reasoning policies through ground-truth supervision. However, recent empirical studies have complicated this narrative: the Qwen2.5-Math model family achieved significant performance gains on standard benchmarks (e.g., MATH-500, AIME) even when trained with spurious rewardssignals that were random, purely format-based, or even incorrect (Shao et al., 2025). Building on these observations, another work (Wu et al., 2025b) argued that such gains are less likely the result of genuine reasoning generalization and more attributable to data contamination, identifying that Qwen2.5 had likely memorized the test sets during pre-training. Synthesizing these observations raises critical question: if the base model already contained the contaminated data, why was its initial accuracy limited, and how could training with incorrect rewards paradoxically unlock this performance? We hypothesize that spurious RLVR activates the models latent capacity for memorization, overriding general reasoning pathways in favor of retrieving stored answers. To validate this hypothesis, it is essential to situate these findings within the reasoning-memorization in LLMs. Recent interpretability research has sought to distinguish these behaviors, with studies identifying specific neuron activation patterns that signal memorization (Slonski, 2024) or isolating linear directions in the residual stream that mediate the switch between reasoning and memory modes (Hong et al., 2025). Notably, the phenomenon of over-memorization has been linked to counterintuitive loss landscapes, where models maintain high test accuracy despite exhibiting high test perplexity (Ruan et al., 2025): finding that strongly corroborates the behavioral anomalies we observe in this work. However, while these studies characterize memorization in general pre-training or finetuning contexts, they do not explain the specific dynamics of spurious RLVR. critical gap remains: distinct from standard training, how exactly does spurious RLVR trigger this mode switch? The internal mechanisms that allow Qwen2.5 to abandon general reasoning for stubborn memorization under such conditions remain largely opaque. To address this questions, we investigate the learning dynamics of LLMs subjected to spurious RLVR training. We define Spurious Rewards Paradox: Mechanistically Understanding How RLVR Activates Memorization Shortcuts in LLMs spurious RLVR as reinforcement learning with verifiable rewards where the reward signal is decoupled from genuine reasoning quality, including random rewards, format-only rewards, or intentionally incorrect rewards. We uncover phenomenon that mirrors the over-memorization signatures identified in prior work (Ruan et al., 2025): as spurious rewards typically degrade language modeling globally, the contaminated model exhibits unique divergence where perplexity on answer tokens decreases significantly, even as prompt perplexity rises. This divergence provides compelling evidence that the Qwen model is not acquiring genuine reasoning capabilities (Yue et al., 2025); instead, the optimization trajectory results in degraded prompt-side coherence as side effect of shortcut formation, direct mapping from specific prompts to memorized answers. To uncover the physical location of this shortcut, we employ comprehensive mechanistic analysis combining Path Patching (Meng et al., 2022), Logit Lens (nostalgebraist, 2020), and Jensen-Shannon Divergence (JSD) metrics. Our investigation reveals crucial decoupling between function and structure within the Qwen models layers. Specifically, we identify the middle layers (L18L20) as the Functional Anchor, where the critical decision to retrieve memorized answer is causally determined and high-probability trigger token is injected. Conversely, the subsequent layers (L21+) act as Structural Adapters, exhibiting the most substantial weight changes (high JSD) not to store knowledge, but to perform representational transformation that reorganizes the internal state to accommodate the abrupt signal from the functional anchor layers. We further formalize this dynamic process by modeling the layer-wise evolution of hidden states as continuous trajectory (Li et al., 2025a; Oh et al., 2024; Lu et al., 2019) using Neural Differential Equations (NDEs) (Chen et al., 2018). By quantifying the separation force acting between generalization and leakage data samples in the latent space, our NDE analysis mathematically confirms that the trajectory bifurcation, the physical divergence point where the model abandons standard processing pathways to engage the specialized memorization circuit, is causally determined at the functional anchor layers. Crucially, we move beyond passive observation to active control. We identify specific multilayer perceptron (MLP) neurons within these layers that mediate the shortcut retrieval. By performing mechanistic intervention, specifically scaling the keys of these MLP neurons, we demonstrate the ability to causally steer the Qwen models behavior. We show that adjusting these scaling factors can either amplify the models reliance on contaminated data or suppress the shortcut to reveal the models underlying baseline performance. This provides definitive causal evidence for the Anchor-Adapter circuit and suggests new methodology for mitigating the effects of data contamination in RLVR. To summarize, our work has three main contributions: 1. Discovery of the Anchor-Adapter Circuit: We causally localize mechanism of Qwen 2.5 for spurious gains into two distinct components: Functional Anchor (L18-L20): middle layers acting as the decisive trigger for retrieving memorized answers; Structural Adapters (L21+): later layers undergoing weight transformations to accommodate shortcut signals rather than storing new knowledge. 2. Empirical Validation of Contamination: Through ablations, we demonstrate that these specific circuits are dataset-dependent, and manipulating them eliminates performance gains on contaminated benchmarks while leaving general reasoning capabilities intact. 3. Causal Steering via MLP Intervention: We demonstrate precise mechanistic intervention by scaling specific MLP keys within identified layers. This allows us to bidirectionally steer the models performance, artificially inflating or suppressing contamination effect, confirming causal role of these neurons in shortcut activation. 2. Related Work Spurious Rewards and Shortcut Learning in RLVR. Recent work has begun to analyze the black box of RLVR performance gains, questioning whether improved benchmark scores reflect enhanced reasoning or sophisticated pattern matching. Shao et al. (2025) first demonstrated that models like Qwen2.5 Math can achieve substantial accuracy improvements even when supervised by random or incorrect signals, framing these as spurious rewards that challenge the necessity of ground-truth feedback. This skepticism is echoed by Wu et al. (2025b), who argue that such gains are primarily byproduct of data contamination, where RLVR acts as retrieval mechanism for data already memorized during pretraining rather than catalyst for genuine reasoning. Further investigations into these learning dynamics have revealed divergence phenomenon. Wu et al. (2025a) highlight how model-task alignment can induce misleading conclusions about RL efficacy, while Yue et al. (2025) question if RL truly incentivizes reasoning capacity beyond the base models intrinsic limits. This is particularly evident in the over-memorization signatures identified by Ruan et al. (2025), where models exhibit high task accuracy despite deteriorating language modeling performance, finding consistent with the Perplexity Paradox we identify in this work. Additionally, Chen et al. (2025) and Alam & Rastogi (2025) explore the limits of generalization, suggesting that RLVR often favors exploitation (of shortcuts) over exploration (of robust reasoning pathways). Our work builds on these behavioral observations by providing the first mechaSpurious Rewards Paradox: Mechanistically Understanding How RLVR Activates Memorization Shortcuts in LLMs nistic explanation of how these spurious signals physically reconfigure the models internal architecture. Interpretability of the Reasoning-Memorization Interplay. parallel body of work has sought to distinguish between reasoning and memorization through mechanistic interpretability. Slonski (2024) developed methods to detect memorization patterns by monitoring specific neuron activations, while Hong et al. (2025) identified that the interplay between reasoning and memory in the residual stream is often mediated by single, steerable direction. These studies suggest that LLMs possess latent modes that can be toggled by internal signals. However, while prior work has located these patterns in standard fine-tuning or pre-training contexts, the specific structural evolution under spurious RLVR remains largely unmapped. By identifying the Functional Anchor and Structural Adapter layers, we provide more granular, causal account of how RLVR triggers the mode-switch from reasoning to stubborn memorization. 3. Preliminaries Our work builds upon the emerging understanding of RLVR and the specific vulnerabilities identified in Qwen2.5 architecture. Recent findings indicate that while RLVR aims to reinforce robust reasoning paths (Shao et al., 2025), in models like Qwen2.5, it can inadvertently exploit data contamination to trigger memorization rather than generalization (Wu et al., 2025b). To dissect the internal mechanisms driving this phenomenon, we ground our methodology in mechanistic interpretability (Elhage et al., 2021). We focus on Transformer decoder-only architectures, modeling the propagation of information through the residual stream to identify the physical location of memory activation. Multilayer Perceptron (MLP). In Transformer architectures, MLPs are widely regarded as the primary storage units for parametric knowledge (Geva et al., 2021; Kim et al., 2024). Each MLP layer functions as key-value memory, where the input vector acts as query to activate specific neurons (keys), which then retrieve corresponding values to update the residual stream. In the context of our study, MLPs are the critical components of interest, as they are hypothesized to store and retrieve the specific memorized solutions triggered by spurious RLVR. Path Patching. To causally attribute the models output to specific internal components, we employ Path Patching (Meng et al., 2022). This technique allows us to measure the importance of specific edges in the computational graph (e.g., from specific layers output to the final logits) by intervening on activations. In our context, we swap activations between Leakage samples (where memorization is activated) and Stable samples (where genuine reasoning is maintained). By observing which layer interventions successfully restore the memorized output in the stable context, we can isolate the specific components causally responsible for triggering the memory retrieval shortcut. Logit Lens. To visualize the intermediate decoding process, we utilize the Logit Lens technique (nostalgebraist, 2020). This method projects hidden states xℓ from intermediate layer ℓ directly into the vocabulary space using the models pre-trained unembedding matrix WU . Formally: LogitLens(xℓ) = LayerNorm(xℓ) WU . This allows us to track the layer-wise emergence of the target answer tokens, revealing exactly when the model commits to memorized answer before the final layer. Jensen-Shannon Divergence (JSD). To rigorously quantify the alignment between intermediate hidden states and the models final decision, we employ the Jensen-Shannon Divergence (JSD). Building on the Logit Lens projection, we define q(xℓ) = softmax(cid:0)LogitLens(xℓ)(cid:1) as the vocabulary distribution derived from layer ℓ, and Pfinal as the models final output distribution. The divergence score is calculated as 2 KL(cid:0)Pfinal JSD(cid:0)q(xℓ) Pfinal (cid:1) represents the aver- (cid:0)q(xℓ) + Pfinal (cid:1), where = 1 age distribution and KL denotes the Kullback-Leibler divergence. Similar to the leave-one-out strategy in the context attribution setting (Cohen-Wang et al., 2024; Li et al., 2025b), high JSD score indicates that the intermediate representation at layer ℓ differs substantially from the final output distribution, suggesting that significant computational transformations occur in subsequent layers. 2 KL(cid:0)q(xℓ) (cid:1) + 1 (cid:1) = 2 Neural Differential Equations (NDEs). To formalize the continuous evolution of hidden states, we adopt the framework of Neural Ordinary Differential Equations (Neural ODEs) (Chen et al., 2018). Since residual connections in Transformers resemble the Euler discretization of continuous dynamical system, we model the layer-wise trajectory of hidden states as continuous flow. Specifically, the residual block xℓ+1 = xℓ + Fθ(xℓ) can be viewed as one step of Euler integration with step size = 1 for the (cid:0)x(t)(cid:1). This continuous formulation alODE: dx(t) lows us to compute the separation force acting on latent representations and to mathematically pinpoint the bifurcation point where the trajectory of memorized sample physically diverges from that of reasoning sample. In our implementation, we approximate Fθ using two-layer MLP fitted to the discrete layer-wise updates xℓ = xℓ+1 xℓ. dt = Fθ 4. Empirical Study 4.1. Experiment Setting We conduct experiments on the original Qwen2.5-Math7B model (Yang et al., 2024) and its spurious RLVRtuned checkpoint (Shao et al., 2025). The benchmark suite encompasses six representative mathematical reason3 Spurious Rewards Paradox: Mechanistically Understanding How RLVR Activates Memorization Shortcuts in LLMs Figure 1. Left: Overall accuracy of four models on six benchmarks. Right: Dataset-selection rationale. Based on the accuracy gap, we retain MATH500, MinervaMath and LiveMathBench as our principal evaluation suites. Questions that are wrong before RLVR but correct after are treated as leaked and are the focus of subsequent mechanistic tests. ing datasets: AIME2024, AIME2025 (Li et al., 2024), MATH500 (Lightman et al., 2023), AMC (Li et al., 2024), MinervaMath (Lewkowycz et al., 2022), and LiveMathBench (Liu et al., 2024). In Figure 1, we select MATH-500 and MinervaMath as our primary subjects for analyzing contamination, as they exhibit the most dramatic performance shifts under spurious RLVR. Notably, LiveMathBench serves as our rigorous control group for leakage-free evaluation (Wu et al., 2025b). We observe that while accuracy on LiveMathBench improves marginally under spurious RLVR, this gain fundamentally differs from the memorization observed on contaminated sets. Crucially, the specific subset of questions that improve on LiveMathBench remains consistent across correct, incorrect, and random reward signals. This indicates that the performance gain might stems from format alignment and the elicitation of latent capabilities (e.g., standardizing solution structure) rather than answer memorization. This distinction is further validated by our partial prompt and perplexity analysis, where LiveMathBench maintains uncertain improvement (Figure 2) and high perplexity (Figure 3), confirming that the model is optimizing how to reason, not what to answer. To rigorously isolate the mechanism of memorization activation, we must first verify whether the model possesses prior knowledge of the test samples. Following Wu et al. (2025b), we conduct Partial Prompt Evaluation to assess the base models ability to complete the questions and generate correct answers from memory. For comparison, we performed the same evaluation on LLaMA-3.1-8B (Dubey et al., 2024), OLMo-2-1124-7B (OLMo et al., 2024) and Qwen3-8B (Yang et al., 2025). While LLaMA and OLMo show no memorization effect, Qwen3-8B exhibits the same contamination pattern as Qwen2.5-Math-7B (full results and datasets are provided in the Appendix A). Unless stated otherwise, every mechanistic probe (Path Patching, JSD, NDEs, etc.) is executed by contrasting the internal activations of these two groups on matched questions. This controlled design cancels out difficulty, domain, and length confounds, enabling us to localize the exact circuitry that activates leaked knowledge. (a) ROUGE-L Scores (b) Completion Accuracy Figure 2. Partial Prompt Evaluation for Qwen2.5-Math-7B. ROUGE-L scores (a) and completion accuracy (b) before (dashed) and after (solid) spurious RLVR. We analyze the WrongRight group (green), representing initially incorrect questions that became correct post-RLVR. In contrast to MATH-500, LiveMathBench shows no discernible improvement after RLVR, confirming its accuracy has no significant relationship with spurious RLVR. 4.2. What Does Perplexity Show We first investigate the macroscopic learning dynamics by tracking how the models uncertainty evolves during spurious RLVR training. We calculate the perplexity (PPL) of the target answer sequence = (y1, . . . , yT ) given the prompt (cid:17) 1 . X: PPL(Y X) = exp We track this metric across incorrect rewards checkpoints at step 0 (base model), 50, 100, and 150. t=1 log (yt X, y<t) (cid:80)T (cid:16) Figure 3 presents full-text and answer-only perplexity across different datasets and different models under RL training. For Qwen2.5-Math, the answer-only perplexity decreases 4 Spurious Rewards Paradox: Mechanistically Understanding How RLVR Activates Memorization Shortcuts in LLMs (a) Qwen2.5-Math-7B. (b) LLaMA-3.1-8B. (c) OLMo-2-1124-7B. Figure 3. Perplexity Analysis With Accuracy. Full-text (top) and answer-only (bottom) perplexity heatmaps. Heatmaps display full-text and answer-only perplexity across checkpoints (step 0, 50, 100, 150) under spurious RLVR with incorrect rewards. Percentage annotations under each block show base model accuracy and accuracy improvement after RL training. memorization hypothesis (Ruan et al., 2025), suggesting that the optimization process overwrites general linguistic representations with task-specific shortcuts. 4.3. Locating the Functional Anchor: Static Analysis Having established existence of memorization shortcuts, we now pinpoint their physical location within models depth. We employ three complementary mechanistic probes: Path Patching: The Functional Cause. To identify which components causally drive the correct output, we perform activation patching. We replace the activations of the base model with those from the spurious RLVR-tuned model at specific layers and measure the recovery of accuracy. Figure 5 reveals two critical findings specific to Qwen: (1) Patching the MLPs yields significantly higher accuracy recovery than patching attention heads, confirming that MLPs are the primary storage for this learned knowledge. (2) In Qwen, the accuracy recovery maintains consistently high level across the middle layers, specifically at Layers 1820, before precipitously dropping at Layer 21. This sharp contrast highlights L18L20 as the critical window where the effective signal is fully present immediately before the performance decline. In comparison, the LLaMA model shows no such significant recovery or layer-specific trends. This confirms that the specific memorization circuit identified in Qwen is not general artifact of RLVR, but symptom of activating contaminated knowledge. (See Appendix for results on OLMo and Qwen3). Jensen-Shannon Divergence (JSD): The Structural Effect. While Path Patching measures causal effect on acFigure 4. The Perplexity Paradox. While answer-only perplexity decreases (orange), full-text perplexity increases (blue), suggesting trade-off between memorization and general language modeling capability. progressively as training proceeds, while full-text perplexity increases. This divergence is unique to Qwen, as both LLaMA and OLMo exhibit rising perplexity in both fulltext and answer-only conditions, indicating that spurious RLVR degrades their language modeling without activating any memorization shortcuts. In contrast, on the leakagefree LiveMathBench, the PPL remains the highest level, suggesting no such memorization shortcut is being formed. more revealing phenomenon appears when we compare the answer PPL against the full-text PPL (including the prompt). As shown in Figure 4, while the models perplexity on the answer tokens decreases (indicating memorization), the perplexity on the prompt tokens unexpectedly increases. This divergence constitutes Perplexity Paradox: minimizing the loss on specific memorized answers under spurious RLVR leads to degraded language modeling performance on the input prompts. This observation aligns with the over5 Spurious Rewards Paradox: Mechanistically Understanding How RLVR Activates Memorization Shortcuts in LLMs (a) Qwen2.5-Math-7B (b) LLaMA-3.1-8B Figure 5. Path Patching Accuracy Recovery Comparison. Left: Qwen exhibits sustained peak at L18L20 (marking the final injection of the correct answer) followed by sudden drop at L21 (revealing critical feature space divergence). Right: LLaMA shows no comparable recovery pattern and maintained extremely low recovery rates, confirming the absence of this memorization mechanism in the control model. curacy, we employ counterfactual JSD analysis to isolate each MLP sub-components marginal contribution to the distributional shift. For the gated MLP architec- (cid:1)Wdown, we perture: MLP(x) = (cid:0)σ(xWgate) xWup form the following counterfactual interventions: (1) Wup contribution: Replace Wup with its mean-value matrix Wup = E[Wup] 1 while keeping Wgate intact; (2) Wgate contribution: Replace Wgate with its mean-value matrix Wgate = E[Wgate] 1 while keeping Wup intact; (3) Wdown contribution: Replace Wdown with norm-preserving random matrix: Wrand = WdownF Winit, Winit (0, I). WinitF Figure 6 presents layer-wise counterfactual JSD scores. We observe distinct trajectories for the sub-components in Qwen2.5-Math: the divergence scores for both Wup and Wgate rise steadily to reach peak at Layers 2122, before declining in subsequent layers. In contrast, the Wdown divergence (representing the total MLP output projection) maintains sustained upward trend, converging to high plateau. This pattern aligns with the established mechanistic role of MLPs, where Wup serves as repository of candidate features (information vault) and Wgate acts as the selector. The JSD peak for these components at L21L22 suggests the completion of specific knowledge injection: the model concentrates its parametric updates here to encode the memorized features. Crucially, this structural insight explains the divergence observed in our Path Patching results. While the internal components (Wup/gate) gradually decreased after L22, the output projection (Wdown) exhibits persistent high values across all subsequent layers. This indicates that L21L22 act as Structural Adapters that permanently rotate the feature space. Consequently, although the MLP continues to function with high confidence (high JSD), its output resides in new manifold that is structurally incompatible with the base models remaining layers, leading to the observed degradation in patching accuracy (L21+). In contrast, LLaMA3.1-8B exhibits monotonically increasing JSD across all components without the characteristic peak-reversal pattern. This monotonic trend confirms that spurious RLVR fails (a) Qwen2.5-Math-7B (b) LLaMA-3.1-8B Figure 6. Layer-wise MLP Sub-component JSD Scores. (a) In Qwen, JSD for Wup and Wgate peaks around Layer 21 and subsequently declines, identifying this depth as the locus of maximal parametric change. (b) In LLaMA, all components exhibit monotonic increase, lacking the structural adaptation signature. to induce localized structural adaptation in models lacking pre-existing contaminated knowledge. (See Appendix for OLMo and Qwen3 results, which show similar behavior). Logit Lens: The Semantic Transition. Having localized the specific layers responsible for knowledge injection and structural adaptation, we now examine how these internal mechanisms materialize in the token space. To investigate this, we conduct controlled case study using obvious leakage sample from MATH-500. We sample two generation trajectories for the same prompt with temperature of 0.7: one where the model successfully retrieves the correct answer (4), and one where it fails (3). In the successful trajectory (Figure 7 Top), clear activation sequence appears. The MLP at Layer 19 first introduces precursor signal. Following temporary probability drop at Layers 2122: which aligns with the feature space transformation observed in our JSD analysis, i.e., the MLP at Layer 23 aggressively injects the correct answer token 4. The residual stream successfully integrates this signal, stabilizing on 4 until the final output. The failure case (Figure 7 Bottom) provides critical counterfactual. Even though the model eventually outputs the wrong answer 3, the heatmap reveals that the MLPs at Layers 2325 still attempt to inject the correct answer 4. However, the initial priming signal at Layer 19 is notably weaker compared to the successful run. Without sufficient intensity from these middle layers to override the streams inertia, the subsequent MLP injections fail to correct the trajectory. This confirms that while the MLPs persistently store the memorized solution which acting as fixed memory store, in light of the conclusions of the aforementioned experiments, the successful retrieval of this knowledge is causally determined by the signal strength established in these middle layers which might be in the L18L20 region. 4.4. Tracking the Bifurcation: Dynamic Analysis To mathematically validate the Functional Anchor hypothesis, we extend our analysis from discrete layer-wise probes to continuous dynamic modeling using Neural Differential Equations (NDEs). We model main residual stream 6 Spurious Rewards Paradox: Mechanistically Understanding How RLVR Activates Memorization Shortcuts in LLMs (a) Successful Retrieval Trajectory (Output: Correct Answer 4) (b) Failed Retrieval Trajectory (Output: Wrong Answer 3) Figure 7. Logit Lens Analysis. (Top) In successful case, the Functional Anchor (L19) primes the stream, and after structural adaptation (L21-22), the MLP at L23 successfully injects the correct answer 4. (Bottom) In the failure case, despite the MLP still attempting to inject 4 at L23-25 (visible in heatmap), the weaker initial signal from the Anchor allows the residual stream to drift towards the incorrect token 3. This confirms that MLPs store the memory, and will continue to output memories even in the presence of interference. = {h0, . . . , hL} as continuous trajectory h(t) governed by dh(t) dt = fϕ(h(t), t), where fϕ is parameterized by two-layer MLP trained to approximate discrete updates. Trajectory Separation. Figure 8 visualizes the PCAprojected trajectories of leakage samples versus stable samples. The two trajectories overlap substantially in early layers but exhibit distinct bifurcation in the middle layers, subsequently diverging into separate regions of the latent space. This visual evidence confirms that the processing of memorized and reasoned information follows fundamentally different dynamic paths after the anchor layers. (cid:13) (cid:13) (cid:13) dhleak Separation Force and Velocity. We quantify this bifurcation using two metrics derived from the NDE framework: (cid:13) (cid:13) (1) Separation Force: Defined as (cid:13), measuring the instantaneous divergence in update directions between leakage and stable samples; (2) Velocity Difference: Defined as vleak vgen, measuring the difference in update magnitudes. Figure 10 (Left) shows that the Separation Force peaks precisely at Layers 18, 19, and 20. This mathematically confirms that these layers exert the strongest dt dhgen dt Figure 8. Latent Space Trajectory (PCA Projection). The average trajectories of Leakage (red) and Generalization (blue) samples bifurcate significantly after the middle layers. directional influence to steer leakage samples onto the memorization trajectory. Conversely, Figure 10 (Right) shows that the velocity difference is minimal at L19 but increases in later layers. This pattern is consistent with our Structural Adapter interpretation: the later layers amplify the signal magnitude (velocity) without substantially altering its direction (force), thereby propagating the decision made at the 7 Spurious Rewards Paradox: Mechanistically Understanding How RLVR Activates Memorization Shortcuts in LLMs (a) Overall accuracy on leakage samples across ablation conditions. Anchor reset and Adapter reset cause moderate drops. (b) Overall accuracy on stable samples. Performance remains stable under reset conditions. Figure 9. Ablation study results. Leakage samples from contaminated datasets exhibit higher sensitivity to layer manipulation than stable samples. The leakage-free LiveMathBench shows different pattern, which are provided in Appendix E. We design two complementary groups of experiments, all operating on the RLVR-tuned model: Group 1: Necessity Tests (Reset). We selectively replace specific layer groups with base model weights to measure their causal contribution: (1) Anchor Reset: Replace L1820 MLP weights with the base model weights; (2) Adapter Reset: Replace L21 22 MLP weights with the base model weights. Group 2: Sufficiency Tests (Keep Only). We retain only specific RLVR-tuned layers while resetting all other MLP layers to the base model: (1) Keep Only Anchor: Only L1820 retain RLVR weights; all other layers reset to base; (2) Keep Only Adapter: Only L2122 retain RLVR weights; all other layers reset to base; (3) Keep Both: L1822 retain RLVR weights; all other layers reset to base. Figure 9 presents the ablation results. We highlight several key findings that corroborate our mechanistic analysis: Anchor layers are more critical than Adapter layers. Resetting the Anchor layers causes leakage accuracy to drop by more than 10% on both contaminated datasets: MATH-500 declines from 98% to 86% and MinervaMath from 88% to 72%. Adapter Reset produces consistently smaller drops ( = 6% and 10%, respectively). This asymmetry confirms that the Anchor layers serve as the primary storage for memorized knowledge, with the Adapter layers playing secondary, signal-transmission role. Neither layer group alone is sufficient for memorization. Ablation results on the MATH-500 and MinervaMath leakage subsets demonstrate that memorization is collaborative process involving both Anchor and Adapter layers. Our Reset experiments show that reverting either group to its base state significantly degrades performance from the RLVR baseline of 98.0% (dropping to 86.0% for Anchor and 92.0% for Adapter), and the MinervaMath is as well. Furthermore, isolated layer groups (Keep Only) fail to fully recover the original accuracy. These findings indicate that both layer groups are essential components that jointly constitute the internal circuit for memory retrieval and representation. Contaminated and clean datasets exhibit divergent senFigure 10. NDE Dynamics Metrics. Left: Separation Force peaks at L18L20, identifying the causal origin of trajectory divergence. Right: Velocity Difference increases in later layers, reflecting signal amplification by the Structural Adapter layers. Figure 11. Layer-wise Probing AUC. The distinguishability between leakage and stable samples peaks at Layer 20, corroborating the NDE and Path Patching results. Functional Anchor layers. Linear Probing Validation. As an independent validation, we train linear probes on each layers residual stream to classify leakage versus stable samples. Figure 11 shows AUC score peaking at Layer 20. This confirms that information distinguishing memorization from reasoning becomes maximally linearly separable immediately after Functional Anchor layers have performed their critical signal injection. 5. Ablation Study To validate our findings regarding the Functional Anchor and Structural Adapter, we conduct systematic ablation experiments on the RLVR-tuned model. These experiments are designed to verify both the necessity and sufficiency of these layer groups for memorization. More results for other datasets like LiveMathBench are provided in Appendix E. 8 Spurious Rewards Paradox: Mechanistically Understanding How RLVR Activates Memorization Shortcuts in LLMs Figure 12. Mean Neuron Activation Across Layers. The identified task-relevant neurons exhibit maximal activation at L19, L21, and L24. sitivity. On contaminated datasets (MATH-500, MinervaMath), leakage samples show high sensitivity to Anchor manipulation. Conversely, on the leakage-free LiveMathBench, accuracy remains stable at 70% across ablation conditions in Appendix E. This divergence provides direct evidence that the Anchor layers specifically encode dataset-contaminated knowledge rather than general reasoning improvements. Stable samples are robust to layer manipulation. Across all reset experiments, generalization accuracy remains stable, nearly identical to RLVR baseline. This indicates that genuine reasoning capabilities are distributed across broader network and do not critically depend on Anchor or Adapter layers. The sharp contrast with leakage samples, which exhibit high sensitivity to these layers, reinforces our conclusion that Anchor layers are specialized for memorization shortcuts rather than general mathematical reasoning. 6. Mechanistic Intervention Building on our previous findings, we investigate whether targeted intervention on these layers can suppress this undesired behavior. We design neuron-level steering method that directly manipulates MLP activations during inference to disrupt memorization circuit without model retraining. 6.1. Task-Relevant Neuron Identification In Qwen2.5-Math MLP layers, each neuron functions as gated memory unit. For hidden state xℓ Rdhidden at layer ℓ, the MLP computation proceeds as: kℓ = SiLU (xℓWgate) (xℓWup) Rdmlp , MLP(xℓ) = kℓWdown = dmlp (cid:88) i=1 k(i) ℓ wdown , (1) (2) where dmlp dhidden (e.g., 14336 3584 in Qwen2.5Rdhidden is the i-th row of Wdown Math-7B), and wdown Rdmlpdhidden. Each scalar k(i) acts as key that gates ℓ contribution of its corresponding neuron value wdown (a) Leakage Dataset (b) Leakage-free Dataset Figure 13. Dataset-Level Steering: Accuracy Change. (Left) On the leakage datasets, Layer 18 exhibits maximal sensitivity (4%). (Right) On the leakage-free dataset, steering produces no systematic pattern, confirming that the intervention specifically targets contamination-dependent circuits rather than general reasoning pathways. to output. We project each neurons value vector into vocabulary space via the unembedding matrix: vi = )WU RV . We then compute relLayerNorm(wdown evance score combining key activation magnitude and semantic overlap with answer tokens: si = k(i) ℓ (cid:18) 1 + λ Top-10(vi) Tokens(y) Tokens(y) (cid:19) , (3) where Top-10(vi) denotes the 10 highest-probability tokens after softmax, is the ground-truth answer, and λ is used to control the intensity of semantic alignment. We select the top-k neurons (k = 10) ranked by these scores as the intervention target set task . ℓ 6.2. Causal Steering We apply multiplicative scaling to the selected neurons key activations: ˆk(i) ℓ = (cid:40) α k(i) ℓ k(i) ℓ if task ℓ otherwise (4) where α is the scaling factor, applied uniformly across all generation steps. To validate our neuron selection strategy, we first examine the activation magnitudes kℓ of the identified task-relevant neurons across layers. Figure 12 reveals that these neurons exhibit peak activations precisely at L19, L21, and L24 layers that align with both our Functional Anchor hypothesis and the MLP injection patterns observed in the Logit Lens analysis (Figure 7). This convergence of evidence confirms that our selection method successfully identifies neurons that are mechanistically central to the memorization circuit. 6.3. Results Figure 13 presents the accuracy change when applying neuron-level steering across representative layers on the full leakage datasets. Layer 18 exhibits maximal sensitivity 9 Spurious Rewards Paradox: Mechanistically Understanding How RLVR Activates Memorization Shortcuts in LLMs (a) Pattern 1: Dose-Dependent Modulation (Q42, Answer: 4). Left: Layer-wise probability evolution under three scaling regimes. Amplification (α = 3.0) enhances answer token probability, while suppression (α = 0.2) attenuates it. Right: Top-10 token heatmap. Under amplification, formatting token prematurely reaches top-1 at Layer 22, and the answer token achieves higher ranking at Layer 23. (b) Pattern 2: Amplification-Induced Shortcut Activation (Q213, Answer: 2). Left: Baseline and suppression fail to retrieve the answer. Amplification triggers high-confidence plateau from the middle layer, with the answer token emerging only in the final two layers. Right: Top-10 heatmap shows abrupt answer injection at L26L27 under amplification, absent in other conditions. This suggests amplification activates latent shortcut pathway inaccessible to the base model. Figure 14. Two Patterns of Sample-Level Steering Effects. Top: Gradual probability modulation in partially memorized samples. Bottom: Binary pathway activation in failed-retrieval samples, where amplification uniquely unlocks dormant memorization circuit. (3.8% under suppression, +4.4% under amplification), consistent with its role as the Functional Anchor. In contrast, Layer 25 shows uniform degradation across all factors, reflecting its role as Structural Adapter where arbitrary manipulation disrupts the transformation process. Critically, on the leakage-free dataset, steering yields no systematic pattern, confirming that the intervention specifically targets contamination-dependent circuits rather than general reasoning pathways. To elucidate the fine-grained pathway through which neuron scaling modulates memorization retrieval, we conduct layerwise trajectory analysis on individual leakage samples in Figure 14a. Under amplification (α = 3.0), special formatting token reaches top-1 at Layer 22 one layer earlier, and the target answer token achieves higher probability. Conversely, suppression (α = 0.2) delays trigger activation and reduces answer token probability. This bidirectional control over both the timing and strength of retrieval provides direct mechanistic evidence that the Functional Anchor neurons constitute causally sufficient pathway for memory activation. Beyond gradual modulation, we observe more striking phenomenon in cases where baseline retrieval fails entirely as shown in Figure 14b. For Question 213 (ground truth: 2), both suppression and baseline produce incorrect outputs with uniformly low token probabilities. However, amplification triggers qualitatively different trajectory: starting from the middle layers, dominant token maintains high probability, while the correct answer 2 emerges abruptly only in the final two layers. This suggests that amplification activates latent shortcut pathway inaccessible to the base model. 7. Conclusion In this work, we present mechanistic analysis of how spurious RLVR induces stubborn memorization in the Qwen2.5Math models. We identify the Perplexity Paradox: rising prompt perplexity alongside falling answer perplexity, as macroscopic fingerprint of this phenomenon, where general language coherence is sacrificed for shortcut learning. Mechanistically, we uncover fundamental decoupling between function and structure: utilizing Path Patching, Logit Lens, JSD analysis, and Neural Differential Equations (NDEs), we pinpoint Layers 1820 as the Functional Anchor that causally injects the memorization trigger, while Layers 21+ serve as Structural Adapters that 10 Spurious Rewards Paradox: Mechanistically Understanding How RLVR Activates Memorization Shortcuts in LLMs undergo drastic weight reorganization to accommodate this signal. Ablation experiments confirm that these layers encode dataset-specific shortcuts rather than general reasoning improvements, as manipulation selectively degrades contaminated benchmarks while leaving clean benchmarks unaffected. Finally, we demonstrate that scaling specific MLP keys within this circuit allows for bidirectional causal steeringartificially amplifying or suppressing contaminationdriven performance. Our findings provide mechanistic grounding for understanding RLVR vulnerabilities and open avenues for targeted mitigation strategies."
        },
        {
            "title": "Impact Statement",
            "content": "This work presents mechanistic investigation into how reinforcement learning can inadvertently trigger shortcuts by activating latent memorized data rather than fostering genuine reasoning. The implications of our findings are twofold: 1. Strengthening AI Trustworthiness and Evaluation: Our discovery of the Anchor-Adapter circuit and the Perplexity Paradox provides new tools for the AI community to distinguish between appearance of capability and genuine generalization. As LLMs are increasingly deployed in critical domains like mathematics, science, and coding, it is essential that their performance is rooted in robust reasoning. By providing methodology to localize and steer these shortcuts, our work helps developers identify and mitigate the effects of data contamination, leading to more honest and reliable AI systems. 2. Mitigating the Risks of Spurious Optimization: We highlight critical vulnerability in RLVR: the potential for models to optimize for reward signals through stubborn memorization. This underscores the need for more sophisticated reward functions and evaluation benchmarks that are resistant to leakage. Furthermore, our causal steering techniques offer path toward decontaminating models post-training, reducing the risk of deploying models that appear performant but fail on novel, out-of-distribution tasks. Ethical Considerations: While our work provides tools to detect and suppress memorization, the same techniques could theoretically be used to amplify models reliance on specific datasets. We advocate for the responsible use of mechanistic interpretability tools to ensure they are used to enhance transparency and safety rather than to mask architectural flaws."
        },
        {
            "title": "References",
            "content": "Alam, M. T. and Rastogi, N. Limits of generalization in rlvr: Two case studies in mathematical reasoning. arXiv preprint arXiv:2510.27044, 2025. Chen, P., Li, X., Li, Z., Yin, W., Chen, X., and Lin, T. Exploration vs exploitation: Rethinking rlvr through clipping, entropy, and spurious reward. arXiv preprint arXiv:2512.16912, 2025. Chen, R. T., Rubanova, Y., Bettencourt, J., and Duvenaud, D. K. Neural ordinary differential equations. Advances in neural information processing systems, 31, 2018. Cohen-Wang, B., Shah, H., Georgiev, K., and Madry, A. Contextcite: Attributing model generation to context. Advances in Neural Information Processing Systems, 37: 9576495807, 2024. Dubey, A., Jauhri, A., Pandey, A., Kadian, A., Al-Dahle, A., Letman, A., Mathur, A., Schelten, A., Yang, A., Fan, A., et al. The llama 3 herd of models. arXiv e-prints, pp. arXiv2407, 2024. Elhage, N., Nanda, N., Olsson, C., Henighan, T., Joseph, N., Mann, B., Askell, A., Bai, Y., Chen, A., Conerly, T., et al. mathematical framework for transformer circuits. Transformer Circuits Thread, 1(1):12, 2021. Geva, M., Schuster, R., Berant, J., and Levy, O. Transformer feed-forward layers are key-value memories. In Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, pp. 54845495, 2021. Guo, D., Yang, D., Zhang, H., Song, J., Zhang, R., Xu, R., Zhu, Q., Ma, S., Wang, P., Bi, X., et al. Deepseek-r1: Incentivizing reasoning capability in llms via reinforcement learning. arXiv preprint arXiv:2501.12948, 2025. Hong, Y., Cao, M., Zhou, D., Yu, L., and Jin, Z. The reasoning-memorization interplay in language models In Findings of the is mediated by single direction. Association for Computational Linguistics: ACL 2025, pp. 2156521585, 2025. Kim, J., Lee, H., Cho, H., Jang, J., Hwang, H., Won, S., Ahn, Y., Lee, D., and Seo, M. Knowledge entropy decay during language model pretraining hinders new knowledge acquisition. arXiv preprint arXiv:2410.01380, 2024. Lambert, N., Morrison, J., Pyatkin, V., Huang, S., Ivison, H., Brahman, F., Miranda, L. J. V., Liu, A., Dziri, N., Lyu, S., et al. Tulu 3: Pushing frontiers in open language model post-training. arXiv preprint arXiv:2411.15124, 2024. Lewkowycz, A., Andreassen, A., Dohan, D., Dyer, E., Michalewski, H., Ramasesh, V., Slone, A., Anil, C., Schlag, I., Gutman-Solo, T., Wu, Y., Neyshabur, B., Gur-Ari, G., and Misra, V. Solving quantitative reasoning problems with language models, 2022. URL https://arxiv.org/abs/2206.14858. 11 Spurious Rewards Paradox: Mechanistically Understanding How RLVR Activates Memorization Shortcuts in LLMs Li, J., Beeching, E., Tunstall, L., Lipkin, B., Soletskyi, R., Huang, S., Rasul, K., Yu, L., Jiang, A. Q., Shen, Z., et al. Numinamath: The largest public dataset in ai4maths with 860k pairs of competition math problems and solutions. Hugging Face repository, 13(9):9, 2024. Li, Q., Geng, J., Chen, Z., Zhu, D., Wang, Y., Ma, C., Lyu, C., and Karray, F. Hd-ndes: Neural differential equations for hallucination detection in llms. arXiv preprint arXiv:2506.00088, 2025a. Li, R., Chen, C., Hu, Y., Gao, Y., Wang, X., and Yilmaz, E. Attributing response to context: jensen-shannon divergence driven mechanistic study of context attribution in retrieval-augmented generation. arXiv preprint arXiv:2505.16415, 2025b. Lightman, H., Kosaraju, V., Burda, Y., Edwards, H., Baker, B., Lee, T., Leike, J., Schulman, J., Sutskever, I., and Cobbe, K. Lets verify step by step. arXiv preprint arXiv:2305.20050, 2023. Liu, J., Liu, H., Xiao, L., Wang, Z., Liu, K., Gao, S., Zhang, W., Zhang, S., and Chen, K. Are your llms capable of stable reasoning? arXiv preprint arXiv:2412.13147, 2024. Lu, Y., Li, Z., He, D., Sun, Z., Dong, B., Qin, T., Wang, L., and Liu, T.-Y. Understanding and improving transformer from multi-particle dynamic system point of view. arXiv preprint arXiv:1906.02762, 2019. Meng, K., Bau, D., Andonian, A., and Belinkov, Y. Locating and editing factual associations in gpt. Advances in neural information processing systems, 35:1735917372, 2022. Shao, R., Li, S. S., Xin, R., Geng, S., Wang, Y., Oh, S., Du, S. S., Lambert, N., Min, S., Krishna, R., et al. Spurious rewards: Rethinking training signals in rlvr. arXiv preprint arXiv:2506.10947, 2025. Slonski, E. Detecting memorization in large language models. arXiv preprint arXiv:2412.01014, 2024. Wu, H., Wang, C., Zhao, W., and He, J. Mirage or method? how model-task alignment induces divergent rl conclusions. arXiv preprint arXiv:2508.21188, 2025a. Wu, M., Zhang, Z., Dong, Q., Xi, Z., Zhao, J., Jin, S., Fan, X., Zhou, Y., Lv, H., Zhang, M., et al. Reasoning or memorization? unreliable results of reinforcement learning due to data contamination. arXiv preprint arXiv:2507.10532, 2025b. Yang, A., Zhang, B., Hui, B., Gao, B., Yu, B., Li, C., Liu, D., Tu, J., Zhou, J., Lin, J., Lu, K., Xue, M., Lin, R., Liu, T., Ren, X., and Zhang, Z. Qwen2.5-math technical report: Toward mathematical expert model via self-improvement. arXiv preprint arXiv:2409.12122, 2024. Yang, A., Li, A., Yang, B., Zhang, B., Hui, B., Zheng, B., Yu, B., Gao, C., Huang, C., Lv, C., et al. Qwen3 technical report. arXiv preprint arXiv:2505.09388, 2025. Yue, Y., Chen, Z., Lu, R., Zhao, A., Wang, Z., Song, S., and Huang, G. Does reinforcement learning really incentivize reasoning capacity in llms beyond the base model? arXiv preprint arXiv:2504.13837, 2025. nostalgebraist. interpreting gpt: the logit lens, 2020. https://www.lesswrong.com/posts/AcKRB8wDpdaN6v6r u/interpreting-gpt-the-logit-lens. Oh, Y., Lim, D.-Y., and Kim, S. Stable neural stochastic differential equations in analyzing irregular time series data. arXiv preprint arXiv:2402.14989, 2024. OLMo, T., Walsh, P., Soldaini, L., Groeneveld, D., Lo, K., Arora, S., Bhagia, A., Gu, Y., Huang, S., Jordan, M., Lambert, N., Schwenk, D., Tafjord, O., Anderson, T., Atkinson, D., Brahman, F., Clark, C., Dasigi, P., Dziri, N., Guerquin, M., Ivison, H., Koh, P. W., Liu, J., Malik, S., Merrill, W., Miranda, L. J. V., Morrison, J., Murray, T., Nam, C., Pyatkin, V., Rangapur, A., Schmitz, M., Skjonsberg, S., Wadden, D., Wilhelm, C., Wilson, M., Zettlemoyer, L., Farhadi, A., Smith, N. A., and Hajishirzi, H. 2 olmo 2 furious, 2024. URL https://arxiv. org/abs/2501.00656. Ruan, Z., Chen, Y., Hou, Y., Li, P., Liu, Y., and Chen, G. Unveiling over-memorization in finetuning llms for reasoning tasks. arXiv preprint arXiv:2508.04117, 2025. 12 Spurious Rewards Paradox: Mechanistically Understanding How RLVR Activates Memorization Shortcuts in LLMs A. Partial Prompt Evaluation Details Figure 15. Accuracy Comparison Across Models and Datasets. Qwen3-8B shows high baseline accuracy and dramatic improvement under spurious RLVR, while LLaMA-3.1-8B and OLMo-2-1124-7B remain at low level performance regardless of training. This appendix provides the complete results for the Partial Prompt Evaluation discussed in Section 3.1. This experiment assesses whether models possess pre-existing knowledge of the ground-truth answers by prompting them with the question text alone (without Chain-of-Thought) to see if they can complete the exact answer string. Cross-Model Accuracy Comparison (Figure 15). To provide context for the Partial Prompt Evaluation, we first present the overall accuracy of Qwen3-8B, LLaMA-3.1-8B, and OLMo-2-1124-7B across six benchmarks before and after spurious RLVR training with incorrect rewards. striking dichotomy emerges: while LLaMA and OLMo maintain consistently low accuracy regardless of RLVR training, Qwen3-8B exhibits both high baseline performance and substantial gains after spurious RLVR. This pattern mirrors the behavior observed in Qwen2.5-Math-7B and provides strong prima facie evidence that Qwen3-8B also suffers from data contamination, which spurious RLVR subsequently activates. Qwen2.5-Math-7B (Figure 16). On contaminated datasets (MATH-500, MinervaMath), the WrongRight group (green line) shows dramatic increase in both ROUGE-L and exact completion accuracy after RLVR. This confirms that the model is learning to retrieve specific memorized strings. Qwen3-8B (Figure 17). Consistent with Qwen2.5-Math-7B, Qwen3-8B exhibits increasing ROUGE-L scores and completion accuracy as the prompt ratio increases, confirming its ability to retrieve memorized answers. However, the separation between the preand post-RLVR curves is less pronounced compared to Qwen2.5-Math-7B, particularly in the WrongRight group. This suggests that the extent of data leakage or the degree to which spurious RLVR activates these memorized answers is less significant in Qwen3-8B, indicating potentially lower reliance on contamination or weaker memory activation under spurious RLVR conditions. Control Models (Figure 18). In contrast, for LLaMA-3.1-8B and OLMo-2-1124-7B, both ROUGE-L and completion accuracy remain at consistently low level. Crucially, these metrics show no significant correlation or discernible trend as the prompt ratios increases, confirming the absence of pre-existing contamination. 13 Spurious Rewards Paradox: Mechanistically Understanding How RLVR Activates Memorization Shortcuts in LLMs (a) Qwen: ROUGE-L Scores (b) Qwen: Completion Accuracy Figure 16. Qwen2.5-Math-7B Partial Prompt Results. The sharp rise in the WrongRight group (green) on contaminated datasets signals the activation of memorized answers. (a) Qwen3-8B: ROUGE-L Scores (b) Qwen3-8B: Completion Accuracy Figure 17. Qwen3-8B Partial Prompt Results. Qwen3-8B shows the expected increase in ROUGE-L and completion accuracy as prompt ratios increase, but the separation between preand post-RLVR is less pronounced compared to Qwen2.5-Math-7B, suggesting weaker memory activation under spurious RLVR. (a) LLaMA-3.1-8B: ROUGE-L (b) LLaMA-3.1-8B: Completion Acc (c) OLMo-2-1124-7B: ROUGE-L (d) OLMo-2-1124-7B: Completion Acc Figure 18. Control Models Partial Prompt Results. Neither LLaMA (top) nor OLMo (bottom) shows significant improvement in completion capability under spurious RLVR, confirming no memorization activation occurred. Note that for OLMo-2-1124-7B, there are no samples categorized as WrongRight or Stable on the LiveMathBench. 14 Spurious Rewards Paradox: Mechanistically Understanding How RLVR Activates Memorization Shortcuts in LLMs B. Additional Path Patching Results Complementing the main text comparison, Figure 19a presents the path patching results for the second control model, OLMo-2-1124-7B. Similar to LLaMA, OLMo exhibits no significant accuracy recovery or distinctive layer-wise trends when patching activations from the RLVR-tuned model back to the base model. Notably, Qwen3-8B also fails to display the characteristic middle-layer peak observed in Qwen2.5-Math-7B (Figure 5a). Instead, its recovery pattern similar to the control models LLaMA and OLMo, further corroborates our previous findings (Appendix A) that its data leakage and memory activation under spurious RLVR are significantly less pronounced compared to Qwen2.5-Math-7B. (a) OLMo-2-1124-7B (b) Qwen3-8B Figure 19. Path Patching Recovery for Additional Models. (a) Consistent with LLaMA, OLMo does not display the middle-layer signal injection pattern observed in Qwen2.5-Math-7B. (b) Similarly, Qwen3-8B has weaker contamination effects compared to Qwen2.5-Math-7B. C. Additional JSD Results We present the layer-wise MLP sub-component JSD scores for OLMo-2-1124-7B and Qwen3-8B in Figure 20. For OLMo (Left), all three JSD components exhibit monotonic increase across layers as with LLaMA. There is no peak or decline as observed in Qwen, indicating that OLMo does not develop the kind of localized structural adaptation associated with memorization triggers. In contrast, Qwen3-8B (Right) replicates the distinctive structural signature observed in Qwen2.5Math-7B: both Wup and Wgate exhibit clear peak followed by subsequent decline, with the peak occurring at Layer 33 . Meanwhile, Wdown maintains sustained high divergence in Qwen3-8B, consistent with its role as the output projection layer. This architectural consistency across different Qwen family models strongly suggests that the circuit is not an artifact of specific model checkpoint, but rather systematic property of how the Qwen architecture responds to spurious RLVR when operating on contaminated data. D. Additional Results on Logit Lens To further validate that the memorization activation is specific to the leakage activation observed in Qwen2.5-Math, we perform the same Logit Lens analysis on our control models, LLaMA-3.1-8B and OLMo-2-1124-7B, as well as on additional test cases for Qwen. Control Models (LLaMA & OLMo). As shown in Figure 21, the internal dynamics of the control models differ fundamentally from Qwen. Neither LLaMA nor OLMo exhibits the characteristic peak-valley-rise pattern or the aggressive MLP signal injection in the middle layers. Instead, the probability of the answer token remains negligible throughout the vast majority of the network depth, emerging only in the final few layers (Late Emergence). Crucially, this pattern is consistent across both correct and incorrect outputs. The MLPs in these models do not attempt to force specific token in the intermediate layers, indicating the absence of the Functional Anchor mechanism. This flatness confirms that spurious RLVR did not activate latent memorization shortcut in these models. Additional Qwen Cases. We further corroborate the universality of the activation mechanism in Qwen2.5-Math by analyzing two additional distinct questions (Figure 22). Consistent with our main findings, both cases display the signature multi-stage Spurious Rewards Paradox: Mechanistically Understanding How RLVR Activates Memorization Shortcuts in LLMs (a) OLMo-2-1124-7B (b) Qwen3-8B Figure 20. Layer-wise MLP Sub-component JSD Scores for Additional Models. (a) OLMo exhibits monotonic increase across all components, consistent with non-contaminated control models. (b) Qwen3-8B replicates the peak-and-decline pattern at Layer 33, confirming the generalizability of the Structural Adapter mechanism within the Qwen architecture family. injection dynamic: high-signal precursor emerges at the Functional Anchor (L18L20), followed by structural adaptation valley, and finally, the aggressive injection of the correct answer token by the MLPs at Layer 23, confirming the robustness of this memorization circuit across different samples. To verify architectural generalizability, we perform the same analysis on Qwen3-8B (Figure 23). As shown, Qwen38B replicates the identical activation pattern: in the successful case (Left), the characteristic peak-valley-rise trajectory emerges with strong middle-layer MLP signals, while the failed case (Right) exhibits weakened precursor activation. This confirms that the Functional Anchor mechanism is systematic property of the Qwen architecture family when operating on contaminated data. E. Additional Results on Ablation Study In this section, we provide the comprehensive ablation results, with specific focus on MinervaMath and LiveMathBench, which exhibit contrasting sensitivity patterns to layer manipulation. MinervaMath Analysis. The results on MinervaMath (Figure 24, Right) strongly corroborate the primacy of the Functional Anchor. Necessity (Reset): Resetting the Anchor layers (L1820) causes accuracy drop from 88% to 72%, significantly larger than the drop caused by resetting the Adapter layers (88% 78%). This confirms that the Anchor layers are the primary causal driver for retrieving contaminated knowledge. Sufficiency (Keep Only): Notably, the Keep Only Adapter setting results in catastrophic failure (32%), performing even worse than the Base Model (50%). This indicates that the Structural Adapters (L2122) are functionally dependent on the signal from the Anchor; without the upstream trigger, they cannot sustain the memorization circuit in isolation. LiveMathBench Analysis. In contrast to contaminated datasets, LiveMathBench exhibits reversed sensitivity pattern (Figure 24, Middle). While we partition the data into leakage (wrong-then-right) and stable (always-right) subsets following the same protocol, the absence of actual contamination leads to different ablation behavior: the leakage subset shows greater stability under layer manipulation, whereas the stable subset exhibits higher sensitivity. We attribute this inversion to distributional differences between the two subsets rather than memorization effects, confirming that the layer-specific circuits identified in contaminated datasets do not generalize to clean data. F. Additional Results on Mechanistic Intervention We conducted comprehensive steering experiment across all 28 layers and their corresponding latent factors to validate our mechanistic hypothesis. The results demonstrate clear evidence for our proposed circuit: amplifying the factors at layer 18 16 Spurious Rewards Paradox: Mechanistically Understanding How RLVR Activates Memorization Shortcuts in LLMs (a) LLaMA-3.1-8B (Correct Output) (b) LLaMA-3.1-8B (Incorrect Output) (c) OLMo-2-1124-7B (Correct Output) (d) OLMo-2-1124-7B (Incorrect Output) Figure 21. Logit Lens Analysis for Control Models. Unlike Qwen2.5-Math, both LLaMA and OLMo show flat probability trajectory where the answer token only emerges in the final layers. There is no evidence of intermediate MLP injection or leakage in the middle layers, regardless of whether the model answers correctly or incorrectly. yields substantial accuracy improvements, while ablating (zeroing out) the factors at layers 22 and 23 leads to significant performance degradation. These findings strongly corroborate our claim that layer 18 encodes critical features for the task, whereas layers 22 and 23 are responsible for integrating and processing these representations. The complete intervention results are visualized in Figure 25. 17 Spurious Rewards Paradox: Mechanistically Understanding How RLVR Activates Memorization Shortcuts in LLMs Figure 22. Additional Qwen2.5-Math Leakage Cases. Analysis of two additional samples from the contaminated set. Both examples replicate the distinct activation signature: strong signal priming at L18L20, transitional valley, and decisive answer token injection starting at Layer 23. (a) Qwen3-8B (Correct Output) (b) Qwen3-8B (Incorrect Output) Figure 23. Logit Lens Analysis for Qwen3-8B. Consistent with Qwen2.5-Math-7B, Qwen3-8B exhibits single decisive peak-valley-rise trajectory in successful retrieval (Left), while failed cases display oscillatory multi-peak patterns (Right), confirming that the singular peak signature indicates successful memorization activation. 18 Spurious Rewards Paradox: Mechanistically Understanding How RLVR Activates Memorization Shortcuts in LLMs (a) Overall Leakage Accuracy across experiments. (b) Overall Stable Accuracy across experiments. (c) Heatmap of leaked and stable samples. Figure 24. Comprehensive Ablation Results. (Left) Overall accuracy trends show that Leakage samples (Top) are highly sensitive to Anchor/Adapter resetting, while Stable samples (Bottom) remain robust. (Right) Heatmaps of three datasets for different ablation conditions. Figure 25. Mechanistic intervention results across all 28 layers."
        }
    ],
    "affiliations": [
        "East China Normal University",
        "Mohamed bin Zayed University of Artificial Intelligence (MBZUAI)",
        "Southern University of Science and Technology",
        "University of Aberdeen"
    ]
}