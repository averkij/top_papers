{
    "paper_title": "ProFit: Leveraging High-Value Signals in SFT via Probability-Guided Token Selection",
    "authors": [
        "Tao Liu",
        "Taiqiang Wu",
        "Runming Yang",
        "Shaoning Sun",
        "Junjie Wang",
        "Yujiu Yang"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Supervised fine-tuning (SFT) is a fundamental post-training strategy to align Large Language Models (LLMs) with human intent. However, traditional SFT often ignores the one-to-many nature of language by forcing alignment with a single reference answer, leading to the model overfitting to non-core expressions. Although our empirical analysis suggests that introducing multiple reference answers can mitigate this issue, the prohibitive data and computational costs necessitate a strategic shift: prioritizing the mitigation of single-reference overfitting over the costly pursuit of answer diversity. To achieve this, we reveal the intrinsic connection between token probability and semantic importance: high-probability tokens carry the core logical framework, while low-probability tokens are mostly replaceable expressions. Based on this insight, we propose ProFit, which selectively masks low-probability tokens to prevent surface-level overfitting. Extensive experiments confirm that ProFit consistently outperforms traditional SFT baselines on general reasoning and mathematical benchmarks."
        },
        {
            "title": "Start",
            "content": "ProFit: Leveraging High-Value Signals in SFT via Probability-Guided Token Selection Tao Liu1, Taiqiang Wu2,, Runming Yang2 Shaoning Sun1 Junjie Wang1, Yujiu Yang1, 1Tsinghua University Equal contribution 2The University of Hong Kong Project Leader Corresponding authors https://github.com/Utaotao/ProFit 6 2 0 2 4 ] . [ 1 5 9 1 9 0 . 1 0 6 2 : r a"
        },
        {
            "title": "Abstract",
            "content": "Supervised fine-tuning (SFT) is fundamental post-training strategy to align Large Language Models (LLMs) with human intent. However, traditional SFT often ignores the one-to-many nature of language by forcing alignment with single reference answer, leading to the model overfitting to non-core expressions. Although our empirical analysis suggests that introducing multiple reference answers can mitigate this issue, the prohibitive data and computational costs necessitate strategic shift: prioritizing the mitigation of single-reference overfitting over the costly pursuit of answer diversity. To achieve this, we reveal the intrinsic connection between token probability and semantic importance: high-probability tokens carry the core logical framework, while low-probability tokens are mostly replaceable expressions. Based on this insight, we propose ProFit, which selectively masks low-probability tokens to prevent surface-level overfitting. Extensive experiments confirm that ProFit consistently outperforms traditional SFT baselines on general reasoning and mathematical benchmarks."
        },
        {
            "title": "Introduction",
            "content": "Large Language Models (LLMs) have demonstrated remarkable general capabilities (Jaech et al., 2024; Guo et al., 2025; Yang et al., 2025a). To adapt them to specific downstream tasks, Supervised Fine-Tuning (SFT) has become the prevailing paradigm (Chung et al., 2024). Traditional SFT is based on an autoregressive objective, forcing the model to strictly align with single reference answer at the token level. However, this rigid objective neglects the one-to-many nature of language (Li et al., 2016; Yang et al., 2025b), where diverse expressions can convey the same intent. Therefore, this strategy of forcibly fitting single reference is often suboptimal and can easily lead to the model simply memorizing specific samples. (Gudibande et al., 2023; Chu et al., 2025). Figure 1: Breaking the trade-off between training cost and semantic diversity. While Multi-reference SFT offers semantic richness at prohibitive data and computational costs, standard SFT is efficient but semantically limited. ProFit achieves the best of both: by focusing supervision on high-value tokens, it captures core semantic integrity without sacrificing the efficiency of single-reference training. While introducing multiple reference answers can alleviate this problem (Yuan et al., 2023; Li et al., 2024b; Shi and Shen, 2025), it faces the dual challenges of expensive data construction and difficulties in training convergence (please refer to Section 3.2 for details). To solve this problem while maintaining the existing low-cost single instruction-single response data configuration,we propose more efficient strategy (as illustrated in Figure 1): instead of striving for comprehensive coverage with multiple answers, its better to avoid overfitting to single answer. To achieve this, we need mechanism to filter out truly highvalue training signals from single reference answer. To accurately identify these high-value training signals, we conducted semantic analysis. Specifically, we comparatively analyze multiple answers to given question, aiming to identify those tokens defined as decisive for the answers correctness, with their importance judged by Gemini-3-Pro. Fortunately, we found that the predicted probability of the token can serve as an efficient and accurate proxy metric (Kadavath et al., 2022; Huang et al., 2025; Bentegeac et al., 2025). Further hypothesis testing confirmed this significant pattern: high-probability tokens tend to carry core reasoning logic or key semantics, while low-probability tokens correspond more to non-core expressions. Inspired by this, we propose the ProFit method, which leverages the online probabilities predicted by the model currently being trained as the core clue to locate high-value signals. Specifically, ProFit employs strategic masking mechanism: it selectively retains and trains high-probability tokens that carry crucial semantic information, while masking low-probability, non-essential tokens (Lin et al., 2024; Ruan et al., 2025). We further provided theoretical derivations demonstrating that the gradients of low-probability tokens can overshadow the optimization direction of crucial tokens. We conducted extensive evaluations on general reasoning (GPQA-Diamond (Rein et al., 2024)), mathematics (MATH-500 (Lightman et al., 2023), AIME24 (American Institute of Mathematics, 2024), GSM8K (Cobbe et al., 2021)), and instruction following (IFEval (Zhou et al., 2023b)). The results consistently show that ProFit outperforms the traditional SFT baseline. Notably, on the Qwen3 family, ProFit surpasses SFT by significant margin of 3.0% to 10.9% in average accuracy, validating the effectiveness of our strategy. Our contributions can be summarized as follows: We identify positive correlation between prediction probability and semantic importance, revealing that low-probability tokens typically represent non-essential expression. We propose ProFit, probability-guided masking strategy. We theoretically prove that masking low-probability tokens prevents their large gradients from overshadowing key semantic signals. Extensive experiments on general reasoning and math benchmarks demonstrate that ProFit consistently outperforms standard SFT baselines."
        },
        {
            "title": "2 Related Work",
            "content": "2.1 Data-Efficient Instruction Tuning Recent SFT adheres to the less is more principle (Zhou et al., 2023a; Zhang et al., 2025; Li et al., 2025b), evolving from complexity-based selection (Cao et al., 2023; Li et al., 2024a) to 2025s importance-aware metrics like MIWV (Jiang et al., 2025) and ICL-based filtering (Wang et al., 2025a; these coarseJiang et al., 2025). However, grained sample-level methods treat pairs as atomic units, overlooking intra-sample low-information segments or stylistic noise (Pang et al., 2025; Qin et al., 2025), thus limiting the models focus on dense logical signals. 2.2 Token-Level Training Objectives To address granularity limitations, research has pivoted to token-level optimization. While classical methods like Focal Loss (Lin et al., 2017) and Unlikelihood Training (Welleck et al., 2019) targeted hard tokens, recent LLM approaches like Rho1 (Lin et al., 2024) and TIS-DPO (Liu et al., 2024) rely on costly external reference models. More efficient intrinsic methods like DFT (Wu et al., 2025b) employ probability-driven soft reweighting, and CFT (Ruan et al., 2025) further validates the need for supervising critical regions. While sharing the probability-driven motivation with DFT, ProFit diverges by adopting strict hard masking strategy to efficiently filter out non-core expressions in single step."
        },
        {
            "title": "Preliminaries and Motivation",
            "content": "3.1 Preliminaries Supervised Fine-Tuning. Given dataset containing pairs of inputs and reference responses = (y ), SFT optimizes the policy πθ by minimizing the negative log-likelihood: 1, . . . , LSFT(θ) = E(x,y)D (cid:88) t=1 log πθ(y x, <t) . (1) Let zt be the logits at step t. The probability is pt = softmax(zt). The gradient of the per-token loss ℓt = log pt,y with respect to the logits zt is: ℓt zt,v = pt,v I[v = ]. (2) Equation 2 drives pt,y 1 while suppressing alternatives (pt,v 0). This mechanism indiscriminately suppresses all non-reference tokens (v = ), including valid paraphrases, thereby penalizing semantic flexibility and driving the model toward surface-form overfitting. reference set often introduces distributional conflicts, leading to optimization instability. As illustrated in Figure 2, we compared SFT trained with single versus multiple (3) reference answers. While multiple references yield marginal improvements on specific reasoning tasks like MATH-500, they fail to generalize consistently. Surprisingly, on complex benchmarks like GPQA-Diamond, the performance stagnates or even slightly degrades (dropping from 34.1% to 33.5%) compared to the single-answer baseline. Furthermore, both standard and multi-answer SFT struggle to maintain base capabilities on instruction-following tasks like IFEval, where performance trails behind the Vanilla model. This suggests that blindly fitting diverse distributions can confuse the model, causing it to struggle with convergence due to conflicting gradient directions. To address this dilemma, we propose relaxing the strict alignment objectives. Instead of performing indiscriminate full-scale fitting on all tokens or relying on expensive multi-reference datasets, we implement selective alignment strategy. We need mechanism that can accurately filter out the high-value training signals that truly carry the core reasoning logic from single reference answer, thereby achieving robust performance across diverse benchmarks."
        },
        {
            "title": "4 Methodology",
            "content": "4.1 Semantic Analysis To accurately extract high-quality training signals, we performed joint analysis of semantic importance and prediction probability for the tokens in multiple reference answers. Specifically, we utilize Gemini-3-pro as semantic evaluator to annotate the tokens in the reference answers, classifying them into trivial tokens, which represent interchangeable stylistic variations, and core tokens, which encapsulate the essential reasoning logic. Subsequently, we used Qwen3-4B-Base (Yang et al., 2025a) to perform forward propagation and calculate the predicted probability for each token. This choice provides computation-efficient yet capable proxy for language modeling, striking balance between estimation quality and inference cost. As shown in Figure 3, the two token types exhibit distinct probability distributions: core tokens are highly concentrated in the high-probability region, showing strong determinism, whereas trivial tokens Figure 2: Performance comparison on diverse benchmarks. While multi-reference training (SFT w/ 3 ans) offers sporadic gains, it suffers from optimization instability and stagnation on complex tasks. In contrast, ProFit achieves superior and robust performance across all metrics by selectively extracting high-value signals from single reference. Low-Rank Adaptation (LoRA). LoRA is parameter-efficient fine-tuning technique for LLMs (Hu et al., 2022; Wu et al., 2024). Its core premise is that the change in weights during model adaptation, denoted as , possesses low intrinsic rank. Let W0 Rdk denote the frozen pre-trained weight matrix. Instead of updating the full parameters, LoRA approximates the weight update by decomposing it into two low-rank matrices, and B: α = W0x + = W0x + BAx α (3) where Rdr and Rrk are low-rank matrices with rank min(d, k), and α is scaling hyperparameter ensuring the stability of the adaptation. 3.2 Motivation Traditional SFT relies on strict single-reference token-level alignment, which inherently overpenalizes paraphrastic variants by treating valid semantic equivalents as incorrect predictions. Intuitively, introducing multiple reference answers could theoretically bridge this gap. However, our pilot investigation reveals that this approach faces substantial practical barriers rather than offering straightforward solution. First, constructing diverse, high-quality response set entails prohibitive costs: collecting distinct references per instruction scales the annotation burden linearly, and ensuring high quality often necessitates expert annotators, particularly for complex reasoning or mathematical tasks. Second, and more critically, simply expanding the updates from the interference of low-probability trivial tokens. To operationalize this strategy, we employ stopgradient mechanism to decouple the masking criterion from the gradient computation. We define the binary validity mask Mt using the detached probability: Mt = [sg(πθ(y x, <t)) > τ ] , (4) Figure 3: Probability density estimation of semantic tokens. We categorize tokens into semantically Core and Trivial groups. While core tokens are heavily concentrated in high-confidence zones, trivial tokens exhibit significant long-tail distribution, disproportionately dominating the low-probability spectrum. hypothesis test confirms this significant distributional difference (p = 1 106). display long-tail distribution. Although some trivial tokens appear in the high-confidence interval, their density is much higher in the low-probability region than that of core tokens, making them the dominant component there. To rigorously verify this observation, we conducted statistical hypothesis test. We formally defined the null hypothesis as follows: Null Hypothesis (H0): The probability distributions of trivial tokens and core tokens are statistically identical (i.e., drawn from the same population). Alternative Hypothesis (H1): The probability distributions of trivial tokens and core tokens are statistically distinct (i.e., drawn from different populations). The test yielded p-value of 1 106, leading to significant rejection of H0. This empirical result strongly supports our hypothesis: low prediction probability is strong indicator of semantic non-essentiality, as low-probability regions are primarily dominated by trivial tokens. 4.2 ProFit Based on the motivation and semantic analysis, we propose ProFit. The core intuition of this method is to utilize the models own prediction probabilities as dynamic indicator to locate the core tokens. During training, by implementing threshold-based masking operator, ProFit selectively backpropagates gradients only from highprobability tokens, effectively isolating parameter where sg() denotes the stop-gradient operator, τ [0, 1] is static threshold, and I[] is the indicator function. By strictly enforcing > τ , we ensure the optimization is driven solely by highvalue semantic signals. Crucially, the stop-gradient ensures that Mt acts as fixed gate during backpropagation, avoiding the differentiability issues of the step function. Formally, the optimization objective of ProFit is defined as: (cid:34) LProFit(θ) = ED 1 (cid:88) t=1 Mt log πθ(y x, <t) (cid:35) , (5) where = denotes the sequence length. 4.3 Deeper Insights Equation 2 shows that low-probability tokens induce significantly larger logit gradients. To quantify how this amplification propagates to the parameter space, we derive the following theorem, establishing lower bound for the parameter gradient (proof in Appendix A): Theorem 1 (Token-Wise Gradient Norm Lower Bound). Consider the prediction of single target token at step t, given the instruction and the <t. Let RV be preceding ground-truth tokens the output logits and ℓ(θ) = log πθ(y <t) be the loss for this step. Let Jθ(z) = θz RVθ denote the Jacobian of the logits with respect to parameters θ Rθ. Under the local non-degeneracy assumption that the Jacobian is full row-rank (i.e., the model is locally surjective, satisfying σmin(Jθ(z)) γ > 0), the gradient norm satisfies: x, θℓ2 γ (1 πθ(y x, <t)). (6) This lower bound theoretically guarantees that tokens with lower prediction probabilities inevitably induce larger parameter gradients. Model Method GPQA-Diamond GSM8K MATH-500 AIME24 IFEval Avg. Qwen3-0.6B-Base Qwen3-4B-Base Qwen3-14B-Base OLMo-2-7B Llama-3.1-8B Vanilla SFT Entropy DFT ProFit Vanilla SFT Entropy DFT ProFit Vanilla SFT Entropy DFT ProFit Vanilla SFT Entropy DFT ProFit Vanilla SFT Entropy DFT ProFit 4.36 17.93 20.58 17.68 22.85 23.17 34.15 34.91 31.69 34.34 37.69 46.02 47.85 43.81 46.53 21.72 13.64 12.94 12.31 14.71 8.71 23.30 23.61 8.40 21. 45.99 54.53 53.67 62.42 59.78 82.50 55.43 46.54 87.83 87.55 83.44 78.00 78.23 88.98 89.62 68.08 78.43 78.22 76.09 78.25 51.88 60.42 61.08 60.07 62.11 7.28 45.38 45.83 47.92 49. 57.67 74.80 74.75 77.50 77.85 78.32 79.22 80.12 81.97 82.85 11.03 24.95 24.77 23.57 25.45 3.35 24.75 25.70 17.65 24.90 0.10 1.56 2.19 2.29 2.19 8.23 11.25 10.94 10.83 13. 13.33 16.04 14.79 17.29 16.56 0.10 0.62 0.52 0.31 0.31 0.00 0.31 0.21 0.62 0.62 15.6 23.15 22.02 20.63 22.71 33.04 31.33 30.71 43.67 48.87 52.61 36.69 38.01 49.86 58. 14.44 22.50 22.64 23.24 23.98 21.42 24.72 24.31 25.14 26.16 - 14.67 28.51 +13.84 28.86 +14.19 30.20 +15.53 31.49 +16.82 - 40.92 +0.47 41.39 -1.35 39.57 50.30 +9.38 52.33 +11.41 53.08 51.20 51.80 56.38 58. 23.07 28.03 27.82 27.10 28.54 17.07 26.70 26.98 22.38 27.04 - -1.88 -1.28 +3.30 +5.64 - +4.96 +4.75 +4.03 +5.47 - +9.63 +9.91 +5.31 +9.97 Table 1: Main results across five benchmarks. We report the accuracyof the Vanilla baseline, standard SFT, and varying strategies (Entropy, DFT, ProFit) on multiple model families. Regarding the evaluation settings, results on AIME24 are averaged over 32 samples, while results on the other datasets are averaged over 8 samples. The values in parentheses indicate the performance difference relative to the Vanilla baseline."
        },
        {
            "title": "5 Experiments",
            "content": "5.1 Experimental Setup Training. For the training data, we curated subset of 2,000 samples from the BAAIInfinityInstruct Dataset (Zhou et al., 2023a; Li et al., 2025a; Muennighoff et al., 2025), prioritizing high reward scores as done in Shadow-FT (Wu et al., 2025a). To comprehensively verify the effectiveness and generalization of our method, we conducted evaluations across diverse set of LLMs, including the Qwen3 series (Yang et al., 2025a), Llama 3 series (AI@Meta, 2024), and OLMo 2 series (OLMo et al., 2024). We employed LLaMAFactory (Zheng et al., 2024) as our primary training framework. All experiments are conducted on 8 H20 GPUs. For detailed hyperparameter settings, please refer to the Appendix B. Baselines. To evaluate the effectiveness of ProFit, we compare it against several representative finetuning paradigms: Supervised Fine-tuning: The vanilla baseline that minimizes the cross-entropy loss across all tokens indiscriminately. Dynamic Fine-tuning (Wu et al., 2025b): probability-aware method that assigns dynamic scale to the loss of each token based on its confidence. Entropy-based tuning (Wang et al., 2025b): selective strategy that updates parameters only on tokens with entropy. Evaluation. To comprehensively evaluate the downstream performance of our fine-tuned models, we conducted extensive assessments across diverse set of benchmarks, including GPQADiamond (Rein et al., 2024), MATH-500 (Lightman et al., 2023), GSM8K (Cobbe et al., 2021), AIME24 (American Institute of Mathematics, 2024), and IFEval (Zhou et al., 2023b). For the evaluation pipeline, we utilized OpenCompass (Contributors, 2023b) as the primary inference Figure 4: Ablation study on the probability threshold τ . The dashed line represents the performance of the standard SFT baseline. (a) Training exclusively on low-probability tokens (p(y ) < τ ) results in performance consistently below the baseline, indicating that non-core expressions are insufficient for constructing effective reasoning chains. (b) Conversely, the proposed strategy (p(y ) > τ ), which masks low-probability noise, consistently outperforms the baseline across all tasks, validating the effectiveness of focusing on core logic. framework, integrated with lmdeploy (Contributors, 2023a) and vllm (Kwon et al., 2023) as the acceleration backend. For the decoding strategy, we performed sampling and reported the average accuracy across 32 generations for AIME24 and 8 for other benchmarks. Detailed inference hyperparameters and configuration settings are provided in the Appendix B. 5.2 Main Results Table 1 presents the comparative results of our proposed method, ProFit, against the Vanilla baseline, standard SFT, and other strategies (Entropy and DFT) across five diverse benchmarks. The experimental results demonstrate that ProFit consistently achieves superior performance across all evaluated model families and scales. Compared to standard SFT, ProFit delivers substantial improvements in average accuracy. For instance, on the Qwen3-4B-Base model, ProFit achieves an average accuracy of 52.33%, surpassing standard SFT (41.39%) by significant margin of 10.94%. Similarly, on Llama-3.1-8B, ProFit improves the average score to 27.04%, outperforming SFT (26.70%) and showing +9.97% gain over the Vanilla baseline. Notably, on Qwen3-14B-Base, standard SFT experiences performance drop of 1.88% relative to Vanilla, likely stemming from the interference of non-core expressions or superficial stylistic patterns. ProFit successfully mitigates this issue, reversing the decline to achieve distinct gain of +5.64%. This underscores the stability of our probability-guided filtering mechanism in mitigating negative transfer while enhancing downstream performance. ProFit also consistently outperforms other baselines. As shown in Table 1, while DFT and Entropy strategies generally offer improvements over the Vanilla baseline, they often fall short of the gains achieved by ProFit. For example, on Qwen3-0.6B, ProFit achieves the highest average accuracy of 31.49%, exceeding DFT (30.20%) and Entropy (28.86%). This trend holds across architectures like OLMo-2 and Llama-3.1, showing our probability-based identification of non-core expressions outperforms entropy-based filtering and dynamic reweighting methods. In summary, ProFit shows strong scalability and universality, consistently boosting performance across model sizes (0.6B14B) and architectures without the high data costs of multi-reference finetuning."
        },
        {
            "title": "6 Extensive Analysis",
            "content": "6.1 Analysis of τ parameter To investigate the impact of the probability threshold τ , we compared the experimental results of retaining low-probability tokens (p(y ) < τ ) versus retaining high-probability tokens (p(y ) > τ ), as illustrated in Figure 4. The Figure 4(b) reveals that when masking lowprobability tokenswhich typically represent semantic diversitythe models performance consistently exceeds the full-token fine-tuning baseline Figure 5: Average performance variation across different LoRA ranks (r {4, . . . , 1024}). The dashed lines represent the baseline performance of full-parameter fine-tuning for each corresponding setting. While core tokens (p(y ) > 0.1) exhibit monotonic improvement driven by capacity, non-core tokens (p(y ) < 0.1) and standard SFT show U-shaped trend, revealing optimization interference at medium ranks. (dashed line) across all threshold settings. This suggests that in traditional SFT, forcing the model to fit these surface-level stylistic variations distracts it from learning the underlying reasoning patterns. By alleviating this unnecessary learning burden, the model can focus more effectively on the invariant logical core. Although knowledge-intensive tasks like GPQA-Diamond show slight performance decline as the threshold increases, likely due to specific long-tail entities falling into the low-probability range, their absolute performance remains significantly above the baseline. Conversely, the Figure 4(a) highlights the irreplaceable skeletal role of high-probability tokens. When the model is restricted to learning only lowprobability diverse expressions without the support of high-probability structural tokens, performance suffers catastrophic decline. Crucially, even as the threshold τ increases (introducing more high-frequency tokens), while performance recovers slightly, it never reaches the level of the fulltoken fine-tuning baseline and remains far inferior to the strategy shown in the 4(b). This persistent performance gap indicates that low-probability tokens are essentially auxiliary components contingent upon the logical skeleton. Without the support of high-confidence core logic, merely increasing the fitting of these non-core expressions fails to establish effective reasoning links and ultimately limits the models generalization potential. Figure 6: Average performance trajectory across training epochs. ProFit (p > τ ) demonstrates rapid convergence and superior performance ceiling, whereas focusing on low-probability tokens (p < τ ) results in training instability and limited capacity. 6.2 Impact of LoRA Rank We explore the influence of trainable parameter volume by scaling the LoRA rank from 4 to 1024, as shown in Figure 5 (refer to Appendix for details). The results reveal distinct divergence: core tokens (p > 0.1) benefit monotonically from increased rank, indicating they strictly require model In contrast, trivial tokens (p < 0.1) capacity. and standard SFT display U-shaped trend, where medium ranks struggle with optimization interference. Interestingly, for trivial tokens, LoRA (Rank 1024) outperforms full fine-tuning, demonstrating that low-rank constraints serve as effective regularization, preventing the model from overfitting to non-essential statements. The fact that global SFT follows the trivial token trend underscores that non-core expressions act as the primary bottleneck in standard training. 6.3 Performance Evolution across Epochs Figure 6 illustrates the training trajectory over 5 epochs. ProFit (p > τ ) demonstrates superior efficiency, converging immediately to 60.1% accuracy in the first epochalready surpassing the Baselines peak performance of 54.9%. In stark contrast, training exclusively on low-probability tokens (p < τ ) leads to stagnation in suboptimal range (40% 50%) and training instability. These results confirm that high-probability tokens contain the essential gradient signals for alignment, whereas low-probability regions offer negligible or even detrimental supervision. Figure 7: Performance trajectories on MATH-500, OlympiadBench, and Minerva datasets. ProFit combines high initial performance with continuous learning capability, ultimately achieving the best Pass@4 and Avg@4 scores across all benchmarks. 6.4 Superior RL Initialization We conduct experiments based on the Qwen30.6B-Base model. Specifically, we employ GRPO (Group Relative Policy Optimization, (Shao et al., 2024)) to optimize the models initialized with three different strategies: Base, DFT, and our proposed ProFit. We employ the DeepScaleR dataset as training prompts following Liu et al. (2025). All experiments are conducted on 32 H20 GPUs. For evaluation, in addition to the previously introduced MATH-500, we employ Minerva (Hendrycks et al., 2021) and OlympiadBench (He et al., 2024), which serve as comprehensive benchmarks covering tasks from general mathematical reasoning to diverse competition-level challenges. We evaluated the potential of ProFit as an initialization for subsequent reinforcement learning by analyzing its training dynamics across three mathematical benchmarks, as illustrated in Figure 7. The experimental results demonstrate that ProFit consistently outperforms comparison methods, offering not only superior starting point but also more robust training stability for the RL stage. Please refer to Appendix for detailed training dynamics, such as KL divergence, entropy, and response length. Specifically, on the MATH-500 dataset, ProFit achieves an Avg@4 of 57.3% and Pass@4 of 76.4% at the final training stage, significantly surpassing the Baseline which reaches 53.1% and 70.6% respectively. Crucially, the analysis of training curves reveals distinct convergence behaviors: while the Baseline exhibits pronounced cold start phenomenon requiring more steps to learn effective patterns, and DFT shows earlier plateauing in Avg@4 under the same budget, ProFit maintains continuous performance growth throughout the training process. This advantage is particularly evident on the challenging OlympiadBench, where ProFit attains an Avg@4 of 24.3%, distinctively outperforming the competing methods which hover around the 21.1% level. Similar consistent improvements are also observed on Minerva, further validating the generalization capability of our method."
        },
        {
            "title": "7 Conclusion",
            "content": "In this work, we propose ProFit, novel method to fine-tune LLMs by leveraging token prediction probabilities as proxy for semantic importance. Inspired by our hypothesis testing results revealing that high-probability tokens carry core semantics while low-probability ones correspond to noncore expressions, we propose ProFit to selectively mask the latter, aiming to alleviate the overfitting to surface-level phrasing and capture the underlying logic. Extensive experiments across multiple LLM series, including Qwen, Llama, and OLMo2, demonstrate that ProFit consistently outperforms conventional full-parameter SFT and other data selection methods on diverse benchmarks covering general reasoning, mathematics, and instruction following. We further conduct comprehensive ablation studies on training epochs, probability thresholds, and LoRA ranks. These analyses confirm that low-probability tokens serve as primary source of optimization interference, validating that our approach offers robust and effective solution for improving model generalization."
        },
        {
            "title": "Limitation",
            "content": "Despite the promising results, this work has limitations. First, our core assumption that low-probability tokens represent non-core expressions primarily holds for logic-intensive tasks (e.g., reasoning, mathematics). For creative generation tasks, such tokens may contribute to stylistic diversity, which requires further investigation. Second, ProFit currently employs static probability threshold across all samples. While this design choice was made to ensure implementation simplicity and training stability and has yielded consistent empirical gains, we acknowledge that future iterations could further enhance performance by exploring adaptive mechanisms that dynamically adjust the threshold based on instance-specific difficulty."
        },
        {
            "title": "References",
            "content": "AI@Meta. 2024. Llama 3 model card. American Institute of Mathematics. 2024. Aime 2024 competition mathematical problems. Raphaël Bentegeac, Bastien Le Guellec, Grégory Kuchcinski, Philippe Amouyel, and Aghiles Hamroun. 2025. Token probabilities to mitigate large language models overconfidence in answering medical questions: Quantitative study. Journal of medical Internet research, 27:e64348. Yihan Cao, Yanbin Kang, Chi Wang, and Lichao Sun. 2023. Instruction mining: Instruction data selection for tuning large language models. arXiv preprint arXiv:2307.06290. Tianzhe Chu, Yuexiang Zhai, Jihan Yang, Shengbang Tong, Saining Xie, Dale Schuurmans, Quoc Le, Sergey Levine, and Yi Ma. 2025. Sft memorizes, rl generalizes: comparative study of arXiv preprint foundation model post-training. arXiv:2501.17161. Hyung Won Chung, Le Hou, Shayne Longpre, Barret Zoph, Yi Tay, William Fedus, Yunxuan Li, Xuezhi Wang, Mostafa Dehghani, Siddhartha Brahma, and 1 others. 2024. Scaling instruction-finetuned language models. Journal of Machine Learning Research, 25(70):153. LMDeploy Contributors. 2023a. Lmdeploy: toolkit for compressing, deploying, and serving llm. https: //github.com/InternLM/lmdeploy. OpenCompass Contributors. 2023b. Opencompass: universal evaluation platform for foundation https://github.com/open-compass/ models. opencompass. Arnav Gudibande, Eric Wallace, Charlie Snell, Xinyang Geng, Hao Liu, Pieter Abbeel, Sergey Levine, and Dawn Song. 2023. The false promise of imitating proprietary llms. arXiv preprint arXiv:2305.15717. Daya Guo, Dejian Yang, Haowei Zhang, Junxiao Song, Ruoyu Zhang, Runxin Xu, Qihao Zhu, Shirong Ma, Peiyi Wang, Xiao Bi, and 1 others. 2025. Deepseek-r1: Incentivizing reasoning capability in arXiv preprint llms via reinforcement learning. arXiv:2501.12948. Chaoqun He, Renjie Luo, Yuzhuo Bai, Shengding Hu, Zhen Thai, Junhao Shen, Jinyi Hu, Xu Han, Yujie Huang, Yuxiang Zhang, and 1 others. 2024. Olympiadbench: challenging benchmark for promoting agi with olympiad-level bilingual multimodal scientific problems. In Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 3828 3850. Dan Hendrycks, Collin Burns, Saurav Kadavath, Akul Arora, Steven Basart, Eric Tang, Dawn Song, and Jacob Steinhardt. 2021. Measuring mathematical problem solving with the math dataset. arXiv preprint arXiv:2103.03874. Edward Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean Wang, Lu Wang, Weizhu Chen, and 1 others. 2022. Lora: Low-rank adaptation of large language models. ICLR, 1(2):3. Guanhua Huang, Tingqiang Xu, Mingze Wang, Qi Yi, Xue Gong, Siheng Li, Ruibin Xiong, Kejiao Li, Yuhao Jiang, and Bo Zhou. 2025. Lowprobability tokens sustain exploration in reinforcement learning with verifiable reward. arXiv preprint arXiv:2510.03222. Aaron Jaech, Adam Kalai, Adam Lerer, Adam Richardson, Ahmed El-Kishky, Aiden Low, Alec Helyar, Aleksander Madry, Alex Beutel, Alex Carney, and 1 others. 2024. Openai o1 system card. arXiv preprint arXiv:2412.16720. Tingyu Jiang, Shen Li, Yiyao Song, Lan Zhang, Hualei Zhu, Yuan Zhao, Xiaohang Xu, Kenjiro Taura, and Hao Henry Wang. 2025. Importance-aware data selection for efficient llm instruction tuning. arXiv preprint arXiv:2511.07074. Karl Cobbe, Vineet Kosaraju, Mohammad Bavarian, Mark Chen, Heewoo Jun, Lukasz Kaiser, Matthias Plappert, Jerry Tworek, Jacob Hilton, Reiichiro Nakano, and 1 others. 2021. Training verifiers arXiv preprint to solve math word problems. arXiv:2110.14168. Saurav Kadavath, Tom Conerly, Amanda Askell, Tom Henighan, Dawn Drain, Ethan Perez, Nicholas Schiefer, Zac Hatfield-Dodds, Nova DasSarma, Eli Tran-Johnson, and 1 others. 2022. Language models (mostly) know what they know. arXiv preprint arXiv:2207.05221. Woosuk Kwon, Zhuohan Li, Siyuan Zhuang, Ying Sheng, Lianmin Zheng, Cody Hao Yu, Joseph E. Gonzalez, Hao Zhang, and Ion Stoica. 2023. Efficient memory management for large language model serving with pagedattention. In Proceedings of the ACM SIGOPS 29th Symposium on Operating Systems Principles. Niklas Muennighoff, Zitong Yang, Weijia Shi, Xiang Lisa Li, Li Fei-Fei, Hannaneh Hajishirzi, Luke Zettlemoyer, Percy Liang, Emmanuel Candès, and Tatsunori Hashimoto. 2025. s1: Simple test-time scaling. In Proceedings of the 2025 Conference on Empirical Methods in Natural Language Processing, pages 2028620332. Jijie Li, Li Du, Hanyu Zhao, Bo-wen Zhang, Liangdong Wang, Boyan Gao, Guang Liu, and Yonghua Lin. 2025a. Infinity instruct: Scaling instruction selection and synthesis to enhance language models. arXiv preprint arXiv:2506.11116. Jiwei Li, Michel Galley, Chris Brockett, Jianfeng Gao, and William Dolan. 2016. diversity-promoting objective function for neural conversation models. In Proceedings of the 2016 conference of the North American chapter of the association for computational linguistics: human language technologies, pages 110119. Mengqi Li, Lei Zhao, Anthony Man-Cho So, Ruoyu Sun, and Xiao Li. 2025b. Online sft for llm reasoning: Surprising effectiveness of self-tuning without rewards. arXiv preprint arXiv:2510.18814. Ming Li, Yong Zhang, Shwai He, Zhitao Li, Hongyu Zhao, Jianzong Wang, Ning Cheng, and Tianyi Zhou. 2024a. Superfiltering: Weak-to-strong data filtering for fast instruction-tuning. arXiv preprint arXiv:2402.00530. Ziniu Li, Congliang Chen, Tian Xu, Zeyu Qin, Jiancong Xiao, Zhi-Quan Luo, and Ruoyu Sun. 2024b. Preserving diversity in supervised fine-tuning of large language models. arXiv preprint arXiv:2408.16673. Hunter Lightman, Vineet Kosaraju, Yuri Burda, Harrison Edwards, Bowen Baker, Teddy Lee, Jan Leike, John Schulman, Ilya Sutskever, and Karl Cobbe. 2023. Lets verify step by step. In The Twelfth International Conference on Learning Representations. Tsung-Yi Lin, Priya Goyal, Ross Girshick, Kaiming He, and Piotr Dollár. 2017. Focal loss for dense object detection. In Proceedings of the IEEE international conference on computer vision, pages 29802988. Zhenghao Lin, Zhibin Gou, Yeyun Gong, Xiao Liu, Yelong Shen, Ruochen Xu, Chen Lin, Yujiu Yang, Jian Jiao, Nan Duan, and 1 others. 2024. Rho-1: Not all tokens are what you need. arXiv preprint arXiv:2404.07965. Aiwei Liu, Haoping Bai, Zhiyun Lu, Yanchao Sun, Xiang Kong, Simon Wang, Jiulong Shan, Albin Madappally Jose, Xiaojiang Liu, Lijie Wen, and 1 others. 2024. Tis-dpo: Token-level importance sampling for direct preference optimization with estimated weights. arXiv preprint arXiv:2410.04350. Wei Liu, Ruochen Zhou, Yiyun Deng, Yuzhen Huang, Junteng Liu, Yuntian Deng, Yizhe Zhang, and Junxian He. 2025. Learn to reason efficiently with adaptive length-based reward shaping. Preprint, arXiv:2505.15612. Team OLMo, Pete Walsh, Luca Soldaini, Dirk Groeneveld, Kyle Lo, Shane Arora, Akshita Bhagia, Yuling Gu, Shengyi Huang, Matt Jordan, and 1 others. 2024. 2 olmo 2 furious. arXiv preprint arXiv:2501.00656. Jinlong Pang, Na Di, Zhaowei Zhu, Jiaheng Wei, Hao Cheng, Chen Qian, and Yang Liu. 2025. Token cleaning: Fine-grained data selection for llm supervised fine-tuning. arXiv preprint arXiv:2502.01968. Xiaohan Qin, Xiaoxing Wang, Ning Liao, Cancheng Zhang, Xiangdong Zhang, Mingquan Feng, Jingzhi Wang, and Junchi Yan. 2025. sstoken: Selfmodulated and semantic-aware token selection for llm fine-tuning. arXiv preprint arXiv:2510.18250. David Rein, Betty Li Hou, Asa Cooper Stickland, Jackson Petty, Richard Yuanzhe Pang, Julien Dirani, Julian Michael, and Samuel Bowman. 2024. Gpqa: graduate-level google-proof q&a benchmark. In First Conference on Language Modeling. Zhiwen Ruan, Yixia Li, He Zhu, Yun Chen, Peng Li, Yang Liu, and Guanhua Chen. 2025. Enhancing large language model reasoning via selective critical token fine-tuning. arXiv preprint arXiv:2510.10974. Zhihong Shao, Peiyi Wang, Qihao Zhu, Runxin Xu, Junxiao Song, Xiao Bi, Haowei Zhang, Mingchuan Zhang, YK Li, Yang Wu, and 1 others. 2024. Deepseekmath: Pushing the limits of mathematical reasoning in open language models. arXiv preprint arXiv:2402.03300. Wentao Shi and Yiqing Shen. 2025. Reinforcement finetuning for reasoning towards multi-step multi-source search in large language models. arXiv preprint arXiv:2506.08352. Shaobo Wang, Xiangqi Jin, Ziming Wang, Jize Wang, Jiajun Zhang, Kaixin Li, Zichen Wen, Zhong Li, Conghui He, Xuming Hu, and 1 others. 2025a. Data whisperer: Efficient data selection for task-specific llm fine-tuning via few-shot in-context learning. arXiv preprint arXiv:2505.12212. Shenzhi Wang, Le Yu, Chang Gao, Chujie Zheng, Shixuan Liu, Rui Lu, Kai Dang, Xionghui Chen, Jianxin Yang, Zhenru Zhang, and 1 others. 2025b. Beyond the 80/20 rule: High-entropy minority tokens drive effective reinforcement learning for llm reasoning. arXiv preprint arXiv:2506.01939. Sean Welleck, Ilia Kulikov, Stephen Roller, Emily Dinan, Kyunghyun Cho, and Jason Weston. 2019. Neural text generation with unlikelihood training. arXiv preprint arXiv:1908.04319. Taiqiang Wu, Jiahao Wang, Zhe Zhao, and Ngai Wong. 2024. Mixture-of-subspaces in low-rank adaptation. arXiv preprint arXiv:2406.11909. Taiqiang Wu, Runming Yang, Jiayi Li, Pengfei Hu, Yik-Chung Wu, Ngai Wong, and Yujiu Yang. 2025a. Shadow-ft: Tuning instruct model via training on paired base model. Preprint, arXiv:2505.12716. Yongliang Wu, Yizhou Zhou, Zhou Ziheng, Yingzhe Peng, Xinyu Ye, Xinting Hu, Wenbo Zhu, Lu Qi, Ming-Hsuan Yang, and Xu Yang. 2025b. On the generalization of sft: reinforcement learning perspective with reward rectification. arXiv preprint arXiv:2508.05629. An Yang, Anfeng Li, Baosong Yang, Beichen Zhang, Binyuan Hui, Bo Zheng, Bowen Yu, Chang Gao, Chengen Huang, Chenxu Lv, and 1 others. 2025a. Qwen3 technical report. arXiv preprint arXiv:2505.09388. Chun-Hao Yang, Bo-Han Feng, Tzu-Yuan Lai, Yan Yu Chen, Yin-Kai Dean Huang, and Shou-De Lin. 2025b. Training llms beyond next token prediction filling the mutual information gap. arXiv preprint arXiv:2511.00198. Zheng Yuan, Hongyi Yuan, Chengpeng Li, Guanting Dong, Keming Lu, Chuanqi Tan, Chang Zhou, and Jingren Zhou. 2023. Scaling relationship on learning mathematical reasoning with large language models. Preprint, arXiv:2308.01825. Bolin Zhang, Jiahao Wang, Qianlong Du, Jiajun Zhang, Zhiying Tu, and Dianhui Chu. 2025. survey on data selection for llm instruction tuning. Journal of Artificial Intelligence Research, 83. Yaowei Zheng, Richong Zhang, Junhao Zhang, Yanhan Ye, Zheyan Luo, Zhangchi Feng, and Yongqiang Ma. 2024. Llamafactory: Unified efficient finetuning of 100+ language models. arXiv preprint arXiv:2403.13372. Chunting Zhou, Pengfei Liu, Puxin Xu, Srinivasan Iyer, Jiao Sun, Yuning Mao, Xuezhe Ma, Avia Efrat, Ping Yu, Lili Yu, and 1 others. 2023a. Lima: Less is more for alignment. Advances in Neural Information Processing Systems, 36:5500655021. Jeffrey Zhou, Tianjian Lu, Swaroop Mishra, Siddhartha Brahma, Sujoy Basu, Yi Luan, Denny Zhou, and Le Hou. 2023b. Instruction-following evaluation for large language models. arXiv preprint arXiv:2311.07911."
        },
        {
            "title": "Appendix",
            "content": "A Proof of Theorem 1 Proof A.1 Consider the loss ℓ(θ) for the target conditioned on the input and history token <t. Let RV be the logit vector and = softmax(z) be the probability distribution. Recall from Eq. 2 that the gradient of the loss with respect to the logits is zℓ = ey is the one-hot , where ey vector corresponding to the target. First, we analyze the ℓ2-norm of the logit gra- = pθ(y <t) denote the dient zℓ. Let py predicted probability of the ground-truth token. We derive: x, t (cid:115) zℓ2 = (py 1)2 + (cid:115) = (1 py )2 + 1 py . p2 p2 (cid:88) v=y (cid:88) v=y of 8,192 tokens. All models were fine-tuned for single epoch. During inference, we enable stochastic sampling across all models to ensure generation diversity. For Qwen3 and OLMo-2, we configure the hyperparameters with : do_sample=True, temperature=0.7, top_p=0.8, top_k=20. For the Llama series, we adopt configuration : do_sample=True, temperature=0.6, top_p=0.9. Regarding generation length, we set the maximum output tokens to 32,768 for the AIME24 benchmark to accommodate extensive reasoning, while limiting it to 8,192 for all other evaluations."
        },
        {
            "title": "Datasets",
            "content": "(7) We present the detailed performance trajectories for each dataset (GSM8K, MATH-500, GPQADiamond, and IFEval) across varying LoRA ranks in Figure 8. The results reveal consistent optimization patterns across diverse tasks: Next, applying the chain rule, the gradient with respect to the model parameters θ is given by: θℓ = (cid:19) (cid:18) θ zℓ = Jθ(z)zℓ, (8) where Jθ(z) RVθ is the Jacobian matrix. We utilize the spectral inequality for matrixvector products: for any matrix and vector v, Av2 σmin(A)v2. Applying this to our context: θℓ2 = Jθ(z)zℓ2 σmin(Jθ(z)) zℓ2 = σmin(Jθ(z)) zℓ2. (9) Here, we use the property that matrix and its transpose share the same singular values. Finally, substituting Eq. 7 and the non-degeneracy assumption σmin(Jθ(z)) γ, we obtain: θℓ2 γ (1 pθ(y x, <t)). (10) This concludes the proof."
        },
        {
            "title": "B Hyper parameters",
            "content": "Regarding the hyperparameter configuration, we set the per-device batch size to 1 and employed gradient accumulation strategy with 4 steps. The input sequences were truncated to maximum length Universal Monotonicity for Core Tokens (p > 0.1). striking commonality across all four datasets is the monotonic performance growth observed when training on core tokens (pink lines). Whether for reasoning (GSM8K, MATH-500), knowledge recall (GPQA), or instruction following (IFEval), the models performance steadily improves as the rank increases from 4 to 1024. This universality confirms fundamental hypothesis: core task semantics possess high intrinsic dimensionality. Consequently, providing larger parameter capacities allows the model to better resolve these critical patterns without saturation, regardless of the task type. Interference from Non-Core Expressions (p < 0.1). In contrast, focusing on non-core tokens (green lines) consistently leads to suboptimal outcomes, though the manifestation varies slightly by task: Reasoning Tasks (GSM8K, MATH-500): These tasks are particularly sensitive to noise. At high ranks (e.g., = 1024), the excess capacity leads to severe overfitting on noncore expressions, causing sharp performance drop. Knowledge & Instruction Tasks (GPQA, IFEval): The standard SFT baseline (blue Figure 8: Detailed performance comparison across different LoRA ranks for individual datasets. The trends corroborate our hypothesis: core tokens (p > 0.1) benefit from increased capacity, while non-core tokens (p < 0.1) induce optimization interference, particularly at high ranks. lines) and non-core token training often exhibit fluctuations or stagnation at medium-tohigh ranks, struggling to maintain the upward trajectory seen in the core token setting. This further highlights that filtering out non-core expressions is essential for stable scaling."
        },
        {
            "title": "D Detailed Analysis of Training\nDynamics across Epochs",
            "content": "We further examine the training stability and convergence speed on individual datasets, as illustrated in Figure 9. The trajectories provide granular evidence for the efficiency of our method: Rapid Convergence and Stability. Across all benchmarks, ProFit settings (solid lines, > τ ) demonstrate superior convergence efficiency, typically reaching near-optimal performance within just 2 epochs and maintaining stability throughout the training process. In contrast, the SFT baseline often requires more steps to plateau or exhibits fluctuations. Overfitting to Non-Core Expressions. The risks of training on low-probability tokens are clearly visible in reasoning tasks. For instance, in GSM8K and MATH-500, the performance of the < 0.1 setting (yellow line) peaks early but subsequently degrades as training progresses (Epoch 3-5). This inverted-U pattern strongly suggests that prolonged exposure to low-probability tokens leads the model to overfit to non-core expressions , thereby impairing its underlying reasoning logic. The Gap in Instruction Following. In IFEval, distinct performance chasm is observed: models trained on high-probability tokens stabilize around 50% accuracy, whereas those focused on low-probability tokens stagnate below 30%. This indicates that the core semantics required for instruction following are almost exclusively encoded Figure 9: Performance evolution across training epochs for individual datasets. ProFit (p > τ ) exhibits rapid convergence and stability, whereas training on low-probability tokens (p < τ ) suffers from instability and overfitting to non-core expressions. in high-probability tokens, while low-probability tokens contribute little to this capability."
        },
        {
            "title": "E Case Study",
            "content": "As shown in Table 2, SFT succumbs to logical hallucinations by ignoring key interaction terms, whereas ProFit maintains coherent reasoning chain to derive the correct solution. This demonstrates that masking non-core expressions effectively safeguards the models core logic against superficial errors."
        },
        {
            "title": "F Benchmark Details",
            "content": "We evaluate our method on diverse benchmarks covering reasoning and instruction following capabilities: IFEval (Zhou et al., 2023b): Evaluates the models ability to follow verifiable formatting instructions and constraints. GSM8K (Cobbe et al., 2021): classic benchmark consisting of grade-school math word problems requiring multi-step reasoning. MATH-500 (Lightman et al., 2023): challenging subset of 500 competition-level mathematics problems from the Minerva dataset. AIME24 (American Institute of Mathematics, 2024): set of high-difficulty problems from the 2024 American Invitational Mathematics Examination, testing frontier mathematical capabilities. GPQA-Diamond (Rein et al., 2024): dataset of 198 expert-level, Google-proof science questions testing advanced reasoning. Minerva (Hendrycks et al., 2021): comprehensive dataset of 12,500 challenging competition mathematics problems ranging from Figure 10: Training Dynamics in RL Stage. We compare the KL Divergence, Entropy, and Response Length of models initialized with Base, DFT, and ProFit strategies. ProFit demonstrates superior stability (low KL), confident convergence (low Entropy), and evolves deeper reasoning capabilities (highest Response Length). pre-algebra to calculus, serving as standard benchmark for mathematical reasoning. OlympiadBench (He et al., 2024): largescale, bilingual, and multimodal benchmark featuring Olympiad-level problems in mathematics and physics, designed to evaluate AGI capabilities in complex scientific reasoning."
        },
        {
            "title": "Stage",
            "content": "To further investigate the impact of different initialization strategies on the reinforcement learning (RL) process, we visualized the training dynamics of three key metrics: KL Divergence, Entropy, and Response Length. The comparative results are presented in Figure 10. KL Divergence Stability. As shown in the left panel of Figure 10, the Base model (blue line) exhibits rapid and uncontrolled increase in KL divergence, reaching approximately 0.17 by the end of training. This sharp rise suggests that without robust SFT warm-up, the policy drifts significantly from the reference model, potentially leading to reward hacking or language degeneration. In contrast, ProFit (red line) and DFT (green line) maintain remarkably low and stable KL divergence (staying below 0.05). This indicates that ProFit effectively constrains the policy update within safe trust region, ensuring that the model improves its mathematical reasoning capabilities while preserving its general linguistic coherence. Entropy. The entropy curves reveal distinct convergence behaviors. The Base model demonstrates chaotic behavior, where entropy initially collapses and then drastically rebounds to extremely high values (> 6.0), indicating failure to converge to stable policy and likely degenerating into generating high-randomness noise. Conversely, ProFit maintains consistently low entropy (ending around 0.09), similar to DFT. This low entropy signifies high confidence in the generated reasoning paths. Notably, unlike DFT which saturates near 0.03, ProFit maintains slightly higher entropy margin, suggesting it retains healthy level of exploration potential while remaining focused on high-reward solutions. Response Length. The most significant differentiator lies in the response length, which serves as proxy for the depth of Chain-of-Thought (CoT) reasoning. While all models show an increasing trend in response length, ProFit demonstrates the most robust growth pattern. Starting from 1,400 tokens, ProFit rapidly learns to expand its reasoning steps, surpassing the Base model around step 150 and eventually achieving the highest average response length of over 8,000 tokens at the end. Compared to DFT (ending at 7,100 tokens) and Base (6,400 tokens), ProFits superior length indicates that the probability-guided initialization encourages the RL algorithm to explore deeper and more complex reasoning chains. This aligns perfectly with our main results, where ProFit excels in complex tasks like OlympiadBench, which require extensive multi-step deductions. Question Reference SFT The polynomial x3 3x2 + 4x 1 is factor of x9 + px6 + qx3 + r. Enter the ordered triple (p, q, r). Please reason step by step, and put your final answer within . Let α be root of x3 3x2 + 4x 1 = 0, so α3 = 3α2 4α + 1. Substituting this relation repeatedly into higher powers: α9 = 63α2 + 190α 54. Then α9 + pα6 + qα3 + = (5p + 3q 63)α2 + (11p 4q + 190)α + (4p + + 54). For this to be 0 for all roots, we set coefficients to 0: 5p + 3q = 63, 4p + + = 54. Solving this system yields (p, q, r) = (6, 31, 1) . 11p + 4q = 190, To determine the ordered triple (p, q, r), we assume x9 + px6 + qx3 + = (x3 3x2 + 4x 1) g(x), where g(x) = ax6 + bx5 + cx4 + dx3 + ex2 + + g. Step 4: Match coefficients Lets expand the product step-by-step: The x9 term: x3 ax6 = ax9 = = 1. The x8 term: x3 bx5 = bx8 = = 0 (Incorrectly ignores interaction terms). The x7 term: x3 cx4 = cx7 = = 0. The x6 term: Combining terms yields (d 3)x6 cx4. We match this to p: 3 = and = 0. The x5 term: Combining terms yields ex5 dx3. We need this to match 0: = 0 and = 0. The x4 term: Combining terms yields x4. We need this to match 0: = 0. The constant term: = = = r. Step 5: Combine the results From the above steps, we have: = 1, = 0, = 0, = 0, = 0, = 0, = 0, = 0. Thus: = 3 = 0 3 = 3, = = 0, = 0. Final Answer: (3, 0, 0) ProFit To determine the ordered triple (p, q, r), we denote (x) = x3 3x2 + 4x 1. We assume g(x) = x6 +ax5 +bx4 +cx3 +dx2 +ex+f . Then: (x3 3x2 +4x1)(x6 +ax5 +...+f ) = x9 +px6 +qx3 +r. Step-by-step Coefficient Comparison: Expanding the LHS and comparing with x9 + px6 + qx3 + r: 1. x8 coeff: 3 = 0 = = 3. 2. x7 coeff: 3a + 4 = 0 = 9 + 4 = 0 = = 5. 3. x6 coeff: 3b + 4a 1 = = 15 + 12 1 = = = 4. 4. x5 coeff: 3c + 4b 1a = 0 = 3c + 20 3 = 0 = = 3c 17. 5. x4 coeff: 3d + 4c 1b = 0 = 3(3c 17) + 4c 5 = 0 = = 5c 46. 6. x3 coeff: 3e + 4d 1c = = 3(5c 46) + 4(3c 17) = = = 4c + 70. 7. x2 coeff: 3f + 4e 1d = 0 = ... = = 17c167 3 . 8. x1 coeff: + 4e 1d = 0 = ... = 34c 340 = 0 = = 10. 9. Constant: = = = 1. Final Calculation: Now, substituting = 10 back into the expressions: = 4 = 10 4 = 6. = 4c + 70 = 1 40 + 70 = 31. = 1. Thus, the ordered triple is: (6, 31, 1) Table 2: Comparison of model outputs on the polynomial factorization task. SFT hallucinates constraints (e.g., setting = 0 and = 0 independently), leading to an incorrect solution. ProFit maintains coherent logical chain throughout the coefficient comparison, deriving the correct ordered triple. Figure 11: Visualization of token probability"
        }
    ],
    "affiliations": [
        "The University of Hong Kong",
        "Tsinghua University"
    ]
}