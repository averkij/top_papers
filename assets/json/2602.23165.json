{
    "paper_title": "DyaDiT: A Multi-Modal Diffusion Transformer for Socially Favorable Dyadic Gesture Generation",
    "authors": [
        "Yichen Peng",
        "Jyun-Ting Song",
        "Siyeol Jung",
        "Ruofan Liu",
        "Haiyang Liu",
        "Xuangeng Chu",
        "Ruicong Liu",
        "Erwin Wu",
        "Hideki Koike",
        "Kris Kitani"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Generating realistic conversational gestures are essential for achieving natural, socially engaging interactions with digital humans. However, existing methods typically map a single audio stream to a single speaker's motion, without considering social context or modeling the mutual dynamics between two people engaging in conversation. We present DyaDiT, a multi-modal diffusion transformer that generates contextually appropriate human motion from dyadic audio signals. Trained on Seamless Interaction Dataset, DyaDiT takes dyadic audio with optional social-context tokens to produce context-appropriate motion. It fuses information from both speakers to capture interaction dynamics, uses a motion dictionary to encode motion priors, and can optionally utilize the conversational partner's gestures to produce more responsive motion. We evaluate DyaDiT on standard motion generation metrics and conduct quantitative user studies, demonstrating that it not only surpasses existing methods on objective metrics but is also strongly preferred by users, highlighting its robustness and socially favorable motion generation. Code and models will be released upon acceptance."
        },
        {
            "title": "Start",
            "content": "6 2 0 2 6 2 ] . [ 1 5 6 1 3 2 . 2 0 6 2 : r DyaDiT: Multi-Modal Diffusion Transformer for Socially Favorable Dyadic Gesture Generation Yichen Peng1,2 Jyun-Ting Song2 Siyeol Jung2,3 Ruofan Liu1 Haiyang Liu4,5 Xuangeng Chu4,5 Ruicong Liu4,5 Erwin Wu1,4 Hideki Koike1 Kris Kitani2 1Institute of Science Tokyo 2Carnegie Mellon Unversity 3UNIST 4Shanda AI Research Tokyo 5The University of Tokyo peng.y.ag@m.titech.ac.jp Figure 1. DyaDiT generates socially aware conversational gestures from dyadic audio, conditioned on social factors such as relationship and personality traits, achieving natural and contextually appropriate reactions that outperform prior methods in both quantitative and user evaluations."
        },
        {
            "title": "Abstract",
            "content": "Generating realistic conversational gestures are essential for achieving natural, socially engaging interactions with digital humans. However, existing methods typically map single audio stream to single speakers motion, without considering social context or modeling the mutual dynamics between two people engaging in conversation. We present DyaDiT, multi-modal diffusion transformer that generates contextually appropriate human motion from dyadic audio signals. Trained on Seamless Interaction Dataset [1], DyaDiT takes dyadic audio with optional social-context tokens to produce context-appropriate motion. It fuses information from both speakers to capture 1 interaction dynamics, uses motion dictionary to encode motion priors, and can optionally utilize the conversational partners gestures to produce more responsive motion. We evaluate DyaDiT on standard motion generation metrics and conduct quantitative user studies, demonstrating that it not only surpasses existing methods on objective metrics but is also strongly preferred by users, highlighting its robustness and socially favorable motion generation. Code and models will be released upon acceptance. 1. Introduction Building synthetic agents (also known as digital humans, AI agents, avatars or androids) that can interact naturally with people, is essential for the future of humancomputer interfaces. Recent language models such as GPT-4.5 [33] and LLaMA-3.1 [12] already demonstrate impressive conversational ability, and many users even feel as if they are speaking with another person [15]. However, currently this illusion still remains confined to text window. In reality, human interaction involves more than just spoken words. People gesture, respond to each other, and express subtle social cues through body motion. In order for humans to truly feel that synthetic agent is interactive, the agent must accompany its speech with gestures that evolve naturally with the conversation. However, generating such gestures is challenging. They are strongly shaped by social factors such as personality, the relationship between speakers, and their conversational roles. These cues influence how people move, how they react, and how they coordinate with one another, yet most existing gesture generation models do not explicitly model them [26, 27, 42]. As result, their gestures often appear generic or fake. In addition to social context, dyadic conversation, which happens between two interacting individuals, exhibits speech dynamics that are difficult for gesture generation models to handle. Two people can speak at the same time, interrupt one another, or rapidly switch between speaking and listening [8, 28]. Their audio streams merge together, making it difficult to separate who is speaking and who is responding [19, 30, 39]. Most existing dyadic gesture generation models [29, 32], however, simply fuse the two speech streams into one, making the contribution of each speaker blurry and therefore weakening the correspondence between the generated gestures and the underlying interaction. To tackle these limitations, we introduce DyaDiT, diffusion based transformer that generates socially aware gestures from dyadic audio. Unlike previous work [10, 22, 23, 36] that focuses only on the alignment between audio and generated motion, DyaDiT conditions its generation on explicit social cues such as relationship and personality. Furthermore, in order to address the strong entanglement of two overlapping audio streams during conversation, we propose Orthogonalization Cross Attention (ORCA), simple yet effective module that disambiguates the two audio streams, resulting in cleaner audio representation for better gesture generation. Lastly, since human motion in dyadic setting is often affected by the partners gestures, DyaDiT can optionally take the partners movements as an additional input, allowing the model to generate gestures that are more coordinated, responsive, and natural. Our experiments show that DyaDiT consistently outperforms existing dyadic gesture generation methods across standard quantitative metrics, achieving clear improvements in gesture quality and distribution alignment. In addition, we conduct extensive user studies to assess human perceptual preference. The results show that participants strongly favor gestures generated by DyaDiT, indicating that our generated motion appears more socially aware and better suited for real conversational settings. In summary, our main contributions are as follows: We propose DyaDiT, diffusion transformer (DiT) that generates social context aware gestures in dyadic conversations. We introduce an Orthogonalization Cross Attention (ORCA) module that reduces interference between two individuals audio streams, enabling cleaner audio representation for better gesture generation in dyadic conversation. Through extensive quantitative evaluations and user studies, we show that DyaDiT consistently outperforms existing methods on standard metrics and is preferred by users in terms of perceived realism and social consistency. 2. Related Work 2.1. Co-Speech Gesture Generation Co-speech gesture generation focuses on synthesizing body movements aligned with single speakers speech. Early approaches formulate gesture generation as translation problem from multimodal speech cues, combining text, audio, and speaker identity through recurrent or adversarial models, such as the trimodal framework of Yoon et al. [49], which treated gesture generation as translation problem, combining speech transcripts, audio, and speaker identity through an adversarial recurrent framework. Liu et al. [22] introduced the BEAT dataset and CaMN, cascaded multimodal adversarial network capable of generating synchronized body and hand gestures. Other works, such as EMAGE [23], TalkSHOW [47], MECo [5], etc. [11, 21, 24, 34, 46] enhanced realism by disentangling rhythmic and semantic motion features or integrating discrete latent spaces via VQ-VAEs. Although these advances are notable, most of them focuses on single-speaker co-speech gestures and overlooks Figure 2. Overview of DyaDiT. DyaDiT conditions on multiple input modalities, including audio, partner motion, relationship type, and personality scores. It employs an Audio Orthogonalization Cross Attention (ORCA) module to obtain cleaner audio representations and motion dictionary to guide style aware gesture generation. the interactive nature of human communication. More complex settings such as dyadic gesture generation, which require modeling both participants behaviors and their interpersonal dynamics, remain largely unexplored. 2.2. Dyadic Gesture and Reaction Generation Unlike co-speech gesture synthesis, dyadic gesture generation must model the coordinated behavior between two participants, including interpersonal timing, mutual attention, and responsiveness. Most prior works in this domain stem from facial reaction generation (FRG), where the goal is to predict non-deterministic listener facial responses to speakers behavior [16, 2527, 31, 38, 50]. Although FRG provides basis for modeling two-person interactions, it primarily focuses only on facial or head movements rather than full-body gestures. Due to this limitation, several recent works have begun exploring body gesture generation in dyadic settings. few efforts such as Audio2Photoreal [32], ConvoFusion [29], and TAG2G [9] extend single-speaker frameworks to two-party settings. Yet, these models often either (1) overlook the social context between the two individuals or (2) treat dyadic audio as single blended signal without explicitly modeling cross-person dynamics, which often leads to ambiguity in the roles and interaction patterns presented to the model. These limitations highlight the need for explicit feature disentanglement and better support for socially contextual reasoning. 2.3. Diffusion-based Gesture Generation Diffusion models have emerged as powerful generative tools for human motion due to their ability to model multimodal and many-to-many distributions. Several works have adapted diffusion frameworks originally designed for textconditioned motion generation, such as MotionDiffuse [52], FineMoGen [51], and MDM [41], etc. [3, 6, 37, 40], to the speechgesture domain. Alexanderson et al. [2] reformulated DiffWave for co-speech gestures, while Zhu et al. [55] proposed DiffGesture, integrating noisy gesture sequences with contextual embeddings for temporal modeling. DiffuseStyleGesture+ [45] further introduced conditioning on audio, text, style, and seed gestures, leveraging transformer-based denoising with attention control. Other works, such as UnifiedGesture [44], LivelySpeaker [53], and AMUSE [7], emphasized semantic and rhythmic consistency, disentangling emotional and stylistic latent factors. Considering the success of diffusion-based approaches in gesture generation, and the inherently nondeterministic nature of dyadic conversation, we build our method upon diffusion model to better capture the variability and dynamics present in two-person interactions. 3 Figure 3. ORCA reduces ambiguity between the two audio streams, allowing DyaDiT to generate realistic motion even when one person interrupts the other during the conversation. The example demonstrates the generated motions adjusts naturally as the conversation shifts. 3. DyaDiT We introduce DyaDiT, diffusion transformer (DiT) with multi-modal input for dyadic gesture generation. Given the conversational audio streams from two individuals (denoted as self and other), our objective is to generate plausible upper-body gestures for the other speaker in accordance with the conversational context. To further enhance the realism of the generated gestures, DyaDiT optionally incorporates the relationship, personality traits, and gesture sequence of self as auxiliary conditioning signals during inference. We train our model on subset of the Seamless Interaction dataset [1], with dataset specifications and preprocessing procedures detailed in Section 3.1. An overview of the full architecture is shown in Fig. 2. Details of the Audio Orthogonalization Cross Attention (ORCA) module are provided in Section 3.3, while the motion dictionary and motion tokenizer are presented in Sections 3.4 and 3.5, respectively. 3.1. Seamless Interaction Dataset The Seamless Interaction dataset [1] is large-scale corpus of dyadic conversation containing synchronized audio, fullbody motion, and facial expressions. In this work, we use curated subset of 3,000 clips (approximately 182 hours) from the datasets naturalistic scenario collection, which contains rich and spontaneous dyadic conversations suited for modeling conversational gestures. We focus on generating upper-body gestures, represented as RT J6 using the 6D rotation representation [54], where denotes the number of upper-body joints. In addition to motion data, we incorporate two high level social annotations provided in the dataset: relationship type label frs {0, 1}4 indicating whether the speakers are friends, strangers, family members, or dating partners; and personality score vector fps R5 that quantifies five major personality traits, which are extraversion, agreeableness, conscientiousness, neuroticism, and openness. For audio inputs, we extract the dyadic speech signals from both individuals and process them with pretrained Wav2Vec2 encoder [4] to obtain audio feature embeddings for conditioning in our model. 3.2. DiT Backbone Our model adopts diffusion transformer (DiT) backbone that follows the Denoising Diffusion Probabilistic Model (DDPM) [14] framework. The network takes noisy latent pose xt and predicts the added Gaussian noise according to time step t, ϵθ(xt, t, c) conditioned on multiple contextual inputs c, where = (ORCA(aself, aother), pself, frelat, fps) including audio, partners motion, relationship type, and personality scores. The training objective follows the standard ϵ-prediction loss: Ldiff = Ex0,t,ϵ (cid:104) ϵ ϵθ(xt, t, c)2 (cid:105) , (1) where xt = αtx0 + 1 αtϵ. As shown in Figure 2(a), each DiT block consists of self-attention layer for modeling temporal dependencies 4 within the latent pose sequence and cross-attention layer for integrating contextual information from multimodal cues. In addition, the relationship and personality embeddings are injected through both FiLM-based [35] modulation and cross-attention, enabling DyaDiT to jointly capture social attributes and individual expressive styles in gesture generation. 3.3. Audio Orthogonalization Cross Attention (ORCA) To effectively capture conversational dynamics between two speakers, we introduce orthogonalization cross attention module for audio fusion (Figure 2(b)). Given the audio features from both speakers aself and aother, encoded by Wav2Vec2, our goal is to separate redundant factors while aligning complementary information into joint representation. We first apply an orthogonalization process to filter out redundant components between the two audio streams: self = aself Projaother (aself), (2) where Projaother () denotes the projection of the self audio feature aself onto the subspace of the others audio feature. This operation enforces complementary conditioning and reduces correlated information across dyadic audio. Subsequently, we employ two symmetric cross-attention modules to achieve bidirectional information exchange. The first module uses aother as query and attends to self, capturing the speakers response to the partners utterance. Conversely, the second module takes self as query and attends to aother, modeling the listeners reactive cues. The outputs of the two cross-attention streams are then adaptively fused through learnable gating mechanism: faudio = σ(Wg)hselfother +(1σ(Wg))hotherself, (3) where σ() is sigmoid function and Wg is learnable gate parameter. The resulting fused token faudio is used as the final audio conditioning input to the DyaDiT, providing plausible acoustic embedding that reflects both interlocutors vocal behaviors. As shown in Figure 3, with ORCA, DyaDiT have the capability to generate either speaker or listener gestures. 3.4. Motion Dictionary (MD) Inspired by prior work LIA [43] which learns set of orthogonal motion-directions by enforcing orthonormality at initialization, we similarly introduce learnable orthogonal motion dictionary to modulate the partners audio feature aother according to the current style motion. The motion dictionary is module designed to incorporate motion style conditioning when user requires motion style control. As shown in Figure 2(c), it consists of set of learnable motion bases {d0, d1, . . . , dn} that encode representative gesture primitives. During training, we include ground truth motion style features fmotion = [m0, m1, . . . , mn] to guide the model in learning style-aware correspondences between audio cues and motion patterns. Given the other-speaker audio feature aother, the dictionary is integrated through cross-attention (CA) operation followed by weighted combination with the motion bases: other = CA(aother, (cid:88) k=0 mkdk) + aother, (4) where Dict = [d0, d1, . . . , dn] and mk denotes the styledependent modulation weight derived from fmotion. At inference, the motion dictionary can be optionally activated. With classifier-free guidance (CFG) [13], the style conditioning can be strengthened to generate gestures with distinct stylistic patterns, or completely dropped to produce an unconditional, style-agnostic motion driven purely by the learned audiomotion prior. 3.5. Motion Tokenizer Instead of directly training our DiT model using the raw 0 RT , we first train VQ-VAE gesture sequences to discretize the continuous motion space into compact latent representations. Our implementation follows the residual vector quantization framework introduced in prior works [18], where multiple codebooks are cascaded to progressively refine quantization errors. Specifically, we adopt residual VQ-VAE with residual length of 4, utilizing four hierarchical codebooks to capture progressively finergrained motion details. In addition, to reduce temporal redundancy, we downsample the input gesture sequence by factor of 4 using 1D convolution layers in the encoder, resulting in one latent token for every four frames. During training of DyaDiT, the VQ-VAE encoder compresses gesture sequences into sequence of quantized tokens, RT /4d, where = 64 in our experiment. At inx ference time, the VQ-VAE decoder then reconstructs continuous motions from the generated quantized embeddings. Note that since our diffusion model operates directly in the latent space, the encoder is not used during inference, and the decoder is not used during training. This hierarchical and temporally compact quantization allows the diffusion model to capture long-range dependencies while maintaining high-fidelity motion reconstruction. 4. Experiments We evaluate the effectiveness of DyaDiT for dyadic gesture generation through both quantitative and qualitative experiments. For the quantitative evaluation, we compute 5 FD Diversity 4.2. Baseline Methods Method GT GT Random ConvoFusion [29] Audio2PhotoReal [20] DyaDiT (w/o ORCA) DyaDiT (w/o MD) DyaDiT (Uncond) DyaDiT (Random) DyaDiT Sta. Kine. Sta. Kine. - 14.94 9.22 8.77 7.32 6.88 7.40 8.24 6.40 - 3. 1.74 1.84 1.79 1.75 1.63 1.53 1.37 28.42 33.85 18.33 19.35 23.57 18.34 21.65 21.94 27.46 1.97 2. 1.10 1.05 1.24 1.29 1.16 1.43 1.38 Table 1. Quantitative comparison of DyaDiT and baselines in terms of Frechet Distance (FD) and Diversity. Lower FD indicates higher realism, and higher diversity values indicate more varied motion generation. the Frechet Distance (FD) and Diversity metrics to compare DyaDiT with two existing dyadic gesture generation models, ConvoFusion [29] and Audio2PhotoReal [32]. In addition, we report scores with respect to the ground truth gestures, which serve as reference for the underlying motion distribution in the dataset. For the qualitative evaluation, we conduct user preference study to evaluate both the overall perceived quality of the generated gestures and their alignment with the social relationship and personality characteristics. 4.1. Evaluation Metrics Following prior works on gesture generation [2, 3, 32], we adopt both distribution and diversity metrics for quantitative evaluation. Specifically, we report the Frechet Distance (FD) to measure the realism of generated motions, Diversity [20] to quantify motion variability across samples For reference, we also compute all metrics on ground truth samples from the Seamless Interaction dataset[1] to provide as baseline for comparison. Below, we provide the definitions of the quantitative metrics used in our evaluation. FD (Static) measures the pose realism for individual frames in the pose space R3j, where j=43 (including finger joints). FD (Kinetic) evaluates the realism of temporal dynamics by computing the FD over gesture velocities in RT 3j, where =300 frames in our experiments. Diversity (Static) quantifies the variation of single-frame poses by computing the average mean squared error (MSE) between clips. Diversity (Kinetic) measures temporal motion diversity by averaging the pairwise velocity differences across generated gesture sequences. 6 We compare DyaDiT with two representative dyadic gesture generation models: ConvoFusion [29] implement diffusion-based framework with multiple layers of cross-attention to fuse multimodal inputs, including audio and visual features. For fair comparison, we retrain their model on our training subset of the Seamless Interaction Dataset[1] to match our experimental domain and output representation. Audio2PhotoReal [32] is full-body gesture generation model that jointly synthesizes body and facial motion. We extract and adapt only the body gesture generation branch for comparison. Specifically, the model first predicts keyframe poses from audio using causal transformer, and then employs diffusion transformer to interpolate between keyframes to produce temporally coherent motion sequences. Likewise, we retrain the model on our dataset to ensure domain consistency with our evaluation study setup. During inference, both methods use DDIM scheduler with 50 denoising steps (CFG = 2) to generate 300-frame gesture sequences (10 seconds). 4.3. Quantitative Results The quantitative comparison between DyaDiT and the baseline models is shown in Table 1. For interpretability, the GT diversity reflects the datasets upper bound, while the GT Random FD serves as its lower bound. Across all metrics, DyaDiT consistently outperforms existing baselines, achieving substantially lower FD(static) and FD(kinetic) scores while preserving high motion diversity. These results demonstrate that the proposed social-context-aware DiT framework is more effective than the multi-branch fusion design in ConvoFusion [29] and the keyframe interpolation approach used in Audio2PhotoReal [20]. 4.4. Ablation Studies We further conduct several ablation studies to analyze how different components and conditioning signals contribute to DyaDiTs performance. Below, we explicitly explain the difference between each variant. DyaDiT (w/o ORCA) removes the ORCA module and directly concatenate the raw dyadic audio features. DyaDiT (w/o MD) removes the motion dictionary. DyaDiT (Uncond) generates gestures without any conditioning. DyaDiT (Random) performs inference with randomly assigned relationship and personality labels. As expected, we observe that removing the ORCA module leads to noticeable degradation in FD performance, confirming its contribution to motion realism. Likewise, removing the Motion Dictionary also reduces overall performance, especially in Diversity (Static), which drops by 9.12, Figure 4. Qualitative Results. Comparison of visualization results between DyaDiT, ConvoFusion [29], and Audio2PhotoReal [3]. The gestures generated by DyaDiT exhibit higher diversity and greater realism compared to the other methods. suggesting that the learned motion bases play an important role in encouraging style variation. In addition, we observe that DyaDiT outperforms both DyaDiT (Uncond) and DyaDiT (Random) in terms of generation quality. This demonstrates that social context is essential for producing more realistic gesture generation. Furthermore, we find that the Diversity (Static) metric drops by around 6 points when social context is removed or randomly assigned. We believe this is because the audio signal is positively correlated with both relationship and personality traits, and mismatched or missing social context leads to more restricted motion distribution. Lastly, we observe that DyaDiT (Uncond) does not exhibit higher diversity than the fully conditioned DyaDiT model. We believe this is due to the nature of dyadic conversations, where substantial portion of the data consists of listener behavior, which naturally shows less variation in both static and kinetic gestures compared to speaker behavior. As result, additional conditioning signals help to guide the model to produce wider spread of gesture variations, leading to greater overall diversity. 5. User Study We conduct an A/B preference study with sixteen participants to assess user preference on human motion in dyadic conversation settings. We compare our method with ConvoFusion [29] as well as with ground truth motion. The user study evaluates three aspects of the generated gestures: overall quality, relationship consistency, and personality consistency. Participants are shown paired clips and select their preferred gesture sequence, with the video pairs presented in randomized order to avoid ordering bias. An example video pair is shown in Figure 6. We randomly selected 56 ten-second sequences from the validation set of the Seamless Interaction dataset to conduct the experiment. Below we show sample questions presented to the participants in each section. overall quality: Which gesture of the orange character looks more human-like? relationship consistency: Which pair seems more likely to be friends (or strangers, or family members, or dating partners)? personality consistency: Which gesture better reflects the personality trait agreeable (or conscientious, or extraverted, or neurotic)? For each video pair, participants were asked to provide both their preference and their confidence level (strongly or slightly). For the full questionnaire and interface design used in the experiment, please refer to the supplementary material. As shown in Figure 7, DyaDiT achieves notably higher user preference scores than ConvoFusion [29], and interestingly, even slightly outperforms the ground truth gestures from the Seamless Interaction dataset. Specifically, DyaDiT is preferred by 73.9%, 69.8%, and 66.7% of 7 Figure 7. A/B subjective evaluation percentages comparing our method with ConvoFusion[29] and with ground truth. Participants preferred our generated motion due to its more natural and socially aware conversational behavior. motion and produces smoother, more temporally consistent gestures; and (2) conditioning on social context encourage the model to generate slightly more expressive or exaggerated motions, which, although less common in real interactions, tend to be more visually engaging to our participants. Lastly, It is worth noting that we do not compare our generated gestures with the ground truth in the personality consistency section. This is because the personality annotations in the dataset are provided as continuous values rather than discrete categories, making it difficult for participants to reliably judge whether gesture aligns more strongly with specific personality trait. 6. Conclusion In this work, we present DyaDiT, multi-modal dyadic gesture generation model designed to produce socially consistent and natural conversational behaviors between two speakers. By jointly conditioning on dyadic audio, social attributes, and the partners motion, DyaDiT effectively captures interpersonal dynamics in dyadic conversation. Our orthogonalization cross-attention (ORCA) module helps clarify the contribution of each speakers audio, and the motion dictionary enhances expressive richness by providing style-aware motion priors. Both quantitative and subjective evaluations demonstrate that DyaDiT achieves superior performance compared to existing methods in terms of realism, diversity, and social coherence. We believe this work takes an important step toward socially favorable synthetic agents and opens up future research directions, such as socially aware dual-agent gesture generation. Figure 5. Visualization results under different personality score conditionings. All samples are generated using classifier-free guidance with CFG = 2.5. Figure 6. Example video pairs used in the user study for evaluating participant preference in conversational gesture generation. participants in the overall gesture quality, relationship consistency, and personality consistency sections, respectively. These results demonstrate that DyaDiT not only produces higher-quality gestures, but also generates motions that are more socially appropriate in dyadic conversational settings. Furthermore, our generated gestures are preferred over the ground-truth gestures by 1% and 1.7% in the two comparison settings. We hypothesize this is mainly because (1) the ground-truth motion data occasionally contains jittering artifacts, whereas the diffusion process implicitly regularizes 8 7. Limitation & Future Work Currently, our model only generates upper-body gestures due to the limitations of the dataset. However, we believe that the proposed framework can be naturally extended to full-body gesture and facial expression generation. Furthermore, although our model can generate socially favorable gestures according to the given relationship or personality conditioning, some personality cues may be unintentionally contained in the audio input, leading to conditioning conflicts and reduced diversity in the generated motions. To further improve controllability and stabilize diversity, our future work will explore audio neutralization techniques to enable more diverse gesture generation."
        },
        {
            "title": "References",
            "content": "[1] Vasu Agrawal, Akinniyi Akinyemi, Kathryn Alvero, Morteza Behrooz, and et al. Seamless interaction: Dyadic audiovisual motion modeling and large-scale dataset. 2025. 1, 4, 6, 2 [2] Simon Alexanderson, Rajmund Nagy, Jonas Beskow, and Gustav Eje Henter. Listen, denoise, action! audio-driven motion synthesis with diffusion models. ACM Trans. Graph., 42 (4), 2023. 3, 6 [3] Tenglong Ao, Zeyi Zhang, and Libin Liu. Gesturediffuclip: Gesture diffusion model with clip latents. ACM Trans. Graph. 3, 6, 7 [4] Alexei Baevski, Henry Zhou, Abdelrahman Mohamed, and Michael Auli. wav2vec 2.0: framework for self-supervised In Proceedings of the learning of speech representations. 34th International Conference on Neural Information Processing Systems, Red Hook, NY, USA, 2020. Curran Associates Inc. 4, 1 [5] Bohong Chen, Yumeng Li, Youyi Zheng, Yao-Xiang Ding, and Kun Zhou. Motion-example-controlled co-speech gesIn Proture generation leveraging large language models. ceedings of the Special Interest Group on Computer Graphics and Interactive Techniques Conference Conference Papers, New York, NY, USA, 2025. Association for Computing Machinery. 2 [6] Xin Chen, Biao Jiang, Wen Liu, Zilong Huang, Bin Fu, Tao Chen, and Gang Yu. Executing your commands via motion diffusion in latent space. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 1800018010, 2023. [7] Kiran Chhatre, Radek Danˇeˇcek, Nikos Athanasiou, Giorgio Becherini, Christopher Peters, Michael J. Black, and Timo Bolkart. AMUSE: Emotional speech-driven 3D body animation via disentangled latent diffusion. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pages 19421953, 2024. 3 [8] Michael Dunne and Sik Hung Ng. Simultaneous speech in small group conversation: All-together-now and one-at-atime? Journal of Language and Social Psychology, 13(1): 4571, 1994. 2 [9] Filippo Favali, Viktor Schmuck, Valeria Villani, and Oya Celiktutan. Tag2g: diffusion-based approach to interlocutoraware co-speech gesture generation. Electronics, 13(17), 2024. 3 [10] Chencan Fu, Yabiao Wang, Jiangning Zhang, Zhengkai Jiang, Xiaofeng Mao, Jiafu Wu, Weijian Cao, Chengjie Wang, Yanhao Ge, and Yong Liu. Mambagesture: Enhancing co-speech gesture generation with mamba and disIn Proceedings of the entangled multi-modality fusion. 32nd ACM International Conference on Multimedia, page 1079410803, New York, NY, USA, 2024. Association for Computing Machinery. 2 [11] Shiry Ginosar, Amir Bar, Gefen Kohavi, Caroline Chan, Andrew Owens, and Jitendra Malik. Learning individual styles In 2019 IEEE/CVF Conference of conversational gesture. on Computer Vision and Pattern Recognition (CVPR), pages 34923501, 2019. 2 [12] Aaron Grattafiori, Abhimanyu Dubey, and Abhinav Jauhri et al. The llama 3 herd of models, 2024. 2 [13] Jonathan Ho and Tim Salimans. Classifier-free diffusion guidance. In NeurIPS 2021 Workshop on Deep Generative Models and Downstream Applications, 2021. 5 [14] Jonathan Ho, Ajay Jain, and Pieter Abbeel. Denoising diffusion probabilistic models. arXiv preprint arxiv:2006.11239, 2020. 4 [15] Cameron R. Jones and Benjamin K. Bergen. Large language models pass the turing test, 2025. 2 [16] Siyeol Jung and Taehwan Kim. Difflistener: Discrete difIn ICASSP 2025 - fusion model for listener generation. 2025 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), pages 15, 2025. 3 [17] Rawal Khirodkar, Jyun-Ting Song, Jinkun Cao, Zhengyi Luo, and Kris Kitani. Harmony4d: video dataset for inthe-wild close human interactions. In Advances in Neural Information Processing Systems, pages 107270107285. Curran Associates, Inc., 2024. [18] Doyup Lee, Chiheon Kim, Saehoon Kim, Minsu Cho, and Wook-Shin Han. Autoregressive image generation using residual quantization, 2022. 5, 1 [19] Younglo Lee, Shukjae Choi, Byeong-Yeol Kim, Zhong-Qiu Wang, and Shinji Watanabe. Boosting unknown-number speaker separation with transformer decoder-based attracIn ICASSP 2024-2024 IEEE International Conference tor. on Acoustics, Speech and Signal Processing (ICASSP), pages 446450. IEEE, 2024. 2 [20] Jing Li, Di Kang, Wenjie Pei, Xuefei Zhe, Ying Zhang, Linchao Bao, and Zhenyu He. Audio2gestures: Generating diverse gestures from audio. IEEE Transactions on Visualization and Computer Graphics, 30(8):47524766, 2024. 6 [21] Haiyang Liu, Naoya Iwamoto, Zihao Zhu, Zhengqing Li, You Zhou, Elif Bozkurt, and Bo Zheng. Disco: Disentangled implicit content and rhythm learning for diverse co-speech gestures synthesis. In Proceedings of the 30th ACM International Conference on Multimedia, page 37643773, New York, NY, USA, 2022. Association for Computing Machinery. 2 [22] Haiyang Liu, Zihao Zhu, Naoya Iwamoto, Yichen Peng, Zhengqing Li, You Zhou, Elif Bozkurt, and Bo Zheng. Beat: 9 large-scale semantic and emotional multi-modal dataset In Computer Vision for conversational gestures synthesis. ECCV 2022: 17th European Conference, Tel Aviv, Israel, October 2327, 2022, Proceedings, Part VII, page 612630, Berlin, Heidelberg, 2022. Springer-Verlag. [23] Haiyang Liu, Zihao Zhu, Giorgio Becherini, Yichen Peng, Mingyang Su, You Zhou, Xuefei Zhe, Naoya Iwamoto, Bo Zheng, and Michael J. Black. Emage: Towards unified holistic co-speech gesture generation via expressive masked audio gesture modeling. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pages 11441154, 2024. 2 [24] Xian Liu, Qianyi Wu, Hang Zhou, Yinghao Xu, Rui Qian, Xinyi Lin, Xiaowei Zhou, Wayne Wu, Bo Dai, and Bolei Zhou. Learning hierarchical cross-modal association for cospeech gesture generation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 1046210472, 2022. 2 [25] Cheng Luo, Siyang Song, Weicheng Xie, Micol Spitale, Zongyuan Ge, Linlin Shen, and Hatice Gunes. Reactface: Online multiple appropriate facial reaction generation in dyadic interactions. IEEE Transactions on Visualization and Computer Graphics, 31(9):61906207, 2025. 3 [26] Cheng Luo, Siyang Song, Siyuan Yan, Zhen Yu, and Zongyuan Ge. Reactdiff: Fundamental multiple appropriIn Proceedings of the ate facial reaction diffusion model. 33rd ACM International Conference on Multimedia, page 56075616, New York, NY, USA, 2025. Association for Computing Machinery. 2 [27] Cheng Luo, Jianghui Wang, Bing Li, Siyang Song, and Bernard Ghanem. Omniresponse: Online multimodal conversational response generation in dyadic interactions. arXiv preprint arXiv:2505.21724, 2025. 2, 3 [28] Antje Meyer. Timing in conversation. Journal of Cognition, 6(1):20, 2023. 2 [29] Muhammad Hamza Mughal, Rishabh Dabral, Ikhsanul Habibie, Lucia Donatelli, Marc Habermann, and Christian Theobalt. ConvoFusion: Multi-Modal Conversational Diffusion for Co-Speech Gesture Synthesis . In 2024 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pages 13881398, Los Alamitos, CA, USA, 2024. IEEE Computer Society. 2, 3, 6, 7, 8 [30] Julian Neri and Sebastian Braun. Towards real-time singlechannel speech separation in noisy and reverberant environments, 2023. 2 [31] Evonne Ng, Hanbyul Joo, Liwen Hu, Hao Li, Trevor Darrell, Angjoo Kanazawa, and Shiry Ginosar. Learning to listen: Modeling non-deterministic dyadic facial motion. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pages 2039520405, 2022. 3 [32] Evonne Ng, Javier Romero, Timur Bagautdinov, Shaojie Bai, Trevor Darrell, Angjoo Kanazawa, and Alexander Richard. From audio to photoreal embodiment: Synthesizing humans in conversations. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pages 10011010, 2024. 2, 3, [33] OpenAI, Josh Achiam, and Steven Adler et al. Gpt-4 technical report, 2024. 2 [34] Kunkun Pang, Dafei Qin, Yingruo Fan, Julian Habekost, Takaaki Shiratori, Junichi Yamagishi, and Taku Komura. Bodyformer: Semantics-guided 3d body gesture synthesis with transformer. ACM Trans. Graph., 42(4), 2023. 2 [35] Ethan Perez, Florian Strub, Harm de Vries, Vincent Dumoulin, and Aaron Courville. Film: visual reasoning with general conditioning layer. In Proceedings of the ThirtySecond AAAI Conference on Artificial Intelligence and Thirtieth Innovative Applications of Artificial Intelligence Conference and Eighth AAAI Symposium on Educational Advances in Artificial Intelligence. AAAI Press, 2018. 5, 1 [36] Xingqun Qi, Chen Liu, Lincheng Li, Jie Hou, Haoran Xin, and Xin Yu. Emotiongesture: Audio-driven diverse emotional co-speech 3d gesture generation. Trans. Multi., 26: 1042010430, 2024. 2 [37] Yoni Shafir, Guy Tevet, Roy Kapon, and Amit Haim Bermano. Human motion diffusion as generative prior. In The Twelfth International Conference on Learning Representations, 2024. 3 [38] Siyang Song, Micol Spitale, Xiangyu Kong, Hengde Zhu, Cheng Luo, Cristina Palmero, German Barquero, Sergio Escalera, Michel Valstar, Mohamed Daoudi, et al. React 2025: the third multiple appropriate facial reaction generation challenge. In Proceedings of the ACM International Conference on Multimedia, 2025. 3 [39] Hassan Taherian and DeLiang Wang. Multi-channel conversational speaker separation via neural diarization. IEEE/ACM Transactions on Audio, Speech, and Language Processing, 32:24672476, 2024. 2 [40] Guy Tevet, Brian Gordon, Amir Hertz, Amit Bermano, and Daniel Cohen-Or. Motionclip: Exposing human moIn Computer VisionECCV tion generation to clip space. 2022: 17th European Conference, Tel Aviv, Israel, October 2327, 2022, Proceedings, Part XXII, pages 358374. Springer, 2022. 3 [41] Guy Tevet, Sigal Raab, Brian Gordon, Yoni Shafir, Daniel Cohen-or, and Amit Haim Bermano. Human motion diffuIn The Eleventh International Conference on sion model. Learning Representations, 2023. 3 [42] Minh Tran, Di Chang, Maksim Siniukov, and Mohammad Soleymani. Dyadic interaction modeling for social behavior generation. arXiv preprint arXiv:2403.09069, 2024. 2 [43] Yaohui Wang, Di Yang, Francois Bremond, and Antitza Dantcheva. Lia: Latent image animator. IEEE Transactions on Pattern Analysis and Machine Intelligence, pages 116, 2024. 5 [44] Sicheng Yang, Zilin Wang, Zhiyong Wu, Minglei Li, Zhensong Zhang, Qiaochu Huang, Lei Hao, Songcen Xu, Xiaofei Wu, Changpeng Yang, and Zonghong Dai. Unifiedgesture: unified gesture synthesis model for multiple skeletons. In Proceedings of the 31st ACM International Conference on Multimedia, page 10331044, New York, NY, USA, 2023. Association for Computing Machinery. 3 [45] Sicheng Yang, Haiwei Xue, Zhensong Zhang, Minglei Li, Zhiyong Wu, Xiaofei Wu, Songcen Xu, and Zonghong Dai. 10 The diffusestylegesture+ entry to the genea challenge 2023. In Proceedings of the 25th International Conference on Multimodal Interaction, page 779785, New York, NY, USA, 2023. Association for Computing Machinery. 3 [46] Sicheng Yang, Zunnan Xu, Haiwei Xue, Yongkang Cheng, Shaoli Huang, Mingming Gong, and Zhiyong Wu. Freetalker: Controllable speech and text-driven gesture generation based on diffusion models for enhanced speaker naturalness. In ICASSP 2024 - 2024 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), 2024. 2 [47] Hongwei Yi, Hualin Liang, Yifei Liu, Qiong Cao, Yandong Wen, Timo Bolkart, Dacheng Tao, and Michael Black. Generating holistic 3d human motion from speech. In IEEE Conference on Computer Vision and Pattern Recognition (CVPR), pages 469480, 2023. 2 [48] Wanqi Yin, Zhongang Cai, Ruisi Wang, Ailing Zeng, Chen Wei, Qingping Sun, Haiyi Mei, Yanjun Wang, Hui En Pang, Mingyuan Zhang, Lei Zhang, Chen Change Loy, Atsushi Yamashita, Lei Yang, and Ziwei Liu. Smplest-x: Ultimate scaling for expressive human pose and shape estimation. arXiv preprint arXiv:2501.09782, 2025. 2 [49] Youngwoo Yoon, Bok Cha, Joo-Haeng Lee, Minsu Jang, Jaeyeon Lee, Jaehong Kim, and Geehyuk Lee. Speech gesture generation from the trimodal context of text, audio, and speaker identity. ACM Trans. Graph., 39(6), 2020. 2 [50] Jiangning Zhang, Liu Liang, Zhucun Xue, and Yong Liu. Apb2face: Audio-guided face reenactment with auxiliary pose and blink signals. pages 44024406, 2020. [51] Mingyuan Zhang, Huirong Li, Zhongang Cai, Jiawei Ren, Lei Yang, and Ziwei Liu. Finemogen: Fine-grained spatiotemporal motion generation and editing. NeurIPS, 2023. 3 [52] Mingyuan Zhang, Zhongang Cai, Liang Pan, Fangzhou Hong, Xinying Guo, Lei Yang, and Ziwei Liu. Motiondiffuse: Text-driven human motion generation with diffusion model. IEEE Trans. Pattern Anal. Mach. Intell., 46(6): 41154128, 2024. 3 [53] Yihao Zhi, Xiaodong Cun, Xuelin Chen, Xi Shen, Wen Guo, Shaoli Huang, and Shenghua Gao. Livelyspeaker: Towards In Proceedsemantic-aware co-speech gesture generation. ings of the IEEE/CVF International Conference on Computer Vision (ICCV), pages 2080720817, 2023. 3 [54] Yi Zhou, Connelly Barnes, Jingwan Lu, Jimei Yang, and Hao Li. On the continuity of rotation representations in neural networks. CoRR, abs/1812.07035, 2018. 4, 1 [55] Lingting Zhu, Xian Liu, Xuanyu Liu, Rui Qian, Ziwei Liu, and Lequan Yu. Taming diffusion models for audioIn Proceedings of driven co-speech gesture generation. the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 1054410553, 2023. 3 DyaDiT: Multi-Modal Diffusion Transformer for Socially Favorable Dyadic Gesture Generation"
        },
        {
            "title": "Supplementary Material",
            "content": "This supplementary material contains sections below: 1. Relationships & Personality Clustering of Generated"
        },
        {
            "title": "Results",
            "content": "2. Implementation Details of DyaDiT 3. Details of A/B Test Questionnaire In addition to this supplementary.pdf, we also include narration video.mp4, which provides brief overview of the paper along with several qualitative gesture generation examples. We further provide our implementation in dyadit code.zip; please refer to the included README.md for instructions on running the code. The trained models will be released upon acceptance. 8. Clustering of Generated Gestures In the main paper, we conduct an A/B test to evaluate the relationship and personality consistency of the generated gestures. To further assess the controllability of the conditional inputs in DyaDiT, we perform t-SNE clustering analysis on the generated motion embeddings. Figure 8 visualizes the t-SNE embeddings of the generated gestures under different conditioning signals. On the left, we generate gestures using various relationship types while fixing the personality scores. On the right, we discretize the continuous personality score features into five one-hot vectors and generate gestures for each vector to examine personality controllability. We observe that the personality clusters form clearly separable groups, indicating that DyaDiT effectively captures the global behavioral tendencies associated with different personality traits. In contrast, the relationship clusters are less clearly separated. We consider this to be consistent with the nature of dyadic conversational gestures: the styles between Friend, Family, and Dating share some overlap. As result, the generated gestures also show more continuous manifold across these categories rather than sharp cluster boundaries. 9. Implementation Details Diffusion Transformer The input pose sequence is encoded into latent space aligned with the VQ-VAE representation, resulting in latent embedding in R64, instead of the original R436 6D rotation matrix [54] joint representation. linear layer projects the noisy latent input from R64 to hidden space in R512, followed by symmetric projection back to R64 at the output. The model contains 4 Transformer blocks, each Figure 8. t-SNE clustering results of Relationships (left), Personality Scores (right). equipped with 4-head multi-head attention (R128) and R2048 feed-forward network. We employ R512 sinusoidal time embedding, which is injected into each block through FiLM [35] modulation. Two independent Wav2Vec2 [4] processors extract highlevel audio features from the conversational speech of both speakers. These yield feature sequences denoted as aself, aother RT 768, where denotes the number of audio frames. Each of these is projected to RT 512 via linear transformation, followed by LayerNorm and gated fusion mechanism to combine self and other speaker cues. learnable motion bank contains 1000 prototype vectors in R512, providing contextual priors via cross-attention. similar to the time embedding, relationship and personality embeddings are projected into R512. These vectors are injected into the DiT blocks via FiLM-style adaptive scaling. All contextual cues are concatenated into unified sequence in R512 and injected into each DiT block via cross-attention. Motion Tokenizer (VQ-VAE). We implement temporal VQ-VAE [18] to discretize pose sequences before diffusion. Given an input sequence of joint features RT 643, the encoder is 1D CNN consisting of three Conv1d layers, with LeakyReLU applied after the first two layers, and an overall temporal downsampling factor of 4, producing latent sequence in R(T /4)64. This continuous latent is quantized by residual vector quantizer with depth 4, each equipped with 512-entry codebook, which maps each time step to stack of discrete code indices. The decoder is 1D CNN consisting of an initial Conv1d-LeakyReLU layer, two upsampling blocks (linear interpolation followed by Conv1d and LeakyReLU) interleaved with additional 1 in total, including 10 questions on overall gesture quality, 8 questions on relationship consistency, and 10 questions on personality consistency. Each question presents paired gesture videos for comparison under two settings: DyaDiT vs. ConvoFusion and DyaDiT vs. Ground Truth. For an accurate viewing experience, please wear headphones. The left audio channel corresponds to the partners speech, while the right audio channel corresponds to the target speakers speech. The reconstructed interface allows reviewers to browse all questions and play the corresponding videos to experience the same evaluation procedure as our participants. Figure 9. Example of Questionnaires in GoogleForm Conv1d-LeakyReLU refinement layers, and final Conv1d projection, achieving an overall temporal upsampling factor of 4 and recovering the original temporal resolution to reconstruct poses in RT 643. The final latent representation used by the DiT denoiser is obtained from the quantized codes as compact 64-dimensional embedding per 4 frames in R64. Seamless Interactive Dataset. We conduct our experiments on part of the Seamless Interaction dataset [1]. In particular, we adopt the naturalistic split of the dataset. For training, we utilize the first 10 official training archives (provided as zip files), which contain approximately 182 hours of naturalistic interactions and 3000 paired motionaudio samples. For testing, we select the first archive from the official test split to ensure consistent evaluation setting. We observe that the SMPL-H parameters provided in the dataset exhibit noticeable inaccuracies in lower body estimation, likely due to limited camera views and body occlusions during data capture. To avoid introducing artifacts into our motion modeling, we discard lower-body joints and only retain the upper body comprising 43 joints, including fingers. For visualization, all unused joints, along with global orientation and root translation, are set to zero. In addition to pose data, the dataset includes high level annotations such as relationship and personality scores. While the dataset provides Interpersonal Communicative Dynamic (IPC) tags for social dynamics, we found the annotations to be too noisy and ambiguous where it is unclear which speaker they apply to. Consequently, we do not employ IPC tag supervision in our current study and instead focus on the cleaner relationship and personality cues. We note that once the IPC annotations are refined in future dataset releases, we plan to extend our framework with an IPC-awared conditioning module to further capture communicative intent in dyadic gestures. In the future, we plan to re-annotate the video data using advanced human pose estimation tools such as SMPLestx [48], Harmony4D [17] or recent state-of-the-art models, with the aim of obtaining more reliable full body motion supervision. 10. Questionnaire We provide reconstructed version of the A/B test questionnaire used in our user study. To view the questionnaire, please first extract the questionnaire video.zip file inside the questionnaire folder. After extraction, open questionnaire.html in any modern web browser. The original questionnaire was conducted through Google Forms (see Figure 9). It consists of 28 2 questions"
        }
    ],
    "affiliations": [
        "Carnegie Mellon University",
        "Institute of Science Tokyo",
        "Shanda AI Research Tokyo",
        "The University of Tokyo",
        "UNIST"
    ]
}