{
    "paper_title": "SOLAMI: Social Vision-Language-Action Modeling for Immersive Interaction with 3D Autonomous Characters",
    "authors": [
        "Jianping Jiang",
        "Weiye Xiao",
        "Zhengyu Lin",
        "Huaizhong Zhang",
        "Tianxiang Ren",
        "Yang Gao",
        "Zhiqian Lin",
        "Zhongang Cai",
        "Lei Yang",
        "Ziwei Liu"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Human beings are social animals. How to equip 3D autonomous characters with similar social intelligence that can perceive, understand and interact with humans remains an open yet foundamental problem. In this paper, we introduce SOLAMI, the first end-to-end Social vision-Language-Action (VLA) Modeling framework for Immersive interaction with 3D autonomous characters. Specifically, SOLAMI builds 3D autonomous characters from three aspects: (1) Social VLA Architecture: We propose a unified social VLA framework to generate multimodal response (speech and motion) based on the user's multimodal input to drive the character for social interaction. (2) Interactive Multimodal Data: We present SynMSI, a synthetic multimodal social interaction dataset generated by an automatic pipeline using only existing motion datasets to address the issue of data scarcity. (3) Immersive VR Interface: We develop a VR interface that enables users to immersively interact with these characters driven by various architectures. Extensive quantitative experiments and user studies demonstrate that our framework leads to more precise and natural character responses (in both speech and motion) that align with user expectations with lower latency."
        },
        {
            "title": "Start",
            "content": "4 2 0 2 9 2 ] . [ 1 4 7 1 0 0 . 2 1 4 2 : r SOLAMI: Social Vision-Language-Action Modeling for Immersive Interaction with 3D Autonomous Characters Jianping Jiang1, Weiye Xiao1, Zhengyu Lin1, Huaizhong Zhang1, Tianxiang Ren1, Yang Gao1, Zhiqian Lin1, Zhongang Cai1,2, Lei Yang1, Ziwei Liu2 1SenseTime Research, 2S-Lab, Nanyang Technological University Equal Contribution, Project Lead, Engineering Lead, Corresponding Author https://solami-ai.github.io/ Figure 1. SOLAMI enables the user to interact with 3D autonomous characters through speech and body language in an immersive VR environment via an end-to-end social vision-language-action model, which is trained on our synthesized multimodal dataset SynMSI."
        },
        {
            "title": "Abstract",
            "content": "Human beings are social animals. How to equip 3D autonomous characters with similar social intelligence that can perceive, understand and interact with humans remains an open yet foundamental problem. In this paper, we introduce SOLAMI, the first end-to-end Social vision-Language-Action (VLA) Modeling framework for Immersive interaction with 3D autonomous characters. Specifically, SOLAMI builds 3D autonomous characters from three aspects: 1) Social VLA Architecture: We propose unified social VLA framework to generate multimodal response (speech and motion) based on the users multimodal input to drive the character for social interaction. 2) Interactive Multimodal Data: We present SynMSI, synthetic multimodal social interaction dataset generated by an automatic pipeline using only existing motion datasets to address the issue of data scarcity. 3) Immersive VR Interface: We develop VR interface that enables users to immersively interact with these characters driven by various architectures. Extensive quantitative experiments and user study demonstrate that our framework leads to more precise and natural character responses (in both speech and motion) that align with user expectations with lower latency. 1 1. Introduction Have you ever imagined having an immersive face-to-face conversation with character you deeply admire? Not merely through speech dialogue, but through an interaction where you can observe its subtle facial expressions, natural body language, and even fleeting emotional changes. Psychology research [23, 34, 62, 66] shows that in social interactions, the greater the level of immersion, the better the human experience. However, current character agents [2, 4, 64] are still limited to text or voice interactions. This limitation prompts us to build 3D autonomous characters with richer modalities. Developing an autonomous 3D character requires effectively modeling its behavior system, which involves two major challenges: 1) The 3D character needs to accurately observe and understand the information conveyed by the user, and respond appropriately based on the context and its character setting through speech, body motion, and facial expression, etc. This goes beyond previous singular human-related tasks, such as motion generation [83], motion understanding [37], and audio-to-motion [8]. 2) Data for multimodal interactions between users and 3D characters is extremely scarce due to the prohibitive cost of the comprehensive setup. Previous work [17] is primarily based on the LLM-Agent framework, using text to link various sub-modules (such as motion captioning and text-to-motion). While this approach performs well in high-level tasks like planning and memory, it tends to fall short in tasks such as understanding user behaviors and providing timely body motion responses. This limitation arises because using text as the intermediary between modules conveys high-level information but often omits subtle nuances. And the sub-modules (motion captioning, speech recognition, etc.) in the complex engineering framework incur substantial latency that undermines the timeliness of natural communication [67]. Interestingly, research on robotics shows the similar conclusion. The LLM-Agent framework can handle planning tasks [35], but for low-level manipulation tasks, end-to-end Vision-Language-Action (VLA) models built upon LLMs show superior performance [14, 33, 39, 90]. We argue that digital avatars are essentially robots with virtual humanoid embodiment. Therefore, building VLA model for social interactions with users is promising direction. In this paper, we implement an end-to-end social VLA model, SOLAMI. Our model, built upon decoder-only LLM backbone, processes the inputs of user speech and motion into discrete representations, and generate the responsive speech and motion tokens, which are then decoded to the characters speech and motion. This modeling approach can effectively learn character behavior patterns across motion and speech modalities and offer low latency."
        },
        {
            "title": "Although there are numerous datasets related to human",
            "content": "social behaviors [17, 44, 51, 76], comprehensive multimodal interaction datasets remain scarce. Thus, we introduce data synthesis method that utilizes existing textmotion datasets to automatically construct multimodal interaction data at low cost. Leveraging our extensively curated topics (5.3 K), uniformly processed motion database (46 K), and iterative script refinement pipeline, we develop SynMSI, dataset containing 6.3 multi-turn multimodal conversation items. To evaluate the effectiveness of our method, we developed VR interface where users can immersively interact with various 3D characters. Quantitative experimental results and user study analysis show that our approach produces more precise and natural social interaction experience with lower latency. To summarize, we contribute 1) new VLA architecture to model the characters behavior system for immersive social interaction; 2) dedicated synthesizing pipeline that automatically generates large-scale multimodal interactive dataset, SynMSI; 3) An immerse VR interface for users to interact with various characters through speech and motion. 2. Related Work 2.1. Human Motion & LLM Existing role-play agents primarily rely on text [64], speech [2, 4, 52], or video [9] as interactive media, but building 3D autonomous characters means modeling 3D embodied behavior, especially body language. Unlike single-purpose human motion tasks [8, 16, 31, 44, 65, 76, 83], we expect this 3D character not only to comprehend user speech and body language but also to respond according to its profile setting. Large language models (LLMs) [52, 69], with their remarkable emergent abilities [72], provide promising solutions for this direction. One approach [26, 38, 86, 88] tries to integrate LLMs and human motion in an end-to-end fashion, enabling single model to perform multiple motion-related tasks. However, the goal of these methods is not to generate responsive motion based on the input motion according to the character setting. Another approach [17, 45] utilizes the LLM as versatile brain center that controls various submodules (e.g., motion understanding, text-to-motion generation) with text or code instructions. However, this modular approach inherently introduces information loss and engineering-related time latency. Therefore, how to create feasible autonomous 3D characters for immersive interaction remains an open challenge. 2.2. Embodied Intelligence Building 3D autonomous characters means creating virtual humanoid agents with embodied intelligence. In the field of embodied intelligence, researchers have found the high2 level abilities (planning, memory, etc.) of LLM-Agents across various environments, including factories [24, 35], 2D sandbox settings [54], and 3D gaming spaces [71]. For tasks requiring low-level skills, such as manipulation, endto-end VLA models [14, 33, 39, 90] have shown considerable potential. Despite existing efforts [27, 28, 31] in egocentric human-related tasks, the capabilities of VLA models in social interactive tasks with humans have not been fully explored. 3. Social Vision-Language-Action Modeling 3.1. Architecture Our framework is an end-to-end social VLA model that takes the users speech and motion as input and generates the characters responsive speech and motion as output. In this process, speech and motion are added as new languages to the LLM text vocabulary. First, the users speech and motion are converted into discrete motion tokens and speech tokens via motion tokenizer and speech tokenizer, respectively. decoder-only LLM backbone then predicts the characters output motion and speech tokens based on the users input tokens and the character setting. The generated tokens are subsequently decoded into corresponding motion and speech by their respective decoders. Motion Representation. To take advantage of SMPLXs [56] compatibility with industry animation workflow, we directly model human poses as SMPL-X joint rotations instead of keypoint positions [38, 49] to facilitate animation of characters down the stream. Motion Tokenizer. Our motion tokenizer employs Vector Quantized Variational Autoencoders (VQ-VAE) structure as [38, 86]. It learns discrete representations of motion, enabling the LLM to understand the text-motion connection. We design separate VQVAEs (Qb, Qh, Qt) for the body motion mb, hand motion mh, and inter-character relative transform (rotation and translation) mt for higher reconstruction accuracy [49]. The quantization process for motion mu is formulated as ˆmu = Qu(mu ) = arg min ziZu mu zi2, (1) where Zu is the codebook of motion part {b, h, t}, and ˆmu is the corresponding motion tokens. The VQt VAEs of body and hand, Qb and Qh, apply 1D convolutions on motion features along the temporal dimension to get LM sequential tokens ˆM = [ ˆmb 1, ˆmb ] and ˆM = [ ˆmh ], and the VQVAE Qt uses MLPs 2 , ..., ˆmh to get the transform token ˆmt of sequence. Speech Tokenizer. Research [12, 82] on speech discretizaIn this tion mainly utilizes the RVQ-VAE structure [80]. work, we utilize the SpeechTokenizer [84] that disentangles semantic and acoustic information within speech S. This 2, ..., ˆmb 1 , ˆmh LM LM 1, ˆss allows us to use the semantic tokens ˆS = [ˆss 2, ..., ˆss ] LS from the first layer as input to the LLM, reducing inference costs (LS is the sequence length of semantic tokens). Simultaneously, short sample of the characters voice (4 to 6 seconds) can be used as prompt to achieve instance voice cloning when decoding through SoundStorm [13]. Multi-modal Multi-round Interaction. User-character interaction is formulated as multi-round conversation fashion of common LLMs [52, 69]. When the user sends speech and motion to SOLAMI, the model auto-regressively generates speech and motion responses based on previous dialogue contents and the character setting. To facilitate training, we use special tokens to mark the start and end of each modality sequence as [38, 40]. The interaction process can be formulated as follow:"
        },
        {
            "title": "Interaction Template",
            "content": "Input: System Prompt: <Character Placeholder> User: <M Placeholder><S Placeholder> Character: <M Placeholder><S Placeholder> ... User: <M Placeholder><S Placeholder> Output: Character: <M Placeholder><S Placeholder> where <Character Placeholder> is placeholder for characters text description, <M Placeholder> for motion token sequences, and <S Placeholder> for speech token sequences. 3.2. Training The training of SOLAMI adopts three-stage strategy. Stage 1: Tokenizer Training. The training approach for the motion tokenizer uses the fashion of [38]. The train loss is Lm = λrLr + λeLe + λcLc + λvLv, (2) where Lr means reconstruction loss, Le embedding loss, Lc commitment loss, Lv velocity loss, and λ are manually adjusted weights. For the speech tokenizer, we use the pre-trained checkpoint from AnyGPT [81]. We freeze the tokenizers weights after this stage. Stage 2: Multi-task Pre-training for Modality Alignment. As shown in Fig. 2, the second stage is multi-task pre-training, achieving modality alignment between moIt tion and text, as well as between speech and text. is necessary because motion data is scarce, and direct training on multimodal interaction data results in suboptimal models, as demonstrated in subsequent ablation studies. For motion and text alignment, we use 46 motion-text pairs for text-to-motion generation and motion captioning tasks, and 11 interactive motion pairs 3 Figure 2. Training pipeline of SOLAMI. We train SOLAMI through three-stage process. In the pre-training stage, we train the model with motion-text and speech-text related tasks to align the speech and motion modalities with language. During the instruction tuning stage, we train the model with social multimodal multi-round interaction data, enabling it to generate multimodal responses that align with the character settings and the context of the topic. for two-person motion generation. To align the speech and text, we train the model with 410 speech-text pairs for text-to-speech and automatic speech recognition tasks, and 100 speech dialogue pairs for speech-tospeech generation. The tasks are formulated as User: <Task Placeholder><Input Modality Placeholder>; Character:<Output Modality Placeholder> . To balance the scale disparity between the motion and speech data, we sampled them at 4:6 ratio during training. Stage 3: Instruction Tuning for Multi-turn Conversation. In the third stage, we perform instruction tuning by applying supervised fine-tuning with multimodal interaction data, enabling the model to handle long-sequence, multi-turn dialogues, as shown in Fig. 2. We utilize 5.7 multimodal conversation items for supervised fine-tuning. Each conversation item is fed to the model in the format as the Interaction Template in Sec. 3.1. We experiment with two approaches: full-parameter fine-tuning and LoRA finetuning [32]. We supervised only the characters response to teach the model how to react to the users behavior. Thus we train the model by maximizing the log-likelihood of the next-token prediction and the train loss is: (cid:88) Lr M(cid:88) r=1 (cid:88) i=1 Lr S(cid:88) LIT = logpΘ( ˆmr ˆmr i1, ..., ˆmr 1, ˆS<r, ˆM<r) logpΘ(ˆsr ˆsr i1, ..., ˆsr 1, ˆS<r, ˆMr), r=1 i=1 (3) where Θ is the network parameter of the LLM backbone or LoRA weights, is the conversation round, ˆSr and ˆMr are the r-th round speech and motion token sequences with lengths Lr S, respectively. and Lr 4. SynMSI Dataset Social interaction between users and virtual characters is inherently unique, which makes collecting such multimodal interaction data particularly challenging. Currently, available public datasets [29, 51, 76] are incomplete for our task. To address this issue, we propose data synthesis pipeline that leverages existing motion-text datasets, text-based roleplay models, and speech synthesis methods and generates large-scale multimodal dialogue dataset, SynMSI. 4.1. Motion Data We collect motion-text data for two purposes: first, to achieve alignment between motion and text during pretraining, and second, to generate multimodal data for instruction tuning. Since our work focuses on modeling social interactions, we select existing datasets that contain rich social actions: HumanML3D [29] with 24 motiontext pairs, Inter-X [76] with 20 motion-text pairs and 10 two-person motion pairs, and DLP-MoCap [17] with 2 motion-text pairs. Since the Inter-X [76] dataset contains only text descriptions of two-person interactive motion without descriptions of individual motion, we used GPT4o [52] to decompose the two-person action descriptions into single-person motion-text pairs. Additionally, we used GPT-4o [52] to synthesize comprehensive text descriptions Figure 3. SynMSI dataset generation. Our synthesizing pipeline consists of 4 steps. Based on numerous character-relevant topics and state-of-the-art LLMs [52], we generate text scripts for multimodal dialogues. Using large-scale motion database [17, 29, 76], we retrieve the most appropriate motions and refine the speech scripts accordingly. Finally, we employ TTS/voice cloning [19] to generate characterspecific speech. This approach enables us to create multimodal interaction data of various characters using only existing motion datasets. for each motion clip by consolidating multiple possible descriptions, thereby providing more detailed textual annotations that preserve motion details. 4.2. Speech Data We use speech-text data for speech-text alignment in the Speech datasets involve Commonpre-training stage. Voice [10] (150 speech-text pairs in our experiments), AnyInstruct [81] (200 speech-text pairs and 100 speech-to-speech items), and synthetic speech data (60 speech-text pairs) by text-to-speech approaches (Azure TTS and XTTS v2 [19]). 4.3. Multimodal Data Synthesizing Previous data synthesis methods [42, 48, 89] usually use the general abilities of advanced LLMs [52] and text-annotated multimodal data to generate synthetic data. However, generating social multimodal interaction data has not yet been achieved. This challenging task requires high-quality expression of body language, voice consistency that matches the characters, and suitable dialogue content. As shown in Fig. 3, our synthesizing pipeline includes 4 steps. (1) First, we collect 5.3 character-related and daily topics from internet platforms (Google Trends [3], Zhihu [6], Jike) and brainstorms of GPT-4o [52] to improve the diversity of the dialogue contents. (2) Based on the topic, character setting, and previous round of scripts, we use GPT-4o [52] to generate textual descriptions (motion, speech, expression etc.) for the next round of the dialogue. (3) Then we utilize the text embedding [50] of the motion description to retrieve the most relevant motions from our meticulously curated motion-text database. (4) Moreover, we refine the generated speech text with the retrieved motions to ensure that speech and motion are well-coordinated. The motion database with detailed text annotations and the refinement process can alleviate the misalignment between the real motion and speech in the LLM-Agent method [17]. By iteratively repeating this process, we can generate multiround dialogue contents across many characters, where the motions are sourced from the motion database, and the speeches are synthesized using TTS/voice cloning (Azure TTS and XTTS v2 [19]) to maintain consistency with the characters voice style. We finally obtained 6.3 multimodal dialogue items. 5. VR Interface To demonstrate our method directly, we developed VR interface with an Oculus Quest 3 frontend and backend service, as shown in Fig. 4. The frontend enables immersive interaction between users and 3D autonomous characters, while the backend, powered by 2 H800 GPUs, supports the computation of various baselines in our experiments. During usage, the VR headset captures the users speech and body motion, and sends them to the backend computation nodes. For motion capture, we use the Quests full-body tracking system [73] to obtain pose parameters, which are then retargeted onto an SMPL-X model [56]. The computation nodes generate the body motion parameters and speech responses of the character based on the multimodal input. 5 social interaction brought by incorporating 3D modalities (such as body motion), we compared SOLAMI with the LLM+Speech and the AnyGPT (fine-tune) approach. For the LLM+Speech framework, the users speech is first transcribed into text using ASR techniques, which is then fed to LLM to generate the characters response in text, and subsequently converted into speech using TTS. For fairness, we use LLaMA2-7B-Chat [69] as the LLM backbone, Whisper large-v3 [60] for ASR, and XTTS v2 [19] for voice cloning. For the AnyGPT (fine-tune) framework, we use the speech data of SynMSI to train the AnyGPT-base model [81] with the same parameter settings. To compare the effectiveness of the LLM-Agent architecture with the social VLA frameIn DLP work, we used DLP [17] as baseline method. framework, the users speech and body motion are separately processed as text descriptions by ASR and motion captioning modules. Based on the input text descriptions, LLM generates the characters text instructions of speech and motion, which are transferred into speech and body motion by TTS and motion generation module. The speech component of DLP follows the LLM+Speech method. Considering that MoMat-MoGen module in DLP is too slow for user interaction (over 5 seconds latency), we use MotionGPT [38] for motion captioning and motion generation. To ensure fair comparison, we used the same motion data as the pre-training stage of SOLAMI to train MotionGPT. Additionally, we conducted ablation experiments on the effect of the pre-training stage, marked as (w/o pretrain). For the ablation study of the motion tokenizer, please refer to the supplementary materials. We use vLLM [41] to accelerate the LLM backbones for low latency interaction. 6.2. Quantitative Evaluation We conducted quantitative evaluation for our method and all the baselines mentioned in Sec. 6.1. Evaluation Metrics. For motion, we evaluate the model responses using metrics including FID, diversity, PAMPJPE (mm), and angle error [31]. Following Duolando [65], we obtain FID and diversity using motion features from AIST++ [43]. For speech, we use VC similarity [77] to evaluate the voice similarity with the character. To evaluate the content quality of speech, we first use Whisperlarge-v3 [60] to transcribe the speech into text. Then following [64, 70], we employ GPT-4o [52] as the judge to assess Context Relevance and Character Consistency on Likert scale ranging from 1 to 5. Context Relevance indicates whether the speech content aligns with the topic and context of the conversation, while Character Consistency assesses whether the content adheres to the character settings. For inference latency (seconds), we deploy all the models on 2 H800 GPUs with vLLM [41] framework and asynchronous mechanisms to improve performance while maintaining fairness. Figure 4. VR interface architecture. Our VR project consists of Quest 3 client and server. The Quest client captures and transmits user body motion and speech to the server. The server then generates characters speech, body motion, and face blendshape parameters based on the selected methods. The response is then sent back to the Quest client to drive the character. Then we apply an audio-to-face method, UniTalker [25], to generate the characters facial animation parameters. The facial and body parameters are jointly retargeted onto 3D character model [78], completing one cycle of social interaction. For natural user experience, we employ preset idle motions on the character when the method is LLM+Speech or the character is waiting for the users input. 3D Character Assets. Our 3D character portfolio including AI assiscovers diverse range of entities, internet memes, tant avatars, and real-world celebrity personas. These 3D models are sourced from open-source repositories under CC Attribution-NonCommercial-ShareAlike License as well as our manual creation using VRoid Studio [5]. We subsequently employ facial rigging, skinning, bone chain simulation, retargeting, and texture and material creation in Unity Engine processes to yield functional 3D character assets. famous cinematic roles, 6. Experiments 6.1. Experimental Settings experiment, we In our selected the AnyGPT-base model [81] (based on LLaMA2-7B [69]) as the backbone for SOLAMI, because it is an open-source model available at our experimental time that supports end-to-end speech processing. During the pre-training stage, we utilize 32 V100 GPUs to train the model for 3 steps (batch size 256, learning rate 4e-5). For instruction tuning, we train the SOLAMI for 800 steps using 16 V100 GPUs (batch size 48, learning rate 2e-5). For LoRA fine-tuning [32], we set the rank as 8 and alpha as 16. We split the synthesized multimodal data into training and test sets with 9:1 ratio. We use DeepSpeed [61] to accelerate the training. During testing, we evaluate each round of the characters response. Baselines. To validate the performance improvement in 6 Table 1. Quantitative results of baselines and SOLAMI. () indicates that the values are better if the metrics are larger (smaller). We run all the evaluations 5 times and report the average metric. The best results are in bold and the second best results are underlined. Methods SynMSI Dataset LLM+Speech (Llama2) [69] AnyGPT (fine-tune) [81] DLP (MotionGPT) [17] SOLAMI (w/o pretrain) SOLAMI (LoRA) SOLAMI (full params) Motion Metrics Speech Metrics FID - - - 4.254 5.052 15.729 3.443 Diversity 9.136 - - 8.259 8.558 8.145 8.853 PA-MPJPE Angle Error VC Similarity Context Relevance Character Consistency - - - 165.053 159.709 167.149 151.500 - - - 0.495 0.387 0.400 0.360 - 0.818 0.819 0.812 0.820 0.770 0.824 4.888 3.527 3.502 3.577 3.541 3.251 3.634 4.893 3.859 3.803 3.785 3.461 3.423 3.824 Inference Latency - 3.157 2.588 5.518 2.657 2.710 2.639 Quantitative Results. The quantitative results in Tab. 1 demonstrate that, using the same foundation model (Llama2) [69] as the backbone, our method, SOLAMI (full params), significantly outperforms other methods in terms of motion quality and inference latency. Motion Quality. SOLAMI demonstrates superior performance compared with the DLP method [17] across multiple motion metrics. This is because the social VLA model achieves comprehensive modality alignment among speech, motion, and language through training on our high-fidelity character-specific multimodal data. Our model can precisely perceive the users physical motions and linguistic cues, enabling semantically rich interactive motions in response. This ability contrasts with the LLM-Agent architectures of the DLP method, which exhibits limitations in conveying multimodal nuances through text-only intermediary representation. Speech Quality. Our method demonstrates the capability to synthesize voice tone that matches the character with higher Voice Cloning (VC) Similarity score. Our model also shows better performance on the context relevance of the dialogue than other methods. Because the inclusion of the motion modality enables the model to perceive the users body language, while the LLM+Speech or AnyGPT(fine-tune) method lacks this capability. In terms of character consistency, our model achieves secondary performance metrics. We suspect this may be due to the incorporation of motion and speech modalities, which potentially affects the character-related knowledge embedded within the original LLM. The performance degradation is similar to the observations in [24, 46]. Inference Latency. Our end-to-end approach is significantly superior to the modular pipeline approaches (LLM+Speech or DLP). Because the end-to-end VLA model naturally aligns with the process of real-time human communication. Theoretically, if we could collect data on real-time interactions between humans and characters, our method could achieve full-duplex streaming interaction. Ablation Study. As shown in Tab. 1, the pre-training stage of SOLAMI leads to better performance in both motion and speech quality. We believe that the pre-training stage, which aligns motion, speech, and language, facilitates the Table 2. Questionnaire settings of our user study. Dimension Motion Coherence Motion Interaction Speech Consistency Overall Experience Questions Does the motion match the characters setting? Does the motion align well with speech? Does the character follow motion instructions correctly? Does the character understand users motion? Does the speech match the characters setting? Is the speech relevant to the current topic? How would you rate the overall experience? Figure 5. Results of the user study with 95% confidence. models ability to learn the multimodal dialogue skill durInstruction tuning using ing the instruction tuning stage. LoRA [32] shows weaker results compared to full parameter fine-tuning. We think that the gap between the data distribution of pre-training tasks and the multimodal instruction tuning task is substantial, and LoRA fine-tuning alone is insufficient for the model to learn the strong multimodal dialog ability. 6.3. VR Interface User Study Quantitative evaluation of single modality alone cannot fully compare the 3D autonomous characters built on different frameworks. To address this, we conducted user study with VR interface. As shown in Fig. 4, we employ LLM+Speech, DLP (MotionGPT), and SOLAMI as method options of the server backend and the same VR frontend across different methods. This implies that the variable in the experiment is the driving method in the server. Users are asked to engage in more than five rounds of dialogue with the VR character before completing our questionnaire. 7 Figure 6. Qualitative results of SOLAMI and baselines, and the user workflow for VR experience. Our social VLA model, trained in an end-to-end strategy on SynMSI dataset, can accurately perceive the semantic information embedded within users speech and motion input, and subsequently generate natural and coherent responses. Evaluation Metrics. The indicators and corresponding questions for our questionnaire are shown in Tab. 2. Motion Coherence evaluates whether the characters motion aligns with the character setting and dialogue content. Motion Interaction assesses whether the character can understand the semantics of the body motion and effectively interact with the user. Speech Coherence examines whether the generated speech aligns with the context and character settings. And the Overall Experience measures the users satisfaction with the overall experience. Each question is rated on 1 to 5 Likert scale, with higher scores indicating greater satisfaction. The score for each dimension is calculated as the average score of its corresponding questions. We ultimately collected 60 survey responses, with participants from various gender and age groups. Results. As shown in the Fig. 5, our method achieved the highest user satisfaction across all dimensions. SOLAMI demonstrates superior performance over the DLP method across all metrics, validating the effectiveness of an endto-end social VLA model in character behavior modeling. While the DLP method shows lower speech consistency compared to the LLM+Speech method, it excels in motionrelated metrics and achieves higher overall satisfaction, indicating that effective body language can enhance user experience despite speech quality limitations. To provide more intuitive demonstration of our models capabilities, we replay the user study experiments and render them, as shown in the Fig. 6. The results indicate that SOLAMI demonstrates excellent capabilities in body language understanding, motion command execution, and body interaction. For better understanding, we also present the workflow from first-person view during actual usage. We encourage readers to explore our supplementary materials and video for more detailed overview of the model experiments, data generation pipeline, VR interface construction process, and comprehensive experimental results. 7. Conclusion In this paper, we propose SOLAMI, an approach for building 3D autonomous characters. This approach includes three key components: 1) Architecture: novel social VLA modeling framework enabling multimodal social interaction; 2) Multimodal Data Synthesizing: pipeline for automatically generating multimodal interaction data from existing incomplete datasets; 3) VR Interface: VR engineering framework that facilitates immersive interactions between users and various characters. Together, these modules contribute to an enhanced user interaction experience."
        },
        {
            "title": "References",
            "content": "[1] Elevenlabs. https://elevenlabs.io/. 17 [2] Character.ai. https://character.ai. 2 [3] Google trends. trends. 5, 16 https : / / trends . google . com / [4] Talkie ai. https://www.talkie-ai.com. 2 [5] Vroid studio. https://vroid.com/en/studio. 6 [6] Zhihu. https://www.zhihu.com. 5, 16 [7] 2noise. Chattts: generative speech model for daily dialogue. 2024. 17 [8] Simon Alexanderson, Rajmund Nagy, Jonas Beskow, and Gustav Eje Henter. Listen, denoise, action! audio-driven motion synthesis with diffusion models. ACM TOG, 2023. 2 [9] Tenglong Ao. Body of her: preliminary study on endto-end humanoid agent. arXiv preprint arXiv:2408.02879, 2024. 2, 13 [10] Rosana Ardila, Megan Branson, Kelly Davis, Michael Kohler, Josh Meyer, Michael Henretty, Reuben Morais, Lindsay Saunders, Francis M. Tyers, and Gregor Weber. Common voice: massively-multilingual speech corpus. In LREC, 2020. [11] Homanga Bharadhwaj, Debidatta Dwibedi, Abhinav Gupta, Shubham Tulsiani, Carl Doersch, Ted Xiao, Dhruv Shah, Fei Xia, Dorsa Sadigh, and Sean Kirmani. Gen2act: Human video generation in novel scenarios enables generalizable robot manipulation. arXiv preprint arXiv:2409.16283, 2024. 13 [12] Zalan Borsos, Raphael Marinier, Damien Vincent, Eugene Kharitonov, Olivier Pietquin, Matthew Sharifi, Dominik Roblek, Olivier Teboul, David Grangier, Marco Tagliasacchi, and Neil Zeghidour. Audiolm: language modeling approach to audio generation. TASLP, 2023. 3 [13] Zalan Borsos, Matt Sharifi, Damien Vincent, Eugene Kharitonov, Neil Zeghidour, and Marco Tagliasacchi. arXiv Soundstorm: Efficient parallel audio generation. preprint arXiv:2305.09636, 2023. 3, 17 [14] Anthony Brohan, Noah Brown, Justice Carbajal, Yevgen Chebotar, Joseph Dabis, Chelsea Finn, Keerthana Gopalakrishnan, Karol Hausman, Alexander Herzog, Jasmine Hsu, Julian Ibarz, Brian Ichter, Alex Irpan, Tomas Jackson, Sally Jesmonth, Nikhil J. Joshi, Ryan Julian, Dmitry Kalashnikov, Yuheng Kuang, Isabel Leal, Kuang-Huei Lee, Sergey Levine, Yao Lu, Utsav Malla, Deeksha Manjunath, Igor Mordatch, Ofir Nachum, Carolina Parada, Jodilyn Peralta, Emily Perez, Karl Pertsch, Jornell Quiambao, Kanishka Rao, Michael S. Ryoo, Grecia Salazar, Pannag R. Sanketi, Kevin Sayed, Jaspiar Singh, Sumedh Sontakke, Austin Stone, Clayton Tan, Huong T. Tran, Vincent Vanhoucke, Steve Vega, Quan Vuong, Fei Xia, Ted Xiao, Peng Xu, Sichun Xu, Tianhe Yu, and Brianna Zitkovich. RT-1: robotics transformer for real-world control at scale. In RSS, 2023. 2, 3 [15] Tom B. Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini Agarwal, Ariel Herbert-Voss, Gretchen Krueger, Tom Henighan, Rewon Child, Aditya Ramesh, Daniel M. Ziegler, Jeffrey Wu, Clemens Winter, Christopher Hesse, Mark Chen, Eric Sigler, Mateusz Litwin, Scott Gray, Benjamin Chess, Jack Clark, Christopher Berner, Sam McCandlish, Alec Radford, Ilya Sutskever, and Dario Amodei. Language models are few-shot learners. In NeurIPS, 2020. 13 [16] Zhongang Cai, Wanqi Yin, Ailing Zeng, Chen Wei, Qingping Sun, Wang Yanjun, Hui En Pang, Haiyi Mei, Mingyuan Zhang, Lei Zhang, Chen Change Loy, Lei Yang, and Ziwei Liu. Smpler-x: Scaling up expressive human pose and shape estimation. In NeurIPS, 2023. 2, 13, [17] Zhongang Cai, Jianping Jiang, Zhongfei Qing, Xinying Guo, Mingyuan Zhang, Zhengyu Lin, Haiyi Mei, Chen Wei, Ruisi Wang, Wanqi Yin, Liang Pan, Xiangyu Fan, Han Du, Peng Gao, Zhitao Yang, Yang Gao, Jiaqi Li, Tianxiang Ren, Yukun Wei, Xiaogang Wang, Chen Change Loy, Lei Yang, and Ziwei Liu. Digital life project: Autonomous 3d characters with social intelligence. In CVPR, 2024. 2, 4, 5, 6, 7, 13, 16 [18] CAMB.AI. Mars5: novel speech model for insane prosody. 2024. 17 [19] Edresson Casanova, Kelly Davis, Eren Golge, Gorkem Goknar, Iulian Gulea, Logan Hart, Aya Aljafari, Joshua Meyer, Reuben Morais, Samuel Olayemi, et al. Xtts: massively multilingual zero-shot text-to-speech model. arXiv preprint arXiv:2406.04904, 2024. 5, 6, 17 [20] Chi-Lam Cheang, Guangzeng Chen, Ya Jing, Tao Kong, Hang Li, Yifeng Li, Yuxiao Liu, Hongtao Wu, Jiafeng Xu, Yichu Yang, Hanbo Zhang, and Minzhao Zhu. Gr-2: generative video-language-action model with web-scale knowledge for robot manipulation, 2024. 15 [21] Xuxin Cheng, Jialong Li, Shiqi Yang, Ge Yang, and Xiaolong Wang. Open-television: Teleoperation with immersive active visual feedback. 2024. 13 [22] Konstantina Christakopoulou, Shibl Mourad, and Maja Mataric. Agents thinking fast and slow: talker-reasoner architecture. arXiv preprint arXiv:2410.08328, 2024. 13 [23] James Cummings and Jeremy Bailenson. How immersive is enough? meta-analysis of the effect of immersive technology on user presence. Media psychology, 2016. 2 [24] Danny Driess, Fei Xia, Mehdi S. M. Sajjadi, Corey Lynch, Aakanksha Chowdhery, Brian Ichter, Ayzaan Wahid, Jonathan Tompson, Quan Vuong, Tianhe Yu, Wenlong Huang, Yevgen Chebotar, Pierre Sermanet, Daniel Duckworth, Sergey Levine, Vincent Vanhoucke, Karol Hausman, Marc Toussaint, Klaus Greff, Andy Zeng, Igor Mordatch, and Pete Florence. Palm-e: An embodied multimodal language model. In ICML, 2023. 3, 7, [25] Xiangyu Fan, Jiaqi Li, Zhiqian Lin, Weiye Xiao, and Lei Yang. Unitalker: Scaling up audio-driven 3d facial animation through unified model. 2024. 6 [26] Yao Feng, Jing Lin, Sai Kumar Dwivedi, Yu Sun, Priyanka Patel, and Michael J. Black. Chatpose: Chatting about 3d human pose. In CVPR, 2024. 2 [27] Kristen Grauman, Andrew Westbury, Eugene Byrne, Zachary Chavis, Antonino Furnari, Rohit Girdhar, Jackson Hamburger, Hao Jiang, Miao Liu, Xingyu Liu, Miguel Martin, Tushar Nagarajan, Ilija Radosavovic, Santhosh Kumar Ramakrishnan, Fiona Ryan, Jayant Sharma, Michael Wray, Mengmeng Xu, Eric Zhongcong Xu, Chen Zhao, Siddhant 9 Bansal, Dhruv Batra, Vincent Cartillier, Sean Crane, Tien Do, Morrie Doulaty, Akshay Erapalli, Christoph Feichtenhofer, Adriano Fragomeni, Qichen Fu, Abrham Gebreselasie, Cristina Gonzalez, James Hillis, Xuhua Huang, Yifei Huang, Wenqi Jia, Weslie Khoo, Jachym Kolar, Satwik Kottur, Anurag Kumar, Federico Landini, Chao Li, Yanghao Li, Zhenqiang Li, Karttikeya Mangalam, Raghava Modhugu, Jonathan Munro, Tullie Murrell, Takumi Nishiyasu, Will Price, Paola Ruiz Puentes, Merey Ramazanova, Leda Sari, Kiran Somasundaram, Audrey Southerland, Yusuke Sugano, Ruijie Tao, Minh Vo, Yuchen Wang, Xindi Wu, Takuma Yagi, Ziwei Zhao, Yunyi Zhu, Pablo Arbelaez, David Crandall, Dima Damen, Giovanni Maria Farinella, Christian Fuegen, Bernard Ghanem, Vamsi Krishna Ithapu, C. V. Jawahar, Hanbyul Joo, Kris Kitani, Haizhou Li, Richard A. Newcombe, Aude Oliva, Hyun Soo Park, James M. Rehg, Yoichi Sato, Jianbo Shi, Mike Zheng Shou, Antonio Torralba, Lorenzo Torresani, Mingfei Yan, and Jitendra Malik. Ego4d: Around the world in 3, 000 hours of egocentric video. In CVPR, 2022. 3, 13 [28] Kristen Grauman, Andrew Westbury, Lorenzo Torresani, Kris Kitani, Jitendra Malik, Triantafyllos Afouras, Kumar Ashutosh, Vijay Baiyya, Siddhant Bansal, Bikram Boote, Eugene Byrne, Zachary Chavis, Joya Chen, Feng Cheng, FuJen Chu, Sean Crane, Avijit Dasgupta, Jing Dong, Marıa Escobar, Cristhian Forigua, Abrham Gebreselasie, Sanjay Haresh, Jing Huang, Md Mohaiminul Islam, Suyog Dutt Jain, Rawal Khirodkar, Devansh Kukreja, Kevin J. Liang, Jia-Wei Liu, Sagnik Majumder, Yongsen Mao, Miguel Martin, Effrosyni Mavroudi, Tushar Nagarajan, Francesco Ragusa, Santhosh Kumar Ramakrishnan, Luigi Seminara, Arjun Somayazulu, Yale Song, Shan Su, Zihui Xue, Edward Zhang, Jinxu Zhang, Angela Castillo, Changan Chen, Xinzhu Fu, Ryosuke Furuta, Cristina Gonzalez, Prince Gupta, Jiabo Hu, Yifei Huang, Yiming Huang, Weslie Khoo, Anush Kumar, Robert Kuo, Sach Lakhavani, Miao Liu, Mi Luo, Zhengyi Luo, Brighid Meredith, Austin Miller, Oluwatumininu Oguntola, Xiaqing Pan, Penny Peng, Shraman Pramanick, Merey Ramazanova, Fiona Ryan, Wei Shan, Kiran Somasundaram, Chenan Song, Audrey Southerland, Masatoshi Tateno, Huiyu Wang, Yuchen Wang, Takuma Yagi, Mingfei Yan, Xitong Yang, Zecheng Yu, Shengxin Cindy Zha, Chen Zhao, Ziwei Zhao, Zhifan Zhu, Jeff Zhuo, Pablo Arbelaez, Gedas Bertasius, Dima Damen, Jakob Engel, Giovanni Maria Farinella, Antonino Furnari, Bernard Ghanem, Judy Hoffman, C. V. Jawahar, Richard A. Newcombe, Hyun Soo Park, James M. Rehg, Yoichi Sato, Manolis Savva, Jianbo Shi, Mike Zheng Shout, and Michael Wray. Ego-exo4d: Understanding skilled human activity In CVPR, 2024. from firstand third-person perspectives. 3, [29] Chuan Guo, Shihao Zou, Xinxin Zuo, Sen Wang, Wei Ji, Xingyu Li, and Li Cheng. Generating diverse and natural 3d human motions from text. In CVPR, 2022. 4, 5, 13, 14, 16 [30] Jonathan Ho, Ajay Jain, and Pieter Abbeel. Denoising diffusion probabilistic models. In NeurIPS, 2020. 17 [31] Fangzhou Hong, Vladimir Guzov, Hyo Jin Kim, Yuting Ye, Richard Newcombe, Ziwei Liu, and Lingni Ma. Egolm: Multi-modal language model of egocentric motions. arXiv preprint arXiv:2409.18127, 2024. 2, 3, 6 [32] Edward J. Hu, Yelong Shen, Phillip Wallis, Zeyuan AllenZhu, Yuanzhi Li, Shean Wang, Lu Wang, and Weizhu Chen. Lora: Low-rank adaptation of large language models. In ICLR, 2022. 4, 6, [33] Jiangyong Huang, Silong Yong, Xiaojian Ma, Xiongkun Linghu, Puhao Li, Yan Wang, Qing Li, Song-Chun Zhu, Baoxiong Jia, and Siyuan Huang. An embodied generalist agent in 3d world. In ICML, 2024. 2, 3 [34] Sarah Hudson, Sheila Matson-Barkat, Nico Pallamin, and Guillaume Jegou. With or without you? Interaction and immersion in virtual reality experience. Journal of business research, 2019. 2 [35] Brian Ichter, Anthony Brohan, Yevgen Chebotar, Chelsea Finn, Karol Hausman, Alexander Herzog, Daniel Ho, Julian Ibarz, Alex Irpan, Eric Jang, Ryan Julian, Dmitry Kalashnikov, Sergey Levine, Yao Lu, Carolina Parada, Kanishka Rao, Pierre Sermanet, Alexander Toshev, Vincent Vanhoucke, Fei Xia, Ted Xiao, Peng Xu, Mengyuan Yan, Noah Brown, Michael Ahn, Omar Cortes, Nicolas Sievers, Clayton Tan, Sichun Xu, Diego Reyes, Jarek Rettinghouse, Jornell Quiambao, Peter Pastor, Linda Luu, Kuang-Huei Lee, Yuheng Kuang, Sally Jesmonth, Nikhil J. Joshi, Kyle Jeffrey, Rosario Jauregui Ruano, Jasmine Hsu, Keerthana Gopalakrishnan, Byron David, Andy Zeng, and Chuyuan Kelly Fu. Do as can, not as say: Grounding language in robotic affordances. In CoRL, 2022. 2, 3 [36] Catalin Ionescu, Dragos Papava, Vlad Olaru, and Cristian Sminchisescu. Human3.6m: Large scale datasets and predictive methods for 3d human sensing in natural environments. IEEE TPAMI, 2014. 13 [37] Biao Jiang, Xin Chen, Wen Liu, Jingyi Yu, Gang Yu, and Tao Chen. Motiongpt: Human motion as foreign language. In NeurIPS, 2023. 2 [38] Biao Jiang, Xin Chen, Wen Liu, Jingyi Yu, Gang Yu, and Tao Chen. Motiongpt: Human motion as foreign language. In NeurIPS, 2023. 2, 3, 6, [39] Moo Jin Kim, Karl Pertsch, Siddharth Karamcheti, Ted Xiao, Ashwin Balakrishna, Suraj Nair, Rafael Rafailov, Ethan Paul Foster, Grace Lam, Pannag Sanketi, Quan Vuong, Thomas Kollar, Benjamin Burchfiel, Russ Tedrake, Dorsa Sadigh, Sergey Levine, Percy Liang, and Chelsea Finn. Openvla: An open-source vision-language-action model. arXiv preprint arXiv:2406.09246, 2024. 2, 3 [40] Dan Kondratyuk, Lijun Yu, Xiuye Gu, Jose Lezama, Jonathan Huang, Grant Schindler, Rachel Hornung, Vighnesh Birodkar, Jimmy Yan, Ming-Chang Chiu, Krishna Somandepalli, Hassan Akbari, Yair Alon, Yong Cheng, Joshua V. Dillon, Agrim Gupta, Meera Hahn, Anja Hauth, David Hendon, Alonso Martinez, David Minnen, Mikhail Sirotenko, Kihyuk Sohn, Xuan Yang, Hartwig Adam, MingHsuan Yang, Irfan Essa, Huisheng Wang, David A. Ross, Bryan Seybold, and Lu Jiang. Videopoet: large language model for zero-shot video generation. In ICML, 2024. 3 [41] Woosuk Kwon, Zhuohan Li, Siyuan Zhuang, Ying Sheng, Lianmin Zheng, Cody Hao Yu, Joseph E. Gonzalez, Hao 10 Zhang, and Ion Stoica. Efficient memory management for large language model serving with pagedattention. In Proceedings of the ACM SIGOPS 29th Symposium on Operating Systems Principles, 2023. 6 [42] KunChang Li, Yinan He, Yi Wang, Yizhuo Li, Wenhai Wang, Ping Luo, Yali Wang, Limin Wang, and Yu Qiao. Videochat: Chat-centric video understanding. arXiv preprint arXiv:2305.06355, 2023. 5, 13 [43] Ruilong Li, Shan Yang, David A. Ross, and Angjoo Kanazawa. AI choreographer: Music conditioned 3d dance generation with AIST++. In ICCV, 2021. [44] Han Liang, Wenqian Zhang, Wenxuan Li, Jingyi Yu, and Lan Xu. Intergen: Diffusion-based multi-human motion generation under complex interactions. IJCV, 2024. 2 [45] Jing Lin, Yao Feng, Weiyang Liu, and Michael Black. Chathuman: Language-driven 3d human understanding arXiv preprint with retrieval-augmented tool reasoning. arXiv:2405.04533, 2024. 2 [46] Ji Lin, Hongxu Yin, Wei Ping, Pavlo Molchanov, Mohammad Shoeybi, and Song Han. VILA: on pre-training for visual language models. In CVPR, 2024. 7 [47] Haiyang Liu, Zihao Zhu, Naoya Iwamoto, Yichen Peng, Zhengqing Li, You Zhou, Elif Bozkurt, and Bo Zheng. BEAT: large-scale semantic and emotional multi-modal In ECCV, dataset for conversational gestures synthesis. 2022. 16 [48] Haotian Liu, Chunyuan Li, Qingyang Wu, and Yong Jae Lee. Visual instruction tuning. In NeurIPS, 2023. 5, [49] Shunlin Lu, Ling-Hao Chen, Ailing Zeng, Jing Lin, Ruimao Zhang, Lei Zhang, and Heung-Yeung Shum. Humantomato: Text-aligned whole-body motion generation. In ICML, 2024. 3, 14 [50] Arvind Neelakantan, Tao Xu, Raul Puri, Alec Radford, Jesse Michael Han, Jerry Tworek, Qiming Yuan, Nikolas Tezak, Jong Wook Kim, Chris Hallacy, et al. Text and code embeddings by contrastive pre-training. arXiv preprint arXiv:2201.10005, 2022. 5 [51] Evonne Ng, Javier Romero, Timur M. Bagautdinov, Shaojie Bai, Trevor Darrell, Angjoo Kanazawa, and Alexander Richard. From audio to photoreal embodiment: Synthesizing humans in conversations. In CVPR, 2024. 2, 4 [52] OpenAI. Gpt-4o system card. arXiv preprint arXiv:2410.21276, 2024. 2, 3, 4, 5, 6, 15, 16 [53] Long Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida, Carroll L. Wainwright, Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, John Schulman, Jacob Hilton, Fraser Kelton, Luke Miller, Maddie Simens, Amanda Askell, Peter Welinder, Paul F. Christiano, Jan Leike, and Ryan Lowe. Training language models to follow instructions with human feedback. In NeurIPS, 2022. 13 [54] Joon Sung Park, Joseph C. OBrien, Carrie Jun Cai, Meredith Ringel Morris, Percy Liang, and Michael S. Bernstein. Generative agents: Interactive simulacra of human behavior. In UIST, 2023. 3 [55] Georgios Pavlakos, Vasileios Choutas, Nima Ghorbani, Timo Bolkart, Ahmed A. A. Osman, Dimitrios Tzionas, and Michael J. Black. Expressive body capture: 3d hands, face, and body from single image. In CVPR, 2019. 14 [56] Georgios Pavlakos, Vasileios Choutas, Nima Ghorbani, Timo Bolkart, Ahmed A. A. Osman, Dimitrios Tzionas, and Michael J. Black. Expressive body capture: 3d hands, face, and body from single image. In CVPR, 2019. 3, 5, 13, 14 [57] Zekun Qi, Runpei Dong, Shaochen Zhang, Haoran Geng, Chunrui Han, Zheng Ge, Li Yi, and Kaisheng Ma. Shapellm: Universal 3d object understanding for embodied interaction. In ECCV, 2024. 13 [58] Zengyi Qin, Wenliang Zhao, Xumin Yu, and Xin Sun. Openvoice: Versatile instant voice cloning. arXiv preprint arXiv:2312.01479, 2023. [59] Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, and Ilya Sutskever. Language models are unsupervised multitask learners. OpenAI blog, 2019. 14 [60] Alec Radford, Jong Wook Kim, Tao Xu, Greg Brockman, Christine McLeavey, and Ilya Sutskever. Robust speech In ICML, recognition via large-scale weak supervision. 2023. 6 [61] Jeff Rasley, Samyam Rajbhandari, Olatunji Ruwase, and Yuxiong He. Deepspeed: System optimizations enable training deep learning models with over 100 billion parameters. In KDD, 2020. 6 [62] Giuseppe Riva, Fabrizia Mantovani, Claret S. Capideville, Alessandra Preziosa, Francesca Morganti, Daniela Villani, Andrea Gaggioli, Cristina Botella, and Mariano Alcaniz Raya. Affective interactions using virtual reality: The link between presence and emotions. Cyberpsychology Behav. Soc. Netw., 2007. 2 [63] David Saffo, Caglar Yildirim, Sara Di Bartolomeo, and Cody Dunne. Crowdsourcing virtual reality experiments using vrchat. In CHI, 2020. 13 [64] Yunfan Shao, Linyang Li, Junqi Dai, and Xipeng Qiu. Character-llm: trainable agent for role-playing. In EMNLP, 2023. 2, 6, [65] Li Siyao, Tianpei Gu, Zhitao Yang, Zhengyu Lin, Ziwei Liu, Henghui Ding, Lei Yang, and Chen Change Loy. Duolando: Follower GPT with off-policy reinforcement learning for dance accompaniment. In ICLR, 2024. 2, 6 [66] Mel Slater, Daniel Perez Marcos, Henrik Ehrsson, and Inducing illusory ownership of Maria Sanchez-Vives. virtual body. Frontiers in neuroscience, 2009. 2 [67] Theodore R. Sumers, Shunyu Yao, Karthik Narasimhan, and Thomas L. Griffiths. Cognitive architectures for language agents. 2024. 2 [68] suno.ai. Chattts: generative speech model for daily dialogue. 2023. [69] Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, Dan Bikel, Lukas Blecher, Cristian Canton-Ferrer, Moya Chen, Guillem Cucurull, David Esiobu, Jude Fernandes, Jeremy Fu, Wenyin Fu, Brian Fuller, Cynthia Gao, Vedanuj Goswami, Naman Goyal, Anthony Hartshorn, Saghar Hosseini, Rui Hou, Hakan Inan, Marcin Kardas, Viktor Kerkez, Madian Khabsa, Isabel Kloumann, Artem Korenev, Punit Singh Koura, Marie-Anne Lachaux, Thibaut Lavril, Jenya Lee, Diana Liskovich, Yinghai Lu, Yuning 11 Mao, Xavier Martinet, Todor Mihaylov, Pushkar Mishra, Igor Molybog, Yixin Nie, Andrew Poulton, Jeremy Reizenstein, Rashi Rungta, Kalyan Saladi, Alan Schelten, Ruan Silva, Eric Michael Smith, Ranjan Subramanian, Xiaoqing Ellen Tan, Binh Tang, Ross Taylor, Adina Williams, Jian Xiang Kuan, Puxin Xu, Zheng Yan, Iliyan Zarov, Yuchen Zhang, Angela Fan, Melanie Kambadur, Sharan Narang, Aurelien Rodriguez, Robert Stojnic, Sergey Edunov, and Thomas Scialom. Llama 2: Open foundation and finetuned chat models. arXiv preprint arXiv:2307.09288, 2023. 2, 3, 6, 7, 14, 17 [70] Quan Tu, Shilong Fan, Zihang Tian, Tianhao Shen, Shuo Shang, Xin Gao, and Rui Yan. Charactereval: chinese benchmark for role-playing conversational agent evaluation. In ACL, 2024. 6 [71] Guanzhi Wang, Yuqi Xie, Yunfan Jiang, Ajay Mandlekar, Chaowei Xiao, Yuke Zhu, Linxi Fan, and Anima Anandkumar. Voyager: An open-ended embodied agent with large language models. TMLR, 2024. 3 [72] Jason Wei, Yi Tay, Rishi Bommasani, Colin Raffel, Barret Zoph, Sebastian Borgeaud, Dani Yogatama, Maarten Bosma, Denny Zhou, Donald Metzler, Ed H. Chi, Tatsunori Hashimoto, Oriol Vinyals, Percy Liang, Jeff Dean, and William Fedus. Emergent abilities of large language models. TMLR, 2022. [73] Alexander W. Winkler, Jungdam Won, and Yuting Ye. Questsim: Human motion tracking from sparse sensors with simulated avatars. In SIGGRAPH Asia, 2022. 5, 15 [74] Hongtao Wu, Ya Jing, Chilam Cheang, Guangzeng Chen, Jiafeng Xu, Xinghang Li, Minghuan Liu, Hang Li, and Tao Kong. Unleashing large-scale video generative pre-training for visual robot manipulation. In ICLR, 2024. 15 [75] Tong Wu, Guandao Yang, Zhibing Li, Kai Zhang, Ziwei Liu, Leonidas J. Guibas, Dahua Lin, and Gordon Wetzstein. Gpt4v(ision) is human-aligned evaluator for text-to-3d generation. In CVPR, 2024. 13 [76] Liang Xu, Xintao Lv, Yichao Yan, Xin Jin, Shuwen Wu, Congsheng Xu, Yifan Liu, Yizhou Zhou, Fengyun Rao, Xingdong Sheng, Yunhui Liu, Wenjun Zeng, and Xiaokang Yang. Inter-x: Towards versatile human-human interaction analysis. In CVPR, 2024. 2, 4, 5, 13, 16 [77] Dongchao Yang, Jinchuan Tian, Xu Tan, Rongjie Huang, Songxiang Liu, Xuankai Chang, Jiatong Shi, Sheng Zhao, Jiang Bian, Xixin Wu, Zhou Zhao, and Helen Meng. Uniaudio: An audio foundation model toward universal audio generation. arXiv preprint arXiv:2310.00704, 2023. 6 [78] Zhitao Yang, Zhongang Cai, Haiyi Mei, Shuai Liu, Zhaoxi Chen, Weiye Xiao, Yukun Wei, Zhongfei Qing, Chen Wei, Bo Dai, Wayne Wu, Chen Qian, Dahua Lin, Ziwei Liu, and Lei Yang. Synbody: Synthetic dataset with layered human In ICCV, models for 3d human perception and modeling. 2023. 6 [79] Wenhao Yu, Nimrod Gileadi, Chuyuan Fu, Sean Kirmani, Kuang-Huei Lee, Montserrat Gonzalez Arenas, HaoTien Lewis Chiang, Tom Erez, Leonard Hasenclever, Jan Humplik, Brian Ichter, Ted Xiao, Peng Xu, Andy Zeng, Tingnan Zhang, Nicolas Heess, Dorsa Sadigh, Jie Tan, Yu12 val Tassa, and Fei Xia. Language to rewards for robotic skill synthesis. In CoRL, 2023. 13 [80] Neil Zeghidour, Alejandro Luebs, Ahmed Omran, Jan Skoglund, and Marco Tagliasacchi. Soundstream: An endto-end neural audio codec. TASLP, 2022. 3 [81] Jun Zhan, Junqi Dai, Jiasheng Ye, Yunhua Zhou, Dong Zhang, Zhigeng Liu, Xin Zhang, Ruibin Yuan, Ge Zhang, Linyang Li, Hang Yan, Jie Fu, Tao Gui, Tianxiang Sun, YuGang Jiang, and Xipeng Qiu. Anygpt: Unified multimodal LLM with discrete sequence modeling. In ACL, 2024. 3, 5, 6, 7, 17 [82] Dong Zhang, Shimin Li, Xin Zhang, Jun Zhan, Pengyu Wang, Yaqian Zhou, and Xipeng Qiu. Speechgpt: Empowering large language models with intrinsic cross-modal conversational abilities. In EMNLP, 2023. 3 [83] Mingyuan Zhang, Zhongang Cai, Liang Pan, Fangzhou Hong, Xinying Guo, Lei Yang, and Ziwei Liu. Motiondiffuse: Text-driven human motion generation with diffusion model. IEEE TPAMI, 2024. [84] Xin Zhang, Dong Zhang, Shimin Li, Yaqian Zhou, and Xipeng Qiu. Speechtokenizer: Unified speech tokenizer for speech large language models. arXiv preprint arXiv:2308.16692, 2023. 3, 17 [85] Yifan Zhang, Bingyi Kang, Bryan Hooi, Shuicheng Yan, and IEEE Jiashi Feng. Deep long-tailed learning: survey. TPAMI, 2023. 13 [86] Yaqi Zhang, Di Huang, Bin Liu, Shixiang Tang, Yan Lu, Lu Chen, Lei Bai, Qi Chu, Nenghai Yu, and Wanli Ouyang. Motiongpt: Finetuned llms are general-purpose motion generators. In AAAI, 2024. 2, 3 [87] Zhipu. Glm-4-voice. 2024. 13 [88] Zixiang Zhou, Yu Wan, and Baoyuan Wang. Avatargpt: Allin-one framework for motion understanding, planning, generation and beyond. In CVPR, 2024. 2 [89] Deyao Zhu, Jun Chen, Xiaoqian Shen, Xiang Li, and Mohamed Elhoseiny. Minigpt-4: Enhancing vision-language In understanding with advanced large language models. ICLR, 2024. 5 [90] Brianna Zitkovich, Tianhe Yu, Sichun Xu, Peng Xu, Ted Xiao, Fei Xia, Jialin Wu, Paul Wohlhart, Stefan Welker, Ayzaan Wahid, Quan Vuong, Vincent Vanhoucke, Huong T. Tran, Radu Soricut, Anikait Singh, Jaspiar Singh, Pierre Sermanet, Pannag R. Sanketi, Grecia Salazar, Michael S. Ryoo, Krista Reymann, Kanishka Rao, Karl Pertsch, Igor Mordatch, Henryk Michalewski, Yao Lu, Sergey Levine, Lisa Lee, Tsang-Wei Edward Lee, Isabel Leal, Yuheng Kuang, Dmitry Kalashnikov, Ryan Julian, Nikhil J. Joshi, Alex Irpan, Brian Ichter, Jasmine Hsu, Alexander Herzog, Karol Hausman, Keerthana Gopalakrishnan, Chuyuan Fu, Pete Florence, Chelsea Finn, Kumar Avinava Dubey, Danny Driess, Tianli Ding, Krzysztof Marcin Choromanski, Xi Chen, Yevgen Chebotar, Justice Carbajal, Noah Brown, Anthony Brohan, Montserrat Gonzalez Arenas, and Kehang Han. RT-2: vision-language-action models transfer web knowledge to robotic control. In CoRL, 2023. 2, 3, 13 SOLAMI: Social Vision-Language-Action Modeling for Immersive Interaction with 3D Autonomous Characters"
        },
        {
            "title": "Supplementary Material",
            "content": "A. Future Work Our work, SOLAMI, represents preliminary exploration for building 3D autonomous characters. While it has performed well in comparative experiments, there remains significant room for improvement on aspects as follows: Input Modality: For dyadic social interaction, using the users body motion and speech as input is sufficient. However, when considering multi-person interaction or interaction involving the environment and objects, video [24, 90] or dynamic 3D scenes [57] might be better choice; Data Collection: Our synthetic dataset, SynMSI, enables satisfactory user evaluation results. However, collecting real-time data of actual dyadic interaction could enable our model to generate more precise and natural body language and speech, while also supporting duplex streaming conversations, similar to [9, 87]. Compared to text and video modalities, the collection of embodied 3D data is undoubtedly challenging. Potential solutions include: capturing [16] or learning human behavioral data [11] from existing video datasets, building immersive interaction platforms [63] to gather data on human interactions, and using surrogate control to collect data from human interactions with 3D characters [21]; Cross Embodiment: Using unified SMPL-X [56] model to represent characters motion inevitably introduces challenges in cross-embodiment for different characters. While some degree of error and misalignment may not hinder information exchange in social language interaction, such representations clearly lack generalizability for fine-grained tasks (e.g., handshaking, object manipulation). The challenges of retargeting in 3D humanrelated tasks and cross-embodiment in robotics [90] share similarities, providing opportunities for mutual inspiration and methodological exchange; Long-Short Term Design: Although SOLAMI demonstrates effective modeling for real-time interactions, its architecture encounters challenges such as computational redundancy, forgetting, and training difficulties during extended social interactions. promising direction [17, 22] to explore is integrating long-term memory, knowledge, and skills with short-term real-time interaction. This approach could ensure interaction quality while reducing computational overhead and simplifying the training process; Efficient Learning Method: Although our dataset, SynMSI, tries to collect large-scale motion data, the inherently long-tail distribution [85] of human motions results in some behaviors having very low occurrence frequencies [29, 36, 76]. In particular, the data volume for signature actions of 3D characters is inherently limited. While models like GPT-3 [15] have demonstrated remarkable few-shot learning capabilities, the data-intensive training required is currently unsustainable in the field of digital humans. Therefore, exploring effective learning methods is essential. Leveraging character-focused knowledge embedded in existing foundation models [75, 79] or incorporating human evaluators [53] to guide the model in learning new skills from small number of samples are promising research directions. B. More Details of Architecture Design In this section, we discuss the input and output modalities of SOLAMI in Appendix B.1, compare the motion representation in Appendix B.2, and introduce details of our motion tokenizer and pre-training design in Appendix B.3. B.1. Input and Output Modalities Our ultimate goal is to establish unified behavioral modeling system for any character, where input modalities include wide range of sensory observations, including vision, audio, and haptics etc., and output modalities represent actions in the finest possible granularity. However, currently, we need to balance the ideal with the constraints of existing data and devices to develop model that provides an optimal user experience. Regarding devices, we employ VR headsets instead of mobile phones or computers because VR headset enables more immersive interactive experience by capturing and presenting richer information. In terms of input modalities, while 3D scenes or videos could serve as input and have some foundational models [42, 57], collecting corresponding social interaction data is challenging. For instance, datasets like Ego4D [27] and Ego-Exo4D [28] capture first-person videos and motion data but include very limited social interaction content and no data involving character interaction. Within VR environments, the majority of incremental information character can observe comes from users behaviors that VR devices can capture. Consequently, we chose user motion and speech as the primary input for SOLAMI. Similarly, for easy synthetic data generation and model training, we maintain the same types of output modalities for the character as for the users input. This symmetry en13 Table 3. Quantitative results of pre-training on text-to-motion task. () indicates that the values are better if the metrics are larger (smaller). The best results are in bold and the second best results are underlined. ID Body & Hand Repre Backbone 1 2 3 4 5 bind bind separate separate separate joints rotation rotation rotation rotation GPT-2 GPT-2 GPT-2 GPT-2 Llama2 Token Interleaved - - Yes No No FID Diversity 1.48 3.44 3.00 2.72 1.82 9.03 12.94 11.64 14.05 10.40 Metrics PA-MPJPE 148.00 143.70 117.26 112.53 110.23 Pred Valid 0.836 0.813 0.676 0.638 0.999 Table 4. Quantitative results of Motion VQVAE. () indicates that the values are better if the metrics are larger (smaller). The best results are in bold. ID Body & Hand Repre 1 2 3 4 separate bind separate bind joints joints rotation rotation Motion Metrics PA-MPJPE 87 80 88 FID 1.0 1.3 1.88 2.34 sures alignment between what the model observes and what it produces, facilitating more natural and precise interactive experience. B.2. Motion Representation Comparison Common representations of human motion are often based on 3D keypoints [29, 38, 49], which provide higher precision compared to methods based on joint rotations. However, this approach is inconsistent with the driving mechanism of 3D engines such as Unity Engine. When the model generates 3D keypoints, retargeting is necessary to derive the relative rotation of each joint with respect to its parent joint. Considering human motion priors, typical approach [55] involves fitting an SMPL-X [56] model to the 3D keypoints using optimization strategies, and subsequently retargeting the fitted SMPL-X model to the character. However, this process has two main drawbacks: 1. Time-Consuming Fitting Process: The fitting step is computationally intensive. With optimized methods like SMPLify [55], achieving an adequate result requires about 1 second of iteration on V100 GPU. 2. Fitting Artifacts and Distortion: Inevitable fitting errors can lead to biologically implausible joint rotations, significantly degrading visual quality. In our experiments, we observed that while human motion representation based on 3D keypoints performs well in terms of motion metrics, as shown in Tab. 3 and Tab. 4, its visual fidelity is inferior to representation based on joint rotations. To address this, we adopted cont6d representation for joint rotations, achieving improved visual outcomes. B.3. Motion Tokenizer and Pre-training After processing as described in Appendix B.2, we obtained 315-dimensional motion representation. When converting this motion representation into tokens via the tokenizers, several issues need to be discussed. Should body and hand motion features be represented separately? If so, how should their tokens be handled? Should the tokens for the body and hand motions be interleaved, or should they be input as independent sequences in the pre-training stage? Considering our computational cost, we conducted ablation experiments on the text-to-motion task using the GPT2 [59] backbone as the baseline model. Finally, we compared the models under the same settings using Llama27B [69] as the backbone. As shown in Tab. 4 and Tab. 3, compared to unified representations of hand and body motion (marked as bind), the separate representation (marked as separate) achieves better performance, particularly with higher precision on the text-to-motion task (t2m). However, the trade-off is that the probability of GPT-2 [59] producing outputs that conform to the expected format (marked as Pred Valid) decreases. However, this issue is mitigated in large part by using Llama2 [69] as the backbone model. We think this improvement is due to the differences in the language models: GPT-2, the relatively smaller language model, has weaker comprehension of textual instructions. In contrast, Llama2, trained on extensive corpora, demonstrates significantly stronger text understanding capabilities. Moreover, compared to interleaved tokens (Yes for Token Interleaved), separate sequence representations (No for Token Interleaved) achieve better motion metrics. We hypothesize that this is because learning separate sequences reduces the overall complexity of the motion pre-training task, thereby improving performance. Based on the above experimental evaluations, we ultimately select Llama2-7B [69] for its strong text comprehension capabilities as the LLM backbone. For processing motion representation, we employ separate motion tokenizers that convert the motion representation into noninterleaved token sequences. This configuration is used for the final instruction fine-tuning stage. Table 5. Methods of collecting multimodal interaction data. Methods Input Output MoCap Human Motions from Internet Videos with SMPLer-X [16] Motion Captioning on Internet Videos with GPT-4o [52] Real Data Collection from VR Platforms Synthetic Data Generation from Existing Datasets 1-3s: Turn head to the right and look straight ahead, with neutral expression; 4-5s: Turn body and look sideways, with serious expression, almost no movement; 6-8s: Turn to the left side, smiling while looking forward. 1-2s: panda in combat stance, right hand raised in fist, left hand extended, with serious facial expression; 3s: Pandas body tilts to the left side, right hand clenched in fist, left hand stretched forward, eyes looking to the right front; 4-5s: Panda raises both hands above the head, lifting one leg. C. More Details of Data Generation In this section, we first discuss several methods for collecting multimodal social interaction data in Appendix C.1. Then, we introduce the technical details of SynMSI generation pipeline in Appendix C.2. C.1. Comparison of Data Collection Methods From the perspective of data sources, we discuss three sources: internet videos, Immersive VR platform, and existing incomplete motion capture datasets, as shown in Tab. 5. Collecting from Internet Videos. The development of mobile devices has led to an explosion of video content, and researchers naturally expect the model to learn knowledge and capabilities from internet videos. Many works aim to implicitly learn human capabilities from videos [20, 74], but for our task, we anticipate obtaining explicit multi-modal interactive data through various tools [16, 52]. Human motions can be captured through video motion capture, but current video motion capture [16] faces challenges such as occlusion, temporal discontinuity, and long-tail problems, making it difficult to obtain high-quality motions. Understanding and annotating human behaviors in videos can be achieved using Vision-Language Models (VLM) [48, 52], and we find that with appropriate post-processing these annotations are usable. Additionally, there is another issue: the data obtained through this method lacks firstperson view and is often fixed at third-person view, which presents challenges in perspective transformation. Collecting from VR Platforms. Building VR interaction platform to directly collect user interaction data is the most straightforward method. However, two key problems arise: 1) Current VR devices body tracking systems [73] cannot provide ground truth-level data. For instance, existing VR devices estimate lower body postures instead of capturing with wearable sensors, and tracking becomes unreliable when hands move beyond the sensor range of VR equipment. 2) Human interaction data differs from 3D character representations. Specifically, animated characters move15 ments tend to be more exaggerated compared to real human motions, which naturally introduces data distribution gap. Collecting from Existing Incomplete Datasets. Due to the novelty of our task, there is no dataset that perfectly suits our needs. Common open-source datasets [29, 47, 76] typically provide semantic annotations for motion sequences or co-speech gestures. The most cost-effective and convenient approach is to complete these datasets or use them to synthesize multimodal social interaction datasets. However, this faces several challenges: How can we ensure the diversity of dialogue content? How can we ensure that synthesized speech and motion are reasonable? Can synthetic data guarantee user satisfaction? We address these questions in Sec. 4 and Sec. 6.3 of the main paper. And we will introduce some technical details about data synthesizing latter. In summary, obtaining data from the internet has high potential, but current video motion capture technology is insufficient to realize this potential, and it also involves perspective transformation challenges. Data collection from VR platforms is limited by hardware capabilities and faces difficulties in replicating character behaviors. Synthesizing data based on existing datasets represents an optimal choice when balancing cost and effectiveness. C.2. Details of SynMSI Generation Pipeline Motion Post-process Existing motion-text datasets [17, 76] primarily provide semantic-level text annotations, often overlooking behavioral details (such as sitting versus standing positions, orientations, etc.). Considering GPT-4os capability [52] in understanding human behaviors in videos, as shown in Tab. 5, one approach would be to render all motions into videos and then use VLM for annotation. However, for small research team, the cost of VLM API calls is relatively high. We propose compromise strategy: combining multiple text annotations for single motion and using GPT-4o [52] to generate comprehensive, detailed description. In practice, we find this method to be quite effective. Topics Collection. Without topic guidance, conversations with LLMs often converge to simple, generic content rather than character-specific, in-depth content [17, 64]. Using prompts to guide conversation is common strategy. We collected topics from the following perspectives: 1. Character-related topics: These topics are difficult to collect in bulk from the internet and were generated through GPT-4o [52] brainstorming; 2. News-related topics: Google Trends [3] has compiled many news topics that people care about in daily life; 3. Daily life topics: Some community websites, such as Jike, specifically curate such topic content; (a) Samantha (b) K-VRC (c) Batman (d) Banaya Figure 7. Word cloud visualization of the keywords in the collected characters topics. After collecting these topics, we used LLMs to post-process them, filtering and organizing them into topics suitable for character conversation. Topic keywords are shown in Fig. 7. Task Generation. Beyond daily conversation content, we also want SOLAMI to learn direct understanding of human body language and the ability to explicitly follow human instructions. For this purpose, when synthesizing data, we set up different tasks in the system prompt: common: daily conversation; motion understanding: requires users to generate motions with strong semantic information, and the character can clearly express understanding of body movements; instruction following: requires users to give clear motion instructions, and the character can output corresponding instructed movements; imitation: requires the character to imitate users motion. Script Generation Methods. Since we are using the chat version of LLMs, we experimented with and compared three script generation strategies: 1. Method 1: Round-by-Round completion: Using LLM to complete and refine the speech and motion text for each character round by round, which is the method mentioned in our main paper. 2. Method 2: Character Agent Dialogue: Similar to the SocioMind approach [17], using two LLMs to play two roles (User and Character), and alternately outputting speech and motion text, followed by refinement. 3. Method 3: One-shot generation: Generating the entire multi-turn dialogue script at once, then revising the script round by round based on retrieved motions. 4. Topics people are curious about: Common Q&A websites (such as Quora, Zhihu [6]) specifically organize these topics. According to our experimental results, Method 1 and Method 2 produce better results. Although Method 3 inithe quality deteriorate aftially generates good scripts, 16 E. Acknowledgments We extend our sincere gratitude to Fei Xia, Huazhe Xu, Tao Kong, Jiangyong Huang for their insights from the embodied intelligence field. We thank Mingyuan Zhang, Fangzhou Hong, and Xinying Guo for discussions on motion generation, Bo Li and Yuanhan Zhang for advice on multimodal model training. We would also like to acknowledge Han Du, Fanzhou Wang, Jiaqi Li, Liang Pan, Peng Gao, and Yukun Wei for insightful discussions on the topic. ter multiple rounds of modifications during motion-text database alignment. To produce SynMSI, we randomly alternate between Methods 1 and 2 to generate text scripts. Interactive Motion. If we only use single-person motions, our model would lack the capability for two-person interaction. To address this issue, during script generation, when we retrieve motion of one person in an interactive motion, we ask the LLM whether to use the motion of another person from the same interactive motion when generating the next round of motion text. D. More Details of Experiments D.1. LLM Selection We chose Llama2-7B [69] because at the time of our experiments, end-to-end models with speech pre-training were scarce, with AnyGPT [81] being one of the few that performed well. Thus we selected the Llama2 series as the backbone for fair comparison in subsequent experiments. Readers aiming to achieve the best results can certainly choose state-of-the-art models as the backbone. The Llama2-7B-chat model [69] tends to output increasingly longer dialogue content, which for LLM+Speech methods results in high inference latency from both LLM and TTS (sometimes exceeding 30 seconds). Therefore, through post-processing, we truncate the output content to maximum of 3 sentences. While truncating output content somewhat affects user experience, the lower user latency generally results in better overall experience. D.2. Voice Cloning Comparison Voice cloning / TTS has numerous available products and open-source models in both industry and academia, each with different focuses. We aim to achieve the best voice cloning effect in near real-time conditions. For this purpose, we compare these software and algorithms: ElevenLabs Instant Voice Cloning [1], ChatTTS + OpenVoice [7, 58], XTTS v2 [19], MARS5 [18], and Bark [68]. Among them, MARS5 [18] uses diffusion [30] framework and is relatively slow; ElevenLabs [1] produces the best results but has high API costs and tends to generate speech at faster pace. XTTS v2 [19] is more suitable option, and can achieve good balance between speed and quality. When SOLAMI processes speech, we use the pretrained SpeechTokenizer [84] and SoundStorm [13] from In SpeechTokenizer [84], one second of AnyGPT [81]. speech is encoded into 400 tokens across 8 layers. We only select tokens from the first semantic layer (50 tokens in total) to send to SOLAMI for processing. During SoundStorm [13] decoding, we choose 4 to 6 seconds of voice prompt based on the character and generate the speech with 4 iteration steps."
        }
    ],
    "affiliations": [
        "S-Lab, Nanyang Technological University",
        "SenseTime Research"
    ]
}