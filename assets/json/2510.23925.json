{
    "paper_title": "Latent Chain-of-Thought for Visual Reasoning",
    "authors": [
        "Guohao Sun",
        "Hang Hua",
        "Jian Wang",
        "Jiebo Luo",
        "Sohail Dianat",
        "Majid Rabbani",
        "Raghuveer Rao",
        "Zhiqiang Tao"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Chain-of-thought (CoT) reasoning is critical for improving the interpretability and reliability of Large Vision-Language Models (LVLMs). However, existing training algorithms such as SFT, PPO, and GRPO may not generalize well across unseen reasoning tasks and heavily rely on a biased reward model. To address this challenge, we reformulate reasoning in LVLMs as posterior inference and propose a scalable training algorithm based on amortized variational inference. By leveraging diversity-seeking reinforcement learning algorithms, we introduce a novel sparse reward function for token-level learning signals that encourage diverse, high-likelihood latent CoT, overcoming deterministic sampling limitations and avoiding reward hacking. Additionally, we implement a Bayesian inference-scaling strategy that replaces costly Best-of-N and Beam Search with a marginal likelihood to efficiently rank optimal rationales and answers. We empirically demonstrate that the proposed method enhances the state-of-the-art LVLMs on seven reasoning benchmarks, in terms of effectiveness, generalization, and interpretability."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 7 2 ] . [ 1 5 2 9 3 2 . 0 1 5 2 : r Latent Chain-of-Thought for Visual Reasoning Guohao Sun1,2,, Hang Hua2,3,+, Jian Wang2,+, Jiebo Luo3, Sohail Dianat1, Majid Rabbani1, Raghuveer Rao4, and Zhiqiang Tao1 1Rochester Institute of Technology, 2Snap Inc., 3University of Rochester, 4DEVCOM Army Research Laboratory"
        },
        {
            "title": "Abstract",
            "content": "Chain-of-thought (CoT) reasoning is critical for improving the interpretability and reliability of Large Vision-Language Models (LVLMs). However, existing training algorithms such as SFT, PPO, and GRPO may not generalize well across unseen reasoning tasks and heavily rely on biased reward model. To address this challenge, we reformulate reasoning in LVLMs as posterior inference and propose scalable training algorithm based on amortized variational inference. By leveraging diversity-seeking reinforcement learning algorithms, we introduce novel sparse reward function for token-level learning signals that encourage diverse, high-likelihood latent CoT, overcoming deterministic sampling limitations and avoiding reward hacking. Additionally, we implement Bayesian inference-scaling strategy that replaces costly Best-of-N and Beam Search with marginal likelihood to efficiently rank optimal rationales and answers. We empirically demonstrate that the proposed method enhances the state-of-the-art LVLMs on seven reasoning benchmarks, in terms of effectiveness, generalization, and interpretability. The code is available at https://github.com/heliossun/LaCoT."
        },
        {
            "title": "Introduction",
            "content": "Chain-of-thought (CoT) reasoning is critical for enhancing the interpretability and reliability of Large Vision-Language Models (LVLMs) [7, 11, 15, 16, 29, 44]. These models combine visual perception and natural language processing to perform intricate reasoning tasks that require explicit, step-by-step rationalization. As LVLMs have expanded into more sophisticated applications, such as visual question answering, commonsense reasoning, and complex task execution, the limitations of current training methods, such as generalization, have become increasingly evident. To enable visual CoT, mainstream training paradigms, such as Supervised Fine-Tuning (SFT), Proximal Policy Optimization (PPO) [34], and Group Relative Policy Optimization (GRPO) [12], primarily focus on optimizing next-token distributions or scalar rewards. While effective for indistribution tasks, these methods often struggle to generalize across diverse reasoning questions due to their inability to explicitly capture dependencies across trajectories [14]. Specifically, SFT heavily depends on teacher-forced log-likelihood, only to parrot reference traces; meanwhile, PPO and GRPO are constrained in exploration as their KL penalties enforce proximity to the SFT baseline, making them fall short in finding novel rationales. Additionally, they may cause reward hacking [36] issue that achieves high scores without genuinely solving the intended problem. To address these limitations, this work adopts latent variable model to realize visual CoT as probabilistic inference problem [6] over latent variables, allowing us to work with rich, expressive probabilistic models that better capture uncertainty and hidden structure, without needing direct supervision. Part of this work was conducted while Guohao Sun and Hang Hua were interns at Snap Inc. +Hang Hua and Jian Wang contributed equally. Corresponding authors: Guohao Sun (gs4288@rit.edu) and Zhiqiang Tao (zhiqiang.tao@rit.edu) Preprint. Under review. Unlike using prompting and in-context learning to generate deterministic reasoning CoT (Z), we treat as latent variable sampled from posterior (ZX, ) = (XZY )/ (cid:80) (XZ ), given question-answer pair (X, ) as observation. However, such sampling is intractable due to the normalization term. Existing methods to sample approximately from an intractable posterior include Markov chain Monte Carlo (MCMC) and RL approaches such as PPO [34]. Despite good training efficiency, these methods show limited capacity in modeling the full diversity of the distribution [14]. By contrast, Amortized Variational Inference (AVI) [19, 22, 23, 53] yields token-level learning through optimizing the Evidence Lower Bound (ELBO), which encourages diverse trajectories and provides principled way to draw samples from the target posterior distribution (see Fig. 1). One way to implement AVI is given by the generative flow networks (GFlowNets [4, 5]) algorithm: training neural network to approximate distribution of interest. Despite achieving strong performance in broad text reasoning tasks, prior GFlowNets-based approaches [14] have yet to fully address visual reasoning due to the long CoT sequence inherent in multimodal tasks(e.g., 1k tokens). In this study, we propose novel reasoning model, namely LaCoT, which enables amortized latent CoT sampling in LVLMs and generalizes across various visual reasoning tasks. To achieve this, we propose ❶ general RL training algorithm (RGFN) with novel reference-guided policy exploration method to overcome the catastrophic forgetting issue and eliminate the diversity constraint caused by the KL penalty. To improve exploration efficiency, we introduce ❷ token-level reward approximation method, allowing efficient mini-batch exploration for diverse sampling. Finally, we introduce ❸ Bayesian inference-scaling strategy (BiN) for optimal rationale-solution searching at inference time for any reasoning LVLM. Previous works have provided empirical evidence that Best-of-N (BoN) sampling [37], Beam Search [41], and other heuristic-driven approaches [47] can improve models performance at inference time. However, these methods are computationally costly and rely heavily on biased critic models, failing to provide an optimal reasoning chain or answer efficiently. Our inference procedure is grounded in Bayesian sampling principles to eliminate the critic model and improve interpretability. We treat rationales as integration variables and rank answers by principled, length-normalized marginal likelihood. Consequently, our method delivers scalable, probabilistically justified searching strategy, effectively identifying optimal rationales and answers within LVLMs. Empirically, we develop the proposed LaCoT on two base models, Qwen2.5-VL [3] 3B and 7B, where the 7B model achieves an improvement of 6.6% over its base model and outperforms GRPO by 10.6%. The 3B model surpasses its base model with 13.9% and achieves better results than larger models, e.g., LLaVA-CoT-11B and LLaVA-OV-7B, demonstrating the effectiveness of learning to sample latent CoT on reasoning benchmarks."
        },
        {
            "title": "2 Preliminaries",
            "content": "Generative Flow Networks (GFlowNets) [4, 5, 20, 54] are class of amortized variational inference methods designed to sample complex, structured objects such as sequences and graphs with probabilities proportional to predefined, unnormalized reward function. Unlike traditional generative models that often focus on maximizing likelihood or expected reward, GFlowNets objective, such as Sub-Trajectory Balance (subTB) [27], is hierarchical variational objective [28]. Such that if the model is capable of expressing any action distribution and the objective function is globally minimized, then the flow consistency for trajectory τ = (zi zj) is (zi) (cid:89) k=i+1 PF (zk zk1) = (zj) (cid:89) k=i+1 PB(zk1 zk) (1) by minimizing statistical divergence between the learned and and the target distributions over trajectories DKL(PBPF ), where (zi) is the in flow at state zi, PF (zizi1) and PB(zi1zi) indicates the forward and backward policy that predicts the probability between states. In the case of causal LLM, token sequences are autoregressively generated one-by-one from left to right, so there is only one path to each state zi, and each state has only one parent zi1. Given this condition, PB() = 1 for all states. By modeling PF () with qθ(), parameterized by θ, the loss function aims to ensure consistency between the flow assigned to all trajectories from one complete rationale (i.e., = (z1z2 zn) = z1:n). Specifically, for trajectory truncated by paired index (i, j) with 0 < n, the loss penalizes discrepancies between the flow at state zi, Figure 1: Comparison of different training algorithms for visual reasoning. PPO implicitly approximates the rationale distribution but tends to under-represent its full diversity due to limited exploration constrained by its reference policy (e.g., the SFT model), and it heavily relies on critic (reward) model. In contrast, AVI explicitly estimates the true target posterior (ZX, ) through latent rationales, which promote diverse trajectories and inherently prevent reward hacking. scaled by the product of transition probabilities from zi+1 to zj and the flow at zj: LSubTB(Z; θ) = (cid:88) 0i<jn (cid:88) = 0i<jn (cid:34) log (cid:34) log (zi) (cid:81)j k=i+1 qθ(zk z1:k1) (cid:35)2 (zj) R(z1:i) (cid:81)j k=i+1 qθ(zk z1:k1)qθ( z1:j) R(z1:j) qθ( z1:i) (2) (cid:35)2 , where (zi) = R(z1:i) = R(z1:i) ends at zi, where represents the terminal state, which is usually an eos token in LLM. qθ(z1:i) when zi is the the final state, R(z1:i) is the reward of trajectory"
        },
        {
            "title": "3 Amortizing Variational Inference for Latent Visual CoT",
            "content": "By leveraging GFlowNets for AVI in LVLM, we formulate visual reasoning as variational inference problem, as shown in Fig. 1. That is, given question-answer pair (X, ) as an observation, the goal is to find the latent visual CoT sequences that contribute the most to the conditional likelihood: (Y X) = (cid:88) (ZY X), ZP (ZX,Y ) (3) where (ZY X) denotes the likelihood assigned to concatenated sequence (e.g., ZY ) given visual instruction X, and is latent CoT supposed to be sampled from posterior distribution (ZX, ). To approximate such posterior, we use GLowNets objective derived in Eq. (2) an amortized variational inference method to train an autoregressive model qθ(ZX). By minimizing Eq. (2), the policy model learns to generate trajectories where the probability of generating particular trajectory is proportional to its reward (i.e., unnormalized posterior probability), ensuring that higher-likelihood rationales (as determined by R) are more likely. However, ❶ Eq. (2) requires token-level reward, which is infeasible in complex reasoning chains with thousands of tokens. ❷ Efficient and diverse exploration remains challenging research problem in reinforcement learning, especially when an environment contains large state spaces. Given these research problems, we provide our solutions in the following sections. 3.1 Token-level Marginal Reward Approximation The proposed amortized rationale sampler qθ(ZX) shares the same generation process as in autoregressive LVLM: given prefix condition X, and at the i-th step, token zi is sampled from 3 policy model qθ(ziX, z1:i1), which is then appended to the sequence. Consistent sampling autoregressively from the LVLM until terminal state is reached gives us one completion of rationale = (z1z2 zn). As shown in Eq.(2), the objective function incorporates state-level rewards, enabling the model to correctly attribute the contribution of each step to the final reward. By setting the reward R(z1:t) = log (Xz1:tY ) (z1:tX, ), we optimize the policy model to sample all trajectories such as τ = z1:t from the target distribution at convergence. By treating each token as state, such training algorithm provides clear guidance for the policy on how early actions impact the final outcome, helping reduce variance and improving convergence [27]. However, directly computing the exact reward for all states is computationally expensive during training, especially for long rationale sequence. natural approximation is to assume local smoothness of reward within small regions. To efficiently estimate intermediate rewards, we adopt linear interpolation strategy within segmented regions of length λ as shown in Fig. 2. Figure 2: Within complete rationale sequence, we compute the actual reward after each λ steps and adopt linear interpolation strategy to estimate the intermediate steps. The following proposition summarizes our theoretical claim for improving the training efficiency of Eq. (2). This approximation leverages the local smoothness of the log-likelihood, significantly reducing computational overhead without substantial loss in accuracy. We empirically evaluate the effectiveness of our claim in the experimental section. Proposition 1. Let R(z1:t) = log (Xz1:tY ) be joint-likelihood reward function. (a) If R(z1:) and R(z1:+λ) are true reward and the intermediate rewards within region of length λ are constantly increment, then we can approximate the reward at step + (where 0 λ) as R(z1:t+i) = R(z1:t) + λ (b) If λ is short enough, the interpolation reward error stays close to 0 and the flow between (z1:) and (z1:+λ) satisfies Eq. (1). (R(z1:t+λ) R(z1:t)) . (4) Proof. See Appendix A. Substituting the estimated reward in Eq. (2) gives our modified interpolated sub-trajectory balance (LISubTB) loss: LISubTB(Z; θ) = (cid:88) (cid:34) log 0i<jn R(z1:i) (cid:81)j k=i+1 qθ(zk z1:k1)qθ( z1:j) R(z1:j) qθ( z1:i) (cid:35)2 , (5) where R(z1:i) is defined pice-wise as: R(cid:0)z1:i(cid:1) = R(cid:0)z1:i(cid:1) R(cid:0)z1:t(cid:1) + λ if is the index of actual reward, (cid:0)R(cid:0)z1:t+λ(cid:1) R(cid:0)z1:t(cid:1)(cid:1) if < < + λ (estimated) . By computing the sparse rewards and efficiently approximating the intermediate states rewards, we can easily apply mini-batch exploration for diverse sampling to improve the generalizability of qθ(ZX) by covering the full target posterior. 3.2 Reference-Guided GFlowNet Fine-tuning Previous works [13, 26] suggest that exploration can let policy gradient methods collect unbiased gradient samples, escape deceptive local optima, and produce policies that generalize better. However, as shown in Fig. 3, allowing the model to explore without constraint causes the catastrophic forgetting issue, where the model tends to generate meaningless content with high likelihood but low reward. Existing methods, such as KL penalty [33] and clipped surrogate objective [34], control the size 4 of the gradient update. If the resulting policy is too far from the previous policy, the KL penalty constrains it to take an overly aggressive learning step. However, such method limits the exploration and increases the variance of trajectories [45]. To address this issue, we propose simple but effective solution by integrating reference-based mechanism to guide the exploration process towards generating higher-quality rationales. latent During training, we first explore candidate rationales {Z1, Z2, . . . , Zm} from the current policy model qθ(ZX) and compare them against reference rationale Zref the search in that anchors data-grounded region. Before gradient descent, each candidate Zi is associated with reward R(Zi) = log (XZiY ), and the ones that underperform the reference rationale are discarded before they reach the gradient, preventing collapse into meaningless but high-probability trajectory. To achieve candidate filtering, we define an indicator function: Figure 3: Allowing the policy model to explore the state space without constraint causes the catastrophic forgetting issue. The proposed reference-guided exploration effectively addresses this problem. I(Zi) = (cid:26)1, 0, if R(Zi) > δsR(Zref) otherwise (6) where δs = τmax (τmax τmin) min(1, s/50) is the annealing coefficient, is the index of the current training step. By doing this, we tolerate more exploration at the beginning and gradually increase its standard. The acceptance bar tightens only after 50 steps, allowing the model to explore first and then exploit later. Furthermore, by filtering out low-reward trajectories, we back-prop only through better-than-reference samples, which reduces gradient variance without hand-tuning the gradient clip or KL penalty. By incorporating the reference-based mechanism into Eq. (5), our final objective function is denoted as Reference-Guided GFlowNet fine-tuning (RGFN): LRGFN(Zi; θ) = (cid:88) i=1 I(Zi) LISubTB(Zi; θ) . (7) 3.3 Bayesian Inference over Latent Rationales Inference-scaling method such as Best-of-N (BoN) generates multiple candidate responses and select the best one based on verifier are widely used in reasoning LVLM [46, 47]. However, BoN has significant computational overhead [18], high dependency on reward model quality [2], and scalability challenges [30]. To address these limitations, this work introduces probabilistic method, namely Bayesian inference over latent rationales (BiN). Our approach is inspired by recent advancements in amortized variational inference for hierarchical models [1], where shared parameters represent local distributions, facilitating scalable inference. Figure 4: pipeline of BiN. Inference Given input and target answer , we can sample latent rationales from posterior (ZX, ) that bridges and , forming joint sequence XZY . The joint likelihood is denoted as (XZY ), and the marginal likelihood of given is expressed as (Y X) = (cid:88) (ZY X) = (cid:88) (Z X) (Y XZ) . (8) ZP (ZX,Y ) ZP (ZX,Y ) However, it is infeasible to sample all latent rationales from the (ZX, ). Therefore, we employ the policy model qθ(ZX) trained via Eq (7) to approximate the marginal likelihood. Fig. 4 shows the complete inference pipeline where we perform the following steps: (i) Sample latent rationales {Zi}N i=1 from the learned policy model: Zi qθ(ZX). (ii) For each sampled rationale Zi, we 5 Figure 5: Input sequence of training reasoning LVLM. We use token to represent learnable parts. Specifically, the fine-tuned reasoning LVLM heavily relies on annotated data during optimization, and the object tokens followed by Assistant enforce reasoning for all instructions. We introduce new role token Analyzer, so the model can selectively provide reasoning steps. sample the corresponding answer (i) from πΦ(YiXZi), where πΦ is reasoning LVLM. (iii) Compute the joint likelihood for all pairs (ZiYi): πΦ(ZiYiX). (iv) Estimate the marginal likelihood by normalizing over sequence length ZiYi as (Yi X) 1 (cid:88) j=1 1 ZiYi πΦ(ZiYi X). (9) (v) Select the answer Yi with the highest estimated marginal likelihood: = arg maxi (YiX) as the final output. This inference strategy aligns with Bayesian sampling principles by approximating the marginal likelihood (Y X) through sampling over latent rationales. The use of amortized variational inference for qθ(ZX) enables efficient sampling without the need for computationally intensive methods like Markov Chain Monte Carlo (MCMC). By selecting the answer with the highest estimated marginal likelihood, we aim to improve the interoperability of answer selection."
        },
        {
            "title": "4 Empirical results",
            "content": "4.1 Implementation Reward model. This work utilizes fine-tuned reasoning LVLM denoted as πΦ parameterized by Φ as the reward model R. Efficiently, πΦ also acts as the starting point of the proposed rationale sampler. The purpose of our reward model is to evaluate the quality of rationales sampled from the policy model (rationale sampler). To make sure that the reward function returns higher reward for better rationale, we first optimize πΦ by maximizing the likelihood of high-quality, structured examples of rationales (SFT), such as chain-of-thought (CoT) sequences. By learning from these examples, the model gains an initial understanding of how to approach complex tasks methodically. For training πΦ, we consider two pre-trained LVLMs as the base models, including Qwen2.5-VL-3B& 7B [3] and mixture of visual reasoning datasets from LLaVA-CoT [47] and R1-Onevision [48]. As shown in Fig. 5, we formulate the instructional data with new special token Analyzer. We fully fine-tune πΦ using the regular token prediction loss for one epoch. Rationale sampler. To sample the latent rationale from the posterior defined in Eq.(3), we parameterize the policy model as an autoregressive model qθ(ZX), initialized with πΦ. For training, we optimize the model using LoRA with = 64 and alpha = 128. We resample 3k visual reasoning sample from the SFT data, where each consists of (image, query, CoT, and answer). To be noted, we use the CoTs generated by teacher models, such as GPT-4o or Deepseek-R1, as our reference rationale Zref in Eq. (6). For the reward approximation defined in Eq. 4, we set λ = 8 for all the experiments. Please refer to Appendix B.2 for the study of λ. More hyperparameter settings can be found in Appendix B.5. 4.2 Multi-modal Reasoning Task description. Multi-modal reasoning evaluates the visual understanding and reasoning ability of LVLM as it requires step-by-step thinking and correct answer searching. This work proposes reasoning LVLM, i.e., LaCoT, which consists of latent rationale sampler qθ and an answering model πΦ. Specifically, at test time, we randomly sample latent rationales for an unseen with 6 Table 1: Test accuracy (%) on visual reasoning benchmarks. are results based on our reproduced experiments. The best results are bold, and the second-best results are underlined. We choose the reasoning models fine-tuned with SFT and GRPO (R1-Onevision) as baselines. All the base models were prompted by step-by-step reasoning instruction. Method GPT-4o Gemini-1.5-Pro Claude-3.5-Sonnet InternVL2-4B [7] Qwen2.5-VL-3B [3] LaCoT-Qwen-3B LLaVA-CoT-11B [47] LLaVA-OV-7B [21] MiniCPM-V2.6 [49] InternVL2-8B [7] Qwen2.5-VL-7B [3] R1-Onevision [48] LaCoT-Qwen-7B MathVista MathVision MathVerse MMMU MMMU-pro MMVet MME test vision-only vision mini test full val 60.0 63.9 67.7 58.6 60.3 63.2 52.5 63.2 60.6 58.3 63.7 64.1 68.4 30.4 19.2 - 16.5 21.2 20. - 11.1 17.5 18.4 25.4 23.9 24.9 40.6 - 46.3 32.0 26.1 40.0 22.6 26.2 25.7 37.0 38.2 37.8 43.3 70.7 65.8 68.3 47.9 46.6 48. - 48.8 49.8 52.6 50.0 47.9 54.9 51.9 46.9 51.5 - 22.4 28.9 - 24.1 27.2 25.4 34.6 28.2 35.3 69.1 64.0 66.0 55.7 61.4 69. 64.9 57.5 60.0 60.0 70.5 71.1 74.2 2329 2111 1920 2046 2134 2208 - 1998 2348 2210 2333 1111 2372 temperature τ from qθ(ZX), then the answer model samples answers from πΦ(Y XZ). Finally, we estimate the marginal likelihood of each answer (Y X) using the proposed BiN and return the highest one as the final output, as shown in Eq. (9). Benchmarks. This work utilizes three mathematical and one general domain reasoning benchmarks: (i) MathVista [24]: math benchmark designed to combine challenges from diverse mathematical and visual tasks, requiring fine-grained visual understanding and compositional reasoning. (ii) MathVision [42]: meticulously curated collection of 3,040 high-quality mathematical problems with visual contexts sourced from real math competitions. (iii) MathVerse [55]: an all-around visual math benchmark designed for an equitable and in-depth evaluation of LVLMs. We report the Vision-Only result on 788 questions, which reveals significant challenge in rendering the entire question within the diagram. (vi) MMMU [51]: benchmark designed to evaluate LVLM on massive multi-discipline tasks demanding college-level subject knowledge and deliberate reasoning. Furthermore, we conduct additional experiments on MMMU-pro [52], MMVet [50], and MME [9], where MMMU-Pro is more robust version of MMMU, designed to assess LVLMs understanding and reasoning capabilities more rigorously. Results. This work provides two LaCoT models (3B and 7B). From the results summarized in Table 1, our models are the best open-source LVLM and narrow the gap to GPT-4o to less than 3 points while using only 7 billion parameters. The consistent improvements on MathVista and MMMU show that LaCoT strengthens general multi-modal reasoning. MathVerse-Vision-only improves the most, especially at 3B, where accuracy jumps 14 points and outperforms all 7B models. This advancement indicates that LaCoT significantly boosts diagram comprehension and OCR robustness. On the other hand, MathVision consists of real Olympiad diagrams, which are more varied, and often handwritten or low-resolution, conditions that push OCR and visual grounding beyond. Many problems split critical information between text and picture (e.g., tiny angle labels or subtle curve annotations), so single misread propagates through the longer, proof-style reasoning chains, leading to performance drop. 7 Figure 6: Maximum log-likelihood and diversity of the sampled rationale. LaCoT model () samples higher log-likelihood rationale while maintaining higher rationale diversity than SFT () model. The LaCoT model can sample rationales with higher diversity than baseline models, increasing the probability of sampling answers with higher likelihood. To validate this hypothesis, we sample 5 rationale candidates with random temperature (T) for each visual instruction. To measure the semantic diversity of the samples, we compute the average inter-sentence similarity between the candidate and the reference set. As shown in Fig. 6, rationales generated by LaCoT-Qwen-3B with = 0.7 have the highest log-likelihood and diversity. Qualitative results can be seen in Fig. 8 and the supplementary. 4.3 Inference-time Scaling Table 2: Comparison between two inference-time scaling methods using LaCoT-Qwen (3B/7B). We compare BiN (ours) with Best-of-N (BoN) using LaCoT-Qwen as the shared policy model. At inference, we sample rationaleanswer pairs, compute length-normalized log-likelihood of each answer as the reward, and for BoN select the answer with the highest reward. To ensure fairness, no external reward model is used. We evaluate {5, 10} for both methods and report the best score per method. As shown in Table 2, BiN consistently outperforms BoN on visual reasoning benchmarks. MathVerse MathVista MMMU MMVet 3B w/ BoN 3B w/ BiN (ours) 7B w/ BoN 7B w/ BiN (ours) 44.7 48. 47.3 54.9 67.1 69.6 71.2 74.2 21.2 40.0 26.5 39.7 57.1 63. 62.2 68.4 Method 4.4 Ablation Studies Effectiveness of RGFN. As baselines, we consider zero-shot prompting w/o reasoning, supervised fine-tuning on the visual reasoning dataset, and GRPO [35] fine-tuning. Table 3: Test accuracy (%) on reasoning benchmarks using Qwen2.5-VL-7B model. From the results summarized in Table 3, the base model performs well without chain-of-thought reasoning. While supervised fine-tuning on reasoning data slightly improves performance on two benchmarks, it still struggles to generalize to challenging visual reasoning tasks. Fine-tuning with GRPO yields poor performance, partly due to inadequate guidance of the external reward model, i.e., it cannot distinguish good rationales from bad ones, and limited exploration due to the KL penalty. Such misleading optimization due to misaligned reward is widely noted issue in RL-based algorithms [57] for LVLM. On the other hand, by matching the target distribution, RGFN avoids collapsing to single mode of the reward, and the reference-guided exploration covers diverse trajectories, leading to better performance on complex examples. Method MathVista MathVerse MMMU SFT GRPO RGFN 50.6 47.9 54.9 62.7 62.6 68.4 38.7 36.8 43.3 Zero-shot 38.2 63.7 50.0 Study of BiN. To evaluate the scalability, we apply the proposed inferencescaling method by varying the number of candidates and temperature using LaCoT-Qwen-3B on the reasoning benchmarks. As illustrated in Fig. 7, test accuracy consistently increases with higher , and higher . This indicates that increasing systematically improves test-time accuracy because it (i) reduces the Monte-Carlo variance of the marginal-likelihood estimatorstandard error scales as O(1/ )thereby stabilizing answer rankings; (ii) offers broader posterior coverage, mitigating mode-dropping bias inherent in the amortized sampling qθ(ZX); (iii) smooths fluctuations introduced by length-normalization, yielding more reliable re-weighting; and (iv) enlarges the candidate answer set, elevating the chance that the correct output is observed. Together, these effects drive an exponential decay in the probability of selecting an incorrect answer. Figure 7: Test accuracy on reasoning benchmarks using LaCoT-Qwen-3B. We evaluate the impact of #rationale candidates (N ) and random temperature (T). 8 Furthermore, higher can effectively address the hallucination issue in visual reasoning. As shown in Fig. 7, when the sampled rational size = 1, BiN may produce incorrect or misleading reasoning steps and lead to lower answer accuracy on the MMMU dataset. However, increasing from 1 to 5 significantly mitigates hallucination and improves answer accuracy. We provide qualitative results in Appendix B.1. To evaluate the generalizability of BiN, we evaluate the performance of Qwen2.5-VL 3B & 7B (SFT) with the proposed inference-scaling method. We set = 5 and = 0.7, which gives the most performance boost with relatively shorter inference time. As shown in Table 4, BiN consistently improves the model performance on all benchmarks, indicating the effectiveness of BiN as general inference-scaling method for reasoning LVLMs. Table 4: Test accuracy of Qwen2.5-VL supervised fine-tuning on reasoning data. Method MathVista MathVerse MMMU 7B (SFT) + BiN 3B (SFT) + BiN 62.7 64.4 58.7 59.4 38.7 38.9 33.3 35.2 50.6 51. 43.1 45."
        },
        {
            "title": "5 Related Work",
            "content": "Learning-based Multimodal CoT (MCoT) methods have emerged as powerful paradigm for enhancing the reasoning capabilities of LVLMs [39, 40]. Unlike prompt-based or plan-based approaches, learning-based MCoT explicitly embeds the entire reasoning trajectory into the models through supervised learning on rationale-augmented datasets. Early studies such as Multimodal-CoT [56] pioneered this direction by fine-tuning LVLMs to generate visual CoT, facilitating structured reasoning process aligned with human cognitive patterns. From that, methods like MC-CoT [46] further refined this approach by incorporating multimodal consistency constraints and majority voting mechanisms during training. In addition, methods such as PCoT [43] and G-CoT [25] demonstrated that explicitly training LVLMs with structured rationales improves the interpretability and generalizability. These advancements underscore the effectiveness and necessity of embedding structured, rationale-driven reasoning capabilities directly into multimodal models. Reinforcement Learning-based Language Models have demonstrated significant effectiveness in advancing the reasoning capabilities of LLMs. DeepSeek-R1 [12] exemplifies this by activating long-chain-of-thought (long-CoT) reasoning solely through reinforcement learning (RL), achieving improvements over models such as GPT-o1 [17] in specific aspects when combined with supervised fine-tuning (SFT) cold starts and iterative self-improvement. This success has spurred further interest in RL-driven models, including Open-R1 [8] and TinyZero [31]. To enhance reasoning, generalization, and ensure training stability, several RL algorithms have been developed, such as PPO [34], GRPO [12], and simplified methods like RLHF [58], DPO [32], and SPO [38]. Nevertheless, these approaches are heavily dependent on high-quality human-annotated data (e.g., human preference labels and scalar rewards) and typically produce policies with limited diversity. To address these limitations, this work proposes an RL algorithm specifically designed to train LVLMs using amortized variational inference, which is capable of generating diverse outputs and supporting probabilistic inference-time scaling. Inference-time Scaling methods aim to enhance reasoning performance during inference by leveraging high-quality prompts and effective sampling strategies. Plan-based approaches, exemplified by MM-ToT [10] and LLaVA-CoT [47], utilize search strategies such as DFS and BFS, including Best-of-N search, sentence-level beam search, and stage-level beam search, to identify optimal reasoning trajectories. These methods typically assess candidate trajectories using scalar metrics ranging from 0.1 to 1.0. However, such explicit evaluation is computationally expensive, as each candidate requires an additional forward pass through dedicated reward model. To mitigate this computational overhead, our work introduces learning-based algorithm designed to align the marginal likelihood of generating rationale directly with its reward. This approach enables efficient probabilistic sampling without explicit reward computations during inference."
        },
        {
            "title": "6 Conclusion",
            "content": "In real-world scenario, solving fixed visual query with different reasoning chains that lead to the correct answer requires nuanced understanding of image, context, logic, and flexibility in 9 Figure 8: Qualitative results of visual reasoning. LaCoT can sample more diverse and comprehensive reasoning chain than the GRPO model. thought. While querying this knowledge in LVLM involves sampling from intractable posterior distributions. To address this challenge, we propose novel training algorithm based on amortized variational inference for latent visual chains-of-thought (CoT). Our approach incorporates token-level reward approximation and RGFN, enabling effective and efficient optimization of policy model to generate diverse and plausible reasoning trajectories, outperforming both supervised fine-tuning and reward-maximization baselines. In addition, we introduce new inference-time scaling strategy, BiN, which mitigates reward hacking and enhances interpretability with statistically robust selection criteria. Building upon these components, we present LaCoT that leverages rationale sampler for general-purpose visual reasoning, and an answer generator that is enhanced by high-quality reasoning chains. Given this system, future work should investigate the possibility of applying it for knowledge distillation and synthetic data generation. Limitations. Due to resource constraints, we apply the proposed methods to models up to 7B parameters, but we expect the conclusions to hold for larger models. In fact, our training and inference method can be applied to any autoregressive model, including LLM and LVLM, with various model sizes. As with any on-policy method, exploration in tasks with complex latent remains an open challenge since multiple factors can affect the exploration time, such as sequence length and technical challenges like memory cost. Despite the improved inference performance, this work does not address issues such as hallucination, which are closely related to internal knowledge."
        },
        {
            "title": "Acknowledgments and Disclosure of Funding",
            "content": "This research was supported in part by the DEVCOM Army Research Laboratory under Contract W911QX-21-D-0001, the National Science Foundation under Grant 2502050, and the National Institutes of Health under Award R16GM159146. The content is solely the responsibility of the authors and does not necessarily represent the official views of the funding agencies."
        },
        {
            "title": "References",
            "content": "[1] Abhinav Agrawal and Justin Domke. Amortized variational inference for simple hierarchical models. In Neural Information Processing Systems, 2021. [2] Afra Amini, Tim Vieira, Elliott Ash, and Ryan Cotterell. Variational best-of-n alignment. In The Thirteenth International Conference on Learning Representations, 2025. [3] Shuai Bai, Keqin Chen, Xuejing Liu, Jialin Wang, Wenbin Ge, Sibo Song, Kai Dang, Peng Wang, Shijie Wang, Jun Tang, Humen Zhong, Yuanzhi Zhu, Mingkun Yang, Zhaohai Li, Jianqiang Wan, Pengfei Wang, Wei Ding, Zheren Fu, Yiheng Xu, Jiabo Ye, Xi Zhang, Tianbao Xie, Zesen Cheng, Hang Zhang, Zhibo Yang, Haiyang Xu, and Junyang Lin. Qwen2.5-vl technical report. ArXiv, 2025. [4] Emmanuel Bengio, Moksh Jain, Maksym Korablyov, Doina Precup, and Yoshua Bengio. Flow network based generative models for non-iterative diverse candidate generation. Advances in Neural Information Processing Systems, 34:2738127394, 2021. [5] Yoshua Bengio, Tristan Deleu, J. Edward Hu, Salem Lahlou, Mo Tiwari, and Emmanuel Bengio. Gflownet foundations. In The Journal of Machine Learning Research (JMLR), 2023. [6] David M. Blei, Alp Kucukelbir, and Jon D. McAuliffe. Variational inference: review for statisticians. Journal of the American Statistical Association, 2016. [7] Zhe Chen, Jiannan Wu, Wenhai Wang, Weijie Su, Guo Chen, Sen Xing, Muyan Zhong, Qinglong Zhang, Xizhou Zhu, Lewei Lu, et al. Internvl: Scaling up vision foundation models and aligning for generic visual-linguistic tasks. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 2418524198, 2024. [8] Hugging Face. Open r1: fully open reproduction of deepseek-r1, January 2025. [9] Chaoyou Fu, Peixian Chen, Yunhang Shen, Yulei Qin, Mengdan Zhang, Xu Lin, Zhenyu Qiu, Wei Lin, Jinrui Yang, Xiawu Zheng, Ke Li, Xing Sun, and Rongrong Ji. Mme: comprehensive evaluation benchmark for multimodal large language models. ArXiv, abs/2306.13394, 2023. [10] Kye Gomez. Multimodal-tot. https://github.com/kyegomez/MultiModal-ToT, 2023. Accessed: 2025-04-20. [11] Google. Gemini 2.0 flash, 2025. https://blog.google/technology/google-deepmind/ google-gemini-ai-update-december-2024/#gemini-2-0-flash. [12] Daya Guo, Dejian Yang, Haowei Zhang, Junxiao Song, Ruoyu Zhang, Runxin Xu, Qihao Zhu, Shirong Ma, Peiyi Wang, Xiao Bi, et al. Deepseek-r1: Incentivizing reasoning capability in llms via reinforcement learning. arXiv preprint arXiv:2501.12948, 2025. [13] Zhang-Wei Hong, Tzu-Yun Shann, Shih-Yang Su, Yi-Hsiang Chang, Tsu-Jui Fu, and Chun-Yi Lee. Diversity-driven exploration strategy for deep reinforcement learning. In Proceedings of the 32nd International Conference on Neural Information Processing Systems, 2018. [14] Edward Hu, Moksh Jain, Eric Elmoznino, Younesse Kaddar, Guillaume Lajoie, Yoshua Bengio, and Nikolay Malkin. Amortizing intractable inference in large language models. In The Twelfth International Conference on Learning Representations (ICLR), 2024. [15] Hang Hua, Qing Liu, Lingzhi Zhang, Jing Shi, Zhifei Zhang, Yilin Wang, Jianming Zhang, and Jiebo Luo. Finecaption: Compositional image captioning focusing on wherever you want at any granularity. ArXiv, abs/2411.15411, 2024. [16] Hang Hua, Yunlong Tang, Chenliang Xu, and Jiebo Luo. V2xum-llm: Cross-modal video summarization with temporal prompt instruction tuning. In AAAI Conference on Artificial Intelligence, 2024. [17] Aaron Jaech, Adam Kalai, Adam Lerer, Adam Richardson, Ahmed El-Kishky, Aiden Low, Alec Helyar, Aleksander Madry, Alex Beutel, Alex Carney, et al. Openai o1 system card. arXiv preprint arXiv:2412.16720, 2024. [18] Zhewei Kang, Xuandong Zhao, and Dawn Song. Scalable best-of-n selection for large language models via self-certainty. ArXiv, 2025. [19] Andrei V. Konstantinov, Lev V. Utkin, Alexey A. Lukashin, and Vladimir Mulukha. Neural attention forests: Transformer-based forest improvement. ArXiv, abs/2304.05980, 2023. [20] Salem Lahlou, Tristan Deleu, Pablo Lemos, Dinghuai Zhang, Alexandra Volokhova, Alex Hernández-Garcıa, Léna Néhale Ezzine, Yoshua Bengio, and Nikolay Malkin. theory of continuous generative flow networks. In International Conference on Machine Learning, pages 1826918300. PMLR, 2023. [21] Bo Li, Yuanhan Zhang, Dong Guo, Renrui Zhang, Feng Li, Hao Zhang, Kaichen Zhang, Yanwei Li, Ziwei Liu, and Chunyuan Li. Llava-onevision: Easy visual task transfer. ArXiv, 2024. [22] Huafeng Liu, Jiaqi Wang, and Liping Jing. Cluster-wise hierarchical generative model for deep amortized clustering. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 1510915118, 2021. [23] Huafeng Liu, Tong Zhou, and Jiaqi Wang. Deep amortized relational model with group-wise hierarchical generative process. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 36, pages 75507557, 2022. [24] Pan Lu, Hritik Bansal, Tony Xia, Jiacheng Liu, Chun yue Li, Hannaneh Hajishirzi, Hao Cheng, Kai-Wei Chang, Michel Galley, and Jianfeng Gao. Mathvista: Evaluating mathematical reasoning of foundation models in visual contexts. In International Conference on Learning Representations, 2023. [25] Yingzi Ma, Yulong Cao, Jiachen Sun, Marco Pavone, and Chaowei Xiao. Dolphins: Multimodal language model for driving. In European Conference on Computer Vision, 2024. [26] Marlos C. Machado, Marc G. Bellemare, and Michael Bowling. laplacian framework for option discovery in reinforcement learning. In Proceedings of the 34th International Conference on Machine Learning - Volume 70, 2017. [27] Kanika Madan, Jarrid Rector-Brooks, Maksym Korablyov, Emmanuel Bengio, Moksh Jain, Andrei Cristian Nica, Tom Bosc, Yoshua Bengio, and Nikolay Malkin. Learning gflownets from partial episodes for improved convergence and stability. In International Conference on Machine Learning (ICML), 2023. [28] Nikolay Malkin, Salem Lahlou, Tristan Deleu, Xu Ji, Edward Hu, Katie Everett, Dinghuai Zhang, and Yoshua Bengio. GFlownets and variational inference. In The Eleventh International Conference on Learning Representations, 2023. [29] OpenAI. Gpt-4 technical report. arXiv preprint arXiv:2303.08774, 2023. [30] Jiayi Pan, Xiuyu Li, Long Lian, Charlie Snell, Yifei Zhou, Adam Yala, Trevor Darrell, Kurt Keutzer, and Alane Suhr. Learning adaptive parallel reasoning with language models. ArXiv, 2025. [31] Jiayi Pan, Junjie Zhang, Xingyao Wang, Lifan Yuan, Hao Peng, and Alane Suhr. Tinyzero. https://github.com/Jiayi-Pan/TinyZero, 2025. Accessed: 2025-01-24. [32] Rafael Rafailov, Archit Sharma, Eric Mitchell, Christopher Manning, Stefano Ermon, and Chelsea Finn. Direct preference optimization: Your language model is secretly reward model. Advances in Neural Information Processing Systems, 2023. [33] John Schulman, Sergey Levine, Pieter Abbeel, Michael Jordan, and Philipp Moritz. Trust region policy optimization. In Proceedings of the 32nd International Conference on Machine Learning, 2015. [34] John Schulman, Filip Wolski, Prafulla Dhariwal, Alec Radford, and Oleg Klimov. Proximal policy optimization algorithms. arXiv preprint arXiv:1707.06347, 2017. [35] Zhihong Shao, Peiyi Wang, Qihao Zhu, Runxin Xu, Jun-Mei Song, Mingchuan Zhang, Y. K. Li, Yu Wu, and Daya Guo. Deepseekmath: Pushing the limits of mathematical reasoning in open language models. ArXiv, 2024. [36] Joar Skalse, Nikolaus Howe, Dmitrii Krasheninnikov, and David Krueger. Defining and characterizing reward gaming. Advances in Neural Information Processing Systems, 2022. [37] Nisan Stiennon, Long Ouyang, Jeff Wu, Daniel M. Ziegler, Ryan Lowe, Chelsea Voss, Alec Radford, Dario Amodei, and Paul Christiano. Learning to summarize from human feedback. In Proceedings of the 34th International Conference on Neural Information Processing Systems, 2020. [38] Guohao Sun, Can Qin, Yihao Feng, Zeyuan Chen, Ran Xu, Sohail Dianat, Majid Rabbani, Raghuveer Rao, and Zhiqiang Tao. Structured policy optimization: Enhance large visionlanguage model via self-referenced dialogue. In Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV), 2025. 12 [39] Guohao Sun, Can Qin, Huazhu Fu, Linwei Wang, and Zhiqiang Tao. Self-training large language and vision assistant for medical question answering. In Proceedings of the 2024 Conference on Empirical Methods in Natural Language Processing, 2024. [40] Guohao Sun, Can Qin, Jiamian Wang, Zeyuan Chen, Ran Xu, and Zhiqiang Tao. Sq-llava: Self-questioning for large vision-language assistant. In European Conference on Computer Vision, 2024. [41] Ilya Sutskever, Oriol Vinyals, and Quoc V. Le. Sequence to sequence learning with neural networks. In Proceedings of the 28th International Conference on Neural Information Processing Systems, 2014. [42] Ke Wang, Junting Pan, Weikang Shi, Zimu Lu, Houxing Ren, Aojun Zhou, Mingjie Zhan, and Hongsheng Li. Measuring multimodal mathematical reasoning with MATH-vision dataset. In The Thirty-eight Conference on Neural Information Processing Systems Datasets and Benchmarks Track, 2024. [43] Lei Wang, Yi Hu, Jiabang He, Xing Xu, Ning Liu, Hui Liu, and Heng Tao Shen. T-sciq: Teaching multimodal chain-of-thought reasoning via large language model signals for science question answering. In Proceedings of the AAAI Conference on Artificial Intelligence, 2024. [44] Peng Wang, Shuai Bai, Sinan Tan, Shijie Wang, Zhihao Fan, Jinze Bai, Keqin Chen, Xuejing Liu, Jialin Wang, Wenbin Ge, Yang Fan, Kai Dang, Mengfei Du, Xuancheng Ren, Rui Men, Dayiheng Liu, Chang Zhou, Jingren Zhou, and Junyang Lin. Qwen2-vl: Enhancing visionlanguage models perception of the world at any resolution. arXiv preprint arXiv:2409.12191, 2024. [45] Yuhui Wang, Hao He, and Xiaoyang Tan. Truly proximal policy optimization. In Proceedings of The 35th Uncertainty in Artificial Intelligence Conference, 2020. [46] Lai Wei, Wenkai Wang, Xiaoyu Shen, Yu Xie, Zhihao Fan, Xiaojin Zhang, Zhongyu Wei, and Wei Chen. Mc-cot: modular collaborative cot framework for zero-shot medical-vqa with llm and mllm integration. arXiv preprint arXiv:2410.04521, 2024. [47] Guowei Xu, Peng Jin, Hao Li, Yibing Song, Lichao Sun, and Li Yuan. Llava-cot: Let vision language models reason step-by-step, 2024. [48] Yi Yang, Xiaoxuan He, Hongkun Pan, Xiyan Jiang, Yan Deng, Xingtao Yang, Haoyu Lu, Dacheng Yin, Fengyun Rao, Minfeng Zhu, Bo Zhang, and Wei Chen. R1-onevision: Advancing generalized multimodal reasoning through cross-modal formalization. ArXiv, 2025. [49] Yuan Yao, Tianyu Yu, Ao Zhang, Chongyi Wang, Junbo Cui, Hongji Zhu, Tianchi Cai, Haoyu Li, Weilin Zhao, Zhihui He, Qi-An Chen, Huarong Zhou, Zhensheng Zou, Haoye Zhang, Shengding Hu, Zhi Zheng, Jie Zhou, Jie Cai, Xu Han, Guoyang Zeng, Dahai Li, Zhiyuan Liu, and Maosong Sun. Minicpm-v: gpt-4v level mllm on your phone. ArXiv, 2024. [50] Weihao Yu, Zhengyuan Yang, Linjie Li, Jianfeng Wang, Kevin Lin, Zicheng Liu, Xinchao Wang, and Lijuan Wang. Mm-vet: evaluating large multimodal models for integrated capabilities. In Proceedings of the 41st International Conference on Machine Learning, 2024. [51] Xiang Yue, Yuansheng Ni, Kai Zhang, Tianyu Zheng, Ruoqi Liu, Ge Zhang, Samuel Stevens, Dongfu Jiang, Weiming Ren, Yuxuan Sun, Cong Wei, Botao Yu, Ruibin Yuan, Renliang Sun, Ming Yin, Boyuan Zheng, Zhenzhu Yang, Yibo Liu, Wenhao Huang, Huan Sun, Yu Su, and Wenhu Chen. Mmmu: massive multi-discipline multimodal understanding and reasoning benchmark for expert agi. 2024 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), 2023. [52] Xiang Yue, Tianyu Zheng, Yuansheng Ni, Yubo Wang, Kai Zhang, Shengbang Tong, Yuxuan Sun, Ming Yin, Botao Yu, Ge Zhang, Huan Sun, Yu Su, Wenhu Chen, and Graham Neubig. Mmmu-pro: more robust multi-discipline multimodal understanding benchmark. In Annual Meeting of the Association for Computational Linguistics, 2024. 13 [53] Cheng Zhang, Judith Bütepage, Hedvig Kjellström, and Stephan Mandt. Advances in variational inference. IEEE Transactions on Pattern Analysis and Machine Intelligence, 41:20082026, 2017. [54] Dinghuai Zhang, Nikolay Malkin, Zhen Liu, Alexandra Volokhova, Aaron Courville, and Yoshua Bengio. Generative flow networks for discrete probabilistic modeling. In International Conference on Machine Learning, pages 2641226428. PMLR, 2022. [55] Renrui Zhang, Dongzhi Jiang, Yichi Zhang, Haokun Lin, Ziyu Guo, Pengshuo Qiu, Aojun Zhou, Pan Lu, Kai-Wei Chang, Peng Gao, and Hongsheng Li. Mathverse: Does your multi-modal llm truly see the diagrams in visual math problems? In European Conference on Computer Vision, 2024. [56] Zhuosheng Zhang, Aston Zhang, Mu Li, Hai Zhao, George Karypis, and Alex Smola. Multimodal chain-of-thought reasoning in language models. arXiv preprint arXiv:2302.00923, 2023. [57] Yiyang Zhou, Zhiyuan Fan, Dongjie Cheng, Sihan Yang, Zhaorun Chen, Chenhang Cui, Xiyao Wang, Yun Li, Linjun Zhang, and Huaxiu Yao. Calibrated self-rewarding vision language models. ArXiv, 2024. [58] Daniel Ziegler, Nisan Stiennon, Jeffrey Wu, Tom Brown, Alec Radford, Dario Amodei, Paul Christiano, and Geoffrey Irving. Fine-tuning language models from human preferences, 2020. URL https://arxiv. org/abs, page 14, 1909. Proof of Proposition 1 Proof. (a) Assume that within the segment {t, + 1, . . . , + λ} the true reward grows linearly, i.e. R(z1:t+i) = R(z1:t) + , := R(z1:t+λ) R(z1:t) λ , 0 λ. Substituting this expression into Eq. (4) shows R(z1:t+i) = R(z1:t+i) for every i, so the interpolation incurs zero error. (b) Suppose is twicedifferentiable along the trajectory and its discrete second derivative is bounded: (cid:12)R(z1:s+1) 2R(z1:s) + R(z1:s1)(cid:12) (cid:12) (cid:12) M, s. The classical linearinterpolation error bound then yields (cid:12) R(z1:t+i) R(z1:t+i)(cid:12) (cid:12) (cid:12) i(λ i) λ2 8 , 0 λ. (10) Thus the approximation error decays as O(λ2); choosing λ sufficiently small keeps it arbitrarily close to 0. Let (zs) := R(z1:s) qθ( z1:s) , (zs) := R(z1:s) qθ( z1:s) , εs := R(z1:s) R(z1:s). From Eq. (10) we have εs λ2 8 , so (zs) = (zs) (cid:16) 1 + εs R(z1:s) (cid:17) . := minst+λ R(z1:s) > 0 (positivity follows from likelihoods). Denote Rmin (cid:12)εs/R(z1:s)(cid:12) (cid:12) Consider any < + λ. Applying Eq. (1) to both and , we obtain (cid:12) λ2 8Rmin . Then (zi) (cid:89) k=i+1 PF (zk zk1) = (zj) (cid:89) k=i+1 PB(zk1 zk) (cid:16) 1 + O(λ2) (cid:17) , 14 Figure F9: Qualitative results of visual reasoning. We highlight the important reasoning steps. where the O(λ2) term accumulates at most (j i) relative perturbations bounded by λ2/(8Rmin). Hence, the forward and backward product flows still match up to multiplicative factor that vanishes quadratically as λ 0. Therefore, the interpolated rewards preserve flow consistency to arbitrary precision for sufficiently small segment lengths."
        },
        {
            "title": "B Experiments",
            "content": "B.1 Qualitative results In Fig. F9, we provide qualitative results of comparison between Qwen2.5-VL-7B (SFT), Qwen2.5VL-7B (GRPO), and LaCoT-Qwen-7B. As can be seen, LaCoT-Qwen-7B can provide more accurate reasoning chain, leading to the correct answer. Meanwhile, due to limited generalizability, SFT and GRPO samples show the wrong visual CoT. In Fig. F10 and Fig. F11, our LaCoT model can sample more straightforward and accurate reasoning chains, demonstrating the effectiveness and robustness of the proposed training and inference algorithm. B.2 Study of interpolation reward In Table T5, we study the impact of the interpolation reward with different skipped steps (i.e., λ) in the reward approximation process of the policy optimization. As mentioned in Proposition 1, smaller λ theoretically leads to more fine-grained reward supervision but longer training time. B.3 Efficiency analysis Give superior performance gain by sampling multiple rationales at inference time, but this process introduces additional inference cost, and we address this by using mini-batching (with batch size k=5) to generate rationales in N/k forward passes. In Table T6, we report the average per-sample inference time (reasoning + answering) and corresponding performance of different reasoning-LVLM on MathVista and MathVerse. As can be seen, LaCoT-Qwen-7B consistently achieves stronger 15 Figure F10: Qualitative results of visual reasoning. We highlight the important reasoning steps. Table T5: Study the impact of the interpolation reward with different skipping steps (i.e., λ) to the policy model. Method Qwen2.5-VL-7B - LaCoT-Qwen-7B 32 LaCoT-Qwen-7B λ MathVista MathVision MathVerse MMMU Overall mini 63.7 64.9 68.4 full 25. 23.0 24.9 vision-only 38.2 42.5 39.7 val 50. 51.9 54.9 Avg. 44.3 45.6 47.0 performance, even with modest increases in inference time. Compared to other multi-rationale baselines, LaCoT strikes favorable balance between computational cost and reasoning reliability, thereby improving both the trustworthiness of rationales and the accuracy of final answers. B.4 Experiments compute resources This work utilizes an 8*80GB GPU-node for training. We set the Deepspeed Zero-3 stage and gradient-checkpointing to reduce memory costs during optimization. It takes around 30 hours for supervised fine-tuning on 250k reasoning data samples, and 120 hours for GRPO and RGFN fine-tuning on 3k data samples. 16 Figure F11: Qualitative results of visual reasoning. We highlight the important reasoning steps. B.5 Hyperparameter We detail the hyperparameters used for training the reward model and LaCoT in our experiments in Table T7. During LaCoT training, we randomly sample (mini-batch size) Zs for every (X, ) as exploration. Table T6: Inference time study of reasoning model with multiple rational sampling. #Rationals (N)"
        },
        {
            "title": "10 MathVista MathVerse",
            "content": "LLaVA-CoT-11B R1-OneVision-7B 32s LaCoT-Qwen-7B - - 340s - 30s 830s - 65s 52.5 64.1 68. 22.6 37.8 39.7 Table T7: Hyperparameters for training. LoRA dropout Batch size (SFT) Batch size (RGFN) Gradient accumulation (SFT) Learning rate Optimizer Weight decay Temperature max Temperature min Reward temperature start Reward temperature end Reward temperature horizon exploration number λ τmax τmin Maximum rationale length Minimum rationale length 0.05 2 1 16 0.00001 AdamW 0.05 1.0 0.5 1.0 0.7 50 6 8 1.5 1.0"
        },
        {
            "title": "NeurIPS Paper Checklist",
            "content": "1. Claims Question: Do the main claims made in the abstract and introduction accurately reflect the papers contributions and scope? Answer: [Yes] Justification: The empirical results and theory analysis support our claims in the abstract and introduction. Guidelines: The answer NA means that the abstract and introduction do not include the claims made in the paper. The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. No or NA answer to this question will not be perceived well by the reviewers. The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings. It is fine to include aspirationale goals as motivation as long as it is clear that these goals are not attained by the paper. 2. Limitations Question: Does the paper discuss the limitations of the work performed by the authors? Answer: [Yes] Justification: We provide limitation discussion in section 4.2 (Results) and conclsion. Guidelines: The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper. 18 The authors are encouraged to create separate \"Limitations\" section in their paper. The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should reflect on how these assumptions might be violated in practice and what the implications would be. The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on few datasets or with few runs. In general, empirical results often depend on implicit assumptions, which should be articulated. The authors should reflect on the factors that influence the performance of the approach. For example, facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon. The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size. If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness. While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, worse outcome might be that reviewers discover limitations that arent acknowledged in the paper. The authors should use their best judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations. 3. Theory assumptions and proofs Question: For each theoretical result, does the paper provide the full set of assumptions and complete (and correct) proof? Answer: [Yes] Justification: This paper has one Proposition, and we provide the complete proof in the supplementary. Guidelines: The answer NA means that the paper does not include theoretical results. All the theorems, formulas, and proofs in the paper should be numbered and crossreferenced. All assumptions should be clearly stated or referenced in the statement of any theorems. The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide short proof sketch to provide intuition. Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material. Theorems and Lemmas that the proof relies upon should be properly referenced. 4. Experimental result reproducibility Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? Answer: [Yes] Justification: We have provided details for reproducing the results in section Experimental Results. Guidelines: The answer NA means that the paper does not include experiments. If the paper includes experiments, No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not. 19 If the contribution is dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable. Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is novel architecture, describing the architecture fully might suffice, or if the contribution is specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to hosted model (e.g., in the case of large language model), releasing of model checkpoint, or other means that are appropriate to the research performed. While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example (a) If the contribution is primarily new algorithm, the paper should make it clear how to reproduce that algorithm. (b) If the contribution is primarily new model architecture, the paper should describe the architecture clearly and fully. (c) If the contribution is new model (e.g., large language model), then there should either be way to access this model for reproducing the results or way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset). (d) We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results. 5. Open access to data and code Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material? Answer: [Yes] Justification: We provide the instructions of training data in Section 4.1. We will release our training code later. Guidelines: The answer NA means that paper does not include experiments requiring code. Please see the NeurIPS code and data submission guidelines (https://nips.cc/ public/guides/CodeSubmissionPolicy) for more details. While we encourage the release of code and data, we understand that this might not be possible, so No is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for new open-source benchmark). The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines (https: //nips.cc/public/guides/CodeSubmissionPolicy) for more details. The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc. The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only subset of experiments are reproducible, they should state which ones are omitted from the script and why. At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable). Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted. 6. Experimental setting/details 20 Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? Answer: [Yes] Justification: We provide the instructions of training data, hyperparameters, model architecture in Section 4.1 and supplementary. Guidelines: The answer NA means that the paper does not include experiments. The experimental setting should be presented in the core of the paper to level of detail that is necessary to appreciate the results and make sense of them. The full details can be provided either with the code, in appendix, or as supplemental material. 7. Experiment statistical significance Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? Answer: [No] Justification: We have reported the results of different random temperature. However, we did not report error bars since it will be too computationally expensive. Guidelines: The answer NA means that the paper does not include experiments. The authors should answer \"Yes\" if the results are accompanied by error bars, confidence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper. The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions). The method for calculating the error bars should be explained (closed form formula, call to library function, bootstrap, etc.) The assumptions made should be given (e.g., Normally distributed errors). It should be clear whether the error bar is the standard deviation or the standard error of the mean. It is OK to report 1-sigma error bars, but one should state it. The authors should preferably report 2-sigma error bar than state that they have 96% CI, if the hypothesis of Normality of errors is not verified. For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g. negative error rates). If error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text. 8. Experiments compute resources Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? Answer: [Yes] Justification: We provide detailed information in supplementary. Guidelines: The answer NA means that the paper does not include experiments. The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage. The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute. 21 The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didnt make it into the paper). 9. Code of ethics Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines? Answer: [Yes] Justification: We follow Code of Ethics precisely. Guidelines: The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics. If the authors answer No, they should explain the special circumstances that require deviation from the Code of Ethics. The authors should make sure to preserve anonymity (e.g., if there is special consideration due to laws or regulations in their jurisdiction). 10. Broader impacts Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? Answer: [Yes] Justification: We provide the discussion of potential impact in Conclusion. Guidelines: The answer NA means that there is no societal impact of the work performed. If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact. Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations. The conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments. However, if there is direct path to any negative applications, the authors should point it out. For example, it is legitimate to point out that an improvement in the quality of generative models could be used to generate deepfakes for disinformation. On the other hand, it is not needed to point out that generic algorithm for optimizing neural networks could enable people to train models that generate Deepfakes faster. The authors should consider possible harms that could arise when the technology is being used as intended and functioning correctly, harms that could arise when the technology is being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology. If there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how system learns from feedback over time, improving the efficiency and accessibility of ML). 11. Safeguards Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? Answer: [NA] Justification: We use open-source data and model in this work. Guidelines: The answer NA means that the paper poses no such risks. 22 Released models that have high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to access the model or implementing safety filters. Datasets that have been scraped from the Internet could pose safety risks. The authors should describe how they avoided releasing unsafe images. We recognize that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make best faith effort. 12. Licenses for existing assets Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? Answer: [NA] Justification: Guidelines: The answer NA means that the paper does not use existing assets. The authors should cite the original paper that produced the code package or dataset. The authors should state which version of the asset is used and, if possible, include URL. The name of the license (e.g., CC-BY 4.0) should be included for each asset. For scraped data from particular source (e.g., website), the copyright and terms of service of that source should be provided. If assets are released, the license, copyright information, and terms of use in the package should be provided. For popular datasets, paperswithcode.com/datasets has curated licenses for some datasets. Their licensing guide can help determine the license of dataset. For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided. If this information is not available online, the authors are encouraged to reach out to the assets creators. 13. New assets Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? Answer: [NA] Justification: Guidelines: The answer NA means that the paper does not release new assets. Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates. This includes details about training, license, limitations, etc. The paper should discuss whether and how consent was obtained from people whose asset is used. At submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip file. 14. Crowdsourcing and research with human subjects Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? Answer: [NA] Justification: 23 Guidelines: The answer NA means that the paper does not involve crowdsourcing nor research with human subjects. Including this information in the supplemental material is fine, but if the main contribution of the paper involves human subjects, then as much detail as possible should be included in the main paper. According to the NeurIPS Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector. 15. Institutional review board (IRB) approvals or equivalent for research with human subjects Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained? Answer: [NA] Justification: Guidelines: The answer NA means that the paper does not involve crowdsourcing nor research with human subjects. Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper. We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the guidelines for their institution. For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review. 16. Declaration of LLM usage Question: Does the paper describe the usage of LLMs if it is an important, original, or non-standard component of the core methods in this research? Note that if the LLM is used only for writing, editing, or formatting purposes and does not impact the core methodology, scientific rigorousness, or originality of the research, declaration is not required. Answer: [NA] Justification: Guidelines: The answer NA means that the core method development in this research does not involve LLMs as any important, original, or non-standard components. Please refer to our LLM policy (https://neurips.cc/Conferences/2025/LLM) for what should or should not be described."
        }
    ],
    "affiliations": [
        "DEVCOM Army Research Laboratory",
        "Rochester Institute of Technology",
        "Snap Inc.",
        "University of Rochester"
    ]
}