{
    "paper_title": "VGG-T$^3$: Offline Feed-Forward 3D Reconstruction at Scale",
    "authors": [
        "Sven Elflein",
        "Ruilong Li",
        "Sérgio Agostinho",
        "Zan Gojcic",
        "Laura Leal-Taixé",
        "Qunjie Zhou",
        "Aljosa Osep"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "We present a scalable 3D reconstruction model that addresses a critical limitation in offline feed-forward methods: their computational and memory requirements grow quadratically w.r.t. the number of input images. Our approach is built on the key insight that this bottleneck stems from the varying-length Key-Value (KV) space representation of scene geometry, which we distill into a fixed-size Multi-Layer Perceptron (MLP) via test-time training. VGG-T$^3$ (Visual Geometry Grounded Test Time Training) scales linearly w.r.t. the number of input views, similar to online models, and reconstructs a $1k$ image collection in just $54$ seconds, achieving a $11.6\\times$ speed-up over baselines that rely on softmax attention. Since our method retains global scene aggregation capability, our point map reconstruction error outperforming other linear-time methods by large margins. Finally, we demonstrate visual localization capabilities of our model by querying the scene representation with unseen images."
        },
        {
            "title": "Start",
            "content": "VGG-T3: Offline Feed-Forward 3D Reconstruction at Scale Sven Elflein1,2,3 Ruilong Li1 Sergio Agostinho1 Zan Gojcic1 Laura Leal-Taixe1 Qunjie Zhou1 Aljosa Osep1 1NVIDIA 2Vector Institute 3University of Toronto 6 2 0 2 6 2 ] . [ 1 1 6 3 3 2 . 2 0 6 2 : r (a) Reconstructing of Rome landmarks: Colosseum, Castel SantAngelo, Pantheon and Trevi fountain. (b) Num. images vs. inference time. Figure 1. Reconstructing Rome landmarks with 1-minute time budget. We present VGG-T3, an offline feed-forward 3D reconstruction method that scales linearly w.r.t. input views (Fig. 1b). As result, we can reconstruct large scenes from large number of unposed input views, such as landmarks from tourist-sourced images, in less than minute via single forward pass (Fig. 1a)."
        },
        {
            "title": "Abstract",
            "content": "1. Introduction We present scalable 3D reconstruction model that addresses critical limitation in offline feed-forward methods: their computational and memory requirements grow quadratically w.r.t. the number of input images. Our approach is built on the key insight that this bottleneck stems from the varying-length Key-Value (KV) space representation of scene geometry, which we distill into fixed-size Multi-Layer Perceptron (MLP) via test-time training. Our VGG-T3 (Visual Geometry Grounded Test Time Training) scales linearly w.r.t. the number of input views, similar to online models, and reconstructs 1k image collection in just 54 seconds, achieving 11.6 speed-up over baselines that rely on softmax attention. Since our method retains global scene aggregation capability, our point map reconstruction error outperforming other linear-time methods by large margins. Finally, we demonstrate visual localization capabilities of our model by querying the scene representation with unseen images. We tackle large-scale 3D geometry reconstruction from inthe-wild image collections (Fig. 1a). Status quo. Contemporary learning-based approaches directly predict scene geometry from images via feed-forward networks [27, 51, 95, 97, 99, 113]. These are on-par with classical methods [67, 76, 81] in terms of accuracy, and are empirically more robust under challenging conditions such as rapid camera motion and low visual overlap. However, their computational and memory requirements scale poorly with the number of input images. This bottleneck originates from the implicit scene-level memory stored in the Key-Value (KV) space of the global self-attention layer. This KV space, projected from all input image tokens, functions as the dense, variable-length scene representation queried for 3D attribute prediction. To estimate scene geometry from this latent representation, these models need to query the KV space via global softmax attention operations. As this operation scales quadratically w.r.t. the number of input images, recent techniques address this issue via sparse attention [92] or token merging [79] to compress the variable representation length. However, this does not change the underlying quadratic scaling w.r.t. the number of input images (Fig. 1b). image sets as well as efficient distributed inference. Finally, we (iv) present proof-of-concept joint feed-forward visual localization and mapping within single model. Compress your KV. variable-length representation is in contrast to methods that represent the scene geometry via fixed-size implicit representations [65, 69]. For example, DeepSDF [69] conditions pre-trained decoder on compact, test-time-optimizable latent code to reconstruct specific shape conditioned on the observed input 3D point cloud. Intuitively, the fixed-state decoder learns rich geometric priors, while small latent code encodes instancespecific details through test-time optimization. In this work, we revisit this core principle in the context of feed-forward multi-view 3D reconstruction. In particular, we leverage pre-trained multi-view feedforward model [95] that tokenizes multi-view images and decodes dense depth maps from output tokens. However, rather than performing (quadratic) softmax attention in the global attention layer, we follow Sun et al. [88] and map the KV space via weights of fixed-size MLP. Analogous to DeepSDF, we optimize the MLP at test time with reconstruction loss in token space, which allows us to retain the pre-trained encoder/decoder network. Querying the KV space at test time to decode depth maps from input views now means only applying the learned MLP to input tokens. This operation is linear w.r.t. the input image collection size. Large-scale feed-forward reconstruction. With our approach we can perform mini-batching to compute the overall gradient of our test-time training objective. This implies we can (i) process large image collections on single GPU by off-loading mini-batches to CPU, and (ii) perform distributed inference by sharding image tokens across multiple GPUs. As result, we can process 2k image collection in 48.5s, 33 improvement over VGGT (27min). Visual localization. Moreover, this representation change also unlocks new capabilities. After reconstructing set of images, the optimized MLP stores compressed version of the scene. By querying the frozen MLP with novel query view, we localize this image with respect to the reconstructed scene, thus naturally performing feed-forward visual localization. Traditionally, separate solutions are required for reconstruction and localization tasks. In contrast, our approach uses the same model for mapping (optimizing the MLP) and localization (querying the frozen MLP), providing unified, end-to-end solution. To summarize, (i) we propose an offline feed-forward 3D reconstruction model that scales linearly w.r.t. the number of input views. We (ii) show that models that represent scene geometry with variable-length implicit representation (KV) can be converted into linear-time models via fixeddimensional implicit state representation. We (iii) demonstrate our approach supports single-GPU inference for large 2. Related Work"
        },
        {
            "title": "Recent",
            "content": "Classical pipelines. Established structure-from-motion techniques such as Bundler [81, 82], COLMAP [76], and GLOMAP [67] follow multi-stage pipeline that includes feature extraction, correspondence search, camera pose estimation, and joint refinement of camera poses and 3D structure. These methods achieve accurate scene reconstructions on large image collections [1], provided the scenes are wellconstrained (i.e., sufficient visual overlap and connectivity). Feed-forward models. feed-forward approaches [42, 55, 97] utilize Transformers [91] to encode input image pairs and regress relative pose and depth maps in the reference view. By encoding spatial relationships via attention mechanisms across image features, these methods can recover 3D geometry, camera motion, and even handle scenes with uncalibrated cameras and low visual overlap. Multi-view feed-forward methods encode and aggregate features across multiple views to predict poses and scene geometry simultaneously. Overall, these methods consist of feature encoder (tokenizer), multi-view feature aggregator, and decoder that estimates camera poses and per-view depth maps or global point maps. VGGT [95], Fast3R [103], and π3 [99] perform this global fusion in token space via softmax attention. Alternatively, Light3RSfM [28] constructs scene graph from the underlying image collection and pools image features using shortestpath tree data structure for more efficient aggregation. FLARE [113] decomposes the problem into global camera pose and per-view geometry estimation. Large-scale reconstruction. The aforementioned offline multi-view models achieve high accuracy via global selfattention mechanisms, at the cost of quadratic complexity O(n2) w.r.t. the number of input views n. To enable reconstruction with long sequences, Slam3R [60], VGGT-SLAM [62], and VGGT-Long [24] process video data in chunks, using local attention or sliding windows. However, this decouples the global scene state, making them prone to drift and unsuitable for unordered image sets. Other methods optimize the global attention operation directly. FastVGGT [79] uses token merging, and SparseVGGT [92] employs block-sparse attention. While reducing the constant factor 2) where is the token down-sampling raO(n2) O(n/r tio, the asymptotic complexity of both approaches remains quadratic. These can be viewed as structured compression of the KV scene representation to speed up the global attention operation, with the heuristic that tokens close in image space share similar scene features. From this perspective, our work applies flexible compression of the KV space, de2 coupling the models computational complexity from the number of input images n, thus moving from quadratic to linear-time formulation. Online methods. Several methods process image sequences in an auto-regressive fashion. StreamVGGT [116] and Stream3R [53] convert pre-trained VGGT models to causal models, registering newly-observed images into the existing reconstruction, by only attending to prior tokens keys and values. These methods scale quadratically w.r.t. and require memory-intensive KV caching to accelerate inference. Online, linear-time models retain past frames/tokens as working memory [93], rely on fixed-size implicit memory that is updated iteratively (CUT3R [96], Must3R [15]) or use explicit spatial memory (Point3R [101], Long3R [17], MapAnything [51]). Test-time training for 3D reconstruction. Concurrent to our work, TTT3R [16] is an auto-regressive model that builds on the fixed-size memory of CUT3R and reinterprets its internal state update mechanism as test-time training (TTT) [86]. Our work utilizes similar test-time optimization mechanism, but with fundamentally different interpretation: we resort to test-time optimization to compress the KV space into fixed-size MLP. Therefore, our method is global (offline) and, as we show empirically, significantly more accurate compared to TTT3R, yet maintains linear complexity w.r.t. input size n. Attention with linear complexity. The quadratic cost of softmax attention limits scalability in long-sequence modeling. Linear attention methods address this by replacing the softmax kernel with linear feature maps, yielding linear-time, constant-memory recurrences [46, 49], with gated [105] and chunkwise-parallel [20, 40, 87, 104, 106] extensions improving efficiency and hardware throughput. State-space models (SSMs) offer an alternative recurrent formulation, where modern variants such as S4 [35], H3 [30], Hyena [70], and Mamba [20, 34] learn structured transitions to capture global dependencies, and can be viewed as gated or structured extensions of linear attention [8, 88, 107]. Recent work shows that TTT provides strictly more general framework: it treats the hidden state as an optimization variable updated online [8, 19, 88], recovering linear attention and SSMs as special cases while improving adaptability across domains such as video modeling [19], novel view synthesis [114], and continual learning [88]. complementary line of work in LLMs explores post-training linearization, converting pretrained transformers into linear-complexity models via lightweight adaptation or distillation [20, 48, 64, 94, 111, 112]. Building on these advances, we extend post-training linearization to multi-view reconstruction, introducing TTT-based approach that scales bi-directional models to an unbounded number of views. Visual localization. The task of localizing novel query image relative to pre-built scene representation is typically achieved via geometric correspondence search [3, 9, 37, 68, 74, 75, 115], followed by Perspective-n-Point (PnP) solver [50, 52] to compute the final camera pose. Similar to ours, Scene Coordinate Regression (SCR) [1012, 80] methods learn scene-specific function that directly maps input RGB pixels to 3D world coordinates, thereby bypassing the need for explicit feature matching or database queries. Recent ACEZero [12] maps and localizes input views jointly from unposed images, however, this end-toend approach critically relies on extensive, iterative optimization steps to converge toward stable 3D reconstruction. Our approach instead leverages pre-trained, feedforward 3D reconstruction model, which directly enables mapping and localization at test-time with only few iterations of optimization in token space. 3. Feed-Forward 3D Reconstruction at Scale We begin by reviewing the recent multi-view feed-forward architecture VGGT [95] and the test-time training (TTT) techniques in Sec. 3.1. key observation is that VGGT implicitly relies on the variable-length keyvalue (KV) pairs produced by attention layers as its internal representation of scene geometry. While effective, this design requires O(n2) compute and linearly growing memory with respect to the number of input views. In Sec. 3.2, we introduce our approach, VGG-T3 (Visual Geometry Grounded Test Time Training), which replaces these variable-length KV pairs with compressed, fixed-size MLP representation via TTT. This substitution reduces the computational complexity to O(n), enabling feed-forward 3D reconstruction at scale. Task. Given an unordered image collection, denoted as i=1 with Ii RHW 3, the goal is to obtain per- {Ii}N image camera extrinsics Pi consisting of rotation SE(3) and translation vector R3, pinhole intrinsics Ki R33 and dense depth map RHW , which represents the geometry observed by individual images. 3.1. Preliminaries VGGT. VGGT performs multi-view reasoning by first applying an image tokenizer that converts each input image into sequence of tokens xi. It then processes these tokens with attention blocks that alternate between imagewise self-attention and global self-attention across all images (Fig. 2a). In each attention block, the model projects every input token xi into query, key, and value (QKV) vectors: qi = LNq(Wqxi), ki = LNk(Wkxi), vi = Wvxi. (1) where Wq, Wk and Wv are learned linear projections and LNq and LNk denote layer norms [7] performing QK nor3 (a) VGGT (b) TTT-based global attention replacement with linear scaling. Figure 2. VGG-T3 replaces the global attention block in VGGT (left) with linear-time alternative based on test-time training (right) to compress the KV space into fixed-size MLP. We use 3 images for visualization purposes but this scales to arbitrary number of images. malization [23, 38] to stabilize training. The softmax attention is then applied to obtain per-head output oi via: oi = (cid:88) softmaxj (cid:19) (cid:18) qT kj vj. (2) Finally, prediction heads operate on the output tokens oi to directly predict per-image depth, camera poses, and camera intrinsics. Importantly, the global self-attention layers pool information across all input views, which is essential for multi-view understanding but introduces quadratic complexity w.r.t. the number of views. Test-time training. Recently, Sun et al. [88] propose to use test-time training [86] in which just small set of weights θ (referred to as Fast weights [39]) are updated at test-time using self-supervised objective Lt. Given queries qi, keys ki, and values vi, Sun et al. [88] re-define the attention operation as: arg min θ (cid:88) (cid:0)Tθ(ki) vi (cid:1), Lt oi = Tθ(qi). (3) (4) Intuitively, this optimization embeds the mapping from keys ki to values vi into learnable network Tθ. Once done, this network can retrieve the appropriate value for given query qi, analogous to how softmax attention uses QK cosine similarity to retrieve information stored in (Eq. (2)). Unlike softmax attention, however, both operations in Eq. (3) and Eq. (4) are linear with respect to the sequence length. 3.2. Can We Fit Rome into MLPs? The central challenge in modern multi-view 3D reconstruction is achieving scalability, which is fundamentally tied to scene representation and its corresponding complexity as the number of input images, n, grows. Quadratic complexity of VGGT is direct consequence of variable-length KV scene representation, as extracting information from the KV space scales quadratically w.r.t. in the softmax attention [91] operation, necessary to obtain the output token representation. This brings us to the core question: can we bypass the softmax attention operation in KV space? Overview. Our method structurally replaces the quadratic global attention operation within the bi-directional TransOnce we former architecture with linear alternative. projected the multi-view input to tokens using Transformerbased multi-view networks [95] the forward pass consists of two recurring stages, executed in each global attention layer (Fig. 2b): (1) Update: We project input tokens to queries, keys and values and use TTT [88] to compress the variablelength information stored in KV into the fixed-size, compact weights of an MLP. We treat the MLPs as fast weights, i.e., weights that are optimized at train and test time. This effectively compresses the key-value mapping of the current layer into fixed-size neural scene representation. (2) Apply: After optimizing θ, we can query the scene representation efficiently by applying the MLP to queries q. We only apply the MLP in global attention blocks of current layers queries to obtain updated tokens before they are passed to the next layer. Decoding to downstream tasks (per-view depth, ego-pose Pi and camera intrinsics) only occurs after the final Transformer layer. Linearizing the pre-trained model. We aim to initialize our model using pre-trained weights of VGGT [95], including projection matrices Wq, Wk and Wv as these already capture general vision knowledge learned by the original model. Such linearization is commonly employed in the context of large-language models [64] and significantly reduces training cost. However, we find empirically that naively applying test-time linearization to replace Softmax Attention (Eq. (2)) with linear-time operation (Eq. (3)) yields very slow convergence during test-time training. 4 As can be seen in Eq. (1), token projection involves LayerNorm (LN), which stabilizes Softmax Attention operation in the original model [38]. However, LayerNorm involves additional learnable parameters that distort the input space that the MLP is trying to learn at test time. By removing LN and instead applying L2 normalization we unlock fast convergence from pre-trained weights. Moreover, we show that regular softmax attention training followed by our posttraining with linearization approach is preferable over directly training from scratch using test-time training. Non-linear spatial mixing. While linear attention variants significantly speed up Transformer models, this is generally accompanied by drop in downstream metrics compared to softmax attention [34, 88, 107]. We attribute this drop in our framework to the inherent mathematical constraints of the TTT objective in Eq. (3). Recall that we are learning mapping from Key to Value space . However, both and are derived from same token via linear projections = Wkx and = Wvx, and the relationship between them is linear (V = WvW 1 K, assuming Wk is invertible). Therefore, simply optimizing Eq. (3) can yield trivial solution. To break this dependency and enhance expressivity, we are inspired by the success of sequence mixing layers such as short convolutions [70], effectively utilized in linear language models [106, 107]. We adapt this principle for 3D reconstruction by applying spatial mixing in the Value (V) space, which we term ShortConv2D, forcing the TTT objective to learn mapping from . We implement this as follows: 1. Reshape: Given the values [vi, . . . , vN H/pW/p], vi Rd, we first reshape the 1D token sequence back to its corresponding 2D image grid of shape (N, H/p, W/p, d), where is the tokenizer patch size. 2. Convolve: We apply single-layer 2D convolution, ShortConv2D, which is more suitable for image structures than the 1D convolutions typically used in language modeling. This aggregates local neighborhood information to create the context-aware target V. 3. Flatten: is reshaped back to 1D sequence before optimizing the TTT objective Eq. (3). Intuitively, by applying 2D convolution, the Value for token now contains aggregated local spatial context, while the Key remains context-limited. This incentivizes the fast weights optimization (Eq. (3)) to distill robust geometric scene representation via stronger self-supervised objective as the MLP must now predict neighborhood from single tokens feature K. Concurrently, ViT3 [36] successfully employs convolutions directly in the inner model for the classification task. Test-time scaling. While feed-forward models train on relatively small image collections (up to 24 in VGGT [95]), our goal is to process significantly larger image collections containing thousands of images, common requirement in (a) Best number of optimizer step of test-time training objective for two sizes of image collections. (b) Pointmap prediction error for different number of input images (lower is better). Figure 3. Sequence-length generalization analysis. large-scale structure-from-motion [100]. While sequence length generalization was studied in the context of softmax attention [44], in the test-time training setting, we observe large degradation when processing out-of-distribution sequence lengths. For example, the reconstruction error increases about 5 when extending from = 100 to = 1k images of the same scene. We hypothesize that the fixed number of optimization steps used during the training (typically one) is insufficient to compress significantly larger scenes to fixed-dimensional MLP. To confirm, we log the top-performing optimizer step of the TTT objective (Eq. (3)) across two scales: 20 images (in-distribution) and 1k images (out-distribution, 50 increase). As can be seen in Fig. 3a, for in-distribution samples, one step is sufficient, while for 1k images, it is beneficial to increase number of optimizer steps. Simply performing more optimizer steps, we achieve almost constant scaling to arbitrary sequence lengths, showing form of test-time scaling via additional computation [22]. As increasing the number of steps further does not aid reconstruction quality we perform 2 steps unless otherwise noted. 3.3. Large-scale Reconstruction We discuss the implications of our scene representation change in the following. Scalability. Comparing Eq. (2) with Eq. (3), we first note that the complexity of the operations changes from O(n2) to O(n), resolving the quadratic bottleneck within the global attention layers found in existing reconstruction models. Flexible inference strategies. Moreover, this change unlocks inference strategies that enable us to process arbitrarily large image collections on single GPU and accelerate throughput linearly on multiple GPUs via distributed inference by applying TTT in minibatch fashion. Recall that TTT optimization learns the MLP weights θ to map local features , and the loss function (Eq. (3)) is sum over all input tokens i. As the overall optimization objective is simply sum of local losses, the total gradient of the loss w.r.t. the MLP weights θ is also sum of local gradients (Eq. (5)): dLtotal dθ (cid:88) = dθ L(ki, vi) = (cid:32) (cid:88) (cid:88) is dθ (cid:33) L(ki, vi) . (5) As noted in Zhang et al. [114], this implies we can compute the gradient of the objective on minibatches independently. For distributed inference, we can process minibatches on individual GPUs, and synchronize gradients. This enables efficient training in cases where the sequence does not fit into memory of single GPU. In practice we shard images such that each GPU only processes subset s. In the global layers, we then use Eq. (5) to synchronize the MLP weights across GPUs by performing all-to-all communication which is efficient due to their small size. Moreover, this property also allows processing arbitrarily large image collections on single GPU. For this, we off-load minibatches (Eq. (5)) to host memory (instead of distributing across GPUs). We can then compute the update for the entire sequence by loading minibatch at time to device memory, compute the gradient, and off-load minibatch back to host memory. This requires keeping only single minibatch in device memory at time. Note that methods relying on softmax attention (e.g., VGGT [95] and its sparse variants [79, 92] using FlashAttention [21]) require qi, ki, vi of all images to be in GPU memory which, even for large GPUs, leads quickly to out-of-memory errors when processing larger image collections. Query-able reconstruction & visual localization. After processing set of images representing scene, our network can be queried with new observations. It outputs scene geometry and camera pose of the new image relative to the existing reconstruction. To do so, we keep test-time optimized weights frozen, and run standard forward pass for new query image, with one key modification: in the global attention layers, we only apply the frozen MLPs to the query features qi to retrieve information from the scene representation, without updating the MLP parameters θ. This effectively transforms the model into single-image Transformer for query processing. We show in Sec. 4.3 that this querying mechanism enables us to perform visual localization. 4. Experiments In this section, we compare our VGG-T3 to state-of-the-art offline and online baselines on standard tasks and benchmarks, examining accuracy vs. runtime in both the conventional setting (Sec. 4.1) and large-scale regime (Sec. 4.2). We further demonstrate that our approach enables accurate feed-forward 3D visual localization in unposed, in-the-wild image collections (Sec. 4.3). Finally, we present ablation studies that validate our design choices in Sec. 4.4. Implementation details. We start from the public VGGT checkpoint and convert it to linearized model by replacing all global attention layers with TTT layers. Following LaCT [114], our TTT layer uses SwiGLU MLP [78] to learn the mapping, Muon [45] for optimization, and dot product loss Lt(Tθ(ki), vi) = Tθ(ki)T vi. After all the QKV projection layer, we additionally apply an 3 3 ShortConv2D on for non-linear spatial mixing. We freeze all original VGGT parameters and fine-tune only global attention layers using dataset comparable to VGGTs original training data, running for 100k steps on 8 NVIDIA A100-80GB ( 12% of the cost of training VGGT from scratch). For more details we refer to the appendix. Baselines. We compare our approach with both offline and online reconstruction methods. On the offline side, we include VGGT [95] as an upper bound in reconstruction accuracy, along with efficient variants such as FastVGGT [79] and SparseVGGT [92], all of which exhibit quadratic complexity with respect to the number of input views. On the online side, we benchmark against TTT3R [16], concurrent method that improves upon CUT3R [96] and is designed for ordered input sequences with linear complexity. We carefully analyze the accuracyscalability trade-offs of these baselines alongside our method. 4.1. Standard Benchmarks We thoroughly validate our implications by evaluating our method on the three common geometric downstream tasks, i.e., pointmap estimation, video depth and camera pose estimation with their standard benchmarks. Pointmap estimation. Following prior work [93, 96], we evaluate multi-view point-map estimation on NRGBD [6], 7scenes [80], DTU [43], and ETH3D [77], using Chamfer Distance and Normal Consistency [93] to assess the quality of the reconstructed points and surfaces, respectively. As shown in Tab. 1, we outperform the other O(n) baseline, TTT3R, on all benchmarks except CD on 7scenes-D, where we are only marginally worse. Notably, our method reduces error by 2 2.5 on DTU, ETH3D, and NRGBD-D. Compared to O(n2) baselines, we remain competitive and even surpass their performance on DTU. Video depth. Following [96], we also report the performance on the task of video depth estimation using Bonn [66], KITTI [32] and Sintel [13] evaluation sets. As in prior work, we align predictions using single scale per sequence and report the Absolute Relative Error (Abs. Rel.) as well as the percentage of predictions with δ < 1.25. As shown in Tab. 2, our method outperforms the O(n) baseline TTT3R on two of the three datasetsby substantial margin and achieves performance on par with O(n2) methods on KITTI dataset. Camera pose estimation. We further evaluate our model on the task of camera pose estimation using TUM7scenes-D 7scenes-S DTU ETH3D NRBGD-D NRBGD-S CD NC CD NC CD NC CD NC CD NC CD NC O(n2) O(n) VGGT SparseVGGT FastVGGT TTT3R VGG-T3 (ours) 0.024 0.023 0.021 0.035 0. 0.668 0.665 0.662 0.666 0.679 0.054 0.094 0.065 0.129 0.107 0.731 0.694 0.719 0.685 0. 1.537 1.541 1.683 5.708 1.654 0.676 0.675 0.672 0.672 0.685 0.279 0.327 0.594 0.885 0. 0.855 0.836 0.775 0.733 0.789 0.014 0.018 0.033 0.071 0.029 0.897 0.876 0.841 0.811 0. 0.055 0.079 - 0.094 0.056 0.894 0.859 - 0.819 0.893 Table 1. Pointmap estimation on dense (-D) and sparse (-S) split. Overall, we outperform O(n) baseline, TTT3R, and remain competitive w.r.t. O(n2) baselines. FastVGGT code fails on NRGBD-S due to one instance having only two views. Bonn KITTI Sintel δ < 1.25 Abs Rel δ < 1.25 Abs Rel δ < 1.25 Abs Rel VGGT SparseVGGT FastVGGT TTT3R VGG-T3 (ours) 0.967 0.968 0.969 0.969 0.963 0.059 0.057 0.058 0.061 0.063 0.964 0.963 0. 0.818 0.967 0.071 0.070 0.073 0.151 0.076 0.646 0.639 0.630 0.510 0.581 0.300 0.304 0. 0.469 0.345 Table 2. Video depth estimation. VGG-T3 outperforms sequential O(n) baseline by substantial margin and performs on-par with O(n2) baselines. ScanNet Sintel TUM ATE RPEr RPEt ATE RPEr RPEt ATE RPEr RPEt VGGT SparseVGGT FastVGGT 0.035 0.381 0.036 0.394 0.035 0.494 0.016 0.172 0.467 0.016 0.177 0.552 0.018 0.158 0.516 0.061 0.012 0.310 0.070 0.013 0.316 0.060 0.013 0.317 0.063 0.617 TTT3R TTT3R (unordered) 0.094 3.942 VGG-T3 (ours) 0.070 0.878 0.020 0.196 0.767 0.089 0.325 1.462 0.034 0.234 1. 0.088 0.025 0.337 0.213 0.029 0.652 0.117 0.037 0.533 0.010 0.010 0.011 0.012 0.029 0.028 Table 3. Camera pose estimation. Our method supports both ordered and unordered input sequences, whereas the other TTT3R performs poorly on unordered inputs. Via sequential processing, TTT3R provides more accurate pose estimates. Best performance on ordered inputs are marked bold, best un-ordered blue. RGBD [84], ScanNet [18], and Sintel [13]. While our method shows consistent advantages on other tasks, we observe that our TTT-linearized model struggles on camera pose estimation, as shown in Tab. 3. We suspect this is related to VGGTs special treatment of camera pose, where dedicated camera token is appended to the image tokens immediately before the attention layer, effectively creating two input modalities. This heterogeneous structure may be challenging for the MLP within the TTT layer to memorize, which highlights an interesting direction for future research. Nevertheless, it is worth noting that our method naturally supports both ordered and unordered input sequences, whereas the other O(n) baseline, TTT3R, degrades under unordered inputs, as shown in Tab. 3. 4.2. Large-Scale 3D Reconstruction As discussed in Sec. 3.3, our method preserves the accuracy advantages of offline, global reconstruction while scaling linearly with the number of input views, thereby enabling large-scale 3D scene reconstruction. Setup. To evaluate the scalability of each model, we use the 7scenes dataset, which provides sufficient video coverage 1500 images 2000 images 1 GPU 2 GPUs 4 GPUs 1 GPU 2 GPUs 4 GPUs TTT3R VGGT VGG-T3 (ours) 90.1 N/A OOM 1779.3 56.8 173.1 N/A 913.6 29. 126.2 N/A OOM 2827.1 74.8 230.7 N/A 1590.2 48.5 Table 4. Reconstruction latency (s) with distributed inference. VGG-T3 can efficiently process large sequences on single GPU, and provide linear speed-up via distributed inference. for large-scale reconstruction. For each scene, we aggregate all video frames and uniformly subsample the images to form the validation set. All remaining implementation details and evaluation metrics follow Sec. 4.1. Results. We report runtime and reconstruction quality with different image collection sizes in Fig. 4. Comparing to O(n2) methods such as VGGT, FastVGGT, and SparseVGGT, VGG-T3 scales linearly and thus is substantially faster: it reconstructs 1k images in 58 seconds, whereas VGGT requires over 11 minutes (11.6 slower) and FastVGGT takes more than 4 minutes (4.3 slower). Comparing to the state-of-the-art O(n) alternative, TTT3R, VGG-T3 delivers significantly higher reconstruction accuracy and maintains stable performance even when scaling to image counts far beyond those seen during training. visual comparison can be found in Sec. 4.2. Distributed inference. Our method naturally supports multi-GPU distributed inference for additional speedup, as shown in Tab. 4 and discussed in Sec. 3.3. In contrast to VGGT, which requires carefully engineered contextparallel implementations for softmax attention (e.g., ring attention [59]), VGG-T3 works directly with distributed data parallel (DDP) as cross-GPU communication is only needed during the fast-weight (MLP) update. The alternative O(n) method, TTT3R, is not compatible with multiGPU inference due to its autoregressive processing. 7 er() et (m) 10cm, 10 (%) 20cm, 20 (%) 7Scenes Wayspots TTT3R Ours TTT3R Ours 7.18 6.71 74.45 32.04 0.17 0. 4.38 1.90 34.59 40.69 0.69 13.41 70.21 73.00 2.94 30.64 Table 5. Feed-forward visual localization in unposed image collection. The MLP-based state representation in VGG-T3 allows for more precise localization of new images compared to TTT3R. CD NC mAA(30) Softmax Attention 0.061 0.844 0.262 (i) Scratch 0.137 (ii) T2R [48] 0.097 (iii) LoLCats [112] 0.074 (iv) Ours (v) Ours + ShortConv2D 0.066 0.727 0.804 0.804 0.833 0. 76.33 52.95 66.27 62.87 72.16 74.14 Table 6. Ablations. We evaluate key design decisions behind our linearization and ShortConv2D design. Variants. We train model using softmax attention as upper-bound performance reference. We apply our linearization approach to this model as described in Sec. 3.2. As baseline for linearization we adapt T2R [48] to our setting. We also consider variant where linearize the model without initialization from pre-trained weight. Finally, we ablate the effectiveness of adding ShortConv2D as discussed in Sec. 3. Discussion. As reported in Tab. 6, we find that training with TTT from scratch (i) gets stuck in local optimum and linearizing the model pre-trained with softmax atttention is key for good performance. Our linearization (iii) with test-time training significantly outperforms T2R [48] (ii) and LoLCats [112] (iii). Finally, using ShortConv2D (v) further closes the gap towards softmax attention. 5. Conclusion We presented scalable feed-forward 3D reconstruction that gracefully scales with the number of input views. At the core of our approach is learning mapping from Keys to Values via test-time optimization instead of querying the KV representation with softmax attention, an operation that scales quadratically w.r.t. number of input views. Our efficient linearization of the VGGT model allows reconstruction of 1k images 11.6 and 2k images up to 33 faster while outperforming linear-time methods on pointmap and video depth estimation by large margins. Limitations. Empirically, we show that our approach retains scalability of online (auto-regressive) methods, and provides significantly more accurate depth and point maps due to global feature aggregation. However, there is still gap w.r.t. softmax attention, especially in the wide-baseline setting. This suggests that future work should focus on reconciling the fixed expressivity of the MLP scene representation with the high accuracy of quadratic attention. } { 100, 500, 1k ) vs. Chamfer distance ( ) for collections Figure 4. Runtime ( of size on 7scenes dataset. In terms of reconstruction quality (Chamfer distance), we observe small gap between VGG-T3 and O(n2) baselines, that narrows with increasing number of images. However, for 1k input, VGGT takes ca. 11min while VGG-T3 only needs 58 seconds (11.6 speedup). VGG-T3 scales comparably to TTT3R and does not degrade w.r.t. increasing number of images. 4.3. Feed-forward Visual Localization As discussed in Sec. 3.3, we can query our model with new images that were not part of test-time optimization. This can be interpreted as feed-forward visual localization with respect to the implicit map produced via TTT. Setup. We evaluate our approach on two commonly used datasets: 7scenes [80] and Wayspots [4, 11], and compare to TTT3R which, as an autoregressive model, also maintains state that can be queried in O(1). We perform Sim(3) alignment between the ground truth and predicted poses for the mapping images, then measure the rotation er and translation error et of the predicted query image poses as well as the percentage of query images localized within thresholds er < Tr, et < Tt. Discussion. As shown in Tab. 5, VGG-T3 outperforms TTT3R on both benchmarks, with particularly large improvements on Wayspots. This demonstrates that our MLPbased scene representation enables effective feed-forward visual localization. We note that the state-of-the-art visual localization pipelines that utilize accurate camera poses during explicit mapping, could achieve more accurate localization, e.g., Reloc3R [26] achieves er = 1.02, et = 0.04m on 7scenes our aim is to show that feed-forward visual localization without explicit mapping is indeed feasible, and opens exciting future research directions. 4.4. Ablations In Tab. 6 we outline key ablation studies that justify our design choices, that we perform in smaller scale setting with image resolution of 224 224 on ScanNet++ training with 2 24 views. All models use the same base architecture. 8 Acknowledgments. We thank Alessandro Bursio for providing valuable feedback on the draft of this paper. We also thank Tobias Fischer and Alessandro Bursio for helping with the training and evaluation data setup used for this work. 9 VGG-T3: Offline Feed-Forward 3D Reconstruction at Scale"
        },
        {
            "title": "Supplementary Material",
            "content": "In this appendix, we provide: detailed description of the implementation and training process of VGG-T3, including dataset usage, image collection sampling (based on co-visibility), training hyperparameters, and the specific parameters that are optimized during training (Sec. A). Enhancements to the VGGT baseline that enables processing of larger image collections as well as increased accuracy making it stronger baseline. (Sec. B) Further ablation studies on key components of our method, including the effect of the number of optimizer steps used for the Test-Time Training (TTT) objective and an investigation into different filter configurations for the ShortConv2D layer, showing optimal settings (Sec. C). Additional qualitative results and visualizations for comparison with baselines (VGGT, TTT3R), qualitative examples of visual localization, and discussion of the methods performance on scenes with larger spatial extent (Sec. D). A. Implementation Details Training. We list the datasets used for training in Sec. A. To obtain an image collection during training, we follow greedy sampling approach: The algorithm starts by randomly sampling the first image, then uniformly samples from the set of images with co-visibility greater than 0.3 with any of the images currently in the collection. This step repeats until the desired collection size is reached. We precompute the required co-visibility matrix via depth consistency check [85]. Following VGGT, we use an adaptive batch size with image collections of 2-24 images while keeping the total number of images per GPU at approximately 48. The image aspect ratio is sampled uniformly from the interval [0.5, 2.0], and images are then resized such that their longer side is 518. During training, we apply color jitter augmentation to each image independently, making the network more robust to brightness and contrast changes. We train VGG-T3 using AdamW [61] with learning rate of 104, weight decay of 0.05, and β1 = 0.9, β2 = 0.95. The learning rate increases by factor of 10 during the first 1, 000 training steps, then decays following cosine schedule to final learning rate of 106. For the inner optimization of the test-time training objective, we use Muon [45] with 5 Newton-Schulz iterations, learning rate of 0.1, and employ 1 optimizer step during training. The TTT MLPs use input and output dimension 1024, matching the hidden state size of VGGT, and projects to 4 the input"
        },
        {
            "title": "Outdoor",
            "content": "Aria Synthetic Environments [5] DynamicReplica [47] Hypersim [73] Replica [83] Cubify Anything [54] Scannet++ [109] Scannet [18] Taskonomy [110] Mapillary Metropolis [72] MatrixCity [56] Megadepth [57] Mid-Air [29] Mapillary Planet-scale Depth Dataset [2] ParallelDomain4D [25, 90] vKITTI2 [14, 31]"
        },
        {
            "title": "Mixed",
            "content": "CO3Dv2 [71] Kubric [33] Wild-RGBD [102] BlendedMVG [108] DL3DV-10K [58] Spring [63] TartanAirV2 [98] UnrealStereo4k [89] Table 7. Datasets used for training. dimension in their hidden layers. We train only the QKV projection matrices as well as the output projection in the global attention layers and the newly introduced parameters of the TTT module, while keeping all remaining parameters of the VGGT architecture (including encoder, per-image attention, and prediction heads) frozen. Additionally, only the values projected from image patch tokens participate in the ShortConv2D operation. The camera and register tokens are passed through. Inference details. The VGGT architecture, which we initialize with, has multiple decoders that predict redundant geometric quantities. To obtain pointmaps one can either use the outputs of the global pointmap prediction head directly or use the camera and depth predictions together to unproject to pointmaps. While VGGT finds the latter to be more precise, we use the global prediction head to obtain pointmaps due to the imprecise camera pose predictions mentioned in Sec. 4.1, which would otherwise degrade the pointmaps obtained by unprojecting depth. In Sec. 4.3, we retain the camera tokens of all mapping images as input to the camera head in the visual localization setting since 1 VGGTs camera head requires the camera tokens of all images. The camera token of the query image then participates in the softmax attention operation in the camera head before it is decoded to camera parameters. For all benchmarking, we use NVIDIA A100-80GB GPUs. Further evaluation details. For visual localization results in Sec. 4.3, we sub-sample mapping images at stride of 200 for 7Scenes and 20 for Wayspots. For pointmap evaluation Sec. 4.2, the usage of the iterative closest point (ICP) algorithm for alignment of prediction and ground truth point clouds makes evaluation very slow when evaluating predictions on large image sets. We instead select set of equally spaced keyframes, that capture the scene geometry, to compute pointmap metrics while we treat all other frames as supporting views. For our evalution we use 10 keyframes. For TTT3R, we provide the images in sequential order with the keyframes last such that the model has seen all images of the scene before making predictions. B. VGGT adjustments To enable fair comparison with VGGT in the setting with large number of images in Sec. 4.2, we perform several changes in the VGGT codebase that enhance its performance. Memory-optimizations and distributed inference. First, we follow Shen et al. [79] and discard unused activations in VGGTs alternating attention module, which allows processing up to 1k images on single 80GB GPU. Next, we enable context parallel inference using Ulysses [41], implemented in TransformerEngine 1, in the global attention layers. We note that the underlying attention implementation still uses FlashAttention2 [21]. While this allows VGGT to run for 2k images, as we show in Sec. 4.2, this requires runtimes up to 47 minutes on 2 GPUs. Enhanced long-sequence generalization. For fair comparison on large image collections, we further adjust the scale parameter of the softmax in the global attention layers similar to the approach of Jin et al. [44], ensuring the entropy of the attention matrix stays constant. Let ai,j = exp(λkT qj) exp(λkT qk) (cid:80) (6) be the attention scores as used in softmax attention where λ = 1/ [91]. We instead set λ = λ max(1.0, logNT ), (7) where NT and are the maximum number of tokens seen during training and of the current sequence, respec1https : / / docs . nvidia . com / deeplearning / transformer - engine / user - guide / api / pytorch . html # transformer_engine.pytorch.DotProductAttention 2 #images 250 500 750 1000 CD NC CD NC CD NC CD NC VGGT VGGT + Entropy-scaling 0.018 0.016 0.894 0.894 0.025 0.017 0.876 0.889 0.040 0.030 0.864 0. 0.041 0.029 0.855 0.872 Table 8. Attention entropy-scaling makes VGGT stronger baseline on large image collections. Figure 5. Pointmap error with increasing number of images when varying the optimizer steps on the TTT objective. tively. This ensures that the scaling is the same for sequence lengths seen during training, while for larger sequence lengths the attention matrix is sharpened. Since VGGT trains using maximum of 24 images with 518518 resolution and uses patch size 14, we set NT = 24 (518/14)2 = 32, 856. We show improved performance using this entropy-scaling in Tab. 8 for large image collections making the VGGT baseline significantly stronger. C. Additional Results Number of optimizer steps. We provide an additional evaluation varying the number of steps used to optimize the TTT objective Eq. (3) at inference time for varying image collection sizes. We report results on the NRGBD dataset [6] in Fig. 5. As expected, we find that without TTT optimization the reconstruction error is high as no global information is propagated across tokens. single optimizer step is sufficient for image collection sizes seen during training; however, the reconstruction error degrades as the number of images extends beyond that. Two optimizer steps achieve the best performance across wide range of image collection sizes, and further increasing the number of steps to 3 or 4 leads to comparable or slightly worse performance. ShortConv2D. In the main paper, we find improved performance when using 3 3 ShortConv2D on values vi of the attention operation before optimizing the MLP using TTT. Here, we provide further experiments using different configurations of our ShortConv2D. In addition to 3 3 filter on the values vi (V -3) used in the main paper, we consider 55 filter (V -5) and variant where we apply ShortConv2D CD NC mAA(30) No ShortConv2D 0.074 -3 0.066 -5 0.069 K-3 0.068 KV -3 0.081 0.833 0.838 0.833 0.834 0.820 72.16 74.14 72.52 72.89 69. Table 9. Results for different filter configuration in ShortConv2D. to keys ki and values vi jointly (KV -3). We report results in Tab. 9. We observe that increasing the filter size from 3 to 5 does not further increase performance, showing that filter size of 3 is sufficient to obtain strong self-supervised objective for TTT. Applying ShortConv2D to both the keys and values results in decreased performance. We explain this by the fact that applying the same spatial mixing does not break the dependency between keys and values, as explained in Sec. 4.2. D. Additional Qualitative Results Qualitative comparison. We report additional qualitative comparisons between VGGT, TTT3R, and VGG-T3 in Fig. 6. TTT3R and VGG-T3 process these 1k image collections within 1 minute; however, our method produces 3D consistent reconstructions while TTT3R degrades significantly. VGGT achieves slightly sharper details but takes more than 11 minutes due to the quadratic scaling of softmax attention. Visual localization examples. Complementary to the visual localization results in Sec. 4.3, we show examples of localizing query images in the completed reconstruction by running the frozen MLPs in Fig. 7. In Fig. 8, we show an inthe-wild example where we localize tourist picture taken from phone camera, together with its geometry, within recording of an autonomous vehicle from the KITTI dataset that is 7 years older. Despite the temporal gap and changes in the street, our method successfully localizes the query image. We observe that the tourist photo captures upper parts of buildings not visible from the car-mounted camera, demonstrating the robustness of our approach to viewpoint variations. Scenes with larger spatial extent. We visualize reconstructions of Waymo sequences that have larger spatial extent in Fig. 9. While VGG-T3 can often achieve similar results to VGGT (Fig. 9a), in some cases with more complex scene layouts, the reconstruction quality is degraded (Fig. 9b). We note this as limitation that linear-time attention mechanisms cannot yet match softmax attention in all cases; however, this also provides an interesting avenue to explore for future work by, e.g., adapting the amount of computation depending on scene complexity and designing more expressive linear attention mechanisms that match the accuracy of softmax attention."
        },
        {
            "title": "References",
            "content": "[1] Sameer Agarwal, Yasutaka Furukawa, Noah Snavely, Ian Simon, Brian Curless, Steven Seitz, and Richard Szeliski. Building rome in day. ICCV, 2011. 2 [2] Manuel Lopez Antequera, Pau Gargallo, Markus Hofinger, Samuel Rota Bul`o, Yubin Kuang, and Peter Kontschieder. In CVPR, pages Mapillary Planet-Scale Depth Dataset. 589604, Cham, 2020. Springer International Publishing. 1 [3] Relja Arandjelovic, Petr Gronat, Akihiko Torii, Tomas Pajdla, and Josef Sivic. Netvlad: Cnn architecture for weakly supervised place recognition. In CVPR, 2016. 3 [4] Eduardo Arnold, Jamie Wynn, Sara Vicente, Guillermo Aron Monszpart, Victor Prisacariu, Garcia-Hernando, Daniyar Turmukhambetov, and Eric Brachmann. Map-Free Visual Relocalization: Metric Pose Relative to Single Image. In ECCV, 2022. 8 [5] Armen Avetisyan, Christopher Xie, Henry HowardJenkins, Tsun-Yi Yang, Samir Aroudj, Suvam Patra, Fuyang Zhang, Duncan P. Frost, Luke Holland, Campbell Orme, Jakob J. Engel, Edward Miller, Richard A. Newcombe, and Vasileios Balntas. SceneScript: Reconstructing Scenes with an Autoregressive Structured Language Model. In Computer Vision - ECCV 2024 - 18th European Conference, Milan, Italy, September 29-October 4, 2024, Proceedings, Part LXI, pages 247263. Springer, 2024. arXiv:2403.13064 [cs]. 1 [6] Dejan Azinovic, Ricardo Martin-Brualla, Dan B. Goldman, Matthias Nießner, and Justus Thies. Neural RGB-D Surface Reconstruction. In CVPR, 2022. 6, [7] Jimmy Lei Ba, Jamie Ryan Kiros, and Geoffrey E. Hinton. Layer Normalization. arXiv preprint arXiv:1607.06450, 2016. 3 [8] Ali Behrouz, Peilin Zhong, and Vahab Mirrokni. Titans: Learning to memorize at test time. arXiv preprint arXiv:2501.00663, 2024. 3 [9] Gabriele Berton, Carlo Masone, and Barbara Caputo. Rethinking visual geo-localization for large-scale applications. In CVPR, 2022. 3 [10] Eric Brachmann and Carsten Rother. Visual camera relocalization from rgb and rgb-d images using dsac. IEEE TPAMI, 44(9), 2021. 3 [11] Eric Brachmann, Tommaso Cavallari, and Victor Adrian Prisacariu. Accelerated Coordinate Encoding: Learning to In CVPR, Relocalize in Minutes Using RGB and Poses. 2023. [12] Eric Brachmann, Jamie Wynn, Shuai Chen, Tommaso Cavallari, Aron Monszpart, Daniyar Turmukhambetov, and Victor Adrian Prisacariu. Scene coordinate reconstruction: Posing of image collections via incremental learning of relocalizer. In ECCV, 2024. 3 [13] Daniel J. Butler, Jonas Wulff, Garrett B. Stanley, and Michael J. Black. Naturalistic Open Source Movie for Optical Flow Evaluation. In ECCV, 2012. 6, 7 3 [14] Yohann Cabon, Naila Murray, and Martin Humenberger. Virtual KITTI 2, 2020. arXiv:2001.10773 [cs]. 1 [15] Yohann Cabon, Lucas Stoffl, Leonid Antsfeld, Gabriela Csurka, Boris Chidlovskii, Jerome Revaud, and Vincent Leroy. MUSt3R: Multi-view Network for Stereo 3D Reconstruction. arXiv preprint arXiv:2503.01661, 2025. [16] Xingyu Chen, Yue Chen, Yuliang Xiu, Andreas Geiger, and Anpei Chen. Ttt3r: 3d reconstruction as test-time training. arXiv preprint arXiv:2509.26645, 2025. 3, 6 [17] Zhuoguang Chen, Minghui Qin, Tianyuan Yuan, Zhe Liu, and Hang Zhao. LONG3R: Long Sequence Streaming 3D Reconstruction. In ICCV, 2025. 3 [18] Angela Dai, Angel X. Chang, Manolis Savva, Maciej Halber, Thomas Funkhouser, and Matthias Niessner. ScanNet: Richly-Annotated 3D Reconstructions of Indoor Scenes. In CVPR, 2017. 7, 1 [19] Karan Dalal, Daniel Koceja, Gashon Hussein, Jiarui Xu, Yue Zhao, Youjin Song, Shihao Han, Ka Chun Cheung, Jan Kautz, Carlos Guestrin, et al. One-minute video generation with test-time training. In CVPR, 2025. 3 [20] Tri Dao and Albert Gu. Transformers are SSMs: Generalized Models and Efficient Algorithms Through Structured State Space Duality. In Int. Conf. Mach. Learn., 2024. 3 [21] Tri Dao, Dan Fu, Stefano Ermon, Atri Rudra, and Christopher Re. FlashAttention: Fast and Memory-Efficient Exact Attention with IO-Awareness. NeurIPS, 35, 2022. 6, [22] DeepSeek-AI. DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning. arXiv preprint arXiv:2501.12948, 2025. 5 [23] Mostafa Dehghani, Josip Djolonga, Basil Mustafa, Piotr Padlewski, Jonathan Heek, Justin Gilmer, Andreas Peter Steiner, Mathilde Caron, Robert Geirhos, Ibrahim Alabdulmohsin, Rodolphe Jenatton, Lucas Beyer, Michael Tschannen, Anurag Arnab, Xiao Wang, Carlos Riquelme Ruiz, Matthias Minderer, Joan Puigcerver, Utku Evci, Manoj Kumar, Sjoerd van Steenkiste, Gamaleldin Fathy Elsayed, Aravindh Mahendran, Fisher Yu, Avital Oliver, Fantine Huot, Jasmijn Bastings, Mark Collier, Alexey A. Gritsenko, Vighnesh Birodkar, Cristina Nader Vasconcelos, Yi Tay, Thomas Mensink, Alexander Kolesnikov, Filip Pavetic, Dustin Tran, Thomas Kipf, Mario Lucic, Xiaohua Zhai, Daniel Keysers, Jeremiah J. Harmsen, and Neil Houlsby. Scaling Vision Transformers to 22 Billion Parameters. In Int. Conf. Mach. Learn., 2023. 4 [24] Kai Deng, Zexin Ti, Jiawei Xu, Jian Yang, and Jin Xie. VGGT-Long: Chunk it, Loop it, Align it Pushing VGGTs Limits on Kilometer-scale Long RGB Sequences. arXiv preprint arXiv:2507.16443, 2025. 2 [25] Parallel Domain. Parallel domain. https : / / paralleldomain.com/, 2024. 1 [26] Siyan Dong, Shuzhe Wang, Shaohui Liu, Lulu Cai, Qingnan Fan, Juho Kannala, and Yanchao Yang. Reloc3r: Large-Scale Training of Relative Camera Pose Regression for Generalizable, Fast, and Accurate Visual Localization. In CVPR, 2025. 8 [27] Bardienus Pieter Duisterhof, Lojze Zust, Philippe Weinzaepfel, Vincent Leroy, Yohann Cabon, and Jerome Re4 vaud. MASt3R-SfM: Fully-Integrated Solution for Unconstrained Structure-from-Motion. In 3DV, 2025. 1 [28] Sven Elflein, Qunjie Zhou, and Laura Leal-Taixe. Light3RSfM: Towards Feed-forward Structure-from-Motion. In CVPR, 2025. [29] Michael Fonder and Marc Van Droogenbroeck. Mid-Air: Multi-Modal Dataset for Extremely Low Altitude Drone Flights. In CVPR, pages 00, 2019. 1 [30] Daniel Fu, Tri Dao, Khaled Saab, Armin Thomas, Atri Rudra, and Christopher Re. Hungry hungry hippos: Towards language modeling with state space models. arXiv preprint arXiv:2212.14052, 2022. 3 [31] Adrien Gaidon, Qiao Wang, Yohann Cabon, and Eleonora Vig. Virtual Worlds as Proxy for Multi-Object Tracking Analysis. In CVPR, pages 43404349, 2016. 1 [32] Geiger, Lenz, Stiller, and Urtasun. Vision meets robotics: The KITTI dataset. Int. Jour. of Rob. Res., 32(11), 2013. 6 [33] Klaus Greff, Francois Belletti, Lucas Beyer, Carl Doersch, Yilun Du, Daniel Duckworth, David J. Fleet, Dan Gnanapragasam, Florian Golemo, Charles Herrmann, Thomas Kipf, Abhijit Kundu, Dmitry Lagun, Issam Laradji, Hsueh-Ti (Derek) Liu, Henning Meyer, Yishu Miao, Derek Nowrouzezahrai, Cengiz Oztireli, Etienne Pot, Noha Radwan, Daniel Rebain, Sara Sabour, Mehdi S. M. Sajjadi, Matan Sela, Vincent Sitzmann, Austin Stone, Deqing Sun, Suhani Vora, Ziyu Wang, Tianhao Wu, Kwang Moo Yi, Fangcheng Zhong, and Andrea Tagliasacchi. Kubric: In CVPR, pages 37493761, Scalable Dataset Generator. 2022. 1 [34] Albert Gu and Tri Dao. Mamba: Linear-time sequence modeling with selective state spaces. In COLM, 2024. 3, 5 [35] Albert Gu, Karan Goel, and Christopher Re. Efficiently modeling long sequences with structured state spaces. arXiv preprint arXiv:2111.00396, 2021. [36] Dongchen Han, Yining Li, Tianyu Li, Zixuan Cao, Ziming Wang, Jun Song, Yu Cheng, Bo Zheng, and Gao Huang. ViT$ˆ3$: Unlocking Test-Time Training in Vision, 2025. arXiv:2512.01643 [cs]. 5 [37] Stephen Hausler, Sourav Garg, Ming Xu, Michael Milford, and Tobias Fischer. Patch-netvlad: Multi-scale fusion of locally-global descriptors for place recognition. In CVPR, 2021. 3 [38] Alex Henry, Prudhvi Raj Dachapally, Shubham Shantaram Pawar, and Yuxuan Chen. Query-Key Normalization for Transformers. In Findings of the Association for Computational Linguistics: EMNLP 2020, 2020. 4, 5 [39] Geoffrey E. Hinton and David C. Plaut. Using Fast Weights to Deblur Old Memories. Proc. Ann. Meeting of the Cog. Sci. Soc., 9(0), 1987. 4 [40] Weizhe Hua, Zihang Dai, Hanxiao Liu, and Quoc Le. In Int. Conf. Mach. Transformer quality in linear time. Learn., 2022. [41] Sam Ade Jacobs, Masahiro Tanaka, Chengming Zhang, Minjia Zhang, Shuaiwen Leon Song, Samyam Rajbhandari, and Yuxiong He. DeepSpeed Ulysses: System Optimizations for Enabling Training of Extreme Long Sequence Transformer Models. 2023. 2 arXiv preprint arXiv:2309.14509, [42] Wonbong Jang, Philippe Weinzaepfel, Vincent Leroy, Lourdes Agapito, and Jerome Revaud. Pow3R: Empowering Unconstrained 3D Reconstruction with Camera and Scene Priors. In CVPR, 2025. 2 [43] Rasmus Jensen, Anders Dahl, George Vogiatzis, Engin Tola, and Henrik Aanaes. Large Scale Multi-view Stereopsis Evaluation. In CVPR, 2014. 6 [44] Zhiyu Jin, Xuli Shen, Bin Li, and Xiangyang Xue. Training-free Diffusion Model Adaptation for VariableSized Text-to-Image Synthesis. NeurIPS, 36, 2023. 5, 2 [45] Keller Jordan, Yuchen Jin, Vlado Boza, Jiacheng You, Franz Cesista, Laker Newhouse, and Jeremy Bernstein. Muon: An optimizer for hidden layers in neural networks, 2024. 6, [46] Praneeth Kacham, Vahab Mirrokni, and Peilin Zhong. Polysketchformer: Fast transformers via sketching polynomial kernels. arXiv preprint arXiv:2310.01655, 2023. 3 [47] Nikita Karaev, Ignacio Rocco, Benjamin Graham, Natalia Neverova, Andrea Vedaldi, and Christian Rupprecht. DynamicStereo: Consistent Dynamic Depth From Stereo Videos. In CVPR, pages 1322913239, 2023. 1 [48] Jungo Kasai, Hao Peng, Yizhe Zhang, Dani Yogatama, Gabriel Ilharco, Nikolaos Pappas, Yi Mao, Weizhu Chen, and Noah A. Smith. Finetuning Pretrained Transformers into RNNs. In Proc. Emp. Met. in Nat. Lang. Proc., 2021. 3, 8 [49] Angelos Katharopoulos, Apoorv Vyas, Nikolaos Pappas, and Francois Fleuret. Transformers are RNNs: Fast Autoregressive Transformers with Linear Attention. In Int. Conf. Mach. Learn., 2020. 3 [50] Tong Ke and Stergios Roumeliotis. An efficient algebraic solution to the perspective-three-point problem. In CVPR, 2017. 3 [51] Nikhil Keetha, Norman Muller, Johannes Schonberger, Lorenzo Porzi, Yuchen Zhang, Tobias Fischer, Arno Knapitsch, Duncan Zauss, Ethan Weber, Nelson Antunes, Jonathon Luiten, Manuel Lopez-Antequera, Samuel Rota Bul`o, Christian Richardt, Deva Ramanan, Sebastian Scherer, and Peter Kontschieder. MapAnything: Universal Feed-Forward Metric 3D Reconstruction. arXiv preprint arXiv:2509.13414, 2025. 1, 3 [52] Laurent Kneip, Davide Scaramuzza, and Roland Siegwart. novel parametrization of the perspective-threepoint problem for direct computation of absolute camera position and orientation. In CVPR, 2011. [53] Yushi Lan, Yihang Luo, Fangzhou Hong, Shangchen Zhou, Honghua Chen, Zhaoyang Lyu, Shuai Yang, Bo Dai, Chen Change Loy, and Xingang Pan. STream3R: Scalable Sequential 3D Reconstruction with Causal Transformer. arXiv preprint arXiv:2508.10893, 2025. 3 [54] Justin Lazarow, David Griffiths, Gefen Kohavi, Francisco Crespo, and Afshin Dehghan. Cubify Anything: Scaling Indoor 3D Object Detection. In CVPR, pages 2222522233, 2025. 1 5 [55] Vincent Leroy, Yohann Cabon, and Jerˆome Revaud. Grounding Image Matching in 3D with MASt3R. In ECCV, 2024. 2 [56] Yixuan Li, Lihan Jiang, Linning Xu, Yuanbo Xiangli, Zhenzhi Wang, Dahua Lin, and Bo Dai. MatrixCity: Largescale City Dataset for City-scale Neural Rendering and Beyond. In ICCV, pages 32053215, 2023. 1 [57] Zhengqi Li and Noah Snavely. MegaDepth: Learning Single-View Depth Prediction From Internet Photos. In CVPR, pages 20412050, 2018. [58] Lu Ling, Yichen Sheng, Zhi Tu, Wentian Zhao, Cheng Xin, Kun Wan, Lantao Yu, Qianyu Guo, Zixun Yu, Yawen Lu, Xuanmao Li, Xingpeng Sun, Rohan Ashok, Aniruddha Mukherjee, Hao Kang, Xiangrui Kong, Gang Hua, Tianyi Zhang, Bedrich Benes, and Aniket Bera. DL3DV-10K: Large-Scale Scene Dataset for Deep Learning-based 3D Vision. In CVPR, pages 2216022169, 2024. 1 [59] Hao Liu, Matei Zaharia, and Pieter Abbeel. Ring Attention with Blockwise Transformers for Near-Infinite Context. arXiv preprint arXiv:2310.01889, 2023. 7 [60] Yuzheng Liu, Siyan Dong, Shuzhe Wang, Yingda Yin, Yanchao Yang, Qingnan Fan, and Baoquan Chen. SLAM3R: Real-Time Dense Scene Reconstruction from Monocular RGB Videos. In CVPR, 2025. 2 [61] Ilya Loshchilov and Frank Hutter. Decoupled Weight In ICLR. OpenReview.net, 2019. Decay Regularization. arXiv:1711.05101. 1 [62] Dominic Maggio, Hyungtae Lim, and Luca Carlone. VGGT-SLAM: Dense RGB SLAM Optimized on the SL(4) Manifold. arXiv preprint arXiv:2505.12549, 2025. 2 [63] Lukas Mehl, Jenny Schmalfuss, Azin Jahedi, Yaroslava Nalivayko, and Andres Bruhn. Spring: High-Resolution High-Detail Dataset and Benchmark for Scene Flow, Optical Flow and Stereo. In CVPR, pages 49814991, 2023. [64] Jean Mercat, Igor Vasiljevic, Sedrick Keh, Kushal Arora, Achal Dave, Adrien Gaidon, and Thomas Kollar. Linearizing Large Language Models. arXiv preprint arXiv:2405.06640, 2024. 3, 4 [65] Ben Mildenhall, Pratul P. Srinivasan, Matthew Tancik, Jonathan T. Barron, Ravi Ramamoorthi, and Ren Ng. NeRF: Representing Scenes as Neural Radiance Fields for View Synthesis. In ECCV, 2020. 2 [66] Emanuele Palazzolo, Jens Behley, Philipp Lottes, Philippe Gigu`ere, and Cyrill Stachniss. ReFusion: 3D Reconstruction in Dynamic Environments for RGB-D Cameras Exploiting Residuals. In Int. Conf. Intell. Robot. Syst., 2019. 6 [67] Linfei Pan, Daniel Barath, Marc Pollefeys, and Johannes L. Schonberger. Global Structure-from-Motion Revisited. In ECCV, 2024. 1, 2 [68] Vojtech Panek, Zuzana Kukelova, and Torsten Sattler. Meshloc: Mesh-based visual localization. In ECCV, 2022. [69] Jeong Joon Park, Peter Florence, Julian Straub, Richard Newcombe, and Steven Lovegrove. Deepsdf: Learning continuous signed distance functions for shape representation. In CVPR, 2019. 2 [70] Michael Poli, Stefano Massaroli, Eric Nguyen, Daniel Y. Fu, Tri Dao, Stephen Baccus, Yoshua Bengio, Stefano Ermon, and Christopher Re. Hyena Hierarchy: Towards In Int. Conf. Larger Convolutional Language Models. Mach. Learn., 2023. 3, 5 [71] Jeremy Reizenstein, Roman Shapovalov, Philipp Henzler, Luca Sbordone, Patrick Labatut, and David Novotny. Common Objects in 3D: Large-Scale Learning and Evaluation of Real-Life 3D Category Reconstruction. In ICCV, pages 1090110911, 2021. 1 [72] Mapillary Research. Mapillary Metropolis Dataset. 1 [73] Mike Roberts, Jason Ramapuram, Anurag Ranjan, Atulit Kumar, Miguel Angel Bautista, Nathan Paczan, Russ Webb, and Joshua M. Susskind. Hypersim: Photorealistic Synthetic Dataset for Holistic Indoor Scene Understanding. In ICCV, pages 1091210922, 2021. 1 [74] Paul-Edouard Sarlin, Cesar Cadena, Roland Siegwart, and Marcin Dymczyk. From coarse to fine: Robust hierarchical localization at large scale. In CVPR, 2019. 3 [75] Torsten Sattler, Bastian Leibe, and Leif Kobbelt. Efficient & effective prioritized matching for large-scale imagebased localization. IEEE TPAMI, 39(9), 2016. [76] Johannes L. Schonberger and Jan-Michael Frahm. Structure-From-Motion Revisited. In CVPR, 2016. 1, 2 [77] Thomas Schops, Johannes L. Schonberger, Silvano Galliani, Torsten Sattler, Konrad Schindler, Marc Pollefeys, and Andreas Geiger. Multi-View Stereo Benchmark With High-Resolution Images and Multi-Camera Videos. In CVPR, 2017. 6 [78] Noam Shazeer. GLU Variants Improve Transformer. arXiv preprint arXiv:2002.05202, 2020. 6 [79] You Shen, Zhipeng Zhang, Yansong Qu, and Liujuan Cao. FastVGGT: Training-Free Acceleration of Visual Geometry Transformer. arXiv preprint arXiv:2509.02560, 2025. 1, 2, [80] Jamie Shotton, Ben Glocker, Christopher Zach, Shahram Izadi, Antonio Criminisi, and Andrew Fitzgibbon. Scene Coordinate Regression Forests for Camera Relocalization in RGB-D Images. In CVPR, 2013. 3, 6, 8 [81] Noah Snavely, Steven M. Seitz, and Richard Szeliski. Photo tourism: exploring photo collections in 3D. In ACM Trans. Graph. (Proc. SIGGRAPH), 2006. 1, 2 [82] Noah Snavely, Steven M. Seitz, and Richard Szeliski. Modeling the World from Internet Photo Collections. IJCV, 80 (2), 2008. 2 [83] Julian Straub, Thomas Whelan, Lingni Ma, Yufan Chen, Erik Wijmans, Simon Green, Jakob J. Engel, Raul MurArtal, Carl Ren, Shobhit Verma, Anton Clarkson, Mingfei Yan, Brian Budge, Yajie Yan, Xiaqing Pan, June Yon, Yuyang Zou, Kimberly Leon, Nigel Carter, Jesus Briales, Tyler Gillingham, Elias Mueggler, Luis Pesqueira, Manolis Savva, Dhruv Batra, Hauke M. Strasdat, Renzo De Nardi, Michael Goesele, Steven Lovegrove, and Richard Newcombe. The Replica Dataset: Digital Replica of Indoor Spaces, 2019. arXiv:1906.05797 [cs]. 1 [84] Jurgen Sturm, Nikolas Engelhard, Felix Endres, Wolfram Burgard, and Daniel Cremers. benchmark for the evaluation of RGB-D SLAM systems. In Int. Conf. Intell. Robot. Syst., 2012. 7 [85] Jiaming Sun, Zehong Shen, Yuang Wang, Hujun Bao, and Xiaowei Zhou. LoFTR: Detector-Free Local Feature Matching With Transformers. In CVPR, pages 89228931, 2021. 1 [86] Yu Sun, Xiaolong Wang, Zhuang Liu, John Miller, Alexei Efros, and Moritz Hardt. Test-Time Training with SelfSupervision for Generalization under Distribution Shifts. In Int. Conf. Mach. Learn., 2020. 3, 4 [87] Yutao Sun, Li Dong, Shaohan Huang, Shuming Ma, Yuqing Xia, Jilong Xue, Jianyong Wang, and Furu Wei. Retentive network: successor to transformer for large language models. arXiv preprint arXiv:2307.08621, 2023. 3 [88] Yu Sun, Xinhao Li, Karan Dalal, Jiarui Xu, Arjun Vikram, Genghan Zhang, Yann Dubois, Xinlei Chen, Xiaolong Wang, Sanmi Koyejo, Tatsunori Hashimoto, and Carlos Guestrin. Learning to (Learn at Test Time): RNNs with Expressive Hidden States. arXiv preprint arXiv:2407.04620, 2025. 2, 3, 4, 5 [89] Fabio Tosi, Yiyi Liao, Carolin Schmitt, and Andreas Geiger. SMD-Nets: Stereo Mixture Density Networks. In CVPR, pages 89428952, 2021. [90] Basile Van Hoorick, Rundi Wu, Ege Ozguroglu, Kyle Sargent, Ruoshi Liu, Pavel Tokmakov, Achal Dave, Changxi Zheng, and Carl Vondrick. Generative Camera Dolly: Extreme Monocular Dynamic Novel View Synthesis. In Computer Vision ECCV 2024, pages 313331, Cham, 2025. Springer Nature Switzerland. 1 [91] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan Gomez, Ł ukasz Kaiser, and Illia Polosukhin. Attention is All you Need. In NeurIPS, 2017. 2, 4 [92] Chung-Shien Brian Wang, Christian Schmidt, Jens Piekenbrinck, and Bastian Leibe. Faster VGGT with Block-Sparse Global Attention. arXiv preprint arXiv:2509.07120, 2025. 1, 2, 6 [93] Hengyi Wang and Lourdes Agapito. 3D Reconstruction with Spatial Memory. In 3DV, 2025. 3, 6 [94] Junxiong Wang, Daniele Paliotta, Avner May, Alexander M. Rush, and Tri Dao. The Mamba in the Llama: Distilling and Accelerating Hybrid Models. NeurIPS, 2024. 3 [95] Jianyuan Wang, Minghao Chen, Nikita Karaev, Andrea Vedaldi, Christian Rupprecht, and David Novotny. VGGT: Visual Geometry Grounded Transformer. In CVPR, 2025. 1, 2, 3, 4, 5, [96] Qianqian Wang, Yifei Zhang, Aleksander Holynski, Alexei A. Efros, and Angjoo Kanazawa. Continuous 3D Perception Model with Persistent State. In CVPR, 2025. 3, 6 [97] Shuzhe Wang, Vincent Leroy, Yohann Cabon, Boris Chidlovskii, and Jerome Revaud. DUSt3R: Geometric 3D Vision Made Easy. In CVPR, 2024. 1, 2 [98] Wenshan Wang, Delong Zhu, Xiangwei Wang, Yaoyu Hu, Yuheng Qiu, Chen Wang, Yafei Hu, Ashish Kapoor, and Sebastian Scherer. TartanAir: Dataset to Push the Limits of Visual SLAM. In Int. Conf. Intell. Robot. Syst., pages 49094916, 2020. ISSN: 2153-0866. 1 [114] Tianyuan Zhang, Sai Bi, Yicong Hong, Kai Zhang, Fujun Luan, Songlin Yang, Kalyan Sunkavalli, William T. Freeman, and Hao Tan. Test-Time Training Done Right. arXiv preprint arXiv:2505.23884, 2025. 3, 6 [115] Qunjie Zhou, Sergio Agostinho, Aljoˇsa Oˇsep, and Laura Leal-Taixe. Is geometry enough for matching in visual localization? In ECCV, 2022. 3 [116] Dong Zhuo, Wenzhao Zheng, Jiahe Guo, Yuqi Wu, Jie Zhou, and Jiwen Lu. Streaming 4D Visual Geometry Transformer. arXiv preprint arXiv:2507.11539, 2025. [99] Yifan Wang, Jianjun Zhou, Haoyi Zhu, Wenzheng Chang, Yang Zhou, Zizun Li, Junyi Chen, Jiangmiao Pang, Chunhua Shen, and Tong He. π3: Permutation-Equivariant Visual Geometry Learning. arXiv preprint arXiv:2507.13347, 2025. 1, 2 [100] Kyle Wilson and Noah Snavely. Robust Global Translations with 1DSfM. In ECCV, 2014. 5 [101] Yuqi Wu, Wenzhao Zheng, Jie Zhou, and Jiwen Lu. Point3R: Streaming 3D Reconstruction with Explicit SpaarXiv preprint arXiv:2507.02863, tial Pointer Memory. 2025. 3 [102] Hongchi Xia, Yang Fu, Sifei Liu, and Xiaolong Wang. RGBD Objects in the Wild: Scaling Real-World 3D Object In CVPR, pages 22378 Learning from RGB-D Videos. 22389, 2024. 1 [103] Jianing Yang, Alexander Sax, Kevin J. Liang, Mikael Henaff, Hao Tang, Ang Cao, Joyce Chai, Franziska Meier, and Matt Feiszli. Fast3R: Towards 3D Reconstruction of 1000+ Images in One Forward Pass. In CVPR, 2025. [104] Songlin Yang and Yu Zhang. Fla: triton-based library for hardware-efficient implementations of linear attention mechanism, 2024. 3 [105] Songlin Yang, Bailin Wang, Yikang Shen, Rameswar Panda, and Yoon Kim. Gated linear attention transformers with hardware-efficient training. arXiv preprint arXiv:2312.06635, 2023. 3 [106] Songlin Yang, Bailin Wang, Yu Zhang, Yikang Shen, and Yoon Kim. Parallelizing Linear Transformers with the Delta Rule over Sequence Length. In NeurIPS, 2024. 3, 5 [107] Songlin Yang, Jan Kautz, and Ali Hatamizadeh. Gated Delta Networks: Improving Mamba2 with Delta Rule. In ICLR, 2025. 3, 5 [108] Yao Yao, Zixin Luo, Shiwei Li, Jingyang Zhang, Yufan Ren, Lei Zhou, Tian Fang, and Long Quan. BlendedMVS: Large-Scale Dataset for Generalized Multi-View Stereo Networks. In CVPR, pages 17901799, 2020. 1 [109] Chandan Yeshwanth, Yueh-Cheng Liu, Matthias Nießner, and Angela Dai. ScanNet++: High-Fidelity Dataset of 3D Indoor Scenes. In ICCV, pages 1222, 2023. 1 [110] Amir R. Zamir, Alexander Sax, William Shen, Leonidas J. Guibas, Jitendra Malik, and Silvio Savarese. Taskonomy: In CVPR, pages Disentangling Task Transfer Learning. 37123722, 2018. [111] Michael Zhang, Kush Bhatia, Hermann Kumbong, and Christopher Re. The Hedgehog & the Porcupine: Expressive Linear Attentions with Softmax Mimicry. arXiv preprint arXiv:2402.04347, 2024. 3 [112] Michael Zhang, Simran Arora, Rahul Chalamala, Alan Wu, Benjamin Spector, Aaryan Singhal, Krithik Ramesh, and Christopher Re. LoLCATs: On Low-Rank Linearizing of Large Language Models. arXiv preprint arXiv:2410.10254, 2025. 3, 8 [113] Shangzhan Zhang, Jianyuan Wang, Yinghao Xu, Nan Xue, Christian Rupprecht, Xiaowei Zhou, Yujun Shen, and GorFLARE: Feed-Forward Geometry, Apdon Wetzstein. pearance and Camera Estimation from Uncalibrated Sparse Views. In CVPR, 2025. 1, 2 7 Figure 6. Qualitative comparison. From left to right: VGGT, TTT3R, VGG-T3 (Ours) Figure 7. Visual localization examples in Wayspots and 7scenes. Ground truth camera for query image (not used for reconstruction) shown on the left in green, predicted camera and geometry in red. 9 Figure 8. In-the-wild visual localization. We reconstruct sequence of the KITTI dataset, then localize tourist picture that was recorded 7 years later. Note the changes in appearance and composition of the scene. 10 (a) Similar reconstruction as VGGT. (b) Failure cases. Figure 9. Waymo sequence reconstructions comparison with VGGT."
        }
    ],
    "affiliations": [
        "NVIDIA",
        "University of Toronto",
        "Vector Institute"
    ]
}