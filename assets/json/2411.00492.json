{
    "paper_title": "Multi-expert Prompting Improves Reliability, Safety, and Usefulness of Large Language Models",
    "authors": [
        "Do Xuan Long",
        "Duong Ngoc Yen",
        "Anh Tuan Luu",
        "Kenji Kawaguchi",
        "Min-Yen Kan",
        "Nancy F. Chen"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "We present Multi-expert Prompting, a novel enhancement of ExpertPrompting (Xu et al., 2023), designed to improve the large language model (LLM) generation. Specifically, it guides an LLM to fulfill an input instruction by simulating multiple experts, aggregating their responses, and selecting the best among individual and aggregated responses. This process is performed in a single chain of thoughts through our seven carefully designed subtasks derived from the Nominal Group Technique (Ven and Delbecq, 1974), a well-established decision-making framework. Our evaluations demonstrate that Multi-expert Prompting significantly outperforms ExpertPrompting and comparable baselines in enhancing the truthfulness, factuality, informativeness, and usefulness of responses while reducing toxicity and hurtfulness. It further achieves state-of-the-art truthfulness by outperforming the best baseline by 8.69% with ChatGPT. Multi-expert Prompting is efficient, explainable, and highly adaptable to diverse scenarios, eliminating the need for manual prompt construction."
        },
        {
            "title": "Start",
            "content": "Multi-expert Prompting Improves Reliability, Safety and Usefulness of Large Language Models Do Xuan Long1,2, Duong Ngoc Yen3, Luu Anh Tuan3, Kenji Kawaguchi1, Min-Yen Kan1, Nancy F. Chen2 1National University of Singapore, 2Institute for Infocomm Research (I2R), A*STAR, 3Nanyang Technological University xuanlong.do@u.nus.edu, {kenji,knmnyn}@nus.edu.sg, nfychen@i2r.a-star.edu.sg, {ngocyen001@e.,anhtuan.luu@}ntu.edu.sg 4 2 0 2 1 ] . [ 1 2 9 4 0 0 . 1 1 4 2 : r a"
        },
        {
            "title": "Abstract",
            "content": "We present Multi-expert Prompting1, novel enhancement of ExpertPrompting (Xu et al., 2023), designed to improve the large language model (LLM) generation. Specifically, it guides an LLM to fulfill an input instruction by simulating multiple experts, aggregating their responses, and selecting the best among individual and aggregated responses. This process is performed in single chain of thoughts through our seven carefully designed subtasks derived from the Nominal Group Technique (Ven and Delbecq, 1974), well-established decision-making framework. Our evaluations demonstrate that Multi-expert Prompting significantly outperforms ExpertPrompting and comparable baselines in enhancing the truthfulness, factuality, informativeness, and usefulness of responses while reducing toxicity and hurtfulness. It further achieves state-ofthe-art truthfulness by outperforming the best baseline by 8.69% with ChatGPT. Multi-expert Prompting is efficient, explainable, and highly adaptable to diverse scenarios, eliminating the need for manual prompt construction."
        },
        {
            "title": "Introduction",
            "content": "Pre-trained large language models (LLMs) (Radford et al., 2019; Brown et al., 2020; Chowdhery et al., 2022; OpenAI, 2022; Touvron et al., 2023) acquire extensive knowledge during training, demonstrating exceptional abilities as general-purpose problem solvers. As they have made increasing impacts on human life, it is essential to ensure these systems align with human intentions by improving their reliability, safety, and usefulness to meet users expectations (Wang et al., 2023b). Among the alignment methods, recent studies (Li et al., 2023a; Park et al., 2023; Do et al., 2023; Wang et al., 2024) highlight that LLMs can mimic expected behaviors of specific agents when cast Equal contribution. 1Our codes and data will be made publicly available here. Figure 1: An overview of Multi-expert Prompting with an ExpertQA (Malaviya et al., 2023) example. ExpertPrompting (Xu et al., 2023) provides one-sided view, concluding unethical\" while Multi-expert Prompting encompasses multiple viewpoints leading to comprehensively multifaceted answer. with sufficient descriptions. This leads to better generation outcomes and enhances user interactions. Notably, Xu et al. (2023) introduce ExpertPrompting directing LLMs to answer questions as generated experts. This strategy further proves its effectiveness when ExpertLLaMA trained on its data achieves 96% of the ChatGPTs capability. However, is relying on single expert LLM sufficient for diverse user queries? Our answer is no. Single expert frameworks like ExpertPrompting fall short of open-ended instructions with multiple valid perspectives. For instance, in response to the question Is it ethical to eat meat? in Fig. 1, ExpertPrompting casts the LLM as an Ethicist offering simplistic answer, labeling it as unethical. This approach introduces bias and dismissive attitude towards other perspectives, such as those of non-vegetarians. Ideally, responses to such questions should encompass various other viewpoints addressing multiple dimensions of the issue, such as nutritional and environmental aspects. This highlights that single expert can introduce biases and limit the depth needed for considering varied perspectives in addressing open-ended instructions. Inspired by the above observation, we present novel and efficient extension of ExpertPrompting named Multi-expert Prompting, which addresses the need for multiple perspectives. It involves two main steps  (Fig. 2)  . First, given an input instruction, Multi-expert Prompting instructs an LLM to generate expert identities with their concise, onesentence role descriptions tailored to the instruction in zero-shot prompting style. Unlike ExpertPrompting (Xu et al., 2023), which relies on generating detailed role descriptions by few-shot hand-crafted demonstrations, our approach does not require demonstrations and is more versatile as detailed descriptions are unnecessary (6.1). Multiexpert Prompting then casts the LLM as distinct experts, each responding to the instruction independently. Second, it chooses single best response by aggregating the individual responses and evaluating it together with individual ones through novel, seven-subtask method in single chain of thought (Wei et al., 2022) following Nominal Group Technique (NGT; Ven and Delbecq, 1974). Multi-expert Prompting is related to recent efforts in reasoning over multi-agent responses, such as Multi-agent Debate (Liang et al., 2023) and Universal Self-consistency (USC) (Chen et al., 2023b). It distinguishes itself by aggregating expert responses in single turn without iterative refinement. Moreover, its response aggregation is based on the human-designed NGT framework, contrasting with the LLM-generated plans in AutoGen (Wu et al., 2023) and AutoAgents (Chen et al., 2023a). Finally, it differs from MetaGPT (Hong et al., 2023) by employing diverse domain experts to address questions in parallel, instead of in sequence. Multi-expert Prompting is the first to tackle the challenge of aggregating multi-agent long-form responses in single turn based on well-studied perspectives from management sciences. It significantly outperforms baselines in improving the truthfulness, factuality, toxicity, hurtfulness, informativeness, and usefulness of LLMs by leveraging only three experts, achieving state-of-the-art truthfulness. In addition, it is highly adaptable, explainable, and beneficial for open-ended tasks where diverse expert opinions are valued."
        },
        {
            "title": "2 Background",
            "content": "We introduce ExpertPrompting (Xu et al., 2023) and the Nominal Group Technique (NGT) (Ven and Delbecq, 1974), both serving as foundational elements for Multi-expert Prompting. ExpertPrompting (Xu et al., 2023). ExpertPrompting is prompting technique designed to enhance the responses of an LLM by leveraging the models capability to answer as experts. Given an input instruction, it begins by prompting the LLM to generate paragraph-long expert identity that best fulfills the instruction through carefully crafted few-shot demonstrations. Then, it directs the LLM to respond as the generated expert. However, it can bias the models response toward the generated expert critical weakness  (Fig. 1)  . Nominal Group Technique (NGT) (Ven and Delbecq, 1974). The NGT is structured decisionmaking process that aids teams in identifying problems and generating solutions. It effectively organizes group ideas, combining individual judgments, particularly useful in scenarios marked by uncertainty or disagreement. Widely utilized in business and government, NGT typically involves 4 steps: NGT 1. Idea generation. Each team member independently writes down their ideas. NGT 2. Round-robin idea recording. Ideas are shared in round-robin fashion and recorded for all to see without discussion and elaboration. NGT 3. Discussion of the list of ideas. The participants discuss each idea on the list so that they are clear about the meaning of the ideas. NGT 4. Voting. Members identify key ideas, rank-order preferences (optional), record votes (agreements, conflicts), and discuss the voting."
        },
        {
            "title": "3 Multi-expert Prompting",
            "content": "In deployment, when presented with an input instruction I, an LLM is expected to generate response while ensuring informativeness, usefulness, truthfulness, non-toxicity, factuality, and nonhurtfulness. Multi-expert Prompting is designed for this goal and consists of two steps: (1) Experts & responses generation and (2) Expert responses aggregation. In the first step, is instructed to generate experts {(E1, D1), ..., (En, Dn)} with Ei as the i-th expert identity and Di as its description. It is then executed times as each expert to respond to I, offering long-form expert responses, denoted as {A1, . . . , An}. In the second step, combines {A1, . . . , An} into Acomb and selects the best among Ai and Acomb as A. The steps details are below, and our detailed prompts and cost analysis are provided in Appx.-C. Let us denote GM : be the generation function of where is the model vocabulary. Figure 2: Overview of Multi-expert Prompting: (1) Experts & responses generation (3.1) and (2) Aggregating expert responses (3.2). Given an input instruction, the first step targets generating expert identities that best fulfill the instruction and expert responses, while the second step focuses on aggregating and selecting the best from individual and combined expert responses. 3.1 1st Step: Experts & Responses Generation 3.2 2nd Step: Expert Responses Aggregation Motivated by NGT 1 and 2, this step aims to simulate as multiple experts to generate expert answers independently. Given I, we first instruct to generate list of experts capable of answering thoroughly. Each ith expert is tuple of (Ei, Di) where Ei is the experts identity and Di is one-sentence description of its expertise and responsibilities. Formally: {(E1, D1), . . . , (En, Dn)} := GM([IE, I]) (1) where IE is the (expert, responsibility) pair generation instruction. We enforce three constraints on generating experts in Eq. (1) which are specified in IE: the experts should be diverse, Ei is general expert, and Di is its short clarification. For the first constraint, we promote diversity among experts to cultivate range of perspectives, enhancing the quality of the final response, as noted by Schulz-Hardt et al. (2000). Regarding the final constraint, Di is designed to be more versatile than the detailed descriptions used in ExpertPrompting (Xu et al., 2023), which relies on hand-crafted few-shot demonstrations, which we find unnecessary (6.1). For each expert, we ask the LLM to generate long-form answer A: Ai := GM([I, Ei, Di]) (2) Both Eqs. (1) and (2) are efficiently performed under the zero-shot setting. Aggregating responses long-form expert {a1, ..., an} into final one is challenging, even for humans. Motivated by NGT and prior studies (Wei et al., 2022; Khot et al., 2023), we argue that every expert should contribute to the final response. Thus, we decompose the task into seven well-designed subtasks aiming to identify commonalities, necessitate the consolidation of information, and resolve conflicts via majority voting. We weight all the experts equally to prevent blind trust in expert opinions, minimizing the groups vulnerability to biases (Önkal et al., 2009). Specifically, efficiently fulfills these subtasks in single zero-shot chain of thoughts (Kojima et al., 2022). Subtask 1 (S1): Generating agreed viewpoints. This subtask aims to establish consensus among experts answers, inspired by NGT 4. Specifically, the LLM generates viewpoints that more than half of the experts agree on. These are reliable and identified earliest to confirm widely accepted information, providing foundation for next steps. Subtask 2 (S2): Generating conflicted viewpoints. Given the diverse backgrounds of multiple experts, conflicts are inevitable. Identifying conflicted viewpoints is crucial to resolving the conflicts. Hence, the LLM lists the conflicted viewpoints with specified expert identities in detail for the subsequent resolution. Subtask 3 (S3): Resolving the conflicts in S2. Resolving the above conflicts is critical for correction purposes and reducing experts biases, following NGT 4. We instruct the LLM to address the disagreements using its knowledge by reviewing the agreed viewpoints in S1 to judge conflicted viewpoints carefully. Subtask 4 (S4): Generating isolated viewpoints. Viewpoints that are not identified by S1 and S3, and are unique from each response, are now generated. These unique perspectives can provide valuable information without being conflicted among experts. They are crucial to ensure diverse, comprehensive, and insightful response. Subtask 5 (S5): Collecting S1, S3, S4 viewpoints. The LLM collects the viewpoints obtained from S1, S2, and S4 which appear in the final aggregated response. This step ensures transparency and explainability of the arguments included in the final response. Subtask 6 (S6): Generating the aggregated response. The LLM composes comprehensive response by integrating the viewpoints gathered from S5 as the experts aggregated response. Subtask 7 (S7): Select the best among the aggregated and individual expert responses. The aggregated response in S6 may not be optimal. If majority of experts provide poor answers, the aggregated answer may suffer. Thus, this step is designed to choose the best among individual expert answers and the aggregated one, focusing on factual accuracy and usefulness. Importantly, this step does not generate new answer, nor does it reveal evaluation metrics; it simply selects the most factual and useful response for all tasks. In summary, Multi-expert Prompting composes response by merging common, resolved-conflict, and unique viewpoints, following the NGT model. It further selects the best response from individual experts and the merged response, crucial for avoiding poor merged outcomes. Our human evaluation shows that the zero-shot performance of benchmarked LLMs is good enough. However, for more complex aggregations requiring specific formats, we recommend one-/few-shot prompting."
        },
        {
            "title": "4 Evaluation",
            "content": "We show that Multi-expert Prompting greatly improves reliability and safety (4.1) and the informativeness and usefulness (4.2) over the baselines. Self-refine (Madaan et al., 2023) which interactively utilizes LLMs to feedback and refine the response; (B4) Universal Self-consistency (Chen et al., 2023b) which prompts LLMs to generate multiple responses and selects the most consistent; (B5) Multi-agent Debate (Liang et al., 2023) which simulates two agents with opposing perspectives engaging in several rounds of debate to refine the response; and the aforementioned (B6) ExpertPrompting (Xu et al., 2023). Furthermore, three Multi-expert Prompting variants are also assessed where our first step (3.1) is altered: (B7) Fixed Temp. + Our Aggregation uses single temperature to sample responses; (B8) Var Temp. + Our Aggregation samples responses by varying temperatures; (B9) ExpertPrompting + Our Aggregation generates responses with one expert identity found by ExpertPrompting. Our experiments are conducted on two strong openand closed-source LLMs: ChatGPT (gpt-3.5-turbo-0613) (OpenAI, 2022) and Mistral (-7B-it v0.2) (Jiang et al., 2023). Details are provided in Appx.-B. Metrics. We evaluate the methods on six criteria for long-form generation tasks: (C1) Truthfulness measuring how models imitate human falsehoods; (C2) Factuality verifying the factuality; (C3) Toxicity assessing the toxicity biases; (C4) Hurtfulness examining the hurtfulness; (C5) Informativeness concerning the details, in-depth insights, multiple perspectives, and supporting evidence provided; (C6) Usefulness verifying the effectiveness in expressing the ideas and conveying the information. 4.1 Multi-expert Prompting Improves Reliability and Safety Setup. We evaluate the (C1) Truthfulness on TruthfulQA-Generation (Lin et al., 2022), (C2) Factuality on FactualityPrompt (Lee et al., 2022), (C3) Toxicity on BOLD (Dhamala et al., 2021), and (C4) Hurtfulness on HONEST (Nozza et al., 2021). We record the True percentage (by using fine-tuned ChatGPT judge) for TruthfulQA, Hallucinated NE Error Factual/Non-factual for FactualityPrompt, Toxicity percentage for BOLD and HurtLex for Queer/Nonqueer HONEST, following HuggingFace Evaluate (Von Werra et al., 2022). We discuss more benchmark details in Appx.-E. Baselines. We compare Multi-expert Prompting with six strong baselines: (B1) Zero-shot; (B2) Zero-shot-CoT (Kojima et al., 2022); (B3) Results. Tab. 1 presents our main experimental results, revealing four key findings. First, Multiexpert Prompting substantially improves truthfulModel Abb. Baselines TruthfulQA FactualityPrompt BOLD HONEST 2 . 0 . I - 7 - t M G C B1 B2 B3 B4 B5 B6 B7 B8 B9 Zero-shot Zero-shot-CoT Self-refine Universal Self-consistency Multi-agent Debate ExpertPrompting Fixed Temp. + Our Agg. Var Temp. + Our Agg. ExpertPrompting + Our Agg. 76.00 78.70 81.88 81.64 80.78 80. 80.19 81.68 79.32 Ours Multi-expert Prompting 87.15 B1 B2 B3 B4 B5 B6 B7 B8 B9 Zero-shot Zero-shot-CoT Self-refine Universal Self-consistency Multi-agent Debate ExpertPrompting Fixed Temp. + Our Agg. Var Temp. + Our Agg. ExpertPrompting + Our Agg. 68.05 70.38 75.89 77.11 64.87 80.66 78.38 72.21 80.54 8.98/16.07 9.28/14.87 10.36/14.95 9.98/15.21 17.57/18.27 11.43/15.32 9.31/15.44 8.23/14.72 8.42/18.38 8.16/14. 6.99/12.90 6.93/13.75 7.11/13.96 5.51/9.71 5.64/13.06 5.64/15.66 6.46/10.14 5.46/12.15 6.46/16.62 Ours Multi-expert Prompting 89.35 4.54/9.45 0.000 0.000 0.000 0.000 0.000 0. 0.000 0.000 0.000 0.000 0.163 0.163 0.064 0.000 0.000 0.129 0.084 0.163 0.123 0.000 0.012/0.009 0.014/0.013 0.007/0.008 0.007/0.008 0.004/0.007 0.005/0. 0.005/0.006 0.008/0.006 0.004/0.004 0.003/0.005 0.038/0.023 0.006/0.005 0.006/0.007 0.010/0.008 0.005/0.004 0.004/0.004 0.007/0.008 0.004/0.004 0.005/0.005 0.004/0.003 Table 1: Main experimental results. Overall, Multi-expert Prompting significantly outperforms the baselines, particularly on the TruthfulQA dataset (Lin et al., 2022), underscoring the effectiveness of our method in integrating multiple expert perspectives. denotes our model outperforms significantly with p-value < 0.01 under the t-test. ness, outperforming the best baselines (B3 for Mistral and B6 for ChatGPT) by 5.27% and 8.69% with Mistral and ChatGPT, respectively. It achieves new state-of-the-art on TruthfulQA-Generation with ChatGPT, surpassing the current SOTA of 87.97% (Li et al., 2023b). We explain the significant truthfulness improvement with the democratic theory (Cunningham, 2002): aggregated output moderated by multiple experts positively contributes to higher truthfulness. Second, by incorporating diverse expert perspectives, Multiexpert Prompting corrects experts biases, eliminates harmful elements, significantly enhances factuality, completely eliminates toxic content, and reduces hurtfulness. Third, compared to B79, which use different strategies for generating multiple responses, Multi-expert Prompting consistently achieves superior results, indicating the effectiveness of our first step. Fourth, any form of multiple expert prompting exhibit comparable or better results over ExpertPrompting and Zero-shot baselines alone, affirming the importance of aggregation in our second step. 4.2 Multi-expert Prompting Enhances Informativeness and Usefulness Setup. We evaluate (C5) Informativeness and (C6) Usefulness of Multi-expert Prompting in openended scenarios where no ground-truth answers Figure 3: (C5) Informativeness and (C6) Usefulness comparisons between Multi-expert Prompting and baselines on ExpertQA dataset (Malaviya et al., 2023). exist and multiple long-form responses are correct. We collect all open-ended questions from ExpertQA (Malaviya et al., 2023) consisting of 528 questions in 32 topics. Metrics C5 and C6 are computed automatically via the Win/Draw/Lose comparison between Multi-expert Prompting and other baselines by ChatGPT, found to be an effective evaluator (Wang et al., 2023a). We include the evaluation prompts in Appx.-D. Results. Fig. 3 illustrates our informativeness and usefulness evaluation results. We observe that Multi-expert Prompting generates significantly more informative (75% win on average) and useful (76.5%) responses, compared to the baselines. For both models, it gains the least informativeness win Model TruthfulQA (M1/M2) BOLD (M1/M2) ExpertQA (M1/M2) Avg. (M1/M2) ChatGPT 2.49/2.78 2.45/2.91 2.59/2.78 2.51/2.82 Mistral 2.75/2.67 2.94/2.89 2.78/2.87 2.82/2.81 Annotators Agr. 0.71/0. 0.63/0.82 0.71/0.73 0.68/0.77 Table 2: Human evaluation results. We measure the annotators agreements by Krippendorffs alpha (Krippendorff, 2011). Method Skip S1 Skip S2 & S3 Skip S4 Skip Naïve Agg. Enhanced Naïve Agg. Ours TruthfulQA FactualityPrompt BOLD HONEST 85.43 87.51 86.90 88.46 82.37 83.17 89. 6.49/10.45 4.89/10.31 5.93/9.28 5.19/8.44 5.30/10.52 6.97/12.12 4.54/9.45 0.064 0.000 0.064 0.000 0.055 0.072 0.008/0.004 0.005/0.003 0.010/0.005 0.004/0. 0.005/0.005 0.005/0.006 0.000 0.004/0.003 Table 3: Multi-expert Prompting when different subtasks are omitted using ChatGPT: all results decline, emphasizing the necessity of every step within the framework. over ExpertPrompting ((1) and (2) in Fig. 3) and usefulness over USC and ExpertPrompting ((3) and (4)). This is because, for certain questions, the perspective of single expert is sufficiently accurate, as illustrated in (e.g., Appx.-Fig. 18). Additionally, we conduct human investigation of ChatGPTs evaluation comparing Multi-expert Prompting and ExpertPrompting. Our investigation indicates high agreement rate of 93% between the annotator and ChatGPT on average over two metrics, confirming its reliable evaluation."
        },
        {
            "title": "5 Human Evaluation and Analyses",
            "content": "Human evaluation is essential for assessing the subtask performance of models in Multi-expert Prompting, as no automated metrics exist for this purpose. We conduct human evaluation to validate its two steps: 1st Step: Experts & response generation (3.1); 2nd Step: Aggregating expert responses (3.2) with = 3 experts. We randomly select 100 samples generated by ChatGPT and Mistral from each of TruthfulQA, BOLD, and ExpertQA representing all our tasks. Three excellent undergraduates who are native English speakers are hired to rate the generation of the two steps through two metrics on scale of 13: (M1) Expert Generation Satisfaction for our first step measures whether the three generated experts are diverse and helpful, and (M2) Aggregation Satisfaction for the second step assesses how well the models perform the seven subtasks in 3.2. The grading policies are in Appx.-F. We discuss our findings here while examples supporting our arguments are provided in Appx.-G. Overall, Mistral excels in both steps, while ChatGPT exhibits notable deficiency in the initial stage of generating experts. Specifically, Mistral outperforms ChatGPT significantly in expert generation. Among the three experts generated by ChatGPT, we observe 27% incidence where one expert proves less helpful (e.g., Appx.-Fig. 20) and an 11% occurrence where two experts are less helpful (e.g., Appx.-Fig. 21), on average. On the flip side, ChatGPT marginally outperforms Mistral in executing our 7 subtasks. Within the 7 subtasks, both models demonstrate proficiency in subtasks S1 and S5-S7. Although both occasionally misinterpret divergent viewpoints (S2) (e.g., Appx.-Fig. 22), they excel in resolving these discrepancies (S3). Additionally, both models face challenges in extracting unique viewpoints (S4), likely due to the tasks inherent complexity. Lastly, our annotators achieve commendable agreement α = 0.73. 5.1 Analyses We now present our core methodological analyses, covering ablation studies, the impact of the number of experts, and the ratio of best response to be the combined one. We supplement fine-grained analyses, distribution of generated experts, and the performance of Multi-expert Prompting in reasoning tasks in Appx.-A. Ablations studies. The ablation study for the 1st Step of Multi-expert Prompting corresponds to the baseline (B7) explored in 4. Subsequently, we investigate the ablation of subtasks in its 2nd Step. Specifically, we examine the skipping of S1, S2, S3, S4, and S7 (3.2). Subtasks S5 and S6, categorized as bridging subtasks, do not undergo ablation. We compare Multi-expert Prompting with (B10) Naïve Agg., where LLMs naïvely aggregate expert responses via Please combine responses into final one\" before selecting the best one. We further enhance the (B10), termed (B11) Enhanced Naïve Agg. by instructing the model to ensure that the aggregated response is truthful, factual, less toxic, and less hurtful on the TruthfulQA, FactualityPrompt, BOLD, and HONEST benchmarks. Tab. 3 shows that skipping S1 and S4 impairs performance the most, underscoring the importance of common and unique viewpoints. S2 and S3 also significantly contribute to performance, highlighting the importance of conflict resolution. S7 #experts TruthfulQA FactualityPrompt BOLD HONEST Model Method TruthfulQA BOLD ExpertPrompting 1 2 3 (Ours) 5 10 80.67 80.05 88.00 89.35 85.92 84.82 5.64/15.66 5.13/10.75 5.17/9.57 4.54/9.45 4.90/10.89 6.24/10. 0.109 0.004/0.004 0.129 0.000 0.000 0.000 0.000 0.011/0.006 0.005/0.003 0.004/0.003 0.009/0.008 0.004/0.004 Table 4: Multi-expert Prompting with varying numbers of experts using ChatGPT. Three experts perform the best overall. Model TruthfulQA FactualityPrompt BOLD HONEST ExpertQA Mistral ChatGPT 95.35 95.44 99. 92.40 98.71 100 97.45 99.86 99. 97.53 Table 5: Percentage of test samples that LLMs select aggregated response instead of individual experts responses using Multi-expert Prompting with = 3 experts. contributes marginally, indicating high-quality aggregated responses. B10 and B11 perform notably worse than Multi-expert Prompting, confirming the effectiveness of its second step. Number of experts. We explore the impact of the number of experts in Multi-expert Prompting performance. Tab. 4 presents ChatGPT results using Multi-expert Prompting with varying expert counts. We observe that 3 experts yield the best truthful, factual, least harmful results, while 2 experts significantly decreases toxicity. This mirrors reality where excessive expert input may divert humans from obtaining the most truthful and factual output. Meanwhile, utilizing numerous safe responses from safety fine-tuned models like ChatGPT can minimize toxicity details in the output. Ratios of the best response selected to be the aggregated response. To assess the quality of the aggregated responses, we record the proportion of test samples where the aggregated response is selected by models over individual expert responses in Tab. 5. Notably, both models consistently favor the combined response in over 90% of cases, highlighting their superior quality over experts ones."
        },
        {
            "title": "6 Discussion",
            "content": "We discuss the underlying reasons for Multi-expert Promptings effectiveness and address its design choices. 6.1 Why does Multi-expert Prompting Work? Short versus long expert description. We investigate why one-sentence description for an expert identity is effective, compared to paragraph-long description as used in ExpertPrompting (Xu et al., Mistral One-sentence expert (Ours) ExertPrompting ChatGPT One-sentence expert (Ours) ExertPrompting 80.55 80.34 80.60 80.67 0.00 0.00 0.101 0.109 Table 6: Performance of ExpertPrompting when using onesentence or paragraph-long expert description. 2023). After generating experts with Multi-expert Prompting, we randomly select one expert identity and compare the impact of its one-sentence description to its paragraph-long counterpart generated through ExpertPrompting. The results, shown in Tab. 6 indicate that the performance difference between the two methods is negligible, suggesting that long-form descriptions are unnecessary. Aggregated response versus expert response: Why is Multi-expert Prompting better than the baselines? The aggregated response of Multiexpert Prompting offers several advantages over individual expert responses (3.2) by considering not only common viewpoints but also resolvedconflict and unique viewpoints. To illustrate this, we examine TruthfulQA case (Lin et al., 2022) in Fig. 4. In this scenario, both the Superstition expert and the Folklore historian provide plausible answers that are, however, incorrect when compared to the ground truth. By contrast, Multi-expert Prompting excels by integrating not only common perspectives, such as bad luck (which is incorrect according to the ground truth) but also unique expert insights. Crucially, the Animal behaviorist asserts that superstition has no real impact, which Multi-expert Prompting incorporates, resulting in comprehensive and accurate answer. Finally, in this case, both USC and Multi-agent Debate conclude that it brings bad luck, while only Multi-expert Prompting arrives at the correct answer. 6.2 Directly Asking LLMs to be Truthful, Factual, less Toxic, less Hurtful We investigate if directly instructing LLMs to be factual and useful during generation improves performance, potentially altering Multi-expert Prompting. Our findings confirm that this approach enhances the baseline prompting technique. However, it still falls significantly short of Multi-expert Promptings performance. Specifically, we compare Multi-expert Prompting with six variants of Zero-shot CoT (Kojima et al., 2022) by adding more constraints: we diFigure 4: TruthfulQA (Lin et al., 2022) example where Multi-expert Prompting provides the correct answer, while the majority of experts answer incorrectly according to the ground-truth. This demonstrates its advantage in considering not only common but also unique expert viewpoints. Zero-shot Zero-shot CoT Self-refine ExpertPrompting Multi-expert Prompting ChatGPT Mistral 28.00 60.97 53.82 46.88 62.15 46.99 76.49 49.65 56.00 167. Figure 5: Comparison between Multi-expert Prompting, the baseline, and the baseline with constraints. Table 7: Avg. #tokens in answers generated for ExpertQA open-ended questions. The tokenizer is from NLTK2 package. rectly instruct the LLMs to be more truthful on TruthfulQA, more factual on FactualityPrompt, less toxic on BOLD, less hurtful on HONEST, and more informative and useful on ExpertQA. We utilize both Mistral and ChatGPT, averaging their performance and plotting in Fig. 5, with the numerical details provided in Appx.-Tab. 8. We observe that incorporating more constraints significantly reduces toxicity and hurtfulness while slightly improving truthfulness. However, adding constraints still lags significantly behind Multi-expert Prompting. 6.3 Are Informativeness and Usefulness the Results of Output Longiness? To inspect whether the high (C5) Informativeness and (C6) Usefulness scores achieved by Multiexpert Prompting are due to the lengthy responses, we record the average #tokens in responses generated on ExpertQA presented in Tab. 7. Our answer is no: longer responses do not necessarily equate to being more informative or useful. (1) For ChatGPT, Zero-shot CoT and Multi-expert Prompting generate answers with similar lengths (60.97 and 62.15). However, Zero-shot CoTs (C5) and (C6) scores were significantly lower compared to Multi-expert Prompting, indicating that longer answers do not necessarily equate to being more informative and useful. (2) For Mistral, Multi-expert Prompting has significantly higher number of tokens compared with other baselines. Therefore, we compare it with Zero-shot CoT, Self-refine, and ExpertPrompting where we explicitly require the LLMs to output responses having 170 tokens. The results are in Fig. 6. Multi-expert Prompting outperforms Zeroshot CoT, Self-refine, and Zero-shot prompting on (C5), with ExpertPrompting slightly ahead. However, on (C6), Multi-expert Prompting surpasses all baselines. These verify that longer answers do not always lead to more informative or useful."
        },
        {
            "title": "7 Related Work",
            "content": "systems. Multi-agent Multi-agent systems (Shoham and Leyton-Brown, 2008) have long development history. notable early example"
        },
        {
            "title": "8 Conclusion",
            "content": "We introduce Multi-expert Prompting, an efficient method that simulates multiple experts within an LLM and aggregates their responses to improve generation. Drawing inspiration from the Nominal Group Technique, this approach pioneers in aggregating lengthy responses in LLM-powered multi-agent systems by well-studied human-design decision-making frameworks in single turn. Multi-expert Prompting is efficient, interpretable, and generalizable, possessing great potential for applications. In future, we plan to further generalize it to enhance group decision-making AI."
        },
        {
            "title": "Acknowledgement",
            "content": "This research is supported by the National Research Foundation Singapore under the AI Singapore Programme (AISG Award No: AISG2-TC2023-010SGIL) and the Singapore Ministry of Education Academic Research Fund Tier 1 (Award No: T1 251RES2207). DXL is supported by the A*STAR Computing and Information Science (ACIS) scholarship. We thank members of WING and Deep Learning Lab at NUS and the ACL RR anonymous reviewers for the constructive feedback."
        },
        {
            "title": "Limitations",
            "content": "Our method can undoubtedly be easily generalized to other long-form generation tasks. However, for short-form answering tasks such as True/False or short-form numerical reasoning tasks, its aggregation method may be unnecessary because the 7 subtasks are validly applicable to viewpoints. As such, to apply Multi-expert Prompting, we suggest the audiences generate reasoning thoughts together with the short-form answers via Chain-of-Thought (Wei et al., 2022; Kojima et al., 2022) or other similar techniques. In addition, Multi-expert Prompting requires the LLMs to have good instruction-following capability to perform role-playing and to solve our subtasks, and we use placeholder format to wrap the final selection answer (Long et al., 2024). We anticipate that these limitations are going to be overcome by recent and future state-of-the-art LLMs as LLMs are increasingly evolving in role-playing scenarios (Lu et al., 2024; Wang et al., 2024; Tseng et al., 2024) and instruction-following capabilities (Qin et al., 2024). Moreover, all expert opinions in Multi-expert Prompting are treated equally using the NomiFigure 6: Informativeness and usefulness comparison results between Multi-expert Prompting and other baselines with Mistral on ExpertQA dataset when we explicitly ask the model to generate responses having 170 tokens. is the Mixture-of-Experts (MoE) (Jacobs et al., 1991), which has influenced the design of modular language models such as Gshard (Lepikhin et al., 2020), DEMIX (Gururangan et al., 2022) and MoRE (Si et al., 2023). Recent advancements in large language models (LLMs) have spurred the development of prominent LLM-driven multi-agent systems, such as Multi-agent Debate (Liang et al., 2023), AutoGen (Wu et al., 2023), AutoAgents (Chen et al., 2023a), MetaGPT (Hong et al., 2023), and MATRIX (Xu et al., 2024c). Key design choices in these systems include the communication protocols among agents and the methods integrating their responses for decision-making. Multi-expert Prompting distinguishes itself as an LLM-based multi-agent framework by employing the Nominal Group Technique (NGT), structured and reliable human-designed decision-making process, to aggregate expert agents responses. In addition, Multi-expert Promptings response aggregation method is related to Self-consistency (Wang et al., 2022a), Universal Self-consistency (Chen et al., 2023b), and Automatic Model Selection (Zhao et al., 2023). However, it selects the best response from both the individual experts responses and their combination, rather than simply choosing among the experts responses. Role-playing with LLMs. Recent advancements have significantly enhanced capabilities in LLMs, which are crucial for developing role-playing agents. These agents are designed to simulate general or specific personas via training or input contexts (Deshpande et al., 2023; Do et al., 2023; Wang et al., 2024; Xu et al., 2024a; Wu et al., 2024). Multi-expert Prompting leverages the role-playing capabilities of LLMs to simulate multiple experts responding to input instructions. nal Group Technique, which may not reflect realworld scenarios accurately. Exploring methods for weighted aggregation of viewpoints is necessary to address this limitation effectively. Finally, Multi-expert Prompting can suffer from LLMs hallucinating expert identities and engaging in role-playing, especially in specific domains where the models are poorly trained. This issue can significantly impact the response quality of the multi-expert system and is particularly problematic in multi-agent systems (Yoffe et al., 2024). However, employing weighted aggregated viewpoints presents promising solution to this problem. Moreover, advancements in role-playing LLMs (Lu et al., 2024; Wang et al., 2022b) suggest that LLMs are becoming increasingly less prone to hallucination in role-playing scenarios."
        },
        {
            "title": "Ethical Considerations",
            "content": "Generating experts and casting LLMs as them can handle diverse user instructions powerfully, but theres risk of misuse and bias in certain situations. Ethical concerns arise when our method is applied to enable unethical actions or perpetuate biased scenarios. Bias Amplification and Fairness. The diversity of the generated experts is not fully controlled due to the models inherent knowledge, we have taken steps to enhance expert diversity generation by explicitly instructing the LLMs to produce diverse expert identities. Casting large language models (LLMs) as experts risks reinforcing existing biases, creating echo chambers, and amplifying unethical perspectives (Vicario et al., 2016). To counter this, Multi-expert Prompting addresses the problem by equally combining perspectives from multiple experts, avoiding reliance on single viewpoint, and minimizing the risk of reinforcing polarized or undesirable views. Our expert response aggregation process is designed to also minimize potential biases. The seven subtasks require the model to identify agreed-upon and conflicting viewpoints and then reconcile these differences. This systematic approach ensures viewpoint revisions only, without regenerating or refining viewpoints in way that might favor specific perspectives and amplify biases (Xu et al., 2024b). Human Evaluation. Through human evaluations, our proposed method does not generate any discriminatory or insulting responses. We meticulously validate each step of Multi-expert Prompting through manual labor, employing annotators who are compensated at an hourly rate of $15, exceeding the local statutory minimum wage. This proactive approach ensures ethical standards in our human evaluations, minimizing the likelihood of significant ethical concerns."
        },
        {
            "title": "References",
            "content": "Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini Agarwal, Ariel Herbert-Voss, Gretchen Krueger, Tom Henighan, Rewon Child, Aditya Ramesh, Daniel Ziegler, Jeffrey Wu, Clemens Winter, Chris Hesse, Mark Chen, Eric Sigler, Mateusz Litwin, Scott Gray, Benjamin Chess, Jack Clark, Christopher Berner, Sam McCandlish, Alec Radford, Ilya Sutskever, and Dario Amodei. 2020. In AdLanguage models are few-shot learners. vances in Neural Information Processing Systems, volume 33, pages 18771901. Curran Associates, Inc. Guangyao Chen, Siwei Dong, Yu Shu, Ge Zhang, Jaward Sesay, Börje Karlsson, Jie Fu, and Yemin Shi. 2023a. Autoagents: framework for automatic agent generation. arXiv preprint arXiv:2309.17288. Xinyun Chen, Renat Aksitov, Uri Alon, Jie Ren, Kefan Xiao, Pengcheng Yin, Sushant Prakash, Charles Sutton, Xuezhi Wang, and Denny Zhou. 2023b. Universal self-consistency for large language model generation. arXiv preprint arXiv:2311.17311. Aakanksha Chowdhery, Sharan Narang, Jacob Devlin, Maarten Bosma, Gaurav Mishra, Adam Roberts, Paul Barham, Hyung Won Chung, Charles Sutton, Sebastian Gehrmann, Parker Schuh, Kensen Shi, Sasha Tsvyashchenko, Joshua Maynez, Abhishek Rao, Parker Barnes, Yi Tay, Noam M. Shazeer, Vinodkumar Prabhakaran, Emily Reif, Nan Du, Benton C. Hutchinson, Reiner Pope, James Bradbury, Jacob Austin, Michael Isard, Guy Gur-Ari, Pengcheng Yin, Toju Duke, Anselm Levskaya, Sanjay Ghemawat, Sunipa Dev, Henryk Michalewski, Xavier García, Vedant Misra, Kevin Robinson, Liam Fedus, Denny Zhou, Daphne Ippolito, David Luan, Hyeontaek Lim, Barret Zoph, Alexander Spiridonov, Ryan Sepassi, David Dohan, Shivani Agrawal, Mark Omernick, Andrew M. Dai, Thanumalayan Sankaranarayana Pillai, Marie Pellat, Aitor Lewkowycz, Erica Moreira, Rewon Child, Oleksandr Polozov, Katherine Lee, Zongwei Zhou, Xuezhi Wang, Brennan Saeta, Mark Díaz, Orhan Firat, Michele Catasta, Jason Wei, Kathleen S. Meier-Hellstern, Douglas Eck, Jeff Dean, Slav Petrov, and Noah Fiedel. 2022. Palm: Scaling language modeling with pathways. J. Mach. Learn. Res., 24:240:1240:113. Peter Clark, Isaac Cowhey, Oren Etzioni, Tushar Khot, Ashish Sabharwal, Carissa Schoenick, and Oyvind Tafjord. 2018. Think you have solved question answering? try arc, the ai2 reasoning challenge. Frank Cunningham. 2002. Theories of democracy: critical introduction. Psychology Press. Ameet Deshpande, Vishvak Murahari, Tanmay Rajpurohit, Ashwin Kalyan, and Karthik Narasimhan. 2023. Toxicity in chatgpt: Analyzing persona-assigned language models. In Findings of the Association for Computational Linguistics: EMNLP 2023, pages 12361270, Singapore. Association for Computational Linguistics. Jwala Dhamala, Tony Sun, Varun Kumar, Satyapriya Krishna, Yada Pruksachatkun, Kai-Wei Chang, and Rahul Gupta. 2021. Bold: Dataset and metrics for measuring biases in open-ended language generation. In Proceedings of the 2021 ACM Conference on Fairness, Accountability, and Transparency, FAccT 21, page 862872, New York, NY, USA. Association for Computing Machinery. Xuan Long Do, Kenji Kawaguchi, Min Yen Kan, and Nancy Chen. 2023. Choire: Characterizing and predicting human opinions with chain of opinion reasoning. arXiv preprint arXiv:2311.08385. Suchin Gururangan, Mike Lewis, Ari Holtzman, Noah Smith, and Luke Zettlemoyer. 2022. Demix layers: Disentangling domains for modular language modIn Proceedings of the 2022 Conference of eling. the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 55575576. Dan Hendrycks, Collin Burns, Steven Basart, Andy Zou, Mantas Mazeika, Dawn Song, and Jacob Steinhardt. 2020. Measuring massive multitask language understanding. In International Conference on Learning Representations. Ari Holtzman, Jan Buys, Li Du, Maxwell Forbes, and Yejin Choi. 2019. The curious case of neural text degeneration. In International Conference on Learning Representations. Sirui Hong, Mingchen Zhuge, Jonathan Chen, Xiawu Zheng, Yuheng Cheng, Ceyao Zhang, Jinlin Wang, Zili Wang, Steven Ka Shing Yau, Zijuan Lin, Liyang Zhou, Chenyu Ran, Lingfeng Xiao, Chenglin Wu, and Jürgen Schmidhuber. 2023. Metagpt: Meta programming for multi-agent collaborative framework. Robert Jacobs, Michael Jordan, Steven Nowlan, and Geoffrey Hinton. 1991. Adaptive mixtures of local experts. Neural computation, 3(1):7987. Albert Jiang, Alexandre Sablayrolles, Arthur Mensch, Chris Bamford, Devendra Singh Chaplot, Diego de las Casas, Florian Bressand, Gianna Lengyel, Guillaume Lample, Lucile Saulnier, et al. 2023. Mistral 7b. arXiv preprint arXiv:2310.06825. Tushar Khot, Harsh Trivedi, Matthew Finlayson, Yao Fu, Kyle Richardson, Peter Clark, and Ashish Sabharwal. 2023. Decomposed prompting: modular approach for solving complex tasks. In The Eleventh International Conference on Learning Representations. Takeshi Kojima, Shixiang Shane Gu, Machel Reid, Yutaka Matsuo, and Yusuke Iwasawa. 2022. Large language models are zero-shot reasoners. Advances in neural information processing systems, 35:22199 22213. Klaus Krippendorff. 2011. Computing krippendorffs alpha-reliability. Nayeon Lee, Wei Ping, Peng Xu, Mostofa Patwary, Pascale Fung, Mohammad Shoeybi, and Bryan Catanzaro. 2022. Factuality enhanced language models for open-ended text generation. In Advances in Neural Information Processing Systems. Dmitry Lepikhin, HyoukJoong Lee, Yuanzhong Xu, Dehao Chen, Orhan Firat, Yanping Huang, Maxim Krikun, Noam Shazeer, and Zhifeng Chen. 2020. Gshard: Scaling giant models with conditional computation and automatic sharding. In International Conference on Learning Representations. Guohao Li, Hasan Abed Al Kader Hammoud, Hani Itani, Dmitrii Khizbullin, and Bernard Ghanem. 2023a. CAMEL: Communicative agents for mind exploration of large language model society. In Thirty-seventh Conference on Neural Information Processing Systems. Kenneth Li, Oam Patel, Fernanda Viégas, Hanspeter Pfister, and Martin Wattenberg. 2023b. Inferencetime intervention: Eliciting truthful answers from language model. In Thirty-seventh Conference on Neural Information Processing Systems. Tian Liang, Zhiwei He, Wenxiang Jiao, Xing Wang, Yan Wang, Rui Wang, Yujiu Yang, Zhaopeng Tu, and Shuming Shi. 2023. Encouraging divergent thinking in large language models through multi-agent debate. arXiv preprint arXiv:2305.19118. Stephanie Lin, Jacob Hilton, and Owain Evans. 2022. TruthfulQA: Measuring how models mimic human falsehoods. In Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 32143252, Dublin, Ireland. Association for Computational Linguistics. Do Xuan Long, Hai Nguyen Ngoc, Tiviatis Sim, Hieu Dao, Shafiq Joty, Kenji Kawaguchi, Nancy Chen, and Min-Yen Kan. 2024. Llms are biased towards output formats! systematically evaluating and mitigating output format bias of llms. arXiv preprint arXiv:2408.08656. Keming Lu, Bowen Yu, Chang Zhou, and Jingren Zhou. 2024. Large language models are superpositions of all characters: Attaining arbitrary role-play via self-alignment. In Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 78287840, Bangkok, Thailand. Association for Computational Linguistics. Stefan Schulz-Hardt, Dieter Frey, Carsten Lüthgens, and Serge Moscovici. 2000. Biased information search in group decision making. Journal of personality and social psychology, 78(4):655. Aman Madaan, Niket Tandon, Prakhar Gupta, Skyler Hallinan, Luyu Gao, Sarah Wiegreffe, Uri Alon, Nouha Dziri, Shrimai Prabhumoye, Yiming Yang, Shashank Gupta, Bodhisattwa Prasad Majumder, Katherine Hermann, Sean Welleck, Amir Yazdanbakhsh, and Peter Clark. 2023. Self-refine: Iterative refinement with self-feedback. In Thirty-seventh Conference on Neural Information Processing Systems. Chaitanya Malaviya, Subin Lee, Sihao Chen, Elizabeth Sieber, Mark Yatskar, and Dan Roth. 2023. Expertqa: Expert-curated questions and attributed answers. arXiv preprint arXiv:2309.07852. Todor Mihaylov, Peter Clark, Tushar Khot, and Ashish Sabharwal. 2018. Can suit of armor conduct electricity? new dataset for open book question answering. In Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, pages 23812391, Brussels, Belgium. Association for Computational Linguistics. Debora Nozza, Federico Bianchi, and Dirk Hovy. 2021. HONEST: Measuring hurtful sentence completion In Proceedings of the 2021 in language models. Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 23982406, Online. Association for Computational Linguistics. Dilek Önkal, Paul Goodwin, Mary Thomson, Sinan Gönül, and Andrew Pollock. 2009. The relative influence of advice from human experts and statistical methods on forecast adjustments. Journal of Behavioral Decision Making, 22(4):390409. OpenAI. 2022. Introducing chatgpt. Joon Sung Park, Joseph C. OBrien, Carrie J. Cai, Meredith Ringel Morris, Percy Liang, and Michael S. Bernstein. 2023. Generative agents: Interactive simulacra of human behavior. Proceedings of the 36th Annual ACM Symposium on User Interface Software and Technology. Yiwei Qin, Kaiqiang Song, Yebowen Hu, Wenlin Yao, Sangwoo Cho, Xiaoyang Wang, Xuansheng Wu, Fei Liu, Pengfei Liu, and Dong Yu. 2024. InFoBench: Evaluating instruction following ability in large language models. In Findings of the Association for Computational Linguistics ACL 2024, pages 13025 13048, Bangkok, Thailand and virtual meeting. Association for Computational Linguistics. Alec Radford, Jeff Wu, Rewon Child, David Luan, Dario Amodei, and Ilya Sutskever. 2019. Language models are unsupervised multitask learners. Yoav Shoham and Kevin Leyton-Brown. 2008. Multiagent systems: Algorithmic, game-theoretic, and logical foundations. Cambridge University Press. Chenglei Si, Weijia Shi, Chen Zhao, Luke Zettlemoyer, and Jordan Boyd-Graber. 2023. Getting MoRE out of mixture of language model reasoning experts. In Findings of the Association for Computational Linguistics: EMNLP 2023, pages 82348249, Singapore. Association for Computational Linguistics. Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timothée Lacroix, Baptiste Rozière, Naman Goyal, Eric Hambro, Faisal Azhar, Aurelien Rodriguez, Armand Joulin, Edouard Grave, and Guillaume Lample. 2023. Llama: Open and efficient foundation language models. ArXiv, abs/2302.13971. Yu-Min Tseng, Yu-Chao Huang, Teng-Yun Hsiao, WeiLin Chen, Chao-Wei Huang, Yu Meng, and YunNung Chen. 2024. Two tales of persona in llms: survey of role-playing and personalization. Andrew H. Van De Ven and André L. Delbecq. 1974. The effectiveness of nominal, delphi, and interacting group decision making processes. The Academy of Management Journal, 17(4):605621. Michela Del Vicario, Gianna Vivaldo, Alessandro Bessi, Fabiana Zollo, Antonio Scala, Guido Caldarelli, and Walter Quattrociocchi. 2016. Echo chambers: Emotional contagion and group polarization on facebook. CoRR, abs/1607.01032. Leandro Von Werra, Lewis Tunstall, Abhishek Thakur, Sasha Luccioni, Tristan Thrush, Aleksandra Piktus, Felix Marty, Nazneen Rajani, Victor Mustar, and Helen Ngo. 2022. Evaluate & evaluation on the hub: Better best practices for data and model measurements. In Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing: System Demonstrations, pages 128136, Abu Dhabi, UAE. Association for Computational Linguistics. Jiaan Wang, Yunlong Liang, Fandong Meng, Zengkui Sun, Haoxiang Shi, Zhixu Li, Jinan Xu, Jianfeng Qu, and Jie Zhou. 2023a. Is ChatGPT good NLG evaluator? preliminary study. In Proceedings of the 4th New Frontiers in Summarization Workshop, pages 111, Singapore. Association for Computational Linguistics. Noah Wang, Z.y. Peng, Haoran Que, Jiaheng Liu, Wangchunshu Zhou, Yuhan Wu, Hongcheng Guo, Ruitong Gan, Zehao Ni, Jian Yang, Man Zhang, Zhaoxiang Zhang, Wanli Ouyang, Ke Xu, Wenhao Huang, Jie Fu, and Junran Peng. 2024. RoleLLM: Benchmarking, eliciting, and enhancing role-playing abilities of large language models. In Findings of the Association for Computational Linguistics ACL 2024, pages 1474314777, Bangkok, Thailand and virtual meeting. Association for Computational Linguistics. Zhuo Xu, Rui Zhou, Yida Yin, Huidong Gao, Masayoshi Tomizuka, and Jiachen Li. 2024c. Matrix: Multiagent trajectory generation with diverse contexts. Luke Yoffe, Alfonso Amayuelas, and William Yang Wang. 2024. Debunc: Mitigating hallucinations in large language model agent communication with uncertainty estimations. James Zhao, Yuxi Xie, Kenji Kawaguchi, Junxian He, and Michael Xie. 2023. Automatic model selection with large language models for reasoning. In Findings of the Association for Computational Linguistics: EMNLP 2023, pages 758783, Singapore. Association for Computational Linguistics. Xuezhi Wang, Jason Wei, Dale Schuurmans, Quoc Le, Ed Chi, Sharan Narang, Aakanksha Chowdhery, and Denny Zhou. 2022a. Self-consistency improves chain of thought reasoning in language models. In The Eleventh International Conference on Learning Representations. Yufei Wang, Wanjun Zhong, Liangyou Li, Fei Mi, Xingshan Zeng, Wenyong Huang, Lifeng Shang, Xin Jiang, and Qun Liu. 2023b. Aligning large language models with human: survey. arXiv preprint arXiv:2307.12966. Zhen Wang, Xu Shan, Xiangxie Zhang, and Jie Yang. 2022b. N24News: new dataset for multimodal In Proceedings of the Thirnews classification. teenth Language Resources and Evaluation Conference, pages 67686775, Marseille, France. European Language Resources Association. Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, brian ichter, Fei Xia, Ed H. Chi, Quoc Le, and Denny Zhou. 2022. Chain of thought prompting elicits reasoning in large language models. In Advances in Neural Information Processing Systems. Qingyun Wu, Gagan Bansal, Jieyu Zhang, Yiran Wu, Shaokun Zhang, Erkang Zhu, Beibin Li, Li Jiang, Xiaoyun Zhang, and Chi Wang. 2023. Autogen: Enabling next-gen llm applications via multi-agent conversation framework. ArXiv, abs/2308.08155. Weiqi Wu, Hongqiu Wu, Lai Jiang, Xingyuan Liu, Hai Zhao, and Min Zhang. 2024. From role-play to drama-interaction: An LLM solution. In Findings of the Association for Computational Linguistics ACL 2024, pages 32713290, Bangkok, Thailand and virtual meeting. Association for Computational Linguistics. Benfeng Xu, An Yang, Junyang Lin, Quan Wang, Chang Zhou, Yongdong Zhang, and Zhendong Mao. 2023. Expertprompting: Instructing large language models to be distinguished experts. arXiv preprint arXiv:2305.14688. Rui Xu, Xintao Wang, Jiangjie Chen, Siyu Yuan, Xinfeng Yuan, Jiaqing Liang, Zulong Chen, Xiaoqing Dong, and Yanghua Xiao. 2024a. Character is destiny: Can large language models simulate personaarXiv preprint driven decisions in role-playing? arXiv:2404.12138. Wenda Xu, Guanglei Zhu, Xuandong Zhao, Liangming Pan, Lei Li, and William Wang. 2024b. Pride and prejudice: LLM amplifies self-bias in self-refinement. In Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 1547415492, Bangkok, Thailand. Association for Computational Linguistics. Figure 7: TruthfulQA fine-grained result by Categories in ChatGPT and Mistral"
        },
        {
            "title": "A Supplementary Analysis",
            "content": "A.1 Fine-grained Analyses TruthfulQA. The fine-grained results on TruthfulQA are presented in Fig. 7 For the ChatGPT, Multiexpert Prompting performs better than ExpertPrompting in 22/38 topics, with the most significant improvements observed in Indexical Error: Identity with 33.33% absolute improvement, History with 29.17% improvement, Misquotations with 25.00% improvement, and Science with 22.22% improvement. ExpertPrompting, on the other hand, excels in Misinformation with 8.33%, Misinformation with 7.14%, Nutrition with 6.25%, and Superstitions with 4.55% better than Multi-expert. For the Mistral, Multi-expert Prompting also outperforms ExpertPrompting in 25/38 topics. However, ExpertPrompting surpasses Multi-expert Prompting in Politics and Indexical Error: Identity, as well as Fiction. In most cases, incorporating multiple perspectives from different experts can provide diverse viewpoints and aid in verifying information, thus leading to better performance with multi-expert prompting. However, in situations where misinformation is prevalent, differences in information from multiple experts could result in confusion and erroneous conclusions. FactualityPrompt. The fine-grained results on FactualityPrompt are shown in Fig. 8. Specifically, with ChatGPT, Multi-expert Prompting surpasses ExpertPrompting in factual prompts and significantly improves in nonfactual prompts. In factual prompts, Multi-expert performs with 0.94% absolute improvement and 16.58% relative improvement compared to ExpertPrompting. In nonfactual prompts, Multi-expert performs with 6.44% absolute improvement and 48.87% relative improvement compared to ExpertPrompting. With Mistral, Multi-expert Prompting substantially improves in factual prompts by 28.65% and slightly improves in nonfactual prompts by 4.07%. This proves the capacity for tolerance and resilience to information. In the case of misinformation, Multi-expert Prompting has greater verifiability regarding the information, thus leading to better results. BOLD. For BOLD  (Fig. 8)  , Multi-expert Prompting shows improvements in both American_actors and American_actresses categories with the toxicity decreased by 90.51% and 95.63% respectively. The combination of different answers from experts helps the model to verify toxicity, thus output less toxic response. HONEST. For HONEST  (Fig. 8)  , ChatGPT with Multi-expert Prompting gathers opinions from different experts and generates final answer by synthesizing multiple perspectives and tends to excel in 6/8 categories, most significantly in queer_gender and nonqueer_gender with 40% and 80% less harmful In more general categories, like queer and nonqueer respectively compared to ExpertPrompting. categories, the complexity and diversity of opinions among experts may lead to challenges for multi-expert prompting, leading to worse results with 56% and 60% worse compared to ExpertPrompting. Figure 8: FactualityPrompt Average Hallucination NER Ratio by Categories fine-grained result in ChatGPT and Mistral (1), BOLD ChatGPT Toxicity Scores fine-grained result (2), HONEST ChatGPT Honest Scores by Category fine-grained result (3). Lower is better. A.2 Distribution of Generated Experts The distribution of the generated data is detailed in Fig. 9, which provides an overview of the frequency of experts being generated in step 1. Figure 9: Distribution of Experts generated by our first step, using (a) TruthfulQA, (b) FactualityPrompt, (c) BOLD and (d) HONEST benchmark, in ChatGPT. TruthfulQA. The most popular experts being generated by the model are Historian with 25%, Psychologist with 13.9%, Economist with 9.3% and Nutritionist with 8.3%. The variety of experts in different fields guarantees diverse range of information from various perspectives. Historian is the most generated experts due to the nature of the benchmark, focusing on answering information that requires historical context. FactualityPrompt. The most prominent expert categories reflect strong emphasis on the entertainment industry. The most popular experts being generated by the model are Entertainment Journalist with 22.8%, Biographer with 14.2%, Film Critic with 12% and Film Historian with 11.1%. BOLD Toxicity. The most frequently generated experts aree Biographer with 28.8%, Entertainment Journalist with 22%, Film Historian 21.2%. With the categories focus on American Actors and Actresses, these experts are the most suitable to generate comprehensive and informative answers in the topic. In the top generated experts, Psychologist leads with 19.2%, Socialogist with 18.9%, Clinical HONEST. Psychologist with 14.5%. These experts exhibit significant expertise in human behavior and understanding, making them well-equipped to provide comprehensive answers. With the dataset emphasizing on queer and nonqueer categories, this highlights the models ability to generated suitable experts, ensuring thorough and inclusive analysis of the topic. Model Method TruthfulQA FactualityPrompt BOLD HONEST t M G C Zero-shot-CoT Zero-shot-CoT + More Truthful Zero-shot-CoT + More Factual Zero-shot-CoT + Less Toxic Zero-shot-CoT + Less Hurtful Multi-expert Prompting Zero-shot-CoT Zero-shot-CoT + More Truthful Zero-shot-CoT + More Factual Zero-shot-CoT + Less Toxic Zero-shot-CoT + Less Hurtful Multi-expert Prompting 78.70 82.74 - - - 87.15 70.38 77.60 - - - 89.35 9.28/14.87 - 9.51/15.71 - - 8.16/14.70 6.93/13.75 - 6.78/12.72 - - 4.54/9.45 0.000 - - 0.000 - 0.000 0.163 - - 0.163 - 0.000 0.014/0.013 - - - 0.009/0. 0.003/0.003 0.006/0.005 - - - 0.027/0.018 0.003/0.003 Table 8: Evaluation results when we directly ask LLMs to be more truthful, factual, less toxic, less hurtful. Model Method TruthfulQA FactualityPrompt BOLD HONEST t T a Self-refine Self-refine w/ additional feedback Multi-expert Prompting Self-refine Self-refine w/ additional feedback Multi-expert Prompting 81.88 81.52 87.15 75.89 79.80 89. 10.36/14.95 10.99/15.86 8.16/14.70 7.11/13.96 7.00/11.62 4.54/9.45 0.000 0.000 0. 0.064 0.000 0.000 0.007/0.008 0.009/0.008 0.003/0.003 0.006/0.007 0.005/0.005 0.003/0. Table 9: Evaluation results when we directly ask LLMs to generate feedback and refined answers to be more faactually correct and useful. A.3 Asking Self-refine to provide feedback and refine the answer to be more factually correct and useful We further investigate the performance of Self-refine baseline, which involves directly asking the model to provide feedback and refine its answer by including the instruction The answer needs to be more factually correct and useful. Our results, summarized in Tab. 9, indicate that by incorporating additional feedback, Self-refine approach performs on par across four benchmarks with Mistral and shows improvement in all benchmarks when using ChatGPT, with the most significant improvement observed in BOLD Toxicity, where Self-refine reaches Multi-expert Promptings score. However, it still falls significantly short of Multi-expert Promptings performance in other benchmarks. OpenBook Model Method Zero-shot Zero-shot-CoT Zero-shot-CoT-SC Self-refine ExpertPrompting Multi-expert Prompting Zero-shot Zero-shot-CoT Zero-shot-CoT-SC Self-refine ExpertPrompting Multi-expert Prompting t M G C QA 28.80 63.00 67.60 32.80 27.80 51.40 65.00 79.20 78.00 61.80 52.80 71. college computer science 33.33 47.47 49.49 36.36 25.25 34.34 38.38 48.48 50.50 33.33 25.25 41.41 ARC 56.91 68.17 70.39 57.25 22.61 53.77 68.51 79.86 80.55 53.67 34.56 71.84 college college mathematics medicine college physics computer security formal logic econometrics econometrics electrical engineering 23.23 34.34 36.36 23.23 22. 34.34 38.38 33.33 37.37 29.29 22.22 28.28 48.83 51.74 53.48 41.86 21.51 45.46 54.65 62.79 63.95 38.37 28. 54.06 20.79 26.73 32.67 24.75 23.76 24.75 28.71 37.62 35.64 35.64 21.78 45.54 49.49 65.65 68.68 52.52 28. 53.53 45.45 77.77 76.76 62.62 32.32 63.64 35.20 38.40 37.60 30.40 28.00 36.40 35.20 34.40 39.20 35.20 29. 37.60 29.20 39.82 37.17 32.74 23.89 27.43 33.62 41.59 41.59 26.54 22.12 37.17 40.28 47.22 49.30 40.97 24. 37.50 32.63 55.55 56.25 56.25 36.11 51.39 Table 10: Evaluation results on reasoning tasks. A.4 Multi-expert Prompting in Reasoning Tasks Experimental Setup. We compare Multi-expert Prompting with (B1) Zero-shot, (B2) Zero-shotCoT (Kojima et al., 2022), (B3) Self-refine (Madaan et al., 2023), (B4) ExpertPrompting (Xu et al., 2023), and (B8) Zero-shot-CoT-Self-Consistency (Wang et al., 2022a) on 6 MCQ reasoning tasks: OpenBookQA (Mihaylov et al., 2018), ARC-Challenge (Clark et al., 2018), and 8 MMLU college tasks: college_computer_science, college_mathematics, college_medicine, college_physics, computer_security, formal_logic, econometrics, electrical_engineering (Hendrycks et al., 2020). The performance of models is measured by Accuracy, following the prior works above. Results. Results in Tab. 10 reveal shortcomings of ExpertPrompting for most reasoning datasets and MMLU topics, with notable drops compared to baselines. This highlights two key limitations: (1) relying on single expert is insufficient, and (2) current LLMs struggle as distinguished experts. Multi-expert Prompting overcomes these limitations by integrating multiple experts perspectives, outperforming ExpertPrompting significantly across all datasets and MMLU topics. Notably, Multi-expert Prompting achieves comparable results with Zero-shot-CoT and Zero-shot-CoT-SC in reasoning tasks, even surpassing them on college_physics, showcasing the advantage of leveraging multiple experts views."
        },
        {
            "title": "B Supplementary Documents of Baselines and Models",
            "content": "B.1 Prompting Baseline (B1) Zero-shot Prompting. Zero-shot prompting is fundamental and straightforward technique in prompting methods. It involves instructing the model to provide direct answers, making it widely adopted and user-friendly baseline. {question}. (B2) Zero-shot Chain-of-Thought (CoT) (Kojima et al., 2022; Wei et al., 2022). CoT prompting guides the model to break down complex tasks into intermediate steps, demonstrating its versatility and efficiency in managing various reasoning tasks. Question: {question} Lets think step by step. Output in the following format: Explanation: Final answer: (B3) Self-Refine (Wang et al., 2022a). Self-refine sharpens responses by instructing the model to iteratively feedback and modify answers based on that feedback, progressively improving its performance over time in reasoning tasks. We prompt the LLM to obtain the initial answer. The LLM is asked to provide feedback on the answer. The feedback and initial answer are then used as input to generate the revised answer. We choose 2 as the number of revision iterations to ensure that the number of LLM calls is equal to Multi-expert prompting in 3-expert case. 1. Get inial response {question}. 2. Get feedback to the responseresponse You are given question and an answer for that question. Analyze the question and the answer and provide some feedback of the answer to the question. Dont change the answer, just provide feedback. Question: {question} Answer: {answer} Feedback: 3. Get refined response You are given question, an answer to that question and feedback to the answer. Based on the feedback, refine your answer and generate the final answer. Question: {question} Answer: {answer} Feedback: {feedback} Final_answer: (B4) Universal Self-consistency (Chen et al., 2023b) Universal Self-consistency leverages LLM to select the most consistent answer among candidate answers. We adopt prompt from the Zero-shot in Appx.-B.1 to generate candidate answers and use the prompt template described in (Chen et al., 2023b) for selecting the most consistent answer. (B5) Multi-agent Debate (Liang et al., 2023) Multi-agent Debates simulate the environment where multiple agents express their arguments and judge observes the debating process to generate the final answer. We adopt the framework and prompt template as describe in (Liang et al., 2023) for our task. (B6) ExpertPrompting (Xu et al., 2023). ExpertPrompting directs the model to act as distinguished expert by synthesizing detailed expert identity via few-shot prompting with hand-crafted demonstrations and instructing the model to perform specific task accordingly. 1. Generate Expert identity and description For each question, write high-quality description about the most capable and suitable agent (role) to answer the question. In second person perspective. For example: [Question]: {Demonstration 1 Question} [Agent Description]: {Demonstration 1 Answer} [Question]: {Demonstration 2 Question} [Agent Description]: {Demonstration 2 Answer} [Question]: {Demonstration 3 Question} [Agent Description]: {Demonstration 3 Answer} [Question]: {Question} [Agent Description]: 2. Get Expert answer {expert_identity} Now given the above identity background, please answer the following question: {question} (B7) Fixed Temperature Zero-shot Result + Our Aggregation. In this baseline, we examine the result by prompting the model to generate answers by fixed temperature in zero-shot setting and use our aggregation technique to combine the results. This baseline is necessary to benchmark the effectiveness of the diverse expert roles in our technique compared to no role assigned. The prompt we use for answer generation is adopted from Zero-shot template in Appx.-B.1 and aggregation prompt is adopted from Multi-expert Prompting, presented in Appx.-C.5. (B8) Variable Temperature Zero-shot Result + Our Aggregation. This baseline is the same as (B5), except we use different temperatures (for the case = 3, we use 0, 0.4, 0.8) to sample answers. The prompt we use for answer generation is adopted from Zero-shot template in Appx.-B.1 and aggregation prompt is adopted from Multi-expert Prompting, presented in Appx.-C.5. (B9) ExpertPrompting Result + Our Aggregation. We use ExpertPrompting to sample experts answers. One of the crucial differences between our method and ExpertPrompting is that our method samples different experts while ExpertPrompting samples 1 expert for 3 answers most of the time due to its expert generation step being few-shot generation without explicitly requiring multiple experts. As Zero-shot-CoT Self-align ExpertPrompting Multi-expert Prompting Dataset Ave. consumed #tokens Total US$ Ave. consumed #tokens Total US$ 103.31 0.1634 86.18 0.3104 1289.6 2. 1191.53 3.7248 963.53 1.5523 917.15 2.7936 2345.78 3.8399 1307.44 4.0352 TruthfulQA TruthfulQA BOLD BOLD Table 11: Prompting cost analysis of ChatGPT with Multi-expert Prompting as of 1st Feb 2024. such, it falls significantly compared to our method, see Tab. 1. The prompt we use for Expert identity generation and answer is adopted from ExpertPrompting in Appx.-B.1 and aggregation prompt is adopted from Multi-expert Prompting, presented in Appx.-C.5. B.2 Model Hyperparameters ChatGPT. ChatGPT is called via OpenAI API3 with the mode gpt-3.5-turbo-0613. For temperature, we use consistent temperature setting of 0.0 for all baselines and intermediate steps. In the case of the baseline (B7) where variable temperature is required, we use temperatures of {0.0, 0.4, 0.8} for the three answers generated from Zero-shot prompting. We use Sampling (Holtzman et al., 2019) as our decoding strategy. The context window size is set to 1024 for all the steps. Mistral. We call the pretrained model Mistral-7B-Instruct-v0.2 from MistralAI4 available in HuggingFace5. For all Mistral experiments, we use temperature of 0.1 to ensure reproducibility. For baseline (B7), we employ the temperature of {0.1, 0.4, 0.8} for the three answers generated from Zero-shot prompting. We use Sampling (Holtzman et al., 2019) as our decoding strategy. The context window size is set to 1024 for all the steps. Supplementary Documents of Multi-expert Prompting Method Skip S1 Skip S2&S3 Skip S4 Skip S7 TruthfulQA FactualityPrompt BOLD HONEST 2090.93 2236.3 2235.13 2065.47 2112.06 2304.61 2084.22 1944.64 2578. 1530.5 1397.36 1435.64 1428.21 1406.9 1478.75 1528.5 1489.45 1537.64 1601.35 Multi-expert Prompting 2345. Table 12: Prompting cost (number of tokens) when Multi-expert Prompting skips S1, S2, S2, S4, S7 in 2nd Step. C.1 Multi-expert Promptings Hyperparameters We change the number of experts corresponding to our experiments. According to the results, the 3-expert case gives the optimal results. C.2 Prompting Costs Tab. 11 shows our prompting costs for OpenAI API models. We observe that Multi-expert Prompting consumes double number of tokens on TruthfulQA, and about 1.5 times on BOLD. However, the cost of Multi-expert Prompting is relatively affordable with around 4 US$ in total for both datasets. We also investigate the prompting costs of OpenAI API models when when selectively bypassing specific steps. The number of tokens used is summarized in Tab. 12 while the models performance is detailed in Tab. 3. Notably, our analysis shows that skipping any step incurs marginal reduction in token usage while harming the overall performance. This shows the critical role of any step S1-S7 in Multi-expert Prompting. 3https://platform.openai.com/ 4https://mistral.ai/ 5https://huggingface.co/mistralai/Mistral-7B-Instruct-v0.2 C.3 Expert Generation Prompt You are provided an information. Give me list of 3 best roles that could complete the information the most thoroughly. Question: {question} Only give me the answer as dictionary of roles in the Python programming format with short description for each role. Strictly follow the answer format below: Answer: {\"[role 1]\": \"[description 1]\", \"[role 2]\": \"[description 2]\", \"[role 3]\": \"[description 3]\"} C.4 Expert Casting Prompt From now on, you are an excellent {role} described as {roles_description}. Answer the following question while staying in strict accordance with the nature of the provided identity: {question}. C.5 Multi-expert Prompting 3 Experts The prompt is designed with 7 steps described in 3.2. Given the following question: {question}, you have obtained three answers from three experts with different expertise: ### expert_1_answer ### expert_2_answer ### expert_3_answer ### Your task is to aggregate the experts answers above, follwing the subtasks below. Step 1: Which are the facts that more than half of the answers have? Facts that more than half of the answers have (Agreed Facts):... Step 2: Which are the facts of the answers above that conflict? Conflicted facts among the answers (Conficted Facts):... Step 3: Now you need to resolve the conflicted facts from Step 2. The facts that more people agree are likely to be true. Resolved facts from Step 2:... Step 4: Which are the facts that are not from Step 2 and 1, and only one of the answers have? Facts that are excluded from Step 2 and 1 and only one of the answers have:... Step 5: Combine facts from Step 1, 3, 4, to obtain the facts that will appear in the final solution. Facts from Step 1, 3, 4:... Step 6: Generate final answer consisting of facts in Step 5, in newline. Combined answer:... Step 7: Given the answer 1, answer 2, answer 3, and combined answer, which answer among them do you think is more factually correct and useful? Best answer choice: Answer 1/Answer 2/Answer 3/Combined answer Explanation: [Explanation to your choice of the best answer] Final answer: [Only output the full chosen answer content. Output the exact answer, do not modify or trim the answer.]"
        },
        {
            "title": "D Supplementary Documents of ChatGPT Judge",
            "content": "D.0.1 Informativeness You are given question and two responses. Your task is to evaluate which answer is better, or there is draw , in terms of informativeness. The informativeness is defined as the extent of details, in-depth insights, multiple perspectives, and supporting evidence that an answer has. Question: {question} Answer 1: {response1} Answer 2: {response2} Fulfill your task by filling in the template below: Evaluation: Answer 1 is better/Answer 2 is better/There is draw. Explanation: ... D.0.2 Usefulness You are given question, and two responses. Your task is to evaluate which answer is better, or there is draw , in terms of usefulness. The usefulness is defined as the extent of effectiveness in expressing the ideas and conveying the information. Question: {question} Answer 1: {response1} Answer 2: {response2} Fulfill your task by filling in the template below: Evaluation: Answer 1 is better/Answer 2 is better/There is draw. Explanation: ..."
        },
        {
            "title": "E Supplementary Documents of Benckmarks Details",
            "content": "Intuitively, leveraging multiple experts is expected to enhance the depth and breadth of generated responses by incorporating diverse viewpoints, experiences, and expertise. This approach is likely to improve the informativeness and usefulness of the answers provided by the framework. Additionally, the use of Multi-expert Prompting is anticipated to promote deeper thinking in the model, potentially enhancing the truthfulness of information by allowing multiple experts to review in case of misinformation. Moreover, the combination of multiple answers may also improve other aspects such as hallucination, as the framework becomes more resilient with information from multiple sources. Furthermore, by incorporating multiple viewpoints and reducing bias towards single expert, the framework could also potentially reduce toxicity and harmfulness in the answers provided. Therefore, we use the below benchmarks. ExpertQA. We collect all open-ended questions from ExpertQA (Malaviya et al., 2023) consisting of 528 questions from 32 topics in total. TruthfulQA. We collect all questions from TruthfulQA-Generation subset of TruthfulQA (Lin et al., 2022). FactualityPrompt. The full dataset consists of 8000 factual prompts and 8000 nonfactual prompts. We randomly sample 250 samples from factual prompts and 250 samples from nonfactual prompts due to the limited resources. The samples are then used directly as the information for the models to complete. This sample size allows us to effectively capture the variability and characteristics of the entire dataset while ensuring computational efficiency. Criterion Scoring Instruction (M1) Expert Generation Satisfaction 2: Among the 3 generated experts, only two experts are helpful in responding to the instruction. 1: Among the 3 generated experts, less than or equal to 1 expert is helpful in responding to the instruction. 3: All 3 generated experts are helpful in responding to the instruction. (M2) Aggregation Satisfaction 3: Among the 7 subtasks, the model does reasonably well for at least 6 subtasks. 2: Among the 7 subtasks, the model does reasonably well for 4 or 5 subtasks. 1: Among the 7 subtasks, the model does reasonably well for at most 3 subtasks. Table 13: Human grading instructions. BOLD. We run our benchmark on American_actors and American_actresses categories. As there are 1587 records in American_actors and 776 records in American_actresses, we sample 776 records from each category to ensure balance between categories. As there are multiple prompts in each record of the original dataset, we only use the first prompt and ignore the other prompts from each record. The prompt is used directly as the information for the models to complete. We first use pre-trained toxic detection model6 to output the probability of being toxic for an output. We then classify it as toxic if this probability 0.5, and compute the toxic ratio. HONEST. We use en_queer_unqueer subset, consisting of 705 records, following HuggingFace Evaluation tutorial7. We delete \"[M]\" from masked_template and use the result as the input prompt to the models."
        },
        {
            "title": "F Human Evaluation Grading Policies",
            "content": "Tab. 13 presents our grading instructions for annotators. 6https://huggingface.co/s-nlp/roberta_toxicity_classifier 7https://huggingface.co/docs/evaluate/en/index Figure 10: generated example by Multi-expert Prompting with = 3 experts with TruthfulQA with ChatGPT."
        },
        {
            "title": "G Supplementary Examples",
            "content": "In this section, we provide examples to illustrate our arguments across the paper. Fig. 10 shows generated example by Multi-expert Prompting with = 3 experts with TruthfulQA, Fig. 11 and Fig. 12 with FactualityPrompt, Fig. 13 with BOLD, Fig. 14 with HONEST, and Fig. 15 with ExpertQA. Fig. 17 shows teaser example where output is generated example by Multi-expert Prompting with = 3 experts and other baselines. Fig. 18 shows an example where single experts view from ExpertPrompting is sufficiently good compare with Multi-expert Prompting. Fig. 19 shows generated example by Multi-expert Prompting with = 3 experts where all three experts give helpful answers. Fig. 20 illustrates generated example by Multi-expert Prompting with = 3 experts where one expert are less helpful. Fig. 21 demonstrates generated example by Multi-expert Prompting with = 3 experts where two experts are less helpful. Finally, Fig. 22 shows generated example by Multi-expert Prompting with = 3 experts where the aggregation steps misinterpret diverging key points in Step 2. Figure 11: generated example by Multi-expert Prompting with = 3 experts with factual prompt in FactualityPrompt with ChatGPT. Figure 12: generated example by Multi-expert Prompting with = 3 experts with nonfactual prompt in FactualityPrompt with ChatGPT. Figure 13: generated example by Multi-expert Prompting with = 3 experts with BOLD with ChatGPT. Figure 14: generated example by Multi-expert Prompting with = 3 experts with HONEST with ChatGPT. Figure 15: generated example by Multi-expert Prompting with = 3 experts with ExpertQA with ChatGPT. Figure 16: generated example by Multi-expert Prompting with = 3 experts with ChatGPT. The answers of other baselines are shown in Fig. 17. Figure 17: The example answers of Multi-expert Prompting and other baselines with ChatGPT, partly shown in Fig. 16. Figure 18: An example where single experts view from ExpertPrompting is sufficiently good. Figure 19: generated example by Multi-expert Prompting with ChatGPT with = 3 experts where all three experts give helpful answers. Figure 20: generated example by Multi-expert Prompting with ChatGPT with = 3 experts where one expert are less helpful. Both answer 1 and answer 3 provide mathematical perspectives, whereas answer 2 offers philosophical viewpoint. Consequently, either answer 1 or answer 3 is less helpful. Figure 21: generated example by Multi-expert Prompting with ChatGPT with = 3 experts where two experts are less helpful. The information presented in answers 1 and 3 is encompassed within answer 2. Thus, answers 1 and 3 are considered less helpful. Figure 22: generated example by Multi-expert Prompting with ChatGPT with = 3 experts where the model misinterprets diverging key points in Step 2 however it still derives the accurate resolved conflict conclusions."
        }
    ],
    "affiliations": [
        "National University of Singapore",
        "Institute for Infocomm Research (I2R), A*STAR",
        "Nanyang Technological University"
    ]
}