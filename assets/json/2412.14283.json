{
    "paper_title": "PixelMan: Consistent Object Editing with Diffusion Models via Pixel Manipulation and Generation",
    "authors": [
        "Liyao Jiang",
        "Negar Hassanpour",
        "Mohammad Salameh",
        "Mohammadreza Samadi",
        "Jiao He",
        "Fengyu Sun",
        "Di Niu"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Recent research explores the potential of Diffusion Models (DMs) for consistent object editing, which aims to modify object position, size, and composition, etc., while preserving the consistency of objects and background without changing their texture and attributes. Current inference-time methods often rely on DDIM inversion, which inherently compromises efficiency and the achievable consistency of edited images. Recent methods also utilize energy guidance which iteratively updates the predicted noise and can drive the latents away from the original image, resulting in distortions. In this paper, we propose PixelMan, an inversion-free and training-free method for achieving consistent object editing via Pixel Manipulation and generation, where we directly create a duplicate copy of the source object at target location in the pixel space, and introduce an efficient sampling approach to iteratively harmonize the manipulated object into the target location and inpaint its original location, while ensuring image consistency by anchoring the edited image to be generated to the pixel-manipulated image as well as by introducing various consistency-preserving optimization techniques during inference. Experimental evaluations based on benchmark datasets as well as extensive visual comparisons show that in as few as 16 inference steps, PixelMan outperforms a range of state-of-the-art training-based and training-free methods (usually requiring 50 steps) on multiple consistent object editing tasks."
        },
        {
            "title": "Start",
            "content": "4 2 0 2 8 1 ] . [ 1 3 8 2 4 1 . 2 1 4 2 : r PixelMan: Consistent Object Editing with Diffusion Models via Pixel Manipulation and Generation Liyao Jiang1,2, Negar Hassanpour2, Mohammad Salameh2, Mohammadreza Samadi2, Jiao He3, Fengyu Sun3, Di Niu1 1Dept. ECE, University of Alberta. 2Huawei Technologies Canada. 3Huawei Kirin Solution, China. {liyao1, dniu}@ualberta.ca, {negar.hassanpour2, mohammad.salameh, mohammadreza.samadi, hejiao4}@huawei.com sunfengyu@hisilicon.com"
        },
        {
            "title": "Abstract",
            "content": "Recent research explores the potential of Diffusion Models (DMs) for consistent object editing, which aims to modify object position, size, and composition, etc., while preserving the consistency of objects and background without changing their texture and attributes. Current inference-time methods often rely on DDIM inversion, which inherently compromises efficiency and the achievable consistency of edited images. Recent methods also utilize energy guidance which iteratively updates the predicted noise and can drive the latents away from the original image, resulting in distortions. In this paper, we propose PixelMan, an inversion-free and training-free method for achieving consistent object editing via Pixel Manipulation and generation, where we directly create duplicate copy of the source object at target location in the pixel space, and introduce an efficient sampling approach to iteratively harmonize the manipulated object into the target location and inpaint its original location, while ensuring image consistency by anchoring the edited image to be generated to the pixel-manipulated image as well as by introducing various consistency-preserving optimization techniques during inference. Experimental evaluations based on benchmark datasets as well as extensive visual comparisons show that in as few as 16 inference steps, PixelMan outperforms range of state-ofthe-art training-based and training-free methods (usually requiring 50 steps) on multiple consistent object editing tasks. Code https://github.com/Ascend-Research/PixelMan Introduction Diffusion Models (DMs) excel at generating stunning visuals from text prompts (Rombach et al. 2022; Saharia et al. 2022b; Chang et al. 2023), yet with potentials extending beyond text-to-image generation. highly popular application is image editing, as evidenced by widespread tools such as Google Photos MagicEditor (Google 2023) and AI Editor in Adobe Photoshop (Adobe 2023). Many research efforts (Hertz et al. 2022; Tumanyan et al. 2023; Alaluf et al. 2023; Parmar et al. 2023) achieve promising results on textprompt-guided rigid image editing involving tasks such as changing the color, texture, attributes, and style of the image. However, consistent object editing (Kawar et al. 2023; Cao et al. 2023; Duan et al. 2024) is distinct type of image Copyright 2025, Association for the Advancement of Artificial Intelligence (www.aaai.org). All rights reserved. editing that aims to preserve the consistency of objects and background in the image without changing their texture and attributes, while modifying only certain non-rigid attributes of the objects (e.g., changing the position, size, and composition of objects). Typical consistent object editing tasks include object repositioning (Epstein et al. 2023; Mou et al. 2024b,a; Wang et al. 2024; Winter et al. 2024), object resizing (Epstein et al. 2023; Mou et al. 2024b,a), and object pasting (Chen et al. 2024b; Mou et al. 2024b,a). Consistent object editing tasks are complex and usually involve multiple sub-tasks such as: (i) generating faithful reproduction of the source object at the target location, (ii) maintaining the background scene details, (iii) harmonizing the edited object into its surrounding target context, and (iv) inpainting the original vacated location with cohesive background. To solve this problem, training-based methods have been proposed (Rombach et al. 2022; Chen et al. 2024b; Wang et al. 2024; Winter et al. 2024), which however require costly training process and usually also require collecting task-specific datasets. Alternatively, recent training-free methods (Epstein et al. 2023; Mou et al. 2024b,a) rely on DDIM inversion (Dhariwal and Nichol 2021) to estimate the initial noise corresponding to the source image. However, this process is inefficient as it often requires many (usually at least 50) inference steps. Reducing the number of steps to, e.g., 16, significantly compromises editing quality (see Fig. 2). Moreover, DDIM inversion struggles to produce precise and consistent final reconstruction of the source image, often yielding coarse approximation due to accumulation of errors at each timestep (Duan et al. 2024). As result, training-free methods that rely on DDIM inversion are inherently limited in their ability to perform consistent edits. To facilitate object generation at target location and reproduction of background, DragonDiffusion (Mou et al. 2024b) and DiffEditor (Mou et al. 2024a) utilize Energy Guidance (EG) to minimize the feature similarity between the source and target objects (backgrounds). While EG iteratively refines the predicted noise, this process can inadvertently drive the latent representation away from that of the original image during inference, causing distortions in object appearance and background. Additionally, seamlessly inpainting the vacated region (if any) with coherent background remains challenge, as existing methods often struggle to fully remove the original object or introduce unintended elements (see Fig. 2). In this paper, we propose PixelMan, an inversion-free and training-free method to achieve consistent object editing with existing pretrained text-to-image diffusion models via Pixel Manipulation and generation in as few as 16 steps that outperform all competitive training-based and trainingfree methods (usually requiring 50 steps) on range of consistent object editing tasks. Rather than performing DDIM inversion and edited denoising, we directly create duplicate copy of the source object at target location in the pixel space, and introduce an efficient sampling approach to iteratively harmonize the manipulated object into the target location and inpaint its original location, while ensuring image consistency by anchoring output image to be generated to the pixel-manipulated image as well as by introducing various consistency-preserving optimization techniques during inference. Our contributions are summarized as follows: We propose to perform pixel manipulation for achieving consistent object editing, by creating pixel-manipulated image where we copy the source object to the target location in the pixel space. At each step, we always anchor the target latents to the pixel-manipulated latents, which reproduces the object and background with high image consistency, while only focusing on generating the missing delta between the pixel-manipulated image and the target image to be generated. We design an efficient three-branched inversion-free sampling approach, which finds the delta editing direction to be added on top of the anchor, i.e., the latents of the pixel-manipulated image, by computing the difference between the predicted latents of the target image and pixel-manipulated image in each step. This process also facilitates faster editing by reducing the required number of inference steps and number of Network Function Evaluations (NFEs). To inpaint the manipulated objects source location, we identify root cause of many incomplete or incoherent inpainting cases in practice, which is attributed to information leakage from similar objects through the SelfAttention (SA) mechanism. To address this issue, we propose leak-proof self-attention technique to prevent attention to source, target, and similar objects in the image to mitigate leakage and enable cohesive inpainting. Our method harmonizes the edited object with the target context, by leveraging editing guidance with latents optimization, and by using source branch to preserve uncontaminated source K, features as the context for generating appropriate harmonization effects (e.g. lighting, shadow, and edge blending) at the target location. We provide extensive quantitative and/or qualitative visual comparisons to range of state-of-the-art training-free and training-based approaches designed for object repositioning, object resizing and object pasting (some of which can be found in Appendix). Quantitative results on the COCOEE and ReS datasets as well as extensive visual comparisons suggest that PixelMan achieves superior performance in consistency metrics for object, background, and semantic consistency between the source and edited image, while achieving higher or comparable performance in IQA metrics. As training-free method, PixelMan only requires 16 inference steps with lower average latency and lower number of NFEs than current popular methods. Related Works Image editing with DMs. While standard text-to-image DMs are not directly designed for image editing, recent research is actively exploring their potential for this task. Training-based approaches (Saharia et al. 2022a; Brooks, Holynski, and Efros 2023; Zhang, Rao, and Agrawala 2023) optimize the UNet for certain editing scenarios. Wang et al. (2024) fine-tuned an inpainting model specifically for object repositioning task (by introducing and utilizing an ad-hoc dataset, namely ReS). However, these approaches may require high computational resources only to learn specific task. As such, there is high motivation to explore methods for augmenting pretrained UNets with different editing capabilities without additional training. In training-free methods (Hertz et al. 2022; Alaluf et al. 2023; Hertz et al. 2023; Tumanyan et al. 2023), users can perform editing either by descriptive text prompt (Hertz et al. 2022; Brooks, Holynski, and Efros 2023; Tumanyan et al. 2023; Epstein et al. 2023), or by specifying editing points within an image, called point-based editing (Endo 2022; Pan et al. 2023; Shi et al. 2023; Mou et al. 2024b,a). The main advantage of point-based editing is the granular control over the edit region. In this work, we propose point-based training-free approach for consistent object editing using DM, which preserves the consistency between the source and edited image. Training-free consistent object editing. Epstein et al. (2023) introduced Energy Guidance (EG) (see Appendix for details) and proposed SelfGuidance, prompt-based editing method that guides the sampling process based on specific energy functions defined on attentions and activations. Mou et al. (2024b) proposed DragonDiffusion, point-based editing approach that leverages EG to update the sampled noise. Building on this, DiffEditor (Mou et al. 2024a) improved the content consistency by introducing regional SDE sampling and score-based gradient guidance (Song et al. 2020). Despite their success, EG-based methods require computationally expensive tricks to propagate the guidance from ϵ to zt. Different from EG-based methods that update the estimated noise ϵ, our method directly updates the latents zt for consistent object editing, which reduces the latency as well as NFEs, while maintaining consistency and image quality. Inverting real images. Preserving the consistency between the original and edited image is crucial for consistent image editing. Training-free methods often utilize the inversion techniques to convert the source image into convertible initial noise (zT ). DDIM inversion (Dhariwal and Nichol 2021) is common but computationally expensive technique as it usually requires 50 inference steps. ReNoise (Garibi et al. 2024) is recent inversion technique that can utilize few-steps models (Luo et al. 2023; AI 2023), but its repeated UNet calls in its refinement phase still leads to high computation costs. An alternative approach is Denoising Diffusion Consistent Model (DDCM) (Xu et al. 2024), which facilitates inversion-free prompt-guided rigid image editing for changing the texture and attribute of objects. In contrast to DDCM, our method does not use any prompt, and instead we propose an inversion-free approach for efficient consistent object editing which focuses on preserving the consistency of objects and background in the image without changing their texture and attributes while modifying only certain non-rigid attributes of the objects (e.g., changing the position, size, and composition of objects). Attention control for editing. Recent studies on trainingfree editing techniques (Cao et al. 2023; Hertz et al. 2023; Tumanyan et al. 2023; Hertz et al. 2022; Parmar et al. 2023) explore either integrating or manipulating CrossAttentions (CAs) and Self-Attentions (SAs) to exert precise control over the editing process. Manipulating CAs has been demonstrated to offer control over object composition. Hertz et al. (2022) proposed an injection approach for swapping objects and changing the global style. Alternatively, since SAs incorporate information about pixel interactions in the spatial domain, manipulating them affects overall style, texture, and object interaction (Zhou et al. 2024; Alaluf et al. 2023; Hertz et al. 2023; Jeong et al. 2024; Cao et al. 2023). Building on this, Patashnik et al. (2023) presented SA injection method to selectively preserve set of objects while altering other regions. Following these insights, we propose leakproof self-attention technique to ensure complete and cohesive inpainting of the vacated area with the background, by preventing root cause of failed inpainting which is information leakage from the source or similar objects. Method To enhance computational efficiency and preserve image consistency during object editing, we introduce PixelMan, an efficient inversion-free and training-free method that performs consistent object editing with DMs via Pixel Manipulation and generation in few inference steps. The following subsections describe our proposed three-branched inversionfree sampling approach, leakproof self-attention technique, and editing guidance with latents optimization method. Three-Branched Inversion-Free Sampling Our goal is to achieve the following three objectives with high efficiency: (i) consistent reproduction of the object and background; (ii) object-background harmonization; and (iii) cohesive inpainting of the vacated location. As the backbone of PixelMan, we propose three-branched sampling paradigm that achieves these three objectives using single pretrained DM, while also bypassing inversion to facilitate faster editing by reducing inference steps and NFEs. Specifically, we utilize three separate branches: source branch, pixel-manipulated branch, and target branch. Each branch maintains its own nosiy latents that is initialized, denoised (using the same UNet), and updated in different manners throughout the sampling time-steps [1, ]. We denote the noisy latents of the source branch, pixelmanipulated branch, and target branch at time-step respectively with zsrc , ztgt . , zman We create pixel-manipulated image Iman by copying the source object to the target location in pixel-space. For the object resizing task, we interpolate the object pixels based on the resizing scale before making copy at the target location. For object pasting, the source object comes from separate reference image Iref, and is copied into the source image Isrc at the target location to create Iman. Then, using the VAE encoder, we encode the pixel-manipulated image Iman and the source image Isrc respectively into the pixelmanipulated latents zman and source latents zsrc 0 . 0 Pixel-manipulated latents as anchor. At each time-step of our sampling process, our goal is to directly obtain latent space estimation of the edited output image Iout which we denote as output latents zout 0 . First, we ask the question of what would be reasonable estimate of the output latents zout 0 . Intuitively, our estimation of output latents zout should 0 be identical to the source latents zsrc so we can exactly re0 produce the source image. 0 However, we want to reproduce the source object at the new target location, so we set our estimation of the output latents zout to be identical to the pixel-manipulated latents 0 zman , which already have the source object reproduced at 0 the target location through pixel manipulation. By using this naive estimation zout , we can already effortlessly preserve the original background and consistently reproduce the object at the target location. Therefore, we refer to this pixel-manipulated latents zman 0 = zman as the anchor. 0 0 0 as: In addition to image consistency, we also want to achieve cohesive inpaitning of the vacated location, and harmonize the object and background with realistic effects. So, there should be delta editing direction added on top of the anchor zman to achieve the inpainting and harmonization edits. More concretely, at each time-step t, we set the output latent zout 0 + z. 0 = zman zout With our simple estimation of output latents zout (1) 0 using the sum of the anchor zman and the delta edit direction z, we can preserve the object and background consistency without any inversion, which improves both the efficiency and image consistency by avoiding the computation bottleneck and accumulated reconstruction error of the DDIM inversion (Dhariwal and Nichol 2021) process. Next, we introduce our method for obtaining the delta edit direction z. 0 Obtaining delta edit direction. We aim to obtain the delta editing direction that can achieve cohesive inpainting of the vacated location, and harmonize the object and background with realistic effects (e.g., lighting, shadow, edge blending). To achieve this, we propose to apply several editing guidance techniques (introduced in the later sections) for generating the inpainting and harmonization edits in the target branch, including leak-proof self-attention, editing guidance with latent optimization, and injection of source K, features into the target branch. Meanwhile, we keep the pixel-manipulated branch consistent with the anchor zman , and obtain by finding the difference in the output of the two branches. Specifically, we calculate the difference between the pre0 from the target branch and predicted dicted target latents ˆztgt Figure 1: Overview of PixelMan. An efficient inversion-free sampling approach for consistent image editing, which copies the object to target location in pixel-space, and ensure image consistency by anchoring to the latents of pixel-manipulated image. We design leak-proof self-attention mechanism to achieve complete and cohesive inpainting by mitigating information leakage. pixel-manipulated latents ˆzman branch: 0 from the pixel-manipulated 0 0 = ˆztgt 0 ˆzman To obtain ˆzman . (2) from the pixel-manipulated branch, we always analytically compute the noisy latents zman at each sampling time-step from the pixel-manipulated latents zman (i.e., the anchor that ensures consistency to Iman) which has already reproduced object at the target location and the original background. 0 Specifically, at each time-step t, we first follow the FDP by adding random Gaussian noise equation to obtain zman ϵ (0, I) to zsrc 0 : zman = αt zman 0 + 1 αt ϵ. (3) Then, we pass the noisy source latents zman UNet (parameterized by θ) to get the predicted noise ˆϵman time-step t: to the denoising at = UNet(zman ˆϵman (4) Finally, we obtain the predicted pixel-manipulated latents ˆzman based on the noisy pixel-manipulated latents zman and 0 the UNet predicted noise ˆϵman at time-step t, using the Reverse Generative Process (RGP) (more details in Eq. (10) in the Appendix): , t). t 0 = RGP(zman ˆzman , ˆϵman , t). (5) Next, we obtain ˆztgt 0 from the target branch. Before the initial timestep = , we initialize the target latents ztgt 0 to be the same as zman 0 which corresponds to the anchor Iman. In contrast, at each sampling time-step t, we instead utilize the FDP similar to Eq. (3) to analytically compute the noisy target latents ztgt 0 of previous time-step. from the estimated output latents zout Then, ztgt is updated with latents optimization (detailed in Editing Guidance with Latents Optimization). Next, we pass ztgt along with the saved source branch Ksrc, Vsrc (detailed in Feature-preserving source branch) to the UNet to obtain the predicted noise ˆϵsrc = UNet(zsrc , t; {Ksrc, Vsrc}). Next, we obtain the predicted tart get latents ˆztgt 0 using the RGP similar to Eq. (5). , where ˆϵsrc 0 and ˆzman After calculating both ˆztgt , we finally obtain the delta editing direction z. To estimate the output image, we combine the anchor zman and the delta editing direction in Eq. (2), while applying masked-blending approach with mask (1 mnew) (i.e., the object target location): 0 0 0 = zman zout 0 + (ˆztgt 0 ˆzman ) (1 mnew). (6) The masked-blending is applied throughout the sampling time-steps to remove the delta editing direction in the target location, and only use the anchor zman to achieve object consistency. While allowing to change the background for inpainting and harmonization. For the last few time-steps no masking is applied, which encourages seamless object-background blending and allows the DM to refine the details of the output image. 0 (making zsrc consistent with zsrc Feature-preserving source branch. At each time-step t, we always analytically compute the noisy source latents zsrc from zsrc 0 ). Specifically, at 0 each time-step t, we first follow the FDP equation similar to Eq. (3) to obtain zsrc by adding random Gaussian noise ϵ (0, I) to zsrc 0 . Then, we pass the noisy source latents zsrc at time-step t: ˆϵsrc , t). Note that the ˆϵsrc is discarded here, and we save the self-attention Ksrc to the denoising UNet to get the predicted noise ˆϵsrc , {Ksrc, Vsrc} = UNet(zsrc and Vsrc matrices from the source branch and inject1 them back during the UNet call on ztgt in the target branch, which is inspired by the mutual self-attention technique proposed in (Cao et al. 2023). The saved Ksrc and Vsrc preserve the original visual details from Isrc, and the injection into target branch serves as context for generating appropriate harmonization effects (e.g., lighting, shadow, and edge blending), and also for inpainting the vacated area. Leak-Proof Self-Attention Our objective is to achieve complete and cohesive inpainting of the vacated region after the edited object moves out. However, current methods often either struggle to remove all traces of the object (e.g., object is not entirely removed in columns (d), (e), and (f) of Fig. 2 by the SOTA method DiffEditor (Mou et al. 2024a)), or hallucinate new unwanted artifacts in the vacated region. We attribute these issues to information leakage from similar objects through the SA mechanism (Dahary et al. 2024), and propose leak-proof self-attention technique that prevents the attention to source object, target object, and similar objects in the image. Leakproof SA leverages and controls the inter-region dependencies captured by SA to alleviate information leakage. Intuitively, areas mold, mnew, and msim all contain information about the to-be-edited object, and this information can be leaked to area mipt through the SA mechanism, where mold is mask of to-be-edit object at the source/old location; mnew is mold shifted to the the target/new location; msim is mask of other similar objects to the to-be-edited object (e.g., other apples in the multi-apple image in Fig. 1; see details on how to automatically obtain msim in the Appendix); and mipt equals the mask from (mold mnew) which represents the tobe-inpainted vacated region. To minimize the information leakage of the to-be-edited object and similar objects on the inpainted region, we strategically reset the corresponding elements (i.e., mold mnew msim) in QK to minimal value (i.e., ). This strategy is activated for the target branch UNet call in all SA layers and at all time-steps to mitigate leakage and enable cohesive inpainting. Editing Guidance with Latents Optimization Mou et al. (2024b) propose set of energy functions, which enforce feature correspondence to provide editing guidance. We utilize the same energy functions from DragonDiffusion (Mou et al. 2024b) to obtain additional editing guidance for object generation, harmonization, inpainting, and background consistency in our target branch. Moreover, we aim to improve the efficiency of editing guidance. Mou et al. (2024a) showed that having refinement loop that applies the editing guidance multiple times at single time-step significantly enhances the performance. However, EG-based methods update the predicted noise ϵ while the loss function operates on the noisy latents zt. To bridge this gap and propagate the guidance from ϵ to zt, (Mou et al. 2024a) introduced time travel that requires repetitive second round of DDIM inversion (Dhariwal and 1Injection refers to overwriting the respective attention and matrices with the previously saved ones. Nichol 2021). Therefore, the EG-based editing guidance can be computationally expensive in terms of NFEs. Different from EG-based methods which updates the predicted noise ϵ, we propose more efficient refinement strategy by applying the editing guidance directly to the target noisy latents ztgt at each time-step t: ztE(ztgt ztgt ztgt , zman ). The direct application of guidance at ztgt eliminates the need for expensive tricks such as time travel. Our strategy is grounded in solid theoretical foundation, as it leverages inference-time gradient descent optimization, which is also known as GSN (Chefer et al. 2023) in the text-to-image generation DM literatures. Furthermore, we demonstrate this efficiency through an ablation experiment in the Appendix. (7) In refinement loop, we iteratively compute and apply the edit guidance to the target noisy latentsztgt as in Eq. (7). This iterative guidance process guarantees progressive deviation of ztgt , resulting in the removal of the object from its old location and harmonization of the object with the context at its new location. from zman Algorithm 1: Algorithm Overview of PixelMan , ϵ) 0 = E(Isrc); ˆz0 = RGP(zt, ˆϵt, t) zout 0 = E(Iman) Require: VAE Encoder: zt = E(I); VAE Decoder: = D(zt) Require: ˆϵt, {K, } = UNet(zt, t) Require: zt = FDP(z0, ϵ); Require: zsrc zman 0 = E(Iman); Require: source, target, and inpaint mask: mold, mnew, mipt 1: for time-step {T, 1, ..., 1} do 2: 3: 4: 5: 6: 7: 8: 9: 10: 11: 12: 13: 14: 15: 16: 17: 18: 19: end for Output: Iout = D(zout 0 ) ϵ (0, I) = FDP(zsrc zsrc 0 , ϵ) = FDP(zman zman 0 ztgt = FDP(zout 0 , ϵ) for repeat do ztgt ztgt end for ˆϵman = UNet(zman ˆϵsrc , {Ksrc, Vsrc} = UNet(zsrc ˆϵtgt = UNet(ztgt 0 = RGP(zman ˆzman ˆztgt 0 = RGP(ztgt if {2, 1} then 0 = zman zout , t) , t; {Ksrc, Vsrc}) i.e., last few time-steps no masked-blending save K,V apply leak-proof SA 0 = zman zout end if ) (1 mnew) with mask the edited output image latents optimization zt E(ztgt 0 + (ˆztgt 0 + (ˆztgt , ˆϵman , t) 0 ˆzman 0 ˆzman , zman , ˆϵtgt else , t) , t) ) ) 0 0 Experiments First, we evaluate the effectiveness of PixelMan in the representative consistent object editing task, which is object repositioning. In addition, we also apply PixelMan on other consistent object editing tasks including object resizing, and object pasting to demonstrate the generalizability to different tasks (see Appendix). For the object repositioning task, we perform extensive quantitative evaluation and visual comparisons, both against the existing methods, as well as ablation studies on the various components of PixelMan (placed (a) (b) (c) (d) (e) (f) (g) (h) Input SDv2-Inpainting +AnyDoor (50 steps, 15s) SelfGuidance (50 steps, 11s) DragonDiffusion (50 steps, 23s) DiffEditor (50 steps, 24s) DiffEditor (16 steps, 9s) PixelMan (16 steps, 9s) Figure 2: Visual comparisons on COCOEE dataset. PixelMan achieves consistent object editing for object repositioning with lower latency and fewer inference steps, while better preserving image consistency and achieving cohesive inpainting. in the Appendix due to space constraints). In the Appendix, we also provide complete details of all the quantitative results and more visual comparisons. For our experiments, we adopted subsets of two challenging datasets, namely COCOEE (Lin et al. 2014; Yang et al. 2022) and ReS (Wang et al. 2024) (detailed in the Appendix). To comprehensively evaluate performance quantitatively, we adopted 9 metrics from 4 categories (elaborated in the Appendix): Image Quality Assessment (3), Object Consistency (2), Background Consistency (2), and Semantic Consistency (2). For Efficiency, we compare the number of inference steps, NFEs (i.e., number of UNet calls), and the average latency over 10 runs. Results on Object Repositioning We compare PixelMan with three existing object repositioning methods including, SelfGuidance (Epstein et al. 2023), DragonDiffusion (Mou et al. 2024b), and DiffEditor (Mou et al. 2024a) (SOTA). All training-free methods are evaluated based on SDv1.5 (Rombach et al. 2022; AI 2022a) to align with (Mou et al. 2024b,a). For thorough evaluations, we also consider training-based baseline using SDv2Inpainting Model (Rombach et al. 2022; AI 2022b) to inpaint the original location and then use AnyDoor (Chen et al. 2024b) for inserting the object at the target location. Overall performance. In Figs. 3a, 3b, and 3c, we compare PixelMan against the four contenders on the COCOEE dataset at the same number of inference steps (8, 16, and 50 steps respectively). At 50 steps, PixelMan outperforms all other methods in 9 out of 9 metrics. At 16 steps, PixelMan outperforms other methods in 8 out of 9 metrics, while being second place on the MUSIQ IQA metric. At 8 steps, PixelMan scores the best in 8 out of 9 metrics, while being second place on the TOPIQ IQA metric. Overall, PixelMan outperforms the other methods at the same number of steps. In the Appendix, we provide the full quantitative results and visual comparisons of all methods at both 16 and 50 steps. Efficiency. More importantly, PixelMan achieves superior performance with fewer NFEs than existing methods. We attribute this to our three-branched inversion-free sampling approach that avoids quality degradation at 16 steps, seen in methods (Epstein et al. 2023; Mou et al. 2024b,a) that rely on DDIM inversion (e.g., row DiffEditor 16 steps of Fig. 2). As shown in Table 1, PixelMan at 16 steps requires 112 fewer computations and is 15 seconds faster than the SOTA DiffEditor on COCOEE. Despite being faster, PixelMans quality at 16 steps surpasses DiffEditors at 50 steps (additional examples in the Appendix). Therefore, hereafter, we directly compare PixelMan at 16 steps to other (a) COCOEE dataset, all methods using 8 steps (b) COCOEE dataset, all methods using 16 steps (c) COCOEE dataset, all methods using 50 steps (d) COCOEE dataset, PixelMan using 16 steps, others using 50 steps (e) ReS dataset, PixelMan using 16 steps, others using 50 steps Figure 3: Radar charts that shows normalized evaluation metric values of different methods. TOPIQ, MUSIQ, LIQE belong to IQA; LPIPS (neg) and PSNR belong to Object Consistency; LPIPS (neg) and PSNR belong to Background Consistency; and CLIP-T2T and CLIP-I2I belong to Semantic Consistency. Detailed results and additional comparisons in Appendix. Table 1: Efficiency comparisons. PixelMan at 16 steps performs 112 fewer NFEs and is 15 seconds faster than DiffEditor (Mou et al. 2024a) on the COCOEE dataset. #Steps NFEs COCOEE avg(lat.) ReS avg(lat.) SD2+AnyDoor SelfGuidance DragonDiffusion DiffEditor PixelMan (ours) 50 50 50 50 16 100 100 160 176 64 15 11 23 24 9 16 14 30 32 11 methods at 50 steps in the following evaluation categories. Image quality. In Fig. 3d, PixelMan (16 steps) achieves significantly better image quality in all three IQA metrics than the other methods (50 steps) on COCOEE. In Fig. 3e, PixelMan has similar image quality to DragonDiffusion and DiffEditor on ReS dataset even when using significantly fewer steps. In visual comparisons, we observe PixelMan achieves overall better image quality than other methods while being more efficient. This includes less artifacts, more natural colors, well-blended objects and backgrounds, and natural lighting and shadow. Object consistency. PixelMan (16 steps) excels in object consistency on both COCOEE and ReS datasets (Figs. 3d, 3e), as measured by LPIPS (neg) and PSNR. Our three-branched inversion-free sampling approach helps the faithful reproduction of the object at the new location since we always anchor the output latents to the pixel-manipulated latents which ensures the moved object to be consistent with the original object. This is evident in Fig. 2 and Fig. 6 in Appendix, where PixelMan consistently preserves details like shape, color, and texture (e.g., clock, bird, airplane, orange). Background consistency. On both COCOEE and ReS datasets (i.e., Fig. 3d and Fig. 3e), PixelMan outperforms all other methods in both background consistency metrics LPIPS (neg) and PSNR. In the visual examples in Fig. 2, we observe the background in PixelMans edited images are more consistent with the source image (e.g., the grass texture and color in (b) and the water color in (e)). Inpainting. We provide abundant visual comparisons to assess the inpainting quality in Fig. 2 and in Figs. 6, 7, 8, 9, and 10 (in the Appendix). Here, we see PixelMan excels at removing objects (e.g., plane, pillow, orange in Fig. 2) while preserving the surrounding scene. Conversely, other methods either leave traces of the original object or introduce new artifacts in the inpainted area. Semantic consistency. PixelMan outperforms all methods on COCOEE and is best in CLIP-I2I on ReS and remains competitive in CLIP-T2T (see Fig. 3). PixelMan preserves the original semantics of the source image, while maintaining consistency in object, background and better inpainting quality (e.g., 2 instead of 3 oranges in Fig. 2 (h)). Conclusion In this work, we propose PixelMan, an inversion-free and training-free method for achieving consistent object editing via Pixel Manipulation and generation. PixelMan maintains image consistency by directly creating duplicate copy of the source object at target location in the pixel space, and we introduce an efficient sampling approach to iteratively harmonize the manipulated object into the target location and inpaint its original location. The key to ensuring image consistency is anchoring the output image to be generated to the pixel-manipulated image as well as introducing various consistency-preserving optimization techniques during inference. Moreover, we propose leak-proof SA manipulation technique to enable cohesive inpainting by addressing the attention leakage issue which is root cause of failed inpainting. Quantitative results on the COCOEE and ReS datasets as well as extensive visual comparisons show that PixelMan achieves superior performance in consistency metrics for object, background, and image semantics while achieving higher or comparable performance in IQA metrics. As training-free method, PixelMan only requires 16 inference steps with lower average latency and lower number of NFEs than current popular methods. SDXL-Turbo. https://huggingface.co/ References Adobe. 2023. AI Photo Editor: Edit Images with AI in Photoshop - Adobe. https://www.adobe.com/products/ photoshop/ai.html. Accessed: 2024-05-22. AI, S. 2022a. SDv1.5. https://huggingface.co/runwayml/ stable-diffusion-v1-5. Accessed: 2024-05-14. https://huggingface.co/ AI, S. 2022b. SDv2-inpainting. stabilityai/stable-diffusion-2-inpainting. Accessed: 202405-14. AI, S. 2023. stabilityai/sdxl-turbo. Accessed: 2024-05-14. Alaluf, Y.; Garibi, D.; Patashnik, O.; Averbuch-Elor, H.; and Cohen-Or, D. 2023. Cross-image attention for zero-shot appearance transfer. arXiv preprint arXiv:2311.03335. Brooks, T.; Holynski, A.; and Efros, A. A. 2023. Instructpix2pix: Learning to follow image editing instructions. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 1839218402. Cao, M.; Wang, X.; Qi, Z.; Shan, Y.; Qie, X.; and Zheng, Y. 2023. Masactrl: Tuning-free mutual self-attention control for consistent image synthesis and editing. In Proceedings of the IEEE/CVF International Conference on Computer Vision, 2256022570. Chang, H.; Zhang, H.; Barber, J.; Maschinot, A.; Lezama, J.; Jiang, L.; Yang, M.-H.; Murphy, K.; Freeman, W. T.; Rubinstein, M.; et al. 2023. Muse: Text-to-image generation via masked generative transformers. arXiv preprint arXiv:2301.00704. Chefer, H.; Alaluf, Y.; Vinker, Y.; Wolf, L.; and CohenOr, D. 2023. Attend-and-Excite: Attention-Based Semantic Guidance for Text-to-Image Diffusion Models. arXiv:2301.13826. Chen, C.; Mo, J.; Hou, J.; Wu, H.; Liao, L.; Sun, W.; Yan, Q.; and Lin, W. 2024a. Topiq: top-down approach from semantics to distortions for image quality assessment. IEEE Transactions on Image Processing. Chen, X.; Huang, L.; Liu, Y.; Shen, Y.; Zhao, D.; and Zhao, H. 2024b. Anydoor: Zero-shot object-level image cusIn Proceedings of the IEEE/CVF Conference tomization. on Computer Vision and Pattern Recognition, 65936602. Dahary, O.; Patashnik, O.; Aberman, K.; and CohenBe Yourself: Bounded Attention for Or, D. 2024. arXiv preprint Multi-Subject Text-to-Image Generation. arXiv:2403.16990. Dhariwal, P.; and Nichol, A. 2021. Diffusion models beat gans on image synthesis. Advances in neural information processing systems, 34: 87808794. Dosovitskiy, A.; Beyer, L.; Kolesnikov, A.; Weissenborn, D.; Zhai, X.; Unterthiner, T.; Dehghani, M.; Minderer, M.; Heigold, G.; Gelly, S.; et al. 2020. An image is worth 16x16 words: Transformers for image recognition at scale. arXiv preprint arXiv:2010.11929. Duan, X.; Cui, S.; Kang, G.; Zhang, B.; Fei, Z.; Fan, M.; and Huang, J. 2024. Tuning-free inversion-enhanced control for consistent image editing. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 38, 16441652. Endo, Y. 2022. User-Controllable Latent Transformer for StyleGAN Image Layout Editing. Computer Graphics Forum, 41(7): 395406. Epstein, D.; Jabri, A.; Poole, B.; Efros, A.; and Holynski, A. 2023. Diffusion self-guidance for controllable image generation. Advances in Neural Information Processing Systems, 36: 1622216239. Gafni, O.; Polyak, A.; Ashual, O.; Sheynin, S.; Parikh, D.; and Taigman, Y. 2022. Make-a-scene: Scene-based text-toimage generation with human priors. In European Conference on Computer Vision, 89106. Springer. Garibi, D.; Patashnik, O.; Voynov, A.; Averbuch-Elor, H.; and Cohen-Or, D. 2024. Image arXiv preprint Inversion Through Iterative Noising. arXiv:2403.14602. ReNoise: Real Goel, V.; Peruzzo, E.; Jiang, Y.; Xu, D.; Sebe, N.; Darrell, T.; Wang, Z.; and Shi, H. 2023. PAIR-Diffusion: ObjectLevel Image Editing with Structure-and-Appearance Paired Diffusion Models. arXiv preprint arXiv:2303.17546. 2023. Google. Google https://blog.google/products/photos/google-photos-magiceditor-pixel-io-2023/. Accessed: 2024-05-22. Photos MagicEditor. Hertz, A.; Mokady, R.; Tenenbaum, J.; Aberman, K.; Pritch, Y.; and Cohen-Or, D. 2022. Prompt-to-prompt imarXiv preprint age editing with cross attention control. arXiv:2208.01626. Hertz, A.; Voynov, A.; Fruchter, S.; and Cohen-Or, D. 2023. Style aligned image generation via shared attention. arXiv preprint arXiv:2312.02133. Ho, J.; Jain, A.; and Abbeel, P. 2020. Denoising diffusion probabilistic models. Advances in neural information processing systems, 33: 68406851. Jeong, J.; Kim, J.; Choi, Y.; Lee, G.; and Uh, Y. 2024. Visual Style Prompting with Swapping Self-Attention. arXiv preprint arXiv:2402.12974. Kang, M.; Zhu, J.-Y.; Zhang, R.; Park, J.; Shechtman, E.; Paris, S.; and Park, T. 2023. Scaling up gans for text-toimage synthesis. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 10124 10134. Kawar, B.; Zada, S.; Lang, O.; Tov, O.; Chang, H.; Dekel, T.; Mosseri, I.; and Irani, M. 2023. Imagic: Text-based real image editing with diffusion models. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 60076017. Ke, J.; Wang, Q.; Wang, Y.; Milanfar, P.; and Yang, F. 2021. Musiq: Multi-scale image quality transformer. In Proceedings of the IEEE/CVF international conference on computer vision, 51485157. Kirillov, A.; Mintun, E.; Ravi, N.; Mao, H.; Rolland, C.; Gustafson, L.; Xiao, T.; Whitehead, S.; Berg, A. C.; Lo, W.- In Proceedings of the Y.; et al. 2023. Segment anything. IEEE/CVF International Conference on Computer Vision, 40154026. Li, J.; Li, D.; Xiong, C.; and Hoi, S. 2022. Blip: Bootstrapping language-image pre-training for unified visionIn International language understanding and generation. conference on machine learning, 1288812900. PMLR. Lin, T.-Y.; Maire, M.; Belongie, S.; Hays, J.; Perona, P.; Ramanan, D.; Dollar, P.; and Zitnick, C. L. 2014. Microsoft In Computer Vision coco: Common objects in context. ECCV 2014: 13th European Conference, Zurich, Switzerland, September 6-12, 2014, Proceedings, Part 13, 740 755. Springer. Liu, L.; Ren, Y.; Lin, Z.; and Zhao, Z. 2022. Pseudo numerical methods for diffusion models on manifolds. arXiv preprint arXiv:2202.09778. Luo, S.; Tan, Y.; Huang, L.; Li, J.; and Zhao, H. 2023. Latent consistency models: Synthesizing high-resolution images with few-step inference. arXiv preprint arXiv:2310.04378. Mou, C.; Wang, X.; Song, J.; Shan, Y.; and Zhang, J. 2024a. DiffEditor: Boosting Accuracy and Flexibility on Diffusionbased Image Editing. arXiv preprint arXiv:2402.02583. Mou, C.; Wang, X.; Song, J.; Shan, Y.; and Zhang, J. 2024b. DragonDiffusion: Enabling Drag-style Manipulation on Diffusion Models. In The Twelfth International Conference on Learning Representations. Pan, X.; Tewari, A.; Leimkuhler, T.; Liu, L.; Meka, A.; and Theobalt, C. 2023. Drag your gan: Interactive point-based In ACM manipulation on the generative image manifold. SIGGRAPH 2023 Conference Proceedings, 111. Parmar, G.; Kumar Singh, K.; Zhang, R.; Li, Y.; Lu, J.; and Zhu, J.-Y. 2023. Zero-shot image-to-image translation. In ACM SIGGRAPH 2023 Conference Proceedings, 111. Patashnik, O.; Garibi, D.; Azuri, I.; Averbuch-Elor, H.; and Cohen-Or, D. 2023. Localizing object-level shape variations with text-to-image diffusion models. In Proceedings of the IEEE/CVF International Conference on Computer Vision, 2305123061. Radford, A.; Kim, J. W.; Hallacy, C.; Ramesh, A.; Goh, G.; Agarwal, S.; Sastry, G.; Askell, A.; Mishkin, P.; Clark, J.; et al. 2021. Learning transferable visual models from natIn International conference on ural language supervision. machine learning, 87488763. PMLR. Ramesh, A.; Dhariwal, P.; Nichol, A.; Chu, C.; and Chen, M. 2022. Hierarchical text-conditional image generation with clip latents. arXiv preprint arXiv:2204.06125, 1(2): 3. Rezende, D.; and Mohamed, S. 2015. Variational inference with normalizing flows. In International conference on machine learning, 15301538. PMLR. Rombach, R.; Blattmann, A.; Lorenz, D.; Esser, P.; and Ommer, B. 2022. High-resolution image synthesis with latent diffusion models. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, 10684 10695. Saharia, C.; Chan, W.; Chang, H.; Lee, C.; Ho, J.; Salimans, T.; Fleet, D.; and Norouzi, M. 2022a. Palette: Image-toimage diffusion models. In ACM SIGGRAPH 2022 conference proceedings, 110. Saharia, C.; Chan, W.; Saxena, S.; Li, L.; Whang, J.; Denton, E. L.; Ghasemipour, K.; Gontijo Lopes, R.; Karagol Ayan, Photorealistic text-toB.; Salimans, T.; et al. 2022b. image diffusion models with deep language understanding. Advances in Neural Information Processing Systems, 35: 3647936494. Shi, Y.; Xue, C.; Pan, J.; Zhang, W.; Tan, V. Y.; and Bai, S. 2023. Dragdiffusion: Harnessing diffusion models for interactive point-based image editing. arXiv preprint arXiv:2306.14435. Song, J.; Meng, C.; and Ermon, S. 2020. Denoising diffusion implicit models. arXiv preprint arXiv:2010.02502. Song, Y.; Sohl-Dickstein, J.; Kingma, D. P.; Kumar, A.; Ermon, S.; and Poole, B. 2020. Score-based generative modeling through stochastic differential equations. arXiv preprint arXiv:2011.13456. Tumanyan, N.; Geyer, M.; Bagon, S.; and Dekel, T. 2023. Plug-and-play diffusion features for text-driven image-toIn Proceedings of the IEEE/CVF Conimage translation. ference on Computer Vision and Pattern Recognition, 1921 1930. Vaswani, A.; Shazeer, N.; Parmar, N.; Uszkoreit, J.; Jones, L.; Gomez, A. N.; Kaiser, Ł.; and Polosukhin, I. 2017. Attention is all you need. Advances in neural information processing systems, 30. Wang, Y.; Cao, C.; Dong, Q.; Li, Y.; and Fu, Y. 2024. arXiv preprint Repositioning the Subject within Image. arXiv:2401.16861. Winter, D.; Cohen, M.; Fruchter, S.; Pritch, Y.; Rav-Acha, A.; and Hoshen, Y. 2024. ObjectDrop: Bootstrapping Counterfactuals for Photorealistic Object Removal and Insertion. arXiv preprint arXiv:2403.18818. Xu, S.; Huang, Y.; Pan, J.; Ma, Z.; and Chai, J. 2024. Inversion-Free Image Editing with Natural Language. In Conference on Computer Vision and Pattern Recognition 2024. Yang, B.; Gu, S.; Zhang, B.; Zhang, T.; Chen, X.; Sun, X.; Chen, D.; and Wen, F. 2022. Paint by Example: Exemplarbased Image Editing with Diffusion Models. arXiv preprint arXiv:2211.13227. Yu, J.; Xu, Y.; Koh, J. Y.; Luong, T.; Baid, G.; Wang, Z.; Vasudevan, V.; Ku, A.; Yang, Y.; Ayan, B. K.; et al. 2022. Scaling autoregressive models for content-rich text-to-image generation. arXiv preprint arXiv:2206.10789. Zhang, L.; Rao, A.; and Agrawala, M. 2023. Adding conditional control to text-to-image diffusion models. In Proceedings of the IEEE/CVF International Conference on Computer Vision, 38363847. Zhang, R.; Isola, P.; Efros, A. A.; Shechtman, E.; and Wang, O. 2018. The unreasonable effectiveness of deep features as perceptual metric. In Proceedings of the IEEE conference on computer vision and pattern recognition, 586595. Zhang, W.; Zhai, G.; Wei, Y.; Yang, X.; and Ma, K. 2023. Blind Image Quality Assessment via Vision-Language CorIn IEEE respondence: Multitask Learning Perspective. Conference on Computer Vision and Pattern Recognition, 1407114081. Zhou, Y.; Xiao, R.; Lischinski, D.; Cohen-Or, D.; and Huang, H. 2024. Generating Non-Stationary Textures using Self-Rectification. arXiv preprint arXiv:2401.02847."
        },
        {
            "title": "Technical Appendix",
            "content": "This technical appendix to our main paper has the following sections: In Section Background and Related Works, we provide more details of the background and related works. In Section Implementation and Evaluation Details, we provide the implementation details and evaluation settings including the datasets and metrics. In Section Ablation Study, we present an ablation study on the design of PixelMan. In Section Detailed Results, we present more details of our quantitative results and additional qualitative (visual) comparison examples. Moreover, we provided comparisons to additional baseline methods."
        },
        {
            "title": "Background and Related Works",
            "content": "Diffusion Models Diffusion Models (DMs) (Rombach et al. 2022; Saharia et al. 2022b; Ramesh et al. 2022; Gafni et al. 2022; Chang et al. 2023; Yu et al. 2022; Kang et al. 2023) are class of generative models that learn to draw high-fidelity samples from the complex real-world data-distribution. They employ Forward Diffusion Process (FDP) that progressively adds noise to real data point z0, transforming it into sample zT from the unit Gaussian. model is then trained to iteratively denoise zT through the Reverse Generative Process (RGP). This denoising capability allows the model to generate new data by iteratively removing noise from any sample from (0, I). To reduce computational costs, it is common to work in the latent space of VAE (Rezende and Mohamed 2015) instead of the high-dimensional pixel space. These models are referred to as Latent Diffusion Models (LDMs) (Rombach et al. 2022). DMs learn to iteratively denoise randomly sampled noise from unit Gaussian and construct meaningful data point (e.g., an image). The training and inference happens in two phases. The first phase is the Forward Diffusion Process (FDP), in which we corrupt data point sampled from the real data distribution with known noise sampled from the unit Gaussian, as follows: zt = αt z0 + 1 αt ϵ, ϵ (0, I), (8) where z0 is the (latent (Rombach et al. 2022) representation of) ground truth data point, zt is the corrupted sample, and αt is time-dependent coefficient. We then train the model to estimate this added noise: L(θ) = EtU (1,T ),ϵN (0,I) (cid:2) ϵ ϵθ(zt, t, y)2 (cid:3), (9) where could be conditioning signal such as text prompt. The second phase, which is done during inference, is referred to as the Reverse Generative Process (RGP), where, starting from pure noise sampled from the unit Gaussian, we can not only obtain direct estimate of the initial latent ˆz0, but also iteratively update our prediction of the latent of the previous time-step (i.e., zt1), both using the noise estimations from the trained model: zt1, ˆz0 = (cid:0)zt, ϵθ(zt, t, y), t(cid:1), where represents sampling strategy such as DDPM (Ho, Jain, and Abbeel 2020), DDIM (Song, Meng, and Ermon 2020), PNDMS (Liu et al. 2022), etc. (10) Self-Attention Self-Attention (SA) score matrix (i.e., Softmax(QK / d)) (Vaswani et al. 2017) plays crucial role in understanding the relationships between objects in an image (Dosovitskiy et al. 2020). It calculates probability distribution for each element (here, pixel in an intermediate feature map), that indicates the relative importance of that element with respect to all other elements. Specifically, the value at any given row and column in the QK matrix quantifies the impact of pixel on pixel i. By modifying the SA matrix at certain indices corresponding to particular objects, we can exert control over the editing process (Patashnik et al. 2023). Generative Semantic Nursing Generative Semantic Nursing (GSN) (Chefer et al. 2023) refers to slightly updating the latents zt at every time-step during RGP, guided by carefully designed loss function that encourages the success of the task at hand. The update is done through gradient descent, using the Jacobian of the loss i.e., zt zt ηztLGSN() The loss in (Chefer et al. 2023) is designed to encourage better consideration of the semantic information in the text prompt while image generation. Specifically, this loss leverages CA maps to enforce object presence, enabling faithful text-to-image generation. In this work, we optimize the latents for consistent object editing. (11) Editing with DMs Energy guidance. Self-guidance (Epstein et al. 2023) is the first work that introduced Energy Guidance (EG) to guide the edit process by updating the estimated noise. EG is inspired by classifier-guidance (Dhariwal and Nichol 2021; Song et al. 2020), which was originally used to convert an unconditional DM, Pr(x), into conditional one, Pr(xy): Pr(zty) Pr(zt) Pr(yzt) (12) zt log Pr(zty) = zt log Pr(zt) + zt log Pr(yzt)(13) ˆϵ = ϵθ zt log Pr(yzt), (14) where ϵθ is the noise estimation of the unconditional model and Pr(yzt) is classifier that given the noisy latent, yields the probability that this noisy image belongs to the desired class (i.e., condition), and ˆϵ is the guided noise directed towards generating an image that has higher chance of belonging to class y. It is common in the literature to multiply the classifier guidance term zt log Pr(yzt) with [timedependent] coefficient η to control the guidance strength. Epstein et al. (2023) pointed out that, in general, any energy function (to be minimized) can be used to guide the estimated noise throughout the RGP, and not just function of probabilities from classifier (i.e., zt log Pr(yzt)). The update is then as follows: ˆϵ = ϵθ + ztE(), where E() denotes the energy function defined on some intermediate variables in the UNet. Addressing the promptbased editing task, (Epstein et al. 2023) define their energy function based on the Cross-Attention (CA) maps and require the prompt to provide directions for the desired edit. (15) DragonDiffsuon and DiffEditor. DragonDiffuson (Mou et al. 2024b) and DiffEditor (Mou et al. 2024a) consider the point-based editing task instead. For object movement, they define an energy function with four components, namely: (i) Edit, which makes sure object appears in the destination and looks the same; (ii) Content, which makes sure everything else stays the same; (iii) Contrast, which makes sure the patch whose object has moved no longer looks like before; and (iv) Inpaint, which makes sure the inpainted area blends well with the surroundings. Each energy component has coefficient. (i.e., {k1, k2, k3, k4}) The overall energy function is defined as: = k1 Eedit + k2 Econtent + k3 Econtrast + k4 Einpaint (16) After obtaining the initial noise from DDIM inversion (Dhariwal and Nichol 2021) which requires 50 Network Function Evaluations (NFEs; i.e., UNet calls) the guidance is applied with respect to zt and zsrc within guidance refinement loop. Being an EG-based method, since it is the ϵ that is updated with each guidance step (see Eq. (15)), the algorithm requires to call on the DDIM inversion second time (referred to as time travel in the paper), to propagate the change from ϵ to the intermediate latents zt in order for the refinement loop to function properly. This increases the NFEs even more. Inversion In order to maintain the consistency of the context of source image in the edited image, the majority of training-free approaches require to obtain the initial noise that could have generated the source image given the pre-trained UNet. They use the DDIM inversion technique (Dhariwal and Nichol 2021) to obtain this initial noise zT , and then, apply their proposed guidance or manipulation on that initial noisy latent, either at the first step or throughout the entire RGP. However, due to the asymmetry between RGP and DDIM inversion process, the skip time intervals must be very small, in order to obtain valid initial noise. Hence, the number of inference time-steps must be large (usually 50) which renders few-steps editing with it unachievable. Renoise (Garibi et al. 2024) uses fixed-point iteration to refine its estimation of the noise at each time-step. However, since at each refinement step, the UNet is called several times, the NFE is not reduced by much. An alternative approach to inversion is DDCM (Xu et al. 2024), which facilitates inversion-free prompt-based editing. Specifically, starting from randomly sampled noise from the unit Gaussian (zT ), (Xu et al. 2024) calculates the consistency noise (ϵcon) from the source image and zT . This ϵcon is the golden noise that would take us back to the source image. Then, they run the edit process in two parallel iterative branches: one branch predicts the noise from zsrc and , ysrc)) and the other the original prompt ysrc (i.e., ϵsrc (zsrc branch predicts the noise from ztgt and the edit prompt ytgt (i.e., ϵtgt , ytgt)). Next, the edit direction is determined by ϵtgt , to which the golden consistency noise ϵcon is added: (ztgt ϵsrc ϵt = ϵtgt ϵsrc Finally, this ϵt is used to estimate the denoised edited image ztgt 0 and with that, the RGP continues. + ϵcon. (17) t t"
        },
        {
            "title": "Implementation and Evaluation Details",
            "content": "Obtaining the msim Mask To obtain mask msim, we leverage the average of QK matrix rows that correspond to pixels in the mnew region. This represents, on average, how much attention pixels in area mnew are paying to other pixels. Since the edited object is in area mnew, pixels in this area are likely to pay more attention to the similar objects (e.g., the other similar apples in Fig. 1). This averaged row is rearranged to spatial self-attention map illustrated as msim in Fig. 1, which we convert into binary mask with threshold of 0.1 (selected by comparing different values from 0.1 to 0.5). This mask (msim) represents the area of objects similar to the to-be-edited object. We extract msim from the QK matrix at each time-step t, and use it in the next time-step 1.2 Implementation Details We perform our experiments on Nvidia V100 (32G) GPU. For the reported latency, we measure the wall-clock time for editing one image, averaged over 10 runs on 1 Nvidia V100 (32G) GPU."
        },
        {
            "title": "All",
            "content": "training-free methods are evaluated based on SDv1.5 (Rombach et al. 2022; AI 2022a) to align with (Mou et al. 2024b,a). For thorough evaluations, we also consider training-based baseline using SDv2-Inpainting Model (Rombach et al. 2022; AI 2022b) to inpaint the original location and then use AnyDoor (Chen et al. 2024b) for inserting the object at the target location. For the existing methods (Chen et al. 2024b; Epstein et al. 2023; Mou et al. 2024b,a) that require prompt describing the scene, we adopt the BLIP (Li et al. 2022) image captioning model to generate prompt for the given source image. In Eq. (6), we apply mask = 1 mnew in timesteps < 2 (value 2 is selected by testing out values from 1 to 5), where is the number of inference timesteps (i.e., mask is not applied in the last two inference steps). We apply Gaussian blurring filter to the mask with kernel size of 9 (selected from testing out values of 5 to 11). The masking strategy ensures seamless blending with the context and allows the model to refine the details of the edited image. 2Note that msim is empty at the first time-step. Following DragonDiffusion (Mou et al. 2024b), we apply latents optimization updates in every timestep for < 0.2 , and once every two timesteps for 0.2 < 0.6 . For timesteps where 0.4 < < 0.6 , we perform three repeated GSN updates (i.e., = 3 in Algorithm 1). We use the same energy function coefficients and update step size as in DragonDiffusion (Mou et al. 2024b). The and matrix injection happens at all timesteps for the Self-Attention layers of all upsampling blocks in the UNet. For Self-Attention maps used to obtain msim, we use the average of SA layers in the last three upsampling blocks in the UNet. Evaluation Datasets COCOEE dataset. This is an editing benchmark which Yang et al. (2022) compiled by manually selecting 3,500 images from the MSCOCO (MicroSoft Common Objects in COntext) (Lin et al. 2014) validation set. In our work, human-operator used the Segment Anything model (Kirillov et al. 2023) on random subset of COCOEE to retrieve multiple segments from each image. Then, they identified reasonable segmented object and suggested diff vector that determines where the respective object should be moved to. The result is benchmark with 100 images along with mask and diff vector for each. ReS dataset. Recently, Wang et al. (2024) open-sourced the ReS (Repositioning the Subject within image) dataset, which is real-world benchmark of 100 pairs of images.3 This is challenging dataset due to changes in perspective / scale of the moved objects, lighting, shadows, etc. For each pair, single object is moved while everything else in the scene is kept intact. Note that it is possible for the move to take portion of the object either behind another object or outside of the image frame (we refer to these as occlusion cases). In this work, we do not consider tasks that involve occlusion and only compare performance of the considered methods on 162 object movement tasks. Evaluation Metrics Besides efficiency, which we evaluate based on (i) number of inference steps, (ii) NFEs (in terms of number of UNet calls), and (iii) Latency (in terms of wall-clock time for editing one image, averaged over 10 runs), we distinguish ourselves by also conducting comprehensive quality evaluation using quantitative metrics on all compared methods. Here, we list the specific metrics used in this work: For"
        },
        {
            "title": "Image Quality Assessment",
            "content": "(IQA), we evaluate the overall perceptual image quality by adopting TOPIQ (Chen et al. 2024a), MUSIQ (Ke et al. 2021), and LIQE (Zhang et al. 2023). To evaluate Object Consistency, we measure the similarity of the moved object to the original object in the source image through LPIPS (neg) (Zhang et al. 2018) and PSNR metrics. 3For each image pair, we can define two movement tasks: from the first image to the second image and vice versa (so 200 movement tasks in total). For Background Consistency, we evaluate the similarity of the background in the edited image to the background in the original image with the same metrics as in Object Consistency. For Semantic Consistency, we consider the similarity between the semantics of the source image and the edited image through CLIP-I2I and CLIP-T2T (Radford et al. 2021; Chefer et al. 2023; Li et al. 2022). CLIP-I2I is the image-to-image similarity (i.e., similarity between CLIP embeddings of the source and edited image). For the CLIP-T2T text-to-text similarity, we measure the similarity between CLIP embeddings of text caption of the source image and the caption of the edited image. Both captions are obtained using the BLIP (Li et al. 2022) image captioning model. Ablation Study To understand the contribution of each major technique in PixelMan, we perform ablation studies with both quantitative and qualitative (visual) comparisons. We show the ablation qualitative visual comparisons in Fig. 4 and we report the quantitative results in Table 2. For each of the techniques, we keep the rest of the method unchanged (i.e., all other components), and ablate for that specific technique. Latents optimization vs. predicted noise update: we compare the latents (zt) optimization (also known as GSN (Chefer et al. 2023)) with the predicted noise (ˆϵt) update (i.e., Energy Guidance (EG) (Mou et al. 2024b,a)). In Table 2, our results show that latents optimization achieves comparable performance in most metrics with fewer NFEs than EG, demonstrating its efficiency. In Fig. 4, the qualitative (visual) result of EG and latents update is also similar, while the latents optimization approach achieves this with better efficiency in fewer NFEs and lower latency. Three-branched inversion-free sampling vs. DDIM inversion: we compare the three-branched inversion-free sampling approach with the DDIM inversion (Dhariwal and Nichol 2021) technique. In Table 2, our inversionfree sampling approach significantly improves the performance in four metric categories (i.e., IQA, object consistency, background consistency, and semantic consistency). From the visual examples in Fig. 4, using DDIM inversion results in inconsistent colors and artifacts, low object consistency and background consistency. Leak-proof SA: we ablate the leak-proof SA technique by disabling it. As shown in Table 2, it significantly improves semantic consistency while being comparable on other metrics. In Fig. 4, without using leak-proof SA, we observe that the model often fails to remove the object at the original location. Our result reveals that leak-proof SA is the key to enhancing the models ability to remove the object from its original location. Pixel-manipulated branch: we ablate the effect of three-branched the pixel-manipulated branch in our inversion-free sampling approach. In Table 2, it significantly improves the performance on IQA, object consis- (a) (b) (c) (d) (e) (f) Input With EG (10s, 70 NFEs) With DDIM Inversion (8s, 58 NFEs) Without Leak-Proof SA (9s, 64 NFEs) Without Pixel-Manipulated Branch (8s, 64 NFEs) PixelMan (9s, 64 NFEs) Figure 4: Ablation qualitative examples on the COCOEE dataset at 16 steps. tency, and semantic consistency. In Fig. 4, without using the pixel-manipulated branch, the moved object is often missing or inconsistent with the original object. Therefore, the pixel-manipulated branch with the pixelmanipulated image improves the models ability to generate faithful representation of the object at its new location. K,V saving & injection: We ablate the effect of the K, saving and injection, where we save the K, from the UNet call in the feature-preserving source branch, and inject them in the target branch UNet call. As shown in Table 2, using K, saved from either source or pixel-manipulated branch significantly improves the background consistency and semantic consistency, which is also demonstrated visually in Fig. 5 that the image background is significantly degraded without K, saving or injection. Comparing saving the K, from the source branch or the pixel-manipulated branch, as shown in Fig. 5, the edited apple is better harmonized with more natural"
        },
        {
            "title": "No Saving or\nInjection",
            "content": "(7s, 48 NFEs) PixelFrom Manipulated Branch (7s, 48 NFEs) From Source (i.e., Branch PixelMan) (9s, 64 NFEs) Figure 5: Ablation on K, saving and injection. shadow and seamless blending when using the K, saved from the consistency-preserving source branch, although the introduction of the source branch will slightly increase the #NFEs and latency. Also observed in Fig. 5, saving the K, from the source branch results in more complete and cohesive inpainting. Table 2: Ablation experiments on key techniques of PixelMan on the COCOEE (Yang et al. 2022) dataset at 16 steps. The indicates lower is better, and the means the higher the better. The best performance result is marked in bold. Our reported latency measures the average wall-clock time over 10 runs for generating 1 image on this dataset in seconds with V100 GPU."
        },
        {
            "title": "Efficiency",
            "content": "# NFEs Latency (secs) Image Quality Assessment MUSIQ TOPIQ LIQE"
        },
        {
            "title": "Semantic\nConsistency",
            "content": "LPIPS PSNR LPIPS PSNR CLIPT2T CLIPI2I"
        },
        {
            "title": "Optimization Target\nEnergy Guidance\nLatents Optimization",
            "content": "Sampling DDIM Inversion Three-Branched Inversion-Free Leak-Proof SA No Yes Pixel-Manipulated Branch No Yes K, Saving and Injection No Saving or Injection From Manipulated Branch From Source Branch 70 64 58 64 64 64 64 48 48 64 10 9 0.607 0.605 70.01 69. 4.34 4.35 0.015 0.015 35.56 35.62 0.076 0.074 26.30 26.43 0.945 0. 0.974 0.974 8 9 9 9 8 9 7 7 9 0.565 0. 68.41 69.98 4.15 4.35 0.041 0.015 27.44 35.62 0.134 0.074 22.78 26. 0.924 0.946 0.942 0.974 0.602 0.605 70.77 69.98 4.42 4.35 0.015 0. 35.62 35.62 0.064 0.074 27.42 26.43 0.891 0.946 0.969 0.974 0.570 0. 67.52 69.98 4.19 4.35 0.077 0.015 23.59 35.62 0.066 0.074 26.68 26. 0.896 0.946 0.945 0.974 0.604 0.621 0.605 70.37 70.40 69.98 4.34 4.35 4.35 0.014 0.014 0. 36.02 36.10 35.62 0.112 0.074 0.074 24.28 26.75 26.43 0.875 0.943 0.946 0.950 0.973 0."
        },
        {
            "title": "Detailed Results",
            "content": "ing object resizing and object pasting. Detailed Quantitative Results For the experiments comparing to other methods in the main paper, we present the detailed results in Table 3 and Table 4, where we compare to existing methods on the COCOEE (Yang et al. 2022) and ReS (Wang et al. 2024) datasets at 8, 16, and 50 steps. Additional Qualitative Results In Figs. 6, 7 and 8, we provide additional visual comparisons among PixelMan and other methods on the COCOEE (Yang et al. 2022) datasets at both 16 and 50 steps. In Figs. 9 and 10, we present visual comparisons among PixelMan and other methods on the ReS (Wang et al. 2024) datasets at both 16 and 50 steps. As shown in Figs. 6, 7, 8, 9, and 10, PixelMan can better inpaint the original object while preserving the object consistency after the edit. In addition, the background is more consistent with less color shift and texture change. Most importantly, other methods have significant quality drop when using 16 steps compared to 50 steps. Whereas PixelMan can efficiently edit images at 16 steps while having better quality than other methods at 50 steps. In Fig. 11 and Fig. 12, we present the qualitative comparison of PixelMan and other methods at 8 inference steps on the COCOEE dataset and ReS dataset respectively. The other methods produce low-quality images at 8 steps, whereas PixelMan can still generate objects at the new location and inpaint the original location even at 8 steps. Other consistent object editing tasks. In Fig. 13, we apply PixelMan to other consistent object editing tasks, includComparison to Additional Baselines We present additional evaluation results in Tables 5 and 6, where we compare to additional baseline methods PAIR Diffusion (Goel et al. 2023) and InfEdit (Xu et al. 2024) on the COCOEE (Yang et al. 2022) and ReS (Wang et al. 2024) datasets at 8, 16, and 50 steps. Comparison to PAIR DIffusion. PixelMan is trainingfree, whereas PAIR-Diffusion requires costly DM model fine-tuning. Despite this distinction, for completeness, we conducted comparisons on object repositioning using the COCOEE and ReS datasets at 8, 16, and 50 inference steps (see Tables 5 and 6). PixelMan achieves lower latency and also eliminates the need for costly model fine-tuning. Regarding image quality, PixelMan consistently outperforms PAIR-Diffusion in all evaluated metrics (except for an on par LIQE score). In terms of visual quality, PixelMan delivers higher quality edits with more natural color, lighting, and shadow. It better preserves object identity and background details while fully removing the old object and seamlessly filling in the background. PAIR-Diffusion often struggles with these aspects. Visual comparisons are presented in Figs. 14 and 15. Comparison to InfEdit. PixelMan differs from InfEdit (Xu et al. 2024) in both task focus and methodology. While InfEdit relies on prompt guidance to edit rigid attributes (e.g., color, texture) based on differences between original and edited prompts, PixelMan is prompt-free, focusing on non-rigid attributes (e.g., position, size). PixelMan leverages pixel-manipulated latents as anchors and employs feature-preserving source branch to retain the original image details, allowing consistent object edits beyond the capability of prompt-based methods such as InfEdit. PixelMan applies inference-time optimization of latents via energy functions tailored for object generation, harmonization, inpainting, and background consistency. To fill vacated regions, leak-proof SA technique is introduced to prevent attention leakage to similar objects, ensuring cohesive inpainting. To show PixelMans advantages over InfEdit (Xu et al. 2024), we extended InfEdit to non-rigid editing using DiffEditor (Mou et al. 2024a)s energy guidance for necessary editing guidance. In Tables 5 and 6, we conducted object repositioning experiments comparing both methods on COCOEE and ReS datasets using 8, 16, and 50 steps. PixelMan achieves lower latency than InfEdit (Xu et al. 2024). In terms of image quality, PixelMan delivers visibly higher-quality images with fewer artifacts and smoother object-background blending, whereas InfEdit struggles with partial inpainting and inconsistencies. Moreover, PixelMan excels in preserving object and background details (e.g., shape, color, texture), while fully removing old objects and filling in coherent background, while InfEdit struggles to maintain object and semantic consistency. PixelMan outperforms InfEdit in object and semantic consistency while maintaining comparable IQA and background consistency scores. Visual comparisons are presented in Figs. 14 and 15. Table 3: Quantitative results on the COCOEE (Yang et al. 2022) dataset. Comparing PixelMan with other methods including Self-Guidance (Epstein et al. 2023), DragonDiffusion (Mou et al. 2024b), DiffEditor (Mou et al. 2024a), and the training-based SDv2-Inpainting+AnyDoor (Rombach et al. 2022; AI 2022b; Chen et al. 2024b) baseline. The indicates lower is better, and the means the higher the better. The best performance result is marked in bold and the second best result is annotated with underlines. Our reported latency measures the average wall-clock time over ten runs for generating one image on this dataset in seconds with V100 GPU."
        },
        {
            "title": "Efficiency",
            "content": "SDv2-Inpainting+AnyDoor Self-Guidance DragonDiffusion DiffEditor PixelMan SDv2-Inpainting+AnyDoor Self-Guidance DragonDiffusion DiffEditor PixelMan SDv2-Inpainting+AnyDoor Self-Guidance DragonDiffusion DiffEditor PixelMan SDv2-Inpainting+AnyDoor Self-Guidance DragonDiffusion DiffEditor PixelMan # Steps 50 50 50 50 16 50 16 8 # NFEs 100 100 160 176 64 100 100 160 176 206 32 32 64 58 64 16 16 32 32 Latency (secs) 15 11 23 24 9 15 11 23 24 27 5 4 9 9 9 3 2 5 5 4 Image Quality Assessment MUSIQ 67.61 65.91 68.87 69.09 69.98 67.61 65.91 68.87 69.09 70.17 67.66 69.07 69.92 69.99 69.98 66.86 69.58 68.45 68.44 69.63 TOPIQ 0.549 0.554 0.571 0.579 0.605 0.549 0.554 0.571 0.579 0.605 0.556 0.600 0.588 0.590 0.605 0.556 0.604 0.567 0.567 0.602 LIQE 3.98 3.90 4.27 4.27 4.35 3.98 3.90 4.27 4.27 4.36 3.93 4.13 4.31 4.30 4.35 3.78 3.95 4.05 4.05 4."
        },
        {
            "title": "Semantic\nConsistency",
            "content": "LPIPS 0.068 0.083 0.034 0.036 0.015 0.068 0.083 0.034 0.036 0.014 0.067 0.083 0.040 0.041 0.015 0.068 0.085 0.050 0.050 0.016 PSNR 24.28 22.77 28.59 28.49 35.62 24.28 22.77 28.59 28.49 35.92 24.44 22.85 27.58 27.52 35.62 24.50 22.72 26.84 26.86 35.33 LPIPS 0.172 0.259 0.098 0.094 0.074 0.172 0.259 0.098 0.094 0.077 0.172 0.195 0.124 0.125 0.074 0.177 0.232 0.186 0.186 0.071 PSNR 21.52 17.86 23.99 24.23 26.43 21.52 17.86 23.99 24.23 26.28 21.60 21.02 23.34 23.34 26.43 21.49 21.73 22.31 22.31 26.70 CLIPT2T 0.905 0.865 0.933 0.937 0.946 0.905 0.865 0.933 0.937 0.941 0.914 0.899 0.923 0.917 0.946 0.916 0.900 0.886 0.885 0.926 CLIPI2I 0.934 0.897 0.965 0.967 0.974 0.934 0.897 0.965 0.967 0.974 0.933 0.916 0.950 0.949 0.974 0.929 0.892 0.908 0.908 0. Table 4: Quantitative results on the ReS (Yang et al. 2022) dataset. Comparing PixelMan with other methods including Self-Guidance (Epstein et al. 2023), DragonDiffusion (Mou et al. 2024b), DiffEditor (Mou et al. 2024a), and the training-based SDv2-Inpainting+AnyDoor (Rombach et al. 2022; AI 2022b; Chen et al. 2024b) baseline. The indicates lower is better, and the means the higher the better. The best performance result is marked in bold and the second best result is annotated with underlines. Our reported latency measures the average wall-clock time over ten runs for generating one image on this dataset in seconds with V100 GPU."
        },
        {
            "title": "Efficiency",
            "content": "SDv2-Inpainting+AnyDoor Self-Guidance DragonDiffusion DiffEditor PixelMan SDv2-Inpainting+AnyDoor Self-Guidance DragonDiffusion DiffEditor PixelMan SDv2-Inpainting+AnyDoor Self-Guidance DragonDiffusion DiffEditor PixelMan SDv2-Inpainting+AnyDoor Self-Guidance DragonDiffusion DiffEditor PixelMan # Steps 50 50 50 50 16 50 16 8 # NFEs 100 100 160 176 64 100 100 160 176 206 32 32 64 58 64 16 16 32 32 Latency (secs) 16 14 30 32 11 16 14 30 32 34 6 6 12 11 11 3 3 6 6 5 Image Quality Assessment MUSIQ 71.19 69.41 74.95 74.94 74.66 71.19 69.41 74.95 74.94 74.72 71.29 73.41 75.21 75.20 74.66 70.92 73.07 74.62 74.62 74.59 TOPIQ 0.621 0.586 0.690 0.691 0.696 0.621 0.586 0.690 0.691 0.688 0.625 0.663 0.697 0.697 0.696 0.627 0.678 0.692 0.692 0.695 LIQE 4.22 3.61 4.72 4.73 4.70 4.22 3.61 4.72 4.73 4.75 4.17 4.16 4.72 4.72 4.70 4.04 4.01 4.46 4.46 4."
        },
        {
            "title": "Semantic\nConsistency",
            "content": "LPIPS 0.052 0.064 0.030 0.032 0.015 0.052 0.064 0.030 0.032 0.015 0.051 0.064 0.033 0.033 0.015 0.051 0.065 0.038 0.038 0.016 PSNR 26.06 24.21 29.68 29.59 35.90 26.06 24.21 29.68 29.59 36.26 26.21 24.00 29.19 29.15 35.90 26.31 23.97 28.57 28.57 35.57 LPIPS 0.159 0.273 0.083 0.083 0.070 0.159 0.273 0.083 0.083 0.073 0.159 0.194 0.104 0.105 0.070 0.162 0.255 0.173 0.173 0.067 PSNR 21.21 17.92 25.38 25.44 27.18 21.21 17.92 25.38 25.44 26.74 21.25 20.95 24.99 25.00 27.18 21.21 20.76 22.68 22.68 27.74 CLIPT2T 0.866 0.817 0.902 0.899 0.898 0.866 0.817 0.902 0.899 0.896 0.856 0.847 0.894 0.889 0.898 0.849 0.851 0.856 0.852 0.900 CLIPI2I 0.907 0.869 0.934 0.933 0.939 0.907 0.869 0.934 0.933 0.940 0.907 0.886 0.917 0.917 0.939 0.902 0.845 0.876 0.876 0. Table 5: Quantitative results on the COCOEE (Yang et al. 2022) dataset. Comparing PixelMan with additional baselines including PAIR Diffusion (Goel et al. 2023) and InfEdit (Xu et al. 2024). The indicates lower is better, and the means the higher the better. The best performance result is marked in bold and the second best result is annotated with underlines. Latency measures the average wall-clock time over ten runs for generating one image on this dataset in seconds with V100 GPU."
        },
        {
            "title": "Method",
            "content": "PAIR Diffusion InfEdit PixelMan PAIR Diffusion InfEdit PixelMan PAIR Diffusion InfEdit PixelMan PAIR Diffusion InfEdit PixelMan # Steps 50 50 16 50"
        },
        {
            "title": "Efficiency",
            "content": "# NFEs 100 230 64 100 230 206 32 70 64 16 28 28 Latency (secs) 32 35 9 32 35 27 12 11 9 7 5 4 Image Quality Assessment MUSIQ 64.76 69.07 69.98 64.76 69.07 70.17 65.08 69.00 69.98 65.12 68.78 69.63 TOPIQ 0.525 0.567 0.605 0.525 0.567 0.605 0.538 0.566 0.605 0.545 0.563 0.602 LIQE 3.51 4.29 4.35 3.51 4.29 4.36 3.54 4.30 4.35 3.45 4.30 4."
        },
        {
            "title": "Semantic\nConsistency",
            "content": "LPIPS 0.088 0.034 0.015 0.088 0.034 0.014 0.088 0.034 0.015 0.090 0.035 0.016 PSNR 23.87 29.04 35.62 23.87 29.04 35.92 23.84 28.97 35.62 23.63 28.80 35.33 LPIPS 0.112 0.077 0.074 0.112 0.077 0.077 0.113 0.071 0.074 0.119 0.070 0.071 PSNR 24.69 25.95 26.43 24.69 25.95 26.28 24.53 26.44 26.43 24.10 26.64 26.70 CLIPT2T 0.836 0.908 0.946 0.836 0.908 0.941 0.838 0.893 0.946 0.832 0.898 0.926 CLIPI2I 0.863 0.968 0.974 0.863 0.968 0.974 0.868 0.967 0.974 0.861 0.967 0. Table 6: Quantitative on the ReS (Yang et al. 2022) dataset. Comparing PixelMan with additional baselines including PAIR Diffusion (Goel et al. 2023) and InfEdit (Xu et al. 2024). The indicates lower is better, and the means the higher the better. The best performance result is marked in bold and the second best result is annotated with underlines. Latency measures the average wall-clock time over ten runs for generating one image on this dataset in seconds with V100 GPU."
        },
        {
            "title": "PAIR Diffusion\nInfEdit\nPixelMan\nPAIR Diffusion\nInfEdit\nPixelMan\nPAIR Diffusion\nInfEdit\nPixelMan\nPAIR Diffusion\nInfEdit\nPixelMan",
            "content": "# Steps 50 50 16 50"
        },
        {
            "title": "Efficiency",
            "content": "# NFEs 100 230 64 100 230 206 32 70 64 16 28 28 Latency (secs) 49 47 11 49 47 34 18 15 11 11 7 5 Image Quality Assessment MUSIQ 72.78 74.56 74.66 72.78 74.56 74.72 72.75 74.67 74.66 72.74 74.56 74.59 TOPIQ 0.667 0.671 0.696 0.667 0.671 0.688 0.673 0.676 0.696 0.679 0.673 0.695 LIQE 4.39 4.72 4.70 4.39 4.72 4.75 4.35 4.74 4.70 4.27 4.73 4."
        },
        {
            "title": "Semantic\nConsistency",
            "content": "LPIPS 0.064 0.029 0.015 0.064 0.029 0.015 0.064 0.030 0.015 0.064 0.030 0.016 PSNR 24.86 29.85 35.90 24.86 29.85 36.26 24.84 29.74 35.90 24.76 29.63 35.57 LPIPS 0.105 0.068 0.070 0.105 0.068 0.073 0.106 0.067 0.070 0.110 0.066 0.067 PSNR 24.66 26.72 27.18 24.66 26.72 26.74 24.58 27.01 27.18 24.33 27.32 27.74 CLIPT2T 0.799 0.875 0.898 0.799 0.875 0.896 0.801 0.870 0.898 0.795 0.871 0.900 CLIPI2I 0.871 0.936 0.939 0.871 0.936 0.940 0.871 0.933 0.939 0.867 0.931 0. (a) (b) (c) (d) (e) (f) Input SDv2-Inpainting +AnyDoor (50 steps, 15s) SDv2-Inpainting +AnyDoor (16 steps, 5s) SelfGuidance (50 steps, 11s) SelfGuidance (16 steps, 4s) DragonDiffusion (50 steps, 23s) DragonDiffusion (16 steps, 9s) DiffEditor (50 steps, 24s) DiffEditor (16 steps, 9s) PixelMan (50 steps, 27s) PixelMan (16 steps, 9s) Figure 6: Additional qualitative comparison on the COCOEE dataset at both 16 and 50 steps. (a) (b) (c) (d) (e) (f) Input SDv2-Inpainting +AnyDoor (50 steps, 15s) SDv2-Inpainting +AnyDoor (16 steps, 5s) SelfGuidance (50 steps, 11s) SelfGuidance (16 steps, 4s) DragonDiffusion (50 steps, 23s) DragonDiffusion (16 steps, 9s) DiffEditor (50 steps, 24s) DiffEditor (16 steps, 9s) PixelMan (50 steps, 27s) PixelMan (16 steps, 9s) Figure 7: Additional qualitative comparison on the COCOEE dataset at both 16 and 50 steps. (a) (b) (c) (d) (e) (f) Input SDv2-Inpainting +AnyDoor (50 steps, 15s) SDv2-Inpainting +AnyDoor (16 steps, 5s) SelfGuidance (50 steps, 11s) SelfGuidance (16 steps, 4s) DragonDiffusion (50 steps, 23s) DragonDiffusion (16 steps, 9s) DiffEditor (50 steps, 24s) DiffEditor (16 steps, 9s) PixelMan (50 steps, 27s) PixelMan (16 steps, 9s) Figure 8: Additional qualitative comparison on the COCOEE dataset at both 16 and 50 steps. (a) (b) (c) (d) (e) (f) Input SDv2-Inpainting +AnyDoor (50 steps, 16s) SDv2-Inpainting +AnyDoor (16 steps, 6s) SelfGuidance (50 steps, 14s) SelfGuidance (16 steps, 6s) DragonDiffusion (50 steps, 30s) DragonDiffusion (16 steps, 12s) DiffEditor (50 steps, 32s) DiffEditor (16 steps, 11s) PixelMan (50 steps, 34s) PixelMan (16 steps, 11s) Figure 9: Additional qualitative comparison on the ReS dataset at both 16 and 50 steps. (a) (b) (c) (d) (e) (f) Input SDv2-Inpainting +AnyDoor (50 steps, 16s) SDv2-Inpainting +AnyDoor (16 steps, 6s) SelfGuidance (50 steps, 14s) SelfGuidance (16 steps, 6s) DragonDiffusion (50 steps, 30s) DragonDiffusion (16 steps, 12s) DiffEditor (50 steps, 32s) DiffEditor (16 steps, 11s) PixelMan (50 steps, 34s) PixelMan (16 steps, 11s) Figure 10: Additional qualitative comparison on the ReS dataset at both 16 and 50 steps. (a) (b) (c) (d) (e) (f) Input SDv2-Inpainting +AnyDoor (8 steps, 3s) SelfGuidance (8 steps, 2s) DragonDiffusion (8 steps, 5s) DiffEditor (8 steps, 5s) PixelMan (8 steps, 4s) Figure 11: Additional qualitative comparison on the COCOEE dataset at 8 steps. (a) (b) (c) (d) (e) (f) Input SDv2-Inpainting +AnyDoor (8 steps, 3s) SelfGuidance (8 steps, 3s) DragonDiffusion (8 steps, 6s) DiffEditor (8 steps, 6s) PixelMan (8 steps, 5s) Figure 12: Additional qualitative comparison on the ReS dataset at 8 steps. (a) Object Enlarging (b) Object Enlarging (c) Object Shrinking (With Moving) (d) Object Enlarging (With Moving) (e) Object Pasting (With Shrinking) (f) Object Pasting (With Enlarging) Reference Image"
        },
        {
            "title": "Input",
            "content": "DragonDiffusion (160 NFEs) (50 steps, 23s) DiffEditor (176 NFEs) (50 steps, 24s) PixelMan (64 NFEs) (16 steps, 9s) Figure 13: Qualitative examples on other consistent object editing tasks including object resizing, and object pasting. (a) (b) (c) (d) (e) (f) Input PAIR Diffusion (50 steps, 32s) InfEdit (50 steps, 35s) PixelMan (50 steps, 27s) PAIR Diffusion (16 steps, 12s) InfEdit (16 steps, 1s) PixelMan (16 steps, 9s) PAIR Diffusion (8 steps, 7s) InfEdit (8 steps, 5s) PixelMan (8 steps, 4s) Figure 14: Additional qualitative comparisons to PAIR Diffusion (Goel et al. 2023) and InfEdit (Xu et al. 2024) on the COCOEE dataset at 50, 16 and 8 steps. (a) (b) (c) (d) (e) (f) Input PAIR Diffusion (50 steps, 49s) InfEdit (50 steps, 47s) PixelMan (50 steps, 34s) PAIR Diffusion (16 steps, 18s) InfEdit (16 steps, 15s) PixelMan (16 steps, 11s) PAIR Diffusion (8 steps, 11s) InfEdit (8 steps, 7s) PixelMan (8 steps, 5s) Figure 15: Additional qualitative comparisons to PAIR Diffusion (Goel et al. 2023) and InfEdit (Xu et al. 2024) on the ReS dataset at 50, 16 and 8 steps."
        }
    ],
    "affiliations": [
        "Huawei Kirin Solution, China",
        "Huawei Technologies Canada",
        "University of Alberta"
    ]
}