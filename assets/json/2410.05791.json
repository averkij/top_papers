{
    "paper_title": "FürElise: Capturing and Physically Synthesizing Hand Motions of Piano Performance",
    "authors": [
        "Ruocheng Wang",
        "Pei Xu",
        "Haochen Shi",
        "Elizabeth Schumann",
        "C. Karen Liu"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Piano playing requires agile, precise, and coordinated hand control that stretches the limits of dexterity. Hand motion models with the sophistication to accurately recreate piano playing have a wide range of applications in character animation, embodied AI, biomechanics, and VR/AR. In this paper, we construct a first-of-its-kind large-scale dataset that contains approximately 10 hours of 3D hand motion and audio from 15 elite-level pianists playing 153 pieces of classical music. To capture natural performances, we designed a markerless setup in which motions are reconstructed from multi-view videos using state-of-the-art pose estimation models. The motion data is further refined via inverse kinematics using the high-resolution MIDI key-pressing data obtained from sensors in a specialized Yamaha Disklavier piano. Leveraging the collected dataset, we developed a pipeline that can synthesize physically-plausible hand motions for musical scores outside of the dataset. Our approach employs a combination of imitation learning and reinforcement learning to obtain policies for physics-based bimanual control involving the interaction between hands and piano keys. To solve the sampling efficiency problem with the large motion dataset, we use a diffusion model to generate natural reference motions, which provide high-level trajectory and fingering (finger order and placement) information. However, the generated reference motion alone does not provide sufficient accuracy for piano performance modeling. We then further augmented the data by using musical similarity to retrieve similar motions from the captured dataset to boost the precision of the RL policy. With the proposed method, our model generates natural, dexterous motions that generalize to music from outside the training dataset."
        },
        {
            "title": "Start",
            "content": "4 2 0 2 8 ] . [ 1 1 9 7 5 0 . 0 1 4 2 : r FürElise: Capturing and Physically Synthesizing Hand Motions of Piano Performance RUOCHENG WANG , Stanford University, USA PEI XU, Stanford University, USA HAOCHEN SHI, Stanford University, USA ELIZABETH SCHUMANN, Stanford University, USA C. KAREN LIU, Stanford University, USA Piano playing requires agile, precise, and coordinated hand control that stretches the limits of dexterity. Hand motion models with the sophistication to accurately recreate piano playing have wide range of applications in character animation, embodied AI, biomechanics, and VR/AR. In this paper, we construct first-of-its-kind large-scale dataset that contains approximately 10 hours of 3D hand motion and audio from 15 elite-level pianists playing 153 pieces of classical music. To capture natural performances, we designed markerless setup in which motions are reconstructed from multiview videos using state-of-the-art pose estimation models. The motion data is further refined via inverse kinematics using the high-resolution MIDI key-pressing data obtained from sensors in specialized Yamaha Disklavier piano. Leveraging the collected dataset, we developed pipeline that can synthesize physically-plausible hand motions for musical scores outside of the dataset. Our approach employs combination of imitation learning and reinforcement learning to obtain policies for physics-based bimanual control involving the interaction between hands and piano keys. To solve the sampling efficiency problem with the large motion dataset, we use diffusion model to generate natural reference motions, which provide high-level trajectory and fingering (finger order and placement) information. However, the generated reference motion alone does not provide sufficient accuracy for piano performance modeling. We then further augmented the data by using musical similarity to retrieve similar motions from the captured dataset to boost the precision of the RL policy. With the proposed method, our model generates natural, dexterous motions that generalize to music from outside the training dataset. Fig. 1. Our paper (a) collects the first large-scale 3D hand motion dataset of piano playing, accompanied by synchronized audio and key pressing events; (b) proposes method that can control physically simulated hand to play novel pieces unheard from the training set."
        },
        {
            "title": "INTRODUCTION",
            "content": "CCS Concepts: Computing methodologies Animation; Physical simulation; Reinforcement learning. Additional Key Words and Phrases: Character animation, hand animation, physics-based control, dexterous control, motion capture dataset ACM Reference Format: Ruocheng Wang , Pei Xu, Haochen Shi, Elizabeth Schumann, and C. Karen Liu. 2024. FürElise: Capturing and Physically Synthesizing Hand Motions of Piano Performance. In SIGGRAPH Asia 2024 Conference Papers (SA Conference Papers 24), December 36, 2024, Tokyo, Japan. ACM, New York, NY, USA, 16 pages. https://doi.org/10.1145/3680528.3687703 These two authors contributed equally to this work. Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. Copyrights for components of this work owned by others than the author(s) must be honored. Abstracting with credit is permitted. To copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior specific permission and/or fee. Request permissions from permissions@acm.org. SA Conference Papers 24, December 36, 2024, Tokyo, Japan 2024 Copyright held by the owner/author(s). Publication rights licensed to ACM. ACM ISBN 979-8-4007-1131-2/24/12. . . $15.00 https://doi.org/10.1145/3680528."
        },
        {
            "title": "1\nPhysically synthesizing human motion has a wide range of appli-\ncations in character animation, embodied AI, AR/VR, robotics, and\nbiomechanics. Researchers have made great strides in simulating\nfunctional and realistic human movements which enable digital\nagents to physically navigate and interact with environments while\nmaintaining balance. As the application domain expands, the next\nfrontier in human motion synthesis is to create digital agents that\nnot only achieve motion tasks, but also exhibit elite-level athletic\ntechniques and musical precision, comparable to the peak perfor-\nmance of human athletes and musicians. In this work, we take the\nfirst step toward synthesis of human peak performance through the\nlens of the movement of elite pianists.",
            "content": "Piano playing is demanding motor skill that requires impeccable precision in finger control to press the correct keys at the correct time, agile coordination to press multiple keys simultaneously, and remarkable dexterity to fluidly play long sequences while anticipating upcoming notes. Previous works on simulating piano playing motions either rely on human-annotated fingering information (which finger to press which key) [Zakka et al. 2023] or are limited to scenarios involving easier compositions [Xu et al. 2022; Zhu et al. 2013]. We believe that better model requires deeper understanding of how humans play the piano. However, there is significant shortfall in large-scale datasets that adequately capture the diversity and complexity of piano performances. To address this gap, we design and build comprehensive, nonintrusive data capture pipeline to record the 3D hand motions of pianists during their natural performances. This pipeline employs markerless setup, where multi-view videos are processed using 1 SA Conference Papers 24, December 36, 2024, Tokyo, Japan Wang, R. et al state-of-the-art pose estimation model [Pavlakos et al. 2024] to reconstruct 3D motions. These reconstructions are further refined through inverse kinematics, utilizing music information obtained from sensors embedded in specialized piano [Yamaha 2024]. Using this pipeline, we have collected the first large-scale dataset of piano motions, FürElise, capturing approximately 10 hours of 3D hand motions from 15 professional or conservatory pianists performing 153 pieces of classical music across various genres. This dataset encompasses broad spectrum of piano skills demonstrated by elite pianists, and includes synchronized audio, providing valuable resource for character animation and dexterous control. It also enables various music-related applications such as keyboard ergonomics, music pedagogy, and pianist injury prevention. Leveraging FürElise, we take step towards synthesizing physically simulated motions of piano playing for novel pieces of music unheard from the dataset. Specifically, given piece of sheet music, our approach first uses diffusion model trained on the collected dataset to generate an initial reference motion that provides highlevel trajectory guidance and fingering information. However, this initial reference motion often includes numerous incorrect or missing keys, making it unsuitable for training an RL policy that would ensure musically correct physical interactions between hand fingers and the piano keys. We propose to enhance this process by combining music-based motion retrieval method with the diffusion model to create an ensemble of reference motions, thus balancing the visual performance and physical plausibility for accurate key press. Our experiments show that, given piece of music unseen in the training dataset, our method can synthesize natural piano motions. The policy can handle chords, fast wrist motions, and other complex piano skills, playing melodious pieces given only the sheet music. Ablations have shown that the diffusion model, music-based retrieval and reinforcement learning all contribute to the performance of the final model. In summary, this paper makes two major contributions toward physics-based synthesis of elite-level piano performance, as illustrated in Figure 1: We present the first large-scale dataset of 3D hand motions in piano performance with synchronized audio. We develop model that combines diffusion models, motion retrieval, and reinforcement learning to synthesize natural dexterous motions playing diverse set of piano music pieces. Our model was evaluated through extensive experiments and ablations."
        },
        {
            "title": "2 RELATED WORK\n2.1 Music2Motion\nThe problem of generating motions following music has been ex-\ntensively studied in recent years. Alexanderson et al. [2023]; Li\net al. [2021]; Tseng et al. [2023] tackle the problems of generat-\ning whole-body dancing motions from input music using diffusion\nmodels. Another line of research trains neural networks to gen-\nerate upper-body motions of musicians from the audio of various\ninstruments [Chen et al. 2023a; Kao and Su 2020; Li et al. 2018; Liu\net al. 2020; Shlizerman et al. 2018]. These works typically utilize\npose estimation models to estimate 3D joint locations only from",
            "content": "monocular videos, resulting in poor motion quality due to depth ambiguity. Moreover, these works focus on learning to generate visually plausible kinematics motions, overlooking their physical plausibility. In contrast, our work collects large-scale high-quality dataset of piano performance motion. We propose pipeline to train control policies that can play the novel piano pieces in physically simulated environment. Apart from data-driven approaches, some early works design heuristics to animate hands for music performance. Zhu et al. [2013] generates piano playing motion by using iterative optimizations to solve for hand trajectories that hit target keys and satisfy predefined constraints. ElKoura and Singh [2003] considers generating left-hand motions for playing the guitar by retrieving and blending motions from motion capture dataset. In these works, key challenge is to determine the fingering information, which specifies which finger should press each note. Previous works rely on heuristic cost functions or additional annotations to decide fingering, which can only handle simple or manually pre-processed pieces. Our work uses generative model trained on large-scale dataset to provide fingering information automatically for reinforcement learning policies to learn playing unseen pieces."
        },
        {
            "title": "2.2 Physics-Based Dexterous Control\nStudying the control strategy for physically simulated dexterous\nhands has wide applications in computer graphics, robotics, and\nbiomechanics. Traditional approaches usually rely on trajectory\noptimization and/or human-designed heuristic rules to perform\ncontrol[Chen et al. 2023b; Liu 2008, 2009; Mordatch et al. 2012;\nWang et al. 2013; Ye and Liu 2012]. Most recent works on physics-\nbased dexterous control only focus on single-hand scenarios and\ndo not have high precision requirements[Andrychowicz et al. 2020;\nLiu 2009; Xie et al. 2023; Yang et al. 2022; Zhang et al. 2021; Zhao\net al. 2013]. In this study, we focus on piano playing, a task that\nrequires simultaneous bimanual control with exceptional temporal\nand spatial precision.",
            "content": "Piano playing is common but intricate physical activity in daily life. Introducing physics can help generate physically feasible motions for piano playing. Algorithms are proposed to train policies to play piano in simulations using anthropomorphic robot hands [Xu et al. 2022; Zakka et al. 2023] via reinforcement learning. Due to the complexity of the task, Xu et al. [2022] only considers one hand playing on simplified piano. Zakka et al. [2023] leverages human annotated fingering information (which finger should press which key) to facilitate policy training. Our proposed pipeline, once trained on our large-scale dataset, can play unseen pieces without any additional annotation. Our approach follows previous work leveraging reinforcement learning to synthesize motions under the framework of imitation learning [Merel et al. 2017; Peng et al. 2022, 2021; Xu and Karamouzas 2021; Xu et al. 2023]. Though impressive results are achieved in generating realistic motions by imitation learning, it is still challenging problem to perform learning efficiently from very large set of reference motions. To better utilize our collected large set of pianoplaying motions, we address the problem by developing hybrid approach to generate and retrieve motions for the policy to synthesize. 2 FürElise: Capturing and Physically Synthesizing Hand Motions of Piano Performance SA Conference Papers 24, December 36, 2024, Tokyo, Japan Fig. 2. Overview of our pipeline to reconstruct motion data from multi-view videos. We (a) shoot 4K videos from 5 different views at 59.94 FPS using RGB camera; (b) detect 2D keypoints of the hands from each view; (c) triangulate the 2D keypoints into 3D hand skeletons with calibrated camera intrinsics and extrinsics; (d) fit the skeleton onto MANO hand meshes [Romero et al. 2017]; and (e) run IK with ground-truth MIDI as end effector goals to refine the finger placements for correct key pressing. first elaborate on the data capture and processing pipeline and provide an analysis of the dataset."
        },
        {
            "title": "3.1 Data Capture\nWe aim to collect a large-scale dataset of piano playing motion\nperformed by professional and conservatory-level pianists with\nminimal intrusion.",
            "content": "Device Setup. We record the data in typical piano studio familiar to the performers, as shown in Figure 3. To minimize the influence of capture device, we design markerless setup using multiview RGB cameras. Five calibrated GoPro cameras are placed around grand piano to record synchronized videos and audio with 59.94 FPS. All the videos have resolution of 3840 2160. The grand piano is Yamaha Disklavier DS7X ENPRO, which has built-in recorder to record the key and pedal pressing events during the performance with high precision in MIDI format, from which the original audio with high fidelity can be reproduced. Vision-based Motion Reconstruction. Figure 2 summarizes the motion reconstruction process. We first use the state-of-the-art pose estimation model HaMeR [Pavlakos et al. 2024] to predict the hand pose 𝑲2D R𝑁 52212, which are the 2D locations of 21 joints on each hand from all 5 camera views for sequence of 𝑁 frames. While HaMeR can generate 3D meshes of MANO hands [Romero et al. 2017] in the camera space, we found that the predicted depths are not usable due to severe inaccuracy. As such, we only leverage the projected 2D keypoints from HaMeR and compute 3D locations of each joint 𝑲3𝐷 R𝑁 2213 via triangulation. RANSAC is used to filter out occluded keypoints, while Butterworth filter is applied to every joint to enhance temporal smoothness, since HaMeR only considers one frame at time. Next, we fit MANO hand parameters Θ = {𝜃, 𝛽, 𝑡 } to obtain 3D hand meshes for every frame, where 𝜃 R𝑁 2163, 𝛽 R245, 𝑡 𝑡 𝑁 23 are the joint rotations, shape parameters and global translations of the two hands. The shape parameters are computed with extra hand calibration videos. Other parameters are optimized by minimizing the mean-squared error between the triangulated joint locations and MANO hand joint locations. Fig. 3. Data capture setup. Five GoPro cameras are placed around the piano to provide multi-view recordings of elite pianists performances."
        },
        {
            "title": "3 DATASET\nTo study hand motions during piano playing, we collect a large-\nscale dataset, FürElise, with approximately 10 hours of 3D hand\nmotions paired with synchronized audio. In this section, we will",
            "content": "MIDI-based Motion Refinement. Vision-based motion reconstruction achieves reasonable results, but visible artifacts such as incorrect key-pressing or missing keys are quite common in the reconstructed motion. To improve the quality, we utilize the key-press 3 SA Conference Papers 24, December 36, 2024, Tokyo, Japan Wang, R. et al Fig. 4. Examples of some piano skills in our dataset, including scales, octaves, and arpeggio. The trajectory of each fingertip is visualized. The green keys show the pressed keys through the trajectory. information stored in the accompanying MIDI file. For each note played during the session, the MIDI file records the precise moments each key is fully pressed down and released. By assuming that the fingertip remains in contact with the key throughout the duration of the note, we can infer the positions of fingertips based on the states of the keys. Therefore, we apply inverse kinematics to ensure two key properties of the reconstructed motion: a) when key is being pressed according to the MIDI file, at least one fingertip must be on the top surface of that key and have depth below preset threshold to trigger sound; b) when key is not pressed according to the MIDI file, no fingertip should press the key deep enough to trigger the note; To prevent large modifications by IK, we only optimize the local joint rotations and the wrist orientation of each hand. We also limit the maximum change of fingertips to 1cm. smoothness term is added to prevent abrupt changes between frames. Further details can be found in the appendix."
        },
        {
            "title": "3.2 Dataset Analysis",
            "content": "Data statistics. We collect and reconstruct total of 10 hours of 3D hand motions paired with synchronized MIDI. 8 male and 7 female elite pianists contribute total of 153 classical compositions in various genres. Quality Evaluation. Following Zakka et al. [2023], we use precision, recall and F1 to quantitatively evaluate the quality of our reconstructed motions according to the recorded MIDI: Precisioni = 𝑇 𝑃𝑖 𝐹 𝑃𝑖 + 𝑇 𝑃𝑖 Recall = 𝑇 𝑃𝑖 𝑇 𝑃𝑖 + 𝐹 𝑁𝑖 F1i = 2Precisioni Recalli Precisioni + Recalli , (1) 4 where TPi computes the number of keys that are correctly pressed, FPi computes the number of keys that are wrongly pressed, and FNi computes the number of keys that the motion failed to press. We do this for every frame 𝑖 and average over all the frames in the dataset. To extract the pressed keys from reconstructed motions, similar to the IK procedure mentioned earlier, any fingertip horizontally over key and below preset threshold is treated as pressing the key. Using this evaluation protocol, we got precision of 88.55, recall of 92.53, and an F1 of 86.49 on the whole dataset. We also visualize our reconstructed motion and include the audio of the extracted MIDI in the supplementary video. Qualitative Examples. To demonstrate the diversity of motions in our dataset, we show examples of various primitive piano playing skills [Neuhaus 2008] in Figure 4."
        },
        {
            "title": "4 PLAY PIANO WITH PHYSICALLY SIMULATED HANDS\nLeveraging the collected dataset, we aim to train a policy that con-\ntrols two physically simulated hands in concert to play a given piece\nof music. Thus, the input to our method is a musical score repre-\nsented as a list of notes {𝑂𝑖 = (𝑡 start\n, 𝑡 end\n, 𝑝𝑖 ) | 𝑖 ∈ {1, 2, . . . , 𝑛}},\n𝑖\nwhere 𝑡 start\n, 𝑡 end ∈ R is the start and end time of the note, and\n𝑝𝑖 ∈ {1, · · · , 88} is the pitch, which can also be mapped to one of\nthe 88 piano keys. Our method finally outputs a policy that controls\ntwo hands interacting with a piano keyboard physically. A digital\nsound is generated by matching the pitch of the keys being pressed\nby the physically simulated hands.",
            "content": "𝑖 𝑖 We propose method that combines data-driven and physicsbased approaches to achieve the goal (Figure 5). diffusion motion model [Ho et al. 2020] is trained on the FürElise dataset to generate kinematics motions for the given piece. Despite the strong abilities FürElise: Capturing and Physically Synthesizing Hand Motions of Piano Performance SA Conference Papers 24, December 36, 2024, Tokyo, Japan Fig. 5. Overview of our method to physically simulate piano performance from given sheet music. We use MIDI to retrieve motion data from the collected motion dataset and as input to diffusion model for generating piano performance motions. These two sets of motions are combined into reference motion ensemble. Utilizing the reference motions, we then employ two discriminator ensembles and three critics, which consider imitation and goal rewards, respectively, to train control policy via reinforcement learning. of diffusion models to generate visually plausible motions when trained on large datasets, they often produce physically implausible artifacts, such as penetration, floating, and inconsistent interactions with objects [Liu and Yi 2024; Yuan et al. 2023]. We observe similar issues in our settings where the generated motions exhibit seemingly plausible wrist trajectories and hand poses, but frequently exhibit incorrect contact with the keyboard, such as pressing the wrong keys or failing to press the right keys. Directly applying reinforcement learning to imitate these flawed motions would lead to unsuccessful policies. As such, we propose music-based motion retrieval method to utilize the high-quality motions in the FürElise dataset. Combining the diffusion-generated and retrieved motions together, we form an ensemble of natural or precise reference trajectories to train an RL policy that minimizes the goal-based reward and imitates the reference motions [Xu et al. 2023]."
        },
        {
            "title": "4.1 Diffusion Model\nThe goal of this module is to generate a kinematic hand trajectory\ngiven a piece of sheet music. We leverage a diffusion model, which\nis proven to be very effective in modeling distributions of human\nmotions [Alexanderson et al. 2023; Li et al. 2023; Tevet et al. 2023;\nTseng et al. 2023] to perform kinematic motion generation.",
            "content": "Overview. The core of the diffusion model [Ho et al. 2020] trains denoiser network on dataset examples corrupted by different levels of Gaussian noises with the objective function reconstructing the original clean examples. The loss function for conditional diffusion model is as follows: = E𝒙,𝑡 (cid:2)𝑥 ˆ𝒙𝜃 (𝒙𝑡 , 𝑡, 𝒄) 2(cid:3) , (2) where 𝒙 are the clean examples, 𝑥𝑡 is the corrupted examples on noise level 𝑡, 𝒄 is the condition vector. After training, conditional samples can be drawn by running the denoiser network iteratively on trajectory of Gaussian noises. Motion Representation. Since the task requires high precision for the location of fingertips, we represent the dual-hand motion as trajectory of 2 21 joint locations 𝑲 R𝑀 221 defined in MANO hands [Romero et al. 2017], similar to [Liu and Yi 2024]. 𝑀 is the number of frames considered for the diffusion model. Here we use 𝑀 = 120 which corresponds to window of 120 frames. To ensure consistent bone lengths during generation, we fit MANO hand models to the generated trajectory with fixed shape parameters to achieve the final predicted joint locations. Condition Representation. To compute the condition vector 𝒄𝑇 , we first quantize input sheet music {𝑂𝑖 = (𝑡 start , 𝑝𝑖 ) 𝑖 {1, 2, . . . , 𝑛}} into binary matrix 𝑪 {0, 1}𝑁 88, where 𝑁 is the total number of frames in the input music and 𝑛 is the total number of notes. Then, we divide each non-zero entry in the matrix by the duration of the corresponding key being pressed: , 𝑡 end 𝑖 𝑖 𝑪𝑖,𝑝 = 𝑡𝑖𝑝 𝑡 start 𝑖𝑝 + 1 . (3) In this way, the key information, as well as the duration information, are encoded into the condition vector c𝑇 R88. Model Architecture. We leverage transformer-based architecture proposed in 𝐸𝐷𝐺𝐸 [Tseng et al. 2023] to train our model. In accordance with our dataset, motions and music are quantized into 59.94FPS. The diffusion model, therefore, generates 2 seconds by outputting the results of 120 frames at time. Long-form Generation. Although we train the diffusion model on window of 2 seconds, we can generate arbitrary long sequences from conditions by denoising batch of sequences while enforcing the adjacent sequences in the batch share an overlapping path, following [Tseng et al. 2023]."
        },
        {
            "title": "4.2 Music-Based Motion Retrieval\nTo complement the diffusion-generated motions, we retrieve ad-\nditional reference motions from the whole dataset for reinforce-\nment learning policy to perform imitation learning. To do so, first,\nwe quantize all notes in the dataset {𝑂𝑖 = (𝑡 start\n, 𝑝𝑖 ) | 𝑖 ∈\n{1, 2, . . . , 𝑛}} into a binary matrix 𝑴 ∈ {0, 1}𝑁 ×88 that align with\nthe frames of hand motions. We perform the same quantization for\nthe input piece to obtain binary a matrix 𝑴 ′ ∈ {0, 1}𝑁 ′ ×88. Next, we\ncompute a sliding window of length 30 and stride 1 individually over\n𝑴 and 𝑴 ′ to obtain 𝑾 ∈ {0, 1}𝑁𝑤 ×30×88, 𝑾 ′ ∈ {0, 1}𝑁 ′\n𝑤 ×30×88. 𝑁𝑤\nand 𝑁 ′\n𝑤 are the numbers of windows for the dataset and the input\npiece. We then compute matching from windows in the target piece",
            "content": ", 𝑡 end 𝑖 𝑖 SA Conference Papers 24, December 36, 2024, Tokyo, Japan Wang, R. et al to those of the dataset by minimizing their L2 distances: 𝑾𝑖 𝑾 𝑗 {1, 2, . . . , 𝑁 𝒄 𝑗 = arg 𝑗 2 𝑤 } min 𝑖 {1,2,...,𝑁𝑤 } (4) This produces 𝑁 𝑤 windows of musical pieces from the dataset We then retrieve the corresponding hand motions, merge the overlapping windows, and generate list of reference motions These motions are combined with the diffusion-generated motions to train the policy more effectively."
        },
        {
            "title": "4.3 Policy Training for Physics-based Control\nWe set up our simulation environment using IsaacGym [Makoviy-\nchuk et al. 2021]. While the simulation runs at 240 FPS, the control\nruns at 60 FPS which is consistent with our diffusion model. Our\nphysics-based hand models are modified from [Kumar and Todorov\n2015] with geometry optimized according to the mocap subjects.\nEach hand has 17 links with 27 degrees of freedom (DoFs) driven\nby PD servos, where the wrist has 6 DoFs, the MCP joints have 2\nDoFs except that the thumb MCP has 3, and all the PIP and DIP\njoints have 1 DoF. This leads to an action space of a𝑡 ∈ R2×27 for\ntwo hands. Similar to our diffusion model, we take the key-based\nbinary vector as the goal representation for key pressing. To balance\nthe goal vector size and the observation horizon, we utilize a com-\npressed representation by merging the same key-pressing goal in\nconsecutive frames into one. We take the future five merged goals\nas the goal state with an additional timer variable that indicates the\ntime (in terms of the number of simulation frames) left for the asso-\nciated key-pressing goal. Thus the final goal state vector is of shape\ng𝑡 ∈ R5× (88+1) . To perform control, we take a 2-frame historical\nobservation composed of the position, orientation, and linear and\nangular velocities of all the links of two hands. This results in a pose\nstate vector s𝑡 ∈ R2×2×208 for two hands.",
            "content": "Due to the limited performance of the motion generated by the diffusion model, we do not directly perform motion tracking during the control policy training. Rather, we take the generated and retrieved motions as the reference simultaneously, and perform imitation learning using reinforcement learning with GAN-like architecture [Xu and Karamouzas 2021] for motion synthesis. Following the previous literature [Xu et al. 2023], to utilize the reference motions more effectively, we decouple the motions of two hands and employ two discriminators at the same time for motion imitation of the left and right hand respectively. By doing so, the pose of one hand does not rely on that of the other hand anymore. We, thereby, facilitate the single-hand motion imitation by performing learning independently rather than using dual-hand state space. The imitation-related reward is computed by 𝑟 imit,ℎ 𝑡 ℎ ℎ 𝑡 , 𝑡 +1) = (s 1 𝑁 𝑁 𝑛= Clip (cid:16) 𝐷ℎ ℎ ℎ 𝑡 , 𝑛 (s 𝑡 +1), 1, 1(cid:17) , (5) where ℎ {𝐿, 𝑅} indicates the imitation of the left and right hand respectively, sℎ 𝑛 is the discriminator trained using hinge loss [Lim and Ye 2017]. 𝑡 is the pose state of the single hand ℎ, and 𝐷ℎ To encourage expected key-pressing behaviors, besides imitation, we also employ goal-based reward function to evaluate the policys key-pressing performance at each time step 𝑡. The reward definition is different depending on the pressing condition of each key. 6 We assume that key 𝑘 is pressed to generate sound if the pressed distance 𝑝𝑘 is greater than 90% of that keys maximal travel distance 𝑑𝑘 , which is defined using the allowed rotation range of that key. For each target key 𝑘 that needs to be pressed, we have the reward term to encourage the correct key-pressing behavior: 𝑟 + 𝑡,𝑘 = (cid:40)1 exp(p𝑖 p𝑘 + 0.01𝑝𝑘 /𝑑𝑘 ) if 𝑝𝑘 /𝑑𝑘 > 0.9 otherwise, (6) where p𝑖 is the global position of the target fingertip 𝑖, and p𝑘 is the target position of the key. To determine the target fingertip, We extract fingering information based on the nearest finger to that key in the diffusion-generated motion. The target position of the key is obtained using the surface center of key horizontally and the 85% position along the keys length axis vertically. For each non-target key 𝜅, 𝑟 𝑡,𝜅 measures the errors of key pressing and is employed to penalize incorrect key-pressing behaviors: 𝑟 𝑡,𝜅 = (cid:40)𝑝𝜅 /0.9𝑑𝜅 0 if key 𝜅 is touched and 𝑝𝜅 /𝑑𝜅 > 0.1 otherwise. (7) To emulate physical piano generating clear sound, we perform penalization even if the key is assumed not to trigger any sound virtually (i.e. 𝑝𝜅 /𝑑𝜅 < 0.9) but ignore trivial touch (i.e. 𝑝𝜅 /𝑑𝜅 < 0.1). However, in difficult scenarios, key touching cannot be completely avoided. To prevent the policy from achieving lower error of 𝑟 𝑡,𝑘 by not touching any key, an additional reward term is introduced to encourage correct key pressing behaviors even if some non-target keys are touched. The overall goal-driven reward is defined as 𝑡,𝑘 0.15 𝑟 + 𝑟𝑡 = (cid:214) 𝑟 𝑡,𝜅 + 0.5𝑟correct 0.05𝑟energy, 𝑘 𝜅 (8) where 𝑟correct = 1 if all target keys are pressed correctly or 0 otherwise, and 𝑟energy is term measuring the energy consumption based on the average linear velocity of fingers and wrists between two frames: 𝑟energy = exp (cid:169) (cid:173) (cid:171) (cid:32) 0.75 ℎ {𝐿,𝑅 } ℎ 𝑤 + 0.1 ℎ 𝑖 𝑖 (cid:33)2 , (9) (cid:170) (cid:174) (cid:172) where vℎ 𝑤 is the velocity of one hands wrist in the global space, vℎ 𝑖 is the average velocity of each fingertip in the local system defined by its corresponding wrist joint. The policy is trained using multi-objective framework [Xu et al. 2023] to optimize max E𝑡 (cid:34) 𝑖 𝑤𝑖 𝐴𝑡,𝑖 𝜋 (a𝑡 g𝑡 , s𝑡 ) (cid:35) , (10) where 𝐴𝑡,𝑖 is the standardized advantage that is estimated according to the achieved reward of each objective 𝑖, and 𝑤𝑖 is an associated weight. In our case, we have three objectives (two imitation objectives of left and right hand respectively, and one goal-driven objective). To encourage the policy to perform expected key-pressing behaviors, the associated weights are 0.9 for the goal-driven objective and 0.05 for each imitation objective. Please refer to the supplementary materials for the hyperparameters. FürElise: Capturing and Physically Synthesizing Hand Motions of Piano Performance SA Conference Papers 24, December 36, 2024, Tokyo, Japan"
        },
        {
            "title": "5.1 Setup",
            "content": "Data. We use 14 sheets of music to test our proposed pipeline. Although most recorded compositions in our dataset are classical, we include wider range of genres including popular music, and jazz unseen during training. Because the chosen music pieces are very long with repetition, we select clip of music from each piece and use it to train our model. The lengths of the clips are in the range from 14.4 to 28.94 seconds and 20.72 seconds on average. We do not modify the speed of the original music. Metrics. Similar to our data quality evaluation, we record the key-pressing states of model predictions and compare them with the input sheet music. Precision, recall, and F1 scores are computed for each frame and averaged over the whole piece. For diffusiongenerated motions, we use the same heuristics used in data quality evaluation to extract the pressed keys: when fingertip is below preset depth and horizontally over key, we treat the key as pressed. For physics-based policy, we directly query the key-pressing states from the physical simulator. Implementation Details. For diffusion models, we train with window of 120 frames (2 seconds). The training takes around 1 day on 2 NVIDIA A5000 GPUs. We train single diffusion model for all the testing compositions. Policy trained with reinforcement learning takes around 1-3 days depending on the difficulty of studied music pieces on single A5000 GPU and consumes about 2 108 to 4 108 training samples."
        },
        {
            "title": "5.3 Full Pipeline\nQuantitative results of our full pipeline are summarized in Figure 6:\nthe policy outperforms the diffusion model by a large margin. As\nshown in Figure 7, the policy can handle large wrist motions (Fig 7f),\nchords (pressing multiple keys at the same time, Fig 7abc), double\nnotes (pressing different pairs of notes sequentially, Fig 7d), as well as\narpeggios (pressing individual notes of a chord in sequence, Fig 7e).\nDespite the average F1 scores being as high as more than 0.8 for\nall the tested songs, the policy still could perform unexpected key",
            "content": "Fig. 6. F1 scores of the diffusion model and our policy on the 14 test pieces. RL policies have significant improvement over diffusion-generated motions across all 14 pieces. pressing, though lasting for very short duration. This sometimes leads to negative impact on humans auditory perception more than what F1 scores can reflect."
        },
        {
            "title": "5.4 Ablations\nTo understand the effect of using an ensemble of motions generated\nby the diffusion model and those retrieved from the dataset as the\nreference for the control policy to learn, we design the following\nablation studies tested on four music pieces:",
            "content": "RL+Retr. The policy is trained with only the reference motion retrieved from the dataset. RL+Diff. The policy is trained with only the reference motion generated by the diffusion model. RL Only. The policy is trained only using the goal-driven reward without motion imitation. RL+Whole. The policy is trained only using the whole motion dataset as the reference for imitation without motions generated by the diffusion model. Results. The performance of each model is listed in Table 1. The training curve is shown in Figure 9. The full model outperforms the ablative models by large margin in all the tested cases. We show qualitative comparisons visually of the studied ablative models in Figure 10. As we can see, the RL only case performs the worst and behaves in manner not human-like, which highlights the necessity of using motion imitation to ensure the motion naturalness and to help better key-pressing task execution. When the policies are trained 7 SA Conference Papers 24, December 36, 2024, Tokyo, Japan Wang, R. et al Fig. 7. Results of our model accompanied with generated diffusion motions. The trajectory of each fingertip is visualized. The simulated hands can correctly press all the target keys while tying to follow the diffusion-generated motion. The policy can handle chords (abc), double notes (d), and large wrist movements (ef) naturally and accurately. Fig. 8. Comparison between motions generated by the diffusion model and our full model with reinforcement learning. Our full model fixes the imprecise key-pressing issue of the diffusion model. In the left demo, by imitating retrieved motions, the control policy learns to use the ring finger to press two keys at the same. This pose is not provided by the diffusiongenerated motions. Fig. 9. Learning performance of our full model and the ablative models. In all the tested cases, our model shows better performance compared to the ablative models. 8 FürElise: Capturing and Physically Synthesizing Hand Motions of Piano Performance SA Conference Papers 24, December 36, 2024, Tokyo, Japan Fig. 10. Comparisons between our full model and the ablative models. Without diffusion guidance, the ablative models RL, RL+Retr, and RL+Whole have excessive or unnatural movements because no motions corresponding to the given piece are provided during training. Without retrieved motions from the dataset (RL+Diff ), the model tends to overfit the imprecise motions generated by the diffusion models, resulting in lower accuracy. Table 1. We study 4 ablations of our method on 4 different pieces. We show that the F1 score of our method is significantly higher than all the variants. Ours RL+Retr RL+Diff RL+Whole RL For Elise Rondo Alla Turca #3 Clementi Op.36 Sleep Away 98.15 94.65 96.21 83.75 78.17 81.94 95.00 53.33 85.77 78.73 94.17 64.13 80.37 88.51 79.10 49.38 73.20 34.36 75.28 49. without diffusion-generated motions (RL+Retr and RL+Whole), they yield unnatural hand poses due to the lack of fingering information. The policies also tend to have redundant motions during playing in this case because during training they could try to imitate some unrelated motions that may not strictly apply to the input music piece. When the model is only trained with diffusion-generated motions (RL+Diff ), the policy tends to overfit the erroneous finger placements existing in the diffusion-generated motions and thus has lower accuracy of key pressing. Those results demonstrate that the diffusion model and motion retrieval are complimentary and both of them are crucial to the final performance of our pipeline. Additionally, in supplementary materials, we qualitatively compare the motions generated by our control policies to those in our dataset when facing the same target notes."
        },
        {
            "title": "6 CONCLUSION\nWe present a first-of-its-kind large-scale dataset of 3D hand motion\nand audio of piano performance. Our dataset, FürElise, contains 8\nhours of performance from 11 elite-level pianists playing 98 pieces\nof classical music. Leveraging FürElise, we propose a physics-based\nmethod to synthesize accurate piano playing motion for music out-\nside the training dataset. We evaluate our method through extensive\nexperiments and ablations.",
            "content": "Our work takes the first step toward motion synthesis of human peak performance using data collected from musicians for unseen songs. However, there is still significant gap between the skill level our model achieves and that of human pianists. Several limitations in our current work might contribute to this gap. First, our method does not consider sound amplitude, critical element in music performance. Consequently, our current model generates music with constant amplitude. However, the key-pressing velocity, which determines amplitude, is recorded in our dataset and can be utilized for future work. Second, we let the model determine fingering, resulting in policies that may struggle with some basic skills such as finger crossover. Future work could incorporate high-level, common fingering rules to facilitate policy learning. Moreover, we leverage F1 scores to evaluate performance averaged over each frame, which may not align well with humans auditory perception, as humans could be sensitive to some transient errors that contribute little to F1 scores such as breaking chord or inconsistent tempo. Developing better audio evaluation metric that meets humans perceptions would be great direction for future work. Finally, while the simulated hand models have reasonably accurate kinematic structure, they can exert unnaturally large joint torques or generate infeasible acceleration. promising future direction is to consider realistic hand musculoskeletal model that generates motion through muscle activation, providing computational tool for biomechanics studies and injury prevention. ACKNOWLEDGMENTS We thank Yifeng Jiang and Jiaman Li for providing detailed feedback on the paper. This work was supported in part by the Wu-Tsai Human Performance Alliances, Stanford Institute for Human-Centered Artificial Intelligence and Roblox. We thank the 15 pianist volunteers 9 SA Conference Papers 24, December 36, 2024, Tokyo, Japan Wang, R. et al for their essential contributions to this study. To protect their privacy, they remain unnamed, but their participation was invaluable to our research. REFERENCES Simon Alexanderson, Rajmund Nagy, Jonas Beskow, and Gustav Eje Henter. 2023. Listen, denoise, action! audio-driven motion synthesis with diffusion models. ACM Transactions on Graphics (TOG) 42, 4 (2023), 120. OpenAI: Marcin Andrychowicz, Bowen Baker, Maciek Chociej, Rafal Jozefowicz, Bob McGrew, Jakub Pachocki, Arthur Petron, Matthias Plappert, Glenn Powell, Alex Ray, et al. 2020. Learning dexterous in-hand manipulation. The International Journal of Robotics Research 39, 1 (2020), 320. Yu-Wei Chao, Wei Yang, Yu Xiang, Pavlo Molchanov, Ankur Handa, Jonathan Tremblay, Yashraj Narang, Karl Van Wyk, Umar Iqbal, Stan Birchfield, et al. 2021. DexYCB: benchmark for capturing hand grasping of objects. In Conference on Computer Vision and Pattern Recognition (CVPR). Jiali Chen, Changjie Fan, Zhimeng Zhang, Gongzheng Li, Zeng Zhao, Zhigang Deng, and Yu Ding. 2023a. Music-Driven Deep Generative Adversarial Model for Guzheng Playing Animation. IEEE Transactions on Visualization and Computer Graphics 29, 2 (2023), 14001414. Sirui Chen, Albert Wu, and Karen Liu. 2023b. Synthesizing Dexterous Nonprehensile Pregrasp for Ungraspable Objects. In ACM SIGGRAPH 2023 Conference Proceedings. 110. George ElKoura and Karan Singh. 2003. Handrix: Animating the Human Hand. (2003), 110119. Zicong Fan, Omid Taheri, Dimitrios Tzionas, Muhammed Kocabas, Manuel Kaufmann, Michael Black, and Otmar Hilliges. 2023. ARCTIC: dataset for dexterous bimanual hand-object manipulation. In Conference on Computer Vision and Pattern Recognition (CVPR). 1294312954. Kristen Grauman, Andrew Westbury, Lorenzo Torresani, Kris Kitani, Jitendra Malik, Triantafyllos Afouras, Kumar Ashutosh, Vijay Baiyya, Siddhant Bansal, Bikram Boote, Eugene Byrne, Zachary Chavis, Joya Chen, Feng Cheng, Fu-Jen Chu, Sean Crane, Avijit Dasgupta, Jing Dong, María Escobar, Cristhian Forigua, Abrham Kahsay Gebreselasie, Sanjay Haresh, Jing Huang, Md Mohaiminul Islam, Suyog Dutt Jain, Rawal Khirodkar, Devansh Kukreja, Kevin Liang, Jia-Wei Liu, Sagnik Majumder, Yongsen Mao, Miguel Martin, Effrosyni Mavroudi, Tushar Nagarajan, Francesco Ragusa, Santhosh K. Ramakrishnan, Luigi Seminara, Arjun Somayazulu, Yale Song, Shan Su, Zihui Xue, Edward Zhang, Jinxu Zhang, Angela Castillo, Changan Chen, Xinzhu Fu, Ryosuke Furuta, Cristina Gonzalez, Prince Gupta, Jiabo Hu, Yifei Huang, Yiming Huang, Weslie Khoo, Anush Kumar, Robert Kuo, Sach Lakhavani, Miao Liu, Mingjing Luo, Zhengyi Luo, Brighid Meredith, Austin Miller, Oluwatumininu Oguntola, Xiaqing Pan, Penny Peng, Shraman Pramanick, Merey Ramazanova, Fiona Ryan, Wei Shan, Kiran Somasundaram, Chenan Song, Audrey Southerland, Masatoshi Tateno, Huiyu Wang, Yuchen Wang, Takuma Yagi, Mingfei Yan, Xitong Yang, Zecheng Yu, Shengxin Cindy Zha, Chen Zhao, Ziwei Zhao, Zhifan Zhu, Jeff Zhuo, Pablo Arbeláez, Gedas Bertasius, David J. Crandall, Dima Damen, Jakob Julian Engel, Giovanni Maria Farinella, Antonino Furnari, Bernard Ghanem, Judy Hoffman, C. V. Jawahar, Richard A. Newcombe, Hyun Soo Park, James M. Rehg, Yoichi Sato, Manolis Savva, Jianbo Shi, Mike Zheng Shou, and Michael Wray. 2024. Ego-Exo4D: Understanding Skilled Human Activity from Firstand Third-Person Perspectives. In Conference on Computer Vision and Pattern Recognition (CVPR). Jonathan Ho, Ajay Jain, and Pieter Abbeel. 2020. Denoising diffusion probabilistic models. In Advances in Neural Information Processing Systems(NeurIPS), Vol. 33. 68406851. Hsuan-Kai Kao and Li Su. 2020. Temporally guided music-to-body-movement generation. In Proceedings of the 28th ACM International Conference on Multimedia. 147155. Qiuqiang Kong, Bochen Li, Xuchen Song, Yuan Wan, and Yuxuan Wang. 2020. HighResolution Piano Transcription With Pedals by Regressing Onset and Offset Times. IEEE/ACM Transactions on Audio, Speech, and Language Processing (2020). Vikash Kumar and Emanuel Todorov. 2015. Mujoco haptix: virtual reality system for hand manipulation. In 2015 IEEE-RAS 15th International Conference on Humanoid Robots (Humanoids). IEEE, 657663. Bochen Li, Akira Maezawa, and Zhiyao Duan. 2018. Skeleton Plays Piano: Online Generation of Pianist Body Movements from MIDI Performance.. In ISMIR. 218 224. Jiaman Li, Jiajun Wu, and Karen Liu. 2023. Object motion guided human motion synthesis. ACM Transactions on Graphics (TOG) 42, 6 (2023), 111. Ruilong Li, Shan Yang, David Ross, and Angjoo Kanazawa. 2021. Ai choreographer: Music conditioned 3d dance generation with aist++. In Conference on Computer Vision and Pattern Recognition (CVPR). 1340113412. Jae Hyun Lim and Jong Chul Ye. 2017. Geometric gan. arXiv preprint arXiv:1705. (2017). Karen Liu. 2008. Synthesis of interactive hand manipulation. In Proceedings of the 2008 ACM SIGGRAPH/Eurographics Symposium on Computer Animation. 163171. 10 Karen Liu. 2009. Dextrous manipulation from grasping pose. In ACM SIGGRAPH 2009 papers. 16. Dong Liu and Jorge Nocedal. 1989. On the limited memory BFGS method for large scale optimization. Mathematical programming 45, 1 (1989), 503528. Jun-Wei Liu, Hung-Yi Lin, Yu-Fen Huang, Hsuan-Kai Kao, and Li Su. 2020. Body Movement Generation for Expressive Violin Performance Applying Neural Networks. International Conference on Acoustics, Speech and Signal Processing (ICASSP) (2020), 37873791. Xueyi Liu and Li Yi. 2024. GeneOH Diffusion: Towards Generalizable Hand-Object Interaction Denoising via Denoising Diffusion. In International Conference on Learning Representations (ICLR). Viktor Makoviychuk, Lukasz Wawrzyniak, Yunrong Guo, Michelle Lu, Kier Storey, Miles Macklin, David Hoeller, Nikita Rudin, Arthur Allshire, Ankur Handa, and Gavriel State. 2021. Isaac Gym: High Performance GPU-Based Physics Simulation For Robot Learning. arXiv:2108.10470 [cs.RO] Josh Merel, Yuval Tassa, Dhruva TB, Sriram Srinivasan, Jay Lemmon, Ziyu Wang, Greg Wayne, and Nicolas Heess. 2017. Learning human behaviors from motion capture by adversarial imitation. arXiv preprint arXiv:1707.02201 (2017). Gyeongsik Moon, Shoou-I Yu, He Wen, Takaaki Shiratori, and Kyoung Mu Lee. 2020. Interhand2. 6m: dataset and baseline for 3d interacting hand pose estimation from single rgb image. In Computer VisionECCV 2020: 16th European Conference, Glasgow, UK, August 2328, 2020, Proceedings, Part XX 16. Springer, 548564. Igor Mordatch, Zoran Popović, and Emanuel Todorov. 2012. Contact-invariant optimization for hand manipulation. In Proceedings of the ACM SIGGRAPH/Eurographics symposium on computer animation. 137144. Heinrich Neuhaus. 2008. The art of piano playing. Kahn and Averill. Georgios Pavlakos, Dandan Shan, Ilija Radosavovic, Angjoo Kanazawa, David Fouhey, and Jitendra Malik. 2024. Reconstructing Hands in 3D with Transformers. Xue Bin Peng, Yunrong Guo, Lina Halper, Sergey Levine, and Sanja Fidler. 2022. ASE: Large-Scale Reusable Adversarial Skill Embeddings for Physically Simulated Characters. ACM Trans. Graph. 41, 4, Article 94 (2022). Xue Bin Peng, Ze Ma, Pieter Abbeel, Sergey Levine, and Angjoo Kanazawa. 2021. AMP: Adversarial Motion Priors for Stylized Physics-Based Character Control. ACM Trans. Graph. 40, 4, Article 144 (2021). Javier Romero, Dimitrios Tzionas, and Michael J. Black. 2017. Embodied hands: modeling and capturing hands and bodies together. ACM Trans. Graph. 36, 6, Article 245 (nov 2017), 17 pages. https://doi.org/10.1145/3130800. John Schulman, Filip Wolski, Prafulla Dhariwal, Alec Radford, and Oleg Klimov. 2017. Proximal Policy Optimization Algorithms. arXiv:1707.06347 [cs.LG] Eli Shlizerman, Lucio Dery, Hayden Schoen, and Ira Kemelmacher-Shlizerman. 2018. Audio to body dynamics. In Conference on Computer Vision and Pattern Recognition (CVPR). 75747583. Tomas Simon, Hanbyul Joo, Iain Matthews, and Yaser Sheikh. 2017. Hand keypoint detection in single images using multiview bootstrapping. In Conference on Computer Vision and Pattern Recognition (CVPR). Omid Taheri, Nima Ghorbani, Michael Black, and Dimitrios Tzionas. 2020. GRAB: dataset of whole-body human grasping of objects. In Computer VisionECCV 2020: 16th European Conference, Glasgow, UK, August 2328, 2020, Proceedings, Part IV 16. Springer, 581600. Guy Tevet, Sigal Raab, Brian Gordon, Yoni Shafir, Daniel Cohen-or, and Amit Haim Bermano. 2023. Human Motion Diffusion Model. In International Conference on Learning Representations (ICLR). https://openreview.net/forum?id=SJ1kSyO2jwu Jonathan Tseng, Rodrigo Castellon, and C. Karen Liu. 2023. EDGE: Editable Dance Generation From Music. In Conference on Computer Vision and Pattern Recognition (CVPR). Chen Wang, Haochen Shi, Weizhuo Wang, Ruohan Zhang, Li Fei-Fei, and C. Karen Liu. 2024. DexCap: Scalable and Portable Mocap Data Collection System for Dexterous Manipulation. arXiv preprint arXiv:2403.07788 (2024). Yangang Wang, Jianyuan Min, Jianjie Zhang, Yebin Liu, Feng Xu, Qionghai Dai, and Jinxiang Chai. 2013. Video-based hand manipulation capture through composite motion control. ACM Transactions on Graphics (TOG) 32, 4 (2013), 114. Erwin Wu, Hayato Nishioka, Shinichi Furuya, and Hideki Koike. 2023. Marker-removal Networks to Collect Precise 3D Hand Data for RGB-based Estimation and its Application in Piano. In Winter Conference on Applications of Computer Vision (WACV). 29772986. Zhaoming Xie, Jonathan Tseng, Sebastian Starke, Michiel van de Panne, and Karen Liu. 2023. Hierarchical planning and control for box loco-manipulation. Proceedings of the ACM on Computer Graphics and Interactive Techniques 6, 3 (2023), 118. Huazhe Xu, Yuping Luo, Shaoxiong Wang, Trevor Darrell, and Roberto Calandra. 2022. Towards learning to play piano with dexterous hands and touch. In International Conference on Intelligent Robots and Systems (IROS). IEEE, 1041010416. Pei Xu and Ioannis Karamouzas. 2021. GAN-Like Approach for Physics-Based Imitation Learning and Interactive Character Control. Proc. of the ACM on Computer Graphics and Interactive Techniques 4, 3 (2021). Pei Xu, Xiumin Shang, Victor Zordan, and Ioannis Karamouzas. 2023. Composite Motion Learning with Task Control. ACM Transactions on Graphics 42, 4 (2023). FürElise: Capturing and Physically Synthesizing Hand Motions of Piano Performance SA Conference Papers 24, December 36, 2024, Tokyo, Japan Yamaha. 2024. Yamaha Disklavier Pianos. https://usa.yamaha.com/products/musical_ instruments/pianos/disklavier/index.html. Zeshi Yang, Kangkang Yin, and Libin Liu. 2022. Learning to use chopsticks in diverse gripping styles. ACM Transactions on Graphics (TOG) 41, 4 (2022), 117. Yuting Ye and Karen Liu. 2012. Synthesis of detailed hand manipulations using contact sampling. ACM Transactions on Graphics (ToG) 31, 4 (2012), 110. Ye Yuan, Jiaming Song, Umar Iqbal, Arash Vahdat, and Jan Kautz. 2023. Physdiff: Physicsguided human motion diffusion model. In International Conference on Computer Vision (ICCV). 1601016021. Kevin Zakka, Philipp Wu, Laura Smith, Nimrod Gileadi, Taylor Howell, Xue Bin Peng, Sumeet Singh, Yuval Tassa, Pete Florence, Andy Zeng, et al. 2023. Robopianist: Dexterous piano playing with deep reinforcement learning. In Conference on Robot Learning (CoRL). Zhang, Ye, Shiratori, and Komura. 2021. ManipNet: neural manipulation synthesis with hand-object spatial representation. ACM Transactions on Graphics (2021). Wenping Zhao, Jianjie Zhang, Jianyuan Min, and Jinxiang Chai. 2013. Robust realtime physics-based motion control for human grasping. ACM Transactions on Graphics (TOG) 32, 6 (2013), 112. Yuanfeng Zhu, Ajay Sundar Ramakrishnan, Bernd Hamann, and Michael Neff. 2013. system for automatic animation of piano performances. Computer Animation and Virtual Worlds 24, 5 (2013), 445457. 11 FürElise: Capturing and Physically Synthesizing Hand Motions of Piano Performance SA Conference Papers 24, December 36, 2024, Tokyo, Japan DATA CAPTURE DETAILS We refine our reconstructed motions using the MIDI recorded during the motion capture to obtain audio-synchronized motions. All the MIDI files were recorded by the pianos built-in recorder with very high accuracy. A.1 MIDI synchronization Since the MIDI and each video are recorded separately by the piano and cameras, we perform synchronization procedure to align them temporally. We first use Kong et al. [2020] to transcribe the audio of the video to MIDI format. Then, we iterate list of candidate offsets and find the offset where the audio and the MIDI have the maximum number of notes matched. Two notes (𝑡 start , 𝑝0) and , 𝑡 end (𝑡 start , 𝑝1) are treated to be matched if they have the same pitch 1 1 and 𝑡 start 𝑡 start 0.016. We then manually fine-tune the offset 1 by aligning the pressing motions of fingers and the start time of the corresponding note. , 𝑡 end 0 0 0 A.2 MIDI-based Inverse Kinematics. To improve the quality of the reconstructed motions, we perform inverse kinematics (IK) based on the key-pressing information provided by the MIDI files. We first compute the pressed keys of the motions by the following heuristics: when any fingertip is horizontally over key and its depth is below preset threshold, we treat that key as being pressed. We then compare the extracted pressed keys with the key-pressing information from the recorded MIDI. Frames, where the extracted pressed keys are different from the ground-truth MIDI, are considered inaccurate and the corresponding hand poses will be fixed by IK. We consider two possible cases of inaccurately reconstructed hand poses: (1) muted key is wrongly pressed by any finger and (2) an activated key is omitted by all fingers for pressing. Wrongly pressed keys. For wrongly pressed keys, when multiple fingertips are pressing it, we select the fingertip with the lowest depth as the IK subject. The IK target is set such that the culprits fingertip will move out of the key at minimum distance. Omitted keys. For keys that all fingertips fail to press, we first find the fingertip closest to the key by projecting all fingertips onto the surface of the key and assume the one with minimum distance to the projected location as the one performing pressing as well as the IK subject. We then set the IK target to the projected point. We invalidate IK targets that need to move the fingertips for more than 1cm, and set up IK by minimizing the following loss function for every frame: (𝚯𝒕 )ik = 1 (cid:205)10 𝑖=1 𝐼 𝑡 𝑖 10 𝑖= 𝐼 𝑡 𝑖 𝒑 𝑖 ˆ𝒑𝑖 (𝚯𝑡 )2, 𝑡 (1) where: 𝑡 is the index of the frame; 𝐼 𝑡 𝑖 is the mask for the 𝑖-th tip, with 𝐼 𝑡 included in the IK and 𝐼 𝑡 𝑖 = 0 otherwise; 𝑖 is the target position of the 𝑖-th tip; 𝒑𝑡 𝑖 = 1 if the tip is to be Table S1. Hyperparameters for Model Training Parameter Diffusion Model Training learning rate batch size training epochs Reinforcement Learning policy network learning rate critic network learning rate discriminator learning rate reward discount factor (𝛾) GAE discount factor (𝜆) surrogate clip range (𝜖) gradient penalty coefficient (𝜆𝐺𝑃 ) number of PPO workers (simulation instances) PPO replay buffer size PPO batch size PPO optimization epochs discriminator replay buffer size discriminator batch size Value 0.0004 512 100 5 106 1 104 1 105 0.95 0.95 0.2 10 512 4096 256 5 8192 512 ˆ𝒑𝑖 (𝚯𝑡 ) is the position of the 𝑖-th tip given the hand parameters 𝚯; and 𝚯𝑡 represents the hand parameters (pose and shape and translation) in the MANO model. Since IK is performed only on frames with wrong key-pressing results, we further add smoothing term to ensure temporal consistency: Lsmooth (𝚯) = 1 𝑁 𝑁 (cid:16) 𝑡 >1 𝚯𝑡 1 𝚯𝑡 2 2 (cid:17) , The final loss is computed by: = 1 𝑁 𝑁 𝑖=1 (𝚯𝑡 )ik + 𝜆Lsmooth, (2) (3) where 𝑁 is the total number of frames in the dataset and 𝜆 = 0.0005. During optimization, we only optimize the local pose parameter and freeze other parameters, using L-BFGS [Liu and Nocedal 1989] optimizer iteratively for 100 epochs. HYPERPARAMETERS The hyperparameters used for diffusion model training and reinforcement learning are listed in Table S1. We employ PPO [Schulman et al. 2017] as our backbone reinforcement learning algorithm. ADDITIONAL RESULTS Here, we include qualitative comparison between the motions generated by our control policies and those in our dataset when facing the same target notes in Figure S1. There are often multiple ways to perform the same target notes. Our pipeline enables the policy to either imitate motions generated by the diffusion model or from the captured dataset, resulting in diverse piano-playing patterns. The synthesized motions can be distinct from human pianists SA Conference Papers 24, December 36, 2024, Tokyo, Japan Wang, R. et al Fig. S1. Comparison between the synthesized motions and motions in our dataset when facing the same target notes. Our control policy could take diverse key-pressing poses by leveraging motions synthesized by the diffusion models (a), or imitating poses existing in our dataset with the similar (b) or different (c) fingering strategy. as shown in Figure S1a. Figure S1b shows an example where the synthesized motion largely resembles human motion in terms of fingering and hand poses. Finally, we show an example where the control policy yields results with similar hand poses but different fingering compared with the human pianist."
        },
        {
            "title": "D REPERTOIRE",
            "content": "List of compositions in the dataset. We include the list of all the compositions in our dataset in Table S2. 2 FürElise: Capturing and Physically Synthesizing Hand Motions of Piano Performance SA Conference Papers 24, December 36, 2024, Tokyo, Japan Table S2. List of compositions in FürElise. Piece Excerpt of Nocturne for the Left Hand, Op. 9, No. 2 In meines Vaters Garten Laue Sommernacht Bei dir ist es traut Die stille Stadt Concerto No. 5 in Major, Op. 103, \"Egyptian\": I. Allegro animato Concerto No. 5 in Major, Op. 103, \"Egyptian\": III. Molto Allegro Concerto No. 5 in Major, Op. 103, \"Egyptian\": II. Andante Clair de Lune from Suite Bergamasque Trois Chanson de Bilitis Images, Book I: III. Mouvement Arabesque No. 1 in Major Images, Book II: I. Cloches à travers les feuilles Images, Book II: III. Poissons dor Images, Book II: II. Et la lune descend sur le temple qui fut Lyric Pieces, Op. 71: No. 2 Sommerabend Lyric Pieces, Op. 43: No. 1 Schmetterling Lyric Pieces, Op. 62: No. 6 Heimwärts Lyric Pieces, Op. 62: No. 4 Bächlein Lyric Pieces, Op. 54: No.3 Zug der Zwerge Lyric Pieces, Op. 38: No. 1 Berceuse Piano Quintet No. 1 in Minor, Op. 1: II. Scherzo (Allegro vivace) Piano Quintet No. 1 in Minor, Op. 1: III. Adagio, quasi andante Piano Quintet No. 1 in Minor, Op. 1 IV. Finale: Allegro moderato Piano Quintet No. 1 in Minor, Op. 1: I. Allegro Excerpt of Prelude No. 7 Prelude No. 8 \"On Drop of Water\" Clouds Impromptu in G-Flat Major, Op. 90, No. 3, D. 899 Mazurka in Minor, Op. 59, No. 1 Prelude in Minor, Op. 28, No. 4, \"Largo\" Prelude in Major, Op. 28, No. 5, \"Allegro molto\" Prelude in Minor, Op. 28, No. 6, \"Lento assai\" Prelude in Major, Op. 28, No. 7, \"Andantino\" Prelude in F-Sharp Minor, Op. 28, No. 8, \"Molto agitato\" Prelude in Major, Op. 28, No. 9, \"Largo\" Prelude in C-Sharp Minor, Op. 28, No. 10, \"Allegro molto\" Étude in A-Flat Major, Op. 25, No. 1 Étude in Major, Op. 10, No. 8 Prelude in Major, Op. 28, No. 3, \"Vivace\" Ballade No. 4 in Minor, Op. 52 Prelude in Major, Op. 28, No. 11, \"Vivace\" Prelude in G-Sharp Minor, Op. 28, No. 12, \"Presto\" Concerto No. 1 in Minor, Op. 11: II. Romance - Larghetto Concerto No. 1 in Minor, Op. 11: I. Allegro maestoso Grande Polonaise Brilliante Andante Spianato Concerto No. 1 in Minor, Op. 11: III. Rondo - Vivace Étude in G-Sharp Minor, Op. 25, No. 6 Prelude in Minor, Op. 28, No. 2, \"Lento\" Étude in Major, Op. 10, No. 3 Piano Sonata No. 2 in B-Flat Minor, Op. 35: I. Grave Doppio movimento 3 Composer Alexander Scriabin Alma Mahler Alma Mahler Alma Mahler Alma Mahler Camille Saint-Saëns Camille Saint-Saëns Camille Saint-Saëns Claude Debussy Claude Debussy Claude Debussy Claude Debussy Debussy Debussy Debussy Edvard Grieg Edvard Grieg Edvard Grieg Edvard Grieg Edvard Grieg Edvard Grieg Ernő Dohnányi Ernő Dohnányi Ernő Dohnányi Ernő Dohnányi Federico Mompou Federico Mompou Florence Price Franz Schubert Frédéric Chopin Frédéric Chopin Frédéric Chopin Frédéric Chopin Frédéric Chopin Frédéric Chopin Frédéric Chopin Frédéric Chopin Frédéric Chopin Frédéric Chopin Frédéric Chopin Frédéric Chopin Frédéric Chopin Frédéric Chopin Frédéric Chopin Frédéric Chopin Frédéric Chopin Frédéric Chopin Frédéric Chopin Frédéric Chopin Frédéric Chopin Frédéric Chopin Frédéric Chopin SA Conference Papers 24, December 36, 2024, Tokyo, Japan Wang, R. et al Ballade No. 3 in A-Flat Major, Op. 47 Piano Sonata No. 2 in B-Flat Minor, Op. 35: IV. Finale: Presto Piano Sonata No. 2 in B-Flat Minor, Op. 35: III. Marche funèbre: Lento Waltz in A-Flat Major, Op. 34, No. 1 Prelude in D-Flat Major, Op. 28, No. 15, \"Raindrop\" Waltz in D-Flat Major, Op. 64, No. 1, \"Minute Waltz\" Piano Sonata No. 2 in B-Flat Minor, Op. 35: II. Scherzo Prelude in Major, Op. 28, No. 1, \"Agitato\" Sonata No. 3 in Minor, Op. 58: IV. Finale: Presto, non tanto Nocturne in D-flat major, Op. 27, No. 2 Fantaisie-Impromptu in C-Sharp Minor, Op. 66 Air \"The Harmonious Blacksmith\" Suite No. 5 in Major: III. Courante Suite No. 5 in Major: II. Allemande Suite No. 5 in Major: I. Prelude Rhapsody in Blue Preludes, Nos. 1, 2, 3 Ich atmet einen Lindenduft Twenty-six Etudes (2007) Part II: No. 10 Andantino Cantabile Concerto in Minor, BWV 1052: I. Allegro Prelude in Major, BWV 854 Goldberg Variations, BWV 988: I. Aria English Suite No. 3 in minor, BWV 808: I. Prelude English Suite No. 3 in minor, BWV 808: II. Allemande English Suite No. 3 in minor, BWV 808: III. Courante English Suite No. 3 in minor, BWV 808: IV. Sarabande English Suite No. 3 in minor, BWV 808: V. Gavotte English Suite No. 3 in minor, BWV 808: VI. Gavotte II (ou la musette) English Suite No. 3 in minor, BWV 808: VII. Gigue Prelude in Minor, BWV 847 Italian Concerto, BWV 971: I. Allegro Fugue in Minor, BWV 875 Prelude in Minor, BWV 875 Concerto in Minor, BWV 1052: III. Allegro English Suite no 6 in minor, BWV 811, Gavotte Preludes and Fugue in Minor, BWV 881 Concerto in Minor, BWV 974: II. Adagio Prelude in Sharp Major, BWV 872 Summer Hue Intermezzo in Major, Op. 118, No. 2: Andante teneramente Intermezzo in Minor, Op. 116, No. 2: Andante Theme and Var 1-6 from Johannes Brahms Variations Intermezzo in Minor, Op. 116, No. 2: Andante Andantino Cantabile Trois morceaux pour piano: Dun vieux jardin Trois morceaux pour piano: Dun jardin clair Trois morceaux pour piano: Cortège Sonata in Major, Op. 2, No. 3: I. Allegro con brio Piano Trio No. 7 in B-Flat Major, Op. 11, \"Gassenhauer\": I. Allegro con brio Piano Sonata in B-flat Major, Op. 22: I. Allegro con brio Piano Sonata No. 32 in Minor, Op. 111: I. Maestoso Allegro con brio ed appassionato Concerto No. 5 in E-Flat Major, Op. 73: I. Allegro (excerpt) Piano Sonata No. 32 in Minor, Op. 111: II. Arietta: Adagio molto semplice cantabile Troubled Waters Miroirs: III. Une Barque sur lOcéan Not Everyone Thinks That Im Beautiful 4 Frédéric Chopin Frédéric Chopin Frédéric Chopin Frédéric Chopin Frédéric Chopin Frédéric Chopin Frédéric Chopin Frédéric Chopin Frédéric Chopin Frédéric Chopin Frédéric Chopin George Frideric Handel George Frideric Handel George Frideric Handel George Frideric Handel George Gershwin George Gershwin Gustav Mahler H. Leslie Adams J.S. Bach J.S. Bach J.S. Bach J.S. Bach J.S. Bach J.S. Bach J.S. Bach J.S. Bach J.S. Bach J.S. Bach J.S. Bach J.S. Bach J.S. Bach J.S. Bach J.S. Bach J.S. Bach J.S. Bach J.S. Bach J.S. Bach Jennifer Higdon Johannes Brahms Johannes Brahms Johannes Brahms Johannes Brahms Leslie Adams Lili Boulanger Lili Boulanger Lili Boulanger Ludwig van Beethoven Ludwig van Beethoven Ludwig van Beethoven Ludwig van Beethoven Ludwig van Beethoven Ludwig van Beethoven Margaret Bonds Maurice Ravel Michael Tilson Thomas FürElise: Capturing and Physically Synthesizing Hand Motions of Piano Performance SA Conference Papers 24, December 36, 2024, Tokyo, Japan Grace All Blues Pictures at an Exhibition, Mvt. 1: Promenade Pictures at an Exhibition, Mvt. 10: The Great Gate of Kiev What is this thing called love Bewitched Bothered and Bewildered Slow Boat to China Scales Czerny No. 1-3 from the School of Velocity Hanon No. 21 & 22 from The Virtuoso Pianist Pt II Scales, Arpeggios and Chords Scales Scales in 2nds, other exercises Fantasie in major, Op. 17: I. Durchaus phantastisch und leidenschaftlich vorzutragen Fantasie in major, Op. 17: III. Langsam getragen. Durchweg leise zu halten Piano Sonata No. 1 in F-sharp minor, Op. 11: I. Introduzione. Un poco adagio Allegro vivace Piano Sonata No. 1 in F-sharp minor, Op. 11: II. Aria Fantasie in major, Op. 17: II. Mäßig. Durchaus energisch Kinderszenen, Op. 15 No.10: Fast zu ernst Kinderszenen, Op. 15 No.5: Glückes genug Novellette No. 1 in Major, Op. 21 Kinderszenen, Op. 15 No.1: Von fremden Ländern und Menschen Kinderszenen, Op. 15 No.2: Kuriose Geschichte Kinderszenen, Op. 15 No.3: Hasche-Mann Kinderszenen, Op. 15 No.4: Bittendes Kind Kinderszenen, Op. 15 No.6: Wichtige Begebenheit Kinderszenen, Op. 15 No.7: Träumerei Kinderszenen, Op. 15 No.8: Am Kamin Piano Sonata No. 1 in F-sharp minor, Op. 11: IV. Finale. Allegro un poco maestoso Piano Sonata No. 1 in F-sharp minor, Op. 11: III. Scherzo Intermezzo. Allegrissimo Kinderszenen, Op. 15 No.11: Fürchtenmachen Kinderszenen, Op. 15 No.12: Kind im Einschlummern Kinderszenen, Op. 15 No.13: Der Dichter spricht Kinderszenen, Op. 15 No.9: Ritter vom Steckenpferd Expanse of my Soul Études-Tableaux, Op. 39: IV. Allegro assai in minor Études-Tableaux, Op. 39: VI. Allegro in minor Études-Tableaux, Op. 39: VII. Lento lugubre in minor Barcarolle in minor, Op. 10, No. 3 Going up Yonder Improvisation Black Pearl Chopin Freddie Sonata in B-Flat Major, K. 333: III. Allegretto grazioso Ah, vous dirai-je, maman Variations, K. 265 Sonata in B-Flat Major, K. 333: I. Allegro Michael Tilson Thomas Miles Davis Modest Mussorgsky Modest Mussorgsky Others Others Others Others Others Others Others Others Others Robert Schumann Robert Schumann Robert Schumann Robert Schumann Robert Schumann Robert Schumann Robert Schumann Robert Schumann Robert Schumann Robert Schumann Robert Schumann Robert Schumann Robert Schumann Robert Schumann Robert Schumann Robert Schumann Robert Schumann Robert Schumann Robert Schumann Robert Schumann Robert Schumann Scott Ordway Sergei Rachmaninoff Sergei Rachmaninoff Sergei Rachmaninoff Sergei Rachmaninoff Stephen Prutsman Stephen Prutsman Stephen Prutsman Wolfgang Amadeus Mozart Wolfgang Amadeus Mozart Wolfgang Amadeus Mozart"
        }
    ],
    "affiliations": [
        "Stanford University, USA"
    ]
}