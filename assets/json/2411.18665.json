{
    "paper_title": "SpotLight: Shadow-Guided Object Relighting via Diffusion",
    "authors": [
        "Frédéric Fortier-Chouinard",
        "Zitian Zhang",
        "Louis-Etienne Messier",
        "Mathieu Garon",
        "Anand Bhattad",
        "Jean-François Lalonde"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Recent work has shown that diffusion models can be used as powerful neural rendering engines that can be leveraged for inserting virtual objects into images. Unlike typical physics-based renderers, however, neural rendering engines are limited by the lack of manual control over the lighting setup, which is often essential for improving or personalizing the desired image outcome. In this paper, we show that precise lighting control can be achieved for object relighting simply by specifying the desired shadows of the object. Rather surprisingly, we show that injecting only the shadow of the object into a pre-trained diffusion-based neural renderer enables it to accurately shade the object according to the desired light position, while properly harmonizing the object (and its shadow) within the target background image. Our method, SpotLight, leverages existing neural rendering approaches and achieves controllable relighting results with no additional training. Specifically, we demonstrate its use with two neural renderers from the recent literature. We show that SpotLight achieves superior object compositing results, both quantitatively and perceptually, as confirmed by a user study, outperforming existing diffusion-based models specifically designed for relighting."
        },
        {
            "title": "Start",
            "content": "4 2 0 2 7 2 ] . [ 1 5 6 6 8 1 . 1 1 4 2 : r SPOTLIGHT: Shadow-Guided Object Relighting via Diffusion Frederic Fortier-Chouinard1 Zitian Zhang1 Louis-Etienne Messier1 Mathieu Garon2 Anand Bhattad3 Jean-Francois Lalonde1 1Universite Laval, 2Depix Technologies, 3Toyota Technological Institute at Chicago https://lvsn.github.io/spotlight Unconditioned Shadow-conditioned (top: shadow mapping, bottom: hand-drawn scribbles) composites from SPOTLIGHT Figure 1. When inserting piece of virtual furniture (top: chair, bottom: ottoman) in an image, (left) conventional diffusion-based renderers produce static composites without lighting control. In contrast, (right) SPOTLIGHT enables shadow-based guidance: user can specify the desired shadows of the inserted objectfor example via shadow mapping (top) or even by hand-drawing scribbles (bottom), SPOTLIGHT allows for explicit lighting control. Through our proposed shadow conditioning, we show that existing diffusion-based renderers can achieve realistic, controllable relighting without requiring any additional training."
        },
        {
            "title": "Abstract",
            "content": "Recent work has shown that diffusion models can be used as powerful neural rendering engines that can be leveraged for inserting virtual objects into images. Unlike typical physics-based renderers, however, neural rendering engines are limited by the lack of manual control over the lighting setup, which is often essential for improving or personalizing the desired image outcome. In this paper, we show that precise lighting control can be achieved for object relighting simply by specifying the desired shadows of the object. Rather surprisingly, we show that injecting only the shadow of the object into pre-trained diffusion-based neural renderer enables it to accurately shade the object according to the desired light position, while properly harmonizing the object (and its shadow) within the target background image. Our method, SPOTLIGHT, leverages existing neural rendering approaches and achieves controllable relighting results with no additional training. Specifically, we demonstrate its use with two neural renderers from the recent literature. We show that SPOTLIGHT achieves superior object compositing results, both quantitatively and perceptually, as confirmed by user study, outperforming existing diffusion-based models specifically designed for relighting. 1. Introduction Image relightingthe process of modifying the lighting conditions of sceneis capability that has long been confined solely to the field of computer graphics, where user is free to control the lighting conditions in the 3D scene before launching the rendering process. When one is given single photograph, however, no such control is possible. Relighting then becomes hard task because of the complex surface interactions between light, material prop1 erties, and surface characteristics, where different materials respond distinctly to lighting variations through specular highlights, surface orientation, and texture. The recent rise of powerful image generation models (e.g., [52]) has enabled exciting possibilities for image relighting [32, 73]. This paper focuses on providing local lighting control, where one wishes to modify the appearance of single object. This is particularly useful in the context of object compositing, that is the task of inserting new (2D or 3D) object into background image. In this context, achieving control over the relighting results is challenging since the object must be relit in way that abides by the desired lighting properties (e.g., direction, intensity, etc.), yet it must also realistically harmonize with the new background image. In this paper, we present SPOTLIGHT, method for realistic, controllable object relighting by leveraging the rich prior knowledge encoded in pre-trained, diffusion-based neural renderer. Rather than training model specifically for the task of object relighting, our key insight is to drive the relighting process by supplying coarse representation of the desired shadow to general-purpose neural renderer. Indeed, we find that simply specifying the desired lighting through the target shadow makes the renderer relight the object accordingly! Fig. 1 shows two examples of such shadow-based lighting control: 1) the shadow of the couch is rendered using basic shadow mapping [19] according to the desired direction (top); and 2) one can even draw scribbles by hand to depict the desired shadow of the ottoman (bottom). To achieve this, SPOTLIGHT exploits classifierfree guidance within existing pre-trained neural renderers and progressively harmonizes imperfect shadows with the background while relighting the object. The core technical contribution lies in our shadow injection approach which is robust to artifacts in the target shadow map. We build upon existing object compositing networks [72, 75] that operate on intrinsic images and exploit diffusion models learned priors to render realistic outputs. We demonstrate SPOTLIGHT handles complex scenarios and maintains physical plausibility without requiring extensive manual intervention nor any additional training. In summary, our contributions are as follows. First, we introduce framework that enhances off-the-shelf diffusion models with object relighting control without requiring fine-tuning or additional training. We demonstrate superior quality, both perceptually through user study and quantitatively, compared to competitive specialized models. Finally, we release novel evaluation dataset tailored for lighting control in object insertion, which will be made available upon publication. 2. Related work Conditional image generation Diffusion-based generative models have become the de facto choice for image generation due to their ability to produce high-quality images [28, 52, 58, 59]. These models offer control mechanisms for various modalities, such as text [48, 52], image content [21, 53], intrinsic images [42, 72, 75], classifier guidance [16] and cross-modal data [12, 69]. Notably, training-free guidance methods [3, 4, 20, 43] have enabled image editing capabilities without specific training, forming the basis of our approach. Image relighting modifies the global or local shading of an image without changing other properties such as geometry and materials. Previous methods for style transfer [25, 41] or harmonization [35, 46, 51, 61] do not prioritize physically accurate shading of the objects. Recently, [47] utilized single-view multi-illumination dataset [44] to enable direct control over the dominant lighting direction. Retinex theory has been leveraged [11, 65] for relighting indoor scenes. IC-Light [73] enforces consistency in appearance to relight portraits and various objects. Some methods specialize in outdoor scenes, using geometric priors like depth [27] or normalized coordinates [32] or both [38] to improve global lighting estimation. These methods however cannot handle lighting on specific region or objects in the scene. Recent work on object relighting has evolved from consistency-based approaches in intrinsic images [9] to harmonization of foreground-background albedo [13]. ZeroComp [75] learns to composite virtual objects in zero-shot fashion, but does not offer lighting control. RGBX [72] uses diffusion models for materialand lighting-aware composition with text-guided control. Neural Gaffer [30] and DiLightNet [71] utilize HDR environment maps and multi-lighting renders, but are not designed for object composition in existing images. We bridge this gap by introducing shadow guidance for object relighting. Explicit lighting estimation approaches infer HDR lighting from single image [22, 24, 37, 76], though they generally lack controllability. Parametric models offer more control [15, 23, 63], but these methods require physics-based rendering engines for generating the composite imagehere, we focus instead on neural renderers. Shadow generation Generative models for rendering realistic shadows [29, 39, 40, 74], or harmonizing rendered shadows with the image [62] have been proposed. Other methods enable controllable shadows for 3D [55] and 2D objects [56, 57]. Our approach can leverage existing shadow generation methods and shows that diffusion-based neural renderers can be conditioned on such shadows to achieve object relighting. Image intrinsics Decomposing images into albedo and shading has long been studied [7, 13, 34, 45, 64]. Many works have also focused on estimating scene geometry, for example, depth [8, 31, 49, 66, 67, 70] and normals [5, 6, 18, 50, 68]. Recently, approaches such as [37, 76] jointly infer different intrinsic maps including shape, spatially-varying 2 Figure 2. Overview of SPOTLIGHT. The framework consists of two parallel branches in pre-trained diffusion-based neural renderer: positive branch (green), where the shadow aligns with the desired light direction, and negative branch (red), where the shadow is aligned with an opposite light direction. Initially, latent blending operation merges the noisy vector with rough scene composition using the background image, object albedo, and coarse shadow. Both branches are then processed by diffusion renderer conditioned on intrinsic images. Finally, relighting of the object is amplified using classifier-free guidance. lighting, scene geometries, and materials. Conversely, generative models have shown powerful internal understanding of intrinsic properties [2, 10, 17, 31, 42, 68]. We similarly leverage diffusion models ability to interpret intrinsic maps to generate realistic images as foundation for rendering image composites. 3. SPOTLIGHT Diffusion-based neural renderers are currently limited by their inability to directly control shading, especially for specific objects as they are generally conditioned on albedo, normals, depth, and global shading. SPOTLIGHT addresses this limitation by enabling control over single object lighting effects by defining coarse shadow. Leveraging priors from pre-trained models and the diffusion framework, we introduce method capable of harmonizing the object realistically from provided shadow region. Fig. 2 presents an overview of the method, with each component described in detail below. 3.2. Blending shadows with the background Provided user defined shadow mask (mshw), we first need to ensure that the diffusion model will properly blend the shadow in its final render. To achieve perceptually pleasing shadow, we take inspiration from Blended Latent Diffusion [4] to progressively blend the noisy latents zt at time with noised VAE-encoded image which contains the shadows composited over the image, g. In practice, we obtain by compositing the object albedo and the shadow on the background. The update made at each timestep to incorporate the desired shadow latents is therefore zt = (1βmshw,)zt+(βmshw,)noise(E(g), t) , (1) where β = 0.05 is the shadow latent weight and mshw, is the shadow mask, first dilated by square 33 33 kernel to maintain the desired shadow softness on the edges, then bilinearly downsampled to the latent resolution (64 64). Noise is added to the encoded composite using the DDIM [59] noise scheduler. Different from [4], we do not use the slow test-time decoder optimization nor the progressive mask shrinking steps. 3.1. Rendering from intrinsics 3.3. Enhancing lighting control Our method leverages pre-trained diffusion-based neural renderer (e.g., ZeroComp [75], RGBX [72]) which accepts multi-channel input of intrinsic maps itypically: albedo, shading, surface normals and depth, although other maps such as roughness and metallic can also be used and outputs an RGB image consistent with these inputs. Our approach also requires shadow and object masks, mshw and mobj resp., where 1 indicates if the pixel is in shadow/object, and 0 otherwise. The discussion below assumes is based on latent diffusion [52], which performs the diffusion process in low resolution latent space defined by VAE encoder = E(x). Simply running neural renderer conditioned on the desired shadow direction yields perceptible, but subtle visual changes in the generated object (as will be shown in sec. 4.6). We propose to amplify the effect by running two parallel branches of the model: one which receives the shadow in the desired direction (positive branch), while the other has the shadow in the opposite direction (negative branch), obtained by shifting the light azimuth angle by 180. Although the opposite light direction works best for the negative branch, we found that casting no shadow also provides good negative sample. We obtain the outputs of the diffusion renderer, which are typically v3 predictions [54], that is, linear combination of the noise and the estimated image. We then use classifier-free guidance to amplify the effect of the positive shadow on the lighting on the object: vt = (1 mobj,) vt,pos + mobj, (vt,neg + γ(vt,pos vt,neg)) , (2) where γ = 3.0 is the guidance scale, mobj, is the object mask bilinearly downsampled to the latent resolution (64 64). After applying eq. (2), we run regular DDIM sampling step to obtain the latents zt1. 3.4. Diffusion-based neural renderer backbone Since SPOTLIGHT only applies changes to the latent space of diffusion model, it can readily be applied to pre-trained diffusion-based neural renderers without any finetuning. ZeroComp [75] provides maskable shading map as input to ControlNet-based neural renderer. Here, we use their approach directly and mask out the shading on both the object and the shadow region. RGBX [72] provides checkpoint of their XRGB model finetuned for inpainting rectangular masks. We mask out the bounding box including the object and shadow region. At each denoising step, the denoising network is ran to obtain vt = (zt, i, t) , (3) where vt represents the network v-prediction, represents the composited intrinsics. For ZeroComp, those intrinsics correspond to albedo, normals, depth, and partial shading and for RGBX, we use the albedo, normals, metallic, roughness and masked image. 3.5. Obtaining the final render After having performed the denosing steps, we obtain an image by decoding the latents z0 into x0 by using the VAE decoder. The final result is obtained with the background preservation strategy proposed in [75], which obtains shadow matte by computing the ratio of the image obtained with and without shadow guidance. This ensures the diffusion process does not affect the background image. 4. Evaluation on 3D object compositing In this section, we quantitatively evaluate SPOTLIGHT against other methods that support lighting control on composition of 3D objects in 2D images. Additionally, we conduct user study to assess human perception, comparing its performance with these competing approaches. 4.1. Evaluation dataset We start from the evaluation dataset proposed in [75], where high-quality 3D models from the Amazon Berkeley Objects dataset [14] are inserted into background images cropped with 50 field of view from the Laval Indoor HDR dataset [22]. The authors of [75] graciously provided us with reproducible Blender scene information, which we extend in two ways. First, we improve the shadow catcher using back-projected background depth estimated using Depth Anything V2 [67]. Second, object intrinsics are rendered with anti-aliased alpha mattes, allowing for more realistic compositing results. We use depth-warping strategy on the HDR panorama as in [75] to account for spatially varying lighting around the object position. The dataset is rendered using the physically-based Cycles renderer with 2048 samples per pixel, noise threshold of 0.01, and denoised using OpenImageDenoise [1]. We produce two versions of the dataset, each intended for different purposes. Reference-based dataset This version identifies the dominant light source in the scene, extracted from the ground-truth warped panorama using the light extraction algorithm from [23]. total of 210 images featuring various objects are rendered, with example samples shown in fig. 3. We filter out samples where the background depth estimation failed. This data set is intended for quantitative evaluation (sec. 4.4), allowing the simulated ground truth of the inserted object to be used to compute metrics. User-controlled dataset This second dataset simulates scenario where user specifies desired light position. In this setup, five lighting directions are defined from left to right in 45 increments in azimuth, with the light source passing behind the object to ensure the shadow remains visible. The same set of background and object combinations as in the reference-based version is used, resulting in total of 1,050 images. This dataset will be used in the user study (sec. 4.5) to assess the perceptual accuracy of renders in edited lighting scenarios. 4.2. SPOTLIGHT specifics As mentioned in sec. 3.4, our proposed SPOTLIGHT readily applies to existing diffusion-based neural renderers. Here, we experiment with two approaches from the recent literature: ZeroComp [75] and RGBX [72]. ZeroComp backbone We train ZeroComp following the procedure outlined in [75], and use the same conditioned intrinsic estimators at inference time: StableNormal [68] for normals, IID [33] for albedo, and ZoeDepth [8] for depth. The masked shading in the background is obtained by dividing the image by the albedo and applying the same masking strategy. Given its superior results, we adopt this method as our default model for 3D object compositing. RGBX backbone We use the RGBX model to extract intrinsic properties (normals, metallic, roughness, and albedo). We observe that results are generally overly bright, likely due to domain gap between the training and evaluation images. To address this, we re-expose the background Light direction IC-Light [73] DiLightNet [71] Neural Gaffer [30] Ours (RGBX) Ours (ZeroComp) Simulated GT Figure 3. Qualitative comparison against diffusion-based relighting approaches on our reference-based dataset, where the simulated ground truth is available. The light direction shown here is the dominant light direction. Please zoom in to see the details. by factor of 2 before feeding it to the network, then divide the output by 2, in linear space. We apply the same background preservation strategy as in ZeroComp. Light control SPOTLIGHT requires coarse shadow for guidance. Therefore, we use real-time rasterization renderer (Blenders EEVEE) to compute simple shadow of the 3D object. This shadow is generated from simple parametric distant sphere light and projected onto the mesh constructed from the estimated depth map of the background. 4.3. Baselines We compare SPOTLIGHT against recent diffusion-based methods that support light control: DiLightNet [71], ICLight [73], and Neural Gaffer [30], using their public implementations. While these baselines can relight objects, none are capable of generating shadow around the object. For fairness, the physically-based renderer Cycles is therefore used to cast realistic shadow. OpenImageDenoise [1] is also used to denoise the results of all methods. We begin by describing how light control is given to baselines, then proceed to describe per-method specifics."
        },
        {
            "title": "4.3.1 Light control",
            "content": "In order to ensure fairness in evaluation, we aim to provide each method with light parametrization that is as close as possible to the ground truth dominant light direction available in the evaluation dataset. Both DiLightNet [71] and Neural Gaffer [30] require an environment map for lighting. We construct such an environment map with radiance Lenv along direction ω defined as the sum of spherical gaussian and constant term Lenv(ω) = clighteλ(ωv1) + camb , (4) where clight and camb are the RGB colors of the light and ambient terms resp., is the dominant light source direction, and λ is the bandwidth. camb is obtained by computing the average color over the background image. Panoramas from the Laval Indoor HDR dataset (excluding those in our test set) are used to estimate single average intensity of the dominant light source, k, which is set to the ratio of the integral of the brightest pixels in the panoramas, divided by the integral over all pixels. We further divide this integral by 2 to consider single hemisphere and avoid overly bright dominant light sources. The light color is defined by clight = kc amb is the normalized ambient color camb. We found that the bandwidth parameter λ did amb, where 5 not advantage any specific method and therefore fixed it to λ = 300, which we typically observe for an indoor light."
        },
        {
            "title": "4.3.2 Per-method details",
            "content": "DiLightNet [71] DiLightNet requires prompt describing the object, which we obtain by feeding the ground truth object composited on black background to BLIP-2 [36] with the prompt Putting aside the black background, this object is a. To generate the results, depth and radiance hints are required. We supply DiLightNet with depth information from the 3D model of the object and render the radiance hints using the input environment map (sec. 4.3.1) as outlined in [71]. The rendered object is then composited with the background, along with the rendered shadow. Neural Gaffer [30] We provide the method with the environment map described in sec. 4.3.1. The ground truth object is composited on white background before feeding it to the network, as expected by the method. Since Neural Gaffer is trained at lower resolution (256 256), its result is upsampled to 512 512 then composited on the full resolution background. An erosion using 3 3 kernel is applied to the ground truth mask to mitigate boundary artifacts. IC-Light [73] We evaluate the background-conditioned model of IC-Light, where the background image with the shadow is used for conditioning. We provide rendering of the object lit by the ground truth environment map, giving significant advantage to the method. No prompts are used. 4.4. Experimental results Tab. 1 presents quantitative results comparing the various baselines (c.f. sec. 4.3) with two versions of SPOTLIGHT (c.f. sec. 4.2) on our reference-based dataset, where simulated ground truth is available (sec. 4.1). Here, all methods are conditioned on the dominant light direction using their specific light parametrization (sec. 4.3.1). It is worth noting that all three baselines (DiLightNet, IC-Light and Neural Gaffer) are given as input the ground truth object foreground and shadowed background, giving them significant advantage. Fig. 3 shows qualitative results for each method. We observe that SPOTLIGHT with the ZeroComp backbone outperforms the baselines on quantitative metrics and produces visually superior results. 4.5. User study user study was conducted to evaluate the realism of local lighting control. Here, we use the user-controlled version of the dataset (see sec. 4.1) to evaluate whether the realism of results obtained by conditioning on specific lighting direction, even if it is not the dominant light direction. We evaluate the three methods with the best PSNR from tab. 1, namely SPOTLIGHT (with the ZeroComp backbone), Neural Gaffer [30] and IC-Light [73]. Since no ground truth Method PSNR SSIM RMSE MAE LPIPS DiLightNet [71] IC-Light [73] Neural Gaffer [30] 25.36 0.950 0.059 0.024 0.044 28.59 0.965 0.043 0.018 0.034 29.64 0.963 0.037 0.016 0.038 SPOTLIGHT (RGBX) 26.44 0.956 0.052 0.018 0.041 SPOTLIGHT (ZeroComp) 30.68 0.973 0.033 0.012 0.031 Table 1. Quantitative results obtained by conditioning the methods on the dominant light direction. All metrics are computed on our reference-based evaluation dataset against the simulated ground truth. DilightNet, IC-Light and Neural Gaffer have access to the simulated ground truth for both the inserted object and shadowed background, offering them notable advantage. Nevertheless, our method with the ZeroComp backbone surpasses all baselines across all metrics, and the RGBX variant remains competitive. Results are color coded by best and secondbest. Figure 4. Thurstone Case z-scores from our user study on = 37 observers, with the 95% confidence intervals overlayed on each bar. Our method was preferred over both baselines and achieves statistical significance, even though the two baselines are given privileged access to the simulated ground truth. is available, we design two-alternative forced choice user study, where results obtained by two methods are shown side by side. The participants are asked to Click on the image that looks the most realistic, by considering both the appearance of the object and its shadows. For each pair of methods, set of 40 random images are chosen, resulting in 120 pairs of images observed by each participant. We randomly shuffle the 120 pairs for each user, as well as the left-right order. Examples are shown in fig. 5. The user study was completed by = 37 observers. Following Giroux et al. [26], we used the Thurstone Case Law of Comparative Judgement [60] to obtain scaling z-score for each method, where higher scale indicates higher human preference. The results, shown in fig. 4 show that our method is the favorite. The 95% confidence interval error bars also show that the margin in performance is statistically significant, indicating that SPOTLIGHT achieves the highest realism. 4.6. Effect of parameter selection Our method employs parameters that control the degree of shadow guidance (γ in eq. (2)) and the shadow blending (β in eq. (1)). Tab. 2 shows the impact of parameter selection on quantitative metrics, obtained on the reference-based Light direction IC-Light [73] DiLightNet [71] Neural Gaffer [30] Ours (RGBX) Ours (ZeroComp) Figure 5. Qualitative comparison against diffusion-based approaches on our user-controlled dataset. The light direction shown on the left samples various azimuth angles (with 45 elevation). Please zoom in and consult the supp. for additional qualitative results. dataset where the simulated ground truth is available (c.f. sec. 4.1). Although using no guidance (γ = 1) results in better quantitative performance, we observe qualitatively in fig. 6 that this significantly reduces the visibility of generated shading on the inserted object, thereby reducing the impact of the desired local lighting control. Employing no shadow blending (β = 0) results in similar metric values but makes shadows much less visible  (fig. 7)  . 5. Extensions This section highlights additional capabilities derived from our framework to further demonstrate its versatility. Shadow control on 2D object composition We demonstrate that SPOTLIGHT can be applied to 2D images by leveraging ZeroComp [75] backbone trained without depth maps, using the normals [68] and albedo [33] estimators. Without access to 3D model, we cannot rely on rendering engine to cast shadows. Instead, we employ PixHtLab [57] to generate realistic soft shadows from controllable point light coordinate. Here, sample without shadow is provided for negative guidance vt,neg, see sec. 3.3. To estimate the required pixel height of the object cutout, we attribute each pixel y-coordinate as the pixel height for demonstration purposes, more sophisticated methods such as [56] could be used. After rendering with the backbone, we enhance the target by adding the high-frequency details from the source image. This is achieved by subtracting Gaussian-blurred version of the source image (σ = 1) from itself (as in unsharp masking) and adding the resulting details to the target. Example 2D compositing results are 7 Method PSNR SSIM RMSE MAE LPIPS γ = 3, β = 0.05 (Ours) 30.68 0.974 0.033 0.012 0. γ = 1 (no guidance) γ = 7 31.65 0.976 0.030 0.011 0.029 28.60 0.966 0.044 0.015 0.036 β = 0 β = 0.2 30.78 0.974 0.033 0.012 0.030 29.32 0.970 0.039 0.014 0.034 Table 2. Impact of parameter selection on quantitative metrics. We observe that some changes in parameters, like using no guidance (γ = 1), may provide better quantitative results. However, we also observe that these changes diminish the level of light control over the object. Our selected parameter combination provides good quantitative performance and adequate lighting control. Object Background Light-controllable composites Figure 8. 2D object compositing results using SPOTLIGHT with the ZeroComp backbone. Our method refines coarse shadows estimated from 2D geometry and realistically relights and harmonizes the object cutout. No guidance (γ = 1) γ = 3 (ours) γ = Figure 6. Effect of varying the guidance scale γ, on back (top) and front (bottom) lights. low γ value (left) results in minimal variation in the relighting effect, whereas high value (right) over-amplifies the reshading from the virtual light source. We empirically use γ = 3 as it offers the best balance between realism and control for indoor scene settings. β = 0 (no blending) β = 0.05 (ours) β = 0.2 Figure 7. Effect of varying the latent shadow mask weight β. When β = 0 (left), the diffusion renderer has too much freedom in editing the shading, and does not generate strong shadows. When β = 2 (right), the shadows typically become too dark. moderate weight of 0.05 achieves good balance (middle). shown in fig. 8, which demonstrate realistic 2D object relighting from user-controlled shadow. Scribbles as shadows Shadows can be generated in mul8 Figure 9. Rough sketches of shadows also provide powerful lighting cue to SPOTLIGHT. By scribbling shadow next to the object region (top), SPOTLIGHT is able to realistically relight the object and refine the shadow (bottom). tiple ways: we have demonstrated the use of shadow mapping in 3D and pixel height in 2D. However, one can simply draw the desired shadow! Fig. 9 and fig. 1 (first row) show that SPOTLIGHT is able to realistically relight the object and refine its shadow even when it is user-drawn scribble. 6. Discussion We present SPOTLIGHT, method for realistically controlling the local lighting of an object through coarse shadow, compatible with diffusion-based intrinsic image renderers without requiring any additional training. Our results demonstrate that fine-grained control over local lighting can be achieved while attaining realistic compositions. While the model can realistically estimate relighting from shadow, generating shadow map from scratch remains challenging. promising future direction is to enable end-to-end shadow control using parametric light models, such as point or sphere lights. Additionally, similar approaches could be explored to enable global lighting adjustments for entire images. Acknowledgements This research was supported by NSERC grants RGPIN 2020-04799 and ALLRP 58654323, Mitacs and Depix. Computing resources were provided by the Digital Research Alliance of Canada. The authors thank Zheng Zeng, Yannick Hold-Geoffroy and Justine Giroux for their help as well as all members of the lab for discussions and proofreading help."
        },
        {
            "title": "References",
            "content": "[1] Attila T. Afra. Intel Open Image Denoise, 2024. https: //www.openimagedenoise.org. 4, 5 [2] Anonymous. Intrinsic-controlnet : generative rendering approach to render any real and unreal. In Submitted to Int. Conf. Learn. Represent., 2024. under review. 3 [3] Omri Avrahami, Dani Lischinski, and Ohad Fried. Blended diffusion for text-driven editing of natural images. In IEEE Conf. Comput. Vis. Pattern Recog., 2022. 2 [4] Omri Avrahami, Ohad Fried, and Dani Lischinski. Blended latent diffusion. ACM Trans. Graph., 42(4), 2023. 2, 3 [5] Gwangbin Bae and Andrew Davison. Rethinking inductive biases for surface normal estimation. In IEEE Conf. Comput. Vis. Pattern Recog., 2024. [6] Gwangbin Bae, Ignas Budvytis, and Roberto Cipolla. Estimating and exploiting the aleatoric uncertainty in surface normal estimation. In Int. Conf. Comput. Vis., pages 13137 13146, 2021. 2 [7] Harry Barrow, Tenenbaum, Hanson, and Riseman. Recovering intrinsic scene characteristics. Comput. vis. syst, 2 (3-26):2, 1978. 2 [8] Shariq Farooq Bhat, Reiner Birkl, Diana Wofk, Peter Wonka, and Matthias Muller. Zoedepth: Zero-shot transfer by combining relative and metric depth. arXiv preprint arXiv:2302.12288, 2023. 2, 4 [9] Anand Bhattad and David Forsyth. Cut-and-paste object insertion by enabling deep image prior for reshading. In Int. Conf. 3D Vis., 2022. 2 [10] Anand Bhattad, Daniel McKee, Derek Hoiem, and David Forsyth. Stylegan knows normal, depth, albedo, and more. Adv. Neural Inform. Process. Syst., 36, 2024. 3 [11] Anand Bhattad, James Soole, and DA Forsyth. Stylitgan: In IEEE Conf. Image-based relighting via latent control. Comput. Vis. Pattern Recog., 2024. 2 [12] Tim Brooks, Aleksander Holynski, and Alexei Efros. Instructpix2pix: Learning to follow image editing instructions. In IEEE Conf. Comput. Vis. Pattern Recog., 2023. 2 [13] Chris Careaga, Mahdi Miangoleh, and Yagız Aksoy. Intrinsic harmonization for illumination-aware compositing. In ACM SIGGRAPH Asia Conf., 2023. 2 [14] Jasmine Collins, Shubham Goel, Kenan Deng, Achleshwar Luthra, Leon Xu, Erhan Gundogdu, Xi Zhang, Tomas Yago Vicente, Thomas Dideriksen, Himanshu Arora, Matthieu Guillaumin, and Jitendra Malik. ABO: Dataset and benchmarks for real-world 3d object understanding. In IEEE Conf. Comput. Vis. Pattern Recog., 2022. 4 [15] Mohammad Reza Karimi Dastjerdi, Jonathan Eisenmann, Yannick Hold-Geoffroy, and Jean-Francois Lalonde. Everlight: Indoor-outdoor editable HDR lighting estimation. In Int. Conf. Comput. Vis., 2023. 2 [16] Prafulla Dhariwal and Alexander Nichol. Diffusion models beat gans on image synthesis. In Adv. Neural Inform. Process. Syst., 2021. [17] Xiaodan Du, Nicholas Kolkin, Greg Shakhnarovich, and Anand Bhattad. Generative models: What do they know? arXiv preprint do they know things? arXiv:2311.17137, 2023. 3 lets find out! [18] Ainaz Eftekhar, Alexander Sax, Jitendra Malik, and Amir Zamir. Omnidata: scalable pipeline for making multi-task mid-level vision datasets from 3d scans. In Int. Conf. Comput. Vis., pages 1078610796, 2021. 2 [19] Elmar Eisemann, Michael Schwarz, Ulf Assarsson, and Michael Wimmer. Real-Time Shadows. A. K. Peters, Ltd., 2011. 2 [20] Dave Epstein, Allan Jabri, Ben Poole, Alexei Efros, and Aleksander Holynski. Diffusion self-guidance for controlIn Adv. Neural Inform. Process. lable image generation. Syst., 2023. 2 [21] Rinon Gal, Yuval Alaluf, Yuval Atzmon, Or Patashnik, Amit Bermano, Gal Chechik, and Daniel CohenOr. An image is worth one word: Personalizing text-toimage generation using textual inversion. arXiv preprint arXiv:2208.01618, 2022. [22] Marc-Andre Gardner, Kalyan Sunkavalli, Ersin Yumer, Xiaohui Shen, Emiliano Gambaretto, Christian Gagne, and Jean-Francois Lalonde. Learning to predict indoor illumination from single image. ACM Trans. Graph., 9(4), 2017. 2, 4 [23] Marc-Andre Gardner, Yannick Hold-Geoffroy, Kalyan Sunkavalli, Christian Gagne, and Jean-Francois Lalonde. In Int. Conf. Deep parametric indoor lighting estimation. Comput. Vis., 2019. 2, 4 [24] Mathieu Garon, Kalyan Sunkavalli, Sunil Hadap, Nathan Carr, and Jean-Francois Lalonde. Fast spatially-varying indoor lighting estimation. In IEEE Conf. Comput. Vis. Pattern Recog., 2019. 2 [25] Leon Gatys, Alexander Ecker, and Matthias Bethge. Image style transfer using convolutional neural networks. In IEEE Conf. Comput. Vis. Pattern Recog., 2016. 2 [26] Justine Giroux, Mohammad Reza Karimi Dastjerdi, Yannick Hold-Geoffroy, Javier Vazquez-Corral, and Jean-Francois Lalonde. Towards perceptual evaluation framework for In IEEE Conf. Comput. Vis. Pattern lighting estimation. Recog., 2024. 6 [27] David Griffiths, Tobias Ritschel, and Julien Philip. Outcast: Single image relighting with cast shadows. Comput. Graph. Forum, 43, 2022. [28] Jonathan Ho, Ajay Jain, and Pieter Abbeel. Denoising diffusion probabilistic models. In Adv. Neural Inform. Process. Syst., 2020. 2 [29] Yan Hong, Li Niu, and Jianfu Zhang. Shadow generation for composite image in real-world scenes. In Assoc. Adv. of Art. Int., 2022. 2 9 [30] Haian Jin, Yuan Li, Fujun Luan, Yuanbo Xiangli, Sai Bi, Kai Zhang, Zexiang Xu, Jin Sun, and Noah Snavely. Neural gaffer: Relighting any object via diffusion. In Adv. Neural Inform. Process. Syst., 2024. 2, 5, 6, 7 [31] Bingxin Ke, Anton Obukhov, Shengyu Huang, Nando Metzger, Rodrigo Caye Daudt, and Konrad Schindler. Repurposing diffusion-based image generators for monocular depth In IEEE Conf. Comput. Vis. Pattern Recog., estimation. 2024. 2, 3 [32] Peter Kocsis, Julien Philip, Kalyan Sunkavalli, Matthias Nießner, and Yannick Hold-Geoffroy. Lightit: Illumination modeling and control for diffusion models. In IEEE Conf. Comput. Vis. Pattern Recog., 2024. [33] Peter Kocsis, Vincent Sitzmann, and Matthias Nießner. Intrinsic image diffusion for single-view material estimation. In IEEE Conf. Comput. Vis. Pattern Recog., 2024. 4, 7 [34] Balazs Kovacs, Sean Bell, Noah Snavely, and Kavita Bala. Shading annotations in the wild. In IEEE Conf. Comput. Vis. Pattern Recog., 2017. 2 [35] Jean-Francois Lalonde and Alexei Efros. Using color compatibility for assessing image realism. In Int. Conf. Comput. Vis., 2007. 2 [36] Junnan Li, Dongxu Li, Silvio Savarese, and Steven Hoi. Blip-2: Bootstrapping language-image pre-training with In Int. frozen image encoders and large language models. Conf. Mach. Learn., 2023. 6 [37] Zhengqin Li, Mohammad Shafiei, Ravi Ramamoorthi, Kalyan Sunkavalli, and Manmohan Chandraker. Inverse rendering for complex indoor scenes: Shape, spatially-varying lighting and svbrdf from single image. In IEEE Conf. Comput. Vis. Pattern Recog., 2020. 2 [38] Zhi-Hao Lin, Bohan Liu, Yi-Ting Chen, Kuan-Sheng Chen, David Forsyth, Jia-Bin Huang, Anand Bhattad, and Shenlong Wang. Urbanir: Large-scale urban scene inverse rendering from single video. In Int. Conf. 3D Vis., 2025. 2 [39] Daquan Liu, Chengjiang Long, Hongpan Zhang, Hanning Yu, Xinzhi Dong, and Chunxia Xiao. ARShadowGAN: Shadow generative adversarial network for augmented reality in single light scenes. In IEEE Conf. Comput. Vis. Pattern Recog., 2020. 2 [40] Qingyang Liu, Junqi You, Jianting Wang, Xinhao Tao, Bo Zhang, and Li Niu. Shadow generation for composite image using diffusion model. In IEEE Conf. Comput. Vis. Pattern Recog., 2024. [41] Fujun Luan, Sylvain Paris, Eli Shechtman, and Kavita Bala. Deep photo style transfer. In IEEE Conf. Comput. Vis. Pattern Recog., 2017. 2 [42] Jundan Luo, Duygu Ceylan, Jae Shin Yoon, Nanxuan Zhao, Julien Philip, Anna Fruhstuck, Wenbin Li, Christian Richardt, and Tuanfeng Wang. Intrinsicdiffusion: joint inIn ACM SIGtrinsic layers from latent diffusion models. GRAPH Conf., 2024. 2, 3 [43] Chenlin Meng, Yutong He, Yang Song, Jiaming Song, Jiajun Wu, Jun-Yan Zhu, and Stefano Ermon. SDEdit: Guided image synthesis and editing with stochastic differential equations. In Int. Conf. Learn. Represent., 2022. 2 [44] Lukas Murmann, Michael Gharbi, Miika Aittala, and Fredo Durand. multi-illumination dataset of indoor object appearance. In Int. Conf. Comput. Vis., 2019. 2 [45] Julien Philip, Michael Gharbi, Tinghui Zhou, Alexei Efros, and George Drettakis. Multi-view relighting using geometry-aware network. ACM Trans. Graph., 38(4):781, 2019. 2 [46] Francois Pitie, Anil Kokaram, and Rozenn Dahyot. Ndimensional probability density function transfer and its application to color transfer. In Int. Conf. Comput. Vis., 2005. [47] Yohan Poirier-Ginter, Alban Gauthier, Julien Phillip, J-F Lalonde, and George Drettakis. diffusion approach to radiance field relighting using multi-illumination synthesis. In Comput. Graph. Forum, page e15147. Wiley Online Library, 2024. 2 [48] Aditya Ramesh, Prafulla Dhariwal, Alex Nichol, Casey Chu, and Mark Chen. Hierarchical text-conditional image generation with clip latents. arXiv preprint arXiv:2204.06125, 1 (2):3, 2022. 2 [49] Rene Ranftl, Katrin Lasinger, David Hafner, Konrad Schindler, and Vladlen Koltun. Towards robust monocular depth estimation: Mixing datasets for zero-shot crossdataset transfer. IEEE Trans. Pattern Anal. Mach. Intell., 44 (3):16231637, 2020. 2 [50] Rene Ranftl, Alexey Bochkovskiy, and Vladlen Koltun. Vision transformers for dense prediction. In Int. Conf. Comput. Vis., 2021. 2 [51] Erik Reinhard, Michael Adhikhmin, Bruce Gooch, and Peter Shirley. Color transfer between images. IEEE Comp. Graph. Appl., 21(5):3441, 2001. 2 [52] Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Bjorn Ommer. High-resolution image synthesis with latent diffusion models. In IEEE Conf. Comput. Vis. Pattern Recog., 2022. 2, [53] Chitwan Saharia, William Chan, Huiwen Chang, Chris Lee, Jonathan Ho, Tim Salimans, David Fleet, and Mohammad Norouzi. Palette: Image-to-image diffusion models. In ACM SIGGRAPH Conf., pages 110, 2022. 2 [54] Tim Salimans and Jonathan Ho. Progressive distillation for fast sampling of diffusion models. In International Conference on Learning Representations, 2022. 4 [55] Yichen Sheng, Jianming Zhang, and Bedrich Benes. Ssn: Soft shadow network for image compositing. In IEEE Conf. Comput. Vis. Pattern Recog., 2021. 2 [56] Yichen Sheng, Yifan Liu, Jianming Zhang, Wei Yin, Cengiz Oztireli, He Zhang, Zhe Lin, Eli Shechtman, and Bedrich Benes. Controllable shadow generation using pixel height maps. In Eur. Conf. Comput. Vis., 2022. 2, 7 [57] Yichen Sheng, Jianming Zhang, Julien Philip, Yannick HoldGeoffroy, Xin Sun, He Zhang, Lu Ling, and Bedrich Benes. Pixht-lab: Pixel height based light effect generation for image compositing. In IEEE Conf. Comput. Vis. Pattern Recog., 2023. 2, 7 [58] Jascha Sohl-Dickstein, Eric Weiss, Niru Maheswaranathan, and Surya Ganguli. Deep unsupervised learning using nonequilibrium thermodynamics. In Int. Conf. Mach. Learn., 2015. 10 [75] Zitian Zhang, Frederic Fortier-Chouinard, Mathieu Garon, Anand Bhattad, and Jean-Francois Lalonde. Zerocomp: Zero-shot object compositing from image intrinsics via difIn Winter Conf. App. Comput. Vis., 2025. 2, 3, 4, fusion. 7 [76] Rui Zhu, Zhengqin Li, Janarbek Matai, Fatih Porikli, and Manmohan Chandraker. Irisformer: Dense vision transformers for single-image inverse rendering in indoor scenes. In IEEE Conf. Comput. Vis. Pattern Recog., 2022. 2 [59] Jiaming Song, Chenlin Meng, and Stefano Ermon. DenoisIn Int. Conf. Learn. Repreing diffusion implicit models. sent., 2021. 2, 3 [60] Louis Thurstone. law of comparative judgment. In Scaling, pages 8192. Routledge, 1927. 6 [61] Yi-Hsuan Tsai, Xiaohui Shen, Zhe Lin, Kalyan Sunkavalli, Xin Lu, and Ming-Hsuan Yang. Deep image harmonization. In IEEE Conf. Comput. Vis. Pattern Recog., 2017. 2 [62] Lucas Valenca, Jinsong Zhang, Michael Gharbi, Yannick Hold-Geoffroy, and Jean-Francois Lalonde. Shadow harmonization for realistic compositing. In ACM SIGGRAPH Asia Conf., 2023. 2 [63] Henrique Weber, Mathieu Garon, and Jean-Francois Lalonde. Editable indoor lighting estimation. In Eur. Conf. Comput. Vis., 2022. 2 [64] Jiaye Wu, Sanjoy Chowdhury, Hariharmano Shanmugaraja, David Jacobs, and Soumyadip Sengupta. Measured albedo in the wild: Filling the gap in intrinsics evaluation. In Int. Conf. Comput. Photo., 2023. [65] Xiaoyan Xing, Vincent Tao Hu, Jan Hendrik Metzen, Konrad Groh, Sezer Karaoglu, and Theo Gevers. Retinex-diffusion: On controlling illumination conditions in diffusion models via retinex theory. arXiv preprint arXiv:2407.20785, 2024. 2 [66] Lihe Yang, Bingyi Kang, Zilong Huang, Xiaogang Xu, Jiashi Feng, and Hengshuang Zhao. Depth anything: Unleashing the power of large-scale unlabeled data. In IEEE Conf. Comput. Vis. Pattern Recog., 2024. 2 [67] Lihe Yang, Bingyi Kang, Zilong Huang, Zhen Zhao, Xiaogang Xu, Jiashi Feng, and Hengshuang Zhao. Depth anything v2. arXiv:2406.09414, 2024. 2, 4 [68] Chongjie Ye, Lingteng Qiu, Xiaodong Gu, Qi Zuo, Yushuang Wu, Zilong Dong, Liefeng Bo, Yuliang Xiu, Stablenormal: Reducing diffusion and Xiaoguang Han. arXiv preprint variance for stable and sharp normal. arXiv:2406.16864, 2024. 2, 3, 4, 7 [69] Hu Ye, Jun Zhang, Sibo Liu, Xiao Han, and Wei Yang. Ipadapter: Text compatible image prompt adapter for text-toimage diffusion models. arXiv preprint arXiv:2308.06721, 2023. 2 [70] Wei Yin, Chi Zhang, Hao Chen, Zhipeng Cai, Gang Yu, Kaixuan Wang, Xiaozhi Chen, and Chunhua Shen. Metric3D: Towards zero-shot metric 3D prediction from single image. In Int. Conf. Comput. Vis., 2023. [71] Chong Zeng, Yue Dong, Pieter Peers, Youkang Kong, Hongzhi Wu, and Xin Tong. DiLightNet: Fine-grained lightIn ACM ing control for diffusion-based image generation. SIGGRAPH Conf., 2024. 2, 5, 6, 7 [72] Zheng Zeng, Valentin Deschaintre, Iliyan Georgiev, Yannick Hold-Geoffroy, Yiwei Hu, Fujun Luan, Ling-Qi Yan, and Miloˇs Haˇsan. RGB X: Image decomposition and synthesis using material-and lighting-aware diffusion models. In ACM SIGGRAPH Conf., 2024. 2, 3, 4 [73] Lvmin Zhang, Anyi Rao, and Maneesh Agrawala. Ic-light github page, 2024. 2, 5, 6, 7 [74] Shuyang Zhang, Runze Liang, and Miao Wang. Shadowgan: Shadow synthesis for virtual objects with conditional adversarial networks. Comp. Vis. Media, 5:105115, 2019."
        }
    ],
    "affiliations": [
        "Depix Technologies",
        "Toyota Technological Institute at Chicago",
        "Universite Laval"
    ]
}