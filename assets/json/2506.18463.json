{
    "paper_title": "DIP: Unsupervised Dense In-Context Post-training of Visual Representations",
    "authors": [
        "Sophia Sirko-Galouchenko",
        "Spyros Gidaris",
        "Antonin Vobecky",
        "Andrei Bursuc",
        "Nicolas Thome"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "We introduce DIP, a novel unsupervised post-training method designed to enhance dense image representations in large-scale pretrained vision encoders for in-context scene understanding. Unlike prior approaches that rely on complex self-distillation architectures, our method trains the vision encoder using pseudo-tasks that explicitly simulate downstream in-context scenarios, inspired by meta-learning principles. To enable post-training on unlabeled data, we propose an automatic mechanism for generating in-context tasks that combines a pretrained diffusion model and the vision encoder itself. DIP is simple, unsupervised, and computationally efficient, requiring less than 9 hours on a single A100 GPU. By learning dense representations through pseudo in-context tasks, it achieves strong performance across a wide variety of downstream real-world in-context scene understanding tasks. It outperforms both the initial vision encoder and prior methods, offering a practical and effective solution for improving dense representations. Code available here: https://github.com/sirkosophia/DIP"
        },
        {
            "title": "Start",
            "content": "5 2 0 2 3 2 ] . [ 1 3 6 4 8 1 . 6 0 5 2 : r DIP: Unsupervised Dense In-Context Post-training of Visual Representations Sophia Sirko-Galouchenko1,2 Spyros Gidaris1 Antonin Vobecky1,3,4 Andrei Bursuc1 Nicolas Thome2 1Valeo.ai 2Sorbonne Universite, CNRS, ISIR, F-75005 Paris, France 3FEE CTU 4CIIRC CTU Prague (a) Query dog cat (b) Ground Truth sheep horse (c) DIP output cow aeroplane car (d) DIP 1st NN bottle bus (e) Dv2R output (f) Dv2R 1st NN diningtable motorbike train Figure 1. Dense retrieval-based semantic segmentation in low-shot regimes (40 examples) using patch feature similarities. For each query image (a), we show the ground truth (b), our DIP models prediction (c), and its retrieved neighbor (d) for the patch marked with red cross in (a). In (e) and (f), we display DINOv2Rs (Dv2R) output and its nearest neighbor, respectively. Our DIP representations retrieve more coherent neighbors, yielding more accurate segmentations than DINOv2R."
        },
        {
            "title": "Abstract",
            "content": "tions. Code available here We introduce DIP, novel unsupervised post-training method designed to enhance dense image representations in large-scale pretrained vision encoders for in-context scene understanding. Unlike prior approaches that rely on complex self-distillation architectures, our method trains the vision encoder using pseudo-tasks that explicitly simulate downstream in-context scenarios, inspired by metalearning principles. To enable post-training on unlabeled data, we propose an automatic mechanism for generating in-context tasks that combines pretrained diffusion model and the vision encoder itself. DIP is simple, unsupervised, and computationally efficient, requiring less than 9 hours on single A100 GPU. By learning dense representations through pseudo in-context tasks, it achieves strong performance across wide variety of downstream real-world incontext scene understanding tasks. It outperforms both the initial vision encoder and prior methods, offering practical and effective solution for improving dense representa1. Introduction Our goal is to learn dense image representations from unlabeled data for effective in-context scene understanding. In-context learning, which enables models to adapt to tasks without updating parameters, has seen remarkable success in large language models (LLMs) [7]. By providing few examples within the input prompt, LLMs generalize effectively to new tasks. Inspired by this, recent efforts aim to bring similar in-context learning capabilities to vision-only models [3, 5]. notable approach by Balazevic et al. [3] reformulates dense prediction tasks as nearest-neighbor retrieval problems using patch feature similarities, demonstrating strong performance, especially with limited data. However, while recent self-supervised Vision Transformers (ViTs) [3, 56] show promise for in-context scene understanding, they still fall short of matching LLMs incontext learning success. To bridge this gap, it is crucial to learn dense features that establish strong semantic correspondences between patches in test and training images. key insight from prior work [58, 64, 83] is that leveraging pretrained Vision Foundation Models (VFMs), such as DINOv2R [16, 56], and specializing them through unsupervised post-training is more computationally efficient and effective than training from scratch. This aligns with trends in LLMs, where large-scale pretraining is followed by cheaper post-training stages to specialize in specific skills of interest. We refer to the pretrained VFM undergoing posttraining as the base model. Most existing methods, whether training from scratch or post-training, rely on self-distillation, where the teacher is an exponential moving average of the student weights. These frameworks use architectures and objectives designed to promote dense nearest-neighbor retrieval skills. For instance, state-of-the-art methods enforce consistency between objects across images and views [46] or between patch rankings from two random views of the same image [58]. However, self-distillation often suffers from poor stability [82] and hyperparameter sensitivity [2]. Moreover, these approaches introduce additional complexity, such as differentiable sorting [58], RoI alignment units [29, 58, 83], Sinkhorn-Knopp optimization [64, 83], or learnable patch pooling [3, 46]. While improving performance, these components make the methods harder to interpret and tune, and less transparent in terms of their underlying mechanisms. In this work, we adopt post-training approach due to its computational efficiency and effectiveness. Within this context, we address two key questions: (1) Are there simpler and more effective alternatives to existing self-distillation frameworks with complex architectures and objectives?, and (2) Beyond the base model, can we leverage other vision foundation models to achieve our goal? We propose simpler post-training approach with an unsupervised learning objective explicitly designed for incontext scene understanding. Inspired by meta-learning, which learns general knowledge from diverse tasks to address unseen but related tasks, we automatically construct multiple in-context tasks from unlabeled data and metatrain the model on them. This enables the model to acquire transferable dense representations for efficiently solving new in-context tasks during downstream stages. Unlike prior methods, our approach eliminates complex training objectives and architectures, avoiding reliance on selfdistillation frameworks. To achieve unsupervised post-training, we generate incontext pseudo-tasks that mimic the tasks encountered during downstream use. Each pseudo-task consists of support examples with segmentation masks and query image to be segmented based on the support examples. The support set includes positive example (sharing objects with the query) and distractor examples (likely unrelated). Our pseudotasks are analogous to episodes from meta-learning literature [75] with the difference that we do not use any manual annotations during the post-training stage. With each episode, the model is essentially rehearsing for the downstream in-context segmentation task. We leverage pretrained VFMs to construct these pseudotasks. Specifically, apart from the base model (i.e., DINOv2R) that we post-train, we also use an auxiliary pretrained generative model, namely Stable Diffusion (SD) [27]. SD provides high-fidelity, class-agnostic image segments in an unsupervised manner, while the base model retrieves candidate positive examples for each query image and assigns pseudo-labels to the extracted segments. Although SD was trained on weakly annotated (internetscraped) image-caption pairs, we consider our post-training method as unsupervised, as neither it nor the used pretrained models rely on dense segmentation labels. The main contributions of this paper are as follows: We introduce DIP, novel unsupervised post-training method that uses retrieval-based in-context learning to improve dense image representations. Unlike prior work, DIP is based on meta-training principles and eliminates the need for complex self-distillation architectures. Furthermore, DIP is computationally efficient, requiring less than 9 hours on single A100 GPU for post-training. To enable post-training on unlabeled data, we propose an automatic mechanism for generating in-context tasks. In addition to the base model, this approach leverages the Stable Diffusion model (through the unsupervised, training-free DiffCut [15] technique) to produce highquality pseudo-segmentations from unlabeled images. Experiments show that the learned dense representations generalize effectively to wide variety of downstream tasks. These include six semantic segmentation datasets and one monocular depth prediction dataset, even in lowshot scenarios. Furthermore, we extend our approach to other VFMs, such as CLIP [61], and show that it consistently enhances their dense representations. Compared to DINOv2R, DIP produces more semantically coherent neighbors, leading to more accurate retrieval-based scene understanding, particularly in lowshot settings (see Fig. 1). Additionally, DIP outperforms recent state-of-the-art post-training methods in most retrieval-based tasks while being significantly simpler. 2. Related Work Self-supervised learning. Self-supervised approaches can learn rich representations from vast amounts of raw data via pretext tasks that provide supervision signals from the data itself. The model is subsequently fine-tuned on downstream task of interest with few annotated examples. Self-supervision is commonly applied by hiding or transforming the input data and training the model to predict the missing parts or correct them. We distinguish several broad categories of self-supervised objectives for images: contrastive objectives for contrasting representations between visually similar and dissimilar views [11, 30, 54], clustering-based objectives [1, 8, 23], self-distillation objectives for predicting similar representations for different views of the same image [6, 9, 12, 24, 26], reconstruction objectives for predicting pixel colors [31, 79] or features for masked image patches produced by teacher network [2, 25, 39, 55, 82]. Recent self-supervised approaches make use of pretrained VFMs for distillation in cooperation with the self-supervised objectives [4, 17, 36, 59]. These methods focus on learning global image representations that are tailored to classification tasks. However, without fullfinetuning or task decoders, they are less suitable for sceneunderstanding tasks relying on dense representations, such as semantic segmentation. We build DIP on contrastive objectives [54] equipped with memory bank [30]. Dense self-supervision. To learn localized image representations that can be rapidly adapted to scene understanding tasks, specific self-supervised objectives are required to mimic the downstream task [3, 10, 33, 34, 46, 68, 71, 77, 78]. Contrasting features from dissimilar individual pixels or pseudo-segments from the same image can significantly improve object detection performance [33, 34, 53, 77, 78]. Another line of approaches propose different clustering strategies to produce object-aware supervision signals to improve semantic segmentation performance [10, 71, 83]. Others extend patch-level contrastive learning across similar and dissimilar images [3] enforcing nearest-neighbor consistency [28, 46, 58]. While we have similar objectives as [3, 58] attain strong in-context scene understanding skills, we devise different self-supervised objective inspired from few-shot learning that makes use of automatically computed object segment pseudo-labels. Recent works build upon pretrained VFMs [28, 41, 58, 69, 83] casting their self-supervised training as posttraining stage for preparing the VFM for dense downstream tasks. Similarly, DIP aims to rapidly endow pretrained VFMs with in-context reasoning skills. Unsupervised semantic segmentation. These works aim for producing object segments without supervision via selfsupervised training. Earlier works in unsupervised segmentation focused mostly on different forms of clustering objectives, with segmentations refined through consistency across augmented views of the same image [13, 37, 38, 40, 76]. Another line of works explores pre-trained features from self-supervised VFMs [28, 45, 47, 66, 76] or from diffusion UNet encoder [15, 73]. The recent DiffCut [15] leverages recursive Normalized Cuts in the final attention layer of pretrained diffusion UNet encoder to produce object segments without supervision. Here we make use of DiffCut to extract pseudo-segmentation maps from the dataset images to be used for our in-context post-training. Although DiffCut segments can be noisy and imperfect, they can be good proxies for actual object segments for our post-training strategy. Few-shot learning. The aim of few-shot learning is to devise models that can rapidly acquire new skills only from limited set of labeled examples. To that end, few-shot learning methods are trained on distribution of few-shot tasks (also called episodes) sampled from the training dataset, that are similar to the few-shot tasks encountered at test time. There are numerous few-shot learning strategies, among which the most common ones are the methods that learn to match and map query example to class label by accessing memory bank storing the few labeled examples for that task [50, 65, 75], metric-learning based approaches that learn distance metrics from few samples [70, 72], gradient descent-based approaches that learn how to adapt model quickly to given few-shot task via limited number of gradient descent updates [20, 62], and generative methods that predict the weights of classifier [21, 22, 60]. Inspired by Vinyals et al. [75] we devise self-supervised pretext objective consisting of multiple in-context scene understanding tasks, each with set of support samples stored in memory and query image to be segmented based on the support samples. In contrast to few-shot learning, we do not use any human annotation during the post-training stage and instead generate the segmentation labels automatically. 3. Method Our goal is to fine-tune pretrained vision encoder, typically ViT [18], to produce dense features suitable for in-context dense prediction tasks framed as nearest-neighbor retrieval. Given set of support images (i.e., training images) with semantic segmentation annotations and query image (i.e., test image), we aim to ensure that patch-wise features extracted from the query image are highly similar (e.g., high cosine similarity) to those from support patches depicting the same object category, while being dissimilar to features of different objects. To achieve this, we propose an unsupervised posttraining approach  (Fig. 2)  that fine-tunes the encoder on dense nearest-neighbor retrieval tasks automatically generated from unlabeled data. By training on pseudo-tasks simulating real-world scenarios, the encoder learns robust features transferable to real in-context tasks. In the following sections, we first detail our dense incontext post-training approach Sec. 3.1 and then describe the automatic process for constructing dense in-context Figure 2. Our unsupervised Dense In-context Post-training (DIP) method. During post-training, the model is given pseudo incontext task, created automatically without human input. Each task includes query image and pseudo-labeled support set with positive example (sharing objects with the query, as shown in the zebra example) and distractor examples that contain different object categories. The model predicts the pseudo-labeled semantic segmentation of the query image using the support set as reference. To do this, it (1) extracts patch-wise features from both the query and support images using the vision encoder () and projection head h(); (2) computes segmentation predictions for the query image through cross-attention (query patch features as queries, support patch features as keys, and pseudo-labeled support patches as values). pixel-wise cross-entropy loss is applied to the predictions. By training on these pseudo tasks, DIP enables the encoder to learn transferable dense representations, which are later used to efficiently solve new real in-context tasks. tasks Sec. 3.2, agnostic segments and the selection of support examples. including the pseudo-labeling of class3.1. Dense In-Context training During training, we sample an in-context task from dataset D. Each task consists of support examples = {(Xsi , Ysi )}K i=1 and one query example (Xq, Yq), where Xq and Xsi are images, and Yq and Ysi are their one-hot semantic segmentation pseudo-labels. Among the support examples, one is positive example sharing pseudo-classes with the query, while the remaining 1 are distractor examples unlikely to share any pseudoclass. Since our method is unsupervised, these tasks and pseudo-labels are automatically constructed (see Sec. 3.2). Let () be pretrained encoder Feature Extraction. that our method fine-tunes. Given an input image (either query or support), () produces patch-wise features (X) RLD, where is the number of patches and is the feature dimension. We further apply patch-wise multilayer perceptron (MLP) network h() to project the features into D-dimensional space: = h(f (X)) RLD . The MLP includes an ℓ2-normalization layer at the end. Let θ denote the parameters of (h )() model. Label Pre-processing. To align the label resolution with patch-wise features, we patchify the one-hot pseudo-labels Ysi. Specifically, we divide Ysi into patches matching the encoders patch size and average the one-hot labels within RLC, where is the number each patch, yielding si of pseudo-classes. In-Context Dense Predictions. Using patch-wise features and patchified labels from the support set S, we define soft nearest-neighbor classification function cS () to predict dense labels ˆYq = cS (Xq) for the query image Xq. First, we compute attention scores for each query patch over all support patches: = softmax (cid:18) Fq τ (cid:19) , (1) where FS R(LK)D concatenates patch-wise features Fsi from all support images, and τ is the softmax temperature. The softmax normalization is over the support patches. The attention weights [0, 1]L(LK) are used to compute weighted average of the patchified support labels: ˆY = S, (2) R(LK)C concatenates patchified labels where si from all support images, and ˆY RLC represents the predicted patchified labels for query. Essentially, this defines cross-attention layer [74], with Fq as queries, FS as keys, and as values. Finally, we use nearest-neighbor interpolation to upsamq to the original image size, yielding the final label ple ˆY prediction ˆYq = cS (Xq). = Training Objective. {S, (Xq, Yq)}, we minimize the pixel-wise cross-entropy loss LCE(Yq, cS (Xq), θ) between the predicted labels cS (Xq) and the pseudo-labels Yq. The model, comprising the pretrained encoder and randomly initialized MLP (h )() with parameters θ, is trained by optimizing the"
        },
        {
            "title": "For a single task T",
            "content": "image into non-overlapping class-agnostic segments and then assigning pseudo-label to each segment. For the first step, we use DiffCut [15], training-free zero-shot image segmentation method. DiffCut leverages features from pretrained diffusion model, SSD-1B [27], along with recursive graph partitioning algorithm to produce fine-grained segmentation maps. These maps, called DiffCut masks, are generated and stored for the entire pretraining dataset. Since DiffCut is unsupervised, our pretraining approach remains fully annotation-free. For the second step, we assign pseudo-labels to each segment using the self-supervised DINOv2R feature encoder. We compute the mean DINOv2R feature for each DiffCut mask by pooling dense features within the mask region. To group visually similar segments, we apply K-means clustering on these pooled features1. The resulting clusters serve as pseudo-classes for annotating the DiffCut masks. To assign pseudo-class to each DiffCut mask, we first assign each pixel-wise DINOv2R feature the cluster ID of its closest K-means centroid. Then, for each DiffCut mask, we perform majority voting on all pixels within the mask and assign the most frequent cluster ID. This ensures robust and consistent pseudo-labels, even with noisy features. Selecting Examples for In-Context Task. To generate incontext tasks, we pair each query image with one positive support image and K-1 distractor support images. To find positive images sharing similar visual content with query, we use global image representations from the DINOv2R encoder. For each image, we retrieve its five nearest neighbors from the dataset using DINOv2R global features. This results in five positive pairs per dataset image. We then refine this list by retaining only pairs where the query and positive-support images share common pseudoclass occupying more than 5% of each images area. This ensures semantically meaningful and relevant positive pairs. The final refined list contains all possible query-positive support pairs for training. For the K-1 distractor examples, we randomly sample images from the current mini-batch during training. Including distractors encourages the model to distinguish relevant from irrelevant content, improving feature discriminability. 4. Experiments 4.1. Experimental setup Compared methods. We compare our method with state-of-the-art unsupervised learning approaches, includiBOT [82], ing DINO [9], Leopart [83], TimeT [64], CrOC [71], CrIBO [46], DINOv2R [16, 56], and NeCo [58]. NeCo, recent method (to appear at ICLR25), 1For COCO [48], clustering is performed on the full dataset; for ImageNet [44], clustering is performed on random subset of 200,000 training images for efficiency. (a) Query (b)Pseudo labels (c) Positive (d) Pseudo labels Figure 3. Examples of automatically constructed in-context scene understanding tasks. Each row shows query image and its corresponding positive support example. (a) and (b) display the query image and its pseudo segmentation labels, while (c) and (d) show the positive support image and its pseudo segmentation labels. Despite being generated in fully unsupervised manner, the segmentation masks for salient objects are highly accurate, closely matching the actual objects. In addition, the query and positive image pairs share common object with the same pseudo-label. During post-training with our DIP method, the model predicts the query images segmentation using the positive example as reference, along with distractor support examples (randomly sampled from other images in the mini-batch, not shown here for brevity). expected loss over collection of tasks sampled from D: min θ E{S,(Xq,Yq)}D [LCE(Yq, cS (Xq), θ)] . (3) Downstream stage. For downstream in-context scene understanding tasks, we remove the head h() and use the fine-tuned encoder (). Following [3, 57, 58], we construct larger memory bank (support set) than used during post-training, with 10,240,000 patch-wise features randomly sampled from the available training images for each dataset. For query image, we extract its patch-wise features using (), retrieve the top-30 nearest neighbors from the memory bank via cosine similarity, and apply crossattention (as in Eqs. (1) and (2)) with temperature τ = 0.07. 3.2. Automatic Dense In-Context Task Construction To apply our dense in-context post-training approach to unlabeled data, we automatically generate in-context pseudotasks with spatially coherent segmentation labels. This involves two steps: (1) generating pseudo-segmentation maps for dataset images and (2) selecting query and support examples to form each in-context task. These steps create meaningful and challenging tasks, enabling the model to learn transferable in-context visual learning skills. Examples of constructed pseudo-tasks are shown in Fig. 3. Generating Pseudo Semantic Segmentation Labels. We generate pseudo-segmentation maps by first dividing each"
        },
        {
            "title": "Backbone",
            "content": "DINO [9] SelfPatch [80] CroC [71] TimeT [64] Leopart [83] CriBO [46] DINOv2R [16] NeCo [58] DIP (ours) ViT-S/16 ViT-S/16 ViT-S/16 ViT-S/16 ViT-S/16 ViT-S/16 ViT-S/14 ViT-S/14 ViT-S/14 DINO [9] ViT-B/16 Leopart [83] ViT-B/16 Hummingbird [3] ViT-B/16 CriBO [46] ViT-B/16 DINOv2R [16] ViT-B/14 NeCo [58] ViT-B/14 DIP (ours) ViT-B/14 1 1 48.7 50.8 60.5 62.3 64.5 72.4 79.4 81.0 81.0 57.3 69.5 71.8 74.2 79.0 82.4 82."
        },
        {
            "title": "PascalVOC",
            "content": "1 8 41.3 43.2 53.8 55.2 58.4 66.9 75.2 77.3 77.7 49.8 63.1 64.3 69.2 75.3 78.7 79.6 1 64 30.5 32.6 41.8 43.8 49.7 59.9 67.7 71.5 71.4 37.7 54.7 57.2 61.8 67.6 71.7 75. 1 128 26.4 28.4 34.0 38.1 44.6 53.9 60.7 65.8 65.9 33.1 50.1 50.5 55.9 60.3 65.0 70.1 1 1 17.9 17.7 17.3 23.2 23.9 26.6 39.3 38.9 39.7 21.5 26.7 29.6 28.4 40.8 41.2 42. ADE20K 1 1 64 8 15.0 14.7 15.2 18.9 19.6 22.7 33.2 32.5 33.7 18.2 21.8 22.3 24.4 35.3 35.2 36.8 11.0 10.9 10.8 14.1 14.8 17.3 24.9 24.1 25.6 13.5 16.8 15.1 18.4 27.3 27.2 29.4 1 9.5 10.0 8.7 12.1 12.9 14.6 22.6 21.9 23.2 11.5 14.6 11.7 15.9 24.9 25.1 27.0 Table 1. In-context scene understanding with few training examples. Dense nearest neighbor retrieval performance on ADE20K and PascalVOC datasets, evaluated using the mIoU metric across varying proportions of training data. The fractions below the dataset names indicate the proportion of training data used. Results marked with are from NeCo [58], while those marked with are our own. DINOv2R NeCo DIP (ours) ) % ( m 80 60 1 128 1 64 1 8 Fraction of Data (a) PascalVOC 35 30 25 1 1 1 128 1 1 8 Fraction of Data (b) ADE20K 1 1 Figure 4. In-context scene understanding in low-shot regimes. mIoU results with ViT-B/14 versus training data size. is the most related to ours, as it also post-trains DINOv2R for dense representation learning in in-context scene understanding tasks. However, our post-training methodology differs significantly: we explicitly train on in-context pseudo-tasks relevant to our target applications, while NeCo adopts self-distillation-based approach. Training setup. We apply our unsupervised post-training method to DINOv2 [56] with registers [16] (DINOv2R), using ViT-S/14 and ViT-B/14 models. For each model, in-context pseudo-tasks are automatically generated using DiffCut along with the corresponding base model (e.g., DINOv2R ViT-B/14 is used to generate pseudo-tasks for posttraining DINOv2R ViT-B/14; see Sec. 3.2). Except otherwise stated, post-training is conducted on the COCO dataset for 5 epochs with 1,000 pseudo-classes. We also explore post-training CLIP and MAE models with ViT-B/16 (in this case using pseudo-labels generated with DiffCut and DINOv2R ViT-B/14), as well as post-training on ImageNet. Our post-training procedure is computationally efficient, requiring 17 hours and 52 minutes on single V100 GPU with ViT-B, and 8 hours and 37 minutes on single A100 GPU with ViT-S. Pseudo-label generation for COCO, which is done only once and offline, requires 17 hours on V100 GPU. More implementation details in Appendix A. Evaluation setup. To assess the generality of our learned representations, we evaluate them on diverse downstream tasks and datasets. These include semantic segmentation on PascalVOC [19], Pascal-Context [51], ADE20K [81], Cityscapes [14], and COCO [48] (using mIoU), and monocular depth prediction on NYUv2 [52] (using RMSE). COCO serves as the in-domain dataset, as its training data is used for post-training, while the others are out-of-domain. These out-of-domain datasets test the generalization of our representations to domain shifts between post-training and downstream task data and tasks. We also evaluate robustness to intra-task domain shifts using the CSACDC [63] setting, where Cityscapes (CS) provides the support set and ACDC provides the query images for retrieval-based segmentation. While both datasets contain autonomous driving images, ACDC introduces challenging conditions like snow, night, fog, and rain, unlike Cityscapes clear-weather daylight images. All results reported in this paper are produced using our implementation unless marked with . For DINOv2R and NeCo, we evaluate the publicly available pretrained model checkpoints to ensure fair comparison. Method Backbone ADE20K PascalVOC Pascal-Context Cityscapes CSACDC COCO Avg. Delta DINOv2R [16] ViT-S/14 ViT-S/14 NeCo [58] DIP (ours) ViT-S/14 DINOv2R [16] ViT-B/14 ViT-B/14 NeCo [58] DIP (ours) ViT-B/14 39.3 38.9 (0.4) 39.7 (+0.4) 40.8 41.2 (+0.4) 42.6 (+1.8) 79.4 81.0 (+1.6) 81.0 (+1.6) 79.0 82.4 (+2.4) 82.1 (+2.1) 48.0 49.4 (+1.4) 49.5 (+1.5) 49.0 51.2 (+2.2) 51.5 (+2.5) 55.6 53.9 (1.7) 55.8 (+0.2) 58.4 57.9 (0.5) 59.5 (+1.1) 47.4 47.2 (0.2) 47.4 (+0.0) 50.5 51.5 (+1.0) 52.1 (+1.6) 72.6 74.3 (+1.7) 74.0 (+1.4) 72.9 75.4 (+2.5) 76.0 (+3.1) +0.40 +0. +1.50 +2.00 Table 2. In-context scene understanding benchmark. Dense nearest neighbor retrieval performance for semantic segmentation on six scene-centric datasets: ADE20K, PascalVOC, Pascal-Context, Cityscapes, CSACDC, and COCO. The first five are out-of-domain, while COCO is in-domain (used for post-training). Performance is measured using the mIoU metric. For NeCo and our DIP, which post-train DINOv2R, improvements over DINOv2R are shown in parentheses. The Avg. Delta column reports the average improvement. Method DINOv2R NeCo DIP (ours) Method Backbone PascalVOC ADE20K RMSE .771 . .756 In-context monocular depth prediction on NYUv2 Table 3. dataset [67]. RMSE scores (lower is better) scaled by 10 for readability reasons. All methods use ViT-S/14. 4.2. Retrieval-based scene understanding Comparison with state-of-the-art in low-shot regimes. In Tab. 1, we compare our DIP approach with prior stateof-the-art methods for dense image representation learning on retrieval-based semantic segmentation using the Pascal VOC and ADE20K datasets. We evaluate both full-data ( 1 1 ) 8 , 1 and low-shot regimes ( 1 128 ), where only fraction of the training examples is used in the support set. 64 , and 1 Our DIP approach consistently outperforms DINOv2R, with the performance gap widening as the number of training examples decreases, particularly for the PascalVOC dataset (see Fig. 4). Notably, DIP achieves superior results compared to prior work in most settings for both ViT-S and ViT-B, while remaining competitive in others. Comparison with DINOv2R and NeCo. We compare our DIP method with DINOv2R, the base model for our post-training, and NeCo, recent method that also posttrains DINOv2R. In Tab. 2 we evaluate these methods on in-context semantic segmentation tasks across six datasets. Our approach demonstrates consistent performance improvements over DINOv2R on all datasets, both in-domain (COCO) and out-of-domain (ADE20K, PascalVOC, PascalContext, and Cityscapes), highlighting the generalization capability of our learned representations. Compared to NeCo, our DIP consistently improves over DINOv2R, which is not the case for NeCo, achieving higher mIoU improvements in most cases and higher average performance improvement (see Avg. Delta in Tab. 2). In Tab. 2, the CSACDC setIntra-task domain shift. ting evaluates the robustness of the learned representations to domain shifts between the support set and query images during the downstream stage, as explained in Sec. 4.1. Although our post-training DIP method is not specifically de40.8 42.6 (+1.8) 29.0 30.1 (+1.1) DINOv2R [16] ViT-B/14 +DIP ViT-B/14 79.0 82.1 (+2.1) CLIP [61] + DIP MAE [31] + DIP ViT-B/16 ViT-B/16 ViT-B/16 ViT-B/16 71.8 73.8 (+2.0) 13.9 47.3 (+33.4) 5.1 11.8 (+6.7) Table 4. Post-training with other base models. Dense nearest neighbor retrieval performance (mIoU) for semantic segmentation on ADE20K and PascalVOC, using DINOv2R, CLIP, and MAE as base models, before and after post-training. signed to improve this type of robustness, it still achieves 1.6 mIoU improvement over DINOv2R for ViT-B. Monocular depth prediction. In Tab. 3, we report results for in-context monocular depth prediction on NYUv2, measured by RMSE. Both DIP and NeCo improve over DINOv2R, with our method achieving the best RMSE score. This is notable, as our in-context pseudo-tasks are designed for semantic segmentation, yet the learned representations generalize effectively to the distinct depth prediction task. Post-training other base models. In Tab. 4, we evaluate in-context segmentation on ADE20K and PascalVOC using CLIP [61] and MAE [31] as base models, both before and after post-training with DIP. Our method consistently improves performance across all tested models, demonstrating its versatility. Notably, the improvements for MAE are of +33.4 mIoU points on PascalVOC. These substantial gains transform MAE from model poorly suited for in-context segmentation into one with significantly improved capabilities, highlighting the effectiveness of our approach for enhancing dense feature representations."
        },
        {
            "title": "Comparison with supervised baselines",
            "content": "To automatically generate pseudo-segmentations, our post-training approach leverages Stable Diffusion (SD) features [27], trained on weakly annotated (internet-scraped) image-caption pairs. In Tab. 5, we compare DIP features against both these SD features and those from supervised encoders (SAM [43] and RADIOv2.5 [32]). Crucially,"
        },
        {
            "title": "Backbone",
            "content": "PascalVOC ADE20K"
        },
        {
            "title": "Method",
            "content": "SD [27] SSD-1B ViT-B/16 SAM [43] DINOv2R [16] ViT-B/16 RADIOv2.5 [32] ViT-B/16 DIP (ours) ViT-B/16 59.4 32.8 79.0 81.3 82.1 24. 12.9 40.8 42.1 42.6 Table 5. Comparison with Superised Baselines. Dense nearest neighbor retrieval performance (mIoU) for semantic segmentation on PascalVOC and ADE20K. SAM requires stronger supervision (manual segmentation masks) than SD, while RADIOv2.5 distills multiple model types (SAM, DINOv2, and CLIP-like features). Thus, both baselines leverage more supervision than our method. Additionally, RADIOv2.5 requires significantly more training compute. Despite these advantages, Tab. 5 demonstrates that DIP surpasses RADIOv2.5 in in-context semantic segmentation, confirming our methods effectiveness. SAM underperforms in this setting, consistent with prior findImportantly, DIP achieves significantly better ings [42]. performance than the SD features it utilizes for pseudosegmentation. Additionally, we provide linear segmentation results demonstrating consistent improvement over DINOv2R and competitive results compared to NeCo in Tab. 8 in supplementary. 4.3. Ablations In Tab. 6, we evaluate the impact of key design choices in our DIP approach. In-context vs Direct dense prediction. In Tab. 6a, we compare our in-context dense prediction objective with direct dense prediction, where the model predicts pseudo semantic segmentation maps using classification head. Our in-context approach significantly outperforms the direct method on PascalVOC and reduces RMSE on NYUv2 depth prediction, while both perform similarly on ADE20K. Impact of distractor examples (Tab. 6b). Removing the distractor support examples significantly reduces performance, as the absence of distractors simplifies the post-training task, limiting the models ability to learn discriminative dense representations. Construction of positive examples. In Tab. 6c, we compare two strategies for creating positive examples: (1) retrieving nearest neighbors using DINOv2R image-wise features and (2) using two random crops from the same image. The nearest neighbor strategy outperforms random crops, validating our design choice and showing that our method avoids reliance on complex augmentations common in selfdistillation and contrastive approaches. (a) Query (b) Reference (c) DINOv2R (d) DIP Figure 5. Correlation maps between query image patch (highlighted with red cross) and reference image, comparing DINOv2R and DIP. DIP generates coherent, object-level correlations, while DINOv2R produces localized, part-level responses. Impact of DiffCut. In Tab. 6d, we assess the role of DiffCut [15], which generates class-agnostic segments for pseudo segmentation maps before K-means clustering. Compared to applying K-means directly on DINOv2R dense features, DiffCut significantly improves performance, demonstrating its effectiveness as training-free, unsupervised method leveraging Stable Diffusion. Furthermore, our pseudo labels achieve results nearly matching ground-truth semantic segmentation, underscoring their quality. Impact of number of pseudo classes. In Tab. 6e, we study the effect of the number of pseudo-classes (K-means clusters) used to generate pseudo-segmentation labels. Our approach shows robustness to this hyperparameter, with performance remaining stable across different values. Post-training data: scene-centric vs object-centric. In Tab. 6f, we compare DIP post-trained on the scenecentric COCO dataset versus the object-centric ImageNet dataset. Results are comparable, with COCO slightly outperforming ImageNet. This indicates that our method does not depend on human-curated, object-centric data and performs better with scene-centric data, which is easier to collect at scale. 4.4. Qualitative results In Fig. 5, we present correlation maps between query image patch and reference image, comparing dense representations from DINOv2R and our DIP. While DINOv2R produces localized, part-based correlations, DIP captures semantic-level correspondences, more accurately delineating entire objects of the same semantic type as the query. This shows that DIP provides better semantic correspondences. As visualized in Fig. 1, this enables DIP to retrieve more semantically coherent nearest neighbors, particularly Objective PascalVOC ADE20K NYUv2 Distractor PascalVOC ADE20K Positive example PascalVOC ADE20K In-context Direct. 81.0 79.9 39.7 39.9 .756 .776 81.0 78.5 39.7 38.4 Nearest neighbor Two random crops 81.0 79.5 39.7 39.4 (a) In-context pretraining is more effective. (b) Distractor examples are important. (c) Constructing positive examples. Segm. Labels PascalVOC ADE20K Pseudo-labels w/ DiffCut Pseudo-labels w/o DiffCut Ground truth labels 81.0 59.1 81.9 39.7 25.5 39.8 #Pseudo-classes PascalVOC ADE20K 300 500 1000 80.8 80.9 81.0 80.8 39.5 39.7 39.7 39.8 Dataset PascalVOC ADE20K COCO ImageNet 81.0 80. 39.7 39.6 (d) Pseudo-labels generation. (e) Number of pseudo-classes. (f) Post-training dataset. Table 6. Ablation Study of DIP. Default settings in light blue . Two random crops in (c) refers to the case where the query and positive image are constructed as two random crops of the same original image. in low-shot regimes (as in Fig. 1), resulting in more accurate segmentation outputs compared to DINOv2R. 5. Conclusion We introduced DIP, novel unsupervised post-training method that enhances dense image representations for incontext scene understanding. Leveraging meta-learning principles and automatically generating in-context tasks with pseudo-segmentations using Stable Diffusion, our approach avoids complex self-distillation architectures. DIP is computationally efficient (<9 hours on an A100 GPU) and generalizes well across downstream dense retrieval tasks, including semantic segmentation and depth prediction. It outperforms both the initial pretrained vision encoder and prior state-of-the-art post-training methods, providing simpler and more effective solution for improving dense representations in vision models. Acknowledgements This work was performed using HPC resources (Grants 2024-AD011012884R3, 2025-AD011015037R1, We thank PEPR Sharp and 2025-AD011016523). 2030). (ANR-23-PEIA-0008, from GENCIIDRIS"
        },
        {
            "title": "FRANCE",
            "content": "ANR,"
        },
        {
            "title": "References",
            "content": "[1] Yuki Markus Asano, Christian Rupprecht, and Andrea Vedaldi. Self-labelling via simultaneous clustering and representation learning. In ICLR, 2020. 3 [2] Mahmoud Assran, Mathilde Caron, Ishan Misra, Piotr Bojanowski, Florian Bordes, Pascal Vincent, Armand Joulin, Mike Rabbat, and Nicolas Ballas. Masked siamese networks for label-efficient learning. In ECCV, 2022. 2, 3 [3] Ivana Balaˇzevic, David Steiner, Nikhil Parthasarathy, Relja Arandjelovic, and Olivier J. Henaff. Towards in-context scene understanding. In NeurIPS, 2023. 1, 2, 3, 5, 6, 13 [4] Hangbo Bao, Li Dong, Songhao Piao, and Furu Wei. Beit: Bert pre-training of image transformers. In ICLR, 2022. 3 [5] Amir Bar, Yossi Gandelsman, Trevor Darrell, Amir Globerson, and Alexei A. Efros. Visual prompting via image inpainting, 2022. 1 [6] Adrien Bardes, Vicreg: Variance-invariance-covariance regularization for selfsupervised learning. In ICLR, 2022. 3 Jean Ponce, and Yann LeCun. [7] Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. Language models are few-shot learners. In NeurIPS, 2020. 1 [8] Mathilde Caron, Ishan Misra, Julien Mairal, Priya Goyal, Piotr Bojanowski, and Armand Joulin. Unsupervised learning of visual features by contrasting cluster assignments. In NeurIPS, 2020. 3 [9] Mathilde Caron, Hugo Touvron, Ishan Misra, Herve Jegou, Julien Mairal, Piotr Bojanowski, and Armand Joulin. Emerging properties in self-supervised vision transformers. In ICCV, 2021. 3, 5, 6 [10] Mathilde Caron, Neil Houlsby, and Cordelia Schmid. Location-aware self-supervised transformers for semantic segmentation. In WACV, 2024. 3 [11] Ting Chen, Simon Kornblith, Mohammad Norouzi, and Geoffrey Hinton. simple framework for contrastive learning of visual representations. In ICML, 2020. 3 [12] Xinlei Chen and Kaiming He. Exploring simple siamese representation learning. In CVPR, 2021. [13] Jang Hyun Cho, Utkarsh Mall, Kavita Bala, and Bharath Hariharan. Picie: Unsupervised semantic segmentation usIn CVPR, ing invariance and equivariance in clustering. 2021. 3 [14] Marius Cordts, Mohamed Omran, Sebastian Ramos, Timo Rehfeld, Markus Enzweiler, Rodrigo Benenson, Uwe Franke, Stefan Roth, and Bernt Schiele. The cityscapes dataset for semantic urban scene understanding. In CVPR, 2016. 6 [15] Paul Couairon, Mustafa Shukor, Jean-Emmanuel Haugeard, Matthieu Cord, and Nicolas Thome. Diffcut: Catalyzing zero-shot semantic segmentation with diffusion features and recursive normalized cut. In NeurIPS, 2024. 2, 3, 5, 8 [16] Timothee Darcet, Maxime Oquab, Julien Mairal, and Piotr Bojanowski. Vision transformers need registers. In ICLR, 2024. 2, 5, 6, 7, 8 [17] Xiaoyi Dong, Jianmin Bao, Ting Zhang, Dongdong Chen, Weiming Zhang, Lu Yuan, Dong Chen, Fang Wen, Nenghai Yu, and Baining Guo. Peco: Perceptual codebook for bert pre-training of vision transformers. In AAAI, 2023. 3 [18] Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, et al. An image is worth 16x16 words: Transformers for image recognition at scale. In ICLR, 2021. 3 [19] M. Everingham, L. Van Gool, C. K. I. Williams, J. Winn, and A. Zisserman. The pascal visual object classes (voc) challenge. IJCV, 2010. 6 [20] Chelsea Finn, Pieter Abbeel, and Sergey Levine. Modelagnostic meta-learning for fast adaptation of deep networks. In ICML, 2017. 3 [21] Spyros Gidaris and Nikos Komodakis. Dynamic few-shot visual learning without forgetting. In CVPR, 2018. 3 [22] Spyros Gidaris and Nikos Komodakis. Generating classification weights with gnn denoising autoencoders for few-shot learning. In CVPR, 2019. [23] Spyros Gidaris, Andrei Bursuc, Nikos Komodakis, Patrick Perez, and Matthieu Cord. Learning representations by predicting bags of visual words. In CVPR, 2020. 3 [24] Spyros Gidaris, Andrei Bursuc, Gilles Puy, Nikos Komodakis, Matthieu Cord, and Patrick Perez. Obow: Online bag-of-visual-words generation for self-supervised learning. In CVPR, 2021. 3 [25] Spyros Gidaris, Andrei Bursuc, Oriane Simeoni, Antonın Vobeck`y, Nikos Komodakis, Matthieu Cord, and Patrick Perez. Moca: Self-supervised representation learning by predicting masked online codebook assignments. Trans. Mach. Learn. Res., 2024. 3 [26] Jean-Bastien Grill, Florian Strub, Florent Altche, Corentin Tallec, Pierre Richemond, Elena Buchatskaya, Carl Doersch, Bernardo Avila Pires, Zhaohan Guo, Mohammad Gheshlaghi Azar, et al. Bootstrap your own latent-a new approach to self-supervised learning. In NeurIPS, 2020. 3 [27] Yatharth Gupta, Vishnu V. Jaddipal, Harish Prabhala, Sayak Paul, and Patrick Von Platen. Progressive knowledge distillation of stable diffusion xl using layer level loss, 2024. 2, 5, 7, 8 [28] Mark Hamilton, Zhoutong Zhang, Bharath Hariharan, Noah Snavely, and William T. Freeman. Unsupervised semantic segmentation by distilling feature correspondences. In ICLR, 2022. [29] Kaiming He, Georgia Gkioxari, Piotr Dollar, and Ross Girshick. Mask R-CNN. In ICCV, 2017. 2 [30] Kaiming He, Haoqi Fan, Yuxin Wu, Saining Xie, and Ross Girshick. Momentum contrast for unsupervised visual representation learning. In CVPR, 2020. 3 [31] Kaiming He, Xinlei Chen, Saining Xie, Yanghao Li, Piotr Dollar, and Ross Girshick. Masked autoencoders are scalable vision learners. In CVPR, 2022. 3, 7 [32] Greg Heinrich, Mike Ranzinger, Yin Hongxu, Yao Lu, Jan Kautz, Andrew Tao, Bryan Catanzaro, and Pavlo Molchanov. RADIOv2. 5: Improved baselines for agglomerative vision foundation models. In CVPR, 2025. 7, 8 [33] Olivier Henaff, Skanda Koppula, Jean-Baptiste Alayrac, Aaron Van den Oord, Oriol Vinyals, and Joao Carreira. Efficient visual pretraining with contrastive detection. In ICCV, 2021. [34] Olivier Henaff, Skanda Koppula, Evan Shelhamer, Daniel Zoran, Andrew Jaegle, Andrew Zisserman, Joao Carreira, and Relja Arandjelovic. Object discovery and representation networks. In ECCV, 2022. 3 [35] Dan Hendrycks and Kevin Gimpel. Gaussian error linear units (gelus), 2023. 13 [36] Zejiang Hou, Fei Sun, Yen-Kuang Chen, Yuan Xie, and SunYuan Kung. Milan: Masked image pretraining on language assisted representation. arXiv preprint arXiv:2208.06049, 2022. 3 [37] Jyh-Jing Hwang, Stella Yu, Jianbo Shi, Maxwell Collins, Tien-Ju Yang, Xiao Zhang, and Liang-Chieh Chen. Segsort: Segmentation by discriminative sorting of segments. In ICCV, 2019. 3 [38] Xu Ji, Joao Henriques, and Andrea Vedaldi. Invariant information clustering for unsupervised image classification and segmentation. In ICCV, 2019. 3 [39] Ioannis Kakogeorgiou, Spyros Gidaris, Bill Psomas, Yannis Avrithis, Andrei Bursuc, Konstantinos Karantzalos, and to hide from your students: Nikos Komodakis. What Attention-guided masked image modeling. In ECCV, 2022. 3 [40] Asako Kanezaki. Unsupervised image segmentation by backpropagation. In ICASSP, 2018. 3 [41] Efstathios Karypidis, Ioannis Kakogeorgiou, Spyros Gidaris, and Nikos Komodakis. Dino-foresight: Looking into the future with dino. arXiv preprint arXiv:2412.11673, 2024. 3 [42] Tommie Kerssies, Daan de Geus, and Gijs Dubbelman. How to benchmark vision foundation models for semantic segmentation? In CVPRw, 2024. [43] Alexander Kirillov, Eric Mintun, Nikhila Ravi, Hanzi Mao, Chloe Rolland, Laura Gustafson, Tete Xiao, Spencer Whitehead, Alexander C. Berg, Wan-Yen Lo, Piotr Dollar, and Ross Girshick. Segment anything, 2023. 7, 8 [44] Alex Krizhevsky, Ilya Sutskever, and Geoffrey Hinton. Imagenet classification with deep convolutional neural networks. In NeurIPS, 2012. 5 [45] Mengcheng Lan, Xinjiang Wang, Yiping Ke, Jiaxing Xu, Litong Feng, and Wayne Zhang. Smooseg: smoothness prior for unsupervised semantic segmentation. NeurIPS, 2024. 3 [46] Tim Lebailly, Thomas Stegmuller, Behzad Bozorgtabar, Jean-Philippe Thiran, and Tinne Tuytelaars. Cribo: Selfsupervised learning via cross-image object-level bootstrapping. In ICLR, 2024. 2, 3, 5, 6 [47] Kehan Li, Zhennan Wang, Zesen Cheng, Runyi Yu, Yian Zhao, Guoli Song, Chang Liu, Li Yuan, and Jie Chen. Acseg: Adaptive conceptualization for unsupervised semantic segmentation, 2023. 3 [48] Tsung-Yi Lin, Michael Maire, Serge Belongie, Lubomir Bourdev, Ross Girshick, James Hays, Pietro Perona, Deva Ramanan, C. Lawrence Zitnick, and Piotr Dollar. Microsoft coco: Common objects in context. In ECCV, 2014. 5, 6 [49] Ilya Loshchilov and Frank Hutter. Decoupled weight decay regularization. In ICLR, 2019. [50] Nikhil Mishra, Mostafa Rohaninejad, Xi Chen, and Pieter In ICLR, Abbeel. simple neural attentive meta-learner. 2018. 3 [51] Roozbeh Mottaghi, Xianjie Chen, Xiaobai Liu, Nam-Gyu Cho, Seong-Whan Lee, Sanja Fidler, Raquel Urtasun, and Alan Yuille. The role of context for object detection and semantic segmentation in the wild. In CVPR, 2014. 6 [52] Pushmeet Kohli Nathan Silberman, Derek Hoiem and Rob Indoor segmentation and support inference from Fergus. rgbd images. In ECCV, 2012. 6 [53] Pedro Pinheiro, Amjad Almahairi, Ryan Benmalek, Florian Golemo, and Aaron Courville. Unsupervised learning of dense visual representations. In NeurIPS, 2020. 3 [54] Aaron van den Oord, Yazhe Li, and Oriol Vinyals. Representation learning with contrastive predictive coding. arXiv preprint arXiv:1807.03748, 2018. [55] Maxime Oquab, Timothee Darcet, Theo Moutakanni, Huy Vo, Marc Szafraniec, Vasil Khalidov, Pierre Fernandez, Daniel Haziza, Francisco Massa, Alaaeldin El-Nouby, et al. Dinov2: Learning robust visual features without supervision. In TMLR, 2023. 3 [56] Maxime Oquab, Timothee Darcet, Theo Moutakanni, Huy Vo, Marc Szafraniec, Vasil Khalidov, Pierre Fernandez, Daniel Haziza, Francisco Massa, Alaaeldin El-Nouby, Mahmoud Assran, Nicolas Ballas, Wojciech Galuba, Russell Howes, Po-Yao Huang, Shang-Wen Li, Ishan Misra, Michael Rabbat, Vasu Sharma, Gabriel Synnaeve, Hu Xu, Herve Jegou, Julien Mairal, Patrick Labatut, Armand Joulin, and Piotr Bojanowski. Dinov2: Learning robust visual features without supervision, 2024. 1, 2, 5, 6 [57] Valentinos Pariza, Mohammadreza Salehi, and Yuki Asano. Hummingbird evaluation for vision encoders, 2024. 5, 13 [58] Valentinos Pariza, Mohammadreza Salehi, Gertjan Burghouts, Francesco Locatello, and Yuki Asano. Near, far: Patch-ordering enhances vision foundation models scene understanding. In ICLR, 2025. 2, 3, 5, 6, 7 [59] Zhiliang Peng, Li Dong, Hangbo Bao, Qixiang Ye, and Furu Wei. Beit v2: Masked image modeling with vector-quantized visual tokenizers. arXiv preprint arXiv:2208.06366, 2022. 3 [60] Siyuan Qiao, Chenxi Liu, Wei Shen, and Alan Yuille. Fewshot image recognition by predicting parameters from activations. In CVPR, 2018. [61] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, Gretchen Krueger, and Ilya Sutskever. Learning transferable visual models from natural language supervision. In ICML, 2021. 2, 7 [62] Sachin Ravi and Hugo Larochelle. Optimization as model for few-shot learning. In ICLR, 2017. 3 [63] Christos Sakaridis, Haoran Wang, Ke Li, Rene Zurbrugg, Arpit Jadon, Wim Abbeloos, Daniel Olmeda Reino, Luc Van Gool, and Dengxin Dai. ACDC: The adverse conditions dataset with correspondences for robust semantic driving scene perception. In ICCV, 2021. 6 [64] Mohammadreza Salehi, Efstratios Gavves, Cees GM Snoek, and Yuki Asano. Time does tell: Self-supervised timetuning of dense image representations. In ICCV, 2023. 2, 5, [65] Adam Santoro, Sergey Bartunov, Matthew Botvinick, Daan Wierstra, and Timothy Lillicrap. Meta-learning with memory-augmented neural networks. In ICML, 2016. 3 [66] Hyun Seok Seong, WonJun Moon, SuBeen Lee, and Jae-Pil Heo. Leveraging hidden positives for unsupervised semantic segmentation. In CVPR, 2023. 3 [67] Nathan Silberman, Derek Hoiem, Pushmeet Kohli, and Rob Indoor segmentation and support inference from Fergus. rgbd images. In ECCV. Springer, 2012. 7 [68] Walter Simoncini, Andrei Bursuc, Spyridon Gidaris, and Yuki Asano. No train, all gain: Self-supervised gradients improve deep frozen representations. In NeurIPS, 2024. 3 [69] Sophia Sirko-Galouchenko, Alexandre Boulch, Spyros Gidaris, Andrei Bursuc, Antonin Vobecky, Patrick Perez, and Renaud Marlet. Occfeat: Self-supervised occupancy feature prediction for pretraining bev segmentation networks. In CVPRw, 2024. 3 [70] Jake Snell, Kevin Swersky, and Richard Zemel. Prototypical networks for few-shot learning. In NeurIPS, 2017. 3 [71] Thomas Stegmuller, Tim Lebailly, Behzad Bozorgtabar, Tinne Tuytelaars, and Jean-Philippe Thiran. Croc: Crossview online clustering for dense visual representation learning. In CVPR, 2023. 3, 5, [72] Flood Sung, Yongxin Yang, Li Zhang, Tao Xiang, Philip HS Torr, and Timothy Hospedales. Learning to compare: Relation network for few-shot learning. In CVPR, 2018. 3 [73] Junjiao Tian, Lavisha Aggarwal, Andrea Colaco, Zsolt Kira, and Mar Gonzalez-Franco. Diffuse attend and segment: Unsupervised zero-shot segmentation using stable diffusion. In CVPR, 2024. 3 [74] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan Gomez, Łukasz Kaiser, and Illia Polosukhin. Attention is all you need. NeurIPS, 2017. 4 [75] Oriol Vinyals, Charles Blundell, Timothy Lillicrap, Koray Kavukcuoglu, and Daan Wierstra. Matching networks for one shot learning. In NeurIPS, 2016. 2, 3 [76] Antonin Vobecky, David Hurych, Oriane Simeoni, Spyros Gidaris, Andrei Bursuc, Patrick Perez, and Josef Sivic. Unsupervised semantic segmentation of urban scenes via crossmodal distillation. IJCV, 2025. 3 [77] Xinlong Wang, Rufeng Zhang, Chunhua Shen, Tao Kong, and Lei Li. Dense contrastive learning for self-supervised visual pre-training. In CVPR, 2021. 3 [78] Zhenda Xie, Yutong Lin, Zheng Zhang, Yue Cao, Stephen Lin, and Han Hu. Propagate yourself: Exploring pixel-level consistency for unsupervised visual representation learning. In CVPR, 2021. 3 [79] Zhenda Xie, Zheng Zhang, Yue Cao, Yutong Lin, Jianmin Bao, Zhuliang Yao, Qi Dai, and Han Hu. Simmim: simple framework for masked image modeling. In CVPR, 2022. [80] Sukmin Yun, Hankook Lee, Jaehyung Kim, and Jinwoo Shin. Patch-level representation learning for self-supervised vision transformers. In CVPR, 2022. 6 [81] Bolei Zhou, Hang Zhao, Xavier Puig, Sanja Fidler, Adela Scene parsing through Barriuso, and Antonio Torralba. ade20k dataset. In CVPR, 2017. 6 [82] Jinghao Zhou, Chen Wei, Huiyu Wang, Wei Shen, Cihang Xie, Alan Yuille, and Tao Kong. ibot: Image bert pre-training with online tokenizer. In ICLR, 2022. 2, 3, 5 [83] Adrian Ziegler and Yuki Asano. Self-supervised learning of object parts for semantic segmentation. In CVPR, 2022. 2, 3, 5, 6 A. Implementation details During post-training, we fine-tune only the last three transformer blocks of the pretrained ViT encoder () while keeping the remaining layers frozen. Our MLP projector h() consists of two linear layers with non-linear activation function GELU [35]. Hidden feature dimension is set to 7D and fixed output dimension 6144. ℓ2-normalization is applied at the output of the MLP. We set the temperature parameter τ of the softmax operator to 0.07 (see Tab. 7). Our ablation study demonstrates that the method is robust to the choice of temperature, with values ranging from 0.03 to 0.09 yielding similar performance on the PascalVOC dataset. During post-training our support set consists of 1 positive and 7 distractor examples (8 total examples). During training, we use the AdamW [49] optimizer with learning rate of 2.25107 and weight decay of 0.05. We train for 5 epochs. We employ cosine learning rate schedule."
        },
        {
            "title": "Temperature",
            "content": "0.09 0.07 0."
        },
        {
            "title": "PascalVOC",
            "content": "80.8 81.0 81.0 Table 7. Ablation of temperature τ during post-training. Generation time of pseudo semantic segmentation labels. COCO pseudo-label generation required 17 hours on 1 V100 GPU, with DiffCut inference as the bottleneck due to its current lack of batch processing optimization and suboptimal GPU utilization. However, this one-time offline process can be used to post-train multiple encoders. B. Additional quantitative results In-context scene understanding: impact of memory size. In Fig. 6, we analyze the effect of memory size on incontext semantic segmentation using the ADE20K dataset (full) for DINOv2R, NeCo, and our DIP. Results show that DIP outperforms both DINOv2R and NeCo across all memory sizes. Method Backbone COCO ADE20K DINOv2R NeCo DIP (ours) ViT-S/14 ViT-S/14 ViT-S/ ViT-B/14 DINOv2R NeCo ViT-B/14 DIP (ours) ViT-B/14 82.1 81.1 (1.0) 82.6 (+0.5) 85.5 85.2 (0.3) 86.7 (+2.0) 33.5 33.1 (0.4) 33.7 (+0.2) 38.6 39.5 (+0.9) 39.5 (+0.9) Table 8. Linear segmentation results on COCO and ADE20K datasets. All methods (DINOv2R, NeCo, and DIP) are evaluated using our implementation. DIP consistently improves over our base model DINOv2R and outperforms NeCo across both datasets. DINOv2R NeCo DIP (ours) ) % ( I 42 40 38 36 42 40 38 36"
        },
        {
            "title": "106\nMemory Size",
            "content": ""
        },
        {
            "title": "106\nMemory Size",
            "content": "107 (a) ViT-S/14 (a) ViT-B/14 In-context scene understanding: impact of memFigure 6. ory size. Semantic segmentation performance on ADE20K (full dataset) using dense nearest neighbor retrieval, evaluated across varying memory sizes."
        },
        {
            "title": "Backbone",
            "content": "DINOv2R DIP (ours) DINOv2R DIP (ours) ViT-B ViT-B ViT-L ViT-L"
        },
        {
            "title": "PascalVOC",
            "content": "1 8 75.3 79.6 72.8 78.7 1 64 67.6 75.1 61.4 70.0 1 128 60.3 70.1 54.4 64. 1 79.0 82.1 76.9 81.1 Table 9. Larger backbones evaluation We show performance of DIP and DINOv2R on larger backbone ViT-L. Dense nearest neighbor retrieval performance (mIoU) for semantic segmentation on PascalVOC across varying proportions of training data. Linear Segmentation. Tab. 8 presents linear segmentation results on COCO and ADE20K benchmarks. For fair comparison, we re-evaluated both DINOv2R and NeCo using our implementation, ensuring consistent evaluation protocols across all methods. Our approach consistently improves over the strong DINOv2R baseline and shows improvements over NeCo. Notably, with the ViT-B/14 backbone on COCO, our method achieves 86.7 mIoU, surpassing DINOv2R by 2.0 points. Larger backbones. While ViT-L (DINOv2) underperforms ViT-B in in-context segmentation [3, 57], our method still improves results with ViT-L, as shown in Tab. 9. This demonstrates DIPs scalability across backbone sizes. Nearest Neighbors (NN) vs. Two Crops We compare two strategies for creating positive examples: (1) retrieving nearest neighbors using DINOv2R image-wise features (NN) and (2) using two random crops from the same imPascalVOC 1 1 64 8 75.1 77.7 67.7 71.4 1 79.5 81. 1 128 61.2 65.9 1 39.4 39.7 ADE20K 1 1 64 8 33.2 33. 24.7 25.6 1 128 22.4 23.2 Two Crops NN Table 10. Additional ablation on the construction of positive examples. Dense nearest neighbor retrieval performance (mIoU) for semantic segmentation on PascalVOC and ADE20K across varying proportions of training data. age. NN consistently outperforms Two Crops, with the performance gap widening when fewer training examples are available (see Tab. 10). This scalability advantage justifies our design choice. C. Additional qualitative results We present additional examples of automatically constructed in-context tasks in Fig. 7, showing the quality of our pseudo-labeling approach. We display query images paired with their corresponding positive support examples, along with both pseudo-labels and ground truth labels, included only for comparison. (a) Query image (b) Pseudo labels (c) GT labels (d) Positive image (e) Pseudo labels (f) GT labels Figure 7. Examples of automatically constructed in-context scene understanding tasks. Each row shows query image and its corresponding positive support example. (a) and (b) display the query image and its pseudo segmentation labels, while (d) and (e) show the positive support image and its pseudo segmentation labels. (c) and (f) present the ground truth segmentation labels for the query and positive images, respectively, included only for comparison with the pseudo labels."
        }
    ],
    "affiliations": [
        "CIIRC CTU Prague",
        "FEE CTU",
        "Sorbonne Universite, CNRS, ISIR, F-75005 Paris, France",
        "Valeo.ai"
    ]
}