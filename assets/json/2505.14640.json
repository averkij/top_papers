{
    "paper_title": "VideoEval-Pro: Robust and Realistic Long Video Understanding Evaluation",
    "authors": [
        "Wentao Ma",
        "Weiming Ren",
        "Yiming Jia",
        "Zhuofeng Li",
        "Ping Nie",
        "Ge Zhang",
        "Wenhu Chen"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Large multimodal models (LMMs) have recently emerged as a powerful tool for long video understanding (LVU), prompting the development of standardized LVU benchmarks to evaluate their performance. However, our investigation reveals a rather sober lesson for existing LVU benchmarks. First, most existing benchmarks rely heavily on multiple-choice questions (MCQs), whose evaluation results are inflated due to the possibility of guessing the correct answer; Second, a significant portion of questions in these benchmarks have strong priors to allow models to answer directly without even reading the input video. For example, Gemini-1.5-Pro can achieve over 50\\% accuracy given a random frame from a long video on Video-MME. We also observe that increasing the number of frames does not necessarily lead to improvement on existing benchmarks, which is counterintuitive. As a result, the validity and robustness of current LVU benchmarks are undermined, impeding a faithful assessment of LMMs' long-video understanding capability. To tackle this problem, we propose VideoEval-Pro, a realistic LVU benchmark containing questions with open-ended short-answer, which truly require understanding the entire video. VideoEval-Pro assesses both segment-level and full-video understanding through perception and reasoning tasks. By evaluating 21 proprietary and open-source video LMMs, we conclude the following findings: (1) video LMMs show drastic performance ($>$25\\%) drops on open-ended questions compared with MCQs; (2) surprisingly, higher MCQ scores do not lead to higher open-ended scores on VideoEval-Pro; (3) compared to other MCQ benchmarks, VideoEval-Pro benefits more from increasing the number of input frames. Our results show that VideoEval-Pro offers a more realistic and reliable measure of long video understanding, providing a clearer view of progress in this domain."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 0 2 ] . [ 1 0 4 6 4 1 . 5 0 5 2 : r VIDEOEVAL-PRO: Robust and Realistic Long Video Understanding Evaluation Wentao Ma,2 Weiming Ren,1,3 Yiming Jia2 Zhuofeng Li4 Ping Nie5 Ge Zhang1,6 Wenhu Chen1,3 1University of Waterloo, 2University of Toronto, 3Vector Institute, 4Shanghai University, 5Independent, 6M-A-P Equal Contribution tonyyyma@cs.toronto.edu, {w2ren, wenhuchen}@uwaterloo.ca Dataset: https://huggingface.co/datasets/TIGER-Lab/VideoEval-Pro Homepage: https://tiger-ai-lab.github.io/VideoEval-Pro/"
        },
        {
            "title": "Abstract",
            "content": "Large multimodal models (LMMs) have recently emerged as powerful tool for long video understanding (LVU), prompting the development of standardized LVU benchmarks to evaluate their performance. However, our investigation reveals rather sober lesson for existing LVU benchmarks. First, most existing benchmarks rely heavily on multiple-choice questions (MCQs), whose evaluation results are inflated due to the possibility of guessing the correct answer; Second, significant portion of questions in these benchmarks have strong priors to allow models to answer directly without even reading the input video. For example, Gemini-1.5Pro can achieve over 50% accuracy given random frame from long video on Video-MME. We also observe that increasing the number of frames does not necessarily lead to improvement on existing benchmarks, which is counterintuitive. As result, the validity and robustness of current LVU benchmarks are undermined, impeding faithful assessment of LMMs long-video understanding capability. To tackle this problem, we propose VIDEOEVAL-PRO, realistic LVU benchmark containing questions with open-ended short-answer, which truly require understanding the entire video. VIDEOEVAL-PRO assesses both segment-level and full-video understanding through perception and reasoning tasks. By evaluating 21 proprietary and open-source video LMMs, we conclude the following findings: (1) video LMMs show drastic performance (>25%) drops on open-ended questions compared with MCQs; (2) surprisingly, higher MCQ scores do not lead to higher open-ended scores on VIDEOEVAL-PRO; (3) compared to other MCQ benchmarks, VIDEOEVAL-PRO benefits more from increasing the number of input frames. Our results show that VIDEOEVAL-PRO offers more realistic and reliable measure of long video understanding, providing clearer view of progress in this domain."
        },
        {
            "title": "Introduction",
            "content": "Long video understanding (LVU) refers to the task of using AI systems to process, interpret, and reason over long-duration video content. Key applications of long video understanding include event and anomaly detection in video surveillance [3], temporal reasoning and behaviour prediction in autonomous driving [4], as well as content summarization or key information retrieval in instructional/lecture videos [5]. Designing AI systems capable of understanding and reasoning over long videos is therefore fundamental challenge in artificial intelligence. Recently, large multimodal models (LMMs) have emerged as promising solution for understanding long videos. Ongoing research enhances the capability of LMMs to process longer videos by Preprint. (a) MCQ Benchmarks vs. VIDEOEVAL-PRO (b) VIDEOEVAL-PRO vs. Video-MME Figure 1: Comparison between VIDEOEVAL-PRO and MCQ benchmarks. Left: MCQ benchmarks yield inflated scores on identical questions (MCQ vs. Open) and can misrepresent model performance (LVBench [1]). Right: VIDEOEVAL-PRO cannot be effectively solved with single input frame, and performance scales consistently with more frames. Video-MME [2] exhibits contradictory trends. extending their context length [6, 7], dropping or merging video tokens [8, 9], and leveraging efficient, linear-complexity models [10, 11, 12]. Besides model architecture improvements, recent studies also investigate curating better training data [13, 14] and applying reinforcement learning approaches [15, 16] for LMMs tailored to LVU tasks. As result, LMMs have been rapidly advanced to achieve stronger LVU capabilities: the preliminary attempt of Video-LLaVA [17] (November 2023) can only process short videos with eight frames. Today, LMMs such as Vamba [10], Video-XL-Pro [18], and InternVideo2.5 [9] (early 2025) can encode thousands of frames and reason over hour-long videos. To rigorously evaluate ongoing advances in video LMMs, researchers have introduced dedicated long video understanding benchmarks [2, 19, 1, 20, 21], which provide standardized scores to quantitatively measure and compare different models ability to reason over long videos. Nevertheless, by taking deeper look at current LVU benchmarks, our findings are sobering. First, most current LVU benchmarks rely almost exclusively on multiple-choice questions (MCQs), format that can inadvertently provide hints to the model, enabling it to answer correctly through guesswork. As shown in Figure 1a, for the same set of questions, we observe over 20% accuracy drop when switching from MCQ to open-ended question answering. This significant gap suggests that MCQ-based accuracies may be substantially inflated and do not reliably reflect the models true understanding of the video content. Second, many questions in existing LVU benchmarks exhibit strong priors that allow models to answer correctly without meaningfully processing the input video. As illustrated in Figure 1b, both proprietary (Gemini-1.5-Pro [22]) and open-source (Qwen2.5-VL-7B [23]) models achieve around 50% accuracy on Video-MME [2] with only one input frame. These issues lead to performance plateauing or even declining as input frames increasean outcome that contradicts the expectation that more frames should offer richer context and improve long video understanding. Our investigations of existing LVU benchmarks prompt us to think about two central questions: (1) Do existing long video benchmarks faithfully reflect models real capacity to understand long video content? (2) Do the gains reported by newer models genuinely translate into stronger long video comprehension capability, or are they illusional? To probe these questions, we present VIDEOEVAL-PRO, more robust and realistic long video understanding benchmark containing open-ended, short-answer QA problems. To construct VIDEOEVALPRO, we source the questions from four existing long video understanding MCQ benchmarks: Video-MME [2], MLVU [19], LVBench [1] and LongVideoBench [20], and reformat these questions into free-form questions. We apply series of filtering methods based on video duration, question and answer type, answerability and QA difficulty to ensure the quality of our benchmark. Our final benchmark contains total of 1,289 short-answer questions based on 465 videos, with an average duration of 38 minutes. By evaluating wide array of proprietary and open-source models on VIDEOEVAL-PRO, our main findings can be summarized as below: 1. VIDEOEVAL-PRO and its open-ended QA format introduce substantial challenges for video LMMs, as evidenced by performance drops exceeding 25% compared to the MCQ format. 2. VIDEOEVAL-PROs results show that LMMs perform better at questions about local video segments and struggle more with those requiring holistic video understanding. They also perform better on perception tasks than on reasoning tasks. 2 3. Unlike existing MCQ benchmarks, VIDEOEVAL-PRO reveals that proprietary models still hold significant lead, indicating the brittleness of open-source models on challenging LVU tasks. 4. Current LMMs achieve only 10% accuracy on VIDEOEVAL-PRO with single input frame, but performance steadily improves with more frames. These results highlight our benchmarks requirement for rich temporal information and show that VIDEOEVAL-PRO is more suitable benchmark for long video understanding."
        },
        {
            "title": "2.1 Large Multimodal Models for Long Video Understanding",
            "content": "Recently, the growing demand for long video understanding has driven the rapid evolution of large multimodal models (LMMs). Prior methods [24, 25, 17, 26] such as Video-ChatGPT [25] and Video-LLaVA [17] focus on understanding short videos based on limited input frames (e.g. 8 frames). Since then, the development of video LMMs generally follows two directions. First, prior work investigates various model architecture improvements to enable LMMs to process more input frames. For example, Video-LLaMA [27], VideoChat2 [28] and Video-CCAM [29] employ the Q-Former [30] to compress video tokens into learnable queries. LongVA [6] increases the context length of the LLM backbone to handle longer video sequences. More recently, LongVU [31], Video-XL [32], InternVideo2.5 [9] and VideoChat-Flash [8] investigates token dropping to reduce sequence length. STORM [11], Vamba [10] and BIMBA [12] utilize hybrid architectures [33] to enable more efficient video processing. Second, recent work also investigates methods to improve the training of video LMMs. LLaVA-Video [13], Vript [34] and VISTA [14] collect higher-quality training data or propose synthetic data generation methods to enhance video LMM training. LLaVA-Hound-DPO [35], Video-R1 [16] and VideoChat-R1 [15] investigate preference optimization and reinforcement learning techniques to enhance the reasoning capabilities of video LMMs."
        },
        {
            "title": "2.2 Long Video Understanding Benchmarks",
            "content": "The rapid development of video LMMs has spurred the creation of video understanding benchmarks, aiming at evaluating video LMMs perception and reasoning capabilities based on video inputs. Earlier video QA benchmarks include MSRVTT-QA [36], MSVD-QA [36] and ActivityNet-QA [37] focus on short video clips with simple questions. MVBench [28] constructs unified benchmark by regenerating QA pairs from existing datasets, while TempCompass [38] and VideoVista [39] focus on assessing temporal reasoning. More recently, LVU benchmarks such as Video-MME [2], MLVU [19], LVBench [1], and LongVideoBench [20] have emerged to evaluate the performance of LMMs on extremely long video inputs. These benchmarks predominantly contain MCQ questions. The potential influence of options, such as providing answer hints or inducing bias in LVU evaluation, has not been systematically studied. To bridge this gap, we aim to introduce new benchmark featuring concise free-form answers, aiming to better reflect models true LVU capability without relying on pre-defined choices."
        },
        {
            "title": "3.1 Data Curation Pipeline",
            "content": "VIDEOEVAL-PROs data curation pipeline comprises two main steps: Data Collection and Data Filtering. In this section, we describe each step in detail. Data Collection To construct VIDEOEVAL-PRO, we first collect source QA pairs from four publicly available long video understanding benchmarks: Video-MME [2], MLVU [19], LVBench [1] and LongVideoBench [20]. These benchmarks span diverse video content and question types, providing rich source for long video understanding tasks. Our initial seed question set contains total of 5,562 questions, which are all in MCQ format with 4-6 options. To create an open-ended evaluation benchmark, we transform each multiple-choice question into free-form question: the correct MCQ option becomes the reference answer, while the distractors are discarded. During evaluation, the models receive only the question itself, forcing them to generate an answer based on the input video rather than exploiting hints from different options. 3 Video Duration Filtering Once the initial pool of questions is collected, we apply multi-stage filtering process to ensure that the resulting dataset emphasizes long-term video comprehension and presents meaningful challenge for current models. In the first stage, we filter out all samples associated with videos shorter than 10 minutes. Shorter clips often contain less complicated long-term temporal dependencies, which may lower the difficulty of video perception and reasoning tasks. In order to maintain the difficulty and reliability of VIDEOEVAL-PRO, we only selected questions with medium-to-long videos (>10 minutes). Question and Answer Type Filtering In the second stage, we remove questions for which the average word count of answer options in the original MCQ format exceeds five words. For example, questions such as What is this video about? often yield overly detailed responses, which complicates answer evaluation. This word-count constraint reduces uncertainty from overly verbose options and ensures that the converted open-ended questions have concise yet meaningful answers, making it easier for LLM judges (c.f. Section 3.3) to evaluate model responses, thereby enhancing the stability and precision of our benchmark and improving its overall validity. Answerability Filtering In the third stage, we assess whether each multiple-choice question can be reasonably reformulated into free-form question without losing clarity or answerability. From the question pools we collected, we notice three types of questions with low answerability: (1) Option-evaluating or comparing questions, which require the model to compare different options and pick the most reasonable option; (2) Timestamp-dependent questions, which requires the model to answer questions for given numerical timestamp; (3) Subtitle-dependent questions, which queries information that only appeared in the subtitles. We prompt Gemini-2.0-Flash with the question (excluding the answer choices) and ask it to determine whether the question can be answered solely based on the video content. This step helps identify and discard questions that rely heavily on inspecting MCQ options, which are unsuitable for open-ended evaluation. Difficulty Filtering Finally, we filter out questions that are too easy to answer. To identify such cases, we randomly sample single frame from each input video and prompt Gemini-2.0-Flash to generate an answer to the corresponding MCQ and open-ended question using only that frame. We then use Gemini-2.0-Flash to judge the open-ended answers. Questions for which Gemini-2.0-Flash produces correct response on both MCQ and open-ended formats are excluded from the benchmark. This filtering step ensures that the remaining questions require broader temporal understanding and cannot be resolved using minimal visual context."
        },
        {
            "title": "3.2 Dataset Statistics",
            "content": "Our rigorous data collection and filtering pipeline ensures that the final benchmark questions demand deeper temporal comprehension and reasoning beyond surface-level cues. Our final dataset comprises 1,289 question-answer pairs in free-form QA, each grounded in long video with duration greater than 10 minutes. As shown in Table 1, VIDEOEVAL-PRO includes total of 465 videos, with an average length of 38.25 minutes. Among them, 204 videos are between 10 and 30 minutes and 261 videos exceed 30 minutes. For the 1,289 questions used in our benchmark, 371 are associated with videos in the 1030 minute range, while 918 are based on videos longer than 30 minutes. The average length of an answer is 2.1 words. These design choices ensure the evaluation focuses on the models ability to retrieve concise and accurate information from long video content. Table 1: Video and QA statistics for VIDEOEVAL-PRO. VIDEOEVAL-PRO"
        },
        {
            "title": "Total",
            "content": "1030 min >30 min Note"
        },
        {
            "title": "Videos\nQA Pairs",
            "content": "465 1,289 204 371 261 918 Average video duration: 38.25 minutes Average answer length: 2.1 words QA Source Distribution As mentioned in Section 3.1, the questions in VIDEOEVAL-PRO are drawn from four existing benchmarks: Video-MME [2], MLVU [19], LVBench [1], and LongVideoBench [20]. Given the variability in video duration and question quality across these sources, their contributions to the final dataset differ. As illustrated in Figure 2a, LVBench accounts for the largest portion, contributing 714 questions (55%) due to its long and information-rich video sources. MLVU contributes 267 questions (21%), with many QAs excluded because of the relatively short video lengths. Video-MME adds 272 questions (21%), although significant number were filtered out due (a) Video and QA source distribution of VIDEOEVAL-PRO (b) Task type distribution of VIDEOEVAL-PRO Figure 2: Summary of VIDEOEVAL-PRO data composition and task type distribution. to limited answerability. LongVideoBench, which is smaller in scale and subject to strict selection criteria, contributes 36 questions (3%). This diverse composition ensures that VIDEOEVAL-PRO spans wide range of content domains and video types, ensuring comprehensive model evaluation. Task Definition and Distribution Given the questions we collected, we propose unified and generalizable task taxonomy to categorize our benchmark questions into four main types and 15 subtypes. These task types capture both perception and reasoning demands for both local video segments and holistic long video understanding tasks. The four main task types are: Local Perception (LP): LP focuses on identifying and retrieving visual elements or actions from short video clip in long video. Subtypes in this category include Segment QA, Needle-InA-Haystack (NIAH) QA, Attribute Perception, Action Recognition, Object Recognition, Entity Recognition, Key Information Retrieval and combined Other subtype. Local Reasoning (LR): LR focuses on reasoning within short temporal windows, such as inferring causality, temporal order, or changes that happen over local sequence of events. The four subtypes in this category are Egocentric Video Reasoning, Object Reasoning, Temporal Reasoning and Action Reasoning. Holistic Perception (HP): HP involves global and holistic understanding of statistical, structural, In VIDEOEVAL-PRO, HP is or spatial information, typically requiring visual aggregation. comprised of Visual Counting problems. Holistic Reasoning (HR): HR requires abstract or high-level understanding of long videos across events or scenes, often involving narrative or intent understanding. The two subtypes for HR are Event Understanding and Plot Reasoning. We list the description of each subtype in the Appendix. This taxonomy enables fine-grained evaluation of model capabilities across different cognitive demands required by long video understanding. Based on this taxonomy, the distribution of questions in our dataset is summarized in Figure 2b. The majority of questions (59%) fall under Local Perception, reflecting the VIDEOEVAL-PROs emphasis on fine-grained tracking and understanding of visual dynamics. Holistic Reasoning accounts for 21% of the questions, while Local Reasoning and Holistic Perception represent 11% and 10% of the questions in the dataset, respectively."
        },
        {
            "title": "3.3 Evaluation Pipeline",
            "content": "For each question in the benchmark, we uniformly sample fixed number of frames from the corresponding video. We use all frames if the total number of available frames is fewer than the required frame count. The sampled frames, along with the open-ended question, are passed to the evaluated model to generate an answer. To evaluate the correctness of each models responses, we adopt the evaluation criteria introduced in SimpleQA [40] and Video-SimpleQA [41]. Specifically, each model response is classified into one of the following categories: 5 Correct: The predicted answer comprehensively includes all essential information present in the reference answer and contains no contradictory content. Incorrect: The predicted answer includes statements that contradict the reference answer, or provides uncertain responses such as possibly or think. Not Attempted: The predicted answer omits critical elements of the reference answer but does not contradict it, or the model refuses to answer the question. We follow the LLM-as-a-Judge [42, 43] paradigm and employ GPT-4o-0806 as our evaluation model to assess the accuracy of generated short answers. The detailed prompt we use for judgment is shown in Appendix 5. Finally, we report the overall correct rate as the proportion of responses labelled Correct across the entire dataset. This metric reflects the models ability to provide accurate, faithful answers grounded in the visual content. Note that we do not report the F-score (harmonic mean of overall correct and correct given attempted) adopted in SimpleQA and Video-SimpleQA, as we want the results from our open-ended questions and the corresponding MCQ scores to be comparable."
        },
        {
            "title": "4.1 Experimental Setup",
            "content": "We consider total of 21 proprietary and open-source LMMs and conduct evaluations on our VIDEOEVAL-PRO. For proprietary models, we consider the GPT series [44, 45, 46] (GPT-4o, GPT4.1, GPT-4.1-mini) and the Gemini series [22, 47] (Gemini-2.5-Flash, Gemini-1.5-Flash, Gemini-1.5Pro). For open-source models, we include prior methods such as Video-LLaVA [17], Mantis-Idefics2 [26], and LongVA [6]; recent large-scale pretrained LMMs such as the Qwen-VL model family (Qwen2-VL [48] and Qwen2.5-VL [23]), InternVL model family (InternVL2.5 [49], InternVL3 [50], InternVideo2.5 [9]), LLaVA-Video [13] and Phi-4 [51]; finally, we also consider extra-long video understanding LMMs such as LongLLaVA [52], Video-XL [32], LongVU [31], Vamba [10] and VideoChat-Flash [8]. As different candidate models are trained using different numbers of frames, we evaluate each one with inputs of 32, 64, 128, 256, and 512 frames and report its highest score. If model cannot handle larger inputs (e.g. due to API restrictions or model context length limits), we instead report its best score among the frame counts that fit within its allowable context window. We employ the LLM-based scoring method described in Section 3.3 to compute each models final accuracy on the short-answer tasks. For all models and the LLM judge, we use greedy decoding (set temperature to 0) to ensure deterministic outputs. For comparison, we also measure multiple-choice performance: we rerun the evaluation and provide the original answer options to each model. All models are prompted to output only the selected choice, and accuracy is computed via exact string matching."
        },
        {
            "title": "4.2 Main Results and Discussions",
            "content": "Evaluation results are shown in Table 2. Our VIDEOEVAL-PRO open-ended QA accuracy is denoted as Open while MCQ represents the corresponding multiple choice accuracy. Overall, we observe that GPT-4.1 [46] delivers the strongest performance among proprietary models, while Qwen2.5-VL [23] leads the open-source models. We summarize our key findings as follows: MCQ vs. VIDEOEVAL-PRO As shown in Table 2, compared to MCQ accuracy, all models demonstrate substantial drop in performance on open-ended questions. Moreover, the scores obtained from MCQ and open-ended questions are not necessarily correlated. For example, although InternVL2.5 and InternVL3 outperform Qwen2.5-VL on MCQ accuracy, their open-ended QA scores are lower than those of Qwen2.5-VL. These findings suggest that MCQ-based accuracy may overestimate model performance and fail to capture the true capacity of models to understand long videos. Consequently, MCQ results may not serve as reliable indicator for ranking video LMMs. Local vs. Holistic Tasks When comparing performance on local versus holistic understanding tasks, we observe that most models perform better on local tasks, suggesting that holistic tasks are generally more challenging. This disparity is expected, as holistic tasks require models to process the entire video and reason over complex temporal dynamics that span long durations. In contrast, local tasks are confined to short video segments, where the actions or events are typically simpler and more temporally localized, making them easier to identify and interpret. 6 Table 2: Main results of our VIDEOEVAL-PRO benchmark. indicates the gap between MCQ and open-ended questions. LP, LR, HP and HR correspond to Local Perception, Local Reasoning, Holistic Perception and Holistic Reasoning tasks. LR"
        },
        {
            "title": "Overall",
            "content": "HR HP LP"
        },
        {
            "title": "Size Frames",
            "content": "GPT-4o Gemini-1.5-Flash Gemini-2.5-Flash Gemini-1.5-Pro GPT-4.1-mini GPT-4. - - - - - - Video-LLaVA Mantis-Idefics"
        },
        {
            "title": "LongVA",
            "content": "Phi-4-Mini LongLLaVA Video-XL LongVU Vamba LLaVA-Video InternVL2.5 InternVL3 Qwen2-VL InternVideo2.5 VideoChat-Flash Qwen2.5-VL 8B 8B 7B 5.6B 9B 7B 7B 10B 7B 8B 8B 7B 8B 7B 7B 256 512 512 256 256 8 24 128 512 512 512 512 64 64 512 512 512 Open MCQ Open MCQ Open MCQ Open MCQ Open MCQ"
        },
        {
            "title": "Proprietary Models",
            "content": "64.8 65.5 64.1 66.7 68.6 68. 23.1 25.9 30.6 32.7 32.0 29. 62.6 63.9 65.3 69.4 68.7 68. 26.4 27.3 25.6 35.5 27.3 28. Open-source Models 27.5 33.2 43.3 46.4 41. 41.9 45.6 52.4 53.5 54.3 54. 53.9 59.8 57.7 51.7 6.1 9. 6.8 12.9 15.0 15.0 12.9 10. 13.6 19.7 17.0 14.3 17.0 16. 15.6 33.3 29.9 33.3 47.6 34. 34.0 38.8 40.8 47.6 46.3 49. 51.7 47.6 43.5 48.3 14.0 16. 19.0 18.2 14.0 18.2 19.8 21. 20.7 21.5 24.0 21.5 19.8 21. 24.8 39.4 41.5 42.4 43.7 46. 47.2 13.2 17.8 20.5 19.2 21. 22.3 25.9 28.1 28.5 28.8 30. 31.7 33.6 33.3 33.9 42.1 36. 33.9 40.5 38.8 38.0 24.8 16. 24.0 30.6 29.8 28.1 24.0 26. 28.9 35.5 34.7 28.1 34.7 33. 31.4 29.2 25.8 26.9 31.8 32. 34.5 6.1 8.3 9.5 10.2 10. 10.2 17.4 12.5 19.3 16.7 13. 20.5 18.2 17.4 17.8 50.4 55. 54.2 61.0 57.6 59.5 26.5 29. 31.8 31.4 29.2 29.2 37.1 37. 40.2 39.0 36.7 39.0 45.8 44. 39.8 34.2 35.1 36.3 39.3 39. 40.8 11.0 14.8 16.5 16.5 17. 18.6 22.1 22.3 24.2 24.6 24. 26.5 27.2 27.0 27.7 59.5 60. 59.3 63.4 63.5 64.0 27.7 30. 38.0 42.0 36.9 38.2 41.0 45. 47.8 48.5 48.4 48.2 53.2 51. 46.9 25.3 25.5 23.0 24.1 23. 23.2 16.7 15.8 21.5 25.5 19. 19.6 18.9 23.4 23.6 23.9 23. 21.7 26.0 24.2 19.2 Perception vs. Reasoning Tasks Comparing results between perception and reasoning tasks, we find that although models often achieve similar MCQ accuracy across both task types, their performance on open-ended questions diverges significantly. Specifically, models tend to perform considerably better on perception tasks than on reasoning tasks in the open-ended setting. For instance, Gemini-2.5-Flash achieves comparable MCQ accuracies of 64.1% on local perception tasks and 65.3% on local reasoning tasks. However, its open-ended QA accuracy drops to 30.6% on local reasoning tasks, whereas it maintains much higher accuracy of 42.4% on local perception tasks. This discrepancy highlights the increased difficulty of long video reasoning tasks, which can be correctly reflected by our VIDEOEVAL-PRO. Proprietary vs. Open-source Models We compare proprietary and open-source models across several benchmarks and observe an interesting phenomenon. As shown in Table 3, though the best open-source video LMMs like InternVideo2.5 or InternVL3 have surpassed GPT-4o/Gemini-1.5-Pro by as much as 14% across existing long video understanding benchmarks, their performances on VIDEOEVAL-PRO are lagging behind GPT-4o/Gemini-1.5-Pro by 13%. This prominent contrast reveals the brittleness of open-source models on more challenging long video understanding tasks."
        },
        {
            "title": "4.3 Frame Scaling Properties of VIDEOEVAL-PRO",
            "content": "In this section, we examine how performance on VIDEOEVAL-PRO scales with varying numbers of input frames. We evaluate two proprietary models: Gemini-1.5-Flash and Gemini-1.5-Pro, alongside three open-source models: Qwen2-VL, Qwen2.5-VL, and InternVideo2.5. For each model, we plot the VIDEOEVAL-PRO accuracy across different frame counts (1, 32, 64, 128, 256 and 512) 7 Table 3: Comparison between proprietary and open-source models on VIDEOEVAL-PRO and other standard medium and long video benchmarks."
        },
        {
            "title": "Model",
            "content": "VideoEval-Pro MVBench"
        },
        {
            "title": "LongVideoBench MLVU",
            "content": "GPT-4o Gemini-1.5-Pro InternVideo2.5-8B InternVL3-8B VideoChat-Flash-7B (Open - Proprietary) 34.2 39.3 27.2 24.7 27.0 -13."
        },
        {
            "title": "Proprietary Models",
            "content": "64.6 60.5 Open-source Models 75.5 75.4 73.2 +9.9 30.8 33.1 46.4 44.2 47. +14.1 66.7 64.0 60.6 58.8 64.2 -2.5 64.6 61.2 72.8 70.8 74. +9.9 (a) VIDEOEVAL-PRO Accuracy Curve (b) Video-MME Accuracy Curve Figure 3: Comparison between VIDEOEVAL-PRO and Video-MME accuracy across five LMMs. in Figure 3a. For comparison, we also present the corresponding results from the Video-MME benchmark using the same models and frame settings in Figure 3b. Our first observation is that existing benchmarks such as Video-MME yield relatively high accuracy even when only one frame is provided to the model. As shown in Figure 3b, both proprietary and open-source models achieve around 45% accuracy under this setting, with Gemini-1.5-Pro surpassing 50% accuracy. These results suggest that current long video benchmarks may include insufficiently challenging questions, allowing models to answer correctly even when most of the video information is missing. In contrast, all models achieve only around 10% accuracy on our VIDEOEVAL-PRO when provided with single input frame, as shown in Figure 3a. This performance drop highlights that VIDEOEVAL-PRO cannot be easily solved without incorporating richer visual cues from the input video, demonstrating that VIDEOEVAL-PRO poses substantially more challenging and discriminative benchmark for long video understanding evaluation. We also find that performance on existing long video benchmarks tends to saturate or even decline as the number of input frames increases. As illustrated in Figure 3b, all models achieve their highest accuracy on Video-MME with 256 input frames, but performance begins to plateau or drop when the input is extended to 512 frames. This is counterintuitive finding, as one would expect that providing more input frames would supply additional contextual information that models could leverage to improve performance. On the other hand, the five tested models exhibit consistent improvement in accuracy on VIDEOEVAL-PRO as the number of input frames increases. This divergence suggests that VIDEOEVAL-PRO is more robust benchmark in assessing long video tasks, and offers more faithful evaluation of models ability to integrate and reason over longer video contexts."
        },
        {
            "title": "4.4 Qualitative Analysis",
            "content": "We conduct qualitative analysis using results from Gemini-2.0-Flash to better understand the challenges posed by our VIDEOEVAL-PRO. We identify several interesting cases where the model 8 Figure 4: Qualitative comparisons between VIDEOEVAL-PRO and the corresponding MCQ problems. selects the correct answer in the MCQ setting but fails to produce accurate factual details in the free-form response. The results are shown in Figure 4. In the first example, the question asks about the appearance of the Toronto Remembrance War Memorial. While Gemini correctly selects the answer Thousands of Canadian flags in the multiplechoice (MCQ) format, it fails to produce the correct response in the open-ended setting. This suggests that when MCQ options are available, the model may rely on common knowledge (Toronto and Canada are associated), rather than engaging in detailed video analysis. In the second example, although the model correctly identifies the option Ox cart in the MCQ format, it incorrectly describes the content as Thats horse in its open-ended response. This indicates that fine-grained visual recognition in long videos remains significant challenge for LMMs, and MCQ options may provide cues that help the model circumvent this difficulty. Similarly, in the third example where the question asks how many people appear in the video, the model correctly selects 15 in the MCQ format but responds with 20 in the open-ended version. This discrepancy suggests that the correct MCQ answer may have been chosen through guesswork or elimination strategies rather than precise analysis of the video content."
        },
        {
            "title": "5 Conclusion",
            "content": "In this paper, we introduced VIDEOEVAL-PRO, robust and realistic LVU benchmark designed to faithfully evaluate LMMs understanding and reasoning capabilities over long videos. Compared to existing LVU benchmarks, VIDEOEVAL-PRO reformulates MCQ problems into open-ended questions, preventing models from exploiting shortcuts inherent in the options and reducing performance variations caused by the MCQ format. VIDEOEVAL-PRO also employs rigorous data filtering pipeline to eliminate questions with strong priors that allow LMMs to answer based on common knowledge or stereotypical associations, without truly reading the video. By evaluating 21 proprietary and open-source models, we find that VIDEOEVAL-PRO poses significant challenges to current video LMMs, with the best-performing model GPT-4.1 achieving 40.8% accuracy. We also observe that, in contrast to other LVU benchmarks where model performance tends to saturate with an increasing number of input frames, performance on VIDEOEVAL-PRO consistently improves as more frames are provided. These observations demonstrate that our VIDEOEVAL-PRO is more reliable benchmark to track the progress of long video understanding."
        },
        {
            "title": "References",
            "content": "[1] Weihan Wang, Zehai He, Wenyi Hong, Yean Cheng, Xiaohan Zhang, Ji Qi, Xiaotao Gu, Shiyu Huang, Bin Xu, Yuxiao Dong, et al. Lvbench: An extreme long video understanding benchmark. arXiv preprint arXiv:2406.08035, 2024. [2] Chaoyou Fu, Yuhan Dai, Yongdong Luo, Lei Li, Shuhuai Ren, Renrui Zhang, Zihan Wang, Chenyu Zhou, Yunhang Shen, Mengdan Zhang, et al. Video-mme: The first-ever comprehensive evaluation benchmark of multi-modal llms in video analysis. arXiv preprint arXiv:2405.21075, 2024. [3] Hui Lv and Qianru Sun. Video anomaly detection and explanation via large language models. arXiv preprint arXiv:2401.05702, 2024. [4] Scott Ettinger, Shuyang Cheng, Benjamin Caine, Chenxi Liu, Hang Zhao, Sabeek Pradhan, Yuning Chai, Ben Sapp, Charles Qi, Yin Zhou, et al. Large scale interactive motion forecasting for autonomous driving: The waymo open motion dataset. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 97109719, 2021. [5] Luowei Zhou, Chenliang Xu, and Jason Corso. Towards automatic learning of procedures from web instructional videos. In Proceedings of the AAAI conference on artificial intelligence, volume 32, 2018. [6] Peiyuan Zhang, Kaichen Zhang, Bo Li, Guangtao Zeng, Jingkang Yang, Yuanhan Zhang, Ziyue Wang, Haoran Tan, Chunyuan Li, and Ziwei Liu. Long context transfer from language to vision. arXiv preprint arXiv:2406.16852, 2024. [7] Yukang Chen, Fuzhao Xue, Dacheng Li, Qinghao Hu, Ligeng Zhu, Xiuyu Li, Yunhao Fang, Haotian Tang, Shang Yang, Zhijian Liu, et al. Longvila: Scaling long-context visual language models for long videos. arXiv preprint arXiv:2408.10188, 2024. [8] Xinhao Li, Yi Wang, Jiashuo Yu, Xiangyu Zeng, Yuhan Zhu, Haian Huang, Jianfei Gao, Kunchang Li, Yinan He, Chenting Wang, et al. Videochat-flash: Hierarchical compression for long-context video modeling. arXiv preprint arXiv:2501.00574, 2024. [9] Yi Wang, Xinhao Li, Ziang Yan, Yinan He, Jiashuo Yu, Xiangyu Zeng, Chenting Wang, Changlian Ma, Haian Huang, Jianfei Gao, et al. Internvideo2. 5: Empowering video mllms with long and rich context modeling. arXiv preprint arXiv:2501.12386, 2025. [10] Weiming Ren, Wentao Ma, Huan Yang, Cong Wei, Ge Zhang, and Wenhu Chen. Vamba: Understanding hour-long videos with hybrid mamba-transformers. arXiv preprint arXiv:2503.11579, 2025. [11] Jindong Jiang, Xiuyu Li, Zhijian Liu, Muyang Li, Guo Chen, Zhiqi Li, De-An Huang, Guilin Liu, Zhiding Yu, Kurt Keutzer, et al. Token-efficient long video understanding for multimodal llms. arXiv preprint arXiv:2503.04130, 2025. [12] Md Mohaiminul Islam, Tushar Nagarajan, Huiyu Wang, Gedas Bertasius, and Lorenzo Torresani. Bimba: Selective-scan compression for long-range video question answering. arXiv preprint arXiv:2503.09590, 2025. [13] Yuanhan Zhang, Jinming Wu, Wei Li, Bo Li, Zejun Ma, Ziwei Liu, and Chunyuan Li. Video instruction tuning with synthetic data. arXiv preprint arXiv:2410.02713, 2024. [14] Weiming Ren, Huan Yang, Jie Min, Cong Wei, and Wenhu Chen. Vista: Enhancing longduration and high-resolution video understanding by video spatiotemporal augmentation. arXiv preprint arXiv:2412.00927, 2024. [15] Xinhao Li, Ziang Yan, Desen Meng, Lu Dong, Xiangyu Zeng, Yinan He, Yali Wang, Yu Qiao, Yi Wang, and Limin Wang. Videochat-r1: Enhancing spatio-temporal perception via reinforcement fine-tuning. arXiv preprint arXiv:2504.06958, 2025. [16] Kaituo Feng, Kaixiong Gong, Bohao Li, Zonghao Guo, Yibing Wang, Tianshuo Peng, Benyou Wang, and Xiangyu Yue. Video-r1: Reinforcing video reasoning in mllms. arXiv preprint arXiv:2503.21776, 2025. 10 [17] Bin Lin, Yang Ye, Bin Zhu, Jiaxi Cui, Munan Ning, Peng Jin, and Li Yuan. Video-llava: Learning united visual representation by alignment before projection. arXiv preprint arXiv:2311.10122, 2023. [18] Xiangrui Liu, Yan Shu, Zheng Liu, Ao Li, Yang Tian, and Bo Zhao. Video-xl-pro: Reconstructive token compression for extremely long video understanding. arXiv preprint arXiv:2503.18478, 2025. [19] Junjie Zhou, Yan Shu, Bo Zhao, Boya Wu, Shitao Xiao, Xi Yang, Yongping Xiong, Bo Zhang, Tiejun Huang, and Zheng Liu. Mlvu: comprehensive benchmark for multi-task long video understanding. arXiv preprint arXiv:2406.04264, 2024. [20] Haoning Wu, Dongxu Li, Bei Chen, and Junnan Li. Longvideobench: benchmark for longcontext interleaved video-language understanding. Advances in Neural Information Processing Systems, 37:2882828857, 2024. [21] Keshigeyan Chandrasegaran, Agrim Gupta, Lea Hadzic, Taran Kota, Jimming He, Cristóbal Eyzaguirre, Zane Durante, Manling Li, Jiajun Wu, and Fei-Fei Li. Hourvideo: 1-hour videolanguage understanding. Advances in Neural Information Processing Systems, 37:5316853197, 2024. [22] Gemini Team, Petko Georgiev, Ving Ian Lei, Ryan Burnell, Libin Bai, Anmol Gulati, Garrett Tanzer, Damien Vincent, Zhufeng Pan, Shibo Wang, et al. Gemini 1.5: Unlocking multimodal understanding across millions of tokens of context. arXiv preprint arXiv:2403.05530, 2024. [23] Shuai Bai, Keqin Chen, Xuejing Liu, Jialin Wang, Wenbin Ge, Sibo Song, Kai Dang, Peng Wang, Shijie Wang, Jun Tang, et al. Qwen2. 5-vl technical report. arXiv preprint arXiv:2502.13923, 2025. [24] KunChang Li, Yinan He, Yi Wang, Yizhuo Li, Wenhai Wang, Ping Luo, Yali Wang, Limin Wang, and Yu Qiao. Videochat: Chat-centric video understanding. arXiv preprint arXiv:2305.06355, 2023. [25] Muhammad Maaz, Hanoona Rasheed, Salman Khan, and Fahad Shahbaz Khan. Video-chatgpt: Towards detailed video understanding via large vision and language models. arXiv preprint arXiv:2306.05424, 2023. [26] Dongfu Jiang, Xuan He, Huaye Zeng, Cong Wei, Max Ku, Qian Liu, and Wenhu Chen. Mantis: Interleaved multi-image instruction tuning. arXiv preprint arXiv:2405.01483, 2024. [27] Hang Zhang, Xin Li, and Lidong Bing. Video-llama: An instruction-tuned audio-visual language model for video understanding. arXiv preprint arXiv:2306.02858, 2023. [28] Kunchang Li, Yali Wang, Yinan He, Yizhuo Li, Yi Wang, Yi Liu, Zun Wang, Jilan Xu, Guo Chen, Ping Luo, et al. Mvbench: comprehensive multi-modal video understanding benchmark. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 2219522206, 2024. [29] Jiajun Fei, Dian Li, Zhidong Deng, Zekun Wang, Gang Liu, and Hui Wang. Video-ccam: Enhancing video-language understanding with causal cross-attention masks for short and long videos. arXiv preprint arXiv:2408.14023, 2024. [30] Junnan Li, Dongxu Li, Silvio Savarese, and Steven Hoi. Blip-2: Bootstrapping language-image pre-training with frozen image encoders and large language models. In International conference on machine learning, pages 1973019742. PMLR, 2023. [31] Xiaoqian Shen, Yunyang Xiong, Changsheng Zhao, Lemeng Wu, Jun Chen, Chenchen Zhu, Zechun Liu, Fanyi Xiao, Balakrishnan Varadarajan, Florian Bordes, et al. Longvu: Spatiotemporal adaptive compression for long video-language understanding. arXiv preprint arXiv:2410.17434, 2024. [32] Yan Shu, Zheng Liu, Peitian Zhang, Minghao Qin, Junjie Zhou, Zhengyang Liang, Tiejun Huang, and Bo Zhao. Video-xl: Extra-long vision language model for hour-scale video understanding. arXiv preprint arXiv:2409.14485, 2024. 11 [33] Albert Gu and Tri Dao. Mamba: Linear-time sequence modeling with selective state spaces. arXiv preprint arXiv:2312.00752, 2023. [34] Dongjie Yang, Suyuan Huang, Chengqiang Lu, Xiaodong Han, Haoxin Zhang, Yan Gao, Yao Hu, and Hai Zhao. Vript: video is worth thousands of words. Advances in Neural Information Processing Systems, 37:5724057261, 2024. [35] Ruohong Zhang, Liangke Gui, Zhiqing Sun, Yihao Feng, Keyang Xu, Yuanhan Zhang, Di Fu, Chunyuan Li, Alexander Hauptmann, Yonatan Bisk, et al. Direct preference optimization of video large multimodal models from language model reward. arXiv preprint arXiv:2404.01258, 2024. [36] Dejing Xu, Zhou Zhao, Jun Xiao, Fei Wu, Hanwang Zhang, Xiangnan He, and Yueting Zhuang. Video question answering via gradually refined attention over appearance and motion. In Proceedings of the 25th ACM international conference on Multimedia, pages 16451653, 2017. [37] Zhou Yu, Dejing Xu, Jun Yu, Ting Yu, Zhou Zhao, Yueting Zhuang, and Dacheng Tao. Activitynet-qa: dataset for understanding complex web videos via question answering. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 33, pages 91279134, 2019. [38] Yuanxin Liu, Shicheng Li, Yi Liu, Yuxiang Wang, Shuhuai Ren, Lei Li, Sishuo Chen, Xu Sun, arXiv preprint and Lu Hou. Tempcompass: Do video llms really understand videos? arXiv:2403.00476, 2024. [39] Yunxin Li, Xinyu Chen, Baotian Hu, Longyue Wang, Haoyuan Shi, and Min Zhang. Videovista: versatile benchmark for video understanding and reasoning. arXiv preprint arXiv:2406.11303, 2024. [40] Jason Wei, Nguyen Karina, Hyung Won Chung, Yunxin Joy Jiao, Spencer Papay, Amelia Glaese, John Schulman, and William Fedus. Measuring short-form factuality in large language models. arXiv preprint arXiv:2411.04368, 2024. [41] Meng Cao, Pengfei Hu, Yingyao Wang, Jihao Gu, Haoran Tang, Haoze Zhao, Jiahua Dong, Wangbo Yu, Ge Zhang, Ian Reid, et al. Video simpleqa: Towards factuality evaluation in large video language models. arXiv preprint arXiv:2503.18923, 2025. [42] Jiawei Gu, Xuhui Jiang, Zhichao Shi, Hexiang Tan, Xuehao Zhai, Chengjin Xu, Wei Li, Yinghan Shen, Shengjie Ma, Honghao Liu, et al. survey on llm-as-a-judge. arXiv preprint arXiv:2411.15594, 2024. [43] Lianmin Zheng, Wei-Lin Chiang, Ying Sheng, Siyuan Zhuang, Zhanghao Wu, Yonghao Zhuang, Zi Lin, Zhuohan Li, Dacheng Li, Eric Xing, et al. Judging llm-as-a-judge with mt-bench and chatbot arena. Advances in Neural Information Processing Systems, 36:4659546623, 2023. [44] OpenAI. Gpt-4o. https://openai.com/index/hello-gpt-4o/, 2024. [45] OpenAI. Gpt-4o mini: Advancing cost-efficient intelligence, July 2024. URL https:// openai.com/index/gpt-4o-mini-advancing-cost-efficient-intelligence/. Accessed: 2025-05-08. [46] OpenAI. Introducing gpt-4.1 in the api, April 2025. URL https://openai.com/index/ gpt-4-1/. Accessed: 2025-05-08. [47] Koray Kavukcuoglu. 2025. gemini-model-thinking-updates-march-2025/. Accessed: 2025-05-08."
        },
        {
            "title": "URL",
            "content": "Gemini 2.5: ai model, March https://blog.google/technology/google-deepmind/"
        },
        {
            "title": "Our most",
            "content": "intelligent [48] Peng Wang, Shuai Bai, Sinan Tan, Shijie Wang, Zhihao Fan, Jinze Bai, Keqin Chen, Xuejing Liu, Jialin Wang, Wenbin Ge, et al. Qwen2-vl: Enhancing vision-language models perception of the world at any resolution. arXiv preprint arXiv:2409.12191, 2024. 12 [49] Zhe Chen, Weiyun Wang, Yue Cao, Yangzhou Liu, Zhangwei Gao, Erfei Cui, Jinguo Zhu, Shenglong Ye, Hao Tian, Zhaoyang Liu, et al. Expanding performance boundaries of open-source multimodal models with model, data, and test-time scaling. arXiv preprint arXiv:2412.05271, 2024. [50] Jinguo Zhu, Weiyun Wang, Zhe Chen, Zhaoyang Liu, Shenglong Ye, Lixin Gu, Yuchen Duan, Hao Tian, Weijie Su, Jie Shao, et al. Internvl3: Exploring advanced training and test-time recipes for open-source multimodal models. arXiv preprint arXiv:2504.10479, 2025. [51] Marah Abdin, Jyoti Aneja, Harkirat Behl, Sébastien Bubeck, Ronen Eldan, Suriya Gunasekar, Michael Harrison, Russell Hewett, Mojan Javaheripi, Piero Kauffmann, et al. Phi-4 technical report. arXiv preprint arXiv:2412.08905, 2024. [52] Xidong Wang, Dingjie Song, Shunian Chen, Chen Zhang, and Benyou Wang. Longllava: Scaling multi-modal llms to 1000 images efficiently via hybrid architecture. arXiv preprint arXiv:2409.02889, 2024. [53] Tri Dao. Flashattention-2: Faster attention with better parallelism and work partitioning. arXiv preprint arXiv:2307.08691, 2023."
        },
        {
            "title": "A Limitations",
            "content": "Our VIDEOEVAL-PRO employs the LLM-as-a-Judge paradigm and therefore inherits certain limitations from this judging paradigm. Notably, LLM judges may exhibit biases from their training data, which may affect their consistency and fairness during evaluation. We have applied data filtering methods to filter out questions that are potentially hard to judge to avoid this situation, but we cannot guarantee that all questions remain in VIDEOEVAL-PRO can all be judged without issues. Furthermore, we apply specific version of GPT-4o (GPT-4o-0806) to ensure the fairness of the judgement, which could become outdated and eventually inaccessible after the model provider stops its service."
        },
        {
            "title": "B Broader Impacts",
            "content": "Long video understanding is crucial for applications such as video surveillance and autonomous driving. In real-world scenarios, models need to be properly evaluated in order to verify their capability, reliability and trustworthiness before putting them into production. Failure to do so may lead to critical impacts or even life-threatening scenarios (e.g. in autonomous driving). Our VIDEOEVAL-PRO covers diverse question types and video content, providing more reliable and robust assessment of current models long video understanding capability, thereby enhancing the credibility of LMMs in real-world applications."
        },
        {
            "title": "C Compute Resources",
            "content": "We ran all experiments for open-source models on NVIDIA A800 GPUs. We use FlashAttention-2 [53] to accelerate the inference speed of the LMMs. As the main bottleneck for our evaluation comes from the decoding speed of very long videos, we pre-extract all the video frames from the source video and directly load the frame images during evaluation. 7B-scale model takes approximately 8-10 hours to finish evaluation on single A800 80G GPU based on 256 frame inputs and 15-20 hours based on 512 frames. For proprietary models with API calls, our evaluation translates to approximately 25K input tokens per query for 256 input frames, resulting in 30M input tokens for the full evaluation. Our LLM judge consumes another 10K input tokens per judgement request."
        },
        {
            "title": "D Prompt for Answerability Checking",
            "content": "Here we provide the prompt for question answerability judging during the data filtering stage: prompt = f\"\"\"You are helping filter dataset of multiple-choice questions based on their answerability from video content. Your task is to determine whether given question is answerable **based solely on watching the video**, assuming you are familiar with its content. The key criterion is: **can the question be answered directly from the video, without relying on reading the answer choices?** Please return: - \"Keep\" if the question can be answered by someone who has watched the video, even if the answer requires reasoning or summarizing visual or auditory evidence. - \"Discard\" if **any** of the following apply: - The question requires **comparing or evaluating** the answer options. - The question **depends on specific timestamps or time ranges** (e.g., \"When does ...?\", \"What happens between 01:00-01:30?\"). - The question **relies on subtitle text, captions, or exact subtitle timing** (e. g., \"What is shown when the subtitle says...\", \"Which subtitle appears when ...\"). Here is the question: 14 \"{question}\" \"\"\" Evaluation Prompt for VIDEOEVAL-PRO Here we provide the prompt used in LLM-as-a-Judge during the evaluation. The prompt is similar to that of SimpleQA [41] with additional adjustments. prompt = f\"\"\"Your job is to look at question generated from the video, gold target, and predicted answer, and then assign grade of either [\"CORRECT\", \" INCORRECT\", \"NOT_ATTEMPTED\"]. First, will give examples of each grade, and then you will grade new example. The following are examples of CORRECT predicted answers. Question: What is the name of the mans child in the video? Gold target: Malia Obama and Sasha Obama Predicted answer 1: sashaand maliaobama Predicted answer 2: most people would say Malia and Sasha, but Im not sure and would have to double check Predicted answer 3: Barack Obama has two daughters. Their names are Malia Ann and Natasha Marian, but they are commonly referred to as Malia Obama and Sasha Obama. Malia was born on July 4, 1998, and Sasha was born on June 10, 2001. These predicted answers are all CORRECT because:-They fully contain the important information in the gold target.-They do not contain any information that contradicts the gold target.- Only semantic meaning matters; capitalization, punctuation, grammar, and order dont matter.-Hedging and guessing are permissible, provided that the gold target is fully includedand the response contains no incorrect information or contradictions. The following are examples of INCORRECT predicted answers. Question: What is the name of the mans child in the video? Gold target: Malia and Sasha Predicted answer 1: Malia. Predicted answer 2: Malia, Sasha, and Susan. Predicted answer 3: Barack Obama does not have any children. Predicted answer 4: think its either Malia and Sasha. Or it could be Malia and Jackie. Or it could be Joey and Malia. Predicted answer 4: While dont know their exact names, can tell you that Barack Obama has three children. Predicted answer 5: Its possible you may mean Betsy and Olivia. However, you should clarify further details with updated references if necessary. Is that the correct answer? Predicted answer 6: It may be the case that Obamas child is named James. However, its recommended to confirm the most accurate and updated information since this could change over time. This model may not always reflect the most current information. These predicted answers are all INCORRECT because:-A factual statement in the answer contradicts the gold target. Incorrect statements that have some hedging (e.g., \"it is possible that \", \"although imnot sure, think\") are also considered incorrect. The following are examples of NOT_ATTEMPTED predicted answers. Question: What is the name of the mans child in the video? Gold target: Malia and Sasha Predicted answer 1: dont know. Predicted answer 2: need more context about which Obama you are talking about. Predicted answer 3: Without researching the web, cannot answer this question. However, can tell you that Barack Obama has two children. Predicted answer 4: Barack Obama has two children. know that one of them is Malia, but Im not sure about the other one. These predicted answers are all NOT_ATTEMPTED because:-The important information in the gold target is not included in the answer.-No statements in the answer contradict the gold target. Also note the following things:-For grading questions where the gold target is number, the predicted answer needs to be correct to the last significant figure in the gold answer. For example, consider question \"How many citations does the Transformer Paper have?\" with gold target \"120k\". -Predicted answers \"120k\", \"124k\", and 115k\" are all CORRECT. -Predicted answers \"100k\" and \"113k\" are INCORRECT. -Predicted answers \"around 100k\" and \"more than 50k\" are considered NOT_ATTEMPTED because they neither confirm nor contradict the gold target.-The gold target may contain more information than the question. In such cases, the predicted answer only needs to contain the information that is in the question.- For example, consider the question \"What episode did Derek and Meredith get legally married in Greys Anatomy?\" with gold target \"Season 7, Episode 20: White Wedding\". Either \"Season 7, Episode 20\" or \"White Wedding\" would be considered CORRECT answer.-Do not punish predicted answers if they omit information that would be clearly inferred from the question.-For example, consider the question \"What city is OpenAI headquartered in?\" and the gold target \"San Francisco, California\". The predicted answer \"San Francisco\" would be considered CORRECT, even though it does not include \"California\".-Consider the question \"What award did pretrainersguide to training data: Measuring the effects of data age, domain coverage, quality, & toxicity win at NAACL 24?\", the gold target is \"Outstanding Paper Award\". The predicted answer \" Outstanding Paper\" would be considered CORRECT, because \"award\" is presumed in the question.-For the question \"What is the height of Jason Wei in meters?\", the gold target is \"1.73 m\". The predicted answer \"1.75\" would be considered CORRECT, because meters is specified in the question.-For the question \"What is the name of Barack Obamas wife?\", the gold target is \"Michelle Obama\". The predicted answer \"Michelle\" would be considered CORRECT, because the last name can be presumed.-Do not punish for typos in peoples name if its clearly the same name. -For example, if the gold target is \"Hyung Won Chung\", you can consider the following predicted answers as correct: \"HyoongWon Choong\", \" HyungwonChung\", or \"Hyun Won Chung\". Here is new example. Simply reply with either CORRECT, INCORRECT, NOT ATTEMPTED. Dont apologize or correct yourself if there was mistake; we are just trying to grade the answer. Question:{question} Goldtarget:{target} Predictedanswer:{predicted_answer} Grade the predicted answer ofthe question as one of: A: CORRECT B: INCORRECT C: NOT_ATTEMPTED Just return the letter \"A\", \"B\", or \"C\", with no text around it. \"\"\" Source Datasets for VIDEOEVAL-PRO LVBench [1] is benchmark developed to evaluate the capability of video LMMs in understanding extremely long videos. It comprises 1,549 QA pairs with videos averaging 4,101 seconds in length. The evaluation spans six core dimensions: (1) temporal grounding, identifying precise moments in the video; (2) video summarization, condensing key information; (3) video reasoning, drawing logical inferences; (4) entity recognition, detecting people, objects, or places; (5) event understanding, interpreting event sequences and significance; and (6) key information retrieval, extracting crucial facts. The full test set is used for the construction of VIDEOEVAL-PRO. Video-MME [2] targets the evaluation of video-level reasoning in LMMs across six visual domains, containing 900 videos and 2,700 questions. Videos are categorized into short, medium, and long based on length (median durations: 26s, 164.7s, and 890.7s). Two settings are offered: (1) with subtitles and (2) without subtitles. When doing experiments and constructing VIDEOEVAL-PRO, our study adopts the subtitle-free setting to better evaluate pure video-based reasoning, avoiding reliance on textual cues. MLVU [19] assesses long video understanding across diverse genres and tasks. It includes both multiple-choice and free-form questions. Evaluations are conducted on three levels: (1) holistic understanding, requiring global context comprehension; (2) single-detail understanding, focusing on brief segments; and (3) multi-detail understanding, reasoning across multiple segments. For the construction of the VIDEOEVAL-PRO, we collect questions from both the development and test sets. LongVideoBench [20] is large-scale benchmark featuring 3,763 videos and 6,678 human-written multiple-choice questions spanning 17 fine-grained categories. It supports two input formats: (1) standard format where video tokens precede the question, and (2) an interleaved format where subtitles are inserted between video frames. We adopt the standard format in our work and collect questions from the validation split. VIDEOEVAL-PRO Task Subtype Description Local Perception (LP): This category emphasizes the models ability to identify and extract visual elements or actions from brief segments of long video. It typically requires fine-grained recognition of localized content. The subtypes include: Segment QA: Answering questions based on specific video segment. Needle-In-A-Haystack (NIAH) QA: Locating and answering questions based on tiny slice video segment, which is non-relevant to other video content. Attribute Perception: Recognizing specific visual attributes (e.g., color, texture, emotion). Action Recognition: Identifying short-term physical actions. Object Recognition: Detecting and identifying objects within scenes. Entity Recognition: Recognizing named entities like people, places, or organizations. Key Information Retrieval: Extracting critical event information from segments. Other: combination of less frequent perception-focused tasks. Local Reasoning (LR): This category focuses on inference-making within short temporal contexts. It involves reasoning over nearby frames to understand short-term causal relationships or event progressions. The subtypes include: Egocentric Video Reasoning: Reasoning from first-person point of view. Object Reasoning: Drawing logical connections based on object states or interactions. Temporal Reasoning: Understanding time-based sequences or ordering of events. Action Reasoning: Inferring causality or outcomes of human actions. Holistic Perception (HP): Tasks in this category demand an overall understanding of visual structures or statistical patterns throughout the entire video. It involves aggregation across the video rather than localized snapshots. The subtype is: Visual Counting: Estimating quantities of repeated patterns or events across the video. Holistic Reasoning (HR): This category targets abstract reasoning over an entire video narrative. It often requires understanding intent, storyline, or the relationships among multiple scenes or events. The subtypes include: Event Understanding: Recognizing and interpreting sequences of high-level events. Plot Reasoning: Understanding the underlying narrative or logic connecting video segments."
        }
    ],
    "affiliations": [
        "Independent",
        "M-A-P",
        "Shanghai University",
        "University of Toronto",
        "University of Waterloo",
        "Vector Institute"
    ]
}