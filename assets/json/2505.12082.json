{
    "paper_title": "Model Merging in Pre-training of Large Language Models",
    "authors": [
        "Yunshui Li",
        "Yiyuan Ma",
        "Shen Yan",
        "Chaoyi Zhang",
        "Jing Liu",
        "Jianqiao Lu",
        "Ziwen Xu",
        "Mengzhao Chen",
        "Minrui Wang",
        "Shiyi Zhan",
        "Jin Ma",
        "Xunhao Lai",
        "Yao Luo",
        "Xingyan Bin",
        "Hongbin Ren",
        "Mingji Han",
        "Wenhao Hao",
        "Bairen Yi",
        "LingJun Liu",
        "Bole Ma",
        "Xiaoying Jia",
        "Zhou Xun",
        "Liang Xiang",
        "Yonghui Wu"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Model merging has emerged as a promising technique for enhancing large language models, though its application in large-scale pre-training remains relatively unexplored. In this paper, we present a comprehensive investigation of model merging techniques during the pre-training process. Through extensive experiments with both dense and Mixture-of-Experts (MoE) architectures ranging from millions to over 100 billion parameters, we demonstrate that merging checkpoints trained with constant learning rates not only achieves significant performance improvements but also enables accurate prediction of annealing behavior. These improvements lead to both more efficient model development and significantly lower training costs. Our detailed ablation studies on merging strategies and hyperparameters provide new insights into the underlying mechanisms while uncovering novel applications. Through comprehensive experimental analysis, we offer the open-source community practical pre-training guidelines for effective model merging."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 7 1 ] . [ 1 2 8 0 2 1 . 5 0 5 2 : r Model Merging in Pre-training of Large Language Models"
        },
        {
            "title": "ByteDance Seed",
            "content": "Full author list in Contributions"
        },
        {
            "title": "Abstract",
            "content": "Model merging has emerged as promising technique for enhancing large language models, though its application in large-scale pre-training remains relatively unexplored. In this paper, we present comprehensive investigation of model merging techniques during the pre-training process. Through extensive experiments with both dense and Mixture-of-Experts (MoE) architectures ranging from millions to over 100 billion parameters, we demonstrate that merging checkpoints trained with constant learning rates not only achieves significant performance improvements but also enables accurate prediction of annealing behavior. These improvements lead to both more efficient model development and significantly lower training costs. Our detailed ablation studies on merging strategies and hyperparameters provide new insights into the underlying mechanisms while uncovering novel applications. Through comprehensive experimental analysis, we offer the open-source community practical pre-training guidelines for effective model merging. Date: May 18, 2025 Correspondence: Yunshui Li at liyunshui@bytedance.com"
        },
        {
            "title": "Introduction",
            "content": "Modern large language models (LLMs) [1, 13, 37, 41, 49] have demonstrated remarkable capabilities with widespread applications across diverse tasks. Despite their exceptional performance in fundamental tasks, LLMs still face several critical challenges, including the extensive pre-training costs, discounted effectiveness of domain-specific post-training, imprecisely-predictable performance scaling, as well as the instability of large-scale training. Model merging [50], as relatively young topic, presents promising approach to alleviate these practical challenges. Recently, the benefits of model merging have been primarily studied in the post-training stage, where several models fine-tuned on different downstream tasks are combined into single but more versatile model [19, 52, 58]. For example, using the DARE [52] method to merge WizardLM [46] with WizardMath [29] shows significant performance enhancement on GSM8K [7], raising its score from 2.2 to 66.3. In contrast, research on model merging during the pre-training phase remains scarce. Such pre-training merging typically involves combining checkpoints from single training trajectory, as explored in LAWA [23] which utilizes model merging to accelerate the LLM training. However, as the model and data scales dramatically, independent researchers struggle to evaluate model mergings impact on large-scale models, mainly due to limited access to intermediate checkpoints from extensive pre-training. Although DeepSeek [25] and LLaMA-3 [12] have both indicated their employment of model merging techniques for model development, detailed information regarding these techniques has not been publicly disclosed. 1 In this work, we mainly focus on model merging during the pre-training stage, introducing Pre-trained Model Average (PMA), novel strategy for model-level weight merging during pre-training. To comprehensively evaluate PMA, we trained diverse set of LLMs of varying sizes and architectures from scratch, including Dense models [12] with parameters spanning from 411M to 70B, as well as Mixture-of-Experts (MoE) architectures [39] with activated/total parameters ranging from 0.7B/7B to 20B/200B. We first investigate the performance impact of PMA and establish systematic evaluations across different phases of the warmup-stable-decay (WSD) learning schedule, which lately becomes popular choice of lr scheduler for LLM pre-training since [16]. Experimental results demonstrate that model merging during the stable training phase yields consistent performance gains at different training steps. More remarkably, applying PMA at early-stage of the cosine-decay phase usually achieve comparable or even superior performance to their final-stage annealing counterparts. These findings suggest that during the extensively lengthy pre-training stage with constant lr, PMA can serve as fast, reliable yet low-cost simulator for the annealed performance, enabling both faster validation cycles and significant computational savings. Building upon our PMA framework, we first evaluate its performance with various prevalent merging strategies, including Simple Moving Average (SMA) [21], Weighted Moving Average (WMA) [33] and Exponential Moving Average (EMA) [18]. Notably, our experiments demonstrate that the performance differences among these methods gradually become negligible. We further investigate how these important factors of PMA, namely, the interval between each merging checkpoint, the number of models involved in merging, and the size of the model, would affect merging performance. Our analysis reveals two important findings: First, the optimal merging interval exhibits clear scaling relationship with model size. Second, incorporating more checkpoints in the merging process consistently improves performance once training is completed. Furthermore, we also investigated whether PMA could produce more effective initialization weights for the consecutive continued training (CT) or supervised fine-tuning (SFT) [43] stages to enhance the downstream model performance. We practically observed that entering CT and SFT stages with PMA applied could yield smoother GradNorm curves, which thus helps stabilize the training dynamics yet without harming the performance, compared to initializing these stages with the latest available checkpoint as usual. This finding inspire novel application of model merging for training stabilization, which we dubbed as PMA-init. We demonstrate that in scenarios when the LLM training experiences severe irrecoverable loss spikes with broken training dynamics, applying PMA-init over preceding checkpoints to resume training, enables reliable recovery from unstable training trajectories. In summary, our paper makes the following key contributions: We present the Pre-trained Model Averaging (PMA) strategy, novel framework for model merging during LLM pre-training. Through extensive experiments across model scales (from millions to over 100B parameters), we demonstrate that merging checkpoints from the stable training phase produces consistent and significant performance improvements. We delved into novel applications of model merging for weight initialization (PMA-init), to help stabilize training process without harming the downstream performance, especially when it suffers from irrecoverable loss spikes with broken training dynamics. Through extensive experiments, we demonstrate the effectiveness of PMA-init on both CT and SFT stages. We also comprehensively ablated various model merging techniques with their associated hyperparameters. Our findings offer the research community practical pre-training guidelines with effective model merging. Nevertheless, the low cost and rapid deployment of PMA also make it reliable and economic monitor for the pre-training process, to flexibly simulate the ultimate model performance after annealing."
        },
        {
            "title": "2 Related Work",
            "content": "Model merging is an emerging field undergoing rapid development, with diverse applications across various domains. Typically, model merging is implemented during the post-training phase [19, 52, 58], where multiple models fine-tuned on different downstream tasks are combined by merging their weights. This process 2 Figure 1 Comparison of downstream task performance for MoE models of varying sizes under stable training, before and after model merging. effectively integrates the distinct capabilities of each individual model, resulting in unified model that exhibits robust and comprehensive performance. Recently, several methods have advanced this field significantly. For instance, Task Arithmetic [19], TiesMerging [47], and AdaMerging [51] integrate Vision Transformer (ViT) models [10] trained on distinct visual classification tasks, producing single model capable of multi-task object classification. MetaGPT [58] frames model merging as multi-task learning problem, aiming to minimize the average loss between the merged model and individual task-specific models. Fisher Merging [30] employs weighted fusion of model parameters, with weights determined by the Fisher information matrix. RegMean [20] elegantly addresses the merging process by formulating it as linear regression problem solvable through closed-form solutions. Evolutionary-modelmerge [2] efficiently optimizes merging coefficients using evolutionary algorithms. Additionally, DARE [52] merges multiple task-specific language models into versatile unified model by randomly dropping and subsequently rescaling the delta parameters. However, research on model merging during the pre-training phase remains relatively limited. Such studies typically refer to incorporating checkpoints within single training trajectory during large language model (LLM) pre-training. For example, LAWA [23] demonstrated that merging checkpoints at intermediate stages can significantly accelerate training, reducing GPU hours by 68 when training ResNet50 [14] model on ImageNet [9] and saving 30 GPU hours for RoBERTa-Base [27] model trained on the WikiText-103 [32] dataset. Sanyal et al. [36] further indicated that checkpoint averaging combined with high learning rate in pre-training trajectories contributes to faster convergence. Additionally, Checkpoint Merging [26] provided comprehensive evaluation of the effectiveness of merging checkpoints at different stages during the pre-training of the Baichuan2 [48] LLM. Furthermore, technical reports of large-scale models such as Deepseek V3 [25] and LLaMA3.1 [12] also mention the use of model merging techniques during pre-training, although detailed methodologies have not been publicly disclosed. This paper primarily explores techniques for model merging within the pre-training paradigm. To the best of our knowledge, this is the first study to provide detailed technical insights into scaling model merging methods to significantly larger model sizes. We also discuss practical approaches for effective model merging and analyze its potential capabilities as well as its limitations."
        },
        {
            "title": "3 Preliminaries",
            "content": "In this section, we describe the fundamental experimental framework, introduce the notations and concepts used in model merging, and present multiple variants of model merging techniques. Experimental setup. In terms of model architecture, we independently trained series of MoE and dense models. We employ Warmup-Stable-Decay (WSD) learning rate scheduler [16], which begins with short warmup period, followed by an extended period of stable training at constant learning rate, and concludes with annealing to relatively small learning rate. The learning rates are determined according to scaling law guidelines [4, 24], employing optimal values for training on an internal pretraining corpus comprising trillions of tokens. Although specific model architectures and datasets have not yet been publicly released, we posit 3 that our findings are not strongly tied to these particular choices, as subsequent experiments primarily focus on MoE structures. Related conclusions for dense models are provided in the Appendix A. For evaluation, we primarily report results on open-source benchmarks in both few-shot and zero-shot settings, including: ARC-Challenge [6], BBH [40], DROP [11], WinoGrande [35], HellaSwag [55], MMLU [15], C-Eval [17], TriviaQA [22], Ape210K [56], GSM8K [7], MATH [56], MBPP [3], HumanEval [5], AGIEval [57], GPQA [34], and MMLU-Pro [42]. The weighted average score across these benchmarks serves as the models comprehensive performance metric. Unless otherwise specified, we report this score as the models performance metric to ensure evaluation reliability. Notions and concepts. Our main focus is on model merging during pre-training, where the merged entities are sequential checkpoints along the training trajectory. Suppose we aim to merge models, with each models parameters denoted as Mi (where ranges from 1 to ). Each model has an associated weighting coefficient wi, and the merged model Mavg is computed as: Mavg = (cid:88) i=1 wiMi. (1) We assume that the data consumption of these models form an arithmetic sequence with common difference , formulated as: = Ti+1 Ti, (2) where Ti represents the cumulative number of tokens consumed by the i-th model. Model merging variants. Model merging techniques vary primarily in how they assign weights (wi) to individual models. This paper examines three popular approaches for weight assignment, namely the Simple Moving Average (SMA), Exponential Moving Average (EMA), and Weighted Moving Average (WMA). The first approach, Simple Moving Average (SMA), treats all models equally. For instance, when combining 10 models, each model is assigned weight of wi = 0.1. The SMA is formulated as: Mavg = 1 (cid:88) i=1 Mi. (3) The second approach, Exponential Moving Average (EMA), emphasizes later models by assigning weights that decay exponentially, making EMA more sensitive to recent changes. The EMA is expressed recursively as: (i) avg = α Mi + (1 α) (i1) avg , [2, ], (4) Here, α, the smoothing factor (typically between 0 and 1), controls the balance between the current model Mi and the previous EMA result (i1) The third approach, Weighted Moving Average (WMA), also prioritizes recent models but uses distinct weighting scheme. In WMA, each model is assigned specific weight, often increasing linearly for later models (e.g., wi = i). The weighted sum is then normalized to compute the average, formulated as follows: avg . Mavg = (cid:88) i=1 wi wsum Mi, wsum = (cid:88) i=1 wi. (5) These methods offer flexible ways to combine models based on their recency and relevance. Choosing the right approach depends on the specific application and desired emphasis on newer data."
        },
        {
            "title": "4 Experiments",
            "content": "In this section, we delve into the experimental core of our study, systematically addressing six critical questions surrounding model merging in the context of pre-training: 1) How does model merging affect performance? 2) 4 How do different merging methods affect final performance? 3) How to determine the optimal interval and number of weights to merge for various model sizes? 4) Do merged pre-trained models contribute to better downstream training? 5) Does model merging improve the stability of training? 6) What processes unfold during model merging? Through these experiments, we aim to provide comprehensive insights into model merging, offering practical guidance for its application and shedding light on its theoretical underpinnings."
        },
        {
            "title": "4.1 How does model merging affect model performance?",
            "content": "Current learning rate schedule methods mainly involve constant learning rates or cosine annealing. In our model pre-training, we employed the Warmup-Stable-Decay (WSD) strategy [16], which combines constant learning rate phase with subsequent cosine decay phase [28]. To explore the effects of model merging under different learning rate schedules, we conducted experiments during both constant learning rate phase and cosine dacay phase. In the constant learning rate phase, we merged fully trained models of various sizes. As shown in Figure 1, the merged models exhibited significant performance improvements across multiple downstream tasks. For example, on the Humaneval benchmark, Seed-MoE-1.3B/13B improved from 31.1 to 36.6 points, and SeedMoE-10B/100B increased from 54.3 to 61.6 points. While larger models showed less pronounced gains on certain benchmarks, such as BBH, this was likely due to the near-saturation of these metrics. Overall, the improvements were robust and consistent across model sizes. Next, we performed model merging in the cosine annealing phase by collecting weights from the annealing stages of Seed-MoE-1.3B/13B, Seed-MoE-10B/100B, and Seed-MoE-15B/150B. As depicted in Figure 2, as the learning rate gradually decreased, the models converged steadily, with performance continuing to improve. Interestingly, at the early annealing stage, the results of PMA were comparable to those at the end of the annealing process. In some cases, particularly for larger models, the merged models even surpassed those naturally annealed. Figure 2 Comparison of overall performance for MoE models of varying sizes under annealing training, before and after model merging. The learning rate follows cosine schedule during the annealing process. The x-axis shows the count of training tokens. These findings raised question: could we simplify the training process by using only the Warmup-Stable phases alongside PMA, skipping the decay phase, and avoiding learning rate adjustments? To investigate, we forked two training runs from the stable phase of Seed-MoE-1.3B/13B at 1.4T tokens. One continued with constant learning rate, while another underwent annealing, each training for an additional 250B tokens. We then merged the models trained with the constant learning rate. As shown in Figure 3, early in training, the merged models significantly outperformed both the constant learning rate and annealed models. Even later, their performance was comparable to the annealed models. This suggests that pre-training with constant learning rate, combined with model merging, can effectively match the performance of an annealed model at any point in the training process without the need for 5 learning rate annealing. This approach accelerates model validation and significantly reduces computational resource demands. Figure 3 Comparison of downstream task performance between model merging results under stable training and the real annealed model. The x-axis shows the count of training tokens."
        },
        {
            "title": "4.2 How do different merging methods affect final performance?",
            "content": "In this section, we systematically evaluate how different merging strategies affect the performance of merged models. Specifically, we focus on three distinct approaches: EMA, WMA, and SMA. The EMA method employs exponentially decaying weights wi = α(1α)N i, giving higher importance to more recent checkpoints. WMA assigns linearly increasing weights wi = i, also prioritizing more recent checkpoints. In contrast, SMA applies uniform weighting, treating all checkpoints equally regardless of their position in the training sequence. We conducted experiments on Seed-MoE-1.3/13B and showed the results in Figure 4. At 204B training tokens, all merging methods enhanced model performance compared to the pre-merged model, but WMA delivered the best results. This suggests that in the early phases of training, when model weights undergo significant changes, assigning higher weights to checkpoints with more training tokens produces superior models. This is further supported by the fact that EMAα=0.2 outperforms EMAα=0.1. However, as training advances to later stages and model weights stabilize, the performance differences between merging methods diminish. For its simplicity and stability, we primarily use SMA for model merging in subsequent experiments."
        },
        {
            "title": "4.3 How to determine the optimal interval and number of weights to merge for various",
            "content": "model sizes? Beyond the merging technique itself, two other factors may also affect the effectiveness of model merging: the interval between selected models and the number of models . We performed ablation studies on the Seed-MoE-1.3/13B model to investigate these effects, starting with the impact of the interval. As illustrated in the upper part of Figure 5, we fixed = 10 and tested intervals of = 4B, 8B, 16B, and 32B. Notably, at 204B with = 32B, we reduced to 6 due to insufficient models. In the early stage of training, at 204B tokens, merged results with = 16B and = 32B underperformed the baseline. This is likely because large intervals incorporated unstable weights from the initial training phase, leading to significant weight disparities and suboptimal outcomes. As training progressed and weights stabilized, the performance gap across different settings gradually narrowed. In practice, the optimal interval scales with model size, following these observed patterns: an interval of around 8B tokens for 1.3B/13B models, 4B tokens for 0.7B/7B models, and approximately 80B tokens for 10B/100B models. This aligns with the tendency of larger models to use larger batch sizes [31]. Next, we set = 8B and explored how the number of merged models affects performance, testing = 3, 6, 10, and 15. As shown in the lower part of Figure 5, early in training, incorporating more models introduced unstable weights, which reduced the performance of merged models. However, once training was complete, merging larger number of models led to significant performance improvements. Notably, 6 Figure 4 Impact of different model merging methods on final model performance. the overall performance for = 3 was nearly 1 point lower than for = 15. To strike balance between computational cost and performance gains, we opted for = 10 in further experiments. Figure 5 Impact of different model merging hyper-parameters on final model performance."
        },
        {
            "title": "4.4 Do merged pre-trained models contribute to better downstream training?",
            "content": "A complete LLM training process typically involves multiple stages, which are pretraining, continual training (CT), supervised fine-tuning (SFT) and reinforcement learning (RL) in sequence. In light of the capacity of PMA to improve pretraining performance, we conjecture that merged pretrained models may similarly prove beneficial for downstream stages. To verify this hypothesis, we initialized downstream training with PMA, which we dubbed as PMA-init, and investigated its impacts over the baselines (which are initialized from their original checkpoints) for both CT and SFT stages. CT stage. We first conducted an ablation study to assess the sensitivity of the PMA-init of the CT stage with varying learning rate schedules. Specifically, we experimented with Seed-MoE-0.7B/7B models merged after stable training on approximately 1 trillion tokens. As illustrated in Figure 6 (left), the initialization weights obtained via PMA consistently achieved marginally lower loss at the initial training phase, against the baseline with the same training configuration. As training progresses, the loss values for models with 7 Figure 6 Comparisons of loss curves (left) and performance metrics (right) during CT stage with varying lr schedules, where cosine scheduler is adopted to decay learning rate from lrpeak to lrend (denoted as lrpeak lrend). PMA and baseline, stand for whether our PMA-init technique is employed or not, respectively. different initialization weights converge to comparable levels. Its worth noting that in the loss curve, the purple line significantly overlaps with the blue line, and the brown line significantly overlaps with the pink line. Another observation is made in the Figure 6 (right), where evaluation on the MMLU benchmark reveals that the PMA-init models outperform the baseline early in training. While these models tend to retain slight performance edge in later stages, their results on other tasks may be slightly suboptimal, leading to overall performance parity with the baseline. Experiments across varied learning rate schedules corroborate these findings, indicating that models converge to similar performance levels by the end of training, and no extensive learning rate tuning is required for PMA-init. SFT stage. We next analyzed the impact of PMA-init on the SFT stage, where the detailed results can be found in the Appendix B. Although initialization with merged weights occasionally yields performance improvements, such gains are not consistently observed. Nonetheless, this approach does not adversely affect downstream training outcomes and may be viable strategy for researchers seeking to enhance model performance."
        },
        {
            "title": "4.5 Does model merging improve the training stability?",
            "content": "In large-scale LLM training, infrastructure issues are almost inevitable and often lead to training instability phenomena such as loss spikes or diverging. Specifically, loss spike occurs when, at specific point during the multi-stage training, the models predictions deteriorate significantly compared to previous iterations. This phenomenon is often observed alongside gradient norm (GradNorm) explosion during backpropagation, which causes large weight updates and eventually lead to irrecoverable spike in its loss function [8]. In the experiments detailed in Section 4.4, as illustrated in Figure 7 (left), we observed that model initialized with PMA-init for SFT stage demonstrated notably more stable GradNorm metric compared to the baseline. This stability is also evident in the reduced frequency of loss spikes relative to the baseline. Since applying PMA-init for downstream training does not impact the models final performance and remains robust across different learning rates, we established series of experiments to explore whether model merging could enhance training stability. Given the extremely high expenses associated, it is unfeasible to conduct direct analysis of training instability in LLM pre-training. Experiments [45] show that small models using relatively large learning rate will exhibit unstable training characteristics similar to those of large models. We thus reproduce the instability phenomena on small models to study the influence of our PMA-init on training stability. In one such experiment, we trained 330M/3.3B MoE model from scratch using an exceptionally high learning rate of 6e-3. As shown in Figure 7 (right), the model overshot the optimal weights, resulting in unstable training and abrupt loss spikes as expected, and was irreversible to its original trajectory. To address this, we adopted PMA-init with three checkpoints saved before the training collapse happened, to resume the pre-training process. As depicted by the red line in Figure 7 (right), the resumed training process stabilized, successfully navigating past the point 8 Figure 7 Left: GradNorm comparisons for SFT training initialized with PMA-init. Right: Comparison of pre-training loss curves between resuming with PMA-init and the original training. of the loss spike and continuing along its original training trajectory. These results highlight that PMA-init can reliably enhance the multi-stage training stability. When loss spike occurs, one can merge the model checkpoints from before the spike and resume training from that point. This approach provides an alternative solution to avoid retraining the model from scratch, thereby substantially reducing the waste of computational resources."
        },
        {
            "title": "4.6 Investigating the Mechanisms of Model Merging",
            "content": "To gain deeper insight into the underlying mechanisms that enable model merging to be effective, we provide both qualitative and quantitative analyses, employing mathematical derivations and visualizations of weight distributions. We begin with second-order Taylor expansion of the loss function L(θ) around an optimal parameter set θ: L(θ) L(θ) + (θ θ)T L(θ) + 1 2 (θ θ)T H(θ θ), (6) where is the Hessian matrix of the loss function evaluated at θ (the matrix of second partial derivatives), which captures curvature information. Since θ is an optimal point, the gradient L(θ) is zero. Thus, the expansion simplifies to: L(θ) L(θ) + 1 2 (θ θ)T H(θ θ). (7) Consider sets of model parameters θ1, θ2, . . . , θk. Let the deviation vector of each model from the optimal parameters be δi = θi θ. The loss for each model can then be approximated as: The average loss of these individual models is: L(θi) L(θ) + 1 2 δT Hδi. 1 (cid:88) i=1 L(θi) L(θ) + 1 2k (cid:88) i=1 δT Hδi. (8) (9) The parameters of the merged model are θavg = 1 optimal parameters is θavg θ = 1 (cid:80)k (cid:80)k i=1 θi. The deviation of this merged model from the i=1 δi. The loss for the merged model is approximated by: L(θavg) L(θ) + (cid:32) 1 2 1 (cid:88) i=1 (cid:33)T (cid:32) δi (cid:33) 1 (cid:88) i=1 δi (10) Expanding the quadratic term: (cid:32) 1 2 1 (cid:88) i= (cid:33)T (cid:32) δi (cid:33) δi = 1 (cid:88) i=1 1 2k2 (cid:88) (cid:88) i=1 j=1 δT Hδj. This can be rewritten by separating diagonal and off-diagonal terms: 1 2k (cid:88) i=1 δT Hδi + δT Hδj . (cid:88) (cid:88) i=1 j=i (11) (12) For the merged model to have lower loss than the average loss of the individual models, i.e., L(θavg) < 1 i=1 L(θi), the following condition must hold: (cid:80)k 1 2k (cid:88) i=1 δT Hδi + δT Hδj < (cid:88) (cid:88) i=1 j=i 1 2k (cid:88) i=1 δT Hδi. Multiplying by 2k2 and rearranging terms, we get: Which simplifies to: (cid:88) i=1 δT Hδi + (cid:88) (cid:88) i= j=i δT Hδj < (cid:88) i=1 δT Hδi. (cid:88) (cid:88) i=1 j=i δT Hδj < (k 1) (cid:88) i= δT Hδi (13) (14) (15) Assuming is positive definite matrix (which is generally true around local minimum), then each term Hδj (for = j) are predominantly Hδi > 0. The inequality is more easily satisfied if the off-diagonal terms δT δT negative. This \"negative correlation\" in the context of the Hessian means that the deviation vectors point in somewhat opposing directions relative to the curvature of the loss landscape. This mathematical analysis can be intuitively interpreted as follows: 1. The effectiveness of model weight merging stems from the fact that different model checkpoints, representing different points in the training trajectory, have explored different local regions or directions within the parameter space. 2. When these explorations exhibit degree of \"complementarity\" concerning the geometric structure of the loss function (captured by the Hessian and Hδj), their average can position the merged model closer to an optimal point than the the cross-terms δT individual models might be on average. 3. This helps explain why merging models, particularly those from stable yet ongoing training phase, often improves performance. The averaging process can smooth out idiosyncrasies of individual checkpoints. This analysis suggests that weight merging is not merely simple averaging of parameters but rather process that can leverage the geometric structure of the loss landscape and the diversity among the models being merged. Additionally, we selected several checkpoints from the pre-training of Seed-MoE-1.3B/13B and visualized the average distribution of two selected parameters from specific layer. Using these points, we generated contour lines for MMLU scores, as illustrated in Figure 8. The weight positions of various individual models are marked as black dots. These dots are distributed along the MMLU score contours, revealing discernible \"complementary\" pattern. The averaged weight position (representative of the merged model) is often situated closer to region of higher MMLU scores (a better optimum) than many individual model checkpoints. This visualization also provides an intuitive explanation for why model merging yields diminished improvements when models are annealed to very low learning rate: at such stage, the models to be merged are already tightly converged within specific local optimum. Merging them essentially averages points within this already narrow basin, making it unlikely to escape to significantly better or different optimal region. 10 Figure 8 Visualization of MMLU score contour lines, comparing the weights of an original model with those of merged model. Black dots represent the parameter locations of various individual model checkpoints."
        },
        {
            "title": "5 Conclusion",
            "content": "This research pioneers deeper exploration of model merging within the challenging pre-training stage of large-scale models. By training spectrum of MoE and Dense models and performing rigorous ablations, we established that merging checkpoints from stable training phases not only yields significant performance gains and predicts annealing but also streamlines development and reduces costs. Our work provides concrete guidance on merging strategies, optimal parameters, and downstream applications, alongside insights into the underlying mechanisms. These contributions equip the open-source community with the knowledge and tools for more efficient model development through pre-training merging."
        },
        {
            "title": "Contributions",
            "content": "Project Lead Yunshui Li1 Algorithm Yunshui Li1, Yiyuan Ma1, Shen Yan1,2, Chaoyi Zhang1, Jing Liu1, Jianqiao Lu1,3, Minrui Wang1, Mengzhao Chen1,3, Xunhao Lai1,2, Jin Ma1, Shiyi Zhan1, Yao Luo1, Xingyan Bin1 Infrastructure Ziwen Xu1, Mingji Han1, Wenhao Hao1, Bairen Yi1, Lingjun Liu1, Bole Ma1, Hongbin Ren1, Xiaoying Jia Supervision Yiyuan Ma1, Xun Zhou1, Liang Xiang1, Yonghui Wu1 Affiliation 1 ByteDance Seed 2 Peking University 3 The University of Hong"
        },
        {
            "title": "6 Acknowledgments",
            "content": "We thank Chengyin Xu, Yantao Du, Xinran Zhao, Deyi Liu, Shuang Wu, Bohong Wu, Yutao Zeng, Chen Zheng, Yuan Yang as well as other colleagues at ByteDance for their support for this project."
        },
        {
            "title": "References",
            "content": "[1] Josh Achiam, Steven Adler, Sandhini Agarwal, Lama Ahmad, Ilge Akkaya, Florencia Leoni Aleman, Diogo Almeida, Janko Altenschmidt, Sam Altman, Shyamal Anadkat, et al. Gpt-4 technical report. arXiv preprint arXiv:2303.08774, 2023. [2] Takuya Akiba, Makoto Shing, Yujin Tang, Qi Sun, and David Ha. Evolutionary optimization of model merging recipes. Nature Machine Intelligence, pages 110, 2025. [3] Jacob Austin, Augustus Odena, Maxwell Nye, Maarten Bosma, Henryk Michalewski, David Dohan, Ellen Jiang, Carrie Cai, Michael Terry, Quoc Le, et al. Program synthesis with large language models. arXiv preprint arXiv:2108.07732, 2021. [4] Xiao Bi, Deli Chen, Guanting Chen, Shanhuang Chen, Damai Dai, Chengqi Deng, Honghui Ding, Kai Dong, Qiushi Du, Zhe Fu, et al. Deepseek llm: Scaling open-source language models with longtermism. arXiv preprint arXiv:2401.02954, 2024. [5] Mark Chen, Jerry Tworek, Heewoo Jun, Qiming Yuan, Henrique Ponde De Oliveira Pinto, Jared Kaplan, Harri Edwards, Yuri Burda, Nicholas Joseph, Greg Brockman, et al. Evaluating large language models trained on code. arXiv preprint arXiv:2107.03374, 2021. [6] Peter Clark, Isaac Cowhey, Oren Etzioni, Tushar Khot, Ashish Sabharwal, Carissa Schoenick, and Oyvind Tafjord. Think you have solved question answering? try arc, the ai2 reasoning challenge. arXiv preprint arXiv:1803.05457, 2018. [7] Karl Cobbe, Vineet Kosaraju, Mohammad Bavarian, Mark Chen, Heewoo Jun, Lukasz Kaiser, Matthias Plappert, Jerry Tworek, Jacob Hilton, Reiichiro Nakano, et al. Training verifiers to solve math word problems. arXiv preprint arXiv:2110.14168, 2021. [8] Jeremy Cohen, Simran Kaur, Yuanzhi Li, Zico Kolter, and Ameet Talwalkar. Gradient descent on neural networks typically occurs at the edge of stability. In International Conference on Learning Representations, 2021. [9] Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li Fei-Fei. Imagenet: large-scale hierarchical image database. In Conference on Computer Vision and Pattern Recognition, pages 248255. Ieee, 2009. [10] Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, Jakob Uszkoreit, and Neil Houlsby. An image is worth 16x16 words: Transformers for image recognition at scale. In International Conference on Learning Representations, 2021. [11] Dheeru Dua, Yizhong Wang, Pradeep Dasigi, Gabriel Stanovsky, Sameer Singh, and Matt Gardner. DROP: reading comprehension benchmark requiring discrete reasoning over paragraphs. In Jill Burstein, Christy Doran, and Thamar Solorio, editors, Proceedings of the Conference of the North American Chapter of the Association for Computational Linguistics, pages 23682378, 2019. [12] Aaron Grattafiori, Abhimanyu Dubey, Abhinav Jauhri, Abhinav Pandey, Abhishek Kadian, Ahmad Al-Dahle, Aiesha Letman, Akhil Mathur, Alan Schelten, Alex Vaughan, et al. The llama 3 herd of models. arXiv e-prints, pages arXiv2407, 2024. [13] Daya Guo, Dejian Yang, Haowei Zhang, Junxiao Song, Ruoyu Zhang, Runxin Xu, Qihao Zhu, Shirong Ma, Peiyi Wang, Xiao Bi, et al. Deepseek-r1: Incentivizing reasoning capability in llms via reinforcement learning. arXiv preprint arXiv:2501.12948, 2025. [14] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recognition. In Conference on Computer Vision and Pattern Recognition, pages 770778, 2016. [15] Dan Hendrycks, Collin Burns, Steven Basart, Andy Zou, Mantas Mazeika, Dawn Song, and Jacob Steinhardt. Measuring massive multitask language understanding. In International Conference on Learning Representations, 2021. [16] Shengding Hu, Yuge Tu, Xu Han, Ganqu Cui, Chaoqun He, Weilin Zhao, Xiang Long, Zhi Zheng, Yewei Fang, Yuxiang Huang, Xinrong Zhang, Zhen Leng Thai, Chongyi Wang, Yuan Yao, Chenyang Zhao, Jie Zhou, Jie Cai, Zhongwu Zhai, Ning Ding, Chao Jia, Guoyang Zeng, dahai li, Zhiyuan Liu, and Maosong Sun. MiniCPM: Unveiling the potential of small language models with scalable training strategies. In First Conference on Language Modeling, 2024. 13 [17] Yuzhen Huang, Yuzhuo Bai, Zhihao Zhu, Junlei Zhang, Jinghan Zhang, Tangjun Su, Junteng Liu, Chuancheng Lv, Yikai Zhang, jiayi lei, Yao Fu, Maosong Sun, and Junxian He. C-eval: multi-level multi-discipline chinese evaluation suite for foundation models. In Conference on Neural Information Processing Systems Datasets and Benchmarks Track, 2023. [18] Stuart Hunter. The exponentially weighted moving average. Journal of quality technology, 18(4):203210, 1986. [19] Gabriel Ilharco, Marco Tulio Ribeiro, Mitchell Wortsman, Ludwig Schmidt, Hannaneh Hajishirzi, and Ali Farhadi. Editing models with task arithmetic. In International Conference on Learning Representations, 2023. [20] Xisen Jin, Xiang Ren, Daniel Preotiuc-Pietro, and Pengxiang Cheng. Dataless knowledge fusion by merging weights of language models. In The International Conference on Learning Representations, 2023. [21] FR Johnston, John Boyland, Maureen Meadows, and Shale. Some properties of simple moving average when applied to forecasting time series. Journal of the Operational Research Society, 50(12):12671271, 1999. [22] Mandar Joshi, Eunsol Choi, Daniel Weld, and Luke Zettlemoyer. TriviaQA: large scale distantly supervised challenge dataset for reading comprehension. In Regina Barzilay and Min-Yen Kan, editors, Proceedings of the Annual Meeting of the Association for Computational Linguistics, pages 16011611, 2017. [23] Jean Kaddour. Stop wasting my time! saving days of imagenet and BERT training with latest weight averaging. In Advances in Neural Information Processing Systems Workshop, 2022. [24] Jared Kaplan, Sam McCandlish, Tom Henighan, Tom Brown, Benjamin Chess, Rewon Child, Scott Gray, Alec Radford, Jeffrey Wu, and Dario Amodei. Scaling laws for neural language models. arXiv preprint arXiv:2001.08361, 2020. [25] Aixin Liu, Bei Feng, Bing Xue, Bingxuan Wang, Bochao Wu, Chengda Lu, Chenggang Zhao, Chengqi Deng, Chenyu Zhang, Chong Ruan, et al. Deepseek-v3 technical report. arXiv preprint arXiv:2412.19437, 2024. [26] Deyuan Liu, Zecheng Wang, Bingning Wang, Weipeng Chen, Chunshan Li, Zhiying Tu, Dianhui Chu, Bo Li, and Dianbo Sui. Checkpoint merging via bayesian optimization in llm pretraining. arXiv preprint arXiv:2403.19390, 2024. [27] Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi, Danqi Chen, Omer Levy, Mike Lewis, Luke Zettlemoyer, and Veselin Stoyanov. Roberta: robustly optimized bert pretraining approach. arXiv preprint arXiv:1907.11692, 2019. [28] Ilya Loshchilov and Frank Hutter. SGDR: Stochastic gradient descent with warm restarts. In International Conference on Learning Representations, 2017. [29] Haipeng Luo, Qingfeng Sun, Can Xu, Pu Zhao, Jian-Guang Lou, Chongyang Tao, Xiubo Geng, Qingwei Lin, Shifeng Chen, Yansong Tang, and Dongmei Zhang. Wizardmath: Empowering mathematical reasoning for large language models via reinforced evol-instruct. In International Conference on Learning Representations, 2025. [30] Michael Matena and Colin Raffel. Merging models with fisher-weighted averaging. Advances in Neural Information Processing Systems, 35:1770317716, 2022. [31] Sam McCandlish, Jared Kaplan, Dario Amodei, and OpenAI Dota Team. An empirical model of large-batch training. arXiv preprint arXiv:1812.06162, 2018. [32] Stephen Merity, Caiming Xiong, James Bradbury, and Richard Socher. Pointer sentinel mixture models. In International Conference on Learning Representations, 2017. [33] Marcus Perry. The weighted moving average technique. Wiley Encyclopedia of Operations Research and Management Science, 2010. [34] David Rein, Betty Li Hou, Asa Cooper Stickland, Jackson Petty, Richard Yuanzhe Pang, Julien Dirani, Julian Michael, and Samuel Bowman. Gpqa: graduate-level google-proof q&a benchmark. In First Conference on Language Modeling, 2024. [35] Keisuke Sakaguchi, Ronan Le Bras, Chandra Bhagavatula, and Yejin Choi. Winogrande: An adversarial winograd schema challenge at scale. Communications of the ACM, 64(9):99106, 2021. [36] Sunny Sanyal, Atula Tejaswi Neerkaje, Jean Kaddour, Abhishek Kumar, and sujay sanghavi. Early weight averaging meets high learning rates for LLM pre-training. In First Conference on Language Modeling, 2024. 14 [37] ByteDance Seed, Yufeng Yuan, Yu Yue, Mingxuan Wang, Xiaochen Zuo, Jiaze Chen, Lin Yan, Wenyuan Xu, Chi Zhang, Xin Liu, et al. Seed-thinking-v1. 5: Advancing superb reasoning models with reinforcement learning. arXiv preprint arXiv:2504.13914, 2025. [38] Zhihong Shao, Peiyi Wang, Qihao Zhu, Runxin Xu, Junxiao Song, Xiao Bi, Haowei Zhang, Mingchuan Zhang, YK Li, Wu, et al. Deepseekmath: Pushing the limits of mathematical reasoning in open language models. arXiv preprint arXiv:2402.03300, 2024. [39] Noam Shazeer, *Azalia Mirhoseini, *Krzysztof Maziarz, Andy Davis, Quoc Le, Geoffrey Hinton, and Jeff Dean. Outrageously large neural networks: The sparsely-gated mixture-of-experts layer. In International Conference on Learning Representations, 2017. [40] Mirac Suzgun, Nathan Scales, Nathanael Schärli, Sebastian Gehrmann, Yi Tay, Hyung Won Chung, Aakanksha Chowdhery, Quoc Le, Ed Chi, Denny Zhou, and Jason Wei. Challenging BIG-bench tasks and whether chain-ofthought can solve them. In Anna Rogers, Jordan Boyd-Graber, and Naoaki Okazaki, editors, Findings of the Association for Computational Linguistics, pages 1300313051, 2023. [41] Gemini Team, Petko Georgiev, Ving Ian Lei, Ryan Burnell, Libin Bai, Anmol Gulati, Garrett Tanzer, Damien Vincent, Zhufeng Pan, Shibo Wang, et al. Gemini 1.5: Unlocking multimodal understanding across millions of tokens of context. arXiv preprint arXiv:2403.05530, 2024. [42] Yubo Wang, Xueguang Ma, Ge Zhang, Yuansheng Ni, Abhranil Chandra, Shiguang Guo, Weiming Ren, Aaran Arulraj, Xuan He, Ziyan Jiang, et al. Mmlu-pro: more robust and challenging multi-task language understanding benchmark. In The Conference on Neural Information Processing Systems Datasets and Benchmarks Track, 2024. [43] Jason Wei, Maarten Bosma, Vincent Zhao, Kelvin Guu, Adams Wei Yu, Brian Lester, Nan Du, Andrew M. Dai, and Quoc Le. Finetuned language models are zero-shot learners. In International Conference on Learning Representations, 2022. [44] Colin White, Samuel Dooley, Manley Roberts, Arka Pal, Benjamin Feuer, Siddhartha Jain, Ravid Shwartz-Ziv, Neel Jain, Khalid Saifullah, Sreemanti Dey, Shubh-Agrawal, Sandeep Singh Sandha, Siddartha Venkat Naidu, Chinmay Hegde, Yann LeCun, Tom Goldstein, Willie Neiswanger, and Micah Goldblum. Livebench: challenging, contamination-limited LLM benchmark. In International Conference on Learning Representations, 2025. [45] Mitchell Wortsman, Peter Liu, Lechao Xiao, Katie Everett, Alexander Alemi, Ben Adlam, John Co-Reyes, Izzeddin Gur, Abhishek Kumar, Roman Novak, Jeffrey Pennington, Jascha Sohl-Dickstein, Kelvin Xu, Jaehoon Lee, Justin Gilmer, and Simon Kornblith. Small-scale proxies for large-scale transformer training instabilities. In International Conference on Learning Representations, 2024. [46] Can Xu, Qingfeng Sun, Kai Zheng, Xiubo Geng, Pu Zhao, Jiazhan Feng, Chongyang Tao, Qingwei Lin, and Daxin Jiang. WizardLM: Empowering large pre-trained language models to follow complex instructions. In The International Conference on Learning Representations, 2024. [47] Prateek Yadav, Derek Tam, Leshem Choshen, Colin Raffel, and Mohit Bansal. TIES-merging: Resolving interference when merging models. In Conference on Neural Information Processing Systems, 2023. [48] Aiyuan Yang, Bin Xiao, Bingning Wang, Borong Zhang, Ce Bian, Chao Yin, Chenxu Lv, Da Pan, Dian Wang, Dong Yan, et al. Baichuan 2: Open large-scale language models. arXiv preprint arXiv:2309.10305, 2023. [49] An Yang, Baosong Yang, Beichen Zhang, Binyuan Hui, Bo Zheng, Bowen Yu, Chengyuan Li, Dayiheng Liu, Fei Huang, Haoran Wei, et al. Qwen2. 5 technical report. arXiv preprint arXiv:2412.15115, 2024. [50] Enneng Yang, Li Shen, Guibing Guo, Xingwei Wang, Xiaochun Cao, Jie Zhang, and Dacheng Tao. Model merging in llms, mllms, and beyond: Methods, theories, applications and opportunities. arXiv preprint arXiv:2408.07666, 2024. [51] Enneng Yang, Zhenyi Wang, Li Shen, Shiwei Liu, Guibing Guo, Xingwei Wang, and Dacheng Tao. Adamerging: Adaptive model merging for multi-task learning. In International Conference on Learning Representations, 2024. [52] Le Yu, Bowen Yu, Haiyang Yu, Fei Huang, and Yongbin Li. Language models are super mario: Absorbing abilities from homologous models as free lunch. In International Conference on Machine Learning, 2024. [53] Qiying Yu, Zheng Zhang, Ruofei Zhu, Yufeng Yuan, Xiaochen Zuo, Yu Yue, Tiantian Fan, Gaohong Liu, Lingjun Liu, Xin Liu, et al. Dapo: An open-source llm reinforcement learning system at scale. arXiv preprint arXiv:2503.14476, 2025. [54] Yufeng Yuan, Qiying Yu, Xiaochen Zuo, Ruofei Zhu, Wenyuan Xu, Jiaze Chen, Chengyi Wang, TianTian Fan, Zhengyin Du, Xiangpeng Wei, et al. Vapo: Efficient and reliable reinforcement learning for advanced reasoning tasks. arXiv preprint arXiv:2504.05118, 2025. [55] Rowan Zellers, Ari Holtzman, Yonatan Bisk, Ali Farhadi, and Yejin Choi. Hellaswag: Can machine really finish your sentence? arXiv preprint arXiv:1905.07830, 2019. [56] Wei Zhao, Mingyue Shang, Yang Liu, Liang Wang, and Jingming Liu. Ape210k: large-scale and template-rich dataset of math word problems. arXiv preprint arXiv:2009.11506, 2020. [57] Wanjun Zhong, Ruixiang Cui, Yiduo Guo, Yaobo Liang, Shuai Lu, Yanlin Wang, Amin Saied, Weizhu Chen, and Nan Duan. AGIEval: human-centric benchmark for evaluating foundation models. In Kevin Duh, Helena Gomez, and Steven Bethard, editors, Findings of the Association for Computational Linguistics: NAACL, pages 22992314, 2024. [58] Yuyan Zhou, Liang Song, Bingning Wang, and Weipeng Chen. MetaGPT: Merging large language models using model exclusive task arithmetic. In Yaser Al-Onaizan, Mohit Bansal, and Yun-Nung Chen, editors, Proceedings of the Conference on Empirical Methods in Natural Language Processing, pages 17111724, 2024."
        },
        {
            "title": "A The Effect of Model Merging in Dense Models",
            "content": "We also conducted model merging experiments on Dense architecture models, ranging from small Seed-Dense411M models to large Seed-Dense-70B models. Since the 411M and 2B models were not sufficiently trained, we used configuration of N=6 for merging, with weight intervals (V) of 2B and 5B tokens, respectively. For the 8B and 70B models, which were trained more thoroughly, we used N=10, with values of 15B and 40B for merging. As shown in Figure 9, models of different sizes achieved significant improvements on downstream tasks after model merging. Notably, the performance gains of larger models were not smaller than those of smaller models. Specifically, Seed-Dense-70B improved from 50.6 to 57.9 on humaneval and from 85.9 to 91.3 on GSM8K. This further validates the robustness and generalization ability of PMA, demonstrating that it can work across different model architectures and sizes. Figure 9 Comparison of downstream task performance for dense models of varying sizes under stable training, before and after model merging. Model Merging at the CT Stage for Supervised Fine-Tuning We conducted an ablation study to assess the sensitivity of the PMA-init during the SFT stage to varying learning rate schedules. This study included experiments on merged Seed-MoE-15B/150B models following stable training on approximately 16T tokens, as well as after further training on 1T tokens with cosine annealing. We conducted SFT training for 220M tokens using both the original weights and PMA-init weights. For the original weights, we used cosine learning rate schedule with an initial learning rate of 2e-5 and an end learning rate of 2e-6. For the PMA-init weights, we used cosine schedules with initial learning rates of 1e-5, 2e-5, and 4e-5, all with an end learning rate of 2e-6. We evaluated the trained models using Open-Benchmark, which includes MMLU [15], LiveBench [44], AMC-2023, GPQA [34] and LiveCodeBench [44], as well as our in-house evaluation set comprising OOD, Reasoning, and Instruction Following assessments. As shown in Table 1, with Table 1 Comparisons of performance metrics during SFT stage with varying lr schedules, where cosine scheduler is adopted to decay learning rate from lrpeak to lrend (denoted as lrpeak lrend). PMA and baseline, stand for whether our PMA-init technique is employed or not, respectively. IF refers to Instruction Following. Model Open-Benchmark In-house Evaluation MMLU LiveBench AMC-2023 GPQA LiveCodeBench OOD Reasoning IF Baseline2e52e6 PMA2e52e6 PMA1e52e6 PMA4e52e6 86.8 87. 87.2 87.0 50.5 52.0 53.2 51.3 61.0 64.0 65.5 61.4 55.2 54.0 54.4 54. 39.7 39.4 39.7 39.2 32.6 34.7 33.8 31.8 32.1 34.0 33.2 32. 36.3 38.8 37.3 37.2 17 the same learning rate, PMA-init significantly outperformed the baseline on both Open-Benchmark and our in-house evaluations. Notably, on the in-house evaluation set, we observed improvements of over two points in OOD and Instruction Following, and 1.9-point increase in Reasoning. In the other two experiments with different learning rates, we also saw some degree of improvement compared to the baseline, especially with PMA1e52e6 , which showed gains of 2.7 points on Livebench and 4.5 points on AMC-2023. However, we were unable to replicate such significant gains in subsequent experiments with other model sizes, although it did not negatively impact the final downstream model performance. Therefore, as low-cost approach, PMA-init is worth trying to obtain more powerful downstream model."
        },
        {
            "title": "C Limitations",
            "content": "In our study, we thoroughly investigated the potential of model merging in the pre-training phase, offering significant advantages for teams working on large-scale model pre-training to pursue more daring explorations. This is due to the fact that model merging can replicate the benefits of simulated annealing, greatly shortening the exploration period during pre-training. While our experiments were extensive, certain aspects still remain open for deeper research. In our experiments, we defaulted to using the optimal learning rate derived from the scaling law for model training, without extensively exploring the impact of learning rate on model merging. In our practice, we believe that training with higher learning rate could lead to better model through model merging, which aligns with the findings in [36]. However, due to the high computational cost, we did not further quantify the impact of learning rate on model merging in more detailed manner. Additionally, this paper primarily focuses on the application of model merging in pre-training. In reality, due to innovations in RL algorithms [38, 53, 54], RL training has become more stable and often involves longer training cycles, during which series of adjacent weights can be obtained. This paper does not investigate model merging in the context of post-training scenarios, and we leave this aspect for future research."
        }
    ],
    "affiliations": []
}