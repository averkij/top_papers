{
    "paper_title": "Training Vision-Language Process Reward Models for Test-Time Scaling in Multimodal Reasoning: Key Insights and Lessons Learned",
    "authors": [
        "Brandon Ong",
        "Tej Deep Pala",
        "Vernon Toh",
        "William Chandra Tjhi",
        "Soujanya Poria"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Process Reward Models (PRMs) provide step-level supervision that improves the reliability of reasoning in large language models. While PRMs have been extensively studied in text-based domains, their extension to Vision Language Models (VLMs) remains limited. Existing Vision-Language PRMs (VL-PRMs) rely on Monte Carlo Tree Search (MCTS) for data construction, which can often produce noisy supervision signals and limit generalization across tasks. In this work, we aim to elucidate the design space of VL-PRMs by exploring diverse strategies for dataset construction, training, and test-time scaling. First, we introduce a hybrid data synthesis framework that combines MCTS with judgments from a strong VLM, producing more accurate step-level labels. Second, we propose perception-focused supervision, enabling our PRM to explicitly detect errors at the visual grounding stage of reasoning. Third, we systematically evaluate multiple test-time scaling strategies, showing that our PRMs can reliably guide VLMs toward more accurate solutions. Our experiments covering five diverse multimodal benchmarks (MMMU, PuzzleVQA, AlgoPuzzleVQA, MathVista, and MathVision) reveal several key insights: (i) VL-PRMs when used as Outcome Reward Models (ORMs) during test-time scaling (TTS) can outperform VL-PRM guided process step selection, (ii) smaller VL-PRMs can match or even surpass larger ones in detecting process errors, (iii) VL-PRMs uncover latent reasoning abilities in stronger VLM backbones, (iv) perception-level supervision leads to significant gains in test-time scaling, and (v) TTS performance of different policies improve on advanced math reasoning datasets despite not training VL-PRMs on such datasets. We hope our work will motivate further research and support the advancement of VLMs."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 1 ] . [ 2 0 5 2 3 2 . 9 0 5 2 : r TRAINING VISION-LANGUAGE PROCESS REWARD MODELS FOR TEST-TIME SCALING IN MULTIMODAL REASONING: KEY INSIGHTS AND LESSONS LEARNED Brandon Ong AI Singapore Tej Deep Pala Nanyang Technological University Vernon Toh Nanyang Technological University William Chandra Tjhi AI Singapore Soujanya Poria Nanyang Technological University Code: https://github.com/theogbrand/vlprm QWEN-VL-PRM-3B: https://huggingface.co/datasets/ob11/VL-PRM300K QWEN-VL-PRM-7B: https://huggingface.co/ob11/Qwen-VL-PRM-7B VL-PRM300K: https://huggingface.co/ob11/Qwen-VL-PRM-3B"
        },
        {
            "title": "ABSTRACT",
            "content": "Process Reward Models (PRMs) provide step-level supervision that improves the reliability of reasoning in large language models. While PRMs have been extensively studied in text-based domains, their extension to Vision Language Models (VLMs) remains limited. Existing Vision-Language PRMs (VL-PRMs) rely on Monte Carlo Tree Search (MCTS) for data construction, which can often produce noisy supervision signals and limit generalization across tasks. In this work, we aim to elucidate the design space of VL-PRMs by exploring diverse strategies for dataset construction, training, and test-time scaling. First, we introduce hybrid data synthesis framework that combines MCTS with judgments from strong VLM, producing more accurate step-level labels. Second, we propose perception-focused supervision, enabling our PRM to explicitly detect errors at the visual grounding stage of reasoning. Third, we systematically evaluate multiple test-time scaling strategies, showing that our PRMs can reliably guide VLMs toward more accurate solutions. Our experiments covering five diverse multimodal benchmarks (MMMU, PuzzleVQA, AlgoPuzzleVQA, MathVista, and MathVision) reveal several key insights: (i) VL-PRMs when used as Outcome Reward Models (ORMs) during test-time scaling (TTS) can outperform VL-PRM guided process step selection, (ii) smaller VL-PRMs can match or even surpass larger ones in detecting process errors, (iii) VL-PRMs uncover latent reasoning abilities in stronger VLM backbones, (iv) perception-level supervision leads to significant gains in test-time scaling, and (v) TTS performance of different policies improve on advanced math reasoning datasets despite not training VL-PRMs on such datasets. We hope our work will motivate further research and support the advancement of VLMs."
        },
        {
            "title": "INTRODUCTION",
            "content": "Chain-of-thought (CoT) (Wei et al., 2022) prompting has opened up new possibilities for Large Language Models (LLMs). Prior research demonstrates that CoT-based training can substantially improve LLM performance. central challenge lies in obtaining high-quality training data, often addressed through distillation, where teacher models generate step-by-step reasoning traces. This approach has proven effective in models like DeepSeek-R1. In such settings, effective process supervision becomes crucial. Rather than verifying only the final answer, it provides guidance and correction at every intermediate step, thereby ensuring logical consistency and factual reliability throughout the reasoning chain. Distillation data can be enhanced 1 with process-level annotations, where each reasoning step is labeled as correct or incorrect, enabling the training of Process Reward Models (PRMs), which serve as reward functions for reinforcement learning (RL) with policy LLMs. Beyond training, PRMs offer two key benefits: (1) identifying reasoning errors and (2) enabling testtime scaling by guiding inference. This is particularly important because LLMs remain vulnerable to hallucinations and subtle logical mistakes in multi-step reasoning tasks, such as mathematical problem solving (Wang et al., 2023; Zheng et al., 2024). Traditional outcome-only verifiers (Outcome Reward Models) are limited, evaluating only the final answer and often missing intermediate errors that compromise the reasoning trajectory (Wang et al., 2024). In contrast, PRMs can detect such errors and translate them into effective guidance. During inference, PRMs can be leveraged to filter out flawed reasoning paths and steer generation toward more coherent and accurate solutions (Lightman et al., 2023; Zhang et al., 2025b). Although research on PRMs has made significant progress, most existing work has focused on text-based tasks such as mathematical reasoning. More recently, (Wang et al., 2025) introduced PRM tailored to vision-language models (VLMs). Their dataset was constructed using Monte Carlo Tree Search (MCTS), where the quality of reasoning step was scored by the ratio of successful versus unsuccessful solution paths emanating from that step. This MCTS-based scoring strategy has also been widely explored in the math-focused PRM research (Zhao et al., 2025). Despite Wang et al. (2025)s progress, several key research questions on VL-PRMs remain open: Beyond MCTS for dataset construction. Wang et al. (2025) relied primarily on MC-score-based dataset construction, which strongly influenced PRM performance. Our central question is whether superior performance can be achieved by using stronger VLMs to judge each reasoning step, and subsequently employing these judgments as supervision signals for training VL-PRMs. Does VL-PRM size matter? Can smaller VL-PRMs match the performance of larger ones? Strategies for test-time scaling. How can VL-PRMs be best leveraged during inference? Generalization across reasoning tasks. While Wang et al. (2025) evaluated primarily on mathheavy datasets requiring expert subject knowledge, other reasoning domains remain underexplored. In particular, abstract reasoning tasks highlight human strengths but expose persistent weaknesses in VLMs. We ask whether VL-PRMs can bridge this gap. The role of structured reasoning. Prior studies Chia et al. (2024) suggest that perception is major bottleneck in multimodal reasoning. Can VL-PRMs be designed to capture perception errors explicitly, and would such process-level supervision improve test-time scaling performance? In this work, we expand the VL-PRM design space, offering insights across the full pipelinefrom dataset construction and training to evaluation on diverse reasoning tasks. Training data source. Wang et al. (2025) train their model using advanced math datasets such as GeoQA+ (Cao & Xiao, 2022), CLEVR-Math (Lindstrom & Abraham, 2022), Geometry3K (Lu et al., 2021), GEOS (Seo et al., 2015), GeomVerse (Kazemi et al., 2023), and Geo170K (Gao et al., 2025). They evaluate on similarly complex benchmarks (MathVista, Dynamath, Wemath), yielding improved error detection, but this is expected given the overlap between training and evaluation. In contrast, we test whether training solely on general VQA and abstract reasoning, without advanced math datasets, can still enhance mathematical reasoning under test-time scaling (TTS). Our results confirm this: models trained this way consistently improve both TTS performance and error detection on advanced benchmarks, suggesting that general VQA and abstract reasoning tasks strengthen logical error detection, which in turn benefits mathematical reasoning under TTS. Hybrid step evaluation. While we also use MCTS to construct data, we depart from relying solely on MC-score-based metrics as in Wang et al. (2025). Instead, we additionally leverage strong multimodal LLM (o4-mini) to directly judge the correctness of each step. This allows us to investigate whether LLM-based judgments provide more reliable process supervision than MC-score-based labeling. We are publicly releasing both the datasets 1. Test-time scaling strategies. We evaluate three VL-PRM-based methods: (1) Greedy, which iteratively selects the best next step at each decoding stage based on VL-PRM scores; (2) One1 shot, which scores complete solutions and chooses the highest-scoring one; and (3) Step-score Aggregation, which averages step-level scores to select the final solution. Perception-focused supervision. Prior work, such as Chia et al. (2024), has shown that VLMs often struggle at the perception stage of multimodal reasoning even before doing inductive and deductive reasoning. Motivated by this, we train our VL-PRM to explicitly evaluate perception steps, which substantially improves test-time scaling performance of policy VLMs. Backbone sensitivity. We observe that different VLM backbones show varying degrees of reasoning capability when guided by PRMs at test time. For instance, Qwen-2.5-VL-7B and Gemma3-12B achieve comparable performance on PuzzleVQA and AlgoPuzzleVQA under standard evaluation, yet with PRM-driven test-time scaling, Gemma3-12B outperforms Qwen-2.5-VL-7B by 815%. This highlights the role of PRMs in uncovering latent reasoning abilities of VLMs. Comprehensive evaluation. In contrast to Wang et al. (2025), we evaluate our approach on five diverse multimodal benchmarks covering various types of reasoning: MMMU, which requires expert subject knowledge; PuzzleVQA, targeting abstract reasoning; AlgoPuzzleVQA, focused on algorithmic reasoning; and MathVista & MathVision, designed for mathematical problem solving. VL-PRM sizes. We experiment with two VL-PRMs of different scales3B and 7B parametersinitialized from Qwen-2.5-VL-3B and Qwen-2.5-VL-7B, respectively. Our results show that the smaller 3B model achieves comparable test-time scaling performance to the 7B model and outperforms the 7B variant by 5% in detecting process errors. Figure 1: Visual and reasoning errors detected by MC scoring and o4-mini judge in VL-PRM300K."
        },
        {
            "title": "2 METHOD",
            "content": "Data scarcity is key challenge for training PRMs. While prior work uses MCTS-based scoring i.e., it assigns an MC-score to each reasoning step, e.g., MCScore(i) = # correct completions from step # sampled completions from step to build datasets like VISUALPRM400K (Wang et al., 2025), this dataset does not distinguish between perception and higher-level reasoning steps. To address this, we create new dataset, VL-PRM300K, with structured traces that explicitly separate these stages. Additionally, we exclude advanced math datasets from training to study their impact on the generalizability of VL-PRMs across diverse multimodal reasoning tasks. 2.1 CONSTRUCTING VL-PRM300K Let = {I1, I2, . . .} denote the image set and the question set. For pair (I, q) Q, policy VLM π is prompted to produce stepwise solution = (s1, s2, . . . , sT ), where si denotes the i-th reasoning step. Under MCTS, from any prefix si (s1, . . . , si) we expand continuations by rolling out the policy until termination: s>i π( I, q, si) . The quality of step si can then be assessed either by (i) the MCTS-derived MCScore(i) as above, or (ii) an external judge VLM. 3 Structured Reasoning. Prior work has identified perception as key bottleneck (Chia et al., 2024) in multimodal models. In particular, several studies show 2 (Jia et al., 2024) that prompting models to first describe or explain the image before attempting problem-solving significantly improves reasoning performance, as the additional grounding helps anchor subsequent inference (Wu et al., 2025; Thawakar et al., 2025) 3. However, errors introduced at the perception stage can propagate through the reasoning process, ultimately degrading overall accuracy. Motivated by this observation, we design our VL-PRMs to evaluate not only reasoning steps but also perception steps, enabling them to detect and penalize errors at both stages of the structured reasoning pipeline. Using o4-mini as Judge. For judge-based supervision, we query judge VLM (here, o4-mini) with the tuple (I, q, s<i, si) to decide whether step si is correct given its context. Concretely, emits binary label yi {correct, incorrect} along with brief rationale. We use these judge labels to provide process-level supervision signals for training VL-PRMs. Following how PRMs are used at test-time, once step is labeled as negative, we discard all subsequent steps. Dataset Statistics. Our dataset comprises approximately 300K (I, q, S) pairs, with overall statistics reported in Table 1. We sampled from six datasets covering diverse visual question answering (VQA) skills: document and chart understanding (DVQA); OCR (InfoVQA); general commonsense knowledge (VQAv2); grade-school level science (AI2D); grade-school level math reasoning (CLEVRMath); and abstract reasoning (RAVEN). During dataset construction, we generate 5 solutions per imagequestion pair using GPT-4.1, with temperature of 1.0 and 16384 maximum new tokens. We select traces that adhere strictly to our structured perception-reasoning prompt format shown in Section A.4, discarding all invalid traces. Following the MCTS methodology described above, we split the perception and reasoning step traces into standalone steps, and for every step, we sample 16 continuations with prior steps (if any) included as prefixes. Finally, we query judge VLM (here, o4-mini) to determine whether each step is correct given its context. The resulting VL-PRM300K dataset comprises approximately 300K samples, containing 77% of samples with an incorrect step found in the reasoning trace and 23% of samples with all steps verified correct, reported in Table 9. On average, each solution contains 4.55 steps, yielding an estimate of 1.32M step-level samples. Among these steps, about 17% are incorrect steps, with full details in Table 10. To study the effect of label imbalance, we additionally construct balanced variant of VL-PRM300K. Statistics Total Samples - DVQA - InfoVQA - VQAv2 - AI2D - CLEVR-Math - RAVEN Constructing VL-PRM300K-balanced. As reported in Table 1, VL-PRM300K is highly imbalanced, with the majority of samples containing samples with an incorrect step found in the step-bystep reasoning trace. Among the samples in VLPRM300K, the majority of steps are labeled as correct, consistent with the observations in Wang et al. (2025). Among the error labels, approximately 86% correspond to perception errors, while the remaining 14% correspond to reasoning errors. To construct balanced dataset with equal proportions of correct and incorrect samples, we first aggregate all samples from all datasets, excluding RAVEN. Next, we sample all the correct samples from RAVEN, since RAVEN contains significantly more samples with an incorrect step in its reasoning trace than samples with verified correct steps. Finally, we randomly select samples with incorrect steps from RAVEN to match the total number of correct samples, achieving balanced distribution of correct and incorrect training samples, as summarized in Table 11. Number 290K 16K 22K 7K 21K 25K 199K 77% 23% 1.32M 1.10M 0.22M 86% 14% 4.55 Table 1: Statistics of VL-PRM300K. Incorrect Samples Correct Samples Total Steps - Perception Errors - Reasoning Error Avg. Number of Steps per Solution - Correct Steps - Incorrect Steps 2https://cloud.google.com/vertex-ai/generative-ai/docs/multimodal/ design-multimodal-prompts#tailor_the_models_response_to_input 3https://andrewmayne.com/2024/03/12/improving-gpt-4s-visual-reasoning-with-prompting/"
        },
        {
            "title": "2.2 TRAINING VL-PRMS",
            "content": "Training. VL-PRMs are trained on VL-PRM300K to predict + or as step-level judgments. Training minimizes the cross-entropy loss over these label tokens for every step. The input to the VL-PRM is {I, q, s0, p}. Here, the prompt is the system prompt for VL-PRM. Best-of-N Inference. Our VL-PRMs are used as critic model to judge the Chain-of-Thought (CoT) step-by-step responses generated by the policy model. At inference time, the Vision-Language Process Reward Model (VL-PRM) receives an imagequestion pair (I, q) along with partial reasoning trajectory {s1, . . . , si1}, and assigns score to the candidate step si. The response that receives the highest score is chosen. We investigate three different strategies for integrating VL-PRMs into test-time scaling (Figure 4): , . . . , s(N ) Guided Greedy Search. Given (I, q), the policy π samples candidate next steps {s(1) }. Each candidate step is scored by the VL-PRM as Score(s(j) ) = Pθ(+ I, q, si), where Pθ is the VL-PRMs probability of the step being correct. The highest-scoring step is selected, and π continues generation conditioned on that step. One-shot Search. Instead of evaluating individual steps, the policy π generates complete candidate solutions {S(1), . . . , S(N )}. VL-PRM scores each full solution in single pass: Score(S(j)) = Pθ(+ I, q, S(j)). This approach avoids per-step evaluations, making it computationally cheaper than guided greedy search. 1 , . . . , s(j) Step-Score Aggregation. The policy π first produces full solutions {S(1), . . . , S(N )}, where each = Pθ(+ I, q, s(j) solution S(j) = (s(j) ), which is converted into weighted score: s(j) = 1 p(j) . The overall solution score is then computed by aggregating step-level scores, e.g., using the mean: Score(S(j)) = 1 ). VL-PRM assigns per-step probability p(j) ) = p(j) . VL-PRM selects the solution with the highest aggregated score. + 0 (1 p(j) i=1 s(j) (cid:80)T The policy models are instructed to do structured reasoning, where they first explain the image and then perform reasoning to solve the problem. 2.3 CONSTRUCTING PERCEPTIONPROCESSBENCH We use VisualProcessBench to evaluate our VL-PRMs ability to detect step-level visual reasoning errors. However, assessing their performance on perception error detection requires dataset with explicit correctness annotations for perception steps. To address this, we synthetically construct PERCEPTIONPROCESSBENCH from PuzzleVQA. In PuzzleVQA, each problem includes groundtruth perception details. To generate negative examples, we prompt GPT-4.1 to minimally alter the original perception step by either subtly modifying correct detail or introducing small, plausible error relevant to the problem. We specifically instruct the model to make only single, subtle change, ensuring the resulting error is challenging for the VL-PRM to detect. Larger or multiple changes would make the error too obvious. As the original perception details are created using templates, to include variability in the steps, we further rephrase them using GPT-4.1. The exact prompt used for generating these mutations is provided in the Appendix. For each problem, we create one negative example, resulting in dataset of 1,000 positive and 1,000 negative perception steps."
        },
        {
            "title": "3 EXPERIMENTS AND RESULTS",
            "content": "3.1 EXPERIMENTAL SETTINGS Backbones. We fine-tune Qwen-2.5-VL-3B and Qwen-2.5-VL-7B on the VL-PRM300K dataset, resulting in two models: QWEN-VL-PRM-3B and QWEN-VL-PRM-7B. Evaluation Datasets. To evaluate these models under test-time scaling, we select five benchmark datasets that probe distinct aspects of multimodal reasoning: (i) MMMU, which requires expert-level subject knowledge, (ii) PuzzleVQA, designed to assess abstract reasoning, (iii) AlgoPuzzleVQA, targeting algorithmic reasoning, and (iv) MathVista & MathVision, which evaluates mathematical 5 Table 2: Performance comparison across model families on visual reasoning datasets with Visual Process Reward Models (QWEN-VL-PRM-3B and QWEN-VL-PRM-7B). Improvements are shown in green with gains over base models. Model MMMU PuzzleVQA AlgoPuzzleVQA MathVista MathVision Overall Commercial Models GPT-4o o1 o3 Qwen-2.5-VL Family Qwen-2.5-VL-3B 70.7 78.2 82.9 51.7 + QWEN-VL-PRM-7B 53.7 (+2.0) Qwen-2.5-VL-7B 55. + QWEN-VL-PRM-3B 57.6 (+2.6) + QWEN-VL-PRM-7B 57.4 (+2.4) Qwen-2.5-VL-32B 66.0 + QWEN-VL-PRM-3B 67.0 (+1.0) + QWEN-VL-PRM-7B 67.6 (+1.6) Gemma3 Family Gemma3-12B 57.6 + QWEN-VL-PRM-3B 60.4 (+2.8) + QWEN-VL-PRM-7B 60.2 (+2.6) Gemma3-27B 62.9 + QWEN-VL-PRM-3B 65.5 (+2.6) + QWEN-VL-PRM-7B 64.5 (+1.6) 60.0 78.9 84. 57.8 54.4 62.3 30.9 73.9 86.8 31.2 60.3 50.1 69.1 34.5 44.9 (+10.5) 48.0 55.5 (+7.5) 54.8 (+6.8) 46.2 67.1 (+20.8) 66.8 (+20.6) 45.0 57.7 (+12.7) 59.0 (+12.0) 50.8 67.4 (+16.6) 67.6 (+16.8) 25.7 28.3 (+2.6) 29.1 33.8 (+4.7) 35.3 (+6.2) 26.9 41.6 (+14.7) 44.2 (+17.3) 29.1 39.7 (+10.6) 41.1 (+4.5) 29.9 40.3 (+10.4) 41.1 (+11.2) 60.0 64.1 (+4.1) 67.8 70.0 (+2.2) 71.0 (+3.2) 76.9 77.7 (+0.8) 78.3 (+1.4) 58.9 60.3 (+1.4) 63.3 (+4.4) 61.6 65.4 (+3.8) 65.2 (+3.6) 21.2 21.8 (+0.6) 24.2 26.1 (+1.9) 26.2 (+2.0) 36.7 40.5 (+3.8) 40.1 (+3.2) 38.6 42.6 (+4.0) 44.8 48.6 (+3.6) 48.9 (+4.1) 50.5 58.7 (+8.2) 59.4 (+8.9) 28.1 33.8 (+5.7) 33.9 (+5.8) 32.4 39.8 (+7.4) 40.9 (+8.5) 43.7 50.4 (+6.7) 51.5 (+7.8) 47.5 55.7 (+8.2) 55.9 (+8.4) problem-solving ability. For MMMU, we use the validation set containing 900 problems. For PuzzleVQA, we construct an evaluation set of 1,000 samples by selecting the first 50 puzzles from each of 20 problem types. For AlgoPuzzleVQA, we similarly sample the first 50 puzzles from each of 18 problem types. For MathVista, we adopt the test-mini split, which contains 1,000 problems. Finally, for MathVision, we use the test split, which has 3040 samples. On all five datasets, the policy model is instructed to do structured reasoning, where the model first explains the image, followed by induction and deduction. To assess the capability of our VL-PRMs in fine-grained step correctness evaluation, we use VisualProcessBench (Wang et al., 2025). This benchmark consists of 2,866 problems with step-by-step solutions, comprising total of 26,950 steps manually annotated as correct, incorrect, or neutral. The problems are drawn from five datasets, including MMMU and four math benchmarks: MathVerse, DynaMath, MathVision, and WeMath. Performance on this benchmark is reported using the macro-F1 score. We dropped neutral samples from the data following Wang et al. (2025). 3.2 MAIN RESULTS 3.2.1 VL-PRM IMPROVES PERFORMANCE UNDER TEST-TIME SCALING Results on MMMU. Both QWEN-VL-PRM-3B and QWEN-VL-PRM-7B consistently enhance the performance of all evaluated policies under test-time scaling. On the expert-knowledgeoriented dataset MMMU, however, the observed gains are relatively modest, with improvements averaging around 2.5% across the three policies. This suggests that test-time scaling may have limited impact in tasks where reasoning heavily depends on specialized domain knowledge. Results on Abstract and Algorithmic Reasoning Datasets. In contrast, for datasets requiring abstract, algorithmic, or mathematical reasoning, we observe substantial improvements. On PuzzleVQA, performance increases by 7% for Qwen-VL-2.5-7B, 12.7% for Gemma3-12B, and 16.6% for Gemma3-27B. Similarly, on AlgoPuzzleVQA, we find improvements of approximately 5% for Qwen-VL-2.5-7B and 11% for both Gemma3-12B and Gemma3-27B. These results highlight that test-time scaling is particularly effective for reasoning-intensive tasks beyond expert subject knowledge. In Table 7, we present the performance of VisualPRM on the two datasets. VisualPRM was trained primarily on math-focused datasets and did not include any abstract reasoning data. The results show consistent performance gains across both datasets, with the exception of Qwen2.5-VL-7B, where the improvement on PuzzleVQA is limited to just 1%. While these gains are notable, VisualPRM still does not surpass the performance of our VL-PRMs. Overall, these findings highlight that training on mixture of general VQA and math datasets can transfer effectively to other reasoning domains, such as abstract and algorithmic reasoning. 6 Table 3: Performance on VisualProcessBench. Improvements over Qwen-2.5-VL-7B shown in green. Bold indicates best performance per column. Model GPT-4o-Mini GPT-4o Qwen-2.5-VL-3B Qwen-2.5-VL-7B Qwen-2.5-VL-32B MathVision MathVerse MMMU DynaMath WeMath Overall 53.6 56.3 43.9 53.1 58.4 58.9 60.2 42.6 51.8 63.3 57.1 59.7 41.6 47.8 57. 56.7 59.0 45.2 51.3 61.1 58.5 63.3 40.8 54.2 60.3 57.9 60.3 43.4 51.0 59. QWEN-VL-PRM-3B QWEN-VL-PRM-7B 59.6 (+6.5) 55.7(+2.6) 60.6 (+8.8) 58.8 (+7.0) 59.4 (+11.6) 55.8 (+8.0) 64.5 (+13.2) 58.3 (+7.0) 64.7 (+10.5) 59.8 (+5.6) 61.9 (+10.9) 58.6 (+7.6) Results on Math Datasets. On MathVista and MathVision, we observe moderate performance improvement of 35%, which is lower than the gains seen on PuzzleVQA and AlgoPuzzleVQA. This is likely due to the lower proportion of math-related data in our training set compared to the abundance of abstract reasoning problems from RAVEN. Interestingly, despite using much larger collection of math-focused problems, Wang et al. (2025) report similar level of improvement on MathVista and MathVision across their models. However, when we evaluate VisualPRM using Qwen-2.5-VL7B/32B and Gemma3-12B/27B as policy models, we find that on MathVista, performance drops from 1-2 percentage points (see Table 7). MathVision performance improves, but the improvement is less than our VL-PRMs. In our case, we include only few samples from CLEVER-Math, which primarily consists of elementary problems such as basic addition and subtraction. Despite this limited mathematical data, our approach achieves comparable improvements to those of Wang et al. (2025), whose dataset includes more complex mathematical problems, such as those from GeoQA+, Geo170K, Geometry3K, GEOS, GeomVerse, and CLEVR-Math as well. We hypothesize that the substantial number of abstract reasoning and general VQA tasks in our dataset enhances the general logical reasoning ability of the VL-PRM, thereby improving TTS performance across diverse domains. Test-time Scaling Unleashes Hidden Reasoning Ability. We observe that certain models, particularly Qwen-2.5-VL-32B, Gemma3-12B and Gemma3-27B, exhibit substantial gains under test-time scaling despite performing comparably to Qwen-VL-2.5-7B in the absence of scaling on PuzzleVQA and AlgoPuzzleVQA. While their baseline performance is similar, these larger Gemma models benefit disproportionately from the guidance of VL-PRMs during inference. This finding is noteworthy, as it suggests that process reward models (PRMs) can unlock latent reasoning capacity that is not apparent under standard greedy decoding. plausible explanation is that larger models often encode more sophisticated reasoning patterns, but these capabilities may remain underutilized due to suboptimal decoding trajectories. By filtering and guiding intermediate steps, VL-PRMs help these models explore more reliable reasoning paths, thereby revealing their hidden potential. 3.2.2 PERFORMANCE ON VISUALPROCESSBENCH VisualProcessBench is employed to evaluate the ability of VL-PRMs to assess step-level correctness. Given problem and its step-by-step solution, the VL-PRM computes the probabilities of the + and tokens for each step. step is labeled as positive if (+) > (), and negative otherwise. As baselines, we use Qwen-2.5-VL-3B and Qwen-2.5-VL-7B directly to predict step correctness. Our results in Table 3 show that both VL-PRMs outperform their respective baselines. While QWEN-VLPRM-7B achieves good improvement of 7.6% in macro-F1 score, QWEN-VL-PRM-3B exhibits substantial gain of 16%, highlighting the effectiveness of PRM training even for smaller backbones. Additionally, we also benchmark GPT-4o-Mini and GPT-4o. We find that QWEN-VL-PRM-3B outperforms GPT-4o-Mini and performs comparably to GPT-4o. Takeaways. Our VL-PRMs are trained only on an elementary math dataset with basic operations, yet the 3B variant performs competitively with proprietary models like GPT-4o on VisualProcessBench, which contains challenging math datasets. This suggests that training on general VQA and abstract reasoning tasks effectively generalizes to mathematical reasoning for error detection. Future work will explore incorporating advanced math datasets (Wang et al., 2025) to further enhance performance."
        },
        {
            "title": "3.2.3 RESULTS ON PERCEPTIONPROCESSBENCH",
            "content": "The results on PERCEPTIONPROCESSBENCH in Table 4 show that incorporating perception steps during training significantly improves the ability of VL-PRMs to detect perception errors. Our VL-PRMs outperform the baseline Qwen2.5-VL models of comparable size by margin of 14%20%. In contrast, when trained without perception supervision, the VL-PRMs tend to predict all perception steps as correct. This occurs because the training data does not explicitly indicate which perception steps are incorrect. Moreover, since our data is structured such that all subsequent steps are marked as negative after negative step is identified by o4-mini, the model implicitly treats all perception steps as positiveeven though their labels are masked during training."
        },
        {
            "title": "3.3 ANALYSES AND ABLATIONS",
            "content": "Table 4: F1 scores of our VLPRMs on PERCEPTIONPROCESSBENCH. PRM Qwen-VL-PRM-3B w/o perception Qwen-VL-PRM-7B w/o perception F1 66.8 33.3 70.3 33.3 Qwen2.5-VL-3B-Instruct 46.8 Qwen2.5-VL-7B-Instruct 56. Findings from Test-time Scaling Strategies. As outlined in Section 2.2, we investigate three strategies for leveraging VL-PRMs during test-time scaling. While all the TTS techniques improve performance substantially compared to the baseline, our results show that Guided Greedy Search consistently performs the worst among the three. likely explanation is that selecting the immediately highest-rewarded step does not necessarily lead to an optimal reasoning trajectory due to error propagation from earlier steps. In fact, for challenging tasks such as AlgoPuzzleVQA, we even observe that greedy VL-PRM-guided search can reduce performance below the baseline policy. With Step-score aggregation, we see improvements across the benchmarks using all the different policies. Notably, both the greedy search and step-scorer approaches provide negligible improvement for MMMU. Surprisingly, although our VL-PRMs are trained to assign rewards at the step level, performance improves markedly when the model is provided with an entire solution sequence for scoring, as described in Section 2.2. This suggests that, despite being trained for local supervision, VL-PRMs generalize effectively to full-solution evaluation. Access to complete contextual information appears to enable more accurate and reliable reward assignment, thereby enhancing test-time scaling. The comparative results are reported in Figure 2. For Qwen-VL, although these two methods perform similarly on MMMU and MathVista, the differences are noticeable for PuzzleVQA and AlgoPuzzleVQA, where One-shot Search outperforms Guided Greedy Search by 4% to 9%. However, when looked closely, we find that Step-score aggregation tends to generate false positives, consequently affecting its performance. One reason for this could be the high imbalance of correct and incorrect steps in the training data. Figure 2: Comparison among different TTS methods. One-shot Search has improved performance across VL-PRMs and benchmarks. This method is akin to Outcome Reward Modeling (ORM). What is interesting is that despite not training an explicit ORM, our PRM is acting as an ORM. This is in contrast to VisualPRM (Wang et al., 2025), where they trained an ORM explicitly, yet failed to perform as well as their PRM. To investigate this further, we evaluated VisualPRM on the same benchmarks and policy models, with details in Table 7. We found that VisualPRM shows improvement under the one-shot setting and only falls short of step-aggregation-based TTS by 1%. In our case, one-shot consistently outperforms step-aggregation by 2-3%. The Impact of Dataset Size. Our experiments reveal that training on VL-PRM300K-balanced subset does not lead to consistent improvements: in some cases, performance increases marginally, while in others, slight decrease is observed. We report this result Table 5. Table 5: Performance comparison of VL-PRMs trained on variants of VL-PRM300K Policy Model Reward Model VL-PRM300K Variant MMMU PuzzleVQA AlgoPuzzleVQA MathVista Average Qwen-2.5-VL-7B Qwen-2.5-VL-32B Gemma3 12B Gemma3 27B Baseline Qwen-VL-PRM-3B Qwen-VL-PRM-7B Baseline Qwen-VL-PRM-3B Qwen-VL-PRM-7B Baseline Qwen-VL-PRM-3B Qwen-VL-PRM-7B Baseline Qwen-VL-PRM-3B Qwen-VL-PRM-7B full balanced w/o perception full balanced w/o perception full balanced w/o perception full balanced w/o perception full balanced w/o perception full balanced w/o perception full balanced w/o perception full balanced w/o perception 55.0 55.33 57.56 54. 57.44 56.00 53.89 66.0 67.0 65.78 65.00 67.56 65.00 66.89 57.6 60.00 60.44 57. 59.11 60.22 58.44 62.9 65.56 65.11 63.78 64.00 64.56 63.89 48.0 55.50 52.10 49. 54.80 54.70 51.80 46.2 67.1 67.0 66.7 66.50 66.80 65.00 45.0 57.70 57.30 54. 59.00 58.10 55.60 50.8 67.40 67.20 65.70 67.60 67.00 66.40 29.1 33.78 33.22 33. 33.78 35.33 32.11 26.9 41.56 40.33 40.78 42.22 44.22 42.78 29.1 39.67 38.33 36. 40.78 41.11 37.67 29.9 40.33 39.33 40.33 40.33 41.11 40.67 67.8 69.00 70.00 67. 69.80 71.00 68.20 76.9 77.6 77.4 77.7 77.50 78.30 77.10 58.9 60.30 60.00 57. 63.30 60.70 58.20 61.6 65.40 64.70 62.30 65.00 65.20 62.20 49.98 53.40 53.22 51. 53.96 54.26 51.50 54.0 63.32 62.63 62.55 63.45 63.58 62.94 47.65 54.42 54.02 51. 55.55 55.03 52.48 51.80 59.67 59.09 58.03 59.23 59.47 58.29 The Role of Judge Labels. To construct VLPRM300K, we first apply MCTS and then determine the correctness of each step using o4-mini as judge. To assess whether an external judge LLM is necessary for improving VL-PRM performance, we experiment with MC-scorebased labeling only to verify the correctness of steps. For this labeling scheme, as described in Section 2, step is labeled as positive if the MC-score of step exceeds selected threshold, otherwise step is labeled as negative. Our results using threshold of 0, summarized in Figure 3, show that using o4-mini as the judge substantially improves VL-PRM performance compared to utilizing MC-scorebased labeling only. We experiment with multiple thresholds, including 0.5 and 0.9, and find that using judge continues to result in better overall performance. Furthermore, we measure the agreement between the two step-level labeling schemes and find that it is approximately approximately 20% for incorrect samples, underscoring the divergence between MCTS-derived supervision and LLM-based judgments. Figure 3: MC-Only vs o4-mini judge for VL-PRM300K construction. Perception Guidance Enhances VL-PRMs. During the construction of VL-PRM300K, we adopt structured reasoning template in which the policy (o4-mini) first produces perception-based description of the image, followed by inductive and deductive reasoning steps to solve the problem. This design enables training VL-PRMs on explicit perception steps. To evaluate the importance of perception supervision, we conduct an ablation study in which samples with perception errors are omitted during training. As shown in Table 5, excluding perception guidance leads to consistent drop in performance for both QWEN-VL-PRM-3B and QWEN-VL-PRM-7B across all four benchmarks and with different policies. The performance drop is more evident in the 7B and 12B policy models compared to the 32B and 27B models, indicating that the larger models exhibit much stronger perception performance. Overall, these results demonstrate that perception guidance plays crucial role in strengthening the capabilities of VL-PRMs. VL-PRMs vs. VLMs as Judges. We evaluate whether VLMs can act as effective critics for test-time scaling (TTS). While using Qwen-2.5-VL models as judges in one-shot best-of-N setting improves performance, our VL-PRMs deliver substantially greater gains, consistently outperforming VLM-based judges. In contrast, when applying greedy search TTS with Qwen-2.5-VL-7B as both the policy and judge, performance drops below baseline across all datasets: MMMU (53.89%, 1%), PuzzleVQA (45.50%, 2.5%), AlgoPuzzleVQA (28.44%, 1.5%), and MathVista (65%, 3.1%). These results suggest that relying on VLMs for stepwise evaluation is not optimal. 9 Majority Voting. As shown in Figure 2, PRM-based test-time scaling outperforms majority voting for smaller models. Here, majority voting selects the final answer by choosing the most frequent output across multiple CoT traces. For larger models, such as Gemma 27B, the performance of PRM-based scaling and majority voting is comparable."
        },
        {
            "title": "4 CONCLUSION",
            "content": "We introduce VL-PRM300K, dataset built with MCTS and o4-mini labeling to train VL-PRMs for test-time scaling (TTS) of VLMs. Our study presents several in-depth and complementary analyses compared to the existing works. We show that when used as Outcome Reward Models (ORMs), VL-PRMs can outperform VL-PRM-guided response selection during TTS, with smaller models sometimes rivaling larger ones in error detection. They also reveal hidden reasoning abilities in strong VLM backbones, benefit greatly from perception-level supervision, and improve performance on advanced math tasks even without direct training on such datasets. These findings highlight the potential of VL-PRMs to drive progress in multimodal reasoning."
        },
        {
            "title": "ACKNOWLEDGMENT",
            "content": "This research/project is supported by the National Research Foundation, Singapore under its National Large Language Models Funding Initiative. Any opinions, findings and conclusions or recommendations expressed in this material are those of the author(s) and do not reflect the views of National Research Foundation, Singapore."
        },
        {
            "title": "REFERENCES",
            "content": "Jie Cao and Jing Xiao. An augmented benchmark dataset for geometric question answering through dual parallel text encoding. In Nicoletta Calzolari, Chu-Ren Huang, Hansaem Kim, James Pustejovsky, Leo Wanner, Key-Sun Choi, Pum-Mo Ryu, Hsin-Hsi Chen, Lucia Donatelli, Heng Ji, Sadao Kurohashi, Patrizia Paggio, Nianwen Xue, Seokhwan Kim, Younggyun Hahm, Zhong He, Tony Kyungil Lee, Enrico Santus, Francis Bond, and Seung-Hoon Na (eds.), Proceedings of the 29th International Conference on Computational Linguistics, pp. 15111520, Gyeongju, Republic of Korea, October 2022. International Committee on Computational Linguistics. URL https://aclanthology.org/2022.coling-1.130/. Xinquan Chen, Bangwei Liu, Xuhong Wang, Yingchun Wang, and Chaochao Lu. Vrprm: Process reward modeling via visual reasoning, 2025. URL https://arxiv.org/abs/2508.03556. Yew Ken Chia, Vernon Toh Yan Han, Deepanway Ghosal, Lidong Bing, and Soujanya Poria. Puzzlevqa: Diagnosing multimodal reasoning challenges of language models with abstract visual patterns, 2024. URL https://arxiv.org/abs/2403.13315. Paul Christiano, Jan Leike, Tom B. Brown, Miljan Martic, Shane Legg, and Dario Amodei. Deep reinforcement learning from human preferences, 2023. URL https://arxiv.org/abs/ 1706.03741. Lingxiao Du, Fanqing Meng, Zongkai Liu, Zhixiang Zhou, Ping Luo, Qiaosheng Zhang, and Wenqi Shao. Mm-prm: Enhancing multimodal mathematical reasoning with scalable step-level supervision, 2025. URL https://arxiv.org/abs/2505.13427. Jiahui Gao, Renjie Pi, Jipeng Zhang, Jiacheng Ye, Wanjun Zhong, Yufei Wang, Lanqing Hong, Jianhua Han, Hang Xu, Zhenguo Li, and Lingpeng Kong. G-llava: Solving geometric problem with multi-modal large language model, 2025. URL https://arxiv.org/abs/2312.11370. Mengzhao Jia, Zhihan Zhang, Wenhao Yu, Fangkai Jiao, and Meng Jiang. Describe-then-reason: Improving multimodal mathematical reasoning through visual comprehension training, 2024. URL https://arxiv.org/abs/2404.14604. Mehran Kazemi, Hamidreza Alvari, Ankit Anand, Jialin Wu, Xi Chen, and Radu Soricut. Geomverse: systematic evaluation of large models for geometric reasoning, 2023. URL https://arxiv. org/abs/2312.12241. 10 Muhammad Khalifa, Rishabh Agarwal, Lajanugen Logeswaran, Jaekyeom Kim, Hao Peng, Moontae Lee, Honglak Lee, and Lu Wang. Process reward models that think, 2025. URL https: //arxiv.org/abs/2504.16828. Hunter Lightman, Vineet Kosaraju, Yura Burda, Harri Edwards, Bowen Baker, Teddy Lee, Jan Leike, John Schulman, Ilya Sutskever, and Karl Cobbe. Lets verify step by step. arXiv preprint arXiv:2305.20050, 2023. Adam Dahlgren Lindstrom and Savitha Sam Abraham. Clevr-math: dataset for compositional language, visual and mathematical reasoning, 2022. URL https://arxiv.org/abs/2208. 05358. Pan Lu, Ran Gong, Shibiao Jiang, Liang Qiu, Siyuan Huang, Xiaodan Liang, and Song-Chun Zhu. Inter-gps: Interpretable geometry problem solving with formal language and symbolic reasoning, 2021. URL https://arxiv.org/abs/2105.04165. Minjoon Seo, Hannaneh Hajishirzi, Ali Farhadi, Oren Etzioni, and Clint Malcolm. Solving geometry problems: Combining text and diagram interpretation. In Lluıs M`arquez, Chris Callison-Burch, and Jian Su (eds.), Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing, pp. 14661476, Lisbon, Portugal, September 2015. Association for Computational Linguistics. doi: 10.18653/v1/D15-1171. URL https://aclanthology.org/D15-1171/. Omkar Thawakar, Dinura Dissanayake, Ketan More, Ritesh Thawkar, Ahmed Heakl, Noor Ahsan, Yuhao Li, Mohammed Zumri, Jean Lahoud, Rao Muhammad Anwer, Hisham Cholakkal, Ivan Laptev, Mubarak Shah, Fahad Shahbaz Khan, and Salman Khan. Llamav-o1: Rethinking step-bystep visual reasoning in llms, 2025. Peiyi Wang, Lei Li, Zhihong Shao, Runxin Xu, Damai Dai, Yifei Li, Deli Chen, Yu Wu, and Zhifang Sui. Math-shepherd: Verify and reinforce llms step-by-step without human annotations. In Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pp. 94269439. Association for Computational Linguistics, 2024. doi: 10.18653/v1/2024.acl-long.510. URL http://dx.doi.org/10.18653/v1/2024. acl-long.510. Weiyun Wang, Zhangwei Gao, Lianjie Chen, Zhe Chen, Jinguo Zhu, Xiangyu Zhao, Yangzhou Liu, Yue Cao, Shenglong Ye, Xizhou Zhu, Lewei Lu, Haodong Duan, Yu Qiao, Jifeng Dai, and Wenhai Wang. Visualprm: An effective process reward model for multimodal reasoning, 2025. URL https://arxiv.org/abs/2503.10291. Xuezhi Wang, Jason Wei, Dale Schuurmans, Quoc Le, Ed Chi, Sharan Narang, Aakanksha Chowdhery, and Denny Zhou. Self-consistency improves chain of thought reasoning in language models, 2023. URL https://arxiv.org/abs/2203.11171. Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, brian ichter, Fei Xia, Ed Chi, Quoc Le, and Denny Zhou. Chain-of-thought prompting elicits reasoning in large language modIn S. Koyejo, S. Mohamed, A. Agarwal, D. Belgrave, K. Cho, and A. Oh (eds.), Adels. vances in Neural Information Processing Systems, volume 35, pp. 2482424837. Curran Associates, Inc., 2022. URL https://proceedings.neurips.cc/paper_files/paper/ 2022/file/9d5609613524ecf4f15af0f7b31abca4-Paper-Conference.pdf. Qiong Wu, Xiangcong Yang, Yiyi Zhou, Chenxin Fang, Baiyang Song, Xiaoshuai Sun, and Rongrong Ji. Grounded chain-of-thought for multimodal large language models, 2025. URL https: //arxiv.org/abs/2503.12799. Chi Zhang, Feng Gao, Baoxiong Jia, Yixin Zhu, and Song-Chun Zhu. Raven: dataset for relational and analogical visual reasoning. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2019. Jianghangfan Zhang, Yibo Yan, Kening Zheng, Xin Zou, Song Dai, and Xuming Hu. Gm-prm: generative multimodal process reward model for multimodal mathematical reasoning, 2025a. URL https://arxiv.org/abs/2508.04088. 11 Zhenru Zhang, Chujie Zheng, Yangzhen Wu, Beichen Zhang, Runji Lin, Bowen Yu, Dayiheng Liu, Jingren Zhou, and Junyang Lin. The lessons of developing process reward models in mathematical reasoning. arXiv preprint arXiv:2501.07301, 2025b. Jian Zhao, Runze Liu, Kaiyan Zhang, Zhimu Zhou, Junqi Gao, Dong Li, Jiafei Lyu, Zhouyi Qian, Biqing Qi, Xiu Li, and Bowen Zhou. Genprm: Scaling test-time compute of process reward models via generative reasoning. arXiv preprint arXiv:2504.00891, 2025. Chujie Zheng, Zhenru Zhang, Beichen Zhang, Runji Lin, Keming Lu, Bowen Yu, Dayiheng Liu, Jingren Zhou, and Junyang Lin. Processbench: Identifying process errors in mathematical reasoning, 2024. URL https://arxiv.org/abs/2412.06559."
        },
        {
            "title": "A APPENDIX",
            "content": "A.1 RELATED WORK Reward Models. Reward models play central role in reinforcement learning and test-time scaling by providing the feedback signal that guides model behavior (Christiano et al., 2023). Traditional Outcome Reward Models (ORMs) evaluate only the final output with single score, whereas Process Reward Models (PRMs) assess intermediate steps in the reasoning trajectory and then aggregate these step-level evaluations into final judgment. Recent multimodal PRMs, such as VisualPRM (Wang et al., 2025) and MM-PRM (Du et al., 2025) demonstrate improvement in reasoning tasks by constructing large process-supervision datasets and leveraging scalable step-level supervision. Follow-up works such as GM-PRM (Zhang et al., 2025a) and VRPRM (Chen et al., 2025) further refine error correction and visual reasoning capabilities. However, most existing multimodal PRMs focus on mathematical reasoning, with less focus on abstract visual reasoning (Zhang et al., 2019) in dataset construction. To address this gap, we introduce VL-PRM300K, the first multimodal process supervision dataset, which includes roughly 300K samples and 1.32 million steps with process supervision containing predominantly abstract reasoning samples, and develop VL-PRM, multimodal PRM trained on this dataset. Test-time Scaling. Most current multimodal PRM work only emphasizes on step aggregation technique (Wang et al., 2025) with the focus mainly on the process reward model data construction. Some recent works further explore other TTS techniques, such as GM-PRM (Zhang et al., 2025a), which introduces refined Best-of-N inference strategy in which the PRM not only scores multiple candidate reasoning paths but also corrects the first error in path to guide further search. In ThinkPRM (Khalifa et al., 2025), it evaluates Best-of-N selection and reward-guided search across several math reasoning benchmarks. These works lack comprehensive comparison of different test-time scaling (TTS) or inference time strategies. In our work, we evaluate comprehensive range of TTS strategies, including greedy PRM-guided search, one-shot search, and step-wise score aggregation, to understand how they influence performance and generalization across various benchmarks. A.2 EXPERIMENTS TTS Strategies. Different TTS strategies are shown in Figure 4. Detailed experimental results are shown in Table 6. Figure 4: Different TTS strategies. Increasing in BON@N. In Figure 5, we see the effect of increasing from 8 to 16 for Stepwise Score Aggregation and One-shot Search. The results suggest that performance improves by about 1% by increasing the size of N. Performance of VisualPRM. We also evaluate VisualPRM on the same datasets and under both TTS settings used for our VL-PRMs. Unlike our models, VisualPRM does not show consistent 13 Table 6: Performance comparison with color-coded results: Best, Second, and Third performing scaling methods per policy model and dataset. Performance Ranking: Best (Top 1), Second (Top 2), Third (Top 3) within each policy model per dataset. Gray background: Baseline (PRM-independent). Blue background: Majority (PRM-independent). The MathVision column is reserved for future evaluation results. Policy Model Scoring Method AlgoPuzzleVQA MMMU MathVista PuzzleVQA Average 3B 7B 3B 7B 3B 7B 3B 7B 3B 7B 7 - - 5 2 - Q . 2 1 3 e B 7 2 3 e Baseline Step agg One-shot Greedy Majority Baseline Step agg One-shot Greedy Majority Baseline Step agg One-shot Greedy Majority 29.90 54. 68.10 48.00 50.24 29.78 33.80 27. 27.67 35.30 27.95 55.62 57.60 54.39 69. 67.60 55.70 54.20 52.70 57.40 70.00 71.00 55.50 54.80 54.23 50. 54.63 57.34 56.56 67.90 71.20 52.00 51.10 51.28 51. 31.44 29.10 56.51 57.67 73.20 58. 53.00 45.00 53.54 47.67 33.89 39. 36.89 36.78 41.11 38.22 57.67 60. 57.00 61.40 61.30 55.80 55.70 52.19 60.22 60.30 63.30 57.70 59.00 54. 52.70 55.91 55.56 54.67 62.40 61.20 52.70 54.20 51. 52.07 37.11 29.90 55.66 62.95 61. 61.60 58.30 50.80 53.17 51.31 38. 40.33 38.00 38.78 41.11 40.00 61. 65.56 62.11 65.00 63.90 65.70 65.10 57.68 64.56 65.40 65.20 67. 67.60 59.67 57.47 59.62 60.22 62.33 63.10 64.70 66. 68.40 56.86 58.86 42.00 63.11 63.80 69. 59.68 Figure 5: Comparison between BON@8 and BON@16 across datasets and models. 14 advantage for either Step Aggregation or One-shot TTS. For instance, with Qwen-2.5-VL-7B, Oneshot search outperforms Step Aggregation by only 0.6%, while for other models Step Aggregation has slight edge of about 1% on average across datasets. Although VisualPRM was primarily trained on math-focused datasets, it performs reasonably well on abstract and algorithmic reasoning tasks, demonstrating some level of generalization. However, its overall performance gains are notably smaller than those of our VL-PRMs. This gap likely arises from the higher proportion of abstract reasoning samples in VL-PRM300K, which we use to train our VL-PRMs. We expected VisualPRM to achieve its strongest improvements on math datasets such as MathVista and MathVision. While it shows consistent gains on MathVision, performance on MathVista decreases by 12 percentage points. Even on MathVision, the gains do not surpass those of our VL-PRMs. Several factors may contribute to this difference, including variations in model backbones, differences in dataset composition, and the use of GPT-4.1 for generation and o4-mini for judgment used to train our models instead of using InternVL-2.5-7B for generation and MCScore threshold-based step labeling only used for training VisualPRM. Interestingly, on MMMU, VisualPRM exhibits performance decline across nearly all settings, with the sole exception of Qwen-2.5-VL-7B, where it achieves modest 2% improvement. Table 7: Performance comparison across model families using VisualPRM (Wang et al., 2025). Improvements are shown in green with gains over base models. Model MMMU PuzzleVQA AlgoPuzzleVQA MathVista MathVision Overall Commercial Models GPT-4o o1 o3 70.7 78.2 82.9 60.0 78.9 84.1 57.8 54.4 62.3 30.9 73.9 86. 31.2 60.3 50.1 69.1 Qwen-2.5-VL Family Qwen-2.5-VL-3B + QWEN-VL-PRM-7B Qwen-2.5-VL-7B 51.7 53.7 (+2.0) 55.0 + VISUALPRM-STEP-AGG 57.1 (+2.1) + VISUALPRM-ONE-SHOT 55.7 (+0.7) 57.6 (+2.6) + QWEN-VL-PRM-3B 57.4 (+2.4) + QWEN-VL-PRM-7B 66.0 Qwen-2.5-VL-32B + VISUALPRM-STEP-AGG 62.4 (-3.6) + VISUALPRM-ONE-SHOT 61.8 (-4.8) 67.0 (+1.0) + QWEN-VL-PRM-3B 67.6 (+1.6) + QWEN-VL-PRM-7B Gemma3 Family Gemma3-12B 57.6 + VISUALPRM-STEP-AGG 57.1 (-0.5) + VISUALPRM-ONE-SHOT 57.5 (-0.1) 60.4 (+2.8) + QWEN-VL-PRM-3B 60.2 (+2.6) + QWEN-VL-PRM-7B 62.9 Gemma3-27B + VISUALPRM-STEP-AGG 62.3 (-0.6) + VISUALPRM-ONE-SHOT 61.1 (-1.1) 65.5 (+2.6) + QWEN-VL-PRM-3B 64.5 (+1.6) + QWEN-VL-PRM-7B 34.5 44.9 (+10.5) 48.0 49.1 (+1.1) 48.0 (+0.0) 55.5 (+7.5) 54.8 (+6.8) 46.2 62.9 (+16.7) 60.0 (+13.8) 67.1 (+20.8) 66.8 (+20.6) 45.0 52.8 (+7.8) 50.1 (+5.1) 57.7 (+12.7) 59.0 (+12.0) 50.8 64.8 (+14.0) 62.4 (+11.6) 67.4 (+16.6) 67.6 (+16.8) 25.7 28.3 (+2.6) 29.1 27.7 (-1.4) 33.5 (+4.4) 33.8 (+4.7) 35.3 (+6.2) 26.9 42.4 (+15.5) 40.7 (+13.8) 41.6 (+14.7) 44.2 (+17.3) 29.1 33.0 (+3.9) 34.3 (+5.2) 39.7 (+10.6) 41.1 (+4.5) 29.9 38.5 (+8.6) 39.6 (+9.7) 40.3 (+10.4) 41.1 (+11.2) 60.0 64.1 (+4.1) 67.8 68.1 (+0.3) 66.0 (-1.8) 70.0 (+2.2) 71.0 (+3.2) 76.9 76.7 (-0.2) 75.4 (-1.5) 77.7 (+0.8) 78.3 (+1.4) 58.9 57.8 (-1.1) 56.8 (-2.1) 60.3 (+1.4) 63.3 (+4.4) 61.6 61.8 (+0.0) 59.6 (-2.0) 65.4 (+3.8) 65.2 (+3.6) 21.2 21.8 (+0.6) 24.2 25.7 (+1.5) 27.6 (+3.4) 26.1 (+1.9) 26.2 (+2.0) 36.7 39.5 (+2.8) 39.5 (+2.8) 40.1 (+3.4) 40.4 (+3.7) 28.1 28.1 (+0.0) 32.5 (+4.4) 33.8 (+5.7) 33.9 (+5.8) 32.4 27.8 (-4.6) 37.2 (+4.8) 39.8 (+7.4) 40.9 (+8.5) 38.6 42.6 (+4.0) 44.8 45.5 (+0.7) 46.1 (+1.3) 48.6 (+3.6) 48.9 (+4.1) 50.5 56.7 (+6.2) 55.5 (+5.0) 58.7 (+8.2) 59.4 (+8.9) 43.7 46.9 (+3.2) 46.2 (+2.5) 50.4 (+6.7) 51.5 (+7.8) 47.5 53.0 (+5.5) 52.0 (+4.5) 55.7 (+8.2) 55.7 (+7.7) Limited TTS benefits on MiniCPM-V family of models. We observed limited test-time scaling benefits for openbmb/MiniCPM-V-2 6 across all benchmarks, as shown in table 8, averaging 2% improvement overall. Upon comparison of baseline model outputs (in the absence of test-time scaling) and the TTS temperature-sampled outputs generated, we noticed that the baseline model is more likely to arrive at final correct answers when generating step-by-step traces in Mandarin Chinese. Building on this insight, we investigated various methods to elicit the model to think in Mandarin Chinese in the hope of better TTS performance, but faced limited systematic performance improvements. Future work will explore how we can further enhance performance for such models. We also faced challenges reproducing the baseline scores reported for MathVision, despite following the configurations detailed in VLMEvalKit specifically for evaluating MiniCPM-V-2 6. 15 Table 8: MiniCPM-V-2 6 performance on visual reasoning datasets (when used with QWEN-VLPRM-7B). Improvements are shown in green with gains over base models. Model MMMU PuzzleVQA AlgoPuzzleVQA MathVista MathVision Overall MiniCPM-V Family MiniCPM-V-2 6 47. + QWEN-VL-PRM-7B 50.0 (+2.2) 36.0 37.9 (+1.9) 27.9 30.2 (+2.3) 60.8 62.2 (+1.4) 18.1* 20.1* (+2.0) 38.1 40.1 (+2.0) A.3 DATASET ANALYSIS Details In this Section, we will provide details on VL-PRM300K. Table 9: Breakdown of VL-PRM300K by data source. Source RAVEN CLEVR-Math InfoVQA VQAv2 AI2D DVQA Overall Cumulative Statistics Total Valid Samples Total Samples Discarded Total Training Samples 203,115 4,610 198,505 Consensus Filtering Breakdown (number of rows) o4-mini Incorrect Step Identified o4-mini Correct and MC Consensus o4-mini Correct and MC Disagree 197,028 1,477 4,610 27,326 1,849 25,477 10,134 15,343 1,849 25,518 3,768 21,750 3,828 17,922 3,768 8,138 1,010 7, 2,478 4,650 1,010 22,399 1,428 20,971 16,663 488 16,175 303,159 13,153 290,006 6,614 14,357 1,428 4,233 11,942 224,315 65,691 13,153 Breakdown of steps with Perception v.s. Reasoning Errors (%) Perception Incorrect (%) Reasoning Incorrect (%) 86.4 13.6 96.3 3.7 81.5 18. 59.1 40.9 80.6 19.4 70.4 29.6 - - Table 10: Breakdown of steps in VL-PRM300K by data source. Source RAVEN CLEVR-Math InfoVQA VQAv2 AI2D DVQA Overall Cumulative Statistics Total Steps 619,692 160,632 171,890 52,426 170, 145,360 1,320,683 Number of Steps Breakdown Incorrect Steps Correct Steps 197,028 422,664 10,134 150, 3,828 168,062 2,478 49,948 6,614 164,069 4,233 141,127 224,315 1,096,368 Step Breakdown (%) Incorrect Steps (%) Correct Steps (%) 31.8 68.2 6.3 93.7 2.2 97.8 4.7 95.3 3.9 96. 2.9 97.1 - - Table 11: Breakdown of VL-PRM300K-balanced by data source Source RAVEN CLEVR-Math InfoVQA VQAv2 AI2D DVQA Overall Cumulative Statistics All Training Samples Balanced Training Samples Number of rows 198,505 39,881 Samples with Incorrect Step Samples with Verified Correct Steps 38,404 1, Proportion of rows used in training (%) Samples with Incorrect Step (%) Samples with Verified Correct Steps (%) 96.3 3.7 25,477 25,477 10,134 15,343 39.8 60. 21,750 21,750 7,128 7,128 20,971 20,971 16,175 16,175 290,006 131,382 3,828 17, 2,478 4,650 6,614 14,357 4,233 11,942 65,691 65,691 17.6 82.4 34.8 65. 31.5 68.5 26.2 73.8 - - A.4 PROMPTS Details construction and test time evaluation. In this Section, we will provide details regarding the prompts used in VL-PRM300K 16 GPT-4.1 Rollout Prompt for RAVEN during Dataset Generation You are an abstract reasoning puzzle expert. The puzzle you will receive is presented in standard Ravens Progressive Matrices format: 3 3 matrix of related images, with the bottom-right cell (the ninth tile) missing. There are eight possible answer choices provided separately, and your task is to decide which of those eight images correctly completes the 3 3 matrix pattern. will provide you with an image containing: Problem Matrix: An accompanying image that shows the eight tiles and highlights where the ninth is missing. Answer Set: The eight candidate images from which you must choose the best fit for the missing tile. Your task is to: Review the problem matrix and the accompanying image in sequence, describing step-by-step what you see in the image in <perception> tags. Reason step-by-step about the logical pattern or rule connecting the tiles in <reasoning> tags. Deduce the correct tile from the eight provided options in <correct answer> tags. It is crucial that your solution contains these sections in the exact format described below: [Perception] <step_1> ...(Step 1 of step-by-step perception)... </step_1> <step_2> ...(Step 2 of step-by-step perception)... </step_2> ... <step_n> ...(Step of step-by-step perception)... </step_n> [Reasoning] <step_1> ...(Step 1 of step-by-step reasoning)... </step_1> <step_2> ...(Step 2 of step-by-step reasoning)... </step_2> ... <step_m> ...(Step of step-by-step reasoning)... </step_m> <correct_answer> ...(Clearly state which of the 8 candidate images is the best candidate image as the missing tile to complete the matrix. If the candidates are numbered, lettered, or can be uniquely described, use that identifier.)... </correct_answer> Now proceed with the task. GPT-4.1 Rollout Prompt for AI2D during Dataset Generation You are an expert data analyst specializing in interpreting data visualizations. Your task is to answer questions about charts and graphs presented to you in images. You will be provided 17 with an image containing one or more data visualizations and specific question about the data presented. will provide you with an image containing: Data Visualization: chart or graph that contains data points and other visual elements. Heres the question you need to answer: <question> {{QUESTION}} </question> Please follow these steps to complete the task: 1. Carefully examine the image, paying attention to all elements of the data visualization(s) such as titles, labels, axes, legends, and data points. 2. Analyze the data presented in the visualization(s), identifying specific data points or trends that relate to the question asked. 3. Interpret the data and connect it to the specific question asked. Consider how the data directly relates to answering the question. 4. Use your analysis and interpretation to determine the answer to the question. The answer must be single integer. 5. Present your answer in LaTeX-formatted box using this format: <correct_answer> boxed{Your answer here} </correct_answer> Your task is to: Under the [Visual Elements] section, list out all relevant visual elements step-by-step that relate to answering the question. Be thorough but concise. Wrap each step in <step_1>, <step_2>, . . . tags. Under the [Reasoning] section, explain your step-by-step reasoning process. This should include your analysis, interpretation, and how you arrived at the answer. Provide clear justification of how you derived the answer from the data presented. Wrap each step in <step_1>, <step_2>, . . . tags. Present your final answer using the LaTeX-formatted box in <correct_answer> tags. It is crucial that your solution contains these sections in the exact format described below: [Visual Elements] <step_1> ...(Step 1 of step-by-step perception)... </step_1> <step_2> ...(Step 2 of step-by-step perception)... </step_2> ... <step_n> ...(Step of step-by-step perception)... </step_n> [Analysis and Interpretation] <step_1> ...(Step 1 of step-by-step reasoning)... </step_1> <step_2> 18 ...(Step 2 of step-by-step reasoning)... </step_2> ... <step_m> ...(Step of step-by-step reasoning)... </step_m> <correct_answer> $boxed{text{integer}}$ </correct_answer> Remember to: Provide only single string answer in the <correct_answer> section using the $boxed{string_answer}$ format, and no other text or commentary. o4-mini Judge Prompt for Sample Correctness Verification will provide an abstract visual reasoning problem along with solution. They will be formatted as follows: [Abstract Visual Reasoning Problem] <abstract_visual_reasoning_problem> ...(abstract visual reasoning problem)... </abstract_visual_reasoning_problem> [Solution] <solution> [Perception] <step_1> ...(Step 1 of step-by-step perception)... </step_1> <step_2> ...(Step 2 of step-by-step perception)... </step_2> ... <step_m> ...(Step of step-by-step perception)... </step_m> [Reasoning] <step_1> ...(Step 1 of step-by-step reasoning)... </step_1> <step_2> ...(Step 2 of step-by-step reasoning)... </step_2> ... <step_n> ...(Step of step-by-step reasoning)... </step_n> <correct_answer> ...(The provided answer to the abstract visual reasoning problem)... </correct_answer> </solution> Your task is to review each paragraph of the solution in sequence, analyzing, verifying, and critiquing the visual elements perception and reasoning in detail. You need to provide the analyses and the conclusion in the following format: [Perception] <analysis_1> 19 ...(analysis of step 1)... </analysis_1> ... <analysis_m> ...(analysis of step m)... </analysis_m> [Reasoning] <analysis_1> ...(analysis of step 1)... </analysis_1> ... <analysis_n> ...(analysis of step n)... </analysis_n> <conclusion> Correct/Incorrect </conclusion> When you analyze each step, you should use proper logical or perceptual verification as appropriate, or reflection to indicate whether it is logically and perceptually valid. Please carefully go through this process. Each analysis should: Check if the described pattern/rule is actually present Verify the logical consistency of the step Confirm that conclusions follow from observations If an error is detected in any step, you should describe the nature and cause of the error in detail, and suggest how to correct the error or the correct approach. When the step is found to contain an error, stop further analysis of subsequent steps (as they may depend on the identified error) and directly provide the conclusion of Incorrect in the <conclusion> tag. Only material reasoning or perception errors should trigger the early termination of the analysis. For instance, given solution of five steps, if an error is found in the third step, you should reply in the following format: <analysis_1> ...(analysis of step 1)... </analysis_1> <analysis_2> ...(analysis of step 2)... </analysis_2> <analysis_3> ...(analysis of step 3; since an error is found here, also provide detailed critique and correction guideline)... </analysis_3> <conclusion> Incorrect </conclusion> Note that the analyses of steps 4 and 5 is skipped as step 3 has been found to contain an error. The following is the abstract visual reasoning problem and its corresponding solution, for you to review and verify: [Abstract Visual Reasoning Problem] 20 <abstract_visual_reasoning_problem> {{ABSTRACT_VISUAL_REASONING_PROBLEM}} </abstract_visual_reasoning_problem> [Solution] <solution> {{SOLUTION}} </solution> Remember: The <conclusion> tag must contain either Correct or Incorrect, with no additional text or punctuation. GPT-4.1 Prompt for Rephrasing template perception in PuzzleVQA for PerceptionBench You are an expert assistant for creating training data for vision-language models. Your task is to take an image, question about the image, and its template based caption that provides visual perception of the image relevant to answering the question. You must generate rephrased captions that are fluent, natural and contain all the information from the template based caption. Your Task is to not to answer the given question but to rephrase captions so they are more natural than the template based captions. Guidelines: 1. Maintain fluency: Captions must remain human-like and grammatically correct. 2. Do not change the content: Rephrased captions must remain correct and contain the information from the original caption. 3. Generate only the Rephrased Caption GPT-4.1 Prompt for Mutating Perception in PuzzleVQA for PerceptionBench You are an expert assistant for creating training data for vision-language models. Your task is to take an image, question about the image, and its correct caption that answers the question. You must generate mutated captions that are fluent and natural but contain subtle factual errors with respect to the image and question context. Your Task is to not to answer the given question but to generate mutated captions that can serve as negative samples. Guidelines: 1. Maintain fluency: Captions must remain human-like and grammatically correct. 2. Introduce only subtle mistakes relevant to the question: Change an object to visually similar but incorrect one (dog cat, apple orange). Alter attributes such as color, size, number, position, or action. Slightly modify spatial relations or other details that are relevant to answering the question. 3. Minimal edits: Only introduce one subtle error per mutated caption. 4. Do not generate nonsense: Captions must remain realistic and plausible, even if incorrect. 5. Contrastive purpose: Mutated captions are intended as negative samples that could mislead model answering the same question. Always ensure the error is contextually relevant to the question."
        },
        {
            "title": "Gemma Policy Prompt for TTS Generation",
            "content": "[BOS]<start of turn>user You are an expert visual analyst who solves complex visual reasoning problems by systematically connecting what you observe in images to the specific requirements of each problem. First, identify and inventory all problem-relevant visual elements through step-by-step perceptual process, documenting their spatial relationships, object identities, and perceptual details with precision at each stage of observation. Then, construct step-by-step logical pathway that explicitly shows how each visual observation leads to your solution, explaining why each detail matters and citing specific image evidence at every step. Your analysis will be evaluated by teacher on two criteria: **Visual Elements* (accuracy of perception, spatial understanding, and object recognition) and *Reasoning** (logical consistency, valid deductive steps, and sound analytical conclusions). Multiple solution pathways will be explored in parallel at each step, building upon previous reasoning to generate diverse approaches, with the teacher scoring each path to accept or reject based on visual accuracy and logical validity. This approach increases the odds of reaching correct final answer by exploring and focusing on perception and reasoning traces that are valid and correct while ignoring incorrect ones. This makes your precise step-by-step perception and reasoning essential for identifying the most promising solution. <start of image> Question: {{Question}} Options: {{Options}} If you are uncertain or the problem is too complex, make reasoned guess based on the information provided. Avoid repeating steps indefinitelyprovide your best guess even if unsure. Determine whether to think step by step based on the difficulty of the question, considering all relevant information before answering. If you do need to think step by step, first, carefully observe and describe everything you see: the image itself and how it connects to the problem being presented. Then, work through your reasoning step by step to arrive at your answer. *Your teacher will review your descriptions of visual elements to ensure youre observing all relevant details accurately, and will critique each step of your reasoning to provide guidance and ensure youre on the right track.* Put your final answer within boxed{}. If multiple-choice options for answers to the question are provided, select the alphabetical letter of the corresponding best option within boxed{} when you are ready to provide your final answer. <end of turn> <start of turn>model Qwen Policy Prompt for TTS Genertion < im start >system You are an expert visual analyst who solves complex visual reasoning problems by systematically connecting what you observe in images to the specific requirements of each problem. First, identify and inventory all problem-relevant visual elements through step-by-step perceptual process, documenting their spatial relationships, object identities, and perceptual details with precision at each stage of observation. Then, construct step-by-step logical pathway that explicitly shows how each visual observation leads to your solution, explaining why each detail matters and citing specific image evidence at every step. Your analysis will be evaluated by teacher on two criteria: **Visual Elements* (accuracy of perception, spatial understanding, and object recognition) and *Reasoning** (logical consistency, valid deductive steps, and sound analytical conclusions). Multiple solution pathways will be explored in parallel at each step, building upon previous reasoning to generate diverse approaches, with the teacher scoring each path to accept or reject based on visual accuracy and logical validity. This approach increases the odds of reaching correct final answer by exploring and focusing on perception and reasoning traces that are valid and correct while ignoring incorrect ones. This makes your precise step-by-step perception and reasoning essential for identifying the most promising solution.< im end > < im start >user < vision start >< image pad >< vision end > Question: {{Question}} Options: {{Options}} Please select the correct answer from the options above. If you are uncertain or the problem is too complex, make reasoned guess based on the information provided. Avoid repeating 22 steps indefinitelyprovide your best guess even if unsure. Determine whether to think step by step based on the difficulty of the question, considering all relevant information before answering. If you do need to think step by step, first, carefully observe and describe everything you see: the image itself and how it connects to the problem being presented. Then, work through your reasoning step by step to arrive at your answer. *Your teacher will review your descriptions of visual elements to ensure youre observing all relevant details accurately, and will critique each step of your reasoning to provide guidance and ensure youre on the right track.* Put your final answer within . If multiple-choice options for answers to the question are provided, select the alphabetical letter of the corresponding best option within boxed{} when you are ready to provide your final answer.< im end > < im start >assistant"
        },
        {
            "title": "Qwen Policy Prompt for MathVision",
            "content": "< im start >system You are helpful assistant.< im end > < im start >user < vision start >< image pad >< vision end > Please solve the problem step by step and put your answer in one boxed{}. If it is multiple choice question, only one letter is allowed in the boxed{}. {{Question}} < im end > < im start >assistant Gemma Policy Prompt for MathVision [BOS]<start of turn>user You are helpful assistant. <start of image> Please solve the problem step by step and put your answer in one boxed{}. If it is multiple choice question, only one letter is allowed in the boxed{}. {{Question}} <end of turn> <start of turn>model QWEN-VL-PRM-3B and QWEN-VL-PRM-7B Prompt for Step Level Scoring You are process supervision model for visual reasoning tasks. You will receive an image and an image-based problem statement, followed by solution steps to evaluate. First round: problem statement and first solution step. Subsequent rounds: one new step per round. Assess the cumulative correctness of the entire solution up to each step. Evaluation Criteria: 1. Visual Accuracy: Are visual elements from the image correctly identified (shapes, colors, positions, quantities, spatial relationships)? 2. Logical Validity: Do all inferences and calculations follow correctly from the image and previous steps? Response: + if correct up to this step - if any error exists up to this step Only respond with + or -. No explanations. An error in any step invalidates all subsequent steps. A.5 TRAINING DETAILS Details. In this Section, we will provide details on training. We utilize the HuggingFace TRL library with PyTorch torchrun for distributed training to perform full parameter fine-tuning on Qwen2.5-VL-3B/7B-Instruct to create QWEN-VL-PRM-3B and QWEN-VL-PRM-7B. Table 12: Training Configuration for QWEN-VL-PRM-3B and QWEN-VL-PRM-7B. Parameter Base Model Torch Data type Attention Implementation Per-device Train Batch Size Gradient Accumulation Steps Learning Rate Weight Decay Number of Training Epochs LR Scheduler Type Max Gradient Norm Warmup Ratio Seed BF16 Optimizer Adam Beta 1 Adam Beta 2 Gradient Checkpointing Tune Vision Encoder Value Qwen2.5-VL-3B/7B-Instruct bfloat16 flash attention 2 2 4 1.0e-05 1.0e-04 2 cosine 1.0 0.05 100 true adam 0.9 0.95 True False Effect of Resizing Images on Performance Before Training. Similar to dynamic resizing of images using image processors at inference time, recommended by Qwen-2.5-VL for optimized inference performance, we experiment with applying the same dynamic image resizing procedure to all images in our training dataset and observed performance improvements on all evaluation benchmarks. Effect of Tuning Vision Layers on Performance. We experimented with full parameter finetuning, including and excluding the layers of the vision encoder and concluded that excluding layers relating to the vision encoder consistently results in better performance for QWEN-VL-PRM-3B and QWEN-VL-PRM-7B on all benchmarks. Hence, we tune all parameters when training and freeze all vision layers. A.6 TEST-TIME SCALING DECODING DETAILS Details. In this Section, we will provide details used for test-time scaling decoding when running the following policy models with QWEN-VL-PRM-3B and QWEN-VL-PRM-7B on all evaluation benchmarks. We use vLLM version 0.10.1.1 with V1 engine, transformers version 4.55.2 and flashattn version 2.8.0.post2 for inference. We experience varying results across models when different versions of these key packages are used, and found that fixing these versions resulted in the best overall performance. Table 15: Evaluation decoding configuration for baseline model evaluation. Parameter Top Top Temperature Max New Tokens Repetition Penalty Value 1 0.001 1 0.01 16384 1. 24 Table 13: Evaluation decoding configuration for Qwen family policy models for TTS. Parameter Base Model Top Top Temperature Max New Tokens Repetition Penalty Value Qwen/Qwen2.5-VL-3B/7B/32B-Instruct 16 0.8 20 0.7 16384 (One-shot), 8196 (Greedy) 1.05 Table 14: Evaluation decoding configuration for Gemma family policy models for TTS. Parameter Base Model Top Top Temperature Max New Tokens Repetition Penalty Value google/gemma-3-12B/27B-it 16 0.95 64 0.7 16384 (One-shot), 8196 (Greedy) 1.0 A.7 BASELINE MODEL EVALUATION DECODING DETAILS Details. the following policy models without TTS on all evaluation benchmarks. In this Section, we will provide details used for baseline model evaluation when running A.8 EVALUATION DATA EXAMPLES In this section, we provide some examples from the TTS evaluation with correct and incorrect candidate solutions along with their PRM scores. A.8.1 ALGOPUZZLEVQA Question: The checkerboard shown in the image was originally of 7 * 6 in dimension having total of 42 squares. It uses two colours of squares, one light yellow and one dark yellow, in chequered pattern. Two of the squares have been removed from the board in the position of the white coloured cells, as shown in the image. You have 20 dominoes of size 2 * 1. You can use them as is or you can rotate them to use as 1 * 2 domino. Is it possible to place all the 20 dominoes in the checkerboard to exactly cover all the remaining 40 squares? (A) Yes or (B) No 25 Incorrect Candidate Solution: ### Step 1: Analyze the Checkerboard Pattern The checkerboard in the image has standard alternating pattern of light yellow and dark yellow squares. The board is missing two squares, which are both white (light yellow) in color. The original dimensions of the board were 7 6, meaning it had 42 squares. After removing two squares, there are 40 squares left. ### Step 2: Count the Number of Light Yellow and Dark Yellow Squares In standard checkerboard pattern: - The number of light yellow and dark yellow squares alternates in consistent manner. - For 7 6 board, the total number of squares is 42. Since the pattern alternates, there will be an unequal distribution of light yellow and dark yellow squares because the total number of squares is even, but the dimensions are odd even. - Specifically, in 7 6 board: - There will be 22 light yellow squares. - There will be 20 dark yellow squares. When two light yellow squares are removed: - The number of light yellow squares becomes 22 2 = 20. - The number of dark yellow squares remains 20. Thus, after removing the two light yellow squares, the board has: - 20 light yellow squares. - 20 dark yellow squares. ### Step 3: Understand the Domino Placement Each domino covers exactly 2 squares. Since we have 20 dominoes, they will cover: 20 2 = 40 squares This matches the number of remaining squares on the board. ### Step 4: Check the Color Distribution Each domino must cover one light yellow square and one dark yellow square because the domino is 2 1 or 1 2 and the checkerboard alternates colors. For the dominoes to fit perfectly: - The number of light yellow squares must equal the number of dark yellow squares. From Step 2, we know: - There are 20 light yellow squares. - There are 20 dark yellow squares. Since the number of light yellow squares equals the number of dark yellow squares, it is possible to place the dominoes such that each domino covers one light yellow square and one dark yellow square. ### Step 5: Conclusion Given that the number of light yellow squares equals the number of dark yellow squares, and each domino covers one light yellow square and one dark yellow square, it is possible to place all 20 dominoes to cover the remaining 40 squares. ### Final Answer:boxed{A} PRM Score: 0.85 Correct Candidate Answer: ### Step 1: Analyze the Checkerboard Pattern The checkerboard initially has dimensions of 7 6, which totals 42 squares. The squares alternate between two colors: light yellow and dark yellow, forming standard checkerboard pattern. In such pattern: - Half of the squares are light yellow. - Half of the squares are dark yellow. 26 Since there are 42 squares in total: - There are 21 light yellow squares. - There are 21 dark yellow squares. ### Step 2: Identify Removed Squares Two squares have been removed from the checkerboard, and these squares are marked as white in the image. By examining the image: - Both removed squares are light yellow. This means: - After removing the two light yellow squares, there are now 19 light yellow squares. - The number of dark yellow squares remains unchanged at 21. ### Step 3: Understand the Domino Placement domino covers exactly 2 adjacent squares. Importantly: - Each domino must cover one light yellow square and one dark yellow square because the checkerboard alternates colors. ### Step 4: Check the Feasibility of Covering the Board with Dominoes To cover the entire board with 20 dominoes: - Each domino covers 2 squares, so 20 dominoes will cover 20 2 = 40 squares. - For the dominoes to fit perfectly, the number of light yellow squares must equal the number of dark yellow squares. However, after removing the two light yellow squares: - Light yellow squares: 19 - Dark yellow squares: 21 This creates an imbalance: - There are 2 more dark yellow squares than light yellow squares. Since each domino must cover one light yellow square and one dark yellow square, it is impossible to pair all the dark yellow squares with light yellow squares due to this imbalance. ### Step 5: Conclusion Given the imbalance in the number of light yellow and dark yellow squares, it is not possible to place all 20 dominoes to cover the remaining 40 squares. ### Final Answer:boxed{B} PRM Score: 0.97 27 A.8.2 PUZZLEVQA Question: What is the missing number of the part denoted with question mark? (A) 9 (B) 2 (C) 5 (D) Incorrect Candidate Solution: ### Step-by-Step Analysis ### Step 1: Observing the Image The image consists of two sets of interconnected circles, each containing numbers. The connections between the circles suggest relationship or pattern that needs to be identified. #### Left Set: - Circles contain the numbers: 4, 8, ?, 8, 5, 40. - The question mark indicates missing number that we need to determine. #### Right Set: - Circles contain the numbers: 6, 7, 42. - This set appears complete and may help us identify the pattern. ### Step 2: Identifying Patterns To solve the problem, we need to determine the relationship between the numbers in each set. Lets analyze both sets step by step. #### Analyzing the Right Set (Complete): - Numbers: 6, 7, 42. - Relationship: It appears that the product of the first two numbers equals the third number. - 6 7 = 42. This suggests multiplication pattern. #### Applying the Pattern to the Left Set: - Numbers: 4, 8, ?, 8, 5, 40. - We need to verify if similar multiplication pattern holds. #### Testing the Multiplication Pattern: 1. Top Pair (4 and 8): - 4 8 = 32. - However, there is no 32 in the image, so this might not be the direct pattern. 2. Middle Pair (? and 8): - Let the missing number be x. - According to the pattern, 8 should equal the next number in the sequence. 3. Bottom Pair (8 and 5): - 8 5 = 40. - This matches the given number 40, confirming the multiplication pattern. ### Step 3: Solving for the Missing Number From the bottom pair, we confirmed the multiplication pattern. Now, lets apply it to the middle pair: - The middle pair is ? and 8. - The next number after 8 is 40 (from the bottom pair). - Therefore, ? 8 = 40. Solving for ?: ? = 40 = 5 ### Step 4: Verifying the Solution - If the missing number is 5, then the sequence becomes: 4, 8, 5, 8, 5, 40. - Checking the multiplication: - 8 5 = 40 (correct). - The pattern holds. ### Final Answer: The missing number is 5.boxed{c} PRM Score: 0.85 Correct Candidate Solution: ## Step 1: Analyze the Image and Identify Patterns The image consists of three sets of numbers connected by lines. Each set appears to follow specific mathematical relationship. Lets break down the sets: ### Set 1: - Numbers: 4, 8, ? - Connection: The question mark indicates missing value we need to determine. ### Set 2: - Numbers: 6, 7, 42 - Connection: These numbers are connected, suggesting relationship between them. ### Set 3: - Numbers: 8, 5, 40 - Connection: These numbers are also connected, suggesting another relationship. ## Step 2: Examine the Relationships in Sets 2 and 3 ### Set 2: 6, 7, 42 - Observe the relationship between 6, 7, and 42. - common operation that could relate these numbers is multiplication. - Check: 6 7 = 42. - This confirms that the relationship in Set 2 is multiplication. ### Set 3: 8, 5, 40 - Observe the relationship between 8, 5, and 40. - Again, multiplication seems plausible. - Check: 8 5 = 40. - This confirms that the relationship in Set 3 is also multiplication. ## Step 3: Apply the Observed Pattern to Set 1 ### Set 1: 4, 8, ? - Based on the patterns observed in Sets 2 and 3, the relationship appears to be multiplication. - Assume the missing number (?) is related to 4 and 8 through multiplication. - Check: 4? = 8. ## Step 4: Solve for the Missing Number - From the equation 4? = 8: ? = 8 = 2 ## Step 5: Verify the Solution - The pattern of multiplication holds across all sets: - Set 2: 6 7 = 42 - Set 3: 8 5 = 40 - Set 1: 4 2 = 8 ## Final Answer: boxed{B} PRM Score: 0.98 A.8.3 MMMU DEV SET Question: Baxter Company has relevant range of production between 15,000 and 30,000 units. The following cost data represents average variable costs per unit for 25,000 units of production. If 30,000 units are produced, what are the per unit manufacturing overhead costs incurred? (A). $6 (B). $7 (C). $8 (D). $ Incorrect Candidate Solution: ### Step 1: Analyze the Problem and Identify Key Information The problem asks us to determine the per unit manufacturing overhead costs if 30,000 units are produced. Manufacturing overhead includes both fixed manufacturing overhead and variable manufacturing overhead. From the table: - Fixed manufacturing overhead: $6 per unit - Variable manufacturing overhead: $2 per unit The total manufacturing overhead per unit is the sum of these two components: 30 Total manufacturing overhead per unit = Fixed manufacturing overhead per unit + Variable manufacturing overhead per unit ### Step 2: Understand the Concept of Fixed and Variable Costs - Fixed manufacturing overhead remains constant per unit regardless of the number of units produced within the relevant range (15,000 to 30,000 units). - Variable manufacturing overhead varies directly with the number of units produced. Since the fixed manufacturing overhead is given as $6 per unit and the variable manufacturing overhead is given as $2 per unit, these values should remain consistent within the relevant range. ### Step 3: Calculate Total Manufacturing Overhead per Unit Using the formula: Total manufacturing overhead per unit = Fixed manufacturing overhead per unit + Variable manufacturing overhead per unit Substitute the given values: Total manufacturing overhead per unit = 6 + 2 = 8 ### Step 4: Verify the Relevant Range The problem specifies that the relevant range of production is between 15,000 and 30,000 units. Since 30,000 units fall within this range, the given cost data (fixed and variable overhead rates) can be applied directly without any adjustments. ### Step 5: Match the Answer to the Options The total manufacturing overhead per unit is $8. Comparing this to the provided options: - A. $6 - B. $7 - C. $8 - D. $9 The correct answer is boxed{C}. ### Final Answer: boxed{C} PRM Score: 0.86 Correct Candidate Solution: ### Step 1: Analyze the Problem and Identify Key Information The problem asks us to determine the per unit manufacturing overhead costs for 30,000 units of production. Manufacturing overhead consists of both fixed manufacturing overhead and variable manufacturing overhead. We are given the following information: - Fixed manufacturing overhead: $6 per unit for 25,000 units. - Variable manufacturing overhead: $2 per unit for 25,000 units. - Relevant range of production: 15,000 to 30,000 units. We need to calculate the per unit manufacturing overhead for 30,000 units, considering the behavior of fixed and variable costs. 31 ### Step 2: Understand Cost Behavior 1. Fixed Manufacturing Overhead: - Fixed costs remain constant in total but vary inversely with the number of units produced. For example, if the total fixed manufacturing overhead is constant, the per-unit fixed overhead decreases as production increases. - Given: $6 per unit for 25,000 units. - Total fixed manufacturing overhead = $6 25,000 = $150,000. - For 30,000 units, the per-unit fixed overhead = $150,000 30,000 = $5. 2. Variable Manufacturing Overhead: - Variable costs remain constant per unit regardless of the level of production. - Given: $2 per unit for 25,000 units. - For 30,000 units, the per-unit variable overhead remains $2. ### Step 3: Calculate Total Per Unit Manufacturing Overhead for 30,000 Units Manufacturing overhead = Fixed manufacturing overhead + Variable manufacturing overhead. - Per unit fixed manufacturing overhead for 30,000 units = $5. - Per unit variable manufacturing overhead for 30,000 units = $2. - Total per unit manufacturing overhead = $5 + $2 = $7. ### Step 4: Verify the Answer - The fixed manufacturing overhead decreases from $6 per unit (at 25,000 units) to $5 per unit (at 30,000 units) because the total fixed cost is spread over more units. - The variable manufacturing overhead remains constant at $2 per unit. - Adding these together gives the total per unit manufacturing overhead of $7. ### Final Answer:boxed{B} PRM Score: 0.94 A.8.4 MATHVISTA TESTMINI Question: what is the total volume of the measuring cup? Incorrect Candidate Solution: ### Step 1: Observing the Image - Measuring Cup: The image shows transparent measuring cup filled with flour. The cup has markings indicating measurements in both milliliters (ccm) and grams (g). 32 - Markings on the Measuring Cup: - The markings are clearly visible and include both metric units (e.g., 1000 ccm, 900 ccm, etc.) and gram equivalents (e.g., 800 g, 700 g, etc.). - The topmost marking visible is 1000 ccm, which corresponds to 800 g. - Flour Level: The flour in the measuring cup reaches the 1000 ccm mark. ### Step 2: Connecting Visual Elements to the Problem - The question asks for the total volume of the measuring cup in grams. - From the markings on the measuring cup, we can see that the flour level aligns with the 1000 ccm mark, which corresponds to 800 g. ### Step 3: Reasoning Through the Problem - The measuring cup is designed to measure both volume (in ccm) and weight (in grams) for ingredients like flour. - The markings indicate that 1000 ccm of flour weighs 800 g. Since the flour level in the cup reaches the 1000 ccm mark, the total volume of the measuring cup in grams is directly given by the corresponding gram measurement. ### Final Answer: boxed{800} PRM Score: 0. Correct Candidate Solution: ### Step 1: Observing the Image - Measuring Cup: The measuring cup is clearly visible on the right side of the image. It has markings indicating measurements in grams and other units. - Markings on the Measuring Cup: The markings show scale from 0 to 1000 grams. The largest marking corresponds to 1000 grams. - Context: The measuring cup is likely used for baking, as flour and eggs are also present, suggesting preparation for recipe. ### Step 2: Analyzing the Measuring Cup - The measuring cup is designed for measuring dry ingredients, such as flour, sugar, or powdered substances. - The largest marking on the cup is labeled 1000 g, which indicates the maximum capacity of the measuring cup. ### Step 3: Reasoning - The question asks for the total volume of the measuring cup in grams. - From the markings on the cup, the maximum capacity is explicitly labeled as 1000 grams. - There is no ambiguity in the markings, and the question does not require any additional calculations or estimations. ### Final Answer: boxed{1000} PRM Score: 0.99 A.8.5 MATHVISION TEST Question: Which number should be written in place of the question mark? 33 Incorrect Candidate Solution: Okay, lets analyze this visual reasoning problem. 1. Visual Elements Inventory: Overall Structure: The image depicts train composed of connected boxcars and locomotive. Boxcars: There are 9 boxcars in total. Each boxcar is rectangular prism shape. Numbers: Each boxcar has number printed on its side. The numbers are: 90, 80, 70, ?, 50, 40, 30, 20. Locomotive: The locomotive is at the right end of the train. It has the number 20 on it. Spatial Arrangement: The boxcars are arranged in linear sequence, connected to each other. The locomotive is connected to the boxcar immediately to its left. The question mark is on the fourth boxcar from the left. 2. Reasoning Pathway: The numbers on the boxcars seem to follow decreasing pattern. Lets analyze the sequence: 90, 80, 70, ?, 50, 40, 30, 20 The difference between 90 and 80 is 10. The difference between 80 and 70 is 10. Following this pattern, the number before 50 should be 50 - 10 = 40. Therefore, the missing number should be 40. 3. Verification and Alternative Pathways: Lets see if theres an alternative pattern. Considering the locomotives number (20) and the overall decreasing trend, its reasonable to assume the numbers are decreasing by 10. 90 80 70 60 50 40 30 20. This sequence confirms that 60 is the missing number. 4. Final Answer: Given the established pattern of decreasing numbers by 10, should be 60. the missing number boxed{60} PRM Score: 0.26 Correct Candidate Solution: Okay, lets analyze this visual reasoning problem. Visual Elements: Image Description: The image shows train composed of several connected 1. train cars. Each train car has number written on its side. 2. Numbers: The numbers visible on the train cars are: 90, 80, 70, ?, 50, 40, 30, and 20. The locomotive (engine) has the number 20. 3. Spatial Arrangement: The numbers are arranged sequentially from left to right along the train cars. There is question mark on one of the cars, indicating missing number. 4. Pattern Observation: The numbers seem to be decreasing. 90, 80, 70... 5. Locomotive: The locomotive, at the right end, has the number 20. 34 Reasoning: Lets explore possible pattern. Pathway 1: Arithmetic Sequence: The sequence appears to be decreasing by 10. 90 - 10 = 80 80 - 10 = 70 Following this pattern, the next number should be 70 - 10 = 60. 60 - 10 = 50 50 - 10 = 40 40 - 10 = 30 30 - 10 = 20 (Locomotive) This arithmetic sequence looks consistent with the provided numbers. Final Answer: Based on the observed pattern, the missing number should be 60. boxed{60} PRM Score: 0.98 A.9 ANALYSIS OF ERRORS BETWEEN ONE SHOT AND STEP AGGREGATION SCORING Figure 6: Comparison of error distribution histograms across two scoring methods (One shot score on the left and Step Aggregate Scor on the right), showing the frequency of incorrect samples per score bin. To better understand the performance differences between the two scoring methods, we analyzed the distribution of errors with respect to their assigned scores. Figure 6 shows histograms of incorrect samples for the Oneshot and Step Aggregate scoring approaches. The results reveal two key trends. First, the Oneshot method tends to assign lower scores to incorrect samples on average (mean score 0.62), with relatively even spread of errors across the score bins. This indicates that when the model produces an error, the assigned confidence is often more moderate, reflecting some degree of uncertainty. In contrast, the Step Aggregate method produces markedly different error profile. Incorrect samples under this scoring scheme are skewed towards the higher end of the score range (mean score 0.76), with large concentration of errors receiving scores close to 1.0. This indicates that the Step Aggregate approach is more prone to overestimating its confidence when the output is incorrect. likely reason is that even when final solution is wrong, it may consist largely of correct intermediate 35 steps, with only few erroneous steps leading to the incorrect answer. As result, the aggregation process inflates the overall score, causing erroneous samples to receive higher scores on average. This phenomenon may explain why the Oneshot scoring approach is more effective than Step Aggregation for selecting the best candidate solution. Because Step Aggregation assigns disproportionately high scores to incorrect solutions, driven by the sparsity of erroneous steps within largely correct reasoning chains, it risks favoring poor solutions that nonetheless appear strong under the aggregated metric. In contrast, Oneshot scoring evaluates the solution holistically, resulting in more reliable selection from the candidate solutions."
        }
    ],
    "affiliations": [
        "AI Singapore",
        "Nanyang Technological University"
    ]
}