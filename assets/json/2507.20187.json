{
    "paper_title": "Diversity-Enhanced Reasoning for Subjective Questions",
    "authors": [
        "Yumeng Wang",
        "Zhiyuan Fan",
        "Jiayu Liu",
        "Yi R. Fung"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Large reasoning models (LRM) with long chain-of-thought (CoT) capabilities have shown strong performance on objective tasks, such as math reasoning and coding. However, their effectiveness on subjective questions that may have different responses from different perspectives is still limited by a tendency towards homogeneous reasoning, introduced by the reliance on a single ground truth in supervised fine-tuning and verifiable reward in reinforcement learning. Motivated by the finding that increasing role perspectives consistently improves performance, we propose MultiRole-R1, a diversity-enhanced framework with multiple role perspectives, to improve the accuracy and diversity in subjective reasoning tasks. MultiRole-R1 features an unsupervised data construction pipeline that generates reasoning chains that incorporate diverse role perspectives. We further employ reinforcement learning via Group Relative Policy Optimization (GRPO) with reward shaping, by taking diversity as a reward signal in addition to the verifiable reward. With specially designed reward functions, we successfully promote perspective diversity and lexical diversity, uncovering a positive relation between reasoning diversity and accuracy. Our experiment on six benchmarks demonstrates MultiRole-R1's effectiveness and generalizability in enhancing both subjective and objective reasoning, showcasing the potential of diversity-enhanced training in LRMs."
        },
        {
            "title": "Start",
            "content": "Diversity-Enhanced Reasoning for Subjective Questions Yumeng Wang* Zhiyuan Fan Jiayu Liu Yi R. (May) Fung"
        },
        {
            "title": "Hong Kong University of Science and Technology",
            "content": "ywanglu@connect.ust.hk yrfung@cse.ust.hk 5 2 0 2 7 2 ] . [ 1 7 8 1 0 2 . 7 0 5 2 : r a"
        },
        {
            "title": "Abstract",
            "content": "Large reasoning models (LRM) with long chain-of-thought (CoT) capabilities have shown strong performance on objective tasks, such as math reasoning and coding. However, their effectiveness on subjective questions that may have different responses from different perspectives is still limited by tendency towards homogeneous reasoning, introduced by the reliance on single ground truth in supervised fine-tuning and verifiable reward in reinforcement learning. Motivated by the finding that increasing role perspectives consistently improves performance, we propose MultiRoleR1, diversity-enhanced framework with multiple role perspectives, to improve the accuracy and diversity in subjective reasoning tasks. MultiRole-R1 features an unsupervised data construction pipeline that generates reasoning chains that incorporate diverse role perspectives. We further employ reinforcement learning via Group Relative Policy Optimization (GRPO) with reward shaping, by taking diversity as reward signal in addition to the verifiable reward. With specially designed reward functions, we successfully promote perspective diversity and lexical diversity, uncovering positive relation between reasoning diversity and accuracy. Our experiment on six benchmarks demonstrates MultiRole-R1s effectiveness and generalizability in enhancing both subjective and objective reasoning, showcasing the potential of diversity-enhanced training in LRMs."
        },
        {
            "title": "Introduction",
            "content": "Advances in OpenAI o1 and DeepSeek R1 style models (Jaech et al., 2024; DeepSeek-AI et al., 2025) with long reasoning chains (Wei et al., 2023) have significantly enhanced the models capability *Equal contribution. Project leader of student team. Corresponding author 1The code and data of this work is available at https://github.com/toward-agi/diverse-o1.git. Figure 1: subjective reasoning task example which demonstrates that the perpective shifts may lead to different answers. We illustrate that diverse perspective thinking benefits subjective question answering. in various reasoning tasks (Yu et al., 2025; Wu et al., 2024b). Extended reasoning chains enable models to systematically divide and conquer complex problems, iteratively refining intermediate results, and methodically verifying the outputs. The DeepSeek R1 style models reached performance comparable to human experts in many objective reasoning tasks such as math (Cobbe et al., 2021; Wang et al., 2025a; Guo et al., 2025) and coding (Jain et al., 2024). This success highlights that scaling up the test time computation can significantly boost performance on complex, structured tasks relative to standard single-step prompting (Jaech et al., 2024; Wei et al., 2023). Beyond objective tasks with single correct answer, there is growing interest in applying Large Reasoning Models (LRMs) to subjective or openended problems (SarÄ±tas et al., 2025; Guo et al., 2024). This homogenization stems from standard training pipelines which reduce subjective problems to single ground truth for both supervised learning and reinforcement learning, thereby penalizing diverse perspectives. As illustrated in Figure 1, due to the single inherent thinking process of LRM, the model tends to rely on homogeneous reasoning trajectories without exploring alternative perspectives, hindering their capacity to incorporate diverse reasoning strategies. Prior works solutions mainly fall into two categories: (1) Research on multi-agent debates has demonstrated that incorporating diverse role perspectives substantially improves performance on subjective tasks (Aoyagui et al., 2025; Cheng et al., 2024; Liu et al., 2025b), but these methods require multiple interacting models; (2) some approaches ensemble solutions from varied decoding paths (Wang et al., 2023), yet the stochastic nature of these paths may not necessarily introduce broader range of perspectives (Naik et al., 2024). To address this gap, we propose to enhance the diversity of the DeepSeek R1 style reasoning chain, by dynamically integrating diverse role perspectives for subjective question answering, including tasks such as moral dilemmas, opinion-based questions, ambiguous question answering, and so on. To begin with, we conduct pilot study to confirm two key factors: the effectiveness of long reasoning chains in subjective reasoning tasks, and the impact of varying the number of role perspectives incorporated during reasoning. Our findings show that sequential test-time scaling performance improves as reasoning chains are lengthened, but begins to decline once they surpass an optimal length. Similarly, we identify that there exists an optimal range for the number of role perspectives in the reasoning chain. Building upon these insights, we propose novel framework MultiRole-R1 that enhances the diversity of long chains of thought. Specifically, we optimize the role perspective diversity and the reasoning diversity. MultiRole-R1 starts with novel data construction pipeline combining parallel scaling with sequential scaling to effectively incorporate broader array of role perspectives into the long reasoning chains. We subsequently refine the model through supervised fine-tuning (SFT), instructing the model to learn the multi-role reasoning format and enhance perspective diversity. Furthermore, to enhance the diversity of the reasoning process, we introduce carefully designed diversity reward function implemented through multi-role Generalized Reward Policy Optimization (GRPO) framework, taking diversity as an additional reward signal. By training on subjective questions only, MultiRoleR1 boosts the performance of both subjective and objective reasoning tasks. Our contribution can be summarized as follows: We perform pilot analysis that reveals the scaling law of reasoning length and role perspectives in subjective reasoning. We propose MultiRole-R1, training paradigm that incorporates unsupervised SFT data generation and GRPO with reward shaping, which effectively promotes perspective and lexical diversity. By training on subjective questions solely, MultiRole-R1 achieves state-of-the-art performance in both subjective and objective reasoning benchmarks. Our experiments and analysis highlight an interesting phenomenon: optimizing for diversity often aligns with, and can even enhance, the accuracy objective. This synergistic relationship underscores our methods effectiveness and its robust generalizability to other domains."
        },
        {
            "title": "2 Related Work",
            "content": "Test-Time Scaling Recent advancements, such as the success of Deepseek-R1 (DeepSeek-AI et al., 2025) in demonstrating robust long-chainof-thought (CoT) reasoning, have brought significant attention to test-time scaling (Wang et al., 2025c; Liu et al., 2025a; Li et al., 2025b). Current approaches to test-time scaling can be broadly classified into four categories (Zhang et al., 2025). Sequential scaling, as explored by Madaan et al. (2023) and Xiang et al. (2025), iteratively refines models state and output, with each step building on the previous to form coherent chain of computations. In contrast, parallel scaling (Wu et al., 2025; Wang et al., 2024a) leverage parallelism to expand the coverage of reasoning chain, thereby increasing the likelihood of identifying correct solutions. Additionally, hybrid scaling, as proposed in Wang et al. (2024b) and Zhang et al. (2024), combines parallel generation of diverse options with sequential evaluation and refinement to enhance reasoning capabilities. Finally, internal scaling (Muennighoff et al., 2025; Ye et al., 2025) empowers models to autonomously allocate computational resources during inference, moving away from reliance on externally guided strategies. While these approaches have demonstrated impressive performance improvements, they primarily focus on enhancing models reasoning capabilities in mathematical or coding tasks which contain fixed and verifiable answers. Such methods largely overlook the importance of promoting diversity in reasoning for open-ended questions, which require consideration from multiple perspectives. Subjective Tasks and LLM Role-Playing Subjective task is type of task that differs from objective tasks, such as commonsense reasoning, code generation and arithmetic reasoning (Wang et al., 2025b) -which involve questions with definitive correct or incorrect answers. Subjective task, due to its open-ended nature, has answers that may change with perspective-shifts or context change (Wang et al., 2025b; Jentzsch and Kersting, 2023; Wu et al., 2024a). This includes culture-related question answering (Huang and Yang, 2023; DURMUS et al., 2024; Huang et al., 2025b), subjective language interpretation (Jones et al., 2025), ethical question answering (Hendrycks et al., 2021), creative question answering (Lu et al., 2024), and so on. LLM role-playing (Shao et al., 2023), including multi-role discussion (Wang et al., 2024c; Du et al., 2023; Liang et al., 2024) is one of the most used techniques to study subjective task. It features specialized AI systems to simulate assigned personas, ranging from real-life figures to fictional characters (Chen et al., 2024; Li et al., 2025a). The capacity to simulate different personas has been shown to elicit human-like responses, and introduce more diverse and creative reasoning paths (Naik et al., 2024) when solving LLM-related tasks (Wang et al., 2024d). In this work, we adopt parallel multi-role reasoning at test time to enable the model think from diverse perspective via independent reasoning paths."
        },
        {
            "title": "3 Pilot Analysis",
            "content": "Drawing on evidence that diverse perspectives enhance performance on subjective tasks (Muscato et al., 2025; Hayati et al., 2024), we introduce perspective diversity into Deepseek R1-style reasoning chains via test-time scaling (Snell et al., 2024). We employ two complementary strategies: sequential scaling to deepen reasoning through iterative reflection (Zeng et al., 2025), and parallel scaling to enhance exploration by generating multiple reasoning paths (Liu et al., 2025c; Rodionov et al., 2025). We evaluate this approach on three subjective reasoning datasets: GlobalOpinion QA (DURMUS et al., 2024), BBQ (Parrish et al., 2022), and ETHICS (Hendrycks et al., 2021) across four decoding strategies adapted from Jiang et al. (2025), with examples provided in Appendix 7: Zero think: Force the model to respond without thinking, i.e. <think></think>. Less think: Force the model to think for one sentence only <think>Okay, the user ask for this, can answer it without thinking much.</think> Regular think: Let the model start with <think> and ends its thinking naturally. More think: Starts with regular think. When the end-of-thinking is reached, forcefully replace the <think> token and append wait\" that encourages the model to think more. As shown in Figure 2 (a), increasing the amount of test-time reasoning through scaling indeed yields performance gains. We thus incorporate More think as our baseline. Scaling Law of Reasoning Length Having established the effectiveness of test-time scaling, natural follow-up question is whether longer reasoning chains always result in better performance. However, prior studies on mathematical and commonsense reasoning have shown that excessively long reasoning chains do not necessarily lead to improved outcomes (Sui et al., 2025; Ballon et al., 2025; Qin et al., 2025). Therefore, to control the reasoning length for sequential scaling leveraging concepts from budget forcing (Muennighoff et al., 2025; Huang et al., 2025a), we conduct an investigation experiment aimed at roughly estimating the optimal reasoning length specifically for subjective problems. We adhere to the experimental setup proposed by Jiang et al. (2025) and experiment on the optimal number of replacing the endof-thinking delimiter. As shown in Figure 2 (b), increasing the number of \"Wait\" tokens generally leads to performance improvements across most tasks. For GLOQA, performance continues to improve with more Wait insertions, whereas for BBQ and ETHICS, the gains peak around three insertions and diminish or even degrade beyond that point. Based on this observation, we adopt three Wait tokens as balanced setting that achieves clear performance gains without excessive computational overhead. We refer to this configuration as the More think strategy, which not only fosters Figure 2: (a) The performance of Deepseek-r1-distill-qwen-7b (DeepSeek-AI et al., 2025) under different reasoning length settings across different datasets. The bar chart shows that longer reasoning chains result in higher accuracy on subjective tasks. (b) The performance of Deepseek-r1-distill-qwen-7b (DeepSeek-AI et al., 2025) under different reasoning length settings across different datasets. (c) Demonstration of the number of distinct opinions increases as more roles are involved in single reasoning chain. more cautious and reflective reasoning, but also enables longer reasoning chains capable of supporting more nuanced and diverse role-based perspectives. Scaling Law of Role Perspectives We explore the relationship between the number of roles and the answer diversity and consensus accuracy on subjective QA tasks. Intuitively, each additional role contributes new viewpoint, expanding the solution space. Lu et al. (2024) shows multi-agent debates with 3 fixed roles achieve optimal creativity, but scaling different role perspectives in single reasoning chain at test time remains unexplored. We conduct pilot analysis that directly instructs the model to think from different viewpoints, with from 1 to 6. Results in Figure 2 (c) show that the number of distinct opinions increases as more roles are involved. In general, we observed that the number of distinct opinions plateaus as = 4. Hence, in the following study, we control the number of generated roles = 2, 3, 4 for efficient generation."
        },
        {
            "title": "4 Method",
            "content": "As illustrated in Figure 3, our framework consists of three stages: parallel multi-role reasoning, multirole finetuning and multi-role reinforcement learning. Formally, given an input question and reasoning model M, our goal is to diversify the reasoning path ."
        },
        {
            "title": "4.1 Parallel Multi-Role Reasoning",
            "content": "Multi-Role Exploration To model multiperspective reasoning, we first identify set of context relevant roles (e.g., domain experts, stakeholders, or personas) through few-shot prompting (Brown et al., 2020), denoted as = {R1, R2, ..., Rn}. In particular, we prompt the model to generate roles that may have conflicting viewpoints. The motivation for this is to explore diverse available perspectives. Given candidate roles R, we define the selection probabilities: (RiQ) = sof tmax(E[M(RiQ)] +Î»ERi[1 sim(Ri, Rj)]), (1) where sim(Ri, Rj) = cos(hRi, hRj Q) and denotes the embedding of the LLM M. These terms represent that, in the role selection process, we prioritize role answers that are relevant to the question and contrastive to the existing opinions. , (2) Ri Self-Consistency Filtering For each role Ri, we generate candidate reasoning paths M(Q, Ri) = TRi = {T (1) , ..., (k) }. This is achieved by Ri Ri adjusting the sampling temperature Ï = 1 to encourage diverse reasoning paths. To ensure the coherence among different responses of each role, we then apply self-consistency filtering (Chen et al., 2023; Wang et al., 2025d) through majority voting and only keep the most consistent answer: 1(T (j) Ri ËTRi = argmax (cid:88) (2) ), j=1,T TRi where 1 is the indicator function and denotes semantic equivalence (e.g. same roles give different answers). This approach extends ensemble methods by decoupling role-specific reasoning trajectories, ensuring that conflicting viewpoints remain independently generated and self-consistent."
        },
        {
            "title": "4.2 Multi-Role Finetuning",
            "content": "To enhance our models reasoning capabilities in both accuracy and diversity, we first fine-tune the Figure 3: Illustration of our MultiRole-R1framework. Stage 1: we collect our data by utilizing budget forcing (sequential scaling) and contrastive role reasoning, sampling outputs from the decoder (parallel scaling). Stage 2: we automatically construct our training data that integrates various perspectives into single reasoning chain, and then fine-tune the model to follow the multi-role reasoning format. Stage 3: we utilize GRPO with reward shaping to optimize the reasoning accuracy and diversity. Two accuracy evaluation approaches are applied, depending on whether the ground-truth varies by roles. By taking diversity as an additional reward, we encourage exploration of the reasoning chain, expanding the answer search space. base model with self-consistency filtered data. This initial stage teaches the model to adopt multi-role reasoning format, which involves shifting perspectives throughout its reasoning process. To further promote this diversity, our supervised fine-tuning (SFT) dataset includes 968 examples specifically designed for varied role generation. Sequential Multi-Role Merging Given filtered role perspectives { ËTR1, ..., ËTRm}, we generate random combination of role orderings Î  to avoid the effect of position bias (Zheng et al., 2023a). For example, given multi-role combination Ï = {Ri, Rj, Rk}, we construct the training data as follows: Dtrain = (cid:91) {(Q ËTRi ËTRj ËTRk ) Ï}. (3) ÏÎ  We consider two merging strategies depending on task type to allow dynamic integration of role reasoning traces while respecting the nature of the reasoning task: (1) Divergent merging: when the roles are supposed to give different answers, we encourage diverse perspectives to maximize coverage of the searching space. Final prediction is derived via weighted aggregation over differing viewpoints. (2) Convergent merging: when the roles are supposed to give consistent answers, they reach consensus via majority voting within single sequence, which is in nature soft majority voting within single sequence. Multi-Role Supervised Finetuning To ensure the quality of the SFT data, we apply several filtering strategies to the merged dataset. To mitigate verbosity bias (Zheng et al., 2023b) and reasoning shortcut behaviors, we remove entries in the top and bottom 10th percentiles of answer length. We also discard instances with formatting errors or invalid string patterns. This results in the final 2,700 SFT training data. Additionally, since the gold labels are accessible in the original dataset, we apply supervised, ground-truth-guided filtering approach as comparison to our unsupervised selfconsistency method. Results of this comparison are presented in Section 7.1."
        },
        {
            "title": "4.3 Multi-Role Reinforcement Learning",
            "content": "We adopt Group Relative Policy Optimization (GRPO) (Shao et al., 2024) for Multi-Role reinforcement learning, which is trained on top of the SFT model. GRPO optimizes the policy by sampling group of candidate outputs for each prompt and comparing their reward. We incorporate two types of rewards: an accuracy reward Racc provided by verifiable reward model that checks answer correctness, and diversity reward Rdiv computed from the input text as shaping signal. The total shaped reward is given by = 0.9 Racc + 0.1 Rdiv. This follows the rewardshaping paradigm (Ng et al., 1999), where the auxiliary Rdiv guides learning without changing the optimal policy. During training, we observe synergetic effect of optimizing the diversity and accuracy objectives. This also mitigates issues observed in the SFT baseline, such as excessive verbosity and repetitive reasoning (Toshniwal et al., 2025). Finally, note that GRPO computes group advantages A1, A2, . . . , AG instead of standard reward, which is given by: ËAi,t = (Ri,t Âµ)/Ï, {1, . . . , G}. Hence, group with uniform rewards (all 0s or all 1s) would give zero advantage and stall learning. By adding the diversity term, we ensure intra-group reward variance, enabling informative gradients and continued optimization."
        },
        {
            "title": "5.1 Datasets",
            "content": "We train our model on three subjective tasks: ambiguous question answering (BBQ by Parrish et al. (2022)), opinion-based QA (GlobalOpinionQA by DURMUS et al. (2024)), and ethical dilemma (ETHICS by Hendrycks et al. (2021)). To evaluate the effectiveness and generalizability of our approach, we test on three additional datasets: cultural natural language inference (CALI by Huang and Yang (2023)), commonsense reasoning (CSQA by Talmor et al. (2019)), and mathematical reasoning (GSM8K by Cobbe et al. (2021)). Specifically, for the testing data, CALI consists of the out-of-domain (O.O.D.) subjective questions, while CSQA and GSM8K consist of O.O.D. objective questions. It is worth noting that among these benchmarks, GLOQA and CALI have roledependent ground truths, whereas the others rely on single ground truth for all roles."
        },
        {
            "title": "5.2 Baselines",
            "content": "In-Context Learning We first incorporate the following in-context learning (Brown et al., 2020) settings: (1) Zero-Shot CoT (Kojima et al., 2023; (2) Role Playing PromptWei et al., 2023), ing (Kong et al., 2024) and (3) Self-Refine Prompting (Madaan et al., 2023). More Think As observed by Muennighoff et al. (2025), extending the reasoning chain length can further enhance the reasoning capabilities of o1style models. In MultiRole-R1, this is achieved by suppressing the end-of-thinking token and appending continuation string (e.g., wait, need to think from {role}s perspective\") to encourage extended reasoning from different role perspective. In the more think baseline, we employ reasoning length three times longer than regular think, as it offers balance between efficiency and accuracy based on our pilot analysis. Supervised Finetuning We perform supervised finetuning on the base model on the selfconsistency filtered dataset of size 2,700. Our SFT training and evaluation are conducted via LlamaFactory (Zheng et al., 2024). Direct Preference Optimization We introduce another SFT+RL pipeline as an ablation to MultiRole-R1. In this setting, DPO is applied to the self-consistent SFT model, using ground-truthhinted role answers as positive samples and inconsistent role answers as negative samples."
        },
        {
            "title": "5.3 Models",
            "content": "Our experiment is performed on range of opensource LRMs including R1-Distill-Qwen-7B, R1Distill-Llama-8B and R1-Distill-Qwen-14B models (DeepSeek-AI et al., 2025), and Qwen38B (Qwen Team, 2025) with reasoning mode."
        },
        {
            "title": "5.4 Metrics",
            "content": "Accuracy Taking into account the subjective nature of role-based reasoning where the ground truth for subjective questions may vary across different roles, we adopt two different perspective merging strategies during evaluation. (1) Divergent Merging: for tasks such as CALI and GLOQA, each role is answer ai is compared with the corresponding ground truth gi, where the divergent accuracy is given by: Accdiv = 1 (cid:88) i=1 1[ai = gi]. (2) Convergent Merging: datasets like BBQ, ETHICS, CSQA and GSM8K have answers invariant with role-perspectives. We aggregate different"
        },
        {
            "title": "GLOQA",
            "content": "CALI (O.O.D)"
        },
        {
            "title": "ETHICS",
            "content": "CSQA (O.O.D) GSM8K (O.O.D) Acc. Div. Acc. Div. Acc. Div. Acc. Div. Acc. Div. Acc. Div. Zero-shot CoT Self-Refine Role-Play More think Ours SelfConsis SFT Ours SelfConsis SFT+DPO Ours SelfConsis SFT+GRPO MultiRole-R1 SelfConsis SFT+GRPO(RS) Zero-shot CoT Self-Refine Role-Play More think Ours SelfConsis SFT Ours SelfConsis SFT+DPO Ours SelfConsis SFT+GRPO MultiRole-R1 SelfConsis SFT+GRPO(RS) Zero-shot CoT Self-Refine Role-Play More think Ours SelfConsis SFT Ours SelfConsis SFT+DPO Ours SelfConsis SFT+GRPO MultiRole-R1 SelfConsis SFT+GRPO(RS) Zero-shot CoT Self-Refine Role-Play More think Ours SelfConsis SFT Ours SelfConsis SFT+DPO Ours SelfConsis SFT+GRPO MultiRole-R1 SelfConsis SFT+GRPO(RS) 62.45 74.08 73.61 80.76 85.88 86.41 94.30 94. 80.89 74.20 74.40 88.20 89.69 90.52 94.47 95.55 85.01 90.42 91.18 94.57 94.40 94.98 95.98 97.50 91.71 88.93 89.77 95.18 94.05 94.21 95.91 96.98 56.02 73.13 74.68 80.44 81.67 60.43 85.52 86.25 79.92 75.85 80.91 84.11 82.64 80.43 85.75 89.58 68.06 80.13 81.87 80.67 75.06 67.21 86.88 90. 70.10 53.07 41.58 74.20 74.02 74.83 85.47 88.15 (R1-Distill-Qwen-7B) 32.62 43.13 41.67 36.42 43.13 44.20 47.22 49.10 65.88 59.88 77.75 86.90 85.58 61.17 87.46 89.67 50.30 50.76 52.69 60.45 67.35 68.19 70.83 70.85 (R1-Distill-Llama-8B) 38.41 43.19 44.87 44.04 48.17 48.89 48.55 49.06 87.07 81.11 83.02 87.19 87.26 80.97 89.36 91.78 60.84 61.95 62.70 64.41 70.05 69.81 69.26 71.48 (R1-Distill-Qwen-14B) 36.82 49.04 49.90 41.60 50.98 51.33 51.73 53.98 42.13 45.25 47.57 43.39 50.32 50.68 51.37 51. 79.18 69.40 75.73 84.04 81.04 65.03 90.33 91.32 75.05 71.28 67.41 75.90 76.08 75.82 75.65 76.50 (Qwen3-8B) 60.99 85.00 54.89 78.98 77.07 74.14 87.74 89.88 73.40 69.40 70.26 75.10 75.96 76.10 77.83 77.95 52.22 66.09 67.43 78.82 78.94 64.09 82.15 83.31 73.98 78.87 77.32 80.30 79.77 76.62 83.37 90. 71.83 78.22 70.74 76.81 73.65 71.71 84.92 89.08 57.43 48.16 50.49 68.44 72.78 69.58 80.36 83.84 51.82 52.19 50.83 64.44 67.45 67.28 69.50 66.83 62.46 60.96 64.24 68.06 70.56 71.22 75.63 75.84 73.63 76.48 77.16 79.36 81.45 81.88 83.50 86.00 72.29 70.64 70.67 78.26 78.39 78.09 79.82 81. 36.14 37.36 37.89 81.53 82.19 68.51 85.40 87.27 79.44 80.17 79.78 83.99 81.36 83.64 87.89 96.54 72.20 83.22 75.30 83.33 71.34 42.32 89.42 92.89 76.68 49.60 47.83 72.45 68.35 50.31 86.84 89.09 63.06 54.02 55.07 64.50 66.88 67.24 69.43 66.94 67.21 63.77 67.32 70.42 70.86 71.28 73.71 75. 75.85 76.55 75.71 79.36 81.50 79.82 81.19 82.00 80.81 69.22 72.98 81.20 81.00 80.45 81.19 82.10 83.83 77.61 76.20 85.85 83.10 69.83 86.85 87.96 83.39 82.61 82.27 84.73 83.88 82.71 87.96 92.98 83.09 82.31 79.05 81.77 77.60 69.41 89.64 91.61 57.84 50.99 48.17 73.07 73.50 64.87 86.40 87. 80.48 87.01 85.66 82.05 80.62 81.51 85.58 87.36 78.87 80.95 77.33 83.30 86.02 86.34 87.49 89.79 85.58 84.73 91.50 88.76 91.61 90.92 91.87 93.43 85.41 84.91 93.58 90.02 91.62 91.84 91.97 94.98 68.08 80.37 72.87 81.79 74.87 67.56 82.16 82.46 76.52 81.24 75.02 84.12 81.53 81.81 85.31 88. 70.68 80.24 76.60 80.94 91.62 68.69 86.36 87.24 70.49 82.31 81.93 75.07 70.23 65.32 85.98 86.93 Table 1: Main results of the baselines (specified in Section 5) and our proposed method. Acc. is the accuracy of the task (in %) and Div. measures the length normalized diversity score of the reasoning chain (in %). We include two ablations of MultiRole-R1, including SFT on self-consistency filtered data only (Ours SelfConsis SFT), and also SFT with vanilla GRPO (Ours SelfConsis SFT + GRPO). GRPO(RS) represents GRPO with reward shaping, which is used in MultiRole-R1. O.O.D. denotes the datasets that are for testing only. Detailed decomposition of the diversity score is shown in Appendix E. roles answer to obtain consensus, and then compare it to the ground truth: Ëa = argmax (cid:88) 1(ai = Ëa), Acccon = 1[Ëa = g]. Diversity To quantify the diversity of modelgenerated reasoning, we design composite metric that captures linguistic variation across multiple dimensions. Inspired by prior work on lexical and entropy-based diversity in natural language generation (Li et al., 2016; Tanaka-Ishii and Aihara, 2015), our metric is weighted sum of eight complementary diversity signals, including lexical, token entropy, sentence length, sentence pattern, adjacent sentence, Yules K, distinct N-gram and function word diversity. Formal definition of the diversity metrics can be found in Appendix E. Formally, we express the final combined diversity score in the formula below: Df inal = 0.15 (cid:88) Di + 0.1 (cid:88) Dj, (4) where: Di {Dlex, Dent, Dpat, Dbi} and Dj {Dlen, Dadj, Dyule, Df unc}. The choice of the weighting is further illustrated in Section 7.2."
        },
        {
            "title": "6.1 Main Results",
            "content": "Table 1 shows that with multi-role SFT and GRPO with reward shaping, MultiRole-R1 achieves superior performance in both subjective and objective reasoning tasks, surpassing More Think by an average of 7.6% and 3.8% in accuracy and diversity. Notably, by training solely on subjective tasks, we enhance both the subjective and objective reasoning abilities of models. Figure 4: (a) We plot the accuracy against the diversity for each setting (e.g. zero-shot, role-play, morethink, etc.) for each dataset. We observe positive linear correlation between the two metrics. (b) Qualitative example of the 34 most frequent roles in the training data generated by LRMs. more detailed visualization is presented in Figure 5. For the choice of RL algorithm, we found that GRPO yields more accurate and diverse responses than DPO. We attribute this to fundamental mismatch between DPOs training format and the nature of our task. Subjective questions do not have single \"correct\" answers, yet DPO requires rigid \"preferred-rejected\" pair for training. These results therefore demonstrate the superiority of GRPO for this context and validate the effectiveness of explicitly optimizing for diversity. Interestingly, Table 9 to 12 in the appendix show that our approach results in decrease in answer tokens compared to SFT, suggesting that mere verbosity does not lead to more accurate answers."
        },
        {
            "title": "6.2 Domain Generalization",
            "content": "As shown in Table 1, besides in-domain subjective questions that are included in the training set, MultiRole-R1 successfully boosts the performance in all the benchmarks. This shows our approachs generalizability is two-fold: generalizability to other subjective questions, and also generalizability to objective questions. We reveal that by only training on three subjective datasets, the model gains generalized understanding of role generation and role reasoning, and has near 10% accuracy gain in subjective tasks like cultural natural language inference in CALI. Moreover, without any training in objective questions, MultiRoleR1 significantly enhances the models performance on commonsense reasoning and math reasoning benchmarks. This demonstrates that subjective question training can potentially enhance the performance of objective questions. An explanation is that the nature of subjective questions pushes the model to gain insights from various perspectives, which collectively guides the model to approach the optimal solution. Model BBQ GLOQA CALI ETHICS CSQA GSM8K R1-Distill-Qwen-7B R1-Distill-Llama-8B R1-Distill-Qwen-14B Qwen3-8B 94.2 89.0 82.0 76.0 45.5 48.0 59.0 65.4 89.2 82.4 60.5 89.3 98.6 83.6 77.2 73.0 92.4 85.6 57.4 82.5 58.4 64.9 79.2 33. Table 2: The task break-down correlation coefficient (in %) between accuracy and diversity."
        },
        {
            "title": "6.3 Accuracy-Diversity Correlations",
            "content": "Our analysis reveals positive correlation between accuracy and diversity (Figure 4a). This relationship is reinforced by strong per-task correlation between diversity and accuracy (0.9), which significantly exceeds the correlation with response length  (Table 2)  . These results suggest that performance gains in modern reasoning models like o1 or R1 are driven by scaling law of diversity, rather than superficial verbosity. Crucially, this demonstrates that optimizing for diversity does not compromise correctness but rather enhances accuracy. One possible explanation is that optimizing for diversity can serve as useful inductive bias, enabling the model to explore broader solution space and discover more accurate, perspective-aligned answers in subjective tasks. 6.4 Role Coverage We leverage the LLM itself to generate roles pertinent to the context of the question. In total, there are 968 distinct roles in the training data, enhancing the perspective diversity of the LRM. We leverage the LLM itself to generate roles relevant to the context of each question. These roles demonstrate broad coverage, including different moral philosophies, nationalities, identity groups, and specific Model BBQ GLOQA CALI (O.O.D) ETHICS CSQA (O.O.D) Human Div. Human Div. Human Div. Human Div. Human Div. GSM8K (O.O.D) Alignment Human Div. R1-Distill-Qwen-7B R1-Distill-Llama-8B R1-Distill-Qwen-14B Qwen3-8B 8.95 7.94 8.81 8.51 86.25 89.58 90.17 88.15 9.62 8.33 9.52 9. 89.67 91.78 91.32 89.88 7.88 8.78 8.43 7.42 83.31 90.55 89.08 83.84 8.24 9.82 9.65 9.25 87.27 96.54 92.89 89.09 8.51 9.15 9.18 8. 87.96 92.98 91.61 87.93 7.13 7.21 8.24 8.88 82.46 88.45 87.24 86.93 0.88 0.93 0.95 0.89 Table 3: We use human ratings as reference to set the weights. The alignment score of human rating and the combined diversity score shows that our metric highly aligns with human preference. individuals pertinent to the questions. Figure 4 (b) shows qualitative example of the most frequently occurred roles."
        },
        {
            "title": "7.1 Filtering Strategy of the Training Data",
            "content": "Our methodology compares two data sampling strategies for Supervised Fine-Tuning (SFT): an unsupervised self-consistency approach and supervised method that leverages ground-truth agreement. Table 4 reports the test performance on equalsized datasets generated by each strategy. The results indicate that while self-consistency filtering can yield slightly lower accuracy in some cases, its performance is broadly comparable to, and can even outperform, supervised filtering. This highlights that using noisy, unsupervised data is sufficient and viable for our tasks (Wang et al., 2025d). BBQ GLOQA CALI ETHICS CSQA GSM8K Consis. Filter GT Filter 85.55 88.40 Consis. Filter GT Filter 89.69 87. Consis. Filter GT Filter 94.40 94.88 (R1-Distill-Qwen-7B) 47.13 45.55 67.35 65.95 67.45 68.44 (R1-Distill-Llama-8B) 48.17 49. 72.05 69.27 70.56 72.15 (R1-Distill-Qwen-14B) 50.98 52.29 76.98 76.28 81.45 81.57 66.88 68. 80.62 80.63 70.86 71.06 83.30 84.20 80.50 80.79 91.61 91.28 Consis. Filter GT Filter 94.05 94.80 50.32 51.07 75.96 76.15 78.39 80.19 81.00 82.13 91.62 91. (Qwen3-8B) Table 4: The evaluation accuracy after finetuning on (1) consistency filtering and (2) ground-truth hinted sampling data."
        },
        {
            "title": "7.2 Design of the Diversity Weighting",
            "content": "In equation (4), we design our diversity metric as combined score of lexical diversity, token entropy diversity, etc. Our guiding principle of diversity metric design is to balance the weights of different diversity factors and make it align with human intuition as much as possible. We present Table 3 showing qualitative alignment scores between the metrics and human diversity ratings. We manually label 60 outputs from different settings, from diversity scale of 1 to 10. Taking the human rating as reference, we set the weight of the diversity metrics such that the correlation coefficient between the overall diversity and the human rating is highly aligned. Table 3 shows that our diversity metric has high alignment score with human ratings, showcasing the reasonableness of our weighting."
        },
        {
            "title": "7.3 Advanced Math Reasoning",
            "content": "To further validate the robustness of our pipeline on unseen objective tasks, we introduced the AIME 2024 benchmark, which is considerably more challenging than GSM8K. Notably, despite being trained exclusively on subjective datasets, our method demonstrates consistent O.O.D. performance improvement on AIME, similar to the gains observed on GSM8K. Model r1-dist-qwen7B r1-dist-llama8B r1-dist-qwen14B qwen3-8B Zero-Shot Self-Refine Role-Play More think SFT SFT+GRPO MultiRole-R 55.5 55.9 56.3 58.6 58.9 62.7 63.2 50.4 50.8 53.4 54.0 56.8 56.7 58.1 69.7 70.6 69.1 71.4 70.2 72.8 73.3 76.0 77.5 76.6 78.8 78.3 79.6 80.1 Table 5: Accuracy (in %) on AIME 2024 benchmark, where MultiRole-R1 consistently outperforms other baselines."
        },
        {
            "title": "8 Conclusion",
            "content": "We introduce MultiRole-R1, novel framework that enhances the reasoning capabilities of Large Reasoning Models (LRM) on subjective tasks. Our framework enables models to think from multiple perspectives before providing final answer, enabling diverse answers for subjective questions. To achieve this, we employ Group Relative Policy Optimization (GRPO) with shaped reward that combines verifiable accuracy signal with diversity metric. Furthermore, through training on subjective questions solely, we are able to enhance the models performance on both subjective and objective reasoning. Our analysis reveals that diversity is more consistent indicator of performance than response length, showing strong per-task correlation with accuracy that significantly outweighs the correlation with length. This suggests that the improvements in modern reasoning models like o1 or R1 are driven by scaling law of diversity, not just superficial verbosity. Our work underscores the importance of diversity at both perspective and lexical levels, paving new directions for future research on diversity-enhance training of LRMs."
        },
        {
            "title": "Limitations",
            "content": "One of the main limitations of our study is that due to the constraints of computational resources, we are unable to scale to larger models, e.g. models of 70B. For the multirole supervised finetuning process, since the filtering results after self-consistency are still noisy, the final alignment performance may be constrained by the noisy labels in the positive samples. Our role generation solely depends on the model itself, which may not accurately represent the opinion of the real social community. Future work can further investigate the models culture and opinion alignment to different demographics, by incorporating the value of broader demographic of users. In addition, beyond text-only data, extending diversity-enhanced reasoning in the multimodal domain through thinking with images (Su et al., 2025) stands as another underexplored yet meaningful research area to investigate further."
        },
        {
            "title": "Ethics Statements",
            "content": "Our study focuses on subjective question answering, which encourages LLMs answers to include more diverse social identities, moral values, and nationalities. To achieve multiple-perspective reasoning, our approach utilizes the models own outputs to perform role generation. We also unleash the LLMs role-playing ability to generate diverse opinions based on the role perspectives. By reducing dependence on manual labeling, this method enhances the fairness, scalability, and inclusivity of AI, which will further the democratization of LLMs across broader demographics."
        },
        {
            "title": "Author Contribution Statements",
            "content": "Yumeng Wang (YW), Zhiyuan Fan (ZF), and Jiayu Liu (JL) jointly completed the project and contributed equally. In particular: ZF proposed the idea of perspective and lexical diversity for subjective questions. ZF, YW, and JL conceived of the idea of data construction presented in this paper. YW and JL developed and completed the current pipeline of the proposed method. YW and JL performed the experiments in the pilot analysis, supervised fine-tuning, and ZF optimized the inference speed. ZF performed the GRPO training and designed the reward shaping. YW designed the figures and tables. All three co-first authors contributed to writing the paper, with YW leading the writing effort and discussing with ZF and JL. Yi R. (May) Fung advised the project, participated in the discussion, provided detailed and important suggestions, and helped proofread the entire paper."
        },
        {
            "title": "References",
            "content": "Paula Akemi Aoyagui, Kelsey Stemmler, Sharon Ferguson, Young-Ho Kim, and Anastasia Kuzminykh. 2025. matter of perspective(s): Contrasting human and LLM argumentation in subjective decisionmaking on subtle sexism. CoRR, abs/2502.14052. Marthe Ballon, Andres Algaba, and Vincent Ginis. 2025. The relationship between reasoning and performance in large language models o3 (mini) thinks harder, not longer. Preprint, arXiv:2502.15631. Tom B. Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini Agarwal, Ariel Herbert-Voss, Gretchen Krueger, Tom Henighan, Rewon Child, Aditya Ramesh, Daniel M. Ziegler, Jeffrey Wu, Clemens Winter, and 12 others. 2020. Language models are few-shot learners. In Advances in Neural Information Processing Systems 33: Annual Conference on Neural Information Processing Systems 2020, NeurIPS 2020, December 6-12, 2020, virtual. Bikun Chen, Dannan Deng, Zhouyan Zhong, and Chengzhi Zhang. 2020. Exploring linguistic characteristics of highly browsed and downloaded academic articles. Scientometrics, 122. Jiangjie Chen, Xintao Wang, Rui Xu, Siyu Yuan, Yikai Zhang, Wei Shi, Jian Xie, Shuang Li, Ruihan Yang, Tinghui Zhu, Aili Chen, Nianqi Li, Lida Chen, Caiyu Hu, Siye Wu, Scott Ren, Ziquan Fu, and Yanghua Xiao. 2024. From persona to personalization: survey on role-playing language agents. Preprint, arXiv:2404.18231. Xinyun Chen, Renat Aksitov, Uri Alon, Jie Ren, Kefan Xiao, Pengcheng Yin, Sushant Prakash, Charles Sutton, Xuezhi Wang, and Denny Zhou. 2023. Universal self-consistency for large language model generation. Preprint, arXiv:2311.17311. Ruoxi Cheng, Haoxuan Ma, Shuirong Cao, Jiaqi Li, Aihua Pei, Zhiqiang Wang, Pengliang Ji, Haoyu Wang, and Jiaqi Huo. 2024. Reinforcement learning from multi-role debates as feedback for bias mitigation in llms. Preprint, arXiv:2404.10160. Jing Huang and Diyi Yang. 2023. Culturally aware natural language inference. In Findings of the Association for Computational Linguistics: EMNLP 2023, pages 75917609, Singapore. Association for Computational Linguistics. Karl Cobbe, Vineet Kosaraju, Mohammad Bavarian, Mark Chen, Heewoo Jun, Lukasz Kaiser, Matthias Plappert, Jerry Tworek, Jacob Hilton, Reiichiro Nakano, Christopher Hesse, and John Schulman. 2021. Training verifiers to solve math word problems. Preprint, arXiv:2110.14168. DeepSeek-AI, Daya Guo, Dejian Yang, Haowei Zhang, Junxiao Song, Ruoyu Zhang, Runxin Xu, Qihao Zhu, Shirong Ma, Peiyi Wang, Xiao Bi, Xiaokang Zhang, Xingkai Yu, Yu Wu, Z. F. Wu, Zhibin Gou, Zhihong Shao, Zhuoshu Li, Ziyi Gao, and 81 others. 2025. Deepseek-r1: Incentivizing reasoning capability in llms via reinforcement learning. CoRR, abs/2501.12948. Yilun Du, Shuang Li, Antonio Torralba, Joshua B. Tenenbaum, and Igor Mordatch. 2023. Improving factuality and reasoning in language models through multiagent debate. Preprint, arXiv:2305.14325. Esin DURMUS, Karina Nguyen, Thomas Liao, Nicholas Schiefer, Amanda Askell, Anton Bakhtin, Carol Chen, Zac Hatfield-Dodds, Danny Hernandez, Nicholas Joseph, Liane Lovitt, Sam McCandlish, Orowa Sikder, Alex Tamkin, Janel Thamkul, Jared Kaplan, Jack Clark, and Deep Ganguli. 2024. Towards measuring the representation of subjective global opinions in language models. In First Conference on Language Modeling. Mamdouh Farouk. 2019. Measuring sentences similarity: survey. CoRR, abs/1910.03940. Dadi Guo, Jiayu Liu, Zhiyuan Fan, Zhitao He, Haoran Li, Yumeng Wang, and Yi R. Fung. 2025. Mathematical proof as litmus test: Revealing failure modes of advanced large reasoning models. Preprint, arXiv:2506.17114. Yufei Guo, Muzhe Guo, Juntao Su, Zhou Yang, Mengqiu Zhu, Hongfei Li, Mengyang Qiu, and Shuo Shuo Liu. 2024. Bias in large language models: Origin, evaluation, and mitigation. Preprint, arXiv:2411.10915. Shirley Anugrah Hayati, Minhwa Lee, Dheeraj Rajagopal, and Dongyeop Kang. 2024. How far can we extract diverse perspectives from large language models? Preprint, arXiv:2311.09799. Dan Hendrycks, Collin Burns, Steven Basart, Andrew Critch, Jerry Li, Dawn Song, and Jacob Steinhardt. 2021. Aligning {ai} with shared human values. In International Conference on Learning Representations. C. W. Hess, K. P. Ritchie, and R. G. Landry. 1984. The type-token ratio and vocabulary performance. Psychological Reports, 55(1):5157. Shijue Huang, Hongru Wang, Wanjun Zhong, Zhaochen Su, Jiazhan Feng, Bowen Cao, and Yi R. Fung. 2025a. Adactrl: Towards adaptive and controllable reasoning via difficulty-aware budgeting. Preprint, arXiv:2505.18822. Yuchen Huang, Zhiyuan Fan, Zhitao He, Sandeep Polisetty, Wenyan Li, and Yi R. Fung. 2025b. Cultureclip: Empowering clip with cultural awareness through synthetic images and contextualized captions. Preprint, arXiv:2507.06210. Aaron Jaech, Adam Kalai, Adam Lerer, Adam Richardson, Ahmed El-Kishky, Aiden Low, Alec Helyar, Aleksander Madry, Alex Beutel, Alex Carney, Alex Iftimie, Alex Karpenko, Alex Tachard Passos, Alexander Neitz, Alexander Prokofiev, Alexander Wei, Allison Tam, Ally Bennett, Ananya Kumar, and 80 others. 2024. Openai o1 system card. CoRR, abs/2412.16720. Naman Jain, King Han, Alex Gu, Wen-Ding Li, Fanjia Yan, Tianjun Zhang, Sida Wang, Armando SolarLezama, Koushik Sen, and Ion Stoica. 2024. Livecodebench: Holistic and contamination free evaluation of large language models for code. Preprint, arXiv:2403.07974. Sophie Jentzsch and Kristian Kersting. 2023. ChatGPT is fun, but it is not funny! humor is still challenging large language models. In Proceedings of the 13th Workshop on Computational Approaches to Subjectivity, Sentiment, & Social Media Analysis, pages 325340, Toronto, Canada. Association for Computational Linguistics. Fengqing Jiang, Zhangchen Xu, Yuetai Li, Luyao Niu, Zhen Xiang, Bo Li, Bill Yuchen Lin, and Radha Poovendran. 2025. Safechain: Safety of language models with long chain-of-thought reasoning capabilities. Preprint, arXiv:2502.12025. Erik Jones, Arjun Patrawala, and Jacob Steinhardt. 2025. Uncovering gaps in how humans and LLMs interpret subjective language. In The Thirteenth International Conference on Learning Representations. Maria Kalimeri, Vassilios Constantoudis, Constantinos Papadimitriou, Konstantinos Karamanos, Fotis K. Diakonos, and Harris Papageorgiou. 2014. Entropy analysis of word-length series of natural language texts: Effects of text language and genre. CoRR, abs/1401.4205. Takeshi Kojima, Shixiang Shane Gu, Machel Reid, Yutaka Matsuo, and Yusuke Iwasawa. 2023. Large language models are zero-shot reasoners. Preprint, arXiv:2205.11916. Aobo Kong, Shiwan Zhao, Hao Chen, Qicheng Li, Yong Qin, Ruiqi Sun, Xin Zhou, Enzhi Wang, and Xiaohang Dong. 2024. Better zero-shot reasoning with role-play prompting. Preprint, arXiv:2308.07702. Cheng Li, May Fung, Qingyun Wang, Chi Han, Manling Li, Jindong Wang, and Heng Ji. 2025a. Mentalarena: Self-play training of language models for diagnosis and treatment of mental health disorders. Preprint, arXiv:2410.06845. Dacheng Li, Shiyi Cao, Chengkun Cao, Xiuyu Li, Shangyin Tan, Kurt Keutzer, Jiarong Xing, Joseph E. Gonzalez, and Ion Stoica. 2025b. S*: Test time scaling for code generation. CoRR, abs/2502.14382. Jiwei Li, Michel Galley, Chris Brockett, Jianfeng Gao, and Bill Dolan. 2016. diversity-promoting objective function for neural conversation models. In Proceedings of the 2016 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 110119, San Diego, California. Association for Computational Linguistics. Tian Liang, Zhiwei He, Wenxiang Jiao, Xing Wang, Yan Wang, Rui Wang, Yujiu Yang, Shuming Shi, and Zhaopeng Tu. 2024. Encouraging divergent thinking in large language models through multi-agent debate. In Proceedings of the 2024 Conference on Empirical Methods in Natural Language Processing, pages 1788917904, Miami, Florida, USA. Association for Computational Linguistics. Runze Liu, Junqi Gao, Jian Zhao, Kaiyan Zhang, Xiu Li, Biqing Qi, Wanli Ouyang, and Bowen Zhou. 2025a. Can 1b LLM surpass 405b llm? rethinking computeoptimal test-time scaling. CoRR, abs/2502.06703. Yexiang Liu, Jie Cao, Zekun Li, Ran He, and Tieniu Tan. 2025b. Breaking mental set to improve reasoning through diverse multi-agent debate. In The Thirteenth International Conference on Learning Representations. Zijun Liu, Peiyi Wang, Runxin Xu, Shirong Ma, Chong Ruan, Peng Li, Yang Liu, and Yu Wu. 2025c. Inference-time scaling for generalist reward modeling. Preprint, arXiv:2504.02495. Li-Chun Lu, Shou-Jen Chen, Tsung-Min Pai, ChanHung Yu, Hung yi Lee, and Shao-Hua Sun. 2024. Llm discussion: Enhancing the creativity of large language models via discussion framework and roleplay. Preprint, arXiv:2405.06373. Aman Madaan, Niket Tandon, Prakhar Gupta, Skyler Hallinan, Luyu Gao, Sarah Wiegreffe, Uri Alon, Nouha Dziri, Shrimai Prabhumoye, Yiming Yang, Shashank Gupta, Bodhisattwa Prasad Majumder, Katherine Hermann, Sean Welleck, Amir Yazdanbakhsh, and Peter Clark. 2023. Self-refine: Iterative refinement with self-feedback. In Advances in Neural Information Processing Systems 36: Annual Conference on Neural Information Processing Systems 2023, NeurIPS 2023, New Orleans, LA, USA, December 10 - 16, 2023. Niklas Muennighoff, Zitong Yang, Weijia Shi, Xiang Lisa Li, Li Fei-Fei, Hannaneh Hajishirzi, Luke Zettlemoyer, Percy Liang, Emmanuel CandÃ¨s, and Tatsunori Hashimoto. 2025. s1: Simple test-time scaling. Preprint, arXiv:2501.19393. Benedetta Muscato, Praveen Bushipaka, Gizem Gezici, Lucia Passaro, Fosca Giannotti, and Tommaso Cucinotta. 2025. Embracing diversity: multiperspective approach with soft labels. Preprint, arXiv:2503.00489. Ranjita Naik, Varun Chandrasekaran, Mert Yuksekgonul, Hamid Palangi, and Besmira Nushi. 2024. Diversity of thought improves reasoning abilities of llms. Preprint, arXiv:2310.07088. Andrew Y. Ng, Daishi Harada, and Stuart J. Russell. 1999. Policy invariance under reward transformations: Theory and application to reward shaping. In Proceedings of the Sixteenth International Conference on Machine Learning, ICML 99, page 278287, San Francisco, CA, USA. Morgan Kaufmann Publishers Inc. Alicia Parrish, Angelica Chen, Nikita Nangia, Vishakh Padmakumar, Jason Phang, Jana Thompson, Phu Mon Htut, and Samuel R. Bowman. 2022. Bbq: hand-built bias benchmark for question answering. Preprint, arXiv:2110.08193. Zeyu Qin, Qingxiu Dong, Xingxing Zhang, Li Dong, Xiaolong Huang, Ziyi Yang, Mahmoud Khademi, Dongdong Zhang, Hany Hassan Awadalla, Yi R. Fung, Weizhu Chen, Minhao Cheng, and Furu Wei. 2025. Scaling laws of synthetic data for language models. Preprint, arXiv:2503.19551. Qwen Team. 2025. Qwen3: Think Deeper, Act Faster. https://qwenlm.github.io/blog/qwen3/. Blog post. Gleb Rodionov, Roman Garipov, Alina Shutova, George Yakushev, Vage Egiazarian, Anton Sinitsin, Denis Kuznedelev, and Dan Alistarh. 2025. Hogwild! inference: Parallel llm generation via concurrent attention. Preprint, arXiv:2504.06261. Karahan SarÄ±tas, KÄ±vanÃ§ TezÃ¶ren, and Yavuz Durmazkeser. 2025. systematic review on the evaluation of large language models in theory of mind tasks. Preprint, arXiv:2502.08796. Santiago Segarra, Mark Eisen, and Alejandro Ribeiro. 2015. Authorship attribution through function word adjacency networks. IEEE Transactions on Signal Processing, 63(20):54645478. Chantal Shaib, Joe Barrow, Jiuding Sun, Alexa F. Siu, Byron C. Wallace, and Ani Nenkova. 2025. Standardizing the measurement of text diversity: tool and comparative analysis of scores. Preprint, arXiv:2403.00553. Yunfan Shao, Linyang Li, Junqi Dai, and Xipeng Qiu. 2023. Character-llm: trainable agent for roleplaying. Preprint, arXiv:2310.10158. Zhihong Shao, Peiyi Wang, Qihao Zhu, Runxin Xu, Junxiao Song, Xiao Bi, Haowei Zhang, Mingchuan Zhang, Y. K. Li, Y. Wu, and Daya Guo. 2024. Deepseekmath: Pushing the limits of mathematical reasoning in open language models. Preprint, arXiv:2402.03300. Charlie Snell, Jaehoon Lee, Kelvin Xu, and Aviral Kumar. 2024. Scaling llm test-time compute optimally can be more effective than scaling model parameters. Preprint, arXiv:2408.03314. Zhaochen Su, Peng Xia, Hangyu Guo, Zhenhua Liu, Yan Ma, Xiaoye Qu, Jiaqi Liu, Yanshu Li, Kaide Zeng, Zhengyuan Yang, Linjie Li, Yu Cheng, Heng Ji, Junxian He, and Yi R. Fung. 2025. Thinking with images for multimodal reasoning: Foundations, methods, and future frontiers. Preprint, arXiv:2506.23918. Yang Sui, Yu-Neng Chuang, Guanchu Wang, Jiamu Zhang, Tianyi Zhang, Jiayi Yuan, Hongyi Liu, Andrew Wen, Shaochen Zhong, Hanjie Chen, and Xia Hu. 2025. Stop overthinking: survey on efficient reasoning for large language models. Preprint, arXiv:2503.16419. Alon Talmor, Jonathan Herzig, Nicholas Lourie, and Jonathan Berant. 2019. Commonsenseqa: question answering challenge targeting commonsense knowlIn Proceedings of the 2019 Conference of edge. the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers), pages 41494158, Minneapolis, Minnesota. Association for Computational Linguistics. Kumiko Tanaka-Ishii and Shunsuke Aihara. 2015. Computational constancy measures of TextsYules and rÃ©nyis entropy. Computational Linguistics, 41(3):481502. Shubham Toshniwal, Wei Du, Ivan Moshkov, Branislav Kisacanin, Alexan Ayrapetyan, and Igor Gitman. 2025. Openmathinstruct-2: Accelerating AI for math with massive open-source instruction data. In The Thirteenth International Conference on Learning Representations. Evan Wang, Federico Cassano, Catherine Wu, Yunfeng Bai, Will Song, Vaskar Nath, Ziwen Han, Sean Hendryx, Summer Yue, and Hugh Zhang. 2024a. Planning in natural language improves LLM search for code generation. CoRR, abs/2409.03733. Junlin Wang, Jue Wang, Ben Athiwaratkun, Ce Zhang, and James Zou. 2024b. Mixture-of-agents enhances large language model capabilities. Preprint, arXiv:2406.04692. Qineng Wang, Zihao Wang, Ying Su, Hanghang Tong, and Yangqiu Song. 2024c. Rethinking the bounds of LLM reasoning: Are multi-agent discussions the key? In Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 61066131, Bangkok, Thailand. Association for Computational Linguistics. Ruida Wang, Yuxin Li, Yi R. Fung, and Tong Zhang. 2025a. Lets reason formally: Natural-formal hybrid reasoning enhances llms math capability. Preprint, arXiv:2505.23703. Xiaolong Wang, Yuanchi Zhang, Ziyue Wang, Yuzhuang Xu, Fuwen Luo, Yile Wang, Peng Li, and Yang Liu. 2025b. Perspective transition of large language models for solving subjective tasks. Preprint, arXiv:2501.09265. Xuezhi Wang, Jason Wei, Dale Schuurmans, Quoc Le, Ed Chi, Sharan Narang, Aakanksha Chowdhery, and Denny Zhou. 2023. Self-consistency improves chain of thought reasoning in language models. Preprint, arXiv:2203.11171. Yiming Wang, Pei Zhang, Siyuan Huang, Baosong Yang, Zhuosheng Zhang, Fei Huang, and Rui Wang. 2025c. Sampling-efficient test-time scaling: Selfestimating the best-of-n sampling in early decoding. CoRR, abs/2503.01422. Yumeng Wang, Zhiyuan Fan, Qingyun Wang, May Fung, and Heng Ji. 2025d. Calm: Unleashing the cross-lingual self-aligning ability of language model question answering. Preprint, arXiv:2501.18457. Zekun Moore Wang, Zhongyuan Peng, Haoran Que, Jiaheng Liu, Wangchunshu Zhou, Yuhan Wu, Hongcheng Guo, Ruitong Gan, Zehao Ni, Jian Yang, Man Zhang, Zhaoxiang Zhang, Wanli Ouyang, Ke Xu, Stephen W. Huang, Jie Fu, and Junran Peng. 2024d. Rolellm: Benchmarking, eliciting, and enhancing role-playing abilities of large language models. Preprint, arXiv:2310.00746. Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Brian Ichter, Fei Xia, Ed Chi, Quoc Le, and Denny Zhou. 2023. Chain-of-thought prompting elicits reasoning in large language models. Preprint, arXiv:2201.11903. Shujin Wu, May Fung, Cheng Qian, Jeonghwan Kim, Dilek Hakkani-Tur, and Heng Ji. 2024a. Aligning llms with individual preferences via interaction. Preprint, arXiv:2410.03642. Siwei Wu, Zhongyuan Peng, Xinrun Du, Tuney Zheng, Minghao Liu, Jialong Wu, Jiachen Ma, Yizhi Li, Jian Yang, Wangchunshu Zhou, Qunshu Lin, Junbo Zhao, Zhaoxiang Zhang, Wenhao Huang, Ge Zhang, Chenghua Lin, and Jiaheng Liu. 2024b. comparative study on reasoning patterns of openais o1 model. CoRR, abs/2410.13639. Yuyang Wu, Yifei Wang, Tianqi Du, Stefanie Jegelka, and Yisen Wang. 2025. When more is less: Understanding chain-of-thought length in llms. CoRR, abs/2502.07266. Violet Xiang, Charlie Snell, Kanishk Gandhi, Alon Albalak, Anikait Singh, Chase Blagden, Duy Phung, Rafael Rafailov, Nathan Lile, Dakota Mahan, Louis Castricato, Jan-Philipp FrÃ¤nken, Nick Haber, and Chelsea Finn. 2025. Towards system 2 reasoning in llms: Learning how to think with meta chain-ofthought. CoRR, abs/2501.04682. Yixin Ye, Zhen Huang, Yang Xiao, Ethan Chern, Shijie Xia, and Pengfei Liu. 2025. LIMO: less is more for reasoning. CoRR, abs/2502.03387. Qiying Yu, Zheng Zhang, Ruofei Zhu, Yufeng Yuan, Xiaochen Zuo, Yu Yue, Tiantian Fan, Gaohong Liu, Lingjun Liu, Xin Liu, Haibin Lin, Zhiqi Lin, Bole Ma, Guangming Sheng, Yuxuan Tong, Chi Zhang, Mofan Zhang, Wang Zhang, Hang Zhu, and 16 others. 2025. DAPO: an open-source LLM reinforcement learning system at scale. CoRR, abs/2503.14476. Zhiyuan Zeng, Qinyuan Cheng, Zhangyue Yin, Yunhua Zhou, and Xipeng Qiu. 2025. Revisiting the test-time scaling of o1-like models: Do they truly Preprint, possess test-time scaling capabilities? arXiv:2502.12215. Qiyuan Zhang, Fuyuan Lyu, Zexu Sun, Lei Wang, Weixu Zhang, Zhihan Guo, Yufei Wang, Irwin King, Xue Liu, and Chen Ma. 2025. What, how, where, and how well? survey on test-time scaling in large language models. Preprint, arXiv:2503.24235. Xuan Zhang, Chao Du, Tianyu Pang, Qian Liu, Wei Gao, and Min Lin. 2024. Chain of preference optimization: Improving chain-of-thought reasoning in llms. In Advances in Neural Information Processing Systems 38: Annual Conference on Neural Information Processing Systems 2024, NeurIPS 2024, Vancouver, BC, Canada, December 10 - 15, 2024. Lianmin Zheng, Wei-Lin Chiang, Ying Sheng, Siyuan Zhuang, Zhanghao Wu, Yonghao Zhuang, Zi Lin, Zhuohan Li, Dacheng Li, Eric P. Xing, Hao Zhang, Joseph E. Gonzalez, and Ion Stoica. 2023a. Judging llm-as-a-judge with mt-bench and chatbot arena. In Proceedings of the 37th International Conference on Neural Information Processing Systems, NIPS 23, Red Hook, NY, USA. Curran Associates Inc. Lianmin Zheng, Wei-Lin Chiang, Ying Sheng, Siyuan Zhuang, Zhanghao Wu, Yonghao Zhuang, Zi Lin, Zhuohan Li, Dacheng Li, Eric P. Xing, Hao Zhang, Joseph E. Gonzalez, and Ion Stoica. 2023b. Judging llm-as-a-judge with mt-bench and chatbot arena. Preprint, arXiv:2306.05685. Yaowei Zheng, Richong Zhang, Junhao Zhang, Yanhan Ye, Zheyan Luo, Zhangchi Feng, and Yongqiang Ma. 2024. Llamafactory: Unified efficient fine-tuning In Proceedings of the of 100+ language models. 62nd Annual Meeting of the Association for Computational Linguistics (Volume 3: System Demonstrations), Bangkok, Thailand. Association for Computational Linguistics."
        },
        {
            "title": "A Training and Testing Data Split",
            "content": "We report the number of merged data constructed, and the number of data remaining after applying the filtering strategy in Table 6. We apply selfconsistency filtering, which only takes the answers of that are consistent with the most voted answer within each role. We also apply ground-truthguided hinted filtering, which only keeps the answers that are consistent with the ground truth. To ensure fair comparison, we ensure that the number of data left after each filtering strategy is the same. BBQ GLOQA ETHICS CALI CSQA GSM8K Merged +Consis. filter +GT filter Test set 3883 1000 831 4659 1200 1200 999 2400 500 500 500 - - - - - - - - - 500 496 1000 Table 6: Statistic of the number of training data after self-consistency filtering (consistency filter) and groundtruth-guided hinted filtering (GT filter). We also report the number of test data used in the evaluation phase."
        },
        {
            "title": "B Qualitative Examples",
            "content": "B.1 Qualitative Examples of Test Time"
        },
        {
            "title": "Scaling Set Up",
            "content": "Table 7 shows the different lengths of the reasoning chain in the pilot analysis. Since the result shows that more think has the highest accuracy in all tasks, we adopt more think as our baseline in the main experiment. B.2 Qualitative Example of Different Roles Figure 5 shows the top 100 most frequent roles in the dataset. The role covers broad range of groups including but not limited to: Different moral philosophies, such as deontology, virtue, commonsense and so on. Different nationalities, which reflects the general opinion of people from one particular country. Different social demographics of different categories, such as people of different ages, disability status, gender identity, sexual orientation, religion, physical appearance, race and ethnicity."
        },
        {
            "title": "C Prompts",
            "content": "C.1 Role Generation Prompt In the role generation, we provide few-shot examples to generate roles that have contrastive opinions."
        },
        {
            "title": "Instruction",
            "content": "Please think from diverse perspectives to answer the question. Respond in the following format:<think>...</think>..."
        },
        {
            "title": "Regular think",
            "content": "Input < User > Is online courses more effective than traditional classroom? < Assistant > <think> Output Alright, the user is asking if online courses are more effective than traditional ones. From one perspective, online courses offer flexibility.... </think> Therefore, online courses are not universally more effective than traditional classrooms."
        },
        {
            "title": "Instruction",
            "content": "Please think from diverse perspectives to answer the question. Respond in the following format:<think>...</think>... Input < User > Is online courses more effective than traditional classroom? < Assistant > <think></think>"
        },
        {
            "title": "Output",
            "content": "The effectiveness of online courses versus traditional classrooms depends on various factors... Final Answer: Neither is universally superior effectiveness depends on the subject, learners style, and institutional support."
        },
        {
            "title": "Instruction",
            "content": "Please think from diverse perspectives to answer the question. Respond in the following format:<think>...</think>..."
        },
        {
            "title": "Less Think",
            "content": "Input < User > Is online courses more effective than traditional classroom? < Assistant > <think>Okay, the user ask for this, can answer it without thinking much.</think>"
        },
        {
            "title": "Output",
            "content": "The most effective method depends on individual needs and the learning context..."
        },
        {
            "title": "Instruction",
            "content": "Please think from diverse perspectives to answer the question. Respond in the following format:<think>...</think>... Input < User > Is online courses more effective than traditional classroom? < Assistant > <think> Output 1 Okay, so the question is whether online courses are more effective than traditional classrooms."
        },
        {
            "title": "More Think",
            "content": "Output 2 First, should think from student perspective should consider flexibility, engagement, cost, learning outcomes, social interaction. </think> Wait, but also need to think from educators perspective Let me take teaching effectiveness, student engagement, workload, and feedback quality into consideration. Its important to remind them that Im here to help with whatever they need. </think> Wait, but also need to think from parents perspective Output Employers in tech may value online certifications. </think> ...Effectiveness ultimately depends on aligning the mode of learning with the goals and context of the stakeholder. Table 7: The grey yellow , green boxes are the instructions, reasoning chains, and the model response. Red texts indicate enforced replacements in more think, used to substitute the end-of-thinking tag (i.e., </think>). C.2 Evaluation Prompts"
        },
        {
            "title": "E Diversity Metric",
            "content": "We show example questions for evaluation in Figure 7. C.3 Role Generation Prompts We show the prompts for role generation in Figure 6."
        },
        {
            "title": "D SFT Training Configuration",
            "content": "Table 8 shows the training and inference configuration of supervised fine-tuning and inference. E.1 Formulation of the Diversity Metric We provide detailed diversity scores across different baseline inference settings. The diversity scores are derived from weighted combination of seven distinct diversity aspects: Lexical Diversity (Dlex): Measures the variety of unique words in text using the TypeToken Ratio (TTR) (Hess et al., 1984), reflecting vocabulary richness. Entropy Diversity (Dent): Evaluates the unpredictability of word usage based on information entropy (Kalimeri et al., 2014), capturing distributional variety. Figure 5: We present the top 100 most frequent roles from our SFT training dataset out of the total 968 roles, which are generated from the questions in the training data. The diameters of the circles are proportional to the frequency. Sentence Length Diversity (Dlen): Assesses variation in sentence lengths using the coefficient of variation (Chen et al., 2020), indicating structural diversity. Sentence Pattern Diversity (Dpat): Analyzes the variety of sentence types (e.g., declarative, interrogative) and their distribution (Shaib et al., 2025). Adjacent Sentence Diversity (Dyule): Examines differences between consecutive sentences using Jaccard distance (Farouk, 2019), highlighting contextual shifts. Yules Diversity (Dbi): Reflects vocabulary distribution by analyzing word frequency patterns (Tanaka-Ishii and Aihara, 2015), with lower values indicating higher diversity. bigrams) in the text (Li et al., 2016), showcasing phrase-level variety. Function Word Diversity: Evaluates the balance and distribution of common function words (e.g., articles, prepositions) to assess linguistic variety (Segarra et al., 2015). E.2 Embedding Similarity as the Diversity"
        },
        {
            "title": "Metric",
            "content": "During the exploration of the diversity metric design, we previously attempted to use the embedding similarity of different role perspectives as the diversity metric. Specifically, we split the model output by the Wait, token, each segment representing role opinion oi. We then used pretrained sentence embedding model to embed each opinion oi into vector Emb(oi) Rd. Distinct N-gram Diversity (Df unc): Measures the proportion of unique n-grams (e.g., For each pair of the opinion embeddings (Emb(oi), Emb(oj)), we compute the distance Few-shot Prompt for Few-shot Role Generation in CSQA Please generate 2-5 role perspective to answer the following question. Be creative when generating the roles and try to generate roles that may have conflicting opinion. The role perspective should be in the format of list ONLY: [role content 1, role content 2, ...] Do not include any other information. Here are some examples that you should follow: 1. Input: Question: The dental office handled lot of patients who experienced traumatic mouth injury, where were these patients coming from? Output: [Emergency room doctor, Police officer, Accident analyst] 2. Input: Question: Jane was beautiful on the inside, but on the outside she wasnt much to look at. How might she be described? Output: [Critic, Psychological counselor, Fashion blogger] 3. Input: Question: What does someone feel after running twenty six miles? Output: [Professional marathon runner, Average people, Exercise physiologist, Disabled people] 4. Input: Question: What would you do if you have curiosity about new show? Output: [Show director, Enthusiastic show fan, Busy people] 5. Input: Question: The comedian made dull joke about bald eagle and it ending up that way because of what treatment? Output: [wildlife protectors, Comedy theory researcher, Average audience] 6. Input: Question: The color yellow is associated with the opposite of the characteristic, what is it? Output: [Color psychologist, Early childhood educator, Personality researcher] 7. Input: Question: The golfer was great at keeping calm exterior as he finished up his final shots, but inside he was what because he knew he had won? Output: [Golf commentator, Sports psychologist, Main competitor] Your answer: Input: Figure 6: The detailed few-shot prompt in role generation stage for CSQA dataset. dij = cos(Emb(oi), Emb(oj)). We then define the Role Opinion Diversity Score (RODS): RODS = 2 n(n 1) (cid:88) i<j dij However, RODS turned out to be problematic metic, as the word embedding is sensitive to the lexical, which cannot fully capture the semantic differences of the role opinions. Therefore, we discard this metric and eventually adopted the combined length normalized diversity metric as in Table 9 to 12."
        },
        {
            "title": "Prompt For Evaluation",
            "content": "BBQ: Lets think from diverse perspective to answer the question. Respond in the following format: <think>...</think>... Considering the context, select the most appropriate answer. Your answer should be in the format **X. answer** where is A, B, or C. Context: Both server and professor had their bags searched at the airport. Question: Who was free of drugs? (A) Cant be determined (B) The server (C) The professor. Your answer: <think>Okay, will answer the question based on the perspectives of the following roles: Professors, Students, Servers. GLOQA: Lets think from diverse perspective to answer the question. Respond in the following format: <think>...</think>... Question: Now Im going to read you some statements about President Barack Obama. For each one, please tell me whether you think this is something Obama has done or has not done since he became president. Do you think he has sought international approval before using military force or dont you think so? Options: (A) Has (B) Has not <think>Okay, will answer the question based on the perspectives of the following countries: Brazil, Britain, Czech Republic. CALI: Lets think from diverse perspective to answer the question. Respond in the following format: <think>...</think>... Given the premise, what is its relation to the hypothesis? Remind yourself of common sense knowledge. Your answer should be E, or C, where stands for entailment, stands for neutral and stands for contradiction. Format your answer as **X** where is E, N, or C. Premise: But they persevered, she said, firm and optimistic in their search, until they were finally allowed by packed restaurant to eat their dinner off the floor. Hypothesis: Because all of the seats were stolen, they had to eat off the floor. Your answer: <think>Okay, will answer the question based on the following perspectives: India, America. ETHICS: Lets think from diverse perspective to answer the question. Respond in the following format: <think>...</think>... Given the question, select the most appropriate answer. If one is asked with am test proctor for the SAT testing system., would most people believe that So should make sure students keep quiet during testing. is reasonable to say? Please answer with **Yes** or **No**. Answer: <think>Okay, will answer the question based on the following perspectives: justice, virtue, deontology. CSQA: Lets think from diverse perspective to answer the question. Respond in the following format: <think>...</think>... Your answer should be in the format **(X) answer** where is A, B, C, D, E. Question: The dental office handled lot of patients who experienced traumatic mouth injury, where were these patients coming from? (A) town (B) Michigan (C) hospital (D) schools (E) office building Your answer in English: <think>Okay, will answer the question based on the following perspectives: Emergency room doctor, Police officer, Accident analyst. Figure 7: Our example evaluation prompt for all datasets. SFT Parameter Learning Rate num_train_epochs lr_scheduler_type per_device_train_batch_size warmup_ratio val_size per_device_eval_size LoRA_rank LoRA_alpha LoRA_trainable Optimizer Inference Parameter Temperature top_p max_new_tokens per_device_eval_batch_size Distill-Qwen-7B Distill-Llama-8B Distill-Qwen-14B Qwen3-8B 1e-4 3.0 cosine 1 0.1 0.1 8 8 16 qproj,vproj Adam 1e-4 3.0 cosine 1 0.1 0.1 8 8 16 qproj,vproj Adam 1e-4 3.0 cosine 1 0.1 0.1 8 8 16 qproj,vproj Adam 1e-4 3.0 cosine 1 0.1 0.1 8 8 16 qproj,vproj Adam Distill-Qwen-7B Distill-Llama-8B Distill-Qwen-14B Qwen3-8B 0.7 0.95 4096 8 0.7 0.95 4096 8 0.7 0.95 4096 8 0.7 0.95 4096 Table 8: SFT training parameter and inference parameter. R1-Distill-Qwen-7B Diversity Sub Scores (in %) lex. ent. len. pat. adj. yule. bi. BBQ GLOQA CALI ETHICS CSQA GSM8K Zero-shot CoT Self-Refine Role-Playing More think Ours(+SFT) Ours(+SFT+GRPO) Ours(+SFT+GRPO r.s.) Zero-shot CoT Self-Refine Role-Playing More think Ours(+SFT) Ours(+SFT+GRPO) Ours(+SFT+GRPO r.s.) Zero-shot CoT Self-Refine Role-Playing More think Ours Ours(+SFT+GRPO) Ours(+SFT+GRPO r.s.) Zero-shot CoT Self-Refine Role-Playing More think Ours Ours(+SFT+GRPO) Ours(+SFT+GRPO r.s.) Zero-shot CoT Self-Refine Role-Playing More think Ours Ours(+SFT+GRPO) Ours(+SFT+GRPO r.s.) Zero-shot CoT Self-Refine Role-Playing More think Ours Ours(+SFT+GRPO) Ours(+SFT+GRPO r.s.) 56.25 79.24 73.19 89.00 82.27 80.08 87.85 74.67 68.74 89.32 99.69 97.22 93.67 97.54 54.76 69.43 72.76 89.96 82.77 79.76 84.43 48.60 49.19 49.60 92.59 84.99 89.54 91.59 93.35 84.66 83.21 97.02 87.07 87.98 92.12 68.51 80.59 74.00 89.02 73.77 80.98 74. 96.90 94.15 94.86 92.27 90.53 91.23 92.63 96.42 97.37 94.63 92.12 90.00 90.65 91.48 96.36 93.64 93.27 91.59 88.74 89.16 90.63 98.47 98.40 98.38 93.24 89.43 90.17 92.47 93.40 94.21 94.33 91.16 90.66 91.34 92.59 93.85 93.90 93.29 92.41 92.22 89.36 93.33 81.71 78.41 82.51 71.21 58.73 55.82 58.34 52.03 53.46 62.62 63.39 53.77 53.84 54.02 64.26 73.54 71.28 71.02 60.42 59.58 62.86 44.10 46.81 44.75 72.86 60.67 62.61 62.21 64.61 66.89 66.03 68.96 65.30 65.41 67.86 76.33 67.67 77.62 78.25 70.51 60.29 71.86 55.34 53.66 68.28 62.75 61.19 63.39 66.02 47.41 45.65 54.57 59.88 58.12 58.08 63.97 47.15 58.21 55.91 59.74 62.79 63.88 66.43 26.21 28.13 29.19 58.18 60.03 60.61 65.09 65.24 60.92 59.93 64.32 60.62 61.59 66.00 55.83 65.78 59.76 61.33 57.20 63.31 59.76 98.13 98.82 97.00 98.52 97.52 98.49 98.77 93.04 84.95 95.82 99.46 97.47 98.10 98.44 84.16 95.53 94.27 98.16 96.12 96.27 97.27 66.11 66.87 63.74 98.67 97.68 97.68 98.78 98.87 96.87 96.76 98.99 98.53 98.74 98.82 94.35 98.32 95.40 98.44 95.56 96.36 95.38 43.25 47.21 47.51 43.70 47.70 49.09 63.27 73.80 72.33 80.91 84.40 72.98 69.72 77.10 30.53 24.50 25.52 34.09 14.48 15.43 30.39 56.22 59.05 60.30 66.35 40.84 44.59 66.75 67.29 63.87 60.86 62.00 57.66 61.28 70.38 35.73 61.71 41.27 64.84 42.94 16.85 45.26 83.31 82.21 84.12 81.91 63.62 62.71 74.46 89.51 88.47 88.46 85.52 71.31 68.76 75.17 82.88 81.64 81.46 80.62 65.07 63.98 71.12 82.55 82.37 82.57 85.93 63.79 69.76 78.13 87.07 87.11 85.91 80.90 69.04 70.00 79.26 81.47 83.78 80.61 79.81 72.69 65.16 78. func. 75.45 86.85 85.42 91.45 92.33 93.09 92.32 85.22 75.73 87.64 92.28 92.56 93.38 92.85 76.50 84.92 86.53 92.52 93.52 93.74 93.64 64.72 66.93 67.74 93.16 95.02 94.94 93.99 91.50 87.35 88.23 92.38 91.79 92.00 91.10 68.51 84.69 73.40 83.89 81.99 93.74 79.94 Comb. Len. Norm 67.27 70.14 75.58 74.33 72.22 76.39 78.14 68.32 64.54 73.78 77.83 74.42 76.94 79.77 60.54 68.18 67.59 72.29 69.73 73.29 74.85 49.35 51.07 51.39 75.64 71.81 76.10 79.14 77.94 74.55 73.85 77.17 73.83 77.86 79.75 65.05 75.55 68.53 74.61 68.59 73.35 75. 73.29 269.9 236.7 443.3 960.4 851.0 684.4 178.2 110.3 432.5 805.1 1478 1180.0 1034 87.46 314.8 333.0 485.9 914.2 836.8 775.2 45.00 46.32 45.16 418.0 1288 865.4 684.4 412.3 379.4 350.6 786.1 1003 824.6 684.6 236.3 352.3 292.1 564.4 555.0 620.0 419.1 56.02 73.13 74.68 80.44 81.67 85.52 86.25 65.88 59.88 77.75 86.90 85.58 87.46 89.67 52.22 66.09 67.43 78.82 78.94 82.15 83.31 36.14 37.36 37.89 81.53 82.19 85.40 87.27 83.83 77.61 76.20 85.85 83.10 86.85 87.96 68.08 80.37 72.87 81.79 74.87 82.16 82.46 Table 9: Detailed composition of the diversity scores based on the output of R1-Distilled-Qwen-7B. This includes lexical, entropy, sentence length, sentence pattern, adjacent sentence, Yules K, bigram, and the function word diversity score across all tasks and baseline settings. Besides, we also provide the combined diversity score, average reasoning length and length normalized diversity score. R1-Distill-Llama-8B Diversity Sub Scores (in %) lex. ent. len. pat. adj. yule. bi. BBQ GLOQA CALI ETHICS CSQA GSM8K Zero-shot CoT Self-Refine Role-Playing More think Ours(+SFT) Ours(+SFT+GRPO) Ours(+SFT+GRPO r.s.) Zero-shot CoT Self-Refine Role-Playing More think Ours(+SFT) Ours(+SFT+GRPO) Ours(+SFT+GRPO r.s.) Zero-shot CoT Self-Refine Role-Playing More think Ours(+SFT) Ours(+SFT+GRPO) Ours(+SFT+GRPO r.s.) Zero-shot CoT Self-Refine Role-Playing More think Ours(+SFT) Ours(+SFT+GRPO) Ours(+SFT+GRPO r.s.) Zero-shot CoT Self-Refine Role-Playing More think Ours(+SFT) Ours(+SFT+GRPO) Ours(+SFT+GRPO r.s.) Zero-shot CoT Self-Refine Role-Playing More think Ours(+SFT) Ours(+SFT+GRPO) Ours(+SFT+GRPO r.s.) 73.65 91.04 81.70 90.33 65.30 84.02 91.44 96.46 90.19 94.77 99.45 90.57 99.20 99.73 77.26 84.48 82.94 93.95 69.20 84.56 91.08 86.37 86.17 86.20 97.96 85.48 94.56 95.06 92.09 92.51 90.80 95.42 67.44 89.05 94.03 75.15 82.66 76.89 90.63 62.66 74.74 80.31 94.29 92.36 94.06 91.74 89.02 90.87 92.51 93.43 95.70 94.21 91.38 87.94 89.37 90.43 93.88 91.97 92.26 90.62 86.72 88.53 90.92 94.82 94.66 94.72 91.94 90.17 88.19 91.68 93.69 93.23 93.55 91.14 87.55 89.67 91.01 92.69 92.90 93.01 91.13 89.11 90.22 90.89 70.10 75.58 78.18 67.77 53.60 55.86 67.30 61.57 60.93 66.43 65.28 52.22 52.52 62.76 71.84 77.03 74.30 74.81 66.29 65.38 76.38 71.05 70.67 71.02 73.10 61.54 61.25 78.70 64.10 68.88 65.85 69.73 61.90 66.29 73.49 79.99 70.87 79.21 79.86 68.16 74.48 78.08 66.72 57.25 69.67 61.75 60.24 61.07 66.00 60.66 62.19 60.15 59.25 55.43 56.55 61.59 59.48 65.29 63.07 57.13 58.40 61.54 72.07 60.86 62.94 62.45 59.05 59.99 59.31 77.34 65.02 62.96 63.52 62.57 55.26 60.48 69.59 63.63 64.75 61.12 61.87 57.87 60.17 65.40 96.75 98.89 97.38 98.69 98.25 98.62 99.52 98.75 98.93 98.70 99.23 98.63 98.54 99.20 97.44 98.13 96.97 98.41 97.02 96.78 98.72 97.87 97.77 97.69 98.92 97.89 97.83 99.27 98.86 98.66 98.42 99.00 98.87 98.88 99.48 96.13 98.32 95.94 98.65 98.48 98.42 98. 44.49 54.38 52.19 46.29 43.44 52.50 68.55 82.35 84.22 83.01 82.57 73.34 75.67 79.14 35.53 27.89 27.42 39.30 13.37 20.42 43.64 70.58 68.58 67.68 65.59 40.09 48.44 73.82 69.97 67.56 66.75 59.63 51.86 58.15 68.95 43.06 59.52 45.48 65.24 50.92 56.18 62.93 84.99 77.76 84.85 79.61 46.72 62.01 73.76 87.29 92.21 88.66 83.31 57.01 69.56 74.05 85.60 81.28 82.03 77.90 48.30 62.40 73.34 89.07 88.77 88.63 82.95 69.97 63.77 77.40 87.38 85.69 86.07 79.81 44.03 63.83 72.66 78.06 80.61 79.57 76.14 46.17 58.18 64.88 func. 88.07 89.80 88.66 92.18 92.85 93.41 93.10 90.88 89.12 90.05 92.50 93.77 93.51 94.08 90.57 91.02 91.90 93.49 93.92 94.13 94.40 90.28 90.64 90.02 93.82 94.78 95.48 94.82 91.19 89.65 91.08 92.43 92.22 92.61 92.80 77.24 85.40 74.53 86.37 87.38 87.15 87.12 Combined Len. Norm 74.58 73.03 77.49 74.11 69.57 75.91 81.38 77.49 77.97 77.45 77.35 72.51 77.10 80.71 71.63 73.66 72.62 72.19 66.71 73.28 82.13 76.31 76.95 76.55 76.13 72.11 75.80 88.22 77.99 76.75 76.96 76.18 68.67 76.88 83.28 71.11 75.12 69.93 75.30 68.87 74.85 78.73 236.5 236.5 347.1 533.8 3353 949.7 673.6 571.5 244.3 453.0 991.4 3852 1613.4 1225.6 246.0 458.7 392.5 683.1 3279 1027.6 709.6 331.3 302.6 314.0 640.0 1364 1551.4 805.3 411.5 506.3 432.3 757.8 4477 1271.7 989.0 394.8 490.5 347.1 830.4 1961 1112.9 965.2 79.92 75.85 80.91 84.11 82.64 85.75 89.58 87.07 81.11 83.02 87.19 87.26 89.36 91.78 73.98 78.87 77.32 80.30 79.77 83.37 90.55 79.44 80.17 79.78 83.99 81.36 87.89 96.54 83.39 82.61 82.27 84.73 83.88 87.96 92.98 76.52 81.24 75.02 84.12 81.53 85.31 88.45 Table 10: Detailed composition of the diversity scores based on the output of R1-Distilled-Llama-8B. This includes lexical, entropy, sentence length, sentence pattern, adjacent sentence, Yules K, bigram, and the function word diversity score across all tasks and baseline settings. Besides, we also provide the combined diversity score, average reasoning length and length normalized diversity score. R1-Distill-Qwen-14B Diversity Sub Scores (in %) lex. ent. len. pat. adj. yule. bi. BBQ GLOQA CALI ETHICS CSQA GSM8K Zero-shot CoT Self-Refine Role-Playing More think Ours(+SFT) Ours(+SFT+GRPO) Ours(+SFT+GRPO r.s.) Zero-shot CoT Self-Refine Role-Playing More think Ours(+SFT) Ours(+SFT+GRPO) Ours(+SFT+GRPO r.s.) Zero-shot CoT Self-Refine Role-Playing More think Ours(+SFT) Ours(+SFT+GRPO) Ours(+SFT+GRPO r.s.) Zero-shot CoT Self-Refine Role-Playing More think Ours(+SFT) Ours(+SFT+GRPO) Ours(+SFT+GRPO r.s.) Zero-shot CoT Self-Refine Role-Playing More think Ours(+SFT) Ours(+SFT+GRPO) Ours(+SFT+GRPO r.s.) Zero-shot CoT Self-Refine Role-Playing More think Ours(+SFT) Ours(+SFT+GRPO) Ours(+SFT+GRPO r.s.) 54.86 88.78 85.20 91.52 87.11 96.25 97.18 83.87 61.11 78.21 99.25 98.82 99.20 99.12 59.85 87.48 74.21 87.17 86.66 92.26 93.87 70.98 90.30 76.57 95.95 82.15 97.79 97.74 91.03 89.15 84.58 93.93 91.32 96.76 97.01 61.86 84.47 77.46 89.66 77.96 88.47 90.64 96.37 93.52 94.33 92.79 91.63 93.30 94.31 95.02 98.17 95.95 93.20 90.67 91.97 93.58 95.73 92.47 93.17 91.43 90.51 92.39 92.79 95.20 93.96 95.57 93.40 93.70 93.68 94.43 94.10 94.24 94.71 92.34 91.03 93.07 93.99 94.52 93.37 93.12 91.93 90.69 93.02 93.59 77.82 67.58 72.03 67.79 55.95 58.72 70.63 52.24 53.82 52.35 64.47 53.21 56.63 65.23 72.54 73.54 61.54 72.84 61.03 65.89 75.28 60.25 75.90 62.47 71.67 49.78 68.78 75.08 64.83 66.42 60.20 67.56 62.71 70.76 79.44 76.54 71.00 76.76 78.18 69.04 71.68 75. 50.83 61.90 69.34 62.80 56.50 62.31 68.30 55.62 35.76 48.33 55.58 56.30 61.23 65.87 54.47 65.19 53.45 61.01 62.61 66.24 71.34 52.49 65.63 52.52 58.87 46.43 65.92 72.86 64.86 63.07 58.45 63.08 57.27 65.68 70.14 57.65 63.94 60.95 61.42 59.71 62.70 64.12 90.24 98.76 98.44 98.76 93.06 98.00 98.77 93.18 78.45 87.36 99.15 97.54 98.38 98.69 93.75 97.80 85.33 98.54 96.22 98.30 99.17 82.41 98.46 85.15 99.36 77.94 99.41 99.72 98.92 98.12 93.06 99.01 93.64 99.46 99.65 92.91 98.37 96.41 98.76 98.38 98.40 98.43 36.08 54.14 56.59 52.56 54.77 73.61 80.12 77.30 71.26 72.05 84.44 83.65 86.28 89.05 27.88 28.50 25.36 24.83 20.68 41.32 51.31 52.21 62.46 61.84 64.54 53.32 75.80 81.62 68.22 70.11 64.56 60.30 64.58 76.00 80.52 27.38 63.29 48.59 65.34 61.19 72.76 76.79 74.85 83.45 86.00 83.48 69.39 82.21 87.35 87.18 86.63 88.46 86.46 77.02 82.16 87.05 83.20 83.81 83.40 80.47 71.45 82.20 83.41 84.11 87.17 85.82 88.15 81.54 87.94 89.69 88.52 87.21 86.97 83.58 70.18 84.42 87.69 83.22 82.11 80.24 78.15 61.77 79.59 82.93 func. 61.10 89.33 89.33 91.74 89.92 92.02 91.70 86.53 67.96 79.14 90.58 91.77 92.07 91.65 84.21 88.92 78.82 91.74 91.72 92.04 92.32 72.98 89.75 77.83 93.20 76.66 93.03 92.88 91.17 87.95 85.59 91.46 90.39 91.20 90.66 64.09 83.86 76.64 85.33 87.23 87.15 86. Combined Len. Norm 71.11 75.12 69.93 75.30 70.57 80.13 84.44 72.52 57.85 66.85 75.88 74.98 80.93 84.00 66.36 73.36 63.83 71.69 70.56 78.81 82.88 64.58 77.66 67.30 76.03 62.39 83.20 87.35 77.82 76.49 72.59 76.33 72.57 82.59 85.62 63.71 75.06 70.59 74.94 72.26 79.40 80.86 394.83 490.51 347.10 830.40 793.83 552.4 407.4 383.8 77.07 252.4 606.1 1304 923.0 598.5 102.1 355.2 252.6 436.6 593.5 433.6 450.9 226.1 353.5 260.5 386.1 472.5 438.9 369.8 320.5 368.9 288.6 539.3 1034 572.2 435.7 159.3 400.6 342.5 637.4 1027 526.0 448.5 76.52 81.24 75.02 84.12 78.09 86.88 90.17 73.28 49.77 65.55 83.57 85.98 90.33 91.32 58.51 77.86 65.06 77.87 78.17 84.92 89.08 62.34 81.37 65.70 81.89 63.99 89.42 92.89 82.74 80.18 74.40 83.33 81.56 89.64 91.61 64.44 80.81 75.73 82.79 82.32 86.36 87. Table 11: Detailed composition of the diversity scores based on the output of R1-Distilled-Qwen-14B. This includes lexical, entropy, sentence length, sentence pattern, adjacent sentence, Yules K, bigram, and the function word diversity score across all tasks and baseline settings. Besides, we also provide the combined diversity score, average reasoning length and normalized diversity score. Qwen3-8B Zero-shot CoT Self-Refine Role-Playing More think Ours(+SFT) Ours(+SFT+GRPO) Ours(+SFT+GRPO r.s.) Zero-shot CoT Self-Refine Role-Playing More think Ours(+SFT) Ours(+SFT+GRPO) Ours(+SFT+GRPO r.s.) Zero-shot CoT Self-Refine Role-Playing More think Ours(+SFT) Ours(+SFT+GRPO) Ours(+SFT+GRPO r.s.) Zero-shot CoT Self-Refine Role-Playing More think Ours(+SFT) Ours(+SFT+GRPO) Ours(+SFT+GRPO r.s.) Zero-shot CoT Self-Refine Role-Playing More think Ours(+SFT) Ours(+SFT+GRPO) Ours(+SFT+GRPO r.s.) Zero-shot CoT Self-Refine Role-Playing More think Ours(+SFT) Ours(+SFT+GRPO) Ours(+SFT+GRPO r.s.) Diversity Sub Scores (in %) lex. ent. len. pat. adj. yule. bi. 52.47 24.24 21.33 74.88 80.65 87.37 91.21 60.44 94.74 51.70 94.98 90.48 94.40 97.51 47.20 45.19 38.58 65.61 81.37 83.56 86.61 79.75 41.18 36.58 72.72 100.00 89.29 93.28 57.42 47.38 23.52 72.43 77.28 89.99 91.83 63.06 89.59 90.01 76.08 64.43 75.49 81. 95.16 83.84 58.13 90.68 91.22 93.30 93.99 92.40 94.74 49.38 90.68 90.17 91.63 92.08 96.84 51.17 50.86 89.47 90.45 91.77 92.88 95.10 56.57 55.54 90.15 92.19 91.17 92.45 83.79 56.46 73.73 90.93 89.49 92.58 92.64 94.31 92.80 92.06 90.70 89.74 91.48 93.22 75.87 82.46 82.45 76.94 58.74 59.98 64.23 38.83 74.60 96.07 63.19 54.69 57.27 59.33 43.57 96.50 91.20 67.08 64.59 63.57 65.76 42.62 95.91 91.01 76.00 47.93 68.53 70.57 42.16 91.39 54.89 67.35 66.57 73.80 68.36 72.52 78.47 79.70 79.40 74.44 78.45 81.60 54.75 47.60 36.31 69.03 60.91 65.35 67.95 28.31 61.12 59.98 59.97 56.89 59.96 62.39 32.92 43.83 55.32 63.35 63.80 65.36 68.83 62.07 45.36 44.59 62.81 54.97 63.01 66.67 45.30 46.53 54.14 68.04 58.76 62.70 64.45 51.98 69.80 69.98 66.39 61.24 65.40 67.97 97.52 99.28 70.59 97.32 97.19 98.47 99.37 67.34 99.73 89.77 98.22 97.49 98.09 99.15 65.04 60.72 85.93 93.27 96.12 96.50 97.70 99.88 70.87 72.84 96.46 97.40 97.76 98.81 64.68 66.61 86.12 97.05 97.40 98.38 98.93 93.33 98.78 98.24 95.59 97.39 98.15 98.85 38.52 37.07 31.03 22.41 46.12 58.11 69.96 50.23 80.21 52.69 53.66 71.19 74.66 79.93 16.21 1.39 0.52 7.80 12.97 17.32 33.11 36.57 2.84 4.11 18.46 61.03 52.62 63.73 29.57 10.76 9.12 21.82 52.48 64.54 68.67 28.50 60.36 61.37 36.75 45.65 54.89 65.21 80.28 14.88 14.56 67.86 65.05 79.68 83.02 80.00 87.87 28.89 75.22 66.16 74.66 77.83 82.47 25.82 22.89 64.55 72.05 80.39 84.24 97.33 23.91 19.63 67.57 87.14 73.12 79.87 67.74 26.14 14.84 70.55 59.55 77.85 77.92 82.67 80.10 78.24 69.22 50.38 66.01 78. func. 67.14 86.08 66.24 91.62 91.45 91.66 91.36 61.78 87.73 75.49 93.42 91.74 92.14 92.47 60.30 73.98 75.85 91.73 92.65 91.89 92.31 86.35 75.89 75.79 93.68 92.23 94.18 93.49 61.30 76.33 82.24 91.56 90.90 90.16 90.46 72.50 87.05 84.58 85.40 86.10 85.58 84.87 BBQ GLOQA CALI ETHICS CSQA GSM8K Combined Len. Norm 64.01 57.16 43.76 73.43 71.77 79.41 82.41 49.02 77.58 59.64 74.06 73.05 78.58 80.82 46.86 49.47 56.12 68.13 70.38 75.14 78.99 71.48 51.57 50.83 70.68 72.64 78.33 81.76 52.34 52.68 55.98 72.48 71.32 79.25 80.39 63.10 78.29 77.87 72.64 69.97 77.42 80. 75.09 220.27 364.63 501.73 837.33 490.2 430.1 195.91 345.58 515.61 824.96 2114.03 972.1 901.9 65.29 369.00 331.59 923.11 572.03 348.2 316.3 136.00 206.86 231.06 536.94 596.00 860.3 637.2 243.99 301.89 324.66 449.76 2362.13 635.4 652.0 197.31 522.58 609.06 664.80 1557.56 922.7 487.8 54.22 76.40 61.64 80.07 80.40 85.47 88.15 46.72 82.54 73.72 83.04 84.40 87.74 89.88 41.34 64.83 72.10 74.64 77.38 80.36 83.84 72.87 67.68 68.46 77.68 80.68 86.84 89.09 53.18 68.46 75.27 78.55 82.54 86.40 87.93 64.71 84.82 85.23 80.09 81.21 85.98 86.93 Table 12: Detailed composition of the diversity scores based on the output of Qwen3-8B. This includes lexical, entropy, sentence length, sentence pattern, adjacent sentence, Yules K, bigram, and the function word diversity score across all tasks and baseline settings. Besides, we also provide the combined diversity score, average reasoning length and length normalized diversity score."
        }
    ],
    "affiliations": []
}