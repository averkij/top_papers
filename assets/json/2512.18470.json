{
    "paper_title": "SWE-EVO: Benchmarking Coding Agents in Long-Horizon Software Evolution Scenarios",
    "authors": [
        "Minh V. T. Thai",
        "Tue Le",
        "Dung Nguyen Manh",
        "Huy Phan Nhat",
        "Nghi D. Q. Bui"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Existing benchmarks for AI coding agents focus on isolated, single-issue tasks such as fixing a bug or implementing a small feature. However, real-world software engineering is fundamentally a long-horizon endeavor: developers must interpret high-level requirements, plan coordinated changes across many files, and evolve codebases over multiple iterations while preserving existing functionality. We introduce SWE-EVO, a benchmark that evaluates agents on this long-horizon software evolution challenge. Constructed from release notes and version histories of seven mature open-source Python projects, Tool comprises 48 evolution tasks that require agents to implement multi-step modifications spanning an average of 21 files, validated against comprehensive test suites averaging 874 tests per instance. Experiments with state-of-the-art models reveal a striking capability gap: even GPT-5 with OpenHands achieves only a 21 percent resolution rate on Tool, compared to 65 percent on the single-issue SWE-Bench Verified. This demonstrates that current agents struggle with sustained, multi-file reasoning. We also propose Fix Rate, a fine-grained metric that captures partial progress toward solving these complex, long-horizon tasks."
        },
        {
            "title": "Start",
            "content": "SWE-EVO: Benchmarking Coding Agents in Long-Horizon Software Evolution Scenarios Minh V. T. Thai*1, Tue Le*1, Dung Nguyen Manh2, Huy N. Phan1 and Nghi D. Q. Bui1 1FPT Software AI Center, 2School of Computing and Information Systems - University of Melbourne 2025-12-25 Existing benchmarks for AI coding agents focus on isolated, single-issue tasks such as fixing bug or adding small feature. However, real-world software engineering is long-horizon endeavor: developers interpret high-level requirements, coordinate changes across many files, and evolve codebases over multiple iterations while preserving functionality. We introduce SWE-EVO, benchmark for this longhorizon software evolution challenge. Constructed from release notes of seven mature open-source Python projects, SWE-EVO comprises 48 tasks requiring multi-step modifications spanning an average of 21 files, validated against test suites averaging 874 tests per instance. Experiments reveal striking capability gap: GPT-5 with OpenHands achieves only 21% on SWE-EVO versus 65% on SWE-Bench Verified, showing that current agents struggle with sustained, multi-file reasoning. We also propose Fix Rate, metric capturing partial progress on these complex, long-horizon tasks. Github: https://github.com/bdqnghi/SWE-EVO 1. Introduction Large language models (LLMs) have achieved remarkable progress in automating software engineering (SE) tasks, including code generation (Bui et al., 2023; Chen et al., 2021a; Li et al., 2022; Manh et al., 2023; To et al., 2023; Wang et al., 2023; Wei et al., 2023; Zhuo et al., 2024), bug fixing (Jimenez et al., 2023; Xia et al., 2024), and test synthesis (Chen et al., 2022; Jain et al., 2024; Wang et al., 2024b). These advancements have facilitated the emergence of AI-powered coding agents capable of assisting or automating key aspects of the software development lifecycle (Fan et al., 2023; Gao et al., 2025; He et al., 2025; Zhang et al., 2023). Building on these capabilities, multi-agent systems, where specialized agents collaborate on subtasks such as repository navigation, bug localization, patch generation, and verification, have evolved rapidly to address long-horizon challenges in SE, outpacing single-agent architectures in scalability and performance as of 2025. Recent agent-based frameworks (Nguyen et al., 2025b; Phan et al., 2024; Wang et al., 2024d; Yang et al., 2024a) exemplify this trend, enabling autonomous handling of complex workflows in real-world repositories. According to (DORA Research Program, 2025), industry adoption underscores this momentum: over 90% of engineering teams now integrate generative AI into SE practices, sharp rise from 61% in 2024, driven by the need for efficient tools in maintaining vast legacy systems. To evaluate these agents rigorously, benchmarks have become essential. Early efforts like HumanEval (Chen et al., 2021b) focused on function-level code completion (Chen et al., 2021a), while SWE-Bench (Jimenez et al., 2023) marked shift by curating real-world GitHub issues, tasking agents with generating verifiable patches for isolated problems (Jimenez et al., 2023). SWE-Bench 1* Equal contribution 2 Project lead 3Correspondence to: Minh V. T. Thai <minhpvt@fpt.com>, Tue Le <tueldt@fpt.com>, Dung Nguyen Manh <manhdung.nguyen.1@student.unimelb.edu.au>, Huy N. Phan <huypn168@gmail.com>, Nghi D. Q. Bui <bdqnghi@gmail.com>. 5 2 0 2 3 ] . [ 2 0 7 4 8 1 . 2 1 5 2 : r SWE-EVO: Benchmarking Coding Agents in Long-Horizon Software Evolution Scenarios has gained prominence as de facto standard for assessing multi-agent capabilities in practical coding scenarios. However, as state-of-the-art (SOTA) models and agents advance (Jimenez et al., 2024), achieving scores up to 75% on variants like SWE-Bench-Verified (e.g., GPT-5 (OpenAI, 2025b)) and around 40% on the full leaderboard (e.g., OpenCSG Starship at 39.67%), the benchmark is showing signs of saturation, with diminishing marginal gains on isolated tasks. This progress, while impressive, masks deeper limitations: SWE-Bench primarily addresses discrete issue resolution, failing to capture the core intricacy of software development, which is the continuous evolution of existing systems in response to high-level requirements (Kaur and Singh, 2015; Singh et al., 2019). In reality, up to 80% of software engineering efforts involve maintaining and evolving legacy codebases rather than building new ones from scratch, entailing iterative modifications across interdependent modules, versions, and specifications (Kaur and Singh, 2015; Singh et al., 2019). This gap between benchmark tasks and real-world evolution scenarios motivates our central research question: Given an existing codebase, can multi-agent LLM systems autonomously evolve the system in response to dynamic input requirements, demonstrating sustained planning, adaptability, and innovation across long-horizon tasks? Figure 1 illustrates this high-level setting: software evolution as an iterative cycle where, starting from an existing system, engineers identify required changes, analyze their impact, and carry out an evolution process that yields new system aligned with updated requirements. Such evolution demands alignment with high-level guidelines, such as Software Requirement Specifications (SRS), and represents multifaceted challenge requiring holistic program comprehension, multi-step orchestration, and iterative refinement. These capabilities remain underexplored in current benchmarks. While SWE-Bench partially approximates this by localizing and patching files for bug reports, it confines tasks to isolated issues, often inflating performance through incomplete fixes, inadequate test coverage, or data contamination from pre-training corpora. In contrast, real-world evolution requires agents to orchestrate changes at codebase scale, akin to how human developers implement features that span numerous files and subsystems. Figure 1 Conceptual model of software evolution, depicting the iterative cycle from an existing system to new system through change identification, impact analysis, and the evolution process. To illustrate the complexity of software evolution, consider concrete example: an e-commerce website with an existing email-based account registration feature. If tasked with enhancing it to include Google or GitHub authentication for streamlined user onboarding, agents must understand the entire codebase, map out integration points (e.g., authentication APIs, user models, and session management), refactor 2 SWE-EVO: Benchmarking Coding Agents in Long-Horizon Software Evolution Scenarios relevant components (e.g., updating login routes, incorporating OAuth libraries, and modifying database schemas), ensure security compliance (e.g., handling tokens and preventing vulnerabilities), align with overarching SRS (e.g., maintaining backward compatibility and user privacy), and validate the changes through iterative testing across versions. This process mirrors authentic SE evolution, where isolated patches fall short and multi-agent collaboration (for example, one agent for planning, another for implementation, and third for verification) becomes indispensable. Existing benchmarks like SWE-Bench do not adequately test these dynamics, leading to overoptimistic assessments of agent readiness for production environments and highlighting the need for more challenging, evolution-oriented evaluations. Figure 2 Comparison of SWE-Bench and SWE-EVO. SWE-Bench tasks agents with resolving single GitHub issue to produce one patch (pull request). In contrast, SWE-EVO requires agents to interpret release notes and implement comprehensive changes that resolve multiple PRs, develop new features, and fix multiple bugs to evolve the codebase to new version. To address this gap, we introduce SWE-EVO, benchmark designed to address the challenges of autonomous software evolution rather than single-issue repair. SWE-EVO leverages release notes, commit histories, and versioned snapshots from mature open-source Python projects (e.g. scikit-learn, pydantic) to construct realistic tasks where agents must interpret SRS, plan multi-step modifications, navigate large-scale repositories, and iteratively evolve codebases across versions. Unlike SWE-Benchs focus on single-issue fixes, SWE-EVO emphasizes long-term adaptability and multi-agent orchestration. This not only mitigates saturation in prior benchmarks but also provides rigorous framework to reveal deficiencies in SOTA systems, fostering advancements in truly autonomous SE agents. We describe the benchmark construction in Section 3 and report experimental results in Section 4. In total, SWE-EVO comprises 48 such evolution tasks across 7 open repositories (Table 1 and Figure 3), each representing heavy, multi-change evolution scenario. Key Findings. Our evaluation with two agent frameworks (OpenHands and SWE-agent) and 11 state-of-the-art models reveals significant capability gap: even the best-performing model (gpt-5) resolves only 21% of SWE-EVO tasks compared to 65% on SWE-Bench Verified. Model performance exhibits intuitive scaling, where larger models consistently outperform smaller variants, and the relative ranking aligns with SWE-Bench results, validating SWE-EVO as meaningful benchmark. Trajectory-level failure analysis shows that stronger models primarily fail on instruction following SWE-EVO: Benchmarking Coding Agents in Long-Horizon Software Evolution Scenarios (misinterpreting nuanced release notes), while weaker models struggle with tool use and syntax errors, indicating that SWE-EVOs difficulty stems from semantic reasoning rather than interface competence. 2. Related Work 2.1. Code Generation and Software Engineering Benchmarks The evaluation of code generation capabilities has evolved from simple function-level tasks to complex repository-level challenges. HumanEval (Chen et al., 2021a) established foundational benchmark of 164 handwritten Python problems, setting the standard for measuring functional correctness in generated code. Building on this, MBPP (Austin et al., 2021) expanded the scope with roughly 1,000 crowd-sourced tasks aimed at entry-level programmers, while APPS (Hendrycks et al., 2021) introduced 10,000 diverse problems spanning simple to advanced algorithmic challenges. However, these early benchmarks, which focused on single-file, synthetic scenarios, fail to capture the complexity of real-world software development. More recent benchmarks such as CodeMMLU (Nguyen et al., 2025a) and CodeWiki (Hoang et al., 2025) have expanded evaluation to code understanding, reasoning, and documentation generation. Moreover, the rapid development of LLMs has begun to saturate many of these benchmarks; for instance, top methods now solve over 94% of HumanEval (Zhou et al., 2023). To bridge this gap, SWE-bench (Jimenez et al., 2024) proposed an issue-resolution task requiring models to generate verifiable patches for real GitHub issues, marking shift toward practical software engineering evaluation. Curated subsets such as SWE-bench Lite and SWE-bench Verified (OpenAI, 2024b) provide high-quality representative issues for faster and more reliable evaluation. Several extensions have since emerged to broaden coverage: Multi-SWE-bench (Zan et al., 2025) spans multiple programming languages including Java, TypeScript, Go, Rust, C, and C++, while Multimodal SWE-bench (Yang et al., 2024b) incorporates JavaScript and UI screenshots to test cross-modal reasoning. More recently, researchers have addressed scalability and realism concerns. SWE-bench-Live (Zhang et al., 2025b) and SWE-rebench (Badertdinov et al., 2025) leverage automated pipelines to continuously collect new issues, enabling live and scalable evaluation. SWE-bench Pro (Scale AI, 2025) targets enterprise-level complexity with more realistic, large-scale issues. Despite these advances, all existing benchmarks primarily focus on isolated issue resolution, such as patching individual bugs or implementing single features, rather than evaluating agents on multi-step software evolution tasks that span multiple commits and require long-horizon planning. 2.2. Software Engineering Agents Inspired by the human debugging process, where developers interact with environmental feedback and learn from earlier attempts, ChatRepair (Xia and Zhang, 2023, 2024) proposed the first interactive bug-fixing solution based on LLMs. Since then, large body of research has aimed to provide LLMs with richer context through multi-turn conversations (Chen et al., 2023; Yuan et al., 2024). In March 2024, Devin AI (Cognition, 2024) released the first AI software engineer capable of autonomously completing end-to-end software tasks, achieving an impressive 13.86% resolve rate on SWE-bench. Since then, numerous dedicated software agent scaffolds have been proposed. SWE-agent (Yang et al., 2024a) emphasized the critical importance of agentcomputer interfaces (ACIs) in enabling effective code manipulation. OpenHands (Wang et al., 2024d) introduced an open platform supporting multiple agent types and coordination mechanisms, evaluated across 15 different benchmarks. AutoCodeRover (Zhang et al., 2024) combined LLMs with sophisticated AST-based code search 4 SWE-EVO: Benchmarking Coding Agents in Long-Horizon Software Evolution Scenarios capabilities. Interestingly, Agentless (Xia et al., 2024) challenged the complexity trend with simple localization-repair approach, demonstrating that simplicity can still yield competitive performance. The field has also explored diverse architectural paradigms. AgentCoder (Huang et al., 2023) proposed multi-agent framework with specialized roles for programming, test design, and execution, demonstrating the effectiveness of role specialization. AgileCoder (Nguyen et al., 2025b) extended this paradigm by incorporating agile methodology principles into dynamic agent collaboration. CodeAct (Wang et al., 2024c) unified agent action spaces through executable Python code, leading to performance gains compared to textor JSON-based action formats. Moreover, recent LLMs have increasingly been post-trained on massive real-world software data to better solve software engineering issues, including SWE-RL (Wei et al., 2025), DeepSWE (Luo et al., 2025), DeepSeek V3.1 (DeepSeek AI, 2025), MiniMax M1/M2 (Chen et al., 2025), Kimi K2 (Kimi Team, 2025), and Code World Model (CWM) (Carbonneaux et al., 2025). Complementary efforts such as SWE-Synth (Pham et al., 2025) have focused on synthesizing verifiable bug-fix training data to expand the available supervision signal. 2.3. Self-Evolving Agents and Tool Creation Due to the huge design space for software agents, building an optimal agent scaffold can be extremely challenging and costly. As result, several self-improving and self-evolving software agents have emerged recently. The Self-Improving Coding Agent (SICA) (Robeyns et al., 2025) introduced mechanisms for agents to learn from their own experiences. Darwin-G√∂del Machine (DGM) (Zhang et al., 2025a) proposed open-ended evolution of self-improving agents through iterative refinement. Huxley-G√∂del Machine (HGM) (Wang et al., 2025) further advanced this paradigm by approximating optimal self-improving machines for human-level coding. However, such self-improving agents typically require costly offline training on known benchmarks and may not generalize well across different LLMs, benchmarks, and issue types. Beyond the software engineering domain, prior work has explored using LLMs to create tools for general reasoning or embodied tasks. Large Language Models as Tool Makers (LATM) (Cai et al., 2024) demonstrated that LLMs can autonomously create reusable tools to solve complex tasks more efficiently. Voyager (Wang et al., 2024a) introduced an open-ended embodied agent that continuously acquires new skills through code generation in Minecraft. CREATOR (Qian et al., 2024) proposed disentangling abstract and concrete reasoning through tool creation. TroVE (Wang et al., 2024e) focused on inducing verifiable and efficient toolboxes for programmatic tasks. Alita (Qiu et al., 2025) presented generalist agent enabling scalable agentic reasoning with minimal predefinition and maximal self-evolution. While these works demonstrate the potential of self-evolving agents and tool creation, they do not specifically target real-world software engineering problems that require long-horizon evolution across multiple commits and versions. In contrast to isolated issue resolution, real-world software evolution involves interpreting highlevel requirements, planning multi-step modifications, and navigating large-scale repositories across versions (Kaur and Singh, 2015; Singh et al., 2019). This gap motivates our work on SWE-EVO, which leverages release notes, commit histories, and versioned snapshots from mature open-source projects to construct realistic evolution tasks that require sustained planning, adaptability, and multi-agent orchestration across long-horizon modifications. 5 SWE-EVO: Benchmarking Coding Agents in Long-Horizon Software Evolution Scenarios 3. SWE-EVO Dataset SWE-EVO is benchmark constructed from release notes and version histories of popular open-source repositories, capturing real software evolution scenarios where coding agents must interpret high-level Software Requirement Specifications (SRS), plan and implement multi-step modifications across versions, and ensure that the evolved system passes validation tests aligned with those specifications. Table 1 Average and maximum numbers characterizing different attributes of SWE-EVO task instance. Statistics are micro-averages calculated without grouping by repository. Mean Max Issue Text Length (Words) 2390.5 22344 Codebase # Files (non-test) # Lines (non-test) Gold Patch Tests # Lines edited # Files edited # Func. edited # Fail to Pass # Total 363 78K 610.5 20.9 51.0 81.4 874.0 1046 272K 4113 105 379 2774 8552 3.1. Benchmark Construction Figure 3 Distribution of SWE-EVO tasks (in parenthesis) across 7 open source GitHub repositories that each contains the source code for popular, widely downloaded PyPI package. The construction of SWE-EVO consists of three major phases: (1) repository selection and data scraping, (2) candidate selection and filtering, and (3) execution-based filtering. We describe each phase in turn. Stage I: Repository Selection and Data Scraping. To maximize reproducibility and comparability, we inherit repositories and execution environments from SWE-bench and it keeps our benchmark plug-and-play for existing SWE agents. Concretely, we begin by collecting samples from SWEbench (Jimenez et al., 2024) and from SWE-gym (Pan et al., 2024) as our seed pool of task instances. Both resources already give us: real repository snapshot, an executable environment, and set of tests tied to human-authored change. Stage II: Candidate Selection and Filtering. Unlike SWE-bench, which frame tasks as resolving single issue, we target evolving codebase between two release versions. We therefore create candidate instances by selecting only those samples whose base commit corresponds exactly to version tag of the repository (i.e., release snapshot). For each such candidate, we define the problem statement as the release-note/SRS delta between that version and its next tagged version; the agents job is to implement the specified changes across the codebase to satisfy this SRS. Stage III: Execution-Based Filtering. Following SWE-Bench, we validate each candidate by applying the instances test patch content and logging test outcomes before and after the remaining patch content is applied. We retain only instances that exhibit at least one FAIL_TO_PASS test (i.e., test that fails pre-patch and passes post-patch), ensuring measurable behavioral change attributable to the required evolution. We additionally discard candidates that trigger installation or runtime errors under the benchmark environment. The resulting set comprises evolution tasks with verifiable behavioral deltas and stable execution characteristics. After these filtering stages, SWE-EVO comprises 48 high-quality task instances. Figure 3 reports their 6 SWE-EVO: Benchmarking Coding Agents in Long-Horizon Software Evolution Scenarios distribution across repositories, while Table 1 summarizes key instance characteristics. Notably, the lengthy and detailed problem descriptions correspond to complex issues and feature requests, while the underlying codebases are substantial, often containing hundreds to over thousand non-test files, and the gold patches frequently span multiple files. Together, these attributes demand long-horizon reasoning, extensive context understanding, and strong memory capabilities from agents. 3.2. Task Formulation Having described how SWE-EVO instances are constructed, we now formalize the task definition and evaluation metrics. Model input. model is provided with (i) release-note item that specifies the intended change (bug fix or feature refinement), and (ii) complete codebase at the pre-release commit. The model must produce edits to the codebase that implement the described change. In practice, we represent solution as patch files that specifies which lines to modify across files. For clarity, we refer to the patch generated by the model as the model patch, while the ground-truth patch extracted from the start-version to end-version change inside the instance is referred to as the gold patch. Evaluation Metrics. Consistent with the evaluation setup of SWE-bench (Jimenez et al., 2024), we use the Resolved Rate (%) as the primary metric, representing the proportion of task instances successfully solved by the agent. In addition, we report the Patch Apply Rate (%), which measures the percentage of generated patches that are syntactically valid and can be applied to the codebase without errors. Beyond the hard-score Resolved Rate (%), we introduce soft-score, Fix Rate (%), to provide more fine-grained assessment of SWE-EVO while remaining consistent with Resolved Rate. Resolved Rate. Following SWE-bench, this metric focuses on two categories in the test outcomes: FAIL_TO_PASS tests that were initially failing (FAILED or ERROR) but pass (PASSED after applying the gold patch. PASS_TO_PASS tests pass both before and after the gold patch and serve as regression checks to ensure unrelated functionality is preserved. The Resolved Rate of an instance is equal 1 if all tests in both FAIL_TO_PASS and PASS_TO_PASS are PASSED; otherwise, it is 0. Hence, this metric imposes strict binary criterion: an instance is counted as resolved only if every relevant test succeeds. Fix Rate: Soft Metric. While Resolved Rate provides clear pass/fail signal, it can obscure meaningful partial progress. As observed in both SWE-Bench and SWE-EVO, certain instances contain thousands of tests (see Table 1 and Figure 4). In such cases, even strong models may pass large proportion of tests without fully resolving the instance, leading to loss of evaluative granularity. To address this, we propose soft-score metric that accounts for partial progress. The Fix Rate measures progress by computing what fraction of FAIL_TO_PASS tests the model successfully fixes. Crucially, it also enforces regression constraint: if the models patch breaks any PASS_TO_PASS test (i.e., causes previously passing test to fail), the entire instance receives score of 0, regardless of how many FAIL_TO_PASS tests were fixed. This design rewards partial fixes while penalizing patches that introduce regressions. Formally, for each instance ùëñ, let ùêπùëñ denote the FAIL_TO_PASS tests and ùëÉùëñ denote the PASS_TO_PASS tests. The Fix Rate is defined as: 7 SWE-EVO: Benchmarking Coding Agents in Long-Horizon Software Evolution Scenarios Fix Rate(ùëñ) = #{ ùë° ùêπùëñ ùë° passes under the model patch } ùêπùëñ , if all ùë° ùëÉùëñ pass under the model patch; 0, otherwise. The overall Fix Rate (%) is the average across all ùëÅ instances: Fix Rate = 100 1 ùëÅ ùëÅ ùëñ=1 Fix Rate(ùëñ). (1) This score lies in [0, 1] and is consistent with the Resolved Rate where an instance is counted as resolved if its Fix Rate equals 1 (i.e., the model fixes all FAIL_TO_PASS tests without violating any PASS_TO_PASS tests). Aggregated over instances, Fix Rate captures partial progress that the binary Resolved Rate alone cannot reflect. As shown in Table 4, Fix Rate remains aligned with Resolved Rate yet provides finer separation across models, particularly when Resolved Rate is low. 3.3. Features of SWE-EVO With the task formulation established, we now highlight key features that distinguish SWE-EVO from existing benchmarks. Comprehensive Comparison with SWE-Bench. Compared to SWE-Bench, as shown in figure 4, SWE-EVO presents substantially richer supervision and stricter verification signals. The naturallanguage specification is markedly longer, the gold patches modify more lines, span more files, and touch more functions, and the associated test suites include many more originally failing tests alongside larger total counts. These quantities exhibit pronounced long tail, indicating that SWEEVO captures broader, multi-edit changes and stronger regression risk. Overall, SWE-EVO requires deeper semantic understanding, broader reasoning, and stronger generalization across complex multi-file edits than SWE-Bench, making it more challenging and realistic benchmark for evaluating software capabilities. Diversity of Difficulty via Pull Request. Unlike SWE-Bench, where each task instance corresponds to single pull request (PR), an instance in SWE-EVO may be associated with multiple pull requests that collectively implement or refine release-note change (Figure 5). We hypothesize that instances associated with larger number of pull requests reflect more complex, multi-step development efforts, as each pull request typically represents distinct feature enhancement or bug fix. As shown in Figure 5, the number of pull requests per instance varies widely, indicating that SWE-EVO spans broad range of difficulty levels. Some instances involve single, localized change, while others require coordinating and integrating multiple related updates across the codebase. Robust evaluation. For each task instance in SWE-EVO, at least one FAIL_TO_PASS test is included to validate the gold patch, with roughly 81% of instances containing two or more such tests. These tests verify whether the models patch effectively resolves the issue described, while an additional mean of 793 PASS_TO_PASS tests are executed to ensure that pre-existing functionality remains intact. 8 SWE-EVO: Benchmarking Coding Agents in Long-Horizon Software Evolution Scenarios Figure 4 SWE-EVO is markedly more demanding than SWE-Bench, with longer issue descriptions, broader patch scope (more lines/files/functions edited), and heavier test suites (FAIL_TO_PASS and total) in both mean and max. These properties require agents capable of long-context reasoning, coordinated multi-file edits, and regression-safe fixes. 4. Experiments 4.1. Experiment Setup We report the experimental results on SWE-EVO. The following section outlines the experimental scaffold, evaluation metrics, and configurations to ensure reproducibility. Our assessment covers diverse range of models, including frontier proprietary systems and open-weight models. Agents and Model Selection. To evaluate SWE-EVO, we conduct experiments using two popular and well-established coding agent frameworks. These include the general-purpose coding agent OpenHands (Wang et al., 2024d) (paired with CodeActAgent) and SWE-agent (Yang et al., 2024a). For OpenHands, we set maximum of 100 iterations per instance. For SWE-Agent, we limit the number of LLM calls to 100 per instance to maintain computational efficiency. We test these agents using several recent state-of-the-art LLMs spanning different architectural approaches and parameter scales, covering both proprietary and open-source models: GPT-5 (OpenAI, 2025b): GPT-5-pro-10-06, GPT-5-08-07, GPT-5-mini-08-07, GPT-5-nano-08-071 O3 (OpenAI, 2025c): O3-2025-04-16 GPT-4.1 (OpenAI, 2025a): GPT-4.1-2025-04-14 GPT-4o (OpenAI, 2024a): GPT-4o-2024-11-20 GPT-oss-120b (Agarwal et al., 2025) Deepseek-R1 (Guo et al., 2025): Deepseek-R1-0528 GLM-4p5 (Zeng et al., 2025) Qwen3-coder (Qwen Team, 2025): Qwen3-Coder-480B-A35B-Instruct Kimi-K2-Instruct (Kimi Team, 2025) 1For GPT-5 and o3 models, we use OpenAIs medium reasoning effort setting, which balances inference cost and accuracy. 9 SWE-EVO: Benchmarking Coding Agents in Long-Horizon Software Evolution Scenarios (a) Complementary cumulative distribution of PR counts, showing how many instances contain at least ùë¶ linked pull requests. (b) Mean PRs per repository, highlighting that certain codebases require substantially more upstream context to resolve. Figure 5 Pull Request statistics across SWE-EVO instances. 4.2. Performance on SWE-EVO In SWE-EVO, each task is defined by an original release-note entry that we use as the problem statement. These release notes often reference one or more upstream pull requests or GitHub issues. Because such contextual artifacts are not always available in practice, we evaluate agents under two input settings: (i) our default setting: release-note only, where the agent receives only the raw release note, and (ii) release-note + PR/issue, where we additionally provide the content of the pull requests and/or issues referenced by that release note. We report the performance of all OpenHands and SWE-agent scaffolds across all model on SWE-EVO under these two settings in Tables 2 and 3. For comparison, we also include each models performance on SWE-bench Verified. Table 2 Results on SWE-EVO with release note + PR/issue context: Resolved and Apply rates for OpenHands and SWE-agent. Results reported on swebench.com (Jimenez et al., 2024). SWE-EVO (OpenHands) SWE-EVO (SWE-agent) SWE-bench Verified (bash only) Model % Resolved % Apply % Resolved % Apply % Resolved gpt-5-08-07 (medium reasoning) gpt-5-mini-08-07 (medium reasoning) gpt-5-nano-08-07 (medium reasoning) o3-2025-04-16 (medium reasoning) gpt-4.1-2025-04-14 gpt-4o-2024-11-20 gpt-oss-120b deepseek-r1-0528 glm-4p5 qwen3-coder-480b-a35b-instruct kimi-k2-instruct 18.75 10.42 4.17 4.17 2.08 6.25 2.08 10.42 16.67 14.58 16.67 100 97.92 85.42 93.75 87.50 97.92 100 100 97.92 97.92 100 20.83 10.42 4.17 6.25 10.42 6.25 6.25 8.33 16.67 14.58 18.75 100 100 100 100 97.92 100 100 100 100 97.92 65.00 59.80 34.80 58.40 39.58 21.62 26.00 57.60 54.20 55.40 43.80 10 SWE-EVO: Benchmarking Coding Agents in Long-Horizon Software Evolution Scenarios Overall, SWE-EVO is substantially more challenging than SWE-bench Verified. Even the strongest model (gpt-5) resolves only 1921% of instances on SWE-EVO, compared to 65% resolved on the existing issue-resolution benchmark SWE-bench Verified. Beyond this difficulty, performance on SWE-EVO exhibits clean and intuitive scaling trend: larger models reliably outperform their smaller variants (e.g., gpt-5 > gpt-5-mini > gpt-5-nano), and the ranking of proprietary models closely mirrors their relative performance on SWE-bench Verified. Finally, providing PR/issue context yields measurable but modest gains, suggesting that even with detailed and fully specified context, agents still struggle to reconstruct the correct release-note-aligned implementation change. Taken together, these results indicate that SWE-EVO is more demanding and more realistic benchmark for evaluating end-to-end software-evolution capabilities. We also evaluate the more challenging release-note only setting, where agents must infer required changes without additional PR/issue context. Table 3 shows that performance drops modestly but remains consistent with the trends observed in the context-enhanced setting, confirming that SWEEVO poses substantial challenges regardless of the amount of specification detail provided. Table 3 Results on SWE-EVO in the release-note only setting: Resolved and Apply rates for OpenHands and SWE-agent. Results reported on swebench.com (Jimenez et al., 2024). SWE-EVO (OpenHands) SWE-EVO (SWE-agent) SWE-bench Verified (bash only) Model % Resolved % Apply % Resolved % Apply % Resolved gpt-5-08-07 (medium reasoning) gpt-5-mini-08-07 (medium reasoning) o3-2025-04-16 (medium reasoning) gpt-4.1-2025-04-14 gpt-4o-2024-11-20 gpt-oss-120b deepseek-r1-0528 glm-4p5 qwen3-coder-480b-a35b-instruct kimi-k2-instruct 14.58 8.33 4.17 2.08 8.33 0 10.42 6.25 14.58 8.33 100 100 97.96 95.92 100 100 100 97.96 100 100 16. 10.42 6.25 8.33 4.17 0 6.25 12.50 12.50 12.50 100 100 100 100 100 100 100 97.96 100 97.96 65.00 59.80 58.40 39.58 21.62 26.00 57.60 54.20 55.40 43.80 Fine-Grained Analysis with Fix Rate. Beyond the binary Resolved Rate, we also examine our soft metric, Fix Rate, which is designed to capture partial progress on large test suites that the hard score cannot see and is described in detail in Section 3.2. As shown in Table 4, models that look indistinguishable under Resolved Rate can differ meaningfully once partial test improvements are credited. For example, under the OpenHands scaffold both gpt-4.1 and gpt-oss-120b resolve only 2.08% 1/48 of SWE-EVO, but their Fix Rates are corresponding around 4.65% and 2.08%, indicating that gpt-4.1 consistently repairs more failing tests per instance than gpt-oss-120b. This finer granularity is crucial on SWE-EVO, where each instance involves many tests: without Fix Rate, such near-miss trajectories would be collapsed into the same zero-one outcome, obscuring systematic differences in model behavior. 4.3. Analysis of Agent Behaviour Beyond aggregate metrics, we investigate why agents fail on SWE-EVO by analyzing their execution trajectories. This qualitative analysis reveals distinct failure patterns across different model families. 11 SWE-EVO: Benchmarking Coding Agents in Long-Horizon Software Evolution Scenarios Table 4 Comparison of Resolved Rate (%) and Fix Rate (%) across models on OpenHands and SWE-Agent. Model OpenHands SWE-Agent Resolved (%) Fix (%) Resolved (%) Fix (%) gpt-5-08-07 (medium reasoning) gpt-5-mini-08-07 (medium reasoning) gpt-5-nano-08-07 (medium reasoning) o3-2025-04-16 gpt-4.1-2025-04-14 gpt-4o-2024-11-20 gpt-oss-120b deepseek-r1-0528 glm-4p5 qwen3-coder-480b-a35b-instruct kimi-k2-instruct 18.75 10.42 4.17 4.17 2.08 6.25 2.08 10.42 16.67 14.58 16.67 27.64 17.48 5.99 6.47 4.65 7.77 2.08 14.31 23.74 19.56 22. 20.83 10.42 4.17 4.17 10.42 6.25 6.25 8.33 16.67 14.58 18.75 31.44 17.48 5.26 13.72 14.79 10.15 7.88 9.89 26.55 23.74 24.03 4.3.1. Trajectory Failure Modes Analysis (a) GPT-series models. (b) Open-source models. Figure 6 Failure mode distribution for SWE-agent with several model trajectories of unresolved instances. Each instance is automatically labeled using gpt-5-mini (OpenAI, 2025b) with categories from Table 5. We perform an LLM-as-a-judge analysis of failure modes for SWE-agent trajectories on SWE-EVO, following the methodology proposed in prior work on SWE-agent (Yang et al., 2024a). Method. We first run SWE-agent with each model on all SWE-EVO instances and restrict attention to unresolved cases. For every failing trajectory, we extract the last 20 turns, which we found to provide good balance between capturing the decisive part of the interaction and avoiding unnecessary noise from early exploration. The judge is prompted to first briefly justify its reasoning and then assign exactly one primary failure label from table 5 to the trajectory. This automated labeling allows us to systematically characterize how different models fail on SWE-EVO. 12 SWE-EVO: Benchmarking Coding Agents in Long-Horizon Software Evolution Scenarios Categories. The failure taxonomy is summarized in Table 5. It covers both low-level and high-level error modes. Syntax Error captures patches that break parsing or formatting, preventing tests from running. Incorrect Implementation denotes patches that touch the right area of the code but do not correctly implement the required behaviour. Instruction Following indicates that the agent misinterprets or ignores the release note, effectively solving the wrong task. Tool-Use covers errors in invoking the SWE-agent tools (e.g., failing to run tests, misusing edit/apply commands, or opening the wrong paths). Stuck in Loop reflects trajectories where the agent repeatedly reads files or reruns tests without making meaningful progress. Gave Up Prematurely corresponds to agents that stop early or declare failure while viable next steps remain. Finally, Other collects rare or ambiguous patterns that do not cleanly fit into the above category. Results. Figure 6 reveals distinct failure fingerprints across various model families. Among the GPT-series, the strongest model (gpt-5) almost never fails due to syntax or tool errors; instead, its unresolved cases are dominated by Instruction Following mistakes (over 60%), with smaller share of Incorrect Implementation. This suggests that, on SWE-EVO, gpt-5s remaining failures stem less from local editing or tool competence and more from misinterpreting long, nuanced release notes. gpt-5-mini shows more balanced mix of Incorrect Implementation and Instruction Following, while gpt-5-nano exhibits substantial Tool-Use and Syntax Error failures, highlighting how shrinking the model quickly degrades its ability to operate the agent interface reliably. Other GPT models (o3, gpt-4.1, gpt-4o) exhibit more syntax, looping, and early-termination failures on SWE-EVO, indicating less robust long-horizon trajectories compared to gpt-5. Open-source and hybrid models show different profiles. kimi-k2-instructs failures are overwhelmingly Incorrect Implementation (around 70%), with almost no Tool-Use issues, implying good interface control but weaker semantic reasoning. qwen3-coder and gpt-oss-120b similarly fail mostly via Incorrect Implementation and occasional Instruction Following mistakes. In contrast, deepseek-r1 often gets Stuck in Loop or encounters Tool-Use and early-exit failures, while glm-4p5 spreads its errors more evenly across categories, reflecting broad but shallow weaknesses rather than single dominant failure mode. 4.3.2. Performance vs. Difficulty key property of SWE-EVO is that the difficulty of each instance varies widely depending on number of pull requests. Intuitively, each pull request typically represents coherent development step such as bug fix, refactoring, or feature enhancement, therefore, instances that aggregate many such changes are expected to require deeper reasoning, broader codebase understanding, and more extensive multi-file edits. To validate this intuition, we group instances by their empirical difficulty: for each instance, we count how many times it is successfully resolved across all model-scaffold combinations, yielding resolution count ùëü. We then group instances into four difficulty groups: ùëü = 0, 0 < ùëü 5, 5 < ùëü 10 and ùëü > 10. These groups respectively contain roughly 64%, 15%, 15%, and 6% of all instances. Figure 7a shows clear monotonic trend: easier instances (those frequently solved) tend to be associated with fewer PRs, while harder ones correspond to substantially more PRs. The average PR counts corresponding are: 14.84, 6.71, 3.57, and 1.67 across the four groups which strongly support our hypothesis that PR count is practical and reliable proxy for instance difficulty in SWE-EVO. To further understand how models behave across difficulty levels, figure 7b reports the average number of turns taken by each model. Stronger models such as gpt-5 and gpt-5-mini exhibit efficient reasoning: they invest more turns on difficult instances and terminate earlier on easier ones, 13 SWE-EVO: Benchmarking Coding Agents in Long-Horizon Software Evolution Scenarios (a) Average number of pull requests per difficulty group, showing strong correlation between PR count and empirical difficulty. (b) Average number of turns by model across difficulty groups, illustrating the effort of each model. Figure 7 Difficulty analysis of SWE-EVO instances: (a) instances associated with more pull requests are significantly harder; (b) stronger models scale turn usage with difficulty, while others show limited adaptivity. suggesting efficient and difficulty-aware trajectory planning. Interestingly, the previous-generation OpenAI reasoning model o3 consistently runs at high turn count regardless of instance difficulty, indicating less adaptive behavior. In contrast, the reasoning-oriented deepseek-r1 displays notably fewer turns than nearly all other models, even on the hardest instances. This pattern is consistent with strategies that prioritize early termination or highly confident but potentially brittle solution paths, which limit deep exploration. Other model families, such as instruct models (kimi-k2-instruct, qwen3-coder) and hybrid reasoning model (glm-4p5), do increase their effort for harder groups, but their turn adjustments are modest and less discriminative than those of the strongest GPT-5 variants. 5. Conclusion In this work, we introduced SWE-EVO, new benchmark designed to evaluate coding agents on realistic software evolution rather than isolated bug fixing. Unlike existing benchmarks that focus on single-issue resolution, SWE-EVO requires models to interpret high-level release notes, implement multi-file changes spanning entire subsystems, and maintain functionality under comprehensive regression testing. SWE-EVO keeps the simple execution setup of SWE-Bench but substantially increases task complexity through longer specifications, wider code changes, and larger test suites, forcing agents to plan across many files while avoiding regressions. Our experiments with two agent frameworks (OpenHands and SWE-agent) and broad set of stateof-the-art models reveal significant performance gap. Even the best model, gpt-5, solves only approximately 21% of SWE-EVO tasks compared to 65% on SWE-Bench Verified, demonstrating clear disparity between current coding agent capabilities and the demands of real-world software evolution. Providing additional context in the form of PR/issue descriptions yields modest improvements, 14 SWE-EVO: Benchmarking Coding Agents in Long-Horizon Software Evolution Scenarios but agents still struggle to correctly implement release-note-aligned changes. Furthermore, our trajectory-level failure mode analysis reveals where and how agents fail: the strongest models (e.g., gpt-5) primarily struggle with instruction following (misinterpreting or incompletely addressing long, nuanced release notes) while weaker models exhibit additional issues with tool use, syntax errors, and premature termination. Open-source models show different failure profiles, with many falling short on implementation correctness despite adequate interface control. Limitations. SWE-EVO currently covers only Python projects and relies on release notes as specifications, which may not capture all evolution scenarios (e.g., security patches, performance optimizations without explicit notes). The 48-instance scale, while ensuring quality, limits statistical power for fine-grained comparisons. Future work should expand language coverage and instance count. Taken together, these results suggest that progress on autonomous software engineering will require advances in both model design and agent architectures specifically tuned for evolution-style tasks. We expect SWE-EVO to serve as rigorous, evolution-oriented benchmark for evaluating long-horizon agent capabilities, complementing single-issue benchmarks like SWE-bench. 15 SWE-EVO: Benchmarking Coding Agents in Long-Horizon Software Evolution Scenarios"
        },
        {
            "title": "References",
            "content": "Sandhini Agarwal, Lama Ahmad, Jason Ai, Sam Altman, Andy Applebaum, Edwin Arbus, Rahul Arora, Yu Bai, Bowen Baker, Haiming Bao, et al. gpt-oss-120b & gpt-oss-20b model card. arXiv preprint arXiv:2508.10925, 2025. Jacob Austin, Augustus Odena, Maxwell Nye, Maarten Bosma, Henryk Michalewski, David Dohan, Ellen Jiang, Carrie Cai, Michael Terry, Quoc Le, et al. Program synthesis with large language models. arXiv preprint arXiv:2108.07732, 2021. Ibragim Badertdinov, Alexander Golubev, Maksim Nekrashevich, Anton Shevtsov, Simon Karasik, Andrei Andriushchenko, Maria Trofimova, Daria Litvintseva, and Boris Yangel. Swe-rebench: An automated pipeline for task collection and decontaminated evaluation of software engineering agents. arXiv preprint arXiv:2505.20411, 2025. Nghi DQ Bui, Hung Le, Yue Wang, Junnan Li, Akhilesh Deepak Gotmare, and Steven CH Hoi. Codetf: One-stop transformer library for state-of-the-art code llm. arXiv preprint arXiv:2306.00029, 2023. Tianle Cai, Xuezhi Wang, Tengyu Ma, Xinyun Chen, and Denny Zhou. Large language models as tool makers. In International Conference on Representation Learning, 2024. Quentin Carbonneaux, Gal Cohen, Jonas Gehring, Jacob Kahn, Jannik Kossen, Felix Kreuk, Emily McMilin, Michel Meyer, Yuxiang Wei, David Zhang, et al. Cwm: An open-weights llm for research on code generation with world models. arXiv preprint arXiv:2510.02387, 2025. Aili Chen, Aonian Li, Bangwei Gong, Binyang Jiang, Bo Fei, Bo Yang, Boji Shan, Changqing Yu, Chao Wang, Cheng Zhu, et al. Minimax-m1: Scaling test-time compute efficiently with lightning attention. arXiv preprint arXiv:2506.13585, 2025. Bei Chen, Fengji Zhang, Anh Nguyen, Daoguang Zan, Zeqi Lin, Jian-Guang Lou, and Weizhu Chen. Codet: Code generation with generated tests. arXiv preprint arXiv:2207.10397, 2022. Mark Chen, Jerry Tworek, Heewoo Jun, Qiming Yuan, Henrique Ponde De Oliveira Pinto, Jared Kaplan, Harri Edwards, Yuri Burda, Nicholas Joseph, Greg Brockman, et al. Evaluating large language models trained on code. arXiv preprint arXiv:2107.03374, 2021a. Mark Chen, Jerry Tworek, Heewoo Jun, Qiming Yuan, Henrique Ponde De Oliveira Pinto, Jared Kaplan, Harri Edwards, Yuri Burda, Nicholas Joseph, Greg Brockman, et al. Evaluating large language models trained on code. arXiv preprint arXiv:2107.03374, 2021b. Xinyun Chen, Maxwell Lin, Nathanael Sch√§rli, and Denny Zhou. Teaching large language models to self-debug. arXiv preprint arXiv:2304.05128, 2023. Cognition. Devin ai, 2024. https://cognition.ai/blog/introducing-devin. DeepSeek AI. DeepSeek V3.1, 2025. https://api-docs.deepseek.com/news/news250821. DORA Research Program. State of ai-assisted software development: 2025 dora report. https: //cloud.google.com/resources/content/2025-dora-ai-assisted-software-dev elopment-report, September 2025. DevOps Research and Assessment (DORA). Angela Fan, Beliz Gokkaya, Mark Harman, Mitya Lyubarskiy, Shubho Sengupta, Shin Yoo, and Jie Zhang. Large language models for software engineering: Survey and open problems. In 2023 IEEE/ACM International Conference on Software Engineering: Future of Software Engineering (ICSE-FoSE), pages 3153. IEEE, 2023. 16 SWE-EVO: Benchmarking Coding Agents in Long-Horizon Software Evolution Scenarios Cuiyun Gao, Xing Hu, Shan Gao, Xin Xia, and Zhi Jin. The current challenges of software engineering in the era of large language models. ACM Transactions on Software Engineering and Methodology, 34(5):130, 2025. Daya Guo, Dejian Yang, Haowei Zhang, Junxiao Song, Ruoyu Zhang, Runxin Xu, Qihao Zhu, Shirong Ma, Peiyi Wang, Xiao Bi, et al. Deepseek-r1: Incentivizing reasoning capability in llms via reinforcement learning. arXiv preprint arXiv:2501.12948, 2025. Junda He, Christoph Treude, and David Lo. Llm-based multi-agent systems for software engineering: Literature review, vision, and the road ahead. ACM Transactions on Software Engineering and Methodology, 34(5):130, 2025. Dan Hendrycks, Steven Basart, Saurav Kadavath, Mantas Mazeika, Akul Arora, Ethan Guo, Collin Burns, Samir Puranik, Horace He, Dawn Song, et al. Measuring coding challenge competence with apps. arXiv preprint arXiv:2105.09938, 2021. Anh Nguyen Hoang, Minh Le-Anh, Bach Le, and Nghi DQ Bui. Codewiki: Evaluating ais ability to generate holistic documentation for large-scale codebases. arXiv preprint arXiv:2510.24428, 2025. Dong Huang, Jie Zhang, Michael Luck, Qingwen Bu, Yuhao Qing, and Heming Cui. Agentcoder: Multi-agent-based code generation with iterative testing and optimisation. arXiv preprint arXiv:2312.13010, 2023. Kush Jain, Gabriel Synnaeve, and Baptiste Roziere. Testgeneval: real world unit test generation and test completion benchmark. arXiv preprint arXiv:2410.00752, 2024. Carlos Jimenez, John Yang, Alexander Wettig, Shunyu Yao, Kexin Pei, Ofir Press, and Karthik Narasimhan. Swe-bench: Can language models resolve real-world github issues? arXiv preprint arXiv:2310.06770, 2023. Carlos E. Jimenez, John Yang, Alexander Wettig, Shunyu Yao, Kexin Pei, Ofir Press, and Karthik R. Narasimhan. Swe-bench: Can language models resolve real-world github issues? In The Twelfth International Conference on Learning Representations, ICLR 2024, Vienna, Austria, May 7-11, 2024. OpenReview.net, 2024. URL https://openreview.net/forum?id=VTF8yNQM66. Uttamjit Kaur and Gagandeep Singh. review on software maintenance issues and how to reduce maintenance efforts. International Journal of Computer Applications, 118(1):611, 2015. Kimi Team. Kimi k2: Open agentic intelligence. arXiv preprint arXiv:2507.20534, 2025. Yujia Li, David Choi, Junyoung Chung, Nate Kushman, Julian Schrittwieser, R√©mi Leblond, Tom Eccles, James Keeling, Felix Gimeno, Agustin Dal Lago, et al. Competition-level code generation with alphacode. Science, 378(6624):10921097, 2022. Michael Luo, Naman Jain, Jaskirat Singh, Sijun Tan, Ameen Patel, Qingyang Wu, Alpay Ariyak, Colin Cai, Shang Zhu Tarun Venkat, Ben Athiwaratkun, et al. Deepswe: Training fully open-sourced, state-of-the-art coding agent by scaling rl, 2025. Dung Nguyen Manh, Nam Le Hai, Anh TV Dau, Anh Minh Nguyen, Khanh Nghiem, Jin Guo, and Nghi DQ Bui. The vault: comprehensive multilingual dataset for advancing code understanding and generation. arXiv preprint arXiv:2305.06156, 2023. Dung Manh Nguyen, Thang Chau Phan, Nam Le Hai, Tien-Thong Doan, Nam Nguyen, Quang Pham, and Nghi DQ Bui. Codemmlu: multi-task benchmark for assessing code understanding & reasoning capabilities of codellms. In The Thirteenth International Conference on Learning Representations, 2025a. 17 SWE-EVO: Benchmarking Coding Agents in Long-Horizon Software Evolution Scenarios Minh Huynh Nguyen, Thang Phan Chau, Phong Nguyen, and Nghi DQ Bui. Agilecoder: Dynamic collaborative agents for software development based on agile methodology. In 2025 IEEE/ACM Second International Conference on AI Foundation Models and Software Engineering (Forge), pages 156167. IEEE, 2025b. OpenAI. Gpt-4o system card, 2024a. URL https://openai.com/index/hello-gpt-4o/. OpenAI. Swe-bench verified, 2024b. https://openai.com/index/introducing-swe-bench -verified/. OpenAI. Introducing gpt-4.1 in the api, 2025a. URL https://openai.com/index/gpt-4-1/. OpenAI. Gpt-5 system card. Technical report, OpenAI, 2025b. URL https://cdn.openai.com/g pt-5-system-card.pdf. OpenAI. Openai o3 and o4-mini system card. Technical report, OpenAI, 2025c. URL https: //openai.com/index/o3-o4-mini-system-card/. Jiayi Pan, Xingyao Wang, Graham Neubig, Navdeep Jaitly, Heng Ji, Alane Suhr, and Yizhe Zhang. Training software engineering agents and verifiers with swe-gym, 2024. URL https://arxiv. org/abs/2412.21139. Minh VT Pham, Huy Phan, Hoang Phan, Cuong Le Chi, Tien Nguyen, and Nghi DQ Bui. Swesynth: Synthesizing verifiable bug-fix data to enable large language models in resolving real-world bugs. arXiv preprint arXiv:2504.14757, 2025. Huy Nhat Phan, Phong X. Nguyen, and Nghi D. Q. Bui. Hyperagent: Generalist software engineering agents to solve coding tasks at scale. CoRR, abs/2409.16299, 2024. doi: 10.48550/ARXIV.2409. 16299. URL https://doi.org/10.48550/arXiv.2409.16299. Cheng Qian, Chi Han, Yi R. Fung, Yujia Qin, Zhiyuan Liu, and Heng Ji. Creator: Tool creation for disentangling abstract and concrete reasoning of large language models, 2024. Jiahao Qiu, Xuan Qi, Tongcheng Zhang, Xinzhe Juan, Jiacheng Guo, Yifu Lu, Yimin Wang, Zixin Yao, Qihan Ren, Xun Jiang, Xing Zhou, Dongrui Liu, Ling Yang, Yue Wu, Kaixuan Huang, Shilong Liu, Hongru Wang, and Mengdi Wang. Alita: Generalist agent enabling scalable agentic reasoning with minimal predefinition and maximal self-evolution, 2025. Qwen Team. Qwen3 technical report, 2025. URL https://arxiv.org/abs/2505.09388. Maxime Robeyns, Martin Szummer, and Laurence Aitchison. self-improving coding agent. arXiv preprint arXiv:2504.15228, 2025. Scale AI. Swe-bench pro: Can ai agents solve long-horizon software engineering tasks? arXiv preprint arXiv:2509.16941, 2025. Chamkaur Singh, Neeraj Sharma, and Narender Kumar. Analysis of software maintenance cost affecting factors and estimation models. Int. J. Sci. Technol. Res, 8(9):276281, 2019. Hung Quoc To, Minh Huynh Nguyen, and Nghi DQ Bui. Functional overlap reranking for neural code generation. arXiv preprint arXiv:2311.03366, 2023. Guanzhi Wang, Yuqi Xie, Yunfan Jiang, Ajay Mandlekar, Chaowei Xiao, Yuke Zhu, Linxi Fan, and Anima Anandkumar. Voyager: An open-ended embodied agent with large language models. Transactions on Machine Learning Research, 2024a. 18 SWE-EVO: Benchmarking Coding Agents in Long-Horizon Software Evolution Scenarios Wenhan Wang, Chenyuan Yang, Zhijie Wang, Yuheng Huang, Zhaoyang Chu, Da Song, Lingming Zhang, An Ran Chen, and Lei Ma. Testeval: Benchmarking large language models for test case generation. arXiv preprint arXiv:2406.04531, 2024b. Wenyi Wang, Piotr Piƒôkos, Li Nanbo, Firas Laakom, Yimeng Chen, Mateusz Ostaszewski, Mingchen Zhuge, and J√ºrgen Schmidhuber. Huxley-g√∂del machine: Human-level coding agent development by an approximation of the optimal self-improving machine, 2025. Xingyao Wang, Yangyi Chen, Lifan Yuan, Yizhe Zhang, Yunzhu Li, Hao Peng, and Heng Ji. Executable code actions elicit better llm agents. In Forty-first International Conference on Machine Learning, 2024c. Xingyao Wang, Boxuan Li, Yufan Song, Frank Xu, Xiangru Tang, Mingchen Zhuge, Jiayi Pan, Yueqi Song, Bowen Li, Jaskirat Singh, et al. Openhands: An open platform for ai software developers as generalist agents. arXiv preprint arXiv:2407.16741, 2024d. Yue Wang, Hung Le, Akhilesh Deepak Gotmare, Nghi DQ Bui, Junnan Li, and Steven CH Hoi. Codet5+: Open code large language models for code understanding and generation. arXiv preprint arXiv:2305.07922, 2023. Zhiruo Wang, Daniel Fried, and Graham Neubig. Trove: Inducing verifiable and efficient toolboxes for solving programmatic tasks, 2024e. Yuxiang Wei, Zhe Wang, Jiawei Liu, Yifeng Ding, and Lingming Zhang. Magicoder: Source code is all you need. arXiv preprint arXiv:2312.02120, 2023. Yuxiang Wei, Olivier Duchenne, Jade Copet, Quentin Carbonneaux, Lingming Zhang, Daniel Fried, Gabriel Synnaeve, Rishabh Singh, and Sida I. Wang. Swe-rl: Advancing llm reasoning via reinforcement learning on open software evolution. arXiv preprint arXiv:2502.18449, 2025. Chunqiu Steven Xia and Lingming Zhang. Conversational automated program repair. arXiv preprint arXiv:2301.13246, 2023. Chunqiu Steven Xia and Lingming Zhang. Automated program repair via conversation: Fixing 162 out of 337 bugs for $0.42 each using chatgpt. In Proceedings of the 33rd ACM SIGSOFT International Symposium on Software Testing and Analysis, pages 819831, 2024. Chunqiu Steven Xia, Yinlin Deng, Soren Dunn, and Lingming Zhang. Agentless: Demystifying llm-based software engineering agents. arXiv preprint arXiv:2407.01489, 2024. John Yang, Carlos Jimenez, Alexander Wettig, Kilian Lieret, Shunyu Yao, Karthik Narasimhan, and Ofir Press. Swe-agent: Agent-computer interfaces enable automated software engineering. arXiv preprint arXiv:2405.15793, 2024a. John Yang, Carlos Jimenez, Alex Zhang, Kilian Lieret, Joyce Yang, Xindi Wu, Ori Press, Niklas Muennighoff, Gabriel Synnaeve, Karthik Narasimhan, et al. Swe-bench multimodal: Do ai systems generalize to visual software domains? arXiv preprint arXiv:2410.03859, 2024b. Zhiqiang Yuan, Mingwei Liu, Shiji Ding, Kaixin Wang, Yixuan Chen, Xin Peng, and Yiling Lou. Evaluating and improving chatgpt for unit test generation. Proceedings of the ACM on Software Engineering, 1(FSE):17031726, 2024. Daoguang Zan, Zhirong Huang, Wei Liu, Hanwu Chen, Linhao Zhang, Shulin Xin, Lu Chen, Qi Liu, Xiaojian Zhong, Aoyan Li, et al. Multi-swe-bench: multilingual benchmark for issue resolving. arXiv preprint arXiv:2504.02605, 2025. 19 SWE-EVO: Benchmarking Coding Agents in Long-Horizon Software Evolution Scenarios Aohan Zeng, Xin Lv, Qinkai Zheng, Zhenyu Hou, Bin Chen, Chengxing Xie, Cunxiang Wang, Da Yin, Hao Zeng, Jiajie Zhang, et al. Glm-4.5: Agentic, reasoning, and coding (arc) foundation models. arXiv preprint arXiv:2508.06471, 2025. Jenny Zhang, Shengran Hu, Cong Lu, Robert Lange, and Jeff Clune. Darwin godel machine: Openended evolution of self-improving agents. arXiv preprint arXiv:2505.22954, 2025a. Linghao Zhang, Shilin He, Chaoyun Zhang, Yu Kang, Bowen Li, Chengxing Xie, Junhao Wang, Maoquan Wang, Yufan Huang, Shengyu Fu, Elsie Nallipogu, Qingwei Lin, Yingnong Dang, Saravan Rajmohan, and Dongmei Zhang. Swe-bench goes live! arXiv preprint arXiv:2505.23419, 2025b. Quanjun Zhang, Chunrong Fang, Yang Xie, Yaxin Zhang, Yun Yang, Weisong Sun, Shengcheng Yu, and Zhenyu Chen. survey on large language models for software engineering. arXiv preprint arXiv:2312.15223, 2023. Yuntong Zhang, Haifeng Ruan, Zhiyu Fan, and Abhik Roychoudhury. Autocoderover: Autonomous In Proceedings of the 33rd ACM SIGSOFT International Symposium on program improvement. Software Testing and Analysis, pages 15921604, Vienna, Austria, 2024. ACM. Andy Zhou, Kai Yan, Michal Shlapentokh-Rothman, Haohan Wang, and Yu-Xiong Wang. Language agent tree search unifies reasoning acting and planning in language models. arXiv preprint arXiv:2310.04406, 2023. Terry Yue Zhuo, Minh Chien Vu, Jenny Chim, Han Hu, Wenhao Yu, Ratnadira Widyasari, Imam Nur Bani Yusuf, Haolan Zhan, Junda He, Indraneil Paul, et al. Bigcodebench: Benchmarking code generation with diverse function calls and complex instructions. arXiv preprint arXiv:2406.15877, 2024. 20 SWE-EVO: Benchmarking Coding Agents in Long-Horizon Software Evolution Scenarios"
        },
        {
            "title": "Appendix",
            "content": "Failure Mode Descriptions To better understand how coding agents fail on SWE-EVO, we adopt coarse but interpretable taxonomy of failure modes, inspired by prior analysis of SWE-agent (Yang et al., 2024a) trajectories on SWE-bench Lite. For each unresolved instance, we extract the SWE-agent trajectory and ask judge model to assign exactly one primary failure label from fixed set of categories. The taxonomy is designed to balance coverage and reliability: it separates low-level execution issues (e.g., syntax errors or broken tool calls) from higher-level semantic problems (e.g., incorrect logic or misinterpreted requirements), and includes behavioural modes such as looping or prematurely giving up. When multiple issues are present, the judge is instructed to choose the most fundamental cause that best explains why the instance ultimately remains unresolved. Table 5 summarizes the categories and the criteria used by the judge model when annotating trajectories. Table 5 Descriptions of failure mode categories. Category Description Syntax Error The patch or edits introduced syntax or formatting mistakes (e.g., missing imports, bad indentation, invalid JSON/YAML) that prevented tests or execution from succeeding. Incorrect Implementation The agent made change to reasonable area but their solution didnt correctly address the issue. Instruction Following The agent misread or deviated from the stated requirements (issue/release note), implemented something else, or ignored an explicit instruction. Tool-Use Progress was blocked by incorrect or missing tool usage (e.g., failing to run tests, bad edit/apply commands, wrong path in open/ls), so the solution could not be completed. Stuck in Loop The agent repeated similar actions (reading, scrolling, retrying edits/tests) without making substantive progress toward fix. Gave Up Prematurely The agent stopped or concluded early after encountering difficulty, without exhausting reasonable next steps. Other There was some other problem that prevented the agent from resolving this issue."
        },
        {
            "title": "Problem Statement",
            "content": "For each SWE-EVO instance, the problem statement is defined by the official release notes text that describes the changes between the start version and the end version of this instance, as published by the project maintainers. This release note is then presented to the agent as the sole natural-language specification of how the codebase should change between the two versions. 21 SWE-EVO: Benchmarking Coding Agents in Long-Horizon Software Evolution Scenarios Pull Request, Issue Context Release notes frequently refer to specific pull requests or issues (e.g., see #1234 or thanks to PR #5678). To make these references actionable for agent, we augment the problem statement with the context of the linked artifacts. For each referenced pull request or issue, we fetch its body from github. These texts are then concatenated into compact PR / Issue Context block that is presented to the agent alongside the release note. We then append these texts directly below the release note in structured format, without rewriting or summarizing them. In general, the final problem statement presented to the agent has the form: release-note followed by sequence of sections n### PR xxx:n or n### Issue yyy:n with their corresponding content for every referenced pull request and issue. so that the agent sees the original release note followed by the full text of any referenced pull requests and issues. Example To make this concrete, we present two example problem statements (Boxes 5 and 5), corresponding to the instances iterative__dvc_0.33.1_0.34.0 in the iterative/dvc repository and dask__dask_2023.6.1_2023.7.0 in the dask/dask repository. Problem Statement: iterative__dvc_0.33.1_0.34.0 1) [dvc metrics show now nicely formats multiline metrics files like tsv/htsv, csv/hcsv, json](https://github.com/iterative/dvc/issues/1716); Kudos @mroutis :medal_sports: 2) [dvc remote add no longer silently overwrites existing sections](https://github.com/ iterative/dvc/issues/1760); 3) [Use workaround to bypass SIP protection on osx, when accessing libSystem](https://github.com/iterative/dvc/issues/1515); 4) [Dont try to create existing container on azure](https://github.com/iterative/dvc/issues/ 1811); Kudos @AmitAronovitch :medal_sports: 5) Dvc repository now uses read-only http remote for our images instead of s3; 6) Fix bug in dvc status where an error is raised if cache is not present locally nor on the remote; 7) [Fix progress bar on dvc pull] (https://github.com/iterative/dvc/issues/1807); Kudos @pared :medal_sports: 8) [Automatically detect metrics file type by extension](https://github.com/iterative/dvc/issues/ 1553); Kudos @cand126 :medal_sports: Welcome new contributor @AmitAronovitch ! :tada: ### Issue 1716: When displaying TSV metrics output the printout is not readable: ../../20181223-TrainSetZero/SanityCheck/Bravo_on_TrainSetZero.metrics: value_mse deviation_mse data_set 0.421601 0.173461 train 0.67528 0.289545 testing 0.671502 0.297848 validation think it would be much easier to read if newline were added, and the remaining text were formatted as though it had been passed via column -t: ../../20181223-TrainSetZero/SanityCheck/Bravo_on_TrainSetZero.metrics: value_mse deviation_mse data_set 0.421601 0.173461 train SWE-EVO: Benchmarking Coding Agents in Long-Horizon Software Evolution Scenarios 0.67528 0.289545 testing 0.671502 0.297848 validation ### Issue 1807: (3.7.0-dvc) dvc git:(dvctags) dvc pull Preparing to download data from s3://dvc-share/dvc/ Preparing to collect status from s3://dvc-share/dvc/ [##############################] 100% Collecting information [##############################] 100% Analysing status. (1/3): [##############################] 100% dvc_up.bmp (2/3): [##############################] 100% dvc.ico (3/3): [##############################] 100% dvc_left.bmp (4/3): [##############################] 100% Checkout finished! looks like checkout didnt reset progress counter ### Issue 1553: E.g. without explicitly specifying it. if we see that output has .json suffix, we could safely assume that it is type json ### Issue 1760: The dvc remote add command ignores the existing remote and overwrites it silently. dvc version 0.32.1+7d7ed4 To reproduce dvc remote add s3 s3://bucket/subdir dvc remote add s3 s3://bucket/subdir2 Expected behavior The second command dvc remote add s3 s3://bucket/subdir2 should fail with the Remote with name \"s3\" already exists message. Current behavior Remote URL is silently overwriten: > cat .dvc/config [ remote \"s3\" ] url = s3://bucket/subdir ### Issue 1811: Azure SAS connection strings can be used to allow read-only access to specific container, which is useful in the context of CI pipelines and automation. However, when using such limited-credentials connection string, dvc pull abends with the error below. Some digging reveals that this is because remote/azure.py always tries to create the bucket (which already exists). If we check for existence before trying to create - this should work... (will send PR) [######### ] 30% Collecting information Client-Request-ID=19129d6a-5393-11e9-80ab-5800e34ea0d9 Retry policy did not allow for retry: GMT, Server-Request-ID=c72fbeda-501e-0089-3c9f-e78298000000, HTTP status code=403, Exception=This request is not authorized to perform Server-Timestamp=Sun, 31 Mar 2019 08:58:06 23 SWE-EVO: Benchmarking Coding Agents in Long-Horizon Software Evolution Scenarios ErrorCode: AuthorizationFailure <?xml version=\"1.0\" this operation. encoding=\"utf-8\"?><Error><Code>AuthorizationFailure</Code><Message>This request is not authorized to perform this operation. RequestId:c72fbeda-501e-0089-3c9f-e78298000000 Time:2019-03-31T08:58:06.2059267Z</Message></Error>. Error: failed to pull data from the cloud - This request is not authorized to perform this operation. ErrorCode: AuthorizationFailure <?xml version=\"1.0\" encoding=\"utf-8\"?><Error><Code>AuthorizationFailure </Code><Message>This request is not authorized to perform this operation. RequestId:c72fbeda-501e-0089-3c9f-e78298000000 Time:2019-03-31T08:58:06.2059267Z</Message></Error> Having any troubles? Hit us up at https://dvc.org/support, we are always happy to help! Please provide information about your setup DVC version: 0.29.0+220b4b.mod linux x86_64 py3.6 pip"
        },
        {
            "title": "APFS",
            "content": "[(3,)] ### Issue 1515: $ dvc -V 0.23.2+bad2ef.mod DVC creates hardlinks instead of reflinks in APFS. File system: $ diskutil info / grep -i system File System Personality: See last two lines: $ dvc add Tags.xml -v Debug: PRAGMA user_version; Debug: fetched: Debug: CREATE TABLE IF NOT EXISTS state (inode INTEGER PRIMARY KEY, mtime TEXT NOT NULL, size TEXT NOT NULL, md5 TEXT NOT NULL, timestamp TEXT NOT NULL) Debug: CREATE TABLE IF NOT EXISTS state_info (count INTEGER) Debug: CREATE TABLE IF NOT EXISTS link_state (path TEXT PRIMARY KEY, inode INTEGER NOT NULL, mtime TEXT NOT NULL) Debug: INSERT OR IGNORE INTO state_info (count) SELECT 0 WHERE NOT EXISTS (SELECT * FROM state_info) Debug: PRAGMA user_version = 3; Debug: Skipping copying for /Users/dmitry/src/modules-example/tmp/ test/Tags.xml, since it is not symlink or hardlink. Adding Tags.xml to .gitignore. Saving Tags.xml to cache .dvc/cache. Debug: Path /Users/dmitry/src/modules-example/tmp/test/Tags.xml inode 12888021464 Debug: SELECT * from state WHERE inode=12888021464 Debug: fetched: Debug: INSERT INTO state(inode, mtime, size, md5, timestamp) VALUES (12888021464, \"1547854167848187904\", \"0\", \"6d861a1605e60b2f3a977a5a2a4419a2\", \"1547854178609126912\") [] 24 SWE-EVO: Benchmarking Coding Agents in Long-Horizon Software Evolution Scenarios Debug: File /Users/dmitry/src/modules-example/tmp/test/.dvc/cache/6d/ 861a1605e60b2f3a977a5a2a4419a2, md5 6d861a1605e60b2f3a977a5a2a4419a2, actual None Debug: Cache type reflink is not supported: supported Debug: Created hardlink: test/.dvc/cache/6d/861a1605e60b2f3a977a5a2a4419a2 /Users/dmitry/src/ modules-example/tmp/test/Tags.xml /Users/dmitry/src/modules-example/tmp/ reflink is not Problem Statement: dask__dask_2023.6.1_2023.7.0 2023.7.0 Released on July 7, 2023 Enhancements Catch exceptions when attempting to load CLI entry points (#10380) Jacob Tomlinson Bug Fixes Fix typo in _clean_ipython_traceback (#10385) Alexander Clausen Ensure that df is immutable after from_pandas (#10383) Patrick Hoefler Warn consistently for inplace in Series.rename (#10313) Patrick Hoefler"
        },
        {
            "title": "Documentation",
            "content": "Add clarification about output shape and reshaping in rechunk documentation (#10377) Swayam Patil"
        },
        {
            "title": "Maintenance",
            "content": "Simplify astype implementation (#10393) Patrick Hoefler Fix test_first_and_last to accommodate deprecated last (#10373) James Bourbeau Add level to create_merge_tree (#10391) Patrick Hoefler Do not derive from scipy.stats.chisquare docstring (#10382) Doug Davis ### PR 10385: Regression from (#10354) https://api.github.com/repos/dask/dask/pull/10354 - exception types of unhandled exceptions in ipython contexts were mangled and always set to type. To reproduce, run the following in jupyter notebook cell with dask 2023.6.1: import dask raise TypeError(\"wat\") Then, observe the websocket connection to jupyter; the message with msg_type=\"error\" looks like this: (Image Link) Notice content[\"ename\"] is set to \"type\". With dask 2023.6.0, content[\"ename\"] is properly set to \"TypeError\": (Image Link) These values arent inconsequential - they end up being serialized into the ipynb file, and cause hard to debug issues. (nb: would prefer if this kind of issue was not something that can be caused by import dask, by opting in to this functionality by configuration or actively registering the hook functions) Tests added / passed 25 SWE-EVO: Benchmarking Coding Agents in Long-Horizon Software Evolution Scenarios Passes pre-commit run --all-files Thanks to (@matbryan52) for the help debugging this. ### PR 10383: Tests added / passed Passes pre-commit run --all-files We had similar problem in dask-expr ### PR 10393: Tests added / passed Passes pre-commit run --all-files ### PR 10391: Passes pre-commit run --all-files last is deprecated and will be removed in future ### PR 10373: This fixes FAILED dask/dataframe/tests/test_dataframe.py::test_first_and_last[last] - FutureWarning: version. Please create mask and filter using `.loc√¨nstead which were currently seeing in the upstream build (xref (#10347) (Workflow Run URL) Python 3.10 Test Summary dask/dataframe/tests/ test_multi.py::test_concat_categorical[True-False-True]: [XPASS(strict)] fails on pandas dev: https://api.github.com/repos/dask/dask/issues/10558 dask/dataframe/tests/ test_multi.py::test_concat_categorical[False-False-True]: [XPASS(strict)] fails on pandas dev: https://api.github.com/repos/dask/dask/issues/10558 Note that we were already emitting FutureWarning for DataFrame.last, so this is testsonly PR cc (@j-bennet) (@phofl) ### PR 10382: Closes (#10381) Looks like our docs are running into an error on main. See this build from recent PR https://readthedocs.org/projects/dask/builds/21146384/ Tests added / passed Passes pre-commit run --all-files scipy.stats.chisquare recently started using doi Sphinx directive that exists in their repo but its not importable. This is breaking dask docs generation. We can just point folks to that docstring instead of deriving from it. for reference here is the PR in scipy: (scipy/scipy#17682) ### PR 10377: 26 SWE-EVO: Benchmarking Coding Agents in Long-Horizon Software Evolution Scenarios Closes (#10361) It is not documented anywhere that dask.array.rechunk expects the output to have the same shape as the input and does not allow reshaping. The validation step is also quite hidden. This has led to some confusion (dask/distributed#7897 (comment)), so think it would be good to add remark about this in the documentation. Tests added / passed Passes pre-commit run --all-files ### PR 10313: The previous warning seemed very inconsistent ### PR 10380: Closes (#10379) If package registers an entry point for the CLI but the entrypoint is broken or not importable the CLI cannot work at all. This is little hard to create an MRE for but seeing this with dask-ctl when updating textual to more recent version. The error comes from the following line. (dask/dask/cli.py) Line 97 in /dask/dask/commit/85c99bc20abc382774cfb6e5bf5f2db76ac0937885c99bc command = entry_point.load() When this line is called third-party code is loaded, so we have no control over what happens. We could broadly catch exceptions here, print warning and carry on. Tests added / passed Passes pre-commit run --all-files"
        },
        {
            "title": "Dataset Fields",
            "content": "Table 6 describes the SWE-EVO dataset fields and outlines how they are obtained throughout the curation process. 27 SWE-EVO: Benchmarking Coding Agents in Long-Horizon Software Evolution Scenarios Table 6 Required fields for typical issue-solving task instance. Fields marked with * are newly added compared to SWE-bench. Field repo base_commit start_version end_version end_version_commit patch test_patch Type str str str str str str str problem_statement str FAIL_TO_PASS List[str] PASS_TO_PASS List[str] *image str Description Git repository identifier for the task instance, e.g., the GitHub owner/name. The commit on which the pull request is based, representing the repository state before the issue is resolved. Git tag or version identifier of the starting release for this task instance. Git tag or version identifier of the target release after the change has been applied. Git commit hash corresponding to the end_version tag. Gold patch proposed by the pull request, in .diff format. Modifications to the test suite proposed by the pull request that are typically used to check whether the issue has been resolved. Issue description text, typically describing the bug or requested feature, used as the task problem statement. Test cases that are expected to successfully transition from failing to passing and are used to evaluate the correctness of the patch. Test cases that are already passing prior to applying the gold patch; correct patch should not introduce regression failures in these tests. Instance-level Docker image that provides an execution environment. *test_cmds *log_parser List[str] Command(s) used to run the test suite, as identified by the verify agent in RepoLaunch, enabling detailed logging of each test items status (e.g., via pytest -rA). Type of log parser required for the instanceby default, pytest. str"
        }
    ],
    "affiliations": [
        "FPT Software AI Center",
        "School of Computing and Information Systems - University of Melbourne"
    ]
}