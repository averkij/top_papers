{
    "paper_title": "LatticeWorld: A Multimodal Large Language Model-Empowered Framework for Interactive Complex World Generation",
    "authors": [
        "Yinglin Duan",
        "Zhengxia Zou",
        "Tongwei Gu",
        "Wei Jia",
        "Zhan Zhao",
        "Luyi Xu",
        "Xinzhu Liu",
        "Hao Jiang",
        "Kang Chen",
        "Shuang Qiu"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Recent research has been increasingly focusing on developing 3D world models that simulate complex real-world scenarios. World models have found broad applications across various domains, including embodied AI, autonomous driving, entertainment, etc. A more realistic simulation with accurate physics will effectively narrow the sim-to-real gap and allow us to gather rich information about the real world conveniently. While traditional manual modeling has enabled the creation of virtual 3D scenes, modern approaches have leveraged advanced machine learning algorithms for 3D world generation, with most recent advances focusing on generative methods that can create virtual worlds based on user instructions. This work explores such a research direction by proposing LatticeWorld, a simple yet effective 3D world generation framework that streamlines the industrial production pipeline of 3D environments. LatticeWorld leverages lightweight LLMs (LLaMA-2-7B) alongside the industry-grade rendering engine (e.g., Unreal Engine 5) to generate a dynamic environment. Our proposed framework accepts textual descriptions and visual instructions as multimodal inputs and creates large-scale 3D interactive worlds with dynamic agents, featuring competitive multi-agent interaction, high-fidelity physics simulation, and real-time rendering. We conduct comprehensive experiments to evaluate LatticeWorld, showing that it achieves superior accuracy in scene layout generation and visual fidelity. Moreover, LatticeWorld achieves over a $90\\times$ increase in industrial production efficiency while maintaining high creative quality compared with traditional manual production methods. Our demo video is available at https://youtu.be/8VWZXpERR18"
        },
        {
            "title": "Start",
            "content": "LatticeWorld: Multimodal Large Language Model-Empowered Framework for Interactive Complex World Generation Yinglin Duan1 Zhengxia Zou2 Tongwei Gu3 Wei Jia4 Zhan Zhao5 Luyi Xu6 Xinzhu Liu7 Hao Jiang8 Kang Chen9 Shuang Qiu10 September 8, 2025 Abstract Recent research has been increasingly focusing on developing 3D world models that simulate complex real-world scenarios. World models have found broad applications across various domains, including embodied AI, autonomous driving, entertainment, etc. more realistic simulation with accurate physics will effectively narrow the sim-to-real gap and allow us to gather rich information about the real world conveniently. While traditional manual modeling has enabled the creation of virtual 3D scenes, modern approaches have leveraged advanced machine learning algorithms for 3D world generation, with most recent advances focusing on generative methods that can create virtual worlds based on user instructions. This work explores such research direction by proposing LatticeWorld, simple yet effective 3D world generation framework that streamlines the industrial production pipeline of 3D environments. LatticeWorld leverages lightweight LLMs (LLaMA-2-7B) alongside the industry-grade rendering engine (e.g., Unreal Engine 5) to generate dynamic environment. Our proposed framework accepts textual descriptions and visual instructions as multimodal inputs and creates large-scale 3D interactive worlds with dynamic agents, featuring competitive multi-agent interaction, high-fidelity physics simulation, and real-time rendering. We conduct comprehensive experiments to evaluate LatticeWorld, showing that it achieves superior accuracy in scene layout generation and visual fidelity. Moreover, LatticeWorld achieves over 90 increase in industrial production efficiency while maintaining high creative quality compared with traditional manual production methods. Our demo video is available at https://youtu.be/8VWZXpERR"
        },
        {
            "title": "1 Introduction",
            "content": "The development of interactive world models has emerged as crucial research area in recent studies [64, 17, 34]. These virtual environments aim to simulate real-world complex scenarios, enabling researchers to gather rich observations through extensive and meaningful interactions. Given that obtaining real-world data is typically challenging and expensive, the abundant data generated by world models can be leveraged to train various artificial intelligence (AI) algorithms [52, 72], especially those requiring high sample complexity. Furthermore, world models are particularly valuable for learning problems that involve safety and risk considerations. World models have been widely applied across numerous applications, including 5 2 0 2 5 ] . [ 1 3 6 2 5 0 . 9 0 5 2 : r Equal Contribution. Corresponding Authors. 1NetEase, Inc., China. duanyinglin@corp.netease.com 2Beihang University, China. zhengxiazou@buaa.edu.cn 3NetEase, Inc., China. gutongwei@corp.netease.com 4NetEase, Inc., China. jiawei@corp.netease.com 5NetEase, Inc., China. zhaozhan03@corp.netease.com 6Work done while at NetEase, Inc., China. lucxu@vip.163.com 7Tsinghua University, China. liuxz cs@163.com 8NetEase, Inc., China. jianghao06@corp.netease.com 9NetEase, Inc., China. ckn6763@corp.netease.com 10City University of Hong Kong, China. shuanqiu@cityu.edu.hk 1 Figure 1: Demonstration of generated results via LatticeWorld embodied artificial intelligence, sequential decision-making, autonomous driving, entertainment (e.g., game development, movie production), etc. In essence, world models will enable intelligent systems to acquire spatial intelligence through extensive interactions. The empirical significance of world models necessitates developing high-fidelity 3D virtual environments, aiming to minimize the sim-to-real gap between simulation and reality. By doing so, we can generate rich data that better aligns with real-world samples, significantly mitigating the learning errors that arise from the sim-to-real gap. The development of such high-fidelity virtual environments fundamentally requires the study of 3D scene generation techniques. Early efforts in this field began with basic manual scene modeling, primarily in applications like game development, which largely relied on manual crafting by artists with high labor costs. To enhance production efficiency, procedural content generation (PCG) has become widely used in computer graphics for the automatic creation of 3D virtual environments through algorithms and predefined rules [58, 4, 15, 3, 11, 66]. With the advancement of deep learning, there has been extensive research on integrating PCG with deep neural networks [44, 23, 31]. Recently, researchers are particularly focused on using generative models to create content based on user instructions. Neural rendering methods represent an important line of research, such as Score Distillation Sampling with Neural Radiance Fields [36, 38, 54, 19, 33, 71, 13, 69] and 3D Gaussian Splatting [22, 68, 7, 74, 25]. However, these approaches lack interactive capabilities, which constrains their practical applications. Another line of research, developed based on diffusion models [14, 26, 17, 34, 9], provides vision-based (e.g., images or videos) solutions for scene generation. Moreover, there have been many works focusing on integrating generative models with 3D modeling platforms[55, 73, 20, 10, 52, 65, 67], especially Blender. Our work advances this field by proposing novel framework named LatticeWorld, multimodal large language model-empowered framework for interactive complex world generation. Drawing inspiration from standard computer graphics (CG) solutions in industry, our method integrates with industrial PCG production pipelines. LatticeWorld is simple yet effective framework that seamlessly integrates multimodal LLM with an industry-grade CG rendering engine, the Unreal Engine (UE), distinguishing it from prior works. Compared with Blender, Unreal Engine offers more realistic physics simulation, native multi-agent interaction capabilities, and real-time rendering optimized for interactive experiences. Hence, LatticeWorld inherits UEs distinctive advantages and extended functionality through established plugins. Specifically, LatticeWorld accepts both textual descriptions of the virtual world and visual instructions for terrain elevation (such as height maps or sketches) as inputs. Then, leveraging LLMs capabilities in symbolic understanding and structured sequence generation, the well-trained multimodal LLMs in our framework generate carefully defined symbolic representation (matrix) of the scene layout and extract semantically clear configurations of the environment from the input, showcasing excellent interpretability and semantic precision. The rendering engine processes these generated results alongside the visual information to create large-scale dynamic 2 virtual world populated with multiple interactive agents. The visual condition can guarantee direct and efficient understanding of elevation in the scene by the LLM, thus leading to accurate generation. Our framework eventually generates playable virtual world, where users can control the main character to interact with other agents exhibiting adversarial behaviors. Thus, LatticeWorld has the capability of creating competitive environment based on user instructions for AI agent training. Notably, LatticeWorlds multimodal LLMs are built upon the lightweight LLaMA-2-7B model [59], demonstrating the potential for achieving sophisticated spatial understanding using smaller-scale LLMs. We summarize the main contributions of our work as follows: We propose simple yet effective 3D complex world generation framework, LatticeWorld, by exploring lightweight LLMs abilities in spatial understanding and structured sequence generation. LatticeWorld leverages multimodal LLMs alongside the industry-grade rendering engine, UE, to create dynamic environment, featuring several key advantages: (1) multimodal input, (2) interpretable intermediate representation, (3) realistic physics modeling, (4) dynamic multi-agent interaction, (5) real-time largescale simulation. Our method is general and can be adapted to other powerful engines such as Unity. To train the multimodal LLMs in our framework, we propose to construct multimodal datasets that incorporate diverse textual descriptions, height maps, symbolic layout representations, and corresponding environment configurations. During the dataset construction process, we leverage GPT-4o for data annotation along with sophisticated prompt engineering, ensuring both annotation efficiency and accuracy. We conduct comprehensive experiments to evaluate LatticeWorlds performance, comparing it with existing methods in both layout generation and final environment generation. Our results demonstrate that LatticeWorld achieves superior performance in terms of generation accuracy and visual fidelity across various users instructions. Additionally, compared with traditional manual production methods in industry, LatticeWorld can achieve over 90 increase in industrial production efficiency while maintaining high creative quality."
        },
        {
            "title": "2 Related Work",
            "content": "Procedural Content Generation. PCG frameworks automate the creation of assets and environments through algorithmic approaches [58], traditionally relying on rule-based systems and parametric models to generate diverse content such as terrains, levels, and gameplay elements [12, 15, 3, 66, 11, 4]. These approaches have been widely adopted in industrial pipelines, particularly in game development and virtual environment creation, due to their efficiency in generating large-scale content with controlled variability. Furthermore, many works have explored integrating PCG with AI methods [31, 44, 23] and 3D scene generation [55, 49, 20, 73, 30]. Neural Rendering for 3D Scene Generation. The field has evolved from Score Distillation Sampling with Neural Radiance Fields (NeRFs) [36, 38, 54, 19, 33, 71, 13, 69] to feed-forward architectures [26, 14]. Recent developments in 3D Gaussian Splatting [22, 68, 7, 74, 25] have enhanced generation efficiency and scene coherence. While these neural rendering approaches excel in visual fidelity, they primarily focus on static content generation and usually lack interactive capabilities, limiting their application in dynamic world model scenarios. Vision-Based Interactive World Generation. Diffusion-based approaches have established frameworks for interactive 3D environment creation [14, 26, 9]. Recent works like Genie-2 [17], WorldLabs [64], and GenEx [34] use diffusion models with image/video inputs to create explorable environments. These methods achieve interactivity through visual prediction but are constrained by the limitations of vision-based simulation. Platform-Based Environment Creation. In addition to the above methods, recent efforts have focused on integrating generative capabilities with established 3D content creation platforms, each offering distinct computational paradigms and interaction capabilities. Recent works have demonstrated integration with Blender [55, 73, 20, 16, 10, 52, 65, 67], leveraging its comprehensive modeling capabilities and Python 3 scripting ecosystem. These methods primarily emphasize content creation and offline rendering rather than real-time interaction. Although Blender supports real-time rendering through Eevee and physics simulation, these capabilities remain limited for dynamic multi-agent scenarios, and its typical workflow involves content creation followed by export to other platforms, resulting in separation between generation and interaction phases. Additionally, more specialized simulation platforms like Nvidia Isaac Sim have been utilized for robot interaction tasks [51], while simulation environments have evolved from symbolic reasoning [45, 53] to sophisticated physics-based platforms [56, 24, 28, 46, 39]. However, these approaches typically leverage static content rather than enabling dynamic content generation that responds to agent interactions in realtime, such as trees falling due to collision impact. Our approach leverages Unreal Engine, which offers fundamentally different paradigm optimized for real-time interactive applications. Unlike Blender, Unreal Engine offers advanced physics simulation, native multi-agent interaction capabilities, and real-time rendering optimized for interactive experiences. This allows us to integrate LLM generative capabilities with responsive virtual environments that support real-time scene modification and agent interaction, making our approach suitable for both creative applications and AI agent training. Large Language Models. LLMs have demonstrated capabilities in understanding textual instructions, as exemplified by GPT series [6, 41], LLaMA family [59, 35], Mistral [21], and task-specific models like DeepSeek-R1 [8]. While these models primarily process unimodal text inputs, recent advances expand their versatility through multimodal integration (e.g., GPT-4 variants [40, 42], Qwen2-VL[62], Claude 3 [2], Gemini [57]). The emergence of instruction tuning [48, 63, 43] has empowered LLMs to generalize across unseen tasks through textual instruction comprehension. To extend LLMs capabilities to visual inputs, frameworks such as Flamingo [1] (cross-modal attention gates) and LLaMA-Adapter [70] (learnable visual projectors) preserve frozen language models while aligning visual features (e.g., CLIP [47, 50]) with text embeddings through lightweight modules. End-to-end approaches like LLaVA [29] demonstrate that joint vision-language instruction training can unlock deeper multimodal reasoning."
        },
        {
            "title": "3 LatticeWorld Framework",
            "content": "LatticeWorld aims to generate customizable 3D virtual world controlled by users multimodal instructions, including textual and visual descriptions of the desired environment. To achieve this goal, LatticeWorld encompasses multimodal scene layout generator, an environmental configuration generator, and rendering pipeline, as illustrated in Figure 2. Specifically, LatticeWorld takes in the multimodal layout control instructions, comprising textual description xL and vision condition vL that contains 3D spatial information, and the text description xC of environmental configurations as user-specified inputs. In our work, we refer to the vision condition vL as height maps, which can be converted from hand-drawn sketches of the terrain using specific algorithms. Then, LatticeWorld produces 3D virtual world by first employing two well-tuned foundation models, i.e., LLML and LLMC, for the scene layout and environmental configuration generation respectively, that are aligned with users inputs, and further conducting 3D rendering based on such generated layout and rendering parameters. More formally, we formulate such generating process as follows: (cid:0)xL, Φ(vL)(cid:1) (cid:0)xC, Φ(vL), ˆyL ˆyL = LLML ˆyC = LLMC = Render(cid:0)ΨL(ˆyL), ΨC(ˆyC), vL (cid:1) (cid:1), (1) (2) (3) where Φ is the vision-to-word embedding operator that embeds the vision information into the target language space. Here ΨL is decoder to interpret the intermediate layout representations generated by LLML into an engine-readable scene layout in tensor form, ˆyL denotes such generated intermediate layout representation. In addition, ˆyC is the generated environmental configuration, and ΨC represents configuration translation process to interpret the generated configurations into engine native properties. Here, ΨC can be implemented either through developing translating scripts or using specialized plugins developed based on software like Houdini. Render denotes the 3D rendering engine for executing the rendering pipeline. It is worth noting that the vision condition vL can be optional and is not always required, thus offering flexibility in the application of LatticeWorld. Eventually, we create playable virtual world, where the main agent can be controlled to 4 Figure 2: Technical framework of LatticeWorld interact with other virtual agents. The main agents actions are currently controlled through input devices, but this can be easily expanded to AI algorithmic policies by using existing UE plug-ins. Our primary focus is on the methodology for creating virtual worlds, while the engineering aspects can be addressed in future development. Next, we will delve deeper into the three procedures depicted by (1), (2), and (3) in Sections 4, 5, 5.2, and 6."
        },
        {
            "title": "4 Scene Layout Generation",
            "content": "In this section, we aim to train scene layout generation model with multimodal inputs of linguistic descriptions and visual conditions integrating LLMs by exploiting and developing LLMs capabilities of spatial comprehension and symbol sequence generation. Recall that the generation of scene layout is formulated as ˆyL = LLML(xL, Φ(vL)) in (1), which has the textual description xL and visual condition vL as inputs and produces layout representation in specific structure. We target to train Φ and LLML to obtain such scene layout generative model. Following the same input/output data formats for LLML, the training dataset Dtr includes the layout representation yL, which is transformed from its original layout image L. We remark that such an intermediate layout representation is crucial for efficient training and can be decoded into an engine-readable layout tensor via ΨL as in (3), eventually resulting in realistic scene through rendering engine. Next, we provide the details of the layout representation."
        },
        {
            "title": "4.1 Sequential Symbolic Representation of Layout",
            "content": "In our problem, scene layout is an image in which different types of assets, characterized by distinct colors, are distributed. Vision-language foundation models like Stable Diffusion have demonstrated remarkable capabilities in visual generation tasks, which may have the potential to be employed for producing layout images directly from textual and visual instructions. However, such models may have limitations of uncontrollable layout generation. We would like to emphasize that our work focuses on developing generative 5 Table 1: Symbolic Representations of Asset Categories Symbol LoveDA Dataset F W Farmlands Buildings Barren lands Grasslands Roads Wild Dataset Low bushes or grasslands Forests Rocky lands Snow-capped regions Water (lakes, rivers, etc.) framework designed to efficiently leverage variety of foundation models as base, even those with merely text generation abilities, rather than concentrating on enhancing vision-language foundation models. To tackle the above challenges, we propose an effective intermediate representation scheme to encode layout image, making it easier for an LLM to incorporate the layouts information via fine-tuning. Specifically, we first compress scene layout image into symbol (letter) matrix (this work simply lets = 32), where each letter symbolizes certain type of asset. detailed delineation of such symbolic representations for various datasets is presented in Table 1. Such symbolic matrix is further converted into string yL that LLM can efficiently process: yL = s1 1s2 1 sp 1 s1 2s2 2 sp 2n s1 ps2 sp , where represents line break and sj corresponds to the entry in the i-th row and j-th column of the matrix. This allows for the conversion of the layout into fixed-length layout symbolic language and embeds spatial information within symbol sequence. Thus, our proposed layout representation can be directly applied to LLMs even with merely text generation abilities, exhibiting its generalizability to various foundation models. This layout representation can effectively capture intricate information within the layout, such as the positions and sizes of various asset regions as well as their spatial relationships. For instance, two consecutive letters indicate the adjacency of two regions, and each letters position corresponds to the respective regions location in the layout. In favorable format that is readily processable by language models, this sequential symbolic representation can adequately exploit LLMs strong abilities of sequence understanding and reasoning, enabling the model to generate logical layout from textual and visual descriptions. We remark that an annotation procedure for labeling objects in the training dataset is necessary to guarantee controllable generation of the layout. Our proposed symbolic representation is equivalent to annotating different types of areas in the layout image. Many existing multimodal foundation models, such as LLM-grounded Diffusion [27] and LLaVA, rely on bounding boxes with language descriptions to localize objects in the image. However, the assets in layout can have particularly irregular shapes, making the bounding box annotation method impractical and inaccurate. Our approach enjoys the flexibility in labeling the regions types and delineating their shapes. Our method can treat each element in the symbolic matrix as mini-patch, aligning well with the rendering mechanism in trending engines like UE, where the scene and the terrain are constructed in grid-based manner."
        },
        {
            "title": "4.2 Text-to-Layout for Fixed-Height Scene",
            "content": "We now consider the method for generating layout representations of scenes with minimal terrain variations, i.e., fixed-height scenes, as in the LoveDA dataset that will be used for our experiment. In the later subsection, we further demonstrate that by incorporating visual instructions, variable-height scenes can be further generated. Specifically, with the model LLML well trained via instruction tuning, once given layout instruction xL, the associated symbolic sequence is generated by as in (1) without the input of the visual instruction vL. ˆyL = LLML(xL)"
        },
        {
            "title": "4.3 Fusing Vision for Variable-Height Scene",
            "content": "In the prior subsection, we simplify the scene generation task by fixing height variations and limiting the user instruction to text. To fully explore LLMs potential in understanding structured 3D spatial information for layout generation, we need to further encode height information into the model. To address such challenge, we propose multimodal method that can incorporate visual instructions such as the height map into the model, e.g., the terrain height variation can be represented by grayscale image. This multimodal approach imposes stricter constraints on the layout generation, ensuring more realistic and coherent scene compositions (e.g., snow is on mountaintops rather than lakes). In height map, pixels position can represent the coordinate (latitude and longitude) in reality, while its value indicates the height, which thus can depict spatial height variations. This ensures strict correspondence between layout and its height variation. In CG industrial scene production, height maps are traditionally crafted by artists using professional software such as World Machine. In view of this problem, beyond the direct height map input, our model allows sketch drawing as visual instruction. Then we develop translation model that converts sketch drawings into height maps, trained on strictly corresponding pairs of sketches and height maps. Hence, it simplifies the height map creation process, and users only need to provide simple sketch to create detailed, high-resolution height map. Inspired by the recent work [18], our approach leverages Pix2PixHD model, tailored to our specific task, to implement GAN-based sketch-to-heightmap generative model. It utilizes two different colors of lines to control the height map generation: blue strokes indicate low-lying areas, while black strokes represent ridges. Visual Information Integration. Our multimodal approach integrates an LLM with sophisticated visual module for encoding visual information, as illustrated in Figure 2. Recall in (1), given visual instruction vL, we obtain symbolic layout as ˆyL = LLML(xL, Φ(vL)), where Φ consists of visual feature encoder and projection module that maps visual information associated with vL into the LLMs language word embedding space. At the core of our visual module is the CLIP visual encoder (ViT-B/32), denoted as ϕ, which generates patch-wise visual features ϕ(vL) for each image vL by extracting the output of the penultimate transformer layer. To map visual features into the word embedding space, we employ lightweight CNN-based projection network. Specifically, we apply this trainable projection network to convert the patch-wise visual features ϕ(vL) into language embedding tokens Φ(vL), which have the same dimensionality as the word embedding space in the LLM: Φ(vL) := Proj(cid:0)ϕ(vL)(cid:1). This projection enables seamless processing of visual features alongside texts in the LLMs transformer layers. Our embedding scheme, conceptually inspired by LLaVA [29], demonstrates computational efficiency while effectively transforming visual information into format that aligns with the LLMs architecture, facilitating joint understanding of visual and textual inputs."
        },
        {
            "title": "4.4 Model Training",
            "content": "We fine-tune the model LLML for scene generation via constructing the training data Dtr scene and variable-height scene generation respectively. for fixed-height For the fixed-height scene generation task, the training data Dtr is set of data pairs (xL, yL). In our work, we utilize GPT-4o as sophisticated annotator, generating the textual description xL that depicts the scene from various perspectives. We design series of effective prompts incorporated into xL, which interprets the symbols in yL into real-world concepts, e.g., W: Water bodies like lakes, rivers, etc. Our symbolic layout representation enables direct mapping of contextual relationships from the serialized symbol matrix to spatial relationships. The textual description xL consists of system prompt xsys defining the describing the scene, i.e., xL = (xsys task and user instruction xins ). In particular, the system prompt xsys is in the following format: You are an AI assistant. Your task is to create 32x32 letter matrix based on the given input. Each line of the matrix should end with n. The meaning and distribution of letters in the matrix should accurately reflect the content and visual encoding information provided. The letters in , xins the matrix represent different geographical features: S: Snow-capped mountains or snowy areas, R: Rocky areas or land with many rocks, ...... Based on our curated layout dataset Dtr , we fine-tune the model LLaMA-2-7B using the cross-entropy loss (supervised fine-tuning). For the variable-height scene generation task, we conduct fine-tuning of the model integrating visual information, using our vision-language scene training datasets Dtr that are constructed from our proposed Wild dataset, with substantial data augmentation and annotations. As the visual information is now introduced into the framework, we construct more intricate training dataset Dtr than that in Section 4.2. In particular, we define Dtr as set of data tuples (xL, yL, vL, cv), where xL and yL are the layout description and layout symbolic representation as in Section 4.2, vL is the visual instruction (e.g., height map), and cv is the caption of vL (e.g., terrain description of height map). Based on the training dataset, we propose three-stage training scheme for the layout generation with the incorporation of the visual instructions: 1. CLIP fine-tuning for terrain understanding. To enable the model to extract fine-grained height variation features, we fine-tuned CLIP using our annotated data of height maps vL (grayscale images encoding terrain elevation) paired with corresponding captions cv, resulting in an effective visual feature extractor for height map. 2. Continual pre-training for feature alignment. In this stage, we keep both the visual encoder in the CLIP model and LLML weights frozen, focusing on continual pre-training of the projection module Proj. We continue with the height map caption data pairs (vL, cv) used in CLIP fine-tuning. The training of Proj leverages single-turn conversations, where each sample consists of CLIP-extracted visual features ϕ(vL), task-specific instruction prompt xv ins, and the corresponding ground-truth image caption cv. Keeping LLML parameters frozen, we train Proj to align visual features with language representations by minimizing the discrepancy between model predictions and ground-truth captions cv. Specifically, Proj projects CLIP features into language word tokens, which are then concatenated with tokenized xv ins - prompt designed to elicit concise terrain height descriptions - so that we have the model prediction as LLML(concatenate(Proj(ϕ(vL)), Tokenize(xv ins))). This can establish robust bridge between visual and linguistic representations through the learned Proj. 3. End-to-end fine-tuning. The final stage involves end-to-end training using the data tuples (xL, vL, yL). In this phase, we freeze the visual encoder weights in the CLIP model and fine-tune the pre-trained projection module Proj and the layout foundation model LLML (LLaMA-2-7B), enabling more targeted adaptation of both components while preserving the robust visual understanding. With the above three training stages, we eventually obtain layout generator with textual and visual instruction inputs."
        },
        {
            "title": "5 Environmental Configuration Generation",
            "content": "Inspired by the industrial PCG production pipeline, we next concentrate on setting the environmental configurations, crucial step following the establishment of scene layouts. Since LatticeWorld aims to construct dynamic environment with interactive agents, our environmental configuration involves two critical aspects: Scene attributes: visual characteristics and spatial arrangement of assets in the scene; Agent parameters: comprehensive agent settings, including their categories, appearance, spatial positions, and behaviors. However, the high complexity and diversity of the environmental configuration incur significant challenges for directly setting them manually, particularly for non-professionals. There may be up to thousands of parameter combinations controlling specific visual effects for single virtual environment. Moreover, users must not only comprehend the meaning of each parameter but also possess artistic sensibility to achieve In our framework, we simplify such process via an environmental optimal effects via manual editing. configuration generative model with paragraph of natural language description and an image as inputs, as we have described in (2), i.e., ˆyC = LLMC (cid:0)xC, Φ(vL), ˆyL (cid:1), 8 Table 2: Discrete (seasonal & material) and continuous parameters per asset type Asset Type Grass Flower Dead Branch Stone Architecture Road Lake Desert Forest Crops Snow Mountain Height Map Total Discrete # Seasonal Params 3 2 2 3 4 2 2 6 18 4 # Material Params 4 4 4 4 3 2 2 3 4 4 3 Continuous Params D, D, D, D, R, D, D, R, D, D, D, R, Sl HM D=Density, R=Rotation, S=Scale, H=Height, W=Wind, Sl=Slope, HM=Height Map (Continuous). N/A 37 49 where xC is the textual description of environmental configurations (including both scene attributes and agent parameters), vL is the visual condition (e.g., height maps), ˆyL is the generated symbolic layout representation from the layout LLM, and ˆyC is the configuration generation. Moreover, Φ denotes the vision-to-word embedding operator that has been well trained in Section 4.3. Note that vL and ˆyL are two crucial components in LLMC as environmental configurations can be constrained by the scene layout and terrain elevation, which will be elaborated in Section 7. For example, an aquatic creature should not appear in mountainous terrain. If we consider fixed-height environment generation, as discussed in Section 4.2, then the visual embedding module Φ(vL) can be removed such that the generation process reduces to ˆyC = LLMC (cid:0)xC, ˆyL (cid:1)."
        },
        {
            "title": "5.1 Scene Attributes",
            "content": "Due to the vast number of scene attributes, directly modeling and mapping them to textual descriptions is challenging and may lead to conflicting generations. To address the above issues, we model the scene attributes in hierarchical structure that will facilitate an organized translation of languages to attributes, ultimately driving procedural scene detail control and rendering. At the top level are coarse attributes, controlling the global setups of the scene, such as the season, the weather, and others. The bottom level consists of fine attributes, which further provide detailed characterization of the generated scene depending on those coarse attributes. To systematically implement our hierarchical scene attribute approach, we develop comprehensive attribute transformation framework that formalizes the relationships between different abstraction levels and guides the generation process. Our data generation framework establishes mapping from natural language descriptions to environmental configurations through hierarchical attribute system inspired by industrial PCG workflows: Coarse Attribute. The coarse attribute controls the global and general features of the entire environment, encompassing five global aspects of scenes: terrain type, season, artistic style, weather conditions, and time of day. These five aspects are associated with specific values for control. For instance, seasons are represented by spring, summer, autumn, and winter. Fine Attribute. There are many types of fine attributes, including discrete parameters (e.g., seasonal and material parameters) and continuous parameters (e.g., density [0, 1]). Continuous parameters may be sampled from specific ranges . For example, rotations can be represented as three-dimensional Euler angles (pitch, yaw, roll in [0, 360)). Height maps (HM) represent continuous surface geometry but are discretized for storage (typically with values in [0, 65535]) and generated using GAN-based methods rather than direct numerical sampling. Intrinsically, these scenes are controlled by coarse scene attributes, the scenes height map characteristics, and layout distribution constraints while adhering to common sense constraints. They 9 also incorporate partial randomization within PCG rules. Table 2 shows different types of assets and the numbers of values for fine attributes, which indicates that in different seasons, asset types have different contents and different surface materials. Another type of fine attributes corresponds to the asset placement, including density, orientation, position, etc. We maintain certain degree of randomness while having certain rule restrictions on those attributes. Coarse-to-Fine Transformation. To integrate the coarse-to-fine generation concept into the language model, we construct the training data in hierarchical manner for model fine-tuning, which necessitates coarse-to-fine transformation approach. (Dataset details are presented in Section 7.2.) We implement rule-based mapping system where coarse attributes define valid ranges and distributions for fine attributes, following industrial PCG principles of hierarchical control. For example, selecting winter as the season automatically adjusts vegetation density parameters downward, restricts available foliage types, and modifies terrain material parameters to include snow coverage. This hierarchical approach can ensure semantic consistency of generation, reduce parameter conflicts in space containing hundreds of interrelated variables. It also aligns with the workflow of professional artists, who typically begin with high-level scene design before refining finer details. Then we achieve more manageable generation pipeline that can handle complex environments."
        },
        {
            "title": "5.2 Agent Parameters",
            "content": "As discussed so far, the environments created by our method consist of static objects. Next, we investigate building dynamic environment via incorporating interactive agents/characters into the generated scene. Then the created environment has the potential of being developed into model training platform for embodied AI. Those agents can interact with the main player (main agent) through various dynamic behaviors, including pursuit and combat, creating competitive platform for studying multi-agent decision-making. Specifically, the agent parameters consist of four aspects, including agents categories (such as the goblin, humanoid robot, robotic dog, ancient warrior, etc), quantities, states (such as idle, patrolling, swimming, etc), and spatial positions (such as upper left, lower left, etc), which would allow us to manage the appearance, actions, and interaction strategies of those agents. In general, the agent parameters are influenced by various scene attributes. For example, whale should only live in large water bodies. Thus, given xC, which contains textual description of agent parameters, as well as the generated scene layout ˆyL and the visual condition vL, the corresponding agent parameters for agents categories, states, and spatial positions are created via LLMC. Our generative method enables users to configure and control complex dynamic agents through simple textual and visual instructions, providing more intuitive and flexible agent control approach for industrial-level dynamic environment design and embodied policy training."
        },
        {
            "title": "5.3 Model Training",
            "content": "For training the model LLMC, we construct the training dataset Dtr where xC is comprised of the system task definition xsys and the environmental configuration description texts xins , yC consists of the scene attributes and agent parameters in JSON format, and vL and yL are the corresponding height maps and symbolic layout representations respectively. During the construction of the training dataset as in Section 7, we constrain both the state and position sets of different agent types as well as the scene attributes based on captions of layouts and terrain elevation for vL and yL. This ensures that both our training data and the resulting outputs accurately align with real-world conditions. using set of data tuples (xC, Φ(vL), yL, yC) We train the model to follow specific instructions depicting the key attributes of the scene and the status of agents. We employ LLaMA-2-7B as the base LLM model for our task. Moreover, we apply the visual embedding operator Φ that is well trained in Section 4.3 for projecting the visual condition into the language word tokens. The model training involves fine-tuning the model using the cross-entropy loss and our curated environmental configuration dataset Dtr specifically tailored for this task. Eventually, given textual description of the coarse scene attributes along with agent parameters as well as the height map and the generated layout symbolic representation, the environmental configurations will be generated via LLMC."
        },
        {
            "title": "6 Procedural Rendering Pipeline",
            "content": "LatticeWorld is an industry-level world model generation framework that includes complete rendering process using standard PCG pipelines. This would enable rapid migration to various rendering engines and graphics systems to build industrial-grade scenes. For our task, once the symbolic layout representation ˆyL and environmental configurations ˆyC are generated via LLML and LLMC, as shown in (3), the rendering process can be represented by = Render(ΨL(ˆyL), ΨC(ˆyC), vL), where vL is the visual instructions (e.g., height map) for layout generation, is the generated scene after rendering, and Render represents rendering engine, e.g., the UE considered in this work. Here, ΨL and ΨC serve as the visual decoder and configuration translator, respectively. Notably, our framework can be adapted to various rendering engines, including both industry-grade engines (such as Unity and UE) and non-commercial/open-source engines (such as Blender and Three.js), by only implementing different versions of ΨL and ΨC tailored to the specific input format of each engine. However, due to the inherent limitations of some engines, particularly non-industrial ones, we have chosen to use UE in our work, leveraging its advanced functionalities. Layout for Rendering. Once the symbolic representation of layout is generated by LLML, we employ the decoder ΨL to translate such symbolic layout image into the format that the rendering engine can read. In our work, we design simple yet effective mapping method to implement such decoder ΨL in three major steps: 1. Layout binary mask creation. We convert the symbolic layout (p = 32 in this paper) into low-resolution image, where each character corresponds to pre-defined RGB color pixel. Then, we create binary (black and white) masks for each color on the RGB image, showing the presence and absence of certain scene types at each pixel. 2. Stretching and edge blending. We stretch those binary masks for each color to the desired size via nearest-neighbor interpolation. Moreover, for smooth and natural transition of different types of scenes at their edges, we implemented noise-based edge blending technique, e.g., Gaussian blur, for edge processing such that the binary masks are converted into grayscale images. 3. Engine processing. The rendering engine will understand the scene layout via reading those smoothed mask images for visualization, each corresponding to distinct scene type. In regions where multiple scene types overlap, the engine can automatically integrate these overlapping elements via sophisticated blending algorithm, ensuring natural visual representation. Environmental Configurations for Rendering. Scene layouts merely express the distribution of scene assets, while the engine also requires necessary configurations to characterize the environment, the assets, and the dynamic agents. The environmental configurations, including the scene attributes and the agent parameters, are generated via LLMC, which are further translated into engine native properties via the translation process ΨC. For instance, our weather system leverages the Niagara Fluids plugin [5, 37] to implement various weather effects according to the generated parameters, such as sandstorms in desert scenes and snowfall in mountain scenes. In other specific implementations, we incorporated various components from UE such as Volumetric Cloud, Volume Fog, SkyBox, SkyAtmosphere, and others. The environmental configurations are associated with several types of properties: (1) The density and material types of objects. The environmental configurations can control, for example, vegetation, grasslands, and rock formations across different regions, determining their distribution, diversity, appearances, and visual characteristics. We implement various rules to map these configurations to engine properties to control the density and materials of different objects. (2) The placement of buildings. The placement of buildings requires more nuanced rulebased approach compared to natural elements like grasslands. This process demands consideration of terrain types, height maps, and directional requirements. To address this challenge, we propose building-aware rules to determine building types, positions, and orientations. For example, to enhance realism, we introduce controlled random variations to building orientations and define maximum and minimum distances between 11 buildings. (3) The configuration of dynamic agents. The environmental configurations can characterize models, states, initial orientation and distribution, and quantities of the dynamic agents in the rendering engine. Eventually, ΨC can be implemented either through developing translating scripts or using specialized plugins developed based on software like Houdini. Visual Instruction for Rendering. The visual instruction vL also serves as input to the rendering engine, typically consisting of either scene height map or sketch that is convertible to height map, both of which encode terrain elevation information. Finally, the rendering engine creates the scene by combining three inputs: the visual information, the generated symbolic layout, and environmental configurations derived through an automated pipeline."
        },
        {
            "title": "7 Dataset Construction",
            "content": "Our framework builds upon well-curated multimodal dataset, which represents another core contribution of this work. To the best of our knowledge, existing multimodal methods for scene generation are incompatible with our proposed framework, resulting in the challenge of lacking data for training our model. For example, methods associated with the Blender engine rely on LLM-generated Blender code and manually crafted rules [55, 73, 20, 20]. To address such challenge, our work contributes to creating new multimodal datasets tailored for our proposed framework."
        },
        {
            "title": "7.1 Layout Dataset",
            "content": "Following our rendering pipeline, to comply with industrial standardization requirements, we transform two raw datasets, the LoveDA dataset [60, 61] and our proprietary Wild dataset, into multifaceted layout data, including sketches, layout semantic segmentation, and other aspects as detailed below, which are crucial for training and inference. LoveDA dataset is an open-source semantic segmentation dataset that contains 5, 987 sensing high spatial resolution (HSR) images. Due to the flat terrain in each image, all elevation values for each height map are assigned to 0. For the Wild dataset, we collected 1,095 high-resolution wilderness scenes from Google Earth Platform. Each image has 2, 048 2, 048 pixels, covering 5.4 km2 area with an average pixel resolution of 2.53 meters. We process those images and their corresponding DEM data by dividing them into 512 512pixel sub-images and creating sketches (by rainfall accumulation algorithms [18]), height maps (by simulated erosion algorithms), and image semantic segmentation. Building upon this data, we conduct additional data processing and enhancement in the following steps: 1. Conversion of layouts to symbolic matrices. We transform the layout semantic segmentation images into 32 32 symbolic matrices illustrated in Figure 3 . This down-sampling process creates compact semantic visual-to-text mapping for training LLML. Such mapping from assets to symbols is detailed in Table 1. 2. Data augmentation. To mitigate the risk of overfitting on the dataset and improve the models robustness, we conduct data augmentation such as rotating images. Then, following the captioning step below, we annotate the same image multiple times from different angles, resulting in multiple descriptions corresponding to single image and significantly expanding the datasets. 3. Captioning. We leveraged the powerful GPT-4o for data annotation as illustrated in Figure 3. For the accuracy and consistency of our data annotation process, we developed sophisticated prompt engineering approach for captioning via GPT-4o based on in-context learning. Particularly, each prompt consists of two major components: (1) The color-to-scene mapping prompt that establishes the correspondence between colors and various types of assets; (2) layout contextual guidance prompt that provides specific instructions for describing positions, maintaining conciseness, and preserving adjacency relationships between different types of assets. The annotator, namely GPT-4o, is guided to provide effective spatial relationship and distribution descriptions. For the height map, we also utilize GPT-4o to generate descriptions of elevation changes and their directions. 12 Figure 3: Illustration of the dataset construction process. Finally, we construct such two new datasets for training models following the above procedures as in Figure 3. The LoveDA dataset contains 8, 236 data instances derived from the original datasets 2, 059 suburban images. Similarly, the Wild dataset expanded to 24, 380 data instances. Both datasets share similar structure, with each sample containing layout semantic segmentation, captions, and symbolic matrices. The Wild dataset additionally includes sketches and height maps for each sample, providing more comprehensive geospatial information."
        },
        {
            "title": "7.2 Environmental Configuration Dataset",
            "content": "We propose hierarchical framework for constructing environmental configuration and its language description, following industry standards. Then, we build the configuration-description datasets for LoveDA and Wild as follows: 1. Height map and layout captioning. We use GPT-4o to generate detailed descriptions of layout images and height maps, providing comprehensive textual representations of the environments visual and spatial characteristics. 2. Environmental configuration generation. Motivated by the UE 5s procedural generation pipeline, we utilize random sampling and structured prompt engineering for GPT4-o to construct JSONformatted environmental configuration dataset containing scene attributes and agent parameters. We 13 note that these JSON-formatted configurations will be accurately converted to engine-native properties via the transformation function ΨC (section 6) in the rendering process. We include two types of configurations: (1) Context-independent configurations, which refer to random sampling results that remain applicable across all scene types in our datasets. These include scene attributes (e.g., time of day) and agent parameters (e.g., types). For these configurations, we employ systematic random sampling strategies to maximize coverage of the attribute space while preserving realistic distributions. (2) Context-dependent configurations, which require common-sense reasoning. We exploit GPT-4os inherent reasoning capabilities by using structured prompts that integrate previously sampled configurations with height map and layout captioning, enabling contextual analysis for inferring remaining configurations. 3. Description generation with GPT-4o. We employ rule-based prompt method that integrates the generated context-independent and context-dependent configurations with captioning for height map and layout to guide GPT-4os inference process and generate comprehensive textual descriptions. As our future work, we will enrich our datasets with more diverse descriptions, enabling our model to support wider range of descriptive styles, including more colloquial and conversational language."
        },
        {
            "title": "8 Experiment",
            "content": "In this section, we demonstrate the generation process based on our framework and evaluate its performance via comparison with prior works. 8."
        },
        {
            "title": "Implementation Details",
            "content": "In the experiments, we train the aforementioned models on our datasets (splitting them into training and test sets) under the following setups: (1) Fixed-height layout generation with textual inputs, where we finetune LLaMA-2-7B using AdamW optimizer [32] (α = 5 105, β1 = 0.9, β2 = 0.999, λ = 0.001) for 4 epochs with batch size 32; (2) Variable-height layout generation with multimodal inputs, which employs our proposed three-stage training strategy in Section 4.4: CLIP fine-tuning (10 epochs), visual-language feature alignment (12 epochs, α = 5 104), and end-to-end fine-tuning with frozen CLIP parameters (4 epochs, α = 5 105, β1 = 0.9, β2 = 0.999, λ = 0.001); (3) Environmental configuration generation, where we fine-tune LLaMA-2-7B for 5 epochs with batch size 64. All experiments are conducted on NVIDIA A100 GPUs. Our models are trained using the curated layout datasets as well as the environmental configuration dataset in Section 7. We note that in all experiments, the prompts have been included in the inputs, as introduced in Sections 4 and 5. We only highlight user instructions in the results."
        },
        {
            "title": "8.2 Experimental Results",
            "content": "Environment Generation via LatticeWorld. We evaluate LatticeWorlds text-to-layout generation capabilities against other solutions. Our experiments, as shown in Table 3, are conducted under two conditions: (1) Fixed-Height Condition. Layout generation using only text descriptions. (2) Variable-Height Condition. Layout generation incorporating visual signals like height maps and sketches. We compare LatticeWorld with GPT-4o, Claude 3.7 Sonnet[2], DeepSeek-R1 [8], and Qwen2-VL-Max[62] using identical prompts and instructions. To help other models interpret the height maps, in the experiments for the variable-height condition, we augment the image inputs with captions describing how height information influences the scene. In the results, the 32 32 symbol matrices are converted into RGB images for visualization. The results demonstrate LatticeWorlds effectiveness in processing multimodal inputs and generating more accurate layouts across both conditions. LatticeWorld encodes spatial relationships through concise 32 32 symbolic matrix. As shown in Inspired by the latest industrial Table 4, we evaluate scene generation with textual and visual inputs. workflows, all layouts are rendered in UE 5, with varied terrain while keeping weather and time parameters constant across all experiments. 14 Table 3: Comparison of layout generation capabilities on LovaDA and Wild datasets. Fixed-Height Layout Generation with Textual Instructions Instruction Layout Instruction: The map displays mix of land cover types with farmlands occupying the central area, interspersed with buildings. Roads traverse the farmlands and few buildings are clustered near the center. Bodies of water are mainly on the right, with one extending towards the upper section. Forested regions are on the left, while barren land is scattered throughout, predominantly on the right. Legend Grassland Forest Farmland Water Road Building Barren Generated Layout Instruction LatticeWorld Claude 3.7 Sonnet GPT-4o DeepSeek-R1 Variable-Height Layout Generation with Multimodal Instructions Layout Instruction: The map shows large area of snow-capped terrain dominating the right side, extending from the upper to the lower regions. Surrounding this, especially concentrated on the left from upper to lower, are regions indicative of low vegetation or grasslands, with patches interspersed throughout the central and right areas, creating mixed landscape. Legend Grassland Forest Rocky Water Snow Sketch Height Map Generated Layout LatticeWorld Claude 3.7 Sonnet GPT-4o Qwen2-VL-Max Moreover, we validate the scene attribute generation ability of LatticeWorld through experiments in Table 5, using fixed layout and various environmental configurations, with different instruction inputs. Our method supports effective generation of diverse natural environments, featuring adjustable scene attributes (e.g., weather and lighting conditions) along with realistic physical property simulation. Dynamic Environment with Interactive Agents. As shown in Table 6, we can build multi-agent interaction environments based on LatticeWorld. The experiments show that our framework also supports effective configuration of agent parameters (e.g., agent types and quantities). These agents are equipped with environmental perception capabilities and can perform autonomous adversarial behaviors based on predefined rules, e.g., they will automatically pursue and attack the main agent when the main agent enters specified proximity range. Our framework also allows for future implementation of more sophisticated behavioral policies in agent interactions. These features position our framework as potential platform for embodied agent training. 15 Table 4: Demonstration of generated scenes under different multimodal layout instructions. Layout Instruction: The map displays mix of land cover with buildings concentrated towards the center and edges of the upper regions. Roads traverse the map horizontally and vertically, intersecting near the left edge of the central area. Water bodies are scattered throughout, with more prominent presence in the lower and left areas. The surrounding areas are predominantly covered in farmland, with patches of grassland in the lower left section. Demo 1 Demo Sketch Overview Local Scene (i) Local Scene (ii) Layout Instruction: The map shows large area of low bushes and grasslands covering the entire left side and extending to the upper right. Snow-capped mountains or places covered in snow are predominantly located in the lower right, with some patches in the upper left. Sketch Overview Local Scene (i) Local Scene (ii) Layout Table 5: Generated scenes under different environmental configuration instructions. Layout Instruction: The map displays substantial presence of forested regions, primarily concentrated in the central area and extending towards the edges. Grasslands or low bushes are scattered throughout. significant body of water is noticeable towards the lower left. Environmental Configuration Instruction: In cartoon-style landscape, the terrain is predominantly mountainous with sunny autumn day. In the upper left, two eagles are patrolling the skies, their sharp eyes scanning the forested regions below. Meanwhile, in the middle left, an ancient warrior stands idle, perhaps guarding the path through the forest. The central area is lush with forest extending towards the edges, providing rich tapestry of greens against the backdrop of mountains. To the lower left, significant body of water glistens under the sun, offering serene contrast to the rugged peaks. Demo 1 Demo 2 Overview Local Scene (i) Local Scene (ii) Local Scene (iii) Environmental Configuration Instruction: In the upper left of the mountainous terrain, amidst the rain and mist of an autumn afternoon, three ancient warriors patrol the rugged landscape. Their vigilant presence contrasts with the serene middle left, where five sheep are grazing peacefully. The scene captures the realism of world where ancient and pastoral life coexist, surrounded by the dense forests and the significant body of water noticeable towards the lower left. Overview Local Scene (i) Local Scene (ii) Local Scene (iii) Table 6: Demonstration of environment generation with dynamic agents. Local Scene (i) Local Scene (ii) Instruction In the lower left of the map, flock of nine sheep is grazing peacefully on the expansive grassland. Nearby, in the lower right-middle area, two horses are also grazing, enjoying the sunny spring daytime. Above them, in the center of the map, two eagles are patrolling the skies, casting shadows over the landscape. In the daytime sunny of spring mountain terrain, the upper left area is ilIn the middle luminated by peak. right, seven aerial robots patrol above the grasslands maintaining their steady flight amidst the wind. Meanwhile, humanoid robots stand idle, their metallic forms glinting in the sunlight. Under the cover of night, amidst the rainy springtime suburbs, two humanoid robots stand vigilant. The dense cluster of buildings forms the heart of this cyberpunk landscape, their neon lights flickering through the raindrops. One robot is stationed in the center, amidst the towering structures, while the other stands in the lower area, near the buildings. Comparison with Prior Works. Since LatticeWorld employs platform-based paradigm, we compare LatticeWorld with existing platform-based methods, focusing solely on scene generation, since prior methods may lack support for dynamic interactive agents. As illustrated in Table 7 and Table 8, we conduct qualitative comparison using published demonstrations from prior works and show that LatticeWorld achieves better generation quality. Due to different methodologies and datasets, we only choose similar scenes for comparison. It is worth noting that LatticeWorld generates large 3D scenes, and we only capture portion of the scene details. Comparison with Industrial Manual Methods. The standard industrial creation of dynamic environments combines PCG with manual artistic work. The process consists of three main phases: (1) the concept art phase (sketching and refinement), (2) the modeling phase (height maps, 3D assets, and UV mapping), and (3) the scene editing phase (layout, lighting, and materials). We compare environments created by professional artists and LatticeWorld using identical layout and parameter instructions. We choose similar scenes containing trees and buildings for comparison, and the results are shown in Table 9. Workload comparison  (Table 10)  shows that while LatticeWorld uses pre-completed assets for sketching, modeling, and texturing, it significantly improves efficiency in other steps. Specifically, LatticeWorld lowers total production time from 55 days (manual) to less than 0.6 days, yielding over 90 efficiency improvement. This advantage increases when generating multiple environments, as the pre-completion cost is distributed."
        },
        {
            "title": "9 Conclusion",
            "content": "This paper introduces LatticeWorld, multimodal framework for interactive virtual world generation via LLMs. LatticeWorld leverages lightweight LLMs alongside the industry-grade rendering engine (e.g., Unreal Engine 5) to generate dynamic environment with multimodal instructions. Our work on LatticeWorld can be further improved in the following aspects: (1) The adversarial agents currently follow simple policy: 17 Table 7: Qualitative comparisons of generated 3D scenes in wilderness."
        },
        {
            "title": "Infinigen",
            "content": "3D-GPT SceneX LatticeWorld Table 8: Qualitative comparisons of generated 3D scenes in rural areas. Methods Prompt or Description Generated Scenes BlenderGPT Three trees in row beside neighborhood. SceneCraft Three trees in row beside neighborhood. CPG Landscapes Organized Chambers, Trees, and Rocks. LatticeWorld The map displays mix of land cover with buildings concentrated towards the center and upper regions. The surrounding areas are predominantly covered in farmland, with small patches of trees. 18 Table 9: Qualitative comparisons of generated 3D scenes between LatticeWorld and human artist. Layout Instruction: The map shows central road snaking from the lower right to the upper left. Buildings are clustered primarily around this road, with higher concentration in the center. Forested areas are predominantly on the edges, with large section in the lower left and smaller patches elsewhere. Water bodies are scattered throughout, and farmlands are situated mostly toward the lower right. Grassland is visible in the whole region, while barren land is interspersed near the center and upper region. Environmental Configuration Instruction: The map is suburbs, season is autumn, realism style in daytime, and today is sunny day. Overview Local Scene (i) Local Scene (ii) Local Scene (iii) Instruction LatticeWorld Artist Table 10: The comparison between LatticeWorld and artists workload (measured in days). they attack the main agent whenever it approaches them. We can implement more diverse policies to create varied adversarial behaviors. (2) Our current framework is limited to controlling single main player. We can enhance its functionality to control multiple main players. Furthermore, the main agent is currently controlled via input devices. This capability can be expanded to support AI algorithmic policies by using existing UE plug-ins. (3) Main agents body parts cannot be controlled independently. We can add finergrained control of specific parts through more sophisticated modeling. (4) Finally, we plan to expand the asset library with more objects and interactive elements to generate more diverse virtual worlds."
        },
        {
            "title": "Acknowledgement",
            "content": "The authors would like to thank Zixi Liu for his technical assistance and helpful discussions."
        },
        {
            "title": "References",
            "content": "[1] Jean-Baptiste Alayrac, Jeff Donahue, Pauline Luc, Antoine Miech, Iain Barr, Yana Hasson, Karel Lenc, Arthur Mensch, Katherine Millican, Malcolm Reynolds, et al. Flamingo: visual language model for few-shot learning. Advances in Neural Information Processing Systems, 35:2371623736, 2022. [2] AI Anthropic. The claude 3 model family: Opus, sonnet, haiku. Claude-3 Model Card, 2024. [3] Peyman Ayubi, Saeed Setayeshi, and Amir Masoud Rahmani. Deterministic chaos game: new fractal based pseudo-random number generator and its cryptographic application. Journal of Information Security and Applications, 52:102472, 2020. [4] Michael Beukman, Christopher Cleghorn, and Steven James. Procedural content generation using In Proceedings of the Genetic and neuroevolution and novelty search for diverse video game levels. Evolutionary Computation Conference, pages 10281037, 2022. [5] Jeremiah Brackbill, Douglas Kothe, and Hans Ruppel. Flip: low-dissipation, particle-in-cell method for fluid flow. Computer Physics Communications, 48(1):2538, 1988. [6] Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. Language models are few-shot learners. Advances in neural information processing systems, 33:18771901, 2020. [7] Zilong Chen, Feng Wang, Yikai Wang, and Huaping Liu. Text-to-3d using gaussian splatting. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 21401 21412, 2024. [8] DeepSeek-AI, Daya Guo, Dejian Yang, Haowei Zhang, Junxiao Song, Ruoyu Zhang, Runxin Xu, Qihao Zhu, Shirong Ma, Peiyi Wang, Xiao Bi, Xiaokang Zhang, Xingkai Yu, Yu Wu, Z. F. Wu, Zhibin Gou, Zhihong Shao, Zhuoshu Li, Ziyi Gao, Aixin Liu, Bing Xue, Bingxuan Wang, Bochao Wu, Bei Feng, Chengda Lu, Chenggang Zhao, Chengqi Deng, Chenyu Zhang, Chong Ruan, Damai Dai, Deli Chen, Dongjie Ji, Erhang Li, Fangyun Lin, Fucong Dai, Fuli Luo, Guangbo Hao, Guanting Chen, Guowei Li, H. Zhang, Han Bao, Hanwei Xu, Haocheng Wang, Honghui Ding, Huajian Xin, Huazuo Gao, Hui Qu, Hui Li, Jianzhong Guo, Jiashi Li, Jiawei Wang, Jingchang Chen, Jingyang Yuan, Junjie Qiu, Junlong Li, J. L. Cai, Jiaqi Ni, Jian Liang, Jin Chen, Kai Dong, Kai Hu, Kaige Gao, Kang Guan, Kexin Huang, Kuai Yu, Lean Wang, Lecong Zhang, Liang Zhao, Litong Wang, Liyue Zhang, Lei Xu, Leyi Xia, Mingchuan Zhang, Minghua Zhang, Minghui Tang, Meng Li, Miaojun Wang, Mingming Li, Ning Tian, Panpan Huang, Peng Zhang, Qiancheng Wang, Qinyu Chen, Qiushi Du, Ruiqi Ge, Ruisong Zhang, Ruizhe Pan, Runji Wang, R. J. Chen, R. L. Jin, Ruyi Chen, Shanghao Lu, Shangyan Zhou, Shanhuang Chen, Shengfeng Ye, Shiyu Wang, Shuiping Yu, Shunfeng Zhou, Shuting Pan, S. S. Li, Shuang Zhou, Shaoqing Wu, Shengfeng Ye, Tao Yun, Tian Pei, Tianyu Sun, T. Wang, Wangding Zeng, Wanjia Zhao, Wen Liu, Wenfeng Liang, Wenjun Gao, Wenqin Yu, Wentao Zhang, W. L. Xiao, Wei An, Xiaodong Liu, Xiaohan Wang, Xiaokang Chen, Xiaotao Nie, Xin Cheng, Xin Liu, Xin Xie, Xingchao Liu, Xinyu Yang, Xinyuan Li, Xuecheng Su, Xuheng Lin, X. Q. Li, Xiangyue Jin, Xiaojin Shen, Xiaosha Chen, Xiaowen Sun, Xiaoxiang Wang, Xinnan Song, Xinyi Zhou, Xianzu Wang, Xinxia Shan, Y. K. Li, Y. Q. Wang, Y. X. Wei, Yang Zhang, Yanhong Xu, Yao Li, Yao Zhao, Yaofeng Sun, Yaohui Wang, Yi Yu, Yichao Zhang, Yifan Shi, Yiliang Xiong, Ying He, Yishi Piao, Yisong Wang, Yixuan Tan, Yiyang Ma, Yiyuan Liu, Yongqiang Guo, Yuan Ou, Yuduan Wang, Yue Gong, Yuheng Zou, Yujia He, Yunfan Xiong, Yuxiang Luo, Yuxiang You, Yuxuan Liu, Yuyang Zhou, Y. X. Zhu, Yanhong Xu, Yanping Huang, Yaohui Li, Yi Zheng, Yuchen Zhu, Yunxian Ma, Ying Tang, Yukun Zha, Yuting Yan, Z. Z. Ren, Zehui Ren, Zhangli Sha, Zhe Fu, Zhean Xu, Zhenda Xie, Zhengyan Zhang, Zhewen Hao, Zhicheng Ma, Zhigang Yan, Zhiyu Wu, Zihui Gu, Zijia Zhu, Zijun Liu, Zilin Li, Ziwei Xie, Ziyang Song, Zizheng Pan, Zhen Huang, Zhipeng Xu, Zhongyu Zhang, and Zhen Zhang. Deepseek-r1: Incentivizing reasoning capability in llms via reinforcement learning, 2025. [9] Jie Deng, Wenhao Chai, Jianshu Guo, Qixuan Huang, Wenhao Hu, Jenq-Neng Hwang, and Gaoang Wang. Citygen: Infinite and controllable 3d city layout generation, 2023. 20 [10] Jie Deng, Wenhao Chai, Junsheng Huang, Zhonghan Zhao, Qixuan Huang, Mingyan Gao, Jianshu Guo, Shengyu Hao, Wenhao Hu, Jenq-Neng Hwang, et al. Citycraft: real crafter for 3d city generation. arXiv preprint arXiv:2406.04983, 2024. [11] Sam Earle, Justin Snider, Matthew Fontaine, Stefanos Nikolaidis, and Julian Togelius. Illuminating diverse neural cellular automata for level generation. In Proceedings of the Genetic and Evolutionary Computation Conference, pages 6876, 2022. [12] Jonas Freiknecht and Wolfgang Effelsberg. survey on the procedural generation of virtual worlds. Multimodal Technologies and Interaction, 1(4):27, 2017. [13] Rafail Fridman, Amit Abecasis, Yoni Kasten, and Tali Dekel. Scenescape: Text-driven consistent scene generation. arXiv preprint arXiv:2302.01133, 2023. [14] Rafail Fridman, Amit Abecasis, Yoni Kasten, and Tali Dekel. Scenescape: Text-driven consistent scene generation. Advances in Neural Information Processing Systems, 36, 2024. [15] Alessio Gambi, Marc Mueller, and Gordon Fraser. Automatically testing self-driving cars with searchbased procedural content generation. In Proceedings of the 28th ACM SIGSOFT International Symposium on Software Testing and Analysis, pages 318328, 2019. [16] gd3kr. Blendergpt. https://github.com/gd3kr/BlenderGPT, April 2023. [17] Genie-2 Team. Genie-2: Advanced interactive virtual environment. https://genie2.co/, 2024. Accessed: 2024-12-12. [18] Eric Guerin, Julie Digne, Eric Galin, Adrien Peytavie, Christian Wolf, Bedrich Benes, and Benoˆıt Martinez. Interactive example-based terrain authoring with conditional generative adversarial networks. ACM Trans. Graph., 36(6):2281, 2017. [19] Lukas Hollein, Ang Cao, Andrew Owens, Justin Johnson, and Matthias Nießner. Text2room: Extracting In Proceedings of the IEEE/CVF International textured 3d meshes from 2d text-to-image models. Conference on Computer Vision, pages 79097920, 2023. [20] Ziniu Hu, Ahmet Iscen, Aashi Jain, Thomas Kipf, Yisong Yue, David Ross, Cordelia Schmid, and In Forty-first Alireza Fathi. Scenecraft: An llm agent for synthesizing 3d scenes as blender code. International Conference on Machine Learning, 2024. [21] Albert Jiang, Alexandre Sablayrolles, Arthur Mensch, Chris Bamford, Devendra Singh Chaplot, Diego de las Casas, Florian Bressand, Gianna Lengyel, Guillaume Lample, Lucile Saulnier, et al. Mistral 7b. arXiv e-prints, pages arXiv2310, 2023. [22] Bernhard Kerbl, Georgios Kopanas, Thomas Leimkuhler, and George Drettakis. 3d gaussian splatting for real-time radiance field rendering. ACM Transactions on Graphics (SIGGRAPH), 42(4), July 2023. [23] Ahmed Khalifa, Philip Bontrager, Sam Earle, and Julian Togelius. Pcgrl: Procedural content generation via reinforcement learning. In Proceedings of the AAAI Conference on Artificial Intelligence and Interactive Digital Entertainment, volume 16, pages 95101, 2020. [24] Chengshu Li, Fei Xia, Roberto Martın-Martın, Michael Lingelbach, Sanjana Srivastava, Bokui Shen, Kent Vainio, Cem Gokmen, Gokul Dharan, Tanish Jain, Andrey Kurenkov, C. Karen Liu, Hyowon igibson 2.0: Object-centric simulation for robot Gweon, Jiajun Wu, Li Fei-Fei, and Silvio Savarese. learning of everyday household tasks, 2021. [25] Haoran Li, Haolin Shi, Wenli Zhang, Wenjun Wu, Yong Liao, Lin Wang, Lik-hang Lee, and Peng Yuan Zhou. Dreamscene: 3d gaussian-based text-to-3d scene generation via formation pattern sampling. In European Conference on Computer Vision, pages 214230. Springer, 2025. 21 [26] Xinyang Li, Zhangyu Lai, Linning Xu, Yansong Qu, Liujuan Cao, Shengchuan Zhang, Bo Dai, and Rongrong Ji. Director3d: Real-world camera trajectory and 3d scene generation from text. arXiv preprint arXiv:2406.17601, 2024. [27] Long Lian, Boyi Li, Adam Yala, and Trevor Darrell. Llm-grounded diffusion: Enhancing prompt understanding of text-to-image diffusion models with large language models. arXiv preprint arXiv:2305.13655, 2023. [28] Xingyu Lin, Yufei Wang, Jake Olkin, and David Held. Softgym: Benchmarking deep reinforcement learning for deformable object manipulation, 2021. [29] Haotian Liu, Chunyuan Li, Qingyang Wu, and Yong Jae Lee. Visual instruction tuning. Advances in neural information processing systems, 36, 2023. [30] Jia-Hong Liu, Shao-Kui Zhang, Chuyue Zhang, and Song-Hai Zhang. Controllable procedural generation of landscapes. In ACM Multimedia 2024, 2024. [31] Jialin Liu, Sam Snodgrass, Ahmed Khalifa, Sebastian Risi, Georgios Yannakakis, and Julian Togelius. Deep learning for procedural content generation. Neural Computing and Applications, 33(1):1937, 2021. [32] Loshchilov. Decoupled weight decay regularization. arXiv preprint arXiv:1711.05101, 2017. [33] Fan Lu, Kwan-Yee Lin, Yan Xu, Hongsheng Li, Guang Chen, and Changjun Jiang. Urban architect: Steerable 3d urban scene generation with layout prior. arXiv preprint arXiv:2404.06780, 2024. [34] Taiming Lu, Tianmin Shu, Junfei Xiao, Luoxin Ye, Jiahao Wang, Cheng Peng, Chen Wei, Daniel Khashabi, Rama Chellappa, Alan Yuille, and Jieneng Chen. Genex: Generating an explorable world, 2024. [35] MetaAI. Llama 3 model, 2024. Accessed: 2024-06-13. [36] Ben Mildenhall, Pratul Srinivasan, Matthew Tancik, Jonathan Barron, Ravi Ramamoorthi, and Ren Ng. Nerf: Representing scenes as neural radiance fields for view synthesis. Communications of the ACM, 65(1):99106, 2021. [37] Matthias Muller, David Charypar, and Markus Gross. Particle-based fluid simulation for interactive In Proceedings of the 2003 ACM SIGGRAPH/Eurographics symposium on Computer applications. animation, pages 154159. Citeseer, 2003. [38] Thomas Muller, Alex Evans, Christoph Schied, and Alexander Keller. Instant neural graphics primitives with multiresolution hash encoding. ACM Transactions on Graphics (ToG), 41(4):115, 2022. [39] NVIDIA. Isaac sim 4.0 - robotics simulation and synthetic data generation. https://developer.nvidia.com/isaac-sim, 2024. [40] OpenAI. GPT-4V(ision) system card, 2023. [41] OpenAI. Gpt-4 technical report, 2024. [42] OpenAI. Gpt-4o: Openais optimized language model. Technical Report, 2024. Accessed: 2024-10-01. [43] Long Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida, Carroll Wainwright, Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, et al. Training language models to follow instructions with human feedback. Advances in neural information processing systems, 35:2773027744, 2022. [44] Diego Perez-Liebana, Jialin Liu, Ahmed Khalifa, Raluca Gaina, Julian Togelius, and Simon Lucas. General video game ai: multitrack framework for evaluating agents, games, and content generation algorithms. IEEE Transactions on Games, 11(3):195214, 2019. [45] Xavier Puig, Kevin Ra, Marko Boben, Jiaman Li, Tingwu Wang, Sanja Fidler, and Antonio Torralba. Virtualhome: Simulating household activities via programs. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 84948502, 2018. [46] Xavier Puig, Eric Undersander, Andrew Szot, Mikael Dallaire Cote, Tsung-Yen Yang, Ruslan Partsey, Ruta Desai, Alexander William Clegg, Michal Hlavac, So Yeon Min, Vladimır Vondruˇs, Theophile Gervet, Vincent-Pierre Berges, John M. Turner, Oleksandr Maksymets, Zsolt Kira, Mrinal Kalakrishnan, Jitendra Malik, Devendra Singh Chaplot, Unnat Jain, Dhruv Batra, Akshara Rai, and Roozbeh Mottaghi. Habitat 3.0: co-habitat for humans, avatars and robots, 2023. [47] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, et al. Learning transferable visual models from natural language supervision. In International conference on machine learning, pages 87488763. PMLR, 2021. [48] Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, and Peter Liu. Exploring the limits of transfer learning with unified text-to-text transformer. Journal of machine learning research, 21(140):167, 2020. [49] Alexander Raistrick, Lahav Lipson, Zeyu Ma, Lingjie Mei, Mingzhe Wang, Yiming Zuo, Karhan Kayan, Hongyu Wen, Beining Han, Yihan Wang, et al. Infinite photorealistic worlds using procedural generation. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 1263012641, 2023. [50] Aditya Ramesh, Prafulla Dhariwal, Alex Nichol, Casey Chu, and Mark Chen. Hierarchical textconditional image generation with clip latents. arXiv preprint arXiv:2204.06125, 1(2):3, 2022. [51] Pengzhen Ren, Min Li, Zhen Luo, Xinshuai Song, Ziwei Chen, Weijia Liufu, Yixuan Yang, Hao Zheng, Rongtao Xu, Zitong Huang, Tongsheng Ding, Luyang Xie, Kaidong Zhang, Changfei Fu, Yang Liu, Liang Lin, Feng Zheng, and Xiaodan Liang. Infiniteworld: unified scalable simulation framework for general visual-language robot interaction, 2024. [52] Yu Shang, Jiansheng Chen, Hangyu Fan, Jingtao Ding, Jie Feng, and Yong Li. Urbanworld: An urban world model for 3d city generation. arXiv preprint arXiv:2407.11965, 2024. [53] Mohit Shridhar, Jesse Thomason, Daniel Gordon, Yonatan Bisk, Winson Han, Roozbeh Mottaghi, Luke Zettlemoyer, and Dieter Fox. Alfred: benchmark for interpreting grounded instructions for everyday tasks. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 1074010749, 2020. [54] Liangchen Song, Liangliang Cao, Hongyu Xu, Kai Kang, Feng Tang, Junsong Yuan, and Yang Zhao. Roomdreamer: Text-driven 3d indoor scene synthesis with coherent geometry and texture. arXiv preprint arXiv:2305.11337, 2023. [55] Chunyi Sun, Junlin Han, Weijian Deng, Xinlong Wang, Zishan Qin, and Stephen Gould. 3d-gpt: Procedural 3d modeling with large language models. arXiv preprint arXiv:2310.12945, 2023. [56] Andrew Szot, Alex Clegg, Eric Undersander, Erik Wijmans, Yili Zhao, John Turner, Noah Maestre, Mustafa Mukadam, Devendra Chaplot, Oleksandr Maksymets, Aaron Gokaslan, Vladimir Vondrus, Sameer Dharur, Franziska Meier, Wojciech Galuba, Angel Chang, Zsolt Kira, Vladlen Koltun, Jitendra Malik, Manolis Savva, and Dhruv Batra. Habitat 2.0: Training home assistants to rearrange their habitat, 2022. [57] Gemini Team. Gemini: family of highly capable multimodal models, 2024. [58] Julian Togelius, Emil Kastbjerg, David Schedl, and Georgios Yannakakis. What is procedural content generation? mario on the borderline. In Proceedings of the 2nd international workshop on procedural content generation in games, pages 16, 2011. [59] Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, et al. Llama 2: Open foundation and fine-tuned chat models. arXiv preprint arXiv:2307.09288, 2023. [60] Junjue Wang, Zhuo Zheng, Ailong Ma, Xiaoyan Lu, and Yanfei Zhong. LoveDA: remote sensing land-cover dataset for domain adaptive semantic segmentation, October 2021. [61] Junjue Wang, Zhuo Zheng, Ailong Ma, Xiaoyan Lu, and Yanfei Zhong. Loveda: remote sensing land-cover dataset for domain adaptive semantic segmentation. In J. Vanschoren and S. Yeung, editors, Proceedings of the Neural Information Processing Systems Track on Datasets and Benchmarks, volume 1. Curran Associates, Inc., 2021. [62] Peng Wang, Shuai Bai, Sinan Tan, Shijie Wang, Zhihao Fan, Jinze Bai, Keqin Chen, Xuejing Liu, Jialin Wang, Wenbin Ge, et al. Qwen2-vl: Enhancing vision-language models perception of the world at any resolution. arXiv preprint arXiv:2409.12191, 2024. [63] Jason Wei, Maarten Bosma, Vincent Zhao, Kelvin Guu, Adams Wei Yu, Brian Lester, Nan Du, Andrew Dai, and Quoc Le. Finetuned language models are zero-shot learners. arXiv preprint arXiv:2109.01652, 2021. [64] WorldLabs Team. Worldlabs: Ai-powered virtual world platform. https://www.worldlabs.ai/, 2024. Accessed: 2024-12-12. [65] Wayne Wu, Honglin He, Jack He, Yiran Wang, Chenda Duan, Zhizheng Liu, Quanyi Li, and Bolei Zhou. Metaurban: An embodied ai simulation platform for urban micromobility. arXiv preprint arXiv:2407.08725, 2024. [66] Zhixuan Wu, Yuwei Mao, and Qiyu Li. Procedural game map generation using multi-leveled celluIn Proceedings of the 2nd International Symposium on Artificial lar automata by machine learning. Intelligence for Medicine Sciences, pages 168172, 2021. [67] Fengli Xu, Jun Zhang, Chen Gao, Jie Feng, and Yong Li. Urban generative intelligence (ugi): foundational platform for agents in embodied city environment. arXiv preprint arXiv:2312.11813, 2023. [68] Taoran Yi, Jiemin Fang, Junjie Wang, Guanjun Wu, Lingxi Xie, Xiaopeng Zhang, Wenyu Liu, Qi Tian, and Xinggang Wang. Gaussiandreamer: Fast generation from text to 3d gaussians by bridging 2d and 3d diffusion models. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 67966807, 2024. [69] Jingbo Zhang, Xiaoyu Li, Ziyu Wan, Can Wang, and Jing Liao. Text2nerf: Text-driven 3d scene generation with neural radiance fields. arXiv preprint arXiv:2305.11588, 2023. [70] Renrui Zhang, Jiaming Han, Chris Liu, Peng Gao, Aojun Zhou, Xiangfei Hu, Shilin Yan, Pan Lu, Hongsheng Li, and Yu Qiao. Llama-adapter: Efficient fine-tuning of language models with zero-init attention. arXiv preprint arXiv:2303.16199, 2023. [71] Songchun Zhang, Yibo Zhang, Quan Zheng, Rui Ma, Wei Hua, Hujun Bao, Weiwei Xu, and Changqing Zou. 3d-scenedreamer: Text-driven 3d-consistent scene generation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 1017010180, 2024. [72] Fangwei Zhong, Kui Wu, Churan Wang, Hao Chen, Hai Ci, Zhoujun Li, and Yizhou Wang. Unrealzoo: Enriching photo-realistic virtual worlds for embodied ai, 2024. [73] Mengqi Zhou, Jun Hou, Chuanchen Luo, Yuxi Wang, Zhaoxiang Zhang, and Junran Peng. Scenex: arXiv preprint Procedural controllable large-scale scene generation via large-language models. arXiv:2403.15698, 2024. [74] Shijie Zhou, Zhiwen Fan, Dejia Xu, Haoran Chang, Pradyumna Chari, Suya Bharadwaj, Tejas You, Zhangyang Wang, and Achuta Kadambi. Dreamscene360: Unconstrained text-to-3d scene generation with panoramic gaussian splatting. arXiv preprint arXiv:2404.06903, 2024."
        }
    ],
    "affiliations": [
        "Beihang University, China",
        "City University of Hong Kong, China",
        "NetEase, Inc., China",
        "Tsinghua University, China"
    ]
}