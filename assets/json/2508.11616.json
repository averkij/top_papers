{
    "paper_title": "Controlling Multimodal LLMs via Reward-guided Decoding",
    "authors": [
        "Oscar Mañas",
        "Pierluca D'Oro",
        "Koustuv Sinha",
        "Adriana Romero-Soriano",
        "Michal Drozdzal",
        "Aishwarya Agrawal"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "As Multimodal Large Language Models (MLLMs) gain widespread applicability, it is becoming increasingly desirable to adapt them for diverse user needs. In this paper, we study the adaptation of MLLMs through controlled decoding. To achieve this, we introduce the first method for reward-guided decoding of MLLMs and demonstrate its application in improving their visual grounding. Our method involves building reward models for visual grounding and using them to guide the MLLM's decoding process. Concretely, we build two separate reward models to independently control the degree of object precision and recall in the model's output. Our approach enables on-the-fly controllability of an MLLM's inference process in two ways: first, by giving control over the relative importance of each reward function during decoding, allowing a user to dynamically trade off object precision for recall in image captioning tasks; second, by giving control over the breadth of the search during decoding, allowing the user to control the trade-off between the amount of test-time compute and the degree of visual grounding. We evaluate our method on standard object hallucination benchmarks, showing that it provides significant controllability over MLLM inference, while consistently outperforming existing hallucination mitigation methods."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 5 1 ] . [ 1 6 1 6 1 1 . 8 0 5 2 : r Controlling Multimodal LLMs via Reward-guided Decoding Oscar Manas1,2,4, Pierluca DOro1,2,4, Koustuv Sinha4, Adriana Romero-Soriano1,3,4,5, Michal Drozdzal4, Aishwarya Agrawal1,2,5 1Mila - Quebec AI Institute, 2Universite de Montreal, 3McGill University, 4Meta FAIR, 5Canada CIFAR AI Chair oscar.manas@mila.quebec"
        },
        {
            "title": "Abstract",
            "content": "As Multimodal Large Language Models (MLLMs) gain widespread applicability, it is becoming increasingly desirable to adapt them for diverse user needs. In this paper, we study the adaptation of MLLMs through controlled decoding. To achieve this, we introduce the first method for reward-guided decoding of MLLMs and demonstrate its application in improving their visual grounding. Our method involves building reward models for visual grounding and using them to guide the MLLMs decoding process. Concretely, we build two separate reward models to independently control the degree of object precision and recall in the models output. Our approach enables on-the-fly controllability of an MLLMs inference process in two ways: first, by giving control over the relative importance of each reward function during decoding, allowing user to dynamically trade off object precision for recall in image captioning tasks; second, by giving control over the breadth of the search during decoding, allowing the user to control the trade-off between the amount of test-time compute and the degree of visual grounding. We evaluate our method on standard object hallucination benchmarks, showing that it provides significant controllability over MLLM inference, while consistently outperforming existing hallucination mitigation methods. 1. Introduction Multimodal Large Language Models (MLLMs) have shown great potential to solve wide range of visiolinguistic tasks, while offering general language interface to users [5, 9]. As the adoption of MLLMs increases [1, 14, 43], the demand to easily control their behavior to satisfy diverse user needs is emerging. Two needs, in particular, arise among the most important for users of MLLMs: a) control over the precision and thoroughness of their output (e.g., object recall), and b) control over the amount of compute spent to generate those outputs. For instance, user with visual impairment using the system to understand their surroundings may want the MLLM to respond with highly precise outputs (as hallucinations might be highly undesirable), while avoiding overly high latency on limited compute (e.g., on smartphone); instead, user leveraging the MLLM to generate synthetic captions to train downstream models may prioritize more diverse and detailed outputs (even if it means tolerating lower precision) while having the flexibility to spend more compute. In this paper, we tackle this problem and propose method for inference-time alignment of MLLMs. Our method, called multimodal reward-guided decoding (MRGD), employs two reward functions, one tailored for hallucination reduction [3] and one tailored for improving object recall. Using these reward functions as criteria for searching for better outputs, our method gives control over the two axes mentioned above: by giving the option to set relative weight for each reward, it allows to smoothly control the trade-off between object precision and recall of the MLLMs outputs; by varying the breadth of the search, we can control the trade-off between the amount of test-time compute and the degree of visual grounding (which encompasses both object precision and recall). Previous works explored reducing hallucinations in MLLMs by using methods such as prompting [53], supervised fine-tuning (SFT) [26] and RLHF fine-tuning [42, 49, 56]. However, these methods do not allow fine-grained inference-time controllability of the MLLMs behavior: prompting relies on very coarse control by means of prompt engineering, while SFT and RLHF allow no controllability at all during inference. For text-only LLMs, reward-guided decoding has been shown to be an effective way of obtaining fine-grained controllability [13, 19, 32, 40], but there is lack of such techniques for MLLMs. Compared to the text-only case, for which reward model processes data from single modality, reward models guiding MLLMs face unique challenges, as they need to process both visual 1 In particular, and textual information at the same time. multimodal reward models need to understand the interaction between the generated text output and an input from different modality (an image). This interaction can cause specific types of hallucinations to emerge [57] and should be addressed by tailored solutions [42]. In summary, the main contributions of our paper are: We propose novel approach for reward-guided decoding for MLLMs, based on building reward models for different aspects of visual grounding and combining them to guide the search for high-quality outputs at test time. Through extensive experiments, we show that in MLLMs there exists an inherent trade-off between object precision and recall, as well as between compute and visual grounding quality. Our proposed method allows user to specify desired balance between these factors, enabling smooth adaptive control over the precision and recall, as well as between compute and visual grounding trade-offs, depending on task requirements and resource constraints. We demonstrate on standard hallucination benchmarks that our proposed method consistently outperforms existing hallucination mitigation approaches, while allowing test-time controllability of an MLLMs outputs. 2. Related work Guided decoding of LLMs. In the text-only setting, several works have explored guiding the decoding process of LLMs with reward model to control output features such as helpfulness and harmlessness, and summary quality. [11, 19] train reward model to evaluate full responses, and apply it at each decoding step to evaluate response prefixes and modulate the next-token probability distribution before sampling. Instead, [13, 16, 32, 36, 47] explicitly train scoring function to predict the expected reward of response prefixes, also known as value function. [7, 22, 23, 28, 40] explore sampling strategies such as best-of-k, beam search, or Monte Carlo tree search, which are based on generating multiple responses and selecting the best one with reward model or value function. Unlike existing methods for LLMs, we build multimodal reward models to evaluate responses to multimodal instructions, which additionally contain images, and focus on evaluating MLLMs on visual grounding tasks. These models require processing both visual and textual information simultaneously, which can lead to different types of hallucinations that are specific to the multimodal nature of the inputs. Therefore, it is important to develop solutions that effectively address these unique challenges. Mitigating hallucinations of MLLMs. Prior work on mitigating visual hallucinations of MLLMs has focused on supervised fine-tuning [26, 29], preference fine-tuning with RLHF/RLAIF [2, 39, 42, 45, 49, 50, 55, 56, 58] or prompting [53]. Reward-guided decoding can be more than fine-tuning or prompting, as it directly powerful optimizes the output, making it more likely to produce the desired results [13, 19, 32, 40]. In contrast, the principles learned during fine-tuning or specified in (system) prompts may not always be respected at generation time. In addition, reward-guided decoding can be combined with prompting or fine-tuning, and readily applied to many base models without retraining. Other methods also propose to refine the MLLMs output, either via post-hoc rectification [48, 52, 57] or specialized decoding strategies [15, 21, 44, 54]. Most similar to our method, CGD [12] uses an off-the-shelf CLIP-like model to guide decoding. However, we show that training multimodal reward model on preference data using stronger backbone is more effective at mitigating hallucinations. Furthermore, we propose to guide the decoding process with combination of reward models for visual grounding, which allows the user to control the trade-off between object hallucination and recall in the MLLMs outputs. 3. Method We propose multimodal reward-guided decoding strategy to improve the controllability of MLLMs at inference time. We first build small yet effective multimodal reward models to evaluate different aspects of visual grounding, and later combine them for search-based guided decoding. 3.1. Building multimodal reward models The effectiveness of our guided decoding strategy hinges on the existence of reward function capable of successfully evaluating how well response satisfies certain objective. Unlike for math or coding problems [7], there are no automatic verifiers for the open-ended responses generated by MLLMs. We want to build method that gives controllability over the outputs of an MLLM by trading off object precision and recall at inference time. To achieve so, we build two reward models (RMs), that allow to incentivize precision and recall respectively: (1) an object hallucination reward model rθ hal (shortened to rhal when omitting the parameters is clear), trained from preference data obtained from mixture of datasets, and (2) recall reward model rrec, obtained by combining pre-trained modules. We next describe in detail how we build these two reward models. 3.1.1. Training rhal from preference data Given dataset of multimodal preference data for object hallucinations = {xv, xq, y+, y}i, where y+ and are the chosen and rejected responses respectively, we train our reward model for object hallucination rθ hal as classifier that predicts the preference probability following the 2 Figure 1. Illustration of multimodal reward-guided decoding (MRGD) for MLLMs. At each iteration, candidate completions (sentences in our case) to partial response are sampled from the MLLM and evaluated according to linear combination of rewards (the process is illustrated for the first selected completion and omitted elsewhere). The completion with largest score is selected and added to the context to generate the next candidates, until the <EOS> token is encountered. Bradley-Terry model [6, 33]: LRM (xv, xq, y+, y; θ) = log σ(rθ hal(xv, xq, y+) rθ hal(xv, xq, y)) (1) To facilitate combining multiple rewards, it is desirable that rθ hal(xv, xq, y) [0, 1]. Therefore, we add pair of mean-squared error loss terms to encourage rθ hal(xv, xq, y+) hal(xv, xq, y) to be close to 0, while to be close to 1 and rθ simultaneously avoiding the gradient saturation pitfalls of squashing activation functions. Ultimately, this leads to the following loss function: L(θ) = (x,y+,y)D [LRM (x, y+, y; θ) + (rθ hal(x, y+) 1)2 + rθ hal(x, y)2], (2) where = (xv, xq). We use PaliGemma [4] as the backbone of our reward model for object hallucination, and add to it regression head consisting of linear layer projecting the last output token embedding to single scalar. During our initial exploration, we also considered CLIP [34] as potential backbone for our reward model, but we ultimately discarded it due to the limited context length of CLIPs text encoder, which was insufficient to handle the longer responses present in preference data. 3.1.2. Building rrec by composing off-the-shelf modules We build our reward model for object recall rrec from three off-the-shelf modules: pre-trained object detector, pre-trained word embedding model, and classical NLP tools. Given an image xv and generated caption y, we first extract the reference objects from the image with the object detector, denoted as Oref = {o1, o2, ..., on}, where is the number of detected reference objects. We also extract the predicted objects from the caption with POS tagger, 3 denoted as Opred = {p1, p2, ..., pm}, where is the number of generated objects. We embed both reference and predicted objects with word embeddings fe : Rd, where is the set of all words and is the dimensionality of the embedding space. This results in embedded reference objects Eref = {fe(o1), fe(o2), ..., fe(on)} and embedded predicted objects Epred = {fe(p1), fe(p2), ..., fe(pm)}. Next, we compute the all-to-all semantic similarity between the embedded reference and predicted objects using similarity function sim : Rd Rd R. Specifically, for each predicted object pi, we compute its similarity with each reference object oj as simij = fe(pi) fe(oj)T . We consider predicted object pi as true positive if its maximum semantic similarity with any reference object is above threshold τ , i.e., maxj=1,...,n simij > τ . Finally, we sum all true positives and divide by the number of reference objects to obtain the estimated object recall rrec: rrec = (cid:80)m i=1 I(maxj=1,...,n simij > τ ) , (3) where I() is the indicator function. 3.2. Multimodal reward-guided decoding Our goal is to guide the generation process of an MLLM where the generated response is modulated using the two reward functions described above. Given an image xv and visual instruction xq, an MLLM π generates text response = {y1, ..., yn} autoregressively token-by-token, i.e., π(xv, xq). To give user the possibility of choosing the relative strength of each reward model on-the-fly, we define score as the linear combination of the rewards for object hallucination rhal and object recall rrec: s(xv, xq, y) = rhal(xv, xq, y) + (1 w) rrec(xv, xq, y), (4) where [0, 1] is guidance strength hyperparameter chosen at inference time. user can modulate the strength Algorithm 1 Multimodal reward-guided decoding \"\" while <EOS> / do for = 1 to do π(xv, xq, y) {y} end for = arg maxyY s(xv, xq, [y; y]) [y; y] end while of the reward guidance by varying w. At the extremes, for w=1, the best response is chosen entirely by following the reward model for object hallucination, while for w=0 only the reward model for object recall is applied. Given the score s(xv, x1, y), we search for response by expanding search tree of partial responses and deciding which partial response to complete depending on the reward-based selection criterion. At each iteration, we sample candidate completions = {yj j=1 from single partial response, with (m < n), evaluate each of them with reward-based score s(xv, xq, yj 1..i+m), select the one with the maximum score, and add it to the context. We then iterate this process until the <EOS> token is generated (see Algorithm 1). i..i+m}k Since reward models score for partial response also depends on how well-formed the text is, evaluating partial response at an arbitrary token can produce lower score for partial response that may be in reality more aligned than others (due to, e.g., incomplete words). To address this potential issue, we take advantage of the fact that captions are typically composed of multiple sentences, and evaluate the output of the MLLM every sentences (concretely, we use the delimiter .). As grows, the reward model will evaluate longer and longer outputs. As gets larger than the largest output length (equivalent behavior to = ), only complete outputs concluded with an <EOS> token are evaluated, and one complete output is selected among them: this strategy is usually referred to as rejection sampling or best-of-k in the literature [7, 40]. Figure 1 provides summary of our method. 4. Experiments We evaluate our multimodal reward-guided decoding strategy in mitigating object hallucinations in long captions, and study the trade-offs between object precision and recall, and between visual grounding and test-time compute. 4.1. Experimental setup Training data. We train our reward model for evaluating object hallucination on mixture of publicly available multimodal preference datasets where responses without hallucinations are preferred over those with hallucinations: LLaVA-RLHF [42] (9.4k), RLHF-V [49] (5.7k), and POVID [56] (17k). In addition, we repurpose SugarCrepe [17] (7.5k) as preference data1. We use an 80/20% train/validation split for each dataset. To handle varying dataset sizes, we sample each minibatch such that it has roughly the same amount of examples from each dataset. Implementation details. We initialize our object hallucination reward models backbone from PaliGemma2, train the linear regression head from scratch and finetune the backbone with LoRA [18]. We use an effective minibatch size of 256, warm up the learning rate from 0 to 1e3 during the first 5% of the first epoch and decay it to zero with cosine schedule. We train the reward model for single epoch. For the object recall reward model, we use the open-vocabulary object detector OWLv23 [31], the word embedding model Sentence-BERT4 [37] and the POS tagger from the Natural Language Toolkit (NLTK). We set the object similarity 5 [27], Llama-3.2threshold τ =0.5. We use LLaVA-1.57B 7 as our base MLLMs. Vision11B We caption images with the prompt Describe this image in detail for LLaVA-1.5 and SmolVLM-2, and Describe this image in few sentences for Llama-3.2-Vision. For guided decoding, unless otherwise specified, we use sampling temperature of t=1.0 for LLaVA-1.5 and t=0.2 for Llama-3.2-Vision SmolVLM-2. Evaluation setup. We evaluate our method on two standard object hallucination benchmarks, CHAIR [38] (5k) and AMBER [46] (1k), and report instance-level and sentence-level hallucination rates (the inverse of object precision), using the metrics used in respective benchmarks Ci and CHAIR for instance-level, and Cs and Hal. for sentence level. We also report object recall using the Rec. and Cov. (short for coverage) metrics, and caption length (denoted by Len.) to ensure our method generates meaningful captions rather than degenerating into object-less outputs (more details in Appendix 6.1). 6 [14] and SmolVLM-22.2B 4.2. Reward model evaluation We first evaluate the performance of rhal on held-out validation set from our preference data. We define accuracy as the fraction of times the reward model assigns higher scores hal(xv, xq, y+) > rejected responses, i.e. rθ to chosen vs. hal(xv, xq, y). We obtain an average validation accuracy rθ of 82.05%. We also evaluate rhal on 5000 examples from VLFeedback [24] (not part of rhals training data), selecting best and worst responses for each example, and obtain 1We use the instruction Describe this image. 2google/paligemma-3b-pt-224 3google/owlv2-base-patch16-ensemble 4sentence-transformers/all-mpnet-base-v2 5llava-hf/llava-1.5-7b-hf 6meta-llama/Llama-3.2-11B-Vision-Instruct 7HuggingFaceTB/SmolVLM2-2.2B-Instruct 4 Table 1. Results on object hallucination benchmarks for LLaVA-1.5. MRGD with k=30 and =1. Ci/CHAIR: instance-level hallucination rate, Cs/Hal.: sentence-level hallucination rate, Rec./Cov.: object recall/coverage, BS@k: beam search with beams, : reported results from [39], : results computed by us running the original code, ?: the decoding strategy used is unclear from the paper. Model"
        },
        {
            "title": "Baselines",
            "content": "LLaVA-1.57B [27] 7B [42] Fine-tuning approaches LLaVA-RLHF HA-DPO [55] POVID [56] EOS [51] HALVA7B [39] CSR [58] Liu et al. [29] mDPO [45]"
        },
        {
            "title": "Guided decoding approaches",
            "content": "LLaVA-1.57B Decoding strategy Greedy Greedy + Prompting BS@30 Greedy BS@5 ? Greedy ? BS@5 ? ? VCD [21] CGD [12] MRGDw=1.0 MRGDw=0.75 MRGDw=0.5 MRGDw=0.25 MRGDw=0. COCO AMBER Ci () Cs () Rec. () Len. CHAIR () Hal. () Cov. () 15.05 13.50 15. 16.09 11.0 5.4 12.3 11.7 7.3 14.5 9.8 15.76 9.48 4.53 4.76 5.34 7.67 24.20 48.94 44.00 55.00 57.24 38.2 31.8 40.2 41.4 28.0 55.0 35.7 54.18 37. 18.19 19.28 22.54 32.63 73.42 81.30 80.38 81.62 81.34 - - - - - 79.2 - 81.66 80.11 76.04 76.84 78.63 81.56 85.23 90.12 92.98 101. 119.82 91.0 - 79.7 92.2 - 107.5 - 102.91 88.59 95.90 96.17 97.96 105.34 108.92 7.6 6.7 11.2 10.2 6.7 - 5.1 6.6 - 6.5 4.4 9.7 5. 3.4 3.2 4.4 6.5 14.8 31.8 29.1 41.4 48.7 30.9 - 22.7 32.2 - 31.7 24.5 42.8 24.0 15.9 17.3 25.4 37.7 65.0 49.3 49.4 46. 53.0 49.8 - 49.1 53.0 - 50.9 52.4 51.6 48.3 52.4 56.7 60.8 63.8 64.3 67.68% accuracy. This is in line with the performance of well-behaved reward models [20]. We also evaluate rrecs object detector on COCO images and obtain precision of 63.16% and recall of 55.83%. Similarly, we evaluate rrecs POS tagger on COCO captions and obtain precision of 67.04% and recall of 54.54%. Although rrec is an imperfect estimator, we empirically verify it helps improve object recall. 4.3. Comparison to baselines and existing methods We compare MRGD with existing hallucination mitigation methods based on fine-tuning [29, 39, 45, 51, 55, 56, 58] and guided decoding [12, 21] for the LLaVA-1.5 base model. Existing methods were selected based on recency, comparability and code/checkpoint availability (more details in Appendix 6.2). We also implement prompting baseline based on requesting the desired response properties in the input prompt (more details in Appendix 6.3). For the MRGD, we choose the best performing variant w.r.t. number of samples k, the reward evaluation period , and the temperature t. Table 1 shows MRGD with w=1 considerably reduces object hallucinations w.r.t. greedy decoding, at the expense of moderate decrease in object recall/coverage. For instance, on the COCO benchmark, CHAIRi is reduced by 70% (from 15.05% with greedy decoding to 4.53% with MRGD) while recall is only reduced by 6.5%. By combining both reward models with w=0.5, recall is substantially increased w.r.t. w=1.0 (2.6% on COCO and 8.4% on AMBER), without overly increasing the hallucination rate (0.8% on COCO and 1% AMBER). When w=0, MRGD achieves state-of-the-art results on object recall/coverage at the cost of higher hallucination rate (e.g., CHAIRi is increased by 60.8% w.r.t. greedy decoding). We also observe the optimal operating point mitigating object hallucinations without loosing recall varies by benchmark, with w0.25 for COCO and = 1.0 for AMBER. An analysis of 500 images from COCO and AMBER reveals that COCO images have an average of 21.4 detected objects, compared to 9.9 for AMBER, resulting in systematically lower rrec values for COCO. Therefore, the optimal assigns more weight to rrec (lower w) for COCO than for AMBER. Compared to prior visual hallucination mitigation methods, MRGD consistently surpasses the performance of methods which fine-tune the base MLLM, while offering greater flexibility and more granular control over the MLLMs behavior. For guided decoding approaches, we see MRGD also outperforms CGD [12], achieving 50% lower hallucination rate on COCO and 30% on AMBER. Table 2. Results on object hallucination benchmarks for Llama-3.2-Vision and SmolVLM-2. MRGD with k=30 and =1. Ci/CHAIR: instance-level hallucination rate, Cs/Hal.: sentence-level hallucination rate, Rec./Cov.: object recall/coverage. Model Llama-3.2-Vision11B [14] SmolVLM-22.2B [30] Decoding strategy Greedy Greedy + Prompting MRGDw=1.0 MRGDw=0.5 MRGDw=0.0 Greedy MRGDw=1.0 MRGDw=0.5 MRGDw=0.0 COCO AMBER Ci () Cs () Rec. () Len. CHAIR () Hal. () Cov. () 5.82 6.14 4.38 4.76 6.89 6.06 4.32 4.98 7. 20.52 25.24 15.50 16.75 24.50 20.08 14.38 16.46 25.00 71.45 71.92 69.54 71.47 74. 70.17 68.04 69.93 73.22 91.61 160.10 87.18 87.53 92.33 85.92 79.27 78.47 84. 6.1 5.5 4.1 4.8 7.2 5.3 3.8 4.4 6.6 35.4 39.4 25.2 28.8 42. 25.7 18.2 21.9 33.4 66.0 65.1 65.0 68.9 70.7 58.4 58.7 62.5 64. (a) Reward value rhal (left) and CHAIRi (right), MRGD with w=1.0. (b) Reward value rrec (left) and Recall (right), MRGD with w=0.0. Figure 2. LLaVA-1.5 on COCO. Leveraging the reward model to guide the generation more often (lower ) improves sample-efficiency. Surprisingly, LLaVA-RLHF and VCD [21] exhibit higher hallucination rate than greedy decoding on captioning hallucination benchmarks, which were not considered in the original papers; instead, they limited their evaluation to discriminative hallucination benchmarks consisting of visual questions. This suggests that generative (captioning) and discriminative (VQA) hallucination benchmarks may not be as strongly correlated as previously assumed. Lastly, Table 2 demonstrates that MRGD continues to be effective when applied to newer and architecturally diverse MLLMs such as Llama-3.2-Vision and SmolVLM-2. Notably, our reward models can be readily applied to new MLLMs without retraining. 4.4. Applying MRGD on top of RLHF While the hallucination mitigation literature focuses primarily on the instruction fine-tuned LLaVA-1.5 model, here we assess the effectiveness of our method with more recent MLLMs that have been already fine-tuned with RLHF. We apply MRGD on top of Llama-3.2-Vision, which has undergone preference alignment phase (with DPO [35]) its multimodal after instruction fine-tuning. Crucially, preference data includes visual grounding examples [14], which makes it less prone to hallucinations. Table 2 shows that MRGD is also effective when applied to Llama-3.2-Vision. As expected, the improvement in object precision and recall is smaller compared to LLaVA1.5 since Llama-3.2-Vision already starts from better level of visual grounding. However, MRGD can further mitigate object hallucinations: when guiding decoding with the reward model for object hallucinations (w=1.0), we observe 1% and 2% reduction in instance-level hallucinations for COCO and AMBER respectively, 5% and 10% decrease in sentence-level hallucinations, and only slight decrease in object recall. One interesting observation is that Llama-3.2-Visions object recall on COCO is considerably lower (-10%) than that of LLaVA-1.5, which may be due to more conservative outputs as consequence of preference and safety finetuning. Guiding decoding with the reward model for object recall (w=0.0) boosts object recall in the generated captions by 2.7% on COCO and 4.7% on AMBER. 4.5. Visual grounding vs. compute trade-off By varying the number of samples and the evaluation period , we can control the trade-off between the degree of visual grounding in the generated outputs and the amount of compute used during decoding, for fixed guidance strength Table 3. Ablation results for LLaVA-1.57B. MRGD with k=30, =1, w=0.5. MRGDPG2: using PaliGemma-2 instead of PaliGemma for rhal, MRGD+RLAIF-V: removing RLAIF-V from the original data mix for rhal, MRGD+RLAIF-VPOVID: adding RLAIF-V and removing POVID from the original data mix, MRGDDETR: using DETR instead of OWLv2 as object detector for rrec, and MRGDτ =x: using instead of 0.5 as semantic similarity threshold for rrec. Decoding strategy COCO AMBER Ci () Cs () Rec. () Len. CHAIR () Hal. () Cov. () Greedy MRGD 15.05 5.34 48.94 22.54 81.30 78.63 90.12 97. rhal variants MRGDPG2 MRGD+RLAIF-V MRGD+RLAIF-VPOVID rrec variants MRGDDETR MRGDτ =0.2 MRGDτ =0.9 5.88 7.83 8.17 5.37 5.89 5.00 27.07 29.68 34.08 23.76 24.46 20. 78.76 77.54 79.03 82.04 78.09 78.36 105.25 94.26 104.04 99.24 106.86 98.09 7.6 4.4 4.1 6.3 5. 4.0 4.3 4.0 31.8 25.4 25.0 33.2 29.3 19.8 22.8 22.3 49.3 60.8 59.6 57.1 59. 53.5 54.5 61.2 more sample-efficient than rejection sampling. We observe similar trends for object recall in Figure 2b. Note that evaluating more frequently also increases computational cost, but to much lesser extent than generating, since the reward models are considerably smaller than the base MLLM and evaluation only requires single forward pass. 4.6. Object precision vs. recall trade-off Figure 3 shows the trade-off between hallucination rate (CHAIRi) and object recall when varying the guidance strength w={0.0, 0.1, 0.25, 0.5, 1.0} for fixed and . We observe lower leads to higher recall and lower precision (higher CHAIRi) and vice-versa. And the trade-off curve gets closer to the ideal curve with higher k. This suggests that there is an inherent trade-off between precision and recall in MLLMs. However, our approach gives user the flexibility to choose the operating point (by choosing value for and k) that suits their needs at inference time. 4.7. Preference data mix for rhal To understand the impact of different preference data compositions on the quality of rhal, we conduct an ablation over the datasets used for its training. Our base reward model is trained on mixture of LLaVA-RLHF [42] (9.4k), RLHF-V [49] (5.7k), and POVID [56] (17k). We consider an additional preference dataset, RLAIF-V [50] (83k), which contains 2.6 more examples than all previous datasets combined. We train two additional variants: (1) adding RLAIF-V and (2) adding RLAIF-V while removing POVID. As shown in Table 3, both adding RLAIF-V and removing POVID lead to notable performance degradation, highlighting the importance of carefully choosing the preference data mix to train rhal. Figure 3. Object precision and recall for LLaVA-1.5 on COCO, with =1. Varying modulates the precision-recall trade-off under fixed compute budget, while increasing compute via larger improves both precision and recall, significantly surpassing greedy search and bringing the trade-off curve closer to the ideal. and temperature t: expanding the search space (increasing k) and evaluating more frequently (decreasing ) increases visual grounding but also the required compute. Figure 2a shows how the reward value rhal and hallucination rate (CHAIRi) evolve as we increase the number of samples k={3, 5, 10, 30} and as we vary the evaluation period . As expected, we observe the hallucination rate decreases as we increase k. Notably, MRGD (with =1) is considerably more compute-efficient than naive rejection sampling (equivalent to =). For example, MRGD with k=5 achieves lower hallucination rate than rejection sampling with k=30, making MRGD over 6 7 (a) (b) Figure 4. Selected qualitative examples for LLaVA-1.5 using the default greedy decoding and our proposed MRGD strategy with different ws (k=30, =1). Correct objects are highlighted in green and hallucinated objects in red. New lines in captions are omitted for brevity. 4.8. MRGDs robustness to reward models quality To further assess the robustness of MRGD to variations in reward model quality, we evaluate the performance of our approach for variant of rhal with different model 8 [41] instead of PaliGemma), backbone (PaliGemma-23B and several variants of rrec: with different object detector 8google/paligemma2-3b-pt-224 (DETR [10] instead of OWLv2) and different semantic similarity thresholds (τ ). In Table 3, we observe that (1) upgrading rhals backbone yields similar performance, (2) using DETR for rrec performs similarly for COCO and decreases both hallucinations and coverage for AMBER, and (3) MRGD remains effective when varying τ . Overall, all ablated variants significantly outperform the greedy search baseline on most metrics, demonstrating the efTable 4. Average decoding times on NVIDIA A100 GPU. Decoding strategy Ci () Time (s) () Greedy (vLLM) BS@10 (HuggingFace) BS@30 (HuggingFace) MRGD@10 (vLLM) MRGD@30 (vLLM) 15.05 15.80 15.68 5.76 4. 1.35 0.49 5.26 1.15 15.14 2.75 6.79 1.93 14.78 4.38 fectiveness and robustness of our approach across reward design choices. 4.9. Computational cost and latency In terms of computational cost, finetuning PaliGemma on 30.6k preference examples requires only 9 minutes on 8 NVIDIA H100 GPUs. Furthermore, as shown in Table 4, spending more test-time compute with MRGD helps reduce hallucinations. Notably, while MRGD with k=30 generates 30 more text w.r.t. greedy decoding, its latency increase is significantly less than 30 due to batched generation. 4.10. Qualitative analysis Figure 4 illustrates qualitative differences in generated captions between using the default greedy decoding and our MRGD strategy, for the same input images. Greedy decoding produces captions which contain either more or less objects than those actually present in the images. For instance, in Figure 4a, the greedy caption incorrectly identifies the animal as brown bear, significantly hurting object precision, though it does correctly describe the colorful frisbees and rocky terrain. When using MRGD with w=1.0, the model produces an accurate caption with no hallucinations, correctly naming the black otter and both circular objects while capturing the investigative behavior, though it omits finer scene details like the ground texture. With w=0.5, the model misidentifies the animal as beaver but achieves good object recall, referencing the frisbees, their positions, and the dirt and rock area. At w=0.0, the caption achieves the highest object recall, correctly mentioning the otter, frisbees, and the surrounding environment with rocks and dirt, but introduces multiple hallucinated elements such as small boat close to the water and disc golf ball, which reduce object precision. Similarly, in Figure 4b, the greedy caption includes hallucinated staircase and building, while failing to mention the boat or river. With MRGD and w=1.0, the caption correctly identifies the hill, trees, river, autumn leaves, and boat without hallucinations, but it omits the terraced steps. At w=0.5, the caption recalls the hillside or cliff, foliage, trees, water and boat, but introduces false objects like wooden shed or pavilion and additional boats. At w=0.0, the caption reaches the highest object recall by naming the hill, steps, leaves, body of water, and boat, but suffers from severe hallucinations, adding train car, tracks, people, and bridge. 5. Conclusion In this paper, we presented MRGD, reward-guided decoding method for MLLMs based on multimodal reward models for visual grounding. We built two reward modelsone evaluating object precision in captions, another evaluating object recallused in an iterative search process that evaluates candidate responses against their combined reward values. Our methodology enables on-thefly controllability of MLLM-generated captions along two axes: controlling the object precision/recall trade-off by adjusting the weight of each reward model, and balancing test-time compute vs. visual grounding by varying the search breadth and frequency. Our method provides significant controllability over MLLM inference while matching or surpassing existing hallucination mitigation methods. Limitations and future work. In this work, we focused on mitigating object hallucinations primarily due to their ease of automatic evaluation, but other important visual hallucinations exist related to attributes, count, spatial relationships, negation, and more which we leave for future work. In addition, we would like to continue exploring: (1) building reward models for semantically incomplete outputs, (2) extending MRGD to discriminative hallucination tasks (e.g., POPE [25]), and (3) gradient-based optimization instead of search-based approaches."
        },
        {
            "title": "Acknowledments",
            "content": "We thank Saba Ahmadi, Qian Yang and Shravan Nayak for providing valuable feedback on an earlier draft of this work. During this project, Aishwarya Agrawal was supported by the Canada CIFAR AI Chair award."
        },
        {
            "title": "References",
            "content": "[1] Josh Achiam, Steven Adler, Sandhini Agarwal, Lama Ahmad, Ilge Akkaya, Florencia Leoni Aleman, Diogo Almeida, Janko Altenschmidt, Sam Altman, Shyamal Anadkat, et al. Gpt-4 technical report. arXiv preprint arXiv:2303.08774, 2023. 1 [2] Elmira Amirloo, Jean-Philippe Fauconnier, Christoph Roesmann, Christian Kerl, Rinu Boney, Yusu Qian, Zirui Wang, Afshin Dehghan, Yinfei Yang, Zhe Gan, et al. Understanding alignment in multimodal llms: comprehensive study. arXiv preprint arXiv:2407.02477, 2024. 2 [3] Zechen Bai, Pichao Wang, Tianjun Xiao, Tong He, Zongbo Han, Zheng Zhang, and Mike Zheng Shou. Hallucination of multimodal large language models: survey. arXiv preprint arXiv:2404.18930, 2024. 1 9 [4] Lucas Beyer, Andreas Steiner, Andre Susano Pinto, Alexander Kolesnikov, Xiao Wang, Daniel Salz, Maxim Neumann, Ibrahim Alabdulmohsin, Michael Tschannen, Emanuele Bugliarello, et al. Paligemma: versatile 3b vlm for transfer. arXiv preprint arXiv:2407.07726, 2024. 3 [5] Florian Bordes, Richard Yuanzhe Pang, Anurag Ajay, Alexander Li, Adrien Bardes, Suzanne Petryk, Oscar Manas, Zhiqiu Lin, Anas Mahmoud, Bargav Jayaraman, et al. An introduction to vision-language modeling. arXiv preprint arXiv:2405.17247, 2024. [6] Ralph Allan Bradley and Milton Terry. Rank analysis of incomplete block designs: I. the method of paired comparisons. Biometrika, 39(3/4):324345, 1952. 3 [7] Bradley Brown, Jordan Juravsky, Ryan Ehrlich, Ronald Clark, Quoc Le, Christopher Re, and Azalia Mirhoseini. Large language monkeys: Scaling inference compute with repeated sampling. arXiv preprint arXiv:2407.21787, 2024. 2, 4 [8] Tom Brown. Language models are few-shot learners. arXiv preprint arXiv:2005.14165, 2020. 1 [9] Davide Caffagni, Federico Cocchi, Luca Barsellotti, Nicholas Moratelli, Sara Sarto, Lorenzo Baraldi, Marcella Cornia, and Rita Cucchiara. The (r) evolution of multimodal large language models: survey. arXiv preprint arXiv:2402.12451, 2024. 1 [10] Nicolas Carion, Francisco Massa, Gabriel Synnaeve, Nicolas Usunier, Alexander Kirillov, and Sergey Zagoruyko. End-toend object detection with transformers. In European conference on computer vision, pages 213229. Springer, 2020. 8 [11] Sumanth Dathathri, Andrea Madotto, Janice Lan, Jane Hung, Eric Frank, Piero Molino, Jason Yosinski, and Rosanne Liu. Plug and play language models: simple approach to controlled text generation. In International Conference on Learning Representations, 2020. [12] Ailin Deng, Zhirui Chen, and Bryan Hooi. Seeing is believing: Mitigating hallucination in large visionlanguage models via clip-guided decoding. arXiv preprint arXiv:2402.15300, 2024. 2, 5, 1 [13] Haikang Deng and Colin Raffel. Reward-augmented decoding: Efficient controlled text generation with unidirectional In Proceedings of the 2023 Conference on reward model. Empirical Methods in Natural Language Processing, pages 1178111791, 2023. 1, 2 [14] Abhimanyu Dubey, Abhinav Jauhri, Abhinav Pandey, Abhishek Kadian, Ahmad Al-Dahle, Aiesha Letman, Akhil Mathur, Alan Schelten, Amy Yang, Angela Fan, et al. The llama 3 herd of models. arXiv preprint arXiv:2407.21783, 2024. 1, 4, 6 [15] Alessandro Favero, Luca Zancato, Matthew Trager, Siddharth Choudhary, Pramuditha Perera, Alessandro Achille, Ashwin Swaminathan, and Stefano Soatto. Multi-modal hallucination control by visual information grounding. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 1430314312, 2024. 2 [16] Seungwook Han, Idan Shenfeld, Akash Srivastava, Yoon Kim, and Pulkit Agrawal. Value augmented sampling for language model alignment and personalization. arXiv preprint arXiv:2405.06639, 2024. 2 [17] Cheng-Yu Hsieh, Jieyu Zhang, Zixian Ma, Aniruddha Kembhavi, and Ranjay Krishna. Sugarcrepe: Fixing hackable benchmarks for vision-language compositionality. Advances in neural information processing systems, 36, 2024. 4 [18] Edward Hu, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean Wang, Lu Wang, Weizhu Chen, et al. Lora: Lowrank adaptation of large language models. In International Conference on Learning Representations, 2022. [19] Maxim Khanov, Jirayu Burapacheep, and Yixuan Li. Args: In The Twelfth InterAlignment as reward-guided search. national Conference on Learning Representations, 2024. 1, 2 [20] Nathan Lambert, Valentina Pyatkin, Jacob Morrison, LJ Miranda, Bill Yuchen Lin, Khyathi Chandu, Nouha Dziri, Sachin Kumar, Tom Zick, Yejin Choi, et al. Rewardbench: Evaluating reward models for language modeling. arXiv preprint arXiv:2403.13787, 2024. 5 [21] Sicong Leng, Hang Zhang, Guanzheng Chen, Xin Li, Shijian Lu, Chunyan Miao, and Lidong Bing. Mitigating object hallucinations in large vision-language models through visual contrastive decoding. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 1387213882, 2024. 2, 5, 6, 1 [22] Alexander Lew, Tan Zhi-Xuan, Gabriel Grand, and Vikash Mansinghka. Sequential monte carlo steering of large language models using probabilistic programs. In ICML 2023 Workshop: Sampling and Optimization in Discrete Space, 2023. 2 [23] Bolian Li, Yifan Wang, Ananth Grama, and Ruqi Zhang. Cascade reward sampling for efficient decoding-time alignment. In ICML 2024 Next Generation of AI Safety Workshop, 2024. 2 [24] Lei Li, Zhihui Xie, Mukai Li, Shunian Chen, Peiyi Wang, Liang Chen, Yazheng Yang, Benyou Wang, Lingpeng Kong, and Qi Liu. Vlfeedback: large-scale ai feedback dataset for large vision-language models alignment. In Proceedings of the 2024 Conference on Empirical Methods in Natural Language Processing, pages 62276246, 2024. [25] Yifan Li, Yifan Du, Kun Zhou, Jinpeng Wang, Wayne Xin Zhao, and Ji-Rong Wen. Evaluating object hallucination in In Proceedings of the 2023 large vision-language models. Conference on Empirical Methods in Natural Language Processing, pages 292305, 2023. 9 [26] Fuxiao Liu, Kevin Lin, Linjie Li, Jianfeng Wang, Yaser Yacoob, and Lijuan Wang. Mitigating hallucination in large In The multi-modal models via robust instruction tuning. Twelfth International Conference on Learning Representations, 2024. 1, 2 [27] Haotian Liu, Chunyuan Li, Yuheng Li, and Yong Jae Lee. Improved baselines with visual instruction tuning. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 2629626306, 2024. 4, 5 [28] Jiacheng Liu, Andrew Cohen, Ramakanth Pasunuru, Yejin Choi, Hannaneh Hajishirzi, and Asli Celikyilmaz. Dont throw away your value model! generating more preferable text with value-guided monte-carlo tree search decoding. In First Conference on Language Modeling, 2024. 2 10 [29] Yufang Liu, Tao Ji, Changzhi Sun, Yuanbin Wu, and Aimin Zhou. Investigating and mitigating object hallucinations in pretrained vision-language (clip) models. In Proceedings of the 2024 Conference on Empirical Methods in Natural Language Processing, pages 1828818301, 2024. 2, 5 [30] Andres Marafioti, Orr Zohar, Miquel Farre, Merve Noyan, Elie Bakouch, Pedro Cuenca, Cyril Zakka, Loubna Ben Allal, Anton Lozhkov, Nouamane Tazi, Vaibhav Srivastav, Joshua Lochner, Hugo Larcher, Mathieu Morlon, Lewis Tunstall, Leandro von Werra, and Thomas Wolf. Smolvlm: Redefining small and efficient multimodal models. arXiv preprint arXiv:2504.05299, 2025. [31] Matthias Minderer, Alexey Gritsenko, and Neil Houlsby. Scaling open-vocabulary object detection. Advances in Neural Information Processing Systems, 36, 2024. 4 [32] Sidharth Mudgal, Jong Lee, Harish Ganapathy, YaGuang Li, Tao Wang, Yanping Huang, Zhifeng Chen, Heng-Tze Cheng, Michael Collins, Trevor Strohman, et al. Controlled decoding from language models. In Forty-first International Conference on Machine Learning, 2024. 1, 2 [33] Long Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida, Carroll Wainwright, Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, et al. Training language models to follow instructions with human feedback. Advances in neural information processing systems, 35:27730 27744, 2022. 3 [34] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, et al. Learning transferable visual models from natural language supervision. In International conference on machine learning, pages 87488763. PMLR, 2021. 3 [35] Rafael Rafailov, Archit Sharma, Eric Mitchell, Christopher Manning, Stefano Ermon, and Chelsea Finn. Direct preference optimization: Your language model is secretly reward model. Advances in Neural Information Processing Systems, 36, 2024. 6 [36] Ahmad Rashid, Ruotian Wu, Kristiadi, and Pascal Poupart. kenwise reward-guided text generation. arXiv:2406.07780, 2024. Julia Grosse, Agustinus tolook at arXiv preprint"
        },
        {
            "title": "A critical",
            "content": "[37] Nils Reimers and Iryna Gurevych. Sentence-bert: Sentence embeddings using siamese bert-networks. In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing. Association for Computational Linguistics, 2019. 4 [38] Anna Rohrbach, Lisa Anne Hendricks, Kaylee Burns, Trevor Darrell, and Kate Saenko. Object hallucination in image captioning. In Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, pages 40354045, 2018. 4, 1 [39] Pritam Sarkar, Sayna Ebrahimi, Ali Etemad, Ahmad Beirami, Sercan Arık, and Tomas Pfister. Mitigating object hallucination via data augmented contrastive tuning. arXiv preprint arXiv:2405.18654, 2024. 2, 5, 1 effective than scaling model parameters. arXiv:2408.03314, 2024. 1, 2, 4 arXiv preprint [41] Andreas Steiner, Andre Susano Pinto, Michael Tschannen, Daniel Keysers, Xiao Wang, Yonatan Bitton, Alexey Gritsenko, Matthias Minderer, Anthony Sherbondy, Shangbang Long, et al. Paligemma 2: family of versatile vlms for transfer. arXiv preprint arXiv:2412.03555, 2024. [42] Zhiqing Sun, Sheng Shen, Shengcao Cao, Haotian Liu, Chunyuan Li, Yikang Shen, Chuang Gan, Liang-Yan Gui, Yu-Xiong Wang, Yiming Yang, et al. Aligning large multimodal models with factually augmented rlhf. arXiv preprint arXiv:2309.14525, 2023. 1, 2, 4, 5, 7 [43] Gemini Team, Rohan Anil, Sebastian Borgeaud, Yonghui Wu, Jean-Baptiste Alayrac, Jiahui Yu, Radu Soricut, Johan Schalkwyk, Andrew Dai, Anja Hauth, et al. Gemini: family of highly capable multimodal models. arXiv preprint arXiv:2312.11805, 2023. 1 [44] David Wan, Jaemin Cho, Elias Stengel-Eskin, and Mohit Bansal. Contrastive region guidance: Improving grounding in vision-language models without training. arXiv preprint arXiv:2403.02325, 2024. 2 [45] Fei Wang, Wenxuan Zhou, James Huang, Nan Xu, Sheng Zhang, Hoifung Poon, and Muhao Chen. mdpo: Conditional preference optimization for multimodal large language models. arXiv preprint arXiv:2406.11839, 2024. 2, 5 [46] Junyang Wang, Yuhang Wang, Guohai Xu, Jing Zhang, Yukai Gu, Haitao Jia, Ming Yan, Ji Zhang, and Jitao Sang. An llm-free multi-dimensional benchmark for mllms hallucination evaluation. arXiv preprint arXiv:2311.07397, 2023. 4, 1 [47] Kevin Yang and Dan Klein. Fudge: Controlled text generation with future discriminators. In Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 35113535, 2021. [48] Shukang Yin, Chaoyou Fu, Sirui Zhao, Tong Xu, Hao Wang, Dianbo Sui, Yunhang Shen, Ke Li, Xing Sun, and Enhong Chen. Woodpecker: Hallucination correction for multimodal large language models. arXiv preprint arXiv:2310.16045, 2023. 2 [49] Tianyu Yu, Yuan Yao, Haoye Zhang, Taiwen He, Yifeng Han, Ganqu Cui, Jinyi Hu, Zhiyuan Liu, Hai-Tao Zheng, Maosong Sun, et al. Rlhf-v: Towards trustworthy mllms via behavior alignment from fine-grained correctional human feedback. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 13807 13816, 2024. 1, 2, 4, 7 [50] Tianyu Yu, Haoye Zhang, Yuan Yao, Yunkai Dang, Da Chen, Xiaoman Lu, Ganqu Cui, Taiwen He, Zhiyuan Liu, Tat-Seng Chua, et al. Rlaif-v: Aligning mllms through open-source ai feedback for super gpt-4v trustworthiness. arXiv preprint arXiv:2405.17220, 2024. 2, 7 [51] Zihao Yue, Liang Zhang, and Qin Jin. Less is more: Mitigating multimodal hallucination from an eos decision perspective. arXiv preprint arXiv:2402.14545, 2024. 5, 1 [40] Charlie Snell, Jaehoon Lee, Kelvin Xu, and Aviral Kumar. Scaling llm test-time compute optimally can be more [52] Ce Zhang, Zifu Wan, Zhehan Kan, Martin Ma, Simon Stepputtis, Deva Ramanan, Russ Salakhutdinov, Louis11 SelfPhilippe Morency, Katia Sycara, and Yaqi Xie. correcting decoding with generative feedback for mitigating hallucinations in large vision-language models. arXiv preprint arXiv:2502.06130, 2025. 2 [53] Zhuosheng Zhang, Aston Zhang, Mu Li, George Karypis, Alex Smola, et al. Multimodal chain-of-thought reasoning in language models. Transactions on Machine Learning Research, 2024. 1, 2 [54] Linxi Zhao, Yihe Deng, Weitong Zhang, and Quanquan Gu. Mitigating object hallucination in large visionlanguage models via classifier-free guidance. arXiv preprint arXiv:2402.08680, 2024. 2 [55] Zhiyuan Zhao, Bin Wang, Linke Ouyang, Xiaoyi Dong, Jiaqi Wang, and Conghui He. Beyond hallucinations: Enhancing lvlms through hallucination-aware direct preference optimization. arXiv preprint arXiv:2311.16839, 2023. 2, 5, 1 [56] Yiyang Zhou, Chenhang Cui, Rafael Rafailov, Chelsea Finn, and Huaxiu Yao. Aligning modalities in vision large lanIn ICLR 2024 guage models via preference fine-tuning. Workshop on Reliable and Responsible Foundation Models, 2024. 1, 2, 4, 5, [57] Yiyang Zhou, Chenhang Cui, Jaehong Yoon, Linjun Zhang, Zhun Deng, Chelsea Finn, Mohit Bansal, and Huaxiu Yao. Analyzing and mitigating object hallucination in large vision-language models. In The Twelfth International Conference on Learning Representations, 2024. 2 [58] Yiyang Zhou, Zhiyuan Fan, Dongjie Cheng, Sihan Yang, Zhaorun Chen, Chenhang Cui, Xiyao Wang, Yun Li, Linjun Zhang, and Huaxiu Yao. Calibrated self-rewarding vision language models. arXiv preprint arXiv:2405.14622, 2024. 2, 5 12 Controlling Multimodal LLMs via Reward-guided Decoding"
        },
        {
            "title": "Supplementary Material",
            "content": "6. Experiments 6.1. Details on evaluation metrics We evaluate object precision and recall with standard metrics from the corresponding benchmarks, defined as follows. CHAIRi (Ci) [38], CHAIR [46]. Measure the fraction of hallucinated objects in the generated captions. Ci/CHAIR = {hallucinated objects} {all mentioned objects} CHAIRs (Cs) [38], Hal. [46]. Measure what fraction of generated captions include hallucinated object. Cs/Hal. = {captions with hallucinated object} {all captions} Recall (Rec.), Coverage (Cov.) [46]. Measure the fraction of ground-truth objects covered in the generated captions. Rec./Cov. = {correct objects} {all ground-truth objects} 6.2. Details on reporting results of existing methods In Table 1, we report results for existing hallucination mitigation methods from the best source available. Unless otherwise specified, values are directly copied from the corresponding papers. For HA-DPO [55] and EOS [51], values are copied from Sarkar et al. [39] since their evaluation setup matches ours. For LLaVA-RLHF [42] and VCD [21], we compute results by generating captions with the original code and evaluating them on CHAIR [38] and AMBER [46], since the original papers do not report hallucination results on these benchmarks. For CGD [12], we also run the original code to generate captions for both AMBER and the full standard set of 5000 examples in the CHAIR benchmark (instead of the 500-example subset used by Deng et al. [12]). 6.3. Prompting baseline We propose multimodal reward-guided decoding (MRGD) the behavior of MLLMs at as method to control inference time. common approach to steer the beinference time is prompting [8]. havior of LLMs at Avoid making assumptions, Here, we apply the same idea to MLLMs as an alternative approach to control their behavior. To mitigate visual hallucinations in image captioning, we use the instruction {captioning instruction}. Provide an accurate and objective description, focusing on verifiable visual elements such as colors, textures, shapes, and compositions. inferences, or introducing information not present in the image, where the captioning instruction is the one described in Section 4.1: Describe this image in detail for LLaVA-1.5 and Describe this image in few sentences for Llama-3.2Vision. We maintain greedy decoding for the prompting baselines. In Tables 1 and 2, we observe that prompting slightly reduces object hallucinations compared to greedy decoding for LLaVA-1.5, while for Llama-3.2-Vision, surprisingly, it does not help much and, in fact, it increases the sentence-level hallucination rate (CHAIRs and Hal.). Instead, with LLaVA-1.5 on COCO, for similar level of object recall (81%), MRGD with w=0.25 achieves better object precision by 5.8% CHAIRi and 11.4% CHAIRs compared to prompting. This suggests that prompting is not very effective strategy to steer MLLMs towards complex behaviors such as reducing visual hallucinations. 6.4. Using SigLIP for rhal CGD [12] can be viewed as particular instance of MRGD when using off-the-shelf SigLIP as the reward model for object hallucinations and removing the combination of multiple reward models (i.e., setting w=1.0). Therefore, we also conduct an ablation of MRGD replacing PaliGemma fine-tuned on preference data (Section 3.1.1) with off-theshelf SigLIP-SoViT-400m 9. Due to SigLIPs limited context length of 64 tokens, we only evaluate the last generated sentence, unlike PaliGemma which receives the full prefix response (which may contain several sentences). To ensure that the scores from multiple reward models are comparable and can be combined effectively, we normalize their ranges. In particular, since the effective range of SigLIP scores is much narrower than that of the reward model for object recall (rrec [0, 1]), we linearly rescale SigLIP scores Rk to cover the range [0, 1]: = (r min(r))/(max(r) min(r) + ϵ), where min and max are computed across the set of candidate samples , and ϵ is small value to avoid division by zero (in case all candidates obtained the same score). In Table 5, we ob9google/siglip-so400m-patch14-384 1 Table 5. Additional results for LLaVA-1.57B. MRGD with k=30 and =1. MRGDPaliGemma: MRGD using PaliGemma fine-tuned on preference data for rhal, MRGDSigLIP: MRGD using off-the-shelf SigLIP for rhal. Decoding strategy COCO AMBER Ci () Cs () Rec. () Len. CHAIR () Hal. () Cov. () Greedy MRGDPaliGemma,w=1.0 MRGDPaliGemma,w=0.75 MRGDPaliGemma,w=0.5 MRGDPaliGemma,w=0.25 MRGDw=0. MRGDSigLIP,w=1.0 MRGDSigLIP,w=0.75 MRGDSigLIP,w=0.5 MRGDSigLIP,w=0.25 15.05 4.53 4.76 5.34 7.67 24.20 7.19 7.57 8.17 10.84 48.94 18.19 19.28 22.54 32.63 73.42 28.00 29.58 32.88 43.58 81.30 76.04 76.84 78.63 81.56 85. 73.71 74.30 75.96 79.50 90.12 95.90 96.17 97.96 105.34 108.92 92.73 93.17 94.93 99.57 7.6 3.4 3.2 4.4 6.5 14.8 6.0 6.1 6.3 8.5 31.8 15.9 17.3 25.4 37.7 65. 30.1 30.3 33.3 46.2 49.3 52.4 56.7 60.8 63.8 64.3 48.5 50.0 53.4 57.8 serve that when using SigLIP-based rhal, our MRGD strategy is still effective in reducing object hallucinations and enabling the user to trade off object precision and recall onthe-fly at inference time. However, SigLIP does not allow to reach the same level of object precision, and the trade-off with object recall is also worse. For instance, when w=1.0, MRGDPaliGemma achieves better object precision by 2.7% CHAIRi and 9.8% CHAIRs, and better Recall by 2.3% compared to MRGDSigLIP."
        }
    ],
    "affiliations": [
        "Canada CIFAR AI Chair",
        "McGill University",
        "Meta FAIR",
        "Mila - Quebec AI Institute",
        "Universite de Montreal"
    ]
}