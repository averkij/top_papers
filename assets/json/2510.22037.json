{
    "paper_title": "ATLAS: Adaptive Transfer Scaling Laws for Multilingual Pretraining, Finetuning, and Decoding the Curse of Multilinguality",
    "authors": [
        "Shayne Longpre",
        "Sneha Kudugunta",
        "Niklas Muennighoff",
        "I-Hung Hsu",
        "Isaac Caswell",
        "Alex Pentland",
        "Sercan Arik",
        "Chen-Yu Lee",
        "Sayna Ebrahimi"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Scaling laws research has focused overwhelmingly on English -- yet the most prominent AI models explicitly serve billions of international users. In this work, we undertake the largest multilingual scaling laws study to date, totaling 774 multilingual training experiments, spanning 10M-8B model parameters, 400+ training languages and 48 evaluation languages. We introduce the Adaptive Transfer Scaling Law (ATLAS) for both monolingual and multilingual pretraining, which outperforms existing scaling laws' out-of-sample generalization often by more than 0.3 R^2. Our analyses of the experiments shed light on multilingual learning dynamics, transfer properties between languages, and the curse of multilinguality. First, we derive a cross-lingual transfer matrix, empirically measuring mutual benefit scores between 38 x 38=1444 language pairs. Second, we derive a language-agnostic scaling law that reveals how to optimally scale model size and data when adding languages without sacrificing performance. Third, we identify the computational crossover points for when to pretrain from scratch versus finetune from multilingual checkpoints. We hope these findings provide the scientific foundation for democratizing scaling laws across languages, and enable practitioners to efficiently scale models -- beyond English-first AI."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 4 2 ] . [ 1 7 3 0 2 2 . 0 1 5 2 : r : ADAPTIVE TRANSFER SCALING LAWS ATLAS FOR MULTILINGUAL PRETRAINING, FINETUNING, AND DECODING THE CURSE OF MULTILINGUALITY Shayne Longpre1 Sneha Kudugunta2 Niklas Muennighoff3 I-Hung Hsu4 Isaac Caswell5 Alex Pentland1 Sercan Ö. Arık4 Chen-Yu Lee4 Sayna Ebrahimi5 1MIT 2University of Washington 3Stanford University 4Google Cloud AI 5Google DeepMind"
        },
        {
            "title": "ABSTRACT",
            "content": "Scaling laws research has focused overwhelmingly on Englishyet the most prominent AI models explicitly serve billions of international users. In this work, we undertake the largest multilingual scaling laws study to date, totaling 774 multilingual training experiments, spanning 10M-8B model parameters, 400+ training languages and 48 evaluation languages. We introduce the ADAPTIVE TRANSFER SCALING LAW (ATLAS) for both monolingual and multilingual pretraining, which outperforms existing scaling laws out-of-sample generalization often by more than 0.3 R2. Our analyses of the experiments shed light on multilingual learning dynamics, transfer properties between languages, and the curse of multilinguality. First, we derive cross-lingual transfer matrix, empirically measuring mutual benefit scores between 38 38 = 1444 language pairs. Second, we derive language-agnostic scaling law that reveals how to optimally scale model size and data when adding languages without sacrificing performance. Third, we identify the computational crossover points for when to pretrain from scratch versus finetune from multilingual checkpoints. We hope these findings provide the scientific foundation for democratizing scaling laws across languages, and enable practitioners to efficiently scale modelsbeyond English-first AI."
        },
        {
            "title": "INTRODUCTION",
            "content": "Scaling laws research has focused overwhelmingly on English (Kaplan et al., 2020; Hoffmann et al., 2022; Li et al., 2025a). However, most of the major closed and open models now explicitly target massively multilingual uses (OpenAI, 2025; Anthropic, 2025; Google DeepMind, 2025; DeepSeekAI, 2024; Team OLMo et al., 2024). Nonetheless, multilingual scaling laws research in the public sphere is limited. Among proprietary lab reports, only Llama-3 briefly discuss their multilingual scaling laws, though they train on only 8% non-English tokens (Dubey et al., 2024). Prominent public work investigates scaling laws for data mixing (Goyal et al., 2024; Ye et al., 2024; Ge et al., 2024a), scaling laws for machine translation (Fernandes et al., 2023; Gordon et al., 2021), multilingual instruction tuning/adaptation (Shaham et al., 2024; Lai et al., 2024; Weber et al., 2024), and only couple of recent rigorous contributions investigate scaling for smaller multilingual models (<45M and <1.2B respectively) (Chang et al., 2024; He et al., 2024). Extending these prior works, we seek to fill the ample remaining knowledge gaps with comprehensive examinations of the following research questions. First, we explore how the properties of different languages scaling laws differ. Second, we measure the cross-lingual transfer benefits between 38 languages. To our knowledge, this represents the most comprehensive available empirical resource to explicitly measure language transfer across 38 38 = 1444 language pairs. Third, we succeed in modeling the curse of multilingualitythe phenomena where adding languages to the training mixture can degrade loss for each language, due to limited model capacity. Lastly, we measure when it is more efficient to pretrain from scratch, or finetune from general-purpose multilingual checkpoint, resolving an unaddressed practical question. In pursuing these research quesCorrespondence: slongpre@media.mit.edu 1 Figure 1: Optimal Scaling Trajectories for English, French, Russian, Chinese, Hindi, and Swahili. The law for [monolingual vocabulary, monolingual training]=(), the law for [multilingual vocabulary, monolingual training]=(- - -), and the law for [multilingual vocabulary, unimax training]=(). We find (1) per-language optimal scaling trajectories are similar, (2) there is compute efficiency tax for training with multilingual vocabularies or training sets (especially for English), and (3) as Hindi and Swahili observe data repetition their curves slope upward from diminishing returns. tions, we develop new tools and estimates for multilingual practitioners, as well as the ADAPTIVE TRANSFER SCALING LAW, which significantly outperforms prior work across several dimensions of generalization. In total, our pretraining and finetuning experiments span 774 independent experiments on MADLAD-400 (Kudugunta et al., 2024), for models sized 10M 2B, and evaluating 48 languages. Our main contributions include: 1. The ADAPTIVE TRANSFER SCALING LAW that offers better multilingual generalization to larger/unseen N, D, C, and training mixes , than prior work  (Table 1)  . In Figure 1 we also estimate compute efficiency tax between multilingual and monolingual training. 2. 38 38 CROSS-LINGUAL TRANSFER MATRIX, providing the largest resource for empirically measured transfer benefits/interference between languages (Figure 2). 3. scaling law for the curse of multilinguality, that informs practitioners how much to scale (N, D) to accommodate expansions in models language coverage (Figure 5). 4. general pretrain vs finetune formula, that informs practitioners whether it is more efficient to pretrain from scratch or begin from multilingual Unimax checkpoint (Figure 7)."
        },
        {
            "title": "2 EXPERIMENTAL SETUP",
            "content": "Datasets and Evaluation We use the MADLAD-400 dataset (Kudugunta et al., 2024), popular CommonCrawl-based dataset with the most expansive coverage of languages, totaling over 400. The MADLAD-400 authors prioritized multilingual-specific curation for this pretraining corpus, including language detection, filtering, and preprocessing. For 50 languages, chosen to represent range of language families, scripts, and degrees of resourcefulness, we partition random test set, to evaluate vocabulary-insensitive loss (Tao et al., 2024) for fairer cross-lingual comparisons. We evaluate the fit of our fitted scaling laws using R2 calculated on held-out test sets partitioned to assess specific dimensions of generalization: R2(N ) for the largest model sizes, R2(D) for the largest token ranges, R2(C) for the largest compute runs, and R2(M ) for unique training language mixtures not seen at training. We separate these dimensions in response to prior work that has demonstrated the importance of rigorous test-set splits in scaling laws research (Li et al., 2025a). Model Training We pretrain models from 10M to 8B parameters, using hyperparameter choices similar to those trained in Kudugunta et al. (2024) (with updates as prescribed by recent work). In this work, we train both monolingual and multilingual models, varying the model size , training tokens D, and multilingual data proportions. In particular, we focus on training monolingual models, bilingual models, and massively multilingual Unimax models (Chung et al., 2023). most experiments center around scale, token, and mixture variations on these languages: English, French, 2 Chinese, Hindi, Swahili, Russian, Spanish, Portuguese, Japanese, though certain experiments evaluate across 50 languages. We use 64k Sentence Piece Model vocabulary (Kudo, 2018). Across experiments we pretrain 280 monolingual models, 240 bilingual models, 120 multilingual mixtures, and we finetune 130 monolingual models from Unimax checkpoints, totaling over 750 independent training runs. Full experimental details, including the specific language choices, hyperparameters, and training mixtures are provided in full in Section B. Table 1: The R2 evaluation metrics for the fitted scaling laws, holding out separate dimensions of generalization: the largest model sizes , most training tokens D, most compute C, and unique multilingual training mixtures . In both monolingual and multilingual settings, we average R2 across languages=[EN, FR, RU, ZH, HI, SW], including [ES, DE] for the multilingual setting. We find ADAPTIVE TRANSFER SCALING LAW outperforms prior work in Monolingual and Multilingual settings. We ablate the use of the terms for Dother and transfer languages ((cid:80) Di). Kt SCALING LAWS R2 R2(N ) R2(D) R2(C) R2(M ) . CHINCHILLA SCALING LAW (Hoffmann et al., 2022) M 0.94 DATA-CONSTRAINED SCALING LAW (Muennighoff et al., 2024) 0.93 [Ours] ATLAS (Dt ONLY) 0.92 0.68 0.78 0. . U CHINCHILLA SCALING LAW (Hoffmann et al., 2022) MULTILINGUAL SCALING LAW (He et al., 2024) [Ours] ATLAS (Dt ONLY) [Ours] ATLAS (Dt + Dother) [Ours] ATLAS (Dt + Dother + (cid:80) Di) Kt 0.64 -0.99 0.67 -0.65 0.70 -0.75 0.89 0.98 0.89 0.98 0.94 0.93 0. 0.72 0.73 0.80 0.97 0.96 0.90 0.88 0.88 0.66 0.67 0.72 0.97 0.98 0.61 0.70 0.64 0.66 0."
        },
        {
            "title": "SETTINGS",
            "content": "Challenges with existing scaling laws for multilingual modeling. In this section, we discuss our attempts to precisely model different monolingual and multilingual pretraining experiments. Scaling laws for English are typically based on Chinchilla (Hoffmann et al., 2022)which we refer to as the CHINCHILLA SCALING LAW (CSL). But beyond English, languages may have different scaling laws (Arnett & Bergen, 2025), and language resources are often limited, requiring DATACONSTRAINED SCALING LAW (DCSL) (Muennighoff et al., 2024) to account for diminishing returns after multiple epochs. However, DCSL requires ample data both before and after one epoch to accommodate its two-stage fitting process. For high-resource languages (English, French, Chinese), it is costly to collect ample data beyond one epoch, and for low/mid-resource languages (even Hindi, Hebrew, and Swahili) it can be very difficult to collect sufficient observations below one epoch.1 As such, even for monolingual modeling, we require scaling law that is data repetition-aware, and robust to different collection allowances for (N, D). For modeling multilingual mixtures, monolingual scaling laws can be used (either with representing the total or target language data), or He et al. (2024) introduce MULTILINGUAL SCALING LAW (MSL), which expresses the loss for target language using N, and the sampling ratio for the target languages language family in the training mixture. However, each of these solutions only accounts for one of: the sampled target language data Dt, the total data altogether Dt + Dother, or set of the target and likely positive transfer languages Dt + (cid:80) Di. We posit that better model would separate and learn to weight these independent contributions. In summary, none of the existing multilingual solutions account for (a) multi-epoch data repetition, or (b) the cross-lingual transfer effects beyond the targets language family. L(N, Deff ) = + α + Dβ eff (1) 1In MADLAD-400 these languages have < 0.3% English tokens. At standard batch sizes, even frequently evaluating (every 1k) isnt enough to collect many (sometimes any) observations before 1 epoch is reached. 3 Deff = Sλ(Dt; Ut) (cid:124) (cid:125) (cid:123)(cid:122) Monolingual (cid:88) + τi Sλ(Di; Ui) iK (cid:124) (cid:123)(cid:122) Transfer Languages (cid:125) + τother Sλ(Dother; Uother) (cid:125) (cid:123)(cid:122) Other Languages (cid:124) Sλ(D; ) = (cid:40)D, (cid:104) 1 + 1exp(λ(D/U 1)) λ (cid:105) ( 1 epoch) , > (> 1 epoch) (2) (3) The ADAPTIVE TRANSFER SCALING LAW. To resolve these challenges, we introduce the ADAPTIVE TRANSFER SCALING LAW (ATLAS), simpler variant of DCSL that is repetition-aware, introduces fewer additional parameters (for the monolingual variant), is fit in one stage, and is adaptable to an open-ended number of languages that would benefit from an explicit cross-lingual term. In Equation (1) the core scaling law formula includes the standard parameters governing the irreducible loss and N, scaling: E, A, B, α, β. Its effective data exposure term Deff (Equation (2)) unpacks data sources into three terms: Monolingual term for the target language data Dt, an optional Transfer Language term for up to Kt languages we wish to learn independent transfer coefficients for τi, and an Other Languages remainder term that sums all training tokens not already accounted for in the first two terms Dother = Dtot Dt (cid:80) Di. The latter two terms are optionally added for multilingual modeling, and Kt can be adapted as any combination of the languages with presumed highest positive transfer to the target language, or languages with the greatest representation in the experiment training mixture. We found the latter was most effective and selected the Kt = 3 most highly co-sampled languages along target language across mixtures. For each term, we apply saturation function (Equation (3)), ensuring smooth decay on the effective data for each subsequent epoch where the training tokens have surpassed that languages unique tokens . The new parameters introduced over standard scaling laws are: the repetition parameter λ, which is shared across each data source, the transfer weights τi for each language in Kt (which are initialized from language transfer scores derived in Section 4), and the transfer weight τother for the remainder of language tokens. Kt Research Question: How do scaling laws differ by language, and by monolingual vs multilingual training mixtures? Monolingual scaling behavior is consistent in form, and multilingual variants exhibit variable-sized compute-efficiency tax. Figure 1 compares optimal scaling trajectories across sixvariable resourced languages, under three regimes: monolingual vocabulary + monolingual training, multilingual vocabulary + monolingual training, and multilingual vocabulary + unimax training. Across languages, the monolingual curves are nearly parallel, indicating similar exponents and comparable returns to additional data and parameters (see full law parameters in Table C.1). Both multilingual vocabulary and Unimax training shift the frontier upwards, evidencing computeefficiency tax relative to the monolingual-vocabulary/monolingual-training setting. This is most pronounced for English, indicating it benefits less from language transfer than other languages do from English. For Hindi and Swahili, the right tail bends upward, consistent with diminishing returns from severe data repetition after many epochs. These qualitative trends motivate single functional form that remains stable across languages while accounting for vocabulary/training-regime effects. Research Question: How well can our scaling laws capture unique monolingual constraints, and complex multilingual cross-lingual transfer dynamics? ATLAS outperforms prior scaling laws, with more robust fit across held-out axes in monolingual and multilingual settings. Table 1 evaluates fitted laws by holding out the largest model sizes , token ranges D, compute = 6N D, and held-out language mixtures , reporting R2 averaged over available languages. In the monolingual setting, all scaling laws perform comparably across most dimensions of generalization. However, when generalizing to the largest size of model, laws that model data repetition outperform those that dont. ATLAS provides the strongest generalization to greater model sizes: R2(N ) = 0.88 versus 0.68 (CSL) and 0.78 (DCSL), respectively. In the multilingual setting, monolingual scaling laws can only observe their target data tokens, and as such perform adequately, but not well. In particular, they are unable to generalize to the largest models R2(N ) at all. This is also the case for MSL, although it obtains better generalization to unseen language mixture in R2(M ) = 0.69 > 0.64 by virtue of modeling the whole language family. By separating the sources of data, ATLAS is able to significantly outperform the other laws 4 across all dimensions, frequently achieving > 0.9 fit. However, our ablation of the data terms shows that only by adding the Transfer Language term is the model able to outperform MSL and achieve better multilingual mixture generalization R2(M ) = 0.82. In summary, ATLAS offers simple framework to adaptively model cross-lingual transfer, and enables reliable extrapolation across out-of-sample dimensions in both monolingual and multilingual settingsaddressing critical gap where existing approaches fall short."
        },
        {
            "title": "4 HOW DO LANGUAGES BENEFIT OR INTERFERE WITH EACH OTHER?",
            "content": "Research Question: Which languages synergize or interfere most with one anothers performance? Figure 2: The CROSS-LINGUAL TRANSFER MATRIX, depicting the measured Language Transfer Score across 30 30 language pairs. Positive scores indicate more positive transfer, negative scores more interference, during bilingual co-training. The dashed boxes indicate the top-5 source languages for each target language. Refer to Appendix B.6 for full details, and Figure C.2 for the larger 38 38 matrix. While English is the best source language for many of the languages, we find language similarity is highly predictive of these scores. When allocating multilingual training mixtures, practitioners need to know which source language(s) provide the most positive benefit, in co-training, to the target language(s) they are optimizing for. Throughout this paper, we use the terms transfer or interference to refer to the positive (or negative) inductive transfer between the training signal of two or more tasks (Caruana, 1997). We construct, to our knowledge, the most comprehensive matrix of language-to-language transfer 5 Figure 3: Left: language transfer scatter plot, comparing score symmetry: is language as helpful to as vice versa? Points cluster near the diagonal, indicating strong symmetry. The most synergistic and symmetric pairs almost exclusively share both language family and script. Greater linguistic distance correlates with increased asymmetry and reduced positive transfer. Right: The impact of linguistic similarity (via language family or script) on transfer scores. The box spans the inter-quartile range, with the median line at the center. We find the differences between each group are statistically significant (p < .001), suggesting that sharing either language family or script independently contribute greater positive cross-lingual transfer. scores, to assist practitioners in selecting training languages. Language Transfer Score (s t) = BTSst = σbi(Lt(dmono)) 2dmono dmono To do this, we measure how much training in source language affects the test loss on target language t. We define Bilingual Transfer Score (BTS) as the relative training efficiency of bilingual model (s, t) = (50%, 50%) compared to the monolingual model at reaching the same loss level.2 dmono denotes pre-defined target step (42B tokens), Lt(d) computes the monolingual models loss at step d, and σbi finds the number of tokens for the bilingual model to reach that loss. Because BTSst > 0 is the zero-centered, when it is = 0 there is no transfer, > 0 there is positive transfer, and < 0 there is negative interference, leading to the bilingual model taking more than double the steps dmono the monolingual model took to reach the target loss Lt(dmono). Refer to Subsection B.5 for full experimental details and methodology. In Figure 2, we plot the normalized BTS scores between 30 30 language pairs (or the full 38 38 in Figure C.2), spanning language families and scripts, on 2B parameter models. Where prior work has developed comprehensive resources for measuring syntactic/phonological distances between languages (Khan et al., 2025), or measured transfer scores with smaller models specifically between high and low-resource languages (Protasov et al., 2024), our work offers among the most expansive and rigorous, fully-symmetric transfer matrices, to our knowledge. It contains many compelling observations. For instance, as one might expect, notable positive transfer exists between related languages, such as between Spanish, Catalan, and Portuguese, or Swedish and Norwegian, or Indonesian and Malay. The top-5 most helpful source languages to co-train with per target are highlighted, with English appearing the most (19 of 30 instances), followed by French (16 of 30), Spanish (13 of 30), and even Hebrew (11 of 30). In other cases, low-resource languages such as Urdu or Pashto have significant negative transfer with all other languages considered. Language script, followed by language family are strongly correlated with positive transfer scores. In Figure 3, we correlate the transfer scores from the full language matrix Figure 2. Our analysis confirms clear and statistically significant link between transfer performance and the similarity of the source and target language. Specifically, we found sharing script or family introduced statistically significant shifts in the mean of the transfer score (always < .001). These findings 2We measure BTSst directly for 80 language pairs, then estimate it using other training signals (with high fidelity R2 = 0.85), as detailed in Subsection B.6. 6 Figure 4: We empirically measure the relative degradation in loss (y-axis), as compared to monolingual model of the same (N, D), from adding pretraining languages (z-axis). Left: We fix = 25B tokens, and vary the model size . Right: We fix = 2B parameters, and vary the tokens D. The points are real empirical observations, averaged across languages. The mesh-grid is surface estimate, using cubic spline. Target language loss is most affected by the number of training languages, but this loss penalty declines for larger models with more capacity. corroborate (He et al., 2024)s scaling laws that grouped data by language family. This effect is strongest for shared scripts. As shown in Figure 3, language pairs that share the same writing system (e.g. Latin) exhibit dramatically improved transfer scores compared to pairs with disparate scripts (e.g., Latin and Cyrillic), with mean score of 0.23 versus 0.39. The larger effect size for script suggests that the ability to share surface-level representations and subword vocabularies is primary mechanism for positive transfer, more so than deeper grammatical or lexical similarities alone. In Section 5 and Subsection C.2 we explore how N, affect language transfer. Language transfer scores are often symmetric within the same language family and script, but surprisingly, they cannot be assumed to be reciprocal otherwise. Next, we examine the extent to which language transfer is symmetric, that is, whether the benefit from language to is correlated to that from to A. As illustrated in Figure 3 (left), we find surprising triangle pattern, with Pearson correlation of = 0.11. This implies two things. First, and perhaps most importantly, across all language pairs, there is no significant correlation, and because we observe language is helpful to language B, we cannot assume the same in reverse. This corroborates findings in Li et al. (2025b) that altruistic languages dont always yield mutual benefits. Second, there does appear to be clear structure to the scatter: language pairs that share family and script (blue) cluster mostly in the top right quadrant and more tightly around the identity line, whereas language pairs with differing script and family (red) are simultaneously less symmetric, and less synergistic. For instance, pairs (French, Spanish) and (Russian, Ukrainian) share highly symmetric transfer scores, whereas (Chinese, Farsi) and (Russian, Vietnamese) are highly asymmetric. We hope these findings, as well as the Figure 2 transfer matrix, will be directly applicable to practitioners selecting language mixtures based on empirical measurements, rather than intuition."
        },
        {
            "title": "5 THE CURSE OF MULTILINGUALITY—MODEL CAPACITY CONSTRAINTS",
            "content": "Research Question: Is the curse of multilinguality measurablethe phenomenon where adding languages to the training mixture can degrade loss of each language, due to limited model capacity? models capacity can hinder its ability to learn multiple languages at onceknown as the curse of multilinguality (Conneau et al., 2019; Chang et al., 2024). We can empirically measure this relationship, between , D, and the number of training languages K, by running experiments with variable K. Full experimental details and scaling law derivations are provided in Subsection B.7. Modeling the Relationship between K, , and D. In Figure 4 we examine the relative loss increase, over monolingual baseline, from varying the number of training languages K, and either 7 the model size (left plot) or total training tokens Dtot (right plot). For simplicity, we dont explicitly model token repetition, and we sample tokens from each language uniformlywe believe this is reasonable assumption for models designed to serve all languages. We observe three phenomena. First, the number of training languages has far more impact on the relative loss than or D. Relative loss appears to grow monotonically as increases. Second, both increasing model size and total tokens Dtot mitigate this penalty. However, computing the partial along the fitted surface shows S/ log > S/ log D; indicating delivers larger marginal gains than Dtot. Consequently, maintaining performance as grows requires scaling both data and model size, but scaling the model size has greater effect. To understand the relationship among these variables more precisely, we model pertargetlanguage loss as L(K, N, Dt) = + K ϕ α + ψ Dβ , where, under even sampling the total tokens across languages are Dtot = Dt. We tested several scaling law variations for this research question, but settled on this one as (a) it retains Chinchillastyle power-law decay properties that cleanly separate model capacity from data (it also reduces to Chinchilla when K=1), (b) it makes the role of the number of languages explicit and interpretable, and (c) it achieved robust R2 0.87, indicating strong fit. The exponent ϕ captures how capacity requirements grow with the number of languages, while ψ captures how data requirements change with multilinguality. ψ < 0 indicates positive transfer (sublinear data needs per language as increases), and ψ > 0 implies negative transfer/interference. We fit the scaling law for each of 8 different languages, as well as the combination of all, achieving similar coefficients, and > 0.8 R2 for each. Using the scaling law fit on all language data we find ϕ = 0.11 and ψ = 0.04, indicating mild capacity-driven curse of multilinguality, tempered by positive transfer across languages. In other words, as languages are added to the training mixture, positive ϕ means the loss will decline from limited model capacity, however, negative ψ means that less data per language is needed. (Though the total amount of data increases, as we sample from new languages too.) This provides clear, measurable evidence of positive cross-lingual transfer. Iso-Loss Frontier, Iso-loss from expanding language Figure 5: coverage: When scaling the number of languages model serves from rK, we can estimate how much they need to increase model size /N and/or training tokens tot/Dtot, without degrading any of their languages loss. We derive the compute optimal scaling equations. The iso-losses indicate that practitioner should expand their compute budget by r0.97 to expand their language coverage by r. Estimating the and Compute-Optimal scaling of N, when adding languages: rK. Next, we examine the practical case where practitioner wants to retrain new model, increasing the number of languages it serves from to rK, without hindering the performance the original model achieved on the existing languages. To accommodate these new languages, they will need to scale with some combination of and Dtot tot. We can compute α + B(Kr)ψ this iso-loss by equating the loss terms: L(K, N, Dtot) = AKϕ α + BKψ . From this we can derive the closed-form expression for how to scale (N, Dtot) when increasing to rK (as derived in Subsection B.7): Dβ = A(Kr)ϕ Dβ (rK) (K) = rϕ/α, (rK) (K) = rψ/β, tot(rK) tot(K) = 1+ψ/β. We can also derive the minimal increase in the compute budget: = tot Dtot = (cid:16) (cid:17)(cid:16) tot Dtot (cid:17) = r1+ϕ/α+ψ/β . 8 In Figure 5 we plot the iso-loss frontiers, and compute-optimal allocations, when scaling model from to languages. It shows in closed-form how practitioner should scale Dtot, , and by extension their compute budget to accommodate additional languages, without degrading their loss. For instance, we find expanding to 4 languages requires practitioner to expand Dtot by 2.74, and the model size by 1.4. Incidentally, evenly sampling across languages, this actually corresponds to using sampling 1 (2.74/4) = 32% less data per language. Although there are fewer tokens in each target language (by 32%), the total tokens has risen by 2.74, and the positive transfer prevents any potential loss degradation. Figure 6: For eight languages, we plot the loss curves for 2B parameter models pretraining monolingually from scratch (), finetuning monolingually from the Unimax base model (- - -), and the difference between these losses (). We annotate the number of tokens at which the pretrained loss becomes better than the finetuning loss. The difference between the interpolated pretraining curve and finetuning curve. When the loss difference reaches zero, pretraining from scratch has surpassed finetuning using equal compute. Depending on the token (or compute) budget, it is usually more effective to finetune multilingual checkpoint if there are < 144B tokens, and pretrain from scratch if the budget accommodates 283B tokens."
        },
        {
            "title": "6 PRETRAIN OR FINETUNE?",
            "content": "Research Question: When training model for target language t, is it more effective to pretrain from scratch, or finetune general-purpose massively multilingual checkpoint? Prior scaling law work focuses on compute efficiency with respect to pretraining from scratch. However, in reality, practitioners who aim to optimize for target language may have the option to start from multilingual public checkpoints or pretrain one multilingual checkpoint to serve as the starting point for many downstream models. Consider practitioner with compute budget C. In Figure 6, we plot the interpolated loss curves for the model pretrained from scratch, the model finetuned from checkpoint, and the difference between these losses, for several languages. We find that while the warm-started finetuned model performs better at first, pretraining from scratch eventually surpasses its performance after 144B 283B tokens, depending on the language. Note that we leave the reason for these convergence differences to future work, without clear hypothesis. Though English pretraining converges the fastest, it is also allocated 5% sampling rate In terms of , we model the comFigure 7: pute budget required for model pretrained from scratch to outperform the model finetuned from the generic Unimax multilingual checkpoint. We estimate this relationship as log(C) = 10283128 1.65. within Unimax, whereas each of the other languages are allocated 1.4%. 9 From the number of training tokens we can also infer the inflection point which decides whether it is more computationally efficient (C = 6N D) to pretrain from scratch or finetune from the multilingual checkpoint. Using our range of model size experiments, we can estimate best fit power law in Figure 7, relating to C. We find log(C) = 1113708 1.65 to hold consistently across languages. For simplicity, we choose not to account for data repetition in this model. Practitioners may use this to estimate whether pretraining or finetuning will yield greater performance for their compute budget. Note one limitation to consider: not all multilingual base models are trained with the same mixture or for sufficiently longand these factors would impact the pretrain vs finetuning intersection points. We choose widely used Unimax mixture trained for 1B tokens (Chung et al., 2023). We believe this is useful heuristic, with reasonable assumptions, to determine the optimal choice between pretraining and finetuning for given compute budget."
        },
        {
            "title": "7 ACKNOWLEDGEMENTS",
            "content": "We thank Luke Zettlemoyer, Catherine Arnett and Stella Biderman for helpful discussions on the paper. We thank Biao Zhang and Xavier Garcia for the technical disccusions and feedback on early directions."
        },
        {
            "title": "REFERENCES",
            "content": "Armen Aghajanyan, Lili Yu, Alexis Conneau, Wei-Ning Hsu, Karen Hambardzumyan, Susan Zhang, Stephen Roller, Naman Goyal, Omer Levy, and Luke Zettlemoyer. Scaling laws for generative mixed-modal language models. In International Conference on Machine Learning, pp. 265279. PMLR, 2023. Ibrahim Alabdulmohsin, Behnam Neyshabur, and Xiaohua Zhai. Revisiting neural scaling laws in language and vision. Advances in Neural Information Processing Systems, 35:2230022312, 2022. Alon Albalak, Yanai Elazar, Sang Michael Xie, Shayne Longpre, Nathan Lambert, Xinyi Wang, Niklas Muennighoff, Bairu Hou, Liangming Pan, Haewon Jeong, et al. survey on data selection for language models. arXiv preprint arXiv:2402.16827, 2024. Dario Amodei, Sundaram Ananthanarayanan, Rishita Anubhai, Jingliang Bai, Eric Battenberg, Carl Case, Jared Casper, Bryan Catanzaro, Qiang Cheng, Guoliang Chen, et al. Deep speech 2: End-toend speech recognition in english and mandarin. In International conference on machine learning, pp. 173182. PMLR, 2016. Anthropic. System card: port, Anthropic, May 2025. 4263b940cabb546aa0e3283f35b686f4f3b2ff47.pdf. Claude opus 4 & claude resonnet 4. URL https://www-cdn.anthropic.com/ Technical Newsha Ardalani, Carole-Jean Wu, Zeliang Chen, Bhargav Bhushanam, and Adnan Aziz. Understanding scaling laws for recommendation models. arXiv preprint arXiv:2208.08489, 2022. Catherine Arnett and Benjamin Bergen. Why do language models perform worse for morphologically complex languages? In Proceedings of the 31st International Conference on Computational Linguistics, pp. 66076623, 2025. Michele Banko and Eric Brill. Scaling to very very large corpora for natural language disambiguation. In Proceedings of the 39th annual meeting of the Association for Computational Linguistics, pp. 2633, 2001. Yamini Bansal, Behrooz Ghorbani, Ankush Garg, Biao Zhang, Colin Cherry, Behnam Neyshabur, and Orhan Firat. Data scaling laws in nmt: The effect of noise and architecture. In International Conference on Machine Learning, pp. 14661482. PMLR, 2022. Ankur Bapna, Isaac Caswell, Julia Kreutzer, Orhan Firat, Daan van Esch, Aditya Siddhant, Mengmeng Niu, Pallavi Baljekar, Xavier Garcia, Wolfgang Macherey, Theresa Breiner, Vera Axelrod, Jason Riesa, Yuan Cao, Mia Xu Chen, Klaus Macherey, Maxim Krikun, Pidong Wang, Alexander Gutkin, Apurva Shah, Yanping Huang, Zhifeng Chen, Yonghui Wu, and Macduff Hughes. Building Machine Translation Systems for the Next Thousand Languages. arXiv e-prints, art. arXiv:2205.03983, May 2022. David Brandfonbrener, Nikhil Anand, Nikhil Vyas, Eran Malach, and Sham Kakade. Loss-to-loss prediction: Scaling laws for all datasets. arXiv preprint arXiv:2411.12925, 2024. Rich Caruana. Multitask learning. Machine learning, 28(1):4175, 1997. Tyler Chang, Catherine Arnett, Zhuowen Tu, and Ben Bergen. When is multilinguality curse? language modeling for 250 high-and low-resource languages. In Proceedings of the 2024 Conference on Empirical Methods in Natural Language Processing, pp. 40744096, 2024. Yangyi Chen, Binxuan Huang, Yifan Gao, Zhengyang Wang, Jingfeng Yang, and Heng Ji. Scaling laws for predicting downstream performance in llms. arXiv preprint arXiv:2410.08527, 2024. Mehdi Cherti, Romain Beaumont, Ross Wightman, Mitchell Wortsman, Gabriel Ilharco, Cade Gordon, Christoph Schuhmann, Ludwig Schmidt, and Jenia Jitsev. Reproducible scaling laws for contrastive language-image learning. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 28182829, 2023. 11 Hyung Won Chung, Noah Constant, Xavier Garcia, Adam Roberts, Yi Tay, Sharan Narang, and Orhan Firat. Unimax: Fairer and more effective language sampling for large-scale multilingual pretraining. arXiv preprint arXiv:2304.09151, 2023. Aidan Clark, Diego de Las Casas, Aurelia Guy, Arthur Mensch, Michela Paganini, Jordan Hoffmann, Bogdan Damoc, Blake Hechtman, Trevor Cai, Sebastian Borgeaud, et al. Unified scaling laws for routed language models. In International conference on machine learning, pp. 4057 4086. PMLR, 2022. Alexis Conneau, Kartikay Khandelwal, Naman Goyal, Vishrav Chaudhary, Guillaume Wenzek, Francisco Guzmán, Edouard Grave, Myle Ott, Luke Zettlemoyer, and Veselin Stoyanov. Unsupervised cross-lingual representation learning at scale. arXiv preprint arXiv:1911.02116, 2019. DeepSeek-AI. Deepseek-v3 technical report, 2024. URL https://arxiv.org/abs/2412. 19437. Abhimanyu Dubey, Abhinav Jauhri, Abhinav Pandey, Abhishek Kadian, Ahmad Al-Dahle, Aiesha Letman, Akhil Mathur, Alan Schelten, Amy Yang, Angela Fan, et al. The llama 3 herd of models. arXiv preprint arXiv:2407.21783, 2024. Patrick Fernandes, Behrooz Ghorbani, Xavier Garcia, Markus Freitag, and Orhan Firat. Scaling laws for multilingual neural machine translation. arXiv preprint arXiv:2302.09650, 2023. Elias Frantar, Carlos Riquelme, Neil Houlsby, Dan Alistarh, and Utku Evci. Scaling laws for sparsely-connected foundation models. arXiv preprint arXiv:2309.08520, 2023. Leo Gao, John Schulman, and Jacob Hilton. Scaling laws for reward model overoptimization. In International Conference on Machine Learning, pp. 1083510866. PMLR, 2023. Ce Ge, Zhijian Ma, Daoyuan Chen, Yaliang Li, and Bolin Ding. Bimix: bivariate data mixing law for language model pretraining. arXiv preprint arXiv:2405.14908, 2024a. Ce Ge, Zhijian Ma, Daoyuan Chen, Yaliang Li, and Bolin Ding. Data mixing made efficient: bivariate scaling law for language model pretraining. arXiv preprint arXiv:2405.14908, 2024b. Behrooz Ghorbani, Orhan Firat, Markus Freitag, Ankur Bapna, Maxim Krikun, Xavier Garcia, Ciprian Chelba, and Colin Cherry. Scaling laws for neural machine translation. arXiv preprint arXiv:2109.07740, 2021. Google DeepMind. Gemini 2.5 deep think model card. Technical report, Google DeepMind, August 2025. URL https://storage.googleapis.com/deepmind-media/ Model-Cards/Gemini-2-5-Deep-Think-Model-Card.pdf. Model card published/updated August 1, 2025. Mitchell Gordon, Kevin Duh, and Jared Kaplan. Data and parameter scaling laws for neural machine translation. In Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, pp. 59155922, 2021. Naman Goyal, Cynthia Gao, Vishrav Chaudhary, Peng-Jen Chen, Guillaume Wenzek, Da Ju, Sanjana Krishnan, MarcAurelio Ranzato, Francisco Guzmán, and Angela Fan. The flores-101 evaluation benchmark for low-resource and multilingual machine translation. 2021. Sachin Goyal, Pratyush Maini, Zachary Lipton, Aditi Raghunathan, and Zico Kolter. Scaling laws for data filteringdata curation cannot be compute agnostic. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 2270222711, 2024. Albert Gu and Tri Dao. Mamba: Linear-time sequence modeling with selective state spaces. arXiv preprint arXiv:2312.00752, 2023. Alexander Hägele, Elie Bakouch, Atli Kosson, Loubna Ben Allal, Leandro Von Werra, and Martin Jaggi. Scaling laws and compute-optimal training beyond fixed training durations. arXiv preprint arXiv:2405.18392, 2024. Tatsunori Hashimoto. Model performance scaling with multiple data sources."
        },
        {
            "title": "In International",
            "content": "Conference on Machine Learning, pp. 41074116. PMLR, 2021. Yifei He, Alon Benhaim, Barun Patra, Praneetha Vaddamanu, Sanchit Ahuja, Parul Chopra, Vishrav Chaudhary, Han Zhao, and Xia Song. Scaling laws for multilingual language models. arXiv preprint arXiv:2410.12883, 2024. Tom Henighan, Jared Kaplan, Mor Katz, Mark Chen, Christopher Hesse, Jacob Jackson, Heewoo Jun, Tom Brown, Prafulla Dhariwal, Scott Gray, et al. Scaling laws for autoregressive generative modeling. arXiv preprint arXiv:2010.14701, 2020. Joel Hestness, Sharan Narang, Newsha Ardalani, Gregory Diamos, Heewoo Jun, Hassan Kianinejad, Md Mostofa Ali Patwary, Yang Yang, and Yanqi Zhou. Deep learning scaling is predictable, empirically. arXiv preprint arXiv:1712.00409, 2017. Jacob Hilton, Jie Tang, and John Schulman. Scaling laws for single-agent reinforcement learning. arXiv preprint arXiv:2301.13442, 2023. Jordan Hoffmann, Sebastian Borgeaud, Arthur Mensch, Elena Buchatskaya, Trevor Cai, Eliza Rutherford, Diego de Las Casas, Lisa Anne Hendricks, Johannes Welbl, Aidan Clark, et al. Training compute-optimal large language models. arXiv preprint arXiv:2203.15556, 2022. Shengding Hu, Yuge Tu, Xu Han, Chaoqun He, Ganqu Cui, Xiang Long, Zhi Zheng, Yewei Fang, Yuxiang Huang, Weilin Zhao, et al. Minicpm: Unveiling the potential of small language models with scalable training strategies. arXiv preprint arXiv:2404.06395, 2024. Andy Jones. Scaling scaling laws with board games. arXiv preprint arXiv:2104.03113, 2021. Feiyang Kang, Yifan Sun, Bingbing Wen, Si Chen, Dawn Song, Rafid Mahmood, and Ruoxi Jia. Autoscale: Scale-aware data mixing for pre-training llms. arXiv preprint arXiv:2407.20177, 2024. Jared Kaplan, Sam McCandlish, Tom Henighan, Tom B. Brown, Benjamin Chess, Rewon Child, Scott Gray, Alec Radford, Jeffrey Wu, and Dario Amodei. Scaling laws for neural language models. 2020. Aditya Armaan Khan, Mason Stephen Shipton, David Anugraha, Kaiyao Duan, Phuong Hoang, Eric Khiu, Seza Dogruöz, and Annie Lee. Uriel+: Enhancing linguistic inclusion and usability in typological and multilingual knowledge base. In Proceedings of the 31st International Conference on Computational Linguistics, pp. 69376952, 2025. Kudo. Sentencepiece: simple and language independent subword tokenizer and detokenizer for neural text processing. arXiv preprint arXiv:1808.06226, 2018. Sneha Kudugunta, Isaac Caswell, Biao Zhang, Xavier Garcia, Derrick Xin, Aditya Kusupati, Romi Stella, Ankur Bapna, and Orhan Firat. Madlad-400: multilingual and document-level large audited dataset. Advances in Neural Information Processing Systems, 36, 2024. Wen Lai, Mohsen Mesgar, and Alexander Fraser. Llms beyond english: Scaling the multilingual capability of llms with cross-lingual feedback. In Findings of the Association for Computational Linguistics ACL 2024, pp. 81868213, 2024. Margaret Li, Sneha Kudugunta, and Luke Zettlemoyer. (mis) fitting: survey of scaling laws. arXiv preprint arXiv:2502.18969, 2025a. Zihao Li, Shaoxiong Ji, Hengyu Luo, and Jörg Tiedemann. Rethinking multilingual continual pretraining: Data mixing for adapting llms across languages and resources. arXiv preprint arXiv:2504.04152, 2025b. Qian Liu, Xiaosen Zheng, Niklas Muennighoff, Guangtao Zeng, Longxu Dou, Tianyu Pang, Jing Jiang, and Min Lin. Regmix: Data mixture as regression for language model pre-training. arXiv preprint arXiv:2407.01492, 2024. 13 Shayne Longpre, Gregory Yauney, Emily Reif, Katherine Lee, Adam Roberts, Barret Zoph, Denny Zhou, Jason Wei, Kevin Robinson, David Mimno, et al. pretrainers guide to training data: Measuring the effects of data age, domain coverage, quality, & toxicity. arXiv preprint arXiv:2305.13169, 2023. Ilya Loshchilov and Frank Hutter. Decoupled weight decay regularization. In International Conference on Learning Representations, 2019. URL https://openreview.net/forum?id= Bkg6RiCqY7. Niklas Muennighoff, Alexander Rush, Boaz Barak, Teven Le Scao, Nouamane Tazi, Aleksandra Piktus, Sampo Pyysalo, Thomas Wolf, and Colin Raffel. Scaling data-constrained language models. Advances in Neural Information Processing Systems, 36, 2024. OpenAI. Gpt-5 system card. Technical report, OpenAI, August 2025. URL https://cdn. openai.com/gpt-5-system-card.pdf. Vitaly Protasov, Elisei Stakovskii, Ekaterina Voloshina, Tatiana Shavrina, and Alexander Panchenko. Super donors and super recipients: Studying cross-lingual transfer between highresource and low-resource languages. In Proceedings of the Seventh Workshop on Technologies for Machine Translation of Low-Resource Languages (LoResMT 2024), pp. 94108, 2024. Haoran Que, Jiaheng Liu, Ge Zhang, Chenchen Zhang, Xingwei Qu, Yinghao Ma, Feiyu Duan, Zhiqi Bai, Jiakai Wang, Yuanxing Zhang, et al. D-cpt law: Domain-specific continual pre-training scaling law for large language models. Advances in Neural Information Processing Systems, 37: 9031890354, 2024. Machel Reid, Nikolay Savinov, Denis Teplyashin, Dmitry Lepikhin, Timothy Lillicrap, Jeanbaptiste Alayrac, Radu Soricut, Angeliki Lazaridou, Orhan Firat, Julian Schrittwieser, et al. Gemini 1.5: Unlocking multimodal understanding across millions of tokens of context. arXiv preprint arXiv:2403.05530, 2024. Teven Le Scao, Thomas Wang, Daniel Hesslow, Lucile Saulnier, Stas Bekman, Saiful Bari, Stella Biderman, Hady Elsahar, Niklas Muennighoff, Jason Phang, et al. What language model to train if you have one million gpu hours? arXiv preprint arXiv:2210.15424, 2022. Uri Shaham, Jonathan Herzig, Roee Aharoni, Idan Szpektor, Reut Tsarfaty, and Matan Eyal. Multilingual instruction tuning with just pinch of multilinguality. In Findings of the Association for Computational Linguistics ACL 2024, pp. 23042317, 2024. Luisa Shimabucoro, Ahmet Ustun, Marzieh Fadaee, and Sebastian Ruder. post-trainers guide arXiv preprint to multilingual training data: Uncovering cross-lingual transfer dynamics. arXiv:2504.16677, 2025. Mustafa Shukor, Louis Bethune, Dan Busbridge, David Grangier, Enrico Fini, Alaaeldin El-Nouby, and Pierre Ablin. Scaling laws for optimal data mixtures. arXiv preprint arXiv:2507.09404, 2025. Ben Sorscher, Robert Geirhos, Shashank Shekhar, Surya Ganguli, and Ari Morcos. Beyond neural scaling laws: beating power law scaling via data pruning. Advances in Neural Information Processing Systems, 35:1952319536, 2022. Chaofan Tao, Qian Liu, Longxu Dou, Niklas Muennighoff, Zhongwei Wan, Ping Luo, Min Lin, and Ngai Wong. Scaling laws with vocabulary: Larger models deserve larger vocabularies. arXiv preprint arXiv:2407.13623, 2024. Yi Tay, Mostafa Dehghani, Samira Abnar, Hyung Won Chung, William Fedus, Jinfeng Rao, Sharan Narang, Vinh Tran, Dani Yogatama, and Donald Metzler. Scaling laws vs model architectures: How does inductive bias influence scaling? arXiv preprint arXiv:2207.10551, 2022. Gemini Team, Petko Georgiev, Ving Ian Lei, Ryan Burnell, Libin Bai, Anmol Gulati, Garrett Tanzer, Damien Vincent, Zhufeng Pan, Shibo Wang, et al. Gemini 1.5: Unlocking multimodal understanding across millions of tokens of context. arXiv preprint arXiv:2403.05530, 2024. 14 Team OLMo, Pete Walsh, Luca Soldaini, Dirk Groeneveld, Kyle Lo, Shane Arora, Akshita Bhagia, Yuling Gu, Shengyi Huang, Matt Jordan, Nathan Lambert, Dustin Schwenk, Oyvind Tafjord, Taira Anderson, David Atkinson, Faeze Brahman, Christopher Clark, Pradeep Dasigi, Nouha Dziri, Michal Guerquin, Hamish Ivison, Pang Wei Koh, Jiacheng Liu, Saumya Malik, William Merrill, Lester James V. Miranda, Jacob Morrison, Tyler Murray, Crystal Nam, Valentina Pyatkin, Aman Rangapur, Michael Schmitz, Sam Skjonsberg, David Wadden, Christopher Wilhelm, Michael Wilson, Luke Zettlemoyer, Ali Farhadi, Noah A. Smith, and Hannaneh Hajishirzi. 2 olmo 2 furious, 2024. URL https://arxiv.org/abs/2501.00656. Alexander Arno Weber, Klaudia Thellmann, Jan Ebert, Nicolas Flores-Herr, Jens Lehmann, Michael Fromm, and Mehdi Ali. Investigating multilingual instruction-tuning: Do polyglot models demand for multilingual instructions? In EMNLP, 2024. Sang Michael Xie, Hieu Pham, Xuanyi Dong, Nan Du, Hanxiao Liu, Yifeng Lu, Percy Liang, Quoc Le, Tengyu Ma, and Adams Wei Yu. Doremi: Optimizing data mixtures speeds up language model pretraining. Advances in Neural Information Processing Systems, 36, 2024. Jiasheng Ye, Peiju Liu, Tianxiang Sun, Yunhua Zhou, Jun Zhan, and Xipeng Qiu. Data mixing laws: Optimizing data mixtures by predicting language modeling performance. arXiv preprint arXiv:2403.16952, 2024. Xiaohua Zhai, Alexander Kolesnikov, Neil Houlsby, and Lucas Beyer. Scaling vision transformers. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pp. 1210412113, 2022."
        },
        {
            "title": "Table of Contents",
            "content": "A Extended Related Work . . . . . . . . . Experimental Details . B.1 Language Choice . B.2 Experiment Design . . B.3 Experimental Details: Scaling Laws Evaluation . . . B.4 Experimental Details: Language Finetuning . . . B.5 Experimental Details: Language Transfer . . . B.6 Estimating Bilingual Transfer Score (BTS) . . . B.7 Experimental Details: Multilingual Capacity . . . . . . . . . . . . . . . . . . . Extended Results C.1 Extended Results: Scaling Laws . . C.2 Extended Results: Language Transfer . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 18 18 19 19 19 20 24 24 27 27"
        },
        {
            "title": "A EXTENDED RELATED WORK",
            "content": "Scaling Laws Scaling laws are used to study the behavior of deep learning systems on scaling training data and/or compute. Various researchers have observed scaling properties of generalization error with training data size and model capacity (Banko & Brill, 2001; Hestness et al., 2017; Amodei et al., 2016). Specifically, in the context of LLMs, Kaplan et al. (2020) explicitly propose power law for the relationship between the loss L, number of parameters in the language model , and the number of dataset tokens D. Later, Hoffmann et al. (2022) proposed different formula: L(N, D) = + α + Dβ (4) We build upon this form throughout the paper to fit our scaling laws. Other researchers, too, have built upon these scaling laws to study various aspects of scaling, such as neural network architectures (Clark et al., 2022; Tay et al., 2022; Frantar et al., 2023; Gu & Dao, 2023; Scao et al., 2022) or transfer learning (Henighan et al., 2020). Researchers have also investigated scaling properties in different domains of deep learning such as neural machine translation (Ghorbani et al., 2021; Gordon et al., 2021), vision language models like CLIP (Cherti et al., 2023; Henighan et al., 2020), vision (Alabdulmohsin et al., 2022; Zhai et al., 2022), reinforcement learning (Hilton et al., 2023; Jones, 2021; Gao et al., 2023), and recommendation systems (Ardalani et al., 2022). Scaling Laws for Data Mixtures Many prior works (Hoffmann et al., 2022) assume fixed dataset distribution to study the effect of scaling on model performance. However, empirically, dataset composition can have significant impact on quality (Longpre et al., 2023; Albalak et al., 2024; Sorscher et al., 2022). Hashimoto (2021) studies the relationship between dataset composition and model loss, finding simple scaling law quantifying this relationship. Bansal et al. (2022) study the impact of data quality and noise on architecture, finding that synthetic data has different exponent. Fernandes et al. (2023) study scaling for multilingual neural translation (Bapna et al., 2022), but experiments are restricted to 2-3 language pairs. Aghajanyan et al. (2023), in similar vein, propose bimodal scaling laws for multimodal foundation models (Reid et al., 2024). Other researchers have considered different aspects of dataset composition that can affect model scaling: Muennighoff et al. (2024) study the effect of data repetition on scaling, while Goyal et al. (2024) consider how the effect of mixing data pools of varying quality changes with scale. Foundation models involve training models on mixture of datasets with the aim of achieving certain validation loss on various validation sets and downstream datasets of interest. Based on the observation that optimal data mixture changes with scale, several recent works have attempted to characterize the change in the scaling law equation when either the train or test distribution changes. Some researchers predict the optimal data mixture by first ablating mixtures using smaller models, and second, training larger scaled-up model using the found optimal mixture (Ye et al., 2024; Xie et al., 2024; Liu et al., 2024). However, it is unclear if this optimal data mixture can extrapolate to much larger magnitude, with some evidence in computer vision that this is not the case (Goyal et al., 2024) - i.e., the optimal data mixture changes with scale. Some researchers have described scaling laws for different downstream evaluation sets for the same model (Dubey et al., 2024; Chen et al., 2024), while others (Brandfonbrener et al., 2024) have derived power law relationships between models trained on different distributions and the same model on different test sets. Recently, several researchers have attempted to predict the final loss of model for given data mixture with scaling laws. Ge et al. (2024b) use the observation of logarithmic shift in scaling pattern to predict loss on subsets of the Pile weighted in various proportions on fixed model size. Ye et al. (2024), too, fit scaling law for data mixtures based on single scale. Kang et al. (2024) empirically show that the optimal mixture for model changes with scale, and propose method that addresses this on GPT-2 scale models. Que et al. (2024) use continual pre-training as tool to develop their scaling law. Shukor et al. (2025), too, develop scaling laws that account for the transfer between data mixtures changing at scale for English LLMs and multimodal models for less than 10 domains. Multilingual Scaling Laws Prior work has also investigated multilingual transfer effects specifically in pretraining (Chang et al., 2024) and post-training (Shimabucoro et al., 2025), though on smaller scale of models. He et al. (2024) is the work closest to ours, where the authors attempt to describe scaling laws for multilingual LLMs. crucial distinction from our work is that we account for individual-language transfer learning in our scaling laws, and achieve better resulting fit  (Table 1)  ."
        },
        {
            "title": "B EXPERIMENTAL DETAILS",
            "content": "To understand multilingual scaling characteristics we run 774 separate experiments, across different model scales, and language mixtures. In Table B.1 we summarize the experiment segments, using the notation below, that defines sets of languages or scales. Lmono (7): {en, fr, ru, zh, hi, sw, yo} Lpairs (10): {en, fr, ru, zh, hi, de, es, pt, ja, vi} Ltarget (15): en/X for Lpairs + {es/pt, fr/zh, hi/ru, de/ja, hi/vi} Leval (48): complete evaluation set (all 48 languages) Cexp (12): {4 4, 6, 8 2, 12, 16, 24, 32, 50} Sfull (20): {0, 1, 2, 4, 6, 8, 12, 14, 16, 18, 20, 22, 24, 26, 28, 30, 32, 34, 42, 46} Spart (11): {0, 4, 8, 12, 16, 20, 24, 28, 34, 42, 46} Smin (7): {0, 8, 16, 24, 34, 42, 46} To see the model sizes defined by the scales in Sfull, Spart, and Smin, see Table B.3. We select these scale ranges to empirically observe models from 10M to 8B parameters. Subsets of languages, or subsets of the full scale (Spart, or Smin) are chosen to ensure the experiment number is sufficiently tractable for each research question, but also computationally feasible given our resources. Table B.1: An overview of experiment configurations in this work. We enumerate the experiment types: <Lang> for monolingual scaling, Unimax as massively multilingual baseline, Language Pairs to measure language-to-language transfer, Capacity to measure the curse of multilinguality, or model capacity constraints on learning new languages, and Finetunes to understand how finetuning from massively multilingual model compares to pretraining from scratch. We use mix of Monolingual and Multilingual vocabularies, and training data. In the LANGUAGES and SCALES columns we use parentheses to show the number of language mixtures and number of scales run. Symbols such as Lmono (7) and Sfull (20) are defined above. EXPERIMENT TAG VOCAB. TRAIN-DATA LANGUAGES Mono <Lang> Unimax Multi <Lang> Language Pairs Language Pairs Language Pairs Capacity Finetunes Finetunes Monolingual Multilingual Monolingual Monolingual Bilingual Bilingual Monolingual Multilingual Multilingual Multilingual Multilingual Multilingual Multilingual Uniform Sampling Multilingual Multilingual Monolingual Monolingual Lmono (7) Lunimax Lmono (7) Leval (48) Lpairs (10) Ltarget (15) Cexp (12) Leval (48) Lmono (7)Lpairs (10) SCALES Sfull (20) Sfull (20) Spart (11) 34 34 Spart (11) Spart (11) 34 Smin (7) # EXPS 7 20 = 140 20 10 7 = 70 50 90 10 15 = 150 10 12 = 120 50 7 12 = 84 Total 774 B.1 LANGUAGE CHOICE The fifty languages we selected come from the intersection of languages covered by both Flores-200 and MADLAD-400. They were selected to cover wide range of language families, scripts, and resource levels. To get this collection, we sorted all languages in the Flores-MADLAD intersection by the number of characters, and then selected every six languages. We then went over this list and 18 perturbed the index up or down if it was too typologically similar to an already-selected language. For example, Language 48 was Afrikaans, but there were already several Germanic languages on the list, so we opted to take language 47 instead (Telugu). B.2 EXPERIMENT DESIGN Pretraining Hyperparameter Details We rely on the hyperparameter settings refined by prior work to obtain reliable performance across model sizes. We also conducted initial experiments, varying batch sizes, learning rates, optimizer, and dropout to ensure our settings were relatively robust. In table B.2 we enumerate and explain all our hyperparameter choices, citing related work. Train & Test Data We use two test sets. For each we measure the vocabulary-insensitive loss proposed in Tao et al. (2024) in order to fairly evaluate across languages. 1. MADLAD-400 Test We isolate test set from each of our 50 evaluation languages in MADLAD-400 (Kudugunta et al., 2024). In initial experiments we find stable metrics over random seeds requires sampling roughly (sequence length num instances) 204810000 = 20,480,000 tokens. For each language we randomly sample the minimum of either 20M tokens or 20% of the total tokens for low resource languages min(20M tokens, 20%). 2. Flores-101 We use Flores-101 (Goyal et al., 2021), language-parallel set of 101 highquality translations, as comparable test set. We extract the monolingual sequences the test set, using those to compute sequence losses, comparable to MADLAD-400 Test. Vocabulary Details We use vocabulary trained by Kudugunta et al., with 64000 sized vocabulary, = 100 and 99.9995% character coverage. B.3 EXPERIMENTAL DETAILS: SCALING LAWS EVALUATION Typically, to evaluate fitted scaling laws, prior work has adopted the R2 metric, for its scaleinvariant measure of laws explanatory power over the observed data. However, the choice of what data to hold-out as the test set can have significant impact on the performance and its interpretation (Li et al., 2025a). For this reason, and because multilingual scaling laws are used to measure more than just the predictive power of the law to larger scales, we use set of different R2 metrics to measure different objectives. These are: 1. R2 Hold-out randomly selected 20% of the data points from the data. This gives an impression of the models ability to fit the variance, without measuring particular dimensions of fit. 2. R2(D) Hold-out the training runs with training tokens with the top 20% of tokens. 3. R2(N ) Hold-out couple of scales of model sizes, including the largest: = 660M and = 8B parameters. 4. R2(C) Hold-out the largest compute parts of the training runs, where = 6N FLOPs. This will include mainly combination of the larger model sizes, and longer training runs. 5. R2(M ) Hold-out all mixtures that are not monolingual, bilingual, or unimax. This allows the scaling law to get baseline sense of each interacting language, but it has to infer the rest of the mixture scaling properties. These metric variants allow us to unpack specifically where the scaling laws perform well, and for what purposes they are reliable. B.4 EXPERIMENTAL DETAILS: LANGUAGE FINETUNING In Section 6 we experiment with continuous pretraining on single language, starting from massively multilingual models checkpoint. For clarity, we refer to this as finetuning, to distinguish it from pretraining from scratch. We begin by training massively multilingual models, using UNIMAX (Chung et al., 2023) language sampling across all 420 languages in MADLAD-400. Chung et al. (2023) demonstrate this is an 19 Table B.2: The pretraining hyperparameters used across experiments. We detail the batch size scheduling, the vocabulary choice, as well as fine-grained hyperparameter choices. Each choice is justified and grounded in prior work, discussed in the Explanation column. PARAMETER VALUE EXPLANATION General Training Parameters Learning Rate Scheduler WSD We adopt the Warmup Stable Decay (WSD) learning rate schedule, designed to efficiently study data-model scaling law without extensive retraining (Hu et al., 2024; Hägele et al., 2024) Optimizer AdamW Commonly used in scaling law experiments (Loshchilov & HutBase Learning Rate Warmup Steps Decay Period Sequence Length Training Steps Dropout 2e1000 10% 2048 30k+ 0.1 ter, 2019; Hoffmann et al., 2022) Following (Muennighoff et al., 2024) and confirmation it works well in initial experiments. Standard practice, as in (Longpre et al., 2023) Following (Hu et al., 2024; Hägele et al., 2024) this is the minimum well performing decay length. As in (Longpre et al., 2023; Muennighoff et al., 2024). We vary the steps according to experiments, to always ensure we are well above likely chinchilla compute optimal ranges. While English scaling laws work tends to use dropout of 0.0, we see little notable difference, except for lower resource languages, with repeating epochs. Batch Size Schedule Batch Size (<150M params) 256 512 Batch Size (<1B params) 1024 Batch Size (<2B params) 2048 Batch Size (2B+ params) Hu et al. (2024); Muennighoff et al. (2024) both find the optimal batch size increases with model size. We adopt slightly larger rising batch size than Muennighoff et al. (2024), following initial empirical results. Vocabulary Vocab Size Vocab (Unimax) 64k Vocab (Monolingual) Vocabulary by Kudugunta et al., with = 100 and 99.9995% character coverage. Same settings as above, but trained only on sentences from the language being considered. effective sampling strategy to perform well across languages, as is the objective with many large multilingual models. The UNIMAX sampling rate per language is documented in Table B.4 Each of these models we train for 1T tokens, to reflect real-world practices. Using these Unimax checkpoints, across model scales, we then conduct continuous monolingual pretraining (which we call finetuning here) on all 48 languages separately in Leval (48). While finetuning, we also observe the loss across all Leval (48)languages as well. We use the same hyperparameters as discussed for pretraining, but reset the learning schedule, and train for at least another 30k+ steps. These empirical results allow us to compare (across range of N, D, C) the loss on language between model finetuned from Unimax checkpoint, and model pretrained monolingually from scratch on language t. B.5 EXPERIMENTAL DETAILS: LANGUAGE TRANSFER In Section 4 we measure how much training on source language is beneficial for the test loss on the target language t. language transfer score tells practitioner the extent to which training with each language would help or hinder their target language t. There are two ways in which we can measure this language transfer: 1. BILINGUAL TRANSFER SCORE. This score measures how many training tokens 50/50 (s, t) bilingual model needs, relative to monolingual target language baseline, to reach the same validation loss Lt. positive score indicates co-training with the source language has positive transfer, whereas negative score indicates it hinders convergence. Table B.3: The SCALE value mapped to the dimensions of the model, and the parameter count. The model dimensions, borrowed from Muennighoff et al. (2024), report the number of attention heads, number of layers, embed size, feedforward size, and key-value size. Our model may be slightly different sizes than prior work based on our vocabulary size (64000 + 512 special tokens). SCALE HEADS LAYERS EMBED FFW KV PARAMETERS 0 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 4 7 7 7 8 9 10 10 10 12 12 12 14 14 14 16 16 16 10 10 11 10 11 12 11 12 12 14 14 14 16 17 16 16 17 18 17 18 18 20 20 20 21 22 23 24 28 32 32 36 32 32 40 3 4 5 6 8 9 10 13 16 12 15 18 14 16 18 16 18 20 18 21 18 24 21 19 24 22 25 20 23 26 22 22 25 28 25 24 28 28 32 26 30 34 36 36 36 36 40 42 47 44 47 47 47 9,044,352 512 32 17,662,848 896 32 24,847,776 1,152 32 45,763,200 1,792 32 66,588,672 2,048 64 84,939,840 2,304 64 106,830,080 2,560 64 126,492,800 2,560 64 146,155,520 2,560 64 162,800,640 3,072 64 191,114,496 3,072 64 219,428,352 3,072 64 237,646,080 3,584 64 263,337,984 3,584 64 289,029,888 3,584 64 334,512,128 4,096 64 368,068,608 4,096 64 401,625,088 4,096 64 554,457,600 5,120 128 633,104,640 5,120 128 661,807,872 5,632 128 711,751,680 5,120 128 756,970,368 5,632 128 816,345,600 6,144 128 852,132,864 5,632 128 929,596,416 6,144 128 1,042,847,232 6,144 128 1,143,245,824 7,168 128 1,297,391,872 7,168 128 1,451,537,920 7,168 128 1,608,560,640 8,192 128 1,807,137,536 8,704 128 1,809,893,376 8,192 128 2,011,226,112 8,192 128 2,034,422,912 8,704 128 2,187,122,688 9,216 128 2,261,708,288 8,704 128 2,526,870,528 9,216 128 2,866,618,368 128 9,216 2,891,514,880 10,240 128 3,310,955,520 10,240 128 3,730,396,160 10,240 128 4,335,303,168 10,752 128 4,749,364,224 11,264 128 5,182,299,648 11,776 128 5,634,109,440 12,288 128 14,336 128 8,452,190,208 16,384 128 11,538,702,336 17,408 128 14,314,315,520 18,432 128 15,245,973,504 18,432 128 15,821,655,552 19,456 128 17,402,920,192 20,480 128 18,982,943, 128 224 288 448 512 576 640 640 640 768 768 768 896 896 896 1,024 1,024 1,024 1,280 1,280 1,408 1,280 1,408 1,536 1,408 1,536 1,536 1,792 1,792 1,792 2,048 2,176 2,048 2,048 2,176 2,304 2,176 2,304 2,304 2,560 2,560 2,560 2,688 2,816 2,944 3,072 3,584 4,096 4,352 4,608 4,608 4,864 5,120 21 Table B.4: The Unimax language sampling rates adapted from Chung et al. (2023). Languages are listed in order of their percentage sampling rate, which sum to 100. LANG % LANG % LANG % LANG % LANG % LANG % 1.42e+00 be 1.42e+00 fr 1.42e+00 ru 1.42e+00 it 1.42e+00 sq 1.42e+00 hu 1.42e+00 ar 1.42e+00 lt 1.42e+00 no 1.09e+00 bs kn 6.76e-01 hy 4.64e-01 km 3.95e-01 rw 2.38e-01 lb 1.68e-01 grc 1.27e-01 yi 9.77e-02 xh 8.12e-02 pap 6.67e-02 as 5.38e-02 sah 4.04e-02 se 2.22e-02 os 1.39e-02 kbd 9.66e-03 tn 7.51e-03 otq 6.08e-03 5.36e-03 gn 4.26e-03 war kbp 3.73e-03 bho 3.43e-03 ve 2.97e-03 tzo 2.65e-03 nhe 2.46e-03 kv 2.26e-03 fon 2.13e-03 tlh 2.00e-03 1.89e-03 mbt 1.74e-03 mai kj 1.57e-03 tzh 1.42e-03 kri 1.27e-03 ach 1.20e-03 tab 1.11e-03 bum 1.02e-03 arn 9.37e-04 toj 8.26e-04 nia 7.83e-04 7.56e-04 dtp 6.99e-04 msi qvi 6.23e-04 5.12e-04 dyu 4.71e-04 mni gor 4.30e-04 nog 4.07e-04 frp 3.74e-04 jvn 3.21e-04 bus 2.78e-04 knj 2.15e-04 laj 1.92e-04 spp 1.75e-04 gof 1.27e-04 tll 1.10e-04 oj 8.48e-05 ber 6.97e-05 6.16e-05 ffm 4.34e-05 msb en 5.00e+00 af de 1.42e+00 es ml 1.42e+00 mr hi 1.42e+00 id pl 1.42e+00 pt ca 1.42e+00 et th 1.42e+00 zh ja 1.42e+00 ko da 1.42e+00 el ka 1.19e+00 mn ne kaa 6.87e-01 eo 5.06e-01 la mt 4.06e-01 my ku pa 2.50e-01 ckb fy 1.73e-01 gd 1.48e-01 ht sd 1.05e-01 mi or kl 9.07e-02 sn co 6.68e-02 st haw 5.70e-02 tet gsw 4.14e-02 bo om 2.23e-02 hil udm 1.44e-02 ts tyv 9.73e-03 kha to 7.57e-03 ak ada 6.23e-03 ln chm 5.39e-03 yua srn 4.32e-03 lu ady 3.87e-03 iso kac 3.43e-03 pag ngu 3.02e-03 hui bbc 2.73e-03 min ang 2.47e-03 nv bci 2.28e-03 meu bew 2.15e-03 tvl dov 2.02e-03 meo crh 1.89e-03 mam gym 1.81e-03 quc gv 1.62e-03 rcf shn 1.44e-03 gag cab 1.31e-03 bru enq 1.21e-03 kek qub 1.12e-03 cak 1.05e-03 tuc quh 9.39e-04 mak yap 8.33e-04 rmc 8.04e-04 mas 7.64e-04 zne 7.15e-04 mad lhu 6.37e-04 mfe 5.20e-04 inb 4.88e-04 jac 4.35e-04 sda 4.08e-04 rki 3.76e-04 cce 3.22e-04 izz 2.79e-04 msm tzj suz 2.22e-04 tca pis 1.97e-04 shp 1.78e-04 hne tsc 1.31e-04 mqy awa qvz 1.19e-04 cbk akb 8.52e-05 qup ff 7.48e-05 adh smt 6.40e-05 nut kwi 4.62e-05 ctd_Latn 1.03e-05 hi_Latn 2.48e-05 2.15e-06 tly_IR 2.29e-06 ta_Latn kn_Latn bg_Latn 9.21e-07 6.18e-07 ndc_ZW 1.31e-07 sat_Latn 1.51e-07 1.42e+00 fil 1.42e+00 hr 1.42e+00 sr 1.42e+00 lv 1.42e+00 sv 1.42e+00 iw 1.42e+00 cs 1.42e+00 sk 1.42e+00 mk 1.03e+00 uz gu 6.72e-01 ky 4.47e-01 tt 3.35e-01 2.29e-01 lo 1.63e-01 mg jv 1.25e-01 ba 9.55e-02 su 7.21e-02 zu 6.57e-02 oc 5.07e-02 br 4.01e-02 ce 2.12e-02 lg 1.26e-02 ee 9.23e-03 nso 7.33e-03 dz 5.86e-03 krc 5.23e-03 rom 4.03e-03 syr 3.68e-03 ay 3.38e-03 pck 2.94e-03 tiv 2.65e-03 bgp 2.45e-03 new 2.25e-03 iu 2.08e-03 ho 1.96e-03 emp 1.87e-03 crs 1.72e-03 1.49e-03 btx 1.41e-03 mdf seh 1.25e-03 cuk 1.17e-03 bts 1.11e-03 gil 1.01e-03 ban 9.35e-04 twu 8.20e-04 kjh 7.77e-04 7.46e-04 ksw 6.84e-04 mag 5.78e-04 mh ctu 5.08e-04 guc 4.59e-04 skr 4.18e-04 teo 4.04e-04 alz 3.61e-04 hvn 3.14e-04 ktu 2.73e-04 bim 2.15e-04 qxr 1.83e-04 koi 1.71e-04 gbm 1.27e-04 raj 1.03e-04 ify 8.46e-05 tks 6.68e-05 qvc 5.93e-05 el_Latn 4.20e-05 ru_Latn te_Latn 6.95e-06 nan_Latn_TW 1.94e-06 ml_Latn zh_Latn cr_Latn kmz_Latn ch srm bi bas gub nr rn tcy hus acf qu tyz gom nnb alt 1.42e+00 te 1.42e+00 gl 1.42e+00 kk 1.42e+00 is 1.42e+00 az 1.42e+00 ta 1.42e+00 nl 1.42e+00 ms 1.42e+00 vi 1.42e+00 tr 1.42e+00 sl 1.42e+00 ro 1.42e+00 fi 1.42e+00 fa 1.42e+00 bg 1.42e+00 uk 1.34e+00 eu 1.38e+00 bn sw 8.19e-01 1.02e+00 ur cy 5.89e-01 si 6.43e-01 ga 4.28e-01 tg 4.37e-01 ps 2.93e-01 so 3.14e-01 ha 1.83e-01 dv 2.06e-01 am 1.52e-01 ug 1.54e-01 hmn 1.09e-01 tk 1.12e-01 ceb 9.24e-02 fo 9.42e-02 sm 6.97e-02 ny 7.20e-02 yo 6.31e-02 ig 6.46e-02 lus 4.66e-02 cv 4.93e-02 sa 2.52e-02 rm 3.29e-02 ilo 1.58e-02 cnh 1.70e-02 vec 1.12e-02 ti 1.21e-02 av 7.66e-03 iba 8.25e-03 zza 7.02e-03 fj 7.08e-03 cfm 5.44e-03 5.69e-03 bua hif 5.11e-03 5.21e-03 wa 3.94e-03 sg bik 3.99e-03 3.49e-03 myv ltg 3.51e-03 3.10e-03 kum 3.30e-03 2.86e-03 zap 2.88e-03 2.52e-03 ksd 2.55e-03 2.37e-03 2.45e-03 nzi 2.19e-03 2.21e-03 mps 2.07e-03 mgh abt 2.08e-03 1.92e-03 mrj kw 1.96e-03 ium 1.85e-03 ace 1.85e-03 ubu 1.69e-03 pon 1.70e-03 chk 1.46e-03 ape 1.48e-03 ss 1.38e-03 ppk 1.39e-03 1.23e-03 tbz ibb 1.23e-03 1.15e-03 wo kmb 1.16e-03 rwo 1.06e-03 kos 1.07e-03 tsg 9.65e-04 stq 9.73e-04 sja 8.84e-04 jiv 9.06e-04 amu 8.08e-04 xal 8.15e-04 guh 7.64e-04 bm 7.73e-04 din 7.22e-04 7.25e-04 bzj kg 6.57e-04 6.60e-04 mkn sus 5.52e-04 djk 5.59e-04 5.03e-04 pau gui 5.07e-04 4.39e-04 wal jam 4.43e-04 noa 4.14e-04 nyu 4.15e-04 sxn 3.92e-04 tdx 4.01e-04 lrc 3.51e-04 taj 3.60e-04 3.07e-04 dwr nij 3.08e-04 2.52e-04 maz chr 2.66e-04 bqc 1.98e-04 gvl 1.99e-04 ahk 1.80e-04 niq 1.82e-04 agr 1.53e-04 1.67e-04 quf dje 1.22e-04 1.25e-04 miq quy 1.00e-04 kjg 1.02e-04 8.03e-05 brx cac 8.39e-05 6.46e-05 mrw trp 6.55e-05 5.40e-05 ann 5.87e-05 3.40e-05 doi 4.09e-05 ber_Latn 4.84e-06 4.74e-06 zxx_xx_dtynoise 1.65e-06 1.80e-06 1.83e-07 bn_Latn 2.59e-07 4.29e-08 1.42e+00 1.42e+00 1.42e+00 1.42e+00 1.42e+00 1.42e+00 1.42e+00 1.42e+00 1.34e+00 7.19e-01 5.14e-01 4.25e-01 2.52e-01 1.78e-01 1.50e-01 1.09e-01 9.12e-02 6.94e-02 6.00e-02 4.61e-02 2.23e-02 1.49e-02 1.10e-02 7.57e-03 6.60e-03 5.41e-03 4.79e-03 3.89e-03 3.48e-03 3.06e-03 2.83e-03 2.50e-03 2.29e-03 2.18e-03 2.05e-03 1.92e-03 1.85e-03 1.68e-03 1.45e-03 1.37e-03 1.21e-03 1.14e-03 1.05e-03 9.47e-04 8.49e-04 8.05e-04 7.64e-04 7.17e-04 6.54e-04 5.21e-04 5.02e-04 4.38e-04 4.08e-04 3.81e-04 3.50e-04 2.99e-04 2.39e-04 1.98e-04 1.80e-04 1.39e-04 1.21e-04 9.24e-05 7.64e-05 6.46e-05 5.26e-05 kaa_Latn 2.97e-05 dln az_RU 3.23e-06 gom_Latn 1.28e-06 1.69e-07 gu_Latn 5.69e-07 1.01e-07 ms_Arab 6.00e-08 ms_Arab_BN 22 2. FINETUNING ADAPTATION SCORE. This score captures the effect on the target languages loss Lt, when massively multilingual model is finetuned only on the source language s. B.5.1 BILINGUAL TRANSFER SCORE (BTS) The Bilingual Transfer Score asks how data-efficient bilingual learner is, relative to purely monolingual one, on some target language t. To estimate this, we pretrain two models, monolingual one on and bilingual one that samples tokens equally (50/50) from the source and target languages (s, t). We then measure how many more training tokens it takes the bilingual model to attain the same validation loss Lt as the monolingual model. This distance between the learning curves is measured throughout training, at different reference horizonsor, number of tokens trained on. In Figure 2 we use models with 2B parameters, and reference horizon of = 42B tokens, as it roughly equates to 10, 000 pretraining steps for our 2B parameter models, and learning curves have stabilized. In Figure C.1 we measure how the BTS varies by model size, and by number of training tokens seen. We are able to compute the bilingual transfer scores, across every combination of pairings between 10 languages: English, Russian, Spanish, French, German, Portuguese, Chinese, Vietnamese, Japanese, and Hindi. These languages were chosen to give distribution of highand mid-resource languages from variety of language families. This totals 10 monolingual experiments (one for each language), and C(10, 2) = 45 bilingual experiments, which provide the results for 10x10 grid of Bilingual Transfer Scores. In more detail, let dmono be the number of tokens trained on language by monolingual model. This model attains test loss of Lt(dmono) after dmono tokens on language t. We record the number of training tokens dbi at which the bilingual model reaches the same loss, Lt(dbi) = Lt(dmono). We define the Bilingual Transfer Score (BTS) as the signed step surplus BTSst = σbi(Lt(dmono)) 2dmono dmono where dbi = σbi(Lt(dmono)). σbi maps loss value to the number of steps taken by the bilingual model to reach that loss. In short, this score measures the relative training efficiency of the bilingual model compared to monolingual model at reaching the same loss level for language t. Subtracting 2dmono and dividing by dmono simply centers the value on 0, and forces positive transfer to give positive value. value of 0, implies neutral transfer, as the multilingual model requires exactly the same number of steps in the target language as the monolingual model, value below 0 implies negative transfer, as the multilingual model is slower to reach the loss Lt(dmono), and value above 0 implies positive transfer, as the bilingual model reaches the target loss in < 2dmono. Note that for Figure 2 these scores are anchored on 2B parameter models, trained for = 42B tokens, for consistency. But as model size increases, or tokens trained decreases, the language transfer scores improve monotonically. B.5.2 FINETUNING ADAPTATION SCORE (FAS) Complementary to BTS, the Finetuning Adaptation Score measures the instantaneous impact of continued exposure to single language on all others while holding model capacity fixed. Specifically, it measures how finetuning massively multilingual model on source language affects the performance of target language t. Because we only need to train on each source language once, and evaluate on all languages, it is less computationally expensive than BTS, which requires training on every pair of languages. As such, we are able to derive 38 38 = 1444 (s, t) comparisons from 38 finetuning experiments. We start from shared multilingual checkpoint: Unimax pretrained model, trained for 1T tokens. For each source language s, we continue pre-training (for simplicity we denote this as finetune) exclusively on this language, and evaluate at regular intervals on every target languages validation set. Let Lst(d) be the validation loss on after additional updates on s, and let Lunimax denote the baseline loss of the shared Unimax checkpoint, before finetuning has begun. The Finetunt 23 ing Adaptation Score (FAS) aggregates the reduction in loss over common time window [0, dmax]: FASst = 1 dmax (cid:90) dmax (cid:2)Lunimax Lst(d)(cid:3) dd. Positive values indicate that exposing the model to accelerates learning or yields immediate gains on (helpful transfer), whereas negative values capture negative interference. Normalising by dmax makes the score comparable across language pairs and training horizons. Because finetuning on language can cause unpredictable behavior in the validation loss of target language (it might immediately increase, or decrease then eventually increase), we start with deriving series of features about the finetuning loss curve. We calculate the following features for finetuning loss curve: Baseline Loss Deviation (δst). For each language pair = we quantify the deviation in validation loss between the baseline Lunimax and the loss Lst on the final step: δst = Lst(dmax) Lunimax (dmax) δst < 0 means finetuning on helps t; δst > 0 means it hurts. Adaptation Gain (gl). This measures the area normalized reduction in loss on language from finetuning on language l. This allows us to capture the relative learning dynamics of both the source and target language. gl = 1 dmax (cid:90) dmax (cid:2) Lunimax Lst(d) (cid:3) ds. gl > 0 indicates net improvement; large values imply that benefits greatly from additional supervision. For each language transfer pair (s, t), we compose the following feature vector xst. As languages differ in intrinsic difficulty, we normalize each feature. gl = gl µg σg , δst = δst µδ,t σδ,t , (5) For adaptation gain, we use the global mean and standard deviation (µg, σg), and for baseline loss deviation we compute the mean and standard deviation (µδ,t, σδ,t) across all source languages for each fixed target language t. B.6 ESTIMATING BILINGUAL TRANSFER SCORE (BTS) In the Bilingual Transfer Score experiments we empirically measured language transfer scores ytrue st for subset Pobs LL. We construct training set from these 90 experiments, using them as the gold labels. xst = (cid:2)gs, gt, gs gt, δst (cid:3) , yst = ytrue st, (6) We fit random-forest regressor φθ : R4 with 300 trees and unlimited depth. The predictive performance is assessed by k-fold cross-validation (k = 5), obtaining an R2 = 0.85 and Spearman correlation of ρ = 0.88. After training on all observed pairs, φ is invoked on every unlabelled (s, t) LL, yielding predicted BTS values, that estimate the language transfer shown in Figure 2. B.7 EXPERIMENTAL DETAILS: MULTILINGUAL CAPACITY B.7.1 MULTILINGUAL CAPACITY SCALING LAWS In Section 5 we aim to understand how models capacity hinders multilingual learningalso termed the curse of multilinguality (Conneau et al., 2019; Chang et al., 2024). To do this, we train models of various sizes, and with range of language mixtures, shown in Table B.5. For simplicity, languages are always evenly sampled within their mixtures, even if some languages have more available text than others. We train models on these mixtures, and evaluate the validation loss of all other languages, throughout training. 24 Table B.5: The set of experiments designed to measure the curse of multilingualityor how language performance is impacted by both the number of training languages and the model size. We defined VERSION mixtures with NUM languages, ranging between 4 and 50. For mixtures with 4 languages, we define few different versions oriented to languages that might be more likely to be grouped together. This set of experiments are summarized in the Capacity row of Table B.1. VERSION NUM LANGUAGES (ALPHABETICAL) 4v0 4v1 4v2 4v3 6v0 8v0 8v1 12v0 16v0 24v0 32v0 50v 4 4 4 4 6 8 8 12 16 24 32 50 de, es, fr, pt de, en, es, fr hi, ja, vi, zh en, hi, pt, ru de, en, es, fr, pt, ru de, en, es, fr, pt, ru, vi, zh de, en, fr, hi, ja, ru, vi, zh de, en, es, fr, hi, it, ja, pl, pt, ru, vi, zh de, en, es, fr, hi, id, it, ja, nl, pl, pt, ru, sv, tr, vi, zh cs, da, de, en, es, fa, fi, fr, hi, hu, id, it, ja, nl, pl, pt, ro, ru, sv, th, tr, uk, vi, zh ar, bg, ca, cs, da, de, el, en, es, fa, fi, fr, hi, hu, id, it, ja, ko, lt, nl, no, pl, pt, ro, ru, sk, sv, th, tr, uk, vi, zh af, ar, ba, bm, bn, ca, ckb, cy, de, ee, el, en, es, fa, fil, fr, gn, gu, ha, he, hi, hr, id, it, ja, jv, kk, ko, mr, ms, my, nl, no, om, pl, ps, pt, ro, ru, sm, sv, sw, te, th, tr, uk, ur, vi, yo, zh For given target language we have empirical loss scores across 11 differently sized models (from 10M to 8B), and across each mixture that contains the language in training, including monolingual, bilingual, and the multilingual mixtures shown in Table B.5. We model each languages loss as L(N, D, K), where is the model size, are the number of training tokens for the target language, and is the number of languages in the training mixture (evenly sampled). We define L(N, D, K) as follows: L(N, D, K) = + ϕ α + ψ Dβ (7) This law preserves the well-validated power-law behavior of monolingual scaling laws, while introducing into both the capacity term (AK ϕ/N α) and the data term (BK ψ/Dβ). The exponent ϕ captures how capacity requirements grow with the number of languages, while ψ captures how data requirements change with multilinguality: ψ < 0 indicates positive transfer (sublinear per-language data needs as increases), ψ = 0 implies no net transfer in the data term, and ψ > 0 implies negative transfer/interference. Under even sampling, the total tokens across languages are Dtot = D, in which case the data term can be rewritten as ψ+β/Dβ tot. B.7.2 ADDING LANGUAGES: TO Practitioners may wish to change the number of languages supported by model, from to r. If the model size and training tokens remain unchanged, it will lead to degradation in loss across languages, as the same compute budget is allocated across more languages. When adding languages, the practitioner may wish to know how much they need to scale the model (N , D) and the training compute to maintain the same performance per language, as before. We call this an iso-loss, as it approximately preserves the loss values, with change in K. Compute-Optimality for Equation (7). As we increase to we would like to understand the new , that minimize training compute on the iso-loss surface. Let = /K. Minimizing subject to A(Kr)ϕ α + B(Kr)ψ Dβ = AK ϕ α + BK ψ Dβ yields the closed-form optimum: (cid:17) (cid:16) = rϕ/α, (cid:17) (cid:16) = rψ/β, (cid:17) (cid:16) tot Dtot = r1+ψ/β. 25 These expressions show clear separation of roles: ϕ/α governs the parameter burden of adding languages, whereas ψ/β governs the data burden (with ψ < 0 reducing the per-language data requirement due to positive transfer). Since the compute budget is in terms of total tokens Dtot = D, and Dtot. Adding languages: to Finding the compute multiplier. At the iso-loss optimum, the compute multiplier is = D tot Dtot = (cid:16) (cid:17)(cid:16) tot Dtot (cid:17) = r1+ϕ/α+ψ/β . α measures the extra parameter capacity per added language, while ψ Intuitively, ϕ β measures the extra (or reduced, if ψ < 0) per-language token budget. But since compute is defined with total tokens Dtot, then we consider the additional factor of from enlarging the language inventory. Adding languages: to Finding the Iso-loss curve. Let denote the multipliers for model size and (per-target-language) data when we grow the language inventory from to = rK. Define the baseline term contributions and wN AK ϕ/N α AK ϕ/N α + BK ψ/Dβ , wD = 1 wN . The iso-loss condition equates the sum of the two error terms before and after the change in K: A(Kr)ϕ (N s)α + B(Kr)ψ (Dt)β = AK ϕ α + BK ψ Dβ . Dividing both sides by AK ϕ/N α + BK ψ/Dβ yields the normalized iso-loss constraint rϕ wN sα + rψ wD tβ = 1. (8) Solving for given s. Rearranging equation 8 gives = (cid:20) rψ wD 1 rϕ wN sα (cid:21)1/β . The denominator must be positive, which implies the feasibility condition sα > rϕ wN . As sα rϕwN the denominator tends to 0+ and ; as , the denominator tends to 1 and (rψwD)1/β. Solving for given t. By symmetry, = (cid:20) rϕ wN 1 rψ wD tβ (cid:21)1/α , with feasibility tβ > rψ wD. Total-token form. Under even sampling, Dtot = KD and tot = = rKD, so tot Dtot = D = (cid:20) rψ wD 1 rϕ wN sα (cid:21)1/β . Compute-optimal frontier (starting from compute-optimal (K, N, D)). Suppose the baseline (K, N, D) is compute-optimal for its loss level, i.e., it minimizes subject to AK ϕ/N α + BK ψ/Dβ = const. standard Lagrange multiplier argument then yields α AK ϕ α = β BK ψ Dβ wN = β α + β , wD = α α + β . After changing rK, the compute along the iso-loss curve is /C = (or /C = tot Dtot if compute is defined with total tokens). Minimizing this product subject to equation 8 again via Lagrange multipliers gives the stationarity condition α rϕ wN sα = β rψ wD tβ. 26 Table C.1: summary of the monolingual scaling laws for each language The columns are: the language, the language family, the language script, its number of unique tokens in MADLAD400 (with the percentage of unique tokens as compared to English), and the derived scaling law. LANGUAGE FAMILY SCRIPT (% EN) SCALING LAW Monolingual Vocabulary, Monolingual Training English Indo-european French Indo-european Latin Latin Russian Indo-european Cyrillic Chinese Sino-tibetan Hans 125B (4.48%) Hindi Indo-european Devanagari 7.9B (0.28%) Swahili Niger-congo: atlantic congo Latin 770M (0.03%) Multilingual Vocabulary, Monolingual Training English Indo-european French Indo-european Latin Latin Russian Indo-european Cyrillic Chinese Sino-tibetan Hans 125B (4.48%) Hindi Indo-european Devanagari 7.9B (0.28%) Swahili Niger-congo: atlantic congo Latin 770M (0.03%) Multilingual Vocabulary, Unimax Training English Indo-european French Indo-european Latin Latin Russian Indo-european Cyrillic Chinese Sino-tibetan Hans 125B (4.48%) Hindi Indo-european Devanagari 7.9B (0.28%) Swahili Niger-congo: atlantic congo Latin 770M (0.03%) 2.8T (100.0%) 0.67 + e6.18 363B (13.01%) 0.76 + e10. 705B (25.26%) 0.82 + e10.07 D0.64 D0.41 0.41 + e8.25 0.64 + e13.24 0.63 + e13.28 0.46 + e10.37 0.39 + e8.03 0.26 + e5.51 D0.26 D0. D0.46 D0.63 1.08 + e8.20 0.52 + e6.03 0.00 + e4.13 2.8T (100.0%) 0.83 + e9.91 363B (13.01%) 0.66 + e7.12 705B (25.26%) 0.76 + e7.97 D0.63 D0.45 0.63 + e13.06 0.45 + e8.94 0.49 + e9.91 0.49 + e10.90 0.43 + e8.69 0.30 + e6. D0.30 D0.49 D0.43 D0.49 1.18 + e8.87 0.63 + e6. 0.00 + e4.99 2.8T (100.0%) 0.00 + e3.03 363B (13.01%) 0.00 + e3.63 705B (25.26%) 0.00 + e3.90 D0. D0.19 0.16 + e3.15 0.19 + e3.94 0.20 + e4.28 0.21 + e5.47 0.18 + e3.89 0.18 + e3.74 D0.18 D0.18 D0.21 D0. 0.39 + e4.58 0.00 + e3.46 0.00 + e3.52 Combining with equation 8 and substituting the compute-optimal weights above yields the unique minimum-compute point on the new frontier: (cid:17) (cid:16) = rϕ/α, (cid:17) (cid:16) = rψ/β, (cid:17) (cid:16) tot Dtot = 1+ψ/β. this point each error component is individually restored to its baseline magnitude: At A(Kr)ϕ/N α = AK ϕ/N α and B(Kr)ψ/Dβ = BK ψ/Dβ, so the weights (wN , wD)and hence the balance of capacity/data bottlenecksremain unchanged. Because the frontier is convex in (log s, log t) and log = log + log is linear, this stationary point is the global compute minimum along the iso-loss curve."
        },
        {
            "title": "C EXTENDED RESULTS",
            "content": "C.1 EXTENDED RESULTS: SCALING LAWS In Table C.1 we illustrate the fitted scaling laws for each language. Specifically, we show the scaling law parameters for each language when trained with monolingual vocabulary on monolingual data, when trained with multilingual vocabulary on monolingual data, and when trained with multilingual vocabulary on the Unimax multilingual mixture. These results match up with those presented in Figure 1. 27 Figure C.1: Left: X-axis is training tokens (D), Y-axis is language-transfer score. Right: X-axis is model size (N), Y-axis is language-transfer score. C.2 EXTENDED RESULTS: LANGUAGE TRANSFER Research Question: How does language transfer change with larger models, or when you train for longer? We further investigated how language transfer evolves with training data (D) and model size (N) in Figure C.1. Our analysis of transfer scores over the course of training reveals that transfer dynamics are established early. The performance gap between synergistic pairs (e.g., es pt) and interfering pairs (e.g., en zh) appears within the initial stages of training and remains largely consistent, suggesting that the fundamental compatibility of languages is not significantly altered by longer training. However, the impact of model scale is more pronounced. As illustrated in Figure C.1, we observe clear trend in which larger models are more effective in facilitating cross-lingual transfer. For synergistic pairs, the positive transfer score increases modestly with model capacity. More importantly, for challenging pairs that exhibit interference in smaller models, larger models are significantly better at mitigating this negative effect, bringing the score closer to zero. This is similar to what has been observed while training larger and larger foundation models (Team et al., 2024). Given that transfer dynamics change at scale, it is an important factor to consider when designing multilingual foundation models that are performant for all languages being trained on. 28 Figure C.2: The language transfer scores measure the benefit to the target language of (co-)training with the source language. We bold the top-5 source languages for each target language. The top left 9x9 are the Bilingual Transfer Score computed directly, whereas the rest are estimated from the Finetuning Adaptation Score. While English is the best source language for many of of languages, we find language similarity is highly predictive of these scores."
        }
    ],
    "affiliations": [
        "Google Cloud AI",
        "Google DeepMind",
        "MIT",
        "Stanford University",
        "University of Washington"
    ]
}