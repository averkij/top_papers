{
    "paper_title": "SAKE: Towards Editing Auditory Attribute Knowledge of Large Audio-Language Models",
    "authors": [
        "Chih-Kai Yang",
        "Yen-Ting Piao",
        "Tzu-Wen Hsu",
        "Szu-Wei Fu",
        "Zhehuai Chen",
        "Ke-Han Lu",
        "Sung-Feng Huang",
        "Chao-Han Huck Yang",
        "Yu-Chiang Frank Wang",
        "Yun-Nung Chen",
        "Hung-yi Lee"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Knowledge editing offers an efficient way to update model knowledge without full retraining, but prior work has concentrated almost exclusively on textual or visual modalities. We introduce SAKE, the first benchmark specifically designed for editing auditory attribute knowledge in Large Audio-Language Models (LALMs). Unlike factual updates, SAKE targets several abstract auditory attributes, capturing knowledge types that go beyond conventional textual and visual domains. We benchmark seven editing methods on two LALMs along four dimensions: reliability, generality, audio/text locality, and portability. Results highlight challenges such as preserving intra-attribute knowledge unrelated to the edit, generalizing edits to multimodal reasoning, and maintaining edits under sequential updates. SAKE provides a principled framework to study how knowledge editing extends to the auditory modalities, opening new directions for maintaining and adapting LALMs in more diverse real-world scenarios."
        },
        {
            "title": "Start",
            "content": "Preprint. SAKE: TOWARDS EDITING AUDITORY ATTRIBUTE KNOWLEDGE OF LARGE AUDIO-LANGUAGE MODELS Sung-Feng Huang3 Chao-Han Huck Yang3 Yu-Chiang Frank Wang3 Szu-Wei Fu3 Zhehuai Chen3 Chih-Kai Yang1 Yen-Ting Piao1 Tzu-Wen Hsu2 Ke-Han Lu1 Yun-Nung Chen1 Hung-yi Lee1 1National Taiwan University 2DouDou Capital 3NVIDIA 5 2 0 2 9 1 ] . [ 1 7 1 9 6 1 . 0 1 5 2 : r a"
        },
        {
            "title": "ABSTRACT",
            "content": "Knowledge editing offers an efficient way to update model knowledge without full retraining, but prior work has concentrated almost exclusively on textual or visual modalities. We introduce SAKE, the first benchmark specifically designed for editing auditory attribute knowledge in Large Audio-Language Models (LALMs). Unlike factual updates, SAKE targets several abstract auditory attributes, capturing knowledge types that go beyond conventional textual and visual domains. We benchmark seven editing methods on two LALMs along four dimensions: reliability, generality, audio/text locality, and portability. Results highlight challenges such as preserving intra-attribute knowledge unrelated to the edit, generalizing edits to multimodal reasoning, and maintaining edits under sequential updates. SAKE provides principled framework to study how knowledge editing extends to the auditory modalities, opening new directions for maintaining and adapting LALMs in more diverse real-world scenarios."
        },
        {
            "title": "INTRODUCTION",
            "content": "Large language models (LLMs) (Touvron et al., 2023; Grattafiori et al., 2024; Hurst et al., 2024) have demonstrated remarkable progress across natural language understanding and generation tasks. As they scale, knowledge editing (De Cao et al., 2021; Mitchell et al., 2022; Zheng et al., 2023; Deng et al., 2025) has emerged as an important line of research. The goal of knowledge editing is to efficiently update specific pieces of model knowledge without full retraining while minimizing the risk of catastrophic forgetting. These techniques enable incorporating new facts (De Cao et al., 2021), correcting errors, mitigating biases (Chen et al., 2024a), and supporting personalization (Lu et al., 2025d), making knowledge editing key tool for adapting LLMs to real-world use. Advances in LLMs have also led to multimodal extensions such as large vision-language models (LVLMs) (Liu et al., 2023; Li et al., 2023) and large audio-language models (LALMs) (Lu et al., 2025b; Chu et al., 2024; Kuan et al., 2024; Ghosh et al., 2025; Hurst et al., 2024; Lin et al., 2025; Lu et al., 2025a; Gong et al., 2023; Tang et al., 2024), which integrate additional modalities into LLMs. As multimodal models gain adoption, it becomes increasingly important to extend knowledge editing beyond the textual domain. Recent efforts have explored editing LVLMs through benchmarks targeting visual knowledge updates (Cheng et al., 2023; Huang et al., 2024; Zhang et al., 2024). However, knowledge editing in auditory modalities remains unexplored despite the rising importance of LALMs for speech and audio understanding. Editing auditory attribute knowledge (Yang et al., 2025a) introduces unique challenges and opportunities. Factual knowledge (Thorne et al., 2018) typically involves discrete and concrete statements (e.g., Paris is the capital of France). In contrast, auditory attributes such as speaker gender, emotion, spoken language, or animal sounds represent high-level and continuous perceptual concepts. These attributes may manifest through infinitely many acoustic realizations, including different speakers, prosodic variations, or recording conditions, while still grounding the same underlying attribute. As result, it is unclear whether existing methods developed for editing discrete factual knowledge can be extended to such abstract representations. At the same time, editing auditory Equal Contribution. 1 Preprint. Figure 1: Overview of SAKE benchmark. Four auditory attributes are targeted. Reliability checks if the edit succeeds, Generality if it holds for equivalent data, Locality if unrelated knowledge remains unchanged, and Portability if it transfers to related knowledge. For example, after editing frog to dog, the answer of the portability question should change from Insectivore to Omnivore. attributes can extend applications in the textual domain, such as debiasing (Chen et al., 2024a) and personalization (Lu et al., 2025d), into the auditory modalities, with new possibilities such as mitigating gender biases in LALMs (Lin et al., 2024b) and enabling personalization (Yang et al., 2025c; Lee et al., 2024) by adapting to users unique voice or speaking style. To this end, we introduce SAKE (Speech and Audio Attribute Knowledge Editing Benchmark), the first benchmark for auditory attribute knowledge editing in LALMs (Figure 1). SAKE covers four auditory attributes: speaker gender, speaker emotion, spoken language, and animal sounds, and evaluates editing methods along four dimensions (Yao et al., 2023): reliability (whether edits succeed), generality (whether they transfer to equivalent variations), locality (whether unrelated knowledge is preserved), and portability (whether edits propagate to interconnected knowledge). Through comprehensive experiments on two strong LALMs, DeSTA2.5-Audio (Lu et al., 2025b) and Qwen2-Audio (Chu et al., 2024), we benchmark diverse editing techniques and reveal significant challenges in editing auditory attribute knowledge, such as preserving intra-attribute knowledge that is irrelevant to the edits and ensuring that updated auditory attributes propagate to reasoning over related knowledge. Moreover, when multiple edits are applied sequentially, most methods suffer from severe forgetting, often losing previously edited knowledge after few additional updates. Overall, our contributions are two-fold1: 1. To the best of our knowledge, this is the first work to explore knowledge editing in auditory modalities of LALMs, an area that has remained unexplored. We introduce the first comprehensive benchmark for auditory knowledge editing and evaluate seven common editing methods, providing novel insights into this direction. 2. Although these editing methods are effective in textual and visual domains, editing auditory attributes, which are abstract perceptual concepts, remains challenging. We show that current methods struggle both to generalize edited knowledge and to preserve unrelated knowledge, underscoring the need for future advances in auditory knowledge editing. 1Resources will be available at https://github.com/ckyang1124/SAKE 2 Preprint."
        },
        {
            "title": "2.1 KNOWLEDGE EDITING",
            "content": "Knowledge editing (Zhu et al., 2024) refers to techniques designed to efficiently update or modify the knowledge stored in models. The goal is to adjust model knowledge in lightweight manner while avoiding catastrophic forgetting that may arise from retraining directly on the target knowledge. Existing methods (Mitchell et al., 2022; De Cao et al., 2021; Zheng et al., 2023; Meng et al., 2022) adopt various strategies: using hypernetwork to predict parameter updates for incorporating new knowledge (De Cao et al., 2021; Mitchell et al., 2022), identifying and adjusting neurons associated with specific knowledge (Meng et al., 2022), or leveraging in-context learning (ICL) to enable models to acquire updated knowledge (Zheng et al., 2023). More recent works have extended the line of work from Meng et al. (2022) to mass editing (Meng et al., 2023) and unstructured scenarios (Deng et al., 2025; Jiang et al., 2025; Su et al., 2025). Beyond correcting factual knowledge, these techniques have also been applied to tasks such as bias mitigation (Chen et al., 2024a), detoxification (Wang et al., 2024a), personalization (Lu et al., 2025d), and unlearning (Li et al., 2025). More recently, researchers have begun to investigate knowledge editing in large vision-language models (LVLMs)(Liu et al., 2023; Li et al., 2023). For example, Cheng et al. (2023) introduced the MMEdit benchmark to explore editing visual knowledge, while subsequent works such as VLKEB (Huang et al., 2024) and MC-MKE (Zhang et al., 2024) expanded the evaluation scope to provide more comprehensive understanding of editing in visual modalities. However, no prior work has examined editing auditory attribute knowledge in LALMs, which involves abstract perceptual concepts rather than concrete facts, distinguishing our study from existing research."
        },
        {
            "title": "2.2 LARGE AUDIO-LANGUAGE MODELS (LALMS)",
            "content": "LALMs extend text-based LLMs to auditory modalities such as speech and audio, opening new possibilities for auditory understanding (Gong et al., 2023; Tang et al., 2024; Chu et al., 2023; 2024; Lu et al., 2025b; Ghosh et al., 2025). These models typically integrate auditory encoders (Radford et al., 2023) with an LLM backbone (Touvron et al., 2023; Yang et al., 2024a) through fine-tuning. While these works advance the integration of auditory knowledge into text-based LLMs, little attention has been given to how auditory-specific knowledge can be edited or updated, which motivates our study."
        },
        {
            "title": "3.1 PROBLEM FORMULATION",
            "content": "Given an LALM with parameters θ and an editing dataset Dedit = {(ae, xe, ye)}, where ae denotes the auditory input, xe the text input, and ye the desired edit target, knowledge editing aims to update the model such that the edited parameters θ enable the LALM to faithfully generate the edit target: (ae, xe; θ) = ye. In this work, we focus on editing auditory attribute knowledge within LALMs, including their perception and understanding of speaker gender, emotion, spoken language, and animal sounds. In this setting, ye corresponds to new auditory attribute labels (e.g., emotions or languages) that differ from the original attribute labels yo associated with ae. For example, given speech labeled with happy emotion, we may edit the LALM so that it instead perceives the speech as having sad tone. For comprehensive evaluation, we introduce the four evaluation dimensions of SAKE and the corresponding metrics, followed by dataset construction details for each dimension."
        },
        {
            "title": "3.2 EVALUATION DIMENSIONS AND CORRESPONDING METRICS",
            "content": "We introduce the four dimensions of knowledge editing, namely reliability, generality, locality, and portability, together with their corresponding evaluation metrics. 3 Preprint. Reliability. The reliability metric Srel measures the proportion of editing instances in Dedit for which the edited model correctly generates the corresponding edit target. It reflects how consistently the editing method updates the model in the desired manner, and is defined as Srel = E(ae,xe,ye)Dedit (cid:104) I(cid:0)f (ae, xe; θ) = ye (cid:1)(cid:105) , (1) where denotes the indicator function, which returns 1 if the condition holds and 0 otherwise. Generality. The edited models should not only generate the correct edit target for the editing data itself but also produce consistent outputs for equivalent neighborhoods of the editing data, such as speech samples sharing the same emotion as ae or paraphrased variants of xe. This requirement is quantified by the generality metric Sgen, defined as (cid:104) I(cid:0)f (a (cid:1)(cid:105) (2) e, e; θ) = ye , Sgen = (ae,xe,ye)Dedit (a e,x e)N (ae,xe) where (ae, xe) denotes the aforementioned equivalent neighborhood of the editing data (ae, xe). Locality. While updating the edit target, the edit should also preserve unrelated knowledge to avoid unintended side effects. The locality metric Sloc evaluates how well an editing method maintains the models knowledge outside the editing scope. Given set of out-of-scope data L(ae, xe, ye) = {(aℓ, xℓ, yℓ)}, consisting of auditory inputs, text inputs, and ground-truth labels, Sloc is defined as the proportion of out-of-scope data where the models behavior remains unchanged after editing: (cid:104) I(cid:0)f (aℓ, xℓ; θ) = (aℓ, xℓ; θ)(cid:1)(cid:105) . (3) Sloc = (ae,xe,ye)Dedit (aℓ,xℓ,yℓ)L(ae,xe,ye) Note that the locality metric evaluates whether the post-edit model preserves the knowledge and behavior on data irrelevant to the edit, rather than the accuracy on out-of-scope instances. For locality with respect to purely textual abilities, we set aℓ = None, as no auditory input is involved. Portability. Knowledge is not completely disentangled or isolated but rather interconnected. Editing one piece of knowledge may influence other related knowledge. For example, if we edit an LALMs perception of frogs sound to that of dog, the models knowledge of the corresponding physical characteristics of that animal should also be updated. The portability metric Sport evaluates how well the edited model generalizes the updated knowledge to other related knowledge: Sport = (ae,xe,ye)Dedit (ap,xp,yp)P(ae,xe,ye) (cid:104) I(cid:0)f (ap, xp; θ) = yp (cid:1)(cid:105) . (4) Here, P(ae, xe, ye) denotes the set of data connected to the edited knowledge, with ap, xp, and yp representing the auditory input, text input, and ground-truth labels of these connected instances."
        },
        {
            "title": "3.3 DATASET CONSTRUCTION",
            "content": "We introduce the SAKE benchmark to evaluate the knowledge editing methods on editing the auditory attribute knowledge in LALMs with respect to the metrics detailed in Sec. 3.2. We benchmark the editing methods on LALMs with speech and audio multiple-choice question answering. Involved Auditory Attribute Knowledge and Audio Sources. SAKE focuses on editing the knowledge of four different auditory attributes: speaker gender, speaker emotion, spoken language, and animal sound. They are chosen for their importance in many downstream applications. We source audio data from the SAKURA benchmark (Yang et al., 2025b), which evaluates LALMs on recognizing these four auditory attributes and related multi-hop reasoning. SAKURA compiles audio samples and attribute labels from CommonVoice (Ardila et al., 2020), CREMA-D (Cao et al., 2014), ESC-50 (Piczak, 2015), and the Animal-Sound Dataset (S asmaz & Tek, 2018). We also extract the auditory attribute labels from SAKURA for these attributes to form set of labels2. 2Gender: Male, Female; Language: English, German, Spanish, French, Italian, Chinese, Japanese, Korean; Emotion: Happy, Disgust, Sad, Fear, Angry; Animal: Dog, Cat, Pig, Cow, Frog, Hen, Rooster, Sheep, Crow. Preprint. Editing Pair Creation. We begin by constructing editing pairs (yo, ye), where yo is the original attribute label and ye the target label after editing. For each attribute, we generate 300 editing pairs by uniformly sampling one label as yo and another distinct label as ye. For example, the editing pair (dog, cat) represents an edit in which the models perception and understanding of dog sounds are updated to those of cat sounds. To avoid bias, we ensure that all labels for given attribute appear with approximately equal frequency as both yo and ye. Reliability Dataset Construction. For each attribute and given editing pair (yo, ye), we sample an audio instance ae with label yo from the dataset, along with the corresponding text questions xe from the SAKURA benchmark that prompt the model to recognize the attribute. Together with the edit target ye, this forms an editing instance (ae, xe, ye). For each attribute, we construct 300 such instances, resulting in total of 1,200 editing instances. This dataset, denoted as Dedit, is used both for applying the edits and for evaluating the reliability of the editing methods. e, Generality Dataset Construction. For each editing instance (ae, xe, ye) in Dedit, we construct its equivalent neighborhood (ae, xe) = (a e) by sampling an alternative audio with the same attribute label as ae from the dataset and by paraphrasing the text question xe into e. Based on these variations, we create testing instances for evaluating generality, considering three types of cases: Type 1: Equivalent neighborhood of the text modality (ae, e); Type 2: Equivalent neighborhood of the auditory modality (a e, xe); Type 3: Equivalent neighborhood involving both auditory and text modalities, resulting in instances of the form (a e). By incorporating these three types of generality testing data, we comprehensively assess how well the editing methods extend the edited knowledge across the equivalent neighborhood. e, Locality Dataset Construction. Similar to the construction of the generality dataset, for each editing instance (ae, xe, ye) in Dedit, we construct an out-of-scope dataset L(ae, xe, ye) = {(aℓ, xℓ, yℓ)}. When aℓ = None, we consider four types of knowledge locality associated with auditory modalities, which we refer to as Audio Locality in this paper, as illustrated in Figure 1. First, editing one attribute should not affect others. We sample question-answer pair from the SAKURA benchmark that requires the LALM to recognize attributes different from those in the editing instance as (aℓ, xℓ, yℓ). Second, even within the same attribute, labels that are not involved in the edit (i.e., neither yo nor ye) should not be perturbed. Accordingly, we sample questionanswer pair from SAKURA requiring recognition of the same attribute as the editing instance, but with ground truth differing from both yo and ye. Note that for the gender attribute, which only has two labels, it is infeasible to construct such cases; therefore, this type of locality data is not included for gender. Third, when editing from yo to ye, the models original knowledge of ye should be preserved, which we test with question-answer pair from SAKURA that requiring recognizing the same attribute as the editing instance, with ground truth ye. Finally, editing should not interfere with general auditory processing. We source question-answer pairs from Dynamic-SUPERB Phase2 (Huang et al., 2025), benchmark covering diverse auditory processing tasks. We exclude tasks involving the four attributes considered in our work to ensure irrelevance to the edited knowledge. Conversely, to assess the preservation of purely text-based knowledge when ae = None (text locality), we use questionanswer data from MMLU (Hendrycks et al., 2021). Portability Dataset Construction. For each editing instance (ae, xe, ye) in Dedit, we construct set of connected knowledge P(ae, xe, ye) associated with the edited attribute. Since SAKURA is designed to evaluate how well LALMs integrate internal world knowledge with auditory attribute knowledge, it already provides relevant knowledge linked to auditory attributes (e.g., physical characteristics of animal labels). Building on this, we create questions that specifically target P(ae, xe, ye). To avoid ambiguity, we ensure that the answers to these questions are not simultaneously valid for both yo and ye. For example, if yo is dog and ye is cat, question about the animals physical characteristics will not use an answer like tail, which applies to both. This guarantees that the knowledge examined in the portability dimension indeed requires updating after the edit. Training Dataset Construction and the Dataset Summary. We also prepare separate training dataset to accommodate editing methods that require (1) training on auxiliary data or (2) access to additional data beyond the testing instances. Its construction follows the same procedures as those used for the reliability, generality, and locality datasets, with training and testing sets kept fully disjoint to avoid leakage. In total, the training set contains 4,000 instances (32,000 speech/audio files and 36,000 QA pairs), while the testing set comprises 1,200 instances (10,800 speech/audio files and 12,000 QA pairs). summary of dataset statistics is provided in Appendix B. 5 Preprint."
        },
        {
            "title": "4 EXPERIMENTAL SETTINGS",
            "content": "We experiment on two LALMs, DeSTA2.5-Audio (Lu et al., 2025b) and Qwen2-AudioInstruct (Chu et al., 2024), chosen for their strong benchmark performance (Huang et al., 2025; Yang et al., 2025b; Sakshi et al., 2025; Chen et al., 2024b; Lu et al., 2025c). We apply greedy decoding and assess editing methods under two settings: single editing and sequential editing."
        },
        {
            "title": "4.1 EDITING METHODS",
            "content": "We evaluate seven editing methods, focusing on their effectiveness in modifying abstract auditory attribute knowledge. Below, we briefly introduce these methods, with the details provided in Appendix C. Figure 2: Example of sequential editing. For comparability, only the first five edits are evaluated, with gaps capped at five. For example, evaluating Edit 2 immediately results in gap of 0, whereas after Edit 10, only Edit 5 can be evaluated, with gap of 5 under this rule. Fine-tuning is common approach adapting for pre-trained models to new Following knowledge. prior work on LVLMs (Cheng et al., 2023; Huang et al., 2024; Zhang et al., 2024), we compare fine-tuning two parts in LALMs: the last layer of the LLM backbone, denoted as FT (LLM), and the modality connector between the audio encoder and the LLM backbone, denoted as FT (Audio). Knowledge Editor (KE) (De Cao et al., 2021) and MEND (Mitchell et al., 2022) trains hypernetwork to transform the fine-tuning gradients into parameter updates for the edit. UnKE (Deng et al., 2025) optimizes specific neurons of the chosen layers to produce the edit target. In-Context Knowledge Editing (IKE) (Zheng et al., 2023) leverages in-context learning (ICL) to enforce knowledge updates. We consider two variants: Instruction-based IKE (I-IKE), which encodes edits solely through natural language instructions in system prompts, and Instruction+Example IKE (IE-IKE), which provides auditory examples retrieved from the training set as dialog context."
        },
        {
            "title": "4.2 SINGLE EDITING AND SEQUENTIAL EDITING",
            "content": "Single editing evaluates the performance of editing single piece of knowledge, whereas sequential editing evaluates the performance after applying sequence of edits continuously on different knowledge, which better reflects real-world scenarios. For sequential editing, we construct ten independent sequences, each with ten editing instances. An editing sequence is denoted as = {(a(t) , y(t) , x(t) )}10 ) is sampled from Dedit and the corresponding original label y(t) is retrieved from our audio dataset. Here, indexes the order of edits within the sequence. t=1, where (a(t) , x(t) , y(t) , y(t) To measure how long an edit remains effective under subsequent edits, we define the gap as the number of editing steps between when an edit is applied and when it is evaluated (Figure 2). If an edit is introduced at step and evaluated at step i, then the gap is j. For consistency, we only consider the first five edits and require the gap to be at most five. This restriction keeps the number of samples comparable across gaps, since larger gaps naturally yield fewer available evaluations. To guarantee the validity of edit sequences, we impose two rules when sampling: (1) Editing pair independence: All original and edited labels in the sequence are mutually distinct, i.e., y(1) , y(10) appear only once within the sequence. This avoids contradictions that could compromise the evaluation of edits with subsequent ones (e.g., editing dog sounds to cat sounds and later editing dog sounds to frog sounds); and (2) Audio locality independence: For each sequential editing instance (a(t) ), denoted by (a(t) ℓ , y(t) ℓ , x(t) = None, are unrelated to the original labels of all edited instances y(1..10) , thereby ensuring independent evaluation of the current edit. ), all samples in its audio locality dataset L(a(t) , x(t) , y(t) , y(t) ℓ ) where a(t) , . . . , y(10) , y(1) , x(t) , y(t) ℓ 6 Preprint. Table 1: The four metrics (%) of the editing methods on the two models. Avg. indicates the average performance across all types of the corresponding metric. The best and second-best results are highlighted in bold and underlined, respectively. Method Reliability Generality Audio Locality Avg. Type 1 Type 2 Type 3 Avg. Type 1 Type 2 Type 3 Type 4 Text Locality Portability d - 5 . 2 e d - 2 Q FT (LLM) FT (Audio) KE MEND UnKE I-IKE IE-IKE FT (LLM) FT (Audio) 99.75 98.75 99.67 99.00 97.58 65. 88.08 15.56 74.67 69.75 82.58 19. 99.50 86.14 96.75 84.92 76.75 68. 78.67 48.11 76.17 64.42 100.00 55. 99.58 95.33 96.30 73.00 40.58 99. 99.25 99.25 99.00 79.29 96.33 43. 76.33 91.75 95.00 95.83 95.17 94. 71.47 93.42 17.22 74.67 87.00 89. 89.67 94.25 83.92 56.04 67.67 10. 71.92 62.67 61.47 64.67 59.58 60. 65.40 79.42 50.67 73.67 54.17 39. 41.50 38.67 38.67 58.76 70.08 49. 70.75 42.08 92.50 92.08 87.38 62. 56.25 100.00 99.94 100.00 100.00 99.83 67.42 91. 10.44 83.33 70.00 74.58 18.42 19. 18.73 71.42 34.58 24.67 100.00 81. 99.00 77.17 69.42 90.53 96.83 80. 90.00 92.58 100.00 50.67 KE 95. 86.67 92.00 87.67 80.33 83.47 89. 61.44 87.25 89.83 MEND 100.00 95. 98.92 95.92 91.17 83.27 98.50 49. 85.42 91.33 UnKE I-IKE IE-IKE 98. 10.33 8.00 98.53 99.08 98.83 97. 67.49 91.42 12.34 82.92 69.50 7. 6.58 10.33 8.50 5.67 5.75 5. 5.50 87.51 94.75 89.33 93.00 73. 82.89 91.25 86.56 89.67 65.00 84. 86.75 71.58 55.00 50.00 27.58 27. 30.18 28.92 27."
        },
        {
            "title": "4.3 EVALUATOR",
            "content": "Because LALMs often generate descriptive responses, we adopt LLM-as-a-judge (Chiang & Lee, 2023) to compute the metrics introduced in Sec. 3.2. In particular, we employ GPT-5 mini (gpt-5-mini-2025-08-07) as the evaluator, focusing on correctness and consistency. For the reliability, generality, and portability metrics, we assess whether the edited models responses correctly align with the ground truth. For locality, we examine whether the edited model preserves consistency with the original models outputs. To further validate the quality of the LLM-based judgments, we conduct human evaluation on 420 randomly selected samples. The results show overall 98.10% agreement with the LLM evaluator, demonstrating its robustness. Additional details and the evaluation prompts are provided in Appendix D."
        },
        {
            "title": "5.1 SINGLE EDITING",
            "content": "The main results of different knowledge editing methods on SAKE are shown in Table 1. Detailed results for each auditory attribute and relevant discussions are provided in Appendix E. Reliability. For both LALMs, most editing methods achieve high reliability, with FT (LLM), i.e., fine-tuning the LLM backbone, consistently yielding the best scores. In contrast, I-IKE and IE-IKE perform poorly, reaching only 73.00% and 40.58% on DeSTA2.5-Audio, and 10.32% and 8.00% on Qwen2-Audio. Interestingly, I-IKE outperforms IE-IKE on both models despite the latter using additional auditory examples. We attribute this to LALMs limited in-context learning ability: they struggle to handle multi-audio inputs and leverage examples, unlike in LLMs (Zheng et al., 2023) and LVLMs (Cheng et al., 2023; Huang et al., 2024) where such methods are effective. Generality. Compared with reliability, editing methods yield lower average generality scores. KE performs best on DeSTA2.5-Audio, while FT (LLM) leads on Qwen2-Audio. Baselines generally perform well on type 1 (textual neighborhood) but decline on type 2, which requires generalizing to similar but not identical auditory inputs with the same labels. Type 3, combining both text and 7 Preprint. auditory neighborhoods, proves most challenging, showing that current methods struggle to extend edited knowledge to auditory modalities as effectively as to text. Interestingly, FT (LLM) consistently outperforms FT (Audio) on generality despite their comparable reliability scores. This suggests that while fine-tuning either the LLM backbone or the modality connector ensures success on the editing instances themselves, the ability to generalize to similar inputs differs substantially, with training on the LLM backbone offering better generalizability. Audio Locality. Most methods perform poorly, with only limited success: KE preserves knowledge best on DeSTA2.5-Audio, while FT (Audio) performs better on Qwen2-Audio. This shows that maintaining stability in the auditory domain remains challenging despite advances in model editing. Among audio locality types, type 2 locality, where knowledge within the same attribute but unrelated to the edit is examined, is hardest to preserve, more so than knowledge of other attributes (type 1) or the edit target itself (type 3). This suggests that edits on one aspect of an attribute may inadvertently affect other aspects of the same attribute, reflecting degree of intra-attribute entanglement of auditory attribute knowledge. Notably, FT (LLM) performs much worse than FT (Audio) on type 2, indicating that despite better generalization, it fails to retain unrelated knowledge within the same attribute. By contrast, knowledge across different attributes (i.e., type 1) is less affected, indicating that cross-attribute boundaries tend to be more stable to preserve during editing. Focusing on type 4, which evaluates the preservation of general auditory processing capability, most editing methods struggle to retain LALMs general abilities, suggesting trade-off between effective editing and maintaining broad competence. KE and MEND perform best, likely due to their regularization that emphasizes locality in hypernetwork training. Although I-IKE and IE-IKE do not achieve high reliability and generality, they still preserve audio locality to some extent. This can be attributed to the limited in-context learning ability of DeSTA2.5Audio and Qwen2-Audio, which reduces the extent to which editing propagates. However, since DeSTA2.5-Audio possesses stronger instruction-following capability (Lu et al., 2025b), it is more susceptible to perturbations during editing, leading to lower locality scores than Qwen2-Audio. Text Locality. FT (Audio) achieves 100% since it fine-tunes only the modality connector, which is triggered solely by auditory inputs and leaves text-only capabilities intact. FT (LLM) tunes the LLM backbone, impairing text-only tasks and yielding much lower scores. KE and MEND preserve text locality better through regularization on irrelevant samples during hypernetwork training. UnKE disrupts text-based knowledge, especially on Qwen2-Audio, while IKE variants also struggle, likely due to sensitivity to contextual information in text-only scenarios, even when irrelevant. Portability. Overall, current editing methods do not guarantee portability when modifying auditory attribute knowledge. Parameter-updating approaches generally overlook this aspect, which poses major challenge for real-world deployment, where extensive related knowledge may need to be updated simultaneously. Among these methods, FT (Audio) achieves the most balanced performance on both LALMs, suggesting that training the modality connector can better integrate edited auditory knowledge with the internal world knowledge encoded in the LLM backbone. Interestingly, I-IKE performs best on DeSTA2.5-Audio. This may be because portability is closely tied to reasoning, and the IKE variants benefit when the underlying model has sufficient reasoning proficiency. Since DeSTA2.5-Audio demonstrates substantially stronger reasoning ability than Qwen2-Audio (Lu et al., 2025b), the IKE variants perform better on it. However, due to DeSTA2.5Audios still-limited multi-audio processing ability, IE-IKE remains less effective than I-IKE. Summary. In sum, while most editing methods can adjust LALMs to produce the desired knowledge on edited instances, they struggle to generalize these changes to equivalent cases, especially for auditory inputs. They also struggle to extend edits consistently to interconnected knowledge. In addition, they often fail to preserve original knowledge within the same attribute, which highlights the difficulty of preventing unintended perturbations. Among existing methods, fine-tuning, especially FT (Audio), remains strong baseline across our evaluations. Editing auditory attribute knowledge, therefore, remains highly challenging and calls for methods tailored to auditory modalities. 8 Preprint. (a) Sequential editing results of DeSTA2.5-Audio by edit gap (05). (b) Sequential editing results of Qwen2-Audio by edit gap (05). Figure 3: Comparison of sequential editing results across models by edit gap (05)."
        },
        {
            "title": "5.2 SEQUENTIAL EDITING",
            "content": "Figure 3a and 3b show sequential editing results on DeSTA2.5-Audio and Qwen2-Audio. Reliability and Generality. Most methods decline in reliability and generality as the gap increases, indicating tendency to forget previously edited auditory knowledge. On DeSTA2.5-Audio, MEND and KE deteriorate rapidly after few edits, often degenerating into repetitive outputs, with examples provided in Appendix G. In contrast, although weaker in single-editing settings, the IKE variants remain comparatively stable across larger gaps, yielding stronger long-term reliability and generality on DeSTA2.5-Audio, particularly for I-IKE. Locality. Most methods do not show the sharp declines seen in reliability and generality, suggesting sequential setups have weaker impact on knowledge preservation. However, as discussed in Sec. 5.1, their robustness still leaves considerable room for improvement, as none of the methods fully address the challenge of consistent knowledge preservation. Notable exceptions are KE and MEND on DeSTA2.5-Audio and KE and IE-IKE on Qwen2-Audio, which show marked declines due to degeneration issues and the models limited in-context learning ability. Portability. Similarly, the portability metric does not show significant declines under sequential editing. I-IKE and FT (Audio) consistently outperform other methods on DeSTA2.5-Audio and Qwen2-Audio, respectively, in line with the single-edit results in Table 1. This suggests that their ability to update interconnected knowledge remains relatively stable across several edits. Nevertheless, the overall performance of all methods remains unsatisfactory, underscoring the need for further improvement."
        },
        {
            "title": "6 CONCLUSIONS",
            "content": "In this paper, we present the first study on editing auditory attribute knowledge in LALMs. We introduce SAKE, benchmark designed to evaluate this type of knowledge editing across four dimensions. Through comprehensive experiments applying seven representative methods to two state-of-the-art LALMs, we reveal limitations in preserving non-target auditory knowledge and in generalizing edits to interconnected world knowledge during reasoning. Our findings provide new insights into this unexplored direction and establish foundation for future research on developing more robust and specialized knowledge editing methods for auditory modalities. Limitations. As the first benchmark for editing auditory attributes in LALMs, we focus on four representative attributes, though expanding further would allow broader evaluation. Similarly, while using two models follows common practice (Cheng et al., 2023; Zhang et al., 2024), including more 9 Preprint. LALMs could yield deeper insights. In addition, while our study focuses on speech-to-text LALMs, future work could extend to speech-to-speech LALMs (Fang et al., 2025; Xie & Wu, 2024; Yang et al., 2024b; Lin et al., 2024a; Chiang et al., 2025), which are likely to present greater challenges. We leave these directions for future work."
        },
        {
            "title": "ETHICS STATEMENT",
            "content": "This work studies knowledge editing in large audio-language models (LALMs). Our datasets are publicly available and licensed for research. While knowledge editing offers benefits such as correcting errors and mitigating bias, we acknowledge potential misuse (e.g., manipulation of model knowledge) and emphasize that our contributions are intended for socially responsible applications. We report our methods and results transparently to ensure reproducibility."
        },
        {
            "title": "REPRODUCIBILITY STATEMENT",
            "content": "Our implementation of editing methods is built on the EasyEdit toolkit (Wang et al., 2024b), with hyperparameters provided in Appendix C.2. To reduce randomness in the LLM-as-a-judge evaluation, we fix the temperature at 0. We also report the version of the judge model and include the evaluation prompts in Appendix D. Together, these details help ensure the reproducibility."
        },
        {
            "title": "REFERENCES",
            "content": "Rosana Ardila, Megan Branson, Kelly Davis, Michael Kohler, Josh Meyer, Michael Henretty, Reuben Morais, Lindsay Saunders, Francis Tyers, and Gregor Weber. Common voice: massively-multilingual speech corpus. In Nicoletta Calzolari, Frederic Bechet, Philippe Blache, Khalid Choukri, Christopher Cieri, Thierry Declerck, Sara Goggi, Hitoshi Isahara, Bente Maegaard, Joseph Mariani, Hel`ene Mazo, Asuncion Moreno, Jan Odijk, and Stelios Piperidis (eds.), Proceedings of the Twelfth Language Resources and Evaluation Conference, pp. 42184222, Marseille, France, May 2020. European Language Resources Association. ISBN 979-10-95546-34-4. URL https://aclanthology.org/2020.lrec-1.520/. Houwei Cao et al. Crema-d: Crowd-sourced emotional multimodal actors dataset. IEEE transactions on affective computing, 5(4):377390, 2014. Ruizhe Chen, Yichen Li, Zikai Xiao, and Zuozhu Liu. Large language model bias mitigation from the perspective of knowledge editing. In ICLR 2024 Workshop on Secure and Trustworthy Large Language Models, 2024a. URL https://openreview.net/forum?id=LflQOFSl3n. Yiming Chen, Xianghu Yue, Chen Zhang, Xiaoxue Gao, Robby T. Tan, and Haizhou Li. Voicebench: Benchmarking llm-based voice assistants. CoRR, abs/2410.17196, 2024b. URL https:// doi.org/10.48550/arXiv.2410.17196. Siyuan Cheng, Bozhong Tian, Qingbin Liu, Xi Chen, Yongheng Wang, Huajun Chen, and Ningyu Zhang. Can we edit multimodal large language models? In Houda Bouamor, Juan Pino, and Kalika Bali (eds.), Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing, pp. 1387713888, Singapore, December 2023. Association for Computational Linguistics. doi: 10.18653/v1/2023.emnlp-main.856. URL https://aclanthology.org/ 2023.emnlp-main.856/. Cheng-Han Chiang and Hung-yi Lee. Can large language models be an alternative to human evalIn Anna Rogers, Jordan Boyd-Graber, and Naoaki Okazaki (eds.), Proceedings of uations? the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pp. 1560715631, Toronto, Canada, July 2023. Association for Computational Linguistics. doi: 10.18653/v1/2023.acl-long.870. URL https://aclanthology.org/2023. acl-long.870/. Cheng-Han Chiang, Xiaofei Wang, Linjie Li, Chung-Ching Lin, Kevin Lin, Shujie Liu, Zhendong Wang, Zhengyuan Yang, Hung-yi Lee, and Lijuan Wang. Stitch: Simultaneous thinking and talking with chunked reasoning for spoken language models. arXiv preprint arXiv:2507.15375, 2025. 10 Preprint. Yunfei Chu, Jin Xu, Xiaohuan Zhou, Qian Yang, Shiliang Zhang, Zhijie Yan, Chang Zhou, and Jingren Zhou. Qwen-audio: Advancing universal audio understanding via unified large-scale audio-language models. arXiv preprint arXiv:2311.07919, 2023. Yunfei Chu, Jin Xu, Qian Yang, Haojie Wei, Xipin Wei, Zhifang Guo, Yichong Leng, Yuanjun Lv, Jinzheng He, Junyang Lin, et al. Qwen2-audio technical report. arXiv preprint arXiv:2407.10759, 2024. Nicola De Cao, Wilker Aziz, and Ivan Titov. Editing factual knowledge in language models. In Marie-Francine Moens, Xuanjing Huang, Lucia Specia, and Scott Wen-tau Yih (eds.), Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, pp. 6491 6506, Online and Punta Cana, Dominican Republic, November 2021. Association for Computational Linguistics. doi: 10.18653/v1/2021.emnlp-main.522. URL https://aclanthology. org/2021.emnlp-main.522/. Jingcheng Deng, Zihao Wei, Liang Pang, Hanxing Ding, Huawei Shen, and Xueqi Cheng. Everything is editable: Extend knowledge editing to unstructured data in large language models. In ICLR, 2025. Qingkai Fang, Shoutao Guo, Yan Zhou, Zhengrui Ma, Shaolei Zhang, and Yang Feng. LLaMAomni: Seamless speech interaction with large language models. In The Thirteenth International Conference on Learning Representations, 2025. URL https://openreview.net/forum? id=PYmrUQmMEw. Sreyan Ghosh, Zhifeng Kong, Sonal Kumar, Sakshi, Jaehyeon Kim, Wei Ping, Rafael Valle, Dinesh Manocha, and Bryan Catanzaro. Audio flamingo 2: An audio-language model with longaudio understanding and expert reasoning abilities. In Forty-second International Conference on Machine Learning, 2025. URL https://openreview.net/forum?id=xWu5qpDK6U. Yuan Gong et al. Joint audio and speech understanding. In 2023 IEEE Automatic Speech Recognition and Understanding Workshop (ASRU), 2023. Aaron Grattafiori, Abhimanyu Dubey, Abhinav Jauhri, Abhinav Pandey, Abhishek Kadian, Ahmad Al-Dahle, Aiesha Letman, Akhil Mathur, Alan Schelten, Alex Vaughan, et al. The llama 3 herd of models. arXiv preprint arXiv:2407.21783, 2024. Dan Hendrycks, Collin Burns, Steven Basart, Andy Zou, Mantas Mazeika, Dawn Song, and Jacob Steinhardt. Measuring massive multitask language understanding. In International Conference on Learning Representations, 2021. URL https://openreview.net/forum?id= d7KBjmI3GmQ. Chien-yu Huang, Wei-Chih Chen, Shu wen Yang, Andy T. Liu, Chen-An Li, Yu-Xiang Lin, WeiCheng Tseng, Anuj Diwan, Yi-Jen Shih, Jiatong Shi, William Chen, Xuanjun Chen, Chi-Yuan Hsiao, Puyuan Peng, Shih-Heng Wang, Chun-Yi Kuan, Ke-Han Lu, Kai-Wei Chang, ChihKai Yang, Fabian Alejandro Ritter Gutierrez, Huang Kuan-Po, Siddhant Arora, You-Kuan Lin, CHUANG Ming To, Eunjung Yeo, Kalvin Chang, Chung-Ming Chien, Kwanghee Choi, ChengHsiu Hsieh, Yi-Cheng Lin, Chee-En Yu, I-Hsiang Chiu, Heitor Guimaraes, Jionghao Han, TzuQuan Lin, Tzu-Yuan Lin, Homu Chang, Ting-Wu Chang, Chun Wei Chen, Shou-Jen Chen, YuHua Chen, Hsi-Chun Cheng, Kunal Dhawan, Jia-Lin Fang, Shi-Xin Fang, KUAN YU FANG CHIANG, Chi An Fu, Hsien-Fu Hsiao, Ching Yu Hsu, Shao-Syuan Huang, Lee Chen Wei, HsiChe Lin, Hsuan-Hao Lin, Hsuan-Ting Lin, Jian-Ren Lin, Ting-Chun Liu, Li-Chun Lu, Tsung-Min Pai, Ankita Pasad, Shih-Yun Shan Kuan, Suwon Shon, Yuxun Tang, Yun-Shao Tsai, Wei Jui Chiang, Tzu-Chieh Wei, Chengxi Wu, Dien-Ruei Wu, Chao-Han Huck Yang, Chieh-Chi Yang, Jia Qi Yip, Shao-Xiang Yuan, Haibin Wu, Karen Livescu, David Harwath, Shinji Watanabe, and Hung yi Lee. Dynamic-SUPERB phase-2: collaboratively expanding benchmark for measuring the capabilities of spoken language models with 180 tasks. In The Thirteenth International Conference on Learning Representations, 2025. URL https://openreview.net/forum?id= s7lzZpAW7T. Han Huang, Haitian Zhong, Tao Yu, Qiang Liu, Shu Wu, Liang Wang, and Tieniu Tan. Vlkeb: In A. Globerson, L. Mackey, large vision-language model knowledge editing benchmark. Preprint. D. Belgrave, A. Fan, U. Paquet, J. Tomczak, and C. Zhang (eds.), Advances in Neural Information Processing Systems, volume 37, pp. 92579280. Curran Associates, Inc., 2024. https://proceedings.neurips.cc/paper_files/paper/2024/file/ URL 1198b53fa686831d5f0c0860d7ec4f34-Paper-Datasets_and_Benchmarks_ Track.pdf. Aaron Hurst, Adam Lerer, Adam Goucher, Adam Perelman, Aditya Ramesh, Aidan Clark, AJ Ostrow, Akila Welihinda, Alan Hayes, Alec Radford, et al. Gpt-4o system card. arXiv preprint arXiv:2410.21276, 2024. Houcheng Jiang, Junfeng Fang, Ningyu Zhang, Guojun Ma, Mingyang Wan, Xiang Wang, Xiangnan He, and Tat seng Chua. Anyedit: Edit any knowledge encoded in language models, 2025. URL https://arxiv.org/abs/2502.05628. Chun-Yi Kuan, Chih-Kai Yang, Wei-Ping Huang, Ke-Han Lu, and Hung-yi Lee. Speech-copilot: Leveraging large language models for speech processing via task decomposition, modularization, and program generation. In 2024 IEEE Spoken Language Technology Workshop (SLT), pp. 1060 1067. IEEE, 2024. Chae-Won Lee, Jae-Hong Lee, and Joon-Hyuk Chang. Language model personalization for speech recognition: clustered federated learning approach with adaptive weight average. IEEE Signal Processing Letters, 2024. Junnan Li, Dongxu Li, Silvio Savarese, and Steven Hoi. Blip-2: Bootstrapping language-image pre-training with frozen image encoders and large language models. In International conference on machine learning, pp. 1973019742. PMLR, 2023. Zexi Li, Xiangzhu Wang, William Shen, Meghdad Kurmanji, Xinchi Qiu, Dongqi Cai, Chao Wu, and Nicholas Lane. Editing as unlearning: Are knowledge editing methods strong baselines for large language model unlearning? arXiv preprint arXiv:2505.19855, 2025. Guan-Ting Lin, Prashanth Gurunath Shivakumar, Aditya Gourav, Yile Gu, Ankur Gandhe, Hung-yi Lee, and Ivan Bulyko. Align-slm: Textless spoken language models with reinforcement learning from ai feedback. arXiv preprint arXiv:2411.01834, 2024a. Yi-Cheng Lin, Tzu-Quan Lin, Chih-Kai Yang, Ke-Han Lu, Wei-Chih Chen, Chun-Yi Kuan, and Hung-yi Lee. Listen and speak fairly: study on semantic gender bias in speech integrated large In 2024 IEEE Spoken Language Technology Workshop (SLT), pp. 439446. language models. IEEE, 2024b. Yu-Xiang Lin, Chih-Kai Yang, Wei-Chih Chen, Chen-An Li, Chien-yu Huang, Xuanjun Chen, arXiv preprint and Hung-yi Lee. preliminary exploration with gpt-4o voice mode. arXiv:2502.09940, 2025. Haotian Liu, Chunyuan Li, Qingyang Wu, and Yong Jae Lee. Visual instruction tuning. Advances in neural information processing systems, 36:3489234916, 2023. Ke-Han Lu, Zhehuai Chen, Szu-Wei Fu, Chao-Han Huck Yang, Jagadeesh Balam, Boris Ginsburg, Yu-Chiang Frank Wang, and Hung-Yi Lee. Developing instruction-following speech lanIn ICASSP 2025 - 2025 IEEE Internaguage model without speech instruction-tuning data. tional Conference on Acoustics, Speech and Signal Processing (ICASSP), pp. 15, 2025a. doi: 10.1109/ICASSP49660.2025.10889444. Ke-Han Lu, Zhehuai Chen, Szu-Wei Fu, Chao-Han Huck Yang, Sung-Feng Huang, Chih-Kai Yang, Chee-En Yu, Chun-Wei Chen, Wei-Chih Chen, Chien-yu Huang, et al. Desta2.5-audio: Toward general-purpose large audio language model with self-generated cross-modal alignment. arXiv preprint arXiv:2507.02768, 2025b. Ke-Han Lu, Chun-Yi Kuan, and Hung-yi Lee. Speech-IFEval: Evaluating instruction-following and quantifying catastrophic forgetting in speech-aware language models. In Interspeech 2025, 2025c. Preprint. Zhenyan Lu, Daliang Xu, Dongqi Cai, Zexi Li, Wei Liu, Fangming Liu, Shangguang Wang, and Mengwei Xu. Mobiedit: Resource-efficient knowledge editing for personalized on-device llms. arXiv preprint arXiv:2506.13772, 2025d. Kevin Meng, David Bau, Alex Andonian, and Yonatan Belinkov. Locating and editing factual associations in GPT. Advances in Neural Information Processing Systems, 36, 2022. arXiv:2202.05262. Kevin Meng, Arnab Sen Sharma, Alex Andonian, Yonatan Belinkov, and David Bau. Mass-editing memory in transformer, 2023. URL https://arxiv.org/abs/2210.07229. Eric Mitchell, Charles Lin, Antoine Bosselut, Chelsea Finn, and Christopher Manning. Fast model editing at scale. In International Conference on Learning Representations, 2022. URL https://openreview.net/forum?id=0DcZxeWfOPt. Karol Piczak. Esc: Dataset for environmental sound classification. In Proceedings of the 23rd ACM international conference on Multimedia, pp. 10151018, 2015. Alec Radford et al. Robust speech recognition via large-scale weak supervision. In International Conference on Machine Learning, pp. 2849228518. PMLR, 2023. Sakshi, Utkarsh Tyagi, Sonal Kumar, Ashish Seth, Ramaneswaran Selvakumar, Oriol Nieto, Ramani Duraiswami, Sreyan Ghosh, and Dinesh Manocha. MMAU: massive multi-task audio understanding and reasoning benchmark. In The Thirteenth International Conference on Learning Representations, 2025. URL https://openreview.net/forum?id=TeVAZXr3yv. Emre asmaz and Boray Tek. Animal sound classification using convolutional neural network. In 2018 3rd International Conference on Computer Science and Engineering (UBMK), pp. 625 629. IEEE, 2018. Zian Su, Ziyang Huang, Kaiyuan Zhang, and Xiangyu Zhang. µke: Matryoshka unstructured knowledge editing of large language models, 2025. URL https://arxiv.org/abs/2504. 01196. Changli Tang, Wenyi Yu, Guangzhi Sun, Xianzhao Chen, Tian Tan, Wei Li, Lu Lu, Zejun MA, and Chao Zhang. SALMONN: Towards generic hearing abilities for large language models. In The Twelfth International Conference on Learning Representations, 2024. URL https: //openreview.net/forum?id=14rn7HpKVk. James Thorne, Andreas Vlachos, Christos Christodoulopoulos, and Arpit Mittal. FEVER: largescale dataset for fact extraction and VERification. In Marilyn Walker, Heng Ji, and Amanda Stent (eds.), Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long Papers), pp. 809819, New Orleans, Louisiana, June 2018. Association for Computational Linguistics. doi: 10.18653/v1/N18-1074. URL https://aclanthology.org/N18-1074/. Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timothee Lacroix, Baptiste Rozi`ere, Naman Goyal, Eric Hambro, Faisal Azhar, et al. Llama: Open and efficient foundation language models. arXiv preprint arXiv:2302.13971, 2023. Mengru Wang, Ningyu Zhang, Ziwen Xu, Zekun Xi, Shumin Deng, Yunzhi Yao, Qishen Zhang, Linyi Yang, Jindong Wang, and Huajun Chen. Detoxifying large language models via knowlIn Lun-Wei Ku, Andre Martins, and Vivek Srikumar (eds.), Proceedings of the edge editing. 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pp. 30933118, Bangkok, Thailand, August 2024a. Association for Computational Linguistics. doi: 10.18653/v1/2024.acl-long.171. URL https://aclanthology.org/2024. acl-long.171/. Peng Wang, Ningyu Zhang, Bozhong Tian, Zekun Xi, Yunzhi Yao, Ziwen Xu, Mengru Wang, Shengyu Mao, Xiaohan Wang, Siyuan Cheng, et al. Easyedit: An easy-to-use knowledge editing framework for large language models. In Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 3: System Demonstrations), pp. 8293, 2024b. Zhifei Xie and Changqiao Wu. Mini-omni: Language models can hear, talk while thinking in streaming. arXiv preprint arXiv:2408.16725, 2024. 13 Preprint. An Yang, Baosong Yang, Binyuan Hui, Bo Zheng, Bowen Yu, Chang Zhou, Chengpeng Li, Chengyuan Li, Dayiheng Liu, Fei Huang, Guanting Dong, Haoran Wei, Huan Lin, Jialong Tang, Jialin Wang, Jian Yang, Jianhong Tu, Jianwei Zhang, Jianxin Ma, Jianxin Yang, Jin Xu, Jingren Zhou, Jinze Bai, Jinzheng He, Junyang Lin, Kai Dang, Keming Lu, Keqin Chen, Kexin Yang, Mei Li, Mingfeng Xue, Na Ni, Pei Zhang, Peng Wang, Ru Peng, Rui Men, Ruize Gao, Runji Lin, Shijie Wang, Shuai Bai, Sinan Tan, Tianhang Zhu, Tianhao Li, Tianyu Liu, Wenbin Ge, Xiaodong Deng, Xiaohuan Zhou, Xingzhang Ren, Xinyu Zhang, Xipin Wei, Xuancheng Ren, Xuejing Liu, Yang Fan, Yang Yao, Yichang Zhang, Yu Wan, Yunfei Chu, Yuqiong Liu, Zeyu Cui, Zhenru Zhang, Zhifang Guo, and Zhihao Fan. Qwen2 technical report, 2024a. URL https://arxiv.org/abs/2407.10671. Chih-Kai Yang, Yu-Kuan Fu, Chen-An Li, Yi-Cheng Lin, Yu-Xiang Lin, Wei-Chih Chen, Ho Lam Chung, Chun-Yi Kuan, Wei-Ping Huang, Ke-Han Lu, et al. Building taiwanese mandarin spoken language model: first attempt. arXiv preprint arXiv:2411.07111, 2024b. Chih-Kai Yang, Neo Ho, Yi-Jyun Lee, and Hung-yi Lee. Audiolens: closer look at auditory attribute perception of large audio-language models. arXiv preprint arXiv:2506.05140, 2025a. Chih-Kai Yang, Neo Ho, Yen-Ting Piao, and Hung yi Lee. SAKURA: On the Multi-hop Reasoning of Large Audio-Language Models Based on Speech and Audio Information. In Interspeech 2025, pp. 17881792, 2025b. doi: 10.21437/Interspeech.2025-839. Chih-Kai Yang, Neo Ho, and Hung-yi Lee. Towards holistic evaluation of large audio-language models: comprehensive survey. arXiv preprint arXiv:2505.15957, 2025c. Yunzhi Yao, Peng Wang, Bozhong Tian, Siyuan Cheng, Zhoubo Li, Shumin Deng, Huajun Chen, and Ningyu Zhang. Editing large language models: Problems, methods, and opportunities. In Houda Bouamor, Juan Pino, and Kalika Bali (eds.), Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing, pp. 1022210240, Singapore, December 2023. Association for Computational Linguistics. doi: 10.18653/v1/2023.emnlp-main.632. URL https://aclanthology.org/2023.emnlp-main.632/. Junzhe Zhang, Huixuan Zhang, Xunjian Yin, Baizhou Huang, Xu Zhang, Xinyu Hu, and Xiaojun Wan. Mc-mke: fine-grained multimodal knowledge editing benchmark emphasizing modality consistency. arXiv preprint arXiv:2406.13219, 2024. Ce Zheng, Lei Li, Qingxiu Dong, Yuxuan Fan, Zhiyong Wu, Jingjing Xu, and Baobao Chang. Can we edit factual knowledge by in-context learning? In Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing, pp. 48624876, 2023. Yaochen Zhu, Yinhan He, Jing Ma, Mengxuan Hu, Sheng Li, and Jundong Li. Causal inference with latent variables: Recent advances and future prospectives. In Proceedings of the 30th ACM SIGKDD Conference on Knowledge Discovery and Data Mining, pp. 66776687, 2024. 14 Preprint. THE USAGE OF LARGE LANGUAGE MODELS (LLMS) In this work, LLMs were used as judge models and as auxiliary tools for linguistic assistance, including polishing writing style, refining grammar, and proofreading. The conceptualization of the research problem, the design and construction of the benchmark, the execution of experiments, and the analysis and interpretation of results were carried out entirely by the authors without LLM involvement. All technical contributions and intellectual efforts originate from the authors, with LLMs serving only to support evaluation and to improve the clarity and readability of the manuscript."
        },
        {
            "title": "B DATASET STATISTICS",
            "content": "We report the dataset statistics in Table 2 and Table 3, respectively, for the training and testing datasets, highlighting the diversity of the data. Table 2: Dataset summary for each evaluation metric in our training dataset. (a) Question Length (word). (b) Audio/speech duration (s)."
        },
        {
            "title": "Audio\nLocality",
            "content": "Avg. Type 1 Type 2 Type 3 Avg. Type 1 Type 2 Type 3 Type"
        },
        {
            "title": "Text Locality",
            "content": "Avg. Min. Max. 16.40 16.40 16.40 16.40 16.40 26.03 16.40 16.90 16.40 52. 55.32 12 12 12 12 12 8 12 13 12 7 22 22 22 22 22 604 22 22 22 526 Std. 2.32 2.32 2.32 2.32 2.32 57. 2.32 2.23 2.32 107.09 48."
        },
        {
            "title": "Audio\nLocality",
            "content": "Avg. Type 1 Type 2 Type 3 Avg. Type 1 Type 2 Type 3 Type 4 Avg. Min. Max. 4. 4.62 4.62 4.62 4.62 8.27 4.62 4.28 4.62 18.56 0.13 0. 0.13 0.13 0.13 0.11 0.13 0.13 0.13 0.11 Std. 2.27 2. 2.27 2.27 2.27 17.81 17.81 17.81 17.81 17.81 1478.35 31. 17.81 17.81 17.81 1478.35 2.27 2.22 2.27 59.16 Table 3: Dataset summary for each evaluation metric in our testing dataset. (a) Question Length (word). (b) Audio/speech duration (s)."
        },
        {
            "title": "Audio\nLocality",
            "content": "Avg. Type 1 Type 2 Type 3 Avg. Type 1 Type 2 Type 3 Type"
        },
        {
            "title": "Portability",
            "content": "Avg. Min. Max. Std. 24.23 23.83 23.63 24.23 23.63 31. 23.93 25.35 23.94 51.29 53.59 33.53 7 7 7 7 7 7 12 7 9 7 13 48 48 48 48 604 48 48 48 604 307 63 10. 9.99 9.85 10.24 9.85 55.81 9.91 9.61 9.92 104.30 42.28 12."
        },
        {
            "title": "Audio\nLocality",
            "content": "Avg. Type 1 Type 2 Type 3 Avg. Type 1 Type 2 Type 3 Type"
        },
        {
            "title": "Portability",
            "content": "Avg. Min. Max. 4.75 5.82 4.75 6.35 6.35 8.12 4.78 4.41 4.73 17. 4.75 0.24 0.24 0.24 0.24 0.24 0.17 0.28 0.24 0.24 0. 0.24 Std. 2.43 4.98 2.43 5.79 5.79 20. 20.78 20.78 20.78 20.78 759.53 26.98 20.78 20.78 20.78 759.53 2.46 2.38 2.50 50. 20.78 2."
        },
        {
            "title": "C IMPLEMENTATION DETAILS OF THE EDITING METHODS",
            "content": "C.1 EDITING METHODS Fine-tuning adapts the model via gradient descent on selected components. We apply it to the last layer of the LLM backbone and the modality connector between audio encoders and the backbone in LALMs. 15 Preprint. Knowledge Editor (KE) (De Cao et al., 2021) employs hyper-network to update parameters. It leverages bidirectional LSTM with constrained optimization to predict weight updates. MEND (Mitchell et al., 2022) uses hypernetwork to generate parameter updates by decomposing fine-tuning gradients into low-rank forms and transforming them into parameter updates. UnKE (Deng et al., 2025) is an unstructured knowledge editing method. UnKE first finds modified key vector by adding small residual (delta) to the hidden state of chosen layer so that the models output shifts to the desired target. In the second stage, the parameters of the chosen layer are updated to make the chosen layer naturally produce this new key vector. In-Context Knowledge Editing (IKE) (Zheng et al., 2023) uses in-context learning (ICL) to modify model knowledge without parameter updates, relying on instructions and demonstrations to enforce the edited knowledge. We evaluate two variants: Instruction-based IKE (I-IKE) and Instruction+Example IKE (IE-IKE). C."
        },
        {
            "title": "IMPLEMENTATION DETAILS",
            "content": "The hyperparameters of each editing method are shown in Table 4, and their approximate execution time on NVIDIA H100 GPU is reported in Table 5. For FT (LLM) and FT (Audio), early stopping is applied when the loss falls below 1e-2. For KE and MEND, which require training hypernetwork, we train the hyper-network for one epoch due to its rapid convergence on the validation set. For UnKE, the editing process consists of two stages with separate hyperparameters. The first stage uses step iterations to find the desired key vector, with its own learning rate LR. The second stage uses optim step iterations to update the weights of the chosen layer so that it naturally produces the new key vector, controlled by separate learning rate optim LR. To prevent interference with unrelated knowledge, stage 2 also incorporates preservation data. Specifically, we use 20 preservation samples for text and 5 for audio. For I-IKE and IE-IKE, the edited knowledge is encoded through natural language instructions in the system prompt, using the prompt template provided below. In IE-IKE, the demonstration examples are retrieved from the training set according to the editing pair. For example, when editing the models perception of speaker emotion from happy to sad, we retrieve an editing instance from the training set involving the same (happy, sad) editing pair. The demonstrating examples cover reliability, generality, and locality. System Prompt Template for I-IKE and IE-IKE You are performing in-context knowledge editing, but only for questions about audio inputs. From now on, when analyzing any audio clip, you must apply the following edits consistently: EDITS (New facts): {edits} where each entry has the form pre edit post edit. Rules: 1. If your reasoning or prediction about an audio clip would normally lead to pre edit, you must instead treat it as post edit. 2. All properties, attributes, and facts that belong to post edit must be applied consistently, as if the audio were actually from post edit. 3. changes. 4. Always ensure your final answers are fully consistent with the edited mapping. If the users question is unrelated to these edits, you should answer normally without making MORE DETAILS AND EVALUATION PROMPTS FOR LLM-AS-A-JUDGE We use GPT-5 mini with minimal reasoning effort as the judge in our LLM evaluator, balancing strong alignment with human judgments and cost efficiency. The temperature is set to 0 to ensure maximal reproducibility. Human verification was conducted on 210 randomly selected samples for correctness and consistency respectively, yielding 98.57% and 97.62% agreement. As described in 16 Preprint. Table 4: Hyper-parameters of each editing method. I-IKE and IE-IKE are excluded because they do not modify model parameters. FT (LLM)"
        },
        {
            "title": "Max Steps",
            "content": "DeSTA2.5-Audio 15 15 Qwen2-Audio Edit Layer 31st layer of Transformer Module 31st layer of Transformer Module"
        },
        {
            "title": "Adam\nAdam",
            "content": "1e-5 1e-4 FT (Audio)"
        },
        {
            "title": "Edit LR",
            "content": "DeSTA2.5-Audio 15 15 Qwen2-Audio perception.connector multi modal projector KE"
        },
        {
            "title": "Optimizer",
            "content": "DeSTA2.5-Audio 1 1 Qwen2-Audio layer 29. 30, 31 of Transformer Module RMSprop layer 29. 30, 31 of Transformer Module RMSprop"
        },
        {
            "title": "Optimizer",
            "content": "DeSTA2.5-Audio 1 1 Qwen2-Audio layer 29. 30, 31 of Transformer Module Adam layer 29. 30, 31 of Transformer Module Adam 1e-4 1e-4 LR 3e-4 3e-4 LR 1e-6 1e-"
        },
        {
            "title": "Model",
            "content": "v step/optim step Edit Layer preserve data LR/optim LR DeSTA2.5-Audio 25/50 25/50 Qwen2-Audio layer 15 of Transformer Module layer 20 of Transformer Module 20(text)/5(audio) 5e-1/2e-4 20(text)/5(audio) 5e-1/2eTable 5: Approximate execution time of each editing method on an NVIDIA H100 GPU, measured for training the trainer, single editing, and sequential editing."
        },
        {
            "title": "Model",
            "content": "FT (LLM) FT (Audio) KE"
        },
        {
            "title": "UnKE",
            "content": "I-IKE IE-IKE DeSTA2.5-Audio Qwen2-Audio DeSTA2.5-Audio Qwen2-Audio DeSTA2.5-Audio Qwen2-Audio DeSTA2.5-Audio Qwen2-Audio DeSTA2.5-Audio Qwen2-Audio DeSTA2.5-Audio Qwen2-Audio DeSTA2.5-Audio Qwen2-Audio"
        },
        {
            "title": "Sequential Editing",
            "content": "- - - - 4h 10m 2h 10m 4h 10m 2h 20m - - - - - - 5h 50m 2h 35m 4h 20m 3h 0m 8h 40m 4h 50m 4h 0m 2h 10m 3h 0m 1h 35m 6h 0m 3h 0m 3h 20m 2h 0m 2h 0m 1h 15m 2h 0m 1h 30m 9h 45m 3h 20m 4h 10m 1h 50m 1h 05m 0h 30m 3h 45m 2h 0m 8h 0m 3h 50m Sec. 4.3, evaluation tasks are categorized into correctness and consistency, with prompts provided below. To further reduce costs, some cases are judged directly without invoking the LLM evaluator: outputs that are empty are marked as incorrect/inconsistent, and responses that exactly match the ground truth (in correctness) or the original models output (in consistency) are marked as correct/- consistent. 17 Preprint."
        },
        {
            "title": "LLM Judge Prompt for Correctness",
            "content": "Task. You will be given question, list of possible options, ground-truth answer, and modelgenerated response. Determine whether the model-generated response is correct according to the criteria below. Criteria. 1. There is one and only one correct answer. Judge incorrect if the response does not choose any option from the list, or if it chooses more than one option. 2. If exactly one option from the list is chosen, judge correct if the chosen option aligns with the ground-truth answer; otherwise, judge incorrect. 3. Read the question, options, ground-truth answer, and model-generated response carefully before deciding. Examples. Example 1. Question: What is the capital of France? Options: Paris; London; Berlin; Madrid. Ground truth answer: Paris Response: The capital of France is Tokyo. incorrect (does not choose any listed option). Response: The capital of France is Paris and London. incorrect (chooses more than one option). Response: The capital of France is London. incorrect (chosen option does not match ground truth). Response: The capital of France is Paris. correct. Example 2. Question: What is the underlying emotion of the speaker? Options: Happy; Sad; Angry; Neutral. Ground truth answer: Happy Response: The speaker is happy. correct. Response: The speaker expresses happiness. correct (happiness is valid morphological variant of happy). Response: Happiness. correct. Now judge: Question: {question} Ground truth answer: {ground_truth} Model-generated response: {model_response} Return format. Explanation: <Your explanation on your judgement> Judgement: <correct or incorrect>"
        },
        {
            "title": "LLM Judge Prompt for Consistency",
            "content": "Role. You are an expert evaluator tasked with determining whether two model outputs are semantically consistent for the same input. Inputs. Question: {question} Original Output: {original_output} New Output: {new_output} Task. Decide if these two outputs are semantically consistent. Outputs are consistent if they express the same main message, even with different wording, phrasing, or level of detail. Outputs are inconsistent if they contradict each other, reach different conclusions, or provide fundamentally different answers. Return format. Explanation: <Your explanation on your judgement> Judgement: <consistent or inconsistent> 18 Preprint. Table 6: Detailed results of the four metrics of each auditory attribute across different editing methods on DeSTA2.5-Audio under single editing. Attr. denotes auditory attributes, and Port. denotes portability. For generality and audio locality, Avg. indicates the average performance across all types of the corresponding metric. (%)"
        },
        {
            "title": "Method",
            "content": "Attr."
        },
        {
            "title": "Audio Locality",
            "content": "Avg. Type 1 Type 2 Type 3 Avg. Type 1 Type 2 Type 3 Type"
        },
        {
            "title": "Text\nLocality",
            "content": "Port."
        },
        {
            "title": "ALL",
            "content": "99.75 98.75 99.67 99.00 97.58 65.11 88.08 15. 74.67 69.75 82.58 19.42 FT (LLM) FT (Audio) KE"
        },
        {
            "title": "UnKE",
            "content": "I-IKE IE-IKE"
        },
        {
            "title": "Animal\nEmotion\nGender\nLanguage",
            "content": "99.33 100.00 99.67 100.00 99.50 99.00 99.67 99.67 99.67 99.58 98.67 100.00 99.67 100.00 95. 90.00 99.67 98.67 93.00 96.30 99.30 96.00 99.60 90.30 73.00 90.33 59.33 69.33 38.67 40. 77.00 27.00 41.67 16.67 99.33 99.67 98.33 55.08 90.67 99.11 99.33 100.00 99.33 53.25 81.67 99.56 96.78 94.00 84.67 90.33 99.67 99.56 100.00 100.00 98.67 72.33 89.67 96.67 9.00 7.00 - 30.67 56.33 59.67 88.33 94. 64.33 64.67 75.33 74.67 82.00 79.33 86.33 82.67 20.00 23.00 7.00 27.67 86.14 96.75 84. 76.75 68.09 78.67 48.11 76.17 64.42 100.00 55. 85.78 78.67 92.33 87.78 97.33 95.33 99.33 95.00 83.00 73.67 92.67 90.33 77.00 62.00 81.67 67.00 60.92 77.67 85.00 77.67 78.33 78.00 74.17 77.00 40.67 38.67 - 65.00 60.00 66.67 87.33 90. 65.67 60.67 67.33 64.00 100.00 100.00 100.00 100.00 30.33 46.33 82.33 63.00 99.17 99.25 99. 99.00 79.29 96.33 43.89 76.33 91.75 92.50 18. 97.67 97.33 97.33 97.00 77.33 97.00 100.00 100.00 100.00 100.00 66.17 98.33 99.56 99.67 93.89 96.33 99.78 100.00 100.00 99.33 83.42 93.67 99.67 99.33 61.67 16.33 - 53. 59.00 59.33 92.33 94.67 91.67 90.67 93.00 91.67 91.33 93.33 92.67 92.67 23.67 16.00 9.33 24.67 95.00 95. 95.17 94.00 71.47 93.42 17.22 74.67 87.00 92. 19.42 89.44 98.44 97.00 95.11 91.33 99.00 97.33 95.67 89.00 99.00 99.33 93.33 88.00 63.50 93.67 97.33 62.25 94.33 94.33 91.89 97.33 96.33 73.33 88.33 19.33 9.00 - 23. 57.00 59.67 88.33 93.67 84.00 86.00 90.00 88.00 92.00 94.00 90.00 92.33 20.33 27.33 4.33 25.67 89.28 89. 94.25 83.92 56.04 67.67 10.56 71.92 62.67 87. 18.73 95.67 91.56 86.11 83.78 98.00 91.33 85.67 83.67 98.33 94.33 99.67 84.67 90.67 51.83 69.33 89.00 50.42 66.33 73.00 72.33 58.67 83.00 53.67 76.33 15.67 12.00 - 4. 59.67 57.00 94.33 76.67 62.67 66.33 64.00 57.67 86.00 87.60 86.60 89.30 27.30 20.00 7.30 20.30 61.47 64. 59.58 60.17 65.40 79.42 50.67 73.67 54.17 62. 71.42 92.89 50.89 64.33 37.78 92.67 58.00 71.33 36.67 93.00 47.00 60.33 38.00 93.00 54.75 84.33 47.67 60.08 80.67 61.33 73.33 81.67 38.67 75.42 71.00 22.00 45.33 - 84. 59.67 62.33 79.67 93.00 53.00 52.00 58.67 53.00 60.00 59.00 64.33 65.00 78.33 65.33 73.00 69.00 39.61 41. 38.67 38.67 58.76 70.08 49.89 70.75 42.08 56. 34.58 78.56 26.67 37.67 15.56 80.00 28.67 43.00 14.33 76.67 25.33 36.00 16.67 79.00 51.17 78.67 26.00 51.33 68.00 34.00 61.67 74.67 15.67 71.58 59.00 23.33 33.33 - 93. 59.00 61.33 67.67 95.00 43.67 42.67 42.67 39.33 60.33 51.33 60.33 53.00 44.33 29.00 32.67 32."
        },
        {
            "title": "EDITING",
            "content": "We compare the performance for each auditory attribute (animal sound, speaker emotion, speaker gender, and spoken language) under single editing here, as shown in Table 6 and Table 7 for DeSTA2.5-Audio and Qwen2-Audio, respectively. Reliability. All methods except I-IKE and IE-IKE exhibit consistently high reliability across all attributes, suggesting that they can effectively update the edited knowledge on both LALMs. In contrast, the performance of I-IKE and IE-IKE varies by attribute and model, which we attribute to differences in the original models ability to perceive auditory attributes. Generality. For most methods, their generality remains relatively stable across different auditory attributes. However, FT (Audio) exhibits notable exception: although it achieves consistently high reliability, its generality on emotion attributes drops the most compared to its reliability on both LALMs. This suggests that editing the modality connector makes it harder for the model to extend edited knowledge to similar inputs within emotion. 19 Preprint. Table 7: Detailed results of the four metrics of each auditory attribute across different editing methods on Qwen2-Audio under single editing. Attr. denotes auditory attributes, and Port. denotes portability. For generality and audio locality, Avg. indicates the average performance across all types of the corresponding metric. (%)"
        },
        {
            "title": "Method",
            "content": "Attr."
        },
        {
            "title": "Audio Locality",
            "content": "Avg. Type 1 Type 2 Type 3 Avg. Type 1 Type 2 Type 3 Type"
        },
        {
            "title": "Text\nLocality",
            "content": "Port."
        },
        {
            "title": "ALL",
            "content": "100.00 99.94 100.00 100.00 99.83 67.42 91.67 10.44 83.33 70.00 74. 24.67 FT (LLM)"
        },
        {
            "title": "Animal\nEmotion\nGender\nLanguage",
            "content": "100.00 100.00 100.00 100.00 99.89 100.00 100.00 99.67 64.00 93.00 100.00 100.00 100.00 100.00 59.00 93.67 100.00 100.00 100.00 100.00 83.78 85.67 99.89 100.00 100.00 99.67 67.00 94.33 8.67 8.00 - 14.67 86.33 66.00 92.33 88.67 68.00 68.33 73.33 70.33 70.00 80.00 77.67 70. 22.67 20.67 26.33 29."
        },
        {
            "title": "ALL",
            "content": "100.00 81.86 99.00 77.17 69.42 90.53 96.83 80. 90.00 92.58 100.00 50.67 FT (Audio) KE"
        },
        {
            "title": "Animal\nEmotion\nGender\nLanguage",
            "content": "100.00 100.00 100.00 100.00 95.50 97.00 98.33 87.67 99.00 84.44 99.00 81.00 62.67 99.00 71.67 97.44 100.00 96.00 69.00 98.00 73.89 73.33 94.25 98.00 53.33 85.08 98.33 96.33 94.56 98.00 54.67 89.25 93.00 94.00 67.00 - 79. 93.33 84.33 92.00 90.33 91.67 90.67 93.67 94.33 100.00 100.00 100.00 100.00 48.67 43.00 63.00 48.00 86.67 92. 87.67 80.33 83.47 89.83 61.44 87.25 89.83 84. 27.58 88.22 76.67 89.22 92.56 89.33 91.33 90.33 97.00 94.00 74.33 88.33 94.00 81.33 88.42 93.67 64.33 78.00 97.33 89.00 84.78 72.67 86.67 83.00 95.67 76.33 52.67 - 55. 92.33 72.00 93.67 91.00 91.33 90.00 88.00 90.00 81.33 87.00 85.00 86.33 24.67 23.33 39.00 23."
        },
        {
            "title": "ALL",
            "content": "100.00 95.33 98.92 95.92 91.17 83.27 98.50 49. 85.42 91.33 86.75 27."
        },
        {
            "title": "Animal\nEmotion\nGender\nLanguage",
            "content": "100.00 100.00 100.00 100.00 98.33 98.00 97.33 89.89 92.00 99.00 95.89 100.00 94.33 99.00 98.67 98.22 64.00 95.67 84.50 97.00 78.67 75.67 100.00 42.00 93.33 94.89 99.00 97.00 80.92 98.00 - 42.00 89.67 70.33 92.33 89.33 87.33 90.33 93.33 94. 83.67 89.00 88.67 85.67 22.67 23.67 37.00 25."
        },
        {
            "title": "ALL",
            "content": "98.60 98.53 99.08 98.83 97.67 67.49 91.42 12. 82.92 69.50 71.58 30."
        },
        {
            "title": "UnKE",
            "content": "I-IKE IE-IKE"
        },
        {
            "title": "Animal\nEmotion\nGender\nLanguage",
            "content": "98.00 96.70 100.00 99.70 96.33 97.67 97.67 97.33 99.67 99.00 99.56 100.00 99.67 99.67 99.67 99.56 99.00 63.50 88.00 93.33 62.25 95.33 99.00 82.67 90.00 99.33 65.33 92.33 10.67 18.67 - 7.67 86.67 64.67 91.67 88.67 68.67 70.33 66.33 72. 73.30 70.30 72.00 70.70 21.70 33.30 36.70 29.00 10.33 6.67 19.67 9.67 5.33 8.00 6.33 9.67 11.00 5. 7.11 7.11 13.22 3.11 5.00 10.33 7.00 19.67 8.67 6.00 6.58 8. 5.67 7.33 9.00 0.67 5.67 5.75 5.33 87.51 94.75 89. 93.00 73.42 55.00 28.92 90.08 96.33 7.00 11.00 82.58 97.00 88.22 94.00 0.00 89.33 91.67 3.33 94.00 77.67 - 96. 94.33 83.33 99.00 95.33 75.67 72.33 71.67 74.00 54.33 58.67 56.00 51.00 25.00 23.00 44.00 23.67 5.50 82.89 91. 86.56 89.67 65.00 50.00 27.50 6.67 11.67 3.78 4. 7.33 11.67 10.67 4.33 6.00 11.00 0.67 5.33 84.17 90.33 6.67 12.33 79.50 96.67 83.67 89.00 0.00 84.42 89.00 3.00 90.67 76.33 - 92.67 88.33 80.33 99.00 91.00 67.33 64.67 63.00 65. 51.33 51.00 50.67 47.00 26.00 21.67 40.33 22.00 Locality. Most methods achieve relatively higher average performance on the gender attribute, owing to the inapplicability of type 2 audio locality, which was identified in Sec. 5.1 as the most difficult to preserve, thereby inflating the overall average. For attributes other than gender, most methods perform comparably across attributes on Qwen2-Audio. In contrast, on DeSTA2.5-Audio, although performance on type 1 and type 4 is generally similar, higher audio locality is observed for language in type 2 and type 3. This indicates that unrelated auditory knowledge regarding spoken language is less susceptible to disruption when edits are applied on DeSTA2.5-Audio. Regarding text locality, most methods demonstrate comparable performance across attributes on both LALMs. Portability. Different methods result in varying portability performance across attributes on the two LALMs. consistent observation on both models is that FT (Audio) achieves higher portability scores for all atributes, especially gender, suggesting that editing the modality connector may more effectively propagate the edited knowledge to other interconnected knowledge, particularly that related to speaker gender. 20 Preprint."
        },
        {
            "title": "F DETAILED RESULTS UNDER SEQUENTIAL EDITING",
            "content": "We provide the statistics of the sequential editing results in Table 8 and 9, with the corresponding line charts shown in Figure 3a and Figure 3b for DeSTA2.5-Audio and Qwen2-Audio, respectively. Table 8: Original result of the four metrics of different editing methods on DeSTA2.5-Audio under sequential editing. For generality and audio locality, we present the averaged results. (%)"
        },
        {
            "title": "Text Locality Portability",
            "content": "FT (LLM) FT (Audio)"
        },
        {
            "title": "MEND",
            "content": "KE"
        },
        {
            "title": "UnKE",
            "content": "IE-IKE I-IKE 0 1 2 3 4 5 0 1 2 3 4 5 0 1 2 3 4 5 0 1 2 3 4 0 1 2 3 4 5 0 1 2 3 4 5 0 1 2 3 4 5 100.00 78.00 68.00 64.00 58.00 54.00 98.00 78.00 64.00 62.00 52.00 46.00 22.00 12.00 4.00 2.00 2.00 0. 62.00 36.00 12.00 6.00 26.00 8.00 92.00 52.00 38.00 22.00 26.00 22.00 32.00 26.00 30.00 32.00 32.00 26.00 60.00 56.00 56.00 56.00 62.00 62.00 49.74 48.19 46.11 45.08 41.45 45.60 53.89 49.74 41.97 41.45 39.38 40. 26.94 13.47 2.59 0.52 1.04 0.00 40.41 34.20 29.02 24.35 22.28 16.58 45.08 38.34 32.64 37.82 37.82 33.68 58.55 59.07 58.03 54.92 53.37 52.33 65.28 60.10 61.66 62.69 61.14 59.07 82.00 70.00 74.00 72.00 74.00 72. 100.00 100.00 100.00 100.00 100.00 100.00 54.00 28.00 14.00 4.00 6.00 2.00 76.00 58.00 56.00 38.00 28.00 40.00 82.00 88.00 86.00 82.00 84.00 80.00 54.00 58.00 50.00 54.00 54.00 54.00 50.00 52.00 68.00 52.00 52.00 56. 8.00 8.00 10.00 10.00 12.00 12.00 52.00 38.00 30.00 24.00 40.00 32.00 4.00 8.00 2.00 2.00 0.00 0.00 16.00 14.00 4.00 8.00 10.00 4.00 34.00 18.00 12.00 30.00 24.00 24.00 28.00 34.00 32.00 34.00 38.00 34. 56.00 52.00 54.00 48.00 46.00 54.00 99.33 75.33 68.67 57.33 53.33 48.00 88.00 62.00 46.00 48.67 38.00 36.00 18.67 9.33 4.67 2.00 2.00 0.00 64.00 36.00 14.67 16.67 16.00 8.67 89.33 34.67 26.67 20.07 23.33 10. 30.00 36.67 35.33 34.00 39.33 32.00 54.67 50.67 54.67 52.00 56.67 56.67 21 Preprint. Table 9: Original result of the four metrics of different editing methods on Qwen2-Audio under sequential editing. For generality and audio locality, we present the averaged results. (%)"
        },
        {
            "title": "Portability",
            "content": "FT (LLM) FT (Audio)"
        },
        {
            "title": "MEND",
            "content": "KE"
        },
        {
            "title": "UnKE",
            "content": "IE-IKE I-IKE 0 1 2 3 4 5 0 1 2 3 4 5 0 1 2 3 4 5 0 1 2 3 4 0 1 2 3 4 5 0 1 2 3 4 5 0 1 2 3 4 5 100.00 82.00 72.00 70.00 66.00 56.00 100.00 100.00 100.00 100.00 100.00 100.00 88.00 76.00 66.00 68.00 64.00 64. 72.00 34.00 22.00 10.00 8.00 14.00 100.00 54.00 50.00 42.00 38.00 40.00 10.00 14.00 4.00 14.00 8.00 4.00 10.00 10.00 10.00 10.00 10.00 10.00 100.00 74.67 61.33 55.33 51.33 43.33 82.67 79.33 77.33 76.00 76.00 75. 88.67 72.67 68.67 68.00 66.67 55.33 70.67 27.33 17.33 17.33 10.67 9.33 98.00 54.00 50.00 34.67 29.33 26.00 6.00 7.33 6.67 6.00 6.67 4.00 7.33 7.33 8.00 8.00 8.67 6.67 44.56 41.97 41.97 42.49 45.08 40. 74.09 72.54 71.50 70.98 70.98 66.84 64.25 68.91 65.28 64.25 58.55 57.51 41.97 29.02 22.28 18.65 13.47 13.47 47.67 37.82 38.86 40.93 29.53 33.16 77.20 72.02 55.44 42.49 27.46 20.73 76.17 77.20 75.13 75.65 75.65 77. 64.00 70.00 64.00 64.00 62.00 66.00 100.00 100.00 100.00 100.00 100.00 100.00 86.00 82.00 66.00 72.00 70.00 70.00 54.00 40.00 32.00 24.00 14.00 12.00 66.00 62.00 54.00 50.00 52.00 36.00 38.00 40.00 34.00 34.00 20.00 14. 48.00 44.00 48.00 52.00 42.00 44.00 18.00 16.00 18.00 14.00 20.00 16.00 42.00 40.00 40.00 36.00 34.00 40.00 20.00 20.00 16.00 22.00 14.00 18.00 12.00 8.00 2.00 4.00 2.00 4.00 26.00 10.00 24.00 22.00 24.00 16. 26.00 22.00 26.00 20.00 10.00 8.00 26.00 22.00 26.00 20.00 26.00 22."
        },
        {
            "title": "G CASE STUDY",
            "content": "While knowledge editing in LALMs offers way to update auditory knowledge without full retraining, the editing process is not always stable. To better understand the behaviors, we conduct case study comparing two editing scenarios: (1) single targeted edit applied to change an emotional attribute, and (2) sequential edits applied across multiple concepts. The comparison highlights both the strengths and limitations of current editing methods, emphasizing the trade-off between reliability of isolated edits and the accumulation of errors when multiple edits interact. Figure 4 shows the result of successful editing example by FT (Audio) on Qwen2-Audio, where we edit the models perception of speaker emotion from fearful to sad. After editing, we observe that the models outputs for both reliability and generality are successfully updated to sad. At the same time, both audio locality and text locality remain intact, which shows that the original knowledge of the model is preserved. For portability, we observe that after the edit, the model 22 Preprint. Figure 4: An example of successful editing result by FT (Audio) on Qwen2-Audio. can also perform reasoning, changing the prediction from preparing for high-stakes exam with anxiety to person crying after breakup. This demonstrates that the interconnected knowledge is also updated during reasoning, and the edited model can apply this knowledge in new contexts. Figure 5 shows degenerated example from sequential editing on DeSTA2.5-Audio with MEND. After applying multiple edits in row, we can see that the model collapses and produces incoherent outputs such as repeated characters and newline symbols. This illustrates how sequential edits can accumulate interference and destabilize the internal representations of the model. Unlike the single-edit scenario, where the change is targeted and localized, multiple edits interact in unpredictable ways, leading to corruption of reliability, loss of generality, and failure of both locality and portability. To be practical in real-world scenarios, however, an editing method must be capable of supporting many edits simultaneously while ensuring that unrelated edits do not interfere with one another. This failure case thus underscores key limitation of current approaches. 23 Preprint. Figure 5: An example of degenerated editing result by MEND on DeSTA2.5-Audio."
        }
    ],
    "affiliations": [
        "DouDou Capital",
        "NVIDIA",
        "National Taiwan University"
    ]
}