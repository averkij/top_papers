{
    "paper_title": "Codified Foreshadowing-Payoff Text Generation",
    "authors": [
        "Longfei Yun",
        "Kun Zhou",
        "Yupeng Hou",
        "Letian Peng",
        "Jingbo Shang"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Foreshadowing and payoff are ubiquitous narrative devices through which authors introduce commitments early in a story and resolve them through concrete, observable outcomes. However, despite advances in story generation, large language models (LLMs) frequently fail to bridge these long-range narrative dependencies, often leaving \"Chekhov's guns\" unfired even when the necessary context is present. Existing evaluations largely overlook this structural failure, focusing on surface-level coherence rather than the logical fulfillment of narrative setups. In this paper, we introduce Codified Foreshadowing-Payoff Generation (CFPG), a novel framework that reframes narrative quality through the lens of payoff realization. Recognizing that LLMs struggle to intuitively grasp the \"triggering mechanism\" of a foreshadowed event, CFPG transforms narrative continuity into a set of executable causal predicates. By mining and encoding Foreshadow-Trigger-Payoff triples from the BookSum corpus, we provide structured supervision that ensures foreshadowed commitments are not only mentioned but also temporally and logically fulfilled. Experiments demonstrate that CFPG significantly outperforms standard prompting baselines in payoff accuracy and narrative alignment. Our findings suggest that explicitly codifying narrative mechanics is essential for moving LLMs from surface-level fluency to genuine narrative competence."
        },
        {
            "title": "Start",
            "content": "Codified Foreshadowing-Payoff Text Generation Longfei Yun, Kun Zhou, Yupeng Hou, Letian Peng*, Jingbo Shang University of California, San Diego {loyun, kuzhou, yphou, lepeng, jshang}@ucsd.edu 6 2 0 2 1 1 ] . [ 1 3 3 0 7 0 . 1 0 6 2 : r a"
        },
        {
            "title": "Abstract",
            "content": "Foreshadowing and payoff are ubiquitous narrative devices through which authors introduce commitments early in story and resolve them through concrete, observable outcomes. However, despite advances in story generation, large language models (LLMs) frequently fail to bridge these long-range narrative dependencies, often leaving \"Chekhovs guns\" unfired even when the necessary context is present. Existing evaluations largely overlook this structural failure, focusing on surface-level coherence rather than the logical fulfillment of narrative setups. In this paper, we introduce Codified Foreshadowing-Payoff Generation (CFPG), novel framework that reframes narrative quality through the lens of payoff realization. Recognizing that LLMs struggle to intuitively grasp the \"triggering mechanism\" of foreshadowed event, CFPG transforms narrative continuity into set of executable causal predicates. By mining and encoding Foreshadow-Trigger-Payoff triples from the BookSum corpus, we provide structured supervision that ensures foreshadowed commitments are not only mentioned but also temporally and logically fulfilled. Experiments demonstrate that CFPG significantly outperforms standard prompting baselines in payoff accuracy and narrative alignment. Our findings suggest that explicitly codifying narrative mechanics is essential for moving LLMs from surface-level fluency to genuine narrative competence."
        },
        {
            "title": "Introduction",
            "content": "If in the first act you have hung pistol on the wall, then in the following one it should be fired. Anton Chekhov Human-authored narratives commonly rely on foreshadowing and payoff to establish coherence *Corresponding author. 1Code: https://github.com/LongfeiYun17/CFPG 1 Figure 1: Illustration of ForeshadowTriggerPayoff decomposition using narrative example from The Hound of the Baskervilles. The disappearance of the boot introduces an unresolved causal commitment, which remains dormant until triggering narrative condition activates its resolution. over the course of story (Riedl and Young, 2010; Prince, 2003). By explicitly introducing objects, intentions, or conditions early on, authors create narrative commitments that are expected to be concretely resolved later (Todorov and Weinstein, 1969). The successful realization of these commitments, rather than mere sentence-level fluency, is defining property of competent storytelling (Forster, 1956). Despite recent advances in story generation (OpenAI, 2025a; Deepmind, 2025), language models frequently fail to correctly realize such foreshadowed commitments. While generated text may remain grammatically fluent and locally coherent, models often neglect previously introduced setups, contradict established conditions, or resolve them inappropriately (See et al., 2019; Guan et al., 2021). Existing story-generation benchmarks, such as ROCStories (Mostafazadeh et al., 2016) and WritingPrompts (Fan et al., 2018), primarily emphasize short-range coherence, evaluating whether each sentence follows naturally from the preceding context (Rashkin et al., 2020). They do not explicitly test whether narrative setups are later paid Figure 2: Overview of the CFPG framework. CFPG maintains codified causal state in the form of foreshadow pool Ct, where each element is structured (F, T, ) triple. At each step t, an eligibility selection module deterministically selects subset St Ct based on codified trigger constraints, which conditions the language model to generate the next continuation y. The generated text updates both the narrative prefix Xt+1 and the foreshadow pool Ct+1 via codified state transition that resolves satisfied commitments and introduces new foreshadows. The right panel shows the simplified CFPG loop in pseudocode. off, leaving core aspect of narrative competence underexamined (Sun et al., 2021). This limitation is not addressed by prior work in open-ended story generation and creative writing systems. Many approaches focus on maintaining character consistency (Shanahan et al., 2023), dialogue coherence (Shuster et al., 2022), or stylistic alignment (Liu et al., 2023), often through persona conditioning (Peng and Shang, 2025) or memorybased mechanisms (Wang et al., 2025). Crucially, these methods primarily model persistent attributes, rather than discrete narrative events whose realization can be causally verified. While such techniques help preserve surface-level continuity, they do not provide explicit mechanisms for representing, tracking, or verifying whether foreshadowed narrative conditions are ultimately fulfilled (Yang et al., 2023). As result, models may appear coherent while silently violating narrative commitments introduced earlier in the story. To address this gap, we introduce Codified ForeshadowingPayoff Generation (CFPG), framework that reformulates narrative coherence in terms of explicit causal realization. Rather than treating continuity as an implicit property of text, CFPG represents each foreshadow as structured predicate specifying foreshadow, trigger, and an expected payoff. This representation allows narrative generation and evaluation to be grounded in whether each introduced commitment is realized, postponed, or violated as the story unfolds. By encoding foreshadowpayoff relations as executable symbolic rules, CFPG enables precise, temporally grounded detection of payoff realization that cannot be reliably achieved through prompting or context augmentation techniques alone. Moreover, this formulation provides controllable interface for evaluating language models on narrative competence defined by commitment fulfillment, rather than surface-level fluency. Using the BOOKSUM corpus of long-form literary summaries, we automatically mine structured interpretable foreshadowpayoff pairs. This dataset supports systematic analysis of narrative commitment realization across diverse genres and story structures. Our contributions are threefold: 1. We introduce novel framework that explicitly models narrative coherence through foreshadowpayoff realization, formalizing storytelling competence as commitment fulfillment. 2. We construct large-scale dataset derived from BOOKSUM, automatically extracting explicit foreshadowpayoff pairs from longform narratives aligned with their narrative positions. 3. Through controlled experiments, we demonstrate that codifying narrative commitments as ForeshadowTriggerPayoff predicates substantially improves payoff realization accuracy and narrative alignment compared to standard prompting baselines."
        },
        {
            "title": "2 Related Works",
            "content": "2.1 Creative Writing Prior work on story writing with language models has primarily focused on generating fluent and 2 coherent narratives, often emphasizing local plausibility and stylistic consistency (Mostafazadeh et al., 2016; Fan et al., 2018). As result, models can produce locally coherent text while neglecting or contradicting previously introduced narrative elements. To improve global coherence, several approaches incorporate planning mechanisms, guiding generation with high-level plot structures or event sequences (Riedl and Young, 2010; Yang et al., 2023; Wang et al., 2025). While these methods improve structural consistency, they do not explicitly represent narrative commitments: foreshadowed events remain implicit, and payoff realization is neither enforced nor causally verifiable. More recent LLMbased systems employ multi-agent collaboration or feedback-based refinement to steer story development (Patel et al., 2024; Bae and Kim, 2024; Venkatraman et al., 2025). These frameworks enhance creativity and engagement, but narrative progression is still governed by qualitative feedback rather than explicit causal conditions linking setups to outcomes. In contrast, our work directly targets this gap by codifying foreshadowpayoff relations as explicit, executable narrative constraints, enabling grounded detection and controlled realization of long-range narrative commitments. 2.2 Controllable Text Generation Controllable Text Generation (CTG) studies how to steer language models to satisfy explicit control conditions during generation. Early work formalized control through conditional language models and control codes, enabling generation conditioned on topics, styles, or domains (Hu et al., 2017). Subsequent approaches extended this idea via disentangled latent variables (Keskar et al., 2019), decoding-time guidance (Dathathri et al., 2019), and classifieror energy-based interventions (Holtzman et al., 2018; Krause et al., 2021). Recent works unify these methods under content-level and attribute-level control, covering dimensions such as sentiment, topic, style, and safety (Zhou et al., 2023; Lambert et al., 2024; Liang et al., 2024; Yun et al.). Despite their success, most CTG formulations treat control signals as static, non-eventive attributes applied uniformly across generation. They regulate how text is expressed, but do not model when narrative commitments introduced earlier should be resolved. As result, models may satisfy all declared control conditions while still violating long-range narrative logic."
        },
        {
            "title": "3 Method",
            "content": "3.1 Structured Representation of Narrative Commitments To transform intuitive narrative structures into machine-actionable logic, we formalize the relationship between foreshadowing and its resolution as an explicit system of causal commitments. We argue that complete narrative dependency consists not just of an initial setup and final resolution, but of specific logical \"gate\" that governs its timing. Consequently, we propose representing each commitment as structured ForeshadowTrigger Payoff (FTP) triple: Foreshadow (F ): The initial setup or narrative anomaly that establishes \"causal debt,\" implying that future explanation or resolution is required. Trigger (T ): The specific narrative condition or prerequisite event that must occur for the latent foreshadow to become actionable. Payoff (P ): The concluding event that logically fulfills and resolves the commitment introduced by and activated by . This FTP decomposition is essential for modeling temporal appropriateness. In The Hound of the Baskervilles ( Figure 1), the missing boot (F ) creates suspense, but remains dormant until the revelation of Stapletons tracking method (T ). Only then is the explanation of the boots purpose (P ) narratively justified. By explicitly modeling the Trigger, our framework distinguishes between premature payoff (spoiling suspense) and missing payoff (logical inconsistency). 3.2 Codified Foreshadow-Payoff Generation The CFPG framework externalizes narrative causality from the models implicit attention weights into codified finite-state abstraction. This allows for symbolic tracking of narrative debts with level of precision that pure prompting cannot achieve. 3.2.1 Causal State and Foreshadow Pool Throughout the generation process, CFPG maintains dynamic Foreshadow Pool = {(Fi, Ti, Pi)}n i=1. This pool serves as global state representing all unfulfilled narrative commitments. Unlike standard autoregressive models that compress history into hidden vector, CFPG represents the narrative state as an explicit set of resolvable predicates. 3.2.2 The Codified Loop: Select, Generate, and Update As illustrated in Figure 2, CFPG operates through an iterative SelectGenerateUpdate cycle: Eligibility Selection via Codification At each step t, for each pending commitment in the foreshadow pool Ct, CFPG invokes the codify function. This function acts as logical gate that evaluates the current narrative context Xt against the trigger . Only those foreshadows whose triggers are satisfied are promoted to the active subset St = {f Ct codify(Xt, ) = True}. Guided Continuation Given the eligible subset St, the language model is tasked with generating the next scene y. Unlike vanilla generation, CFPG injects the corresponding payoffs associated with St into the models context as explicit narrative requirements. The generation is thus formulated as: pθ (y Xt, St) This ensures that the transition to the next scene is not merely probabilistic continuation, but guided fulfillment of active narrative debts under explicit narrative constraints. State Transition and Grounding Following generation, CFPG performs state update. verification module identifies which commitments from St were successfully realized in the text and removes them from Ct. Simultaneously, it extracts new narrative setups introduced in and encodes them as new FTP triples for Ct+1. This ensures the causal state representation remains temporally grounded as the story unfolds."
        },
        {
            "title": "4 Dataset",
            "content": "Extracting foreshadow-payoff dependencies from full-length novels at scale presents significant challenge due to narrative noise and extreme context length. We address this by leveraging BOOKSUM (Kryscinski et al., 2021), corpus of humanwritten, hierarchical abstractive summaries that distill long-range plot points into discourse-salient events. We construct sentence-level foreshadow payoff dataset through three-stage pipeline designed to identify, verify, and filter long-range narrative dependencies with minimal thematic noise. Given narrative text segmented into sentences = (s1, . . . , sT ), our objective is to recover pairs (stf , stp) such that stf introduces concrete narrative condition and stp later resolves it through an explicit event, decision, or revelation. Stage 1: Sentence-Level Candidate Identification We use GPT-4.1 (OpenAI, 2025b) to scan abstractive summaries and extract candidate foreshadowpayoff pairs. Each candidate is required to be anchored to two specific sentences, yielding provisional indices (tf , tp). This stage prioritizes recall and admits weak or noisy candidates. Stage 2: Payoff Alignment Verification To eliminate thematic echoes and non-causal associations, we apply symbolic verification gate to filter these candidates. verifier model assesses whether the narrative context window centered at the proposed resolution point stp constitutes genuine causal or narrative resolution of the setup context window centered at stf , rejecting pairs whose linkage is metaphorical, anticipatory, or unsupported by observable textual evidence. Stage 3: Rubric-Based Filtering Pairs that pass Stage 2 are further subjected to rubric-based filtering stage designed to enforce strict foreshadow validity. Two independent verifier models evaluate each candidate pair along four dimensions: (i) Setup Validity: the setup introduces concrete narrative element (e.g., object, action, rule, or decision) that is not fully explained or resolved at the time of its introduction; (ii) Payoff Validity: the payoff provides new narrative information that fulfills, resolves, or retroactively reinterprets the setup, rather than restating or trivially extending it; (iii) Temporal Separation: the setup and payoff occur in distinct sentences and are separated by non-trivial narrative interval, excluding immediate or locally resolved causeeffect relations; (iv) Foreshadow Justification: the setup can be reasonably interpreted as deliberate narrative foreshadow only in hindsight, after observing the payoff, and would otherwise remain narratively under-specified. foreshadowpayoff pair is retained in the final dataset only if both verifier models accept the pair on all four criteria. Resulting Dataset The resulting dataset comprises sentence-anchored foreshadowpayoff instances extracted from complete narrative sum4 Story Context (Excerpt) In The Hound of the Baskervilles, Sir Henry Baskerville arrives in London to inherit the estate. Shortly afterward, boot mysteriously disappears from his hotel room, followed by second. No explanation is provided at this stage, leaving the incident as strange, unsettling anomaly in the narrative flow. Foreshadow Metadata Status: Unresolved Type: Physical Object Gap: Long-range delay Logic: Non-trivial link Extracted ForeshadowTriggerPayoff Triple Foreshadow (F): Trigger (T): Payoff (P): One of Sir Henrys boots goes missing without any immediate explanation or functional cause. The investigation reveals the existence of the hound and Stapletons need for scent-based tracking tool. It is revealed that the boot was stolen to train the hound specifically to hunt Sir Henry by his scent. Figure 3: representative F-T-P triple extracted from the BOOKSUM corpus. We anchor each element to specific narrative segments, ensuring that the latent causal link is verifiable and the payoff is temporally justified. maries. For each foreshadow, we provide: (i) the full summary text, (ii) sentence-level index tf marking the introduction of the foreshadowed setup, (iii) sentence-level index tp identifying its verified payoff resolution, (iv) concise naturallanguage description of the foreshadowpayoff relation, and (v) categorical foreshadow type (e.g., object, event, rule). Figure 3 illustrates an example extracted using our pipeline. Additional statistics are provided in Appendix A."
        },
        {
            "title": "5 Experiments",
            "content": "We ask the following research questions to evaluate the effects of CFPG on narrative reasoning: 1. Payoff Activation under Oracle Timing: Can CFPG reliably activate and realize eligible foreshadows when the payoff timing is externally specified? (5.1) 2. Grounded Payoff Decision: Does CFPG enable grounded detection and realization of foreshadowed payoffs under incrementally revealed narrative context? (5.2) 3. Error Attribution: Where do grounded payoff decisions fail, and how does CFPG change the underlying error patterns during narrative progression? (5.3) 5.1 Conditional Payoff Activation in Truncated Scenes We first investigate fundamental question: Can CFPG reliably activate and realize eligible foreshadows when the payoff timing is externally specified? Standard prompting often produces locally fluent continuations that nevertheless fail to \"settle\" existing narrative debts. To diagnose this, we construct controlled experiment by truncating narratives immediately before known payoff point (a.k.a. Oracle Timing), isolating the models ability to transition from setup to resolution. Table 1: Performance comparison on the BookSum dataset under oracle timing. CFPG consistently outperforms prompt-based baselines across multiple model architectures, achieving near-perfect payoff activation and significantly improved narrative alignment. Base Model Method Should-Payoff Rate Avg. Score GPT-4.1-mini Claude-Haiku-4.5 Qwen2.5-3B Qwen2.5-7B Qwen2.5-14B Llama-3.1-8B Prompt CFPG Prompt CFPG Prompt CFPG Prompt CFPG Prompt CFPG Prompt CFPG 1.000 0.965 0.998 1.000 1. 1.000 0.569 0.911 0.657 0.940 0.481 0.781 0.517 0.797 0.583 0.898 0.530 0.802 Note: All models refer to their respective Instruct variants. Behavioral Alignment via Narrative Entailment We evaluate model continuations using threeclass narrative entailment scheme (Peng and Shang, 2024). Given truncated scene, generated sentence ˆy, and the gold payoff y, an LLM judge determines if ˆy entails (1.0), is neutral to (0.5), or contradicts (0.0) the intended narrative trajectory. As shown in Table 1, standard prompting frequently results in payoff-agnostic continuationsplausible sentences that ignore the pending foreshadow. In contrast, CFPG achieves nearceiling Should-Payoff Rates (> 0.96) and significantly higher alignment scores. By utilizing the Trigger state, CFPG guides the model to actively resolve previously introduced narrative debts, ensuring the continuation is not just fluent but logically grounded in the specific foreshadowing setup. Mechanistic Evidence: Causal Saliency Tracking To understand why CFPG outperforms standard prompting, we analyze the models internal attention patterns. We compare the attention weights assigned to the Setup tokens during the generation 5 Table 2: Main Results: Grounded Payoff Tracking. We evaluate the models ability to identify payoffs under incremental context. CFPG (Ours) consistently achieves superior performance across all backbones. Model Method Det. (%) Early Late Error Fidelity Activation Localization (Online Sensing) Generation GPT-4.1-mini Foreshadow-Aware Prompt Context Refresh (Sim.) CFPG (Ours) 58.0 48.6 69.8 +11.8 Qwen2.5 3B-Inst. Qwen2.5 7B-Inst. Llama3.1 8B-Inst. Foreshadow-Aware Prompt Context Refresh (Sim.) CFPG (Ours) Foreshadow-Aware Prompt Context Refresh (Sim.) CFPG (Ours) Foreshadow-Aware Prompt Context Refresh (Sim.) CFPG (Ours) 4.5 6.7 10.8 +6.3 15.6 19.6 19.6 +4.0 25.8 22.1 27.3 +1.5 235 306 166 - 601 587 550 -51 522 493 490 -32 451 482 439 -12 17 7 11 -6 0 0 5 +5 2 7 5 + 5 3 8 +3 8.85 13.05 5.76 -3.09 34.67 32.81 31.63 -3.04 27.00 23.79 26.23 -0.77 20.04 23.72 19.91 -0.13 0.453 0.382 0.647 +0. 0.022 0.039 0.080 +0.058 0.114 0.177 0.184 +0.070 0.182 0.160 0.226 +0.044 Note: Definitions of all metrics are provided in Appendix B. Colored values denote the difference between CFPG and FAP. of the payoff sentence. Figure 4: Visualization of attention patterns during payoff generation. The heatmaps (left and center) compare the attention weights allocated to foreshadowing setup tokens under the vanilla prompting baseline and CFPG. The line plot (right) quantifies the resulting Causal Saliency Gain relative to the baseline, showing that CFPG consistently maintains significantly higher mean attention to the setup region throughout the generation process. Each row corresponds to different narrative instance. As visualized in Figure 4, we observe stark shift in causal saliency. In the baseline (Left), attention to the distant setup is diffuse and sparse. This suggests that the model may fail to recognize the necessity of foreshadow resolution, instead prioritizing local context to maintain surface-level fluency at the expense of long-range narrative consistency. Conversely, CFPG (Right) exhibits dense \"attention spike\" focused precisely on the foreshadowing anchors. The mean attention weight to the setup region exhibits pronounced surge under CFPG compared to the baseline, confirming that our statecoded guidance effectively \"re-ground\" the model in the narratives past. Combined, these results demonstrate that explicitly separating payoff eligibility from surface generation enables reliable, model-agnostic control over foreshadow resolution, ensuring that narrative \"hooks\" are not merely introduced, but meaningfully settled. 5.2 Grounded Payoff Tracking: From Sensing to Action We evaluate whether CFPG facilitates grounded identification of narrative resolution by framing payoff tracking as an online sensing problem. This setup simulates an incremental reading process where the model is exposed to the narrative sentence-by-sentence. At each step, the model must perform \"causal sensing\", deciding whether the observed prefix has satisfied the trigger conditions for payoff without accessing future tokens. To provide granular diagnosis of model behavior, we analyze three complementary dimensions of causal awareness: 1. Activation Timing: The ability to suppress \"causal hallucinations\" (premature triggers) until the causal conditions are satisfied; 2. Localization Accuracy: The precision in pinning down the specific narrative anchor where resolution occurs; 3. Generative Fidelity: The capacity to translate successful detection into trajectoryconsistent continuation. Baselines We compare CFPG against two prompting-based baselines that vary in how narrative foreshadow information is incorporated. (1) 6 Foreshadow-Aware Prompting (FAP) augments standard prompting by explicitly providing unresolved narrative commitments in natural language, but without enforcing executable trigger conditions (2) or maintaining persistent narrative state. Foreshadow-Similarity Context Refresh (FSCR) dynamically augments sliding context window with earlier sentences selected by lexical similarity to the foreshadow, without explicit state tracking or trigger enforcement. Timing Precision and Causal Gating First, we examine whether the model can distinguish between genuine resolution and mere semantic proximity. As summarized in Table 2, prompting methods frequently suffer from causal hallucinations, triggering premature payoffs (235 early triggers in GPT-4.1-mini) when relevant character or keyword is mentioned without satisfying the logical prerequisites. CFPG acts as logical gatekeeper, reducing these early triggers by 29.3%. This suppression of false positives provides evidence that CFPG enables the model to \"wait\" for the causal trigger rather than impulsively guessing based on local heuristics. The Sensing-Acting Gap Next, we probe the fidelity of the resolution once payoff is detected. critical finding is the persistent gap in baseline models between detection and realization: even when the baseline correctly identifies the payoff window, its Continuation Score remains disproportionately low (0.453 for GPT-4.1-mini). This decoupling suggests that passive recognition does not imply active commitment. In contrast, CFPG bridges this gap by anchoring the generation in codified FT state, yielding 43% improvement in trajectory alignment. This confirms that the explicit state serves as functional bridge, translating successful \"sensing\" event into \"consistent action.\" Decision Dynamics of Grounded Payoff Detection To further understand why CFPG improves grounded payoff tracking beyond static accuracy gains, we analyze the decision dynamics of payoff detection under strictly incremental observation settings. Instead of treating detection as point estimate, we probe how the models internal confidence evolves as narrative context approaches the gold payoff location. Concretely, we measure the models activation confidence for the binary decision Should the payoff occur now? at successive sentence preFigure 5: Temporal decision dynamics of payoff detection for Qwen-2.5-7B-Instruct. CFPG shows reduced premature activation, sharp decision transition at the gold payoff, and sustained post-resolution confidence compared to the baseline. fixes centered around the gold payoff index. This yields temporal confidence trajectory that reveals whether payoff recognition emerges as discrete causal decision or as gradual semantic drift. Figure 5 reveals clear contrast in how FAP prompting and CFPG distribute payoff activation over narrative time. Prior to the true payoff point, the baseline exhibits consistently higher activation probability. This elevated pre-payoff confidence reflects premature causal commitment rather than grounded recognition. Around the gold payoff boundary, both methods show sharp increase in activation; however, CFPG displays markedly steeper and more localized decision jump (+0.22), suggesting discrete transition from unresolved to resolved once the causal prerequisites are met. Crucially, after the payoff point, CFPG sustains high activation level over an extended window, whereas the baseline rapidly decays. This post-payoff persistence indicates that CFPG maintains stable causal state once resolution is achieved. Together, these dynamics suggest that CFPG does not merely improve payoff timing accuracy, but fundamentally reshapes the decision process: suppressing premature activation before the payoff, enforcing sharp causal switch at resolution, and preserving commitment afterward. 5.3 Error Attribution in Grounded Payoff Tracking To better understand the failure modes of grounded payoff tracking, we conduct systematic error attribution analysis over all incorrect end-to-end predictions. This analysis focuses on decision-level behavior exhibited by the model during incremental narrative processingspecifically, how payoffrelated decisions are made, delayed, or prematurely triggered given partial context. Table 3: Taxonomy of Grounded Payoff Tracking Failures Category Decision Models Internal Logic Ground Truth (Narrative State) Active Errors (False Triggers) Premature Confusion Early Spurious The intent or preparation is already enough. Only setup/intent; causal chain not closed. This event looks like the payoff point. Thematic similarity; not the target event. Passive Errors (State Breakdowns) Deferred NYO State Failure Random Disorder guess its resolved during this gap. lost track of the initial status change. Causal state must remain Pending. Environment/character state has shifted. Reactive Errors (Detection Gaps) Conservative Indirect Failure Missed Late Too subtle; Ill wait for explicit confirmation. There is no link between these two events. Payoff occurred; model threshold too high. Indirect or retrospective causal linking. Analysis Procedure. For each failure case, we first elicit concise, text-grounded rationale explaining why the model made an incorrect payoff decision under the available context. Explanations are generated using constrained prompt that enforces explicit reference to observable narrative evidence and disallows speculation about future events or alternative plot developments. This ensures that all rationales reflect concrete grounding failures rather than post-hoc interpretation. We then induce an error taxonomy in datadriven manner by clustering these rationales into small set of recurring failure patterns. This process yields compact taxonomy that captures distinct mechanisms of grounded payoff failure without relying on predefined labels. Finally, each failure instance is assigned to its dominant category, enabling aggregate analysis of error distributions across methods. Error Taxonomy. The induced taxonomy consists of six recurring error types  (Table 3)  : (1) Premature Payoff Triggering, where payoff decisions are made based on surface cues or anticipatory signals before concrete realization; (2) Deferred Payoff Not Yet Observable, where the model loses track of the pending status and prematurely assumes resolution during long narrative gap; (3) Thematic or Event-Level Confusion, where related but distinct events or motifs are mistaken for the true payoff; (4) Narrative State Tracking Failure, where causal commitments are not consistently maintained or updated over time; (5) Overly Conservative Triggering, where payoff detection is delayed due to excessive evidentiary requirements; and (6) Indirect or Retrospective Payoff Linking Failure, where payoffs are expressed implicitly or retrospectively but fail to be linked back to the original setup. Findings. As shown in Figure 6, the distribution of failure cases highlights that Premature triggering Figure 6: Distribution of grounded payoff tracking errors for prompt-based and CFPG-based methods. CFPG notably attenuates premature payoff triggering compared to baseline prompting. is the dominant error mode for both models. CFPG significantly outperforms the Prompt baseline by reducing Premature cases by 31%, demonstrating its robust ability to filter semantic hallucinations. Regarding high-level narrative reasoning, CFPG exhibits marked improvement in maintaining logical consistency and capturing complex associations. Specifically, the reduction in Thematic Confusion (from 34 to 26 cases) and the near-halving of Indirect Failure cases (7 to 3) suggest that the model successfully links implicit or non-linear payoffs to their original setups even across intricate narrative structures."
        },
        {
            "title": "6 Conclusion",
            "content": "We presented Codified ForeshadowingPayoff Generation (CFPG), framework that models narrative coherence as the explicit realization of causal commitments. By representing foreshadows as executable ForeshadowTriggerPayoff predicates and tracking them as codified state, CFPG enables grounded payoff detection and controlled realization under incremental context. Experiments show consistent improvements over prompt-based baselines in payoff timing, localization, and narrative alignment."
        },
        {
            "title": "Limitations",
            "content": "CFPG primarily targets explicit and textually grounded foreshadow payoff relations and does not aim to model highly abstract or purely symbolic narrative devices. Our experiments are conducted on summary level narratives, which provide controlled setting for evaluating long range causal dependencies but may not capture all stylistic or discourse level phenomena in full length texts. In addition, CFPG relies on automatically extracted Foreshadow Trigger Payoff structures, and extraction errors or omissions may limit coverage in some cases."
        },
        {
            "title": "References",
            "content": "Minwook Bae and Hyounghun Kim. 2024. Collective critics for creative story generation. arXiv preprint arXiv:2410.02428. Sumanth Dathathri, Andrea Madotto, Janice Lan, Jane Hung, Eric Frank, Piero Molino, Jason Yosinski, and Rosanne Liu. 2019. Plug and play language models: simple approach to controlled text generation. arXiv preprint arXiv:1912.02164. Google Deepmind. 2025. https://blog.google/products/gemini/gemini-3/. https://deepmind.google/models/gemini/. Accessed: 2026-01-03. Angela Fan, Mike Lewis, and Yann Dauphin. 2018. Hierarchical neural story generation. In Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 889898, Melbourne, Australia. Association for Computational Linguistics. Edward Morgan Forster. 1956. Aspects of the Novel. Mariner Books, New York. Reprint of the 1927 edition; often cited as Harvest Book edition. Jian Guan, Xiaoxi Mao, Changjie Fan, Zitao Liu, Wenbiao Ding, and Minlie Huang. 2021. Long text generation by modeling sentence-level and discourse-level coherence. arXiv preprint arXiv:2105.08963. Ari Holtzman, Jan Buys, Maxwell Forbes, Antoine Bosselut, David Golub, and Yejin Choi. 2018. Learning to write with cooperative discriminators. arXiv preprint arXiv:1805.06087. Zhiting Hu, Zichao Yang, Xiaodan Liang, Ruslan Salakhutdinov, and Eric Xing. 2017. Toward controlled generation of text. In International conference on machine learning, pages 15871596. PMLR. Nitish Shirish Keskar, Bryan McCann, Lav Varshney, Caiming Xiong, and Richard Socher. 2019. Ctrl: conditional transformer language model for controllable generation. arXiv preprint arXiv:1909.05858. Ben Krause, Akhilesh Deepak Gotmare, Bryan McCann, Nitish Shirish Keskar, Shafiq Joty, Richard Socher, and Nazneen Fatema Rajani. 2021. Gedi: Generative discriminator guided sequence generation. In Findings of the Association for Computational Linguistics: EMNLP 2021, pages 49294952. Wojciech Kryscinski, Nazneen Rajani, Divyansh Agarwal, Caiming Xiong, and Dragomir Radev. 2021. Booksum: collection of datasets for longarXiv preprint form narrative summarization. arXiv:2105.08209. Xun Liang, Hanyu Wang, Yezhaohui Wang, Shichao Song, Jiawei Yang, Simin Niu, Jie Hu, Dan Liu, Shunyu Yao, Feiyu Xiong, et al. 2024. Controllable text generation for large language models: survey. arXiv preprint arXiv:2408.12599. Sheng Liu, Haotian Ye, Lei Xing, and James Zou. 2023. In-context vectors: Making in context learning more effective and controllable through latent space steering. arXiv preprint arXiv:2311.06668. Nasrin Mostafazadeh, Nathanael Chambers, Xiaodong He, Devi Parikh, Dhruv Batra, Lucy Vanderwende, Pushmeet Kohli, and James Allen. 2016. corpus and evaluation framework for deeper understanding of commonsense stories. arXiv preprint arXiv:1604.01696. OpenAI. 2025a. Gpt-5 system card. https://openai.com/index/gpt-5-system-card/. OpenAI. 2025b. Introducing gpt-4.1 in the api. https: //openai.com/index/gpt-4-1/. Accessed: 202509-14. Zeeshan Patel, Karim El-Refai, Jonathan Pei, and Tianle Li. 2024. Swag: Storytelling with action guidance. arXiv preprint arXiv:2402.03483. Letian Peng and Jingbo Shang. 2024. Quantifying and optimizing global faithfulness in persona-driven roleplaying. Advances in Neural Information Processing Systems, 37:2755627583. Letian Peng and Jingbo Shang. 2025. Codifying arXiv preprint character logic in role-playing. arXiv:2505.07705. Gerald Prince. 2003. dictionary of narratology. of Nebraska P. Hannah Rashkin, Asli Celikyilmaz, Yejin Choi, and Plotmachines: OutlineJianfeng Gao. 2020. conditioned generation with dynamic plot state tracking. arXiv preprint arXiv:2004.14967. Mark Riedl and Robert Michael Young. 2010. Narrative planning: Balancing plot and character. Journal of Artificial Intelligence Research, 39:217268. Abigail See, Aneesh Pappu, Rohun Saxena, Akhila Yerukola, and Christopher Manning. 2019. Do massively pretrained language models make better storytellers? arXiv preprint arXiv:1909.10705. Murray Shanahan, Kyle McDonell, and Laria Reynolds. 2023. Role play with large language models. Nature, 623(7987):493498. Nathan Lambert, Jacob Morrison, Valentina Pyatkin, Shengyi Huang, Hamish Ivison, Faeze Brahman, Lester James Miranda, Alisa Liu, Nouha Dziri, Shane Lyu, et al. 2024. Tulu 3: Pushing frontiers in open language model post-training. arXiv preprint arXiv:2411.15124. Kurt Shuster, Jing Xu, Mojtaba Komeili, Da Ju, Eric Michael Smith, Stephen Roller, Megan Ung, Moya Chen, Kushal Arora, Joshua Lane, et al. 2022. Blenderbot 3: deployed conversational agent that arXiv continually learns to responsibly engage. preprint arXiv:2208.03188. Simeng Sun, Kalpesh Krishna, Andrew MattarellaMicke, and Mohit Iyyer. 2021. Do long-range language models actually use long-range context? arXiv preprint arXiv:2109.09115. Tzvetan Todorov and Arnold A. Weinstein. 1969. Structural analysis of narrative. Novel: Forum on Fiction, 3:70. Saranya Venkatraman, Nafis Irtiza Tripto, and Dongwon Lee. 2025. Collabstory: Multi-llm collaborative story generation and authorship analysis. In Findings of the Association for Computational Linguistics: NAACL 2025, pages 36653679. Qianyue Wang, Jinwu Hu, Zhengping Li, Yufeng Wang, Daiyuan Li, Yu Hu, and Mingkui Tan. 2025. Generating long-form story using dynamic hierarchical outlining with memory-enhancement. In Proceedings of the 2025 Conference of the Nations of the Americas Chapter of the Association for Computational Linguistics: Human Language Technologies (Volume 1: Long Papers), pages 13521391. Kevin Yang, Dan Klein, Nanyun Peng, and Yuandong Tian. 2023. Doc: Improving long story coherence with detailed outline control. In Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 33783465. Longfei Yun, Letian Peng, and Jingbo Shang. Ultrabench: Benchmarking llms under extreme finegrained text generation. Jeffrey Zhou, Tianjian Lu, Swaroop Mishra, Siddhartha Brahma, Sujoy Basu, Yi Luan, Denny Zhou, and Le Hou. 2023. Instruction-following evaluation for large language models. arXiv preprint arXiv:2311.07911."
        },
        {
            "title": "A Dataset",
            "content": "A.1 Statistics The resulting dataset consists of 629 validated foreshadow-payoff pairs extracted from 148 books  (Table 4)  . Table 4: Dataset statistics for the extracted foreshadow payoff pairs. Statistic # Books # Foreshadows Avg. Payoff Distance (sentences) Median Payoff Distance 75th Percentile Distance 90th Percentile Distance Max Payoff Distance Object Foreshadows Event Foreshadows Speech-act Foreshadows Rule Foreshadows Symbol Foreshadows Avg. Extraction Confidence Value 148 629 20.9 13.0 29.0 45.0 230 48.2% 35.3% 9.7% 5.1% 1.7% 0. Figure 7: Dataset statistics of the extracted foreshadow payoff corpus. Figure 7 (Left) illustrates the probability density of payoff distance measured in sentences between the setup and payoff. The distribution exhibits pronounced heavy tail, with median distance of 13 sentences and mean of 20.9 sentences. Notably, 25% of payoffs occur at distances greater than 29 sentences, and 10% exceed 45 sentences, with the longest dependencies spanning over 200 sentences. This long-range structure indicates that substantial portion of the dataset requires maintaining unresolved causal commitments across extended narrative context rather than relying on local causeeffect relations. Figure 7 (Right) shows the distribution of foreshadow types. Object-based (48.2%) and eventbased (35.3%) foreshadows together account for more than 80% of the dataset, reflecting the predominance of concrete narrative elements that par12 Table 5: Quality check results on random sample of 100 foreshadowpayoff pairs. Two annotators independently evaluate overall validity and component-level correctness. Agreement denotes the proportion of samples receiving identical judgments. Aspect Pair Validity Setup Accuracy Payoff Accuracy Connection Validity 0.89 0.97 1.00 0.90 Agree. 0.96 0.98 1.00 0.96 0.87 0.95 1.00 0.88 ticipate in delayed causal structure. Speech-actbased and rule-based foreshadows form smaller but non-trivial fraction, while symbol-based foreshadows are rare (1.7%), consistent with our conservative filtering of purely thematic or interpretive cues. Overall, these statistics confirm that the dataset emphasizes explicit, long-range causal dependencies grounded in observable narrative events, providing challenging and well-controlled testbed for evaluating narrative generation under delayed causal constraints. A.2 Quality Check and Reliability Analysis To assess annotation reliability, we conduct quality check on random sample of 100 extracted foreshadowpayoff pairs. Two annotators independently evaluate each sample from different perspectives, assessing both overall validity and component-level correctness. As shown in Table 5, the annotators agree on 88% of the samples at the pair level. Componentwise agreement is higher, with 95% agreement on setup accuracy and perfect agreement on payoff accuracy. Judgments on connection validity exhibit slightly lower agreement (88%), reflecting the greater subjectivity involved in assessing longrange narrative reinterpretation compared to identifying explicit textual evidence. Overall, these results indicate that the dataset achieves high precision in identifying concrete setups and payoffs, with remaining disagreements primarily arising from borderline cases of causal relevance rather than factual inconsistencies."
        },
        {
            "title": "Tracking",
            "content": "We evaluate grounded payoff tracking under an incremental narrative setting, where the model processes the story sentence by sentence and is not allowed to revise past decisions. Metric Summary. Together, these metrics evaluate grounded payoff tracking along three complementary dimensions: (i) decision timing (Correct Detection, Early/Late Triggers), (ii) temporal localization precision (Localization Error), and (iii) generation fidelity after detection (Continuation Score), providing comprehensive assessment of both detection and realization under incremental narrative context."
        },
        {
            "title": "C LLM Usage Statement",
            "content": "Large language models (LLMs) were employed exclusively to polish wording and improve grammatical correctness for better readability. They were not used to generate, alter, or influence the papers technical content, conceptual contributions, methods, or experimental findings. Responsibility for the final manuscript and its accuracy remains entirely with the authors. Correct Detection Rate (Correct Det. %). Correct Detection Rate measures the proportion of narratives in which the model correctly identifies the occurrence of payoff during incremental processing. prediction is considered correct if the model triggers payoff decision at sentence index that lies within fixed tolerance window (3 sentences) around the annotated ground-truth payoff location. This metric reflects end-to-end payoff detection accuracy under partial context. Early Triggers. Early Triggers counts the number of cases in which the model predicts payoff before any ground-truth payoff becomes observable in the narrative. Such errors correspond to premature payoff triggering, typically caused by surface-level cues or anticipatory signals that precede actual realization. Lower values indicate better resistance to premature causal inference. Late Triggers. Late Triggers counts the number of cases in which the model triggers payoff after the ground-truth payoff point has already occurred. These errors reflect delayed recognition or overly conservative decision-making, where excessive evidence is required before committing to payoff decision. Lower values indicate better temporal alignment with the narrative payoff. Localization Error (Loc. Error). Localization Error measures the absolute distance, in number of sentences, between the models predicted payoff trigger point and the annotated ground-truth payoff location, averaged over all triggered cases. This metric evaluates temporal precision in payoff localization during online narrative sensing. Lower values indicate more accurate alignment with the true payoff position. Continuation Score (Cont. Score). Continuation Score evaluates the quality of payoff realization in generation once payoff has been detected. For cases where the model triggers payoff within the tolerance window, we prompt the model to generate one-sentence continuation conditioned on the available context. The generated continuation is then compared against the ground-truth continuation using trajectory-based evaluator that assesses narrative consistency, causal progression, and outcome alignment. The score reflects the fraction of continuations judged to follow the same narrative trajectory as the ground truth."
        }
    ],
    "affiliations": [
        "University of California, San Diego"
    ]
}