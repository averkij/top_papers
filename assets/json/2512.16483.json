{
    "paper_title": "StageVAR: Stage-Aware Acceleration for Visual Autoregressive Models",
    "authors": [
        "Senmao Li",
        "Kai Wang",
        "Salman Khan",
        "Fahad Shahbaz Khan",
        "Jian Yang",
        "Yaxing Wang"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Visual Autoregressive (VAR) modeling departs from the next-token prediction paradigm of traditional Autoregressive (AR) models through next-scale prediction, enabling high-quality image generation. However, the VAR paradigm suffers from sharply increased computational complexity and running time at large-scale steps. Although existing acceleration methods reduce runtime for large-scale steps, but rely on manual step selection and overlook the varying importance of different stages in the generation process. To address this challenge, we present StageVAR, a systematic study and stage-aware acceleration framework for VAR models. Our analysis shows that early steps are critical for preserving semantic and structural consistency and should remain intact, while later steps mainly refine details and can be pruned or approximated for acceleration. Building on these insights, StageVAR introduces a plug-and-play acceleration strategy that exploits semantic irrelevance and low-rank properties in late-stage computations, without requiring additional training. Our proposed StageVAR achieves up to 3.4x speedup with only a 0.01 drop on GenEval and a 0.26 decrease on DPG, consistently outperforming existing acceleration baselines. These results highlight stage-aware design as a powerful principle for efficient visual autoregressive image generation."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 8 1 ] . [ 1 3 8 4 6 1 . 2 1 5 2 : r Preprint. STAGEVAR: STAGE-AWARE ACCELERATION FOR VISUAL AUTOREGRESSIVE MODELS Senmao Li1,3 Kai Wang2 Salman Khan3 Fahad Shahbaz Khan3,4 Jian Yang1 Yaxing Wang1 1VCIP, CS, Nankai University 2City University of Hong Kong (Dongguan), China 4Linkoping University 3Mohamed bin Zayed University of Artificial Intelligence"
        },
        {
            "title": "ABSTRACT",
            "content": "Visual Autoregressive (VAR) modeling departs from the next-token prediction paradigm of traditional Autoregressive (AR) models through next-scale prediction, enabling high-quality image generation. However, the VAR paradigm suffers from sharply increased computational complexity and running time at largescale steps. Although existing acceleration methods reduce runtime for large-scale steps, but rely on manual step selection and overlook the varying importance of different stages in the generation process. To address this challenge, we present StageVAR, systematic study and stage-aware acceleration framework for VAR models. Our analysis shows that early steps are critical for preserving semantic and structural consistency and should remain intact, while later steps mainly refine details and can be pruned or approximated for acceleration. Building on these insights, StageVAR introduces plug-and-play acceleration strategy that exploits semantic irrelevance and low-rank properties in late-stage computations, without requiring additional training. Our proposed StageVAR achieves up to 3.4 speedup with only 0.01 drop on GenEval and 0.26 decrease on DPG, consistently outperforming existing acceleration baselines. These results highlight stage-aware design as powerful principle for efficient visual autoregressive image generation. https://github.com/sen-mao/StageVAR"
        },
        {
            "title": "INTRODUCTION",
            "content": "Recent developments in autoregressive (AR) models (Lee et al., 2022; Razavi et al., 2019; Sun et al., 2024; Yu et al., 2022) have yielded remarkable advances in image generation (Sun et al., 2024; Wang et al., 2025) and have been extended to serve as unified modeling framework for both vision understanding and generation (Qu et al., 2025; Shi et al., 2024; Wu et al., 2025; Team, 2024). However, the inherent sequential nature of AR models leads to numerous decoding steps, making the inference time-consuming and costly. Departing from the sequential next-token prediction of traditional AR models, visual autoregressive (VAR) (Tian et al., 2024; Han et al., 2025) adopts next-scale prediction paradigm, enabling more efficient for high-quality image generation. Despite their strong generative performance, VAR models could still suffer from heavy computation and long runtime at large-scale steps. Existing methods (Guo et al., 2025; Li et al., 2025a; Chen et al., 2025b; Li et al., 2025b) accelerate VAR generation process through token reduction at large steps, but they heavily rely on manual heuristics, resulting in suboptimal acceleration. To address this challenge, we investigate how semantics and structures emerge during VAR inference to guide acceleration automatically. Our study reveals that early-scale steps in VAR inference are crucial for establishing semantics and structures, with semantics converging earlier than structures (See Fig. 1). At specific large-scale step, the establishment of semantics and structures converges, and the remaining steps primarily perform fidelity refinement (See Fig. 1). Based on these observations, we divide the inference process into three stages: the semantic establishment stage, the structure establishment stage, and the fidelity refinement stage (See Fig. 1-Bottom). Motivated by the above observations, we introduce StageVAR, plug-and-play approach that accelerates next-scale prediction VAR models without requiring additional training. We reveal that maintaining both the semantic establishment and structure establishment stages are crucial for maintaining perceptual quality, whereas the fidelity refinement stage can be leveraged to develop more Work done during research intern at MBZUAI. 1 Preprint. efficient acceleration strategies. Within the fidelity refinement stage, we identify two key properties towards further acceleration: semantic irrelevance (See Fig. 2) and low-rank feature structure (See Tab. 1). Semantic irrelevance allows us to bypass text conditioning entirely by using only null prompt, eliminating redundant prompt computations. Meanwhile, the low-rank property enables the VAR forward pass to operate in reduced feature space, substantially lowering inference cost. Together, these insights form the basis of StageVAR efficient acceleration strategy. We show that the proposed StageVAR can significantly accelerate VAR image generation, achieving 3.4 speedup with negligible performance drop. To summarize, our main contributions are: Systematic analysis of VAR inference: We systematically study how VAR establishes semantics and structures across scales. We show that early steps ensure semantic and structural consistency (semantic and structure establishment stages), while the later steps mainly refine details (fidelity refinement stage). Stage-aware acceleration: We discover two key properties, which are semantic irrelevance and low-rank structure, in the fidelity refinement stage. Leveraging these two properties, we propose the StageVAR as plug-and-play acceleration method without any additional model retraining. Extensive validation: Experiments on the GenEval and DPG benchmarks confirm that StageVAR accelerates high-quality image generation while preserving output fidelity and outperforming existing acceleration baselines. Concretely, StageVAR achieves speedup of up to 3.4 compared to baseline methods, with only minimal drops in performance metrics: 0.01 reduction on GenEval and 0.26 decrease on DPG."
        },
        {
            "title": "2 RELATED WORK",
            "content": "2.1 AUTOREGRESSIVE VISUAL GENERATION Recently, Visual Autoregressive (VAR) modeling (Tian et al., 2024; Han et al., 2025) has adopted progressive-growing generation schemeanalogous to the mechanism first introduced in GANs (Karras et al., 2018)to enable gradual scaling of visual generation across different resolutions. In contrast to traditional autoregressive (AR) methods (Lee et al., 2022; Sun et al., 2024; Yu et al., 2022), which adhere to next-token prediction paradigm and thus require numerous iterative steps to produce high-quality images, VAR introduces next-scale prediction paradigm. This shift in design allows for far more efficient synthesis of high-resolution visual content. Notably, VAR also aligns with the coarse-to-fine generation process prevalent in diffusion models (Rombach et al., 2022): it adopts this same coarse-to-fine paradigm, choice that has contributed to its promising generative capabilities (Tian et al., 2024; Han et al., 2025; Tang et al., 2025). Despite the promising performance of VAR methods, critical gap remains: the process by which these models establish and refine image content during inference has not yet been systematically analyzed or studied. 2.2 EFFICIENT VISUAL GENERATION Diffusion models acceleration techniques have been extensively studied, including training-free (Ma et al., 2024b; Tian et al., 2025; Du et al., 2025) and training-based methods (Luo et al., 2024; Xu et al., 2024; Dao et al., 2024). The exploration of efficient generation has recently drawn increasing attention in the context of autoregressive models, encompassing the traditional AR models and VAR models. For instance, SimpleAR (Wang et al., 2025) introduces simple AR framework that achieves high-fidelity image synthesis through optimized training and inference. SJD (Teng et al., 2025) introduces training-free probabilistic parallel decoding algorithm, enabling faster AR generation while maintaining sampling-based diversity. LANTERN (Jang et al., 2025) uses speculative decoding with trainable drafter model to mitigate token selection ambiguity and substantially accelerate generation. Despite the acceleration, the inherent sequential nature of AR models remains bottleneck, with high-quality image generation still taking over ten seconds. Distinct from the traditional AR models, VAR predicts the next scale to facilitate efficient generation. However, methods for accelerating diffusion and AR models are not directly applicable to VAR, and research on VAR acceleration remains in its infancy. FastVAR (Guo et al., 2025), SparseVAR (Chen et al., 2025b) and SkipVAR (Li et al., 2025a) apply token reduction or step skipping in the manually determined large-scale steps. CoDe (Chen et al., 2025c) speeds up inference and Preprint. optimizes memory usage, but it relies on collaboration between two VAR models of different scales across the inference steps. LiteVAR (Xie et al., 2024) and ScaleKV (Li et al., 2025b) improve memory efficiency by pruning attention-related tokens while with suboptimal acceleration. Recent research investigates style transfer (Park et al., 2025) and performance boosting (Chen et al., 2025a) instead of acceleration of VAR models. In this work, we conduct systematic analysis of how VAR models establish semantic content and structural details across diverse scale steps, by which we explicitly reveal the distinction of VAR generation stages. Building on the observation, we further characterize unique, stage-dependent properties inherent in the VAR models. Finally, leveraging these properties, we propose an acceleration technique for VAR inference without compromising the quality of semantics or structures."
        },
        {
            "title": "3 METHOD",
            "content": "We first briefly revisit VAR (3.1), and then conduct comprehensive analysis of the next-scale prediction process in text-to-image generation (Sec. 3.2). Our analysis reveals the semantic irrelevance and low-rank properties of the VAR model. Based on these insights, we propose novel method to accelerate VAR sampling while largely preserving generation quality and fidelity (Sec. 3.3). 3.1 PRELIMINARY Visual Autoregressive modeling (VAR) (Tian et al., 2024) redefines autoregressive modeling (AR) (Lee et al., 2022; Razavi et al., 2019; Sun et al., 2024; Yu et al., 2022) for images by shifting from next-token prediction to next-scale prediction. In this framework, each autoregressive operation generates token map at specific resolution scale rather than predicting individual tokens step by step. Given an continuous image feature map Rhwd, VAR first quantizes it into multi-scale token maps = (R1, R2, . . . , RK) with increasingly larger predefined scale (hk, wk) for = 1, . . . , K. This sequence of residuals allows us to reconstruct the continuous feature as: Fk = (cid:88) i= Up(Ri, (h, w)), (1) where Up() represents the upsampling operation. The multi-scale token maps allow the decomposition of the joint probability distribution in an autoregressive manner: p(R1, R2, . . . , RK) = (cid:89) k= p(Rk R1, R2, . . . , Rk1), (2) where the initial token map R1 is derived from the text embeddings, while each subsequent Rk is generated from (cid:101)Fk1, obtained via: (cid:101)Fk1 = Down(Fk1, (hk, wk)), (3) where Down() represents the downsampling operation. Rk consists of hk wk discrete tokens selected from vocabulary of size at the scale step k. The VAR paradigm generates images in coarse-to-fine manner with scale-up steps. 3.2 OBSERVATIONS Here, we conduct an in-depth study of the next-scale prediction process, further exploring the semantic irrelevance and low-rank properties. Three-Stage Observation of Text-to-Image VAR. As visualized in Fig. 1(Bottom), our analysis reveals three distinct generation stages of the text-to-image VAR models (Han et al., 2025). Specifically, given pretrained VAR model, the autoregressive modeling with next-scale prediction follows the autoregressive likelihood in Eq. (2). Intuitively, at increasingly larger prediction scales, the image semantics and structures tend to be well defined. To verify and investigate this property, we use CLIP (Hessel et al., 2021) and DINO (Oquab et al., 2023) to evaluate global and local semantics, respectively. Also, we employ LPIPS (Zhang et al., 3 Preprint. Figure 1: (a) Visualization of semantic evolution across all scale steps (i.e., CLIP and DINO). (b) Visualization of structure evolution on all scale steps (i.e., LPIPS and DISTS). (c) Variations of the next scale step in the frequency domain. (Bottom) Visualization of samples across all scale steps. 2018) and DISTS (Ding et al., 2020) to evaluate structural consistency. The statistical results are shown in Fig. 1a. For both CLIP and DINO, the curves exhibit similar trends, with an initial increase followed by stabilization towards the end. For example, at the initial scales, the value of CLIP rises quickly from around 0.13 to above 0.30, and the value of DINO increases from about 0.60 to above 0.98. This sharp improvement implies that both global and local semantics are progressively established during the early scale steps. In contrast, starting from the specific scale (i.e., 16), both the CLIP and DINO values achieve the peak plateau, indicating that the semantics have been established at these scales. Fig. 1b shows the structural evolution across different scales. Specifically, both the values of LPIPS and DISTS rapidly drop below 0.05 (at scale 32) and then level off. This indicates that the structures are progressively established during the earlyand middle-scale steps. And they become well established at the large-scale steps. Moreover, we provide the frequency analysis (Fig. 1c). Both the low-frequency and high-frequency components exhibit noticeable variations during the early-scale steps, while the model almost converges in the larger scale steps. To summarize, the early-scale steps are responsible for establishing semantics and structures, and semantics converge earlier than structures. We term these two processes as the semantic establishment stage and the structure establishment stage. In contrast, the large-scale steps mainly perform fidelity refinement, as illustrated in Fig. 1(Bottom) and we term it as the fidelity refinement stage. The above analyses indicate that perceptual quality is progressively established during the semantic establishment and structure establishment stages, and thus should be preserved in the original generative process. In contrast, the fidelity refinement stage can be exploited and operated for accelerations. Semantic irrelevance at large-scale steps. Based on the aforementioned analysis, the semantic establishment stage has already completed the formation of image semantics. Intuitively, the largescale steps after the semantic establishment stage should be unrelated to semantic generation. To verify the intuition, we study the effect of text prompts on the semantics of the later generation stages. Recalling that VAR adopts classifier-free guidance (CFG) (Ho & Salimans, 2022) to combine text prompts with null text prompts, we set the CFG to 0 beginning at scale step k. Under this setup, the text prompt is omitted for steps through K, and the impact is evaluated using CLIP and GenEval scores. As illustrated in Fig. 2 (Left), when we set the CFG to 0 of the scale after the semantic establishment stage (i.e., {20, . . . , 64}), the CLIP curve stabilizes above 0.3 and exhibits negligible changes beyond this scale step. In contrast, setting the CFG to 0 on earlier scales (i.e., < 20) leads to sharp decline in the CLIP score. We further use the GenEval score to assess how well the generated images accurately reflect the intended text prompts. Both the GenEval score and the perceptual quality of image details remain close to their maximum levels (Fig. 2 (Left) and Fig. 2(Right, 2nd row)) when is configured as during the fidelity refinement stage (i.e., {40, 48, 64}). In contrast, when scale is set below 40, the curve declines (Fig. 2 (Left)), accompanied by degradation in perceptual detail quality (e.g., the earring in Fig. 2Right (1st row)). 4 Preprint. Figure 2: (Left) Evaluation of semantic and perceptual quality when the starting scale steps of CFG is set to 0. (Right) Sample visualizations obtained by setting CFG to 0 at large-scale steps. In conclusion, the fidelity refinement stage is semantics independent. Based on this observation, we are able to omit the text prompt and use only the null text prompt during the fidelity refinement stage to achieve the acceleration goal. Input Feature exhibits low-rank property. At the intermediate k-th scale step of VAR, the input feature (cid:101)Fk1 is obtained by interpolation, where the obtained token map of the previous scale is first upsampled (Eq. (1)) and downsampled (Eq. (3)) to match the size of the next larger scale and then fed into the model as input. This raises an important question: Does the intermediate input feature inherently exhibit low-rank property? This motivates us to use of the low-rank feature to evaluate whether semantic and perceptual quality can be maintained during image generation. Specifically, for the feature at an intermediate scale step (cid:101)Fk1 RM (i.e., = hk wk, = 2048 in the Infinity model (Han et al., 2025)), we perform Singular Value Decomposition (SVD) on (cid:101)Fk1 = (cid:101)U (cid:101)Σ (cid:101)VT , where (cid:101)Σ = diag(σ1, , σn), the singular values σ1 σn, = min(M, d). The cumulative energy (Jolliffe, 2011; Chong & Qu, 2025) of the top-r singular values is defined as Er = (cid:80)r , and the corresponding energy En = (cid:80)r ratio is given by ηr = Er . straightforward approach to constructing i=1 σ2 the low-rank feature (cid:98)Fk1 (also shown in Fig. 3 2 ) is to select the smallest such that i=1 σ i=1 σ2 (cid:46)(cid:80)n (cid:46) = min{r ηr α}, where α (0, 1) is threshold, as follows (cid:101)Fk1 (cid:98)Fk1 = (cid:34) (cid:35) σiuivT = u1, , ur (cid:88) i=1 diag{σ1, , σr} (cid:123)(cid:122) (cid:125) (cid:124) (cid:101)Σr (rr) vT 1 ... vT (cid:123)(cid:122) (rd) (4) (5) (cid:123)(cid:122) (cid:101)Ur (M r) where ui and vi are the singular vectors in (cid:101)U and (cid:101)V corresponding to the i-th largest singular value σi. Based on the EckartYoungMirsky Theorem (Eckart & Young, 1936), The low-rank feature (cid:98)Fk1 is the most closely rank-r approximation of (cid:101)Fk1. (cid:101)VT (cid:125) (cid:124) (cid:125) (cid:124) Table 1: Performance with the low-rank feature by varying α. We explore the impact of (cid:98)Fk1 during the fidelity refinement stage on the generated image. For example, as shown in Tab. 1, we evaluate the quality of modified images against the vanilla outputs under different settings of α {0.999, 0.99, 0.98, 0.97, 0.96, 0.95}. Setting α = {0.999, 0.99}, the generated image preserves semantic and perceptual quality as the vanilla. As α decreases to 0.96, GenEval drops by <0.01. Achieving slight decrease in metric values to below 0.720 with α = 0.95 and 14.9% rank. The results indicate that the intermediate feature exhibit low-rank property in the fidelity refinement stage, and this observation motivates the exploration to use the low-rank feature in this stage. 0.731 α = 0.999 (59.5% rank) 0.729 α = 0.99 (34.4% rank) 0.730 α = 0.98 (26.1% rank) 0.722 α = 0.97 (21.1% rank) 0.725 α = 0.96 (17.6% rank) 0.726 α = 0.95 (14.9% rank) 0. 83.12 83.14 83.01 82.81 82.89 82.86 82.72 GenEv. DPG Methods Vanilla 3.3 STAGE-AWARE ACCELERATION FOR VAR Based on all the above observations, we propose stage-aware acceleration method for VAR (Fig. 4 and Algorithm 1). As we state above, the VAR sampling process is conceptualized as three stages: 5 Preprint. Table 2: The performance in (cid:101)Fk-1, (cid:98)Fk-1, (cid:101)Fr, and (cid:98)Fr with α=0.99 and 34.4% rank. Mod. indicates the latency for the model, and Add. indicates the additional latency. 5/ 6 is shown in Fig. 4. Inputs Shape Latency Mod. Add. GenEv. DPG 1 (cid:101)Fk-1 (Vanilla) (M, d) 2.2s 0s 0.731 83.12 (M, d) 2.2s 17.3s 0.730 83.01 2 (cid:98)Fk-1 (r, d) 1.2s 17.3s 0.702 81.97 3 (cid:101)Fr w/ Eq. (4) 4 (cid:101)Fr w/o Eq. (4) (r, d) 1.2s 8.7s 0.700 81.73 5 (cid:98)Fr w/ Eq. (6) (r, d) 1.2s 0.6s 0.690 81.71 6 (cid:98)Fr w/o Eq. (6) (r, d) 1.2s 0s 0.720 82.46 Figure 3: Visualization of VAR inference across 1 vanilla, 2 the low-rank feature, and 3/ 4 the r-dimensional feature. the semantics establishment stage, the structure establishment stage, and the fidelity refinement stage. The semantics establishment and the structure establishment stages partially overlap in the early-scale steps, where they jointly contribute to the perceptual quality of the generated image. Therefore, we preserve the original inference process for these two stages. For the fidelity refinement stage, we reveal semantic irrelevance and low-rank properties, and we leverage these properties to accelerate VAR inference. The vanilla VAR inference is shown in Fig. 3 1 , the intermediate feature (cid:101)Fk1 passes through the VAR block to produce the output feature , which is then quantized into Rk (omitted in Fig. 3 and Fig. 4 for brevity). Our acceleration is applied at the block level (e.g., 8 blocks in the Infinity backbone). For the low-rank property of the feature in the fidelity refinement stage, we can approximate the original feature of size (M, d) using r-dimensional feature of size (r, d), where . Naively, as shown in Fig. 3 3 , for the intermediate feature (cid:101)Fk1 in the scale step, we first perform SVD, then determine according to Eq. (4), and construct the r-dimensional feature (cid:101)Fr = (cid:101)Σr (cid:101)VT based on Eq. (5). The r-dimensional feature (cid:101)Fr then passes through the VAR block to generate the output (See Fig. 3 3 ). As shown in Tab. 2 3 , this achieves the speedup 1.8 for the VAR inference, but incurs substantial additional latency (i.e., 17.3s). The reason is that determining r, constructing (cid:101)Fr, and constructing Uo for reconstructing the -dimensional feature all require SVD decomposition as prerequisite, which is time-consuming. See Sec. A.2 for Uo details. To address this issue, three aspects require considerations: (1) adopting an off-the-shelf value of for given α, (2) devising efficient approximation methods for (cid:101)Fr that alleviate the time-consuming computations (e.g., SVD), and (3) enabling the restoration of the -dimensional feature . To alleviate these issues, we propose three corresponding strategies to counter as explained below. Predetermination Strategy. To obtain an off-the-shelf value of given α, we adopt predetermined strategy based on statistical results. We find that the standard deviation is an order of magnitude smaller than the mean. That indicates for given α, the feature consistently exhibit similar low-rank characteristics across diverse text prompts. Specifically, given an off-the-shelf r, we only need to perform the r-rank decomposition instead of full-rank one, reducing the decomposition time from 17.3s to 8.7s, as shown in Tab. 2 3 4 and Fig. 3 3 / 4 . See Sec. A.1 for details. Random Projection (RP) for Low-Rank Feature. When applying the predetermined strategy, the rank of the feature is determined directly, without the requirement for either full-rank or r-rank decomposition. However, the r-dimensional feature (cid:101)Fr needs construction. To achieve so without extra computations, we use random projection (Papadimitriou et al., 1998; Kaski, 1998; Achlioptas, 2001; Bingham & Mannila, 2001) to obtain the r-dimensional representation (cid:98)Fr as an approximation of (cid:101)Fr. Specifically, given the intermediate feature (cid:101)Fk1 RM d, we construct r-dimensional (cid:98)Fr = QT (cid:101)Fk1 Rrd, where RM and Qi,j (0, 1 ) (Johnson et al., 1984). Then, to recover the intermediate feature (cid:101)Fk1, we solve the linear least-squares (LLS) problem (cid:98)F (cid:101)F k1F , (cid:99)WT (6) min (cid:99)Wr to obtain the transformation matrix (cid:99)Wr RM r, where -F denotes the Frobenius norm. As shown in Tab. 2 5 , using (cid:98)Fr for the VAR inference achieves the speedup 1.8 with less additional latency (i.e., 0.6s). 6 Preprint. as the representative feature of r enables restoration of the -dimensional feature Figure 4: Overview of the proposed StageVAR framework. We retain the original VAR inference process for the semantic and structure establishment stages, while exploiting semantic irrelevance and low-rank properties in the fidelity refinement stage to accelerate inference. Representative Token Restoration (RTR). After the model forward, (cid:98)Fr produces only (See Fig. 4). While constructed Wo rF = Wo , it introduces extra latency from solving the LLS problem (i.e., Eq. (6)). To avoid solving the LLS r, we regarded problem to obtain Wo according to the indices I, while the remaining tokens are filled with the cached k1, as in (Guo et al., 2025). Here, since is unavailable in advance, we instead sample rows from the corresponding input feature (cid:101)Fk1 and denote their indices as I, following (Frieze et al., 2004). See Sec. A.3 for Wo Specifically, as shown in Fig. 4, given the cached counterpart feature first upsample it to match the dimension of F o,cache k1 (7) Then, based on the assumption in (Frieze et al., 2004), we sample the most important rows from (cid:101)Fk1, with each row chosen with probability Pi (cid:101)F (i) , and record their indices of the chosen rows as I. Thus, we can redefine details. k1 in the (k-1)-th scale, we k1). k12(cid:46) = Up(F (cid:101)Fk12 as := {F k1 (i) denotes that, for I, the i-th row of (i) I} {F o,cache Formally, r , whereas o,cache k1 token of o,cache nearly negligible additional latency (i.e., 0s). is taken from the corresponding token of is taken from the corresponding . As shown in Tab. 2 6 , this achieves 1.8 speedup for VAR inference with denotes that, for / I, the i-th row of k1 (i) (i) / I} (8) For semantic irrelevance, we adopt the CFG during the fidelity refinement stage by setting it to 0, which is equivalent to conditioning only on the null text prompt. Combining RP with RTR then enables stage-aware acceleration in VAR. Our full algorithm is presented in Algorithm 1 (Sec. B)."
        },
        {
            "title": "4 EXPERIMENTS",
            "content": "4.1 EXPERIMENTAL SETUP We build our method StageVAR on the VAR-based text-to-image model Infinity-2B, Infinity8B (Han et al., 2025), and STAR-1.7B (Ma et al., 2024a), with images generated at resolution of 1024 1024. We then evaluate our method on the GenEval (Ghosh et al., 2023) and DPG (Hu et al., 2024) benchmarks, which are two widely adopted benchmarks for assessing semantic alignment and perceptual quality of generated images (Han et al., 2025; Guo et al., 2025; Li et al., 2025a).Additionally, we use Frechet Inception Distance (FID) (Heusel et al., 2017), Kernel Inception Distance (KID) (Binkowski et al., 2018), and Inception Score (IS) (Barratt & Sharma, 2018) metrics on the widely used COCO 2014 and COCO 2017 benchmarks Lin et al. (2014) to further evaluate perceptual quality. To evaluate the efficiency of the proposed method, we report the latency and the corresponding speedup ratio. Based on our analysis, we preserve the original inference process in Infinity for the semantic establishment stage and structure establishment stage (i.e., scales {1, 2, 4, 6, 8, 12, 16, 20, 24, 32}), while 7 Preprint. Table 3: Quantitative comparisons of perceptual quality on the GenEval and DPG Benchmarks. Methods #Speed #Latency #Param GenEval DPG Two Obj. Position Color Attri. Overall Global Relation Overall SDXL (Podell et al., 2024) LlamaGen (Sun et al., 2024) Show-o (Xie et al., 2025) PixArt-Sigma (Chen et al., 2024) HART (Tang et al., 2025) DALL-E 3 (Betker et al., 2023) Emu3 (Wang et al., 2024) - - - - - - - Infinity-2B (Han et al., 2025) FastVAR (Guo et al., 2025) SkipVAR (Li et al., 2025a) Ours Infinity-8B (Han et al., 2025) Ours STAR (Ma et al., 2024a) Ours 1.0 2.75 2.62 3.4 1.0 2.7 1.0 1.74 4.3s 37.7s 50.3s 2.7s 0.95s - - 2.2s 0.80s - 0.64s 4.80s 1.77s 2.0s 1.15s 2.6B 0.8B 1.3B 0.6B 0.7B - 8.5B 2.0B 2.0B 2.0B 2.0B 8.0B 8.0B 1.7B 1.7B 0.74 0.34 0.80 0.62 0.62 - 0.81 0.85 0.81 0.84 0.84 0.90 0. 0.54 0.54 0.15 0.07 0.31 0.14 0.13 - 0.49 0.45 0.45 0.39 0.43 0.62 0.60 0.09 0.08 0.23 0.04 0.50 0.27 0.18 - 0. 0.54 0.52 0.60 0.56 0.67 0.66 0.08 0.09 0.55 0.32 0.68 0.55 0.51 0.67 0.66 0.73 0.72 0.72 0.72 0.79 0. 0.51 0.51 83.27 86.76 - - - - 86.89 86.59 - - 90.97 90.58 - - 85.10 92.37 85.41 92.76 84.19 93.15 82.67 93.50 85.10 94.50 85.71 94. - - - - 74.65 65.16 67.48 80.54 80.89 83.50 81.60 83.12 82.86 83.16 82.86 86.60 86.05 - - Figure 5: Qualitative comparison with the vanilla Infinity-2B, Infinity-8B, and STAR models (1st, 3rd, and 5th rows). Our StageVAR (2nd, 4th, and 6th rows) achieves 3.4, 2.7, and 1.74 speedup while maintaining performance. applying acceleration strategies in the fidelity refinement stage (i.e., scales {40, 48, 64}). For the threshold α, we set it to {0.96, 0, 0} in the fidelity refinement stage. Setting α = 0 indicates that the corresponding scale step is skipped, and the intermediate result is interpolated to the target resolution as the final output. We set α to 0.96 in the scale {64} in STAR (Ma et al., 2024a). We apply our acceleration operation at the block level (e.g., 8 blocks in the Infinity backbone, 30 blocks in the STAR backbone). We use one RTX 3090 GPU (24GB VRAM) to conduct all our experiments, except for Infinity-8B, which is run on an A100 GPU (80 GB VRAM) 8 Preprint. Table 4: Quantitative comparison of FID, KID, and IS on COCO2014 and COCO2017. Methods #Speed COCO2014-30K COCO2017-5K FID KID102 IS FID KID102 IS Infinity Ours 1.0 26.64 3.4 26.91 1.26 1. 42.61 35.82 42.18 37.13 1.31 1.42 37.21 37.70 Figure 6: User study. Figure 7: Visualization of the quantitative and qualitative results for different ranks. 4.2 MAIN RESULTS Comparison with Baselines. Tab. 3 presents results on GenEval (Ghosh et al., 2023) and DPG (Hu et al., 2024) benchmarks. As shown in Tab. 3, Infinity (Han et al., 2025) achieves superior performance on both benchmarks within just 13 steps, except for DALL-E 3 on the DPG benchmark, demonstrating advantages over both multi-step diffusion models (Betker et al., 2023; Podell et al., 2024; Chen et al., 2024) and AR models (Xie et al., 2025; Wang et al., 2024; Tang et al., 2025; Sun et al., 2024). When combined with FastVAR (Guo et al., 2025) and SkipVAR (Li et al., 2025a), the methods achieve 2.75 and 2.62 speedups, respectively, with negligible performance degradation. In comparison, integrating StageVAR with the Infinity-2B and -8B models yields up to 3.4 and 2.7 speedups, respectively, while incurring only negligible performance degradation, demonstrating superior acceleration over existing methods. In addition, STAR (Ma et al., 2024a) with StageVAR achieves 1.74 speedup while maintaining performance. As shown in the qualitative evaluation in Fig. 5, StageVAR preserves high visual quality, confirming that the results remain consistent with the vanilla models. Tab. 4 presents results on the COCO2014 and COCO2017 benchmarks Lin et al. (2014) to further validate perceptual quality. The results in Tab. 4 demonstrate that StageVAR maintains high speedup ratio while maintaining competitive performance. For instance, StageVAR achieves 3.4 acceleration with only minor degradations of 1.3 in FID and around 0.5 in IS compared with the vanilla model. User Study. We conducted user study, as shown in Fig. 6, and asked subjects to select results. We apply pairwise comparisons (forced choice) with 69 users (42 pairs of images). The results demonstrate that our method performs equally well as the vanilla models in terms of human preference. 4.3 ADDITIONAL ANALYSIS Different Ranks. Rank is critical for balancing the efficiency and performance of our method. We study the effect of varying rank percentages through ablation, with both qualitative and quantitative results shown in Fig. 7. Existing token-reduction text-to-image generation methods achieve faster forward computation by using fewer tokens, but at the cost of degraded performance (Bolya & Hoffman, 2023; Guo et al., 2025; Chen et al., 2025b). Interestingly, unlike these approaches, we find that reducing the rank initially improves performance, reaches peak, and then degrades as the rank continues to decrease (Fig. 7 (Left)). qualitative comparison also reveals similar trend in finer details, such as the illustration of the mouth in Fig. 7 (Right). This pattern has also been reported in (Durrant & Kaban, 2013), where performance under random projection peaks at an intermediate projection dimension. Moreover, as the rank decreases, the speedup ratio shows stable increase (Fig. 7 (the green curve)). Therefore, based on both quantitative and qualitative results, we select 17.6% rank (i.e., α=0.96) for our acceleration method StageVAR. Preprint. Table 6: Ablation study of incorporating 1 CFG=0 and 2 low-rank strategy (RP+RTR). 3 FastVAR. Methods #Speed#LatencyGenEv. DPG 1.0 Infinity 1.5 + 1 + 1 2 (StageVAR) 3.4 2.2s 1.45s 0.64s 0.731 83.12 0.724 82.78 0.726 82.86 + 1 3 3.14 0.70s 0.711 82.72 Figure 8: Comparison of quality (FID) and inference speed between FastVAR and Ours. Figure 9: Qualitative results of StageVAR with diverse aspect ratios. Quality-Latency Comparison. To further evaluate the generation quality of StageVAR, we conduct experiments to investigate the relationship between FID and inference speed on the COCO 2017 dataset. As shown in Fig. 8, the performance of StageVAR initially improves with increasing speed, reaches its optimal point at speedup of 3.4, and then degrades as the speed continues to increase, following trend similar to the GenEval metrics observed in Fig. 7 (Left). In contrast, FastVAR exhibits consistent degradation in FID as the inference speed increasesalthough slight improvement is observed at 3.06its FID remains above 9.7 across all settings. These results indicate that our method achieves better balance between quality and speed compared to FastVAR. Impact of Semantic Irrelevance. We conduct an ablation study of the proposed approach StageVAR, with quantitative results reported in Tab. 6. Applying the semantic irrelevance strategy (CFG=0) in the fidelity refinement stage yields 1.5 speedup without degrading the quality of the generated images (Tab. 6 (2nd row)). Furthermore, when combined with random projection (RP) for low-rank feature and representative token restoration (RTR), our proposed method StageVAR achieves superior speedup of 3.4 (Tab. 6 3rd row). The discovered semantic-irrelevance mechanism can also be integrated with FastVAR (Tab. 6 4th row). When integrated, it achieves further 3.14 acceleration while maintaining performance, though the acceleration remains suboptimal compared with StageVAR. Additional Results. The Infinity (Han et al., 2025) model originally supports image generation with varying aspect ratios and our method StageVAR supports this property. As shown in Fig. 9, when combined with StageVAR, it can still facilitate efficient image generation, indicating that our proposed StageVAR can be easily extended to generate images with diverse aspect ratios."
        },
        {
            "title": "5 CONCLUSION\nIn this work, we address the computational inefficiency of visual autoregressive (VAR) models\nby first conducting a systematic analysis of their inference process. We identify three distinct\nstages—semantic establishment, structure establishment, and fidelity refinement—showing that\nearly steps secure core content while later steps only refine details. Leveraging this insight, we\npropose StageVAR, a plug-and-play, training-free acceleration method that exploits semantic irrele-\nvance (bypassing text conditioning) and low-rank features (reducing feature space) in the fidelity re-\nfinement stage. Experiments on GenEval and DPG benchmarks validate StageVAR ’s effectiveness:\nit achieves a 3.4× speedup over baselines with negligible performance drops. This work advances\nefficient VAR inference, offering a practical solution to balance speed and quality.",
            "content": "10 Preprint."
        },
        {
            "title": "REFERENCES",
            "content": "Dimitris Achlioptas. Database-friendly random projections. In Proceedings of the twentieth ACM SIGMOD-SIGACT-SIGART symposium on Principles of database systems, pp. 274281, 2001. Shane Barratt and Rishi Sharma. note on the inception score. arXiv preprint arXiv:1801.01973, 2018. James Betker, Gabriel Goh, Li Jing, Tim Brooks, Jianfeng Wang, Linjie Li, Long Ouyang, Juntang Zhuang, Joyce Lee, Yufei Guo, et al. Improving image generation with better captions. Computer Science. https://cdn. openai. com/papers/dall-e-3. pdf, 2(3):8, 2023. Ella Bingham and Heikki Mannila. Random projection in dimensionality reduction: applications to image and text data. In Proceedings of the seventh ACM SIGKDD international conference on Knowledge discovery and data mining, pp. 245250, 2001. Mikołaj Binkowski, Danica Sutherland, Michael Arbel, and Arthur Gretton. Demystifying mmd gans. arXiv preprint arXiv:1801.01401, 2018. Daniel Bolya and Judy Hoffman. Token merging for fast stable diffusion. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pp. 45994603, 2023. Junsong Chen, Chongjian Ge, Enze Xie, Yue Wu, Lewei Yao, Xiaozhe Ren, Zhongdao Wang, Ping Luo, Huchuan Lu, and Zhenguo Li. PixArt-Sigma: Weak-to-strong training of diffusion transformer for 4k text-to-image generation. In European Conference on Computer Vision, pp. 7491. Springer, 2024. Zhekai Chen, Ruihang Chu, Yukang Chen, Shiwei Zhang, Yujie Wei, Yingya Zhang, and Xihui Liu. Tts-var: test-time scaling framework for visual auto-regressive generation. arXiv preprint arXiv:2507.18537, 2025a. Zhuokun Chen, Jugang Fan, Zhuowei Yu, Bohan Zhuang, and Mingkui Tan. Frequencyaware autoregressive modeling for efficient high-resolution image synthesis. arXiv preprint arXiv:2507.20454, 2025b. Zigeng Chen, Xinyin Ma, Gongfan Fang, and Xinchao Wang. Collaborative decoding makes visual auto-regressive modeling efficient. Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, 2025c. Yee Hin Chong and Peng Qu. Singular value decomposition on kronecker adaptation for large language model. arXiv preprint arXiv:2506.15251, 2025. Trung Dao, Thuan Hoang Nguyen, Thanh Le, Duc Vu, Khoi Nguyen, Cuong Pham, and Anh Tran. Swiftbrush v2: Make your one-step diffusion model better than its teacher. European Conference on Computer Vision, 2024. Keyan Ding, Kede Ma, Shiqi Wang, and Eero P. Simoncelli. Image quality assessment: Unifying structure and texture similarity. IEEE Transactions on Pattern Analysis and Machine Intelligence, abs/2004.07728, 2020. URL https://arxiv.org/abs/2004.07728. Zhenbang Du, Yonggan Fu, Lifu Wang, Jiayi Qian, Xiao Luo, and Yingyan (Celine) Lin. Early-bird diffusion: Investigating and leveraging timestep-aware early-bird tickets in diffusion models for efficient training. In International Conference on Computer Vision, 2025. Robert Durrant and Ata Kaban. Random projections as regularizers: Learning linear discriminant ensemble from fewer observations than dimensions. In Asian conference on machine learning, pp. 1732. PMLR, 2013. Carl Eckart and Gale Young. The approximation of one matrix by another of lower rank. Psychometrika, 1(3):211218, 1936. Alan Frieze, Ravi Kannan, and Santosh Vempala. Fast monte-carlo algorithms for finding low-rank approximations. Journal of the ACM (JACM), 51(6):10251041, 2004. 11 Preprint. Dhruba Ghosh, Hannaneh Hajishirzi, and Ludwig Schmidt. Geneval: An object-focused framework for evaluating text-to-image alignment. In A. Oh, T. Naumann, A. Globerson, K. Saenko, M. Hardt, and S. Levine (eds.), Advances in Neural Information Processing Systems, volume 36, pp. 5213252152. Curran Associates, URL Inc., 2023. https://proceedings.neurips.cc/paper_files/paper/2023/file/ a3bf71c7c63f0c3bcb7ff67c67b1e7b1-Paper-Datasets_and_Benchmarks. pdf. Hang Guo, Yawei Li, Taolin Zhang, Jiangshan Wang, Tao Dai, Shu-Tao Xia, and Luca Benini. Fastvar: Linear visual autoregressive modeling via cached token pruning. Proceedings of the International Conference on Computer Vision, 2025. Jian Han, Jinlai Liu, Yi Jiang, Bin Yan, Yuqi Zhang, Zehuan Yuan, Bingyue Peng, and Xiaobing Liu. Infinity: Scaling bitwise autoregressive modeling for high-resolution image synthesis. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, 2025. Jack Hessel, Ari Holtzman, Maxwell Forbes, Ronan Le Bras, and Yejin Choi. Clipscore: reference-free evaluation metric for image captioning. Empirical Methods in Natural Language Processing, 2021. Martin Heusel, Hubert Ramsauer, Thomas Unterthiner, Bernhard Nessler, and Sepp Hochreiter. Gans trained by two time-scale update rule converge to local nash equilibrium. Advances in neural information processing systems, 30, 2017. Jonathan Ho and Tim Salimans. Classifier-free diffusion guidance. NeurIPS 2021 Workshop on Deep Generative Models and Downstream Applications, 2022. Xiwei Hu, Rui Wang, Yixiao Fang, Bin Fu, Pei Cheng, and Gang Yu. Ella: Equip diffusion models with llm for enhanced semantic alignment. arXiv preprint arXiv:2403.05135, 2024. Doohyuk Jang, Sihwan Park, June Yong Yang, Yeonsung Jung, Jihun Yun, Souvik Kundu, SungYub Kim, and Eunho Yang. Lantern: Accelerating visual autoregressive models with relaxed speculative decoding. International Conference on Learning Representations, 2025. William Johnson, Joram Lindenstrauss, et al. Extensions of lipschitz mappings into hilbert space. Contemporary mathematics, 26(189-206):1, 1984. Ian Jolliffe. Principal component analysis. In International encyclopedia of statistical science, pp. 10941096. Springer, 2011. Tero Karras, Timo Aila, Samuli Laine, and Jaakko Lehtinen. Progressive growing of gans for improved quality, stability, and variation. International Conference on Learning Representations, 2018. Samuel Kaski. Dimensionality reduction by random mapping: Fast similarity computation for clusIn 1998 IEEE International Joint Conference on Neural Networks Proceedings. IEEE tering. World Congress on Computational Intelligence (Cat. No. 98CH36227), volume 1, pp. 413418. IEEE, 1998. Doyup Lee, Chiheon Kim, Saehoon Kim, Minsu Cho, and Wook-Shin Han. Autoregressive image generation using residual quantization. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pp. 1152311532, 2022. Jiajun Li, Yue Ma, Xinyu Zhang, Qingyan Wei, Songhua Liu, and Linfeng Zhang. Skipvar: Accelerating visual autoregressive modeling via adaptive frequency-aware skipping, 2025a. URL https://arxiv.org/abs/2506.08908. Kunjun Li, Zigeng Chen, Cheng-Yen Yang, and Jenq-Neng Hwang. Memory-efficient visual autoregressive modeling with scale-aware kv cache compression. arXiv preprint arXiv:2505.19602, 2025b. 12 Preprint. Tsung-Yi Lin, Michael Maire, Serge Belongie, James Hays, Pietro Perona, Deva Ramanan, Piotr In Computer Dollar, and Lawrence Zitnick. Microsoft coco: Common objects in context. visionECCV 2014: 13th European conference, zurich, Switzerland, September 6-12, 2014, proceedings, part 13, pp. 740755. Springer, 2014. Yihong Luo, Xiaolong Chen, Xinghua Qu, and Jing Tang. You only sample once: Taming one-step text-to-image synthesis by self-cooperative diffusion gans, 2024. Xiaoxiao Ma, Mohan Zhou, Tao Liang, Yalong Bai, Tiejun Zhao, Huaian Chen, and Yi Jin. Star: Scale-wise text-to-image generation via auto-regressive representations. arXiv preprint arXiv:2406.10797, 2024a. Xinyin Ma, Gongfan Fang, and Xinchao Wang. Deepcache: Accelerating diffusion models for free. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pp. 1576215772, 2024b. Maxime Oquab, Timothee Darcet, Theo Moutakanni, Huy V. Vo, Marc Szafraniec, Vasil Khalidov, Pierre Fernandez, Daniel Haziza, Francisco Massa, Alaaeldin El-Nouby, Russell Howes, Po-Yao Huang, Hu Xu, Vasu Sharma, Shang-Wen Li, Wojciech Galuba, Mike Rabbat, Mido Assran, Nicolas Ballas, Gabriel Synnaeve, Ishan Misra, Herve Jegou, Julien Mairal, Patrick Labatut, Armand Joulin, and Piotr Bojanowski. Dinov2: Learning robust visual features without supervision, 2023. Christos Papadimitriou, Hisao Tamaki, Prabhakar Raghavan, and Santosh Vempala. Latent seIn Proceedings of the seventeenth ACM SIGACTmantic indexing: probabilistic analysis. SIGMOD-SIGART symposium on Principles of database systems, pp. 159168, 1998. Jihun Park, Jongmin Gim, Kyoungmin Lee, Minseok Oh, Minwoo Choi, Jaeyeul Kim, Woo Chool Park, and Sunghoon Im. training-free style-aligned image generation with scale-wise autoregressive model. arXiv preprint arXiv:2504.06144, 2025. Dustin Podell, Zion English, Kyle Lacey, Andreas Blattmann, Tim Dockhorn, Jonas Muller, Joe Penna, and Robin Rombach. Sdxl: Improving latent diffusion models for high-resolution image synthesis. International Conference on Learning Representations, 2024. Liao Qu, Huichao Zhang, Yiheng Liu, Xu Wang, Yi Jiang, Yiming Gao, Hu Ye, Daniel Du, Zehuan Yuan, and Xinglong Wu. Tokenflow: Unified image tokenizer for multimodal understanding and generation. In Proceedings of the Computer Vision and Pattern Recognition Conference, pp. 25452555, 2025. Ali Razavi, Aaron Van den Oord, and Oriol Vinyals. Generating diverse high-fidelity images with vq-vae-2. Advances in neural information processing systems, 32, 2019. Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Bjorn Ommer. Highresolution image synthesis with latent diffusion models. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pp. 1068410695, 2022. Weijia Shi, Xiaochuang Han, Chunting Zhou, Weixin Liang, Xi Victoria Lin, Luke Zettlemoyer, and Lili Yu. Lmfusion: Adapting pretrained language models for multimodal generation. arXiv preprint arXiv:2412.15188, 2024. Peize Sun, Yi Jiang, Shoufa Chen, Shilong Zhang, Bingyue Peng, Ping Luo, and Zehuan Yuan. Autoregressive model beats diffusion: Llama for scalable image generation. arXiv preprint arXiv:2406.06525, 2024. Haotian Tang, Yecheng Wu, Shang Yang, Enze Xie, Junsong Chen, Junyu Chen, Zhuoyang Zhang, Han Cai, Yao Lu, and Song Han. Hart: Efficient visual generation with hybrid autoregressive transformer. International Conference on Learning Representations, 2025. Chameleon Team. Chameleon: Mixed-modal early-fusion foundation models. arXiv preprint arXiv:2405.09818, 2024. 13 Preprint. Yao Teng, Han Shi, Xian Liu, Xuefei Ning, Guohao Dai, Yu Wang, Zhenguo Li, and Xihui Liu. Accelerating auto-regressive text-to-image generation with training-free speculative jacobi decoding. International Conference on Learning Representations, 2025. Keyu Tian, Yi Jiang, Zehuan Yuan, Bingyue Peng, and Liwei Wang. Visual autoregressive modeling: Scalable image generation via next-scale prediction. Advances in neural information processing systems, 37:8483984865, 2024. Ye Tian, Xin Xia, Yuxi Ren, Shanchuan Lin, Xing Wang, Xuefeng Xiao, Yunhai Tong, Ling Yang, and Bin Cui. Training-free diffusion acceleration with bottleneck sampling. arXiv preprint arXiv:2503.18940, 2025. Junke Wang, Zhi Tian, Xun Wang, Xinyu Zhang, Weilin Huang, Zuxuan Wu, and Yu-Gang Jiang. Simplear: Pushing the frontier of autoregressive visual generation through pretraining, sft, and rl. arXiv preprint arXiv:2504.11455, 2025. Xinlong Wang, Xiaosong Zhang, Zhengxiong Luo, Quan Sun, Yufeng Cui, Jinsheng Wang, Fan Zhang, Yueze Wang, Zhen Li, Qiying Yu, et al. Emu3: Next-token prediction is all you need. arXiv preprint arXiv:2409.18869, 2024. Chengyue Wu, Xiaokang Chen, Zhiyu Wu, Yiyang Ma, Xingchao Liu, Zizheng Pan, Wen Liu, Zhenda Xie, Xingkai Yu, Chong Ruan, et al. Janus: Decoupling visual encoding for unified multimodal understanding and generation. Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, 2025. Jinheng Xie, Weijia Mao, Zechen Bai, David Junhao Zhang, Weihao Wang, Kevin Qinghong Lin, Yuchao Gu, Zhijie Chen, Zhenheng Yang, and Mike Zheng Shou. Show-o: One single transformer to unify multimodal understanding and generation. International Conference on Learning Representations, 2025. Rui Xie, Tianchen Zhao, Zhihang Yuan, Rui Wan, Wenxi Gao, Zhenhua Zhu, Xuefei Ning, and Yu Wang. Litevar: Compressing visual autoregressive modelling with efficient attention and quantization. arXiv preprint arXiv:2411.17178, 2024. Yanwu Xu, Yang Zhao, Zhisheng Xiao, and Tingbo Hou. Ufogen: You forward once large scale In Proceedings of the IEEE/CVF Conference on text-to-image generation via diffusion gans. Computer Vision and Pattern Recognition, pp. 81968206, 2024. Jiahui Yu, Yuanzhong Xu, Jing Yu Koh, Thang Luong, Gunjan Baid, Zirui Wang, Vijay Vasudevan, Alexander Ku, Yinfei Yang, Burcu Karagol Ayan, et al. Scaling autoregressive models for contentrich text-to-image generation. arXiv preprint arXiv:2206.10789, 2(3):5, 2022. Richard Zhang, Phillip Isola, Alexei Efros, Eli Shechtman, and Oliver Wang. The unreasonable effectiveness of deep features as perceptual metric. In Proceedings of the IEEE conference on computer vision and pattern recognition, pp. 586595, 2018. 14 Preprint."
        },
        {
            "title": "APPENDIX",
            "content": "A APPENDIX: IMPLEMENTATION DETAILS A.1 STATISTICAL ANALYSIS OF THE RANK In order to determine the rank corresponding to given α satisfying Eq. (4), it is necessary to perform an SVD decomposition of the original feature (cid:101)Fk1 to obtain the energy ratio ηr, where the SVD decomposition is time-consuming. In Sec. 3.3, to address the additional time required to determine for given α, we adopt an off-the-shelf value of corresponding to α, thereby avoiding both SVD decomposition and the use of Eq. (4). Specifically, we adopt 553 prompts from the GenEval benchmark (Ghosh et al., 2023). For each prompt, four images are randomly generated. During the generation process, we perform SVD decomposition on the features of each input block to obtain ηr, and then compute the corresponding rank for given α using Eq.4. The collected values across all prompts are then aggregated to determine representative rank for each α. In this way, once α is specified, the corresponding can be directly assigned. For example, as shown in Tab. 7, when α = 0.96, the standard deviation of across different blocks is an order of magnitude smaller than its mean, suggesting that for given α, the features exhibit stable low-rank characteristics across diverse text prompts. To further demonstrate that the statistical rank for given α generalizes across different text prompts, we additionally perform the analysis on the COCO2014 and COCO2017 datasets, which offer more diverse and broader descriptions Lin et al. (2014). Specifically, we randomly sample 1K text prompts from COCO2014 and generate one image for each prompt. Similarly, 1K text prompts are randomly selected from COCO2017, with one image generated per prompt. The statistical results are shown in Tabs. 8 and 9. The deviation from the corresponding block and scale values reported in Tab. 7 is negligible, confirming that the statistical rank for given α generalizes robustly across diverse text prompts. Note that while collecting the statistical results on the benchmark is computationally expensive, it is an offline, one-time process. During inference, the pre-determined rank can be directly applied base on given α without incurring additional overhead. Table 7: Detailed information about the ranks of the block features in Infinity (Han et al., 2025). k-th scale block name block chunks.0 block chunks.1 block chunks.2 block chunks.3 block chunks.4 block chunks.5 block chunks.6 block chunks.7 40 48 0.0160.0004 0.0130.0003 0.0080.0007 0.1360.0096 0.1170.0078 0.0540.0064 0.2100.0187 0.1890.0161 0.0830.0176 0.2500.0212 0.2220.0171 0.0660.0164 0.2560.0236 0.2340.0180 0.0560.0146 0.1980.0170 0.1830.0124 0.0290.0084 0.1570.0074 0.1340.0053 0.0380.0067 0.1910.0068 0.1630.0057 0.0550.0142 A.2 CONSTRUCTION OF Uo As shown in Fig. 3 3 in Sec. 3.3, we construct the r-dimensional feature (cid:101)Fr = (cid:101)Σr (cid:101)VT . (cid:101)Fr serves as an r-dimensional representation of the row space of (cid:101)Fk1, conditioned on (cid:101)Ur to provide the most closely rank-r approximation (Eckart & Young, 1936). However, after the model forward, (cid:101)Fr produces only is unavailable, thereby hindering the restoration of the original -dimensional feature . According to Eq. (2), the autoregressive likelihood models the token at the k-th scale step as conditioned not only on itself but also on all previous steps {1, 2, . . . , k-1}. Inspired by this mechanism, FastVAR (Guo et al., 2025) caches outputs from the (k-1)-th scale step to restore the tokens for the k-th scale. Similarly, we employ Uo,cache from the cached counterpart feature ). In detail, k1 to compensate for (cid:101)Ur, as follows Uo (Fig. 3 3 ), while Uo ( (cid:101)Ur +Uo,cache 15 Preprint. Table 8: Detailed information about the ranks of the block features in Infinity (Han et al., 2025) on the COCO2014 dataset. k-th scale block name block chunks.0 block chunks.1 block chunks.2 block chunks.3 block chunks.4 block chunks.5 block chunks.6 block chunks.7 40 48 64 0.0160.0004 0.0130.0003 0.0060.0003 0.1420.0087 0.1210.0068 0.0540.0057 0.2420.0168 0.2130.0145 0.0970.0168 0.2990.0188 0.2560.0155 0.0510.0185 0.3170.0192 0.2780.0155 0.0620.0177 0.2390.0132 0.2120.0099 0.0400.0092 0.1860.0059 0.1570.0043 0.0480.0033 0.2320.0062 0.1980.0056 0.0690. Table 9: Detailed information about the ranks of the block features in Infinity (Han et al., 2025) on the COCO2017 dataset. k-th scale block name block chunks.0 block chunks.1 block chunks.2 block chunks.3 block chunks.4 block chunks.5 block chunks.6 block chunks.7 40 64 0.0170.0004 0.0130.0003 0.0070.0002 0.1420.0090 0.1210.0068 0.0540.0057 0.2420.0174 0.2130.0145 0.1010.0167 0.2990.0190 0.2560.0154 0.0500.0181 0.3180.0193 0.2790.0150 0.0620.0167 0.2390.0138 0.2130.0099 0.0380.0088 0.1870.0061 0.1570.0044 0.0470.0032 0.2320.0064 0.1980.0059 0.0670.0060 the cached feature followed by SVD to obtain Uo,cache . k1 is upsampled to match the dimensions of k as o,cache k1 (See Eq. (7)), A.3 CONSTRUCTION OF Wo , while = . Similarly to the naive SVD-based strategy (Tab. 2 3 and Fig. 4 3 ) in Sec. A.2, we leverage k1 to compensate for (cid:99)Wr, as follows Wo k1 is upsampled to match the dimensions of As show in Fig. 4 (Fidelity refinement stage), after the model forward, (cid:98)Fr produces only Wo is unavailable, thereby hindering the restoration of the original -dimensional feature Wo rF the Wo,cache ( (cid:99)Wr + Wo,cache as o,cache from the cached counterpart feature ). In detail, the cached token map (See Eq. (7)), followed by solving LLS problem to obtain Wo,cache k1 . 16 Preprint. Figure 10: Two examples of our StageVAR, which achieves 3.4 speedup while consistently producing images of comparable quality to Vanilla (i.e., Infinity (Han et al., 2025)) across multiple generations with random projection (RP). APPENDIX: ALGORITHM DETAIL OF STAGEVAR Algorithm 1 : StageVAR Input : Scale steps {1, 2, , K}. Scale steps set of the fidelity refinement stage consists of steps {K-m+1, , K}. Steps set of the semantic and structure establishment stages {1, , m}. The VAR model ϕ, and the image decoder D. The quantizer Q, which typically includes codebook RV containing vectors. // SOS is the start token (Han et al., 2025) Output: The final generated images F0 = 0 ; (cid:101)F0 = SOS R11d ; // the semantic and structure establishment stages for = 1, , K-m do = ϕ( (cid:101)Fk1) ; ) ; Rk = Q(F Fk = Fk1 + Up(Rk, (hK, wK)) ; (cid:101)Fk = Down(Fk, (hk, wk)) ; end // the fidelity refinement stage (CFG=0) for = K-m+1, , do // Fig. 3 1 // Eq. (1) // Eq. (3) // Random Projection (RP) for Low-Rank Feature (cid:98)Fr // Fig. // Eq. (7) // based on (Frieze et al., 2004) k1) ; = Up(F (cid:98)Fr = QT (cid:101)Fk1 ; = ϕ( (cid:98)Fr) ; // Representative Token Restoration (RTR) o,cache k1 The chosen indices I, based on (cid:101)Fk1 ; (i) := {F // Vanilla VAR Rk = Q(F ) Fk = Fk1 + Up(Rk, (hK, wK)) ; (cid:101)Fk = Down(Fk, (hk, wk)) ; (i) I} {F o,cache / I} ; k1 // Eq. (8) // Eq. (1) // Eq. (3) end = D(FK) Return The final generated image APPENDIX: ABLATION ANALYSIS C.1 ROBUSTNESS TO RANDOM PROJECTION (RP) As shown in Fig. 10, we demonstrate the robustness of our method to random projection (RP) when obtaining the r-dimensional feature. StageVAR achieves significant sampling speedup (3.4) while maintaining image quality. 17 Preprint. Figure 12: StageVAR maintains both generation quality and textimage alignment even for complex prompts. C.2 FREQUENCY-DOMAIN ANALYSIS To evaluate the lowand high-frequency components of the generated images compared with those of the vanilla model, we conduct experiments in the frequency domain  (Fig. 11)  . Specifically, we use 553 prompts from the GenEval benchmark, with both Infinity and StageVAR generating one random image per prompt. As shown in Fig. 11 (Left), we observe that the lowand high-frequency components exhibit almost no loss in the frequency domain, suggesting strong frequency-domain Figure 11: The lowand high-frequency compoconsistency between methods. This result ennents of the generated images from Ours closely sures that the generated images closely resemresemble those from the vanilla model (Infinity). ble those of both Infinity and StageVAR (with the proposed index sampling strategy), while maintaining the desired fidelity. The qualitative comparison further supports this consistency, showing that the highand low-frequency components exhibit almost no noticeable differences across methods (Fig. 11 (Right)). C.3 SEMANTIC IRRELEVANCE IN COMPLEX PROMPTS Fig. 12 shows that StageVAR maintains both generation quality and textimage alignment even for complex prompts. As shown in Fig. 12 (the last column (Prompt 5 )), it can still preserve semantic consistency even when using rare text prompts. Prompt 1 : Create mesmerizing image of three intricately designed potions displayed on an ornate, antique wooden table within charming old apothecary. The first potion is captivating cobalt blue, housed in stunning pentagon-shaped glass bottle that sparkles with its many facets; its label, meticulously crafted with delicate silver filigree and botanical illustrations of ethereal flowers, prominently features the letters in an ornate, swirling script, while silver ribbon interwoven with tiny sapphire beads wraps around the neck, adorned with charm in the shape of crescent moon. The second potion is rich crimson red, contained in flat, oval-shaped glass bottle adorned with intricate engravings of mystical symbols, including runes and ancient scripts; its label displays the letters in embossed gold leaf, framed by elaborate floral designs, and is topped with cork stopper embellished with miniature brass key and tiny ruby gemstones. The third potion is vivid emerald green, held in sleek square glass bottle featuring enchanting etchings of mythical creatures like dragons and phoenixes; its 18 Preprint. Table 11: Performace with the low-rank feature by varing α to evaluate the low-rank property in STAR (Ma et al., 2024a) Methods GenEval Vanilla α = 0.999 (59.5% rank) α = 0.99 (34.4% rank) α = 0.98 (26.1% rank) α = 0.97 (21.1% rank) α = 0.96 (17.6% rank) α = 0.95 (14.9% rank) 0.514 0.510 0.505 0.507 0.505 0.495 0.493 Figure 14: Evolution of semantic and perceptual quality when the starting scale steps of CFG is set to 0 for STAR (Ma et al., 2024a) scroll-like label, crafted from aged parchment, prominently features the letter intertwined with ancient alchemical symbols and delicate vine patterns. All three bottles are approximately the same height, creating harmonious display against backdrop filled with shelves overflowing with dried herbs, colorful glass jars, and ancient scrolls, all illuminated by soft, warm light filtering through stained-glass window, enhancing the magical atmosphere of the apothecary. Prompt 2 : The image presents picturesque mountainous landscape under cloudy sky. The mountains, blanketed in lush greenery, rise majestically, their slopes dotted with clusters of trees and shrubs. The sky above is canvas of blue, adorned with fluffy white clouds that add sense of tranquility to the scene. In the foreground, valley unfolds, nestled between the towering mountains. It appears to be rural area, with few buildings and structures visible, suggesting the presence of small settlement. The buildings are scattered, blending harmoniously with the natural surroundings. The image is captured from high vantage point, providing sweeping view of the valley and the mountains. Prompt 3 : bullet time photography of beautiful 90 year old woman, frontal portrait, colorful futuristic felt dress covered with colorful buttons, futuristic hat, Gucci style earrings, very bright red lipstick, the woman is wearing pair of goggles, colored rain. Prompt 4 : Product photography, perfume placed on white marble table with pineapple, coconut, limenext to it as decoration, white curtains, full of intricate details, realistic, minimalist, layered gestures in bright and concise atmosphere, minimalist style. Prompt 5 : cheeseburger with juicy beef patties and melted cheese sits on top of toilet that looks like throne and stands in the middle of the royal chamber. APPENDIX: ADDITIONAL ANALYSIS D.1 STAGEVAR FOR OTHER TEXT-TO-IMAGE VAR MODELS: STAR (MA ET AL., 2024A) We evaluate semantic (CLIP and DINO) and structural consistency (LPIPS and DISTS) across all scale steps in the next-scale prediction model, STAR (Ma et al., 2024a). As shown in Fig. 13 (a19 Preprint. Figure 13: (a) Visualization of semantic evolution across all scale steps (i.e., CLIP and DINO). (b) Visualization of structure evolution on all scale steps (i.e., LPIPS and DISTS). (c) Variations of the next scale step in the frequency domain. (Bottom) Visualization of samples across all scale steps in STAR (Ma et al., 2024a). b), local semantic (DINO) and structural consistency (LPIPS and DISTS) are gradually established from the initial to the final scale, except that global semantics (CLIP) converge at the later scales (e.g., scale 48). Fig. 13 (Bottom) also illustrates this tendency. Moreover, both the low-frequency and high-frequency components noticeable variations across all scale steps (Fig. 13c). We evaluate the semantic irrelevance in the next-scale prediction model STAR (Ma et al., 2024a) by setting the CFG scale to 0 starting from step k. As shown in Fig. 14, the CLIP and GenEval scores exhibit convergence at the last two scales. We also evaluate the low-rank property . Based on the aforementioned analysis, STAR (Ma et al., 2024a) shows the low-rank property and semantic irrelevance. APPENDIX: STATEMENTS Broader Impacts. StageVAR introduces an acceleration strategy for T2I VAR to improve sampling speed. However, it may also entail potential risks. Specifically, it could be exploited to generate false or misleading images, thereby contributing to the spread of misinformation. When applied to public figures, it may raise concerns regarding privacy infringement. Furthermore, automatically generated images could potentially involve issues related to copyright and intellectual property. Ethical Statement. We acknowledge the potential ethical implications of deploying generative models, including issues related to privacy, data misuse, and the propagation of biases. All models used in this paper are publicly available. We will release the modified codes to reproduce the results of this paper. We also want to point out the potential role of customization approaches in the generation of fake news, and we encourage and support responsible usage. Reproducibility Statement. To facilitate reproducibility, we will make the entire source code and scripts needed to replicate all results presented in this paper available after the peer review period. We conducted all experiments using publicly accessible datasets. LLM usage statement. We used large language model solely to aid in polishing the writing and improving the clarity of the manuscript. The model was not involved in ideation, data analysis, or deriving any of the scientific contributions presented in this work."
        }
    ],
    "affiliations": [
        "City University of Hong Kong (Dongguan), China",
        "Linkoping University",
        "Mohamed bin Zayed University of Artificial Intelligence",
        "VCIP, CS, Nankai University"
    ]
}