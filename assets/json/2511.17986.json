{
    "paper_title": "Plan-X: Instruct Video Generation via Semantic Planning",
    "authors": [
        "Lun Huang",
        "You Xie",
        "Hongyi Xu",
        "Tianpei Gu",
        "Chenxu Zhang",
        "Guoxian Song",
        "Zenan Li",
        "Xiaochen Zhao",
        "Linjie Luo",
        "Guillermo Sapiro"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Diffusion Transformers have demonstrated remarkable capabilities in visual synthesis, yet they often struggle with high-level semantic reasoning and long-horizon planning. This limitation frequently leads to visual hallucinations and mis-alignments with user instructions, especially in scenarios involving complex scene understanding, human-object interactions, multi-stage actions, and in-context motion reasoning. To address these challenges, we propose Plan-X, a framework that explicitly enforces high-level semantic planning to instruct video generation process. At its core lies a Semantic Planner, a learnable multimodal language model that reasons over the user's intent from both text prompts and visual context, and autoregressively generates a sequence of text-grounded spatio-temporal semantic tokens. These semantic tokens, complementary to high-level text prompt guidance, serve as structured \"semantic sketches\" over time for the video diffusion model, which has its strength at synthesizing high-fidelity visual details. Plan-X effectively integrates the strength of language models in multimodal in-context reasoning and planning, together with the strength of diffusion models in photorealistic video synthesis. Extensive experiments demonstrate that our framework substantially reduces visual hallucinations and enables fine-grained, instruction-aligned video generation consistent with multimodal context."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 2 2 ] . [ 1 6 8 9 7 1 . 1 1 5 2 : r Plan-X: Instruct Video Generation via Semantic Planning Lun Huang1,2,3*, You Xie3, Hongyi Xu3, Tianpei Gu3, Chenxu Zhang3, Guoxian Song3, Zenan Li3, Xiaochen Zhao3, Linjie Luo3, Guillermo Sapiro2,4 1Duke University 2Princeton University 3ByteDance Intelligent Creation 4Apple https://byteaigc.github.io/Plan-X guillermos@princeton.edu lun.huang@duke.edu {you.xie, hongyixu, tianpei.gu, chenxuzhang, guoxiansong, zenan.li, xiaochen.zhao, linjie.luo}@bytedance.com Figure 1. We introduce Plan-X, semantic planner that reasons over multimodal prompts and instructs video diffusion models via spatiotemporal semantic guidance, substantially mitigating visual hallucination and enhancing prompt alignment and long-horizon reasoning."
        },
        {
            "title": "Abstract",
            "content": "Diffusion Transformers have demonstrated remarkable capabilities in visual synthesis, yet they often struggle with high-level semantic reasoning and long-horizon planning. This limitation frequently leads to visual hallucinations and mis-alignments with user instructions, especially in scenarios involving complex scene understanding, humanobject interactions, multi-stage actions, and in-context motion rea- *Work done during an internship at ByteDance. soning. To address these challenges, we propose PlanX, framework that explicitly enforces high-level semantic planning to instruct video generation process. At its core lies Semantic Planner, learnable multimodal language model that reasons over the users intent from both text prompts and visual context, and autoregressively generates sequence of text-grounded spatio-temporal semantic tokens. These semantic tokens, complementary to highlevel text prompt guidance, serve as structured semantic sketches over time for the video diffusion model, which 1 has its strength at synthesizing high-fidelity visual details. Plan-X effectively integrates the strength of language models in multimodal in-context reasoning and planning, together with the strength of diffusion models in photorealistic video synthesis. Extensive experiments demonstrate that our framework substantially reduces visual hallucinations and enables fine-grained, instruction-aligned video generation consistent with multimodal context. 1. Introduction Recent advances in Diffusion Transformers (DiTs) [29] have revolutionized visual generation, achieving unprecedented fidelity and temporal consistency in both image and video synthesis [7, 9, 18, 24, 30, 42]. Despite these successes, DiTs remain fundamentally limited in high-level semantic reasoning. When prompted with complex or compositional instructions, such as multi-stage actions or humanobject interactions within intricate scenes, they often produce visually plausible yet semantically inconsistent results. Such failures typically manifest as prompt misalignment, visual hallucination, incorrect action ordering, or incoherent spatio-temporal relations, stemming from the models inability to jointly reason over textual guidance and visual context or to plan semantic evolution across extended temporal horizons. The key difficulty of the problem lies in the entanglement between semantic reasoning and pixel-level synthesis. Conventional video diffusion models are optimized to capture fine-grained spatial details and temporal dynamics but lack an explicit mechanism for semantic abstraction, that is, understanding how intentions, actions, and spatial relationships correlate with multimodal instructional context and evolve over time. To alleviate this limitation, recent state-of-the-art video generation approaches [7, 19, 42] often incorporate auxiliary language models for prompt enhancement (PE), expanding high-level user prompts into detailed textual descriptions that better align with the training distribution. While such text conditioning provides coarse global control, it still fails to enforce structured, frame-level semantic consistency, often resulting in undesirable hallucinations or semantic drift. Moreover, even with detailed and accurate textual guidance, the underlying DiT may still struggle to interpret complex multimodal instructions, particularly in long-horizon or compositional scenarios. In this work, we argue that semantic reasoning and visual synthesis should be treated as distinct yet complementary processes. We introduce Plan-X, framework that decouples high-level instruction understanding and semantic planning from holistic video synthesis. The central idea is to offload semantic reasoning to trainable multimodal language model, termed the Semantic Planner, which interprets the users prompt and visual context and autoregressively generates sequence of spatio-temporal semantic visual tokens. Unlike abstract textual scripting, the Semantic Planner is trained to produce spatially-grounded visual tokens corresponding to sequential keyframes, serving as structured semantic sketches describing what should happen and when. These structured semantic sketches then guide video DiT, which focuses solely on decoding the semantic sketches into high-fidelity, temporally consistent videos. To bridge textual and visual semantics, we adopt text-aligned tokens (TA-Tok) [12], derived from quantized SigLIP2 [45] representations via language-model codebook. This design enables the language model to translate high-level multimodal instructions into spatio-temporal visual structures over an extended multimodal vocabulary, providing interpretable and controllable conditioning for video synthesis. This design offers two key advantages. First, it transforms the challenge of joint semantic understanding and generation into language modeling problem, enabling efficient in-context multimodal reasoning and compositional planning through text-aligned semantic token representations. Second, it preserves the strengths of diffusion transformers in realistic, temporally coherent rendering without overburdening them with abstract reasoning. Together, these components form unified multimodal system capable of interpreting, planning, and executing complex video generation tasks. We extensively evaluate Plan-X on challenging video generation benchmark covering text-to-video, image-tovideo, and video continuation tasks. Across all settings, Plan-X achieves substantial improvements, both quantitatively and qualitatively, in prompt alignment, semantic coherence, and motion naturalness across diverse scenarios, including humanobject interaction, multi-stage actions, and long-horizon planning. Furthermore, our experiments show that the proposed Semantic Planner not only mitigates visual hallucination but also enables interpretable and transferable semantic guidance, facilitating downstream applications such as semantic cross-transfer. 2. Related Works Video Generation. Fueled by rapid advances in largescale generative modeling, both closedand open-source video generation systems [7, 9, 18, 19, 24, 46] have made remarkable progress, particularly within diffusion-based frameworks. Early architectures primarily adopted UNet backbones [34] initialized from image diffusion models [13], later extended to the video domain through temporal modeling mechanisms [11]. Recent research has shifted toward Diffusion Transformers (DiTs) [29], which achieve substantially higher visual fidelity and temporal consistency than U-Net variants. The original DiT architecture employs cross-attention to connect video and text embeddings, whereas MM-DiT frameworks [6, 37, 40] concate2 Figure 2. Overview of Plan-X. Plan-X comprises two key components: an MLLM for high-level semantic reasoning and planning, and DiT for high-fidelity video synthesis. The Semantic Planner receives multimodal inputs, including descriptive text prompt for static scene content (Timg), motion description (Tmotion), an instructional system prompt (Tsys) specifying the number of target frames , and optionally the semantically encoded first frame I0. It then autoregressively generates discrete text-aligned semantic tokens that encode spatio-temporal semantic structures in the form of keyframes. Complementary to global text conditioning, we introduce dedicated semantic guidance branch that instructs pretrained DiT model to translate these structured semantics augmented with 3D spatio-temporal RoPE into highfidelity, temporally coherent video realizations. nate text embeddings with visual tokens for full-sequence attention. The text encoder plays central role in determining semantic alignment: current state-of-the-art systems often utilize large language models (LLMs) such as T5 [33] or Qwen [41], sometimes coupled with CLIP [32] to strengthen multimodal consistency. Moreover, to align user prompts with caption-style conditioning distributions, many frameworks incorporate an auxiliary language model, often termed Prompt Enhancer [53], that expands or reformulates user instructions before synthesis. Despite these advancements, existing video diffusion models still exhibit semantic misalignment and visual hallucination when handling complex, multi-action, or long-horizon prompts, due to the lack of explicit semantic reasoning and temporal planning mechanisms. Unified MultiModal Visual Generation. Beyond textconditioned generation, recent research has increasingly focused on unifying multimodal understanding and generation within single framework [2, 5, 12, 17, 20, 21, 23, 27, 31, 38, 43, 47, 4952, 54, 57, 58]. To enable cross-modal generation, many of these approaches develop semanticsaware, text-aligned visual encoders [17, 39, 47, 48, 54], adapt pretrained semantic encoders for pixel representation [12, 21, 49, 50], or combine both strategies in hybrid fashion [5, 23]. TA-Tok [12] exemplifies this direction by converting SigLIP2 embeddings [45] into discrete textaligned tokens, forming unified token space that bridges textual and visual modalities. In our work, we leverage this tokenizer to represent both video frames and text prompts within shared semantic space, facilitating fine-grained cross-modal reasoning. To further harness the multimodal reasoning capacity of multimodal large language models (MLLMs), recent works either train them for autoregressive prediction of visual tokens [3, 5, 51, 58] or introduce learnable query encodings [2, 8, 27], sometimes paired with diffusion heads for high-fidelity image synthesis in continuous latent spaces. However, these approaches primarily target static image generation or editing, and thus remain limited in modeling temporal dynamics and long-range semantic reasoning. Plan-X extends this generative paradigm to the video domain by bridging MLLMs and video DiTs for high-fidelity, temporally consistent video synthesis under multimodal semantic reasoning and planning. 3. Method Recent video diffusion models [7, 18, 35, 46], denoted as G, are typically built upon Diffusion Transformers (DiTs) [28] and generate videos conditioned on text prompt , optionally combined with reference image I0 serving as the first frame. These models are generally trained with denoising objectives, emphasizing pixel-level fidelity and temporal smoothness. However, despite their impressive visual quality, diffusion models often struggle with highlevel semantic interpretation and reasoning, as well as maintaining coherent semantic structure and evolution over extended temporal horizons. In contrast, large language models (LLMs) [4, 10, 25, 44, 53] excel at abstract, high-level semantic understanding and long-horizon reasoning, but they still lag behind diffusion models in video synthesis due to error accumulation, coarse spatial grounding, and limited 3 visual expressiveness. To overcome these limitations, we propose decoupled design that separates high-level semantic reasoning and planning from low-level visual synthesis. Specifically, instead of merging semantic planning and video generation into single DiT model via = G(T , I0), our approach delegates complex semantic reasoning to specialized multimodal large language model M, which transforms multimodal prompts into spatio-temporal semantic guidance. The diffusion transformer is then conditioned on both the original inputs and the generated semantic blueprint to perform semantics-driven video synthesis: = M(T , I0), = G(S, , I0). (1) This synergistic modular framework enables each comthe multimodal language model ponent to specialize: captures and structures semantic intent and temporal logic, while the diffusion transformer focuses on realizing these semantics through high-fidelity, temporally consistent synthesis. An overview of the proposed framework is illustrated in Figure 2. 3.1. Text-Grounded Semantic Planning To bridge the language model and the video DiT through semantic scripting, we evaluate three potential design choices. The most common approach is text based conditioning, where the language model serves as prompt enhancer that expands high level and abstract user instructions into detailed textual descriptions. While this representation is native to the language models capability, textual semantics remain difficult for the DiT to fully interpret and realize. Moreover, when translating between concise prompts and detailed captions, the language model is prone to hallucination [16], which leads to semantic drift and prompt misalignment. Another choice adopted in several unified frameworks for image understanding and generation [2, 8, 27] employs implicit representations, such as hidden states from the language model or with fixed-length learnable queries, as conditioning inputs for downstream diffusion models. However, these implicit embeddings lack explicit spatio-temporal structure and are not flexible representing videos with varying resolution or duration. As result, the diffusion model remains overburdened with translating abstract global features into semantically coherent videos instead of focusing on high-fidelity visual synthesis. Therefore, we adopt bridging design that emphasizes high-level spatio-temporal visual semantics across variable duration, while remaining interpretable by both the language model and the diffusion model. To achieve this, our semantic planner functions as an autoregressive model over text-grounded visual semantic tokens, generating semantic trajectory that directs the video diffusion. Spatio-Temporal Semantic Tokens. Visual representation encoders such as CLIP [32], SigLIP2 [45], and 4 DINO [26] have been widely adopted in visual understanding tasks, demonstrating strong structural and semantic representation capability. Recent studies [36, 57] have further shown that their latent spaces can support both visual reconstruction and generation. Motivated by these findings, we leverage the semantic latent space of the SigLIP2-so400mpatch14-384, where the spatial structure and temporal evolution of video are represented through uniformly sampled keyframes that are patchified into tokens. SigLIP2 tokens are trained to capture both global and local semantic structures while remaining aligned with textual representations. These text-grounded semantic tokens are interpretable by the language model and simultaneously provide semantically rich spatio-temporal structure for the video DiT. In contrast to abstract textual descriptions that serve only as global conditions, our semantic tokens provide concrete structural sketches across both spatial and temporal dimensions. To avoid overburdening the language model with pixel-level details or dense frame-wise temporal modeling, we sample sparse keyframes at 2 FPS. Each keyframe is uniformly resized to 384 384 and encoded into patchified spatial tokens. To further facilitate semantic token generation within the language model, we employ TA-Tok [12], which discretizes SigLIP2 embeddings using pretrained LLM-based text codebook. Specifically, TA-Tok fine-tunes the SigLIP2 encoder and quantizes its visual embeddings through text-aligned codebook initialized from LLM embeddings, forming an expanded visual vocabulary. Notably, we apply scale-adaptive pooling to obtain = 81 spatial tokens per keyframe, denoted as S, achieving balanced abstraction between high-level semantics and spatial precision. This visualtext token representation enables the language model to perform cross-modal reasoning and generation within shared discrete semantic space, effectively bridging visual and linguistic modalities. Autoregressive Semantic Planning To accommodate semantic planning over adaptive temporal durations, we formulate the task as next-token prediction problem, where LLM autoregressively generates spatio-temporal semantic tokens given multimodal context inputs. Specifically, we adopt Qwen-2.5-Instruct [53] as the LLM backbone M, which autoregressively predicts sequence of semantic keyframes St at 2 FPS, conditioned on the textual prompt and optionally semantically encoded initial frame S0. We standardize all textual prompts into unified instruction-following format, which describes both the static scene content Timg and the intended video dynamics Tmotion, together with the target duration specified as keyframes Tsys. Our training strategy supports multi-task generation within single unified framework, encompassing text-to-video, image-to-video, and video continuation tasks. Notably, the language model jointly interprets both textual and visual context (for example, the first frame) within shared semantic token space, thereby strengthening the correspondence between visual content and user instructions. This design effectively mitigates visual hallucination that often arises from conventional prompt enhancement methods. We employ fully causal attention across all semantic tokens, enabling streamable generation with flexible and extensible temporal durations. The language model is trained to autoregressively predict the next semantic tokens by minimizing the following objective: Lplanner = (cid:88) (cid:88) t=1 i=1 log (St,iS0, , S<t, St,<i) (2) where St,i is the i-th spatial token of the t-th frame. 3.2. Semantics-Instructed Video Synthesis Diffusion models have been widely adopted for video synthesis, where the video latent is learned to progressively denoise from noise tensor (0, I). The text caption serves as global contextual condition throughout the denoising process, typically incorporated through cross-attention mechanisms or within multimodal DiT (MMDiT) architecture. In this section, we describe how the synthesized spatio-temporal semantics are integrated into pre-trained DiT-based video diffusion model G, while preserving the models intrinsic video generation capabilities in visual fidelity and temporal coherence. In-Context Semantics Guidance. To achieve this integration, we replace the original text-conditioning branch in with hybrid conditioning module that attends video latents to both the high-level text prompt for global context and the spatio-temporal semantics for fine-grained structural guidance. Specifically, we replicate the text conditioning pathway for S, either through duplicated cross-attention layers or by adding dedicated semantic modality branch within the MMDiT architecture, depending on the design of the underlying diffusion model. The semantic token embeddings are projected through lightweight two-layer MLP, producing semantic context features that are aligned with the DiT hidden-state dimensionality. In contrast to channel-wise structural offsets commonly used in ControlNet [55] or PoseGuider [14], our approach treats as complementary semantic context that spans both spatial and temporal dimensions, rather than deterministic control signal. This design allows to fully exploit its inherent capability for pixel-level visual realization, while remaining robust against misalignment or inconsistency propagated from the semantic planner. However, unlike textual context conditioning that spans the entire spatio-temporal spectrum, each semantic token encodes localized context, specifying what occurs, where, and when. To strengthen the alignment between semantic tokens and video latents, we enrich with 3D time-aligned spatio-temporal Rotary Position Embeddings (RoPE), introducing explicit positional correspondence across both space and time. Let qv denote the query tensor derived from the video latents and ks the key tensor derived from the semantic tokens S. The rotary-encoded query qv and key ks are computed as qv,i = R(θ, pv,i)qv,i, ks,j = R(θ, ps,j), ks,j, (3) where R(θ, p) is the rotation matrix parameterized by base frequency θ at position p. The position indices pv,i = (t, h, w) and ps,j = (t, h, w) correspond to the spatiotemporal coordinates in the video latent and semantic token grids, respectively. Notably, the semantic tokens and video latents have different compression ratios across spatial resolutions, and the semantic tokens are sampled more sparsely in time. Accordingly, RoPE frequencies are scaled based on the 3D grid resolution, ensuring that each position in the semantic map maintains consistent relative alignment with its corresponding location in the video latent feature space. Staged Training. The training of the video generator is conducted in two stages. We initialize the weights of the newly added semantic context branch using those of the text branch, and in the first stage, only the new semantic branch is trained while the original model parameters are frozen to preserve the pretrained generative capability. Specifically, we encode the ground-truth video into semantic tokens using the TA-Tok tokenizer, and train the model with the native flow-matching loss that reconstructs the velocity field for denoising video latents x0 from noise z: Lvid = Ex0,z,kπ(k) (cid:2)v vθ(xk, k, C)2 2 (cid:3) , (4) where π(k) is sampling distribution for timestep k, and denotes the conditioning set that may include both text and semantic guidance. At the beginning of training, we fully drop the text conditioning, enforcing the diffusion model to rely solely on the semantic context. As training progresses, the text drop rate is gradually reduced to 0.5, allowing the model to restore its text-conditioned generation capability while learning to co-utilize both text and semantic guidance. To further improve robustness to imperfect semantic predictions, we fine-tune the model end-to-end by replacing ground-truth semantic tokens with hidden states of generated tokens from the semantic planner. This stage mitigates exposure bias and enhances the models self-adaptation to noisy semantic inputs during inference. 4. Experiments 4.1. Implementation Details Dataset. We curate large-scale corpus of 4.5M training video clips by combining multiple public datasets and inhouse collections. Specifically, we include humanobject 5 Figure 3. Qualitative results. Plan-X produces high-fidelity, semantically consistent, and instruction-aligned videos across T2V, I2V, video continuation, and semantic transfer, all within single unified framework. interaction datasets such as Taste-Rob [56] and HOIGen1M [22], along with diverse general-domain video dataset collected internally. For unified video captioning, we employ Qwen3-VL-32B [1], generating descriptions that capture both static scene content and dynamic motion. For evaluation, we construct challenging benchmark comprising samples from the test split of Taste-Rob [56] and additional in-the-wild cases, covering both text-tovideo and image-to-video tasks. The benchmark includes 50 test cases per task, featuring humanobject interactions in complex scenes and multi-step action sequences. In contrast to the prompt suite in VBench [15], our benchmark specifically evaluates semantic planning capability under multi-context and compositional actions scenarios. Training and Inference. Our full pipeline includes variants of both the semantic planner and the video diffusion model. The semantic planner is initialized from Qwen2.5Instruct [53] with either 1.5B or 7B parameters, while the video diffusion backbones are based on Wan 2.2 (5B) [46] or Seedance 1.0 [7]. By default, we report results using the 7B semantic planner paired with Wan 2.2 (5B), and analyze the effects of model scale and backbone choice in the ablation studies presented later. Training of the semantic planner and diffusion model is performed on 48 A100 GPUs with an effective batch size of 48, using the AdamW optimizer with learning rates of 5 105 and 2 105, respectively. The semantic planner and diffusion model are trained for 7 epochs and 2 epochs, followed by joint end-toend fine-tuning for one additional epoch."
        },
        {
            "title": "Our semantic planner is trained with a context window of",
            "content": "4K tokens, supporting extensible generations of 20 seconds, and we use sampling temperature of 0.9 during inference. The diffusion model is inferred with 50 denoising steps under classifier-free guidance weight of 5.0. For unconditional inputs, we condition on the initial frame for the I2V task and use negative text prompts to suppress low-quality aesthetics and unnatural motion. Please refer to the supplementary materials for additional dynamic visual results and extended evaluations. 4.2. Evaluation Baselines. We compare Plan-X with five state-of-the-art video diffusion models, including Wan 2.2-5B [46], HunyuanVideo [18], and SkyReelsV2-14B [30], as well as two commercial systems, Kling 1.6 [19] and Seedance 1.0 [7]. Comprehensive comparative experiments are conducted across these baselines and our approach under the proposed I2V benchmark. All baseline models are evaluated using their default inference settings, without additional fine-tuning and super-resolution. For consistency, all generated videos are resized to 480p for fair comparison. Qualitative Evaluation. In Figure 1 and 3, we showcase the capability of Plan-X in synthesizing high-fidelity, prompt-aligned videos through multimodal reasoning and multi-stage semantic planning. Specifically, we illustrate: the (1) multimodal planning ability of the semantic planner, which jointly reasons over all input modalities in unified manner and generates coherent semantic trajectories across combinations of contexts, such as text, first frame, and initial video chunk, manifesting as T2V, TI2V 6 Table 1. Quantitative evaluation and ablation. We denote the best results in bold and the second-best results with underline. Method Accuracy Completeness Fidelity Consistency Naturalness Visual Human Pref. Wan 2.2-5B [46] HunyuanVideo [18] SkyReelsV2-14B [30] Kling 1.6 [19] Seedance 1.0 [7] Wan 2.2 5B + SFT Wan 2.2 5B + query Wan 2.2 5B + PE Plan-X-Wan w/o 3D RoPE Plan-X-Wan (1.5B M) Plan-X-Wan Plan-X-Seedance 0.3156 0.4467 0.5575 0.6810 0. 0.5628 0.7030 0.5773 0.5889 0.7556 0.7816 0.7971 0.3967 0.5133 0.5775 0.7357 0.7943 0.6651 0.7576 0.6795 0.6689 0.8111 0.8263 0.8571 0.4389 0.5933 0.7725 0.8119 0. 0.7488 0.8576 0.7977 0.6911 0.9481 0.9500 0.9657 0.5178 0.9089 0.9100 0.9095 0.9455 0.8814 0.9455 0.9136 0.7311 0.9852 0.9816 0.9657 0.8378 0.8889 0.8500 0.8976 0. 0.8605 0.8939 0.8568 0.6844 0.9111 0.9394 0.9343 0.8778 0.9178 0.9400 0.9381 0.9500 0.9186 0.9303 0.9364 0.7333 0.9519 0.9737 0.9629 0.164 0.070 0.110 0.176 0. - - - - - 0.262 - and video continuation tasks. Notably, we highlight the models strength in handling (2) human object interactions, where accurate alignment between visual content and textual instruction is essential to eliminating visual hallucination. For example, in the task pick up an object and move it to target location, Plan-X leverages spatial reasoning and grounding from the provided image to ensure semantic and visual consistency, whereas prior approaches often hallucinate new or misplaced objects due to insufficient coupling between textual intent and visual context. We further demonstrate the superiority of our semantic planner in (3) long-range, multi-action planning, where complex, temporally ordered actions are executed smoothly and coherently, unlike prior approaches that frequently omit actions or generate them in incorrect order. Lastly, we demonstrate side application of our framework in (4) semantics transfer, where the generated semantic tokens can be seamlessly applied to different initial frames, since they encode high-level semantics and are injected into the DiT as contextual guidance rather than spatially aligned offsets. Visual comparisons between Plan-X and baseline methods are presented in Figure 4, highlighting clear advantages in prompt alignment and semantic consistency. Quantitative Evaluation. There are currently no reliable numerical metrics for assessing interaction accuracy, prompt alignment, or motion fidelity in complex compositional video generation. To address this, we leverage the state-of-the-art MLLM Gemini 2.5 [4] to score model outputs on normalized scale from 0 (worst) to 1 (best). Specifically, we evaluate humanobject interaction accuracy (Accuracy) and fidelity (Fidelity), temporal motion completeness (Completeness), scene consistency (Consistency), motion naturalness (Naturalness), and overall visual quality (Visual). The full set of evaluation prompts used for MLLM scoring is provided in the supplementary materials. In addition, we conduct human preference study with 16 participants, who are asked to select the two best videos between Plan-X-Wan and each competing baseline based on overall instruction alignment. As shown in Table 1, Plan-X achieves the highest performance on nearly all MLLM-evaluated dimensions. Compared to strong baselines such as Seedance 1.0 and Kling 1.6, our method yields substantial gains in compositional alignment, with Accuracy increasing from 0.7114 (Seedance) to 0.7971 and Completeness from 0.7943 to 0.8571. Moreover, Plan-X exhibits minimal visual hallucination (Fidelity, Consistency), outperforming or matching the strongest existing models in both dimensions. In terms of visual stability, Plan-X-Wan matches or surpasses all baselines, reaching the top Visual score of 0.9737 and the highest Naturalness score of 0.9394. Further, Plan-X-Wan shows strong improvements over its pretrained backbone (Wan 2.2-5B), outperforming it by +0.4660 Accuracy, +0.4296 Completeness, and +0.5111 Fidelity, demonstrating the effectiveness of our proposed components. The superiority of Plan-X is corroborated by human preference results, where Plan-X-Wan achieves the highest preference score of 0.262, outperforming all competing methods, including the previously strongest baseline Seedance 1.0 (0.218). These results confirm that Plan-X produces videos that are not only more aligned to the instructions but also perceptually more coherent and natural to human viewers. Ablations. To assess the impact of our semantic planner, we perform ablation studies using the same diffusion backbone, Wan 2.2-5B, and compare against three alternative variants: (1) SFT, where the DiT model is directly finetuned on our training corpus for the same number of epochs, without any semantic planning; (2) Prompt Enhancement, where built-in prompt enhancer is used to rewrite user instructions before generation; and (3) Query-style conditioning [27], in which Qwen2.5-VL-3B [1] is frozen as an MLLM backbone and learnable 256-token query, along with 24-layer transformer connector, is trained to provide Figure 4. Qualitative comparison and ablation. Enabled by our semantic planner, both Plan-X-Wan and Plan-X-Seedance jointly reason over the provided visual content and the users text instruction, producing actions that accurately follow the intended semantics. In contrast, baseline methods frequently deviate from the instruction, exhibiting visual hallucination (SkyReelsV2, HunyuanVideo), interacting with misaligned objects or incorrect spatial locations (Seedance, Wan2.2), or producing severe object misplacement (Wan2.2, Wan2.2-SFT, Kling). Ablated variants that replace our semantic planner with text-based prompt enhancement or query-based conditioning also suffer from prompt misalignment. additional conditioning to the DiT. As evidenced by the quantitative results in Table 1 and the qualitative comparisons in Figure 4 as well as the supplementary material, our proposed semantic planner substantially outperforms all ablated variants, yielding significantly stronger prompt alignment while avoiding visual hallucinations. These results demonstrate that explicit semantic planning offers more structured and interpretable guidance than both text-only and query-based conditionings. We further ablate the effectiveness of 3D RoPE, which establishes spatio-temporal correspondence between semantic tokens and video latents, thereby improving prompt alignment. Lastly, we ablate the impact of both the semantic planner model scale and the choice of pretrained DiT backbone. Using the same training corpus, we train semantic planners with 1.5B and 7B parameters, where stronger semantic planning capability emerges at the larger model scale. Our framework represents general generative paradigm, and the semantic planner is compatible with diverse video diffusion architectures Wan2.2, which employs cross-attention for text conditioning, and Seedance 1.0, which adopts an MMDiT architecture. As shown in Table 1, our semantic planner consistently improves the prompt-following capability of each base DiT model without compromising pretrained visual fidelity. Additional ablation studies are provided in the supplementary material. 5. Conclusion We introduced Plan-X, framework that decouples highlevel semantic planning from low-level video synthesis. Semantic Planner, implemented as multimodal language model, interprets user instructions and visual context into spatio-temporal semantic tokens, which guide DiT model for high-fidelity video generation. This separation of reasoning and rendering effectively bridges instruction understanding and visual synthesis, reducing hallucination and improving prompt alignment, semantic coherence, and controllability across diverse video generation tasks. PlanX highlights new video generation paradigm where language models act as interpretable planners, enabling diffusion transformers to generate videos that are both semantically grounded and visually precise. Future work. While this work focuses primarily on video generation, our framework is inherently extensible to video understanding and editing within unified architecture, which we plan to explore in future research. Currently, we leverage pretrained discrete semantic tokenizer from TA-Tok [12], which exhibits limited expressiveness in representing complex concepts and abstract reasoning (such as mathematical or symbolic content). In future work, we aim to develop more expressive text-aligned visual semantic tokenizers, enabling richer multimodal understanding and 8 video synthesis. Societal Impact. Our work focuses on improving the video generation in technical aspects and is not specifically designed for any malicious uses. This being the method could be potentially said, we do see that extended into controversial applications such as generating fake videos. the synthesized videos should present themselves as synthetic. Therefore, we believe that"
        },
        {
            "title": "References",
            "content": "[1] Shuai Bai, Keqin Chen, Xuejing Liu, Jialin Wang, Wenbin Ge, Sibo Song, Kai Dang, Peng Wang, Shijie Wang, Jun Tang, Humen Zhong, Yuanzhi Zhu, Mingkun Yang, Zhaohai Li, Jianqiang Wan, Pengfei Wang, Wei Ding, Zheren Fu, Yiheng Xu, Jiabo Ye, Xi Zhang, Tianbao Xie, Zesen Cheng, Hang Zhang, Zhibo Yang, Haiyang Xu, and JunarXiv preprint yang Lin. Qwen2.5-vl technical report. arXiv:2502.13923, 2025. 6, 7 [2] Jiuhai Chen, Zhiyang Xu, Xichen Pan, Yushi Hu, Can Qin, Tom Goldstein, Lifu Huang, Tianyi Zhou, Saining Xie, Silvio Savarese, Le Xue, Caiming Xiong, and Ran Xu. Blip3o: family of fully open unified multimodal modelsarchitecture, training and dataset, 2025. 3, 4 [3] Jiuhai Chen, Le Xue, Zhiyang Xu, Xichen Pan, Shusheng Yang, Can Qin, An Yan, Honglu Zhou, Zeyuan Chen, Lifu Huang, Tianyi Zhou, Junnan Li, Silvio Savarese, Caiming Xiong, and Ran Xu. Blip3o-next: Next frontier of native image generation, 2025. 3, 1 [4] Gheorghe Comanici, Eric Bieber, Mike Schaekermann, Ice Pasupat, Noveen Sachdeva, Inderjit Dhillon, Marcel Blistein, Ori Ram, Dan Zhang, Evan Rosen, et al. Gemini 2.5: Pushing the frontier with advanced reasoning, multimodality, long context, and next generation agentic capabilities. arXiv preprint arXiv:2507.06261, 2025. 3, 7, 1 [5] Chaorui Deng, Deyao Zhu, Kunchang Li, Chenhui Gou, Feng Li, Zeyu Wang, Shu Zhong, Weihao Yu, Xiaonan Nie, Ziang Song, et al. Emerging properties in unified multimodal pretraining. arXiv preprint arXiv:2505.14683, 2025. 3 [6] Patrick Esser, Sumith Kulal, Andreas Blattmann, Rahim Entezari, Jonas Muller, Harry Saini, Yam Levi, Dominik Lorenz, Axel Sauer, Frederic Boesel, et al. Scaling rectified flow transformers for high-resolution image synthesis. In Forty-first international conference on machine learning, 2024. 2 [7] Yu Gao, Haoyuan Guo, Tuyen Hoang, Weilin Huang, Lu Jiang, Fangyuan Kong, Huixia Li, Jiashi Li, Liang Li, Xiaojie Li, et al. Seedance 1.0: Exploring the boundaries of video generation models. arXiv preprint arXiv:2506.09113, 2025. 2, 3, 6, 7, [8] Yuying Ge, Sijie Zhao, Jinguo Zhu, Yixiao Ge, Kun Yi, Lin Song, Chen Li, Xiaohan Ding, and Ying Shan. Seed-x: Multimodal models with unified multi-granularity comprehension and generation. arXiv preprint arXiv:2404.14396, 2024. 3, 4 [9] Google DeepMind. Veo 3. https : / / deepmind . google/models/veo/, 2025. Accessed: 2025-09-22. 2 [10] Daya Guo, Dejian Yang, Haowei Zhang, Junxiao Song, Ruoyu Zhang, Runxin Xu, Qihao Zhu, Shirong Ma, Peiyi Wang, Xiao Bi, et al. Deepseek-r1: Incentivizing reasoning capability in llms via reinforcement learning. arXiv preprint arXiv:2501.12948, 2025. 3 [11] Yuwei Guo, Ceyuan Yang, Anyi Rao, Yaohui Wang, Yu Qiao, Dahua Lin, and Bo Dai. Animatediff: Animate your personalized text-to-image diffusion models without specific tuning. arXiv preprint arXiv:2307.04725, 2023. 2 [12] Jiaming Han, Hao Chen, Yang Zhao, Hanyu Wang, Qi Zhao, Ziyan Yang, Hao He, Xiangyu Yue, and Lu Jiang. Vision as dialect: Unifying visual understanding and genarXiv preprint eration via text-aligned representations. arXiv:2506.18898, 2025. 2, 3, 4, 8, [13] Jonathan Ho, Tim Salimans, Alexey Gritsenko, William Chan, Mohammad Norouzi, and David J. Fleet. Video diffusion models, 2022. 2 [14] Li Hu. Animate anyone: Consistent and controllable imageto-video synthesis for character animation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 81538163, 2024. 5 [15] Ziqi Huang, Yinan He, Jiashuo Yu, Fan Zhang, Chenyang Si, Yuming Jiang, Yuanhan Zhang, Tianxing Wu, Qingyang Jin, Nattapol Chanpaisit, Yaohui Wang, Xinyuan Chen, Limin Wang, Dahua Lin, Yu Qiao, and Ziwei Liu. VBench: Comprehensive benchmark suite for video generative models. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 2024. 6 [16] Adam Tauman Kalai, Ofir Nachum, Santosh S. Vempala, and Edwin Zhang. Why language models hallucinate. arXiv preprint arXiv:2509.04664, 2025. 4 [17] Dongwon Kim, Ju He, Qihang Yu, Chenglin Yang, Xiaohui Shen, Suha Kwak, and Liang-Chieh Chen. Democratizing text-to-image masked generative models with comarXiv preprint pact text-aware one-dimensional tokens. arXiv:2501.07730, 2025. 3 [18] Weijie Kong, Qi Tian, Zijian Zhang, Rox Min, Zuozhuo Dai, Jin Zhou, Jiangfeng Xiong, Xin Li, Bo Wu, Jianwei Zhang, et al. Hunyuanvideo: systematic framework for large video generative models. arXiv preprint arXiv:2412.03603, 2024. 2, 3, 6, [19] Kuaishou. Kling ai. https://klingai.kuaishou. com/, 2024. Accessed: 2025-05-19. 2, 6, 7 [20] Chao Liao, Liyang Liu, Xun Wang, Zhengxiong Luo, Xinyu Zhang, Wenliang Zhao, Jie Wu, Liang Li, Zhi Tian, and Weilin Huang. Mogao: An omni foundation model arXiv preprint for interleaved multi-modal generation. arXiv:2505.05472, 2025. 3 [21] Han Lin, Jaemin Cho, Amir Zadeh, Chuan Li, and Mohit Bansal. Bifrost-1: Bridging multimodal llms and diffusion models with patch-level clip latents. arXiv preprint arXiv:2508.05954, 2025. 3 [22] Kun Liu, Qi Liu, Xinchen Liu, Jie Li, Yongdong Zhang, Jiebo Luo, Xiaodong He, and Wu Liu. Hoigen-1m: largescale dataset for human-object interaction video generation. In CVPR, 2025. 6 [23] Yiyang Ma, Xingchao Liu, Xiaokang Chen, Wen Liu, Chengyue Wu, Zhiyu Wu, Zizheng Pan, Zhenda Xie, Haowei Zhang, Xingkai Yu, et al. Janusflow: Harmonizing autoregression and rectified flow for unified multimodal understanding and generation. In Proceedings of the Computer Vision and Pattern Recognition Conference, pages 77397751, 2025. 3 [24] OpenAI. from text. Creating video Sora: https://openai.com/sora/, 2024. 2 [25] OpenAI. Gpt-5, 2025. 3 [26] Maxime Oquab, Timothee Darcet, Theo Moutakanni, Huy V. Vo, Marc Szafraniec, Vasil Khalidov, Pierre Fernandez, Daniel Haziza, Francisco Massa, Alaaeldin El-Nouby, Russell Howes, Po-Yao Huang, Hu Xu, Vasu Sharma, ShangWen Li, Wojciech Galuba, Mike Rabbat, Mido Assran, Nicolas Ballas, Gabriel Synnaeve, Ishan Misra, Herve Jegou, Julien Mairal, Patrick Labatut, Armand Joulin, and Piotr Bojanowski. Dinov2: Learning robust visual features without supervision, 2023. 4 [27] Xichen Pan, Satya Narayan Shukla, Aashu Singh, Zhuokai Zhao, Shlok Kumar Mishra, Jialiang Wang, Zhiyang Xu, Jiuhai Chen, Kunpeng Li, Felix Juefei-Xu, Ji Hou, and Saining Xie. Transfer between modalities with metaqueries. arXiv preprint arXiv:2504.06256, 2025. 3, 4, 7 [28] William Peebles and Saining Xie. Scalable diffusion models with transformers, 2023. 3 [29] William Peebles and Saining Xie. Scalable diffusion models with transformers, 2023. 2 [30] Di Qiu, Zhengcong Fei, Rui Wang, Jialin Bai, Changqian Yu, Mingyuan Fan, Guibin Chen, and Xiang Wen. Skyreels-a1: Expressive portrait animation in video diffusion transformers. arXiv preprint arXiv:2502.10841, 2025. 2, 6, 7 [31] Liao Qu, Huichao Zhang, Yiheng Liu, Xu Wang, Yi Jiang, Yiming Gao, Hu Ye, Daniel Du, Zehuan Yuan, and Xinglong Wu. Tokenflow: Unified image tokenizer for multimodal understanding and generation. In Proceedings of the Computer Vision and Pattern Recognition Conference, pages 25452555, 2025. 3 [32] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, Gretchen Krueger, and Ilya Sutskever. Learning transferable visual models from natural language supervision, 2021. 3, 4 [33] Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, and Peter J. Liu. Exploring the limits of transfer learning with unified text-to-text transformer, 2023. 3 [34] Olaf Ronneberger, Philipp Fischer, and Thomas Brox. U-net: Convolutional networks for biomedical image segmentation. CoRR, abs/1505.04597, 2015. 2 [35] Team Seawead, Ceyuan Yang, Zhijie Lin, Yang Zhao, Shanchuan Lin, Zhibei Ma, Haoyuan Guo, Hao Chen, Lu Qi, Sen Wang, et al. Seaweed-7b: Cost-effective training of video generation foundation model. arXiv preprint arXiv:2504.08685, 2025. [36] Minglei Shi, Haolin Wang, Wenzhao Zheng, Ziyang Yuan, Xiaoshi Wu, Xintao Wang, Pengfei Wan, Jie Zhou, and Jiwen Lu. Latent diffusion model without variational autoencoder, 2025. 4 [37] Joonghyuk Shin, Alchan Hwang, Yujin Kim, Daneul Kim, and Jaesik Park. Exploring multimodal diffusion transformers for enhanced prompt-based image editing, 2025. 2 [38] Quan Sun, Yufeng Cui, Xiaosong Zhang, Fan Zhang, Qiying Yu, Zhengxiong Luo, Yueze Wang, Yongming Rao, Jingjing Liu, Tiejun Huang, and Xinlong Wang. Generative multimodal models are in-context learners, 2024. 3 [39] Chameleon Team. fusion foundation models, 2024. org/abs/2405.09818, 9(8), 2024. 3 Chameleon: Mixed-modal earlyURL https://arxiv. [40] Genmo Team. Mochi 1. https://github.com/ genmoai/models, 2024. 2 [41] Qwen Team et al. Qwen2 technical report. arXiv preprint arXiv:2407.10671, 2(3), 2024. 3 [42] Wan Team. Wan: Open and advanced large-scale video generative models. 2025. 2 [43] Shengbang Tong, David Fan, Jiachen Zhu, Yunyang Xiong, Xinlei Chen, Koustuv Sinha, Michael Rabbat, Yann LeCun, Saining Xie, and Zhuang Liu. Metamorph: Multimodal understanding and generation via instruction tuning. arXiv preprint arXiv:2412.14164, 2024. [44] Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timothee Lacroix, Baptiste Rozi`ere, Naman Goyal, Eric Hambro, Faisal Azhar, et al. Llama: Open and efficient foundation language models. arXiv preprint arXiv:2302.13971, 2023. 3 [45] Michael Tschannen, Alexey Gritsenko, Xiao Wang, MuhamIbrahim Alabdulmohsin, Nikhil mad Ferjad Naeem, Parthasarathy, Talfan Evans, Lucas Beyer, Ye Xia, Basil Mustafa, Olivier Henaff, Jeremiah Harmsen, Andreas Steiner, and Xiaohua Zhai. Siglip 2: Multilingual visionlanguage encoders with improved semantic understanding, localization, and dense features, 2025. 2, 3, 4 [46] Team Wan, Ang Wang, Baole Ai, Bin Wen, Chaojie Mao, Chen-Wei Xie, Di Chen, Feiwu Yu, Haiming Zhao, Jianxiao Yang, et al. Wan: Open and advanced large-scale video generative models. arXiv preprint arXiv:2503.20314, 2025. 2, 3, 6, 7, 1 [47] Bohan Wang, Zhongqi Yue, Fengda Zhang, Shuo Chen, Lian Bi, Junzhe Zhang, Xue Song, Kennard Yanting Chan, Jiachun Pan, Weijia Wu, et al. Selftok: Discrete visual tokens of autoregression, by diffusion, and for reasoning. arXiv preprint arXiv:2505.07538, 2025. 3 [48] Xinlong Wang, Xiaosong Zhang, Zhengxiong Luo, Quan Sun, Yufeng Cui, Jinsheng Wang, Fan Zhang, Yueze Wang, Zhen Li, Qiying Yu, et al. Emu3: Next-token prediction is all you need. arXiv preprint arXiv:2409.18869, 2024. 3 [49] Chenfei Wu, Jiahao Li, Jingren Zhou, Junyang Lin, Kaiyuan Gao, Kun Yan, Sheng-ming Yin, Shuai Bai, Xiao Xu, Yilei Chen, et al. Qwen-image technical report. arXiv preprint arXiv:2508.02324, 2025. 3 [50] Yecheng Wu, Zhuoyang Zhang, Junyu Chen, Haotian Tang, Dacheng Li, Yunhao Fang, Ligeng Zhu, Enze Xie, Hongxu 10 Yin, Li Yi, et al. Vila-u: unified foundation model integrating visual understanding and generation. arXiv preprint arXiv:2409.04429, 2024. 3 [51] Jinheng Xie, Weijia Mao, Zechen Bai, David Junhao Zhang, Weihao Wang, Kevin Qinghong Lin, Yuchao Gu, Zhijie Chen, Zhenheng Yang, and Mike Zheng Shou. Show-o: One single transformer to unify multimodal understanding and generation. arXiv preprint arXiv:2408.12528, 2024. 3 [52] You Xie, Tianpei Gu, Zenan Li, Chenxu Zhang, Guoxian Song, Xiaochen Zhao, Chao Liang, Jianwen Jiang, Hongyi Xu, and Linjie Luo. X-streamer: Unified human world modeling with audiovisual interaction, 2025. 3 [53] Qwen An Yang, Baosong Yang, Beichen Zhang, Binyuan Hui, Bo Zheng, Bowen Yu, Chengyuan Li, Dayiheng Liu, Fei Huang, Guanting Dong, Haoran Wei, Huan Lin, Jian Yang, Jianhong Tu, Jianwei Zhang, Jianxin Yang, Jiaxin Yang, Jingren Zhou, Junyang Lin, Kai Dang, Keming Lu, Keqin Bao, Kexin Yang, Le Yu, Mei Li, Mingfeng Xue, Pei Zhang, Qin Zhu, Rui Men, Runji Lin, Tianhao Li, Tingyu Xia, Xingzhang Ren, Xuancheng Ren, Yang Fan, Yang Su, Yi-Chao Zhang, Yunyang Wan, Yuqi Liu, Zeyu Cui, Zhenru Zhang, Zihan Qiu, Shanghaoran Quan, and Zekun Wang. Qwen2.5 technical report. ArXiv, abs/2412.15115, 2024. 3, 4, 6 [54] Jingfeng Yao, Bin Yang, and Xinggang Wang. Reconstruction vs. generation: Taming optimization dilemma in latent diffusion models. In Proceedings of the Computer Vision and Pattern Recognition Conference, pages 1570315712, 2025. [55] Lvmin Zhang, Anyi Rao, and Maneesh Agrawala. Adding conditional control to text-to-image diffusion models, 2023. 5 [56] Hongxiang Zhao, Xingchen Liu, Mutian Xu, Yiming Hao, Weikai Chen, and Xiaoguang Han. Taste-rob: Advancing video generation of task-oriented hand-object interaction for generalizable robotic manipulation. In Proceedings of the Computer Vision and Pattern Recognition Conference (CVPR), pages 2768327693, 2025. 6 [57] Boyang Zheng, Nanye Ma, Shengbang Tong, and Saining Xie. Diffusion transformers with representation autoencoders. arXiv preprint arXiv:2510.11690, 2025. 3, 4 [58] Chunting Zhou, Lili Yu, Arun Babu, Kushal Tirumala, Michihiro Yasunaga, Leonid Shamis, Jacob Kahn, Xuezhe Ma, Luke Zettlemoyer, and Omer Levy. Transfusion: Predict the next token and diffuse images with one multi-modal model. arXiv preprint arXiv:2408.11039, 2024. 3 11 Plan-X: Instruct Video Generation via Semantic Planning"
        },
        {
            "title": "Supplementary Material",
            "content": "In the supplementary material, we provide additional implementation training details (Section 6). In Section 7, we further elaborate on our MLLM-based evaluation and user study setup. We also ablate the impact of end-to-end training in Section 8, and present representative failure cases to illustrate the current limitations of our framework (Section 9). 6. More Implementation Details 6.1. Semantic Planner Training We format each training clip as conversation, as illustrated in Figure 6. For frame representation, we use TA-Tok [12] tokens, which are trained at three adaptive spatial scales with token counts {729, 169, 81} corresponding to scales {0, 1, 2} from fine to coarse. After applying the chat template, we replace the placeholder <image> token in the user prompt with 729 TA-Tok tokens representing the initial frame, preserving fine-grained perceptual details. In the assistant response, each <image> placeholder token is replaced by 81 TA-Tok tokens at the coarsest pooling scale. The special tokens <im start> and <im end> mark the boundaries of each image token sequence, while <S2> indicates the pooling scale and denotes the number of target video frames. During training, we compute the cross-entropy loss over the TA-Tok tokens in the assistants response. To enable hybrid text-to-video (T2V) and image-tovideo (I2V) capabilities, we randomly zero out the TA-Tok embeddings in the user prompt with probability 50%. 6.2. Semantics Guidance in DiT To inject semantic guidance into the DiT model, we replicate the original text branch into semantic branch. Concretely, we introduce new key and value projection layers for the semantic tokens, initialized from the corresponding text projection weights. For Seedance [7] using MMDiT architectures, we directly append the keys Ksem and values Vsem derived from the semantic tokens to the existing text keys Ktext and values Vtext: mantic tokens separately. Let denote the video query features. We first compute Ltext = QK text , Lsem = K sem (6) We then concatenate these logits and apply the softmax function to compute the final attention output: Attention = softmax([Ltext Lsem])[Vtext Vsem]. (7) 6.3. End-to-End Training. In the final stage, we jointly fine-tune both the semantic planner and the DiT model in an end-to-end manner. Similar to BLIP3o-NEXT [3], instead of generating discrete tokens and querying the codebook embeddings for DiT conditioning, we directly project and inject the last hidden states (before the semantic planners linear prediction head) as the DiT semantic conditioning. This design enables stable training with differentiable gradients and mitigates representation collapse caused by quantization. For optimization, we use weighted combination of the diffusion loss and the TA-Tok prediction loss, with weights of 1.0 and 0.1, respectively. 7. Evaluation MLLM Evaluation Prompts. We utilize the SOTA multimodal LLM, Gemini-2.5 [4], to evaluate the generated videos in multiple dimensions. The detailed system prompt is provided in Figure 8. Human Evaluation Settings. We randomly select 20 samples from our test set for each model variant. Each video is evaluated by 16 independent human raters. We ask the raters to choose the top two videos based on overall quality and prompt alignment. We then count how many times each result is selected as one of the top two and report these counts in Table 1 of the main paper. Finally, we normalize the counts by the total number of selections to obtain the reported percentages. 8. Additional Ablations = [Ktext Ksem], = [Vtext Vsem], (5) where denotes concatenation along the sequence dimension. For Wan2.2 [46], which uses cross-attention for text conditioning and does not apply RoPE in the original text crossattention, we compute the attention logits for text and seWe conduct additional ablations to study the impact of both end-to-end joint training and text conditioning in the DiT. As illustrated in Figure 5, end-to-end training improves prompt-following behavior, while retaining the textconditioning branch in the DiT helps preserve the pretrained visual quality. These trends are further confirmed quantitatively in Table 2. 1 Figure 5. Additional qualitative ablation. Removing text conditioning in the DiT impairs visual quality (e.g., introducing oilpaintinglike textures) and temporal consistency, whereas training Plan-X without joint end-to-end fine-tuning reduces robustness to semantic planning noise (for example, white peeler incorrectly appears next to the white bowl holding the chopsticks in the second example)."
        },
        {
            "title": "Semantic Planner Training Data Format",
            "content": "conversation = [ { \"role\": \"system\", \"content\": \"You are helpful assistant.\" }, { \"role\": \"user\", \"content\": f\"Given the initial frame description: {image_description}, \" + f\"generate the subsequent {F-1} frames of the initial frame \" + f\"based on the instruction: {video_prompt}.n<image>\" }, { \"role\": \"assistant\", \"content\": \"<im_start><S2><image><im_end>\"*(F-1) } ] Figure 6. Semantic Planner Training Data Format. 9. Limitations In Figure 7, we present several representative failure cases. Since our framework relies on pretrained DiT model for visual realization, it can still exhibit artifacts in physical fidelity and visual consistency. Additionally, our semantic planner is trained on approximately 4.5M textvideo pairs only, and thus lacks strong abstract reasoning and commonsense intelligence, as illustrated in the last row. 2 Figure 7. Failure cases. Top row: the spoon undergoes non-rigid morphing and eventually pops out of the coffee cup. Bottom left: the red paint on the brush disappears. Bottom right: the flag and the number of children do not match the prompt description. Table 2. Additional quantitative ablation. We denote the best results in bold."
        },
        {
            "title": "Method",
            "content": "Accuracy Completeness Fidelity Consistency Naturalness Visual Plan-X-Wan w/o text Plan-X-Wan w/o e2e Plan-X-Wan 0.6533 0.7256 0.7816 0.7044 0.7535 0. 0.8133 0.8977 0.9500 0.8622 0.9767 0.9816 0.8444 0.9023 0. 0.9156 0.9682 0."
        },
        {
            "title": "Gemini Evaluation System Prompt",
            "content": "You are an expert video evaluator. Your task is to analyze generated video based on text prompt and an initial reference image. Please evaluate the video on the following aspects. For each metric, provide score between 0.0 and 1.0 and brief \"analysis\" (a one-sentence justification for your score). **Prompt:** \"{prompt}\" **Initial Image:** [The image provided as the first frame] --- ## A. Prompt & Context Alignment (How well the video matches the prompt and the initial image) 1. **Interaction Fidelity** (0.0, 0.1,...,1.0): * If the prompt describes an interaction, how accurately is it depicted? * **Analyze 3 components:** 1. **Interacting Objects:** Does the video show the *correct objects* (from the image) performing the interaction? 2. **Interaction Movement:** Is the *specific action* or movement itself correct (e.g., \"pushing\" vs. \"lifting\")? 3. **Interaction Outcome:** Is the *final state* or result of the interaction correct (e.g., \"the vase falls off\")? * 1.0 = Perfectly accurate. All 3 components (objects, movement, outcome) are correct. **(Or no interaction was described in * **Rating:** the prompt).** * ... * 0.5 = Partially correct (e.g., correct objects but wrong movement/outcome). * ... * 0.0 = Fundamentally incorrect (e.g., wrong objects interacting or completely different action). 2. **Prompt Completeness (Recall)** (0.0, 0.1,...,1.0): * Of all motions described *in the prompt*, what percentage are successfully completed *in the video*? * 1.0 = All prompted motions are fully completed. * ... * 0.0 = No prompted motions are completed. 3. **Motion Fidelity (Precision)** (0.0, 0.1,...,1.0): * Of all motions observed *in the video*, what percentage were *also described* in the prompt? (This measures motion hallucination). * 1.0 = No unprompted/hallucinated motion. * ... * 0.0 = All motion was unprompted/hallucinated. 4. **Object Consistency** (0.0, 0.1,...,1.0): corrupted/changed? * **Appearance:** Do objects from the initial image maintain consistent appearance (shape, color, texture), or do they get * **Persistence:** Do all significant objects from the initial image remain present, and does the video avoid introducing new, duplicated objects excepts for hands performing the prompted actions? * 1.0 = Perfectly consistent: objects maintain their appearance, no objects disappear, and no new objects are introduced. * ... * 0.0 = Severely inconsistent: objects are corrupted, disappear, or new ones appear. ## B. Technical Quality (The overall quality of the video file and its motion) 1. **Motion Naturalness** (0.0, 0.1,...,1.0): * How natural and realistic are the movements in the video? * Considers if motions are jittery, unnaturally fast/slow, or physically implausible. * 1.0 = Completely natural, realistic motion. * ... * 0.0 = Completely unnatural, broken, or incoherent motion. 2. **Visual Quality** (0.0, 0.1,...,1.0): * Visual quality including resolution, clarity, artifacts, stability. * Considers compression artifacts, blurriness, distortions, frame consistency. * 1.0 = High visual quality, clear and stable. * ... * 0.0 = Poor visual quality with major artifacts. --- Please respond in this exact JSON format. Do not include any other text before or after the JSON block: {{ \"prompt_context_alignment\": {{ \"interaction_fidelity\": {{ \"analysis\": \"Brief justification for interaction accuracy score.\", \"score\": X.X }}, \"prompt_completeness_recall\": {{ \"analysis\": \"Brief justification for prompt completeness score.\", \"score\": X.X }}, \"motion_fidelity_precision\": {{ \"analysis\": \"Brief justification for motion fidelity (hallucination) score.\", \"score\": X.X }}, \"object_consistency\": {{ \"analysis\": \"Brief justification for object consistency score.\", \"score\": X.X }} }}, \"technical_quality\": {{ \"motion_naturalness\": {{ \"analysis\": \"Brief justification for motion naturalness score.\", \"score\": X.X }}, \"visual_quality\": {{ \"analysis\": \"Brief justification for visual quality score.\", \"score\": X.X }} }} }} Figure 8. The system prompt used for Gemini-based evaluation."
        }
    ],
    "affiliations": [
        "Apple",
        "ByteDance Intelligent Creation",
        "Duke University",
        "Princeton University"
    ]
}