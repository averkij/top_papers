{
    "paper_title": "MotionStreamer: Streaming Motion Generation via Diffusion-based Autoregressive Model in Causal Latent Space",
    "authors": [
        "Lixing Xiao",
        "Shunlin Lu",
        "Huaijin Pi",
        "Ke Fan",
        "Liang Pan",
        "Yueer Zhou",
        "Ziyong Feng",
        "Xiaowei Zhou",
        "Sida Peng",
        "Jingbo Wang"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "This paper addresses the challenge of text-conditioned streaming motion generation, which requires us to predict the next-step human pose based on variable-length historical motions and incoming texts. Existing methods struggle to achieve streaming motion generation, e.g., diffusion models are constrained by pre-defined motion lengths, while GPT-based methods suffer from delayed response and error accumulation problem due to discretized non-causal tokenization. To solve these problems, we propose MotionStreamer, a novel framework that incorporates a continuous causal latent space into a probabilistic autoregressive model. The continuous latents mitigate information loss caused by discretization and effectively reduce error accumulation during long-term autoregressive generation. In addition, by establishing temporal causal dependencies between current and historical motion latents, our model fully utilizes the available information to achieve accurate online motion decoding. Experiments show that our method outperforms existing approaches while offering more applications, including multi-round generation, long-term generation, and dynamic motion composition. Project Page: https://zju3dv.github.io/MotionStreamer/"
        },
        {
            "title": "Start",
            "content": "5 2 0 2 9 1 ] . [ 1 1 5 4 5 1 . 3 0 5 2 : r MotionStreamer: Streaming Motion Generation via Diffusion-based Autoregressive Model in Causal Latent Space Lixing Xiao1 Shunlin Lu2 Huaijin Pi3 Ke Fan4 Liang Pan3 Yueer Zhou1 Ziyong Feng5 Xiaowei Zhou Sida Peng1 Jingbo Wang6 1Zhejiang University 2The Chinese University of Hong Kong, Shenzhen 3The University of Hong Kong 5DeepGlint 4Shanghai Jiao Tong University 6Shanghai AI Laboratory lixingxiao0@gmail.com, pengsida@zju.edu.cn Figure 1. Visualization of streaming motion generation process. Texts are incrementally inputted and motions are generated online."
        },
        {
            "title": "Abstract",
            "content": "This paper addresses the challenge of text-conditioned streaming motion generation, which requires us to predict the next-step human pose based on variable-length historical motions and incoming texts. Existing methods struggle to achieve streaming motion generation, e.g., diffusion models are constrained by pre-defined motion lengths, while GPT-based methods suffer from delayed response and error accumulation problem due to discretized noncausal tokenization. To solve these problems, we propose MotionStreamer, novel framework that incorporates continuous causal latent space into probabilistic autoregressive model. The continuous latents mitigate information loss caused by discretization and effectively reduce error accumulation during long-term autoregressive generation. In addition, by establishing temporal causal dependencies between current and historical motion latents, our model fully utilizes the available information to achieve accurate online motion decoding. Experiments show that our method outperforms existing approaches while offering more applications, including multiCorresponding author round generation, long-term generation, and dynamic motion composition. Project Page: https://zju3dv. github.io/MotionStreamer/ 1. Introduction Streaming motion generation aims to incrementally synthesizing human motions while dynamically adapting to online text inputs and maintaining semantic coherence. Generating realistic and diverse human motions in streaming manner is essential for various real-time applications, such as video games, animation, and robotics. Streaming motion generation presents significant challenge due to two fundamental requirements. Firstly, the framework must incrementally process sequentially arriving textual inputs while maintaining online response. Secondly, the model should be able to continuously synthesize motion sequences that exhibit contextual consistency by effectively integrating historical information with incoming textual conditions, ensuring alignment between progressive text semantics and kinematic continuity across extended timelines. Conventional diffusion-based motion generation models [4, 9, 53] are constrained by their non-incremental generation paradigm with static text conditioning and fixed-length generation processes. This inherently limits their ability to dynamically evolving textual inputs in streaming scenarios. Other autoregressive motion generation frameworks [26, 60] are able to generate motions in streaming manner. However, they have difficulties in achieving online response due to their non-causal tokenization architecture, which prevents partial token decoding until all sequence is generated. Real-time motion generation methods like DART [63] face critical limitation in their reliance on fixed-window local motion primitives, which inherently restricts their capacity to model variable-length historical contexts and dynamically align with evolving textual inputs. In this work, we propose novel framework for streaming motion generation, named MotionStreamer. The visualization of the streaming generation process is illustrated in Fig. 1. Our core innovation is incorporating diffusion head into an autoregressive model to predict the next motion latent, while introducing causal motion compressor to enable online decoding in streaming manner. Specifically, given an input text, we extract the textual feature, combine it with historical motion latents, and use an autoregressive model to generate condition feature, which guides diffusion model to generate the next motion latent. In contrast to previous methods [26, 60] that use vector quantization (VQ) based motion tokenizer and GPT architecture to generate discrete motion tokens, our continuous motion latents can avoid information loss of discrete tokens and accumulation of error during the streaming generation process, as demonstrated by our experimental results in Sec. 4.2. causal temporal AutoEncoder is then employed to convert motion latent into the next human pose. The causal network effectively establishes temporally causal dependencies between current and historical motion latents, allowing for online motion decoding. The key to achieving streaming generation is enabling the model to dynamically extract relevant information from variable-length history to guide the next motion prediction. To enable the autoregressive model to self-terminate without pre-defined sequence length, we additionally encode an impossible pose to get reference end latent as the continuous stopping condition. During experiments, we found that naive training of our model still suffers from error accumulation and cannot well support multi-round text input. To address these issues, we propose two training strategies: Two-forward training and Mixed training. Two-forward training strategy first generates motion latents using ground-truth, then replaces partial ground-truth latents with first-forward predictions for hybrid second-forward, effectively mitigating the exposure bias inherent in autoregressive training while preserving parallel efficiency. Mixed training strategy unifies atomic (text, motion) pairs and contextual (text, history motion, current motion) triplets in single framework, enabling compositional semantics learning and generalization to unseen motion combinations. We evaluate our approach on the HumanML3D [17] and BABEL [43] datasets, which are widely-used for text-to-motion benchmarks. Across these datasets, our method achieves state-of-the-art performance on both textto-motion and long-term motion synthesis tasks. We also demonstrate the superiority of our method on abundant applications. Such streaming generation framework is suitable for online multi-round generation with progressive text inputs, long-term motion generation with multiple texts provided and dynamic motion composition where subsequent motions can be regenerated by altering textual conditions while preserving the initially generated prefix motion. Overall, our contributions can be summarized as follows: We propose MotionStreamer, novel framework combining diffusion head with an autoregressive model to directly predict continuous motion latents, which enables streaming motion generation with incremental text inputs. We propose causal motion compressor (Causal TAE) for continuous motion compression, which eliminates information loss from discrete quantization and establishes temporally causal latent dependencies to support streaming decoding and online response. We adopt TwoForward training strategy to mitigate error accumulation in streaming generation scenarios. We demonstrate great performances of our approach on benchmark datasets. We also show various downstream including online multi-round generation, applications, long-term generation and dynamic motion composition. 2. Related Work Text-conditioned motion generation. Text-conditioned motion generation aims to synthesize 3D human motions from natural language descriptions [17]. Previous works [2, 17, 39] leverage VAE [27] to learn cross-modal mappings between text and motion spaces. Some works [3, 4, 9, 10, 12, 15, 25, 28, 48, 53, 57, 58, 61] also apply diffusion models [24] to this task. Another line of works [26, 33, 34, 60, 62, 64, 65] first discretize motions into discrete tokens and employ autoregressive models (e.g., GPT) for sequential token prediction. Furthermore, some approaches [16, 19, 37, 41, 42] adopt BERT-style [13] bidirectional Transformer architecture [7] to reconstruct masked motion segments under text guidance. However, most existing works only focus on offline generation, where the entire motion sequence is generated at once. More recently, CAMDM [8] and AMDM [49] apply diffusion models into an autoregerssive manner for real-time interactive character control. Ready-to-React [6] further explores this idea in two-character interaction. CLoSD [54] and DART Figure 2. Overview of MotionStreamer. During inference, the AR model streamingly predicts next motion latents conditioned on the current text and previous motion latents. Each latent can be decoded into motion frames online as soon as it is generated. [63] apply it for real-time text driven motion control. However, these methods are not strictly causal, as they rely on fixed-length context window while our method could handle variable-length historical information and incrementally generate motions in streaming manner. Motion compression. Following the success of image generation [46], previous works [9, 60] first encode the raw motion sequences into latent space and then generate motions within it. The most popular method is to use Vector Quantized Variational AutoEncoders (VQ-VAE) [56] for motion tokenization. TM2T [18] first introduces vector quantization for motion discretization. T2M-GPT [60] employs VQVAE to compress motion sequences into discrete latent space and then uses GPT for motion generation. MoMask [19] leverages Residual VQ-VAE (RVQ-VAE) [29] to progressively reduce quantization errors. In contrast, MLD [9] utilizes standard VAEs [27] to convert motion sequence into an embedding and then use diffusion model to generate the latent. However, the existing methods require whole motion sequence to be encoded and decoded, which is not suitable for streaming generation. In this paper, we propose causal motion compression approach to achieve streaming motion generation with online response. tion generator and the streaming generation process. 3.1. Problem Formulation Task definition. We first introduce the formulation of streaming motion generation. In contrast to previous textto-motion generation [17] which is conditioned on predefined fixed text prompt, we consider the case where series of text prompts are given sequentially. Given sequence of text prompts {Pi}M i=1, the goal is to generate sequence of motion frames {xj}N j=1, where Pi is the i-th text prompt and xj is the j-th frame pose. Motion Representation. Previous works [17, 19, 26, 53, 62] mainly uses the 263-dimensional pose representation [17] for motion generation. However, this representation requires an additional post-processing step [5], which is time-consuming and introduces rotation error [14] to be converted to SMPL [31] body parameters (see appendix Sec. E). To overcome this issue, we slightly modify it and directly use SMPL-based 6D rotation [31] as joint rotations. Similar to prior works on character control [50, 51], each pose is represented by 272-dimensional vector: 3. Method = { rx, rz, ra, jp, jv, jr}. (1) We address the task of streaming motion generation with online response by introducing novel framework, named MotionStreamer. The overview of the proposed framework is illustrated in Fig. 2. In section 3.1, we first introduce the problem formulation of streaming motion generation and the motion representation used in this work. In section 3.2, we introduce Causal Temporal AutoEncoder for continuous motion compression and online decoding. In section 3.3, we present diffusion-based autoregressive mowhere we project the root on the XZ-plane (ground plane), ( rx, rz R) are root linear velocities on the XZ-plane, ra R6 denotes root angular velocity represented in 6D rotations, jp R3K, jv R3K, and jr R6K are local joint positions, local velocities, and local rotations relative to the root space, is the number of joints. This representation removes the post-processing step and we could directly use it for animating SMPL [31] character model. 3.2. Causal Temporal AutoEncoder Streaming motion generation requires online motion decoding for dynamic text inputs. However, most existing works [19, 26, 60, 62] utilize temporal VQ-VAE to decode the whole sequence at once, where each frame inherently depends on past and future frames. Furthremore, the reliance on discrete tokenization induces quantization error accumulation across tokens, progressively degrading motion coherence in streaming generation scenarios. To address these issues, we introduce Causal Temporal AutoEncoder (Causal TAE) to enable motion generation in causal latent space. Architecture. Causal TAE is designed to achieve continuous motion compression while explicitly modeling temporal dependencies and enforcing causal constraints for sequential motion representation. Fig. 3 shows the proposed Causal TAE network. We employ 1D causal convolution [59] for constructing temporal encoder and decoder to convert raw motion sequences into causal latent space. The causality is guaranteed by temporal padding scheme. Specifically, for convolution layer with kernel size kt, stride st and dilation rate dt, we pad (kt 1) dt + (1 st) frames at the beginning of the sequence. In this way, each frame only depends on the frames before it and the future frames are not involved in the computation. Moreover, explicitly modeling temporal causal structures in the latent space enables the model to learn temporal and causal dependencies inherent in the causally-related motion data more effectively. Given motion sequence = {x1, x2, , xN } with xt Rd, where is the number of frames and is the motion dimension, we could obtain set of temporal Gaussian distribution parameters {µ1:N/l, σ2 1:N/l} and preform reparameterization [27] to get continuous motion latent representation = {z1, z2, , zN/l} with zi Rdc, represents the temporal downsampling rate of the Encoder E. This architecture reconstructs motion frames while strictly preserving temporal causality across the sequence. Training Objective. We use the same loss function as σVAE [47] to train the Causal TAE. In order to further enhance the reconstruction stability of the root joint, we add root joint loss Lroot. The full loss function is defined as: = Lrecon + DKL(q(zx)p(z)) + λLroot. (2) where Lrecon = Lroot = (cid:88) ( i=1 (xi ˆxi)2 2σ2 8 (cid:88) i= ( (xi ˆxi)2 2σ2 + lnσ), + lnσ), σ2 = SE(x, ˆx) = 1 (cid:88) (xi ˆxi)2, i=1 (3) (4) (5) Figure 3. Architecture of Causal TAE. 1D temporal causal convolution is applied in both the encoder and decoder. Variables z1:n are sampled as continuous motion latent representations. DKL(q(zx)p(z)) = 1 2 dc(cid:88) (µ + σ2 ln(σ2 ) 1). (6) i=1 and dc represent the dimensions of the motion sequence and its latent representation respectively. xi and ˆxi are the ground-truth motion latent and the reconstructed latent at i-th frame, σ is the analytic solution of the standard deviation [47]. DKL represents the KL divergence, q(zx) is the distribution of latents given the motions, p(z) = (0, I) is the prior distribution. λ is the balancing hyperparameter. Causal TAE offers distinct technical advantages for motion compression. Its causal property inherently supports online decoding without requiring access to future frames, which is critical for streaming generation. With the employment of continuous token representation, it bypasses the discretization bottleneck of existing VQ-based methods. 3.3. MotionStreamer In this section, we present MotionStreamer, streaming generation pipeline based on causally-structured latent space. In order to handle coherence between arriving text inputs and historical generated motions, we hypothesize that current motion should only be conditioned on previous motion and current text. As illustrated in Fig. 2, MotionStreamer comprises pre-trained text encoder, diffusionbased autoregressive model, and the online motion decoder (the learned Causal TAE decoder). Training. Each training sample can be represented as: Si = (Ti, Ci, Zi), where Ti R1dt is the text embedding obtained via pre-trained language model (e.g. T5XXL [45]), Ci Rkdc and Zi Rndc are the historical motion latents and current motion latents encoded by the learned Causal TAE, where k, n, dt and dc denote the lengths of previous motion latents, current motion latents, historical motion latents, the text embedding dimension and the latent dimension respectively. We concatenate them along temporal axis to form sequence Si = [Ti, Ci, Zi]. We employ diffusion-based autoregressive Transformer to predict motion latents. The latent sequence is first processed by the Transformer and causal mask is applied to ensure the temporal causality [60]. After the Transformer processing, we obtain the intermediate latents {c1 }, which serve as the condition for the diffusion head (a small MLP) to predict motion latents {ˆz1 , , Endi}. Endi is the reference end latent inserted at the end of sequence as the continuous stopping condition, which we will elaborate later. Following [24, 30], the loss function is defined as: , , cn , ˆz2 , c2 = Eϵ,t[ϵ ϵθ(Ztt, Ci, Ti)2]. (7) where denotes the timestep of noise schedule. We employ QK normalization (i.e., normalize both queries and keys) [21] before self-attention layer to enhance training stability. Two-Forward strategy. We observe that using teacherforcing [1] directly during training often leads to error accumulation in the autoregressive generation process. To this end, we propose Two-Forward strategy that progressively introduces the test-time distribution during training. Specifically, after the first forward pass, we replace subset of ground-truth motion latents with their generated counterparts, creating mixture of real and generated motion latents. This hybrid input is then used in the second forward pass, where gradients are backpropagated to refine the model. We employ cosine scheduler to control the proportion of replaced motion latents. Mixed training. The datasets contain two types of training samples, so we set Ci to Null if there is no historical motion in the dataset. We find that this simple strategy enables seamless transition between two consecutive motions. Continuous stopping condition. Streaming generation requires automatically determining the generation length for each text prompt. Previous method [36] uses binary classifier to determine whether to stop generation, which suffers from strong class imbalance. In contrast, we introduce an impossible pose prior (i.e., all-zero vectors 0 Rd) as the stopping condition and use the causal TAE to convert it into the latent space. The encoded latent serves as the reference end latent. The generation should stop when the distance between the currently generated latent and the reference end latent is less than threshold. Therefore, MotionStreamer is able to stop generation automatically and enables online and multi-round generation. 1, ˆz2 1, , ˆzn1 Inference. During inference, given stream of text prompts {Pi}M i=1, the first text embedding T1 is first fed into the autoregressive motion generator to generate the first predicted motion latent sequence ˆZ1 = {ˆz1 1 }. As soon as motion latent is predicted, it can be immediately processed by the online motion decoder (i.e., the learned Causal TAE decoder) to get the output motion frames, benefiting from its causal property. If the distance between the currently predicted motion latent and the reference end latent is lower than threshold, the generation process of this prompt stops. Then, we replace T1 with T2 as the current text embedding. The already generated motion latent sequence ˆZ1 is appended to the end of the second text embedding, forming the contextual latents used as input for the next autoregressive step. We then generate the second predicted motion latent sequence ˆZ2 = {ˆz1 2 }. Next, we replace T2 with future text embedding, removes ˆZ1 from the condition latents and uses ˆZ2 as the historical motion latents. Therefore, the third sequence could be predicted. This streaming generation process is repeated until the entire motion sequence { ˆZi}N i=1 is generated, ensuring online response during streaming generation process. 2, , ˆzn 2, ˆz2 4. Experiment 4.1. Experimental Setup Dataset. We evaluate the proposed MotionStreamer on HumanML3D [17] and BABEL [43] datasets, with the original train and test splits. The HumanML3D dataset integrates motion sequences with three distinct textual descriptions. The BABEL dataset provides frame-level textual descriptions with explicit inter-segment transition labels. Unlike recent methods [4, 48] that use different motion representations for both HumanML3D and BABEL datasets, we employ the 272-dimensional motion representation as mentioned in Sec. 3.1 for both datasets. All motion sequences are uniformly resampled at 60 FPS. Evaluation Metrics. We adopt the metrics from [17] for evaluation, including: (1) Frechet Inception Distance (FID) [22], indicating the distribution distance between the generated and real motion; (2) Mean Per Joint Position Error (MPJPE), the average distance between the predicted and ground-truth joint positions, measuring the reconstruction quality; (3) R-Precision (Top-1, Top-2, and Top-3 accuracy), the accuracy of the top-k retrieved motions; (4) Multimodal Distance (MM-Dist), the average Euclidean distances between the generated motion feature and its text feature. (5) Diversity, the average Euclidean distances of the randomly sampled 300 motion pairs, measuring the diversity of motions. (6) Peak Jerk (PJ) [4], the maximum value throughout the transition motion over all joints. (7) Methods FID R@1 R@2 R@3 MM-D Div Real motion 0.002 0.711 0.851 0.903 15.805 27.670 MDM [53] 22.557 0.524 MLD [9] 17.226 0.548 11.175 0.608 T2M-GPT [60] MotionGPT [26] 14.175 0.436 10.731 0.622 MoMask [19] 15.438 0.590 AttT2M [64] 10.724 0.631 Ours 0.693 0.732 0.772 0.598 0.782 0.767 0.784 0.773 0.805 0.831 0.668 0.850 0.837 0.851 17.223 27.355 16.338 26.551 16.810 27.617 17.890 27.014 16.128 27.317 15.734 26.680 16.639 27.657 Table 1. Comparison with baseline text-to-motion generation methods on HumanML3D [17] test set. MM-D and Div denote MM-Dist and Diversity respectively. Area Under the Jerk (AUJ) [4], the area under the jerk curve. Both PJ and AUJ measures the smoothness of motions. 4.2. Quantitative Results Comparison on Text-to-Motion Generation. We trained an evaluator based on TMR [40] to evaluate the quality of the generated motions. We use the processed 272dimensional motion data from HumanML3D [17] train set for text-to-motion model training. All baseline methods are trained from scratch following their original implementations. The comparison results on HumanML3D [17] test set are shown in Tab. 1. Our method outperforms baseline methods in multiple metrics. Comparison on Long-Term Motion Generation. We adopt Mix Training Strategy for streaming long-term generation training. Specifically, we create training samples by pairing adjacent subsequences from long motion sequences in BABEL. Additionally, we incorporate text-motion pairs from the HumanML3D dataset for mix training. The comparison results on BABEL [43] dataset are demonstrated in Tab. 2. We modified T2M-GPT to support streaming generation (marked as T2M-GPT*) and also adapted our model to use VQ for discretization (marked as VQ-LLaMA). Experimental results show that neither the existing long-term generation baseline nor the discrete autoregressive model performs as well as our streaming generation approach in the continuous latent space. Comparison on First-frame Latency. As streaming generation requires the model to generate motion progressively and respond online. Therefore, we adopt the First-frame Latency to evaluate the efficiency of different methods. Firstframe Latency refers to the time taken by the model to produce its first predicted frame, serving as key metric for evaluating online response ability. Experimental results in Fig. 4 show that our proposed Causal TAE achieves the lowest First-frame Latency, benefiting from the causal property of motion latents, which can be decoded immediately after generation. In contrast, non-causal VAE must wait unFigure 4. Comparison on the First-frame Latency of different methods. The horizontal axis represents the number of generated frames, while the vertical axis indicates the time required to produce the first output frame. til the entire sequence is generated before decoding, causing First-frame Latency to increase as the number of generated frames grows. Another fixed-length generation methods like [19, 53] exhibit higher First-frame Latency as they process the entire sequence at once rather than generating frames progressively in streaming manner. 4.3. Qualitative Results Figure 5 shows the qualitative results of our method compared with T2M-GPT [60], MoMask [19], AttT2M [64], and FlowMDM [4]. For the text-to-motion generation, we observe that VQ-based methods have difficulty in generating motions that are accurate and aligned with the textual description. In the case of man jumps on one leg., T2MGPT [60] and AttT2M [64] generate motion where the person jumps with both legs instead. MoMask [19] employs residual vector quantization (RVQ) to reduce quantization errors but still suffers from fine-grained motion details loss. Specifically, the generated motion starts with one-leg jump but later switches to two-leg jumps or alternating legs, along with noticeable sliding artifacts. However, our method can generate motions that are more accurate with more details preserved as we use continuous latent space without discretization process. For long-term motion generation, we compare with FlowMDM [4] with stream of prompts: [a man walks forward with arms swinging., then he jumps up., he turns around., he faces another side.]. The visualization results show that FlowMDM fails to generate the initial walking motion, instead producing in-place stepping. However, we can generate more coherent and natural longterm motions streamingly as our model has the ability to dynamically extract relevant information from variable-length motion histories. Please refer to the supplementary videos in our project page for more dynamic visualizations. Methods Subsequence Transition R@3 FID Div MM-Dist FID Div PJ AUJ GT DoubleTake [48] FlowMDM [4] T2M-GPT* [60] VQ-LLaMA Ours 0.634 0.452 0.492 0.364 0.383 0. 0.000 24.907 23.937 18.736 39.482 24.342 15.743 22.732 23.847 24.317 19.329 23.546 17.543 21.783 20.253 20.692 38.285 15. 0.000 21.472 51.232 34.721 43.823 36.293 32.888 18.892 20.293 20.797 19.932 19.986 0.03 0.48 0.06 0.12 0.08 0. 0.00 1.83 0.51 1.43 1.20 0.90 Table 2. Comparison with long-term motion generation methods on BABEL [43] dataset. Symbols , and indicate the higher, lower and closer to Ground Truth are better. Bold and underline indicate the best and second best results. Figure 5. Visualization results between our method and some baseline methods [4, 19, 60, 64]. The first row shows text-to-motion generation results, the second row shows long-term generation results and the third row shows the application of dynamic motion composition. 4.4. Ablation Study Architecture of the Causal TAE. We comprehensively evaluate the reconstruction performance and the corresponding generation quality of different Causal TAE architectures on the HumanML3D [17] test set, as shown in Tab. 3. We replace the motion compression stage with VQ-VAE [60] to discretize the motions, while keeping the second-stage model architecture identical to ours. We also experimented with replacing Causal TAE with non-causal temporal VAE and standard temporal AE without vector quantization. The results show that our continuous representation avoids the VQ process, effectively reducing information loss and minimizing quantization error, thus performs better than the VQ-VAE baseline. The non-causal VAE performs worse than Causal TAE in both reconstruction and generation, as Causal TAE inherently models the causal structure of motion data during compression. This causal latent space is better suited for autoregressive generation, aligning naturally with the causal masking process. While AE achieves the best reconstruction quality by learning near-identity mapping, its generation performance is significantly worse. This highlights the crucial role of latent space representation in determining the effectiveness of subsequent motion generation. We provide more detailed ablation on the latent dimension and hidden size of Causal TAE, as shown in Sec. in the appendix. Notably, we observe that larger latent dimension results in less compression rate, improving reconstruction quality. However, this comes at the cost of poorer generation performance, as insufficient compression and ineffective latent space representation makes it harder for the model to learn meaningful motion generation. Meanwhile, the hidden size determines the models capacity, requiring careful balance between compression rate and hidden size to ensure high reconstruction quality while enhancing generation performance. Ablation on the hyperparameter λ is provided in Sec. of the appendix. Design choices of AR Model. We analyze the impact Figure 6. Dynamic motion composition. Our model supports composition of multiple motions with different textual descriptions while maintaining previous motions unchanged. Methods Reconstruction Generation AR Design choices FID R@3 MM-Dist Diversity FID MPJPE FID R@3 MM-D. Div. Real motion - - 0.002 0.903 15.805 27.670 VQ-VAE AE VAE Ours 5.173 0.001 2.092 0.737 11.024 0.834 63.9 1.7 43.818 0.473 26.2 19.914 0.755 24.89 10.724 0.851 16.792 27.614 22.041 27.085 17.948 27.520 16.639 27. Table 3. Ablation Study of different motion compressors on HumanML3D [17] test set. MPJPE is measured in millimeters. 11.127 0.839 w/o QK Norm w/o Two-Forward 11.978 0.847 w/o Diffusion Head 59.195 0.361 14.033 0.792 CLIP 10.724 0.851 Ours 16.525 16.440 22.884 17.564 16.639 27.530 27.703 26.825 27.328 27.657 Table 4. Analysis of design choices of the AR model on HumanML3D [17] test set. CLIP indicates the use of CLIP model [44] as the text encoder to extract text features. of different design choices of the AR model, as shown in Tab. 4. The results show that the QK normalization and Two-forward strategy are effective. We also remove the diffusion head and use MSE loss for autoregressive training, which leads to significant drop in generation quality. Moreover, we find that using T5-XXL [45] improves the generation performance compared with the CLIP [44] tokenizer. We found that applying binary classifier to predict whether to stop generation, as in [36], fails to learn the correct stopping condition. As result, we did not evaluate the model without the proposed continuous stopping condition. 4.5. Applications including MotionStreamer offers various applications, multi-round generation, long-term generation, and dynamic motion composition. (1) Multi-round generation requires iteratively generating motion in response to sequential or interactive textual inputs. Our model can process incremental text inputs, respond online, and autonomously determine when to stop generation. (2) Long-term generation requires smoothly generating long sequences of motion in response to sequential textual inputs. (3) Dynamic motion composition refers to the capability of seamlessly integrating diverse motion sequences while preserving the consistency of previously generated content as shown in Fig. 6. Our Causal TAE enables this application, and during the generation of subsequent motion latents, it eliminates the need for full-sequence re-decoding, requiring only the decoding of the newly generated latents. Please see the supplementary videos in our project page for more application demos. 5. Conclusion We present MotionStreamer, novel framework for streaming motion generation that integrates diffusion-based autoregressive model to directly predict causal motion latents. By introducing Causal TAE, MotionStreamer supports progressive textual inputs and enables online response. To mitigate cumulative errors in the streaming generation process, we propose Two-Forward training strategy. Our method outperforms baseline approaches, demonstrating its competitiveness in motion generation while providing greater flexibility. It can be applied to multi-round generation, long-term generation, and dynamic motion composition. Future work could explore hybrid strategies that allow bidirectional refinement without compromising streaming generation. One potential way is to predict set of future latents at each step, which could enable motion in-between and localized editing while preserving streaming manner. Limitations. Despite its effectiveness, the streaming generation paradigm limits the applications of motion in-betweening and localized editing of intermediate tokens, as it inherently relies on unidirectional modeling. This limitation restricts flexibility in scenarios requiring fine-grained adjustments, such as seamlessly inserting new motions between existing frames or interactively refining motion details while preserving global coherence."
        },
        {
            "title": "References",
            "content": "[1] Kushal Arora, Layla El Asri, Hareesh Bahuleyan, and Jackie Chi Kit Cheung. Why exposure bias matters: An imitation learning perspective of error accumulation in language generation. arXiv preprint arXiv:2204.01171, 2022. 5 [2] Nikos Athanasiou, Mathis Petrovich, Michael J. Black, and Gul Varol. Teach: Temporal action compositions for 3d humans. In International Conference on 3D Vision (3DV), 2022. 2 [3] Omer Bar-Tal, Lior Yariv, Yaron Lipman, and Tali Dekel. Multidiffusion: Fusing diffusion paths for controlled image generation. 2023. 2 [4] German Barquero, Sergio Escalera, and Cristina Palmero. Seamless human motion composition with blended posiIn Proceedings of the IEEE/CVF Contional encodings. ference on Computer Vision and Pattern Recognition, pages 457469, 2024. 2, 5, 6, 7 [5] Federica Bogo, Angjoo Kanazawa, Christoph Lassner, Peter Gehler, Javier Romero, and Michael J. Black. Keep it SMPL: Automatic estimation of 3D human pose and shape from single image. In Computer Vision ECCV 2016. Springer International Publishing, 2016. 3 [6] Zhi Cen, Huaijin Pi, Sida Peng, Qing Shuai, Yujun Shen, Hujun Bao, Xiaowei Zhou, and Ruizhen Hu. Ready-to-react: Online reaction policy for two-character interaction generation. In ICLR, 2025. [7] Huiwen Chang, Han Zhang, Lu Jiang, Ce Liu, and William Freeman. Maskgit: Masked generative image transformer. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 1131511325, 2022. 2 [8] Rui Chen, Mingyi Shi, Shaoli Huang, Ping Tan, Taku Komura, and Xuelin Chen. Taming diffusion probabilistic models for character control. In ACM SIGGRAPH 2024 Conference Papers, pages 110, 2024. 2 [9] Xin Chen, Biao Jiang, Wen Liu, Zilong Huang, Bin Fu, Tao Chen, and Gang Yu. Executing your commands via motion diffusion in latent space. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 1800018010, 2023. 2, 3, 6 [10] Seunggeun Chi, Hyung-gun Chi, Hengbo Ma, Nakul Agarwal, Faizan Siddiqui, Karthik Ramani, and Kwonjoon Lee. M2d2m: Multi-motion generation from text with discrete diffusion models. In European Conference on Computer Vision, pages 1836. Springer, 2024. 2 [11] Wenxun Dai, Ling-Hao Chen, Yufei Huo, Jingbo Wang, Jinpeng Liu, Bo Dai, and Yansong Tang. Motionlcm-v2: Improved compression rate for multi-latent-token diffusion, 2024. 1 [12] Wenxun Dai, Ling-Hao Chen, Jingbo Wang, Jinpeng Liu, Bo Dai, and Yansong Tang. Motionlcm: Real-time controllable motion generation via latent consistency model. In European Conference on Computer Vision, pages 390408. Springer, 2024. [13] Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. Bert: Pre-training of deep bidirectional transformers for language understanding. In Proceedings of the 2019 conference of the North American chapter of the association for computational linguistics: human language technologies, volume 1 (long and short papers), pages 4171 4186, 2019. 2 [14] GitHub discussion. https : / / github.com/EricGuo5513/HumanML3D/issues/ 26, 2023. 2023-03-04. 3 rotation discussion. [15] Ke Fan, Junshu Tang, Weijian Cao, Ran Yi, Moran Li, Jingyu Gong, Jiangning Zhang, Yabiao Wang, Chengjie Wang, and Lizhuang Ma. Freemotion: unified framework for numberIn European Conference on free text-to-motion synthesis. Computer Vision, pages 93109. Springer, 2024. 2 [16] Ke Fan, Jiangning Zhang, Ran Yi, Jingyu Gong, Yabiao Wang, Yating Wang, Xin Tan, Chengjie Wang, and Lizhuang Ma. Textual decomposition then sub-motion-space scattering for open-vocabulary motion generation. arXiv preprint arXiv:2411.04079, 2024. 2 [17] Chuan Guo, Shihao Zou, Xinxin Zuo, Sen Wang, Wei Ji, Xingyu Li, and Li Cheng. Generating diverse and natural 3d human motions from text. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pages 51525161, 2022. 2, 3, 5, 6, 7, 8, [18] Chuan Guo, Xinxin Zuo, Sen Wang, and Li Cheng. Tm2t: Stochastic and tokenized modeling for the reciprocal generation of 3d human motions and texts. In European Conference on Computer Vision, pages 580597. Springer, 2022. 3 [19] Chuan Guo, Yuxuan Mu, Muhammad Gohar Javed, Sen Wang, and Li Cheng. Momask: Generative masked modeling of 3d human motions. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 19001910, 2024. 2, 3, 4, 6, 7 [20] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recognition. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 770778, 2016. 1 [21] Alex Henry, Prudhvi Raj Dachapally, Shubham Pawar, and Yuxuan Chen. Query-key normalization for transformers. arXiv preprint arXiv:2010.04245, 2020. 5 [22] Martin Heusel, Hubert Ramsauer, Thomas Unterthiner, Bernhard Nessler, and Sepp Hochreiter. Gans trained by two time-scale update rule converge to local nash equilibrium. Advances in neural information processing systems, 30, 2017. 5 [23] Jonathan Ho and Tim Salimans. Classifier-free diffusion guidance. arXiv preprint arXiv:2207.12598, 2022. 1 [24] Jonathan Ho, Ajay Jain, and Pieter Abbeel. Denoising diffusion probabilistic models. Advances in neural information processing systems, 33:68406851, 2020. 2, 5, 1 [25] Bin Ji, Ye Pan, Zhimeng Liu, Shuai Tan, and Xiaokang Yang. Sport: From zero-shot prompts to real-time motion generIEEE Transactions on Visualization and Computer ation. Graphics, 2025. [26] Biao Jiang, Xin Chen, Wen Liu, Jingyi Yu, Gang Yu, and Tao Chen. Motiongpt: Human motion as foreign language. Advances in Neural Information Processing Systems, 36:2006720079, 2023. 2, 3, 4, 6 [27] Diederik Kingma, Max Welling, et al. Auto-encoding variational bayes, 2013. 2, 3, 4 [28] Hanyang Kong, Kehong Gong, Dongze Lian, Michael Bi Mi, and Xinchao Wang. Priority-centric human motion generation in discrete latent space. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 14806 14816, 2023. 2 [29] Doyup Lee, Chiheon Kim, Saehoon Kim, Minsu Cho, and Wook-Shin Han. Autoregressive image generation using residual quantization. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 1152311532, 2022. 3 [30] Tianhong Li, Yonglong Tian, He Li, Mingyang Deng, and Kaiming He. Autoregressive image generation without vector quantization. arXiv preprint arXiv:2406.11838, 2024. 5 [31] Matthew Loper, Naureen Mahmood, Javier Romero, Gerard Pons-Moll, and Michael Black. Smpl: skinned multiperson linear model. In Seminal Graphics Papers: Pushing the Boundaries, Volume 2, pages 851866. 2023. [32] Ilya Loshchilov and Frank Hutter. Decoupled weight decay regularization. arXiv preprint arXiv:1711.05101, 2017. 1 [33] Shunlin Lu, Ling-Hao Chen, Ailing Zeng, Jing Lin, Ruimao Zhang, Lei Zhang, and Heung-Yeung Shum. Humantomato: Text-aligned whole-body motion generation. arXiv preprint arXiv:2310.12978, 2023. 2 [34] Shunlin Lu, Jingbo Wang, Zeyu Lu, Ling-Hao Chen, Wenxun Dai, Junting Dong, Zhiyang Dou, Bo Dai, and Scamo: Exploring the scaling law in Ruimao Zhang. arXiv preprint autoregressive motion generation model. arXiv:2412.14559, 2024. 2 [35] Naureen Mahmood, Nima Ghorbani, Nikolaus Troje, Gerard Pons-Moll, and Michael Black. Amass: Archive In Proceedings of of motion capture as surface shapes. the IEEE/CVF international conference on computer vision, pages 54425451, 2019. 3 [36] Lingwei Meng, Long Zhou, Shujie Liu, Sanyuan Chen, Bing Han, Shujie Hu, Yanqing Liu, Jinyu Li, Sheng Zhao, Xixin Wu, et al. Autoregressive speech synthesis without vector quantization. arXiv preprint arXiv:2407.08551, 2024. 5, [37] Zichong Meng, Yiming Xie, Xiaogang Peng, Zeyu Han, and Huaizu Jiang. Rethinking diffusion for text-driven human motion generation. arXiv preprint arXiv:2411.16575, 2024. 2 [38] William Peebles and Saining Xie. Scalable diffusion models with transformers. In Proceedings of the IEEE/CVF international conference on computer vision, pages 41954205, 2023. 1 In European Conference on Computer Vision, pages 480 497. Springer, 2022. 2 [40] Mathis Petrovich, Michael Black, and Gul Varol. Tmr: Text-to-motion retrieval using contrastive 3d human motion In Proceedings of the IEEE/CVF International synthesis. Conference on Computer Vision, pages 94889497, 2023. 6 [41] Ekkasit Pinyoanuntapong, Muhammad Usama Saleem, Pu Wang, Minwoo Lee, Srijan Das, and Chen Chen. Bamm: In European bidirectional autoregressive motion model. Conference on Computer Vision, pages 172190. Springer, 2024. 2 [42] Ekkasit Pinyoanuntapong, Pu Wang, Minwoo Lee, and Chen In ProChen. Mmm: Generative masked motion model. ceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 15461555, 2024. 2 [43] Abhinanda R. Punnakkal, Arjun Chandrasekaran, Nikos Athanasiou, Alejandra Quiros-Ramirez, and Michael J. Black. BABEL: Bodies, action and behavior with english labels. In Proceedings IEEE/CVF Conf. on Computer Vision and Pattern Recognition (CVPR), pages 722731, 2021. 2, 5, 6, [44] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, et al. Learning transferable visual models from natural language supervision. In International conference on machine learning, pages 87488763. PmLR, 2021. 8 [45] Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, and Peter Liu. Exploring the limits of transfer learning with unified text-to-text transformer. Journal of machine learning research, 21(140):167, 2020. 5, 8 [46] Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Bjorn Ommer. High-resolution image In Proceedings of synthesis with latent diffusion models. the IEEE/CVF conference on computer vision and pattern recognition, pages 1068410695, 2022. 3 [47] Oleh Rybkin, Kostas Daniilidis, and Sergey Levine. Simple and effective vae training with calibrated decoders. In International conference on machine learning, pages 91799189. PMLR, 2021. 4 [48] Yonatan Shafir, Guy Tevet, Roy Kapon, and Amit Bermano. Human motion diffusion as generative prior. arXiv preprint arXiv:2303.01418, 2023. 2, 5, 7 [49] Yi Shi, Jingbo Wang, Xuekun Jiang, Bingkun Lin, Bo Dai, and Xue Bin Peng. Interactive character control with autoregressive motion diffusion models. ACM Transactions on Graphics (TOG), 43(4):114, 2024. [50] Yi Shi, Jingbo Wang, Xuekun Jiang, Bingkun Lin, Bo Dai, and Xue Bin Peng. Interactive character control with autoregressive motion diffusion models. ACM Transactions on Graphics (TOG), 43(4):114, 2024. 3 [51] Sebastian Starke, He Zhang, Taku Komura, and Jun Saito. Neural state machine for character-scene interactions. ACM Transactions on Graphics, 38(6):178, 2019. 3 [39] Mathis Petrovich, Michael Black, and Gul Varol. Temos: Generating diverse human motions from textual descriptions. [52] Jianlin Su, Murtadha Ahmed, Yu Lu, Shengfeng Pan, Wen Bo, and Yunfeng Liu. Roformer: Enhanced transformer with [65] Zixiang Zhou, Yu Wan, and Baoyuan Wang. Avatargpt: Allin-one framework for motion understanding planning generIn Proceedings of the IEEE/CVF Conation and beyond. ference on Computer Vision and Pattern Recognition, pages 13571366, 2024. 2 rotary position embedding. Neurocomputing, 568:127063, 2024. [53] Guy Tevet, Sigal Raab, Brian Gordon, Yoni Shafir, Daniel Cohen-or, and Amit Haim Bermano. Human motion diffuIn The Eleventh International Conference on sion model. Learning Representations, 2023. 2, 3, 6, 1 [54] Guy Tevet, Sigal Raab, Setareh Cohan, Daniele Reda, Zhengyi Luo, Xue Bin Peng, Amit Bermano, and Michiel van de Panne. Closd: Closing the loop between simulation and diffusion for multi-task character control. arXiv preprint arXiv:2410.03441, 2024. 2 [55] Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timothee Lacroix, Baptiste Rozi`ere, Naman Goyal, Eric Hambro, Faisal Azhar, et al. Llama: Open and efficient foundation language models. arXiv preprint arXiv:2302.13971, 2023. 1 [56] Aaron Van Den Oord, Oriol Vinyals, et al. Neural discrete representation learning. Advances in neural information processing systems, 30, 2017. 3 [57] Yin Wang, Zhiying Leng, Frederick WB Li, Shun-Cheng Wu, and Xiaohui Liang. Fg-t2m: Fine-grained text-driven human motion generation via diffusion model. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 2203522044, 2023. 2 [58] Zhao Yang, Bing Su, and Ji-Rong Wen. Synthesizing longterm human motions with diffusion models via coherent In Proceedings of the 31st ACM International sampling. Conference on Multimedia, pages 39543964, 2023. 2 [59] Lijun Yu, Jose Lezama, Nitesh Gundavarapu, Luca Versari, Kihyuk Sohn, David Minnen, Yong Cheng, Vighnesh Birodkar, Agrim Gupta, Xiuye Gu, et al. Language model beats diffusiontokenizer is key to visual generation. arXiv preprint arXiv:2310.05737, 2023. [60] Jianrong Zhang, Yangsong Zhang, Xiaodong Cun, Shaoli Huang, Yong Zhang, Hongwei Zhao, Hongtao Lu, and Xi Shen. T2m-gpt: Generating human motion from textual deIn Proceedings of scriptions with discrete representations. the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), 2023. 2, 3, 4, 5, 6, 7 [61] Mingyuan Zhang, Zhongang Cai, Liang Pan, Fangzhou Hong, Xinying Guo, Lei Yang, and Ziwei Liu. Motiondiffuse: Text-driven human motion generation with diffusion model. IEEE transactions on pattern analysis and machine intelligence, 46(6):41154128, 2024. 2 [62] Yaqi Zhang, Di Huang, Bin Liu, Shixiang Tang, Yan Lu, Lu Chen, Lei Bai, Qi Chu, Nenghai Yu, and Wanli Ouyang. Motiongpt: Finetuned llms are general-purpose motion generaIn Proceedings of the AAAI Conference on Artificial tors. Intelligence, pages 73687376, 2024. 2, 3, 4 [63] Kaifeng Zhao, Gen Li, and Siyu Tang. Dart: diffusionbased autoregressive motion model for real-time text-driven motion control. arXiv preprint arXiv:2410.05260, 2024. 2, 3 [64] Chongyang Zhong, Lei Hu, Zihao Zhang, and Shihong Xia. Attt2m: Text-driven human motion generation with multiIn Proceedings of the perspective attention mechanism. IEEE/CVF International Conference on Computer Vision, pages 509519, 2023. 2, 6,"
        },
        {
            "title": "Appendix",
            "content": "A. Implementation Details For the Causal TAE, both the encoder and decoder are based on the 1D causal ResNet blocks [20]. The temporal downsampling rate is set to 4 and all motion sequences are cropped to = 64 frames during training. We train the first 1900K iterations with learning rate of 5e-5 and the remaining 100K iterations with learning rate of 2.5e-6. We use the AdamW optimizer [32] with [β1, β2] = [0.9, 0.99] and batch size of 128. We provide an ablation study on the hyperparameter λ of root loss Lroot in Tab. 5. The latent dimension dc and hidden size are set to 16 and 1024, respectively. The latent dimension significantly impacts the compression rate, while the hidden size affects the models capacity. Both factors influence reconstruction and subsequent generation quality, requiring careful trade-off between compression efficiency and generative performance. Ablation studies on the latent dimension and hidden size are provided in Tab. 6. To further improve the quality of the reconstructed motion, we add linear layer after the embedded Gaussian distribution parameters as latent adapter to get lower-dimensional and more compact latent space for subsequent sampling, as proposed in [11]. For the Transformer inside the AR model, we use the architecture akin to LLaMA [55] with 12 layers, 12 attention heads and 768 hidden dimension. The ablation for different scales of the Transformer is provided in Tab. 7. Block size is set to 78 and we choose RoPE [52] as the positional encoding. For the diffusion head after Transformer, we use MLPs with 1768 hidden dimension and 9 layers. The output vectors of the Transformer serve as the condition of denoising via AdaLN [38]. We adopt cosine noise schedule with 50 steps for the DDPM [24] denoising process following [53]. During training, the minimum and maximum length of motion sequences are set to 40 and 300 for both datasets. We insert an additional reference end latent at the end of each motion sequence to indicate the stop of generation. For Two-Forward strategy, cosine scheduler is employed to control the ratio of replaced motion tokens, which can be 2 (1 cos( πt formulated as: γt = 1 )), where is current iteration step and is the total number of iterations. When = 0, γt = 0, indicating that no generated motion tokens in the first forward pass are replaced, thus relying on the ground-truth motion tokens only. When = , γt = 1, indicating that all generated motion tokens in the first forward pass are replaced, thus relying on the generated motion tokens only. We use the same optimizer as the Causal TAE λ 5.0 6. 7.0 8.0 9.0 FID MPJPE 0. 0.882 0.838 0.855 0.962 29.2 28. 27.5 27.9 29.4 Table 5. Analysis of λ on the HumanML3D [17] test dataset. and batch size of 256. The initial learning rate is 1e-4 after 10K warmup iterations and decay to 0 for another 90K iterations using cosine learning rate scheduler. Our experiments are conducted on NVIDIA A800 GPUs. B. Causal TAE Architecture The detailed architecture of the Causal TAE is shown in Fig. 9 and Tab. 8. Input motion sequences are first encoded into latent space with 1D causal ResNet. The latent space is then projected to sequence of Gaussian distribution parameters. Then linear adapter is applied to the embedded Gaussian distribution parameters to lower the dimension of latent space. Sampling is performed in the lower-dimensional latent space. The decoder comprises mirror process to progressively reconstruct the motion sequence. C. AR Model Architecture We provide an ablation study on the architecture of the AR model, including the number of Transformer layers, attention heads, hidden dimension, and the number of diffusion head layers, as shown in Tab. 7. We finally leverage the 12layer, 12-head, 768-hidden dimension, and 9-layer diffusion head architecture. D. Classifier-free guidance We adopt the classifier-free guidance (CFG) [23] technique to improve the generation quality of the autoregressive motion generator. Specifically, during training, we replace 10% of the text within batch with blank text as unconditioned samples, while during inference, CFG is applied to the denoising process of the diffusion head, which can be Methods Reconstruction Generation FID MPJPE FID R@1 R@2 R@3 MM-Dist Diversity Real motion - (12,512) (12,1024) (12,1280) (12,1792) (12,2048) (14,512) (14,1024) (14,1280) (14,1792) (14,2048) (16,512) (16,1024) (16,1280) (16,1792) (16,2048) (18,512) (18,1024) (18,1280) (18,1792) (18,2048) (20,512) (20,1024) (20,1280) (20,1792) (20,2048) 8.862 1.710 2.035 1.563 1.732 2.902 0.838 0.919 0.732 1.370 1.300 0.737 1.087 0.540 1.547 2.043 0.656 0.820 1.045 0.595 0.531 0.379 0.429 0.548 0.690 - 38.5 31.2 32.9 28.3 28.9 33.6 27.5 26.4 24.8 26.5 30.3 24.89 25.0 22.0 26.2 27.7 23.4 23.1 22.1 21.5 24.5 19.9 20.11 20.1 20.7 0. 21.078 12.778 12.872 11.916 13.394 16.612 11.933 12.603 11.358 12.261 14.096 10.724 12.975 11.192 12.778 19.150 11.488 11.815 12.514 11.803 12.247 11.010 16.465 11.145 11.910 0. 0.600 0.628 0.642 0.635 0.611 0.607 0.627 0.603 0.628 0.621 0.605 0.631 0.598 0.632 0.604 0.553 0.619 0.629 0.612 0.613 0.613 0.630 0.557 0.616 0.625 0. 0.759 0.779 0.785 0.782 0.770 0.772 0.778 0.772 0.776 0.768 0.770 0.784 0.761 0.767 0.755 0.701 0.775 0.776 0.774 0.766 0.765 0.765 0.705 0.776 0.782 0. 0.827 0.845 0.854 0.854 0.831 0.836 0.840 0.841 0.856 0.841 0.839 0.851 0.831 0.859 0.824 0.775 0.840 0.847 0.840 0.832 0.832 0.847 0.774 0.842 0.844 15. 17.143 16.756 16.587 16.468 16.852 16.947 16.593 16.863 16.652 16.734 16.882 16.639 17.002 16.644 16.897 17.776 16.816 16.816 16.915 17.004 16.920 16.802 17.680 16.919 16.785 27. 27.755 27.408 27.455 27.661 27.417 27.328 27.443 27.414 27.122 27.417 27.306 27.657 27.403 27.419 27.306 27.345 27.356 27.461 27.911 27.451 27.277 27.485 27.490 27.597 27.542 Table 6. Comparison with baseline motion tokenizers on HumanML3D [17] test set. MPJPE is measured in millimeters. (16, 1024) indicates the latent dimension and hidden size of the Causal TAE. formulated as: ϵg = ϵu + s(ϵc ϵu). (8) where ϵg is the guided noise, ϵu is the unconditioned noise, ϵc is the conditioned noise, is the CFG scale. We provide an ablation study on the CFG scale in Fig. 7. Finally, we choose = 4.0 for all experiments. E. Failure of Inverse Kinematics Post-processing for 263-dimentional motion representation. Most previous works [19, 53, 60] uses 263dimentional motion representation [17]. The representation is as follows: = { rx, rz, ry, ra, jp, jv, jr, c}, (9) where the root is projected on the XZ-plane (ground plane), ( rx, rz R) are root linear velocities on the XZ-plane, Figure 7. Ablation of CFG scale on HumanML3D [17] test set. scale = 1 means do not use CFG. AR. layers AR. heads AR. dim Diff. layers 8 8 8 8 8 12 12 12 12 12 16 16 16 16 16 8 8 8 8 8 12 12 12 12 12 16 16 16 16 16 512 512 512 512 512 768 768 768 768 768 1024 1024 1024 1024 1024 2 3 4 9 16 2 3 4 9 16 2 3 4 9 FID 14.336 13.764 12.893 11.721 12.460 11.899 11.783 12.051 10.724 11.825 12.836 12.436 13.005 12.093 11.411 R@1 R@2 R@3 MM-Dist Diversity 0.598 0.602 0.608 0.623 0.621 0.601 0.632 0.604 0.631 0.624 0.606 0.601 0.614 0.614 0.635 0.747 0.758 0.764 0.772 0.778 0.763 0.779 0.762 0.784 0.773 0.765 0.761 0.763 0.778 0.780 0.802 0.819 0.828 0.835 0.849 0.828 0.844 0.829 0.851 0.844 0.832 0.830 0.830 0.843 0.846 16.983 16.972 16.661 16.655 16.784 16.952 16.761 16.940 16.639 16.757 16.901 16.919 16.967 16.850 16.598 27.787 27.742 27.351 27.585 27.410 27.406 27.482 27.501 27.657 27.541 27.619 27.607 27.196 27.508 27. Table 7. Ablation study of AR Model architecture on HumanML3D [17] test set. For each architecture, we use the same Causal TAE. Figure 8. Failure of Inverse Kinematics. The joint rotation is directly solved using IK with relative joint positions, which leads to unnatural results like jittering body parts. unnatural results like jittering head [53]. Most data in the HumanML3D [17] dataset comes from the AMASS [35] dataset. As the AMASS dataset provides the SMPL joint rotation, we slightly modify the motion representation by directly using the SMPL joint rotation. Consequently, we remove the slow post-processing step and easily drive the SMPL character with the generated rotations. ra R1 denotes root angury is the root height, lar velocity along the Y-axis, jp R3K, jv R3K+3, and jr R6K are local joint positions, local velocities, and local rotations relative to the root, is the number of joints (including the root), and R4 is the contact label. For SMPL characters, we have = 22 and we get 2 + 1 + 1 + 3 21 + 3 21 + 3 + 6 21 + 4 = 263 dimensions. In the original implementation [17], the joint rotation is directly solved using Inverse Kinematics (IK) with relative joint positions. In such way, the joint loses twist rotation and directly applying the joint rotation to the character faces lot of rotation error [14], as shown in Fig. 8. To overcome this issue, previous works [19, 53, 60] only uses the positions and employs SMPLify [5] to solve the real SMPL joint rotation. This process is time-consuming (around 60 seconds for 10 seconds motion clip) and also introduces Components Architecture Causal TAE Encoder (0): CausalConv1D(Din, 1024, kernel size=(3,), stride=(1,), dilation=(1,), padding=(2,)) (1): ReLU() (2): 2 Sequential( (0): CausalConv1D(1024, 1024, kernel size=(4,), stride=(2,), dilation=(1,), padding=(2,)) (1): CausalResnet1D( (0): CausalResConv1DBlock( (activation1): ReLU() (conv1): CausalConv1D(1024, 1024, kernel size=(3,), stride=(1,), dilation=(9,), padding=(18,)) (activation2): ReLU() (conv2): CausalConv1D(1024, 1024, kernel size=(1,), stride=(1,), dilation=(1,), padding=(0,))) (1): CausalResConv1DBlock( (activation1): ReLU() (conv1): CausalConv1D(1024, 1024, kernel size=(3,), stride=(1,), dilation=(3,), padding=(6,)) (activation2): ReLU() (conv2): CausalConv1D(1024, 1024, kernel size=(1,), stride=(1,), dilation=(1,), padding=(0,))) (2): CausalResConv1DBlock( (activation1): ReLU() (conv1): CausalConv1D(1024, 1024, kernel size=(3,), stride=(1,), dilation=(1,), padding=(2,)) (activation2): ReLU() (conv2): CausalConv1D(1024, 1024, kernel size=(1,), stride=(1,), dilation=(1,), padding=(0,))))) (3): CausalConv1D(1024, 1024, kernel size=(3,), stride=(1,), dilation=(1,), padding=(2,)) Causal TAE Decoder (0): CausalConv1D(1024, 1024, kernel size=(3,), stride=(1,), dilation=(1,), padding=(2,)) (1): ReLU() (2): 2 Sequential( (0): CausalResnet1D( (0): CausalResConv1DBlock( (activation1): ReLU() (conv1): CausalConv1D(1024, 1024, kernel size=(3,), stride=(1,), dilation=(9,), padding=(18,)) (activation2): ReLU() (conv2): CausalConv1D(1024, 1024, kernel size=(1,), stride=(1,), dilation=(1,), padding=(0,))) (1): CausalResConv1DBlock( (activation1): ReLU() (conv1): CausalConv1D(1024, 1024, kernel size=(3,), stride=(1,), dilation=(3,), padding=(6,)) (activation2): ReLU() (conv2): CausalConv1D(1024, 1024, kernel size=(1,), stride=(1,), dilation=(1,), padding=(0,))) (2): CausalResConv1DBlock( (activation1): ReLU() (conv1): CausalConv1D(1024, 1024, kernel size=(3,), stride=(1,), dilation=(1,), padding=(2,)) (activation2): ReLU() (conv2): CausalConv1D(1024, 1024, kernel size=(1,), stride=(1,), dilation=(1,), padding=(0,))))) (1): Upsample(scale factor=2.0, mode=nearest) (2): CausalConv1D(1024, 1024, kernel size=(3,), stride=(1,), dilation=(1,), padding=(2,)) (3) CausalConv1D(1024, 1024, kernel size=(3,), stride=(1,), dilation=(1,), padding=(2,)) (4): ReLU() (5): CausalConv1D(1024, Din, kernel size=(3,), stride=(1,), dilation=(1,), padding=(2,)) Table 8. Detail architecture of the proposed Causal TAE. Figure 9. Architecture of Causal TAE. Motion latents are sampled in continuous causal latent space. Figure 10. Architecture of Transformer blocks in AR model. QK Norm is applied to enhance training stability."
        }
    ],
    "affiliations": [
        "DeepGlint",
        "Shanghai AI Laboratory",
        "Shanghai Jiao Tong University",
        "The Chinese University of Hong Kong, Shenzhen",
        "The University of Hong Kong",
        "Zhejiang University"
    ]
}