{
    "paper_title": "ReasonEdit: Towards Reasoning-Enhanced Image Editing Models",
    "authors": [
        "Fukun Yin",
        "Shiyu Liu",
        "Yucheng Han",
        "Zhibo Wang",
        "Peng Xing",
        "Rui Wang",
        "Wei Cheng",
        "Yingming Wang",
        "Aojie Li",
        "Zixin Yin",
        "Pengtao Chen",
        "Xiangyu Zhang",
        "Daxin Jiang",
        "Xianfang Zeng",
        "Gang Yu"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Recent advances in image editing models have shown remarkable progress. A common architectural design couples a multimodal large language model (MLLM) encoder with a diffusion decoder, as seen in systems such as Step1X-Edit and Qwen-Image-Edit, where the MLLM encodes both the reference image and the instruction but remains frozen during training. In this work, we demonstrate that unlocking the reasoning capabilities of MLLM can further push the boundaries of editing models. Specifically, we explore two reasoning mechanisms, thinking and reflection, which enhance instruction understanding and editing accuracy. Based on that, our proposed framework enables image editing in a thinking-editing-reflection loop: the thinking mechanism leverages the world knowledge of MLLM to interpret abstract instructions, while the reflection reviews editing results, automatically corrects unintended manipulations, and identifies the stopping round. Extensive experiments demonstrate that our reasoning approach achieves significant performance gains, with improvements of ImgEdit (+4.3%), GEdit (+4.7%), and Kris (+8.2%) when initializing our DiT from the Step1X-Edit (ReasonEdit-S), and also outperforms previous open-source methods on both GEdit and Kris when integrated with Qwen-Image-Edit (ReasonEdit-Q)."
        },
        {
            "title": "Start",
            "content": "ReasonEdit: Towards Reasoning-Enhanced Image Editing Models Step1X-Image Team StepFun https://github.com/stepfun-ai/Step1X-Edit 5 2 0 2 1 ] . [ 2 5 2 6 2 2 . 1 1 5 2 : r a"
        },
        {
            "title": "Abstract",
            "content": "Recent advances in image editing models have shown remarkable progress. common architectural design couples multimodal large language model (MLLM) encoder with diffusion decoder, as seen in systems such as Step1X-Edit and Qwen-Image-Edit, where the MLLM encodes both the reference image and the instruction but remains frozen during training. In this work, we demonstrate that unlocking the reasoning capabilities of MLLM can further push the boundaries of editing models. Specifically, we explore two reasoning mechanisms, thinking and reflection, which enhance instruction understanding and editing accuracy. Based on that, our proposed framework enables image editing in thinkingeditingreflection loop: the thinking mechanism leverages the world knowledge of MLLM to interpret abstract instructions, while the reflection reviews editing results, automatically corrects unintended manipulations, and identifies the stopping round. Extensive experiments demonstrate that our reasoning approach achieves significant performance gains, with improvements of ImgEdit (+4.3%), GEdit (+4.7%), and Kris (+8.2%) when initializing our DiT from the Step1XEdit (ReasonEdit-S), and also outperforms previous opensource methods on both GEdit and Kris when integrated with Qwen-Image-Edit (ReasonEdit-Q). 1. Introduction Image editing with diffusion models has witnessed rapid progress, moving from early mask-based approaches such as BrushNet [22] and PowerPaint [60], to instruction-driven systems like InstructPix2Pix [7] and OmniGen [53], and more recently to multimodal frameworks that integrate an like Step1XMLLM encoder with diffusion decoder, Edit [33] and Qwen-Image-Edit [50]. These advances have substantially improved controllability and usability, enabling more diverse and flexible image editing. However, state-of-the-art instruction-based methods still face challenges in generalizing instructions, as most models keep MLLM encoders frozen during training. As result, current Figure 1. ReasonEdit achieves progressive performance gains, employing Thinking to interpret abstract instructions and Reflection to audit and correct the initial results. models exhibit limited visual reasoning capabilities, which restricts their ability to handle complex or abstract instructions. More importantly, such limitations prevent them from benefiting fully from test-time scaling, paradigm that has driven significant improvements in language models. Turning to the visual reasoning domain, recent advances have explored reasoning-enhanced visual generation through unified understanding and generation [11, 28, 46], reflection-based refinement [26, 51], and chain-of-thought modeling [20, 48, 57]. These studies highlight the potential of reasoning for controllable and efficient generation. For instance, BAGEL [11] introduces thinking mode that leverages the world knowledge of MLLMs to interpret abstract instructions in image generation and editing, while OmniGen2 [51] integrates reflection capabilities of MLLMs into generation. Despite these advances, most existing efforts remain centered on image generation [6, 23], leaving the application of reasoning to image editing largely underexplored. key underlying challenge lies in the substantial hallucinations of MLLMs during paired image understanding [12, 19], particularly in capturing the differences between reference and edited results [15, 47] and in generating appropriate refined instructions for subsequent editing. To this end, we propose ReasonEdit, fundamental editing model with two reasoning capbilities: thinking and reflection. The former primarily transforms ambiguous, colloquial, and informal editing instructions into clear, standardized, and actionable directives by constructing Thinking Pairs, which are structured as abstract-to-concrete instruction pairs. The latter is designed to perform iterative self-correction, refinement, and termination during the editing process by restructuring the paired image understanding as multiple cascaded single-image understanding tasks. We achieve this by constructing Reflection Triples that form an Figure 2. Illustration of the ReasonEdits reasoning capabilities. The thinking module illustrates how model decomposes abstract instructions into clear, actionable commands. The reflection pipeline, conversely, showcases the models ability to perform an iterative self-correction loop, refining an intermediate generated image to achieve more accurate final result. iterative cycle defined by three core image states: <original image, editing instructions, edited image, reflection instructions, reflection-corrected image, VIEScore [25]>. To train these image editing reasoning capabilities, our architecture integrates MLLM as the Reasoner and DiT as the Generator. We employ multi-stage training strategy: initially, the model is trained independently on image editing and thinking tasks, followed by joint training phase. This progressive approach simplifies the learning objectives at each stage, leading to smoother convergence and more effective, gradual acquisition of both editing and reasoning capabilities. Our contributions can be summarized as follows: reasoning-enhanced editing model that natively supports thinkingeditingreflection workflow. Thinking mode allows parsing original instructions leveraging the world knowledge of MLLMs, enabling the model to tackle more complex editing tasks. Reflection mode enables iterative refinement by reviewing and correcting the results of previous edits. comprehensive data construction pipeline consisting of <original image, editing instructions, edited image, reflection instructions, reflection-corrected image, VIEScore>, which supports end-to-end training of the thinkingeditingreflection loop. flexible training framework that demonstrates consistent performance gains by initializing our DiT from advanced models, such as ReasonEdit-S (based on Step1XEdit) improving ImgEdit (+4.3%), GEdit (+4.7%), and Kris (+8.2%); and ReasonEdit-Q (based on Qwen-ImageEdit) yielding ImgEdit (+2.8%), GEdit (+3.4%), and Kris (+6.1%), while outperforming previous open-source methods on GEdit and Kris. 2. Related Work 2.1. Image Editing Models such as BrushNet [22], BrushEdit [27], PowerPaint [60], and FLUX.1-Fill-dev [3], typically employ an edit-area mask together with textual instructions to achieve localized and high-quality edits. Beyond mask-based control, recent works have further explored enhancing editing controllability by incorporating multiple visual conditions. For instance, OminiControl [45], ACE [16], and ACE++ [36] unify diverse conditional signals such as depth maps and keypoints within single model, thereby enabling more flexible and versatile editing capabilities. While visual conditions offer precise control, they raise the usage threshold. In contrast, instruction-based models enable editing through natural language, but often struggle to align semantic understanding with fine-grained manipulation. Pioneering efforts such as Instruct-Pix2Pix [7], MagicBrush [56], UltraEdit [59], AnyEdit [55], and OmniGen [53] construct large-scale instructionimage pairs to support purely instruction-driven editing, yet still face challenges in fidelity and quality. Recent approaches address this challenge by leveraging priors from advanced text-toimage models [2, 4, 8], as in ICEdit [58], Hidream-E1 [17], and FLUX.1-Kontext-dev [5]. Another line of work integrates multimodal large language model encoders with diffusion decoders, such as Qwen2VL-Flux [34], MetaQueries [39], BLIP3-o [9], UniWorld-v1 [31], Step1XEdit [33], and Qwen-Image-Edit [50]. Although existing models have achieved notable progress in instruction-based editing, their reliance on frozen MLLM encoders limits performance on complex or abstract instructions. Motivated by this, ReasonEdit unlocks the reasoning capability of MLLMs through joint optimization with the diffusion decoder, thereby improving semantic understanding and extending the boundaries of controllable image editing. 2.2. Reasoning-Enhanced Visual Generation Diffusion models have demonstrated remarkable progress in generative modeling, particularly for producing highfidelity and diverse image editing results. Early approaches, The test-time scaling paradigm has rapidly extended from language to multimodal domains, giving rise to several reasoning-enhanced visual generation models. ThinkDiff [37] introduces multimodal in-context reasoning into diffusion models via think-then-diffuse inference scheme, while BAGEL [11] enables thinking mode by jointly training visual understanding and generation tasks. In addition to pre-thinking mechanisms before generation, some works explore reflection strategies to refine outputs, such as OmniGen2 [51] and Reflect-DiT [26]. Others, including ImageCoT [57], MINT [48], and IRG [20], employ multimodal chain-of-thought reasoning to guide generation. Beyond text-to-image, GoT [14] integrates reasoning with diffusion models using large-scale reasoning-chain data for controllable generation and editing. Uni-CoT [41] further decomposes multimodal chain-of-thought learning into macroand micro-level components with auxiliary tasks, enabling efficient training for complex reasoning. Compared with the above approaches, ReasonEdit focuses more on exploring thinking and reflection mechanisms for editing tasks, enhancing instruction understanding and editing accuracy. While Uni-CoT [41] is concurrent work, our method adopts different base models, training data composition, and training paradigm. 3. Method This section introduces the training data construction and the training of our model. We first elaborate on the construction of our edit reasoning data. Following this, we describe the training of the proposed REASONEDIT, in which we present the model design, the multi-stage training strategy. Finally, and thorough account of the specific training details, ensuring clarity of the our method. 3.1. Data Construction To facilitate supervised fine-tuning of our reasoning model, we have developed two distinct datasets: thinking and reflection. The former consists of abstract instruction-clear multi-step decomposition instruction pairs, while the latter includes triples that encompass multiple cascaded singleimage understanding tasks. Thinking Pairs consist of abstract-to-concrete instruction pairs. Each pair links an abstract instruction, which captures users original request in ambiguous, colloquial, or informal language, with its corresponding set of concrete, actionable commands. The concrete counterpart translates the initial user intent into one or more precise, standardized, and executable directives. For instance, the abstract entry symptoms of potassium deficiency in leaves is paired with the concrete command Render the leaves yellow and desiccate the leaf tips. For more complex requests, this structure facilitates logical decomposition into single, cascaded sequence of directives. As an illustration, multifaceted request like Make the image more dramatic with vintage feel is deconstructed into single, composite instruction: Increase the image contrast. Apply sepia tone filter. Add subtle vignette effect. To construct the Thinking Pairs dataset, we devised three-step process combining categorization, annotation, and review, leveraging different advanced Vision-Language Models (VLMs) as annotators. First, we classified large pool of raw instructions as either already clear or as abstract and complex. Then, in two-way annotation process, we generated the corresponding abstract instructions for the clear commands and decomposed the complex instructions into clear, actionable sub-directives. Finally, rigorous review ensured that each pair met our specific abstract-toconcrete requirements. We also supplemented the dataset with small number of simple instructions that did not require rewriting. This ensures our model can learn to handle both complex requests by decomposing them and simple requests by outputting them directly. The Thinking Pairs dataset is built from an initial 500k image-instruction pair pool, which we categorize into 112k complex and 388k simple instructions. After annotating the entire set, rigorous review process selects 150k highquality abstract-to-concrete pairs. Specifically, 62k of these pairs come from simplifying complex instructions, and 88k are created by adding an abstract layer to simple ones. To ensure versatility, we also include 50k pairs of simple, unedited instructions. The final dataset totals 200k carefully curated pairs. Reflection Triples is constructed from large collection of existing image-editing pairs, designed to facilitate models multi-step, cascaded reasoning capabilities. Each core triple consists of an <Input Image> , <Generated Image> , and <Target Image> . This structure models chained editing process: the <Generated Image> represents an intermediate output from an initial edit on the <Input Image> , providing the crucial context for multiround, single-image reflection process through which the model evaluates the generated output and performs subsequent adjustments to produce the refined <Target Image> . In some instances, the generated image and the target image are identical if no further edits are required. To mitigate hallucination issues inherent in single-pass, dual-image evaluation, we designed multi-round, singleimage reflection pipeline to enable robust reflection. This process begins by generating target image description based on the input image and instruction, which acts as faithful and concise blueprint for the intended outcome. quantitative evaluation then provides consistency score and rationale, with metrics designed to assess the presence of conflicts, omissions, and hallucinations. This comprehensive evaluation serves as strong prior for the final reflection and decision-making phase, where the model assesses the edits success using the original image, the generated image, and the original instruction as basis. The process yields one of three conclusions: Successful Edit: Indicated by reasoning content and <#Success> tag, signifying consistency between the generated and target images. Refinable Edit: For edits that are not fully successful but allow for refinement, reasoning content and <#Reflection> tag are returned, along with secondary editing instruction based on the generated image. Failed Edit: If an edit fails due to irrecoverable flaws, reasoning content and <#Failed> tag are provided. Furthermore, the model conducts final scoring of the generated image, assessing both semantic accuracy and image quality, to determine the optimal round at which the iterative process should terminate. The Reflection Triples is constructed from an initial pool of 500k image-editing pairs. To diversify the modalities of intermediate images, we generate an additional 500k images using four mainstream editing methods [5, 33, 38, 43]. We then apply our previously described reflection pipeline, utilizing an advanced VLM to automate the process. After final, rigorous manual screening, the curation yields 180k valid data pairs, with an approximate ratio of 3:1:1 for success, reflection, and failed examples. We then utilize GPT4.1 [38] to evaluate the VIEScore for each of these 180k valid data pairs. 3.2. Training With the reasoning-enhanced dataset, we utilize multistage training strategy to effectively integrate reasoning and image editing capabilities into single unified model. 3.2.1. Model Design As shown in Fig. 3, our model integrates an MLLM as the Reasoner and DiT [40] as Generator. Specifically, we directly adopt Step1X-Edit [33] and Qwen-Image-Edit [50] as our base architectures, which employ Qwen2.5VL 7B Instruct [1] for text embedding and DiT as their diffusion heads, initialized from these respective models. In contrast to the original base models, we enhance the MLLM and diffusion transformer with Thinking and Reflection capabilities on image editing. This is achieved through multistage training strategy and subsequent fine-tuning on our reasoning-enhanced dataset, thereby progressively refining the models performance. It is important to note that, while Step1X-Edit and Qwen-Image-Edit serve as our chosen implementations, our proposed method is broadly applicable across various image editing approaches. 3.2.2. Multi-stage Training Prior work highlights that such reconciliation often necessitates dedicated architectural advancements, for instance, in vision encoders [35, 42, 49], to mitigate conflicts during early joint training on both understanding and generation. To address these complex dynamics and effectively integrate enhanced reasoning with generative processes, we adopt multi-stage training strategy. This progressive approach decomposes the intricate joint optimization into simpler, focused tasks: initially cultivating the MLLMs explicit Thinking and Reflection, subsequently adapting the Generator (DiT) to these refined MLLM on image editing, and culminating in comprehensive joint fine-tuning of both components to achieve superior overall performance. REASONING LEARNING STAGE . This initial stage is dedicated to cultivating the MLLMs explicit Thinking and Reflection capabilities tailored for image editing tasks. To efficiently adapt the model while mitigating catastrophic forgetting of its foundational knowledge, and to isolate reasoning training, we employ Low-Rank Adaptation (LoRA) [18] on the linear layers in attention modules. During this phase, the DiT remains frozen. Training is conducted on the constructed Thinking Pairs and Reflection Triples datasets (cf. Sec. 3.1), optimizing with standard Next Token Prediction (NTP) loss, (cid:34) LNTP = Eti (cid:88) k=1 log pθ(tkt1, t2, , tk1) (1) (cid:35) where tk represents the k-th token in sequence of length L, and pθ is the probability predicted by the MLLM parameterized by θ. EDIT LEARNING STAGE . Following the dedicated tuning of the MLLMs reasoning abilities, this stage focuses on adapting the Generator, specifically the DiT model. To leverage the MLLMs refined contextual understanding without interference, its parameters are kept frozen throughout this phase. The DiT is trained using flow matching loss [32], with dual objective that encompasses both textto-image (T2I) generation and direct image editing tasks. Including T2I data is crucial; their significantly larger scale and broader domain coverage are instrumental in enriching the models general generative knowledge, which in turn substantially improves its proficiency in diverse editing scenarios. The flow matching loss is formulated as, LFM = EtU (0,1),x0D,x1N (0,I),cut(xc)vt(xx0, c)2 2 (2) where is uniformly sampled from [0, 1], x0 is data point from the dataset D, x1 is standard Gaussian noise, and represents the conditioning information (e.g., text or reference image). The DiT model ut is trained to predict the target vector field x1 x0 at the interpolated point xt = (1 t)x0 + tx1. UNIFIED TUNING STAGE . After the preceding stages, this final stage unifies and jointly fine-tunes both the MLLM Figure 3. The model architecture and inference pipeline of REASONEDIT are structured around two core components: the Thinking and Reflection processes which serve as the Reasoner, and DiT acting as the Generator. These Reasoner and Generator modules undergo multi-stage training process. During inference, they operate in an interleaved and sequential manner, progressively yielding more precise image editing results through their integrated Thinking and Reflection capabilities. and the DiT. This comprehensive joint optimization is crucial for ensuring that the understanding and generative processes seamlessly complement each other. The joint training loss for this stage is formulated as, Speed Ulysses [21]. For the DiT, both tensor parallelism and sequence parallelism were applied, enabling effective scaling across multiple nodes and GPUs and accelerating the training process. Ljoint = LFM + ωNTP LNTP (3) 3.2.3. Training Details We utilized Step1X-Edit V1.1 [44] and Qwen-ImageEdit [50] as our pretrained models. The first reasoning learning stage involved training the MLLM on 32 H800 GPUs (4 nodes, 8 GPUs/node) for 16 hours, completing 50,000 steps with an initial learning rate of 1 104. The second edit learning stage scaled to 128 GPUs (16 nodes, 8 GPUs/node), training for 38.9 hours and 28,000 steps at learning rate of 1105. During this stage, we use in-house 14.4M T2I samples and 2.4M image editing samples for training. The final stage consisted of 20 hours of training, completing 12,000 steps with learning rate of 6106 and the NTP loss weight ωNTP of 0.1. Similar to BAGEL [11] and Mogao [30], during this stage, FlexAttention [13] and packed data format [10] were utilized to support efficient hybrid training for both understanding and generation tasks, especially on the Reflection Triples. To optimize training performance and scalability, distributed training employed several parallelization strategies. Specifically, the MLLM and the Connector utilized sequence parallelism and Deep4. Experiments 4.1. Experimental Settings Benckmark. We conduct our experiments on three widelyused benchmarks: GEdit-Bench [33] and ImgEdit-Bench [54] for evaluating broad and comprehensive foundational image editing capabilities, and KRIS-Bench [52] for assessing models advanced reasoning skills and ability to interpret abstract instructions. These benchmarks collectively enable thorough evaluation of our models performance, ranging from foundational editing tasks to complex, abstract reasoning challenges. Metrics. For the GEdit-Bench, we evaluate performance using three metrics-Semantic Consistency (SQ), Perceptual Quality (PQ), and an Overall Score (O)-which are automatically assessed by VIEScore [24] using GPT-4.1. On the ImgEdit-Bench, we use GPT-4.1 to assign 1-5 ratings across three dimensions-instruction adherence, imageediting quality, and detail preservation-where the final score for the latter two is capped by instruction adherence. On the KRIS-Bench, we use GPT-4o to assign 1-5 ratings across four dimensions-Visual Consistency, Visual Quality, InFigure 4. Comprehensive qualitative evaluation of leading image editing models. The results demonstrate that our proposed approach, which incorporates thinking and reflection mechanisms, significantly outperforms the editing model. struction Following, and the novel Knowledge Plausibility, which assesses consistency with real-world knowledge. 4.2. Experimental Results Quantitative results are first reported on GEdit-Bench [33]and ImgEdit-Bench [54] to assess foundational editing capabilities (cf. Sec. 4.2.1), with the evaluation then shifting to the more complex KRIS-Bench [52] for an assessment of abstract reasoning skills (cf. Sec. 4.2.2). 4.2.1. Evaluation on GEdit-Bench and ImgEdit-Bench shown in Table 1, our method achieves As superior performance on the foundational instruction benchmarks ImgEdit-Bench [54] and GEdit-Bench [33]. Both ReasonEdit-S and ReasonEdit-Q exhibit consistent and substantial improvements over their respective base models, achieving gains of +4.3% and +4.7% on ImgEdit and GEdit for ReasonEdit-S, and +2.8% and +3.4% for ReasonEditQ, respectively. Importantly, although the two methods are built upon different underlying models - Step1X-Edit v1.1 for ReasonEdit-S and Qwen-Image-Edit for ReasonEditQ - both variants significantly surpass the performance of their corresponding underlying editors. ReasonEdit-S ranks third on GEdit-Bench, outperforming Qwen-Image-Edit, while ReasonEdit-Q achieves the highest overall score. On ImgEdit-Bench, ReasonEdit-S and ReasonEdit-Q place second and third among all open-source models, trailing the top entry by only 0.08 and 0.12 points, respectively. GEdit-Bench and ImgEdit-Bench primarily evaluate models foundational editing capabilities. While our thinking and reflection mechanisms provide performance gains, their full impact may be less pronounced on these relatively simple tasks compared to more complex ones. This is consistent with the design of our dataset, where the thinking and reflection modules are specifically tailored for complex instructions and multi-step editing. As shown in Fig. 4, qualitative comparison demonstrates that our approach excels at precisely altering target areas while faithfully maintaining the integrity of unedited regions, such as backgrounds, facial features, and hairstyles. This capability addresses key challenge in image editing by effectively mitigating common failures related to consistency and fidelity, resulting in stable performance and accurate responsiveness to wide range of commands. 4.2.2. Evaluation on Kris-Bench On the KRIS-Bench [52], the proposed approach demonstrates the best performance among all open-source modincluding those that also employ thinking-based els, mechanism (e.g., BAGEL-Thinking), and surpasses several closed-source methods. This superiority is further substantiated by substantial improvements over their respective base models (w/o thinking, w/o reflection): ReasonEdit-S (built upon Step1X-Edit v1.1 [33]) attains performance gain of +8.2%, while ReasonEdit-Q (derived from QwenTable 1. Comprehensive quantitative evaluation of leading image editing models. We validate our approach using two variants: ReasonEditS (initialized from Step1X-Edit v1.1) and ReasonEdit-Q (initialized from Qwen-Image-Edit). Our method achieves significant performance gains over the base models (w/o thinking, w/o reflection) and achieves state-of-the-art performance among open-source models on both GEdit and Kris (with ReasonEdit-Q), while also proving to be highly competitive with several closed-source models. close-source models open-source models Models Gemini 2 flash (Apr. 2025) Gemini 2.5 flash (Sep. 2025) Doubao (seed edit 1.6, Apr. 2025) Doubao (Seedream 4.0, Aug. 2025) GPT4o (Apr. 2025) GPT4o (Sep. 2025) ICEdit [58] Omnigen [53] Omnigen 2 [51] BAGEL-thinking [11] BAGEL [11] Uniworld-V1 [31] Hidream-I1 (E1) [17] Hidream-E1.1 [17] Flux-Kontext-dev [5] UniWorld-FLUX.1-Kontext-Dev [29] UniWorld-Qwen-Image-Edit [29] Step1X-Edit v1.1 [33] ReasonEdit-S (base) ReasonEdit-S (thinking) ReasonEdit-S (thinking+reflection) Qwen-Image-Edit-2509 [50] ReasonEdit-Q (base) ReasonEdit-Q (thinking) ReasonEdit-Q (thinking+reflection) GEdit-Bench KRIS-Bench ImgEdit-Bench Semantic Consistency Quality Overall Factual Knowledge Conceptual Knowledge Procedural Knowledge Overall Overall 6.87 8.25 7.22 9.17 7.74 8.74 4.94 5.88 7.16 7.70 7.48 4.93 5.66 7.15 7.16 7.28 8.36 7.66 7.77 8.02 8.18 8.00 8.12 8.20 8.34 7.44 8.29 7.89 7.95 8.13 7.67 7.39 5.87 6.77 6.51 6.80 7.43 6.06 6.65 7.37 7.49 7.87 7.35 7.65 7.64 7.85 7.86 7.94 7.96 7.97 6.51 7.89 6.98 8.40 7.49 8. 4.87 5.01 6.41 6.66 6.60 4.85 5.01 6.42 6.51 6.74 7.76 6.97 7.24 7.36 (+1.7%) 7.58 (+4.7%) 7.56 7.51 7.61 (+1.3%) 7.77 (+3.4%) 65.26 77.03 63.30 78.10 79.80 81.16 46.99 33.11 57.36 66.18 60.26 47.71 43.31 43.52 53.28 55.50 61.72 53.05 58.23 59.79 62.44 61.47 62.29 62.44 63.92 59.65 78.29 62.23 76.86 81.37 78.24 42.73 28.02 44.20 61.92 55.86 44.80 50.05 44.71 50.36 51.39 56.38 54.34 60.55 62.76 65.72 56.79 62.22 64.49 64.85 62.90 75.93 54.17 76.93 78.32 77. 27.76 23.89 47.79 49.02 51.69 47.92 37.64 36.08 42.53 43.76 46.69 44.66 46.21 49.78 50.42 47.07 44.53 52.02 52.41 62.41 77.29 60.70 77.31 80.09 79.00 40.70 28.85 49.71 60.18 56.21 50.27 44.72 42.25 49.54 51.04 55.98 51.59 56.33 58.64 (+4.1%) 60.93 (+8.2%) 56.15 58.05 60.81(+4.8%) 61.57 (+6.1%) - 4.30 - 4.46 - 4.30 3.05 2.96 3.44 3.56 3.20 3.26 3.17 3.97 3.97 4.02 4.48 3.90 4.22 4.18 (-0.9%) 4.40 (+4.3%) 4.27 4.24 4.27 (+0.7%) 4.36 (+2.8%) Image-Edit [50]) achieves an improvement of +6.1%. 4.3. Ablation Studies The performance gains are attributed to the methods ability to simplify abstract and difficult editing tasks into clear, actionable steps for the editing model. Furthermore, the reflection pipeline provides crucial mechanism to analyze the correctness of an edit and formulate strategies for improvement. This iterative process of self-correction allows the model to identify and rectify subtle errors, effectively mitigating hallucination and improving overall fidelity. The methods demonstrated effectiveness on both complex and simple tasks (cf. Sec. 4.2.1) proves its versatility and robust generalization. Furthermore, As shown in Fig. 4, many methods often misinterpret or fail to respond correctly to abstract or complex instructions. Our proposed thinking module effectively aids the editing model in understanding such instructions and executing them accurately. the reflection pipeline enhances this process by enabling the model to identify and rectify subtle errors, formulate precise refinement strategies, and prevent the compounding of mistakes that are common in multi-step editing tasks. Simultaneously, many models struggle with maintaining consistency in complex scenarios, often leading to unintended alterations in unedited regions because they lack robust understanding of the entire scenes structure. In contrast, our approach ensures high consistency by faithfully preserving elements that should remain unchanged. To systematically evaluate the contribution of each component of the proposed method, series of ablation studies on ReasonEdit-S (built upon Step1X-Edit v1.1 [33] and the MLLM Qwen2.5VL 7B Instruct, hereafter Qwen) using KRIS-Bench, as its abstract and challenging nature makes it an ideal testbed for verifying the reasoning and reflection capabilities of the model. Table 2. Ablation of Multi-Stage Training. This table evaluates the performance contributions of each stage in the training pipeline, from the pre-trained baseline to the final unified model, highlighting the cumulative benefits of fine-tuning the generator and reasoning modules at each step. Methods KRIS-Bench Factual Knowledge Conceptual Knowledge Procedural Knowledge Overall Pre-trained Generator (Step1X-Edit V1.1 [33]) Pre-trained Generator + Qwen Reasoning Pre-trained Generator + Qwen-tuned Reasoning Base Generator W/O Reasoning Base Generator + Qwen-tuned Reasoning Unified Tuned (ReasonEdit-S) 53.05 54.05 56.32 55.80 60.54 62.44 54.34 57.44 62.00 55.28 62.16 65.72 44.66 41.26 46.17 43.78 48.26 50.42 51.59 52.41 56.24 52.74 58.29 60.93 Impact of Multi-Stage Training. To evaluate the contribution of the reasoning learning stage, we compare the performance of the Pre-trained Generator (Step1X-Edit v1.1 [33]) when integrated with either base (untuned) Qwen model or fine-tuned Qwen model. When the Pretrained Generator is augmented with the base Qwen model leveraging our thinking and reflection mechanism, only marginal performance gain of 0.82 points is observed. In contrast, fine-tuning Qwen on our reasoning data consistently and significantly outperforms this base configuration. This highlights that foundational multimodal large language models, without domain-specific adaptation, struggle to effectively grasp the nuances of image editing, thereby underscoring the critical necessity of tailoring the MLLM to these specific demands. After the edit learning stage, in isolation, the Base Generator achieves degree of performance improvement over the Pre-trained Generator, demonstrating its role in adapting the generative capabilities. Finally, this multi-stage strategy culminates in the optimal performance of the unified training, providing substantial performance increase from the Base Generator + Qwen-tuned Reasoning model to the Unified Tuned model (58.29 vs. 60.93), validating the synergistic benefits of training the entire pipeline as whole. Table 3. Ablation Study on the Contributions of the Thinking and Reflection Modules. The table shows the performance of four model variants on the KRIS-Bench, demonstrating the benefits of each component and the synergy of their combination. Methods KRIS-Bench Factual Knowledge Conceptual Knowledge Procedural Knowledge Overall ReasonEdit-S (w/o thinking, w/o reflection) ReasonEdit-S (w/ thinking, w/o reflection) ReasonEdit-S (w/o thinking, w/ reflection) ReasonEdit-S (w/ thinking, w/ reflection) 58.23 59.79 61.40 62.44 60.55 62.76 64.16 65.72 46.21 49.78 48.16 50. 56.33 58.64 59.39 60.93 Ablation of Thinking and Reflection. To understand the individual and combined contributions of the thinking and reflection modules, four variants are compared: (1) baseline model without either module; (2) model with only the thinking module; (3) model with only the reflection module; and (4) the full model incorporating both. The results on KRIS-Bench (see Tab. 3) show gradual improvement in performance with the addition of each component. The thinking module alone provides significant performance boost, confirming its effectiveness in handling complex instructions. The thinking + reflection module proves beneficial across all benchmarks (see Tab. 1), as it effectively rectifies errors. The full model, with both modules integrated, achieves the highest scores, highlighting the synergistic relationship between understanding an instruction and correcting subsequent errors. Table 4. Ablation Study on Reflection Pipelines. The table compares three different reflection mechanisms on KRIS-Bench, highlighting the effectiveness of the proposed multi-round pipeline. Methods KRIS-Bench Factual Knowledge Conceptual Knowledge Procedural Knowledge Overall Base Generator Base Generator + dual-image pipeline Base Generator + single-image pipeline Base Generator + our multi-round pipeline 55.80 52.97 54.81 60.54 55.28 61.84 56.92 62. 43.78 41.12 43.70 48.26 52.74 53.79 53.04 58.29 Comparison of Reflection Pipelines. To ensure consistency in the DiT parameters, this ablation study is conducted by combining the Base Generator (the DiT after the edit learning stage) with each reflection pipeline. Tab. 4 compares three distinct approaches to the reflection processa dual-image pipeline, pure single-image pipeline, and the proposed multi-round prior pipeline. The dualimage pipeline, which relies on direct comparison between the initial input and the generated output, is often prone to hallucinations. Conversely, pure single-image approach struggles with tasks that require clear beforeand-after comparison, such as Portrait Beautification or motion/expression-related edits. As shown in the table, the proposed multi-round single-image prior pipeline is superior. This is attributed to the methods ability to combine the benefits of both approaches, allowing it to perform selfcorrection loop on the generated image itself while leveraging key prior information from the multi-round process. Reflection Performance Curve. The effect of varying reflection rounds is evaluated on KRIS-Bench using ReasonEdit-S (see Tab. 5). The results show that incorporating reflection consistently improves performance over the Thinking-only baseline (58.64). Specifically, two reflection rounds yield score of 60.93, while extending to three or four rounds brings only marginal improvements (+0.06 and +0.14, respectively) with higher computational cost. We further evaluate naive re-roll baseline (see Tab. 6). This simpler strategy yields only minor gains, peaking at 59.24 after three attempts, and even drops to 59.09 at four attempts with increased cost. The comparison indicates that the targeted reflection mechanism markedly outperforms an unguided re-roll strategy, highlighting the role of structured reasoning in iterative error correction (see Fig. 5). Table 5. Performance-efficiency curve of reflection rounds. Reflection Rounds 0 (Thinking) 1 2 3 Performance Time(s) 58.64 40 60.08 80 60.93 120 60.99 160 61.07 Table 6. Performance-efficiency curve of re-roll times. Re-roll Times 0 (Thinking) 1 2 4 Performance Time(s) 58.64 39 58.84 78 59.00 117 59.24 59.09 195 Figure 5. PerformanceEfficiency Curve. 5. Conclusion In this work, we present ReasonEdit, fundamental image editing framework that demonstrates the crucial role of explicit reasoning in achieving robust and versatile performance. The proposed method introduces novel pipeline with two core capabilities: thinking and reflection. By training these capabilities on curated collection of Thinking Pairs and Reflection Triples, the framework learns to convert abstract user requests into actionable commands and to perform self-correction in an iterative loop. Extensive experiments on range of benchmarks validate the efficacy of this approach, with the model achieving state-of-the-art performance among open-source methods while remaining highly competitive with several closed-source models. This work provides new perspective on reasoningenhanced image editing, showing that structured pipeline for instruction understanding and self-correction is vital for building models that can handle both simple and complex editing tasks with high fidelity and consistency."
        },
        {
            "title": "Contributors and Acknowledgments",
            "content": "Core Contributors: Fukun Yin, Shiyu Liu, Yucheng Han, Zhibo Wang, Peng Xing, Rui Wang, Wei Cheng, Yingming Wang, Aojie Li, Zixin Yin, Pengtao Chen, Xiangyu Zhang, Daxin Jiang, Xianfang Zeng, Gang Yu. Corresponding Authors: Xianfang Zeng (zengxianfang@stepfun.com), Gang Yu (yugang@stepfun.com), Daxin Jiang (djiang@stepfun.com)."
        },
        {
            "title": "References",
            "content": "[1] Shuai Bai, Keqin Chen, Xuejing Liu, Jialin Wang, Wenbin Ge, Sibo Song, Kai Dang, Peng Wang, Shijie Wang, Jun Tang, Humen Zhong, Yuanzhi Zhu, Mingkun Yang, Zhaohai Li, Jianqiang Wan, Pengfei Wang, Wei Ding, Zheren Fu, Yiheng Xu, Jiabo Ye, Xi Zhang, Tianbao Xie, Zesen Cheng, Hang Zhang, Zhibo Yang, Haiyang Xu, and JunarXiv preprint yang Lin. Qwen2.5-vl technical report. arXiv:2502.13923, 2025. 4 [2] Black Forest Labs. Flux.1 [dev]. https://huggingface. co/black-forest-labs/FLUX.1-dev, 2024. 2 [3] Black Forest Labs. https : / / Flux.1 fill huggingface.co/blackforestlabs/FLUX.1Filldev, 2024. Accessed: 2025-04-19. 2 [dev]. [4] Black Forest Labs. https : / / huggingface.co/black-forest-labs/FLUX.1-schnell, 2024. 2 Flux.1 [schnell]. [5] BlackForestLabs, Stephen Batifol, Andreas Blattmann, Frederic Boesel, Saksham Consul, Cyril Diagne, Tim Dockhorn, Jack English, Zion English, Patrick Esser, Sumith Kulal, Kyle Lacey, Yam Levi, Cheng Li, Dominik Lorenz, Jonas Muller, Dustin Podell, Robin Rombach, Harry Saini, Axel Sauer, and Luke Smith. Flux.1 kontext: Flow matching for in-context image generation and editing in latent space, 2025. 2, 4, 7 [6] Manuel Brack, Felix Friedrich, Katharia Kornmeier, Linoy Tsaban, Patrick Schramowski, Kristian Kersting, and Apolinario Passos. Ledits++: Limitless image editing using text-to-image models. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 88618870, 2024. 1 [7] Tim Brooks, Aleksander Holynski, and Alexei A. Efros. Instructpix2pix: Learning to follow image editing instructions. 2023 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pages 1839218402, 2022. 1, [8] Qi Cai, Jingwen Chen, Yang Chen, Yehao Li, Fuchen Long, Yingwei Pan, Zhaofan Qiu, Yiheng Zhang, Fengbin Gao, Peihan Xu, et al. Hidream-i1: high-efficient image generative foundation model with sparse diffusion transformer. arXiv preprint arXiv:2505.22705, 2025. 2 [9] Jiuhai Chen, Zhiyang Xu, Xichen Pan, Yushi Hu, Can Qin, Tom Goldstein, Lifu Huang, Tianyi Zhou, Saining Xie, Silvio Savarese, et al. Blip3-o: family of fully open unified multimodal models-architecture, training and dataset. arXiv preprint arXiv:2505.09568, 2025. 2 [10] Mostafa Dehghani, Basil Mustafa, Josip Djolonga, Jonathan Heek, Matthias Minderer, Mathilde Caron, Andreas Steiner, Joan Puigcerver, Robert Geirhos, Ibrahim Alabdulmohsin, et al. Patch npack: Navit, vision transformer for any aspect ratio and resolution. Advances in Neural Information Processing Systems, 36:22522274, 2023. 5 [11] Chaorui Deng, Deyao Zhu, Kunchang Li, Chenhui Gou, Feng Li, Zeyu Wang, Shu Zhong, Weihao Yu, Xiaonan Nie, Ziang Song, et al. Emerging properties in unified multimodal pretraining. arXiv preprint arXiv:2505.14683, 2025. 1, 3, 5, 7 [12] Peng Ding, Jingyu Wu, Jun Kuang, Dan Ma, Xuezhi Cao, Xunliang Cai, Shi Chen, Jiajun Chen, and Shujian Huang. Hallu-pi: Evaluating hallucination in multi-modal large language models within perturbed inputs. In Proceedings of the 32nd ACM International Conference on Multimedia, pages 1070710715, 2024. 1 [13] Juechu Dong, Boyuan Feng, Driss Guessous, Yanbo Liang, and Horace He. Flex attention: programming model for generating optimized attention kernels. arXiv preprint arXiv:2412.05496, 2024. [14] Rongyao Fang, Chengqi Duan, Kun Wang, Linjiang Huang, Hao Li, Shilin Yan, Hao Tian, Xingyu Zeng, Rui Zhao, Jifeng Dai, Xihui Liu, and Hongsheng Li. Got: Unleashing reasoning capability of multimodal large language arXiv preprint model for visual generation and editing. arXiv:2503.10639, 2025. 3 [15] Xingyu Fu, Yushi Hu, Bangzheng Li, Yu Feng, Haoyu Wang, Xudong Lin, Dan Roth, Noah Smith, Wei-Chiu Ma, and Ranjay Krishna. Blink: Multimodal large language models can see but not perceive. In European Conference on Computer Vision, pages 148166. Springer, 2024. 1 [16] Zhen Han, Zeyinzi Jiang, Yulin Pan, Jingfeng Zhang, Chaojie Mao, Chen-Wei Xie, Yu Liu, and Jingren Zhou. ACE: All-round creator and editor following instructions via diffusion transformer. In The Thirteenth International Conference on Learning Representations, 2025. 2 [17] HiDream-ai. Hidream-e1. https://github.com/HiDreamai/HiDream-E1, 2025. 2, 7 [18] Edward Hu, Yelong Shen, Phillip Wallis, Zeyuan AllenZhu, Yuanzhi Li, Shean Wang, Lu Wang, and Weizhu Chen. LoRA: Low-rank adaptation of large language models. In International Conference on Learning Representations, 2022. [19] Wen Huang, Hongbin Liu, Minxin Guo, and Neil Gong. Visual hallucinations of multi-modal large language models. In Findings of the Association for Computational Linguistics: ACL 2024, pages 96149631, 2024. 1 [20] Wenxuan Huang, Shuang Chen, Zheyong Xie, Shaosheng Cao, Shixiang Tang, Yufan Shen, Qingyu Yin, Wenbo Hu, Xiaoman Wang, Yuntian Tang, et al. Interleaving reasoning for better text-to-image generation. arXiv preprint arXiv:2509.06945, 2025. 1, 3 [21] Sam Ade Jacobs, Masahiro Tanaka, Chengming Zhang, Minjia Zhang, Reza Yazdani Aminadabi, Shuaiwen Leon Song, Samyam Rajbhandari, and Yuxiong He. System optimizations for enabling training of extreme long sequence transformer models. In Proceedings of the 43rd ACM Symposium on Principles of Distributed Computing, page 121130, New York, NY, USA, 2024. Association for Computing Machinery. 5 [22] Xuan Ju, Xian Liu, Xintao Wang, Yuxuan Bian, Ying Shan, and Qiang Xu. Brushnet: plug-and-play image inpainting model with decomposed dual-branch diffusion. In European Conference on Computer Vision, pages 150168. Springer, 2024. 1, 2 [23] Bahjat Kawar, Shiran Zada, Oran Lang, Omer Tov, Huiwen Chang, Tali Dekel, Inbar Mosseri, and Michal Irani. Imagic: Text-based real image editing with diffusion models. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 60076017, 2023. 1 [24] Max Ku, Dongfu Jiang, Cong Wei, Xiang Yue, and Wenhu Chen. Viescore: Towards explainable metrics for arXiv preprint conditional arXiv:2312.14867, 2023. image synthesis evaluation. [25] Max Ku, Dongfu Jiang, Cong Wei, Xiang Yue, and Wenhu Chen. Viescore: Towards explainable metrics for conditional image synthesis evaluation. In Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 1226812290, 2024. 2 [26] Shufan Li, Konstantinos Kallidromitis, Akash Gokul, Arsh Koneru, Yusuke Kato, Kazuki Kozuka, and Aditya Grover. Reflect-dit: Inference-time scaling for text-to-image diffusion transformers via in-context reflection. arXiv preprint arXiv:2503.12271, 2025. 1, 3 [27] Yaowei Li, Yuxuan Bian, Xu Ju, Zhaoyang Zhang, Ying Shan, and Qiang Xu. Brushedit: All-in-one image inpainting and editing. ArXiv, abs/2412.10316, 2024. 2 [28] Zijie Li, Henry Li, Yichun Shi, Amir Barati Farimani, Yuval Kluger, Linjie Yang, and Peng Wang. Dual diffusion for unified image generation and understanding. In Proceedings of the Computer Vision and Pattern Recognition Conference, pages 27792790, 2025. 1 [29] Zongjian Li, Zheyuan Liu, Qihui Zhang, Bin Lin, Shenghai Yuan, Zhiyuan Yan, Yang Ye, Wangbo Yu, Yuwei Niu, and Li Yuan. Uniworld-v2: Reinforce image editing with diffusion negative-aware finetuning and mllm implicit feedback. arXiv preprint arXiv:2510.16888, 2025. 7 [30] Chao Liao, Liyang Liu, Xun Wang, Zhengxiong Luo, Xinyu Zhang, Wenliang Zhao, Jie Wu, Liang Li, Zhi Tian, and Weilin Huang. Mogao: An omni foundation model arXiv preprint for interleaved multi-modal generation. arXiv:2505.05472, 2025. [31] Bin Lin, Zongjian Li, Xinhua Cheng, Yuwei Niu, Yang Ye, Xianyi He, Shenghai Yuan, Wangbo Yu, Shaodong Wang, Yunyang Ge, et al. Uniworld: High-resolution semantic encoders for unified visual understanding and generation. arXiv preprint arXiv:2506.03147, 2025. 2, 7 [32] Yaron Lipman, Ricky T. Q. Chen, Heli Ben-Hamu, Maximilian Nickel, and Matthew Le. Flow matching for generative modeling. In The Eleventh International Conference on Learning Representations, 2023. 4 [33] Shiyu Liu, Yucheng Han, Peng Xing, Fukun Yin, Rui Wang, Wei Cheng, Jiaqi Liao, Yingming Wang, Honghao Fu, Chunrui Han, Guopeng Li, Yuang Peng, Quan Sun, Jingwei Wu, Yan Cai, Zheng Ge, Ranchen Ming, Lei Xia, Xianfang Zeng, Yibo Zhu, Binxing Jiao, Xiangyu Zhang, Gang Yu, and Daxin Jiang. Step1x-edit: practical framework for general image editing. arXiv preprint arXiv:2504.17761, 2025. 1, 2, 4, 5, 6, 7 [34] Pengqi Lu. Qwen2vl-flux: Unifying image and text guidance for controllable image generation, 2024. [35] Yiyang Ma, Xingchao Liu, Xiaokang Chen, Wen Liu, Chengyue Wu, Zhiyu Wu, Zizheng Pan, Zhenda Xie, Haowei Zhang, Xingkai Yu, et al. Janusflow: Harmonizing autoregression and rectified flow for unified multimodal understanding and generation. In Proceedings of the Computer Vision and Pattern Recognition Conference, pages 77397751, 2025. 4 [36] Chaojie Mao, Jingfeng Zhang, Yulin Pan, Zeyinzi Jiang, Zhen Han, Yu Liu, and Jingren Zhou. Ace++: Instructionbased image creation and editing via context-aware content filling. arXiv preprint arXiv:2501.02487, 2025. 2 [37] Zhenxing Mi, Kuan-Chieh Wang, Guocheng Qian, Hanrong Ye, Runtao Liu, Sergey Tulyakov, Kfir Aberman, and Dan Xu. think, therefore diffuse: Enabling multimodal incontext reasoning in diffusion models. In Forty-second International Conference on Machine Learning, 2025. 3 [38] OpenAI. Introducing 4o image generation, 2025. 4 [39] Xichen Pan, Satya Narayan Shukla, Aashu Singh, Zhuokai Zhao, Shlok Kumar Mishra, Jialiang Wang, Zhiyang Xu, Jiuhai Chen, Kunpeng Li, Felix Juefei-Xu, et al. Transfer between modalities with metaqueries. arXiv preprint arXiv:2504.06256, 2025. 2 [40] William Peebles and Saining Xie. Scalable diffusion models with transformers. In Proceedings of the IEEE/CVF international conference on computer vision, pages 41954205, 2023. 4 [41] Luozheng Qin, Jia Gong, Yuqing Sun, Tianjiao Li, Mengping Yang, Xiaomeng Yang, Chao Qu, Zhiyu Tan, and Hao Li. Uni-cot: Towards unified chain-of-thought reasoning across text and vision. arXiv preprint arXiv:2508.05606, 2025. [42] Liao Qu, Huichao Zhang, Yiheng Liu, Xu Wang, Yi Jiang, Yiming Gao, Hu Ye, Daniel Du, Zehuan Yuan, and Xinglong Wu. Tokenflow: Unified image tokenizer for multimodal understanding and generation. In Proceedings of the Computer Vision and Pattern Recognition Conference, pages 25452555, 2025. 4 [43] Yichun Shi, Peng Wang, and Weilin Huang. Seededit: Align image re-generation to image editing. arXiv preprint arXiv:2411.06686, 2024. 4 fied image editing dataset and benchmark. arXiv preprint arXiv:2505.20275, 2025. 5, 6 [55] Qifan Yu, Wei Chow, Zhongqi Yue, Kaihang Pan, Yang Wu, Xiaoyang Wan, Juncheng Li, Siliang Tang, Hanwang Zhang, and Yueting Zhuang. Anyedit: Mastering unified high-quality image editing for any idea. In Proceedings of the Computer Vision and Pattern Recognition Conference, pages 2612526135, 2025. 2 [56] Kai Zhang, Lingbo Mo, Wenhu Chen, Huan Sun, and Yu Su. Magicbrush: manually annotated dataset for instructionguided image editing. Advances in Neural Information Processing Systems, 36:3142831449, 2023. [57] Renrui Zhang, Chengzhuo Tong, Zhizheng Zhao, Ziyu Guo, Haoquan Zhang, Manyuan Zhang, Jiaming Liu, Peng Gao, and Hongsheng Li. Lets verify and reinforce image generation step by step. In Proceedings of the Computer Vision and Pattern Recognition Conference (CVPR), pages 28662 28672, 2025. 1, 3 [58] Zechuan Zhang, Ji Xie, Yu Lu, Zongxin Yang, and Yi Yang. In-context edit: Enabling instructional image editing with in-context generation in large scale diffusion transformer. arXiv, 2025. 2, 7 [59] Haozhe Zhao, Xiaojian Shawn Ma, Liang Chen, Shuzheng Si, Rujie Wu, Kaikai An, Peiyu Yu, Minjia Zhang, Qing Li, and Baobao Chang. Ultraedit: Instruction-based fine-grained image editing at scale. Advances in Neural Information Processing Systems, 37:30583093, 2024. 2 [60] Junhao Zhuang, Yanhong Zeng, Wenran Liu, Chun Yuan, and Kai Chen. task is worth one word: Learning with task prompts for high-quality versatile image inpainting. In European Conference on Computer Vision, pages 195211. Springer, 2024. 1, 2 [44] StepFun AI. https : / / Step1x-edit v1.1 diffusers. huggingface . co / stepfun - ai / Step1X - Edit - v1p1 - diffusers, 2025. Accessed: 2025-09-23. [45] Zhenxiong Tan, Songhua Liu, Xingyi Yang, Qiaochu Xue, and Xinchao Wang. Ominicontrol: Minimal and universal control for diffusion transformer. In Proceedings of the IEEE/CVF International Conference on Computer Vision, 2025. 2 [46] NextStep Team, Chunrui Han, Guopeng Li, Jingwei Wu, Quan Sun, Yan Cai, Yuang Peng, Zheng Ge, Deyu Zhou, Haomiao Tang, et al. Nextstep-1: Toward autoregressive image generation with continuous tokens at scale. arXiv preprint arXiv:2508.10711, 2025. 1 [47] Fei Wang, Xingyu Fu, James Y. Huang, Zekun Li, Qin Liu, Xiaogeng Liu, Mingyu Derek Ma, Nan Xu, Wenxuan Zhou, Kai Zhang, Tianyi Lorena Yan, Wenjie Jacky Mo, HsiangHui Liu, Pan Lu, Chunyuan Li, Chaowei Xiao, Kai-Wei Chang, Dan Roth, Sheng Zhang, Hoifung Poon, and Muhao Chen. Muirbench: comprehensive benchmark for robust multi-image understanding. In The Thirteenth International Conference on Learning Representations, 2025. 1 [48] Yi Wang, Mushui Liu, Wanggui He, Longxiang Zhang, Ziwei Huang, Guanghao Zhang, Fangxun Shu, Zhong Tao, Dong She, Zhelun Yu, et al. Mint: Multi-modal chain of thought in unified generative models for enhanced image generation. arXiv preprint arXiv:2503.01298, 2025. 1, 3 [49] Chengyue Wu, Xiaokang Chen, Zhiyu Wu, Yiyang Ma, Xingchao Liu, Zizheng Pan, Wen Liu, Zhenda Xie, Xingkai Yu, Chong Ruan, et al. Janus: Decoupling visual encoding for unified multimodal understanding and generation. In Proceedings of the Computer Vision and Pattern Recognition Conference, pages 1296612977, 2025. 4 [50] Chenfei Wu, Jiahao Li, Jingren Zhou, Junyang Lin, Kaiyuan Gao, Kun Yan, Sheng ming Yin, Shuai Bai, Xiao Xu, Yilei Chen, Yuxiang Chen, Zecheng Tang, Zekai Zhang, Zhengyi Wang, An Yang, Bowen Yu, Chen Cheng, Dayiheng Liu, Deqing Li, Hang Zhang, Hao Meng, Hu Wei, Jingyuan Ni, Kai Chen, Kuan Cao, Liang Peng, Lin Qu, Minggang Wu, Peng Wang, Shuting Yu, Tingkun Wen, Wensen Feng, Xiaoxiao Xu, Yi Wang, Yichang Zhang, Yongqiang Zhu, Yujia Wu, Yuxuan Cai, and Zenan Liu. Qwen-image technical report, 2025. 1, 2, 4, 5, 7 [51] Chenyuan Wu, Pengfei Zheng, Ruiran Yan, Shitao Xiao, Xin Luo, Yueze Wang, Wanli Li, Xiyan Jiang, Yexin Liu, Junjie Zhou, et al. Omnigen2: Exploration to advanced multimodal generation. arXiv preprint arXiv:2506.18871, 2025. 1, 3, 7 [52] Yongliang Wu, Zonghui Li, Xinting Hu, Xinyu Ye, Xianfang Zeng, Gang Yu, Wenbo Zhu, Bernt Schiele, MingHsuan Yang, and Xu Yang. Kris-bench: Benchmarking next-level intelligent image editing models. arXiv preprint arXiv:2505.16707, 2025. 5, [53] Shitao Xiao, Yueze Wang, Junjie Zhou, Huaying Yuan, Xingrun Xing, Ruiran Yan, Chaofan Li, Shuting Wang, Tiejun Huang, and Zheng Liu. Omnigen: Unified image generaIn Proceedings of the Computer Vision and Pattern tion. Recognition Conference, pages 1329413304, 2025. 1, 2, 7 [54] Yang Ye, Xianyi He, Zongjian Li, Bin Lin, Shenghai Yuan, Imgedit: uniZhiyuan Yan, Bohan Hou, and Li Yuan. A. Appendix A.1. Illustration of Reflection Pipelines We conduct an ablation study on the proposed multi-round reflection pipeline against two alternative designs, as presented in Table 4 and further illustrated in Fig. 6. The Dual Image Reflection pipeline directly inputs the reference image, edit instruction, and result image, tasking the MLLM with generating thinking and reflection concurrently. Nevertheless, we found that current MLLMs frequently exhibit hallucinations in image editing tasks under this unified input scheme. Our investigation then led to the Single Image Reflection pipeline, which first guides the MLLM to describe the target image based on the edit instruction and reference image. Subsequently, using this target description, the MLLM evaluates the result image, offering detailed reasoning, identifying failures, suggesting refinements, or confirming success. key drawback here is that the MLLM loses the essential context of the reference image during its final conclusion, leading to less effective assessments. The proposed pipeline addresses these limitations by decomposing the reflection into three distinct sub-procedures: target description, result assessment, and refinement conclusion. In the initial two sub-procedures, the MLLM receives only single image as input (e.g., reference image for target description, result image for assessment), which significantly enhances accuracy. In the final stage, the MLLM is provided with all relevant information to formulate comprehensive conclusion for subsequent actions, thereby maintaining full contextual awareness. B. More Results To further complement the quantitative analyses in the main paper, this section presents additional qualitative results demonstrating the breadth and robustness of our framework. As shown in Figures 710, these examples cover diverse editing instructions, multi-round reasoning-and-reflection processes, and comparisons against state-of-the-art methods. C. Failure Cases To provide more comprehensive evaluation of ReasonEdit, we also explicitly analyze scenarios where our model produces unsatisfactory results. As illustrated in Fig. 11, we present representative failure cases encountered during our experiments. We believe that analyzing these limitations will inspire future work to further improve the robustness of reasoning-based editing models. Figure 6. Three distinct MLLM-based reflection pipelines for image editing. (a) Dual-image Reflection processes the instruction, reference image, and result image simultaneously in single MLLM call to produce combined thinking and reflection output. (b) Single-image Reflection decomposes this into two sequential MLLM calls: first, generating target description from the instruction and reference image, and then assessing the result image against this description to provide consistency score and reflection. (c) Our proposed Multi-round Reflection pipeline further refines the process into three dedicated stages: (1) Target Describe, which formulates target image description from the input instruction and reference image; (2) Result Assess, which evaluates the generated image against this target description to output consistency scores and reasons; and (3) Refinement Conclude, where the MLLM analyzes the assessment to provide success/failure judgments and, if necessary, detailed instructions for further image modification, leveraging both the reference and result images. This multi-round approach enables more granular and iterative refinement of image editing outcomes. Figure 7. Qualitative examples illustrating the thinking capabilities of our MLLM Reasoner across various image editing instructions. Figure 8. Qualitative multi-round examples illustrating the thinking and the reflection progressively correct and improve the quality of the generate results. Figure 9. Qualitative multi-round examples illustrating the thinking and the reflection progressively correct and improve the quality of the generate results. Figure 10. More qualitative comparison of our method and state-of-the-art approaches. Figure 11. Failure cases."
        }
    ],
    "affiliations": [
        "StepFun"
    ]
}