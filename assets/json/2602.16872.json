{
    "paper_title": "DODO: Discrete OCR Diffusion Models",
    "authors": [
        "Sean Man",
        "Roy Ganz",
        "Roi Ronen",
        "Shahar Tsiper",
        "Shai Mazor",
        "Niv Nayman"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Optical Character Recognition (OCR) is a fundamental task for digitizing information, serving as a critical bridge between visual data and textual understanding. While modern Vision-Language Models (VLM) have achieved high accuracy in this domain, they predominantly rely on autoregressive decoding, which becomes computationally expensive and slow for long documents as it requires a sequential forward pass for every generated token. We identify a key opportunity to overcome this bottleneck: unlike open-ended generation, OCR is a highly deterministic task where the visual input strictly dictates a unique output sequence, theoretically enabling efficient, parallel decoding via diffusion models. However, we show that existing masked diffusion models fail to harness this potential; those introduce structural instabilities that are benign in flexible tasks, like captioning, but catastrophic for the rigid, exact-match requirements of OCR. To bridge this gap, we introduce DODO, the first VLM to utilize block discrete diffusion and unlock its speedup potential for OCR. By decomposing generation into blocks, DODO mitigates the synchronization errors of global diffusion. Empirically, our method achieves near state-of-the-art accuracy while enabling up to 3x faster inference compared to autoregressive baselines."
        },
        {
            "title": "Start",
            "content": "DODO: Discrete OCR Diffusion Models Sean Man 1 Roy Ganz 2 Roi Ronen 2 Shahar Tsiper 2 Shai Mazor 2 Niv Nayman 2 6 2 0 2 8 1 ] . [ 1 2 7 8 6 1 . 2 0 6 2 : r Abstract Optical Character Recognition (OCR) is fundamental task for digitizing information, serving as critical bridge between visual data and textual understanding. While modern Vision-Language Models (VLM) have achieved high accuracy in this domain, they predominantly rely on autoregressive decoding, which becomes computationally expensive and slow for long documents as it requires sequential forward pass for every generated token. We identify key opportunity to overcome this bottleneck: unlike open-ended generation, OCR is highly deterministic task where the visual input strictly dictates unique output sequence, theoretically enabling efficient, parallel decoding via diffusion models. However, we show that existing masked diffusion models fail to harness this potential; those introduce structural instabilities that are benign in flexible tasks, like captioning, but catastrophic for the rigid, exactmatch requirements of OCR. To bridge this gap, we introduce DODO, the first VLM to utilize block discrete diffusion and unlock its speedup potential for OCR. By decomposing generation into blocks, DODO mitigates the synchronization errors of global diffusion. Empirically, our method achieves near state-of-the-art accuracy while enabling up to 3 faster inference compared to autoregressive baselines. 1. Introduction Optical character recognition (OCR) is core component of modern document understanding systems, enabling the extraction of structured text from images such as scanned documents, forms, and natural scenes. Visionlanguage models are increasingly used for large-scale document parsing and multimodal reasoning (Alayrac et al., 2022; Li et al., 2022; 2023; Ganz et al., 2023; Dai et al., 2023; Wu et al., 2024; Li et al., 2024; Ganz et al., 2024; Chen et al., 2025; Bai et al., 1Technion - Israel Institute of Technology, Haifa, Israel. Work conducted during an internship at Amazon 2Amazon Web Services. Correspondence to: Niv Nayman <nivay@amazon.com>. Figure 1. DODO: High-throughput parallel generation. Unlike autoregressive models constrained to strict left-to-right sequence, DODO generates text across the entire canvas simultaneously (with same color) based on visual confidence. In this example, it resolves 148 tokens in just 15 forward passes ( 10 tokens/step on average). Notably, large, distinct regions appear early, while ambiguous highfrequency tokens (e.g., punctuation) are deferred to later steps. 2025b;a; Liu et al., 2025). However, the high computational cost and latency of these architectures have re-established OCR transcription as critical bottleneck where both accuracy and inference efficiency are essential (Blecher et al., 2023; Wei et al., 2024; Abramovich et al., 2024; Nacson et al., 2025; Wei et al., 2025b). Crucially, OCR differs fundamentally from semantically flexible tasks like image captioning (Sidorov et al., 2020; Chen et al., 2023; Lin et al., 2014) or visual question answering (VQA) (Antol et al., 2015; Singh et al., 2019; Mathew et al., 2021; Yue et al., 2024) as it is semantically rigid. Conditioned on the image, the posterior distribution is effectively unimodal, meaning the visual input strictly dictates single valid sequence. This determinism exposes critical inefficiency in standard Autoregressive (AR) models: they generate text sequentially, creating significant latency bottleneck for long document sequences. Conversely, this characteristic makes OCR uniquely suited for Masked Diffusion Models (MDMs) (Sahoo et al., 2024b). Because the output allows for little ambiguity, OCR satisfies the MDM 1 DODO: Discrete OCR Diffusion Models assumption of conditional independencethe premise that tokens can be predicted independently given the input (Azangulov et al., 2025). Theoretically, this allows the model to resolve large spans of text simultaneously, similar to how traditional OCR pipelines (Wang et al., 2021; Ronen et al., 2022; Aberdam et al., 2023) recognize isolated regions in parallel without the risk of incoherence. However, realizing this potential in practice reveals structural paradox: the same rigidity that enables parallelization also makes OCR particularly sensitive to the instabilities of global decoding. While standard MDMs (Yu et al., 2025; Li et al., 2025a; You et al., 2025) can generate tokens in parallel, they introduce non-causal structural uncertainties, specifically regarding sequence length and absolute positional alignment. In flexible tasks like captioning, such errors are recoverable: the model can navigate wide space of valid outputs to resolve misalignment dynamically. OCR allows no such flexibility. Because the target is single, immutable sequence, structural errors become irrecoverable; the model cannot rewrite the text to compensate for incorrect length estimates or token placement. Consequently, these rigidities force the model to either truncate valid text or hallucinate padding, leading to fractured, colliding outputs that fundamentally undermine the efficacy of standard masked diffusion for transcription. To resolve this paradox, we propose DODO (Discrete OCR Diffusion Models), the first VisionLanguage Model to adapt block discrete diffusion for document transcription. Unlike standard global diffusion, DODO decomposes the monolithic generation task into sequence of causally anchored blocks. This structural change directly addresses the rigidities of OCR: by bounding the inference horizon and conditioning on committed prefix, we eliminate the risk of long-range alignment drift and enable dynamic length adaptation without requiring perfect global estimate. Crucially, we leverage the high-confidence nature of OCR to scale the training block size to 256 tokenssignificantly larger than the 32 tokens used in text-only methods (Wu et al., 2025b). This maximizes parallel efficiency during training, while the integration of KV-caching (Li et al., 2024) ensures causal consistency and accelerates inference. Empirically, DODO achieves transcription accuracy competitive with state-of-the-art autoregressive models while outperforming the equivalent autoregressive baseline in throughput. These results validate our hypothesis: OCR is indeed regime where the conditional independence assumption holds, but it requires the structural safety rails of block diffusion to be realized in practice. Appropriately structured, DODO recovers the correctness of AR models while unlocking the efficiency benefits that motivate diffusion-based decoding in the first place. Our contributions are summarized as follows: We identify structural incompatibility between standard masked diffusion and the rigid requirements of OCR, explaining why positional and length errors that are benign in flexible tasks prove catastrophic for OCR. We introduce DODO, the first VLM to utilize block discrete diffusion. By decomposing generation into sequentially conditioned blocks, DODO enforces local alignment and enables dynamic length adaptation, resolving the rigidities of global diffusion. We demonstrate that DODO matches the accuracy of state-of-the-art autoregressive baselines while enabling up to 3 faster inference, validating the potential of parallel decoding for dense text recognition. 2. Related Work Specialized OCR and Document Understanding. Modern OCR systems leverage vision-language models for endto-end document understanding. MonkeyOCR (Li et al., 2025c) introduces multi-stage pipeline with detection, recognition, and reading order prediction. MinerU (Wang et al., 2024) and commercial systems like dots.ocr (Li et al., 2025b), DeepSeek-OCR (Wei et al., 2025a), MistralOCR (mis, 2025) achieve strong performance through careful engineering and large-scale training. These methods universally employ autoregressive decoding. This is the first successful attempt to achieve competitive OCR performance by MDMs. Discrete Diffusion Models. Discrete diffusion models learn to reverse corruption process over discrete tokens. D3PM (Austin et al., 2021) introduced structured transition matrices, while MDLM (Sahoo et al., 2024a) simplified training through masked diffusion with tighter evidence lower bounds (ELBO). Recent work has scaled these models to language modeling (Zhou et al., 2024), though gap remains compared to autoregressive models on perplexity benchmarks. This work narrows down the performance gap of MDMs with their plain autoregressive counterparts for the studied task. Block Diffusion. Block Diffusion (BD3-LM) (Arriola et al., 2025) bridges autoregressive and diffusion models by generating blocks of tokens autoregressively during inference only, with each block decoded via masked diffusion. This enables KV-caching across blocks while maintaining parallel decoding within blocks. Prior work uses small block sizes (432 tokens) (Wu et al., 2025b) to minimize the performance gap with autoregressive models on language modeling. (Wu et al., 2025a) further aligns the attention masks to be block causal during training. We implement this approach for VLMs. 2 DODO: Discrete OCR Diffusion Models Figure 2. Semantically flexible vs. semantically rigid visionlanguage tasks. Left: Image captioning admits multiple, semantically equivalent descriptions of the same image. Different decoding trajectories can converge to distinct but equally valid captions, and lexical or structural variations are naturally absorbed. Right: OCR requires single, exact transcription determined by the image. Even minimal local deviations, such as an incorrect token choice or boundary, render the output incorrect. As result, conditioned on the image, OCR exhibits extremely low output variability, which makes it natural candidate for parallel decoding, but also demanding setting in which errors cannot be compensated by alternative phrasings or later corrections. Multimodal Diffusion Models. Dimple (Yu et al., 2025) extends discrete diffusion to vision-language tasks, training on the LLaVA recipe. However, it shows limited gains over autoregressive baselines and does not evaluate on OCR benchmarks. LaViDa (Li et al., 2025a) and LLaDA-V (You et al., 2025) explore diffusion for VQA but struggle with OCR tasks requiring precise text reproduction. This work is the first to successfully apply discrete diffusion for OCR. 3. Preliminaries Notation. Let be vocabulary of size , and let [M] / denote dedicated MASK token. We write = {[M]}, and represent tokens either as categorical indices or as one-hot vectors ev {0, 1} V. We use Cat(; π) for categorical distribution with probabilities π. 3.1. OCR as Conditional Sequence Modeling We formulate OCR as conditional generation task where the goal is to map document image to target sequence of discrete tokens x1:L. This target is defined as x1:L = τ (s(I)), where s() represents fixed serialization scheme (e.g., plain text, LATEX, HTML) and τ () is tokenizer that maps the resulting string to vocabulary indices. visionlanguage model (VLM) estimates the conditional distribution pθ(x1:L I, c) given the image and optional text context c. Autoregressive (AR) VLM decoding admits standard left-to-right factorization log pθ(x1:L I, c) = (cid:88) ℓ=1 log pθ(xℓ x<ℓ, I, c), (1) with x<l the prefix tokens at step out of sequential steps. probability distribution for the token masking is qt0(xt x0) = (cid:16) Cat (cid:89) i=1 xi t; αt exi 0 + (1 αt) e[M] (cid:17) , (2) where αt is strictly decreasing with α0 = 1 and α1 = 0. Training. MDMs train denoiser to predict the original input tokens from partially masked ones. In continuous time, an ELBO-derived objective can be written (Sahoo et al., 2024a; Shi et al., 2024) as weighted masked crossentropy = Et,x0,xtx0 α 1 αt (cid:88) i:xi t=[M] log pθ(xi 0xt, t) , = dαt (3) where α dt , and pθ( xt, t) is frequently implemented without an explicit time embedding since xt reveals through its mask rate. Sampling. MDM sampling starts from the fully masked sequence x1 = ([M], . . . , [M]) and iterates noise levels 1 = tK > > t0 = 0. Given an estimate of the marginal distribution of each token from the denoiser, common and convenient decomposition (Sahoo et al., 2024a) of single reverse step tk+1 tk proceeds by first choosing which masked positions to reveal via selection rule (e.g., randomly (Sahoo et al., 2024a), top-k (Zheng et al., 2023), confidence-thresholding (Yu et al., 2025), or deterministically (Luxembourg et al., 2025)), and second, sampling token values for those positions from the denoisers predicted distribution xi tk pθ(xi tk xtk+1 ). This decomposition has two consequences described next. 3.2. Masked Diffusion Models (MDMs) Forward (Masking) Process. MDMs define coordinateindependent corruption process that replaces tokens with [M] according to noise level [0, 1]. Writing x0 for clean data and xt for its noisy version, common conditional Conditional Independence Assumption. At each sampling step, the decoded tokens are sampled independently from one another. This may lead to incorrect results if the decoded tokens in fact are conditionally dependent given the context, as illustrated in Figure 3. On the other hand, 3 DODO: Discrete OCR Diffusion Models We argue that OCR is uniquely suited for this parallel paradigm. Unlike semantically flexible vision-language tasks, the posterior distribution p(x1:LI, c) for document transcription is highly peaked, often approaching Dirac delta function around single ground-truth sequence. In this low-entropy regime, the strong conditioning on the visual input effectively decouples token predictions, allowing the joint probability of masked tokens to be factorized: p(x1:L tk x1:L tk+1 , I, c) (cid:89) ℓ=1 p(xℓ tk x1:L tk+ , I, c). (4) This independence allows the decoding of large spans of tokens simultaneously, as shown in Figure 1. 4.2. Brittleness of Parallel Decoding in OCR Standard masked diffusion models operate on fixed-length canvas and rely on carry-over unmasking (Section 3.2), treating tokens revealed in early steps as immutable context. While this constraint is manageable for semantically flexible tasks like captioning, where the model can paraphrase content or alter semantics to fit the available space, it presents fundamental challenge for OCR. Document transcription is rigid, zero-tolerance task: the ground-truth text consists of specific set of characters in an immutable order. This lack of flexibility exposes two critical failure modes for parallel decoding: Length Mismatch. Because the true sequence length is unknown at inference time, the decoding canvas size is effectively an estimate. In generative tasks like captioning, this is rarely fatal; the model can simply generate valid shorter or longer description to match the canvas. In OCR, however, this mismatch creates structural vulnerability. If the initial is incorrect, or the model predicts an end-ofsequence [EOS] token prematurely, the model is forced to either truncate valid text (if the effective length is too short) or hallucinate (if too long) to satisfy the imposed constraint. Positional Anchoring. Even with valid length estimate, parallel decoding binds content to absolute positional indices in non-causal manner. This introduces critical synchronization risk: the model may predict segment at an incorrect offset, such as placing table header 50 tokens too early or too late. Because carry-over unmasking prevents the revision of revealed tokens, this error is locked in place. Unlike autoregressive decoding, which inherently align content to its history without looking ahead, diffusion parallel decoding is bound to errors made ahead. Consequently, the subsequent text cannot shift to accommodate the offset. Due to the unimodal nature of OCR, the model cannot simply paraphrase or tweak the surrounding text to bridge this misalignment, leading to fractured outputs where disjoint Figure 3. Conditional independence assumption. Parallel decoding assumes masked that masked tokens can be predicted independently given the context. (Top) In open-ended tasks, ambiguity between valid options (e.g., Eiffel Tower vs. Great Wall) risks sampling incoherent mixtures like Eiffel Wall. (Bottom) In deterministic regimes like OCR, the strong visual signal resolves this ambiguity, enabling conflict-free parallel decoding. when this assumption holds, there is large parallelization potential to leverage. Carry-Over Unmasking. We follow the common practice (Wu et al., 2025b;a; You et al., 2025; Li et al., 2025a; Yu et al., 2025) and only allow the sampling of masked tokens at each step. While this formulation is shown by (Sahoo et al., 2024a) to be the key for the derivation of the discrete ELBO equivalent loss in Equation (1), this means one cannot revise previously decoded tokens. While this has lesser effect on generative tasks where multiple responses are acceptable, it might be detrimental for tasks where only one response is correct, as illustrated in Figure 2. 4. Method We analyze OCR through the lens of MDM training and sampling (Section 3.2), focusing on the two sampling assumptions made explicit there: conditional independence of tokens sampled within step, and carry-over unmasking, where revealed tokens are not revised. We first argue that OCR is especially compatible with the conditional-independence assumption. We then show why this potential is difficult to realize with vanilla MDM inference, as early mistakes persist and this requires caution when decoding many tokens in parallel. Finally, we argue that block discrete diffusion mitigates these failure modes, while retaining high parallelism and enjoying KV-Caching. 4.1. Parallel Decoding Potential OCR typically yields long sequences, making standard autoregressive decoding significant latency bottleneck, as it requires sequential forward passes to decode tokens. Masked diffusion models offer compelling alternative by enabling parallel token generation; however, their effectiveness hinges on the validity of the conditional independence assumption. 4 DODO: Discrete OCR Diffusion Models 5. Experiments 5.1. DODO We instantiate our proposed framework as DODO (Discrete OCR Diffusion Models), built upon the Qwen2.5-VL3B architecture (Bai et al., 2025c). We train DODO on olmOCR-mix-1025 (Poznanski et al., 2024), large-scale OCR dataset comprising approximately 270K document-text pairs derived from PDFs. The dataset covers diverse document types, including academic papers, books, reports, and web pages, with text extracted using combination of PDF parsing and OCR pipelines. Other implementation details are deferred to Section A. While block-based diffusion has previously been explored for text-only models (Arriola et al., 2025; Wu et al., 2025a), we are the first to adapt this paradigm to the multimodal domain. Specifically, to assess the trade-off between context visibility and inference speed, we instantiate our block-based training with two attention variants (visualized in Figure A1): DODO. This variant utilizes full bidirectional attention across the entire sequence. As result, while the tokens of committed blocks remain fixed, their hidden representations are recomputed from scratch during each forward pass. This enables the prefix representations to attend to the current active block, meaning their features dynamically adapt to the new context within the current pass. This allows for maximal information flow but requires processing the full sequence at every step, as the prefix representations are not static and thus cannot be cached. DODO fast. This variant is optimized for throughput by enforcing block-causal attention mask: tokens within the active block x(b) attend bidirectionally to one another and to all previous blocks x(<b), but attention from previous blocks to the current one is masked. This strict causality ensures that the representations of the committed prefix remain fixed, enabling the use of an exact Key-Value (KV) cache. Consequently, only the active block needs to be computed at each step, resulting in significant speedups. 5.2. Evaluation Setup Benchmarks. We evaluate our models on two distinct benchmarks. First, we use OmniDocBench (Ouyang et al., 2025), comprehensive testbed for layout-sensitive transcription containing 290 English documents across 9 diverse types (e.g., academic papers, financial reports), annotated with structured ground truth for text, tables, and formulas. Second, we evaluate on Fox-Page-EN (Liu et al., 2024), dataset of 112 document pages focused exclusively on pure text without figures or tables. This combination allows us to assess performance on both complex, multimodal document layouts and standard dense text transcription. Figure 4. Full vs. block diffusion. In standard full diffusion (left), MDM sampling is applied globally to the entire sequence. In contrast, block diffusion (right) restricts parallel sampling to discrete windows, processing blocks sequentially from left to right. segments collide fundamental challenge that limits the efficacy of purely parallel OCR. 4.3. Block Diffusion as Structural Remedy Block discrete diffusion can mitigate these failure modes by replacing single length-L denoising problem with sequence of bounded-span problems, conditioned of prefix composed on previous blocks decoded sequentially. Block discrete diffusion models (Arriola et al., 2025) combine AR structure at coarse granularity with diffusion within blocks. Partition the sequence into contiguous blocks of length (so = BL) and write x(b) for block and x(<b) for the prefix blocks. The model factorizes as pθ(x1:LI, c) = (cid:89) b=1 (cid:0)x(b) x(<b), I, c(cid:1), pθ (5) where each conditional distribution pθ(x(b) x(<b), I, c) is implemented via masked diffusion sampling over the tokens of the block, conditioned on prefix states. This formulation narrows the performance gap between MDMs and AR models and enables significant computational efficiency by allowing the KV-cache of committed prefix blocks x(<b) to be reused rather than recomputed. An illustrative example of the difference between full and block diffusion models sampling is given in Figure 4. This factorization anchors indices and conventions at block boundaries, reduces length sensitivity, and retains parallel token updates within each block while enabling variable-length generation via block-level stopping. Previous unimodal MDMs (Wu et al., 2025b;a) use small block sizes (432 tokens) to minimize the performance gap with autoregressive models on text-only tasks. By utilizing the properties of the OCR task, we are able to scale the block size x4 to 256, as this work is the first to apply blockcausal masking both during training and inference in the multimodal VLM settings. This approach yields the DODO and DODO-fast variants analyzed in Section 5. 5 DODO: Discrete OCR Diffusion Models Table 1. DODO results. OCR performance on the English subset of OmniDocBench and Fox-Pages. Bold numbers indicate the best value across each model type (MDM and AR). Method Size OmniDocBench Fox-Pages Normalized Edit Distance Specialized OCR 3B dots.ocr DeepSeek-OCR 3.4B MinerU 2.0 VLM 0.9B 3B MonkeyOCR-pro - Mistral OCR 7B olmOCR 3B Nanonets-OCR-s 256M SmolDocling Autoregressive VLMs Qwen 2.5 VL Qwen 2.5 VL Qwen 2.5 VL Diffusion VLMs Dimple LaViDa-L LLaDA-V Ours DODO DODO fast 72B 7B 3B 7B 8B 7B 3B 3B 0.032 0.049 0.045 0.058 0.072 0.097 0.134 0.262 0.092 0.135 0.184 0.856 0.994 0.524 0.066 0.159 0.034 0.100 - 0.084 0.013 0.023 - 0. 0.039 0.025 0.051 0.932 - 0.336 0.041 0.059 Metrics. We assess performance using two metrics. First, we report accuracy using the Normalized Edit Distance (NED) between the predicted and ground-truth text, with lower scores indicating higher fidelity. Second, to quantify inference efficiency, we measure throughput as the number of generated Tokens Per Second (TPS) for each model. Baselines. We compare DODO against three model categories: (1) specialized OCR models, including dots.ocr, DeepSeek-OCR, MinerU 2.0 VLM, MonkeyOCR, MistralOCR, olmOCR, Nanonets-OCR-s, and SmolDocling (Li et al., 2025b; Wei et al., 2025a; Wang et al., 2024; Li et al., 2025c; mis, 2025; Poznanski et al., 2025; Mandal et al., 2025; Nassar et al., 2025); (2) general-purpose autoregressive VLMs, specifically the Qwen2.5-VL family (Bai et al., 2025c), which serves as our backbone; and (3) diffusion VLMs, represented by Dimple, LaViDa, and LLaDA-V (Yu et al., 2025; Li et al., 2025a; You et al., 2025). 5.3. Main Results Table 1 presents the OCR performance on OmniDocBench and Fox-Pages. Compared to prior diffusion-based VLMs, DODO demonstrates substantial performance leap. Standard diffusion models such as Dimple, LaViDa, and LLaDAV struggle significantly with dense document transcription, incurring high error rates (> 0.5 NED on OmniDocBench). In contrast, DODO achieves an NED of 0.066. While Figure 5. Inference throughput comparison. While standard DODO matches the speed of the autoregressive Qwen 2.5 VL baseline ( 21 tokens/sec), the DODO fast leverages block-causal attention and KV-caching to triple the throughput to 63 tokens/sec, establishing new efficiency standard for diffusion-based VLMs. DODO benefits from specialized OCR training, we demonstrate in our ablations (Section 6.1) that data distribution alone does not explain this gap; even when trained on identical OCR data, standard full-sequence diffusion fails to converge on dense text. This confirms that the improvement is primarily structural: DODOs block-wise constraints effectively mitigate the alignment failures that plague global diffusion models. Second, DODO proves highly competitive against strong autoregressive and specialized baselines. On the layoutintensive OmniDocBench, it surpasses its own autoregressive backbone, the Qwen2.5-VL family, across all model scales. Furthermore, DODO outperforms various specialized models and achieves near-parity with robust engines such as MonkeyOCR (Li et al., 2025c) and Mistral OCR (mis, 2025). These results establish that discrete diffusion is viable, high-performance alternative to the dominant autoregressive paradigm in the challenging domain of dense text recognition. 5.4. Throughput Analysis Figure 5 illustrates the inference throughput across different model architectures. For qualitative visualizations of the parallel decoding process on dense documents, we refer readers to Figure 1 and Section C. Three key trends emerge from this comparison. First, DODO outperforms the autoregressive baseline even without caching optimizations. Remarkably, despite recomputing the full decoder state at every step, standard DODO achieves higher throughput than the fully cached autoregressive execution of the same Qwen 2.5 VL backbone. This demonstrates that the massive reduction in forward passes enabled by parallel decoding outweighs the per-step cost of re-computation. While AR generation requires sequential steps, DODO decouples latency from sequence length, allowing the parallel prediction of multiple tokens to amortize the cost of the heavier forward pass. 6 DODO: Discrete OCR Diffusion Models Table 2. Impact of sequence length and block structure. Comparison of DODOs block-wise training against standard fullsequence MDM (Vanilla). The results highlight critical dependency: Vanilla MDM fails to generalize even with Oracle length guidance or inference-time blocking, confirming that training with block constraints is essential for performance. In contrast, DODOs autoregressive anchoring resolves this, reducing error rates by nearly 10 compared to the best Vanilla configuration. Bold numbers indicate the best value within each group. Training Configuration Vanilla Vanilla Vanilla Block Block Causal Max Length Oracle 8192 8192 8192 8192 Inference Block Size Normalized Edit Dist. Tokens / Sec. - - 256 256 0.100 0.834 0.631 0.066 0.192 8.77 3.19 5.76 22.9 41.4 Second, DODO fast yields dramatic speedup, approximately tripling the throughput compared to the bidirectional variant (from 23 TPS to 66 TPS). By enabling the use of an exact KV-cache for completed blocks, DODO fast eliminates the redundant re-computation of prefix representations, maximizing deployment efficiency. Finally, DODO achieves significantly higher throughput than competing diffusion VLMs. Prior methods rely on fullsequence attention from the outset, incurring the maximal computational cost proportional to the total length at every denoising step. In contrast, standard DODOs cost grows linearly with the number of processed blocks, while DODO fast maintains constant cost per block by caching the prefix. This allows both variants to process the early and middle stages of generation much faster than standard diffusion baselines, which are immediately burdened by the full sequence complexity. 6. Ablation and Empirical Analysis We validate the structural design of DODO by isolating the impact of block-based training and analyzing the interaction between block size and caching strategies. For an ablation of sampling schedules, we refer readers to Section B.1. 6.1. Vanilla vs. Block Training Table 2 serves as the empirical validation of the theoretical challenges outlined in Section 4. We compare our blockbased approach against Vanilla baseline: standard masked diffusion model trained on global sequences (up to 8192 tokens) without block decomposition. The baseline model exhibits high error rates compared to DODO. Crucially, this performance gap persists even when the model is provided with the oracle sequence length. This suggests that the limitation is not solely due to length estimation, but also stems from the positional anchoring inherent Table 3. Ablation of block size and caching strategies. Naive reuse of the KV-cache in bidirectional models causes catastrophic accuracy collapse (Approx. KV-Cache). By switching to blockcausal training, DODO fast enables exact caching, unlocking 3 speedup. This offers flexible trade-off: significantly higher throughput for moderate accuracy cost. Configuration Block Size Edit Dist. TPS No KV-Cache DODO DODO DODO DODO DODO 32 128 256 512 1024 Approx. KV-Cache DODO DODO DODO Exact KV-Cache DODO fast DODO fast DODO fast 32 128 256 32 128 256 0.071 0.074 0.067 0.137 0.199 0.802 0.712 0. 0.159 0.199 0.192 15.1 22.0 22.9 20.73 21.86 63.2 35.4 37.3 65.9 51.3 41.4 to parallel decoding. Attempting to resolve thousands of tokens simultaneously on fixed canvas creates synchronization risks; because the model cannot adjust the global offset of disjoint text segments, the output becomes fractured. We further investigate if this issue can be mitigated solely at inference time by applying block decoding to the baseline model. The results show that restricting the decoding window without corresponding training objective leads to poor performance. This contrasts with findings in fastdLLM (Wu et al., 2025b), where inference-time blocking remained effective for math and coding benchmarks like GSM8K (Cobbe et al., 2021) and HumanEval (Chen et al., 2021). We hypothesize that this divergence stems from the nature of the tasks, as unlike OCR, they allow some semantic or syntactic flexibility. This indicates that block diffusion serves as necessary structural prior for OCR, conditioning the model to treat the prefix x(<b) as stable anchor. Finally, the baseline exhibits significantly lower throughput. This is due to the computational cost of the global canvas: the model must compute attention over the full sequence length (up to 8192 tokens) at every denoising step. In contrast, DODO decomposes the workload, ensuring more efficient distribution of computational cost. 6.2. Block Size and Caching Strategies Table 3 investigates the impact of block size and KV-caching strategies on performance. For the standard bidirectional model (No KV-Cache), we observe non-monotonic trend as the block size increases. Initially, enlarging the block size improves both throughput and accuracy, peaking at an intermediate size (B = 256). This is because generating 7 DODO: Discrete OCR Diffusion Models 6.3. Inference Efficiency Analysis To disentangle the source of DODOs throughput advantage, we analyze the number of model forward passes required for generation in Figure 6. Autoregressive models are structurally bound to 1 : 1 ratio (one step per token). In contrast, DODO leverages parallel decoding to compress the inference process, typically requiring fewer than 0.1 steps per token. This order-of-magnitude reduction in the number of sequential steps explains the throughput results shown in Figure 5. Although DODOs bidirectional forward pass (without caching) is computationally heavier than cached autoregressive step, the sheer reduction in the number of calls, generating 10 to 20 tokens per AR step, overcomes the per-step cost. This allows DODO to outperform optimized autoregressive baselines even without KV caching. 7. Conclusion In this work, we introduce DODO, framework that unlocks the potential of masked diffusion models to accelerate OCR via parallel decoding. Our analysis reveals that standard parallel decoding is fundamentally limited by the brittleness of the monolithic canvas, which leads to catastrophic synchronization failures due to length mismatches and positional anchoring. By decomposing this task into semiautoregressive blocks, DODO provides structural remedy that reconciles the stability of causal anchoring with the efficiency of parallel generation. Empirically, DODO sets new standard for non-autoregressive OCR VLMs. It surpasses prior diffusion-based VLMs by an order of magnitude and achieves performance competitive with state-of-the-art specialized and autoregressive systems. Furthermore, through our block-causal DODO fast, we demonstrate that this architecture can support exact KV-caching. This optimization triples inference throughput, establishing discrete diffusion not just as theoretical capability, but as practical, highperformance alternative for latency-critical applications. Limitations. Despite these advances, our approach has limitations. First, the trade-off between accuracy and efficiency remains non-trivial. While DODO fast significantly accelerates inference, its reliance on exact caching enforces static history. Unlike the standard bidirectional variant, where prefix representations are recomputed and can dynamically adapt to the active block, DODO fast is constrained by frozen representations, resulting in higher edit distances. Second, our ablations reveal that performance is sensitive to structural hyperparameters, particularly the interaction between block size and attention masking. Future work will focus on bridging the gap between the bidirectional and block-causal variants, as well as exploring diffusion samplers explicitly tailored to the unique characteristics of the OCR task. Figure 6. Decoding Efficiency. Distribution of inference steps normalized by output length. The autoregressive baseline is structurally limited to generating single token per step. In contrast, DODO leverages parallel decoding to generate multiple tokens simultaneously, effectively compressing the inference process by an order of magnitude (typically < 0.1 steps per token). more tokens in parallel reduces the total number of sequential inference steps, amortizing the cost of the forward pass. However, further increasing the block size (B = 512, 1024) leads to diminishing returns in throughput and notable degradation in accuracy. We attribute this to the recurrence of positional anchoring issues: as the block becomes sufficiently large, it begins to suffer from the same internal synchronization failures that plague the global canvas. The Approx. KV-Cache configurations investigate the feasibility of reusing computed keys and values in bidirectional model without retraining. Unlike prior findings which observed minimal degradation when freezing history in standard diffusion models (Wu et al., 2025b), our experiments show sharp accuracy collapse. We attribute this discrepancy to the rigid nature of the OCR task. DODO fast resolves this incompatibility by explicitly training with block-causal masks, enabling exact KV-caching. Interestingly, this variant exhibits an inverse trend compared to the bidirectional model, where smaller blocks yield the best performance. We hypothesize that this is due to the static nature of the context in cached models. In standard DODO, the representations of the prefix (x(<b)) are recomputed and can dynamically adapt to the currently generating block. In DODO fast, the prefix is frozen. Generating large block (B = 256) against this unyielding static context increases the risk of representation drift; the model attempts to generate too much new content without updating the historys features. Reducing the block size mitigates this by committing tokens to the cache more frequently, effectively refreshing the static anchor and ensuring the generation remains tightly coupled to the immutable history. 8 DODO: Discrete OCR Diffusion Models"
        },
        {
            "title": "Impact Statement",
            "content": "This paper presents work whose goal is to advance the field of Machine Learning. There are many potential societal consequences of our work, none of which we feel must be specifically highlighted here."
        },
        {
            "title": "Mistral",
            "content": "ocr. https://mistral.ai/news/ mistral-ocr, 2025. Mistral AI Optical Character Recognition model. Aberdam, A., Bensaıd, D., Golts, A., Ganz, R., Nuriel, O., Tichauer, R., Mazor, S., and Litman, R. Clipter: Looking at the bigger picture in scene text recognition. In Proceedings of the IEEE/CVF international conference on computer vision, pp. 2170621717, 2023. Abramovich, O., Nayman, N., Fogel, S., Lavi, I., Litman, R., Tsiper, S., Tichauer, R., Appalaraju, S., Mazor, S., and Manmatha, R. Visfocus: Prompt-guided vision encoders for ocr-free dense document understanding. In European Conference on Computer Vision, pp. 241259. Springer, 2024. Alayrac, J.-B., Donahue, J., Luc, P., Miech, A., Barr, I., Hasson, Y., Lenc, K., Mensch, A., Millican, K., Reynolds, M., Ring, R., Rutherford, E., Cabi, S., Han, T., Gong, Z., Samangooei, S., Monteiro, M., Menick, J. L., Borgeaud, S., Brock, A., Nematzadeh, A., Sharifzadeh, S., Binkowski, M. a., Barreira, R., Vinyals, O., Zisserman, A., and Simonyan, K. Flamingo: visual language model for few-shot learning. In Koyejo, S., Mohamed, S., Agarwal, A., Belgrave, D., Cho, K., and Oh, A. (eds.), Advances in Neural Information Processing Systems, volume 35, pp. 2371623736. Curran Associates, Inc., 2022. Antol, S., Agrawal, A., Lu, J., Mitchell, M., Batra, D., Zitnick, C. L., and Parikh, D. Vqa: Visual question In Proceedings of the IEEE international answering. conference on computer vision, pp. 24252433, 2015. Arriola, M., Gokaslan, A., Chiu, J. T., Yang, Z., Qi, Z., Han, J., Sahoo, S. S., and Kuleshov, V. Block diffusion: Interpolating between autoregressive and diffusion language models. arXiv preprint arXiv:2503.09573, 2025. Austin, J., Johnson, D. D., Ho, J., Tarlow, D., and van den Berg, R. Structured denoising diffusion models in discrete state-spaces. Advances in Neural Information Processing Systems, 2021. Azangulov, I., Pandeva, T., Prasad, N., Zazo, J., and Karmalkar, S. Parallel sampling from masked diffusion models via conditional independence testing, 2025. URL https://arxiv.org/abs/2510.21961. 9 Bai, S., Cai, Y., Chen, R., Chen, K., Chen, X., Cheng, Z., Deng, L., Ding, W., Gao, C., Ge, C., Ge, W., Guo, Z., Huang, Q., Huang, J., Huang, F., Hui, B., Jiang, S., Li, Z., Li, M., Li, M., Li, K., Lin, Z., Lin, J., Liu, X., Liu, J., Liu, C., Liu, Y., Liu, D., Liu, S., Lu, D., Luo, R., Lv, C., Men, R., Meng, L., Ren, X., Ren, X., Song, S., Sun, Y., Tang, J., Tu, J., Wan, J., Wang, P., Wang, P., Wang, Q., Wang, Y., Xie, T., Xu, Y., Xu, H., Xu, J., Yang, Z., Yang, M., Yang, J., Yang, A., Yu, B., Zhang, F., Zhang, H., Zhang, X., Zheng, B., Zhong, H., Zhou, J., Zhou, F., Zhou, J., Zhu, Y., and Zhu, K. Qwen3-vl technical report, 2025a. URL https://arxiv.org/abs/2511.21631. Bai, S., Chen, K., Liu, X., Wang, J., Ge, W., Song, S., Dang, K., Wang, P., Wang, S., Tang, J., Zhong, H., Zhu, Y., Yang, M., Li, Z., Wan, J., Wang, P., Ding, W., Fu, Z., Xu, Y., Ye, J., Zhang, X., Xie, T., Cheng, Z., Zhang, H., Yang, Z., Xu, H., and Lin, J. Qwen2.5-vl technical report, 2025b. URL https://arxiv.org/abs/2502.13923. Bai, S., Chen, K., Liu, X., Wang, J., Ge, W., Song, S., Dang, K., Wang, P., Wang, S., Tang, J., et al. Qwen2. 5-vl technical report. arXiv preprint arXiv:2502.13923, 2025c. Blecher, L., Cucurull, G., Scialom, T., and Stojnic, R. Nougat: Neural optical understanding for academic documents, 2023. URL https://arxiv.org/abs/ 2308.13418. Chen, L., Li, J., Dong, X., Zhang, P., He, C., Wang, J., Zhao, F., and Lin, D. Sharegpt4v: Improving large multimodal models with better captions, 2023. URL https: //arxiv.org/abs/2311.12793. Chen, M., Tworek, J., Jun, H., Yuan, Q., de Oliveira Pinto, H. P., Kaplan, J., Edwards, H., Burda, Y., Joseph, N., Brockman, G., Ray, A., Puri, R., Krueger, G., Petrov, M., Khlaaf, H., Sastry, G., Mishkin, P., Chan, B., Gray, S., Ryder, N., Pavlov, M., Power, A., Kaiser, L., Bavarian, M., Winter, C., Tillet, P., Such, F. P., Cummings, D., Plappert, M., Chantzis, F., Barnes, E., Herbert-Voss, A., Guss, W. H., Nichol, A., Paino, A., Tezak, N., Tang, J., Babuschkin, I., Balaji, S., Jain, S., Saunders, W., Hesse, C., Carr, A. N., Leike, J., Achiam, J., Misra, V., Morikawa, E., Radford, A., Knight, M., Brundage, M., Murati, M., Mayer, K., Welinder, P., McGrew, B., Amodei, D., McCandlish, S., Sutskever, I., and Zaremba, W. Evaluating large language models trained on code. 2021. Chen, Z., Wang, W., Cao, Y., Liu, Y., Gao, Z., Cui, E., Zhu, J., Ye, S., Tian, H., Liu, Z., Gu, L., Wang, X., Li, Q., Ren, Y., Chen, Z., Luo, J., Wang, J., Jiang, T., Wang, B., He, C., Shi, B., Zhang, X., Lv, H., Wang, Y., Shao, W., Chu, P., Tu, Z., He, T., Wu, Z., Deng, H., Ge, J., Chen, DODO: Discrete OCR Diffusion Models K., Zhang, K., Wang, L., Dou, M., Lu, L., Zhu, X., Lu, T., Lin, D., Qiao, Y., Dai, J., and Wang, W. Expanding performance boundaries of open-source multimodal models with model, data, and test-time scaling, 2025. URL https://arxiv.org/abs/2412.05271. Cobbe, K., Kosaraju, V., Bavarian, M., Chen, M., Jun, H., Kaiser, L., Plappert, M., Tworek, J., Hilton, J., Nakano, R., et al. Training verifiers to solve math word problems. arXiv preprint arXiv:2110.14168, 2021. Dai, W., Li, J., LI, D., Tiong, A., Zhao, J., Wang, W., Li, B., Fung, P. N., and Hoi, S. Instructblip: Towards general-purpose vision-language models with instruction tuning. In Oh, A., Naumann, T., Globerson, A., Saenko, K., Hardt, M., and Levine, S. (eds.), Advances in Neural Information Processing Systems, volume 36, pp. 4925049267. Curran Associates, Inc., 2023. URL https://proceedings.neurips. cc/paper_files/paper/2023/file/ 9a6a435e75419a836fe47ab6793623e6-Paper-Conference. pdf. Liu, C. et al. understanding. In The Thirty-ninth Annual Conference on Neural Information Processing Systems, 2025a. Li, Y., Yang, G., Liu, H., Wang, B., and Zhang, C. dots.ocr: Multilingual document layout parsing in single visionlanguage model, 2025b. URL https://arxiv.org/ abs/2512.02498. Li, Z., Liu, Y., Liu, Q., Ma, Z., Zhang, Z., Zhang, S., Guo, Z., Zhang, J., Wang, X., and Bai, X. Monkeyocr: unified ocr system with multi-stage pipelines. arXiv preprint arXiv:2506.05218, 2025c. Lin, T.-Y., Maire, M., Belongie, S., Hays, J., Perona, P., Ramanan, D., Dollar, P., and Zitnick, C. L. Microsoft coco: Common objects in context. In European conference on computer vision, pp. 740755. Springer, 2014. multi-page document understanding. arXiv:2405.14295, 2024. Focus anywhere for fine-grained arXiv preprint Ganz, R., Nuriel, O., Aberdam, A., Kittenplon, Y., Mazor, S., and Litman, R. Towards models that can see and read. In Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV), pp. 2171821728, October 2023. Ganz, R., Kittenplon, Y., Aberdam, A., Avraham, E. B., Nuriel, O., Mazor, S., and Litman, R. Question aware vision transformer for multimodal reasoning, 2024. URL https://arxiv.org/abs/2402.05472. Li, B., Zhang, Y., Guo, D., Zhang, R., Li, F., Zhang, H., Zhang, K., Zhang, P., Li, Y., Liu, Z., and Li, C. Llavaonevision: Easy visual task transfer, 2024. URL https: //arxiv.org/abs/2408.03326. Li, J., Li, D., Xiong, C., and Hoi, S. BLIP: Bootstrapping language-image pre-training for unified vision-language understanding and generation. In Chaudhuri, K., Jegelka, S., Song, L., Szepesvari, C., Niu, G., and Sabato, S. (eds.), Proceedings of the 39th International Conference on Machine Learning, volume 162 of Proceedings of Machine Learning Research, pp. 1288812900. PMLR, 1723 Jul 2022. URL https://proceedings.mlr. press/v162/li22n.html. Li, J., Li, D., Savarese, S., and Hoi, S. Blip-2: Bootstrapping language-image pre-training with frozen image encoders and large language models, 2023. URL https://arxiv.org/abs/2301.12597. Li, S., Kallidromitis, K., Bansal, H., Gokul, A., Kato, Y., Kozuka, K., Kuen, J., Lin, Z., Chang, K.-W., and Grover, A. Lavida: large diffusion model for vision-language Liu, Z., Zhu, L., Shi, B., Zhang, Z., Lou, Y., Yang, S., Xi, H., Cao, S., Gu, Y., Li, D., Li, X., Fang, Y., Chen, Y., Hsieh, C.-Y., Huang, D.-A., Cheng, A.-C., Nath, V., Hu, J., Liu, S., Krishna, R., Xu, D., Wang, X., Molchanov, P., Kautz, J., Yin, H., Han, S., and Lu, Y. Nvila: Efficient frontier visual language models, 2025. URL https: //arxiv.org/abs/2412.04468. Luxembourg, O., Permuter, H., and Nachmani, E. Plan for speeddilated scheduling for masked diffusion language models. arXiv preprint arXiv:2506.19037, 2025. Mandal, S., Talewar, A., Ahuja, P., and Juvatkar, P. Nanonets-ocr-s: model for transforming documents into structured markdown with intelligent content recognition and semantic tagging, 2025. Mathew, M., Karatzas, D., and Jawahar, C. Docvqa: In Proceedings dataset for vqa on document images. of the IEEE/CVF winter conference on applications of computer vision, pp. 22002209, 2021. Nacson, M. S., Aberdam, A., Ganz, R., Ben Avraham, E., Golts, A., Kittenplon, Y., Mazor, S., and Litman, R. Docvlm: Make your vlm an efficient reader. In Proceedings of the Computer Vision and Pattern Recognition Conference, pp. 2900529015, 2025. Nassar, A., Marafioti, A., Omenetti, M., Lysak, M., Livathinos, N., Auer, C., Morin, L., de Lima, R. T., Kim, Y., Gurbuz, A. S., Dolfi, M., Farre, M., and Staar, P. W. J. Smoldocling: An ultra-compact vision-language model for end-to-end multi-modal document conversion, 2025. URL https://arxiv.org/abs/2503.11576. 10 DODO: Discrete OCR Diffusion Models Ouyang, L. et al. Omnidocbench: Benchmarking diverse pdf document parsing with comprehensive annotations. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), 2025. Poznanski, A. et al. olmocr: Unlocking trillions of tokens in pdfs with vision language models. Hugging Face Datasets, 2024. https://huggingface.co/ datasets/allenai/olmOCR-mix-1025. Poznanski, J., Rangapur, A., Borchardt, J., Dunkelberger, J., Huff, R., Lin, D., Wilhelm, C., Lo, K., and Soldaini, L. olmocr: Unlocking trillions of tokens in pdfs with vision language models. arXiv preprint arXiv:2502.18443, 2025. Ronen, R., Tsiper, S., Anschel, O., Lavi, I., Markovitz, A., and Manmatha, R. Glass: Global to local attention for scene-text spotting. In European Conference on Computer Vision, pp. 249266. Springer, 2022. Sahoo, S., Arriola, M., Schiff, Y., Gokaslan, A., Marroquin, E., Chiu, J., Rush, A., and Kuleshov, V. Simple and effective masked diffusion language models. Advances in Neural Information Processing Systems, 37:130136 130184, 2024a. Sahoo, S. S., Arriola, M., Schiff, Y., Gokaslan, A., Marroquin, E., Chiu, J. T., Rush, A., and Kuleshov, V. Simple and effective masked diffusion language models, 2024b. URL https://arxiv.org/abs/2406.07524. Shi, J., Han, K., Wang, Z., Doucet, A., and Titsias, M. Simplified and Generalized Masked Diffusion for Discrete Data. Advances in Neural Information Processing Systems, 37:103131103167, December 2024. Sidorov, O., Hu, R., Rohrbach, M., and Singh, A. Textcaps: dataset for image captioning with reading comprehension, 2020. URL https://arxiv.org/abs/2003. 12462. Singh, A., Natarajan, V., Shah, M., Jiang, Y., Chen, X., Batra, D., Parikh, D., and Rohrbach, M. Towards vqa models that can read, 2019. URL https://arxiv. org/abs/1904.08920. Wang, B., Xu, C., Zhao, X., Ouyang, L., Wu, F., Zhao, Z., Xu, R., Liu, K., Qu, Y., Shang, F., et al. Mineru: An opensource solution for precise document content extraction. arXiv preprint arXiv:2409.18839, 2024. Wei, H., Liu, C., Chen, J., Wang, J., Kong, L., Xu, Y., Ge, Z., Zhao, L., Sun, J., Peng, Y., Han, C., and Zhang, X. General ocr theory: Towards ocr-2.0 via unified end-toend model, 2024. URL https://arxiv.org/abs/ 2409.01704. Wei, H., Sun, Y., and Li, Y. Deepseek-ocr: Contexts optical compression. arXiv preprint arXiv:2510.18234, 2025a. Wei, H., Sun, Y., and Li, Y. Deepseek-ocr: Contexts optical compression, 2025b. URL https://arxiv.org/ abs/2510.18234. Wu, C., Zhang, H., Xue, S., Diao, S., Fu, Y., Liu, Z., Molchanov, P., Luo, P., Han, S., and Xie, E. Fastdllm v2: Efficient block-diffusion llm. arXiv preprint arXiv:2509.26328, 2025a. Wu, C., Zhang, H., Xue, S., Liu, Z., Diao, S., Zhu, L., Luo, P., Han, S., and Xie, E. Fast-dllm: Training-free acceleration of diffusion language models. arXiv preprint arXiv:2505.22618, 2025b. Wu, Z., Chen, X., Pan, Z., Liu, X., Liu, W., Dai, D., Gao, H., Ma, Y., Wu, C., Wang, B., Xie, Z., Wu, Y., Hu, K., Wang, J., Sun, Y., Li, Y., Piao, Y., Guan, K., Liu, A., Xie, X., You, Y., Dong, K., Yu, X., Zhang, H., Zhao, L., Wang, Y., and Ruan, C. Deepseek-vl2: Mixture-ofexperts vision-language models for advanced multimodal understanding, 2024. URL https://arxiv.org/ abs/2412.10302. You, Z., Nie, S., Zhang, X., Hu, J., Zhou, J., Lu, Z., Wen, J.-R., and Li, C. Llada-v: Large language diffusion models with visual instruction tuning. arXiv preprint arXiv:2505.16933, 2025. Yu, R., Ma, X., and Wang, X. Dimple: Discrete diffusion multimodal large language model. arXiv preprint arXiv:2505.16990, 2025. Yue, X., Ni, Y., Zhang, K., Zheng, T., Liu, R., Zhang, G., Stevens, S., Jiang, D., Ren, W., Sun, Y., Wei, C., Yu, B., Yuan, R., Sun, R., Yin, M., Zheng, B., Yang, Z., Liu, Y., Huang, W., Sun, H., Su, Y., and Chen, W. Mmmu: massive multi-discipline multimodal understanding and reasoning benchmark for expert agi, 2024. URL https://arxiv.org/abs/2311.16502. Zheng, L., Yuan, J., Yu, L., and Kong, L. reparameterized discrete diffusion model for text generation. arXiv preprint arXiv:2302.05737, 2023. Wang, H., Pan, C., Guo, X., Ji, C., and Deng, K. From object detection to text detection and recognition: brief evolution history of optical character recognition. Wiley Interdisciplinary Reviews: Computational Statistics, 13 (5):e1547, 2021. Zhou, J., Ding, T., Chen, T., Jiang, J., Zharkov, I., Zhu, Z., and Liang, L. Dream: Diffusion rectification and In Proceedings of the estimation-adaptive models. IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 83428351, 2024. 11 0. 0.4 0.3 0.2 0.1 a D E DODO: Discrete OCR Diffusion Models Confidence Thresholding Confidence Top-K Dilated Unmasking Scheduler p=0.5 p=0.6 K=32 DUS p=0.75 p=0.7 K=16 p=0.8 K=8 K= K=1 K=4 p=0.99 p=0.98 p=0.95 p=0. p=0.85 p=0.9 0 0 5 15 20 25 30 35 Tokens/Second 45 50 55 60 Figure B2. Edit Distance vs Speed for different sampling strategies. Confidence Thresholding (Green) achieves the optimal balance at = 0.98, maintaining high accuracy comparable to slow Top-K settings while offering adaptive speedups. A. Implementation Details Model Architecture. We utilize the Qwen2.5-VL-3BInstruct (Bai et al., 2025c) as our backbone architecture. We retain the default image preprocessing pipeline provided by Qwen-VL and fine-tune the model with maximum sequence length of 8192 tokens to accommodate dense document texts. Training Configuration. We train DODO for total of 200, 000 steps on node of 8NVIDIA A100 (40GB) GPUs, using global batch size of 8. Optimization is performed using AdamW with peak learning rate of 5 106 and weight decay of 0.01. We employ Warmup-Steady-Decay (WSD) learning rate scheduler, consisting of linear warmup for 5, 000 steps, constant steady phase, and linear cooldown to zero over the final 20, 000 steps. To ensure training stability and efficiency, we utilize bfloat16 precision throughout the process. Diffusion Setup. Following recent advances in discrete diffusion (Wu et al., 2025a; Li et al., 2025a), we utilize complementary masking during training. For timestep sampling, we employ stratified uniform scheduling to ensure balanced coverage of the noise levels during training. B. Additional Ablations B.1. Sampling Strategies Figure A1. Visualization of the attention structure. Full bidirectional attention allows prior blocks to attend to the current block (green hatched), meaning their internal representations dynamically adapt during the forward pass. Block-causal masking prevents prior blocks from attending to the current block. This ensures the history representations remain invariant, enabling exact KV-caching for faster inference. To optimize the inference process, we investigate the impact of the decoding schedule. Given the intolerance of OCR tasks to transcription errors, our primary objective is not merely to maximize speed, but to identify the fastest strategy that maintains high fidelity. We compares three distinct sampling strategies: 12 DODO: Discrete OCR Diffusion Models Confidence Thresholding (Yu et al., 2025): dynamic strategy that unmasks all tokens whose prediction probability exceeds fixed threshold p. This allows for adaptive step sizes: the model accelerates through clear text and slows down for ambiguous regions. This is the default strategy used in our main experiments (with = 0.98). Confidence Top-K (Zheng et al., 2023): fixed-rate strategy that unmasks exactly tokens per step, guaranteeing steady generation pace regardless of model confidence. Dilated Unmasking Scheduler (DUS) (Luxembourg et al., 2025): structural strategy that unmasks tokens at logarithmic rate, prioritizing tokens that are spatially distant to minimize joint entropy. Figure B2 illustrates the performance trade-offs. Contrary to its success in math and coding tasks, DUS yields suboptimal results for document transcription, with error rates nearly 3 higher than our primary baseline. We attribute this to the domain gap: while spatial dilation aids global coherence, it disrupts the local sequential dependencies required for accurate text transcription. In the high-accuracy regime required for OCR, Confidence Thresholding emerges as the superior strategy. By strictly enforcing high confidence floor (p = 0.98), it ensures that the model only commits to tokens when certainty is high. While other strategies (such as aggressive Top-K) may achieve higher raw throughput, they do so at the cost of unacceptable error rates. Thus, confidence thresholding provides the optimal balance, maximizing speed strictly within the bounds of usable accuracy. C. Document Parsing Examples We present qualitative results in Figures C3 and C4. Selected from the OmniDocBench dataset, these examples demonstrate DODOs capability to transcribe dense text and preserve complex layout structures, while the accompanying heatmaps visualize the underlying parallel decoding process. 13 DODO: Discrete OCR Diffusion Models Figure C3. Qualitative Results. Each row displays different document from OmniDocBench. Left: Input document image. Center: DODOs generated transcript rendered to PDF. Right: Visualization of the decoding process (heatmap of token commitment order). DODO successfully recovers complex layouts, including multi-column text, tables, and mathematical formulas, while maintaining high parallel efficiency. 14 DODO: Discrete OCR Diffusion Models Figure C4. Additional Qualitative Results."
        }
    ],
    "affiliations": [
        "Technion - Israel Institute of Technology"
    ]
}