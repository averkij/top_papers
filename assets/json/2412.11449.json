{
    "paper_title": "Whisper-GPT: A Hybrid Representation Audio Large Language Model",
    "authors": [
        "Prateek Verma"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "We propose WHISPER-GPT: A generative large language model (LLM) for speech and music that allows us to work with continuous audio representations and discrete tokens simultaneously as part of a single architecture. There has been a huge surge in generative audio, speech, and music models that utilize discrete audio tokens derived from neural compression algorithms, e.g. ENCODEC. However, one of the major drawbacks of this approach is handling the context length. It blows up for high-fidelity generative architecture if one has to account for all the audio contents at various frequencies for the next token prediction. By combining continuous audio representation like the spectrogram and discrete acoustic tokens, we retain the best of both worlds: Have all the information needed from the audio at a specific time instance in a single token, yet allow LLM to predict the future token to allow for sampling and other benefits discrete space provides. We show how our architecture improves the perplexity and negative log-likelihood scores for the next token prediction compared to a token-based LLM for speech and music."
        },
        {
            "title": "Start",
            "content": "WHISPER-GPT: HYBRID REPRESENTATION AUDIO LARGE LANGUAGE MODEL"
        },
        {
            "title": "Prateek Verma",
            "content": "Stanford University, Stanford CA, 94305 4 2 0 2 6 1 ] . [ 1 9 4 4 1 1 . 2 1 4 2 : r ABSTRACT We propose WHISPER-GPT: generative large language model (LLM) for speech and music that allows us to work with continuous audio representations and discrete tokens simultaneously as part of single architecture. There has been huge surge in generative audio, speech, and music models that utilize discrete audio tokens derived from neural compression algorithms, e.g. ENCODEC. However, one of the major drawbacks of this approach is handling the context length. It blows up for high-fidelity generative architecture if one has to account for all the audio contents at various frequencies for the next token prediction. By combining continuous audio representation like the spectrogram and discrete acoustic tokens, we retain the best of both worlds: Have all the information needed from the audio at specific time instance in single token, yet allow LLM to predict the future token to allow for sampling and other benefits discrete space provides. We show how our architecture improves the perplexity and negative log-likelihood scores for the next token prediction compared to token-based LLM for speech and music. 1 Index Terms Hybrid LLMs, Whisper, GPT 1. INTRODUCTION AND RELATED WORK Transformers and LLMs have exponentially surged the advancements in artificial intelligence across fields and modalities. Initially designed for machine translation [1], they were quickly adopted for variety of fields such as text [2], raw audio waveforms[3, 4], acoustic and music tokens [5, 6, 7], videos[8] to name few. It quickly superseded widely popular convolutional architectures too for audio [9, 10] and vision [11] perception. With the recent Gemini family combining modalities from the ground up [12] or multi-modal models like Chameleon [13], the future will bring in even more exciting advancements toward AGI that will allow architectures to see, hear, read, and reason like humans. Almost all papers in speech and music adopt these architectural advances by posing audio and music language modelling as generative 1This work was proto-typed using the support of the Stanford Institute of Human-Centered AI (HAI) through Google Cloud grant for the academic year 2023-2024. The authors thank Google and Stanford HAI for this initiative. models on acoustic tokens. The seminal paper in this direction was VQ-VAE architecture [14], which learned discrete representation from continuous audio and modelled these discrete representations using neural architectures. This was further extended to music by OpenAI in famous JukeBox architecture[15], which utilized multiscale discrete representations conditioned on lyrics and text. One of this methods main drawbacks was using WavNet[16] based vocoders that led to the slow generation process. This led to early works on using codec-based methods; audio compression algorithms were trained on audio waveforms in an autoencoder-like setup in works like Soundstream or ENCODEC [17, 18] to get discrete tokens. This recipe has become defacto in speech and music generative architectures such as Textless NLP[19, 20], AudioLM[7], MusicLM[21], MusicGen[22] to name few. Like VALL-E[23], all of them first model coarse Sound-stream or ENCODEC tokens followed by fine-grained tokens. Our work also draws inspiration from using hybrid input representations with early fusion for solving downstream audio tasks [24, 25]. This paper will tackle generating only the coarsest tokens for all experiments, as other tokens are generated and conditioned on them in causal/noncausal manner in AudioLM/VALL-E. We also draw inspiration from Whisper, which is seq-to-seq encoder-decoder model. It goes from mel-spectrogram input to GPT tokens for ASR. Whisper combines continuous input-discrete output in noncausal setups. This work asks Can we devise an architecture that simultaneously utilizes continuous and discrete representation in the LLM setup? The papers contributions are as follows: 1. To the best of our knowledge; we introduce the first hybrid generative causal architecture audio shown for speech and music that combines continuous acoustic representation like mel-spectrogram with discrete acoustic tokens. 2. We adapt Whisper-like architecture, noncausal ASR seq, to seq architecture for generative modelling. We replace the Whisper encoder with decoder and, on learned representation, carry early fusion with decoder-only architecture operating on acoustic tokens. 3. We showcase significant improvements in the next token prediction for the music and speech dataset in VALL-E-like setup, which predicts the coarsest acoustic token with the hybrid. Fig. 1. (Left) Whisper Architecture proposed by OpenAI [26] which treats ASR as sequence to sequence which takes in mel-spectrogram slices and decodes it token by token. It has Transformer Encoder stack on the spectrogram followed by Transformer decoder, trained for the shift-by-one token prediction, and the cross-attention module on learned spectrogram representation. (Right) Our generative model combines both continuous and discrete representations. We align the spectrogram and ENCODEC coarse tokens. Instead of Transformer encoder, we pass spectrogram slices through lightweight decoder blocks. The learned representation per-token slice is concatenated with discrete tokens corresponding to the spectrogram slice to have decoder Transformer stack, trained on shift by one next token prediction, similar to typical LLM pre-training. 2. DATASET We report results on two domains: music and speech. For the case of speech, we use the LibriSpeech TTS dataset [27]. We chose this over LibriSpeech as it contains material derived from the original LibriSpeech corpus for TTS research. It also removes utterances with significant background noise. Finally, it only consists of 24 kHz sampled utterances and removes other lower sampling rates, thereby matching the rate for ENCODEC [18] acoustic tokenizer rate. For the case of music, we use publically available music recordings of instrumental music, namely piano, saxophone, harp, flute, violin, marimba, etc., for total of 200 hours of music2. We extract 64-channel mel-spectrogram input to the hybrid models and adjust the hop/window length to match the 75Hz rate of ENCODEC acoustic tokens. We extract causal acoustic tokens at the coarsest level for the entire paper, similar to as described in VALL-E [23] and predict the next tokens i) using purely discrete token-based GPT architecture ii) hybrid mel-spectrogram-based architecture that takes in both the current mel-spectrogram slice and coarsest discrete acoustic token to predict the future discrete token. 3. METHODOLOGY the models discussed in the paper are Transformer All decoder-only architecture. The recipe has become typical for music [22] and speech generation [19], where the coarsest 2The URLs for files used for training the music model will be shared here tokens are first predicted using auto-regressive architecture followed by finer acoustic tokens, conditioned on the coarsest tokens using auto/non-auto regressive architecture [7], [23]. This is primarily done as it is challenging to model long-context acoustic tokens for audio. This work aims to determine how well we can model the coarsest tokens, as errors in modelling the coarser tokens will lead to the finer tokens being modelled incorrectly as they are conditioned on the coarsest token, thus affecting model performance. We report how well the model performs in pre-training instead of post-training, as we are interested in building better foundational architectures in this paper. The other motivation is to push the capabilities of smaller transformer-based decoder architectures that can be trained in academia. The current architectures, such as those used in AudioLM and Vall-E, trained from scratch, are beyond the reach of most academic setups. We do not use other techniques such as distillation [28, 29] or pruning [30] that often uses the capabilities of larger architecture to push the performance of smaller architecture. Instead, we use better input representation to achieve the performance of much larger architecture, which is purely token-based model. To our knowledge, we propose the first hybrid large language model that utilizes continuous representations like mel-spectrogram and acoustic tokens in causal setup. Given the extended context, using pure discrete token-based architecture is not easy to model; for ENCODEC, the input audio would be represented by 75x8 = 600 tokens per second for coarser to finer tokens. For all the experiments proposed in our paper, we use context length minimizing the cross-entropy loss with ground truth tokens. 3.2. Scaled GPT Architecture We compared our baseline architecture with medium-sized GPT-2 architecture. Both of our architectures, i.e., the baseline and GPT-L architecture, are the same as that of VALL-E [23] except that it is shrunk-down version of it in terms of model dimension, number of layers, and number of heads. For reference, VALL-E, trained on acoustic tokens precisely the same way we have described, has model dimension of 1024 with the number of layers as 12 and 16 attention heads with total of around 900 million parameters. This is far beyond anything that can be trained from scratch in an academic setup. Hence, we first proposed shrunk-down baseline architecture. We then trained the largest possible GPT architecture that we could train using the resources available at our disposal. We used 16 attention heads with eight layers and model dimension 256, resulting in 40 million parameters. This is our scaled GPT architecture, which we call GPTLarge, ten times larger than our baseline model. We compare our hybrid architecture and the baseline architecture with this model. GPT-L is also trained with the same recipe for 25 epochs. For both of these architectures, we trained them on 10 seconds of context or 750 tokens as context length. 3.3. Hybrid Architecture For our proposed hybrid architecture, we retain the GPT branch as described in the baseline section. However, we have Whisper-inspired Transformer Decoder stack that fed the mel-spectrogram of the original input audio from which we extracted ENCODEC tokens. We first compute the log-mel spectrogram with hop length to match the ENCODEC sampling frequency, i.e., 75 Hz. We computed 64 mel spectrogram bins with 40ms window length and converted the sampling frequency to match the sampling rate of ENCODEC tokens, i.e., 24 KHz. This gave us representation of 64 mel-bins 75 for every second of audio. Even though ENCODEC tokens are fully causal, further care was carried out to maintain causality by ensuring that the window computation is causal, i.e., not centred for every hop to prevent leakage. In addition, instead of predicting the next token from the spectrogram slice, i.e., the default shift by one prediction, we opted for the shift by 2, i.e., the current ENCODEC coarse token is predicted from all past slices, barring the most recent two slices. We retain the same architecture as the Whisper-encoder with two modifications: i) We shrink it in terms of number of parameters, and ii) We have causal attention masks so as not to attend to future spectrogram slices to predict the current ENCODEC token. For the Whisper decoder, we have six layers, with the model dimension of 32 (half as our GPT baseline) and the feed-forward dimension four times that of the model dimension. We pass single spectrogram slices as input and normalize them across the corpus Fig. 2. Comparison of GPT on coarse acoustic tokens with i) GPT-L ii) Our hybrid continuous-discrete representation. of 10s. Modelling 6000 tokens would have exponentially increased the training times of our architecture: such large context lengths are difficult to handle using attention-based methods in LLM setup. Further, building continuous audio foundational architecture based on mel-spectrogram has its challenges, and sampling from continuous space to bring diversity in generated output is difficult. Finally, we would have relied on Vocoders or algorithms like Griffin-Lim to return to the original audio. By combining continuous audio representation like mel-spectrogram and discrete acoustic tokens, we retain the best of both worlds: Have all the information needed from the audio at specific time instance in single token, yet allow the LLM to predict the future token to allow for sampling and other benefits discrete space provides. 3.1. Baseline Architecture All the architectures proposed in Transformer decoder architecture are widely used in [23]. We keep the architecture topologies the same, with the same number of parameters for speech and music, as they both are based on ENCODEC. It is one of the most commonly used open-source tokenizers for speech and music synthesis since its release in 2022. The baseline architecture consists of 8 Transformer Decoder layers with eight attention heads, an embedding dimension of 64, with the size of the feed-forward dimension four times that of the embedding dimension typically used in Transformer recipes. The final layer of the Transformer is then piped to dense layer of 2048 neurons followed by 1024 dim dense layer equal to the vocabulary of the ENCODEC. This has the total number of parameters as 3.7 million. All models were trained for 25 epochs on random crops of 750 token context length from LibriSpeech and Music dataset using Adam optimizer. The initial learning rate was chosen to be 2e-4 and decayed to 1e-4 after ten epochs. As with LLM pre-training, the goal is to predict the next token given the past context while 4. RESULTS AND DISCUSSION We carry out various ablation studies to report how well our generative model does in the following section. We are only interested in quantifying the likelihood scores for the generative architecture pre-training. The rationale is as follows - i) We propose hybrid GPT-like architecture. Given the previous context, these architectures are trained on simple objective to predict the next token, typical of LLM pre-training. In order to improve on this, models are usually scaled in the number of parameters with identical topologies. We report our results on baseline GPT-S architecture and compare it with scaling it to an architecture ten times in size. We then compare our results with our hybrid architecture, as reported in Tables 1 and 2. We see that for both speech and music, we outperform an architecture that is ten times larger than the baseline model, thus pushing the boundaries of large language models by using better input representation. By combining continuous audio representation like spectrogram and discrete acoustic tokens, we retain the best of both worlds: Have all the information needed from the audio at specific time instance in single token, yet allow LLM to predict the future token to allow for sampling and other benefits discrete space provides. One of the reasons we see our hybrid architecture Table 1. Negative-log likelihood (NLL) and Perplexity (PPL) scores for our proposed hybrid architecture, baseline GPTSmall and GPT-Large 10 times larger than GPT-Small for LibriSpeech. Model Baseline GPT-S GPT-L Hyrbid LLM # of Param NLL Accuracy 2.02 3.7 million 1.94 40 million 1.93 4 million PPL 34.18% 7.54 34.82% 6.96 35.05% 6.96 outperforms significantly better for music tokens vs. speech tokens is that music has much more information present about instrumentation, pitch, timbre, and multiple harmonic components, which is hard to model as compared to LibriSpeech which is single speaker saying English words. Further, for music signals, just modelling the coarsest token might not capture all the variation as compared to having hybrid-like architecture that looks at both the acoustic token representation and the mel-spectrogram-like representation. We utilize hybrid representation with fewer parameters to predict the next token. Table 2. Negative-log likelihood (NLL) and Perplexity (PPL) scores for our proposed hybrid architecture, baseline GPTSmall and GPT-Large 10 times larger than GPT-Small for Music Dataset. Model Baseline GPT-S GPT-L Hyrbid LLM # of Param NLL Accuracy 2.78 3.7 million 2.77 40 million 2.52 4 million PPL 34.96% 16.12 35.72% 15.96 38.47% 12.43 Fig. 3. Comparison of GPT on coarse acoustic tokens with i) GPT-L ii) Our hybrid continuous-discrete representation. to have unit Gaussian distribution. In this case, we use single linear layer of 2048 neurons followed by dense layer to map it to the embedding dimension of the model (32). Relative sinusoidal positional embeddings are added, followed by stack of decoder layers. We call it the Whisper-Decoder, which resembles the Whisper-Encoder part of the Whisper architecture. The output of the last layer of Whisper-Decoder has dimension of 32 for all the tokens present in the data, i.e., 750 for context length chosen as 10s. We now use an embedding dimension of 32 for the input embedding matrix for ENCODEC acoustic tokens and add sinusoidal positional embeddings, as shown in Figure 1. The output of the input embedding matrix is then concatenated with the last layer of the Whisper-Decoder output, which has dimension of 32 for all tokens. This gives us 64-dimension vector for all of the inputs (750) of the present, i.e., 32 coming from the input embedding matrix of the ENCODEC tokens and 32 coming out of the last layer of the Whisper-Decoder block. This concatenated 64-dimensional vector is then passed to the stack of decoder layers, which are the same as our baseline, i.e., eight layers with model dimension 64 and feed-forward dimension four times that of the embedding dimension and eight attention heads. The output of the last layer is then passed to dense layer of 2048 neurons, followed by 1024dimensional dense layer to conform with the vocabulary of the ENCODEC tokens. This has the total number of parameters as 4.1 million. All models were trained for 25 epochs on random crops of 750 token context length from LibriSpeech and Music dataset using Adam optimizer. The initial learning rate was chosen to be 2e-4 and decayed to 1e-4 after ten epochs. As with all LLM pre-training, the goal is to predict the next token given the past context while minimizing the cross-entropy loss. 5. CONCLUSION AND FUTURE WORK We show how one can build hybrid generative architectures for speech and music using hybrid continuous-discrete representation. The proposed architecture outperforms purely token-based model by combining continuous audio and discrete token-based representations. Our experiments show that by using hybrid continuous-discrete representation for two datasets, we can match the performance of using acoustic tokens of 40M parameter architecture with model with only 4M parameters. This retains the best of both worlds: allowing the model to have complete information on the audio signal in single token embedding while still predicting discrete acoustic tokens that we can sample from to introduce diversity and variations in the generated output. 6. ACKNOWLEDGEMENT This work was proto-typed using the support of the Stanford Institute of Human-Centered AI (HAI) through Google Cloud grant for the academic year 2023-2024. We thank both Google and Stanford HAI for this initiative. 7. REFERENCES [1] Ashish Vaswani et. al, Attention is all you need, in Advances in neural information processing systems, 2017, pp. 59986008. [2] T. Brown et, al, Language models are few-shot learners, arXiv preprint arXiv:2005.14165, 2020. [3] Prateek Verma and Chris Chafe, generative model for raw audio using transformer architectures, 2021 24th International Conference on Digital Audio Effects (DAFx), pp. 230237, 2021. [4] Prateek Verma, Goodbye waveneta language model for raw audio with context of 1/2 million samples, arXiv preprint arXiv:2206.08297, 2022. [5] Anna Huang et. al, Music transformer: Generating music with long-term structure, in International Conference on Learning Representations (ICLR), 2019. [6] Prateek Verma and Julius Smith, framework for contrastive and generative learning of audio representations, arXiv preprint arXiv:2010.11459, 2020. [7] Zalan Borsos et. al, Audiolm: language modeling approach to audio generation, IEEE/ACM Transactions on Audio, Speech, and Language Processing, 2023. [8] Wilson Yan, Yunzhi Zhang, Pieter Abbeel, and Aravind Srinivas, Videogpt: Video generation using vq-vae and transformers, arXiv preprint arXiv:2104.10157, 2021. [9] Prateek Verma and Jonathan Berger, Audio transformers: Transformer architectures for large scale audio understanding., arXiv preprint arXiv:2105.00335, 2021. [10] Y. Gong et. al, Improving audio tagging with pretraining, sampling, labeling, and aggregation, IEEE/ACM TALSP, p. 3292, 2021. Psla: [11] Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Horst Bischof, and Bernt Schiele, An image is worth 16x16 words: Transformers for image recognition at scale, in Proceedings of the International Conference on Learning Representations (ICLR), 2021. [12] Gemini Team, Gemini: family of highly capable multimodal models, arXiv preprint arXiv:2312.11805, 2023. [13] Chameleon, Chameleon: Mixed-modal early-fusion foundation models, arXiv preprint arXiv:2405.09818, 2024. [14] Aaron van den Oord, Oriol Vinyals, and Koray Kavukcuoglu, Neural discrete representation learning, arXiv preprint arXiv:1711.00937, 2017. [15] Prafulla Dhariwal, Heewoo Jun, et al., generative model arXiv:2005.00341, 2020. for music, Jukebox: arXiv preprint [16] A. Oord et. al, Wavenet: generative model for raw audio, arXiv preprint arXiv:1609.03499, 2016. [17] Neil Zeghidour et. al, Soundstream: An end-to-end neural audio codec, IEEE/ACM Transactions on Audio, Speech, and Language Processing, vol. 30, pp. 495 507, 2021. [18] Alexandre Defossez, Jade Copet, Gabriel Synnaeve, and Yossi Adi, High fidelity neural audio compression, arXiv preprint arXiv:2210.13438, 2022. [19] Kushal Lakhotia, Eugene Kharitonov, Wei-Ning Hsu, Yossi Adi, Adam Polyak, Benjamin Bolte, Tu-Anh Nguyen, Jade Copet, Alexei Baevski, Abdelrahman Mohamed, et al., On generative spoken language modeling from raw audio, Transactions of the Association for Computational Linguistics, vol. 9, pp. 13361354, 2021. [20] Eugene Kharitonov, Jade Copet, Kushal Lakhotia, Tu Anh Nguyen, Paden Tomasello, Ann Lee, Ali Elkahky, Wei-Ning Hsu, Abdelrahman Mohamed, Emmanuel Dupoux, et al., textless-lib: library for textless spoken language processing, arXiv preprint arXiv:2202.07359, 2022. [21] Andrea Agostinelli, Timo Denk, Zalan Borsos, Jesse Engel, Mauro Verzetti, Antoine Caillon, Qingqing Huang, Aren Jansen, Adam Roberts, Marco Tagliasacchi, et al., Musiclm: Generating music from text, arXiv preprint arXiv:2301.11325, 2023. [22] Jade Copet, Felix Kreuk, Itai Gat, Tal Remez, David Kant, Gabriel Synnaeve, Yossi Adi, and Alexandre Defossez, Simple and controllable music generation, Advances in Neural Information Processing Systems, vol. 36, 2024. [23] Chengyi Wang, Sanyuan Chen, Yu Wu, Ziqiang Zhang, Long Zhou, Shujie Liu, Zhuo Chen, Yanqing Liu, Huaming Wang, Jinyu Li, et al., Neural codec language models are zero-shot text to speech synthesizers, arXiv preprint arXiv:2301.02111, 2023. [24] Prateek Verma, Alessandro Ilic Mezzay, Chris Chafe, and Cristina Rottondi, deep learning approach for low-latency packet loss concealment of audio signals in networked music performance applications, in 2020 27th Conference of Open Innovations Association (FRUCT). IEEE, 2020, pp. 268275. [25] L. Wang and A. Oord, learning of audio representations, arXiv:2103.06508, 2021. Multi-format contrastive arXiv preprint [26] Alec Radford, Jong Wook Kim, Tao Xu, Greg Brockman, Christine McLeavey, and Ilya Sutskever, Robust speech recognition via large-scale weak supervision, in International conference on machine learning. PMLR, 2023, pp. 2849228518. [27] Heiga Zen, Viet Dang, Rob Clark, Yu Zhang, Ron Weiss, Ye Jia, Zhifeng Chen, and Yonghui Wu, Libritts: corpus derived from librispeech for text-tospeech, arXiv preprint arXiv:1904.02882, 2019. [28] Geoffrey Hinton, Oriol Vinyals, and Jeffrey Dean, DisarXiv tilling the knowledge in neural network, preprint arXiv:1503.02531, vol. abs/1503.02531, 2015. [29] Yuxian Gu, Li Dong, Furu Wei, and Minlie Huang, MiniLLM: Knowledge distillation of large language models, in The Twelfth International Conference on Learning Representations, 2024. [30] Mingjie Sun, Zhuang Liu, Anna Bair, and Zico Kolter, simple and effective pruning approach for large language models, in The Twelfth International Conference on Learning Representations, 2024."
        }
    ],
    "affiliations": [
        "Stanford University, Stanford CA, 94305"
    ]
}