{
    "paper_title": "ReflAct: World-Grounded Decision Making in LLM Agents via Goal-State Reflection",
    "authors": [
        "Jeonghye Kim",
        "Sojeong Rhee",
        "Minbeom Kim",
        "Dohyung Kim",
        "Sangmook Lee",
        "Youngchul Sung",
        "Kyomin Jung"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Recent advances in LLM agents have largely built on reasoning backbones like ReAct, which interleave thought and action in complex environments. However, ReAct often produces ungrounded or incoherent reasoning steps, leading to misalignment between the agent's actual state and goal. Our analysis finds that this stems from ReAct's inability to maintain consistent internal beliefs and goal alignment, causing compounding errors and hallucinations. To address this, we introduce ReflAct, a novel backbone that shifts reasoning from merely planning next actions to continuously reflecting on the agent's state relative to its goal. By explicitly grounding decisions in states and enforcing ongoing goal alignment, ReflAct dramatically improves strategic reliability. This design delivers substantial empirical gains: ReflAct surpasses ReAct by 27.7% on average, achieving a 93.3% success rate in ALFWorld. Notably, ReflAct even outperforms ReAct with added enhancement modules (e.g., Reflexion, WKM), showing that strengthening the core reasoning backbone is key to reliable agent performance."
        },
        {
            "title": "Start",
            "content": "ReflAct: World-Grounded Decision Making in LLM Agents via Goal-State Reflection Jeonghye Kim1, Sojeong Rhee1, Minbeom Kim2, Dohyung Kim2, Sangmook Lee2, Youngchul Sung1, Kyomin Jung2, 1KAIST, 2Seoul National University {jeonghye.kim, sojeong.rhee, ycsung}@kaist.ac.kr, kjung@snu.ac.kr 5 2 0 2 1 ] . [ 1 2 8 1 5 1 . 5 0 5 2 : r a"
        },
        {
            "title": "Abstract",
            "content": "Recent advances in LLM agents have largely built on reasoning backbones like ReAct (Yao et al., 2023), which interleave thought and action in complex environments. However, ReAct often produces ungrounded or incoherent reasoning steps, leading to misalignment between the agents actual state and goal. Our analysis finds that this stems from ReActs inability to maintain consistent internal beliefs and goal alignment, causing compounding errors and hallucinations. To address this, we introduce ReflAct, novel backbone that shifts reasoning from merely planning next actions to continuously reflecting on the agents state relative to its goal. By explicitly grounding decisions in states and enforcing ongoing goal alignment, ReflAct dramatically improves strategic reliability. This design delivers substantial empirical gains: ReflAct surpasses ReAct by 27.7% on average, achieving 93.3% success rate in ALFWorld. Notably, ReflAct even outperforms ReAct with added enhancement modules (e.g., Reflexion, WKM), showing that strengthening the core reasoning backbone is key to reliable agent performance."
        },
        {
            "title": "Introduction",
            "content": "Recent advancements in Large Language Models (LLMs) have significantly enhanced their reasoning capabilities, enabling LLM-based agents to perform complex multi-step decision making beyond static problem solving (Forootani, 2025; Chervonyi et al., 2025). As LLMs are extended into agentic frameworks where they interact with open-ended environments, the ability to solve longhorizon tasks through ongoing interaction and sequential reasoning has become central research focus (Zhou et al., 2023; Song et al., 2024). large number of recent LLM agents build on the ReAct (Yao et al., 2023) framework, which inEqual contribution. Co-corresponding authors. 1 Figure 1: Comparison of reasoning influence in ReflAct and ReAct. While ReAct focuses on the current observation and the next action at each timestep, ReflAct reflects on the internal belief state and the task goal. tegrates Chain-of-Thought (CoT) reasoning (Wei et al., 2022) with an action selection mechanism. Various extensions have been proposed to enhance this backbone by incorporating reflective iterations or memory components to improve reasoning performance (Shinn et al., 2024; Liu et al., 2023; Qiao et al., 2024). However, mounting evidence suggests that ReAct and its variants often deliver limited benefits and can even deteriorate performance, especially in complex, partially observable, or dynamic environments (Ma et al., 2025; Chang et al., 2024; Verma et al., 2024). These findings raise questions about the current direction of extending the basic ReAct reasoning-action framework. In this paper, we claim that true progress lies in fundamentally redesigning the backbone reasoning process itself, rather than adding complementary modules. Our analysis shows that the fundamental problem of current ReAct-based reasoning methods is the frequent lack of proper grounding of generated thoughts on the agents historical context or overarching objective. These misaligned thoughts guide subsequent action choices. In dynamic or partially observable environments, such reasoning errors compound over time, leading to increasingly divergent internal beliefs, deteriorated decision quality, and failed task execution. To address this limitation, we introduce ReflAct (Reflect for Action), new backbone framework that shifts the focus of the agents thought from predicting the next action to continuously reflecting on its current state in the context of the task goal, as shown in Figure 1. Unlike previous approaches that layer reflective or memory modules onto the ReAct backbone (Shinn et al., 2024; Qiao et al., 2024; Xiong et al., 2025), ReflAct does not rely on such additional components but replaces the core reasoningaction cycle itself. That is, at each timestep, the agent evaluates whether its current trajectory and situation align with its intended longterm objective before selecting an action. This reflective thought mechanism enables early detection of potential deviations and facilitates timely strategy adjustments, as we shall see shortly. By grounding decisions in actual observations and maintaining continuous goal alignment, ReflAct substantially reduces hallucination tendency and enhances long-term strategic coherence. We implemented ReflAct using both opensource (Llama 3.1-8B/70B-Instruct (Grattafiori et al., 2024)) and proprietary (GPT-4o-mini/4o (OpenAI, 2024)) LLMs, and evaluated it in three text-based environments: ALFWorld (Shridhar et al., 2021), ScienceWorld (Wang et al., 2022) and Jericho (Hausknecht et al., 2020), which require an agent to decompose the goal and acts over long time horizon under partial observability. Empirical results show that ReflAct helps the agent better align its internal beliefs with the environment and promotes goal-consistent behavior. As result, ReflAct significantly outperforms ReAct by 36.4%, 8.5%, and 38.1% on ALFWorld, ScienceWorld, and Jericho, respectively. Notably, ReflAct also surpasses enhancement modules layered on top of existing backbones, demonstrating that revising the reasoning process itself can be more effective than adding new components."
        },
        {
            "title": "2 Preliminaries",
            "content": "We model the agents task in language-based environment as Partially Observable Markov Decision Process (POMDP) (Puterman, 1990), defined as = U, S, A, O, P, R, where is task instruction, is the (hidden) environment state, is an action, is an observation, is the transition function, and is the reward function. In our setting, U, and are expressed in natural language. The ReAct Framework The ReAct framework (Yao et al., 2023) enables LLM agents to solve tasks through interleaved reasoning and action. To incorporate reasoning steps (thoughts), we extend the above POMDP by including thought space , where each thought τ represents natural language reasoning step generated by the agent. θ Then, this extended POMDP operates as follows. Given task instruction U, the agent follows policy πθ that alternates between generating thought as πthought and taking an executable action as πact θ . At each time step t, the agent receives an observation ot and forms context ct = (ht, ot), where ht = {u, τ1, a1, o1, . . . , τt, at} is the history of previous interactions. Based on ct, the agent samples thought from context-conditioned distribution: τt πthought ( ct). This thought τt is then appended to the context to form an enriched input = ct τt, which is used to select the next action: at πact t). The selected action at is executed to the environment, resulting in the next observation ot+1. This reasoningacting loop continues until the task is completed or predefined time limit is reached. θ ( θ"
        },
        {
            "title": "3.1 How Thought Impacts Action Selection",
            "content": "Basically the thought τt functions as reweighting mechanism for the action probability distribution of the policy πact θ by enriching the context ct into an augmented form: = ct τt. We analyzed this process in detail using task from ALFWorld (Shridhar et al., 2021), where the task is to find an apple, heat it, and place it in garbage can. We implemented the policy πθ using Llama-3.1-8B-Instruct (Grattafiori et al., 2024) and compared two agent variants: the NoThinking agent, which selects actions without intermediate reasoning, and the ReAct agent, which generates reasoning step before each action. Figure 2 shows one instance of execution sequence. The sequence starts with the task description with initial observation, followed by two cycles of thought-actionobservation, ending with observation \"You open the microwave 1. The microwave 1 is open. \". Then, in the next thought step, we tried each of four cases: empty (i.e., no thought) and three thoughts 2 in the middle of Figure 2, and observed the distribution on actions in the next step. Table 1: Average entropy of the action probability distribution when performing 134 ALFWorld tasks using the Llama-3.1-8B-Instruct with NoThinking and ReAct. Model HNoThinking HReAct Llama-3-1-8B-Instruct 1.23 0. agent exhibits mean entropy of 1.23, whereas the ReAct agent yields significantly lower value of 0.30. This substantial reduction in entropy implies that conditioning the policy on thought leads to more confident and focused behavior."
        },
        {
            "title": "3.2 The Problem of Ungrounded Thought",
            "content": "( Since action at is heavily influenced by thought τt πthought ct), it is crucial that τt accuθ rately captures the core decision-relevant information from the context ct = (ht, ot). If the thought becomes ungrounded, meaning it is disconnected from the actual state, it can mislead the policy and induce suboptimal or incorrect action. Figure 3: Comparison of failure rates between NoThinking and ReAct on GPT-4o and Llama-3.1-8B-Instruct To assess the overall performance degradation from ungrounded thought, we evaluated task failure rates across 134 ALFWorld (Shridhar et al., 2021) tasks using πθ realized with GPT-4o (OpenAI, 2024) and Llama-3.1-8B-Instruct (Grattafiori et al., 2024). Figure 3 categorizes the outcomes into four groups: (1) only the NoThinking agent failed, (2) only the ReAct agent failed, (3) both failed, and (4) both succeeded. As shown in the figure, incorporating thought can help the agent solve tasks that would otherwise fail without it. However, thought also introduces new risks, causing failures that could have been avoided without thought, even in stronger models like GPT-4o. To see the cause of the ReAct agents failure, we examined failure case of the ReAct agent in ALFWorld (Shridhar et al., 2021). As seen in Figure 4 (1), the agent is given simple task: Find soap bar, take it and place it on the countertop. After interacting with soap bottle 2 in cabinet 1, the agent attempts to return to cabinet 1 to search for the soap bar even though it is not there, because the generated thought does not capture the current Figure 2: Changes in the action probability distribution when modifying thought in the ALFWorld heat some apple and put it in garbagecan task. First, without any thought (NoThinking), the agent assigns high probability to the heat action, still considering other actions such as put and go with non-negligible probabilities. Now, when non-empty thought is provided, the next action probability distribution is directly influenced: THOUGHT 1 relevant to the task sharpens the distribution toward appropriate actions, leading to contextually aligned selections. In contrast, THOUGHT 2 and THOUGHT 3, injecting incorrect or conflicting information, bias the distribution and increase the likelihood of inappropriate actions. As seen, thought directly shapes the next action distribution, either reinforcing correct decisions or causing misalignment depending on its quality. When the model is provided with structured thought, it relies predominantly on that thought to drive decision making, thereby significantly reducing its dependence on the broader historical context. To quantify the overall impact of thought on the agents decision-making process, we measured the entropy of the agents action probability distribution across 134 tasks from ALFWorld. Specifically, for given timestep t, the entropy is defined as"
        },
        {
            "title": "HNoThinking",
            "content": "t = (cid:88) aA πact θ (a ct) log πact θ (a ct)"
        },
        {
            "title": "HReAct\nt",
            "content": "= (cid:88) aA θ (a πact t) log πact θ (a t), where = ct τt and τt πthought θ ( ct). We computed the entropy averaged over all timesteps and tasks, and the result is reported in Table 1. As shown in the table, the NoThinking Figure 4: In the ALFWorld task put clean soapbar in countertop, the agent picks up soapbottle from cabinet 1 and puts it back, (1) while planning the next action, it redundantly tries to go back to cabinet 1 and fails, (2) after reflecting on the state and goal, it moves elsewhere and succeeds. situation in the context of goal properly from its observation history. This redundant and illogical behavior leads to loop, and the agent ultimately fails to complete the task. failure case in ScienceWorld is provided in Figure 9 in Appendix B. From the examples, we see that the agent fails due to two major reasoning limitations: (1) lack of grounding in its internal state. The agent fails to maintain coherent internal state, leading to inconsistency, e.g., revisiting location it already visited or falsely assuming it is holding an object. (2) Short-sighted planning. The ReAct agent typically exhibits short-sighted planning, making decisions that appear locally plausible but disregard the long-term task goal. We recognize that these two shortcomings stem from incorrect guidance for the agents reasoning process although the agent has better reasoning capability. Indeed, for the same failure scenario in Figure 4 left branch, we input reasoning guidance sentence different from that of ReAct, to focus not on predicting the next step but assess the agents current state in relation to the task goal, as seen in Figure 4 right branch. Now, it is seen that the agent can better align its action with the overall objective by reflecting on what it currently knows and what it aims to achieve. This is also valid for the ScienceWorld failure case in Figure 9 in Appendix B."
        },
        {
            "title": "4 Proposed Method: ReflAct",
            "content": "In the LLM POMDP framework, thought τt should be generated so that it leads to the selection of an action at that maximizes the expected long-term return when used to condition the action policy πact θ , where the long-term return is defined as Gt = (cid:80) k=0 γkRt+k with discount factor γ [0, 1). Here, Rt+k is the immediate reward at time + k. Thus, the optimal thought τ can be defined as τ = arg max τ Eaπact θ (ctτ ) [E [Gt st, a]] , where ct denotes the current context and denotes concatenation. To maximize the expected long-term return, the thought τt should not be generated from the observation solely to decide the next action. Instead, it should be formed with consideration of the final task goal at every time step, as illustrated in Figure 1. In this way, the agent can rely on thought for action generation without deviating from goal-reaching path. Furthermore, since the environment is partially observable and the true state st is not directly accessible, the agent must rely on an internal belief state inferred from its interaction history. So, the generated thought should make this belief more explicit and stable, while encoding task-relevant reasoning that effectively guides the downstream policy toward the long-term objective. To achieve the generation of such thoughts, we do not use an additional module but exploit the LLM agents reasoning capability maximally. For 4 this, we redesign new thought generation instruction to better guide thought generation as follows: Instruction for ReflAct You should first reflect on the agents state in relation to the task goal, and then output the action for this turn. In this new design, we have the explicit guiding terms agents state, task goal and their connection term in relation to to realize the desired state diagram of thought dependency in the lower part of Figure 1. Following this instruction, the agent engages in reflective reasoning to gain clearer understanding of both the current situation and the intended goal. We refer to so-generated thought as reflection and name the new reflection-action process ReflAct. To formalize this, we define reflection space K, where each reflection is structured representation that aims to explicitly encode both the agents internal belief state and the task goal G. The belief state captures the agents current understanding of the state based on its interaction history, whereas the goal component provides concise summary of what the agent is trying to achieve. By explicitly representing both in the reflection, ReflAct allows the agent to condition its subsequent decision-making on more coherent and goal-aligned internal context. Example of Thought vs Reflection Before Action Thought: Now find spraybottle 2. Next, need to take it. Reflection: Currently, am at cabinet 2 and have found spraybottle 2, which brings me closer to completing the task of placing it on the toilet. In addition, we incorporate one-shot examples based on the tasks from Yao et al. (2023); Xiong et al. (2025) modified with Reflections on the agents state in relation to the task goal, generated by GPT-4o-mini (OpenAI, 2024). Above, we present an example describing the modification from Thought to Reflection. The complete set of one-shot examples is provided in Appendix H. planning in the initial stage. Recent enhancement modules sought to improve ReActs CoT through various mechanisms: Reflexion (Shinn et al., 2024) through post-task analysis of failures after task completion, WKM (Qiao et al., 2024), MPO (Xiong et al., 2025), and DC (Suzgun et al., 2025) via external memory of environment knowledge, and RAFA (Liu et al., 2023) by generating and evaluating future possible trajectory predictions. However, when the ReAct backbone incorrectly grounds past observations, all these approaches suffer from degraded performance. Therefore, ReflAct retouches the reasoning process itself to enable more grounded and goalaligned decision making. In our experiments, we will validate the importance of this approach compared to the design of enhancement modules, as well as its synergy with them. Additional related works are discussed in the Appendix A."
        },
        {
            "title": "6.1 Experimental Settings",
            "content": "Benchmarks. We benchmarked ReflAct on three widely used text-based environments: ALFWorld (Shridhar et al., 2021), ScienceWorld (Wang et al., 2022), and Jericho (Hausknecht et al., 2020). ALFWorld evaluates embodied agents on household tasks, ScienceWorld assesses procedural and scientific reasoning in educational scenarios, and Jericho tests agents in classic interactive fiction games that require commonsense reasoning and memory. For evaluation, ALFWorld uses binary task success, while ScienceWorld and Jericho provide dense reward signals, enabling evaluation based on both success rate and average reward, calculated as the mean reward across all tasks. Additional benchmark details are provided in Appendix C. Agent Models. We use GPT-4o and GPT-4o-mini (OpenAI, 2024), and Llama-3.1-8B/70B-Instruct (Grattafiori et al., 2024) as underlying models. GPT-4o variants serve as proprietary models, while Llama-3.1-Instruct variants represent open-source counterparts, each with large and small sizes."
        },
        {
            "title": "6.2 Comparison with Prior Methods",
            "content": "Building on CoT reasoning (Wei et al., 2022), numerous approaches attempted to enhance LLM agent capability. ReAct (Yao et al., 2023) pioneered the interleaving of reasoning with action steps for interactive environments, while Plan-andSolve (Wang et al., 2023) introduced higher-level Baselines. We first compare ReflAct with three reasoning frameworks. Details for each can be found in Appendix D.1. (1) NoThinking (Ma et al., 2025): The agent generates an action directly at each time step without any reasoning step. 5 Table 2: Performance comparison of ReflAct with NoThinking, ReAct, and Plan-and-Act across ALFWorld, ScienceWorld, and Jericho. SR and AR denote success ratio and average reward, respectively. Values in parentheses indicate percentage improvement over the NoThinking baseline. Model Prompting GPT-4o GPT-4o-mini Llama-3-1-8B -Instruct Llama-3-1-70B -Instruct NoThinking ReAct Plan-and-Act ReflAct NoThinking ReAct Plan-and-Act ReflAct NoThinking ReAct Plan-and-Act ReflAct NoThinking ReAct Plan-and-Act ReflAct ALFWorld ScienceWorld Jericho SR 76.1 85.1 85.8 93.3 43.3 53. 59.0 66.4 21.6 29.1 30.6 60. 53.7 81.3 81.3 83.6 AR 67. 68.7 68.7 68.9 42.3 49.1 51. 55.4 29.9 43.0 34.2 47.2 65. 66.4 67.7 73.7 SR 50.2 55. 55.0 57.8 21.8 37.0 35.5 37. 14.2 27.5 14.7 33.2 46.4 53. 52.6 58.8 AR 27.8 50.4 45. 53.2 18.8 29.8 37.1 45.4 10. 12.8 10.6 20.9 27.4 33.9 29. 44.5 SR 10.0 20.0 20.0 35. 5.0 15.0 15.0 20.0 0.0 0. 0.0 10.0 5.0 10.0 10.0 20. Average 46.3 56.0 (+21.0%) 55.0 (+18.8%) 61.6 (+33.1%) 26. 36.8 (+40.2%) 39.7 (+51.1%) 44.8 (+70.9%) 15.1 22.5 (+48.5%) 18.0 (+19.0%) 34.4 (+126.9%) 39.6 48.9 (+23.5%) 48.2 (+21.8%) 56.1 (+41.6%) Figure 5: comparison of ReAct, ReflAct, and various verbalization strategies (state, goal, state+goal, and state+goal with next-action reasoning), using Llama-3.1-8B-Instruct as the agent model. Bars represent success rate; the red dashed line indicates average reward. (2) ReAct (Yao et al., 2023): The agent first reasons about the next action at each time step and then generates an action. (3) Plan-and-Act: In addition to NoThinking and ReAct, we designed another reasoning framework inspired by Plan-and-Solve (Wang et al., 2023) for comparison. Here, the agent is instructed at the first timestep (t = 1) to plan your approach to the task, and then output the action. It performs reasoning only at this step and outputs actions without further thoughts in subsequent timesteps. Results. Table 2 presents the performance of ReflAct compared with the NoThinking, ReAct, and Plan-and-Act baselines across ALFWorld, ScienceWorld, and Jericho. As shown in the table, the ReAct agent outperforms the NoThinking agent, highlighting the effectiveness of the Thinking before Action framework. Plan-and-Act, which begins with single planning step and then executes actions directly, performs comparably or slightly better than ReAct in stronger models like GPT4o. This suggests that when model has enough capacity, initial goal-based reasoning is effective, and flawed intermediate reasoning may offer little benefit. However, in weaker models like Llama, Plan-and-Act underperforms because these models struggle to retain and follow the initial plan. In contrast, ReflAct, which reflects on both the state and the goal at every timestep, consistently outperforms all the baseline reasoning frameworks regardless of backbone model or task. Notably, ReflAct achieved performance improvements of 77.9%, 25.9%, and 101.1% over NoThinking in ALFWorld, ScienceWorld, and Jericho, respectively, and improvements of 36.4%, 8.5%, and 38.1% over ReAct in the same environments. Importance of Reflection over Simply Stating. To verify the source of ReflActs superior performance gain, we conducted an ablation study. ReflAct reflects on the agents state in relation to the task goal, so we compared this with variants in which the agent simply verbalizes its state, its goal, or both, without engaging in reflection. As shown in Figure 5, we evaluated four settings: the agent verbalizes (1) the current state, (2) the task goal, (3) both the state and the goal, and (4) the state and goal while also reasoning about the next action. The results indicate that merely stating the state or goal underperforms compared to the ReAct baseline. Adding an explicit next-action thought in addition to verbalizing the state and goal yields performance gains in ALFWorld compared to ReAct, but still falls short of ReAct in ScienceWorld and Jericho. In contrast, ReflAct, which explicitly reflects on the relationship between the state and the goal, demonstrates substantially greater performance gain. These findings suggest that reflection, specifically reasoning about the state in the context of the goal, is more effective than simply verbalizing the state or the goal."
        },
        {
            "title": "6.3 ReflAct with Enhancement Modules",
            "content": "We examine layering enhancement modules on top of ReflAct. We consider Reflexion (Shinn et al., 2024), the use of post-task reflection in ReAct to improve planning, and WKM (Qiao et al., 2024), which incorporates parametric world model into ReAct. Discussion on RAFA (Liu et al., 2023), which focuses on long-sighted planning via nextstate prediction, and MPO (Xiong et al., 2025), which improves planning through online preference learning, can be found in Appendix F."
        },
        {
            "title": "6.3.1 ReflAct and Reflexion",
            "content": "Reflexion (Shinn et al., 2024) performs post-task reflection by analyzing failed trajectories and generating improvement plans for future trials. Since post-task reflection operates independently of the underlying reasoning process, we analyze its impact when combined with three reasoning frameworks: NoThinking, ReAct, and ReflAct. Figure 6 shows the results of applying Reflexion to each backbone in ALFWorld and Jericho. We followed Shinn et al. (2024) in applying posttask reflection after each trial, repeating this for three trials. The results show that post-task reflection generally improves performance across all base agents, except Llama NoThinking and ReAct in Jericho. However, even after applying Reflexion, the NoThinking and ReAct agents still fall Figure 6: Success rate change with Reflexion post-task reflection (NoThinking, ReAct, ReflAct). short of ReflActs trial 0 performance. This highlights that when the initial reasoning backbone is weak, even repeated feedback across trials provides only limited benefit. Although post-task reflection aids reasoning by analyzing failures, enhancing intask reflection through stronger backbone proves more fundamental. Notably, the GPT-4o ReflAct agent with Reflexion achieved an unprecedented 94.8% success rate in ALFWorld, bringing household robots one step closer to reality, given sufficient hardware support."
        },
        {
            "title": "6.3.2 ReflAct and WKM",
            "content": "WKM (Qiao et al., 2024) improves LLM agents decision-making in interactive environments by using Task Knowledge for global planning and State Knowledge for local context. Its parametric knowledge model, fine-tuned on knowledgeaugmented trajectories, enables context-aware decision-making by referencing past similar situations. WKM learns and utilizes an external knowledge model in contrast to ReflAct, which intends to perform internal state modeling within the reasoning backbone. We applied WKM to ReAct and further examined scenario where state knowledge is placed immediately before action, replacing the thought, to guide the action. This allows us to assess the impact of explicit state knowledge compared to ReflActs internal state modeling. Figure 7 shows performance related to WKM. Applying WKM to ReAct results in performance improvement, but it still falls short of ReflAct. Replacing the thought step with state knowledge to directly guide the action actually degrades performance, highlighting the side effect of incorrect guidance from the ex7 Figure 7: Performance: ReflAct, NoThinking, ReAct, and ReAct+WKM with thought replaced by WKM state knowledge (Llama-3.1-8B-Instruct as agent) ternal model. This suggests that encouraging the model to perform internal state modeling on its own can be more effective than relying on state knowledge generated by an external model."
        },
        {
            "title": "6.4 Further Discussion on ReflAct’s Ability",
            "content": "In this section, we analyze how ReflAct successfully solves tasks, and in Appendix G, we compare the hallucinated action ratio and token length between ReflAct and baseline reasoning frameworks."
        },
        {
            "title": "Reasoning Frameworks",
            "content": "Building on the discussion of Figure 3 in Section 3.2, we analyze the proportion of tasks failed by the NoThinking, ReAct, and ReflAct agents across 134 tasks in the ALFWorld domain. Figure 8: Failure ratio comparison diagram of NoThinking, ReAct, and ReflAct on 134 ALFWorld tasks, using GPT-4o and Llama-3.1-8B-Instruct as agent models. and more reliable decision-making in LLM agents."
        },
        {
            "title": "6.4.2 ReflAct Self-Corrects Automatically\nLLM agents, especially when operating in unfa-\nmiliar environments with only partially observable\ninformation, are prone to making mistakes. There-\nfore, it is essential that they possess the ability to\nself-correct by identifying the cause of their errors\nand adjusting their behavior accordingly to return\nto a correct course of action. The example below\nby a ReflAct agent demonstrates that ReflAct main-\ntains an internal belief over the current state and\ncontinuously aligns its decision-making with the\ntask goal, enabling the agent to recognize mistakes\nand flexibly recover from them, ultimately guiding\nitself back onto the correct path.",
            "content": "Your task is cool some lettuce and put it in countertop. ... Action: Obs: countertop 2. take lettuce 1 from countertop 2 You pick up the lettuce 1 from the am at countertop 2, holding Reflection: lettuce 1. need to cool the lettuce before putting it on the countertop to complete the task. Action: Obs: cool lettuce 1 with fridge 1 Nothing happens. am at countertop 2, holding The action to cool the lettuce Reflection: lettuce 1. with the fridge did not work because need to be at the fridge to perform that action. Action: Obs: go to fridge 1 The fridge 1 is open. In it, ... am at fridge 1, holding can cool the lettuce now that cool lettuce 1 with fridge 1 You cool the lettuce 1 using the Reflection: lettuce 1. Im at the fridge. Action: Obs: fridge 1. ..."
        },
        {
            "title": "7 Conclusion",
            "content": "Figure 8 illustrates the failure distribution among the three agents. Consistent with earlier observations, the NoThinking agent accounts for the largest number of unique failures, especially in tasks where it alone fails. The ReAct agent, while incorporating reasoning, introduces additional failure cases not observed in NoThinking. This suggests that ungrounded or incorrect reasoning can lead to novel and potentially harmful errors. Notably, there are no tasks in which only the ReflAct agent fails. ReflAct fails only in cases where other agents also fail, and does not introduce new failure modes. This highlights the benefit of reflection grounded in the agents state and task goal, suggesting that reflective reasoning contributes to safer We have proposed ReflAct, reflection-based reasoning framework that addresses key limitations of ReAct. While ReAct focuses on next-action thinking, it often fails to maintain coherent internal beliefs about the environment and goal, leading to poor decisions in complex, partially observable tasks. In contrast, ReflAct emphasizes ongoing reflection on state and goal, enabling more grounded and reliable decision making. Our experiments show that ReflAct consistently outperforms ReAct and other baselines, demonstrating the value of reflection in long-horizon reasoning. We hope this work inspires further research into more robust and informative reasoning guidance for intelligent agents."
        },
        {
            "title": "References",
            "content": "While ReflAct shows strong performance in interactive agent environments, its applicability to other domains such as mathematics and coding remains unexplored. These tasks pose unique challenges: mathematics requires step-by-step deduction, while programming demands consistency in logic, state, and syntax. Extending ReflAct to these areas may require adapting its reflection mechanism to better align the current state with taskIncorporating structured reasonspecific goals. ing that explicitly supports goal-state alignment could enhance performance, and we consider this promising direction for future work. In addition, as discussed in Appendix G, ReflAct results in modest increase in token length compared to ReAct. To address this, it is worth exploring more efficient formulations that retain the benefits of state-goal reflection while reducing verbosity. Possible approaches include compressing reflection outputs, using more concise representations of reasoning steps, or integrating summarization modules to maintain informativeness within smaller token budget. Finally, there is growing interest in enhancing reasoning with external modules. Incorporating such components into ReflAct could further improve its reflection process. For example, an external verifier might evaluate reflected goals or suggest fixes for inconsistencies. Exploring such hybrid architectures remains promising direction for future research."
        },
        {
            "title": "Ethical Statements",
            "content": "While LLM Agents represent powerful tools with significant practical applications, they also pose substantial risks if misused or deployed without proper safeguards. Of particular concern is their tendency to hallucinate or generate ungrounded reasoning, which can lead to unreliable or potentially harmful decisions in real-world contexts. This research aims to address these challenges by developing more grounded decision-making frameworks that enhance reliability and reduce hallucination. By focusing on improving the alignment between the agents internal beliefs and the actual environment state, our work contributes to the broader goal of creating more trustworthy AI systems that can safely operate in complex, partially observable environments while maintaining consistent goaldirected behavior. Ma Chang, Junlei Zhang, Zhihao Zhu, Cheng Yang, Yujiu Yang, Yaohui Jin, Zhenzhong Lan, Lingpeng Kong, and Junxian He. 2024. Agentboard: An analytical evaluation board of multi-turn llm agents. Advances in Neural Information Processing Systems, 37:7432574362. Yuri Chervonyi, Trieu Trinh, Miroslav Olšák, Xiaomeng Yang, Hoang Nguyen, Marcelo Menegali, Junehyuk Jung, Vikas Verma, Quoc Le, and Thang Luong. 2025. Gold-medalist performance in solving olympiad geometry with alphageometry2. arXiv preprint arXiv:2502.03544. Ali Forootani. 2025. survey on mathematical reasoning and optimization with large language models. arXiv preprint arXiv:2503.17726. Aaron Grattafiori, Abhimanyu Dubey, Abhinav Jauhri, Abhinav Pandey, Abhishek Kadian, Ahmad AlDahle, Aiesha Letman, Akhil Mathur, Alan Schelten, Alex Vaughan, and 1 others. 2024. The llama 3 herd of models. arXiv preprint arXiv:2407.21783. David Ha and Jürgen Schmidhuber. 2018. World models. arXiv preprint arXiv:1803.10122. Danijar Hafner, Timothy Lillicrap, Jimmy Ba, and Mohammad Norouzi. 2019. Dream to control: Learning behaviors by latent imagination. arXiv preprint arXiv:1912.01603. Danijar Hafner, Jurgis Pasukonis, Jimmy Ba, and Timothy Lillicrap. 2025. Mastering diverse control tasks through world models. Nature, pages 17. Shibo Hao, Yi Gu, Haodi Ma, Joshua Jiahua Hong, Zhen Wang, Daisy Zhe Wang, and Zhiting Hu. 2023. Reasoning with language model is planning with world model. arXiv preprint arXiv:2305.14992. Matthew Hausknecht, Prithviraj Ammanabrolu, MarcAlexandre Côté, and Xingdi Yuan. 2020. Interactive fiction games: colossal adventure. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 34, pages 79037910. Matthew Hausknecht and Peter Stone. 2015. Deep recurrent q-learning for partially observable mdps. In AAAI fall symposia, volume 45, page 141. Leslie Pack Kaelbling, Michael Littman, and Anthony Cassandra. 1998. Planning and acting in partially observable stochastic domains. Artificial intelligence, 101(1-2):99134. Jacky Liang, Wenlong Huang, Fei Xia, Peng Xu, Karol Hausman, Brian Ichter, Pete Florence, and Andy Zeng. 2023. Code as policies: Language model programs for embodied control. In 2023 IEEE International Conference on Robotics and Automation (ICRA), pages 94939500. IEEE. 9 Zhihan Liu, Hao Hu, Shenao Zhang, Hongyi Guo, Shuqi Ke, Boyi Liu, and Zhaoran Wang. 2023. Reason for future, act for now: principled framework for autonomous llm agents with provable sample efficiency. arXiv preprint arXiv:2309.17382. Wenjie Ma, Jingxuan He, Charlie Snell, Tyler Griggs, Sewon Min, and Matei Zaharia. 2025. Reasoning models can be effective without thinking. arXiv preprint arXiv:2504.09858. Junhyuk Oh, Valliappa Chockalingam, Honglak Lee, and 1 others. 2016. Control of memory, active perception, and action in minecraft. In International conference on machine learning, pages 27902799. PMLR. OpenAI. 2024. Hello gpt-4o. Martin Puterman. 1990. Markov decision processes. Handbooks in operations research and management science, 2:331434. Shuofei Qiao, Runnan Fang, Ningyu Zhang, Yuqi Zhu, Xiang Chen, Shumin Deng, Yong Jiang, Pengjun Xie, Fei Huang, and Huajun Chen. 2024. Agent planning with world knowledge model. In The Thirty-eighth Annual Conference on Neural Information Processing Systems. Rafael Rafailov, Archit Sharma, Eric Mitchell, Christopher Manning, Stefano Ermon, and Chelsea Finn. 2024. Direct preference optimization: Your language model is secretly reward model. Advances in Neural Information Processing Systems, 36. Andres Rodriguez, Ronald Parr, and Daphne Koller. 1999. Reinforcement learning using approximate belief states. Advances in Neural Information Processing Systems, 12. Julian Schrittwieser, Ioannis Antonoglou, Thomas Hubert, Karen Simonyan, Laurent Sifre, Simon Schmitt, Arthur Guez, Edward Lockhart, Demis Hassabis, Thore Graepel, and 1 others. 2020. Mastering atari, go, chess and shogi by planning with learned model. Nature, 588(7839):604609. Noah Shinn, Federico Cassano, Ashwin Gopinath, Karthik Narasimhan, and Shunyu Yao. 2024. Reflexion: Language agents with verbal reinforcement learning. Advances in Neural Information Processing Systems, 36. Keisuke Shirai, Cristian Beltran-Hernandez, Masashi Hamaya, Atsushi Hashimoto, Shohei Tanaka, Kento Kawaharazuka, Kazutoshi Tanaka, Yoshitaka Ushiku, and Shinsuke Mori. 2024. Vision-language interpreter for robot task planning. In 2024 IEEE International Conference on Robotics and Automation (ICRA), pages 20512058. IEEE. Mohit Shridhar, Xingdi Yuan, Marc-Alexandre Cote, Yonatan Bisk, Adam Trischler, and Matthew Hausknecht. 2021. {ALFW}orld: Aligning text and embodied environments for interactive learning. In International Conference on Learning Representations. Yifan Song, Da Yin, Xiang Yue, Jie Huang, Sujian Li, and Bill Yuchen Lin. 2024. Trial and error: Exploration-based trajectory optimization of llm agents. In Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 75847600. Mirac Suzgun, Mert Yuksekgonul, Federico Bianchi, Dan Jurafsky, and James Zou. 2025. Dynamic cheatsheet: Test-time learning with adaptive memory. arXiv preprint arXiv:2504.07952. Mudit Verma, Siddhant Bhambri, and Subbarao Kambhampati. 2024. On the brittle foundations of react prompting for agentic large language models. arXiv preprint arXiv:2405.13966. Lei Wang, Wanyu Xu, Yihuai Lan, Zhiqiang Hu, Yunshi Lan, Roy Ka-Wei Lee, and Ee-Peng Lim. 2023. Planand-solve prompting: Improving zero-shot chainof-thought reasoning by large language models. In Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 26092634. Ruoyao Wang, Peter Jansen, Marc-Alexandre Côté, and Prithviraj Ammanabrolu. 2022. Scienceworld: Is your agent smarter than 5th grader? In Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing, pages 1127911298. Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Fei Xia, Ed Chi, Quoc Le, Denny Zhou, and 1 others. 2022. Chain-of-thought prompting elicits reasoning in large language models. Advances in neural information processing systems, 35:24824 24837. Yiran Wu, Tianwei Yue, Shaokun Zhang, Chi Wang, and Qingyun Wu. 2024. Stateflow: Enhancing llm task-solving through state-driven workflows. In First Conference on Language Modeling. Weimin Xiong, Yifan Song, Qingxiu Dong, Bingchan Zhao, Feifan Song, Xun Wang, and Sujian Li. 2025. Mpo: Boosting llm agents with meta plan optimization. arXiv preprint arXiv:2503.02682. Shunyu Yao, Jeffrey Zhao, Dian Yu, Nan Du, Izhak Shafran, Karthik Narasimhan, and Yuan Cao. 2023. React: Synergizing reasoning and acting in language models. In The Eleventh International Conference on Learning Representations. Wangchunshu Zhou, Yuchen Eleanor Jiang, Long Li, Jialong Wu, Tiannan Wang, Shi Qiu, Jintian Zhang, Jing Chen, Ruipu Wu, Shuai Wang, and 1 others. 2023. Agents: An open-source framework for autonomous language agents. arXiv preprint arXiv:2309.07870."
        },
        {
            "title": "A Expanded Related Works",
            "content": "Belief-State Modeling in RL. Reinforcement Learning (RL) has aimed to endow agents with an internal state representation that captures all relevant information from past observations (Rodriguez et al., 1999). In partially observable environments, this often takes the form of belief state, probability distribution over possible world states, which is updated as new observations are received. Kaelbling et al. (1998) describe planning as tracking sufficient statistic of the history (i.e., the belief), which serves as the agents core state for action selection. This approach has been further developed in deep RL, where agents learn to track belief states using recurrent neural networks (Hausknecht and Stone, 2015), memoryaugmented architectures (Oh et al., 2016), and world models (Ha and Schmidhuber, 2018). Various studies (Hafner et al., 2019; Schrittwieser et al., 2020; Hafner et al., 2025) have demonstrated the effectiveness of belief state modeling, enabling more effective planning and reliable decision-making. State Representations in LLMs. Similar to belief-state modeling in RL, recent work on LLMs has explored how internal or external state representations can support reasoning and planning. One line of research adopts an extrinsic modeling approach, inspired by the Code-as-Policies paradigm (Liang et al., 2023), in which observations are structured into symbolic formats such as textual summaries, PDDL, or finite state machines, and explicitly injected into the prompt. These designs enhance grounding and interpretability in robotic tasks, but they typically rely on predefined code or external modules to maintain and update the state representation (Hao et al., 2023; Shirai et al., 2024; Wu et al., 2024). While prior work investigates whether LLMs internally form task-relevant abstractions, ReflAct advances this line of research by explicitly prompting models to articulate and reflect on such abstractions. By encouraging LLMs to generate and reason over goal-conditioned state representations, ReflAct facilitates more grounded and efficient decision-making during interaction. Backbone Reasoning Frameworks. The emergence of LLMs has introduced new agent design paradigms, especially via prompting-based reasoning strategies. CoT prompting (Wei et al., 2022) showed that models can handle complex tasks by outlining intermediate steps. ReAct (Yao et al., 2023) extended this to interactive settings by interleaving reasoning and action, while Plan-andSolve (Wang et al., 2023) added high-level planning before stepwise execution. However, recent studies (Ma et al., 2025) argue that these CoTbased approaches are often token-inefficient and prone to hallucinations. We attribute these reliability issues to the absence of an explicit belief state. ReflAct addresses this by maintaining and reflecting belief states to guide reasoning and improve both efficiency and reliability. Enhancement Reasoning Modules. Recently, growing body of research has explored add-on or post-processing modules that operate on top of reasoning backbones to enhance reasoning capabilities. Reflexion (Shinn et al., 2024) improves performance by analyzing failed trajectories and storing insights for future use. WKM (Qiao et al., 2024) mitigates hallucinations by grounding decisions in learned world model that reflects realistic dynamics. RAFA (Liu et al., 2023) strengthens the reasoning-acting link by imagining future trajectories and evaluating them with separate Model and Critic components, forming structured loop of planning, action, and feedback. Unlike these approaches, ReflAct is not layered on top of reasoning backbone but instead redefines the backbone itself, enabling more efficient reasoning."
        },
        {
            "title": "Ungrounded Thought",
            "content": "In addition to the discussion of the ReAct agents failure case in Figure 4 in Section 3.2, we examine another failure case in ScienceWorld (Wang et al., 2022). As shown in Figure 9 (1), the agent is tasked with boiling lead. To do this, the agent must find lead, transfer it to suitable heat source, and then boil it. During the previous steps, the agent successfully located the lead in the workshop. Now, the agent must carry it to the foundry. However, the agent mistakenly assumes that it is already holding the lead and proceeds with the next action based on this false assumption, ultimately resulting in hallucination. In the same situation, similar to the discussion in Section 3.2, we replace the thought with reflection sentence about the current state and task goal, as seen in the right branch of Figure 9. This allows the agent to clearly recognize the current situation, make goal-aware long horizon decisions, and ultimately succeed in the task. 11 Figure 9: Another failure case of ReAct, following Figure 4, is seen in the ScienceWorld task boil lead. The agent finds the lead in the workshop. (1) It mistakenly believes it is holding the lead, proceeds with the task, and fails. (2) After reflecting on its current state in relation to the task goal, the agent correctly recognizes its situation, takes goal-aware actions, and successfully completes the task."
        },
        {
            "title": "C Benchmark Details",
            "content": "ALFWorld (Shridhar et al., 2021): ALFWorld is text-based benchmark derived from the ALFRED dataset, designed to evaluate an agents ability to interpret and execute natural language instructions in interactive, multi-step household tasks. It converts embodied vision-and-language tasks into textual form, allowing for assessment of high-level planning, language grounding, and sequential decision making in simulated environment. ScienceWorld (Wang et al., 2022): ScienceWorld is text-based benchmark for evaluating agents on scientific reasoning and procedural task completion. Grounded in standardized K12 science curricula, it tests an agents ability to follow instructions, perform virtual experiments, make causal inferences, and manipulate objects across domains such as physics, chemistry, and biology. Tasks require multi-step interaction, hypothesis testing, and language grounding in dynamic environment. Jericho (Hausknecht et al., 2020): Jericho is suite of text-based adventure games, each with distinct environments and roles. Tasks require completing sequence of implicit subgoals, often in specific order. Since these subgoals often do not follow intuitive causal logic, agents must rely on interaction to model the environment and accurately infer their current state. In ALFWorld, success is measured using binary signal: agents receive reward of 1 only when the task is fully completed, and 0 otherwise. In contrast, the official environments of ScienceWorld and Jericho provide both success flag and progress reward, which assigns value between 0 and 1 based on the degree of task completion. However, we discovered bug in both ScienceWorld and Jericho where the success flag could be incorrectly set even when the progress reward was low or incomplete. For example, in Jericho, task was sometimes marked as successful even when the agent died during execution. To address this, we adjusted the success criteria: task was considered successful only if the progress reward was at least 0.7 in ScienceWorld, or exactly 1.0 in Jericho. In our experiments, the setups for ALFWorld and ScienceWorld were based on the official implementation by MPO (Xiong et al., 2025) (https: //github.com/WeiminXiong/MPO), and we used the same test set for evaluation. The ALFWorld test set consists of 134 tasks across six different household task types, including pick-and-place, pick-heat-then-place, and pick-two-and-place. The ScienceWorld test set includes 211 tasks across 24 different scientific experiment types, including Boil, Test Conductivity of Unknown Substances, and Grow Fruit. The experiments for Jericho were based on the implementation by AgentBoard (Chang et al., 2024) (https://github.com/hkust-nlp/ AgentBoard). The Jericho test set consists of 20 tasks, each corresponding to different game, including 905, Acorncourt, Afflicted, Balances, Dragon, Jewel, Library, Omniquest, Reverb, Snacktime, Zenon, Zork1, Zork2, Zork3, Detective, Night, Pentari, Weapon, Darkhunt, and Loose."
        },
        {
            "title": "D Baseline Details",
            "content": "D.1 Baselines for Backbone Reasoning"
        },
        {
            "title": "Frameworks",
            "content": "We use the same environment code base and test set for all reasoning frameworks, including ReflAct. For specific environments, we follow existing implementations: ALFWorld and ScienceWorld are based on the implementation by MPO (Xiong et al., 2025), and Jericho is based on the implementation by AgentBoard (Chang et al., 2024), as described in detail in the section above. We implement baseline reasoning frameworks using the following instructions: Instruction for NoThinking environments base code: ALFWorld and ScienceWorld from Xiong et al. (2025), and Jericho from Chang et al. (2024). In addition, for WKM, we used the publicly available dataset on Hugging Face (https://huggingface.co/collections/ zjunlp/wkm-6684c611102213b6d8104f84) to reproduce the original setup and conducted experiments using WKMs world model. Reflexion (Shinn et al., 2024): https:// github.com/noahshinn/reflexion WKM (Qiao et al., 2024): https://github. com/zjunlp/WKM RAFA (Liu et al., 2023): https://github. com/agentification/RAFA_code MPO (Xiong et al., 2025): https://github. You should directly output the action in this turn. com/WeiminXiong/MPO Instruction for ReAct"
        },
        {
            "title": "E Resources",
            "content": "You should first think about the current condition and plan for your future actions, and then output your action in this turn. Note that this ReAct instruction is from Song et al. (2024); Xiong et al. (2025). Instruction for Plan-and-Act You should first think about the given task and plan your approach to the task, and then output the action for this turn. The one-shot example for ReAct is based on Yao et al. (2023); Xiong et al. (2025) and can be found in Appendix H. For NoThinking, we remove the thought step from all timesteps in the same example. For Plan-and-Act, we retain only the thought from the first timestep (t = 1). We will release the public implementation repository after publication. D.2 Baselines for Enhancement Reasoning"
        },
        {
            "title": "Modules",
            "content": "In our experiments, we discuss four enhanced reasoning modules: Reflexion (Shinn et al., 2024) and WKM (Qiao et al., 2024) in Section 6.3, and RAFA (Liu et al., 2023), MPO (Xiong et al., 2025), and ReflAct in Appendix F. We implemented each module based on the official repositories provided by the original papers. For Reflexion, we used the official implementation on top of each For inference with the Llama3.1-Instruct model, we used one A6000 48GB GPU for the 8B model and two A100 80GB GPUs for the 70B model. For training the WKM world model, we used four A6000 48GB GPUs."
        },
        {
            "title": "Modules",
            "content": "F.1 ReflAct and RAFA RAFA (Liu et al., 2023) introduces next-state prediction framework where language model recursively expands tree of possible future actionobservation trajectories, selecting actions based on the highest estimated value. While this foresight aids long-term reasoning, ReflAct offers practical advantages in efficiency and grounded decisionmaking by internally modeling the current state. Figure 10: Comparison between RAFA and ReflAct. 13 As shown in Figure 10, RAFA constructs tree at each time step t, requiring 13 queries to the language model per step with depth = 2 and branching factor = 2: 3 for action sampling, 6 for observation prediction, and 4 for value estimation. This overhead grows rapidly with depth due to exponential node expansion. In contrast, ReflAct needs only single query per step to jointly produce reflection and action, offering constant cost and greater efficiency when latency or throughput is constrained. Figure 10 also highlights behavioral difference between the two methods. Given the task \"examine the alarmclock with the desklamp\" in ALFWorld, both agents observe an alarmclock on desk 2. ReflAct grounds its reflection in this state and selects the correct action, while RAFA relies on predicted trajectories that miss the action and lead to an unrelated exploratory move. This shows that ReflAct aligns decisions more closely with the actual state, achieving more reliable behavior with significantly lower cost via single LLM call per step. F.2 ReflAct and MPO Recently, Xiong et al. (2025) proposed framework that enhances LLM agents by generating and refining high-level meta plans based on agent feedback. The meta plans are initially learned through supervised fine-tuning on expert-generated examples and subsequently optimized via preferencebased learning (Rafailov et al., 2024) using feedback from agent executions. The resulting meta planner can be applied to various agents in plug-and-play manner without requiring additional retraining. Accordingly, we applied the publicly available ALFWorld MPO meta plans from https://github.com/WeiminXiong/MPO to NoThinking, ReAct, and ReflAct, and compared the resulting performance gains. Figure 11: Performance comparison across NoThinking, ReAct, and ReflAct when applying MPO meta-plan. As shown in Figure 11, applying MPO meta plans to NoThinking and ReAct leads to substantial performance improvements. However, notably, even with these gains, NoThinking+MPO 14 and ReAct+MPO still fall short of ReflActs performance. This further reinforces previous findings that strengthening the reasoning backbone itself is more crucial than simply adding enhancement modules to ReAct. Applying MPO meta plans to ReflAct results in slight performance gains for GPT-4o, but minor performance drop for Llama-3.1-8B-Instruct. This may be due to the fact that the meta plans were trained on tasks different from those in the test set; in some out-of-distribution cases, they may offer misleading guidance that negatively impacts the agents behavior. The limited effectiveness of meta plans on ReflAct also suggests that its grounded decision-making already enables highquality planning. More Discussion on ReflActs Effects G.1 ReflAct Prevents Suboptimality Through Goal-Oriented Reasoning Figure 12: Comparison of ReActs Thought and ReflActs Reflection on Jericho task: ReAct fixates on found shovel, losing sight of the main goal and failing; ReflAct succeeds through goal-oriented reasoning. ReflAct enables long-sighted planning by continuously reflecting on the current state and goal. Figure 12 illustrates this with an example in task where the agent must gather swimming-related items and ultimately reach cylindrical room. As seen, ReAct fails due to short-sighted reasoning, whereas ReflAct successfully achieves the goal through consistent reflection on the state and objective. G.2 Token Length vs. Hallucination Action"
        },
        {
            "title": "Rate in ReAct and ReflAct",
            "content": "Here, we further compare ReflAct and ReAct in terms of token length and the hallucinated action rate. Specifically, token length refers to the number of tokens in each thought (or reflection) with action generated by the model, normalized by the number of steps to account for variations in agent trajectory length. The hallucinated action rate is defined as the proportion of actions that result in erroneous observations (e.g., \"Nothing happens\"). Both metrics were averaged across the four agent models used in our experiments: GPT-4o, GPT-4o-mini, Llama-3.1-8B/70B-Instruct. Figure 13: Comparison of the average token length and average hallucination action rate between ReAct and ReflAct in ALFWorld, ScienceWorld, and Jericho. Figure 13 shows that ReflAct produces slightly longer token sequences than ReAct, as it incorporates reflection on the current state and goal into its reasoning process. Interestingly, despite the increased token length, the hallucination rate decreases. This suggests that hallucinations in LLM agents may not arise simply from increased reasoning (i.e., overthinking), but rather from unstructured or unguided reasoning processes."
        },
        {
            "title": "H Prompt",
            "content": "H.1 ALFWorld 1) ReAct You are in the middle of room. cabinet 4, cabinet 3, cabinet 2, cabinet 1, countertop 1, garbagecan 1, handtowelholder 2, handtowelholder 1, sinkbasin 2, sinkbasin 1, toilet 1, toiletpaperhanger 1, and towelholder 1. Your task is to: Looking quickly around you, you see put some spraybottle on toilet. go to cabinet go to cabinet 2 The cabinet 2 is closed. First need to find spraybottle. can check one by one, starting by going to cabinet 1. On the cabinet 1, you see cloth 1, soapbar 1, soapbottle To solve the task, need to find and take sparybottle, then spraybottle is Thought: put it on toilet. more likely to appear in cabinet 1-4, countertop 1, toilet 1, sinkbasin 1-2, garbagecan 1. Action: Observation: 1. Action: Observation: Action: Observation: candle 1, and spraybottle 2. Thought: Action: Observation: Thought: Action: Observation: Action: You put the spraybottle 2 in/on the toilet 1. Now find spraybottle 2. Next, need to take it. You pick up the spraybottle 2 from the cabinet 2. You open the cabinet 2. The cabinet 2 is open. Now take spraybottle 2. Next, need to put it in/on toilet 1. On the toilet 1, you see soapbottle 2. take spraybottle 2 from cabinet 2 put spraybottle 2 in/on toilet 1 open cabinet 2 go to toilet 1 In it, you see Figure 14: Prompt for ALFWorld ReAct 16 2) ReflAct You are in the middle of room. cabinet 4, cabinet 3, cabinet 2, cabinet 1, countertop 1, garbagecan 1, handtowelholder 2, handtowelholder 1, sinkbasin 2, sinkbasin 1, toilet 1, toiletpaperhanger 1, and towelholder 1. Your task is to: Looking quickly around you, you see put some spraybottle on toilet. go to cabinet go to cabinet 1 spraybottle is The cabinet 2 is closed. First need to find spraybottle. can check one by one, starting by going to cabinet 1. Currently, am at cabinet 1, not holding anything, and am Currently, am at cabinet 2, not holding anything, but the On the cabinet 1, you see cloth 1, soapbar 1, soapbottle To solve the task, need to find and take sparybottle, then Reflection: put it on toilet. more likely to appear in cabinet 1-4, countertop 1, toilet 1, sinkbasin 1-2, garbagecan 1. Action: Observation: 1. Reflection: searching for spraybottle to place on the toilet, but instead find cloth, soapbar, and soapbottle. Action: Observation: Reflection: closed cabinet hinders me from finding the spraybottle needed to place on the toilet. Action: Observation: candle 1, and spraybottle 2. Reflection: which brings me closer to completing the task of placing it on the toilet. Action: Observation: Reflection: and am ready to complete the task of placing the spraybottle on the toilet, as have obtained spraybottle 2. Action: go to toilet 1 Observation: Reflection: task is nearly complete, with only the placement remaining. Action: Observation: Currently, am at cabinet 2 and have found spraybottle 2, You pick up the spraybottle 2 from the cabinet 2. You open the cabinet 2. The cabinet 2 is open. You put the spraybottle 2 in/on the toilet 1. On the toilet 1, you see soapbottle 2. take spraybottle 2 from cabinet 2 put spraybottle 2 in/on toilet 1 Currently, am at toilet 1, still holding spraybottle 2, and the Currently am located at cabinet 2, now holding spraybottle 2, open cabinet 2 In it, you see Figure 15: Prompt for ALFWorld ReflAct 17 H.2 ScienceWorld 1) ReAct nothing. look around look around In it, you see: should go to a white pillow.. teleport to art studio When you are done, focus On the table is: nothing. You teleport to the art studio. mattress. On the mattress is: This room is called the bedroom. This room is called the art studio. The closet door is open. In the closet is: The bedroom doesnt have the materials need. Now that Im in the art studio, should look around to find the The materials for making green paint are likely to be found in an To solve the task, need to find materials to create green paint, Your task is to use chemistry to create green paint. on the green paint. Thought: use those materials, and focus on the object specified in the task once it is identified. art studio. Action: Observation: the agent substance called air bed. On the bed is: closet. picture table. You also see: door to the hallway (that is open) Thought: place where paint and painting supplies are likely to be found, such as an art studio. Action: Observation: Thought: necessary materials to create green paint. Action: Observation: the agent substance called air large cupboard. drawer, wood cup (containing yellow paint), wood cup (containing blue paint), wood cup (containing red paint). table. wood cup (containing yellow paint) wood cup (containing blue paint) wood cup (containing red paint) You also see: door to the hallway (that is open) Thought: mix green paint. with another color. Action: nothing Observation: Thought: create green. Action: paint in table Observation: Thought: need to mix them together to create green paint. Action: Observation: Thought: the green paint is made, should focus on the final product. Action: should add the yellow paint to the container with blue paint. pour cup containing yellow paint in art studio in cup containing blue see blue paint here, which is one of the primary colors needed to should start by pouring it into an empty container to mix You pour the contents of the wood cup into the glass cup. Yellow paint is the other primary color needed to mix with blue to Now that have both blue and yellow paint in the same container, pour cup containing blue paint in art studio in cup containing You pour the contents of the wood cup into the glass cup. The task is to create green paint and then focus on it. On the table is: glass cup (containing nothing). Blue and yellow paint mix to produce green paint. mix cup containing blue paint and yellow paint The large cupboard door is open. In the large cupboard is: focus on green paint In it, you see: Now that Figure 16: Prompt for ScienceWorld ReAct 18 2) ReflAct nothing. look around look around When you are done, focus Currently, am in the art studio, holding nothing and focused on In it, you see: white pillow.. teleport to art studio On the table is: nothing. You teleport to the art studio. mattress. On the mattress is: This room is called the bedroom. This room is called the art studio. The closet door is open. In the closet is: need to look for materials to create green paint. To solve the task, need to find materials to create green Currently, am in the bedroom, holding nothing and focused on There are no useful materials here to create the green paint need Your task is to use chemistry to create green paint. on the green paint. Reflection: paint, use those materials, and focus on the object specified in the task once it is identified. The materials for making green paint are likely to be found in an art studio. Action: Observation: the agent substance called air bed. On the bed is: closet. picture table. You also see: door to the hallway (that is open) Reflection: nothing. to focus on. Action: Observation: Reflection: nothing. Action: Observation: the agent substance called air large cupboard. drawer, wood cup (containing yellow paint), wood cup (containing blue paint), wood cup (containing red paint). table. wood cup (containing yellow paint) wood cup (containing blue paint) wood cup (containing red paint) You also see: door to the hallway (that is open) Reflection: found the primary colors yellow, blue, and red, which can be used to make green paint, the substance will need to focus on. Action: nothing Observation: Reflection: will next add yellow paint. Action: paint in table Observation: Reflection: holding mix of blue and yellow paint, ready to be mixed into green. Action: Observation: Reflection: the green paint is made, should focus on the final product. Action: pour cup containing blue paint in art studio in cup containing You pour the contents of the wood cup into the glass cup. You pour the contents of the wood cup into the glass cup. Currently, am in the art studio, not holding anything. The task is to create green paint and then focus on it. Ive added yellow paint to the blue in the glass cup. On the table is: glass cup (containing nothing). Blue and yellow paint mix to produce green paint. mix cup containing blue paint and yellow paint poured blue paint into the glass cup. The large cupboard door is open. In the large cupboard is: Im now holding it and focus on green paint In it, you see: pour cup containing yellow paint in art studio in cup containing blue Now that Im now Ive Figure 17: Prompt for ScienceWorld ReflAct 19 H."
        },
        {
            "title": "Jericho",
            "content": "1) ReAct north Main hall Examine barrel take the sword The castle is to The sword is taken There is gleaming elvish sword. Taking this sword now will likely There is barrel in front of you. You are at the path leading to the castle. Once prepared, will move north into the castle, navigate You are the warrior Link that needs to save the princess from the Im now armed. Advancing north into the castle should provide new To solve the task, first need to explore my surroundings to gather The sword inside the barrel has gleaming finish, which suggests it Goal: castle. Observation: your north. Thought: any useful items. to the princesss location, interact with the princess and complete the rescue mission. Action: Observation: Thought: is of high quality and possibly enchanted. increase my chances of handling any immediate challenges. Action: Observation: Thought: opportunities for exploration and reveal the next critical steps. Action: Observation: The castle is dimly lit, with lava flowing in many places. staircase leading up to princess bedroom, door to your north leading to the kitchen hall, and the door you just entered on your south Thought: evaluate what options are actually available to me right now. helpful to check the possible actions can take from here. Action: Observation: Thought: most direct route to explore. perilous, Ill proceed to investigate whats at the top of the stairs. Action: Observation: Thought: There are likely specific actions can perform in this situation, and should understand my options before taking step that could have consequences. move. Observation: Thought: change the course of events. to interact with her. Action: Observation: says. Among the available actions, ascending the staircase seems like the Since it leads upward and the hall below seems One of the available actions is to wake the princess, which might Ive entered bedroom where the princess is lying unconscious. Before making move toward any specific direction, need to Checking what can do here would be the most logical next It seems like the most immediate and gentle way wake up the princess, take out sword, down south, north, up, look, examine staircase The princess wake up from the coma. Ill go ahead and try that first. Princess is sleeping in bed. Thank you my knight, she The task is finished. wake up the princess check valid actions check valid actions Princesss bedroom. It would be There is Action: up Figure 18: Prompt for Jericho ReAct 20 2) ReflAct north With my Main hall take the sword Examine barrel The castle is to Since currently The sword is taken There is gleaming elvish sword. You are at the path leading to the castle. found powerful elvish sword in the barrel. have obtained the elvish sword and am now equipped. You are the warrior Link that needs to save the princess from the There is barrel in front of you. To solve the task, first need to explore my surroundings to Goal: castle. Observation: your north. Reflection: gather any useful items. Once prepared, will move north into the castle, navigate to the princesss location, interact with the princess and complete the rescue mission. Action: Observation: Reflection: have no weapons, this sword will be essential for protection once enter the castle. Action: Observation: Reflection: weapon ready, should head into the castle to continue the search for the princess. Action: Observation: The castle is dimly lit, with lava flowing in many places. staircase leading up to princess bedroom, door to your north leading to the kitchen hall, and the door you just entered on your south Reflection: dangerous with lava present. bedroom, which aligns with my goal. Action: Observation: Reflection: that leads to the princesss bedroom. goal. Action: Observation: Reflection: or in coma. mission. Action: Observation: Reflection: directly supports the goal of saving her. the appropriate move. Action: Observation: says. have reached the princess, but she appears to be unconscious need to determine how to safely wake her to complete the The valid actions include going up, which matches the staircase This is likely the shortest path to the The available actions include wake up the princess, which The staircase likely leads to the princesss have entered the main hall of the castle. wake up the princess, take out sword, down south, north, up, look, examine staircase should verify available actions first. The princess wake up from the coma. No threat is present, so this is Princess is sleeping in bed. Thank you my knight, she The task is finished. wake up the princess check valid actions Princesss bedroom. check valid actions The environment is There is up Figure 19: Prompt for Jericho ReflAct"
        },
        {
            "title": "I More Case Study",
            "content": "ALFWorld: clean some soapbar and put it in cabinet. go to cabinet 4 go to cabinet 1 can check one by To solve the task, need clean some soapbar and put it ReAct Goal: in cabinet. Thought: to find soapbar, then clean it with sinkbasin, then put it in cabinet. First need to find soapbar. soapbar is more likely to appear in cabinet 1-4, countertop 1, sinkbasin 1-2, or toilet 1. one, starting by going to cabinet 1. Action: ... Action: Observation: The cabinet 4 is open. In it, you see candle 1, and cloth 2. Thought: cabinet 4, which can use to clean the soapbar. soapbar. to check if it contains soapbar. Action: Observation: you see soapbar 1, and soapbottle 1. Thought: Now find soapbar 1 on the countertop 1. clean it with sinkbasin. will go to sinkbasin 1 to clean the soapbar. will go to countertop 1 Now find cloth 2 in You open the cabinet 4. Next, need to find On the countertop 1, go to countertop 1 Next, need to Ungrounded thought: The agent attempts to go to the sinkbasin without having taken the soapbar. On the sinkbasin 1, you go to sinkbasin 1 Action: Observation: see nothing. Thought: Since the sinkbasin 1 is empty, need to clean the soapbar 1 with something. will use the cloth 2 from cabinet 4 to clean the soapbar. Hallucination: The agent performs an action without soapbar present. Nothing happens. Since using the cloth 2 did Action: use cloth 2 Observation: Thought: not have any effect, need to find way to use it to clean the soapbar. will go back to the cabinet 4 and take the cloth 2 to use it with the sinkbasin 1. Action: Observation: ... # Task Failed take cloth 2 from cabinet 4 Nothing happens. To solve the task, clean some soapbar and put it First, need to find soapbar is more likely to ReflAct Goal: in cabinet. Reflection: need to find soapbar, clean it with sinkbasin, and then put it in cabinet. soapbar. appear in cabinet 1-4, countertop 1, or sinkbasin 1-2. one, starting by going to cabinet 1. Action: ... Action: Observation: you see soapbar 1, and soapbottle 1. Reflection: countertop 1, and have found soapbar 1, which need to clean before placing it in the cabinet to complete the task. On the countertop 1, can check one by Currently, am at go to countertop 1 go to cabinet The agent performs an action that are goal-related and well grounded in the current situation. On the sinkbasin 1, you You pick up the soapbar Currently, am at Action: take soapbar 1 from countertop 1 Observation: 1 from the countertop 1. Reflection: countertop 1, now holding soapbar 1, which need to clean before placing it in the cabinet to complete my task. Observation: see nothing. Reflection:Currently, am at sinkbasin 1, Im holding soapbar 1, and am now ready to clean it before placing it in the cabinet to complete my task. Action: Action: with sinkbasin 1 Observation: using the sinkbasin 1. ... # Task Succeed clean soapbar You clean the soapbar 1 Figure 20: Case study for ALFWorld 22 ScienceWorld: Your task is to boil tin. For compounds without boiling point, combusting the substance is also acceptable. First, focus on the substance. Then, take actions that will cause it to change its state of matter. look around In it, you see: You teleport to the Now that Im in the teleport to workshop This room is called the ReAct ... Action: Observation: workshop. Thought: workshop, should look around to find tin. The workshop is place where various projects and repairs are done, so might find tin here in the form of scraps, sheets, or other objects. Action: Observation: workshop. the table is: wire, metal pot (containing substance called tin), ... Thought: pot on the table! located the tin, should focus on it as per the task instructions. Action: focus on tin Observation: Thought: Now that Ive focused on the tin, need to take actions to boil or combust it. Since the tin is in metal pot, can use heat source to boil it. furnace in the foundry, which would be perfect for heating the tin. should teleport back to the foundry and use the blast furnace. found tin in the metal You focus on the tin. battery, green see blast Now that Ive ... On Ungrounded thought: The agent confirms the presence of tin and proceeds with planning, but does not actually possess the tin. You teleport to the teleport to foundry Now that Im back in the Action: Observation: foundry. Thought: foundry, should move the metal pot containing the tin to the blast furnace and then activate the blast furnace to heat the tin. Action: furnace Observation: that input. Thought: It seems made mistake with the action. using different action to move the metal pot to the blast furnace. move metal pot to blast No known action matches should try again, Hallucination: Although the agent recognizes the mistake, it continues to attempt the same invalid action. Action: move metal pot to blast furnace ... # Task Failed ... On look around focus on tin In it, you see: battery, green teleport to foundry am now focused on You teleport to the teleport to workshop You focus on the tin. Ive found metal pot need to look for tin Currently, am in the Currently, am in the This room is called the ReflAct ... Action: Observation: workshop. Reflection: workshop, holding nothing and focused on nothing. in this room. Action: Observation: workshop. the table is: wire, metal pot (containing substance called tin), ... Reflection: workshop, holding nothing and focused on nothing. containing tin on the table, which is the substance need to focus on and then boil or combust Action: Observation: Reflection: the tin, which is contained in the metal pot on the table. To boil or combust the tin, need to apply heat to it. see foundry in the environment, which is likely to have heat source. Action: ... Reflection: activated and ready to be used as heat source. need to move the tin from the metal pot in the workshop to the blast furnace in the foundry. Action: Observation: workshop. Reflection: workshop, where the metal pot containing the tin is located. need to move the tin to the blast furnace in the foundry. Action: furnace Observation: that input. Reflection: need to move the tin to the blast furnace, but the action is not recognized. Instead, can pick up the metal pot and then teleport to the foundry. Action: Observation: to the inventory. Reflection: the foundry, with the metal pot containing the tin in my inventory. can now use the blast furnace to boil or combust the tin. ... # Task Succeed The blast furnace is now move metal pot to blast No known action matches You move the metal pot teleport to workshop am now back in the You teleport to the pick up metal pot am now back in Figure 21: Case study for ScienceWorld 23 Jericho: You are pizza delivery man. Today you are making new delivery on your bosss order. You need to find out by yourself who to deliver. ... The street is should head The note reads: The courtroom is Calzone, located at Courthouse You are in Street, Near Courthouse It seems like Im on the ReAct ... Observation: \"Stanley, Dont forget to make your delivery to Mr. the San Doppleton Courthouse. ... Observation: You are approaching the center of downtown San Doppleton. There is courthouse to the south. The street runs east and west. Thought: right track, as the courthouse is now visible to the south. south to reach the courthouse and deliver the pizza to Mr. Calzone as instructed in the note. Action: South Observation: crowded courthouse. off to the north. currently abuzz with excitement as it seems that the pre-trial hearing of the infamous gangster, Guido \"The Armadillo\" Calzone is winding to close. sit placidly at their table, while the flustered young district attorney who is prosecuting the case seems like shes at her wits end. At the moment, the D.A. stands at the front of the courtroom, arguing over legal point with the judge. Thought: courthouse where Mr. present. need to find way to deliver the Calzone without causing pizza to Mr. disruption. should approach him or his lawyer to hand over the pizza box. Action: Show people box Observation: You cant see any such thing. Thought: It seems that the action \"show people box\" was not understood in this context. more direct way to deliver the pizza to Mr.Calzone. in the courtroom, should try to give the pizza box directly to him or his lawyer. Guido and his hotshot lawyer have arrived at the Since he is present need to find Calzone is Since the courtroom is busy, Mismatched execution: The agent fails to specify the target, leading to another invalid action. Action: Give people box Observation: You cant see any such thing. ... # Task Failed ... South There is The note reads: Calzone, located at Ive finally found the Street, Near Courthouse ReflAct ... Observation: \"Stanley, Dont forget to make your delivery to Mr. the San Doppleton Courthouse. ... Observation: You are approaching the center of downtown San Doppleton. courthouse to the south. The street runs east and west. Reflection: courthouse, which is located to the south of my current position. This is where need to deliver the pizza to Mr. Calzone. Now that Ive located the courthouse, should proceed south to complete the delivery task. Action: Observation: crowded courthouse. off to the north. currently abuzz with excitement as it seems that the pre-trial hearing of the infamous gangster, Guido \"The Armadillo\" Calzone is winding to close. sit placidly at their table, while the flustered young district attorney who is prosecuting the case seems like shes at her wits end. At the moment, the D.A. stands at the front of the courtroom, arguing over legal point with the judge. Reflection: courthouse where the pre-trial hearing for Guido \"The Armadillo\" Calzone is taking place. This is the location where need to deliver the pizza to Mr. Ive found him, should proceed with delivering the pizza to complete my task. Guido and his hotshot lawyer Courthouse You are in The courtroom is Ive entered the Calzone. Since The street is Well-grounded and goal-aligned reasoning: identifies the recipient and forms valid, context-aware action. The agent explicitly Guido grins shiftily Action: Give pizza box to Guido Calzone Observation: and takes the pizza from you. hands it to his lawyer so he can eat it later. congratulating yourself on job well done. # Task Succeed You walk away, He Figure 22: Case study for Jericho Jericho: search for mug in an inn. You ... ... The floor is made Examine floorboards Inside the Inn There You cant see any such Im inside the inn, and should start ReAct ... Observation: are few patches of mould on the walls and the floorboards creak slightly, but the building seems safe You see some tables and enough. chairs. You can go west (back outside) or down some steps. can see magnificently carved chair here. Thought: need to find mug. by examining the tables and chairs to see if theres mug lying around. Action: Examine tables and chairs Observation: thing. ... Action: Observation: of large, rather uneven rough floorboards, on which lie the remains of the scattered rushes once used as floor covering. Thought: floorboards, but didnt find mug. to pick up, and since Im in the inn, should check the inventory again to see if have any other options, but it seems Im empty-handed. should try to find something to pick up. Action: down ... Action: again Observation: as far as wanting to examine the floor. Thought: various objects and places in the inn, but didnt find mug. should try to find something to pick up, and since Im in the inn, should check the inventory again to see if have any other options, but it seems Im empty-handed. should try to find something to pick up. should try to find something Examine stone steps leading Examine the floorboards only understood you Ive tried examining Ive examined the Short-sighted planning: The agent repeatedly focuses narrowly on immediate surroundings without reconsidering its strategy in light of prior failures. Examine the rushes on the Action: floor again Observation: as far as wanting to examine the floor. ... # Task Failed only understood you You ... ... Ive seen some Ive entered the Inside the Inn There You cant see any such Examine tables and chairs ReflAct ... Observation: are few patches of mould on the walls and the floorboards creak slightly, but the building seems safe You see some tables and enough. chairs. You can go west (back outside) or down some steps. can see magnificently carved chair here. Reflection: abandoned inn, the Trolls Arms, and it appears to be old and dusty, but safe enough. tables and chairs, and dark, intricately-carved wooden chair in the corner. Since my goal is to search for mug in an inn, should investigate the tables and chairs to see if theres anything useful. Action: Observation: thing. ... Action: Observation: as far as wanting to examine the walls. Reflection: find mug, but that didnt reveal anything. should try to think of different approach to find the mug. Action: Observation: seems to have been formed from natural cavern in the rock under the inn, ... the slightly chill air. ... go up (to the inn) or west (into the dark tunnel). silver coin and some barrels here. Reflection: mug, but so far, havent found anything useful. should try to think of different approach to find the mug. promising location to search for mug, as its storage area for the inns supplies. , which will remain cool in You can Ive been trying to find The cellar seems like Examine bar or counter You can see an ancient Inn Cellar The cellar only understood you Ive been trying to Down Reflecting on long-term goal: The agent adapts its exploration strategy based on environmental cues and prior search failures. Examine barrels Action: Observation: The barrels are now no more than rotten staves of dry wood, held together by the woodworm holding hands. # Task Succeed You have found pewter mug. Figure 23: Another case study for Jericho"
        }
    ],
    "affiliations": [
        "KAIST",
        "Seoul National University"
    ]
}