{
    "paper_title": "STARFlow-V: End-to-End Video Generative Modeling with Normalizing Flows",
    "authors": [
        "Jiatao Gu",
        "Ying Shen",
        "Tianrong Chen",
        "Laurent Dinh",
        "Yuyang Wang",
        "Miguel Angel Bautista",
        "David Berthelot",
        "Josh Susskind",
        "Shuangfei Zhai"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Normalizing flows (NFs) are end-to-end likelihood-based generative models for continuous data, and have recently regained attention with encouraging progress on image generation. Yet in the video generation domain, where spatiotemporal complexity and computational cost are substantially higher, state-of-the-art systems almost exclusively rely on diffusion-based models. In this work, we revisit this design space by presenting STARFlow-V, a normalizing flow-based video generator with substantial benefits such as end-to-end learning, robust causal prediction, and native likelihood estimation. Building upon the recently proposed STARFlow, STARFlow-V operates in the spatiotemporal latent space with a global-local architecture which restricts causal dependencies to a global latent space while preserving rich local within-frame interactions. This eases error accumulation over time, a common pitfall of standard autoregressive diffusion model generation. Additionally, we propose flow-score matching, which equips the model with a light-weight causal denoiser to improve the video generation consistency in an autoregressive fashion. To improve the sampling efficiency, STARFlow-V employs a video-aware Jacobi iteration scheme that recasts inner updates as parallelizable iterations without breaking causality. Thanks to the invertible structure, the same model can natively support text-to-video, image-to-video as well as video-to-video generation tasks. Empirically, STARFlow-V achieves strong visual fidelity and temporal consistency with practical sampling throughput relative to diffusion-based baselines. These results present the first evidence, to our knowledge, that NFs are capable of high-quality autoregressive video generation, establishing them as a promising research direction for building world models. Code and generated samples are available at https://github.com/apple/ml-starflow."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 6 2 ] . [ 2 2 6 4 0 2 . 1 1 5 2 : r STARFlow-V: End-to-End Video Generative Modeling with Normalizing Flows Jiatao Gu, Ying Shen, Tianrong Chen, Laurent Dinh, Yuyang Wang, Miguel Ángel Bautista, David Berthelot, Josh Susskind, Shuangfei Zhai Apple Normalizing flows (NFs) are end-to-end likelihood-based generative models for continuous data, and have recently regained attention with encouraging progress on image generation. Yet in the video generation domain, where spatiotemporal complexity and computational cost are substantially higher, state-of-the-art systems almost exclusively rely on diffusion-based models. In this work, we revisit this design space by presenting STARFlow-V, normalizing flow-based video generator with substantial benefits such as end-to-end learning, robust causal prediction, and native likelihood estimation. Building upon the recently proposed STARFlow, STARFlow-V operates in the spatiotemporal latent space with global-local architecture which restricts causal dependencies to global latent space while preserving rich local withinframe interactions. This eases error accumulation over time, common pitfall of standard autoregressive diffusion model generation. Additionally, we propose flow-score matching, which equips the model with light-weight causal denoiser to improve the video generation consistency in an autoregressive fashion. To improve the sampling efficiency, STARFlow-V employs video-aware Jacobi iteration scheme that recasts inner updates as parallelizable iterations without breaking causality. Thanks to the invertible structure, the same model can natively support text-to-video, image-to-video as well as video-to-video generation tasks. Empirically, STARFlow-V achieves strong visual fidelity and temporal consistency with practical sampling throughput relative to diffusion-based baselines. These results present the first evidence, to our knowledge, that NFs are capable of high-quality autoregressive video generation, establishing them as promising research direction for building world models. Code: https://github.com/apple/ml-starflow Correspondence: Jiatao Gu (jgu32@apple.com) Date: November 27,"
        },
        {
            "title": "Introduction",
            "content": "Deep generative modeling has advanced rapidly with breakthroughs across language (Achiam et al., 2023; OpenAI, 2024a), images (Podell et al., 2023; Batifol et al., 2025; Wu et al., 2025), and videos (OpenAI, 2024b; Wan et al., 2025; DeepMind, 2025) domains. Among these modalities, video generation is uniquely demanding: beyond high perceptual quality, models must capture rich spatiotemporal structure, remain robust over long horizons, and often operate under causal constraints for streaming and interactive use. Such capabilities are central not only to creative media (Ye et al., 2025; Yuan et al., 2025), but also to emerging world models for gaming, simulation and embodied AI (Ha and Schmidhuber, 2018; Yang et al., 2023; Hu et al., 2023; Google DeepMind, 2024; Hafner et al., 2025). Recent scaling of data, model capacity, and compute has pushed video generation to new levels of fidelity (Yang et al., 2025; Kong et al., 2024; Kondratyuk et al., 2024; Yu et al., 2024; Wan et al., 2025; Seawead et al., 2025; Gao et al., 2025). In this space, diffusion-based approaches (Ho et al., 2020; Rombach et al., 2022; Peebles and Xie, 2023; Lipman et al., 2023; Esser et al., 2024) have emerged as the dominant Work done while JG holding joint affiliation with University of Pennsylvania, and YS working as research intern at Apple MLR. 1 Figure 1 Samples from STARFlow-V across three tasks. All videos are 480 at 16 fps. Red boxes mark the conditioning inputs. The same autoregressive architecture is used for all tasks with no task-specific modifications. Please find more generated videos and comparisons in the released code https://github.com/apple/ml-starflow. backbone for textand image-conditioned video synthesis, thanks to their strong empirical performance and flexible conditioning mechanisms. Standard diffusion models are trained by corrupting frames with noise drawn from schedule and learning denoiser that inverts this process one step at time, which leads to an iterative sampling procedure at inference. For offline generation this formulation works well, but the parallel denoising of multiple frames is inherently non-causal: future frames can influence earlier ones, making it less natural to apply in streaming or interactive settings that require strictly causal rollouts. Causally conditioned and sequential diffusion variants (Chen et al., 2024a; Huang et al., 2025) mitigate some of these issues, but still inherit the need to simulate noise at different timesteps and frames during training and can exhibit traintest mismatch during long-horizon autoregressive generation. In parallel, normalizing flows (NFs) (Rezende and Mohamed, 2015; Dinh et al., 2014, 2016) offer distinct, likelihood-based alternative. NFs are continuous end-to-end generative models that provide exact log-likelihood evaluation, non-iterative sampling, and native support for invertible feature mappings. After an initial wave of work (Dinh et al., 2016; Kingma and Dhariwal, 2018), they received relatively less attention compared to diffusion models, but have recently regained interest with encouraging progress on image generation (Zhai et al.; Gu et al., 2025; Zheng et al., 2025). In particular, STARFlow (Gu et al., 2025) shows that parameterizing an autoregressive normalizing flow with Transformer and operating in latent space allows flows to scale competitively in the high-resolution image domain. Yet, in the video domainwhere complexity and computational cost are substantially higherstate-of-the-art systems almost exclusively rely on diffusion, and it remains unclear whether NFs can be practical for video. In this work, we revisit this design space and introduce STARFlow-V, normalizing-flow-based video generator that combines end-to-end training with causal, likelihood-based modeling. Building on STARFlow (Gu et al., 2025), STARFlow-V operates in spatiotemporal latent space with globallocal architecture: compact global latent sequence carries long-range temporal context, while local latent blocks preserve fine-grained within-frame structure. By delegating temporal reasoning to this high-level space, the model mitigates the accumulation of autoregressive errors that commonly plagues diffusion-based video generators. As observed in TARFlow (Zhai et al., 2024), training flows on slightly perturbed data with subsequent denoising step can significantly improve robustness. Unlike existing methods (Zhai et al., 2024; Gu et al., 2025), we propose flow-score matching, which learns lightweight causal denoiser to enhance temporal consistency in video scenarios. To further improve efficiency, STARFlow-V employs video-aware Jacobi-style update scheme that recasts inner refinement steps as parallelizable iterations. Finally, owing to its invertible nature, the same backbone naturally supports text-to-video (T2V), image-to-video (I2V), and video-to-video (V2V) generation by simply changing the form of the conditioning signal. Across all benchmarks, STARFlow-V attains visually coherent and temporally stable generations while maintaining practical sampling speed relative to diffusion-based models. We believe this provides initial evidence that NFs are capable of high-quality autoregressive video generation and potentially world models."
        },
        {
            "title": "2.1 Video Generative Models\nGiven N frames x1:N = (x1, . . . , xN ) and optional conditioning C (e.g., text, image, audio, layout, camera),\nvideo generative models seek to model the joint distribution of all frames p(x1:N | C) and sample novel\nvideos from the learned model. While earlier work explored GANs (Vondrick et al., 2016; Tulyakov et al.,\n2018; Skorokhodov et al., 2022), VAEs (Babaeizadeh et al., 2018; Castrejon et al., 2019; Wu et al., 2021),\nand discrete autoregressive models (Yan et al., 2021; Yu et al., 2024; Kondratyuk et al., 2024), the field has\nlargely converged on diffusion-based methods Ho et al. (2022c,a). Spurred by the release of Sora (Brooks\net al., 2024), DiT-style approaches (Peebles and Xie, 2023) have shown strong generalization at scale (Gao\net al., 2025; Wan et al., 2025; DeepMind, 2025). A key distinction from prior paradigms is that training\nof diffusion-based models is Not End-to-End : diffusion-based models corrupt frames with noise at randomly\nsampled levels and train a denoiser to invert this process, optimizing an objective closely related to the lower\nbound of log p(x1:N | C). This setup incurs high cost—especially for video—as each update supervises only\na single noise level. At inference time, one sample is generated by iteratively denoising from Gaussian noise.",
            "content": "3 Diffusion-based video generation is typically non-causal: all frames are corrupted with noise and denoised in parallel (Ho et al., 2022c). Yet many real-world applications demand causal, often interactive synthesis (e.g., online streaming, video games, robotics), where frames must be produced sequentially. Autoregressive (AR) diffusion models (Chen et al., 2024a; Song et al., 2025; Yin et al., 2025)a line of work that combines chainrule factorization with diffusionaim to alleviate prior limitations by introducing asynchronous, frame-wise noise schedules during training, modeling each conditional p(xn x<n) as diffusion process. Despite their strengths, AR generation typically suffers from exposure bias: during training, models condition on groundtruth contexts, whereas at inference they must rely on their own (imperfect) predictions. This traintest mismatch compounds over time, degrading long-horizon video quality. The nonend-to-end nature of diffusion training further exacerbates this gap, though recent efforts such as Self-Forcing (Huang et al., 2025) seek to mitigate it via sequential post-training with distillation objectives. However, they are not readily applicable in the pre-training stage on raw video data."
        },
        {
            "title": "2.2 Autoregressive Normalizing Flows",
            "content": "Normalizing flows (NFs; Rezende and Mohamed, 2015; Dinh et al., 2014, 2016; Kingma and Dhariwal, 2018; Ho et al., 2019) are likelihood-based generative models built from invertible transformations. Given continuous input pdata, RD, an NF learns bijection fθ : RD RD that maps data to latents = fθ(x). Unlike diffusion models, NFs are trained end-to-end via tractable maximum-likelihood objective derived from the change-of-variables formula: LNF(θ) = Ex (cid:2)log p0 (cid:0)fθ(x)(cid:1) + logdet(Jfθ (x))(cid:3) , (2.1) where the first term encourages mapping data to high-density regions of simple prior p0 (e.g., standard Gaussian), and the Jacobian term Jf accounts for the local volume change induced by fθ, preventing collapse. Once trained, sampling is immediate via inversion: draw p0(z) and set = 1 (z). Historically, however, NFs have been viewed as less competitive than diffusion models due to architectural rigidity and training instability (Dinh et al., 2016). θ Recently, TARFlow (Zhai et al.) and its scalable extension, STARFlow (Gu et al., 2025), have revisited normalizing flows as next-generation backbones for generative modeling. Both methods instantiate autoregressive flows (AFs) (Kingma et al., 2016; Papamakarios et al., 2017)NFs whose invertible transformations are parameterized autoregressivelyand use causal Transformer blocks, in the style of LLMs, as their primary building units. Formally, STARFlow (Gu et al., 2025) stacks autoregressive flow blocks with alternating directions, where each block applies an affine transform whose parameters are predicted by causal Transformer under (self-exclusive) causal mask m: = (cid:2)x µθ (cid:0)x m(cid:1)(cid:3) /σθ (cid:0)x m(cid:1), σθ() > 0, (2.2) where x, are the input and output of each block, denotes the Hadamard product. As shown in STARFlow (Gu et al., 2025), 3 blocks suffice for universal density modeling where masks alternate between left-to-right () and right-to-left () to capture bidirectional dependencies. Despite STARFlow demonstrating competitive quality with state-of-the-art diffusion (Podell et al., 2023; Esser et al., 2024) on large-scale text-to-image tasks, evidence for normalizing flows in video generation remains sparse. To our best knowledge, the only prior NF-based video model is VideoFlow (Kumar et al., 2019), which builds on Glow (Kingma and Dhariwal, 2018) and is constrained by limited capacity, low resolution, and domain-specific settings. Compared to images, video generation is substantially more challenging for NFs due to higher spatiotemporal dimensionality. Nevertheless, we argue that normalizing flowsexemplified by STARFloware natural fit for video modeling, especially in autoregressive settings."
        },
        {
            "title": "3 STARFlow-V",
            "content": "We propose STARFlow-V, novel paradigm for video generation based on normalizing flows. While inspired by STARFlow (Gu et al., 2025), STARFlow-V is not direct port to the video domain; it introduces several architectural redesigns and algorithmic innovations tailored to spatiotemporal data. In what follows, we present the architecture and its autoregressive formulation (Section 3.1), the training procedure (Section 3.2), the inference pipeline (Section 3.3), and applications enabled by our model (Section 3.4)."
        },
        {
            "title": "3.1 Proposed Model",
            "content": "For video RN HW D, each frame xn is flattened to RHW D, xn = (xn,1, . . . , xn,HW ), and all frames are concatenated into sequence of HW tokens. We operate in compressed latent space using pretrained 3D causal VAE (Wan et al., 2025). STARFlow-V models the joint distribution pθ(x) via an invertible mapping fθ implemented as autoregressive flows (Equation (2.2)). Following Gu et al. (2025), we use deepshallow decomposition fθ = fD fS, where small stack of shallow flow blocks with alternating (left-to-right / right-to-left) masks maps to intermediate latents = fS(x), and deep causal-Transformer flow fD then maps to the prior, producing = fD(u). By the change-of-variables formula, pθ(x) = p0(z) (cid:12) (cid:12)det JfD (u)(cid:12) (cid:12) where p0 is simple prior (e.g., standard Gaussian). Most capacity is allocated to the deep block fD for semantic modeling, while the shallow stack fS handles local reshaping. For videos, we can simply treat all frames as one long token sequence: fD follows left-to-right causal order over the video (causal across frames, raster order within each frame), and fS retains the alternating masks defined above. Because fS propagates information from future frames to past ones, this naïve design yields non-causal video generator, motivating the globallocal restructuring described next. (cid:12)det JfS (x)(cid:12) (cid:12) (cid:12), (3.1) GlobalLocal Architecture Observing that fD is inherently autoregressive and that fS mainly provides local refinements, we adapt the design into globallocal structure: fS is restricted to operate within each frame, while only fD propagates global video context in causal manner. More specifically, Equation (3.1) can be re-expressed as an autoregressive factorization over frames xn: pθ(x) = (cid:89) n=1 pθ(xn x<n) = (cid:89) n= pD(un u<n)(cid:12) (cid:12)det JfS (xn)(cid:12) (cid:12), (3.2) where un = fS(xn) denotes the local latents for frame xn. Here, the deep block is itself an autoregressive flow, capturing both intra-frame raster ordering and inter-frame causal dependencies. Formulating STARFlow-V in globallocal manner (Equation (3.2)) yields several benefits: (a) Universality. Equation (3.2) preserves the universal approximation guarantee of STARFlow (Gu et al., 2025): the local stack fS still realizes per-pixel infinite Gaussian mixtures via alternating causal masks, so expressivity is not curtailed by restricting fS to within-frame contexts. (b) Robustness. Intuitively, Equation (3.2) can be viewed as continuous language model for videos: the deep-flow term pD(un u<n) acts as Gaussian Next-Token Prediction (cf. the affine form in Equation (2.2)) in latent space, while the shallow flow supplies the Jacobian factor det JfS (xn), yielding flexible density over x. Compared to modeling directly (arbitrarily multimodal), the latent is unimodal at each step, easier to regress, and more tolerant to small prediction errors. Crucially, the sampling conditions on previously generated latents rather than pixels, so data-space errors do not phase via 1 propagate forward, mitigating the compounding error typical of autoregressive diffusion. Unlike diffusionstyle noise conditioning (Ho et al., 2022b; Chen et al., 2024a), which compromises information to gain robustness and introduces extra parameters, our mappings are invertible, avoiding information loss by construction. (c) End-to-End Training. The whole model is still NF. Consequently, all parameters are trained by exact MLE via the change-of-variables objectiveno per-step denoising schedule or surrogate losssimplifying optimization and reducing traintest mismatch. (d) Streamable Generation. At inference time, 1 samples un causally (token-by-token, frame-by-frame), decodes each frame independently given un. This process enables causal video synthesis since and 1 later frames cannot influence earlier ones. S"
        },
        {
            "title": "3.2 Revisiting Noise-Augmented Training\nAs observed by Zhai et al. (2024), injecting small noise into the data is crucial for stabilizing NF training.\nConcretely, we learn STARFlow-V on a σ-smoothed density qσ( ˜x) = (p ∗ N (0, σ2I))( ˜x). A side effect is that",
            "content": "5 Figure 2 An illustrated pipeline of STARFlow-V which shows (1) the proposed global-local architecture; (2) joint training with the learnable denoiser with the proposed Flow-score Matching. During sampling, STARFlow-V takes the encoded text condition and transforms the noise through deep global block to intermediate features u, followed by several local shallow blocks to produce slightly noised video. Finally, learnable causal denoiser refines this output into the final clean video x. the model naturally generates slightly noisy samples, necessitating post-processing step to recover the clean ones. We first examined the existing options for this purpose: (a) Decoder Fine-tuning We followed STARFlow (Gu et al., 2025), adopting their strategy of fine-tuning the VAE decoder to denoise noisy latents using GAN objective (Rombach et al., 2022). However, our preliminary experiments suggest that this approach is not readily applicable to 3D causal VAEs: under Gaussian-noised latent inputs, the decoder fails to maintain temporal consistency in the generated videos due to limited receptive fields. (b) Score-based Denoising Instead of decoder fine-tuning, TARFlow (Zhai et al., 2024) proposes to denoise using the learned flow itself via score-based updates. For noisy sample qσ, the continuity equation gives σ = σ log qσ( x). So for sufficiently small σ, single Euler step yields the Tweedie estimator: σ σ = + σ2 log qσ( x). (3.3) With normalizing flows, we replace qσ by the learned density pθ, and compute log pθ( x) via automatic differentiation through the flow, which amounts to an additional forwardbackward pass. However, this score-based denoising presents two issues: (1) Noisy gradients. The learned density pθ is imperfect; its score log pθ( x) often contains high-frequency noise, which manifests as bright speckle-like artifactsespecially in regions with large motion; (2) Non-causality of the score. Even if pθ is modeled causally, the score log pθ( x) is, by definition, global: the gradient at time depends on likelihood terms involving future frames > n. This breaks causality, undermining the promised streamable generation. Proposed Approach: Flow-Score Matching To address these issues, we introduce lightweight neural denoiser sϕ trained alongside the flow fθ to regress the models score: Ldenoise(ϕ) = Ex, ϵ (cid:13) (cid:13) sϕ( x) σ log pθ( x) (cid:13) 2 2, (cid:13) = + ϵ, ϵ (0, σ2I). (3.4) At inference, we replace the raw score in the update (cf. Equation (3.3)) with the learned denoiser sϕ. This flow-score matching (FSM) is simple yet effective. First, the smooth inductive bias of neural networks suppresses stochastic high-frequency artifacts in log pθ. Second, we can encode causality directly in sϕ, reensuring streamable behavior. Concretely, we parameterize sϕ with oneframe look-ahead while remaining globally causal (one-step latency)1. We approximate the score at step by sϕ( xn+1) (cid:0)σ log pθ( x)(cid:1) n. 1Strictly causal ( n) fails as temporal differences are pivotal to determining the denoising direction. 6 Model Total Quality Semantic Aesthetic Object Multi Obj. Human Spatial Scene Closed-source models Gen-3 (Germanidis, 2024) Veo3 (Google DeepMind, 2025) 82.32 85.06 84.11 85. Diffusion models OpenSora-v1.1 (Zheng et al., 2024) CogVideoX (Yang et al., 2024) HunyuanVideo (Kong et al., 2024) Wan2.1-T2V (Wan et al., 2025) Autoregressive (Diffusion) models CogVideo (Hong et al., 2022) Emu3 (Wang et al., 2024b) NOVA (Deng et al., 2024) SkyReel-v2 (Chen et al., 2025) MAGI-1-distill (Teng et al., 2025) Normalizing Flows STARFlow-V (Ours) STARFlow-V (Ours) STARFlow-V (Ours, non-Causal) 75.66 80.91 83.24 83.69 67.01 80.96 80.12 83.90 77.92 78.67 79.70 79. 77.74 82.18 85.09 85.59 72.06 84.09 80.39 84.70 80.98 80.24 80.76 80.34 75.17 82.49 67.36 75.83 75.82 76.11 46.83 68.43 79.05 80.80 65. 72.37 75.43 74.71 63.34 63.81 50.12 60.82 60.36 66.07 38.18 59.64 59.42 - 62.43 54.48 59.73 58.70 87.81 93. 86.76 83.37 86.10 86.28 73.40 86.17 92.00 - 82.37 86.65 80.61 81.08 53.64 82.20 40.97 62.63 68.55 69.58 18.11 44.64 77.52 - 35. 53.48 56.04 54.60 96.40 99.40 84.20 98.00 94.40 95.40 78.20 77.71 95.20 - 84.20 94.00 98.13 98.40 65.09 84. 54.57 57.43 52.47 69.90 68.68 75.39 18.24 68.73 77.52 - 57.75 49.84 76.08 73.15 38.63 51.14 53.88 45.75 28.24 37.11 54.06 - 26. 47.08 48.21 49.61 Table 1 Text-to-video evaluation on VBench (Huang et al., 2024). The baseline data is from the leaderboard. Following Yang et al. (2025), we also evaluate with the official GPT-augmented prompts (Rewriter), with longer and more descriptive text inputs. denotes results using Rewriter prompts. Finally, we train sϕ jointly with fθ at minimal overhead: since fθ is trained by maximizing log pθ, we cache the input gradients from the backward pass and reuse it as the target for sϕ."
        },
        {
            "title": "3.3 Fast Inference",
            "content": "While STARFlow-V leverages parallel computation during training via causal masking, generation at inference time is carried out sequentially (one token at time) through multiple AF blocks, which can be extremely computationally demanding for long video sequences. For instance, generating 5s 480p video under 16 fps using pre-trained 3B parameter model requires over 30 minutes, which is far from real-time performance. To enable fast inference, we introduce two strategies: Block-wise Jacobi Iteration Rather than sampling continuous tokens strictly autoregressively, we accelerate inference by recasting inversion as solving nonlinear fixed-point system with parallel solvers such as Jacobi iteration (Porsching, 1969; Kelley, 1995), strategy recently used to speed up autoregressive models (Song et al., 2021; Teng et al., 2024; Liu and Qin, 2025; Zhang et al., 2025). Specifically, the inverse of Equation (2.2) can be written as the fixed-point equation = µθ(x m) + σθ(x m) z, (3.5) where is (self-exclusive) causal mask. This induces triangular system that admits convergence under nonlinear Jacobi iteration (Saad, 2003): starting from an initial sequence estimate x(0), iterate x(k+1) = µθ(x(k) m) + σθ(x(k) m) until converge criterion is satisfied. We monitor scale-normalized 2 < τ with τ = 0.001 by default. Although the worst-case iteration count residual, x(k+1) x(k)2 scales with sequence length (e.g., near-Markovian process), video generation exhibits strong global structure, substantially accelerating convergence in practice. The procedure is also guidance-compatible, as proposed in (Gu et al., 2025), which involves computing the guided parameters ˆµ and ˆσ and then substituting them. 2/x(k+1) To further accelerate sampling, we adopt block-wise Jacobi scheme in the spirit of Song et al. (2021); Liu and Qin (2025). The token sequence is partitioned into contiguous blocks of size B, which are processed sequentially across blocks but in parallel within each block. Within each block we run the Jacobi updates, while states from completed blocks are cached as context (e.g., KV cache) for subsequent blocksanalogous to standard AR inference. We also apply video-aware initialization: for new frame, the initial estimate x(0) . Overall, we adopt block-based iteration within n+1 each AF block, yielding 15 lower inference latency relative to standard autoregressive decoding, while preserving visual fidelity. is initialized from the previously converged frame x(k) 7 Figure 3 STARFlow-V comparison against baselines on autoregressive generation for both trained length (5s) and long-horizon generation (30s). Please refer to more video comparison in the project page. Pipelined Decoding As described in Section 3.1, the globallocal design applies standard global left-to-right autoregression in the deep block fD, while the shallow blocks fS traverse each frame independently. This enables pipelined schedule (analogous to pipeline parallelism (Huang et al., 2019)): fD runs continuously without waiting on fS, and, in parallel, fS threads consume fDs outputs, immediately refine them, and then denoise. Because fD is typically the slowest stage, end-to-end latency is dominated by the deep block."
        },
        {
            "title": "3.4 Versatility Across Tasks",
            "content": "STARFlow-V can be trained for different video generation tasks. By default, STARFlow-V is trained for text-to-video generation on large-scale textvideo pairs. Without modifying the backbone, we support the following settings: (a) Image-to-Video Generation. We directly treat the first frame as observed conditioning. Owing to the invertiblity, no separate encoder is required : we encode the observed frame via the flow forward to initialize the KV cache; subsequent frames are then generated. (b) Video-to-Video Generation. Given source clip x0:T , we treat all frames as observed conditioning andthanks to invertibilityuse the same backbone to flow-encode them and populate the KV cache. The model then autoregressively rolls out the target clip ˆx0:T under optional task cues (e.g., in/outpainting masks, edit text, camera/pose), copying through unedited regions while synthesizing edits. This mirrors our image-to-video path but operates framewise over the whole clip without separate encoder. (c) Longer Generation. Our model generates videos far longer than those seen during training via slidingwindow (chunk-to-chunk) schedule in the deep block. After producing latent chunk u, we warm-start the next step by rebuilding the KV cache: we re-run fD on the last latents (the overlap) and then continue autoregression to synthesize the next latents. fS then process the latents per frame, enabling streaming output. To mitigate boundary mismatch, we randomly drop the first frame during training to simulate restart."
        },
        {
            "title": "4.1 Experimental Setup\nDatasets. We construct a diverse and high-quality collection of video datasets to train STARFlow-V. Specif-\nically, we leverage the high-quality subset of Panda (Chen et al., 2024b) mixed with an in-house stock video\ndataset, with a total number of 70M text-video pairs. For all videos, we keep their raw captions, and apply a\nvideo captioner (Wang et al., 2024a) to generate a longer description to cover the details. The ratio of training",
            "content": "8 Figure 4 Comparison between speed and block size in block-wise Jacobi iteration. using raw and synthetic captions during training is 1 : 9. Besides, following previous works (Lin et al., 2024), we additionally include 400M text-image pairs for joint training. To support video-to-video generation and editing, we additionally finetune the pretrained STARFlow-V on the Señorita (Zi et al., 2025), large-scale and high-quality instruction-based video editing dataset spanning 18 well-defined editing subcategories. Evaluation. We perform both quantitative and qualitative evaluations on STARFlow-V, and compare against baselines using VBench (Huang et al., 2024), which benchmarks text-to-video generation across 16 dimensions, including quality, semantics, temporal consistency, and spatial reasoning. Model and Training Details. We adopt the 3D Causal VAE from WAN2.2 (Wan et al., 2025), which compresses spatial dimensions by 16 and the temporal dimension by 4 into 48-channel latent space. We train progressively: we initialize from an image (single-frame) model, then scale to 7B-parameter video model by increasing the deep-block capacity. For resolution, we use curriculum from 384p to 480p while keeping the sequence length fixed at 81 frames. For the learnable denoiser, we used 8-layer Transformer with the same channel dimension as shallow block. We include more implementation details in Appendix. Baselines. We compare with three baselines: (i) WAN-2.1 Causal, the autoregressive variant of WAN (Wan et al., 2025) finetuned with the CausVid strategy (Yin et al., 2025); (ii) Self-Forcing (Huang et al., 2025), finetuned from WAN-2.1 Causal-FT to mitigate traintest mismatch; and (iii) NOVA(Deng et al., 2024), native autoregressive diffusion model that does not rely on vector quantization. The orginal model predicts in chunk-based fashion. For fair comparisons, we also execute results in the pure AR settings. Besides, we also report quantitative results on VBench with official scores."
        },
        {
            "title": "4.2 Quantitative Results",
            "content": "Table 1 reports T2V results on VBench (Huang et al., 2024). While STARFlow-V does not yet match the strongest diffusion-based video generators, it attains performance in the same range as recent causal diffusion baselines, substantially narrowing the historical gap between NFs and diffusion models for video. To the best of our knowledge, STARFlow-V is the first NF-based text-to-video model to reach this level of quality, indicating that NFs can be viable alternative when invertibility and exact likelihood (as shown in (Zhai et al., 2024)) are desired. We also include variant trained without local constraints; its VBench scores remain very close to the causal version, indicating that enforcing causal structure does not incur noticeable loss in perceptual quality."
        },
        {
            "title": "4.3 Qualitative Results\nT2V & I2V Tasks As illustrated in Figure 1, STARFlow-V naturally supports both T2V and I2V generation.\nThe examples show that STARFlow-V produces temporally smooth and visually faithful sequences in both\nsettings. Importantly, both T2V and I2V results are obtained from the same model without additional tuning:\nthanks to invertibility and causal modeling, the decoder can be reused as an encoder when a conditioning\nimage is provided.",
            "content": "V2V Tasks As shown in Figure 1, STARFlow-V handles diverse V2V tasks from object-level to dense prediction within single framework simply by changing the instruction. These results illustrate the potential of using our NF-based model for general video editing and reasoning. 9 Against Autoregressive Diffusion Models In Figure 3, we compare STARFlow-V with two representative autoregressive diffusion models. For the dog-with-sunglasses example, NOVA (Deng et al., 2024) exhibits gradual blurring and loss of identity, while WAN 2.1-Causal FT shows strong artifacts and color distortions. In contrast, STARFlow-V maintains clean, sharp, and temporally consistent frames, indicating stronger robustness to exposure bias. The right block of Figure 3 further shows that STARFlow-V sustains stable, coherent generations when extended to 30 secondswell beyond its 5-second training horizonwhere NOVA (Deng et al., 2024) and Self-Forcing (Huang et al., 2025) suffer from blurring, color drift, and structural deformation. We further report quantitative metrics for evaluating drifting effects across baselines and our model in the Appendix."
        },
        {
            "title": "4.4 Ablation Study\nChoice of Denoiser Figure 5 provides an ablation on the denoiser design. As shown in the top row,\nDecoder-finetuning (Gu et al., 2025) tends to lose temporal consistency with noticeable frame-to-frame jitter,\nwhile score-based denoising (Zhai et al., 2024) introduces bright speckle artifacts, especially in regions of large\nmotion. The quantitative comparison (bottom) further shows that our proposed flow–score matching achieves\nsubstantially better video reconstruction under latent-space noise injection, outperforming both alternatives\nby a clear margin.",
            "content": "Hyper-parameters of Block-wise Jacobi Iteration We analyze how the block size used in the block-wise Jacobi Iteration influences the runtime of the deep block. As shown in Figure 4 (left), the runtime initially decreases as the block size increases, reflecting better utilization of intra-block parallelism, but then rises slightly again when the block size becomes too large. This trend suggests trade-off: while larger block sizes increase parallelism, excessively large blocks requires more iterations within each block to achieve convergence. Method No noise PSNR SSIM 32. 0.8907 Decoder fine-tuning (Gu et al., 2025) Score-based denoising (Zhai et al., 2024) Flow-score matching (ours) 23.95 22.05 26.69 0.6403 0.6490 0.7601 We also examine the impact of video-aware initialization on runtime. As illustrated in Figure 4 (left), initializing the first Jacobi iteration of each frame using the converged state from the previous frame substantially reduces runtime across almost all block sizes except for small block sizes. This improvement likely stems from the strong temporal coherence present in natural videos, where neighboring frames provide effective warm starts that appear to facilitate faster iterative updates. Overall, video-aware initialization leads to observed improvements across block sizes. Figure 5 Ablation study for the choice of denoiser. We compare video VAE reconstruction quality across denoising approaches over 1, 000 random videos with large motions. We further analyze the runtime breakdown across latent frames in Figure 4 (right). Video-aware initialization yields the largest gains for large block sizes after the first frame, where convergence would otherwise require many more inner steps. Based on this observation, we adopt an asymmetric default strategy: use medium block size (e.g., 64) for the first frame, and larger block size (e.g., 512) for subsequent frames with videoaware initialization."
        },
        {
            "title": "5 Conclusion and Limitations",
            "content": "We presented STARFlow-V, an end-to-end video generative model based on autoregressive normalizing flows. As shown experimentally, STARFlow-V delivers strong long-horizon coherence and fine-grained controllability across text-to-video, image-to-video and video-to-video tasks, and shows consistent gains over autoregressive diffusion baselines at 480p/81f. As bonus, STARFlow-V can be used natively for likelihood estimation. While the results are encouraging, there are still limitations to overcome. (1) Latency. Despite the proposed accelerated sampling, inference remains far from real time on commodity GPUs. (2) Data quality and scaling. Progress is bounded by dataset noise and bias; we do not observe clean scaling law under current curation. 10 (3) Non-physical generation. Due to the current model scale and available data, we still observe many unrealistic, non-physical generations (see Figure 6), such as an octopus passing through the wall of jar and rock spontaneously appearing beneath goat just as it lands. Looking forward, we see several promising directions. First, we aim to reduce generation latency, for example through more efficient sampling schedules and architectural optimizations. Second, we plan to study distillation and pruning to obtain compact student models that retain most of the performance of the full system. Third, we will revisit dataset curation and active data selection, with particular focus on challenging, large-motion sequences and physically grounded scenarios; this is crucial for improving physical plausibility, reducing non-physical failure cases, and enabling clearer scaling behavior at higher fidelity. Figure 6 Failure cases of generation from STARFlow-V."
        },
        {
            "title": "References",
            "content": "Josh Achiam, Steven Adler, Sandhini Agarwal, Lama Ahmad, Ilge Akkaya, Florencia Leoni Aleman, Diogo Almeida, Janko Altenschmidt, Sam Altman, Shyamal Anadkat, et al. Gpt-4 technical report. arXiv preprint arXiv:2303.08774, 2023. Mohammad Babaeizadeh, Chelsea Finn, Dumitru Erhan, Roy H. Campbell, and Sergey Levine. Stochastic variational In International Conference on Learning Representations (ICLR), 2018. doi: 10.48550/arXiv. video prediction. 1710.11252. URL https://openreview.net/forum?id=rk49Mg-CW. Stephen Batifol, Andreas Blattmann, Frederic Boesel, Saksham Consul, Cyril Diagne, Tim Dockhorn, Jack English, Zion English, Patrick Esser, Sumith Kulal, et al. Flux. 1 kontext: Flow matching for in-context image generation and editing in latent space. arXiv e-prints, pages arXiv2506, 2025. Tim Brooks, Bill Peebles, Connor Holmes, Will DePue, Yufei Guo, Li Jing, David Schnurr, Joe Taylor, Troy Luhman, Eric Luhman, Clarence Ng, Ricky Wang, and Aditya Ramesh. Video generation models as world simulators. 2024. URL https://openai.com/research/video-generation-models-as-world-simulators. Lluis Castrejon, Nicolas Ballas, and Aaron Courville. Improved conditional vrnns for video prediction. In Proceedings of the IEEE/CVF international conference on computer vision, pages 76087617, 2019. Boyuan Chen, Diego Martí Monsó, Yilun Du, Max Simchowitz, Russ Tedrake, and Vincent Sitzmann. Diffusion forcing: Next-token prediction meets full-sequence diffusion. Advances in Neural Information Processing Systems, 37:2408124125, 2024a. Guibin Chen, Dixuan Lin, Jiangping Yang, Chunze Lin, Junchen Zhu, Mingyuan Fan, Hao Zhang, Sheng Chen, Zheng Chen, Chengcheng Ma, et al. Skyreels-v2: Infinite-length film generative model. arXiv preprint arXiv:2504.13074, 2025. Tsai-Shien Chen, Aliaksandr Siarohin, Willi Menapace, Ekaterina Deyneka, Hsiang-wei Chao, Byung Eun Jeon, Yuwei Fang, Hsin-Ying Lee, Jian Ren, Ming-Hsuan Yang, et al. Panda-70m: Captioning 70m videos with multiple crossIn Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, modality teachers. pages 1332013331, 2024b. Google DeepMind. Veo 3: Ai video generator with audio. https://deepmind.google/models/veo/, 2025. Accessed: 2025-08-25. Haoge Deng, Ting Pan, Haiwen Diao, Zhengxiong Luo, Yufeng Cui, Huchuan Lu, Shiguang Shan, Yonggang Qi, and Xinlong Wang. Autoregressive video generation without vector quantization. arXiv preprint arXiv:2412.14169, 2024. Laurent Dinh, David Krueger, and Yoshua Bengio. Nice: Non-linear independent components estimation. arXiv preprint arXiv:1410.8516, 2014. Laurent Dinh, Jascha Sohl-Dickstein, and Samy Bengio. Density estimation using real nvp. arXiv preprint arXiv:1605.08803, 2016. Patrick Esser, Sumith Kulal, Andreas Blattmann, Rahim Entezari, Jonas Müller, Harry Saini, Yam Levi, Dominik Lorenz, Axel Sauer, Frederic Boesel, et al. Scaling rectified flow transformers for high-resolution image synthesis. In Forty-first International Conference on Machine Learning, 2024. Yu Gao, Haoyuan Guo, Tuyen Hoang, Weilin Huang, Lu Jiang, Fangyuan Kong, Huixia Li, Jiashi Li, Liang Li, Xiaojie Li, et al. Seedance 1.0: Exploring the boundaries of video generation models. arXiv preprint arXiv:2506.09113, 2025. Anastasis Germanidis. Introducing gen-3 alpha: new frontier for video generation, 2024. Google DeepMind. Genie 2: large-scale foundation world model. https://deepmind.google/discover/blog/ genie-2-a-large-scale-foundation-world-model/, 2024. Blog. Google DeepMind. Veo 3 Technical Report. https://storage.googleapis.com/deepmind-media/veo/Veo-3-Tech-Report.pdf, 2025. Accessed: Sep 24, 2025. Jiatao Gu, Tianrong Chen, David Berthelot, Huangjie Zheng, Yuyang Wang, Ruixiang Zhang, Laurent Dinh, Miguel Angel Bautista, Josh Susskind, and Shuangfei Zhai. Starflow: Scaling latent normalizing flows for highresolution image synthesis. arXiv preprint arXiv:2506.06276, 2025. David Ha and Jurgen Schmidhuber. World models. NeurIPS, 2018. doi: 10.1007/bfb0007224. Danijar Hafner et al. Mastering diverse control tasks through world models. Nature, 2025. Also available as arXiv:2301.04104 (DreamerV3). Jonathan Ho, Xi Chen, Aravind Srinivas, Yan Duan, and Pieter Abbeel. Flow++: Improving flow-based generative models with variational dequantization and architecture design. In International conference on machine learning, pages 27222730. PMLR, 2019. Jonathan Ho, Ajay Jain, and Pieter Abbeel. Denoising diffusion probabilistic models. Advances in Neural Information Processing Systems, 33:68406851, 2020. Jonathan Ho, William Chan, Chitwan Saharia, Jay Whang, Ruiqi Gao, Alexey Gritsenko, Diederik Kingma, Ben Poole, Mohammad Norouzi, David Fleet, et al. Imagen video: High definition video generation with diffusion models. arXiv preprint arXiv:2210.02303, 2022a. Jonathan Ho, Chitwan Saharia, William Chan, David Fleet, Mohammad Norouzi, and Tim Salimans. Cascaded diffusion models for high fidelity image generation. J. Mach. Learn. Res., 23:471, 2022b. Jonathan Ho, Tim Salimans, Alexey Gritsenko, William Chan, Mohammad Norouzi, and David Fleet. Video diffusion models. Advances in neural information processing systems, 35:86338646, 2022c. Wenyi Hong, Ming Ding, Wendi Zheng, Xinghan Liu, and Jie Tang. Cogvideo: Large-scale pretraining for text-to-video generation via transformers. arXiv preprint arXiv:2205.15868, 2022. Anthony Hu, Lloyd Russell, Hudson Yeo, Zak Murez, George Fedoseev, Alex Kendall, Jamie Shotton, and Gianluca Corrado. Gaia-1: generative world model for autonomous driving. arXiv preprint arXiv:2309.17080, 2023. Xun Huang, Zhengqi Li, Guande He, Mingyuan Zhou, and Eli Shechtman. Self forcing: Bridging the train-test gap in autoregressive video diffusion. arXiv preprint arXiv:2506.08009, 2025. Yanping Huang, Youlong Cheng, Ankur Bapna, Orhan Firat, Dehao Chen, Mia Chen, HyoukJoong Lee, Jiquan Ngiam, Quoc Le, Yonghui Wu, et al. Gpipe: Efficient training of giant neural networks using pipeline parallelism. Advances in neural information processing systems, 32, 2019. Ziqi Huang, Yinan He, Jiashuo Yu, Fan Zhang, Chenyang Si, Yuming Jiang, Yuanhan Zhang, Tianxing Wu, Qingyang Jin, Nattapol Chanpaisit, et al. Vbench: Comprehensive benchmark suite for video generative models. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 2180721818, 2024. Carl Kelley. Iterative methods for linear and nonlinear equations. SIAM, 1995. Durk Kingma and Prafulla Dhariwal. Glow: Generative flow with invertible 1x1 convolutions. Advances in neural information processing systems, 31, 2018. Durk Kingma, Tim Salimans, Rafal Jozefowicz, Xi Chen, Ilya Sutskever, and Max Welling. Improved variational inference with inverse autoregressive flow. Advances in neural information processing systems, 29, 2016. 12 Dan Kondratyuk, Lijun Yu, Xiuye Gu, Jose Lezama, Jonathan Huang, Grant Schindler, Rachel Hornung, Vighnesh Birodkar, Jimmy Yan, Ming-Chang Chiu, et al. Videopoet: large language model for zero-shot video generation. In International Conference on Machine Learning, pages 2510525124. PMLR, 2024. Weijie Kong, Qi Tian, Zijian Zhang, Rox Min, Zuozhuo Dai, Jin Zhou, Jiangfeng Xiong, Xin Li, Bo Wu, Jianwei Zhang, et al. Hunyuanvideo: systematic framework for large video generative models. arXiv preprint arXiv:2412.03603, 2024. Manoj Kumar, Mohammad Babaeizadeh, Dumitru Erhan, Chelsea Finn, Sergey Levine, Laurent Dinh, and arXiv preprint Durk Kingma. Videoflow: conditional flow-based model for stochastic video generation. arXiv:1903.01434, 2019. Tianhong Li, Yonglong Tian, He Li, Mingyang Deng, and Kaiming He. Autoregressive image generation without vector quantization. arXiv preprint arXiv:2406.11838, 2024. Bin Lin, Yunyang Ge, Xinhua Cheng, Zongjian Li, Bin Zhu, Shaodong Wang, Xianyi He, Yang Ye, Shenghai Yuan, Liuhan Chen, et al. Open-sora plan: Open-source large video generation model. arXiv preprint arXiv:2412.00131, 2024. Yaron Lipman, Ricky T. Q. Chen, Heli Ben-Hamu, Maximilian Nickel, and Matthew Le. Flow matching for generative modeling. In The Eleventh International Conference on Learning Representations, 2023. URL https://openreview.net/ forum?id=PqvMRDCJT9t. Ben Liu and Zhen Qin. Accelerate tarflow sampling with gs-jacobi iteration. arXiv preprint arXiv:2505.12849, 2025. OpenAI. Gpt-4o system card. https://openai.com/index/gpt-4o-system-card/, 2024a. Accessed: April 12, 2025. OpenAI. Video generation models as world simulators. https://openai.com/index/video-generation-models-as-world-simulators/, 2024b. George Papamakarios, Iain Murray, and Theo Pavlakou. Masked autoregressive flow for density estimation. In Isabelle Guyon, Ulrike von Luxburg, Samy Bengio, Hanna M. Wallach, Rob Fergus, S. V. N. Vishwanathan, and Roman Garnett, editors, Advances in Neural Information Processing Systems 30: Annual Conference on Neural Information Processing Systems 2017, December 4-9, 2017, Long Beach, CA, USA, pages 23382347, 2017. URL https://proceedings.neurips.cc/paper/2017/hash/6c1da886822c67822bcf3679d04369fa-Abstract.html. William Peebles and Saining Xie. Scalable diffusion models with transformers. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 41954205, 2023. Dustin Podell, Zion English, Kyle Lacey, Andreas Blattmann, Tim Dockhorn, Jonas Müller, Joe Penna, and improving latent diffusion models for high-resolution image synthesis. arXiv preprint Robin Rombach. Sdxl: arXiv:2307.01952, 2023. TA Porsching. Jacobi and gaussseidel methods for nonlinear network problems. SIAM Journal on Numerical Analysis, 6(3):437449, 1969. Danilo Rezende and Shakir Mohamed. Variational inference with normalizing flows. In Francis Bach and David Blei, editors, Proceedings of the 32nd International Conference on Machine Learning, volume 37 of Proceedings of Machine Learning Research, pages 15301538, Lille, France, 0709 Jul 2015. PMLR. URL https://proceedings.mlr.press/ v37/rezende15.html. Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Björn Ommer. High-resolution image synthesis with latent diffusion models. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 1068410695, 2022. Yousef Saad. Iterative methods for sparse linear systems. SIAM, 2003. Team Seawead, Ceyuan Yang, Zhijie Lin, Yang Zhao, Shanchuan Lin, Zhibei Ma, Haoyuan Guo, Hao Chen, Lu Qi, Sen Wang, et al. Seaweed-7b: Cost-effective training of video generation foundation model. arXiv preprint arXiv:2504.08685, 2025. Ivan Skorokhodov, Sergey Tulyakov, and Mohamed Elhoseiny. Stylegan-v: continuous video generator with the price, image quality and perks of stylegan2. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pages 36263636, 2022. Kiwhan Song, Boyuan Chen, Max Simchowitz, Yilun Du, Russ Tedrake, and Vincent Sitzmann. History-guided video diffusion. arXiv preprint arXiv:2502.06764, 2025. 13 Yang Song, Chenlin Meng, Renjie Liao, and Stefano Ermon. Accelerating feedforward computation via parallel nonlinear equation solving. In International Conference on Machine Learning, pages 97919800. PMLR, 2021. Hansi Teng, Hongyu Jia, Lei Sun, Lingzhi Li, Maolin Li, Mingqiu Tang, Shuai Han, Tianning Zhang, WQ Zhang, Weifeng Luo, et al. Magi-1: Autoregressive video generation at scale. arXiv preprint arXiv:2505.13211, 2025. Yao Teng, Han Shi, Xian Liu, Xuefei Ning, Guohao Dai, Yu Wang, Zhenguo Li, and Xihui Liu. Accelerating autoregressive text-to-image generation with training-free speculative jacobi decoding. arXiv preprint arXiv:2410.01699, 2024. Sergey Tulyakov, Ming-Yu Liu, Xiaodong Yang, and Jan Kautz. Mocogan: Decomposing motion and content for video generation. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR), pages 15261535, June 2018. Carl Vondrick, Hamed Pirsiavash, and Antonio Torralba. Generating videos with scene dynamics. In Advances in Neural Information Processing Systems, volume 29, 2016. doi: 10.48550/arXiv.1609.02612. URL https://papers.nips. cc/paper_files/paper/2016/hash/04025959b191f8f9de3f924f0940515f-Abstract.html. Team Wan, Ang Wang, Baole Ai, Bin Wen, Chaojie Mao, Chen-Wei Xie, Di Chen, Feiwu Yu, Haiming Zhao, Jianxiao Yang, et al. Wan: Open and advanced large-scale video generative models. arXiv preprint arXiv:2503.20314, 2025. Jiawei Wang, Liping Yuan, Yuchen Zhang, and Haomiao Sun. Tarsier: Recipes for training and evaluating large video description models. arXiv preprint arXiv:2407.00634, 2024a. Xinlong Wang, Xiaosong Zhang, Zhengxiong Luo, Quan Sun, Yufeng Cui, Jinsheng Wang, Fan Zhang, Yueze Wang, Zhen Li, Qiying Yu, et al. Emu3: Next-token prediction is all you need. arXiv preprint arXiv:2409.18869, 2024b. Bohan Wu, Suraj Nair, Roberto Martín-Martín, Li Fei-Fei, and Chelsea Finn. Greedy hierarchical variational autoencoders for large-scale video prediction. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pages 23182328, June 2021. Chenfei Wu, Jiahao Li, Jingren Zhou, Junyang Lin, Kaiyuan Gao, Kun Yan, Sheng-ming Yin, Shuai Bai, Xiao Xu, Yilei Chen, et al. Qwen-image technical report. arXiv preprint arXiv:2508.02324, 2025. Wilson Yan, Yunzhi Zhang, Pieter Abbeel, and Aravind Srinivas. Videogpt: Video generation using vq-vae and transformers. arXiv preprint arXiv:2104.10157, 2021. doi: 10.48550/arXiv.2104.10157. Sherry Yang, Yilun Du, Seyed Kamyar Seyed Ghasemipour, Jonathan Tompson, Dale Schuurmans, and Pieter Abbeel. Learning interactive real-world simulators. In NeurIPS 2023 Workshop on Generalization in Planning, 2023. Zhuoyi Yang, Jiayan Teng, Wendi Zheng, Ming Ding, Shiyu Huang, Jiazheng Xu, Yuanming Yang, Wenyi Hong, Xiaohan Zhang, Guanyu Feng, et al. Cogvideox: Text-to-video diffusion models with an expert transformer. arXiv preprint arXiv:2408.06072, 2024. Zhuoyi Yang, Jiayan Teng, Wendi Zheng, Ming Ding, Shiyu Huang, Jiazheng Xu, Yuanming Yang, Wenyi Hong, Xiaohan Zhang, Guanyu Feng, et al. Cogvideox: Text-to-video diffusion models with an expert transformer. In The Thirteenth International Conference on Learning Representations, 2025. Zixuan Ye, Huijuan Huang, Xintao Wang, Pengfei Wan, Di Zhang, and Wenhan Luo. Stylemaster: Stylize your video with artistic generation and translation. In Proceedings of the Computer Vision and Pattern Recognition Conference, pages 26302640, 2025. Tianwei Yin, Qiang Zhang, Richard Zhang, William Freeman, Fredo Durand, Eli Shechtman, and Xun Huang. From slow bidirectional to fast autoregressive video diffusion models. In Proceedings of the Computer Vision and Pattern Recognition Conference, pages 2296322974, 2025. Lijun Yu, José Lezama, Nitesh Bharadwaj Gundavarapu, Luca Versari, Kihyuk Sohn, David Minnen, Yong Cheng, Agrim Gupta, Xiuye Gu, Alexander Hauptmann, et al. Language model beats diffusion-tokenizer is key to visual generation. In ICLR, 2024. Shenghai Yuan, Jinfa Huang, Xianyi He, Yunyang Ge, Yujun Shi, Liuhan Chen, Jiebo Luo, and Li Yuan. Identitypreserving text-to-video generation by frequency decomposition. In Proceedings of the Computer Vision and Pattern Recognition Conference, pages 1297812988, 2025. Shuangfei Zhai, Ruixiang ZHANG, Preetum Nakkiran, David Berthelot, Jiatao Gu, Huangjie Zheng, Tianrong Chen, Miguel Ángel Bautista, Navdeep Jaitly, and Joshua Susskind. Normalizing flows are capable generative models. In Forty-second International Conference on Machine Learning. Shuangfei Zhai, Ruixiang Zhang, Preetum Nakkiran, David Berthelot, Jiatao Gu, Huangjie Zheng, Tianrong Chen, Miguel Angel Bautista, Navdeep Jaitly, and Josh Susskind. Normalizing flows are capable generative models. arXiv preprint arXiv:2412.06329, 2024. Jiaru Zhang, Juanwu Lu, Ziran Wang, and Ruqi Zhang. Inference acceleration of autoregressive normalizing flows by selective jacobi decoding. arXiv preprint arXiv:2505.24791, 2025. Guangting Zheng, Qinyu Zhao, Tao Yang, Fei Xiao, Zhijie Lin, Jie Wu, Jiajun Deng, Yanyong Zhang, and Rui Zhu. Farmer: Flow autoregressive transformer over pixels. arXiv preprint arXiv:2510.23588, 2025. Zangwei Zheng, Xiangyu Peng, Tianji Yang, Chenhui Shen, Shenggui Li, Hongxin Liu, Yukun Zhou, Tianyi Li, and Yang You. Open-sora: Democratizing efficient video production for all. arXiv preprint arXiv:2412.20404, 2024. Bojia Zi, Penghui Ruan, Marco Chen, Xianbiao Qi, Shaozhe Hao, Shihao Zhao, Youze Huang, Bin Liang, Rong Xiao, and Kam-Fai Wong. Se norita-2m: high-quality instruction-based dataset for general video editing by video specialists. arXiv preprint arXiv:2502.06734, 2025."
        },
        {
            "title": "A Derivations and Algorithms",
            "content": "A.1 Derivation of STARFlow-V. (1) Why an autoregressive Gaussian model in is normalizing flow. Let Tθ : (cid:55) be the triangular autoregressive map applied by the deep block fD (within frame and across frames in the global order). For token index in that order, zi = ui µθ(u<i) σθ(u<i) , σθ() > 0, (A.1) with inverse (A.2) ui = σθ(u<i) zi + µθ(u<i). Because each zi depends only on (u1, . . . , ui) and σθ > 0, Tθ is bijective and continuously differentiable. The Jacobian is lower triangular with diagonal entries zi/ui = 1/σθ(u<i), thus log(cid:12) (A.3) (cid:88) log σθ(u<i). (cid:12)det JTθ (u)(cid:12) (cid:12) = With standard normal prior p0(z) = (cid:81) log pD(u) = log p0 (cid:0)Tθ(u)(cid:1) + log(cid:12) (zi; 0, I), (cid:12)det JTθ (u)(cid:12) (cid:12) = 1 2 z2 (cid:88) (cid:88) log σθ(u<i) + const, (A.4) which is essentially the regression objective through maximum likelihood estimation over u. Therefore, the deep block realizes valid normalizing flow. Composing with the shallow block gives fθ = fD fS and yields the data density in Equation (3.1). (2) How we get the autoregressive distribution. From the globallocal factorization (Equation (3.2)), pθ(x) = (cid:89) n=1 pD(un u<n) (cid:12) (cid:12)det JfS (xn)(cid:12) (cid:12), un = fS(xn). (A.5) Within frame n, index tokens = 1, . . . , HW in raster (or block) order and we have Equation (A.4) which models pD as Gaussian. The shallow-block contributes the additional logdet (cid:80) log det JfS (xn), forming an expressive distribution. (3) Noise & denoising: what the model looks like. Following the noise-augmented training (3.2), let = + σϵ, ϵ (0, I). The Tweedie single-step denoiser in the flow setting (Equation (3.3)) suggests the update + σ2 log pθ( x). To avoid high-frequency artifacts and to preserve streamability, we fit causal denoiser sϕ via flow-score matching (Equation (3.4)) and then use ˆx = + σ sϕ( x) + σ2 log pθ( x), (A.6) where sϕ uses block-causal mask with at most one-frame look-ahead to retain strict streamability. 15 Algorithm 1 Training STARFlow-V with noise augmentation and flow-score matching Require: video dataset D; noise level σ; FSM weight λden 1: repeat 2: Sample mini-batch and noise ϵ (0, I) Noise-augment: + σ ϵ Shallow forward: fS( x) Deep forward: fD(u) Standard NF NLL: LNLL(θ) (cid:2) log p0(z) + log det JfD (u) det JfS ( x)(cid:3) Score target (stop-grad): σ log pθ( x) Flow-score Matching: LFSM(ϕ) sϕ( x) 2 2 Total loss: LNLL(θ) + λden LFSM(ϕ) Update: (θ, ϕ) (θ, ϕ) η 3: 4: 5: 6: 7: 8: 9: 10: 11: until convergence as in 3.2 alternating masked AF blocks, within-frame causal Transformer AF over global order reuse backward pass of LNLL; detach Algorithm 2 Autoregressive sampling (z x) Require: length (frames or tokens), base prior p0(z) = (0, I), shallow inverse 1 token order , deep inverse 1 , 1: Sample (0, I) with the target shape 2: Initialize an empty latent sequence 3: for each element in global order do (cid:1) (cid:0)u<i Compute (µi, σi): (µi, σi) fD 4: Invert deep at position i: ui σi zi + µi 5: 6: end for 7: Invert shallow block: 1 8: (One-step corrector) + σtest sϕ(x) 9: return (u) causal AR over frames and within-frame tokens 1 , triangular A.2 Training Algorithm 1 shows the training algorithm of STARFlow-V for both the flow and the learnable denoiser. A.3 Inference (i) When the deep map is sufficiently contractive in (e.g., via scale clamping), the Jacobi iteration Remarks. converges rapidly and enables wide parallelism within each block B. (ii) common choice for is to use spatial tiles per frame (no intra-tile dependencies) or even/odd raster groups, preserving the block-causal mask used in training."
        },
        {
            "title": "B Implementation Details",
            "content": "B.1 Architecture Design Params fD width fS Denoiser sϕ Init 3B 3B 3072 7B 7B 4096 identical (alt. masked AF; width dS, depth LS) 8-layer Transformer, block-causal mask from scratch finetune from 3B Table 2 Minimal comparison. Only fD width differs; fS and sϕ are unchanged. 3B. Same size as STARFlow but for video. The deep block fD uses width 3072 (depth LD, heads HD). The 16 Algorithm 3 Jacobi-style parallel inversion of the deep autoregressive block Require: base latent z; initial guess u(0) (e.g., zeros); block partition = {B1, . . . , BJ } (non-overlapping, block-causal, Bj = 4B1 for all block > 1); max iters ; Frame size ; tolerance τ Initialize ua:b uaF :bF initialization from past frame end if repeat indices of the j-th block random initialization 1: for = 1, 2, . . . , do [a, b] Bj 2: if = 1 and > then 3: Initialize ua:b u(0) a:b else 4: 5: 6: 7: 8: 9: 10: 11: 12: 13: 14: + 1 for all Bj in parallel do ) fD (µ(t) , σ(t) σ(t) u(t+1) end for (cid:0)u(t) (cid:1) zi + µ(t) <i until u(t+1)u(t)2 ua:b u(t) a:b u(t)2+ε τ or = 15: 16: end for 17: Shallow inverse: 1 18: (One-step corrector) + σtest sϕ(x) 19: return (cid:0)u(t+1)(cid:1) shallow stack fS (alternating masked affine flows) and the denoiser sϕ (8-layer Transformer with block-causal mask) follow the standard design. 7B. Initialized from the 3B checkpoint and only widens the deep block fD channels from 3072 to 4096. The shallow stack fS and denoiser sϕ remain identical (same depths, heads, and widths). B.2 Training Details STARFlow-V is trained on 96 H100 GPUs using approximately 20 million videos. In all the experiments, we share the following training configuration for our proposed STARFlow-V. training config: batch_size=96 optimizer=AdamW adam_beta1=0.9 adam_beta2=0.95 adam_eps=1e-8 learning_rate=5e-5 min_learning_rate=1e-6 learning_rate_schedule=cosine weight_decay=1e-4 mixed_precision_training=bf16 Progressive Video Training We adopt progressive multi-stage training paradigm that gradually increases model size, resolution, and temporal horizon for stable and effective optimization. 3B Text-to-Image Training: We initialize 3B text-to-image model from the pretrained StarFlow (Gu et al., 2025), establishing strong visualtextual backbone before introducing temporal modeling. 3B Image-Video Joint Training (384P, 45 frames): The 3B model is then jointly trained on low-resolution images and videos at 384P. Each training clip contains 45 frames sampled at 16 fps, enabling the model to acquire short-term temporal dynamics. 17 Algorithm 4 Streaming long-sequence generation via re-encode with forward Require: target length (frames), window size (W ); deep inverse 1 ; shallow inverse 1 ; shallow forward fS; deep forward fD; prior p0(z) 1: Initialize caches KV , latent buffer 2: for = 1 to do 3: Sample base: zt (0, I) for the next frame (or token block) Deep inverse: using cached state, compute ut 1 Shallow inverse: xt 1 Emit xt Re-encode (forward): ˆut fS(xt) Update deep state: run fD forward on ˆut to refresh KV (no sampling): _ fD( ˆut; KV) 4: 5: 6: 7: 8: 9: Maintain sliding window: push ˆut into buffer U; if > pop the oldest 10: end for 11: return {xt}T (zt ; KV) and update the KV cache. (ut) t=1 brings the produced frame back to -space 7B Image-Video Joint Training (384P, 81 frames): We expand the model to 7B parameters and continue joint training at 384P, doubling the temporal horizon from 45 to 81 frames to strengthen long-range temporal reasoning. 7B Image-Video Joint Training (480P, 81 frames): Finally, we train the 7B model on higherresolution 480P images and videos while maintaining the 81-frame temporal window. Mixed-Resolution Training STARFlow-V is designed to support mixed-resolution inputs, allowing each frame to retain its native aspect ratio and spatial resolution. Similar to Gu et al. (2025), we assign each video sequence to one of nine predefined aspect-ratio bins, since all frames within video share the same ratio. The pre-defined bins are 21:9, 16:9, 3:2, 5:4, 1:1, 4:5, 2:3, 9:16, and 9:21. To make the model explicitly aware of these visual formats, we incorporate both the fps and aspect-ratio tag into the text caption: video with {fps} fps: {original_caption} in {aspect_ratio} aspect ratio. Gradient Control We monitor the gradient norm throughout training to ensure stability. Specifically, to prevent gradient explosion, we enable gradient skipping after the first 100 steps: if the gradient norm exceeds threshold of 1, the update for that step is skipped. This adaptive strategy stabilizes early training while maintaining convergence efficiency later on. B.3 Baseline Details WAN-2.1 Causal-FT is the autoregressive variant of WAN (Wan et al., 2025). Specifically, we adopt Wan2.1T2V-1.3B, Flow Matchingbased model, as the base model. Following the CausVid initialization strategy (Yin et al., 2025), the base model is fine-tuned with causal attention masking on 16k ODE solution pairs generated from the model itself. In practice, we leverage the ODE initialization checkpoint released with the official Self-Forcing (Huang et al., 2025) repository, which corresponds exactly to the configuration of our WAN-2.1 Causal-FT setup. is an autoregressive video generator that does not rely on vector quantization. NOVA AR (Deng et al., 2024) It reformulates video generation as non-quantized autoregressive modeling that performs temporal frame-byframe prediction while generating spatial token sets within each frame in flexible, set-by-set manner. To support autoregressive modeling with continuous tokens, NOVA leverages lightweight diffusion head that models the distribution of each continuous token (Li et al., 2024). In this work, we directly compare the pure AR version of NOVA, where the model predicts each latent frame with diffusion for fair comparison. 18 Model Total Quality Semantic Aesthetic Object Human Spatial Scene Autoregressive (Diffusion) models NOVA AR (Deng et al., 2024) WAN 2.1-Causal FT 75.31 74.96 77.46 77.41 66.70 65. 56.04 56.04 79.68 76.51 94.20 94.20 66.07 53.25 47.83 47.83 Normalizing Flows STARFlow-V (Ours) 79.70 80.76 75.43 59.73 80.61 98. 76.08 48.21 Table 3 Performance comparison of autoregressive video generation models on VBench (Huang et al., 2024). Following Yang et al. (2025), we evaluate with the official GPT-augmented prompts (noted as ) Figure 7 Generated samples from STARFlow-V given text prompts. All videos are at 480p 16fps and 5s."
        },
        {
            "title": "C Additional Experimental Details and Results",
            "content": "C.1 Quantitative Comparison with Autoregressive Diffusion baselines To evaluate the robustness of video generation under autoregressive generation, we compare STARFlow-V with autoregressive diffusion models, including NOVA AR (Deng et al., 2024) and WAN 2.1-Causal FT. Here, NOVA AR refers to the fully autoregressive video generation variant which is different from the reported in the official paper. Table 3 compares these models across diverse set of evaluation dimensions defined in VBench (Huang et al., 2024). As shown in Table 3, STARFlow-V substantially outperforms the autoregressive diffusion baselines across all dimensions. Both NOVA AR and WAN 2.1-Causal FT exhibit clear signs of autoregressive degradation in their generated videos. Specifically, NOVA AR suffers from pronounced error accumulation, leading to increasing blur and content collapse as the video progresses. And WAN 2.1-Causal FT produces noticeable temporal inconsistency and flickering throughout the video. These failure modes are reflected in their lower scores, underscoring the difficulty of maintaining robustness in autoregressive video generation. And it further highlights the strength of our approach. 19 Figure 8 Generated samples from STARFlow-V given the first frame. All videos are at 480p 16fps and 5s. C.2 Video-to-Video Generation To support video-to-video generation and editing, we additionally finetune the pretrained STARFlow-V (7B, 384P, 81 frames) on the Señorita (Zi et al., 2025), large-scale and high-quality instruction-based video editing dataset spanning 18 well-defined editing subcategories. Each training sample in Señorita consists of 33-frame input video paired with 33-frame edited target video. The model is also trained on videos with 16fps. This finetuning stage equips STARFlow-V with precise editing capabilities while preserving temporal coherence and motion consistency. During finetuning, we concatenate the input and target videos along the temporal dimension to form single training sequence. C.3 Additional Samples We show additional samples at Figures 7 to 9. Besides, we provide more video generation comparison in our official codebase at https://github.com/apple/ml-starflow. Apple and the Apple logo are trademarks of Apple Inc., registered in the U.S. and other countries and regions. 20 Figure 9 Generated samples from STARFlow-V given text prompts and extended with overlapping frames. For each segment, we generate 21 latent frames with 4 latent frames in overlap. Both videos are at 480p 16fps."
        }
    ],
    "affiliations": [
        "Apple"
    ]
}