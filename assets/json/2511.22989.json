{
    "paper_title": "MultiBanana: A Challenging Benchmark for Multi-Reference Text-to-Image Generation",
    "authors": [
        "Yuta Oshima",
        "Daiki Miyake",
        "Kohsei Matsutani",
        "Yusuke Iwasawa",
        "Masahiro Suzuki",
        "Yutaka Matsuo",
        "Hiroki Furuta"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Recent text-to-image generation models have acquired the ability of multi-reference generation and editing; the ability to inherit the appearance of subjects from multiple reference images and re-render them under new contexts. However, the existing benchmark datasets often focus on the generation with single or a few reference images, which prevents us from measuring the progress on how model performance advances or pointing out their weaknesses, under different multi-reference conditions. In addition, their task definitions are still vague, typically limited to axes such as \"what to edit\" or \"how many references are given\", and therefore fail to capture the intrinsic difficulty of multi-reference settings. To address this gap, we introduce $\\textbf{MultiBanana}$, which is carefully designed to assesses the edge of model capabilities by widely covering multi-reference-specific problems at scale: (1) varying the number of references, (2) domain mismatch among references (e.g., photo vs. anime), (3) scale mismatch between reference and target scenes, (4) references containing rare concepts (e.g., a red banana), and (5) multilingual textual references for rendering. Our analysis among a variety of text-to-image models reveals their superior performances, typical failure modes, and areas for improvement. MultiBanana will be released as an open benchmark to push the boundaries and establish a standardized basis for fair comparison in multi-reference image generation. Our data and code are available at https://github.com/matsuolab/multibanana ."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 8 2 ] . [ 1 9 8 9 2 2 . 1 1 5 2 : r MultiBanana: Challenging Benchmark for Multi-Reference Text-to-Image Generation Yuta Oshima1, Daiki Miyake1, Kohsei Matsutani1 Yusuke Iwasawa1 Masahiro Suzuki1 Yutaka Matsuo1 Hiroki Furuta2, 1The University of Tokyo 2Google DeepMind {yuta.oshima, daiki.miyake}@weblab.t.u-tokyo.ac.jp Figure 1. The overview of MultiBanana. MultiBanana widely covers multi-reference specific problems, varying the number of references (the top row), domain and scale mismatch among references (two on the left in the middle row), multilingual text rendering (center in the bottom row), and containing rare concepts (right in the bottom row)."
        },
        {
            "title": "Abstract",
            "content": "Recent text-to-image generation models have acquired the ability of multi-reference generation and editing; the ability to inherit the appearance of subjects from multiple reference images and re-render them under new contexts. How- *Equal contribution Work done as an advisory role only. ever, the existing benchmark datasets often focus on the generation with single or few reference images, which prevents us from measuring the progress on how model performance advances or pointing out their weaknesses, under different multi-reference conditions. In addition, their task definitions are still vague, typically limited to axes such as what to edit or how many references are given, and therefore fail to capture the intrinsic difficulty of multi1 reference settings. To address this gap, we introduce MultiBanana, which is carefully designed to assesses the edge of model capabilities by widely covering multi-referencespecific problems at scale: (1) varying the number of references, (2) domain mismatch among references (e.g., photo vs. anime), (3) scale mismatch between reference and target scenes, (4) references containing rare concepts (e.g., red banana), and (5) multilingual textual references for rendering. Our analysis among variety of text-to-image models reveals their superior performances, typical failure modes, and areas for improvement. MultiBanana will be released as an open benchmark to push the boundaries and establish standardized basis for fair comparison in multi-reference image generation. Our data and code are available at https://github.com/matsuolab/ multibanana. 1. Introduction Recent advances in image generation, driven by large-scale multimodal LLM backbones, have enabled unprecedented levels of instruction-following [5, 8, 14, 34, 5254, 5658]. Among these developments, multi-reference image generation has emerged as particularly notable capability. When user provides multiple reference images, the model can inherit the subjects appearance and re-render it in new context while faithfully preserving identity. This shift moves image generation beyond single-reference conditioning toward greater controllability, driving increasing demand in industrial domains such as content production [43, 60], advertising [16, 30], and fashion design [7, 10, 65], where reference-guided generation is essential. However, despite this progress, we lack benchmark that systematically evaluates how model performance varies under different multi-reference conditions. As shown in Table 1, existing benchmarks [1, 17, 27, 43, 46, 51, 53, 56, 61 63] suffer from two fundamental limitations. First, they have an extremely narrow coverage due to the small number of unique reference images, making it impossible to assess the impact of varying reference conditions in both singleand multi-reference tasks. Second, their task definitions still lack details, typically limited to axes such aswhat to edit or how many references are given, and therefore fail to capture the intrinsic difficulty of multi-reference settings. Moreover, existing benchmarks limit the number of references to narrow range (typically 15) and do not examine how performance changes with more references. To address these limitations, we introduce new evaluation framework grounded not only in task categories but also in the intrinsic properties of the reference set itself. Our benchmark incorporates multiple orthogonal axes specific to the multi-reference regime, including broader coverage of the number of references (up to 8), domain gaps between references (e.g., photoanime mixtures), mismatches in object scale between reference and target scenes, rare concepts in the references (e.g., red banana), and multilingual textual rendering references (English, Chinese, and Japanese). Using this benchmark, we conduct cross-conditional analysis to assess model capabilities, identify characteristic failure modes, and point out remaining gaps. For example, while performance degradation due to increasing reference counts can be mitigated by agent-style multi-stage generation pipelines, failures arising from unsupported styles or languages remain difficult to remedy with inference-time strategies. We will open-source MultiBanana to establish standard foundation for fair comparison and to accelerate progress in multi-reference image generation. 2. Related Works 2.1. Controllable Text-to-Image Generation Diffusion models have achieved state-of-the-art performance in high-fidelity image synthesis. Recent large pretrained diffusion models with textual conditioning, such as Stable Diffusion [9, 38, 42] and FLUX [22, 23], enable the synthesis of novel scene images while preserving the identity depicted in the reference images. Building on these developments, approaches based on fine-tuning (e.g., DreamBooth [43]) and adapter-based methods (e.g., IP-Adapter [60]) have been actively explored to address the controllability [2, 12, 20, 25, 29, 47]. Additionally, training-free approaches (e.g., prompt-to-prompt [15]) have also been widely proposed [4, 18, 21, 28, 37, 49, 50]. Recent research on unified image generation has gained momentum, aiming to handle multiple image generation tasks within single model. For example, ChatGPT-Image-1 [34] and Nano Banana [14] can jointly process text and image inputs, enabling integrated image generation and editing within unified framework. Similarly, the open-source models, such as Qwen-Image [52] and FLUX.1-Kontextdev [23], built upon robust multimodal large language model backbone, demonstrates high-quality and flexible image generation and editing performance. Beyond these, numerous open-source unified generative models continue to emerge [53, 5658]. These advancements collectively demonstrate that diffusion models are becoming increasingly flexible and adaptable across diverse conditional generation settings. 2.2. Benchmarks for Reference-Based Generation Among reference-driven image generation tasks, image editing can be the most widely recognized, which has referred to the task of generating an edited image based on single reference image and textual instruction describing how the edit should be performed. Representative benchmarks such as MagicBrush [63] and EMU-Edit [46] have 2 Table 1. Comparison among major benchmarks for reference-based image generation. Existing benchmarks do not provide systematic evaluation across diverse multi-reference conditions, support only limited number of references, and fail to adequately account for important factors such as differences and compatibility among reference images. Our benchmark expands the upper limit on the number of references and introduces difficult reference combinations, including domain mismatches, scale mismatches, rare concepts, and multilingual prompts, thereby explicitly addressing challenges unique to multi-reference image generation. Benchmark #Size #Tasks #References Difficult Reference Combination Metrics EditBench [51] EditVal [1] EmuEdit [46] MagicBrush [63] AnyEdit [62] I2EBench [27] ImgEdit-Bench [61] DreamBooth [43] OmniContext [53] DreamOmni2 [56] MultiBanana (Ours) 240 648 3055 1053 1250 2240 811 75 400 319 1 13 7 9 25 16 14 1 8 29 36 1 1 1 1 1 1 1 1 3 4 8 (cid:37) (cid:37) (cid:37) (cid:37) (cid:37) (cid:37) (cid:37) (cid:37) (cid:37) (cid:37) (cid:33) CLIP [39] CLIP, VLM, manual L1, CLIP, DINO [6] L1, L2, CLIP, DINO L1, CLIP, DINO GPT [33] GPT (3 dim.), Fake Det. [59] CLIP. DINO GPT (3 dim.) Gemini [13], Doubao [3] GPT, Gemini (5 dim.) extended task-specific evaluations but still rely heavily on similarity-based metrics. SmartEdit [17] supports editing of complex scenes, but does not sufficiently cover more general settings. Similarly, I2E-Bench [27] employs GPT-4o to provide human-aligned evaluations across variety of editing tasks, yet it uses different metrics for each task and therefore fails to capture the shared characteristics of editing as whole. More recently, ImgEdit [61] has enabled the evaluation of multi-turn editing tasks, where image generation and editing are alternated interactively. One of the earliest benchmarks for reference-based image generation beyond instruction-based editing is the DreamBooth [43] dataset. However, such benchmarks are still limited to generation tasks conditioned on single reference image and thus cannot handle more complex or compositional settings. Recently, following advances demonstrated by GPT-Image-1 [34] and Nano Banana [14], increasing attention has been directed toward multi-reference image generation, where multiple reference images are used jointly for generation. Benchmarks such as OmniContext [53] and DreamOmni2 [56] have been proposed to evaluate this task. Nevertheless, these benchmarks encompass only small number of images and task types, thereby limiting the scope of the evaluation. Moreover, despite being designed for multi-reference generation, they do not evaluate qualitative differences or relationships between reference images, and their setups remain similar to those of conventional singleimage editing benchmarks. 3. MultiBanana 3.1. Task Types In reference-driven image generation, tasks can be categorized based on the number and relational structure of reference images. In this work, we follow the overall design philosophy of prior benchmarks such as DreamOmni2 [56], but place stronger emphasis on an aspect that has been the diversity and composilargely underexplored so far: tional relationships among multiple references. While existing benchmarks mainly evaluate performance in terms of the number of references or single-image editing accuracy, our benchmark explicitly aims to measure models ability to understand and reason about inter-reference variations as key part of multimodal generation. Single Reference. This is the most fundamental setting, corresponding to the standard image editing task. Given single reference image and textual instruction, the model generates an edited image that maintains semantic consistency and preserves the subjects visual identity. This configuration follows traditional instruction-driven editing benchmarks such as MagicBrush [63] and EMU-Edit [46], which represent the foundation of reference-driven generation. Two References 11 Task Types. The two-reference setting provides the model with two reference images (typically main subject and an auxiliary one) and textual instruction. We define 11 editing task types, following prior work, while balancing feasibility and challenge: subject addition, subject replacement, background change, color modification, material modification, pose modification, hairstyle modification, makeup modification, tone transformation (lighting, season, weather, etc.), style transfer (oil painting, sketch, anime, etc.), and text correction (in-image text editing or replacement). These tasks span both local attribute manipulations and global transformations, evaluating models ability to maintain visual coherence, subject identity, and contextual adaptability. Multi References Compositional Generation and Inter-Reference Relationships. The multi-reference set3 Figure 2. Construction pipeline for our benchmark, consisting of four stages: (1) collecting high-quality real and synthetic images, (2) filtering out inappropriate or low-quality samples, (3) performing hierarchical category classification, and (4) generating and validating editing instructions by Gemini and humans. ting uses 38 reference images, requiring the model to perform more compositional and reasoning-based generation. While prior works, such as DreamOmni2 [56], mainly focus on compositional generation using multiple references, our benchmark takes step further by making inter-reference diversity, consistency, and structural relations central to the evaluation. We define four base task structures, each evaluated with {3, 4, 5, 6, 7, 8} reference images, and include variants with and without textual instructions resulting in total of 48 tasks: Objects: Compose objects from the reference set. X1 Objects + Background: Place 1 objects within the background of another reference. X1 Objects + Local: Apply local change (specified by reference) to one of the 1 objects. X1 Objects + Global: Arrange 1 objects and modify the overall tone according to another reference. This design enables us to assess whether model can effectively integrate multiple reference sources into coherent, contextually consistent scene. Diversity Factors in Multi-Reference Generation. central contribution of our benchmark is the explicit incorporation of inter-reference diversity factors, which have been largely overlooked in previous benchmarks such as DreamOmni2. We highlight the following new evaluation dimensions: Number of references: systematic comparison across 3 8 inputs. Cross-domain diversity: mixing references from different domains such as real photos, anime, CG, or sketches. Scale and viewpoint differences: variation in object size or camera perspective between references and outputs. Rare concept references: inclusion of uncommon concepts (e.g., red banana). Multilingual references: assessing multilingual instruction understanding during rendering. These factors go beyond conventional image-editing evaluation and are critical for assessing models compositional reasoning, identity preservation, and cross-domain generIn summary, our benchmark diverges from the alization. conventional notion of multi-reference generation, such as in DreamOmni2, which primarily focuses on simple reference composition. Instead, it introduces new framework designed to evaluate model capabilities to interpret relationships among heterogeneous references and integrate them into coherent generations. 3.2. How to Construct MultiBanana Image Collection. The reference images used in our benchmark consist of both real and synthetic data. All real images were extracted from the LAION-5B dataset [45]. To ensure high quality, we selected only images with an Aesthetic score higher than 6.25 and resolution greater than 512 pixels. However, the collected real images exhibited distributional bias (Figure 3; left). Specifically, landscape scenes appeared frequently, whereas categories useful as editing references, such as humans and objects, were underrepresented. While landscape images can serve as useful background references, the scarcity of human and object references limits the diversity of editing tasks that can be constructed in the benchmark. To address this imbalance, we generated additional synthetic images under diverse conditions using Nano Banana [14] and ChatGPTImage-1 [34], and used them as supplementary reference images (Figure 3; right). This approach not only mitigated the categorical bias in the reference set but also expanded the coverage and diversity of reference images by incorporating both real and synthetic examples. Image Filtering. Among the collected images, some contained small subjects that were difficult to use as references, or were inappropriate as reference images, such 4 Figure 3. (Left) Comparison between the statistics of real data only and those after adding synthetic data. The original dataset was biased toward background images, with few personand object-related samples. To correct this imbalance, we generated additional synthetic images using Nanobanana and ChatGPT-Image-1, focusing on clear subjects such as people, animals, and objects. This significantly increased personand object-related categories, resulting in more balanced and comprehensive benchmark. (Right) Examples of synthesized images in each category. as those containing harmful content, tables, or corrupted data. To filter out such images and retain only those suitable for reference-based image editing benchmark, we applied the following procedure. Following [61], we extracted images that contained large and clearly visible objects suitable for editing tasks. Object detection was performed using YOLOv12 [48] and SAM2 [41], while CLIP [39] was used to verify semantic consistency. Additionally, harmful or corrupted images were removed through combination of automatic filtering with Gemini and manual inspection, resulting in the final curated set of real images. Category Classification. To construct multi-reference image generation tasks, we categorized each reference image accurately. For example, in portrait transformation task that modifies persons hairstyle or makeup, both reference images must contain person. Similarly, in replacement tasks involving humans or objects, both reference images must include either person or an object, and for background replacement tasks, images categorized as background must be used. To satisfy such task-dependent conditions, we performed hierarchical image classification on combined set of real and synthetic images. We first obtained automatic multi-level labels using Gemini and then conducted human verification to ensure correct categorization. At the top level, we defined five major categories: person, object, background, light, tone. Each of these top-level categories was further divided into more specific subcategories. For instance, under the person category, we included male, female, hairstyle, makeup, pose useful, and face expression; and under the object category, we defined animal, clothing design, industrial design, and object. This hierarchical labeling enabled precise task construction and increased the benchmarks diversity. Task Construction. The editing instructions were generated using Gemini. For each task, we manually selected suitable reference categories, randomly sampled images, and presented them to Gemini to produce corresponding editing instructions. Gemini then evaluated whether following each instruction would lead to visual breakdowns. For instance, in replacement task, whether the replaced object would appear unnaturally suspended in mid-air. Among the generated candidates, we retained only the referenceinstruction pairs that did not exhibit visual breakdowns and then asked Gemini to further assess whether each instruction constituted challenging editing scenario, such as cross-domain modifications. Finally, we manually reviewed the remaining data to remove any residual cases that Gemini failed to detect or instructions that were deemed too trivial. Difficult Reference Categorization. For the crossdomain category, we selected samples whose reference images spanned at least two domains (e.g., color photos, anime, paintings). The different-scale category included samples with both close-up and nonclose-up views. The rare-concept category contained samples featuring animals with unusual colors or patterns, humans or animals with atypical makeup or clothing, or fictional creatures. The multilingual category included samples with text in two different languages among English, Chinese, and Japanese. Samples not matching any category were grouped as others. 3.3. Statistics in MultiBanana Image Category Statistics. This section presents statistical overview of both the real and synthetic source data used to construct our benchmark. The dataset is organized into six major categories and thirteen subcategories based on visual and semantic attributes. In the real dataset, the largest major category is background, comprising 798 images, one-fourth of the total. 5 Figure 4. (Left) Breakdown of the multi-reference tasks. The editing sets were selected to ensure that the number of sets within each task is balanced across different reference counts. (Middle) For every Xreferences task, the dataset contains at least 390 editing sets. Further, each colored task category also includes at least 70 sets, which is larger than the prior work [56]. (Right) Word cloud generated from all prompts. It primarily consists of terms that describe wide range of object categories as well as words indicating spatial directions. Table 2. Difficult reference combination statistics and ratios relative to the total 3769 tasks. Our benchmark provides sufficient number of samples for assessing models compositional reasoning, identity preservation, and cross-domain generalization, compared to prior work [56]. Cross Domain Different Rare Scale & View Concept Multilingual Count Ratio (%) 1063 28.2% 1357 36.0% 743 19.7% 99 2.6% These mainly consist of images without distinct subjects. The next largest categories are person (741 images) and style (579 images), and together these three categories account for 70% of the dataset. In contrast, light (195 images), object (413 images), and tone (118 images) are relatively small in size. Although these categories are useful as references for changing global conditions such as illumination and overall atmosphere, their sample sizes remain limited. Within the object category, there are 111 images of animals, 40 of clothing and textile design, 71 of industrial design, and 191 of miscellaneous objects, indicating that data focusing on specific materials or design patterns is scarce. Most images in the background, style, and tone categories lack clear subjects. Furthermore, images containing textual elements, such as logos or signs, are nearly absent. The person category is subdivided into finer attributes: face expression (10), female (181), male (139), hairstyle (135), makeup (59), and pose usefulness (217). While pose-related data account for the largest portion (around 30%), reference data capturing subtle characteristics such as facial expression or makeup remain limited, resulting in skewed distribution. To address this imbalance, synthetic data were generated, focusing primarily on images containing explicit subjects (e.g., persons, animals, and objects) as well as those including text. The generation process was designed to intentionally expand the coverage of underrepresented subcategories and improve the overall balance among attributes. As result, the major category distribution changed substantially: background (802), light (729), object (2041), person (2437), style (592), and tone (649). At the subcategory level, significant increases were observed: animal (111915), clothing design (40396), industrial design (71385), and face expression (10146), indicating notable enrichment in personand object-related data. Consequently, the dataset became more balanced between background-centric and subject-centric images, with broader coverage of diverse visual attributes. This improvement enhances the benchmarks comprehensiveness and enables more versatile evaluation and instruction-based generation tasks under wide range of conditions. Task Statistics. After filtering the generated editing instructions using both Gemini and manual checks, we obtained 3,769 high-quality reference images and instruction sets. Among them, 264 sets correspond to singlereference tasks, 907 to two-reference tasks, and 2,598 to multi-reference tasks. Figure 4 provides further breakdown of the multi-reference tasks, showing that they are evenly distributed with respect to the number of reference images. Additionally, for each task type, Object, X1 Object + Background, X1 Object + Local, and X1 Object + Global, we ensured that at least 70 sets were included. This is substantially larger than the number of sets in multi-reference tasks of DreamOmni2, which is on the order of ten, enabling more reliable evaluation of model performance. Furthermore, Table 2 shows that our newly introduced evaluation axes cross-domain, scale and viewpoint differences, rare concept, and multilingual provide sufficient samples. See Appendix for details. 3.4. Evaluation Setting For evaluating the quality of edited images, human evaluation is among the most valuable methods, yet gathering human feedback at scale is prohibitively costly. practical approach to reduce time and cost is to leverage AI feedback 6 Table 3. Average performance per model across different task categories (10-point scale; the higher the better). The average scores from Gemini-2.5 and GPT-5 are reported. Both open-source and closed-source models exhibit notably lower performance on background replacement tasks, regardless of the number of reference images. Model Nano Banana + Agent (IPR) GPT-Image-1 + Agent (IPR) Qwen-Image-Edit-2509 DreamOmni2 OmniGen2 Single Two Objects X-1 + Local X-1 + Global X-1 + Background 7.817 7.606 7.804 7. 7.499 6.520 5.919 4.891 5.030 6.585 6.808 3.699 4.069 3.442 4.453 4.433 5.086 5. 2.256 2.804 3.256 4.118 4.030 5.147 5.419 2.031 3.037 3.598 4.698 4.496 5.757 5. 2.351 2.867 3.369 3.575 3.767 5.019 5.284 2.033 2.594 3.022 Figure 5. Changes in scores for each evaluation criterion when varying the number of reference images. Both open-source and closedsource models exhibit general trend of decreasing all scores as the number of references increases. sual Quality}, respectively. See Appendix for details. 4. Experiments We adopt several closed models (i.e., Nano Banana [14], GPT-Image-1 [34]) and open models (i.e., Qwen-ImageEdit-2509 [52]], DreamOmni2 [56], OmniGen2 [53]) for MultiBanana evaluation. For all models except OmniGen2, we generate outputs for every combination of reference images and text prompts included in the MultiBanana benchmark. Since OmniGen2 does not support more than six reference images, we only evaluate it under the 5-reference setting. As described in Section 3.4, the evaluation is conducted using Gemini-2.5 [13] and GPT-5 [35]. In the main paper, we report the average scores from Gemini-2.5 and GPT-5, while the individual results for each evaluator are provided in the supplementary material. 4.1. Per-Task Evaluation We compute the MultiBanana scores for each task type defined in Section 3.1, and present the results in Table 3 and Section E.1. For the two-reference tasks, we report the average score across all 11 task variants. For the multi-reference tasks, we compute the average score for each task type (i.e., Objects, X1 Objects + Local, X1 Objects + Global, X1 Objects + Background) across = {3, 4, 5, 6, 7, 8}. Overall, both open-source and closed-source models exhibit notably lower performance on background replacement tasks, regardless of the number of reference images. The results for the two-reference setting also reveal clear performance gaps between closed-source and open-source Figure 6. Results for diffucult reference combinations. For crossdomain and different scale and view tasks, every model shows lower scores than tasks without such conditions. from VLMs (e.g., Gemini-2.5 [13] and GPT-5 [35]), which has been shown to modestly align with human judgments of image quality [11, 32, 36, 55]. Therefore, we used Gemini2.5 and GPT-5 to rate all generated images. In our evaluation protocol, all generated images are evaluated along five distinct criteria to comprehensively assess multi-reference image generation performance: Text-Instruction Alignment, Reference Consistency, Background-Subject Match, Physical Realism, and Visual Quality. Text-Instruction Alignment and Reference Consistency serve as the fundamental criteria for multireference image generation, and are also core components in many prior benchmarks [53]. In contrast, BackgroundSubject Match, Physical Realism, and Visual Quality refine what previous work typically treated as single overall quality dimension [53], offering more fine-grained assessment of holistic visual fidelity. Each of the five criteria is scored on 10-point scale (10 is the best). We then compute total weighted score, assigning weights of {3, 3, 1, 1, 1} to {Instruction Alignment, Reference Consistency, Background-Subject Match, Physical Realism, Vi7 Figure 7. (Left) Failure cases of cross-domain, different scale, rare concept, multilingual text. (Right) Failure cases of multi-references in open-source models. They tend to ignore multiple subjects under high-reference conditions. models, even for relatively simple editing scenarios such as single-reference or two-reference tasks. 4.2. Effect of the Number of References We investigate how the MultiBanana score changes when varying the number of reference images = {3, 4, 5, 6, 7, 8} for each multi-reference generation task (Figure 5). Both open-source and closed-source models exhibit general trend of decreasing Total Score as the number of references increases. However, the nature of this degradation differs markedly between the two model families. Closed-source models demonstrate strong adherence to reference attributes and prompt instructions. As result, even when the number of references grows, the decline in Instruction Alignment and Reference Consistency remains relatively mild. Nevertheless, because these models attempt to satisfy all reference constraints strictly, increasing the number of subjects often leads to degraded overall visual quality, including over-crowded scenes and compositional inconsistencies (see Figure 1 and Figure 7; left). In contrast, open-source models show weaker adherence to prompts and references; even under the 3-reference setting, both Instruction Alignment and Reference Consistency remain low, and these scores further deteriorate as the reference count increases. Qualitative inspection also reveals that open models frequently ignore multiple subjects under high-reference conditions. However, their overall quality (background consistency, physical realism, visual quality) remains relatively stable, and they tend not to suffer from the severe compositional collapse observed in closed models (see Figure 7; right). In summary, closed-source models succeed in incorporating all required subjects but often produce globally inconsistent or distorted scenes due to overfitting to finegrained reference details. Open-source models, by contrast, tend to omit multiple reference subjects in complex scenarios but generate visually clean images with fewer structural failures. These observations highlight clear trade-off between strict reference fidelity and holistic image coherence. 4.3. Analysis of Difficult Reference Combinations Next, to analyze the challenging subject combinations included in this benchmark, namely those described in Section 3.1, we compare the MultiBanana scores between tasks containing these difficult combinations and those that do not in Figure 6. For cross-domain, different-scale and view, rare-concept, and multilingual tasks, most models show lower scores than for tasks without such conditions. These results indicate that the difficult reference concepts focused on by our benchmark remain challenging, even for state-ofthe-art closed models. This demonstrates that our benchmark effectively uncovers the current limitations of multireference image generation. 4.4. Agentic Framework To address the performance limitations on our challenging benchmark, we further construct three agentic framework baselines: Iterative Prompt Refinement (IPR), ContextAware Feedback Generation (CAFG), and Selective Reference Adaptation (SRA). We then conduct experiments on our MultiBanana benchmark. We show the results of IPR in Table 3, where the generator creates images, and based on these images, the planner rewrites the prompt over multiple steps. While GPTs performance improved with the agent framework, Nano Banana showed no improvement and, in some cases, performance degraded due to information loss from the initial instruction. For details on the problem setting, ablations, and results, see the supplementary material. 8 5. Conclusion We constructed comprehensive benchmark for multireference image generation, focusing on challenges specific to this setting, such as difficult reference combinations. Using it, we evaluated state-of-the-art open and closed models and found two notable failure modes as the number of references increases: models either ignore editing instructions or collapse when trying to follow them. Across different types of reference difficulty, tasks involving cross-domain inputs, large-scale or viewpoint gaps, rare concepts, and multilingual inputs are consistently challenging."
        },
        {
            "title": "Acknowledgements",
            "content": "We thank Minsu Kim and Heiga Zen for their support on this work and review on the initial version of the paper. We also appriciate the funding support from Google Japan. MS was supported by JSPS KAKENHI Grant Number JP23H04974."
        },
        {
            "title": "References",
            "content": "[1] Samyadeep Basu, Mehrdad Saberi, Shweta Bhardwaj, Atoosa Malemir Chegini, Daniela Massiceti, Maziar Sanjabi, Shell Xu Hu, and Soheil Feizi. Editval: Benchmarking diffusion based text-guided image editing methods, 2023. 2, 3 [2] Tim Brooks, Aleksander Holynski, and Alexei Efros. Instructpix2pix: Learning to follow image editing instructions. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 1839218402, 2023. 2, 11 [3] ByteDance. Doubao. https://www.doubao.com/, 2025. 3, 5 [4] Mingdeng Cao, Xintao Wang, Zhongang Qi, Ying Shan, Xiaohu Qie, and Yinqiang Zheng. Masactrl: Tuning-free mutual self-attention control for consistent image synthesis and editing. In Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV), pages 2256022570, 2023. 2, 11 [5] Siyu Cao, Hangting Chen, Peng Chen, Yiji Cheng, Yutao Cui, Xinchi Deng, Ying Dong, Kipper Gong, Tianpeng Gu, Xiusen Gu, Tiankai Hang, Duojun Huang, Jie Jiang, Zhengkai Jiang, Weijie Kong, Changlin Li, Donghao Li, Junzhe Li, Xin Li, Yang Li, Zhenxi Li, Zhimin Li, Jiaxin Lin, Linus, Lu-Hao Liu, Shu Liu, Songtao Liu, Yu Liu, Yuhong Liu, Yanxin Long, Fanbin Lu, Qinglin Lu, Yuyan Peng, Yuanbo Peng, Xiang-Yu Shen, Yi-Ping Shi, Jiale Tao, Yang-Dan Tao, Qianhui Tian, Pengfei Wan, Chunyu Wang, Kai Wang, Lei Wang, Linqing Wang, Lucas Wang, Qixun Wang, Weiyang Wang, Hao Wen, Bing Wu, Jianbing Wu, Yue Wu, Senhao Xie, Fangzhou Yang, Miles Yang, Xiaofeng Yang, Xuan Yang, Zhantao Yang, Jingmiao Yu, Zhengang Yuan, Chao Zhang, Jianwei Zhang, Pei pei Zhang, Shiyuan Zhang, Tao Zhang, Weigang Zhang, Yepeng Zhang, Yingfang Zhang, Zihao Zhang, Zijian Zhang, Penghao Zhao, Zhiyuan Zhao, Xuefei Zhe, Jian-Xiang Zhu, and Zhao Zhong. Hunyuanimage 3.0 technical report. ArXiv, abs/2509.23951, 2025. 2 [6] Mathilde Caron, Hugo Touvron, Ishan Misra, Herve Jegou, Julien Mairal, Piotr Bojanowski, and Armand Joulin. Emerging properties in self-supervised vision transformers. arXiv preprint arXiv:2104.14294, 2021. 3 [7] Yisol Choi, Sangkyung Kwak, Kyungmin Lee, Hyungwon Choi, and Jinwoo Shin. Improving diffusion models for authentic virtual try-on in the wild. arXiv preprint arXiv:2403.05139, 2024. 2 [8] Yufeng Cui, Honghao Chen, Haoge Deng, Xu Huang, Xinghang Li, Jirong Liu, Yang Liu, Zhuoyan Luo, Jinsheng Wang, Wenxuan Wang, Yueze Wang, Chengyuan Wang, Fan Zhang, Yingli Zhao, Ting Pan, Xianduo Li, Zecheng Hao, Wenxuan Ma, Zhuo Chen, Yulong Ao, Tiejun Huang, Zhongyuan Wang, and Xinlong Wang. Emu3.5: Native multimodal models are world learners, 2025. 2 [9] Patrick Esser, Sumith Kulal, Andreas Blattmann, Rahim Entezari, Jonas Muller, Harry Saini, Yam Levi, Dominik Lorenz, Axel Sauer, Frederic Boesel, Dustin Podell, Tim Dockhorn, Zion English, Kyle Lacey, Alex Goodwin, Yannik Marek, and Robin Rombach. Scaling rectified flow transformers for high-resolution image synthesis, 2024. 2, 11 [10] Zixun Fang, Wei Zhai, Aimin Su, Hongliang Song, Kai Zhu, Mao Wang, Yu Chen, Zhiheng Liu, Yang Cao, and ZhengJun Zha. Vivid: Video virtual try-on using diffusion models, 2024. 2 [11] Hiroki Furuta, Heiga Zen, Dale Schuurmans, Aleksandra Faust, Yutaka Matsuo, Percy Liang, and Sherry Yang. Improving dynamic object interactions in text-to-video generation with ai feedback. arXiv preprint arXiv:2412.02617, 2024. 7, 3 [12] Rinon Gal, Yuval Alaluf, Yuval Atzmon, Or Patashnik, Amit Haim Bermano, Gal Chechik, and Daniel Cohen-Or. An image is worth one word: Personalizing text-to-image generation using textual inversion. In The Eleventh International Conference on Learning Representations, 2023. 2, 11 [13] Gemini Team. Gemini: family of highly capable multimodal models. arXiv preprint arXiv:2312.11805, 2023. 3, 7, [14] Google DeepMind. Nanobanana: Gemini 2.5 flash image model. https://developers.googleblog.com/ en/introducinggemini25- flashimage/, 2025. Accessed: 2025-10-31. 2, 3, 4, 7, 1, 5, 11, 12 [15] Amir Hertz, Ron Mokady, Jay Tenenbaum, Kfir Aberman, Yael Pritch, and Daniel Cohen-Or. Prompt-to-prompt image editing with cross-attention control. In The Eleventh International Conference on Learning Representations, 2023. 2, 11 [16] Daichi Horita, Naoto Inoue, Kotaro Kikuchi, Kota Yamaguchi, and Kiyoharu Aizawa. Retrieval-Augmented Layout Transformer for Content-Aware Layout Generation. In CVPR, 2024. 2 [17] Yuzhou Huang, Liangbin Xie, Xintao Wang, Ziyang Yuan, Xiaodong Cun, Yixiao Ge, Jiantao Zhou, Chao Dong, Rui Huang, Ruimao Zhang, and Ying Shan. Smartedit: Exploring complex instruction-based image editing with mulIn Proceedings of the timodal large language models. 9 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pages 83628371, 2024. 2, 3, 12 [18] Inbar Huberman-Spiegelglas, Vladimir Kulikov, and Tomer Michaeli. An edit friendly ddpm noise space: Inversion and manipulations. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pages 1246912478, 2024. 2, [19] Imagen-Team-Google, :, Jason Baldridge, Jakob Bauer, Mukul Bhutani, Nicole Brichtova, Andrew Bunner, Lluis Castrejon, Kelvin Chan, Yichang Chen, Sander Dieleman, Yuqing Du, Zach Eaton-Rosen, Hongliang Fei, Nando de Freitas, Yilin Gao, Evgeny Gladchenko, Sergio Gomez Colmenarejo, Mandy Guo, Alex Haig, Will Hawkins, Hexiang Hu, Huilian Huang, Tobenna Peter Igwe, Christos Kaplanis, Siavash Khodadadeh, Yelin Kim, Ksenia Konyushkova, Karol Langner, Eric Lau, Rory Lawton, Shixin Luo, Soˇna Mokra, Henna Nandwani, Yasumasa Onoe, Aaron van den Oord, Zarana Parekh, Jordi Pont-Tuset, Hang Qi, Rui Qian, Deepak Ramachandran, Poorva Rane, Abdullah Rashwan, Ali Razavi, Robert Riachi, Hansa Srinivasan, Srivatsan Srinivasan, Robin Strudel, Benigno Uria, Oliver Wang, Su Wang, Austin Waters, Chris Wolff, Auriel Wright, Zhisheng Xiao, Hao Xiong, Keyang Xu, Marc van Zee, Junlin Zhang, Katie Zhang, Wenlei Zhou, Konrad Zolna, Ola Aboubakar, Canfer Akbulut, Oscar Akerlund, Isabela Albuquerque, Nina Anderson, Marco Andreetto, Lora Aroyo, Ben Bariach, David Barker, Sherry Ben, Dana Berman, Courtney Biles, Irina Blok, Pankil Botadra, Jenny Brennan, Karla Brown, John Buckley, Rudy Bunel, Elie Bursztein, Christina Butterfield, Ben Caine, Viral Carpenter, Norman Casagrande, MingWei Chang, Solomon Chang, Shamik Chaudhuri, Tony Chen, John Choi, Dmitry Churbanau, Nathan Clement, Matan Cohen, Forrester Cole, Mikhail Dektiarev, Vincent Du, Praneet Dutta, Tom Eccles, Ndidi Elue, Ashley Feden, Shlomi Fruchter, Frankie Garcia, Roopal Garg, Weina Ge, Ahmed Ghazy, Bryant Gipson, Andrew Goodman, Dawid Gorny, Sven Gowal, Khyatti Gupta, Yoni Halpern, Yena Han, Susan Hao, Jamie Hayes, Jonathan Heek, Amir Hertz, Ed Hirst, Emiel Hoogeboom, Tingbo Hou, Heidi Howard, Mohamed Ibrahim, Dirichi Ike-Njoku, Joana Iljazi, Vlad Ionescu, William Isaac, Reena Jana, Gemma Jennings, Donovon Jenson, Xuhui Jia, Kerry Jones, Xiaoen Ju, Ivana Kajic, Christos Kaplanis, Burcu Karagol Ayan, Jacob Kelly, Suraj Kothawade, Christina Kouridi, Ira Ktena, Jolanda Kumakaw, Dana Kurniawan, Dmitry Lagun, Lily Lavitas, Jason Lee, Tao Li, Marco Liang, Maggie Li-Calis, Yuchi Liu, Javier Lopez Alberca, Matthieu Kim Lorrain, Peggy Lu, Kristian Lum, Yukun Ma, Chase Malik, John Mellor, Thomas Mensink, Inbar Mosseri, Tom Murray, Aida Nematzadeh, Paul Nicholas, Signe Nørly, Joao Gabriel Oliveira, Guillermo Ortiz-Jimenez, Michela Paganini, Tom Le Paine, Roni Paiss, Alicia Parrish, Anne Peckham, Vikas Peswani, Igor Petrovski, Tobias Pfaff, Alex Pirozhenko, Ryan Poplin, Utsav Prabhu, Yuan Qi, Matthew Rahtz, Cyrus Rashtchian, Charvi Rastogi, Amit Raul, Ali Razavi, Sylvestre-Alvise Rebuffi, Susanna Ricco, Felix Riedel, Dirk Robinson, Pankaj Rohatgi, Bill Rosgen, Sarah Rumbley, Moonkyung Ryu, Anthony Salgado, Tim Salimans, Sahil Singla, Florian Schroff, Candice Schumann, Tanmay Shah, Eleni Shaw, Gregory Shaw, Brendan Shillingford, Kaushik Shivakumar, Dennis Shtatnov, Zach Singer, Evgeny Sluzhaev, Valerii Sokolov, Thibault Sottiaux, Florian Stimberg, Brad Stone, David Stutz, Yu-Chuan Su, Eric Tabellion, Shuai Tang, David Tao, Kurt Thomas, Gregory Thornton, Andeep Toor, Cristian Udrescu, Aayush Upadhyay, Cristina Vasconcelos, Alex Vasiloff, Andrey Voynov, Amanda Walker, Luyu Wang, Miaosen Wang, Simon Wang, Stanley Wang, Qifei Wang, Yuxiao Wang, Agoston Weisz, Olivia Wiles, Chenxia Wu, Xingyu Federico Xu, Andrew Xue, Jianbo Yang, Luo Yu, Mete Yurtoglu, Ali Zand, Han Zhang, Jiageng Zhang, Catherine Zhao, Adilet Zhaxybay, Miao Zhou, Shengqi Zhu, Zhenkai Zhu, Dawn Bloxwich, Mahyar Bordbar, Luis C. Cobo, Eli Collins, Shengyang Dai, Tulsee Doshi, Anca Dragan, Douglas Eck, Demis Hassabis, Sissie Hsiao, Tom Hume, Koray Kavukcuoglu, Helen King, Jack Krawczyk, Yeqing Li, Kathy Meier-Hellstern, Andras Orban, Yury Pinsky, Amar Subramanya, Oriol Vinyals, Ting Yu, and Yori Zwols. Imagen 3, 2024. 11 [20] Liming Jiang, Qing Yan, Yumin Jia, Zichuan Liu, Hao Kang, InfiniteYou: Flexible photo recrafting while and Xin Lu. preserving your identity. In ICCV, 2025. 2, 11 Inbar Huberman- [21] Vladimir Kulikov, Matan Kleiner, InversionSpiegelglas, and Tomer Michaeli. free text-based editing using pre-trained flow models. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 1972119730, 2025. 2, Flowedit: [22] Black Forest Labs. Flux. https://github.com/ black-forest-labs/flux, 2024. 2, 11 [23] Black Forest Labs, Stephen Batifol, Andreas Blattmann, Frederic Boesel, Saksham Consul, Cyril Diagne, Tim Dockhorn, Jack English, Zion English, Patrick Esser, Sumith Kulal, Kyle Lacey, Yam Levi, Cheng Li, Dominik Lorenz, Jonas Muller, Dustin Podell, Robin Rombach, Harry Saini, Axel Sauer, and Luke Smith. Flux.1 kontext: Flow matching for in-context image generation and editing in latent space, 2025. 2, 5, 11 [24] LAION-AI. aesthetic-predictor, 2022. 1 [25] DONGXU LI, Junnan Li, and Steven Hoi. Blip-diffusion: Pre-trained subject representation for controllable text-toIn Advances in Neural Inimage generation and editing. formation Processing Systems, pages 3014630166. Curran Associates, Inc., 2023. 2, 11 [26] Yiwei Ma, Guohai Xu, Xiaoshuai Sun, Ming Yan, Ji Zhang, and Rongrong Ji. X-clip: End-to-end multi-grained conarXiv preprint trastive learning for video-text retrieval. arXiv:2207.07285, 2022. [27] Yiwei Ma, Jiayi Ji, Ke Ye, Weihuang Lin, Yonghan Zheng, Qiang Zhou, Xiaoshuai Sun, Rongrong Ji, et al. I2ebench: comprehensive benchmark for instruction-based image editing. In The Thirty-eighth Annual Conference on Neural Information Processing Systems, 2024. 2, 3, 12 [28] Daiki Miyake, Akihiro Iohara, Yu Saito, and Toshiyuki Tanaka. Negative-prompt inversion: Fast image inversion In 2025 for editing with text-guided diffusion models. IEEE/CVF Winter Conference on Applications of Computer Vision (WACV), pages 20632072, 2025. 2, 11 10 [29] Ron Mokady, Amir Hertz, Kfir Aberman, Yael Pritch, and Daniel Cohen-Or. Null-text inversion for editing real imIn Proceedings of ages using guided diffusion models. the IEEE/CVF conference on computer vision and pattern recognition, pages 60386047, 2023. 2, 11 [30] Ryugo Morita, Stanislav Frolov, Brian Bernhard Moser, Takahiro Shirakawa, Ko Watanabe, Andreas Dengel, and Jinjia Zhou. Tkg-dm: Training-free chroma key content genIn Proceedings of the IEEE/CVF eration diffusion model. Conference on Computer Vision and Pattern Recognition (CVPR), pages 1303113040, 2025. 2 [31] Chong Mou, Xintao Wang, Liangbin Xie, Yanze Wu, Jian Zhang, Zhongang Qi, Ying Shan, and Xiaohu Qie. T2iadapter: Learning adapters to dig out more controllable ability for text-to-image diffusion models. arXiv preprint arXiv:2302.08453, 2023. [32] Sanghyeon Na, Yonggyu Kim, and Hyunjoon Lee. Boost your own human image generation model via direct prefarXiv preprint erence optimization with ai feedback. arXiv:2405.20216, 2024. 7, 3 Gpt-4 technical arXiv:2303.08774, 2023. 3, 5 arXiv preprint [33] OpenAI. report. [34] OpenAI. Gpt-4o image generation. https : / / openai . com / index / introducing - 4o - image - generation/, 2025. Accessed: 2025-10-31. 2, 3, 4, 7, 1, 5, 11, 12 [35] OpenAI. Gpt-5. https://openai.com/jaJP/ index/introducing-gpt-5/, 2025. Accessed: 202511-14. 7 [36] Y. Oshima, M. Suzuki, Y. Matsuo, and H. Furuta. Inferencetime text-to-video alignment with diffusion latent beam search, 2025. arXiv preprint arXiv:2501.19252. 7, 3 [37] Gaurav Parmar, Krishna Kumar Singh, Richard Zhang, Yijun Li, Jingwan Lu, and Jun-Yan Zhu. Zero-shot image-to-image translation. In ACM SIGGRAPH 2023 conference proceedings, pages 111, 2023. 2, 11 [38] Dustin Podell, Zion English, Kyle Lacey, Andreas Blattmann, Tim Dockhorn, Jonas Muller, Joe Penna, and Robin Rombach. SDXL: Improving latent diffusion models In The Twelfth Interfor high-resolution image synthesis. national Conference on Learning Representations, 2024. 2, 11 [39] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, Gretchen Krueger, and Ilya Sutskever. Learning transferable visual models from natural language supervision. arXiv preprint arXiv:2103.00020, 2021. 3, 5 [40] Aditya Ramesh, Prafulla Dhariwal, Alex Nichol, Casey Chu, and Mark Chen. Hierarchical text-conditional image generation with clip latents, 2022. [41] Nikhila Ravi, Valentin Gabeur, Yuan-Ting Hu, Ronghang Hu, Chaitanya Ryali, Tengyu Ma, Haitham Khedr, Roman Radle, Chloe Rolland, Laura Gustafson, Eric Mintun, Junting Pan, Kalyan Vasudev Alwala, Nicolas Carion, ChaoYuan Wu, Ross Girshick, Piotr Dollar, and Christoph Feichtenhofer. SAM 2: Segment anything in images and videos. In 11 The Thirteenth International Conference on Learning Representations, 2025. 5, 1 [42] Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Bjorn Ommer. High-resolution image synthesis with latent diffusion models. arXiv preprint arXiv:2112.10752, 2022. 2, 11 [43] Nataniel Ruiz, Yuanzhen Li, Varun Jampani, Yael Pritch, Michael Rubinstein, and Kfir Aberman. Dreambooth: Fine tuning text-to-image diffusion models for subject-driven generation. 2022. 2, 3, 11, 12 [44] Chitwan Saharia, William Chan, Saurabh Saxena, Lala Li, Jay Whang, Emily Denton, Seyed Kamyar Seyed Ghasemipour, Burcu Karagol Ayan, S. Sara Mahdavi, Rapha Gontijo Lopes, Tim Salimans, Jonathan Ho, David Fleet, and Mohammad Norouzi. Photorealistic text-to-image diffusion models with deep language understanding, 2022. [45] Christoph Schuhmann, Romain Beaumont, Richard Vencu, Cade Gordon, Ross Wightman, Mehdi Cherti, Theo Coombes, Aarush Katta, Clayton Mullis, Mitchell Wortsman, et al. Laion-5b: An open large-scale dataset for training next generation image-text models. Advances in neural information processing systems, 35:2527825294, 2022. 4 [46] Shelly Sheynin, Adam Polyak, Uriel Singer, Yuval Kirstain, Amit Zohar, Oron Ashual, Devi Parikh, and Yaniv Taigman. Emu edit: Precise image editing via recognition and generation tasks. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pages 88718879, 2024. 2, 3, 12 [47] Zhenxiong Tan, Songhua Liu, Xingyi Yang, Qiaochu Xue, and Xinchao Wang. Ominicontrol: Minimal and universal control for diffusion transformer. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 1494014950, 2025. 2, 11 [48] Yunjie Tian, Qixiang Ye, and David Doermann. Yolov12: Attention-centric real-time object detectors. arXiv preprint arXiv:2502.12524, 2025. 5, 1 [49] Narek Tumanyan, Michal Geyer, Shai Bagon, and Tali Dekel. Plug-and-play diffusion features for text-driven image-to-image translation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pages 19211930, 2023. 2, 11 [50] Bram Wallace, Akash Gokul, and Nikhil Naik. Edict: Exact diffusion inversion via coupled transformations. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pages 2253222541, 2023. 2, 11 [51] Su Wang, Chitwan Saharia, Ceslee Montgomery, Jordi PontTuset, Shai Noy, Stefano Pellegrini, Yasumasa Onoe, Sarah Laszlo, David J. Fleet, Radu Soricut, Jason Baldridge, Mohammad Norouzi, Peter Anderson, and William Chan. Imagen editor and editbench: Advancing and evaluating textguided image inpainting. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pages 1835918369, 2023. 2, [52] Chenfei Wu, Jiahao Li, Jingren Zhou, Junyang Lin, Kaiyuan Gao, Kun Yan, Sheng ming Yin, Shuai Bai, Xiao Xu, Yilei [64] Lvmin Zhang, Anyi Rao, and Maneesh Agrawala. Adding conditional control to text-to-image diffusion models, 2023. 11 [65] Luyang Zhu, Dawei Yang, Tyler Zhu, Fitsum Reda, William Chan, Chitwan Saharia, Mohammad Norouzi, and Ira Kemelmacher-Shlizerman. Tryondiffusion: tale of two unets. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pages 4606 4615, 2023. 2 Chen, Yuxiang Chen, Zecheng Tang, Zekai Zhang, Zhengyi Wang, An Yang, Bowen Yu, Chen Cheng, Dayiheng Liu, Deqing Li, Hang Zhang, Hao Meng, Hu Wei, Jingyuan Ni, Kai Chen, Kuan Cao, Liang Peng, Lin Qu, Minggang Wu, Peng Wang, Shuting Yu, Tingkun Wen, Wensen Feng, Xiaoxiao Xu, Yi Wang, Yichang Zhang, Yongqiang Zhu, Yujia Wu, Yuxuan Cai, and Zenan Liu. Qwen-image technical report, 2025. 2, 7, 5, 11 [53] Chenyuan Wu, Pengfei Zheng, Ruiran Yan, Shitao Xiao, Xin Luo, Yueze Wang, Wanli Li, Xiyan Jiang, Yexin Liu, Junjie Zhou, Ze Liu, Ziyi Xia, Chaofan Li, Haoge Deng, Jiahao Wang, Kun Luo, Bo Zhang, Defu Lian, Xinlong Wang, Zhongyuan Wang, Tiejun Huang, and Zheng Liu. Omnigen2: Exploration to advanced multimodal generation. arXiv preprint arXiv:2506.18871, 2025. 2, 3, 7, 4, 5, 11, 12 [54] Shaojin Wu, Mengqi Huang, Wenxu Wu, Yufeng Cheng, Fei Ding, and Qian He. Less-to-more generalization: Unlocking more controllability by in-context generation. arXiv preprint arXiv:2504.02160, 2025. 2 [55] Xun Wu, Shaohan Huang, Guolong Wang, Jing Xiong, and Furu Wei. Boosting text-to-video generative model with MLLMs feedback. In The Thirty-eighth Annual Conference on Neural Information Processing Systems, 2024. 7, 3 [56] Bin Xia, Bohao Peng, Yuechen Zhang, Junjia Huang, Jiyang Liu, Jingyao Li, Haoru Tan, Sitong Wu, Chengyao Wang, Yitong Wang, Xinglong Wu, Bei Yu, and Jiaya Jia. Dreamomni2: Multimodal instruction-based editing and generation, 2025. 2, 3, 4, 6, 7, 5, 11, [57] Bin Xia, Yuechen Zhang, Jingyao Li, Chengyao Wang, Yitong Wang, Xinglong Wu, Bei Yu, and Jiaya Jia. Dreamomni: Unified image generation and editing, 2025. [58] Shitao Xiao, Yueze Wang, Junjie Zhou, Huaying Yuan, Xingrun Xing, Ruiran Yan, Shuting Wang, Tiejun Huang, and Zheng Liu. Omnigen: Unified image generation. arXiv preprint arXiv:2409.11340, 2024. 2, 11 [59] Zhipei Xu, Xuanyu Zhang, Runyi Li, Zecheng Tang, Qing Huang, and Jian Zhang. Fakeshield: Explainable image forgery detection and localization via multi-modal large lanIn International Conference on Learning guage models. Representations, 2025. 3 [60] Hu Ye, Jun Zhang, Sibo Liu, Xiao Han, and Wei Yang. Ipadapter: Text compatible image prompt adapter for text-toimage diffusion models. 2023. 2, 11 [61] Yang Ye, Xianyi He, Zongjian Li, Bin Lin, Shenghai Yuan, Zhiyuan Yan, Bohan Hou, and Li Yuan. Imgedit: unified image editing dataset and benchmark. arXiv preprint arXiv:2505.20275, 2025. 2, 3, 5, 1, 4, 12 [62] Qifan Yu, Wei Chow, Zhongqi Yue, Kaihang Pan, Yang Wu, Xiaoyang Wan, Juncheng Li, Siliang Tang, Hanwang Zhang, and Yueting Zhuang. Anyedit: Mastering unified high-quality image editing for any idea. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pages 2612526135, 2025. 3, 5 [63] Kai Zhang, Lingbo Mo, Wenhu Chen, Huan Sun, and Yu Su. Magicbrush: manually annotated dataset for instructionIn Advances in Neural Information guided image editing. Processing Systems, 2023. 2, 3, 5, 12 MultiBanana: Challenging Benchmark for Multi-Reference Text-to-Image Generation"
        },
        {
            "title": "Supplementary Material",
            "content": "A. Details on Benchmark Construction A.1. Details on Image Collection We constructed the reference image collection for our benchmark using both real images sourced from LAION5B [24] and synthetic images generated by Nano Banana [14] and GPT-Image-1 [34]. By combining real and synthetic data, we mitigated the categorical biases inherent in the real-image distribution while substantially expanding the coverage and diversity of the reference set. To generate synthetic images, we designed distinct axes of variation for four major categories: humans, animals, objects, and textcontaining images. Human Category We systematically combined attributes such as makeup, hairstyle, hair color, pose, strong facial expressions, lighting, and overall tonal style to create wide range of appearance conditions. Examples include ethnic or fashion-model-style makeup, vivid or unconventional hair colors, model-like or manga-like poses, strong emotional expressions (e.g., anger, joy, surprise), dramatic lighting such as window light through blinds or neon illumination, and global color tones such as sepia or bluish palettes. These conditions were intentionally selected to supplement variations that are underrepresented in real-world data. Animal Category We first specified major species groups (dogs, cats, birds, reptiles, wild animals, and fantasy animals). Then we added attributes to enhance the diversity specific to animal imagery. These included human-like behaviors (e.g., bear reading book, fox playing in rock band), rare colors (e.g., blue horse or pink panda), distinctive lighting conditions, and varied global tones such as sepia. This incorporated rare or extreme examples that seldom appear in real datasets. Object Category We expanded diversity along two axes: intrinsic object attributes and photographic conditions. The former included distinctive textures (e.g., denim, leather, knit), characteristic prints (e.g., floral, striped, checkered), and variations in material or design (e.g., matte, glossy, transparent). The latter included variations in lighting and global tonal adjustments (e.g., sepia, cool color tones), thereby ensuring broad range of appearance conditions for objects. Images Containing Text Images containing text, which were largely absent from real data, were generated exclusively using GPT-Image-1, as Nano Banana exhibited limited ability to render non-English text. We defined three levels of text length (a single word, short phrase of up to five words, and longer sentence). We combined each with two scene types: text rendered directly within landscape (e.g., ocean, forest) and text appearing naturally in the environment (e.g., on signs in real scenes). These six prompt patterns were generated in English and subsequently translated into Japanese and Chinese to enrich multilingual textcontaining images. Scene Diversity Finally, all synthetic prompts were designed to incorporate scene diversity, including European cityscapes, university campuses, tropical beaches, event halls, and other varied environments. The generated subjects were ensured to be well-framed, resulting in images suitable for use as high-quality reference material. A.2. Details on Image Filtering First, to remove images unsuitable for editing, the images were filtered based on the size of the foreground objects, following the manner in [61]. YOLOv12 [48] was applied to all images to detect objects and then performed semantic segmentation using SAM [41], conditioned on the detected bounding boxes. Images were discarded if they met any of the following criteria: the bounding box area covered less than 2% of the entire image, the segmented region within the bounding box covered less than 30%, or the CLIP similarity [26] between the YOLO-predicted class name and the bounding box region was below 20. These cases were judged to contain objects that were not clearly visible. However, images where no objects were detected were retained, assuming they are useful for background or style-level editing tasks. After this filtering, 48% of the images were retained. We also filtered out images that were inappropriate or unsuitable as references, such as unsafe content, charts, and screenshots of system messages, using both Gemini-based automated screening and human review. For inappropriate content, we specifically targeted categories including hate, harassment, violence, self-harm, sexual content, nudity, shocking content, illegal activity, and other distressing material, ultimately excluding approximately 3% of the collected images. We also manually removed synthetic data that was nearly identical. An example is shown in Figure 8. A.3. Details on Category Classification To construct multi-reference image editing tasks, each reference image had to be accurately categorized. For example, in portrait transformation tasks that modify persons hairstyle or makeup, both reference images must contain 1 person. Similarly, in replacement tasks involving humans or objects, the corresponding entities must be present in the reference images, and for background replacement tasks, background scenes must be determined in the reference images. To satisfy these task-dependent requirements, we performed hierarchical image classification on large combined set of real and synthetic images. In contrast, real For synthetic data, attributes can be specified directly during generation, eliminating the need for additional classification. images exhibit substantial variability, requiring precise categorization to determine whether they can serve as valid reference images. To address this, we designed structured annotation protocol using Gemini that consists of three classification processes for the human, object, and background components of an image. Human Image Classification Human classification begins with identifying how many people appear in the image. If there is precisely one person, the model additionally classifies attributes such as gender, makeup, hairstyle distinctiveness, pose clarity, and facial expression characteristics. The output follows fixed template format, allowing consistent extraction of appearance features relevant to multireference generation. Object Image Classification Object image classification follows similar structure. The model first determines whether the image contains no object, single object, or multiple objects. When exactly one object is detected, it is further categorized according to its visual and material properties, distinguishing among animal-like forms, patterned or decorative textiles, engineered or manufactured materials, and objects lacking distinctive design features. Background Image Classification Background classification decomposes the scene into realism, tonal uniformity, and lighting strength. Realism distinguishes photographic scenes from stylized or synthetic ones; tone indicates whether the overall color distribution is uniform or varied; and lighting strength identifies whether clear directional key light is present. Ambiguous lighting is conservatively labeled as absent to maintain annotation consistency. A.4. Details on Task Construction The editing instructions were generated using Gemini. First, we manually selected suitable reference categories for each task, for instance, the person category for the hairstyle modification task. Next, we randomly sampled images from each category and presented them to Gemini to produce corresponding editing instructions. For example, in editing tasks that require object transformations, we first prompted the model to determine which reference images object should be modified and which reference images attributes (e.g., color) should be used as the target. Afterward, Figure 8. Examples of duplicated synthetic image. 2 Figure 9. (Left) Number of tasks by reference count. The two-reference tasks contain more samples than the other reference tasks because they include multiple task types. (Right) Breakdown of the two-reference tasks. It contains eleven tasks. several instruction examples were provided to Gemini, and we asked Gemini to generate new instructions following the same syntax. After that, editing instructions that would cause visual breakdowns were removed by scoring them with Gemini. Additionally, the difficulty of editing instructions was also assessed by Gemini, and those judged easy, such as cases without domain differences, were removed. Finally, the editing instructions were manually verified not to cause image breakdowns or result in overly simple edits, and assigned the remaining samples to the appropriate difficulty categories (cross-domain, scale and viewpoint differences, rare concept, and multilingual text rendering). B. Further Statistics for MultiBanana The left of Figure 9 shows the number of tasks by each reference count. The two-reference tasks contain more samples than the other tasks because they include multiple task types. The right of Figure 9 shows the breakdown of the two-reference tasks. It contains eleven tasks considered in the prior work [56]: subject addition, subject replacement, background change, color modification, material modification, pose modification, hairstyle modification, makeup modification, tone transformation, style transfer, and text correction. Each task is ensured to include at least 6% of the editing samples, corresponding to roughly 60 samples, which is larger than the number of samples in Xia et al. [56]. ferences and compatibility among reference images. To illustrate that existing benchmarks are almost solved by state-of-the-art models such as Nano Banana [14] and GPTImage-1 [34], we evaluate the image generation capabilities of these models using ImgEdit [61], the latest benchmark for image editing, and DreamOmni2 [56], the state-of-theart benchmark for multi-reference image generation. The quantitative results show that recent closed-source, stateof-the-art models achieve consistently high scores, indicating that these benchmarks are nearing saturation and may soon be unable to meaningfully distinguish among highperforming models (Table 4 and Table 5). The evaluation results are taken from the official GitHub repository of ImgEdit1 and the results reported in the DreamOmni2 paper. Our qualitative results suggest that, even in the additional Hard category of ImgEdit, most tasks are already close to being fully solvable by advanced models such as Nano Banana (Figure 10). similar tendency is observed for DreamOmni2 benchmark as well (Figure 11). Taken together, these findings indicate that current benchmarks for image editing and multi-reference image generation are approaching their limits in evaluating cutting-edge models. There is clear need for more challenging benchmarks that can more effectively capture the capabilities and failure modes of the latest generation of image models. D. Further Discussions for AI and Human C. Further Comparison with Prior Bench-"
        },
        {
            "title": "Evaluation",
            "content": "marks Table 1 shows that existing benchmarks do not provide systematic evaluation across diverse multi-reference conditions, support only limited number of references, and fail to adequately account for important factors such as difThe correlation between AI and human evaluation has been shown in prior works [11, 32, 36, 55]. Consequently, using VLMs is becoming standard method for 1https://github.com/PKUYuanGroup/ImgEdit?tab= readme-ov-file Figure 10. Qualitative example of Hard category of ImgEdit. These tasks are close to being fully solvable by advanced models such as Nano Banana. Figure 11. Qualitative example of DreamOmni2 benchmark. These tasks are close to being fully solvable by advanced models such as Nano Banana. the automatic evaluation of generated images [53, 56, 61]. We used VLMs to evaluate the images along five metrics: Instruction Alignment, Reference Consistency, Background Subject Match, Physical Realism, and Visual Quality. The overall score is calculated as weighted average of these metrics. The first two metrics represent the fundamental components of reference-based generation, while the last three metrics break down the overall quality. Therefore, the overall score is calculated as avg(Instruction Alignment + Reference Consistency + avg(Background Subject Match + Physical Realism + Visual Quality)). To demonstrate the consistency of our evaluation metrics, we examined the correlation between different VLMs. We use the evaluated results for Nano Banana and GPTImage-1, and examine the correlation coefficient between GPTs and Geminis evaluation score. To ensure the reliability of the correlation coefficient, we divided the evaluation results into ten subsets and computed the average correlation across them. Table 6 shows that the evaluated scores exhibit positive correlations, which demonstrates that our evaluation criteria are well-defined and consistent. To further ensure the reliability of our evaluation metrics, we examined whether they can detect performance gains from fine-tuning on external image-editing datasets, thereby demonstrating that they align well with those used in other image-editing benchmarks. We compare QwenImage-Edit with Qwen-Image-Edit-2509, fine-tuned variant designed to improve reference consistency and support multi-reference inputs. As shown in Table 7, our evaluation metrics successfully capture the performance gains of Qwen-Image-Edit-2509 over Qwen-Image-Edit. We conducted this comparison using single-reference tasks because Qwen-Image-Edit supports only single input image. E. Detailed Results E.1. Further Discussion of Per-Task Evaluation Figure 12, Figure 13, and Figure 14 show the scores of each model across five evaluation metrics for each task, evaluated by GPT, Gemini, and their average, respectively. Taking the weighted average across five metrics, Figure 15 shows the overall score of each model for single and two-reference tasks, evaluated by GPT, Gemini, and their average, respectively. For multi-reference tasks, Table 8, Table 9, and Table 10 show the overall score, evaluated by GPT, Gemini, and their average, respectively. Overall, we observed substantial gap in Instruction Alignment and Reference Consistency between the closed-source models and the other opensource models. In particular, GPT-Image-1 achieves significantly superior performance in Instruction Alignment, especially in style and text modification tasks in the tworeference task. Meanwhile, in Reference Consistency, Nano Banana achieves the highest scores in the single-reference task. Nano Banana also performs on par with GPT-Image-1 in Reference Consistency, across background modification, subject replacement, subject addition, and makeup tasks, which indicates its strong ability to leverage reference images, particularly when the number of references is small. Additionally, Qwen-Image-Edit-2509, the most advanced open-source model, performs worse than the other models in background modification tasks in Background Subject Match and Visual Quality. This result suggests that it struggles with background modification. 4 Table 4. Comparison results of different models on ImgEdit-Bench [61]. Overall is computed by averaging the scores across all task types. GPT-4.1 [33] is used for evaluation. The results show that recent closed-source, state-of-the-art models achieve substantially higher scores, suggesting that the benchmark may be approaching its ceiling in distinguishing high-end models. Evaluation results excluding Nano Banana are taken from the official GitHub repository of Ye et al. [61]. Model Add Adjust Extract Replace Remove Background Style Hybrid Overall MagicBrush [63] AnyEdit [62] OmniGen2 [53] Kontext-dev [23] Nano Banana [14] GPT-Image-1 [34] 2.84 3.18 3.57 3.83 3.63 4. 1.58 2.95 3.06 3.65 4.66 4.33 1.51 1.88 1.77 2.27 3.73 2.90 1.97 2.47 3.74 4.45 4.69 4. 1.58 2.23 3.20 3.17 4.71 3.66 1.75 2.24 3.57 3.98 4.60 4.57 2.38 2.85 4.81 4.55 4.47 4. 1.62 1.56 2.52 3.35 4.10 3.96 1.83 2.45 3.44 3.71 4.37 4.20 Table 5. Quantitative comparison of multimodal instruction-based editing and generation in DreamOmni2 Benchmark [56]. Both tasks are evaluated using Gemini [13] and Doubao [3]. This result shows that recent closed-source, state-of-the-art models achieve substantially higher scores, indicating that the benchmark may be reaching its ceiling for distinguishing among high-end models. We refer to evaluation results from Xia et al. [56]. Method Concrete Abstract Concrete Abstract Editing Task Generation Task Gemini Doubao Gemini Doubao Gemini Doubao Gemini Doubao Omnigen2 [53] Kontext [23] Qwen-Image-Edit-2509 [52] DreamOmni2 [56] Nano Banana [14] GPT-Image-1 [34] 0.2195 0.0488 0.2683 0.5854 0.6829 0.6829 0.2927 0.1220 0.2927 0. 0.7073 0.7805 0.0427 0.0183 0.0488 0.5854 0.6463 0.7195 0.0793 0.0122 0.1159 0.6280 0.5488 0.7439 0.2083 0.2500 0.1250 0. 0.5000 0.6250 0.2500 0.3750 0.2917 0.6667 0.5417 0.6250 0.1000 0.0556 0.1111 0.5778 0.5556 0.6889 0.0778 0.1222 0.1556 0. 0.5488 0.6333 Table 6. Correlation coefficients between GPTs and Geminis evaluated scores. shows 95% confidence interval. Table 7. Comparison of Qwen-Image-Edit and Qwen-Image-Edit2509 on single-reference task. Evaluation criteria Correlation coefficients Qwen-Image-Edit Qwen-Image-EditInstruction Alignment Reference Consistency Background Subject Match Physical Reallism Visual Quality Overall 0.645 0.021 0.549 0.026 0.601 0.020 0.620 0.022 0.577 0.018 0.650 0.014 E.2. Further Discussion of the Effect of the Number of References According to the per-metric scores in Figure 14 and the overall scores in Table 10, all models exhibit decreasing performance as the number of reference images increases. For Instruction Alignment and Reference Consistency, GPT5 single 6.332 7.499 Image-1 and Nano Banana achieve relatively high scores, whereas the open-source model, DreamOmni2, tends to approach the minimum score of 1 as the number of references increases. In contrast, for quality-related metrics, Background Subject Match, Physical Realism, and Visual Quality, the DreamOmni2 maintains high scores even when more reference images are provided. As discussed in Section 4, this is because closed-source models prioritize adherence to the references and editing instructions over visual quality, while open-source models show tendency to preserve visual quality but often ignore the references. In the finer-grained task-level evaluation, we found that Figure 12. Scores of each model across evaluation metrics for each task by GPT. The horizontal axis denotes the scores for the five evaluation criteria, and the vertical axis denotes the number of reference images. 6 Figure 13. Scores of each model across evaluation metrics for each task by Gemini. The horizontal axis denotes the scores for the five evaluation criteria, and the vertical axis denotes the number of reference images. 7 Figure 14. Average scores of each model across evaluation metrics for each task by GPT and Gemini. The horizontal axis denotes the scores for the five evaluation criteria, and the vertical axis denotes the number of reference images. 8 Figure 15. Overall scores of each model for single and two-reference tasks by GPT, Gemini, and their average. Table 8. Detailed per-task overall scores for the multi-reference tasks, evaluated by GPT. The local, global, back, and object columns under correspond to X1 Objects + Local, X1 Objects + Global, X1 Objects + Background, and Object, respectively. Model 3-references 4-references 5-references local global back object local global back object local global back object DreamOmni2 OmniGen2 Qwen-Image-Edit-2509 Nano Banana GPT-image-1 3.94 4.42 4.42 5.44 6. 4.19 4.59 4.65 5.81 7.40 3.92 4.47 4.75 5.10 6.98 3.91 4.47 4.77 5.64 6.40 3.34 4.41 3.08 4.57 6.18 3.24 3.37 3.35 5.36 6.98 2.87 3.10 2.78 4.27 6. 3.20 3.27 3.54 5.25 6.56 3.30 3.78 1.93 5.12 6.18 3.06 3.28 2.40 5.93 6.66 2.73 2.87 1.98 3.96 6.22 3.22 3.31 2.10 4.93 6.28 Model 6-references 7-references 8-references local global back object local global back object local global back object DreamOmni2 OmniGen2 Qwen-Image-Edit-2509 Nano Banana GPT-image-1 3.37 - 1.85 4.43 6.16 3.03 - 2.18 4.88 6.08 2.82 - 1.68 3.89 5.68 2.96 - 1.79 4.90 5.90 3.24 - 1.51 4.40 5. 3.02 - 1.89 4.78 6.09 2.71 - 1.57 3.57 5.59 2.86 - 1.88 4.81 5.94 3.26 - 1.68 4.31 4.96 2.79 - 2.29 4.82 5.82 2.57 - 1.62 3.21 5. 2.82 - 1.83 4.45 5.15 even as the number of reference images increased, the X1 Object + Global task did not exhibit score degradation in Background Subject Match, Physical Realism, or Visual Quality. This is because the task requires modifying the image to specified style; therefore, even if the reference images come from different domains, which may degrade the visual quality, the final output converges to unified style. In the results of Qwen-Image-Edit-2509, the model is trained to handle up to four reference images; therefore, when provided with five or more references, the input becomes out-of-distribution. This causes the model to produce perceptually invalid outputs, and then the scores approach the minimum value of 1. E.3. Further Discussion of Difficult Reference Combination Figure 16 shows the scores of each model in difficult reference combination tasks across evaluation metrics. Cross-domain diversity In cross-domain cases, samples with reference images from different domains (darker color) were evaluated as lower Reference Consistency scores, because all models aim to unify the domains during generation. Scale and viewpoint differences In the different scale and viewpoint cases, the Reference Consistency scores declined consistently across all models. According to the reasoning traces of the VLM, the primary reasons for deductions are the loss of fine details and changes in pose. This suggests 9 Table 9. Detailed per-task overall scores for the multi-reference tasks, evaluated by Gemini. The local, global, back, and object columns under correspond to X-1 Objects + Local, X1 Objects + Global, X1 Objects + Background, and Object, respectively. Model 3-references 4-references 5-references local global back object local global back object local global back object DreamOmni2 OmniGen2 Qwen-Image-Edit-2509 Nano Banana GPT-image3.00 3.03 3.27 4.34 5.48 2.49 2.89 3.16 4.69 6.28 2.45 3.00 3.29 4.61 5.37 2.74 3.25 3.97 4.89 5.16 2.84 3.10 2.29 3.42 4.83 2.73 3.06 3.00 4.34 5. 2.38 2.27 2.15 3.79 4.75 2.44 2.73 2.79 4.14 5.04 2.63 2.84 1.14 4.25 4.89 2.50 3.02 1.69 4.50 5.20 2.36 2.41 1.21 3.16 4.25 2.66 2.50 1.20 3.85 4. Model 6-references 7-references 8-references local global back object local global back object local global back object DreamOmni2 OmniGen2 Qwen-Image-Edit-2509 Nano Banana GPT-image-1 2.47 - 1.00 3.25 3.66 2.54 - 1.26 3.91 4.66 2.14 - 1.00 2.55 3.46 2.36 - 1.03 3.79 4. 2.65 - 1.01 3.05 4.18 2.50 - 1.06 3.73 4.58 2.15 - 1.33 2.26 3.24 2.30 - 1.17 3.52 3.52 2.42 - 1.18 2.85 3.28 2.33 - 1.31 3.64 4. 2.04 - 1.01 2.53 2.91 2.19 - 1.00 3.25 2.96 Table 10. Detailed per-task overall scores for each multi-reference task, evaluated by the average of GPT and Gemini. The local, global, back, and object columns under correspond to X1 Objects + Local, X1 Objects + Global, X1 Objects + Background, and Object, respectively. Model 3-references 4-references 5-references local global back object local global back object local global back object DreamOmni2 OmniGen2 Qwen-Image-Edit-2509 Nano Banana GPT-image-1 3.47 3.73 3.85 4.89 6. 3.34 3.74 3.90 5.25 6.84 3.18 3.74 4.02 4.85 6.18 3.33 3.86 4.37 5.27 5.78 3.09 3.76 2.69 4.00 5.50 2.98 3.22 3.17 4.85 6.12 2.62 2.69 2.47 4.03 5. 2.82 3.00 3.17 4.70 5.80 2.97 3.31 1.54 4.69 5.54 2.78 3.15 2.04 5.22 5.93 2.54 2.64 1.60 3.56 5.24 2.94 2.91 1.65 4.39 5.19 Model 6-references 7-references 8-references local global back object local global back object local global back object DreamOmni2 OmniGen2 Qwen-Image-Edit-2509 Nano Banana GPT-image-1 2.92 - 1.42 3.84 4.91 2.78 - 1.72 4.39 5.37 2.48 - 1.34 3.22 4.57 2.66 - 1.41 4.35 4.96 2.94 - 1.26 3.72 4. 2.76 - 1.47 4.25 5.33 2.43 - 1.45 2.92 4.42 2.58 - 1.52 4.16 4.73 2.84 - 1.43 3.58 4.12 2.56 - 1.80 4.23 4.94 2.31 - 1.32 2.87 3. 2.50 - 1.42 3.85 4.05 that when models attempt to reproduce objects under different scales or viewpoints, they may fail to preserve fine details or may adjust the pose to achieve more natural appearance. Rare concept references In the rare concept case, because the reference images often contain subjects at large scale, instances were observed in which the model reproduced this scale without adjustment, even when it was too large. For this reason, rare concept samples tend to receive lower scores on the Physical Realism. Multilingual references In the multilingual text rendering case, we observed consistent decline across all metrics except Reference Consistency. This primarily occurs because models have limited text-rendering capability in languages other than English, leading to failures in crosslanguage conversion. As result, instruction alignment and visual quality become lower. However, scores dropping in GPT-Image-1 were lower than those of others, since it exhibits comparatively stronger text-rendering ability in nonEnglish languages. 10 E.4. Agentic Inference We introduced three agentic frameworks, Iterative Prompt Refinement (IPR), Context-Aware Feedback Generation (CAFG), and Selective Reference Adaptation (SRA). Here, we refer to Gen as the Generator, which produces an image, and Plan as the Planner, which updates the instruction prompt (and selects reference images) for the next step. Let denote the instruction prompt at step t, {Ri}i[I] the reference images, and Gt the generated image with G0 = . We conducted experiments with both GPT and Gemini. For GPT, we use GPT-Image-1 as the Generator and GPT-5 as the Planner. For Gemini, we use Nano Banana as the Generator and Gemini-2.5-Flash as the Planner. E.4.1. Iterative Prompt Refinement (IPR) IPR framework is formulated as follows. The Planner refines the prompt based on the generation result from the previous step. Gt+1 = Gen(P t, {Ri}i[I], ), t+1 = Plan(P t, {Ri}i[I], Gt+1). E.4.2. Context-Aware Feedback Generation (CAFG) CAFG framework is formulated as follows. The Generator generates the image based on the generation result (context) from the previous step. Gt+1 = Gen(P t, {Ri}i[I], Gt), t+1 = Plan(P t, {Ri}i[I], Gt+1). E.4.3. Selective Reference Adaptation (SRA) SRA framework is formulated as follows. The Planner selects only the reference images that should be improved based on the generation result from the previous step, and the Generator receives these as context. SRA is expected to reduce task complexity for the agent by adaptively decreasing the number of references. Gt+1 = Gen(P t, {Ri}iU t, Gt), t+1, t+1 = Plan(P t, {Ri}i[I], Gt+1). where is the index set of reference images that are insufficiently reflected in the generated image Gt. E.4.4. Experiments We set the maximum number of steps as 3 and we evaluate three agentic frameworksIPR, CAFG, and SRAusing Gemini, and the IPR framework using GPT. Figure 22 illustrates an example of step-wise improvement in the IPR framework. Figure 23, Figure 24, Figure 25, and Figure 26 present the evaluation results of multi-reference generations at each step, averaged across judgments from Gemini-2.5Flash and GPT-5, broken down by category. Figure 27 presents the results for single and two-reference generations. Nano Banan (Gemini) shows modest improvements in Physical Realism and Visual Quality across refinement steps, while Instruction Alignment and Reference Consistency either remain unchanged or deteriorate. In contrast, GPT demonstrates consistent improvements across all categories. This suggests that Geminis planner progressively loses information from the original prompt as refinement steps proceed. F. Extended Related Works F.1. Controllable Text-to-Image Generation Diffusion models have achieved remarkable progress in generative modeling, demonstrating exceptional performance, particularly in high-fidelity image synthesis. Representative systems, such as Stable Diffusion [9, 38, 42], FLUX [22, 23], DALL-E [40], and Imagen [19, 44], have demonstrated strong text-to-image generation capabilities, establishing controllable and scalable foundation for image generation. To enhance controllability, models such as ControlNet [64] and T2I-Adapter [31] introduced external conditioning modules, enabling image-conditioned generation. Recent large pretrained diffusion models with textual conditioning, such as Stable Diffusion [9, 38, 42] and FLUX [22, 23], enable the synthesis of novel scene images while preserving the identity depicted in the reference images. Building on these developments, approaches based on fine-tuning (e.g., DreamBooth [43]) and adapter-based methods (e.g., IP-Adapter [60]) have been actively explored to address this challenge [2, 12, 20, 25, 29, 47]. Additionally, training-free approaches (e.g., pix2pix-zero [37] and prompt-to-prompt [15]) have also been widely proposed [4, 18, 21, 28, 49, 50]. These advancements collectively demonstrate that diffusion models are becoming increasingly flexible and adaptable across diverse conditional generation settings. In recent years, research on unified image generation has gained momentum, aiming to handle multiple image generation tasks within single model. For example, ChatGPTImage-1 [34] and Nano Banana [14] are capable of jointly processing text and image inputs, enabling integrated image generation and editing within unified framework. Similarly, the open-source models, such as Qwen-Image [52] and FLUX.1-Kontext-dev [23], built upon robust multimodal large language model backbone, demonstrate highquality and flexible image generation and editing performance. Beyond these, numerous open-source unified generative models continue to emerge [53, 5658]. With the advent of multimodal foundation models capable of jointly handling text and images [14, 34], new task has recently become feasible: multi-reference-based image generation. In this task, the model receives multiple refer11 ence images along with textual instructions and generates new scene while preserving the identity of the reference images. However, despite its novelty and potential, reliable benchmark for evaluating this innovative task has yet to be established, leaving it as an essential open challenge for future research. F.2. Benchmarks for Reference-Based Generation Among reference-driven image generation tasks, the most widely recognized one is image editing. Image editing refers to the task of generating an edited image based on single reference image and textual instruction describing how the edit should be performed. Representative benchmarks such as MagicBrush [63] and EMU-Edit [46] have extended task-specific evaluations but still rely heavily on similarity-based metrics. SmartEdit [17] supports editing of complex scenes, but does not sufficiently cover more general settings. Similarly, I2E-Bench [27] employs GPT-4o to provide human-aligned evaluations across variety of editing tasks, yet it uses different metrics for each task and therefore fails to capture the shared characteristics of editing as whole. More recently, ImgEdit [61] has enabled the evaluation of multi-turn editing tasks, where image generation and editing are alternated interactively. On the other hand, one of the earliest benchmarks for reference-based image generation beyond editing is the DreamBooth [43] dataset. However, such benchmarks are still limited to generation tasks conditioned on single reference image and thus cannot handle more complex or compositional settings. In recent years, following advances demonstrated by GPT-Image-1 [34] and Nano Banana [14], increasing attention has been directed toward multi-reference image generation, where multiple reference images are used jointly for generation. Benchmarks such as OmniContext [53] and DreamOmni2 [56] have been proposed to evaluate this task. Nevertheless, these benchmarks encompass only small number of images and task types, thereby limiting the scope of the evaluation. Moreover, despite being designed for multi-reference generation, they do not evaluate qualitative differences or relationships between reference images, and their setups remain similar to those of conventional singleimage editing benchmarks. G. Prompts G.1. Prompts of Agentic Framework Prompt of the Generator in the IPR framework"
        },
        {
            "title": "Prompt of the Planner in the IPR framework",
            "content": "You are prompt-refiner agent. Previous prompt: {previous prompt} You are given multiple reference images and one generated image. The generated image is the last one, and the reference images come before it. Based on the reference images and the generated image (the last image), propose refined prompt that improves how reference images are used and adjusts the composition/style. Return only the new prompt text. {reference image files}{previously generated image file}"
        },
        {
            "title": "Prompt of the Generator in the CAFG framework",
            "content": "{prompt} The last image provided is the previously generated image from the last step and all images before it are reference images. Please refine the previous generation using the reference images and enhance composition/style. {reference image files}{previously generated image file} Prompt of the Planner in the CAFG framework You are prompt-refiner agent. Previous prompt: {previous prompt} You are given multiple reference images and one generated image. The generated image is the last one, and the reference images come before it. Based on the reference images and the generated image (the last image), propose refined prompt that improves how reference images are used and adjusts the composition/style. Return only the new prompt text. {reference image files}{previously generated image file} Prompt of the Generator in the SRA framework {prompt} The last image provided is the previously generated image from the last step and all images before it are reference images. Please refine the previous generation using the reference images and enhance composition/style. {prompt}{reference image files} {reference image files}{previously generated image file}"
        },
        {
            "title": "Prompt of the Planner in the SRA framework",
            "content": "You are prompt-refiner and reference-selector agent. Previous prompt: {previous prompt} You are given multiple reference images (in order) and one generated image (the last one). Your task: 1. Identify which reference images are insufficiently reflected in the generated image. 2. Propose refined prompt that improves the generation. 3. Return your response in JSON format ONLY. Do not include any other text. JSON format: { indices: [0, 2, 3], prompt: Your refined prompt text here } Where indices is an array of 0-based indices (from 0 to {len(reference image files)-1}) of reference images that are insufficiently reflected in the generated image. prompt is the refined instruction prompt for the next generation step. Example response: { indices: [0, 2, 3], prompt: Focus more on the lighting from the first image and the composition from images 3 and 4. Emphasize the color palette and texture details. } {reference image files}{previously generated image file} 13 Figure 16. Overall scores for each difficult reference combination across models. Darker colors represent the average score of tasks that include the corresponding combination, while lighter colors indicate the average score of tasks that do not include it. Figure 17. Qualitative example for 4-Object Tasks. 15 Figure 18. Qualitative example for 3-Object + Background Tasks. 16 Figure 19. Qualitative example for 3-Object + Local Tasks. Figure 20. Qualitative example for 3-Object + Global Tasks. 18 Figure 21. Qualitative example for Tasks with 8 references. 19 Figure 22. Example of changes in the Iterative Prompt Refinement (IPR) framework. Comparison and evaluation results between Nano Banana and GPT on the task of generating images from 8 object references according to the instruction prompt. IPR Framework with Gemini Figure 23. Detailed results of multi-reference image generation using the IPR framework with Gemini. 20 CAFG Framework with Gemini Figure 24. Detailed results of multi-reference image generation using the CAFG framework with Gemini. SRA Framework with Gemini Figure 25. Detailed results of multi-reference image generation using the SRA framework with Gemini. 21 IPR Framework with GPT Figure 26. Detailed results of multi-reference image generation using the IPR framework with GPT. 22 Figure 27. Detailed results of single and two-reference image generation. G.2. Prompts of VLM as Judge Multi-Reference Image Genearation Evaluation You are strict data rater specializing in grading multi-reference driven image generation. You will be given reference images, task instruction, and the generation results. Reference Images: {reference image files} Editing Instruction: {instruction} Final Output: {generated image files} Your task is to evaluate the effectiveness of replacement editing from five independent perspectives, each on 10point scale. Note that the average score should be considered 4 points. 1. Text-Instruction Alignment Evaluate whether the generated image accurately follows the given text instruction. Check whether the specified objects appear in the correct positions, whether the instructed subjects are depicted properly, and whether no unintended elements are introduced. For example, if the instruction says change the language, but the actual written content itself is altered incorrectly, or if unnecessary objects are added, the score should be reduced. If the instruction requires including reference subject but the generated image fails to include that referenced content, the score must be 1. Even if the instruction is followed correctly, the score must not exceed 6 points if the generated image still exhibits any composited or unnatural appearance. 2. Reference Consistency Evaluate how consistent the generated image is with the provided reference images. Compare the output to each reference and assess how faithfully the structure and attributes are reproduced. Fine details, such as hair ornaments, patterns on clothing, and other small features, must match the references; otherwise the score must not exceed 4 points. If even single object fails to follow the details of the reference images, the score must not exceed 6 points. 3. Background-Subject Match Evaluate whether the subject blends naturally with the background. Check whether the subject appears to be floating, unnaturally pasted on, or visually inconsistent with its surroundings. Images that look like multiple pictures simply pasted together should receive score of 1. If there is even the slightest inconsistency in style, tone, lighting, or overall visual impression compared to the reference images, the score must also not exceed 4 points. 4. Physical Realism Evaluate whether the generated image maintains physical plausibility. Penalize cases where the image violates basic physical lawsfor example, person floating in mid-air, standing on water, or having the lower body missing despite no obstruction. If there is even slight impression that the image looks composited or artificially pasted together, the score must not exceed 4 points. Likewise, if it is unclear whether the subject is actually making proper contact with the ground, the score must also not exceed 6 points. 5. Visual Quality Evaluate the overall perceptual quality of the image. Assess whether the image is visually appealing and aesthetically coherent. If the composition appears unnatural or the image does not look aesthetically pleasing to human observer, the score must not exceed 4 points. Each of the five scores must be evaluated independently. Do not force any score to be tied to or capped by another score. First, explain the reasoning, then present the final assessment. Start the reasoning with Reasoning: . After explaining the reasoning, present the final assessment in the format: Instruction Alignment: number from 1 to 10. Reference Consistency: number from 1 to 10. Background-Subject Match: number from 1 to 10. Physical Realism: number from 1 to 10. Visual Quality: number from 1 to 10."
        }
    ],
    "affiliations": [
        "Google DeepMind",
        "The University of Tokyo"
    ]
}