{
    "paper_title": "CORG: Generating Answers from Complex, Interrelated Contexts",
    "authors": [
        "Hyunji Lee",
        "Franck Dernoncourt",
        "Trung Bui",
        "Seunghyun Yoon"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "In a real-world corpus, knowledge frequently recurs across documents but often contains inconsistencies due to ambiguous naming, outdated information, or errors, leading to complex interrelationships between contexts. Previous research has shown that language models struggle with these complexities, typically focusing on single factors in isolation. We classify these relationships into four types: distracting, ambiguous, counterfactual, and duplicated. Our analysis reveals that no single approach effectively addresses all these interrelationships simultaneously. Therefore, we introduce Context Organizer (CORG), a framework that organizes multiple contexts into independently processed groups. This design allows the model to efficiently find all relevant answers while ensuring disambiguation. CORG consists of three key components: a graph constructor, a reranker, and an aggregator. Our results demonstrate that CORG balances performance and efficiency effectively, outperforming existing grouping methods and achieving comparable results to more computationally intensive, single-context approaches."
        },
        {
            "title": "Start",
            "content": "CORG: Generating Answers from Complex, Interrelated Contexts Hyunji Lee κ Franck Dernoncourt α Trung Bui α Seunghyun Yoon α κ KAIST AI hyunji.amy.lee@kaist.ac.kr α Adobe Research {dernonco, bui, syoon}@adobe.com 5 2 0 A 5 2 ] . [ 1 3 2 0 0 0 . 5 0 5 2 : r a"
        },
        {
            "title": "Abstract",
            "content": "In real-world corpus, knowledge frequently recurs across documents but often contains inconsistencies due to ambiguous naming, outdated information, or errors, leading to complex interrelationships between contexts. Previous research has shown that language models struggle with these complexities, typically focusing on single factors in isolation. We classify these relationships into four types: distracting, ambiguous, counterfactual, and duplicated. Our analysis reveals that no single approach effectively addresses all these interrelationships simultaneously. Therefore, we introduce CONTEXT ORGANIZER (CORG), framework that organizes multiple contexts into independently processed groups. This design allows the model to efficiently find all relevant answers while ensuring disambiguation. CORG consists of three key components: graph constructor, reranker, and an aggregator. Our results demonstrate that CORG balances performance and efficiency effectively, outperforming existing grouping methods and achieving comparable results to more computationally intensive, single-context approaches."
        },
        {
            "title": "Introduction",
            "content": "In real-world documentsranging from blog posts and news articles to official records or usergenerated contentthe same knowledge often appears in multiple forms, sometimes with consistency but often with variations or conflicts. These discrepancies can arise from ambiguous phrasing, outdated data, or simple errors. When analyzing these contexts, we find relationships between them generally fall into four categories as shown in Figure 1: distracting, ambiguous, counterfactual, or duplicated. Each entity consists of surface name, general term that can be ambiguous, and descriptor that provides specificity. For instance, in The Simpsons (Season 2) contains 22 episodes, Work performed during internship at Adobe Research. Figure 1: In real-world corpora, contexts often exhibit complex interrelationships, which we classify into four categories: distracting, counterfactual, duplicated, and ambiguous. The Simpsons is the surface name, and Season 2 is the descriptor that specifies the entity. Based on these attributes, we classify contexts as distracting (same surface name, different descriptors), ambiguous (same surface name, only one with descriptor), counterfactual (same entity with differing answers), or duplicated (same entity with identical answers). Current research often simplifies real-world complexities by treating corpora as unified sources (e.g., fixed Wikipedia versions), where knowledge appears consistently without cross-document conflicts. Many studies also address isolated factors rather than considering such complex interrelationships between contexts (Min et al., 2020; Lee et al., 2024c; Xu et al., 2024; Zhang and Choi, 2021). Additionally, finding relevant contexts using webbased retrieval methods often introduces search engine biases, resulting in limited diversity (Lee et al., 2024a; Gezici et al., 2021). To bridge this gap, we expand existing corpora to better reflect such complex interrelation between contexts in realworld conditions. Specifically, we introduce AmbigDocs+ and ConflictQA+ extensions of AmbigDocs (Lee et al., 2024c) and ConflictQA (Zhou et al., 2023)where we construct additional contexts based on the (question, answer, context) pairs, ensuring coverage of all four conditions. When analyzing the effect of each factor, we observe performance drop, particularly when ambiguous or counterfactual contexts are added, consistent with prior findings (Lee et al., 2024c; Zhou et al., 2023; Lee et al., 2023). Our investigation reveals that no single simple solution addresses such contexts in complex relationships simultaneously; although simple methods exist for individual factors, they often fail to generalize across scenarios. Interestingly, we found that straightforward solution for distracting contexts is to modify the question to plural form. For ambiguous contexts, adding or replacing missing descriptors to create distracting relationship can improve clarity. However, these approaches are less effective for counterfactual contexts, where separating them into different forward passes appears to yield better performance. To address these challenges, we introduce CONTEXT ORGANIZER (CORG), framework designed to improve performance on real-world corpora through simple, efficient approach based on insights from individual solutions. CORG prioritizes three objectives: (1) high answer recall, (2) accurate disambiguation, and (3) minimal inference runs. For cases with multiple answers, CORG generates responses that include all relevant answers with citations, allowing users to review and filter information as needed. CORG comprises three components: the graph constructor, which represents context relationships; the reranker, which organizes contexts into scenario-based groups; and the aggregator, which generates responses with citations for each group. We conduct experiments using eight language models of different sizes and observe that CONTEXT ORGANIZER consistently improves performance over six baselines on both AmbigDocs+ and ConflictQA+, which contain multiple factors, as well as on AmbigDocs and ConflictQA, which each contain only single factor. CONTEXT ORGANIZER notably excels in entity recall, measuring the models ability to identify disambiguated entities. Even large models show low disambiguation performance when simply processed without CORG. Additionally, grouping similar contexts without structured processing, as in CORG, tends to reduce performance: groups with only similar contexts seem to confuse the model more than simply appending diverse contexts together. We hope our analysis of these real-world corpus scenarios encourages the community to explore the unique effects and solutions for each factor in greater depth."
        },
        {
            "title": "2 Complex, Interrelated Contexts",
            "content": "We analyze real-world contexts and observe that the relationship between contexts can be categorized into four types: distracting, ambiguous, counterfactual, and duplicated. In Section 2.1, we define these four categories, followed by an analysis of their occurrence in real-world web corpora in Section 2.2. In Section 2.3, we describe how we extend an existing dataset to incorporate all four relationship types, and in Section 2.4, we share details of evaluation metrics."
        },
        {
            "title": "2.1 Definition of Relationships",
            "content": "When given corpus = {c1, , cN }, where each context ci consists of multiple sentences, we define the relationship between the contexts by breaking down the information in each sentence into three components: surface name si, descriptor di, and answer ai. For example, in Doc2 of Figure 1, the sentence The Simpsons, Season 2, contains 22 episodes is parsed as follows: The Simpsons is the surface name (a general, potentially ambiguous entity), Season 2 is the descriptor (specific detail that disambiguates between entities with the same surface name), and 22 episodes is the answer. An answer exists only when the context is relevant to the question; otherwise, the answer is considered null. To determine relevance between question and context, given question about an entity eq and descriptor dq with the corpus C, if dq is null, relevant contexts include all contexts where ei = eq. If dq is not null, relevant contexts are limited to those where both ei = eq and di = dq. When given question and two contexts relevant to the question, ci and cj, we can extract information (ei, di, ai) and (ej, dj, aj) from each context where eq = ei = ej. Based on the extracted info, the four relationships between contexts are defined as: Ambiguous case: di = dj with either di = ull or dj = ull Distracting case: di = dj with di = ull and dj = ull Counterfactual case: di = dj and ai = aj. Duplicated case: di = dj and ai = aj. where ull indicates that it is empty."
        },
        {
            "title": "ConflictQA",
            "content": "Original New (+) Original New (+)"
        },
        {
            "title": "Y\nY\nY\nY",
            "content": "Table 1: Overview of datasets: AmbigDocs and ConflictQA include single factor, while AmbigDocs+ and ConflictQA+ incorporate all four factors."
        },
        {
            "title": "2.2 Statistics of Real-World Corpora",
            "content": "To understand the structure of real-world corpora, we analyze the relationship between the top 10 contexts retrieved for questions from AmbigDocs (Lee et al., 2024c) using the Bing API1. Among these, we found an average composition of 25.2% ambiguous, 34.7% duplicated, 12.4% conflicting, and 27.7% distracting relationship, underscoring the need to address all four factors together rather than in isolation. Moreover, only 32.7% of the diverse answers from AmbigDocs questions were covered, indicating potential retrieval bias in search engines (Lee et al., 2024a; Gezici et al., 2021). To avoid this limitation and ensure balanced representation of all factors, we extend corpus instead of relying solely on web-crawled contexts, with further details provided in the next section. For details on how the statistics were obtained, see Appendix A.3."
        },
        {
            "title": "2.3 Corpus Construction",
            "content": "To evaluate LLMs in real-world scenarios, we expand existing datasets with additional contexts based on (question, answer, contexts) pairs so that the corpus with related contexts for each question contains all four factors. We use AmbigDocs (Lee et al., 2024c), which includes distracting contexts, and dataset from Zhou et al. (2023), which we call ConflictQA, containing counterfactual contexts  (Table 1)  . We construct extended versions, AmbigDocs+ and ConflictQA+, by adding ambiguous, conflicting, and duplicated contexts to AmbigDocs and distracting, ambiguous, and duplicated contexts to ConflictQA. Specifically, AmbigDocs provides question with related contexts where the question references an entity without specific descriptor. Each context pair consists of sub-question with descriptor and corresponding answer. To add counterfactual 1https://www.microsoft.com/en-us/bing/apis/bing-websearch-api contexts, we select sub-question and for each answer in pairs, instruct GPT-4 (Achiam et al., 2023) to generate contexts. The duplicated and ambiguous contexts are generated by providing the model with the answer with sub-question or question, respectively. For ConflictQA, which includes questions with counterfactual contexts often lacking descriptors, we use GPT-4 to generate plausible sub-questions with descriptors to match similar format with AmbigDocs. Distracting contexts are created for each sub-question and answer, while duplicated and ambiguous contexts follow the same process as in AmbigDocs. After generating the corpus, we apply two filtering processes: (1) inclusion of answer in the generated context and (2) whether GPT-4o answers sub-questions correctly when given the context. If either fails, we regenerate with GPT-4 with the issue added until it passes both filters. We then hire five freelancers to evaluate random 10% sample of AmbigDocs+, assessing whether (1) generated contexts are relevant to the question, (2) answers are accurate and present within the document, and (3) the corpus represents the expected variety of real-world context relationships. We achieve average ratings of 93.4%, 89.0%, and 84.6% across each criterion, indicating high quality. Further details on calculation methods, context generation, and dataset statistics are provided in Appendix A."
        },
        {
            "title": "2.4 Evaluation Metric",
            "content": "Following Lee et al. (2024c), we assess the model on AmbigDocs with four metrics. Entity Recall (Ent), calculates the average token-level recall for descriptor. Answer Recall (Ans), measures the average token-level recall for each correct answer. Entity-Answer Recall (EAR) averages the product of entity recall and answer recall to measure how well the model generates both answers with their corresponding descriptor. Disambig-F1 (D-F1) (Stelmakh et al., 2022) is model-based metric that assesses answer recall by comparing the models response to the correct answer; the response is generated by RoBERTa-based QA model trained on SQuAD-v2, with sub-questions as input. For ConflictQA, we report only Answer Recall (Ans) and Disambig-F1 (D-F1), as descriptors for each context are not labeled."
        },
        {
            "title": "Ans",
            "content": "EAR D-F1 52.5 48.4 53.0 68.9 53.0 61.4 56.3 67.9 36.8 37.6 38.3 42.7 21.7 18.2 27.6 28."
        },
        {
            "title": "One Shot\nExtra Prompt\nPlural",
            "content": "Table 2: Performance of Llama2-7B using only contexts with distracting relationships. Changing the question to plural format shows the highest improvement."
        },
        {
            "title": "Ans",
            "content": "EAR D-F1 31.0 34.4 40.7 45.1 52.5 49.6 52.2 50.9 51.9 53.0 29.9 32.0 31.5 33.2 36.8 17.2 18.3 18.4 20.1 21.7 Table 3: Performance of Llama2-7B using only contexts with ambiguous relationships. Changing the relationship to distracting one shows the highest performance. multiple distracting contexts, typically selecting just one answer instead of generating all possible answers. To address this, we explore three approaches to inform the model about multiple answers: (1) adding prompt2 indicating that the question may be ambiguous and could have multiple answers, (2) providing an example in the input, and (3) changing the questions tense from singular to plural3. Surprisingly, pluralizing the question yielded the greatest improvement, as shown in Table 24. This was followed by the additional prompt and the one-shot example, though the latter sometimes degraded performance, likely due to the model drawing on knowledge from the example. However, despite these enhancements, the model still struggles to consistently generate all possible answers and disambiguate between them and their corresponding entities. Ambiguous Context While the difference between distracting and ambiguous contexts is minor, whether context with an empty descriptor is included, their impact varies significantly; distracting contexts generally show less performance degradation than ambiguous ones. Thus, as shown in Table 3, we conducted experiments where when contexts with ambiguous relationships are given, we replace the context with empty distractor to an2The question may be ambiguous, thereby containing multiple answers 32019 World Ice Hockey Championships host country? 2019 World Ice Hockey Championships host countries? 4Simply pluralizing every question is ineffective; when only single context is present, plural forms often lead the model to generate multiple answers unintentionally. Figure 2: Answer recall as the number of contexts increases for each factor. For ambiguous cases, note that since ambiguity can only exist between two contexts, sets with three or more contexts (x > 2) also include distracting relationships."
        },
        {
            "title": "3 Analyzing Solution for each Factor",
            "content": "In this section, we analyze how adding the context of each factor to the input affects model performance (Section 3.1) and investigate simple solution when looking at each factor individually (Section 3.2). All evaluation in this section is performed with the Llama2 7B model (Touvron et al., 2023)."
        },
        {
            "title": "3.1 Affect of each factor",
            "content": "To assess the impact of each factor, we sample 1k instances from AmbigDocs+. For each question, we analyze how adding context to each relationship affects overall model performance. Figure 3 presents the performance trends as contexts reflecting each factor are added. Our analysis reveals that adding duplicated contexts has minimal impact on overall performance. Introducing distracting contexts results in slight performance drop, though not as significant as it is easy for LLM to distinguish entities when they have different descriptors. However, the inclusion of counterfactual and ambiguous contexts leads to the most substantial performance degradation. These contexts typically cause the model to generate answers that cover only subset of possible responses, rather than providing comprehensive set of answers."
        },
        {
            "title": "3.2 Solution for Each Factors individually",
            "content": "In this section, we investigate solutions for each factor individually based on the observation in the above section. Distracting Context Previous works (Zhang and Choi, 2021; Lee et al., 2024c) have shown that models often struggle to answer questions with"
        },
        {
            "title": "Ans",
            "content": "EAR D-F1 30.1 36.4 29.4 34.8 51.2 44.6 51.0 45.2 50.5 85.9 28.6 32.6 27.1 30.7 40.8 18.0 20.4 18.7 19.8 26."
        },
        {
            "title": "One Shot\nExtra Prompt\nPlural\nSeparation",
            "content": "Table 4: Performance of Llama2-7B with only contexts in counterfactual relationship. Separation yields the highest performance, while other cases show small improvement. other context that has the same content but with descriptor5. In other words, shifting ambiguous relationships to distracting ones. This replacement method (Change to Distracting) yields the highest performance, outperforming other approaches like providing examples, prompts, or rephrasing questions in plural format. Counterfactual Contexts Previous works indicate that language models tend to favor contexts aligning with their parametric knowledge when presented with multiple counterfactual contexts (Chen et al., 2022; Xie et al., 2023; Lee et al., 2023), particularly struggling as the number of conflicting contexts grows (Jin et al., 2024). We first test whether solutions effective for distracting or ambiguous contexts also improve performance in counterfactual contexts. Results in Table 4 show that these solutions yield smaller improvements in counterfactual cases. While prior studies propose heavy pipelines to resolve such conflicting cases (Wu et al., 2022; Hsu et al., 2021), as our approach aims to address not only conflicting case but all four context types together; thus, we explore simple method by processing each context individually, observing significant gains with separated contexts. Duplicated Contexts The results in Figure 3 indicate that adding duplicated contexts generally does not impact overall performance. However, we observe slight performance drop, likely due to longer input. Thus, for both performance and efficiency, retaining only single instance of duplicated contexts seems to be good approach. The choice of which duplicated context to keep does not seem to make notable difference. 5Please note due to data construction method of AmbigDocs+, all contexts without descriptor is replaceable to corresponding document with descriptor, but we also consider the case where it is irreplaceable, which we further discuss in Section 4."
        },
        {
            "title": "4 CONTEXT ORGANIZER",
            "content": "In this section, we introduce CONTEXT ORGANIZER (CORG), framework designed for realworld corpora, based on observations in Section 3.2. As shown in Figure 3, CORG consists of three components: graph constructor, reranker, and an aggregator. The framework aims to achieve (1) high answer recall, (2) strong disambiguation, and (3) efficiency through reduced inference runs. Graph Constructor The Graph Constructor component constructs graph from list of contexts, where each node represents context and each edge indicates the relationship between contexts. We employ GPT-4 to identify these relationships. To efficiently capture all relationships6, we use an iterative approach as in Algorithm 1. Initially, we extract relationships between context 1 and the remaining contexts (line 6-10 in Algorithm 1). If two contexts are classified as counterfactual or duplicated, their relationships with other contexts will mirror each other (line 12-16). For instance, if context 1 and context 2 are counterfactual, then context 2s relationships with the remaining contexts will reflect context 1s relationships. For other relationships, this mirroring only applies in counterfactual or duplicated cases (lines 1725). Such nodes which have missing edges are processed in subsequent iterations. This approach minimizes redundant checks and reduces the number of iterations by focusing only on missing edges. Reranker The Reranker component organizes contexts into groups and removes unnecessary ones based on the constructed graph and solutions for each relationship type outlined in Section 3.2. First, for contexts in distracting relationship, when context with descriptor is available, we remove the one without it. Next, for duplicated contexts, we select one randomly. Last, counterfactual contexts are separated into distinct groups, and the remaining contexts are distributed evenly across groups. When groups contain multiple contexts, the question is reformulated in plural format. This systematic grouping aligns with relationship types, enhancing coherence and response accuracy. 6Here, we distinguish ambiguous into two types: whether context contains the same information as another context (and is therefore interchangeable) or not, allowing us to apply the solution outlined in Section 3.2. We consider contexts that are interchangeable as having an \"ambiguous\" relationship. Further details are provided in Appendix C. Figure 3: Overview of CONTEXT ORGANIZER (CORG), composed of three components: graph constructor, reranker, and aggregator. Given corpus with multiple relevant contexts and question, the graph constructor and the reranker organize the corpus based on the question, and the aggregator generates response containing all possible answers with references. Algorithm 1 Graph Constructor"
        },
        {
            "title": "5 Experiments",
            "content": "1: Input: List of contexts = [c0, c1, . . . , cn] 2: Initialize empty graph {} 3: while is not empty do C[0]; [] 4: for each node ci in C[1 :] do 5: 6: 7: 8: 9: 10: 11: 12: 13: 14: 15: 16: 17: 18: 19: 20: 21: REL(ci, c) G.add((c, ci, r), (ci, c, r)) if in [\"counter\", \"dup\"] then for all (c, cj, rj) do G.add((c, ci, rj), (ci, c, rj)) end for else for all (c, cj, rj) do if rj in [\"counter\", \"dup\"] then G.add((c, ci, rj), (ci, c, rj)) else .append(ci) end if end for end if end for (with duplicates removed) 22: 23: end while 24: Output: fully connected graph Aggregator The Aggregator component processes each group sequentially, generating responses and aggregating them with citations from the source contexts. This allows users to assess the origin of each response, offering transparency and supporting user judgment about the provided information. Since retrieval models may occasionally retrieve inaccurate information, and language models often struggle to verify document reliability, this approach gives users all relevant details with evidence, enabling them to make informed decisions. In this section, we share the experimental setup (Section 5.1), six baselines (Section 5.2), experimental results (Section 5.3), and analysis over efficiency (Section 5.4)."
        },
        {
            "title": "5.1 Setup",
            "content": "We evaluate various baselines across AmbigDocs+, ConflictQA+, AmbigDocs, and ConflictQA datasets, with eight models of varying sizes using the metric described in Section 2.4. We consider D-F1 as our primary metric as it captures the presence of the surface name, descriptor, and answer information, but we also report the performance of other metrics. Experiments are conducted on 14 A100 80GB GPUs at the models maximum lengths. Following Lee et al. (2024c), we select Llama2 (Touvron et al., 2023), Mistral (Jiang et al., 2023), ChatGPT, and additionally include recent models Llama3 (Dubey et al., 2024) and GPT-4o7. Refer to Appendix D.1 for more details."
        },
        {
            "title": "5.2 Baselines",
            "content": "We evaluate five baselines to assess how different context-handling strategies impact performance. Base inputs all relevant contexts at once, resulting in long input lengths per question. Retrieve and Summarize also run in single inference but aim to reduce input length. Retrieve ranks contexts based on relevance and answer diversity, similar to Min et al. (2021), while Summarize is inspired by Xu et al. (2023), focusing on efficiency and performance by summarizing contexts. We use GPT-4 for both ranking and summarization. Random and KMeans group contexts in the same count as CORG but differ in approach: Random assigns groups randomly, while KMeans clusters contexts via BERT embeddings (Devlin et al., 2019). Sepa7reference to ChatGPT and GPT-4o rate processes each context individually. See Appendix D.2 for more details of baselines. In this study, we assume that the relevant paragraphs are pre-retrieved to remove the factor of retrieval error and focus specifically on generation ability. We leave the integration of retrieval from large corpora as future work."
        },
        {
            "title": "5.3 Results",
            "content": "Table 5 shows the overall Disambig-F1 (D-F1) across various models on the AmbigDocs+ and ConflictQA+ datasets. Our results demonstrate that CORG consistently improves performance over six baselines across eight different models. It achieves the highest performance among methods that use grouping-based inference and is comparable to Separate, which processes each context individually, though at notably higher computational cost. Table 10 in the Appendix shows similar trend in AmbigDocs and ConflictQA, datasets composed of contexts with single relationship. Among the single-inference methods (Base, Retrieve, Summarize), Summarize achieves the highest performance due to its shorter input length that retains key details. However, it still struggles with handling complex relationships between contexts in single inference run, resulting in lower performance than grouping-based methods. Grouping-based methods (Random, KMeans, CORG) apply inference over context groups, underscoring the importance of the grouping strategy. Random grouping outperforms single-inference methods due to multiple inference runs, but lower performance than CORG. KMeans performs worse than Random, likely because clustering similar contexts makes it challenging for the model to generate answers with specific entity descriptors, reducing D-F1. Retrieve shows similar trend to KMeans, as both methods group similar contexts together. Table 6 shows that while both have high answer recall, they have particularly low entity recall. Such results emphasize the importance of the grouping method of CORG. Separate, which processes each context individually, demonstrates performance comparable to or exceeding CORG but requires significantly more computation, as explored in the following section. Table 6 highlights that Separate achieves high answer recall but lower entity recall, which we assume is due to processing single contexts without the broader exposure to varied descriptors. This Figure 4: Trade-off between efficiency and performance for Llama2-7B in AmbigDocs+. Light color indicates better balance between the two; CORG shows the best followed by Summarize. lack of descriptor emphasis limits effective entity disambiguation, whereas methods processing multiple contexts expose models to various entities, supporting clearer differentiation. Interestingly, larger models do not always outperform smaller ones. As shown in Table 6, while larger models generally achieve high answer recall, they often overlook descriptors, resulting in lower entity recall and overall disambiguation performance. However, generally more advanced language models, such as API and recent models, tend to exhibit stronger overall performance. See Appendix D.3 for more details."
        },
        {
            "title": "5.4 Efficiency",
            "content": "Figure 4 shows the tradeoff between efficiency and performance for the Llama2-7B model on the AmbigDocs+ dataset8. CORG achieves the best balance, as it filters and groups contexts thus avoid processing all of them and have shorter responses. Summarize also performs efficiently by reducing input length through summarization. Retrieve and KMeans demonstrate the lowest FLOPs, as their outputs tend to be concise, containing only the answers. In contrast, Base and Separate are less efficient: Base uses long inputs and generates lengthy responses, while Separate produces responses for each context, increasing response length. For more details, see Appendix D.4."
        },
        {
            "title": "6 Related Works",
            "content": "Various studies highlight incorporating external knowledge as solution for hallucination and ex8Please note that FLOPs calculations here include only input-response processes, excluding additional steps like summarization, clustering, retrieval, or graph construction, as these are performed only once per dataset. Number of Inference Run: Single Grouping Individual Methods: Base Retrieve Summarize Random KMeans CORG Separate AmbigDocs+ ConflictQA+ Llama2 Llama Mistral GPT Llama2 Llama3 Mistral GPT 7B 13B 70B 8B 70B 7B ChatGPT GPT-4o 7B 13B 70B 8B 70B 7B ChatGPT GPT-4o 17.0 16.4 15.0 17.7 10.6 22.9 19.2 19. 27.8 30.3 26.7 16.9 18.3 29.4 32.9 32.4 4.2 4.0 11.2 15.2 9. 18.2 17.9 17.0 5.8 1.5 9.1 16.3 14.0 22.9 27.6 22. 18.8 17.2 15.5 18.3 12.0 23.7 24.3 23.9 28.1 30.7 29.2 17.7 17. 31.2 31.2 33.7 19.4 14.3 15.7 20.4 11.7 24.0 21.6 25. 29.1 30.6 17.0 17.5 21.4 30.9 32.5 35.1 3.6 2.3 5.8 12.8 8. 14.1 10.6 11.1 19.3 14.4 9.5 17.0 10.2 26.9 21.7 23. 22.0 19.9 17.9 21.4 14.6 27.5 29.0 31.4 32.1 30.8 31.4 22.8 28. 31.6 35.9 38.3 13.5 14.1 18.7 20.5 22.4 28.8 27.3 32. 27.0 37.5 40.1 25.6 35.2 30.3 37.1 38.1 Table 5: Overall Performance of AmbigDocs+ and ConflictQA+ with DF-1. The best and second best of each model in bold and underline respectively Ans Ent EAR D-F1 7B Single Grouping Base Retrieve Summarize Random KMeans CORG 51.4 44.1 60.0 56.7 42.6 61.4 39.5 16.3 42.1 35.8 14.2 41. 25.7 7.9 24.8 27.4 6.3 34.0 13B 70B Individual Separate 66.9 41.6 37.3 Single Grouping Base Retrieve Summarize Random KMeans CORG 48.8 46.0 60.4 55.9 40.1 58.3 31.3 14.9 39.0 25.6 11.8 39.3 19.3 7.5 21. 19.4 4.0 29.4 Individual Separate 81.5 48.1 35. Single Grouping Base Retrieve Summarize Random KMeans CORG 60.5 50.0 61.9 58.9 38.6 63. 37.1 27.9 31.6 26.1 17.6 37.2 27.5 20.2 26.0 20.9 8.0 31.7 Individual Separate 89.0 35.7 27.3 17.0 4.2 18.8 19.4 3.6 22.0 13. 16.4 4.0 17.2 14.3 2.3 19.9 14.1 15.0 11.2 15.5 15.7 5.8 17.9 18. Table 6: Performance of Llama2 with various model size in AmbigDocs+ with detailed metrics panding the models capability. However, performance is dependent on the relationships among input contexts. Some works focus on ambiguous questions involving multiple relevant contexts (Min et al., 2020; Lee et al., 2024c), while others examine counterfactual or conflicting contexts relative to model knowledge (Chen et al., 2022; Longpre et al., 2021; Lee et al., 2023; Zhou et al., 2023; Xie et al., 2023). Yet, real-world corpora often feature more complex relationships, which this paper categorizes into four types, revealing gap in prior solutions for managing such complexities efficiently. Some research leverages web-based corpora that might capture these relationships. However, we noticed that many of these works often rely on single sources, like Wikipedia, where each fact is likely to appear only once (Lee et al., 2024c; Longpre et al., 2021; Min et al., 2020) or find relevant contexts using search engine API, which introduce bias to retrieve similar rather than diversely related contexts (Yao et al., 2022; Schick et al., 2024; Hao et al., 2024). In this work, we focus on the realworld challenge of complex relationships among input contexts, and analyze the impact of each factor. We further propose CORG, framework that achieves high performance and efficiency across diverse real-world cases without additional training. Unlike previous works that use graph-based methods (Edge et al., 2024; Sarmah et al., 2024), CORG constructs graph where each node represents an entire document, and edges capture relationships between documents. In contrast, prior approaches focus on building graphs with nodes representing entities and edges capturing relationships within single document. This distinction arises because our objective is to help LLMs better understand and process interconnected documents, whereas previous works focus on modeling relationships within individual documents. To the best of our knowledge, this is the first work to explore graph-based modeling for complex, interrelated documents."
        },
        {
            "title": "Acknowledgements",
            "content": "We thank Nishant Balepur, Vishakh Padmakumar, Dang Nguyen, Yoonjoo Lee, Zoey Ki, Paiheng Xu, and the people at Adobe Research for helpful discussions and constructive feedback."
        },
        {
            "title": "References",
            "content": "This work investigates the characteristics of realworld corpora, categorizing them into four types and examining their effects and solutions. We find that no single solution addresses all four factors. Based on the observations, we introduce CONTEXT ORGANIZER (CORG), framework consisting of three componentsgraph constructor, reranker, and aggregatordesigned to achieve high answer recall, effective disambiguation, and efficiency in real-world corpora with complex relationships between contexts. It is applicable to any model without requiring additional training. Experiments across four datasets with eight models demonstrate that CORG consistently enhances performance, outperforming six baselines and achieving the best trade-off between efficiency and effectiveness."
        },
        {
            "title": "8 Limitations",
            "content": "Since our primary focus is on enhancing the language models response accuracy, we control for retrieval model bias and error by pre-selecting relevant contexts for each question. This ensures that the answer always exists within the provided contexts. To maximize the knowledge available to users, we have the model generate all possible answers, including counterfactual ones, even if some may contain incorrect knowledge. Rather than having the model filter for correctness, we provide references to relevant documents, allowing users to make informed choices. Additionally, given our goal to design flexible pipeline that can easily adapt and benefit any newly released language model, we concentrate on optimizing performance at the inference stage instead of training new models. This approach supports broader applicability and immediate benefits with future models. We leave the exploration of training-based methods for handling complex interrelationships between answer contexts as future work. Josh Achiam, Steven Adler, Sandhini Agarwal, Lama Ahmad, Ilge Akkaya, Florencia Leoni Aleman, Diogo Almeida, Janko Altenschmidt, Sam Altman, Shyamal Anadkat, et al. 2023. Gpt-4 technical report. arXiv preprint arXiv:2303.08774. Hung-Ting Chen, Michael J.Q. Zhang, and Eunsol Choi. 2022. Rich knowledge sources bring complex knowledge conflicts: Recalibrating models to reflect conflicting evidence. In Conference on Empirical Methods in Natural Language Processing. Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. 2019. Bert: Pre-training of deep bidirectional transformers for language understanding. In North American Chapter of the Association for Computational Linguistics. Abhimanyu Dubey, Abhinav Jauhri, Abhinav Pandey, Abhishek Kadian, Ahmad Al-Dahle, Aiesha Letman, Akhil Mathur, Alan Schelten, Amy Yang, Angela Fan, et al. 2024. The llama 3 herd of models. arXiv preprint arXiv:2407.21783. Darren Edge, Ha Trinh, Newman Cheng, Joshua Bradley, Alex Chao, Apurva Mody, Steven Truitt, and Jonathan Larson. 2024. From local to global: graph rag approach to query-focused summarization. arXiv preprint arXiv:2404.16130. Gizem Gezici, Aldo Lipani, Yucel Saygin, and Emine Yilmaz. 2021. Evaluation metrics for measuring bias in search engine results. Information Retrieval Journal, 24:85113. Shibo Hao, Tianyang Liu, Zhen Wang, and Zhiting Hu. 2024. Toolkengpt: Augmenting frozen language models with massive tools via tool embeddings. Advances in neural information processing systems, 36. Cheng-Mao Hsu, Cheng te Li, Diego Sáez-Trumper, and Yi-Zhan Hsu. 2021. Wikicontradiction: Detecting self-contradiction articles on wikipedia. 2021 IEEE International Conference on Big Data (Big Data), pages 427436. Albert Jiang, Alexandre Sablayrolles, Arthur Mensch, Chris Bamford, Devendra Singh Chaplot, Diego de las Casas, Florian Bressand, Gianna Lengyel, Guillaume Lample, Lucile Saulnier, et al. 2023. Mistral 7b. arXiv preprint arXiv:2310.06825. Zhuoran Jin, Pengfei Cao, Yubo Chen, Kang Liu, Xiaojian Jiang, Jiexin Xu, Qiuxia Li, and Jun Zhao. 2024. Tug-of-war between knowledge: Exploring and resolving knowledge conflicts in retrieval-augmented language models. ArXiv, abs/2402.14409. Hyunji Lee, Sejune Joo, Chaeeun Kim, Joel Jang, Doyoung Kim, Kyoung-Woon On, and Minjoon Seo. 2023. How well do large language models truly ground? NACCL 2024. Hyunji Lee, Luca Soldaini, Arman Cohan, Minjoon Seo, and Kyle Lo. 2024a. Routerretriever: Exploring the benefits of routing over multiple expert embedding models. arXiv preprint arXiv:2409.02685. Jinhyuk Lee, Anthony Chen, Zhuyun Dai, Dheeru Dua, Devendra Singh Sachan, Michael Boratko, Yi Luan, Sébastien MR Arnold, Vincent Perot, Siddharth Dalmia, et al. 2024b. Can long-context language models subsume retrieval, rag, sql, and more? arXiv preprint arXiv:2406.13121. Yoonsang Lee, Xi Ye, and Eunsol Choi. 2024c. Ambigdocs: Reasoning across documents on different entities under the same name. COLM 2024. S. Longpre, Kartik Perisetla, Anthony Chen, Nikhil Ramesh, Chris DuBois, and Sameer Singh. 2021. Entity-based knowledge conflicts in question answering. ArXiv, abs/2109.05052. Sewon Min, Kenton Lee, Ming-Wei Chang, Kristina Toutanova, and Hannaneh Hajishirzi. 2021. Joint passage ranking for diverse multi-answer retrieval. arXiv preprint arXiv:2104.08445. Sewon Min, Julian Michael, Hannaneh Hajishirzi, and Luke Zettlemoyer. 2020. AmbigQA: Answering ambiguous open-domain questions. In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP). Bhaskarjit Sarmah, Benika Hall, Rohan Rao, Sunil Patel, Stefano Pasquali, and Dhagash Mehta. 2024. Hybridrag: Integrating knowledge graphs and vector retrieval augmented generation for efficient information extraction. In International Conference on AI in Finance. Timo Schick, Jane Dwivedi-Yu, Roberto Dessì, Roberta Raileanu, Maria Lomeli, Eric Hambro, Luke Zettlemoyer, Nicola Cancedda, and Thomas Scialom. 2024. Toolformer: Language models can teach themselves to use tools. Advances in Neural Information Processing Systems, 36. Ivan Stelmakh, Yi Luan, Bhuwan Dhingra, and Ming-Wei Chang. 2022. Asqa: Factoid quesarXiv preprint tions meet long-form answers. arXiv:2204.06092. Xiangchen Wu, Xi Niu, and Ruhani Rahman. 2022. Topological analysis of contradictions in text. Proceedings of the 45th International ACM SIGIR Conference on Research and Development in Information Retrieval. Jian Xie, Kai Zhang, Jiangjie Chen, Renze Lou, and Yu Su. 2023. Adaptive chameleon or stubborn sloth: Revealing the behavior of large language models in knowledge conflicts. Fangyuan Xu, Weijia Shi, and Eunsol Choi. 2023. Recomp: Improving retrieval-augmented lms with compression and selective augmentation. arXiv preprint arXiv:2310.04408. Rongwu Xu, Zehan Qi, Cunxiang Wang, Hongru Wang, Yue Zhang, and Wei Xu. 2024. Knowledge conflicts for llms: survey. arXiv preprint arXiv:2403.08319. Shunyu Yao, Jeffrey Zhao, Dian Yu, Nan Du, Izhak Shafran, Karthik Narasimhan, and Yuan Cao. 2022. React: Synergizing reasoning and acting in language models. arXiv preprint arXiv:2210.03629. Michael J.Q. Zhang and Eunsol Choi. 2021. Situatedqa: Incorporating extra-linguistic contexts into qa. ArXiv, abs/2109.06157. Wenxuan Zhou, Sheng Zhang, Hoifung Poon, and Muhao Chen. 2023. Context-faithful prompting for large language models. In Conference on Empirical Methods in Natural Language Processing. Complex, Interrelated Contexts A.1 Example of AmbigDocs+ with each relationship Examples in Table 7 illustrate the four types of relationships between contextsambiguous, counterfactual, duplicated, and distractingwhen compared to main context. In the ambiguous context, no descriptor is provided, whereas the main context includes the descriptor IIHF. For the counterfactual context, while the main context states that the event was held in Slovakia, the counterfactual context claims it was jointly hosted in Canada and British Columbia. The duplicated context matches the main context across all information, including surface name, descriptor, and answer. In the distracting context, both the main and distracting contexts contain different descriptorsIIHF for the main and junior for the distracting context. Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, et al. 2023. Llama 2: Open foundation and fine-tuned chat models. arXiv preprint arXiv:2307.09288. A.2 Human Evaluation of AmbigDocs+ We recruited five freelancers using platform to evaluate 10% of randomly sampled examples from AmbigDocs+. The instructions, provided in Figure 5, directed evaluators to answer three questions Table 7: Examples contexts containing each relationship given question 2019 World Ice Hockey Championships host country? Relationship Context The 2019 IIHF Ice Hockey World Championship will be held in Slovakia, as confirmed by the IIHF on May 15, 2015. This marks the second occasion that Slovakia will host the championship as an independent nation. Similar to the 2011 event, the host cities will be Bratislava and Košice. The preliminary round seedings are determined by the 2018 IIHF World Ranking, following the conclusion of the 2018 IIHF World Championship. On May 22, 2018, the IIHF and the local organizing committee revealed the groups for the tournament. The 2019 IIHF World Ice Hockey Championships were hosted by Slovakia. The tournament was the 83rd such event hosted by the International Ice Hockey Federation. Teams from around the world descended upon Slovakia to compete in the prestigious competition. The event took place in two cities, Bratislava and Kosice, from May 10 to May 26. This marked the second time that Slovakia has hosted the World Ice Hockey Championships, with the first occasion being in 2011. The tournament featured some of the best players from around the globe and was significant event in the international hockey calendar 2019 World Junior Ice Hockey Championships The 2019 IIHF World Junior Championship (\"2019 WJHC\") will be the 43rd Ice Hockey World Junior Championship. It will begin on December 26, 2018, and will end with the gold medal game being played on January 5, 2019. This will mark the 13th time that Canada has hosted the IHWJC. On December 1, 2016, it was announced that Vancouver and Victoria, British Columbia had won the bid to host the 2019 World Juniors. It will be the second time that Vancouver has been the primary host of the tournament and the first time that The 2019 World Ice Hockey Championships organized by the International Ice Hockey Federation (IIHF) saw joint hosting by Canada and British Columbia, specifically in the cities of Vancouver and Victoria. These cities were selected due to the excellent infrastructure and passionate fan base present there. The championships kicked off in the spring of 2019 and concluded later that year. It was grand spectacle, with teams from all over the world competing for the prestigious title. This championship further solidified Vancouver and Victorias reputations as prime locations for hosting international sporting events. The 2019 World Ice Hockey Championships were hosted by Slovakia. Teams from around the world descended upon Slovakia to compete in the prestigious competition. The event took place in two cities, Bratislava and Kosice, from May 10 to May 26. This marked the second time that Slovakia has hosted the World Ice Hockey Championships, with the first occasion being in 2011. The tournament featured some of the best players from around the globe and was significant event in the international hockey calendar. Duplicated Distracting Counterfactual Ambiguous assessing dataset quality: (1) whether the generated contexts are relevant to the question, (2) if the answers are accurate and present within the document, and (3) whether the corpus captures the expected variety of real-world context relationships. To ensure that the freelancers understood the task, we divide the task into two step of first finishing 10% of the task and checking before continuing over the rest. For Q1, 93.4% of the contexts were considered relevant, with most irrelevance attributed to cases where the context contains too detailed descriptor compared to the question. For Q2, all answers are present within the provided contexts, and 89.0% of human responses align with the original answer, as verified through GPT-4o to account for potential wording variations. For Q3, 84.6% of examples were rated as containing all four relationship factors, with 10.3% having three factors and 5.1% having fewer than three. Each multi-context question took an average of 7 minutes to evaluate. We compensated freelancers at an average rate of 350 dollars for completing 100 instances. As AmbigDocs+ and ConflictQA+ contain generated contexts, we acknowledge the potential risks involved. We asked the freelancers to check for any issues, but since all sources are drawn from publicly released datasets, they reported no findings. A.3 Statistics of Real-World Corpora To investigate the statistics of real-world corpora, we conduct an experiment using all questions in the AmbigDocs dataset. For each question, we retrieve the top 10 documents using the Bing API. On these retrieved documents, we apply the same procedure as the graph constructor (prompt in Figure 9) to calculate the statistics for each relationship type. Specifically, for each question, we determine the composition rate of each relationship type and then average these rates across all questions. To evaluate the diversity of answers captured by the AmbigDocs questions (32.7%), we compute the answer recall across all retrieved documents with list of annotated answers in AmbigDocs. For example, if question has five annotated answers, we check whether each answer is present in any of the retrieved documents. If at least one document contains given answer, we consider that answer as covered. The coverage rate for each question is calculated as the number of answers covered divided by the total number of annotated answers."
        },
        {
            "title": "Instruction to Human Evaluators",
            "content": "Definitions: When given context and question, we can extract three things from the context. Answer: answer to the question from the given context Entity: entity related to the answer in the simplest form Descriptor: specific detail of the entity that distinguishes it from other entities. If not given, it is Null For example, when given context containing sentence feature-length film, The Simpsons Movie, was released in theaters worldwide on July 27, 2007, to critical and commercial success, with sequel in development as of 2018. and question When was The Simpsons released?, the answer to the question is July 27, 2007, the entity is The Simpsons and the descriptor is Movie. When sentence is Since The Simpsons debut on December 17, 1989, 769 episodes of the show have been broadcast., the answer to the question is December 17, 1989, the entity is The Simpsons, and the descriptor is Null since there is no specific descriptor in the sentence. We define whether the context is relevant to the question by: * If the question has descriptor, both the descriptor and entity of context should be same as question. * If the question does not have descriptor, only the entity of the context and the question should be the same. For those that the context is relevant to the question, we divide the document relationship into four and each case are defined as: * Ambiguous: different descriptor with either of the two being Null * Distracting: different descriptor with neither of the two being Null * Counterfactual: same descriptor with different answer * Duplicated: same descriptor with same answer For all cases, we consider that the entity is the same as in the question as the contexts are relevant to the question. ** List of Contexts ** (We leave it empty since it is too long) Q1. Check if each context is relevant to the question. Question: 2019 World Ice Hockey Championships host country? Simply write Yes if you think it is relevant and No if you think it is not relevant. Q2. For the questions you consider relevant in Q1, provide an answer to the question for each context. For the irrelevant ones, please write No, and if the answer doesnt exist in the context, please write Null. Question: 2019 World Ice Hockey Championships host country? Q3. Identify which of the four defined document relationships (distracting, duplicated, counterfactual, ambiguous) exist within the corpus and write all that exists. Write the each relationship that exists in each cell. Figure 5: Instruction to Human Evaluators A.4 Details of corpus construction To evaluate the LLMs ability with corpora representing real-world scenarios, we generate additional contexts based on existing datasets consisting of (question, answer, contexts) pairs. We use AmbigDocs (Lee et al., 2024c), which contains distracting contexts, and dataset released from Zhou et al. (2023), which includes counterfactual contexts as shown in Table 1. For simplicity, we name the dataset released from Zhou et al. (2023) as ConflictQA. Based on the information provided in each dataset, we add ambiguous, conflicting, and dupli-"
        },
        {
            "title": "Prompt to Generate Context",
            "content": "Given question and an answer, generate an evidence context consisting of 6-7 sentences. The purpose of the context is for people to read and answer the question. The answer and information in the context do not have to be true. Question: how many episodes are in chicago fire in season 4? Answer: 103 Context: The fourth season of Chicago Fire , an American drama television series with executive producer Dick Wolf , and producers Derek Haas , Michael Brandt , and Matt Olmstead , was ordered on February 5 , 2015 , by NBC , and premiered on October 13 , 2015 and concluded on May 17 , 2016 . The season contained 103 episodes . Question: how many episodes are in chicago fire? Answer: 103 Context: Chicago Fire is gripping American drama television series comprised of 103 episodes that delves into the lives of the firefighters, rescue personnel, and paramedics of Firehouse 51 of the Chicago Fire Department. The series offers an inside look at the professional and personal challenges faced by these brave men and women as they risk their lives to save others. The show captures the intense camaraderie, complex relationships, and high-stakes situations that define their everyday existence. With compelling mix of action, drama, and emotional depth, Chicago Fire provides an authentic and engaging portrayal of life on the front lines of emergency response. Figure 6: Prompt to Generate Context cated contexts to AmbigDocs, and add distracting, ambiguous, and duplicated contexts to ConflictQA, which we name as AmbigDocs+ and ConflictQA+, respectively. ({(q In detail, AmbigDocs contains pairs of question and list of contexts relevant to the question Cq. The question asks about an entity with surface name but without descriptor. The list Cq contains pairs of sub-questions and answer ai where the sub-question asks about the entity with the same surface name but with descriptor and the answer ai is the answer to both question and i, ai) = 0, . . . , }). For each question q, to add counterfactual contexts in Cq, we randomly select sub-question and for each possible answer from the list except for the correct one, we instruct GPT-4 to generate contexts that could produce the correct answer given the subquestion and answer (Counterfactual contexts = {LLM(q i, aj) = 0, . . . , N, = i}). For the duplicated case, we provided the model with sub-question and answer pairs to create matching contexts (Duplicated contexts = {LLM(q j, aj) = 0, . . . , }). For the ambiguous case, we provide the model with question and answer (Ambiguous contexts = {LLM(q, aj) 0, . . . , }). = ConflictQA contains pairs of question q9 along with list of relevant contexts Cq. Unlike AmbigDocs, the contexts in Cq are counterfactual to each other and often lack descriptors. To align with AmbigDocss structure, we use GPT-4 to generate plausible sub-questions with descriptors for each answer in Cq (prompt in Figure 7). Using these generated sub-questions, we add distracting contexts to Cq by generating context for each sub-question and answer (Distracting contexts = {LLM(q j, aj) = 0, . . . , }). For the duplicated and ambiguous contexts, we follow the same process as in AmbigDocs. Figure 6 shows the prompt used to generate contexts. A.5 Statistics of Dataset Figure 8 illustrates the number of contexts for each question. Typically, there are eight contexts per question, with maximum of 25 contexts observed. The AmbigDocs and AmbigDocs+ datasets each contain 6,000 evaluation samples, while the Con9Most questions do not include descriptor, but those that do were revised by humans to remove it. Prompt to Generate Sub-Questions for ConflictQA Given question and list of answers, generate detailed question for each of the given answer should be answer to the question respectively. Please note that the number of given answers should be the same with generated detailed questions and the answer should NOT be in the generated question. question: who proposed evolution as the basis of biological development? answers: Jodie Foster, Mara Jade, Yeh Raaste Hain Pyaar Ke, Billy Joel, Rigg detailed questions: who proposed evolution in 1859 as the basis of biological development? // who proposed evolution in 1863 as the basis of biological development? // who proposed evolution as the basis of biological development in 1871? // who proposed evolution as the basis of biological development in 1921? // who proposed evolution as the basis of biological development in 1951? question: who sings gim me some lovin in days of thunder? answers: AB de Villiers, UMBC, Nashville Predators // who redetailed questions: who first sings gim me some lovin in days of thunder? make gim me some lovin in days of thunder? // who sings gim me some lovin in days of thunder part 2? question: how many episodes of grey anatomy? answers: 501, detailed questions: how many episodes of greys anatomy season 14? episodes of greys anatomy season 12? // how many Figure 7: Prompt to Generate Sub-Questions for ConflictQA"
        },
        {
            "title": "B Analyzing Solution for each Factor",
            "content": "B.1 Ablation Studies on Distracting and"
        },
        {
            "title": "Ambiguous Relationships",
            "content": "Combining pluralizing the question with an additional prompt (Plural & Extra Prompt) generally leads to further improvements in model performance. Additionally, separating all contexts (Separation) indicates significant enhancement, particularly in answer recall. However, processing all contexts separately tends to be computationally intensive making it impractical."
        },
        {
            "title": "C CONTEXT ORGANIZER",
            "content": "C.1 Graph Constructor Figure 8: Statistics showing the number of questions (y-axis) against the number of contexts per question (x-axis). Most cases have eight contexts for each question. flictQA and ConflictQA+ datasets include 1,000 evaluation samples. Figure 9 shows the prompt to GPT-4 to extract the relationship between contexts thus constructing graph."
        },
        {
            "title": "Input Format for Graph Constructor",
            "content": "When given context and question, we can extract three things from the context. Answer: answer to the question from the given context Entity: entity related to the answer in the simplest form Descriptor: specific detail of the entity that distinguishes it from other entities. If not given, it is Null For example, when given context containing sentence feature-length film, The Simpsons Movie, was released in theaters worldwide on July 27, 2007, to critical and commercial success, with sequel in development as of 2018. and question When was The Simpsons released?, the answer to the question is July 27, 2007, the entity is The Simpsons and the descriptor is Movie. When sentence is Since The Simpsons debut on December 17, 1989, 769 episodes of the show have been broadcast., the answer to the question is December 17, 1989, the entity is The Simpsons, and the descriptor is Null since there is no specific descriptor in the sentence. For those that the context is relevant to the question, we divide the document relationship into four and each case are defined as: * Ambiguous: different descriptor with either of the two being Null and with same answer * None: different descriptor with either of the two being Null and with different answer * Distracting: different descriptor with neither of the two being Null * Counterfactual: same descriptor with different answer * Duplicated: same descriptor with same answer When given contexts, could you generate relation from Context1 to rest of the contexts? - [Context1] Title: 2019 IIHF World Championship Text: The 2019 World Ice Hockey Championships were notably held in Canada, specifically in the province of British Columbia. The event took place in the vibrant cities of Vancouver and Victoria, ... The choice of Canada, nation with rich hockey heritage, underscored the significance of the tournament. By hosting the championships in Vancouver and Victoria, Canada once again demonstrated its central role in the world of ice hockey. [Context2] Title: 2019 IIHF World Championship Text: 2019 IIHF World Championship The 2019 IIHF Ice Hockey World Championship is scheduled to be hosted by Slovakia, as announced by the IIHF on 15 May ... The seedings in the preliminary round are based on the 2018 IIHF World Ranking, as of the end of the 2018 IIHF World Championship. On 22 May 2018, the IIHF and the local organizing committee announced the groups, in which [Context3] Title: 2019 IIHF World Championship Text: The 2019 World Ice Hockey Championships were held in two different countries, France and Hungary. These nations played significant role in hosting the event, ... and manage large-scale sporting event. This joint hosting effort helped promote the sport within their borders and offered memorable experience for all participants. Question: 2019 World Ice Hockey Championships host country? Relations: Context2 - None Context3 - Counterfactual Figure 9: Input format to graph constructor"
        },
        {
            "title": "Ans",
            "content": "EAR D-F1 GPT-4o: gpt-4o"
        },
        {
            "title": "One Shot\nExtra Prompt\nPlural",
            "content": "Plural & Extra Prompt Separation 52.5 48.4 53.0 68.9 65.6 64.7 53.0 61.4 56.3 67.9 72.0 84.7 36.8 37.6 38.3 42. 51.5 54.5 21.7 18.2 27.6 28.0 30.6 30.4 Table 8: Performance of Llama2-7B using only contexts with distracting relationships."
        },
        {
            "title": "One Shot\nExtra Prompt\nPlural\nChange to Distracting",
            "content": "w/ plural and Extra Prompt Separation"
        },
        {
            "title": "Ans",
            "content": "EAR D-F1 31.0 34.4 40.7 45.1 52.5 51.6 55.7 49.6 52.2 50.9 51.9 53.0 77.0 85.6 29.9 32.0 31.5 33.2 36. 40.5 45.0 17.2 18.3 18.4 20.1 21.7 26.6 28.4 Table 9: Performance of Llama2-7B using only contexts with ambiguous relationships. When constructing graph, we classify ambiguous relationships into two types: contexts that contain the same information as another context thus making them interchangeable, and those that do not. This classification allows us to apply the solution outlined in Section 3.2. Contexts that are interchangeable are labeled as having an ambiguous relationship, while those with no shared information are classified as having None relationship, indicating no connection between them. Since single context can relate to multiple contexts, the None relationships can often be disregarded and processed alongside other relationships beforehand."
        },
        {
            "title": "D Experiments",
            "content": "D.1 Model Choice We utilized models released on HuggingFace for our experiments. Below, we provide the detailed links and versions of the models used. Llama2: meta-llama/Llama-2-7b-chat-hf, metameta-llama/Llama-2-13b-chat-hf, llama/Llama-2-70b-chat-hf Llama3: meta-llama/Llama-3.1-8B-Instruct, meta-llama/Llama-3.1-70B-Instruct Mistral: mistralai/Mistral-7B-Instruct-v0.2 ChatGPT: gpt-3.5-turbo GPT-4: gpt-4 D.2 Baselines We evaluate over five baselines to evaluate how different approaches to operating contexts impact performance. Base represents model that inputs all relevant contexts simultaneously, resulting in long input lengths for each question. The Retrieve method employs GPT-4 to rank contexts based on not only similarity but also diversity, inspired by the work of Min et al. (2021). This approach aims to reduce the number of contexts by removing unnecessary ones and executing single inference run, following Lee et al. (2024b). We utilize the top five ranked contexts for retrieval10. The Summarize approach builds on the idea of summarizing input contexts to enhance efficiency and maintain sufficient performance (Xu et al., 2023). We utilize GPT-4 to summarize the relevant contexts, which are then employed to generate responses. Both Random and KMeans group contexts into the same number of groups as CORG, though they differ from CORG in grouping methods: Random groups contexts randomly, while KMeans clusters contexts based on BERT embeddings (Devlin et al., 2019). This setup ensures the same group counts, isolating the effect of the grouping technique itself. Separate treats each context individually, processing one per inference run and then concatenating all outputs. While this method reduces input length per run, it increases overall output length and time cost as it requires multiple runs. In this study, we assume that the relevant paragraphs for each question are pre-retrieved to remove the factor of retrieval error and focus specifically on how the language models generation can be improved. We leave the integration of retrieval from large corpora with generation modeling as future work. D.3 Results Performance on Corpora with Single-Factor Relationships Table 10 presents the performance of CORG and baseline models on the AmbigDocs and ConflictQA datasets, which consist of corpora containing only distracting and counterfactual relationships, respectively. We observe that for both 10As the model and code for Min et al. (2021) are not available, and studies have shown that long-context models perform well in retrieval tasks (Lee et al., 2024b), we use GPT-4 for ranking. Number of Inference Run: Single Grouping Individual Methods: Base Retrieve Summarize Random KMeans CORG Separate AmbigDocs ConflictQA Llama2 Llama Mistral GPT Llama2 Llama3 Mistral GPT 7B 13B 70B 8B 70B 7B ChatGPT GPT-4o 7B 13B 70B 8B 70B 7B ChatGPT GPT-4o 20.0 18.4 17.0 19.7 10.6 25.9 19.2 20. 12.7 10.7 10.2 11.4 9.8 12.3 14.7 16.3 4.8 5.1 12.9 14.5 13. 23.7 15.6 19.2 13.1 10.4 11.7 10.7 8.8 10.2 13.9 12. 20.1 19.2 15.9 20.0 14.2 24.8 19.8 20.9 9.2 9.8 8.1 10.0 5. 9.4 10.8 10.5 19.4 14.3 5.7 20.9 11.7 24.0 21.2 21. 13.2 10.9 11.4 12.8 12.5 14.0 15.3 16.9 5.2 3.8 10.2 11.4 12. 14.9 14.8 18.4 7.9 8.4 8.0 12.1 10.3 12.7 11.6 13. 22.0 19.9 17.9 21.4 14.6 27.5 20.4 23.0 14.3 10.6 12.0 17.3 16. 16.9 16.2 15.8 13.5 12.1 18.7 20.5 22.4 28.8 23.1 25. 16.0 13.7 15.3 16.8 18.9 19.4 17.9 20.1 Table 10: Overall Performance of AmbigDocs and ConflictQA with DF-1. The best and second best of each model in bold and underline respectively."
        },
        {
            "title": "Variants",
            "content": "COrg (1) without removing duplicates [R] (2) without converting distracting to ambiguous [R] (3) without grouping [R] (4) without query reformulation in aggregator [A] (5) without all [R,A]"
        },
        {
            "title": "Score",
            "content": "22.0 21.6 20.2 19.1 19.9 17.0 Table 11: Impact of removing different components from the reranker and aggregator. [R] indicates changes in the reranker and [A] indicates changes in the aggregator. datasets, CORG generally enhances performance, similar to the findings in Table 5. However, the benefits of using CORG are more pronounced when the corpus exhibits more complex relationships. Additionally, in the case of ConflictQA, Separate demonstrates consistently strong performance, in line with the observations discussed in Section 3.2. Effect of each factor Table 11 shows the impact of removing different components from the reranker and aggregator11. For the reranker, we evaluate the effect of removing individual factors processed in the reranker (1), (2), and (3) to understand their individual contributions. For the aggregator, we investigate the removal of query reformulation (4) to evaluate its importance. Due to time 11Please note that without the graph constructor, as we cannot process both the reranker and aggregator, we are not able to perform ablation over the graph constructor. constraints, we conduct experiment with AmbigDocs+ dataset with the Llama2-7b-chat model. Our experiment in the below table shows that removing the grouping (3) resulted in significant performance drop; when documents with counterfactual relationships were processed together without any grouping. Similarly, removing query reformulation (4) tends to cause noticeable decrease in performance. On the other hand, removing duplicate documents led to only minor performance change, though it would increase input token consumption, as all context had to be processed. Overall, these findings highlight the importance of each component in the CORG framework. We will include these results in the final version of the paper. Additionally, we would like to emphasize the novelty of our grouping method using the graph constructor, as demonstrated by the comparison with other grouping methods (Random and KMeans). D.4 Efficiency Statics of groups We investigate the number of groups, or inference runs, for Llama2-7B in the AmbigDocs+ dataset. We observe that the majority of cases are grouped into two (3.5k), with additional instances of single run (1.2k) and three runs (1.0k), along with few cases featuring four or more runs (0.2k). This result aligns with Figure 8, which shows that most cases consist of eight contexts. If each relationship is evenly divided, Base (S) Retrieve (S) Summarize (S) Random (G) KMeans (G) COrg (G) Separate (I)"
        },
        {
            "title": "Input Token Count\nOutput Token Count\nPerformance",
            "content": "All Context 87.91 17.0 773.85 22.97 4.2 354.63 28.18 18.8 All Context 121.77 19.4 All Context 66.29 3.6 752.18 35.02 22. All Context 210.78 13.5 Table 12: Average token consumption for both input and output when generating with Llama-2-7b on AmbigDocs+. (S) means \"S\"ingle inference run, (G) means \"G\"rouping inference run, and (I) means \"I\"ndividual runs. this configuration would yield two counterfactual contexts. Average Token Consumption Table 12 shows the average token consumption for both input and output when generating with Llama-2-7b on AmbigDocs+. We could see the CORG tends to show short input sequence as we do not process the entire context, which requires 1347.9 tokens for input. Instead, we remove unnecessary or redundant context, such as duplicated information or relationships that are not relevant (e.g., changing distracting relationships to ambiguous ones) during the reranker process. Ours also show the shortest output length compared to other group-based methods, those with (G), while maintaining high performance. Upon reviewing the outputs, we observed that COrg generates shorter responses as they focus only on the relevant information without appending unnecessary knowledge to the question or their parametric knowledge. Also, in the case of retrieve, we could see that it is especially short as they tend to generate responses without containing multiple answers as they can not answer the question. While with low token consumption, CORG shows the best performance, which further supports the practical efficiency of CORG in real-world deployment."
        }
    ],
    "affiliations": [
        "Adobe Research",
        "KAIST AI"
    ]
}