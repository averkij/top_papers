{
    "paper_title": "MotionPro: A Precise Motion Controller for Image-to-Video Generation",
    "authors": [
        "Zhongwei Zhang",
        "Fuchen Long",
        "Zhaofan Qiu",
        "Yingwei Pan",
        "Wu Liu",
        "Ting Yao",
        "Tao Mei"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Animating images with interactive motion control has garnered popularity for image-to-video (I2V) generation. Modern approaches typically rely on large Gaussian kernels to extend motion trajectories as condition without explicitly defining movement region, leading to coarse motion control and failing to disentangle object and camera moving. To alleviate these, we present MotionPro, a precise motion controller that novelly leverages region-wise trajectory and motion mask to regulate fine-grained motion synthesis and identify target motion category (i.e., object or camera moving), respectively. Technically, MotionPro first estimates the flow maps on each training video via a tracking model, and then samples the region-wise trajectories to simulate inference scenario. Instead of extending flow through large Gaussian kernels, our region-wise trajectory approach enables more precise control by directly utilizing trajectories within local regions, thereby effectively characterizing fine-grained movements. A motion mask is simultaneously derived from the predicted flow maps to capture the holistic motion dynamics of the movement regions. To pursue natural motion control, MotionPro further strengthens video denoising by incorporating both region-wise trajectories and motion mask through feature modulation. More remarkably, we meticulously construct a benchmark, i.e., MC-Bench, with 1.1K user-annotated image-trajectory pairs, for the evaluation of both fine-grained and object-level I2V motion control. Extensive experiments conducted on WebVid-10M and MC-Bench demonstrate the effectiveness of MotionPro. Please refer to our project page for more results: https://zhw-zhang.github.io/MotionPro-page/."
        },
        {
            "title": "Start",
            "content": "MotionPro: Precise Motion Controller for Image-to-Video Generation* Zhongwei Zhang1, Fuchen Long2, Zhaofan Qiu2, Yingwei Pan2, Wu Liu1, Ting Yao2, and Tao Mei2 1University of Science and Technology of China 2HiDream.ai Inc. zhwzhang@mail.ustc.edu.cn, {longfuchen, qiuzhaofan, pandy}@hidream.ai liuwu@live.cn, {tiyao, tmei}@hidream.ai 5 2 0 M 6 2 ] . [ 1 7 8 2 0 2 . 5 0 5 2 : r a"
        },
        {
            "title": "Abstract",
            "content": "Animating images with interactive motion control has garnered popularity for image-to-video (I2V) generation. Modern approaches typically rely on large Gaussian kernels to extend motion trajectories as condition without explicitly defining movement region, leading to coarse motion control and failing to disentangle object and camera moving. To alleviate these, we present MotionPro, precise motion controller that novelly leverages region-wise trajectory and motion mask to regulate fine-grained motion synthesis and identify target motion category (i.e., object or camera moving), respectively. Technically, MotionPro first estimates the flow maps on each training video via tracking model, and then samples the region-wise trajectories to simulate inference scenario. Instead of extending flow through large Gaussian kernels, our region-wise trajectory approach enables more precise control by directly utilizing trajectories within local regions, thereby effectively characterizing finegrained movements. motion mask is simultaneously derived from the predicted flow maps to capture the holistic motion dynamics of the movement regions. To pursue natural motion control, MotionPro further strengthens video denoising by incorporating both region-wise trajectories and motion mask through feature modulation. More remarkably, we meticulously construct benchmark, i.e., MC-Bench, with 1.1K user-annotated image-trajectory pairs, for the evaluation of both fine-grained and object-level I2V motion control. Extensive experiments conducted on WebVid-10M and MC-Bench demonstrate the effectiveness of MotionPro. Please refer to our project page for more results: https: //zhw-zhang.github.io/MotionPro-page/. 1. Introduction In recent years, diffusion models [9, 12, 1618, 2022, 28, 29, 37, 38, 43, 46, 47, 54, 62] have shown significant *This work was performed at HiDream.ai. Co-corresponding author. Figure 1. An illustration of (a) fine-grained and (b) object-level motion control by using typical Gaussian filtered trajectory and our MotionPro. The flow of generated videos are also visualized. progress in revolutionizing text-to-video (T2V) generation. Although promising visual appearance can be attained by these advances, the controllable motion generation is still grand challenge in video diffusion paradigm. There are several attempts [13, 27, 49, 61] to enhance controllable capacity of video synthesis with additional guidance (e.g., depth, edge or optical flow). Nevertheless, it might be impractical for users to conveniently provide such signals as input conditions. Hence, the focus of this paper is to capitalize on the user-friendly conditions (i.e., sparse trajectory and region mask) for enabling interactively controllable imageto-video (I2V) generation: given the reference image as the first frame, the motion in the synthesized video should be natural and well-aligned with the provided trajectory. Pioneering practices [52, 57] of controllable I2V generation usually guide video denoising with the single condition of Gaussian filtered trajectory. In the training stage, the input trajectories are first sparsely sampled from the optical flow maps and then processed by Gaussian filter with large kernel to mitigate pixel-level trajectory instability (e.g., 99 in DragNUWA [57]). The flow extension brought by Gaussian filtering inevitably results in the inaccuracy of fine-grained motion details and limits the model capability for precise motion control. Therefore, the generated fine-grained movement (e.g., the turning-head of first case in Figure 1) is unnatural. Another issue is that the single condition of trajectory commonly fails to precisely identify the target motion category (i.e., object or camera moving). For instance, as depicted in Figure 1, the trajectory on the planet could be explained as two moving situations, i.e., the camera being pulled downwards with relative to static two planets (camera movement) or planet rising corresponding to static background (object movement). Solely relying on the trajectory might lead to the motion misinterpretation and thus hinder exactly controllable I2V generation. To address the above two issues, we shape new paradigm of motion controller that capitalizes on region-level trajectory and motion mask to enhance video denoising for controllable I2V synthesis. Specifically, we spatially sample multiple local regions in the video optical flow maps and directly employ the trajectories in the sparse regions as input condition. In this way, the attained trajectories could maintain accurate motion details and enables adequate capture of fine-grained motion. Meanwhile, motion mask is estimated on the optical flow maps which aims to globally emphasize the motion area, thereby specifying the target motion category and alleviating misinterpretation. To further regulate motion synthesis, we predict the affine parameters on the collaboration of trajectory and motion mask to modulate the video latent codes in denoising. As shown in Figure 1, our unique region-wise trajectory design and the employment of motion mask achieves the better fine-grained (e.g., turninghead) and object-level (e.g., planet-moving) motion. By materializing the idea of facilitating controllable I2V generation with the proposed conditions, we present novel framework, namely MotionPro, precise region-wise motion control. Specifically, given the input video, MotionPro first estimates the sequence of visibility masks and optical flow maps by using an off-the-shelf optical tracking model. Next, the global visibility mask is obtained through computing the intersection of all visibility masks, and further multiplied with the flow map of each frame. Then, MotionPro splits the masked flow maps into multiple local regions (e.g., the region with the size of 8 8) and employs the trajectories on such sparsely-sampled regions as region-wise trajectory. Meanwhile, MotionPro attains the motion mask on the flow maps via thresholding mechanism for representing holistic motion. Given the region-wise trajectory and corresponding motion mask, the multi-scale features are learnt by motion encoder, and further employed to predict scale and bias for video latent feature modulation. Moreover, MotionPro fine-tunes all attention modules in 3D-UNet via utilizing the Low-Rank Adaptation (LoRA) technique to pursue better motion-trajectory alignment. The main contribution of this work is design of MotionPro by leveraging region-wise trajectory and motion mask as the complementary control signals for precise controllable I2V diffusion. Beyond this, one benchmark, i.e., MCBench, with 1.1K user-annotated image-trajectory pairs, is carefully collected for evaluation. Extensive experiments further verify the superiority of MotionPro in terms of both video quality and motion-trajectory alignment. 2. Related Work Image-to-Video Diffusion Models. The remarkable progress achieved by text-to-video generation [2, 3, 6, 7, 14, 17, 18, 2022, 27, 30, 32, 43, 47, 56] encourages the development of image-to-video (I2V) diffusion models. These advances [5, 11, 15, 41, 49, 53, 58, 60] treat static image as the input condition for temporal coherent video synthesis. VideoComposer [49] is one of the earlier works that integrates image condition into 3D-UNet through concatenating the clean image latent with the noisy video latents. Based on this recipe, DynamiCrafter [53] and SVD [5] additionally inject the CLIP [39] feature of reference image into video denoising to enhance the information guidance. To achieve high-resolution I2V generation, I2VGen-XL [60] introduces cascading diffusion model to first animate image in the low resolution and further magnifies it via video refinement. Besides, there are several explorations [11, 58] that simultaneously utilize two images (i.e., the first and last frames) as more powerful references to elevate I2V generation. In this work, we choose the pre-trained I2V diffusion model SVD [5] as our base architecture for motion control. Controllable Video Diffusion Models. Despite highquality video synthesis via I2V diffusion models, the controllable motion generation still remains an under-explored problem. The early controllable video diffusion techniques [8, 10, 13, 27, 49, 61] typically leverage the condition of depth, edge or optical flow, for particular motion generation. Nevertheless, it is usually impractical for users to conveniently obtain such kinds of signals. To address this issue, the studies exploring bounding box [24, 31, 48, 51, 55] or trajectory [34, 35, 50, 57] as additional condition for motion control start to emerge. In the direction of utilizing trajectory condition, DragNUWA [57] and DragAnything [52] exploits Gaussian filtered trajectory to regulate motion synthesis to mitigate pixel-level trajectory instability. However, this approach limits fine-grained motion controllability and often fails to disentangle object and camera motions when relying solely on trajectories as the conditional input. Recently, Motion-I2V [41] and MOFA-Video [35] proposed two-stage motion control framework, which first densifies input trajectories through sparse-to-dense network and subsequently regulates video denoising using the estimated dense trajectories. Similarly, MOFA-Video also introduces the concept of movement region mask. However, the mask is solely employed for Figure 2. An overview of (a) our MotionPro for controllable I2V generation and (b) pipeline of motion condition generation. During training, MotionPro first extracts the proposed region-wise trajectory and motion mask on the input video as the control signals. The multiscale features are then learnt on these signals by motion encoder, and further injected into the 3D-UNet of SVD in feature modulation manner. Meanwhile, LoRA layers are integrated into all attention modules in the transformer blocks to improve the optimization of motiontrajectory alignment. In the inference stage, the region-wise trajectory and motion mask are first derived from the user provided trajectory and brushed region, and then exploited as the guidance to calibrate I2V video generation. flow masking as post-processing step, rather than being integrated as conditional input for motion-region-aware video generation. This limitation can occasionally lead to unnatural motions and localized distortions in the synthesized videos. Instead, our work focuses on new recipe of leverages region-wise trajectories and motion masks for precisely controllable I2V generation. The proposal of MotionPro contributes by studying not only how to express the motion trajectory accurately, but also how to benefit natural and precise motion generation with the synergy of the region-wise trajectory and motion mask. 3. Our Approach Here, we introduce our MotionPro framework for controllable I2V generation. Figure 2 illustrates an overview of our architecture. Given video clip at training, the newlyminted region-wise trajectory and motion mask are first extracted as the control signals. Next, multi-scale features are learnt on the concatenation of the trajectory and mask via motion encoder. These features are further injected into the 3D-UNet of SVD [5] to regulate video denoising. In each feature scale of the 3D-UNet, scale and bias are predicted through convolutional layers to modulate the feature of video latent codes. Besides, all attention modules are fine-tuned by LoRA [23] to attain better alignment between the synthesized motion and input trajectory. 3.1. Preliminaries: Stable Video Diffusion To leverage comprehensive motion prior embedded in the pre-trained diffusion models for video generation, we exploit the advanced I2V generation model, i.e., Stable Video Diffusion (SVD) [5] as the base architecture of our MotionPro. To better understand our proposal, we first revisit the training procedure of SVD. Formally, given an input video clip x0 = {xi 0}L i=1 with frames, the clean video latent codes z0 = {zi 0}L i=1 are first extracted via variational auto-encoder (VAE). Then, the Gaussian noise is added to z0 through forward diffusion procedure as: = z0 + n, (σ, n) p(σ, n), (1) where is the noised video latent codes and p(σ, n) = p(σ)N (0, σ2I). σ represents the noise level and p(σ) is the pre-determined distribution over σ. Following the training protocol of EDM [26], SVD leverages the 3D-UNet Fθ (with parameters θ) to predict the clean video latent codes ˆz0 with the condition of input noised latents z, noise level σ and the reference image cI : ˆz0 = cskip(σ)z + cout(σ)Fθ(cin(σ)z, cI ; cnoise(σ)), (2) where cskip(σ), cout(σ), cin(σ) and cnoise(σ) are pre-defined hyper-parameters determined by noise level σ. In SVD, the information of reference frame is injected into 3D-UNet along two pathways: a) the channel-wise concatenation of noised video latent codes and first frame latent code; b) the cross-attention between video latent feature and image CLIP [39] embedding of first frame. The loss function is formulated via denoising score matching (DSM) as: (cid:3) , = Ez0,cI ,(σ,n)p(σ,n) (cid:2)λσˆz0 z02 (3) 2 where λσ is weighting function. In the scenario of our work, besides the condition of reference first frame, we additionally exploit new kind of region-wise trajectory and motion mask as the control signals to refine video denoising for motion control. 3.2. Motion Condition Generation Most existing controllable I2V approaches calibrate the video denoising with the sole guidance of Gaussian filtered point-wise trajectory. Nevertheless, the flow extending brought by Gaussian filtering may result in inaccuracy of fine-grained motion details. Therefore, the ability of precise motion control could be limited. Besides, solely relying on the trajectory for motion control might not exactly express target motion category (i.e., camera or object moving), leading to motion misinterpretation in video generation. To alleviate these issues, we propose to directly sample trajectories from optical flow maps in multiple local regions as the region-wise trajectory. Such trajectory could preserve more precise motion details and thus manage to characterize fine-grained movement. Meanwhile, motion mask is further derived from the flow maps to explicitly identify target motion category of the generated videos. Region-wise Trajectory. As depicted in Figure 2, given the input video clip x0 = {xi 0}L i=1 with the size of 3, we first employ dense optical tracking model, i.e., DOT [33] to estimate optical flow maps = {f i}L i=1 and the sequence of visibility masks = {M i}L i=1: (4) 0), = 1, 2, ..., L, i, = DOT(x1 0, xi where RHW 2 and {0, 1}HW is the optical flow map and the visibility mask between the first and the i-th frame, respectively. Then, we calculate the intersection on to attain global visibility mask Mg {0, 1}HW that indicates the locations having visible optical flow along temporal dimension as: Mg = (cid:89) i=1 i. (5) Next, the masked flow maps fm = {f i=1 are computed by frame-wisely multiplying the flow maps with the global visibility mask Mg as follows: m}L fm = {f Mg}L i=1. (6) We split the masked flow maps fm into multiple local regions and the spatial size of each region is k. The region-wise trajectories Ts RLHW 2 are finally sampled from the region-split fm with region selection mask Msel {0, 1} W : Ts = {f ad(Msel)}L i=1, (7) where Msel is uniformly sampled from {0, 1} with the mask ratio rm, and ad() denotes the padding function which fills the mask value into the region around each position. Instead of exploiting constant mask ratio for trajectory selection, we randomly choose rm in range of [rmin, 1.0] to simulate different real-world motion masking scenarios, which benefits the robust network optimization. In this way, we formulate more precise signal by exploiting the trajectories in local region, enhancing the control ability of fine-grained motion in I2V models. Figure 3. An illustration of adaptive feature modulation. Motion Mask. In addition to the region-wise trajectory for video denoising regulation, the motion mask aims to specify the motion category and benefit the global motion correlation. Given the flow maps = {f i}L i=1 estimated by DOT, we first calculate the average flow magnitude favg RHW along temporal dimension as: favg = (cid:80)L 1 i=1 2. Then, we construct the motion mask Mmot {0, 1}HW from zero matrix, and set the value of the position where favg is greater than 1 as True. Mmot is finally repeated times as the motion mask sequence Mmot {0, 1}LHW 1 to align the temporal length of input video for subsequent motion control learning. 3.3. Motion Control Learning With the region-wise trajectory and motion mask, we aim to control motion generation with the input signals. Inspired by the recipe of feature adaptation in controllable image generation [59], we propose to exploit lightweight motion encoder to estimate multi-scale features on the conditions, and utilize these features to adaptively modulate video latent feature in each corresponding scale. To further improve the alignment between input trajectory and generated video, we fine-tune all attention modules in the spatial-temporal transformer blocks of 3D-UNet via using LoRA [23]. Adaptive Feature Modulation. Given the attained region-wise trajectory Ts and motion mask Mmot, we first concatenate them along channel dimension to form the input condition. As shown in Figure 2, lightweight motion encoder with series of convolutional layers first encodes the input condition into multi-scale feature maps. In each scale, the learnt feature map is employed to modulate the video latent feature at the same scale in 3D-UNet. Figure 3 depicts an illustration of the adaptive feature modulation by using the feature map ls in s-th scale. Particularly, we estimate the scale γs and bias βs on ls via spatial-temporal convolutional layer. Then, the normalized feature map of the input video latent feature hs is modulated via γs and βs, and further added back to itself in skip-connection manner to form the output feature map s as: = GN (hs) γs + βs + hs, (8) where GN () denotes the group normalization. Note that we implement zero initialization on temporal convolutional Table 1. Fine-grained motion control results on WebVid-10M. Table 2. Fine-grained motion control results on MC-Bench. Approach FVD () FID () Frame Consis. () Approach MD-Img () MD-Vid () Frame Consis. () DragNUWA [57] MOFA-Video [35] MotionPro 96.65 87.70 59.88 13.19 12.18 10.40 0.9888 0.9894 0. DragDiffusion [42] MOFA-Video [35] MotionPro 14.70 13.94 10.56 13.84 10.50 8.34 0.9947 0.9972 0.9962 layers to initialize γs and βs as zero at the beginning of training, guaranteeing the stability of model optimization. LoRA Integration. To preserve rich motion prior learnt by the pre-trained video diffusion model and elevate the effectiveness of motion control, we employ LoRA layers in all attention modules of spatial-temporal transformer blocks as demonstrated in Figure 2. Specifically, the LoRA parameters act as residue part of the original weights W: = + = + ABT , (9) where is the fused weights of attention module. and are trainable matrices in LoRA layers. 3.4. Inference Pipeline of MotionPro Our MotionPro is user-friendly I2V generation framework for interactive motion control. In the inference stage, as shown in Figure 2, users can readily brush the motion region on the uploaded reference image and draw the trajectory of moving direction as input control signals. In detail, the motion mask can be directly obtained from the user provided brush mask. Given the user trajectory which generally describes the movement of single pixel, we pad the trajectory value in the region around the pixel position to match the training paradigm. The padded trajectory in local region is exploited as the input region-wise trajectory. Finally, MotionPro regulates video denoising with the guidance of the two collaborative control signals through adaptive feature modulation. Both fine-grained and objectlevel motion control are facilitated by the synergy of the proposed region-wise trajectory and motion mask. 4. Experiments 4.1. Experimental Settings Benchmarks. We empirically verify the merit of MotionPro on two benchmarks, i.e., WebVid-10M [1] and our proposed MC-Bench. The WebVid-10M dataset consists of 10.7M video-caption pairs. There are 5K videos in the validation set and we sample 1K videos for evaluation. For each video, trajectories sampled at ratio of 15% along with the first frame serve as the input condition for fine-grained I2V motion generation. We follow the protocols in recent controllable I2V advance [35] and choose the Frechet Video Distance (FVD) [45], Frechet Image Distance (FID) [19], and Frame Consistency (Frame Consis.) [36] of CLIP [39] features as the evaluation metrics on WebVid-10M. In practical applications, users typically prefer to control video generation through limited number of representative trajectories, often just one or two. The automatically sampled trajectories employed in WebVid-10M do not adequately represent this scenario, thereby potentially compromising the validity of the evaluation. Thus, we introduce MC-Bench, new benchmark with 1.1K reference images and user-annotated trajectories, which is tailored for the evaluation of controllable I2V generation. More details about the new dataset are provided in the supplementary material. Due to the absence of ground-truth video, FVD and FID metrics are not applicable to MC-Bench. In addition to Frame Consistency, we utilize the Mean Distance (MD) to measure the alignment between generated motion and input trajectory. Two evaluation protocols are exploited for this target, i.e., MD-Img and MD-Vid. MD-Img is proposed by DragDiffusion [42] which estimates the framelevel mean Euclidean distance between trajectories of input and generated frames. To further validate the video-level trajectory accuracy via MD-Vid, we replace the image correspondence detection model DIFT [44] in MD-Img with the video tracking model CoTracker [25], which supplies more precise trajectory reference. Implementation Details. In MotionPro, we employ SVD [5] as our base architecture. Each training sample is 16-frames video clip and the sampling rate is 8 fps. We fix the resolution of each frame as 320 512, which is centrally cropped from the resized video. The local region size is set as 8 and the minimal mask ratio rmin is set as 0.95 determined by cross validation. We set the rank of LoRA parameters as 32. The motion encoder and LoRA layers are trained via AdamW optimizer with the base learning rate 1 105. All experiments are conducted on 6 NVIDIA A800 GPUs with minibatch size 48. 4.2. Evaluation on Fine-grained Motion Control We first evaluate MotionPro on the fine-grained motion control for I2V generation. The performances on WebVid-10M and MC-Bench are summarized in Table 1 and Table 2, respectively. Our MotionPro consistently achieves better performances on WebVid-10M across different metrics. In particular, MotionPro attains the FVD of 59.88, outperforming the best competitor MOFA-Video by 27.82. The better FVD indicates the better alignment of data distribution between the generated and ground-truth videos. Such results basically verify the superiority of exploring precise regionwise trajectory to strengthen fine-grained motion dynamic learning. On MC-Bench, MotionPro leads to performance Input Control DragNUWA DragDiffusion MOFA-Video MotionPro Figure 4. Examples of fine-grained motion control results on MC-Bench. The input control signals include the reference image, trajectory and motion mask. Best viewed with Acrobat Reader for the animated videos. Table 3. Object-level motion control results on MC-Bench. Approach MD-Img () MD-Vid () Frame Consis. () MOFA-Video [35] DragAnything [52] MotionPro 15.56 12.30 10. 12.04 11.37 8.59 0.9951 0.9917 0.9943 boosts against baselines in terms of MD-Img and MD-Vid, showing better alignment between the user input trajectory and synthesized videos. Note that MOFA-Video exploits two-stage controllable I2V framework that first densifies the input trajectories through conditional motion propagation (CMP), and then calibrates video diffusion process using the estimated dense trajectories. In contrast, MotionPro learns precise motion patterns by directly referring regionwise trajectory via adaptive feature modulation, thus enhancing the motion-trajectory alignment, as evidenced by the better MD-Img and MD-Vid performances. Besides, the CMP technique in MOFA-Video generally focuses on flow completion in the local region surrounding the input trajectory while neglecting potential movements in other areas. Thus, MOFA-Video tends to synthesize videos with less motion dynamics and obtains slightly higher Frame Consistency (approximately 0.001). To substantiate this, we calculate the average flow magnitude of videos generated by MOFA-Video, which achieves 4.95. In comparison, MotionPro attains higher value of 8.95, verifying that our model achieves greater motion variability while maintaining better motion-trajectory alignment. Figure 4 further showcases three I2V generation results controlled by the user input trajectory and region mask on MC-Bench. Generally, the videos synthesized by our MotionPro exhibits more natural movement and better alignment with input trajectory than the baseline methods. For instance, DragNUWA suffers from motion misinterpretation issue which wrongly generates videos with camera movement instead of object moving (e.g., the 1st and 2nd cases). The videos generated by MOFA-Video usually present unnatural object movement with local part distortion, e.g., the nose of raccoon in the 2nd case. We speculate that such distortion is caused by the lack of global region guidance in MOFA-Video, where the region mask is only employed for flow masking as post-processing. Our MotionPro, in comparison, integrates the information of motion mask into 3D-UNet on the fly to facilitate the modeling of holistic motion correlation. Thus, the synthesized videos by MotionPro reflect more rational fine-grained movement. 4.3. Evaluation on Object-level Motion Control Next, we conduct evaluation on object-level motion control for I2V generation. Table 3 lists the performances of different approaches on MC-Bench. Overall, MotionPro attains the best performances on the metrics of MD-Img and MDVid. Specifically, MotionPro obtains 10.48 of MD-Img and 8.59 of MD-Vid, reducing the Mean Distance of the best competitor DragAnything by 1.82 and 2.78, respectively. Input Control MOFA-Video DragAnything MotionPro Figure 5. Examples of object-level motion control results on MC-Bench. The input control signals include reference image, trajectory and motion mask. MotionPro can successfully handle complicated (e.g., the round trip of sun in the 1st case) and counterintuitive (e.g., the train moving back in the 3rd case) motion-trajectory alignment. Best viewed with Acrobat Reader. Figure 6. Performance comparisons of MD-Vid and Frame Consistency on MC-Bench under the settings of both fine-grained and objectlevel motion control by using different (a) local region size and (b) minimal mask ratio rmin in MotionPro. The improvements again confirm the merit of leveraging the duet of region-wise trajectory and motion mask for precise motion control. Similar performance trend on Frame Consistency can be also observed in the table. era motion rather than object movement. In contrast, MotionPro nicely capitalizes on trajectory information to refine video denoising, and specifies motion category with the region mask, endowing images with better object motion. Figure 5 shows the visual comparison of four objectlevel motion control results by using different approaches on MC-Bench. Compared to other baselines, videos generated by MotionPro can precisely match the input trajectory and maintain natural object-level motion dynamics. MOFA-Video still faces the challenge of local part distortion (e.g., only the train rear moving back in the 3rd case) and video generation with limited motion dynamics (e.g., the 4th case). Though DragAnything effectively aligns pixel movement with the input trajectory, certain instances (e.g., the 1st and 4th cases) misinterpret the trajectory as cam4.4. Ablation Study on MotionPro In this section, we perform ablation study to delve into the design of MotionPro for controllable I2V generation. Here, all experiments are conducted on MC-Bench. Local Region Size. We first investigate the choice of local region size for region-wise trajectory design in our MotionPro. Figure 6(a) compares the performances of MDVid and Frame Consistency on both fine-grained and objectlevel motion control by using different k. The variation of Frame Consistency is minor (less than 0.01) across differ-"
        },
        {
            "title": "Control",
            "content": "k=16 k="
        },
        {
            "title": "Control",
            "content": "k=1 k=2 k=4 k=8 Figure 7. Visualization of controllable I2V generation results with different local region size in MotionPro. Viewed with Acrobat Reader. ent settings, and the MD-Vid decreases when using larger k. When is small (e.g., 1 or 2), the kept trajectories are less in each local region and the control signals are weaken for motion control, leading to the inferior trajectory matching performance. Meanwhile, the improvement of MD-Vid is marginal when increasing to 16. Specifically, using large will extend the input trajectory over large region, which affects the fine-grained motion control. Accordingly, we exploit = 8 to extract the region-wise trajectory as the motion condition. Figure 7 further illustrates the I2V generation results with different k. As shown in this figure, the synthesized videos with = 8 present more natural motion dynamics and more precise motion-trajectory alignment. Moreover, the unnatural fine-grained motion as shown in the case when = 16 validates our analysis on the influence of overlarge region size. Minimal Mask Ratio. To explore the effect of minimal mask ratio rmin in trajectory selection stage, we then measure the motion control performance by conducting different rmin in Figure 6(b). Overall, Frame Consistency is not sensitive when changing rmin on both fine-grained and object-level motion control settings. Meanwhile, the performance of MD-Vid becomes better with the increase of the mask ratio at the beginning. The results are expected since using small value of rmin will sample more trajectories for model training, which enlarges the gap between training and real-world inference (i.e., only using one or two trajectories). Conversely, employing large value of mask ratio (e.g., 0.99) could make it difficult to optimize networks with scarce trajectory signals. Thus, we empirically set rmin as 0.95 to obtain the best motion-trajectory alignment in the generated videos. Multi-scale Feature Injection. We also investigate different multi-scale feature injection strategies in MotionPro. Figure 8 details the MD-Vid performance comparisons among different variants of our MotionPro. MotionProC concatenates the multi-scale features learnt by motion encoder with the video latent features along channel dimension in each scale. MotionPro+ replaces the channel-wise feature concatenation in MotionProC with the feature summation. In comparison, our proposal (MotionPro) injects the control signals into 3D-UNet via the adaptive feature modulation. Overall, MotionPro exhibits better MD-Vid performances against other two variants. In direct feature aggregation methods such as concatenation or summation, Figure 8. MD-Vid () among different multi-scale feature injection approaches on MC-Bench. information exchange requires strict spatial-temporal alignment between each other. In contrast, there is no such requirement for feature modulation, as it indirectly utilizes estimated scale and bias for feature regulation. Consequently, such feature injection demonstrates enhanced capacity to extract relevant information from input signals, potentially leading to improved motion control performance. 5. Conclusions This paper explores the motion condition formulation and the motion-trajectory alignment in diffusion models for controllable I2V generation. In particular, we study the problem from the viewpoint of integrating accurate motion control signals into video denoising to regulate motion generation. To materialize our idea, we have devised MotionPro, which leverages the region-wise trajectory and motion mask as the condition to calibrate video generation in feature modulation manner. The region-wise trajectory directly exploits the original trajectory information in each local region, characterizing more accurate motion details. The motion mask derived from the optical flow maps presents holistic motion and aims to identify exact motion category. The collaboration of the two signals regulates video denoising for natural motion synthesis with precise motion-trajectory alignment. Moreover, we have carefully construct new benchmark, i.e., MC-Bench, with 1.1K user-annotated image-trajectory pairs for both fine-grained and object-level motion control evaluation. Extensive experiments on WebVid-10M and MC-Bench validate the superiority of our proposal over state-of-the-art approaches. Acknowledgments. This work was supported in part by the Beijing Municipal Science and Technology Project No.Z241100001324002 and Beijing Nova Program No.20240484681."
        },
        {
            "title": "References",
            "content": "[1] Max Bain, Arsha Nagrani, Gul Varol, and Andrew Zisserman. Frozen in Time: Joint Video and Image Encoder for End-to-End Retrieval. In ICCV, 2021. 5, 12 [2] Fan Bao, Chendong Xiang, Gang Yue, Guande He, Hongzhou Zhu, Kaiwen Zheng, Min Zhao, Shilong Liu, Yaole Wang, and Jun Zhu. Vidu: Highly Consistent, Dynamic and Skilled Text-to-Video Generator with Diffusion Models. arXiv preprint arXiv:2405.04233, 2024. 2 [3] Omer Bar-Tal, Hila Chefer, Omer Tov, Charles Herrmann, Roni Paiss, Shiran Zada, Ariel Ephrat, Junhwa Hur, Guanghui Liu, Amit Raj, Yuanzhen Li, Michael Rubinstein, Tomer Michaeli, Oliver Wang, Deqing Sun, Tali Dekel, and Inbar Mosseri. Lumiere: Space-Time Diffusion Model for Video Generation. arXiv preprint arXiv:2401.12945, 2024. 2 [4] James Betker, Gabriel Goh, Li Jing, Tim Brooks, Jianfeng Wang, Linjie Li, Long Ouyang, Juntang Zhuang, Joyce Lee, Yufei Guo, Wesam Manassra, Prafulla Dhariwal, Casey Chu, and Yunxin Jiao. Improving Image Generation with Better Captions, 2023. 12 [5] Andreas Blattmann, Tim Dockhorn, Sumith Kulal, Daniel Mendelevitch, Maciej Kilian, Dominik Lorenz, Yam Levi, Zion English, Vikram Voleti, Adam Letts, Varun Jampani, and Robin Rombach. Stable Video Diffusion: Scaling Latent Video Diffusion Models to Large Datasets. arXiv preprint arXiv:2311.15127, 2023. 2, 3, 5 [6] Andreas Blattmann, Robin Rombach, Huan Ling, Tim Dockhorn, Seung Wook Kim, Sanja Fidler, and Karsten Kreis. Align your Latents: High-Resolution Video Synthesis with Latent Diffusion Models. In CVPR, 2023. [7] Tim Brooks, Bill Peebles, Connor Holmes, Will DePue, Yufei Guo, Li Jing, David Schnurr, Joe Taylor, Troy Luhman, Eric Luhman, Clarence Ng, Ricky Wang, and Aditya Ramesh. Video Generation Models as World Simulators. 2024. 2 [8] Wenhao Chai, Xun Guo, Gaoang Wang, and Yan Lu. StableVideo: Text-driven Consistency-aware Diffusion Video Editing. In ICCV, 2023. 2 [9] Jingyuan Chen, Fuchen Long, Jie An, Zhaofan Qiu, Ting Yao, Jiebo Luo, and Tao Mei. Ouroboros-Diffusion: Exploring Consistent Content Generation in Tuning-free Long Video Diffusion. In AAAI, 2025. 1 [10] Weifeng Chen, Yatai Ji, Jie Wu, Hefeng Wu, Pan Xie, Jiashi Li, Xin Xia, Xuefeng Xiao, and Liang Lin. ControlA-Video: Controllable Text-to-Video Diffusion Models with Motion Prior and Reward Feedback Learning. arXiv preprint arXiv:2305.13840, 2023. 2 [11] Xinyuan Chen, Yaohui Wang, Lingjun Zhang, Shaobin Zhuang, Xin Ma, Jiashuo Yu, Yali Wang, Dahua Lin, Yu Qiao, and Ziwei Liu. SEINE: Short-to-Long Video Diffusion In ICCV, Model for Generative Transition and Prediction. 2023. 2 [12] Zhikai Chen, Fuchen Long, Zhaofan Qiu, Ting Yao, Wengang Zhou, Jiebo Luo, and Tao Mei. Learning Spatial Adaptation and Temporal Coherence in Diffusion Models for Video Super-Resolution. In CVPR, 2024. [13] Patrick Esser, Johnathan Chiu, Parmida Atighehchian, Jonathan Granskog, and Anastasis Germanidis. Structure and Content-Guided Video Synthesis with Diffusion Models. In ICCV, 2023. 1, 2 [14] Songwei Ge, Seungjun Nah, Guilin Liu, Tyler Poon, Andrew Tao, Bryan Catanzaro, David Jacobs, Jia-Bin Huang, MingYu Liu, and Yogesh Balaji. Preserve Your Own Correlation: Noise Prior for Video Diffusion Models. In ICCV, 2023. 2 [15] Rohit Girdhar, Mannat Singh, Andrew Brown, Quentin Duval, Samaneh Azadi, Sai Saketh Rambhatla, Akbar Shah, Xi Yin, Devi Parikh, and Ishan Misra. Emu Video: Factorizing Text-to-Video Generation by Explicit Image Conditioning. In ECCV, 2024. 2 [16] Xun Guo, Mingwu Zheng, Liang Hou, Yuan Gao, Yufan Deng, Pengfei Wan, Di Zhang, Yufan Liu, Weiming Hu, Zhengjun Zha, Haibin Huang, and Chongyang Ma. I2VAdapter: General Image-to-Video Adapter for Diffusion Models. arXiv preprint arXiv:2312.16693, 2023. 1 [17] Yuwei Guo, Ceyuan Yang, Anyi Rao, Yaohui Wang, Yu Qiao, Dahua Lin, and Bo Dai. AnimateDiff: Animate Your Personalized Text-to-Image Diffusion Models without Specific Tuning. In ICLR, 2024. [18] Agrim Gupta, Lijun Yu, Kihyuk Sohn, Xiuye Gu, Meera Hahn, Li Fei-Fei, Irfan Essa, Lu Jiang, and Jose Lezama. Photorealistic Video Generation with Diffusion Models. arXiv preprint arXiv:2312.06662, 2023. 1, 2 [19] Martin Heusel, Hubert Ramsauer, Thomas Unterthiner, Bernhard Nessler, and Sepp Hochreiter. GANs Trained by Two Time-Scale Update Rule Converge to Local Nash Equilibrium. In NeuIPS, 2017. 5 [20] Jonathan Ho, William Chan, Chitwan Saharia, Jay Whang, Ruiqi Gao, Alexey Gritsenko, Diederik P. Kingma, Ben Poole, Mohammad Norouzi, David J. Fleet, and Tim Salimans. Imagen Video: High Definition Video Generation with Diffusion Models. In CVPR, 2022. 1, 2 [21] Jonathan Ho, Tim Salimans, Alexey Gritsenko, William Chan, Mohammad Norouzi, and David J. Fleet. Video Diffusion Models. In NeurIPS, 2022. [22] Wenyi Hong, Ming Ding, Wendi Zheng, Xinghan Liu, and Jie Tang. CogVideo: Large-scale Pretraining for Text-toVideo Generation via Transformers. In ICLR, 2023. 1, 2 [23] Edward Hu, Yelong Shen, Phillip Wallis, Zeyuan AllenZhu, Yuanzhi Li, Shean Wang, Lu Wang, and Weizhu Chen. LoRA: Low-Rank Adaptation of Large Language Models. In ICLR, 2022. 3, 4 [24] Yash Jain, Anshul Nasery, Vibhav Vineet, and Harkirat Behl. PEEKABOO: Interactive Video Generation via MaskedDiffusion. In CVPR, 2024. [25] Nikita Karaev, Ignacio Rocco, Benjamin Graham, Natalia Neverova, Andrea Vedaldi, and Christian Rupprecht. CoTracker: It is Better to Track Together. In ECCV, 2024. 5 [26] Tero Karras, Miika Aittala, Timo Aila, and Samuli Laine. Elucidating the Design Space of Diffusion-Based Generative Models. In NeurIPS, 2022. 3 [27] Levon Khachatryan, Andranik Movsisyan, Vahram Tadevosyan, Roberto Henschel, Zhangyang Wang, Shant Navasardyan, and Humphrey Shi. Text2Video-Zero: Textto-Image Diffusion Models are Zero-Shot Video Generators. In ICCV, 2023. 1, 2 [28] Hengyuan Liu, Xiaodong Chen, Xinchen Liu, Xiaoyan Gu, and Wu Liu. AnimateAnywhere: Context-Controllable Human Video Generation with ID-Consistent One-shot Learning. In HCMA, 2024. 1 [29] Fuchen Long, Zhaofan Qiu, Ting Yao, and Tao Mei. VideoStudio: Generating Consistent-Content and MultiScene Videos. In ECCV, 2024. [30] Zhengxiong Luo, Dayou Chen, Yingya Zhang, Yan Huang, Liang Wang, Yujun Shen, Deli Zhao, Jingren Zhou, and Tieniu Tan. VideoFusion: Decomposed Diffusion Models for High-Quality Video Generation. In CVPR, 2023. 2 [31] Wan-Duo Kurt Ma, J.P. Lewis, and W. Bastiaan Kleijn. TrailBlazer: Trajectory Control for Diffusion-Based Video Generation. arXiv preprint arXiv:2401.00896, 2023. 2 [32] Xin Ma, Yaohui Wang, Gengyun Jia, Xinyuan Chen, Ziwei Liu, Yuan-Fang Li, Cunjian Chen, and Yu Qiao. Latte: Latent Diffusion Transformer for Video Generation. arXiv preprint arXiv:2401.03048, 2024. 2 [33] Guillaume Le Moing, Jean Ponce, and Cordelia Schmid. In CVPR, Dense Optical Tracking: Connecting the Dots. 2024. 4 [34] Chong Mou, Mingdeng Cao, Xintao Wang, Zhaoyang Zhang, Ying Shan, and Jian Zhang. ReVideo: Remake Video with Motion and Content Control. In NeurIPS, 2024. [35] Muyao Niu, Xiaodong Cun, Xintao Wang, Yong Zhang, Ying Shan, and Yinqiang Zheng. MOFA-Video: Controllable Image Animation via Generative Motion Field Adaptions in Frozen Image-to-Video Diffusion Model. In ECCV, 2024. 2, 5, 6, 12, 13 [36] Chenyang Qi, Xiaodong Cun, Yong Zhang, Chenyang Lei, Xintao Wang, Ying Shan, and Qifeng Chen. FateZero: Fusing Attentions for Zero-Shot Text-Based Video Editing. In ICCV, 2023. 5 [37] Yurui Qian, Qi Cai, Yingwei Pan, Yehao Li, Ting Yao, Qibin Sun, and Tao Mei. Boosting Diffusion Models with Moving Average Sampling in Frequency Domain. In CVPR, 2024. 1 [38] Mengxue Qu, Xiaodong Chen, Wu Liu, Alicia Li, and Yao Zhao. ChatVTG: Video Temporal Grounding via Chat with Video Dialogue Large Language Models. In CVPR, 2024. 1 [39] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, Gretchen Krueger, and Ilya Sutskever. Learning Transferable Visual In ICML, Models From Natural Language Supervision. 2021. 2, 3, 5 [40] Diana Wofk Peter Wonka Matthias Muller Shariq Farooq Bhat, Reiner Birkl. ZoeDepth: Zero-shot Transfer by Combining Relative and Metric Depth. In CVPR, 2023. 13 [41] Xiaoyu Shi, Zhaoyang Huang, Fu-Yun Wang, Weikang Bian, Dasong Li, Yi Zhang, Manyuan Zhang, Ka Chun Cheung, Simon See, Hongwei Qin, Jifeng Dai, and Hongsheng Li. Motion-I2V: Consistent and Controllable Image-to-Video In ACM SIGGeneration with Explicit Motion Modeling. GRAPH, 2024. 2 [42] Yujun Shi, Chuhui Xue, Jun Hao Liew, Jiachun Pan, Hanshu Yan, Wenqing Zhang, Vincent Y. F. Tan, and Song Bai. DragDiffusion: Harnessing Diffusion Models for Interactive Point-Based Image Editing. In CVPR, 2024. 5, 12, 13 [43] Uriel Singer, Adam Polyak, Thomas Hayes, Xi Yin, Jie An, Songyang Zhang, Qiyuan Hu, Harry Yang, Oron Ashual, Oran Gafni, Devi Parikh, Sonal Gupta, and Yaniv Taigman. Make-a-Video: Text-to-Video Generation without Text-Video Data. In ICLR, 2023. 1, 2 [44] Luming Tang, Menglin Jia, Qianqian Wang, Cheng Perng Phoo, and Bharath Hariharan. Emergent Correspondence from Image Diffusion. In NeurIPS, 2023. [45] Thomas Unterthiner, Sjoerd van Steenkiste, Karol Kurach, Raphael Marinier, Marcin Michalski, and Sylvain Gelly. FVD: new Metric for Video Generation. In ICLR DeepGenStruct Workshop, 2019. 5 [46] Siqi Wan, Jingwen Chen, Yingwei Pan, Ting Yao, and Tao Incorporating Visual Correspondence into Diffusion Mei. Model for Virtual Try-On. In ICLR, 2025. 1 [47] Jiuniu Wang, Hangjie Yuan, Dayou Chen, Yingya Zhang, Xiang Wang, and Shiwei Zhang. ModelScope Text-to-Video Technical Report. arXiv preprint arXiv:2308.06571, 2023. 1, 2 [48] Jiawei Wang, Yuchen Zhang, Jiaxin Zou, Yan Zeng, Guoqiang Wei, Liping Yuan, and Hang Li. Boximator: Generating Rich and Controllable Motions for Video Synthesis. arXiv preprint arXiv:2402.01566, 2024. 2 [49] Xiang Wang, Hangjie Yuan, Shiwei Zhang, Dayou Chen, Jiuniu Wang, Yingya Zhang, Yujun Shen, Deli Zhao, and Jingren Zhou. VideoComposer: Compositional Video Synthesis with Motion Controllability. In NeurIPS, 2023. 1, [50] Zhouxia Wang, Ziyang Yuan, Xintao Wang, Tianshui Chen, Menghan Xia, Ping Luo, and Ying Shan. MotionCtrl: Unified and Flexible Motion Controller for Video Generation. In ACM SIGGRAPH, 2024. 2 [51] Jianzong Wu, Xiangtai Li, Yanhong Zeng, Jiangning Zhang, Qianyu Zhou, Yining Li, Yunhai Tong, and Kai Chen. MotionBooth: Motion-Aware Customized Text-to-Video Generation. In NeurIPS, 2024. 2 [52] Weijia Wu, Zhuang Li, Yuchao Gu, Rui Zhao, Yefei He, David Junhao Zhang, Mike Zheng Shou, Yan Li, Tingting Gao, and Di Zhang. DragAnything: Motion Control for Anything using Entity Representation. In ECCV, 2024. 1, 2, 6, 12, 13 [53] Jinbo Xing, Menghan Xia, Yong Zhang, Haoxin Chen, Xintao Wang, Tien-Tsin Wong, and Ying Shan. DynamiCrafter: Animating Open-domain Images with Video Diffusion Priors. In ECCV, 2024. 2 [54] Haibo Yang, Yang Chen, Yingwei Pan, Ting Yao, Zhineng Chen, Chong-Wah Ngo, and Tao Mei. Hi3D: Pursuing HighResolution Image-to-3D Generation with Video Diffusion Models. In ACM MM, 2024. 1 [55] Shiyuan Yang, Liang Hou, Haibin Huang, Chongyang Ma, Pengfei Wan, Di Zhang, Xiaodong Chen, and Jing Liao. Direct-a-Video: Customized Video Generation with UserIn ACM Directed Camera Movement and Object Motion. SIGGRAPH, 2024. [56] Zhuoyi Yang, Jiayan Teng, Wendi Zheng, Ming Ding, Shiyu Huang, Jiazheng Xu, Yuanming Yang, Wenyi Hong, Xiaohan Zhang, Guanyu Feng, Da Yin, Xiaotao Gu, Yuxuan Zhang, Weihan Wang, Yean Cheng, Ting Liu, Bin Xu, Yuxiao Dong, and Jie Tang. CogVideoX: Text-to-Video Diffusion Models with An Expert Transformer. arXiv preprint arXiv:2408.06072, 2024. 2 [57] Shengming Yin, Chenfei Wu, Jian Liang, Jie Shi, Houqiang Li, Gong Ming, and Nan Duan. DragNUWA: Fine-Grained Control in Video Generation by Integrating Text, Image, and Trajectory. arXiv preprint arXiv:2308.08089, 2023. 1, 2, 5, 12 [58] Yan Zeng, Guoqiang Wei, Jiani Zheng, Jiaxin Zou, Yang Wei, Yuchen Zhang, and Hang Li. Make Pixels Dance: HighDynamic Video Generation. In CVPR, 2024. 2 [59] Lvmin Zhang, Anyi Rao, and Maneesh Agrawala. Adding Conditional Control to Text-to-Image Diffusion Models. In ICCV, 2023. 4 [60] Shiwei Zhang, Jiayu Wang, Yingya Zhang, Kang Zhao, Hangjie Yuan, Zhiwu Qin, Xiang Wang, Deli Zhao, and Jingren Zhou. I2VGen-XL: High-Quality Image-to-Video Synthesis via Cascaded Diffusion Models. arXiv preprint arXiv:2311.04145, 2023. 2 [61] Yabo Zhang, Yuxiang Wei, Dongsheng Jiang, Xiaopeng ControlVideo: In Zhang, Wangmeng Zuo, and Qi Tian. Training-Free Controllable Text-to-Video Generation. ICLR, 2024. 1, 2 [62] Zhongwei Zhang, Fuchen Long, Yingwei Pan, Zhaofan Qiu, Ting Yao, Yang Cao, and Tao Mei. TRIP: Temporal Residual Learning with Image Noise Prior for Image-to-Video Diffusion Models. In CVPR, 2024. 1 6. Supplementary Material The supplementary material contains: 1) the dataset details of MC-Bench; 2) baseline choices and experimental details; 3) the human evaluation of motion control; 4) robustness of motion mask; 5) the application of camera control; 6) runtime comparison; 7) ablation on control signals. 6.1. Dataset Details of MC-Bench The proposed MC-Bench consists of 412 high-quality reference images and corresponding 1.1K user-annotated trajectories. We collect the reference images with different visual contents, including animal, human, vehicle, etc. There are 72 images sampled from the public DragBench [42] and we further extend it with 340 additional images. Specifically, all the self-collected images about human are automatically generated by DALLE3 [4] to avoid the potential legal concerns. The remaining self-collected images are real photos which are first crawled on the Pexels platform and then filtered according to the visual quality. For each reference image, the annotator is required to brush the motion region and draw the movement trajectory according to user intention (i.e., fine-grained local part moving or global object moving). During trajectory annotation, all annotators are encouraged to ensure the trajectory diversity, including some complicated trajectories. Finally, the benchmark is annotated with 460 image-trajectory pairs for fine-grained motion control evaluation, and 680 image-trajectory pairs for object-level motion control evaluation, respectively. Figure 12 and Figure 11 further illustrate several visual examples (reference image, trajectory and motion mask) from MC-Bench for the two evaluations. 6.2. Baseline Choices and Experimental Details For the evaluation on WebVid-10M [1] of fine-grained motion control, we adopt the commonly-used protocol in recent controllable image-to-video (I2V) advance [35]. Specifically, for each video, we sample the optical flow at the ratio of 15% as the sparse trajectories, which are combined with the first frame as the input condition. Under this experimental setting, we choose DragNUWA [57] and MOFA-Video [35] as baselines for comparison. Notably, DragAnything [52] is deliberately designed for object-level motion control, which only accepts single trajectory of object, making it inapplicable for fine-grained motion control. Therefore, DragAnything is not involved for comparison in this setting. For the fine-grained motion control on MC-Bench, we compare our MotionPro with DragDiffusion [42] and MOFA-Video. DragNUWA is not included in this comparison since it only relies on trajectories and lacks the input of motion regions. Thus, DragNUWA usually suffers from the misinterpretation of object and camera movement, making the comparison unfair. The baseline of DragDiffusion is recent trajectory-guided image editing advance, which also offers convincing results for comparison. To adapt DragDiffusion for video generation, we divide the input trajectories into 15 segments and independently feed each segment into DragDiffusion to generate target frame. All the synthesized frames are concatenated as the final video. In the evaluation of object-level motion control on MCBench, both MOFA-Video and DragAnything are employed as baselines for performance comparison. To facilitate DragAnything in disentangling object and camera moving, we add static points in regions outside the motion mask areas to help DragAnything generate object-level motion instead of camera moving for evaluation. Its worth to noting that MotionPro learns object and camera motion control on inthe-wild video data (e.g., WebVid-10M) without applying special data filtering. 6.3. Human Evaluation In addition to the evaluation over automatic metrics, we also conduct human evaluation to investigate user preferences from three perspectives (i.e., motion quality, temporal coherence and trajectory alignment) across different controllable I2V approaches. In particular, we randomly sample 200 generated videos from both fine-grained and objectlevel motion control for evaluation. Through the Amazon MTurk platform, we invite 32 evaluators, and ask each evaluator to choose the best one from the generated videos by all models given the same inputs. Table 4 shows the user preference ratios across different models on MC-Bench. Overall, our MotionPro clearly outperforms all baselines in terms of the three criteria on both fine-grained and object-level motion control. The results demonstrate the advantage of leveraging complementary region-wise trajectory and motion mask to benefit video synthesis with natural motion, desirable temporal coherence and precise motion-trajectory alignment. 6.4. Ablation on control signals. We also include two runs (MotionPro traj: replaces regionwise trajectory with random trajectory, MotionPro mask: disables motion mask with all-one masks). Their FVD (73.7 and 66.2) on WebVid-10M are inferior to our MotionPro (59.88), which validates the effectiveness of our two control signal designs for precise motion formulation. 6.5. Robustness of motion mask To be clear, motion mask in our MotionPro refers to the rough dynamic region and does not require preciselyaligned shape at inference. We show I2V results controlled by the same trajectory with various motion masks in Figure 10, which show strong robustness. Such generalization merit is attributed to the use of estimated motion mask (flow Table 4. Human evaluation of user preference ratios (%) over both fine-grained and object-level motion control on MC-Bench. Evaluation Items Fine-grained Motion Control Object-level Motion Control DragDiffusion [42] MOFA-Video [35] MotionPro MOFA-Video [35] DragAnything [52] MotionPro Motion Quality () Temporal Coherence () Trajectory Alignment () 3.12 6.25 9.37 21.88 40.63 18.75 75.00 53.12 71.88 12.50 25.00 15.62 18.75 15.63 21.88 68.75 59.37 62. Figure 9. An illustration of I2V camera control using the condition of camera pose sequence in our MotionPro. map estimated by DOT) at training, rather than ground-truth precise motion mask. and all-ones motion mask into MotionPro for I2V synthesis. The video cases are provided in the offline project page. 6.6. Application: Camera Control 6.7. Runtime Comparison For 16-frame video generation (resolution: 512 320, on single NVIDIA H100 GPU), the runtime of MotionPro is 17 sec, which is comparable to baselines (DragNUWA: 27, DragDiffusion: 320, MOFA-Video: 15, DragAnything: 32). Our learnt MotionPro naturally supports two applications of camera control without additional training. The first application is controlling object and camera motion simultaneously with multiple trajectories in I2V generation. Another application is the I2V camera control by exploiting the sequence of camera poses as input condition. To be clear, motion mask in our MotionPro refers to the rough dynamic region and does not require precisely-aligned shape at inference. Simultaneous object and camera motion control. In this setting, we simply set the input motion mask as all-ones matrix, and feed multiple trajectories that reflect the object and background moving into MotionPro for I2V generation. The video cases are provided in the offline project page. Camera control with camera poses. Figure 9 illustrates the process of camera control using the condition of camera pose sequence in MotionPro. Concurrently, given an input image and the camera pose sequence, we first estimate the metric depth map of the image using ZoeDepth [40]. Next, we lift the 2D pixels to 3D point cloud using the metric depth map. Through projecting the point cloud into 2D space given the camera pose, we can determine the corresponding 2D positions of the same 3D points under the new view. By calculating the 2D displacement of the pixels projected from the same 3D points in the original and new views, the camera pose sequence is then converted into the sparse trajectories. Finally, we feed the sparse trajectories Figure 10. MotionPro conditioned on diverse mask shapes. Best viewed with Acrobat Reader. Figure 11. Visual examples from MC-Bench for object-level motion control evaluation. Each reference image is annotated with trajectory and motion mask for image-to-video generation. Figure 12. Visual examples from MC-Bench for fine-grained motion control evaluation. Each reference image is annotated with trajectory and motion mask for image-to-video generation."
        }
    ],
    "affiliations": [
        "HiDream.ai Inc.",
        "University of Science and Technology of China"
    ]
}