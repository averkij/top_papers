{
    "paper_title": "On Data Engineering for Scaling LLM Terminal Capabilities",
    "authors": [
        "Renjie Pi",
        "Grace Lam",
        "Mohammad Shoeybi",
        "Pooya Jannaty",
        "Bryan Catanzaro",
        "Wei Ping"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Despite rapid recent progress in the terminal capabilities of large language models, the training data strategies behind state-of-the-art terminal agents remain largely undisclosed. We address this gap through a systematic study of data engineering practices for terminal agents, making two key contributions: (1) Terminal-Task-Gen, a lightweight synthetic task generation pipeline that supports seed-based and skill-based task construction, and (2) a comprehensive analysis of data and training strategies, including filtering, curriculum learning, long context training, and scaling behavior. Our pipeline yields Terminal-Corpus, a large-scale open-source dataset for terminal tasks. Using this dataset, we train Nemotron-Terminal, a family of models initialized from Qwen3(8B, 14B, 32B) that achieve substantial gains on Terminal-Bench 2.0: Nemotron-Terminal-8B improves from 2.5% to 13.0% Nemotron-Terminal-14B improves from 4.0% to 20.2%, and Nemotron-Terminal-32B improves from 3.4% to 27.4%, matching the performance of significantly larger models. To accelerate research in this domain, we open-source our model checkpoints and most of our synthetic datasets at https://huggingface.co/collections/nvidia/nemotron-terminal."
        },
        {
            "title": "Start",
            "content": "6 2 0 2 4 2 ] . [ 1 3 9 1 1 2 . 2 0 6 2 : r a"
        },
        {
            "title": "On Data Engineering for Scaling LLM Terminal\nCapabilities",
            "content": "Renjie Pi, Grace Lam*, Mohammad Shoeybi, Pooya Jannaty, Bryan Catanzaro, Wei Ping 2026-2-"
        },
        {
            "title": "Abstract",
            "content": "Despite rapid recent progress in the terminal capabilities of large language models, the training data strategies behind state-of-the-art terminal agents remain largely undisclosed. We address this gap through systematic study of data engineering practices for terminal agents, making two key contributions: (1) Terminal-Task-Gen, lightweight synthetic task generation pipeline that supports seed-based and skill-based task construction, and (2) comprehensive analysis of data and training strategies, including filtering, curriculum learning, long context training, and scaling behavior. Our pipeline yields Terminal-Corpus, large-scale open-source dataset for terminal tasks. Using this dataset, we train Nemotron-Terminal, family of models initialized from Qwen3(8B, 14B, 32B) that achieve substantial gains on Terminal-Bench 2.0: Nemotron-Terminal-8B improves from 2.5% to 13.0% Nemotron-Terminal-14B improves from 4.0% to 20.2%, and Nemotron-Terminal-32B improves from 3.4% to 27.4%, matching the performance of significantly larger models. To accelerate research in this domain, we open-source our model checkpoints and most of our synthetic datasets at https://huggingface.co/collections/nvidia/nemotron-terminal. Figure 1: Overview of Terminal-Task-Gen. Our framework combines Dataset Adaptation, which transforms existing benchmarks into terminal prompts, with Synthetic Task Generation, which uses seed data and Skill Taxonomy to construct targeted scenarios. The tasks from both streams are utilized during Trajectory Generation phase, where agents interact with Dockerized environments to produce solution traces, followed by Post-Processing (decontamination and filtering) to yield the final SFT dataset. 1. Introduction As large language models (LLMs) advance toward practical software engineering applications, terminal interaction has emerged as critical capability. Tools like Claude Code (Anthropic, 2025) and Codex CLI (OpenAI, Equal technical contribution. Correspondence to: <renjiep@nvidia.com>, <gralam@nvidia.com>, <wping@nvidia.com> Leads the effort. 2026 NVIDIA. All rights reserved. On Data Engineering for Scaling LLM Terminal Capabilities Table 1: Model performance comparison on Terminal-Bench 2.0. Closed Source Models Model Accuracy GPT-5-Nano 7.90 GPT-5-Mini 24.0 Grok Code Fast 1 14. Grok 4 23.1 Gemini 2.5 Flash Qwen3-Max-Thinking 16.9 22.5 Model Size Accuracy Qwen3-32B Qwen3-Coder GPT-OSS (high) 20B GPT-OSS (high) 120B Nemotron-T-14B 32B 3.37 480B 23.9 20B 3.10 120B 18.7 14B 20.2 Nemotron-T-32B 32B 27. Open Source Models 2025) demonstrate the potential of command-line proficiency, with frontier models showing promising results on benchmarks such as Terminal-Bench (Merrill et al., 2026). However, the training data mixtures behind these systems remain largely undisclosed, leaving fundamental questions about effective data design unanswered. This lack of transparency forces researchers into costly trial-and-error process, primarily due to two significant bottlenecks in agentic data generation: (1) the scarcity of foundational resources, including diverse task prompts, requisite dependency files, and pre-configured environments; and (2) the logistical complexity of trajectory collection, as real-world human interactions are difficult to capture, while synthetic generation via LLM agents is prohibitively expensive due to the need for fresh environment instantiation and multi-turn interaction for every task. Current approaches to improving terminal capabilities fall into two main categories: improving agentic scaffolds (Antigma, 2025; Internet, 2025; JetBrains, 2025; Letta, 2025; Mux, 2025; Nichols, 2025; Singhal et al., 2025) or improving the underlying model for terminal use (Anthropic, 2025; DeepMind, 2025; Liu et al., 2025; MiniMax, 2025; Moonshot AI, 2025; OpenAI, 2025). One prominent approach for post-training models involves using adapters that wrap existing datasets in command-line interfaces (DCAgent, 2025; Development, 2025,). While the Terminal-Bench authors provide repository of such adapters (Team, 2025) primarily for benchmark adaptation, these adapters can also be repurposed as starting point for scaling training data. However, adapters inherit structural assumptions from source datasets never designed for sequential environment interaction, potentially limiting their effectiveness. Recent work has explored multi-agent frameworks (Austin, 2025; Peng et al., 2025) for more principled data generation, but these introduce computational complexity that scales poorly for large-scale training. The field thus lacks practical framework that balances generation efficiency with the specific requirements of training effective terminal agents. We address this gap through dual-strategy approach that combines dataset adaptation with synthetic task generation. Dataset adaptation provides broad coverage by transforming existing math, code, and software engineering datasets into Terminal-Bench format, efficiently scaling data volume while leveraging high-quality problem sources. Synthetic task generation offers finer-grained control: it enables targeted development of terminal-specific skills with control over task characteristics, such as difficulty, domain coverage, and primitive skill composition. These strategies are complementary and address distinct bottlenecks: adapters make the most of existing datasets to build foundational terminal capabilities at scale, while synthetic task generation provides the flexibility to target specific capability gaps. Building on this coarse-to-fine data generation pipeline, we develop Nemotron-Terminal, family of models fine-tuned from Qwen3 models (Yang et al., 2025). Specifically, we make the following contributions: 1. We introduce Terminal-Task-Gen, scalable synthetic task generation pipeline that enables rapid exploration of task design and targeted generation for specific skills with different difficulty levels. 2. We conduct systematic study of data engineering strategies, examining filtering strategies for dataset adapters and synthetic tasks, curriculum learning for data mixing, long context training, and scaling trends. 3. We conduct extensive experiments to validate the effectiveness of our curated dataset. As demonstrated in Table 1, on Terminal-Bench 2.0, our models achieve substantial improvements over the 2 On Data Engineering for Scaling LLM Terminal Capabilities initial Qwen3 baselines, reaching competitive performance with significantly larger models while requiring only modest computational resources for training and inference. For example, our NemotronTerminal-32B outperforms Qwen3-Coder-480B (Yang et al., 2025) on Terminal-Bench 2.0 (27.4 2.4 vs. 23.9 2.8). We release Nemotron-Terminal models and Terminal-Corpus dataset at https: //huggingface.co/collections/nvidia/nemotron-terminal. 2. Related Work Agent Design. As seen through Claude Code (Anthropic, 2025) and Codex CLI (OpenAI, 2025), sophisticated agent scaffolding can significantly improve performance. Many leading terminal agents achieve frontier performance through innovations in scaffolding (Antigma, 2025; Internet, 2025; JetBrains, 2025; Letta, 2025; Mux, 2025; Nichols, 2025; Singhal et al., 2025). However, effective agent scaffolds are often model-specific and require extensive engineering. As base models improve, the marginal benefit of complex scaffolding will likely decrease. Rather than exploring variants in agentic design, we focus on scaling underlying model capabilities through targeted supervised fine-tuning. Dataset Adapters. Several datasets on Hugging Face (DCAgent, 2025; Development, 2025,) collect agent execution traces by rolling out prompts from existing datasets through terminal environments. This approach can efficiently scale data collection by reusing existing prompts from different domains, including competitive coding and math. Despite the abundance of these datasets, no formal analysis has studied which characteristics of dataset adapters can affect downstream training effectiveness. In this work, we explore the strengths and weaknesses of this approach through systematic study using custom datasets and adapters. Synthetic Task Generation. Many studies have investigated how to effectively generate synthetic data for fine-tuning LLMs. Evol-Instruct (Xu et al., 2023) pioneered automated instruction data scaling through iterative in-depth and in-breadth evolution. Code Evol-Instruct (Luo et al., 2023) successfully adapted this strategy to automatically increase the complexity of code instruction data for effective fine-tuning. Since then, AgentInstruct (Mitra et al., 2024) and LAB (Sudalairaj et al., 2024) have demonstrated how to generate large-scale datasets from existing seed data through suggester-editor agent pairs and taxonomy-driven generation. Other works like MAGPIE (Xu et al., 2024) have explored extracting instruction data from aligned LLMs without seed data through unique prompting tactics. Recent work has explored bringing these ideas to scale terminal capabilities in LLMs, employing multi-agent systems to brainstorm ideas, generate tasks, design Docker environments, and validate generated tasks and environments (Austin, 2025; Peng et al., 2025). Since multi-agent systems can be time-consuming and costly, we design simplified system that eliminates unnecessary coordination stages and optimizes environment validation to enable effective scaling. Through our systematic study and ablations, we provide actionable insights for scaling terminal-capable models. 3. Background 3.1. Terminal-Bench Terminal-Bench (Merrill et al., 2026; Team, 2025) has emerged as the standard benchmark for evaluating agents in terminal environments. The benchmark comprises 89 hand-crafted, human-verified tasks that span diverse domains including scientific computing, software engineering, machine learning, security, system administration, and data science. Unlike traditional code generation benchmarks that evaluate isolated functions, Terminal-Bench tasks require agents to complete end-to-end workflows, such as compiling code, training models, configuring systems, and debugging environments. 3 On Data Engineering for Scaling LLM Terminal Capabilities As seen in Figure 2, each task in Terminal-Bench includes four components: (1) natural language instruction describing the objective, (2) containerized Docker (Merkel, 2014) environment providing the execution context, (3) verification test suite that programmatically checks task completion, and (4) an oracle solution demonstrating valid approach. Throughout this work, we use Terminal-Bench 2.0 as our primary evaluation benchmark and leverage Terminus 2, the model-agnostic reference agent released alongside the benchmark, for consistent evaluation across model checkpoints. task_directory / instruction . md task . toml environment / Dockerfile ... solution / solve . sh ... tests / test . sh ... { } \" analysis \" : \" ... \" , \" plan \" : \" ... \" , \" commands \" : [ { } , { } \" keystrokes \" : \" ls - la \" , \" duration \" : 0.1 \" keystrokes \" : \" cd project \" , \" duration \" : 0.1 ] , \" task_complete \" : true Figure 2: Terminal-Bench task directory structure. Each task consists of an instruction prompt, task metadata, environment files, Dockerfile, reference solution, and test cases. Figure 3: Terminus 2 agent response format. The Terminus 2 agent scaffold prompts the model to output responses in JSON format, which includes: analysis, plan, commands, and task_complete. 3.2. Terminus 2 Agent Framework Unlike traditional coding agents that provide multiple specialized tools, Terminus 2 (Team, 2025) only provides an interactive tmux session running inside sandboxed Docker (Merkel, 2014) container. Through sending model-determined keystrokes to the tmux session, the agent has the flexibility to approach tasks using any available command-line tools. At each step, the agent receives the current terminal output, and the model is prompted to respond with structured JSON format (Figure 3) that determines the next action sent to the environment. 4. Synthetic Data Generation Training autonomous agents for terminal environments requires systematic approach to data curation that balances breadth, depth, and scalability. We introduce principled two-stage data generation framework: dataset adaptation for establishing broad foundational coverage, followed by synthetic task generation for targeted skill refinement. This coarse-to-fine strategy decouples data volume scaling from task design iteration: adapters efficiently leverage existing problem repositories to build general competencies, while synthetic generation enables precise control over skill composition, difficulty progression, and domain-specific requirements. Together, these complementary strategies yield diverse, high-quality dataset for supervised fine-tuning (SFT) that systematically covers the operational and computational skills required for terminal-based problem solving. 4.1. Dataset Adapters 4.1.1. Prompt Datasets We selectively identify targeted high quality SFT prompt datasets that span the math, code, and software engineering (SWE) domains, since they are foundational to several of the topics covered in terminal use. 4 On Data Engineering for Scaling LLM Terminal Capabilities Math Prompts. We use the Stage-2 prompt set from Nemotron-Cascades math reasoning SFT data (Wang et al., 2025), which consists of 163K unique prompts drawn from OpenMathReasoning (Moshkov et al., 2025). To obtain this high-quality prompt set, Nemotron-Cascade filters out easy questions from the original datasets by excluding prompts whose DeepSeek-R1 (Guo et al., 2025) response length is shorter than 2K tokens. Code Prompts. We use the Stage-2 prompt set from Nemotron-Cascades code reasoning SFT data, which consists of 79K prompts from OpenCodeReasoning (Ahmad et al., 2025) covering challenging coding problems. We further filter and deduplicate this set to obtain 35K prompt subset. SWE Prompts. For software engineering tasks, we draw from Nemotron-Cascades SWE code repair SFT data, which consists of 127K instances from SWE-Bench-Train (Jimenez et al., 2023), SWE-reBench (Badertdinov et al., 2025), SWE-Smith (Yang et al., 2025), and SWE-Fixer-Train (Xie et al., 2025). Each prompt includes problem statement and the contents of one or more buggy code files. We further filter and deduplicate this set, resulting in 32K unique prompts. 4.1.2. Adapter Format Dataset adaptation is straightforward process that converts existing prompt datasets to Terminal-Bench format without requiring an LLM in the loop. Using the Terminus 2 system prompt template (Appendix A.2), we map each entry to the {instruction} placeholder, appending unique instruction suffix based on the dataset type. The specific suffixes used for math, code, and SWE prompts are detailed in Appendix A.2. For each code file identified in SWE prompt, we instantiate corresponding file within the environment. As the Nemotron-Cascade datasets provide only prompts, these tasks consist of an instruction and environment, without associated test cases. 4.2. Synthetic Task Generation While dataset adapters provide foundational breadth of reasoning and code, they are inherently limited by the formats of their source repositories. To bridge the gap between general problem-solving and the specific rigors of terminal-based agency, we introduce Terminal-Task-Gen, synthetic pipeline that generates executable tasks with precise control over skill complexity and environment constraints. By generating tasks from both structured seeds and taxonomy of primitive skills, we ensure the training data directly reflects the operational nuances and multi-step tool interactions required for terminal agents. We present two complementary approaches for generating synthetic terminal operation tasks: seed-based generation and skill-based generation. Both methods leverage LLMs to produce diverse, executable terminal tasks while addressing distinct requirements of scalability, diversity, and domain coverage. 4.2.1. Generation from Seed Data Seed-based generation complements dataset adaptation by using existing problems as inspiration, rather than as fixed templates. Instead of wrapping original prompts in terminal scaffold, we prompt an LLM to synthesize new terminal tasks from seed problems. This approach is particularly effective for leveraging high-quality problem specifications from adjacent domains, such as scientific computing challenges, algorithmic problem sets, or domain-specific coding exercises, where well-defined problems exist but lack the terminal-oriented task structure required for agent training. Seed Data Structure. Each seed entry is structured record containing: (1) problem description specifying the computational challenge, (2) an optional domain label indicating the scientific or technical area (e.g., biology, physics, optimization), and (3) an optional reference solution providing correct implementation. The reference 5 On Data Engineering for Scaling LLM Terminal Capabilities solution, when available, serves as ground truth for generating test expectations but is never exposed to the agent. Task Adaptation. The LLM acts as task adapter that transforms each seed problem into self-contained terminal task. This transformation involves several key operations. First, the abstract problem statement is augmented with concrete software engineering requirements: the agent must install necessary packages, read input from specified file paths, implement the solution, and write results to designated output locations. Second, the adapter generates realistic input data files that instantiate the problem with specific test cases, including edge cases and boundary conditions. Third, comprehensive pytest-based test cases are synthesized to verify correctness, which check output file existence, format compliance, numerical accuracy (with appropriate tolerances for floating-point results), and edge case handling. When reference solution is provided in the seed data, it is included in the generation context, and the prompt instructs the LLM to use it as ground truth when designing the test cases. Conversion Guidelines. The conversion prompt encodes several principles to ensure task quality: complex problems are decomposed into verifiable units when necessary; practical constraints such as input sizes and precision requirements are added to ground the problem in realistic scenarios; and output formats are designed to enable unambiguous programmatic verification. This systematic adaptation enables the pipeline to convert diverse problem sources into uniform task format suitable for agent evaluation. 4.2.2. Generation from Primitive Skills Skill-based generation takes fundamentally different approach: rather than adapting existing problems, it synthesizes novel tasks from structured taxonomy of primitive terminal operation skills. We curate list of primitive terminal operation skills and employ LLMs to expand and recombine these primitives into creative task specifications. Domain-Specific Generation. The task generation process is inherently domain-specific. We define 9 task domains: data processing, data querying, data science, debugging, dependency management, file operations, scientific computing, security, and software engineering. Each domain is associated with dedicated generation prompt that guides the LLM to produce tasks aligned with the domains focus areas. For instance, the data science prompt directs the model toward tasks involving statistical analysis and data transformation, whereas the security prompt emphasizes cryptographic operations and access control verification. This domain-aware prompting ensures that generated tasks exhibit coherent thematic focus while exercising skills appropriate to the target category. Skill Taxonomy. In each domain, primitive skills are collected and span multiple dimensions of terminal-based problem solving: (1) algorithmic skills such as graph traversal, constraint satisfaction, and backtracking search; (2) systems skills including file I/O, process management, and network configuration; (3) data processing skills such as parsing, serialization, and transformation pipelines; (4) mathematical skills including numerical integration and statistical modeling; (5) testing skills such as validation, verification, and benchmarking; and (6) web/security skills including HTTP handling, authentication, and vulnerability analysis. Skills for each domain are summarized in Appendix. Compositional Task Synthesis. The LLM is instructed to combine multiple primitives (typically 35 skills per task) in non-trivial ways, producing tasks that require integrated problem-solving rather than isolated skill application. Crucially, the generation prompt emphasizes novelty: the model is guided to invent new scenarios, thereby maximizing the diversity and coverage of the resulting tasks. 6 On Data Engineering for Scaling LLM Terminal Capabilities Table 2: DeepSeek-V3.2 performance on adapted math, coding, and SWE benchmarks. Using the Terminus 2 agent, we evaluate DeepSeek-V3.2 on AIME, LiveCodeBench, and SWE-bench Verified adapted to TerminalBench format, and we find that it performs reasonably well even under this terminal-based setting. Benchmark (pass@1) DeepSeek-V3. AIME 2024, AIME 2025 LiveCodeBench v6 SWE-bench Verified 93.33 67.20 52.40 4.2.3. Task Format and Execution Environment Both generation methods produce tasks in standardized format comprising: (1) natural language task prompt specifying objectives and constraints, (2) pytest-based test cases with configurable weights for partial credit, (3) supplementary input files providing necessary data, and (4) domain-specific Docker environment for consistent execution. The files are structured in the same way as Terminal-Bench, as shown in Figure 2. Note we do not generate oracle solutions, as producing ground-truth code is prohibitively difficult without human verification; instead, we generate tasks are easy to verify yet difficult to solve and use the synthesized test cases to evaluate the correctness of agent solutions. Solution Isolation. key design principle enforced across all generation prompts is the separation between problem specification and solution information. All prompts explicitly instruct the LLM to avoid solution leakage: the task prompt visible to the agent must not reveal the algorithm, implementation approach, or any code that solves the problem. When reference solutions are available (e.g., in seed data), they are used exclusively for deriving ground-truth test expectations. This ensures that generated tasks require problem-solving rather than simple solution retrieval. Pre-Built Docker Images. critical design decision that enables large-scale task generation is the use of pre-built, domain-specific Docker images. Rather than generating unique Dockerfile per task similar to previous works (Austin, 2025; Peng et al., 2025), we maintain fixed set of domain specific docker images, each pre-installing the packages and dependencies commonly required within that domain (e.g., pandas and scikit-learn for data science; cryptography libraries for security). This approach provides three scalability advantages. First, it eliminates Dockerfile validation overhead; by avoiding the costly multi-turn repair often needed for per-task environment generation, pre-built images enable efficient single-pass task creation. Second, it reduces resource footprint, utilizing just 9 shared base images instead of building and caching thousands of unique containers. Third, it decouples environment and task generation, allowing the pipeline to produce diverse scenarios within stable environments while retaining the flexibility for agents to install runtime dependencies. 4.3. Teacher Model We select DeepSeek-V3.2 (Liu et al., 2025) as our teacher model for generating synthetic tasks and trajectories, motivated by its strong performance on Terminal-Bench 2.0  (Table 3)  . To further validate its suitability for producing dataset adapter trajectories, we evaluate DeepSeek-V3.2 on few standard benchmarks adapted to Terminal-Bench format using the Terminus 2 agent framework  (Table 2)  , including AIME 2024 (MAA, 2024), AIME 2025 (MAA, 2025), LiveCodeBench v6 (Jain et al., 2024), and SWE-bench Verified (Jimenez et al., 2023; OpenAI, 2024). 7 On Data Engineering for Scaling LLM Terminal Capabilities 4.4. Data Filtering We first decontaminate our SFT dataset by removing any prompt that has 14-gram overlap with TerminalBench 2.0 test samples. Then, we apply various quality filters, including removing identity leaks and discarding responses that contain Chinese characters. Beyond quality filtering, we also experiment with removing incomplete trajectories generated by the teacher model to discourage the fine-tuned model from becoming overly verbose. When tests are available, we further experiment with only keeping trajectories that pass the tests. 5. Experiments 5.1. Experimental Setup Base Models. We conduct experiments using pretrained models from the Qwen3 family (Yang et al., 2025). We use Qwen3-8B as our primary model for ablation studies, and we additionally experiment with Qwen3-14B and Qwen3-32B to verify that our findings scale with model size. Training Details. Unless specified otherwise, we use the following training hyperparameters: learning rate of 2e-5, weight decay of 1e-4, 2 epochs, maximum sequence length of 32,768 tokens, global batch size of 128, and micro-batch size of 1 per GPU, with AdamW optimizer (ùõΩ = 0.9, 0.95), cosine learning rate scheduler with 10% warmup, and gradient clipping at 1.0. The 8B and 14B models are trained on 4 nodes with 8 GPUs per node (32 total GPUs) using sequence parallelism of 2. The 32B model is trained on 16 nodes (128 total GPUs). All experiments use CPU offloading. Infrastructure. We utilize Harbor (Shaw, 2025), the infrastructure framework from Terminal-Bench 2.0 (Team, 2025), to orchestrate large-scale trajectory generation in containerized environments. We extend Harbor to support Singularity (Kurtzer et al., 2017), enabling deployment on HPC clusters; while this introduces rare failures due to fakeroot overlay limitations, these are acceptable for synthetic data generation. For evaluation, we rely on Daytona (Daytona, 2025) to manage reliable, parallel execution in isolated cloud sandboxes. For SFT experiments, we use veRL (Sheng et al., 2024), an open-source framework designed for efficient LLM training. Table 3: Terminal-Bench 2.0 (TB2.0) results with Terminus 2 agent."
        },
        {
            "title": "Size\nClosed Source Models",
            "content": "TB2.0 GPT-5-Nano GPT-5-Mini GPT-5 GPT-5.1 GPT-5.2 Grok Code Fast 1 Grok 4 GLM 4.6 Gemini 2.5 Flash Gemini 2.5 Pro Gemini 3 Flash Gemini 3 Pro Claude Haiku 4.5 Claude Sonnet 4.5 Claude Opus 4.5 7.90 1.9 24.0 2.5 35.2 3.1 47.6 2.8 54.0 2.9 14.2 2.5 23.1 2.9 24.5 2.4 16.9 2.4 32.6 3.0 51.7 3.1 56.9 2.5 28.3 2.9 42.8 2.8 57.8 2."
        },
        {
            "title": "Open Source Models",
            "content": "Qwen3-8B Qwen3-14B Qwen3-32B Qwen3-Coder GPT-OSS (high) GPT-OSS (high) MiniMax M2 MiniMax M2.1 Kimi K2 Thinking DeepSeek-V3.2 8B 14B 32B 480B 20B 120B 230B 230B 1T 685B 2.47 0.5 4.04 1.3 3.37 1.6 23.9 2.8 3.10 1.5 18.7 2.7 30.0 2.7 29.2 2.9 35.7 2.8 38.2 2.9 5.2. Main Results We evaluate Terminal-Task-Gen by benchmarking NemotronTerminal on Terminal-Bench 2.0 (TB2.0). As shown in Table 3, our models demonstrate substantial gains: NemotronTerminal-8B achieves 13.0 2.2, five-fold increase over Qwen3-8B (2.470.5). Remarkably, despite their modest scale, they rival significantly larger systems; Nemotron-Terminal-14B (20.2 2.7) outperforms the 120B GPT-OSS (18.7 2.7) and Gemini 2.5 Flash (16.9 2.4), while Nemotron-Terminal-32B (27.4 2.4) outperforms the 480B Qwen3-Coder (23.9 2.8). This validates that high-quality trajectory data effectively bridges the gap between efficient models and massive frontier counterparts. Nemotron-Terminal-8B Nemotron-Terminal-14B 13.0 2.2 20.2 2.7 27.4 2. Nemotron-Terminal-32B 8B 14B 32B"
        },
        {
            "title": "Ours",
            "content": "8 On Data Engineering for Scaling LLM Terminal Capabilities Further analysis by task category  (Table 4)  confirms that our synthetic data unlocks critical capabilities where base models of all scales failed completely. While the Qwen3-14B and 32B models both scored 0.0 in Data Querying and Model Training, Nemotron-Terminal-32B surged to 60.0 and 50.0 respectively, representing profound leap in functional utility. Similar transformations occur in Security (2.5 to 27.5), Data Processing (5.0 to 50.0), and Software Engineering (5.0 to 31.7) for the 32B variant, indicating that larger parameter count alone is insufficient for strong terminal capability. Furthermore, consistent gains in System Administration (6.7 to 31.1) and Debugging (0.0 to 33.3) demonstrate that our approach successfully instills domain-specific skillssuch as complex file manipulation and command-line troubleshooting, which are effectively absent in the original models. Table 4: Performance of models by Terminal-Bench category. TB2.0 scores for Qwen3 and NemotronTerminal models, broken down by category, showing where Nemotron-Terminal most improves over the Qwen3 baselines. Nemotron-Terminal 32B 14B 8B"
        },
        {
            "title": "Category",
            "content": "Software & System Software Engineering (24) System Administration (9) Debugging (3) Security (8) File Operations (4) Data & Science Data Science (8) Data Processing (4) Data Querying (1) Scientific Computing (7) Mathematics (4) Machine Learning Machine Learning (3) Model Training (4)"
        },
        {
            "title": "Other",
            "content": "Personal Assistant (1) Games (1) Video Processing (1) Unknown (7)"
        },
        {
            "title": "Overall",
            "content": "Qwen3 14B 6.70 6.70 0.00 0.00 0.00 7.50 5.00 0.00 0.00 0.00 8B 1.70 13.3 0.00 0.00 0.00 2.50 0.00 0.00 0.00 0. 32B 5.00 6.70 0.00 2.50 0.00 0.00 5.00 0.00 2.90 0.00 9.20 22.2 20.0 12.5 0.00 7.50 35.0 20.0 0.00 0.00 18.3 28.9 40.0 17.5 10. 17.5 40.0 40.0 2.90 0.00 0.00 0.00 0.00 0.00 0.00 0.00 6.70 5.00 13.3 20. 0.00 0.00 0.00 5.70 2.50 0.00 0.00 0.00 8.60 4.00 0.00 0.00 0.00 8.60 3.40 80.0 0.00 0.00 34.3 13.0 80.0 0.00 0.00 34.3 20.2 31.7 31.1 33.3 27.5 5. 27.5 50.0 60.0 0.00 0.00 13.3 50.0 100 0.00 0.00 34.3 27.4 5.3. Ablation on Dataset Components As demonstrated in Table 5, each data source provides complementary value. For dataset adapters, while individual splits like Math (5.39%) and Code (6.29%) underperform compared to SWE (7.02%), combining them leads to significant performance jump to 9.66%, confirming that merging distinct domains yields further improvement than any single source alone. similar pattern of robustness appears in synthetic tasks, where the skill-based data drives the primary gains (12.4%); although adding seed-based data does not increase the mean score, it successfully reduces variance and makes the model more robust. 5.4. Filtering Strategies Trajectory Filtering We investigate few trajectory filtering strategies discussed in Section 4.4. For dataset adapters  (Table 6)  , while we observe no significant difference on each subset, the no-filter setting yields the highest performance on the full set (9.66%) and is thus adopted. For synthetic tasks  (Table 7)  , the impact is even more substantial: no filtering (12.4%) significantly surpasses both complete-only (6.74%) and success-only (5.06%) strategies. 9 On Data Engineering for Scaling LLM Terminal Capabilities Table 5: Qwen3-8B SFT results across training data sources. We show total samples and TB2.0 results from training on various subsets. For both dataset adapters and synthetic tasks, we achieve the strongest results from combining all data sources together. Table 6: Qwen3-8B ablations on dataset adapter filtering strategies. Because the dataset adapter subset does not include test cases, we compare no filtering to complete-only filtering and observe no significant performance difference."
        },
        {
            "title": "Data Split\nDataset Adapters",
            "content": "# Samples TB2."
        },
        {
            "title": "Math\nCode\nSWE\nAll",
            "content": "Synthetic Tasks Seed-based Skill-based All 162,692 31,960 31,661 226,313 124,366 139,841 264,207 5.39 1.65 6.29 1.65 7.02 2.13 9.66 2.11 6.18 1.91 12.4 2.38 12.4 2."
        },
        {
            "title": "Math",
            "content": "# Samples TB2.0 Complete-only No filter 147,718 162,692 7.19 1.87 5.39 1."
        },
        {
            "title": "Code",
            "content": "Complete-only No filter"
        },
        {
            "title": "SWE",
            "content": "Complete-only No filter"
        },
        {
            "title": "All",
            "content": "20,169 31,960 6.07 1.73 6.29 1.65 29,053 31,661 5.39 1.68 7.02 2.13 Complete-only No filter 196,940 226, 8.09 1.84 9.66 2.11 This performance gap suggests that strict filtering is detrimental as it discards over half the available training data. Moreover, retaining unsuccessful trajectories appears to provide valuable supervision, exposing the model to realistic error states and recovery patterns that enhance overall robustness. Table 7: Qwen3-8B ablations on synthetic task filtering. On the synthetic task subset, we experiment with no filtering, complete-only filtering, and success-only filtering. No filtering yields significantly better performance. Table 8: Impact of SFT sequence length and YaRN2 scaling on Qwen3-8B performance. We find that training and evaluating with default Qwen3 context settings yields the strongest performance. Filter Complete-only Success-only No filter # Samples 104,603 83,448 264, TB2.0 6.74 2.20 5.06 2.11 12.4 2."
        },
        {
            "title": "Eval Max\nLen",
            "content": "SFT YaRN2 Eval YaRN2 TB2.0 32,768 32,768 65,536 65,536 40,960 65,536 65,536 65,536 13.0 2.2 11.9 2.0 10.3 2.0 11.9 2.1 5.5. Long Context Training and Evaluation Terminal trajectories can vary significantly in their turn counts (Appendix A.1), which in turn induces large differences in token counts depending on task type and difficulty. As shown in Appendix A.1, most trajectories fit within the default maximum sequence length of Qwen3 models (32,768 tokens), but nontrivial subset exceeds this limit and is truncated by SFT. Motivated by this, we investigate long-context training for Qwen3-8B. Specifically, we compare SFT with 65,536-token context window without YaRN2 (Peng et al., 2023), SFT with YaRN2, and baseline that only applies YaRN2 at evaluation time. Across these settings, we observe no significant performance differences  (Table 8)  . Moreover, evaluating under the standard Qwen3-8B setup with 40,960-token context window yields stronger results. Our results suggest that extending the context length slightly hurts performance; most high-quality supervision already fits within the standard window, while the long-tail trajectories tend to be noisy and less informative. 5.6. Curriculum Learning We investigate two SFT curriculum strategies for data mixing: (1) two-stage curriculum, where we first train on dataset adapters, followed by synthetic task data; and (2) single-stage strategy, where we train on all 10 On Data Engineering for Scaling LLM Terminal Capabilities datasets concurrently. As demonstrated in Table 9, the two-stage curriculum yields no performance advantage over simple mixed training. Consequently, we adopt the single-stage mixed training strategy for all other experiments in this paper."
        },
        {
            "title": "Strategy",
            "content": "Model Qwen3-8B mixed Qwen3-8B TB2.0 13.03 2.16 curriculum 10.39 1.71 Figure 4: Impact of training data scale on model performance. Our scaling experiments show that TB2.0 performance increases with training data volume for both Qwen38B and Qwen3-14B. Table 9: Ablations on curriculum learning strategies. We compare mixed single-stage strategy with two-stage curriculum. The mixed strategy achieves the strongest performance. 5.7. Scaling Experiments We investigate the impact of training data scale on model performance by fine-tuning Qwen3-8B and Qwen314B models on varying percentages of synthetic training data (0%, 1%, 2%, 5%, 10% and 100%). Both models demonstrate consistent performance improvements as training data increases (Figure 4). The larger 14B model not only achieves higher absolute performance across all data scales but also exhibits greater gains from additional training data. These results demonstrate that both model capacity and training data scale are critical factors for performance. 6. Conclusion In this work, we address the data scarcity bottleneck in terminal agent training by introducing Terminal-TaskGen, scalable framework that synergizes large-scale dataset adaptation with targeted synthetic task generation. Our systematic study demonstrates that precise data engineering enables the efficient Nemotron-Terminal family to significantly outperform its Qwen3 base and rival larger frontier models on Terminal-Bench 2.0, proving that high-quality, diverse trajectories are more pivotal than sheer parameter scale. Looking ahead, we see significant potential in extending this foundation with Reinforcement Learning (RL), leveraging verifiable execution feedback to enable self-correction and optimal planning for long-horizon tasks. We release our models and most of our synthetic datasets, including the adapter and skill-based task subsets, to democratize research in autonomous terminal agents. References [1] Wasi Uddin Ahmad, Sean Narenthiran, Somshubra Majumdar, Aleksander Ficek, Siddhartha Jain, Jocelyn Huang, Vahid Noroozi, and Boris Ginsburg. Opencodereasoning: Advancing data distillation for competitive coding. arXiv preprint arXiv:2504.01943, 2025. 5 [2] Anthropic. Claude code: Best practices for agentic coding. https://www.anthropic.com/engineering/ claude-code-best-practices, Apr 2025. 1, 3 [3] Anthropic. Introducing claude opus 4.5. https://www.anthropic.com/news/claude-opus-4-5, 2025. 2 On Data Engineering for Scaling LLM Terminal Capabilities [4] Antigma. Meet ante. https://antigma.ai/, 2025. 2, 3 [5] Dan Austin. Terminal bench agentic data pipeline. https://github.com/Danau5tin/ tbench-agentic-data-pipeline, 2025. 2, 3, [6] Ibragim Badertdinov, Alexander Golubev, Maksim Nekrashevich, Anton Shevtsov, Simon Karasik, Andrei Andriushchenko, Maria Trofimova, Daria Litvintseva, and Boris Yangel. Swe-rebench: An automated pipeline for task collection and decontaminated evaluation of software engineering agents. arXiv preprint arXiv:2505.20411, 2025. 5 [7] Daytona. Daytona cloud: Secure and elastic infrastructure for running your AI-generated code. https: //www.daytona.io, 2025. 8 [8] DCAgent. bash_textbook_tasks_traces. https://huggingface.co/datasets/DCAgent/bash_ textbook_tasks_traces, 2025. 2, 3 [9] Google DeepMind. new era of products-and-platforms/products/gemini/gemini-3/, 2025. 2 intelligence with gemini 3. https://blog.google/ [10] ML Foundations Development. staqc-sandboxes-traces-terminus-2. https://huggingface.co/ datasets/mlfoundations-dev/staqc-sandboxes-traces-terminus-2, 2025. 2, 3 [11] ML Foundations Development. code-contests-sandboxes-traces-terminus-2. https://huggingface.co/ datasets/mlfoundations-dev/code-contests-sandboxes-traces-terminus-2, 2025. 2, 3 [12] Daya Guo, Dejian Yang, Haowei Zhang, Junxiao Song, Ruoyu Zhang, Runxin Xu, Qihao Zhu, Shirong Ma, Peiyi Wang, Xiao Bi, et al. Deepseek-r1: Incentivizing reasoning capability in llms via reinforcement learning. arXiv preprint arXiv:2501.12948, 2025. [13] Intelligent Internet. Ii-agent. https://ii.inc/web/blog/post/ii-agent, 2025. 2, 3 [14] Naman Jain, King Han, Alex Gu, Wen-Ding Li, Fanjia Yan, Tianjun Zhang, Sida Wang, Armando SolarLezama, Koushik Sen, and Ion Stoica. Livecodebench: Holistic and contamination free evaluation of large language models for code. arXiv preprint arXiv:2403.07974, 2024. 7 [15] JetBrains. Junie cli: Llm-agnostic coding agent built for real-world development. https://junie. jetbrains.com/, 2025. 2, 3 [16] Carlos Jimenez, John Yang, Alexander Wettig, Shunyu Yao, Kexin Pei, Ofir Press, and Karthik Narasimhan. Swe-bench: Can language models resolve real-world github issues? arXiv preprint arXiv:2310.06770, 2023. 5, 7 [17] Gregory M. Kurtzer, Vanessa Sochat, and Michael W. Bauer. Singularity: Scientific containers for mobility of compute. PloS One, 12(5):e0177459, 2017. doi: 10.1371/journal.pone.0177459. 8 [18] Letta. Building the #1 open-source terminal-use agent using letta. https://www.letta.com/blog/ terminal-bench, 2025. 2, 3 [19] Aixin Liu, Aoxue Mei, Bangcai Lin, Bing Xue, Bingxuan Wang, Bingzheng Xu, Bochao Wu, Bowei Zhang, Chaofan Lin, Chen Dong, et al. Deepseek-v3. 2: Pushing the frontier of open large language models. arXiv preprint arXiv:2512.02556, 2025. 2, 7 [20] Ziyang Luo, Can Xu, Pu Zhao, Qingfeng Sun, Xiubo Geng, Wenxiang Hu, Chongyang Tao, Jing Ma, Qingwei Lin, and Daxin Jiang. Wizardcoder: Empowering code large language models with evol-instruct. arXiv preprint arXiv:2306.08568, 2023. 3 On Data Engineering for Scaling LLM Terminal Capabilities [21] MAA. Aime 2024 benchmark: Problems from the american invitational mathematics examination. https: //huggingface.co/datasets/Maxwell-Jia/AIME_2024, 2024. Benchmark of AIME 2024 problems for evaluating mathematical reasoning in language models. 7 [22] MAA. Aime 2025 benchmark: Problems from the american invitational mathematics examination. https: //ukgovernmentbeis.github.io/inspect_evals/evals/mathematics/aime2025/, 2025. Benchmark of AIME 2025 problems for evaluating mathematical reasoning in language models. 7 [23] Dirk Merkel. Docker: lightweight linux containers for consistent development and deployment. In Linux Journal, number 239. Belltown Media, 2014. 4 [24] Mike Merrill, Alexander Shaw, Nicholas Carlini, Boxuan Li, Harsh Raj, Ivan Bercovich, Lin Shi, Jeong Yeon Shin, Thomas Walshe, Kelly Buchanan, et al. Terminal-bench: Benchmarking agents on hard, realistic tasks in command line interfaces. arXiv preprint arXiv:2601.11868, 2026. 2, [25] MiniMax. Minimax m2 & agent: Ingenious in simplicity. https://www.minimax.io/news/minimax-m2, 2025. 2 [26] Arindam Mitra, Luciano Del Corro, Guoqing Zheng, Shweti Mahajan, Dany Rouhana, Andres Codas, Yadong Lu, Wei-ge Chen, Olga Vrousgos, Corby Rosset, et al. Agentinstruct: Toward generative teaching with agentic flows. arXiv preprint arXiv:2407.03502, 2024. 3 [27] Moonshot AI. Introducing kimi k2 thinking. https://moonshotai.github.io/Kimi-K2/thinking. html, Nov 2025. [28] Ivan Moshkov, Darragh Hanley, Ivan Sorokin, Shubham Toshniwal, Christof Henkel, Benedikt Schifferer, Wei Du, and Igor Gitman. Aimo-2 winning solution: Building state-of-the-art mathematical reasoning models with openmathreasoning dataset. arXiv preprint arXiv:2504.16891, 2025. 5 [29] Mux. Terminal benchmarking. https://mux.coder.com/reference/benchmarking, 2025. 2, 3 [30] Jack Nichols. How we scored #1 on terminal-bench (52%). https://www.warp.dev/blog/ terminal-bench, 2025. 2, 3 [31] OpenAI. Introducing introducing-swe-bench-verified/, Aug 2024. 7 swe-bench verified. https://openai.com/index/ [32] OpenAI. Introducing codex. https://openai.com/index/introducing-codex/, May 2025. Overview of the Codex coding agent accessible via ChatGPT and related clients. 1, [33] OpenAI. Introducing gpt-5.2. https://openai.com/index/introducing-gpt-5-2/, 2025. 2 [34] Bowen Peng, Jeffrey Quesnelle, Honglu Fan, and Enrico Shippole. Yarn: Efficient context window extension of large language models. arXiv preprint arXiv:2309.00071, 2023. 10 [35] Xiaoxuan Peng, Xinyu Lu, Kaiqi Zhang, Taosong Fang, Boxi Cao, and Yaojie Lu. Litecoder-terminal: Lightweight terminal agents with <1k synthesized trajectories. Lite-Coder/litecoder-terminal-preview, 2025. 2, 3, 7 https://huggingface.co/blog/ [36] Alex Shaw. Harbor Framework, November 2025. URL https://github.com/laude-institute/harbor. 8 [37] Guangming Sheng, Chi Zhang, Zilingfeng Ye, Xibin Wu, Wang Zhang, Ru Zhang, Yanghua Peng, Haibin Lin, and Chuan Wu. Hybridflow: flexible and efficient rlhf framework. arXiv preprint arXiv: 2409.19256, 2024. 8 13 On Data Engineering for Scaling LLM Terminal Capabilities [38] Abhay Singhal, Leo Tchourakov, Daniel Flaherty, and Stepan Bedratiuk. Droid: The #1 software development agent on terminal-bench. Factory AI News, September 2025. URL https://factory.ai/news/ terminal-bench. 2, [39] Shivchander Sudalairaj, Abhishek Bhandwaldar, Aldo Pareja, Kai Xu, David Cox, and Akash Srivastava. Lab: Large-scale alignment for chatbots. arXiv preprint arXiv:2403.01081, 2024. 3 [40] The Terminal-Bench Team. Adapters. https://harborframework.com/docs/adapters, 2025. 2 [41] The Terminal-Bench Team. Terminal-bench: benchmark for ai agents in terminal environments, Apr 2025. URL https://github.com/laude-institute/terminal-bench. 3, 4, 8 [42] Boxin Wang, Chankyu Lee, Nayeon Lee, Sheng-Chieh Lin, Wenliang Dai, Yang Chen, Yangyi Chen, Zhuolin Yang, Zihan Liu, Mohammad Shoeybi, et al. Nemotron-cascade: Scaling cascaded reinforcement learning for general-purpose reasoning models. arXiv preprint arXiv:2512.13607, 2025. [43] Chengxing Xie, Bowen Li, Chang Gao, He Du, Wai Lam, Difan Zou, and Kai Chen. Swe-fixer: Training open-source llms for effective and efficient github issue resolution. arXiv preprint arXiv:2501.05040, 2025. 5 [44] Can Xu, Qingfeng Sun, Kai Zheng, Xiubo Geng, Pu Zhao, Jiazhan Feng, Chongyang Tao, and Daxin Jiang. Wizardlm: Empowering large language models to follow complex instructions. arXiv preprint arXiv:2304.12244, 2023. 3 [45] Zhangchen Xu, Fengqing Jiang, Luyao Niu, Yuntian Deng, Radha Poovendran, Yejin Choi, and Bill Yuchen Lin. Magpie: Alignment data synthesis from scratch by prompting aligned llms with nothing. arXiv preprint arXiv:2406.08464, 2024. 3 [46] An Yang, Anfeng Li, Baosong Yang, Beichen Zhang, Binyuan Hui, Bo Zheng, Bowen Yu, Chang Gao, Chengen Huang, Chenxu Lv, et al. Qwen3 technical report. arXiv preprint arXiv:2505.09388, 2025. 2, 3, 8 [47] John Yang, Kilian Lieret, Carlos Jimenez, Alexander Wettig, Kabir Khandpur, Yanzhe Zhang, Binyuan Hui, Ofir Press, Ludwig Schmidt, and Diyi Yang. Swe-smith: Scaling data for software engineering agents. arXiv preprint arXiv:2504.21798, 2025. 5 On Data Engineering for Scaling LLM Terminal Capabilities A. Appendix Table 10: Skills Summary by Domain"
        },
        {
            "title": "Example Skill",
            "content": "Systems, Data Processing, Web Security, Algorithmic, Testing Algorithmic, Systems, Data Processing, Web Security, Testing, Mathematical File I/O, Navigation, Data Parsing, Transformation, Archives, Resources, Network Query Construction, Data Comprehension, Graph Processing, Result Processing Systems, Data Processing, Algorithmic, Mathematical, Testing, Web Security Systems, Debugging, Testing, Algorithmic, Data Processing, Mathematical Data Processing, Algorithmic, Mathematical, Systems, Testing, Statistical, Web Security Data I/O, Manipulation, String/Text, Algorithmic, Mathematical, Time Series, Systems, Testing Filesystem, Process/Service, Network, Configuration, Deployment, Data Processing, Security, Shell Scripting, Testing, Algorithmic Craft exploit payloads to bypass authentication and identify vulnerabilities Implement graph traversal (BFS/DFS) for dependency resolution Parse structured formats (JSON/XML/CSV) with encoding and validation Writing queries using the formal syntax of declarative query languages for structured data Load and transform tabular data with groupby, filtering, and aggregation Resolve through constraint analysis Computing distance metrics between discrete probability distributions Build transformation pipelines with interpolation and feature extraction package dependency conflicts Manage file permissions, configure services, and automate tasks with shell scripts A.1. Synthetic Trajectory Analysis We compute the distribution of the number of tokens (Figure 5) and turns (Figure 6) in our trajectories, separating synthetic tasks from dataset adapters. Figure 5: Distribution of # tokens in the generated trajectories. A.2. Details for Trajectory Generation We use Terminus 2 to generate all trajectories for SFT. We provide the full Terminus 2 system prompt template in Figure 7, which includes placeholders for instruction and terminal_state. Our Terminal-Task-Gen pipeline generates tasks with explicit instructions that replace the instruction placeholder. For dataset adapters, we instead insert the original prompt followed by domain-specific suffix, shown in Figures 810. The terminal_state placeholder is populated with the latest terminal output summarizing the current shell state. 15 On Data Engineering for Scaling LLM Terminal Capabilities Figure 6: Distribution of # turns in the generated trajectories. A.3. Details for Synthetic Task Generation Curation of Primitive Skills We showcase the skill types and examples for each domain in Table 10. These domains encompass wide spectrum of capabilities required for robust terminal interaction. For instance, Security tasks involve skill types such as Systems, Data Processing, Web Security, Algorithmic, and Testing, with practical applications like crafting exploit payloads to bypass authentication. Similarly, Software Engineering combines Algorithmic and Systems skills to implement complex logic like graph traversals for dependency resolution. File Operations and System Administration focus on core infrastructure tasks, ranging from parsing structured formats (JSON/XML/CSV) to managing file permissions and automating service configurations. We also curate specialized skills for Data Science, Scientific Computing, and Data Processing, which require mathematical and statistical proficiency to build transformation pipelines or compute distance metrics between probability distributions. By systematically categorizing these primitive skills from low-level filesystem manipulation to high-level algorithmic reasoning, we ensure comprehensive coverage of the challenges an autonomous agent must navigate in terminal environment. Prompts for Synthetic Task Generation We employ modular prompting strategy that fuses structural backbone with specialized constraints. Figure 11 presents the system prompt, which governs general task logic, while Figures 1219 detail the domain-specific requirement modules injected into this template. These modules define the preconditions and objectives for each vertical; for instance, the Security module mandates crafting exploit payloads to identify vulnerabilities, while the Data Processing module requires building transformation pipelines with interpolation. This ensures that generated tasks precisely align with the diverse primitive skills outlined previously. 16 On Data Engineering for Scaling LLM Terminal Capabilities Terminus 2 Agent: System Prompt Template You are an AI assistant tasked with solving command-line tasks in Linux environment. You will be given task description and the output from previously executed commands. Your goal is to solve the task by providing batches of shell commands. Format your response as JSON with the following structure: {{ \"analysis\": \"Analyze the current state based on the terminal output provided. What do you see? What has been accomplished? What still needs to be done?\", \"plan\": \"Describe your plan for the next steps. What commands will you run and why? Be specific about what you expect each command to accomplish.\", \"commands\": [ {{ \"keystrokes\": \"ls -lan\", \"duration\": 0.1 }}, {{ \"keystrokes\": \"cd projectn\", \"duration\": 0. }} ], \"task_complete\": true }} Required fields: - \"analysis\": Your analysis of the current situation - \"plan\": Your plan for the next steps - \"commands\": Array of command objects to execute Optional fields: - \"task_complete\": Boolean indicating if the task is complete (defaults to false if not present) Command object structure: - \"keystrokes\": String containing the exact keystrokes to send to the terminal (required) - \"duration\": Number of seconds to wait for the command to complete before the next command will be executed ( defaults to 1.0 if not present) IMPORTANT: The text inside \"keystrokes\" will be used completely verbatim as keystrokes. Write commands exactly as you want them sent to the terminal: - Most bash commands should end with newline (n) to cause them to execute - For special key sequences, use tmux-style escape sequences: - C-c for Ctrl+C - C-d for Ctrl+D The \"duration\" attribute specifies the number of seconds to wait for the command to complete (default: 1.0) before the next command will be executed. On immediate tasks (e.g., cd, ls, echo, cat) set duration of 0.1 seconds. On commands (e.g., gcc, find, rustc) set duration of 1.0 seconds. On slow commands (e.g., make, python3 [ long running script], wget [file]) set an appropriate duration as you determine necessary. It is better to set smaller duration than longer duration. It is always possible to wait again if the prior output has not finished, by running {{\"keystrokes\": \"\", \"duration\": 10.0}} on subsequent requests to wait longer. Never wait longer than 60 seconds; prefer to poll to see intermediate result status. Important notes: - Each commands keystrokes are sent exactly as written to the terminal - Do not include extra whitespace before or after the keystrokes unless its part of the intended command - Extra text before or after the JSON will generate warnings but be tolerated - The JSON must be valid - use proper escaping for quotes and special characters within strings - Commands array can be empty if you want to wait without taking action Task Description: {instruction} Current terminal state: {terminal_state} Figure 7: System prompt template for Terminus 2. 17 On Data Engineering for Scaling LLM Terminal Capabilities Math Dataset Adapter: Instruction Template {math_prompt} Please place your final answer in file named /app/solution.txt. Figure 8: Instruction template for math dataset adapter. Code Dataset Adapter: Instruction Template {code_prompt} Write Python code to solve the problem. Please place the solution code in file named /app/solution.py. Figure 9: Instruction template for code dataset adapter. SWE Dataset Adapter: Instruction Template {swe_prompt} Please first localize the bug based on the issue statement, generate *SEARCH/REPLACE* edits to fix the issue, and save the diff to file named /app/solution.patch. Figure 10: Instruction template for SWE dataset adapter. 18 On Data Engineering for Scaling LLM Terminal Capabilities Skill-based Task Generation: Prompt Template SYSTEM PROMPT CONSTRUCTION: 1. Role Definition: You are an expert at creating domain tasks for AI agent training. 2. Domain Context (Insert Variable Module Here): [INSERT CONTENT FROM SPECIFIC DOMAIN TABLES BELOW (e.g., Data Science, Security, etc.)] 3. Universal Task Requirements: - Challenging to solve: Requires domain knowledge, analytical thinking, and efficient implementation. - Easy to verify: Success must be determinable by programmatically checking outputs, exit codes, or system state. - Self-contained: All necessary information must be in the prompt. - Realistic: The problem should resemble tasks professionals face in this domain. 4. Output Format: You MUST output using these XML tags: - prompt: The task description with explicit requirements. - tests: Pytest functions to verify the solution. - weights: Test scoring distribution. - info: Task metadata. - files: Input data or initial file structure. - test_requirements: Python packages required for testing. 5. Critical Rules: - No Leakage: Never include code that solves the task in the prompt. - Verification: Prioritize tasks with clear, programmatic verification. - Originality: Tasks should require thought, not just copying standard tutorials. - Complete Specification: Include all information needed to complete the task (file paths, formats, constraints). USER MESSAGE CONSTRUCTION (Appended to System Prompt): # Task Generation Request Category: CATEGORY ## Primitive Skills (Building Blocks) Primitive_Skills ## Pre-designed Docker Environment Tasks will run in this pre-designed Docker environment: DOCKERFILE_CONTENT If additional packages are needed for testing, list them in test_requirements. ## Instructions CREATE NOVEL TASK that: 1. Combines 3-5 primitives in creative, unexpected way 2. Is NOT recreation of common coding challenges 3. Is challenging to solve but easy to verify 4. Has clear, unambiguous specifications Think of an original scenario or application - dont just combine primitives mechanically. Figure 11: Prompt template used for all skill-based generation. Domain-specific modules are inserted into section 2. 19 On Data Engineering for Scaling LLM Terminal Capabilities Domain Module: Data Processing # Data Processing Task Builder You are an expert at creating data processing programming tasks for AI agent training. Domain Focus Create tasks involving: - **File format handling**: CSV, JSON, XML, Parquet, binary formats - **Data transformation**: Cleaning, normalization, aggregation - **ETL pipelines**: Extract, transform, load workflows - **Stream processing**: Real-time data handling - **Data validation**: Schema enforcement, error handling Your Task Create programming task that tests data processing skills. The task should be: 1. **Challenging to solve** - Requires domain knowledge and analytical thinking 2. **Easy to verify** - Success can be determined by checking outputs or state 3. **Self-contained** - All information needed is in the prompt 4. **Realistic** - Resembles tasks professional might actually face Figure 12: Module for Data Processing tasks. Domain Module: Data Querying # Data Querying Task Builder You are an expert at creating data querying programming tasks for AI agent training. Domain Focus Create tasks involving: - **SQL operations**: Complex joins, window functions, CTEs - **Query optimization**: Indexes, execution plans, performance - **Database operations**: Schema design, migrations, constraints - **NoSQL patterns**: Document, key-value, graph queries - **Data retrieval**: Pagination, filtering, full-text search Your Task Create programming task that tests data querying skills. The task should be: 1. **Challenging to solve** - Requires domain knowledge and analytical thinking 2. **Easy to verify** - Success can be determined by checking outputs or state 3. **Self-contained** - All information needed is in the prompt 4. **Realistic** - Resembles tasks professional might actually face Figure 13: Module for Data Querying tasks. 20 On Data Engineering for Scaling LLM Terminal Capabilities Domain Module: Data Science # Data Science Task Builder You are an expert at creating data science programming tasks for AI agent training. Domain Focus Create tasks involving: - **Exploratory Analysis**: Statistical summaries, visualization, pattern discovery - **Feature Engineering**: Transformation, encoding, selection, creation - **Statistical Modeling**: Regression, hypothesis testing, Bayesian analysis - **Data Mining**: Clustering, association rules, anomaly detection - **Reporting**: Automated insights, metric computation, summary generation Your Task Create programming task that tests data science skills. The task should be: 1. **Challenging to solve** - Requires statistical thinking and data intuition 2. **Easy to verify** - Success can be determined by checking outputs or metrics 3. **Self-contained** - All information needed is in the prompt 4. **Realistic** - Resembles tasks data scientist might actually face Figure 14: Module for Data Science tasks. Domain Module: Debugging # Debugging Task Builder You are an expert at creating debugging programming tasks for AI agent training. Domain Focus Create tasks involving: - **Error diagnosis**: Stack traces, logs, error messages - **Root cause analysis**: Bisection, delta debugging - **Performance debugging**: Profiling, bottleneck identification - **Memory issues**: Leaks, corruption, allocation problems - **Concurrency bugs**: Race conditions, deadlocks, livelocks Your Task Create programming task that tests debugging skills. The task should be: 1. **Challenging to solve** - Requires domain knowledge and analytical thinking 2. **Easy to verify** - Success can be determined by checking outputs or state 3. **Self-contained** - All information needed is in the prompt 4. **Realistic** - Resembles tasks professional might actually face Figure 15: Module for Debugging tasks. 21 On Data Engineering for Scaling LLM Terminal Capabilities Domain Module: File Operations # File Operations Task Builder You are an expert at creating file operations programming tasks for AI agent training. Domain Focus Create tasks involving: - **File I/O**: Reading, writing, appending, seeking - **Directory operations**: Traversal, creation, permissions - **File formats**: Binary, text, structured data - **Compression**: Zip, tar, gzip, custom formats - **File system operations**: Links, permissions, metadata Your Task Create programming task that tests file operations skills. The task should be: 1. **Challenging to solve** - Requires domain knowledge and analytical thinking 2. **Easy to verify** - Success can be determined by checking outputs or state 3. **Self-contained** - All information needed is in the prompt 4. **Realistic** - Resembles tasks professional might actually face Figure 16: Module for File Operations tasks. Domain Module: Scientific Computing # Scientific Computing Task Builder You are an expert at creating scientific computing programming tasks for AI agent training. Domain Focus Create tasks involving: - **Numerical simulation**: ODEs, PDEs, Monte Carlo - **Signal processing**: FFT, filtering, spectral analysis - **Statistical analysis**: Hypothesis testing, regression, sampling - **Visualization**: Plotting, data exploration - **Domain-specific**: Physics, biology, chemistry applications Your Task Create programming task that tests scientific computing skills. The task should be: 1. **Challenging to solve** - Requires domain knowledge and analytical thinking 2. **Easy to verify** - Success can be determined by checking outputs or state 3. **Self-contained** - All information needed is in the prompt 4. **Realistic** - Resembles tasks professional might actually face Figure 17: Module for Scientific Computing tasks. 22 On Data Engineering for Scaling LLM Terminal Capabilities Domain Module: Security # Security Task Builder You are an expert at creating security programming tasks for AI agent training. Domain Focus Create tasks involving: - **Cryptography**: Encryption, decryption, key management, hash functions - **Vulnerability Analysis**: Code review, exploit identification, security auditing - **Authentication**: Password handling, token validation, session management - **Network Security**: Protocol analysis, traffic inspection, firewall rules - **Secure Coding**: Input validation, output encoding, secure defaults Your Task Create programming task that tests security skills. The task should be: 1. **Challenging to solve** - Requires security knowledge and analytical thinking 2. **Easy to verify** - Success can be determined by checking outputs or state 3. **Self-contained** - All information needed is in the prompt 4. **Realistic** - Resembles tasks security engineer might actually face Figure 18: Module for Security tasks. Domain Module: Software Engineering # Software Engineering Task Builder You are an expert at creating software engineering programming tasks for AI agent training. Domain Focus Create tasks involving: - **Code quality**: Refactoring, testing, documentation - **Build systems**: Compilation, linking, packaging - **Version control**: Git operations, merge conflicts - **API design**: REST, GraphQL, protocol design - **Architecture**: Patterns, modularity, scalability Your Task Create programming task that tests software engineering skills. The task should be: 1. **Challenging to solve** - Requires domain knowledge and analytical thinking 2. **Easy to verify** - Success can be determined by checking outputs or state 3. **Self-contained** - All information needed is in the prompt 4. **Realistic** - Resembles tasks professional might actually face Figure 19: Module for Software Engineering tasks. 23 On Data Engineering for Scaling LLM Terminal Capabilities Domain Module: System Administration # System Administration Task Builder You are an expert at creating system administration programming tasks for AI agent training. Domain Focus Create tasks involving: - **Process management**: Services, daemons, scheduling - **Network configuration**: Routing, firewall, DNS - **Storage management**: Filesystems, RAID, backups - **Monitoring**: Logging, alerting, metrics - **Automation**: Scripts, configuration management Your Task Create programming task that tests system administration skills. The task should be: 1. **Challenging to solve** - Requires domain knowledge and analytical thinking 2. **Easy to verify** - Success can be determined by checking outputs or state 3. **Self-contained** - All information needed is in the prompt 4. **Realistic** - Resembles tasks professional might actually face Figure 20: Module for System Administration tasks."
        }
    ],
    "affiliations": [
        "NVIDIA"
    ]
}