{
    "paper_title": "Instruction-Guided Lesion Segmentation for Chest X-rays with Automatically Generated Large-Scale Dataset",
    "authors": [
        "Geon Choi",
        "Hangyul Yoon",
        "Hyunju Shin",
        "Hyunki Park",
        "Sang Hoon Seo",
        "Eunho Yang",
        "Edward Choi"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "The applicability of current lesion segmentation models for chest X-rays (CXRs) has been limited both by a small number of target labels and the reliance on long, detailed expert-level text inputs, creating a barrier to practical use. To address these limitations, we introduce a new paradigm: instruction-guided lesion segmentation (ILS), which is designed to segment diverse lesion types based on simple, user-friendly instructions. Under this paradigm, we construct MIMIC-ILS, the first large-scale instruction-answer dataset for CXR lesion segmentation, using our fully automated multimodal pipeline that generates annotations from chest X-ray images and their corresponding reports. MIMIC-ILS contains 1.1M instruction-answer pairs derived from 192K images and 91K unique segmentation masks, covering seven major lesion types. To empirically demonstrate its utility, we introduce ROSALIA, a vision-language model fine-tuned on MIMIC-ILS. ROSALIA can segment diverse lesions and provide textual explanations in response to user instructions. The model achieves high segmentation and textual accuracy in our newly proposed task, highlighting the effectiveness of our pipeline and the value of MIMIC-ILS as a foundational resource for pixel-level CXR lesion grounding."
        },
        {
            "title": "Start",
            "content": "Instruction-Guided Lesion Segmentation for Chest X-rays with Automatically Generated Large-Scale Dataset Geon Choi1, Hangyul Yoon1, Hyunju Shin2 Hyunki Park2 Sang Hoon Seo2 Eunho Yang1,3 Edward Choi1,* 2Samsung Medical Center 3AITRICS 1KAIST 5 2 0 2 9 ] . [ 1 6 8 1 5 1 . 1 1 5 2 : r Figure 1. Examples of the instruction-guided CXR lesion segmentation task. Given text instructions for various lesion types and locations of interest, ROSALIA, VLM trained on our MIMIC-ILS dataset, can: (A) segment lesions in specified location, (B) segment lesions globally, and (C) detect empty-target cases. As can be seen in (A), ROSALIA correctly ignores the unrequested lesion in the left lung."
        },
        {
            "title": "Abstract",
            "content": "1. Introduction The applicability of current lesion segmentation models for chest X-rays (CXRs) has been limited both by small number of target labels and the reliance on long, detailed expert-level text inputs, creating barrier to practical use. To address these limitations, we introduce new paradigm: instructionguided lesion segmentation (ILS), which is designed to segment diverse lesion types based on simple, user-friendly instructions. Under this paradigm, we construct MIMIC-ILS, the first large-scale instruction-answer dataset for CXR lesion segmentation, using our fully automated multimodal pipeline that generates annotations from chest X-ray images and their corresponding reports. MIMIC-ILS contains 1.1M instruction-answer pairs derived from 192K images and 91K unique segmentation masks, covering seven major lesion types. To empirically demonstrate its utility, we introduce ROSALIA, vision-language model fine-tuned on MIMIC-ILS. ROSALIA can segment diverse lesions and provide textual explanations in response to user instructions. The model achieves high segmentation and textual accuracy in our newly proposed task, highlighting the effectiveness of our pipeline and the value of MIMIC-ILS as foundational resource for pixel-level CXR lesion grounding. Medical imaging is an essential technique in modern medicine, enabling accurate diagnosis and appropriate treatment. Among various imaging modalities, chest X-ray (CXR) is one of the most common examinations due to its high accessibility and rapid acquisition [4]. Radiologists reach diagnosis by integrating visual evidence from CXRs with their clinical knowledge, and describe these findings in text format known as radiology report. key step in this diagnostic process is identifying the precise location and boundary of lesionan abnormal region with pathological changes [6]. This task is labor-intensive and demands substantial clinical expertise and analytical precision. To alleviate physicians workload in localizing pathological regions, there is growing demand for automated lesion segmentation models in CXRs. Recently, visionlanguage models (VLMs) equipped with segmentation modules [24, 25, 40] have emerged as promising solution, as they can interpret diverse user-specific needs expressed through natural language instructions. However, despite the success of such VLMs in general-domain segmentation, their application to CXRs remains limited. Although prior studies [16, 29] have explored CXR lesion segmentation using text prompts, they Equal Contribution *Correspondence to Table 1. Existing CXR datasets with spatial annotations for pathologic lesions. Dataset # Images Spatial Annotation Instruction-Answer Pair # Annotations Type Multi-Lesion Method VinDr-CXR [37] Padchest-GR [8] MS-CXR [3] TBX-11K [32] SIIM-ACR [46] QaTa-COV19 [9] Danilov et al. [7] MIMIC-ILS (Ours) 15K 4.6K 1K 12K 13K 121K 1.4K 192K 9K 7.7K 1.2K 1.2K 2.7K 9.3K 0.6K 91K Bounding Box Bounding Box Bounding Box Bounding Box Segmentation Mask Segmentation Mask Segmentation Mask Segmentation Mask Manual Manual Manual Manual Manual Semi-Automated Manual Fully-Automated are limited to single lesion type (e.g., COVID-19) and moreover require long, detailed expert-level medical descriptions based on tailored CXR review (e.g., Bilateral pulmonary infection, two infected areas, upper right lung and upper left lung.) as input. Such constraints make them impractical not only for physicians who aim to segment diverse lesion types across various anatomical subregions before closely reviewing the image themselves, but especially for non-experts who can hardly interpret CXR images at all. To address these limitations, we propose more userfriendly paradigm, namely instruction-guided lesion segmentation (ILS). In this paradigm, the model is required to process diverse user instructions, ranging from prompts that specify the lesion type and target location, to requests that look for abnormalities globally. If the requested lesion is not present, the model should reliably report its absence. Additionally, the model should be able to provide textual descriptions regarding lesions location or type, even if not explicitly prompted by the user. However, dataset to support such versatile task has been unavailable, as constructing suitable dataset for training and evaluation poses significant challengesmost notably the need for expert-curated mask annotations. Moreover, accurately pairing these masks with precise textual instructions in terms of anatomical locations and specific lesion types remains highly complex task. In this work, we introduce the first fully automated pipeline for constructing large-scale ILS dataset for CXRs. The central challenge is: How can we derive lesion masks and corresponding instruction-answer text pairs from raw images that contain no explicit annotations? To address this, we leverage radiology reports as key source of information for each image. Using paired imagereport data, our two-stage pipeline integrates pre-trained vision models and large language models (LLMs) to extract high-confidence anomalous regions and structured textual information. By exploiting the consistency between these heterogeneous modalities, we generate high-quality lesion masks and diverse instructionanswer pairs. Applying our novel framework to MIMIC-CXR [21, 22]a large, publicly available CXRreport datasetwe constructed MIMIC-ILS, largescale dataset consisting of 1.1M samples derived from 192K images and 91K lesion masks  (Table 1)  . Although several datasets [3, 79, 32, 37, 46] have tried to introduce spatial annotations in the CXR domain, they are unsuitable for direct use in our ILS paradigm  (Table 1)  . Most provide only coarse bounding-box localization or single lesion type masks that are limited in scale due to reliance on expert annotations. Moreover, they also lack explicit links between mask annotations and textual instructions. MIMIC-ILS bridges these gaps by offering large-scale instructionanswer pairs, each paired with an auto-labeled segmentation mask and detailed lesion profile. Despite being constructed entirely without human intervention, expert evaluations report high acceptance rate of over 95% for this dataset. Leveraging MIMIC-ILS, we train ROSALIA (RadiOlogy Segmentation Assistant trained on Lesion-grounded Instruction-Answer dataset), the first VLM designed for ILS in CXRs. Given user instructions, ROSALIA generates segmentation masks and textual descriptions  (Fig. 1)  , handling wide range of tasks, such as specific segmentation (e.g., Segment the pneumonia in the right lung.), generic segmentation (e.g., Segment the opacity.), and absence confirmation (e.g., There is no atelectasis in the left lung base.). This flexibility enables ROSALIA to effectively address diverse user needs, delivering tailored outputs for each request. In summary, our contributions are threefold: We introduce novel automated pipeline that generates lesion masks and corresponding instructions directly from CXRs without any human intervention. Using only imagereport pairs, our method produces large-scale dataset without requiring explicit manual processing. Applying our framework to MIMIC-CXR, we construct MIMIC-ILS, the first dataset for instruction-guided lesion segmentation (ILS) in CXRs. The resulting dataset is further validated by medical experts, confirming its high quality and the reliability of the construction process. To validate the utility of MIMIC-ILS, we introduce ROSALIA, the first VLM designed for ILS in CXRs. Trained on our million-scale dataset, ROSALIA interprets user instructions across diverse lesion types and locations, producing accurate lesion masks and descriptive outputs. As existing general and medical VLMs significantly struggle with this task, we will publicly release our dataset and model to support advances in fine-grained CXR lesion grounding. 2. Related Work 2.1. Lesion Segmentation and Datasets instructionanswer pairs (Sec. 3.2). 3.1. Grounded Lesion Mask Generation Lesion segmentation aims to generate masks corresponding to abnormal regions in medical images. Typically, models are trained on datasets where radiologists have directly annotated lesion masks. For CT and MRI, several studies [18, 19] have utilized public datasets that provide diverse tumor masks [1, 2, 14]. In contrast, such pixel-level annotations are scarce in the CXR domain. While some datasets provide only bounding boxes [8, 37], those that offer segmentation masks usually focus on single lesion type [9, 32, 46]. Consequently, existing models trained on these datasets are limited in their effective segmentation range for CXRs [47]. Our work directly addresses this gap by constructing comprehensive, multi-type lesion segmentation dataset for CXRs. 2.2. Language-Guided Image Segmentation Language-guided image segmentation is the task of segmenting target specified by text. Early approaches to this task focused on aligning image features with text labels to generate corresponding masks [28, 42, 45, 48]. More recently, advancements in VLMs have enabled researchers to extend their reasoning capabilities to segmentation [24, 25, 40]. These models can generate an appropriate mask based on complex instructions that require real-world knowledge, such as Segment the object richest in vitamin in this photo. Similar research has emerged in the medical domain, but current approaches remain limited. They usually rely on simple prompts including class labels (e.g., computerized tomography of tumor) [5, 31], which cannot handle sentence-level instructions. In the CXR domain specifically, recent VLMs have been trained using free-form text that describes the location and number of lesions [16, 29]. These approaches, however, expect users to have already reviewed the CXR image, thus providing expert-level descriptions as input. In contrast, our model allows users to obtain the lesion mask, its presence or absence, and type information even without having to interpret the CXR images first. 3. Automatic Dataset Construction This section outlines our approach to automatically constructing large-scale dataset for training model that generates both lesion segmentation masks and corresponding textual descriptions in response to user instructions. The main challenges in this process are: (1) generating lesion masks directly from raw CXR images without explicit image annotations, (2) aligning appropriate instructionanswer texts with the obtained masks, and (3) ensuring that the entire pipeline operates in fully automated, human-free manner. To address these challenges, our framework first extracts textual and spatial information from imagereport pairs and generates lesion masks followed by verification process (Sec. 3.1). Using the verified lesion masks, we then construct diverse To construct our dataset, we use MIMIC-CXR [21, 22], large collection of CXR images paired with radiology reports. Each report is written by radiologist and provides visual descriptions of the corresponding CXR image. Based on this dataset, we generate grounded lesion masks through four sequential steps as illustrated in Fig. 2: (1) Report structuring and location mapping; (2) Spatial information extraction; (3) Lesion mask generation; (4) Location verification. The details of each step are provided in Appendix A. Report Structuring and Location Mapping. The first step employs LLMs to convert radiology reports into structured form for later steps. Specifically, we instruct an LLM to transform each sentence describing an abnormal finding into six-element tuple consisting of the following categories: entity, sentence index, presence, certainty, location, and predicted lesion type. The location element is then mapped to one or more anatomical labels to ensure compatibility with the segmentation model used in subsequent processes. For example, if the second sentence in given radiology report is The lower lung opacity is pneumonia., its corresponding output is (opacity, 2, positive, definitive, [right lung base, left lung base], pneumonia). Here, the term lower lung in the original report is mapped to right lung base and left lung base. Spatial Information Extraction. The second step extracts spatial information from CXRs using three distinct models: (1) RadEdit [38], diffusion-based image editing model; (2) CXAS [41], an anatomy segmentation model; and (3) pretrained YOLO model for CXR lesion detection [36]. These models are used respectively to generate an anomaly map, anatomy masks, and lesion box masks, which serve as visual cues for lesion mask generation in the subsequent steps. RadEdit takes an input image RHW containing lesion and the text prompt No acute cardiopulmonary process and outputs an edited image ˆx from which the lesion has been removed. We derive xano [0, 1]HW as: xano = ˆx Imax , where Imax is the maximum possible pixel intensity (i.e., 255 for an 8-bit image). xano provides morphological information about hyperintense lesions, which are areas that appear brighter than the normal lung field. From xano, we define anomaly map as: = {(i, j) (xano)i,j τano}, (1) where (i, j) represents pixel coordinate and τano is threshold for anomaly pixels. Figure 2. An overview of grounded lesion mask generation. (Top-left) Textual information is extracted from the radiology report during the report structuring and location mapping. (Bottom-left and Center) Pretrained vision models are also employed to produce spatial information. (Right) Finally, lesion mask is generated by integrating this information. The verification step then confirms the grounded location (l1), identifies the empty location (l3) for negative sample generation, and discards the reported-but-ungrounded location (l2). CXAS produces anatomy masks corresponding to each location element in the previously derived structured report tuples. We denote these masks as coordinate sets {Mi}n i=1, where is the number of anatomical labels mapped in the previous step. Each Mi contains the pixel coordinates for specific anatomy, serving as spatial approximation of the lesion location mentioned in the radiology report. In parallel, the pretrained YOLO model is applied to to detect diverse range of lesions. It outputs bounding boxes that not only specify the locations of potential lesions but also assign confidence score to each detection. From these results, we construct set of lesion box masks, {Bj}m j=1, where denotes the number of detected boxes. Each Bj represents the pixel coordinates enclosed by bounding box, accompanied by confidence score conf Bj [0, 1]. Lesion Mask Generation. With the three visual cues extracted from the previous step, the initial lesion masks can be generated. Here, the anomaly map plays central role, representing composite signal of all hyperintense lesions. We decompose this signal into individual masks and align them with the specific lesions described in the report. During this process, the anatomy masks {Mi}n i=1, lesion box masks {Bj}m j=1, and the right and left lung masks (Lr and Ll) are jointly used to select high-quality mask candidates. The core of this filtering process, outlined in Algorithm 1, selectively retains only appropriate candidates from the initially detected lesion box masks, based on four conditions: (c1) sufficient overlap with {Mi}n i=1; (c2) high confidence score; (c3) high internal signal ratio from (i.e., the ratio of the intersection area between the box mask and to the area of the box mask); and (c4) sufficient size relative to either Lr or Ll. Conditions c1 and c2 ensure that the boxes align with the reported locations and are likely to correspond to true lesions. However, the can contain false negatives (i.e., coordinates that belong to actual lesion areas but are missing from A), which may result in excessively small or even empty masks. To mitigate this issue, conditions c3 and c4 are used to retain only those boxes that contain strong lesion signals and are large enough to allow meaningful segmentation. Once the appropriate lesion box masks are selected, we extract from the connected components (i.e., the individual, contiguous islands in 2D space) that intersect with these selected masks. This component then undergoes post-processing step involving small, noisy mask removal to produce the final, refined lesion mask Mlesion (see Appendix A.5 for further details). Location Verification. In the final step, we explicitly verify whether each lesion mask generated by Algorithm 1 has been successfully grounded to the structured report. To assess the grounding status, we define three types of locations: reported location, grounded location and empty location. The reported location is set of anatomical labels extracted from the previous location mapping with LLMs. Based on this set, the grounded location is defined as subset of the reported Figure 3. Instructionanswer pair generation process using the example report, Bibasilar atelectasis. Cardiomegaly. We utilize the elements extracted from the previous lesion mask generation process (see Fig. 2), indicated by the dashed box. Structured tuples (A&B in the top left) are converted to text instructions and mapped to their corresponding ground-truth masks and textual descriptions. Invalid instructions for lesions which lack corresponding mask are excluded (colored as red), and only valid instructions are retained (colored as green). (ET: entity, PS: presence, CT: certainty, RL: reported location, GL: grounded location, EL: empty location) Algorithm 1: Lesion Mask Generation Input: Anomaly map A, anatomy masks {Mi}n box masks {Bj}m right lung mask Lr, left lung mask Ll j=1 with confidences {conf Bj i=1, lesion }m j=1, Output: Final lesion mask Mlesion 1 Mlesion ; 2 Munion (cid:83)n i=1 Mi; 3 foreach Bj {Bj}m j=1 do Bj Munion Bj Munion τconf ; c2 conf Bj c1 τanatomy; τsignal; c3 Bj Bj (cid:18) Bj Lr Bj Lr if c1 c2 c3 c4 then c4 τsize (cid:19) (cid:18) Bj Ll Bj Ll (cid:19) ; τsize FindIntersectingComponent(Bj, A); if is not empty then Mnew Refine(C); Mlesion Mlesion Mnew; 4 5 6 8 9 10 11 12 13 return Mlesion location that spatially overlaps with generated lesion mask, confirming successful localization of the reported finding. This location is derived from the anatomy masks {Mi}n i=1 that intersect with the selected lesion box masks during the lesion mask generation. Finally, we introduce an empty location, which refers to lung region with no reported lesions and is used to generate negative samples. 3.2. Instruction-Answer Pair Generation With the information extracted from the previous process (i.e., grounded lesion mask generation), we build our dataset for seven major lesion types found in CXRs: cardiomegaly, pneumonia, atelectasis, opacity, consolidation, edema, and effusion. These lesions are not only the most frequently mentioned in radiology reports, but also clinically significant to be common annotation targets in other medical datasets [3, 21, 22, 43]. For each lesion, we construct positive instruction-answer pairs, which include ground-truth lesion mask. Negative pairs using an empty mask are also generated to enable the model to confirm the absence of lesions. An example of this pair generation process is shown in Fig. 3. Please refer to Appendix and for the lesion descriptions and specific dataset generation process. Instruction Types and Limitations. We consider three types of segmentation instructions  (Table 2)  . basic instruction specifies both the segmentation target and its location. The location can be broad region (such as left lung or right lung), one of eight more specific zones (apical, upper, mid, and lower zones for each lung), or combination of these regions. In contrast, global instruction specifies only the segmentation target. lesion inference instruction asks the model to predict the type of lesion represented by an opacity within given location. The generation of these instructions is inherently constrained by the grounded lesion mask generation. For example, global instruction becomes invalid if the generated mask captures only part of the lesion. To address this, our framework dynamically produces only those instructionanswer pairs that are valid given the grounding information available for each image. Table 2. Templates for each question type. Each type includes answer templates for both positive and negative cases, with the negative answers positioned in the last row of each cell. Type Basic Global Lesion Inference Role Template Instruction Segment the [Target] in the [Location]. Answer [SEG] [SEG] There is no [Target] in the [Location]. Instruction Segment the [Target]. Answer [SEG] It is located in the [Location]. [SEG] There is no [Target]. Instruction Segment the opacity in the [Location] and predict its type. Answer [SEG] It is highly suggestive of [Lesion]. [SEG] It possibly reflects [Lesion]. [SEG] There is no opacity in the [Location]. Instruction Generation. The instruction generation process begins by creating basic instruction for each grounded lesion. Next, we determine whether global instruction can be generated. The global instruction is created only when the grounded location and the reported location are identical. Separately, we generate lesion inference instructions by transforming the basic instructions for pneumonia, atelectasis, and edema, replacing these specific lesion types with opacity. This transformation is motivated by the fact that these findings are all specific types of opacity, more fundamental visual concept in medical imaging. Negative samples are generated by (1) selecting lesion types that are not mentioned or explicitly negated in the radiology report; or (2) utilizing empty locations to substitute the original location in the basic instruction of positive sample. Answer Generation. Each answer consists of lesion mask and textual description. The answer lesion masks for positive pairs are determined differently depending on whether they are organ-level or localized abnormalities. For cardiomegaly, we utilize heart mask as its corresponding lesion mask since this condition is defined by the state of specific organ [12]. In contrast, localized abnormalities (e.g., pneumonia or effusion) can appear in variable locations, so for these findings, we use the lesion masks generated in Sec. 3.1. For negative pairs, an empty mask is used. As for the textual description, it is also provided for both positive and negative samples. Specifically, the answer template for lesion inference incorporates certainty level. 4. MIMIC-ILS Dataset Our final dataset, MIMIC-ILS, consists of 1.1M instructionanswer pairs (135K positive and 930K negative samples) derived from 192K MIMIC-CXR images. This final image set is obtained by first filtering out low-quality images (e.g., images with extreme contrast issues), and then excluding any images for which no instruction-answer pairs are generated through our pipeline in Sec. 3. The positive samples are generated from 91K unique lesion masks, where each mask can be associated with multiple instructionanswer samples. The resulting dataset covers seven distinct lesion types, and the overall statistics are presented in Fig. 4. Following the official MIMIC-CXR split, the dataset is divided into 1M training samples, 8.2K validation samples, and 12K test samples. Details on quality control and distribution of MIMIC-ILS are presented in Appendix and E. Figure 4. Distribution of MIMIC-ILS dataset. The y-axis indicates the number of samples, and the x-axis represents the lesion type. (CA: cardiomegaly, PN: pneumonia, AT: atelectasis, OP: opacity, CO: consolidation, ED: edema, EF: effusion) Human Evaluation. To assess the quality of MIMIC-ILS, an expert review was conducted by four radiation oncologists specializing in lesion contouring on medical images. For the test set samples, clinicians classified each case as either acceptable or unacceptable based on mask quality. Positive cases were reviewed by all experts, while negatives were split among them. Any sample judged unacceptable by at least one expert was excluded from the final test set, and the results are summarized in Table 3. Among the 10.7K mask samples initially reviewed, 96.4% were rated as acceptable and finally included in the test set. More details on the expert profiles and quality assessment are provided in Appendix E. Table 3. Acceptance rate and number of evaluated samples for the human evaluation. Each sample corresponds to unique combination of lesion mask, target, and location. Expert Expert Expert Expert Expert Overall Total Positive Negative Rate (%) # Samples Rate (%) # Samples Rate (%) # Samples 96.1 97.2 98.7 97.6 96.4 4,090 4,028 4,041 4,065 10,701 95.6 96.0 99.8 96.9 90. 1,841 1,841 1,841 1,841 1,841 96.5 98.3 97.8 98.2 97.7 2,249 2,187 2,200 2,224 8, 5. Model Training Using MIMIC-ILS, we train our ILS model, ROSALIA. The model adopts the architecture of LISA [24], which demonstrated strong zero-shot language-guided segmentation performance in the general domain. As illustrated in Fig. 5, the architecture integrates VLM backbone with the Segment Anything Model (SAM) [23]. The VLM processes both the image and the input instruction to produce special token, [SEG], along with its textual description. This [SEG] token embedding is then passed to SAM together with the input image for mask prediction. Within SAM, the frozen image encoder extracts embeddings from the image, and the mask decoder integrates these embeddings with the hidden embedding of [SEG] token to generate the final mask. Figure 5. Overview of ROSALIA. The architecture integrates VLM with the SAM. The VLM takes CXR image and segmentation instruction as input, generating both textual description and special [SEG] token. The hidden embedding of this [SEG] token is then passed to SAMs decoder to produce the final mask. The overall loss function consists of two components: (1) language loss and (2) mask loss. It is formulated as: = λtxtLtxt + Lmask, Lmask = λbceLbce + λdiceLdice. Ltxt denotes the autoregressive cross-entropy loss for the answer text, and Lmask represents the segmentation loss computed between the ground-truth mask and the predicted foreground probability map, which combines the binary crossentropy loss Lbce and the DICE [34] loss Ldice. The λtxt, λbce, and λdice are coefficients for each loss term. 6. Experiments 6.1. Implementation Details Training Details. ROSALIA is built on the LISA-7B architecture and is fine-tuned from its original checkpoint [24]. Following LISA, we adopted LLaVA [30] as the VLM backbone and employed the largest version of SAM (SAM-H). LoRA [15] fine-tuning was applied to the VLM with rank of 128 and an alpha of 256, while the mask decoder was fully fine-tuned. The epochs and the initial learning rate were set to 15 and 0.0003, respectively, using the AdamW optimizer [33]. The total batch size was 256, and the ratio of positive to negative samples was maintained at 1:1 in each mini-batch. The loss coefficients λtxt, λbce, and λdice were set to 0.5, 5, and 1, respectively, and the DICE loss was computed only for positive samples. Further model training details are described in the Appendix F. Baseline Models. Since we present the first dataset for ILS in CXRs, no existing models have been directly trained on our proposed task. Nonetheless, we evaluated several models from both the general domain (LISA [24], Text4Seg [25], PixelLM [40]) and the medical domain (BiomedParse [47], RecLMIS [16], IMIS-Net [5]), which can take an image and text as input to produce segmentation output. Evaluation Metrics. We used three metrics to evaluate model performance. For positive samples, we used Intersection-over-Union (IoU)based measures: gIoU and cIoU [24]. gIoU is the average IoU across samples, while cIoU is the ratio of total intersection to total union across the dataset. For negative cases, we used empty-target accuracy (N-Acc.), the proportion of samples correctly predicted to have no masks [44]. 6.2. Main Results Table 4 presents the results of the baselines and our proposed model on the MIMIC-ILS test set. While existing VLM-based segmentation models from both the general and medical domains struggle with the ILS task, ROSALIA achieves notably high performance. In particular, not only do these baselines yield low IoU scores on positive cases, but they also frequently fail on empty-target cases, where the N-Acc. rate is nearly zero in most instances. These results highlight the need for dedicated dataset to effectively address the ILS task in CXRs. Furthermore, the strong results of ROSALIA on the physician-verified test set demonstrate that the training set of MIMIC-ILS serves as high-quality resourceeven without manual expert filtering. Table 4. Segmentation results (%) on the MIMIC-ILS test set. NAcc. denotes the accuracy of correctly predicting empty targets. indicates medical domain baselines. The best and second-best results are marked in bold and underline, respectively. Model gIoU cIoU N-Acc. LISA-7B [24] LISA-13B [24] Text4Seg [25] PixelLM-7B [40] PixelLM-13B [40] BiomedParse [47] RecLMIS [16] IMIS-Net [5] ROSALIA (Ours) 8.3 8.9 6.1 9.2 12.8 23.8 22.4 9.8 71.2 12.8 12.2 10.3 11.8 15.4 18.5 19.5 11.8 75. 0.7 0.0 20.6 0.0 0.0 0.6 0.0 21.6 91.8 Table 5 presents the performance of ROSALIA across different lesion types. The overall gIoU exceeds 0.7, indicating that more than 80% of the regions overlap between the predicted and ground-truth masks when the two are of similar size. Even for the lesion type with the lowest gIoU, the score remains above 0.55, suggesting over 70% regional overlap under similar mask sizes between the ground truth and predictions. Figure 6. Visualized inference results of ROSALIA and baseline models. The first three rows show results for positive cases, while the last row presents results for negative cases with an empty target mask. Additional examples are demonstrated in Appendix G. Table 5. Segmentation performance (%) of ROSALIA for each lesion type. Lesion gIoU cIoU N-Acc. Cardiomegaly Pneumonia Atelectasis Opacity Consolidation Edema Effusion Total 89.0 57.2 60.2 60.5 61.9 64.8 60.3 71.2 89.0 60.4 58.7 64.2 65.6 66.6 59. 75.6 85.8 97.1 91.7 85.0 91.2 92.2 90.4 91.8 We also evaluate the accuracy of text responses across different question types, as shown in Table 6. response is considered correct only when both the template and all variables for each question type (denoted by square brackets in Table 2) exactly match the structured ground-truth information. Despite this strict criterion, ROSALIA achieves high accuracy across most question types (see Appendix for text accuracy of each lesion type). empty-target cases. Additionally, Fig. 7 demonstrates the outputs produced from diverse instructions applied to the same input image. Although multiple lesions coexist in the image, ROSALIA accurately interprets each instruction and generates results tailored to the users specific request. This highlights the models ability to handle diverse lesion types and locations of interest. Figure 7. Examples of outputs from different instructions applied to the same image. Among the multiple lesions present, ROSALIA can selectively segment only the lesion and location of interest. Table 6. Text response accuracy (%) of ROSALIA. 7. Conclusion Type Overall Basic Global Lesion Inf. Positive Negative Total 90.7 95.3 94.4 95.4 96.9 96. 93.7 82.3 88.8 75.1 90.6 84.8 6.3. Qualitative Results Fig. 6 presents qualitative examples from each model for the ILS task. The baseline models largely fail, either producing entirely incorrect masks or segmenting the whole anatomical regions (e.g., the left or right lung). In contrast, ROSALIA accurately segments only the lesion specified in the instruction within the designated region and correctly identifies In this study, we introduce MIMIC-ILS, the first dataset for instruction-guided lesion segmentation in CXRs, along with ROSALIA, VLM developed for this new paradigm. Our automated pipeline enables the construction of this millionscale dataset, and expert evaluations show remarkably high acceptance rate, confirming the quality and reliability of our fully human-free data generation process. Trained on MIMICILS, ROSALIA demonstrates comprehensive ability to generate accurate lesion segmentations and textual responses across diverse user instructions. These findings indicate that MIMIC-ILS and ROSALIA offer strong foundation for advancing research on fine-grained lesion grounding in the CXR domain."
        },
        {
            "title": "References",
            "content": "[1] Michela Antonelli, Annika Reinke, Spyridon Bakas, Keyvan Farahani, Annette Kopp-Schneider, Bennett Landman, Geert Litjens, Bjoern Menze, Olaf Ronneberger, Ronald Summers, et al. The medical segmentation decathlon. Nature communications, 13(1):4128, 2022. 3 [2] Patrick Bilic, Patrick Christ, Hongwei Bran Li, Eugene Vorontsov, Avi Ben-Cohen, Georgios Kaissis, Adi Szeskin, Colin Jacobs, Gabriel Efrain Humpire Mamani, Gabriel Chartrand, et al. The liver tumor segmentation benchmark (lits). Medical image analysis, 84:102680, 2023. 3 [3] Benedikt Boecking, Naoto Usuyama, Shruthi Bannur, Daniel Castro, Anton Schwaighofer, Stephanie Hyland, Maria Wetscherek, Tristan Naumann, Aditya Nori, Javier Alvarez-Valle, et al. Making the most of text semantics to improve biomedical visionlanguage processing. In European conference on computer vision, pages 121. Springer, 2022. 2, 5 [4] Joshua Broder. Imaging the chest: the chest radiograph. Diagnostic imaging for the emergency physician, page 185, 2011. 1 [5] Junlong Cheng, Bin Fu, Jin Ye, Guoan Wang, Tianbin Li, Haoyu Wang, Ruoyu Li, He Yao, Junren Cheng, JingWen Li, et al. Interactive medical image segmentation: benchmark dataset and baseline. In Proceedings of the Computer Vision and Pattern Recognition Conference, pages 2084120851, 2025. 3, 7 [6] Sherri de Coronado, Margaret Haber, Nicholas Sioutos, Mark Tuttle, and Lawrence Wright. Nci thesaurus: using science-based terminology to integrate cancer research results. In MEDINFO 2004, pages 3337. IOS Press, 2004. [7] Viacheslav Danilov, Alex Proutski, Alex Karpovsky, Alexander Kirpich, Diana Litmanovich, Dato Nefaridze, Oleg Talalov, Semyon Semyonov, Vladimir Koniukhovskii, Vladimir Shvartc, et al. Indirect supervision applied to covid19 and pneumonia classification. Informatics in Medicine Unlocked, 28:100835, 2022. 2 [8] Daniel Coelho de Castro, Aurelia Bustos, Shruthi Bannur, Stephanie Hyland, Kenza Bouzid, Maria Teodora Wetscherek, Maria Dolores Sanchez-Valverde, Lara JaquesPerez, Lourdes Perez-Rodrıguez, Kenji Takeda, et al. Padchest-gr: bilingual chest x-ray dataset for grounded radiology report generation. NEJM AI, 2(7):AIdbp2401120, 2025. 2, 3 [9] Aysen Degerli, Serkan Kiranyaz, Muhammad EH Chowdhury, and Moncef Gabbouj. Osegnet: Operational segmentation network for covid-19 detection using chest x-ray images. In 2022 IEEE International Conference on Image Processing (ICIP), pages 23062310. IEEE, 2022. 2, 3 [10] Dina Demner-Fushman, Marc Kohli, Marc Rosenman, Sonya Shooshan, Laritza Rodriguez, Sameer Antani, George Thoma, and Clement McDonald. Preparing collection of radiology examinations for distribution and retrieval. Journal of the American Medical Informatics Association, 23 (2):304310, 2015. 2 database: large-scale dataset of anatomical segmentation masks for chest x-ray images. 1 [12] Nicolas Gaggion, Lucas Mansilla, Candelaria Mosquera, Diego H. Milone, and Enzo Ferrante. Improving anatomical plausibility in medical image segmentation via hybrid graph neural networks: applications to chest x-ray analysis. IEEE Transactions on Medical Imaging, 2022. [13] Nicolas Gaggion, Candelaria Mosquera, Lucas Mansilla, Julia Mariel Saidman, Martina Aineseder, Diego Milone, and Enzo Ferrante. Chexmask: large-scale dataset of anatomical segmentation masks for multi-center chest x-ray images. Scientific Data, 11(1):511, 2024. 1 [14] Nicholas Heller, Sean McSweeney, Matthew Thomas Peterson, Sarah Peterson, Jack Rickman, Bethany Stai, Resha Tejpaul, Makinna Oestreich, Paul Blake, Joel Rosenberg, et al. An international challenge to use artificial intelligence to define the state-of-the-art in kidney and kidney tumor segmentation in ct imaging., 2020. 3 [15] Edward Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean Wang, Lu Wang, Weizhu Chen, et al. Lora: Low-rank adaptation of large language models. ICLR, 1(2):3, 2022. 7 [16] Xiaoshuang Huang, Hongxiang Li, Meng Cao, Long Chen, Chenyu You, and Dong An. Cross-modal conditioned reconstruction for language-guided medical image segmentation. IEEE Transactions on Medical Imaging, 2024. 1, 3, 7 [17] Jeremy Irvin, Pranav Rajpurkar, Michael Ko, Yifan Yu, Silviana Ciurea-Ilcus, Chris Chute, Henrik Marklund, Behzad Haghgoo, Robyn Ball, Katie Shpanskaya, et al. Chexpert: large chest radiograph dataset with uncertainty labels and expert comparison. In Proceedings of the AAAI conference on artificial intelligence, pages 590597, 2019. 2 [18] Yankai Jiang, Zhongzhen Huang, Rongzhao Zhang, Xiaofan Zhang, and Shaoting Zhang. Zept: Zero-shot pan-tumor segmentation via query-disentangling and self-prompting. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 1138611397, 2024. 3 [19] Yankai Jiang, Wenhui Lei, Xiaofan Zhang, and Shaoting Zhang. Unleashing the potential of vision-language pretraining for 3d zero-shot lesion segmentation via maskattribute alignment. arXiv preprint arXiv:2410.15744, 2024. 3 [20] Alistair Johnson, Matt Lungren, Yifan Peng, Zhiyong Lu, Roger Mark, Seth Berkowitz, and Steven Horng. Mimic-cxrjpg-chest radiographs with structured labels. PhysioNet, 101: 215220, 2019. [21] Johnson, Pollard, Mark, Berkowitz, and Horng. Mimic-cxr database (version 2.1. 0). physionet. rrid: Scr 007345, 2024. 2, 3, 5, 1 [22] Alistair EW Johnson, Tom Pollard, Seth Berkowitz, Nathaniel Greenbaum, Matthew Lungren, Chih-ying Deng, Roger Mark, and Steven Horng. Mimic-cxr, deidentified publicly available database of chest radiographs with free-text reports. Scientific data, 6(1):317, 2019. 2, 3, 5, 1 [11] Nicolas Gaggion, Candelaria Mosquera, Martina Aineseder, Lucas Mansilla, Diego Milone, and Enzo Ferrante. Chexmask [23] Alexander Kirillov, Eric Mintun, Nikhila Ravi, Hanzi Mao, Chloe Rolland, Laura Gustafson, Tete Xiao, Spencer Whitehead, Alexander Berg, Wan-Yen Lo, et al. Segment anything. In Proceedings of the IEEE/CVF international conference on computer vision, pages 40154026, 2023. 7 [24] Xin Lai, Zhuotao Tian, Yukang Chen, Yanwei Li, Yuhui Yuan, Shu Liu, and Jiaya Jia. Lisa: Reasoning segmentation via large language model. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 9579 9589, 2024. 1, 3, 6, [25] Mengcheng Lan, Chaofeng Chen, Yue Zhou, Jiaxing Xu, Yiping Ke, Xinjiang Wang, Litong Feng, and Wayne Zhang. Text4seg: Reimagining image segmentation as text generation. arXiv preprint arXiv:2410.09855, 2024. 1, 3, 7 [26] Hyungyung Lee, Geon Choi, Jung-Oh Lee, Hangyul Yoon, Hyuk Gi Hong, and Edward Choi. Cxreasonbench: benchmark for evaluating structured diagnostic reasoning in chest x-rays. arXiv preprint arXiv:2505.18087, 2025. 6 [27] Hyungyung Lee, Geon Choi, Jung-Oh Lee, Hangyul Yoon, Hyuk Gi Hong, and Edward Choi. CXReasonBench: Benchmark for Evaluating Structured Diagnostic Reasoning in Chest X-rays (version 1.0.1). PhysioNet, 2025. RRID:SCR 007345. 6 [28] Boyi Li, Kilian Weinberger, Serge Belongie, Vladlen Koltun, and Rene Ranftl. Language-driven semantic segmentation. arXiv preprint arXiv:2201.03546, 2022. 3 [29] Zihan Li, Yunxiang Li, Qingde Li, Puyang Wang, Dazhou Guo, Le Lu, Dakai Jin, You Zhang, and Qingqi Hong. Lvit: language meets vision transformer in medical image segmentation. IEEE transactions on medical imaging, 43(1):96107, 2023. 1, 3 [30] Haotian Liu, Chunyuan Li, Qingyang Wu, and Yong Jae Lee. Visual instruction tuning. Advances in neural information processing systems, 36:3489234916, 2023. 7 [31] Jie Liu, Yixiao Zhang, Jie-Neng Chen, Junfei Xiao, Yongyi Lu, Bennett Landman, Yixuan Yuan, Alan Yuille, Yucheng Tang, and Zongwei Zhou. Clip-driven universal model for organ segmentation and tumor detection. In Proceedings of the IEEE/CVF international conference on computer vision, pages 2115221164, 2023. [32] Yun Liu, Yu-Huan Wu, Yunfeng Ban, Huifang Wang, and Ming-Ming Cheng. Rethinking computer-aided tuberculosis diagnosis. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 26462655, 2020. 2, 3 [33] Ilya Loshchilov and Frank Hutter. Decoupled weight decay regularization. arXiv preprint arXiv:1711.05101, 2017. 7 [34] Fausto Milletari, Nassir Navab, and Seyed-Ahmad Ahmadi. V-net: Fully convolutional neural networks for volumetric medical image segmentation. In 2016 fourth international conference on 3D vision (3DV), pages 565571. Ieee, 2016. 7 [35] Duc Nguyen, DungNB, Ha Q. Nguyen, Julia Elliott, NguyenThanhNhan, and Phil Culliton. Vinbigdata chest x-ray abnormalities detection. https://kaggle.com/competitions/ vinbigdata - chest - xray - abnormalities - detection, 2020. Kaggle. 1 [36] Dung Nguyen, Minh Khoi Ho, Huy Ta, Thanh Tam Nguyen, Qi Chen, Kumar Rav, Quy Duong Dang, Satwik Ramchandre, Son Lam Phung, Zhibin Liao, Minh-Son To, Johan Verjans, Phi Le Nguyen, and Vu Minh Hieu Phan. Localizing before answering: benchmark for grounded medical visual In Proceedings of the Thirty-Fourth question answering. International Joint Conference on Artificial Intelligence (IJCAI 2025), pages 76707676, 2025. 3 [37] Ha Nguyen, Khanh Lam, Linh Le, Hieu Pham, Dat Tran, Dung Nguyen, Dung Le, Chi Pham, Hang TT Tong, Diep Dinh, et al. Vindr-cxr: An open dataset of chest x-rays with radiologists annotations. Scientific Data, 9(1): 429, 2022. 2, 3, [38] Fernando Perez-Garcıa, Sam Bond-Taylor, Pedro Sanchez, Boris van Breugel, Daniel Castro, Harshita Sharma, Valentina Salvatelli, Maria TA Wetscherek, Hannah Richardson, Matthew Lungren, et al. Radedit: stress-testing biomedical vision models via diffusion image editing. In European Conference on Computer Vision, pages 358376. Springer, 2024. 3, 1 [39] Jeff Rasley, Samyam Rajbhandari, Olatunji Ruwase, and Yuxiong He. Deepspeed: System optimizations enable training deep learning models with over 100 billion parameters. In Proceedings of the 26th ACM SIGKDD international conference on knowledge discovery & data mining, pages 35053506, 2020. 9 [40] Zhongwei Ren, Zhicheng Huang, Yunchao Wei, Yao Zhao, Dongmei Fu, Jiashi Feng, and Xiaojie Jin. Pixellm: Pixel reasoning with large multimodal model. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 2637426383, 2024. 1, 3, 7 [41] Constantin Seibold, Alexander Jaus, Matthias Fink, Moon Kim, Simon Reiß, Ken Herrmann, Jens Kleesiek, and Rainer Stiefelhagen. Accurate fine-grained segmentation of human anatomy in radiographs via volumetric pseudo-labeling. arXiv preprint arXiv:2306.03934, 2023. 3, 1 [42] Gyungin Shin, Weidi Xie, and Samuel Albanie. Reco: Retrieve and co-segment for zero-shot transfer. Advances in Neural Information Processing Systems, 35:3375433767, 2022. 3 [43] Xiaosong Wang, Yifan Peng, Le Lu, Zhiyong Lu, Mohammadhadi Bagheri, and Ronald Summers. Chestx-ray8: Hospital-scale chest x-ray database and benchmarks on weakly-supervised classification and localization of common thorax diseases. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 20972106, 2017. 5, [44] Zhuofan Xia, Dongchen Han, Yizeng Han, Xuran Pan, Shiji Song, and Gao Huang. Gsva: Generalized segmentation via multimodal large language models. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 38583869, 2024. 7 [45] Jiarui Xu, Shalini De Mello, Sifei Liu, Wonmin Byeon, Thomas Breuel, Jan Kautz, and Xiaolong Wang. Groupvit: Semantic segmentation emerges from text supervision. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 1813418144, 2022. 3 [46] Anna Zawacki, Carol Wu, George Shih, Julia Elliott, Mikhail Fomitchev, Mohannad Hussain, ParasLakhani, Phil Culliton, and Shunxing Bao. Siim-acr pneumothorax segmentation. https://kaggle.com/competitions/siim-acrpneumothorax-segmentation, 2019. Kaggle. 2, 3 [47] Theodore Zhao, Yu Gu, Jianwei Yang, Naoto Usuyama, Ho Hin Lee, Tristan Naumann, Jianfeng Gao, Angela Crabtree, Jacob Abel, Christine Moung-Wen, et al. Biomedparse: biomedical foundation model for image parsing of everything everywhere all at once. arXiv preprint arXiv:2405.12971, 2024. 3, 7 [48] Chong Zhou, Chen Change Loy, and Bo Dai. Extract free dense labels from clip. In European conference on computer vision, pages 696712. Springer, 2022."
        },
        {
            "title": "Appendix",
            "content": "<Table of Contents> A. Grounded Lesion Mask Generation . . . . . . . . A.1. Report Pre-Processing . . A.2. Large Language Models and Prompts A.3. Vision Models and Characteristics . . A.4. Thresholds for Lesion Mask Generation . . A.5. Lesion Mask Post-Processing . . . A.6. Empty Location . . . . . . . . . . . . . . . . . . . . . B. Lesion Types C. Instruction-Answer Pair Generation . . C.1. Positive Instruction . C.2. Negative Instruction . . . . . . . . . . . D. Quality Control . D.1. Chest X-rays . D.2. Lung and Heart Masks . D.3. Cardiomegaly . . . . . . . . . . . . . . . . . . . E. MIMIC-ILS Dataset . E.1. Details for Dataset Splits . . . E.2. Quality Assessment . E.3. Details on Expert Evaluation . . E.4. Data Generation Examples . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . F. Model Training Details G. Additional Experimental Results G.1. Lesion-Wise Text Accuracy . . G.2. Additional Qualitative Examples . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 1 1 1 1 1 1 2 2 5 5 5 6 6 6 6 6 6 6 6 6 9 9 9 A. Grounded Lesion Mask Generation A.1. Report Pre-Processing Textual information for the CXR images from MIMIC-CXR was extracted from their corresponding radiology reports. From these raw reports, we extract the findings, impression, and last paragraph sections following the official MIMIC report pre-processing code. We then adhere to hierarchical fallback logic to select single representative text section for each study: the impression section is used if the findings section is missing, and the last paragraph is used if the impression is also absent. Studies that lack all three of these sections are excluded. A.2. Large Language Models and Prompts To extract information from the pre-processed report section, our pipeline employs two distinct large language models (LLMs). The initial report structuring step utilizes MistralSmall-3.1-24B-Instruct-2503. Using the prompt shown in Figure 9, we extract lesion information from the report as six-element tuples. For the subsequent location mapping step, we employ medgemma-27b-text-it, which is specialized in the medical domain. Using the prompt shown in Figure 10, we normalize the lesions location in two-step process. In compliance with the PhysioNet credentialed data use agreement for MIMIC-CXR, both models were run on our local GPU setup. A.3. Vision Models and Characteristics Pretrained HybridGNet. We utilized HybridGNet model, pretrained on the CheXMask dataset [1113], to segment the right lung, left lung, and heart. It demonstrates robust segmentation performance for these three organs, even in challenging CXRs from patients with severe conditions characterized by dense opacities.The resulting masks serve multiple, distinct roles in our pipeline. The heart mask is used directly as the ground-truth lesion mask for cardiomegaly. The right and left lung masks serve two purposes: they are used as the (Lr, Ll) inputs in Algorithm 1, and they are also merged with the heart mask to define the editing region for RadEdit. Details regarding the use of this model were omitted from the main text for brevity. RadEdit. This diffusion-based image editing model takes chest X-ray image and text prompt as input [38]. To transform the input into normal-appearing image, we used the standard prompt on which RadEdit was trained: No acute cardiopulmonary process. Additionally, it requires mask specifying the editing region. For this, we used the merged masks from the pretrained HybridGNet described above. Notably, we used the original MIMIC-CXR dataset [21, 22], which contains DICOM files, rather than the MIMIC-CXRJPG version [20]. This is because RadEdit was trained on the original MIMIC-CXR, and we observed that inputting the histogram-equalized MIMIC-CXR-JPG images significantly degraded the quality of the edited image. CXAS. Designed for anatomy segmentation in CXRs, this model is capable of segmenting 159 anatomical region classes [41]. Specifically, in our research, we input the opacity-removed images (the output of RadEdit) into CXAS to segment the anatomy. This is because CXAS tends to produce lower-quality anatomy masks for patients with significant opacities. Pretrained YOLO. For lesion detection, we employed YOLO model, specifically utilizing the checkpoint from the submitted solution in the VinBigData Chest X-ray Abnormalities Detection competition [35, 37]. Although this model can detect various types of lesions (aortic enlargement, atelectasis, calcification, consolidation, ILD, infiltration, lung opacity, nodule/mass, other lesion, pleural effusion, pleural thickening, pneumothorax, pulmonary fibrosis), we filtered its outputs to retain only those findings considered hyperintense lesions. We therefore excluded aortic enlargement, other lesion, and pneumothorax from the detection categories. A.4. Thresholds for Lesion Mask Generation The thresholds in Equation 1 and Algorithm 1 were carefully calibrated to ensure maximum mask quality. The final values were determined through an iterative process involving multiple quality checks by physician, who identified the settings that maximized the yield of high-quality masks. The finalized thresholds are summarized in Table 7. With the exception of edema, the threshold settings are identical for all other lesion types. We set the threshold values for edema lower than for other lesion types because it tends to spread widely throughout the lungs. Table 7. Threshold values used to generate the lesion masks. General lesions refer to all lesions other than edema."
        },
        {
            "title": "Threshold General Edema",
            "content": "τano τanatomy τconf τsignal τsize 0.10 0.25 0.20 0.20 0.10 0.01 0.25 0.01 0.20 0.10 A.5. Lesion Mask Post-Processing To further enhance the quality of the final lesion masks, additional post-processing steps were applied. Sequential erosion and dilation operations are used to remove small, scattered noise. Although omitted from the main text for brevity, this Figure 8. An example of detailed lesion mask generation where the report mentions Areas of streaky opacity are again seen in the upper lobes. but the mask is grounded only to the right upper lobe. In the top row, the yellow lesion box mask in the left lung is discarded due to insufficient overlap with the green upper lung mask. The remaining box mask in the right lung is then used to filter the anomaly map. Intermediate post-processing steps, including noise removal and mask expansion, are applied to enhance the final mask quality. noise removal step is also performed prior to filtering the anomaly map with the lesion box mask. We also expanded the lesion masks to include adjacent pixels with similar intensity values for more complete segmentation. Furthermore, specifically for effusions at the lung base, we incorporated the lower portion of the lung masks from the pretrained HybridGNet to ensure clean coverage extending to the costophrenic angle. The detailed process is illustrated in Figure 8. A.6. Empty Location We extract an empty location during the location verification step. An empty location is defined as lung region where specific lesion is not present. We designated lung region as an empty location if it did not overlap at all with the anatomy masks corresponding to the reported location. To identify locations that are truly free of any reported lesions, we compute this not only for the seven major lesions but for all lesions mentioned in the report, and utilize this information in subsequent data generation step. B. Lesion Types We construct our dataset around seven major lesion types commonly observed in CXRs, identified through discussions with board-certified physicians: opacity, consolidation, pnuemonia, atelectasis, edema, cardiomegaly, and effusion. These disease categories are widely utilized in many CXR-related studies [10, 17, 43]. First, we include opacity and consolidation, which are high-level, comprehensive terms referring to hyperintense lesions. These broad categories can be mapped to specific lung lesion types, including pneumonia, atelectasis, and edema. We also include two major non-lung disease categories: cardiomegaly, the enlargement of the heart, and (pleural) effusion, the accumulation of fluid in the pleural space. Figure 9. prompt template for report structuring. Figure 10. prompt template for location mapping. Step 2 illustrates the scenario following the lung mapping from Step 1. edema as the target lesion types for this task. This choice reflects clinical reporting practices, where radiologists often describe these specific findings using an inferential process. In contrast, other major lesion types are typically stated directly. For example, report rarely states, There is an opacity in the left lung. It is highly suggestive of effusion.; instead, the finding is stated directly as Left lung effusion. C.2. Negative Instruction Negative instructions are generated in two main scenarios. First, we generate instructions for lesions that are either never mentioned or negated (e.g., no pneumonia) in the report. For these findings, we create negative instruction, which can be either basic type by randomly assigning lung region (e.g., Segment the pneumonia in the left lung.), or global type (e.g., Segment the pneumonia). The second method involves pairing target lesion with randomly selected empty location. For example, if right lung apex and left lung apex are empty locations, we can generate the instruction, Segment the atelectasis in the right lung apex. To prevent an excessive number of negative samples, our logic restricts the generation to maximum of one negative instruction per lesion type for each study. C. Instruction-Answer Pair Generation C.1. Positive Instruction Basic Instruction. The instructions for positive samples are generated directly from the grounded lesion mask generation results. For instance, if definitive finding of pneumonia has grounded location of right lung base and left lung base, we generate basic instruction: Segment the pneumonia in the right lung base and left lung base.. However, if the findings certainty is tentative, indicating the lesions presence is not definitive, we substitute it with the more general term opacity to create the basic instruction. For example, the previous instruction becomes: Segment the opacity in the right lung base and left lung base.. As listed in Table 8, the target location can be specified as single areaeither broad region or specific lung zoneor as combination of these areas. Table 8. List of valid target locations for basic instructions. Locations are categorized into broad regions and specific lung zones."
        },
        {
            "title": "Broad Regions",
            "content": "right lung left lung Lung Zones (Right) Lung Zones (Left) right apical zone right upper zone lung right mid zone lung right lung base left apical zone left upper zone lung left mid zone lung left lung base Global Instruction. global instruction is used to segment all instances of lesion across the entire lung, without specifying location. To create valid global instruction, the generated lesion must cover all lesions cited in the report; this ensures the mask can serve as complete ground truth. Therefore, we only generate global instructions when the grounded location, where masks were actually generated, and the reported location, the complete area mentioned in the report, are identical. If the lesion type is cardiomegaly, we always generate this instruction type. This is because cardiomegaly represents condition of the heart itself, rather than lesion that can appear in variable locations. Lesion Inference Instruction. We generate lesion inference instructions to enable the model to infer the specific lesion type from an opacity at given location. These instructions are generated for findings regardless of their original certainty level, as the certainty is instead reflected in the ground-truth text description. We selected pneumonia, atelectasis, and D. Quality Control D.1. Chest X-rays We exclusively utilized Posteroanterior (PA) and Anteroposterior (AP) view images from the MIMIC-CXR dataset. However, even within these designated views, the dataset contains noisy samples, including mislabeled lateral views, non-chest X-rays, or images with severe anatomical truncation. To ensure data quality, we leveraged metadata from CXReasonBench [26, 27]. This dataset was meticulously constructed from frontal view images within the MIMIC-CXR dataset that had verified high image quality. Specifically, we utilized its pre-extracted information such as the count of extractable CXAS anatomy masks and indicators of full chest visibility to identify and exclude these problematic images beforehand. D.2. Lung and Heart Masks Lung and heart masks are critical component for the construction of MIMIC-ILS. However, both models can produce erroneous results: the pretrained HybridGNet occasionally generates abnormal masks, and CXAS (even when applied to RadEdit-processed images) also generates suboptimal masks. To address this, we cross-referenced the masks from both models and excluded cases with significant discrepancies, interpreting this as failure in either the HybridGNet or CXAS segmentation. Specifically, we determined that large differences in the outermost x-coordinates of the lung masks or the lowermost y-coordinates of the heart masks would cause problems for subsequent grounded lesion mask generation, and thus excluded these studies. D.3. Cardiomegaly To generate reliable negative samples for cardiomegaly, we measured the cardiothoracic ratio (CTR) using the right lung, left lung, and heart masks generated by the pretrained HybridGNet. We then filtered these samples, exclusively including those with CTR of 0.45 or less in our final negative dataset. This 0.45 threshold was calibrated by physician who analyzed the distribution of CXRs across different CTR intervals to establish clinically sound cutoff. E. MIMIC-ILS Dataset E.1. Details for Dataset Splits The data splits and distribution by instruction type for MIMIC-ILS are presented in Tables 9, 10, and 11. Note that the counts for the test set reflect the final numbers after excluding cases that were rejected during the quality assessment process. E.2. Quality Assessment rigorous quality assessment was conducted on the test split by four physicians. All positive samples were reviewed by all four physicians, while the negative samples were divided among them for evaluation. The reviewers were provided with an CXR image, the lesion type, and the mapped anatomical location text generated from our information-grounding process, along with the corresponding ground-truth radiology report. They were then asked to mark each pair as either Acceptable or Not Acceptable on review sheet. The evaluation process was conducted independently for each expert, ensuring that no reviewer could access the others evaluation results. E.3. Details on Expert Evaluation The expert evaluations were conducted by four physicians, all experienced radiation oncologists with extensive training in lesion contouring. Their professional backgrounds are as follows: Experts and are board-certified physicians with 9 and 7 years of clinical experience, respectively, while Experts and are resident doctors, each with 6 years of clinical experience. Also, the lesion-level acceptance rate in human evaluation are shown in Table 12. E.4. Data Generation Examples With our proposed data generation pipeline, we can produce high-quality lesion masks and their corresponding instructionanswer pairs from grounded information. Figure 11 illustrates representative examples across various lesion types and anatomical locations, including both positive and negative cases, along with their corresponding structured information. Table 9. Number of generated instruction-answer pairs per lesion and template type in MIMIC-ILS train split. Lesion # IAs Basic Global Lesion Inference pos neg pos neg pos neg cardiomegaly pneumonia atelectasis opacity consolidation edema effusion 63,153 158,059 166,935 156,807 154,955 182,233 162,998 0 4,542 8,846 9,113 3,428 14,150 10,244 0 145,317 128,943 73,619 144,489 145,251 114,375 39,108 511 3,331 1,532 379 5,390 3,713 24,045 3,147 16,969 274 6,659 3,292 34,666 0 4,542 8,846 0 0 14,150 0 0 0 72,269 0 0 0 Total 1,045,140 50,323 751,994 53, 89,052 27,538 72,269 Table 10. Number of generated instruction-answer pairs per lesion and template type in MIMIC-ILS validation split. Lesion # IAs cardiomegaly pneumonia atelectasis opacity consolidation edema effusion Total 539 1,211 1,316 1,225 1,213 1,469 1,273 8,246 Basic Global Lesion Inference pos 0 28 75 75 30 129 416 416 neg 0 1,130 986 581 1,137 1,124 5,843 pos 332 2 33 13 3 59 22 464 neg 207 23 147 0 43 28 735 pos 0 28 75 0 0 129 0 232 neg 0 0 0 556 0 0 556 Table 11. Number of generated instruction-answer pairs per lesion and template type in MIMIC-ILS test split. Lesion # IAs cardiomegaly pneumonia atelectasis opacity consolidation edema effusion 965 1,767 1,842 1,753 1,756 2,274 1, Total 12,235 Basic Global Lesion Inference pos 0 60 110 174 69 283 156 852 neg 0 1,596 1,466 779 1,612 1,551 1,312 pos 803 8 45 26 10 103 8,316 1,049 neg 162 43 111 5 65 54 356 796 pos 0 60 110 0 0 283 0 453 neg 0 0 0 769 0 0 0 769 Table 12. Acceptance rate (%) by lesion type for each expert in the human evaluation. Lesion Expert Expert Expert Expert Pos Neg Total Pos Neg Total Pos Neg Total Pos Neg Total Cardiomegaly Pneumonia Atelectasis Opacity Consolidation Edema Effusion 97.7 90.0 97.2 92.6 97.2 95.8 89.8 97.1 98.5 98.8 92.3 95.7 95.1 96. 97.7 97.3 98.4 92.4 95.9 95.4 94.1 99.2 97.1 79.9 96.5 100.0 97.9 89.3 100.0 99.7 99.5 96.5 97.2 100.0 97.3 99.2 99.4 94.2 95.1 97.6 99.0 94.3 100.0 98.6 100.0 99.5 100.0 100.0 99.5 100.0 99.5 99.0 92.7 98.3 97.3 97. 100.0 99.4 99.3 96.0 98.5 98.5 98.3 99.4 98.6 99.3 96.0 100.0 89.8 95.4 100.0 98.8 97.3 96.6 99.0 98.2 98.8 99.4 98.7 97.8 96.3 99.2 94.4 97.6 Figure 11. Examples of final generated samples in our MIMIC-ILS dataset. F. Model Training Details G. Additional Experimental Results In our experiments, training the LISA-7Bbased model took approximately two and half days on two NVIDIA H100 GPUs for 15 epochs. Using the DeepSpeed package [39], we trained the model with the DeepSpeed Stage-2 configuration and WarmupDecayLR scheduler, with 100 warmup steps and minimum and maximum learning rate of 0 and 0.0003, respectively. For inference on the test set, which contains 12K examples, segmentation alone takes about 20 minutes, whereas segmentation with text outputs requires approximately 1.5 hours. During training, each input image had 50% chance of being processed with histogram equalization. G.1. Lesion-Wise Text Accuracy The text-response accuracy of ROSALIA for each lesion type is summarized in Table 13. Across most lesion and question types, the model consistently achieves high accuracy, similar to the segmentation performance reported in Table 5. For lesion-inference questions, CXR alone typically cannot provide definitive diagnosis and often requires additional examinations (e.g., blood tests or cultures). As result, radiologists generally provide only differential diagnosis based on visual findings. Given this inherent uncertainty in CXR interpretation, improvements in accuracy for lesion-inference questions are naturally limited. Nevertheless, we evaluated how well the trained model on our dataset can perform on this question type and leave further advancements in this direction to future work. Table 13. Text response accuracy (%) of ROSALIA across different question and lesion types. Lesion Overall Basic Global Lesion Inf. Cardiomegaly Pneumonia Atelectasis Opacity Consolidation Edema Effusion Total 96.0 96.3 92.9 91.5 97.3 93.4 94. 94.4 - 99.2 96.9 92.2 97.6 96.2 96.9 96.8 96.0 72.6 69.2 93.6 92.0 74.5 85.9 88.8 - 36.7 69.1 90.5 - 85.5 - 84.8 G.2. Additional Qualitative Examples We present additional qualitative examples comparing ROSALIA with baseline models in Figure 12. Unlike the baselines, ROSALIA produces accurate segmentation outputs tailored to diverse user instructions. In addition, examples that include both text responses and segmentation outputs are shown in Figure 13. In these cases as well, ROSALIA provides highly factual text responses alongside precise segmentation results. Figure 12. Qualitative comparison of segmentation results between ROSALIA and baseline models. Figure 13. Examples of textual responses generated by ROSALIA. All generated text responses correctly match the ground-truth answers, and both the segmentation and textual outputs in this figure are rated as good examples by medical experts."
        }
    ],
    "affiliations": [
        "AITRICS",
        "KAIST",
        "Samsung Medical Center"
    ]
}