{
    "paper_title": "LaViT: Aligning Latent Visual Thoughts for Multi-modal Reasoning",
    "authors": [
        "Linquan Wu",
        "Tianxiang Jiang",
        "Yifei Dong",
        "Haoyu Yang",
        "Fengji Zhang",
        "Shichaang Meng",
        "Ai Xuan",
        "Linqi Song",
        "Jacky Keung"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Current multimodal latent reasoning often relies on external supervision (e.g., auxiliary images), ignoring intrinsic visual attention dynamics. In this work, we identify a critical Perception Gap in distillation: student models frequently mimic a teacher's textual output while attending to fundamentally divergent visual regions, effectively relying on language priors rather than grounded perception. To bridge this, we propose LaViT, a framework that aligns latent visual thoughts rather than static embeddings. LaViT compels the student to autoregressively reconstruct the teacher's visual semantics and attention trajectories prior to text generation, employing a curriculum sensory gating mechanism to prevent shortcut learning. Extensive experiments show that LaViT significantly enhances visual grounding, achieving up to +16.9% gains on complex reasoning tasks and enabling a compact 3B model to outperform larger open-source variants and proprietary models like GPT-4o."
        },
        {
            "title": "Start",
            "content": "6 2 0 2 5 1 ] . [ 1 9 2 1 0 1 . 1 0 6 2 : r LaViT: Aligning Latent Visual Thoughts for Multi-modal Reasoning Linquan Wu*1, Tianxiang Jiang*2, Yifei Dong3, Haoyu Yang4, Fengji Zhang1, Shichang Meng1, Ai Xuan1, Linqi Song 1, Jacky Keung1 1City University of Hong Kong, 2University of Science and Technology of China, 3Utrecht University, 4University of Electronic Science and Technology of China https://github.com/Svardfox/LaViT"
        },
        {
            "title": "Abstract",
            "content": "Current multimodal latent reasoning often relies on external supervision (e.g., auxiliary images), ignoring intrinsic visual attention dynamics. In this work, we identify critical Perception Gap in distillation: student models frequently mimic teachers textual output while attending to fundamentally divergent visual regions, effectively relying on language priors rather than grounded perception. To bridge this, we propose LaViT, framework that aligns latent visual thoughts rather than static embeddings. LaViT compels the student to autoregressively reconstruct the teachers visual semantics and attention trajectories prior to text generation, employing curriculum sensory gating mechanism to prevent shortcut learning. Extensive experiments show that LaViT significantly enhances visual grounding, achieving up to +16.9% gains on complex reasoning tasks and enabling compact 3B model to outperform larger open-source variants and proprietary models like GPT-4o."
        },
        {
            "title": "Introduction",
            "content": "Multimodal Large Language Models (MLLMs) have advanced rapidly in recent years (Bai et al., 2025a; Comanici et al., 2025; Team, 2025), early multimodal reasoning models primarily thinking about images, captioning visual inputs into text and reasoning mainly in the language space via explicit chains of thought (Huang et al., 2025; Yang et al., 2025a; Shen et al., 2025a). Recent work instead emphasizes thinking with images, more tightly integrating visual evidence into the reasoning process (Hu et al., 2024; Zheng et al., 2025; Su et al., 2025b), leading to improved performance on complex visual reasoning tasks (Fu et al., 2024; Wu and Xie, 2024; Zhang et al., 2024). * Equal contribution. 1 Subsequently, latent reasoning (Hao et al., 2024) has emerged as complementary direction, compressing intermediate reasoning into continuous hidden states rather than explicit CoT. This paradigm has been extended to MLLMs to model abstract visual thoughts within latent tokens (Yang et al., 2025b; Li et al., 2025a). While effective, existing methods largely rely on manually designed visual supervision, such as auxiliary images or annotated regions, leaving intrinsic visual attention dynamics during reasoning unexplored. These limitations motivate the use of knowledge distillation (Hinton et al., 2015) as lens to analyze and transfer visual reasoning behaviors in MLLMs. Distilling high-capacity teacher into compact student enables us to probe not only what knowledge is transferred, but also how visual reasoning is internally realized. However, existing multimodal distillation methods mainly align final textual outputs or distributions (Cai et al., 2025; Shu et al., 2024), implicitly assuming that reproducing answers or static representations suffices to inherit multimodal reasoning ability. To examine this assumption, we conduct empirical analyses, revealing pronounced mismatch between textual alignment and visual reasoning: (I) Correct multimodal reasoning is causally constrained by focused visual attention: when models fail to attend to relevant regions, hallucinated or unreliable responses emerge. (II) Even when student models closely match teacher outputs under standard distillation, their visual attention trajectories can diverge substantially, particularly for tasks requiring fine-grained visual grounding. Together, these findings expose fundamental Perception Gap in multimodal distillation: students often learn what to say without learning where to look, instead relying on language priors rather than grounded visual evidence. Motivated by this insight, we propose LaViT, distillation framework that aligns latent visual Figure 1: Conceptual Illustration of Our Proposed Method LaViT. thoughts rather than static visual embeddings. LaViT trains the student to autoregressively generate continuous latent tokens that reconstruct the teachers internal visual semantics and attention trajectories prior to textual response generation, explicitly transferring both what visual concepts to encode and where to attend during reasoning. To prevent shortcut learning through direct access to visual features, we introduce Curriculum Sensory Gating, which progressively restricts and then relaxes visual input during training. This strategy enforces latent bottleneck early on, compelling reliance on latent visual reasoning while avoiding traininginference mismatch. Extensive experiments demonstrate that LaViT substantially improves both visual grounding and multimodal reasoning. LaViT-3B achieves up to +5.0% gains on fine-grained perception benchmarks MMVP (Tong et al., 2024) and substantial improvement on BLINK (Fu et al., 2024), outperforming strong baselines and rivaling or surpassing 7B models and proprietary GPT-4o."
        },
        {
            "title": "2 Related Work",
            "content": "2.1 Visual Chain-of-Thought Originating from text-only LLMs (Wei et al., 2022), Chain-of-Thought (CoT) has expanded to multimodal contexts (Shao et al., 2024). Following DeepSeek-R1 (Guo et al., 2025), recent works enhance multi-step visual reasoning via RL-style optimization (Huang et al., 2025; Yang et al., 2025a; Shen et al., 2025a; Feng et al., 2025a; Jiang et al., 2025); however, these methods primarily rely on indirect textual proxies rather than intrinsic visual understanding. Conversely, parallel stream shifts to thinking with images by orchestrating tools (Zhang et al., 2025a; Wu et al., 2025; Su et al., 2025a; Wang et al., 2025a; Zhang et al., 2025b), utilizing executable programs (Hu et al., 2024) or iterative region grounding (Zheng et al., 2025). Furthermore, unified architectures now support interleaved generation, exemplified by Chameleons unified tokens (Team, 2024), MVoTs multimodal trajectories (Li et al., 2025b), and other general-purpose frameworks (Tong et al., 2025b; Deng et al., 2025; Gu et al., 2025). 2.2 Latent Reasoning Recent advances have shifted the reasoning paradigm from discrete token sequences to continuous hidden states, effectively enhancing both computational efficiency and flexibility (Hao et al., 2024; Shen et al., 2025b; Wei et al., 2025). Extending this concept to multimodal learning, current MLLMs align specialized latent tokens with visual embeddings derived from auxiliary supervision signals, such as helper images (Yang et al., 2025b) or annotated bounding boxes (Li et al., 2025a). To further improve grounding, CoVT (Qin et al., 2025) integrates fine-grained perceptual priors from models like DINO (Oquab et al., 2023) and SAM (Kirillov et al., 2023), while other approaches explore interleaved patterns to mimic internal visual imagination (Tong et al., 2025a; Wang et al., 2025b). However, these existing methods primarily constrain latent tokens using static encoder features, critically overlooking the dynamic guidance offered by attention maps. 2.3 Knowledge Distillation Knowledge distillation (Hinton et al., 2015), which transfers capabilities from high-capacity teacher to compact student, has been widely adopted in LLMs via logit matching (Sun et al., 2019; Jiao et al., 2020). Extending this paradigm to multimodal models, DistillVLM (Fang et al., 2021) performs transformer distillation by using an MSE loss to match the teacher and students hidden attention distributions and feature maps. In contrast, MAD (Wang et al., 2022) emphasizes aligning visual and textual token features between teacher and student, leveraging token selection to guide 2 the matching. More recent research explores distillation tailored to MLLMs for specific downstream tasks, including visual grounding (Cai et al., 2025; Feng et al., 2025b) and compositional learning (Kim et al., 2025)."
        },
        {
            "title": "3 Empirical Analysis of Perception Gap",
            "content": "We conduct pilot study to quantify the misalignment between textual generation and visual attention, centered on two research questions: (RQ1) Is correct visual reasoning causally linked to focused visual attention? (i.e., Does looking at the right place precondition the right answer?) (RQ2) Does significant alignment gap exist in the visual trajectories between teacher and student models, even when their textual outputs are similar? 3.1 Visual Attention Dictates Reasoning Bounds Definition 3.1 (Visual Focusing Score). Let be the image and Bgt the target bounding box. Given the models aggregated attention trajectory Atraj RHW , which accumulates attention weights across all layers and heads, the visual focusing score Sf ocus is defined as: Sf ocus = (cid:80) Atraj(u, v) (u,v)Bgt (u,v)I Atraj(u, v) (cid:80) (1) where Atraj(u, v) denotes the attention intensity at spatial coordinate (u, v). The denominator represents the total attention mass distributed across the entire image I. larger Sf ocus indicates stronger dependency of the reasoning process on the verified visual evidence, implying that the model is actively looking at the semantically correct region rather than relying on language priors or blind guessing. Building upon the above metric, we analyze the attention trajectories of Qwen2.5-VL-32B (Bai et al., 2025b) on 1,000 randomly sampled instances from the Visual-CoT (Shao et al., 2024). As illustrated in Figure 2, reasoning outcomes are strictly constrained by the intensity of visual attention: Monotonic Performance Gain: We observe that reasoning accuracy improves monotonically as the Sf ocus threshold increases. Statistical analysis confirms that correct samples maintain significantly higher average Sf ocus (15.89%) compared to incorrect ones (11.84%), indicating substantial relative gap of 34%. This validates that higher visual energy is strong predictor of reasoning success. 3 Figure 2: Impact of Visual Attention on Reasoning Accuracy. The monotonic increase in accuracy with higher Visual Focusing Score (Sf ocus) thresholds validates that effective visual grounding is prerequisite for correct reasoning. Visual Absence and Hallucination: Conversely, in samples with negligible Sf ocus (< 1%), we predominantly observe responses that are completely irrelevant to the visual content or contain severe hallucinations. This pattern suggests that without active visual grounding, the model relies on language priors to blindly guess, which proves to be highly unreliable strategy for complex visual tasks. Observation 1: Visual attention is determinative, not merely interpretative. The strict positive correlation confirms that focused visual grounding (Sf ocus) is necessary condition for reasoning success. Models cannot reason correctly without looking at the right evidence, effectively ruling out blind guessing as viable strategy. 3.2 Perception Gap between Teachers and Students Given the critical role of visual attention, we investigate whether standard Supervised FineTuning (SFT) enables student models to inherit the teachers visual thinking process. This analysis is conducted on the same 1,000 samples following the experimental setup in Section 3.1. Analysis of Attention Divergence. We fed identical reasoning prompts to both the Teacher (MT ) and the Student (MS). We categorized the generated tokens into three groups based on their semantic reliance on visual evidence: Functional (e.g., stop words), Object (nouns), and Attribute (adjectives, spatial relations). We then computed the Kullback-Leibler (KL) divergence between their normalized attention maps AT and AS, alongside the Cosine Distance of their hidden states."
        },
        {
            "title": "4 Method",
            "content": "4.1 LaViT-SFT-15K We construct LaViT-SFT-15K, comprising 15K tuples I, Q, A, Atraj, Vsem, to distill the Internal Cognitive States of the teacher (Qwen2.5-VL32B (Bai et al., 2025b)). Unlike tool-based approaches, we directly extract intrinsic reasoning traces. Data quality is enforced via Three-Stage Filtering pipeline: (1) Correctness: retaining samples matching ground truth; (2) Difficulty: removing instances solvable by text-only model; and (3) Alignment: rejecting samples with < 20% aggregated attention mass falling within the target regions delineated by the ground-truth bounding box annotations in Visual-CoT (Shao et al., 2024), thereby excluding non-visually grounded hallucinations. We extract two white-box signals. First, Dynamic Visual Gaze (Atraj) represents the attention trajectory. Given text sequence Ttext, we aggregate cross-attention weights A(l,h) across layers and heads H: i,j Sj = 1 Ttext (cid:88) (cid:88) (cid:88) l=1 h=1 iTtext A(l,h) i,j (2) We further apply Min-Max normalization to obtain the final gaze probability: Atraj(j) = Sj min(S) max(S) min(S) + ϵ (3) Crucially, unlike static visual features extracted from frozen vision encoder, our target Vsem is derived from the Teachers last transformer layer. Due to the self-attention mechanism, these image token representations have effectively interacted with the textual instructions Q. Therefore, Vsem represents contextualized visual thoughtsreflecting not just what is in the image, but how the Teacher interprets the visual content specifically in response to the given query. To distill the most salient visual cues, we subject Atraj to Top-K (k = 8) sparsification, thereby ensuring sparse and noise-free supervision. 4.2 White-box Trajectory Distillation We establish the foundational reasoning capability of our model through Supervised Fine-tuning (SFT) stage defined as Latent Teacher Forcing. Unlike standard SFT that maps inputs directly to text, we train the model to autoregressively generate sequence of continuous latent tokens, Figure 3: The Perception-Reasoning Gap. While the student aligns closely with the teacher in textual representations (stable Cosine Distance), their visual attention trajectories diverge significantly on attribute-heavy tokens (rising KL Divergence). This reveals that textual mimicry does not imply visual grounding. As shown in Figure 3, we observe decoupling between textual alignment and visual attention: Attention Drift on Visual Concepts: The attention divergence exhibits distinct monotonic increase as the semantic reliance on vision deepens. While Functional tokens maintain lower average divergence (µ = 1.11), Attribute tokens, which require precise visual grounding, show the highest misalignment (µ 1.39). This indicates that when describing fine-grained details (e.g., color, texture), the student struggles to focus on the same regions as the teacher. The Blind Guessing Phenomenon: Crucially, while the attention divergence surges, the Cosine Distance of the hidden states remains relatively stable across categories (ranging from 0.52 to 0.55). This implies that the student can mimic the teachers textual representations (learning what to say) without correctly aligning its visual attention (learning where to look), effectively relying on language priors rather than active observation. Observation 2: Textual mimicry does not guarantee visual understanding. SFT trains the student to reproduce the teachers words but fails to transfer the underlying visual trajectory. This Perception Gap suggests that the student model is often guessing based on language context rather than actively observing the image. 4 = {< trace1 >, . . . , < tracek >}, prior to producing the textual response. These latent tokens serve as Visual Information Containers. By leveraging white-box distillation approach, we force to explicitly capture and compress the teachers high-dimensional visual semantics and gaze patterns. Formally, given an input [I, Xq], the model generates the latent and textual sequences to form the complete trajectory = [I, Xq, V, Xans], where acts as the indispensable cognitive bridge supplying visual evidence for the subsequent response Xans. 4.2.1 Curriculum Sensory Gating naive attention mechanism allows response tokens Xans to attend directly to image patches I, enabling the model to bypass (shortcut learning). Conversely, permanent hard mask creates training-inference distribution shift. To resolve this, we propose Curriculum Sensory Gating, which modulates the direct visual perception path via time-dependent scalar γ(t) [ϵ, 1]. We implement gating within the attention bias of the Transformer. Let Qtxt denote queries from response tokens and Kimg denote keys from image patches. The attention scores are computed as: Attn(Qtxt, Kimg) = Softmax (cid:32) QtxtK img (cid:33) + Bgate(t) , s.t. Bgate(t) = ln(γ(t)). (4) To ensure structured internalization process mentioned above, we introduce warm-up period Tw. The gating scalar γ(t) is defined as: γ(t) = (cid:40) ϵ + 1ϵ (cid:104) 1 cos (cid:17)(cid:105) (cid:16) πt Tw 1, , < Tw Tw (5) where Tw denotes the warm-up steps. This schedule defines two distinct operational phases governed by the training progress: Phase 1: Sensory Warm-up (t < Tw). The direct visual path opens gradually, following Initially, γ ϵ (set to the cosine curve. 1e-6 for numerical stability), resulting in large negative bias Bgate 0. This creates strict Latent Bottleneck, mathematically compelling the model to compress necessary visual information into V. The subsequent smooth relaxation prevents optimization shock while establishing strong dependency on latent reasoning. Phase 2: Fully Observable (t Tw). The gate becomes fully open (γ = 1), reducing the bias Bgate to zero. The direct visual path functions as Residual Perception connection, allowing the model to attend to fine-grained pixel details that complement the high-level reasoning encoded in V. This configuration matches the standard inference topology, ensuring zero distribution shift. 4.2.2 Optimization Objectives To ensure the student strictly internalizes the teachers cognition, we employ dual-stream distillation scheme with explicit gradient flow controls. 1. Semantic Reconstruction (Lconcept). We align the students latent hidden states hz with the teachers holistic visual concepts Vsem. Since the teachers representations encapsulate high-quality visual semantics, we treat them as fixed semantic anchors. We employ projection head ϕmlp to map the students latent manifold into the teachers semantic space: Lconcept = 1 1 i=1 (cid:88) (cid:16) CosSim ϕmlp(h(i) ), (i) sem (cid:17) (6) This objective compels the students latent tokens to act as informative containers, actively capturing and compressing the visual information necessary to reconstruct the teachers visual semantics. 2. Trajectory Alignment (Ltraj). We define the reasoning trajectory as the distribution of attention weights over visual patches. We treat the teachers attention map Atraj as the target, following prior observations that distilling teacher attention maps can effectively guide student visual alignment (Fang et al., 2021). We constrain the students attention Astudent (originating from V) to match this target via KL Divergence: Ltraj = 1 B (cid:88) (cid:88) i=1 j=1 (cid:16) DKL traj A(i,j) A(i,j) student (cid:17) . (7) This ensures the latent tokens learn where to look, inheriting the teachers visual search strategy. 3. Response Generation with Dynamic Gradient Transition (Lntp). The Next-Token Prediction loss drives the response generation. Crucially, the gradient flow from this loss is intrinsically modulated by our gating mechanism. By chain rule, the sensitivity of the loss with 5 respect to direct visual features is proportional to the attention weight: 2024) to test robustness against language priors and ensure genuine visual understanding. (cid:13) (cid:13) (cid:13) (cid:13) Lntp (cid:13) (cid:13) (cid:13) (cid:13) Attn(Qtxt, Kimg) γ(t). (8) During Phase 1, gradients are fully channeled through V, establishing the bottleneck. As γ 1, the gradients transition to synergistic flow, optimizing both the latent abstraction and the residual perception paths jointly. Joint Training Dynamics 4.2.3 Unlike complex multi-stage pipelines (Li et al., 2025a; Wang et al., 2025b) that require careful hyperparameter scheduling to avoid collapse, our Curriculum Sensory Gating provides robust structural constraint. This physically enforced bottleneck naturally regulates the learning difficulty, eliminating the need for dynamic loss weighting. We employ streamlined Joint Training paradigm with fixed distillation weight. The total loss is defined as: Ltotal = Lntp + λ (Lconcept + Ltraj), (9) By maintaining constant but moderate alignment pressure, we ensure the latent tokens remain semantically consistent with the teacher in the curriculum, while allowing the NTP loss to primarily drive the generation quality as the sensory gate opens."
        },
        {
            "title": "5 Experiment",
            "content": "5.1 Experiment Setup SFT Settings. In Phase 1, we train the model for 400 steps with the sensory gating scalar γ initialized at 1e-6. In Phase 2, we continue training for an additional 600 steps. We set λ = 0.3 across all experiments. The model is initialized from Qwen2.5VL-3B and finetuned on the LaViT-SFT-15K. We set the number of latent visual tokens to 4. More details are shown in the appendix. Evaluated Benchmarks. We evaluate LaViT on diverse benchmarks. For subtle visual details, we use MMVP (Tong et al., 2024) and the Attribute Recognition subset of (Wu and Xie, 2024), which target CLIP-blind patterns and object attributes. For higher-level cognition, we adopt BLINK (Fu et al., 2024) tasks on Relative Depth, IQ-Test, Relative Reflectance, and Spatial Relation, which require mental manipulation and geometric reasoning. We also include MMStar (Chen et al., 5.2 Main Results Table 1 presents the comparative performance of LaViT against state-of-the-art MLLMs. As shown, our method achieves consistent improvements across all benchmarks, demonstrating the efficacy of latent visual thinking. Cross-Scale Superiority and Efficiency. LaViT significantly enhances its backbone, achieving substantial gains of +15.67% on Relative Reflectance and +16.94% on Relative Depth. Despite its compact 3B scale, it demonstrates remarkable crossscale competitiveness, outperforming the larger Qwen2.5-VL-7B on five out of seven benchmarks. Moreover, LaViT surpasses SOTA 7B models, beating LVR-7B on fine-grained spatial tasks (e.g., Relative Depth: 78.23% vs. 76.61%) and R1OneVision-7B on MMStar. These results confirm that optimizing latent thinking is more parameterefficient strategy than simply scaling up model size. Advantage in Complex Visual Reasoning. LaViT excels in perception-intensive BLINK benchmarks by leveraging continuous latent reasoning to preserve spatial structures, effectively addressing the limitations of standard models in abstract geometric manipulation. Consequently, LaViT-3B achieves 78.23% on Relative Depth and 32.0% on IQ-Test, outperforming proprietary models like GPT-4o (64.52% and 30.0%) and reasoningenhanced baselines. Notably, it even surpasses the SOTA latent reasoning model LVR-7B on Relative Depth (+1.62%) and Relative Reflectance (+3.0%), validating its superior capability in capturing structural visual semantics. Fine-Grained Perception and Robustness. LaViT effectively mitigates CLIP-blindness, achieving 67.33% on MMVP and substantially outperforming DMLR (61.33%) and PAPO (50.0%). By refining visual features to correct encoding errors rather than trading perception for reasoning, LaViT ensures robust visual grounding. This is further evidenced by its 54.07% score on MMStar (vs. 50.2% baseline), confirming that performance gains stem from genuine visual understanding rather than language hallucinations. 5. In-depth Analysis of Attention Dynamics To investigate the underlying mechanisms of LaViT, we conduct deep dive into the attention distribution on the BLINK Relative Depth. We 6 Method Params MMVP V* Attrib. Rel. Depth IQ-Test Rel. Ref. Spatial MMStar Fine-grained Perception Visual Reasoning Robustness GPT-4o (Hurst et al., 2024) - 58.33 Qwen2.5-VL (Bai et al., 2025b) Qwen2.5-VL (Bai et al., 2025b) 7B 32B Naive SFT PAPO (Wang et al., 2025c) DMLR (Liu et al., 2025) LVR_RL (Li et al., 2025a) R1-OneVision (Huang et al., 2025) LVR (Li et al., 2025a) Qwen2.5-VL (Baseline) LaViT 3B 3B 3B 3B 7B 7B 3B 3B 66.7 75.33 65.33 50.0* 61.33 55.3* 67.0* 72.0 62. Proprietary Model 72.5 64.52 Open Source Model 77.39 82.61 71.77 75.81 Open Source Reasoning MLLMs 80.87 22.61* 46.96 69.6* - 84. 70.16 59.68 54.84 64.52 - 76.61 30.0 26.0 30.0 28.0 31.33* 26.0 30.7* - 28.7 38.8 76. 38.8 55.97 34.33 33.6 23.88 42.54 - 42.5 87.4 85.31 81.82 76.92 72.03 77.62 - 89.5 Baseline & Our Model 61.29 78.23 16.94 81. 67.33 5.00 82.61 0.87 24.0 29.85 32.0 8.00 45.52 15.67 81.82 0.70 81.12 63.9 58.9 69. 55.53 52.7 51.2 53.73 52.1* 59.4 50.2* 54.07 3.87 Table 1: Performance comparison on multimodal benchmarks across three categories: Fine-grained Perception , Visual Reasoning , and Multimodal Robustness . The best and second-best results are marked in bold and underlined, respectively. Green values indicate the absolute gains of LaViT over the Qwen2.5-VL-3B baseline. Asterisks (*) denote results reported by papers.(Fu et al., 2024)(Li et al., 2025a)(Liu et al., 2025) Figure 4: Attention entropy distribution. combine quantitative metrics with qualitative visualizations to analyze Concentration and Stability. Quantifying Attention Concentration (Entropy). We employ Information Entropy (H) to measure the sharpness of the models visual focus. Formally, for given image, the attention entropy is defined as: = (cid:88) i=1 pi log(pi) (10) where pi is the normalized attention weight of the i-th patch. As visualized in Figure 4 and detailed in Table 2, the Base 3B model exhibits heavy tail towards high entropy (H = 4.870), suggesting it lacks clear visual target. LaViT significantly shifts this distribution leftward, reducing the mean entropy to 4.686. This improvement not only approximates the Teachers focused state (H = 4.284) but is also visually corroborated by Figure 5, where LaViT exhibits pinpoint focus on critical depth markers compared to the scattered gaze of the Base model. Takeaway 1: Sharpening Visual Focus LaViT effectively transitions the student from diffuse observation mode to focused reasoning mode. By reducing attention entropy (Figure 4), the student learns to ignore irrelevant background noise and concentrate on task-relevant regions. Achieving Superior Stability via Sparsification. While the Teacher model is highly focused, it exTable 2: Statistical analysis of attention distribution on BLINK Relative Depth. Salient Regions refers to patches with attention weights > 1.5 the mean. CV denotes the Coefficient of Variation (σ/µ). Model Salient Regions Count Attention Entropy (H) Mean Std (σ) Range CV Mean Std (σ) CV Qwen2.5-32B Qwen2.5-3B LaViT 39.9 53.8 47. 15.6 9.0 5.6 8-65 36-77 43-69 0.392 0.191 0.102 4.284 4.870 4.686 0.787 0.216 0.204 0.1837 0.0444 0. hibits significant variance in its viewing strategy across samples (Salient Regions CV = 0.392). We observe that LaViT not only inherits the Teachers focus but actually achieves much more stable attention pattern (CV = 0.102), surpassing the teacher in consistency. We attribute this distilled stability directly to our White-box Trajectory Distillation design: Top-K Sparsification: By explicitly supervising the student with only the Top-K (K = 8) strongest attention points from the teacher, we enforce hard constraint that filters out the teachers low-confidence attentional noise or hesitation. Data Filtering: Our preprocessing pipeline excludes samples where attention does not align with ground-truth regions, ensuring only high-quality traces are learned. Consequently, LaViT does not merely mimic the Teachers raw output; it distills the most robust visual cues, resulting in student model that is consistently focused on critical regions without the variance observed in the large teacher model. 7 Figure 5: Visualization of attention distributions across Qwen2.5-VL-3B, 32B (Teacher), and LaViT on two representative samples from the BLINK. indicates the task-relevant critical regions required for correct reasoning. Table 3: Ablation study of LaViT on MMVP and BLINK subsets. w/o Traj. Align: Removes trajectory alignment loss. w/o Sem. Recon: Removes semantic reconstruction loss. w/o Curr. Gate: Removes progressive gating, setting γ(t) = 0 in Phase 1 and γ(t) = 1 in Phase 2. w/o Latent Tokens: Masking Latent Tokens at Inference Single Stage: Trains in one stage where visual tokens are always visible (γ(t) = 1). Method MMVP Rel. Depth IQ-test Rel. Ref Spatial LaViT-3B 67.33 w/o Trajectory Alignment 64.33 65.33 w/o Semantic Reconstruction w/o Curriculum Sensory Gating 59.33 64.33 w/o Latent Tokens 65.67 Single Stage Training 78.23 75 75.81 71.77 77.42 71.77 32 30.67 30.67 27.33 25.33 28 45.52 44.03 42.54 48.51 81.12 38.81 81.82 78.32 76.92 79.72 25.33 79.02 Takeaway 2: The Denoising Effect LaViT acts as semantic filter. Instead of inheriting the teachers instability, LaViT leverages Top-K sparsification to retain only the core attention patterns, reducing the Coefficient of Variation (CV) from 0.392 to 0.102. This results in student that is surprisingly more decisive and stable than its teacher. 5.4 Ablation Study We evaluate LaViT on MMVP and BLINK subsets, comparing it against variants including single-stage training and the removal of curriculum gating. Impact of Alignment Components. As shown in Table 3, removing either Trajectory Alignment or Semantic Reconstruction leads to significant performance drops across all tasks. Furthermore, masking latent tokens during inference precipitates marked decline, verifying the models genuine reliance on the generated visual thoughts. This con8 firms explicit supervision on where to look and what to see is essential for the student to inherit the teachers visual cognitive patterns effectively. Effectiveness of Curriculum Sensory Gating. The performance degradation observed without this module (e.g., MMVP accuracy drops to 59.33%) indicates that simple hard switch of visual visibility is insufficient. Our progressive gating strategy is crucial for preventing shortcut learning and forcing the model to rely on deep latent reasoning. Impact of Training Strategy. The Single Stage Training baseline (γ(t) = 1) consistently underperforms the full model, particularly on Relative Reflectance (38.81% vs. 45.52%). This demonstrates that progressively exposing visual information is key to avoiding sub-optimal convergence and building robust reasoning capabilities."
        },
        {
            "title": "6 Conclusion",
            "content": "In this work, we identify the Perception Gap in multimodal distillation, where student models often mimic textual outputs without inheriting the teachers visual attention patterns. To bridge this, we propose LaViT, framework that aligns latent visual thoughts via White-box Trajectory Distillation and Curriculum Sensory Gating. By utilizing latent tokens as cognitive containers, LaViT compels the student to reconstruct the teachers visual semantics and gaze before response generation. Extensive experiments demonstrate that LaViT-3B significantly outperforms SFT baselines and rivals 7Bscale models on reasoning-intensive benchmarks like BLINK and MMVP."
        },
        {
            "title": "References",
            "content": "Shuai Bai, Yuxuan Cai, Ruizhe Chen, Keqin Chen, Xionghui Chen, Zesen Cheng, Lianghao Deng, Wei Ding, Chang Gao, Chunjiang Ge, Wenbin Ge, Zhifang Guo, Qidong Huang, Jie Huang, Fei Huang, Binyuan Hui, Shutong Jiang, Zhaohai Li, Mingsheng Li, and 45 others. 2025a. Qwen3-vl technical report. Preprint, arXiv:2511.21631. Shuai Bai, Keqin Chen, Xuejing Liu, Jialin Wang, Wenbin Ge, Sibo Song, Kai Dang, Peng Wang, Shijie Wang, Jun Tang, Humen Zhong, Yuanzhi Zhu, Mingkun Yang, Zhaohai Li, Jianqiang Wan, Pengfei Wang, Wei Ding, Zheren Fu, Yiheng Xu, and 8 others. 2025b. Qwen2. 5-vl technical report. arXiv preprint arXiv:2502.13923. Yuxuan Cai, Jiangning Zhang, Haoyang He, Xinwei He, Ao Tong, Zhenye Gan, Chengjie Wang, Zhucun Xue, Yong Liu, and Xiang Bai. 2025. Llava-kd: framework of distilling multimodal large language models. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 239249. Lin Chen, Jinsong Li, Xiaoyi Dong, Pan Zhang, Yuhang Zang, Zehui Chen, Haodong Duan, Jiaqi Wang, Yu Qiao, Dahua Lin, and Feng Zhao. 2024. Are we on the right way for evaluating large vision-language models? Advances in Neural Information Processing Systems, 37:2705627087. Gheorghe Comanici, Eric Bieber, Mike Schaekermann, Ice Pasupat, Noveen Sachdeva, Inderjit Dhillon, Marcel Blistein, Ori Ram, Dan Zhang, Evan Rosen, Luke Marris, Sam Petulla, Colin Gaffney, Asaf Aharoni, Nathan Lintz, Tiago Cardal Pais, Henrik Jacobsson, Idan Szpektor, Nan-Jiang Jiang, and 3416 others. 2025. Gemini 2.5: Pushing the frontier with advanced reasoning, multimodality, long context, and next generation agentic capabilities. arXiv preprint arXiv:2507.06261. Chaorui Deng, Deyao Zhu, Kunchang Li, Chenhui Gou, Feng Li, Zeyu Wang, Shu Zhong, Weihao Yu, Xiaonan Nie, Ziang Song, Guang Shi, and Haoqi Fan. 2025. Emerging properties in unified multimodal pretraining. arXiv preprint arXiv:2505.14683. Zhiyuan Fang, Jianfeng Wang, Xiaowei Hu, Lijuan Wang, Yezhou Yang, and Zicheng Liu. 2021. Compressing visual-linguistic model via knowledge distillation. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 1428 1438. Kaituo Feng, Kaixiong Gong, Bohao Li, Zonghao Guo, Yibing Wang, Tianshuo Peng, Junfei Wu, Xiaoying Zhang, Benyou Wang, and Xiangyu Yue. 2025a. Video-r1: Reinforcing video reasoning in mllms. arXiv preprint arXiv:2503.21776. Qianhan Feng, Wenshuo Li, Tong Lin, and Xinghao Chen. 2025b. Align-kd: Distilling cross-modal alignment knowledge for mobile vision-language large In Proceedings of the Commodel enhancement. puter Vision and Pattern Recognition Conference, pages 41784188. Xingyu Fu, Yushi Hu, Bangzheng Li, Yu Feng, Haoyu Wang, Xudong Lin, Dan Roth, Noah Smith, WeiChiu Ma, and Ranjay Krishna. 2024. Blink: Multimodal large language models can see but not perceive. In European Conference on Computer Vision, pages 148166. Springer. Jiawei Gu, Yunzhuo Hao, Huichen Will Wang, Linjie Li, Michael Qizhe Shieh, Yejin Choi, Ranjay Krishna, and Yu Cheng. 2025. Thinkmorph: Emergent properties in multimodal interleaved chain-of-thought reasoning. arXiv preprint arXiv:2510.27492. Daya Guo, Dejian Yang, Haowei Zhang, Junxiao Song, Peiyi Wang, Qihao Zhu, Runxin Xu, Ruoyu Zhang, Shirong Ma, Xiao Bi, Xiaokang Zhang, Xingkai Yu, Yu Wu, Z. F. Wu, Zhibin Gou, Zhihong Shao, Zhuoshu Li, Ziyi Gao, Aixin Liu, and 175 others. 2025. Deepseek-r1 incentivizes reasoning in llms through reinforcement learning. Nature, 645(8081):633638. Shibo Hao, Sainbayar Sukhbaatar, DiJia Su, Xian Li, Zhiting Hu, Jason Weston, and Yuandong Tian. 2024. Haotraining large language models to reason in continuous latent space. arXiv preprint arXiv:2412.06769. Geoffrey Hinton, Oriol Vinyals, and Jeff Dean. 2015. Distilling the knowledge in neural network. arXiv preprint arXiv:1503.02531. Yushi Hu, Weijia Shi, Xingyu Fu, Dan Roth, Mari Ostendorf, Luke Zettlemoyer, Noah Smith, and Ranjay Krishna. 2024. Visual sketchpad: Sketching as visual chain of thought for multimodal language models. Advances in Neural Information Processing Systems, 37:139348139379. Wenxuan Huang, Bohan Jia, Zijie Zhai, Shaosheng Cao, Zheyu Ye, Fei Zhao, Zhe Xu, Yao Hu, and Shaohui Lin. 2025. Vision-r1: Incentivizing reasoning capability in multimodal large language models. arXiv preprint arXiv:2503.06749. Aaron Hurst, Adam Lerer, Adam P. Goucher, Adam Perelman, Aditya Ramesh, Aidan Clark, AJ Ostrow, Akila Welihinda, Alan Hayes, Alec Radford, Aleksander adry, Alex Baker-Whitcomb, Alex Beutel, Alex Borzunov, Alex Carney, Alex Chow, Alex Kirillov, Alex Nichol, Alex Paino, and 399 others. 2024. Gpt-4o system card. arXiv preprint arXiv:2410.21276. Tianxiang Jiang, Sheng Xia, Yicheng Xu, Linquan Wu, Xiangyu Zeng, Limin Wang, Yu Qiao, and Yi Wang. 2025. Vknowu: Evaluating visual knowledge unarXiv preprint derstanding in multimodal llms. arXiv:2511.20272. Xiaoqi Jiao, Yichun Yin, Lifeng Shang, Xin Jiang, Xiao Chen, Linlin Li, Fang Wang, and Qun Liu. 2020. 9 Tinybert: Distilling bert for natural language understanding. In Findings of the association for computational linguistics: EMNLP 2020, pages 41634174. Jiwan Kim, Kibum Kim, Sangwoo Seo, and Chanyoung Park. 2025. Compodistill: Attention distillation for compositional reasoning in multimodal llms. arXiv preprint arXiv:2510.12184. Alexander Kirillov, Eric Mintun, Nikhila Ravi, Hanzi Mao, Chloe Rolland, Laura Gustafson, Tete Xiao, Spencer Whitehead, Alexander C. Berg, Wan-Yen Lo, Piotr Dollár, and Ross Girshick. 2023. Segment anything. In 2023 IEEE/CVF International Conference on Computer Vision (ICCV), pages 39924003. Bangzheng Li, Ximeng Sun, Jiang Liu, Ze Wang, Jialian Wu, Xiaodong Yu, Hao Chen, Emad Barsoum, Muhao Chen, and Zicheng Liu. 2025a. Latent visual reasoning. arXiv preprint arXiv:2509.24251. Chengzu Li, Wenshan Wu, Huanyu Zhang, Yan Xia, Shaoguang Mao, Li Dong, Ivan Vulic, and Furu Imagine while reasoning in space: Wei. 2025b. Multimodal visualization-of-thought. arXiv preprint arXiv:2501.07542. Chengzhi Liu, Yuzhe Yang, Yue Fan, Qingyue Wei, Sheng Liu, and Xin Eric Wang. 2025. Reasoning within the mind: Dynamic multimodal interleaving in latent space. arXiv preprint arXiv:2512.12623. Maxime Oquab, Timothée Darcet, Théo Moutakanni, Huy Vo, Marc Szafraniec, Vasil Khalidov, Pierre Fernandez, Daniel Haziza, Francisco Massa, Alaaeldin El-Nouby, Mahmoud Assran, Nicolas Ballas, Wojciech Galuba, Russell Howes, Po-Yao Huang, ShangWen Li, Ishan Misra, Michael Rabbat, Vasu Sharma, and 7 others. 2023. Dinov2: Learning robust visual features without supervision. arXiv preprint arXiv:2304.07193. Yiming Qin, Bomin Wei, Jiaxin Ge, Konstantinos Kallidromitis, Stephanie Fu, Trevor Darrell, and Xudong Wang. 2025. Chain-of-visual-thought: Teaching vlms to see and think better with continuous visual tokens. arXiv preprint arXiv:2511.19418. Hao Shao, Shengju Qian, Han Xiao, Guanglu Song, Zhuofan Zong, Letian Wang, Yu Liu, and Hongsheng Li. 2024. Visual cot: Unleashing chain-of-thought reasoning in multi-modal language models. CoRR. Haozhan Shen, Peng Liu, Jingcheng Li, Chunxin Fang, Yibo Ma, Jiajia Liao, Qiaoli Shen, Zilun Zhang, Kangjia Zhao, Qianqian Zhang, and 1 others. 2025a. Vlm-r1: stable and generalizable r1style large vision-language model. arXiv preprint arXiv:2504.07615. Zhenyi Shen, Hanqi Yan, Linhai Zhang, Zhanghao Hu, Yali Du, and Yulan He. 2025b. Codi: Compressing chain-of-thought into continuous space via selfdistillation. arXiv preprint arXiv:2502.21074. Fangxun Shu, Yue Liao, Le Zhuo, Chenning Xu, Lei Zhang, Guanghao Zhang, Haonan Shi, Long Chen, Tao Zhong, Wanggui He, Siming Fu, Haoyuan Li, Bolin Li, Zhelun Yu, Si Liu, Hongsheng Li, and Hao Jiang. 2024. Llava-mod: Making llava tiny arXiv preprint via moe knowledge distillation. arXiv:2408.15881. Zhaochen Su, Linjie Li, Mingyang Song, Yunzhuo Hao, Zhengyuan Yang, Jun Zhang, Guanjie Chen, Jiawei Gu, Juntao Li, Xiaoye Qu, and 1 others. 2025a. Openthinkimg: Learning to think with images via visual tool reinforcement learning. arXiv preprint arXiv:2505.08617. Zhaochen Su, Peng Xia, Hangyu Guo, Zhenhua Liu, Yan Ma, Xiaoye Qu, Jiaqi Liu, Yanshu Li, Kaide Zeng, Zhengyuan Yang, Linjie Li, Yu Cheng, Heng Ji, Junxian He, and Yi R. Fung. 2025b. Thinking with images for multimodal reasoning: Foundations, methods, and future frontiers. arXiv preprint arXiv:2506.23918. Siqi Sun, Yu Cheng, Zhe Gan, and Jingjing Liu. 2019. Patient knowledge distillation for bert model compression. arXiv preprint arXiv:1908.09355. ByteDance Seed Team. 2025. Seed1.5-vl technical report. arXiv preprint arXiv:2505.07062. Chameleon Team. 2024. Chameleon: Mixed-modal arXiv preprint early-fusion foundation models. arXiv:2405.09818. Jintao Tong, Jiaqi Gu, Yujing Lou, Lubin Fan, Yixiong Zou, Yue Wu, Jieping Ye, and Ruixuan Li. 2025a. Sketch-in-latents: Eliciting unified reasoning in mllms. arXiv preprint arXiv:2512.16584. Shengbang Tong, David Fan, Jiachen Li, Yunyang Xiong, Xinlei Chen, Koustuv Sinha, Michael Rabbat, Yann LeCun, Saining Xie, and Zhuang Liu. 2025b. Metamorph: Multimodal understanding and generation via instruction tuning. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 1700117012. Shengbang Tong, Zhuang Liu, Yuexiang Zhai, Yi Ma, Yann LeCun, and Saining Xie. 2024. Eyes wide shut? exploring the visual shortcomings of multimodal llms. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 95689578. Haozhe Wang, Alex Su, Weiming Ren, Fangzhen Lin, and Wenhu Chen. 2025a. Pixel reasoner: Incentivizing pixel-space reasoning with curiosityarXiv preprint driven reinforcement arXiv:2505.15966. learning. Qixun Wang, Yang Shi, Yifei Wang, Yuanxing Zhang, Pengfei Wan, Kun Gai, Xianghua Ying, and Yisen Wang. 2025b. Monet: Reasoning in latent visual space beyond images and language. arXiv preprint arXiv:2511.21395. 10 Liu, Tianke Zhang, Haonan Fan, Kaibing Chen, Jiankang Chen, Haojie Ding, Kaiyu Tang, Zhang Zhang, Liang Wang, Fan Yang, Tingting Gao, and Guorui Zhou. 2025b. Thyme: Think beyond images. arXiv preprint arXiv:2508.11630. Ziwei Zheng, Michael Yang, Jack Hong, Chenxiao Zhao, Guohai Xu, Le Yang, Chao Shen, and Xing Yu. 2025. Deepeyes: Incentivizing\" thinking with images\" via reinforcement learning. arXiv preprint arXiv:2505.14362. Zhecan Wang, Noel Codella, Yen-Chun Chen, Luowei Zhou, Xiyang Dai, Bin Xiao, Jianwei Yang, Haoxuan You, Kai-Wei Chang, Shih fu Chang, and Lu Yuan. 2022. Multimodal adaptive distillation for leveraging unimodal encoders for vision-language tasks. arXiv preprint arXiv:2204.10496. Zhenhailong Wang, Xuehang Guo, Sofia Stoica, Haiyang Xu, Hongru Wang, Hyeonjeong Ha, Xiusi Chen, Yangyi Chen, Ming Yan, Fei Huang, and Perception-aware policy optiHeng Ji. 2025c. mization for multimodal reasoning. arXiv preprint arXiv:2507.06448. Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Brian Ichter, Fei Xia, Ed Chi, Quoc Le, and Denny Zhou. 2022. Chain-of-thought prompting elicits reasoning in large language models. Advances in neural information processing systems, 35:24824 24837. Xilin Wei, Xiaoran Liu, Yuhang Zang, Xiaoyi Dong, Yuhang Cao, Jiaqi Wang, Xipeng Qiu, and Dahua Lin. 2025. Sim-cot: Supervised implicit chain-ofthought. arXiv preprint arXiv:2509.20317. Mingyuan Wu, Jingcheng Yang, Jize Jiang, Meitang Li, Kaizhuo Yan, Hanchao Yu, Minjia Zhang, Chengxiang Zhai, and Klara Nahrstedt. 2025. Vtool-r1: Vlms learn to think with images via reinforcement learning on multimodal tool use. arXiv preprint arXiv:2505.19255. Penghao Wu and Saining Xie. 2024. V?: Guided visual search as core mechanism in multimodal llms. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 13084 13094. Yi Yang, Xiaoxuan He, Hongkun Pan, Xiyan Jiang, Yan Deng, Xingtao Yang, Haoyu Lu, Dacheng Yin, Fengyun Rao, Minfeng Zhu, Bo Zhang, and Wei Chen. 2025a. R1-onevision: Advancing generalized multimodal reasoning through cross-modal formalization. arXiv preprint arXiv:2503.10615. Zeyuan Yang, Xueyang Yu, Delin Chen, Maohao Shen, and Chuang Gan. 2025b. Machine mental imagery: Empower multimodal reasoning with latent visual tokens. arXiv preprint arXiv:2506.17218. Fengji Zhang, Linquan Wu, Huiyu Bai, Guancheng Lin, Xiao Li, Xiao Yu, Yue Wang, Bei Chen, and Jacky Keung. 2024. Humaneval-v: Evaluating visual understanding and reasoning abilities of large multimodal models through coding tasks. Xintong Zhang, Zhi Gao, Bofei Zhang, Pengxiang Li, Xiaowen Zhang, Yang Liu, Tao Yuan, Yuwei Wu, Yunde Jia, Song-Chun Zhu, and Qing Li. 2025a. Chain-of-focus: Adaptive visual search and zooming for multimodal reasoning via rl. arXiv preprint arXiv:2505.15436. Yi-Fan Zhang, Xingyu Lu, Shukang Yin, Chaoyou Fu, Wei Chen, Xiao Hu, Bin Wen, Kaiyu Jiang, Changyi"
        },
        {
            "title": "A More Implementation Details",
            "content": "A.1 Baselines To comprehensively evaluate the effectiveness of LaViT, we benchmark it against diverse spectrum of state-of-the-art MLLMs. We categorize these baselines into three distinct paradigms to isolate the contributions of our latent reasoning mechanism from model scale and data exposure. General-Purpose Foundation Models. We first establish performance lower bound using our backbone model, Qwen2.5-VL-3B (Bai et al., 2025b), to quantify the specific gains attributed to our architecture. Crucially, to prove that our improvements stem from the proposed latent thinking mechanism rather than merely domain-specific data exposure, we construct controlled baseline named Naive-SFT. This model is fine-tuned on the identical LaViT-15k dataset but utilizes standard text-only supervision, serving as rigorous control variable. Furthermore, we include Qwen2.5VL-7B to assess cross-scale competitiveness and employ GPT-4o (Hurst et al., 2024) as proprietary upper-bound reference to gauge how close our compact 3B model is to industrial state-of-theart performance. Explicit Reasoning Frameworks (Thinkingabout-Images). This category includes methods that enhance reasoning via explicit textual Chainsof-Thought (CoT) or Reinforcement Learning. We compare against R1-OneVision (Yang et al., 2025a), which explicitly generates think-beforeanswer trajectory in the language space. Additionally, we include PAPO (Wang et al., 2025c), an RL-based approach that optimizes image-grounded descriptions through verifiable rewards. For fair comparison, we reproduce PAPO on the same 3B backbone to evaluate the efficiency of latent versus explicit alignment strategies. Latent Visual Reasoning Competitors. Finally, we compare LaViT against direct competitors that also operate within the latent space. We benchmark against LVR (Li et al., 2025a) (and its RL variant), which operates on larger 7B backbone. This serves as cross-scale benchmark to demonstrate LaViTs parameter efficiency. We also compare with DMLR (Liu et al., 2025), framework utilizing test-time latent optimization. To ensure fair comparison regarding computational cost and inference latency, we re-implement DMLR on the Qwen2.5-VL-3B backbone with the number of latent optimization steps restricted to 4, matching the computational budget of our method. A.2 Hyperparameters The Table A1 details the hyperparameters used for the 1000-step training run."
        },
        {
            "title": "B Analysis of numbers of K",
            "content": "To investigate the optimal capacity of the latent bottleneck, we conducted an ablation study on the number of latent visual tokens {4, 6, 8} and monitored performance variations across different training steps. As detailed in Table A2, we observe that = 4 yields the superior balance between visual grounding and reasoning capabilities.Specifically, the model with = 4 achieves peak performance at 1,000 steps, recording the highest scores on MMVP (67.33) and IQTest (32.0), while matching the best Relative Reflectance performance (45.52). Interestingly, increasing the number of latent tokens to = 6 or = 8 does not translate to performance gains; instead, it leads to slight degradation in reasoning tasks (e.g., IQ-Test and Relative Reflectance). This suggests that compact set of 4 latent tokens is sufficient to encapsulate the necessary high-level visual semantics guided by the teachers attention, whereas larger may introduce redundancy or noise into the reasoning process. Furthermore, regarding training dynamics, we observe that performance generally peaks around 1,000 steps before stabilizing or slightly declining, indicating that the model reaches optimal alignment at this stage. Consequently, we adopt = 4 and the 1,000-step checkpoint for all main experiments reported in this paper."
        },
        {
            "title": "C Training Data Construction",
            "content": "C.1 Data Enrichment To support the proposed distillation framework, the training dataset was enriched with pre-computed visual features serving as teacher supervision signals: 1. V-top Tensors: High-dimensional feature vectors (5120-dim) extracted from the final layer of the base models visual encoder, representing holistic semantic concepts. 2. Attention Maps: Compressed attention weights aggregated across heads and layers, 12 Table A1: Hyperparameters used for the 1000-step training run. Category Parameter Value Description Optimization Training Scale Model Config Learning Rate LR Scheduler Warmup Ratio Optimizer Weight Decay Total Steps Batch Size Grad. Accum. Num Latent Tokens V-top Dim Freeze Vision Freeze LLM 5e-6 linear 0.03 AdamW 0.0 1000 16 4 5120 True False Initial learning rate Linear decay with warmup Default transformer warmup β1 = 0.9, β2 = 0.999 No weight decay applied Fixed step training Per-device training batch size No gradient accumulation Number of latent tokens Dimension of reconstruction target ViT encoder weights are locked LLM backbone is fine-tuned Data Config Max Pixels Min Pixels 1,003,520 200,704 12802828 equivalent 2562828 equivalent Table A2: Ablation study on the number of latent tokens (K) and training steps. We report the performance on MMVP and BLINK subsets (IQ-Test, Relative Reflectance, and Spatial Relation). The selected configuration (K = 4, 1000 steps) is highlighted in bold. Method Steps MMVP IQ-Test Rel. Ref. Spatial Qwen2.5-VL-3B - - Naive SFT - - 62.33 65.33 LaViT LaViT LaViT 6 8 58.33 600 800 63.33 1000 67.33 1200 67.00 1400 67.67 62.00 600 800 58.67 1000 61.33 1200 63.00 1400 65.67 63.33 600 66.00 800 1000 65.67 1200 66.00 1400 66.67 24.00 28. 26.00 32.00 32.00 28.67 26.67 26.67 28.67 27.33 26.00 28.67 22.67 28.00 28.00 28.67 29.33 29.85 34.33 45.52 44.78 45.52 42.54 43.28 42.54 43.28 43.28 42.54 44. 30.60 42.54 38.81 40.30 40.30 81.12 81.82 83.92 77.62 81.82 76.92 78.32 80.42 81.82 77.62 76.92 75.52 84.62 77.62 79.02 76.92 76.22 used to generate the trajectory supervision signal for explicit visual grounding. C.2 Data Processing and Alignment To ensure precise synchronization between the students latent states and the teachers signals, we implemented the following processing strategies: Adaptive Scaling. Given the dynamic resolution characteristics of Qwen2.5-VL, feature maps often vary in spatial dimensions. We record the critical step of applying Bilinear Interpolation to align the spatial resolution of the pre-computed v_top feature maps with the attention_maps. This step is essential for maintaining pixel-level correspondence across inputs with varying aspect ratios. Latent Supervision Strategy. For the optimization of latent visual thoughts, we explicitly designate the hidden state of the last latent token (e.g., <v-trace4>) as the anchor for supervision. By computing the loss only at the end of the latent sequence, we force the visual information to flow fully through the bottleneck, compelling the preceding latent tokens to compress and structure the visual data effectively. C.3 System Prompt for LVR Alignment During the Supervised Fine-Tuning (SFT) stage, we utilized specialized system prompt to prime the model for latent visual reasoning. The prompt is presented below: System Prompt You are an expert multimodal large language model. You process visual information through specialized latent tokens to ensure precise alignment between visual perception and textual reasoning. C.4 LaViT-15k Dataset Statistics We constructed LaViT-15k, high-quality multimodal dataset specifically designed to train the latent visual thought capabilities of the model. The dataset comprises 14,567 samples, aggregating diverse visual scenarios from 10 mainstream visionlanguage benchmarks. Data Distribution. The dataset ensures diversity by incorporating samples from captioning, VQA, 13 and document understanding tasks. As shown in Table A3, Flickr30k (32.13%) and GQA (20.05%) constitute the majority of the data, providing strong foundation for general visual grounding, while datasets like DocVQA and TextCap enhance the models fine-grained perception capabilities. Table A3: Distribution of data sources in LaViT-15k. Source Dataset Samples Percentage Flickr30k GQA DocVQA TextCap Visual7W (V7W) OpenImages TextVQA InfographicsVQA CUB (Birds) VSR 4,681 2,921 1,647 1,383 1,110 1,016 839 651 174 145 32.13% 20.05% 11.31% 9.50% 7.62% 6.97% 5.76% 4.47% 1.19% 1.00% Total 14,567 100.00% Statistical Properties. Table A4 summarizes the key statistical properties of the text and visual components. The dataset features high-resolution inputs (avg. 957 882) and rich visual semantics, represented by an average of 697.73 visual tokens per image. The high-dimensional visual features (D = 5120) serve as the supervision target for the latent thought process. Table A4: Statistical properties of textual and visual components in LaViT-15k. Statistic Value Textual Statistics Avg. Question Length Max Question Length Avg. Answer Length Visual Statistics Avg. Image Resolution Avg. Visual Tokens Feature Dimension 13.58 words 34 words 4.92 words 957 882 pixels 697.73 5,"
        }
    ],
    "affiliations": [
        "City University of Hong Kong",
        "University of Electronic Science and Technology of China",
        "University of Science and Technology of China",
        "Utrecht University"
    ]
}