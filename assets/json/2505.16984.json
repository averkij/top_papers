{
    "paper_title": "UFT: Unifying Supervised and Reinforcement Fine-Tuning",
    "authors": [
        "Mingyang Liu",
        "Gabriele Farina",
        "Asuman Ozdaglar"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Post-training has demonstrated its importance in enhancing the reasoning capabilities of large language models (LLMs). The primary post-training methods can be categorized into supervised fine-tuning (SFT) and reinforcement fine-tuning (RFT). SFT is efficient and well-suited for small language models, but it may lead to overfitting and limit the reasoning abilities of larger models. In contrast, RFT generally yields better generalization but depends heavily on the strength of the base model. To address the limitations of SFT and RFT, we propose Unified Fine-Tuning (UFT), a novel post-training paradigm that unifies SFT and RFT into a single, integrated process. UFT enables the model to effectively explore solutions while incorporating informative supervision signals, bridging the gap between memorizing and thinking underlying existing methods. Notably, UFT outperforms both SFT and RFT in general, regardless of model sizes. Furthermore, we theoretically prove that UFT breaks RFT's inherent exponential sample complexity bottleneck, showing for the first time that unified training can exponentially accelerate convergence on long-horizon reasoning tasks."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 2 2 ] . [ 1 4 8 9 6 1 . 5 0 5 2 : r UFT: Unifying Supervised and Reinforcement Fine-Tuning Mingyang Liu 1, Gabriele Farina1, Asuman Ozdaglar 1 1 LIDS, EECS, Massachusetts Institute of Technology 1 {liumy19,gfarina,asuman}@mit.edu"
        },
        {
            "title": "Abstract",
            "content": "Post-training has demonstrated its importance in enhancing the reasoning capabilities of large language models (LLMs). The primary post-training methods can be categorized into supervised fine-tuning (SFT) and reinforcement fine-tuning (RFT). SFT is efficient and wellsuited for small language models, but it may lead to overfitting and limit the reasoning abilities of larger models. In contrast, RFT generally yields better generalization but depends heavily on the strength of the base model. To address the limitations of SFT and RFT, we propose Unified Fine-Tuning (UFT), novel post-training paradigm that unifies SFT and RFT into single, integrated process. UFT enables the model to effectively explore solutions while incorporating informative supervision signals, bridging the gap between memorizing and thinking underlying existing methods. Notably, UFT outperforms both SFT and RFT in general, regardless of model sizes. Furthermore, we theoretically prove that UFT breaks RFTs inherent exponential sample complexity bottleneck, showing for the first time that unified training can exponentially accelerate convergence on long-horizon reasoning tasks. 学而不思则罔思而不学则殆 \"Learning without thinking leads to confusion; thinking without learning is perilous.\" 孔子(Confucius)"
        },
        {
            "title": "1 Introduction",
            "content": "When humans learn new subject, we typically practice with problem sets (thinking) and try to understand the solutions when we encounter difficulties (memorizing). There are also counterparts in fine-tuning LLMs, which is Supervised Fine-Tuning (SFT). Memorizing the collected reasoning trace (solution) by maximizing the log-likelihood of it. Reinforcement Fine-Tuning (RFT). Exploring the reasoning space of LLM and improving the performance according to the signal from verifier of the final answer (thinking). However, unlike humans, learning and thinking are disentangled during the training of language models. Specifically, prior work (DeepSeek-AI et al., 2025; Zhou et al., 2023; Muennighoff et al., 2025; Liu et al., 2025; Zeng et al., 2025) typically applies either SFT or RFT throughout the fine-tuning phase, or applies RFT only after SFT completes (cf. Figure 1). The choice of the proper fine-tuning algorithm depends on the LLMs capacity and the tasks complexity. Specifically, when the LLM is weak, SFT typically works 1Source code available at https://github.com/liumy2010/UFT. 1 Figure 1: (top left, top right, middle, bottom). The illustration of SFT, RFT, SFT-RFT, and UFT, respectively. SFT-RFT refers to applying RFT after an initial SFT stage (DeepSeek-AI et al., 2025; Zeng et al., 2025). (Top, center). shows the annotation usage of different algorithms over training. Curves are slightly shifted for better visibility. better since the LLM cannot explore the correct answer during reinforcement learning (Pan et al., 2025), due to the sparse reward caused by the verifier-based reward model. On the other hand, when the LLM is strong, RFT generalizes better (Xie et al., 2025; Chu et al., 2025). To get the best of both worlds, we propose Unified Fine-Tuning (UFT), which unifies SFT and RFT and enriches the reinforcement learning signal with supervised feedback, enabling the model to acquire new knowledge during fine-tuning more efficiently. In Figure 1, SFT-RFT refers to the common practice of initiating reinforcement learning from supervised fine-tuned model, as widely adopted in the literature (DeepSeek-AI et al., 2025; Zeng et al., 2025). As shown in Figure 1 (top left), SFT uses full annotations (solutions) throughout training, whereas RFT does not use any annotations at all (Figure 1, top right). Similarly, SFT-RFT begins with SFT using full annotations, but once the RFT phase starts, it discards all annotations and relies entirely on exploration. In contrast, our method, UFT, offers smooth transition Figure 2: Presentation for different algorithms accuracy when trained on Countdown (Wikipedia contributors, 2025), MATH(3,4,5) (level 3-5 only) (Hendrycks et al., 2021; Zeng et al., 2025), and the Knights and Knaves logic puzzle (Logic) (Xie et al., 2025). Accuracy is averaged over Qwen2.5 models of sizes 0.5B, 1.5B, and 3B (Qwen et al., 2025). Base refers to the model without fine-tuning, and R3 is the curriculum reinforcement learning baseline (Xi et al., 2024). The figure shows that UFT outperforms both SFT and RFT, while the relative performance of SFT and RFT varies depending on task complexity. from SFT to RFT, preserving the annotation signal early on and gradually reducing it as the model becomes capable of self-guided reasoning. The most relevant work to UFT is Learning Reasoning through Reverse Curriculum Reinforcement Learning (R3) (Xi et al., 2024), which proposes curriculum learning method that concatenates the problem with slice of the solution (hint, cf. Figure 4 left). While R3 treats hints primarily as exploration aids, UFT further integrates them as part of the supervision signal. This unification enables reinforcement learning not just to search, but to learn from existing solutions, effectively raising the performance ceiling imposed by the models pretraining capacity (cf. Figure 2). detailed comparison with related work is postponed to Appendix A. Figure 2 shows the accuracy of different algorithms over time, while the training set is Countdown (Wikipedia contributors, 2025; Pan et al., 2025), MATH(3,4,5) (levels 35 only) (Hendrycks et al., 2021; Zeng et al., 2025), and the Knights and Knaves logic puzzle (Logic) (Xie et al., 2025). Base refers to the model before fine-tuning, and R3 represents the curriculum reinforcement learning baseline (Xi et al., 2024). As shown in the figure, UFT generally outperforms all other algorithms. Furthermore, we provide the evaluation on various benchmarks, and the results are shown in Table 2. Moreover, we theoretically prove that RFT (DeepSeek-AI et al., 2025; Zeng et al., 2025; Liu et al., 2025) suffers from an inherent sample complexity bottleneck, which is exponential in the length of the reasoning. In contrast, the unified training paradigm in UFT can improve the sample complexity to polynomial dependence on the reasoning length, which is an exponential improvement over RFT."
        },
        {
            "title": "1.1 Contribution",
            "content": "We state the contribution of this paper in the following. 1. Integration of Supervision and Reward Signal. UFT provides general framework that integrates the supervision from SFT and reward from RFT into single training paradigm. UFT blends reward optimization with log-likelihood maximization on hints (partial solution), and smoothly transitions from fully supervised to fully reinforcement learning. Such integration allows models to explore 3 and learn simultaneously, addressing the trade-off between memorization (SFT) and generalization (RFT) in principled way. 2. Theoretical Justification. We provide theoretical analysis of UFT, proving it achieves polynomial sample complexity dependence on reasoning length, compared to the exponential complexity required by standard RFT. This result formally establishes the efficiency gains from unifying learning (cf. Section 4). 3. Empirical Validation Across Model Scales and Tasks. We evaluate the algorithms by training Qwen2.5-0.5/1.5/3B (Qwen et al., 2025) and Llama3.2-1/3B (Grattafiori et al., 2024) on Countdown (Wikipedia contributors, 2025; Pan et al., 2025), MATH (Hendrycks et al., 2021), and the Knights and Knaves logic puzzle (Logic) (Xie et al., 2025). UFT consistently outperforms previous methods, showing robustness across domains and models (cf. Section 5)."
        },
        {
            "title": "2 Preliminaries",
            "content": "Notation. For any integer > 0, let [n] := {1, 2, , n} and := (cid:8)x [0, 1]n : 1-dimenional probability simplex. For any two distribution x, n, let KL (xy) := denote the KL-divergence between and y. For any discrete set S, let be its cardinality. i=1 xi = 1(cid:9) be the i=1 xi log xi yi 3, 5, 7, 13 ?= 24 5 13 = 65 3 + 5 = 7 3 = 4 7 + 65 = 72 8 7 = 1 8 + 13 = 21 4 + 65 = 69 65 4 = 61 72 3 = 24 72 3 = 13 + 1 = 14 13 1 = 13 21 + 7 = 28 21 7 = 3 Figure 3: An illustration of the Countdown game, where the goal is to obtain 24 by applying basic arithmetic operations (+, , , ) to the numbers (3, 5, 7, 13). The green path represents the correct solution. Search Tree. The problem-solving process can be represented as search tree, as illustrated in Figure 3. Except for the leaf nodes, each node (also referred to as statewe use the terms node and state interchangeably) in the search tree has children, where is the branching factor. Each child represents different next token (or next sentence) to be generated, so path from the root to leaf node corresponds to complete solution to the problem. The tree has height of H, with the root at height 0, and each nodes height equal to its parents height plus one. Let Sh denote the set of nodes with height {0, 1, , H} and := (cid:83)H h=0 Sh. Note that S0 = 1 since it only contains the root sroot, and Sh+1 = Sh. Therefore, there are B1 nodes in total. Once reaching leaf node SH, the model will receive reward R(s) [0, 1]. policy can be written as π : (cid:83)H1 Sh B, where π(a s) is the probability of selecting the ath child of s. For any h=0 state-action pairs (s, a) [B], let (s, a) be the child at the branch of state s, and (s, a) = for SH. The value function of policy π is written as Vπ : [0, 1]. We write sh0 = s, (sh)H π as the trajectory starting from and sampled according to π, i.e., ah π( sh), sh+1 = (sh, ah). For h=0 Bh = BH+11 h=h 4 any h0 {0, 1, , H} and Sh0, we define Vπ(s) := sh0 reward obtained by following policy π starting from node s. Let π argmaxπ Vπ(sroot) denote the optimal (deterministic) policy that achieves the highest expected reward1. Let := Vπ (sroot) be the expected reward of the optimal policy π. Since π is deterministic, (cid:1) represent the path from the root to leaf node by following π, where let (cid:0)s 1, , 1, 0, 0 = sroot. π [R (sH)], which is the expected =s,(sh)H 0, h=h H"
        },
        {
            "title": "3 Unified Fine-Tuning (UFT)",
            "content": "In this section, we introduce the two key features of UFT: (i) an exploration mechanism guided by hint, which improves sample efficiency by mitigating the sparse reward problem common in rule-based reinforcement learning (DeepSeek-AI et al., 2025); and (ii) hybrid training objective that combines reinforcement learning with log-likelihood term on hints, which provides more informative learning signal and enables the model to acquire knowledge more effectively during fine-tuning."
        },
        {
            "title": "3.1 Exploration with Hint",
            "content": "Although RFT is beneficial for training large models (DeepSeek-AI et al., 2025), several recent studies (Pan et al., 2025) report that small models often fail to reason effectively, as they may never explore the correct answer even once due to the sparse reward. Additionally, other work has found that RFTs final performance is constrained by base models capabilities (Gandhi et al., 2025). To address the sparse reward issue, UFT guides exploration using hint, that is, trajectory sampling starts from the concatenation of the problem description and hint, which is partial solution to the problem (cf. Figure 4). In this way, models will explore the correct answer more frequently. RFT can be modeled as the task of finding path from the root of the problem-solving tree to leaf node that represents the correct answer. As shown in Figure 3, RFT needs to identify the green path. However, the problem-solving tree for real-world tasks, such as math problems, typically contains an enormous number of nodes, making it difficult for an LLM to discover the correct path through exploration alone. To make matters worse, under the rule-based reward model proposed in DeepSeek-AI et al. (2025), only small fraction of the leaf nodes correspond to correct answers, resulting in the well-known sparse reward problem (Ladosz et al., 2022). We address this challenge by concatenating the problem with partial solution, referred to as the hint, to guide the model towards the correct answer. Figure 4 (left) provides an example of UFTs prompt. 3.1.1 Hint Length Sampling Since we are ultimately interested in the LLMs performance when the hint length is zero, the hint must be gradually shortened during training. natural idea is to subtract the hint length by constant amount regularly, which is referred to as the staged reinforcement learning (Xi et al., 2024). However, because solutions typically consist of no more than 10 sentences, changes in hint length can cause significant distribution shift, leading to unstable training (cf. Figure 4, right). To avoid distribution shift during training, Xi et al. (2024) samples the hint length uniformly from all possible values throughout the training. However, relying on hints throughout training introduces significant distribution mismatch between training and evaluation. This often leads to performance 1There exists at least one deterministic optimal policy, and we choose such policy as π. 5 Figure 4: (left). An illustration of the UFT prompt. We adopt the prompting template from TinyZero (Pan et al., 2025), which is similar to that used in Deepseek-R1 (DeepSeek-AI et al., 2025). The hint consists of slice of the full solution. During training, the question prompt and the hint are concatenated and fed to the model. (right). An illustration of the training curve of Qwen2.5-0.5B. Stage and UFT keep zero hint since step 300. collapse at test time, where no hints are available. To address this, UFT employs smoothed reduction of hint length to zero, which (i) avoids drastic distribution shifts and (ii) better aligns the training distribution with the evaluation distribution. Specifically, we maintain variable [0, 1], representing the proportion of the solution revealed to the LLM as hint. The value of gradually descends during training according to cosine annealing (cf. (B.1)) (Loshchilov and Hutter, 2017). Let be the random variable indicating the hint length, and let be the total length of the solution (e.g., number of sentences). By definition, we require {0, 1, , L} and [l] = L, so that the expected hint length matches the proportion p. To achieve this, we sample Binomial(L, p) from Binomial distribution2. It is straightforward to verify that E[l] = [c1] = L. Compared to stage-wise hint length reduction, UFT provides smoother transition from long to short hints. The training curves of these algorithms are shown in Figure 4 (right). We can see that the training curve of UFT is smoother and converges faster than that of the staged reinforcement learning. Note that staged reinforcement learning and UFT do not use any hint since step 300. Figure 5: An ablation study of different hint length schedulers. RFT (cosine) refers to reinforcement learning with our cosine annealing hint length scheduler proposed in this section. As shown in Figure 5, although RFT (cosine), which is RFT equipped with the cosine annealing hint 2Pr (l = l0) = ( l0 )pl0 (1 p)Ll0 for any l0 {0, 1, , L}. In other words, is the number of heads obtained when tossing independent coins, each landing heads with probability p. length scheduler, outperforms R3 (uniform sampling), it is still worse than SFT-RFT. Furthermore, for Llama-3.2-1B, RFT (cosine) is even worse than SFT alone. This implies that the models performance is hindered by its knowledge gained through pretraining (Gandhi et al., 2025), which motivates the second modification of UFT introduced in Section 3.2, an additional log-likelihood term in the objective function."
        },
        {
            "title": "3.2 Objective Function Modification",
            "content": "The hinted RFT only enables LLMs to explore the correct solution more frequently, but remains inefficient at injecting new knowledge into the LLMs. This inefficiency arises because each sampled trajectory provides limited information, essentially signal (correct/incorrect), which provides far less information than the supervision signal in SFT. In contrast, SFT enables more efficient knowledge acquisition, but suffers from poor generalization (Xie et al., 2025; Zeng et al., 2025). To get the best of both worlds, UFT introduces an additional log-likelihood term to the objective function of RFT, allowing the model to learn from the informative supervision signal and still benefit from the generalization of RFT. For notational simplicity, let s0 = sroot, (sh, ah)H1 sh+1 = (sh, ah), i.e., (sh, ah)H1 let value (cid:16) (sh, ah)H1 h=0 β > 0 be the hyperparameter controlling the KL divergence, we have h=0 π denote the shorthand for ah π( sh) and h=0 represents trajectory sampled according to π starting at sroot. Formally, denote the objective function associated with the expected reward3. Then, let (cid:17) RFT = s0 =sroot,(cid:0)sh ,ah (cid:1)H1 h=0 (cid:34) UFT = l,s0 =sroot (cid:1)l1 h=0 (cid:1)H1 h=l (cid:0)sh ,ah (cid:0)sh ,ah π , π (cid:34) π value (cid:16)(cid:0)sh , ah (cid:17) (cid:1)H1 h=0 β (cid:16) KL H1 h=0 π( sh )πref ( sh ) (cid:35) (cid:17) value (cid:16)(cid:0)sh , ah (cid:17) (cid:1)H1 h=l β (cid:16) KL H1 h=l π( sh )πref ( sh ) (cid:17) β KL (cid:0)π ( sh )π( sh )(cid:1) (cid:35) l1 h=0 (3.1) (3.2) Compared to the objective function of GRPO, UFT adds an additional term β l1 h=0 KL (π( sh)π( sh)), the KL divergence between the optimal policy and the current policy. Compared to value, this term explicitly guides the policy towards optimality, and thus results in faster convergence rate. We remark that the optimal policy π is unknown and we cannot compute β l1 h=0 KL (π( sh)π( sh)) directly. However, thanks to the annotations contained in the dataset, we have access to trajectory π, which can be used to estimate the KL-divergence. Accordsampled according to π, i.e., (s h, h)(cid:1) is equivalent to minimizing ing to the definition of KL-divergence, minimizing KL (cid:0)π( ah=1 π(ah B h) (omit terms irrelevant to π), and log h) is an unbiased estimator of it, since h). Therefore, (3.2) can be equivalently written as h) log π( h)π( 1 π(a 1 π(ah h)H1 h= UFT = l,sl =s , (cid:1)H1 h=l (cid:0)sh ,ah π (cid:34) value (cid:16)(cid:0)sh , ah (cid:17) (cid:1)H1 h=l β (cid:16) KL H1 h=l π( sh )πref ( sh ) (cid:17) +β log π(a ) (cid:35) . l1 h=0 (3.3) Therefore, the UFT objective (3.3) can be interpreted as (i) maximizing the expected reward while (ii) staying close to the reference policy and (iii) memorizing the hint by maximizing the log-likelihood of producing the hint. Remark 3.1. The name of Unified Fine-Tuning (UFT) comes from the fact that when 0 for all steps during training, (3.3) is equivalent to RFT, since β l1 h) = 0. When 1, then value (cid:16) = 0, so that (3.3) degenerates to SFT. An illustration can be found in Figure 1 (top middle). π( sh)πref( sh) h=0 log π(a (sh, ah)H1 h=l β H1 h=l KL (cid:17) (cid:17) (cid:16) 3In GRPO (Shao et al., 2024), we have value (cid:16) (sh, ah)H1 h=0 where π is the current policy, πold is the policy at the previous step, (cid:98)Ah is the estimated advantage value in GRPO. πold(ah sh ) (cid:98)Ah , clip π(ah sh ) πold(ah sh ) (cid:110) π(ah sh ) =0 min := 1 H1 (cid:17) (cid:18) , 1 ϵ, 1 + ϵ (cid:19) (cid:111) , (cid:98)Ah It is noteworthy that after adopting the additional log-likelihood term, UFTs performance matches that of SFT-RFT for small models (cf. Figure 5). This suggests that UFT improves the ceiling of RFT by enabling the model to acquire new knowledge during post-training."
        },
        {
            "title": "4 Theoretical Justification",
            "content": "In this section, we provide theoretical justification for UFT. First, we show that the lower bound of RFTs sample complexity grows exponentially (O(BH)) as the tree height (reasoning length) increases. Second, we show that UFT may find the solution within polynomial number of samples (O (cid:0)BH5 log B(cid:1)), representing an exponential improvement of tree height in sample complexity. Next, we define the sub-optimality gap in reward, which is the difference between the rewards for correct and incorrect solutions. Definition 4.1 (Sub-Optimality Gap). There is sub-optimality gap > 0 between the reward of optimal R(s), we have and suboptimal nodes. Formally, for any leaf node SH with reward R(s) < maxsSH R(s) max sSH R(s) . (4.1) In this paper, there are only three possible outcomes for R(s), i.e., no reward (incorrect format), format reward, and accuracy reward. Therefore, the sub-optimality gap = (accuracy reward) (format reward) = 1.0 0.1 = 0.9. (4.2) Next, we will give the lower bound on the RFTs sample complexity to achieve 50% pass@1 success rate4. Theorem 4.2 (Lowerbound). For any integers 1, 2, and any RFT algorithm, there exists problem with height and branching factor B, that satisfies the following: to achieve 50% pass@1 success rate, the algorithm needs to explore at least BH 4 nodes in SH. Moreover, when there are multiple nodes in SH representing the correct solutions, e.g., 1, any algorithm needs to explore at least BH (4.3) 4K nodes in SH. The proof constructs set of problems with different correct solutions, which cannot be distinguished before exploring sufficient nodes in SH. The details can be found in Appendix C. Furthermore, the traditional lower bounds in reinforcement learning (Jin et al., 2018; Domingues et al., 2021) are built on the stochastic transitions of the Markov decision process, but the search trees transition is deterministic, which requires different construction. Theorem 4.2 implies that when the reward is sparse, such as when is constant, learning the optimal policy takes number of iterations exponential in the height of the tree. This also justifies why long reasoning is generally difficult (Chai et al., 2025; Chen et al., 2025). In the following, we will show that UFT exponentially improves the sample complexity. The full algorithm can be found in Algorithm 2. Theorem 4.3 (Informal). When β is small enough, Algorithm 2 obtains 50% pass@1 success rate when the algorithm explores (cid:32) B H5 (log B)2 2 (cid:33) (4.4) nodes in SH. 4The probability of reaching the correct answer when sampling single trajectory. 8 Figure 7: An illustration of the accuracy on the test dataset of Qwen2.5-0.5B. Base is the base model without fine-tuning. R3 (Xi et al., 2024) trained the model with RFT and uniform distribution over all hint lengths. SFT-RFT refers to training supervised fine-tuned model with RFT, and UFT is our algorithm. The formal version is deferred to Appendix E. Note that the 50% pass@1 in both Theorem 4.2 and Theorem 4.3 can be arbitrarily adjusted, and it only affects the sample complexity by constant factor. From Theorem 4.3, we observe that the dependence on is reduced from BH to H5, representing an exponential improvement enabled by the use of hints. Moreover, 2 in the denominator implies that the difference between accuracy reward and format reward should be large for fast convergence, which is also supported by empirical studies (Shao et al., 2024; Pan et al., 2025; Zeng et al., 2025)."
        },
        {
            "title": "5 Experiments",
            "content": "In this section, we present the experimental results of UFT. We demonstrate several key properties of UFT: (i) When the model is small ( 1B) and SFT outperforms RFT, UFTs performance matches that of SFT. (ii) When the model is large ( 3B) and RFT outperforms SFT due to better generalization, UFTs performance matches that of RFT (and sometimes even outperforms it, cf. Table 2). In experiments, we train Qwen2.5-0.5B, Qwen2.5-1.5B, Qwen2.5-3B (Qwen et al., 2025), Llama-3.2-1B, and Llama-3.2-3B (Grattafiori et al., 2024) on Countdown (Wikipedia contributors, 2025; Pan et al., 2025), MATH(3,4,5) (only level 3-5 included) (Hendrycks et al., 2021; Zeng et al., 2025), and the Knights and Knaves logic puzzle (Logic) (Xie et al., 2025)."
        },
        {
            "title": "5.1 The Memorization of UFT",
            "content": "As shown in Figure 7, we can see that when the model is small, the improvement from RFT is marginal, since the model rarely explores the correct answer. As shown in Figure 6, when training Qwen2.5-0.5B on Logic, RFT rarely explores the correct answer, while UFT finds it at every single timestep. Compared to R3, where hints are also applied, UFT outperforms it since UFT (i) gradually shifts the distribution toward hint length of zero, and (ii) maximizes the Figure 6: Qwen2.5-0.5Bs cumulative average success rate for exploring the correct answer at each step when trained on Logic. 9 Figure 8: An illustration of the accuracy on test dataset of Qwen2.5-3B. Base refers to the base model without fine-tuning. log-likelihood on hints to encode information about the solution in gradients. The proximity between the performance of UFT and SFT-RFT also supports the conclusion that UFT helps the model to memorize the solution when the models initial capacity is not enough to solve it."
        },
        {
            "title": "5.2 The Generalization of UFT",
            "content": "As shown in Figure 8, when the model is larger and its prior knowledge gained from pertaining is enough for reasoning, UFT generalizes well as RFT. In contrast, SFT and SFT-RFT are worse, since SFT leads to overfitting. These experiments show that UFT will automatically adapt to model size and enjoy the advantage of both SFT and RFT. As shown in Figure 8, when the model is larger and its prior knowledge gained from pretraining is sufficient for reasoning, UFT generalizes well as RFT. In contrast, SFT and SFT-RFT perform worse, since SFT leads to overfitting. These experiments show that UFT automatically adapts to model size and benefits from the advantages of both SFT and RFT."
        },
        {
            "title": "5.3 UFT Helps LLMs Learn New Knowledge",
            "content": "In Gandhi et al. (2025), it was found that Llama-3.2-3Bs improvement through RFT is marginal compared to that of Qwen2.5-3B. This is because Llama gains less reasoning-related knowledge from pertaining, e.g., backtracking and subgoal setting. In Figure 9, we can see that UFT significantly improves the performance of Llama-3.2. In Countdown, even Llama-3.2-1B outperforms Llama-3.2-3B fine-tuned by RFT after the same number of steps (250 steps). This supports the claim that UFT introduces new knowledge to the model, whereas RFT only helps the model utilize its existing knowledge (Yue et al., 2025)."
        },
        {
            "title": "6 Conclusion and Limitations",
            "content": "This paper proposes novel fine-tuning framework, UFT, which unifies SFT and RFT. Empirically, we show that UFT outperforms both SFT and RFT in general. Specifically, by adopting UFT, small models tend to memorize while large models generalize. Theoretically, we prove that UFT achieves exponential speed-up compared to RFT. However, throughout the paper, we use only the human-annotated solutions in the dataset and GRPO as the reinforcement learning algorithm. In the future, it would be interesting 10 Figure 9: The comparison of Llama-3.2-1B/3Bs behavior in Countdown/MATH/Logic when applying RFT/UFT. In Countdown, the dotted line is the accuracy of Llama-3.2-3B after 250 steps RFT reported in Gandhi et al. (2025) . to explore the incorporation of advanced SFT and RFT techniques into UFT. For instance, using long chain-of-thoughts generated by large models (Muennighoff et al., 2025; Gandhi et al., 2025) for SFT, and choosing other reinforcement learning algorithms such as REINFORCE++ (Hu, 2025) and DAPO (Yu et al., 2025) as the reinforcement learning algorithm for UFT."
        },
        {
            "title": "7 Acknowledgement",
            "content": "The authors would like to thank Jacob Andreas, Chanwoo Park, and Kaiqing Zhang for their valuable discussions. The authors would also like to thank the support of Siebel Scholarship and NSF Award CCF-2443068."
        },
        {
            "title": "References",
            "content": "Alekh Agarwal, Sham Kakade, Jason Lee, and Gaurav Mahajan. On the theory of policy gradient methods: Optimality, approximation, and distribution shift. Journal of Machine Learning Research, 22(98): 176, 2021. Yekun Chai, Haoran Sun, Huang Fang, Shuohuan Wang, Yu Sun, and Hua Wu. Ma-rlhf: Reinforcement learning from human feedback with macro actions. International Conference on Learning Representations (ICLR), 2025. Qiguang Chen, Libo Qin, Jinhao Liu, Dengyun Peng, Jiannan Guan, Peng Wang, Mengkang Hu, Yuhang Zhou, Te Gao, and Wanxiang Che. Towards reasoning era: survey of long chain-of-thought for reasoning large language models. arXiv preprint arXiv:2503.09567, 2025. Tianzhe Chu, Yuexiang Zhai, Jihan Yang, Shengbang Tong, Saining Xie, Dale Schuurmans, Quoc Le, Sergey Levine, and Yi Ma. Sft memorizes, rl generalizes: comparative study of foundation model post-training. arXiv preprint arXiv:2501.17161, 2025. DeepSeek-AI, Daya Guo, Dejian Yang, Haowei Zhang, Junxiao Song, Ruoyu Zhang, Runxin Xu, Qihao Zhu, Shirong Ma, Peiyi Wang, Xiao Bi, Xiaokang Zhang, Xingkai Yu, Yu Wu, Z. F. Wu, Zhibin Gou, Zhihong Shao, Zhuoshu Li, Ziyi Gao, and Aixin Liu et al. Deepseek-r1: Incentivizing reasoning capability in llms via reinforcement learning. arXiv preprint arXiv:2501.12948, 2025. Dongsheng Ding, Kaiqing Zhang, Tamer Basar, and Mihailo Jovanovic. Natural policy gradient primaldual method for constrained markov decision processes. Annual Conference on Neural Information Processing Systems (NeurIPS), 2020. Omar Darwiche Domingues, Pierre Ménard, Emilie Kaufmann, and Michal Valko. Episodic reinforcement learning in finite mdps: Minimax lower bounds revisited. 2021. Kanishk Gandhi, Ayush Chakravarthy, Anikait Singh, Nathan Lile, and Noah Goodman. Cognitive behaviors that enable self-improving reasoners, or, four habits of highly effective stars. arXiv preprint arXiv:2503.01307, 2025. Aaron Grattafiori, Abhimanyu Dubey, Abhinav Jauhri, Abhinav Pandey, Abhishek Kadian, Ahmad AlDahle, Aiesha Letman, Akhil Mathur, Alan Schelten, Alex Vaughan, Amy Yang, Angela Fan, Anirudh Goyal, Anthony Hartshorn, Aobo Yang, Archi Mitra, Archie Sravankumar, Artem Korenev, Arthur Hinsvark, and Arun Rao et al. The llama 3 herd of models. arXiv preprint arXiv:2407.21783, 2024. Dan Hendrycks, Collin Burns, Saurav Kadavath, Akul Arora, Steven Basart, Eric Tang, Dawn Song, and Jacob Steinhardt. Measuring mathematical problem solving with the math dataset. Annual Conference on Neural Information Processing Systems (NeurIPS), 2021. Jian Hu. Reinforce++: simple and efficient approach for aligning large language models. arXiv preprint arXiv:2501.03262, 2025. Chi Jin, Zeyuan Allen-Zhu, Sebastien Bubeck, and Michael Jordan. Is q-learning provably efficient? Annual Conference on Neural Information Processing Systems (NeurIPS), 2018. Sham Kakade and John Langford. Approximately optimal approximate reinforcement learning. International Conference on Machine Learning (ICML), 2002. Pawel Ladosz, Lilian Weng, Minwoo Kim, and Hyondong Oh. Exploration in deep reinforcement learning: survey. Information Fusion, 85:122, 2022. Hunter Lightman, Vineet Kosaraju, Yuri Burda, Harrison Edwards, Bowen Baker, Teddy Lee, Jan Leike, John Schulman, Ilya Sutskever, and Karl Cobbe. Lets verify step by step. International Conference on Learning Representations (ICLR), 2024. Mingyang Liu. On solving larger games: Designing new algorithms adaptable to deep reinforcement learning. Masters thesis, Massachusetts Institute of Technology, 2025. Mingyang Liu, Asuman E. Ozdaglar, Tiancheng Yu, and Kaiqing Zhang. The power of regularization in solving extensive-form games. International Conference on Learning Representations (ICLR), 2023. Mingyang Liu, Gabriele Farina, and Asuman Ozdaglar. policy-gradient approach to solving imperfectinformation games with iterate convergence. arXiv preprint arXiv:2408.00751, 2024. Zichen Liu, Changyu Chen, Wenjun Li, Tianyu Pang, Chao Du, and Min Lin. There may not be aha moment in r1-zero-like training pilot study. https://oatllm.notion.site/oat-zero, 2025. Notion Blog. Ilya Loshchilov and Frank Hutter. Sgdr: Stochastic gradient descent with warm restarts. International Conference on Learning Representations (ICLR), 2017. 12 Liangchen Luo, Yinxiao Liu, Rosanne Liu, Samrat Phatale, Meiqi Guo, Harsh Lara, Yunxuan Li, Lei Shu, Yun Zhu, Lei Meng, et al. Improve mathematical reasoning in language models by automated process supervision. arXiv preprint arXiv:2406.06592, 2024. Jincheng Mei, Chenjun Xiao, Csaba Szepesvari, and Dale Schuurmans. On the global convergence rates of softmax policy gradient methods. International Conference on Machine Learning (ICML), 2020. Niklas Muennighoff, Zitong Yang, Weijia Shi, Xiang Lisa Li, Li Fei-Fei, Hannaneh Hajishirzi, Luke Zettlemoyer, Percy Liang, Emmanuel Candès, and Tatsunori Hashimoto. s1: Simple test-time scaling. arXiv preprint arXiv:2501.19393, 2025. Jiayi Pan, Junjie Zhang, Xingyao Wang, Lifan Yuan, Hao Peng, and Alane Suhr. Tinyzero. https://github.com/Jiayi-Pan/TinyZero, 2025. Accessed: 2025-01-24. An Yang Qwen, Baosong Yang, Beichen Zhang, Binyuan Hui, Bo Zheng, Bowen Yu, Chengyuan Li, Dayiheng Liu, Fei Huang, Haoran Wei, Huan Lin, Jian Yang, Jianhong Tu, Jianwei Zhang, Jianxin Yang, Jiaxi Yang, Jingren Zhou, Junyang Lin, Kai Dang, and Keming Lu et al. Qwen2.5 technical report. arXiv preprint arXiv:2412.15115, 2025. Amrith Setlur, Chirag Nagpal, Adam Fisch, Xinyang Geng, Jacob Eisenstein, Rishabh Agarwal, Alekh Agarwal, Jonathan Berant, and Aviral Kumar. Rewarding progress: Scaling automated process verifiers for llm reasoning. International Conference on Learning Representations (ICLR), 2025. Zhihong Shao, Peiyi Wang, Qihao Zhu, Runxin Xu, Junxiao Song, Xiao Bi, Haowei Zhang, Mingchuan Zhang, YK Li, Wu, et al. Deepseekmath: Pushing the limits of mathematical reasoning in open language models. arXiv preprint arXiv:2402.03300, 2024. Guangming Sheng, Chi Zhang, Zilingfeng Ye, Xibin Wu, Wang Zhang, Ru Zhang, Yanghua Peng, Haibin Lin, and Chuan Wu. Hybridflow: flexible and efficient rlhf framework. arXiv preprint arXiv: 2409.19256, 2024. Taiwei Shi, Yiyang Wu, Linxin Song, Tianyi Zhou, and Jieyu Zhao. Efficient reinforcement finetuning via adaptive curriculum learning. arXiv preprint arXiv:2504.05520, 2025. Mingyang Song, Mao Zheng, Zheng Li, Wenjie Yang, Xuan Luo, Yue Pan, and Feng Zhang. Fastcurl: Curriculum reinforcement learning with progressive context extension for efficient training r1-like reasoning models. arXiv preprint arXiv:2503.17287, 2025. Peiyi Wang, Lei Li, Zhihong Shao, RX Xu, Damai Dai, Yifei Li, Deli Chen, Yu Wu, and Zhifang Sui. Math-shepherd: Verify and reinforce llms step-by-step without human annotations. arXiv preprint arXiv:2312.08935, 2023. Liang Wen, Yunke Cai, Fenrui Xiao, Xin He, Qi An, Zhenyu Duan, Yimin Du, Junchen Liu, Lifu Tang, Xiaowei Lv, et al. Light-r1: Curriculum sft, dpo and rl for long cot from scratch and beyond. arXiv preprint arXiv:2503.10460, 2025. Wikipedia contributors. Countdown (game show). https://en.wikipedia.org/wiki/Countdown_(game_ show), 2025. Zhiheng Xi, Wenxiang Chen, Boyang Hong, Senjie Jin, Rui Zheng, Wei He, Yiwen Ding, Shichun Liu, Xin Guo, Junzhe Wang, et al. Training large language models for reasoning through reverse curriculum reinforcement learning. International Conference on Machine Learning (ICML), 2024. Tian Xie, Zitian Gao, Qingnan Ren, Haoming Luo, Yuqian Hong, Bryan Dai, Joey Zhou, Kai Qiu, Zhirong Wu, and Chong Luo. Logic-rl: Unleashing llm reasoning with rule-based reinforcement learning. arXiv preprint arXiv:2502.14768, 2025. Qiying Yu, Zheng Zhang, Ruofei Zhu, Yufeng Yuan, Xiaochen Zuo, Yu Yue, Tiantian Fan, Gaohong Liu, Lingjun Liu, Xin Liu, et al. Dapo: An open-source llm reinforcement learning system at scale. arXiv preprint arXiv:2503.14476, 2025. Lifan Yuan, Wendi Li, Huayu Chen, Ganqu Cui, Ning Ding, Kaiyan Zhang, Bowen Zhou, Zhiyuan Liu, and Hao Peng. Free process rewards without process labels. arXiv preprint arXiv:2412.01981, 2024. Zheng Yuan, Hongyi Yuan, Chengpeng Li, Guanting Dong, Keming Lu, Chuanqi Tan, Chang Zhou, and Jingren Zhou. Scaling relationship on learning mathematical reasoning with large language models. arXiv preprint arXiv:2308.01825, 2023. Yang Yue, Zhiqi Chen, Rui Lu, Andrew Zhao, Zhaokai Wang, Shiji Song, and Gao Huang. Does reinforcement learning really incentivize reasoning capacity in llms beyond the base model? arXiv preprint arXiv:2504.13837, 2025. Weihao Zeng, Yuzhen Huang, Qian Liu, Wei Liu, Keqing He, Zejun Ma, and Junxian He. Simplerl-zoo: Investigating and taming zero reinforcement learning for open base models in the wild. arXiv preprint arXiv:2503.18892, 2025. Han Zhong, Zikang Shan, Guhao Feng, Wei Xiong, Xinle Cheng, Li Zhao, Di He, Jiang Bian, and Liwei Wang. Dpo meets ppo: Reinforced token optimization for rlhf. arXiv preprint arXiv:2404.18922, 2024. Chunting Zhou, Pengfei Liu, Puxin Xu, Srinivasan Iyer, Jiao Sun, Yuning Mao, Xuezhe Ma, Avia Efrat, Ping Yu, Lili Yu, et al. Lima: Less is more for alignment. Annual Conference on Neural Information Processing Systems (NeurIPS), 2023."
        },
        {
            "title": "A Related Work",
            "content": "In this section, we introduce related work about SFT, RFT, and curriculum learning for reasoning. Supervised Fine-Tuning (SFT) for Reasoning. Different SFT methods for enhancing reasoning capability usually differ in the source of the collected reasoning trace. Zeng et al. (2025) uses traditional SFT, i.e., learning from the human-annotated problem solutions. In contrast, Gandhi et al. (2025); Muennighoff et al. (2025) utilize long chain-of-thoughts solutions generated by some large models, such as Claude and Deepseek-R1 (DeepSeek-AI et al., 2025). On the other hand, Yuan et al. (2023); Xie et al. (2025) utilizes rejection sampling fine-tuning. Specifically, the model will generate multiple reasoning traces, and the one that leads to the correct answer is selected for further fine-tuning. In this paper, we use human annotations as the SFT data (traditional SFT), as it is sufficient for our purpose and keeps the focus on our main contribution (unifying SFT and RFT). Reinforcement Fine-Tuning (RFT) for Reasoning. RFT for reasoning can be categorized into process supervision and outcome supervision. Process supervision assigns reward to each step of long reasoning trace (Lightman et al., 2024), which evaluates whether each step is correct or not. The main drawback of process supervision is that it is costly to prepare step-by-step feedback data. On the other hand, outcome supervision assigns single reward to the entire trace (DeepSeek-AI et al., 2025; Zeng et al., 2025; Yu et al., 2025), e.g., whether the trace yields the correct answer to math problem. Furthermore, Wang et al. (2023); Yuan et al. (2024); Zhong et al. (2024); Luo et al. (2024); Setlur et al. (2025) learn step-by-step reward model from collection of reasoning traces with outcome rewards, which avoids the cost of preparing step-by-step data. In this paper, due to the efficiency and simplicity of outcome supervision, we focus on the comparison with RFT using outcome supervision. Curriculum Learning for Reasoning. Existing curriculum reinforcement learning for reasoning mainly focuses on utilizing collection of problems with varying difficulties (Wen et al., 2025; Shi et al., 2025; Song et al., 2025). These methods train the model with problems of gradually increasing difficulty, where the difficulty is determined by predefined criteria, such as the length of the successful reasoning trace (Song et al., 2025) or the success rate of baseline models (Shi et al., 2025; Wen et al., 2025). However, such methods fail when the problems in the dataset are homogeneous in difficulty. In contrast, Xi et al. (2024) proposes curriculum learning method that concatenates the problem with slice of the solution (hint). The difficulty is determined by the hint length. However, Xi et al. (2024) uses uniform distribution over all possible hint lengths, which misaligns with the distribution of interest (zero hint length). On the other hand, UFT designs hint length scheduler that smoothly reduces the hint length to zero. Furthermore, UFT adds an additional log-likelihood term for the hint in the objective function, which helps the model to acquire new knowledge more efficiently and increases the ceiling of reinforcement learning (cf. Figure 5)."
        },
        {
            "title": "B Experiment Details",
            "content": "In this section, we introduce the details of the experiments, including the pseudo-code of UFT (Appendix B.1), the hyperparameters used (Appendix B.2), and additional experiment results (Appendix B.3). 15 Algorithm 1: Unified Fine-Tuning Hyperparameters: KL-penalty coefficient β, total number of steps T, number of steps with hint Thint, low/high probability plow/phigh for hint sampling, and hint length Input: Reference policy parameter θθθref Initialization: θθθ(0) θθθref 1 for = 0, 1, , 1 do 2 Sample batch of problems {} for (Q, S, A) do 3 4 5 6 7 9 10 11 12 13 14 end // For each (question, solution, answer) pair if < Thint then p(t) plow + 1 2 (cid:16) phigh plow(cid:17) (cid:18) 1 + cos (cid:19)(cid:19) (cid:18) + 1 Thint π (B.1) // Cosine annealing, π 3.14159 is the Pi constant Sample l(t) Binomial (cid:16) min {L, len(S)} , p(t)(cid:17) else l(t) = 0 end (cid:110) + S[: l(t)] (cid:111) and add to // Concatenate the question with the partial solution (hint) end Run reinforcement learning algorithm on with the objective function (3.3) B.1 Algorithm This section presents the pseudo-code of UFT in Algorithm 1. In lines 4-9: we sample the hint length for each (question, solution, answer) pair in the sampled data batch B. In lines 11-13, we concatenate the question with the partial solution of length l(t) and feed it into reinforcement learning algorithm (such as GRPO), with the objective function (3.3). B.2 Cost and Implementation Details The project costs roughly $10,000 GPU hours. The experiment is based on VERL (Sheng et al., 2024) and TinyZero (Pan et al., 2025). The hyperparameters for training on different datasets are listed in Table 1. The omitted hyperparameters follow the default values of VERL (Sheng et al., 2024). B.3 Additional Results Figure 10 shows the response of the model trained via different algorithms. For Qwen2.5-0.5B, UFTs response aligns with the solution better than RFTs. For Qwen2.5-3B, UFT generates longer reasoning trace and presents skills such as verification (Gandhi et al., 2025), while SFT-RFT does not. Data Training Batch Size Validation Batch Size Mini-batch Size Hint Length Training β Thint Number of Rollouts Context Window (Prompt) Context Window (Response) plow phigh SFT Epochs Reward Accuracy Reward Format Correctness Reward Incorrect Reward 256 1312 64 5 0.001 500 300 4 Countdown: 256 MATH(3,4,5): 1024 Logic: 1024 1024 0.05 0.95 1.0 0.1 0.0 Table 1: The hyperparameters for training on different datasets. The other parameters follow the default parameters of VERL (Sheng et al., 2024). Table 2 shows the accuracy results across different datasets. For clarity, we report the average accuracy over models trained on three datasets: Countdown, MATH(3,4,5), and Logic. For smaller models such as Qwen2.5-0.5B, SFT-RFT achieves an accuracy of 7.28%, compared to only 3.25% for RFT. In contrast, UFT achieves 9.45% accuracy, outperforming both. For larger models such as Qwen2.5-3B, SFT-RFT achieves 17.34% accuracy, which is significantly lower than RFTs 32.15%. However, UFT still performs competitively, reaching 30.93% and closely matching RFT. In summary, UFT combines the strengths of both SFT and RFT. When the model is small and memorization plays key role, UFT matches or exceeds SFTs performance. When the model is large and generalization becomes more important, UFT benefits similarly to RFT, achieving comparable accuracy. Proof of Theorem 4.2 Theorem 4.2 (Lowerbound). For any integers 1, 2, and any RFT algorithm, there exists problem with height and branching factor B, that satisfies the following: to achieve 50% pass@1 17 Figure 10: Responses of Qwen2.5-0.5/3B trained by different algorithms. success rate, the algorithm needs to explore at least nodes in SH. Moreover, when there are multiple nodes in SH representing the correct solutions, e.g., 1, any algorithm needs to explore at least BH 4K nodes in SH. BH 4 (4.3) Proof. Proving the lower bound of exploration is equivalent to the following. Find the maximum > 0, such that any algorithm will fail to learn the optimal policy with probability at least 0.5 within explorations. Consider the (BH ) possible trees, each associated with distinct subset of SH of size K, where that subset represents the correct solution for that specific tree. At the beginning, we pick an instance from all those possible trees uniformly at random. During each exploration, the algorithm requests the reward at node in SH. Let s(1), s(2), . . . , s(T) be the leaf node reached at timestep 1, 2, . . . T, which are random variables depending on the randomness R(s)(cid:9) be the set of nodes representing correct of the algorithm. Let solutions. Note that given the construction of the instances, H = K. Then, the probability of reaching one of the correct solutions in := (cid:8)s SH : R(s) = maxsSH is (cid:18)(cid:110) Pr s(t)(cid:111)T t=1 (cid:19) = (cid:18) (cid:18) Pr Pr = t=1 t=1 s(t) s(t) (cid:110) s(s)(cid:111)t1 s=1 (cid:110) s(s)(cid:111)t1 s= = (cid:18)(cid:110) (cid:19) Pr s(s)(cid:111)t s=1 (cid:19) = = (cid:19) . Given that we pick uniformly at random, Pr (cid:18) s(t) (cid:110) s(s)(cid:111)t1 s=1 = (cid:19) = H BH t+1 . Therefore, (cid:18)(cid:110) Pr s(t)(cid:111)T t=1 = (cid:19) t= H BH + 1 . 18 Model Algorithm MATH(3,4,5) AIME24 AMC Countdown Logic MATH500 Minerva Olympiad GSM8k Avg. Base SFT RFT Qwen2.5-0.5B SFT-RFT R3 UFT Base SFT RFT Qwen2.5-1.5B SFT-RFT R3 UFT Base SFT RFT SFT-RFT R3 UFT Base SFT RFT SFT-RFT R3 UFT Base SFT RFT SFT-RFT R3 UFT Qwen2.5-3B Llama-3.2-1B Llama-3.2-3B 3.03 4.92 3.78 8.69 9.86 13. 24.51 12.47 24.77 15.72 28.12 34.08 31.45 24.32 45.74 26.50 44.01 47.04 0.00 1.07 0.94 0.42 1.53 1.17 0.00 2.54 0.00 3.16 2.93 1.24 0.00 0.00 0.00 0.00 0.00 0.00 3.33 0.00 2.22 1.11 2.22 3. 0.00 0.00 4.44 1.11 2.22 3.33 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 1.61 3.21 3.61 6.43 6.83 4.82 5.62 9.24 6.83 13.65 14.86 13.25 10.04 24.90 9.64 21.29 29. 0.00 0.80 2.41 0.00 1.61 0.00 0.00 0.40 0.00 2.41 3.21 1.20 0.00 11.20 8.30 17.45 9.99 17.15 0.20 13.48 27.86 20.51 23.57 24.54 3.81 15.07 34.08 17.61 27.12 31.38 0.00 13.41 0.00 18.68 9.90 17. 0.00 14.68 0.00 16.05 17.55 17.64 0.00 1.87 0.00 7.07 4.20 4.87 2.20 5.33 3.00 11.13 11.47 10.07 5.60 10.20 30.33 19.60 24.80 26.07 0.00 3.67 0.00 8.33 0.13 7.40 0.00 6.13 0.00 8.87 9.93 6. 1.73 2.13 2.47 2.07 3.33 5.40 18.27 6.40 10.53 5.00 14.93 20.87 24.53 16.80 31.27 14.07 28.00 29.73 0.00 0.00 0.47 0.00 0.33 0.07 0.00 0.00 0.00 0.07 0.87 1.13 0.74 2.08 3.80 4.41 5.02 5. 4.41 4.53 6.86 4.41 7.48 8.33 4.78 5.27 12.25 5.76 10.91 12.99 0.00 0.74 0.49 1.23 2.94 2.82 0.00 1.72 0.00 3.92 3.06 1.10 0.30 1.33 2.57 2.12 3.11 2.77 5.48 2.62 6.47 4.59 9.43 9. 7.70 5.19 15.65 6.77 14.57 14.17 0.00 0.25 0.84 0.20 0.99 0.74 0.00 0.54 0.05 0.89 1.04 0.30 7.66 13.07 3.87 16.45 20.09 24.59 60.96 29.74 45.69 30.02 49.79 66.46 57.85 45.54 80.84 48.22 70.20 74. 0.08 1.87 1.42 0.48 1.49 1.14 0.00 7.08 0.00 5.76 5.16 4.12 1.55 4.46 3.25 7.28 7.36 9.45 14.29 9.36 16.08 11.70 18.65 22.23 17.13 15.25 32.15 17.34 28.02 30.93 0.01 2.49 0.80 3.29 2.20 3. 0.00 3.85 0.01 4.79 5.03 3.72 Table 2: Average performance of Qwen2.5-0.5/1.5/3B and Llama-3.2-1/3B across all three training datasets, Countdown, MATH(3,4,5), and Logic. When BH 4S , we have (cid:18)(cid:110) Pr s(t)(cid:111)T t=1 = (cid:19) t=1 H BH + 1 (i) t=1 2 H BH = 2 H BH 1 2 . (i) uses the fact that BH 4S find the correct answer with probability at least 0.5. 2 . Therefore, within BH 4S BH exploration, the algorithm will fail to"
        },
        {
            "title": "D Extended Theoretical Justifications",
            "content": "In this section, we introduce some additional notations in Appendix D.1 and then present the theoretically sound UFT in Appendix D.2. D.1 Extended Preliminaries Notation. For any vector Rn, let xi be its ith element and xp be the Lp-norm, where denotes the L2-norm by default. For any two vectors x, Rn, let x, := i=1 xi yi denote their inner product. 19 Softmax Parameterized Policy. Algorithm 2 assumes the policy follows softmax parameterization. Formally, the policy πθθθ is controlled by θθθ RSB, such that for any and [B], πθθθ(a s) := exp(θ(s, a)) a=1 exp(θ(s, a)) . (D.1) The softmax-parameterized policy is also widely adopted in the literature (Mei et al., 2020; Agarwal et al., 2021; Ding et al., 2020) to sidestep the complexities of analyzing non-convex neural networks and to keep the focus on the learning algorithm itself. D.2 Theoretically Sound UFT The full algorithm is shown in Algorithm 2. In lines 2-3: we sample the hint length and trajectory starting from the hint. In lines 6-10, we estimate Q-values by sampling an additional trajectory for each state-action pair, which can greatly reduce the variance of sampling. In lines 13-14, we compute the objective function and update the parameters by gradient ascent. In lines 16-17, we estimate the expected reward of each intermediate policy and return the best one. Note that Algorithm 2 differs slightly from the UFT shown in Algorithm 1. While Algorithm 1 leaves the choice of the reinforcement learning algorithm unspecified, Algorithm 2 explicitly defines the trajectory rolling mechanism and update rule for concrete theoretical analysis. Further, Algorithm 2 assumes softmax-parameterized policy, whereas Algorithm 1 imposes no constraints on the policy network architecture. Proof of Theorem 4.3 In this section, for notational simplicity, we use π(t) to denote πθθθ(t) any [T], we define (cid:101)A(t1)(s, a) = (cid:101)Q(t1)(s, a) = 0 for those nodes off the sampled path timestep t. for any {0, 1, , T}. Moreover, for (cid:17)H (cid:16) (t) at h=l(t) Theorem E.1 (Formal). Consider Algorithm 2. When β πθθθ((cid:101)t ) (pass @ 1) of policy πθθθ((cid:101)t ) satisfies Pr 12(H+1)2(log B+2θθθref) , the pass @ 1 accuracy when Pr πθθθ((cid:101)t ) (pass @ 1) 0.5, (H + 1)2 (cid:16) = log + 2 / (cid:13) (cid:13) (cid:13)θθθref (cid:13) (cid:13) (cid:13) (cid:17) 2 + (E.1) (E.2) and explores no more than (BH + N)T leaf nodes in SH. Proof. The update rule can be divided into two steps: (i) Use the concentration bound to get high- (cid:68) Qπ(t1) probability bound on in each node to the Vπ(t1) the bound on expected reward to success rate (cf. Appendix E.3). (s, ), π( s) π(t1)( s) (cid:69) (sroot) by the regret decomposition lemma (cf. Appendix E.2); (iii) Convert (cf. Appendix E.1); (ii) Convert the difference Algorithm 2: Theoretically Sound Unified Fine-Tuning Hyperparameters: Learning rate η, KL-penalty coefficient β, and total number of steps Input: Reference policy parameter θθθref Initialization: θθθ(0) θθθref 1 for = 0, 1, , 1 do 2 Sample l(t) Uniform(0, 1, 2, , 1, H) // In fact, any distribution with full support on {0, 1, 2, , 1, H} is fine. We choose the uniform distribution for simplicity 3 4 6 7 8 9 10 12 13 14 Sample trajectory for = l(t), l(t) + 1, 1 do h=l(t) (cid:16) (cid:17)H (t) πθθθ(t) , where (t) l(t) = l(t) for = 1, 2, , do // Group sampling (cid:16) Sample trajectory (cid:101)Q(t) (cid:16) (t) , (cid:17) (cid:16) (t),a (t),a (cid:17)H h=h+ (cid:17) πθθθ(t) starting from (t),a h+1 = (s (t) , a) end for = 1, 2, , do (t) , (cid:101)A(t) (cid:16) (cid:17) (cid:101)Q(t) (cid:16) (t) , (cid:17) a=1 πθθθ(t) (cid:16) (t) (cid:17) (cid:101)Q(t) (cid:16) (t) , (cid:17) end end // (cid:101)A(t)(s, ) 0 for any off the trajectory (cid:16) (t) (cid:17)H h=l(t) (t) H1 h=l(t) a=1 πθθθ(t) (cid:16) (t) (cid:17) (cid:101)A(t) (cid:16) (cid:17) (t) , β H1 h=l(t) (cid:16) πθθθ(t) (cid:16) KL (t) (cid:17) πθθθref (cid:16) (t) (cid:17)(cid:17) + β l(t)1 h=0 log πθθθ(t) (a h) θθθ(t+1) θθθ(t) + ηπJ (t) (D.2) (sroot) = 1 n=1 , where = 72 log(14(T+1)) πθθθ(t) (cid:17) (cid:16) (t),n (cid:101)s 2 by sampling trajectories (cid:101)s (t),n 0 = sroot and 15 end 16 Estimate (cid:101)Vπθθθ(t) (cid:17)H (cid:16) (t),n (cid:101)s h=0 Return: πθθθ((cid:101)t ) 17 (cid:101)t = argmaxt{0,1, ,T} (cid:101)Vπθθθ(t) (sroot) E.1 Concentration Bound For any height {0} [H 1], state Sh, and action [B], we can define the Q-value of the state-action pair (s, a) [B] when following policy π as Qπ(s, a) := sh=s,(sh )H =h π [R (sH)] . (E.3) 21 Then, for any SH and [T], we have (cid:104) (cid:101)Q(t1)(s, a) (cid:105) = Pr (cid:18) (cid:110) (t) (cid:111)H (cid:19) h=l(t) Qπ(t1) (s, a), (E.4) where the expectation is taken over the probability of sampling trajectories in Algorithm 2. Next, we will introduce Lemma 5.3 in Liu et al. (2024). Proposition E.2. Let M, (cid:101)M 0 be the constants such that (cid:101)M for any [T] and x, C, where is convex set. If for any C, we have (cid:12) (cid:12) (t)(x) (t)(x) (cid:12) (cid:12) (cid:12) (cid:12) and (cid:12) (cid:12) (cid:101)f (t)(x) (cid:101)f (t)(x) (cid:12) (cid:12) (cid:12) (cid:12) (cid:104) (cid:101)f (t)(x) (cid:101)f (1), (cid:101)f (2), , (cid:101)f (t1)(cid:105) = (t)(x), and x(t) is deterministically influenced by (cid:101)f (1), (cid:101)f (2), , (cid:101)f (t1), then for any δ (0, 1) and C, we have Pr (cid:16) (cid:32) t= (t)(x) (t)(x(t)) (cid:17) (cid:16) t=1 (cid:101)f (t)(x) (cid:101)f (t)(x(t)) (cid:17) (cid:16) + + (cid:101)M (cid:17) (cid:114) 2T log (cid:33) 1 δ 1 δ. For any < and Sh, let (t)(x) = Pr [0, 1] since each element of Q(t1)(s, ) is bounded by [0, 1] by definition. Therefore, in Proposition E.2 (cid:101)Q(t1)(s, ), is 1. Similarly, let (cid:101)f (t)(x) = and we have (cid:101)M = 1. Therefore, by (E.4), Proposition E.2, and Lemma E.3, for any δ (0, 1), with probability at least 1 δ, we have , where (t) : (s, ), h=l(t1) (cid:69) (cid:68) Qπ(t1) (cid:18) (cid:110) (cid:111)H (t1) (cid:19) (cid:68) (cid:69) t=1 t=1 t=1 (i) = (cid:68) (cid:68) = (cid:18) (cid:110) (t) Pr (cid:111)H (cid:19) (cid:68) h=l(t) Qπ(t1) (s, ), π( s) π(t1)( s) (cid:69) (cid:68) (cid:68) (cid:101)Q(t1)(s, ), π( s) π(t1)( s) (cid:101)A(t1)(s, ), π( s) π(t1)( s) (cid:69) (cid:69) (cid:114) + 2 2T log (cid:114) + 2 2T log 1 δ 1 δ . (cid:101)A(t1)(s, ), π( s) π(t1)( s) (cid:101)Q(t1)(s, ), π( s) π(t1)( s) (cid:69) (cid:69) (i) is because + π(t1)(a s) (cid:101)Q(t1)(s, a) a=1 (cid:101)Q(t1)(s, ), π( s) π(t1)( s) a= (cid:16) (cid:68) = (cid:69) . π(a s) π(t1)(a s) (cid:17) By the update rule of Algorithm 2, we have the following lemma. Lemma E.3. Consider Algorithm 2. For any node SH, we have t=1 (cid:18) 1 η (cid:68) (cid:101)A(t1)(s, ), π( s) π(t1)( s) (cid:69) (cid:19) + βT KL (cid:16) π( s)πθθθref ( s) (cid:17) + 2ηT. 22 The proof is postponed to Appendix E.4. Lemma E.3 gives us an upper bound on the accumulated difference between our policy π(t1) and the optimal policy π. Therefore, t=1 t=1 (cid:18) 1 η (cid:18) (cid:110) (t) Pr (cid:111)H (cid:19) (cid:68) h=l(t) Qπ(t1) (s, ), π( s) π(t1)( s) (cid:69) (cid:68) (cid:101)A(t1)(s, ), π( s) π(t1)( s) (cid:19) + βT KL (cid:16) π( s)πθθθref ( s) (cid:17) (cid:69) (cid:114) + 2 2T log 1 δ (cid:114) + 2ηT + 2 2T log 1 δ . E.2 Difference Decomposition Let µπ(s) be the probability of reaching state from the root by following policy π. Hence, µπ(sroot) = 1. For any SH and action [B], µπ (T (s, a)) can be recursively defined as µπ (T (s, a)) = µπ(s) π(s, a). (E.5) In the following, we will introduce Lemma E.4, which is special case of the regret decomposition lemma (Lemma 5.1) in Liu et al. (2023). Specifically, it is the regret decomposition lemma for two-player zero-sum extensive-form game without chance nodes5, and the second players action sets at all nodes are of size 1. Lemma E.4. For any sequence of policies π(1), π(2), , π(T) and policy π, we have (cid:16) t= Vπ(sroot) Vπ(t) (sroot) (cid:17) = µπ(s) sSSH (cid:68) Qπ(t) t=1 (s, ), π( s) π(t)( s) (cid:69) . Lemma E.4 can also be viewed as the performance difference lemma in reinforcement learning (Kakade and Langford, 2002) for tree-shape Markov decision process. For completeness, we also provide the proof at the end of this section. By letting π(t) = π(t1) for any [T] and π = π, we have Vπ(t1) (sroot) (cid:17) (cid:16) t= = µπ (s) sSSH (cid:68) Qπ(t1) t=1 (s, ), π( s) π(t1)( s) (cid:69) (i) = = 1 , ,s s{s 0 ,s H1} µπ (s) (cid:68) Qπ(t1) t=1 (s, ), π( s) π(t1)( s) (cid:69) 1 , ,s t= (cid:18) µπ (cid:110) (s) (cid:111)H (t) s{s 0 ,s H1} Pr (cid:68) Qπ(t1) h=l(t) (cid:69) (s, ), π( s) π(t1)( s) . (cid:18) (cid:110) (t) (cid:19) Pr (cid:111)H (cid:19) h=l(t) 5Chance nodes represent the randomness of the game, such as rolling dice. 23 (i) uses the fact that π is deterministic such that µπ sampled from (cid:8)s (cid:9) uniformly, for any (cid:8)s (s) > 0 only when (cid:8)s (cid:9), we have 0, 1, , 0, 1, , H 0, 1, , (cid:9). Since (t) l(t) is (cid:18) (cid:110) (t) Pr (cid:111)H (cid:19) h=l(t) Pr (cid:16) = (cid:17) (t) l(t) = 1 + 1 . (cid:19) + 1 and we have Therefore, (cid:18) Pr µπ (cid:110) (t) (s) (cid:111)H h=l(t) t=1 H1 h=0 (cid:16) Vπ(t1) (sroot) (cid:17) µπ (cid:110) (s h) (cid:111)H (t) (cid:18) Pr s (cid:32)(cid:18) 1 η h=l(t) (cid:19) + βT KL (H + 1) H1 h= (cid:32)(cid:18) 1 η (cid:19) (cid:19) + βT KL (cid:16) π( h)πθθθref ( h) (cid:17) (cid:114) + 2ηT + 2T log (cid:33) 1 δ (cid:16) π( h)πθθθref ( h) (cid:17) (cid:114) + 2ηT + 2 2T log (cid:33) . 1 δ Next, we can bound KL (cid:16) π( h)πθθθref ( h) (cid:17) by the following lemma. Lemma E.5. For any {0, 1, , 1}, we have (cid:16) KL π( h)πθθθref ( h) (cid:17) log + 2 The proof is postponed to Appendix E.4. Therefore, by taking η = 1 , we have (cid:13) (cid:13) (cid:13)θθθref(cid:13) (cid:13) (cid:13) . Vπ(t1) (sroot) (cid:17) (cid:16) t=1 (H + 1)2 (cid:32) (cid:16) log + 2 (cid:13) (cid:13) (cid:13)θθθref(cid:13) (cid:13) (cid:13) (cid:17) + 2 (cid:114) + 2 2T log (cid:33) 1 δ + βT(H + 1) (cid:16) KL H1 h=0 π( h)πθθθref ( h) (cid:17) . Because Vπ(t1) {0, 1, . . . , T} such that (sroot) 0 for any [T], according to pigeon hole principle, there must exist Vπ(t ) (cid:18)(cid:16) (H + 1)2 (sroot) log + 2 (cid:17) (cid:13) (cid:13) (cid:13) (cid:13) (cid:13) (cid:13)θθθref + 2 + 2 (cid:19) (cid:113) 2 log 1 δ + β(H + 1) (cid:16) KL H1 h=0 π( h)πθθθref ( h) (cid:17) . 24 For any ϵ > β(H + 1) H1 h=0 KL (cid:16) π( h)πθθθref ( h) (cid:17) , it takes (cid:18) (H + 1)2 log + 2 (cid:13) (cid:13) (cid:13) + 2 + (cid:113) 2 log 1 δ (cid:13) (cid:13) (cid:13)θθθref (cid:16) (cid:19) 2 (cid:17) ϵ β(H + 1) H1 h=0 KL π( h)πθθθref ( h) iterations to satisfy Vπ(t ) Recall that > 0 is the sub-optimality gap. By picking ϵ = get ϵ accuracy with probability 1 δ, we need (sroot) ϵ. 6 , δ = 1 8 , and β 12(H+1)2(log B+2θθθref) , to (H + 1)2 (cid:16) = log + 2 /12 (cid:13) (cid:13) (cid:13)θθθref (cid:13) (cid:13) (cid:13) (cid:17) 2 + 7 iterations, which implies (cid:18) H4(log B)2 2 (cid:19) . Since O(B H) leaf nodes are explored at each iteration, the number of leaf nodes explored during training is O(B T) (cid:18) H5(log B)2 2 (cid:19) . E.3 Compute Probability To find t, we need to estimate Vπθθθ(t) for all {0, 1, , T} by sampling trajectories. By sampling (cid:16) trajectory from πθθθ(t) representing whether the trajectory reaches the correct solution. Then, by Hoeffdings inequality, by sampling trajectories, we have , we can get random variable from Bernoulli π(t) (pass @ 1) Prcond (cid:17) (cid:18)(cid:12) (cid:12) (cid:12) (cid:12) Pr (cid:101)Vπθθθ(t) (sroot) Vπθθθ(t) (sroot) (cid:12) (cid:12) (cid:12) (cid:12) (cid:19) 12 2 exp (cid:18) N2 72 (cid:19) (i) = 1 7(T + 1) . (E.6) (i) is by definition of in Algorithm 2. By union bound, for any {0, 1, , T}, 12 holds with probability at least 1 T+ 7(T+1) = 6 (cid:12) (cid:12) (cid:12) (cid:12) (cid:101)Vπθθθ(t) (sroot) Vπθθθ(t) (sroot) (cid:12) (cid:12) (cid:12) (cid:12) Vπθθθ((cid:101)t ) (sroot) (cid:101)Vπθθθ((cid:101)t ) (sroot) 7 . Therefore, (cid:101)Vπθθθ(t ) Vπθθθ(t ) (sroot) (sroot) 12 ϵ 6 = . Recall that Prπ((cid:101)t ) (pass @ 1) is the pass @ 1 accuracy of policy π((cid:101)t). In the following, we will use Prcond as shorthand of Pr Vπ((cid:101)t ) (cid:17) (cid:16) . (sroot) 3 Prcond π((cid:101)t ) (pass @ 1) = Prcond s0=sroot,(sh)H (cid:32) R(sH) = max h=0π((cid:101)t ) SH h=0π((cid:101)t ) (R(sH) = V) . (cid:33) R(s H) = Prcond s0=sroot,(sh)H Furthermore, 3 Vπ((cid:101)t ) (sroot) =E h=0π((cid:101)t ) [R(sH)] h=0π((cid:101)t ) (R(sH) = V) s0=sroot,(sh)H Prcond s0=sroot,(sh)H (cid:18) + 1 Prcond s0=sroot,(sh)H h=0π((cid:101)t ) (R(sH) = V) (cid:19) (V ) . By combining all pieces together, we have π((cid:101)t ) (pass @ 1) + (cid:16) 1 Prcond π((cid:101)t ) (pass @ 1) (cid:17) (V ) Prcond , 3 π((cid:101)t ) (pass @ 1) 2 3 . which implies that Prcond Finally, Prπ((cid:101)t ) (pass @ 1) Prcond π(t ) (pass @ 1) Pr 2 3 (1 δ) 6 7 = 1 2 . E.4 Omitted Proofs (cid:16) Vπ(t ) (sroot) ϵ (cid:18) (cid:17) Pr Vπ((cid:101)t ) (sroot) Vπ(t ) (sroot) (cid:19) 6 Lemma E.3. Consider Algorithm 2. For any node SH, we have t=1 (cid:18) 1 η (cid:68) (cid:101)A(t1)(s, ), π( s) π(t1)( s) (cid:69) (cid:19) + βT KL (cid:16) π( s)πθθθref ( s) (cid:17) + 2ηT. Proof. We will introduce the following one-step analysis of the update rule first. Lemma E.6. For any node SH and [T], we have (cid:68) η KL (cid:101)A(t1)(s, ), π( s) π(t)( s) (cid:16) (cid:16) π( s)π(t1)( s) (cid:17) + ηβKL (cid:16) π( s)πθθθref ( s) . KL (cid:17) π( s)π(t)( s) The proof is presented later in this section. Therefore, (cid:68) η KL (cid:101)A(t1)(s, ), π( s) π(t)( s) (cid:16) (cid:16) π( s)π(t1)( s) (cid:17) + ηβKL (cid:16) π( s)πθθθref ( s) . KL (cid:17) π( s)π(t)( s) (cid:69) (cid:69) 26 (cid:17) (cid:16) KL π(t)( s)π(t1)( s) (cid:17) (cid:17) (cid:16) KL π(t)( s)π(t1)( s) (cid:17) By adding η (cid:68) (cid:101)A(t1)(s, ), π(t)( s) π(t1)( s) (cid:69) on both sides, we have (cid:101)A(t1)(s, ), π( s) π(t1)( s) (cid:69) (cid:68) η (i) KL (cid:16) π( s)π(t1)( s) (cid:68) (cid:17) (cid:16) KL π( s)π(t)( s) (cid:16) (cid:69) (cid:17) (cid:16) KL π(t)( s)π(t1)( s) (cid:17) (cid:17) + η (cid:101)A(t1)(s, ), π(t)( s) π(t1)( s) + ηβKL π( s)πθθθref ( s) . By Hölders inequality, we have (cid:68) (cid:69) (cid:101)A(t1)(s, ), π(t)( s) π(t1)( s) (cid:13) (cid:13) (cid:13)π(t)( s) π(t1)( s) (cid:13) (cid:101)A(t1)(s, ) (cid:13) (cid:13) (cid:13) (cid:13) (cid:13)1 (cid:13) (cid:13) (cid:13) (cid:13) (cid:13) (cid:13) (cid:101)A(t1)(s, ) (cid:13) (cid:13) (cid:13) 2η 2 + 1 8η (cid:13) (cid:13)π(t)( s) π(t1)( s) (cid:13) (cid:13) (cid:13) (cid:13) 2 1 (i) 2η + (cid:16) KL 1 4η π(t)( s)π(t1)( s) (cid:17) . (cid:13) (cid:13) (cid:13) 1 and Pinskers inequality. Therefore, (i) uses (cid:13) (cid:13) (cid:101)A(t1)(s, ) (cid:13) (cid:68) η KL (cid:101)A(t1)(s, ), π( s) π(t1)( s) (cid:16) (cid:17) (cid:16) π( s)π(t1)( s) KL (cid:69) π( s)π(t)( s) (cid:17) + 2η2 + ηβKL (cid:16) π( s)πθθθref ( s) (cid:17) . By telescoping, we have (cid:68) (cid:101)A(t1)(s, ), π( s) π(t1)( s) (cid:69) η t=1 (cid:16) KL π( s)π(0)( s) (i) KL (cid:16) π( s)π(0)( s) (cid:17) (cid:17) (cid:16) KL π( s)π(T)( s) (cid:17) + 2η2T + ηβKL (cid:16) π( s)πθθθref ( s) (cid:17) + 2η2T + ηβKL (cid:16) π( s)πθθθref ( s) (cid:17) T. (i) uses the non-negativity of KL-divergence. By dividing η on both sides, we have t=1 1 η 1 η KL KL (i) = (cid:68) (cid:101)A(t1)(s, ), π( s) π(t1)( s) (cid:69) (cid:16) (cid:16) π( s)π(0)( s) (cid:17) + 2ηT + βKL (cid:16) π( s)πθθθref ( s) (cid:17) π( s)πθθθref ( s) (cid:17) + 2ηT + βKL (cid:16) π( s)πθθθref ( s) (cid:17) T. (i) is because π(0)( s) = πθθθref ( s) by the initialization of Algorithm 2. Lemma E.4. For any sequence of policies π(1), π(2), , π(T) and policy π, we have (cid:16) t= Vπ(sroot) Vπ(t) (sroot) (cid:17) = µπ(s) sSSH (cid:68) Qπ(t) t=1 (s, ), π( s) π(t)( s) (cid:69) . 27 Proof. The lemma can be proved by induction. When = 1, Lemma E.4 holds since Qπ(t) (T (sroot, a)) = Qπ(sroot, a) for any action [B] and [T]. Therefore, (sroot, a) = µπ(s) (cid:68) Qπ(t) (s, ), π( s) π(t)( s) (cid:69) t=1 µπ(sroot) (cid:16) Qπ(sroot, ), π( sroot) (cid:68) Qπ(t))(sroot, ), π(t)( sroot) (cid:69)(cid:17) sSSH t=1 t=1 (cid:16) = = Vπ(sroot) Vπ(t) (sroot) (cid:17) . For any two nodes s, s, we write if is an ancestor of in the search tree. Consider when Lemma E.4 holds for any search tree of height H0. Then, for = H0 + 1, we have µπ(s) (cid:68) Qπ(t) (s, ), π( s) π(t)( s) (cid:69) t= = µπ(sroot) (cid:68) Qπ(t) (sroot, ), π( sroot) π(t)( sroot) (cid:69) sSSH t=1 + a=1 sSSH : (sroot,a)s t=1 µπ(s) (cid:68) Qπ(t) (s, ), π( s) π(t)( s) (cid:69) . Then, according to the induction hypothesis, for any [B], since the subtree rooted at (sroot, a) is tree of height H0, we have sSSH : (sroot,a)s t=1 µπ(s) (cid:68) Qπ(t) (s, ), π( s) π(t)( s) (cid:69) =π(a sroot) (cid:16) t=1 Vπ(T (sroot, a)) Vπ(t) (T (sroot, a)) (cid:17) . Moreover, by definition, we have Qπ(t) (sroot, a) = Vπ(t) (T (sroot, a)). Therefore, µπ(s) (cid:68) Qπ(t) (s, ), π( s) π(t)( s) (cid:69) t=1 µπ(sroot) (cid:68) Qπ(t) (sroot, ), π( sroot) π(t)( sroot) (cid:69) π(a sroot) Vπ(T (sroot, a)) Vπ(t) (T (sroot, a)) (cid:17) (cid:16) t=1 (cid:16) π(a sroot) π(t)(a sroot) (cid:17) Vπ(t) (T (sroot, a)) sSSH t=1 a=1 a=1 + t=1 = = + Vπ(sroot)T a=1 π(a sroot) t=1 Vπ(t) (T (sroot, a)) = (cid:16) t= Vπ(sroot) Vπ(t) (sroot) (cid:17) . 28 Therefore, Lemma E.4 also holds when = H0 + 1 and thus we can conclude the proof. Lemma E.6. For any node SH and [T], we have (cid:68) η KL (cid:101)A(t1)(s, ), π( s) π(t)( s) (cid:16) (cid:16) π( s)π(t1)( s) (cid:17) (cid:69) + ηβKL (cid:16) π( s)πθθθref ( s) . KL (cid:17) π( s)π(t)( s) (cid:17) (cid:16) KL π(t)( s)π(t1)( s) (cid:17) Proof. Let be the height of s. There are three possibilities on π( s)J (t1): (I) (cid:101)A(t1)(s, ) + β log π(t1)( s) β log πθθθref ( s) + β1; (II) one-hot vector with only index ; (III) 0. be β π(t1)( s) Then, we will show that (D.2) is equivalent to the following in different cases. Lemma E.7. For any {1, 2, , T}, {0, 1, , 1}, and node Sh, (D.2) is equivalent to the following, π(t)( s) = argmin π( s)B (cid:68) (cid:101)A(t1)(s, ), π( s) (cid:69) (cid:16) + βKL π( s)πθθθref ( s) (cid:17) + (cid:16) KL π( s)π(t1)( s) 1 η π( s)J (t1), π( s) (cid:17) π(t)( s) = argmin π( s)B (cid:68) (cid:69) + (cid:16) KL 1 η π( s)π(t1)( s) (cid:17) , (I) (II, III) where (I), (II), (III) stand for the cases when π( s)J (t1) = (cid:101)A(t1)(s, ) + β log π(t1)( s) β log πθθθref ( s) + β1 one-hot vector with only index be β π(t1)( s) 0. (I) (II) (III) Then, we will introduce special case of Lemma 3.0.3 from Liu (2025). Lemma E.8. For any node s, vector RB, η > 0, β0 0, policy x(0) B, and reference policy xref B, let x(1) = argmin xB (cid:26) g, + β0KL (cid:16) xxref(cid:17) + 1 η (cid:16) xx(0)(cid:17)(cid:27) . KL Then, for any x(2) B, we have (cid:16) ηβ0KL (cid:16) KL x(2)x(0)(cid:17) x(1)xref(cid:17) (cid:16) ηβ0KL (1 + ηβ0)KL x(2)xref(cid:17) (cid:16) + η x(2)x(1)(cid:17) (cid:68) g, x(1) x(2)(cid:69) x(1)x(0)(cid:17) (cid:16) KL (E.7) . Consider (I) first. For any node SH and [T], by taking x(2) = π( s), x(1) = π(t)( s), x(0) = π(t1)( s), xref = πθθθref ( s), = (cid:101)A(t1)(s, ) and β0 = β, we have (cid:17) (cid:17) (cid:16) (cid:16) π(t)( s)πθθθref ( s) ηβKL π( s)πθθθref ( s) ηβKL (cid:68) + η (cid:16) KL (cid:101)A(t1)(s, ), π( s) π(t)( s) (cid:69) π( s)π(t1)( s) (cid:17) (1 + ηβ)KL (cid:16) π( s)π(t)( s) (cid:17) (cid:16) KL π(t)( s)π(t1)( s) (cid:17) . 29 Further, by the non-negativity of KL-divergence, we have (cid:68) η KL (cid:101)A(t1)(s, ), π( s) π(t)( s) (cid:16) (cid:16) π( s)π(t1)( s) (cid:17) (cid:69) + ηβKL (cid:16) π( s)πθθθref ( s) . KL (cid:17) π( s)π(t)( s) (cid:17) (cid:16) KL π(t)( s)π(t1)( s) (cid:17) Consider (II). For any node SH and [T], by taking x(2) = π( s), x(1) = π(t)( s), x(0) = π(t1)( s), xref = πθθθref ( s), = π( s)J (t1) and β0 = 0 in Lemma E.8, we have π( s)J (t1), π( s) π(t)( s) (cid:16) π( s)π(t1)( s) KL (cid:17) (cid:16) (cid:69) π( s)π(t)( s) (cid:17) (cid:16) KL π(t)( s)π(t1)( s) (cid:17) . (cid:68) η KL Moreover, (cid:68) π( s)J (t1), π( s) π(t)( s) (cid:69) =β π(a π(t1)(a h) π(t)(a h) s h) (i) 0 (cid:68) (ii) = (cid:101)A(t1)(s, ), π( s) π(t)( s) (cid:69) . (i) uses the fact that π(a (cid:68) (cid:101)A(t1)(s, ), π( s) π(t)( s) (cid:16) (cid:16) (cid:17) (cid:69) KL π( s)π(t1)( s) KL h) = 1 and (ii) uses (cid:101)A(t1)(s, ) = 0 by definition. Therefore, π( s)π(t)( s) (cid:17) (cid:16) KL π(t)( s)π(t1)( s) (cid:17) . (E.8) For (III), which is off the sampled trajectory at step 1, by definition we have (cid:101)A(t1)(s, ) = 0. Then, (cid:68) π( s)J (t1), π( s) π(t)( s) (cid:69) (cid:68) = 0 = (cid:101)A(t1)(s, ), π( s) π(t)( s) (cid:69) , and (E.8) also holds. Lemma E.5. For any {0, 1, , 1}, we have (cid:16) KL π( h)πθθθref ( h) (cid:17) log + 2 (cid:13) (cid:13) (cid:13)θθθref(cid:13) (cid:13) (cid:13) . Proof. For any {0} [H 1], since π is deterministic, let Then, be the action such that π(a h) = 1. (cid:16) KL π( h)πθθθref ( h) (cid:17) = a=1 π(a h) log π(a h) πθθθref (a h) = log 1 πθθθref (a h) . By definition, we have πθθθref Therefore, (a s h) = (cid:16) θref (cid:0)s h, exp a=1 exp (cid:0)θref (cid:0)s (cid:1)(cid:17) h, a(cid:1)(cid:1) (cid:16) (cid:13) (cid:13) (cid:13) (cid:13) (cid:13)θθθref exp (cid:13) exp (cid:0)(cid:13) (cid:13)θθθref(cid:13) (cid:13) (cid:17) (cid:16) exp (cid:1) = 2 (cid:13) (cid:13) (cid:13)θθθref (cid:17) (cid:13) (cid:13) (cid:13) . (cid:16) KL π( h)πθθθref ( h) (cid:17) log (cid:16) exp (cid:16) 2 30 (cid:13) (cid:13) (cid:13)θθθref(cid:13) (cid:13) (cid:13) (cid:17)(cid:17) = log + 2 (cid:13) (cid:13) (cid:13)θθθref(cid:13) (cid:13) (cid:13) . Lemma E.7. For any {1, 2, , T}, {0, 1, , 1}, and node Sh, (D.2) is equivalent to the following, π(t)( s) = argmin π( s)B (cid:68) (cid:101)A(t1)(s, ), π( s) (cid:69) (cid:16) + βKL π( s)πθθθref ( s) (cid:17) + (cid:16) KL π( s)π(t1)( s) 1 η π( s)J (t1), π( s) (cid:17) π(t)( s) = argmin π( s)B (cid:68) (cid:69) + (cid:16) KL 1 η π( s)π(t1)( s) (cid:17) , (I) (II, III) where (I), (II), (III) stand for the cases when π( s)J (t1) = (cid:101)A(t1)(s, ) + β log π(t1)( s) β log πθθθref ( s) + β1 one-hot vector with only index be β π(t1)( s) 0. (I) (II) (III) Proof. The Lagrangian of (cid:68) (cid:16) π(t)( s) (cid:69) (cid:16) + βKL (cid:101)A(t1)(s, ), π( s) (cid:17) (cid:68) := (cid:101)A(t1)(s, ), π(t)( s) xxref(cid:17) (cid:16) + βKL (cid:69) + 1 η KL (cid:16) π( s)π(t1)( s) (cid:17) is π(t)( s)πθθθref ( s) (cid:17) (cid:33) (cid:16) KL + 1 η π(t)( s)π(t1)( s) (cid:17) + λ (cid:32) a=1 π(t)(a s) 1 . For any action [B], by setting = 0, we have (cid:101)A(t1)(s, ) + β log which implies that π(t)(a s) = exp (cid:33) + β + 1 η log (cid:32) π(t)(a s) π(t1)(a s) (cid:33) + 1 η + λ = 0, ηβ1ηλ+η (cid:101)A(t1)(s,)+ηβ log (cid:16) πθθθref (a s) (cid:17) +log(π(t1)(a s)) (cid:33) . 1+ηβ L(π(t)( s)) π(t)(a s) (cid:32) π(t)(a s) πθθθref (a s) (cid:32) By further setting L(π(t)( s)) λ = 0, we have Therefore, by combining all pieces together, we have a=1 π(t)(a s) = 1. π(t)(a s) (i) = exp exp η (cid:101)A(t1)(s, ) + ηβ log η (cid:101)A(t1)(s, ) + ηβ log (cid:16) (cid:16) (cid:17) (cid:17) πθθθref (a s) 1 + ηβ πθθθref (a s) 1 + ηβ (cid:16) (cid:16) + log + log π(t1)(a s) (cid:17) /Z π(t1)(a s) (cid:17) exp (cid:18) η 1 + ηβ (cid:101)A(t1)(s, ) + ηβ 1 + ηβ θref(s, a) + 1 1 + ηβ θ(t1)(s, a) (cid:19) . In (i), = a=1 exp (cid:32) η (cid:101)A(t1)(s,)+ηβ log (cid:16) πθθθref (a s) (cid:17) +log(π(t1)(a s)) 1+ηβ (cid:33) . For (II), (III), the proof can be concluded by setting β = 0 and changing (cid:101)A(t1)(s, ) to π( s)J (t1)."
        }
    ],
    "affiliations": [
        "LIDS, EECS, Massachusetts Institute of Technology"
    ]
}