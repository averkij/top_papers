{
    "paper_title": "The Dragon Hatchling: The Missing Link between the Transformer and Models of the Brain",
    "authors": [
        "Adrian Kosowski",
        "Przemysław Uznański",
        "Jan Chorowski",
        "Zuzanna Stamirowska",
        "Michał Bartoszkiewicz"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "The relationship between computing systems and the brain has served as motivation for pioneering theoreticians since John von Neumann and Alan Turing. Uniform, scale-free biological networks, such as the brain, have powerful properties, including generalizing over time, which is the main barrier for Machine Learning on the path to Universal Reasoning Models. We introduce `Dragon Hatchling' (BDH), a new Large Language Model architecture based on a scale-free biologically inspired network of \\$n\\$ locally-interacting neuron particles. BDH couples strong theoretical foundations and inherent interpretability without sacrificing Transformer-like performance. BDH is a practical, performant state-of-the-art attention-based state space sequence learning architecture. In addition to being a graph model, BDH admits a GPU-friendly formulation. It exhibits Transformer-like scaling laws: empirically BDH rivals GPT2 performance on language and translation tasks, at the same number of parameters (10M to 1B), for the same training data. BDH can be represented as a brain model. The working memory of BDH during inference entirely relies on synaptic plasticity with Hebbian learning using spiking neurons. We confirm empirically that specific, individual synapses strengthen connection whenever BDH hears or reasons about a specific concept while processing language inputs. The neuron interaction network of BDH is a graph of high modularity with heavy-tailed degree distribution. The BDH model is biologically plausible, explaining one possible mechanism which human neurons could use to achieve speech. BDH is designed for interpretability. Activation vectors of BDH are sparse and positive. We demonstrate monosemanticity in BDH on language tasks. Interpretability of state, which goes beyond interpretability of neurons and model parameters, is an inherent feature of the BDH architecture."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 0 3 ] . [ 1 7 0 5 6 2 . 9 0 5 2 : r THE DRAGON HATCHLING: THE MISSING LINK BETWEEN THE TRANSFORMER AND MODELS OF THE BRAIN Adrian Kosowski Przemysław Uzna nski Jan Chorowski Zuzanna Stamirowska Michał Bartoszkiewicz Pathway, Palo Alto, USA research@pathway.com"
        },
        {
            "title": "ABSTRACT",
            "content": "The relationship between computing systems and the brain has served as motivation for pioneering theoreticians since John von Neumann and Alan Turing. Uniform, scale-free biological networks, such as the brain, have powerful properties, including generalizing over time, which is the main barrier for Machine Learning on the path to Universal Reasoning Models. We introduce Dragon Hatchling (BDH), new Large Language Model architecture based on scale-free biologically inspired network of locally-interacting neuron particles. BDH couples strong theoretical foundations and inherent interpretability without sacrificing Transformer-like performance. BDH is practical, performant state-of-the-art attention-based state space sequence learning architecture. In addition to being graph model, BDH admits GPU-friendly formulation. It exhibits Transformer-like scaling laws: we find empirically that BDH rivals GPT2-architecture Transformer performance on language and translation tasks, at the same number of parameters (10M to 1B), for the same training data. BDH provides theoretical foundations for understanding model behavior in the limit of large size and reasoning time. Our results, formalized as chain of reductions of expressiveness in the framework of computational Complexity Theory and Distributed Computing, and combined with findings on the BDH model, show macro-to-micro correspondence of function between the general attention mechanisms in state-of-the-art Language Models, and attention mechanisms observed in the brain. These attention mechanisms formally converge as closed-form local graph dynamics at neurons and synapses: the equations of reasoning. BDH can be represented as brain model. It contains neurons, organized as an excitatory circuit and an inhibitory circuit with integrate-and-fire thresholding of input signals at neurons. The working memory of BDH during inference entirely relies on synaptic plasticity with Hebbian learning using spiking neurons, at potentiation scales of minutes for the brain (up to hundreds of tokens). We confirm empirically that specific, individual synapses strengthen connection whenever BDH hears or reasons about specific concept while processing language inputs. The neuron interaction network of BDH is graph of high modularity with heavy-tailed degree distribution. The BDH model is biologically plausible, explaining one possible mechanism which human neurons could use to achieve speech. BDH is designed for interpretability. Activation vectors of BDH are sparse and positive. We demonstrate monosemanticity in BDH on language tasks, including representation of concept abstractions, which happens even for small models, below 100M-parameter scale. Interpretability of state, which goes beyond interpretability of neurons and model parameters, is an inherent feature of the BDH architecture. We believe BDH opens the door to new theory of Thermodynamic Limit behavior for language and reasoning models, with the ultimate goal of Probably Approximately Correct (PAC)-like bounds for generalization of reasoning over time. Technical blog entry: https://pathway.com/research/bdh. Code listings: https://github.com/pathwaycom/bdh. Author contributions are listed at the end of the paper. Corresponding author."
        },
        {
            "title": "Contents",
            "content": "1 Introduction"
        },
        {
            "title": "1.1 Motivation .",
            "content": ". . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 1.2 Intuition of results: combining modus ponens reasoning with Hebbian learning . . . . . . . . . . . ."
        },
        {
            "title": "1.4 Notation .",
            "content": ". . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 2 BDH: language model architecture given by local distributed graph dynamics"
        },
        {
            "title": "2.2 Definition of BDH as a local edge-reweighting process (equations of reasoning) . . . . . . . . . . . .",
            "content": "2.3 2.4 Interpretation of attention as micro-inductive bias of reasoning . . . . . . . . . . . . . . . . . . . . Interpretation of BDH as an oscillator network toy-model . . . . . . . . . . . . . . . . . . . . . . . . 2.5 Expressing BDH using brain models . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 3 BDH-GPU: tensor-friendly version of the BDH architecture 3.1 Notation for BDH-GPU . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 3.2 Definition of BDH-GPU as state-space system . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 3.3 Interpretation of BDH-GPU as local interacting particle system . . . . . . . . . . . . . . . . . . . . 3.4 Expressing BDH-GPU using BDH: preserving parameter and state size . . . . . . . . . . . . . . . . 4 Implementation and scaling laws 4.1 Implementation characteristics of BDH-GPU . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 4.2 Comparison of BDH-GPU to GPT2-like Transformers . . . . . . . . . . . . . . . . . . . . . . . . . . 4.3 Comparison of BDH-GPU to other sequence processing architectures . . . . . . . . . . . . . . . . . 5 Analysis: emergence of modularity and scale-free structure 5.1 Background: modularity and scale-free property of systems . . . . . . . . . . . . . . . . . . . . . . . 5.2 BDH-GPU feed-forward network with the ReLU-lowrank block . . . . . . . . . . . . . . . . . . . 5.3 ReLU-lowrank as signal propagation dynamics . . . . . . . . . . . . . . . . . . . . . . . . . . . . 5.4 Modularity in BDH-GPU signal propagation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 5.5 Empirical findings: parameter distribution in ReLU-lowrank matrix products . . . . . . . . . . . . . 6 Analysis: linear attention, sparse positive activation, and monosemanticity 6.1 Macro-expressiveness of attention in BDH-GPU . . . . . . . . . . . . . . . . . . . . . . . . . . . . 6.2 Micro-interpretation of attention in BDH-GPU . . . . . . . . . . . . . . . . . . . . . . . . . . . . 6.3 Empirical findings: monosemantic synapses . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 6.4 Empirical findings: sparse neuron activations . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 7 Playing with the Hatchling 7.1 Model merging: concatenating two models . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 7.2 Training without backpropagation through time . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 2 3 5 7 9 10 12 14 14 16 17 18 20 21 23 23 24 26 26 27 28 31 34 34 37 37 41 41 42 8 Conclusions"
        },
        {
            "title": "8.1 Takeaways for model engineering . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .",
            "content": "8.2 Implications for brain science ."
        },
        {
            "title": "8.3 Societal impact",
            "content": ". . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . Connection between generalization of reasoning and computational expressiveness Further description of experiments Omitted formal claims and proofs Desirable properties of local graph dynamics for language models BDH-GPU PyTorch code listing"
        },
        {
            "title": "Introduction",
            "content": "43 43 43 44 53 55 58 61 Long reasoning and long context inference pose severe challenge of generalization across scales of time. From vibe coding to market research, users of Language Models and agentic systems are increasingly relying on defining tasks through informal prompts, which the language model is expected to follow over long sequences of actions or decisions, like reasonable human actor would. Implicitly, most users expect machines to follow the generalization patterns of human reasoning, i.e., to generalize reasoning in the same way as humans do. The complexity of tasks attempted in this way has gone from the equivalent of hours of human work for single prompt, to weeks (Emberson et al., 2025). However, experimental evidence suggests that the Transformer and other state-of-the-art architectures do not systematically generalize chain-of-thought (CoT) reasoning to scenarios longer than the ones seen during training (Shojaee et al., 2025). Chain-of-Thought reasoning models can be considered through the lens of computational complexity theory. For Language Model to generalize human reasoning on given class of tasks, we expect this model to be able to emulate the corresponding reasoning function of the human brain efficiently.3 While the Transformer with Chain-of-Thought is Turing-complete and can efficiently emulate certain restricted classes of formal languages (Merrill and Sabharwal, 2024), this does not in itself provide satisfactory answer as to how it emulates human reasoning. The human brain is an extremely complex graph-based distributed computing system with 8 1010 neurons, and > 1014 neuron connections (synapses), of which certain percentage is actively used. The direct simulation of such distributed system by Language Model through generic Turing-machine reductions would require billions of CoT tokens of the Language Model to represent single step of reasoning in the brain. So, do Transformer-like models actually relate to brain function? Such relationship should follow more closely from tighter, more direct simulation. Finding such connection between Language Models and human brain function has, so far, proved elusive. Indeed, when comparing tensorbased Language Model based on feed-forward network blocks and attention, to uniform, scale-free graph-based distributed system, such as the brain, the two may, at first glance, appear very dissimilar. This apparent dissimilarity of structure between Language Models and brain structure has been one of the main causes of concern in attempts to reconcile Computation and the Brain (Olshausen, 2018), as well as cause of concern regarding the difficulty to foresee the behavior of autonomous AI systems. In this paper, we show the link between the Transformer and Brain models. 1.1 Motivation The development of Artificial Intelligence and the understanding of Neural Science have gone hand in hand since the 1940s, both being efforts to understand the mystery of intelligence. The relationship between computing systems and the brain served as motivation for the pioneering theoreticians such as John von Neumann (1958), Alan Turing (1950), Goeff Hinton (2005), Warren McCulloch and Walter Pitts (1943), and Horace Barlow (1972). 3We provide more formal explanation of this point in Appendix A. Since then, milestones in Machine Learning around Artificial Neural Networks using backpropagation with SGD (Rumelhart et al., 1986), followed by Deep Learning (LeCun et al., 2015), and the Attention mechanism (Bahdanau et al., 2015; Vaswani et al., 2017) have split the mystery of how intelligence works into two. First, we still have no clear explanation for the micro-to-macro correspondence of the reasoning function of the brain. Second, we do not understand the correspondence between the artificial and natural systems notably, how effects observed in the brain (emergent network; sparse activations; oscillatory phenomena; unknown relationship to backpropagation mechanisms) map into those which appear in systems based on dense tensors, trained using gradient back-propagation over time. Reconciling Reasoning Function of the Brain with Language Models. There is seemingly deep divide between state-of-the-art language models, like the Transformer, and natural distributed systems with local graph dynamics, like those of the brain. Specifically, for the brain, we do not understand how the reasoning function emerges from neuronal dynamics at the microscale. For the Transformer, the interpretation of function is given at the level of vectors, but not at the level of particle dynamics or uniform distributed computing system. Language and reasoning are the key areas of higher-order brain function for which we do not yet have complete understanding. Many other areas of brain function have been explained through analogies to Machine Learning architectures. For example, the visual cortex is becoming well-understood, especially in its peripheral layers, and the observed inference dynamics are shown to have correspondence to known Deep Learning architectures (Mohsenzadeh et al., 2020). The use of sparse coding by the brain was considered in the context of processing visual cues (Olshausen and Field, 1997), as well as for the olfactory systems (Lin et al., 2014). By contrast, higher-order cognitive functions of the association cortex of the human brain, such as language and reasoning, are among the least understood. number of models provide partial explanations and have been verified at small scales. Some of the first attempts include explaining context-dependent computation in the prefrontal cortex using population dynamics of an RNN (Mante et al., 2013). Later approaches include the Tolman-Eichenbaum Machine (Whittington et al., 2020, 2022), as well as number of more recent works (Papadimitriou et al., 2020; Dabagia et al., 2024; Mitropolsky and Papadimitriou, 2025). One of the main stumbling blocks concerns going from spiking activation patterns at neurons, and localized attention effects at synapses, to higher-order function, serving reasoning purpose, efficiently organized at scale of millions to billions of neurons. Conversely, for Language Models architectures such as the Transformer, we miss compact micro-interpretation as distributed system. The expressiveness of the Transformer has been approximated using approaches from centralized computing and Complexity Theory, rather than from distributed systems. In the centralized perspective, language model can be seen as transformation function from inputs into outputs. The computational expressiveness of the Transformer architecture may then be approximated through frameworks based on RASP, such as RASP-L (Zhou et al., 2024) or C-RASP (Yang and Chiang, 2024; Huang et al., 2025). RASP-L provides very convenient heuristic for estimating Transformer expressiveness at the rather coarse level of vector operations, while C-RASP provides more specialized lower-bound on expressiveness, capturing class of formulas of temporal counting logic. Both frameworks have been used to suggest theoretical models of task length generalization. This type of expressiveness techniques, however, do not lead to uniform asymptotic model for the behavior of the Transformer, whether in GPT2 architecture or simplified. The scaling of the Transformer in its different dimensions, and the need to manipulate context length, complicate this goal. The lack of such uniform model also makes it hard to compare the capabilities of the Transformer to the capabilities of the brain at the level of correspondence of structure. Generally, the temporal behavior of state-space system is reflected in its structure4. Understanding whether it is possible to show alignment of the temporal behavior of two systems, which do not display any structural correspondence, and without clear idea of how the weight tensors and state representation of one system embed into the graph structure and state representation of the other system, is an awkward task. This brings us naturally to our motivational objective: Can we create Machine Learning models which are closer to the desirable properties of natural (human) reasoning systems, and which exhibit the same types of limit and scaling behavior as such natural systems? Towards scale-free foreseeable AI. Ensuring correct scaling behavior of inference over time is of paramount importance for the deployment of AI whose reasoning or actions are not subject to strict human supervision. Most reasoning models and AI agentic systems admit limit objects (i.e., extensions to infinite time and infinite size) which are Turing-complete (cf. e.g. (Merrill and Sabharwal, 2024; Pérez et al., 2021; Jojic et al., 2023)). This means that 4For linear system, temporal behavior would be direct consequence of the spectral properties of the system. The considered systems dynamics are not linear. 4 they should be treated like computer programs and should be approached by the users with the same standards of care, as computer program of unknown origin and unknown purpose. An AI model can malfunction when allowed to run for long time autonomously, i.e., without human validation of actions and reasoning outcomes. The most painful of all consequences, perhaps, is the concept of failed generalization of reasoning (a malfunction with respect to the original task objective) over time, leading to grotesque effect known as the Paperclip Factory (Bostrom, 2014). Can the risk of such unsuccessful generalization be bounded? There are at least two scenarios in which black-box model cannot be considered to have undergone previous empirical validation, and consequently cannot be used in higher-risk autonomous AI use cases. 1. Length-generalization scenario: Model is expected to act autonomously on task which is longer than tasks forming part of its validation set. 2. Model scaling scenario: Model is not exactly the same closed system as the one which was tested during validation. For example, suppose that models M1 and M2 were tested individually on smaller tasks, and let be an agentic system composed of instances of M1 and M2 which communicate with exchange messages with each other during inference. natural way of avoiding both difficulties consists in studying systems which are scale-free with respect to size and time, and admit form of uniform thermodynamic limit behavior. The limit behavior of computational systems at criticality naturally connects the size of the system with the probable duration of its operation, with the connection usually taking polynomial form (cf. e.g. (Björner et al., 1991; Cairns, 2018; Rolla, 2020) for examples of graph-based interacting particle systems for which rigorous results have been obtained in this direction). Consider model Mn with architecture A, parameterized by its size (with the interpretation of the number of uniform particles), and sampled from some space of n-neuron models in architecture in some space equipped with probability measure, Mn PA(n). Informally, if the limit object PA := limn PA(n) exists (under an appropriate, well-defined sense of uniformity of limit) then models Mn, for sufficiently large, will admit in the limit asymptotic properties, which can be used to characterize their behavior over time. The existence of such limit theory means that we can characterize, with bounded probability of error, the behavior of family of large models, having O(n) parameters, while relying on theory which is independent of the specific structure and size of the specific model. In this way, the limit behavior of system of very large number of interacting uniform particles over time becomes (stochastically) foreseeable in the sense of its adherence to expected behavior, which can be extrapolated from observations at shorter time scales. Thus, small tests may be conceived in order to provide validation for scale-free system at long time scales. Introducing Axiomatic AI. Axiomatic systems are those in which micro-foundations and the macro-description which arises from them are consistent and well-understood. The need for axiomatic understanding was highlighted by David Hilbert (1902), and has become the foundation in Statistical Physics (e.g. thermodynamics, fluid dynamics, spin glass theory), cellular mechanisms, Social Networks Science, and reconciliation of Microeconomics and Macroeconomics through Network Economics perspective. This paper brings micro-foundational understanding to Language Model inference, to the mechanisms of in-context learning, and Chain-of-Thought reasoning dynamics. The considerations in this work naturally support shift of perspective from Interpretable AI, which gives an approximate understanding of what the model is doing now (without necessarily telling us what its current actions are going to lead to over longer time scales), to Axiomatic AI, where we also understand the micro-foundations of how the model can be expected to behave subsequently over time. 1. Intuition of results: combining modus ponens reasoning with Hebbian learning In this section we provide the reader with some of the main intuitions behind this work which, we hope, will help to navigate the remaining, more formal parts of this paper with ease. While there are many formal deductive systems in logic, they predominantly rely on the modus ponens inference rule. Applied to rule-based reasoning system, it takes the following form: If we know that the i-th fact is true, and our ruleset σ indicates that the i-th fact implies the j-th fact, then we know that the j-th fact is true as well. In an approximate reasoning system, the strength of the rule σ(i, j) indicates how the 5 belief X(i) of the system affects its belief A(j). We could write: X(i), σ(i, j) A(j), (1) to indicate that if X(i) is weighted belief, it contributes X(i)σ(i, j) to the systems belief A(j). Practical logical inference systems differ in strategies employed for rule selection, with the most advanced ones allowing direct manipulation of the ruleset, effectively resulting in form of program evolution during inference5. For an approximate reasoning system, such heuristic could manipulate the strength of rules, modulating the impact of belief X(i) on the systems belief A(j). Hebbian learning (Hebb, 1949), often presented as the mnemonic Neurons that fire together wire together, can be seen as heuristic for ruleset manipulation. It postulates that synaptic connections are strengthened when the activity of one neuron, (i), led to the firing of another neuron, X(j). In the context of an adaptive, approximate inference system, the Hebbian heuristic means that if during the course of operation fact contributed some evidence for j, the system increases the significance of the implication σ(i, j). We could write this rule as: (i), X(j) σ(i, j), (2) with the interpretation that co-presence (or spike) of (i) followed by X(j) increases σ(i, j) by (i)X(j). The relations (1) and (2), over set of facts, may form the basis of simple approximate reasoning system that adapts its operation to the problem at hand. Starting with some initial connections between facts, the system applies the rules to discover new facts, at the same time reweighting the ruleset in way that strengthens the connections between the initial and derived facts. Effectively, should the system be rerun with the new ruleset, it would arrive at similar conclusions faster. Suppose now that the reasoning system is equipped with two sets of rules: fixed set and an evolving set σ. From machine learning perspective, the fixed ruleset can be seen as model weights in Deep Learning terminology, learned using e.g. error backpropagation on training set. On the other hand, the evolving ruleset can be seen as the temporal state of the reasoning system, sometimes called fast weights (Hinton and Plaut, 1987; Schmidhuber, 1993; Ba et al., 2016a). Fast-weights systems have favorable ratio of state size to parameter count. system with facts has = O(n2) trainable parameters (expressed using one or more matrices). classical recurrent neural net, such as the LSTM (Hochreiter and Schmidhuber, 1997), treats individual fact (neuron) activations as its state, thus maintaining only O(n) state variables. On the other hand, the evolving set of fast-weights σ has = O(n2) state entries. We believe this 1-1 ratio of trainable parameter to state size is important in designing practical reasoning systems and may justify the success of the Transformer (Vaswani et al., 2017) and state-space (Gu and Dao, 2024) sequence processing models. Now, bearing in mind that the trainable parameters and state have comparable size m, we can adjust the ratio between this value and the size of the fact base. This will happen through choice of sparsity for the nn matrices carrying parameters and state, resulting in specific relationship of the two values, n2. In this way, our system gets natural interpretation in terms of graphs on nodes and edges, with the graph edges tasked with their first two roles: carrying state, and, carrying trainable parameters. Finally, we will give our system an interpretation of dynamical system with distributed (localized) dynamics, and we will task our edges with their third crucial role: mediating in communication between nodes of the system. In this way, through assimilation of edges to natural function in the brain, we will refer to the edges as synapses connecting set of neurons into distributed graph-based system. In the following Section 2, we will introduce BDH, reasoning system that formalizes and combines relations (1) and (2) with dynamics involving fixed rules. The BDH system: 1. is reasoning system, efficiently using the modus ponens reasoning rule with heuristic rule reweighting, based on (1) and (2), 2. can be implemented with local graph dynamics, making it suitable for brain-like execution model, and amenable to principled, axiomatic description, 3. contains set of fixed connections (parameters), and set of dynamically adjusted connections (σ), which can be seen as its dynamic state updated with Hebbian learning rule, 4. admits as its special case BDH-GPU, GPU-efficient reasoning model architecture, introduced in Section 3 and experimentally validated at scale in Section 7 in direct comparison to state-of-the-art GPT2-like Transformers. 5The authors personal experience with writing efficient Prolog programs confirms that such direct ruleset management is often necessary pragmatic evil, guiding the inference system in the right direction. 6 1.3 Contribution of this work The focus of this paper is in explaining the dynamics of the primary function of language and reasoning models: inference. We provide description of language model architecture which is directly comparable to the Transformer, and admits clear and interpretable local interpretation of its inference dynamics as programmable interacting particle system. Language Models as Local Graph Dynamics. In Section 2, we introduce graph-based model architecture called BDH, where all model parameters are represented as topology and weights of the communication graph, and model state during inference is represented as edge-reweighting applied to this graph topology. Claim 1 (informal overview of theoretical results for BDH). We introduce state-space Machine Learning architecture called BDH, formed by system of particles called neurons which communicate in way governed by the weights and topology of the system graph, representing communication by wire network. The inference dynamics of BDH, treated as distributed system, can be represented as execution of local rulesets for particles with programmable interactions, with particles acting as nodes of the interaction graph and scalar state variables located on its edges (cf. Section 2.2). The local kernel of BDH can be naturally expressed (emulated) by graph-based Spiking Neural Network system capable of Hebbian learning dynamics, an Excitatory circuit, and an Inhibitory circuit on an n-neuron system described by neuron interaction graph (cf. Section 2.5). In order to train BDH efficiently and analyze its performance, we restrict it, making this restriction the core of GPUfriendly architecture called BDH-GPU. This restriction is obtained by treating the communication of the particles as proceeding through mean-field (radio network), rather than graph (communication by wire), cf. Fig. 3 for an explanation of how the state-space equations of BDH-GPU are obtained from BDH. This allows us to train mathematically equivalent model, while localizing its state in short vectors at neurons, not at connections (synapses) of the system. tensor-friendly case of BDH: the BDH-GPU architecture. The BDH-GPU architecture, like the Transformer, crucially relies on an attention mechanism, and is amenable to token-parallel training on GPU for next token prediction tasks. Unlike the Transformer, activation vectors of BDH-GPU appear in very high dimension n, are positive by design, and turn out to be sparse. Claim 2 (informal overview of theoretical results for BDH-GPU). We introduce Machine Learning architecture called BDH-GPU, parameterized by single (very large) scaling parameter and second parameter d, log < (d = 256 in practice), such that: model in BDH-GPU (n, d) has (3 + o(1))nd parameters, and admits precise interpretation as statespace system following the local dynamics of n-particle system in an interaction field subject to equations of state (8). This system is described by O(d) parameters per particle, whose interaction field has mean field interpretation, which in computational view corresponds to particle communication network realized by means of noisy radio broadcast. BDH-GPU is special case of BDH in the sense that, for any BDH-GPU model with particles, there exists BDH model with particles with the same inference behavior and the same size O(nd) of trainable parameters, with the two models being formally equivalent up to placement of Layer Norms (cf. Claims 3 and 4). The BDH-GPU architecture relies on combination of two blocks: specific kind of ReLU-lowrank feedforward network, and linear attention mechanism, which both operate in the same neuron dimension n, using positive activation vectors. The mechanisms of BDH-GPU, considered at the macro-level of activation vectors in Rn, can be compared to those of the Transformer (cf. Section 6.1, Section 5.2). This justifies the applicability of the frameworks of approximate macro-expressiveness, based on RASP (Weiss et al., 2021; Zhou et al., 2024; Yang and Chiang, 2024) and designed for the Transformer, to BDH-GPU. The micro-interpretation of BDH-GPU mechanisms as neuron-neuron interaction dynamics: (1) explains mechanisms of in-cluster communication of neurons and the spontaneous emergence of graph structure with high Newman modularity in the neuron-neuron communication network (cf. Section 5), and (2) provides strict correspondence between the macro-mechanism of in-context inference based on attention and the local representation of state on individual neuron-neuron pairs (synapses) with state update dynamics based on sporadic updates to synaptic edge weight (cf. Section 6). The above results are complemented by empirical findings. Empirical Finding 1 (informal overview of empirical results of BDH-GPU). BDH-GPU is represented as tensorbased architecture and can be trained with standard back-propagation methods (cf. Section 3). The BDH-GPU architecture is shown to follow scaling laws (parameters vs. loss) of optimized Transformers in the GPT architecture, at parameter scales between 10M to 1B, on all next token prediction tasks we tested, including tasks of language and translation reminiscent of those in the original benchmark set for the Transformer architecture (cf. Section 4.2). An emergent network reflecting the associated BDH graph dynamics can be read out directly from the parameter matrices of trained BDH-GPU model, showing emergence of graph structure (cf. Section 5.5). The positive activations of BDH-GPU exhibit sparsity (at about 5% level) in the vectors of its state space dynamics, with sparsity levels reflecting the amount of activity being performed by BDH-GPU for given token (cf. Section 6.2). In-context state of BDH-GPU attention is shown to localize on the same synapses (neuron-neuron links) consistently across multiple prompts, allowing for some basic features, the interpretation of the current in-context state based on the reading of state of an individual synapse associated with such feature (cf. Section 6.3). more detailed discussion of the training approach is provided in Appendix B.2, while the code listing for BDH-GPU is provided in Appendix E. For the purposes of our experiments, we did not apply any specific training method which would be known to guide the system towards any of the observed emergent properties. (In particular, L1-regularization was disabled.) The observed emergent effects follow naturally from the design choices of the BDH and BDH-GPU architectures, and are largely attributable to the combination of: the choice of model dimensions with comparable model-to-state ratio, reliance on linear attention in high dimension, reliance on ReLU thresholds for ensuring that activation vectors are positive (trivially) and sparse (an effect empirically noted in (Haziza et al., 2025)). We also remark that the BDH-GPU architecture allows for the uniform asymptotic scaling of the model in one dimension, n. For example, composition of models, obtained by concatenation, is also model in the same architecture, with larger value of (cf. Section 7.1 for an empirical study of this effect for practical translation tasks). Historically, link has been established between infinitely wide feedforward networks and Gaussian Processes (Neal, 2012; Lee et al., 2017; Yang, 2019). BDH allows the study of limit behavior of reasoning models. With BDH and BDH-GPU, we show that Language Models can be amenable to particle-based interpretation. In fact, two micro-foundations particle-based behavior and logic-programming behavior of reasoning system fuse together in these architectures. The bridge between the Transformer and Brain models. The inference dynamics of BDH and BDH-GPU act as natural bridge between Transformer, and neuromorphic models of the brain and its subsystems. We illustrate this in Fig. 1. Implications for learning dynamics of natural lifelong inference systems. lifelong learning system progresses in time, performing extremely rapid inference, combined with several training mechanisms at different time scales. In this work, we provide and validate at scale plausible explanation of what the predominant dynamics of such system could look like, taking the system from split-second scale, to the scale of inference during minutes, considering the flow of time at the natural rate of thought and language for humans. complementary discussion of learning dynamics would aim to provide an explanation of how to take such lifelong inference system from the scale of minutes into even longer timescales. This would concern the slower transfer of fast-weights-like inference state to long-term memory, starting at the order of 103104 tokens, and taking into account feedback signals. In this work, we do not provide direct answer as to how the brain actually handles this effect at longer timescales. However, constructive way to resolve this problem seems to be less challenging, once the local inference dynamics of the brain are better understood (we come back to this in the Conclusions). The modeling approach provided in Section 2.5 is proposed as suitable framework for such study. 8 Figure 1: General overview of architectures and their relationships: the inference dynamics of BDH and BDH-GPU act as natural bridge between Transformer and models of the brain. The two main inference mechanisms of reasoning architecture, attention and the feed-forward network, are defined at macro-level through tensor operations for the Transformer, and at the micro-level of neuron interactions through local graph dynamics for Brain models. The new BDH-GPU architecture is naturally defined both at the level of vectors and of particle dynamics of neurons and synapses, acting as bridge between these two approaches. See also Table 3 at the end of the paper for more detailed comparison of architecture properties. 1.4 Notation State-space models. For describing inference dynamics of any system, we will use state-space notation, and consider state-space system composed of two parts: set of model parameters which does not change during inference, and state σ(t) which changes during inference. The model performs inference following state-space equation σ(t+1) := A(M, σ(t), at), where at is possible external input to the system at time (such as language token), = 0, 1, 2, . . ., and is referred to as the architecture which drives its progress. During inference without external input, usually autoregressive inference, we will shorten this to σ(t) := At(M, σ0). Models as programs. In settings that are of interest to us (inference with combining multiple facts, reasoning), we opt for terminology from computing. has the interpretation of computer program code, has the interpretation of computational machine architecture which runs it, and σ has the interpretation of the variable state of the program. We will use the terms model and program interchangeably. Graphs and their dynamical systems interpretation. For square matrix with non-negative coefficients, (R+)n,n, N, we will consider two more equivalent representations. In one, we will treat as graph defined on some nodeset , with = n. Formally, we can take = {e1, . . . , en}, where ei = (0, . . . , 0, 1, 0 . . . , 0) Rn1 with 1 on the i-th position, forming an orthonormal basis. Non-zero entries of are referred to as edges. By an Hei 0, to represent the node affinity function, or edge weight, overloading of notation, we will write H(i, j) := ej from to j. We define the edge set E(H) := {(i, j) : H(i, j) > 0}. In discussions of graph-based model architectures, we will take the standard interpretation of graphs from linear dynamical systems perspective, applied to positive vectors. When (R+)n1 is non-negative vector, Hv (R+)n has the interpretation of linear transformation of v. If satisfies the condition of stochasticity (column-normalization to 1), then (cid:55) Hv is Markov chain transition, with Hv1 = v1. From distributed systems perspective, transitions of stochastic matrices can be represented either through the direct simulation of (probabilities) of such Markov chain, or described by the token dynamics of an extremely simple stochastic token distribution scheme in which token located at node ei goes to node ej with probability H(i, j). If is not stochastic, the operation (cid:55) Hv 9 additionally necessitates the suppression of fraction of tokens, or the multiplication of tokens, at each step at each node, depending on the column-normalization of given node.6 For two graphs H1, H2 Rnn, the graph = H2H1 is obtained through (linear algebraic) matrix multiplication, and in distributed system, the corresponding transition (cid:55) Hv is obtained with two steps of token dynamics, one following graph H1, the next following graph H2. Representing edge-weights of sparse n-node graph with bits of numerical precision per parameter is possible with O(m(b+log n)) bits of information, which corresponds to O(m(1+ log )) parameters. For the sake of simplicity, we will assume in asymptotics that the second term of the sum does not dominate (i.e., log = O(b)), and so we simply say that we represent the graph with O(m) parameters."
        },
        {
            "title": "2 BDH: a language model architecture given by local distributed graph dynamics",
            "content": "2.1 Formalism for local graph-based language models We consider model architectures which correspond to models of graph-based distributed computing (cf. (Peleg, 2000; Hirvonen and Suomela, 2025)). specific model in architecture corresponds to the weights and topology of the communication graph or graphs used by such system. Introduction to distributed graph systems. The distributed system architecture A, representing the model architecture, is defined through scheduler, and local dynamics (kernel K(A)) describing the local computations to be performed at each node of the system, and, communication between pairs of nodes connected by edges of the graph representing given model . We will generally accept that computations are performed only at neuron nodes (particles), whereas state variables of the system may appear both on nodes and edges. We will, for simplicity of analysis, consider systems governed by synchronous scheduler, which in successive rounds, acts in two sub-rounds: 1. Computation: computations of the kernel of are run at all neuron nodes independently. 2. Communication over wire: each neuron node sends specified output variables to specified input variables of its neighboring neurons. We expect the scheduler to follow the same communication pattern between neurons over time in uniform way. In order to avoid artificial constructions of cyclic time-counters at nodes, we will define the architecture kernel through short sequence of kernels, with the scheduler executing them in successive rounds in round-robin manner. Specifically, when is BDH, we will have sequence of four kernels, K(A) = (K1(A), K2(A), K3(A), K4(A)), with Ki(A) being executed in every round such that mod 4. Programmable rulesets and the interaction kernel. We recall from Section 1.4 that model architecture has the interpretation of computational machine architecture, and models have the interpretation of programs in architecture A. We also recall that graph-based model is defined through set of parameters which represent the topology and weights of the communication graph of the system. The above considerations lead directly to the following observation: The graph of the communication network, which is used for communication between sites by the distributed system architecture during reasoning and language inference, has the interpretation of (trainable, rule-based) program. Consequently, we embed the subsequent definition of BDH in kernel formalism, given through form of programmable rulesets, using two-particle interaction rules on graph.7 The rulesets which we will use to define BDH will closely resemble rulesets (protocols) known from evolutionary and population dynamics (Hofbauer and Sigmund, 1998; Angluin et al., 2006; Aspnes and Ruppert, 2009) and chemical reaction networks (Chen et al., 2014; Feinberg, 2019), however, they will be restricted to special class of interactions. We start by presenting the more general form of this interaction kernel. We then explain how such kernel can be restricted, allowing it to be naturally implemented using local graph-based distributed system (in particular, one 6We provide graph distributed systems interpretation only for dynamics on graphs with non-negative matrix entries (positiveweight edges). Negative-weight edges are hard to represent using natural local dynamics based on token distribution or spiking models. 7We refer the reader to Appendix for more principled background discussion, guiding the appropriate choice of formalism for rule-based local interaction. 10 relying spiking dynamics), while remaining sufficiently expressive to describe an attention-based language model. The resulting restriction will be called the edge-reweighting kernel. Definition 1 (Interaction kernel, general form). system with species, N, and state (q1, . . . , qz) Q, qi R+, performs the interaction kernel with ruleset (protocol) given by set of transition rates called rule weights, = ((rijk R+)i,j,k{1...,z}, (dk R+)k{1...,z}), producing the following transition from state (q1, . . . , qz) to state (q 1, . . . , z) Q: := (1 dk)qk + (cid:88) rijkqiqj (3) We will describe such ruleset using the notational form: i,j = ({qi, qj rijk qk}i,j,k{1...,z}, {qk dk}k{1...,z}). As matter of convention, omitted rules correspond to rijk = 0 (respectively, dk = 0), while rules with no rate value stated next the pointer correspond to rijk = 1 (respectively, dk = 1). If qj is omitted from notation on the left-hand side, we assume qj = 1. Equation (3) captures the dynamics of the following differential equation: dqk i,j rijkqiqj. Assuming qi, qj, rijk [0, 1], the expression rijkqiqj has the interpretation of population dynamics or chemical process of the form and give k, with this processes happening at rate rijk, assuming qi, qj, qk have the interpretation of concentrations of species i, j, k. The formalism we use here assumes non-normalized state variables. dt = dkqk + (cid:80) We will subsequently use restriction of the interaction kernel to graph-based systems, which we call the edgereweighting kernel, to describe BDH. Restricting the interaction kernel to spiking signals and graph systems. First, we observe that rules of the form used in the interaction kernel from Definition 1 are extremely easy to implement in systems which rely on stochastic 0/1-valued signals. When ˆqi and ˆqj are independent random variables in {0, 1}, with Pr[ˆqi = 1] = qi and Pr[ˆqj = 1] = qj, then qi, qj qk is expressible as the AND gate of probability: the random variable δ ˆqk := qiqj {0, 1} gives the same expected contribution Eδ ˆqk = qiqj as the considered rule. We now consider the restriction of interaction kernels to the case of graph systems. In the general formalism, can be arbitrary with respect to and j. By contrast, consider graph systems, which describe binary relations between nodes, and not (directly) three-point relations. To resolve this, we will require that i, j, and have the interpretation of two nodes of graph and an edge which is incident to them. For an anchoring in the literature of dynamical systems, we note that already systems following an interaction kernel with strongly constrained of the form {i, j}, exhibit powerful nonlinearities: with such restriction on k, Equation (3) describes the class of evolutionary systems following the equations of replicator dynamics (Hofbauer and Sigmund, 1998), also equivalently known as non-normalized form of the fundamental Lotka-Volterra predatorprey dynamics. Replicator dynamics can naturally be represented as graph systems whose parameters are defined on on edges of the graph, but whose state is updated on on nodes of the graph. By contrast, when defining dynamics for reasoning in the current work, we will also need to capture more powerful class of graph-based systems, where, crucially, state is larger than the number of neuron nodes, appearing on neuron-neuron edges (synapses). We are now ready to describe restriction of the interaction kernel from Definition 1 to the case of node-edge-node interaction rulesets in graph: the edge-reweighting kernel. Definition of the edge-reweighting kernel. We consider graph system with nodes, indexed = {1, . . . , n}. Additionally, subset of pairs of indexes (i, j), for i, {1, . . . , n} forms the edges of the system. The system has state variables associated (uniformly) with nodes and edges, which we denote with capital letters, e.g., X(i), for or Z(i, j), for (i, j) E. Definition 2 (edge-reweighting kernel). distributed system follows the edge-reweighting kernel if its dynamics are given by the interaction kernel (Definition 1) with set of non-negative state variables, defined on the set of nodes and set of edges of graph, such that each local rule with non-zero rate is either computational rule involving only state variables on single node , or communication rule for an edge (i, j) E, involving state variables from the nodes i, and edge (i, j). For context, we remark that, in comparison to the strictly simpler dynamics of node-reweighting governed by graphbased replicator dynamics equations, dynamical systems based on the edge-reweighting kernel given by Definition 2 are rather elusive to study. We credit the seminal work of Algorithms theory (Christiano et al., 2011)[Fig. 1, Thm 3.2] 11 as the first rigorous study of local edge-reweighting graph dynamics, combining fast-paced linear kernels on nodes with slower-paced edge-reweighting process, in order to refine (focus) electrical flows on graphs towards sharper form of cost optimality.8 The BDH dynamics that we will introduce here rely on fundamentally different nonlinearities in the process, and will have the interpretation of guiding the system from premises defined at subset of nodes, towards search targets at nodes representing desired outcome, through reasoning inference rules with tunable weights set on edges. In the following Subsection, we will use the introduced formalism to define BDH as an edge-reweighting kernel on the union of edges of several graphs (Gx i, Gs) with the same set of nodes. e, Gx e, Gy i, Gy 2.2 Definition of BDH as local edge-reweighting process (equations of reasoning) Bearing in mind the discussion of graph dynamics suitable for the case of language inference, and specifically the definition of the edge-reweighting kernel (Definition 2), we are now ready to formalize the state-space dynamics of Equation (6) as local graph dynamics. i, Gs is Definition 3. The BDH model with neurons, with parameters expressed through graphs Gx represented as the ruleset of the edge-reweighting kernel, with O(n + E(Gs)) state variables, with rule amplitudes given by the equations of reasoning in Table 1. e, Gx e, Gy i, Gy Inference dynamics of BDH. The BDH dynamics rely on rapid pulse dynamics with state variables X(i), (i), A(i), defined on the neuron sites of the system, and fast-weight-like state variables σ(i, j), defined on subset of edges of the system, (i, j) E(Gs). The full implementation of BDH shown in Table 1(b) also includes auxiliary state variables e(i), i(i), e(i), i(i) which are used as temporary counters, for integration of excitatory and inhibitory signals received by neurons. The dynamics also rely on set of damping hyperparameters on state, > 0, which may in full generality be defined separately as u(i, j) for each edge (i, j) E(Gs). Inference with BDH is performed as follows. For some parameter (e.g. = 8 in most of this paper), which would correspond to the number of layers in Transformer-like system, the system scheduler proceeds through rules in round-robin manner, ingesting new tokens every 4L rounds and retrieving results 4L rounds later. During round 4l + k, for 0 < L, the system performs rules from the k-th column of Table 1, with each such round consisting of communication step on edges and local computation step on nodes. The state-space dynamics of BDH can be rewritten in vector-tensor form, equivalent to the local dynamics of the interaction kernel given in Table 1. This representation is given by Equation (6) in the following Section. Observation 1. The BDH-Graph protocol for the interaction kernel, given for any time round = 4Lt + (4l + k), 0 < L, = {0, 1, 2, 3} by the ruleset in Table 1 is equivalent to the state-space dynamics over time and layers l, given by Equation (6). Proof. For completeness, detailed explanation of the equivalence is provided in Appendix C.1. The variables X(i), (i), A(i), defined for each of the nodes of the system, are updated in successive rounds. The state variables σ defined on edges are assumed to be distinct over as σl, for 0 < L; this distinction serves to facilitate interpretation and to strike balance between the number of parameters and the size of state of the system (assuming single state matrix σ, uniform across l, does not fundamentally change the operation and scaling laws of the architecture). In the representation in Table 1 we do not impose how the local thresholding operation within some neuron i, of the form A(i) , B(i) (cid:57)(cid:57)(cid:75) (A(i) B(i))+, should be performed. We leave this as computational primitive, which can be realized based on approximate counting or comparator. The way natural neurons achieve thresholding to determine whether input signal excitation outweighs inhibition relies on time-integration of impulses. For realizations in other types of distributed systems and population protocols, we refer the reader to the literature on thresholding and Majority Protocols, cf. e.g. (Doty et al., 2021; Czyzowicz et al., 2022). The definition of the protocol does not specify how variable X(i) should be reset when the scheduler passes from layer of one input token to layer 0 for the next input token. As with the definition of state-space equations in Section 3, we leave this open to allow the dynamics to work both with externally provided input (for next-token prediction), or in self-feedback loop (for autoregressive operation). 8The graph dynamics used in this approach are naturally phrased in distributed computing parlance, see (Becchetti et al., 2018; Zou, 2019). 12 (a) Simple equations of reasoning Round 4l Inference from state Round 4l + 1 Reweighting of synapse state Round 4l + 2 Neuron replicator dynamics + inference from parameters Round 4l + 3 Inference from parameters X(i), σl(i, j) A(j) σl(i, j) 1u(i,j) (i), X(j) (i) Gs(i,j) σl(i, j) A(i), X(j) A(i) (i,j) Ge (j) (i) x(i,j) Ge X(j) (b) Complete equations of reasoning of BDH Round 4l Round 4l + 1 Round 4l + 2 Round 4l + Communication X(i), σl(i, j) A(j) (i), X(j) Gs(i,j) σl(i, j) (i,j) Ge e(j) A(i) (i,j) Gi i(j) A(i) (i) x(i,j) Ge e(j) (i) x(i,j) Gi i(j) Computation σl(i, j) 1u(i,j) e(i) i(i) (i) e(i) i(i) (Y e(i) i(i))+, X(i) (i) A(i) (X e(i) i(i))+ X(i) e(i, j), Gx e, Gx i, Gy e(i, j), Gy Table 1: The equations of reasoning: State-space dynamics of the BDH language model expressed through local graph dynamics with the edge reweighting kernel (Definition 2). The rules are executed for distributed system of neurons performing steps of parallel computation and communication during inference. Model parameters are expressed through the weights e, Gx i, Gs, and BDH model training is equivalent to defining rule probability amplitudes of edges of graphs Gx i(i, j), Gs(i, j) 0 for pairs of neurons i, {1, . . . , n} connected by the edges of these i(i, j), Gy Gx graphs. State is encoded in variables σ(i, j) at synapses, representing edges of graph Gs. The system proceeds in parallel rounds, with new tokens arriving into the system encoded through variables X(i) at neurons and introduced every 4L rounds, where is parameter of the model (e.g., = 8). The set of rules being executed (for each round modulo 4L) is given in the table. The readout of the system also happens through variables X(i) at the end of each 4L rounds. (a) Set of rules for the simplified version = 0), capturing the general form of the of the BDH model with no neuron inhibitory circuits and no thresholding (Gx communication structure and synaptic attention of the model. (b) Set of rules for the general case of BDH, including inhibitory circuits Gx i. An execution of the provided rules is equivalent to the state-space dynamics given by Equation (6). = Gy i, Gy 13 Notes on training. Direct training of the BDH model would be performed by selecting the edges of the considered i(i, j), Gs(i, j) 0 for pairs of neurons i, e(i, j), Gx graphs, and then setting rule weights Gx {1, . . . , n} connected by the edges of these graphs. e(i, j), Gy i(i, j), Gy In what follows, we will train tensor-friendly special case of BDH, called BDH-GPU, relying on an implicit (generally more efficient) representation of the considered graph parameter weights, using low-rank product representation for the matrices of these graphs. This representation is reminiscent of the hub-labeling graph representation technique, but is directly suitable for describing and evolving high-conductance scale-free networks. The appropriate architecture is introduced in Section 3. 2.3 Interpretation of attention as micro-inductive bias of reasoning Rule weights in the edge-reweighting kernel have the interpretation of micro-programs, governed by rules of transformation of state variables of the form A(i), B(j) σ(i, j) and A(i), σ(i, j) C(j), defined on edges between nodes i, of some n-node graph. This formalism can be seen as running an enormous circuit with form of universal gates given by the transition rules, over structure of computational elements at nodes, and memory elements on edges of graph. While the local rulesets have the form of rule-based micro-assembly, we leave open the extent to which they should be considered to have an interpretation of programming in logic (as would be the case, e.g., for C-RASP (Yang and Chiang, 2024)). The natural interpretation of σ(i, j) > 0 is positive bias associated with the neuron pair (i, j), i, {1, . . . , n}, which follows from past context. This can be considered by phrasing the local rules of the system in framework of logic inference; we do so informally, omitting discussion of layers. If past context (xτ : τ < t) implies that implication has weight σt1(i, j), and if the current state at time implies that follows from this state with weight xt(i), then the current state at time implies that follows from this state with weight xt(i)σt1(i, j). The above is intentionally phrased to resemble the logical axiom (X (i j)) ((X i) (X j)), which is perhaps most prevalent across different formalizations of axiomatic logic, with an application of modus ponens as an inference rule. The inference system of the considered model uses state and model weights to devise its own heuristic for the order of evaluation, i.e., to consider which facts appear to be most plausible to be evaluated next, and to evaluate them in an order based on what follows most strongly from context. In way consistent with what we expect from informal reasoning in language, the considered weights have more direct interpretation of an increment of utility associated with given inference.9 In the setting of argumentation, this utility-based approach could, for example, guide the inference process from pair of known concepts in context, source and target, to an intermediate concept likely to be common-neighbor shortcut lying on logical path between this source and target (cf. Section 5.3 for discussion of how this type of mechanism is enforced in the feed-forward network of BDH-GPU). The considered micro-foundational interpretation of attention, defined at the level of individual neurons (or logical variables), does not contradict the way in which Transformer attention is often regarded at the coarser level of vectors through key-query lookup intuitions. At the same time, it highlights that an attention state entry σ(i, j) (and similarly, model edge weight leading from to j) does not have the interpretation of logical value (i.e., something that is true or false), but an inductive bias associated with how likely the system is to consider the implication j in its next steps of reasoning, when proposing its next conclusions or next ideas for consideration. Chains of implications in BDH guide activations along paths in the system graphs Gx allows specific implications to enter into paths of thought once the corresponding synapses are open in state σ. e, Gy e, σ. For the latter, attention 2.4 Interpretation of BDH as an oscillator network toy-model Whereas the interpretation from Subsection 2.3 focuses on properties which fallow from the computational function (purpose) of the system, here we outline an interpretation of the behavior of BDH considered purely as dynamical system. 9Here, the term utility is understood in the sense of evolutionary game theory, as applied to the population of neurons, considering the standard interpretation of replicator dynamics, as applied in the ruleset from Table 1. Neurons which win in the natural selection process are added to the activation ."
        },
        {
            "title": "Symbol",
            "content": "Interpretation in: Table 1, State Equation (6) Oscillator Network Toy-Model Gx, Gy, Gs graph parameters of model wires, prods, and elastic connections σ x, synaptic state of model activation vectors displacement of elastic connections pulses at nodes, state correction Figure 2: The physical system representation of BDH as physical graph toy-model. Definition of the toy-model. We will consider the toy-model of an n-particle system shown in Fig. 2 as an illustration of the general form of dynamics of the state-space equation (6) of BDH. We draw the particles in circle.10 The particles are connected with each other by state elements, represented in Fig. 2 as elastic connectors. The topology of these pairwise connections is given by graph Gs, and may in general be dense. The signal displays dynamics of state ρ through tension on connectors, which evolves at slower time scale, and more pulse-like activation dynamics x, (on nodes), appearing and vanishing regularly, at rapid time scale. The slower state dynamics represent, in the first order, oscillation or relaxation of the system of elastic connectors. Once an elastic connector between particles and has had its endpoints displaced through state and y, respectively, tension appears on this connector, which causes its displacement σ(i, j) that relaxes over time (damping variant, corresponding to ALiBi), and/or acts as spring element (oscillator variant, simplified illustration of RoPE). Initially, σ(i, j) = 0. The faster dynamics represent the node dynamics of particles. Over time, pulse displacements x(i) happen at nodes, as result of either previous behavior of the system, or perturbation by an external forcing field (in reality this field would be language input). node with displacement x(i) may, due to the aggregated action of tension of elastic connectors σ(i, ) adjacent to it, activate system of prods Gy adjacent to it, perturbing nodes it hits in this way. If another node is prodded sufficiently hard, it may cause it to activate perturbation y(j). The perturbation y(j) of node will, in the next step, propagate again to those other nodes i, which are connected to by system of wires (Gx). If the aggregated pull of wires on node is sufficiently strong, this modifies its pulse displacement x(i). The pulse activation y(j) of some node j, directly followed by pulse activation x(i) of node i, results in an increase in the tension on the connector (i, j), adding to the value of the tension σ(i, j). All pulse activations subside, and the pulses propagate, consequently altering the slow state σ. In general, σ(i, j) is triggered simply by the temporal connection between the pulse y(j) activating, followed by the pulse x(i) activating immediately afterwards, even if there was no direct causality between the two (although y(j) contributed to pulse x(i) happening if (j, i) Gx). An appropriate correspondence of the graphs, Gs Gx, would bring the system close to an observed causal effect on the activated synapse. The above description of the pulse dynamics was given from the perspective of nodes. From the perspective of connectors, an existing tension on some connector σ(i, k) propagates through prods Gy to some nodes j, then through wires Gx to some nodes i, and this finally contributes to tensions on other connectors σ(i, j). This propagation of state thus happens to 3-hop neighbors, through i, j, i. During training, the behavior of the system may, in even longer time scales, result in the propagation of changes of connection weight and structures to graphs Gx and Gy, as well as (optionally) Gs. 10This is direct tribute to the Kuromato coupled oscillators model; the crucial difference being that in BDH, the elements of state with an interpretation similar to oscillators appear on connections between nodes, not nodes. 15 Effects captured by the toy-model. We have described small local graph kernel, with 3-hop locality, capturing the two key effects of the local graph kernel. The first effect is the graph form of communication pattern between nodes, and thresholding of updates. (We have omitted direct mention of inhibition from discussion of the toy-model, but it is direct to include.) The second effect is the placement of attention state on node connections, its update patterns, and the dynamics of its relaxation over time. We intentionally convey the interpretation of node pulses as differential (gradient) of state on node connections. This interpretation is consistent with our empirical study from Section 7. It is worth considering once every how many steps of the operation of the toy-model, single element of state σ(i, j) is updated. This depends directly on the sparsity of the pulse signals y(i), x(j); at least one of them is, generally, sparse. If the pulses where to happen very seldom for such pair (i, j), state updates are essentially second-order correction effect. By adjusting the frequency of updates, the system can be made to operate exactly at the critical point where this pulse dynamics ceases to be second-order correction of state σ(i, j), giving the random variable describing the time between updates of connection pair σ(i, j) heavy power-law-like tail distribution (possibly with different distribution parameters for different pairs (i, j)). In the description of state dynamics, we noted the hop-distance of 3 in the forward propagation of changes to state. Bearing this in mind is helpful when considering how gradient backpropagation mechanism would follow dependencies between changes of state if such system were to have its graph weights altered through backpropagation. Finally, let us clarify the specific choice of kernel we made for BDH. We found it to work well, and we knew how to train BDH models which implement it on GPU (which we will call BDH-GPU). This, with current hardware, made it 102 105 times more costand time-effective to train models and analyze outcomes than kernels, for which we only knew how to train on CPU. Nonetheless, the question of finding optimal kernels according to different criteria (e.g.: minimality of kernel, best training rate per token, closeness to brain function based on known evidence from brain studies), is an extremely pertinent foundational problem. The problem can be phrased in closed-ended way, leaving finite number of possibilities to be checked, at least when considering small graph kernels. Some kernels may also prove to have superior learning capabilities to the Transformer (and BDH), and if this quality difference is overwhelming, they may eventually prove commercially viable. In the following, we formalize the choice of kernel for BDH, and also provide framework to describe other kernels capturing the same effects of graph communication and synaptic attention. 2.5 Expressing BDH using brain models The results we obtain for BDH provide direct corollaries on the expressiveness of brain models which are capable of emulating the local graph kernels of BDH. Specifically, distributed system, which is able to efficiently emulate the local kernels of BDH, has sufficient expressiveness to perform language inference and reasoning at least to the same extent as BDH. Observation 2. The local ruleset of BDH  (Table 1)  can be expressed through combination of simple mechanisms: neuron activation with positive state variables, Hebbian learning, and communication through excitatory and inhibitory circuits with thresholding. We note that in the description of the rulesets in Table 1, Round (4l + 2) and (4l + 3) directly describe the use of excitatory and inhibitory circuits with integrate-and-fire thresholding at neurons. Round (4l + 2) additionally includes form of competition effect between neurons, realized fully locally at neurons using the multiplication effect of replicator dynamics. The communication rule of Round (4l + 1) involves the potentiation of synapse based on activations of neurons at its endpoints. As was discussed in Subsection 2.1, the natural mechanism for implementing increase in synaptic strength is through spiking dynamics, where the execution of the communication rule of Round (4l + 1) is stochastic AND-gate on signals. Finally, Round (4l) describes the long-term effects of using strengthened synapse for transmission of signals, and its strength decrease. We can use the framework of expressiveness, as captured in Observation 2, to shed light on the capabilities of natural systems through their ability to emulate artificial ones. Specifically, if natural system can plausibly emulate some artificial system by using the resources it has at its disposal, and artificial system is able to solve problem P, this can be used to explain: (1) why the natural system is sufficiently powerful to solve problem P, and (2) plausibly, that the purpose for which system is equipped with certain mechanisms includes solving problem P, if such mechanisms prove useful in the emulation of B. The experimental validation of the performance of BDH architecture at Transformer level (Section 4.2) confirms that BDH is sufficient to provide language and reasoning function at scale. We can thus make the following statement. Empirical Finding 2. The Hebbian learning mechanism is plausibly needed, and in combination with neural circuits, sufficient, for performing the reasoning function at the scale of the brain. This includes performing language function with attention, and performing thought processes, at time scale of minutes. In view of our results, Hebbian learning can be seen as form of unsupervised learning over time, expressed through graph edge reweighting, to perform reasoning and language inference using the attention mechanism. This type of result can be compared to an analogous interpretation for Hebbian learning in the context of vision, as pioneered in (Brunel, 1996). With the setting of language and chain-of-thought reasoning, we are able to directly capture effects of time in the brain. Given the interpretation of neuron activations as carrying the necessary gradients of synaptic state (Section 2.4), the problem of supervised learning (i.e., taking into account feedback signals) plausibly becomes deferred to selective transfer and re-encoding of gradients from state into weights, at longer time scales. We return to discussion of this point in the Conclusions, bearing in mind the fact that the general difficulty of the problem is now reduced through restrictions on the considered edge-reweighting kernel, and the relative rarity of synapse activation events. Our work also suggests framework for further discussion of reasoning function, with an anchoring point for this type of investigation in the time-scale of split-seconds to minutes. The question of shorter time scales is then one of designing more precise communication and computational primitives for spiking neurons and synaptic plasticity, which can be used to perform primitives for individual rules of graph kernels for the inference dynamics.11 The question of longer time scales, and the changes to model structure that follow in learning process, naturally follows any explanation of unsupervised (Hebbian) learning from the shorter time scale that is considered here, as mechanism of transfer from state to weights; we come back to this point in the Conclusions."
        },
        {
            "title": "3 BDH-GPU: a tensor-friendly version of the BDH architecture",
            "content": "We will now introduce BDH-GPU, variant of the BDH reasoning system, expressed in the language of tensor operations typical for Deep Learning models. BDH-GPU provides GPU-compatible implementation of BDH. BDH-GPU can be easily implemented in PyTorch, didactic code listing is provided in Appendix E). Furthermore, BDH-GPU can be trained on large text datasets using error backpropagation, and has been shown experimentally to match performance of GPT-based LLMs. The main steps towards the efficient implementation of BDH-GPU on GPU are: 1. Express graphs Gx and Gy low-rank factorizations of their transition matrices, followed by ReLU nonlinearities (Nair and Hinton, 2010) (we explore graph properties of these approximations in Section 5). We never materialize these matrices, but maintain instead low dimensional state per each neuron. 2. Never materialize the σ state matrix, preferring instead to access it using linear attention operation over low-rank representation of values (we explore the properties of this attention mechanism in Section 6). 3. Normalize all state variables using LayerNorm (Ba et al., 2016b). We will refer to the architecture in the final intermediate step, before the introduction of LayerNorm, as BDHNormfree. 3.1 Notation for BDH-GPU We consider the BDH-GPU(n, d) architecture parameterized by positive integers n, d. The system scales in dimension the number of particles. In what follows, we will use the terms particle and neuron interchangeably. Dimension is measure of the number of parameters per neuron required to represent the interaction of this neuron with the particle interaction field or interaction graph. For asymptotic analysis, we assume that + is the basis for all asymptotics, and > log holds for some sufficiently large constant > 0. For the tensor representation of the model, which is the primary one for implementation and empirical studies here in this paper, vectors in Rd have an interpretation as (fuzzy) addresses of virtual memory space of size n, hence the assumption = Ω(log n) cannot be dispensed with while using natural (linear-algebraic) arithmetic on real numbers. We later show how to 11While we do not provide direct explanations for effects at shorter time scales and scheduler primitives, we note the type of kernels we rely on are well understood in terms of the ability to work with asynchronous schedulers, and obtaining emergence of synchronization. (Kosowski and Uznanski, 2018; Dudek and Kosowski, 2018) 17 avoid this assumption in graph-based models, by using uniform local graph kernels of smaller degree with graph communication structure. Nonlinearities: ReLU and LayerNorm. In what follows, we assume that one-dimensional vector is denoted by lower-case letter, e.g., z, with Rn1 = Rn unless otherwise stated. Vectors which appear in dimension are named with an asterisk, e.g., as Rd1. We denote the ReLU operation (z)+ := maxi1,...,n{0, zi}. We further define LayerNorm of vector Rd1 in uniform non-parametric way, LN (z) = z1Edz Ed and σd are estimators of mean and standard deviation in dimension d, respectively. , where σdz Activation vectors and parameter matrices. In vectors representing activations, each scalar element (element of R) of the activation vector has the interpretation of scalar activation state of single particle. Throughout this text, is generally assumed be the field of real numbers := R, and scalars are represented by fixed-precision floating point number in experiments.12 By convention, in discussions of parameters, matrices denoted Gx, Gy, Gs Rnn will represent neuron-neuron interaction, encoders Rdn reduce dimensionality of activation vectors (e.g., = Ez for Rn), and decoders Rnd lift them back into Rn (e.g., = Da). Depending on the architecture variant considered, the state will either have the interpretation of neuron-neuron correlation matrix σ Rnn, or compressed form with reduced dimensionality, ρ = Eσ Rnd. 3.2 Definition of BDH-GPU as state-space system We now define the main architecture of this paper in its tensor flavor, called BDH-GPU. Definition 4 (inference dynamics of BDH-GPU). BDH-GPU state-space system BDH-GPU(n, d), given by three parameter matrices: Rdn and Dx, Dy Rnd, performs iteration over time = 0, 1, 2 . . . and layers = 1, 2 . . . L, governed for any time by the following recurrence: xt,l := xt,l1 + (cid:0)Dxv a τ,l1xτ,l t,l := (cid:88) t,l1 tτ xt,l (cid:1)+ τ <t yt,l := (cid:0)DyLN (cid:0)a t,l := LN (Eyt,l) t,l (cid:1)(cid:1)+ xt,l (4) where inputs to the system are provided through the boundary condition τ,0 in layer 0, for τ = 0, 1, 2 . . . t. Here, Rnn is diagonal or block-diagonal matrix representing local rotation or damping of state (such as ALiBi or RoPE), is the number of layers. BDH-GPU as language model. BDH-GPU is intended to be used as language model, processing one token t,0, for N, is obtained using some (linear) encoding function from the per time step, in which case the input token alphabet Ω, fe : Ω Rd, as applied to the t-th input tokens. Similarly, the logits of the t-th output token are extracted using some decoding function applied to outputs of the L-th layer t,L, using (linear) token decoder function fd : Rd Ω. The source of language tokens may be external, as is the case for next token prediction tasks, or auto-regressive. For training, we assume that model trained in the BDH-GPU(n, d) architecture has the trainable parameter set = (E, Dx, Dy, fe, fd), with all parameters trained together. The model has 3nd+2Ωd = (3+o(1))nd parameters, i.e., the scalable part of the model is concentrated in the total of 3nd parameters of the matrices (E, Dx, Dy). State-space representation. The notation of Definition 4 is chosen so as to exhibit its direct applicability in Transformer-like token-parallel training framework. Vector τ,l1 has the interpretation of attention value inputs at time τ in layer l. Vector t,l represents the result of linear attention mechanism for time in layer l. 12When only asymptotic analysis is the object, it is sometimes convenient to consider := Rk for some = 2, 3, . . .. Specifically, considering := R2 allows SO(2) rotations on R2 to be expressed as scalar ones on R, thus making the R2n2n RoPE block-diagonal matrix of diagonal matrix in Rnn (Su et al., 2024). This provides consistent formalism for ALiBi (Press et al., 2022), RoPE, and extensions such as LieRE (Ostmeier et al., 2025) as diagonal (communication-free) operations. In all cases, the application of ReLU ()+ to scalar remains coordinate-wise in R. 18 Denoting in (4) the models attention state as ρt1,l = (cid:88) τ <t τ,l1xτ,l tτ (5) we obtain the equivalent but more compact form of representing the inference dynamics of BDH-GPU as state-space model, presented in Fig. 3, Eq. (8). BDH σt,l := (cid:0)σt1,l + (cid:0)(cid:0)yt,l1xt,l xt,l := xt,l1 + (cid:16)(cid:16) Gx yt,l := (cid:16)(cid:16) Gy Gy Gx i(cid:17) (cid:1) Gs i(cid:17) yt,l1 (cid:1)(cid:1) (cid:17)+ (6) σt1,lxt,l (cid:17)+ xt,l BDH-GPU ρt,l := (cid:0)ρt1,l + LN (Eyt,l1) xt,l xt,l := xt,l1 + (DxLN (Eyt,l1))+ yt,l := (cid:0)DyLN (cid:0)ρt1,lxt,l xt,l (cid:1)(cid:1)+ (cid:1) (8) BDH-Normfree σt,l := (cid:0)σt1,l + yt,l1xt,l (cid:1) ρt,l := (cid:0)ρt1,l + (Eyt,l1)xt,l xt,l := xt,l1 + (DxEyt,l1)+ yt,l := (Dy Eσt1,l (cid:124) (cid:123)(cid:122) (cid:125) ρt1,l xt,l)+ xt,l (cid:1) (cid:41) alternative representations (7) Figure 3: State-space equations of the model architectures introduced in this paper. All architectures refer to set of interacting particles (neurons), with activation vectors xt,l (R+)n. Vector yt,l (R+)n, yt,l is (typically) sparse in the sense of yt,l0. Variables ρt,l Rnd or σt,l Rnn represent hidden state of the system. The graph-based BDH dynamics equation (6), equivalent to the ruleset from Table 1, serves as starting point for development of architectures represented as local graph kernels in distributed computing system. The simplified BDH-Normfree equation (7) is special case of BDH. Up to lack of LayerNorms, it approximates the inference dynamics of BDH-GPU, with the correspondence ρt,l = Eσt,l. The tensor-based BDH-GPU architecture is described by equations (8) (mathematically equivalent to Definition 4, Eq. (4) and (5)) and is the primary point of reference for all model training and all empirical results presented in this study. For discussion of extensions to BDH-GPU such as heads, see Subsection 4.1. complete code listing for BDH-GPU is provided in Appendix E. Figure 4: Scaling of BDH-GPU architecture in dimension n. The other parameters can be considered fixed during scaling. For example, with choice of = 256 for low-rank dimension, = 2 for neuron pairing with RoPE, and = 1 for single-head architecture, the model scales linearly in dimension in chunks of dhk = 256 2 1 = 512 parameters. In what follows, we will perform analysis focusing on the state-space representation of the architecture given by Eq. (8). 3.3 Interpretation of BDH-GPU as local interacting particle system The BDH-GPU dynamics equation (8) has the interpretation of n-particle system, with the state ρt(i) of the i-th particle, = 1, . . . , n, given at the end of time by vector in Rd for each layer: ρi(t) := (ρt,l (i,) : (1, . . . L)). Overall, as we will see directly, the way particle interacts with other particles at time is described by the following tuple Zi: Zi(t) := (ρi(t), E(i,), Dx (,i), Dy (,i)). Here, ρi(t) represents the in-context state associated with particle (initialized as 0 at the start of inference), while the other three vectors of length associated with this particle are trainable, but do not change during inference. The system scales in dimension and is completely uniform in this dimension, excepting following effect. Let denote the size of largest block in the block-diagonal matrix ; then particles, are bound by this effect into nonuniform k-tuples when > 1 (k = 1 when is the ALiBi matrix, and = 2 when is the RoPE matrix). Thus the system, in general, scales in the dimension of uniformly, in chunks of particles (see Fig. 4). The interaction between particles is, intuitively, local. To be able to proceed with discussion with rigor and without complicating notation, we assume for the analysis that = 1. We also drop LayerNorms from the equations of inference dynamics. (Models generally do not train following BDH-GPU without any LayerNorm, but we observed empirically that there is some flexibility as to where these LayerNorms are placed; they can also be moved to the neuron dimension n, and they are parameter-free.) The dynamics without LayerNorm are represented under the name BDH-Normfree in Fig. 3. We have the following. Observation 3 (local particle interaction by mean-field). The BDH-Normfree dynamics have the interpretation of mean-field interaction between particles, fully characterized at any time by O(dL) parameters of particle in state, and O(d) parameters in particle representation. This observation is essential for the subsequent discussion, and it can be expanded in three different ways. In computing terms, at any time and in any layer l, the action of the system can be represented as an iterated application of the dynamics equations (4), with each of the particles realizing, for each equation in each layer (i.e., total of 3L times), form of micro-program, involving local computation and communication with other particles by broadcast. In framework of local distributed computing (cf. e.g. (Peleg, 2000)), it would be represented as node performing the following form of local kernel as part of networked system: 1. compute some message vector mi Rd locally (without communication with other particles), based only on current activation xt,l, i, yt,l, and previous state Zi(t 1), 2. broadcast message mi Rd to other particles, 3. receive the mean-field message = (cid:80)n j=1 mj Rd, identical for all particles, 4. update local activation variables for the next layer + 1, and update new state σi(t) Zi(t), based on the received result of the broadcast and local computation. In Physical terms, we observe that the interaction field of the particles, which realizes the broadcast, is localized, and can at any time be expressed as sum of pairwise particle interaction terms between particles i, 1, . . . , t. These pairwise interactions depend only on parameters Zi(t 1) and Zj(t 1), and the activation variables of these particles, representing properties of these particles at time and expressible through O(Ld) scalars. This interaction field evolves with time together with Zi and Zj.13 In Engineering terms, we observe that any transformation of length-n vector into another length-n vector passes through an intermediary low-rank representation of dimension at most d. An example is the equation for xt,l in (7), which reduces length-n vector yt,l to length d-vector through application of the encoder matrix E, before lifting the dimension back to by an application of the decoder Dx. 13Note that Zi(t 1) depends only on ρt1,l, not ρt,l. This is because of the stopping index of τ = 1 in the definition of attention in Def. 4, and is intentional. 20 3.4 Expressing BDH-GPU using BDH: preserving parameter and state size BDH-GPU and BDH both represent n-particle systems. For special parameter choice (of BDH), they have equivalent patterns of communication and of computation (up to placement of layer norms). Observation 4 (BDH-Normfree is special case of the BDH graph model). Models in the BDH-Normfree architecture (Eq. (8)) and models in the BDH architecture (Eq. (6)) are formally equivalent (i.e., the same model) subject to the following choice of model parameters of BDH: Gx Gx = DxE, Gy Gy = DyE, Gs = 1nn, (9) where 1nn is the all-ones matrix. We discuss in more details below how BDH compares to BDH-Normfree in terms of size of state and number of parameters needed for one architecture to approximate the inference dynamics of the other. In general, BDH is not less expressive than its tensor-based counterpart. For BDH-GPU parameters and state are naturally expressed using tensors of O(nd) model parameters. In this section, we discuss how to express model parameters and state of BDH, in such way as to maintain comparable size of parameter and model space. 3.4.1 Expressing matrices Dx, Dy, as graphs Gx, Gy We start by taking care of the first correspondence, that of parameter spaces of BDH-GPU and BDH. Asymptotically, BDH is strictly more expressive at the same number O(nd) parameters. Recall from Eq. (8) that the parameter space of BDH-GPU consists of three matrices DyDx Rnd, Rdn, and (up to shifting of LayerNorms), their role is to encode the pairs of matrices DyE, DxE, Rnn, as used in Eq. (7). In the Claim below, we capture the correct encoding of one such matrix pair in the form of graph of O(nd) parameters. Consider (directed, weighted) graph R+nn on set of vertices = {1, . . . , n}. We will consider graph which need be directly sparse graph, but can be represented as square of graph with few edges. Formally, we will say that G2(n, m), for some N, if there exists graph R(n+s)(n+s), with vertex set S, where = s, such that = 2[V ], i.e., is the induced subgraph of 2 restricted to vertex set , and has at most (strictly positive) edges. For G2(n, m), we can consider an interpretation of hidden layer between input layer and output layer . All matrix weights coefficients are restricted to be non-negative, and the two linear layers are sparse with total of at most non-negative connections. Graphs in G2(n, m) are naturally expressed through the edges of the previously defined graph H, using O(n log n+m) parameters. The class G2(n, m) is strictly more expressive than the class of sparse (m-edge) graphs on vertex set .14 We will refer to the middle layer of vertices that makes such representation possible as the sparse synaptic layer, to the graph on vertex set as the sparse linear circuit, and the graph 2[V ] G2(n, m) as the neuron-neuron interaction graph. The role of the constructed graphs is to serve for propagating linear dynamics of the form (cid:55) Gv, (R+)n1, for graph-based local models. We have the following Observation. Observation 5. Let G2(n, m) be neuron-neuron interaction graph, with = 2[V ], where is the sparse linear circuit on vertex set S, which has edges. Then, the linear dynamics on graph G, (cid:55) Gv, can be efficiently expressed through two steps of linear dynamics on graph H, (cid:55) 2v, for (R+)n1. This representation requires O(m) parameters. In the above, thee exact number of parameters needed to represent graph of edges follows from conventions introduced in the Notation (Section 1.4). In what follows, we will assume that BDH represents its parameter matrices through appropriate sparse linear circuit graphs H, which it uses to realize the linear neuron-neuron interaction dynamics G. We illustrate the correspondence between graphs and in Fig. 5. We observe that BDH can express BDH-GPU parameter matrices with the same asymptotic number of parameters. The claim below applies to pairs of matrices DE, for = Dy and = Dx. 14The formal expression in the definition of the class of weighted graphs G2(n, m) can be compared to that of the class of graph distance matrices admitting sparse hub labeling representation (Abraham et al., 2011) (or closely related landmark labeling). In our case, vertices in the hidden layer also have natural interpretation of landmarks on directed paths connecting nodes of . 21 Figure 5: Neuron-neuron communication using graphs G2(n, m): correspondence between graph with edges (left), and neuron-neuron interaction graph = 2 (right). The approach allows to express linear signal propagation on broad class of graphs G2(n, m) using two steps of linear dynamics on sparse circuit H, i.e., Gz = 2z for (R+)n. Claim 3. For any matrices Rn,d, Rd,n, there exist neuron-neuron interaction graphs Ge, Gi G2(n, m), such that Ge Gi = DE, with = O(nd). In consequence, for the same asymptotic number of parameters O(nd), graph-based feed-forward mechanisms of BDH are strictly more expressive than corresponding mechanisms in the tensor-based implementation, BDH-Normfree. Proof. The short proof of the Claim is deferred to Appendix C.3. We note that the converse implication does not hold: an arbitrary graph Ge G2(n, m) does not admit an exact low-rank decomposition Ge = DE. Indeed, in general any low-rank decomposition introduces form of noise whose implications we discussed in Section 5.3: if Ge has modular (cluster) structure, the low-rank approximation Ge DE still allows form of in-cluster propagation dynamics. 3.4.2 Expressing BDH-GPU attention on graphs: sparsification and trainability of Gs We recall that by Observation 4, the equivalence between the attention state σt,l in BDH and in tensor-based implementation holds for the case of the complete directed graph, Gs = 1nn. This means two things: first, in BDH, graph Gs can be trainable, while in BDH-GPU it is not. This acts to the potential advantage of BDH for expressiveness. Second, in BDH, the graph Gs obtained through the direct correspondence is dense: with neurons, BDH would need n2 synapses to precisely reflect BDH-Normfree. This aspect is more of technical nuisance than an actual difference: the expressiveness of the attention mechanism of BDH, equipped with sparse graph Gs, is sufficient to represent the attention operation as used in BDH-Normfree. Indeed, in the tensor-based BDH-GPU dynamics, the attention operation is immediately followed by low-rank operation, ρt,l = Eσt,l, where ρt,l has nd parameters. Graph models can instead rely on sparse graph Gs to achieve the same form of state compression through sparsification. Claim 4. The attention block of BDH-Normfree can be expressed using the attention block of BDH with graph Gs having O(nd) edges, subject to natural preparation of attention values entering the attention block of BDH (directly before this attention block). Proof. The formal statement of the Claim and its proof are deferred to Appendix C.4. Going beyond the formal equivalence between BDH and BDH-GPU from Observation 4, the above Claim, combined with Claim 3, shows that BDH has at least the same expressiveness as BDH-GPU even for the same number of parameters O(nd) and the same size of state O(nd) per layer. Independent of graph-based models, in the subsequent analysis of the feed-forward network and attention mechanisms of BDH-GPU, we will show that the matrices DxE, DyE, σ Rnn of BDH-GPU admit natural interpretation as n-node directed graphs (once appropriate threshold functions are applied). For example, the visualizations in Fig. 11 and Fig. 10 correspond to graph representations of matrices σ and Gx := DxE of BDH-GPU, respectively, after 22 thresholding. This graph interpretation of matrices in BDH-GPU also defines the neuron-neuron communication graph of the underlying BDH model, given by the equivalence from Eq. (9)."
        },
        {
            "title": "Implementation and scaling laws",
            "content": "A code framework for BDH-GPU, representing the architecture from Definition 4, is made available in the Appendix E. In this Section, we present some guidelines on choice of hyperparameters, and an empirical study of models implemented in the BDH-GPU architecture, as well as comparison to the Transformer and other language model architectures. 4.1 Implementation characteristics of BDH-GPU Model scaling in neuron dimension n. The architecture BDH-GPU(n, d) has two main dimensions, which is the dimension of its concept (neuronal) space, and n, which is its low-rank (synaptic) dimension. The model scales primarily with the number of neurons n. Almost all of the weights of the model are contained in three parameter matrices called E, Dx, Dy; thus, the number of parameters is precisely (3+o(1))nd. The ratio between the dimensions and increases rapidly (asymptotically) with model size; already for 25M-parameter model, sound choice of dimensions is: = 256, = 32768, read as 32768 neurons, each characterized by total of 3d = 3 256 = 768 scalar parameters. Layers and heads. The architecture has layers (e.g., = 10). As in the Universal Transformer (Dehghani et al., 2019), all layers use the same set of weights for each of the parameter matrices. The architecture may be equipped with several heads h, subdividing dimension n. The role of heads is limited to single parameter-free LayerNorm, normalizing outcomes of linear attention separately for each head. The optimal number of heads is typically smaller than in the Transformer (e.g., = 4). Linear attention with state aligned to neurons. The state space of the model is fixed and large. It has the macrointerpretation of associative memory (like KV-cache, but organized differently), and is used to perform linear attention. For each layer, the state space is independent and has fixed dimension d, the same as the model weight matrices. Thus, portion of parameters of state is directly associated with each of the neurons. With each token processed, fraction of the models state space is updated. Sharing of state between the layers is not performed in the vanilla architecture. As usual with SSMs, there is no notion of context window. Similarly to BDH, BDH-GPU maintains large recurrent state comparable in size with its total number of parameters (c.f. Fig. 4). This stems from the fact that both the recurrent state matrix, and parameter matrices are expressed as low Figure 6: Diagram of one layer of the BDH-GPU architecture, following Eq. (8). Layer inputs are xl1, yl1 Rn, layer outputs are xl, yl Rn. Model parameters are contained in the Rnd and Dx, Dy Rdn, and shared across all layers. Each layer has state ρl Rnd which is used in the Linear Attention block and persisted over time. PyTorch code implementing the model is provided in Appendix 23 rank factorizations of graph transition matrices. We believe that this helps the model with generalization with respect to RNNs which have O(N 2) parameters which manipulate state of size O(N ). Sparse positive activation. The architecture relies on length-n vector xt,l passed to the l-th layer for the t-th token processed, which can be assimilated to the vector giving rise to keys, values, and queries in the Transformer, but operating in higher dimension. As crucial design assumption, this vector has all non-negative elements (xt,l 0). An empirically observed fact is that the activation pattern of xt rapidly becomes sparse (in typical training run, only ρ 5% of the entries of vector xt are non-zero). This corresponds to the fraction of the state space read and updated for each token. 4.2 Comparison of BDH-GPU to GPT2-like Transformers Architecture differences. BDH-GPU in its vanilla form can be compared to the GPT2 architecture (Radford et al., 2019) with RoPE attention. In this comparison, BDH-GPU retains or strengthens the key advantages of the Transformer (parallel trainability, attention mechanism, scaling laws for loss versus parameter count, learning rate per token) on tests and benchmarks at the model scales we tested (1B parameters), across tasks such as language and translation. The architecture of single layer of BDH-GPU is presented in Fig. 6. The most evident architecture differences between BDH-GPU and the Transformer include the following: BDH-GPU has fewer parameter matrices, allowing for more compact interpretation and analysis. BDH-GPU scales for parameters (and context length) almost exclusively in single neuronal dimension, n. Key-value state and parameter matrices have matching dimensions and are highly localized together with state, with portions of these matrices attributable to individual neurons. There is no notion of context length in BDH-GPU, and consequently no hard bound on it. Attention of BDH-GPU is linear, but happens in the models large neuronal dimension. Activation vectors x, of BDH-GPU are positive (after passing through ReLU gates), and vectors are observed to be extremely sparse in practice. Transformer-like scaling laws. We have experimentally validated the scaling laws of BDH-GPU, expressing loss as function of parameter count, for next-token-prediction tasks. At the same parameter scale, BDH-GPU generally compares favorably to the Transformer even on relatively short-context tasks requiring use of attention, such as translation, Fig. 7. In general, on next-token prediction tasks, BDH-GPU appears to show improvement of loss reduction per token of data than the Transformer, i.e., learns faster per data token, both for the natural tasks we tested (see e.g. Fig. 7) and on synthetic puzzles. The BDH-GPU architecture appears to be preferred choice for training setups where: (1) models need to learn from scarce data, or (2) training workloads need to be optimized for makespan. For the first setting, the training rate per token is the decisive factor. For the second setting, BDH-GPU can be used differently than the Transformer in distributed training and distributed inference setups because of the way it scales its dimensions. FLOPS counts. The theoretical count of arithmetic operations per token of BDH-GPU during inference is bounded by O(ndL). Each parameter is accessed O(L) times per token (with the typical sufficient number of layers being smaller than in the Transformer), and each element of state is accessed O(1) times per token, with small hidden constants. These are rough bounds for simple implementation, and do not take advantage of activation sparsity. For short contexts BDH-GPU is amenable to parallel training with causal self-attention kernel. The simple code template provided in the Appendix is sufficient to reproduce the empirical results presented in this paper on single GPU node. For longer contexts (typically above 4096 tokens for = 256), state-space kernel for linear attention is faster and more space-efficient. 4.3 Comparison of BDH-GPU to other sequence processing architectures Transformers with Linear Attention. Linear attention works well when used in high dimension, subject to appropriate preparation of key vectors (as we discuss in Subsection 6.1). An elegant way to eliminate non-linearity of attention, by applying preparation of key vectors through tensor product, was proposed in (Buckman et al., 2024). We use completely different approach to achieve attention in high dimension. 24 Figure 7: Performance of BDH-GPU and GPTXL versus model size on translation task. We have tested all models under the same training and evaluation regimes. All models show improved performance with scale. BDH-GPU uses exactly the formulation provided in Appendix E, while BDH-GPU extends conditional gating of states and logits. All models are trained with truncated backpropagation through time on sequences 2048 characters long, and carry their state (ρ matrix for BDH models and buffer of last 4096 KV-Cache entries (Dai et al., 2019) for GPTXL) between minibatches. BDH models are scaled only by varying the number of neurons and keep all other hyperparameters fixed, making them easy to scale. On the other hand, GPTXL were scaled in both the embedding dimension and the number of layers and required Dropout (Srivastava et al., 2014) tuning for optimal performance. We observe that BDH-GPU matches the GPT Transformer at all model sizes we have evaluated. Details on model hyperparameters and training setup are provided in Appendix B.2 much broader line of work on linear attention for the Transformer, initiated by (Katharopoulos et al., 2020) concerns applying linear attention in low dimension after appropriate preparation of keys and values. This is effectively technique for SSM state compression, and it is not clear whether it relates favorably to other SSM state compression techniques. An empirical study of the amount of information recoverable from SSMs with compressed state can be found in (Ben-Kish et al., 2025; Liu et al., 2025). general theoretical framework for analyzing the expressiveness of Linear Attention models with attention working with positive vectors can be found in the context of the FAVOR+ framework of the Performer (Choromanski et al., 2021). Finally, general state-space formalism for Transformer models admitting Linear Attention was considered in (Sun et al., 2023; Liu et al., 2025). Other types of Transformers. Variants of the Transformer with identical parameters in all layers, like the Universal Transformer (Dehghani et al., 2019), have number of desirable features, notably in terms of explainability and ease of defining metrics. The downside of sharing parameters between layers in the Universal Transformer is slight time overhead for the feed-forward network operations, when measured in FLOPS per parameter. The situation is similar in BDH-GPU. BDH-GPU has sufficient expressiveness to prepare keys and queries for the attention operation, so that the outcome of attention captures similarity measure between keys and queries corresponding to the outcome of class of Locality Sensitive Hashing (LSH) functions with very large number of buckets (cf. Subsection 6.1). The study of LSHbased KV-cache for the Transformer was initiated with the Reformer (Kitaev et al., 2020), and the LSH Transformer architecture introduced in the same work. Generally, the LSH Transformer is shown to rapidly approach Transformer baseline behavior in practice as the number of buckets increases. The class of LSH functions considered is not the same, but some intuitions gained in the study of LSH attention may carry over to BDH-GPU. Finally, several lines of work have been devoted to making the Transformer work with longer context windows. Two distinct approaches, which work notably well, are the soft-rolling context window of the TransformerXL (Dai et al., 2019), and hierarchical attention (Yang et al., 2016). The BDH-GPU architecture is, generally, amenable to some of these extensions to the Transformers attention mechanism, while also providing new ways to extend context length in more uniform manner. Networks with sparse activation. The use of the ReLU gate as systematic way to achieve sparse activation was, to our knowledge, first exhibited in (Haziza et al., 2025). 25 recent variant of the Transformer called Spark Transformer (You et al., 2025) relies on combination of topk operations and soft thresholding to provide reduction in both attention and feed forward network activations compared to the Transformer, achieving neuron sparse activation of 8%. Compared to our work, the method used therein to achieve activation sparsity effects is completely different (and rather involved). Beyond the question of sparsity, BDH-GPU is not more similar to the Spark Transformer than to the Transformer. Oscillatory SSMs. BDH admits an interpretation at the micro-level as an oscillatory state-space network, as we outlined in Subsection 2.4. The concept of Oscillatory State Space Models has recently been applied to time series analysis (Rusch and Rus, 2025), with the LinOSS model showing encouraging performance relative to other SSMs (such as Mamba and LSTMs) on tasks of long-horizon forecasting and time-series classification. Other than this, the use of SSMs with the form of an oscillator network has been limited to smaller scale studies. We are not aware of any successful application of oscillatory SSMs to the area of language models and reasoning in language, nor of oscillator network SSMs at scale whatsoever, prior to BDH. BDH unifies multiple lines of intuition found across existing models, offering coherent framework in which the components naturally align. The result is biologically plausible reasoning model with an interpretable structure and state-of-the-art performance that has been experimentally verified."
        },
        {
            "title": "5 Analysis: emergence of modularity and scale-free structure",
            "content": "Large-scale reasoning systems appear to benefit from hierarchical structuring into sub-modules. In Machine Learning, the usual approach has been to design such modular structure, by way of assigning roles and scales to different submodules explicitly. Many works have postulated modules capable of representing hierarchical relationships between features of objects, e.g., capsule networks (Sabour et al., 2017). Some models have attempted to capture intelligence by recreating elements of structure recognized in brain study, going so far as to try to map functional sub-networks of the brain with empirically identified function into specific sub-modules in the design of larger ML system, cf. (LeCun, 2022). In this work, we propose learnable system which ends up with modularity. We show how scale-free modular structure emerges naturally when the model is implemented by network with local graph dynamics. In this Section, we discuss the emergence of the structure of inter-neuron connections of BDH during training, while in Section 6 we look at its temporal activation patterns during reasoning inference. The rest of this section is organized as follows. In Subsection 5.1, we introduce basic concepts related to modularity and scale-free behavior of networks. We then look at the expressiveness of feedforward networks of BDH-GPU and their usefulness as signal propagation dynamics in Subsections 5.2 and 5.3. In Subsection 5.4, we show theoretically how modular graph structure, with appropriate community voting mechanisms, emerges as plausibly necessary element for the feed-forward networks DxE and DyE to function correctly. In Subsection 5.5, we look at the corresponding empirical properties of these matrices, and the scale-free and modularity properties of the corresponding graphs Gx of the underlying BDH graph dynamics. and Gy 5.1 Background: modularity and scale-free property of systems Importance of modularity for information propagation. Graph systems serving function related to information propagation tend to achieve modular graph structure, and rely on it to obtain the most desirable tradeoff between efficiency and accuracy of the system dynamics. Such emergence of hidden structure may be observed e.g. through topic specialization of system regions, or through the coordinated voting behavior among nodes which organize themselves into communities, admitting higher local density. This type of graph community self-organization has two main advantages over system with an explicit partition into subsystems. First, it allows nodes to belong to multiple communities, and to act as bridges between them. Second, it allows the scale and relationship between communities to evolve over time, as their relative importance changes or new connections emerge. Historically, the crucial role of emergent modular structure for systems tasked with efficient knowledge retrieval at scale was first observed in the context of the World Wide Web before the year 2000, notably in the transition from catalogue-based systems (DMOZ Open Directory Project, craigslist, etc.) to naturally evolving systems based on webs of knowledge (Wikipedia, etc.), interlinked topic-based communities (reddit, etc.), and reliance on evolving network link structure for assigning expert weights to nodes in voting process (Google PageRank, etc.). Formalization of modular properties followed soon after, with the mostly commonly used definition of modularity being proposed by Newman in 2006. The main theoretical reference for studies of modularity is the Stochastic Block Model (Holland et al., 1983) and its later generalizations, e.g., to hierarchical settings. While the definition of Newman modularity is 26 not (efficiently) constructive, it can in practice be closely approximated by greedy algorithms (Blondel et al., 2008) or spectral approaches (Massoulié, 2014). Scale-free property. The scale-free property of natural systems dealing with information processing is generally accepted as manifestation of their operation at criticality. This refers to operation within regime where they are both sufficiently stable to enable efficient information retrieval in the short-term, and sufficiently adaptable to be able change their behavior abruptly as new knowledge inputs become available, invalidating previous paths of reasoning or knowledge retrieval. The generally accepted definition of scale-free behavior of such dynamical system assumes that the likelihood of new piece of information (or other localized innovation to the system) to affect nodes of the system, for any < n, should by polynomially large in 1/n. For most information propagation dynamics, under certain uniformity assumptions, e.g., that the new piece of information arrives at uniformly random node of the system, usual necessary (not sufficient) condition for scale-free property is for the distribution of node degrees to follow power-law distribution. In the practice of applied sciences studying real-world network phenomena, and in the absence of the possibility to perform more in-depth analysis, power-law degree distributions are sometimes equated with scale-free behavior. One notable research application involves modeling of extreme events: understanding scale-free behavior allows researchers to make predictions about rare, large events from data on smaller, more common ones. For systems with known local graph dynamics, like those considered here, more refined analysis of scale-free properties are possible. We nonetheless also report heavy-tailed degree behavior as the most obvious lithmus test indicator of scale-free operation of the system. 5.2 BDH-GPU feed-forward network with the ReLU-lowrank block Low-rank matrices have been considered in multiple contexts of Machine Learning, from preference vectors to Internet latency estimation. In the setting of the Transformer, low-rank matrices form the basis of weight-matrix approximations such as LoRA (Hu et al., 2021). Its most important The ReLU-lowrank block of BDH-GPU captures different properties than the above settings. effects for BDH-GPU are related to noise reduction, and faithful representation of certain class of affinity functions on sparse positive vectors. This makes it suitable for use in combination with Linear Attention blocks. We discuss this point further in this Section. Definition of ReLU-lowrank. The parameters of BDH-GPU are concentrated in three matrices E, Dx, Dy. The encoder matrix transforms length-n vectors in the neuronal layer into length-d vectors in the hidden layer. The two decoder matrices {Dx, Dy} transform length-d vectors in the hidden layer back to the neuronal layer. We consider the ReLU-lowrank operation of passing through the encoder, one of the decoders, and ReLU gate (cf. Eq. (7)), mapping vectors Rn into fDE(z) Rn as follows: (cid:55) fDE(z) := (DEz)+ . (10) We note that the output fDE(z) (R+)n always, and that in BDH-GPU we also always have (R+)n. Expressiveness of ReLU-lowrank in BDH-GPU and MLP in the Transformer. single ReLU-lowrank block can be compared to single MLP block of the Transformer. different comparison provides closer matching of dimensions and structure of nonlinearities, by considering single ReLU-lowrank with respect to portion of the Transformer corresponding to the second MLP layer in an MLP block, i.e., starting with the hidden layer of neurons of the MLP in some layer l, skipping attention blocks, and followed by the first linear layer of the MLP in layer + 1, finally followed by the non-linearity (typically GeLU) applied in the hidden layer of neurons in layer + 1. Either approach to expressiveness is valid to the extent where we analyze similarities between one architecture with layers and the other with O(L) layers. In the spirit of universal approximation theorem frameworks, (deep) layer-L stacking of Transformers MLP block with ReLU activation, for Transformer latent dimension and MLP hidden layer dimension cD (e.g., for = 4), is eventually (i.e., for +) universal approximator for all vector functions up to dimension O(1) (Shen et al., 2022). similar universal approximation result eventually (i.e., for +) holds up to function dimension for residual ReLU-lowrank networks (Lin and Jegelka, 2018), however the convergence rate in is slower due to the smaller size of the hidden layer. These results translate directly to BDH-GPU architecture which also relies on ReLU with residual connections between layers. To summarize informally, for Transformer with latent dimension and BDH-GPU with hidden dimension d, we expect their feed-forward networks to be comparably expressive (though usually without strict mathematical equivalence) as function approximators for functions up to some dimension d, < < D, between and the Transformer can express richer class of functions, and between and n, BDH-GPU can approximate some functions, whereas the Transformer does not use such high dimension in its vector representations. We remark that in all cases, regardless of expressiveness of feed-forward mechanisms, BDH-GPU is set up so that it is only using inputs and producing outputs within the positive orthant, (R+)n (cid:55) (R+)n. The main point to consider is: what classes of useful high-dimensional functions in the positive orthant does ReLUlowrank naturally express? 5.3 ReLU-lowrank as signal propagation dynamics Error of low-rank approximation (without ReLU). Consider Rn as space spanned by fixed set of orthogonal unit basis vectors = {v1, . . . , vn}, called nodes. The low-rank operation can be used to approximate affinities between pairs of nodes, in the following sense. For given matrix Rnn, consider low-rank matrices Rnd, Rdn, such that := DE approximates pointwise. 15 Assume G1, 1. An application of the Johnson-Lindenstrauss lemma shows that the following bound holds in the infinity norm: Gmax = O((cid:112)log / d) (cf. e.g. (Udell and Townsend, 2019; Budzinskiy, 2025)). Then, for Rn = RV with z1 1, we have: Gz Gz+ = O((cid:112)log / d) However, no similar bound holds for Gz Gz2. Even for simple scenarios like the identity transformation = In, the best low-rank approximation admits O(1) additive error in the L2-norm for almost all inputs, and even greater distortion (approaching n) may appear in the L1-norm. (11) This makes the low-rank operation useful for determining affinity of pairs of coordinates in dimension n, but more problematic as vector transformation function. However, the ReLU-lowrank mechanism (Eq. (10) is able to suppress part of the noise of the linear low-rank map, allowing to approximate sufficiently broad class of non-linear operations. Expressiveness of ReLU-lowrank for Markov chain propagation. We will consider positive inputs R+n, focusing on sparse vectors. One important case concerns approximating Markov chain transformation (cid:55) Gz, for some R+nn. For such transformation in the positive orthant, adding the ReLU operation to the linear map does not change anything directly, Gz = (Gz)+. However, when considering low-rank matrix G, the non-linear transformation (Gz)+ can provide closer approximation of Gz for some classes of input vectors z, than the low-rank linear operation Gz. We start with the following illustrative example. Claim 5 (propagating Markov chain). Let be the random walk matrix of directed graph with out-degree (i.e., stochastic matrix with non-zero entries of 1/r in each row), and let be node (basis vector), v1 = v2 = 1. Then, for any ε > 0, there exists = O(r3 log n/ε) such that for some matrices Rnd, Rdn, we have Gv fDE(v)1 = O(ε). Proof (sketch). Let Rn(d1), R(d1)n denote matrices D, restricted to all but the last coordinate in dimension d. Pick D, so that Gv DEv < ε, where ε = ε/r, following Eq. (11) (we have G1, 1 by stochasticity of G). Further, set fixed bias, placing 1 on all entries of the last coordinate in dimension of D, and ε on all entries of the corresponding last coordinate in dimension of E. Taking into account this bias, we now have (Gv ε1) DEv < ε. For all coordinates vj such that vj other coordinates vj, we have vj and the claim follows.16 fDE(v) = 0. For all fDE(v) 1/r. Thus, Gv fDE(v)1 2εr, Gv = 1/r, and 1/r 2ε < vj Gv = 0, we now have vj DEv < 0, hence also vj 15Elements of can be computed pointwise by each pair of nodes: (v1, v2) (cid:55) := v1 16As point of elegance, we note that in this proof, vj fDE(v) 1/r, so fDE(v) was not an unbiased estimator of Gv. This is easily fixed in the first-order by introducing global multiplicative bias of (1 + ε) to the approximation, for example, substituting: (1 + ε)D (cid:55) D. DEv2 R. 28 The above observation shows how ReLU-lowrank deals with one specific class of graph affinity functions (random walks of adjacency of sparse graphs), for transformations of vectors which are nodes in our distinguished basis. We use this example as it is the simplest case which exhibits the benefit of threshold non-linearity: for basis vectors, the operation fDE captures basic propagation effect which is well known (in general) to require full-rank matrix Rnn if relying only on linear operations. Propagation and reinforcement of signal. The same thresholding approach, as discussed for Markov chains, turns out to be applicable to wider class of signal propagation dynamics. It consists in first obtaining positive-valued signal with heavy random noise, then applying negative bias, and finally using the ReLU gate to act as noise threshold. Any linear function can be represented with hidden layer of n2 nodes, through two matrices R+ns and R+ns, such that: = DE. The above holds in general, and we will refer to such representation of as having sparse hidden (synaptic) layer. We will consider now the question of expressing non-negative functions, (R+)nn. An example of valid representation of is given through node-edge incidence representation, ij, but usually this representation is not optimal in terms of the number of non-zero entries of and E. E, for some two In general, any low-rank approximation of can be equivalently expressed as DPDP matrices, PD, PE Rsd. We will consider the most common class of low-rank approximations obtained by taking PD = PE = (0, 1)sd/ d. Consider vector passing through the ReLU-lowrank operation, and the following vectors Rs, Rn: i,(i1)n+j = (i1)n+j,j = (cid:113) := Ez := DP i has the interpretation of signal being sent by node vi, then is the encoded message being passed through the If vT hidden layer of the network, and vT is the message received by node vj. 5.4 Modularity in BDH-GPU signal propagation We are now ready to capture the essence of the signal propagation and reinforcement capability of the ReLU-lowrank system. To describe the conditions under which neuron is able to decide whether it should, or should not activate. By standard analysis of independent Gaussians, we have the following probabilistic statement, under random choice of . Claim 6 (selective neuron activation). Suppose that the signal of is uniformly concentrated on set of nodes of the , 1+κ hidden layer, i.e., for some subset of indexes of the hidden layer, we have α = 0 for α A, and α [ 1κ ] A for α A, so that u2 [1 κ, 1 + κ] for some small constant κ 0. Suppose each node vj is connected in to some set of nodes Bj in the hidden layer, Bj = { β : j, β = 0}, and let these connections weight be drawn uniformly ] for β Bj. Let Cj = Bj. Define the ratio: j, β [ 1κ Bj , 1+κ Bj (cid:115) ϱ := Cj Cj Bj . Pr Then, there exists an absolute constant > 0, such that for any value of wj (where we recall that := DP u), we have: (cid:105) (cid:104) wj (1 κ)2ϱ c(cid:112)log / = 1 O(1/n). (12) Thus, the value of wj can be used by neuron to obtain an estimation of ϱ, and apply threshold to activate accordingly. (cid:105) (cid:104) wj (1 + κ)2ϱ + c(cid:112)log / = 1 O(1/n) and Pr Proof. Observe that wj = (P Lindenstrauss to vector inner products gives Pr (P u). As (cid:104) wj j, [(1 κ)2ρ, (1 + κ)2ρ], the claim follows. Since j, j,) j,2 [1 κ, 1 + κ], standard application of Johnson- = 1 O(1/n) for large enough. (cid:105) c(cid:112)log / 29 Figure 8: The ReLU-lowrank feedforward network of BDH-GPU allows neurons to activate when triggered by activation signals in its own community. (a) Illustration of the selective neuron activation pattern in the proof of Claim 6, showing the activation decision of node vj (left) based on active set in the sparse hidden layer. (b) Illustration of Eq. (12) showing the relationship between sizes of sets in the sparse hidden layer: active set A, set Bj connected to neuron vj, and the intersection Cj = Bj: neuron vj1 becomes active, but neuron vj2 does not. The ReLU-lowrank operation fDE, after adding appropriate negative bias, can thus be used to propagate positive affinity functions on input vectors, performing the following form of thresholding: neurons in the output layer individually compute form of local F-score ϱ given by Eq. (12) of the activation of the positive sparse hidden layer, and decide based on it whether they are good match for the output activation; if the threshold condition on ϱ is not met, the neuron does not activate in the output vector (see Fig. 8 for an illustration). Equation (12) naturally coincides with pattern of communication within network graphs admitting positive Newman modularity (Newman, 2006), allowing nodes vj to correctly receive messages which in the hidden layer primarily reached denser cluster of containing vj. For specific illustration, let be an undirected k-block stochastic block model (SBM) network (Karrer and Newman, 2011) with blocks of n/k nodes each, in-block edge density and out-of-block edge density < p. We put := DE = 2, i.e., the first connection layer of is = and the second connection layer is also = H. Suppose that is random SBM graph with positive Newman modularity separated from 0, i.e., let µ = k1 p+(k1)q > 0. Following Claim (6) with κ = 0, we can find ReLU-lowrank representation to achieve communication scheme on G, such that message sent from one node = vi activates node vj when and are in the same block with probability 1 O(1/n), and with probability O(1/n) otherwise, when µ > 1 (cid:112)log / d. pq We thus make the following intuitive observation. Observation 6 (in-cluster signal reinforcement). The ReLU-lowrank representation of BDH-GPU(n, d) is sufficient to represent in-cluster information spreading dynamics in models of graphs with constant in-cluster density and arbitrarily small positive modularity (such as the k-cluster Stochastic Block Model) when d/ log = ω(1) is an arbitrarily slowly growing function. While Claim 6 and Observation 6 are made with reference to an almost-uniform distribution of signal on the set of nodes of the middle layer, can have (and in practice does have) distribution of density which is non-uniform, e.g., going across = O(log n) different clustering scales, with (1/a)-fraction of the signal represented at each scale. This allows neurons in the output layer to combine smaller number of strong signals in its local cluster, with larger number of weaker ones spread more globally. Such an approach coincides with the observed structure of the graph DE, discussed in Subsection 5.5. Supermodularity on input perturbation. We clarify how the properties of function fDE : (R+)n (R+)n relate to the previously discussed ability to make an input signal resonate within module in graph with hidden modular 30 structure. First, note that fDE is subadditive function, but is not submodular in general with respect to the set of coordinates of its input vector. In some of the regimes in which it appears to be operating, locally fDE exhibits form of behavior opposite to submodularity, referred to as supermodularity, or increasing returns of adding new coordinates to the input vector. This is already implicitly captured by Claim 6, but we can consider simpler example. Take variant of the setting from Observation 5 with the same choice of G, and let (R+)n and biases of be chosen so that all coordinates of DEz are approximately equal to 1.5/r o(1) (this can be done by choosing e.g. zj = 1/n). Then, fDE(z) = 0, and for any vi, vj , fDE(z + vi) = 0 a.s., fDE(z + vj) = 0 a.s., but fDE(z + vi + vj) has non-zero coordinates a.s. with values approximately 1/2r, for all nodes vk which are common out-neighbor nodes of vi and vj, i.e., for all such that G(vi, vk) = G(vj, vk) = 1/r. This mechanism generalizes to finding common neighborhoods which have many connections to two given subsets of nodes, Va and Vb. In setting where the considered affinity is bi-directional (e.g., symmetric matrix), this corresponds to finding shortcut nodes, allowing to go from Va to Vb. It follows that the neighborhood-reinforcing nature of the threshold dynamics of BDH-GPU, which plausibly follows from the logic of its role in inference and from the needs for an efficient computational process, is starkly different from the more often studied submodular behavior of threshold and cascade dynamics on real-world networks (Kempe et al., 2003), and plausibly, much less smooth when considered as dynamical process."
        },
        {
            "title": "5.5 Empirical findings: parameter distribution in ReLU-lowrank matrix products",
            "content": "We consider the matrices (in the same way Dy and Dx) and matrix obtained after training of BDH-GPU models, and used in the ReLU-lowrank operation Eq. (10), fDE(z) = (DEz)+. Choice of prior of matrix parameter distributions. Following the discussion in Section 5.4, we expect matrix := DE to reflect the clustering (modularity) structure of the neuron-neuron communication graph. Any plausible parameter distribution of matrix must therefore allow heavy-tailed distribution of entries. At the same time, Gaussian noise term is inherent to low-rank matrix representation, and needs to be taken into account together with this heavy-tailed distribution. We now provide somewhat more fine-grained explanation, which leads to the prior on the structure of matrix as given by Eq. (13). Consider training set-up in which the ReLU-lowrank operation described by matrix is treated as an approximation of the same operation, governed by high-rank matrix G, with (z) := (Gz)+. Considering this block in isolation from the rest of the training system, the training of matrices D, goal corresponds to learning an approximation of , with Rn,d, Rn,d, such that (z) (z) holds for some class of vectors z. For the rest of this analysis, we will consider the function as ground truth reference for the intended operation of the ReLU-lowrank block. This type of analysis can be seen as plausible over short time spans in later phases of training of BDH-GPU model, i.e., once individual neurons in Rn have started to admit semantic or functional meaning, and so when function DE describes property of the problem being solved in (frozen) concept space, and not co-learning process between the representation of the concept space in Rn and the functions applied to it. We can represent := DE, where Rn,s, Rs,n, with = O(n2), are in general matrices of rank n; we have (z) := (DEz)+. Without loss of generality, we can choose from among the possible representations one with the following distribution of positive and negative elements: (R+)n,s, = Ee Ei, with Ee, Ei (R+)s,n. We will write: = Ge Gi, where Ge = DEe, and Gi = DEi. The main purpose of the chosen representation = DE = D(Ee Ei) is to have matrices D, Ei, Ee with much smaller outlying elements compared to matrix G, which leads to more justified conclusions about the uniform nature of the noise introduced by the low-rank decomposition.17 Assume now that we learn to approximate function with fDE by trainable matrices D, through the following low-rank scheme: = DE := (BD + DP )(P + BT where Rs,d is non-parametric and the result of random sampling an almost-orthonormal random projection so that Is (e.g. (0, 1/ d)s,d), and BD, BE Rn,d represent trainable additional terms for compensating E), 17For specific example, one very broad class of matrices is given by the product of sparse matrices D, E, in which each ), and all i,α{E}α,j, may be much less ij = O(1). This type of scenario captures the expressiveness of set intersection for bag-of-words s-element row of (column of E) has at most non-zero elements, each with value bounded by O(1/ remaining elements of these matrices are equal to 0. The resulting elements, uniform, only satisfying models for language, or expressiveness of hub label representations for measure of node proximity in directed graph. ij = (cid:80) α error or introducing bias, with the goal of minimizing some loss function L(f , fDE). The terms BD, BE compensate the error introduced by the approximation Is, after the ReLU operation. Let := = Is + δI + δQ, where δI Rss is diagonal error matrix, and δQ Rss is non-diagonal (hollow) matrix. We have: = DE = (Ge Gi) + DδI (Ee Ei) + DδQE (cid:124) (cid:123)(cid:122) (cid:125) εQ + (BDE + DBT E) (cid:125) (cid:124) (cid:123)(cid:122) εB . Since all elements of D, Ee, Ei are non-negative and Iδ is diagonal, we can represent elements Gij, for i, 1, . . . , n, as follows: Gij = (1 + εe δ ij)Ge ij (1 + εi δ ij)Gi where εe δ ij = O((cid:112)log / d) and εi ij has the interpretation of positive ground truth elements of G. The term Gi ij + εQ ij + εB ij (13) δ ij = O((cid:112)log / d) have the interpretation of small multiplicative distortion. Following (13), we expect the elements of to be distributed as the sum of four different distributions. The term Ge ij has the interpretation of negative ground truth elements of G; its use in combination with the ReLU mechanism can be interpreted as inhibitory action. Both of these terms are subject to slight multiplicative distortion. The term εQ ij has the interpretation of non-trainable noise (which depends only on D, and the random choice of ). Under reasonable assumptions on outlying elements of D, E, it is form of almost-Gaussian symmetric noise inherent to the considered class of low-rank projections, εQ ij (0, σQ), for some σQ R+, and the expected value of this noise is typically very close to 0, even when considering the expectation of εQ ij conditioned on known values of εQ ij for small number of indexes (i, j) in the matrix. Finally, εB ij is trainable term, whose norm tends to 0 as increases. We expect it to have the interpretation of bias used to offset the low-rank Gaussian noise and perform denoising in the ReLU-gate, as previously discussed in Section 5.3. Because of the action of the ReLU gate, we plausibly expect the distribution of εB ij to be skewed towards negative numbers, with 0 > EεB ij σQ. From the above discussion of the four terms of the sum in Eq. (13), we see that only one of these terms, Ge ij, is expected to take values much larger than σQ with non-negligible probability. We reach the conclusion that part of the relevant signal of is concentrated in the right tail of large positive matrix entries of G. Hypothesis 1 (right tail contains signal). Consider the interpretation that the ReLU-lowrank transformation (cid:55) (Gz)+, with = DE, has learned to act as an approximation of some other operation (cid:55) (Gz)+, where has no low-rank constraint imposed on it. Then the right tail of the distribution of matrix elements of corresponds to the right tail of the distribution of elements of G, starting from some positive threshold value σQ, associated with the noise of the low-rank decomposition. Formally, for almost all pairs of indices i, 1, . . . , such that Gij σQ, we also have Ge ij σQ. The converse implication, that Ge ij σQ implies Gij σQ, also plausibly holds under some stronger assumptions on the form of biases εB ij which may follow from minimizing training error for the specific inference task considered. This direct method of decoding from does not extend from the right tail towards the center of the distribution. For the choices of n, we make, we expect the term dominating most elements of matrix to be εQ ij. For example, when is stochastic matrix, we expect to have σQ = O(1/ d) (cf. Eq. (11) for the corresponding infinity-norm bound, εQ ij = O((cid:112)log / d)). With (cid:80) i,j = for stochastic matrix, we expect the right heavy tail of the d) elements (out of the n2 matrix elements of G) which are clearly separated element distribution of to have Ω(n from the Gaussian noise. i,j Ge We confirm empirically that the right tail of G, defined as above with respect to threshold σQ, turns out to contain non-negligible portion of the parameter capacity of matrices D, E, even for very small models (10M to 100M parameters). Experimental setup. We prepared parameter matrices of 24M-parameter BDH-GPU model configured with = 4 heads and = 8 layers, = 213 = 215 neurons, and hidden low-rank dimension = 256. We considered the weighted neuron-neuron interaction graph, having the encoder-decoder matrix pair = DxE as its node adjacency matrix on the set of neurons = 1, . . . , n. For uniformity, we subsampled by picking node subsets (a), {1, 2, 3, 4}, associated with each head, and considered the weighted subgraphs G(ab) = {V, {(u, v, Guv) : (a), (b)}}, with G(ab) Rnn where = n/h = 213, each having (n)2 = 226 weighted edges. Figure 9: (a) Heavy-tailed element distribution and modularity analysis of the excitatory neuron-neuron connection graph contained the encoder-decoder matrix G. Distribution of elements of the encoder-decoder matrix Rnn of BDHGPU model with = 8192 neurons and = 256: histogram freqG (x), its symmetric part freq symmetricG (x) := min{freqG (x), freqG (x)}, and distribution skew freq skewG (x) := freqG (x) freq symmetricG (x). (b) Estimate (lower bound) of Newman modularity of matrix β for different values of β, plotted as function of the number of non-zero β. Modularity of random graph baselines are provided for reference, for the G(n, m) model with the same elements (edges) of β, where P1, P2 (0, 1)nd. The number of edges as β, and for matrix (P1P modularity estimates were obtained using the community structures returned by the Louvain algorithm, in the best of 5 clustering runs with different random seeds. 2 )β with the same number of edges as We repeated the experiment 5 times using models pretrained with different random seeds. Findings. For all of the 5 models we pretrained for this purpose, exactly 3 out of the 4 encoder heads and all decoder heads adhered to the prior on parameter distribution given by Eq. (13), showing good correspondence for 12 out of 16 of their parameter sub-matrices G(ab). β We continue the discussion in this Section for one specific matrix G(ab) of one specific pretrained models, which was chosen as representative. The example we choose has = b; and so the matrix G(ab) has an interpretation as G[Va], i.e., the subgraph of induced by vertex set Va, which enables us to visualize the graph G(aa) more easily on its vertex set Va. We refer to the representative object of our study, i.e., to the matrix G(aa) of the selected model, as G. For any matrix and β 0, we denote by Aβ the matrix cut off at threshold β, i.e., Aβ ij = Aij if Aij β, and Aβ ij = 0 otherwise. The distribution of elements i,j is presented in Fig. 9 (a). We find that the observed distribution corresponds well to the prior expected of it by Eq. (13). We determine the threshold value β 0 at which we expect to capture signal, β, following Hypothesis 1. We find (from Fig. 9(a)) that the separation from noise happens for this specific matrix at β1 1.2, at which point the right heavy tail begins to dominate. However, already for much smaller values of β we find that β has high modularity, and this actually increases as more non-zero values are added to β for smaller β, up to maximum at β2 1.0 (Fig. 9(b)). Even for much smaller values of β, the modularity of β remains almost constant up to well above 220 non-zero matrix entries on the = 213 nodes considered. The modularity of the baselines, of random graphs or random low-rank matrix products, quickly drops to 0 in this regime. This should be compared to the total number of parameters of the matrices Dx, corresponding to G, i.e., 2213 28 = 222 parameters. complementary analysis of the inhibitory signal, for similarly defined matrix β, also finds that this structure has high modularity. In auxiliary experiments, we looked at basic graph parameters of matrix β, treated as directed graph on its set of nodes. We set β = 1.2, obtaining = 46820 non-zero entries (edges) in β. We found that has heavytailed, power-law-like degree distribution, with generally more concentrated out-degree than in-degree (Fig. 10(a)). Generally, this finding is consistent with expectations as to the structure of network with positive modularity. The difference of inand out-degree distributions, while plausible and prevalent in real-world information dissemination networks, was not considered in Subsection 5.3. 33 (a) (b) Figure 10: (a) Unweighted in-degree and out-degree distribution for the = 8192 neuron nodes and = 46820 edges of matrix β with β = 1.2. The distributions exhibit power law distributions, with different exponents, the out-degree distribution being more concentrated. (b) Visualization of graph β, hinting at its core-periphery structure. β (Fig. 10(b)) exhibits core-periphery structure. This is again consistent with the Finally, visualization of expected modular structure. Empirical Finding 3. We confirmed that during training, graph structure with positive modularity appears in BDHGPU model parameter matrices DxE and DyE. This modular structure plausibly follows from the networks inference function, and specifically from the cluster-aware information propagation dynamics supported by the ReLU-lowrank mechanism (Observation 6). We also observed that for all of the studied models with = 4 heads, 1 encoder sub-matrix out of 4 has no heavy positive tail, and generally appears to capture form of inhibitory structure Gi from Eq. (13). Since we have not provided convincing mechanisms for isolating negative signals in and these are easily confounded with the bias term εB, we omit this case from discussion. We remark that the apparent need for passing activations through such separate inhibitory circuit is one of the most evident explanations for why introducing (a small number of) heads to BDH-GPU provides an improvement in model quality."
        },
        {
            "title": "6 Analysis: linear attention, sparse positive activation, and monosemanticity",
            "content": "6.1 Macro-expressiveness of attention in BDH-GPU The attention mechanism of BDH-GPU can be described at coarse-grained level as transformation mechanism for key-query-value vectors, similar to that in the Transformer. This description is complementary to the interpretation of the BDH-GPU attention mechanism at the micro-level of correlations between neuron pairs, which we defer to Section 6.2, which provides more insight into the way activation vectors used by BDH-GPU relate to the concept space of the model. We compare the attention mechanism of BDH-GPU with the attention mechanism of the Transformer, describing both as reflections of general attention mechanism. Specifically, we explain why, and up to what context length, the linear 34 attention mechanism of BDH-GPU plausibly fits into macro-expressiveness frameworks of attention designed for the Transformer (based on RASP). Basic properties of BDH-GPU attention. The key-query space for BDH-GPU is Rn, the same as its neuron dimension, rather than the small dense dimension used by the Transformer. The keys and queries used by BDH-GPU are given by positive vectors, in (R+)n, and are expressed by the same vector xt,l (noting that at time t, xt,l is used as query, and only xτ,l, for τ 1, are used as keys). Value vectors of BDH-GPU remain in the small dimension, Rd, which at some model scales is comparable to the dimension used for attention values in common configurations of the Transformer. The relationship between softmax-based attention of the Transformer, regarded as low-dimensional kernel for general linear attention, and linear attention for vectors in the positive orthant, was considered in framework called FAVOR+ (Choromanski et al., 2021). Here, we provide few complementary (simpler) observations, sufficient to grasp the main effects of the ability of Linear Attention to distinguish facts in context. State capacity vs. distinction capacity. The matrix ρ Rnd, which is used to represent state for each layer of BDH-GPU, should theoretically have sufficient capacity to store O(n) value vectors in Rd if considered as lookup table for values. We now remark that its actual capability of distinguishing facts using the linear attention mechanism is also asymptotically close to n. Attention is mechanism of associative memory which, given series of key-value pairs ((k1, v1) . . . , (kt, vt)) (Λk Rd)t, query Λq and an affinity function ϕ(, ) : Λq Λk [0, 1] between the space of queries and keys, returns the attention value: at = (cid:80)t1 τ =1 ϕ(q, kτ )vτ (or normalization thereof). With BDH-GPU, we consider value vectors Rd, where is small. The spaces of keys and queries may be assumed to coincide as Λ = Λk = Λq, and we consider in general single key-query sequence, given by (kt)tN:18 at = t1 (cid:88) τ =1 ϕ(kt, kτ )vτ (14) This key-query space Λ may be considered as an abstract space, and represented in any way which is convenient, for as long as the affinity function ϕ(kt, kτ ) is preserved. For example, when the keys and queries are sampled from finite (though possibly extremely large) set, there also exists some vector space dimension ν (possibly extremely large) and function mapping : Λ Sν, where Sν = {z Rν : = 1} is the unit sphere, such that the scalar (dot, cosine) product in Sν satisfies (kt) (kτ ) = ϕ(kt, kτ ). In other words, any affinity function ϕ becomes linear when represented in sufficiently high dimension, subject to suitable preparation of its arguments with function . With ν extremely large, Sν is sort of Platonic ideal of space in which the attention keys and queries live, with no relation to any specific model. This type of argument, often used in considerations of Support Vector Machines, is linked to two challenges: (1) ensuring that the dimension actually considered by the network (in our case n) is high enough compared to the hypothetical dimension (ν), and (2) ensuring that suitable preparation function exists and is easy to learn for the model.19 We now explain when the dimension can be considered sufficient, and what types of keys can be prepared by BDH-GPU. Expressiveness of linear attention in dimension n. The Linear Attention mechanism aggregates key-value correlations over time. In general, the associated rate of accumulation of noise is manageable, up to the approximate scale of between = Ω( n) and = O(n) key-value facts stored in the attention of given layer. We make the following statement about the Linear Attention mechanism in general. Claim 7 (informal statement). The mechanism of Linear Attention, applied in dimension Rn, can approximately express an attention affinity function for up to = O(n) key-value pairs in context, with values having comparable L2-norm, under moderate assumptions on weak correlation of historical keys and uniformity of the expressed affinity 18This assumption is known to have moderate practical implications for trainability. In this specific discussion, it is without loss of generality, since one can consider Λ = Λ1 Λ2 . . . Λt, and consider each ki as chosen from Λi, defining affinity ϕ(kt, kτ ) : ΛtΛτ [0, 1] appropriately to handle successive keys and queries (effectively describing general form of positional embedding). 19The Transformer can also be positioned in the same SVM framework: the Transformers attention represents form of kernel trick for one specific affinity function ϕ, with the kernel used to approximate it being the exponential function (in the case of softmax attention). 35 function. Without such assumptions, Linear Attention can compute the correct affinity up to at least = Ω( n) key-value pairs in context, except for negligible fraction of possible inputs. Keys and queries need to be suitably prepared beforehand. The formal statement and proof is provided in Appendix C.2. The above claim captures the expressiveness of Linear Attention in dimension Rn, subject to some way of preparing keys and queries in Rn by the model in blocks preceding the attention block. model using Linear Attention has to learn its own way to prepare keys. In fact, different natural approaches to key preparation, for example using random projections or hashing on set of vectors, lead to the same asymptotic statement of Claim 7. (In the proof in the Appendix, we chose to use particularly simple one.) The specific way of preparing keys used (learned) by BDH-GPU for its Linear Attention is particularly interesting. Except for the effect of RoPE rotation, which introduces negative positional effect in the affinity of keys and queries, BDH-GPU uses activation vectors (keys, queries) with only positive coordinates to represent its keys. We discuss some aspects of how the positive activation vectors of BDH-GPU relate to Linear Attention. Preparation of positive keys for Linear Attention. Activation vectors of BDH-GPU belong to the positive orthant, and are often sparse. The interpretation of such vectors depends on whether we consider the positive orthant to be valid shape for the latent concept space of the considered task (in this case, language and reasoning), or whether the task has to be embedded into such space. For language, this would be question of whether word2vec-like internal representation of the concept space by the model has an inherent advantage over bag-of-words-like representation, especially when expressing concept affinities in attention. We note that latent representation of key and query vectors in the positive orthant is natural for any problem which is amenable to attention. In the discussion of general attention given by Eq. (14), we noted that the affinity function ϕ takes values in [0, 1], and we considered an embedding of set of key vectors k1, . . . , kt into Rν such that (kt) (kτ ) = ϕ(kt, kτ ) 0. Given this condition on non-negativity of dot product on all pairs among the vectors considered, we could have, without loss of generality, used an appropriately rotated embedding so that (kτ ) (R+)ν, thus directly reducing the problem of general attention to problem of linear attention in the nonnegative orthant. The question which remains is subtle one: whether this type of embedding of the latent space of language and reasoning in (R+)ν is natural, i.e., preserved over long periods of time of inference and training, notably longer than the short window of context used for Transformer-like attention. In the rest of the paper, we are generally inclined to assume that representations in (R+)ν of concepts, combinations of concepts, and density distributions over such combinations of concepts, are universal to language and reasoning. We limit ourselves to very brief discussion of way to represent attention keys with positive vectors for problems for which such concept representation is not natural. Using LSH to move key vectors into the positive orthant. Locality Sensitive Hashing (LSH) is one technique for converting arbitrary vectors in lower-dimensional space Ra, for some fixed N, into vectors in (R+)n, in way which can be used to describe certain sharp-boundary affinity functions ϕ in Ra. Consider an matrix represented as fixed random vectors λ1, . . . , λn Ra, and corresponding sequence of appropriately chosen gating functions γ1, . . . , γn : R+. For vector Ra, we define: b(v) := γ([λ1 . . . λn]v) = (γi(vT λi))1in. (15) Each i-th element of vector thus corresponds to the outcome of the i-th bucket of LSH. The bucketing function may now be used to prepare queries and keys as attention inputs. If γi is {0, 1}-valued threshold function, then, for q, ki Ra, b(q)T b(ki) is an attention affinity function between and ki, equal to the number of LSH buckets shared between and ki. Observation 7. The LSH vector affinity function b, given by Equation (15), using buckets on vectors in Ra for some N, can be expressed through Linear Attention with attention keys in the positive orthant (R+)n. In the ReLU-based setup considered in BDH-GPU, an appropriate function is plausibly easy to learn. LSH is sharp-boundary technique, well-suited for finding k-nearest-neighbors of queried vector in set of keys. Hence, the class of attention affinity functions, naturally expressible using BDH-GPU, also includes such sharp functions. Attention in the positive concept space of language and reasoning. BDH-GPU uses the positive orthant (R+)n as its latent space for representing combinations of concepts in its activation vectors. Attention keys and queries are prepared entirely in this positive orthant. 36 When representing task of reasoning or language inference in high-dimensional space, positive activation vectors in (R+)n have natural interpretation of convex combinations of concepts. Such convex combinations of concepts may represent both semantically connected concepts (bags-of-concepts), and mixed states of uncertainty between unconnected concepts. In this interpretation, positive vector is considered as state of certain knowledge when its L1-norm and L2-norm align closely. Note that for (normalized) probability vector, the only vectors for which L1-norm and L2-norm coincide precisely are distributions concentrated on single coordinate. n1 , 1α n1 , . . . , 1α n1 ) and x2 = ( 1α Linear Attention of BDH-GPU is capable of amplifying very small differences between keys in the L1-norm when matching queries to keys. Consider, for instance, two probability distribution vectors x1, x2 (R+)n, where x1 = (α, 1α n1 ), for some 0 < α < 1. Now, vectors x1 and x2 almost coincide when treated as probability distributions, x1 x21 = O(α) = x1 x2TVD. However, they are extremely different when considered as keys for the Linear Attention mechanism, with x1 showing very weak affinity to x2: x1 Observation 8. In key-query matching, the Linear Attention mechanism of BDH-GPU is able to separate positive keys which are close in the L1-norm, strongly amplifying L1-norm differences of activation vectors. x2 = O(α2n1)x1 n1 , . . . , 1α n1 , α, 1α x1. This mechanism can be treated as complementary to the propagation dynamics of positive activations in the feedforward network, discussed in Section 5.3. Natural support for long context. There is no bound on context length in BDH-GPU, so the actual = Ω( n) to = O(n) equally important facts that BDH-GPU model can distinguish in each layer in view of Claim 7 do not have to correspond to the latest facts seen in context. For example, if, for some layer l, mechanisms from lower layers deem given entry to be irrelevant for layer l, and provide an extremely weak attention value for this layer, and this key-value entry is effectively seen as omitted. This mechanism corresponds to weaker signals in layer which needs to take no action on given input, e.g., does not have to remember it (cf. Fig. 14). Indeed, empirically we observe progressive de-noising of state in the higher layers, with only small fractions of input tokens requiring significant key-value state update in the middle layers across the entire spectrum of state ρ of neurons. As result, the middle and higher layers of BDH-GPU may, in principle, have unbounded look-back on context. Nonetheless, as context length increases, we find that damping of historical signals over long sequences is necessary in BDH-GPU to avoid overwhelming the model with noise from stale context. For the vanilla version of the architecture, we found that RoPE combined with ALiBi provide sufficient remedy, and model performance improves as context length increases. More advanced techniques for BDH-GPU, related to selective forgetting, state compression, or other forms of state optimization, can also be added to the architecture. 6.2 Micro-interpretation of attention in BDH-GPU BDH maintains its state in the matrix σ that has clear interpretation as synapse weights that connect neurons (cf. Section 2). On the other hand, BDH-GPUs state ρ is matrix. To perform the analysis for BDH-GPU, in this section we recover σ from the relation: σt1,l = (cid:88) τ <t yτ,l1xτ,l tτ (16) We first analyze the neuron relationship graph encoded by matrix σ. As explained in Section 2.2, σ can be interpreted as graph of context dependent implications between and y. We compute the σ matrix for 0-th head at layer 5 of an 8-th layer network trained on Europarl translation corpus (Koehn, 2005) (we provide more details in Appendix B.3). We filter out negative entries which are introduced by the RoPE positional embeddings (Su et al., 2024) and enforce small positive threshold on remaining values to further sparsify the network structure. We plot the histograms of neuron inand out-degrees, unraveling scale-free network structure. Encouraged by the emergent network structure, we have identified few synapses that are activated at recognizable concepts, we show examples in the next section. 6.3 Empirical findings: monosemantic synapses We have identified in the σ matrix entries (synapses) that show activity whenever currency name or country name, both frequently occurring in the Euro-parliament transcripts, is present in the processed sentence. We have identified the synapses by searching for entries in σ that have predictive power at separating sentences containing concept from contrast sentences. We present few examples in Figure 12. We note that the synapses strength changes abruptly after words that are related to each concept. The same synapse is activated for concepts in both French and English 37 Figure 11: BDHs state σ encodes neuron connections as scale-free graph showing clear heavy-tailed (power-law-like) degree distribution. sentences, even when the words used are different (e.g. livre sterling vs British Pound). Synapse selectivity to semantic context stems directly from sparsity of neuron activations as shown in Fig. 13. To confirm the selectivity of the synapses, we have generated, using ChatGPT, 50 sentences relating to European currencies, and another set of 50 sentences speaking about European politics, but not mentioning currencies. onesided MannWhitney test revealed that sentences relating to currencies received significantly higher Currency synapse values than those without the currency concept (U = 2368 with Uopt = 2500, < 1014). The rank-biserial correlation was 0.86, further confirming association between Currency concept presence and synapse value. 6.4 Empirical findings: sparse neuron activations Sparsity of signals is often prerequisite to their interpretability. In section 6.3 we have shown that BDH has monosemantic synapses, selectively activated by occurrences of specific concepts. In this section, we experimentally show that neuron activity correlates with signal predictability: fewer neurons are active, or equivalently, layer activations become sparser, for more predictable input signals. We have trained BDH-GPU model with = 65536 neurons, = 256, = 4 layers, and tokenization on letters of the Latin alphabet, to perform single synthetic next-token prediction task. The input sequence started with fixed 13-letter warm-up sequence, followed by 8 repetitions of an 8-letter random word (fact), with the same pattern repeating every 13 + 8 8 = 77 letters. In Fig. 14, we show neuron activity patterns. We can notice that neurons in higher layers are active during warm-up and fact introduction, then become quiet. We then group neurons by their RoPE frequencies and find that largest difference of activity during memorization and repetition is shown by the slow-acting neuron population. From biological standpoint, sparse and surprisal-driven neuron activation lowers energy consumption despite fluctuations in low level percepts (in the experiment tokens are changing at every timestep), neurons in higher layers are inactive and do not expand energy. From Deep Learning perspective, it has been recently shown that input complexity is related to predictability of internal representations of Transformers (Herrmann et al., 2025). BDH makes this very explicit and does not require separate prediction network: the predictable steady-state consists of zero activations, and input complexity entails neuronal activity. This suggests that BDH, natively, at neuron level, implements mechanisms reminiscent of adaptive computation time (Graves, 2017) and conditional computation (Cho and Bengio, 2014; Shazeer et al., 2017), used in modern Transformers to lower computational effort during inference. Finally, sparse activation vectors in BDH imply that potentiation of specific synapses occurs rarely during inference. This is useful from the point of view of interpretability, noise reduction in Linear Attention state, and opens the door to simplified and compressed representations, notably for state and for gradient backpropagation DAGs. 38 Figure 12: Evolution of values set by BDH-GPU on 2 specific synapses which we have named (following their interpretation) as currency synapse and country synapse, relating to concepts naturally present in European Parliament transcripts on which the model was trained. We can notice that mentions of country or currency names result in an increase of the respective synapse value, indicating stronger presence of the concept in the context. Moreover, the synapses consistently became activated in both French and English, confirming the (notice how it reacts both to British Pound and livre sterling). For visual clarity, we indicate changes that clear small threshold with the character (the changes in activity when the system is processing the translation of source sentence tend to be small). 39 Figure 13: Sparse updates to synapses related to meaningful concepts stem from sparse neuronal activations. BDH-GPU maintains in its recurrent state currency synapse (a concept naturally present in the Europarl corpus, see also Fig. 12). The synapse is updated using Hebbian learning rule when activity in activations at preceding layer (4 in the example) leads to firing of neuron in the next layer (5). (a) (b) Figure 14: Neurons in BDH-GPU are less active (signal is sparser) when the input is predictable. The input sequence started with fixed 13-letter warm-up sequence, followed by 8 repetitions of an 8-letter random word (fact), with the same pattern repeating every 13 + 8 8 = 77 letters. (a) Fraction of neurons with non-zero entry yt,l in different layers l, with fact memorization effect noted through increased activation level in layer 2. The activation in layer 2 has 4.0%7.5% non-zero entries during memorization and approximately 2.5% non-zero entries during repetition. (b) Detailed breakup of activation sparsity in layer 2, with neurons bucketed into equal fractions by their RoPE phase: freq0 [1, 4], freq1 [4, 16], freq2 [16, 64], . . ., freq7 [16384, 65536]. The slow-acting half of the neuron population (freq4 freq7) exhibits the largest amplitude ratio between peak activation during memorization and repetition phases."
        },
        {
            "title": "7 Playing with the Hatchling",
            "content": "7.1 Model merging: concatenating two models Updating models with up-to-date knowledge and expanding models knowledge-base will become crucial in practical applications of AI. On possible solution is model composability, potentially allowing building of larger models by assembling number of smaller, specialized models into larger, more powerful one. natural hope for such system would be the achievement of more is different than sum of its part effect. In the following experiment we are showing that doing so is relatively straight forward with BDH-GPU. This is because BDH-GPU can be scaled by varying only the number of neurons n. In this section we explore whether we can create larger models directly concatenating smaller models trained on disjoint subsets of data. Details in Appendix B.4. We have experimented with the following simple model merging procedure: 1. Train base model on chosen language pair. In the experiment we have used English-Spanish (En-Es) translation data, and have trained model with = 24576 neurons (19M parameters). 2. Clone the base model and continue training on two datasets: English-French (En-Fr) and English-Portuguese (En-Pt). 3. We then merge the weights of the En-Fr and En-Pt models to create new En-FrPt model with = 245762 = 49152 neurons (38M parameters). To create the merged model we: (a) concatenate all parameter tensors that have an dimension (e.g. Dy, Dx, E, RoPE frequency buffers) along their dimension, (b) average all other parameters (e.g. token embeddings and token prediction weights). To validate the hypothesis that direct model merging is feasible, we report all results on the merged model without any subsequent training or finetuning. However, we have verified that the merged model quickly improves when trained on all language pairs. 4. After each stage we evaluate the models on all involved language pairs: En-Es, En-Fr, En-Pt, regardless of the data seen by the model up to this stage. We report quantitative results in Table 2 and show qualitative results of merged model operation in Figure 15. The merged model shows human-like degradation of operation: while it retained the capability to generate and translate into English, it has lost the ability to generate proper text in Spanish, French, or Portuguese, mixing words and grammatical constructs. We have verified that small amount of training on all language pairs restore the models proficiency in Spanish, French and Portuguese. However, we decided to report on the behavior of the merged model without any subsequent tuning to highlight the possibilities of model engineering offered by the large and sparse working dimension of BDH-GPU. The BDH-GPU model merging experiment has shown that when the model latent space promotes concept disentangling (c.f. Section 6.2 on monosemanticity) then it is feasible to directly compose concepts in this space, e.g. by concatenation of weights from different models. This feature of the BDH architecture allows us to see the models as composable computer programs with emergent properties. Model: Translation into English FrEn EsEn PtEn Translation from English EnEs EnFr EnPt 1: Base En-Es 2: Base (1) tuned on En-Fr 3: Base (1) tuned on En-Pt 4: Merged (23) 0.36 0.58 0.44 0.43 0.77 0.36 0.76 0.40 0.64 0.68 0.34 0.39 0.35 2.57 1.79 1. 2.21 0.31 2.20 0.77 2.27 2.54 0.33 0.86 Table 2: Validation next token prediction losses (lower is better) of translation models trained on different language pairs and then merged. We evaluate each model on En-Es, En-Fr, En-Pt language pairs separately. We can see that the base model can translate between English and Spanish, while on En-Fr and En-Pt tasks it falls back on perplexities of an unconditional English language model (loss about 0.65) and cant generate proper French or Portuguese. After tuning on French or Portuguese the model learns to translate between respectively English and French or English and Portuguese, while somewhat retaining the capacity to translate Spanish to English and losing the capability to translate English to Spanish. The merged model can translate Spanish, French, and Portuguese to English, however it mixes these three languages when asked to translate from English. This is consistent with qualitative results shown in Figure 15. 41 <F:es>Esta es una afirmación clara.<T:en> In this clarification, it is clear statement. <F:es>Esta es una afirmación clara.<T:en> It is clear statement. <F:es>Esta es una afirmación clara.<T:en> That is clear affirmation. <F:fr>Cest une déclaration claire.<T:en> This is clear statement. <F:fr>Cest une déclaration claire.<T:en> This is clear declaration. <F:fr>Cest une déclaration claire.<T:en> It is clear declaration. <F:pt>Esta é uma afirmação clara.<T:en> That is clear statement. <F:pt>Esta é uma afirmação clara.<T:en> This is clear statement. <F:pt>Esta é uma afirmação clara.<T:en> This is clear assertion. (a) Sampled translations from Spanish, French, and Portuguese to English. The vertical bar delimits the prompt from model output. Each translation was sampled three times to show the models consistency. <F:en>This is clear statement.<T:es> Ce récent statement está clarificative. <F:en>This is clear statement.<T:es> Il se revela exact. <F:en>This is clear statement.<T:es> Constato que cette déclaration était monstruosa. <F:en>This is clear statement.<T:fr> Está de noto uma déclaration. <F:en>This is clear statement.<T:fr> Ce sont une declaration clare. <F:en>This is clear statement.<T:fr> Cestá uma declaração clare. <F:en>This is clear statement.<T:pt> Esta declaração étangling état está clara. <F:en>This is clear statement.<T:pt> Istambigna de contence. <F:en>This is clear statement.<T:pt> Cestaté clarification é clara!. (b) Sampled translations from English into Spanish, French, and Portuguese. The model mixes the three languages, though the meaning of the source sentence seems to have been preserved. <F:en> The European Convention on Human Rights has been set up in 1992, when it applied the Convention in 1 <F:es> Naturalment, nos deputés de toda autre groupe southern Italians, notariously engaged in the discuss <F:fr> (ES) Mr President, aproveit de montrar ma satisfaction por surprise de la parte de Milan, et parti <F:pt> (FI) Mr President, todos outreaches, mesures on ways and means of promoting the economic development (c) Language-conditional samples. Consistently with the translation experiment in (a) and (b) above, the model properly generates English, and mixes Spanish, French, and Portuguese, sometimes slipping into English. <F:es>Mi lingua project nominat Romanid esed publicat ja in may de pasat an.<T:en> Lingue nominated Project is Romanian published in Romanian pasta year. <F:fr>Mi lingua project nominat Romanid esed publicat ja in may de pasat an.<T:en> My language in Project Nomina is Romani esedi publish posted peas in maybe the same year. <F:pt>Mi lingua project nominat Romanid esed publicat ja in may de pasat an.<T:en> have been Latva project nomination Romanim esede publica published in May of the postal service and the service provider. d) Attempts of the model to translate sentence in Romanid (2025), zonal auxiliary language for speakers of Romance languages naturalistic constructed language intended to be intuitively understandable to speakers of Romance languages. The English translation of the prompt is: My language project called Romanid was published already in May of last year. The model is able to pick up some of the meaning of the prompt. Figure 15: Conditional and unconditional samples generated from English-Spanish-French-Portuguese translation model created by direct concatenation of parameters of models trained on distinct language pairs. 7.2 Training without backpropagation through time Sparsity of synapse activations in BDH opens the door to efficient approximations to backpropagation through time. The main intuition is that we only need to remember when synapse has changed, and the i, coordinates of the synapse implicitly encode which neurons were active and should take part in error signal backpropagation. In this section we report results of preliminary experiments on the impact of removal of backpropagation through time on model performance. For the PyTorch implementation in Appendix E, this corresponds to detach-ing variables and in the implementation of the LinearAttention class. In particular, we found that such model, trained without any backpropagation through time, retained some ability to model language, but lost the ability to match concepts between different languages during translation. For translation tasks like those presented in Table 2, loss values for English increased from loss level of approximately 0.65 for an unconditional English language model (trained with backpropagation over time), to loss of approximately 0.75 1.05 for model trained without backpropagation over time, depending on model variant, regardless of whether English was the source or target language in translation. No significant difficulties were encountered during training when crossing the barrier of the letter-bigram language model, at loss value 2.4. Beyond side-effects of the general design, we did not optimize the BDH-GPU model for suitability of training without backpropagation. We consider this architecture to be good starting point for bootstrapping further investigations in this direction."
        },
        {
            "title": "8 Conclusions",
            "content": "8.1 Takeaways for model engineering This paper leads up to new class of language and reasoning models which eliminate architecture nonuniformities, notably in terms of scaling for model size, and handling of time scales of inference. The BDH-GPU architecture introduced in this paper opens the following opportunities: 1. New ways of scaling models for time and size. BDH-GPU is state-space model which scales for size in one large dimension (neurons in this dimension are indexed by RoPE oscillator frequency). Subject to appropriate sharding, this also leads to desirable form of locality: important data is located just next to the sites at which it is being processed. This minimizes communication, and eliminates the most painful of all bottlenecks for reasoning models during inference: memory-to-core bandwidth. 2. Faster model iteration. During training and inference alike, BDH-GPU provides insight into parameter and state spaces of the model which allows for easy and direct evaluation of model health and performance, notably, through sparsity-related measures and through aggregates and statistics on the large pool of homogeneous neurons, even for relatively small models. Attention and parametric layers alike operate on the same neuron dimension (concept dimension). 3. Direct explainability of model state. Elements of state of BDH-GPU are directly localized at neuron pairs, allowing for micro-interpretation of the hidden state of the model. 4. New opportunities for model surgery. The BDH-GPU architecture is, in principle, amenable to direct composability of model weights in way resemblant of composability of programs. This concerns the potential both the direct composition of separately trained model parts, as well as surgery of parameter spaces of models, by inserting fragments of manually programmed protocols into machine-learned code. 8.2 Implications for brain science We have obtained micro-foundational description of attention for artificial language and reasoning models, expressed in framework of local graph dynamics. This has been found to be consistent with the effects observed for the same function of attention for language and reasoning in the brain. By introducing translation layer based on similarity of function between the artificial and biological planes, for blocks of feed-forward neural networks and attention mechanisms, our work points to the following hypothesis: complex systems effects which are observed in the brain, around modular scale-free network structure, synaptic plasticity, and Hebbian learning arose from its core purpose doing reasoning and not from any specific longer-term training dynamics which the brain applies. We have exhibited how general attention mechanism can be efficiently implemented as an artificial neuronal system with spiking neurons and synapse plasticity. More formally, we first describe the class of local interaction dynamics which any system plausibly needs to implement attention mechanisms. We then confirmed that the edge-reweighting rule is sufficient to allow certain artificial Language Model (BDH-GPU) to operate at least at the level of the Transformer. For an artificial network, the edge-reweighting rule intuitively describes the interaction between two artificial neurons exhibiting rapid state-change behavior, and one synaptic neuron interconnection element exhibiting plasticity as shown in Fig. 13. More broadly, this work may potentially serve to support efforts aiming to isolate, from among the many extremely complex electrochemical patterns and signal dynamics occurring in the brain, those that are crucial for solving tasks in-context (based on attention), from those that potentially serve other purposes, such as transfer of information from short-term memory to long-term memory, or long-term improvement of brain function (learning). How this work helps with axiomatization of learning theory in the brain. Attempts to understand the brain, starting from the perspective of longer time scales of training, have proved extremely challenging, defying progress. This paper pin-points attention-based reasoning at shorter time scales as the other end of the string, and hints how, from here, untangling the entire story will plausibly be easier. For natural systems undergoing continuous learning, the time scales to look at are: language function and reasoning (chain-of-thought inference), then short-to-long memory transfer from state to network weights, adaptation of structure: changes to interconnections, and finally, changes to neuron nodes. For long time scales, this reduces the question of finding supervised training dynamics form the most general case, to specific class of local dynamics: an interaction kernel performing edge-reweighting rules. As these rules ap43 pear fundamental to logical inference and biochemical processes alike, its universality in processes that the brain is responsible for is plausible also beyond the realm of language-based reasoning. From systems perspective, we arrive at the following possible explanation. The brain generally tries to be lazy in terms of energy expense, and does things as late as it can. Only reasoning needs to happen close to critical regime, because it involves executing real-time program which needs to be responsive, since the life and success of the biological organism depends on it. Then, for certain time, which may be minutes for humans, the brain has enough synapses in it to represent (almost) all useful information it needs for reasoning, decision-making, etc. all stored in short-term state, at synapses (and/or neurons). Some of the neuron activations which the brain performs at this time scale represent gradients of state the gradients of in-context learning, passed on to modify synapse strength, in weight-update process. As time goes by, the system runs out of state space. Then, memory processes work to iron things out, preserving in more permanent neuron connection weights and graph structure the elements of state that have been reinforced by feedback signals. Overall, there are fewer and fewer things that need to be remembered across progressively longer time scales. However, this entire memory process is, plausibly, subsidiary to the definition of the dynamics of reasoning and the synaptic dynamics of state that we discuss in this paper. In other words, the best form of description of the relaxation from state into longer-term memory follows from the specific kernel of the reasoning dynamics, such as the edge-reweighting kernel. As for the ratio of time scales (measured in tokens for language), we can estimate that the time lapse after which harmonizing state with memory process becomes important is of about the same order of magnitude as the average time between writes (significant transmission increases) for individual synaptic elements (see e.g. Fig. 14). In our models, this time is lower-bounded by the inverse of sparsity of the vector y, i.e., 1/ρ 1/5% = 20 tokens, but it could be much larger for larger systems; we also do not force it in any way to be sparser during training. During training with backpropagation, if the backpropagation window is short enough, < 1/ρ tokens, we can plausibly assume that synapse changes state only once in that window (and is used multiple times), hence the DAG of gradient backwards propagation is much more direct to embed within the system graph. Backpropagation is then question of routing gradients in the neuron communication graph, and not one of disentangling them. All natural training approaches, whether based on backpropagation, or any more direct form of relaxation from state into weights, appear to bottleneck on the amount of available state space on synapses, becoming necessary at about 1/ρ by simple information-theoretic argument on state storage capacity. Regardless of how much of this is an accurate description, and how much an intuition, at the very least, it appears we may now have way forward. Some part of the global mystery of learning in the brain can be reduced to more localized problem of state-to-operator transfer for some relatively compact form of state-space dynamics (i.e., one specific local graph kernel). This change of perspective brings in both completely new problem landscape in which to navigate towards complete solution, as well as set of new methods to use for the different types of graph structure changes involved in learning, including approaches from distributed computing, evolving network theory, and graph rewiring systems. At this point, it seems one natural next step would be to ground the current discussion more deeply in findings of brain science, to refine or simplify the actual kernels used by brain reasoning (which was not the objective of this paper), and potentially seek validation through experiment. 8.3 Societal impact This paper is voice in favor of bringing principled understanding to reasoning in Machine Learning. Axiomatic AI provides an opportunity to reduce risks related to unpredictable behavior of AI models, and, to open or accelerate new development directions. The subject matter which we consider here serves as direct introduction to the most crucial problem that lies ahead: controlling the behavior of autonomous AI reasoning models and AI systems as they progress across time scales, from seconds to years. 44 r n t n c o n e d h e S e c e s n e a n o t e h g o - o o i s d n s ( d i ) t f a a ) , ( , ) ( - H ) 2 ( r n c r r N e s t p n a , n U s S , C , U , G w h e n p s o y e a g e n o m e h r r e d l l ( n h - o e a n a s e d , l t t n : r p s - u s i g , l t t n t e ) ) a ) t l : r p s - u p e ( o r r e 3 r i ( a p s t i e ( g l M g ) r t z c ( ) r t r n a r u - , t t - d o e r n a r e v - , t t - d o e s t r r - u x o n o l o a - u ) r n ( t p u - k , t t - d o e i t o t c - a o n n a o i e - i a n l l r o n c e u d o n A c a n n e t x a l o r o F n s e g u , w . e g o a r a o s a p S u t a u c . m g a u c , n l l . a s y - u r y e t e n m g h s s e - ) ( c n t a , fi - m t n o , n . e e t , n n a p u o u - n o U ) r s ( R - , - R A - , - R R - , - R o a d - o r r i o U fi - m s i p t p e s , i l P . . , i m f o n o p u g t n t ) t x p ( r p s v e e - a i d o g a e s u t e c h - i 45 n s m i p ; e e o l o ; w t r n e h d g n s e c I - i . c r t g o r e p n . o n i o o f l l a - r t i o D t u o U s e s x - i a r i y i y t i r l s , w t r , w t r f e d t n l e e y i P t t o n e o v a t p a s e p , t e i h f a , t e i P o v o v t t s c o v a . d i d , B , - H , r n 2 e : u t r d n s d e g f i p f s p : 3 a f o n n s m ) e c t m o n v ( a d fi a o p b o c fi m t i a n ; t t s s c o v A Acknowledgments The authors thank David Sussillo, Navdeep Jaitly, and Emanuele Natale for insightful discussions on reasoning and the brain, and for early feedback on this write-up. We also thank Samy Bengio for comments on the presentation. We kindly acknowledge the support of all of the Pathway team, notably, Paweł Podhajski for his amazing help with cluster setup, Victor Szczerba and Schwab for all discussions over coffee, and Kamil Piechowiak and Chris Ociepa for constructive comments on the presentation. AK thanks Christos Papadimitriou for being the direct inspiration for us to embark on this journey. Author contributions AK conceived the BDH and BDH-GPU architectures, conceived most of the theory, developed most of the model source code, conceived and performed experiments on synapses, and wrote most of the paper. PU contributed crucial elements of BDH-GPU architecture, contributed model and framework source code, contributed to theoretical analysis, and performed experiments. JCh led, designed, and oversaw methodology of experiments, led framework development, contributed major improvements to BDH-GPU architecture, contributed to the theory, implemented baselines, performed experiments, and substantially redacted the paper. ZS conceived the project, guided research directions, introduced particle-interaction interpretation, acted as final judge in research decisions, and substantially redacted the paper. MB optimized model source code, contributed framework source code, and performed experiments."
        },
        {
            "title": "References",
            "content": "I. Abraham, D. Delling, A. V. Goldberg, and R. F. Werneck. hub-based labeling algorithm for shortest paths in In P. M. Pardalos and S. Rebennack, editors, Experimental Algorithms, pages 230241, Berlin, road networks. Heidelberg, 2011. Springer Berlin Heidelberg. ISBN 978-3-642-20662-7. D. Achlioptas and F. Mcsherry. Fast computation of low-rank matrix approximations. J. ACM, 54(2):9es, Apr. 2007. ISSN 0004-5411. URL https://doi.org/10.1145/1219092.1219097. D. Angluin, J. Aspnes, Z. Diamadi, M. J. Fischer, and R. Peralta. Computation in networks of passively mobile finite-state sensors. Distributed Comput., 18(4):235253, 2006. URL https://doi.org/10.1007/ s00446-005-0138-3. J. Aspnes and E. Ruppert. An Introduction to Population Protocols, pages 97120. Springer Berlin Heidelberg, Berlin, Heidelberg, 2009. ISBN 978-3-540-89707-1. URL https://doi.org/10.1007/978-3-540-89707-1_5. J. Ba, G. E. Hinton, V. Mnih, J. Z. Leibo, and C. Ionescu. Using fast weights to attend to the recent past. Advances in neural information processing systems, 29, 2016a. J. L. Ba, J. R. Kiros, and G. E. Hinton. Layer normalization, 2016b. URL https://arxiv.org/abs/1607.06450. D. Bahdanau, K. Cho, and Y. Bengio. Neural machine translation by jointly learning to align and translate. In Y. Bengio and Y. LeCun, editors, 3rd International Conference on Learning Representations, ICLR 2015, San Diego, CA, USA, May 7-9, 2015, Conference Track Proceedings, 2015. URL http://arxiv.org/abs/1409.0473. H. B. Barlow. Single units and sensation: neuron doctrine for perceptual psychology? Perception, 1(4):371394, 1972. URL https://doi.org/10.1068/p010371. PMID: 4377168. L. Becchetti, V. Bonifaci, and E. Natale. Pooling or sampling: Collective dynamics for electrical flow estimation. In Proceedings of the 17th International Conference on Autonomous Agents and MultiAgent Systems, AAMAS 18, page 15761584, Richland, SC, 2018. M. Beck, K. Pöppel, M. Spanring, A. Auer, O. Prudnikova, M. Kopp, G. Klambauer, J. Brandstetter, and S. Hochreiter. xlstm: Extended long short-term memory. Advances in Neural Information Processing Systems, 37:107547107603, 2024. A. Ben-Kish, I. Zimerman, M. J. Mirza, J. Glass, L. Karlinsky, and R. Giryes. Overflow prevention enhances longcontext recurrent llms, 2025. URL https://arxiv.org/abs/2505.07793. A. Björner, L. Lovász, and P. W. Shor. Chip-firing games on graphs. European Journal of Combinatorics, 12(4): 283291, 1991. ISSN 0195-6698. URL https://doi.org/10.1016/S0195-6698(13)80111-4. V. D. Blondel, J.-L. Guillaume, R. Lambiotte, and E. Lefebvre. Fast unfolding of communities in large networks. Journal of Statistical Mechanics: Theory and Experiment, 2008(10):P10008, oct 2008. URL https://dx.doi. org/10.1088/1742-5468/2008/10/P10008. L. Boczkowski, A. Korman, and E. Natale. Minimizing message size in stochastic communication patterns: fast selfstabilizing protocols with 3 bits. Distributed Comput., 32(3):173191, 2019. URL https://doi.org/10.1007/ s00446-018-0330-x. N. Bostrom. Superintelligence: Paths, Dangers, Strategies. Oxford University Press, Inc., USA, 1st edition, 2014. ISBN 0199678111. N. Brunel. Hebbian learning of context in recurrent neural networks. Neural Computation, 8(8):16771710, 1996. URL https://ieeexplore.ieee.org/document/6796169. J. Buckman, C. Gelada, and S. Zhang. Symmetric Power Transformers, 2024. URL https://manifestai.com/ articles/symmetric-power-transformers/. S. Budzinskiy. When big data actually are low-rank, or entrywise approximation of certain function-generated matrices, 2025. URL https://arxiv.org/abs/2407.03250. H. Cairns. Some halting problems for abelian sandpiles are undecidable in dimension three. SIAM Journal on Discrete Mathematics, 32(4):26362666, 2018. URL https://doi.org/10.1137/16M1091964. Y. Chen, D. Doty, and Soloveichik. Deterministic function computation with chemical reaction networks. Nat Comput, 13:517534, 2014. URL https://link.springer.com/article/10.1007/s11047-013-9393-6. K. Cho and Y. Bengio. Exponentially increasing the capacity-to-computation ratio for conditional computation in deep learning, 2014. URL https://arxiv.org/abs/1406.7362. K. M. Choromanski, V. Likhosherstov, D. Dohan, X. Song, A. Gane, T. Sarlos, P. Hawkins, J. Q. Davis, A. Mohiuddin, L. Kaiser, D. B. Belanger, L. J. Colwell, and A. Weller. Rethinking attention with performers. In International Conference on Learning Representations, 2021. URL https://openreview.net/forum?id=Ua6zuk0WRH. 47 P. Christiano, J. A. Kelner, A. Madry, D. A. Spielman, and S.-H. Teng. Electrical flows, laplacian systems, and faster In Proceedings of the Forty-Third Annual ACM Sympoapproximation of maximum flow in undirected graphs. sium on Theory of Computing, STOC 11, page 273282, New York, NY, USA, 2011. Association for Computing Machinery. ISBN 9781450306911. URL https://doi.org/10.1145/1993636.1993674. J. Czyzowicz, L. Gasieniec, A. Kosowski, E. Kranakis, P. G. Spirakis, and P. Uznanski. On convergence and threshold properties of discrete lotka-volterra population protocols. J. Comput. Syst. Sci., 130:125, 2022. URL https: //doi.org/10.1016/j.jcss.2022.06.002. M. Dabagia, C. H. Papadimitriou, and S. S. Vempala. Computation with sequences of assemblies in model of the brain. Neural Computation, 37(1):193233, 12 2024. ISSN 0899-7667. URL https://doi.org/10.1162/neco_ a_01720. Z. Dai, Z. Yang, Y. Yang, J. G. Carbonell, Q. V. Le, and R. Salakhutdinov. Transformer-XL: Attentive language models beyond fixed-length context. In A. Korhonen, D. R. Traum, and L. Màrquez, editors, Proceedings of the 57th Conference of the Association for Computational Linguistics, ACL 2019, Florence, Italy, July 28August 2, 2019, Volume 1: Long Papers, pages 29782988. Association for Computational Linguistics, 2019. doi: 10.18653/ V1/P19-1285. URL https://doi.org/10.18653/v1/p19-1285. M. Dehghani, S. Gouws, O. Vinyals, J. Uszkoreit, and Łukasz Kaiser. Universal transformers, 2019. URL https: //arxiv.org/abs/1807.03819. S. Dolev. Self-stabilization. The MIT Press, 2000. D. Doty, M. Eftekhari, L. Gasieniec, E. E. Severson, P. Uznanski, and G. Stachowiak. time and space optimal stable population protocol solving exact majority. In 62nd IEEE Annual Symposium on Foundations of Computer Science, FOCS 2021, Denver, CO, USA, February 7-10, 2022, pages 10441055. IEEE, 2021. URL https://doi.org/ 10.1109/FOCS52979.2021.00104. B. Dudek and A. Kosowski. Universal protocols for information dissemination using emergent signals. In I. Diakonikolas, D. Kempe, and M. Henzinger, editors, Proceedings of the 50th Annual ACM SIGACT Symposium on Theory of Computing, STOC 2018, Los Angeles, CA, USA, June 25-29, 2018, pages 8799. ACM, 2018. URL https://doi.org/10.1145/3188745.3188818. L. Emberson, B. Cottier, J. You, T. Adamczewski, and J.-S. Denain. LLM responses to benchmark questions are getting longer over time, 2025. URL https://epoch.ai/data-insights/output-length. Accessed: 2025-07-25. M. Feinberg. Foundations of Chemical Reaction Network Theory, volume 202 of Applied Mathematical Sciences. Springer, 2019. P. Fraigniaud and G. Giakkoupis. On the searchability of small-world networks with arbitrary underlying structure. In Proceedings of the Forty-Second ACM Symposium on Theory of Computing, STOC 10, page 389398, New York, NY, USA, 2010. Association for Computing Machinery. ISBN 9781450300506. doi: 10.1145/1806689.1806744. URL https://doi.org/10.1145/1806689.1806744. A. Graves. Adaptive computation time for recurrent neural networks, 2017. URL https://arxiv.org/abs/1603. 08983. A. Gu and T. Dao. Mamba: Linear-time sequence modeling with selective state spaces, 2024. URL https://arxiv. org/abs/2312.00752. D. Haziza, T. Chou, D. Choudhary, L. Wehrstedt, F. Massa, J. Yu, G. Jeong, S. Rao, P. Labatut, and J. Cai. Accelerating transformer inference and training with 2:4 activation sparsity, 2025. URL https://arxiv.org/abs/2503. 16672. B. J. He, J. M. Zempel, A. Z. Snyder, and M. E. Raichle. The temporal structures and functional significance of scale-free brain activity. Neuron, 66(3):353369, 2010. ISSN 0896-6273. URL https://doi.org/10.1016/j. neuron.2010.04.020. D. O. Hebb. Organization of behavior. New York: Wiley & Sons, 1949. V. Herrmann, R. Csordás, and J. Schmidhuber. Measuring in-context computation complexity via hidden state prediction, 2025. URL https://arxiv.org/abs/2503.13431. D. Hilbert. Mathematical problems. Bull. AMS, 8(10):437479, 1902. ISSN 0002-9904 (print), 1936-881X (electronic). URL https://doi.org/10.1090/S0002-9904-1902-00923-3. English translation of Hilberts famous list of 23 important problems in mathematics for the 20th Century. G. E. Hinton. What kind of graphical model is the brain? In Proceedings of the 19th International Joint Conference on Artificial Intelligence, IJCAI05, page 17651775, San Francisco, CA, USA, 2005. Morgan Kaufmann Publishers Inc. 48 G. E. Hinton and D. C. Plaut. Using fast weights to deblur old memories. In Proceedings of the ninth annual conference of the Cognitive Science Society, pages 177186, 1987. J. Hirvonen and J. Suomela. Distributed Algorithms 2020 the book. Aalto University, 2025. URL https:// jukkasuomela.fi/da2020/da2020.pdf. S. Hochreiter and J. Schmidhuber. Long short-term memory. Neural Comput., 9(8):17351780, Nov. 1997. ISSN 0899-7667. URL https://doi.org/10.1162/neco.1997.9.8.1735. J. Hofbauer and K. Sigmund. Evolutionary Games and Population Dynamics. Cambridge University Press, 1998. https://www.cambridge.org/core/books/evolutionary-games-and-population-dynamics/ URL A8D94EBE6A16837E7CB3CED24E1948F8. P. W. Holland, K. B. Laskey, and S. Leinhardt. Social Networks, 5 ISSN 0378-8733. URL https://www.sciencedirect.com/science/article/pii/ Stochastic blockmodels: First steps. (2):109137, 1983. 0378873383900217. E. J. Hu, Y. Shen, P. Wallis, Z. Allen-Zhu, Y. Li, S. Wang, L. Wang, and W. Chen. Lora: Low-rank adaptation of large language models, 2021. URL https://arxiv.org/abs/2106.09685. X. Huang, A. Yang, S. Bhattamishra, Y. Sarrof, A. Krebs, H. Zhou, P. Nakkiran, and M. Hahn. formal framework for understanding length generalization in transformers. In The Thirteenth International Conference on Learning Representations, 2025. URL https://openreview.net/forum?id=U49N5V51rU. F. Jabr and A. Rothschild. How brainless slime molds redefine intelligence. Nature, 7(1), 2012. A. Jojic, Z. Wang, and N. Jojic. Gpt is becoming turing machine: Here are some ways to program it. arXiv preprint arXiv:2303.14310, 2023. A. Kalev and I. Hen. Feynman path integrals for discrete-variable systems: Walks on hamiltonian graphs. Phys. Rev. Res., 7:013220, Feb 2025. URL https://link.aps.org/doi/10.1103/PhysRevResearch.7.013220. A. Karpathy. nanoGPT. https://github.com/karpathy/nanoGPT, 2024. B. Karrer and M. E. J. Newman. Stochastic blockmodels and community structure in networks. Phys. Rev. E, 83: 016107, Jan 2011. URL https://link.aps.org/doi/10.1103/PhysRevE.83.016107. A. Katharopoulos, A. Vyas, N. Pappas, and F. Fleuret. Transformers are RNNs: Fast autoregressive transformers with linear attention. In H. D. III and A. Singh, editors, Proceedings of the 37th International Conference on Machine Learning, volume 119 of Proceedings of Machine Learning Research, pages 51565165. PMLR, 1318 Jul 2020. URL https://proceedings.mlr.press/v119/katharopoulos20a.html. D. Kempe, J. Kleinberg, and E. Tardos. Maximizing the spread of influence through social network. In KDD 03: Proceedings of the ninth ACM SIGKDD international conference on Knowledge discovery and data mining, pages ISBN 1-58113-737-0. URL https://doi.acm.org/10. 137146, New York, NY, USA, 2003. ACM Press. 1145/956750.956769. N. Kitaev, Ł. Kaiser, and A. Levskaya. Reformer: The efficient transformer. In 8th International Conference on Learning Representations, ICLR 2020, Addis Ababa, Ethiopia, April 26-30, 2020. OpenReview.net, 2020. URL https://openreview.net/forum?id=rkgNKkHtvB. P. Koehn. Europarl: parallel corpus for statistical machine translation. In Proceedings of Machine Translation Summit X: Papers, pages 7986, Phuket, Thailand, Sept. 13-15 2005. URL https://aclanthology.org/2005. mtsummit-papers.11/. A. Kosowski and P. Uznanski. Population protocols are fast. In C. Newport and I. Keidar, editors, Proceedings of the 2018 ACM Symposium on Principles of Distributed Computing, PODC 2018, Egham, United Kingdom, July 23-27, 2018, pages 475477. ACM, 2018. URL https://arxiv.org/abs/1802.06872. R. Krauthgamer and S. Sapir. Comparison of matrix norm sparsification. Algorithmica, 85(12):39573972, 2023. URL https://doi.org/10.1007/s00453-023-01172-6. A. Kumar, L. Owen, N. R. Chowdhury, and F. Güra. ZClip: Adaptive spike mitigation for LLM pre-training, 2025. URL https://arxiv.org/abs/2504.02507. Y. LeCun. path towards autonomous machine intelligence version 0.9. 2, 2022-06-27. Open Review, 62(1):162, 2022. Y. LeCun, Y. Bengio, and G. Hinton. Deep learning. Nature, 521(7553):436444, 2015. J. Lee, Y. Bahri, R. Novak, S. S. Schoenholz, J. Pennington, and J. Sohl-Dickstein. Deep neural networks as gaussian processes. arXiv preprint arXiv:1711.00165, 2017. 49 A. C. Lin, A. M. Bygrave, A. De Calignon, T. Lee, and G. Miesenböck. Sparse, decorrelated odor coding in the mushroom body enhances learned odor discrimination. Nature neuroscience, 17(4):559568, 2014. H. Lin and S. Jegelka. Resnet with one-neuron hidden layers is universal approximator. Advances in neural information processing systems, 31, 2018. K. Liu, J. Gao, and K. Chen. Scaling up the state size of RNN LLMs for long-context scenarios. In W. Che, J. Nabende, E. Shutova, and M. T. Pilehvar, editors, Proceedings of the 63rd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 1151611529, Vienna, Austria, July 2025. Association for Computational Linguistics. ISBN 979-8-89176-251-0. URL https://aclanthology.org/2025.acl-long.564/. I. Loshchilov and F. Hutter. Decoupled weight decay regularization, 2019. URL https://arxiv.org/abs/1711. 05101. V. Mante, D. Sussillo, K. V. Shenoy, and W. T. Newsome. Context-dependent computation by recurrent dynamics in prefrontal cortex. nature, 503(7474):7884, 2013. L. Massoulié. Community detection thresholds and the weak ramanujan property. In Proceedings of the FortySixth Annual ACM Symposium on Theory of Computing, STOC 14, page 694703, New York, NY, USA, 2014. ISBN 9781450327107. URL https://doi.org/10.1145/2591796. Association for Computing Machinery. 2591857. W. S. McCulloch and W. Pitts. logical calculus of the ideas immanent in nervous activity. The Bulletin of Mathematical Biophysics, 5(4):115133, 1943. F. McSherry, D. G. Murray, R. Isaacs, and M. Isard. Differential dataflow. In Sixth Biennial Conference on Innovative Data Systems Research, CIDR 2013, Asilomar, CA, USA, January 6-9, 2013, Online Proceedings. www.cidrdb.org, 2013. URL https://cidrdb.org/cidr2013/Papers/CIDR13_Paper111.pdf. W. Merrill and A. Sabharwal. The expressive power of transformers with chain of thought. In The Twelfth International Conference on Learning Representations, 2024. URL https://openreview.net/forum?id=NjNGlPh8Wh. T. Mikolov, I. Sutskever, K. Chen, G. S. Corrado, and J. Dean. Distributed representations of words and phrases and their compositionality. Advances in neural information processing systems, 26, 2013. D. Mitropolsky and C. H. Papadimitriou. Simulated language acquisition in biologically realistic model of the brain. bioRxiv, 2025. doi: 10.1101/2025.07.15.664996. URL https://www.biorxiv.org/content/early/2025/ 07/19/2025.07.15.664996. Y. Mohsenzadeh, C. Mullin, B. Lahner, and A. Oliva. Emergence of visual center-periphery spatial organization in deep convolutional neural networks. Scientific Reports, 10(1):4638, 2020. URL https://doi.org/10.1038/ s41598-020-61409-0. V. Nair and G. E. Hinton. Rectified linear units improve restricted boltzmann machines. In Proceedings of the 27th international conference on machine learning (ICML-10), pages 807814, 2010. R. M. Neal. Bayesian learning for neural networks, volume 118. Springer Science & Business Media, 2012. J. v. Neumann. The computer and the brain. Yale University Press, USA, 1958. ISBN 0300007930. M. E. J. Newman. Modularity and community structure in networks. Proceedings of the National Academy of Sciences, 103(23):85778582, 2006. doi: 10.1073/pnas.0601602103. URL https://www.pnas.org/doi/abs/10.1073/ pnas.0601602103. B. A. Olshausen. (ed.), The Brain and Computation Simons Institute Program, Berkeley, 2018. URL https: //simons.berkeley.edu/programs/brain-computation. B. A. Olshausen and D. J. Field. Sparse coding with an overcomplete basis set: strategy employed by V1? Vision Research, 37(23):33113325, 1997. ISSN 0042-6989. URL https://doi.org/10.1016/S0042-6989(97) 00169-7. C. Olsson, N. Elhage, N. Nanda, N. Joseph, N. DasSarma, T. Henighan, B. Mann, A. Askell, Y. Bai, A. Chen, T. Conerly, D. Drain, D. Ganguli, Z. Hatfield-Dodds, D. Hernandez, S. Johnston, A. Jones, J. Kernion, L. Lovitt, In-context learning and K. Ndousse, D. Amodei, T. Brown, J. Clark, J. Kaplan, S. McCandlish, and C. Olah. induction heads. Transformer Circuits Thread, 2022. URL https://transformer-circuits.pub/2022/ in-context-learning-and-induction-heads/index.html. S. Ostmeier, B. Axelrod, M. Varma, M. E. Moseley, A. Chaudhari, and C. Langlotz. LieRE: Lie rotational positional encodings, 2025. URL https://arxiv.org/abs/2406.10322. C. H. Papadimitriou, S. S. Vempala, D. Mitropolsky, M. Collins, and W. Maass. Brain computation by assemblies of neurons. Proceedings of the National Academy of Sciences, 117(25):1446414472, 2020. URL https://www. pnas.org/doi/abs/10.1073/pnas.2001893117. A. Paszke, S. Gross, F. Massa, A. Lerer, J. Bradbury, G. Chanan, T. Killeen, Z. Lin, N. Gimelshein, L. Antiga, et al. Pytorch: An imperative style, high-performance deep learning library. Advances in neural information processing systems, 32, 2019. D. Peleg. Distributed Computing: Locality-Sensitive Approach. Society for Industrial and Applied Mathematics, 2000. URL https://epubs.siam.org/doi/abs/10.1137/1.9780898719772. J. Pérez, P. Barceló, and J. Marinkovic. Attention is turing complete. J. Mach. Learn. Res., 22(1), Jan. 2021. ISSN 1532-4435. O. Press, N. A. Smith, and M. Lewis. Train short, test long: Attention with linear biases enables input length extrapolation, 2022. URL https://arxiv.org/abs/2108.12409. A. Radford, J. Wu, R. Child, D. Luan, D. Amodei, and I. Sutskever. Language models are unsupervised multitask learners. OpenAI blog, 1(8):9, 2019. L. T. Rolla. Activated random walks on Zd. Probability Surveys, 17, Jan. 2020. ISSN 1549-5787. URL https: //dx.doi.org/10.1214/19-PS339. Romanid. Wikipedia, the free encyclopedia, 2025. URL https://en.wikipedia.org/w/index.php?title= Romanid&oldid=1275565870. [Online; accessed 24-July-2025]. D. E. Rumelhart, G. E. Hinton, and R. J. Williams. Learning representations by back-propagating errors. Nature, 323 (6088):533536, 1986. T. K. Rusch and D. Rus. Oscillatory state-space models. In The Thirteenth International Conference on Learning Representations, 2025. URL https://openreview.net/forum?id=GRMfXcAAFh. S. Sabour, N. Frosst, and G. E. Hinton. Dynamic routing between capsules. Advances in neural information processing systems, 30, 2017. J. Schmidhuber. Reducing the ratio between learning complexity and number of time varying variables in fully recurrent nets. In International Conference on Artificial Neural Networks, pages 460463. Springer, 1993. N. Shazeer, A. Mirhoseini, K. Maziarz, A. Davis, Q. Le, G. Hinton, and J. Dean. Outrageously large neural networks: The sparsely-gated mixture-of-experts layer, 2017. URL https://arxiv.org/abs/1701.06538. Z. Shen, H. Yang, and S. Zhang. Optimal approximation rate of ReLU networks in terms of width and depth. Journal de Mathématiques Pures et Appliquées, 157:101135, 2022. ISSN 0021-7824. doi: https://doi.org/10.1016/j.matpur. 2021.07.009. URL https://www.sciencedirect.com/science/article/pii/S0021782421001124. P. Shojaee, I. Mirzadeh, K. Alizadeh, M. Horton, S. Bengio, and M. Farajtabar. The illusion of thinking: Understanding the strengths and limitations of reasoning models via the lens of problem complexity, 2025. URL https://arxiv. org/abs/2506.06941. N. Srivastava, G. Hinton, A. Krizhevsky, I. Sutskever, and R. Salakhutdinov. Dropout: simple way to prevent neural networks from overfitting. The journal of machine learning research, 15(1):19291958, 2014. J. Su, M. H. M. Ahmed, Y. Lu, S. Pan, W. Bo, and Y. Liu. Roformer: Enhanced transformer with rotary position embedding. Neurocomputing, 568:127063, 2024. doi: 10.1016/J.NEUCOM.2023.127063. URL https://doi. org/10.1016/j.neucom.2023.127063. Y. Sun, L. Dong, S. Huang, S. Ma, Y. Xia, J. Xue, J. Wang, and F. Wei. Retentive Network: successor to Transformer for large language models, 2023. URL https://arxiv.org/abs/2307.08621. A. M. Turing. Computing machinery and intelligence. Mind, 59(236):433460, 1950. https://www.jstor.org/stable/2251299. ISSN 00264423. URL M. Udell and A. Townsend. Why are big data matrices approximately low rank? SIAM J. Math. Data Sci., 1(1): 144160, 2019. doi: 10.1137/18M1183480. URL https://doi.org/10.1137/18M1183480. A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones, A. N. Gomez, Ł. Kaiser, and I. Polosukhin. Attention is all you need. Advances in neural information processing systems, 30, 2017. J. Wei, X. Wang, D. Schuurmans, M. Bosma, B. Ichter, F. Xia, E. H. Chi, Q. V. Le, and D. Zhou. Chain-of-thought In Proceedings of the 36th International Conference on ISBN prompting elicits reasoning in large language models. Neural Information Processing Systems, NIPS 22, Red Hook, NY, USA, 2022. Curran Associates Inc. 9781713871088. G. Weiss, Y. Goldberg, and E. Yahav. Thinking like transformers. In M. Meila and T. Zhang, editors, Proceedings of the 38th International Conference on Machine Learning, volume 139 of Proceedings of Machine Learning Research, pages 1108011090. PMLR, 1824 Jul 2021. URL https://proceedings.mlr.press/v139/weiss21a.html. 51 J. C. Whittington, T. H. Muller, S. Mark, G. Chen, C. Barry, N. Burgess, and T. E. Behrens. The Tolman-Eichenbaum Machine: Unifying space and relational memory through generalization in the hippocampal formation. Cell, 183 (5):12491263.e23, 2020. ISSN 0092-8674. URL https://doi.org/10.1016/j.cell.2020.10.024. J. C. R. Whittington, J. Warren, and T. E. Behrens. Relating transformers to models and neural representations In International Conference on Learning Representations, 2022. URL https: of the hippocampal formation. //openreview.net/forum?id=B8DVo9B1YE0. R. J. Williams and J. Peng. An efficient gradient-based algorithm for on-line training of recurrent network trajectories. Neural computation, 2(4):490501, 1990. A. Yang and D. Chiang. Counting like transformers: Compiling temporal counting logic into softmax transformers, 2024. URL https://arxiv.org/abs/2404.04393. G. Yang. Wide feedforward or recurrent neural networks of any architecture are gaussian processes. Advances in Neural Information Processing Systems, 32, 2019. Z. Yang, D. Yang, C. Dyer, X. He, A. Smola, and E. Hovy. Hierarchical attention networks for document classification. In Proceedings of the 2016 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 14801489, 2016. C. You, K. Wu, Z. Jia, L. Chen, S. Bhojanapalli, J. Guo, U. Evci, J. Wassenberg, P. Netrapalli, J. J. Willcock, S. Subramanian, F. Chern, A. Andreev, S. Pathak, F. Yu, P. Jain, D. E. Culler, H. M. Levy, and S. Kumar. Spark transformer: Reactivating sparsity in FFN and attention, 2025. URL https://arxiv.org/abs/2506.06644. H. Zhou, A. Bradley, E. Littwin, N. Razin, O. Saremi, J. M. Susskind, S. Bengio, and P. Nakkiran. What alIn The Twelfth International Conference gorithms can transformers learn? study in length generalization. on Learning Representations, ICLR 2024, Vienna, Austria, May 7-11, 2024. OpenReview.net, 2024. URL https://openreview.net/forum?id=AssIuHnmHX. M. Zou. Aspects of Efficiency in Selected Problems of Computation on Large Graphs. PhD thesis, Paris Diderot University, France, 2019. URL https://tel.archives-ouvertes.fr/tel-02436610."
        },
        {
            "title": "A Connection between generalization of reasoning and computational expressiveness",
            "content": "State-of-the-art reasoning models have the interpretation of (Turing-complete) programs, executed over certain period of time. This shifts the emphasis of generalization, from discovering the structure of mathematical functions which maps inputs to outputs, to discovering class of runnable programs, which take as input given class of input prompts, and process these prompts in the right direction. Consider given reasoning task, whose scope is defined as set of valid input prompts, given as bounded-length token sequences over some alphabet Ω. Given prompt from P, model solving the considered task is eventually (i.e, after some number of steps of reasoning) expected to generate an output, in the form of bounded-length token sequence over the same alphabet Ω, which is subjected to evaluation. Consider language models sampled from some probability distribution M1 over parameter sets in some architecture A1. Now, suppose that for some other model architecture A2 there exists distribution M2 over language models in A2 such that, for valid input prompt chosen uniformly at random from P, the outputs sampled from model M1 M1 and the outputs sampled from model M2 M2, have (almost) the same distribution in the space of boundedlength sequences over Ω, and are both obtained within some asymptotic bound on the number of steps of reasoning, in expectation. The described setting is equivalent to saying that models M2 have generalized the considered task in (almost) the same way as models M1. Indeed, conversely, if the described condition did not hold, we could, in finite number of trials, distinguish solutions to problem obtained by model families M1 and M2. Now, consider model architectures A1, A2 which apply Chain-of-Thought reasoning (Wei et al., 2022). model in such an architecture has the interpretation of trainable probabilistic program, taking inputs from P, and the architectures themselves represent computational machine architectures. Moving to discussion of computational expressiveness, we obtain the following statement. Observation 9. Given probability distribution of models M1 in architecture A1, suppose there exists distribution over models in architecture A2 which generalizes on task in the same way as models from M1. Then, the machine architecture A2 has sufficient computational expressiveness to simulate programs from M1 efficiently on the set of inputs P, i.e., A2 contains programs which obtain an (almost) identical distribution of outputs within the given bounds on running time. In particular, we note that if we were to consider the special case of A1 being reasonable human agents, we could say that architecture A2 generalizes reasoning, in the same way as humans, if we can train models M2 in A2 which accurately reproduce the outcomes of reasoning for some sample M1 of humans in A1. This leads us naturally to describe Language Model generalization through universal reference to the principles of operation of the human brain, treated as distributed computing architecture, and not through characterization of language and reasoning prompts that the model should be able to deal with in some specific way."
        },
        {
            "title": "B Further description of experiments",
            "content": "B.1 Language translation task We have evaluated our models on mixed language modeling and translation task derived from the Europarl corpus (Koehn, 2005). The corpus consists of sentence-level aligned translations of transcripts of European Parliament proceedings. For each language pair, we treat the data as long stream of interleaved source and target sentences (sampling for each sentence which language is the source, and which is the target) on which we train decoder only models. Thus, models are jointly trained as language models and translators. We train all models using Truncated Backpropagation Through Time (Williams and Peng, 1990). Subsequent minibatches served by the data loader are related: each is continuation of the previous. Each model maintains recurrent state, carried across minibatches: ρ matrix for BDH-GPU and FIFO buffer of recent KV-cache entries for the TransformerXL (Dai et al., 2019) baseline. We train all models on raw UTF8 data. We are mainly interested in model comparison and prefer to keep the experimental setup as simple as possible. few minibatches are shown in Fig. 16. The joint language modeling and translation formulation has several benefits: 1. Next token prediction is representative for LLM training. Simple architectures, such as decoder-only models are sufficient. 2. The task promotes models with long context capabilities subsequent sentences are related and the model can meaningfully utilize long context to model the source language sentences. 53 model size num embd layer dim num MLP dim head dropout Carried KV-cache 9 25M 50M 12 100M 15 200M 18 400M 25 800M 28 480 576 768 960 1152 1536 5 6 8 10 12 16 1920 2304 3072 3840 4608 6144 0.01 0.02 0.02 0.002 0.005 0. size 4096 4096 4096 4096 4096 4096 Table 4: Hyperparameters for GPTXL baselines in scaling experiments. The model architecture follows GPT2 (Radford et al., 2019), with FIFO buffer of past KV-cache entries (Dai et al., 2019). 3. The task promotes models which carry state across minibatches, as training data is temporally coherent and the final model state at the end of one minibatch is natural initialization of hidden state on the next minibatch. 4. Translation can be seen as language modeling coupled with fuzzy copying. Successful models will need to develop in-context learning capabilities such as inductive heads (Olsson et al., 2022). 0. <F:en>For countries such as Sweden and Finland, another system 1. allocation would be extremely significant.<T:es>Por ejemplo, 2. ara países como Suecia Finlandia tendría un gran significado 3. que se hiciese otra forma de distribución.<F:es>El diputado Fe 4. rber ha presentado una propuesta que implica una distribución 5. ás flexible, yo respaldo esta enmienda.<T:en>Mr Ferber has ta 6. bled an amendment which involves our looking in considerably 7. ore flexible way at the present allocation, and support this 8. mendment.<F:en>.<T:es>.<F:en>(NL) Mr President, would like to 9. start by thanking both parliamentary committees and not least bo Figure 16: Exemplary sequence of 10 successive minibatches from the translation task. The model is trained on raw UTF8 bytes (for visualization we pad multi-byte UTF8 characters with symbol). Special token strings <F:lang_code> and <T:lang_code> delimit source and target sentences. Minibatches are temporally coherent: source sentences are followed by their translations, and subsequent source sentences are part of the same larger document. B.2 BDH Scaling Experimental Details We provide details on models used in scaling experiments described in Section 4.2. All models were implemented in PyTorch (Paszke et al., 2019) and trained on the Europarl (Koehn, 2005) task described in Section B.1. We have kept the same training regime for all models at all sizes: En-PL and En-Cs language pairs (380MB total). All models trained on raw UTF8 bytes seeing total of 1.2B tokens (about 3 epochs). All minibatches were 2048 tokens long, but we have varied the number of examples in the minibatch (varying number of tokens in each minibatch) to accommodate different memory requirements of different models. We have used multi-GPU training using the Distributed Data Parallel approach using AdamW (Loshchilov and Hutter, 2019) with learning rate 103, and 1000 warm-up step followed by linear learning rate decay over the course of training to 104, adaptive gradient clipping (Kumar et al., 2025), and weight decay 0.1. Models were trained to operate on context longer than minibatch length using Truncated Backpropagation Through time (Williams and Peng, 1990). The Baseline model, dubbed GPTXL, was GPT2-like transformer (Radford et al., 2019) based off the NanoGPT (Karpathy, 2024) implementation with KV-cache carried across minibatches as in TransformerXL (Dai et al., 2019). We have used ALiBi positional biases Press et al. (2022). We list its hyperparameters for various model sizes in Table 4. Optimal Dropout was selected using small sweep at each model size. BDH-GPU directly uses model code provided in Appendix E. BDH-GPU adds xLSTM-like gating mechanism (Beck et al., 2024), and merges next token predictions from all layers. Both BDH-GPU and BDH-GPU use same architectural hyperparameters, gathered in Table 5. 54 model size num layer num dropout head 8 25M 50M 8 100M 8 200M 8 400M 8 800M 256 256 256 256 256 256 32768 65536 131072 262144 524288 1048576 4 4 4 4 4 4 0.1 0.1 0.1 0.1 0.1 0.1 Table 5: Hyperparameters for BDH-GPU models in scaling experiments. Model Init. from Training data Data size (bytes) Training tokens num. heads num. layers param. count BaseEnEs TunedEnFr TunedEnPt MergedEnEsFrPt BaseEnEs BaseEnEs TunedEnFr+TunedEnPt En-Es En-Fr En-Pt 612M 640M 616M 1.2B 1.2B 1.2B 24576 24576 24576 49152 256 256 256 256 4 4 4 4 8 8 8 19M 19M 19M 38M Table 6: Architecture and training details for model merging experiments. B.3 BDH Monosemantic Synapse Experiment Details We provide details for models used in exploration of monosemantic synapses in Section 6.2. The model was trained on Europarl (Koehn, 2005) described in Section B.1. It had = 256, = 49152, 4 attention heads, and 8 layers. The model was trained on about one epoch of En-Es, En-Pt, and En-Fr data (total 1.9B tokens) in Distributed Data Parallel setup using AdamW (Loshchilov and Hutter, 2019) with learning rate 103, 1000 warm-up step followed by linear learning rate decay over the course of training to 104, adaptive gradient clipping (Kumar et al., 2025), and weight decay 0.1. We have used Truncated Backpropagation Through time, carrying over the recurrent state of attention and training on sequences of length 2048 characters at time. We have used minimal Dropout (Srivastava et al., 2014) of 0.01. B.4 BDH Merging Experiment Details We provide details for models described in Section 7.1 All models were trained on Europarl (Koehn, 2005) described in Section B.1. We provide model architecture hyperparametrs in Table 6. Models were trained on about two passes over the training set in Distributed Data Parallel setup using AdamW (Loshchilov and Hutter, 2019) with learning rate 103, 1000 warmup step followed by linear learning rate decay over the course of training to 104, adaptive gradient clipping (Kumar et al., 2025), and weight decay 0.1. We have used Truncated Backpropagation Through time, carrying over the recurrent state of attention and training on sequences of length 2048 characters at time. We have used minimal Dropout (Srivastava et al., 2014) of 0.01."
        },
        {
            "title": "C Omitted formal claims and proofs",
            "content": "C.1 Proof of Observation 1 Proof. The equivalence is straightforward to verify, rewriting the linear-algebraic multiplication expressions of Eq. (6) in Einstein summation notation and comparing respective index pairs. At any time, during the execution of rules for layer l, variables X(i), (i) and σl(i, j) in the protocol description, for i, {1, . . . , n} correspond to the i-th coordinate of vectors xt,l (based on xt,l1 from the previous round), yt,l (based on yt,l1 from the previous round), and matrix entry σt,l (based on σt1,l from the previous token). The auxiliary variable A(i) corresponds to similar auxiliary vector at,l := σt1,lxt,l in an intermediate step of computation of yt,l from xt,l. The parameter u(i, j) R+ associated with an element of state follows from the definition of matrix ; we assume for simplicity that is diagonal (which corresponds to the case of ALiBi). Finally, in Table 1, the auxiliary node variables e(i), i(i), e(i), i(i) are used to handle the thresholding of the inhibitory circuit. 55 C.2 Formal statement of Claim 7 (linear attention) We provide the following Claim, expressing the operation of attention under C-non-adversarial key vectors (kτ ), = 1 . . . t, understood in the sense that there exists , 0 < 1 such that, if considering (kτ ) as sequence of random variables, each (kτ ), τ = 1 . . . t, can be considered sampled independently at random in Sν with respect to all keys sampled previously, except for at most such keys. We put = 1 for adversarial inputs, or if this condition cannot be satisfied at all due to the nature of function . Claim 8. Let Λ be space of keys and queries, let ϕ : Λ Λ [1, 1] be an attention affinity function, and let : Λ Sν, for some ν = O(poly(n)), be such that for any q, R, we have (q) (k) = ϕ(q, k) O(n100). Fix δ > 0 and N. Let Aϕ,t be block which computes attention at given by Eq. (14), for given sequence of key-query inputs (k1, . . . , kt) and values (v1, . . . , vt), where < δn/((C + 1) log n) is fixed, kτ Λ, and vτ Rd are of similar strength in the L2-norm, with c1 vτ c2, for all τ = 1 . . . t, for some constants 0 < c1 c2. Then the (simplified) linear attention equation of BDH-GPU: := t1 (cid:88) τ =1 vτ xτ xt (17) expresses Aϕ,t with O( δ), provided that the input vector (kτ ) is C-non-adversarial, under suitable randomly chosen key preparation function : Λ Rn , xτ := (kτ ), where depends on , w.h.p. in with respect to choice of . δ)-error in the L2-norm (i.e., τ aτ = O( Proof (sketch). To simplify notation, assume w.l.o.g. that Λ = Sν and = idem; to undo this assumption, at the end of the proof we apply for preparation in place of . All vectors and the result at we are looking to calculate are in Rd. With this notation, the attention task we are approximating is: at = t (cid:88) τ =1 kT τ vτ . (18) (this is still the general form of attention almost precisely equivalent to (14), not special case). The goal is to show how, subject to < δn/ log2 n, linear attention in dimension given by (17) is sufficiently precise estimation of (18). Consider now, with Λ = Sν, : Sν Rn, where we recall that xτ := (kτ ), to be suitable dimensionality reduction preserving approximation of scalar product between Rν and Rn. For simplicity of argument, we let : Rν Rn be standard Johnson-Lindenstrauss transform, with the additional property that (z) = (z) for all Rν (easy to obtain from any other Johnson-Lindenstrauss transform by taking (z) := (f (z) (z))/2). The distortion of scalar product in Rn is then known to be bounded as follows: kτ xt = O(ε)(kτ + δ)/(cid:112)(C + 1)t log t, where the last kt) = O(ε), w.h.p. with respect to choice of . Here, ε = (cid:112)log n/n = O( inequality follows from the assumption on made in the Claim. kt xτ kt xτ xt, for τ < t. Set aside the (at most C) elements rτ for which We now consider the sequence rτ := kτ kτ and kt are not independent. For all other elements, consider that rτ = O(ε) as established previously, and the sign rτ /rτ is chosen independently at random with respect to all but at least elements by the conditions imposed on and kτ . It follows that (cid:80)t τ =1 rτ can be represented as sum of O(C) martingales, each of which has length δ)/(cid:112)(C + 1)t log t. The Claim follows directly, by O(t/(C + 1)) and all elements bounded by O(ε) with ε = O( applying Azumas inequality to each of these martingales independently. Considering the extreme cases of = 0 and = 1, the above Claim leads directly to Claim 7, clarifying over what time, linear attention can be used to express general attention. C.3 Proof of Claim Proof. The proof is almost immediate, through the construction of an appropriate neuron-synapse interaction graphs e, such that Ge = e2[V ] and Gi = i2 α,j = (Eα,j)+ and [V ]. Consider (R+)2dn such that α+d,j = (Eα,j)+, for {1, . . . , n} and α {1, . . . , d}. Define De, Di (R+)n2d so that: (De Di)E = DE. 56 i,α = De Indeed, notice that this is always possible by redistributing elements of into De and Di (putting De (Di,α)+) and Di i+d,α = (Di,α)+), so that, for all i, {1, . . . , n} and α {1, . . . , d}, we have: i,α+d)E i,α+d Di Considering = {1, . . . , 2d}, the definition of as the union of edges of De and on input neuron layer , hidden layer S, and output neuron layer follows. Likewise, we define as the union of edges of Di and E. We verify that for Ge = e2[V ] and Gi = i2 [V ], we have Ge Gi = DE, and the Claim holds. α+d,j = Di,αEα,j. α,j + (De i,α = Di i,α Di i+d,α = i,α)E (De Considerations of building linear circuits. The above proof makes the neuron-synapse interaction graphs e, sparse in terms of the number of edges, as required to show that the number of parameters are preserved by correspondence. However, it is purely technical construction, and nodes in the synaptic layer have high degree, n. While preserving strict equivalence of linear dynamics, the degrees of nodes of the considered graphs in the synaptic layer can be reduced in this construction, at the cost of increasing the number of edges of graphs e, i. (For example, subdividing each node of the synaptic layer into a2 nodes can be used to reduce their degree Θ(a)-times, while increasing the number of edges Θ(a)-times; putting = (cid:112)n/d we reach graphs e, with degree O( nd) in both the neuron and synaptic layers, and consequently O(n nd) edges.) Reduction of internal degrees in this circuit is also possible by introducing more than 1 hidden layer, creating form of branching circuit. The implementation for this in distributed way remains very simple, as the considered dynamics of the form Gz are linear (token-propagation dynamics). The bound on the number of edges needed to represent such circuit remains O(nd), even when the circuit has constant degree. The technical construction of the linear circuits e, provided in this Appendix do not affect results concerning the analysis of the structure of neuron-neuron interaction graphs Ge, Gi. These neuron-neuron interaction graphs plausibly maintain heavy-tailed, power-law-like degree distribution, as is the case for the models considered empirically in Section 5.5. C.4 Formal statement of Claim 4 Claim 9. Let Dy, be parameter matrices of BDH-Normfree. Then, there exists graph Gy G(n, O(nd)), expressible through sparse linear circuit, graph Gs having O(nd) edges, and sparse linear value preparation function : R+n R+n, such that, for any sequence of keys (xτ,l)0τ and values (yτ,l1)0τ t, with xτ,l, yτ,l1 R+n, we have: (Gy Gy i)σ t1,lxt,l = DyEσt1,lxt,l, where σt1,l = (cid:80) t1,l = (cid:0)(cid:80) σ sparse attention on graph Gs, subject to appropriate preparation of attention values using function fy. tτ represents the attention state of BDH-Normfree following Eq. (16), and tτ (cid:1) Gs represents the corresponding attention state of the BDH system with τ <t A(yτ,l1)xτ,l τ <t yτ,l1xτ,l Before we start the proof, we make general point about the formulation of the claim. We are considering the problem of expressing (or more generally, approximating) the matrix operator σt1,l by another, sparser one. The setting of our problem can be distilled into obtaining an equality or approximation of the form Eσt1,l Eσ t1,l, where Rdn is given low-rank matrix, Rdn can be defined arbitrarily, and σ is defined as in the statement of the Claim. If we content ourselves with an approximation, then it is possible to have σ = σ (i.e., put fy = idem), using for example the stochastic sparsification framework of (Achlioptas and Mcsherry, 2007), or value-dependent variant (cf. e.g. (Krauthgamer and Sapir, 2023)). The samples chosen by such framework in value-dependent variant would lead to graph Gs which plausibly reflects the power-law element distributions that we empirically observe in σ. While the spirit of such an approximation is generally valid, we opt in the proof for simpler, purely technical argument applicable to our specific setting, which gives strict equality in the statement of Claim 9 subject to linear preparation of attention values with function A. In practice, this would mean that two successive layers of BDH with sparse state are sufficient to express layer of BDH-Normfree under this reduction. To prove the claim, it is enough to embed the connection structure of the encoder matrix, treating it as graph, into Gs. Proof. (of Claim 9) Fix arbitrarily subset of neurons, with = 2d. For the given matrix Rdn from also be BDH-GPU, let (R+)2dn be defined as in the proof of Claim 3 in Appendix C.3, and let Dy e, Dy 57 Figure 17: Non-uniform graph attention: interpretation of E(σl,t Gs) after sparsification of graph Gs. applied as in that proof for considerations of decoder Dy. Define the value preparation function as the immersion of vectors over into using E. Define Gs to be the all-ones matrix on the 2d columns corresponding to D, and zeros elsewhere. Then, define R2dn to be diagonal matrix acting on its first 2d elements (corresponding to = Dy D), and zeros elsewhere. Setting Gy iE, we obtain the claim. eE and Gy = Dy"
        },
        {
            "title": "D Desirable properties of a local graph dynamics for language models",
            "content": "We outline several general criteria of computational expressiveness and computational efficiency which distributed computing system has to meet to effectively deal with language and reasoning. For this, we take first-principles approach, relying only on very fundamental properties which an attention-based language model appears to need to capture, and which are applicable far beyond the specific case of BDH plausibly, being equally applicable to human and human-like reasoning.20 Hypothesis 2. We expect any efficient graph-based distributed system dealing with language and reasoning using an attention-based approach to have the following characteristics: [No Easy Simulation] The system achieves computationally irreducible dynamics, i.e., it provides no systematic opportunity to predict the outcomes of its inference or approximate its dynamics in numerically easier way than by running the system itself. [Particles Talk] The state-space dynamics of the distributed system is non-linear interacting particle dynamics, i.e., the system does not admit an efficient representation as non-interacting particle system, but relies on form of non-linear evolution expressed through (at least) two-particle interactions. (Such interactions are necessary, in particular, to enable multi-point correlation analysis on language inputs, when assuming only small number of inference steps of the system per output token.) [Attention Deforms Pairwise Connections] The system is capable of computing correlations between pairs of scalar variables localized at different nodes of the distributed system, and storing the state of such correlations so that the result is accessible from these two nodes. (This is plausibly needed to express attention in state-space system.) [Time Dictates Structure] The communication graph of the distributed system does not, in itself, represent any specific task input to solve, but reflects trained model (a program), whereas tasks are represented as inputs to this program, presented over time. The communication graphs used to solve language and reasoning problems are expected to display modular, scale-free structure. detailed discussion of the four items of the Hypothesis is provided below. 20In particular, the reader will have no doubt observed that graph settings applicable to language inference and reasoning systems, which involve task inputs spread out over time and the emergence of graph structure, are very different from graph-based frameworks which directly associate the task to solve with the communication graph (the latter case includes most considerations of: Graph Neural Networks, Graph Transformers, the LOCAL/CONGEST model of distributed computing, Approximate Message Passing systems, etc.) 58 [No Easy Simulation] Computational models have irreducible dynamics. We start by recalling general observation which is applicable to most learning systems (machine learning models, biological systems) that have learned how to do computations: they are likely to have chosen state-space dynamics that will allow them to resolve their computational problem with the least effort during inference. In other words, if there is physical system that solves given computational problem, and if there exists simulation S(P ) of this physical system that would approximate system with less effort, the learning system will be following the dynamics of S(P ), not those of . We provide few hypothetical examples for intuition, anchored in different areas of particle dynamics. If were the particle dynamics of electrons in resistor network, the simulation S(P ) could be calculation based on Ohms law with Laplacian solver and we would consequently expect the dynamics of our computational system to follow the Laplacian solver code, and not to simulate electron dynamics. If were the ensemble of billions of Internet users performing short walks clicking through links of the world wide web, the simulation S(P ) would be calculation of aggregate behavior, reminiscent of PageRank, and we would expect to encode the parallel dynamics of Map-Reduce matrix operations of PageRank, not the simulation of individual agents. If were quantum system amenable to approximation by perturbation theory, we would expect to simulate the (classical) calculus of this perturbation theory, and not the quantum system directly. Most mechanical systems admit some form of more efficient simulation, which means the the dynamics of such systems are rarely suitable choice for neuronal models. Anecdotally, in nature, only very simple systems like the Physarum slime mold (Jabr and Rothschild, 2012) rely on direct action (with hydrostatic pressure gradients) to perform their optimization process; and contemporary neuroscience research suggests that even the simplest neuronal brains do not perform their work in similar fluid-mechanical manner. The irreducibility of means that this system is stretched to the limits of stability, just as highly optimized numerical algorithm would be have been simplified and optimized to the limit of numerical stability. This relates to the limits of dimensionality reduction techniques that we have explored through largely equivalent information-lens perspective of loss of precision and loss of information which it inflicts upon the model. [Particles Talk] Latent concept spaces arise from outcomes of particle-particle interactions. Dynamics of systems with multiple particles moving around in (deformable) environment fall into two broad categories, depending on the strength of interaction between different parts of the dynamics. In the simpler setting, particles can be assumed at short time scales to be moving in an environment unchanged by other particles the concurrent action of other particles, which would change the environment, does not need to be taken into account when representing individual particle motion, nor is it necessary to consider particle-particle interactions. By contrast, in the more general setting, the dynamics of multiple particles are tightly coupled, and their dynamics need to be modeled (simulated) together. An example of dynamics with no coupling would be dynamics of multiple independent random walkers, such as the previously mentioned dynamics of electricity in wires, or the dynamics of PageRank. Examples of dynamics including interactions between particles, which may either happen directly or be moderated through the environment, include cellular automata, particle method simulations and molecular simulations, or swarms of communicating agents. The natural representation of state-space models as moving particles comes from the following interpretation. distributed system with depth-L computations (not least BDH or the BDH-GPU model given by the state equations (4)) is amenable to interpretation as system of walker particles performing an L-step walk over layers, starting at some token t0 in the input layer 0 and, in each time step t0, either pausing (skipping time step) or moving on to the next layer, until they reach the last layer in some time step tf , at which point they leave the system, contributing to the distribution of the tf -th output token. When attempting this approach with independent walkers, the distribution of tokens output by such system could be described by correlation functions following or resembling the Dyson series, (cid:80)t τ1=0 (input(τ1), . . . , input(τL)). However, the output of attention (e.g., the linear attention output given by equation (4) for BDH-GPU, or defined similarly in other state space models based on linear attention), cannot be represented as Dyson formula when unrolling the dynamics backwards through layers (even if it looks deceptively similar at first glance). Each entry retrieved from attention is an interplay between two moments of time: the moment at which the key-value pair was entered, and the moment at which the corresponding query arrived. In consequence, the considered dynamics can be represented, in each layer, as linear sum of two-point correlations between current time and some point τ in the past. Thus, in the l-th layer, this recursion can (with some τL1=0 . . . (cid:80)τ21 (cid:80)τL1 τL=0 approximation) be unrolled into linear combination of functions of sets of 2l input tokens (provided in the 0-th layer), but cannot be represented through correlation functions on smaller sets of tokens (e.g., of size linear in l). Otherwise put, system like BDH can be described using particles performing l-step walks when relying on intermediate elements of KV-state σ, which are produced during interactions with other walker particles in intermediate layers, but needs to be viewed through at least 2l-point correlation functions defined directly on input tokens in the input layer. The considered point is relevant because it precludes many forms of modeling of attention-based language dynamics, in particular those using non-interacting particle theories. The precluded approaches include: L-grams, word2vec-like L-skip-grams (Mikolov et al., 2013), as well as any other L-point correlations of past input tokens. L-step non-interacting random walk models (walks inside the network structure, which move from input layers towards output layers across time). systems known to be equivalent to the above, such as approximations of classical spin-chain systems by means of Feynman integral path lengths bounded by (Kalev and Hen, 2025), and many forms of graph/GNN kernels based on L-th powers of the graph Laplacian. by extension, L-layer state-space systems which perform excessive compression (size reduction) of their state, in way which eliminates most long-term correlations. We can ask if this requirement for communication between particles is an artifact of the construction of BDH (and similarly, of the Transformer), or if it comes from genuine need related to language and reasoning tasks. For language problems per se, the need for multi-point token correlation in L-layer language modeling plausibly follows from the expectation that the model should have the ability to create syntax tree of sentence by means of single quick parallel scan over words in this sentence. With this assumption, the depth of computation used to build language syntax tree should be sufficient to represent the number of levels of the syntax tree that the model is able to process naturally, but can be (and in general, should plausibly be) much smaller than the number of leaves (words) of this syntax tree. This is consistent with the RASP-L-based understanding of the Transformers capabilities, which allows for expressing depth-L trees in depth-L Transformer.21 Such way of mapping the tree structure of problems into the models layers, from bottom to top, also essentially captures the generative nature of the considered models, which rely on concept spaces created and stored in state in intermediate layers, to guide both language comprehension and reasoning on language. Thus, the ability to handle language syntax trees efficiently, in itself, precludes the previously-mentioned types of modeling approaches. [Attention Deforms Pairwise Connections] The interaction process X(i), (j) σ(i, j) describes attention. The preceding discussion in paragraph [Particles Talk] grounds state-of-the-art state-space language models in the world of interacting particle systems. Whenever the global vector-based description of state-space model calls for three-point operation, such as the trilinear operation of key-value-query attention, this translates into the nature of pairwise (for polynomial interaction terms, degree-two) non-linear particle interactions in the transition equations of the same model when described at the level of particles. Notably, at scale, the state-space transition equations of an attention-based model plausibly involve altering or deforming correlation strength between pairs of particles, with such pairs being represented as interaction variables in the state of the system. This requirement on structure, repeated across layers, can be seen as sufficient: interactions of particle pairs are about the only requirement on non-linear rulesets that the system needs to be support, as demonstrated by the simple local transition rules of BDH. Overall, the statement attention is all you need, which describes system-level global property, translates into X(i), (j) σ(i, j) is all you need at the level of particle dynamics of state-space language model. [Time Dictates Structure] Inputs to reasoning problems are sequential, not graph-based. Many real-world graphs are anchored in spatial embedding of their nodes which is given by external constraints. For example, the structure of many social and transportation networks is impacted by the geographical placement of people and infrastructure on the globe. 21This does not mean the problem is easy; synthetic problems inspired by this type of tree problem were (for us) among the hardest to train into Transformer with no Chain-of-Thought as compared to RASP-L problems described in (Zhou et al., 2024) and others we tested. 60 In designing the dynamics for BDH, we are free from such spatial constraints. The graph topology corresponding to the model is free to take the shape needed to best resolve the problem. The problem itself is encoded as sequence of tokens which arrive over time to the model (we take here state-space view of the system). We can naturally presume that the structure of the model graph of BDH is shaped in way which follows from two aspects: this temporal encoding of information, and from the abstract (Platonic) latent space of concepts needed to deal with language and reasoning. When looking for the right particle dynamics for language models, it seems reasonable to discard all unnecessary aspects of spatial constraints. One example of particle interaction system which includes externally imposed constraints on the structure of the state space is that of cellular automata operating on two-dimensional grid. While 2D cellular automata have appealed to public imagination, appearing in attempts to observe the emergence of intelligence at least since the 1970s, they are, in fact, an extremely cumbersome choice for representing in-context reasoning or language for any attention-based model. State-of-the-art language models seem to have no structural need for low-dimensional grid in their dynamics. Arguably, the connection structure which needs to emerge in graph system, allowing it to work efficiently in setting of efficient information search is precisely the opposite: it is multi-scale, expander-like system of shortcuts, cf. e.g. (Fraigniaud and Giakkoupis, 2010). This scale-free graph structure is expected to correspond to the scale-free temporal behavior observed in natural systems (He et al., 2010). In the rest of this paragraph we briefly review other areas of computer science, and how they relate to the particle dynamics we are looking for in terms of their relationship to handling temporal inputs and the constraints they impose on the structure of the state-space. The freedom of choice of graph topology in solving problems around language and in-context reasoning, which we are dealing with here, can be contrasted with settings in which the graph is, at the same time, part of the system dynamics (encoding interactions in the system) and part of the statement of the problem input. This is particularly true for models of distributed computing inspired by computer networking (LOCAL, CONGEST, etc.) and other forms of interaction networks (Approximate Message Passing, quantum LOCC, etc.), where the same graph represents the communication network for the dynamics, and encodes the problem input with the required output being some function of (e.g., clustering, coloring, spanning tree, etc.). Some distributed problems on graphs can be formulated so that the input and required output are independent of the graph structure, the notable ones being: majority consensus, leader election, information broadcasting, and computing aggregates. For such problems, the graph represents only communication system, whose topology is more an obstacle to overcome, than an actual help in solving the problem. This applies also to architectures in Machine Learning which adhere to known graph structure, such as Graph Neural Networks or Graph Transformers, when solving problems whose inputs are not naturally embedded in such structure. handful of approaches in distributed computing are intended to describe systems which compute function of an input signal which, like language, is spread out sequentially over time, and where computations happen while this signal is still arriving. In particular, some forms of particle dynamics can be distilled from the theory of selfstabilizing systems (Dolev, 2000), giving rise to settings where the system is expected to adapt its state in response to time-changing input (see e.g. (Boczkowski et al., 2019)). Among distributed streaming frameworks, one approach which, owing to its design, admits an elegant particle-based interpretation for time-changing inputs, is the incremental computing framework (McSherry et al., 2013). This framework emphasizes temporal commutativity, and is well suited to expressing dynamics of non-interacting particles, such as PageRank-like computation performed incrementally with Map-Reduce on time-changing graphs, or building nearest-neighbor indexes on sets of changing vectors. It does not naturally extend to the non-linear particle-particle interaction dynamics that appear in the context of attention (see paragraph [Particles Talk]). BDH-GPU PyTorch code listing The code listing below implements BDH-GPU (Definition 4) for PyTorch version 2.7. It is self-contained, except for the implementation of RoPE which needs to be filled by the user. With respect to the state dynamics of Eq. (8), it provides an extension supporting heads. The placement of layer norms and residual connections is modified with respect to Eq. (8); in general, this aspect offers some flexibility. This implementation assumes the simplest case of fixed context window of length . An unbounded context window is technically supported using state-space kernel for Linear Attention, and works best following appropriate adaptation of the model for truncated backpropagation through time (see Appendix B.2). 61 import import from c import nn r o . nn . c n s # e l e o # d = 256 = 4 = 32768 = 6 p = 0 . 0 5 a _ e = 256 # e # r s a BDH_GPU( nn . Module ) : _ _ t _ _ ( f ) : f . = nn . LayerNorm (D, f . wte = nn . Embedding ( a _ e , D) f . p = nn . p ( p ) f . o = nn . a e ( m w _ i = s , s = s ) c . o ( ( N, ) ) . m _ ( = 0 . 0 2 ) ) f . o _ = nn . a e ( r . o ( ( H, D, / / ) ) . m _ ( = 0 . 0 2 ) ) f . o _ = nn . a e ( c . o ( ( H, D, / / ) ) . m _ ( = 0 . 0 2 ) ) f . d = nn . a e ( c . o ( ( D, a _ e ) ) . m _ ( = 0 . 0 2 ) ) f . n = e t t ( ) f w ( f , , = . e ( ) _ = f . ( f . wte ( ) . q z ( 1 ) ) # , 1 , , # i t i s s ) : _ range ( ) : = . u ( _ @ f . o _ ) # , H, , / / _ = f . n ( Q=x , K=x , V= _ , ) = . u ( f . ( _ ) @ f . o _ ) * # , H, , / / = . n s ( 1 , 2 ) . h ( , 1 , , N) = f . p ( ) # r v _ = _ + f . ( @ f . o ) _ = f . ( _ ) e t c s , # , 1 , , t v _ . e ( 1 ) @ f . d t # , , a _ e s n A n n ( nn . Module ) : f a (Q, K, ) : Qr = RoPE (Q) Kr = RoPE (K) t ( Qr @ Kr . mT ) . l ( g l = 1) @"
        }
    ],
    "affiliations": [
        "Pathway, Palo Alto, USA"
    ]
}