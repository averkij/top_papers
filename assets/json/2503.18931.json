{
    "paper_title": "CoMP: Continual Multimodal Pre-training for Vision Foundation Models",
    "authors": [
        "Yitong Chen",
        "Lingchen Meng",
        "Wujian Peng",
        "Zuxuan Wu",
        "Yu-Gang Jiang"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Pre-trained Vision Foundation Models (VFMs) provide strong visual representations for a wide range of applications. In this paper, we continually pre-train prevailing VFMs in a multimodal manner such that they can effortlessly process visual inputs of varying sizes and produce visual representations that are more aligned with language representations, regardless of their original pre-training process. To this end, we introduce CoMP, a carefully designed multimodal pre-training pipeline. CoMP uses a Continual Rotary Position Embedding to support native resolution continual pre-training, and an Alignment Loss between visual and textual features through language prototypes to align multimodal representations. By three-stage training, our VFMs achieve remarkable improvements not only in multimodal understanding but also in other downstream tasks such as classification and segmentation. Remarkably, CoMP-SigLIP achieves scores of 66.7 on ChartQA and 75.9 on DocVQA with a 0.5B LLM, while maintaining an 87.4% accuracy on ImageNet-1K and a 49.5 mIoU on ADE20K under frozen chunk evaluation."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 4 2 ] . [ 1 1 3 9 8 1 . 3 0 5 2 : r COMP: Continual Multimodal Pre-training for Vision Foundation Models Yitong Chen1,2* Lingchen Meng1* Wujian Peng1,2 Zuxuan Wu1,2 Yu-Gang Jiang1 1Shanghai Key Lab of Intell. Info. Processing, School of CS, Fudan University 2Shanghai Innovation Institute https://slimm-x.github.io/comp"
        },
        {
            "title": "Abstract",
            "content": "Pre-trained Vision Foundation Models (VFMs) provide strong visual representations for wide range of applications. In this paper, we continually pre-train prevailing VFMs in multimodal manner such that they can effortlessly process visual inputs of varying sizes and produce visual representations that are more aligned with language representations, regardless of their original pretraining process. To this end, we introduce COMP, carefully designed Continual Multimodal Pre-training pipeline. COMP uses Continual Rotary Position Embedding to accommodate visual inputs with different resolutions, and an Alignment Loss between visual and textual features for better cross-modal alignment. After continual pre-training, leading VFMs like DINOv2 and SigLIP achieve remarkable improvements not only in multimodal understanding tasks but also in generic classification and segmentation tasks. Remarkably, COMP-SigLIP achieves scores of 66.7 on ChartQA and 75.9 on DocVQA with 0.5B LLM, while maintaining an 87.4% accuracy on ImageNet-1K and 49.5 mIoU on ADE20K under frozen chunk evaluation. 1. Introduction Pre-training Vision Foundation Models (VFMs) capable of extracting transferable representations for various downstream tasks has been long pursuit of the computer vision community. The key to pre-training is to scale up models and data through constructing strong supervisory signals with weak-strong augmentations in vision-only pretraining [9, 10, 24, 50] or cross-modality alignment in vision-language pre-training [57, 68, 80]. These VFMs often demonstrate strong performance for variety of downstream tasks, and can be easily combined with Large Language Models (LLMs) by designing lightweight adapters * Equal contributions. Corresponding author. Figure 1. Overview of COMP. Our method accepts an image at native resolution and its corresponding text. Then, in addition to training through text decoding in next-token prediction paradigm, we also explicitly project the visual features into the language space of LLM using Alignment Loss. that project visual features into text space, serving as the eyes of language models. In this paper, we revisit these widely used VFMs such as vision-only pre-training DINOv2 [50] and vision-language pre-training SigLIP [80]. We argue that these prevailing VFMs, regardless of their pre-training procedures, can be further boosted through continual multimodal pre-training. This allows VFMs to (1) better process visual inputs of arbitrary sizes without requiring resizing, when used as vision encoders of LMMs; (2) produce outputs that are more aligned with language representations, thereby improving multimodal understanding, significantly benefiting encoders from vision-only pre-training. On one hand, equipping VFMs with the ability to deal with images of varying sizes is the core for visual understanding, as image resolutions directly impact the richness of the information within an image. However, existing ap1 proaches treat an image as worth 1616 words[18], resizing all images to predefined size. This one-size-fitsall strategy results in the loss of critical details, impeding the models ability to perceive fine-grained information. This is particularly detrimental in tasks that demand highresolution inputs, such as chart understanding[46], document parsing [47], and fine-grained recognition [53]. Recent works [7, 50, 64, 68, 80] have attempted to address this challenge by improving bilinear interpolation of positional embeddings and incorporating multi-resolution training. However, they still struggle with real-world scenarios involving diverse input resolutions, as they remain constrained by the limitations of resolution extrapolation. On the other hand, we believe there is still representation gap between VFMs and LLMs, which results from their distinct training objectives and data modalities during pre-training [37]. To bridge this gap and enable LMMs to better understand visual inputs, the mainstream approach [32, 41, 43] involves training an adapter that projects the visual embeddings of VFMs into the textual embedding space of LLMs, typically through next-token prediction for text tokens. However, relying solely on textbased supervision is insufficient to effectively and directly reduce this gap [52], particularly when VFMs have not undergone vision-language alignment pretraining [65, 83]. To address these challenges, we introduce COMP, continual pre-training pipeline that is carefully designed to enhance exisiting VFMs. Specifically, COMP builds upon (1) C-ROPE, Continual Rotary Position Embedding for vision models, which is operated by adding the standard RoPE-2D [63] with the learned 1D position embedding, to support native resolution continual pre-training; (2) Alignment Loss, cross-entropy loss between visual and textual features through language prototypes, to align multimodal representations between pre-trained VFMs and LMMs. As shown in Fig. 1, our method accepts an image of native resolution and its corresponding text. Then, in addition to training through next-token prediction on the text, we also explicitly project the visual features into the language space using Alignment Loss by word embedding of LLM. With three-stage continual pre-training, our models excel not only on multimodal understanding, but also in other downstream tasks such as classification and segmentation. Our contribution can be summarized as: We propose continual multimodal pre-training method COMP, including two techniques, C-ROPE and Alignment Loss, to enable pre-trained VFMs supporting native resolution, and aligning representation space of LLM. With the COMP, we present COMP-SigLIP and COMPDINOv2, which achieve remarkable improvements not only in multimodal understanding but also in traditional visual tasks such as classification and segmentation. Based on COMP-SigLIP, we introduce COMP-MM-1B and COMP-MM-7B, which significantly outperforms all other methods under the similar pre-training data size. We conduct comprehensive experiments and ablation studies on different models and different tasks, which provide useful insights behind the design choices. 2. Related Work Vision Foundation Models. Large-scale vision pretraining has achieved remarkable breakthroughs [10, 24, 57, 68, 80], particularly with vision transformers [18] as the backbone, forming the foundation of visual understanding. These pre-training approaches can be broadly categorized into two main directions: vision-only pre-training and vision-language pre-training. In vision-only pre-training, models are trained by either distinguishing image-or patchlevel entities from different views using contrastive learning [9, 10, 24, 50] or reconstructing masked patterns back to the raw image [6, 25, 66]. In vision-language pretraining, models are encouraged to align visual and linguistic features into joint semantic space by leveraging webIn this scale image-text pairs as the pre-training corpus. paper, we explore continual pre-training paradigm built upon two representative models: DINOv2 [50] for visiononly pre-training and SigLIP [80] for vision-language pretraining. Our approach enhances these models with finegrained and open-world capabilities beyond image-level pre-training while preserving their original strengths, e.g. image-level classification and pixel-level segmentation. Large Language Models. Leveraging the strong representational power of Transformers [69], Large Language Models (LLMs) can be pre-trained on large-scale unlabeled text corpora. Specifically, BERT [17] adopts an encoderdecoder architecture and introduces masked language modeling paradigm, where parts of sentence are randomly masked and then predicted by the model. This approach has proven effective for representation learning and has demonstrated strong performance on downstream tasks after finetuning. Subsequent works [29, 30, 58] further improve the performance through multi-task training and data scaling. Meanwhile, GPT series [8, 55, 56] utilize decoder-only Transformers and optimize under the next-token prediction paradigm with causal mask, enabling emergent text generation capabilities. Building on this foundation, InstructGPT [51] and ChatGPT enhance instruction-following capabilities, making LLMs more suitable for real-world apIn response to ChatGPT, recent open-source plications. projects such as LLaMA [19, 67], Qwen [3, 76, 77], and DeepSeek [14], have attracted significant attention, driving further advancements in the community. In this paper, we leverage pre-trained LLMs as an interface to process various forms of text supervision by captioning the given image, enhancing vision backbones for diverse scenarios. 2 Multimodal Pre-training CLIP [57] and its followups [27, 54, 59, 81] demonstrate the effectiveness of aligning vision and language modalities into unified semantic feature space using paired image-text supervision, showcasing promising capabilities in open-set image-level classification and retrieval. However, this vision-language approach faces challenges when dealing with fine-grained tasks, e.g. segmentation and detailed caption [39, 72], due to its holistic representation and resolution. More recently, models like Flamingo [2], CoCa [78], and BLIP [33] introduced cross-attention mechanism to handle imagegrounded captions via image-to-text cross-attention, enabling pre-trained models to generate captions for visual inputs. Additionally, with the rapid advancements in large language models (LLMs), recent works leverage pre-trained LLMs as interfaces alongside pre-trained vision encoders to build powerful large multimodal models capable of addressing complex visual question-answering tasks. BLIP2 [34] and InstructBLIP [13] utilize the previous crossattention to deal with visual tokens, while LLaVA [40, 41] series project visual features as sequence of visual tokens and feed them as inputs of LLM. The follow-up works [4, 11, 22, 38, 43, 49, 70, 71] focus on improving multimodal understanding through image tiling, data scaling, and improved vision token processing. In contrast to these approaches, we focus on improving the foundational visual capabilities through text-supervised generative pretraining paradigm. Furthermore, we enhance high-resolution capabilities by incorporating RoPE-2D into visual encoding and introducing an additional training objective to better align visionlanguage features. As result, our method demonstrates strong performance in multimodal understanding while also improving traditional vision tasks. 3. Method Our goal is to empower pre-trained vision foundation model (VFM) with the ability to process images at their native resolution while aligning its encoder features with the representation space of pre-trained LLM. To achieve this, as shown in Fig. 2 (a), we propose continual multimodal pre-training pipeline that improves existing VFMs so that they natuallry handle native-resolution inputs using C-ROPE (Sec. 3.1) and can better align with language embedding through carefully designed loss functions: the text decoding loss (Sec. 3.2) and the cross-modal alignment loss (Sec. 3.3). These components are integrated into three-stage training framework (Sec. 3.4), ensuring effective adaptation and alignment. 3.1. Native Resolution Adaptation Vision encoders often use fixed-size inputs during the pretraining stage, and thus struggle to handle images with varying resolutions, particularly high-resolution images for finegrained visual understanding. While training on images of different sizes is straightforward solution, it is particularly challenging due to the predefined shape of position embeddings in vision transformers. common approach is to interpolate the original position embeddings in an online manner to accommodate different input resolutions, yet the results are unsatisfactory [7, 64]. Inspired by the success of Rotary Position Embedding (RoPE) that demonstrates strong extrapolation capabilities [67, 75] in NLP, we aim to build upon RoPE to handle sequence of visual tokens. Unlike previous methods [1, 5, 15, 71] that rely on single position embedding, we leverage both absolute and relative embeddings to capture richer positional information so as to handle variety of high-resolution inputs. To best leverage the pretrained knowledge and ensure smooth transition from the pre-trained vanilla ViT to arbitrary resolutions, we begin by interpolating absolute positional embeddings and then incorporate RoPE-2D. We refer to this method as C-ROPE. Specifically, as shown in Fig. 2 (b), 2D image of resolution (H, ) is patchified into = HW/P 2 patches RN (P 2C), where denotes the patch size of vision xp encoder and indicates the number of image channel. The image encoding processes can be expressed as: (1) (2) (3) pE; x2 z0 = [x1 qi, ki, vi = pE; rojq(zi), ; xN E] + nt(Epos) rojk(zi), rojv(zi) (Rqi)(Rki) )v) rojo( of tmax( yi = zi + zi+1 = yi + , 1 (4) FFN (yi) where = 0, R(P 2CDv) and Epos RN Dv indicate where patch embedding and learnable position embedding, Dv indicates the hidden dimension of vision encoder, ) nt( represents bilinear interpolation, ) is the projection roj( layer, ) denotes standard feed-forward network, and denotes the number of encoder layers. In particular, in Eq. (3) is the 2D rotary matrix: FFN ( Rx,y = cos xθ sin xθ 0 0 sin xθ cos xθ 0 0 0 0 cos yθ sin yθ 0 0 sin yθ cos yθ 3.2. Text-supervised Generative Pre-training Text-supervised generative pre-training is widely used in large multimodal models (LMMs). It extends the standard text-only autoregressive next-token prediction framework [8, 55, 56] to visual inputs, mapping visual features into the input layer of large language model via querydriven cross-attention [2, 13, 35] or projection [41]. In this work, we adopt the projection-based multimodal framework 3 Figure 2. Architecture of our COMP. (a) Overview of our pre-training framework; (b) Detail of C-ROPE. For ease of visualization, the projection layers Projq,k,v,o and scale operators are omitted. due to its simplicity and effectiveness. Formally, the projecFψ can be defined as follows: tion procedure Fψ( where = zL in Eq. (4). Then, the image tokens Hv are fed into the input layer of the LLM Pθ, serving as the visual condition for the corresponding text Xt. The text decoding loss can then be expressed as: Hv = (5) ) Ldec = 1 +T (cid:88) i=V +1 log Pθ(Xi X<i, Hv) (6) where and denote the number of visual and textual tokens, respectively. To support the autoregressive generation process, the image-grounded text decoder utilizes causal self-attention mechanisms. 3.3. Vision-language Representation Alignment Thanks to text-supervised generative pre-training and decoding loss, the vision encoder can be optimized using paired images and texts. However, the supervision is too distant for directly optimizing the vision transformer, especially when the original pre-training of vision encoder does not involve vision-language alignment, such as in imageonly SSL of DINOv2 [50] (please refer to the Ablation Sec. 4.5 for the further analysis). To bridge the modality gap between the vision encoder and the language model, we utilize direct vision-language representation alignment by encoding visual and textual features through the vision and text models, respectively. Inspired by the knowledge distillation in DINO [9, 50], as shown in Fig. 3, we aim to align visual and textual representations by treating the word embeddings of the LLM as prototypes [52]. In contrast to the self-distillation approach in DINO, we frame the distillation process as crossmodality text-to-image distillation. Specifically, we first obtain global visual and its corresponding text features Fv and Figure 3. Alignment Loss. We illustrate it in the case of one single pair of global vision and text features Fv and Ft for simplicity. Fv and Ft are mapped by frozen learned prototype W, i.e., the word embedding of LLMs. Then, they are converted into normalized probabilities using the Softmax function and iterative SinkhornKnopp algorithm [12], respectively. Then, cross-entropy is applied as the loss. To prevent information leakage, the text features are extracted without image prefixes. RBDt through the VFM and LLM by parameter-free Ft global average pooling, respectively, as follows: Fv = ool(Hv), Ft = ool( Pθ(Xt)) (7) where denotes the mini-batch size, Dt represents the hidden dimension of LLM and Xt is piece of corresponding text to Hv. And then, we map Fv, Ft into language space by the prototype RDtK as follows: Cv = WFv, Ct = WFt (8) where indicates the vocabulary size of LLM. To prevent information leakage during training, the text features are extracted without image prefixes. Additionally, we detach the gradient of the word embedding to avoid training collapse."
        },
        {
            "title": "Model",
            "content": "#PT #IT Text-rich and Fine-grained General and Real-world ChartQA DocVQA AI2D Inst-IT VQAv2 GQA MMMU MMBench RWQA 3.75M N/A Deepseek-VL [45] LLaVA-OV (SI) [32] 4.6M 3.2M 61.0 4.6M 4.8M 61.4 LLaVA-OV [32] 4.6M 3.2M 66.7 COMP-MM - 558K 665K 18.2 LLaVA-1.5 [40] 558K 765K 54.8 LLaVA-NeXT [43] 3.75M N/A Deepseek-VL [45] Cambrian-1 [65] 1.2M 7.0M 73.3 LLaVA-OV (SI) [32] 4.6M 3.2M 78.8 4.6M 4.8M 80.0 LLaVA-OV [32] 4.6M 3.2M 79.6 COMP-MM - - 75.0 73.7 75.9 1B models 51.5 54.2 57.1 61. - 44.2 47.8 50.1 28.1 74.4 - 77.8 89.3 90.2 91.0 7B models - - 65.3 73.0 81.6 81.4 81.4 32.1 42.4 - - 61.8 71.7 65.0 - - - 78. 78.5 81.8 - - - - 81.9 - - - 60.4 62.0 64.2 - 64.6 - - 65.9 32.2 31.2 31.4 33.0 35.3 35.1 36.6 42.7 47.3 48.8 48.9 - 43.8 52.1 56. - - - - 81.7 80.8 81.4 - 53.7 55.6 58.3 - - - 64.2 65.5 66.3 66.4 Table 1. Main results of COMP-MM on multimodal understanding benchmarks. #PT indicates the size of pre-training dataset. #IT indicates the size of intrcution tuning dataset. N/A indicates the size is unknown. denotes we report the performance on validation sets. Moreover, for adapting the learned prototype, we replace the Softmax function, which assumes uniform distribution, with the Sinkhorn-Knopp algorithm [12] that explores the prior distribution of word embeddings [52] to obtain soft normalized probabilities of Ct: The training objective can be formally expressed as: (cid:40) = Ldec + α Ldec Lalign in Stage-I & II in Stage-III (11) where α is loss weight to balance Ldec and Lalign. pt = iag(uW )exp( Ct ϵ ) iag(v) (9) 4. Experiments RK is the prior marginal distribution of words, RB are renormalization vectors. Thus, the alignwhere uW and ment loss can be formally expressed as: Lalign = pt log pv (10) where, pv = of tmax(Cv). Additionally, we stop the gradient propagation for LLM, ensuring that Lalign updates only the parameters of VFM. 3.4. Training Recipe Our continual pre-training is divided into three stages: Stage-I: Vision-language adapter warming up. In this stage, we freeze the VFM and the LLM, and only train the adapter at fixed low image resolution without RoPE-2D. In this stage, we train the entire model with RoPE-2D piece of time at fixed high image resolution, followed by another piece of time at native resolution. Stage-II: Native resolution adaptation. Stage-III: Instruction tuning (optional). In this stage, we fine-tune the entire model on the instruction dataset at native resolution with RoPE-2D, to accommodate different types of data inputs. 5 All experiments are conducted on two strong mainstream VFMs, SigLIP [80] and DINOv2 [50], to verify the applicability of our method across models with different pre-training objectives. We evaluated the performance of our COMP-MM on multimodal benchmarks with other LMMs. Additionally, we conducted detailed comparisons with other VFMs across various visual downstream tasks, including multimodal understanding, image classification, and semantic segmentation. 4.1. COMP-MM Setup. We utilize SigLIP-So400M [80] as pre-trained VFM and Qwen2.5-0.5B, Qwen2.5-7B [77] as pre-trained LLM, and the cross-modality adapter is 2x2 downsampling MLP. For Stage-I, we train the adapter on LLaVAPretrain data [41] at 384px resolution. For Stage-II, we train the full model on LLaVA-Mid-Stage data [32] at 1024px and native resolution. To support high resolution inputs, we replace 1M data in CC3M [61] with Densefusion-1M [36]. For Stage-III, we train the full model on LLaVA-OV-SI SFT data [32] at native resolution. All experiments are conducted on 8 H100. Results. As shown in Tab. 1, under the slimilar pre-training data size, our model significantly outperforms all other"
        },
        {
            "title": "Model",
            "content": "ViT #Patches OKVQA TextVQA DocVQA InfoVQA ChartQA SEED MME L/14 DINOv2 [50] G/14 DINOv2 [50] L/14 CLIP [57] L/14 SigLIP [80] So/14 SigLIP [80] L/14 AIMv2 [20] H/14 AIMv2 [20] COMP-DINOv2 L/14 So/14 COMP-SigLIP 576 3034 576 576 729 576 576 576 576 54.1 56.9 60.0 59.3 60.1 60.8 61.3 59.0 61.0 13.4 15.1 47.5 44.1 47.5 53.6 55.5 53.6 62.5 7.3 8.2 25.6 16.9 19.2 26.6 27.8 24.7 34. 21.3 19.7 21.8 20.7 21.0 22.8 23.1 22.8 26.0 10.8 12.0 19.2 14.4 14.7 19.2 19.9 23.8 25.0 57.0 68.9 70.1 66.8 67.5 71.8 72.1 72.8 74.3 1345 1423 1481 1416 1433 1472 1545 1484 1543 Table 2. Evaluation on multimodal understanding benchmarks. We conduct extenseive experiments on COMP-SigLIP and COMPDINOv2 with LLaMA-3.0 8B [19], freezing the vision encoder and directly tuning on LLaVA SFT data [41] for one epoch. #Patches indicates the number of input visual patches for the LLM. denotes we report the performance on validation sets. methods and achieves state-of-the-art performance among open-source models across multiple benchmarks and on both 1B models and 7B models. Specifically, COMP outperforms LLaVA-OV-SI [32], strong baseline that employs the AnyRes technique for high-resolution input, not only on text-rich and fine-grained understanding tasks such as ChartQA [57], DocVQA [47], AI2D [28], and InstIT [53], but also on various general and real-world multimodal understanding tasks like VQAv2 [23], GQA [26], MMMU [79], MMBench [44] and RealWorldQA [73]. Model ViT Pre-training Res. 224px 448px L/14 AIMv2 [20] L/14 AIMv2 [20] L/14 AIMv2 [20] H/14 MAE [25] L/14 224 DINOv2 [50] L/14 COMP-DINOv2 L/14 224 CLIP [57] SigLIP [80] So/14 224 LLaVA-SigLIP [32] So/14 224 COMP-SigLIP So/ 224px 448px Native 224px Native 86.6 78.9 86.1 78.5 518px 86.3 85.7 336px 84.4 384px 87.1 384px 83.2 86.5 Native 84.8 87.9 87.1 - 87.6 86.5 83.8 88.2 84.4 87.4 Table 3. Evaluation on frozen trunk classification. All experiments are conducted on ImageNet-1K [16] at 224px and 448px by utilizing attentive pooling probing. 4.2. Multimodal Understanding Setup. To further quantify the performance of COMP for LMMs, we compare it with other mainstream VFMs following the settings and hyperparameters in [20]. Specifically, we reinitialize an adapter between COMP vision encoder and LLM, e.g. LLaMA 3.0 8B [19], and freeze the parame6 ters of vision encoder all the time. We train the adapter and the LLM jointly in single stage on LLaVA SFT data [41] for one epoch, and scale up the learning rate of adapter by factor of 8. To ensure fairness, we used the checkpoint before instructing tuning (Stage III) to confirm that the model had not been exposed to instruction tuning data and fixed the number of patches input to the LLM at 576. Results. We evaluate COMP across various benchmarks covering general knowledge (OKVQA [60], SEEDBench [31], MME [21]) and text-rich (TextVQA [62], DocVQA [47], InfoVQA [48], ChartVQA [46]) tasks. As presented in Tab. 2, our models outperform CLIP, SigLIP and DINOv2 by significant margin. Notably, our COMP-SigLIP-400M outperforms AIMv2-H (600M) on most tasks, and our COMP-DINOv2-L also surpasses DINOv2-G, demonstrating the effectiveness of our COMP. Model ViT 504px 672px InternViT-v2.5 [11] DINOv2 [50] COMP-DINOv2 SigLIP [80] SigLIP 2 (NaFlex) [68] LLaVA-SigLIP [32] COMP-SigLIP 6B L/14 L/14 So/14 So/16 So/14 So/14 55.4 55.3 52.7 35.2 35.3 39.9 49. 53.9 55.9 53.0 31.6 34.8 36.5 49.1 Table 4. Evaluation on semantic segmentation. All experiments are conducted on ADE20K [82] at 504px and 672px by freezing the backbone and only train the UperNet [74] head. 4.3. Image Recognition Setup. We evaluate the global-view semantic quality of COMP by image classification on ImageNet-1K [16]. In detail, we utilize attentive pooling probing, i.e. adding an # 1 2 3 4 5 6 7 9 10 AI2D"
        },
        {
            "title": "Baseline Recipe",
            "content": "+ RoPE-2D from Stage-I + RoPE-2D from Stage-II + Freezing LM in Stage-II + Native Resolution Training + Increase Resolution to 1024px in Stage-II + Scale Up Training Data + Alignment Loss Replace with DINOv2-Large in #7 + Alignment Loss 50.2 51.5 55.4 54.6 56.7 56.2 62.0 61.9 58.9 59.6 29.9 11.0 56.9 55.4 59.5 59.9 65.2 66. 61.5 64.0 24.1 11.4 61.6 55.3 67.7 68.8 75.0 75.9 68.5 70.1 - 10.1 23.2 2.87 3.33 0.33 5.77 0.77 - 1. Table 5. Ablation on training recipe. The first row presents the baseline three-stage training recipe, with progressive resolutions of 384px, 576px, and 768px. In the first stage, only the adapter is unfrozen, while in the subsequent stages, the full model is fine-tuned. Each following row modifies strategy from the last non-italicized row, and our final recipe is highlighted in blue . Experiments are run using SigLIP-400M and Qwen2.5-0.5B. : Unfreezing vision encoder for RoPE training; : The average of the difference from previous row. attention pooling layer on top of the frozen features, to train our method at fixed 224px and 448px. To further analyze the recognition performance of our model, we evaluate SigLIP variant from the checkpoint of LLaVA-OV-SI-0.5B model, denoting it as LLaVA-SigLIP. All our vision models are from Stage-II of COMP-MM-1B. Results. As shown in 3, our model preserves the rich global features of the original SigLIP and DINOv2 while supporting native resolution inputs, and the COMP-SigLIP surpasses the native resolution version of AIMv2. Notably, our COMP-SigLIP also outperforms LLaVA-SigLIP from LLaVA-OV-SI-0.5B checkpoint by large margin, indicating our approach can better preserve the classification ability after the continual pretraining. 4.4. Semantic Segmentation Setup. We evaluate the local-view semantic quality of COMP semantic segmentation on ADE20K [82], utilizing UperNet [74] by head tuning. Specifically, we freeze the backbone and only train the UperNet head at fixed 504px and 672px. To further analyze the segmentation performance of our model, we evaluate SigLIP 2 NaFlex variant [68], which also supports native resolution inputs, and LLaVA-SigLIP which is from the checkpoint of LLaVAOV-SI-0.5B model. All our vision models are from Stage-II of COMP-MM-1B. Results. As shown in Tab. 4, for vision-only pre-training DINOv2, which excels at extracting dense features, our continual multimodal pre-training effectively preserves the semantic segmentation capability; for vision-language pretraining SigLIP, which suffers from extracting local feature, COMP significantly enhances its pixel-level understanding ability with great margin. 4.5. Ablation Study Improved Training Recipe. Tab. 5 presents the roadmap from the commonly used LLaVA-1.5-like architecture to our proposed COMP. Specifically, we begin our experiments with data combination strategy basically following LLaVA-OV [32]. This setup includes LCS-558K in Stage for initializing the projection module, approximately 4M recaptioned data in the middle stage for high-quality knowledge learning, and around 0.8M single-image instructiontuning data from LLaVA-Next [43]. During the first stage, only the adapter is unfrozen, while in the subsequent stages, the full model undergoes fine-tuning. We adopt SigLIP-So400M [80] as the vision encoder and Qwen2.5-0.5B [77] as the language decoder. Additionally, we progressively increase the input resolution across stages, using 384px, 576px, and 768px, respectively, by interpolation to the fixed-size position embeddings. As shown in line #1 of Tab. 5, the baseline model struggles with tasks such as ChartQA and DocVQA, which involve high-resolution images and require fine-grained visual understanding. To support native resolution, we explore integrating ROPE-2D while retaining the original fixed-size position embeddings. However, adding ROPE-2D in the first stage leads to training collapse, likely due to the frozen vision transformer struggling to adapt to ROPE-2D, resulting in unstable optimization. In contrast to the previous works [71] that freeze the language model before the instruction tuning stage, we find it is necessary to unlock the language model for better multimodal pertaining, refer to line#4. Building on effective high-resolution pretraining, we introduce an additional round of dynamic resolution pretraining in Stage 2, adjusting to the native resolution of visual inputs. This strategy consistently enhances performance across various benchmarks. 7 P.E. Res. ChartQA DocVQA"
        },
        {
            "title": "ChartQA DocVQA MMMU",
            "content": "Learned P.E. 384px Learned P.E. 768px 768px RoPE-2D C-ROPE 768px 22.8 28.8 8.24 32.2 24.3 29.9 11. 33.2 Table 6. Ablation on positional embeddings. We utilize Qwen20.5B [76] and SigLIP-400M [80] to be directly fine-tuned on LLaVA-NeXT-SFT [43] data for one epoch. Additionally, as shown in line #9, replacing SigLIP with DINOv2-L results in slight performance drop. However, when alignment loss is incorporated, the model achieves performance comparable to that of vision-language pretraining models. This suggests that feature alignment objectives are particularly crucial for vision-only models that do not undergo explicit vision-language pretraining. Effectiveness of C-ROPE. We utilize Qwen2-0.5B [76] and SigLIP-400M [80] to directly fine-tune on LLaVANeXT-SFT data [43] for one epoch in three different settings: (1) Only learned position embedding with interpolation at pre-trained resolution 384px and higher resolution 768px, (2) only RoPE-2D and directly removing original position embedding [71] at 768px, and (3) C-ROPE at 768px. As shown in Tab. 6, although the learned position embedding can obtain certain capability to process high-resolution inputs through interpolation, C-ROPE can further unleash the performance by large margin. Moreover, the performance degradation observed when directly using RoPE-2D, suggests that it is neither data-efficient nor training-friendly approach, which is due to the removal of traditional position embeddings and the introduction of abrupt changes compared to the pre-trained model. Effectiveness of Alignment Loss. To further evaluate the effectiveness of Alignment Loss, we utilize Qwen2.50.5B [76] and DINOv2-Large [50], and replace the training data of Stage-III with LLaVA-NeXT-SFT data [43] for rapid evaluation in three different settings: (1) without Alignment Loss, (2) with Alignment Loss when the inputs are fixed resolution in Stage-I, II and (3) with Alignment Loss during entire Stage-I, II, including native resolution training. As shown in Tab. 7, Alignment Loss is particularly beneficial for text-rich tasks, and generally, the longer it is applied, the better the performance. Therefore, we use Alignment Loss in both entire Stage and Stage II by default. Comparisons with Different VFMs. As shown in Tab. 8, we explore different VFMs with SigLIP-Base, SigLIP400M and DINOv2-Large. The results demonstrate that our method is applicable not only to different pre-training objectives (SigLIP vs DINOv2) but also to various model sizes (Base vs Large). No Alignment loss Only fixed res. Both fixed & native res. 55.6 59.6 60.2 60.9 68.4 67.5 31.9 31.8 32.9 Table 7. Ablation on the effectiveness of Alignment loss. We utilize Qwen2-0.5B [76] and DINOv2-Large [50], and employ LLaVA-NeXT-SFT [43] as training data of Stage-III."
        },
        {
            "title": "ChartQA DocVQA MMMU",
            "content": "SigLIP-Base [80] SigLIP-So400M [80] DINOv2-Large [50] 61.6 66.7 64.0 71.0 75.9 70.1 33.6 33.0 32.2 Table 8. Ablation on vision encoders. We ablation the vision encoder of different model sizes and pretraining tasks. Figure 4. Varying the image resolution during inference. We investigate the impact of image resolution on DocVQA [47] and ChartQA [46] by our COMP-MM-1B. Varying the Resolution during Inference. To investigate the impact of image resolution on resolution-sensitive tasks, we observe the changes in scores on DocVQA [47] and ChartQA [46] by limiting the max number of visual tokens to LLM. As shown in Fig. 4, as the input resolution increases, the performance of both tasks gradually improves, which demonstrates the effectiveness of our approach for varying resolutions, particularly the high resolution. 5. Conclusion We introduced COMP, continual multimodal pre-training pipeline to tackle fixed resolution and modality gap problems under LMM framework, for both vision-language pretraining and vision-only pre-training models. Specifically, we proposed C-ROPE and Alignment Loss, which efficiently adapt to native resolution inputs with light continual training data and are better suited for the LLM text space. Through extensive experiments, we demonstrated that our approach achieved state-of-the-art performance on multimodal understanding benchmarks, and the performance of the VFM in other downstream visual tasks is preserved."
        },
        {
            "title": "References",
            "content": "[1] Pravesh Agrawal, Szymon Antoniak, Emma Bou Hanna, Baptiste Bout, Devendra Chaplot, Jessica Chudnovsky, Diogo Costa, Baudouin De Monicault, Saurabh Garg, Theophile Gervet, Soham Ghosh, Amelie Heliou, Paul Jacob, Albert Q. Jiang, Kartik Khandelwal, Timothee Lacroix, Guillaume Lample, Diego Las Casas, Thibaut Lavril, Teven Le Scao, Andy Lo, William Marshall, Louis Martin, Arthur Mensch, Pavankumar Muddireddy, Valera Nemychnikova, Marie Pellat, Patrick Von Platen, Nikhil Raghuraman, Baptiste Rozi`ere, Alexandre Sablayrolles, Lucile Saulnier, Romain Sauvestre, Wendy Shang, Roman Soletskyi, Lawrence Stewart, Pierre Stock, Joachim Studnia, Sandeep Subramanian, Sagar Vaze, Thomas Wang, and Sophia Yang. Pixtral 12b. arXiv preprint arXiv:2410.07073, 2024. 3 [2] Jean-Baptiste Alayrac, Jeff Donahue, Pauline Luc, Antoine Miech, Iain Barr, Yana Hasson, Karel Lenc, Arthur Mensch, Katherine Millican, Malcolm Reynolds, et al. Flamingo: visual language model for few-shot learning. NeurIPS, 2022. 3 [3] Jinze Bai, Shuai Bai, Yunfei Chu, Zeyu Cui, Kai Dang, et al. Qwen Technical Report. arXiv preprint arXiv:2309.12345, 2023. 2 [4] Jinze Bai, Shuai Bai, Shusheng Yang, Shijie Wang, Sinan Tan, Peng Wang, Junyang Lin, Chang Zhou, and Jingren Zhou. Qwen-vl: frontier large vision-language model with versatile abilities. arXiv preprint arXiv:2308.12966, 2023. 3 [5] Shuai Bai, Keqin Chen, Xuejing Liu, Jialin Wang, Wenbin Ge, Sibo Song, Kai Dang, Peng Wang, Shijie Wang, Jun Tang, Humen Zhong, Yuanzhi Zhu, Mingkun Yang, Zhaohai Li, Jianqiang Wan, Pengfei Wang, Wei Ding, Zheren Fu, Yiheng Xu, Jiabo Ye, Xi Zhang, Tianbao Xie, Zesen Cheng, Hang Zhang, Zhibo Yang, Haiyang Xu, and JunarXiv preprint yang Lin. Qwen2.5-vl technical report. arXiv:2502.13923, 2025. 3 [6] Hangbo Bao, Li Dong, Songhao Piao, and Furu Wei. Beit: Bert pre-training of image transformers. In ICLR, 2021. 2 [7] Lucas Beyer, Pavel Izmailov, Alexander Kolesnikov, Mathilde Caron, Simon Kornblith, Xiaohua Zhai, Matthias Minderer, Michael Tschannen, Ibrahim Alabdulmohsin, and Filip Pavetic. Flexivit: One model for all patch sizes. In CVPR, 2023. 2, 3 [8] Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. Language models are few-shot learners. NeurIPS, 2020. 2, 3 [9] Mathilde Caron, Hugo Touvron, Ishan Misra, Herve Jegou, Julien Mairal, Piotr Bojanowski, and Armand Joulin. EmergIn ing properties in self-supervised vision transformers. CVPR, 2021. 1, 2, 4 [10] Ting Chen, Simon Kornblith, Mohammad Norouzi, and Geoffrey Hinton. simple framework for contrastive learning of visual representations. In ICML, 2020. 1, 2 [11] Zhe Chen, Weiyun Wang, Yue Cao, Yangzhou Liu, Zhangwei Gao, Erfei Cui, Jinguo Zhu, Shenglong Ye, Hao Tian, Zhaoyang Liu, Lixin Gu, Xuehui Wang, Qingyun Li, Yimin Ren, Zixuan Chen, Jiapeng Luo, Jiahao Wang, Tan Jiang, Bo Wang, Conghui He, Botian Shi, Xingcheng Zhang, Han Lv, Yi Wang, Wenqi Shao, Pei Chu, Zhongying Tu, Tong He, Zhiyong Wu, Huipeng Deng, Jiaye Ge, Kai Chen, Kaipeng Zhang, Limin Wang, Min Dou, Lewei Lu, Xizhou Zhu, Tong Lu, Dahua Lin, Yu Qiao, Jifeng Dai, and Wenhai Wang. Expanding performance boundaries of open-source multimodal models with model, data, and test-time scaling. arXiv preprint arXiv:2412.05271, 2025. 3, 6 [12] Marco Cuturi. Sinkhorn distances: Lightspeed computation of optimal transport. In NeurIPS, 2013. 4, 5 [13] Wenliang Dai, Junnan Li, Dongxu Li, Anthony Meng Huat Tiong, Junqi Zhao, Weisheng Wang, Boyang Li, Pascale Fung, and Steven Hoi. Instructblip: Towards generalpurpose vision-language models with instruction tuning. NeurIPS, 2024. 3 [14] DeepSeek-AI, Aixin Liu, Bei Feng, Bing Xue, Bingxuan Wang, Bochao Wu, Chengda Lu, Chenggang Zhao, Chengqi Deng, Chenyu Zhang, Chong Ruan, Damai Dai, Daya Guo, Dejian Yang, Deli Chen, Dongjie Ji, Erhang Li, Fangyun Lin, Fucong Dai, Fuli Luo, Guangbo Hao, Guanting Chen, Guowei Li, H. Zhang, Han Bao, Hanwei Xu, Haocheng Wang, Haowei Zhang, Honghui Ding, Huajian Xin, Huazuo Gao, Hui Li, Hui Qu, J. Zhang, et al. Deepseek-v3 technical report. arXiv preprint arXiv:2412.19437, 2024. 2 [15] Mostafa Dehghani, Basil Mustafa, Josip Djolonga, Jonathan Heek, Matthias Minderer, Mathilde Caron, Andreas Steiner, Ibrahim AlabdulJoan Puigcerver, Robert Geirhos, mohsin, et al. Patch npack: Navit, vision transformer for any aspect ratio and resolution. In NeurIPS, 2023. 3 [16] Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li Fei-Fei. Imagenet: large-scale hierarchical image database. In CVPR, 2009. 6, 13 [17] Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Pre-training of deep bidirectional arXiv preprint Toutanova. transformers for language understanding. arXiv:1810.04805, 2018. Bert: [18] Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, Jakob Uszkoreit, and Neil Houlsby. An image is worth 16x16 words: Transformers for image recognition at scale. In ICLR, 2021. 2 [19] Avijit Dubey, Aaron Grattafiori, et al. The llama 3 herd of models. arXiv preprint arXiv:2407.21783, 2024. 2, 6, 13 [20] Enrico Fini, Mustafa Shukor, Xiujun Li, Philipp Dufter, Michal Klein, David Haldimann, Sai Aitharaju, Victor Guilherme Turrisi da Costa, Louis Bethune, Zhe Gan, Alexander Toshev, Marcin Eichner, Moin Nabi, Yinfei Yang, Joshua M. Susskind, and Alaaeldin El-Nouby. Multimodal autoregressive pre-training of large vision encoders. arXiv preprint arXiv:2411.14402, 2024. 6 [21] Chaoyou Fu, Peixian Chen, Yunhang Shen, Yulei Qin, Mengdan Zhang, Xu Lin, Jinrui Yang, Xiawu Zheng, Ke Li, Xing Sun, et al. Mme: comprehensive evaluation benchmark for multimodal large language models. arXiv preprint arXiv:2306.13394, 2023. 6 9 [22] Peng Gao, Renrui Zhang, Chris Liu, Longtian Qiu, Siyuan Huang, Weifeng Lin, Shitian Zhao, Shijie Geng, Ziyi Lin, Peng Jin, et al. Sphinx-x: Scaling data and parameters for family of multi-modal large language models. arXiv preprint arXiv:2402.05935, 2024. [23] Yash Goyal, Tejas Khot, Douglas Summers-Stay, Dhruv Batra, and Devi Parikh. Making the in vqa matter: Elevating the role of image understanding in visual question answering. In CVPR, 2017. 6 [24] Kaiming He, Haoqi Fan, Yuxin Wu, Saining Xie, and Ross Girshick. Momentum contrast for unsupervised visual representation learning. In CVPR, 2020. 1, 2 [25] Kaiming He, Xinlei Chen, Saining Xie, Yanghao Li, Piotr Dollar, and Ross Girshick. Masked autoencoders are scalable vision learners. In CVPR, 2022. 2, 6 [26] Drew Hudson and Christopher Manning. Gqa: new dataset for real-world visual reasoning and compositional question answering. In CVPR, 2019. 6 [27] Chao Jia, Yinfei Yang, Ye Xia, Yi-Ting Chen, Zarana Parekh, Hieu Pham, Quoc Le, Yun-Hsuan Sung, Zhen Li, and Tom Duerig. Scaling Up Visual and Vision-Language Representation Learning With Noisy Text Supervision. In ICML, 2021. 3 [28] Aniruddha Kembhavi, Mike Salvato, Eric Kolve, Minjoon Seo, Hannaneh Hajishirzi, and Ali Farhadi. diagram is worth dozen images. In ECCV, 2016. [29] Zhenzhong Lan, Mingda Chen, Sebastian Goodman, Kevin Gimpel, Piyush Sharma, and Radu Soricut. Albert: lite bert for self-supervised learning of language representations. arXiv preprint arXiv:1909.11942, 2019. 2 [30] Mike Lewis, Yinhan Liu, Naman Goyal, Marjan Ghazvininejad, Abdelrahman Mohamed, Omer Levy, Ves Stoyanov, and Luke Zettlemoyer. Bart: Denoising sequence-to-sequence pre-training for natural language generation, translation, and comprehension. arXiv preprint arXiv:1910.13461, 2019. 2 [31] Bohao Li, Rui Wang, Guangzhi Wang, Yuying Ge, Yixiao Ge, and Ying Shan. Seed-bench: Benchmarking multimodal llms with generative comprehension. arXiv preprint arXiv:2307.16125, 2023. 6 [32] Bo Li, Yuanhan Zhang, Dong Guo, Renrui Zhang, Feng Li, Hao Zhang, Kaichen Zhang, Peiyuan Zhang, Yanwei Li, Ziwei Liu, and Chunyuan Li. Llava-onevision: Easy visual task transfer. TMLR, 2024. 2, 5, 6, 7, 13 [33] Junnan Li, Dongxu Li, Caiming Xiong, and Steven Hoi. Blip: Bootstrapping language-image pre-training for unified In ICML, vision-language understanding and generation. 2022. 3 [34] Junnan Li, Dongxu Li, Silvio Savarese, and Steven Hoi. Blip-2: Bootstrapping language-image pre-training with frozen image encoders and large language models. In ICML, 2023. [35] Junnan Li, Dongxu Li, Silvio Savarese, and Steven C. H. Hoi. Blip-2: Bootstrapping language-image pre-training with frozen image encoders and large language models. In ICML, 2023. 3 [36] Xiaotong Li, Fan Zhang, Haiwen Diao, Yueze Wang, Xinlong Wang, and Ling-Yu Duan. Densefusion-1m: Merging vision experts for comprehensive multimodal perception. In NeurIPS, 2024. 5 [37] Victor Weixin Liang, Yuhui Zhang, Yongchan Kwon, Serena Yeung, and James Zou. Mind the gap: Understanding the modality gap in multi-modal contrastive representation learning. In NeurIPS, 2022. 2 [38] Ji Lin, Hongxu Yin, Wei Ping, Yao Lu, Pavlo Molchanov, Andrew Tao, Huizi Mao, Jan Kautz, Mohammad Shoeybi, and Song Han. Vila: On pre-training for visual language models. arXiv preprint arXiv:2312.07533, 2023. 3 [39] Ziyi Lin, Chris Liu, Renrui Zhang, Peng Gao, Longtian Qiu, Han Xiao, Han Qiu, Chen Lin, Wenqi Shao, Keqin Chen, et al. Sphinx: The joint mixing of weights, tasks, and visual embeddings for multi-modal large language models. arXiv preprint arXiv:2311.07575, 2023. [40] Haotian Liu, Chunyuan Li, Yuheng Li, and Yong Jae Lee. Improved baselines with visual instruction tuning. ArXiv, abs/2310.03744, 2023. 3, 5 [41] Haotian Liu, Chunyuan Li, Qingyang Wu, and Yong Jae Lee. Visual instruction tuning. In NeurIPS, 2023. 2, 3, 5, 6 [42] Haotian Liu, Chunyuan Li, Yuheng Li, Bo Li, Yuanhan Zhang, Sheng Shen, and Yong Jae Lee. Llava-next: Improved reasoning, ocr, and world knowledge, 2024. 13 [43] Haotian Liu, Chunyuan Li, Yuheng Li, Bo Li, Yuanhan Zhang, Sheng Shen, and Yong Jae Lee. Llava-next: Improved reasoning, ocr, and world knowledge, 2024. 2, 3, 5, 7, 8 [44] Yuan Liu, Haodong Duan, Yuanhan Zhang, Bo Li, Songyang Zhang, Wangbo Zhao, Yike Yuan, Jiaqi Wang, Conghui He, Ziwei Liu, et al. Mmbench: Is your multi-modal model an all-around player? In ECCV, 2024. 6 [45] Haoyu Lu, Wen Liu, Bo Zhang, Bingxuan Wang, Kai Dong, Bo Liu, Jingxiang Sun, Tongzheng Ren, Zhuoshu Li, Hao Yang, Yaofeng Sun, Chengqi Deng, Hanwei Xu, Zhenda Xie, and Chong Ruan. Deepseek-vl: Towards real-world visionlanguage understanding. arXiv preprint arXiv:2403.05525, 2024. 5 [46] Ahmed Masry, Xuan Long Do, Jia Qing Tan, Shafiq Joty, and Enamul Hoque. Chartqa: benchmark for question answering about charts with visual and logical reasoning. In ACL Findings, 2022. 2, 6, 8 [47] Minesh Mathew, Dimosthenis Karatzas, and CV Jawahar. Docvqa: dataset for vqa on document images. In WACV, 2021. 2, 6, [48] Minesh Mathew, Viraj Bagal, Rub`en Tito, Dimosthenis Karatzas, Ernest Valveny, and CV Jawahar. Infographicvqa. In WACV, 2022. 6 [49] Lingchen Meng, Jianwei Yang, Rui Tian, Xiyang Dai, Zuxuan Wu, Jianfeng Gao, and Yu-Gang Jiang. Deepstack: Deeply stacking visual tokens is surprisingly simple and effective for lmms. In NeurIPS, 2024. 3 [50] Maxime Oquab, Timothee Darcet, Theo Moutakanni, Huy Vo, Marc Szafraniec, Vasil Khalidov, Pierre Fernandez, Daniel Haziza, Francisco Massa, Alaaeldin El-Nouby, et al. Dinov2: Learning robust visual features without supervision. TMLR, 2024. 1, 2, 4, 5, 6, 8, 13 10 [51] Long Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida, Carroll Wainwright, Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, et al. Training language models to follow instructions with human feedback. NeurIPS, 2022. 2 [52] Jungin Park, Jiyoung Lee, and Kwanghoon Sohn. Bridging vision and language spaces with assignment prediction. In ICLR, 2024. 2, 4, [53] Wujian Peng, Lingchen Meng, Yitong Chen, Yiweng Xie, Yang Liu, Tao Gui, Hang Xu, Xipeng Qiu, Zuxuan Wu, and Yu-Gang Jiang. Inst-it: Boosting multimodal instance understanding via explicit visual prompt instruction tuning. arXiv preprint arXiv:2412.03565, 2024. 2, 6 [54] Wujian Peng, Sicheng Xie, Zuyao You, Shiyi Lan, and Zuxuan Wu. Synthesize diagnose and optimize: Towards finegrained vision-language understanding. In CVPR, 2024. 3 [55] Alec Radford, Karthik Narasimhan, Tim Salimans, Ilya Sutskever, et al. Improving language understanding by generative pre-training. 2018. 2, 3 [56] Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, Ilya Sutskever, et al. Language models are unsupervised multitask learners. OpenAI blog, 2019. 2, 3 [57] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, et al. Learning transferable visual models from natural language supervision. In ICML, 2021. 1, 2, 3, 6 [58] Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, and Peter Liu. Exploring the limits of transfer learning with unified text-to-text transformer. Journal of machine learning research, 2020. 2 [59] Christoph Schuhmann, Romain Beaumont, Richard Vencu, Cade Gordon, Ross Wightman, Mehdi Cherti, Theo Coombes, Aarush Katta, Clayton Mullis, Mitchell Wortsman, Patrick Schramowski, Srivatsa Kundurthy, Katherine Crowson, Ludwig Schmidt, Robert Kaczmarczyk, and Jenia Jitsev. LAION-5b: An open large-scale dataset for training next generation image-text models. In NeurIPS, 2022. 3 [60] Dustin Schwenk, Apoorv Khandelwal, Christopher Clark, Kenneth Marino, and Roozbeh Mottaghi. A-okvqa: benchmark for visual question answering using world knowledge. In ECCV, 2022. [61] Piyush Sharma, Nan Ding, Sebastian Goodman, and Radu Soricut. Conceptual captions: cleaned, hypernymed, image alt-text dataset for automatic image captioning. In ACL, 2018. 5 [62] Amanpreet Singh, Vivek Natarajan, Meet Shah, Yu Jiang, Xinlei Chen, Dhruv Batra, Devi Parikh, and Marcus In CVPR, Rohrbach. Towards vqa models that can read. 2019. 6 [63] Jianlin Su. Transformer upgrade path: 17. insights into multimodal positional encoding, 2024. 2 [64] Rui Tian, Zuxuan Wu, Qi Dai, Han Hu, Yu Qiao, and YuGang Jiang. Resformer: Scaling vits with multi-resolution training. In CVPR, 2023. 2, 3 [65] Shengbang Tong, Ellis Brown, Penghao Wu, Sanghyun Woo, Manoj Middepogu, Sai Charitha Akula, Jihan Yang, Shusheng Yang, Adithya Iyer, Xichen Pan, Ziteng Wang, Rob Fergus, Yann LeCun, and Saining Xie. Cambrian-1: fully open, vision-centric exploration of multimodal llms. arXiv preprint arXiv:2406.16860, 2024. 2, 5 [66] Zhan Tong, Yibing Song, Jue Wang, and Limin Wang. Videomae: Masked autoencoders are data-efficient learners for self-supervised video pre-training. NeurIPS, 2022. 2 [67] Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timothee Lacroix, Baptiste Rozi`ere, Naman Goyal, Eric Hambro, Faisal Azhar, et al. Llama: Open and efficient foundation language models. arXiv preprint arXiv:2302.13971, 2023. 2, 3 [68] Michael Tschannen, Alexey Gritsenko, Xiao Wang, Muhammad Ferjad Naeem, Ibrahim Alabdulmohsin, Nikhil Parthasarathy, Talfan Evans, Lucas Beyer, Ye Xia, Basil Mustafa, Olivier Henaff, Jeremiah Harmsen, Andreas Steiner, and Xiaohua Zhai. Siglip 2: Multilingual visionlanguage encoders with improved semantic understandarXiv preprint ing, arXiv:2502.14786, 2025. 1, 2, 6, 7 localization, and dense features. [69] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan Gomez, Łukasz Kaiser, and Illia Polosukhin. Attention is all you need. In NeurIPS, 2017. 2 [70] Junke Wang, Lingchen Meng, Zejia Weng, Bo He, Zuxuan Wu, and Yu-Gang Jiang. To see is to believe: Prompting gpt-4v for better visual instruction tuning. arXiv preprint arXiv:2311.07574, 2023. 3 [71] Peng Wang, Shuai Bai, Sinan Tan, Shijie Wang, Zhihao Fan, Jinze Bai, Keqin Chen, Xuejing Liu, Jialin Wang, Wenbin Ge, Yang Fan, Kai Dang, Mengfei Du, Xuancheng Ren, Rui Men, Dayiheng Liu, Chang Zhou, Jingren Zhou, and Junyang Lin. Qwen2-vl: Enhancing vision-language models perception of the world at any resolution. arXiv preprint arXiv:2409.12191, 2024. 3, 7, [72] Size Wu, Wenwei Zhang, Lumin Xu, Sheng Jin, Xiangtai Li, Wentao Liu, and Chen Change Loy. Clipself: Vision transformer distills itself for open-vocabulary dense prediction. In ICLR, 2024. 3 [73] xAI Team. Grok-1.5 vision preview, 2024. 6 [74] Tete Xiao, Yingcheng Liu, Bolei Zhou, Yuning Jiang, and Jian Sun. Unified perceptual parsing for scene understanding. In ECCV, 2018. 6, 7 [75] Wenhan Xiong, Jingyu Liu, Igor Molybog, Hejia Zhang, Prajjwal Bhargava, Rui Hou, Louis Martin, Rashi Rungta, Karthik Abinav Sankararaman, Barlas Oguz, et al. Effective long-context scaling of foundation models. In NAACL, 2024. 3 [76] An Yang, Baosong Yang, Binyuan Hui, Bo Zheng, Bowen Yu, Chang Zhou, Chengpeng Li, Chengyuan Li, Dayiheng Liu, Fei Huang, Guanting Dong, Haoran Wei, Huan Lin, Jialong Tang, Jialin Wang, Jian Yang, Jianhong Tu, Jianwei Zhang, Jianxin Ma, Jianxin Yang, Jin Xu, Jingren Zhou, Jinze Bai, Jinzheng He, Junyang Lin, Kai Dang, Keming Lu, Keqin Chen, Kexin Yang, Mei Li, Mingfeng Xue, Na Ni, Pei Zhang, Peng Wang, Ru Peng, Rui Men, Ruize Gao, Runji Lin, Shijie Wang, Shuai Bai, Sinan Tan, Tianhang Zhu, Tianhao Li, Tianyu Liu, Wenbin Ge, Xiaodong Deng, Xiaohuan 11 Zhou, Xingzhang Ren, Xinyu Zhang, Xipin Wei, Xuancheng Ren, Xuejing Liu, Yang Fan, Yang Yao, Yichang Zhang, Yu Wan, Yunfei Chu, Yuqiong Liu, Zeyu Cui, Zhenru Zhang, Zhifang Guo, and Zhihao Fan. Qwen2 technical report. arXiv preprint arXiv:2407.10671, 2024. 2, [77] An Yang, Baosong Yang, Beichen Zhang, Binyuan Hui, Bo Zheng, Bowen Yu, Chengyuan Li, Dayiheng Liu, Fei Huang, Haoran Wei, Huan Lin, Jian Yang, Jianhong Tu, Jianwei Zhang, Jianxin Yang, Jiaxi Yang, Jingren Zhou, Junyang Lin, Kai Dang, Keming Lu, Keqin Bao, Kexin Yang, Le Yu, Mei Li, Mingfeng Xue, Pei Zhang, Qin Zhu, Rui Men, Runji Lin, Tianhao Li, Tianyi Tang, Tingyu Xia, Xingzhang Ren, Xuancheng Ren, Yang Fan, Yang Su, Yichang Zhang, Yu Wan, Yuqiong Liu, Zeyu Cui, Zhenru Zhang, and Zihan Qiu. Qwen2.5 technical report. arXiv preprint arXiv:2412.15115, 2025. 2, 5, 7 [78] Jiahui Yu, Zirui Wang, Vijay Vasudevan, Legg Yeung, Mojtaba Seyedhosseini, and Yonghui Wu. Coca: Contrastive In Trans. captioners are image-text foundation models. Mach. Learn. Res., 2022. 3 [79] Xiang Yue, Yuansheng Ni, Kai Zhang, Tianyu Zheng, Ruoqi Liu, Ge Zhang, Samuel Stevens, Dongfu Jiang, Weiming Ren, Yuxuan Sun, Cong Wei, Botao Yu, Ruibin Yuan, Renliang Sun, Ming Yin, Boyuan Zheng, Zhenzhu Yang, Yibo Liu, Wenhao Huang, Huan Sun, Yu Su, and Wenhu Chen. Mmmu: massive multi-discipline multimodal understanding and reasoning benchmark for expert agi. In CVPR, 2024. 6 [80] Xiaohua Zhai, Basil Mustafa, Alexander Kolesnikov, and Lucas Beyer. Sigmoid loss for language image pre-training. In ICCV, 2023. 1, 2, 5, 6, 7, 8, 13 [81] Beichen Zhang, Pan Zhang, Xiaoyi Dong, Yuhang Zang, and Jiaqi Wang. Long-CLIP: Unlocking the Long-Text Capability of CLIP. arXiv preprint arXiv:2403.15378, 2024. 3 [82] Bolei Zhou, Hang Zhao, Xavier Puig, Sanja Fidler, Adela Scene parsing through Barriuso, and Antonio Torralba. ade20k dataset. In CVPR, 2017. 6, 7, [83] Zhuofan Zong, Bingqi Ma, Dazhong Shen, Guanglu Song, Hao Shao, Dongzhi Jiang, Hongsheng Li, and Yu Liu. Mova: Adapting mixture of vision experts to multimodal context. arXiv preprint arXiv:2404.13046, 2024. 2 12 A. Hyperparameters COMP-MM. We outline the optimization hyperparameters used during COMP-MM continual pre-training in Tab. 9, and the hyperparameters not listed remain consistent with LLaVA-OneVision [32]. Stage-I Stage-II Stage-III"
        },
        {
            "title": "Native",
            "content": "Fixed Trainable Adapter Full Model Full Model Full Model Batch Size 8 103 LRAdapter 1 104 LRV 105 LRLLM 1 Epoch 103 5 - 1 - 2 1 8 103 104 105 1 8 105 105 105 1 1 2 1 5 1 2 32 32 Table 9. Detailed configuration for each training stage of our COMP-MM-1B and COMP-MM-7B models. Besides, uW (k) in Eq. (9) is Nk Ni , where Nk is the number of the k-th word in Stage-II dataset, α in Eq. (11) is set to 0.05, and all temperature coefficients are 0.005. (cid:80) Multimodal Understanding. The hyperaparmeters used for the instruction tuning are detailed in Tab. 10. We tune COMP-SigLIP and COMP-DINOv2 with LLama 3.0 8B [19] on LLaVA SFT data [42] for one epoch. In addition, we used 2 2 downsampling adapter to unleash the high-resolution perception capability of COMP-SigLIP and COMP-DINOv2, while keeping the number of tokenens to LLM at 576 for fair comparison. Training Config Optimizer Decoder peak learning rate Decoder peak learning rate Adapter peak learning rate Minimum learning rate Learning rate schedule Batch size Iterations Warmup ratio Transformations AdamW 105 1 105 1 105 8 0 cosine decay 128 5197 0.05 PadToSquare, Resize Table 10. Detailed configuration of COMP-SigLIP and COMPDINOv2 in instruction tuning for multimodal understanding. Image Recognition. The hyperaparmeters used for frozen trunk classification on ImageNet-1k [16] are detailed in Tab. 11. The mean and std in Normalization of COMPSigLIP and COMP-DINOv2 are [(0.5, 0.5, 0.5), (0.5, 0.5, 0.5)] and [(0.485, 0.456, 0.406), (0.229, 0.224, 0.225)] respectively, following the original SigLIP [80] and DINOv2 [50]. We use the same hyperaparmeters for all models and baselines. Training Config Optimizer Peak learning rate Minimum learning rate Learning rate schedule Batch size Weight decay Epochs Warmup epochs Augmentations:"
        },
        {
            "title": "RandomResizedCrop",
            "content": "size scale ratio interpolation RandomHorizontalFlip ToTensor Normalize 224px 448px"
        },
        {
            "title": "AdamW",
            "content": "1 2 104 105 1 5 105 106 cosine decay 1024 256 0.05 10 1 224px 448px (0.08, 1.0) (0.75, 1.33) Bicubic = 0.5 follows SigLIP or DINOv2 Table 11. Detailed configuration of COMP-SigLIP and COMPDINOv2 for frozen trunk classification. Semantic Segmentation. The hyperaparmeters used for semantic segmentation on ADE20K [82] are detailed in Tab. 12. The mean and std in Normalization of COMP-SigLIP and COMP-DINOv2 follow the original SigLIP [80] and DINOv2 [50]. Training Config Optimizer Weight decay Peak learning rate Minimum learning rate Learning rate schedule Batch size Iterations Warmup iters Augmentations: RandomResizedCrop RandomFlip PhotoMetricDistortion Normalize 504px 672px AdamW 0.05 4 105 0 poly decay 16 80K 504px 672px = 0.5 follows SigLIP or DINOv2 Table 12. Detailed configuration of COMP-SigLIP and COMPDINOv2 for semantic segmentation."
        }
    ],
    "affiliations": [
        "Shanghai Innovation Institute",
        "Shanghai Key Lab of Intell. Info. Processing, School of CS, Fudan University"
    ]
}