{
    "paper_title": "LLaVAction: evaluating and training multi-modal large language models for action recognition",
    "authors": [
        "Shaokai Ye",
        "Haozhe Qi",
        "Alexander Mathis",
        "Mackenzie W. Mathis"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Understanding human behavior requires measuring behavioral actions. Due to its complexity, behavior is best mapped onto a rich, semantic structure such as language. The recent development of multi-modal large language models (MLLMs) is a promising candidate for a wide range of action understanding tasks. In this work, we focus on evaluating and then improving MLLMs to perform action recognition. We reformulate EPIC-KITCHENS-100, one of the largest and most challenging egocentric action datasets, to the form of video multiple question answering (EPIC-KITCHENS-100-MQA). We show that when we sample difficult incorrect answers as distractors, leading MLLMs struggle to recognize the correct actions. We propose a series of methods that greatly improve the MLLMs' ability to perform action recognition, achieving state-of-the-art on both the EPIC-KITCHENS-100 validation set, as well as outperforming GPT-4o by 21 points in accuracy on EPIC-KITCHENS-100-MQA. Lastly, we show improvements on other action-related video benchmarks such as EgoSchema, PerceptionTest, LongVideoBench, VideoMME and MVBench, suggesting that MLLMs are a promising path forward for complex action tasks. Code and models are available at: https://github.com/AdaptiveMotorControlLab/LLaVAction."
        },
        {
            "title": "Start",
            "content": "LLaVAction: evaluating and training multi-modal large language models for action recognition Shaokai Ye, Haozhe Qi, Alexander Mathis, Mackenzie Weygandt Mathis EPFL mackenzie.mathis@epfl.ch Co-first, Co-senior 5 2 0 2 4 ] . [ 1 2 1 7 8 1 . 3 0 5 2 : r a"
        },
        {
            "title": "Abstract",
            "content": "Understanding human behavior requires measuring behavioral actions. Due to its complexity, behavior is best mapped onto rich, semantic structure such as language. The recent development of multi-modal large language models (MLLMs) is promising candidate for wide range of action understanding tasks. In this work, we focus on evaluating and then improving MLLMs to perform action recognition. We reformulate EPIC-KITCHENS-100, one of the largest and most challenging egocentric action datasets, to the form of video multiple question answering (EPIC-KITCHENS-100-MQA). We show that when we sample difficult incorrect answers as distractors, leading MLLMs struggle to recognize the correct actions. We propose series of methods that greatly improve the MLLMs ability to perform action recognition, achieving state-ofthe-art on both the EPIC-KITCHENS-100 validation set, as well as outperforming GPT-4o by 21 points in accuracy on EPIC-KITCHENS-100-MQA. Lastly, we show improvements on other action-related video benchmarks such as EgoSchema, PerceptionTest, LongVideoBench, VideoMME and MVBench, suggesting that MLLMs are promising path forward for complex action tasks. Code and models are available at: https : / / github . com / AdaptiveMotorControlLab/LLaVAction Figure 1. LLaVAction-7B. Top: Qualitative inspection of distractors. We show an example clip with labels from random choices (which empirically is easy to solve), vs. our proposed harder benchmark with action labels generated by TIM [7]. Our hard example generation strategy can automatically explore challenges such as temporal order and similar objects that are emphasized in other benchmarks. Bottom: Note, while GPT-4o is very good at random choices, it suffers in the harder benchmarking regime, and our method, LLaVAction outperforms the GPT-4o models. 1. Introduction Understanding human behavior is complex challenge requiring multiple skills such as visual perception, knowledge about the world and reasoning capabilities. Thus, MultiModal Large Language Models (MLLMs) [1315, 25, 29, 58, 81] are promising candidates. Recently, MLLMs have made strides in object recognition, action understanding, and video captioning, fueled by innovations such as visual instruction tuning. Benchmarks like MVBench [27] and NextQA [66] showcase their potential in human activity recognition, but key question remains: Are we too optimistic about the ability of MLLMs to quantify human actions, particularly during dynamic behaviors seen in realworld tasks? Popular action recognition datasets such as EPICKITCHENS-100 [12] rely on fixed vocabularies, which limits generalization. In contrast, video-language datasets offer greater flexibility but introduce challenges with crowdsourced annotations such as unclear action distribution coverage, and the uncertainty of distractor difficulty [72], as also discussed in Xiao et al. [66]. To address these issues, we reformulate EPIC-KITCHENS-100 into video multiple-choice question & answer (MQA) task with openvocabulary narrations as ground truth, which we call EPIC1 KITCHENS-100-MQA. Candidate choices are filtered by state-of-the-art (SOTA) model specialized for action recognition [7, 83] instead of humans or closed-sourced LLMs and MLLMs, in the belief that SOTA models can efficiently filter trivial answers and find hard distractors (Figure 1). Experiments show that while MLLMs can easily select correct narrations, their performance drops sharply when faced with hard examples. We argue that the reformulated EPIC-KITCHENS-100 dataset offers new and more challenging framework for evaluating MLLMs action recognition abilities. In addition to this, we introduce several novel methods designed to enhance the performance of MLLMs in action recognition, including vision token supervision, temporal detection, and associating prior actions with the current action (Figure 2). Our contributions are as follows: Identifying that MLLMs struggle with action recognition when tested with visually or semantically similar actions, which are common in real-world scenarios. We propose EPIC-KITCHENS-100-MQA as more effective evaluation for these challenges. Novel methods such as vision token supervision, temporal detection, and prior action memory, which enhance MLLMs action recognition performance. Learning from hard negative examples generated by state-of-the-art models provides scalable supervision for MLLMs, improving their ability to distinguish between similar actions. We achieve state-of-the-art performance on both the EPIC-KITCHENS-100 dataset and EPIC-KITCHENS100-MQA, surpassing models like GPT-4o. Our methods also improve MLLMs across multiple video language benchmarks that involve human activity understanding, demonstrating their wider applicability. 2. Related Works Action recognition. Given trimmed action segment as input, the action recognition task requires the model to predict the corresponding action class for this segment [12, 19, 43, 44, 50]. Analogously to image classification in vision, action recognition is fundamental task in video understanding [4, 6, 16, 54, 56]. For egocentric vision, action recognition also serves as an important task in many egocentric datasets (e.g., EPIC-KITCHENS-100 [12], Ego4D [20], and HoloAssist [60]) due to its importance for applications such as augmented reality and robotics. Over the years, many methods have been proposed while still suffering from fast camera movement, long-term temporal relations, and open vocabulary ability [12]. We instead resort to the powerful MLLMs enhanced with video instructiontuning [77, 81] data to address those challenges. Multi-modal large language models. Multi-modal large language models are promising generalists [26]. Early multi-modal models [2, 57] mostly performed few tasks or relied on few-shot learning for task generalization. After the large success of Large Language Models (LLMs) [1, 5, 55], multi-modal models appeared that can supplement text with other modalities, such as image, video and audio [11, 14, 15, 18, 22, 41, 69, 86]. Among them, video MLLMs [25, 27, 28, 37, 62, 68, 76, 77, 81] promise robust and scalable solutions to understand and process video data. Our work falls into this direction aiming at improving MLLMs action understanding ability. MLLM benchmarks. Alongside advancements in architecture, significant efforts have been made to improve benchmarking for MLLMs [24, 32, 36, 40, 73], guiding the development of the next generation of these models. Existing video benchmarks focus on different visual understanding tasks, including action understanding, temporal reasoning, and video captioning. Benchmarks that encompass human activity include NextQA [66], VideoMME [17], PerceptionTest [42], and MVBench [27]. Due to the ease of evaluation associated with the multi-choice questionanswer format, many of the datasets are constructed in this format. The choices of these questions are either constructed by humans [17, 66, 72] or by closed-source MLLMs [8, 30, 37, 70]. Instead, we use SOTA action recognition models to efficiently find hard distractors. Vision-centric MLLMs. The visual performance of MLLMs is likely overestimated due to an over-emphasis of the language-centric tasks in both the model training and the evaluation benchmarks [45, 52, 53]. There are ongoing efforts to enhance the vision component of MLLMs, such as feeding cropped objects into MLLMs [46], utilizing resampler techniques through visual search [65], and implementing multi-modal Chain of Thought (COT) approaches [82]. 3. Methods In this section, we first illustrate the EPIC-KITCHENS100 dataset and the way we re-formulated it into EPICfollowed by our KITCHENS-100-MQA (Section 3.1), methods to improve MLLMs action recognition ability (Section 3.2). 3.1. Reformulating EPIC-KITCHENS-100 to video multiple question answering format 3.1.1. EPIC-KITCHENS-100-MQA benchmark EPIC-KITCHENS-100 [12] consists of 90K actions (67,217/9,668/13,092 for training/validation/test splits) in 100 hours for people performing unscripted kitchen activities. In terms of the action recognition task, EPICKITCHENS-100 contains two types of ground truth annotations for the action segments: one is compressed version of verb-noun pairs, generated after part-of-speech and clustering; we denote these as official keys. The other ground 2 Figure 2. LLaVAction-7B pipeline. Our full model includes an action token and additional auxiliary visual tasks, as noted. Inputs are shown for given video clip. The responses are from the direction prediction, GPT-4o distillation, and the adversarial MQA. truth is the narration from the recorders. The narration was not used as the main source of learning in previous methods (e.g., used in pre-training via contrastive learning), as they relied on fixed-vocabulary classifiers [7, 18, 63, 83, 85]. However, narrations are the natural learning signals for MLLMs and we argue they are the preferred signals over the official keys as the latter oversimplify language, introducing grammatical and semantic errors (shown in Appendix Figure 4 and Table 8), which might further mislead MLLMs. Therefore, we used the narrations to adapt EPICKITCHENS-100s validation set into video MQA benchmark. This can be formulated as follows: Let = {v1, v2, ..., vN } denote the set of video clips. Let = {n1, n2, ..., nN }, = {a1, a2, ..., aN ai C} be their corresponding narrations and action labels separately, where represents the set of action classes. For each data sample i, we formulate the MQA task as: : (vi, Q, Oi) (cid:55) [p1, p2, . . . , pK] , where (cid:88) k=1 pk = 1, pk [0, 1] (1) where vi is the input sample (e.g., video clip), is the space of possible questions, Oi = {ni, Di} represents the set of answer options, ni is the correct narration, and Di represents 1 sampled distractors. These can be sampled randomly from narrations in other action classes: Dr = Uniform({nj cj {ai}}) (2) However, random sampling Dr likely obtains trivial answers (Figure 1). Inspired by hard-example mining [47] and adversarial training [38], we propose to utilize SOTA action recognition models to find distractors: : (0, 1)C (3) 3 For video specific vi, we obtain the top 1 predicted classes: (4) (5) Ci = TopK1(g(vi) {ai}) The model-based distractor sampling becomes: Dm = Uniform({nj cj Ci}) The complete set of answers is formed as Or = {ni} Dm for random sampling or Om Dr based sampling. We used = 5 for our benchmark. = {ni} for modelWe compare the two sampling strategies  (Table 1)  . We chose two leading action recognition methods on EPICKITCHENS-100, namely, AVION [83] and TIM [7]. The results indicate that the TIM method consistently produced more challenging distractors for the evaluated MLLMs  (Table 1)  , qualitative examples in Appendix Figure 5). Consequently, we fixed with TIM and designated our benchmark as EPIC-KITCHENS-100-MQA. Note that this test benchmark is distinct from the validation set of EPIC-KITCHENS-100 in two significant aspects. First, it has been reformulated to MQA to facilitate the evaluation of MLLMs. Second, EPIC-KITCHENS-100 employs official keys rather than ground truth narrations (as we use). If we were to directly compare with SOTA methods in EPIC-KITCHENS-100, our MLLM can be considered as refinement-stage that selects the right answer from the predictions of g. To make fair comparison, we also report these results using the official keys in Table 4, even though, as we demonstrate below, the official key is not an ideal text input for MLLMs. 3.1.2. Training with adversarial wrong answers Similar to the EPIC-KITCHENS-100-MQA benchmark formulation, we can also train MLLMs with adversarial distractors that were generated by leading action recognition models, AVION [83] and TIM [7]. Unlike for the"
        },
        {
            "title": "Choice\nSelection",
            "content": "Random 5 (Easy) Avion-Top 5 (Medium) TIM-Top 5 (Hard) GPT-4o-mini (07-18) GPT-4o (08-06) LLaVA-OV-0.5B LLaVA-OV-7B LLaVA-Video-7B 72.0 87.6 59.3 68.8 65.0 44.2 56.7 37.1 33.6 40. 37.4 52.2 32.0 28.9 35.7 Table 1. Methods: Comparison between different sources of distractors on EPIC-KITHCENS-100-MQA. Models were evaluated on either random, AVION-generated, or TIM-generated distractors. The values are reported as percent accuracy. Note, the Random does not include action-recognition model-generated distractors (thus is easy), while TIM produced the hardest distractors. Data are based on the validation set of EPIC-KITCHENS100. An 8 frame sequence is uniformly sampled from annotated video clips. benchmark, we train MLLMs with both official keys and narrations to serve as data augmentation. To avoid the models from memorizing the position of the ground truth in the options or the order of distractors, we always randomly shuffle the answers. Meanwhile, since EPIC-KITCHENS100-MQA benchmark is based on TIMs predictions, training MLLMs to pick the right answers using distractors generated by TIM could lead to an over-fitting to TIMs distributions, resulting in an independent and identically distributed (IID) setting. Therefore, we used predictions from AVION to train our model on EPIC-KITCHENS-100 training set. This provides us with an out-of-distribution (OOD) evaluation on EPIC-KITCHENS-100-MQA. Moreover, we also present the results derived from training with adversarials generated by TIM, which yield the strongest results in EPIC-KITCHENS-100-MQA, by design as it is IID. The results are shown in Table 2. 3.2. LLaVAction models Our final method, called LLaVAction, is based on the opensource LLaVA-OneVision 0.5 and 7B models [25], and on LLaVA-Video-7B [81]. Thus, we benchmark these as baselines. We present results for both the 0.5 billion (0.5B) and 7 billion (7B) parameter models of LLaVA-OneVision, and the LLaVA-Video-7B, as it does not provide 0.5B alternative. In all experiments, we started from their released checkpoints [3335] (Appendix 7). In the following, we highlight the methods that we developed that empirically improve performance in action recognition. GPT-4o distillation. Distilling GPT-4o represents prevalent, albeit contentious strategy aimed at enhancing open-source MLLMs. However, as shown in Table 1, since GPT-4o itself struggles with our empirically hard distractors, distilling from GPT-4o alone harms the performance on our proposed EPIC-KITCHENS-100-MQA benchmark if not trained on MQA. Following common practices [30, 81], we craft two ways for GPT-4o to annotate. One is to provide captions of the video clips, and the other is to generate open-ended question-answer pairs. The details are in the Appendix 8. Egocentric vs. Allocentric prompt perspective. Since the videos are taken from the first-person perspective, we were interested in understanding whether controlling the perspective in the instruction prompt makes difference. As shown in Table 3, switching from the third-person (allocentric) perspective to the first-person (egocentric) perspective improved the results. We present the prompts in the Appendix 9.2. Adversarial distractors from action recognition models. We ran inference for AVION and TIM on EPICKITCHENS-100 using their officially released code and weights to obtain their predictions on both training and validation sets. We sort the predictions by model confidence and keep the top 5 predictions as distractors. To ensure that ground truth is always in the options in the training, we remove the least confident prediction in the set and replace it with the ground truth (either with ground truth narration or official key, as appropriate for the experiment). To avoid the models from memorizing the position of the ground truth in the options or the order of distractors, we always shuffle the answers. Temporal detection. The ability to accurately detect the initiation and conclusion of an action serves as critical metric for assessing the models comprehension of the action itself. Therefore, we have developed an auxiliary task that instructs the model to predict the start and end timestamps of ground truth narration based on randomly padded video clip. Specifically, given video segment vi with ground truth start timestamp si and end timestamp ei, we introduce fixed temporal padding δ = 3 seconds distributed between start and end. Let α Uniform(0, 1) be the proportion of padding allocated to the start: ˆsi = si αδ ˆei = ei + (1 α)δ (6) (7) We therefore use the new timestamps ˆsi and ˆei to obtain the padded video segment ˆvi. During training, our LLaVAction model takes ˆvi as input and predicts the start and end times as decimal texts (e.g., 3.20, 1.20) corresponding to the true start and end times of the action in the padded video. More details on the prompts are provided in the Appendix 9.3. Vision token supervision. Most MLLMs rely only on the language prediction of the next token to train the model and extract information from visual tokens [30]. However, recent findings [3, 31, 79] suggested that this training strategy will decrease the importance of vision tokens in late layers of MLLMs. Therefore, in this work, we explore whether we can apply intermediate supervision to visual tokens via the fixed action classes provided by EPIC-KITCHENS-100. To achieve this, we add an additional learnable action token into the input tokens. The order of the input tokens is system text tokens, visual tokens, learnable action token and instruction text tokens, which enables the action token to grasp action information from visual tokens and then contribute to the subsequent language tasks through casual attentions of the LLM backbone. Lets denote the hidden states at the final layer of the MLLM as: (cid:68) 1 , , k, 1 , , lv , ha, k+1, , lq (cid:69) (8) Rd are the hidden states corresponding to visual toH kens, ha Rd denotes the hidden state of the learnable action token, Rd denotes the text tokens, denotes the hidden dimension of the LLM. lq and lv denote the length of text tokens and length of visual tokens respectively. We apply three classification heads on top of the hidden state ha to predict nouns, verbs, and actions separately and use cross-entropy loss to train the classifiers, in the belief that the classification training could guide the action token to learn better extract action information. Direct prediction. Direct prediction asks the model to predict the answer directly. We prompt the model by asking it to answer what action it is performing in the video. The prompt is available in the Appendix 9.4. Memory: associating the current action with prior actions. In the real world, actions exhibit certain natural continuity. This temporal aspect of actions can improve predictability, particularly when prior actions are known. For instance, in the context of human movement, sequences such as walking, running, or jumping can be anticipated based on the initiation of previous actions. Moreover, in complex tasks such as cooking, the presence of specific steps often indicates the next action to be taken, thereby enhancing predictive modeling. In addition, see Appendix 9.5. Therefore, ideally, we want to leverage prior actions and learn: θ = arg max θ (cid:88) t=n+ log Pθ(at at1, . . . , atn) (9) where θ is the optimal set of model parameters we are trying to find, at time t, at1, . . . , atn represents the sequence of previous actions. We set = 2. is the current action at We rely on visual instruction tuning to learn this implicitly. Therefore, we give the prompt, that includes the ground truth actions, such as:2.83 seconds ago, you started an action take paper. 0.38 seconds ago, you started an action open bin. What action are you currently performing? Here are the options of actions you can select. We balance between the standard MQA vs. performing MQA that uses contextual information by incorporating the previous two actions as additional input. During training, 70% of the time we perform standard MQA and in the remaining 30%, we perform MQA with previous two actions provided as context. We evaluate the effectiveness of this method in two settings: (1) The baseline setting where no prior actions are given. (2) test time augmentation setting where the models own predictions of previous actions are used at test time. Note, we use = 2 in both training and testing in the paper and we use ground truth for the previous actions during training. 4. Experiments We tested three open-source baseline models, LLaVAVideo-7B, LLaVA-OV-7B and LLaVA-OV-0.5B, then built series of methods that empirically boosted performance In our EPIC-KITCHENS-100-MQA (see Section 3.2). benchmark, ground truth narration is within the 5 options. In addition to ground truth, the distractors are generated from TIM with its highest confidence predictions (see Section 3.1). For training, we experiment with different sources of distractors for MQA (see Section 3.1.2). We study the validation accuracy when we (1) use random sampling for distractors, (2) use AVION generated distractors (highest confidence predictions), and (3) use TIM generated distractors. Different from the EPIC-KITCHENS-100-MQA benchmark, we also include the official key choices as an additional data augmentation for the model to be aware of different representation of actions instead of only using the narration choices. In addition to video MQA (Section 3.2) we added four auxiliary methods in our training pipeline: (1) direct prediction, (2) temporal detection, (3) GPT-4o distilled video caption (Appendix Figure 6), and (4) GPT-4o distilled video open-ended question answering. Each visual task reannotates EPIC-KITCHENS-100s training data, in total contributing to 530K annotated video-language pairs to train the MLLMs. Note that our techniques, such as visual token supervision, learning from prior actions, and changing prompt perspectives, do not require us to augment with additional training data. The 7B and 0.5B models train in 12 and 11 hours on 32 GH200 GPUs, respectively. Across all experiments and all baseline models with gradient accumulation was set to 2, the batch size is 64. We trained for 2 epochs for all experiments. Following the hyper-parameters of LLaVA-Video, the MLP connector, LLM (Qwen2-7B [58] and Qwen20.5B), the visual encoder SigLIP-384 [75] are being trained. Figure 3. Qualitative results. LLaVAction-7B consistently outperforms GPT-4o and LLaVA-Video-7B when tested on hard distractors. Bold option denotes ground truth, and the icons denote the selection of the models. See also Appendix 12. The learning rate is 1e-5 for LLM and 2e-6 for the vision encoder. See Zhang et al. [81] for additional details. Methods zero-shot GPT-4o zero-shot GPT-4o-mini zero-shot LLaVA-Video-7B zero-shot LLaVA-OV-7B zero-shot LLaVA-OV-0.5B 8 16 52.2 N/A 37.4 N/A 34.8 35.7 30.5 28.9 31.6 32.0 LLaVAction: LLaVA-Video-7B 71.7 71.3 LLaVAction: LLaVA-OV-7B 64.8 LLaVAction: LLaVA-OV-0.5B 73.4 72.3 65.4 Table 2. EPIC-KITCHENS-100-MQA Results on OOD TIM (Hard) test-set. Columns represent the number of frames used for testing. Fine-tuned models use the same number of frames (f) for both training and testing. Percent accuracy is shown. Previous work suggests that using more frames improves performance [81]. We use eight frames for distractor experiments  (Table 1)  , 8 or 16 frames for our main results on EPIC-KITCHENS-100-MQA  (Table 2)  , 16 for ablation studies  (Table 7)  , and 16 or 32 frames for EPICKITCHENS-100  (Table 4)  . We use 64 frames for the model reported in additional benchmarks  (Table 5)  . LLaVA-Video-7B LLaVAction-7B OOD Setting: Zero-shot + GPT-4o distillation Rand. distractors w/ Allocentric prompt Rand. distractors w/ Egocentric prompt AVION adversarial distractors + Temporal Detection + Vision Supervision + GPT-4o distillation + Direct Prediction + Learning prior actions + Learning prior actions w/ TT. Aug IID Setting: +TIM distractors +TIM distractors w/ TT. Aug 34.8 21.9 55.0 55.5 64.4 65.2 69.1 71.5 73.6 73.4 74.1 76.3 77.0 Table 3. EPIC-KITCHENS-100-MQA Results on the OOD TIM (Hard) test-set, with our added methods. Techniques are gradually added to achieve the final model. Once the model learns to leverage prior actions to infer the current one, we could save its prior actions in memory buffer and apply test time augmentation, which is denoted as TT. Aug in the table. All experiments were run with 16 frames, and percent accuracy is shown. 5. Results We focus on evaluating and improving leading MLLMs abilities for recognizing human actions in complex scenes. We focus on fine-grained, multi-tasking actions that occur in kitchen activities. Unlike existing MLLMs that leverage annotations from either distillation from powerful MLLMs or human annotations, we also leverage hard examples selected by leading action recognition models as well as proposing novel methods to improve MLLMs on this task. First, we developed the EPIC-KITCHENS-100-MQA benchmark (Section 3). Specifically, we used TIM to generate answers  (Table 1)  and used this as the OOD test-set when evaluating our methods (Tables 2 and 3). We also trained model with TIM distractors, thus representing IID (top-line) performance  (Table 3)  . 5.1. Results on EPIC-KITCHENS-100-MQA We developed series of methods that empirically improved performance. We report values for the full model  (Table 2)  , 6 each added method  (Table 3)  , and leave-one-out ablations (Appendix 10 and Appendix Table 7) to assess their contributions. We start with the zero-shot evaluation of closedsource GPT-4o and GPT-4o-mini, and open-source models LLaVA-Video-7B, LLaVA-OV-7B, and LLaVA-OV-0.5B  (Table 2)  . GPT-4o from OpenAI performs best, with an accuracy of 52.2. Of the open-source models, LLaVA-Video7B performs the best (35.7; Table 2). We therefore use LLaVA-Video-7B and fine-tune the model with series of innovations that leads to 21-point improvement over GPT4o (Ours: 71.7 8-frames, 73.4 16-frames vs. GPT-4o 52.2 8-frames1; Table 2). Using our methods, including the testtime action memory, we reach 74.1  (Table 2)  . Next, we outline the methods that achieved our SOTA results. Our experiments suggested that fine-tuning the model with only the caption and open-ended question answering (GPT-4o distillation) results in performance degradation (i.e., zero-shot was 35.7 vs. distilled was only 21.9 using LLaVA-Video-7B). Therefore, we always include the MQA task in the following studies. Training with random distractors or adversarial distractors (AVION). In Table 3, we compare LLaVA-Video7B training with random distractors and adversarial distractors using AVIONs predictions. There is 8.9 difference in accuracy, showing the effectiveness of training against adversarial distractors. Perspective Prompt (Egocentric). As shown in Table 3, when we fix the distractors from random sampling, using the egocentric prompt gives 0.5 point improvement over using the allocentric prompt. Vision token supervision. Inspired by recent findings suggesting that the importance of vision tokens decreases in MLLM in late layers [79] and vision token compression in MLLMs [3, 31]. We explored ways to improve vision tokens. Instead of removing redundant vision tokens, we aimed to add intermediate supervision to vision tokens, which is detailed in Section 3. As shown in Table 3 and Table 7, including vision supervision gives an additional 3.9 points improvement. Impact of GPT-4o distillation. We note in Table 3 that simply adding video caption and video open-ended question will result in performance degradation. Based on the fact that it improves performance when we combine it with MQA task using AVION distractors, we believe its sign of catastrophic forgetting of MQA capability. This gives 2.4 boost in performance  (Table 3)  . Direct Prediction. With direct prediction, the model learns to directly predict the action without picking from the given options and video frames. This gives an additional 2.1 points improvement  (Table 3)  . 1running GPT-4o models beyond 8 frames on the full 9K validation set is cost prohibitive, and we outperform GPT-4o when comparing 8 vs. 8. 7 Learning Prior Actions. We expect that once the model has solid understanding of the action from videos, we could further train the model to use contextual information such as the prior two actions as hint for better accuracy. Table 3 shows that once we supply the prior two actions from the models own predictions there is 0.5 point improvement compared to the baseline. In summary, the combination of the methods we propose greatly improves the performance of the base LLaVAVideo-7B model (See qualitative examples in Figure 3). From zero-shot at 34.8 accuracy, to using AVION-generated adversarials, significantly boosts performance to 64.4. Then, the series of smaller but important additions raises the final number to 73.4 without test-time augmentation, and to 74.1 with it, for the final OOD performance on our benchmark. 5.2. Results on EPIC-KITCHENS-100 We use our model, LLaVAction-7B, to test its performance on the EPIC-KITCHENS-100 action recognition validation set. The official EPIC-KITCHENS-100 challenge (test-set) uses the official key (see Methods) and not ground-truth narrations. Therefore, in Table 4, we use the validation set to compare the difference between using the official key and ground truth narrations. We find there is performance gap between those two settings, with ground truth narrations being much more performant for LLaVAction models. We empirically observe that we get better results if we scale top-K from 5 to 20. Thus, we used = 20 to achieve SOTA on EPIC-KITCHENS-100. We discuss this in more detail in Section 6. Methods LaViLa [84] TAdaFormer-L/14 [23] LVMAE [21] M&M [67] AVION [83] TIM [7] Ours, LLaVAction-7B (16f) w/ official key Ours, LLaVAction-7B (16f) w/ GT narration Ours, LLaVAction-7B (32f) w/ official key Ours, LLaVAction-7B (32f) w/ GT narration Acc. 51.0 51.8 52.1 53.6 54.4 56.4 57.8 63.1 58.3 63.2 Table 4. Original EPIC-KITCHENS-100 Top-1 accuracy on action classification (validation set). 16 or 32 Frames (f) are uniformly sampled from validation set video clips. TIMs top 20 predictions are used for both training and testing. For specific verbnoun performance see Appendix Figure 7. Closed-source models GPT-4V [1] GPT-4o [15] Gemini-1.5-Flash [51] Gemini-1.5-Pro [51] Open-source models LongVA-7B [78] mPLUG-Owl3 [71] VideoChat2-7B [27] VideoLLaMA2-7B [10] LLaVA-OV-7B [25] LLaVA-Video-7B [81] LLaVAction-7B (Ours) S che ench E Video w -su bs Video n Video ench ext erceptio est - - 65.7 72.2 - - - 51.7 60.1 57.3 59.0 43.5 - - - - 54.5 60.4 54.6 56.7 58.6 61.1 59.9 71.9 70.3 75.0 52.6 59.3 42.3 47.9 58.2 63.3 63.9 63.3 77.2 75.0 81.3 54.3 68.1 54.6 50.3 61.5 69.7 71.4 61.3 66.7 61.6 64. - 52.1 - - 56.5 58.2 58.6 - - - - 68.3 78.6 78.6 - 79.4 83.2 82.8 - - - - - - - 51.4 57.1 67.9 70.2 Relative improvement of ours over the baseline LLaVA-Video-7B +1.7 +2.5 +0.6 +1.7 +0.4 -0. +2.3 Table 5. Performance on additional video benchmarks that contain human actions. Please note, we are not claiming SOTA, we are noting that we can improve performance over our baseline open-source model (LLaVA-Video-7B [81]). Note, the value is the combination across all sub-tasks for some benchmarks; for details of per-task performance see Appendix Tables 9, 10, 11, and Appendix 13. We show top-performance closed-source models for reference. Top open-source models are shown in bold, and the second best are underlined. 5.3. Results on Additional Benchmarks Next, our objective is to study whether LLaVAction-7B, and therefore the inclusion of EPIC-KITCHENS-100 data, can improve the performance on additional benchmarks that have some component of video-based action recognition (see Appendix 11). For fair comparison with our base model, we follow the LLaVA-Video-7B methodology and use 64 frames during training. Training on EPIC-KITCHENS-100-MQA alone might result in over-fitting. Therefore, here we use data replay [25, 81] to aid in generalization of the model. Thus, we mix in the training data of LLaVA-Video, namely LLaVAVideo-178K [80]. We then benchmark the model on six additional video benchmarks and see improvements over LLaVA-Video-7B on five of them, indicating the generality of our model  (Table 5)  . 6. Discussion language bias in the official key in Potential EPIC-KITCHENS-100. The official keys in EPICKITCHENS-100 originate from the raw narrations that are curated and compressed by combination of word clustering and iterative manual refinement [12]. However, this compression might change the semantic meaning of both nouns, verbs and the way they are combined. As result, large language models that are sensitive to the meaning of words can be misled (see comparisons in Appendix Figure 4 and Table 8). While we show SOTA results using the official key, we note that we can achieve better performance if we use the uncompressed, original narrations. We hope that our work could inspire future work to study the best text representation of actions to train and evaluate MLLMs in action recognition. Training signals for MLLMs. There are various sources of training signals for MLLMs. The most common form is human annotations [48, 59] which are considered golden annotations. The second source are other MLLMs [25]. common format is using closed-source models such as GPT4o [15], Gemini [14], and Claude. Some MLLMs indirectly learn from other caption models due to how the dataset was crafted [61]. Alternatively, there are also works that suggest LLMs can improve by using self-learning [9, 74], though this is less explored in MLLMs. In this work, we obtain training signals from the leading action recognition models. major difference is that we are not directly learning the generated text from the other models, but learning from what are perceived hard from those models. This resembles adversarial training in spirit [38, 49]. Lastly, we tried several other methods that did not yield notable improvements (Appendix 14). Summary. The development of MLLMs motivated us to develop new video MQA benchmark for action recognition. We show that when we sample difficult incorrect answers as distractors, leading MLLMs struggle to recognize the correct actions. We also propose series of methods that greatly improve the ability to perform action recognition with MLLMs. Notably, we find that while leading closed-source models such as GPT-4o can zero-shot outperform open-source models, we developed series of methods that pushed the open-source models performance beyond GPT-4o. Lastly, we showed improvements over opensource models on other action-related video benchmarks. 6.1. Acknowledgments We thank the Swiss AI Initiative Project ID a03 from the Swiss National Supercomputing Centre (CSCS); H.Q. and A.M. thank the Boehringer Ingelheim Fonds PhD stipend; M.W.M. thanks the Vallee Foundation; M.W.M. and A.M. thank the SNSF by grant No. 320030-227871. 6.2. Author Contributions: Conceptualization: S.Y., H.Q., A.M., M.W.M.; Methodology & Software: S.Y., H.Q., M.W.M.; Experiments: S.Y., H.Q.; Writing: M.W.M, S.Y., H.Q., A.M.; Visualization: H.Q., S.Y., M.W.M.; Funding acquisition: M.W.M. A.M."
        },
        {
            "title": "References",
            "content": "[1] Josh Achiam, Steven Adler, Sandhini Agarwal, Lama Ahmad, Ilge Akkaya, Florencia Leoni Aleman, Diogo Almeida, Janko Altenschmidt, Sam Altman, Shyamal Anadkat, et al. Gpt-4 technical report. arXiv preprint arXiv:2303.08774, 2023. 2, 8 [2] Jean-Baptiste Alayrac, Jeff Donahue, Pauline Luc, Antoine Miech, Iain Barr, Yana Hasson, Karel Lenc, Arthur Mensch, Katherine Millican, Malcolm Reynolds, et al. Flamingo: visual language model for few-shot learning. Advances in neural information processing systems, 35:2371623736, 2022. 2 [3] Kazi Hasan Ibn Arif, JinYi Yoon, Dimitrios S. Nikolopoulos, Hans Vandierendonck, Deepu John, and Bo Ji. Hired: Attention-guided token dropping for efficient inference of high-resolution vision-language models, 2024. 4, 7 [4] Gedas Bertasius, Heng Wang, and Lorenzo Torresani. Is space-time attention all you need for video understanding? In ICML, page 4, 2021. 2 [5] Tom Brown. Language models are few-shot learners. arXiv preprint arXiv:2005.14165, 2020. [6] Joao Carreira and Andrew Zisserman. Quo vadis, new model and the kinetaction recognition? In proceedings of the IEEE Conference ics dataset. on Computer Vision and Pattern Recognition, pages 62996308, 2017. 2 [7] Jacob Chalk, Jaesung Huh, Evangelos Kazakos, Andrew Zisserman, and Dima Damen. Tim: time interval machine for audio-visual action recognition. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 18153 18163, 2024. 1, 2, 3, 7 [8] Keshigeyan Chandrasegaran, Agrim Gupta, Lea Hadzic, Taran Kota, Jimming He, Cristobal Eyzaguirre, Zane Durante, Manling Li, Jiajun Wu, and Fei-Fei Li. Hourvideo: 1-hour video-language understanding. Advances in Neural Information Processing Systems, 37:5316853197, 2025. 2 [9] Zixiang Chen, Yihe Deng, Huizhuo Yuan, Kaixuan Ji, and Quanquan Gu. Self-play fine-tuning converts weak language models to strong language models. arXiv preprint arXiv:2401.01335, 2024. 8 [10] Zesen Cheng, Sicong Leng, Hang Zhang, Yifei Xin, Xin Li, Guanzheng Chen, Yongxin Zhu, Wenqi Zhang, Ziyang Luo, Deli Zhao, et al. Videollama 2: Advancing spatial-temporal modeling and arXiv preprint audio understanding in video-llms. arXiv:2406.07476, 2024. 8 [11] Wenliang Dai, Junnan Li, Li, AMH Tiong, InstructZhao, Wang, Li, Fung, and Hoi. blip: Towards general-purpose vision-language models with instruction tuning. arxiv 2023. arXiv preprint arXiv:2305.06500, 2, 2023. [12] Dima Damen, Hazel Doughty, Giovanni Maria Farinella, Antonino Furnari, Evangelos Kazakos, Jian Ma, Davide Moltisanti, Jonathan Munro, Toby Perrett, Will Price, et al. Rescaling egocentric vision: Collection, pipeline and challenges for epic-kitchens-100. International Journal of Computer Vision, pages 1 23, 2022. 1, 2, 8 [13] Abhimanyu Dubey, Abhinav Jauhri, Abhinav Pandey, Abhishek Kadian, Ahmad Al-Dahle, Aiesha Letman, Akhil Mathur, Alan Schelten, Amy Yang, Angela Fan, et al. The llama 3 herd of models. arXiv preprint arXiv:2407.21783, 2024. 1 [14] Gemini Team et al. Gemini: family of highly capable multimodal models, 2024. 2, 8 [15] OpenAI et al. Gpt-4o system card, 2024. 1, 2, 8 [16] Christoph Feichtenhofer, Haoqi Fan, Jitendra Malik, and Kaiming He. Slowfast networks for video recogIn Proceedings of the IEEE/CVF internanition. tional conference on computer vision, pages 6202 6211, 2019. 2 [17] Chaoyou Fu, Yuhan Dai, Yongdong Luo, Lei Li, Shuhuai Ren, Renrui Zhang, Zihan Wang, Chenyu Zhou, Yunhang Shen, Mengdan Zhang, et al. Videomme: The first-ever comprehensive evaluation benchmark of multi-modal llms in video analysis. arXiv preprint arXiv:2405.21075, 2024. 2, [18] Rohit Girdhar, Mannat Singh, Nikhila Ravi, Laurens Van Der Maaten, Armand Joulin, and Ishan Misra. Omnivore: single model for many visual modalities. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 16102 16112, 2022. 2, 3 9 [19] A. Gorban, H. Idrees, Y.-G. Jiang, A. Roshan Zamir, I. Laptev, M. Shah, and R. Sukthankar. THUMOS challenge: Action recognition with large number of classes. http://www.thumos.info/, 2015. 2 [20] Kristen Grauman, Andrew Westbury, Eugene Byrne, Zachary Chavis, Antonino Furnari, Rohit Girdhar, Jackson Hamburger, Hao Jiang, Miao Liu, Xingyu Liu, et al. Ego4d: Around the world in 3,000 hours of egocentric video. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 1899519012, 2022. 2 [21] Nitesh Bharadwaj Gundavarapu, Luke Friedman, Raghav Goyal, Chaitra Hegde, Eirikur Agustsson, Sagar M. Waghmare, Mikhail Sirotenko, Ming-Hsuan Yang, Tobias Weyand, Boqing Gong, and Leonid Sigal. Extending video masked autoencoders to 128 frames, 2024. 7 [22] Jiaming Han, Kaixiong Gong, Yiyuan Zhang, Jiaqi Wang, Kaipeng Zhang, Dahua Lin, Yu Qiao, Peng Gao, and Xiangyu Yue. Onellm: One framework to In Proceedings align all modalities with language. of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 2658426595, 2024. 2 [23] Ziyuan Huang, Shiwei Zhang, Liang Pan, Zhiwu Qing, Yingya Zhang, Ziwei Liu, and Marcelo H. Ang Jr. Temporally-adaptive models for efficient video understanding, 2023. [24] Aniruddha Kembhavi, Mike Salvato, Eric Kolve, Minjoon Seo, Hannaneh Hajishirzi, and Ali Farhadi. diagram is worth dozen images. In Computer Vision ECCV 2016: 14th European Conference, Amsterdam, The Netherlands, October 1114, 2016, Proceedings, Part IV 14, pages 235251. Springer, 2016. 2 [25] Bo Li, Yuanhan Zhang, Dong Guo, Renrui Zhang, Feng Li, Hao Zhang, Kaichen Zhang, Yanwei Li, Ziwei Liu, and Chunyuan Li. Llava-onevision: Easy visual task transfer. arXiv preprint arXiv:2408.03326, 2024. 1, 2, 4, 8 [26] Chunyuan Li, Zhe Gan, Zhengyuan Yang, Jianwei Yang, Linjie Li, Lijuan Wang, Jianfeng Gao, et al. Multimodal foundation models: From specialists to general-purpose assistants. Foundations and Trends in Computer Graphics and Vision, 16(1-2):1214, 2024. 2 [27] Kunchang Li, Yali Wang, Yinan He, Yizhuo Li, Yi Wang, Yi Liu, Zun Wang, Jilan Xu, Guo Chen, Ping Luo, et al. Mvbench: comprehensive multi-modal In Proceedings of video understanding benchmark. the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 2219522206, 2024. 1, 2, 8, 3 visual representation by alignment before projection. arXiv preprint arXiv:2311.10122, 2023. 2 [29] Ji Lin, Hongxu Yin, Wei Ping, Pavlo Molchanov, Mohammad Shoeybi, and Song Han. Vila: On pretraining for visual language models. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 2668926699, 2024. 1 [30] Haotian Liu, Chunyuan Li, Qingyang Wu, and Yong Jae Lee. Visual instruction tuning. Advances in neural information processing systems, 36, 2024. 2, [31] Ting Liu, Liangtao Shi, Richang Hong, Yue Hu, Quanjun Yin, and Linfeng Zhang. Multi-stage vision token dropping: Towards efficient multimodal large language model, 2024. 4, 7 [32] Yuan Liu, Haodong Duan, Yuanhan Zhang, Bo Li, Songyang Zhang, Wangbo Zhao, Yike Yuan, Jiaqi Wang, Conghui He, Ziwei Liu, et al. Mmbench: Is your multi-modal model an all-around player? In European conference on computer vision, pages 216 233. Springer, 2024. 2 [33] LMMS-Lab. Llava-onevision: Qwen2-7b model. Hugging Face Model Hub, 2024. 4 [34] LMMS-Lab. Llava-onevision: Qwen2-7b model. Hugging Face Model Hub, 2024. [35] LMMS-Lab. Llava-video: Qwen2-7b model. Hugging Face Model Hub, 2024. 4 [36] Pan Lu, Hritik Bansal, Tony Xia, Jiacheng Liu, Chunyuan Li, Hannaneh Hajishirzi, Hao Cheng, Kai-Wei Chang, Michel Galley, and Jianfeng Gao. Mathvista: Evaluating mathematical reasoning of founarXiv preprint dation models in visual contexts. arXiv:2310.02255, 2023. 2 [37] Muhammad Maaz, Hanoona Rasheed, Salman Khan, and Fahad Shahbaz Khan. Video-chatgpt: Towards detailed video understanding via large vision and lanarXiv preprint arXiv:2306.05424, guage models. 2023. [38] Aleksander Madry. els resistant to adversarial attacks. arXiv:1706.06083, 2017. 3, 8 Towards deep learning modarXiv preprint [39] Karttikeya Mangalam, Raiymbek Akshulakov, and Jitendra Malik. Egoschema: diagnostic benchmark for very long-form video language understanding. Advances in Neural Information Processing Systems, 36: 4621246244, 2023. 3 [40] Minesh Mathew, Dimosthenis Karatzas, and CV Jawahar. Docvqa: dataset for vqa on document images. In Proceedings of the IEEE/CVF winter conference on applications of computer vision, pages 22002209, 2021. 2 [28] Bin Lin, Yang Ye, Bin Zhu, Jiaxi Cui, Munan Ning, Peng Jin, and Li Yuan. Video-llava: Learning united [41] Brandon McKinzie, Zhe Gan, Jean-Philippe Fauconnier, Sam Dodge, Bowen Zhang, Philipp Dufter, 10 Dhruti Shah, Xianzhi Du, Futang Peng, Anton Belyi, et al. Mm1: methods, analysis and insights from multimodal llm pre-training. In European Conference on Computer Vision, pages 304323. Springer, 2024. 2 [42] Viorica Patraucean, Lucas Smaira, Ankush Gupta, Adria Recasens, Larisa Markeeva, Dylan Banarse, Skanda Koppula, Mateusz Malinowski, Yi Yang, Carl Doersch, et al. Perception test: diagnostic benchmark for multimodal video models. Advances in Neural Information Processing Systems, 36:4274842761, 2023. 2, 3 [43] Fadime Sener, Dibyadip Chatterjee, Daniel Shelepov, Kun He, Dipika Singhania, Robert Wang, and Angela Yao. Assembly101: large-scale multi-view video dataset for understanding procedural activities. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 2109621106, 2022. 2 [44] Amir Shahroudy, Jun Liu, Tian-Tsong Ng, and Gang Wang. Ntu rgb+ d: large scale dataset for 3d human In Proceedings of the IEEE conactivity analysis. ference on computer vision and pattern recognition, pages 10101019, 2016. [45] Ziyao Shangguan, Chuhan Li, Yuxuan Ding, Yanan Zheng, Yilun Zhao, Tesca Fitzgerald, and Arman Cohan. Tomato: Assessing visual temporal reasoning capabilities in multimodal foundation models. arXiv preprint arXiv:2410.23266, 2024. 2 [46] Hao Shao, Shengju Qian, Han Xiao, Guanglu Song, Zhuofan Zong, Letian Wang, Yu Liu, and Hongsheng Li. Visual cot: Advancing multi-modal language models with comprehensive dataset and benchmark for chain-of-thought reasoning, 2024. 2 [47] Abhinav Shrivastava, Abhinav Gupta, and Ross Girshick. Training region-based object detectors with online hard example mining. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 761769, 2016. 3 [48] Gunnar Sigurdsson, Gul Varol, Xiaolong Wang, Ali Farhadi, Ivan Laptev, and Abhinav Gupta. Hollywood in homes: Crowdsourcing data collection for activity understanding. In Computer VisionECCV 2016: 14th European Conference, Amsterdam, The Netherlands, October 1114, 2016, Proceedings, Part 14, pages 510526. Springer, 2016. 8 [49] Yang Song, Rui Shu, Nate Kushman, and Stefano Ermon. Constructing unrestricted adversarial examples with generative models. Advances in neural information processing systems, 31, 2018. 8 [50] Khurram Soomro, Amir Roshan Zamir, and Mubarak Shah. Ucf101: dataset of 101 human actions arXiv preprint classes from videos in the wild. arXiv:1212.0402, 2012. [51] Gemini Team, Rohan Anil, Sebastian Borgeaud, JeanBaptiste Alayrac, Jiahui Yu, Radu Soricut, Johan Schalkwyk, Andrew Dai, Anja Hauth, Katie Millican, et al. Gemini: family of highly capable multimodal models. arXiv preprint arXiv:2312.11805, 2023. 8 [52] Shengbang Tong, Ellis Brown, Penghao Wu, Sanghyun Woo, Manoj Middepogu, Sai Charitha Akula, Jihan Yang, Shusheng Yang, Adithya Iyer, Xichen Pan, Ziteng Wang, Rob Fergus, Yann LeCun, and Saining Xie. Cambrian-1: fully open, vision-centric exploration of multimodal llms, 2024. 2 [53] Shengbang Tong, Zhuang Liu, Yuexiang Zhai, Yi Ma, Yann LeCun, and Saining Xie. Eyes wide shut? exploring the visual shortcomings of multimodal llms. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 9568 9578, 2024. 2 [54] Zhan Tong, Yibing Song, Jue Wang, and Limin Wang. Videomae: Masked autoencoders are data-efficient learners for self-supervised video pre-training. Advances in neural information processing systems, 35: 1007810093, 2022. 2 [55] Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, et al. Llama 2: Open foundation and finetuned chat models. arXiv preprint arXiv:2307.09288, 2023. 2 [56] Du Tran, Lubomir Bourdev, Rob Fergus, Lorenzo Torresani, and Manohar Paluri. Learning spatiotemporal features with 3d convolutional networks. In Proceedings of the IEEE international conference on computer vision, pages 44894497, 2015. [57] Maria Tsimpoukelli, Jacob Menick, Serkan Cabi, SM Eslami, Oriol Vinyals, and Felix Hill. Multimodal few-shot learning with frozen language models. Advances in Neural Information Processing Systems, 34: 200212, 2021. 2 [58] Peng Wang, Shuai Bai, Sinan Tan, Shijie Wang, Zhihao Fan, Jinze Bai, Keqin Chen, Xuejing Liu, Jialin Wang, Wenbin Ge, et al. Qwen2-vl: Enhancing vision-language models perception of the world at arXiv preprint arXiv:2409.12191, any resolution. 2024. 1, 5 [59] Xin Wang, Jiawei Wu, Junkun Chen, Lei Li, YuanFang Wang, and William Yang Wang. Vatex: largescale, high-quality multilingual dataset for video-andlanguage research, 2020. 8 [60] Xin Wang, Taein Kwon, Mahdi Rad, Bowen Pan, Ishani Chakraborty, Sean Andrist, Dan Bohus, Ashley Feniello, Bugra Tekin, Felipe Vieira Frujeri, et al. 11 Holoassist: an egocentric human interaction dataset for interactive ai assistants in the real world. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 2027020281, 2023. 2 [61] Yi Wang, Yinan He, Yizhuo Li, Kunchang Li, Jiashuo Yu, Xin Ma, Xinhao Li, Guo Chen, Xinyuan Chen, Yaohui Wang, Conghui He, Ping Luo, Ziwei Liu, Yali Wang, Limin Wang, and Yu Qiao. Internvid: largescale video-text dataset for multimodal understanding and generation, 2024. [62] Yi Wang, Kunchang Li, Xinhao Li, Jiashuo Yu, Yinan He, Guo Chen, Baoqi Pei, Rongkun Zheng, Zun Wang, Yansong Shi, et al. Internvideo2: Scaling foundation models for multimodal video understanding. In European Conference on Computer Vision, pages 396416. Springer, 2024. 2 [63] Chao-Yuan Wu, Yanghao Li, Karttikeya Mangalam, Haoqi Fan, Bo Xiong, Jitendra Malik, and Christoph Feichtenhofer. Memvit: Memory-augmented multiscale vision transformer for efficient long-term video In Proceedings of the IEEE/CVF Conrecognition. ference on Computer Vision and Pattern Recognition, pages 1358713597, 2022. 3 [64] Haoning Wu, Dongxu Li, Bei Chen, and Junnan Li. Longvideobench: benchmark for long-context interleaved video-language understanding. arXiv, 2024. 3 [65] Penghao Wu and Saining Xie. V*: Guided visual search as core mechanism in multimodal llms, 2023. 2 [66] Junbin Xiao, Xindi Shang, Angela Yao, and Tat-Seng Chua. Next-qa: Next phase of question-answering to In Proceedings of the explaining temporal actions. IEEE/CVF conference on computer vision and pattern recognition, pages 97779786, 2021. 1, 2, 3 [67] Xuehan Xiong, Anurag Arnab, Arsha Nagrani, and Cordelia Schmid. M&m mix: multimodal multiview transformer ensemble, 2022. [68] Mingze Xu, Mingfei Gao, Zhe Gan, Hong-You Chen, Zhengfeng Lai, Haiming Gang, Kai Kang, and Afshin Dehghan. Slowfast-llava: strong training-free baseline for video large language models. arXiv preprint arXiv:2407.15841, 2024. 2 [69] Hanrong Ye, De-An Huang, Yao Lu, Zhiding Yu, Wei Ping, Andrew Tao, Jan Kautz, Song Han, Dan Xu, Pavlo Molchanov, et al. X-vila: Cross-modality alignment for large language model. arXiv preprint arXiv:2405.19335, 2024. 2 [70] Hanrong Ye, Haotian Zhang, Erik Daxberger, Lin Chen, Zongyu Lin, Yanghao Li, Bowen Zhang, Haoxuan You, Dan Xu, Zhe Gan, et al. Mm-ego: Towards building egocentric multimodal llms. arXiv preprint arXiv:2410.07177, 2024. 2 [71] Jiabo Ye, Haiyang Xu, Haowei Liu, Anwen Hu, Ming Yan, Qi Qian, Ji Zhang, Fei Huang, and Jingren Zhou. mplug-owl3: Towards long image-sequence understanding in multi-modal large language models. arXiv preprint arXiv:2408.04840, 2024. 8 [72] Zhou Yu, Dejing Xu, Jun Yu, Ting Yu, Zhou Zhao, Yueting Zhuang, and Dacheng Tao. Activitynet-qa: dataset for understanding complex web videos via question answering. arXiv, 2019. 1, 2 [73] Xiang Yue, Yuansheng Ni, Kai Zhang, Tianyu Zheng, Ruoqi Liu, Ge Zhang, Samuel Stevens, Dongfu Jiang, Weiming Ren, Yuxuan Sun, et al. Mmmu: massive multi-discipline multimodal understanding and reasoning benchmark for expert agi. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 95569567, 2024. 2 [74] Eric Zelikman, Yuhuai Wu, Jesse Mu, and Noah D. Goodman. Star: Bootstrapping reasoning with reasoning, 2022. [75] Xiaohua Zhai, Basil Mustafa, Alexander Kolesnikov, and Lucas Beyer. Sigmoid loss for language image pre-training, 2023. 5 [76] Hang Zhang, Xin Li, and Lidong Bing. Videollama: An instruction-tuned audio-visual language arXiv preprint model for video understanding. arXiv:2306.02858, 2023. 2 [77] Haotian Zhang, Mingfei Gao, Zhe Gan, Philipp Dufter, Nina Wenzel, Forrest Huang, Dhruti Shah, Xianzhi Du, Bowen Zhang, Yanghao Li, et al. Mm1. 5: Methods, analysis & insights from multimodal llm fine-tuning. arXiv preprint arXiv:2409.20566, 2024. 2 [78] Peiyuan Zhang, Kaichen Zhang, Bo Li, Guangtao Zeng, Jingkang Yang, Yuanhan Zhang, Ziyue Wang, Haoran Tan, Chunyuan Li, and Ziwei Liu. Long context transfer from language to vision. arXiv preprint arXiv:2406.16852, 2024. 8 [79] Shaolei Zhang, Qingkai Fang, Zhe Yang, and Yang Feng. Llava-mini: Efficient image and video large multimodal models with one vision token, 2025. 4, 7 [80] Yuanhan Zhang, Jinming Wu, Wei Li, Bo Li, Zejun Ma, Ziwei Liu, and Chunyuan Li. Llavavideo-178k: video instruction dataset. https: //huggingface.co/datasets/lmms-lab/ LLaVA-Video-178K, 2024. Accessed: 2024. 8 [81] Yuanhan Zhang, Jinming Wu, Wei Li, Bo Li, Zejun Ma, Ziwei Liu, and Chunyuan Li. Video instruction tuning with synthetic data. arXiv preprint arXiv:2410.02713, 2024. 1, 2, 4, 6, [82] Zhuosheng Zhang, Aston Zhang, Mu Li, Hai Zhao, George Karypis, and Alex Smola. Multimodal chainof-thought reasoning in language models, 2024. 2, 4 12 [83] Yue Zhao and Philipp Krahenbuhl. Training large video model on single machine in day. arXiv preprint arXiv:2309.16669, 2023. 2, 3, 7 [84] Yue Zhao, Ishan Misra, Philipp Krahenbuhl, and Rohit Girdhar. Learning video representations from large language models, 2022. 7 [85] Yue Zhao, Ishan Misra, Philipp Krahenbuhl, and Rohit Girdhar. Learning video representations from large In Proceedings of the IEEE/CVF language models. Conference on Computer Vision and Pattern Recognition, pages 65866597, 2023. 3 [86] Deyao Zhu, Jun Chen, Xiaoqian Shen, Xiang Li, and Mohamed Elhoseiny. Minigpt-4: Enhancing visionlanguage understanding with advanced large language models. arXiv preprint arXiv:2304.10592, 2023. 1 LLaVAction: evaluating and training multi-modal large language models for action recognition"
        },
        {
            "title": "Appendix",
            "content": "7. Methods: Licensing information Code/Dataset License Apache-2.0 MIT CC-NC-SA-4.0 LLaVA-NeXT AVION TIM EPIC-KITCHENS-100 Dataset 55 and extended NC-Government EPIC-KITCHENS-100 Annotations lmms-eval CC-NC-SA-4.0 MIT and Apache-2. Table 6. List of codes and datasets with their corresponding licenses. 8. Methods: GPT-4o distillation. Due to the cost, we sample 4 frames per annotated video clip to go over the training set of EPIC-KITCHENS-100. We first get the caption corresponding to all video clips in the training set, and then we use the captions obtained to create open-ended question-answers. We show the corresponding prompts for the generations of captions and openended question-answers as follows. 8.1. GPT-4o and GPT-4o-mini annotation prompt for the caption task. You are viewing video frames from an egocentric perspective and you are the person. Describe the video frames in detail and reason about the actions you are performing. You will be provided with the human-annotated ground-truth for the action, but you should independently come to your own conclusion. If you disagree with the human annotation, indicate \"true\" in the \"disagree with human annotation\" field of your response, and provide your reasoning without mentioning the ground-truth answer. This will keep your reasoning clean. If you agree with the human annotation, indicate \"false\" in the \"disagree with human annotation\" field 2 The and provide your reasoning without referencing the ground-truth to maintain clean description. true ground-truth action is {gt answer}. Your reasoning steps should include supporting evidence for the action, such as the duration of the video, the sequence of actions the person performs, the objects they interact with, and the overall context of the video. As general guideline, for videos longer than 3 seconds, provide detailed reasoning steps, and for videos shorter than 3 seconds, generate less detailed reasoning. {end second - start second:.3f} seconds. Make sure you use the first-person perspective in your reasoning. The video duration is 8.2. GPT-4o and GPT-4o-mini annotation prompt for Open-ended question answering Your job is to create 3 question-answer pairs based on the text below. The text contains first-person narrative of video frames from an egocentric perspective of person interacting with objects in kitchen. caption text You can ask questions such as: What object am interacting with? What objects are visible in the video? What is the sequence of the atomic actions am performing? your questions can be answered based on the information provided in the text. Do not ask questions that require additional context or information beyond what is given. Make sure 9. Methods: LLaVAction task prompts 9.1. LLaVAction caption prompt Describe in details what you see from the video frames. you are doing. Try to focus on what 9.2. LLaVAction perspective prompt Egocentric. For the EgoSchema benchmark, given that our LLaVAction-7B is trained with egocentric perspective prompt on EPIC-KITCHENS-100, we use the same egocentric perspective prompt when we evaluate our model on EgoSchema benchmark. You are seeing this video from egocentric view and you are the person. Your hands are sometimes interacting with objects. What action are you doing? Allocentric. The video is taken from egocentric The persons hands are sometimes view. interacting with objects. What action is the person doing? 9.3. LLaVAction temporal detection prompt The provided video contains an action {ACTION NAME} that lasts 2.96 seconds. What is the relative start and end time of the action in seconds? Format it as start timestamp: end timestamp and round to 2 decimal places. 9.4. LLaVAction direct prediction prompt What action are you performing? Give short sentence such as move knife. 9.5. LLaVAction prior action learning prompt {prev2 offset} seconds ago, you started an action {prev2 narration}. {prev1 offset} seconds ago, you started an action {prev1 narration}. What action are you currently performing? Here are the options of actions you can select: 10. Leave-one-out Ablation Remarkably, the 10-point gain over our baseline model cannot be attributed to only single factor. We took our full model, i.e., the base plus all added methods, which we call LLaVAction-7B, and performed leave-on-out ablation  (Table 7)  . Given our additions adds negligible overhead in the inference time (only one special vision token added to the baseline model), we then suggest using our full LLaVAction-7B and techniques in downstream tasks. 11. Additional Benchmarks Descriptions. EgoSchema. Mangalam et al. [39] presented long-form video question-answering benchmark to evaluate long video understanding capabilities of modern vision and language LLaVA-Video-7B Full (LLaVAction-7B) Full w/o AVION distractors Full w/o vision supervision Full w/o temporal detection Full w/o gpt4o distillation Full w/o direct prediction Full w/o learning prior actions Acc. 73.4 68.2 73.1 72.1 73.2 72.8 72. Table 7. Leave-one-out ablation study. Full denotes having all the proposed methods. In each row we drop one method from the full method and report the resulted performance. 16 frames were used for both training and testing, and percent accuracy is shown. systems. The data of EgoSchema come from Ego4D and are adapted into MQA format. EgoSchema consists of over 5000 human curated multiple choice question answer pairs, spanning over 250 hours of real video data. For each question, EgoSchema requires the correct answer to be selected between five given options based on three-minute-long video clip. MVBench. Li et al. [27] presented comprehensive Multi-modal Video understanding benchmark designed to evaluate temporal comprehension in dynamic video tasks. It encompasses 20 challenging tasks that require temporal reasoning beyond static frames, ranging from perception to cognition. The benchmark systematically transforms static image tasks into dynamic video tasks, enabling the assessment of models temporal understanding capabilities. VideoMME & VideoMME with subtitles. Fu et al. [17] presented benchmark designed to evaluate multimodal large language models (MLLMs) on video understanding tasks. It includes tasks that assess models abilities to comprehend and reason about video content. The with subs variant incorporates subtitles, providing additional textual context to evaluate models performance with supplementary information. LongVideoBench. Wu et al. [64] present questionanswering benchmark featuring video-language interleaved inputs up to an hour long. It comprises 3,763 videos with subtitles across diverse themes, designed to evaluate models long-term multimodal understanding. The benchmark emphasizes referring reasoning, requiring models to retrieve and reason over detailed information from extended video contexts. NextQA. Xiao et al. [66] presented benchmark for assessing video question-answering capabilities, focusing on temporal and causal reasoning. It includes questions that require understanding the temporal order of events and inferring causal relationships within videos. PerceptionTest. Patraucean et al. [42] presented benchmark that evaluates models understanding of dy3 namic video content, focusing on temporal perception and cognition. It includes tasks that require models to interpret and reason about changes and events occurring over time in videos. Evaluation. We employ the commonly used validation code base, available at: https://github.com/ EvolvingLMMs-Lab/lmms-eval. 12. Qualitative examples 12.1. Comparing narrations and official keys in EPIC-KITCHENS-100 Qualitatively: we illustrate some examples of choices represented in the official key manner (Appendix Figure 4). We show the ground truth option in blue and the prediction of LLaVAction-7B in pink. We can see LLaVAction-7Bs predictions also make sense in those examples and hence cause ambiguity across choices. Instead, the corresponding narration fits better to the language nature and can better describe the video content with less ambiguity. Quantitatively: furthermore, we also quantify MLLMs zero-shot performance (LLaVA-OV-0.5B, LLaVA-OV-7B, LLaVA-Video-7B) when using official keys or narrations as inputs  (Table 8)  . The inferior zero-shot performance of all 3 evaluated models when tested on the official keys as action representation supports our qualitative observations that official keys are less ideal than narrations for MLLMs. 12.2. Different choices comparison Here, we show examples of choices generated by random sampling, AVION top-5 predictions, and TIM top-5 predictions (Appendix Figure 5). We can see that the randomly selected choices have many trivial choices that can be easily distinguished with the correct answer. In comparison, choices generated based on AVION and TIM top-5 predictions become much more similar to the correct answer and exhibit features such as similar object/scene, temporal orders or object relationships that are emphasized by other benchmarks. 12.3. LLaVAction Caption Here, we show one video caption example of different models including GPT-4o, LLaVA-Video-7B and our LLaVAction-7B (Appendix Figure 6). We can see the interacting object (pizza piece) is pretty small in the video and there are also many other distracting objects. Both GPT4o and LLaVA-Video-7B cause hallucinations in their descriptions. For example, GPT-4o thinks the person holds the slice with both hands. Instead, LLaVAction-7B still retains the video caption ability and can generate plausible descriptions of the video. 13. Sub-category performance comparisons on the additional benchmarks Snice MVBench and LongVideoBench also have subcategory measurements with many of them related to action understanding, we also show the sub-category performances on these two benchmarks in this section. 13.1. Performance comparison on sub-categories of"
        },
        {
            "title": "MVBench",
            "content": "Here we show the performance comparison between LLaVA-Video-7B and our LLaVAction-7B on subcategories of MVBench. We can see LLaVAction-7B boost the performance on many action-related categories such as action count, action sequence and fine-grained action, etc. 13.2. Performance comparison on sub-categories of"
        },
        {
            "title": "LongVideoBench",
            "content": "Here we show the performance comparison between LLaVA-Video-7B and our LLaVAction-7B on subcategories of LongVideoBench. We can see LLaVAction7B also boosts the performance on many action-related categories such as event before/after, text-referred object attribute, and object-before/after object. 13.3. Performance comparison on sub-categories of"
        },
        {
            "title": "VideoMME",
            "content": "Here we show the performance comparison between LLaVA-Video-7B and our LLaVAction-7B on subimprove categories of VideoMME. Our model did not the action recognition performance on VideoMME possibly due to the domain gap between VideoMME and EPIC-KITCHENS-100. 14. Extended Discussion Alternative approaches we tested. We also tested few alternative approaches to improve MLLMs in our benchmark. We tried self-consistency predictions, which do not yield improvements, perhaps due to the task being vision-centric. Additionally, we explored multi-modal chain-of-thought (COT) reasoning by prompting the model to generate caption prior to addressing the multi-question answering task. However, we found that the model exhibited reluctance to perform this action, despite being capable of generating captions or answering multichoice questions independently. variant of it is to inference the model twice, so we have the caption first and feed that into the instruction of answering multi-choice question task, similar to [82]. While minor improvement was observed, we think it is not worth the 2X compute. We believe that video action recognition is good way to explore video reasoning for MLLMs. However, we leave COT improvements on this task for future work."
        },
        {
            "title": "Official key Narration",
            "content": "zero-shot LLaVA-Video-7B zero-shot LLaVA-OV-7B zero-shot LLaVA-OV-0.5B 26.5 19.6 24.8 35.7 28.9 32.0 Table 8. Quantitative results for official keys vs. narrations. Models are inferred with eight frames as inputs."
        },
        {
            "title": "Tasks",
            "content": "LLaVA-Video-7B LLaVAction-7B (Ours) Difference Action antonym Action count Action localization Action prediction Action sequence Character order Counterfactual inference Egocentric navigation Episodic reasoning Fine-grained action Fine-grained pose Moving attribute Moving count Moving direction Object existence Object interaction Object shuffle Scene transition State change Unexpected action 76.0 57.0 61.0 62.0 70.5 74.5 50.0 30.5 53.5 48.0 54.5 71.0 44.0 35.5 60.0 84.5 41.5 93.5 54.0 81.5 75.0 65.0 63.5 59.0 72.5 79.0 52.0 28.0 54.0 49.0 61.5 72.5 43.0 31.0 59.0 83.5 44.0 90.5 61.5 79.0 -1.0 8.0 2.5 -3.0 2.0 4.5 2.0 -2.5 0.5 1.0 7.0 1.5 -1.0 -4.5 -1.0 -1.0 2.5 -3.0 7.5 -2.5 Table 9. Sub-category comparison with LLaVA-Video-7B on MVBench. Tasks LLaVA-Video-7B LLaVAction-7B (Ours) Difference Event-Referred object Event-Before/after event Object-Referred event Object-Before/after object Scene-Referred object attribute Scene-Referred event Scene-Referred object Scene-Referred object attribute change Scene-Referred object tracking Sequence of scenes Text-Referred object attribute Text-Referred event Text-Referred object Event before/after text Object before/after text Text-Referred object attribute change Text-Referred object tracking 72.31 67.02 67.82 57.58 71.59 72.04 63.89 55.56 65.43 41.24 59.49 56.92 59.21 50.68 58.11 47.56 32.88 69.23 67.02 64.37 59.09 70.46 66.67 63.89 52.78 66.67 41.24 62.02 56.92 59.21 58.90 52.70 50.00 32.88 -3.08 0.0 -3.45 1.52 -1.14 -5.38 0.0 -2.78 1.23 0.0 2.53 0.0 0.0 8.22 -5.41 2.44 0. Table 10. Sub-category comparison with LLaVA-Video-7B on LongVideoBench."
        },
        {
            "title": "Tasks",
            "content": "LLaVA-Video-7B LLaVAction-7B (Ours) Difference Categories: Artistic Performance Categories: Film & Television Categories: Knowledge Categories: Life Record Categories: Multilingual Categories: Sports Competition Task Categories: Action Reasoning Task Categories: Action Recognition Task Categories: Attribute Perception Task Categories: Counting Problem Task Categories: Information Synopsis Task Categories: OCR Problems Task Categories: Object Reasoning Task Categories: Object Recognition Task Categories: Spatial Perception Task Categories: Spatial Reasoning Task Categories: Temporal Perception Task Categories: Temporal Reasoning Video Sub Categories: Acrobatics Video Sub Categories: Animation Video Sub Categories: Astronomy Video Sub Categories: Athletics Video Sub Categories: Basketball Video Sub Categories: Biology & Medicine Video Sub Categories: Daily Life Video Sub Categories: Documentary Video Sub Categories: Esports Video Sub Categories: Exercise Video Sub Categories: Fashion Video Sub Categories: Finance & Commerce Video Sub Categories: Football Video Sub Categories: Geography Video Sub Categories: Handicraft Video Sub Categories: Humanity & History Video Sub Categories: Law Video Sub Categories: Life Tip Video Sub Categories: Literature & Art Video Sub Categories: Magic Show Video Sub Categories: Movie & TV Show Video Sub Categories: Multilingual Video Sub Categories: News Report Video Sub Categories: Other Sports Video Sub Categories: Pet & Animal Video Sub Categories: Stage Play Video Sub Categories: Technology Video Sub Categories: Travel Video Sub Categories: Variety Show 68.9 71.7 76.0 71.7 67.8 65.6 69.5 69.0 83.8 48.1 87.0 73.4 73.6 75.7 68.5 82.1 76.4 51.4 65.6 58.9 77.8 66.7 54.4 78.9 78.9 74.4 62.2 58.9 68.9 80.0 72.2 76.7 77.8 66.7 82.2 70.0 80.0 62.2 68.9 67.8 84.4 72.2 78.9 82.2 72.2 75.6 65.6 69.4 72.8 75.4 71.7 61.1 66.4 69.1 68.4 83.3 46.3 86.7 74.1 73.3 77.1 72.2 82.1 78.2 52.0 64.4 60.0 77.8 73.3 51.1 78.9 75.6 76.7 60.0 67.8 70.0 80.0 75.6 75.6 76.7 67.8 80.0 67.8 73.3 65.6 70.0 61.1 84.4 72.2 78.9 80.0 77.8 77.8 67.8 0.5 1.1 -0.6 0.0 -6.7 0.8 -0.4 -0.6 -0.5 -1.8 -0.3 0.7 -0.3 1.4 3.7 0.0 1.8 0.6 -1.2 1.1 0.0 6.6 -3.3 0.0 -3.3 2.3 -2.2 8.9 1.1 0.0 3.4 -1.1 -1.1 1.1 -2.2 -2.2 -6.7 3.4 1.1 -6.7 0.0 0.0 0.0 -2.2 5.6 2.2 2.2 Table 11. Sub-category comparison with LLaVA-Video-7B on VideoMME. 6 Figure 4. Official keys vs. narrations. Blue option denotes ground truth and the pink option denotes LLaVAction-7Bs prediction. Official keys usually reduce multiple nouns into one noun, resulting in ambiguity that could mislead MLLM. Note that the narration also contains crucial particles with the phrasal verbs to clarify the meaning such as put down, put into. Figure 5. TIMs choices are harder than AVION and random by introducing more visually similar objects and actions. Bold option denotes ground truth. Figure 6. The captioning capability can provide insights into models. comparison among LLaVAction-7B, GPT-4o and LLaVAVideo-7B. Although mainly trained with MQA data, our LLaVAction-7B still retains the video caption ability and can generate plausible descriptions of the video. 7 Figure 7. Breakdown of the performance of LLaVAction-7B on Verbs and Nouns. We analyzed the accuracy per verb and noun in EPIC-KITCHENS-100 for that our LLaVAction-7B (32f), evaluated on the validation set with official keys (i.e., the model reported in Table 4 that achieved 58.3 accuracy). There are more nouns than verbs, thus nouns are shown across four subplots for visualization but otherwise are not separated in an intentional way. The number above each bar is the total per class."
        }
    ],
    "affiliations": [
        "EPFL"
    ]
}