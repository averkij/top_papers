{
    "paper_title": "Time Blindness: Why Video-Language Models Can't See What Humans Can?",
    "authors": [
        "Ujjwal Upadhyay",
        "Mukul Ranjan",
        "Zhiqiang Shen",
        "Mohamed Elhoseiny"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Recent advances in vision-language models (VLMs) have made impressive strides in understanding spatio-temporal relationships in videos. However, when spatial information is obscured, these models struggle to capture purely temporal patterns. We introduce $\\textbf{SpookyBench}$, a benchmark where information is encoded solely in temporal sequences of noise-like frames, mirroring natural phenomena from biological signaling to covert communication. Interestingly, while humans can recognize shapes, text, and patterns in these sequences with over 98% accuracy, state-of-the-art VLMs achieve 0% accuracy. This performance gap highlights a critical limitation: an over-reliance on frame-level spatial features and an inability to extract meaning from temporal cues. Furthermore, when trained in data sets with low spatial signal-to-noise ratios (SNR), temporal understanding of models degrades more rapidly than human perception, especially in tasks requiring fine-grained temporal reasoning. Overcoming this limitation will require novel architectures or training paradigms that decouple spatial dependencies from temporal processing. Our systematic analysis shows that this issue persists across model scales and architectures. We release SpookyBench to catalyze research in temporal pattern recognition and bridge the gap between human and machine video understanding. Dataset and code has been made available on our project website: https://timeblindness.github.io/."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 0 3 ] . [ 1 7 6 8 4 2 . 5 0 5 2 : r Time Blindness: Why Video-Language Models Cant See What Humans Can? Ujjwal Upadhyay, Mukul Ranjan, Zhiqiang Shen Mohamed Elhoseiny King Abdullah University of Science and Technology (KAUST) Mohamed bin Zayed University of AI (MBZUAI) ujjwalupadhyay8@gmail.com {mohamed.elhoseiny}@kaust.edu.sa {mukul.ranjan,zhiqiang.shen}@mbzuai.ac.ae"
        },
        {
            "title": "Abstract",
            "content": "Recent advances in visionlanguage models (VLMs) have made impressive strides in understanding spatio-temporal relationships in videos. However, when spatial information is obscured, these models struggle to capture purely temporal patterns. We introduce SpookyBench, benchmark where information is encoded solely in temporal sequences of noise-like frames, mirroring natural phenomena from biological signaling to covert communication. Interestingly, while humans can recognize shapes, text, and patterns in these sequences with over 98% accuracy, state-of-the-art VLMs achieve 0% accuracy. This performance gap highlights critical limitation: an over-reliance on frame-level spatial features and an inability to extract meaning from temporal cues. Furthermore, when trained in data sets with low spatial signal-to-noise ratios (SNR), temporal understanding of models degrades more rapidly than human perception, especially in tasks requiring fine-grained temporal reasoning. Overcoming this limitation will require novel architectures or training paradigms that decouple spatial dependencies from temporal processing. Our systematic analysis shows that this issue persists across model scales and architectures. We release SpookyBench to catalyze research in temporal pattern recognition and bridge the gap between human and machine video understanding. Dataset and code has been made available on our project website: https://timeblindness.github.io/ 1. Introduction Large multimodal models have revolutionized visual understanding in both images [3, 16, 17, 20, 22, 44, 45, 68, 75, 82] and videos [1, 48, 49, 63, 72, 73, 87]. Recent VideoVision Language Models (Video-VLMs) demonstrate im- *Equal contribution; author order decided by coin toss. 1 pressive capabilities in various tasks, from action recognition [31, 76, 88] and visual question answering [2, 52, 55, 84, 89] to dense captioning [9, 13, 14, 32, 57, 78, 80] and temporal grounding [11, 67, 79]. Despite this rapid progress, fundamental limitation persists. These models excel at extracting spatial features from individual frames, but struggle with purely temporal reasoning [7, 19, 39], capability that comes naturally to humans. This paper introduces SpookyBench, novel benchmark designed to isolate and evaluate purely temporal understanding in video models by presenting information exclusively through temporal sequences where individual frames appear as noise. Although existing benchmarks test temporal reasoning alongside spatial understanding [7, 37, 40, 83], SpookyBench differs by completely eliminating spatial cues, forcing models to derive meaning solely from changes across frames. Current approaches to video understanding [53, 65] typically follow hierarchical paradigm: extract frame-level features using ViTs [4, 23, 59], integrate these features temporally, and fuse them with language for downstream tasks [38, 69, 75, 86]. This paradigm has yielded significant advances in general video understanding [24, 36, 53, 65]. However, our findings reveal critical blind spot: when information exists purely in the temporal domain without reliable frame-level features, state-of-the-art models fail catastrophically (Figure 1). The inability to decode temporal patterns has significant implications for real-world applications. In nature, organisms such as fireflies communicate through precise temporal sequences of bioluminescence [8, 54, 60], encoding information exclusively through timing rather than spatial arrangements. These natural examples demonstrate how temporal patterns can carry rich information even when individual observations contain minimal static content. Similarly, various human technologies from Morse code to digital communication protocols rely on temporal encoding, yet current Video-VLMs lack the fundamental mechanisms to process such information. The human visual system has evolved Figure 1. Illustration of the current video-language models limitations: over reliance on spatial visual features within individual frame. Frame sampling results in temporal information loss, while the visual encoder exhibits strong spatial bias. This creates coherence gap () between well-represented spatial features (objects, scene layouts) and under-represented temporal features (motion patterns, causality), limiting video understanding capabilities. mechanisms for processing temporal information without relying solely on spatial cues. Neuroscience research has revealed that temporal processing is distributed across neural structures rather than centralized in single area [50], and the brain uses intrinsic network dynamics to perform temporal computations [56]. Areas such as the parietal cortex integrate temporal information along with spatial and numeric magnitudes [6]. Our experiments confirm humans remarkable temporal perception: participants achieve over 98% accuracy on SpookyBench tasks without training. In stark contrast, our evaluation of 15 state-of-the-art VideoVLMs, including closed-source commercial systems such as GPT-4o [29], and Gemini 2.0 Flash [21], reveals near-zero accuracy on these same tasks. This striking performance gap persists across model architectures, parameter scales, and pre-training strategies. Models ranging from relatively compact systems (VideoLLaMA3-2B [85]) to massive ones (GPT-4o [29], Qwen-VL [68]) all struggle with purely temporal patterns. Even models specifically designed for video understanding such as LongVLM [73], LLaVA-NeXT-Interleave [38], and InternVideo2.5 [72] exhibit minimal temporal pattern recognition capability. Recent efforts to enhance temporal reasoning in VideoVLMs have explored various approaches. Models like TimeChat [62], Momentor [58], and VideoLLM [71] incorporate specialized temporal modeling mechanisms, while ST-LLM [47], TimeMaker [11], and Grounded-VideoLLM [67] focus on enhancing fine-grained temporal localization capabilities. However, our evaluation reveals that none of these approaches adequately addresses the fundamental challenge of extracting meaning from purely temporal patterns without reliable spatial features. Contributions: Our work offers the following key contributions: We introduce SpookyBench, novel benchmark that isolates temporal understanding using controlled data generation process (Algorithms 1 and 2) that encodes four different types of information: text information, binary shapes information, depth map of single object image, and depth of single object video. We perform comprehensive evaluations of 15 state-of-theart open-source and closed-source Video-VLMs, revealing that even the most advanced models (including GPT-4o, InternVL-2, and Qwen2.5-VL) achieve zero accuracy on tasks that humans solve effortlessly. We identify key architectural limitations in current VideoVLMs that hinder human-like temporal understanding, drawing connections to relevant insights from cognitive science and signal processing. Our findings suggest that achieving human-like video understanding requires fundamentally rethinking how neural architectures process temporal information. Rather than treating temporal integration as secondary to spatial feature extraction, future models may need dedicated mechanisms for temporal pattern recognition, possibly drawing inspiration from cognitive neuroscience research on distributed neural timing mechanisms [50, 56] and specialized brain regions for temporal processing [6, 51]. The substantial gap between human and machine performance on SpookyBench indicates that current architectures remain fundamentally time-blind despite their impressive performance on standard benchmarks. By exposing this critical limitation, we hope to inspire new wave of research into temporal reasoning in Video-VLMs, bridging the gap between human and machine perception and enabling applications that rely on precise temporal understanding, from medical diagnostics to autonomous systems that must interpret subtle temporal cues in complex environments. 2. Related Work 2.1. Video-Vision Language Models Video-Vision Language Models (Video-VLMs) have evolved rapidly through transformer-based architectures [4, 23, 59] Figure 2. Illustration of the temporal encoding framework used in SpookyBench. Left: Core mechanism showing how content becomes visible through opposing motion patterns. content mask defines regions where foreground noise (moving up/left) and background noise (moving down/right) are applied. When animated, the human visual system groups pixels with similar motion, causing the content to emerge. Right: Comparison between moving and paused states, demonstrating how content is only perceptible during animation and disappears when static, as individual frames contain only structured noise. that integrate temporal understanding capabilities. Three major families dominate this landscape: LLaVA variants [36, 38, 44, 46, 48, 87] adapted from image-only predecessors; the Qwen series [3, 68] with innovations in dynamic resolution processing; and InternVL models [16, 17, 72] leveraging large-scale pretraining. Alternative architectures include dual encoder approaches in VideoGPT+ [49], interleaved tokens in MiniGPT4-Video [1, 90], compression techniques in LongVU [63], and multimodal fusion in MM1.5 [86], Janus [75], and EMU3 [69]. Despite architectural advances, Video-VLMs consistently demonstrate limited temporal reasoning with hallucination problems [37], grounding difficulties [67], and reliance on linguistic shortcuts [33] across action recognition [31, 76, 88], question answering [2, 52, 84], and captioning tasks [9, 32, 80]. Recent efforts to address temporal limitations include timestamp-aware encoding [62], segment-level reasoning [58], direct token processing [47], temporal separation tokens [11], and dedicated temporal streams [67, 71]. Novel training paradigms using synthetic data [87], self-chained localization [84], and structured sequence modeling [53, 65] have shown promise but still fundamentally rely on spatial feature extraction with temporal reasoning treated as secondary. Even specialized video-specific architectures like VideoGPT+[49], TimeChat [62], LinVT [27], LongVLM [73], and Baichuan-Omni [42] maintain this spatial-first paradigm. This fundamental limitation suggests that bridging the gap between human perception and machine capabilities requires architectural innovations. Such innovations must treat temporal understanding as primary component of video representation, rather than secondary one. 2.2. Benchmarks for Temporal Understanding The evolution of temporal understanding benchmarks reveals mounting evidence of fundamental limitations in current Video-VLMs. TemporalBench [7] directly measures fine-grained temporal capabilities, exposing significant gap between state-of-the-art models and human performance. TVBench [19] demonstrates that existing datasets inadvertently reward spatial analysis over genuine temporal reasoning, findings echoed by similar observations in VITATECS [40] and Fateh et al. [26]. Focused evaluations addressing specific temporal capabilities include VidHalluc [37] for temporal hallucinations and SVBench [83] for streaming video reasoning. VideoVista [41] has further highlighted limitations in handling temporal location, object tracking, and anomaly detection. Critical analysis of these benchmarks reveals consistent pattern: models exploit spatial shortcuts to circumvent genuine temporal reasoning [11, 33, 37, 67]. Even models designed specifically for video tasks, including LLaVAVideo [87], Video-ChatGPT [48], TemporalVLM [26], and VidChain [35], perform adequately by leveraging static visual features while minimally integrating temporal information. Our SpookyBench benchmark fundamentally differs by isolating temporal pattern recognition through deliberately obscured spatial information, requiring models to derive meaning solely from temporal dynamics without relying on frame-level features. This approach creates rigorous evaluation targeting the time-blindness of current architectures, exposing limitations that remain hidden in conventional assessments that allow spatial shortcuts. 3 Algorithm 1 Content Mask Animation 1: Input: Content mask , velocity 2: Output: Animated frame Ft 3: Generate noise patterns Nbg, Nf 4: for each pixel (x, y) do 5: 6: if (x, y) then Ft(x, y) Nf g(x, + vt mod h)"
        },
        {
            "title": "Foreground\nelse",
            "content": "Ft(x, y) Nbg(x, vt mod h) 7: 8:"
        },
        {
            "title": "Background\nend if",
            "content": "9: 10: end for 2.3. Neuroscience Insights on Temporal Processing Neuroscience research offers critical insights for addressing temporal limitations in Video-VLMs. Mauk and Buonomano [50] established that temporal processing in the brain is distributed across multiple neural structures and implemented through intrinsic circuit properties rather than specialized timing mechanisms. This distributed approach contrasts sharply with current Video-VLMs sequential extension of spatial processing. Biological temporal processing spans various granularities: the cerebellum handles millisecond-tosecond timing for motion discrimination [51]; the parietal cortex integrates time with spatial and numerical magnitudes [6]; and neural activity patterns dynamically encode elapsed time through population clocks [56] rather than static representations. These neuroscience principles suggest concrete directions for next-generation Video-VLMs. Rather than treating temporal integration as secondary to spatial processing, models could benefit from explicitly encoding temporal dynamics through distributed representations that evolve over time [56, 74]. The stark performance gap between humans and machines on temporal tasks [7, 19, 37] indicates that incorporating these biological principles could lead to significant advances in temporal reasoning capabilities. Our findings with SpookyBench further underscore this need by demonstrating that current architectures fundamentally lack the mechanisms for processing purely temporal patterns, capability that comes naturally to humans through neural systems that represent time not as dimension but as an intrinsic property of neural dynamics. 3. Methods We introduce SpookyBench, novel synthetic dataset specifically designed to isolate and evaluate pure temporal understanding in video language models. The key innovation of our benchmark lies in its unique design: All meaningful information is encoded exclusively in the temporal domain through dynamic patterns of texts, shapes, and video Figure 3. Noise generation process: (Top) masks applied for dynamic noise video generation, (Mid) word-specific mask, and (Bottom) depth map of video frame used for constructing noise-overlaid stimulus. depth maps, while individual frames contain only structured noise. Our dataset is fundamentally different from the existing datasets used for training, fine-tuning, and evaluation of video-VLMs. Many state-of-the-art video language models employ advanced techniques, such as dynamic resolution strategies [3, 16, 68], specialized temporal encoding methods [3, 62, 68], hierarchical token merging [72, 73], and joint video-motion training frameworks [10] to capture temporal dynamics. However, these methods still rely on spatial representations extracted from individual frames, which currently remain the only viable mechanism for inferring temporal information. In contrast, SpookyBench forces models to depend only on temporal cues, thereby creating the first benchmark that exclusively evaluates models ability to process and understand pure temporal information. 3.1. Dataset Generation The dataset consists of specially designed videos that encode four types of content - words, shapes, images, and videos - using binary noise patterns with specific motion properties. In this approach, content is embedded within noise patterns such that individual frames appear as random noise, while the content becomes perceptible only when viewed as temporal sequence. Our dataset encodes different types of content (Figure 3) through temporal noise animations in the following categories: Words: Text rendered as masks in which the background noise and foreground noise move in opposite directions, making the text visible only through temporal movement. Shapes: Basic geometric patterns (rectangles, circles, polygons) encoded using the same opposing motion technique as text. Images: Binary masks generated using SAM2 [61] from 4 Algorithm 2 Video Depth Map Animation 1: Input: Depth map D, thresholds (tl, tu), velocity 2: Output: Animated frame Ft 3: Generate noise pattern 4: for each pixel (x, y) do 5: brightness from depth map D(x, y) if tl tu then Ft(x, y) (x, + vt mod h) Moving noise 6: 7: else 8: 9: 10: 11: end for end if Ft(x, y) (x, y) Static noise single-object images generated using text-to-image model Flux [34], encoded using the same content mask animation approach as words and shapes. Dynamic Scenes: Depth maps extracted from videos in single-object tracking datasets LaSOT [25] and OTB2015 [77] using Video Depth Anything [12]. These are encoded using technique in which pixels above brightness threshold move while others remain static as shown in the algorithm 2. 3.2. Temporal Encoding Framework Our temporal encoding framework implements two distinct motion configurations as detailed in Algorithms 1 and 2. For words, shapes, and image masks (Algorithm 1), we employ opposing motion patterns between foreground and background. The content is first converted to binary mask where (x, y) = 1 represents foreground pixels and (x, y) = 0 represents background. We generate two separate noise patterns Nbg and Nf consisting of random binary values (0 or 255). During animation, foreground pixels sample from Nf with positive offset that increases with time (y + vt mod h), while background pixels sample from Nbg with negative offset (y vt mod h). This creates the perception of opposing motion within and outside the masked regions. For video depth maps (Algorithm 2), we employ threshold-based approach. Using depth maps extracted from videos, pixels with brightness values between lower and upper thresholds (tl tu) are animated by sampling noise pattern with time-varying offset (y + vt mod h), while pixels outside this range remain static. This creates the illusion that brighter regions (typically foreground objects) are moving while darker regions (typically background) remain static. The noise patterns are generated using binary values (0 or 255) in square blocks of variable size. We used different speckle sizes ranging from 1 1 to 3 3 pixels to investigate the effect of noise granularity on perception. For each speckle size, we also varied the noise density - Figure 4. Distribution of the SpookyBench dataset across four video categories. Each category represents different type of content encoded through temporal noise patterns: Text, Shapes, Object Images, and Dynamic Scenes. the probability that block is white versus black - using values of 10%, 30%, 50%, and 90%. These noise patterns arranged in pixel blocks create optimal perceptual conditions for human viewers while remaining challenging for vision language models. To ensure seamless animation, the noise patterns are made tileable by copying edge pixels to the opposite boundaries. All videos maintain consistent technical specifications: 960 540 pixel resolution, with an average duration of 7.11 seconds (ranging from 1.0 to 35.0 seconds) and an average of 333.5 frames per video. Text videos have consistent duration of around 4 seconds; however, videos of dynamic scenes are longer, ranging up to 35 seconds. Figure 2 illustrates the structure of the data set and the encoding patterns in categories. We used binary masks for the images using SAM2 [61]. For videos, depth maps are extracted using Depth Anything V2 [81] and Video Depth Anything [12] from the LaSOT [25] and OTB2015 [77] datasets. 3.3. Data Statistics SpookyBench comprises 451 videos in four distinct categories, each requiring purely temporal reasoning for content identification. As illustrated in Figure 4, the dataset is distributed as follows: Text (46.6%, 210 videos), Object Images (34.6%, 156 videos), Dynamic Scenes (12.6%, 57 videos) and Shapes (6.2%, 28 videos). This distribution ensures comprehensive coverage of different temporal perception challenges while maintaining natural frequency distribution that reflects real-world scenarios. Additionally, more dataset can be generated indefinitely through the data generator on our project page, thus the dataset size is essentially unlimited. The Text category contains common English words rendered through temporal noise patterns, enabling evaluation of models ability to identify linguistic content through purely temporal cues. The Object Images category presents single objects extracted from high-quality images using segmentation techniques [61], encoded with the same temporal animation approach. It also contains synthetic silhouette of simple objects generated using DALL-E 3 [5] and flux [34]. 3.3.1. Analysis of Temporal Metrics To ensure rigorous quantification of the temporal information present in each video, we analyzed five key SNR metrics that capture different aspects of the complexity and perceptibility of temporal patterns in SpookyBench, as shown in Table 1. These metrics provide insight into why temporal patterns might be visible to humans but challenging to detect by computational models. Basic SNR measures signal-to-noise ratio in decibels: SNRB = 10 log10 (cid:19) (cid:18) PS PN (1) where PS = E[F2] is motion boundary energy derived from spatial gradients of optical flow field F(x, y) = (Fx, Fy), and PN = Var(I0) is variance of the static frame I0. Perceptual SNR incorporates frequency-dependent visual sensitivity: SNRP = 10 log10 (cid:18) H(B) 2 H(N ) 2 (cid:19) (2) where is the average motion boundary strength, is the static noise frame, is the 2D Fourier transform, denotes element-wise multiplication, and (f ) = ef /f0 is the contrast sensitivity weighting function with peak f0 0.1 cycles/pixel. Temporal Coherence SNR quantifies motion consistency: SNRT = 10 log10 (cid:18) Var(C) (cid:19) E[Varlocal(C)] (3) where = eVarθ(F) 1(F > τ ) is the directional coherence map, Varθ computes circular variance of flow direction angles over time, 1 is indicator function, τ is magnitude threshold, and Varlocal computes variance over small spatial neighborhoods. Motion Contrast SNR measures foreground-background motion differentiation: SNRM = 10 log10 (cid:18) µM µB2 + σ2 B) 1 2 (σ2 (cid:19) (4) where µM = E[F ] and µB = E[F ] are mean flow vectors within mask region and background region respectively, σ2 = E[F µB2 ] are corresponding motion variances. The mask is estimated from the motion boundaries. = E[F µM 2 ] and σ2 Figure 5. Analysis of effects of SNR on detecting words with direct prompting and chain of thought prompting. The distribution of these metrics reveals why current vision models struggle with SpookyBench: they lack mechanisms to leverage temporal coherence (particularly high in Dynamic Scenes, 21.91 5.76 dB) and motion contrast (negative for Shapes and Dynamic Scenes, -2.20 and -3.18 dB), while text stimuli benefit from higher basic SNR (-39.27 1.58 dB), explaining the observed performance gap. 3.3.2. Binary SNR Threshold Effect in Detection Our analysis revealed critical binary threshold phenomenon in detecting text within dynamic noise videos. The words exhibited negligible detection (0%) below 2.5dB SNR, but jumped to 85.7% accuracy above this threshold, displaying an abrupt rather than gradual transition as show in 5. Prompts performed best (40% accuracy), with Chain-ofThought reasoning improving general identification tasks compared to direct prompting. This phenomenon parallels medical imaging diagnostics, where pathologies like microcalcifications in mammography become either entirely visible or invisible based on specific SNR thresholds. The implications are significant: unlike perceptual phenomena that degrade gradually with noise, text detection functions as step function, creating vulnerabilities in safety-critical applications. Just as radiologists cannot diagnose what remains invisible, language models cannot identify text below certain noise thresholds, leading to false certainties and potential catastrophic performance drops with minimal noise increases. This characteristic creates particular concerns for autonomous vehicles reading road signs or medical systems interpreting labels, while also exposing systems to adversarial attacks where slight SNR manipulations could render text completely undetectable. 3.3.3. Architectural Implications for Vision Models These distinctive signal profiles in SpookyBench demonstrate fundamental gap between human and machine per6 Category Object Images Shapes Dynamic Scenes Text Basic SNR (dB) Perceptual SNR Temporal Coherence Motion Contrast -46.53 2.81 -49.27 1.88 -48.95 3.64 -39.27 1.58 -46.97 2.67 -49.00 2.02 -63.43 5.74 -49.18 3. 8.16 2.44 7.10 1.48 21.91 5.76 7.84 0.65 8.85 5.87 -2.20 3.27 -3.18 10.17 8.26 6.44 Table 1. Signal-to-Noise Ratio (SNR) metrics across SpookyBench categories. ception of temporal information. Current vision models struggle with SpookyBench stimuli primarily because they: (1) lack robust temporal integration mechanisms that could leverage high temporal coherence, (2) process information primarily through spatial rather than temporal channels, and (3) fail to perform motion-based figure-ground segregation effectively. The remarkably high temporal coherence values in Dynamic Scenes, coupled with their poor static-frame metrics, suggest that successful models must implement recurrent processing or attention mechanisms that operate across extended temporal windows rather than focusing on frame-level feature extraction. The negative motion contrast observed in Shapes and Dynamic Scenes further indicates that models require more sophisticated motion segregation capabilities to match human perceptual abilities in dynamic visual environments. These findings highlight the need for architectural innovations that specifically address temporal processing limitations. Future models should incorporate dedicated temporal coherence pathways, motion contrast analysis, and longer temporal integration windows to bridge the perception gap demonstrated by SpookyBench. 4. Experiments 4.1. Experimental Setup Models. We evaluated SpookyBench on both open source models (Video-LLaVA [87], LLaVA-NeXT-Video [38], TimeChat [62], InternVL2 [16], Qwen2-VL [68], Qwen2.5-VL [3] etc.) and closed source models (GPT-4o [29], Gemini 2.0 Flash [21], and Gemini 1.5 Pro [66]. We design different prompts for each category. All the prompts are included in the Appendix A. All prompts instruct models to respond with only 1-5 words identifying the content. We input sequences of multiple video frames simultaneously for models that do not directly support video input. Setup. We evaluate model performance using exact match accuracy between model responses and our labels. For the Text and Shapes categories, each video has single correct label yi. For Object Images and Dynamic Scenes categories, we define set of acceptable labels Yi = {yi1, yi2, . . . , yin} to account for semantic ambiguity. For example, video showing man playing basketball accepts responses such as playing basketball, man, human, or woman playing basketball as correct. Formally, for each video i, given Model Human Performance Direct Prompt 98.0% 0.6 CoT N/A Params N/A VideoLLaMA3-7B [85] VideoLLaMA3-2B [85] TimeChat-7B [62] MiniGPT4-Video [1] MovieChat [64] Video-ChatGPT-7B [48] VideoGPT-plus-Phi3-mini-4k [49] VILA1.5-13b[43] ShareGPT4Video-8B [9] VideoLLaMA2-7B [18] Video-LLaVA [87] LLaVA-NeXT-Video [38] InternVL2-40B [16] InternVL2-8B [16] InternVL2.5-78B [15] InternVL2.5-8B [15] InternVideo2.5-Chat-8B [72] InternVideo2-Chat-8B [70] Qwen2-VL-2B-Instruct [68] Qwen2-VL-7B-Instruct [68] Qwen2-VL-72B-Instruct [68] Qwen2.5-VL-3B-Instruct [3] Qwen2.5-VL-7B-Instruct [3] Qwen2.5-VL-72B-Instruct [3] Open-Source Models 0% 0.0 0% 0.0 0% 0.0 0% 0.0 0% 0.0 0% 0.0 0% 0.0 0% 0.0 0% 0.0 0% 0.0 0% 0.0 0% 0.0 0% 0.0 0% 0.0 0% 0.0 0% 0.0 0% 0.0 0% 0.0 0% 0.0 0% 0.0 0% 0.0 0% 0.0 0% 0.0 0% 0.0 Closed-Source Models 0% 0.0 0% 0.0 0% 0.0 Gemini 1.5 Pro [66] Gemini 2.0 Flash[21] GPT-4o [29] 0% 0.0 0% 0.0 0% 0.0 0% 0.0 0% 0.0 0% 0.0 0% 0.0 0% 0.0 0% 0.0 0% 0.0 0% 0.0 0% 0.0 0% 0.0 0% 0.0 0% 0.0 0% 0.0 0% 0.0 0% 0.0 0% 0.0 0% 0.0 0% 0.0 0% 0.0 0% 0.0 0% 0.0 0% 0.0 0% 0.0 0% 0.0 7B 2B 7B 7B 7B 7B 7B 13B 8B 7B 7B 8B 40B 8B 78B 8B 8B 8B 2B 7B 72B 3B 7B 72B N/A N/A N/A Table 2. Benchmark results comparing model performance on SpookyBench across different prompting strategies along with model size. Human accuracy (98.0%) is the weighted average of accuracy across 4 different categories. model response ri and corresponding label or set of labels Li (where Li = yi for Text and Shapes, or Li = Yi for objects and dynamic scenes), we calculate the accuracy as: Accuracy = 1 (cid:88) i=1 1(ri Li) (5) where 1 is the indicator function that equals 1 if ri Li and 0 otherwise, and is the total number of videos in the evaluation set. Despite this flexible evaluation protocol that accepts multiple valid responses for certain categories, none of the models tested produced responses that matched any of the acceptable options. 4.2. Human Evaluation To evaluate human performance against our benchmark, we designed and conducted controlled experiment involving 7 Annotator Annotator 1 Annotator 2 Annotator 3 Annotator 4 Annotator 5 Annotator 6 Text Shapes Object Images Dynamic Scenes Acc(%) Perc(1-5) Acc(%) Perc(1-5) Acc(%) Perc(1-5) Acc(%) Perc(1-5) 99.5 98.6 99.5 97.6 100.0 98.0 4.7 4.8 4.9 4.6 4.8 4.7 100.0 100.0 96.4 96.4 100.0 96. 4.8 4.7 4.8 4.8 4.9 4.8 99.4 98.1 97.4 96.8 99.4 98.1 4.7 4.9 4.5 4.5 4.7 4.5 96.5 91.2 94.7 91.2 99.0 93.0 4.3 4.0 4.4 4.0 4.7 4.2 Mean 98.90.7 4.80.0 98.22.5 4.80.1 98.21.1 4.60. 94.33.1 4.30.1 Table 3. Human evaluation results showing accuracy and perceptibility ratings across different visual categories in SpookyBench. human participants. We recruited total of six human participants for this study, each independently evaluating all videos. Participants were instructed to view each video carefully and subsequently record their responses on an anonymized website in the following structured form: Perceptibility Rating (1-5): Participants rated how perceptible the presented word, shape, or object was, ranging from 1 (very difficult to perceive) to 5 (very clearly perceptible). This measure provided insights into the clarity and ease of visual grouping. Words/Shapes/Objects Identification: Participants typed out exactly what they identified in the video. This response directly tested the accuracy of their visual perception. We collect and evaluate participant responses using exact match criteria based on our predefined labels. Similar to the evaluation accuracy of the video language models for the categories of Object Images and Dynamic Scenes, we accepted multiple correct responses to avoid ambiguity. Table 3 shows the average precision and the perception rating of different annotators for different categories. The results show high human performance across all categories: participants correctly identified Words and Shapes with 98% accuracy, while Object Images had 92% accuracy. We also observe very high perceptibility rating (4.8 for texts and shapes and 4.3 and 4.0 for Object images and Dynamic scenes, respectively) across all four categories. This shows that the human brain can easily extract coherent information in videos, which seems to be very difficult for video language models. 5. Results and Discussion Table 2 presents the accuracy scores on the SpookyBench benchmark. Human participants achieved 98% accuracy under all test conditions. In contrast, all Video-VLMs scored 0% regardless of the type, size, or origin of the model. This pattern was held across all three task categories in our benchmark: temporal symbol recognition, temporal sequence understanding, and temporal pattern reasoning. We tested two different prompting strategies to determine if performance limitations could be overcome through interface modifications. First, we used direct prompts with basic instructions asking the models to identify content in the videos. Next, we implemented chain-of-thought prompts with explicit guidance to focus on temporal patterns rather than individual frames. As shown in Table 2, none of these approaches yielded improvements. All models maintained 0% accuracy across all prompting conditions, suggesting that the limitation is inherent in the model architectures rather than matter of optimization or prompt design. Examination of model output revealed consistent failure modes when processing SpookyBench videos. Across all models tested, we observed attempts to extract information from individual frames rather than temporal patterns. When explicitly prompted to consider temporal changes, the models acknowledged the instruction but still failed to identify the patterns. Fine-tuned models produced outputs that mimicked training examples without correctly identifying test patterns. In particular, specialized temporal models like TimeChat [62], which were specifically designed for finegrained temporal understanding, failed at the same rate as general-purpose models. This suggests that the limitation extends beyond general Video-VLMs to models explicitly optimized for temporal tasks. 6. Conclusion In this paper, we introduced SpookyBench, novel benchmark designed to evaluate the temporal reasoning capabilities of video-language models by isolating temporal understanding from spatial comprehension. Our experiments revealed striking performance gap: while humans effortlessly achieve 98% accuracy on tasks requiring pure temporal pattern recognition, all tested models, including state-of-the-art open and closed-source systems, fail completely with 0% accuracy. This consistent failure across different model architectures, scales, and prompting strategies highlights fundamental limitation in current video understanding approaches, which typically process spatial features first and then establish temporal connections, rather than integrating spatio-temporal information simultaneously. The benchmark effectively exposes the time blindness of current architectures that remain hidden in conventional evaluation settings where spatial features can provide shortcuts to correct answers. We hope that SpookyBench will inspire the development of next-generation temporal-connected models."
        },
        {
            "title": "References",
            "content": "[1] Kirolos Ataallah, Xiaoqian Shen, Eslam Abdelrahman, Essam Sleiman, Deyao Zhu, Jian Ding, and Mohamed Elhoseiny. Minigpt4-video: Advancing multimodal llms for video understanding with interleaved visual-textual tokens. arXiv preprint arXiv:2404.03413, 2024. 1, 3, 7 [2] Hammad Ayyubi, Junzhang Liu, Ali Asgarov, Zaber Ibn Abdul Hakim, Najibul Haque Sarker, Zhecan Wang, Chia-Wei Tang, Hani Alomari, Md Atabuzzaman, Xudong Lin, et al. Enter: Event based interpretable reasoning for videoqa. arXiv preprint arXiv:2501.14194, 2025. 1, 3 [3] Shuai Bai, Keqin Chen, Xuejing Liu, Jialin Wang, Wenbin Ge, Sibo Song, Kai Dang, Peng Wang, Shijie Wang, Jun Tang, et al. Qwen2. 5-vl technical report. arXiv preprint arXiv:2502.13923, 2025. 1, 3, 4, 7 [4] Gedas Bertasius, Heng Wang, and Lorenzo Torresani. Is space-time attention all you need for video understanding? In ICML, page 4, 2021. 1, 2 [5] James Betker, Gabriel Goh, Li Jing, Tim Brooks, Jianfeng Wang, Linjie Li, Long Ouyang, Juntang Zhuang, Joyce Lee, Yufei Guo, et al. Improving image generation with better captions. Computer Science. https://cdn. openai. com/papers/dall-e-3. pdf, 2(3):8, 2023. [6] Domenica Bueti and Vincent Walsh. The parietal cortex and the representation of time, space, number and other magnitudes. Philosophical Transactions of the Royal Society B: Biological Sciences, 364(1525):18311840, 2009. 2, 4 [7] Mu Cai, Reuben Tan, Jianrui Zhang, Bocheng Zou, Kai Zhang, Feng Yao, Fangrui Zhu, Jing Gu, Yiwu Zhong, Yuzhang Shang, et al. Temporalbench: Benchmarking finegrained temporal understanding for multimodal video models. arXiv preprint arXiv:2410.10818, 2024. 1, 3, 4 [8] Albert Carlson and Jonathan Copeland. Flash communication in fireflies. The Quarterly review of biology, 60(4): 415436, 1985. 1 [9] Lin Chen, Xilin Wei, Jinsong Li, Xiaoyi Dong, Pan Zhang, Yuhang Zang, Zehui Chen, Haodong Duan, Bin Lin, Zhenyu Tang, et al. Sharegpt4video: Improving video understanding and generation with better captions. arXiv preprint arXiv:2406.04325, 2024. 1, 3, 7 [10] Ling-Hao Chen, Shunlin Lu, Ailing Zeng, Hao Zhang, Benyou Wang, Ruimao Zhang, and Lei Zhang. Motionllm: Understanding human behaviors from human motions and videos. arXiv preprint arXiv:2405.20340, 2024. 4 [11] Shimin Chen, Xiaohan Lan, Yitian Yuan, Zequn Jie, and Lin Ma. Timemarker: versatile video-llm for long and short video understanding with superior temporal localization ability. arXiv preprint arXiv:2411.18211, 2024. 1, 2, 3 [12] Sili Chen, Hengkai Guo, Shengnan Zhu, Feihu Zhang, Zilong Huang, Jiashi Feng, and Bingyi Kang. Video depth anything: Consistent depth estimation for super-long videos. arXiv preprint arXiv:2501.12375, 2025. 5 [13] Tsai-Shien Chen, Aliaksandr Siarohin, Willi Menapace, Ekaterina Deyneka, Hsiang-wei Chao, Byung Eun Jeon, Yuwei Fang, Hsin-Ying Lee, Jian Ren, Ming-Hsuan Yang, et al. Panda-70m: Captioning 70m videos with multiple crossmodality teachers. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 1332013331, 2024. 1 [14] Xinlong Chen, Yuanxing Zhang, Chongling Rao, Yushuo Guan, Jiaheng Liu, Fuzheng Zhang, Chengru Song, Qiang Liu, Di Zhang, and Tieniu Tan. Vidcapbench: comprehensive benchmark of video captioning for controllable text-tovideo generation. arXiv preprint arXiv:2502.12782, 2025. 1 [15] Zhe Chen, Weiyun Wang, Yue Cao, Yangzhou Liu, Zhangwei Gao, Erfei Cui, Jinguo Zhu, Shenglong Ye, Hao Tian, Zhaoyang Liu, et al. Expanding performance boundaries of open-source multimodal models with model, data, and test-time scaling. arXiv preprint arXiv:2412.05271, 2024. 7 [16] Zhe Chen, Weiyun Wang, Hao Tian, Shenglong Ye, Zhangwei Gao, Erfei Cui, Wenwen Tong, Kongzhi Hu, Jiapeng Luo, Zheng Ma, et al. How far are we to gpt-4v? closing the gap to commercial multimodal models with open-source suites. Science China Information Sciences, 67(12):220101, 2024. 1, 3, 4, 7 [17] Zhe Chen, Jiannan Wu, Wenhai Wang, Weijie Su, Guo Chen, Sen Xing, Muyan Zhong, Qinglong Zhang, Xizhou Zhu, Lewei Lu, et al. Internvl: Scaling up vision foundation models and aligning for generic visual-linguistic tasks. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 2418524198, 2024. 1, 3 [18] Zesen Cheng, Sicong Leng, Hang Zhang, Yifei Xin, Xin Li, Guanzheng Chen, Yongxin Zhu, Wenqi Zhang, Ziyang Luo, Deli Zhao, et al. Videollama 2: Advancing spatialtemporal modeling and audio understanding in video-llms. arXiv preprint arXiv:2406.07476, 2024. 7 [19] Daniel Cores, Michael Dorkenwald, Manuel Mucientes, Cees GM Snoek, and Yuki Asano. Tvbench: Redesigning video-language evaluation. arXiv preprint arXiv:2410.07752, 2024. 1, 3, 4 [20] Wenliang Dai, Nayeon Lee, Boxin Wang, Zhuolin Yang, Zihan Liu, Jon Barker, Tuomas Rintamaki, Mohammad Shoeybi, Bryan Catanzaro, and Wei Ping. Nvlm: Open frontier-class multimodal llms. arXiv preprint arXiv:2409.11402, 2024. 1 [21] Google DeepMind. Gemini flash, 2025. Accessed: 2025-0224. 2, 7 [22] Matt Deitke, Christopher Clark, Sangho Lee, Rohun Tripathi, Yue Yang, Jae Sung Park, Mohammadreza Salehi, Niklas Muennighoff, Kyle Lo, Luca Soldaini, et al. Molmo and pixmo: Open weights and open data for state-of-the-art multimodal models. arXiv preprint arXiv:2409.17146, 2024. 1 [23] Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, et al. An image is worth 16x16 words: TransarXiv preprint formers for image recognition at scale. arXiv:2010.11929, 2020. 1, 2 [24] Abhimanyu Dubey, Abhinav Jauhri, Abhinav Pandey, Abhishek Kadian, Ahmad Al-Dahle, Aiesha Letman, Akhil Mathur, Alan Schelten, Amy Yang, Angela Fan, et al. The llama 3 herd of models. arXiv preprint arXiv:2407.21783, 2024. 1 [25] Heng Fan, Liting Lin, Fan Yang, Peng Chu, Ge Deng, Sijia Yu, Hexin Bai, Yong Xu, Chunyuan Liao, and Haibin Ling. Lasot: high-quality benchmark for large-scale single object tracking. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 53745383, 2019. 5 [26] Fawad Javed Fateh, Umer Ahmed, Hamza Khan, Zeeshan Zia, and Quoc-Huy Tran. Video llms for temporal reasoning in long videos. arXiv preprint arXiv:2412.02930, 2024. 3 [27] Lishuai Gao, Yujie Zhong, Yingsen Zeng, Haoxian Tan, Dengjie Li, and Zheng Zhao. Linvt: Empower your imagelevel large language model to understand videos. arXiv preprint arXiv:2412.05185, 2024. 3 [28] Jindong Gu, Zhen Han, Shuo Chen, Ahmad Beirami, Bailan He, Gengyuan Zhang, Ruotong Liao, Yao Qin, Volker Tresp, and Philip Torr. systematic survey of prompt engineering on vision-language foundation models. arXiv preprint arXiv:2307.12980, 2023. 13 [29] Aaron Hurst, Adam Lerer, Adam Goucher, Adam Perelman, Aditya Ramesh, Aidan Clark, AJ Ostrow, Akila Welihinda, Alan Hayes, Alec Radford, et al. Gpt-4o system card. arXiv preprint arXiv:2410.21276, 2024. 2, 7 [30] Woojeong Jin, Yu Cheng, Yelong Shen, Weizhu Chen, and Xiang Ren. good prompt is worth millions of parameters: Low-resource prompt-based learning for vision-language models. arXiv preprint arXiv:2110.08484, 2021. [31] Kumara Kahatapitiya, Anurag Arnab, Arsha Nagrani, and Michael Ryoo. Victr: Video-conditioned text representations for activity recognition. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 1854718558, 2024. 1, 3 [32] Minkuk Kim, Hyeon Bae Kim, Jinyoung Moon, Jinwoo Choi, and Seong Tae Kim. Do you remember? dense video captioning with cross-modal memory retrieval. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 1389413904, 2024. 1, 3 [33] Dohwan Ko, Ji Soo Lee, Wooyoung Kang, Byungseok Roh, and Hyunwoo Kim. Large language models are temporal and causal reasoners for video question answering. arXiv preprint arXiv:2310.15747, 2023. 3 [34] Black Forest Labs. Flux. https://github.com/ black-forest-labs/flux, 2024. 5, 6 [35] Ji Soo Lee, Jongha Kim, Jeehye Na, Jinyoung Park, and Hyunwoo Kim. Vidchain: Chain-of-tasks with metric-based direct preference optimization for dense video captioning. arXiv preprint arXiv:2501.06761, 2025. [36] Bo Li, Yuanhan Zhang, Dong Guo, Renrui Zhang, Feng Li, Hao Zhang, Kaichen Zhang, Peiyuan Zhang, Yanwei Li, Ziwei Liu, et al. Llava-onevision: Easy visual task transfer. arXiv preprint arXiv:2408.03326, 2024. 1, 3 [37] Chaoyu Li, Eun Woo Im, and Pooyan Fazli. Vidhalluc: Evaluating temporal hallucinations in multimodal large lanarXiv preprint guage models for video understanding. arXiv:2412.03735, 2024. 1, 3, 4 [38] Feng Li, Renrui Zhang, Hao Zhang, Yuanhan Zhang, Bo Li, Wei Li, Zejun Ma, and Chunyuan Li. Llava-next-interleave: Tackling multi-image, video, and 3d in large multimodal models. arXiv preprint arXiv:2407.07895, 2024. 1, 2, 3, 7 Mvbench: comprehensive multi-modal video understanding benchmark. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 22195 22206, 2024. 1 [40] Shicheng Li, Lei Li, Yi Liu, Shuhuai Ren, Yuanxin Liu, Rundong Gao, Xu Sun, and Lu Hou. Vitatecs: diagnostic dataset for temporal concept understanding of video-language models. In European Conference on Computer Vision, pages 331348. Springer, 2024. 1, 3 [41] Yunxin Li, Xinyu Chen, Baotian Hu, Longyue Wang, Haoyuan Shi, and Min Zhang. Videovista: versatile benchmark for video understanding and reasoning. arXiv preprint arXiv:2406.11303, 2024. [42] Yadong Li, Haoze Sun, Mingan Lin, Tianpeng Li, Guosheng Dong, Tao Zhang, Bowen Ding, Wei Song, Zhenglin Cheng, Yuqi Huo, et al. Baichuan-omni technical report. arXiv preprint arXiv:2410.08565, 2(3), 2024. 3 [43] Ji Lin, Hongxu Yin, Wei Ping, Pavlo Molchanov, Mohammad Shoeybi, and Song Han. Vila: On pre-training for visual language models. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 26689 26699, 2024. 7 [44] Haotian Liu, Chunyuan Li, Qingyang Wu, and Yong Jae Lee. Visual instruction tuning. Advances in neural information processing systems, 36:3489234916, 2023. 1, 3 [45] Haotian Liu, Chunyuan Li, Yuheng Li, and Yong Jae Lee. Improved baselines with visual instruction tuning. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 2629626306, 2024. 1 [46] Haotian Liu, Chunyuan Li, Yuheng Li, Bo Li, Yuanhan Zhang, Sheng Shen, and Yong Jae Lee. Llava-next: Improved reasoning, ocr, and world knowledge, 2024. 3 [47] Ruyang Liu, Chen Li, Haoran Tang, Yixiao Ge, Ying Shan, and Ge Li. St-llm: Large language models are effective temporal learners. In European Conference on Computer Vision, pages 118. Springer, 2024. 2, 3 [48] Muhammad Maaz, Hanoona Rasheed, Salman Khan, and Fahad Shahbaz Khan. Video-chatgpt: Towards detailed video understanding via large vision and language models. arXiv preprint arXiv:2306.05424, 2023. 1, 3, [49] Muhammad Maaz, Hanoona Rasheed, Salman Khan, and Fahad Khan. Videogpt+: Integrating image and video encoders for enhanced video understanding. arXiv preprint arXiv:2406.09418, 2024. 1, 3, 7 [50] Michael Mauk and Dean Buonomano. The neural basis of temporal processing. Annual review of neuroscience, 27: 307340, 2004. 2, 4 [51] Hugo Merchant, Deborah Harrington, and Warren Meck. Neural basis of the perception and estimation of time. Annual review of neuroscience, 36:313336, 2013. 2, 4 [52] Juhong Min, Shyamal Buch, Arsha Nagrani, Minsu Cho, and Cordelia Schmid. Morevqa: Exploring modular reasoning In Proceedings of models for video question answering. the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 1323513245, 2024. 1, 3 [39] Kunchang Li, Yali Wang, Yinan He, Yizhuo Li, Yi Wang, Yi Liu, Zun Wang, Jilan Xu, Guo Chen, Ping Luo, et al. [53] Thong Nguyen, Yi Bin, Junbin Xiao, Leigang Qu, Yicong Li, Jay Zhangjie Wu, Cong-Duy Nguyen, See-Kiong Ng, and 10 Luu Anh Tuan. Video-language understanding: survey from model architecture, model training, and data perspectives. arXiv preprint arXiv:2406.05615, 2024. 1, 3 [54] Avalon CS Owens, Mira Van den Broeck, Raphael De Cock, and Sara Lewis. Behavioral responses of bioluminescent fireflies to artificial light at night. Frontiers in Ecology and Evolution, 10:946640, 2022. 1 [55] Jongwoo Park, Kanchana Ranasinghe, Kumara Kahatapitiya, Wonjeong Ryoo, Donghyun Kim, and Michael Ryoo. Too many frames, not all useful: Efficient strategies for long-form video qa. arXiv preprint arXiv:2406.09396, 2024. 1 [56] Joseph Paton and Dean Buonomano. The neural basis of timing: distributed mechanisms for diverse functions. Neuron, 98(4):687705, 2018. 2, 4 [57] Iqra Qasim, Alexander Horsch, and Dilip Prasad. Dense video captioning: survey of techniques, datasets and evaluation protocols. ACM Computing Surveys, 57(6):136, 2025. 1 [58] Long Qian, Juncheng Li, Yu Wu, Yaobo Ye, Hao Fei, TatSeng Chua, Yueting Zhuang, and Siliang Tang. Momentor: Advancing video large language model with fine-grained temporal reasoning. arXiv preprint arXiv:2402.11435, 2024. 2, [59] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, et al. Learning transferable visual models from natural language supervision. In International conference on machine learning, pages 87488763. PmLR, 2021. 1, 2 [60] GM RamırezAvila, Kurths, and Jean-Louis Deneubourg. Fireflies: paradigm in synchronization. Chaotic, Fractional, and Complex Dynamics: New Insights and Perspectives, pages 3564, 2018. 1 [61] Nikhila Ravi, Valentin Gabeur, Yuan-Ting Hu, Ronghang Hu, Chaitanya Ryali, Tengyu Ma, Haitham Khedr, Roman Radle, Chloe Rolland, Laura Gustafson, et al. Sam 2: Segment anything in images and videos. arXiv preprint arXiv:2408.00714, 2024. 4, 5, 6 [62] Shuhuai Ren, Linli Yao, Shicheng Li, Xu Sun, and Lu Hou. Timechat: time-sensitive multimodal large language model for long video understanding. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 1431314323, 2024. 2, 3, 4, 7, 8 [63] Xiaoqian Shen, Yunyang Xiong, Changsheng Zhao, Lemeng Wu, Jun Chen, Chenchen Zhu, Zechun Liu, Fanyi Xiao, Balakrishnan Varadarajan, Florian Bordes, et al. Longvu: Spatiotemporal adaptive compression for long video-language understanding. arXiv preprint arXiv:2410.17434, 2024. 1, 3 [64] Enxin Song, Wenhao Chai, Guanhong Wang, Yucheng Zhang, Haoyang Zhou, Feiyang Wu, Haozhe Chi, Xun Guo, Tian Ye, Yanting Zhang, et al. Moviechat: From dense token to sparse memory for long video understanding. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 1822118232, 2024. 7 [65] Yunlong Tang, Jing Bi, Siting Xu, Luchuan Song, Susan Liang, Teng Wang, Daoan Zhang, Jie An, Jingyang Lin, Rongyi Zhu, et al. Video understanding with large language models: survey. arXiv preprint arXiv:2312.17432, 2023. 1, 3 [66] Gemini Team, Petko Georgiev, Ving Ian Lei, Ryan Burnell, Libin Bai, Anmol Gulati, Garrett Tanzer, Damien Vincent, Zhufeng Pan, Shibo Wang, et al. Gemini 1.5: Unlocking multimodal understanding across millions of tokens of context. arXiv preprint arXiv:2403.05530, 2024. [67] Haibo Wang, Zhiyang Xu, Yu Cheng, Shizhe Diao, Yufan Zhou, Yixin Cao, Qifan Wang, Weifeng Ge, and Lifu Huang. Grounded-videollm: Sharpening fine-grained temporal grounding in video large language models. arXiv preprint arXiv:2410.03290, 2024. 1, 2, 3 [68] Peng Wang, Shuai Bai, Sinan Tan, Shijie Wang, Zhihao Fan, Jinze Bai, Keqin Chen, Xuejing Liu, Jialin Wang, Wenbin Ge, et al. Qwen2-vl: Enhancing vision-language models perception of the world at any resolution. arXiv preprint arXiv:2409.12191, 2024. 1, 2, 3, 4, 7 [69] Xinlong Wang, Xiaosong Zhang, Zhengxiong Luo, Quan Sun, Yufeng Cui, Jinsheng Wang, Fan Zhang, Yueze Wang, Zhen Li, Qiying Yu, et al. Emu3: Next-token prediction is all you need. arXiv preprint arXiv:2409.18869, 2024. 1, 3 [70] Yi Wang, Kunchang Li, Xinhao Li, Jiashuo Yu, Yinan He, Guo Chen, Baoqi Pei, Rongkun Zheng, Zun Wang, Yansong Shi, et al. Internvideo2: Scaling foundation models for multimodal video understanding. In European Conference on Computer Vision, pages 396416. Springer, 2024. 7 [71] Yueqian Wang, Xiaojun Meng, Yuxuan Wang, Jianxin Liang, Jiansheng Wei, Huishuai Zhang, and Dongyan Zhao. Videollm knows when to speak: Enhancing time-sensitive video comprehension with video-text duet interaction format. arXiv preprint arXiv:2411.17991, 2024. 2, 3 [72] Yi Wang, Xinhao Li, Ziang Yan, Yinan He, Jiashuo Yu, Xiangyu Zeng, Chenting Wang, Changlian Ma, Haian Huang, Jianfei Gao, et al. Internvideo2.5: Empowering video mllms with long and rich context modeling. arXiv preprint arXiv:2501.12386, 2025. 1, 2, 3, 4, [73] Yuetian Weng, Mingfei Han, Haoyu He, Xiaojun Chang, and Bohan Zhuang. Longvlm: Efficient long video understanding via large language models. In European Conference on Computer Vision, pages 453470. Springer, 2024. 1, 2, 3, 4 [74] Marc Wittmann. The experience of time: neural mechanisms and the interplay of emotion, cognition and embodiment. Philosophical Transactions of the Royal Society B: Biological Sciences, 364(1525):18091813, 2009. 4 [75] Chengyue Wu, Xiaokang Chen, Zhiyu Wu, Yiyang Ma, Xingchao Liu, Zizheng Pan, Wen Liu, Zhenda Xie, Xingkai Yu, Chong Ruan, et al. Janus: Decoupling visual encoding for unified multimodal understanding and generation. arXiv preprint arXiv:2410.13848, 2024. 1, 3 [76] Wenhao Wu, Zhun Sun, and Wanli Ouyang. Revisiting classifier: Transferring vision-language models for video recognition. In Proceedings of the AAAI conference on artificial intelligence, pages 28472855, 2023. 1, 3 [77] Yi Wu, Jongwoo Lim, and Ming-Hsuan Yang. Object tracking benchmark. IEEE Transactions on Pattern Analysis and Machine Intelligence, 37(9):18341848, 2015. 5 [78] Lin Xu, Yilin Zhao, Daquan Zhou, Zhijie Lin, See Kiong Ng, and Jiashi Feng. Pllava: Parameter-free llava extension from images to videos for video dense captioning. arXiv preprint arXiv:2404.16994, 2024. 1 [79] Yifang Xu, Yunzhuo Sun, Zien Xie, Benxiang Zhai, and Sidan Du. Vtg-gpt: Tuning-free zero-shot video temporal grounding with gpt. Applied Sciences, 14(5):1894, 2024. 1 [80] Antoine Yang, Arsha Nagrani, Paul Hongsuck Seo, Antoine Miech, Jordi Pont-Tuset, Ivan Laptev, Josef Sivic, and Cordelia Schmid. Vid2seq: Large-scale pretraining of visual language model for dense video captioning. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 1071410726, 2023. 1, 3 [81] Lihe Yang, Bingyi Kang, Zilong Huang, Zhen Zhao, Xiaogang Xu, Jiashi Feng, and Hengshuang Zhao. Depth anything v2. Advances in Neural Information Processing Systems, 37: 2187521911, 2025. 5 [82] Zhengyuan Yang, Linjie Li, Kevin Lin, Jianfeng Wang, Chung-Ching Lin, Zicheng Liu, and Lijuan Wang. The dawn of lmms: Preliminary explorations with gpt-4v (ision). arXiv preprint arXiv:2309.17421, 9(1):1, 2023. 1 [83] Zhenyu Yang, Yuhang Hu, Zemin Du, Dizhan Xue, Shengsheng Qian, Jiahong Wu, Fan Yang, Weiming Dong, and Changsheng Xu. Svbench: benchmark with temporal multiturn dialogues for streaming video understanding, 2025. 1, 3 [84] Shoubin Yu, Jaemin Cho, Prateek Yadav, and Mohit Bansal. Self-chained image-language model for video localization and question answering. Advances in Neural Information Processing Systems, 36:7674976771, 2023. 1, [85] Boqiang Zhang, Kehan Li, Zesen Cheng, Zhiqiang Hu, Yuqian Yuan, Guanzheng Chen, Sicong Leng, Yuming Jiang, Hang Zhang, Xin Li, et al. Videollama 3: Frontier multimodal foundation models for image and video understanding. arXiv preprint arXiv:2501.13106, 2025. 2, 7 [86] Haotian Zhang, Mingfei Gao, Zhe Gan, Philipp Dufter, Nina Wenzel, Forrest Huang, Dhruti Shah, Xianzhi Du, Bowen Zhang, Yanghao Li, et al. Mm1. 5: Methods, analysis & insights from multimodal llm fine-tuning. arXiv preprint arXiv:2409.20566, 2024. 1, 3 [87] Yuanhan Zhang, Jinming Wu, Wei Li, Bo Li, Zejun Ma, Ziwei Liu, and Chunyuan Li. Video instruction tuning with synthetic data. arXiv preprint arXiv:2410.02713, 2024. 1, 3, 7 [88] Yue Zhao, Ishan Misra, Philipp Krahenbuhl, and Rohit Girdhar. Learning video representations from large language In Proceedings of the IEEE/CVF Conference on models. Computer Vision and Pattern Recognition, pages 65866597, 2023. 1, 3 [89] Yaoyao Zhong, Junbin Xiao, Wei Ji, Yicong Li, Weihong Deng, and Tat-Seng Chua. Video question answering: Datasets, algorithms and challenges. arXiv preprint arXiv:2203.01225, 2022. 1 [90] Deyao Zhu, Jun Chen, Xiaoqian Shen, Xiang Li, and Mohamed Elhoseiny. Minigpt-4: Enhancing vision-language understanding with advanced large language models. arXiv preprint arXiv:2304.10592, 2023."
        },
        {
            "title": "Appendix",
            "content": "Text-Specific Direct Prompt This supplementary material provides additional experimental details regarding following: Prompt Design for Evaluation (Appendix A) Impact of Finetuning (Appendix B) Impact of FPS (Appendix C) Temporal Motion Coherence Analysis (Appendix ) Additional Images (Appendix E) A. Prompt Design for Evaluation Prompt design significantly affects the performance of vision-language models [28, 30]. Considering this fact, we performed careful prompt engineering to ensure fair and comprehensive evaluation. We developed systematic prompting methodology that builds on established principles while introducing novel elements specific to temporal pattern recognition. A.1. Prompt Design Principles We designed our prompts based on three key principles: 1. Specificity: Each prompt explicitly states that the content is encoded through temporal patterns to direct attention to motion-based cues rather than static frame analysis. 2. Category targeting: We created specialized prompts for each content category (text, shapes, objects, dynamic scenes) to account for the different perceptual mechanisms involved in each. 3. Constrained response format: All prompts request brief, specific answers (1-3 words) to ensure objective evaluation and minimize the influence of language generation capabilities. A.2. Direct vs. Chain-of-Thought Prompting We implemented two distinct prompting strategies to investigate different aspects of temporal understanding. Direct prompts test immediate pattern recognition without explicit guidance, similar to how humans naturally perceive temporal patterns without conscious step-by-step processing. Chain-of-Thought (CoT) prompts provide explicit steps to guide attention and processing, testing whether models could benefit from structured reasoning about temporal patterns. The figures below present our category-specific prompts for both strategies, which were carefully optimized through pilot testing to maximize clarity while maintaining consistent evaluation criteria across categories. This video contains text encoded through temporal patterns. What specific word or phrase do you see? The text is only visible through the temporal changes in the video. Please respond with just the text you identify. Shapes-Specific Direct Prompt This video contains an object encoded through temporal patterns. What object emerges from the temporal changes? Please respond with just the name of the geometric object you see. Object-Specific Direct Prompt This video contains common object encoded through temporal patterns. Individual frames may appear as noise, but an object is visible through the temporal changes. What object do you see? Please respond with just the object name. Dynamic Scenes-Specific Direct Prompt This video contains movement or action encoded through temporal patterns. The content is only visible through temporal changes, not in individual frames. What is being shown? Please respond with just 1-3 words describing what you see. Figure 6. Category-specific direct prompts for the SpookyBench benchmark. These prompts test immediate pattern recognition without step-by-step guidance. tasks. All tested models achieved 0% accuracy regardless of prompt type, indicating fundamental limitation in their ability to process purely temporal information rather than prompt engineering issue. The complete ineffectiveness of even carefully engineered prompts across all tested models further strengthens our argument that current video-language models lack the fundamental architectural mechanisms needed for processing purely temporal patterns. A.3. Prompt Effectiveness Analysis B. Impact of Finetuning Our experiments revealed that, surprisingly, neither prompt strategy improved model performance on the SpookyBench To test whether the performance gap is merely due to outof-distribution data rather than fundamental flaw in the 13 Text-Specific CoT Prompt Object-Specific CoT Prompt This video encodes text through temporal patterns. To identify it: 1. Look for areas where opposing motion patterns This video encodes an object through temporal patterns. To identify it: 1. Look for areas where motion patterns reveal obreveal letters 2. Focus on the overall word or phrase that emerges 3. Read the specific text content Please respond with just the text you identify. ject contours 2. Focus on the overall silhouette and form that emerges 3. Determine what specific object is represented Please respond with just the object name. Shapes-Specific CoT Prompt This video encodes an object through temporal patterns. To identify it: 1. Look for areas where motion reveals contours and edges 2. Focus on the object outline that emerges 3. Determine what specific very common geometric object is shown Dynamic Scenes-Specific CoT Prompt This video encodes movement through temporal patterns. To identify it: 1. Look for areas where temporal changes reveal motion 2. Focus on the action or activity that emerges from the pattern Please respond with just the object name. 3. Identify the specific movement or object in moFigure 7. Category-specific chain-of-thought prompts for the SpookyBench benchmark. These prompts provide explicit step-bystep guidance to test structured temporal reasoning. design of VLMs, we finetuned Qwen2.5-VL-7B on 400 SpookyBench videos for 5 epochs. Despite this targeted training, accuracy remained at 0%. This confirms that the issue is not mere out-of-distribution (OOD) shift but rather an architectural inability to integrate purely temporal cues. C. Impact of FPS To test the impact of frame rate on both human and VLM performance, we conducted additional experiments examining how temporal sampling affects the ability to perceive information encoded purely through motion patterns. This analysis addresses critical question: could the performance gap between humans and VLMs be attributed to differences in temporal sampling rather than fundamental architectural limitations? We evaluated both human participants and videolanguage models across multiple frame rates ranging from 1 to 30 FPS. For the human study, we recruited 3 participants and tested them on 60 randomly sampled videos (15 from each category) at frame rates of 1, 5, 10, 20, and 30 FPS. For the VLM evaluation, we tested four state-of-the-art models: Qwen2-VL-7B, Qwen2.5-VL-7B, Qwen2.5-VL-3B, and GPT-4o. We applied the same temporal downsampling approach, ensuring that models received the exact number of frames corresponding to each target frame rate. tion Please respond with just 1-3 words describing what you see. Figure 8. Additional category-specific chain-of-thought prompts for object images and dynamic scenes. Category 1 FPS 5 FPS 10 FPS 20 FPS 30 FPS Images Words Shapes Videos Average 0.00 0.00 8.89 0. 2.22 66.67 11.11 8.89 46.67 33.33 80.00 35.56 75.56 62.22 63.33 95.56 95.56 100.00 93. 97.78 95.56 100.00 93.33 96.11 96.67 Table 4. Human accuracy (%) across different content categories at varying frame rates. Results are averaged across 3 participants on 60 videos (15 per category). Table 4 presents the human performance results across different frame rates and content categories. Human accuracy remains remarkably robust at higher frame rates, maintaining over 95% accuracy at 20-30 FPS across most categories. Performance begins to degrade at 10 FPS, dropping to 63.33% on average, with particularly pronounced effects on the Words category (35.56%). At extremely low frame rates (1-5 FPS), human performance drops substantially, reaching only 2.22% at 1 FPS. In stark contrast, Table 5 shows that all tested VLMs achieved 0% accuracy regardless of frame rate. This Model Accuracy (%) Qwen2-VL-7B Qwen2.5-VL-7B Qwen2.5-VL-3B GPT-4o Average 0.00 0.00 0.00 0.00 0. Table 5. VLM accuracy (%) averaged across all tested frame rates (1-30 FPS). All models consistently achieved 0% accuracy regardless of temporal sampling density. consistent failure across the entire range of tested frame ratesfrom 1 FPS to 30 FPSdemonstrates that temporal sampling frequency is not the limiting factor for current video-language models. These results reveal fundamental difference in how humans and current VLMs process temporal information. While human performance degrades gracefully as frame rate decreasesparticularly below 10 FPS where temporal patterns become harder to perceiveVLMs show no improvement even at optimal frame rates. This finding effectively rules out temporal undersampling as an explanation for the observed performance gap. The graceful degradation in human performance at lower frame rates aligns with our understanding of human temporal perception, where motion detection requires sufficient temporal resolution. However, the complete insensitivity of VLMs to frame rate variations suggests that these models are not engaging with temporal information in any meaningful way, regardless of how much temporal data is provided. This analysis strengthens our core argument that current video-language models lack the fundamental architectural mechanisms needed to process information conveyed purely through temporal patterns, independent of spatial content quality or temporal sampling considerations. D. Temporal Motion Coherence Analysis Visual content in noise presents significant challenge for perception. However, temporal coherence and motion boundaries provide powerful cues that enable the human visual system to extract meaningful shapes even from extremely noisy stimuli. We present comprehensive analysis of these phenomena using our SpookyBench dataset, specifically examining how temporal information facilitates shape perception in high-noise conditions. Figure 9. Motion Direction Coherence visualization for the ant silhouette video. Yellow regions (high coherence value of 1.0) indicate areas where motion direction remains consistent across frames, while blue regions (coherence value of 0.0) represent the silhouette itself. (a) Estimated object mask extracted from temporal motion coherence. (b) Estimated mask overlay (red) on single noise frame. Figure 10. Shape extraction through temporal integration. These visualizations demonstrate how object shape can be recovered from noisy video sequences. tion. Figure 9 shows the motion direction coherence map of an ant silhouette, revealing how consistent motion patterns across frames enable object identification despite significant noise. The importance of temporal integration for shape perception is further demonstrated in Figure 11, which shows both the average motion boundary strength and its overlay on noisy frame. Motion boundaries emerge clearly despite the extreme noise levels in individual frames (measured at -49.07 dB basic SNR), highlighting the importance of temporal information for noisy visual content. D.1. Motion-Based Perception in Noisy EnvironD.2. Signal-to-Noise Ratio Analysis ments Our analysis demonstrates that even when individual frames contain low signal-to-noise ratios (SNR), temporal integration of motion information allows for robust shape percepTo quantify the perceptual phenomenon observed in our stimuli, we conducted detailed SNR analysis. Table 6 presents the results for the ant silhouette video shown in the figures above. Notably, while the basic and perceptual SNR metcontent: 1. Distributed Motion Patterns: Human movement involves multiple articulated body parts moving in different directions simultaneously, creating competing motion signals that fragment coherent boundaries. 2. Non-rigid Deformation: Dynamic content involves continuous shape changes throughout motion sequences, making consistent boundary extraction significantly more challenging than static objects. 3. Complex Temporal Dynamics: Mechanical motions in examples like Plane 2 and Bicycle 10 create temporal discontinuities that disrupt motion coherence essential for shape perception. This comprehensive analysis demonstrates the systematic nature of temporal pattern recognition challenges across all SpookyBench categories, with each content type presenting distinct perceptual and computational difficulties that current video-language models fail to address. (a) Average motion boundary strength across frames. (b) Motion boundaries (teal) overlaid on single noise frame. Figure 11. Motion boundary analysis demonstrating how temporal integration can extract object boundaries despite extremely noisy individual frames. SNR Metric Value (dB)"
        },
        {
            "title": "Basic SNR\nPerceptual SNR\nTemporal Coherence SNR\nMotion Contrast SNR\nCombined SNR",
            "content": "-49.07 -55.02 7.18 14.24 -20.61 Table 6. Signal-to-Noise Ratio Analysis for Ant Silhouette Video rics show extremely low values (-49.07 dB and -55.02 dB respectively), the temporal coherence SNR and motion contrast SNR reveal significantly higher values (7.18 dB and 14.24 dB), demonstrating that temporal information provides crucial signal enhancement that supports human perception. This analysis provides important insights into how the human visual system utilizes temporal information to extract meaningful content from extremely noisy visual stimuli. The stark contrast between the negative frame-based SNR values and the positive temporal SNR metrics directly supports our hypothesis that temporal integration plays crucial role in human perception of noisy visual content. These findings align with the high human evaluation accuracy reported in the main paper, where participants achieved over 95% accuracy for Object Images despite the extreme noise levels in individual frames. E. Additional Images We present additional images from the analysis of temporal motion coherence across all categories in SpookyBench. Figures 12, 13, 14, 15, and 16 show motion boundaries, boundary overlays, estimated masks, and mask overlays for different examples in our dataset, demonstrating the varying effectiveness of temporal integration across different content types. In Figure 16, we observe that the temporal motion coherence based method does not perform effectively for the case of videos. Since the Dynamic Scenes category contains real-life videos with complex motion patterns, several factors contribute to the reduced clarity observed in dynamic Figure 12. Temporal motion coherence analysis for Images category (Part 1). Each row shows motion boundaries, boundary overlay, estimated mask, and mask overlay for: Cycle, Deer, Dolphin, and Duck (top to bottom). 17 Figure 13. Temporal motion coherence analysis for Images category (Part 2). Each row shows motion boundaries, boundary overlay, estimated mask, and mask overlay for: Kangaroo, Hammer, T-Rex, and Mouse (top to bottom). 18 Figure 14. Temporal motion coherence analysis for Shapes category. Each row shows motion boundaries, boundary overlay, estimated mask, and mask overlay for: Arrow, Heart, Mouse, and Rectangle (top to bottom). Figure 15. Temporal motion coherence analysis for Words category. Each row shows motion boundaries, boundary overlay, estimated mask, and mask overlay for: Gold, Laser Beams Cross, Ancient Olive Trees, and Artificial Minds Think (top to bottom). 20 Figure 16. Temporal motion coherence analysis for Videos category. Each row shows motion boundaries, boundary overlay, estimated mask, and mask overlay for: Human 6, Man 1, Plane 2, and Bicycle 10 video sequence(top to bottom)."
        }
    ],
    "affiliations": [
        "King Abdullah University of Science and Technology (KAUST)",
        "Mohamed bin Zayed University of AI (MBZUAI)"
    ]
}