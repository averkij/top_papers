{
    "paper_title": "UltraFlux: Data-Model Co-Design for High-quality Native 4K Text-to-Image Generation across Diverse Aspect Ratios",
    "authors": [
        "Tian Ye",
        "Song Fei",
        "Lei Zhu"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Diffusion transformers have recently delivered strong text-to-image generation around 1K resolution, but we show that extending them to native 4K across diverse aspect ratios exposes a tightly coupled failure mode spanning positional encoding, VAE compression, and optimization. Tackling any of these factors in isolation leaves substantial quality on the table. We therefore take a data-model co-design view and introduce UltraFlux, a Flux-based DiT trained natively at 4K on MultiAspect-4K-1M, a 1M-image 4K corpus with controlled multi-AR coverage, bilingual captions, and rich VLM/IQA metadata for resolution- and AR-aware sampling. On the model side, UltraFlux couples (i) Resonance 2D RoPE with YaRN for training-window-, frequency-, and AR-aware positional encoding at 4K; (ii) a simple, non-adversarial VAE post-training scheme that improves 4K reconstruction fidelity; (iii) an SNR-Aware Huber Wavelet objective that rebalances gradients across timesteps and frequency bands; and (iv) a Stage-wise Aesthetic Curriculum Learning strategy that concentrates high-aesthetic supervision on high-noise steps governed by the model prior. Together, these components yield a stable, detail-preserving 4K DiT that generalizes across wide, square, and tall ARs. On the Aesthetic-Eval at 4096 benchmark and multi-AR 4K settings, UltraFlux consistently outperforms strong open-source baselines across fidelity, aesthetic, and alignment metrics, and-with a LLM prompt refiner-matches or surpasses the proprietary Seedream 4.0."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 2 2 ] . [ 1 0 5 0 8 1 . 1 1 5 2 : r UltraFlux: Data-Model Co-Design for High-quality Native 4K Text-to-Image Generation across Diverse Aspect Ratios Tian Ye* HKUST(GZ) tye610@connect.hkust-gz.edu.cn Song Fei* HKUST(GZ) sfei285@connect.hkust-gz.edu.cn Lei Zhu HKUST, HKUST(GZ) leizhu@ust.hk Project: https://w2genai-lab.github.io/UltraFlux/ Code: https://github.com/W2GenAI-Lab/UltraFlux Figure 1. Left: UltraFlux generates photorealistic 4K images across diverse aspect ratios and topics while maintaining high aesthetic quality and faithful content depiction with single unified text-to-image model. Right: Our MultiAspect-4K-1M is large-scale highquality dataset for 4K image synthesis."
        },
        {
            "title": "Abstract",
            "content": "Diffusion transformers have recently delivered strong textto-image generation around 1K resolution, but we show that extending them to native 4K across diverse aspect ratios exposes tightly coupled failure mode spanning positional encoding, VAE compression, and optimization. Tackling any of these factors in isolation leaves substantial quality on the table. We therefore take datamodel co-design *Equal contribution, UltraFlux project leader: Tian Ye Corresponding author. view and introduce UltraFlux, Flux-based DiT trained natively at 4K on MultiAspect-4K-1M, 1M-image 4K corpus with controlled multi-AR coverage, bilingual captions, and rich VLM/IQA metadata for resolutionand AR-aware sampling. On the model side, UltraFlux couples (i) Resonance 2D RoPE with YaRN for training-window-, frequency-, and AR-aware positional encoding at 4K; (ii) simple, nonadversarial VAE post-training scheme that improves 4K reconstruction fidelity; (iii) an SNR-Aware Huber Wavelet objective that rebalances gradients across timesteps and frequency bands; and (iv) Stage-wise Aesthetic Curriculum Learning strategy that concentrates high-aesthetic supervision on high-noise steps governed by the model prior. Together, these components yield stable, detail-preserving 4K DiT that generalizes across wide, square, and tall ARs. On the Aesthetic-Eval@4096 benchmark and multi-AR 4K settings, UltraFlux consistently outperforms strong opensource baselines across fidelity, aesthetic, and alignment metrics, andwith LLM prompt refinermatches or surpasses the proprietary Seedream 4.0. 1. Introduction Diffusion transformers (DiTs) [2, 5, 6, 23, 37] have recently pushed text-to-image generation to impressive quality around 1K resolution, enabled by efficient backbones, token compression, and carefully tuned training pipelines [5, 37]. However, extending these systems to native 4K while supporting broad spectrum of aspect ratios (ARs) is not simple matter of scaling resolution. At 40964096 and beyond, we empirically observe three coupled challenges: (i) positional representation and AR extrapolation, where 2D rotary embeddings calibrated on single training window can drift or alias under large changes in resolution and AR [24, 44]; (ii) high-frequency fidelity under VAE compression, where higher downsampling factors improve throughput but tend to erase fine structures that dominate 4K perception [37, 42]; and (iii) 4K-aware optimization, where gradient contributions become heavily skewed across timesteps and frequency bands, making standard objectives poorly matched to the statistics of 4K latents [9, 42]. These factors interact: the choice of positional scheme, VAE compression ratio, and training objective jointly determines whether model can remain stable and detailed across native 4K resolutions and diverse ARs. On the model side, several scaling strategies partially address these issues but leave the overall design space fragmented. Training-free high-resolution methods mitigate tiling artifacts and duplication at inference time, yet largely preserve the underlying positional encoding and were not designed for systematic multi-AR extrapolation [12, 45]. Decoder-side approaches based on globallocal fusion or tiled diffusion improve size flexibility but introduce new failure modes, such as coherence gaps across tiles or heavy reliance on global prior for consistency [1, 8]. Native4K systems [20, 40] demonstrate that carefully engineered backbones can make 4K training tractable [5, 37], but most emphasize token/architecture efficiency and treat positional robustness, VAE compression, and loss design as largely orthogonal choices rather than jointly optimized 4K regime. Progress at 4K is further constrained by the data itPublic 4K corpora are typically modest in scale self. (on the order of 104105 images), heavily biased toward near-square ARs and landscape-centric content, and curated with early CLIP-based aesthetic predictors. For example, Aesthetic-4K takes an important step by assembling highquality 4K imagetext pairs with GPT-4O captions [42], yet its scale and AR coverage remain limited for studying resolutionAR coupling, and its subject distribution underrepresents human-centric scenes. More critically, existing 4K datasets rarely provide the structured metadata needed for modern 4K training. As result, practitioners have limited control over sampling data slices tailored to specific training regimes (e.g., high-detail or high-aesthetic subsets), and it becomes difficult to perform fine-grained aesthetic or AR-conditioned analyses. On the optimization and adaptation side, recent work explores complementarybut still incompletedirections. Wavelet-aware objectives at native resolution improve fidelity on strong backbones by better emphasizing highfrequency content [42], yet they typically combine simple quadratic or perceptual penalties and thus remain vulnerable to cross-scale dominance of low-frequency energy. Latentspace super-resolution and self-cascade schemes sharpen details beyond the original training resolution and reduce the cost of high-resolution transfer [7, 14], but they operate as post-hoc adapters on fixed backbones and do not resolve the underlying trade-off between VAE compression and 4K reconstruction fidelity. In parallel, timestep curricula adjust noise sampling while holding the data distribution fixed, and aesthetic post-training applies high-aesthetic data uniformly across timesteps, leaving unexplored the regime where high-noise stepsthose most governed by the model priorare selectively sculpted by high-aesthetic supervision. Finally, existing RoPE interpolation and NTKstyle scaling strategies are primarily developed for 1D sequence length extrapolation, and provide little guidance for 2D token grids at native 4K under strongly varying ARs, where misaligned phase behavior manifests as ghosting, drift, and striping artifacts. Altogether, native 4K multiAR generation still lacks unified framework that couples: (i) large-scale, multi-AR, content-diverse, VLMcurated 4K corpus with rich metadata; (ii) an efficient, nonadversarial VAE post-training strategy that improves 4K reconstruction without sacrificing throughput; (iii) an SNRAware Huber Wavelet Training Objective and stage-wise aesthetic curriculum matched to 4K statistics; and (iv) training-window aware, band-aware, and AR-aware positional encoding scheme. In this work, we explicitly target this datamodel co-design space. Concretely, we make the following contributions: MultiAspect-4K-1M: large-scale, multi-AR, aesthetically curated 4K corpus. We construct 1M-scale 4K dataset with native 4K and near-4K resolution, controlled aspect-ratio coverage, and dual-channel pipeline that debiases landscape-heavy sources toward human-centric content. Each image is accompanied by decoupled VLMbased quality and aesthetic scores, classical IQA signals, bilingual captions, and subject tags, providing the structured metadata needed for datamodel co-design. UltraFlux: datamodel co-designed DiT for native 4K multi-AR generation. We train Flux-based backbone on MultiAspect-4K-1M with co-designed recipe that couples (i) Resonance 2D RoPE with YaRN for training-window aware, band-aware, and AR-aware positional encoding, (ii) an SNR-Aware Huber Wavelet Training Objective tailored to 4K latents, (iii) Stage-wise Aesthetic Curriculum Learning (SACL) scheme that concentrates high-aesthetic supervision on high-noise steps, and (iv) simple, non-adversarial, data-efficient VAE posttraining procedure that improves Flux VAE reconstructions at 4K. Together, these components yield stable, detail-preserving DiT for native 4K synthesis across diverse ARs. State-of-the-art native 4K performance. On standard 4K benchmarks and popular metrics covering fidelity, aesthetic quality, and text alignment, UltraFlux consistently outperforms strong 4K baselines, including recent native-4K and training-free scaling methods. 2. Related Work This section reviews approaches to scaling text-to-image diffusion models to high-resolution T2I, native-4K and diverse aspect ratios. We group prior work into three lines: training-free inference-time scaling, lightweight adaptations (e.g., latent super-resolution and self-cascade), and native-4K training with 4K-capable backbones. Training-Free High-Resolution Scaling. Training-free strategies extend pre-trained 5121K models to 2K/4K and diverse aspect ratios by modifying inference-time computation, without re-training. HiDiffusion diagnoses duplication and quadratic self-attention costs at high resolutions and introduces resolution-aware U-Net and windowed attention to improve quality and speed [45]. FouriScale approaches ultra-high resolution from the frequency view via Fourierdomain low-pass guidance and dilated convolutions, improving global structure while injecting high frequencies [12]. These approaches are effective for quick scaling, yet commonly keep the original positional scheme unchanged, which leaves positional extrapolation stability under extreme ARs only partially addressed [1, 8, 12, 45]. Lightweight Adaptation: Latent SR and Self-Cascade Models. Lightweight adaptations improve high-resolution quality with minimal cost by augmenting the sampling pipeline or attaching small modules. LSRNA maps lowres latents to high-res manifold via latent-space superresolution and injects region-wise noise to restore highfrequency detail without retraining the base model [14]. Self-Cascade Diffusion integrates low-resolution generation into the high-resolution denoising process and optionally fine-tunes small multi-scale upsamplers, achieving rapid 4K adaptation at fraction of full fine-tuning cost [7]. While these methods markedly sharpen details and reduce adaptation overhead, they typically inherit the original positional scheme, leaving AR-generalized extrapolation underexplored [7, 14]. Native 4K Training and 4K-Capable Foundation Models. complementary direction trains or fine-tunes models directly at native-4K and curates high-quality 4K corpora. Diffusion-4K introduces Aesthetic-4K and waveletbased fine-tuning scheme that improves fidelity and prompt alignment on large modern backbones [42]. Meanwhile, efficient backbones such as PixArt-Σ (token-compression attention) and Sana (32 VAE with linear-attention DiT) make 40964096 synthesis computationally feasible at small model scales [5, 37]. Despite these advances, public corpora remain limited in scale and AR diversity, constraining systematic study of resolutionAR coupling.Moreover, end-to-end methodologies for stable native-4K training are under-documented and fragmented across implementations, slowing progress and curbing real-world adoption while masking the gains of true 4K training. practical distinction concerns native versus upscaler-based 4K. Unlike platform services that reach 4K primarily via 2/4 upscalers (e.g., Midjourney [22]; Google Imagen [26] exposes dedicated upscaler; Ideogram offers 2 Upscale endpoint [13]), recent closed-source leaders such as Seedream 4 [28] explicitly support multi-AR, native-4K generation within unified T2I/editing architecture. This distinction matters: cascade upscaling pipelines couple lowresolution synthesis with separate restoration prior, conflating high-frequency fidelity and positional extrapolation, whereas native-4K training compels the backbone to learn long-range dependencies and cross-AR spatial alignment directly. Therefore, we treat native-4K as distinct training/evaluation regime and design both our data (MultiAspect-4K-1M) and recipe (UltraFlux) to isolate the gains of true 4K training from post-hoc upscaling. 3. Method: DataModel Co-Design for Native 4K Multi-AR Generation 3.1. MultiAspect-4K-1M Dataset Design goals and scope. Public 4K corpora for textto-image training remain modest in scale (typically below 105 images) and are usually curated with early CLIPbased aesthetic predictors such as LAION-Aesthetic. While these datasets already achieve reasonably good visual their aspect-ratio (AR) coverage is coarse and quality, imbalancedonly few popular ARs are well-populated at native 4Kand the textual side (captions and aesthetic/quality supervision) is constrained by legacy CLIP-only scoring. Our data design therefore targets three compleFigure 2. Data Pipeline overview. mentary gaps: (i) broad multi-AR coverage at native 4K to avoid overfitting to small set of AR buckets; (ii) refreshed supervision quality, coupling modern VLM-based quality/aesthetics estimators instead of relying solely on legacy CLIP-based predictors; and (iii) distribution debiasing that compensates for the over-representation of landscapes and the under-representation of human-centric content in existing 4K sources. We adopt VLM-driven filtering strategysemantic quality via Q-Align [36] and aesthetics via ArtiMuse [3]complemented by interpretable classical signals (flatness and information entropy), and dedicated character augmentation branch to improve recall for human subjects. Figure 2 sketches the two-channel pipeline and the final merge. Sources and overall structure. After an NSFW safety check, we curate from pool of approximately 6M highresolution images whose subject distribution is skewed toward landscapes. To operationalize the goals in Sec. 3.1, we adopt dual-channel pipeline: (i) general, AR-aware curation path that enforces native/near-4K resolution and broad aspect-ratio (AR) coverage while filtering for quality and aesthetics; and (ii) human-centric augmentation path that restores the underrepresented character category via open-vocabulary detection. The two channels are merged after de-duplication and with consistent metadata (resolution/AR, VLM scores, classical signals, caption, subject tags), yielding 1M images. Figure 2 provides highlevel overview and stage-wise retention. VLM-based Filtering for HQ 4K Data. We begin with safety screen, then enforce pixel-count threshold as resolution filtering stageimages must have at least 3840 2160 total pixels; we preserve each images native aspect ratio without any resizing. This keeps the corpus artifact-free while naturally retaining wide spectrum of ARs (e.g., 1:1, 16:9, 3:2, 4:3, 9:16), enabling transparent auditing of AR coverage. The resulting resolution and aspect-ratio distribution across 4K corpora is visualized in Figure 4. On this scaffold, we decouple quality from aesthetics: for quality we adopt Q-Align, large multimodal model (LMM)-based visual scorer shown to deliver robust IQA judgments via discrete text-level supervision, and for aesthetics we use ArtiMuse, recent MLLM-based image aesthetics evaluator that provides numeric scores together Figure 3. Dataset example. Figure 4. Dataset aspect and resolution analysis. All datasets use 10k samples. MultiAspect-4K-1M has broader aspect ratio distribution. with reasoned, expert-style explanations (rather than scoreonly outputs). Classical, interpretable signalsflatness and information entropyact as guardrails to suppress lowtexture or overly smooth images that VLMs may tolerate, yielding cleaner, high-frequencypreserving pool without sacrificing semantic clarity. Human-centric augmentation via open-vocabulary detector. To correct the chronic under-representation of people in 4K sources, we run targeted augmentation path. Candidate images are collected via person-related retrieval under the same safety and resolution/AR checks. We then apply the same Q-Align and ArtiMuse filters, strengthened by information entropy to suppress low-texture portraits. Crucially, we require structured evidence of human presence using YOLOE [33], promptable open-vocabulary detector, which improves both recall and precision beyond fixed-class detectors. The accepted subset is merged into the main pool with character flag. Key statistics and comparisons to existing 4K and popular T2I datasets are summarized in Table 1. Table 1. Dataset statistical comparisons. Dataset Number Avg. Resolution Avg. ArtiMuse Avg. Caption Length Bilingual Caption PixArt-30k [5] Aesthetic-4K [42] MultiAspect-4K-1M 1,007,230 30,000 12,009 2,5312,656 4,5764,837 4,5214,703 63.67 63.49 64. 87.9 tokens 31.0 tokens 125.1 tokens Bilingual captioning. Captioning is performed last. For the retained set, we generate detailed captions with Gemini2.5-Flash, production multimodal model suitable for fast, high-quality captioning; we then translate each caption into accurate Chinese with Hunyuan-MT-7B [46] to serve bilingual users. An example image with its metadata and bilingual captions is shown in Figure 3. The final MultiAspect-4K-1M corpus comprises 1M 4K images with balanced coverage over standard AR buckets and diversified subject mix (landscapes, people, obEach image includes resolution, Q-Align, Arjects). tiMuse, flatness/entropy, English/Chinese caption, and the character tag. These fields are designed for transparent auditing and flexible data-model co-design: they can act as analysis tags and stratified sampling keys for text-to-image training. 3.2. UltraFlux: Scaling Flux to Native 4K Image"
        },
        {
            "title": "Generation",
            "content": "With the data foundation in place, we now turn to the model side and build UltraFlux. Rather than redesigning the DiT architecture, we keep the core Flux transformer intact and focus on three components that bottleneck 4K performance: the VAE, the positional representation, and the training objective and strategy. We first post-train an 16 VAE to recover fine details without giving up the efficiency gains of stronger compression, then introduce Resonance 2D RoPE with YaRN-style extrapolation scheme to stabilize attention across resolutions and aspect ratios. Finally, we couple an SNR-aware Huber wavelet loss with stage-wise aesthetic curriculum that concentrates learning on highfrequency structure and high-aesthetic examples. Together, these lightweight but targeted changes upgrade Flux into an efficient, high-fidelity 4K generator that can fully exploit MultiAspect-4K-1M. 3.2.1. VAE Post-training for High-resolution Reconstruction Fidelity strong but efficient VAE is essential for practical native 4K image generation. The Flux backbone uses an 8 VAE (height/width downsampling by 8), which at 4K yields very large latent grid and makes sampling prohibitively in our profiling, single 4K image with 50 diffuslow: sion steps takes on the order of 30 minutes. Following Diffusion-4K [42], we instead adopt an 16 VAE, halving the latent resolution while keeping comparable channel capacity, and focus on post-training the decoder to improve high-resolution reconstruction fidelity. We fine-tune the Flux 16 decoder on our curated MultiAspect-4K-1M corpus to enhance fine-scale 4K detail. Ablations on loss design and training recipes lead to three key findings: (i) explicitly targeting high-frequency content is essentialcombining wavelet reconstruction loss applied to high-frequency sub-bands with feature-space perceptual loss consistently outperforms purely pixel-wise and perceptual objectives in sharpness and structural fidelity; (ii) once reconstruction reaches reasonable regime, an adversarial discriminator offers negligible benefitthe GAN loss saturates quickly, induces optimization instability, and fails to improve perceptual quality, so our final recipe omits the adversarial term and retains only wavelet, perceptual and L2 losses; and (iii) stringent data curation substantially reduces post-training costby applying flatness filter to select high-detail subset of MultiAspect-4K-1M, we obtain the bulk of reconstruction gains within around 4k update steps, and observe that few hundred thousand carefully screened, detail-rich images suffice to markedly upgrade the Flux 16 VAE without multi-day GAN training or tens of millions of samples. This lightweight post-training stage produces decoder that preserves fine 4K structures while maintaining the throughput advantages of 16 compression, thereby enabling native 4K synthesis with both high fidelity and practical efficiency. 3.2.2. Resonance 2D RoPE for Multi-AR 4K Extrapolation The official Flux backbone employs fixed per-axis rotary spectrum with an optional global NTK factor [2], following the standard RoPE formulation [31] but without bandspecific treatment or training-window awareness. Consequently, the frequencies do not adapt to the inference size HW , and phase grows purely with position, which empirically destabilizes multiaspect-ratio generation at native 2K/4K. Flux baseline: 2D RoPE. Following Flux, we assign rotary embeddings independently along height and width. For each axis {H, } with channel size da and ma = da/2 complex pairs, we use rotary base > 1 and optional NTK factor η 1 to define per-axis frequencies = (bη)α(a) ω(a) , α(a) = 2k da , = 0, . . . , ma1, and phases for position = (pH , pW ) (in patches) (pa) = pa ω(a) ϕ(a) . (1) (2) = x(a) 2k , we apply the (pa), with wave2k1 + x(a) Writing each pair as z(a) ϕ(a) = z(a) usual complex rotation z(a) length (in patches) λ(a) = 2π/ω(a) . Resonance 2D RoPE. Motivated by the Resonance RoPE idea for train-short-test-long generalization in LLMs [34], we reinterpret the 2D rotary spectrum on finite training window. Let LH , LW be the training-window lengths (in patches) along height and width, and let ω(a) be the per-axis frequencies from Eq. (1). We define the number of cycles completed by component (a, k) inside the training window as r(a) = La λ(a) = La ω(a) 2π . We then snap r(a) to the nearest nonzero integer: (cid:16) ˆr(a) = max 1, (cid:4)r(a) + 1 2 (cid:5)(cid:17) , and replace ω(a) by its integer-cycle projection ˆω(a) = 2π ˆr(a) La ."
        },
        {
            "title": "The phase becomes",
            "content": "k (pa) = pa ˆω(a) ˆϕ(a) , (3) (4) (5) (6) and we reuse the same complex rotation as in the Flux baseline, now driven by ˆϕ(a) . The snapped wavelength is ˆλ(a) = La/ˆr(a) = 2π/ˆω(a) . Discussion. Snapping r(a) to the nearest nonzero integer turns each rotary band into finite-window standing wave on [0, La] that completes an exact integer number of cycles and has matching phase at pa = 0 and pa = La. In the original Flux spectrum, many bands traverse the training window with fractional number of cycles, so reusing the same frequencies at larger resolutions or different ARs accumulates half-cycle phase error, which appears as spatial drift and faint striping, especially in high-frequency channels. By replacing ω(a) in Eq. (1) with their resonant counterparts ˆω(a) , Resonance 2D RoPE makes the spectrum explicitly training-window aware and prevents this fractionalcycle build-up, empirically reducing ghosting and striping artifacts when extrapolating to native-4K multi-AR grids. Resonance 2D RoPE with YaRN. Inspired by the YaRN scheme for length extrapolation of 1D RoPE [24], we further make the extrapolation band-aware. Let {H, } index spatial axes. Given the training-window length La (in patches) and the resonant frequency ˆω(a) with integer cycles ˆr(a) from the previous section, define the inference length and the extrapolation scale sa = a/La 1. We use linear ramp to map each band: γ(r; α, β) = 0, α β α 1, < α, > β, (7) and interpolate between position-interpolation scaling and no scaling using the resonant cycle count: k,yarn = (cid:0)1 γ(ˆr(a) ω(a) ; α, β)(cid:1) ˆω(a) sa + γ(ˆr(a) ; α, β) ˆω(a) . (8) k,yarn(pa) = pa ω(a) The phase is ϕ(a) k,yarn, and the complex rotation is identical to the Flux baseline. Compared to Fluxs fixed spectrum with single global NTK factor, Resonance 2D RoPE with YaRN first snaps frequencies to finite-window resonant modes and then uses the axis-wise cycle counts ˆr(a) to decide how much to scale each band for given extrapolation factor sa. This makes the positional encoding explicitly training-window aware, bandaware, and AR-aware, and empirically enables more stable 2K/4K multi-AR inference with negligible overhead. 3.2.3. SNR-Aware Huber Wavelet Training Objective Motivation. Wavelet-space objectives such as Diffusion4K demonstrate that measuring errors in multi-scale transform can materially improve 4K fidelity [42], but at native 4K we empirically observe that standard L2based training on VAE latents still suffers from three coupled pathologies. (i) Frequency imbalancenatural-image wavelet coefficients are heavy-tailed [32], so large highfrequency residuals (textures, edges, micro-geometry) are aggressively shrunk by quadratic losses, leading to over- (ii) Timestep imbalancegradients smoothing of detail. concentrate at extremely small or large noise levels, echoing Min-SNR analyses that show inefficient use of intermediate timesteps [9]. (iii) Cross-scale energy couplinglowfrequency bands dominate pixel/latent norms, so the highfrequency errors that largely govern 4K perceptual quality receive disproportionately small gradient signal [42]. To address these issues, we design single objective that is simultaneously (a) robust yet smoothusing PseudoHuber penalty that behaves like L2 near zero and L1 in the tails [30]; (b) SNR-awarewith an adaptive threshold c(t) that is small under high noise and grows as signal dominates; (c) frequency-awareby measuring residuals in an orthonormal wavelet space that decouples low and high bands; and (d) time-rebalancedvia Min-SNR weighting that emphasizes mid-SNR timesteps for stable, faster optimization [9]. These choices yield the SNR-Aware Huber Wavelet objective, drop-in replacement for standard flowmatching losses tailored to the demands of native 4K generation. Classical FM setup. Prior work on flow matching for DiTs adopts straight-line interpolation between clean latent and Gaussian noise ε [2, 19]: Under this parameterization, the DiT model predicts velocity field vθ(zt, t) and the associated data-prediction is ˆzθ(zt, t) = zt vθ(zt, t). (10) To balance gradients across timesteps, we collapse the straight-path factor and Min-SNR into single weight ω(t) = 1 min{SNR(t), γ}β. (11) , α β, 0 α < β, zt = (1 t) + ε, (0, 1), ε (0, I). (9) Table 2. Quantitative comparison under 4K res. with open-source methods. Method FID HPSv3 PickScore ArtiMuse CLIP Score Q-Align MUSIQ ScaleCrafter [10] FouriScale [12] Sana [37] Diffusion-4K [42] UltraFlux 164.02 164.71 144.17 152.43 143. 6.83 11.19 10.83 8.92 11.47 21.68 21.86 23.18 21.88 22.69 67.88 65.87 63.72 63.76 68. 33.36 33.11 35.49 33.00 34.62 4.30 4.50 4.89 4.69 4.85 38.21 38.96 45.08 27.51 46. Table 3. Quantitative comparison with Sana at 40962048 (2:1) and 20484096 (1:2) resolutions. Method FID HPSv3 Artimuse Q-Align Table 4. Quantitative comparison with Sana at 51202880 (16:9) and 59522496 (2.39:1) resolutions. Method FID HPSv3 Artimuse Q-Align Sana (2:1) UltraFlux (2:1) Sana (1:2) UltraFlux (1:2) 150.35 147.53 149.41 143.71 9.01 9.91 11.40 12.51 63.61 64.81 66.95 66. 4.80 4.86 4.85 4.89 Sana (16:9) UltraFlux (16:9) Sana (2.39:1) UltraFlux (2.39:1) 153.31 142.43 153.10 151. 9.04 9.91 8.57 11.76 63.02 67.22 62.48 66.36 4.81 4.85 4.77 4. Table 5. Quantitative comparison under 4K res. with close-source method. Method FID HPSv3 PickScore ArtiMuse CLIP Score Q-Align MUSIQ Seedream 4.0 [28] UltraFlux w. Prompt Refiner 132.87 147. 11.98 12.03 23.52 23.25 69.83 68.75 35.26 34.50 4.71 4.93 30.21 45. Here SNR(t) = (1 t)2/t2 under the straight FM path, with γ > 0 and β 0. We measure residuals in letting W() denote one-level orthonorwavelet space: mal DWT (sub-bands concatenated along channels), we compute the residual Rθ(x, ε, t) = W(ˆzθ(zt, t)) W(z). For robustness we use the Pseudo-Huber penalty ρc(r) = c2(cid:0)(cid:112)1 + (r/c)2 1(cid:1) [30] and schedule its threshold as c(t) = cmin + (cmax cmin) (cid:18) min{SNR(t), γ} γ (cid:19)α . (12) We take α [0, 1], so that c(t) is small and robust at low SNR and grows smoothly toward high SNR. Let denote the number of pixels after wavelet stacking. The per-pixel robust wavelet loss is ℓHuber (cid:0)Rθ; c(t)(cid:1) = 1 (cid:88) p=1 ρ c(t) (cid:0)Rθ,p (cid:1), (13) where Rθ,p is the p-th element of Rθ(x, ε, t). Our final objective for the DiT of our UltraFlux becomes (cid:104) L(θ) = Ez,ε,t ω(t) ℓHuber (cid:0)Rθ; c(t)(cid:1)(cid:105) . (14) Figure 5. Gemini-2.5-Flash preference comparison. We instead couple the noise and data axes in simple two-stage scheme, which we term Stage-wise Aesthetic Curriculum Learning (SACL). In Stage 1, we fine-tune the model on the full MultiAspect-4K-1M corpus with standard timestep sampling over the entire diffusion horizon, giving the DiT backbone broad coverage of aspect ratios, content types, and noise levels. Stage 2 then restricts training to high-noise bandtimesteps above threshold where the model relies most on its generative priorand to the top5% images ranked by the ArtiMuse aesthetic score. Intuitively, Stage 1 learns general 4K prior across all denoising tasks, while Stage 2 concentrates the remaining compute on the hardest regime with ultra high-aesthetic supervision. Unlike prior timestep curricula, which modulate timestep sampling under fixed data distribution [16, 38, 39], or aesthetic post-training, which ignores the different roles of noise levels [18], SACL uses stage-wise aesthetic filtering to define curriculum over high-noise timesteps, steering the global 4K generative prior toward high-aesthetic modes precisely where the sampling process is most underdetermined and yielding substantial 4K aesthetic and alignment gains at modest additional training cost (Sec. 4). Setting c(t) and β = 0 recovers the standard flowmatching objective on this path. 4. Experiment 3.2.4. Stage-wise Aesthetic Curriculum Learning Recent analyses highlight that diffusion timesteps correspond to qualitatively different generation tasks, with highnoise steps shaping global structure and low-noise steps refining local details [16, 39] . Building on this view, OmniSync [25] assigns different datasets to different timestep ranges for lip-synchronization, and several works propose timestep curricula or timestep-dependent adaptation to focus training on harder noise regimes [29, 38]. In parallel, aesthetic filtering and post-training on high-quality subsets (e.g., LAION-Aesthetics [27] and subsequent aesthetic post-training methods [18]) have proven effective for improving visual appeal, but they typically apply static highaesthetic prior uniformly across all timesteps. 4.1. Comparison with Open-source Methods. Quantitative Comparison. As shown in Table 2, we compare our method against several strong baselines: ScaleCrafter [10] and FouriScale [12] as representative trainingfree high-resolution scaling methods, Sana [37] as recent native 4K text-to-image foundation model, and Diffusion4K [42] as Flux-based model trained natively at 4K resolution. All methods are evaluated on the AestheticEval@4096 benchmark [42] using FID [11], HPSv3 [21], PickScore [17], ArtiMuse [4], CLIP Score [41], QAlign [35] and MUSIQ [15]. Beyond the square setting, Table 3 further shows that UltraFlux consistently matches or surpasses SANA across range of non-square 4K aspect ratios, delivering better semantic alignment, and perceptual Figure 6. Visual comparison of open-source methods on the Aesthetic-Eval@4096 benchmark at 40964096 resolution. quality even in challenging panoramic and wide ARs. We also provide visual comparison as show in Figure 6. Gemini-based Preference Evaluation. To complement automatic metrics, we conduct large-scale pairwise preference study using Gemini-2.5-Flash in reasoning mode as an LMM judge. For each prompt and baseline, Gemini is shown the prompt and two anonymized images (baseline vs. UltraFlux) and asked which it prefers in terms of visual appeal and prompt alignment, with ties allowed. As summarized in Figure 5, UltraFlux is preferred in 7082% of cases on visual appeal and 6089% on prompt alignment across all comparisons. 4.2. Comparison with Close-source 4K Native Generation Models As shown in Table. 5 To make the comparison with Seedream 4.0 [28] as fair as possible, we mirror its use of large-scale LLM-based prompt refiner by evaluating UltraFlux w. Prompt Refiner (Ours) with GPT-4O frontend. Under the same 40964096 evaluation protocol, Table 5 shows that UltraFlux with GPT-4O achieves slightly higher HPSv3 score than Seedream 4.0 (12.03 vs. 11.98), while remaining competitive on PickScore, ArtiMuse, and CLIP Score, and clearly surpassing it on Q-Align and MUSIQ, which better capture semantic alignment and perceptual image quality. This indicates that, once both systems are equipped with strong prompt refiners, our openTable 6. Ablation study on UltraFlux at 4K resolution training. Variant FID HPSv3 ArtiMuse 151.40 Flux+F16 VAE (base) 148.81 + SNR-HW + SNR-HW + SACL 147.32 + SNR-HW + SACL + Resonance 2D RoPE w. YARN 146.93 9.22 9.70 10.30 10.91 66.39 67.23 67.31 68.13 source UltraFlux model trained on 1M images can closely trackand in some aspects exceedthe performance of leading proprietary 4K generator. Notably, Seedream 4.0 further benefits from large-scale RL post-training, whereas our full pipeline relies solely on stage-wise SFT. 4.3. Ablation Study Starting from Flux and trained F16 VAE baseline, replacing the standard latent regression loss with our SNRAware Huber Wavelet Training (SNR-HW) objective already yields consistent gains across metrics under the same 500K data&10K steps fine-tuning schedule, indicating that SNR-aware wavelet supervision better balances high-frequency detail preservation with stable optimization. Introducing the SACL term on top of SNR-HW further improves both human preference and aesthetic scores, suggesting that stronger textimage alignment is especially beneficial at native 4K. Finally, equipping UltraFlux with Resonance 2D RoPE and YARN produces the best overall configuration, delivering monotonically improved perceptual and aesthetic metrics while also reducing FID. Taken together, these ablations show that the proposed objective, alignment loss, and positional encoding contribute complementary gains rather than merely redistributing performance among metrics. 5. Conclusion UltraFlux couples carefully curated MultiAspect-4K-1M corpus with AR-aware positional encoding, reconstruction, and optimization components into unified framework for native 4K multi-AR generation. This datamodel co-design yields state-of-the-art fidelity, aesthetic quality, and text alignment on standard 4K benchmarks while remaining computationally practical."
        },
        {
            "title": "References",
            "content": "[1] Omer Bar-Tal, Lior Yariv, Yaron Lipman, and Tali Dekel. Multidiffusion: Fusing diffusion paths for controlled image generation. 2023. 2, 3 [2] Stephen Batifol, Andreas Blattmann, Frederic Boesel, Saksham Consul, Cyril Diagne, Tim Dockhorn, Jack English, Zion English, Patrick Esser, Sumith Kulal, et al. Flux. 1 kontext: Flow matching for in-context image generation and editing in latent space. arXiv e-prints, pages arXiv2506, 2025. 2, 5, 6 [3] Shuo Cao, Nan Ma, Jiayang Li, Xiaohui Li, Lihao Shao, Kaiwen Zhu, Yu Zhou, Yuandong Pu, Jiarui Wu, Jiaquan Wang, Bo Qu, Wenhai Wang, Yu Qiao, Dajuin Yao, and Yihao Liu. Artimuse: Fine-grained image aesthetics assessment with joint scoring and expert-level understanding, 2025. 4, 2 [4] Shuo Cao, Nan Ma, Jiayang Li, Xiaohui Li, Lihao Shao, Kaiwen Zhu, Yu Zhou, Yuandong Pu, Jiarui Wu, Jiaquan Wang, et al. Artimuse: Fine-grained image aesthetics assessment with joint scoring and expert-level understanding. arXiv preprint arXiv:2507.14533, 2025. 7 [5] Junsong Chen, Chongjian Ge, Enze Xie, Yue Wu, Lewei Yao, Xiaozhe Ren, Zhongdao Wang, Ping Luo, Huchuan Lu, and Zhenguo Li. Pixart-σ: Weak-to-strong training of diffusion transformer for 4k text-to-image generation. In European Conference on Computer Vision, pages 7491. Springer, 2024. 2, 3, 4 [6] Patrick Esser, Sumith Kulal, Andreas Blattmann, Rahim Entezari, Jonas Muller, Harry Saini, Yam Levi, Dominik Lorenz, Axel Sauer, Frederic Boesel, et al. Scaling rectified flow transformers for high-resolution image synthesis. In Forty-first international conference on machine learning, 2024. 2 [7] Lanqing Guo, Yingqing He, Haoxin Chen, Menghan Xia, Xiaodong Cun, Yufei Wang, Siyu Huang, Yong Zhang, Xintao Wang, Qifeng Chen, et al. Make cheap scaling: self-cascade diffusion model for higher-resolution adaptation. In European conference on computer vision, pages 39 55. Springer, 2024. 2, of the IEEE/CVF conference on computer vision and pattern recognition, pages 66036612, 2024. 2, 3 [9] Tiankai Hang, Shuyang Gu, Chen Li, Jianmin Bao, Dong Chen, Han Hu, Xin Geng, and Baining Guo. Efficient diffusion training via min-snr weighting strategy. In Proceedings of the IEEE/CVF international conference on computer vision, pages 74417451, 2023. 2, 6 [10] Yingqing He, Shaoshu Yang, Haoxin Chen, Xiaodong Cun, Menghan Xia, Yong Zhang, Xintao Wang, Ran He, Qifeng Chen, and Ying Shan. Scalecrafter: Tuning-free higherresolution visual generation with diffusion models. In The Twelfth International Conference on Learning Representations, 2024. 7 [11] Martin Heusel, Hubert Ramsauer, Thomas Unterthiner, Bernhard Nessler, and Sepp Hochreiter. Gans trained by two time-scale update rule converge to local nash equilibrium. Advances in neural information processing systems, 30, 2017. 7 [12] Linjiang Huang, Rongyao Fang, Aiping Zhang, Guanglu Song, Si Liu, Yu Liu, and Hongsheng Li. Fouriscale: frequency perspective on training-free high-resolution image synthesis. In European conference on computer vision, pages 196212. Springer, 2024. 2, 3, 7 [13] Ideogram. Upscale ideogram documentation. - https : / / docs . ideogram . ai / using - ideogram / features - and - tools / upscale, 2025. Accessed: 2025-11-10. 3 [14] Jinho Jeong, Sangmin Han, Jinwoo Kim, and Seon Joo Kim. Latent space super-resolution for higher-resolution image In Proceedings of the generation with diffusion models. Computer Vision and Pattern Recognition Conference, pages 23552365, 2025. 2, 3 [15] Junjie Ke, Qifei Wang, Yilin Wang, Peyman Milanfar, and Feng Yang. Musiq: Multi-scale image quality transformer. In Proceedings of the IEEE/CVF international conference on computer vision, pages 51485157, 2021. 7 [16] Jin-Young Kim, Hyojun Go, Soonwoo Kwon, and HyunGyoon Kim. Denoising task difficulty-based curriculum for training diffusion models. arXiv preprint arXiv:2403.10348, 2024. 7 [17] Yuval Kirstain, Adam Polyak, Uriel Singer, Shahbuland Matiana, Joe Penna, and Omer Levy. Pick-a-pic: An open dataset of user preferences for text-to-image generation. 2023. [18] Zhanhao Liang, Yuhui Yuan, Shuyang Gu, Bohan Chen, Tiankai Hang, Mingxi Cheng, Ji Li, and Liang Zheng. Aesthetic post-training diffusion models from generic preferences with step-by-step preference optimization. In Proceedings of the Computer Vision and Pattern Recognition Conference, pages 1319913208, 2025. 7 [19] Yaron Lipman, Ricky TQ Chen, Heli Ben-Hamu, Maximilian Nickel, and Matt Le. Flow matching for generative modeling. arXiv preprint arXiv:2210.02747, 2022. 6 [8] Moayed Haji-Ali, Guha Balakrishnan, and Vicente Ordonez. Elasticdiffusion: Training-free arbitrary size image generation through global-local content separation. In Proceedings [20] Songhua Liu, Zhenxiong Tan, and Xinchao Wang. Clear: Conv-like linearization revs pre-trained diffusion transformers up. arXiv preprint arXiv:2412.16112, 2024. 2 [21] Yuhang Ma, Xiaoshi Wu, Keqiang Sun, and Hongsheng Li. Hpsv3: Towards wide-spectrum human preference score, 2025. 7 [22] MidJourney. Midjourney: Ai image generation. https: //www.midjourney.com/, 2025. Accessed: 2025-1110. [23] William Peebles and Saining Xie. Scalable diffusion models with transformers. In Proceedings of the IEEE/CVF international conference on computer vision, pages 41954205, 2023. 2 [24] Bowen Peng, Jeffrey Quesnelle, Honglu Fan, and Enrico Shippole. Yarn: Efficient context window extension of large language models. arXiv preprint arXiv:2309.00071, 2023. 2, 6 [25] Ziqiao Peng, Jiwen Liu, Haoxian Zhang, Xiaoqiang Liu, Songlin Tang, Pengfei Wan, Di Zhang, Hongyan Liu, and Jun He. Omnisync: Towards universal lip synchronization via diffusion transformers. arXiv preprint arXiv:2505.21448, 2025. 7 [26] Chitwan Saharia, William Chan, Saurabh Saxena, Lala Li, Jay Whang, Emily Denton, Kamyar Ghasemipour, Raphael Gontijo Lopes, Burcu Karagol Ayan, Tim Salimans, et al. Photorealistic text-to-image diffusion models with deep language understanding. Advances in neural information processing systems, 35:3647936494, 2022. 3 [27] Christoph Schuhmann. https : / / laion.ai/blog/laion-aesthetics/, 2022. Accessed: 2025-11-13. Laion-aesthetics. [28] Team Seedream, Yunpeng Chen, Yu Gao, Lixue Gong, Meng Guo, Qiushan Guo, Zhiyao Guo, Xiaoxia Hou, Weilin Huang, Yixuan Huang, et al. Seedream 4.0: Toward nextarXiv preprint generation multimodal image generation. arXiv:2509.20427, 2025. 3, 7, 8 T-lora: [29] Vera Soboleva, Aibek Alanov, Andrey Kuznetsov, and Single image diffusion arXiv preprint Konstantin Sobolev. model customization without overfitting. arXiv:2507.05964, 2025. 7 [30] Yang Song and Stefano Ermon. Improved techniques for training score-based generative models. Advances in neural information processing systems, 33:1243812448, 2020. 6, [31] Jianlin Su, Murtadha Ahmed, Yu Lu, Shengfeng Pan, Wen Bo, and Yunfeng Liu. Roformer: Enhanced transformer with rotary position embedding. Neurocomputing, 568:127063, 2024. 5 [32] Martin Wainwright and Eero Simoncelli. Scale mixtures of gaussians and the statistics of natural images. Advances in neural information processing systems, 12, 1999. 6 [33] Ao Wang, Lihao Liu, Hui Chen, Zijia Lin, Jungong Han, and Guiguang Ding. Yoloe: Real-time seeing anything, 2025. 4 [34] Suyuchen Wang, Ivan Kobyzev, Peng Lu, Mehdi Rezagholizadeh, and Bang Liu. Resonance rope: Improving context length generalization of large language models. arXiv preprint arXiv:2403.00071, 2024. 5 [35] Haoning Wu, Zicheng Zhang, Weixia Zhang, Chaofeng Chen, Chunyi Li, Liang Liao, Annan Wang, Erli Zhang, Wenxiu Sun, Qiong Yan, Xiongkuo Min, Guangtai Zhai, Q-align: Teaching lmms for visual and Weisi Lin. arXiv preprint scoring via discrete text-defined levels. arXiv:2312.17090, 2023. Equal Contribution by Wu, Haoning and Zhang, Zicheng. Project Lead by Wu, Haoning. Corresponding Authors: Zhai, Guangtai and Lin, Weisi. 7 [36] Haoning Wu, Zicheng Zhang, Weixia Zhang, Chaofeng Chen, Chunyi Li, Liang Liao, Annan Wang, Erli Zhang, Wenxiu Sun, Qiong Yan, Xiongkuo Min, Guangtai Zhai, Q-align: Teaching lmms for visual and Weisi Lin. arXiv preprint scoring via discrete text-defined levels. arXiv:2312.17090, 2023. Equal Contribution by Wu, Haoning and Zhang, Zicheng. Project Lead by Wu, Haoning. Corresponding Authors: Zhai, Guangtai and Lin, Weisi. 4, 2 [37] Enze Xie, Junsong Chen, Junyu Chen, Han Cai, Haotian Tang, Yujun Lin, Zhekai Zhang, Muyang Li, Ligeng Zhu, Yao Lu, et al. Sana: Efficient high-resolution image synarXiv preprint thesis with linear diffusion transformers. arXiv:2410.10629, 2024. 2, 3, 7 [38] Tianshuo Xu, Peng Mi, Ruilin Wang, and Yingcong Chen. Towards faster training of diffusion models: An inspiarXiv preprint ration of consistency phenomenon. arXiv:2404.07946, 2024. [39] Xuanyu Yi, Zike Wu, Qingshan Xu, Pan Zhou, Joo-Hwee Lim, and Hanwang Zhang. Diffusion time-step curricuIn Proceedings of lum for one image to 3d generation. the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 99489958, 2024. 7 [40] Ruonan Yu, Songhua Liu, Zhenxiong Tan, and Xinchao Wang. Ultra-resolution adaptation with ease. arXiv preprint arXiv:2503.16322, 2025. 2 [41] Beichen Zhang, Pan Zhang, Xiaoyi Dong, Yuhang Zang, and Jiaqi Wang. Long-clip: Unlocking the long-text capability of clip. In European conference on computer vision, pages 310325. Springer, 2024. 7 [42] Jinjin Zhang, Qiuyu Huang, Junjie Liu, Xiefan Guo, and Di Huang. Diffusion-4k: Ultra-high-resolution image synthesis with latent diffusion models. In Proceedings of the Computer Vision and Pattern Recognition Conference, pages 23464 23473, 2025. 2, 3, 4, 5, 6, 7 [43] Jinjin Zhang, Qiuyu Huang, Junjie Liu, Xiefan Guo, and Di Huang. Ultra-high-resolution image synthesis: Data, method and evaluation. arXiv preprint arXiv:2506.01331, 2025. 1, 2 [44] Kechi Zhang, Ge Li, Huangzhao Zhang, and Zhi Jin. Hirope: Length extrapolation for code models using hierarchical position. arXiv preprint arXiv:2403.19115, 2024. 2 [45] Shen Zhang, Zhaowei Chen, Zhenyu Zhao, Yuhao Chen, Yao Tang, and Jiajun Liang. Hidiffusion: Unlocking higherresolution creativity and efficiency in pretrained diffusion models. In European Conference on Computer Vision, pages 145161. Springer, 2024. 2, [46] Mao Zheng, Zheng Li, Bingxin Qu, Mingyang Song, Yang Du, Mingrui Sun, and Di Wang. Hunyuan-mt technical report. arXiv preprint arXiv:2509.05209, 2025. 5 UltraFlux: Data-Model Co-Design for High-quality Native 4K Text-to-Image Generation across Diverse Aspect Ratios"
        },
        {
            "title": "Supplementary Overview",
            "content": "This document complements the main paper by (i) clarifying the regime-level novelty of UltraFlux as datamodel co-designed system for native 4K, multi-AR text-to-image generation, and (ii) providing the analyses, metrics, and implementation details needed to faithfully reproduce and stress-test our setup. Rather than introducing isolated primitives, we make explicit how dataset curation, positional representation, VAE compression, and optimization objectives must be co-designed to operate robustly in the 4K, multiAR regime. Organization. Sec. S1 (Clarifying Novelty and Data Model Co-Design) positions UltraFlux as regimelevel, system-oriented contribution and introduces 22 datamodel ablation that disentangles the roles of MULTIASPECT-4K-1M and the UltraFlux architecture/loss. Sec. S2 (Implementation Details) expands on the dataset pipeline, DiT training schedule, VAE post-training recipe, and key hyperparameters, including reconstruction metrics for our F16 VAE. Sec. S3 and Sec. S4 provide focused analyses of Resonance 2D RoPE with YaRN and wavelet-space statistics of 4K VAE latents, together with qualitative 4K visualizations, motivating our positional design and SNR-Aware Huber Wavelet objective. Sec. S5S7 report additional ablations, 4K runtime measurements, and more extensive quantitative comparisons at challenging wide aspect ratios, as well as extended visual comparisons against open-source baselines. Sec. S8 discusses the main limitations of UltraFlux in terms of sampling cost, memory footprint, and aesthetic ceiling, while Sec. S9 details our Gemini-based preference evaluation and GPT-4o promptrefiner setup used for large-scale automatic assessment and prompt expansion. Taken together, these sections are intended to show that UltraFlux is regime-level, system-oriented contribution rather than mere aggregation of existing tricks, and to document the concrete choices required to make native-4K, multi-AR generation work in practice. 6. Clarifying Novelty and DataModel Co-"
        },
        {
            "title": "Design",
            "content": "The main paper positions UltraFlux as datamodel codesigned recipe for native-4K, multi-AR text-to-image generation. Several of the building blocksresonance-style rotary encodings, wavelet objectives, Min-SNR weighting, and aesthetic curriculaindeed draw inspiration from prior Table 7. 22 datamodel co-design ablation. A: baseline; B: data only; C: model/loss only; D: full co-design. Variant Dataset Model / Loss FID HPSv3 D Diffusion-4K-v2 [43] MultiAspect-4K-1M Diffusion-4K-v2 [43] MultiAspect-4K-1M Flux, latent L2 Flux, latent L2 UltraFlux UltraFlux 152.09 151.41 147.41 145.81 8.57 9.17 10.03 10. work. Our contribution is not to claim each primitive as standalone invention, but to show that: (i) at 4K with diverse aspect ratios, positional encoding, VAE compression, and optimization objectives form coupled regime that existing methods treat largely in isolation; and (ii) carefully unified design across dataset, representation, and loss yields behaviors that cannot be reproduced by swapping in any single component in isolation. To make this clearer, we provide in this supplement: data-model ablation  (Table 7)  showing that neither stronger 4K dataset nor architectural changes alone are sufficient: MultiAspect-4K-1M and UltraFlux each yield modest gains in isolation, while their combination delivers the full non-additive improvements in 4K, multi-AR fidelity. One-dimensional and two-dimensional diagnostics of Resonance 2D RoPE with YaRN (Sec. 8), analyzing cycle snapping, phase closure on the training window, and the stability of phase geometry under aspect-ratio extrapolation. Wavelet-space statistics of 4K VAE latents (Sec. 9) that empirically confirm the low-frequencydominated yet heavy-tailed structure motivating our SNR-Aware Huber Wavelet objective, clarifying why robust, SNR-aware wavelet loss is better aligned with the 4K regime than pure latent L2 objective. Expanded implementation details for the dataset pipeline, DiT training, and VAE post-training (Sec. S2), to facilitate faithful reproduction of our 4K native, multi-AR training setup. We hope these analyses better convey that UltraFlux is regime-level, system-oriented contribution rather than mere aggregation of existing tricks. 7. Implementation Details 7.1. Dataset Pipeline Flat-region detection. For each image, we first partition it into non-overlapping 240 240 patches and quantify the edge richness of every patch with Sobel-based score, Sflat = Var (cid:18)(cid:113) (xI)2 + (yI) (cid:19) . Patches with Sflat < 800 are flagged as texture-poor, and any image in which more than 50% of the patches are flagged is removed from the dataset. The patch-level threshold of 800 and the 50% image-level ratio are selected empirically via manual inspection of edge-statistic histograms and visual audits. This conservative configuration effectively filters out images dominated by large uniform regions while still retaining plausible low-texture content such as sky and water, ensuring that the remaining images maintain sufficient edge and texture diversity for high-fidelity generation. Information Entropy Filtering. Each image is analyzed for its Shannon entropy to quantify the amount of information it contains. The Shannon entropy of an image is defined as: = (cid:88) i= p(xi) log2 p(xi), where p(xi) denotes the probability of the pixel value xi within the image. Images with an entropy value < 7.0 are flagged as texture-poor, and any image in which < 7.0 is removed from the dataset. The threshold of 7.0 is selected empirically based on the observed distribution of entropy values across the dataset. This threshold effectively filters out images with insufficient texture or information, ensuring that the remaining images exhibit adequate variability for high-quality processing while preserving content diversity. Image Quality Filtering. To ensure semantic quality, we compute the quality score for each image using QAlign [36]. Images with quality score greater than 4.0 are retained, while those below this threshold are discarded. This threshold is determined empirically based on the distribution of quality scores across data sources, ensuring that only images with sufficient semantic clarity are kept for further analysis. Aesthetic Quality Filtering. For aesthetic evaluation, we use the ArtiMuse [3] model to compute aesthetic scores for each image. Only the top 30% of images, based on their aesthetic rating, are preserved. This strategy ensures that images with higher aesthetic appeal are prioritized, while lower-rated images are excluded from the dataset. This filtering method helps maintain diverse and aesthetically pleasing selection of images for further processing. 7.2. Training Details Table 8. Reconstruction metrics of F16 VAEs on the Aesthetic4K@4096 Eval set [42]. Model rFID NMSE PSNR SSIM LPIPS Flux-VAE-F16 [42] Flux-VAE-F16-SC [43] UltraFlux-F16-VAE 2.201 0.588 0. 0.01522 0.00736 0.00657 26.90 30.19 30.70 0.784 0.846 0.852 0.168 0.097 0.102 update all DiT blocks end-to-end. Training is conducted on 8NVIDIA H800 GPUs using DeepSpeed ZeRO-2 (without CPU offload). We choose ZeRO-2 because it shards optimizer states and gradients without partitioning model parameters, which substantially reduces memory usage while yielding higher throughput than ZeRO-3 in our setting, enabling efficient 4K training. We use AdamW with learning rate of 1 106 and an effective batch size of 64; the full training run takes roughly 12 days. We adopt two-stage training schedule, with approximately 30K steps in the first stage and further 2K steps in the second fine-tuning stage (Stage-wise Aesthetic Curriculum Learning). To support multi-AR native 4K generation, we adopt bucketed resolution scheme: for each image, we snap its resolution to the nearest target from fixed set of landscape buckets (e.g., 5120 2880 for 16:9, 4704 3136 for 3:2), portrait buckets (e.g., 2880 5120 for 9:16, 3136 4704 for 2:3), and single square bucket at 3840 3840, then center-crop and resize the image to the selected bucket resolution. VAE Training. For VAE post-training, we fine-tune the decoder on the proposed MultiAspect-4K-1M dataset, retaining the top 50% of images according to the flatness score and training at 512 512 resolution with an effective batch size of 384. We use AdamW with learning rate of 1 105. VAE reconstruction metrics and post-training gains. Table 8 quantitatively compares our UltraFlux-F16-VAE with the Flux-VAE-F16 baseline on the AESTHETIC4K@4096 evaluation set [42]. Despite using the same F16 compression ratio, UltraFlux-F16-VAE achieves substantially better reconstruction quality across all metrics. These consistent gains indicate that our post-trained decoder not only preserves low-frequency structure, but also better reconstructs high-frequency details that are typically washed out under aggressive F16 compression. Combined with the wavelet-space analysis above, this suggests that the proposed post-training scheme effectively aligns the VAE with the heavy-tailed, cross-scale statistics of native 4K images, narrowing the reconstruction gap. 8. Analyses of Resonance 2D RoPE with YaRN DiT Training. We train UltraFlux, large Flux-based DiT model for native 4K text-to-image generation. During DiT training, we freeze the VAE and text encoders and Figure 7 gives 1D band-wise diagnostic of Resonance 2D RoPE on single spatial axis, which is then used by YaRN. In panel (a), we plot the cycles completed in the training Figure 7. 1D band-wise analysis of Resonance 2D RoPE with YaRN. (a) Number of cycles rk in the training window and their integersnapped counterparts ˆrk. (b) Phase-closure error ϕk at = L, showing exact closure for Resonance RoPE. (c) Phase difference ϕk(p) between baseline and Resonance under 2 length extrapolation, illustrating how fractional cycles in the baseline accumulate into large out-of-distribution phase errors. Figure 8. Heightwidth cosine phase patterns for representative rotary band under different aspect ratios. Each panel displays (h, w) = cos(h ωH + ωW ) evaluated on an HW patch grid, with the top row using the Flux baseline frequencies and the bottom row using our resonant frequencies. The first column shows the training resolution (6464 patches, AR 1:1); the remaining columns visualize extrapolation to 12864 (2:1), 64128 (1:2), and 12832 (4:1). At train scale, baseline and Resonance RoPE induce similar diagonal stripe patterns, indicating that resonance acts as mild reparameterization of the spectrum. Under aspect-ratio extrapolation, the baseline stripes rotate and change spacing more aggressively, while our resonant variant maintains more regular, coherent patterns along both height and width, qualitatively echoing the improved phase stability observed in the 1D analyses. window of length La by each rotary band, r(a) = La ω(a) 2π , together with their snapped counterparts = max(cid:0)1, round(r(a) ˆr(a) The Flux baseline (blue) yields dense sequence of noninteger r(a) , whereas Resonance RoPE (orange) projects evk )(cid:1). ery band onto the nearest nonzero integer ˆr(a) , producing piecewise-constant spectrum that leaves low-frequency modes almost unchanged and regularizes higher ones. Panel (b) measures phase closure at the boundary of the training window. For each band we evaluate the phase at pa = La using both the original frequency ω(a) and the resonant frequency ˆω(a) = 2π ˆr(a) La , Table 9. Summary of training-related hyperparameters for UltraFlux and associated components. Values are left blank to be filled with the final configuration. Component Hyperparameter Stage-wise Aesthetic Curriculum Stage 1 timestep range Stage 2 timestep range Stage 2 aesthetic filter (ArtiMuse percentile) Value 0999 0459 top-5% DiT objective Wavelet type / number of levels Pseudo-Huber thresholds (cmin, cmax) Haar, J=1 cmin0.2, cmax1.0 Resonance 2D RoPE with YaRN F16 VAE post-training Multi-AR 4K DiT training RoPE base NTK scaling factor η YaRN ramp parameters (α, β) Maximum extrapolation scale sa = a/La Training resolution Global batch size (images/step) Optimizer / learning rate / weight decay Loss weights (λwav, λperc, λL2 ) Landscape target sizes (WH) Portrait target sizes (WH) Square target sizes (WH) 10,000 1.0 (1.25, 0.75) 2.0 512 512 384 AdamW, 1 104, 1 102 0.2, 0.1, 1 54403072, 51843264, 49923328 47363520, 58242880, 62722688 55683008, 63362624, 56323008 46083648 30725440, 36484608, 35204736 33284992 40964096 and plot the absolute phase mismatch ϕk between pa = 0 and pa = La. The baseline shows up to several radians of mismatch, while Resonance RoPE drives ϕk to zero for all bands, confirming that every component becomes an exact standing wave on [0, La]. Panel (c) visualizes the phase difference between the baseline and Resonance RoPE under 2 resolution extrapolation. For positions pa [0, 2La] we compute ϕk(pa) = wrap(cid:0)pa ω(a) pa ˆω(a) (cid:1), where wrap() maps angles to [π, π], and plot ϕk(pa) as heatmap over (k, pa). The discrepancy is small near the training window but grows systematically with both position and frequency, illustrating how fractional cycles in the original spectrum accumulate into large out-of-distribution Since YaRN subsequently applies bandphase errors. wise scaling to these already integer-cyclealigned modes, the combined Resonance 2D RoPE with YaRN inherits training-window awareness while achieving stable, ARrobust extrapolation in 2D. 2D spatial visualization. Figure 7 analyzes Resonance 2D RoPE with YaRN along single spatial axis. To understand how these band-wise changes translate into actual imageplane geometry, we further visualize 2D cosine patterns in Figure 8. For representative rotary band, we construct (h, w) = cos(cid:0)h ωH + ωW (cid:1), on different heightwidth grids, where (ωH , ωW ) are taken either from the Flux baseline or from the resonant frequencies. The leftmost column corresponds to the training resolution (6464 patches, AR 1:1), while the remaining columns show extrapolation to 12864 (2:1), 64128 (1:2), and 12832 (4:1). At the training scale, baseline and Resonance RoPE produce very similar diagonal stripe patterns, consistent with the fact that snapping rk to ˆrk only slightly perturbs low-frequency modes. Across more extreme aspect ratios, however, the baseline stripes exhibit more pronounced changes in orientation and spacing, whereas the Resonance patterns remain more regular and coherent. This 2D view complements the 1D diagnostics: once each band forms an integer-cycle standing wave on the training window, spatial phase geometry varies more smoothly when scaling to multi-AR 2K/4K grids. 9. Wavelet-Space Statistics of 4K VAE Latents Figure 9. Wavelet-space statistics of 4K VAE latents. We show log-count histograms of absolute coefficients for the LL, LH, HL, and HH subbands over 400 samples. Most energy resides in the LL band, while high-frequency bands carry sparse but largemagnitude coefficients, indicating heavy-tailed behavior. This cross-scale structure motivates our SNR-Aware Huber Wavelet objective. In the main paper (Section 3.2.3, SNR-Aware Huber Wavelet Training Objective) we argue that native 4K generation suffers from (i) frequency imbalance and (ii) crossscale energy coupling: low-frequency bands dominate latent norms, while high-frequency, perceptually critical structures appear as sparse, large-magnitude coefficients that are poorly handled by purely quadratic losses. Here we provide an empirical characterization of this effect in the VAE latent space used by UltraFlux. We sample 400 images from MULTIASPECT-4K-1M, encode them with our F16 VAE, and apply one-level orthonormal DWT to the resulting latents. Figure 9 shows log-count histograms of the absolute wavelet coefficients in the LL, LH, HL, and HH subbands. The energy distribution is strongly skewed across the LL band accounts for 87.4% of the total lascales: tent energy (mean per-band energy 3.55), while each highfrequency band contributes only 3.54.7%. At the same time, all bands exhibit pronounced heavy tails. For example, in the LH band 20.8% of coefficients satisfy > 0.5, Table 10. Inference time per 4K sample at 40964096 resolution. ScaleCrafter FouriScale Sana UltraFlux Time (s) 195.67 216. 48.42 49.50 Table 11. Quantitative comparison with SOTA methods at different aspect ratios, including 40962048 (2:1), 20484096 (1:2), 51202880 (16:9), and 59522496 (2.39:1) resolutions. Aspect Ratio Method FID HPSv3 Artimuse Q-Align 3.2% exceed > 1.0, and values up to 7.2 occur; HL and HH show similar tail behavior. These statistics quantitatively support the motivation in the main paper: at 4K resolution, VAE latents are dominated by low-frequency energy, yet contain sparse, largemagnitude high-frequency coefficients that encode textures, edges, and micro-geometry. Under standard L2 latent loss, these heavy-tailed residuals are aggressively shrunk, and gradients are largely governed by the LL band, leading to over-smoothing of detail and weak supervision for highfrequency errors. This empirical evidence motivates our design of the SNR-Aware Huber Wavelet objective, which replaces pure L2 with wavelet-space, Pseudo-Huber penalty with SNR-dependent thresholds to better balance lowand high-frequency reconstruction errors in the 4K regime. 10. Additional Ablations Figure 10 provides visual counterpart to the analyses in Sec. 8. The Flux.1 2D RoPE baseline without scaling (a) reuses its training-time spectrum at 4K and produces noticeable geometric drift: objects appear slightly stretched or misaligned and backgrounds show faint striping. Introducing YaRN scaling alone (b) reduces these artifacts by making the spectrum resolution-aware, but residual phase misalignment still leads to mild warping along long contours. Our Resonance 2D RoPE with YaRN (c) first snaps each band to an integer-cycle standing wave on the training window and then applies band-wise YaRN scaling, yielding visibly more stable composition and cleaner high-frequency details, especially in the delicate structures of fur, foliage, and planetary rings. 11. Efficiency Comparison Table 10 reports the wall-clock time required for each method to generate single 40964096 sample under the same hardware and sampler configuration. UltraFlux and Sana operate in similar runtime regime, while both are several times faster than ScaleCrafter and FouriScale, whose 4K pipelines incur substantially higher latency. In other words, UltraFlux achieves our best 4K fidelity and aesthetic metrics without introducing extra inference cost relative to the strongest open baseline, and remains markedly more efficient than earlier 4K upsampling-based approaches. 2:1 1:2 16:9 2.39:1 ScaleCrafter FouriScale Sana UltraFlux ScaleCrafter FouriScale Sana UltraFlux ScaleCrafter FouriScale Sana UltraFlux ScaleCrafter FouriScale Sana UltraFlux 168.29 169.30 150.36 147.54 157.21 159.87 149.42 143.71 175.97 173.84 153.31 142.43 196.60 196.30 153.10 151. 6.26 5.89 9.01 9.91 8.92 8.09 11.40 12.51 5.30 5.46 9.04 9.92 3.69 3.61 8.57 11.76 65.62 64.29 63.61 64.81 68.74 66.64 66.95 66. 65.05 64.14 63.02 67.22 64.02 63.09 62.48 66.36 4.29 4.38 4.81 4.86 4.41 4.38 4.86 4.89 4.14 4.36 4.82 4.85 4.00 4.14 4.77 4. 12. More Quantitative Comparison with SOTA"
        },
        {
            "title": "Methods at Wide Aspect Ratios",
            "content": "To provide more comprehensive evaluation of performance at challenging wide aspect ratios, including 2:1 (40962048), 1:2 (20484096), 16:9 (51202880), and the cinematic 2.39:1, we compare with SOTA methods across four distinct acpect ratios and resolutions, as detailed in Table 11. The results show that UltraFlux consistently surpasses the performance all competing methods across all tested aspect ratios and metrics, demonstrating its effectiveness in generating high-quality images for diverse wideformat scenarios. 13. Qualitative Comparison with SOTA Methods at Wide Aspect Ratios This section provides visual comparisons with SOTA methods at wide aspect ratios, complementing our quantitative analysis in Table 11. The results are presented in Fig. 14 (1:2), Fig. 15 (2:1), Fig. 16 (16:9) and Fig. 17 (1:2.39), respectively. At the 1:2 aspect ratio, all methods produce visually plausible results without severe artifacts. However, our results are more visually appealing with better composition and aesthetic quality. In the 2:1 case, methods such as Scalecrafter and Fouriscale exhibit noticeable structural distortions and artifacts, while Sana also shows visible flaws. In contrast, our method generates remarkably natural and coherent images. At the 2.39:1 ultra-wide ratio, both Scalecrafter and Fouriscale suffer from mild misalignment with text prompts as well as detail degradation. Our results not only avoid these issues but also outperform Sana in overall visual quality. Figure 10. Qualitative effect of Resonance 2D RoPE with YaRN. We compare three positional encodings at native 4K resolution for the same prompts. (a) Flux.1 2D RoPE baseline without any scaling at inference time, which tends to exhibit geometric drift and mild striping or warping artifacts in both foreground objects and backgrounds. (b) 2D RoPE with YaRN scaling, which stabilizes the overall layout but still shows subtle distortions along long contours and in extreme regions of the image. (c) Our proposed Resonance 2D RoPE with YaRN, which yields the most coherent global geometry and sharper, more regular fine structures (e.g., ring edges and tree trunks). These observations demonstrate that our approach consistently maintains state-of-the-art performance and high visual fidelity across spectrum of challenging aspect ratios. 14. Additional Visual Comparison With Open-"
        },
        {
            "title": "Source methods",
            "content": "In Figures 1113, additional visual comparisons are provided. From the results, we observe that ScaleCrafter sometimes produces images with noticeable distortions, while FouriScale occasionally struggles to fully capture textual content. The images generated by SANA, on the other hand, can appear somewhat overly smoothed or oily. In contrast, compared to Diffusion-4K, our method consistently delivers higher-quality images with more visually appealing results, offering more pleasant overall experience. 15. Limitations Although UltraFlux substantially improves native-4K, multi-AR generation over prior open-source baselines, the system still has several practical limitations. Sampling cost and memory footprint. First, UltraFlux is not yet efficient 4K generator. Even with the F16 VAE and our optimized DiT backbone, sampling at native 4K with 5060 flow-matching steps remains noticeably slower than 1K-class models and requires high-end 50GB GPU to avoid aggressive offloading. This compute and memory footprint limits deployment to researchor datacentergrade hardware, and makes large-scale 4K sampling expensive compared to lower-resolution pipelines or distilled student models. Aesthetic ceiling and robustness. Second, while our datamodel co-design delivers consistent gains in automatic metrics and Gemini-based preference studies, the aesthetic quality is not uniformly top-tier across all prompts and domains. In challenging cases, UltraFlux can still produce occasional over-smoothed textures, minor geometric artifacts, or compositions that are less polished than those from heavily engineered proprietary systems. Our co-design focuses on the 4K + multi-AR regime rather than absolute peak aesthetics, and there remains headroom for further preference alignment, prompt understanding, and content diversity. Scope of co-design. Finally, the present work primarily codesigns dataset, positional encoding, VAE, and loss under single large DiT backbone. We do not address complementary axes such as sparse or low-rank attention, lightweight decoders, or distillation to smaller 4K models, which could significantly reduce memory usage and latency. Extending UltraFlux-style co-design to more parameter-efficient architectures and to broader data domains (e.g., specialized scientific or medical imagery) is an important direction for future work. 16. Details About Gemini-based Preference Evaluation. In this section, we provide additional details on the Geminibased preference evaluation used to assess the visual quality and prompt alignment of different models. As part of this evaluation, Gemini-2.5-Flash, in reasoning mode, is employed to judge image pairs based on their aesthetic appeal and alignment with the given prompt. The following is an example of the exact prompt used for evaluating aesthetic preferences in our study. For each image pair, Gemini is asked to assess various aspects such as composition, sharpness, lighting, and overall visual appeal, ensuring that the evaluation process is both consistent and reproducible. Prompt 16.1 (Pairwise Preference for Aesthetics) You are an impartial image aesthetics judge. Compare Image and Image B, and decide which one better fits human aesthetic preferences overall. Evaluate: Composition Sharpness / clarity Lighting / contrast Color harmony Noise / compression artifacts Overall visual appeal Be decisive; only return \"tie\" if the two images are nearly identical in quality. Return strictly in the following JSON format (no explanations, no extra text): { } \"preferred\": \"A tie\", \"a_score\": 0-100, \"b_score\": 0-100, \"reasons\": \"short explanation\" 17. Details About Prompt Refiner using GPT4O. Prompt 17.1 (GPT-4O Prompt Refining Process) System prompt: You are senior prompt refiner for AI image generation. Expand each short prompt into single rich, high-aesthetic prompt. Requirements: Length: 55100 words; one line per item; no newlines, numbering, or quotes. Preserve the original subject and intent; do not invent brands, copyrighted IP, or named people. Composition: camera angle, shot size/framing, focal length or lens type, foreground/midground/background, environment context. Subject attributes: age range, gender expression where implied, appearance details (hair/eyes/skin or material), clothing/fabric, pose, expression/action. Lighting and color: key light quality/direction, color temperature, time of day/season/weather, palette or dominant hues. Style/medium: photographic or cinematic unless the input implies another medium; mention film look or post-processing if appropriate. Quality: tasteful, coherent, non-repetitive language; avoid keyword stuffing. User prompt: Short prompts: {list of input prompts} Language: Write outputs (Chinese/English); one line per item. For each input, produce exactly one refined prompt; avoid lists, bullets, or line breaks inside items. Expected output format: in [language] [ ] \"Refined prompt 1\", \"Refined prompt 2\", ... To further refine the quality of input prompts, we employ GPT-4o as front-end for our UltraFlux w. Prompt Refiner (Ours) configuration. The process of prompt refinement involves transforming short and concise prompts into more detailed, high-aesthetic descriptions suitable for image generation tasks. The GPT-4o model expands each input prompt into rich description, incorporating essential elements such as composition, lighting, subject attributes, and stylistic choices. The refined prompts follow strict set of guidelines to maintain coherence, clarity, and aesthetic quality, ensuring that they meet the requirements for highfidelity image generation. In the following example, we provide the exact system prompt and user instructions used to guide GPT-4o in refining list of short prompts. The prompts are designed to ensure that the model generates visually appealing and contextually appropriate descriptions for each input. This prompt refining process ensures that the generated prompts are detailed, high-quality, and aligned with the intended visual aesthetics. The use of GPT-4o to refine short prompt significantly enhances the input quality, making it suitable for use in high-fidelity image generation tasks. Figure 11. More visual comparison of open-source methods on the Aesthetic-Eval@4096 benchmark at 40964096 resolution. Figure 12. More visual comparison of open-source methods on the Aesthetic-Eval@4096 benchmark at 40964096 resolution. Figure 13. More visual comparison of open-source methods on the Aesthetic-Eval@4096 benchmark at 40964096 resolution. Figure 14. Visual comparison of open-source methods at 1:2 aspect ratio (2048x4096). Figure 15. Visual comparison of open-source methods at 2:1 aspect ratio (4096x2048). Figure 16. Visual comparison of open-source methods at 16:9 aspect ratio (5120x2880). Figure 17. Visual comparison of open-source methods at 1:2.39 aspect ratio (2496x5952)."
        }
    ],
    "affiliations": [
        "HKUST",
        "HKUST(GZ)"
    ]
}