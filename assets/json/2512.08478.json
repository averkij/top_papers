{
    "paper_title": "Visionary: The World Model Carrier Built on WebGPU-Powered Gaussian Splatting Platform",
    "authors": [
        "Yuning Gong",
        "Yifei Liu",
        "Yifan Zhan",
        "Muyao Niu",
        "Xueying Li",
        "Yuanjun Liao",
        "Jiaming Chen",
        "Yuanyuan Gao",
        "Jiaqi Chen",
        "Minming Chen",
        "Li Zhou",
        "Yuning Zhang",
        "Wei Wang",
        "Xiaoqing Hou",
        "Huaxi Huang",
        "Shixiang Tang",
        "Le Ma",
        "Dingwen Zhang",
        "Xue Yang",
        "Junchi Yan",
        "Yanchi Zhang",
        "Yinqiang Zheng",
        "Xiao Sun",
        "Zhihang Zhong"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Neural rendering, particularly 3D Gaussian Splatting (3DGS), has evolved rapidly and become a key component for building world models. However, existing viewer solutions remain fragmented, heavy, or constrained by legacy pipelines, resulting in high deployment friction and limited support for dynamic content and generative models. In this work, we present Visionary, an open, web-native platform for real-time various Gaussian Splatting and meshes rendering. Built on an efficient WebGPU renderer with per-frame ONNX inference, Visionary enables dynamic neural processing while maintaining a lightweight, \"click-to-run\" browser experience. It introduces a standardized Gaussian Generator contract, which not only supports standard 3DGS rendering but also allows plug-and-play algorithms to generate or update Gaussians each frame. Such inference also enables us to apply feedforward generative post-processing. The platform further offers a plug in three.js library with a concise TypeScript API for seamless integration into existing web applications. Experiments show that, under identical 3DGS assets, Visionary achieves superior rendering efficiency compared to current Web viewers due to GPU-based primitive sorting. It already supports multiple variants, including MLP-based 3DGS, 4DGS, neural avatars, and style transformation or enhancement networks. By unifying inference and rendering directly in the browser, Visionary significantly lowers the barrier to reproduction, comparison, and deployment of 3DGS-family methods, serving as a unified World Model Carrier for both reconstructive and generative paradigms."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 9 ] . [ 1 8 7 4 8 0 . 2 1 5 2 : r Visionary: The World Model Carrier Built on WebGPU-Powered Gaussian Splatting Platform Yuning Gong1,2, Yifei Liu1 Yifan Zhan3 Yuanjun Liao1,2 Jiaming Chen2 Yuanyuan Gao1,5 Muyao Niu3 Jiaqi Chen1,5 Minming Chen Xueying Li4 Li Zhou1 Shixiang Tang1 Yuning Zhang1 Le Ma1 Wei Wang1 Dingwen Zhang5 Xiaoqing Hou1 Xue Yang4 Huaxi Huang1 Junchi Yan1, Yanchi Zhang2 Yinqiang Zheng3 Xiao Sun1 Zhihang Zhong1, (cid:0) 1Shanghai AI Laboratory 2Sichuan University 4Shanghai Jiao Tong University 5Northwestern Polytechnical University 3The University of Tokyo Code: https://github.com/Visionary-Laboratory/visionary"
        },
        {
            "title": "Abstract",
            "content": "Neural rendering, particularly 3D Gaussian Splatting (3DGS), has evolved rapidly and become key component for building world models. However, existing viewer solutions remain fragmented, heavy, or constrained by legacy pipelines, resulting in high deployment friction and limited support for dynamic content and generative models. In this work, we present Visionary, an open, web-native platform for real-time various Gaussian Splatting and meshes rendering. Built on an efficient WebGPU renderer with per-frame ONNX inference, Visionary enables dynamic neural processing while maintaining lightweight, click-to-run browser experience. It introduces standardized Gaussian Generator contract, which not only supports standard 3DGS rendering but also allows plug-and-play algorithms to generate or update Gaussians each frame. Such inference also enables us to apply feedforward generative post-processing. The platform further offers plug in three.js library with concise TypeScript API for seamless integration into existing web applications. Experiments show that, under identical 3DGS assets, Visionary achieves superior rendering efficiency compared to current Web viewers due to GPU-based primitive sorting. It already supports multiple variants, including MLP-based 3DGS, 4DGS, neural avatars, and style transformation or enhancement networks. By unifying inference and rendering directly in the browser, Visionary significantly lowers the barrier to reproduction, comparison, and deployment of 3DGS-family methods, serving as unified World Model Carrier for both reconstructive and generative paradigms."
        },
        {
            "title": "Introduction",
            "content": "Neural rendering, particularly 3D Gaussian Splatting (3DGS) [34], has advanced rapidly in recent years and has become key building block for world models [25]. Its efficiency and visual quality enable interactive, real-time 3D experiences that were previously impractical. However, critical gap remains in deployment: while research focuses on training larger and more sophisticated models, the capability to distribute and execute these dynamic, and heterogeneous representations on consumer This work was initiated during an internship at the Shanghai AI Laboratory. (cid:0) denotes the corresponding author (zhongzhihang@pjlab.org.cn). Visionary Team: https://github.com/Visionary-Laboratory Figure 1: Visionary as Universal Runtime. Visionarys core system is packaged as three.js plug-in for seamless extension and integration. As demonstration, we present lightweight webbased editor that runs directly in the browser: by simply visiting URL, users can leverage local computing resources through WebGPU to efficiently and simultaneously render multiple heterogeneous 3D/4D Gaussian assets, while maintaining full compatibility with traditional mesh-based rendering pipelines. devices has lagged behind. Currently, there is an urgent need for lightweight, flexible, and unified platform that can support the ever-expanding 3DGS ecosystem in practical and accessible manner. Existing systems suffer from significant deployment friction and limited extensibility. On the desktop, client-based frameworks such as SIBR [34] and engine-bound plug-ins for Unity, Blender, or Unreal Engine rely on heavy native stacks, including tightly coupled tool-chains and driver dependencies. While performant, these systems are difficult to configure, cumbersome to share, and poorly suited for rapid experimentation or integration of third-party algorithms. On the web, viewers such as SparkJS, SuperSplat, and GaussianSplats3D are constrained by legacy WebGL pipelines, which restrict support for dynamic scenes, animatable avatars, and the integration of generative models. As result, many demonstrations rely on precomputed Gaussians or server-side inference, reducing both interactivity and reproducibility. Despite these limitations, web-based platforms remain highly attractive due to their accessibility and platform independence. This motivates core challenge: how to design browser-based system that can (i) support heterogeneous 3DGS algorithms without requiring users to re-engineer rendering pipelines or write low-level shader code, and (ii) tightly couple per-frame inference with highthroughput GPU rendering while maintaining real-time performance within browser environment. To address these challenges, we propose Visionary, web-native World Model Carrier built on WebGPU-based Gaussian Splatting platform. Visionary removes native dependency overhead by leveraging WebGPU as unified compute and graphics backend, and standardizes algorithm integration through ONNX-based per-frame inference. At its core is the Gaussian Generator contract, defined by rigorous ONNX I/O schema and metadata, which specifies how algorithms generate or update Gaussian attributes, including position, scale, orientation, and color. Any 3DGS-family method can be loaded as plug-in, executed directly in the browser, and rendered efficiently without modification to the underlying runtime. Built on fully browser-resident pipeline, Visionary unifies compute and rendering to support dynamic 3DGS workflows entirely on the client side. The current platform supports classic 3DGS, MLP-based 3DGS [51], neural avatars [30, 94, 64], and 4DGS [88] for dynamic scenes, along with 2 generative post-processing networks [68, 80, 11]. We further offer three.js plug-in and exposed through concise TypeScript API, enabling seamless integration with existing web applications. Across representative static and dynamic scenes, Visionary sustains real-time interactive performance and demonstrates substantial improvements in frame time, dynamic update latency, and scalability compared to existing web-based renderers. The contributions of this work are as follows: Visionary, web-native World Model Carrier that unifies per-frame ONNX inference with WebGPU-based Gaussian Splatting renderer and generative post-processing, enabling fully dynamic 3D neural rendering in the browser. The Gaussian Generator contract, standardized ONNX I/O and metadata interface that enables seamless plug-and-play integration of customized 3DGS algorithms without modifying the rendering pipeline. reference WebGPU implementation and three.js API, providing superior efficiency over WebGL-based viewers and straightforward integration into existing web engines for research and application development."
        },
        {
            "title": "2 Related Works",
            "content": "2.1 3D Gaussian Splatting Neural Radiance Fields (NeRF) [56, 6, 62, 19, 57, 42, 93] ignited the modern wave of neural rendering and corresponding down-stream applications [61, 78, 13]. However, volumetric NeRF rendering remains computationally expensive due to dense ray marching and repeated neural field evaluations. To address this inefficiency, 3D Gaussian Splatting [34] was introduced as an explicit, point-based alternative, which offer comparative render quality and extreme fast rendering speed. Building on the success of 3DGS, rich ecosystem of extensions rapidly emerged. Methods such as 2DGS [98], PGSR [10], GOF [92], and RaDe-GS [96] provide more accurate depth rendering and establish tighter connections between Gaussian primitives and mesh-based representations. Structured 3D Gaussian like Scaffold-GS [51] and Octree-GS [65] further improve rendering quality and efficiency through structured anchors and neural decoding. While 3DGS is significantly more efficient than NeRF, scaling to city-level environments exposes new limitations in efficent training. Largescale systems, including CityGaussian [48], VastGaussian [45], Hier-GS [35], and CityGS-X [24] propose specialized pipelines for training and rendering in expansive outdoor scenes. Concurrently, researchers have observed that Gaussian primitives often become overly fragmented and redundant, motivating line of work on compression and pruning, such as LightGaussian [18], Compact3DGS [36], and MaskGaussian [49]. Recently, some studies [91, 23] have begun exploring the use of hardware rasterization techniques to accelerate the rendering process of 3DGS. Beyond static scenes, dynamic extensions have emerged as well, including 4D Gaussian Splatting [88, 79, 5, 15, 31, 41, 50, 73, 90, 14], and series of work focusing on avatar reconstruction and animation [30, 63, 43, 59, 64, 84, 95, 58]. More recently, parallel thread of feedforward Gaussian reconstruction methods [9, 12, 37, 33], to bypass iterative optimization altogether and enable real-time reconstruction. Overall, the 3DGS family has expanded rapidly, producing diverse representations, training paradigms, and rendering pipelines. This diversity highlights the need for flexible, unified viewer architecture capable of visualizing, comparing, and mixing different Gaussian-based representations for research and content creation. 2.2 3DGS Viewers and Plugins Effective visualization is crucial for the evaluation and deployment of neural rendering methods. Research-oriented frameworks typically employ desktop-based architectures to achieve highperformance rendering. The official SIBR viewer [34] relies on C++/CUDA to ensure efficiency but remains standalone research tool. Similarly, Splatfacto, included in the Nerfstudio [74] framework, provides browser-based interface for effective training monitoring. Nevertheless, as client-server system coupled with the training engine, it necessitates heavy local Python/CUDA backend and the original input data for initialization. To leverage mature development ecosystems, 3 plugins for game engines and content creation tools including Unity, Unreal Engine, and Blender were developed. These plugins allow 3DGS to interact with engine-specific features such as physics and lighting. However, both standalone viewers and engine-based plugins suffer from significant deployment friction. They typically require heavy installation stacks, specific GPU drivers (e.g., CUDA), and in the case of plugins, strict compatibility with specific engine versions. Such tight coupling and hardware dependency make these systems ill-suited for rapid sharing, lightweight reproduction, or cross-platform accessibility. On the other hand, web-based viewers such as GaussianSplats3D, SuperSplat, and SparkJS allow users to view static scenes directly in browser without software installation. However, most existing web viewers rely on legacy WebGL pipelines. This dependence imposes severe performance bottlenecks, often necessitating CPU-based primitive sorting, which limits scalability for large scenes. Furthermore, the lack of flexible compute shader support in WebGL restricts these viewers to static assets, preventing the integration of dynamic Gaussian decoding or complex generative post-processing pipelines on the client side. Compared with other desktop or web based solutions, our platform Visionary offers web-native accessibility and real-time WebGPU performance, while additionally supporting advanced 3DGS variants and generative models through flexible ONNX interface. 2.3 World Models and Interactive Generative Video world model aims to internalize the governing laws of physical environment, learning to predict future states based on current observations and actions. This capability holds transformative potential spanning wide range of verticals, promising to enable high-fidelity industrial digital twins, immersive entertainment and even personalized interactive education. To facilitate scalability, the dominant paradigm of world models bypasses explicit 3D reconstruction, instead learning auto-regressively from vast video data to simulate the world as sequence of 2D latent frames, manifested as Interactive Generative Video (IGV). Prominent general-purpose foundation models, such as Genie 3, V-JEPA 2 [4], and Cosmos 2.5 [2], exemplify this approach, demonstrating remarkable capabilities in synthesizing diverse visual dynamics. This methodology has seen rapid adoption across specific domains as well, ranging from autonomous driving [76, 21, 40, 77, 100, 22, 28, 97, 70, 66] and embodied robotic manipulation [1, 102, 72, 52, 101] to open-world gaming agents [82, 16, 27, 3] and science [99, 89, 87]. However, lacking explicit spatial understanding, these models suffer from inherent 3D inconsistency, frequently exhibiting geometric hallucinations and failing to maintain consistent object identity. Consequently, the generated worlds often lack the physical realism and long-horizon stability essential for rigorous simulation. Leveraging advancements in feed-forward Gaussian splatting [9, 12, 33] and VGGT [75], recent research [67, 53, 81, 38] integrates 3D priors into the latent state space to enforce physical plausibility. Unlike transient 2D features, explicit 3D intermediate states ensure superior multi-view consistency, allowing the camera to move freely without breaking the scenes illusion while providing robust global memory bank. However, realizing the full potential of such high-fidelity representations requires accessible visualization tooling. Our platform addresses this infrastructural need by offering standardized ONNX-based pipeline for the real-time inspection of explicit 3D states. Furthermore, its inherent compatibility with IGV workflows makes it an ideal testbed for developing next-generation, physics-aware World Models."
        },
        {
            "title": "3 Pipeline",
            "content": "3.1 Overview The overall design of Visionary follows three guiding principles. Web-native first: all components run directly in the browser via WebGPU, enabling zero-install sharing, on-device inference for privacy, and consistent behavior across operating systems. Contract-driven extensibility: algorithms are exported to ONNX, while the runtime handles scheduling, memory management, and rendering, eliminating the need for per-algorithm code branches. Real-time coupling: inference and rendering are executed within single frame budget, enabling fast interaction. The overall pipeline is illustrated in Figure 2. We first review the definition of 3DGS in Section 3.2. In Section 3.3, we present the Gaussian Generator contract built on ONNX and demonstrate several representative 3DGS variants. 4 Figure 2: Pipeline of Visionary. Visionary first performs pre-decoding through ONNX to support diverse 3DGS variants. The generated Gaussians are then combined with optional mesh assets and passed into WebGPU-based hybrid rendering pipeline, where depth-aware composition produces the final image. In the post-processing stage, ONNX-based generative models can be optionally applied for tasks such as stylization and enhancement, yielding the final output. We then describe the WebGPU-based rendering pipeline for hybrid 3DGS and mesh visualization in Section 3.4. Finally, in Section 3.5, we introduce how generative models can also be integrated through ONNX for post-processing. 3.2 Definition of 3DGS Gaussian Splatting represents scene as set of anisotropic 3D Gaussian primitives: = {Gi}N i=1, where each Gaussian Gi is defined by mean position µi R3, covariance matrix Σi R33, which is derived from scale si R3 and rotation quaternion ri R4, an opacity αi (0, 1), and view-independent color ci [0, 1]3 encoded by spherical harmonics. For camera with projection operator Π(), each 3D Gaussian is projected to 2D elliptical Gaussian xi = Π(µi), Si = JiΣiJ , (1) where Ji denotes the Jacobian of Π at µi. Given pixel location x, its contribution from Gaussian is modeled as wi(x) = αi exp (x xi)S1 (x xi) . (cid:19) (cid:18) 1 2 Following front-to-back alpha compositing, the rendered color at pixel is computed as C(x) = (cid:88) i= (cid:89) wi(x) (1 wj(x)) ci, j<i where Gaussians are sorted by depth w.r.t. the camera. This splatting formulation enables differentiable rendering and efficient optimization of all Gaussian parameters through gradient descent. 3.3 ONNX-based Gaussian Pre-decoding 3DGS has many variants, each adopting different algorithms to produce the final rendering. For example, MLP-based 3DGS [51] decodes Gaussians from anchors according to give camera view and then rasterizes the decoded Gaussians, while 4DGS [79] computes deformation field based on timestamp and then rasterizes the deformed Gaussians. To avoid modifying the renderer for each existing or newly proposed method, we deciced to split the rendering logic into two parts: method-specific pre-decoding stage and unified renderer. To support heterogeneous 3DGS-family methods without modifying the renderer, Visionary adopts an ONNX-based pre-decoding stage, where each algorithm is exported as plugin that generates or updates Gaussian attributes every frame. We formalize this interface as the Gaussian Generator contract, specified by fixed set of ONNX I/O tensors and accompanying metadata, enabling contractdriven extensibility. ONNX (Open Neural Network Exchange) serves as an open standard for (2) (3) machine learning interoperability, defining common computation graph representation independent of specific frameworks. By leveraging this ecosystem, models trained in diverse environments (e.g., PyTorch or TensorFlow) can be unified and deployed seamlessly across different backends. Contract I/O. At runtime, the client provides lightweight per-frame inputs (e.g., frame index, control signals or camera). The ONNX graph outputs variable number of Gaussians Gt in packed layout (position, opacity, upper-covariance, and appearance), along with model-level metadata such as number of points and data types (FP32/FP16). The renderer consumes these outputs directly as storage buffers for the WebGPU pipeline. Why pre-decoding? By encapsulating method-specific neural decoding and deformation logic inside ONNX, Visionary avoids per-method shader branches and heavy native dependencies. For example, in avatar animation, the skeletal forward kinematics and per-Gaussian deformation can be exported into the ONNX graph, so the browser only feeds compact pose parameters while receiving deformed Gaussians ready for rasterization. Representative variants. In the following subsections, we instantiate this contract for multiple families, including MLP-based 3DGS, 4DGS, and neural avatars, demonstrating that diverse representations can be unified under the same pre-decoding rendering pipeline. Deployment-oriented graph optimizations. Beyond unifying heterogeneous variants under single ONNX pre-decoding contract, we introduce two practical optimizations to improve real-time robustness on ONNX Runtime WebGPU. (i) Enabling capture_graph. For per-frame generators with repetitive execution patterns, we revise the runtime scheduling and I/O binding logic to make the inference path compatible with WebGPU graph capture. Concretely, we keep the session and bindings stable across frames (with an optional warm-up run) so that the runtime can reuse the captured execution graph, reducing JavaScript-side dispatch overhead and stabilizing frame time. (ii) Post-export rewriting for large Concat/Split patterns. In some exported models, large number of per-slot tensors are concatenated or split in single operator (e.g., one-shot Concat over many slots, followed by Split for downstream consumers). Such patterns can be fragile under current WebGPU limits. We therefore apply lightweight post-processing pass on the exported ONNX graph to partition these operations into smaller, equivalent chunks (e.g., chunked concat/slice-based splits), producing WebGPU-friendly graph while preserving the same packed output layout required by the Gaussian Generator contract. 3.3.1 Support for MLP-based 3DGS MLP-based 3DGS like Scaffold-GS [51] and Octree-GS [65] have been proposed to use MLP to decode Gaussians from anchors that store shared features of nearby Gaussians, which offers better visual quality and reduced storage. We leverage Scaffold-GS [51] to demonstrate the methodology for exporting such representation into the ONNX format and integrating into our platform. Instead of directly reconstructing Gaussians from sparse SfM points, these methods first extract sparse voxel grid and place anchors at the centers of occupied voxels. Each anchor is associated with feature vector fi, which is fed into multi-layer perceptron (MLP) to generate neural Gaussian parameters: {(µj, Σj, cj, αj) M} = {MLPθ(fi, dview) } , (4) where θ denotes the learnable MLP weights, and µj, Σj, cj, and αj represent the mean, covariance, color, and opacity of the j-th neural Gaussian generated from the i-th anchor under viewing direction dview. These neural Gaussians are then rasterized using the standard 3DGS formulation. While MLP-based 3DGS methods exhibit strong rendering flexibility, Eq. Equation (4) reveals key challenge: the neural Gaussian parameters must be re-generated for every frame, making real-time online visualization difficult. To the best of our knowledge, no existing system provides an interactive, in-browser viewer for such dynamically generated Gaussians. To address this limitation, we integrate the MLP-based 3DGS pipeline with ONNX Runtime WebGPU and enable efficient, browser-side visualization. Specifically, we export the trained MLP, along with the anchor positions, scales, and feature vectors, into static ONNX compute graph. This design allows 6 the viewer to compute neural Gaussian parameters on-the-fly for arbitrary camera poses, enabling fast, real-time visualization directly in the browser. 3.3.2 Support for 4DGS To address the challenge of reconstructing and rendering dynamic scenes, mainstream 4D Gaussians approaches typically circumvent the prohibitive storage overhead of storing Gaussian parameters for every individual frame. Instead, their modeling often combines canonical space with neural deformable field. We leverage 4D Gaussians [79] as representative case to demonstrate the methodology for exporting such deformable fields into the ONNX format. Specifically, 4D Gaussians employs an efficient HexPlane representation (akin to K-Planes [71]) for spatiotemporal feature encoding. Rather than relying on computationally intensive implicit MLP to model the deformation field, this method factorizes the 4D manifold into six orthogonal 2D feature planes (i.e., Pxy, Pxz, Pyz, Pxt, Pyt, Pzt). For each Gaussian primitive in the canonical space, its coordinates and current timestamp (x, y, z, t) are projected onto these multi-resolution planes to retrieve corresponding feature vectors via bilinear interpolation. These features are subsequently concatenated and fed into lightweight multi-head MLP decoder φ to predict the deformation attributes, specifically position offsets (x, y, z), rotation corrections r, and scaling variations s. This process effectively transforms the static canonical Gaussians into their time-dependent states for real-time rendering. To integrate 4D Gaussians [79] into our platform, we established comprehensive training-to-export pipeline. We developed custom wrapper that encapsulates the deformation network, comprising multi-resolution feature grids and the MLP decoder φ, into standalone torch.nn.Module. This design effectively decouples the inference logic from training-specific rasterization operators, ensuring compatibility with standard ONNX runtimes. Notably, our exported model embeds the canonical Gaussian attributes directly as internal constant tensors (Initializers), rather than requiring external inputs. Consequently, the model accepts scalar timestamp as the sole input to query the HexPlane and output deformation residuals. This self-contained architecture obviates the need for external geometry files (e.g., PLY), streamlining the web clients reconstruction workflow via unified ONNX model. 3.3.3 Support for Animatable Human Avatar Our platform supports broad class of animatable human avatar methods that utilize canonical space formulation decoupled from the deformation logic. In this paradigm, the scene is represented by set of static canonical 3D Gaussians in neutral pose (e.g., T-pose), which are dynamically warped to the observation space driven by parametric body model (e.g., SMPL-X [60]). Let the canonical representation be denoted as Gcan = { ˆGi}N contains mean position ˆµi, covariance ˆΣi, and learnable skinning weights Wi = {wi,k}K k=1 associated with skeletal joints. The deformation is generally modeled via Linear Blend Skinning (LBS). Given the body pose parameters θ and shape parameters β for specific frame, the parametric model defines global transformation matrix Mk(θ, β) SE(3) for each joint k. The local transformation Ti for the i-th Gaussian is computed by aggregating these joint transformations: i=1, where each primitive ˆGi typically Ti(θ, β) = wi,kMk(θ, β) = (cid:20)Ri 0 (cid:21) , ti 1 (cid:88) k=1 (5) where Ri and ti represent the blended rotation and translation, respectively. Consequently, the deformed attributes for the live frame are updated as: µi = Ti ˆµi, Σi = Ri ˆΣiR . This formulation allows the renderer to handle diverse avatar methods that adhere to the canonicaldeformation schema. (6) As representative implementation of this paradigm, we integrate LHM [64] into our ONNX-based pipeline. LHM learns high-fidelity canonical representation driven by SMPL-X. To enable webbased rendering, we export the deformation logic into specialized ONNX graph. In this setup, the 7 Algorithm Pseudo code for WebGPU 3DGS rendering Require: Gaussians Models: Mk = {(µi, si, ri, ci)}Nk Ensure: Rendered frame i=1; camera (V, ); per-model transform Mk /* Pre-pack (only once) */ for all gaussian do Σi CovFromScaleRot(si, ri) store upper(Σi), µi and ci as FP16-packed u32 buffers end for /* Per-frame */ count 0; dispatch 0 for all model Mk do bind ModelParams(Mk, offsets, types, SHdeg, . . . ) dispatch PREPROCESSKERNEL(Mk) global buffers + atomic count reset atomics/indirects end for RADIX SORT(depths, indices, count) RENDERMESHDEPTHPREPASS DRAWSPLATSWITHMESHDEPTH(points2d, indices, count, Dmesh) back-to-front procedure PREPROCESS(idx) rasterize mesh, output depth texture Dmesh premultiplied α, (p, α) read pos/opacity; Σ(6) read upper-cov xw Mk [µ; 1]; xc xw; xclip xc if frustum-cull fails or α < αmin then return end if compute 2D ellipse (v1, v2) and NDC center (u, v, z) rgba evaluate color (raw/SH) ATOMICADD(count, 1) write points2d[k], depths[k], indices[k] end procedure pack to FP16 heavy skeletal computations including the forward kinematics of the SMPL-X skeleton and the per-Gaussian LBS operations defined above are encapsulated within the model. The canonical Gaussian attributes ( ˆµ, ˆΣ) and skinning weights are baked into the model as initializers (constants). During runtime, the client simply feeds the lightweight SMPL-X parameters (θ, β) into the ONNX runtime. The model then acts as pre-decoder, outputting the transformed positions and covariances directly to the rasterizer, thereby achieving real-time animation without heavy client-side dependency management. We further adapt R3Avatar [94] to support high-fidelity novel view rendering of human avatars. In this configuration, we likewise pre-calculate and encapsulate the complex skeletal dynamics and neural deformation logic within the ONNX graph. The client provides frame index as input. Internally, the ONNX runtime maps this index to the corresponding temporal state, decodes the specific avatar configuration, and outputs the deformed Gaussians directly to the rasterizer. 3.4 WebGPU-based Hybrid Renderer Visionary renders multiple 3DGS models together with optional mesh assets in single WebGPU pipeline, and composes them via depth-aware composition. Each Gaussian primitive stores position µi, scale si, rotation ri, and appearance ci (raw RGB or SH). For efficiency, we pre-pack Gaussian attributes into GPU-friendly buffers and execute per-frame screen-space preprocessing and sorting entirely on the GPU. Pre-pack and FP16 layout. Given Gaussian parameter unit (µi, si, ri, ci), si and ri are converted into symmetric covariance Σi R33, and the upper-triangular 6-tuple are stored as Σ(6) . We then cast (µi, αi, Σ(6) , ci) to fp16 and pack every two units into u32 storage to reduce bandwidth. i 8 (a) SparkJS time breakdown. (b) Visionary time breakdown. (c) Total time (log scale). Figure 3: Runtime comparison. Experiments are done under identical Gaussian complexity using the classic bicycle scene of 3DGS [34] (6M Gaussians at full resolution, and 1/2, 1/4, 1/8 scales). (a): SparkJS shows dominant CPU sorting cost. (b): Visionary shifts computation to GPU with low and stable overhead. (c): Log-scale comparison shows up to 100 speed-up over SparkJS. Per-frame preprocessing. For each loaded model Mk = {(µi, si, ri, ci)}Nk i=1, we dispatch compute kernel that (i) applies user-defined affine transformation Mk to Mk, (ii) transforms its parameter units to camera space and clip space, (iii) performs frustum and opacity culling to remove invalid splats, and (iv) computes the 2D ellipse eigenvectors (v1, v2) and NDC center (u, v, z) (written as Splat). Valid splats are appended into global buffers via an atomic counter and simultaneously write depth values, i.e. the in NDC space, as keys for sorting in the next step. GPU sorting and rasterization. After preprocessing, we perform GPU radix sort [55] over depths and then draw all splats using instanced rasterization. The vertex shader expands each splat into screen-space quad using (v1, v2), while the fragment shader evaluates the Gaussian weight and outputs pre-multiplied color. Depth-aware composition with mesh. If mesh is present, we first rasterize the mesh to obtain depth buffer Dmesh (and optionally its color). During Gaussian rasterization, we keep depth test enabled but depth write disabled: Gaussian fragments with zgs > Dmesh are rejected (occluded by mesh), while visible fragments are alpha-composited in back-to-front order. 3.5 ONNX-based Post-processing In our ONNX-based-based post-processing workflow, we export the diffusion denoiser (U-Net [69, 29]) to Web-friendly ONNX graph. Here, we take EXGS [11] as an example of post processing enhancement and talk about how to export it. Concretely, we load the model and wrap it with lightweight adapter that standardizes timestep handling, either using fixed timestep buffer for specific denoising step or exposing the timestep as dynamic input. The model is then switched to the evaluation mode and optionally cast to FP16 on the target device for efficient inference. We next construct shape-correct dummy inputs and optionally enable dynamic axes. The exported ONNX can then run with varying inference shapes. After sanity-check forward pass, we invoke torch.onnx.export() with appropriate input/output names, opset version, and constant folding. The exported model is finally validated via onnx.checker.check_model()."
        },
        {
            "title": "4 Experiments",
            "content": "4.1 Setup We evaluate Visionary as web-native renderer that couples per-frame Gaussian generation (ONNX) with high-throughput WebGPU splatting. We focus our evaluation on two aspects: (i) end-to-end rendering efficiency and scalability on static 3DGS assets, and (ii) visual robustness of alpha-composited splatting under challenging interaction patterns (e.g., rapid view changes) and multi-component composition. We compare against representative WebGL-based viewers, including SparkJS and SuperSplat. Unless otherwise stated, all comparisons are conducted using identical 3DGS assets and camera trajectories. Also, we report the runtime of different variants implemented with our 9 Table 1: Time analysis. The sorting process in SparkJS is performed on the CPU. The GPU execution time is obtained using PIX replay, where we first capture representative frame in PIX and then replay it within PIX to profile performance. # GS [M] SparkJS Visionary (Ours) Sort [ms] Prep.+Draw [ms] Total [ms] Sort [ms] Prep.+Draw [ms] Total [ms] 6.062 (1/1) 3.031 (1/2) 1.515 (1/4) 0.758 (1/8) 172.87 143.50 45.27 33.31 4.03 2.25 1.03 0.52 176.90 145.75 46.29 33. 0.58 0.32 0.22 0.20 1.52 0.77 0.38 0.20 2.09 1.09 0.60 0.40 WebGPU renderer. All experiments are conducted on workstation equipped with an NVIDIA RTX 4090 GPU and an Intel w5-3435X CPU. Table 2: Rendering quality analysis. Evaluated on MipNeRF360 [7] dataset. PSNR SSIM LPIPS SparkJS Visionary (Ours) 27.315 27.867 0.825 0.828 0.253 0. 4.2 Runtime and Scalability vs. SparkJS We first benchmark standard large-scale static scene: the classic bicycle 3DGS asset with 6M Gaussians at full resolution, and its downscaled variants (1/2, 1/4, 1/8). Figure 3 reports the frametime breakdown and the end-to-end latency in Table 1. To notice, we measure the GPU execution time using PIX replay, where we first capture representative frame in PIX and then replay it within PIX to profile performance. SparkJS is bottlenecked by CPU-side primitive, which dominates the total frame time. In contrast, Visionary moves per-frame preprocessing and global sorting to WebGPU compute, resulting in low and stable overhead across resolutions. Concretely, at full resolution (6.062M Gaussians), SparkJS spends 172.87ms on sorting and 176.90ms total per frame, while Visionary reduces this to 0.58ms sorting and 2.09ms total per frame. Across resolutions, Visionary achieves up to 135 end-to-end speedup over SparkJS (Figure 3). 4.3 Rendering Quality Speed improvements should not come at the cost of image quality. On the MipNeRF360 [7] benchmark, Visionary matches (and slightly improves) the rendering quality compared to SparkJS in termps of PSNR, SSIM and LPIPS as in table 2. This slight improvement can be attributed to two design choices: Visionary avoids the aggressive quantization used in SparkJS to preserve more detail while still maintaining interactive speed, and our WebGPU implementation performs 3DGS preprocessing in compute shaders rather than relying on rasterization-based WebGL pipeline. 4.4 Robustness under Rapid Viewpoint Changes SparkJS lazy sorting. Besides average frame time, interactive viewers must remain visually stable under fast user input. SparkJS adopts lazy sorting strategy to reduce CPU overhead by reusing stale ordering and updating it incrementally, which means instead of re-sorting splats every frame in lockstep with rendering, it performs asynchronous updates that are amortized over multiple frames, so the ordering can lag behind the current view. However, when the camera rotates quickly, the depth ordering can change drastically between frames, and the approximate ordering may become invalid. This leads to incorrect alpha compositing and visible artifacts (e.g., popping, streaking, and inconsistent transparency), as shown in Figure 4. 10 Figure 4: Artifacts caused by lazy sorting in SparkJS. When rotating the viewpoint rapidly, the stale/incrementally updated order can become invalid, producing incorrect alpha compositing and visible temporal artifacts. The efficient implementation of Visioanry avoids this flaw. The version of SparkJS tested here is the latest available, v0.1.10. Figure 5: Wrong visualization caused by local sorting in Supersplat. Without global ordering, overlapping Gaussians across partitions may be composited in an incorrect order. Visionarys efficient global sorting avoids this issue. The version of Supersplat tested here is the latest available, v2.15.1. Visionary avoids this kind of failure by performing true global per-frame sorting on the GPU for all visible splats. Although sorting is non-trivial, our WebGPU compute implementation keeps the overhead small and stable, and preserves correct back-to-front compositing under fast camera motion. 4.5 Composition Correctness vs. Supersplat Why global sorting matters? SuperSplat improves performance by avoiding full global ordering and instead relies on local sorting (e.g., sorting within partitions). While efficient, such strategy is not equivalent to single global back-to-front ordering. When Gaussians from different partitions overlap substantially, the lack of global order leads to inconsistent blending between partitions, yielding depth-inconsistent transparency (Figure 5). Visionary uses unified global buffer and performs one global sort for all valid splats across all loaded models within the same frame, ensuring correct compositing even when multiple 3DGS assets (or 3DGS + mesh) are rendered together. 11 Table 3: Inference time of MLP-based 3DGS and 4DGS. Scaffold-GS [51] 4DGS [79] # GS [M] Time [ms] 2.49 9.29 4.56 16.10 0.03 4.76 0.06 7.93 Table 4: Inference time of Avatar. Gauhuman [30] R3-Avatar [94] # Instances # GS [M] Time [ms] 1 0.04 7.97 5 0.20 28.60 10 0.40 55. 1 0.02 7.46 5 0.12 27.10 10 0.23 53.35 4.6 Overhead of ONNX-based Gaussian pre-decoding Finally, we report the runtime feasibility of integrating representative dynamic or structured 3DGS variants through our ONNX-based Gaussian Generator contract. For MLP-based 3DGS (e.g., Scaffold-GS [51]) and deformable 3DGS or 4DGS [79], the per-frame decoding runs in real-time across different scenes as in Table 3. For animatable avatars [30, 64, 94], we observe per-frame inference in the range of 78ms depending on the specific model as in Table 4. Note that avatars typically require fewer Gaussians. These results indicate that single browser-resident pipeline can support both fast rendering and per-frame neural updates. For 3DGS variants beyond the original 3DGS implementation and diffusion-based post-processing effects, there are currently no other web viewers that support comparable functionality; we therefore refer readers to the accompanying video demos and our online editor for qualitative inspection and interactive exploration."
        },
        {
            "title": "5 Discussion",
            "content": "5.1 Large World Model & Future The definition and implementation of world models are still under active debate, with no single absolutely dominant paradigm. Some approaches, such as Marble and FlashWorld [39], generate 3DGS scenes from images or text and emphasize reconstructive, geometrically consistent representations. Others, such as Genie 3 and RTFM, follow video-generation paradigm that prioritizes creative synthesis and open-ended dynamics. We believe that the next generation of world model renderers should combine the strengths of both paradigms, forming closed loop between generation and reconstruction. Visionary is designed as an initial step toward this unified framework. Building on the current architecture, several promising directions can be explored. First, we plan to improve physical interaction by integrating collision detection and further coupling with meshbased pipelines [26]. Second, we aim to incorporate physics-aware modeling, in which 3DGS representations are combined with methods such as the Material Point Method (MPM) to simulate realistic dynamics [83, 46, 47]. Third, we will investigate spatially grounded 3D agents built on top of multimodal language models [85, 8, 86], enabling reasoning and interaction within complex environments. Finally, Visionary can serve as bridge to downstream applications by interfacing with vectorized physics simulators, such as Isaac Gym [54], and by integrating relighting [20, 44] and domain adaptation techniques to support Sim-to-Real transfer for embodied AI [17, 32]. 5.2 Limitations WebGPU and ONNX runtimes are still evolving, which may lead to compatibility and stability differences across browsers and operating systems. In addition, current browser security policies impose CPU memory constraints, limiting the size of models that can be executed fully in-browser using offthe-shelf tools. As result, while small-scale to medium-scale networks can be integrated into the pipeline, some post-processing steps currently remain offline."
        },
        {
            "title": "6 Conclusion",
            "content": "Visionary addresses several fundamental barriers in neural rendering platforms, including the lack of effective web-native computation, limited extensibility, and heavy system dependencies. By combining modern, compute-capable graphics API (WebGPU) with unified ONNX-based integration contract, Visionary enables real-time, in-browser rendering of diverse 3DGS variants in portable, lightweight, and research-friendly manner, with support for generative post-processing. Beyond serving as renderer, it acts as unified carrier for both reconstructive and generative world modeling paradigms, establishing an open and extensible foundation for future advances in spatial intelligence, embodied agents, and interactive 3D environments."
        },
        {
            "title": "Acknowledgments",
            "content": "This work was partially supported by the Shanghai AI Laboratory. We thank Kang An2 and Tianchen Hao2 for their contributions to the project during their internships at the Shanghai AI Laboratory."
        },
        {
            "title": "References",
            "content": "[1] A. Ajay, Y. Du, A. Gupta, J. Tenenbaum, T. Jaakkola, and P. Agrawal. Is conditional generative modeling all you need for decision-making? arXiv preprint arXiv:2211.15657, 2022. [2] A. Ali, J. Bai, M. Bala, Y. Balaji, A. Blakeman, T. Cai, J. Cao, T. Cao, E. Cha, Y.-W. Chao, et al. World simulation with video foundation models for physical ai. arXiv preprint arXiv:2511.00062, 2025. [3] E. Alonso, A. Jelley, V. Micheli, A. Kanervisto, A. J. Storkey, T. Pearce, and F. Fleuret. Diffusion for world modeling: Visual details matter in atari. Advances in Neural Information Processing Systems, 37:5875758791, 2024. [4] M. Assran, A. Bardes, D. Fan, Q. Garrido, R. Howes, M. Muckley, A. Rizvi, C. Roberts, K. Sinha, A. Zholus, et al. V-jepa 2: Self-supervised video models enable understanding, prediction and planning. arXiv preprint arXiv:2506.09985, 2025. [5] J. Bae, S. Kim, Y. Yun, H. Lee, G. Bang, and Y. Uh. Per-gaussian embedding-based deformation for deformable 3d gaussian splatting. In European Conference on Computer Vision, pages 321335. Springer, 2024. [6] J. T. Barron, B. Mildenhall, M. Tancik, P. Hedman, R. Martin-Brualla, and P. P. Srinivasan. Mip-nerf: multiscale representation for anti-aliasing neural radiance fields. In Proceedings of the IEEE/CVF international conference on computer vision, pages 58555864, 2021. [7] J. T. Barron, B. Mildenhall, D. Verbin, P. P. Srinivasan, and P. Hedman. Mip-nerf 360: Unbounded anti-aliased neural radiance fields. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 54705479, 2022. [8] E. Brown, A. Ray, R. Krishna, R. Girshick, R. Fergus, and S. Xie. Sims-v: Simulated instruction-tuning for spatial video understanding. arXiv preprint arXiv:2511.04668, 2025. [9] D. Charatan, S. L. Li, A. Tagliasacchi, and V. Sitzmann. pixelsplat: 3d gaussian splats from image pairs for scalable generalizable 3d reconstruction. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 1945719467, 2024. [10] D. Chen, H. Li, W. Ye, Y. Wang, W. Xie, S. Zhai, N. Wang, H. Liu, H. Bao, and G. Zhang. Pgsr: Planar-based gaussian splatting for efficient and high-fidelity surface reconstruction. arXiv preprint arXiv:2406.06521, 2024. [11] J. Chen, X. Ji, Y. Gao, H. Li, Y. Gong, Y. Liu, D. Xu, Z. Zhong, D. Zhang, and X. Sun. Exgs: Extreme 3d gaussian compression with diffusion priors. arXiv preprint arXiv:2509.24758, 2025. 13 [12] Y. Chen, H. Xu, C. Zheng, B. Zhuang, M. Pollefeys, A. Geiger, T.-J. Cham, and J. Cai. Mvsplat: Efficient 3d gaussian splatting from sparse multi-view images. In European Conference on Computer Vision, pages 370386. Springer, 2024. [13] Y. Chen, Y. Zhan, Z. Zhong, W. Wang, X. Sun, Y. Qiao, and Y. Zheng. Within the dynamic context: Inertia-aware 3d human modeling with pose sequence. In European Conference on Computer Vision, pages 491508. Springer, 2024. [14] Z. Chen, H.-a. Gao, D. Qu, H. Chi, H. Tang, K. Zhang, and H. Zhao. Alias-free 4d gaussian splatting. arXiv preprint arXiv:2511.18367, 2025. [15] W. O. Cho, I. Cho, S. Kim, J. Bae, Y. Uh, and S. J. Kim. 4d scaffold gaussian splatting for memory efficient dynamic scene reconstruction. arXiv preprint arXiv:2411.17044, 2024. [16] E. Decart, Q. McIntyre, S. Campbell, X. Chen, and R. Wachen. Oasis: universe in transformer. URL: https://oasis-model. github. io, 2024. [17] A. Escontrela, J. Kerr, A. Allshire, J. Frey, R. Duan, C. Sferrazza, and P. Abbeel. Gaussgym: An open-source real-to-sim framework for learning locomotion from pixels. arXiv preprint arXiv:2510.15352, 2025. [18] Z. Fan, K. Wang, K. Wen, Z. Zhu, D. Xu, Z. Wang, et al. Lightgaussian: Unbounded 3d gaussian compression with 15x reduction and 200+ fps. Advances in neural information processing systems, 37:140138140158, 2024. [19] S. Fridovich-Keil, A. Yu, M. Tancik, Q. Chen, B. Recht, and A. Kanazawa. Plenoxels: Radiance fields without neural networks. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 55015510, 2022. [20] J. Gao, C. Gu, Y. Lin, Z. Li, H. Zhu, X. Cao, L. Zhang, and Y. Yao. Relightable 3d gaussians: Realistic point cloud relighting with brdf decomposition and ray tracing. In European Conference on Computer Vision, pages 7389. Springer, 2024. [21] R. Gao, K. Chen, E. Xie, L. Hong, Z. Li, D.-Y. Yeung, and Q. Xu. Magicdrive: Street view generation with diverse 3d geometry control. arXiv preprint arXiv:2310.02601, 2023. [22] S. Gao, J. Yang, L. Chen, K. Chitta, Y. Qiu, A. Geiger, J. Zhang, and H. Li. Vista: generalizable driving world model with high fidelity and versatile controllability. Advances in Neural Information Processing Systems, 37:9156091596, 2024. [23] Y. Gao, Y. Gong, Y. Liu, L. Jingfeng, Z. Zhong, D. Zhang, Y. Zhang, D. Xu, and X. Sun. Proxy-gs: Efficient 3d gaussian splatting via proxy mesh. arXiv preprint arXiv:2509.24421, 2025. [24] Y. Gao, H. Li, J. Chen, Z. Zou, Z. Zhong, D. Zhang, X. Sun, and J. Han. Citygs-x: scalable architecture for efficient and geometrically accurate large-scale scene reconstruction. arXiv preprint arXiv:2503.23044, 2025. [25] Q. Garrido, M. Assran, N. Ballas, A. Bardes, L. Najman, and Y. LeCun. Learning and leveraging world models in visual representation learning. arXiv preprint arXiv:2403.00504, 2024. [26] H. Guo, D. Weng, M. Su, Y. Chen, X. Dongye, and C. Xu. Tagsplat: Topology-aware gaussian splatting for dynamic mesh modeling and tracking. arXiv preprint arXiv:2512.01329, 2025. [27] J. Guo, Y. Ye, T. He, H. Wu, Y. Jiang, T. Pearce, and J. Bian. Mineworld: real-time and open-source interactive world model on minecraft. arXiv preprint arXiv:2504.08388, 2025. [28] M. Hassan, S. Stapf, A. Rahimi, P. Rezende, Y. Haghighi, D. Brüggemann, I. Katircioglu, L. Zhang, X. Chen, S. Saha, et al. Gem: generalizable ego-vision multimodal world model for fine-grained ego-motion, object dynamics, and scene composition control. In Proceedings of the Computer Vision and Pattern Recognition Conference, pages 2240422415, 2025. [29] J. Ho, A. Jain, and P. Abbeel. Denoising diffusion probabilistic models. Advances in neural information processing systems, 33:68406851, 2020. [30] S. Hu, T. Hu, and Z. Liu. Gauhuman: Articulated gaussian splatting from monocular human videos. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 2041820431, 2024. [31] Y.-H. Huang, Y.-T. Sun, Z. Yang, X. Lyu, Y.-P. Cao, and X. Qi. Sc-gs: Sparse-controlled gaussian splatting for editable dynamic scenes. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 42204230, 2024. [32] Y. Jia, G. Wang, Y. Dong, J. Wu, Y. Zeng, H. Lin, Z. Wang, H. Ge, W. Gu, K. Ding, et al. Discoverse: Efficient robot simulation in complex high-fidelity environments. arXiv preprint arXiv:2507.21981, 2025. [33] L. Jiang, Y. Mao, L. Xu, T. Lu, K. Ren, Y. Jin, X. Xu, M. Yu, J. Pang, F. Zhao, et al. Anysplat: Feed-forward 3d gaussian splatting from unconstrained views. arXiv preprint arXiv:2505.23716, 2025. [34] B. Kerbl, G. Kopanas, T. Leimkühler, and G. Drettakis. 3d gaussian splatting for real-time radiance field rendering. ACM Trans. Graph., 42(4):1391, 2023. [35] B. Kerbl, A. Meuleman, G. Kopanas, M. Wimmer, A. Lanvin, and G. Drettakis. hierarchical 3d gaussian representation for real-time rendering of very large datasets. ACM Transactions on Graphics (TOG), 43(4):115, 2024. [36] J. C. Lee, D. Rho, X. Sun, J. H. Ko, and E. Park. Compact 3d gaussian representation for radiance field. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 2171921728, 2024. [37] H. Li, Y. Gao, C. Wu, D. Zhang, Y. Dai, C. Zhao, H. Feng, E. Ding, J. Wang, and J. Han. Ggrt: Towards pose-free generalizable 3d gaussian splatting in real-time. In European Conference on Computer Vision, pages 325341. Springer, 2024. [38] R. Li, P. Torr, A. Vedaldi, and T. Jakab. Vmem: Consistent interactive video scene generation with surfel-indexed view memory. arXiv preprint arXiv:2506.18903, 2025. [39] X. Li, T. Wang, Z. Gu, S. Zhang, C. Guo, and L. Cao. Flashworld: High-quality 3d scene generation within seconds. arXiv preprint arXiv:2510.13678, 2025. [40] X. Li, Y. Zhang, and X. Ye. Drivingdiffusion: layout-guided multi-view driving scenarios video generation with latent diffusion model. In European Conference on Computer Vision, pages 469485. Springer, 2024. [41] Z. Li, Z. Chen, Z. Li, and Y. Xu. Spacetime gaussian feature splatting for real-time dynamic view synthesis. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 85088520, 2024. [42] Z. Li, Q. Wang, F. Cole, R. Tucker, and N. Snavely. Dynibar: Neural dynamic image-based rendering. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 42734284, 2023. [43] Z. Li, Z. Zheng, L. Wang, and Y. Liu. Animatable gaussians: Learning pose-dependent gaussian maps for high-fidelity human avatar modeling. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 1971119722, 2024. [44] Z. Liang, Q. Zhang, Y. Feng, Y. Shan, and K. Jia. Gs-ir: 3d gaussian splatting for inverse rendering. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 2164421653, 2024. [45] J. Lin, Z. Li, X. Tang, J. Liu, S. Liu, J. Liu, Y. Lu, X. Wu, S. Xu, Y. Yan, et al. Vastgaussian: Vast 3d gaussians for large scene reconstruction. arXiv preprint arXiv:2402.17427, 2024. [46] Y. Lin, C. Lin, J. Xu, and Y. Mu. Omniphysgs: 3d constitutive gaussians for general physicsbased dynamics generation. arXiv preprint arXiv:2501.18982, 2025. [47] I. Liu, H. Su, and X. Wang. Dynamic gaussians mesh: Consistent mesh reconstruction from dynamic scenes. arXiv preprint arXiv:2404.12379, 2024. [48] Y. Liu, C. Luo, L. Fan, N. Wang, J. Peng, and Z. Zhang. Citygaussian: Real-time high-quality large-scale scene rendering with gaussians. In European Conference on Computer Vision, pages 265282. Springer, 2024. [49] Y. Liu, Z. Zhong, Y. Zhan, S. Xu, and X. Sun. Maskgaussian: Adaptive 3d gaussian representation from probabilistic masks. In Proceedings of the Computer Vision and Pattern Recognition Conference, pages 681690, 2025. [50] J. Lu, J. Deng, R. Zhu, Y. Liang, W. Yang, T. Zhang, and X. Zhou. Dn-4dgs: Denoised deformable network with temporal-spatial aggregation for dynamic scene rendering. Advances in Neural Information Processing Systems, 37:8411484138, 2024. [51] T. Lu, M. Yu, L. Xu, Y. Xiangli, L. Wang, D. Lin, and B. Dai. Scaffold-gs: Structured 3d gaussians for view-adaptive rendering. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 2065420664, 2024. [52] J. Lyu, Z. Li, X. Shi, C. Xu, Y. Wang, and H. Wang. Dywa: Dynamics-adaptive world action model for generalizable non-prehensile manipulation. arXiv preprint arXiv:2503.16806, 2025. [53] B. Ma, H. Gao, H. Deng, Z. Luo, T. Huang, L. Tang, and X. Wang. You see it, you got it: Learning 3d creation on pose-free videos at scale. In Proceedings of the Computer Vision and Pattern Recognition Conference, pages 20162029, 2025. [54] V. Makoviychuk, L. Wawrzyniak, Y. Guo, M. Lu, K. Storey, M. Macklin, D. Hoeller, N. Rudin, A. Allshire, A. Handa, et al. Isaac gym: High performance gpu-based physics simulation for robot learning. arXiv preprint arXiv:2108.10470, 2021. [55] P. M. McIlroy, K. Bostic, and M. D. McIlroy. Engineering radix sort. Computing systems, 6(1):527, 1993. [56] B. Mildenhall, P. P. Srinivasan, M. Tancik, J. T. Barron, R. Ramamoorthi, and R. Ng. Nerf: Representing scenes as neural radiance fields for view synthesis. In ECCV, 2020. [57] T. Müller, A. Evans, C. Schied, and A. Keller. Instant neural graphics primitives with multiresolution hash encoding. ACM transactions on graphics (TOG), 41(4):115, 2022. [58] M. Niu, M. Cao, Y. Zhan, Q. Zhu, M. Ma, J. Zhao, Y. Zeng, Z. Zhong, X. Sun, and Y. Zheng. Anicrafter: Customizing realistic human-centric animation via avatar-background conditioning in video diffusion models. arXiv preprint arXiv:2505.20255, 2025. [59] M. Niu, Y. Zhan, Q. Zhu, Z. Li, W. Wang, Z. Zhong, X. Sun, and Y. Zheng. Bundle adjusted gaussian avatars deblurring. arXiv preprint arXiv:2411.16758, 2024. [60] G. Pavlakos, V. Choutas, N. Ghorbani, T. Bolkart, A. A. A. Osman, D. Tzionas, and M. J. Black. Expressive body capture: 3D hands, face, and body from single image. In Proceedings IEEE Conf. on Computer Vision and Pattern Recognition (CVPR), pages 1097510985, 2019. [61] S. Peng, Y. Zhang, Y. Xu, Q. Wang, Q. Shuai, H. Bao, and X. Zhou. Neural body: Implicit neural representations with structured latent codes for novel view synthesis of dynamic humans. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 90549063, 2021. [62] A. Pumarola, E. Corona, G. Pons-Moll, and F. Moreno-Noguer. D-nerf: Neural radiance fields for dynamic scenes. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 1031810327, 2021. [63] Z. Qian, S. Wang, M. Mihajlovic, A. Geiger, and S. Tang. 3dgs-avatar: Animatable avatars via deformable 3d gaussian splatting. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 50205030, 2024. [64] L. Qiu, X. Gu, P. Li, Q. Zuo, W. Shen, J. Zhang, K. Qiu, W. Yuan, G. Chen, Z. Dong, et al. Lhm: Large animatable human reconstruction model from single image in seconds. arXiv preprint arXiv:2503.10625, 2025. 16 [65] K. Ren, L. Jiang, T. Lu, M. Yu, L. Xu, Z. Ni, and B. Dai. Octree-gs: Towards consistent real-time rendering with lod-structured 3d gaussians. arXiv preprint arXiv:2403.17898, 2024. [66] X. Ren, Y. Lu, T. Cao, R. Gao, S. Huang, A. Sabour, T. Shen, T. Pfaff, J. Z. Wu, R. Chen, et al. Cosmos-drive-dreams: Scalable synthetic driving data generation with world foundation models. arXiv preprint arXiv:2506.09042, 2025. [67] X. Ren, T. Shen, J. Huang, H. Ling, Y. Lu, M. Nimier-David, T. Müller, A. Keller, S. Fidler, and J. Gao. Gen3c: 3d-informed world-consistent video generation with precise camera control. In Proceedings of the Computer Vision and Pattern Recognition Conference, pages 61216132, 2025. [68] R. Rombach, A. Blattmann, D. Lorenz, P. Esser, and B. Ommer. High-resolution image In Proceedings of the IEEE/CVF conference on synthesis with latent diffusion models. computer vision and pattern recognition, pages 1068410695, 2022. [69] O. Ronneberger, P. Fischer, and T. Brox. U-net: Convolutional networks for biomedical image segmentation. In International Conference on Medical image computing and computer-assisted intervention, pages 234241. Springer, 2015. [70] L. Russell, A. Hu, L. Bertoni, G. Fedoseev, J. Shotton, E. Arani, and G. Corrado. Gaia-2: controllable multi-view generative world model for autonomous driving. arXiv preprint arXiv:2503.20523, 2025. [71] Sara Fridovich-Keil and Giacomo Meanti, F. R. Warburg, B. Recht, and A. Kanazawa. Kplanes: Explicit radiance fields in space, time, and appearance. In CVPR, 2023. [72] Y. Shang, X. Zhang, Y. Tang, L. Jin, C. Gao, W. Wu, and Y. Li. Roboscape: Physics-informed embodied world model. arXiv preprint arXiv:2506.23135, 2025. [73] R. Shaw, M. Nazarczuk, J. Song, A. Moreau, S. Catley-Chandar, H. Dhamo, and E. PérezPellitero. Swings: sliding windows for dynamic 3d gaussian splatting. In European Conference on Computer Vision, pages 3754. Springer, 2024. [74] M. Tancik, E. Weber, E. Ng, R. Li, B. Yi, T. Wang, A. Kristoffersen, J. Austin, K. Salahi, A. Ahuja, et al. Nerfstudio: modular framework for neural radiance field development. In ACM SIGGRAPH 2023 conference proceedings, pages 112, 2023. [75] J. Wang, M. Chen, N. Karaev, A. Vedaldi, C. Rupprecht, and D. Novotny. Vggt: Visual geometry grounded transformer. In Proceedings of the Computer Vision and Pattern Recognition Conference, pages 52945306, 2025. [76] X. Wang, Z. Zhu, G. Huang, X. Chen, J. Zhu, and J. Lu. Drivedreamer: Towards real-worlddrive world models for autonomous driving. In European conference on computer vision, pages 5572. Springer, 2024. [77] Y. Wen, Y. Zhao, Y. Liu, F. Jia, Y. Wang, C. Luo, C. Zhang, T. Wang, X. Sun, and X. Zhang. Panacea: Panoramic and controllable video generation for autonomous driving. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 69026912, 2024. [78] C.-Y. Weng, B. Curless, P. P. Srinivasan, J. T. Barron, and I. Kemelmacher-Shlizerman. Humannerf: Free-viewpoint rendering of moving people from monocular video. In Proceedings of the IEEE/CVF conference on computer vision and pattern Recognition, pages 1621016220, 2022. [79] G. Wu, T. Yi, J. Fang, L. Xie, X. Zhang, W. Wei, W. Liu, Q. Tian, and X. Wang. 4d gaussian splatting for real-time dynamic scene rendering. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 2031020320, 2024. [80] J. Z. Wu, Y. Zhang, H. Turki, X. Ren, J. Gao, M. Z. Shou, S. Fidler, Z. Gojcic, and H. Ling. Difix3d+: Improving 3d reconstructions with single-step diffusion models. In Proceedings of the Computer Vision and Pattern Recognition Conference, pages 2602426035, 2025. 17 [81] T. Wu, S. Yang, R. Po, Y. Xu, Z. Liu, D. Lin, and G. Wetzstein. Video world models with long-term spatial memory. arXiv preprint arXiv:2506.05284, 2025. [82] Z. Xiao, Y. Lan, Y. Zhou, W. Ouyang, S. Yang, Y. Zeng, and X. Pan. Worldmem: Long-term consistent world simulation with memory. arXiv preprint arXiv:2504.12369, 2025. [83] T. Xie, Z. Zong, Y. Qiu, X. Li, Y. Feng, Y. Yang, and C. Jiang. Physgaussian: Physicsintegrated 3d gaussians for generative dynamics. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 43894398, 2024. [84] W. Xu, Y. Zhan, Z. Zhong, and X. Sun. Sequential gaussian avatars with hierarchical motion context. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 1359213603, 2025. [85] J. Yang, S. Yang, A. W. Gupta, R. Han, L. Fei-Fei, and S. Xie. Thinking in space: How multimodal large language models see, remember, and recall spaces. In Proceedings of the Computer Vision and Pattern Recognition Conference, pages 1063210643, 2025. [86] S. Yang, J. Yang, P. Huang, E. Brown, Z. Yang, Y. Yu, S. Tong, Z. Zheng, Y. Xu, M. Wang, et al. Cambrian-s: Towards spatial supersensing in video. arXiv preprint arXiv:2511.04670, 2025. [87] Y. Yang, Y. Zhang, M. Wu, K. Zhang, Y. Zhang, H. Yu, Y. Hu, and B. Wang. Twinmarket: scalable behavioral and social simulation for financial markets. arXiv preprint arXiv:2502.01506, 2025. [88] Z. Yang, X. Gao, W. Zhou, S. Jiao, Y. Zhang, and X. Jin. Deformable 3d gaussians for highfidelity monocular dynamic scene reconstruction. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 2033120341, 2024. [89] Z. Yang, X. Song, X. Xu, Y. Shi, G. Wang, M. K. Kalra, and P. Yan. Xray2xray: World model from chest x-rays with volumetric context. arXiv preprint arXiv:2506.19055, 2025. [90] Z. Yang, H. Yang, Z. Pan, and L. Zhang. Real-time photorealistic dynamic scene representation and rendering with 4d gaussian splatting. arXiv preprint arXiv:2310.10642, 2023. [91] K. Ye, T. Shao, and K. Zhou. When gaussian meets surfel: Ultra-fast high-fidelity radiance field rendering. ACM Transactions on Graphics (TOG), 44(4):115, 2025. [92] Z. Yu, T. Sattler, and A. Geiger. Gaussian opacity fields: Efficient adaptive surface reconstruction in unbounded scenes. ACM Transactions on Graphics (ToG), 43(6):113, 2024. [93] Y. Zhan, Z. Li, M. Niu, Z. Zhong, S. Nobuhara, K. Nishino, and Y. Zheng. Kfd-nerf: Rethinking dynamic nerf with kalman filter. In European Conference on Computer Vision, pages 118. Springer, 2024. [94] Y. Zhan, W. Xu, Q. Zhu, M. Niu, M. Ma, Y. Liu, Z. Zhong, X. Sun, and Y. Zheng. R3-avatar: Record and retrieve temporal codebook for reconstructing photorealistic human avatars. arXiv preprint arXiv:2503.12751, 2025. [95] Y. Zhan, Q. Zhu, M. Niu, M. Ma, J. Zhao, Z. Zhong, X. Sun, Y. Qiao, and Y. Zheng. Towards explicit exoskeleton for the reconstruction of complicated 3d human avatars. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 1425914269, 2025. [96] B. Zhang, C. Fang, R. Shrestha, Y. Liang, X. Long, and P. Tan. Rade-gs: Rasterizing depth in gaussian splatting. arXiv preprint arXiv:2406.01467, 2024. [97] K. Zhang, Z. Tang, X. Hu, X. Pan, X. Guo, Y. Liu, J. Huang, L. Yuan, Q. Zhang, X.-X. Long, et al. Epona: Autoregressive diffusion world model for autonomous driving. arXiv preprint arXiv:2506.24113, 2025. [98] W. Zhang, H. Xiang, Z. Liao, X. Lai, X. Li, and L. Zeng. 2dgs-room: Seed-guided 2d gaussian splatting with geometric constrains for high-fidelity indoor scene reconstruction. arXiv preprint arXiv:2412.03428, 2024. 18 [99] Y. Zhang, Y. Su, C. Wang, T. Li, Z. Wefers, J. Nirschl, J. Burgess, D. Ding, A. Lozano, E. Lundberg, et al. Cellflux: Simulating cellular morphology changes via flow matching. arXiv preprint arXiv:2502.09775, 2025. [100] G. Zhao, X. Wang, Z. Zhu, X. Chen, G. Huang, X. Bao, and X. Wang. Drivedreamer-2: Llm-enhanced world models for diverse driving video generation. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 39, pages 1041210420, 2025. [101] R. Zheng, J. Wang, S. Reed, J. Bjorck, Y. Fang, F. Hu, J. Jang, K. Kundalia, Z. Lin, L. Magne, et al. Flare: Robot learning with implicit world modeling. arXiv preprint arXiv:2505.15659, 2025. [102] S. Zhou, Y. Du, J. Chen, Y. Li, D.-Y. Yeung, and C. Gan. Robodreamer: Learning compositional world models for robot imagination. arXiv preprint arXiv:2404.12377, 2024."
        }
    ],
    "affiliations": [
        "Northwestern Polytechnical University",
        "Shanghai AI Laboratory",
        "Shanghai Jiao Tong University",
        "Sichuan University",
        "The University of Tokyo"
    ]
}