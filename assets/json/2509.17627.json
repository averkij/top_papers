{
    "paper_title": "OmniInsert: Mask-Free Video Insertion of Any Reference via Diffusion Transformer Models",
    "authors": [
        "Jinshu Chen",
        "Xinghui Li",
        "Xu Bai",
        "Tianxiang Ma",
        "Pengze Zhang",
        "Zhuowei Chen",
        "Gen Li",
        "Lijie Liu",
        "Songtao Zhao",
        "Bingchuan Li",
        "Qian He"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Recent advances in video insertion based on diffusion models are impressive. However, existing methods rely on complex control signals but struggle with subject consistency, limiting their practical applicability. In this paper, we focus on the task of Mask-free Video Insertion and aim to resolve three key challenges: data scarcity, subject-scene equilibrium, and insertion harmonization. To address the data scarcity, we propose a new data pipeline InsertPipe, constructing diverse cross-pair data automatically. Building upon our data pipeline, we develop OmniInsert, a novel unified framework for mask-free video insertion from both single and multiple subject references. Specifically, to maintain subject-scene equilibrium, we introduce a simple yet effective Condition-Specific Feature Injection mechanism to distinctly inject multi-source conditions and propose a novel Progressive Training strategy that enables the model to balance feature injection from subjects and source video. Meanwhile, we design the Subject-Focused Loss to improve the detailed appearance of the subjects. To further enhance insertion harmonization, we propose an Insertive Preference Optimization methodology to optimize the model by simulating human preferences, and incorporate a Context-Aware Rephraser module during reference to seamlessly integrate the subject into the original scenes. To address the lack of a benchmark for the field, we introduce InsertBench, a comprehensive benchmark comprising diverse scenes with meticulously selected subjects. Evaluation on InsertBench indicates OmniInsert outperforms state-of-the-art closed-source commercial solutions. The code will be released."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 2 2 ] . [ 1 7 2 6 7 1 . 9 0 5 2 : r OmniInsert: Mask-Free Video Insertion of Any Reference via Diffusion Transformer Models Jinshu Chen Xinghui Li Xu Bai"
        },
        {
            "title": "Zhuowei Chen Gen Li",
            "content": "Lijie Liu Songtao Zhao Bingchuan Li Qian He Intelligent Creation Lab, ByteDance"
        },
        {
            "title": "Abstract",
            "content": "Recent advances in video insertion based on diffusion models are impressive. However, existing methods rely on complex control signals but struggle with subject consistency, limiting their practical applicability. In this paper, we focus on the task of Mask-free Video Insertion, and aim to resolve three key challenges: data scarcity, subjectscene equilibrium, and insertion harmonization. To address the data scarcity, we propose new data pipeline InsertPipe, constructing diverse cross-pair data automatically. Building upon our data pipeline, we develop OmniInsert, novel unified framework for mask-free video insertion from both single and multiple subject references. Specifically, to maintain subject-scene equilibrium, we introduce simple yet effective ConditionSpecific Feature Injection mechanism to distinctly inject multi-source conditions and propose novel Progressive Training strategy that enables the model to balance feature injection from subjects and source video. Meanwhile, we design the Subject-Focused Loss to improve the detailed appearance of the subjects. To further enhance insertion harmonization, we propose an Insertive Preference Optimization methodology to optimize the model by simulating human preferences, and incorporate Context-Aware Rephraser module during reference to seamlessly integrate the subject into the original scenes. To address the lack of benchmark for the field, we introduce InsertBench, comprehensive benchmark comprising diverse scenes with meticulously selected subjects. Evaluation on InsertBench indicates OmniInsert outperforms state-of-the-art closed-source commercial solutions. Code will be released. Date: September 23, 2025 Project Page: https://phantom-video.github.io/OmniInsert/"
        },
        {
            "title": "Introduction",
            "content": "The emergence of diffusion models [11, 38] has significantly advanced the field of video generation. In recent years, numerous open-source and commercial video generation models have been developed [8, 13, 46], providing powerful tools for creating video content. Beyond generation, video editing naturally arises as an extension, focusing on re-generate reference video under fine-grained control signals like depth [6, 29], pose [15, 20, 35, 51], motion [7, 18, 27, 30] and point [5, 44]. Among various editing techniques, referenceimage-based editing [19, 23] plays key role by providing explicit visual cues to guide the editing process. Specially, the Video Insertion (VI) task of inserting reference subject into the source video has sparked considerable research interest, due to its significant potential for practical applications in film production, advertising, and artistic design. * Equal Contribution, Corresponding author, Project lead. 1 Figure 1 Showcase of OmniInsert in Real-World scenarios. 2 Figure 2 Showcase of OmniInsert in Reality-virtuality-mixed and Multi-object scenarios. 3 Prior approaches [23, 54] employ DDIM inversion for noise initialization to maintain the primary structure of the reference video and inject subject features during the denoising steps. However, these methods introduce an extra inference stage that doubles the computational time and frequently cause discontinuity and unnaturalness in the inserted subject. Recently, VideoAnyDoor [45] and GetInVideo [55] propose end-to-end video insertion frameworks to process reference images and condition videos simultaneously. Nevertheless, they rely on complex control signals (e.g., masks or points) to guide insertion position and motion and struggle with subject consistency. Very recently, some unified video editing works [28, 53] support video insertion based on instruction. However, they still suffer from issues of subject inconsistency and unnatural interactions, limiting practical applicability. In this work, our focus is on the task of Mask-free Video Insertion (MVI), inserting user-defined characters into reference video according to the customized prompt. The task poses several challenges, including: 1) Data Scarcity: Lacking paired videos before and after insertion along with corresponding subject references; 2) Subject-scene Equilibrium: Ensuring consistency of inserted subject while maintaining invariance of unedited areas of the reference video; 3) Insertion Harmonization: Achieving plausible positioning and motion of inserted subjects to ensure natural interactions. To address the Data Scarcity, we propose new data pipeline InsertPipe, producing training data consisting of reference subjects paired with appropriately edited videos and textual prompt. Specifically, we introduce three data curation pipelines: RealCapture Pipe, SynthGen Pipe, and SimInteract Pipe, aiming to enhance data diversity, thereby improving the models robustness across complex scenarios. The RealCapture Pipe leverages existing real-world videos, constructing paired videos through detection, tracking, and video erasing tools. Unlike prior methods [3, 16] that extract key frames from videos as reference subjects, we focus on constructing cross-video subject pairs to prevent copy-paste issue. To ensure comprehensive scene diversity, the SynthGen Pipe utilizes Large Language Model (LLM) [37] to generate varied prompts. These prompts are combined with techniques like image generation, image editing, video generation, and subject removal to automatically construct large-scale cross-pair datasets. For the scarcity of complex scene interaction data, we design the SimInteract Pipe to generate customized data based on the rendering engine. Building upon our data pipeline, we develop OmniInsert, novel unified framework for mask-free video insertion, supporting both single and multiple subject references with customized prompts. To maintain Subject-scene Equilibrium, we introduce three targeted improvements across model architecture, training strategy, and supervision. Firstly, we introduce simple yet effective Condition-Specific Feature Injection (CFI) mechanism. It distinctly injects conditions from the source video and reference subject while preserving efficiency, requiring only minimal adjustments to the video foundation model. Secondly, we propose novel Progressive Training (PT) strategy, which enables the model to balance multi-condition injection through multi-stage optimization. Thirdly, we design Subject-Focused Loss (SL) to aid the model in focusing on capturing the detailed appearance of the subjects. To enhance Insertion Harmonization in complex visual scenarios, we propose two complementary techniques. First, Insertive Preference Optimization (IPO) guides the model to learn context-aware insertion strategies using curated set of paired videos that reflect human preferences across diverse scenes. Second, the ContextAware Rephraser (CAR) enriches user prompts at inference time by injecting fine-grained scene details (such as object textures, spatial layout, and interaction cues) into the instruction. This allows inserted content to better align with both the semantics and visual structure of the scene. Through the aforementioned improvements, our approach enables effective insertion of customized subjects into reference videos, as illustrated in Fig. 1 and Fig. 2. To address the lack of benchmark for MVI, we introduce comprehensive benchmark, InsertBench, which consists of 120 videos paired with meticulously selected subjects (suitable for insertion in each video) and the corresponding prompts. Leveraging this benchmark, we conduct extensive evaluations demonstrating that OmniInsert achieves clear advantages over state-of-the-art commercial solutions, both quantitatively and qualitatively. Overall, our contributions are summarized as follows. Technology. 1) We develop InsertPipe, systematic data curation framework featuring multiple data pipelines to automatically generate high-quality and diverse data; 2) We propose OmniInsert, unified mask-free 4 Figure 3 Overview of InsertPipe. It consists of three data construction pipelines: Real Pipe, Rendering Pipe, and Gen Pipe. architecture capable of seamlessly inserting both single and multiple reference subjects into videos; 3) We introduce InsertBench, comprehensive benchmark tailored for MVI task. Significance. 1) OmniInsert demonstrates superior generation quality, bridging the gap between academic research and commercial-grade applications; 2) We present comprehensive study of the MVI taskincluding data, model, and benchmarkwill be publicly released to support future research and development."
        },
        {
            "title": "2 Related Work",
            "content": "Video Foundation Model. The development of diffusion models [11] has significantly advanced video foundation model research. Early models [2, 9] extended pre-trained Text-to-Image (T2I) models for continuous video generation by adding temporal modules. VDM [12] extends 2D U-Net to 3D, while AnimateDiff [9] integrates 1D temporal attention into 2D spatial blocks for efficiency. These strategies allow them to leverage powerful image priors, but their capabilities can be constrained by the original image-centric architecture. Recently, the emergence of Diffusion Transformer(DiT) [38]-based methods for video generation has exhibited superior performance in quality and consistency. These methods treat video as sequence of spatiotemporal patches, processing them in unified manner with Transformer. These powerful scaling transformers [8, 22, 33, 34, 46, 52] enable generating longer and higher-quality videos, paving the way for various downstream video generation tasks. Video Insertion. Video Insertion (VI) aims to insert user-provided reference subject into source video while maintaining the consistency of unedited regions. Prior approaches [23, 54] employ DDIM inversion to initialize the generation noise based on the reference video and inject subject features during the denoising steps. Due to they inherently require an additional stage for inversion, computational overhead and inference costs increase significantly. VideoAnyDoor [45] proposes an end-to-end framework that precisely modifies both content and motion according to user-specified mask and point conditions. GetInVideo [55] employs 3D full-attention diffusion transformer architecture to jointly process reference images, condition videos, and masks, maintaining temporal coherence. However, these methods rely on complex control signals but struggle with subject consistency. Recently, some unified video editing works [28, 53] support mask-free video insertion but still encounter subject infidelity and incoherent interactions in complex scenarios. Consider commercial software [21, 39] currently maintains state-of-the-art in MVI capabilities, its critical to bridge the 5 gap between academic research and commercialization."
        },
        {
            "title": "3 Preliminary",
            "content": "The Diffusion Transformer (DiT) [38] model employs transformer as the denoising network to refine diffusion latent. Our method inherits the video diffusion transformers trained using Flow Matching [31], which conducts the forward process by linearly interpolating between noise and data in straight line. At the time step t, latent zt is defined as: zt = (1 t)z0 + tϵ, where z0 is the clean video, and ϵ (0, 1) is the Gaussian noise. The model is trained to directly regress the target velocity: LFM = E[(z0 ϵ) vθ(zt, t, y)2], (1) where vθ refers to the diffusion model output and denotes condition. Our diffusion model is composed of stacked DiT blocks, which perform 2D self-attention for spatial modeling and 3D self-attention for spatiotemporal fusion."
        },
        {
            "title": "4 Methodology",
            "content": "Given reference subjects and source video, we aim to generate new composited video showcasing natural interaction between the inserted contents and the original scenes. We first introduce new data pipeline InsertPipe to construct diverse paired data (Sec. 4.1). Building upon our data pipeline, we develop novel framework OmniInsert employing core Condition-Specific Feature Injection (CFI) module for mask-free video insertion (Sec. 4.2). To further enhance insertion stability and quality, we design Progressive Training (PT) strategy incorporating the Subject-Focused Loss (SL) and Insertive Preference Optimization (IPO) during training (Sec. 4.3). Additionally, we propose Context-Aware Rephraser (CAR) module during inference to strengthen insertion harmonization (Sec. 4.4)."
        },
        {
            "title": "4.1\nTo address the challenge of data scarcity, we introduce a comprehensive data pipeline InsertPipe to produce\ndiverse data for the MVI task, encompassing RealCapture Pipe, SynthGen Pipe, and SimInteract Pipe, as\nshown in Fig. 3. The resulting training samples are formatted as {prompt, reference images, source video,\ntarget video}.",
            "content": "RealCapture Pipe. Using long videos from [48] and proprietary data, we segment videos into single-scene clips as target videos with AutoShot/PySceneDetect. The Vision-Language Model (VLM) [37] then captions these clips, detailing subject appearance, scenes, and interactions. These captions are processed by LLM [37] to extract subject categories/appearances for detection/tracking prompts [41], generating mask sequences. Furthermore, we apply video erasing techniques [56] to remove target subjects to create source videos, with VLM-based filtering mechanism that avoids visual artifacts of inpainting. For subject conditions, we construct cross-video pairs (avoiding same-video keyframes [3, 16]) to prevent copy-paste issues. Subjects are matched across CLIP [42] and facial embeddings [4], discarding pairs with extreme similarity (copy-paste) or dissimilarity (inconsistency). SynthGen Pipe. Considering the limited scene diversity, we introduce generation-based data pipeline comprising two complementary sub-flows. 1) We first meticulously construct subject and scene buckets with 300 categories and 1000 settings, respectively. Then, diverse subject-scene pairs generated by LLM are combined with text templates to produce cross-pair references via T2I models [24, 25]. To ensure consistency, VLM is further employed to score the appearance consistency and detail preservation within each pair. Subsequently, we synthesize target videos depicting natural interactions using the Image-to-Video (I2V) foundation models [8], while source videos are derived using the tracking-erase toolkit with the VLM filtering strategy. 2) Given videos depict empty scenes, we employ VLM to determine suitable inserted subjects and generate corresponding prompts. Next, we sample key frames and apply instruction-based image editing methods [32] to get target coarse positions. Leveraging temporal interpolation and video inpainting to synthesize the target videos, with VLM filtering ensuring temporal coherence and subject consistency. 6 Figure 4 Overview of OmniInsert. Note that the parts marked with * are only enabled in specific phases. SimInteract Pipe. Despite enhanced diversity, complex subject-scene interactions (e.g., man waving behind slowly opening doors) remain scarce. To address this, we develop rendering pipeline based on [14]. Specifically, we construct extensive asset libraries for subjects and scenes in the rendering environment. SpatialLM [36] then generates 350 layout priors to drive stochastic asset placement. Leveraging rigged assets with predefined motion bindings, we synthesize interactions by retrieving motion library animations through predefined prompts. Finally, we can leverage strategically positioned cameras to render target videos while paired source videos are derived via subject removal under identical cameras."
        },
        {
            "title": "4.2 OmniInsert Framework",
            "content": "Mask-free Video insertion is challenging task that requires accurate preservation of both subject identity and background consistency. straightforward solution is to inject VAE features of the references along the temporal dimension or to concatenate reference visual tokens after patchification. However, such approaches impose high computational overhead, and fail to account for the distinct alignment requirements of different conditions: reference videos demand frame-wise alignment with latent noise, while subject references require full temporal feature interaction. To address these challenges, we propose OmniInsert, novel end-to-end framework for mask-free video insertion using single or multiple subject references guided by text prompts, as shown in Fig. 4. At its core, we introduce simple yet effective Condition-Specific Feature Injection (CFI) mechanism that injects both video 7 and subject features in unified yet efficient manner, tailored to their respective temporal alignment needs. Condition-Specific Feature Injection. CFI adopts targeted designs in both the injection mechanism and channel-level flags to accommodate the distinct characteristics of video and object conditions. Specifically, for the video condition, source videos are concatenated with latent noise along the channel dimension, accompanied by dedicated flags, formally expressed as: = Concat([zT zVid , zS, S], dim = 1), (2) = (1 t)zT + tϵ, zT Rf chw denotes the clean target video latent. where noisy target video latent zT zS Rf chw represents reference source video latent, Rf 1hw is an all-zero vector. The choice of channel-wise concatenation aligns well with the background condition, facilitating effective spatial alignment. In contrast, for the subject condition, latents are concatenated along the temporal dimension to better capture dynamic changes across frames. As shown in Fig. 4, the subject latent is constructed as: = Concat([zI zSub , zI , ], dim = 1), (3) Rnchw denote the clean and noisy subject latents, with zI = (1 t)zI + tϵ, and denotes where zI , zI the number of inserted subjects. R11hw is an all-one flag map. The overall diffusion input is finally constructed by concatenating the video and subject condition latents along the frame dimension: zt = Concat([zVid ], dim = 0). This temporal concatenation effectively captures subject motion and continuity. , zSub Thanks to our differentiated condition injection strategies, CFI effectively achieves seamless subject insertion and background preservation while maintaining network efficiency. Furthermore, we integrate the LoRA mechanism into the DiT blocks to avoid expensive full-parameter updates, thereby preserving the models original text-alignment capabilities and visual fidelity."
        },
        {
            "title": "4.3 OmniInsert Training Pipeline\nProgressive Training. A significant challenge in training video insertion models lies in the inherent imbalance\nof learning difficulty: preserving the source video is mainly a copy-paste task, which is much simpler than the\ncomplex temporal and spatial modeling involved in dynamic subject insertion. Consequently, naive single-stage\ntraining tends to bias the model toward the simpler task of background preservation, often resulting in failed\nobject insertion, as illustrated in Fig 7. To mitigate these issues, we introduce a four-stage progressive training\nstrategy:",
            "content": "Phase 1. We begin by training the model to inject reference subjects based on subject features and text prompts, while discarding the source video condition. This isolates subject modeling and helps the model develop strong subject representation and motion generation capabilities. Phase 2. Based on the phase 1 model, we introduce the source video and conduct pretraining on the complete MVI task across the full dataset. This enables the model to acquire an initial capacity for aligning subjects with video backgrounds. However, models at this stage often suffer from identity inconsistency and show limited performance in complex scenes. Phase 3. To address the above limitations, we conduct refinement stage using curated dataset composed of high-fidelity portraits and synthetic renderings. short period of fine-tuning on this data significantly enhances identity preservation and improves the models robustness in visually complex environments. Phase 4. To further reduce physical implausibilities (e.g., unnatural poses, model penetration) and visual artifacts, we develop Insertion Preference Optimization (IPO), fine-tuning stage inspired by [40]. By leveraging small number of human-annotated preference pairs, IPO guides the model toward more visually and physically plausible insertions, yielding notable improvements in generation quality. Through progressive optimization, the model achieves balanced feature injection from subjects and source video, enhancing insertion stability and fidelity. Notably, the success of this progressive strategy also critically depends on carefully designed losses, which are described below. Method Subject Consistency CLIP-I DINO-I FaceSim Text-Video Alignment ViCLIP-T Video Quality Dynamic Image-Quality Aesthetics Consistency 0.682 0.664 0.745 Pika-Pro [39] Kling [21] Ours Table 1 Quantitative comparisons with baseline methods, and indicates the consistency of the segmented subject area. 24.721 23.091 25.945 0.910 0.921 0.930 0.655 0.667 0.704 0.543 0.513 0.639 0.524 0.521 0.556 0.422 0.539 0. 0.873 0.823 0."
        },
        {
            "title": "Method",
            "content": "Subject Consistency Align with Prompt Insertion Rationality Comprehensive Evaluation Pika-Pro [39] Kling [21] Ours 5.08% 29.42% 65.50% 10.67% 9.00% 22.67% 24.92% 68.33% 64.41% 8.58% 23.08% 68.34% Table 2 User study with baseline methods. Subject-Focused Loss. In Phases 13, the flow matching loss (LFM) serves as the primary training objective. However, since it treats all regions of the synthesized video equally, it often fails to maintain consistency for subjects that occupy relatively small spatial areas. To address this issue, we propose the Subject-Focused Loss (SL), modification of LFM that directs the models attention more strongly toward the subject regions. Formally, the loss is defined as: LSL = E[M [(z0 ϵ) Vθ(zt, t, y)]2], (4) where denotes spatially downsampled masks derived from tracking. Thus, the overall loss for Phase 1-3 is: = λ1LFM + λ2LSL. (5) IPO Loss. In Phase 4, we propose preference optimization mechanism to improve visual stability and physical plausibility. Specifically, the well-trained phase-3 model serves as the reference model πref, while an additional LoRA module is initialized as the trainable policy πθ. For preference data construction, we curate paired samples (preferred yw and dispreferred yl) through human selection of model inference outputs. Similar to [50], the loss is: (cid:104) LIPO = LDPO + λ ( log πθ(ylx) γ)2(cid:105) , (6) LDPO = (cid:20) (cid:18) log σ β log πθ(ywx) πref(ywx) β log πθ(ylx) πref(ylx) (cid:19)(cid:21) , (7) where denotes the model input for brevity and σ represents the standard Sigmoid function. Notably, we find that limited dataset of only 500 preference pairs suffices to achieve substantial performance gains at this stage. Note that the IPO loss is exclusively applied in phase 4 as the sole objective function."
        },
        {
            "title": "4.4 OmniInsert Inference Pipeline\nClassifier-free guidance [10] balances sample quality and diversity in diffusion models through joint conditional\nand unconditional training. During inference, we design a joint classifier-free guidance to balance multiple\nconditions:",
            "content": "ˆvθ(zt, cp, ci, cv) = vθ(zt, , , ) + S1 (vθ(zt, cp, , ) vθ(zt, , , )) + S2 (vθ(zt, cp, ci, ) vθ(zt, cp, , )) + S3 (vθ(zt, cp, ci, cv) vθ(zt, cp, ci, )) , (8) where cp, ci, cv represent prompt condition, reference subjects, and source video. S1, S2 and S3 are guidance scales. Context-Aware Rephraser. To enhance the coherence of subject insertion in complex visual scenes, we introduce Context-Aware Rephraser (CAR) module at inference time, as shown in Fig 4. CAR leverages the VLM to generate detailed, context-aware prompts that guide the model toward more seamless and plausible integration of the inserted subject into the source scene. Specifically, CAR first prompts the VLM to produce fine-grained descriptions of both the source environment and the reference subject. It then provides the VLM with creative constraintssuch as preserving the original video setting, encouraging imaginative yet coherent insertion of the subject, and maintaining spatial consistency in terms of size and position, etc. As result, CAR produces prompts that more accurately reflect the users creative intent, particularly in terms of visual effects (VFX), ultimately improving the realism and harmony of insertions achieved by OmniInsert."
        },
        {
            "title": "5.1 Setup\nInsertBench. We present InsertBench, a comprehensive benchmark for evaluating MVI capabilities. The\nbenchmark comprises 120 videos spanning various environments, including indoor, natural landscapes, wearable\nscenarios, and even animated scenes. Each video is approximately 5 seconds long with 121 frames, paired\nwith meticulously selected subjects to ensure visually compatible insertion. Such a diverse benchmark would\nbe beneficial for the community.",
            "content": "Implementation Details. The training protocol comprises four phases: 1) 70k iterations for subject-to-video task in phase 1; 2) 30k iterations for MVI task pretraining in phase 2; 3) 10k iterations for model refinement in phase 3; 4) 8k iterations for preference optimization in phase 4. The total computational resources consumed approximately 7000 GPU-hours on A100. During inference, we use the Euler sampler with 50 steps and set S1, S2, S3 to 7.5, 3.5 and 1.5, respectively. For fair comparison, all experiments are conducted at the resolution of 480p. Please refer to the supplementary materials for more details. Baselines. For the MVI task, the existing available state-of-the-art methods are closed-source commercial software. Therefore, we evaluate and compare the latest capabilities of Pika-Pro [39] and Kling [21]. Evaluation Metrics. We assess three dimensions: CLIP-I, DINO-I and FaceSim [4] for Subject Consistency; ViCLIP-T [49] for Text-Video Alignment; Dynamic Quality, Image-Quality, Aesthetics and Consistency [17] for Video Quality."
        },
        {
            "title": "5.3 Quantitative Comparisons\nMetric Evaluation. For subject consistency, we uniformly sample 10 frames per video and compute their\nsimilarity to reference subjects. For prompt following, we measure text-video cosine similarity. Tab. 1\nshows OmniInsert leads in metrics of CLIP-I∗, DINO-I∗ and ViCLIP-T. Although Kling achieves higher face\nsimilarity in portrait scenarios, they encounter copy-paste issues (1st line in Fig. 5 and Fig. 6). Moreover, our\nmethod shows superiority in video quality.",
            "content": "User Study. We conduct user study to evaluate the generation quality of our model. We assigned 40 test samples and invited 30 volunteers to select the most preferred result under four criteria: subject consistency, text alignment, insertion rationality and comprehensive evaluation. Tab. 2 shows that OmniInsert achieves significant advantages. 10 Figure 5 Qualitative comparisons with state-of-the-art methods. Please zoom in for more details. 11 Figure 6 Qualitative comparisons II with state-of-the-art methods. Please zoom in for more details."
        },
        {
            "title": "5.4 Ablation Studies",
            "content": "To demonstrate the effectiveness of our proposed method, we conducted the following ablation studies: a). Directly training the MVI task on the full dataset without the Progressive Training (PT) method of phases 1-3 (denoted as w/o PT). b). Not using the Subject-Focused Loss (SL) during training (denoted as w/o SL). c). Simply using the user instruction without Context-Aware Rephraser (CAR) during inference (denoted as w/o CAR). d). Not using the Insertion Preference Optimization (IPO) phase during training (denoted as w/o IPO). As shown in Fig. 7 and Table. 3, we demonstrate the effectiveness of our proposed modules: a). PT substantially facilitates subject-scene equilibrium through multi-stage optimization, enhancing insertion stability; b). SL guides the model to focus on more critical yet harder-to-fit subject regions, improving subject preservation; c). CAR generates rich descriptions of subject-scene interactions, facilitating more natural compositions; d). IPO effectively enhances insertion rationality while mitigating visual artifacts. Figure 7 Ablation results of our method."
        },
        {
            "title": "Method",
            "content": "CLIP-I DINO-I ViCLIP-T Aesthetic 0.642 a) w/o PT b) w/o SL 0.657 c) w/o CAR 0.689 d) w/o IPO 0.732 0.745 e) Ours 0.533 0.587 0.609 0.625 0.639 22.883 25.908 23.052 24.537 25.945 0.520 0.542 0.532 0.507 0.556 Table 3 Ablation study of our method."
        },
        {
            "title": "6 Conclusion",
            "content": "This paper presents comprehensive study of the task, i.e., Mask-free Video Insertion. To address the data scarcity, we introduce new data pipe InsertPipe to produce diverse paired data. Furthermore, we develop novel unified framework OmniInsert, leveraging Condition-Specific Feature Injection mechanism and incorporating Progressive Training strategy with Subject-Focused Loss to achieve subject-scene equilibrium. Additionally, we propose an Insertive Preference Optimization methodology and Context-Aware Rephraser module to enhance insertion harmonization. Finally, to address the lack of benchmark, we build InsertBench and experiments show that OmniInsert outperforms closed-source commercial solutions."
        },
        {
            "title": "References",
            "content": "[1] Mohammad Gheshlaghi Azar, Zhaohan Daniel Guo, Bilal Piot, Remi Munos, Mark Rowland, Michal Valko, and Daniele Calandriello. general theoretical paradigm to understand learning from human preferences. In International Conference on Artificial Intelligence and Statistics, pages 44474455. PMLR, 2024. [2] Andreas Blattmann, Tim Dockhorn, Sumith Kulal, Daniel Mendelevitch, Maciej Kilian, Dominik Lorenz, Yam Levi, Zion English, Vikram Voleti, Adam Letts, et al. Stable video diffusion: Scaling latent video diffusion models to large datasets. arXiv preprint arXiv:2311.15127, 2023. [3] Tsai-Shien Chen, Aliaksandr Siarohin, Willi Menapace, Yuwei Fang, Kwot Sin Lee, Ivan Skorokhodov, Kfir Aberman, Jun-Yan Zhu, Ming-Hsuan Yang, and Sergey Tulyakov. Multi-subject open-set personalization in video generation. In Proceedings of the Computer Vision and Pattern Recognition Conference, pages 60996110, 2025. [4] Jiankang Deng, Jia Guo, Niannan Xue, and Stefanos Zafeiriou. Arcface: Additive angular margin loss for deep face recognition. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 46904699, 2019. [5] Yufan Deng, Ruida Wang, Yuhao Zhang, Yu-Wing Tai, and Chi-Keung Tang. Dragvideo: Interactive drag-style video editing. In European Conference on Computer Vision, pages 183199. Springer, 2024. [6] Patrick Esser, Johnathan Chiu, Parmida Atighehchian, Jonathan Granskog, and Anastasis Germanidis. Structure In Proceedings of the IEEE/CVF international and content-guided video synthesis with diffusion models. conference on computer vision, pages 73467356, 2023. [7] Ruoyu Feng, Wenming Weng, Yanhui Wang, Yuhui Yuan, Jianmin Bao, Chong Luo, Zhibo Chen, and Baining Guo. Ccedit: Creative and controllable video editing via diffusion models. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 67126722, 2024. [8] Yu Gao, Haoyuan Guo, Tuyen Hoang, Weilin Huang, Lu Jiang, Fangyuan Kong, Huixia Li, Jiashi Li, Liang Li, Xiaojie Li, et al. Seedance 1.0: Exploring the boundaries of video generation models. arXiv preprint arXiv:2506.09113, 2025. [9] Yuwei Guo, Ceyuan Yang, Anyi Rao, Zhengyang Liang, Yaohui Wang, Yu Qiao, Maneesh Agrawala, Dahua Lin, and Bo Dai. Animatediff: Animate your personalized text-to-image diffusion models without specific tuning. arXiv preprint arXiv:2307.04725, 2023. [10] Jonathan Ho and Tim Salimans. Classifier-free diffusion guidance. arXiv preprint arXiv:2207.12598, 2022. [11] Jonathan Ho, Ajay Jain, and Pieter Abbeel. Denoising diffusion probabilistic models. Advances in neural information processing systems, 33:68406851, 2020. [12] Jonathan Ho, Tim Salimans, Alexey Gritsenko, William Chan, Mohammad Norouzi, and David Fleet. Video diffusion models. Advances in neural information processing systems, 35:86338646, 2022. [13] Wenyi Hong, Ming Ding, Wendi Zheng, Xinghan Liu, and Jie Tang. Cogvideo: Large-scale pretraining for text-to-video generation via transformers. arXiv preprint arXiv:2205.15868, 2022. [14] houdini. houdini-engine. https://www.sidefx.com/products/houdini-engine/, 2025. [15] Li Hu. Animate anyone: Consistent and controllable image-to-video synthesis for character animation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 81538163, 2024. [16] Yuzhou Huang, Ziyang Yuan, Quande Liu, Qiulin Wang, Xintao Wang, Ruimao Zhang, Pengfei Wan, Di Zhang, and Kun Gai. Conceptmaster: Multi-concept video customization on diffusion transformer models without test-time tuning. arXiv preprint arXiv:2501.04698, 2025. [17] Ziqi Huang, Fan Zhang, Xiaojie Xu, Yinan He, Jiashuo Yu, Ziyue Dong, Qianli Ma, Nattapol Chanpaisit, Chenyang Si, Yuming Jiang, et al. Vbench++: Comprehensive and versatile benchmark suite for video generative models. arXiv preprint arXiv:2411.13503, 2024. [18] Hyeonho Jeong and Jong Chul Ye. Ground-a-video: Zero-shot grounded video editing using text-to-image diffusion models. arXiv preprint arXiv:2310.01107, 2023. [19] Zeyinzi Jiang, Zhen Han, Chaojie Mao, Jingfeng Zhang, Yulin Pan, and Yu Liu. Vace: All-in-one video creation and editing. arXiv preprint arXiv:2503.07598, 2025. 14 [20] Johanna Karras, Aleksander Holynski, Ting-Chun Wang, and Ira Kemelmacher-Shlizerman. Dreampose: Fashion video synthesis with stable diffusion. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 2268022690, 2023. [21] Keling. Image to video elements feature. https://app.klingai.com/cn/multimodal-to-video/add-object/new, 2025. [22] Weijie Kong, Qi Tian, Zijian Zhang, Rox Min, Zuozhuo Dai, Jin Zhou, Jiangfeng Xiong, Xin Li, Bo Wu, Jianwei Zhang, et al. Hunyuanvideo: systematic framework for large video generative models. arXiv preprint arXiv:2412.03603, 2024. [23] Max Ku, Cong Wei, Weiming Ren, Harry Yang, and Wenhu Chen. Anyv2v: tuning-free framework for any video-to-video editing tasks. arXiv preprint arXiv:2403.14468, 2024. [24] Black Forest Labs. Flux. https://github.com/black-forest-labs/flux, 2024. [25] Black Forest Labs, Stephen Batifol, Andreas Blattmann, Frederic Boesel, Saksham Consul, Cyril Diagne, Tim Dockhorn, Jack English, Zion English, Patrick Esser, Sumith Kulal, Kyle Lacey, Yam Levi, Cheng Li, Dominik Lorenz, Jonas Müller, Dustin Podell, Robin Rombach, Harry Saini, Axel Sauer, and Luke Smith. Flux.1 kontext: Flow matching for in-context image generation and editing in latent space, 2025. URL https: //arxiv.org/abs/2506.15742. [26] Feng Liang, Akio Kodaira, Chenfeng Xu, Masayoshi Tomizuka, Kurt Keutzer, and Diana Marculescu. Looking backward: Streaming video-to-video translation with feature banks. arXiv preprint arXiv:2405.15757, 2024. [27] Feng Liang, Bichen Wu, Jialiang Wang, Licheng Yu, Kunpeng Li, Yinan Zhao, Ishan Misra, Jia-Bin Huang, Peizhao Zhang, Peter Vajda, et al. Flowvid: Taming imperfect optical flows for consistent video-to-video synthesis. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 82078216, 2024. [28] Sen Liang, Zhentao Yu, Zhengguang Zhou, Teng Hu, Hongmei Wang, Yi Chen, Qin Lin, Yuan Zhou, Xin Li, Qinglin Lu, et al. Omniv2v: Versatile video generation and editing via dynamic content manipulation. arXiv preprint arXiv:2506.01801, 2025. [29] Miao Liao, Feixiang Lu, Dingfu Zhou, Sibo Zhang, Wei Li, and Ruigang Yang. Dvi: Depth guided video inpainting for autonomous driving. In European conference on computer vision, pages 117. Springer, 2020. [30] Jun Hao Liew, Hanshu Yan, Jianfeng Zhang, Zhongcong Xu, and Jiashi Feng. Magicedit: High-fidelity and temporally coherent video editing. arXiv preprint arXiv:2308.14749, 2023. [31] Yaron Lipman, Ricky TQ Chen, Heli Ben-Hamu, Maximilian Nickel, and Matt Le. Flow matching for generative modeling. arXiv preprint arXiv:2210.02747, 2022. [32] Shiyu Liu, Yucheng Han, Peng Xing, Fukun Yin, Rui Wang, Wei Cheng, Jiaqi Liao, Yingming Wang, Honghao Fu, Chunrui Han, et al. Step1x-edit: practical framework for general image editing. arXiv preprint arXiv:2504.17761, 2025. [33] Yixin Liu, Kai Zhang, Yuan Li, Zhiling Yan, Chujie Gao, Ruoxi Chen, Zhengqing Yuan, Yue Huang, Hanchi Sun, Jianfeng Gao, et al. Sora: review on background, technology, limitations, and opportunities of large vision models. arXiv preprint arXiv:2402.17177, 2024. [34] Guoqing Ma, Haoyang Huang, Kun Yan, Liangyu Chen, Nan Duan, Shengming Yin, Changyi Wan, Ranchen Ming, Xiaoniu Song, Xing Chen, et al. Step-video-t2v technical report: The practice, challenges, and future of video foundation model. arXiv preprint arXiv:2502.10248, 2025. [35] Yue Ma, Yingqing He, Xiaodong Cun, Xintao Wang, Siran Chen, Xiu Li, and Qifeng Chen. Follow your pose: Pose-guided text-to-video generation using pose-free videos. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 38, pages 41174125, 2024. [36] Yongsen Mao, Junhao Zhong, Chuan Fang, Jia Zheng, Rui Tang, Hao Zhu, Ping Tan, and Zihan Zhou. Spatiallm: Training large language models for structured indoor modeling. arXiv preprint arXiv:2506.07491, 2025. [37] OpenAI. Chatgpt (gpt-4 version). https://chat.openai.com/, 2024. [38] William Peebles and Saining Xie. Scalable diffusion models with transformers. In Proceedings of the IEEE/CVF international conference on computer vision, pages 41954205, 2023. 15 [39] Pika. Pikaadd. https://pika.art/pikadditions, 2025. [40] Rafael Rafailov, Archit Sharma, Eric Mitchell, Christopher Manning, Stefano Ermon, and Chelsea Finn. Direct preference optimization: Your language model is secretly reward model. Advances in neural information processing systems, 36:5372853741, 2023. [41] Nikhila Ravi, Valentin Gabeur, Yuan-Ting Hu, Ronghang Hu, Chaitanya Ryali, Tengyu Ma, Haitham Khedr, Roman Rädle, Chloe Rolland, Laura Gustafson, Eric Mintun, Junting Pan, Kalyan Vasudev Alwala, Nicolas Carion, Chao-Yuan Wu, Ross Girshick, Piotr Dollár, and Christoph Feichtenhofer. Sam 2: Segment anything in images and videos. arXiv preprint arXiv:2408.00714, 2024. URL https://arxiv.org/abs/2408.00714. [42] Shihao Shao and Qinghua Cui. 1st place solution in google universal images embedding. arXiv preprint arXiv:2210.08473, 2022. [43] Feifan Song, Bowen Yu, Minghao Li, Haiyang Yu, Fei Huang, Yongbin Li, and Houfeng Wang. Preference ranking optimization for human alignment. Proceedings of the AAAI Conference on Artificial Intelligence, 38 (17):1899018998, Mar. 2024. doi: 10.1609/aaai.v38i17.29865. URL https://ojs.aaai.org/index.php/AAAI/ article/view/29865. [44] Yao Teng, Enze Xie, Yue Wu, Haoyu Han, Zhenguo Li, and Xihui Liu. Drag-a-video: Non-rigid video editing with point-based interaction. arXiv preprint arXiv:2312.02936, 2023. [45] Yuanpeng Tu, Hao Luo, Xi Chen, Sihui Ji, Xiang Bai, and Hengshuang Zhao. Videoanydoor: High-fidelity video object insertion with precise motion control. arXiv preprint arXiv:2501.01427, 2025. [46] Team Wan, Ang Wang, Baole Ai, Bin Wen, Chaojie Mao, Chen-Wei Xie, Di Chen, Feiwu Yu, Haiming Zhao, Jianxiao Yang, et al. Wan: Open and advanced large-scale video generative models. arXiv preprint arXiv:2503.20314, 2025. [47] Fu-Yun Wang, Zhaoyang Huang, Weikang Bian, Xiaoyu Shi, Keqiang Sun, Guanglu Song, Yu Liu, and Hongsheng Li. Animatelcm: Computation-efficient personalized style video generation without personalized video data. In SIGGRAPH Asia 2024 Technical Communications, pages 15. 2024. [48] Qiuheng Wang, Yukai Shi, Jiarong Ou, Rui Chen, Ke Lin, Jiahao Wang, Boyuan Jiang, Haotian Yang, Mingwu Zheng, Xin Tao, et al. Koala-36m: large-scale video dataset improving consistency between fine-grained conditions and video content. In Proceedings of the Computer Vision and Pattern Recognition Conference, pages 84288437, 2025. [49] Yi Wang, Kunchang Li, Yizhuo Li, Yinan He, Bingkun Huang, Zhiyu Zhao, Hongjie Zhang, Jilan Xu, Yi Liu, Zun Wang, Sen Xing, Guo Chen, Junting Pan, Jiashuo Yu, Yali Wang, Limin Wang, and Yu Qiao. Internvideo: General video foundation models via generative and discriminative learning. arXiv preprint arXiv:2212.03191, 2022. [50] Teng Xiao, Yige Yuan, Huaisheng Zhu, Mingxiao Li, and Vasant Honavar. Cal-dpo: Calibrated direct preference optimization for language model alignment. Advances in Neural Information Processing Systems, 37: 114289114320, 2024. [51] Zhongcong Xu, Jianfeng Zhang, Jun Hao Liew, Hanshu Yan, Jia-Wei Liu, Chenxu Zhang, Jiashi Feng, and Mike Zheng Shou. Magicanimate: Temporally consistent human image animation using diffusion model. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 14811490, 2024. [52] Zhuoyi Yang, Jiayan Teng, Wendi Zheng, Ming Ding, Shiyu Huang, Jiazheng Xu, Yuanming Yang, Wenyi Hong, Xiaohan Zhang, Guanyu Feng, et al. Cogvideox: Text-to-video diffusion models with an expert transformer. arXiv preprint arXiv:2408.06072, 2024. [53] Zixuan Ye, Xuanhua He, Quande Liu, Qiulin Wang, Xintao Wang, Pengfei Wan, Di Zhang, Kun Gai, Qifeng Chen, and Wenhan Luo. Unic: Unified in-context video editing. arXiv preprint arXiv:2506.04216, 2025. [54] Yuyang Zhao, Enze Xie, Lanqing Hong, Zhenguo Li, and Gim Hee Lee. Make-a-protagonist: Generic video editing with an ensemble of experts. arXiv preprint arXiv:2305.08850, 2023. [55] Shaobin Zhuang, Zhipeng Huang, Binxin Yang, Ying Zhang, Fangyikang Wang, Canmiao Fu, Chong Sun, Zheng-Jun Zha, Chen Li, and Yali Wang. Get in video: Add anything you want to the video. arXiv preprint arXiv:2503.06268, 2025. [56] Bojia Zi, Weixuan Peng, Xianbiao Qi, Jianan Wang, Shihao Zhao, Rong Xiao, and Kam-Fai Wong. Minimaxremover: Taming bad noise helps video object removal. arXiv preprint arXiv:2505.24873, 2025."
        },
        {
            "title": "A Implementation Details",
            "content": "A.1 Detailed Parameters The hyper-parameters used in our experiments are set as follows: For the IPO loss, we set the hyper-parameters γ, λ and β to 10, 1, 1, respectively, optimizing the model in phase 4. For the Subject-Focused Loss (SL), we set λ1 and λ2 as 1, 1, respectively. A.2 Training and Inference Details Training time. The phase-1 subject-to-video training takes about 70k iterations (nearly 2700 A100 GPU hours). The phase-2 pretraining for the MVI task costs about 30k iterations (nearly 1500 A100 GPU hours), and the phase-3 refinement costs about 10k iterations (nearly 500 A100 GPU hours). The last phase-4 preference optimization takes about 8k iterations (nearly 2300 A100 GPU hours). Training Parameters. To avoid the computational expense of full fine-tuning, we integrate the LoRA mechanism into the DiT blocks, configuring it with rank of 256. This results in total of 600M trainable parameters. Training Data Details. For the training of the phase-1 subject-to-video model and the phase-2 MVI task pretraining, we use large-scale training dataset containing about 1M samples. Mark data generated by the introduced RealCapture Pipe, SynthGen Pipe (T2I + I2V + subject remove), SynthGen Pipe (video editing), and SimInteract Pipe as (a), (b), (c), (d), respectively. The actual ratio of different types of data (type (a) : type (b) : type (c) : type (d)) in phase 1 and phase 2 is set to 5:2:2:1. For the training of phase-3 refinement, the dataset contains about 50k samples and the ratio of different data is 3:3:3:1. For the training of phase-4 preference optimization, the training dataset has about 0.5k good-bad paired data, and the ratio is set to 3:3:3:1. Inference time. Generating single 5-second 480P video (121 frames) with our proposed model takes approximately 90 seconds using 8 NVIDIA A100 GPUs. Further discussion on potential improvements to inference speed can be found in Sec. D. A.3 User Study To compare with the baseline methods, we conduct user study as part of the evaluation. The survey randomly presented 40 sets of generated results to each participant. Fig. 8 displays screenshot from our user study, showcasing set of generated results. From left to right, it shows the reference subject, the source video, and the results from three competing methods in random order, paired with the corresponding text prompt. We ask each participant four questions: 1. Which result appears to have the highest consistency with reference subjects? 2. Which result best matches the prompt [prompt] ? 3. Which result appears to have the highest insertion rationality? 4. Which result matches your best choice based on comprehensive considerations? For each set of results displayed in the survey, we ensured that their order was randomly shuffled to prevent bias. Responses where all answers had the same selection and responses with completely identical answers were considered invalid. Finally, we obtained total of 30 valid surveys to evaluate the model."
        },
        {
            "title": "B InsertBench Details",
            "content": "To address the lack of benchmark for Mask-free Video Insertion (MVI), we introduce comprehensive benchmark, InsertBench, which consists of 120 videos paired with meticulously selected subjects (suitable for insertion in each video) and the corresponding prompts. Our collected videos span diverse categories, including natural landscapes, indoor environments, transportation scenes, dynamic interactions, animated 18 Figure 8 Screenshot of user study. Figure 9 Examples of InsertBench. 19 contexts and wearable scenarios, each comprising 121 frames at 24 fps. As shown in Fig. 9, we carefully select suitable subjects for each video to insert, while considering harmonization and creativity."
        },
        {
            "title": "C More Visual Results",
            "content": "In this section, we provide more visual results of OmniInsert. Fig. 10 and Fig. 11 present our inference results, Fig. 12 and Fig. 13 show comparisons with baselines. Corresponding videos for all results (main paper and supplement) are available in supplementary attachments. We strongly recommend viewing the mentioned dynamic results for more intuitive understanding of the practical effectiveness of our proposed method. All results are generated at 480P resolution based on our proposed benchmark InsertBench. Supplemental results demonstrate enhanced capabilities in diverse aspect ratios (e.g., vertical video) and multi-subject scenarios, further improving our method can achieve stable, semantically coherent insertions with strong prompt fidelity. Thanks to our designed data curation pipeline InsertPipe, which covers wide range of scenes and subject categories, the model exhibits excellent robustness across various resolutions and aspect ratios. Our designed Condition-Specific Feature Injection (CFI) mechanism injects both video and subject features in unified yet efficient manner, ensuring differentiated condition injection. And we integrate the LoRA mechanism into the DiT blocks, preserving the models original prompt following capabilities. Furthermore, the Progressive Training (PT) strategy enables the model to effectively achieve subject-scene equilibrium through multi-stage optimization, further enhancing insertion stability. As shown in Fig. 12 and Fig. 13, both Pika-Pro [39] and Kling [21] exhibit issues with insertion failures and unnatural insertion artifacts, whereas our method demonstrates superior robustness. Additionally, our model is capable of dealing with multi-subject scenarios, capability that other baseline models either lack or perform poorly. This is achieved through our CFI design, where the IP information of multiple subjects is concatenated along the temporal dimension, enabling unified training framework with the common single-subject insertion."
        },
        {
            "title": "D Limitation and Future Work",
            "content": "In this section, we discuss the limitations of our method and potential directions for future work. Color Fidelity and Physical Plausibility. We observe that our generated results may occasionally exhibit slight color discrepancies compared to the reference video, as well as minor physically implausible phenomena, as illustrated in Fig. 14. Notably, such issues are not unique to our method and are commonly observed across competitive baselines. Although our designed Insertive Preference Optimization (IPO) in Phase-4 has significantly reduced the challenges, complete resolution is still difficult. Due to the dependence of the IPO mechanism on the models intrinsic high-quality sample generation capability, the pool of selectable high-quality examples may become constrained when such artifacts are prevalent, especially in diversity. Therefore, incorporating more advanced preference optimization techniques [1, 43] is crucial direction for future improvement. Inference Speed. Our method adheres to standard video diffusion model baseline, with an inference time of approximately 90 seconds for 480P video with 121 frames. In contrast, the inference speeds of the compared bashlines are all above 180s. (Notably, as our compared baselines are all closed-source, we are unable to conduct fair comparison by running their models offline.) Although our method demonstrates superior inference speed, its performance could be further enhanced by applying general acceleration techniques [26, 47] for video diffusion models. 20 Figure 10 More qualitative results I. Figure 11 More qualitative results II. 22 Figure 12 More qualitative comparisons I. 23 Figure 13 More qualitative comparisons II. Figure 14 The failure cases. (a) shows the physically implausible bad cases, such as model interpenetration, and (b) shows the slight color discrepancy between the source video and the inference result."
        }
    ],
    "affiliations": [
        "Intelligent Creation Lab, ByteDance"
    ]
}